{
    "title": "Long-Horizon Dialogue Understanding for Role Identification in the Game of Avalon with Large Language Models",
    "url": "https://openalex.org/W4389520297",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2568341931",
            "name": "Simon Stepputtis",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2035895243",
            "name": "Joseph Campbell",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2604405749",
            "name": "Yaqi Xie",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2137262959",
            "name": "Zhengyang Qi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2102437259",
            "name": "Wenxin Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1967302726",
            "name": "Ruiyi Wang",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A3110263836",
            "name": "Sanketh Rangreji",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2104319267",
            "name": "Charles Lewis",
            "affiliations": [
                "University of Pittsburgh"
            ]
        },
        {
            "id": "https://openalex.org/A2068240882",
            "name": "Katia Sycara",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4281758439",
        "https://openalex.org/W4287019748",
        "https://openalex.org/W3200895474",
        "https://openalex.org/W2962854379",
        "https://openalex.org/W2962776342",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4305033123",
        "https://openalex.org/W4389524599",
        "https://openalex.org/W2997117909",
        "https://openalex.org/W4311917449",
        "https://openalex.org/W4386907380",
        "https://openalex.org/W2946358633",
        "https://openalex.org/W2962852262",
        "https://openalex.org/W4381586770",
        "https://openalex.org/W4287077191",
        "https://openalex.org/W4311992269",
        "https://openalex.org/W4362655923",
        "https://openalex.org/W2948847269",
        "https://openalex.org/W3049346316",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W2952129646",
        "https://openalex.org/W4387322949",
        "https://openalex.org/W4323572061",
        "https://openalex.org/W2952607215",
        "https://openalex.org/W2982316857",
        "https://openalex.org/W4320559489",
        "https://openalex.org/W4248892918",
        "https://openalex.org/W4320342824",
        "https://openalex.org/W4387636003",
        "https://openalex.org/W2963641152",
        "https://openalex.org/W2955818920",
        "https://openalex.org/W3098201885",
        "https://openalex.org/W4383180654",
        "https://openalex.org/W4284899315",
        "https://openalex.org/W4378505261",
        "https://openalex.org/W2949813784",
        "https://openalex.org/W4225937850",
        "https://openalex.org/W4225323055",
        "https://openalex.org/W3100111242",
        "https://openalex.org/W4321649710",
        "https://openalex.org/W4389523767",
        "https://openalex.org/W2749002090",
        "https://openalex.org/W4377865190",
        "https://openalex.org/W4389520779",
        "https://openalex.org/W2607380417",
        "https://openalex.org/W4288089799"
    ],
    "abstract": "Simon Stepputtis, Joseph Campbell, Yaqi Xie, Zhengyang Qi, Wenxin Zhang, Ruiyi Wang, Sanketh Rangreji, Charles Lewis, Katia Sycara. Findings of the Association for Computational Linguistics: EMNLP 2023. 2023.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 11193–11208\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nLong-Horizon Dialogue Understanding\nfor Role Identification in the Game of Avalon with Large Language Models\nSimon Stepputtis1, Joseph Campbell1, Yaqi Xie1, Zhengyang Qi1, Wenxin Sharon Zhang1,\nRuiyi Wang1, Sanketh Rangreji1, Charles Michael Lewis2, Katia P. Sycara1\n1Carnegie Mellon University, 2University of Pittsburgh\n{stepputtis, jcampbell, yaqixie}@cmu.edu, ml@sis.pitt.edu\n{zqi2, wenxinz3, ruiyiwan, srangrej, sycara}@andrew.cmu.edu\nAbstract\nDeception and persuasion play a critical role in\nlong-horizon dialogues between multiple par-\nties, especially when the interests, goals, and\nmotivations of the participants are not aligned.\nSuch complex tasks pose challenges for current\nLarge Language Models (LLM) as deception\nand persuasion can easily mislead them, es-\npecially in long-horizon multi-party dialogues.\nTo this end, we explore the game of Avalon:\nThe Resistance , a social deduction game in\nwhich players must determine each other’s hid-\nden identities to complete their team’s objec-\ntive. We introduce an online testbed and a\ndataset containing 20 carefully collected and\nlabeled games among human players that ex-\nhibit long-horizon deception in a cooperative-\ncompetitive setting. We discuss the capabili-\nties of LLMs to utilize deceptive long-horizon\nconversations between six human players to\ndetermine each player’s goal and motivation.\nParticularly, we discuss the multimodal inte-\ngration of the chat between the players and\nthe game’s state that grounds the conversation,\nproviding further insights into the true player\nidentities. We find that even current state-\nof-the-art LLMs do not reach human perfor-\nmance, making our dataset a compelling bench-\nmark to investigate the decision-making and\nlanguage-processing capabilities of LLMs. Our\ndataset and online testbed can be found at our\nproject website: https://sstepput.github.\nio/Avalon-NLU/\n1 Introduction\nDespite the remarkable progress of large language\nmodels (LLMs) in natural language understand-\ning and generation, they have largely been ap-\nplied and evaluated in question-answering (Brown\net al., 2020), instruction following (Driess et al.,\n2023), and cooperative dialogue (Madotto et al.,\n2020) tasks. An oft-overlooked, yet important set-\nting is that of multi-party dialogue in cooperative-\ncompetitive scenarios, where participants hold pri-\nGame Chat\nGame State\nQuests Votes\nRound N\nRound 1\n...\nGame Chat\nChat\nGame Rep.\nRound r\n<roles>\n<belief of r-1>\n<roles>\nChat\nState\nModalities Context Representation\nRound-Based\nContext\nFull Context\nRole Prediction\nLLMVerificationRoles\nCollected Game Data\nState\nGame State\nQuests Party Votes\nRound R\nRound 1\n...\nGame Rep.\nRound [0, ..., r]\nFigure 1: Schematic representation of the Avalon role\nprediction pipeline. We experiment with three distinct\nmodalities: Chat only, Chat and State, and State only.\nThese representations are provided to the Language\nModel (LLM) either by using data from a single round\ncomplemented with a carried-over belief (round-based\ncontext) from the preceding round or by using the entire\nhistory since the game’s beginning (full context). Sub-\nsequent role predictions made by the LLM are validated\nfor consistency with the predefined Avalon roles.\nvate and – possibly competing – beliefs and agen-\ndas, yet seek to cooperate in service of a shared\ngoal. While humans excel at such tasks and are\ncapable of reaching group consensus even in the\npresence of bad faith actors and deception, there\nare significant challenges inherent to the setting\nwhich state-of-the-art LLMs are ill-suited to han-\ndle (Wang et al., 2023b).\nIn this work, we aim to explore these limitations\nby introducing a new benchmark and associated\ndataset for a multi-party cooperative-competitive\ntask based on the game of Avalon: The Resistance1.\nOur task features two teams of players with hidden\nroles and conflicting goals; one side seeks to fulfill\na series of objectives and the other sabotage them.\nThrough rounds of dialogue and voting, the players\nmust discover each other’s identity while conceal-\ning their own to win. We model the identification\nof player roles as a long-horizon dialogue under-\nstanding task, and show that it poses a challenge for\n1A game by Don Eskridge: https://en.wikipedia.\norg/wiki/The_Resistance_(game)\n11193\nrecent state-of-the-art LLMs due to three factors:\n1) a large number of participants in a multi-party\ndialogue, 2) the need to ground reasoning in both\ndialogue and game state, and 3) the active usage of\ndeception and persuasion by players.\nWhile prior works have explored dialogue un-\nderstanding in multi-party settings, they typically\ninvolve a small number of participants (Zahiri and\nChoi, 2017) or unnatural dialogue captured from\nonline forums (Lowe et al., 2015). Our task, in\ncontrast, involves natural dialogues with six par-\nticipants, each of whom have their own privileged\ninformation and agendas. This significantly in-\ncreases the complexity of the task, as participants\nmay address each other over longer horizons. Com-\npounding the issue is the need to incorporate game\nstate into the reasoning process, further increasing\nthe horizon if translated to natural language as is\ncommon in zero-shot paradigms. However, the\nability to reason over extended horizons is crucial\nin our context, as inconsistencies in behavior, eva-\nsive responses, and self-contradictions which can\nbe used to identify deceptive behavior may only\nmanifest over time.\nWe explore a selection of recent, publicly avail-\nable LLMs, varying in size and training data,\nto evaluate their effectiveness in understanding\nlong-horizon relations in our proposed benchmark.\nUnder the assumption that LLMs encode large\namounts of general knowledge, we aim to ascertain\nwhether specific state representations can facili-\ntate the comprehension of long-horizon tasks. We\nfind this to be a critical aspect given the small con-\ntext windows associated with many state-of-the-art\nLLMs, despite recent improvements (Dao et al.,\n2022; Ainslie et al., 2023; Press et al., 2021; Packer\net al., 2023).\nIn addition, we publicly release our dataset –\nas well as our browser-based version of Avalon –\nin order to promote further research in this direc-\ntion. Consisting of 20 games with 30 unique hu-\nman players and 19 unique team compositions, our\ndataset comprises 2384 pieces of dialog with hand-\nannotated persuasion strategies for each player, de-\nception strategies for evil players, player beliefs\nover the roles of other players throughout the game,\nand ground truth game state. To the best of our\nknowledge, this is the only high-quality dataset of\nits kind for a long-horizon multi-party dialogue\nfeaturing deception and persuasion. In keeping\nwith recent trends advocating for quality over quan-\ntity (Gunasekar et al., 2023; Raffel et al., 2020;\nLongpre et al., 2023), we expect this dataset to be\nuseful for both fine-tuning and evaluation of mod-\nels. During data collection, we have ensured that\nno spurious information channel exists between the\nplayers, ensuring that all conversation relevant to\nthe game is captured by our dataset. Concretely,\nour contributions are as follows:\n• A testbed and dataset containing 2384 utter-\nances from 20 human player games hand-\nannotated with strategies (persuasion and de-\nception), player beliefs, and game state.\n• A comprehensive analysis of LLM perfor-\nmance in our proposed multimodal long-\nhorizon dialogue understanding benchmark,\nincluding persuasive and deceptive behavior.\n• An exploration of the limitations of current\nmodels and the introduction of state repre-\nsentations that can improve long-horizon dia-\nlogue modeling.\n2 Related Work\nDialogue Understanding in Games: Dialogue\nunderstanding tasks such as the identification of\nintentions and motivations are essential for suc-\ncessful conversation (Weld et al., 2022), and are\nassociated with broader human cognitive activities.\nWhile games have long served as challenges in the\nAI community, they often lack such dialog under-\nstanding due to the difficulty in generating and eval-\nuating realistic dialog in commonly employed rein-\nforcement learning paradigms (Vinyals et al., 2019).\nAs a result, cooperative games with human partners\noften utilize simple, codified communication proto-\ncols such as in the game ofHanabi (Siu et al., 2021)\nor ignored entirely (Serrino et al., 2019). When di-\nalogue is incorporated (Zhao and Eskenazi, 2016;\nPang and Wang, 2020), it is often through dialogue\nstate tracking (DST) (Ren et al., 2018) in which\nparticipant beliefs (Oguntola et al., 2023) and in-\ntentions are modeled as semantic slots (Lee et al.,\n2021), and are inferred throughout the course of\na conversation. However, cooperative-competitive\ngames involving negotiation, persuasion, or decep-\ntion often require a more nuanced application of\nlanguage that leaves room for subjectivity and inter-\npretation. Past work on strategic dialogue systems\nhas avoided these issues by focusing on simpler set-\ntings (Lewis et al., 2017; Keizer et al., 2017; Wang\n11194\net al., 2019), which involve only a single partner,\nshorter dialogue contexts, or simpler strategies.\nRecent advances in LLMs have shown great po-\ntential (Liu et al., 2023; Li et al., 2023) across\nvarying problems, including conversing (OpenAI),\nknowledge parsing (Jiang et al., 2023; Zhang et al.,\n2023), and instruction following (Alayrac et al.,\n2022; Stepputtis et al., 2020; Xie et al., 2023),\nleading to the development of capable language-\nbased agents (Team et al., 2021; Wang et al., 2023a;\nDriess et al., 2023). As shown in White et al.\n(2023), the inherent knowledge embedded in these\nmodels’ weights allows for intricate answers, if\nthe prompt is posed correctly. Such agents have\nrecently been successfully applied to the game of\nDiplomacy (FAIR et al., 2022; Bakhtin et al., 2022),\nalthough a significant source of relevant domain\ndata was required for fine-tuning. This differs from\nthe game of Avalon, in which a) all communication\nis public and b) hidden roles explicitly encourage\ndeception, which leads to a more challenging di-\nalogue understanding problem as an agent must\ncarefully understand, analyze, and ground each\nplayer’s utterance.\nDeception and Persuasion in Dialogue: While\ndeception is increasingly studied in terms of misin-\nformation on social media (Shu et al., 2017), in this\nwork we focus on its analysis and detection in dia-\nlogue. Unlike prior works which explore deception\nthrough analysis of verbal (Hirschberg et al., 2005)\nor visual (Soldner et al., 2019) cues in spoken lan-\nguage from two-party dialogues, our benchmark\nis based on textual linguistic cues in multi-party\ndialogues. Although datasets have previously been\nintroduced for the games of Mafia (Ibraheem et al.,\n2022) and One Night Werewolf (Lai et al., 2022),\nwe find Avalon to be a significantly more challeng-\ning task due to the increased game length, resulting\nin more than double the number of utterances per\ngame in our dataset – 49, 64, and 119 for Mafia,\nWerewolf, and Avalon, respectively. This requires\ndialogue models to reason over significantly longer\ncontext horizons, but also provides enough infor-\nmation for us to reason over hidden player roles\nas opposed to simply inferring utterance labels. In\naddition, we provide high-quality annotated per-\nsuasion strategy labels for each utterance in our\ndataset, following annotation schemes introduced\nin prior works (Yang et al., 2019; Lai et al., 2022),\nas well as deception labels, covering the type of lies\nutilized by the evil players (Houston et al., 2013).\nThough persuasion has often been studied in the\ncontext of negotiation (Lewis et al., 2017; Keizer\net al., 2017) or other two-party dialogues (Wang\net al., 2019), multi-party persuasion analysis is of-\nten limited to online social media discussions (Al-\nthoff et al., 2014; Tan et al., 2016) which often\nmeaningfully differ from real-time dialogue.\n3 The Game of Avalon\nIn this section, we provide a description of the\ngame Avalon: The Resistance2, which is a social\ndeduction board game that can be played by five\nto ten players. Players assume various roles in the\ngame that differ in knowledge and goals, includ-\ning the Assassin, Merlin, Morgana, and Percival.\nAvalon is a cooperative-competitive game in which\ntwo groups of players attempt to infer the roles of\nthe other players by forming allegiances while hid-\ning or intentionally pretending to be a role different\nfrom their own. In this work, Avalon is played\namong six human players P = {p1, . . . , p6} – four\ngood, two evil. Generally, the goal of the evil play-\ners is to hide their identity and to convince the good\nplayers that they can be trusted.\nThe game progresses through up to five rounds\n– referred to as quests – with players engaging in\ndiscussions and voting to determine the composi-\ntion of a party that is subsequently sent on a quest.\nThe required size of a party that is sent on a quest\ndiffers in each round and has to be approved by a\npublic majority vote amongst all players. After a\nparty has been formed, each player in that party\nvotes anonymously whether or not the quest should\nsucceed, requiring all players to vote for success\nin order to succeed the quest. If the evil players\nsucceed in failing three quests, they automatically\nwin the game. Similarly, if the good players suc-\nceed three quests, they win the game, unless the\nAssassin can identify who plays Merlin (see Sec-\ntion A.1). If the Assassin is successful, evil wins\nthe game instead.\nAvalon: The Resistance introduces multiple spe-\ncial roles that impact gameplay, adding layers of\nstrategic depth to the game’s dynamics. In our set-\nting, the roles consist of Merlin, Percival, and two\nLoyal Servants as the forces of good, with Morgana\nand the Assassin on the side of evil. Detailed role\ndescriptions can be found in Section A.1\nWe further impose additional rules to facilitate\n2The game’s rules can be found here: https://www.\nultraboardgames.com/avalon/game-rules.php\n11195\nWe don't have any info so just selected randomly\nGame Started\nPlayer-4 proposed a party: Player-2, Player-6\nP4\nSorry for the last turn, its still random but just included myself\nP4 System\nPlayer-4 proposed a party: Player-2, Player-4\nSystem\nParty Vote Outcome: Player-1: Yes, Player-2: Yes, Player-3: \nNo, Player-4: Yes, Player-5: Yes, Player-6: Yes\nSystem\nVote Succeeded! Initiating Quest Vote!\nSystem\nQuest Succeeded!\nSystem System\nSeems like a weird choice of party in my opinion. Little bit \nsuspicious of player-4 for not picking themself.\nP5\nI don't have opinions at this point \nP6\nWell, technically we don't know anything yet. but I agree that this \nchoice is a little weird\nP1\nI'm good, but I agree this choice is weird. However, I don't know \nwhat changes we can make since this is the first turn. I'll approve \nthe current party unless you make some good arguments\nP2\nNo opinions but a good guy will always place themselves in the \nteam...\nP3\nFigure 2: Example of the players’ conversation during\nround one. Player P4 is the quest leader.\nthe online nature of our data collection. Particularly,\nwe enforce a turn-based discussion in which each\nplayer has a fixed amount of time to convey their\nthoughts via a chat interface, thus ensuring that\nall game-related interactions are captured. Should\na player exceed their allotted time, the game will\nautomatically progress to the next player, initiate\nappropriate votes, or apply default votes depending\non the situation. More details can be found in\nsection A.2\n4 Dialogue Understanding\nWe formulate our problem as identifying a player’s\ncharacter ci given the chat history hchat and\ngame state hstate. Formally, predict the role\nProle(ci|hchat, hstate) for each player in P. Addi-\ntionally, we also predict which of the six players is\nMerlin: PMerlin(pMerlin|hchat, hstate, e) where e is\nthe privileged knowledge of the evil players.\nIn the remainder of this section, we address the\ntwo fundamental issues of 1) understanding long-\nhorizon conversations, and 2) utilizing the environ-\nment’s state to enhance the inference capabilities\nof the LLM.\n4.1 Long-Horizon Dialogue Representation\nLong-horizon conversations between multiple par-\nticipants pose challenges for LLMs as their capa-\nbility for comprehending and tracking context is\nlimited. Especially in situations in which deception\nbehavior is present, as in Avalon, identifying such\nbehavior hinges on multiple factors. For example,\nconsistency when voting for parties depending on\nwhich players are on it, as well as identifying in-\nconsistencies in a player’s arguments. These serve\nas an indicator for deceptive behavior, and con-\nsequently, the true nature of the player’s charac-\nter c. We propose to break up conversations into\n“rounds” r representing a single round of discus-\nsion in which each player has had the chance to\nspeak at least once. Breakpoints are introduced at\nthe current quest leader’s position. An example\nround can be seen in Figure 2. In this example, we\npropose that players’ roles can be predicted given\nonly the current round ri and a structured belief\nstate b about each player’s identity that carries over\nbetween rounds. To track the identities of players\nacross rounds, the model is provided with a list of\ninitial player beliefs b = [b1, . . . , b6], where each\nbelief bi ∈ {good, evil, merlin, unknown}, as con-\ntext. The context is provided prior to consuming\nthe next round of conversation. Finally, the model\nis tasked to provide an updated list of player beliefs\nbnext after each round, which subsequently serves\nas an input for the next round rt+1.\nFigure 1 describes the three modalities we are\nbuilding from the game’s chat and game state. Each\nof these modalities can be used in either context\nrepresentation. Particularly, the game is either con-\nveyed to the LLM as the complete history – which\nwe refer to as full context – from the beginning of\nthe game up to the point of evaluation or as a round\nwith a carried-over belief b holding the role pre-\ndictions after the previous round, which we refer\nto as round-based context. The respective context\nis then utilized by the LLM through TypeScript\n(see Section 4.3.1), allowing for validation of the\ngenerated response, ensuring that a valid role has\nbeen assigned to each player.\n4.2 State Representation\nRecent LLMs have shown impressive, human-like,\nconversational capabilities and are increasingly de-\nployed as public-facing conversational agents (Ope-\nnAI). However, the shortcoming of these models is\nthat their interactions are mostly cooperative and\nthey assume that language does not need to be\ngrounded in the world’s state. With our testbed\nand accompanying dataset, we provide an intrigu-\ning, yet simple world in which language can be\n11196\ngrounded in a structured state representation of the\nrelevant environment.\nIn the game of Avalon, the relevant state infor-\nmation includes the currently proposed party, the\nrecord of successful and failed quests, the play-\ners that have been part of parties in prior rounds,\nas well as previous party formation votes. At its\ncore, we convert state information into linguistic\nstatements that are given to each LLM prior to\nthe player’s conversations. By utilizing this tech-\nnique, the LLM can leverage its inherent zero-\nshot and few-shot capabilities to solve our task of\nidentifying hidden player roles by interleaving spe-\ncially designed inputs, with little to no task-specific\nfine-tuning. In Avalon, states are conveyed to the\nLLMs in two manners: 1) through a global game\nstate that incorporates past quest outcomes, and 2)\nstate changes that happen in the currently evaluated\nround.\n4.2.1 Global State\nThe game’s global state is converted into text by uti-\nlizing a set of templates. Most importantly, the out-\ncome, as well as parties and votes for prior quests\nare translated as follows:\n• quest- i: oi (party: pj | player votes: vj)\nwhere i is quest qi’s ID, oi ∈ {success, failure} is\nthe outcome of quest, pj is the list of players that\nwent on quest qi and vj is a list of tuples including\nthe vote from each player of whether or not they\napproved of party pj. For example, the game’s\nstate after the first successful quest conducted by\nplayer one and two could look as follows:“quest-1:\nsuccess (party: player-1, player-2 | player votes:\nplayer-1: yes, player-2: yes, player-3: yes, player-\n4: yes, player-5: yes, player-6: yes)” . Further,\nthe global state also covers the currently proposed\nparty pproposed in order to provide context for the\nplayers’ discussion.\n4.2.2 Round-Based State Change\nIn addition to the game’s global state each round of\nconversation can be accompanied by proposing par-\nties, voting for such parties, and conducting quests.\nSuch changes have an effect on the global state;\nhowever, the global state only represents the game\nprior to the currently ongoing discussion. Thus,\nchanges that occur during a round are communi-\ncated through a 7-th player named “system”. Fig-\nure 2 demonstrates the conversation between the\nsix players during round one. Here, the system\nsystem: You are a helpful assistant that uses the chat be-\ntween six players, player-1 to player-6, who play Avalon:\nThe Resistance (a cooperative-competitive game) to iden-\ntify who is Merlin, Good or Evil. There are two evil play-\ners, which can usually be found because they are deceptive\nand lie about the good player’s roles and vote for quests\nand parties irrationally. For Merlin, watch out for individ-\nuals with knowledge of evil players’ identities, insightful\ncomments beyond their role, and caution regarding mission\nteams or specific players.\nhuman: The current state is: <STATE>(See Section 4.2.1)\nThe current party proposal is: <PARTY> (See Sec-\ntion 4.2.1)\nYour initial belief is: <BELIEF> (See Section 4.1)\nThis is the chat between player-1 to player-6:\n<CHAT> (See Section 4.2.2)\nWhat do you think is the role of each player? Please do\nnot explain your answer, do not elaborate on it further, and\ndo not say that these are just guesses; only provide the list\nand nothing else.\nTable 1: Prompt used for role inference.\nuser provides updates about the game’s global state\nduring the round.\n4.3 Tasks and Prompt Generation\nThe prompts used to predict roles Prole and predict\nMerlin PMerlin have been hand-designed to covey\nthe basics of the game as well as the expected out-\nput format. The prompts for our two prediction\ntasks are shown in Tables 1 and 4, respectively.\nEach prompt is designed such that various in-\nformation can be given dynamically to the models\nfor inference. In particular, we are evaluating the\nimpact of providing information about the game’s\nstate to the model. Prompts shown in Tables 1 and 4\ndemonstrate the case in which chat hchat and state\nhstate information are given. If only chat is used,\nlines indicating information about the <STATE>\nand current <PARTY> are omitted. Further, mes-\nsages from the 7-th “system” player are removed\nfrom the <CHAT> information, thus only retain-\ning the players’ utterances. Similarly, if only the\ngame’s state is desired, utterances from all players\nare removed from the <CHAT> information.\n4.3.1 Structured LLM Outputs\nWhile very powerful, LLMs struggle adhering to\nstructured outputs, as, for example, required in\nour role-prediction task. An LLM tasked to pro-\nduce a bullet-point list of player names followed\nby one of the roles results in varying answer quali-\nties ranging from half-populated lists to free-form\nresponses with no discernible list, making subse-\nquent utilization of the predictions difficult. How-\never, many LLMs exposed to code during training\n11197\n0\n250\n500\n750Quantity\nassertion\nlogical deduction\nsuggestion\ncritique/opposition\nquestioning\nappeal/defense\nagreement\ncompromise/concession\nFigure 3: Distribution of persuasion strategies.\ncan generate JSON strings and understand simple\ncode blocks. To capitalize on this feature, we uti-\nlize TypeChat (Microsoft, 2023), which, given a\nschema definition, tasks LLMs with answering all\nuser queries with a JSON string that conforms to\nthe provided schema. A benefit of this approach\nis that the LLM response can be verified to satisfy\nthe desired response schema by utilizing the Type-\nScript compiler. In cases where the response is\nincomplete or otherwise insufficient, TypeScript’s\ncompiler error message can be used to formulate\na follow-up query to the LLM to generate a valid\nresponse. We utilize this approach to generate role\npredictions for each player, as well as for predicting\nwhich player is Merlin. Our schema definitions can\nbe found in Section C.3 and enforce that a valid\nrole is predicted for each player. However, our\nschema does not enforce that; for example, the role\nof Merlin can only be assigned to a single player.\n5 Experiments\nThe following sections detail our data collection\nprocess and provide comparisons between three\ncurrent state-of-the-art LLMs, including fine-tuned\nversions of these models, in utilizing different\nmodalities and context representations.\n5.1 Data Collection\nWe created a browser-based version ofAvalon: The\nResistance that allows for unsupervised data col-\nlection from six participants. During the game,\nwe collect the conversations between the players\nand the state of the game throughout each session.\nFurther, we instructed participants to self-label per-\nsuasion strategies (see Section A.1.4 for details) for\neach of their chat messages, as well as indicating\nbeliefs about the other players’ roles throughout\nthe game. In total, we collected 20 games over 24\nhours of in-game time from a total of 30 unique par-\nticipants, forming 19 different teams of six players.\nParticipants were required to be at least 18 years\nold and familiar with the general rules of Avalon.\nThe Office of Research Integrity and Compliance\nsanctions the experiment and data collection.\nThe dataset released with this paper contains\nover 2300 utterances and corresponding hand-\nlabeled persuasion and deception strategies. Games\ntake an average of 1 hour and 12 minutes, with\nthe shortest game lasting only 18 minutes and the\nlongest game over 3 hours. Seven of the 20 games\nwere won by the good players, while 13 were won\nby evil – nine of which were won through the As-\nsassin correctly identifying Merlin.\nWe split the dataset into 14 training and six test-\ning games. The test games were selected such that\nthey contain three good victories and three evil\nvictories, where two of the three evil victories are\nthrough the assassin and one through successfully\nfailing three quests. To form a human baseline, we\ncreated Google Forms surveys and asked players\nnot involved in the test games to label the roles (see\nSection A.3) by reading over the recorded games\nfrom the perspective of an external observer.\n5.1.1 Data Processing\nDue to the chat-based nature of our data collection\nprocess, the collected utterances are of reasonable\nquality. However, we clean the data with an auto-\nmatic spell-checker that corrects any word with a\nLevenshtein Distance between an unknown word\nand an English target dictionary. Similarly, player\nnames are corrected with a custom dictionary con-\ntaining the correct spelling of the player’s names.\nAfter this procedure, the remaining unknown words\nare corrected manually, which mostly contain ab-\nbreviations of long or unusual player names. Af-\nter spelling and player names are corrected, the\ndata is anonymized by replacing player names with\n“player-X” where X is the index of the player in\neach particular game. Note that this means that in\ntwo different games, player-1 can refer to a differ-\nent person. This replacement was done to not bias\nthe models towards certain players having identifi-\nable play styles.\n5.1.2 Strategy Labels\nEvery utterance in our dataset is accompanied by\na label indicating one of our eight different per-\nsuasion strategies (see Figure 3 and Section A.1.4).\nFurther, if not truthful, utterances of evil players are\nlabeled with an additional deception strategy label,\ncovering the three common types of lies (Houston\net al., 2013): commission, omission, and influence.\nTable 6 in Section D.2 shows the prediction capa-\nbilities of GPT-4, GPT-3-Turbo, and Llama-2-13b\nof our eight prediction strategies, including fine-\n11198\nTable 2: F1 scores for role prediction and identifying Merlin given privileged knowledge. We report results using\nthe round-based (first value) and full context (second value). Models are evaluated ten times on the six test games.\nModalities All-Role Prediction Evil find Merlin\nModel Familiar Trained Chat State Good Evil Merlin Final Anytime\n1 Gpt-4 ✓ ✓ 0.67 / 0.67 0.48 / 0.55 0.36 / 0.20 0.17 / 0.17 0.83 / 0.67\n2 Gpt-4 ✓ ✓ 0.59 / 0.57 0.20 / 0.33 0.06 / 0.31 0.17 / 0.33 0.17 / 0.50\n3 Gpt-4 ✓ ✓ ✓ 0.67 / 0.68 0.46 / 0.58 0.05 / 0.27 0.00 / 0.00 0.67 / 0.50\n4 gpt-3.5-turbo ✓ ✓ 0.68 / 0.60 0.46 / 0.40 0.23 / 0.17 0.17 / 0.17 0.33 / 0.50\n5 gpt-3.5-turbo ✓ ✓ 0.57 / 0.47 0.46 / 0.30 0.00 / 0.32 0.17 / 0.17 0.17 / 0.17\n6 gpt-3.5-turbo ✓ ✓ ✓ 0.58 / 0.65 0.34 / 0.47 0.23 / 0.13 0.17 / 0.17 0.33 / 0.33\n7 gpt-3.5-turbo ✓ ✓ ✓ ✓ 0.52 / 0.59 0.38 / 0.41 0.19 / 0.15 0.17 / 0.17 1.00 / 0.67\n8 Llama-2 ✓ 0.68 / 0.61 0.39 / 0.27 0.00 / 0.00 0.17 / 0.00 0.17 / 0.17\n9 Llama-2 ✓ 0.41 / 0.62 0.00 / 0.34 0.00 / 0.00 0.00 / 0.00 0.00 / 0.17\n10 Llama-2 ✓ ✓ 0.61 / 0.55 0.33 / 0.22 0.00 / 0.00 0.17 / 0.00 0.17 / 0.33\n11 Llama-2 ✓ ✓ ✓ 0.65 / 0.63 0.35 / 0.26 0.23 / 0.27 0.33 / 0.00 0.33 / 0.00\n12 Random 0.50 0.34 0.17 0.17 0.60\n13 Human ✓ ✓ ✓ ✓ 0.76 0.72 0.33 0.5 0.67\ntuned versions of GPT-3.5 and Llama-2. We find\nthat fine-tuned GPT-3.5 performs well (micro-f1:\n0.43) when predicting the strategies for each label,\nfollowed by the vanilla version of GPT-4 (micro-\nf1: 0.37). However, Llama-2-13b underperforms\n(micro-f1: 0.15), even in the fine-tuned version\n(micro-f1: 0.20).\n5.2 Turn Order and Utterances\nFor the 20 games we conducted, roles have been\nrandomly assigned to each of the six players so as\nnot to bias the data towards certain advantageous\npositions of Merlin or evil players in the turn or-\nder. We found that identifying good players is\neasier if Merlin is among the first three players\n(p-value 0.039). Similarly, if Morgana or Percival\nspeaks a lot, good players are easier to identify\n(p-value 0.046 and 0.021, respectively). We also\nfound that evil players who uttered lies more fre-\nquently had a statistically significant advantage of\nidentifying Merlin correctly (p-value 0.015). We\nconjecture that Merlin’s attempts to counteract the\nlies of evil players has a high likelihood of reveal-\ning his identity to evil players. Similarly, we found\na statistically significant influence of the number\nof Percival’s utterances and the likelihood of evil\nwinning (p-value 0.018).\n5.3 Comparing Model Performance\nWe compare three state-of-the-art LLMs, namely\nGPT-4 (OpenAI, 2023), GPT-3.5-turbo, and Llama-\n2-13B (Touvron et al., 2023), as well as a fine-tuned\nversion of Llama-2-13B and GPT-3.5-turbo. Each\nmodel is used to predict the roles of all players from\nthe perspective of an external observer, but also on\nthe task of identifying Merlin, given the privileged\nknowledge e of who the evil players are. Each\nevaluation is conducted with three different sets\nof modalities, including the game’s state, player\nchat, and a combination of game state and player\nchat. Our inherent assumption is that these LLMs\npossess enough encoded knowledge in their pre-\ntrained model weights such that inference for our\ntask is possible. To assess the pre-trained model’s\ncapability, we evaluate each model’s familiarity\nwith the rules of Avalon. We run three prompts\nand list selected answers in Table 5. Responses\nto all three prompts are evaluated by a human for\ncorrectness and adherence to the game’s rules and\nare listed in Table 2.\nTable 2 demonstrates the performance of var-\nious LLM approaches. Due to the probabilistic\nnature of the LLMs, each model was evaluated ten\ntimes on each of the validation games. Human per-\nformance (line 13) was evaluated by asking three\nhuman annotators for each of the six evaluation\ngames to provide their estimates about the roles of\neach player in each game. Further, the performance\nof finding Merlin and identifying Merlin correctly\nat any point in the game was directly taken from\nthe game’s belief annotations, while the final pre-\ndiction was taken either from the assassin’s choice\n(if applicable) or the last anytime estimate. Table 2\npresents the F1-score when providing information\nabout the game in a round-based context with a\ncarried-over belief (first number) and when provid-\ning the entire context from the beginning of the\ngame (second number). In comparison, the round-\n11199\nGoodEvilMerlin\nActual Role\n0.93 0.07 0.00\n0.66 0.34 0.00\n0.55 0.30 0.15\nLlama-2-13B (FT)\n0.97 0.03 0.00\n0.66 0.34 0.00\n0.65 0.35 0.00\nLlama-13B\n0.52 0.29 0.19\n0.50 0.38 0.12\n0.43 0.38 0.19\nGPT-3.5-Turbo (FT)\n0.63 0.25 0.12\n0.56 0.35 0.10\n0.31 0.48 0.21\nGPT-3.5-Turbo\n0.82 0.09 0.10\n0.61 0.39 0.00\n0.59 0.38 0.04\nGPT-4\n0.81 0.09 0.10\n0.23 0.70 0.07\n0.46 0.25 0.29\nHuman (Full History)\nGood Evil Merlin\nPredicted Role\nGoodEvilMerlin\nActual Role\n0.97 0.01 0.02\n0.69 0.19 0.12\n0.62 0.05 0.33\nGood Evil Merlin\nPredicted Role\n0.95 0.05 0.01\n0.74 0.23 0.03\n0.78 0.22 0.00\nGood Evil Merlin\nPredicted Role\n0.59 0.26 0.14\n0.40 0.41 0.19\n0.47 0.39 0.15\nGood Evil Merlin\nPredicted Role\n0.71 0.20 0.09\n0.47 0.47 0.06\n0.45 0.46 0.09\nGood Evil Merlin\nPredicted Role\n0.75 0.09 0.16\n0.46 0.53 0.01\n0.49 0.28 0.23\nGood Evil Merlin\nPredicted Role\n0.81 0.09 0.10\n0.23 0.70 0.07\n0.46 0.25 0.29\nFigure 4: Confusion matrices evaluating our LLMs in the two context conditions using Chat+State: round-based\n(top, blue) and full-context (bottom, orange). Human results (right, purple) are for the full context in both cases.\nbased context utilizes an average of 974 (std: 333,\nmax: 1941) tokens, while the full context utilizes\nan average of 2844 (std: 2011, max: 8556) tokens.\nWhile most models and modalities outperform\nrandomly guessing player roles, only GPT-4 in the\nround-based context utilizing chat (line 1) outper-\nforms the human baseline (line 13) when identi-\nfying Merlin among the players. Comparing each\nmodel’s performance when identifying good and\nevil players, we notice a performance drop for evil\nplayers as they actively hide their identity to appear\nas good players. A similar trend can be observed in\nthe human baseline (line 13), however, humans are\nunmatched in their ability to identify them. Gener-\nally, this trend is also observed, to a greater extent,\nin the ability to identify Merlin.\nFor GPT-based models, when predicting good\nand evil roles, we notice that combining game chat\nand game state information results in the highest\nF1 scores (lines 3 and 6) when providing the full\ncontext, confirming our hypothesis that ground-\ning game chat in the game’s state improves perfor-\nmance. However, when utilizing the round-based\ncontext, individual modalities tend to outperform\ntheir full-context equivalents (lines 1, 2, 4, and\n5), yet do not reach the performance of the joint\nmodality. Further, we observe that predicting Mer-\nlin benefits from the availability of the game state\n(lines 2 and 5).\nIn the case of Llama-2, the best performance is\nachieved when only utilizing individual modalities\n(lines 8 and 9) while not allowing any conclusive\ninsight as to which modality tends to perform better,\ndespite significant differences (line 9 (good) and\nline 10 (evil)) in their respective performance).\nFigure 4 provides further insights into the role\npredictions, particularly the two different context\nmodes (round-based and full-context) when using\nthe joint modality. Particularly with round-base\ncontext, GPT-3.5-Turbo (Figure 4, top) suffers by\nmostly confusing evil players with good ones and\nidentifying Merlin as evil. On the other hand, pro-\nviding the full context (Figure 4, bottom), does not\nlead to such confusion about Merlin, where he is\nmostly confused with a good player. We conjec-\nture that identifying players’ underlying intentions,\nespecially when players actively try to hide their\nidentity, remains a challenging task for LLMs.\n5.3.1 LLM Fine-Tuning for Role Prediction\nIn addition to testing the performance of pre-trained\nmodels, we have fine-tuned GPT-3.5-Turboas well\nas Llama-2-13b. Training data for fine-tuning was\ngenerated from the remaining 14 games of Avalon\nthat were not used for evaluation. We have trained\neach model for three epochs across these 14 games\nand report their performance in Table 2 and Fig-\nure 4. Particularly in the case of Llama-2, we notice\nthat fine-tuning improves the model’s performance\nwhen predicting the roles of all players. For GPT-\n3.5, we do not observe the same trend; however,\nwhen predicting Merlin on its own, GPT-3.5 dra-\nmatically improves in performance, even outper-\nforming GPT-4 and the human baseline. When\ncomparing the failure cases in Figure 4, fine-tuning\nreduces failure severity, particularly for Merlin, lim-\niting failed predictions to good players, particularly\nin the case of Llama-2.\n5.4 Comparison to other Deception Tasks\nWe have also compared the prediction capabili-\nties of pre-trained models on existing datasets of\nsocial-deduction games, namely the game of Were-\n11200\nTable 3: F1 scores for role predictions in the game of\nWerewolf. Our results show enhanced role prediction\nscores compared to our Avalon data, suggesting a more\nchallenging social deduction environment in Avalon.\nRoles\nModel Tuned Good Evil Seer Gain\n1 GPT-4 0.66 0.66 0.49 +0.28\n2 GPT-3.5-Turbo 0.53 0.56 0.35 +0.19\n3 GPT-3.5-Turbo ✓ 0.61 0.72 0.43 +0.61\n4 Llama-2-13B 0.43 0.47 0.00 +0.13\n5 Llama-2-13B ✓ 0.45 0.39 0.08 -0.24\nwolf (Lai et al., 2022). Table 3 shows the results in\na similar setting, grouping the players into Good,\nEvil, and Seer by designating roles as Evil if ly-\ning and deception are a part of their strategy and\nkeeping the Seer as a separate role akin to Merlin\nin Avalon. Predictions for individual roles with the\nbest model, GPT-4, are available in Table 7.\nWe demonstrate that in the setting of Werewolf,\nLLMs perform well predicting the players’ roles\n(see “gains” in Table 3), which we hypothesize\nis due to the shorter horizon (Avg. 1786 tokens,\n2795 max) of each game as compared to ourAvalon\ndataset (Avg. 2844 tokens, 8556 max), making the\ntask of identifying roles easier. We find that fine-\ntuning dramatically improves the performance of\nGPT-3.5 over three epochs (line 3 vs. line 2) of\ntraining, while Llama-2-13B fine-tuning does not\nsignificantly affect performance (line 5 vs. line 4).\nThis observation is contrary to our Avalon dataset\n(see Table 2 lines 7 and 11), where Llama-2, in par-\nticular, improved its performance, while GPT-3.5\nroughly maintained its performance, resulting in a\nlarge difference in gains (+0.61 vs. -0.24). We hy-\npothesize that with the high-quality data provided\nin our Avalon dataset, fine-tuning Llama-2 allows\nit to achieve performance levels comparable to an\nuntuned GPT-3.5 (see Table 2). Yet, when faced\nwith the Werewolf dataset, a ’naturalistic-social-\nsetting’ characterized by frequent cross-talk and\noff-topic exchanges, Llama-2 struggles to fine-tune\neffectively. In contrast, the broad training founda-\ntion of GPT-3.5 enables it to learn even under such\nconditions.\n6 Conclusion\nIn this work, we present a novel benchmark, associ-\nated dataset, and testbed for long-horizon dialogue\nunderstanding in scenarios of conflicting interests\namong multiple participants. This task combines\nutterances from six human players at a time, hand-\nlabeled persuasion and deception strategies, player\nbeliefs, and comprehensive game states recorded\nover more than 24 hours of gameplay. We demon-\nstrate that current state-of-the-art LLMs do not\nreach human-level performance in environments\nthat require the understanding and tracking of long-\nhorizon dialogue between multiple participants in\nchallenging social cooperative-competitive settings.\nCompared to similar datasets, our benchmark con-\ntains longer context horizons, stricter game rules,\nand high-quality dialogue, making it well-suited for\nNLU research as all game-relevant communication\nhas been captured and is thus, available to learn-\ning algorithms. This dataset opens doors for di-\nverse research avenues, from detecting deception to\ndeveloping conversational agents for cooperative-\ncompetitive settings. We hope that our high-quality\ndataset will be valuable to further tune and eval-\nuate the capabilities of large language models in\nchallenging social settings.\nLimitations\nThe performance of our evaluated models depends\non the capabilities of the pre-trained language mod-\nels. Given our cooperative-competitive multi-party\nscenario, we observe that current approaches do\nnot reach human-level performance. We also be-\nlieve that the fine-tuning results could be further\nimproved by extending our dataset; however, we\nbelieve that future work will utilize it successfully.\nEthics Statement\nParticipants in our IRB approved human-subject\nstudy were provided consent forms and privacy no-\ntices explaining how we intend to use the data col-\nlected during the experiments. We strictly adhere to\napplicable data protection policies and use the data\nsolely for research purposes. Further, we offered\nopportunities for participants to seek clarification\nand ask questions, fostering informed consent and\nethical, trustworthy interactions.\nAcknowledgements\nThis work has been funded in part by the Air\nForce Office of Scientific Research (AFOSR) un-\nder grants FA9550-18-1-0251 and FA9550-18-1-\n0097, the Army Research Laboratory (ARL) un-\nder grant W911NF-19-2-0146, DARPA award\nHR001120C0036, and the Office of Naval Re-\nsearch (ONR) award N00014-23-1-2840.\n11201\nReferences\nJoshua Ainslie, Tao Lei, Michiel de Jong, Santiago\nOntañón, Siddhartha Brahma, Yury Zemlyanskiy,\nDavid Uthus, Mandy Guo, James Lee-Thorp, Yi Tay,\net al. 2023. Colt5: Faster long-range transform-\ners with conditional computation. arXiv preprint\narXiv:2303.09752.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,\nAntoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm\nReynolds, et al. 2022. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural\nInformation Processing Systems, 35:23716–23736.\nTim Althoff, Cristian Danescu-Niculescu-Mizil, and\nDan Jurafsky. 2014. How to ask for a favor: A case\nstudy on the success of altruistic requests. In Pro-\nceedings of the International AAAI Conference on\nWeb and Social Media, volume 8, pages 12–21.\nAnton Bakhtin, David J Wu, Adam Lerer, Jonathan\nGray, Athul Paul Jacob, Gabriele Farina, Alexan-\nder H Miller, and Noam Brown. 2022. Mastering the\ngame of no-press diplomacy via human-regularized\nreinforcement learning and planning.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and\nChristopher Ré. 2022. Flashattention: Fast and\nmemory-efficient exact attention with io-awareness.\nAdvances in Neural Information Processing Systems,\n35:16344–16359.\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,\nAakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, et al.\n2023. Palm-e: An embodied multimodal language\nmodel. arXiv preprint arXiv:2303.03378.\nMeta Fundamental AI Research Diplomacy Team FAIR,\nAnton Bakhtin, Noam Brown, Emily Dinan, Gabriele\nFarina, Colin Flaherty, Daniel Fried, Andrew Goff,\nJonathan Gray, Hengyuan Hu, et al. 2022. Human-\nlevel play in the game of diplomacy by combining\nlanguage models with strategic reasoning. Science,\n378(6624):1067–1074.\nSuriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio\nCésar Teodoro Mendes, Allie Del Giorno, Sivakanth\nGopi, Mojan Javaheripi, Piero Kauffmann, Gustavo\nde Rosa, Olli Saarikivi, Adil Salim, Shital Shah,\nHarkirat Singh Behl, Xin Wang, Sébastien Bubeck,\nRonen Eldan, Adam Tauman Kalai, Yin Tat Lee, and\nYuanzhi Li. 2023. Textbooks are all you need.\nJulia Bell Hirschberg, Stefan Benus, Jason M Brenier,\nFrank Enos, Sarah Friedman, Sarah Gilman, Cynthia\nGirand, Martin Graciarena, Andreas Kathol, Laura\nMichaelis, et al. 2005. Distinguishing deceptive from\nnon-deceptive speech.\nPhilip Houston, Michael Floyd, Susan Carnicero, and\nDon Tennant. 2013. Spy the lie. Saint Martin’s Grif-\nfin, New York, NY .\nSamee Ibraheem, Gaoyue Zhou, and John DeNero.\n2022. Putting the con in context: Identifying de-\nceptive actors in the game of mafia. arXiv preprint\narXiv:2207.02253.\nJinhao Jiang, Kun Zhou, Zican Dong, Keming Ye,\nWayne Xin Zhao, and Ji-Rong Wen. 2023. Struct-\ngpt: A general framework for large language model\nto reason over structured data. arXiv preprint\narXiv:2305.09645.\nSimon Keizer, Markus Guhe, Heriberto Cuayáhuitl,\nIoannis Efstathiou, Klaus-Peter Engelbrecht, Mi-\nhai Dobre, Alex Lascarides, Oliver Lemon, et al.\n2017. Evaluating persuasion strategies and deep rein-\nforcement learning methods for negotiation dialogue\nagents. ACL.\nBolin Lai, Hongxin Zhang, Miao Liu, Aryan Pariani,\nFiona Ryan, Wenqi Jia, Shirley Anugrah Hayati,\nJames M Rehg, and Diyi Yang. 2022. Werewolf\namong us: A multimodal dataset for modeling per-\nsuasion behaviors in social deduction games. arXiv\npreprint arXiv:2212.08279.\nChia-Hsuan Lee, Hao Cheng, and Mari Ostendorf. 2021.\nDialogue state tracking with a language model using\nschema-driven prompting.\nMike Lewis, Denis Yarats, Yann N. Dauphin, Devi\nParikh, and Dhruv Batra. 2017. Deal or no deal?\nend-to-end learning for negotiation dialogues.\nHuao Li, Yu Quan Chong, Simon Stepputtis, Joseph\nCampbell, Dana Hughes, Michael Lewis, and Katia\nSycara. 2023. Theory of mind for multi-agent col-\nlaboration via large language models. In The 2023\nConference on Empirical Methods in Natural Lan-\nguage Processing.\nYiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang,\nYuanyuan Yang, Jiaming Tian, Hao He, Antong Li,\nMengshen He, Zhengliang Liu, et al. 2023. Summary\nof chatgpt/gpt-4 research and perspective towards\nthe future of large language models. arXiv preprint\narXiv:2304.01852.\nShayne Longpre, Gregory Yauney, Emily Reif, Kather-\nine Lee, Adam Roberts, Barret Zoph, Denny Zhou,\nJason Wei, Kevin Robinson, David Mimno, et al.\n2023. A pretrainer’s guide to training data: Measur-\ning the effects of data age, domain coverage, quality,\n& toxicity. arXiv preprint arXiv:2305.13169.\nRyan Lowe, Nissan Pow, Iulian Serban, and Joelle\nPineau. 2015. The ubuntu dialogue corpus: A large\ndataset for research in unstructured multi-turn dia-\nlogue systems. arXiv preprint arXiv:1506.08909.\n11202\nAndrea Madotto, Zihan Liu, Zhaojiang Lin, and Pascale\nFung. 2020. Language models as few-shot learner\nfor task-oriented dialogue systems. arXiv preprint\narXiv:2008.06239.\nMicrosoft. 2023. TypeChat.\nIni Oguntola, Joseph Campbell, Simon Stepputtis, and\nKatia Sycara. 2023. Theory of mind as intrinsic\nmotivation for multi-agent reinforcement learning.\nOpenAI. Introducing chatgpt.\nOpenAI. 2023. Gpt-4 technical report.\nCharles Packer, Vivian Fang, Shishir G. Patil, Kevin\nLin, Sarah Wooders, and Joseph E. Gonzalez. 2023.\nMemgpt: Towards llms as operating systems.\nWei Pang and Xiaojie Wang. 2020. Visual dialogue state\ntracking for question generation. In Proceedings\nof the AAAI Conference on Artificial Intelligence ,\nvolume 34, pages 11831–11838.\nOfir Press, Noah A Smith, and Mike Lewis. 2021.\nTrain short, test long: Attention with linear biases\nenables input length extrapolation. arXiv preprint\narXiv:2108.12409.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nLiliang Ren, Kaige Xie, Lu Chen, and Kai Yu. 2018.\nTowards universal dialogue state tracking. arXiv\npreprint arXiv:1810.09587.\nJack Serrino, Max Kleiman-Weiner, David C Parkes,\nand Josh Tenenbaum. 2019. Finding friend and foe in\nmulti-agent games. Advances in Neural Information\nProcessing Systems, 32.\nKai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, and\nHuan Liu. 2017. Fake news detection on social me-\ndia: A data mining perspective. ACM SIGKDD ex-\nplorations newsletter, 19(1):22–36.\nHo Chit Siu, Jaime Peña, Edenna Chen, Yutai Zhou,\nVictor Lopez, Kyle Palko, Kimberlee Chang, and\nRoss Allen. 2021. Evaluation of human-ai teams for\nlearned and rule-based agents in hanabi. Advances in\nNeural Information Processing Systems, 34:16183–\n16195.\nFelix Soldner, Verónica Pérez-Rosas, and Rada Mihal-\ncea. 2019. Box of lies: Multimodal deception de-\ntection in dialogues. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 1768–1777.\nSimon Stepputtis, Joseph Campbell, Mariano Phielipp,\nStefan Lee, Chitta Baral, and Heni Ben Amor. 2020.\nLanguage-conditioned imitation learning for robot\nmanipulation tasks. In Advances in Neural Informa-\ntion Processing Systems, volume 33, pages 13139–\n13150. Curran Associates, Inc.\nChenhao Tan, Vlad Niculae, Cristian Danescu-\nNiculescu-Mizil, and Lillian Lee. 2016. Winning ar-\nguments: Interaction dynamics and persuasion strate-\ngies in good-faith online discussions. In Proceedings\nof the 25th international conference on world wide\nweb, pages 613–624.\nDeepMind Interactive Agents Team, Josh Abramson,\nArun Ahuja, Arthur Brussee, Federico Carnevale,\nMary Cassin, Felix Fischer, Petko Georgiev,\nAlex Goldin, Mansi Gupta, et al. 2021. Cre-\nating multimodal interactive agents with imita-\ntion and self-supervised learning. arXiv preprint\narXiv:2112.03763.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki,\nMichaël Mathieu, Andrew Dudzik, Junyoung Chung,\nDavid H Choi, Richard Powell, Timo Ewalds, Petko\nGeorgiev, et al. 2019. Grandmaster level in starcraft\nii using multi-agent reinforcement learning. Nature,\n575(7782):350–354.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-\ndlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and An-\nima Anandkumar. 2023a. V oyager: An open-ended\nembodied agent with large language models. arXiv\npreprint arXiv: Arxiv-2305.16291.\nShenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi,\nShuo Chen, Qisen Yang, Andrew Zhao, Chaofei\nWang, Shiji Song, and Gao Huang. 2023b. Avalon’s\ngame of thoughts: Battle against deception through\nrecursive contemplation.\nXuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh,\nSijia Yang, Jingwen Zhang, and Zhou Yu. 2019. Per-\nsuasion for good: Towards a personalized persua-\nsive dialogue system for social good. arXiv preprint\narXiv:1906.06725.\nHenry Weld, Xiaoqi Huang, Siqu Long, Josiah Poon,\nand Soyeon Caren Han. 2022. A survey of joint intent\ndetection and slot filling models in natural language\nunderstanding. ACM Computing Surveys, 55(8):1–\n38.\nJules White, Quchen Fu, Sam Hays, Michael Sandborn,\nCarlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse\nSpencer-Smith, and Douglas C. Schmidt. 2023. A\nprompt pattern catalog to enhance prompt engineer-\ning with chatgpt.\n11203\nYaqi Xie, Chen Yu, Tongyao Zhu, Jinbin Bai, Ze Gong,\nand Harold Soh. 2023. Translating natural language\nto planning goals with large-language models. CoRR,\nabs/2302.05128.\nDiyi Yang, Jiaao Chen, Zichao Yang, Dan Jurafsky, and\nEduard Hovy. 2019. Let’s make your request more\npersuasive: Modeling persuasive strategies via semi-\nsupervised neural nets on crowdfunding platforms.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 3620–3630.\nSayyed M Zahiri and Jinho D Choi. 2017. Emo-\ntion detection on tv show transcripts with sequence-\nbased convolutional neural networks. arXiv preprint\narXiv:1708.04299.\nXijia Zhang, Yue Guo, Simon Stepputtis, Katia Sycara,\nand Joseph Campbell. 2023. Explaining agent behav-\nior with large language models.\nTiancheng Zhao and Maxine Eskenazi. 2016. Towards\nend-to-end learning for dialog state tracking and man-\nagement using deep reinforcement learning. arXiv\npreprint arXiv:1606.02560.\n11204\nA Player Instructions\nThe following sections provide further detail about\nthe rules of Avalon, as well as the hidden knowl-\nedge imbued to every special role in the game.\nA.1 In-Game Instructions\nA.1.1 How to Play\nAvalon: The Resistance is the game of hidden loy-\nalty. Players are either Loyal Servants of Arthur\nfighting for Goodness and honor or aligned with\nthe Evil ways of Mordred. Good wins the game by\nsuccessfully completing three Quests. Evil wins if\nthree Quests end in failure. Evil can also win by as-\nsassinating Merlin at game’s end or if a Quest can-\nnot be undertaken. Players may make any claims\nduring their turns. Discussion, deception, accusa-\ntion, and logical deduction are all equally important\nin order for Good to prevail or Evil to rule the day.\nBefore playing a game, please check out the official\nrules. The differences in this version of Avalon in\ncomparison to these rules will be explained below.\nA.1.2 Roles\nIn this version of Avalon, you will play with five\nfixed roles:\nMerlin Merlin is on Good’s side and knows the\ntwo evils’ identity. The portrait frames of the evil\npeople will have a red circle around them.\nPercival Percival is on the side of Good and gets\ninformation about two players. These two players\nare Morgana and Merlin; however, Percival does\nnot know who is who. These two players will have\na red-and-blue circle around them.\nServants Servants are on the side of Good but do\nnot have any special knowledge or ability.\nMorgana Morgana plays on the side of Evil and\nknows who the other evil player is, indicated by\na red circle around their profile picture. Morgana\nappears as a potential Merlin to Percival.\nAssassin Assassin plays on the side of Evil and\nknows who the other evil player is, indicated by a\nred circle around their profile picture. At the end\nof the game, should Good win, the Assassin can\nwin the game for Evil by correctly identifying who\nMerlin is.\nA.1.3 Communication\nFor the purposes of this research study, communica-\ntion between the players will be conducted through\nturns. During each player’s turn, they will be able\nto communicate through the chat window, respond-\ning to previous questions or accusations, making\nnew statements, and asking questions to other play-\ners. However, only the player whose turn it is will\nbe able to use the chat. Please only communicate\nwith the players within the interface provided in\nthis version of the game.\nAfter you send a message to the chat, you can in-\ndicate the strategy/motivation you used when writ-\ning the message. While not mandatory, it would be\ngreat if players would provide these insights. The\ninformation provided in the strategy selection will\nnot be shared with other players, so don’t worry\nabout saying that you potentially lied about some-\nthing!\nA.1.4 Communication Labels\nThese are the labels and explanations available for\nyour communication\nAssertion his includes statements where the\nspeaker makes an assertive remark, expresses a\nfirm belief or makes a definitive statement.\nQuestioning This includes any questioning of\nother players regarding their behavior.\nSuggestion This includes all instances where the\nspeaker suggests an action, makes a proposal, or\ngives advice to another player.\nAgreement This encompasses sentences where\nthe speaker is agreeing with another player’s state-\nment or strategy, affirming their own identity or\nsomeone else’s.\nLogical Deduction This includes statements\nwhere the speaker provides a reasoned explana-\ntion, defends a point of view, elaborates a strategy\nor justifies their actions.\nCompromise/Concession This category in-\ncludes any sentences where the speaker concedes\nto another’s point of view, expresses indecision,\nor appears to back down from a previously held\nstance.\nCritique/Opposition This includes instances\nwhere the speaker critiques another’s view, coun-\nters an argument, or points out an inconsistency or\nflaw in another’s reasoning.\nAppeal/Defense This category covers situations\nwhere the speaker appeals to others for trust, to be\n11205\nincluded in the quest, or uses emotional/personal\nappeal to gain favor.\nA.1.5 Turns\nThe player with the crown is the current quest\nleader, while the jester’s hat indicates whose turn\nit currently is. Each turn is limited to 100 seconds,\nafter which the game automatically transitions to\nthe next player. Similarly, if a vote is necessary,\nyou will have 30 seconds to cast your vote. After\nthat, parties/quests will be approved automatically\nfor players that didn’t vote.\nThe quest leader has to allow one round of dis-\ncussion prior to being able to initiate a party vote.\nHowever, party proposals can be changed whenever\nit is the turn of the leader.\nA.1.6 Parties\nParties are indicated by a little shield icon next to a\nplayer’s profile. If there is a shield, players are part\nof the party.\nA.1.7 Selecting Players\nPlayers can be selected for a party or by the Assas-\nsin to indicate who they think is Merlin by clicking\non their player profile picture frame. However,\nremember to confirm your choices. Clicking on\nplayer profile frames alone will only count as a\nchoice if the choice is explicitly confirmed.\nA.2 Differences to Standard Avalon\nTo run the games effectively in an online fashion,\nwe incorporate a few additional rules that are out-\nlined in this section. Particularly, discussions are\nconducted in a turn-based manner, enforcing that\nonly one player can talk at a time. Conversations\nare captured through a chat-based interface, en-\nsuring that all game-related player interactions are\ncaptured precisely. A player’s turn ends when ei-\nther they choose to end their turn via an “End my\nTurn” button or when their turn-time of 200 sec-\nonds runs out. In either case, the next player will\nstart their turn. However, if the turn time runs out\nfor the current quest leader, instead of transitioning\nto the next player, a random party is proposed, or\na vote for the currently proposed party is initiated,\ndepending on the current game state. In addition to\nending their turn, quest leaders have the choice to\ninitiate another round of discussion or to initiate a\nvote for the currently proposed party. Similar to the\nturn-time, votes (either for a party or for quest suc-\ncess) have to be made within 30 seconds. Should\nthe time run out prior to a decision, parties, and\nquests will automatically be approved or succeeded.\nLastly, if the outcome of the game requires it, the\nassassin has 200 seconds to evaluate the game’s\nchat history and decide on who they think is Mer-\nlin, and conduct an assassination. Failure to choose\na player as potential Merlin will automatically fail\nthe vote, proceeding to a victory for the good play-\ners.\nA.3 Human Baseline Labeling Instructions\nOn the following page, you will get see the log\nof a game of Avalon, including a description of\nthe current game state in the form of the \"system\"\nmessages. Please provide your role estimates for\neach player.\n• After each round of discussion, you will have\nthe opportunity to indicate your beliefs about\nthe role of each player.\n• Take your time to carefully analyze the avail-\nable information and make your best judg-\nments about each player’s role.\n• Avoid accessing or considering information\nfrom future rounds or pages when making\nyour decisions. Focus only on the information\navailable up to the current round.\n• Once you have made a belief label for a player\nin a particular round, refrain from changing\nthat label later, even if you change your mind\nabout it in subsequent rounds.\nB Surveys\nThe following section introduces the surveys con-\nducted with our human subjects to collect the base-\nline results.\nB.1 Example of Human Baseline Survey\nThis is an example of the human baseline question-\nair\nB.1.1 Example for Round 1\nRound 1\n• system: Game Started!\n• system: player-2 proposed a party: player-\n2, player-3\n• player-2: This is the first round, we don’t have\na lot of information so I\n11206\n• propose myself (because I am good) and the\nnext person player-3.\n• player-3: I agree with player-2. I am good\ntoo.\n• player-4: I think I am fine with the team as\nwell\n• player-5: i am good with that\n• player-6: No opinions in the first turn. I’m\nfine with this party\n• player-1: Oh, yeah, I am good with this. We\ndon’t know anything at this point anyways\n• system: Party Vote Outcome: player-1: Yes,\nplayer-2: Yes, player-3: Yes, player-4: Yes,\nplayer-5: Yes, player-6: Yes\n• system: Vote Succeeded! Initiating Quest\nVote!\n• system: Quest Succeeded!\nQuest results after quest 1: Success\n——————\nWhich role do you think the players have? Please\nonly consider the information you have up to this\npoint in the game.\n• player-1: <select one of [“good”, “evil”, “mer-\nlin”, “I don’t know”]\n• player-2: <select one of [“good”, “evil”, “mer-\nlin”, “I don’t know”]\n• player-3: <select one of [“good”, “evil”, “mer-\nlin”, “I don’t know”]\n• player-4: <select one of [“good”, “evil”, “mer-\nlin”, “I don’t know”]\n• player-5: <select one of [“good”, “evil”, “mer-\nlin”, “I don’t know”]\n• player-6: <select one of [“good”, “evil”, “mer-\nlin”, “I don’t know”]\nC Further Prompts\nIn this section, we provide additional insights into\nhow we query the LLM.\nC.1 Identifying Merlin\nThe following prompt was utilized to query the\nmodel about the identity of Merlin\nsystem: You are a helpful assistant that uses the chat between six players,\nplayer-1 to player-6, who play Avalon: The Resistance (a cooperative-\ncompetitive game) to identify who is Merlin. There are three good players,\nMerlin, who is also a good player, and two evil players. Evil can usually be\nfound because they are deceptive and lie about the good player’s roles and\nvote for parties and quests irrationally. For Merlin, watch out for individuals\nwith knowledge of evil players’ identities, insightful comments beyond their\nrole, and caution regarding mission teams or specific players.\nhuman: The current state is: <STATE> (See Section 4.2.1)\nThe current party proposal is: <PARTY> (See Section 4.2.1)\nYour initial belief is: <BELIEF> (See Section 4.1, only valid in round-\nbased context)\nYou know that <EVIL> are evil.\nThis is the chat between player-1 to player-6:\n<CHAT> (See Section 4.2.2)\nWho do you think is Merlin? Please do not explain your answer, do not\nelaborate on it further.\nassistant:\nTable 4: Prompt used to infer who Merlin is.\nhuman: Can you explain the rules of Avalon: The Resistance to me?\nAssistant (gpt-3.5-turbo): Avalon: The Resistance is a board game that\ninvolves hidden roles, deduction, and deception. The game is set in the\nworld of King Arthur, and players take on the roles of either loyal servants\nof Arthur (the \"good\" faction) or the minions of the evil Mordred (the \"bad\"\nfaction). The goal of the game is different for each faction. [...]\nhuman: In Avalon: The Resistance, can you tell me about the role of\nMerlin?\nassistant (Llama-2-13B): [...] Merlin is a character with special knowledge\nwho knows the true identities of the minions of Mordred, the traitors in the\ngame. Here are the key points to understand about the role of Merlin: [...]\nhuman: In Avalon: The Resistance, what is a good strategy for Morgana\nwhen playing with Percival and Merlin?\nassistant (gpt-4): A good strategy for Morgana when playing with Percival\nand Merlin in Avalon: The Resistance is to focus on confusing and deceiving\nboth Percival and Merlin to turn the tide in favor of the evil team. Here are\nsome tips to execute this strategy: [...]\nTable 5: Prompts and sample answers to determine a\nmodel’s familiarity with Avalon: The Resistance.\nC.2 Familiarity with Avalon\nThe questions in Table 5 are used to identify\nwhether or not a particular pre-trained LLM is\naware of Avalon: The Resistance. Sample answers\nindicate that all three tested models (GPT-4, GPT-\n3.5, Llama-2-13B) are familiar with Avalon.\nC.3 TypeScript Schemas\nThe following schema is used for role predictions:\n// Define the AvalonRoles interface\nexport interface AvalonRoles {\nplayer_1: \"good\" | \"evil\" | \"merlin\";\nplayer_2: \"good\" | \"evil\" | \"merlin\";\nplayer_3: \"good\" | \"evil\" | \"merlin\";\nplayer_4: \"good\" | \"evil\" | \"merlin\";\nplayer_5: \"good\" | \"evil\" | \"merlin\";\nplayer_6: \"good\" | \"evil\" | \"merlin\";\n}\nThe following schema is utilized to predict the\nplayer who is playing as Merlin:\n// Define the MerlinPlayer interface\nexport interface MerlinPlayer {\n11207\nTable 6: F1 scores for role predictions in the game of Werewolf. Our results show enhanced role prediction scores\ncompared to our Avalon data, suggesting a more challenging social deduction environment in Avalon.\nModel\nStrategy GPT-4 GPT-3.5 GPT-3.5 (FT) Llama-2-13b Llama-2-13b\n1 Assertion 0.27 0.26 0.26 0.25 0.26\n2 Questioning 0.40 0.33 0.39 0.16 0.00\n3 Suggestion 0.57 0.11 0.56 0.12 0.00\n4 Agreement 0.54 0.40 0.55 0.24 0.00\n5 Logical Deduction 0.36 0.41 0.42 0.00 0.26\n6 Compromise/Concession 0.17 0.40 0.23 0.18 0.00\n7 Critique/Opposition 0.36 0.43 0.43 0.07 0.33\n8 Appeal/Defense 0.12 0.00 0.42 0.00 0.00\n9 Overall (micro-f1): 0.37 0.32 0.43 0.15 0.20\nTable 7: F1 scores for Werewolf role predictions across test splits using the optimal model (GPT-4) from Table 3.\nModel Drunk Insomniac Minion Hunter Revealer Villager Werewolf Troublemaker Robber Tanner Seer Mason\n1 GPT-4 0.42 0.70 0.04 0.43 1.00 0.85 0.41 0.71 0.39 0.30 0.55 0.00\nmerlin: \"player_1\" | \"player_2\" |\n\"player_3\" | \"player_4\" |\n\"player_5\" | \"player_6\";\n}\nD Additional Results\nD.1 Werewolf Individual Roles\nIn Table 7, we demonstrate that many key roles in\nWerewolf are predicted with high F1 scores. How-\never, Mason and Minion have low scores as they\nonly have one and four representations in the test\nset of [2]. For Tanner and Robber, we observe a\ncommon confusion with the Werewolf role (and\nvice versa). We attribute this to the Tanner’s goal\nof looking like a werewolf and being eliminated for\ntheir unique win condition, while the Robber (at the\nend of the game) may have played like a werewolf\nwithout being aware of the fraction change.\nD.2 Persuasion Strategy Prediction\nTable 6 reports the performance of our model when\npredicting the persuasion strategies for all utter-\nances across our six test games. In each case, mod-\nels have been run once for each utterance, while\nthe fine-tuned models have been trained for three\nepochs on the 14 training games. We observe that\na fine-tuned GPT-3.5 model performs best, even\noutperforming a vanilla GPT-4 model, while Llama-\n2-13B, even in the fine-tuned case, underperforms.\n11208"
}