{
  "title": "A deep learning based approach for automated plant disease classification using vision transformer",
  "url": "https://openalex.org/W4284670866",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221308799",
      "name": "Yasamin Borhani",
      "affiliations": [
        "K.N.Toosi University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3020463041",
      "name": "Javad Khoramdel",
      "affiliations": [
        "Tarbiat Modares University"
      ]
    },
    {
      "id": "https://openalex.org/A2158450972",
      "name": "Esmaeil Najafi",
      "affiliations": [
        "K.N.Toosi University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4221308799",
      "name": "Yasamin Borhani",
      "affiliations": [
        "K.N.Toosi University of Technology",
        "Robotics Research (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3020463041",
      "name": "Javad Khoramdel",
      "affiliations": [
        "Tarbiat Modares University"
      ]
    },
    {
      "id": "https://openalex.org/A2158450972",
      "name": "Esmaeil Najafi",
      "affiliations": [
        "Robotics Research (United States)",
        "K.N.Toosi University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2982083293",
    "https://openalex.org/W6603394118",
    "https://openalex.org/W3084116196",
    "https://openalex.org/W2910363199",
    "https://openalex.org/W2736026939",
    "https://openalex.org/W2473156356",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W2904583950",
    "https://openalex.org/W2884416373",
    "https://openalex.org/W2758893285",
    "https://openalex.org/W4210247145",
    "https://openalex.org/W4210839135",
    "https://openalex.org/W3212101387",
    "https://openalex.org/W2147800946",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W1977295328",
    "https://openalex.org/W3035160371",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W6828894009"
  ],
  "abstract": null,
  "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2022) 12:11554  | https://doi.org/10.1038/s41598-022-15163-0\nwww.nature.com/scientificreports\nA deep learning based approach \nfor automated plant disease \nclassification using vision \ntransformer\nYasamin Borhani1, Javad Khoramdel2 & Esmaeil Najafi1*\nPlant disease can diminish a considerable portion of the agricultural products on each farm. The \nmain goal of this work is to provide visual information for the farmers to enable them to take the \nnecessary preventive measures. A lightweight deep learning approach is proposed based on the \nVision Transformer (ViT) for real-time automated plant disease classification. In addition to the ViT, \nthe classical convolutional neural network (CNN) methods and the combination of CNN and ViT have \nbeen implemented for the plant disease classification. The models have been trained and evaluated \non multiple datasets. Based on the comparison between the obtained results, it is concluded that \nalthough attention blocks increase the accuracy, they decelerate the prediction. Combining attention \nblocks with CNN blocks can compensate for the speed.\nEarly diagnosis of plant disease is essential for producing healthy products. As humans use the different kinds of \nplants to provide food, it is necessary to reduce the harmful effects of plants disease. In some cases, farmers can \nnot diagnosis the symptoms of plant illness accurately because of the tiny features. Moreover, a number of farm-\ners are not expert in diagnosing the disease, so that the artificial intelligence can help them to provide a better \nrecognition. In the past years, convolutional neural networks have been chosen for image processing as they have \nprogressed in these fields. Recently the Vision Transformer (ViT) structure has been introduced to improve the \nclassification applications. The idea is based on how humans classify images. When a human observes a picture, \nhe or she focuses on a specific area of the image to detect the instance of interest. The ViT structure follows this \napproach for image classification.\nIt is prevalent to use pre-designed architectures like ResNet, ViT, etc., for computer vision tasks. These models \nusually have lots of trainable parameters, requiring a large dataset to find the optimal values for their parameters. \nHence, these models are usually trained on a massive dataset like ImageNet; then, the obtained weights are used \nfor transfer learning. High accuracy will be acquired when the second dataset has a similar domain to the first \ndataset. Still, when the domain is different, the models would struggle to reach an acceptable accuracy on small \ndatasets. In addition to that, the pre-designed architectures are usually heavy models with lots of computations \nwhich slowdowns the prediction. For real-time applications, the prediction needs to be as fast as possible. A ques-\ntion may arise, and an argument can be made whether it is always an excellent choice to go for transfer learning \nor whether light-weighted hand-designed architectures can achieve adequate performance?\nRelated works. In the literature, studies have been conducted on diagnosing the diseases of a particular \nplant. Most of these datasets contains images that are captured in the acceptable circumstances since in the \nagriculture purposes the images are usually recorded in a good conditions. For example, for diagnosing wheat \nrust, aerial and non-aerial images of wheat farms were collected  in1. The images were labeled for object detec-\ntion. They proposed to localize the wheat leaf with object detection networks. After finding the corresponding \nbounding box, the cropped box is the input of a classification network to obtain the true class of that instance. \nOverall, five CNN models were trained for classification including  VGG162, ResNet-503,  Inception4, MobileNet-\nV35 and EfficientNet-B06. It was concluded that the EfficientNet-B0 has the highest accuracy while it has a lower \ncomputational cost.\nOPEN\n1Center of Excellence in Robotics and Control, Advanced Robotics & Automated Systems (ARAS), Faculty of \nMechanical Engineering, K. N. Toosi University of Technology, Tehran, Iran. 2Faculty of Mechanical Engineering, \nTarbiat Modares University, Tehran, Iran. *email: najafi.e@kntu.ac.ir\n2\nVol:.(1234567890)Scientific Reports |        (2022) 12:11554  | https://doi.org/10.1038/s41598-022-15163-0\nwww.nature.com/scientificreports/\nSince there are many structures for machine learning algorithms, different kinds of models have been used like \nthe C-DenseNet structure  in7. Their dataset consists of 5 different categories, and it was considered as a balanced \ndataset because of the distribution of images in each class. Also, the level of the disease intensity is divided into \nsix classes. In order to preprocess the images, all of them were cropped. Because of that, the background of the \npictures was not complicated. As the image features in some levels have slight differences, the C-DenseNet was \nused in order to improve the results. In this structure, the convolutional layers were used. Between these convo-\nlutional layers, the dense layers were implemented. In addition to that, Convolutional Block Attention Modules \n(CBAM) were used which consist of the channel and spatial attention modules. The channel attention module \nuses fully connected layers to extract features from the feature map, then uses sigmoid to find which part of the \ninput channel needs more attention. The outputs of these two modules are multiplied by the input feature map \nand the output of the CBAM is obtained. As the attention mechanism idea is used to build the model structure, \nthe key features of images are detected more accurately. The accuracy of their study is about 97.99%.\nIn addition to the previous studies, the matrix-based convolutional neural network M-bCNN  structure8 \nwas implemented. Because of the slight and tiny wheat leaf disease features, the matrix-based architecture was \nproposed in order to signify the important properties of images. The function of this proposed architecture sup-\npress the  AlexNet9 and VGG-162 networks.\nAnother example of focusing on the disease of a specific plant can be found  in10, where a few pictures of \ninfected rice leaves were collected. The images were taken on a farm in India. To achieve a rice disease clas-\nsifier, they first preprocessed images and removed the background from the complicated images. After that, a \nfew simple image segmentation techniques like K-means clustering and Ostu’s method were applied to find the \ninfected area on the leaves. The segmented images were used as the input to an SVM classifier to find the proper \nlabel for the  image10.\nSome datasets include multiple plants and their diseases, like the Plant Village  dataset11. The original images \nare colored images (RGB). The grayscale version and segmented version of these images are also available.All the \nexperiments  in12 had been done on each of these versions separately. Several trainings with different configura-\ntions had been reported.  AlexNet9 and  GoogLeNet13 were used as the based models. Each version of dataset had \nbeen used for training different train-test splits ( (Train 20% , Test 80%), (Train 40%, Test 60%), (Train 60%, \nTest 40 %), (Train 80 %, Test %20) ). Each training was conducted once with the initial random weights (train-\ning from scratch) once with the transfer learning by using the weights of AlexNet and GoogLeNet, which were \ntrained on the ImageNet; as the initial weights. As was expected, the models with transfer learning gained better \nperformance as compared to the model trained from scratch.\nIn some papers, the main focus has been on specific class or classes from the Plant Village. For example, \n in14, the authors only focused on the tomato leaf images from the Plant Village and implemented a simple CNN \nfor classifying the disease in the tomato leaves images. They used LVQ as the classifier of their network and \nreported an average accuracy of 86% on the test data.  In15 also the tomato leaf images were considered. Instead \nof implementing a CNN from scratch, they used AlexNet and VGG16 architectures. Rather than using the raw \nRGB images, a segmented version of the images was used in which the value of the background pixels is set \nto zero. The accuracy of 97.49% is this paper. Although the segmented images simplify the classification task \nfor the neural network, but in the real scenarios in the field, no segmented images are available, and it requires \neither humans to segment the images which is time-consuming or a neural network for segmentation. If there \nis a neural network for segmentation, that network can also perform the classification task. Moreover, a con-\nvolutional neural network for classification is capable of handling images with non-zero backgrounds by itself. \nBy considering the same class from the Plant Village, Halil et al. tried to achieve a model which can be used in \nreal-time applications. They trained AlexNet and  SqueezeNet16 on the tomato leaves images which obtained \nthe accuracy of 95.6% and %94.3 respectively. On Jetson Tx1, they reported the prediction time of 150 ms for \nAlexNet and 50 ms for  SqueezeNet17.\nThe ViT idea has been used in agricultural applications in recent works  like18  and19, which used the main \nversions of ViT (ViT-B16 with 16 and ViT-B32 with 32 attention blocks) without any changes to do the classifi-\ncation,  or20, which utilized two ViT models in parallel to handle images with two different resolutions. Some of \nthese works focused on the disease of one specific  plant19,20, and a some of them focused on the classification of \nplants and not their  diseases18. However, when the number of plants and diseases increases, the problem becomes \nmore challenging. Moreover, the prediction speed was not studied, which is crucial for real-time classification. \nEmploying such a heavy network for plant disease classification might be extreme, and shallower models might \nalso have sufficient performance in some situations.\nThis paper introduces lightweight models for real-time crop disease classification using ViT structure. The \nproposed models will be challenged on three different datasets with a different number of images. With respect \nto accuracy and prediction speed, these models are compared with CNN-based architecture with almost the \nsame complexity. Combinations of CNN and ViT are also investigated for the possibility of improving the results. \nFurthermore, the effect of image size on the results will be discussed.\nBackground\nBased on convolutional neural networks and ViT, the networks utilized in this paper were constructed. Accord-\ningly, in this section these main structures are briefly explained.\nConvolutional neural networks. The convolutional neural network was introduced by Y en leCun in the \n 1980s21. In 2012, when  AlexNet9 won the challenge of ImageNet, the CNNs have become more popular, and \nthey have been used in different projects for image classification and recognition, processing languages, medical \nimages, etc. In the Convolutional Neural Networks (CNN) structure, the image features are extracted with the \n3\nVol.:(0123456789)Scientific Reports |        (2022) 12:11554  | https://doi.org/10.1038/s41598-022-15163-0\nwww.nature.com/scientificreports/\npreservation of the 2D structure, while this structure consists of a number of filters. By sliding the filters over the \ninput image, the calculation for extracting features takes place.\nVision transformer. The Vision Transformer (ViT)22 is a novel idea for training neural networks on the \nimages. As they presented, this structure can reach higher accuracy in comparison with  ResNet1523 on some \nclassification datasets like  ImageNet23, CIFAR-10024, Oxford-IIIT  Pets25 and outperforms the Noisy Students \n(EfficientNet-L2)26.\nViT is inspired by  Bert27 and attention is all you  need28 papers which take advantage of the attention mecha-\nnism for Natural Language Processing (NLP) applications and introduced the concept of the transformer. The \ntransformer structure gets the sequence of the 1D input array. For processing 2D images, at first 2D patches are \nextracted from them, then they are reshaped to create 1D arrays, hence are appropriate for ViT structure. In \norder to complete preparing the patch embedding for the next layer, they are added to the positional encoder. \nThe positional encoder helps the network to remember the relative position of the patches with respect to each \nother. In the next step, the inputs are normalized with the normalization  layer29, then they enter the transformer \nblock. The most important part of this block is the multi-head attention layer. The purpose of the multi-head \nattention layer is to calculate weights to allocate higher values to the more important areas. In other words, it \nconcentrates the network attention on the more essential parts. The output of the multi-head attention layer is \na linear combination of each head.\nImplemented datasets\nIn this paper, three different datasets are used in order to evaluate the above-mentioned models. The first is a \ngathered small dataset which we call it Wheat Rust Classification dataset, the second is Rice Leaf Disease dataset, \nand the third is the popular dataset named Plant Village.\nWheat rust classification dataset. The first dataset used in this paper is collected by Safari et al.1, which \nis called “Wheat Rust Classification Dataset (WRCD)” . The Wheat Rust Classification dataset includes three \nclasses: healthy wheat, yellow rust and brown rust. As shown in Figure  1, the yellow rust class includes yellow \ndotes on wheat and brown rust makes wheat suffer from brown dotes. The other difference between them is how \ndots are aligned in these two categories. In the yellow rust class, the dots are approximately aligned in a row, but \nthe brown rust dots does not have a particular pattern. So these structures can be helpful for distinguishing the \nbrown and the yellow rust. The healthy class shows wheat without infections.\nIn this dataset, the Brown Rust class has 1128, the Y ellow Rust class has 1156 and the Healthy class has 1395 \npictures. Hence, the samples are approximately distributed uniformly in these three classes. The Wheat Rust \nClassification Dataset is available at: https:// www. kaggle. com/ sinad unk23/ behzad- safari- jalal.\nRice leaf disease dataset. Rice Leaf Disease Dataset (RLDD) is a small database with only 120 images of \ninfected rice leaves. These images have been taken from a rice field in India. There are three classes in this data-\nset: 1-Bacterial Leaf Disease, 2-Brown Spot and 3-Leaf Smut. Each class has 40 samples. Three random samples \nfrom the dataset are selected and shown in Figure  2. The background of these images are either removed and \nreplaced with white color, as illustrated in Figure 2a,b, respectively) or it is very simple as depicted in Figure 2c. \nThe Bacterial Leaf Blight symptom appears in yellow color and is elongated on the leaf. The Brown Spot and Leaf \nSmut are pretty similar to each other, but still, they differ from each other. The Brown Spot causes the circles with \nthe bigger area on the leaves and the color of these circles is darker compared to the Leaf Smut. In the Leaf Smut \ninfection, small dots are scattered through the leaf.\nIn this dataset, the main challenge arises from the limited number of images. In addition to that, Brown Spot \nand Leaf Smut symptoms are very similar to each other which makes the task even harder.\nFigure 1.  Sample images from Wheat Rust Classification Dataset: (a) brown rust, (b) healthy wheat, and (c) \nyellow rust.\n4\nVol:.(1234567890)Scientific Reports |        (2022) 12:11554  | https://doi.org/10.1038/s41598-022-15163-0\nwww.nature.com/scientificreports/\nPlant village. Plant Village is a large-scale dataset that contains 54,306 images of crop leaves. These leaves \nbelong to 14 crop species which some of them are healthy and some of them suffer from the disease. 26 kind of \ndifferent infection has been labeled. Here, the task is to identify the true class of the leaf (the crop species and \nalso its disease) based on the input image. Overall, there are 38 classes in this dataset.\nThese images are taken in particular conditions which may differ from the real world scenarios (for example, \nthe background has a simple texture and different color from the leaves). The distribution of images and these \nclasses are not uniform; some classes have more than 5000 images while others have less than 200 samples. This \nimbalanced distribution makes the classification task more challenging.\nNetwork structure\nIn this paper, a number of structures which are described as follows are studied. At first, two main building blocks \nare defined as CNN and Transformer block:\nConvolutional neural networks block. The CNN block consists of two convolutional layers with 3 by 3 \nkernels. In these two layers, padding and activation are not applied. The output of the second convolutional layer \nis entered into a leaky ReLU layer. Then one max pooling layer with 2 by 2 kernel is considered which can be seen \nin Figure 3a. The idea of using two consecutive convolutional layers with 3 by 3 kernels comes from the VGG \nstructure which suggested using the smaller filters on top of each other has the same receptive field with bigger \nfilters while they have less trainable parameters in comparison with the bigger  ones2.\nTransformer block. The second block is the Transformer block. As it is shown in Figure  3b, the input of \nthe block is fed to the layer normalization layer. The normalization layer is followed by a multi-head attention \nlayer. Four attention heads are used in all the transformer blocks and the projection dimension is 64. Then the \noutput of the multi-head attention layer adds to the input of the block with a skip connection. In the next step, \nthe layer normalization layer is applied again, and the output of this layer goes to the fully connected layer. The \nfinal output of the block is calculated by summing the output of the fully connected layers and the input of those \nlayers with another skip connection. It should be noted that in the mlp part shown in the block diagrams, two \nfully connected layers with 128 and 64 neurons exist and the activation of each of them is gelu, refer  to30.\nStructure of models. The main structure of different backbones and a common head of the implemented \nmodels are shown in Figure 4. The backbones of Model 1 and Model 2 contain Convolutional Blocks only, which \nis shown as Block I in Figure 4a. Model 1 has one Block I, while Mode 2 has two of it. Model 3 and Model 4 have \nonly Transformer Blocks in their backbone, shown as Block II in Figure 4a. Model 3 contains one Block II, while \nFigure 2.  Sample images from Rice Leaf Disease Dataset: (a) bacterial leaf blight, (b) brown spot, and (c) leaf \nsmut.\nFigure 3.  The schematics of the (a) convolutional block structure and (b) transformer block structure.\n5\nVol.:(0123456789)Scientific Reports |        (2022) 12:11554  | https://doi.org/10.1038/s41598-022-15163-0\nwww.nature.com/scientificreports/\nModel 4 contains two of it. Models 5, 6, 7, and 8 are hybrid as they contain both Convolutional Block(s) and \nTransformer Block(s). The backbones of Model 5 and Model 7 are structured by Block III and Block IV (shown \nin Figure 4), respectively. In Model 6, at first, Block I is used, and after that, Block II is attached to the output of \nBlock I. In Model 8, at first, Block II is used and then Block I is attached to the output of Block II. All models have \na similar classification head, shown in Figure 4b.\nExperimental results\nIn this section, the experiments on three datasets; Wheat Rust Classification, Rice Leaf Disease and Plant Village \nare explained.\nExperiment on wheat rust classification dataset. In this part, all the eight model structures have \nbeen used. Three different image resolutions were chosen for training the networks: 50 by 50, 100 by 100, and \n200 by 200. The lower the input resolution, the faster the prediction speed, but the accuracy could decrease. \nAccordingly, the models were trained and evaluated starting from a low resolution (50 by 50). The resolution \nwas doubled twice (100 by 100 and 200 by 200) to check the input resolution effect on the model’s accuracy and \nspeed. The dataset has been divided into 80% of training and 20% of validation part. The optimizer used dur -\ning training the models was the  AdamW31. This optimizer can separately find the optimum learning rate and \nweight decay for changing these two parameters independently. All structures were trained for 100 epochs with \nan initial learning rate of 0.001.\nDiscussion. Figure 5 illustrates the accuracy of the models on the validation data during the training. The fluc-\ntuations in the charts are due to image augmentation, dropout layers, and data shuffling. It is worth noting that \nthese oscillations almost have constant amplitudes, and there are no significant changes observed in the charts \nafter the epoch 50. It may not be necessary to train these models for a more extended time, and 100 epochs seem \nsufficient to reduce the effect of randomness sources. The weights corresponding to the highest validation accu-\nracy were stored and operated for the evaluation phase. It should be noted that in Figure 1a,c, the infected parts \nof the wheat leaves are occluded by other leaves and plants. However, the proposed model succeeded in gaining \nhigh accuracies on validation data.\nThe results of the evaluation are available in Table 1. All models are trained with three different resolutions. \nIn order to validate the results, the average F1 score, recall, and precision are displayed. The Giga floating-point \noperations per second (GFlops) measurement is used to measure the computer’s performance. As this unit is \ndecreased, the calculation cost will be reduced too. The convergence score is defined for comparing the results \nspecifically, and it shows the integration of the validation accuracy per epochs. So the sooner the accuracy con-\nverges, the higher value this unit gets. Model 3, model 4, and model 8 with the input resolution of 200 by 200 \nhave achieved the same average F1-score, recall, and precision with the EfficientNet model presented  by1. How-\never, these models have much lower computational costs based on GFlops in comparison with the EfficientNet. \nAmong all these models, the model 3 and 4 have converged faster than others. Model 3 has the lower GFlops in \ncomparison with Model 4. In the case of wheat rust classification, it seems that Model 3 with the resolution of \n200 by 200 outperforms all the other models in terms of accuracy and GFlops.\nExperiment on rice leaf disease dataset. The Leaf Rice Disease Dataset has a few images. Since the \ncomplexity of the model should be chosen according to the number of data, models 1, 3, 5, and 7 are chosen \nwhich have only two blocks for feature extraction. At first, the resolution of 50 by 50 was chosen for this experi-\nment, but after that, the resolution of 100 by 100 also has been tried. 80% of the images of each class are selected \nFigure 4.  The main parts of models’ structure: (a) the backbone block diagrams and b the classification head of \nmodels.\n6\nVol:.(1234567890)Scientific Reports |        (2022) 12:11554  | https://doi.org/10.1038/s41598-022-15163-0\nwww.nature.com/scientificreports/\nfor the training set and the others for the validation set. The optimizer and initial learning rate are the same as \nthe previous experiment, and the models were trained for 100 epochs.\nDiscussion. The evaluation results are shown in the Table 2. The F1-score, recall, and precision of the models on \neach class are reported (BLB: Bacterial Leaf Blight, BS: Brown Spot, LS: Leaf Smut). Although models 1, 3, and \n7 have achieved similar average F1-score, model 3 has the highest average precision compared to all the models. \nFigure 5.  Accuracy of the models during training on validation set.\nTable 1.  The results of the Wheat Rust Classification Dataset experiment. Significant values are in bold.\nModels Res\nResult of experiments\nAverage F1 Average recall Average precision GFlops Convergence score\nModel 1\n50 0.97 0.96 0.97 0.006 0.88\n100 0.98 0.98 0.98 0.03 0.93\n200 0.98 0.98 0.99 0.12 0.93\nModel 2\n50 0.98 0.98 0.98 0.007 0.91\n100 0.97 0.97 0.97 0.032 0.91\n200 0.99 0.99 0.99 0.14 0.93\nModel 3\n50 0.98 0.98 0.98 0.003 0.96\n100 0.99 0.99 0.99 0.012 0.92\n200 1 1 1 0.053 0.97\nModel 4\n50 0.98 0.98 0.98 0.005 0.96\n100 0.98 0.99 0.99 0.02 0.92\n200 1 1 1 0.089 0.97\nModel 5\n50 0.97 0.96 0.97 0.006 0.83\n100 0.98 0.98 0.98 0.025 0.90\n200 0.99 0.99 0.99 0.1 0.95\nModel 6\n50 0.94 0.94 0.95 0.009 0.82\n100 0.98 0.97 0.98 0.039 0.95\n200 0.98 0.98 0.99 0.17 0.94\nModel 7\n50 0.97 0.97 0.97 0.002 0.92\n100 0.99 0.99 0.99 0.008 0.95\n200 0.99 0.99 0.99 0.037 0.96\nModel 8\n50 0.97 0.97 0.97 0.03 0.92\n100 0.99 0.99 0.99 0.012 0.95\n200 1 1 1 0.053 0.96\nEfficientNetB01 – 1 1 1 0.794 -\n7\nVol.:(0123456789)Scientific Reports |        (2022) 12:11554  | https://doi.org/10.1038/s41598-022-15163-0\nwww.nature.com/scientificreports/\nIn this experiment, the CNN-based model (model1) gained the highest convergence score while having a similar \naverage recall and F1-score with model 3, a ViT based model. Although the work done  by10 has achieved the \nhighest F1-score on class BLB, but the accuracy on the class LS is pretty low and all the models outperform this \nwork in terms of the average F1-score. All the models also have been trained with the resolution of 100 by 100, \nbut no improvement was observed in any of those four cases. Because of that, it seemed unnecessary to try the \nresolution of 200 by 200.\nExperiment on plant village. In this paper, the RGB version of images has been used because the clas-\nsification results could reach higher accuracy in comparison with the grayscale  images12. Moreover, applying \nthe RGB version of images helps specify different kinds of diseases. Thanks to the hardware advancement, the \ncomputation costs of classification with the RGB and grayscale images are almost the same. The Plant Village \ndataset has more images and classes in comparison with WRCD and RLDD datasets. Because of that, models \n2, 4, 6, and 8 have been chosen for this experiment which have more complexity than models 1, 3, 5, and 7. In \naddition to that, the resolution 200 by 200 were chosen for these models. The number of neurons in the two \nfully connected layers before the final classifier was increased up to 200. Eighty percent of the images from each \nclass are selected as the training set, and the rest are used for validation. Same as the previous experiments, the \noptimizer was AdamW , and all the models have been trained for 100 epochs with an initial learning rate of 0.001.\nDiscussion. The evaluation results of the trained models are reported in Table  3. Model 4, which only con-\nsists of the transformer blocks, has gained the highest f1-score, recall, and precision. This model outperforms \nnot only model 2, model 6, and model 8 but also AlexNet and GoogleNet, which were trained from scratch in \nthe  baseline12. Model 4 has far fewer trainable parameters (less than 1 million) in comparison with GoogleNet \n(23 million) and AlexNet (62 million). When AlexNet and GoogleNet were trained with transfer learning, the \nreported precision, recall, and F1-score improved and were higher than all the other models. Since the proposed \nmodels are hand-designed, it will be computationally costly to train these models on ImageNet to use the initial \nweights for transfer learning. In this experiment, Model 6 has obtained better accuracy compared to Model 8. \nThis indicates that using the transformer blocks after the CNN blocks is better than using the transformer blocks \nat the earlier layers. Both models have obtained better accuracy than Model 2, which only has CNN blocks. Simi-\nlar to the previous experiment, Model 4 has the highest convergence score compared to other models. Model 6 \nconverges just a bit faster than Model 8, while Model 2 converges slower than all other models.\nPrediction speed test. An experiment has been designed to measure the prediction speed of the proposed \nmodels. A sample image was randomly chosen and resized to match the required input resolution of each model. \nEach model had to then make 1000 predictions based on this sample, and the time for this prediction was meas-\nured and averaged. It should be noted that since GoogleNet did not support input resolutions less than 256 by \n256, it was examined only with this resolution. All the other models were tested with the resolutions: 50 by 50, \n100 by 100, and 200 by 200. The experiment was conducted on four devices; CPU accelerators powered two, and \nTable 2.  The results of the Rice Leaf Disease Dataset experiment. Significant values are in bold.\nModels Res\nResult of experiments\nClass F1 Recall Precision Converg. score\nModel 1 50\nBLB 0.94 1.00 0.89\n0.69\nBS 0.93 0.88 1.00\nLS 0.88 0.88 0.88\nAverage 0.917 0.920 0.923\nModel 3 50\nBLB 0.93 0.88 1.00\n0.65\nBS 0.93 0.88 1.00\nLS 0.89 1.00 0.80\nAverage 0.917 0.920 0.930\nModel 5 50\nBLB 0.82 0.88 0.78\n0.61\nBS 0.88 0.88 0.88\nLS 0.80 0.75 0.86\nAverage 0.830 0.840 0.840\nModel 7 50\nBLB 0.93 0.87 1.00\n0.65\nBS 0.94 1.00 0.88\nLS 0.88 0.88 0.88\nAverage 0.917 0.917 0.920\nSegmentation +  SVM10 –\nBLB 1.00 – –\n–\nBS 0.80 – –\nLS 0.40 – –\nAverage 0.730 – –\n8\nVol:.(1234567890)Scientific Reports |        (2022) 12:11554  | https://doi.org/10.1038/s41598-022-15163-0\nwww.nature.com/scientificreports/\nTable 3.  The results of the plant village experiment. Significant values are in bold.\nModels Res\nResult of experiments\nAverage F1 Average recall Avarage precision Convergence score\nModel 2 200 0.9581 0.9586 0.9599 0.90\nModel 4 200 0.9877 0.9877 0.9878 0.95\nModel 6 200 0.9838 0.9837 0.9840 0.93\nModel 8 200 0.9783 0.9783 0.9788 0.93\nAlexNet12 (trained from scratch) 256 0.9782 0.9782 0.9782 –\nGoogleNet12 (trained from scratch) 256 0.9837 0.9837 0.9837 –\nAlexNet12 (transfer learning) 256 0.9928 0.9928 0.9927 –\nGoogleNet12 (transfer learning) 256 0.9935 0.9935 0.9935 –\nTable 4.  The devices inspection used for measuring the prediction speed of the models.\nDevice Accelerator Specification\n1 CPU Intel(R) Xeon(R) CPU @ 2.20GHz with 1 core\n2 CPU Intel(R) Xeon(R) CPU @ 2.20GHz with 2 cores\n3 GPU Tesla K80\n4 GPU Tesla P4\nFigure 6.  Prediction time of the proposed models with different image input resolution. The code of this paper \nis available at https:// github. com/ yasam inbor hani/ Plant Disea seCla ssifi cation.\n9\nVol.:(0123456789)Scientific Reports |        (2022) 12:11554  | https://doi.org/10.1038/s41598-022-15163-0\nwww.nature.com/scientificreports/\nGPU powered the other two. Table 4 shows the specifications for each device. The results of this experiment are \ndemonstrated in Figure 6. At the low resolution , the CPU accelerated devices had less prediction speed com-\npared to device 3, which had GPU. Except for the EfficientNetB0 which device 4 was significantly faster than the \nothers, in the other cases, its prediction speed is almost equal to device 2, which is a CPU accelerated device. At \nthe medium resolution , the GPU accelerated devices are faster than CPU accelerated devices when the models \ndo not have attention blocks (model 1, model 2, and EfficientNetB0), but device 3 was slower than CPU acceler-\nated devices in the models with attention blocks (models 3, 4, 5, 6, 7, 8). The device 4 was the fastest device at this \nresolution. For the highest resolution, GPUs were significantly faster than CPUs in all cases.\nIn all resolutions, the purely convolutional-based models (Model 1 and Model 2) were faster than the models \nwith attention blocks. The strictly attention-based models were the slowest compared to all the other hand-\ndesigned models. The hybrid models, which have both convolutional and attention blocks, inherited the speed \nfrom convolutional layers and are faster compared to purely attention-based models. Whether the convolutional \nblocks come first (Models 5 and 6) or the network starts feature extraction with the attention blocks (Models 7 \nand 8), the speed pattern is the same. All the hand-designed models are faster than EfficientNetB0 and Goog-\nleNet on all devices.\nConclusion\nIn this paper, a simplified version of the novel Vision Transformer was compared with a convolutional-based \narchitecture with similar complexity. In addition, the hybrid models which included the combination of the CNN \nand ViT were studied. These networks were employed to solve classification tasks on a small dataset (Rice Leaf \nDisease Dataset), medium dataset (Wheat Rust Classification Dataset), and large scale dataset (Plant Village). \nThe main challenges with the RLDD dataset were the less number of samples and the similarities between the \ntwo given classes. The WRCD dataset had more real scenario images with respect to the two other datasets. The \nPlant Village was a complex dataset with the skewed classes problem. In all cases, when the models are trained \nfrom scratch, the ViT based model not only achieved more accurate performance compared to the CNN or \nhybrid models, but also it obtained a comparable accuracy with the results reported in the literature. Moreo-\nver, the proposed ViT model had considerably lower parameters than the works done in the reviewed papers. \nHowever, the attention blocks utilized in this paper were slower than the implemented convolutional blocks on \nall devices. Combining the attention blocks with convolutional blocks helps the models to predict faster than \nthe ViT-based models while having higher accuracy than the CNN-based models (Plant Village and WRCD \nexperiments). It was also observed that the order of this combination does not significantly affect the prediction \nspeed, but using attention blocks followed by convolutional blocks led to slightly better accuracy (WRCD and \nRice Leaf Disease Dataset).\nFor future work, in order to move toward an automatic disease detection system, object localization networks \nare required to locate the plant leaves in a wide image, and multi-label classification needs to be solved to take \nmultiple diseases into account. In addition to finding a network structure for object localization and multi-label \nclassification, collecting a dataset with appropriate labels for this problem can be noted as future work of this \npaper.\nReceived: 11 October 2021; Accepted: 20 June 2022\nReferences\n 1. Safari, B., Alborzi, Y . & Najafi, E. Automated wheat disease detection using a ROS-based autonomous guided UAV . arXiv preprint \narXiv: 2206. 15042 (2022).\n 2. Simonyan, K. & Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv: 1409.  \n1556 (2014).\n 3. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. in Proceedings of the IEEE Conference on Computer \nVision and Pattern Recognition. 770–778. (2016).\n 4. Szegedy, C., Vanhoucke, V ., Ioffe, S., Shlens, J. & Wojna, Z. Rethinking the inception architecture for computer vision. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern Recognition. 2818–2826. (2016).\n 5. Howard, A. et al. Searching for mobilenetv3. in Proceedings of the IEEE/CVF International Conference on Computer Vision. 1314–\n1324. (2019).\n 6. Tan, M. & Le, Q. Efficientnet: Rethinking model scaling for convolutional neural networks. in International Conference on Machine \nLearning. 6105–6114. (PMLR, 2019).\n 7. Mi, Z., Zhang, X., Su, J., Han, D. & Su, B. Wheat stripe rust grading by deep learning with attention mechanism and images from \nmobile devices. Front. Plant Sci. 11 (2020).\n 8. Lin, Z. et al. A unified matrix-based convolutional neural network for fine-grained image classification of wheat leaf diseases. IEEE \nAccess 7, 11570–11590 (2019).\n 9. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep convolutional neural networks. Adv. Neural Inf. \nProcess. Syst. 25, 1097–1105 (2012).\n 10. Prajapati, H. B., Shah, J. P . & Dabhi, V . K. Detection and classification of rice plant diseases. Intell. Decis. Technol. 11, 357–373 \n(2017).\n 11. Hughes, D., Salathé, M. et al. An open access repository of images on plant health to enable the development of mobile disease \ndiagnostics. arXiv preprint arXiv: 1511. 08060 (2015).\n 12. Mohanty, S. P ., Hughes, D. P . & Salathé, M. Using deep learning for image-based plant disease detection. Front. Plant Sci. 7, 1419 \n(2016).\n 13. Szegedy, C. et al. Going deeper with convolutions. in Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni-\ntion. 1–9. (2015).\n 14. Sardogan, M., Tuncer, A. & Ozen, Y . Plant leaf disease detection and classification based on CNN with LVQ algorithm. in 2018 \n3rd International Conference on Computer Science and Engineering (UBMK). 382–385. (IEEE, 2018).\n 15. Rangarajan, A. K., Purushothaman, R. & Ramesh, A. Tomato crop disease classification using pre-trained deep learning algorithm. \nProc. Comput. Sci. 133, 1040–1047 (2018).\n10\nVol:.(1234567890)Scientific Reports |        (2022) 12:11554  | https://doi.org/10.1038/s41598-022-15163-0\nwww.nature.com/scientificreports/\n 16. Iandola, F . N. et al. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 mb model size. arXiv preprint arXiv: \n1602. 07360 (2016).\n 17. Durmuş, H., Güneş, E. O. & Kırcı, M. Disease detection on the leaves of the tomato plants by using deep learning. in 2017 6th \nInternational Conference on Agro-Geoinformatics. 1–5. (IEEE, 2017).\n 18. Reedha, R., Dericquebourg, E., Canals, R. & Hafiane, A. Transformer neural network for weed and crop classification of high \nresolution UAV images. Remote Sens. 14, 592 (2022).\n 19. Wu, S., Sun, Y . & Huang, H. Multi-granularity feature extraction based on vision transformer for tomato leaf disease recognition. \nin 2021 3rd International Academic Exchange Conference on Science and Technology Innovation (IAECST). 387–390. (IEEE, 2021).\n 20. Thai, H.-T., Tran-Van, N.-Y . & Le, K.-H. Artificial cognition for early leaf disease detection using vision transformers. in 2021 \nInternational Conference on Advanced Technologies for Communications (ATC). 33–38. (IEEE, 2021).\n 21. LeCun, Y . et al. Backpropagation applied to handwritten zip code recognition. Neural Comput. 1, 541–551 (1989).\n 22. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv: 2010. \n11929 (2020).\n 23. Deng, J. et al. ImageNet: A large-scale hierarchical image database. in Proceeding of the IEEE Conference on Computer Vision and \nPattern Recognition. 248–255 (2009).\n 24. Krizhevsky, A., Hinton, G. et al. Learning multiple layers of features from tiny images (2009).\n 25. Parkhi, O. M., Vedaldi, A., Zisserman, A. & Jawahar, C. Cats and dogs. in Proceedings of the IEEE Conference on Computer Vision \nand Pattern Recognition. 3498–3505 (IEEE, 2012).\n 26. Xie, Q., Luong, M.-T., Hovy, E. & Le, Q. V . Self-training with noisy student improves imagenet classification. in Proceedings of the \nIEEE/CVF Conference on Computer Vision and Pattern Recognition. 10687–10698 (2020).\n 27. Devlin, J., Chang, M.-W ., Lee, K. & Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. \narXiv preprint arXiv: 1810. 04805 (2018).\n 28. Vaswani, A. et al. Attention is all you need. in Advances in Neural Information Processing Systems. 5998–6008 (2017).\n 29. Ba, J. L., Kiros, J. R. & Hinton, G. E. Layer normalization. arXiv preprint arXiv: 1607. 06450 (2016).\n 30. Hendrycks, D. & Gimpel, K. Gaussian error linear units (gelus). arXiv preprint arXiv: 1606. 08415 (2016).\n 31. Loshchilov, I. & Frank, H. Decoupled weight decay regularization. arXiv preprint arXiv: 1711. 05101 (2017).\nAuthor contributions\nAll authors have the same contribution.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to E.N.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2022",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8034065961837769
    },
    {
      "name": "Convolutional neural network",
      "score": 0.7998806238174438
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7122457027435303
    },
    {
      "name": "Plant disease",
      "score": 0.6927953958511353
    },
    {
      "name": "Deep learning",
      "score": 0.6533500552177429
    },
    {
      "name": "Machine learning",
      "score": 0.6136752367019653
    },
    {
      "name": "Transformer",
      "score": 0.5109400153160095
    },
    {
      "name": "Artificial neural network",
      "score": 0.42888781428337097
    },
    {
      "name": "Engineering",
      "score": 0.07730603218078613
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Biotechnology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I80543232",
      "name": "K.N.Toosi University of Technology",
      "country": "IR"
    },
    {
      "id": "https://openalex.org/I1516879",
      "name": "Tarbiat Modares University",
      "country": "IR"
    }
  ],
  "cited_by": 213
}