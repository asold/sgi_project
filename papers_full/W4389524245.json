{
  "title": "WhisBERT: Multimodal Text-Audio Language Modeling on 100M Words",
  "url": "https://openalex.org/W4389524245",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2135409877",
      "name": "Lukas Wolf",
      "affiliations": [
        "Mitre (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3081590021",
      "name": "Klemen Kotar",
      "affiliations": [
        "Mitre (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2982782814",
      "name": "Greta Tuckute",
      "affiliations": [
        "Mitre (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2129444909",
      "name": "Eghbal Hosseini",
      "affiliations": [
        "Mitre (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2884410416",
      "name": "Tamar I. Regev",
      "affiliations": [
        "Mitre (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3157889329",
      "name": "Ethan Gotlieb Wilcox",
      "affiliations": [
        "Mitre (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5092427299",
      "name": "Alexander Scott Warstadt",
      "affiliations": [
        "Mitre (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4309419356",
    "https://openalex.org/W3015783745",
    "https://openalex.org/W4394671563",
    "https://openalex.org/W4378768739",
    "https://openalex.org/W2152790380",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W4323572061",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3026408381",
    "https://openalex.org/W4309953147",
    "https://openalex.org/W4307106469",
    "https://openalex.org/W2619383789",
    "https://openalex.org/W4311000453",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W4318621130",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W4312910992",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4385822293",
    "https://openalex.org/W4382404227"
  ],
  "abstract": "Lukas Wolf, Klemen Kotar, Greta Tuckute, Eghbal Hosseini, Tamar I. Regev, Ethan Gotlieb Wilcox, Alexander Scott Warstadt. Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. 2023.",
  "full_text": "Proceedings of the 27th Conference on Computational Natural Language Learning:\nVolume 2: The BabyLM Challenge, pages 253–258\nDecember 6-7, 2023 ©2023 Association for Computational Linguistics\nWhisBERT: Multimodal Text-Audio Language Modeling on 100M Words\nLukas WolfQ Klemen KotarS Greta TuckuteM Eghbal HosseiniM\nTamar I. RegevM Ethan Gotlieb WilcoxQ Alex WarstadtQ\nQETH Zürich MMIT SStanford University\n{wolflu, warstadt, ethan.wilcox}@ethz.ch\nklemen@allenai.org {ehoseini, tamarr}@mit.edu\nAbstract\nTraining on multiple modalities of input can\naugment the capabilities of a language model.\nHere, we ask whether such a training regime\ncan improve the quality and efficiency of these\nsystems as well. We focus on text–audio and\nintroduce WhisBERT, which is inspired by the\ntext–image approach of FLA V A (Singh et al.,\n2022). In accordance with BabyLM (Warstadt\net al., 2023) guidelines, we pretrain WhisBERT\non a dataset comprising only 100 million words\nplus their corresponding speech from the word-\naligned version of the People’s Speech dataset\n(Galvez et al., 2021). To assess the impact\nof multimodality, we compare versions of the\nmodel that are trained on text only and on both\naudio and text simultaneously. We find that\nwhile WhisBERT is able to perform well on\nmultimodal masked modeling and surpasses the\nBabyLM baselines in most benchmark tasks, it\nstruggles to optimize its complex objective and\noutperform its text-only WhisBERT baseline.\nhttps://github.com/lu-wo/whisbert\n1 Introduction\nRecent advances in language modeling and their\ndownstream applications have been driven, in large\npart, by bigger models, both in terms of model size\nand in terms of training data. Larger and larger pre-\ntraining datasets highlight the gap in terms of learn-\ning efficiency between humans and deep learning\nmodels—while state-of-the-art language models\nneed billions of examples to approach human-level\nlanguage performance, people learn their language\nfrom experience with about 100 million words or\nless (Warstadt and Bowman, 2022; Frank, 2023).\nWe hypothesize that one major reason for this\ndata efficiency gap is the difference in input be-\ntween humans and current deep learning systems.\nHuman language learning involves multiple modal-\nities, including both visual and auditory input. In\ncontrast, typical language models are trained on\nrepresentations of text alone. For this BabyLM\nsubmission, we ask whether training on inputs of\nmultiple modalities can increase language models’\ntraining efficiency, with a focus on text-audio mul-\ntimodal input. We conjecture that multimodal data\nsources have the potential to enrich the language\nlearning process, enabling models to leverage com-\nplementary information from different modalities\nand thus augment their learning capacity (Bal-\ntrušaitis et al., 2017).\nMultimodal language modeling has experienced\na noteworthy surge in research productivity lately,\nin applications such as image retrieval, semantic\nembeddings, and image generation (Driess et al.,\n2023; Koh et al., 2023; Yasunaga et al., 2023)\nHowever, text-audio multimodal language mod-\neling (e.g. (Chuang et al., 2019; Lakhotia et al.,\n2021)) remains largely unexplored, especially in\nlow-resource settings such as the 100 million train-\ning regime we employ here. As a first step towards\na text-audio language model, we introduce Whis-\nBERT, a novel masked language model (MLM)\narchitecture inspired by vision-text models such\nas FLA V A (Singh et al., 2022). The core idea is\nthat WhisBERT is trained in a multitask setting on\nboth unimodal (i.e. text- or audio-only) and mul-\ntimodal objectives. In multimodal objectives, the\nmodel receives matched text-audio segments, and\nit can use information from one modality to learn\nrepresentations for the other.\nTo accommodate the specific requirements of\nthe BabyLM challenge (Warstadt et al., 2023), we\npretrain WhisBert on a dataset of matched audio\nand text transcripts comprising 100 million words\nsampled from the People’s Speech dataset (Galvez\net al., 2021). We use an improved version of the\naudio-text-aligned training data, a subset of an up-\ncoming speech production dataset release (see Sec-\ntion 3). This commitment to using high-quality\npretraining data is in line with the data efficiency\nobjectives of the BabyLM challenge.\n253\nWe carry out a rigorous evaluation of the per-\nformance of the audio, text, and multimodal en-\ncoders within this new framework. We find that\neven though the optimization problem in the mul-\ntimodal setting is much harder compared to a uni-\nmodal setting, the multimodal WhisBERT model\noutperforms the text-only baseline in a majority of\nthe BabyLM challenge tasks even when trained for\nonly a single iteration over the dataset.\n2 WhisBERT\nWhisBERT is a multimodal audio and text model\nthat is inspired by OpenAI’s Whisper model (Rad-\nford et al., 2022) for speech recognition and BERT\n(Devlin et al., 2019) for bidirectional language en-\ncoding. WhisBERT contains two separate input\nstreams, one of audio and of its corresponding text\n(i.e., a transcription). The model is trained using a\ncombination of two unimodal and three multimodal\nmasked training objectives. In the unimodal setting,\nthe model must predict either a masked word or a\nmasked patch of audio. In the multimodal training\nsetting, the model must predict pairs of matched\nword/audio patches. This multi-objective training\nsetup is inspired by the visual-audio model FLA V A\n(Singh et al., 2022).\n2.1 Architecture details\nAudio encoder In order to create audio patches\nthat we can process with Whisper’s bidirectional\ntransformer encoder (Vaswani et al., 2017), the\naudio stream is first passed through the Whisper\nFeature Extractor available on Hugging Face1.\nAll audio data is re-sampled to a rate of 16,000\nHz, and an 80-channel log-magnitude Mel spec-\ntrogram representation is computed using 25-\nmillisecond windows with a 10-millisecond stride.\nWe then pass the audio spectrogram through a\npatch embedding layer: a convolutional encoder\nprocesses the extracted frequency features using\na stem of two 1-dimensional convolution layers\n(along the time dimension, filters cover all input\nfrequencies), both with a filter width of 16 and\nincorporating the GELU activation function. The\nsecond convolution layer employs a stride of 10.\nThis patch embedding layer creates overlapping\n1-dimensional audio patches covering 100ms of the\naudio signal as input to the transformer.\nAfter preprocessing and patch embedding, sinu-\nsoidal position embeddings are added to the stem’s\n1Documentation for Whisper is available here.\noutput, which is then processed by Whisper’s trans-\nformer encoder blocks. A notable difference to\nthe standard Whisper encoder is that we prepend a\nlearnable classification (henceforth, CLS) token at\nthe beginning of the audio patch sequence. There-\nfore, the audio encoder produces a list of audio\nhidden states {hA}each corresponding to a contex-\ntualized audio patch, as well as an additional audio\nclassification state hCLS,A.\nText encoder In order to encode the text input,\nwe choose a standard bidirectional transformer\narchitecture following the BERT (Devlin et al.,\n2019) model. We train a WordPiece (Wu et al.,\n2016) tokenizer on the 100M words in our People’s\nspeech (Galvez et al., 2021) subset (see Section 3).\nThe WordPiece tokenizer automatically prepends\na CLS token to the token sequence which is con-\ntextualized with the rest of the sequence. The text\nencoder produces a list of text hidden states {hT }\ncorresponding to a text token, as well as an addi-\ntional text CLS token hCLS,T .\nMultimodal encoder The multimodal encoder\nis a standard transformer encoder that gets as in-\nput the concatenated contextualized audio and text\nsequences. Additionally, we prepend a learnable\nmultimodal CLS token and employ sinusoidal po-\nsitional embeddings. The multimodal encoder con-\ntextualizes the multimodal sequence and outputs\na list of multimodal hidden states {hM }each cor-\nresponding to an unimodal vector from {hA}or\n{hT }, as well as an additional multimodal CLS\ntoken hCLS,M .\nAdapting to downstream tasksThe WhisBert\nmodel can be readily applied to both unimodal\nand multimodal tasks. For audio recognition tasks\n(e.g., speaker identification or speech recognition),\nwe apply a classifier head (e.g., a linear layer or\na multi-layer perceptron) on top of the unimodal\nclassification token, hCLS,A, from the audio en-\ncoder. Similarly, for language understanding and\nmultimodal reasoning tasks, we can apply a classi-\nfier head on top of the classification token, hCLS,T ,\nfrom the text encoder or hCLS,M from the multi-\nmodal encoder, respectively.\n2.2 Pretraining objectives\nOur goal is to pretrain models to have robust con-\ntextual representations for both text and audio on\ntheir own as well as for aligned text-audio pairs.\nWe use the approach from FLA V A (Singh et al.,\n254\n2022) of multitask training over a selection of uni-\nmodal and multimodal training objectives that have\nbeen demonstrated to facilitate joint learning on\nimages and text. We adapt the five objectives used\nby FLA V A for the audio domain.\n2.2.1 Unimodal pretraining objectives\nMasked Language Modeling Masked Language\nModeling (MLM) is a pretraining objective that\nencourages the model to learn a deep understanding\nof the language. In MLM, a portion of the input\ntokens is masked and the model is trained to predict\nthe original identity of the masked tokens based on\ntheir context.\nGiven an input sequence of tokens x =\n[x1,x2,...,x T ], for MLM, a subset M of these to-\nkens is selected to be masked. The objective is to\nminimize the negative log-likelihood of the masked\ntokens:\nLMLM(x) =− 1\n|M|\n∑\nt∈M\nlog pmodel(xt|x¬t) (1)\nHere, xt is a masked token, x¬t represents the\nsequence with the token xt masked, and pmodel is\nthe model’s probability distribution over possible\ntokens. |M|is the size of the subset of masked\ntokens, and the sum is taken over all masked posi-\ntions t. The goal is to adjust the model’s parameters\nto minimize this loss. We obtain a probability dis-\ntribution over the vocabulary by applying a linear\nprediction head on the text hidden states {hT }.\nMasked Audio Modeling We introduce the\nMasked Audio Modeling (MAM) objectiveLMAM\nwhich follows the principles of Contrastive Predic-\ntive Coding (van den Oord et al., 2019). In MAM,\nwe randomly mask audio patches in the input se-\nquence to the audio encoder. The encoder is ex-\npected to generate outputs that are most similar to\nthe unmasked input at a particular masked position\nt. The self-supervised loss function, which aims\nto encourage the model to align masked tokens\nwith their unmasked identities given the context, is\ndefined for a masked token localized at tas:\nLMAM = −log exp(sim(ct,bt)/κ)∑\nbi∈BD exp(sim(ct,bi)/κ) (2)\nHere, ct is the output of the transformer at po-\nsition t, and bi is the audio representation vector\nof the (unmasked) patch at some offset i. BD is\na set of 20 uniformly selected negative samples\nfrom the same sequence, plus bt, and sim() is a\nsimilarity function. For our implementation, we\nuse the cosine similarity function, adjusted by a\ntemperature function, κ, which is set to 0.1. The\nloss function operates by adjusting the output of\nthe transformer at position tto be most similar to\nthe encoded representation at t, despite the fact that\nthis input to the transformer is masked. In this way,\nthe model is encouraged to predict the content of\nthe masked spans based on the unmasked context.\n2.2.2 Multimodal Pretraining Objectives\nMultimodal Contrastive Loss Contrastive loss\n(Gutmann and Hyvärinen, 2010) has been success-\nfully applied to image-text representation learn-\ning in approaches such as CLIP (Radford et al.,\n2021). Our audio-text contrastive loss LMMC\naims to maximize the cosine similarities between\nmatched audio and text pairs and minimize those\nfor the unmatched pairs across a given batch of au-\ndio clips and corresponding text. This is achieved\nby linearly projecting the classification token of\neach audio sequence hCLS,A and text sequence\nhCLS,T into a common embedding space, followed\nby L2-normalization, dot-product, and a softmax\nloss scaled by temperature.\nThe goal of this process is to ensure that the au-\ndio and text representations for the same data point\nare brought closer together in the embedding space,\nwhile representations for different data points are\npushed apart. This encourages the model to learn\nmeaningful representations that capture the shared\ninformation between the audio and text modalities.\nMasked Multimodal Modeling (MMM) We in-\ntroduce a Masked Multimodal Modeling (MMM)\npretraining objective LMMM , that uses the output\nof the multimodal encoder {hM }to attempt to re-\nconstruct the masked tokens from both the audio\nand text sequences. For the multimodal contextual-\nized audio tokens, we employ the Contrastive Pre-\ndictive Coding strategy introduced in Section 2.2.1.\nFor the multimodal text tokens, we add a multi-\nmodal masked language modeling head we com-\npute the MLM loss as introduced in Section 2.2.1.\nThe MMM pretraining objective is designed to\nencourage the model to understand the interdepen-\ndencies between audio and text modalities, which\nin addition to the MMC loss has been found to\nimprove performance on multimodal downstream\ntasks (Singh et al., 2022). It is computed separately\nfrom the contrastive loss, which is applied on audio\nand text tokens without any masking.\n255\nAudio-Text Matching (ATM) Finally, we incor-\nporate an Audio-Text Matching loss, LATM , in\nwhich we feed a batch of samples that include both\nmatched and unmatched audio-text pairs. We apply\na classifier on top of the output from the multi-\nmodal encode to decide if the input audio and text\nmatch each other.\n2.3 Pretraining WhisBERT\nWe pretrain WhisBERT on both text and audio sam-\nples from the dataset introduced in Section 3 for\nfive epochs with stochastic gradient descent. Al-\nthough WhisBERT is able to learn both from paired\nand unpaired examples, in our pretraining dataset\nwe only encounter text-audio pairs. This allows\nus to always apply all unimodal and multimodal\nobjective functions. For further details and hyper-\nparameters we refer to this GitHub repository.\n3 People’s Speech Dataset\nThe People’s Speech dataset (Galvez et al., 2021) is\na free-to-download, 30k hour English speech recog-\nnition dataset. The dataset is collected from appro-\npriately licensed internet audio data with existing\ntranscriptions, consisting of a clean and a dirty sub-\nset. We re-transcribed and re-aligned the People’s\nSpeech dataset using recently-released automatic\nspeech recognition toolkits (Radford et al., 2022;\nBain et al., 2023), which may provide better align-\nment than the baseline, publically available align-\nments. For this step we transcribe speech the Whis-\nper large-v2 model from OpenAI (Radford et al.,\n2022). Numerals and non-standard characters were\nsuppressed in the transcriptions, such that numbers\nwere represented as words and non-standard char-\nacters were omitted. Otherwise, default parameters\nwere used. The transcriptions were force-aligned to\nmatch the audio files using the WhisperX pipeline\n(Bain et al., 2023; Bredin et al., 2019; Baevski et al.,\n2020). We excluded very short transcripts (fewer\nthan 100 words) or transcripts that contained more\nthan 0.1% of words that could not be transcribed.\nThe remaining files were sorted according to mean\nword-level transcription confidence (Whisper es-\ntimates a value between 0 and 1 that denotes the\ntranscription confidence per word). We selected the\nfiles containing the first 100M words in this order-\ning. The average confidence of these final 100M\nwords was 0.78 with 47M words from the clean\naudio subset and 53M words from the dirty audio\nsubset. The transcribed, word-aligned dataset will\nFigure 1: Text-only baseline vs WhisBERT on masked\nlanguage modeling task during the first epoch. Inter-\nestingly, during the first epoch WhisBERT seems to\nperform better (outperforming the text-only baseline in\n11 out of 17 tasks), but after five epochs does not outper-\nform the text-only baseline across all benchmark tasks\nbe released as part of an upcoming speech produc-\ntion dataset.\n4 Experimental Results\nThe main question we are interested in is whether\npretraining on audio–text data can improve model\nperformance. We assess this by comparing the text-\nencoder only version of WhisBERT compared to\nthe exact same architecture trained with the multi-\nmodal objectives introduced in Section 2.2. (This\nis the MLM (text) vs. MM (multi-modal) compari-\nson in Table 1.) Our results suggest that the answer\nis mixed. The MLM (text-only) version of the\nmodel achieves higher scores on 12 out of the 17\ntest suites, with the multi-modal model performing\nhigher for Ellipsis, Island Effects, Quantifiers, Hy-\npernym, and Question/Answer Congruence (tricky)\ntests. Interestingly, the three of these that were in\nthe original BLiMP paper (Ellipsis, Island Effects\nand Quantifiers), were three of the four lowest-\nscoring tests for human accuracy, suggesting that\nwhere multi-modality does help, it is in processing\nparticularly syntactically difficult material. Both\nof our trained models outperform the OPT-125M,\nRoBERTa and T5 baselines, averaging across tasks.\n5 Discussion\nLimitations We begin our discussion by noting\nthe limitations of the current work. First, the Peo-\nple’s V oice dataset presents a unique set of chal-\nlenges, which likely resulted in limitations of the\nWhisBERT model. The most significant of these is\n256\nTask MLM MM OPT-125mRoBERTa-baseT5-base\nanaphor_agreement 83.74% 81.29% 63.8% 81.5% 68.9%\nargument_structure 68.60% 64.88% 70.6% 67.1% 63.8%\nbinding 66.95% 65.38% 67.1% 67.3% 60.4%\ncontrol_raising 65.25% 64.76% 66.5% 67.9% 60.9%\ndeterminer_noun_agreement92.24% 87.93% 78.5% 90.8% 72.2%\nellipsis 83.14% 88.68% 62% 76.4% 34.4%\nfiller_gap 73.12% 72.02% 63.8% 63.5% 48.2%\nirregular_forms 89.62% 85.90% 67.5% 87.4% 77.6%\nisland_effects 53.51% 55.87% 48.6% 39.9% 45.6%\nnpi_licensing 64.77% 55.12% 46.7% 55.9% 47.8%\nquantifiers 69.58% 71.69% 59.6% 70.5% 61.2%\nsubject_verb_agreement 75.05% 70.73% 56.9% 65.4% 65.0%\nhypernym 50.12% 51.98% 50.0% 49.4% 48.0%\nqa_congruence_easy 71.88% 67.19% 54.7% 31.3% 40.6%\nqa_congruence_tricky 52.12% 53.94% 31.5% 32.1% 21.2%\nsubject_aux_inversion 77.90% 74.85% 80.3% 71.7% 64.9%\nturn_taking 61.79% 58.21% 57.1% 53.2% 45.0%\nTable 1: Evaluation scores of text-only (MLM), multimodal WhisBERT (MM), and the BabyLM baselines on\nBLiMP tasks. The BabyLM baselines were trained on the 100M words BabyLM dataset.\nthat it is primarily comprised of audio from movies,\nand thus includes things like background noise,\nmusic and audio effects that accompanied the dia-\nlog. This could have resulted in lower text–audio\nalignment accuracy, and likely made the audio-\nmodeling challenge more difficult than for an in-\nstudio recorded dataset.\nSecond, the requirements of the BabyLM chal-\nlenge presented us with additional restrictions.\nMost notably, we were not allowed to use pre-\ntrained audio encoders, and thus had to train these\nfrom scratch. Likely, this contributed to sub-\noptimal performance and requires further explo-\nration. Furthermore, due to time limitations, we\ndid not fully explore the space of the model’s hy-\nperparameters; it is well known that changes in\nhyperparameter settings can have large impacts on\na model’s performance.\nOur mixed results when comparing WhisBERT\nagainst a text-only model suggest that small data\nsettings are insufficient for effectively training a\ntext-only masked language model. Given that the\narchitectural basis for WhisBERT, Flava, was de-\nsigned and built as a large-data foundation model,\nwe suggest that such larger-data settings serve as\nthe basis for future development and testing of the\nWhisBERT model.\nFuture Work We plan to train versions of Whis-\nBERT on more than 100M words and their cor-\nresponding audio. This would enable investiga-\ntions of the full capacity of the WhisBERT model\nand make it more comparable to similar vision-text\nmodels such as FLA V A (Singh et al., 2022). On\nthe architecture level, one could replace the bidirec-\ntional transformer in the WhisBERT architecture\nwith an autoregressive language model, allowing\nthe use of the standard Whisper pretraining objec-\ntives in addition to the multi-modal ones.\nContribution Statement\nLW, EH, TIR, EGW, and AW conceived of the\nideas presented in this work. KK and GT provided\nthe dataset used in pretraining WhisBERT. LW im-\nplemented the model and carried out the experi-\nments. LW, KK, GT, EGW, AW, and TIR wrote the\nfirst draft of the manuscript. All authors edited the\nmanuscript and reviewed the work.\nReferences\nAlexei Baevski, Henry Zhou, Abdel rahman Mohamed,\nand Michael Auli. 2020. wav2vec 2.0: A framework\nfor self-supervised learning of speech representations.\nArXiv, abs/2006.11477.\nMax Bain, Jaesung Huh, Tengda Han, and Andrew Zis-\nserman. 2023. Whisperx: Time-accurate speech tran-\nscription of long-form audio.\nTadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe\nMorency. 2017. Multimodal machine learning: A\nsurvey and taxonomy.\nHervé Bredin, Ruiqing Yin, Juan Manuel Coria, Gre-\ngory Gelly, Pavel Korshunov, Marvin Lavechin,\nDiego Fustes, Hadrien Titeux, Wassim Bouaziz, and\nMarie-Philippe Gill. 2019. Pyannote.audio: Neural\nbuilding blocks for speaker diarization. ICASSP 2020\n- 2020 IEEE International Conference on Acoustics,\n257\nSpeech and Signal Processing (ICASSP), pages 7124–\n7128.\nYung-Sung Chuang, Chi-Liang Liu, Hung-Yi Lee,\nand Lin-shan Lee. 2019. Speechbert: An audio-\nand-text jointly learned language model for end-\nto-end spoken question answering. arXiv preprint\narXiv:1910.11559.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,\nAakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, et al.\n2023. Palm-e: An embodied multimodal language\nmodel. arXiv preprint arXiv:2303.03378.\nMichael C Frank. 2023. Bridging the data gap between\nchildren and large language models.\nDaniel Galvez, Greg Diamos, Juan Ciro, Juan Felipe\nCerón, Keith Achorn, Anjali Gopi, David Kanter,\nMaximilian Lam, Mark Mazumder, and Vijay Janapa\nReddi. 2021. The People’s Speech: A Large-Scale\nDiverse English Speech Recognition Dataset for\nCommercial Usage. ArXiv:2111.09344 [cs, stat].\nMichael Gutmann and Aapo Hyvärinen. 2010. Noise-\ncontrastive estimation: A new estimation principle\nfor unnormalized statistical models. In Proceedings\nof the Thirteenth International Conference on Artifi-\ncial Intelligence and Statistics, volume 9 of Proceed-\nings of Machine Learning Research, pages 297–304,\nChia Laguna Resort, Sardinia, Italy. PMLR.\nJing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov.\n2023. Generating images with multimodal language\nmodels.\nKushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu,\nYossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh\nNguyen, Jade Copet, Alexei Baevski, Abdelrahman\nMohamed, et al. 2021. On generative spoken lan-\nguage modeling from raw audio. Transactions of the\nAssociation for Computational Linguistics, 9:1336–\n1354.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision. In Proceedings of the 38th International\nConference on Machine Learning , volume 139 of\nProceedings of Machine Learning Research, pages\n8748–8763. PMLR.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brock-\nman, Christine McLeavey, and Ilya Sutskever. 2022.\nRobust speech recognition via large-scale weak su-\npervision.\nAmanpreet Singh, Ronghang Hu, Vedanuj Goswami,\nGuillaume Couairon, Wojciech Galuba, Marcus\nRohrbach, and Douwe Kiela. 2022. FLA V A: A Foun-\ndational Language And Vision Alignment Model.\nArXiv:2112.04482 [cs].\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2019.\nRepresentation learning with contrastive predictive\ncoding.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need.\nAlex Warstadt and Samuel R Bowman. 2022. What\nartificial neural networks can tell us about human\nlanguage acquisition. In Shalom Lappin and Jean-\nPhilippe Bernardy, editors, Algebraic Structures in\nNatural Language, pages 17–60. CRC Press. Pub-\nlisher: CRC Press.\nAlex Warstadt, Leshem Choshen, Aaron Mueller, Adina\nWilliams, Ethan Wilcox, and Chengxu Zhuang. 2023.\nCall for papers – the babylm challenge: Sample-\nefficient pretraining on a developmentally plausible\ncorpus.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le,\nMohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff\nKlingner, Apurva Shah, Melvin Johnson, Xiaobing\nLiu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato,\nTaku Kudo, Hideto Kazawa, Keith Stevens, George\nKurian, Nishant Patil, Wei Wang, Cliff Young, Jason\nSmith, Jason Riesa, Alex Rudnick, Oriol Vinyals,\nGreg Corrado, Macduff Hughes, and Jeffrey Dean.\n2016. Google’s neural machine translation system:\nBridging the gap between human and machine trans-\nlation.\nMichihiro Yasunaga, Armen Aghajanyan, Weijia Shi,\nRichard James, Jure Leskovec, Percy Liang, Mike\nLewis, Luke Zettlemoyer, and Wen-tau Yih. 2023.\nRetrieval-augmented multimodal language modeling.\n258",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6353881359100342
    },
    {
      "name": "Linguistics",
      "score": 0.616793155670166
    },
    {
      "name": "Natural language",
      "score": 0.5092593431472778
    },
    {
      "name": "Natural language processing",
      "score": 0.5020017623901367
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4314609169960022
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.4120097756385803
    },
    {
      "name": "Philosophy",
      "score": 0.18754878640174866
    },
    {
      "name": "History",
      "score": 0.14451539516448975
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I35440088",
      "name": "ETH Zurich",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ],
  "cited_by": 4
}