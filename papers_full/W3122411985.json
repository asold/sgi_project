{
    "title": "SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation",
    "url": "https://openalex.org/W3122411985",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2171195116",
            "name": "Brendan Duke",
            "affiliations": [
                "University of Toronto",
                "Vector Institute",
                "FACE Foundation"
            ]
        },
        {
            "id": "https://openalex.org/A2161834068",
            "name": "Abdalla Ahmed",
            "affiliations": [
                "FACE Foundation"
            ]
        },
        {
            "id": "https://openalex.org/A2032790125",
            "name": "Christian Wolf",
            "affiliations": [
                "Université Claude Bernard Lyon 1",
                "Laboratoire d'Informatique en Images et Systèmes d'Information"
            ]
        },
        {
            "id": "https://openalex.org/A940636539",
            "name": "Parham Aarabi",
            "affiliations": [
                "University of Toronto",
                "FACE Foundation"
            ]
        },
        {
            "id": "https://openalex.org/A2108373160",
            "name": "GRAHAM W TAYLOR",
            "affiliations": [
                "Vector Institute",
                "University of Guelph"
            ]
        },
        {
            "id": "https://openalex.org/A2171195116",
            "name": "Brendan Duke",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2161834068",
            "name": "Abdalla Ahmed",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2032790125",
            "name": "Christian Wolf",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A940636539",
            "name": "Parham Aarabi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2108373160",
            "name": "GRAHAM W TAYLOR",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6767853649",
        "https://openalex.org/W6637373629",
        "https://openalex.org/W6750690285",
        "https://openalex.org/W2990205821",
        "https://openalex.org/W6743968950",
        "https://openalex.org/W2944255943",
        "https://openalex.org/W3108819577",
        "https://openalex.org/W6746549228",
        "https://openalex.org/W2605229288",
        "https://openalex.org/W2470139095",
        "https://openalex.org/W6766904570",
        "https://openalex.org/W6680171035",
        "https://openalex.org/W6753214761",
        "https://openalex.org/W2794847483",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2750515003",
        "https://openalex.org/W2963503215",
        "https://openalex.org/W2972816482",
        "https://openalex.org/W6763688970",
        "https://openalex.org/W6748907426",
        "https://openalex.org/W6771626834",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W6754472059",
        "https://openalex.org/W6737324727",
        "https://openalex.org/W3034885317",
        "https://openalex.org/W6775487857",
        "https://openalex.org/W2962825871",
        "https://openalex.org/W6754033419",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W2963983744",
        "https://openalex.org/W6757010476",
        "https://openalex.org/W2963563276",
        "https://openalex.org/W2798823518",
        "https://openalex.org/W6743811873",
        "https://openalex.org/W6753986461",
        "https://openalex.org/W2981689412",
        "https://openalex.org/W2562457735",
        "https://openalex.org/W2799239273",
        "https://openalex.org/W2963253279",
        "https://openalex.org/W6761628794",
        "https://openalex.org/W2963548592",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W764651262",
        "https://openalex.org/W2916797271",
        "https://openalex.org/W2921536280",
        "https://openalex.org/W6761623811",
        "https://openalex.org/W6740332996",
        "https://openalex.org/W2610147486",
        "https://openalex.org/W6762718338",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2460260369",
        "https://openalex.org/W2994673210",
        "https://openalex.org/W2963732700",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W2137278143",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2964217532",
        "https://openalex.org/W2564998703",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W3094664776",
        "https://openalex.org/W2975357369",
        "https://openalex.org/W2938619957",
        "https://openalex.org/W2962835968",
        "https://openalex.org/W2582761847",
        "https://openalex.org/W2889658408",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2964040697",
        "https://openalex.org/W2916743882",
        "https://openalex.org/W2754124089",
        "https://openalex.org/W2949302831",
        "https://openalex.org/W3103005696",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W2963773230",
        "https://openalex.org/W2772283977",
        "https://openalex.org/W2889986507",
        "https://openalex.org/W2964218467"
    ],
    "abstract": "In this paper we introduce a Transformer-based approach to video object segmentation (VOS). To address compounding error and scalability issues of prior work, we propose a scalable, end-to-end method for VOS called Sparse Spatiotemporal Transformers (SST). SST extracts per-pixel representations for each object in a video using sparse attention over spatiotemporal features. Our attention-based formulation for VOS allows a model to learn to attend over a history of multiple frames and provides suitable inductive bias for performing correspondence-like computations necessary for solving motion segmentation. We demonstrate the effectiveness of attention-based over recurrent networks in the spatiotemporal domain. Our method achieves competitive results on YouTube-VOS and DAVIS 2017 with improved scalability and robustness to occlusions compared with the state of the art. Code is available at https://github.com/dukebw/SSTVOS.",
    "full_text": "SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation\nBrendan Duke1,4* Abdalla Ahmed4 Christian Wolf3 Parham Aarabi1,4 Graham W. Taylor2,5\n1University of Toronto 2University of Guelph 3Universit´e de Lyon, INSA-Lyon, LIRIS\n4Modiface, Inc. 5Vector Institute\nAbstract\nIn this paper we introduce a Transformer-based approach\nto video object segmentation (VOS). To address compound-\ning error and scalability issues of prior work, we propose\na scalable, end-to-end method for VOS called Sparse Spa-\ntiotemporal Transformers (SST). SST extracts per-pixel rep-\nresentations for each object in a video using sparse atten-\ntion over spatiotemporal features. Our attention-based for-\nmulation for VOS allows a model to learn to attend over a\nhistory of multiple frames and provides suitable inductive\nbias for performing correspondence-like computations nec-\nessary for solving motion segmentation. We demonstrate\nthe effectiveness of attention-based over recurrent networks\nin the spatiotemporal domain. Our method achieves com-\npetitive results on YouTube-VOS and DAVIS 2017 with im-\nproved scalability and robustness to occlusions compared\nwith the state of the art. Code is available athttps:\n//github.com/dukebw/SSTVOS.\n1. Introduction\nVideo object segmentation (VOS) involves simultaneous\ntracking and segmentation of one or more objects through-\nout a video clip. VOS is a challenging task in which al-\ngorithms must overcome object appearance changes, occlu-\nsion and disocclusion, as well as distinguish similar objects\nin motion over time.\nA highly performant VOS system is important in down-\nstream tracking applications where pixelwise tracking in-\nformation is useful, such as player tracking in sports ana-\nlytics, person tracking in security footage, and car and road\nobstacle tracking in self-driving vehicle applications. VOS\nmethods are also relevant in interactive annotation of video\ndata, where annotator time can be used more efﬁciently by\nusing automatic video object segmentation in an annotate-\npredict-reﬁne loop. Our work uses VOS as a proxy task to\ninvestigate scalable algorithms for extracting spatiotempo-\n*Corresponding Author: brendanw.duke@gmail.com\nFigure 1: We propose a Transformer-based model for video\nobject segmentation featuring self-attention over time and\nover space. To segment an output frame, the model learns\nto look up similar regions in the temporal history and to\nsearch for reference masks. We address the high computa-\ntional complexity of the problem with a sparse Transformer\nformulation, which allows each cell to attend to each other\ncell over one or multiple hops. Here, interactions propagate\nfrom a given feature cell via our sparse spatiotemporal at-\ntention variants: (a) grid attention, and (b) strided attention.\nral representations from video in general, and these algo-\nrithms can be re-used for yet other video prediction tasks.\nPrevious methods that attempt to solve VOS can be di-\nvided into three major categories: online ﬁnetuning, mask\nreﬁnement, and temporal feature propagation. Each of these\ncategories, reviewed in detail in §2, has inherent drawbacks.\n1\narXiv:2101.08833v2  [cs.CV]  29 Mar 2021\nOnline ﬁnetuning methods cannot adapt to changes in ob-\nject appearance throughout a sequence. The dominant mask\nreﬁnement and temporal feature propagation methods are\nrecurrent. Due to their sequential nature, recurrent methods\nfor VOS exhibit compounding error over time, and are not\nparallelizable across a single example.\nMotivated by the success of Transformer architectures\nin NLP (see §2) we propose a novel method for semi-\nsupervised VOS that overcomes the drawbacks of online\nﬁnetuning and sequential methods. Our method, Sparse\nSpatiotemporal Transformers (SST), processes videos in a\nsingle feedforward pass of an efﬁcient attention-based net-\nwork. At every layer of this net, each spatiotemporal feature\nvector simultaneously interacts with all other feature vec-\ntors in the video. SST does not require online ﬁnetuning,\nalthough it may beneﬁt from this practice at the cost of the\naforementioned runtime penalty. Furthermore, since SST is\nfeedforward, it avoids the compounding error issue inher-\nent in recurrent methods. Finally, SST is fully parallelizable\nacross a single example and can therefore take advantage of\nthe scalability of current and future compute architectures.\nApplying spatiotemporal attention operators to VOS\nraises two challenges: computational complexity and dis-\ntinguishing foreground objects. Na ¨ıve spatiotemporal at-\ntention is square in the dimensionality of the video feature\ntensor, i.e., O((THW )2C). We resolve this computational\ncomplexity issue with sparse attention operators, of which\nwe compare two promising candidates.\nSST reduces feature matching FLOPs by an order of\nmagnitude, and achieves an overall score of 81.8 on\nthe ofﬁcial YouTube-VOS 2019 validation set, comparing\nfavourably with prior work. Furthermore, we observed\nqualitatively improved robustness to occlusions using SST’s\ntemporal buffer of preceding frame embeddings.\nContributions — We propose a Transformer-based\nmodel for VOS, and link its inductive bias to correspon-\ndence calculations. While there is work on Transformers for\nrepresentation learning in video [28, 39], these models at-\ntend over time and not densely over space. There is also re-\ncent work that adapts Transformers to video action recogni-\ntion [14], however, we are unaware of work that uses Trans-\nformers in VOS, which requires dense predictions. We also\ncontribute empirical evaluation of Transformer models ap-\nplied to VOS, and argue superiority over recurrent models.\nWe address computational complexity using sparse at-\ntention operator variants, making it possible to apply self-\nattention on high-resolution videos. We extend sparse at-\ntention variants to video so that they can be used for VOS.\nSpeciﬁcally we extend to 3D, with two spatial axes and one\ntemporal axis, Criss-Cross Attention [18] from 2D semantic\nsegmentation, and Sparse Attention [6] from 1D language\ntranslation. Our sparse video attention operators are not\nVOS speciﬁc, and could be applied to other dense video\nprediction tasks. We provide our implementation [10, 11].\n2. Related Work\nOur work is related to previous efforts in VOS. We are mo-\ntivated by work on Transformer architectures in language\ntranslation, as well as by orthogonal work on correspon-\ndence matching in computer vision.\nVideo Object Segmentation — In the VOS literature,\nonline ﬁnetuning approaches [2, 3, 31, 47, 15, 23, 26, 29]\ndo one-shot, or semi-supervised, VOS by ﬁnetuning a se-\nmantic segmentation network on an initial frame. A num-\nber of methods [4, 3, 31, 47] performed VOS on indepen-\ndent frames using ﬁnetuning alone without explicitly mod-\neling temporal coherence. Maninis et al. [31] built on the\noriginal usage of this approach by Caelles et al. [3] by\nadding instance-level semantic information, while V oigt-\nlaender and Leibe [47] added adaptive training during the\nsequence. Ofﬂine methods must use temporal information\nto produce future segmentations from past frames, as done\nusing optical ﬂow by Jang and Kim [20] and Tsai et al. [43].\nOur method is in principle able to take advantage of online\nﬁnetuning to improve performance, and also performs com-\npetitively using ofﬂine training alone.\nResearch into temporal coherence in VOS splits into two\ncategories: approaches that reﬁne masks, and those that\npropagate temporal features.\nMask reﬁnement approaches [23, 33, 23, 16, 20] reﬁne\na previous mask using feedforward models. Early work [23]\nimplemented mask reﬁnement by a recurrent connection on\nthe concatenated frame t−1 output masks and frametRGB\ninputs where the recurrent connection is a VGG [38] net-\nwork. An extension concatenated the feature map from the\nﬁrst frame [33]. Yang et al. [53] used a spatial prior on\nthe target location, with a channel-wise attention mecha-\nnism and meta-learning to adapt the network to the object\ngiven in the ﬁrst frame. Bao et al. [2] propagated masks by\napproximate inference in an MRF, with temporal dependen-\ncies based on optical ﬂow, and spatial dependencies using a\nCNN. Optical ﬂow has also been used to add motion infor-\nmation via jointly training optical ﬂow and VOS [5, 12, 15].\nTemporal feature propagation approaches [42, 51, 16,\n36, 19, 21] improve upon mask reﬁnement by increasing the\nexpressive power of the mask feature representation. At the\ntime of writing, all such methods have used RNNs to encode\nand propagate spatiotemporal representations through time.\nOur approach falls under the temporal feature propagation\ncategory. We use sparse attention operators to propagate\nfeatures temporally in a single feedforward operation.\nFEELVOS [46] is a simple and fast method for solv-\ning the VOS problem. Unlike most other VOS methods,\nFEELVOS trains end-to-end using a pixel-wise embedding.\nFEELVOS also uses global and local matching to the ref-\nerence and previous frames to predict masks for the video\n2\nsequence. Our work shares similarities with FEELVOS in\nthat both methods are end-to-end trainable and conceptually\nsimple. Our method has the added advantages of simultane-\nously extracting features from multiple frames using atten-\ntion, and using positional encodings to learn spatiotemporal\nposition-aware representations.\nSelf-attention and Correspondence Matching — End-\nto-end attention networks known as Transformers [44] are\na dominant current approach to a multitude of text natu-\nral language processing (NLP) tasks [8, 7], vision and lan-\nguage [40] as well as speech recognition tasks [30, 24]. Re-\ncent work has explored ties between attention heads and dif-\nferent reasoning functions [49, 22]. More recently, Trans-\nformers have also been applied in computer vision with suc-\ncess [50, 13, 55, 1]. In the context of VOS, we argue that\nself-attention also has the potential to overcome the short-\ncomings of the traditionally used recurrent methods [45].\nRNNs and variants are based on a Markovian assumption,\nwhere a ﬂat vectorial hidden state is propagated over time\nthrough the sequence. Our Transformer based model takes\na history of several frames and reference or predicted masks\nas input and allows each output region to attend to any re-\ngion in the input history. This makes the propagated repre-\nsentation inherently structured.\n3. Method\nOur proposed method for VOS consists of propagating a\nhistory of τ frames over the video sequence, and allowing\nthe model to perform spatio-temporal attention inside this\nhistory. We argue, that the proposed high-level SST ar-\nchitecture (§3) provides inductive bias well suited for the\nreasoning skills required for VOS, namely computing opti-\ncal ﬂow (attending to past similar frames) and propagating\nreference masks over time (attending to given frames with\nsimilar appearance). We solve the challenge of computa-\ntion complexity with two variants of sparse spatiotemporal\nattention, the “grid” and “strided” modules (§3).\nSST Architecture — The canonical text-based Trans-\nformer architecture [44] from which we drew motivation\nbears both similarities and differences with our VOS archi-\ntecture. Like NLP Transformers, SST consists of a hierar-\nchy of self-attention layers that form an encoder. In contrast\nto the encoder of an NLP Transformer, which takes as input\nembeddings extracted from a text sequence, SST’s encoder\ninput consists of embeddings extracted from the frames of\nthe video to segment. As in NLP Transformers, the SST\nencoder’s output feeds into a decoder. However, SST’s de-\ncoder is unlike NLP Transformer decoders, which consist of\ncross-attention layers that take the output sequence embed-\ndings as input. Instead, SST’s decoder is a generic convo-\nlutional segmentation network that takes as input a concate-\nnated set of features: current frame embeddings, attention\nvalues produced from SST’s encoder’s hierarchy of atten-\ntion maps, and embedding output by SST’s encoder. For\nthe purpose of fair comparison with state-of-the-art work,\nin §4 we use CFBI’s decoder module [54].\nThe SST architecture (Fig. 2) takes a length T sequence\nof H ×W RGB video frames S ∈R3×T×H×W as input.\nFrom S a CNN feature extractor f extracts\nT = f(S), (1)\na per-frame embedding T ∈RC×T×H′×W′\nat reduced spa-\ntial resolution H′×W′and with embedding channels C. In\nour experiments we used a ResNet-101 as f.\nIn order to meet hardware resource constraints, and sup-\nposing that a given frame’s relation to past frames decreases\nwith time, any given frame embedding attends to a tempo-\nral buffer of its τ preceding frame embeddings. Denote the\ntruncated frame embedding buffer by Tτ. We optionally\nadd information about the spatiotemporal position of cells\nin a video tensor by the positional encoding sum\n˜T = Tτ + P (2)\nwhere P ∈RC×T×H′×W′\nencodes absolute position. We\ncan encode absolute position P using various priors, such\nas sinusoids or learned embeddings [44], or as a zero tensor\nin the case of no positional encoding.\nThe SST encoder processes positionally encoded per-\nframe embeddings ˜T with L self-attention layers, adding\ntemporal context to the video representation. The SST en-\ncoder passes two outputs to the SST decoder, the ﬁrst of\nwhich is spatiotemporal features ˜TL ∈RC×τ×H′×W′\n. A\ncomposition of Llayers gl computes features ˜TL as\n˜TL = gL ◦gL−1 ◦···◦ g1(˜T). (3)\nEach encoder layer gl consists of a sequence of multi-\nhead attention and spatiotemporal position-wise feedfor-\nward components combined with skip connections and nor-\nmalization (Fig. 2). The output ˜TL of the SST encoder\nfeeds directly into the decoder as the representation con-\ntaining spatiotemporal context.\nThe SST decoder’s other input arises as an intermediate\ntensor computed by the multi-head attention component of\neach encoder layer. Each sparse multi-head attention opera-\ntion computes an attention map, which we refer to as an ob-\nject afﬁnity tensor. The role of the object afﬁnity tensor is to\npropagate segmentation information from past frames (ei-\nther reference or predicted) to the future using the attention\ndistributions of the Transformer heads. This can be seen as\ninductive bias for the model allowing it to more easily tie\nattention to semantically relevant motion. Key to the pro-\ncedure are tensors Io\np, which correspond to the pixels in the\nsparse connectivity pattern of feature cell p, and which be-\nlong to object o. Connectivity pattern Ip determines which\nother feature cells are “connected to” and thus interact with\n3\nFigure 2: We propagate a history of τ frames over a video sequence and perform spatio-temporal self attention as a suitable\nbias for video object segmentation, allowing the model to attend to previous video frames for optical-ﬂow like calculation,\nand to attend to reference frames. Computational complexity is addressed through two different sparse variants.\nfeature cell p (Fig. 1). The SST encoder uses connectiv-\nity patterns to compute the decoder’s second input: object\nafﬁnity values.\nObject afﬁnity values Av ∈RL×O×τ×H′×W′\nrepresent\nthe afﬁnity of each of theτ×H′×W′cells in the spatiotem-\nporal feature tensor with each of the Ogeneralized objects\n— all reference objects plus the background. Each object\nafﬁnity value Al\nv(p) is calculated as the maximum attention\nscore in the object afﬁnity tensor Al ∈ R|Ip|×τ×H′×W′\n,\ni.e., the score of the pixel belonging to object o and most\nattended to by the attention head. Each of the Lattention\nlayers in the SST encoder computes its own object afﬁn-\nity value using a reduction operation over its object afﬁnity\ntensor (Fig. 2). To enforce causality, feature cell p’s object\nafﬁnity is computed only over previous timesteps. Further-\nmore, each feature cell p attends only to feature cells in its\nconnectivity pattern Ip. We deﬁne object afﬁnity values as\nAl\nv(p) = max\nIop∪{0}\nAl (4)\nwhere Io\np denotes the (possibly empty) set of feature cells\nbelonging to object o, in connectivity pattern Ip, and de-\nfaulting to zero. By taking the Tth temporal slice of object\nafﬁnity values Av we obtain object-discriminative features\nused to infer the current frame object segmentation.\nDue to the form of the multi-head attention computation\ndescribed in §3, attention maps Al contain pairwise dot\nproducts between feature cells and other feature cells\nwithin their respective connectivity patterns. From these\ndot products we can compute Euclidean distance or normal-\nized cross correlation. Intuitively, by doing so we use the\nattention map features to compare the distances or angles\nbetween per-frame embeddings in the afﬁne subspaces pro-\njected to by each attention head. Taking all attention heads\nin the encoder together forms a hierarchy of such distance\n(or angle) features. This improves the expressiveness of the\nmodel compared with straightforward Euclidean distance\nbetween the per-frame embeddings T from Equation 1.\nThe SST decoder (Fig. 2) is a convolutional decoder\nmodule that takes the spatiotemporal context features ˜TL\nand object afﬁnity features A1..L\nv of all encoder layers as in-\nput. The ﬁnal layer of the SST decoder produces the video\nobject segmentation probability or masksY ∈RH×W from\nthe ﬁnal object-discriminative representation for a given\nframe. It applies a scoring convolution followed by sigmoid\nat training time or argmax at test time. In the case of mul-\ntiple objects we have probability scoremaps in RO×H×W,\ni.e., probabilities for each generalized object (including\nbackground) for each pixel in the video. An inference pro-\ntocol reduces these scoremaps to a tensor in Rτ×H×W of\nobject integer labels. We use the “na ¨ıve” inference proto-\ncol [33] and take, for each pixel, the argmax over all object\nprobabilities including the background probability.\nSparse Attention — In this section we use T to denote\na generic temporal dimension, but as described in §3, we\nactually operate on a reduced sized buffer of length τ.\nAttention is a dense operator that allows each element of\na tensor to interact with all other elements at each attention\nlayer. In VOS, attention can capture long-range dependen-\ncies without recurrence, and can be viewed intuitively as a\ncross-correlation operator that uses CNN features for cor-\nrespondence [27], similar to prior work that used matching\nlayers for optical ﬂow [9].\nFormally, we follow [44] in deﬁning attention as\nAttention\n(\nQ,K,V\n)\n= softmax\n(\nQK⊺)\nV, (5)\nwhere query Q, key K, and value V are all matrices\nin RS×C for ﬂattened spatiotemporal dimensions S =\nTHW . As we alluded to in §3, we use spatiotemporal fea-\ntures T as query, key, and value, i.e., we do self-attention.\nIntuitively we increase the spatiotemporal context of each\n4\nfeature cell p by doing a lookup in the spatiotemporal fea-\ntures connected to p.\nWe adapted for VOS characteristic components of\nthe Transformer architecture as described by Vaswani et\nal. [44], including multi-head attention and positional en-\ncodings. We compare the effectiveness of positional encod-\ning schemes applied to VOS in §4. We did not normalize the\nsoftmax argument in Equation 5 by the inverse square root\nof channels, as we found this scaling factor reduced model\neffectiveness. The difference in impact of scaling factor be-\ntween our VOS attention and Vaswani et al.’s NLP attention\ncould be due to our attention operator having a compara-\ntively low number of channels.\nA computational barrier prevents na ¨ıvely using Equa-\ntion 5 to perform our desired self-attention operation on\nspatiotemporal features T. The attention operation given\nin Equation 5 is O((THW )2C), which poses a problem for\nvideo object segmentation since for dense prediction tasks\nsuch as segmentation, model performance tends to improve\nwith greater input resolution [56]. As an illustration of the\ninfeasibility of using na¨ıve attention for VOS, consider that\na single layer of attention on a16 frame video with a64×64\nfeature map with 32 channels would cost more than137 bil-\nlion FLOPs, far more than the most computationally expen-\nsive CNNs in the literature at the time of writing [41].\nWe propose to use sparse spatiotemporal attention op-\nerators to overcome this computational barrier to applying\nattention for VOS. We deﬁne sparse attention operators us-\ning a connectivity pattern set I = {Ip0 ,...,I pS }where Ip\nis a set of coordinates (i,j,k ) that index a 3D tensor. Here\nagain, connectivity pattern Ip determines which other fea-\nture cells interact with feature cell p.\nFor query Q, key K, and value V tensors all\nin RC×T×H×W, a sparse attention operator is deﬁned as\nSparseAttn\n(\nQ,K,V)p = softmax\n(\nQpK⊺\nIp\n)\nVIp. (6)\nWe adapt two different sparse attention methods from 1D\nor 2D to 3D to make our attention operator computation-\nally tractable at our desired framerate and resolution. We\nachieve computational tractability by careful selection of\nthe connectivity patterns of our sparse attention operators.\nGrid Attention — We adapted our ﬁrst sparse attention\noperator from Huang et al., who also noted the computa-\ntional complexity issue when applying attention for seman-\ntic segmentation [18]. In VOS, however, the computational\ncomplexity issue is exacerbated by the addition of the time\ndimension. We refer to the generalized operator described\nbelow as “grid attention” since the moniker “criss-cross at-\ntention” is no longer ﬁtting in more than two dimensions.\nAt each layer of grid attention, each feature cell of the\nspatiotemporal feature tensor aggregates information from\nother feature cells along its X, Y, and T axes indepen-\ndently. Each feature cell interacts once with every other\nfeature cell in the spatiotemporal feature tensor that shares\nat least two of its X, Y, and T coordinates.\nGrid attention implements Equation 6 with a connectiv-\nity pattern Ip consisting of (T + H + W −2) feature cell\nindices. Ip contains all feature cells along the horizontal,\nvertical, or temporal axes incident to location p ≡(x,y,t ).\nThe grid attention weights softmax\n(\nQpK⊺\nIp\n)\nare then a\nmatrix in RS×(T+H+W−2), each row of which contains\nweights of a convex combination. For a feature cellp in the\nspatiotemporal feature tensor, the grid attention weights are\nover all feature cells along p’s temporal, vertical, and hor-\nizontal axes. By multiplying by the grid attention weights\nwe attend over p’s grid connectivity pattern. Note that we\nimplemented our grid attention operator in place, so we in-\ncur no overhead from indexing tensors by p.\nIn Figure 1 (top) we illustrate how grid attention propa-\ngates interactions from a single attended feature cell to all\nother feature cells in three sequential layers. The ﬁrst grid\nattention layer propagates information to other feature cells\nin the same frame vertically and horizontally, and to fea-\nture cells at the same spatial location in all other frames.\nThe second layer propagates interactions to the entire cur-\nrent frame, and vertical and horizontal axes in other frames.\nFinally, the third layer propagates information to all feature\ncells in the video feature tensor.\nWe can show that composing three applications of grid\nself-attention on spatiotemporal feature tensor T produces\nan output tensor where each spatiotemporal feature cell with\ncoordinates (x,y,t ) is composed of a weighted sum\nW∑\ni=1\nH∑\nj=1\nT∑\nk=1\n(\nT⊺\nxytTiyt\n)(\nT⊺\niytTijt\n)(\nT⊺\nijtTijk\n)\nTijk (7)\nover other feature cells in T with coordinates (i,j,k ).\nEquation 10 shows that grid attention propagates informa-\ntion along “routes” through the spatiotemporal feature ten-\nsor. For a feature cell at position (x,y,t ) to interact with\nanother feature cell at an arbitrary position (i,j,k ), inter-\nactions must propagate along a “route” composed of pairs\nof similar feature cells. Just as we might give travel di-\nrections through a city grid such as “ﬁrst walk ten blocks\nNorth, then walk three blocks East”, grid attention inter-\nactions must propagate a ﬁxed number of feature cells in\nthe X, Y, and T directions, in some order, before connect-\ning the source feature cell with its target feature cell.\nBy replacing dense attention with grid attention we\nreduced the computational complexity of video attention\nfrom O(C(THW )2) to O(C(T+H+W)THW ), achiev-\ning our goal of making attention tractable for video.\nStrided Attention — We investigated strided attention\nas an alternative sparse attention method in addition to grid\nattention. Drawing inspiration from sparse Transformers for\nsequences [6], information propagates by following paths\n5\nModel MACs\n(GFLOPs)\nSlowdown\n(%)\nParams\n(M)\nDeepLab-v3 255.4 - 66.5\nMatching (CFBI) 99.6 39.0 0\nSST (Local) 5.34 2.1 0.3\nSST (Strided) 1.89 0.7 0.3\nSST (Grid) 1.45 0.6 0.3\nNa¨ıve Attention 170.1 66.6 0.3\nTable 1: Runtime and parameter analysis.\nof locations through sparse connectivity patterns in the spa-\ntiotemporal feature tensor.\nWe deﬁne strided attention’s connectivity pattern Ip as\na generalization of Child et al.’s strided attention from 1D\nto 3D. Our strided attention uses two different connectiv-\nity patterns I1\np and I2\np corresponding to separate, sequen-\ntial heads of multihead attention. The ﬁrst connectivity pat-\ntern I1\np routes to all feature cells in a cube of side-length h\nfrom p, i.e., I1\np = (p + (lx,ly,lz) : lx,ly,lz < h). The\nsubsequent connectivity pattern I2\np routes to all feature cells\nin the video tensor that can reachp by taking steps of sizeh\nalong each axis, i.e., I2\np = (p + (lx,ly,lz) : lx,ly,lz\nmod h = 0). We choose h ≈\n√\nH to reduce the compu-\ntational complexity by a square root from O(C(THW )2)\nto O(C(THW )3/2). We visualize strided attention’s con-\nnectivity pattern in Figure 1 (bottom).\nThe relative efﬁciency of grid and strided attention de-\npends on the size of T, since we assume that H and W\nare both large relative to T. In a setup where H,W ∈\n{64,128}, and T ≈ 8, strided attention costs about 1.3\nto 1.4 times as many operations as grid attention.\nRuntime — Table 1 provides a runtime and parameter\nanalysis. We computed the multiply-accumulates (MACs)\nof SST for a 3-frame temporal buffer, input resolution\nof 465 ×465, 128 channels, and 3 Transformer layers. We\nshow MACs in both absolute GFLOPs and as slowdown rel-\native to DeepLab-v3 backbone MACs. We also compare to\nCFBI’s local/global matching. Note that SST’s local tem-\nporal window (τ = 3) is larger than CFBI’s ( τ = 1). Fi-\nnally, we compare to na ¨ıve attention. Both na¨ıve attention\nand CFBI’s global matching attend pairwise to an entire\nfeature map, costing 39.0% and 66.6% slowdown relative\nto DeepLab-v3’s runtime. In contrast, SST factorizes the\ncomputation by attending to all other spatiotemporal feature\ncells over sequential layers (Fig. 1). SST reduces slowdown\nby more than an order of magnitude to ≈2%.\n4. Experiments and Results\nWe present benchmark experiment results against state-\nof-the-art (SOTA) methods on the DA VIS 2017 [35] and\nYouTube-VOS [52] datasets. We further analyze the effect\nMethod O-Ft S G\n(%)\nJseen\n(%)\nJunseen\n(%)\nYouTube-VOS 2018 Validation Split\nMSK [23] \u0013 \u0017 53.1 59.9 45.0\nOnA VOS [47] \u0013 \u0017 55.2 60.1 46.6\nOSVOS [3] \u0013 \u0017 58.8 59.8 54.2\nS2S [51] \u0013 \u0017 64.4 71.0 55.5\nPReMVOS [29] \u0013 \u0017 66.9 71.4 56.5\nBoLTVOS [48] \u0013 \u0017 71.1 71.6 64.3\nRGMP [33] \u0017 \u0013 53.8 59.5 45.2\nSTM [32] \u0017 \u0013 79.4 79.7 72.8\nKMN [37] \u0017 \u0013 81.4 81.4 75.3\nSTM−[32] \u0017 \u0017 68.2 - -\nOSMN [53] \u0017 \u0017 51.2 60.0 40.6\nS2S [51] \u0017 \u0017 57.6 66.7 48.2\nA-GAME [21] \u0017 \u0017 66.0 66.9 61.2\nCFBI [54] \u0017 \u0017 81.4 81.1 75.3\nSST (Local) \u0017 \u0017 81.7 81.2 76.0\nYouTube-VOS 2019 Validation Split\nCFBI [54] \u0017 \u0017 81.0 80.0 77.9\nSST (Local) \u0017 \u0017 81.8 80.9 76.6\nTable 2: Comparison with SOTA methods on YouTube-\nVOS [52] 2018 and 2019. Region similarity over seen\n(Jseen) and unseen (Junseen) categories, and overall score G\nare computed as in standard benchmarks [34]. We distin-\nguish methods by those that use online ﬁnetuning (O-Ft)\nand/or synthetic data (S), and those that do not.\nof different sparse attention operators, history sizes and po-\nsitional encodings through ablation studies on DA VIS 2017.\nYouTube-VOS — is a large scale VOS dataset com-\nprised of 4453 YouTube video clips spanning94 object cat-\negories [52]. YouTube-VOS includes an ofﬁcial validation\nset of 507 videos with held out labels, which can be evalu-\nated only through an evaluation server. YouTube-VOS has\nbeen the basis of challenges in 2018 and 2019, yielding two\nversions of the validation set and evaluation server. The\n2019 version contains new and corrected annotations. The\nYouTube-VOS validation set contains 26 object categories\nthat are unique to the validation set, used to test the gener-\nalization capability of VOS models to object classes unseen\nin the training set. The convention is to compute region\nsimilarity Jand contour accuracy Fas deﬁned by Perazzi\net al. [34]. As a single metric for comparing results, it is\nalso customary to compute overall scoreGas the average of\nfour values comprising region similarity and contour accu-\nracy scores for seen and unseen classes.\nIn Table 2 we present our model’s results on YouTube-\nVOS 2018 and 2019 alongside previous SOTA results. Our\nmodel (SST) performs favourably against all previous meth-\nods in overall score G, even methods that use online ﬁne-\n6\nCFBI SST\n0% 25% 50% 75% 100%\nFigure 3: A qualitative example from YouTube-VOS validation showing SST handling occlusion. In this challenging exam-\nple, SST’s temporal history enables robust tracking of all three ﬁsh, while CFBI confuses two ﬁsh once they overlap.\nMethod O-Ft S J&F\n(%)\nJ\n(%)\nF\n(%)\nDAVIS 2017 Validation Split\nOSVOS-S [31] \u0013 \u0013 68.0 64.7 71.3\nOSVOS [3] \u0013 \u0017 60.3 56.6 63.9\nOnA VOS [47] \u0013 \u0017 65.4 61.6 69.1\nCINM [2] \u0013 \u0017 70.6 67.2 74.0\nPReMVOS [29] \u0013 \u0017 77.7 73.9 81.7\nRGMP [33] \u0017 \u0013 66.7 64.8 68.6\nSTM−[32] \u0017 \u0013 71.6 69.2 74.0\nSTM†[32] \u0017 \u0013 81.8 79.2 84.3\nKMN [37] \u0017 \u0013 76.0 74.2 77.8\nOSMN [53] \u0017 \u0017 54.8 52.5 57.1\nFA VOS [4] \u0017 \u0017 58.2 54.6 61.8\nVM [17] \u0017 \u0017 - 56.5 -\nDyeNet [26] \u0017 \u0017 69.1 67.3 71.0\nA-GAME†[21] \u0017 \u0017 70.0 67.2 72.7\nFEELVOS†[46] \u0017 \u0017 71.5 69.1 74.0\nCFBI [54] \u0017 \u0017 74.9 72.1 77.7\nCFBI†[54] \u0017 \u0017 81.9 79.3 84.5\nSST (Local) \u0017 \u0017 78.4 75.4 81.4\nSST (Local)† \u0017 \u0017 82.5 79.9 85.1\nTable 3: Comparison with SOTA methods on DA VIS\n2017 [35]. We denote online ﬁnetuning methods by O-Ft,\nand synthetic data methods by S. We report results trained\nonly on the DA VIS 2017 training set, and pre-trained on\nYouTube-VOS. YouTube-VOS pre-training is denoted by†.\ntuning (denoted by O-Ft) and pre-training on synthetic data\n(denoted by S). Note that our unique method performs\ncompetitively against recurrent methods that have under-\ngone multiple research and development cycles where one\nmethod builds on the foundation of another, for exam-\nple [21] extends [33], which in turn extends [23].\nDA VIS 2017 — is the latest dataset in the DA VIS ini-\ntiative to promote VOS research. DA VIS 2017 comprises\n150 sequences, which include 376 separately annotated ob-\njects [35]. We additionally evaluate our method on DA VIS\n2017 [35], and compare our results with SOTA in Table 3.\nWe report our DA VIS results following the traditionally\nused region similarity Jand contour accuracy Fmetrics as\nwell as their mean J&F. Our DA VIS 2017 evaluation pro-\nvides additional experimental evidence that SST performs\nfavourably compared with existing SOTA methods, since\nSST achieves a meanJ&Fscore of 78.4, whereas previous\nSOTA scored a J&Fof 74.9 under a comparable experi-\nmental setup (without online ﬁnetuning or synthetic data).\nWe evaluate only on DA VIS 2017 and not DA VIS\n2016 [34] because DA VIS 2017 is strictly a more challeng-\ning superset of DA VIS 2016. Furthermore DA VIS 2016\ncontains only single object annotations and therefore we\ncould make only limited evaluation of SST’s ability to han-\ndle multi-object context using DA VIS 2016.\nAblation Studies — Figure 3 shows a qualitative ex-\nample on the YouTube-VOS validation set of SST handling\nforeground occlusion, where one ﬁsh entirely occludes an-\nother before the second ﬁsh becomes disoccluded again.\nWe used DA VIS 2017 to perform ablation studies on in-\nteresting components of our method, including sparse at-\ntention operators, positional encodings, and temporal his-\ntory size. We ﬁrst describe and compare different design\nchoices for our positional encodings.\nWe investigated sinusoidal positional encodings for the\ntemporal dimension, as used in Transformers for language\ntranslation [44]. We hypothesized that sinusoidal positional\nencodings would be superior to learned positional embed-\ndings because of the data imbalance of temporal positions\nin VOS datasets, which are skewed towards lower frame\nnumbers. Sinusoidal positional encodings can be interpo-\nlated or extrapolated to generalize to underrepresented ab-\nsolute frame numbers, whereas positional embeddings have\nno such generalization mechanism.\nWe present our positional encoding results in Table 4 us-\n7\nPositional\nEncoding\nJ&F\n(%)\nNone 73 ±2\nSinusoidal 75.6 ±0.6\nTable 4: Positional encod-\nings on DA VIS 2017 val.\nτ J&F(%)\n1 75.8 ±0.3\n2 76.2 ±0.3\n3 76.5 ±0.3\nTable 5: Temporal histo-\nries τ on DA VIS 2017 val.\nSparse Attention Layers J&F J F\nGrid 1 65.3 62.3 68.4\nGrid 2 66.1 62.6 69.5\nGrid 3 64.2 61.0 67.4\nLocal 2 76.2 72.8 79.5\nStrided 2 69.5 65.7 73.3\nLocalStrided 2 72.3 69.1 75.6\nTable 6: SST sparse attention variants on DA VIS 2017 val.\ning the J&Fscore on the DA VIS 2017 validation set. The\npositional encoding labeled “None” is our baseline attention\nwith no positional information, while “Sinusoidal” uses si-\nnusoidal positional encodings for all spatiotemporal dimen-\nsions X, Y, and T. To evaluate robustness to hyperparam-\neters, we computed the mean and variance of J&Fover\nmultiple runs for each positional encoding scheme, varying\nthe number of Transformer layers and the temporal history.\nSinusoidal temporal positional encodings performed best,\nachieving both a higher mean score and lower variance. The\nsuperiority of positional encodings supports our hypothesis\nthat information about distance-from-reference is important\nin positional encodings for VOS. The lower variance in-\ndicates that sinusoidal positional encodings form a robust\nprior for ﬁnding correspondences in VOS.\nIn Table 5 we evaluate the effect of increasing SST’s\ntemporal history τ. We varied the temporal history while\nkeeping other hyperparameters ﬁxed, and computed the\nvariance over multiple runs. We observed that even a mod-\nest increase in temporal history improves the J&Fscore.\nWe expect that efﬁciency improvements further increasing\nthe temporal history size will improve the effectiveness of\nSST’s temporal history mechanism even further.\nIn Table 6 we compare the performance of SST using dif-\nferent sparse attention variants. We expected that increasing\nthe number of layers would improve grid attention’s perfor-\nmance due to the increasing receptive ﬁeld of each feature\ncell. A larger receptive ﬁeld should improve the effective-\nness of the object afﬁnity value from multi-head attention.\nGrid attention’sJ&Fscore increased as expected from one\nto two layers, but dropped off for three layers possibly due\nto overﬁtting. We also expected that local attention should\nbe effective for DA VIS 2017’s fast framerate compared to\nstrided attention, both of which we describe in §3. Local-\nStrided attention provides a tradeoff between the local and\nglobal context windows of local and strided attention, re-\nspectively. So that LocalStrided attention can attend tran-\nsitively to all feature cells in just two consecutive sparse\nattention layers, we set the kernel size equal to the square\nroot of the feature tensor width. For fair comparison, we\nkept the same kernel size for all strided attention variants.\nIn general, local and strided attention outperformed grid at-\ntention, showing that local and strided connectivity patterns\nform superior priors for VOS compared with grid attention.\nDiscussion — We present a method for VOS purely\nbased on end-to-end attention. Future work could be\nanalogous to Transformer models’ progression on lan-\nguage translation tasks, where researchers successfully ap-\nplied Transformers to increasingly long sequences. For\nexample, Dai et al. combined recurrence with attention\nto translate arbitrary-length sequences [7], and Kitaev et\nal. introduced locality-sensitive hashing instead of dot-\nproduct attention, to reduce computational complexity from\nsquared to O(Nlog N) while using reversible networks to\nmodel arbitrary-length sequences with constant memory us-\nage [25]. In order to evaluate VOS on long sequences the\nVOS community would have to overcome a dataset creation\nchallenge, since the current benchmark dataset YouTube-\nVOS contains sequences with at most 36 labeled frames,\nsampled at 6 fps. We propose that future work could use\ninteractive and semi-automatic annotation methods, based\non the existing high-quality VOS models, to create datasets\nwith longer and therefore more challenging sequences.\n5. Conclusions\nWe presented Sparse Spatiotemporal Transformers (SST),\nwhich, up to our knowledge, constitutes the ﬁrst application\nof an entirely attention-based model for video object seg-\nmentation (VOS). We evaluated the positive effect of posi-\ntional encodings and the advantage of attending over a his-\ntory of multiple frames, suggesting a superiority of a spa-\ntiotemporally structured representation over the ﬂat hidden\nrepresentation of recurrent models. We showed that SST\nis capable of state-of-the-art results on the benchmark VOS\ndataset YouTube-VOS, attaining an overall score of G =\n81.8, while having superior runtime scalability compared\nwith previous state of the art, including methods based on\nrecurrence. We provide code [11] to reproduce all experi-\nments in our work, including sparse video-attention opera-\ntor implementations [10], so that the community can build\non the promising idea of using attention-based models for\nvideo. Open challenges are the memory requirements in-\nherent in a model with only weak Markovian assumptions,\nwhich for the moment prevents the increase of history size\nto a desirable longer extent.\nAcknowledgements — C. Wolf acknowledges support\nfrom ANR through grant “ Remember” (ANR-20-CHIA-\n0018) of the call “AI chairs in research and teaching”.\n8\nReferences\n[1] Anonymous. An Image is Worth 16x16 Words: Transform-\ners for Image Recognition at Scale. In openreview ICLR\n2020 submissino, 2020. 3\n[2] Linchao Bao, Baoyuan Wu, and Wei Liu. CNN in MRF:\nvideo object segmentation via inference in A cnn-based\nhigher-order spatio-temporal MRF. In CVPR, 2018. 2, 7\n[3] Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset,\nLaura Leal-Taix´e, Daniel Cremers, and Luc Van Gool. One-\nshot video object segmentation. In CVPR, 2017. 2, 6, 7\n[4] Jingchun Cheng, Yi-Hsuan Tsai, Wei-Chih Hung, Shengjin\nWang, and Ming-Hsuan Yang. Fast and accurate online video\nobject segmentation via tracking parts. In CVPR, 2018. 2, 7\n[5] Jingchun Cheng, Yi-Hsuan Tsai, Shengjin Wang, and Ming-\nHsuan Yang. Segﬂow: Joint learning for video object seg-\nmentation and optical ﬂow. In ICCV, 2017. 2\n[6] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.\nGenerating long sequences with sparse transformers. In\narXiv:1904.10509, 2019. 2, 5\n[7] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell,\nQuoc Le, and Ruslan Salakhutdinov. Transformer-XL: At-\ntentive language models beyond a ﬁxed-length context. In\nProceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, 2019. 3, 8\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of the\n2019 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), 2019. 3\n[9] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip\nHausser, Caner Hazirbas, Vladimir Golkov, Patrick van der\nSmagt, Daniel Cremers, and Thomas Brox. Flownet: Learn-\ning optical ﬂow with convolutional networks. In The IEEE\nInternational Conference on Computer Vision (ICCV), De-\ncember 2015. 4\n[10] Brendan Duke. Sparse spatiotemporal transformer.\nhttps : / / github . com / dukebw / sparse -\nspatiotemporal-transformer, 2021. 2, 8\n[11] Brendan Duke. Sstvos. https://github.com/\ndukebw/SSTVOS, 2021. 2, 8\n[12] Suyog Dutt Jain, Bo Xiong, and Kristen Grauman. Fusion-\nseg: Learning to combine motion and appearance for fully\nautomatic segmentation of generic objects in videos. In\nCVPR, 2017. 2\n[13] R. Girdhar, J. Carreira, C. Doersch, and A. Zisserman. Video\nAction Transformer Network. In CVPR, 2020. 3\n[14] Rohit Girdhar, Jo ˜ao Carreira, Carl Doersch, and Andrew Zis-\nserman. Video action transformer network. In CVPR, 2019.\n2\n[15] Ping Hu, Gang Wang, Xiangfei Kong, Jason Kuen, and Yap-\nPeng Tan. Motion-guided cascaded reﬁnement network for\nvideo object segmentation. In CVPR, 2018. 2\n[16] Yuan-Ting Hu, Jia-Bin Huang, and Alexander Schwing.\nMaskrnn: Instance level video object segmentation. In Ad-\nvances in Neural Information Processing Systems 30, 2017.\n2\n[17] Yuan-Ting Hu, Jia-Bin Huang, and Alexander G. Schwing.\nVideomatch: Matching based video object segmentation. In\nECCV, 2018. 7\n[18] Zilong Huang, Xinggang Wang, Lichao Huang, Chang\nHuang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross\nattention for semantic segmentation. 2019. 2, 5\n[19] Varun Jampani, Raghudeep Gadde, and Peter V . Gehler.\nVideo propagation networks. In CVPR, 2017. 2\n[20] Won-Dong Jang and Chang-Su Kim. Online video object\nsegmentation via convolutional trident network. InThe IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2017. 2\n[21] Joakim Johnander, Martin Danelljan, Emil Brissman, Fa-\nhad Shahbaz Khan, and Michael Felsberg. A generative ap-\npearance model for end-to-end video object segmentation. In\nCVPR, 2019. 2, 6, 7\n[22] C. Kervadec, T. Jaunet, G. Antipov, M. Baccouche, R.\nVuillemot, and C. Wolf. How Transferrable are Reasoning\nPatterns in VQA? In CVPR, 2021. 3\n[23] Anna Khoreva, Federico Perazzi, Rodrigo Benenson, Bernt\nSchiele, and Alexander Sorkine-Hornung. Learning video\nobject segmentation from static images. In CVPR, 2017. 2,\n6, 7\n[24] Chanwoo Kim, Minkyu Shin, Abhinav Garg, and Dhanan-\njaya Gowda. Improved V ocal Tract Length Perturbation for\na State-of-the-Art End-to-End Speech Recognition System.\nIn Proc. Interspeech 2019, 2019. 3\n[25] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Re-\nformer: The efﬁcient transformer. In International Confer-\nence on Learning Representations, 2020. 8\n[26] Xiaoxiao Li and Chen Change Loy. Video object segmen-\ntation with joint re-identiﬁcation and attention-aware mask\npropagation. In The European Conference on Computer Vi-\nsion (ECCV), 2018. 2, 7\n[27] Jonathan L Long, Ning Zhang, and Trevor Darrell. Do con-\nvnets learn correspondence? In Advances in Neural Infor-\nmation Processing Systems 27, 2014. 4\n[28] J. Lu, D. Batra, D. Parikh, and S. Lee. ViLBERT: Pre-\ntraining Task-Agnostic Visiolinguistic Representations for\nVision-and-Language Tasks. In NeurIPS, 2019. 2\n[29] Jonathon Luiten, Paul V oigtlaender, and Bastian Leibe. Pre-\nmvos: Proposal-generation, reﬁnement and merging for\nvideo object segmentation. In Asian Conference on Com-\nputer Vision, 2018. 2, 6, 7\n[30] Christoph L ¨uscher, Eugen Beck, Kazuki Irie, Markus Kitza,\nWilfried Michel, Albert Zeyer, Ralf Schl ¨uter, and Hermann\nNey. RWTH ASR Systems for LibriSpeech: Hybrid vs At-\ntention. In Proc. Interspeech 2019, 2019. 3\n[31] Kevis-Kokitsi Maninis, Sergi Caelles, Yuhua Chen, Jordi\nPont-Tuset, Laura Leal-Taix´e, Daniel Cremers, and Luc Van\nGool. Video object segmentation without temporal informa-\ntion. 2018. 2, 7\n[32] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo\nKim. Video object segmentation using space-time memory\nnetworks. ICCV, 2019. 6, 7, 11, 12\n[33] Seoung Wug Oh, Joon-Young Lee, Kalyan Sunkavalli, and\nSeon Joo Kim. Fast video object segmentation by reference-\nguided mask propagation. In CVPR, 2018. 2, 4, 6, 7\n9\n[34] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M.\nGross, and A. Sorkine-Hornung. A benchmark dataset and\nevaluation methodology for video object segmentation. In\nComputer Vision and Pattern Recognition, 2016. 6, 7\n[35] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar-\nbel´aez, Alexander Sorkine-Hornung, and Luc Van Gool.\nThe 2017 davis challenge on video object segmentation.\narXiv:1704.00675, 2017. 6, 7\n[36] Amaia Salvador, Miriam Bellver, Manel Baradad, Ferran\nMarques, Jordi Torres, and Xavier Giro-i Nieto. Recur-\nrent neural networks for semantic instance segmentation. In\narXiv:1712.00617, 2017. 2\n[37] Hongje Seong, Junhyuk Hyun, and Euntai Kim. Kernelized\nmemory network for video object segmentation. In ECCV,\n2020. 6, 7\n[38] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for Large-Scale image recognition. In In-\nternational Conference on Learning Representations, 2015.\n2\n[39] C. Sun, F. Baradel, K. Murphy, and C. Schmid. Learn-\ning Video Representations using Contrastive Bidirectional\nTransformer. In arxiv:1906.05743, 2019. 2\n[40] Hao Tan and Mohit Bansal. LXMERT: Learning Cross-\nModality Encoder Representations from Transformers. In\nEMNLP-IJCNLP, 2019. 3\n[41] Mingxing Tan and Quoc Le. EfﬁcientNet: Rethinking model\nscaling for convolutional neural networks. In Proceedings\nof the 36th International Conference on Machine Learning,\n2019. 5\n[42] Pavel Tokmakov, Karteek Alahari, and Cordelia Schmid.\nLearning video object segmentation with visual memory. In\nICCV, 2017. 2\n[43] Yi-Hsuan Tsai, Ming-Hsuan Yang, and Michael J. Black.\nVideo segmentation via object ﬂow. In CVPR, 2016. 2\n[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems 30. 2017. 3, 4, 5, 7\n[45] Carles Ventura, Miriam Bellver, Andreu Girbau, Amaia Sal-\nvador, Ferran Marques, and Xavier Giro-i Nieto. Rvos: End-\nto-end recurrent network for video object segmentation. In\nCVPR, 2019. 3\n[46] Paul V oigtlaender, Yuning Chai, Florian Schroff, Hartwig\nAdam, Bastian Leibe, and Liang-Chieh Chen. Feelvos: Fast\nend-to-end embedding learning for video object segmenta-\ntion. In CVPR, 2019. 2, 7\n[47] Paul V oigtlaender and Bastian Leibe. Online adaptation of\nconvolutional neural networks for video object segmenta-\ntion. In BMVC, 2017. 2, 6, 7\n[48] Paul V oigtlaender, Jonathon Luiten, and Bastian Leibe.\nBoLTVOS: Box-Level Tracking for Video Object Segmen-\ntation. arXiv:1904.04552, 2019. 6\n[49] Elena V oita, David Talbot, Fedor Moiseev, Rico Sennrich,\nand Ivan Titov. Analyzing Multi-Head Self-Attention: Spe-\ncialized Heads Do the Heavy Lifting, the Rest Can Be\nPruned. In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, 2019. 3\n[50] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local Neu-\nral Networks. In CVPR, 2018. 3\n[51] Ning Xu, Linjie Yang, Yuchen Fan, Jianchao Yang,\nDingcheng Yue, Yuchen Liang, Brian Price, Scott Cohen,\nand Thomas Huang. Youtube-vos: Sequence-to-sequence\nvideo object segmentation. In ECCV, 2018. 2, 6\n[52] Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen\nLiang, Jianchao Yang, and Thomas S. Huang. Youtube-\nvos: A large-scale video object segmentation benchmark. In\nECCV, 2018. 6\n[53] Linjie Yang, Yanran Wang, Xuehan Xiong, Jianchao Yang,\nand Aggelos K. Katsaggelos. Efﬁcient video object segmen-\ntation via network modulation. In CVPR, 2018. 2, 6, 7\n[54] Zongxin Yang, Yunchao Wei, and Yi Yang. Collaborative\nvideo object segmentation by foreground-background inte-\ngration. In Proceedings of the European Conference on Com-\nputer Vision, 2020. 3, 6, 7, 11, 12\n[55] H. Zhao, J. Jia, and V . Koltun. Exploring Self-attention for\nImage Recognition . In CVPR, 2020. 3\n[56] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping\nShi, and Jiaya Jia. Icnet for real-time semantic segmentation\non high-resolution images. In ECCV, 2018. 5\n10\n6. Additional Results\nIn Figures 4 and 5, we compare SST qualitatively to\nCFBI [54] and STM [32]. SST produces superior tracking\nin these challenging sequences, which contain occlusions\nand disocclusions. The positional encoding in the Trans-\nformer representations may enable SST to distinguish sim-\nilar instances under occlusions, using positional informa-\ntion. Whereas CFBI confuses instances that are far apart,\nSST remains robust to these nonlocal failures. This further\nsupports the effectiveness of SST’s use of positional infor-\nmation.\n7. Grid Attention Routing\nTo demonstrate mathematically information propagation in\ngrid attention we consider, for sake of clarity, a sparse at-\ntention function\nSparseAttn\n(\nQ,K,V)p = QpK⊺\nIpVIp. (8)\nwhere we replace the softmax by an identity function. Fur-\nther assume that our query, key, and value all are our video\nfeature tensor, i.e., Q = T, K = T, and V = T. The ﬁrst\nlayer outputs, for each pixel p,\nGridAttn\n(\nQ,K,V)pxyt =\nW∑\ni=1\n(\nT⊺\nxytTiyt\n)\nTiyt+\nH∑\nj=1\nj̸=y\n(\nT⊺\nxytTxjt\n)\nTxjt+\nT∑\nk=1\nk̸=t\n(\nT⊺\nxytTxyk\n)\nTxyk.\n(9)\nWe can show that composing three applications of self-\nattention on T, which we denote for brevity as GridAttn3,\nproduces\nGridAttn3(\nT,T,T\n)\nxyt =\nW∑\ni=1\nH∑\nj=1\nT∑\nk=1\n(\nT⊺\nxytTiyt\n)\n(\nT⊺\niytTijt\n)\n(\nT⊺\nijtTijk\n)\nTijk + ··· ,\n(10)\nwhere ··· represents other similar third order terms. We\nshow in Equation 10 that grid attention propagates informa-\ntion along “routes” through the video feature tensor: for a\npixel at position (x,y,t ) to interact with another pixel at\nan arbitrary position (i,j,k ), interactions must propagate\nalong a “route” through the video feature tensor of pairs\nof similar pixels. Just as we might give travel directions\nthrough a city grid such as “ﬁrst walk ten blocks North, then\nwalk three blocks East”, grid attention interactions must\npropagate a ﬁxed number of pixels in the X, Y and T di-\nrections, in some order, before connecting the interaction\nsource pixel with its target pixel.\nConsider what happens if we replace the value Tijk re-\nturned by the inner cross-attention in Equation 10 with a\nforeground mask value mijk. We see that the output routes\nreference mask values mijk over paths of feature vectors in\nthe video tensor T that transitively correspond to reference\nfeatures Tijk.\n11\nCFBI SST\n0% 25% 50% 75% 100%\nSTM\nFigure 4: Fish tank. This challenging YouTube-VOS 2019 validation set example contains many occlusions and disocclusions\nby similar-looking instances of the same ﬁsh class. SST makes relatively few errors relatively later in the sequence when\ncompared with CFBI [54] or STM [32]. For clarity we labeled errors with yellow dotted boxes (best viewed digitally, with\nzoom and in colour).\nCFBI SST\n0% 25% 50% 75% 100%\nSTM\nFigure 5: Jazz band. In this YouTube-VOS 2019 validation set example the saxophone player self-occludes and disoccludes\ntheir saxophone while playing. SST maintains the correct saxophone segmentation throughout the sequence. In contrast,\nCFBI [54] and STM [32] confuse the saxophone with the saxophone player’s upper body after disocclusion.\n12"
}