{
  "title": "Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models",
  "url": "https://openalex.org/W2975185270",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4302098428",
      "name": "Lee, Cheolhyoung",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753220617",
      "name": "Cho, Kyunghyun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282079667",
      "name": "Kang, Wanmo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2963655672",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2740905737",
    "https://openalex.org/W2963850662",
    "https://openalex.org/W1826234144",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W35527955",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W806995027",
    "https://openalex.org/W1665214252",
    "https://openalex.org/W2626667877",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4919037",
    "https://openalex.org/W2158899491",
    "https://openalex.org/W2590796488",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2952238132",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2898700502",
    "https://openalex.org/W1686810756"
  ],
  "abstract": "In natural language processing, it has been observed recently that generalization could be greatly improved by finetuning a large-scale language model pretrained on a large unlabeled corpus. Despite its recent success and wide adoption, finetuning a large pretrained language model on a downstream task is prone to degenerate performance when there are only a small number of training instances available. In this paper, we introduce a new regularization technique, to which we refer as \"mixout\", motivated by dropout. Mixout stochastically mixes the parameters of two models. We show that our mixout technique regularizes learning to minimize the deviation from one of the two models and that the strength of regularization adapts along the optimization trajectory. We empirically evaluate the proposed mixout and its variants on finetuning a pretrained language model on downstream tasks. More specifically, we demonstrate that the stability of finetuning and the average accuracy greatly increase when we use the proposed approach to regularize finetuning of BERT on downstream tasks in GLUE.",
  "full_text": "Published as a conference paper at ICLR 2020\nMIXOUT : E FFECTIVE REGULARIZATION TO FINETUNE\nLARGE -SCALE PRETRAINED LANGUAGE MODELS\nCheolhyoung Lee∗\ncheolhyoung.lee@kaist.ac.kr\nKyunghyun Cho†‡§\nkyunghyun.cho@nyu.edu\nWanmo Kang∗\nwanmo.kang@kaist.ac.kr\nABSTRACT\nIn natural language processing, it has been observed recently that generalization\ncould be greatly improved by ﬁnetuning a large-scale language model pretrained\non a large unlabeled corpus. Despite its recent success and wide adoption, ﬁnetun-\ning a large pretrained language model on a downstream task is prone to degenerate\nperformance when there are only a small number of training instances available.\nIn this paper, we introduce a new regularization technique, to which we refer as\n“mixout”, motivated by dropout. Mixout stochastically mixes the parameters of\ntwo models. We show that our mixout technique regularizes learning to minimize\nthe deviation from one of the two models and that the strength of regularization\nadapts along the optimization trajectory. We empirically evaluate the proposed\nmixout and its variants on ﬁnetuning a pretrained language model on downstream\ntasks. More speciﬁcally, we demonstrate that the stability of ﬁnetuning and the av-\nerage accuracy greatly increase when we use the proposed approach to regularize\nﬁnetuning of BERT on downstream tasks in GLUE.\n1 I NTRODUCTION\nTransfer learning has been widely used for the tasks in natural language processing (NLP) (Collobert\net al., 2011; Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Phang et al., 2018). In particu-\nlar, Devlin et al. (2018) recently demonstrated the effectiveness of ﬁnetuning a large-scale language\nmodel pretrained on a large, unannotated corpus on a wide range of NLP tasks including question an-\nswering and language inference. They have designed two variants of models, BERTLARGE (340M\nparameters) and BERTBASE (110M parameters). Although BERTLARGE outperforms BERTBASE\ngenerally, it was observed that ﬁnetuning sometimes fails when a target dataset has fewer than 10,000\ntraining instances (Devlin et al., 2018; Phang et al., 2018).\nWhen ﬁnetuning a big, pretrained language model, dropout (Srivastava et al., 2014) has been used\nas a regularization technique to prevent co-adaptation of neurons (Vaswani et al., 2017; Devlin et al.,\n2018; Yang et al., 2019). We provide a theoretical understanding of dropout and its variants, such\nas Gaussian dropout (Wang & Manning, 2013), variational dropout (Kingma et al., 2015), and drop-\nconnect (Wan et al., 2013), as an adaptive L2-penalty toward the origin (all zero parameters 0) and\ngeneralize dropout by considering a target model parameter u (instead of the origin), to which we\nrefer as mixout(u). We illustrate mixout(u) in Figure 1. To be speciﬁc, mixout(u) replaces\nall outgoing parameters from a randomly selected neuron to the corresponding parameters of u.\nmixout(u) avoids optimization from diverging away from u through an adaptive L2-penalty to-\nward u. Unlike mixout(u), dropout encourages a move toward the origin which deviates away\nfrom u since dropout is equivalent to mixout(0).\nWe conduct experiments empirically validating the effectiveness of the proposed mixout(wpre)\nwhere wpre denotes a pretrained model parameter. To validate our theoretical ﬁndings, we train\na fully connected network on EMNIST Digits (Cohen et al., 2017) and ﬁnetune it on MNIST. We\nobserve that a ﬁnetuning solution of mixout(wpre) deviates less from wpre in the L2-sense than\n∗Department of Mathematical Sciences, KAIST, Daejeon, 34141, Republic of Korea\n†New York University\n‡Facebook AI Research\n§CIFAR Azrieli Global Scholar\n1\narXiv:1909.11299v2  [cs.LG]  23 Jan 2020\nPublished as a conference paper at ICLR 2020\n(a) Vanilla network at u\n (b) Dropout network at w\n (c) mixout(u) network at w\nFigure 1: Illustration of mixout(u). Suppose that u and w are a target model parameter and a\ncurrent model parameter, respectively. (a): We ﬁrst memorize the parameters of the vanilla network\nat u. (b): In the dropout network, we randomly choose an input neuron to be dropped (a dotted\nneuron) with a probability of p. That is, all outgoing parameters from the dropped neuron are\neliminated (dotted connections). (c): In the mixout(u) network, the eliminated parameters in (b)\nare replaced by the corresponding parameters in (a). In other words, the mixout(u) network at w\nis the mixture of the vanilla network at u and the dropout network at w with a probability of p.\nthat of dropout. In the main experiment, we ﬁnetune BERTLARGE with mixout(wpre) on small\ntraining sets of GLUE (Wang et al., 2018). We observe that mixout(wpre) reduces the number\nof unusable models that fail with the chance-level accuracy and increases the average development\n(dev) scores for all tasks. In the ablation studies, we perform the following three experiments for\nﬁnetuning BERTLARGE with mixout(wpre): (i) the effect of mixout(wpre) on a sufﬁcient num-\nber of training examples, (ii) the effect of a regularization technique for an additional output layer\nwhich is not pretrained, and (iii) the effect of probability of mixout(wpre) compared to dropout.\nFrom these ablation studies, we observe that three characteristics of mixout(wpre): (i) ﬁnetuning\nwith mixout(wpre) does not harm model performance even with a sufﬁcient number of training\nexamples; (ii) It is beneﬁcial to use a variant of mixout as a regularization technique for the addi-\ntional output layer; (iii) The proposed mixout(wpre) is helpful to the average dev score and to the\nﬁnetuning stability in a wider range of its hyperparameter pthan dropout.\n1.1 R ELATED WORK\nFor large-scale pretrained language models (Vaswani et al., 2017; Devlin et al., 2018; Yang et al.,\n2019), dropout has been used as one of several regularization techniques. The theoretical analysis\nfor dropout as an L2-regularizer toward 0 was explored by Wan et al. (2013) where 0 is the ori-\ngin. They provided a sharp characterization of dropout for a simpliﬁed setting (generalized linear\nmodel). Mianjy & Arora (2019) gave a formal and complete characterization of dropout in deep\nlinear networks with squared loss as a nuclear norm regularization toward 0. However, neither Wan\net al. (2013) nor Mianjy & Arora (2019) gives theoretical analysis for the extension of dropout which\nuses a point other than 0.\nWiese et al. (2017), Kirkpatrick et al. (2017), and Schwarz et al. (2018) used L2-penalty toward\na pretrained model parameter to improve performance. They focused on preventing catastrophic\nforgetting to enable their models to learn multiple tasks sequentially. They however do not discuss\nnor demonstrate the effect of L2-penalty toward the pretrained model parameter on the stability of\nﬁnetuning. Barone et al. (2017) introduced tuneout, which is a special case of mixout. They applied\nvarious regularization techniques including dropout, tuneout, and L2-penalty toward a pretrained\nmodel parameter to ﬁnetune neural machine translation. They however do not demonstrate empirical\nsigniﬁcance of tuneout compared to other regularization techniques nor its theoretical justiﬁcation.\n2 P RELIMINARIES AND NOTATIONS\nNorms and Loss Functions Unless explicitly stated, a norm ∥·∥ refers to L2-norm. A loss\nfunction of a neural network is written as L(w) = 1\nn\n∑n\ni=1 Li(w), where w is a trainable model\nparameter. Li is “a per-example loss function” computed on thei-th data point.\n2\nPublished as a conference paper at ICLR 2020\nStrong Convexity A differentiable function f is strongly convex if there exists m> 0 such that\nf(y) ≥f(x) + ∇f(x)⊤(y −x) + m\n2 ∥y −x∥2, (1)\nfor all x and y.\nWeight Decay We refer as “wdecay(u, λ)” to minimizing\nL(w) + λ\n2 ∥w −u∥2,\ninstead of the original loss function L(w) where λ is a regularization coefﬁcient. Usual weight\ndecay of λis equivalent to wdecay(0, λ).\nProbability for Dropout and DropconnectDropout (Srivastava et al., 2014) is a regularization\ntechnique selecting a neuron to drop with a probability ofp. Dropconnect (Wan et al., 2013) chooses\na parameter to drop with a probability of p. To emphasize their hyperparameter p, we write dropout\nand dropconnect with a drop probability of pas “dropout(p)” and “dropconnect(p)”, respectively.\ndropout(p) is a special case of dropconnect(p) if we simultaneously drop the parameters outgoing\nfrom each dropped neuron.\nInverted Dropout and DropconnectIn the case ofdropout(p), a neuron is retained with a proba-\nbility of 1−pduring training. If we denote the weight parameter of that neuron asw during training,\nthen we use (1 −p)w for that weight parameter at test time (Srivastava et al., 2014). This ensures\nthat the expected output of a neuron is the same as the actual output at test time. In this paper,\ndropout(p) refers to inverted dropout(p) which uses w/(1 −p) instead of w during training. By\ndoing so, we do not need to compute the output separately at test time. Similarly, dropconnect(p)\nrefers to inverted dropconnect(p).\n3 A NALYSIS OF DROPOUT AND ITS GENERALIZATION\nWe start our theoretical analysis by investigating dropconnect which is a general form of dropout\nand then apply the result derived from dropconnect to dropout. The iterative SGD equation for\ndropconnect(p) with a learning rate of ηis\nw(t+1) = w(t) −ηB(t)∇L\n((\nEB(t)\n1\n)−1\nB(t)w(t)\n)\n, t = 0, 1, 2, ··· , (2)\nwhere B(t) = diag(B(t)\n1 , B(t)\n2 , ··· , B(t)\nd ) and B(t)\ni ’s are mutually independentBernoulli(1 −p)\nrandom variables with a drop probability of p for all i and t. We regard equation 2 as ﬁnding a\nsolution to the minimization problem below:\nmin\nw\nEL\n(\n(EB1)−1Bw\n)\n, (3)\nwhere B = diag(B1, B2, ··· , Bd) and Bi’s are mutually independentBernoulli(1 −p) random\nvariables with a drop probability of pfor all i.\nGaussian dropout (Wang & Manning, 2013) and variational dropout (Kingma et al., 2015) use other\nrandom masks to improve dropout rather than Bernoulli random masks. To explain these variants of\ndropout as well, we set a random mask matrix M = diag(M1, M2, ··· , Md) to satisfy EMi = µ\nand Var(Mi) = σ2 for all i. Now we deﬁne a random mixture function with respect to w from u\nand M as\nΦ(w; u,M) = µ−1(\n(I −M)u + Mw −(1 −µ)u\n)\n, (4)\nand a minimization problem with “mixconnect(u, µ, σ2)” as\nmin\nw\nEL\n(\nΦ(w; u,M)\n)\n. (5)\nWe can viewdropconnect(p) equation 3 as a special case of equation 5 whereu = 0 and M = B.\nWe investigate howmixconnect(u, µ, σ2) differs from the vanilla minimization problem\nmin\nw\nEL(w). (6)\nIf the loss function Lis strongly convex, we can derive a lower bound of EL\n(\nΦ(w; u,M)\n)\nas in\nTheorem 1:\n3\nPublished as a conference paper at ICLR 2020\nTheorem 1. Assume that the loss function Lis strongly convex. Suppose that a random mixture\nfunction with respect to w from u and M is given by Φ(w; u,M) in equation 4 where M is\ndiag(M1, M2, ··· , Md) satisfying EMi = µ and Var(Mi) = σ2 for all i. Then, there exists\nm> 0 such that\nEL\n(\nΦ(w; u,M)\n)\n≥L(w) + mσ2\n2µ2 ∥w −u∥2, (7)\nfor all w (Proof in Supplement A).\nTheorem 1 shows that minimizing the l.h.s. of equation 7 minimizes the r.h.s. of equation 7 when\nthe r.h.s. is a sharp lower limit of the l.h.s. The strong convexity ofLmeans that Lis bounded from\nbelow by a quadratic function, and the inequality of equation 7 comes from the strong convexity.\nHence, the equality holds if Lis quadratic, and mixconnect(u, µ, σ2) is an L2-regularizer with a\nregularization coefﬁcient of mσ2/µ2.\n3.1 M IXCONNECT TO MIXOUT\nWe propose mixout as a special case of mixconnect, which is motivated by the relationship between\ndropout and dropconnect. We assume that\nw =\n(\nw(N1)\n1 , ··· , w(N1)\nd1\n, w(N2)\n1 , ··· , w(N2)\nd2\n, ······ ,w(Nk)\n1 , ··· , w(Nk)\ndk\n)\n,\nwhere w(Ni)\nj is the jth parameter outgoing from the neuron Ni. We set the corresponding M to\nM = diag\n(\nM(N1), ··· , M(N1), M(N2), ··· , M(N2), ······ ,M(Nk), ··· , M(Nk)\n)\n, (8)\nwhere EM(Ni) = µand Var(M(Ni)) = σ2 for all i. In this paper, we setM(Ni) to Bernoulli(1−p)\nfor all i and mixout(u) hereafter refers to this correlated version of mixconnect with Bernoulli\nrandom masks. We write it as “mixout(u, p)” when we emphasize the mix probability p.\nCorollary 1.1. Assume that the loss function Lis strongly convex. We denote the random mix-\nture function of mixout(u, p), which is equivalent to that of mixconnect(u, 1 −p, p−p2), as\nΦ(w; u,M) where M is deﬁned in equation 8. Then, there exists m> 0 such that\nEL\n(\nΦ(w; u,B)\n)\n≥L(w) + mp\n2(1 −p)∥w −u∥2, (9)\nfor all w.\nCorollary 1.1 is a straightforward result from Theorem 1. As the mix probability pin equation 9\nincreases to 1, the L2-regularization coefﬁcient of mp/(1 −p) increases to inﬁnity. It means that p\nof mixout(u, p) can adjust the strength ofL2-penalty toward u in optimization. mixout(u) differs\nfrom wdecay(u) since the regularization coefﬁcient ofmixout(u) depends on mdetermined by the\ncurrent model parameter w. mixout(u, p) indeed regularizes learning to minimize the deviation\nfrom u. We validate this by performing least squares regression in Supplement D.\nWe often apply dropout to speciﬁc layers. For instance, Simonyan & Zisserman (2014) applied\ndropout to fully connected layers only. We generalize Theorem 1 to the case in which mixout is\nonly applied to speciﬁc layers, and it can be done by constructing M in a particular way. We\ndemonstrate this approach in Supplement B and show that mixout for speciﬁc layers adaptively\nL2-penalizes their parameters.\n3.2 M IXOUT FOR PRETRAINED MODELS\nHoffer et al. (2017) have empirically shown that\n∥wt −w0∥∼ log t, (10)\nwhere wt is a model parameter after the t-th SGD step. When training from scratch, we usually\nsample an initial model parameter w0 from a normal/uniform distribution with mean 0 and small\nvariance. Since w0 is close to the origin, wt is away from the origin only with a large tby equa-\ntion 10. When ﬁnetuning, we initialize our model parameter from a pretrained model parameter\nwpre. Since we usually obtain wpre by training from scratch on a large pretraining dataset, wpre\nis often far away from the origin. By Corollary 1.1, dropout L2-penalizes the model parameter for\ndeviating away from the origin rather than wpre. To explicitly prevent the deviation from wpre, we\ninstead propose to use mixout(wpre).\n4\nPublished as a conference paper at ICLR 2020\n4 V ERIFICATION OF THEORETICAL RESULTS FOR MIXOUT ON MNIST\nWiese et al. (2017) have highlighted that wdecay(wpre) is an effective regularization technique to\navoid catastrophic forgetting during ﬁnetuning. Because mixout(wpre) keeps the ﬁnetuned model\nto stay in the vicinity of the pretrained model similarly to wdecay(wpre), we suspect that the pro-\nposed mixout(wpre) has a similar effect of alleviating the issue of catastrophic forgetting. To empir-\nically verify this claim, we pretrain a 784-300-100-10 fully-connected network on EMNIST Digits\n(Cohen et al., 2017), and ﬁnetune it on MNIST. For more detailed description of the model architec-\nture and datasets, see Supplement C.1.\nIn the pretraining stage, we run ﬁve random experiments with a batch size of 32 for{1, 2, ··· , 20}\ntraining epochs. We use Adam (Kingma & Ba, 2014) with a learning rate of 10−4, β1 = 0 .9,\nβ2 = 0 .999, wdecay(0, 0.01), learning rate warm-up over the ﬁrst 10% steps of the total steps,\nand linear decay of the learning rate after the warm-up. We use dropout(0.1) for all layers except\nthe input and output layers. We select wpre whose validation accuracy on EMNIST Digits is best\n(0.992) in all experiments.\nFor ﬁnetuning, most of the model hyperparameters are kept same as in pretraining, with the excep-\ntion of the learning rate, number of training epochs, and regularization techniques. We train with\na learning rate of 5 ×10−5 for 5 training epochs. We replace dropout(p) with mixout(wpre, p).\nWe do not use any other regularization technique such as wdecay(0) and wdecay(wpre). We mon-\nitor ∥wft −wpre∥2,1 validation accuracy on MNIST, and validation accuracy on EMNIST Digits to\ncompare mixout(wpre, p) to dropout(p) across 10 random restarts.2\n0.0 0.2 0.4 0.6 0.8\nProbability\n0.5\n1.0\n1.5\n2.0\n2.5L2-norm squared\nDropout\nMixout\n(a) ∥wft −wpre∥2\n0.0 0.2 0.4 0.6 0.8\nProbability\n0.92\n0.94\n0.96Validation accuracy\nDropout\nMixout (b) MNIST\n0.0 0.2 0.4 0.6 0.8\nProbability\n0.55\n0.60\n0.65\n0.70Validation accuracy\nDropout\nMixout (c) EMNIST Digits\nFigure 2: We present ∥wft −wpre∥2, validation accuracy on MNIST (target task), and validation\naccuracy on EMNIST Digits (source task), as the function of the probability pwhere wft and wpre\nare the model parameter after ﬁnetuning and the pretrained model parameter, respectively. We report\nmean (curve) ±std. (shaded area) across 10 random restarts. (a): mixout(wpre, p) L2-penalizes\nthe deviation fromwpre, and this penalty becomes strong aspincreases. However, withdropout(p),\nwft becomes away fromwpre as pincreases. (b): After ﬁnetuning on MNIST, bothmixout(wpre, p)\nand dropout(p) result in high validation accuracy on MNIST for p∈{0.1, 0.2, 0.3}. (c): Valida-\ntion accuracy of dropout(p) on EMNIST Digits drops more than that of mixout(wpre, p) for all\np. mixout(wpre, p) minimizes the deviation from wpre and memorizes the source task better than\ndropout(p) for all p.\nAs shown in Figure 2 (a), after ﬁnetuning with mixout(wpre, p), the deviation from wpre is min-\nimized in the L2-sense. This result veriﬁes Corollary 1.1. We demonstrate that the validation\naccuracy of mixout(wpre, p) has greater robustness to the choice of p than that of dropout(p).\nIn Figure 2 (b), both dropout(p) and mixout(wpre, p) result in high validation accuracy on the\ntarget task (MNIST) for p ∈ {0.1, 0.2, 0.3}, although mixout(wpre, p) is much more robust\nwith respect to the choice of the mix probability p. In Figure 2 (c), the validation accuracy of\nmixout(wpre, p) on the source task (EMNIST Digits) drops from the validation accuracy of the\nmodel at wpre (0.992) to approximately 0.723 regardless of p. On the other hand, the validation\naccuracy of dropout(p) on the source task respectively drops by 0.041, 0.074 and 0.105 which are\nmore than those of mixout(wpre, p) for p∈{0.1, 0.2, 0.3}.\n1wft is a model parameter after ﬁnetuning.\n2Using the same pretrained model parameter wpre but perform different ﬁnetuning data shufﬂing.\n5\nPublished as a conference paper at ICLR 2020\n5 F INETUNING A PRETRAINED LANGUAGE MODEL WITH MIXOUT\nIn order to experimentally validate the effectiveness of mixout, we ﬁnetune BERTLARGE on a sub-\nset of GLUE (Wang et al., 2018) tasks (RTE, MRPC, CoLA, and STS-B) with mixout(wpre). We\nchoose them because Phang et al. (2018) have observed that it was unstable to ﬁnetuneBERTLARGE\non these four tasks. We use the publicly available pretrained model released by Devlin et al. (2018),\nported into PyTorch by HuggingFace. 3 We use the learning setup and hyperparameters recom-\nmended by Devlin et al. (2018). We use Adam with a learning rate of 2 ×10−5, β1 = 0 .9,\nβ2 = 0 .999, learning rate warmup over the ﬁrst 10% steps of the total steps, and linear decay\nof the learning rate after the warmup ﬁnishes. We train with a batch size of 32 for 3 training epochs.\nSince the pretrained BERTLARGE is the sentence encoder, we have to create an additional output\nlayer, which is not pretrained. We initialize each parameter of it withN(0, 0.022). We describe our\nexperimental setup further in Supplement C.2.\nThe original regularization strategy used in Devlin et al. (2018) for ﬁnetuning BERTLARGE is\nusing both dropout(0.1) and wdecay(0, 0.01) for all layers except layer normalization and in-\ntermediate layers activated by GELU (Hendrycks & Gimpel, 2016). We however cannot use\nmixout(wpre) nor wdecay(wpre) for the additional output layer which was not pretrained and\ntherefore does not have wpre. We do not use any regularization for the additional output layer\nwhen ﬁnetuning BERTLARGE with mixout(wpre) and wdecay(wpre). For the other layers, we\nreplace dropout(0.1) and wdecay(0, 0.01) with mixout(wpre) and wdecay(wpre), respectively.\nPhang et al. (2018) have reported that large pretrained models (e.g., BERTLARGE) are prone to\ndegenerate performance when ﬁnetuned on a task with a small number of training examples, and that\nmultiple random restarts4 are required to obtain a usable model better than random prediction. To\ncompare ﬁnetuning stability of the regularization techniques, we need to demonstrate the distribution\nof model performance. We therefore train BERTLARGE with each regularization strategy on each\ntask with 20 random restarts. We validate each random restart on the dev set to observe the behaviour\nof the proposed mixout and ﬁnally evaluate it on the test set for generalization. We present the test\nscore of our proposed regularization strategy on each task in Supplement C.3.\nWe ﬁnetune BERTLARGE with mixout(wpre, {0.7, 0.8, 0.9}) on RTE, MRPC, CoLA, and STS-\nB. For the baselines, we ﬁnetune BERTLARGE with both dropout(0.1) and wdecay(0, 0.01) as\nwell as with wdecay(wpre, {0.01, 0.04, 0.07, 0.10}). These choices are made based on the exper-\niments in Section 6.3 and Supplement F. In Section 6.3, we observe that ﬁnetuning BERTLARGE\nwith mixout(wpre, p) on RTE is signiﬁcantly more stable with p ∈{0.7, 0.8, 0.9}while ﬁne-\ntuning with dropout(p) becomes unstable as p increases. In Supplement F, we demonstrate that\ndropout(0.1) is almost optimal for all the tasks in terms of mean dev score although Devlin et al.\n(2018) selected it to improve the maximum dev score.\nIn Figure 3, we plot the distributions of the dev scores from 20 random restarts when ﬁnetuning\nBERTLARGE with various regularization strategies on each task. For conciseness, we only show\nfour regularization strategies; Devlin et al. (2018)’s: both dropout(0.1) and wdecay(0, 0.01),\nWiese et al. (2017)’s: wdecay(wpre, 0.01), ours: mixout(wpre, 0.7), and ours+Wiese et al.\n(2017)’s: both mixout(wpre, 0.7) and wdecay(wpre, 0.01). As shown in Figure 3 (a–c), we ob-\nserve many ﬁnetuning runs that fail with the chance-level accuracy when we ﬁnetune BERTLARGE\nwith both dropout(0.1) and wdecay(0, 0.01) on RTE, MRPC, and CoLA. We also have a bunch of\ndegenerate model conﬁgurations when we use wdecay(wpre, 0.01) without mixout(wpre, 0.7).\nUnlike existing regularization strategies, when we use mixout(wpre, 0.7) as a regularization tech-\nnique with or without wdecay(wpre, 0.01) for ﬁnetuning BERTLARGE, the number of degenerate\nmodel conﬁgurations that fail with a chance-level accuracy signiﬁcantly decreases. For example, in\nFigure 3 (c), we have only one degenerate model conﬁguration when ﬁnetuning BERTLARGE with\nmixout(wpre, 0.7) on CoLA while we observe respectively seven and six degenerate models with\nDevlin et al. (2018)’s and Wiese et al. (2017)’s regularization strategies.\n3 https : / / s3 . amazonaws . com / models . huggingface . co / bert /\nbert-large-uncased-pytorch_model.bin\n4 Using the same pretrained model parameter wpre but each random restart differs from the others by\nshufﬂing target data and initializing the additional output layer differently.\n6\nPublished as a conference paper at ICLR 2020\nDevlin Wiese Our Our+W\n50\n60\n70Dev score\n(a) RTE (Accuracy)\nDevlin Wiese Our Our+W\n80.0\n82.5\n85.0\n87.5\n90.0\n92.5Dev score\n (b) MRPC (F1 accuracy)\nDevlin Wiese Our Our+W\n0\n20\n40\n60Dev score\n(c) CoLA (Mattew’s correlation)\nDevlin Wiese Our Our+W0\n20\n40\n60\n80\n100Dev score\n (d) STS-B (Spearman correlation)\nFigure 3: Distribution of dev scores on each task from 20 random restarts when ﬁnetuning\nBERTLARGE with Devlin et al. (2018)’s: both dropout(0.1) and wdecay(0, 0.01), Wiese et al.\n(2017)’s: wdecay(wpre, 0.01), ours: mixout(wpre, 0.7), and ours+Wiese et al. (2017)’s: both\nmixout(wpre, 0.7) and wdecay(wpre, 0.01). We write them as Devlin (blue), Wiese (orange), Our\n(green), and Our+W (red), respectively. We use the same set of 20 random initializations across all\nthe regularization setups. Error intervals show mean±std. For all the tasks, the number of ﬁnetuning\nruns that fail with the chance-level accuracy is signiﬁcantly reduced when we use our regularization\nmixout(wpre, 0.7) regardless of using wdecay(wpre, 0.01).\nIn Figure 3 (a), we further improve the stability of ﬁnetuning BERTLARGE by using both\nmixout(wpre, 0.7) and wdecay(wpre, 0.01). Figure 3 (d) shows respectively two and one degener-\nate model conﬁgurations with Devlin et al. (2018)’s and Wiese et al. (2017)’s, but we do not have any\ndegenerate resulting model with ours and ours+Wiese et al. (2017)’s. In Figure 3 (b, c), we observe\nthat the number of degenerate model conﬁgurations increases when we use wdecay(wpre, 0.01)\nadditionally to mixout(wpre, 0.7). In short, applying our proposed mixout signiﬁcantly stabi-\nlizes the ﬁnetuning results of BERTLARGE on small training sets regardless of whether we use\nwdecay(wpre, 0.01).\nIn Table 1, we report the average and the best dev scores across 20 random restarts for each task\nwith various regularization strategies. The average dev scores with mixout(wpre, {0.7, 0.8, 0.9})\nincrease for all the tasks. For instance, the mean dev score of ﬁnetuning withmixout(wpre, 0.8) on\nCoLA is 57.9 which is 49.2% increase over 38.8 obtained by ﬁnetuning with both dropout(p) and\nwdecay(0, 0.01). We observe that using wdecay(wpre, {0.01, 0.04, 0.07, 0.10}) also improves\nthe average dev scores for most tasks compared to using both dropout(p) and wdecay(0, 0.01).\nWe however observe that ﬁnetuning with mixout(wpre, {0.7,0.8,0.9}) outperforms that with\nwdecay(wpre, {0.01, 0.04, 0.07, 0.10}) on average. This conﬁrms that mixout(wpre) has a dif-\nferent effect for ﬁnetuning BERTLARGE compared to wdecay(wpre) since mixout(wpre) is an\nadaptive L2-regularizer along the optimization trajectory.\n7\nPublished as a conference paper at ICLR 2020\nSince ﬁnetuning a large pretrained language model such asBERTLARGE on a small training set fre-\nquently fails, the ﬁnal model performance has often been reported as the maximum dev score (Devlin\net al., 2018; Phang et al., 2018) among a few random restarts. We thus report the best dev score for\neach setting in Table 1. According to the best dev scores as well, mixout(wpre, {0.7, 0.8, 0.9})\nimproves performance for all the tasks compared to using both dropout(p) and wdecay(0, 0.01).\nFor instance, using mixout(wpre, 0.9) improves the maximum dev score by 0.9 compared to\nusing both dropout(p) and wdecay(0, 0.01) on MRPC. Unlike the average dev scores, the\nbest dev scores achieved by using wdecay(wpre, {0.01, 0.04, 0.07, 0.10}) are better than those\nachieved by using mixout(wpre, {0.7, 0.8, 0.9}) except RTE on which it was better to use\nmixout(wpre, {0.7, 0.8, 0.9}) than wdecay(wpre, {0.01, 0.04, 0.07, 0.10}).\nTable 1: Mean ( max) dev scores across 20 random restarts when ﬁnetuning BERTLARGE with\nvarious regularization strategies on each task. We show the following baseline results on the ﬁrst and\nsecond cells: Devlin et al. (2018)’s regularization strategy (bothdropout(p) and wdecay(0, 0.01))\nand Wiese et al. (2017)’s regularization strategy (wdecay(wpre, {0.01, 0.04, 0.07, 0.10})). In the\nthird cell, we demonstrate ﬁnetuning results with only mixout(wpre, {0.7, 0.8, 0.9}). The results\nwith both mixout(wpre, {0.7, 0.8, 0.9}) and wdecay(wpre, 0.01) are also presented in the fourth\ncell. Bold marks the best of each statistics within each column. The mean dev scores greatly increase\nfor all the tasks when we use mixout(wpre, {0.7, 0.8, 0.9}).\nTECHNIQUE 1 TECHNIQUE 2 RTE MRPC CoLA STS-B\ndropout(0.1) wdecay( 0, 0.01) 56.5 (73.6) 83.4 ( 90.4) 38.8 ( 63.3) 82.4 ( 90.3)\n- wdecay(wpre, 0.01) 56.3 (71.5) 86.2 ( 91.6) 41.9 ( 65.6) 85.4 ( 90.5)\n- wdecay(wpre, 0.04) 51.5 (70.8) 85.8 ( 91.5) 35.4 ( 64.7) 80.7 ( 90.6)\n- wdecay(wpre, 0.07) 57.0 (70.4) 85.8 ( 91.0) 48.1 ( 63.9) 89.6 ( 90.3)\n- wdecay(wpre, 0.10) 54.6 (71.1) 84.2 ( 91.8) 45.6 ( 63.8) 84.3 ( 90.1)\nmixout(wpre, 0.7) - 61.6 (74.0) 87.1 ( 91.1) 57.4 ( 62.1) 89.6 ( 90.3)\nmixout(wpre, 0.8) - 64.0 (74.0) 89.0 (90.7) 57.9 ( 63.8) 89.4 ( 90.3)\nmixout(wpre, 0.9) - 64.3 (73.3) 88.2 ( 91.4) 55.2 ( 63.4) 89.4 ( 90.0)\nmixout(wpre, 0.7) wdecay( wpre, 0.01) 65.3 (74.4) 87.8 ( 91.8) 51.9 ( 64.0) 89.6 ( 90.6)\nmixout(wpre, 0.8) wdecay( wpre, 0.01) 62.8 (74.0) 86.3 ( 90.9) 58.3 (65.1) 89.7 (90.3)\nmixout(wpre, 0.9) wdecay( wpre, 0.01) 65.0 (75.5) 88.6 ( 91.3) 58.1 ( 65.1) 89.5 ( 90.0)\nWe investigate the effect of combining both mixout(wpre) and wdecay(wpre) to see whether\nthey are complementary. We ﬁnetune BERTLARGE with both mixout(wpre, {0.7, 0.8, 0.9})\nand wdecay(wpre, 0.01). This leads not only to the improvement in the average dev scores but\nalso in the best dev scores compared to using wdecay(wpre, {0.01, 0.04, 0.07, 0.10}) and us-\ning both dropout(p) and wdecay(0, 0.01). The experiments in this section conﬁrm that using\nmixout(wpre) as one of several regularization techniques prevents ﬁnetuning instability and yields\ngains in dev scores.\n6 A BLATION STUDY\nIn this section, we perform ablation experiments to better understandmixout(wpre). Unless explic-\nitly stated, all experimental setups are the same as in Section 5.\n6.1 M IXOUT WITH A SUFFICIENT NUMBER OF TRAINING EXAMPLES\nWe showed the effectiveness of the proposed mixout ﬁnetuning with only a few training examples\nin Section 5. In this section, we investigate the effectiveness of the proposed mixout in the case\nof a larger ﬁnetuning set. Since it has been stable to ﬁnetune BERTLARGE on a sufﬁcient number\nof training examples (Devlin et al., 2018; Phang et al., 2018), we expect to see the change in the\nbehaviour of mixout(wpre) when we use it to ﬁnetune BERTLARGE on a larger training set.\nWe train BERTLARGE by using both mixout(wpre, 0.7) and wdecay(wpre, 0.01) with 20 random\nrestarts on SST-2.5 We also train BERTLARGE by using both dropout(p) and wdecay(0, 0.01)\nwith 20 random restarts on SST-2 as the baseline. In Table 2, we report the mean and maximum of\n5 For the description of SST-2, see Supplement C.2.\n8\nPublished as a conference paper at ICLR 2020\nSST-2 dev scores across 20 random restarts with each regularization strategy. We observe that there\nis little difference between their mean and maximum dev scores on a larger training set, although\nusing both mixout(wpre, 0.7) and wdecay(wpre, 0.01) outperformed using both dropout(p) and\nwdecay(0, 0.01) on the small training sets in Section 5.\nTable 2: Mean ( max) SST-2 dev scores across 20 random restarts when ﬁnetuning BERTLARGE\nwith each regularization strategy. Bold marks the best of each statistics within each column. For a\nlarge training set, both mean and maximum dev scores are similar to each other.\nTECHNIQUE 1 TECHNIQUE 2 SST-2\ndropout(0.1) wdecay( 0, 0.01) 93.4 (94.0)\nmixout(wpre, 0.7) wdecay( wpre, 0.01) 93.5 (94.3)\n6.2 E FFECT OF A REGULARIZATION TECHNIQUE FOR AN ADDITIONAL OUTPUT LAYER\nIn this section, we explore the effect of a regularization technique for an additional output layer.\nThere are two regularization techniques available for the additional output layer: dropout(p) and\nmixout(w0, p) where w0 is its randomly initialized parameter. Either of these strategies differs\nfrom the earlier experiments in Section 5 where we did not put any regularization for the additional\noutput layer.\nTable 3: We present mean ( max) dev scores across 20 random restarts with various regularization\ntechniques for the additional output layers (ADDITIONAL) when ﬁnetuning BERTLARGE on each\ntask. For all cases, we apply mixout(wpre, 0.7) to the pretrained layers (PRETRAINED). The\nﬁrst row corresponds to the setup in Section 5. In the second row, we apply mixout(w0, 0.7) to\nthe additional output layer where w0 is its randomly initialized parameter. The third row shows\nthe results obtained by applying dropout(0.7) to the additional output layer. In the fourth row, we\ndemonstrate the best of each result from all the regularization strategies shown in Table 1. Bold\nmarks the best of each statistics within each column. We obtain additional gains in dev scores by\nvarying the regularization technique for the additional output layer.\nPRETRAINED ADDITIONAL RTE MRPC CoLA STS-B\nmixout(wpre, 0.7) - 61.6 (74.0) 87.1 ( 91.1) 57.4 ( 62.1) 89.6 ( 90.3)\nmixout(wpre, 0.7) mixout(w0, 0.7) 66.5 (75.5) 88.1 ( 92.4) 58.7 (65.6) 89.7 (90.6)\nmixout(wpre, 0.7) dropout(0 .7) 57.2 (70.8) 85.9 ( 92.5) 48.9 ( 64.3) 89.2 ( 89.8)\nThe best of each result from Table 1 65.3 (75.5) 89.0 (91.8) 58.3 ( 65.6) 89.7 (90.6)\nWe report the average and best dev scores across 20 random restarts when ﬁnetuning BERTLARGE\nwith mixout(wpre, 0.7) while varying the regularization technique for the additional output layer in\nTable 3.6 We observe that using mixout(w0, 0.7) for the additional output layer improves both the\naverage and best dev score on RTE, CoLA, and STS-B. In the case of MRPC, we have the highest\nbest-dev score by using dropout(0.7) for the additional output layer while the highest mean dev\nscore is obtained by using mixout(w0, 0.7) for it. In Section 3.2, we discussed how mixout(w0)\ndoes not differ from dropout when the layer is randomly initialized, since we sample w0 from w\nwhose mean and variance are 0 and small, respectively. Although the additional output layer is\nrandomly initialized, we observe the signiﬁcant difference between dropout andmixout(w0) in this\nlayer. We conjecture that ∥w0 −0∥is not sufﬁciently small because E∥w −0∥is proportional to\nthe dimensionality of the layer (2,048). We therefore expectmixout(w0) to behave differently from\ndropout even for the case of training from scratch.\nIn the last row of Table 3, we present the best of the corresponding result from Table 1. We\nhave the highest mean and best dev scores when we respectively use mixout(wpre, 0.7) and\nmixout(w0, 0.7) for the pretrained layers and the additional output layer on RTE, CoLA, and STS-\nB. The highest mean dev score on MRPC is obtained by usingmixout(wpre, 0.8) for the pretrained\nlayers which is one of the results in Table 1. We have the highest best dev score on MRPC when\nwe use mixout(wpre, 0.7) and dropout(0.7) for the pretrained layers and the additional output\n6 In this experiment, we use neither wdecay(0) nor wdecay(wpre).\n9\nPublished as a conference paper at ICLR 2020\nlayer, respectively. The experiments in this section reveal that using mixout(w0) for a randomly\ninitialized layer of a pretrained model is one of the regularization schemes to improve the average\ndev score and the best dev score.\n6.3 E FFECT OF MIX PROBABILITY FOR MIXOUT AND DROPOUT\nWe explore the effect of the hyperparameterpwhen ﬁnetuning BERTLARGE with mixout(wpre, p)\nand dropout(p). We train BERTLARGE with mixout(wpre, {0.0, 0.1, ··· , 0.9}) on RTE with\n20 random restarts. We also train BERTLARGE after replacing mixout(wpre, p) by dropout(p)\nwith 20 random restarts. We do not use any regularization technique for the additional output\nlayer. Because we use neither wdecay(0) nor wdecay(wpre) in this section, dropout(0.0) and\nmixout(wpre, 0.0) are equivalent to ﬁnetuning without regularization.\n0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nProbability\n45\n50\n55\n60\n65\n70\n75Dev score\nMixout\nDropout\nFigure 4: Distribution of RTE dev scores (Accuracy) from 20 random restarts when ﬁnetun-\ning BERTLARGE with dropout(p) (orange) or mixout(wpre, p) (blue). Error intervals show\nmean±std. We do not use wdecay(0) nor wdecay(wpre). In the case of mixout(wpre, p), the\nnumber of usable models after ﬁnetuning with mixout(wpre, {0.7, 0.8, 0.9}) is signiﬁcantly more\nthan the number of usable models after ﬁnetuning with dropout(p) for all p.\nIt is not helpful to varypfor dropout(p) while mixout(wpre, p) helps signiﬁcantly in a wide range\nof p. Figure 4 shows distributions of RTE dev scores across 20 random restarts when ﬁnetuning\nBERTLARGE with dropout(p) and mixout(wpre, p) for p ∈{0.0, 0.1, ··· , 0.9}. The mean\ndev score of ﬁnetuning BERTLARGE with mixout(wpre, p) increases as pincreases. On the other\nhand, the mean dev score of ﬁnetuning BERTLARGE with dropout(p) decreases as pincreases. If\npis less than 0.4, ﬁnetuning with mixout(wpre, p) does not improve the ﬁnetuning results of using\ndropout({0.0, 0.1, 0.2}). We however observe that mixout(wpre, {0.7, 0.8, 0.9}) yields better\naverage dev scores than dropout(p) for all p, and signiﬁcantly reduces the number of ﬁnetuning\nruns that fail with the chance-level accuracy.\nWe notice that the proposed mixout spends more time than dropout from the experiments in this\nsection. It takes longer to ﬁnetune a model with the proposed mixout than with the original dropout,\nalthough this increase is not signiﬁcant especially considering the waste of time from failed ﬁnetun-\ning runs using dropout. In Supplement E, we describe more in detail the difference between mixout\nand dropout in terms of wall-clock time.\n7 C ONCLUSION\nThe special case of our approach, mixout(wpre), is one of several regularization techniques mod-\nifying a ﬁnetuning procedure to prevent catastrophic forgetting. Unlike wdecay(wpre) proposed\nearlier by Wiese et al. (2017), mixout(wpre) is an adaptive L2-regularizer toward wpre in the sense\nthat its regularization coefﬁcient adapts along the optimization path. Due to this difference, the pro-\nposed mixout improves the stability of ﬁnetuning a big, pretrained language model even with only\na few training examples of a target task. Furthermore, our experiments have revealed the proposed\napproach improves ﬁnetuning results in terms of the average accuracy and the best accuracy over\nmultiple runs. We emphasize that our approach can be used with any pretrained language models\nsuch as RoBERTa (Liu et al., 2019) and XLNet (Yang et al., 2019), since mixout does not depend\non model architectures, and leave it as future work.\n10\nPublished as a conference paper at ICLR 2020\nACKNOWLEDGMENTS\nThe ﬁrst and third authors’ work was supported by the National Research Foundation of Korea\n(NRF) grants funded by the Korea government (MOE, MSIT) (NRF-2017R1A2B4011546, NRF-\n2019R1A5A1028324). The second author thanks support by AdeptMind, eBay, TenCent, NVIDIA\nand CIFAR and was partly supported by Samsung Electronics (Improving Deep Learning using\nLatent Structure).\nREFERENCES\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\nAntonio Valerio Miceli Barone, Barry Haddow, Ulrich Germann, and Rico Sennrich. Regularization\ntechniques for ﬁne-tuning in neural machine translation. arXiv preprint arXiv:1707.09920, 2017.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task\n1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint\narXiv:1708.00055, 2017.\nGregory Cohen, Saeed Afshar, Jonathan Tapson, and Andr ´e van Schaik. Emnist: an extension of\nmnist to handwritten letters. arXiv preprint arXiv:1702.05373, 2017.\nRonan Collobert, Jason Weston, L ´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel\nKuksa. Natural language processing (almost) from scratch.Journal of machine learning research,\n12(Aug):2493–2537, 2011.\nIdo Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment\nchallenge. In Machine learning challenges. evaluating predictive uncertainty, visual object clas-\nsiﬁcation, and recognising tectual entailment, pp. 177–190. Springer, 2006.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nWilliam B Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\nIn Proceedings of the International Workshop on Paraphrasing, 2005.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415, 2016.\nElad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generaliza-\ntion gap in large batch training of neural networks. InAdvances in Neural Information Processing\nSystems, pp. 1731–1741, 2017.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nDurk P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameteri-\nzation trick. In Advances in Neural Information Processing Systems, pp. 2575–2583, 2015.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A\nRusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-\ning catastrophic forgetting in neural networks. Proceedings of the national academy of sciences,\n114(13):3521–3526, 2017.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\nPoorya Mianjy and Raman Arora. On dropout and nuclear norm regularization. In Kamalika Chaud-\nhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Ma-\nchine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 4575–4584, Long\nBeach, California, USA, 09–15 Jun 2019. PMLR. URL http : / / proceedings . mlr .\npress/v97/mianjy19a.html.\n11\nPublished as a conference paper at ICLR 2020\nVinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In\nProceedings of the 27th international conference on machine learning (ICML-10) , pp. 807–814,\n2010.\nJason Phang, Thibault F ´evry, and Samuel R Bowman. Sentence encoders on stilts: Supplementary\ntraining on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088, 2018.\nJonathan Schwarz, Jelena Luketina, Wojciech M Czarnecki, Agnieszka Grabska-Barwinska,\nYee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable frame-\nwork for continual learning. arXiv preprint arXiv:1805.06370, 2018.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556, 2014.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng,\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\ntreebank. In Proceedings of EMNLP, pp. 1631–1642, 2013.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: a simple way to prevent neural networks from overﬁtting. The Journal of Machine\nLearning Research, 15(1):1929–1958, 2014.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nLi Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural\nnetworks using dropconnect. In International conference on machine learning , pp. 1058–1066,\n2013.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding. arXiv\npreprint arXiv:1804.07461, 2018.\nSida Wang and Christopher Manning. Fast dropout training. Ininternational conference on machine\nlearning, pp. 118–126, 2013.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments.\narXiv preprint 1805.12471, 2018.\nGeorg Wiese, Dirk Weissenborn, and Mariana Neves. Neural domain adaptation for biomedical\nquestion answering. arXiv preprint arXiv:1706.03610, 2017.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint\narXiv:1906.08237, 2019.\n12\nPublished as a conference paper at ICLR 2020\nSUPPLEMENTARY MATERIAL\nA P ROOFS FOR THEOREM 1\nTheorem 1. Assume that the loss function Lis strongly convex. Suppose that a random mixture\nfunction with respect to w from u and M is given by\nΦ(w; u,M) = µ−1(\n(I −M)u + Mw −(1 −µ)u\n)\n,\nwhere M is diag(M1, M2, ··· , Md) satisfying EMi = µand Var(Mi) = σ2 for all i. Then,\nthere exists m> 0 such that\nEL\n(\nΦ(w; u,M)\n)\n≥L(w) + mσ2\n2µ2 ∥w −u∥2, (11)\nfor all w.\nProof. Since Lis strongly convex, there exist m> 0 such that\nEL\n(\nΦ(w; u,M)\n)\n= EL\n(\nw +\n(\nΦ(w; u,M) −w\n))\n≥L(w) + ∇L(w)⊤E[Φ(w; u,M) −w] + m\n2 E∥Φ(w; u,M) −w∥2,\n(12)\nfor all w by equation 1. Recall that EMi = µand Var(Mi) = σ2 for all i. Then, we have\nE[Φ(w; u,M) −w] = 0, (13)\nand\nE∥Φ(w; u,M) −w∥2 = E\n\n1\nµ(w −u)(M −µI)\n\n2\n= 1\nµ2\nd∑\ni=1\n(wi −ui)2E(Mi −µ)2\n= σ2\nµ2 ∥w −u∥2. (14)\nBy using equation 13 and equation 14, we can rewrite equation 12 as\nEL\n(\nΦ(w; u,M)\n)\n≥L(w) + mσ2\n2µ2 ∥w −u∥2.\nB A PPLYING TO SPECIFIC LAYERS\nWe often apply dropout to speciﬁc layers. For instance, Simonyan & Zisserman (2014) applied\ndropout to fully connected layers only. We generalize Theorem 1 to the case in which mixconnect is\nonly applied to speciﬁc layers, and it can be done by constructing M in a particular way. To better\ncharacterize mixconnect applied to speciﬁc layers, we deﬁne the index set I as I = {i: Mi = 1}.\nFurthermore, we use ˜w and ˜u to denote (wi)i/∈I and (ui)i/∈I, respectively. Then, we generalize\nequation 7 to\nEL\n(\nΦ(w; u,M)\n)\n≥L(w) + mσ2\n2µ2 ∥˜w −˜u∥2. (15)\nFrom equation 15, applying mixconnect(u, µ, σ2) is to use adaptive wdecay(˜u) on the weight\nparameter of the speciﬁc layers ˜w. Similarly, we can regard applying mixout(u, p) to speciﬁc\nlayers as adaptive wdecay(˜u).\n13\nPublished as a conference paper at ICLR 2020\nC E XPERIMENTAL DETAILS\nC.1 F ROM EMNIST D IGITS TO MNIST\nModel Architecture The model architecture in Section 4 is a 784-300-100-10 fully connected\nnetwork with a softmax output layer. For each hidden layer, we add layer normalization (Ba et al.,\n2016) right after the ReLU (Nair & Hinton, 2010) nonlinearity. We initialize each parameter with\nN(0, 0.022) and each bias with 0.\nRegularization In the pretraining stage, we use dropout(0.1) and wdecay(0, 0.01). We apply\ndropout(0.1) to all hidden layers. That is, we do not drop neurons of the input and output layers.\nwdecay(0, 0.01) does not penalize the parameters for bias and layer normalization. When we\nﬁnetune our model on MNIST, we replace dropout(p) with mixout(wpre, p). We use neither\nwdecay(0) nor wdecay(wpre) for ﬁnetuning.\nDataset For pretraining, we train our model on EMNIST Digits. This dataset has 280,000 char-\nacters into 10 balanced classes. These characters are compatible with MNIST characters. EMNIST\nDigits provides 240,000 characters for training and 40,000 characters for test. We use 240,000 char-\nacters provide for training and split these into the training set (216,000 characters) and validation set\n(24,000 characters). For ﬁnetuning, we train our model on MNIST. This has 70,000 characters into\n10 balance classes. MNIST provide 60,000 characters for training and 10,000 characters for test.\nWe use 60,000 characters given for training and split these into the training set (54,000 characters)\nand validation set (6,000 characters).\nData Preprocessing We only use normalization after scaling pixel values into [0, 1]. We do not\nuse any data augmentation.\nC.2 F INETUNING BERT ON PARTIAL GLUE TASKS\nModel Architecture Because the model architecture of BERTLARGE is identical to the original\n(Devlin et al., 2018), we omit its exhaustive description. Brieﬂy, BERTLARGE has 24 layers, 1024\nhidden size, and 16 self-attention heads (total 340M parameters). We use the publicly available pre-\ntrained model released by Devlin et al. (2018), ported into PyTorch by HuggingFace.7 We initialize\neach weight parameter and bias for an additional output layer with N(0, 0.022) and 0, respectively.\nRegularization In the ﬁnetuning stage, Devlin et al. (2018) used wdecay(0, 0.01) for all param-\neters except bias and layer normalization. They apply dropout(0.1) to all layers except each hidden\nlayer activated by GELU (Hendrycks & Gimpel, 2016) and layer normalization. We substitute\nwdecay(wpre) and mixout(wpre) for wdecay(0, 0.01) and dropout(0.1), respectively.\nDataset We use a subset of GLUE (Wang et al., 2018) tasks. The brief description for each dataset\nis as the following:\n• RTE (2,500 training examples): Binary entailment task (Dagan et al., 2006)\n• MRPC (3,700 training examples): Semantic similarity (Dolan & Brockett, 2005)\n• CoLA (8,500 training examples): Acceptability classiﬁcation (Warstadt et al., 2018)\n• STS-B (7,000 training examples): Semantic textual similarity (Cer et al., 2017)\n• SST-2 (67,000 training examples): Binary sentiment classiﬁcation (Socher et al., 2013)\nIn this paper, we reported F1 accuracy scores for MRPC, Mattew’s correlation scores for CoLA,\nSpearman correlation scores for STS-B, and accuracy scores for the other tasks.\nData Preprocessing We use the publicly available implementation ofBERTLARGE by Hugging-\nFace.8\n7https : / / s3 . amazonaws . com / models . huggingface . co / bert /\nbert-large-uncased-pytorch_model.bin\n8https://github.com/huggingface/pytorch-transformers\n14\nPublished as a conference paper at ICLR 2020\nC.3 T EST RESULTS ON GLUE TASKS\nWe expect that using mixout stabilizes ﬁnetuning results of BERTLARGE on a small training set.\nTo show this, we demonstrated distributions of dev scores from 20 random restarts on RTE, MRPC,\nCoLA, and STS-B in Figure 3. We further obtained the highest average/best dev score on each task\nin Table 3. To conﬁrm the generalization of the our best model on the dev set, we demonstrate the\ntest results scored by the evaluation server9 in Table 4.\nTable 4: We present the test score when ﬁnetuning BERTLARGE with each regularization strat-\negy on each task. The ﬁrst row shows the test scores obtained by using both dropout(p) and\nwdecay(0, 0.01). These results in the ﬁrst row are reported by Devlin et al. (2018). They used\nthe learning rate of {2 ×10−5, 3 ×10−5, 4 ×10−5, 5 ×10−5}and a batch size of 32 for 3\nepochs with multiple random restarts. They selected the best model on each dev set. In the second\nrow, we demonstrate the test scores obtained by using the proposed mixout in Section 6.2: using\nmixout(wpre, 0.7) for the pretrained layers and mixout(w0, 0.7) for the additional output layer\nwhere w0 is its randomly initialized weight parameter. We used the learning rate of 2 ×10−5 and a\nbatch size of 32 for 3 epochs with 20 random restarts. We submitted the best model on each dev set.\nThe third row shows that the test scores obtained by using both dropout(p) and wdecay(0, 0.01)\nwith same experimental setups of the second row. Bold marks the best within each column. The\nproposed mixout improves the test scores except MRPC compared to the original regularization\nstrategy proposed by Devlin et al. (2018).\nSTRATEGY RTE MRPC CoLA STS-B\nDevlin et al. (2018) 70.1 89.3 60.5 86.5\nmixout(wpre, 0.7) & mixout(w0, 0.7) 70.2 89.1 62.1 87.3\ndropout(p) + wdecay(0, 0.01) 68.2 88.3 59.6 86.0\nFor all the tasks except MRPC, the test scores obtained by the proposed mixout 10 are better than\nthose reported by Devlin et al. (2018). We explored the behaviour of ﬁnetuning BERTLARGE with\nmixout by using the learning rate of 2 ×10−5 while Devlin et al. (2018) obtained their results\nby using the learning rate of {2 ×10−5, 3 ×10−5, 4 ×10−5, 5 ×10−5}. We thus present the\ntest scores obtained by the regularization strategy of Devlin et al. (2018) when the learning rate is\n2 ×10−5. The results in this section show that the best model on the dev set generalizes well, and\nall the experiments based on dev scores in this paper are proper to validate the effectiveness of the\nproposed mixout. For the remaining GLUE tasks such as SST-2 with a sufﬁcient number of training\ninstances, we observed that using mixout does not differs from using dropout in Section 6.1. We\ntherefore omit the test results on the other tasks in GLUE.\nD V ERIFICATION OF COROLLARY 1.1 WITH LEAST SQUARES REGRESSION\nCorollary 1.1 shows that mixout(u, p) regularizes learning to minimize the deviation from the tar-\nget model parameteru, and the strength of regularization increases aspincreases when the loss func-\ntion is strongly convex. In order to validate this, we explore the behavior of least squares regression\nwith mixout(u, p) on a synthetic dataset. For randomly given w∗\n1 and w∗\n2, we generated an obser-\nvation ysatisfying y= w∗\n1x+w∗\n2 +ϵwhere ϵis Gaussian noise. We set the model toˆy= w1x+w2.\nThat is, the model parameter w is given by (w1, w2). We randomly pick u as a target model param-\neter for mixout(u, p) and perform least squares regression with mixout(u, {0.0, 0.3, 0.6, 0.9}).\nAs shown in Figure 5, w converges to the target model parameter u rather than the true model\nparameter w∗= (w∗\n1, w∗\n2) as the mix probability pincreases.\n9https://gluebenchmark.com/leaderboard\n10The regularization strategy in Section 6.2: using mixout(wpre, 0.7) for the pretrained layers and\nmixout(w0, 0.7) for the additional output where w0 is its randomly initialized weight parameter.\n15\nPublished as a conference paper at ICLR 2020\n40\n 20\n 0 20 40\nx\n400\n200\n0\n200\n400\ny\ntrue\ntarget\nmixout(target, 0.0)\nObservation\n(a) mixout(u, 0.0)\n40\n 20\n 0 20 40\nx\n400\n200\n0\n200\n400\ny\ntrue\ntarget\nmixout(target, 0.3)\nObservation (b) mixout(u, 0.3)\n40\n 20\n 0 20 40\nx\n400\n200\n0\n200\n400\ny\ntrue\ntarget\nmixout(target, 0.6)\nObservation\n(c) mixout(u, 0.6)\n40\n 20\n 0 20 40\nx\n400\n200\n0\n200\n400\ny\ntrue\ntarget\nmixout(target, 0.9)\nObservation (d) mixout(u, 0.9)\nFigure 5: Behavior of mixout(u, p) for a strongly convex loss function. We plot the line obtained\nby least squares regression with mixout(u, {0.0, 0.3, 0.6, 0.9}) (each green line) on a synthetic\ndataset (blue dots) generated by the true line (each blue dotted line). As pincreases, the regression\nline (each green line) converges to the target line generated by the target model parameter u (each\norange dotted line) rather than the true line (each blue dotted line).\nE T IME USAGE OF MIXOUT COMPARED TO DROPOUT\nWe recorded the training time of the experiment in Section 6.3 to compare the time usage of mixout\nand that of dropout. It took about 843 seconds to ﬁnetune BERTLARGE with mixout(wpre). On\nthe other hand, it took about 636 seconds to ﬁnetune BERTLARGE with dropout. mixout(wpre)\nspends 32.5% more time than dropout since mixout(wpre) needs an additional computation with\nthe pretrained model parameter wpre. However, as shown in Figure 4, at least 15 ﬁnetuning runs\namong 20 random restarts fail with the chance-level accuracy on RTE with dropout(p) for all p\nwhile only 4 ﬁnetuning runs out of 20 random restarts are unusable with mixout(wpre, 0.8). From\nthis result, it is reasonable to ﬁnetune with the proposed mixout although this requires additional\ntime usage compared to dropout.\nF E XTENSIVE HYPERPARAMETER SEARCH FOR DROPOUT\nDevlin et al. (2018) ﬁnetuned BERTLARGE with dropout(0.1) on all GLUE (Wang et al., 2018)\ntasks. They chose it to improve the maximum dev score on each downstream task, but we have\nreported not only the maximum dev score but also the mean dev score to quantitatively compare\nvarious regularization techniques in our paper. In this section, we explore the effect of the hyper-\nparameter pwhen ﬁnetuning BERTLARGE with dropout(p) on RTE, MRPC, CoLA, and STS-B\nto show dropout(0.1) is optimal in terms of mean dev score. All experimental setups for these\nexperiments are the same as Section 6.3.\n16\nPublished as a conference paper at ICLR 2020\n0.5 0.4 0.3 0.2 0.1 0.0\nDrop probability\n50\n55\n60\n65\n70Dev score\n(a) RTE (Accuracy)\n0.5 0.4 0.3 0.2 0.1 0.0\nDrop probability\n80.0\n82.5\n85.0\n87.5\n90.0\n92.5Dev score\n (b) MRPC (F1 accuracy)\n0.5 0.4 0.3 0.2 0.1 0.0\nDrop probability\n0\n20\n40\n60Dev score\n(c) CoLA (Mattew’s correlation)\n0.5 0.4 0.3 0.2 0.1 0.0\nDrop probability\n0\n20\n40\n60\n80\n100Dev score\n (d) STS-B (Spearman correlation)\nFigure 6: Distribution of dev scores on each task from 20 random restarts when ﬁnetuning\nBERTLARGE with dropout({0.0, 0.1, ··· , 0.5}). Error intervals show mean±std. When we use\ndropout(0.1), we have the highest average dev scores on MRPC and STS-B and the second-highest\naverage dev scores on RTE and CoLA. These results show that dropout(0.1) is almost optimal for\nall tasks in terms of mean dev score.\nAs shown in Figure 6, we have the highest average dev score on MRPC withdropout(0.1) as well as\non STS-B. We obtain the highest average dev scores with dropout(0.0) on RTE and CoLA, but we\nget the second-highest average dev scores with dropout(0.1) on them. These experiments conﬁrm\nthat the drop probability 0.1 is almost optimal for the highest average dev score on each task.\n17",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8191646933555603
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.7950872182846069
    },
    {
      "name": "Language model",
      "score": 0.6131682395935059
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5712149739265442
    },
    {
      "name": "Machine learning",
      "score": 0.504048764705658
    },
    {
      "name": "Generalization",
      "score": 0.451474130153656
    },
    {
      "name": "Downstream (manufacturing)",
      "score": 0.4165268540382385
    },
    {
      "name": "Mathematics",
      "score": 0.07438284158706665
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    }
  ],
  "cited_by": 100
}