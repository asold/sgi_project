{
    "title": "Broad Context Language Modeling as Reading Comprehension",
    "url": "https://openalex.org/W2539583692",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4288554044",
            "name": "Chu, Zewei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1840926975",
            "name": "Wang Hai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4281279111",
            "name": "Gimpel, Kevin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287424161",
            "name": "McAllester, David",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1566289585",
        "https://openalex.org/W1631260214",
        "https://openalex.org/W2125436846",
        "https://openalex.org/W2963595025",
        "https://openalex.org/W2411480514",
        "https://openalex.org/W2964267515",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2512077205"
    ],
    "abstract": "Progress in text understanding has been driven by large datasets that test particular capabilities, like recent datasets for reading comprehension (Hermann et al., 2015). We focus here on the LAMBADA dataset (Paperno et al., 2016), a word prediction task requiring broader context than the immediate sentence. We view LAMBADA as a reading comprehension problem and apply comprehension models based on neural networks. Though these models are constrained to choose a word from the context, they improve the state of the art on LAMBADA from 7.3% to 49%. We analyze 100 instances, finding that neural network readers perform well in cases that involve selecting a name from the context based on dialogue or discourse cues but struggle when coreference resolution or external knowledge is needed.",
    "full_text": "arXiv:1610.08431v3  [cs.CL]  16 Feb 2017\nBroad Context Language Modeling as Reading Comprehension\nZewei Chu1 Hai Wang2 Kevin Gimpel2 David McAllester2\n1University of Chicago, Chicago, IL, 60637, USA\n2T oyota T echnological Institute at Chicago, Chicago, IL, 60 637, USA\nzeweichu@uchicago.edu, {haiwang,kgimpel,mcallester}@ttic.edu\nAbstract\nProgress in text understanding has been\ndriven by large datasets that test partic-\nular capabilities, like recent datasets for\nreading comprehension (Hermann et al.,\n2015). W e focus here on the LAMBADA\ndataset (Paperno et al., 2016), a word\nprediction task requiring broader context\nthan the immediate sentence. W e view\nLAMBADA as a reading comprehension\nproblem and apply comprehension models\nbased on neural networks. Though these\nmodels are constrained to choose a word\nfrom the context, they improve the state\nof the art on LAMBADA from 7.3% to\n49%. W e analyze 100 instances, ﬁnding\nthat neural network readers perform well\nin cases that involve selecting a name from\nthe context based on dialogue or discourse\ncues but struggle when coreference reso-\nlution or external knowledge is needed.\n1 Introduction\nThe LAMBADA dataset (Paperno et al., 2016)\nwas designed by identifying word prediction tasks\nthat require broad context. Each instance is drawn\nfrom the BookCorpus (Zhu et al., 2015) and con-\nsists of a passage of several sentences where the\ntask is to predict the last word of the last sen-\ntence. The instances are manually ﬁltered to ﬁnd\ncases that are guessable by humans when given\nthe larger context but not when only given the last\nsentence. The expense of this manual ﬁltering has\nlimited the dataset to only about 10,000 instances\nwhich are viewed as development and test data.\nThe training data is taken to be books in the corpus\nother than those from which the evaluation pas-\nsages were extracted.\nPaperno et al. (2016) provide baseline results\nwith popular language models and neural network\narchitectures; all achieve zero percent accuracy .\nThe best accuracy is 7.3% obtained by randomly\nchoosing a capitalized word from the passage.\nOur approach is based on the observation that\nin 83% of instances the answer appears in the con-\ntext. W e exploit this in two ways. First, we auto-\nmatically construct a large training set of 1.8 mil-\nlion instances by simply selecting passages where\nthe answer occurs in the context. Second, we treat\nthe problem as a reading comprehension task sim-\nilar to the CNN/Daily Mail datasets introduced by\nHermann et al. (2015), the Children’s Book T est\n(CBT) of Hill et al. (2016), and the Who-did-What\ndataset of Onishi et al. (2016). W e show that stan-\ndard models for reading comprehension, trained\non our automatically generated training set, im-\nprove the state of the art on the LAMBADA test\nset from 7.3% to 49.0%. This is in spite of the fact\nthat these models fail on the 17% of instances in\nwhich the answer is not in the context.\nW e also perform a manual analysis of the LAM-\nBADA task, provide an estimate of human perfor-\nmance, and categorize the instances in terms of\nthe phenomena they test. W e ﬁnd that the com-\nprehension models perform best on instances that\nrequire selecting a name from the context based on\ndialogue or discourse cues, but struggle when re-\nquired to do coreference resolution or when exter-\nnal knowledge could help in choosing the answer.\n2 Methods\nW e now describe the models that we employ for\nthe LAMBADA task (Section 2.1) as well as our\ndataset construction procedure (Section 2.2).\n2.1 Neural Readers\nHermann et al. (2015) developed the CNN/Daily\nMail comprehension tasks and introduced ques-\ntion answering models based on neural networks.\nMany others have been developed since. W e re-\nfer to these models as “neural readers”. While a\ndetailed survey is beyond our scope, we brieﬂy\ndescribe the neural readers used in our exper-\niments: the Stanford (Chen et al., 2016), At-\ntention Sum (Kadlec et al., 2016), and Gated-\nAttention (Dhingra et al., 2016) Readers. These\nneural readers use attention based on the question\nand passage to choose an answer from among the\nwords in the passage. W e use d for the context\nword sequence, q for the question (with a blank to\nbe ﬁlled), A for the candidate answer list, and V\nfor the vocabulary . W e describe neural readers in\nterms of three components:\n1. Embedding and Encoding : Each word in d\nand q is mapped into a v-dimensional vector via\nthe embedding function e(w) ∈ Rv, for all w ∈\nd ∪ q.1 The same embedding function is used\nfor both d and q. The embeddings are learned\nfrom random initialization; no pretrained word\nembeddings are used. The embedded context\nis processed by a bidirectional recurrent neural\nnetwork (RNN) which computes hidden vectors\nhi for each position i:\nh→= fRNN (θ→\nd , e (d))\nh←= bRNN (θ←\nd , e (d))\nh = ⟨h→, h ←⟩\nwhere θ→\nd and θ←\nd are RNN parameters, and\neach of fRNN and bRNN return a sequence of\nhidden vectors, one for each position in the in-\nput e(d). The question is encoded into a single\nvector g which is the concatenation of the ﬁnal\nvectors of two RNNs:\ng→= fRNN (θ→\nq , e (q))\ng←= bRNN (θ←\nq , e (q))\ng = ⟨g→\n|q|, g ←\n0 ⟩\nThe RNNs use either gated recurrent\nunits (Cho et al., 2014) or long short-term\nmemory (Hochreiter and Schmidhuber, 1997).\n2. Attention: The readers then compute atten-\ntion weights on positions of h using g. In\ngeneral, we deﬁne α i = softmax(att(hi, g )),\nwhere i ranges over positions in h. The\n1 W e overload the e function to operate on sequences and\ndenote the embedding of d and q as matrices e(d) and e(q).\natt function is an inner product in the At-\ntention Sum Reader and a bilinear product in\nthe Stanford Reader. The computed attentions\nare then passed through a softmax function to\nform a probability distribution. The Gated-\nAttention Reader uses a richer attention archi-\ntecture (Dhingra et al., 2016); space does not\npermit a detailed description.\n3. Output and Prediction: T o output a prediction\na∗, the Stanford Reader computes the attention-\nweighted sum of the context vectors and then an\ninner product with each candidate answer:\nc =\n|d|∑\ni=1\nα ihi a∗= argmax\na∈A\no(a)⊤c\nwhere o(a) is the “output” embedding function.\nAs the Stanford Reader was developed for the\nanonymized CNN/Daily Mail tasks, only a few\nentries in the output embedding function needed\nto be well-trained in their experiments. How-\never, for LAMBADA, correct answers can range\nover the entirety of V, making the output em-\nbedding function difﬁcult to train. Therefore we\nalso experiment with a modiﬁed version of the\nStanford Reader that uses the same embedding\nfunction e for both input and output words:\na∗= argmax\na∈A\ne(a)⊤W c (1)\nwhere W is an additional parameter matrix used\nto match dimensions and model any additional\nneeded transformation.\nFor the Attention Sum and Gated-Attention\nReaders the answer is computed by:\n∀a ∈ A , P (a|d, q) =\n∑\ni∈I(a,d)\nα i\na∗= argmax\na∈A\nP (a|d, q)\nwhere I(a, d) is the set of positions where a ap-\npears in context d.\n2.2 T raining Data Construction\nEach LAMBADA instance is divided into a con-\ntext (4.6 sentences on average) and a target sen-\ntence, and the last word of the target sentence\nis the target word to be predicted. The LAM-\nBADA dataset consists of development ( D EV ) and\ntest ( TEST ) sets; Paperno et al. (2016) also provide\na control dataset ( C O N TRO L ), an unﬁltered sample\nof instances from the BookCorpus.\nW e construct a new training dataset from the\nBookCorpus. W e restrict it to instances that con-\ntain the target word in the context. This decision\nis natural given our use of neural readers that as-\nsume the answer is contained in the passage. W e\nalso ensure that the context has at least 50 words\nand contains 4 or 5 sentences and we require the\ntarget sentences to have more than 10 words.\nSome neural readers require a candidate target\nword list to choose from. W e list all words in the\ncontext as candidate answers, except for punctu-\nation.2 Our new dataset contains 1,827,123 in-\nstances in total. W e divide it into two parts, a\ntraining set ( TR A IN ) of 1,618,782 instances and a\nvalidation set ( V A L) of 208,341 instances. These\ndatasets can be found at the authors’ websites.\n3 Experiments\nW e use the Stanford Reader (Chen et al., 2016),\nour modiﬁed Stanford Reader (Eq. 1), the Atten-\ntion Sum (AS) Reader (Kadlec et al., 2016), and\nthe Gated-Attention (GA) Reader (Dhingra et al.,\n2016). W e also add the simple features from W ang\net al. (2016) to the AS and GA Readers. The fea-\ntures are concatenated to the word embeddings in\nthe context. They include: whether the word ap-\npears in the target sentence, the frequency of the\nword in the context, the position of the word’s ﬁrst\noccurrence in the context as a percentage of the\ncontext length, and whether the text surrounding\nthe word matches the text surrounding the blank\nin the target sentence. For the last feature, we only\nconsider matching the left word since the blank is\nalways the last word in the target sentence.\nAll models are trained end to end without any\nwarm start and without using pretrained embed-\ndings. W e train each reader on TR A IN for a max\nof 10 epochs, stopping when accuracy on D EV de-\ncreases two epochs in a row . W e take the model\nfrom the epoch with max D EV accuracy and eval-\nuate it on TEST and C O N TRO L . V A L is not used.\nW e evaluate several other baseline systems in-\nspired by those of Paperno et al. (2016), but we fo-\ncus on versions that restrict the choice of answers\nto non-stopwords in the context. 3 W e found this\n2 This list of punctuation symbols is at https:\n//raw.githubusercontent.com/ZeweiChu/\nlambada-dataset/master/stopwords/\nshortlist-stopwords.txt\n3 W e use the stopword list from Richardson et al. (2013).\nMethod T E S T C O N T RO L\nall all context\nBaselines (Paperno et al., 2016)\nRandom in context 1.6 0 N/A\nRandom cap. in context 7.3 0 N/A\nn-gram 0.1 19.1 N/A\nn-gram + cache 0.1 19.1 N/A\nLSTM 0 21.9 N/A\nMemory network 0 8.5 N/A\nOur context-restricted non-stopword baselines\nRandom 5.6 0.3 2.2\nFirst 3.8 0.1 1.1\nLast 6.2 0.9 6.5\nMost frequent 11.7 0.4 8.1\nOur context-restricted language model baselines\nn-gram 10.7 2.2 15.6\nn-gram + cache 11.8 2.2 15.6\nLSTM 9.2 2.4 16.9\nOur neural reader results\nStanford Reader 21.7 7.0 49.3\nModiﬁed Stanford Reader 32.1 7.4 52.3\nAS Reader 41.4 8.5 60.2\nAS Reader + features 44.5 8.6 60.6\nGA Reader 45.4 8.8 62.5\nGA Reader + features 49.0 9.3 65.6\nHuman 86.0∗ 36.0† -\nT able 1: Accuracies on TEST and C O N TRO L\ndatasets, computed over all instances (“all”) and\nseparately on those in which the answer is in\nthe context (“context”). The ﬁrst section is from\nPaperno et al. (2016). ∗Estimated from 100\nrandomly-sampled D EV instances. †Estimated\nfrom 100 randomly-sampled C O N TRO L instances.\nstrategy to consistently improve performance even\nthough it limits the maximum achievable accuracy .\nW e consider two n-gram language model base-\nlines. W e use the SRILM toolkit (Stolcke, 2002)\nto estimate a 4-gram model with modiﬁed Kneser-\nNey smoothing on the combination of TR A IN and\nV A L. One uses a cache size of 100 and the other\ndoes not use a cache. W e use each model to score\neach non-stopword from the context. W e also\nevaluate an LSTM language model. W e train it on\nTR A IN , where the loss is cross entropy summed\nover all positions in each instance. The output\nvocabulary is the vocabulary of TR A IN , approxi-\nmately 130k word types. At test time, we again\nlimit the search to non-stopwords in the context.\nW e also test simple baselines that choose partic-\nular non-stopwords from the context, including a\nrandom one, the ﬁrst in the context, the last in the\ncontext, and the most frequent in the context.\n4 Results\nT able 1 shows our results. W e report accuracies\non the entirety of TEST and C O N TRO L (“all”), as\nwell as separately on the part of C O N TRO L where\nthe target word is in the context (“context”). The\nﬁrst part of the table shows results from Paperno\net al. (2016). W e then show our baselines that\nchoose a word from the context. Choosing the\nmost frequent yields a surprisingly high accuracy\nof 11.7%, which is better than all results from Pa-\nperno et al.\nOur language models perform comparably , with\nthe n-gram + cache model doing best. By forcing\nlanguage models to select a word from the con-\ntext, the accuracy on TEST is much higher than the\nanalogous models from Paperno et al., though ac-\ncuracy suffers on C O N TRO L .\nW e then show results with the neural readers,\nshowing that they give much higher accuracies on\nTEST than all other methods. The GA Reader with\nthe simple additional features (W ang et al., 2016)\nyields the highest accuracy , reaching 49.0%. W e\nalso measured the “top k” accuracy of this model,\nwhere we give the model credit if the correct an-\nswer is among the top k ranked answers. On TEST ,\nwe reach 65.4% top-2 accuracy and 72.8% top-3.\nThe AS and GA Readers work much better than\nthe Stanford Reader. One cause appears to be that\nthe Stanford Reader learns distinct embeddings for\ninput and answer words, as discussed above. Our\nmodiﬁed Stanford Reader, which uses only a sin-\ngle set of word embeddings, improves by 10.4%\nabsolute. Since the AS and GA Readers merely\nscore words in the context, they do not learn sepa-\nrate answer word embeddings and therefore do not\nsuffer from this effect.\nW e suspect the remaining accuracy difference\nbetween the Stanford and the other readers is due\nto the difference in the output function. The\nStanford Reader was developed for the CNN and\nDaily Mail datasets, in which correct answers are\nanonymized entity identiﬁers which are reused\nacross instances. Since the identiﬁer embeddings\nare observed so frequently in the training data,\nthey are frequently updated. In our setting, how-\never, answers are words from a large vocabulary ,\nso many of the word embeddings of correct an-\nswers may be undertrained. This could potentially\nbe addressed by augmenting the word embeddings\nwith identiﬁers to obtain some of the modeling\nbeneﬁts of anonymization (W ang et al., 2016).\nAll context restricted models yield poor accu-\nracies on the entirety of C O N TRO L . This is due\nto the fact that only 14.1% of C O N TRO L instances\nlabel # GA+ human\nsingle name cue 9 89% 100%\nsimple speaker tracking 19 84% 100%\nbasic reference 18 56% 72%\ndiscourse inference rule 16 50% 88%\nsemantic trigger 20 40% 80%\ncoreference 21 38% 90%\nexternal knowledge 24 21% 88%\nall 100 55% 86%\nT able 2: Labels derived from manual analysis of\n100 LAMBADA D EV instances. An instance can\nbe tagged with multiple labels, hence the sum of\ninstances across labels exceeds 100.\nhave the target word in the context, so this sets the\nupper bound that these models can achieve.\n4.1 Manual Analysis\nOne annotator, a native English speaker, sampled\n100 instances randomly from D EV , hid the ﬁnal\nword, and attempted to guess it from the context\nand target sentence. The annotator was correct\nin 86 cases. For the subset that contained the\nanswer in the context, the annotator was correct\nin 79 of 87 cases. Even though two annotators\nwere able to correctly answer all LAMBADA in-\nstances during dataset construction (Paperno et al.,\n2016), our results give an estimate of how often a\nthird would agree. The annotator did the same on\n100 instances randomly sampled from C O N TRO L ,\nguessing correctly in 36 cases. These results are\nreported in T able 1. The annotator was correct on\n6 of the 12 C O N TRO L instances in which the an-\nswer was contained in the context.\nW e analyzed the 100 LAMBADA D EV in-\nstances, tagging each with labels indicating the\nminimal kinds of understanding needed to answer\nit correctly . 4 Each instance can have multiple la-\nbels. W e brieﬂy describe each label below:\n• single name cue: the answer is clearly a name\naccording to contextual cues and only a single\nname is mentioned in the context.\n• simple speaker tracking: instance can be an-\nswered merely by tracking who is speaking\nwithout understanding what they are saying.\n• basic reference: answer is a reference to some-\nthing mentioned in the context; simple under-\nstanding/context matching sufﬁces.\n4 The annotations are available from the authors’ websites.\n• discourse inference rule: answer can be found\nby applying a single discourse inference rule,\nsuch as the rule: “ X left Y and went in search\nof Z” → Y ̸= Z.\n• semantic trigger: amorphous semantic informa-\ntion is needed to choose the answer, typically re-\nlated to event sequences or dialogue turns, e.g.,\na customer says “Where is the X?” and a sup-\nplier responds “W e got plenty of X”.\n• coreference: instance requires non-trivial coref-\nerence resolution to solve correctly , typically\nthe resolution of anaphoric pronouns.\n• external knowledge: some particular external\nknowledge is needed to choose the answer.\nT able 2 shows the breakdown of these labels\nacross instances, as well as the accuracy on each\nlabel of the GA Reader with features.\nThe GA Reader performs well on instances in-\nvolving shallower, more surface-level cues. In 9\ncases, the answer is clearly a name based on con-\ntextual cues in the target sentence and there is only\none name in the context; the reader answers all but\none correctly . When only simple speaker tracking\nis needed (19 cases), the reader gets 84% correct.\nThe hardest instances are those that involve\ndeeper understanding, like semantic links, coref-\nerence resolution, and external knowledge. While\nexternal knowledge is difﬁcult to deﬁne, we chose\nthis label when we were able to explicitly write\ndown the knowledge that one would use when\nanswering the instances, e.g., one instance re-\nquires knowing that “when something explodes,\nnoise emanates from it”. These instances make\nup nearly a quarter of those we analyzed, making\nLAMBADA a good task for work in leveraging ex-\nternal knowledge for language understanding.\n4.2 Discussion\nOn C O N TRO L , while our readers outperform our\nother baselines, they are outperformed by the lan-\nguage modeling baselines from Paperno et al. This\nsuggests that though we have improved the state of\nthe art on LAMBADA by more than 40% absolute,\nwe have not solved the general language modeling\nproblem; there is no single model that performs\nwell on both TEST and C O N TRO L . Our 36% esti-\nmate of human performance on C O N TRO L shows\nthe difﬁculty of the general problem, and reveals a\ngap of 14% between the best language model and\nhuman accuracy .\nA natural question to ask is whether applying\nneural readers is a good direction for this task,\nsince they fail on the 17% of instances which\ndo not have the target word in the context. Fur-\nthermore, this subset of LAMBADA may in fact\ndisplay the most interesting and challenging phe-\nnomena. Some neural readers, like the Stanford\nReader, can be easily used to predict target words\nthat do not appear in the context, and the other\nreaders can be modiﬁed to do so. Doing this will\nrequire a different selection of training data than\nthat used above. However, we do wish to note that,\nin addition to the relative rarity of these instances\nin LAMBADA, we found them to be challenging\nfor our annotator (who was correct on only 7 of\nthe 13 in this subset).\nW e note that TR A IN has similar characteristics\nto the part of C O N TRO L that contains the answer\nin the context (the ﬁnal column of T able 1). W e\nﬁnd that the ranking of systems according to this\ncolumn is similar to that in the TEST column. This\nsuggests that our simple method of dataset cre-\nation could be used to create additional training or\nevaluation sets for challenging language modeling\nproblems like LAMBADA, perhaps by combining\nit with baseline suppression (Onishi et al., 2016).\n5 Conclusion\nW e constructed a new training set for LAMBADA\nand used it to train neural readers to improve the\nstate of the art from 7.3% to 49%. W e also pro-\nvided results with several other strong baselines\nand included a manual evaluation in an attempt\nto better understand the phenomena tested by the\ntask. Our hope is that other researchers will seek\nmodels and training regimes that simultaneously\nperform well on both LAMBADA and C O N TRO L ,\nwith the goal of solving the general problem of\nlanguage modeling.\nAcknowledgments\nW e thank Denis Paperno for answering our ques-\ntions about the LAMBADA dataset and we thank\nNVIDIA Corporation for donating GPUs used in\nthis research.\nReferences\nDanqi Chen, Jason Bolton, and Christopher D. Man-\nning. 2016. A thorough examination of the\nCNN/Daily Mail reading comprehension task. In\nProc. of ACL.\nKyunghyun Cho, Bart van Merrienboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Y oshua Bengio. 2014. Learning\nphrase representations using RNN encoder-decoder\nfor statistical machine translation. In Proc. of\nEMNLP.\nBhuwan Dhingra, Hanxiao Liu, William W . Cohen, and\nRuslan Salakhutdinov. 2016. Gated-attention read-\ners for text comprehension. arXiv preprint.\nKarl Moritz Hermann, T om Koisk, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. T eaching machines to read\nand comprehend. In Proc. of NIPS.\nFelix Hill, Antoine Bordes, Sumit Chopra, and Jason\nW eston. 2016. The Goldilocks principle: Reading\nchildren’s books with explicit memory representa-\ntions. In Proc. of ICLR.\nSepp Hochreiter and J ¨ urgen Schmidhuber. 1997. Long\nshort-term memory. Neural Computation, 9(8).\nRudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan\nKleindienst. 2016. T ext understanding with the at-\ntention sum reader network. In Proc. of ACL.\nT akeshi Onishi, Hai W ang, Mohit Bansal, Kevin Gim-\npel, and David McAllester. 2016. Who did What: A\nlarge-scale person-centered cloze dataset. In Proc.\nof EMNLP.\nDenis Paperno, Germn Kruszewski, Angeliki Lazari-\ndou, Quan Ngoc Pham, Raffaella Bernardi, San-\ndro Pezzelle, Marco Baroni, Gemma Boleda, and\nRaquel Fernndez. 2016. The LAMBADA dataset:\nW ord prediction requiring a broad discourse context.\nIn Proc. of ACL.\nMatthew Richardson, Christopher JC Burges, and Erin\nRenshaw . 2013. MCT est: A challenge dataset for\nthe open-domain machine comprehension of text. In\nProc. of EMNLP.\nAndreas Stolcke. 2002. SRILM-an extensible lan-\nguage modeling toolkit. In Proc. of Interspeech.\nHai W ang, T akeshi Onishi, Kevin Gimpel, and David\nMcAllester. 2016. Emergent logical structure\nin vector representations of neural readers. arXiv\npreprint.\nY ukun Zhu, Ryan Kiros, Richard Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio T orralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nT owards story-like visual explanations by watching\nmovies and reading books. In Proc. of ICCV."
}