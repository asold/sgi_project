{
  "title": "Improving Generalization of Transformer for Speech Recognition with Parallel Schedule Sampling and Relative Positional Embedding",
  "url": "https://openalex.org/W2985287635",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2137733982",
      "name": "Zhou Pan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223233434",
      "name": "Fan, Ruchao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1976031155",
      "name": "Chen Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116927284",
      "name": "Jia Jia",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2962778134",
    "https://openalex.org/W2936078256",
    "https://openalex.org/W2962824709",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2919624000",
    "https://openalex.org/W2963729818",
    "https://openalex.org/W2972439411",
    "https://openalex.org/W2193413348",
    "https://openalex.org/W2963827914",
    "https://openalex.org/W2963414781",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W854541894",
    "https://openalex.org/W1586532344",
    "https://openalex.org/W2964260331",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2746192915",
    "https://openalex.org/W648786980",
    "https://openalex.org/W2950913377",
    "https://openalex.org/W1828163288",
    "https://openalex.org/W2514741789",
    "https://openalex.org/W2972818416",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3008181812",
    "https://openalex.org/W3016010032",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W2963236196",
    "https://openalex.org/W2963850025"
  ],
  "abstract": "Transformer has shown promising results in many sequence to sequence transformation tasks recently. It utilizes a number of feed-forward self-attention layers to replace the recurrent neural networks (RNN) in attention-based encoder decoder (AED) architecture. Self-attention layer learns temporal dependence by incorporating sinusoidal positional embedding of tokens in a sequence for parallel computing. Quicker iteration speed in training than sequential operation of RNN can be obtained. Deeper layers of the transformer also make it perform better than RNN-based AED. However, this parallelization ability is lost when applying scheduled sampling training. Self-attention with sinusoidal positional embedding may cause performance degradations for longer sequences that have similar acoustic or semantic information at different positions as well. To address these problems, we propose to use parallel scheduled sampling (PSS) and relative positional embedding (RPE) to help the transformer generalize to unseen data. Our proposed methods achieve a 7% relative improvement for short utterances and a 70% relative gain for long utterances on a 10,000-hour Mandarin ASR task.",
  "full_text": "IMPROVING GENERALIZATION OF TRANSFORMER FOR SPEECH RECOGNITION\nWITH PARALLEL SCHEDULED SAMPLING AND RELATIVE POSITIONAL EMBEDDING\nPan Zhou1∗, Ruchao Fan2†, Wei Chen1, Jia Jia3\n1AI Interaction Division, Sogou Inc., Beijing, P.R.China\n2Department of Electrical and Computer Engineering, University of California, Los Angeles, USA\n3Department of Computer Science and Technology, Tsinghua University, Beijing, P.R.China\n{zhoupan,chenweibj8871}@sogou-inc.com, fanruchao@g.ucla.edu, jjia@mail.tsinghua.edu.cn\nABSTRACT\nTransformer has shown promising results in many sequence\nto sequence transformation tasks recently. It utilizes a number\nof feed-forward self-attention layers to replace the recurrent\nneural networks (RNN) in attention-based encoder decoder\n(AED) architecture. Self-attention layer learns temporal de-\npendence by incorporating sinusoidal positional embedding\nof tokens in a sequence for parallel computing. Quicker it-\neration speed in training than sequential operation of RNN\ncan be obtained. Deeper layers of the transformer also make\nit perform better than RNN-based AED. However, this par-\nallelization ability is lost when applying scheduled sampling\ntraining. Self-attention with sinusoidal positional embedding\nmay cause performance degradations for longer sequences\nthat have similar acoustic or semantic information at differ-\nent positions as well. To address these problems, we propose\nto use parallel scheduled sampling (PSS) and relative posi-\ntional embedding (RPE) to help the transformer generalize to\nunseen data. Our proposed methods achieve a 7% relative\nimprovement for short utterances and a 70% relative gain for\nlong utterances on a 10,000-hour Mandarin ASR task.\nIndex Terms— speech recognition, transformer, parallel\nscheduled sampling, relative positional embedding\n1. INTRODUCTION\nEnd-to-end (E2E) ASR aims to simplify conventional ASR by\njointly learning acoustic model (AM), pronunciation model\n(PM) and language model (LM) within one single neural net-\nwork and has achieved promising results. Connectionist Tem-\nporal Classiﬁcation (CTC) [1, 2], Recurrent Neural Network\nTransducer (RNN-T) [3, 4, 5], Recurrent Neural Aligner [6,\n7], Segment Neural Transduction [8] and Attention-based en-\ncoder decoder (AED) models [9, 10, 11] are such E2E models\nthat are well explored in the literature.\n∗This work was done as a postdoctor at Tsinghua University before join-\ning Sogou.\n†Participated in this work as an intern at Sogou before entering UCLA.\nAED model [9, 10] was examined on many speech tasks.\nAED consists of an encoder, a decoder and an attender which\nbridges encoder and decoder. The encoder often uses RNNs\nto capture higher level temporal features from raw input\nspectral features. The decoder generates tokens sequentially\nconditioned on the context tokens and the output of atten-\nder. While ground truth labels are used as context tokens\nduring training, the context tokens for decoder input are the\npredicted tokens when doing inference. Scheduled sampling\n(SS) [12] is often adopted to compensate the discrepancy\nbetween training and inference. Step-by-step predictions of\nRNN are also suitable for SS to be integrated at each token\nlevel without sacriﬁcing training efﬁciency. However, this se-\nquential feature of RNN makes the training time-consuming.\nAn alternative to RNNs in AED is self-attention layer\nwhich was proposed in Transformer [13] for neural machine\ntranslation (NMT). The self-attention architecture learns tem-\nporal and contextual dependency inside the input sequence\nby employing temporal attention on the input feature itself.\nTransformer has been applied to E2E speech recognition sys-\ntems [14, 15, 16, 17] and has achieved promising results.\nThe transformer-based E2E ASR relies on feed-forward self-\nattention components and can be trained faster with more\nparallelization than RNN based AED. However, the non-\nrecurrent parallel forward process of decoder in transformer\nalso makes it inefﬁcient to utilize scheduled sampling at\ntraining stage. Sequence order of speech, which can be\nrepresented by recurrent processing of input features, is an\nimportant distinction. Although absolute positional embed-\nding (APE) added to input features can make the transformer\nbe aware of the order in a sequence, performance degradation\nof long sentences of AED, as mentioned in [18], may become\nmore serious for transformer. Compared to RNN-based AED,\nwe indeed found that vanilla transformer is more sensitive to\nsentences that are longer than those in the training set.\nIn this paper, we propose to use parallel scheduled sam-\npling (PSS) for efﬁcient SS in transformer. More precisely,\nwe mixed the ground truth label with the output of Kaldi-\nbased hybrid model or transformer itself to form a new input\narXiv:1911.00203v2  [cs.CL]  30 Nov 2020\nto the decoder, so as to simulate the error distribution of trans-\nformer during inference and maintain the parallelization of\ndecoder. Experimental results show that we could get about\n7% relative gain compared to teacher-forcing training.\nTransformer also makes lots of deletion errors for long ut-\nterances. Characters whose positions exceed the max length\nof training data are likely to be deleted. We name this deletion\nas tail deletion (TD). Other deletions appear between similar\nacoustic or semantic segments and we call such deletions as\ninternal deletion (ID). We argue that the attention mechanism\naccessing the whole sentence makes the model confused of\nfocusing on unseen long sentences. In order to restrict the at-\ntention position range to avoid model from this confusion, rel-\native position embedding is proposed to help the model gen-\neralize to unseen longer sentences and 30% absolute gain is\nobtained on test set with utterances longer than training set.\nThe rest of paper is organized as follows. Section 2 brieﬂy\nreviews transformer used in ASR tasks. The proposed meth-\nods are described in details in Section 3. Experimental setup\nand results are presented in Section 4 and the paper is con-\ncluded with our ﬁndings and future work in Section 5.\n2. TRANSFORMER-BASED E2E ASR\nThe AED used in ASR is common to other sequence to se-\nquence task. The encoder transforms an input sequence of\nspectral features (x1, ...,xn) to a higher level representation.\nConditioned on the high level representation, the decoder\npredicts output sequence (y1, ..., yu) of speech modeling\nunits, such as phones, syllables, characters or sub-words\n(BPE [19]), auto-regressively. Transformer follows AED ar-\nchitecture and uses stacked multi-head attention (MHA) and\npoint-wise fully connected feed-forward network (FFN) for\nboth encoder and decoder [13].\n2.1. Multi-head Attention\nThe attention layer used in transformer uses the ”scaled dot-\nproduct attention” with the following form:\nAtt(Q, K, V) =softmax(QKT\n√dk\n)V, (1)\nwhere Q and K are queries and keys of dimension dk, V are\nvalues of dimension dv. In order to allow the model to pay\nattention to different representation subspace, [13] proposed\nto use multi-head attention to perform parallel attention:\nMHA (Q, K, V) =Concat[H1, H2, ..., Hh]WO, (2)\nHi = Att(QWQ\ni , KWK\ni , V WV\ni ), (3)\nwhere WQ\ni ∈Rdm×dk ,WK\ni ∈Rdm×dk ,WV\ni ∈Rdm×dv and\nWO ∈Rhdv×dm are learnable weight matrices, h is the total\nnumber of attention heads, Hi is the output of the i-th atten-\ntion head, dk is the individual dimension for each attention\nhead, dm is the model feature dimension.\nFor encoder and self-attention layers in decoder, all of the\nkeys, values and queries, K, V, Q, come form the output fea-\ntures of previous layer. In the source attention layers, queries\ncome from the previous decoder layer, and the keys and val-\nues come from the ﬁnal output of encoder.\n2.2. Absolute Positional Embedding\nTransformer contains no recurrence and convolution. In order\nto make use of the order of the sequence, input representations\nare added with absolute positional encoding before feeding to\nthe encoder and decoder stacks.\nPE(pos,i) =\n{\nsin(pos/10000i/dm ), i is even,\ncos(pos/10000(i−1)/dm ), i is odd.\n(4)\nwhere pos represents the absolute position in sequence, i rep-\nresents the i-th dimension of input feature. For more details\nplease refer to [13, 14].\n3. PROPOSED METHODS FOR IMPROVING\nGENERALIZATION OF TRANSFORMER\n3.1. Parallel Scheduled Sampling\nScheduled sampling is a training strategy to help the auto-\nregressive models be robust to prediction errors during infer-\nence. However, vanilla transformer, when applied with sched-\nuled sampling, is slower than teacher-forcing training due to\nthe destructed parallelism and duplicated calculations. In or-\nder to alleviate the problem, we implement a parallel sched-\nuled sampling (PSS) mechanism, which obtains all the input\ntokens of decoder in advance by simulating the error distribu-\ntion of inference during training. The PSS mechanism is very\ncompatible with the training of transformer. Speciﬁcally, two\nerror distribution simulation methods for PSS are proposed.\nUnlike scheduled sampling in [20], our methods consider the\nsampling at each decoding step without losing efﬁciency.\n3.1.1. PSS with simulation from chain model\nIn this method, the error distribution of transformer outputs\nis simulated with the decoding results of the Kaldi-based\nchain model (CM) [21]. For each utterance, we ﬁrst ob-\ntain its hypothesized text ˆy = ( ˆy1, ...,ˆyq) from a previously\ntrained chain model, as the simulated outputs of transformer.\nThen, ˆy is mixed with ground truth y = (y1, ..., yu) ac-\ncording to a teacher-force rate to get the ﬁnal decoder input\ny = (y1, ...,yu). The teacher-force rate represents the proba-\nbility of choosing the tokens in the true labely as the decoder\ninput and varies according to the schedule with the following\npiece-wise linear function:\nP(i) =max(min(1, 1 −(1 −Pmin) ∗ i −Nst\nNed −Nst\n), Pmin)\n(5)\nwhere Pmin is the minimum teacher-force probability which\nis usually not zero to prevent under-ﬁtting over training set,\nNst and Ned represent the starting and the ending step in the\nschedule respectively, and i is the training step. The step i in\nEq. 5 can be either epoch or batch.\nAfter obtaining the teacher-force rate, sentence or token\nlevel token mixing methods can be considered. Thesentence\nlevel mixing is more explicit. It is achieved by selecting either\nˆy or y as y according to the teacher-force rate. On the con-\ntrary, the token level is closer to the real scheduled sampling\nby considering mixing of the token at each step. The process\ncan be formulated as:\nyj =\n\n\n\nyj with P(i)\nˆyj with 1 −P(i) and j≤q\npad with 1 −P(i) and j > q\n(6)\nwhere j = 1, 2, ··· , uand pad is the padding token to ﬁll\nup short sentences in a batch. The token sequences y in this\npaper are sequences of scalar. At last, the mixing decoder\ninput y is fed to the transformer at the same time to achieve\nparallelization. Although there exists an increased time cost\nof obtaining the hybrid system results of each utterance, it is\nmuch faster for PSS training than the training of the trans-\nformer with traditional scheduled sampling.\nOur method is related to [22] which adopts back-translation\nto get source text with errors to be used for training. Sched-\nuled sampling aims to make AED robust to several decoding\nerrors. Although there is no evidence that the error distribu-\ntions of hybrid system have any relation to that of AED, hy-\nbrid system results, containing several errors, can be viewed\nas a special kind of data augmentation technique. So it should\nhelp the model to generalize.\n3.1.2. PSS with simulation from self-decoding\nThe second method we describe here is to simulate the er-\nror distribution by the results decoding from transformer it-\nself. The intuitive way is to generate hypothesized text ˆy for\ntrain set in advance with teacher-forcing-trained model. Fur-\nthermore, online training mechanism is applied in our exper-\niments, in which the ˆy are generated batch by batch with the\nnewest updated model. To simplify the online training pro-\ncess, the hypothesized results are obtained by greedy search\ndecoding with ground truth y as decoder input. Since the\nonly difference between this method and the method in Sec-\ntion 3.1.1 is how we simulate the error distribution of trans-\nformer output, the same scheduled strategies as in Eq. (5) is\nused.\nRecently, we ﬁnd that the method in this part is a spe-\ncial case in [23], which is proposed for machine translation.\nThey also proposed a parallel scheduled sampling mechanism\nto achieve parallelism in transformer, where they consider N\ntimes of token mixing. Hence, we reveal that teacher forc-\ning training is the special case when N = 0and our method\nis the case when N = 1. To associate them with conven-\ntional scheduled sampling, we can make the following rea-\nsoning process. When N = 1, only the ﬁrst generated char-\nacter y1 is sampled from decoding. If y1 is fed to decoder\nand repeat the above operation, both y1 and y2 are sampled\nreasonably. Therefore, repeating above process for N >= u\ntimes, PSS with self-decoding result is equivalent to the con-\nventional scheduled sampling. The time cost will also be ap-\nproximately the same as conventional one.\n3.2. Relative Positional Embedding\nWe observe many deletion errors for speech transformer, such\nas TD and ID in long utterances. Many works attempted\nto solve the long sequence generation problem by increasing\ntraining data length. In this direction, their main work focused\non memory controlling by sparse attention mechanism or seg-\nmental transformer [24, 25, 26]. [27] proposed to use diverse\nacoustic data and LSTM state manipulation to simulate long\naudio for training to improve generalization on unseen long\nspeech. Using longer training sentences is indeed a solution\nto the TD problem but the ID problem may not be solved be-\ncause of confusion among similar segments in the global self-\nattention. How to make model generalize well when there is\nonly short data is our focus.\nThe original APE proposed for NMT may not be suitable\nfor speech recognition applications, since alignment between\ntext and speech is monotonic. APE may make model attend\nto wrong positions for long utterance with similar segments\nsince it is never trained on such long sentences. Moreover\nthe attention scores only focus on a local area for speech\nrecognition. Thus we want to use relative positional em-\nbedding for vanilla speech transformer to try to explore how\nthe positional embedding affects long sentence recognition\nperformance. The idea of relative positional encodings has\nbeen previously explored in the context of machine trans-\nlation [28], music generation [25] and language modelling\n[26]. Recently, transformer transducer with RPE [29] is\nproposed for streaming speech recognition. The main dif-\nferent between ours and [29] is that we explore RPE in the\nattention based encoder decoder framework other than RNN\ntransducer framework and we systematically evaluate its im-\npact on speech recognition especially for the model collapse\nproblem on unseen long data mentioned above.\nOur main purpose is to restrict the position range and\nstrengthen the relative position relationship within the range.\nThe TD may be alleviated by RPE by letting the model learn\nhow to pay attention to relative context. Furthermore, fewer\nsimilar segments appear in restricted position range, result-\ning in solving the ID problem. Suppose the input of a self-\nattention layer is z = (z1, ...,zT ) which is a sequence of\nvector, we deﬁne the relative position between eachzi and zj\nas aij. To restrict the position range, we consider a maximum\nabsolute value of relative positionk and acquire 2×k+1 em-\nbeddings, denoted as w = (w−k, ··· , wk). Then, any relative\npositional embedding between two inputs can be formulated\nas:\naij = wmax(−k,min(k,j−i)) (7)\nNext, the relative position embedding aij is incorporated to\nthe similarity computation (softmax input) in Eq. (1) as fol-\nlows:\neij = ziWQ(zjWK + aij)T\n√dk\n(8)\nwhere the notations are consistent with the equations in Sec-\ntion 2. Eq. (8) can be modiﬁed by applying distributional law\nof matrix split into two terms for efﬁcient training:\neij = ziWQ(zjWK)T + ziWQ(aij)T\n√dk\n(9)\nWe use different RPE across layers and share RPE among\nattention heads within the same layer. The ﬁrst term is equiv-\nalent to the original similarity computation. The second term\ncan be calculated with tensor reshaping, which means a ma-\ntrix with size bhn ×dk multiply a matrix with size dk ×n.\nAnd then it is reshaped to ﬁt with the ﬁrst term. b is the batch\nsize, n is the sequence length.\n4. EXPERIMENT\nOur experiments are conducted on a 10,000 hour Chinese\nspeech dictation data. We discard sentence longer than 40\nChinese characters. The main test sets include ∼33K short\nutterances (SU) and ∼5.6K long utterances (LU). The length\ndistribution is listed in Table 1.\n71 dimension fbanks are extracted every 10 ms within\na 25-ms window using the conventional ASR front end.\nEvery four consecutive frames are stacked to form a 284-\ndimensional feature vector and we jump 4 frames to get\nshorter input feature sequence for transformer models.\nTable 1. Sentence number of different length in training and\ntest sets.\n#char. ≤10 (11,40) (41,80) ≥80\ntraining 6,996,966 4,807,048 974 0\nSU 26863 6710 0 0\nLU 0 1 5407 248\n6812 Chinese characters are used as modeling unit. All\nmodels are trained by optimizing the cross entropy via adam\noptimizer. Label smoothing (LS) is set to 0.1 during training\nto improve performance. We also apply 10% dropout rate to\nthe output of each sub-layer, before it is added to the sub-\nlayer input and normalized. The total training epoch is ﬁxed\nto 12. We train our model from random initialization with\nan initial learning rate of 0.0002 and halve it from epoch 7.\nBeam search with beam width of 5 is used without external\nlanguage models to evaluate our model on test sets.\nAll our transformers contain 5 MHA-FFN blocks and 3\nMHA-MHA-FFN blocks with 16 attention heads. The size\nof FFN layer is 2048 and dm = 768. A two layer FFN with\n2048 and 768 nodes is placed before the ﬁrst encoder block.\nWith the same 10,000-hour training data, We also train an\nLAS and a streaming TDNN-LSTM chain model for compar-\nison, which are denoted as B0 and B2 respectively. The LAS\nis constituted of 4-layer BLSTM encoder and 1 LSTM de-\ncoder with the same base settings as in [30]. The CER of our\nbase models are presented in the ﬁrst block of Table 2. Com-\nparing with B0 and B1, we ﬁnd that transformer performs\nbetter than LAS on SU but degrades a lot on LU.\nTable 2. Performance in CER [%] of PSS for transformer.\nPmin is teacher force rate andN is number of decoding pass.\nExpID Model Pmin N SU LU\nB0 LAS 0.8 - 10.78 23.32\nB1 transformer 1.0 - 9.57 42.41\nB2 CM - - 10.52 8.00\nE1 PSS-CM 0.7 - 8.9 41.28\n0.8 - 8.93 41.19\nE2 PSS-ofﬂine 0.8 - 9.00 40.00\nE3 PSS-online 0.5 1 8.88 42.47\n0.5 2 8.93 41.52\n4.1. Improvements by PSS\nDue to lack of space, we only report experiments with token\nand batch style as mentioned in section 3.1.1. Using the\nmodel in B1 as a starting point, we perform several PSS ex-\nperiments. First, we perform PSS with decoded text results\nfrom TDNN-LSTM chain model trained with 50,000 hours\nof data, numbered as E1. Since CM used here is trained\nwith more data than B2, it performance better on SU and LU,\nwhich is 6.65% and 5.32% respectively. With Pmin = 0.8,\nwe obtain a 8.93% of CER on SU. One may argue that this\ngain may come from the larger training data of the CM that\nprovides decoding results for transformer. So we conduct E2\nwith hypothesized text of training data generated by beam\nsearch decoding with transformer of B1. E2 achieves a sim-\nilar result with E1. In E3, we generate hypothesized text\nduring training stage in an online fashion to make it closer to\nreal scheduled sampling process. We reach a CER of 8.88%\non SU with N = 1 and Pmin = 0.5, about a 7.2% relative\nreduction. Better results achieved by E3 show that decoding\nas training proceeds is a better choice for parallel scheduled\nsampling.\nFig. 1. Decoding examples, showing tail deletions and internal deletions of B1, on LU test set for various systems.\n4.2. Improvements by RPE\nAs mentioned above, LAS generalizes better on LU than\ntransformer because of its iterative process of sequence,\nwhich learns a better order information. It reveals that order\ninformation learned by self-attention layers with APE gen-\neralize poorly to sequences that are not seen in the training\nset.\nExperimental results are summarized in Table 3. We ﬁrst\ninvestigate APE in encoder. In E5, we remove APE in en-\ncoder and keep APE in decoder. Although SU drops a lot,\nLU even improves, which indicates APE can not generalize\nwell on long sentences. In E6, we replace the ﬁxed APE\nwith a learnable token ID APE in encoder, just like the to-\nken embedding in the decoder. Token ID embedding is simi-\nlar on SU but worse on LU than sin/cos, which may because\ntoken ID can not be learned for positions not seen in data\nand sin/cos is ﬁxed in advance. Then we try to introduce\nRPE in encoder ( E7) and decoder ( E8) gradually. When\nRPE is used in each MHA layer besides adding APE to in-\nput feature of encoder block, LU improves from 42.41% to\n33.56%. If we discard APE and set k = 10in encoder, it is\nfurther improved to 29.87%. k = 10represents attending to\nabout 2 Chinese characters length (40 frames) on both sides\nof current query of self-attention in encoder. As k = 10per-\nforms better than k = 40, we guess it is enough for acoustic\nself-attention representation learning in transformer encoder.\nThe CER continues to decrease to 12.73% as we utilize a 2-\ncharacter-range RPE in the decoder MHA to replace APE.\nWith RPE, although such long sentences have not been pre-\nsented to the model, the local attention relationship has al-\nready been learned. Besides, the possibility for similar seg-\nments to appear in the near context becomes smaller, which\ndecreases the burden of model to distinguish the similar seg-\nments. Thus, RPE helps to decrease TD and ID. This also\nindicates that local and relative position is more suitable for\nspeech recognition. Overall, with relative position embed-\nding, we lower the CER of LU set from 42.41% to 12.73%,\nan absolute 30% gain.\n4.3. Combination of PSS and RPE\nThen, E8+E3 in last row of Table 3 shows the results of com-\nbination of RPE and PSS with settings in ﬁrst row of E3. SU\ncontinues to be improved to 8.9% and LU stays unchanged.\nThe mixing process with N = 1may be the reason of none\nTable 3. Effects in CER [%] of APE and RPE for transformer.\n10-4 represents position range is 10 for encoder and 4 for\ndecoder. k is the relative position range in RPE.\nExpID Enc Dec k SU LU\nB1 APE APE - 9.57 42.41\nE5 - APE - 10.31 37\nE6 token ID APE APE - 9.58 47.65\nE7 APE +RPE 20 9.17 33.56\nRPE 40 9.38 32.22\nRPE APE 20 9.17 31.17\nRPE 10 9.24 29.87\nE8 RPE RPE 10-2 9.31 12.73\n10-4 9.2 12.89\nE8+E3 RPE RPE 10-4 8.9 12.89\nimprovement on LU. Compared to the base transformer B1,\nthere is a 7.0% relative improvement of CER on SU and a\n70% relative improvement on LU.\nFig. 2. Error number comparison on the LU test set for vari-\nous systems.\nError analysis on LU set is shown in Fig 2, from which we\nobserve that deletions drop signiﬁcantly fromB1 to E8+ E3.\nLastly, we examine some decoding examples in Fig. 1. The\nlength of B1 output is about 40 characters and characters after\n40 is deleted. Attention jump from the ﬁrst ”four thousands”\nto the second ”four thousands” in the ﬁrst example can also\nbe observed. Tail deletions and internal deletions of vanilla\ntransformer in B1, blue characters in labels, are recognised\nby RPE transformer in E8 +E3 which are shown in red.\nAlso, RPE alleviates the self-loop problem in AED decoding\nas shown in green in the second example.\n5. CONCLUSIONS AND FUTURE WORK\nIn this work, we apply PSS and RPE to successfully improve\nthe generalization ability of transformer. PSS simulates the\nreal sampling process of SS efﬁciently and alleviate the dis-\ncrepancy betweent training and testing. RPE endues trans-\nformer the ability to distinguish similar acoustic and seman-\ntic segments and ease the model collapse problem for unseen\nlong sentences. Experiments show 7% relative improvement\nfor short utterances and a 70% relative gain for long utterance\nis achieved. To further improve the generalization of trans-\nformer in speech, attention restriction in source attention is\nour next direction.\n6. REFERENCES\n[1] Alex Graves, Santiago Fern ´andez, Faustino Gomez, and\nJ¨urgen Schmidhuber, “Connectionist temporal classiﬁ-\ncation: labelling unsegmented sequence data with recur-\nrent neural networks,” in Proceedings of the 23rd inter-\nnational conference on Machine learning. ACM, 2006,\npp. 369–376.\n[2] Dario Amodei, Sundaram Ananthanarayanan, Rishita\nAnubhai, Jingliang Bai, Eric Battenberg, Carl Case,\nJared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang\nChen, et al., “Deep speech 2: End-to-end speech recog-\nnition in English and Mandarin,” in International con-\nference on machine learning, 2016, pp. 173–182.\n[3] Alex Graves, “Sequence transduction with recurrent\nneural networks,” arXiv preprint arXiv:1211.3711,\n2012.\n[4] Eric Battenberg, Jitong Chen, Rewon Child, Adam\nCoates, Yashesh Gaur Yi Li, Hairong Liu, Sanjeev\nSatheesh, Anuroop Sriram, and Zhenyao Zhu, “Ex-\nploring neural transducers for end-to-end speech recog-\nnition,” in 2017 IEEE Automatic Speech Recognition\nand Understanding Workshop (ASRU). IEEE, 2017, pp.\n206–213.\n[5] Kanishka Rao, Has ¸im Sak, and Rohit Prabhavalkar,\n“Exploring architectures, data and units for streaming\nend-to-end speech recognition with RNN-transducer,”\nin 2017 IEEE Automatic Speech Recognition and Un-\nderstanding Workshop (ASRU). IEEE, 2017, pp. 193–\n199.\n[6] Hasim Sak, Matt Shannon, Kanishka Rao, and\nFranc ¸oise Beaufays, “Recurrent neural aligner: An\nencoder-decoder neural network model for sequence to\nsequence mapping.,” in Interspeech, 2017, pp. 1298–\n1302.\n[7] Linhao Dong, Shiyu Zhou, Wei Chen, and Bo Xu, “Ex-\ntending recurrent neural aligner for streaming end-to-\nend speech recognition in Mandarin,” arXiv preprint\narXiv:1806.06342, 2018.\n[8] Lei Yu, Jan Buys, and Phil Blunsom, “Online seg-\nment to segment neural transduction,” arXiv preprint\narXiv:1609.08194, 2016.\n[9] Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho,\nand Yoshua Bengio, “End-to-end continuous speech\nrecognition using attention-based recurrent NN: First re-\nsults,” arXiv preprint arXiv:1412.1602, 2014.\n[10] William Chan, Navdeep Jaitly, Quoc Le, and Oriol\nVinyals, “Listen, attend and spell: A neural network\nfor large vocabulary conversational speech recognition,”\nin 2016 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 2016,\npp. 4960–4964.\n[11] Chung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Ro-\nhit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, An-\njuli Kannan, Ron J Weiss, Kanishka Rao, Ekaterina Go-\nnina, et al., “State-of-the-art speech recognition with\nsequence-to-sequence models,” in 2018 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP). IEEE, 2018, pp. 4774–4778.\n[12] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam\nShazeer, “Scheduled sampling for sequence prediction\nwith recurrent neural networks,” in Advances in Neural\nInformation Processing Systems, 2015, pp. 1171–1179.\n[13] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin, “Attention is all you need,”\n2017.\n[14] Linhao Dong, Shuang Xu, and Bo Xu, “Speech-\ntransformer: a no-recurrence sequence-to-sequence\nmodel for speech recognition,” in 2018 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP). IEEE, 2018, pp. 5884–5888.\n[15] Shiyu Zhou, Linhao Dong, Shuang Xu, and Bo Xu,\n“Syllable-based sequence-to-sequence speech recogni-\ntion with the transformer in Mandarin Chinese,” Proc.\nInterspeech 2018, pp. 791–795, 2018.\n[16] Shiyu Zhou, Linhao Dong, Shuang Xu, and Bo Xu, “A\ncomparison of modeling units in sequence-to-sequence\nspeech recognition with the transformer on Mandarin\nChinese,” in International Conference on Neural Infor-\nmation Processing. Springer, 2018, pp. 210–220.\n[17] Shigeki Karita, Nanxin Chen, Tomoki Hayashi, Takaaki\nHori, Hirofumi Inaguma, Ziyan Jiang, Masao Someki,\nNelson Enrique Yalta Soplin, Ryuichi Yamamoto, Xi-\naofei Wang, et al., “A comparative study on transformer\nvs rnn in speech applications,” in 2019 IEEE Auto-\nmatic Speech Recognition and Understanding Workshop\n(ASRU). IEEE, 2019, pp. 449–456.\n[18] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk,\nKyunghyun Cho, and Yoshua Bengio, “Attention-based\nmodels for speech recognition,” in Advances in neural\ninformation processing systems, 2015, pp. 577–585.\n[19] Anoop Kunchukuttan and Pushpak Bhattacharyya,\n“Learning variable length units for SMT between re-\nlated languages via byte pair encoding,” arXiv preprint\narXiv:1610.06510, 2016.\n[20] Jie Li, Xiaorui Wang, Yan Li, et al., “The speech-\ntransformer for large-scale Mandarin Chinese speech\nrecognition,” in ICASSP 2019-2019 IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2019, pp. 7095–7099.\n[21] Daniel Povey, Vijayaditya Peddinti, Daniel Galvez, Pe-\ngah Ghahremani, Vimal Manohar, Xingyu Na, Yim-\ning Wang, and Sanjeev Khudanpur, “Purely sequence-\ntrained neural networks for asr based on lattice-free\nmmi.,” in Interspeech, 2016, pp. 2751–2755.\n[22] Rico Sennrich, Barry Haddow, and Alexandra Birch,\n“Improving neural machine translation models with\nmonolingual data,” arXiv preprint arXiv:1511.06709,\n2015.\n[23] Daniel Duckworth, Arvind Neelakantan, Ben Goodrich,\nLukasz Kaiser, and Samy Bengio, “Parallel scheduled\nsampling,” arXiv preprint arXiv:1906.04331, 2019.\n[24] Rewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever, “Generating long sequences with sparse\ntransformers,” arXiv preprint arXiv:1904.10509, 2019.\n[25] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszko-\nreit, Ian Simon, Curtis Hawthorne, Noam Shazeer, An-\ndrew M Dai, Matthew D Hoffman, Monica Dinculescu,\nand Douglas Eck, “Music transformer: Generating mu-\nsic with long-term structure,” 2018.\n[26] Zihang Dai, Zhilin Yang, Yiming Yang, William W Co-\nhen, Jaime Carbonell, Quoc V Le, and Ruslan Salakhut-\ndinov, “Transformer-XL: Attentive language mod-\nels beyond a ﬁxed-length context,” arXiv preprint\narXiv:1901.02860, 2019.\n[27] Arun Narayanan, Rohit Prabhavalkar, Chung-Cheng\nChiu, David Rybach, Tara N Sainath, and Trevor\nStrohman, “Recognizing long-form speech us-\ning streaming end-to-end models,” arXiv preprint\narXiv:1910.11455, 2019.\n[28] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani,\n“Self-attention with relative position representations,”\narXiv preprint arXiv:1803.02155, 2018.\n[29] Qian Zhang, Han Lu, Hasim Sak, Anshuman Tripathi,\nErik McDermott, Stephen Koo, and Shankar Kumar,\n“Transformer transducer: A streamable speech recog-\nnition model with transformer encoders and rnn-t loss,”\nin ICASSP 2020-2020 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2020, pp. 7829–7833.\n[30] Ruchao Fan, Pan Zhou, Wei Chen, Jia Jia, and Gang\nLiu, “An online attention-based model for speech recog-\nnition,” Proc. Interspeech 2019, 2019.",
  "topic": "Generalization",
  "concepts": [
    {
      "name": "Generalization",
      "score": 0.7084066271781921
    },
    {
      "name": "Embedding",
      "score": 0.6104981303215027
    },
    {
      "name": "Speech recognition",
      "score": 0.6080484390258789
    },
    {
      "name": "Transformer",
      "score": 0.5870015025138855
    },
    {
      "name": "Computer science",
      "score": 0.5599706768989563
    },
    {
      "name": "Schedule",
      "score": 0.4136982858181
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4071909189224243
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4019395112991333
    },
    {
      "name": "Natural language processing",
      "score": 0.3410111367702484
    },
    {
      "name": "Mathematics",
      "score": 0.26954206824302673
    },
    {
      "name": "Engineering",
      "score": 0.11991450190544128
    },
    {
      "name": "Electrical engineering",
      "score": 0.1035204827785492
    },
    {
      "name": "Voltage",
      "score": 0.08772483468055725
    },
    {
      "name": "Mathematical analysis",
      "score": 0.04407086968421936
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I161318765",
      "name": "University of California, Los Angeles",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    }
  ]
}