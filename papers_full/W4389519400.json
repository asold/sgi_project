{
  "title": "Improving Contextual Query Rewrite for Conversational AI Agents through User-preference Feedback Learning",
  "url": "https://openalex.org/W4389519400",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2326620430",
      "name": "Zhongkai Sun",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2112200545",
      "name": "Ying-Xue Zhou",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2096553442",
      "name": "Jie Hao",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2098351511",
      "name": "Xing Fan",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2112153945",
      "name": "Yan-bin Lu",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2104510912",
      "name": "Chengyuan Ma",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1975159644",
      "name": "Wei Shen",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2900894694",
      "name": "Chenlei Guo",
      "affiliations": [
        "Amazon (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2806482527",
    "https://openalex.org/W2736601468",
    "https://openalex.org/W3139085436",
    "https://openalex.org/W3163884224",
    "https://openalex.org/W4319453300",
    "https://openalex.org/W4281765689",
    "https://openalex.org/W2972892266",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W2889789465",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4378771755",
    "https://openalex.org/W2923474765",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4221159953",
    "https://openalex.org/W3171244865",
    "https://openalex.org/W4385571375",
    "https://openalex.org/W4385570211",
    "https://openalex.org/W3035169992",
    "https://openalex.org/W2618097077",
    "https://openalex.org/W2973379954"
  ],
  "abstract": "Zhongkai Sun, Yingxue Zhou, Jie Hao, Xing Fan, Yanbin Lu, Chengyuan Ma, Wei Shen, Chenlei Guo. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 432–439\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nImproving Contextual Query Rewrite for Conversational AI Agents\nthrough User-preference Feedback Learning\nZhongkai Sun1 Yingxue Zhou1 Jie Hao1 Xing Fan1\nYanbin Lu1 Chengyuan Ma1 Wei Shen1 Chenlei Guo1\n1Amazon Alexa AI\n{zhongkas,zyingxue,jieha,fanxing,luyanbin,mchengyu,sawyersw,guochenl}@amazon.com\nAbstract\nContextual query rewriting (CQR) is a crucial\ncomponent in Conversational AI agents, lever-\naging the contextual information from previous\nuser-agent conversations to improve the com-\nprehension of current user intent. However,\ntraditional CQR methods often concentrate on\nsupervised fine-tuning only, neglecting the op-\nportunities to learn from user feedback to align\nwith user preferences. Inspired by recent ad-\nvances in learning from human feedback (LHF),\nthis paper proposes a novel Preference Aligned\nContextual Query Rewriting (PA-CQR) frame-\nwork to enhance the CQR model’s capability\nin generating user preference-aligned rewrites.\nThis paper also investigates the efficacy of var-\nious state-of-the-art feedback learning algo-\nrithms on the CQR task, and proposes a novel\nDynamic Direct Preference Optimization (Dy-\nnamic DPO) algorithm to better adapt the DPO\nalgorithm to large-scale CQR training. Experi-\nments on large-scale real-world CQR data set\ndemonstrate the superiority of the proposed PA-\nCQR framework and the Dynamic DPO.\n1 Introduction\nConversational AI agents, such as Alexa, Siri, and\nGoogle Assistant, play a crucial role in the daily\nlives of individuals. To comprehend multi-turn\nspoken dialogues effectively, it is imperative to\naddress the challenges of referring expressions res-\nolution and entity tracking across the conversation,\nknown as the \"contextual carryover\" problem (Naik\net al., 2018; Anantha et al., 2020). Specifically, in\na multi-turn conversation, users may omit or ref-\nerence entities discussed earlier, causing ambigu-\nity for the AI agent. Contextual query rewriting\n(CQR) (Zhou et al., 2023; Liu et al., 2021; Zuo\net al., 2022; Sun et al., 2022), which rewrites the\nincomplete/ambiguous user query based on con-\ntextual information, have been widely utilized to\naddress the contextual carryover problem.\nRecent research have proposed various advanced\nFigure 1: Contextual Query Rewrite (CQR) example,\nwith both user-preferred and non-preferred rewrites.\nCQR approaches (Naik et al., 2018; Chen et al.,\n2019; Yu et al., 2020). However, these methods\ntypically only involve the supervised fine-tuning\n(SFT) stage, thereby missing some opportunities to\nfurther enhance the model from user-preference\nfeedback. Figure 1 illustrates a CQR example.\nRecently, LHF (learning from human feedback)\n(Ouyang et al., 2022; Ziegler et al., 2019; Rafailov\net al., 2023) has shown promising performance\nin leading language models to generate human-\npreferred content, which has been demonstrated\nas a key factor in the success of LLMs (large-\nlanguage models) (Ouyang et al., 2022; Bai et al.,\n2022). Inspired by RLHF frameworks in (Ouyang\net al., 2022; Bai et al., 2022), this paper proposes a\nuser Preference Aligned Contextual Query Rewrite\nframework, named as PA-CQR. PA-CQR consists\nof three stages: 1) the SFT stage fine-tunes a pre-\ntrained language model (PLM) on the CQR data\n(in which the context with imperfect user query is\nthe input and the ground truth rewrite is the target\noutput); 2) the SFT model from stage 1 is applied\nto conduct inference on provided contexts and the\ngenerated rewrites are then fed into a reward model\nto obtain the feedback that indicates users’ pref-\nerence; 3) the obtained user-preference feedback\n432\nFigure 2: Overview of the proposed user preference-aligned CQR (PA-CQR) framework, which consists of the SFT,\nfeedback collection, and feedback learning stages.\nis utilized to fine-tune the SFT model through a\nfeedback learning algorithm. Figure 2 illustrates\nthe details of the proposed PA-CQR framework.\nIn the proposed PA-CQR framework, we also\ninvestigate the effectiveness of state-of-the-art feed-\nback learning algorithms used in open-ended lan-\nguage generation tasks. To the best of our knowl-\nedge, this paper is the first to investigate effective\nfeedback learning algorithms tailored for the CQR\ntask. Specifically, we have studied the straight-\nforward best-of-n feedback learning Expert Itera-\ntion (Anthony et al., 2017), the Preference Guided\nFeedback Learning inspired by (Lu et al., 2022)\nand Contrastive Feedback Learning inspired by\nChain-of-Hindsight (Liu et al., 2023), the popular\nreinforcement learning algorithm PPO (Schulman\net al., 2017), and the direct preference optimization\nDPO (Rafailov et al., 2023). To relieve the reward\ndistribution-shift issue arises in the DPO algorithm,\nwe also proposes a novel Dynamic DPO algorithm\nwhich gradually weaken the reference model’s im-\npact and switch from DPO objective to Maximum\nlikelihood estimation objective. Extensive exper-\niments on large-scale real-world user-agent CQR\ndatasets demonstrate the effectiveness of our pro-\nposed PA-CQR and the Dynamic DPO.\n2 Related Work\nContextual query rewriting (CQR)Contextual\nquery rewriting (CQR) (Elgohary et al., 2018; Re-\ngan et al., 2019) is a crucial aspect in conversa-\ntional AI as it involves reformulating the original\nquery with additional or substitute terms that cap-\nture the true information need of the user based\non the conversational context. Recently, language\nmodel based methods such as (Regan et al., 2019;\nYu et al., 2020; Zuo et al., 2022) have been widely\nleveraged to conduct query rewriting by capturing\nnecessary information from the context. Such tech-\nniques have also been successfully deployed to con-\nversational AI systems (Rastogi et al., 2019; Zhou\net al., 2023) to improve user experience. However,\nthese works typically focus on the supervised-fine\ntuning stage while ignores the continually improve-\nment procedure to generate better rewrites that can\nbe aligned with user preference.\nAligning User Preference through Feedback\nLearning It has been a vital and challenging task\nto align content generated by the language model\nwith human-preference through feedback learn-\ning. Given the fact that human preference feed-\nback can be in arbitrary format and usually in-\ntrackable in model training, reinforcement learn-\ning (RL) algorithms such as PPO (Schulman et al.,\n2017) has widely adopted in training preference-\naligned language models (Ouyang et al., 2022; Bai\net al., 2022). However, reinforcement learning al-\ngorithms are often unstable, difficult to train, and\nexpensive. Therefore, recently a variety of non-RL\nalternative feedback learning algorithms have been\ndeveloped: Quark (Lu et al., 2022) first quantile\ngenerated content by reward and then re-train the\n433\nlanguage model to generate corresponding content\nconditioned on the its reward; Chain-of-Hindsight\n(CoH) (Liu et al., 2023) encourages the model to\ngenerate both preferred and non-preferred content\nso that learn the key disparity among them, Direct\nPreference Optimization (DPO) (Rafailov et al.,\n2023) converts the reward maximization problem\nto a single stage of a classification training on the\nhuman preference data. In this paper, we have in-\nvestigated both RL and non-RL feedback learning\nalgorithms in the PA-CQR framework.\n3 Preference Aligned CQR\nIn the context of a conversational AI system, we\nfirst introduce the concept of contextual query\nrewriting, which is more evident in the case of\na multi-turn dialogue. For instance, in a multi-\nturn dialogue \"[USER]: Turn on the guest bedroom\nlight [Agent]: Sure [USER]: One hundred percent\nbrightness\", the user’s entity slot \"guest bedroom\nlight\" require carryover to facilitate the generation\nof a contextual query rewrite. Therefore, we can\npose this scenario as a specific rewriting task, aim-\ning to generate a contextually rewritten query, such\nas \"[USER]: Set the guest bedroom light to one\nhundred percent brightness\".\nDespite recent advancements in LLMs, the im-\nportance of CQR is still pronounced, particularly\nfor enhancing conversational AI agents in industrial\nscenarios: 1) Implementing LLMs for every user\nentails high costs and latency; 2) LLMs may still\nmake carryover mistakes 3) it’s more straightfor-\nward to achieve customized CQR to serve diverse\nusers. Besides, the concept of CQR can be adapted\nto fit within future LLM scenarios. For example,\nwhen multiple LLM agents manage user/system\ninteractions, CQR is essential for ensuring context\ncontinuity across the agents. Consequently, CQR\nretains a pivotal role in maintaining coherence and\na seamless user experience, even in the LLM era.\nIn this section, we present the proposed PA-CQR\nframework, which consists of three stages: SFT for\nCQR, feedback collection, and feedback learning\nfor CQR. We discuss each of these stages in the\nsubsequent parts.\n3.1 SFT for CQR\nA pre-trained language model (PLM) is adopted for\nthe SFT for CQR. For every training point, the pre-\nvious dialogue turns (including both user requests\nand agent responses) and the current user request\nare flatten into a single sequence and fed input to\nthe PLM, and the PLM is fine-tuned to generate the\ncorresponding contextual rewrite.\nFormally, the CQR task is cast as a text gen-\neration problem: given a flatten dialogue con-\ntext sequence 1 c = {c1,...,c M}, where ci for\ni ∈{1,...,M }denotes a token in the sequence,\nand the corresponding rewrite r = {r1,...,r N},\nthe ultimate goal of the rewrite generation prob-\nlem is to learn a probability distribution Pθ(r) over\nthe variable-length text sequence r, where θis the\nparameter of the transformer model. Maximum\nlikelihood estimation (MLE) objective is adopted\nto train the language model, which is defined as:\nLMLE\nθ (c,r) =−1\n|r|\n|r|∑\nj=1\nlog Pθ(rj|r<j,c) . (1)\nTypically, given finite training examples, i.e.,\nT pairs of contextual query and rewrite S =\n{qt,ct}T\nt=1, the model is trained by minimizing\nthe empirical finite sample objective loss function\nLMLE\nθ (S) = 1\nT\n∑T\nt=1 LMLE\nθ (ct,rt).\n3.2 Feedback Collection\nThe SFT CQR model is then applied to additional\ncontext to collect feedback. Following recent LHF\nwork (Ouyang et al., 2022; Lu et al., 2022), con-\ntrastive feedback are gathered for every specific\ncontext. Specifically, a context is fed into the SFT\nCQR model and the N-best outputs are considered\nas N rewrite candidates. A reward model, which is\ncapable of representing user preferences concern-\ning the generated rewrites, is subsequently applied\nto every set of <context, query, rewrite candidate>\nto obtain user-preference feedback. Thus, this\napproach facilitates the collection of contrastive\nrewrite candidates (user-preferred rewritev.s. non-\npreferred rewrite) for a specific context.\n3.3 Feedback Learning Algorithms for CQR\nReinforcement learning (RL) algorithms (e.g.,\nPPO) have been widely used to fine tune the SFT\nmodel with feedback (i.e., RLHF). However, such\nreinforcement learning algorithms on large-scale in-\ndustrial data usually faces issues such as high com-\nplexity, high instability, high sensitivity to hyper-\nparameters, and extremely expensive training costs.\n1In the given example, we have the flatten dialogue context\nas \"[USER] Turn on the guest bedroom light [AGENT] Sure\n[USER] One hundred percent brightness\", where the last turn\n\"USER] One hundred percent brightness\" is the query that\nneesds rewrite.\n434\nRecently, alternative feedback learning methods\n(Lu et al., 2022; Rafailov et al., 2023; Liu et al.,\n2023) for language generation has been proposed\nto achieve a similar impact as RLHF with simpler\nimplementation, better stability and lower cost. In\nthis paper, we extensively explore four state-of-the-\narts feedback learning algorithms for the proposed\nPA-CQR framework.\nLearning from Positive Feedback.The most effi-\ncient approach for utilizing feedback data is direct\nfine-tuning the SFT model on positive feedback.\nThis paper employs a common method known as\nExpert-Iteration (Anthony et al., 2017), specifically\ndesigned to learn from positive feedback. Initially,\nthe model generates N rewrites given the context,\nthen the model is subsequently fine-tuned on the\n<context, best positive generated rewrite>pair that\nholds the highest positive feedback reward score\namong the total N pairs.\nPreference Guided Feedback Learning.Exclu-\nsively learning from positive feedback limits the\nmodel’s awareness of undesirable content, poten-\ntially restricting its ability to utilize negative feed-\nback in avoiding non-preferred content. A recent\nreward conditioning algorithm Quark (Lu et al.,\n2022) enforces the model to unlearn the misaligned\ngeneration by fine-tuning the SFT model condi-\ntioned on reward quantile. Inspired by Quark, we\napply the similar preference guided feedback learn-\ning method that leverages both preferred and non-\npreferred feedback rewrites to fine-tune the model.\nSpecifically, we first collect pairs of(c,ˆ r), where c\nis the context, and ˆ ris the generated rewrite of the\nSFT CQR model, assigned with a user preferred\nor non preferred feedback using the reward model\n(denoted as + and −). Next, an indicator prompt\nis added to the context c based on the feedback\nof ˆ rto create new fine-tuning data for the SFT\nCQR model. The learning instance is of the for-\nmat ([p,c],ˆ r), where p is \"generate good rewrite:\"\nwhen ˆ ris + and \"generate bad rewrite\" when ˆ r\nis −. Formally, the Preference Guided Feedback\nLearning (PGFL) objective is\nLPGFL\nθ (c,ˆ r) =−1\n|ˆ r|\n|ˆ r|∑\nj=1\nlog Pθ(ˆrj|ˆ r<j,[p,c]).\nThe model is trained by minimizing the empir-\nical finite sample loss function LPGFL\nθ (S) =\n1\nT\n∑T\nt=1 LPGFL\nθ (ct,ˆ rt).\nContrastive Generation Feedback Learning.In\nPGFL, the preference information is introduced in\nAlgorithm 1Dynamic DPO\nInput: Initial policy model parameters θr, feed-\nback dataset ˆS = {(ci, ˆ r+\ni ,ˆ r−\ni )}T\ni=1\nSet: Total iteration Nt, DPO iteration Nd, batch\nsize b\n1: for step nin 1,2,..,N t do\n2: Sample batch B = {(ci, ˆ r+\ni ,ˆ r−\ni )}b\ni=1 from\nˆS\n3: LMLE\nθ = 1\nb\n∑b\ni=1 LMLE\nθ (ci,ˆ r+\ni )\n4: if n<= Nd then\n5: LDDPO\nθ = 1\nb\n∑b\ni=1 LDDPO\nθ (ci, ˆ r+\ni ,ˆ r−\ni )\n6: Ltotal = LMLE\nθ + LDDPO\nθ\n7: else\n8: Ltotal = LMLE\nθ\n9: end if\n10: Update θ using gradient descent on loss\nLtotal\n11: end for\nthe input end. Alternatively, inspired by the work\nof Chain of Hindsight (CoH) (Liu et al., 2023) , the\npreference can also be introduced in the output end\nvia a contrastive generation, which learns to gener-\nate both preferred and non-preferred rewrite simul-\ntaneously. Specifically, the model can be fine-tuned\nby taking the specific context as input and gen-\nerating both the user-preferred and non-preferred\nrewrite pair ˆ r= (ˆ r+,ˆ r−). This motivation is\nto allow the model to recognize the key dispari-\nties between positive and negative patterns through\nthe generation of comparative forms, therefore to\nenhance the model’s capacity of identifying and\ndifferentiating desirable and undesirable patterns.\nFormally, the loss of this Contrastive Generation\nFeedback Learning (CGFL) algorithm is:\nLCGFL\nθ (c,ˆ r) =−1\n|ˆ r|\n|ˆ r|∑\nj=1\nlog Pθ(ˆrj|ˆ r<j,c).\nSimilarly, the model is trained by minimizing the\nempirical finite sample loss function LCGFL\nθ (S) =\n1\nT\n∑T\nt=1 LCGFL\nθ (ct,rt).\nDPO and Dynamic DPO.Recently (Rafailov et al.,\n2023) proposed a Direct Preference Optimization\n(DPO) algorithm that implicitly optimizes the same\nobjective as existing RLHF. DPO directly opti-\nmizes the model by a straightforward contrastive\nloss to boosting the reward of preferred generation\nand penalizing that of the non-preferred generation.\nThe DPO loss is\n435\nLDPO\nθ (c,ˆ r+,ˆ r−) =\n−log σ\n(\nβlog Pθ(ˆ r+|c)\nPθr(ˆ r+|c) −βlog Pθ(ˆ r−|c)\nPθr(ˆ r−|c)\n)\nwhere σ represents logistic function and β is a\nweight hyper-parameter, θr is the reference model\n(SFT CQR model in our case). Intuitively, the DPO\nloss function implicitly increases the reward of\nthe positive rewrite ˆ r+ and decreases the reward\nof negative rewrite ˆ r−, where the reward is ap-\nproximated by the likelihood re-weighted by the\nreference model θr, i.e., βlog Pθ(ˆ r+|c)\nPθr(ˆ r+|c) .\nHowever, as training progresses, the policy\nmodel gradually diverges from the initial reference\nmodel, and aligning more closely with a distribu-\ntion that is consistent with the feedback data. Con-\nsequently, the reward approximated from the ref-\nerence model may substantially deviate from the\ndistribution of the current policy model, causing an\nimpact on the training of the policy model. This is\nidentified as the reward distribution-shiftissue. To\nmitigate this problem, we propose a Dynamic DPO\nalgorithm which adds a decaying factor in the refer-\nence model and interpolates between normal MLE\ntraining and DPO training. The intuition is to grad-\nually weaken the weight of the reference model in\nDPO and smoothly transit from the DPO objective\nto MLE eventually. The proposed dynamic DPO\n(DDPO) loss is define as\nLDDPO\nθ (c,ˆ r+,ˆ r−) =\n−log σ\n(\nβlog Pθ(ˆ r+|c)\nPθr(ˆ r+|c)ϵn\n−βlog Pθ(ˆ r−|c)\nPθr(ˆ r−|c)ϵn\n)\nwhere ϵn = min(1,C\nn) is the decaying factor\nwhich decays as iteration steps nincreases when\nstep nis larger than a thresholdC. Given a batch of\ndata B = {(ci, ˆ r+\ni ,ˆ r−\ni )}b\ni=1, the loss on the batch\nis LDDPO\nθ (B) =∑b\ni=1 LDDPO\nθ (ci, ˆ r+\ni ,ˆ r−\ni )).Af-\nter a certain iteration steps (i.e., Nd in Algorithm\n1 line 4), we switch the training objective of com-\nbined DDPO loss and MLE loss ((line 6 in Algo-\nrithm 1)) to only MLE loss (line 8 in Algorithm 1).\nThe detailed algorithm is described in Algorithm 1.\n4 Experiments\nWe conduct experiment on a large-scale real-word\nindustry CQR to validate the PA-CQR framework\nand evaluate the feedback learning methods dis-\ncussed in section 3. Note that all data used in this\npaper has been de-identified therefore no user in-\nformation is remained.\nName # trigger # non-trigger\nReal-world Train 1M 1M\nReal-world Test 4k 16k\nTable 1: CQR training and test set statistics.\n4.1 Experiment Setup\nDataset Training data for the CQR task contains\n2M de-identified real world user-agent contextual\nconversations. Among the 2M data, 1M is the\nshould-trigger CQR data, i.e., the last user query\nin each context has a corresponding ground truth\nrewrite. Thus the model needs to take the context\n(includes the last user query) as input and predict\nthe corresponding ground truth rewrite; The re-\nmaining 1M data are selected from non-triggered\nCQR traffic, in which the last user query is either\naccurate enough or not be able to rewritten. The\nmodel then needs to take the context as input and\npredicts \"NULL\" as the rewrite output. The CQR\nmodel is trained on both of the should-trigger and\nnot-triggered CQR data, so that to learn to deter-\nmine when should it provide the query rewrite and\ngenerate correct rewrite simultaneously.\nFor test, a 20k human-annotated dataset on sam-\npled real-world traffic, include both should-trigger\nand non-trigger data, is used. Table 1 demonstrates\nthe statistics of train and test sets.\nEvaluation MetricsThree evaluation metrics are\nutilized: 1) Rewrite Accuracy: Given the fact that\nonly high confidence rewrites will be triggered in\npractical, the utterance-level precision at a set 20%\ntrigger rate is used as the rewrite accuracy; 2) En-\ntity Omission Rate: The utterance-level precision\ncan be limited as it requires a strict match. Thus,\nthe percentage of cases where predicted rewrite\nmisses a key entity in the ground truth rewrite label\nis also examined. The key entities are identified\nas the non stop-words entities in the ground truth\nrewrite. 3) Trigger F1: The F1 score of the trig-\nger prediction is calculated, which is used to mea-\nsure the model’s performance in determining when\nshould trigger the rewrite given the context.\nModel Set-upThe FLAN-T5-Large (Chung et al.,\n2022) serves as the base PLM for all experiments.\nAll experiments are executed on eight A100 GPUs.\nThe epoch number is set as 10 for all experiments.\nThe learning rates for all methods are set as 3e−5.\nThe batch size is set as 32 for DPO/DDPO, 8 for\nPPO, and 128 for all other methods.\n436\nSet-up Annotated Test\nMethod Data-size Rew Acc @ 20%↑ Entity Omission @ 20%↓ Trigger F1↑\nSFT 2M 0.0% 0.0 % 0.0%\nPPO (2M) + 400k + 0.9% - 1.06% - 0.24%\nExp-Iteration (2M) + 400k + 2.56% - 8.48% + 1.43%\nPreference Guided (2M) + 400k - 22.0% + 55.1% - 4.76%\nContrastive Generation (2M) + 400k - 17.3% - 37.1% -2.97%\nDPO (2M) + 400k + 4.51% - 14.1% + 1.18%\nDynamic DPO (2M) + 400k + 6.62% -18.4% + 0.36%\nTable 2: Overall result table of different methods. Relative improvements compared to the SFT model are reported\nfor each feedback learning algorithm. The SFT model represents fine-tuning Flan-t5-large on the 2M data. The\nfeedback is collected by applying the SFT model to the 2M data again. For a fair comparison, 400k feedback data\nare collected for every setting.\n4.2 Experiment Results\nTo evaluate the performance of different feedback\ntypes and LHF algorithms, following the pipeline\nillustrated in Figure 2, the raw FLAN-T5-large\nmodel is first fine-tuned using the 2M training\ndata to obtain the SFT model. The SFT model\nis then utilized to perform inference on the same\n2M data points, and the resulting rewrites are pro-\ncessed by the reward model, which is trained using\ndata from human annotation and heuristic rules.\nWe selected 400k feedback data from the reward re-\nsults, in which each context has one user-preferred\nrewrite and one non-preferred rewrite. (note: the re-\nward model is only deployed to cases where rewrite\ntriggers). Then, the SFT model is fine-tuned with\nadditional feedback data using different methods\ndescribed in section 3.3. PPO (Schulman et al.,\n2017) is also applied as a RL baseline.\nThe primary results of the experiment are shown\nin Table 2. An initial observation reveals that feed-\nback learning techniques such as Expert-Iteration,\nDPO, Dynamic DPO outperform both the SFT\nmodel and the PPO method in terms of CQR-\nrelated metrics, thereby verifies the efficacy of the\nPA-CQR framework we propose. Moreover, the\nExpert-Iteration feedback learning exhibits a sig-\nnificant enhancement in rewrite accuracy and entity\nomission rate. Expert-Iteration can be perceived as\nan approach of seeking the optimal rewrite from an\nexpansive array of self-generated candidates, thus\nfacilitating the SFT model’s feedback learning to-\nwards improved performance. This result further\ndemonstrates the necessity of feedback-learning\nand the great potential of improving the model by\nexamining and learning from its own generated\ncontent. However, it is notable that the Preference\nGuided and Contrastive Generationmethods show\nworse performance on the CQR task. These two\nmethods integrate feedback learning information\nvia text-format by either modifying the input text\nor target text. However, different from general gen-\neration tasks used in (Lu et al., 2022; Liu et al.,\n2023), the key disparity between preferred and non-\npreferred rewrites in CQR tasks could be subtle\n(e.g., \"Set light to green\"versus \"Set bedroom light\nto green\"). Hence, training the SFT on text-level\nfeedback could potentially overlook key factors,\nleading to model confusion. Lastly, both the DPO\nand the proposed Dynamic PPOshow promising\nresults, and the Dynamic DPOshowing superior\nresults in terms of rewrite accuracy and entity omis-\nsion metrics. This verifies the effectiveness of our\nproposed Dynamic DPOalgorithm.\n5 Conclusion\nThis paper introduces the PA-CQR framework,\nwhich is inspired by the recent achievements of hu-\nman feedback learning, to continually improve the\nindustry CQR model to generate better rewrites that\nare aligned with user preference. Besides, to miti-\ngate the limitations of the DPO algorithm in large-\nscale CQR training, the paper also proposes a novel\nDynamic DPO algorithm which gradually weaken\nthe impact of the reference model in training. Ex-\ntensive experiments conducted on real-world user-\nagent CQR dataset experiments have demonstrated\nthe effectiveness of the proposed PA-CQR frame-\nwork, certain feedback learning algorithms such\nas Expert-Iteration, DPO, Dynamic DPO. The re-\nsearch also reveals that feedback learning methods\nsuch as Preference Guided, Contrastive Generatio\nexhibit limited performance when applied to the\nCQR task , highlighting the potential for further\nresearch and development in this area.\n437\nReferences\nRaviteja Anantha, Svitlana Vakulenko, Zhucheng Tu,\nShayne Longpre, Stephen Pulman, and Srinivas\nChappidi. 2020. Open-domain question answering\ngoes conversational via question rewriting. arXiv\npreprint arXiv:2010.04898.\nThomas Anthony, Zheng Tian, and David Barber. 2017.\nThinking fast and slow with deep learning and tree\nsearch. Advances in neural information processing\nsystems, 30.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al.\n2022. Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv\npreprint arXiv:2204.05862.\nTongfei Chen, Chetan Naik, Hua He, Pushpendre Ras-\ntogi, and Lambert Mathias. 2019. Improving long\ndistance slot carryover in spoken dialogue systems.\nIn Proceedings of the First Workshop on NLP for\nConversational AI, pages 96–105.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nAhmed Elgohary, Chen Zhao, and Jordan Boyd-Graber.\n2018. Dataset and baselines for sequential open-\ndomain question answering. In Empirical methods\nin natural language processing.\nHang Liu, Meng Chen, Youzheng Wu, Xiaodong He,\nand Bowen Zhou. 2021. Conversational query rewrit-\ning with self-supervised learning. In ICASSP 2021-\n2021 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pages 7628–\n7632. IEEE.\nHao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023.\nChain of hindsight aligns language models with feed-\nback. arXiv preprint arXiv:2302.02676, 3.\nXiming Lu, Sean Welleck, Jack Hessel, Liwei Jiang,\nLianhui Qin, Peter West, Prithviraj Ammanabrolu,\nand Yejin Choi. 2022. Quark: Controllable text\ngeneration with reinforced unlearning. Advances\nin neural information processing systems, 35:27591–\n27609.\nChetan Naik, Arpit Gupta, Hancheng Ge, Lambert\nMathias, and Ruhi Sarikaya. 2018. Contextual slot\ncarryover for disparate schemas. arXiv preprint\narXiv:1806.01773.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano\nErmon, Christopher D Manning, and Chelsea Finn.\n2023. Direct preference optimization: Your language\nmodel is secretly a reward model. arXiv preprint\narXiv:2305.18290.\nPushpendre Rastogi, Arpit Gupta, and Lambert Math-\nias. 2019. Contextual query rewriting (cqr): natural\nlanguage as interface for dialog state tracking. In\nNAACL 2019.\nMichael Regan, Pushpendre Rastogi, Arpit Gupta,\nand Lambert Mathias. 2019. A dataset for resolv-\ning referring expressions in spoken dialogue via\ncontextual query rewrites (cqr). arXiv preprint\narXiv:1903.11783.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal,\nAlec Radford, and Oleg Klimov. 2017. Proxi-\nmal policy optimization algorithms. arXiv preprint\narXiv:1707.06347.\nZhongkai Sun, Sixing Lu, Chengyuan Ma, Xiaohu Liu,\nand Chenlei Guo. 2022. Query expansion and entity\nweighting for query reformulation retrieval in voice\nassistant systems. arXiv preprint arXiv:2202.13869.\nShi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong, Paul\nBennett, Jianfeng Gao, and Zhiyuan Liu. 2020. Few-\nshot generative conversational query rewriting. In\nProceedings of the 43rd International ACM SIGIR\nconference on research and development in Informa-\ntion Retrieval, pages 1933–1936.\nYingxue Zhou, Jie Hao, Mukund Rungta, Yang Liu,\nEunah Cho, Xing Fan, Yanbin Lu, Vishal Vasudevan,\nKellen Gillespie, Zeynab Raeesy, et al. 2023. Unified\ncontextual query rewriting.\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B\nBrown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. 2019. Fine-tuning lan-\nguage models from human preferences. arXiv\npreprint arXiv:1909.08593.\nSimiao Zuo, Qingyu Yin, Haoming Jiang, Shaohui Xi,\nBing Yin, Chao Zhang, and Tuo Zhao. 2022. Context-\naware query rewriting for improving users’ search\nexperience on e-commerce websites. arXiv preprint\narXiv:2209.07584.\nA Appendix\nA.1 Additional Analysis on un-generating\nNegative Patterns\nTo further verify if the feedback learning algo-\nrithm is effective in fine-tune the SFT CQR model\nin un-learning non-preferred patterns, additional\n<context, non-preferred rewrite> pairs are col-\nlected. Specifically, the non-preferred rewrites\nare obtained from the inference result of the SFT\n438\nmodel that are labeled as non-preferred by the re-\nward model. Next, each model’s likelihood (repre-\nsented as the lossof model(context, non-preferred\nrewrite)) of generating the given non-preferred\nrewrite for the given context can be calculated to\nrepresent the model’s capability in un-generating\nnon-preferred patterns.\nTable 3 demonstrates the result. It is observed\nthat the effective feedback learning algorithms such\nas Expert-Iteration, PPO, DPO, Dynamic DPOall\nhave lower likelihood (higher loss) in generating\nnon-preferred patterns. Besides, methods like PPO,\nDPO, Dynamic DPOhave a higher performance\nthan Expert Iterationbecause they are directly op-\ntimized using both preferred and non-preferred pat-\nterns.\nMethod Non-preferred Loss\nSFT 0.65\nPPO 0.87\nExpert-Iteration 0.71\nPreference Guided 0.72\nContrastive Generation 0.66\nDPO 1.13\nDynamic DPO 1.09\nTable 3: Comparisons of different methods’ capabilities\nin un-generating non-preferred patterns, represented us-\ning each model’s loss value of <context, non-preferred\nrewrite>.\nA.2 Training Speed\nTable 4 illustrates the training speed for every feed-\nback learning algorithms. The training speed is rep-\nresented as the average number of tokens processed\nevery second, on 8 A100 GPUs. The numbers show\nthat the Expert Iteration is the fastest option while\nthe PPO requires a large training cost.\nMethod Training Seed (# tokens / s)\nSFT 6340\nPPO 310\nExpert-Iteration 6200\nPreference Guided 6170\nContrastive Generation 6250\nDPO 2140\nDynamic DPO 1190\nTable 4: Training speed for every feedback learning\nalgorithm.\nB Ethical Discussion\nThis work aims at enhancing the performance of\nContextual Query Rewriting (CQR) for conversa-\ntional AI agents through feedback learning. How-\never, implementing such feedback and correspond-\ning feedback learning algorithms may involve ethi-\ncal considerations in privacy and data protection.\nFor example, the training of the reward model\nand CQR model requires real-world user-agent di-\nalogues. Therefore, it’s critical to guarantee that\nthe acquisition and processing of this data are con-\nducted in a manner that user privacy information\nis well protected. In this work, all data for training\nand testing are from sources where user identifica-\ntion and privacy information have been removed.\nThis procedure ensures that users’ private details\nhave been omitted and are not input into the models.\nMoreover, in the realistic production pipeline, sev-\neral additional safety examinations are employed\nto assure that both the training data collection and\noutput rewriting comply with appropriate content\nstandards.\n439",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.744864821434021
    },
    {
      "name": "Preference",
      "score": 0.7057269811630249
    },
    {
      "name": "Human–computer interaction",
      "score": 0.4878387749195099
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48763981461524963
    },
    {
      "name": "Natural language processing",
      "score": 0.48641467094421387
    },
    {
      "name": "Track (disk drive)",
      "score": 0.4566970467567444
    },
    {
      "name": "Information retrieval",
      "score": 0.34779471158981323
    },
    {
      "name": "Mathematics",
      "score": 0.07567864656448364
    },
    {
      "name": "Statistics",
      "score": 0.05855920910835266
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}