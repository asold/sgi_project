{
  "title": "MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning",
  "url": "https://openalex.org/W3035237998",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A1963970250",
      "name": "Jie Lei",
      "affiliations": [
        "University of North Carolina Health Care",
        "University of North Carolina at Chapel Hill"
      ]
    },
    {
      "id": "https://openalex.org/A2099453415",
      "name": "Liwei Wang",
      "affiliations": [
        "Seattle University",
        "KLA (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2166559730",
      "name": "Yelong Shen",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2044870803",
      "name": "Dong Yu",
      "affiliations": [
        "KLA (United States)",
        "Seattle University"
      ]
    },
    {
      "id": "https://openalex.org/A2560342476",
      "name": "Tamara Berg",
      "affiliations": [
        "University of North Carolina at Chapel Hill",
        "University of North Carolina Health Care"
      ]
    },
    {
      "id": "https://openalex.org/A2070810387",
      "name": "Mohit Bansal",
      "affiliations": [
        "University of North Carolina Health Care",
        "University of North Carolina at Chapel Hill"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2952132648",
    "https://openalex.org/W2950527759",
    "https://openalex.org/W1899504021",
    "https://openalex.org/W1815076433",
    "https://openalex.org/W1596841185",
    "https://openalex.org/W4303633609",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2963916161",
    "https://openalex.org/W2883910824",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2425121537",
    "https://openalex.org/W2891939431",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2951008357",
    "https://openalex.org/W2968101724",
    "https://openalex.org/W2164290393",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963351113",
    "https://openalex.org/W2963811641",
    "https://openalex.org/W2975501350",
    "https://openalex.org/W2904658088",
    "https://openalex.org/W2486996822",
    "https://openalex.org/W2964065937",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W1927052826",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2897439619",
    "https://openalex.org/W2133459682",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2784025607",
    "https://openalex.org/W2962937869",
    "https://openalex.org/W2963579811",
    "https://openalex.org/W2989322838",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2293453011"
  ],
  "abstract": "Generating multi-sentence descriptions for videos is one of the most challenging captioning tasks due to its high requirements for not only visual relevance but also discourse-based coherence across the sentences in the paragraph. Towards this goal, we propose a new approach called Memory-Augmented Recurrent Transformer (MART), which uses a memory module to augment the transformer architecture. The memory module generates a highly summarized memory state from the video segments and the sentence history so as to help better prediction of the next sentence (w.r.t. coreference and repetition aspects), thus encouraging coherent paragraph generation. Extensive experiments, human evaluations, and qualitative analyses on two popular datasets ActivityNet Captions and YouCookII show that MART generates more coherent and less repetitive paragraph captions than baseline methods, while maintaining relevance to the input video events.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2603–2614\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n2603\nMART: Memory-Augmented Recurrent Transformer\nfor Coherent Video Paragraph Captioning\nJie Lei1∗ Liwei Wang2 Yelong Shen3∗ Dong Yu2 Tamara L. Berg1 Mohit Bansal1\n1UNC Chapel Hill 2Tencent AI Lab Seattle USA 3Microsoft Dynamics 365 AI\n{jielei, tlberg, mbansal}@cs.unc.edu\n{liweiwang, dyu}@tencent.com, {yeshe}@microsoft.com\nAbstract\nGenerating multi-sentence descriptions for\nvideos is one of the most challenging cap-\ntioning tasks due to its high requirements for\nnot only visual relevance but also discourse-\nbased coherence across the sentences in the\nparagraph. Towards this goal, we propose a\nnew approach called Memory-Augmented Re-\ncurrent Transformer (MART), which uses a\nmemory module to augment the transformer\narchitecture. The memory module generates\na highly summarized memory state from the\nvideo segments and the sentence history so as\nto help better prediction of the next sentence\n(w.r.t. coreference and repetition aspects),\nthus encouraging coherent paragraph genera-\ntion. Extensive experiments, human evalua-\ntions, and qualitative analyses on two popular\ndatasets ActivityNet Captions and YouCookII\nshow that MART generates more coherent and\nless repetitive paragraph captions than base-\nline methods, while maintaining relevance to\nthe input video events.1\n1 Introduction\nIn video captioning, the task is to generate a natu-\nral language description capturing the content of a\nvideo. Recently, dense video captioning (Krishna\net al., 2017) has emerged as an important task in\nthis ﬁeld, where systems ﬁrst generate a list of tem-\nporal event segments from a video, then decode\na coherent paragraph (multi-sentence) description\nfrom the generated segments. Park et al. (2019)\nsimpliﬁes this task as generating a coherent para-\ngraph from a provided list of segments, removing\nthe requirements for generating the event segments,\nand focusing on decoding better paragraph cap-\ntions from the segments. As noted by Xiong et al.\n∗ Work done while Jie Lei was an intern and Yelong\nShen was an employee at Tencent AI Lab.\n1All code is available open-source athttps://github.\ncom/jayleicn/recurrent-transformer\n(2018); Park et al. (2019), generating paragraph\ndescriptions for videos can be very challenging due\nto the difﬁculties of having relevant, less redundant,\nas well as coherent generated sentences.\nTowards this goal, Xiong et al. (2018) proposed\na variant of the LSTM network (Hochreiter and\nSchmidhuber, 1997) that generates a new sentence\nconditioned on previously generated sentences by\npassing the LSTM hidden states throughout the\nentire decoding process. Park et al. (2019) further\naugmented the above LSTM caption generator with\na set of three discriminators that score generated\nsentences based on deﬁned metrics, i.e., relevance,\nlinguistic diversity, and inter-sentence coherence.\nThough different, both these methods use LSTMs\nas the language decoder.\nRecently, transformers (Vaswani et al., 2017)\nhave proven to be more effective than RNNs\n(e.g., LSTM (Hochreiter and Schmidhuber, 1997),\nGRU (Chung et al., 2014), etc.), demonstrating su-\nperior performance in many sequential modeling\ntasks (Vaswani et al., 2017; Zhou et al., 2018; De-\nvlin et al., 2019; Dai et al., 2019; Yang et al., 2019).\nZhou et al. (2018) ﬁrst introduced the transformer\nmodel to the video paragraph captioning task, with\na transformer captioning module decoding natu-\nral language sentences from encoded video seg-\nment representations. This transformer captioning\nmodel is essentially the same as the original trans-\nformer (Vaswani et al., 2017) for machine trans-\nlation, except that it takes a video representation\nrather than a source sentence representation as its\nencoder input. However, in such design, each video\nsegment caption is decoded individually without\nknowing the context (i.e., previous video segments\nand the captions that have already been generated),\nthus often leading to inconsistent and redundant\nsentences w.r.t. previously generated sentences\n(see Figure 3 for examples). Dai et al. (2019) rec-\nognize this problem as context fragmentationin\n2604\nthe task of language modeling, where the trans-\nformers are operating on separated ﬁxed-length\nsegments, without any information ﬂow across seg-\nments. Therefore, to generate more coherent video\nparagraphs, it is imperative to build a model that\ncan span over multiple video segments and capture\nlonger range dependencies.\nHence, in this work, we propose the Memory-\nAugmented Recurrent Transformer (MART) model\n(see Section 3 for details), a transformer-based\nmodel that uses a shared encoder-decoder archi-\ntecture augmented with an external memory mod-\nule to enable the modeling of the previous history\nof video segments and sentences. Compared to\nthe vanilla transformer video paragraph captioning\nmodel (Zhou et al., 2018), our ﬁrst architecture\nchange is the uniﬁed encoder-decoder design, i.e.,\nthe encoder and decoder in MART use shared trans-\nformer layers rather than separated as in Zhou et al.\n(2018); Vaswani et al. (2017). This uniﬁed encoder-\ndecoder design is inspired by recent transformer\nlanguage models (Devlin et al., 2019; Dai et al.,\n2019; Sun et al., 2019) to prevent overﬁtting and\nreduce memory usage. Additionally, the memory\nmodule works as a memory updater that updates\nits memory state using both the current inputs and\nprevious memory state. The memory state can be\ninterpreted as a container of the highly summarized\nvideo segments and caption history information. At\nthe encoding stage, the current video segment repre-\nsentation is enhanced with the memory state from\nthe previous step using cross-attention (Vaswani\net al., 2017). Hence, when generating a new sen-\ntence, MART is aware of the previous contextual\ninformation and can generate paragraph captions\nwith higher coherence and lower repetition.\nTransformer-XL (Dai et al., 2019) is a re-\ncently proposed transformer language model that\nalso uses recurrence, and is able to resolve con-\ntext fragmentation for language modeling (Dai\net al., 2019). Different from MART that uses\na highly-summarized memory to remember his-\ntory information, Transformer-XL directly uses\nhidden states from previous segments. We mod-\nify the Transformer-XL framework for video para-\ngraph captioning and present it as an additional\ncomparison. We benchmark MART on two stan-\ndard datasets: ActivityNet Captions (Krishna et al.,\n2017) and YouCookII (Zhou et al., 2017). Both\nautomatic evaluation and human evaluation show\nthat MART generates more satisfying results than\nprevious LSTM-based approaches (Xiong et al.,\n2018; Zhou et al., 2019; Zhang et al., 2018) and\ntransformer-based approaches (Zhou et al., 2018;\nDai et al., 2019). In particular, MART can gen-\nerate more coherent (e.g., coreference and order),\nless redundant paragraphs without losing paragraph\naccuracy (visual relevance).\n2 Related Work\nVideo Captioning Recently, video captioning\nhas attracted much attention from both the com-\nputer vision and the natural language process-\ning community. Methods for the task share the\nsame intrinsic nature of taking a video as the in-\nput and outputting a language description that can\nbest describe the content, though they differ from\neach other on whether a single sentence (Wang\net al., 2019; Xu et al., 2016; Chen and Dolan,\n2011; Pasunuru and Bansal, 2017a) or multiple\nsentences (Rohrbach et al., 2014; Krishna et al.,\n2017; Xiong et al., 2018; Zhou et al., 2018; Gella\net al., 2018; Park et al., 2019) are generated for the\ngiven video. In this paper, our goal falls into the\ncategory of generating a paragraph (multiple sen-\ntences) conditioned on an input video with several\npre-deﬁned event segments.\nOne line of work (Zhou et al., 2018, 2019) ad-\ndresses the video paragraph captioning task by de-\ncoding each video event segment separately into\na sentence. The ﬁnal paragraph description is ob-\ntained by concatenating the generated single sen-\ntence descriptions. Though individual sentences\nmay precisely describe the corresponding event\nsegments, when put together the sentences often\nbecome inconsistent and redundant. Another line\nof works (Xiong et al., 2018; Gella et al., 2018) use\nthe LSTM decoder’s last (word) hidden state from\nthe previous sentence as the initial hidden state for\nthe next sentence decoding, thus enabling informa-\ntion ﬂow from previous sentences to subsequent\nsentences. While these methods have shown better\nperformance than their single sentence counterpart,\nthey are still undesirable as the sentence-level recur-\nrence is achieved at word-level, and the context his-\ntory information quickly decays due to vanishing\ngradients (Pascanu et al., 2013) problem. Addition-\nally, these designs also have difﬁculty modeling\nlong-term dependencies (Hochreiter et al., 2001).\nIn comparison, the recurrence in MART resides in\nthe sentence or segment level and is thus more ro-\nbust to the aforementioned problems. AdvInf (Park\n2605\net al., 2019) augments the above LSTM word-level\nrecurrence methods with adversarial inference, us-\ning a set of separately trained discriminators to\nre-rank the generated sentences. The techniques\nin AdvInf can be viewed as an orthogonal way of\ngenerating captions with better quality.\nTransformers Transformer (Vaswani et al.,\n2017) is used as the basis of our approach. Dif-\nferent from RNNs (e.g., LSTM (Hochreiter and\nSchmidhuber, 1997), GRU (Chung et al., 2014),\netc) that use recurrent structure to model long-term\ndependencies, transformer relies on self-attention\nto learn the dependencies between input words.\nTransformers have proven to be more efﬁcient and\npowerful than RNNs, with superior performance\nin many sequential modeling tasks, including ma-\nchine translation (Vaswani et al., 2017), language\nmodeling/pre-training (Devlin et al., 2019; Dai\net al., 2019; Yang et al., 2019) and multi-modal rep-\nresentation learning (Tan and Bansal, 2019; Chen\net al., 2019; Sun et al., 2019). Additionally, Zhou\net al. (2018) have shown that a transformer model\ncan generate better captions than the LSTM model.\nHowever, transformer architectures are still un-\nable to model history information well. This prob-\nlem is identiﬁed in the task of language modeling as\ncontext fragmentation (Dai et al., 2019), i.e., each\nlanguage segment is modeled individually without\nknowing its surrounding context, leading to inefﬁ-\ncient optimization and inferior performance. To re-\nsolve this issue, Transformer-XL (Dai et al., 2019)\nintroduces the idea of recurrence to the transformer\nlanguage model. Speciﬁcally, the modeling of a\nnew language segment in Transformer-XL is con-\nditioned on hidden states from previous language\nsegments. Experimental results show Transformer-\nXL has stronger language modeling capability than\nthe non-recurrent transformer. Transformer-XL di-\nrectly uses all the hidden states from the previous\nsegment to enable recurrence. In comparison, our\nMART uses highly summarized memory states,\nmaking it more efﬁcient in passing useful semantic\nor linguistic cues to future sentences.\n3 Methods\nThough our method provides a general tempo-\nral multi-modal learning framework, we focus on\nthe video paragraph captioning task in this paper.\nGiven a video V, with several temporally ordered\nevent segments [e1,e2,...,e T ], the task is to gener-\nate a coherent paragraph consisting of multiple sen-\nMulti-Head \nAttention \nAdd\t&\tNorm \nFeed\tForward \nAdd\t&\tNorm \nCNN \nMasked\t \nMulti-Head \nAttention \nW ord\tEnbedding \nPE \nAdd\t&\tNorm \nMulti-Head \nAttention \nAdd\t&\tNorm \nFeed\tForward \nAdd\t&\tNorm \nx  N x  N \nLinear \nSoftmax \nOutputs\t(shifted\tright) V ideo\tSegment \nThe\tgirl\tdances\t \naround\tthe\troom . \nPE \nFigure 1: Vanilla transformer video captioning\nmodel (Zhou et al., 2018). PE denotes Positional En-\ncoding, TE denotes token Type Embedding.\ntences [s1,s2,...,s T ] to describe the whole video,\nwhere sentence st should describe the content in\nthe segment et. In the following, we ﬁrst describe\nthe baseline transformer that generates sentences\nwithout recurrent architecture, then introduce our\napproach – Memory-Augmented Recurrent Trans-\nformer (MART). Besides, we also compare MART\nwith the recently proposed Transformer-XL (Dai\net al., 2019) in detail.\n3.1 Background: Vanilla Transformer\nWe start by introducing the vanilla transformer\nvideo paragraph captioning model proposed\nby Zhou et al. (2018), which is an application of the\noriginal transformer (Vaswani et al., 2017) model\nfor video paragraph captioning. An overview of\nthe model is shown in Figure 1. The core of\nthe architecture is the scaled dot-product atten-\ntion. Given query matrix Q∈RTq×dk , key matrix\nK ∈RTv×dk and value matrix V ∈RTv×dv , the\nattentional output is computed as:\nA(Q,K,V ) = softmax\n(QK⊤\n√dk\n,dim=1\n)\nV,\nwhere softmax(·,dim=1) denotes performing soft-\nmax at the second dimension of the the input. Com-\nbining h paralleled scaled dot-product attention,\n2606\n\u0000\nLinear\nSoftmax\nMasked\t\nMulti-Head\nAttention\nAdd\t&\tNorm\nFeed\tForward\nAdd\t&\tNorm\nx\t N \nMemory-Augmented\tRecurrent\tTransformer\t(at\tstep\t)\u0000\nMulti-Head\nAttention\n\u0000\u0000\u0000−1\n\u0000 \u0000\nConcat \n\u0000\nMemory\nUpdater\n\u0000\u0000\u0000−1\n\u0000\u0000\u0000\nFeed\tForward\n\u0000¯\u0000\n\u0000\nMulti-Head\nAttention\n\u0000¯\u0000\n\u0000\u0000\u0000\u0000−1\n\u0000\u0000\nLinearLinear LinearLinear\ntanh\nAdd\nsigmoid\nAdd\n\u0000\u0000\u0000 \u0000\u0000\u0000\n=(1− )⊙ + ⊙\u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000−1\n\u0000\u0000\u0000\n\u0000\u0000\u0000\nLinear\nSoftmax\nFeed\tForward\nAdd\t&\tNorm\nx  N \nMasked\tMulti-Head \nAttention\twith \nRelative\tPE \n\u0000\u0000( )\u0000\u0000−1\u0000−1\n\u0000 \u0000\nConcat \n\u0000\n\u0000\u0000−1\u0000\nAdd\t&\tNorm\nTransformer-XL\t(at\tstep\t)\u0000\nWord\tEnbedding\nOutputs\t(shifted\tright) \nCNN\nPE \nV ideo\tSegment \nConcat\nLinear\t&\tNorm Linear\t&\tNorm\nTE \nThe\tgirl\tdances\t \naround\tthe\troom . \nWord\tEnbedding\nOutputs\t(shifted\tright) \nCNN\nV ideo\tSegment \nConcat\nLinear\t&\tNorm Linear\t&\tNorm\nTE \nThe\tgirl\tdances\t \naround\tthe\troom . \nTE \nPE \ntoken\tT ype\tEmbedding \nPositional\tEncoding \nElement-wise\tAddition \nFigure 2: Left: Our proposed Memory-Augmented Recurrent Transformer (MART) for video paragraph captioning.\nRight: Transformer-XL (Dai et al., 2019) model for video paragraph captioning. Relative PE denotes Relative\nPositional Encoding (Dai et al., 2019). SG(·) denotes stop-gradient, ⊙denotes Hadamard product.\nwe obtain the multi-head attention(Vaswani et al.,\n2017), we denote it as MultiHeadAtt(Q, K, V). The\nattention formulation discussed above is quite gen-\neral. It can be used for various purposes, such as\nself-attention (Vaswani et al., 2017) where query,\nkey, and value matrix are all the same, and cross-\nattention (Vaswani et al., 2017) where the query\nmatrix is different from the key and value matrix.\nIn this paper, we also use multi-head attention for\nmemory aggregation and update, as discussed later.\nThe vanilla transformer video paragraph caption-\ning model has N encoder layers and N decoder\nlayers. At the l-th encoder layer, the multi-head\nattention module takes the last layer’s hidden states\nHl−1 as inputs and performs self-attention. The\nattentional outputs are then projected by a feed-\nforward layer. At the l-th decoder layer, the model\nﬁrst encodes the last decoder layer’s hidden states\nusing masked multi-head attention.2 It then uses\nmulti-head attention, with the masked outputs as\nquery matrix, and the hidden states Hl from l-th\nencoder layer as key and value matrix to gather\n2masked multi-head attentionis used to prevent the model\nfrom seeing future words (Vaswani et al., 2017).\ninformation from the encoder side. Similarly, a\nfeed-forward layer is used to encode the sentences\nfurther. Residual connection (He et al., 2016) and\nlayer-normalization (Ba et al., 2016) are applied\nfor each layer, for both encoder and decoder.\n3.2 Memory-Augmented Recurrent\nTransformer\nThe vanilla transformer captioning model follows\nthe classical encoder-decoder architecture, where\nthe encoder and decoder network are separated. In\ncomparison, the encoder and decoder are shared\nin MART, as shown in Figure 2 ( left). The\nvideo and text inputs are ﬁrstly separately encoded\nand normalized. We denote the encoded video\nand text embeddings as H0\nvideo ∈RTvideo×d and\nH0\ntext ∈RTtext×d, where Tvideo and Ttext are the\nlengths of video and text, respectively. ddenotes\nthe hidden size. We then concatenate these two\nembeddings as input to the transformer layers:\nH0=[H0\nvideo; H0\ntext] ∈RTc×d, where [; ] denotes\nconcatenation, Tc=Tvideo + Ttext. This uniﬁed\nencoder-decoder design is inspired by recent works\non multi-modal representation learning (Chen et al.,\n2019; Sun et al., 2019). We also use two trainable\n2607\ntoken type embedding vectors to indicate whether\nan input token is from video or text, similar to De-\nvlin et al. (2019) where the token type embeddings\nare added to indicate different input sequences. We\nignore the video token positions and only consider\nthe text token positions when calculating loss and\ngenerating words.\nWhile the aforementioned vanilla transformer is\na powerful method, it is less suitable for video\nparagraph captioning due to its inability to uti-\nlize video segments and sentences history infor-\nmation. Thus, given the uniﬁed encoder-decoder\ntransformer, we augment it with an external mem-\nory module, which helps it to utilize video seg-\nments and the corresponding caption history to\ngenerate the next sentence. An overview of the\nmemory module is shown in Figure 2 ( left). At\nstep t, i.e., decoding the t-th video segment, the\nl-th layer aggregates the information from both\nits intermediate hidden states ¯Hl\nt ∈ RTc×d and\nthe memory states Ml\nt−1 ∈RTm×d (Tm denotes\nmemory state length or equivalently #slots in the\nmemory) from the last step, using a multi-head\nattention. The input query matrix of the multi-\nhead attention Q= ¯Hl\nt, key and value matrices are\nK,V =[Ml\nt−1; ¯Hl\nt] ∈ R(Tm+Tc)×d. The memory\naugmented hidden states are further encoded using\na feed forward layer and then merged with the inter-\nmediate hidden states ¯Hl\nt using a residual connec-\ntion and layer norm to form the hidden states output\nHl\nt ∈RTc×d. The memory state Ml\nt−1 is updated\nas Ml\nt , using the intermediate hidden states ¯Hl\nt.\nThis process is conducted in the Memory Updater\nmodule, illustrated in Figure 2. We summarize the\nprocedure below:\nSl\nt = MultiHeadAtt(Ml\nt−1, ¯Hl\nt, ¯Hl\nt),\nCl\nt = tanh(Wl\nmcMl\nt−1 + Wl\nscSl\nt + bl\nc),\nZl\nt = sigmoid(Wl\nmzMl\nt−1 + Wl\nszSl\nt + bl\nz),\nMl\nt = (1 −Zl\nt) ⊙Cl\nt + Zl\nt ⊙Ml\nt−1,\nwhere ⊙denotes Hadamard product, Wl\nmc, Wl\nsc,\nWl\nmz, and Wl\nsz are trainable weights, bl\nc and bl\nz are\ntrainable bias. Cl\nt ∈RTm×d is the internal cell\nstate. Zl\nt ∈RTm×d is the update gate that con-\ntrols which information to retain from the previous\nmemory state, and thus reducing redundancy and\nmaintaining coherence in the generated paragraphs.\nThis update strategy is conceptually similar to\nLSTM (Hochreiter and Schmidhuber, 1997) and\nGRU (Chung et al., 2014). It differs in that multi-\nhead attention is used to encode the memory state\nand thus multiple memory slots are supported in-\nstead of a single one in LSTM and GRU, which\ngives it a higher capacity of modeling complex\nrelations. Recent works (Sukhbaatar et al., 2015;\nGraves et al., 2014; Xiong et al., 2016a) introduce\na memory component into neural networks, where\nthe memory is mainly designed to memorize facts\nin the input context to support downstream tasks,\ne.g., copy (Graves et al., 2014) or question answer-\ning (Sukhbaatar et al., 2015; Xiong et al., 2016a).\nIn comparison, the memory in MART is designed\nto memorize the sequence generation history to sup-\nport the coherent generation of the next sequence.\n3.3 Comparison with Transformer-XL\nTransformer-XL (Dai et al., 2019) is a recently pro-\nposed transformer-based language model that uses\na segment-level recurrence mechanism to capture\nthe long-term dependency in context. In Figure 2\n(right) we show a modiﬁed version of Transformer-\nXL for video paragraph captioning. At step t, at\nits l-th layer, Transformer-XL takes as inputs the\nlast layer’s hidden states from both the current step\nand the last step, which we denote as Hl−1\nt and\nSG(Hl−1\nt−1), where SG(·) stands for stop-gradient,\nand is used to save GPU memory and computa-\ntion (Dai et al., 2019). The input query matrix of\nthe multi-head attention Q= Hl−1\nt , key and value\nmatrices are K,V = [SG(Hl−1\nt−1); Hl−1\nt ]. Note the\nmulti-head attention here is integrated with relative\npositional encoding (Dai et al., 2019).\nBoth designed to leverage the long-term depen-\ndency in context, the recurrence in Transformer-XL\nis between Hl\nt and Hl−1\nt−1, which shifts one layer\ndownwards per step. This mismatch in represen-\ntation granularity may potentially be harmful to\nthe learning process and affect the model perfor-\nmance. In contrast, the recurrence in MART is\nbetween ¯Hl\nt and Ml\nt−1 (updated using ¯Hl\nt−1) of the\nsame layer. Besides, Transformer-XL directly uses\nall the hidden states from the last step to enable\nrecurrence, which might be less effective as less\nrelevant and repetitive information is also passed\nalong. In comparison, MART achieves recurrence\nby using memory states that are highly summarized\nfrom previous steps, which may help the model to\nreduce redundancy and only keep important infor-\nmation from previous steps.\n2608\n4 Experiments\nWe conducted experiments on two popular bench-\nmark datasets, ActivityNet Captions (Krishna et al.,\n2017) and YouCookII (Zhou et al., 2017). We eval-\nuate our proposed MART and compare it with vari-\nous baseline approaches.\n4.1 Data and Evaluation Metrics\nDatasets ActivityNet Captions (Krishna et al.,\n2017) contains 10,009 videos in train set, 4,917\nvideos in val set. Each video in train has a single\nreference paragraph while each video in val has\ntwo reference paragraphs. Park et al. (2019) uses\nthe same set of videos (though different segments)\nin val for both validation and test. To allow better\nevaluation of the models, we use splits provided\nby Zhou et al. (2019), where the original val set\nis split into two subsets: ae-val with 2,460 videos\nfor validation and ae-test with 2,457 videos for test.\nThis setup makes sure the videos used for test will\nnot be seen in validation. YouCookII (Zhou et al.,\n2017) contains 1,333 training videos and 457 val-\nidation videos. Each video has a single reference\nparagraph. Both datasets come with temporal event\nsegments annotated with human written natural lan-\nguage sentences. On average, there are 3.65 event\nsegments for each video in ActivityNet Captions,\n7.7 segments for each video in YouCookII.\nData Preprocessing We use aligned appearance\nand optical ﬂow features extracted at 2FPS to\nrepresent videos, provided by Zhou et al. (2018).\nSpeciﬁcally, for appearance, 2048D feature vectors\nfrom the ‘Flatten-673’ layer in ResNet-200 (He\net al., 2016) are used; for optical ﬂow, 1024D fea-\nture vectors from the ‘global pool’ layer of BN-\nInception (Ioffe and Szegedy, 2015) are used. Both\nnetworks are pre-trained on ActivityNet (Caba Heil-\nbron et al., 2015) for action recognition, provided\nby (Xiong et al., 2016b). We truncate sequences\nlonger than 100 for video and 20 for text and set\nthe maximum number of video segments to 6 for\nActivityNet Captions and 12 for YouCookII. Fi-\nnally, we build vocabularies based on words that\noccur at least 5 times for ActivityNet Captions and\n3 times for YouCookII. The resulting vocabulary\ncontains 3,544 words for ActivityNet Captions and\n992 words for YouCookII.\nEvaluation Metrics (Automatic and Human)\nWe evaluate the captioning performance at\nparagraph-level, following (Park et al., 2019; Xiong\net al., 2018), reporting numbers on standard met-\nrics, including BLEU@4 (Papineni et al., 2002),\nMETEOR (Denkowski and Lavie, 2014), CIDEr-\nD (Vedantam et al., 2015). Since these metrics\nmainly focus on whether the generated paragraph\nmatches the ground-truth paragraph, they fail to\nevaluate the redundancy of these multi-sentence\nparagraphs. Thus, we follow previous works (Park\net al., 2019; Xiong et al., 2018) to evaluate repeti-\ntion using R@4. It measures the degree of N-gram\n(N=4) repetition in the descriptions. Besides the\nautomated metrics, we also conduct human evalu-\nations to provide additional comparisons between\nthe methods. We consider two aspects in human\nevaluation, relevance (i.e., how related is a gener-\nated paragraph caption to the content of the given\nvideo) and coherence (i.e., whether a generated\nparagraph caption reads ﬂuently and is linguisti-\ncally coherent over its multiple sentences).\n4.2 Implementation Details\nMART is implemented in PyTorch (Paszke et al.,\n2017). We set the hidden size to 768, the number\nof transformer layers to 2, and the number of atten-\ntion heads to 12. For positional encoding, we fol-\nlow Vaswani et al. (2017) to use the ﬁxed scheme.\nFor memory module, we set the length of recur-\nrent memory state to 1, i.e., Tm=1. We optimize\nthe model following the strategy used by Devlin\net al. (2019). Speciﬁcally, we use Adam (Kingma\nand Ba, 2014) with an initial learning rate of 1e-4,\nβ1=0.9, β2=0.999, L2 weight decay of 0.01, and\nlearning rate warmup over the ﬁrst 5 epochs. We\ntrain the model for at most 50 epochs with early\nstopping using CIDEr-D and batch size 16. We\nuse greedy decoding as we did not observe better\nperformance using beam search.\n4.3 Baselines\nVanilla Transformer This model originates\nfrom the transformer (Vaswani et al., 2017), pro-\nposed by Zhou et al. (2018) (more details in Sec-\ntion 3.1). It takes a single video segment as input\nand independently generates a single sentence de-\nscribing the given segment. Note that Zhou et al.\n(2018) also have a separate proposal generation\nmodule, but here we only focus on its captioning\nmodule. To obtain paragraph-level captions, the\nindependently generated single sentence captions\nare concatenated as the output paragraph.\n2609\nModel Re. ActivityNet Captions (ae-test) YouCookII ( val)\nB@4 M C R@4 ↓ B@4 M C R@4 ↓\nVTransformer (2018) \u0017 9.31 15.54 21.33 7.45 7.62 15.65 32.26 7.83\nTransformer-XL (2019) \u0013 10.25 14.91 21.71 8.79 6.56 14.76 26.35 6.30\nTransformer-XLRG \u0013 10.07 14.58 20.34 9.37 6.63 14.74 25.93 6.03\nMART \u0013 9.78 15.57 22.16 5.44 8.00 15.9 35.74 4.39\nHuman - - - - 0.98 - - - 1.27\nTable 1: Comparison with transformer baselines on ActivityNet Captionsae-test split and YouCookIIval split. Re.\nindicates whether sentence-level recurrence is used. We report BLEU@4 (B@4), METEOR (M), CIDEr-D (C)\nand Repetition (R@4). VTransformer denotes vanilla transformer.\nDet. Re. B@4 M C R@4 ↓\nLSTM based methods\nMFT (2018) \u0017 \u0013 10.29 14.73 19.12 17.71\nHSE (2018) \u0017 \u0013 9.84 13.78 18.78 13.22\nLSTM based methods with detection feature\nGVD (2019) \u0013 \u0017 11.04 15.71 21.95 8.76\nGVDsup (2019) \u0013 \u0017 11.30 16.41 22.94 7.04\nAdvInf (2019) \u0013 \u0013 10.04 16.60 20.97 5.76\nTransformer based methods\nVTransformer (2018) \u0017 \u0017 9.75 15.64 22.16 7.79\nTransformer-XL (2019)\u0017 \u0013 10.39 15.09 21.67 8.54\nTransformer-XLRG \u0017 \u0013 10.17 14.77 20.40 8.85\n(Ours) MART \u0017 \u0013 10.33 15.68 23.42 5.18\nHuman - - - - - 0.98\nTable 2: Comparison with baselines on ActivityNet\nCaptions ae-val split. Det. indicates whether the model\nuses detection feature. Models that use detection fea-\ntures are shown in gray background to indicate they are\nnot in fair comparison with the others. Re. indicates\nwhether sentence-level recurrence is used. VTrans-\nformer denotes vanilla transformer.\nTransformer-XL Transformer-XL is proposed\nby Dai et al. (2019) for modeling long-term de-\npendency in natural language. Here we adapt it\nfor video paragraph captioning (more details in\nSection 3.3). The original design of Transformer-\nXL stops gradients from passing between differ-\nent recurrent steps to save GPU memory and com-\nputation. To enable a more fair comparison with\nour model, we implemented a version that allows\ngradient ﬂow through different steps, calling this\nTransformer-XLRG (Transformer-XL with Recur-\nrent Gradient).\nAdvInf AdvInf (Park et al., 2019) uses a set of\nthree discriminators to do adversarial inference on a\nstrong LSTM captioning model. The input features\nof the LSTM model are the concatenation of image\nrecognition, action recognition, and object detec-\ntion features. To encourage temporal coherence be-\ntween consecutive sentences, the last hidden state\nfrom the previous sentence is used as input to the\ndecoder (Xiong et al., 2018; Gella et al., 2018). The\nthree discriminators are trained adversarially and\nare speciﬁcally designed to reduce repetition and\nencourage ﬂuency and relevance in the generated\nparagraph.\nGVD An LSTM based model for grounded video\ndescription (Zhou et al., 2019). It uses densely\ndetected object regions as inputs, with a ground-\ning module that grounds generated words to the\nregions. Additionally, we also consider a GVD\nvariant (GVDsup) that uses grounding supervision\nfrom Zhou et al. (2019).\nMFT MFT (Xiong et al., 2018) uses an LSTM\nmodel with a similar sentence-level recurrence as\nin AdvInf (Park et al., 2019).\nHSE HSE (Zhang et al., 2018) is a hierarchi-\ncal model designed to learn both clip-sentence\nand paragraph-video correspondences. Given the\nlearned contextualized video embedding, HSE uses\na 2-layer LSTM to generate captions.\nFor AdvInf, MFT, HSE, GVD, and GVDsup,\nwe obtain generated sentences from the authors.\nWe only report their performance on ActivityNet\nCaptions ae-val split to enable a fair comparison,\nas (i) AdvInf, MFT and HSE have different set-\ntings as ours, where ae-test videos are included\nas part of their validation set; (ii) we do not have\naccess to the ae-test predictions of GVD and GVD-\nsup. For vanilla transformer, Transformer-XL and\nTransformer-XLRG, we borrow/modify the model\nimplementations from the original authors and train\nthem under the same settings as MART.\n4.4 Results\nAutomatic Evaluation Table 1 shows the results\nof MART and several transformer baseline meth-\nods. We observe stronger or comparable perfor-\nmance for the language metrics (B@4, M, C) for\n2610\nMART wins (%) VTransformer wins (%) Delta\nrelevance 37 29.5 +7.5\ncoherence 42.8 26.3 +16.5\nMART wins (%) Transformer-XL wins (%) Delta\nrelevance 40.0 39.5 +0.5\ncoherence 39.2 36.2 +3.0\nTable 3: Human evaluation on ActivityNet Captionsae-\ntest set w.r.t. relevance and coherence. Top: MART vs.\nvanilla transformer (VTransformer). Bottom: MART\nvs. Transformer-XL.\nboth ActivityNet Captions and YouCookII datasets.\nFor R@4, MART produces signiﬁcantly better re-\nsults compared to the three transformer baselines,\nshowing its effectiveness in reducing redundancy\nin the generated paragraphs. Table 2 shows the\ncomparison of MART with state-of-the-art models\non ActivityNet Captions. MART achieves the best\nscores for both CIDEr-D and R@4 and has a com-\nparable performance for B@4 and METEOR. Note\nthat the best B@4 model, GVDsup (Zhou et al.,\n2019), and the best METEOR model, AdvInf (Park\net al., 2019), both use strong detection features, and\nGVDsup has also used grounding supervision. Re-\ngarding the repetition score R@4, MART has the\nhighest score. It outperforms the strong adversarial\nmodel AvdInf (Park et al., 2019) even in an unfair\ncomparison where AdvInf uses extra detection fea-\ntures. Additionally, AdvInf has a time-consuming\nadversarial training and decoding process where a\nset of discriminator models are trained and used to\nre-rank candidate sentences, while MART can do\nmuch faster inference with only greedy decoding\nand no further post-processing. The comparisons\nin Table 1 and Table 2 show that MART is able to\ngenerate less redundant (thus more coherent) para-\ngraphs while maintaining relevance to the videos.\nHuman Evaluation In addition to the automatic\nmetrics, we also run human evaluation on Ama-\nzon Mechanical Turk (AMT) with 200 randomly\nsampled videos from ActivityNet Captions ae-test\nsplit, where each video was judged by three dif-\nferent AMT workers. We design a set of pairwise\nexperiments (Pasunuru and Bansal, 2017b; Park\net al., 2019), where we compare two models at\na time. AMT workers are instructed to choose\nwhich caption is better or the two captions are not\ndistinguishable based on relevance and coherence,\nrespectively. The models are anonymized, and the\npredictions are shufﬂed. In total, we have 54 work-\n#hidden\nlayers\nmem.\nlen. Re. B@4 M C R@4 ↓\n#hidden layers\nMART 1 1 \u0013 10.42 16.0122.87 6.70\nMART 5 1 \u0013 10.48 16.03 24.33 6.74\nmem. len.\nMART 2 2 \u0013 10.30 15.66 22.93 5.94\nMART 2 5 \u0013 10.12 15.48 22.89 6.83\nrecurrence\nMART w/o re. 2 - \u0017 9.91 15.83 22.78 7.56\nMART 2 1 \u0013 10.33 15.68 23.42 5.18\nTable 4: Model ablation on ActivityNet Captions ae-\nval split. Re. indicates whether sentence-level recur-\nrence is used. mem. len. indicates the length of the\nmemory state. MART w/o re.denotes a MART variant\nwithout recurrence. Top two scores are highlighted.\ners participated the MART vs. vanilla transformer\nexperiments, 47 workers participated the MARTvs.\nTransformer-XL experiments. In Table 3 we show\nhuman evaluation results, where the scores are cal-\nculated as the percentage of workers that have voted\na certain option. With its sentence-level recurrence\nmechanism, MART is substantially better than the\nvanilla transformer model for both relevance and\ncoherence. Compared to the strong baseline ap-\nproach Transformer-XL, MART has similar perfor-\nmance in terms of relevance, but still reasonably\nbetter performance in terms of coherence.\nModel Ablation We show model ablation in Ta-\nble 4. MART models with recurrence have better\noverall performance than the variant without, sug-\ngesting the effectiveness of our recurrent memory\ndesign. We choose to use the model with 2 hidden\nlayers and memory state length 1 as it shows a good\nbalance between performance and computation.\nQualitative Examples In Figure 3, we show\nparagraph captions generated by vanilla trans-\nformer, Transformer-XL, and our method MART.\nCompared to the two baselines, MART produces\nmore coherent and less redundant paragraphs. In\nparticular, we noticed that vanilla transformer often\nuses incoherent pronouns/person mentions, while\nMART and Transformer-XL is able to use suitable\npronouns/person mentions across the sentences and\nthus improve the coherence of the paragraph. Com-\npare with Transformer-XL, we found that the para-\ngraphs generated by MART have much less cross-\nsentence repetitions. We attribute MART’s suc-\ncess to its recurrence design - the previous memory\nstates are highly summarized, in which redundant\ninformation is removed. While there is less redun-\n2611\nVanilla Transformer\nHe is sitting down in a chair. He continues playing the harmonica and ends by \nlooking off into the distance. He continues playing the harmonica and looking off \ninto the distance. He stops playing and looks at the camera.\nTransformer-XL\nA man is seen speaking to the camera while holding a harmonica. He continues \nplaying the harmonica while looking at the camera. He continues playing the \ninstrument and looking off into the distance. He continues playing and stops playing.\nMART (ours)\nA man is sitting down talking to the camera while holding a camera. He takes a \nharmonica and begins playing his harmonica. He continues playing the harmonica as \nhe continues playing. He stops and looks at the camera.\nGround-Truth\nA young man wearing a Cuervo black shirt stares and speaks to the camera as he sits \non his chair. He puts a harmonica to his mouth and begins playing. He plays on for \nabout a minute and is very into his song. He then puts the harmonica down and \nlooks into the camera as the video comes to an end.\nVanilla Transformer\nA girl is seen climbing across a set of monkey bars and leads into her climbing \nacross a set of. He jumps off the monkey bars and lands on a bridge.\nTransformer-XL\nA young child is seen climbing across a set of monkey bars and climbing across a set \nof monkey bars. The boy jumps down and jumps down and jumps down.\nMART (ours)\nA girl is seen speaking to the camera and leads into her climbing across a set of \nmonkey bars. She jumps off the bar and walks back to the camera.\nGround-Truth\nA little girl climbs the monkey bars of a play ground. Then, the little girl jumps to \nthe ground and extend her arms.\nFigure 3: Qualitative examples. Red/bold indicates pronoun errors (inappropriate use of pronouns), blue/italic\nindicates repetitive patterns, underline indicates content errors. Compared to baselines, our model generates more\ncoherent, less repeated paragraphs while maintaining relevance.\nA girl is giving a small dog a bath. She has an orange bottle in \nher hand…\nA man on a diving board walks to the end. The man bounces \non the board two times then dives into the water…\nA young girl is seen walking to the end of a diving board with \nseveral other people around her…\nA little girl stands on a diving board. Then the little girl \njumps, flip and dives in the swimming pool…\nFigure 4: Nearest neighbors retrieved using memory\nstates. Top row shows the query, the 3 rows below it\nare the top-3 nearest neighbors.\ndancy between sentences generated by MART, in\nFigure 3 (left), we noticed that repetition still ex-\nists within a single sentence, suggesting further\nefforts on reducing the repetition in single sentence\ngeneration. More examples are in the appendix.\nMemory Ablation To explore whether the\nlearned memory state could store useful informa-\ntion about the videos and captions, we conducted a\nvideo retrieval experiment on ActivityNet Captions\ntrain split with 10K videos, where we extract the\nlast step memory state in the ﬁrst layer of a trained\nMART model for each video as its representation to\nperform nearest neighbor search with cosine simi-\nlarity. Though not explicitly trained for the retrieval\ntask, we observe some positive examples in the ex-\nperiments. We show an example in Figure 4, the\nneighbors mostly show related activities.\n5 Conclusion\nIn this work, we present a new approach – Memory-\nAugmented Recurrent Transformer (MART) for\nvideo paragraph captioning, where we designed an\nauxiliary memory module to enable recurrence in\ntransformers. Experimental results on two standard\ndatasets show that MART has better overall per-\nformance than the baseline methods. In particular,\nMART can generate more coherent, less redundant\nparagraphs without any degradation in relevance.\nAcknowledgments\nWe thank the anonymous reviewers for their help-\nful comments and discussions. This work was\nperformed while Jie Lei was an intern at Ten-\ncent AI Lab, Seattle, USA. It was later partially\nsupported by NSF Awards CAREER-1846185,\n1562098, DARPA KAIROS Grant FA8750-19-2-\n1004, and ARO-YIP Award W911NF-18-1-0336.\nThe views contained in this article are those of the\nauthors and not of the funding agency.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E\nHinton. 2016. Layer normalization. Advances in\n2612\nNeurIPS 2016 Deep Learning Symposium.\nFabian Caba Heilbron, Victor Escorcia, Bernard\nGhanem, and Juan Carlos Niebles. 2015. Activi-\ntynet: A large-scale video benchmark for human ac-\ntivity understanding. In CVPR.\nDavid L Chen and William B Dolan. 2011. Collect-\ning highly parallel data for paraphrase evaluation. In\nACL.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El\nKholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2019. Uniter: Learning univer-\nsal image-text representations. arXiv preprint\narXiv:1909.11740.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho,\nand Yoshua Bengio. 2014. Empirical evaluation of\ngated recurrent neural networks on sequence model-\ning. In NIPS 2014 Workshop on Deep Learning.\nZihang Dai, Zhilin Yang, Yiming Yang, William W\nCohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. 2019. Transformer-xl: Attentive lan-\nguage models beyond a ﬁxed-length context. In\nACL.\nMichael Denkowski and Alon Lavie. 2014. Meteor\nuniversal: Language speciﬁc translation evaluation\nfor any target language. In Proceedings of the ninth\nworkshop on statistical machine translation.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL.\nSpandana Gella, Mike Lewis, and Marcus Rohrbach.\n2018. A dataset for telling the stories of social media\nvideos. In EMNLP.\nAlex Graves, Greg Wayne, and Ivo Danihelka.\n2014. Neural turing machines. arXiv preprint\narXiv:1410.5401.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In CVPR.\nSepp Hochreiter, Yoshua Bengio, Paolo Frasconi,\nJ¨urgen Schmidhuber, et al. 2001. Gradient ﬂow in\nrecurrent nets: the difﬁculty of learning long-term\ndependencies.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation,\n9(8):1735–1780.\nSergey Ioffe and Christian Szegedy. 2015. Batch nor-\nmalization: Accelerating deep network training by\nreducing internal covariate shift. In ICML.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. In ICLR.\nRanjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei,\nand Juan Carlos Niebles. 2017. Dense-captioning\nevents in videos. In ICCV.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In ACL.\nJae Sung Park, Marcus Rohrbach, Trevor Darrell, and\nAnna Rohrbach. 2019. Adversarial inference for\nmulti-sentence video description. In CVPR.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio.\n2013. On the difﬁculty of training recurrent neural\nnetworks. In ICML.\nRamakanth Pasunuru and Mohit Bansal. 2017a. Multi-\ntask video captioning with video and entailment gen-\neration. In ACL.\nRamakanth Pasunuru and Mohit Bansal. 2017b. Rein-\nforced video captioning with entailment rewards. In\nEMNLP.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming\nLin, Alban Desmaison, Luca Antiga, and Adam\nLerer. 2017. Automatic differentiation in PyTorch.\nIn NeurIPS Autodiff Workshop.\nAnna Rohrbach, Marcus Rohrbach, Wei Qiu, An-\nnemarie Friedrich, Manfred Pinkal, and Bernt\nSchiele. 2014. Coherent multi-sentence video de-\nscription with variable level of detail. In GCPR.\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.\n2015. End-to-end memory networks. In NeurIPS.\nChen Sun, Austin Myers, Carl V ondrick, Kevin Mur-\nphy, and Cordelia Schmid. 2019. Videobert: A joint\nmodel for video and language representation learn-\ning. In ICCV.\nHao Tan and Mohit Bansal. 2019. Lxmert: Learning\ncross-modality encoder representations from trans-\nformers. In EMNLP.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. 2015. Cider: Consensus-based image de-\nscription evaluation. In CVPR.\nXin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-\nFang Wang, and William Yang Wang. 2019. Vatex:\nA large-scale, high-quality multilingual dataset for\nvideo-and-language research. In ICCV.\nCaiming Xiong, Stephen Merity, and Richard Socher.\n2016a. Dynamic memory networks for visual and\ntextual question answering. In ICML.\n2613\nYilei Xiong, Bo Dai, and Dahua Lin. 2018. Move for-\nward and tell: A progressive generator of video de-\nscriptions. In ECCV.\nYuanjun Xiong, Limin Wang, Zhe Wang, Bowen\nZhang, Hang Song, Wei Li, Dahua Lin, Yu Qiao,\nLuc Van Gool, and Xiaoou Tang. 2016b. Cuhk &\nethz & siat submission to activitynet challenge 2016.\narXiv preprint arXiv:1608.00797.\nJun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-\nvtt: A large video description dataset for bridging\nvideo and language. In CVPR.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In NeurIPS.\nBowen Zhang, Hexiang Hu, and Fei Sha. 2018. Cross-\nmodal and hierarchical modeling of video and text.\nIn ECCV.\nLuowei Zhou, Yannis Kalantidis, Xinlei Chen, Jason J.\nCorso, and Marcus Rohrbach. 2019. Grounded\nvideo description. In CVPR.\nLuowei Zhou, Chenliang Xu, and Jason J. Corso. 2017.\nTowards automatic learning of procedures from web\ninstructional videos. In AAAI.\nLuowei Zhou, Yingbo Zhou, Jason J Corso, Richard\nSocher, and Caiming Xiong. 2018. End-to-end\ndense video captioning with masked transformer. In\nCVPR.\nA Appendices\nA.1 Additional Qualitative Examples\nWe show more caption examples in Figure 5.\nOverall, we see captions generated by models\nwith sentence-level recurrence, i.e., MART and\nTransformer-XL, tend to be more coherent. Com-\nparing with Transformer-XL, captions generated\nby MART are usually less repetitive. However,\nas shown in the two examples at the last row of\nFigure 5, all three models suffer from the content\nerror, where the models are not able to recognize\nand describe the ﬁne-grained details in the videos,\ne.g., gender and ﬁne-grained objects/actions.\n2614\nVanilla Transformer\nHe continues speaking while holding the violin and showing how to play his \nhands. He continues playing the instrument while looking down at the camera. He \ncontinues playing the violin and then stops to speak to the camera.\nTransformer-XL\nA man is seen speaking to the camera while holding a violin. The man continues \nplaying the instrument while moving his hands up and down. The man continues \nplaying the instrument and ends by looking back to the camera.\nMART (ours)\nA man is seen speaking to the camera while holding a violin and begins playing the \ninstrument. The man continues to play the instrument while moving his hands up \nand down. He continues to play and ends by moving his hands up and down.\nGround-Truth\nA man is seen looking to the camera while holding a violin. The man then begins \nplaying the instrument while the camera zooms in on his fingers. The man continues \nto play and stops to speak to the camera.\nVanilla Transformer\nHe is skateboarding down a road. He goes through the streets and goes. He is \nskateboarding down a road.\nTransformer-XL\nA man is riding a skateboard down a road. He is skateboarding down a road. He is \nskateboarding down a road.\nMART (ours)\nA man is seen riding down a road with a person walking into frame and speaking to \nthe camera. The man continues riding down the road while looking around to the \ncamera and showing off his movements. The man continues to ride around while \nlooking to the camera.\nGround-Truth\nA camera pans all around an area and leads into a man speaking to the camera.\nSeveral shots of the area are shown as well as dogs and leads into a man riding down \na hill. The man rides a skateboard continuously around the area and ends by meeting \nup with the first man.\nVanilla Transformer\nShe continues moving around the room and leads into her speaking to the \ncamera. She continues moving around on the step and ends by speaking to the \ncamera.\nTransformer-XL\nA woman is standing in a gym. She begins to do a step.\nMART (ours)\nA woman is standing in a room talking. She starts working out on the equipment.\nGround-Truth\nA woman is seen speaking to the camera and leads into her walking up and down \nthe board. She then stands on top of the beam while speaking to the camera \ncontinuously.\nVanilla Transformer\nSeveral shots are shown of people riding on the surf board and the people riding \nalong the water. Several shots are shown of people riding around on a surf board\nand leads into several clips of people riding.\nTransformer-XL\nA large wave is seen followed by several shots of people riding on a surf board and \nriding along the. The people continue riding along the water while the camera pans \naround the area and leads into several more shots.\nMART (ours)\nA man is seen riding on a surfboard and surfing on the waves. The man continues \nsurfing while the camera captures him from several angles.\nGround-Truth\nA man is seen moving along the water on a surf board while another person watches \non the side. The person continues riding around and slowing down to demonstrate \nhow to play.\nVanilla Transformer\nA young girl is seen climbing across a set of monkey bars. A young child is seen \nclimbing across a set of monkey bars. A little girl is standing on a platform in a \nplayground.\nTransformer-XL\nA young child is seen standing before a set of monkey bars and begins climbing \nacross monkey bars. The girl then climbs back and fourth on the bars.\nMART (ours)\nA young child is seen climbing across a set of monkey bars while speaking to the \ncamera. She then climbs down across the bars and begins swinging herself around. \nShe continues to swing down and ends by jumping down.\nGround-Truth\nA boy goes across the monkey bars as a lady watches and cheers him on. At the end \nhe begins to struggle bit, but finally finished. When he is done another little boy \ncomes and stands by him.\nVanilla Transformer\nThe man then holds up a bottle of mouthwash and talks to the camera. The man \nthen puts lotion on her face and begins rubbing it down. The man then begins to \nblow dry her face and shows off the camera.\nTransformer-XL\nA man is seen speaking to the camera while holding up a brush. He then rubs lotion \nall over his face and begins brushing his face. He then puts the lotion on the face and \nrubs it on the wall.\nMART (ours)\nA man is seen speaking to the camera and leads into him holding up a bottle of \nwater. The man then holds up a can and begins to shave his face. He finishes putting \nthe paper into the mirror and smiles to the camera.\nGround-Truth\nA girl's face is shown in front of the camera. She showed an orange bottle, read the \nlabel and squirt the orange content on her palm, showed the cream on the camera, \nthen rub the cream all over her face. She bend down and rinse her face, when her \nface is visible on the camera her face is clear.\nFigure 5: Additional qualitative examples. Red/bold indicates pronoun errors (inappropriate use of pronouns or per-\nson mentions), blue/italic indicates repetitive patterns, underline indicates content errors. Compared to baselines,\nour model generates more coherent, less repeated paragraphs while maintaining relevance.",
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.9366150498390198
    },
    {
      "name": "Paragraph",
      "score": 0.9093405604362488
    },
    {
      "name": "Computer science",
      "score": 0.7975577116012573
    },
    {
      "name": "Sentence",
      "score": 0.7552019357681274
    },
    {
      "name": "Transformer",
      "score": 0.7385585904121399
    },
    {
      "name": "Natural language processing",
      "score": 0.5851673483848572
    },
    {
      "name": "Speech recognition",
      "score": 0.5405612587928772
    },
    {
      "name": "Encoder",
      "score": 0.5083057284355164
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4542560577392578
    },
    {
      "name": "Voltage",
      "score": 0.09231248497962952
    },
    {
      "name": "Engineering",
      "score": 0.06922322511672974
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "World Wide Web",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1333535994",
      "name": "University of North Carolina Health Care",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I114027177",
      "name": "University of North Carolina at Chapel Hill",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I70745867",
      "name": "KLA (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I58610484",
      "name": "Seattle University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1290206253",
      "name": "Microsoft (United States)",
      "country": "US"
    }
  ],
  "cited_by": 162
}