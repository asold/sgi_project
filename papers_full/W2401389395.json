{
  "title": "Contrastive Entropy: A new evaluation metric for unnormalized language models",
  "url": "https://openalex.org/W2401389395",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5007392934",
      "name": "Kushal Arora",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5059870257",
      "name": "Anand Rangarajan",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1984635093",
    "https://openalex.org/W1850668662",
    "https://openalex.org/W2137807925",
    "https://openalex.org/W1773599773",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W1494910745",
    "https://openalex.org/W179875071",
    "https://openalex.org/W6328264",
    "https://openalex.org/W2117130368",
    "https://openalex.org/W2113687551",
    "https://openalex.org/W2474824677"
  ],
  "abstract": "Perplexity (per word) is the most widely used metric for evaluating language models. Despite this, there has been no dearth of criticism for this metric. Most of these criticisms center around lack of correlation with extrinsic metrics like word error rate (WER), dependence upon shared vocabulary for model comparison and unsuitability for unnormalized language model evaluation. In this paper, we address the last problem and propose a new discriminative entropy based intrinsic metric that works for both traditional word level models and unnormalized language models like sentence level models. We also propose a discriminatively trained sentence level interpretation of recurrent neural network based language model (RNN) as an example of unnormalized sentence level model. We demonstrate that for word level models, contrastive entropy shows a strong correlation with perplexity. We also observe that when trained at lower distortion levels, sentence level RNN considerably outperforms traditional RNNs on this new metric.",
  "full_text": "Contrastive Entropy: A new evaluation metric for unnormalized language\nmodels\nKushal Arora, Anand Rangarajan\nDept. of Computer and Information Science and Engineering,\nUniversity of Florida, Gainesville, Fl, USA\n{karora, anand}@cise.ufl.edu\nAbstract\nPerplexity (per word) is the most widely used metric for evalu-\nating language models. Despite this, there has been no dearth of\ncriticism for this metric. Most of these criticisms center around\nlack of correlation with extrinsic metrics like word error rate\n(WER), dependence upon shared vocabulary for model compar-\nison and unsuitability for unnormalized language model evalu-\nation. In this paper, we address the last problem and propose\na new discriminative entropy based intrinsic metric that works\nfor both traditional word level models and unnormalized lan-\nguage models like sentence level models. We also propose a\ndiscriminatively trained sentence level interpretation of recur-\nrent neural network based language model (RNN) as an exam-\nple of unnormalized sentence level model. We demonstrate that\nfor word level models, contrastive entropy shows a strong cor-\nrelation with perplexity. We also observe that when trained at\nlower distortion levels, sentence level RNN considerably out-\nperforms traditional RNNs on this new metric.\n1. Introduction\nThere are two standard evaluation metrics for language mod-\nels: perplexity and word error rate (WER). The simpler of these\ntwo, WER, is the percentage of erroneously recognized words\nE (deletions, insertions, substitutions) to the total number of\nwords N in a speech recognition task i.e.\nWER = E\nN ×100%. (1)\nThe second metric, perplexity (per word), is an information the-\noretic measure that evaluates the similarity between the pro-\nposed probability distribution m and the original distribution\np. It can be computed as an inverse of the (geometric) average\nprobability of test set T\nPPL(T) = 1\nn\n√\nm(T)\n(2)\nwhere nis the number of words in the test set T.\nIn many ways, WER is a better metric. Any improvements\non language modeling benchmarks is meaningful only if they\ntranslate to improvements in Automatic Speech Recognition\n(ASR) or Machine Translation. The problem with WER is that\nit needs a complete ASR pipeline to evaluate. Also, almost all\nbenchmarking datasets are behind a pay-wall, hence not readily\navailable for evaluation.\nPerplexity, on the other hand, is a theoretically elegant and\neasy to compute metric which correlates well with WER for\nsimpler n-gram models. This makes PPL a good substitute for\nWER when evaluating n-grams models, but for more complex\nlanguage models the correlation is not so strong [1]. In addition\nto this, due to its reliance on exact probabilities, perplexity is an\nunsuitable metric to evaluate unnormalized models for which\nthe partition function is intractable. Also, when comparing two\nmodels using perplexity, they must share the same vocabulary.\nMost of the previous work done to improve upon perplex-\nity has been focused on achieving better correlation with WER.\nIyer et al. [1] proposed a decision tree based metric that uses\nadditional features like word length, POS tags and phonetic\nlength of words to improve the WER correlation. Chen et al.\n[2] proposed a new metric M-ref which attempts to learn the\nlikelihood curve between WER and perplexity. Clarkson et al.\n[3] attempted to use entropy in conjunction with perplexity—\nempirically learning the mixing coefﬁcients.\nIn this paper we focus on a different problem, the prob-\nlem of extending perplexity for unnormalized language models\nevaluation. We do so by introducing a discriminative approach\nto language model evaluation. Our approach is inspired by Con-\ntrastive Estimation [4] and stems from the philosophical starting\npoint that a superior language model should be able to distin-\nguish better between the sentence from the test set and its de-\nformed version. While we use an unnormalized sentence level\nmodel as an example in this paper this technique should work\nfor all models where partition function is intractable, for ex-\nample unnormalized Model M and feed forward neural network\nlanguage model (NNLM) from [5] or sentence level models like\n[6], [7] and [8].\nIn the next section, we give a sketch derivation of perplex-\nity that highlights its word level model assumption. As we will\nbe using a sentence level language model for evaluation, we\nthen move the probability space to sentences and derive an ex-\npression for cross entropy rate for sentence level models. In\nSection 3, we introduce our new discriminative metric, Con-\ntrastive Entropy, which removes the normalization requirement\nassociated with perplexity. In Section 4, we formulate recur-\nrent neural networks as sentence level language models that we\nuse for validation and in Section 5 we analyze this new metric\nacross various models on the Pen-TreeBank section of the WSJ\ndataset. We conclude this paper by hypothesizing a better corre-\nlation between WER and contrastive entropy based on the fact\nthey share the same goal of minimizing errors in prediction.\n2. Sentence level cross entropy rate\nThe Perplexity deﬁned in equation (2) can also seen as expo-\nnentiated cross entropy rate, H(p,m), with cross entropy ap-\nproximated as\nH(p,m; T) =−1\nnlog(m(T)). (3)\narXiv:1601.00248v2  [cs.CL]  31 Mar 2016\nThis approximation can be derived viewing language as one\ncontinuous, inﬁnite stream of words leading to the following\nexpression for cross entropy rate:\nH(p,m) = lim\nl→∞\n−1\nl\n∑\nwl\n1∈Wl\n1\np(wl\n1) log(m(wl\n1)). (4)\nwhere Wl\n1 is a set of all the sentences of length l\nNow, assuming the language to be ergodic and stationary,\nthe Shannon-McMillan-Breiman Theorem [9] states that (4) can\nbe approximated as a single sequence that is long enough, hence\nH(p,m) =−1\nnlog(m(wn\n1 )). (5)\nHere, wn\n1 is the test set T and nbeing the number of words\nin this test set.\nIn this derivation language was seen as an inﬁnite stream of\nwords. If instead, we build a sample space on sentences, then\nwe can deﬁne the cross entropy of language as an inﬁnite stream\nof sentences as\nH(p,m) = lim\nl→∞\n−1\nl\n∑\nD∈Dl\n1\np(D) log(m(D)).\nDl\n1 here is a set of all documents containing lsentences.\nNow, applying the Shannon-McMillan-Breiman Theorem\nas we did in (5) and assuming that the sentences are indepen-\ndent and identically distributed, we can approximate the cross\nentropy rate of the sentence level model as\nH(p,m; T) =− 1\nN log(m(T))\n− 1\nN\n∑\nWn∈T\nlog(m(Wn)). (6)\nwhere N is the number of sentences in the test set T.\nAs the cross entropy still depends upon the exact probabil-\nity, equation (6) is still intractable. In the next section, we over-\ncome this problem by deﬁning a discriminative evaluation met-\nric which, instead of trying to minimize the distance between\nthe original distribution pand the proposed distribution m, tries\nto maximize the discriminative ability of the model towards the\ntest set from its distorted version.\n3. Contrastive Entropy and Contrastive\nEntropy Ratio\nLet T be the test set. We pass this test set through a noisy chan-\nnel and let the distorted version of this test set be ˆT. We now\ndeﬁne the contrastive entropy rate as\nHC(T; d) = H( ˆT; d) −H(T)\n= −1\nN log\n(\np( ˆT; d)\np(T)\n)\n= −1\nN log\n(\n˜p( ˆT; d)\n˜p(T)\n)\n. (7)\nHere, dis a measure of the distortion introduced in the test set,\n˜pis the unnormalized probability and N is the size of the test\nset, which is the cardinality of words and sentences for word\nand sentence level models respectively.\nHC/HCR\nHigher or similar\nHCR\nLower HCR\nHigher HC Superior Scaling issues\nLower HC Indeterminate Inferior\nTable 1: Contrastive Entropy (HC) and Contrastive Entropy Ra-\ntio (HCR) matrix\nThe intuition behind our evaluation technique is that the dis-\ntorted test set ˆT can be seen as an out of domain text, and that\na superior language model should be able to better discriminate\nin-domain text from the language from the malformed set that\nare less likely to be generated by the same language source.\nThe metric proposed above still has a major drawback. It is\nnot scale invariant. Let’s say a modelM generates a probability\ndistribution mfor test set T. We can simply cheat on this metric\nby proposing a model that exponentiates the probability by a\nfactor of k, i.e. multiplies the entropy by factor of k. This\nlimits the usefulness of the contrastive entropy to intra-model\ncomparison for hyper-parameter optimization.\nWe overcome this issue by reporting an additional value for\neach model which we term the contrastive entropy ratio. The\nidea here is to choose a distortion level as baseline, let’s say 10%\nand report the gain for a higher distortion levels, for example\n30% over this baseline distortion :\nHCR(T; db,d) = Hc(T,d)\nHc(T,db). (8)\nNeither of the two numbers can provide a complete picture\nin isolation. Contrastive entropy can be cheated upon by scal-\ning entropy, on the other hand, there is no guarantee that the\ncontrastive entropy ratio would rise faster for a better discrimi-\nnative model, but together, they balance each other out. Table 1\nshows how to interpret these values. A model with higher con-\ntrastive entropy and a higher or similar contrastive entropy ratio\nwould mean that it performs better at discriminating the good\nexamples from the bad ones, whereas, a larger contrastive en-\ntropy with lower ratio would mean that models use different\nscales, and a higher ratio with lower cross entropy would not\nmean much while comparing the two models.\n4. Sentence-level RNNLM\nAs the metric we proposed here benchmarks the unnormalized\nlevel models, in this section we propose a simple sentence level\nlanguage model that we can use to show the efﬁcacy of our\nmetric. This new model is simply an unfolded Recurrent Neu-\nral Network Language Model [10] build at sentence level and\ntrained to maximize the margin between a valid sentence and\nits distorted version.\nThe Recurrent Neural Network based Language model can\nbe deﬁned recursively using the following equations\nx(t) =\n[\nw(t−1)Ts(t−1)T]T\n, (9)\ns(t) = f(Ux(t)), and (10)\ny(t) = g(Ws(t)). (11)\nEquations (9) and (10) can be seen as building latent space\nrepresentations of phrases using words and history and (11) can\nbe seen as predicting the probability of this word given the con-\ntext. This phrasal representation built in (9) and (10) then would\nbe treated as the history for the next step. A standard sigmoidal\nnonlinearity is used for f and the probability distribution func-\ntion gis a standard softmax.\nIf we limit the context to sentence levels and move the prob-\nability space to the sequence of the words or n-grams, equa-\ntion (9) and (10) can be seen as composition function building\nphrase x(t), of the length n, from sub-phrase x(t−1), of the\nlength n−1, and the nth word w(t). Equation (11) can be seen\nas building the unnormalized probability ˜pover the phrasex(t).\nWe can rephrase the equations (9), (10) and (11) as\nx(t) = f\n(\nU\n[\nx(t−1)\nw(t)\n])\n, and (12)\ny(t) = g(Wx(t)). (13)\nHere we use the standard sigmoidal non linearity for the func-\ntion f and the identity function for g.\nWe now deﬁne the score of a length N sentence W as\nS(W) =\nN∑\nt=1\ny(t). (14)\nThe probability of the sentence can now be modeled as an\nexponential distribution\np(W) = 1\nZe−S(W). (15)\nwhere Z is the partition function and the contrastive entropy\nfrom (7) can be calculated as\nHc(T) = 1/N\n∑\nW∈T\n(\nS( ˆWd) −S(W)\n)\n. (16)\nwhere ˆWd is the distorted version ofW with distortion per-\ncentage d.\nTraining is done using a contrastive criterion where we try\nto maximize the distance between the in-domain sentence and\nits distorted version. This formulation is similar to one followed\nby Collobert et al. [8] and Okanohara et al. [7] for language\nmodeling and by Smith and Eisner [4] for POS tagging. Mathe-\nmatically, we can deﬁne this pseudo discriminative training ob-\njective as\nθ⋆ = arg min\nθ\n∑\nd∈D\nmax\n{\n0,1 −S(Wd) +S( ˆWd)\n}\n. (17)\nwhere ˆWd is the distorted version of sentence Wd and θ =\n(U,X,W ) is the parameter of the model.\nThis simplistic sentence level recurrent neural network\nmodel is implemented in python using Theano [11] and is avail-\nable at https://github.com/kushalarora/sentenceRNN.\n5. Experiments\nWe use the Pen Treebank dataset with the following splits and\npreprocessing: Sections 0-20 were used as training data, sec-\ntions 21-22 for validation and 23-24 for testing. The training,\nvalidation and testing token sizes are 930k, 74k and 82k respec-\ntively. The vocabulary is limited to 10k words with all words\noutside this set mapped to a special token <unk> .\nWe start by examining the distortion generation mecha-\nnism. As the evaluation includes the word level models, we\nneed to preserve the word count. To do this, we restrict dis-\ntortions to only two types: substitution and transpositions. For\nFigure 1: Test contrastive entropy monotonically increasing for\nwith test set distortion level.\nFigure 2: Training contrastive entropy monotonically increas-\ning with epochs.\nFigure 3: Test contrastive entropy increasing with training dis-\ntortion margin. Yellow , red and blue line represent training\nwith distortion margin of 50%, 30% and 10% respectively.\nsubstitutions, we randomly swap the current word in the sen-\ntence with a random word from the vocabulary. For transpo-\nsition, we randomly select a word from the same sentence and\nswap it with the current one. For each word in a sentence, there\nare three possible outcomes: no distortion with probability xN,\nsubstitution with probability xS and transposition with proba-\nbility xT with xN+ xS + xT = 1.\nNow, let’s start by considering the sentence level RNN\nmodel proposed in section 4. For contrastive entropy to be a\nModel PPL 10%\nHC\n30%\nHC\n50%\nHC\n3-gram KN 148.28 1.993 4.179 5.279\n5-gram KN 141.46 2.021 4.198 5.308\nRNN 141.31 2.546 5.339 6.609\nsRNN-75(50) - 1.978 3.961 6.477\nsRNN-75(10) - 2.339 6.759 11.01\nsRNN-150(10) - 2.547 7.581 12.925\nTable 2: Contrastive entropy ( HC) and perplexity ( PPL) for\nn-gram, RNNLM and sRNN models at 10%, 30% and 50% dis-\ntortion levels.\ngood measure for sentence level models, the following asser-\ntions should be true: i) contrastive entropy should monotoni-\ncally increase with distortions, ii) contrastive entropy of training\nset should go down with each epoch, and iii) contrastive entropy\nshould increase with increase in training distortion margin. Fig-\nures 1, 2 and 3 show that the assertions made above empiri-\ncally hold. We see a monotonic increase in contrastive entropy\nwith distortion and training distortion margin in Figures 1 and\n3 respectively. Figure 2 shows the contrastive entropy increase\nfor training data with epochs. All sentence level RNN model\nreferred above and elsewhere in this paper were trained using\ngradient descent with learning rate of 0.1 and ℓ2 regularization\ncoefﬁcient of 10−3.\nFinally, we would like to compare the standard word level\nbaseline models and our sentence level language model on this\nnew metric. The objective here is to verify the hypothesis that\nbetween two language models, the superior one should be able\nto better distinguish the test sentence from their distorted ver-\nsions. This is akin to saying that a better language model should\nhave higher contrastive entropy value with similar or higher\ncross entropy ratio. Tables 2 and 3 shows the results for our\nexperiments. The results were generated using the open source\nlanguage modeling SRILM toolkit [12] for n-gram models and\nthe RNNLM toolkit [13] for the RNN language model. The\nRNN model used had 200 hidden layers, with class size of 50.\nThe sRNN-75(10) row in Tables 2 and 3 indicates that the sen-\ntence level RNN model was trained with latent space size of 75\nand with training distortion level of 10%. All the results here\nwere averaged over 10 runs.\nAs hypothesized, the contrastive entropy rises in Table 2’s\ncolumns 2 to 4 and correlates negatively with perplexity for\nword level models—i.e. the models expected to do better on\nperplexity do better on Contrastive entropy as well. Rows 4\nto 6 compare sentence level RNN models. Here too, as ex-\npected, sRNN trained with distortion level of 10% outperforms\nsRNN trained with distortion margin of 50%. Now, let’s com-\npare word level models to our sentence level model. We can\nsee that sRNN-75(50) performs worse compared to RNN for\nall levels and worse than 3-gram and 5-gram models for 10%\nand 30%. This can be attributed to the training distortion mar-\ngin of 50% which encourages the sRNN to see anything with\nless than 50% distortion as in-domain sentences. On the other\nhand sRNN trained with distortion level of 10% performs the\nbest as compared to all other models as it has been tuned to la-\nbel slightly un-grammatical sentences or ones that have slightly\nun-natural structure as out of domain.\nTable 3 shows that scaling is not an issue for word level\nmodels as ratios are more or less the same. Sentence level mod-\nels at 10% distortion do better than all the word-level models\non both metrics which demonstrates their superior performance.\nModel 30%/10%\nHCR\n50%/10%\nHCR\n3-gram KN 2.096 2.649\n5-gram KN 2.077 2.626\nRNN 2.097 2.596\nsRNN-75(50) 2.002 3.275\nsRNN-75(10) 2.890 5.257\nsRNN-150(10) 2.976 5.074\nTable 3: Contrastive entropy ratio (HCR) for n-gram, RNN and\nsRNN models at 30% and 50% distortion levels with baseline\ndistortion level of 10%.\nsRNN-75(50) is an interesting case. At test distortion level of\n30% it is clearly inferior to all word level models as it was\ntrained on a distortion margin of 50%. With 50% test distor-\ntion the result is unclear as it does worse on contrastive entropy\nbut better on contrastive ratio.\n6. Conclusion\nIn this paper we proposed a new evaluation criteria which can\nbe used to evaluate unnormalized language models and showed,\nusing examples, its efﬁcacy in comparing sentence level models\namong themselves and to word level models. As both WER and\ncontrastive entropy are discriminative measures, we hypothe-\nsize that contrastive entropy should have a better correlation\nwith WER as compared to perplexity.\nWe also proposed a discriminatively trained sentence level\nformulation of recurrent neural networks which outperformed\nthe current state of the art RNN models on our new metric. We\nhypothesize that this formulation of RNN does a better job at\ndiscriminative tasks like lattice re-scoring as compared to stan-\ndard RNN and other traditional language modeling techniques.\nWe conclude by restating that a metric is meaningful only if it\ncan measure improvements in real world applications. Further\nexperiments evaluating contrastive entropy’s correlation with\nthe WER and BLEU metrics over a wide range of datasets are\nrequired to unquestionably demonstrate the usefulness of this\nmetric. Similarly, to establish superior discriminative ability of\nsentence level RNNs over standard RNNs, we must compare\ntheir performance on real word discriminative tasks like n-best\nlist re-scoring.\n7. References\n[1] R. Iyer, M. Ostendorf, and M. Meteer, “Analyzing and\npredicting language model improvements,” in Automatic\nSpeech Recognition and Understanding, 1997. Proceed-\nings., 1997 IEEE Workshop on . IEEE, 1997, pp. 254–\n261.\n[2] S. F. Chen, D. Beeferman, and R. Rosenfeld, “Evaluation\nmetrics for language models,” 1998.\n[3] P. Clarkson, T. Robinson et al., “Towards improved lan-\nguage model evaluation measures.” in EUROSPEECH,\n1999.\n[4] N. A. Smith and J. Eisner, “Contrastive estimation: Train-\ning log-linear models on unlabeled data,” in Proceedings\nof the 43rd Annual Meeting on Association for Computa-\ntional Linguistics. Association for Computational Lin-\nguistics, 2005, pp. 354–362.\n[5] A. Sethy, S. Chen, E. Arisoy, and B. Ramabhad-\nran, “Unnormalized exponential and neural network lan-\nguage models,” in Acoustics, Speech and Signal Process-\ning (ICASSP), 2015 IEEE International Conference on .\nIEEE, 2015, pp. 5416–5420.\n[6] R. Rosenfeld, S. F. Chen, and X. Zhu, “Whole-sentence\nexponential language models: a vehicle for linguistic-\nstatistical integration,” Computer Speech & Language ,\nvol. 15, no. 1, pp. 55–73, 2001.\n[7] D. Okanohara and J. Tsujii, “A discriminative language\nmodel with pseudo-negative samples.” in ANNUAL\nMEETING-ASSOCIATION FOR COMPUTATIONAL\nLINGUISTICS, vol. 45, no. 1, 2007, p. 73.\n[8] R. Collobert and J. Weston, “A uniﬁed architecture for\nnatural language processing: Deep neural networks with\nmultitask learning,” in Proceedings of the 25th interna-\ntional conference on Machine learning. ACM, 2008, pp.\n160–167.\n[9] P. H. Algoet and T. M. Cover, “A sandwich proof of\nthe Shannon-McMillan-Breiman theorem,” The annals of\nprobability, pp. 899–909, 1988.\n[10] T. Mikolov, M. Karaﬁ ´at, L. Burget, J. Cernock `y, and\nS. Khudanpur, “Recurrent neural network based language\nmodel.” in INTERSPEECH, 2010, pp. 1045–1048.\n[11] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pas-\ncanu, G. Desjardins, J. Turian, D. Warde-Farley, and\nY . Bengio, “Theano: a CPU and GPU math expression\ncompiler,” in Proceedings of the Python for Scientiﬁc\nComputing Conference (SciPy) , Jun. 2010, oral Presen-\ntation.\n[12] A. Stolcke, “Srilm-an extensible language modeling\ntoolkit.” in INTERSPEECH, 2002.\n[13] T. Mikolov, S. Kombrink, A. Deoras, L. Burget, and\nJ. Cernocky, “RNNLM-recurrent neural network language\nmodeling toolkit,” in Proc. of the 2011 ASRU Workshop ,\n2011, pp. 196–201.",
  "topic": "Metric (unit)",
  "concepts": [
    {
      "name": "Metric (unit)",
      "score": 0.5704416036605835
    },
    {
      "name": "Computer science",
      "score": 0.5135536193847656
    },
    {
      "name": "Entropy (arrow of time)",
      "score": 0.4613509178161621
    },
    {
      "name": "Natural language processing",
      "score": 0.37946081161499023
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3516438901424408
    },
    {
      "name": "Economics",
      "score": 0.09236633777618408
    },
    {
      "name": "Physics",
      "score": 0.08909532427787781
    },
    {
      "name": "Thermodynamics",
      "score": 0.07472068071365356
    },
    {
      "name": "Operations management",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 6
}