{
  "title": "Graph Transformer for Graph-to-Sequence Learning",
  "url": "https://openalex.org/W2998702685",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2141500565",
      "name": "Deng Cai",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2236715527",
      "name": "Wai Lam",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2252123671",
    "https://openalex.org/W2612881151",
    "https://openalex.org/W2798749466",
    "https://openalex.org/W6764869561",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2951291634",
    "https://openalex.org/W2133459682",
    "https://openalex.org/W2468355276",
    "https://openalex.org/W2304113845",
    "https://openalex.org/W2951309718",
    "https://openalex.org/W1829822087",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W2954922414",
    "https://openalex.org/W2610308073",
    "https://openalex.org/W6690815549",
    "https://openalex.org/W2949889664",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W4241499611",
    "https://openalex.org/W2565245743",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W1816313093",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W6727310639",
    "https://openalex.org/W2587541991",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W2964035651",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2964165364",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2963653811",
    "https://openalex.org/W4288375838",
    "https://openalex.org/W2950898568",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2524520086",
    "https://openalex.org/W2758950307",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2600565200",
    "https://openalex.org/W2796167946",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W655477013",
    "https://openalex.org/W2964116568",
    "https://openalex.org/W4288407114",
    "https://openalex.org/W2952706341"
  ],
  "abstract": "The dominant graph-to-sequence transduction models employ graph neural networks for graph representation learning, where the structural information is reflected by the receptive field of neurons. Unlike graph neural networks that restrict the information exchange between immediate neighborhood, we propose a new model, known as Graph Transformer, that uses explicit relation encoding and allows direct communication between two distant nodes. It provides a more efficient way for global graph structure modeling. Experiments on the applications of text generation from Abstract Meaning Representation (AMR) and syntax-based neural machine translation show the superiority of our proposed model. Specifically, our model achieves 27.4 BLEU on LDC2015E86 and 29.7 BLEU on LDC2017T10 for AMR-to-text generation, outperforming the state-of-the-art results by up to 2.2 points. On the syntax-based translation tasks, our model establishes new single-model state-of-the-art BLEU scores, 21.3 for English-to-German and 14.1 for English-to-Czech, improving over the existing best results, including ensembles, by over 1 BLEU.",
  "full_text": "The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-20)\nGraph Transformer for Graph-to-Sequence Learning∗\nDeng Cai, Wai Lam\nThe Chinese University of Hong Kong\nthisisjcykcd@gmail.com, wlam@se.cuhk.edu.hk\nAbstract\nThe dominant graph-to-sequence transduction models em-\nploy graph neural networks for graph representation learning,\nwhere the structural information is reﬂected by the receptive\nﬁeld of neurons. Unlike graph neural networks that restrict\nthe information exchange between immediate neighborhood,\nwe propose a new model, known as Graph Transformer, that\nuses explicit relation encoding and allows direct communica-\ntion between two distant nodes. It provides a more efﬁcient\nway for global graph structure modeling. Experiments on the\napplications of text generation from Abstract Meaning Rep-\nresentation (AMR) and syntax-based neural machine transla-\ntion show the superiority of our proposed model. Speciﬁcally,\nour model achieves 27.4 BLEU on LDC2015E86 and 29.7\nBLEU on LDC2017T10 for AMR-to-text generation, outper-\nforming the state-of-the-art results by up to 2.2 points. On\nthe syntax-based translation tasks, our model establishes new\nsingle-model state-of-the-art BLEU scores, 21.3 for English-\nto-German and 14.1 for English-to-Czech, improving over\nthe existing best results, including ensembles, by over 1\nBLEU.\nIntroduction\nGraphical structure plays an important role in natural lan-\nguage processing (NLP), they often serve as the central for-\nmalism for representing syntax, semantics, and knowledge.\nFor example, most syntactic representations (e.g., depen-\ndency relation) are tree-based while most whole-sentence\nsemantic representation frameworks (e.g., Abstract Mean-\ning Representation (AMR) (Banarescu et al. 2013)) encode\nsentence meaning as directed acyclic graphs. A range of\nNLP applications can be framed as the process of graph-\nto-sequence learning. For instance, text generation may in-\nvolve realizing a semantic graph into a surface form (Liu\net al. 2015) and syntactic machine translation incorporates\nsource-side syntax information for improving translation\nquality (Bastings et al. 2017). Fig. 1 gives an example of\nAMR-to-text generation.\n∗The work described in this paper is substantially supported by\na grant from the Research Grant Council of the Hong Kong Special\nAdministrative Region, China (Project Code: 14204418).\nCopyright c⃝ 2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nboy\ngirl\nThe boy wants the girl to believe him.\nboy\ngirl\nwant-01\nbelieve-01\nwant-01\nbelieve-01\nARG1\nARG0\nARG0\nARG1\nARG1\nARG0\nARG1\nARG0\nFigure 1: An AMR graph (left) for the reference sentence\n“The boy wants the girl to believe him.” and the correspond-\ning Levi graph (right).\nWhile early work uses statistical methods or neural mod-\nels after the linearization of graphs, graph neural networks\n(GNNs) have been ﬁrmly established as the state-of-the-\nart approaches for this task (Damonte and Cohen 2019;\nGuo et al. 2019). GNNs typically compute the represen-\ntation of each node iteratively based on those of its adja-\ncent nodes. This inherently local propagation nature pre-\ncludes efﬁcient global communication, which becomes crit-\nical at larger graph sizes, as the distance between two nodes\nexceeds the number of stacked layers. For instance, for\ntwo nodes staying L hops away, at least L layers will be\nneeded in order to capture their dependencies. Furthermore,\neven if two distant nodes are reachable, the information\nmay also be disrupted in the long journey (Xu et al. 2018;\nGuo et al. 2019).\nTo address the above problems, we propose a new model,\nknown as Graph Transformer, which relies entirely on the\nmulti-head attention mechanism (V aswani et al. 2017) to\ndraw global dependencies.\n1 Different to GNNs, the Graph\nTransformer allows modeling of dependencies between any\ntwo nodes without regard to their distance in the input graph.\nOne undesirable consequence is that it essentially treats any\n1We note that the nameGraph Transformerwas used in a recent\nwork (Koncel-Kedziorski et al. 2019). However, it merely focuses\non the relations between directly connected nodes as other graph\nneural networks.\n7464\ngraph as a fully connected graph, greatly diluting the ex-\nplicit graph structure. To maintain a graph structure-aware\nview, our proposed model introduces explicit relation en-\ncoding and incorporates it into the pairwise attention score\ncomputation as a dynamic parameter.\nOur treatment of explicit relation encoding also brings\nother side advantages compared to GNN-based methods.\nPrevious state-of-the-art GNN-based methods use Levi\ngraph transformation (Beck, Haffari, and Cohn 2018; Guo et\nal. 2019), where two unlabeled edges are replacing one la-\nbeled edge that is present in the original graph. For example,\nin Fig. 1, the labeled edgewant-01\nARG1\n−→ believe-01\nturns to be two unlabeled edgeswant-01 −→ARG1 and\nARG1 −→ believe-01. Since edge labels are repre-\nsented as nodes, they end up sharing the same semantic\nspace, which is not ideal as nodes and edges are typically\ndifferent elements. In addition, the Levi graph transforma-\ntion at least doubles the number of representation vectors.\nwhich will introduce more complexity for the decoder-side\nattention mechanism (Bahdanau, Cho, and Bengio 2015)\nand copy mechanism (Gu et al. 2016; See, Liu, and Manning\n2017). Through explicit and separate relation encoding, our\nproposed Graph Transformer inherently avoids these prob-\nlems.\nExperiments show that our model is able to achieve better\nperformance for graph-to-sequence learning tasks for nat-\nural language processing. For the AMR-to-text generation\ntask, our model surpasses the current state-of-the-art neu-\nral methods trained on LDC2015E86 and LDC2017T10 by\n1.6 and 2.2 BLEU points, respectively. For the syntax-based\nneural machine translation task, our model is also consis-\ntently better than others, even including ensemble systems,\nshowing the effectiveness of the model on a large training\nset. In addition, we give an in-depth study of the source of\nimprovement gain and the internal workings of the proposed\nmodel.\nRelated Work\nEarly research efforts for graph-to-sequence learning use\nspecialized grammar-based methods. Flanigan et al.(2016)\nsplit input graphs to trees and uses a tree-to-string trans-\nducer. Song et al.(2016) recast generation as a traveling\nsalesman problem. Jones et al.(2012) leverage hyperedge re-\nplacement grammar and Song et al.(2017) use a synchronous\nnode replacement grammar. More recent work employs\nmore general approaches, such as phrase-based machine\ntranslation model (Pourdamghani, Knight, and Hermjakob\n2016) and neural sequence-to-sequence methods (Konstas et\nal. 2017) after linearizing input graphs. Regarding AMR-to-\ntext generation, Cao and Clark(2019) propose an interesting\nidea that factorizes text generation through syntax. One limi-\ntation of sequence-to-sequence models, however, is that they\nrequire serialization of input graphs, which inevitably incurs\nthe obstacle of capturing graph structure information.\nAn emerging trend has been directly encoding the graph\nwith different variants of graph neural networks, which in\ncommon stack multiple layers that restrict the update of\nnode representation based on a ﬁrst-order neighborhood but\nuse different information passing schemes. Some borrow the\nideas from recurrent neural networks (RNNs), e.g, Beck,\nHaffari, and Cohn(2018) use gated graph neural network (Li\net al. 2016) while Song et al.(2018) introduce LSTM-style\ninformation aggregation. Others apply convolutional neu-\nral networks (CNNs), e.g., Bastings et al.(2017);Damonte\nand Cohen(2019);Guo et al.(2019) utilize graph convolu-\ntional neural networks (Kipf and Welling 2017). Koncel-\nKedziorski et al.(2019) update vertex information by atten-\ntion over adjacent neighbors. Furthermore, Guo et al.(2019)\nallow the information exchange across different levels of\nlayers. Damonte and Cohen(2019) systematically compare\ndifferent encoders and show the advantages of graph en-\ncoder over tree and sequential ones. The contrast between\nour model and theirs is reminiscent of the contrast between\nthe self-attention network (SAN) and CNN/RNN.\nFor sequence-to-sequence learning, the SAN-based\nTransformer model (V aswani et al. 2017) has been thede\nfacto approach for its empirical successes. However, it is un-\nclear on the adaptation to graphical data and its performance.\nOur work is partially inspired by the introduction of relative\nposition embedding (Shaw, Uszkoreit, and V aswani 2018;\nDai et al. 2019) in sequential data. However, the extension\nto graph is nontrivial since we need to model much more\ncomplicated relation instead of mere visual distance. To the\nbest of our knowledge, the Graph Transformer is the ﬁrst\ngraph-to-sequence transduction model relying entirely on\nself-attention to compute representations.\nBackground of Self-Attention Network\nThe Transformer introduced by V aswani et al.(2017) is a\nsequence-to-sequence neural architecture originally used for\nneural machine translation. It employs self-attention net-\nwork (SAN) for implementing both the encoder and the de-\ncoder. The encoder consists of multiple identical blocks, of\nwhich the core is multi-head attention. The multi-head atten-\ntion consists ofH attention heads, and each of them learns\na distinct attention function. Given a source vectorx ∈ R\ndx\nand a set of context vectors{y1,y2,...,y m} with the same\ndimension dx or in shorty1:m, for each attention head,xand\ny1:m are transformed into distinct query and value represen-\ntations. The attention score is computed as the dot-product\nbetween them.\nf(x,y\ni)=( Wqx)T Wkyi\nwhere Wq,Wk ∈ Rdz×dx are trainable projection matrices.\nThe attention scores are scaled and normalized by a softmax\nfunction to compute the ﬁnal attention outputattn.\na\ni = exp(f(x,yi)/√dz)∑ m\nj=1 exp(f(x,yi))/√dz)\nattn =\nm∑\ni=1\naiWvyi\nwhere a ∈ Rm is the attention vector (a distribution over all\ninput y1:m), Wv ∈ Rdz×dx is a trainable projection matrix.\nFinally, the outputs of all attention heads are concatenated\nand projected to the original dimension of x, followed by\n7465\nshortest paths\nRelation Encoder\nwant-01\nbelieve-01\nboy\ngirl\n…\nARG0\nARG1\nARG0\nARG1\nGraph Encoder Sequence Decoder\nSelf-Attention\nAttention\n+\n+Position \nEmbedding\nNode\nEmbedding Token\nEmbedding \nPosition \nEmbedding\nDifferent colors represent different \nshortest paths among node pairs.\nRelation-Enhanced\nGlobal Attention\nNode Initialization\nOutput\n...\n(fully-connected view)\nFigure 2: An overview of our propose model.\nfeed-forward layers, residual connection, and layer normal-\nization.2 For brevity, we will denote the whole procedure\ndescribed above as a single function ATT(x,y1:m).\nFor an input sequence x1:n, the SAN-based encoder\ncomputes the vector representations iteratively by xL\ni =\nAT T(xL\ni ,xL−1\n1:n ), where L is the total number of blocks\nand x0\n1:n are word embeddings. In this way, a representa-\ntion is allowed to build a direct relationship with another\nlong-distance representation. To feed the sequential order in-\nformation, the deterministic or learned position embedding\n(V aswani et al. 2017) is introduced to expose the position\ninformation to the model, i.e., x\n0\ni becomes the sum of the\ncorresponding word embedding and the position embedding\nfor i.\nThe aforementioned treatment of SAN on sequential data\ncan be drawn a close resemblance to graph neural networks\nby regarding the token sequence as an unlabeled fully-\nconnected graph (each token as a node) and taking the multi-\nhead attention mechanism as a speciﬁc message-passing\nscheme. Such view on the relationship between SAN and\ngraph neural networks inspires our work.\nGraph Transformer\nOverview\nFor a graph with n nodes, previous graph neural networks\ncompute the node representationvi as a function of the input\nnode iand all its ﬁrst-order neighborhoodsN(i). The graph\nstructure is implicitly reﬂected by the receptive ﬁeld of each\nnode representation. This local communication design, how-\never, could be inefﬁcient for long-distance information ex-\nchange. We introduce a new model, known as Graph Trans-\nformer, which provides an aggressively different paradigm\nthat enables relation-aware global communication.\n2We refer interesting readers to V aswani et al.(2017) for more\ndetails.\nThe overall framework is shown in Fig. 2. The most im-\nportant characteristic of the Graph Transformer is that it has\na fully-connected view on arbitrary input graphs. A node is\nable to directly receive and send information to another node\nno matter whether they directly connected or not. These op-\nerations are achieved by our proposed extension to the origi-\nnal multi-head attention mechanism, the relation-enhanced\nglobal attention mechanism described below. Speciﬁcally,\nthe relationship between any node pair is depicted as the\nshortest relation path between them. These pairwise rela-\ntion paths are fed into a relation encoder for distributed re-\nlation encoding. We initialize node vectors as the sum of the\nnode embedding and absolute position embeddings. Multi-\nple blocks of global attention network are stacked to com-\npute the ﬁnal node representations. At each block, a node\nvector is updated based on all other node vectors and the\ncorresponding relation encodings. The resulted node vectors\nat the last block are fed to the sequence decoder for sequence\ngeneration.\nGraph Encoder\nOur graph encoder is responsible for transforming an input\ngraph into a set of corresponding node embeddings. To ap-\nply global attention on a graph, the central problem is how\nto maintain the topological structure of the graph while al-\nlowing fully-connected communication. To this end, we pro-\npose relation-enhanced global attention mechanism, which\nis an extension of the vanilla multi-head attention. Our idea\nis to incorporate explicit relation representation between two\nnodes into their representation learning. Recall that, in the\nstandard multi-head attention, the attention score between\nthe element x\ni and the elementxj is simply the dot-product\nof their query vector and key vector respectively:\nsij = f(xi,xj)\n= xiWT\nq Wkxj\n(1)\n7466\nSuppose we have learned a vector representation for the\nrelationship rij, which we will refer as relation encoding,\nbetween the node i and the node j. Following the idea of\nrelative position embedding (Shaw, Uszkoreit, and V aswani\n2018; Dai et al. 2019), we propose to compute the attention\nscore as follows:\n[r\ni→j; rj→i]= Wrrij (2)\nwhere we split the relation encoding rij into the forward\nrelation encoding ri→j and the backward relation encoding\nrj→i. Then we compute the attention score based on both\nthe node representations and their relation representation as\nshown below:\ns\nij = g(xi,xj,rij)\n=( xi + ri→j)WT\nq Wk(xj + rj→i)\n= xiWT\nq Wkxj\n  \n(a)\n+ xiWT\nq Wkrj→i\n  \n(b)\n+ ri→jWT\nq Wkxj\n  \n(c)\n+ ri→jWT\nq Wkrj→i\n  \n(d)\n(3)\nEach term in Eq (3) corresponds to some intuitive mean-\ning according to their formalization. The term (a) captures\npurely content-based addressing, which is the original term\nin vanilla attention mechanism. The term (b) represents\na source-dependent relation bias. The term (c) governs a\ntarget-dependent relation bias. The term (d) encodes the\nuniversal relation bias. Our formalization provides a prin-\ncipled way to model the element-relation interactions. In\ncomparison, it has broader coverage than Shaw, Uszkoreit,\nand V aswani(2018) in terms of additional terms (c) and (d),\nand than Dai et al.(2019) in terms of the extra term (c) re-\nspectively. More importantly, previous methods only model\nthe relative position in the context of sequential data, which\nmerely adopts the immediate embeddings of the relative po-\nsitions (e.g, −1,+1). To depict the relation between two\nnodes in a graph, we utilize a shortest-path based approach\nas described below.\nRelation Encoder Conceptually, the relation encoding\ngives the model a global guidance about how information\nshould be gathered and distributed, i.e., where to attend. For\nmost graphical structures in NLP , the edge label conveys di-\nrect relationship between adjacent nodes (e.g., the semantic\nrole played by concept-to-concept, and the dependency re-\nlation between two words). We extend this one-hop relation\ndeﬁnition into multi-hop relation reasoning for characteriz-\ning the relationship between two arbitrary nodes. For exam-\nple, in Fig 1, the shortest path from the conceptwant-01 to\ngirl is “ want-01\nARG1\n−→ believe-01\nARG0\n−→ girl”,\nwhich conveys that girl is the object of the wanted ac-\ntion. Intuitively, the shortest path between two nodes gives\nthe closest and arguably the most important relationship be-\ntween them. Therefore, we propose to use the shortest paths\n(relation sequence) between two nodes to characterize their\nrelationship.\n3 Following the sequential nature of the rela-\n3For the case that there are multiple shortest paths, we randomly\nsample one during training and take the averaged representation\ntion sequence, we employs recurrent neural networks with\nGated Recurrent Unit (GRU) (Cho et al. 2014) for trans-\nforming relation sequence into a distributed representation.\nFormally, we represent the shortest relation pathspi→j =\n[e(i,k1),e(k1,k2),...,e (kn,j)] between the nodeiand the\nnode j, wheree(·,·) indicates the edge label andk1:n are the\nrelay nodes. We employ bi-directional GRUs for sequence\nencoding:\n−→s\nt = GRUf (−−→st−1,spt)\n←−st = GRUb(←−−st+1,spt)\nThe last hidden states of the forward GRU network and the\nbackward GRU networks are concatenated to form the ﬁnal\nrelation encodingr\nij =[ −→sn; ←−s0].\nBidirectionality Though in theory, our architecture can\ndeal with arbitrary input graphs, the most widely adopted\ngraphs in the real problems are directed acyclic graphs\n(DAGs). This implies that the node embedding information\nwill be propagated in one pre-speciﬁed direction. However,\nthe reverse direction informs the equivalent information ﬂow\nas well. To facilitate communication in both directions, we\nadd reverse edges to the graph. The reverse edge connects\nthe same two nodes as the original edge but in a different di-\nrection and with a reversed label. For example, we will draw\na virtual edge believe-01\nRARG1\n−→ want-01 accord-\ning to the original edge want-01\nARG1\n−→ believe-01.\nFor convenience, we also introduce self-loop edges for each\nnode. These extra edges have speciﬁc labels, hence their own\nparameters in the network. We also introduce an extra global\nnode into every graph, who has a direct edge to all other\nnodes with the special labelglobal. The ﬁnal representation\nx\nglobal of the global node serves as a whole graph represen-\ntation.\nAbsolute Position Besides pairwise relationship, some\nabsolute positional information can also be beneﬁcial. For\nexample, the root of an AMR graph serves as a rudimen-\ntary representation of the overall focus, making the mini-\nmum distance from the root node partially reﬂect the impor-\ntance of the corresponding concept in the whole-sentence se-\nmantics. The sequence order of tokens in a dependency tree\nalso provides complementary information to dependency re-\nlations. In order for the model to make use of the absolute\npositions of nodes, we add the positional embeddings to the\ninput embeddings at the bottom of the encoder stacks. For\nexample, want-01 in Fig 1 is the root node of the AMR\ngraph, so its index should be 0. Notice we denote the index\nof the global node as0 as well.\nSequence Decoder\nOur sequence decoder basically follows the same spirit of\nthe sequential Transformer decoder. The decoder yields the\nnatural language sequence by calculating a sequence of hid-\nden states sequentially. One distinct characteristic is that we\nuse the global graph representation x\nglobal for initializing\nthe hidden states at each time step. The hidden stateht at\nduring testing.\n7467\nDataset #train #dev #test #edge types #node types avg #nodes avg #edges avg diameter\nLDC2015E86 16,833 1,368 1,371 113 18735 17.34 17.53 6.98\nLDC2017T10 36,521 1,368 1,371 116 24693 14.51 14.62 6.15\nEnglish-Czech 181,112 2,656 2,999 46 78017 23.18 22.18 8.36\nEnglish-German 226,822 2,169 2,999 46 87219 23.29 22.29 8.42\nTable 1: Data statistics of all four datasets. #train/dev/test indicates the number of instances in each set, avg\n#nodes/edges/diameter represents the averaged value of nodes/edge/diameter size of a graph.\nmodel component hyper-parameter value\nchar-level CNN\nnumber of ﬁlters 256\nwidth of ﬁlters 3\nchar embedding size 32\nﬁnal hidden size 128\nEmbeddings node embedding size 300\nedge embedding size 200\ntoken embedding size 300\nMulti-head attention\nnumber of heads 8\nhidden state size 512\nfeed-forward hidden size 1024\nTable 2: Hyper-parameters settings.\neach time step t is then updated by interleaving multiple\nrounds of attention over the output of the encoder (node\nembeddings) and attention over previously-generated tokens\n(token embeddings). Both are implemented by the multi-\nhead attention mechanism. x\nglobal is removed when per-\nforming the sequence-to-graph attention.\nCopy mechanism To address the data sparsity issue in\ntoken prediction, we include a copy mechanism (Gu et al.\n2016) in similar spirit to previous works. Concretely, a\nsingle-head attention is computed based on the decoder state\nh\nt and the node representation x1:n, where ai\nt denotes the\nattention weight of the node vi in the current time step t.\nOur model can either directly copy the type name of a node\nor generate from a pre-deﬁned vocabularyV. Formally, the\nprediction probability of a tokeny is given by:\nP(y|h\nt)= P(gen|ht)gen(y|ht)+ P(copy|ht)\n∑\ni∈S(y)\nai\nt\nwhere S(y) is the set of nodes that have the same surface\nform as y. P(gen|ht) and P(copy|ht) are computed by a\nsingle layer neural network with softmax activation, and\ngen(y|ht)=e x p (wyT ht)/∑\ny′∈V exp(w′\ny\nT ht), where wy\n(for y ∈ V) denotes the model parameters. The copy mecha-\nnism facilitates the generation of dates, numbers, and named\nentities in both AMR-to-text generation and machine trans-\nlation tasks in experiments.\nExperiments\nWe assess the effectiveness of our models on two typical\ngraph-to-sequence learning tasks, namely AMR-to-text gen-\neration and syntax-based machine translation (MT). Fol-\nlowing previous work, the results are mainly evaluated by\nBLEU (Papineni et al. 2002) and\nCHR F++ (Popovi´c 2017).\nSpeciﬁcally, we use case-insensitive scores for AMR and\ncase sensitive BLEU scores for MT.\nAMR-to-text Generation\nOur ﬁrst application is language generation from AMR, a se-\nmantic formalism that represents sentences as rooted DAGs\n(Banarescu et al. 2013). For this AMR-to-text generation\ntask, we use two benchmarks, namely the LDC2015E86\ndataset and the LDC2017T10 dataset. The ﬁrst block of\nTable 1 shows the statistics of the two datasets. Similar\nto Konstas et al.(2017), we apply entity simpliﬁcation and\nanonymization in the preprocessing steps and restore them\nin the postprocessing steps.\nThe graph encoder uses randomly initialized node em-\nbeddings as well as the output from a learnable CNN with\ncharacter embeddings as input. The sequence decoder uses\nrandomly initialized token embeddings and another char-\nlevel CNN. Model hyperparameters are chosen by a small\nset of experiments on the development set of LDC2017T10.\nThe detailed settings are listed in Table 2. During testing,\nwe use a beam size of8 for generating graphs. To mitigate\noverﬁtting, we also apply dropout (Srivastava et al. 2014)\nwith the drop rate of 0.2 between different layers. We use\na special UNK token to replace the input node tag with a\nrate of 0.33. Parameter optimization is performed with the\nAdam optimizer (Kingma and Ba 2014) withβ\n1 =0 .9 and\nbeta2 =0 .999. The same learning rate schedule of V aswani\net al.(2017) is adopted in our experiments.4 For computation\nefﬁciency, we gather all distinct shortest paths in a train-\ning/testing batch, and encode them into vector representa-\ntions by the recurrent relation encoding procedure as de-\nscribed above.\n5\nWe run comparisons on systems without ensembling nor\nadditional silver data. Speciﬁcally, the comparison methods\ncan be grouped into three categories: (1) feature-based sta-\ntistical methods (Song et al. 2016; Pourdamghani, Knight,\nand Hermjakob 2016; Song et al. 2017; Flanigan et al.\n2016); (2) sequence-to-sequence neural models (Konstas et\nal. 2017; Cao and Clark 2019), which use linearized graphs\nas inputs; (3) recent works using different variants of graph\nneural networks for encoding graph structures directly (Song\net al. 2018; Beck, Haffari, and Cohn 2018; Damonte and Co-\nhen 2019; Guo et al. 2019). The results are shown in Table\n3. For both datasets, our approach substantially outperforms\n4Code available at https://github.com/jcyk/gtos.\n5This strategy reduces the number of related sequences to en-\ncode from O(mn2) to a stable number when a large batch sizem\nis used.\n7468\nModel LDC2015E86 LDC2017T10\nBLEU CHR F++ METEOR BLEU CHR F++ METEOR\nSong et al.(2016)† 22.4 - - - - -\nFlanigan et al.(2016)† 23.0 - - - - -\nPourdamghani, Knight, and Hermjakob(2016)† 26.9 - - - - -\nSong et al.(2017)† 25.6 - - - - -\nKonstas et al.(2017) 22.0 - - - - -\nCao and Clark(2019)‡ 23.5 - - 26.8 - -\nSong et al.(2018) 23.3 - - 24.9 - -\nBeck, Haffari, and Cohn(2018) - - - 23.3 50.4\nDamonte and Cohen(2019) 24.4 - 23.6 24.5 - 24.1\nGuo et al.(2019) 25.7 54.5∗ 31.5∗ 27.6 57.3 34.0∗\nOurs 27.4 56.4 32.9 29.8 59.4 35.1\nTable 3: Main results on AMR-to-text generation. Numbers with∗ are from the contact from the authors. - denotes that the\nresult is unknown because it is not provided in the corresponding paper.\nModel Type English-German English-Czech\nBLEU CHR F++ BLEU CHR F++\nBastings et al.(2017) Single 16.1 - 9.6 -\nBeck, Haffari, and Cohn(2018) Single 16.7 42.4 9.8 33.3\nGuo et al.(2019) Single 19.0 44.1 12.1 37.1\nBeck, Haffari, and Cohn(2018) Ensemble 19.6 45.1 11.7 35.9\nGuo et al.(2019) Ensemble 20.5 45.8 13.1 37.8\nOurs Single 21.3 47.9 14.1 41.1\nTable 4: Main results on syntax-based machine translation.\nall previous methods. On the LDC2015E86 dataset, our\nmethod achieves a BLEU score of 27.4, outperforming pre-\nvious best-performing neural model (Guo et al. 2019) by a\nlarge margin of 2.6 BLEU points. Also, our model becomes\nthe ﬁrst neural model that surpasses the strong non-neural\nbaseline established by Pourdamghani, Knight, and Herm-\njakob(2016). It is worth noting that those traditional methods\nmarked with † train their language models on the external\nGigaword corpus, thus they possess an additional advantage\nof extra data. On the LDC2017T10 dataset, our model es-\ntablishes a new record BLEU score of 29.8, improving over\nthe state-of-the-art sequence-to-sequence model (Cao and\nClark 2019) by 3 points and the state-of-the-art GNN-based\nmodel (Guo et al. 2019) by 2.2 points. The results are even\nmore remarkable since the model of Cao and Clark(2019)\n(marked with ‡) uses constituency syntax from an external\nparser. Similar phenomena can be found on the additional\nmetrics of\nCHR F++ and M ETEOR (Denkowski and Lavie\n2014). Those results suggest that current graph neural net-\nworks cannot make full use of the AMR graph structure, and\nour Graph Transformer provides a promising alternative.\nSyntax-based Machine Translation\nOur second evaluation is syntax-based machine translation,\nwhere the input is a source language dependency syntax tree\nand the output is a plain target language string. We employ\nthe same data and settings from Bastings et al.(2017). Both\nthe English-German and the English-Czech datasets from\nthe WMT16 translation task.\n6 The English sentences are\nparsed after tokenization to generate the dependency trees\non the source side using SyntaxNet (Alberti et al. 2017).\n7\nOn the Czech and German sides, texts are tokenized using\nthe Moses tokenizer.8 Byte-pair encodings (Sennrich, Had-\ndow, and Birch 2016) with 8,000 merge operations are used\nto obtain subwords. The second block of Table 1 shows the\nstatistics for both datasets. For model conﬁguration, we just\nre-use the settings obtained in our AMR-to-text experiments.\nTable 4 presents the results with comparison to existing\nmethods. On the English-to-German translation task, our\nmodel achieves a BLEU score of 41.0, outperforming all of\nthe previously published single models by a large margin of\n2.3 BLEU score. On the English-to-Czech translation task,\nour model also outperforms the best previously reported sin-\ngle models by an impressive margin of 2 BLEU points. In\nfact, our single model already outperforms previous state-\nof-the-art models that use ensembling. The advantages of\nour method are also veriﬁed by the metric\nCHR F++.\nAn important point about these experiments is that we\ndid not tune the architecture: we simply employed the same\nmodel in all experiments, only adjusting the batch size for\ndifferent dataset size. We speculate that even better results\nwould be obtained by tuning the architecture to individ-\nual tasks. Nevertheless, we still obtained improved perfor-\nmance over previous works, underlining the generality of\n6http://www.statmt.org/wmt16/translation-task.html.\n7https://github.com/tensorﬂow/models/tree/master/syntaxnet\n8https://github.com/moses-smt/mosesdecoder.\n7469\nchrF++\n54.5\n56.5\n58.5\n60.5\n62.5\nGraph Size\n1-20 21-30 31-40 >40\nOurs\nGuo’19\n(a)\nchrF++\n54.0\n55.8\n57.6\n59.4\n61.2\nGraph Diameter\n1-7 8-14 >14\nOurs\nGuo’19\n(b)\nchrF++\n54.0\n56.5\n59.0\n61.5\n64.0\nGraph Reentrancies\n0-1 2-3 4-5 >5\nOurs\nGuo’19\n(c)\nFigure 3: CHR F++ scores with respect to (a) the graph size, (b) the graph diameter, and (c) the the number of reentrancies.\nFigure 4: The average distance for maximum attention for\neach head.\nour model.\nMore Analysis\nThe overall scores show a great advantage of the Graph\nTransformer over existing methods, including the state-of-\nthe-art GNN-based models. However, they do not shed light\ninto how this is achieved. In order to further reveal the source\nof performance gain, we perform a series of analysis based\non different characteristics of graphs. For those analyses, we\nuse sentence-level\nCHR F++ scores, and take the macro av-\nerage of them when needed. All experiments are conducted\nwith the test set of LDC2017T10.\nGraph Size To assess the model’s performance for differ-\nent sizes of graphs, we group graphs into four classes and\nshow the curves of\nCHR F++ scores in Figure 3a. The re-\nsults are presented with the contrast with the state-of-the-art\nGNN-based model of Guo et al.(2019), denoted as Guo’19.\nAs seen, the performance of both models decreases as the\ngraph size increases. It is expected since a larger graph of-\nten contains more complex structure and the interactions\nbetween graph elements are more difﬁcult to capture. The\ngap between ours and Guo’19 becomes larger for relatively\nlarger graphs while for small graphs, both models give simi-\nlar performance. This result demonstrates that our model has\nbetter ability for dealing with complicated graphs. As for ex-\ntremely large graphs, the performance of both models have\na clear drop, yet ours is still slightly better.\nGraph Diameter We then study the impact of graph diam-\neter.\n9 Graphs with large diameters have interactions between\ntwo nodes that appear distant from each other. We conjec-\nture that it will cause severe difﬁculties for GNN-based mod-\nels because they solely rely on local communication. Figure\n3b conﬁrms our hypothesis, as the curve of the GNN-based\nmodel shows a clear slope. In contrast, our model has more\nstable performance, and the gap between the two curves also\nillustrates the superiority of our model on featuring long-\ndistance dependencies.\nNumber of Reentrancies We study the ability for han-\ndling the reentrancies, where the same node has multiple\nparent nodes (or the same concept participates in multiple\nrelations for AMR). The recent work (Damonte and Cohen\n2019) has been identiﬁed reentrancies as one of the most dif-\nﬁcult aspects of AMR structure. We bin the number of reen-\ntrancies occurred in a graph into four classes and plot Fig.\n3c. It can be observed that the gap between the GNN-based\nmodel and the Graph transformer becomes noticeably wide\nwhen more than one reentrancies start to happen. Since then,\nour model is consistently better than the GNN-based model,\nmaintaining a margin of over1\nCHRF ++ score.\nHow Far Does Attention Look At The Graph Trans-\nformer shows a strong capacity for processing complex and\nlarge graphs. We attribute the success to the global commu-\nnication design, as it provides opportunities for direct com-\nmunication in long distance. A natural and interesting ques-\ntion is how well the model makes use of this property. To\nanswer this question, following V oita et al.(2019), we turn\nto study the attention distribution of each attention head.\nSpeciﬁcally, we record the speciﬁc distance of its maximum\nattention weight is assigned to. Fig. 4 shows the averaged\nthe attention distance after we run on the development set\nof LDC2017T10. We can observe that nearly half of the at-\ntention heads have an average attention distance larger than\n2. The number of these distance heads generally increases\nas layers go deeper. Interestingly, the longest-reaching head\n(layer1-head5) and the shortest-sighted head (layer1-head2)\ncoexist in the very ﬁrst layer, while the former has an aver-\nage distance over 5.\n9The diameter of a graph is deﬁned as the length of the longest\nshortest path between two nodes.\n7470\nConclusions\nIn this paper, we presented the Graph Transformer, the ﬁrst\ngraph-to-sequence learning based entirely on automatic at-\ntention. Different from previous recurrent models that re-\nquire linearization of input graph and previous graph neural\nnetwork models that restrict the message passing in the ﬁrst-\norder neighborhood, our model enables global node-to-node\ncommunication. With the Graph Transformer, we achieve\nthe new state-of-the-art on two typical graph-to-sequence\ngeneration tasks with four benchmark datasets.\nReferences\nAlberti, C.; Andor, D.; Bogatyy, I.; Collins, M.; Gillick, D.; Kong,\nL.; Koo, T.; Ma, J.; Omernick, M.; Petrov, S.; et al. 2017. Syn-\ntaxnet models for the conll 2017 shared task. arXiv preprint\narXiv:1703.04929.\nBahdanau, D.; Cho, K.; and Bengio, Y . 2015. Neural machine\ntranslation by jointly learning to align and translate. InICLR.\nBanarescu, L.; Bonial, C.; Cai, S.; Georgescu, M.; Grifﬁtt, K.; Her-\nmjakob, U.; Knight, K.; Koehn, P .; Palmer, M.; and Schneider, N.\n2013. Abstract meaning representation for sembanking. In Pro-\nceedings of the 7th Linguistic Annotation Workshop and Interoper-\nability with Discourse, 178–186.\nBastings, J.; Titov, I.; Aziz, W.; Marcheggiani, D.; and Sima’an,\nK. 2017. Graph convolutional encoders for syntax-aware neural\nmachine translation. InEMNLP, 1957–1967.\nBeck, D.; Haffari, G.; and Cohn, T. 2018. Graph-to-sequence learn-\ning using gated graph neural networks. InACL, 273–283.\nCao, K., and Clark, S. 2019. Factorising AMR generation through\nsyntax. In NAACL, 2157–2163.\nCho, K.; V an Merri ¨enboer, B.; Gulcehre, C.; Bahdanau, D.;\nBougares, F.; Schwenk, H.; and Bengio, Y . 2014. Learning phrase\nrepresentations using rnn encoder-decoder for statistical machine\ntranslation. arXiv preprint arXiv:1406.1078.\nDai, Z.; Y ang, Z.; Y ang, Y .; Carbonell, J.; Le, Q.; and Salakhutdi-\nnov, R. 2019. Transformer-XL: Attentive language models beyond\na ﬁxed-length context. InACL, 2978–2988.\nDamonte, M., and Cohen, S. B. 2019. Structural neural encoders\nfor AMR-to-text generation. InNAACL, 3649–3658.\nDenkowski, M., and Lavie, A. 2014. Meteor universal: Language\nspeciﬁc translation evaluation for any target language. InProceed-\nings of the EACL 2014 Workshop on Statistical Machine Transla-\ntion.\nFlanigan, J.; Dyer, C.; Smith, N. A.; and Carbonell, J. 2016. Gener-\nation from abstract meaning representation using tree transducers.\nIn NAACL, 731–739.\nGu, J.; Lu, Z.; Li, H.; and Li, V . O. 2016. Incorporating copying\nmechanism in sequence-to-sequence learning. InACL, 1631–1640.\nGuo, Z.; Zhang, Y .; Teng, Z.; and Lu, W. 2019. Densely con-\nnected graph convolutional networks for graph-to-sequence learn-\ning. Transactions of the Association for Computational Linguistics\n7:297–312.\nJones, B.; Andreas, J.; Bauer, D.; Hermann, K. M.; and Knight,\nK. 2012. Semantics-based machine translation with hyperedge\nreplacement grammars. InCOLING, 1359–1376.\nKingma, D. P ., and Ba, J. 2014. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980.\nKipf, T. N., and Welling, M. 2017. Semi-supervised classiﬁcation\nwith graph convolutional networks. InICLR.\nKoncel-Kedziorski, R.; Bekal, D.; Luan, Y .; Lapata, M.; and Ha-\njishirzi, H. 2019. Text Generation from Knowledge Graphs with\nGraph Transformers. InNAACL, 2284–2293.\nKonstas, I.; Iyer, S.; Y atskar, M.; Choi, Y .; and Zettlemoyer, L.\n2017. Neural AMR: Sequence-to-sequence models for parsing and\ngeneration. In ACL, 146–157.\nLi, Y .; Tarlow, D.; Brockschmidt, M.; and Zemel, R. 2016. Gated\ngraph sequence neural networks. InICLR.\nLiu, F.; Flanigan, J.; Thomson, S.; Sadeh, N.; and Smith, N. A.\n2015. Toward abstractive summarization using semantic represen-\ntations. In NAACL, 1077–1086.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. Bleu: a\nmethod for automatic evaluation of machine translation. InACL,\n311–318.\nPopovi´c, M. 2017. chrf++: words helping character n-grams. In\nProceedings of the second conference on machine translation, 612–\n618.\nPourdamghani, N.; Knight, K.; and Hermjakob, U. 2016. Gen-\nerating english from abstract meaning representations. In INLG,\n21–25.\nSee, A.; Liu, P . J.; and Manning, C. D. 2017. Get to the point:\nSummarization with pointer-generator networks. In ACL, 1073–\n1083.\nSennrich, R.; Haddow, B.; and Birch, A. 2016. Neural machine\ntranslation of rare words with subword units. InACL, 1715–1725.\nShaw, P .; Uszkoreit, J.; and V aswani, A. 2018. Self-attention with\nrelative position representations. InNAACL, 464–468.\nSong, L.; Zhang, Y .; Peng, X.; Wang, Z.; and Gildea, D. 2016.\nAMR-to-text generation as a traveling salesman problem. In\nEMNLP, 2084–2089.\nSong, L.; Peng, X.; Zhang, Y .; Wang, Z.; and Gildea, D.\n2017. AMR-to-text generation with synchronous node replacement\ngrammar. In ACL, 7–13.\nSong, L.; Zhang, Y .; Wang, Z.; and Gildea, D. 2018. A graph-to-\nsequence model for AMR-to-text generation. InACL, 1616–1626.\nSrivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and\nSalakhutdinov, R. 2014. Dropout: a simple way to prevent neu-\nral networks from overﬁtting. The Journal of Machine Learning\nResearch 15(1):1929–1958.\nV aswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is\nall you need. InNIPS, 5998–6008.\nV oita, E.; Talbot, D.; Moiseev, F.; Sennrich, R.; and Titov, I.\n2019. Analyzing multi-head self-attention: Specialized heads do\nthe heavy lifting, the rest can be pruned. InACL, 5797–5808.\nXu, K.; Wu, L.; Wang, Z.; Feng, Y .; Witbrock, M.; and Sheinin, V .\n2018. Graph2seq: Graph to sequence learning with attention-based\nneural networks. arXiv preprint arXiv:1804.00823.\n7471",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7831627130508423
    },
    {
      "name": "Transformer",
      "score": 0.7012181878089905
    },
    {
      "name": "Machine translation",
      "score": 0.6382213830947876
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5977908372879028
    },
    {
      "name": "Graph",
      "score": 0.5611201524734497
    },
    {
      "name": "Natural language processing",
      "score": 0.5004532337188721
    },
    {
      "name": "BLEU",
      "score": 0.45965638756752014
    },
    {
      "name": "Theoretical computer science",
      "score": 0.41755998134613037
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}