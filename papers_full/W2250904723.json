{
  "title": "Exploration of the Impact of Maximum Entropy in Recurrent Neural Network Language Models for Code-Switching Speech",
  "url": "https://openalex.org/W2250904723",
  "year": 2014,
  "authors": [
    {
      "id": "https://openalex.org/A2026971652",
      "name": "Ngoc Thang Vu",
      "affiliations": [
        "Karlsruhe Institute of Technology",
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A2141485797",
      "name": "Tanja Schultz",
      "affiliations": [
        "Karlsruhe Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4230872509",
    "https://openalex.org/W2145867197",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2110415041",
    "https://openalex.org/W2153433699",
    "https://openalex.org/W2049907881",
    "https://openalex.org/W1604771987",
    "https://openalex.org/W2034585809",
    "https://openalex.org/W2407682997",
    "https://openalex.org/W2098355803",
    "https://openalex.org/W4251217171",
    "https://openalex.org/W118155631",
    "https://openalex.org/W2100506586",
    "https://openalex.org/W2013489815",
    "https://openalex.org/W121747529",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2295078202",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W164793848",
    "https://openalex.org/W2405047074",
    "https://openalex.org/W761725120",
    "https://openalex.org/W1965154800",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2168708086",
    "https://openalex.org/W1631260214"
  ],
  "abstract": "This paper presents our latest investigations of the jointly trained maximum entropy and recurrent neural network language models for Code-Switching speech.First, we explore extensively the integration of part-of-speech tags and language identifier information in recurrent neural network language models for Code-Switching.Second, the importance of the maximum entropy model is demonstrated along with a various of experimental results.Finally, we propose to adapt the recurrent neural network language model to different Code-Switching behaviors and use them to generate artificial Code-Switching text data.",
  "full_text": "Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 34–41,\nOctober 25, 2014, Doha, Qatar.c⃝2014 Association for Computational Linguistics\nExploration of the Impact of Maximum Entropy in Recurrent Neural\nNetwork Language Models for Code-Switching Speech\nNgoc Thang Vu1,2 and Tanja Schultz1\n1Karlsruhe Institute of Technology (KIT), 2University of Munich (LMU), Germany\nthangvu@cis.lmu.de, tanja.schultz@kit.edu\nAbstract\nThis paper presents our latest investiga-\ntions of the jointly trained maximum en-\ntropy and recurrent neural network lan-\nguage models for Code-Switching speech.\nFirst, we explore extensively the integra-\ntion of part-of-speech tags and language\nidentiﬁer information in recurrent neu-\nral network language models for Code-\nSwitching. Second, the importance of\nthe maximum entropy model is demon-\nstrated along with a various of experi-\nmental results. Finally, we propose to\nadapt the recurrent neural network lan-\nguage model to different Code-Switching\nbehaviors and use them to generate artiﬁ-\ncial Code-Switching text data.\n1 Introduction\nThe term Code-Switching (CS) denotes speech\nwhich contains more than one language. Speakers\nswitch their language while they are talking. This\nphenomenon appears very often in multilingual\ncommunities, such as in India, Hong Kong or Sin-\ngapore. Furthermore, it increasingly occurs in for-\nmer monolingual cultures due to the strong growth\nof globalization. In many contexts and domains,\nspeakers switch more often between their native\nlanguage and English within their utterances than\nin the past. This is a challenge for speech recog-\nnition systems which are typically monolingual.\nWhile there have been promising approaches to\nhandle Code-Switching in the ﬁeld of acoustic\nmodeling, language modeling is still a great chal-\nlenge. The main reason is a shortage of training\ndata. Whereas about 50h of training data might\nbe sufﬁcient for the estimation of acoustic mod-\nels, the transcriptions of these data are not enough\nto build reliable language models. In this paper,\nwe focus on exploring and improving the language\nmodel for Code-switching speech and as a result\nimprove the automatic speech recognition (ASR)\nsystem on Code-Switching speech.\nThe main contribution of the paper is the exten-\nsive investigation of jointly trained maximum en-\ntropy (ME) and recurrent neural language models\n(RNN LMs) for Code-Switching speech. We re-\nvisit the integration of part-of-speech (POS) tags\nand language identiﬁer (LID) information in recur-\nrent neural network language models and the im-\npact of maximum entropy on the language model\nperformance. As follow-up to our previous work\nin (Adel, Vu et al., 2013), here we investigate\nwhether a recurrent neural network alone without\nusing ME is a suitable model for Code-Switching\nspeech. Afterwards, to directly use the RNN LM\nin the decoding process of an ASR system, we\nconvert the RNN LM into the n-gram language\nmodel using the text generation approach (Deoras\net al., 2011; Adel et al., 2014); Furthermore moti-\nvated by the fact that Code-Switching is speaker\ndependent (Auer, 1999b; Vu et al., 2013), we\nﬁrst adapt the recurrent neural network language\nmodel to different Code-Switching behaviors and\nthen generate artiﬁcial Code-Switching text data.\nThis allows us to train an accurate n-gram model\nwhich can be used directly during decoding to im-\nprove ASR performance.\nThe paper is organized as follows: Section 2\ngives a short overview of related works. In Sec-\ntion 3, we describe the jointly trained maximum\nentropy and recurrent neural network language\nmodels and their extension for Code-Switching\nspeech. Section 4 gives a short description of the\nSEAME corpus. In Section 5, we summarize the\nmost important experiments and results. The study\nis concluded in Section 6 with a summary.\n2 Related Work\nThis section gives a brief introduction about the\nrelated research regarding Code-Switching and re-\n34\ncurrent language models.\nIn (Muysken, 2000; Poplack, 1978; Bokamba,\n1989), the authors observed that code switches\noccur at positions in an utterance following syn-\ntactical rules of the involved languages. Code-\nSwitching can be regarded as a speaker depen-\ndent phenomenon (Auer, 1999b; Vu et al., 2013).\nHowever, several particular Code-Switching pat-\nterns are shared across speakers (Poplack, 1980).\nFurthermore, part-of-speech tags might be useful\nfeatures to predict Code-Switching points. The\nauthors of (Solorio et al., 2008b; Solorio et al.,\n2008a) investigate several linguistic features, such\nas word form, LID, POS tags or the position of\nthe word relative to the phrase for Code-Switching\nprediction. Their best result is obtained by com-\nbining all those features. (Chan et al., 2006)\ncompare four different kinds of n-gram langua-\nge models to predict Code-Switching. They dis-\ncover that clustering all foreign words into their\nPOS classes leads to the best performance. In (Li\net al., 2012; Li et al., 2013), the authors propose\nto integrate the equivalence constraint into lan-\nguage modeling for Mandarin and English Code-\nSwitching speech recorded in Hong Kong.\nIn the last years, neural networks have been\nused for a variety of tasks, including language\nmodeling (Mikolov et al., 2010). Recurrent neu-\nral networks are able to handle long-term contexts\nsince the input vector does not only contain the\ncurrent word but also the previous hidden layer.\nIt is shown that these networks outperform tradi-\ntional language models, such as n-grams which\nonly contain very limited histories. In (Mikolov\net al., 2011a), the network is extended by factoriz-\ning the output layer into classes to accelerate the\ntraining and testing processes. The input layer\ncan be augmented to model features, such as POS\ntags (Shi et al., 2011; Adel, Vu et al., 2013). Fur-\nthermore, artiﬁcial text can be automatically gen-\nerated using recurrent neural networks to enlarge\nthe amount of training data (Deoras et al., 2011;\nAdel et al., 2014).\n3 Joint maximum entropy and recurrent\nneural networks language models for\nCode-Switching\n3.1 Recurrent neural network language\nmodels\nThe idea of RNN LMs is illustrated in Figure 1.\nVector w(t) forms the input of the recurrent neu-\nFigure 1: RNN language model\nral network. It represents the current word using\n1-of-N coding. Thus, its dimension equals the\nsize of the vocabulary. Vector s(t) contains the\nstate of the network - ’hidden layer’. The network\nis trained using back-propagation through time\n(BPTT), an extension of the back-propagation\nalgorithm for recurrent neural networks. With\nBPTT, the error is propagated through recurrent\nconnections back in time for a speciﬁc number of\ntime steps t. Hence, the network is able to capture\na longer history than a traditional n-gram LM. The\nmatrices U, V and W contain the weights for the\nconnections between the layers. These weights are\nlearned during the training phase.\nTo accelerate the training process, (Mikolov et\nal., 2011a) factorized the output layer into classes\nbased on simple frequency binning. Every word\nbelongs to exactly one class. Vector c(t) contains\nthe probabilities for each class and vector w(t)\nprovides the probabilities for each word given its\nclass. Hence, the probability P(wi|history) is\ncomputed as shown in equation 1.\nP(wi|history) =P(ci|s(t))P(wi|ci, s(t)) (1)\nFurthermore in (Mikolov et al., 2011b), the au-\nthors proposed to jointly train the RNN with ME\n- RMM-ME - to improve the language model and\nalso ASR performance. The ME can be seen as\na weight matrix which directly connects the in-\nput with the output layer as well as the input with\nthe class layer. This weight matrix can be trained\njointly with the recurrent neural network. “Direct-\norder” and “direct connection” are the two impor-\ntant parameters which deﬁne the length of history\nand the number of the trained connections.\n35\n3.2 Code-Switching language models\nTo adapt RNN LMs to the Code-Switching task,\n(Adel, Vu et al., 2013) analyzed the SEAME cor-\npus and observed that there are words and POS\ntags which might have a high potential to predict\nCode-Switching points. Therefore, it has been\nproposed to integrate the POS and LID informa-\ntion into the RNN LM. The idea is to factorize\nthe output layer into classes which provide lan-\nguage information. By doing that, it is intended\nto not only predict the next word but also the\nnext language. Hence according to equation 1, the\nprobability of the next language is computed ﬁrst\nand then the probability of each word given the\nlanguage. In that work, four classes were used:\nEnglish, Mandarin, other languages and particles.\nMoreover, a vector f(t) which contains the POS\ninformation is added to the input layer. This vec-\ntor provides the corresponding POS of the current\nword. Thus, not only the current word is activated\nbut also its features. Since the POS tags are in-\ntegrated into the input layer, they are also propa-\ngated into the hidden layer and back-propagated\ninto its history s(t). Hence, not only the previous\nfeatures are stored in the history but also features\nfrom several time steps in the past.\nIn addition to that previous work, the experi-\nments in this paper aim to explore the source of\nthe improvements observed in (Adel, Vu et al.,\n2013). We now clearly distinguish between the\nimpacts due to the long but unordered history of\nthe RNN and the effects of the maximum entropy\nmodel which also captures information about the\nmost recent word and POS tag in the history.\n4 SEAME corpus\nTo conduct research on Code-Switching speech\nwe use the SEAME corpus (South East Asia\nMandarin-English). It is a conversational\nMandarin-English Code-Switching speech corpus\nrecorded by (D.C. Lyu et al., 2011). Originally, it\nwas used for the research project “Code-Switch”\nwhich was jointly performed by Nanyang Tech-\nnological University (NTU) and Karlsruhe Insti-\ntute of Technology (KIT) from 2009 until 2012.\nThe corpus contains 63 hours of audio data which\nhas been recorded and manually transcribed in\nSingapore and Malaysia. The recordings consist\nof spontaneously spoken interviews and conver-\nsations. The words can be divided into four lan-\nguage categories: English words (34.3% of all to-\nkens), Mandarin words (58.6%), particles (Singa-\nporean and Malayan discourse particles, 6.8% of\nall tokens) and others (other languages, 0.4% of\nall tokens). In total, the corpus contains 9,210\nunique English and 7,471 unique Mandarin words.\nThe Mandarin character sequences have been seg-\nmented into words manually. The language dis-\ntribution shows that the corpus does not contain a\nclearly predominant language. Furthermore, the\nnumber of Code-Switching points is quite high:\nOn average, there are 2.6 switches between Man-\ndarin and English per utterance. Additionally, the\nduration of the monolingual segments is rather\nshort: More than 82% of the English segments and\n73% of the Mandarin segments last less than one\nsecond. The average duration of English and Man-\ndarin segments is only 0.67 seconds and 0.81 sec-\nonds, respectively. This corresponds to an average\nlength of monolingual segments of 1.8 words in\nEnglish and 3.6 words in Mandarin.\nFor the task of language modeling and speech\nrecognition, the corpus has been divided into three\ndisjoint sets: training, development and evaluation\nset. The data is assigned to the three different sets\nbased on the following criteria: a balanced distri-\nbution of gender, speaking style, ratio of Singa-\nporean and Malaysian speakers, ratio of the four\nlanguage categories, and the duration in each set.\nTable 1 lists the statistics of the SEAME corpus.\nTraining Dev Eval\n# Speakers 139 8 8\nDuration(hours) 59.2 2.1 1.5\n# Utterances 48,040 1,943 1,029\n# Words 575,641 23,293 11,541\nTable 1: Statistics of the SEAME corpus\n5 Experiments and Results\nThis section presents all the experiments and re-\nsults regarding language models and ASR on the\ndevelopment and the evaluation set of the SEAME\ncorpus. However, the parameters were tuned only\non the development set.\n5.1 LM experiments\n5.1.1 Baseline n-gram\nThe n-gram language model served as the baseline\nin this work. We used the SRI language model\ntoolkit (Stolcke, 2002) to build the CS 3-gram\nbaseline from the SEAME training transcriptions\n36\ncontaining all words of the transcriptions. Modi-\nﬁed Kneser-Ney smoothing (Rosenfeld, 2000) was\napplied. In total, the vocabulary size is around\n16k words. The perplexities (PPLs) are 268.4 and\n282.9 on the development and evaluation set re-\nspectively.\n5.1.2 Exploration of ME and of the\nintegration of POS and LID in RNN\nTo investigate the effect of POS and LID integra-\ntion into the RNN LM and the importance of the\nME, different RNN LMs were trained.\nThe ﬁrst experiment aims at investigating the\nimportance of using LID information for output\nlayer factorization. All the results are summarized\nin table 2. The ﬁrst RNNLM was trained with a\nhidden layer of 50 nodes and without using output\nfactorization and ME. The PPLs were 250.8 and\n301.1 on the development and evaluation set, re-\nspectively. We observed some gains in terms of\nPPL on the development set but not on the eval-\nuation set compared to the n-gram LM. Even us-\ning ME and factorizing the output layer into four\nclasses based on frequency binning (fb), the same\ntrend could be noticed - only the PPL on the devel-\nopment set was improved. Four classes were used\nto have a fair comparison with the output factor-\nization with LID. However after including the LID\ninformation into the output layer, the PPLs were\nimproved on both data sets. On top of that, using\nME provides some additional gains. The results\nindicate that LID is a useful information source\nfor the Code-Switching task. Furthermore, the im-\nprovements are independent of the application of\nME.\nModel Dev Eval\nCS 3-gram 268.4 282.9\nRNN LM 250.8 301.1\nRNN-ME LM 246.6 287.9\nRNN LM with fb 246.0 287.3\nRNN-ME LM with fb 256.0 294.0\nRNN LM with LID 241.5 274.4\nRNN-ME LM with LID 237.9 269.3\nTable 2: Effect of output layer factorization\nIn the second experiment we investigated the\nuse of POS information and the effect of the ME.\nThe results in Table 3 show that an integration of\nPOS without ME did not give any further improve-\nment compared to RNN LM. The reason could lie\nin the fact that a RNN can capture a long history\nbut not the information of the word order. Note\nthat in the syntactic context, the word order is one\nof the most important information. However us-\ning ME allows using the POS of the previous time\nstep to predict the next language and also the next\nword, the PPL was improved signiﬁcantly on de-\nvelopment and evaluation set. These results reveal\nthat POS is a reasonable trigger event which can\nbe used to support Code-Switching prediction.\nModel Dev Eval\nCS 3-gram 268.4 282.9\nRNN LM 250.8 301.1\nRNN-ME LM 246.6 287.9\nRNN LM with POS 250.6 298.3\nRNN-ME LM with POS 233.5 268.0\nTable 3: Effect of ME on the POS integration into\nthe input layer\nFinally, we trained an LM by integrating the\nPOS tags and factorizing the output layer with LID\ninformation. Again without applying ME, we ob-\nserved that POS information is not helpful to im-\nprove the RNN LM. Using the ME provides a big\ngain in terms of PPL on both data sets. We ob-\ntained a PPL of 219.8 and 239.2 on the develop-\nment and evaluation set respectively.\nModel Dev Eval\nCS 3-gram 268.4 282.9\nRNN LM 250.8 301.1\nRNN-ME LM 246.6 287.9\nRNN LM with POS + LID 243.9 277.1\nRNN-ME LM with POS+ LID 219.8 239.2\nTable 4: Effect of ME on the integration of POS\nand the output layer factorization using LID\n5.1.3 Training parameters\nMoreover, we investigated the effect of different\nparameters, such as the backpropagation through\ntime (BPTT) step, the direct connection order and\nthe amount of direct connections on the perfor-\nmance of the RNN-ME LMs. Therefore, different\nLMs were trained with varying values for these\nparameters. For each parameter change, the re-\nmaining parameters were ﬁxed to the most suitable\nvalue which has been found so far.\nFirst, we varied the BPTT step from 1 to 5. The\nBPTT step deﬁnes the length of the history which\nis incorporated to update the weight matrix of the\n37\nRNN. The larger the BPTT step is, the longer is the\nhistory which is used for learning. Table 5 shows\nthe perplexities on the SEAME development and\nevaluation sets with different BPTT steps. The\nresults indicate that increasing BPTT might im-\nprove the PPL. The best PPL can be obtained with\na BPTT step of 4. The big loss in terms of PPL\nby using a BPTT step of 5 indicates that too long\nhistories might hurt the language model perfor-\nmance. Another reason might be the limitation of\nthe training data.\nBPTT 1 2 3 4 5\nDev 244.7 224.6 222.8 219.8 266.8\nEval 281.1 241.4 242.8 239.2 284.5\nTable 5: Effect of the BPTT step\nIt has been shown in the previous section, that\nME is very important to improve the PPL espe-\ncially for the Code-Switching task, we also trained\nseveral RNN-ME LMs with various values for “di-\nrect order” and “direct connection”. Table 6 and\n7 summarize the PPL on the SEAME develop-\nment and evaluation set. The results reveal that\nthe larger the direct order is, the lower is the PPL.\nWe observed consistent PPL improvement by in-\ncreasing the direct order. However, the gain seems\nto be saturated after a direct order of 3 or 4. In this\npaper, we choose to use a direct order of 4 to train\nthe ﬁnal model.\nDirect order 1 2 3 4\nDev 238.6 231.7 220.5 219.8\nEval 271.8 261.4 240.7 239.2\nTable 6: Effect of the direct order\nSince the “direct order” is related to the length\nof the context, the size of the “direct connection” is\na trade off between the size of the language model\nand also the amount of the training data. Higher\n“direct connection” leads to a larger model and\nmight improve the PPL if the amount of training\ndata is enough to train all the direct connection\nweights. The results with four different data points\n(50M, 100M, 150M and 200M) show that the best\nmodel can be obtained on SEAME data set by us-\ning 100M of direct connection.\n5.1.4 Artiﬁcial Code-Switching text\ngeneration using RNN\nThe RNN LM demonstrates a great improvement\nover the traditional n-gram language model. How-\n#Connection 50M 100M 150M 200M\nDev 226.2 219.8 224.7 224.6\nEval 244.7 239.2 243.7 242.0\nTable 7: Effect of the number of direct connections\never, it is inefﬁcient to use the RNN LM directly\nin the decoding process of an ASR system. In or-\nder to convert the RNN into a n-gram language\nmodel, a text generation method which was pro-\nposed in (Deoras et al., 2011) can be applied.\nMoreover, it allows to generate more training data\nwhich might be useful to improve the data sparsity\nof the language modeling task for Code-Switching\nspeech. In (Deoras et al., 2011), the authors ap-\nplied the Gibb sampling method to generate artiﬁ-\ncial text based on the probability distribution pro-\nvided by the RNNs. We applied that technique\nin (Adel et al., 2014) to generate Code-Switching\ndata and were able to improve the PPL and ASR\nperformance on CS speech. In addition to that pre-\nvious work, we now propose to use several Code-\nSwitching attitude dependent language models in-\nstead of the ﬁnal best RNN LM.\nCode-Switching attitude dependent language\nmodeling Since POS tags might have a potential\nto predict Code-Switch points, (Vu et al., 2013)\nperformed an analysis of these trigger POS tags\non a speaker level. The CS rate for each tag was\ncomputed for each speaker. Afterwards, we calcu-\nlated the minimum, maximum and mean values as\nwell as standard deviations. We observed that the\nspread between minimum and maximum values is\nquite high for most of the tags. It indicates that al-\nthough POS information may trigger a CS event,\nit is rather speaker dependent.\nMotivated by this observation, we performed k-\nmean clustering of the training text into three dif-\nferent portions of text data which describe differ-\nent Code-Switching behaviors (Vu et al., 2013).\nAfterwards, the LM was adapted with each text\nportion to obtain Code-Switching attitude depen-\ndent language models. By using these models, we\ncould improve both PPL and ASR performance for\neach speaker.\nArtiﬁcial text generation To generate artiﬁcial\ntext, we ﬁrst adapted the best RNN-ME LM de-\nscribed in the previous section to three different\nCode-Switching attitudes. Afterwards, we gen-\nerated three different text corpora based on these\nspeciﬁc Code-Switching attitudes. Each corpus\n38\ncontains 100M tokens. We applied the SRILM\ntoolkit (Stolcke, 2002) to train n-gram language\nmodel and interpolated them linearly with the\nweight = 1\n3 . Table 8 shows the perplexity of the\nresulting n-gram models on the SEAME develop-\nment and evaluation set. To make a comparison,\nwe also used the unadapted best RNN-ME LM to\ngenerate two different texts, one with 300M to-\nkens and another one with 235M tokens (Adel et\nal., 2014). The results show that the n-gram LMs\ntrained with only the artiﬁcial text data can not\noutperform the baseline CS 3-gram. However they\nprovide some complementary information to the\nbaseline CS 3-gram LM. Therefore, when we in-\nterpolated them with the baseline CS 3-gram, the\nPPL was improved all the cases. Furthermore by\nusing the Code-Switching attitude dependent lan-\nguage models to generate artiﬁcial CS text data,\nthe PPL was slightly improved compared to using\nthe unadapted one. The ﬁnal 3-gram model (Final\n3-gram) was built by interpolating all the Code-\nSwitching attitude dependent 3-gram and the base-\nline CS 3-gram. It has a PPL of 249.3 and 266.9\non the development set and evaluation set.\nModels Dev Eval\nCS 3-gram 268.4 282.9\n300M words text 391.3 459.5\n+ CS 3-gram 250.0 270.9\n235M words text 385.1 454.6\n+ CS 3-gram 249.5 270.5\n100M words text I 425.4 514.4\n+ CS 3-gram 251.4 274.5\n100M words text II 391.8 421.6\n+ CS 3-gram 251.6 266.4\n100M words text III 390.3 428.1\n+ CS 3-gram 250.6 266.9\nInterpolation of I, II and III 377.5 416.1\n+ CS 3-gram (Final n-gram) 249.3 266.9\nRNN-ME LM + POS + LID 219.8 239.2\nTable 8: PPL of the N-gram models trained with\nartiﬁcial text data\n5.2 ASR experiments\nFor the ASR experiments, we applied BioKIT, a\ndynamic one-pass decoder (Telaar et al., 2014).\nThe acoustic model is speaker independent and\nhas been trained with all the training data. To ex-\ntract the features, we ﬁrst trained a multilayer per-\nceptron (MLP) with a small hidden layer with 40\nnodes. The output of this hidden layer is called\nbottle neck features and is used to train the acous-\ntic model. The MLP has been initialized with a\nmultilingual multilayer perceptron as described in\n(Vu et al., 2012). The phone set contains English\nand Mandarin phones, ﬁller models for continu-\nous speech (+noise+, +breath+, +laugh+) and an\nadditional phone +particle+ for Singaporean and\nMalayan particles. The acoustic model applied\na fully-continuous 3-state left-to-right HMM. The\nemission probabilities were modeled with Gaus-\nsian mixture models. We used a context dependent\nacoustic model with 3,500 quintphones. Merge-\nand-split training was applied followed by six it-\nerations of Viterbi training. To obtain a dictio-\nnary, the CMU English (CMU Dictionary, 2014)\nand Mandarin (Hsiao et al., 2008) pronunciation\ndictionaries were merged into one bilingual pro-\nnunciation dictionary. Additionally, several rules\nfrom (Chen et al., 2010) were applied which gen-\nerate pronunciation variants for Singaporean En-\nglish.\nAs a performance measure for decoding Code-\nSwitching speech, we used the mixed error rate\n(MER) which applies word error rates to En-\nglish and character error rates to Mandarin seg-\nments (Vu et al., 2012). With character error\nrates for Mandarin, the performance can be com-\npared across different word segmentations. Ta-\nble 9 shows the results of the baseline CS 3-gram\nLM, the 3-gram LM trained with 235M artiﬁcial\nwords interpolated with CS 3-gram LM and the ﬁ-\nnal 3-gram LM described in the previous section.\nCompared to the baseline system, we are able to\nimprove the MER by up to 3% relative. Further-\nmore, a very small gain can be observed by using\nthe Code-Switching attitude dependent language\nmodel compared to the unadapted best RNN-ME\nLM.\nModel Dev Eval\nCS 3-gram 40.0% 34.3%\n235M words text + CS-3gram 39.4% 33.4%\nFinal 3-gram 39.2% 33.3%\nTable 9: ASR results on SEAME data\n6 Conclusion\nThis paper presents an extensive investigation of\nthe impact of maximum entropy in recurrent neu-\nral network language models for Code-Switching\n39\nspeech. The experimental results reveal that fac-\ntorization of the output layer of the RNN us-\ning LID always improved the PPL independent\nwhether the ME is used. However, the integra-\ntion of the POS tags into the input layer only im-\nproved the PPL in combination with ME. The best\nLM can be obtained by jointly training the ME\nand the RNN LM with POS integration and fac-\ntorization using LID. Moreover, using the RNN-\nME LM allows generating artiﬁcial CS text data\nand therefore training an n-gram LM which car-\nries the information of the RNN-ME LM. This can\nbe directly used during decoding to improve ASR\nperformance on Code-Switching speech. On the\nSEAME development and evaluation set, we ob-\ntained an improvement of up to 18% relative in\nterms of PPL and 3% relative in terms of MER.\n7 Acknowledgment\nThis follow-up work on exploring the impact of\nmaximum entropy in recurrent neural network lan-\nguage models for Code-Switching speech was mo-\ntivated by the very useful comments and sugges-\ntions of the SLSP reviewers, for which we are very\ngrateful.\nReferences\nH. Adel, N.T. Vu, F. Kraus, T. Schlippe, and T. Schultz.\nRecurrent Neural Network Language Modeling for\nCode Switching Conversational Speech In: Pro-\nceedings of ICASSP 2013.\nH. Adel, K. Kirchhoff, N.T. Vu, D.Telaar, T. Schultz\nComparing Approaches to Convert Recurrent Neu-\nral Networks into Backoff Language Models For Ef-\nﬁcient Decoding In: Proceedings of Interspeech\n2014.\nP. Auer Code-Switching in Conversation Routledge\n1999.\nP. Auer From codeswitching via language mixing to\nfused lects toward a dynamic typology of bilingual\nspeech In: International Journal of Bilingualism,\nvol. 3, no. 4, pp. 309-332, 1999.\nE.G. Bokamba Are there syntactic constraints on code-\nmixing? In: World Englishes, vol. 8, no. 3, pp. 277-\n292, 1989.\nJ.Y .C. Chan, PC Ching, T. Lee, and H. Cao Au-\ntomatic speech recognition of Cantonese-English\ncode-mixing utterances In: Proceeding of Inter-\nspeech 2006.\nW. Chen, Y . Tan, E. Chng, H. LiThe development of a\nSingapore English call resource In: Proceedings of\nOriental COCOSDA, 2010.\nCarnegie Mellon University CMU pronoun-\ncation dictionary for English Online:\nhttp://www.speech.cs.cmu.edu/cgi-bin/cmudict,\nretrieved in July 2014\nD.C. Lyu, T.P. Tan, E.S. Cheng, H. Li An Analysis of\nMandarin-English Code-Switching Speech Corpus:\nSEAME In: Proceedings of Interspeech 2011.\nA. Deoras, T. Mikolov, S. Kombrink, M. Karaﬁat, S.\nKhudanpur Variational approximation of long-span\nlanguage models for LVCSR In: Proceedings of\nICASSP 2011.\nR. Hsiao, M. Fuhs, Y . Tam, Q. Jin, T. Schultz The\nCMU-InterACT 2008 Mandarin transcription sys-\ntem In: Procceedings of ICASSP 2008.\nY . Li, P. Fung Code-Switch Language Model with\nInversion Constraints for Mixed Language Speech\nRecognition In: Proceedings of COLING 2012.\nY . Li, P. FungImproved mixed language speech recog-\nnition using asymmetric acoustic model and lan-\nguage model with Code-Switch inversion constraints\nIn: Proceedings of ICASSP 2013.\nM.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.\nBuilding a large annotated corpus of english: The\npenn treebank In: Computational Linguistics, vol.\n19, no. 2, pp. 313-330, 1993.\nT. Mikolov, M. Karaﬁat, L. Burget, J. Jernocky and S.\nKhudanpur. Recurrent Neural Network based Lan-\nguage Model In: Proceedings of Interspeech 2010.\nT. Mikolov, S. Kombrink, L. Burget, J. Jernocky and\nS. Khudanpur. Extensions of Recurrent Neural Net-\nwork Language Model In: Proceedings of ICASSP\n2011.\nT. Mikolov, A. Deoras, D. Povey, L. Burget, J.H. Cer-\nnocky Strategies for Training Large Scale Neu-\nral Network Language Models In: Proceedings of\nASRU 2011.\nP. Muysken Bilingual speech: A typology of code-\nmixing In: Cambridge University Press, vol. 11.\nS. Poplack Syntactic structure and social function\nof code-switching , Centro de Estudios Puertor-\nriquenos, City University of New York.\nS. Poplack Sometimes i’ll start a sentence in spanish\ny termino en espanol: toward a typology of code-\nswitching In: Linguistics, vol. 18, no. 7-8, pp. 581-\n618.\nD. Povey, A. Ghoshal, et al. The Kaldi speech recogni-\ntion toolkit In: Proceedings of ASRU 2011.\nR. Rosenfeld Two decades of statistical language mod-\neling: Where do we go from here? In: Proceedings\nof the IEEE 88.8 (2000): 1270-1278.\nT. Schultz, P. Fung, and C. Burgmer, Detecting code-\nswitch events based on textual features.\n40\nY . Shi, P. Wiggers, M. JonkerTowards Recurrent Neu-\nral Network Language Model with Linguistics and\nContextual Features In: Proceedings of Interspeech\n2011.\nT. Solorio, Y . Liu Part-of-speech tagging for English-\nSpanish code-switched text In: Proceedings of the\nConference on Empirical Methods in Natural Lan-\nguage Processing. Association for Computational\nLinguistics, 2008.\nT. Solorio, Y . Liu Learning to predict code-switching\npoints In: Proceedings of the Conference on Empir-\nical Methods in Natural Language Processing. As-\nsociation for Computational Linguistics, 2008.\nA. Stolcke SRILM-an extensible language modeling\ntoolkit. In: Proceedings of Interspeech 2012.\nD. Telaar, et al. BioKIT - Real-time Decoder For\nBiosignal Processing In: Proceedings of Inter-\nspeech 2014.\nN.T. Vu, D.C. Lyu, J. Weiner, D. Telaar, T. Schlippe,\nF. Blaicher, E.S. Chng, T. Schultz, H. Li A First\nSpeech Recognition System For Mandarin-English\nCode-Switch Conversational Speech In: Proceed-\nings of Interspeech 2012.\nN.T. Vu, H. Adel, T. SchultzAn Investigation of Code-\nSwitching Attitude Dependent Language Modeling\nIn: In Statistical Language and Speech Processing,\nFirst International Conference, 2013.\nN.T. Vu, F. Metze, T. Schultz Multilingual bottleneck\nfeatures and its application for under-resourced lan-\nguages In: Proceedings of SLTU, 2012.\n41",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8261843323707581
    },
    {
      "name": "Code-switching",
      "score": 0.759899377822876
    },
    {
      "name": "Identifier",
      "score": 0.7018106579780579
    },
    {
      "name": "Artificial neural network",
      "score": 0.6609808802604675
    },
    {
      "name": "Recurrent neural network",
      "score": 0.6579057574272156
    },
    {
      "name": "Language model",
      "score": 0.648179292678833
    },
    {
      "name": "Principle of maximum entropy",
      "score": 0.6079584360122681
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5077820420265198
    },
    {
      "name": "Entropy (arrow of time)",
      "score": 0.49600133299827576
    },
    {
      "name": "Code (set theory)",
      "score": 0.4869188070297241
    },
    {
      "name": "Natural language processing",
      "score": 0.4586702585220337
    },
    {
      "name": "Speech recognition",
      "score": 0.4471926987171173
    },
    {
      "name": "Time delay neural network",
      "score": 0.4169505834579468
    },
    {
      "name": "Programming language",
      "score": 0.18212220072746277
    },
    {
      "name": "Linguistics",
      "score": 0.06842264533042908
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}