{
  "title": "Evaluation of Neural Network Transformer Models for Named-Entity Recognition on Low-Resourced Languages",
  "url": "https://openalex.org/W3204386097",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2893536155",
      "name": "Ridewaan Hanslo",
      "affiliations": [
        "University of Pretoria"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3000409384",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2964090065",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W4250877543",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3103147437",
    "https://openalex.org/W2579153919",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W6767737316",
    "https://openalex.org/W2170505850",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W2970049541",
    "https://openalex.org/W3023728302",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2169818249"
  ],
  "abstract": "Neural Network (NN) models produce state-ofthe-art results for natural language processing tasks.Further, NN models are used for sequence tagging tasks on lowresourced languages with good results.However, the findings are not consistent for all low-resourced languages, and many of these languages have not been sufficiently evaluated.Therefore, in this paper, transformer NN models are used to evaluate named-entity recognition for ten low-resourced South African languages.Further, these transformer models are compared to other NN models and a Conditional Random Fields (CRF) Machine Learning (ML) model.The findings show that the transformer models have the highest F-scores with more than a 5% performance difference from the other models.However, the CRF ML model has the highest average F-score.The transformer model's greater parallelization allows lowresourced languages to be trained and tested with less effort and resource costs.This makes transformer models viable for low-resourced languages.Future research could improve upon these findings by implementing a linear-complexity recurrent transformer variant.",
  "full_text": "Abstract— Neural  Network  (NN)  models  produce  state-of-\nthe-art results for natural language processing tasks. Further,\nNN  models  are  used  for  sequence  tagging  tasks  on  low-\nresourced languages with good results. However, the findings\nare not consistent for all low-resourced languages, and many of\nthese languages have not been sufficiently evaluated. Therefore,\nin  this  paper,  transformer  NN  models  are  used  to  evaluate\nnamed-entity recognition for ten low-resourced South African\nlanguages. Further, these transformer models are compared to\nother  NN  models  and  a  Conditional  Random  Fields  (CRF)\nMachine  Learning  (ML)  model.  The  findings  show  that  the\ntransformer models have the highest F-scores with more than a\n5% performance difference from the other models. However,\nthe  CRF  ML  model  has  the  highest  average  F-score.  The\ntransformer  model’s  greater  parallelization  allows  low-\nresourced languages to be trained and tested with less effort\nand resource costs. This makes transformer models viable for\nlow-resourced languages. Future research could improve upon\nthese findings by implementing a linear-complexity recurrent\ntransformer variant.\nI. I NTRODUCTION\nLM-Roberta (XLM-R) is a recent transformer model\nthat  has  reported  state-of-the-art  results  for  Natural\nLanguage Processing (NLP) tasks and applications, such as\nNamed-Entity  Recognition  (NER),  Part-of-Speech  (POS)\ntagging,  phrase  chunking,  and  Machine  Translation  (MT)\n[2], [8].  The NER and POS sequence tagging tasks have\nbeen  extensively  researched  [1]-[6],  [8],  [9].  However,\nwithin  the  past  few  years,  the  introduction  of new  Deep\nLearning  (DL)  transformer  model  architectures  such  as\nXLM-R, Multilingual Bidirectional Encoder Representations\nfrom Transformers (M-BERT) and Cross-Lingual Language\nModel (XLM) lowers the time needed to train large datasets\nthrough  greater  parallelization  [7].  This  allows  low-re-\nsourced languages to be trained and tested with less effort\nand resource costs, with state-of-the-art results for sequence\ntagging tasks [1], [2],  [8]. M-BERT as a single language\nmodel pre-trained from monolingual corpora performs very\nwell with cross-lingual generalization [10]. Furthermore, M-\nBERT is capable of capturing multilingual representations\n[10]. On the other hand, XLM pre-training has led to strong\nimprovements on NLP benchmarks [11]. Additionally, XLM\nmodels have contributed to significant improvements in NLP\nX\nstudies involving low-resource languages [11]. These trans-\nformer models are usually trained on very large corpora with\ndatasets in terabyte (TB) sizes.\nA recent study by [1] researched the “ Viability of Neural\nNetworks  for  Core  Technologies  for  Resource-Scarce\nLanguages ”. These resource-scarce languages are ten of the\n11  official  South  African  (SA)  languages,  with  English\nbeing  excluded.  The  languages  are  considered  low-\nresourced, with Afrikaans (af) being the more resourced of\nthe ten [1], [9]. This recent study looked at sequence tagging\n(POS  tagging  and  NER)  and  sequence  translation\n(Lemmatization and Compound Analysis), comparing two\nBidirectional Long Short-Term Memory with Auxiliary Loss\n(bi-LSTM-aux)  NN  models  to  a  baseline  Conditional\nRandom Fields (CRF) model. The annotated data used for\nthe experiments are derived from the National Centre for\nHuman Language Technology (NCHLT) text project. The\nresults suggest that NN architectures such as bi-LSTM-aux\nare  viable  for  NER  and  POS  tagging  tasks  for most  SA\nlanguages [1]. However, within the study by [1], NN did not\noutperform the CRF Machine Learning (ML) model. Rather\nthey  advised  further  studies  be  conducted  using  NN\ntransformer models on resource-scarce SA languages. For\nthis reason, this study builds upon the previous study, using\nthe XLM-R DL architecture. Therefore, the purpose of this\nstudy  is  to  evaluate  the  performance  of  the  NLP  NER\nsequential task using two XLM-R transformer  models. In\naddition, the experiment results are compared  to previous\nresearch findings.\nA. Research Questions\nRQ\n1\n – How does the XLM-R neural network transformer\nmodels  perform  with  NER  on  the  low-resourced  SA\nlanguages using annotated data?\nRQ\n2\n – How does the XLM-R transformer models compare\nto other neural network and machine learning models with\nNER on the low-resourced  SA languages using annotated\ndata?\nB. Paper Layout\nThe remainder of this paper comprises of the following\nsections: Sect. II provides information on the languages and\nEvaluation of Neural Network Transformer Models for Named-\nEntity Recognition on Low-Resourced Languages\nRidewaan Hanslo\nUniversity of Pretoria\nGauteng, South Africa\nEmail: ridewaan.hanslo@up.ac.za\nProceedings of the 16th Conference on Computer\nScience and Intelligence Systems pp. 115±119\nDOI: 10.15439/2021F7\nISSN 2300-5963 ACSIS, V ol. 25\nIEEE Catalog Number: CFP2185N-AR T ©2021, PTI 115\ndatasets; Sect. III presents the language model architecture.\nThe experiment settings are presented in Sect. IV and the\nresults and a discussion of the research findings are provided\nin  Sect.  V.  Section  VI  concludes  the  paper  with  the\nlimitations of this study and recommendations  for further\nresearch.\nII. L ANGUAGES  AND  D ATASETS\nAs mentioned by [1], SA is a country with at least 35\nspoken  languages.  Of  those  languages,  11  are  granted\nofficial status. The 11 languages can further be broken up\ninto  three  distinct  groups.  The  two  West-Germanic\nlanguages,  English  and  Afrikaans  (af).  Five  disjunctive\nlanguages,  Tshivenda  (ve),  Xitsonga  (ts),  Sesotho  (st),\nSepedi  (nso)  and  Setswana  (tn)  and  four  conjunctive\nlanguages, isiZulu (zu), isiXhosa (xh), isiNdebele (nr) and\nSiswati (ss). A key difference between SA disjunctive and\nconjunctive  languages  is  the  former  has  more  words  per\nsentence  than  the  latter.  Therefore,  disjunctive  languages\nhave a higher token count than conjunctive languages. For\nfurther details on conjunctive and disjunctive languages with\nexamples, see [1].\nThe datasets for the ten evaluated languages are available\nfrom  the  South  African  Centre  for  Digital  Language\nResources online repository (https://repo.sadilar.org/). These\nannotated datasets are part of the NCHLT Text Resource\nDevelopment  Project,  developed  by  the  Centre  for  Text\nTechnology (CTexT, North-West University, South Africa)\nwith  contributions  by  the  SA  Department  of  Arts  and\nCulture.  The annotated  data is  tokenized  into  five phrase\ntypes. These five phrase types are:\n1. ORG - Organization\n2. LOC - Location\n3. PER - Person\n4. MISC - Miscellaneous\n5. OUT - not considered part of any named-entity\nThe datasets consist of SA government domain corpora.\nTherefore, the SA government domain corpora are used to\ndo the experiments and comparisons. Eiselen [9] provides\nfurther details on the annotated corpora.\nIII. L ANGUAGE  M ODEL  A RCHITECTURE\nXLM-Roberta  (XLM-R)  is  a  transformer-based\nmultilingual  masked  language  model  [2].  This  language\nmodel  trained  on  100  languages  uses  2.5  TB  of\nCommonCrawl (CC) data [2]. From the 100 languages used\nby the XLM-R multilingual masked language model, it is\nnoted that Afrikaans (af) and isiXhosa (xh) are included in\nthe pre-training.\nThe benefit of this model, as indicated by [2] is, training\nthe XLM-R model on cleaned CC data increases the amount\nof  data  for  low-resource  languages.  Further,  because  the\nXLM-R  multilingual  model  is  pre-trained  on  many\nlanguages, low-resource languages improve in performance\ndue to positive transfer [2].\nConneau  et  al.  [2]  reports  the  state-of-the-art  XLM-R\nmodel performs better than other NN models such as M-\nBERT and XLM on question-answering, classification, and\nsequence labelling.\nTwo  transformer  models  are  used  for  NER  evaluation.\nThe XLM-R\nBase\n NN model and the XLM-R\nLarge\n NN model.\nThe XLM-R\nBase\n model has 12 layers, 768 hidden states, 12\nattention  heads,  250  thousand  vocabulary  size,  and  270\nmillion parameters.  The XLM-R\nLarge\n model has  24 layers,\n1024  hidden  states,  16  attention  heads,  250  thousand\nvocabulary size, and 550 million parameters [2]. Both pre-\ntrained  models  are  publicly  available  (https://bit.ly/xlm-\nrbase, https://bit.ly/xlm-rlarge).\nIV . E XPERIMENTAL  S ETTINGS\nThe experimental settings for the XLM-R\nBase  \nand XLM-\nR\nLarge\n models are described next, followed by the evaluation\nmetrics and the corpora descriptive statistics.\nA. XLM-R Settings\nThe training, validation, and test dataset split was 80%,\n10%, and 10%, respectively. Both pre-trained models were\nfine-tuned with the following experimental settings:\n1. Training epochs: 10\n2. Maximum sequence length: 128\n3. Learning rate: 0.00006\n4. Training batch size: 32\n5. Gradient accumulation steps: 4\n6. Dropout: 0.2\nB. Evaluation Metrics\nPrecision, Recall and F-score are evaluation metrics used\nfor text classification tasks, such as NER. These metrics are\nused  to  measure  the  model’s  performance  during  the\nexperiments. The formulas for these metrics leave out the\ncorrect  classification  of  true  negatives  ( tn )  and  false\nnegatives ( fn ), referred to as negative examples, with greater\nimportance placed on the correct classification of positive\nexamples such as true positives ( tp ) and false positives ( fp )\n[12]. For example, correctly classified spam emails ( tp ) are\nmore important  than  correctly  classified  non-spam  emails\n( tn ). In addition, multi-class classification was used for the\nresearch experiments to classify a token into a discrete class\nfrom  three  or more  classes.  The metric’s  macro-averages\nwere used for evaluation and comparison. Macro-averaging\n( M ) treats classes equally, while micro-averaging ( µ ) favors\nbigger classes [12]. Each evaluation metric and its formula\nas  described  by  [12]  are  listed  below.  ( M )  treats  classes\nequally,  while  micro-averaging  ( µ )  favors  bigger  classes\n[12]. Each evaluation metric and its formula as described by\n[12] are listed below.\nPrecision M : “ the number  of correctly classified positive\nexamples divided by the number of examples labeled by the\nsystem as positive”  (1).\n(1)\n116 PROCEEDINGS OF THE FEDCSIS. ONLINE, 2021\nRecall M : “ the number of correctly classified positive ex-\namples divided by the number of positive examples in the \ndata”  (2).\n(2)\nFscore M : “ a combination of the above”  (3).\n(3)\nC. Corpora Descriptive Statistics\nTable  I  provides  descriptive  statistics  for  the  language’s\ntraining data.\nV . R ESULTS  AND  D ISCUSSION\nA. Results\nTable II displays the precision scores of the two XLM-R\ntransformer models compared to models used by [1] and [9].\nThe Afrikaans (af) language has the highest precision score\nin this comparison, with 81.74% for the XLM-R\nLarge\n model.\nThe  XLM-R\nBase\n model  has  the  lowest  overall  score  of\n38.59% for the Sesotho (st) language. The CRF model has\nthe highest precision  scores  for  six of the ten  languages,\nincluding  the highest  average  score  of 75.64%.  The bold\nscores in Table II, III and IV show the highest evaluation\nmetric  score  for  each  language  and  the  model  with  the\nhighest average score.\nTable  III  displays  the  recall  scores  for  the  ten  low-\nresourced SA languages. As with the precision evaluation\nmetric, the Afrikaans  (af) language has  the highest  recall\nscore, with 87.07% for the XLM-R\nLarge\n model. The XLM-\nR\nBase\n model has the lowest recall score of 39.41% for the\nSesotho (st) language. The CRF and bi-LSTM-aux models\nhave the highest recall scores for three of the ten languages,\nrespectively,  with  the  latter  model  having  the  highest\naverage score of 72.48%.\nTable IV displays the F-score comparison. The Afrikaans\n(af) language produced the highest F-score, with an 84.25%\nfor the XLM-R\nLarge\n model. The XLM-R\nBase\n model has  the\nlowest F-score of 38.94% for the Sesotho (st) language. The\nCRF  model  has  the  highest  F-score  for  four  of  the  ten\nlanguages, including the highest average score of 73.22%.\nB.  Discussion\nThe two research questions are answered in this section.\nThe first question is on the transformer model’s performance\nusing  the  three-evaluation  metrics,  whereas  the  second\nquestion compares the transformer model’s performance to\nthe CRF and bi-LSTM models used in the previous SA NER\nstudies.\nRQ\n1\n – How does the XLM-R neural network transformer\nmodels  perform  with  NER  on  the  low-resourced  SA\nlanguages using annotated data?\nThe  XLM-R\nLarge\n and  XLM-R\nBase\n transformer  models\nproduced F-scores that ranged from 39% for the Sesotho (st)\nlanguage to 84% for the Afrikaans (af) language. Further,\nmany of the models  recall  scores  were  greater  than  70%\nwhereas  the  precision  scores  were  averaging  at  65%.\nRemember, in this instance, the recall metric emphasizes the\naverage  per-named-entity effectiveness  of the classifier to\nidentify  named-entities,  whereas,  the  precision  metric\ncompares  the  alignment  of  the  classifier’s  average  per-\nnamed-entities to the named-entities in the data. All F-scores\nwere  above  60% except  the Sesotho  language,  which  for\nboth XLM-R models were below 40%. The reason for the\nlow  F-scores  of  the  Sesotho  (st)  language  has  not  been\nidentified, however, it is posited that an investigation into\nusing different hyper-parameter tuning and dataset splits can\nproduce higher F-scores. Sesotho (st) is clearly the outlier\nduring  the  experiments.  For  instance,  the  Sesotho  (st)\nlanguage  exclusion  from  the  transformer  models  results\nmoves the average F-score from 67% to 71%. For the low-\nresourced SA languages, this is a notable improvement.\nRQ\n2\n – How does the XLM-R transformer models compare\nto other neural network and machine learning models with\nNER on the low-resourced  SA languages using annotated\ndata?\nThe  transformer  models  were  also  compared  to  the\nfindings of previous studies. In particular, [9] used a CRF\nML model to do NER sequence tagging on the ten resource-\nscarce SA languages. Further, [1] implemented bi-LSTM-\naux NN models, both with and without embeddings on the\nsame dataset. When analyzing the F-scores, the CRF model\nhas the highest F-scores for four of the ten languages, and\nthe bi-LSTM-aux models shared four of the highest F-scores\nequally (see Table IV). Meanwhile, the XML-R transformer\nmodels have two  of the highest F-scores  (see Table IV).\nAlthough, the transformer models were the only models to\nproduce F-scores  greater  than  80% for the Afrikaans  (af)\nT ABLE  I.\nT HE  TEN  LANGUAGES  TRAINING  DATA  DESCRIPTIVE  STATISTICS\nLanguage Writing System Tokens Phrase Types\nAfrikaans (af) Mixed 184 005 22 693\nisiNdebele (nr) Conjunctive 129 577 38 852\nisiXhosa (xh) Conjunctive 96 877 33 951\nisiZulu (zu) Conjunctive 161 497 50 114\nSepedi (nso) Disjunctive 161 161 17 646\nSesotho (st) Disjunctive 215 655 18 411\nSetswana (tn) Disjunctive 185 433 17 670\nSiswati (ss) Conjunctive 140 783 42 111\nTshivenda (ve) Disjunctive 188 399 15 947\nXitsonga (ts) Disjunctive 214 835 17 904\nRIDEW AAN HANSLO: EV ALUA TION OF NEURAL NETWORK TRANSFORMER MODELS FOR NAMED-ENTITY RECOGNITION 117\nT ABLE  II.\nT HE  PRECISION  % COMPARISON  BETWEEN  TRANSFORMER  MODELS  AND  PREVIOUS  SA LANGUAGE  NER STUDIES\nPrecision\nCRF* bi-LSTM-aux** bi-LSTM-aux emb** XLM-R\nBase\nXLM-R\nLarge\naf\n78.59% 73.61% 73.41% 79.15% 81.74%\nnr\n77.03% 78.58% n/a*** 74.06% 73.43%\nxh\n78.60% 69.83% 69.08% 64.94% 65.97%\nzu\n73.56% 72.43% 73.44% 71.10% 71.91%\nnso\n76.12% 75.91% 72.14% 77.23% n/a****\nst\n76.17% 53.29% 50.31% 38.59% 39.34%\ntn\n80.86% 74.14% 73.45% 67.09% 68.73%\nss\n69.03% 70.02% 69.93% 65.39% 65.99%\nve\n73.96% 67.97% 63.82% 58.85% 60.61%\nts\n72.48% 72.33% 71.03% 63.58% 63.58%\nAverage\n75.64% 70.81% 68.51% 65.99% 65.70%\n* As reported by [9]. ** As reported by [1]. *** No embeddings were available for isiNdebele. \n**** The model was unable to produce scores for Sepedi.\nT ABLE  III.\nT HE  RECALL  % COMPARISON  BETWEEN  TRANSFORMER  MODELS  AND  PREVIOUS  SA LANGUAGE  NER STUDIES\nRecall\nCRF* bi-LSTM-aux** bi-LSTM-aux emb** XLM-R\nBase\nXLM-R\nLarge\naf\n73.32% 78.23% 78.23% 86.16% 87.07%\nnr\n73.26% 79.20% n/a*** 78.51% 78.02%\nxh\n75.61% 73.30% 72.78% 63.53% 64.74%\nzu\n66.64% 72.64% 74.32% 74.23% 74.58%\nnso\n72.88% 79.66% 77.63% 80.59% n/a****\nst\n70.27% 55.56% 57.73% 39.41% 39.71%\ntn\n75.47% 77.42% 74.71% 73.39% 76.22%\nss\n60.17% 71.44% 72.82% 70.09% 70.97%\nve\n72.92% 65.91% 67.09% 63.24% 64.22%\nts\n69.46% 71.44% 71.25% 68.34% 69.40%\nAverage\n71.00% 72.48% 71.84% 69.74% 69.43%\n* As reported by [9]. ** As reported by [1]. *** No embeddings were available for isiNdebele. \n**** The model was unable to produce scores for Sepedi.\nT ABLE  IV.\nT HE  F- SCORE  % COMPARISON  BETWEEN  TRANSFORMER  MODELS  AND  PREVIOUS  SA LANGUAGE  NER STUDIES\nF-score\nCRF* bi-LSTM-aux** bi-LSTM-aux emb** XLM-R\nBase\nXLM-R\nLarge\naf\n75.86% 75.85% 75.74% 82.47% 84.25%\nnr\n75.10% 78.89% n/a*** 76.17% 75.60%\nxh\n77.08% 71.52% 70.88% 63.58% 64.68%\nzu\n69.93% 72.54% 73.87% 72.54% 73.17%\nnso\n74.46% 77.74% 74.79% 78.86% n/a****\nst\n73.09% 54.40% 53.77% 38.94% 39.48%\ntn\n78.06% 75.74% 74.07% 69.78% 71.91%\nss\n64.29% 70.72% 71.35% 67.57% 68.34%\nve\n73.43% 66.92% 65.41% 60.68% 61.99%\nts\n70.93% 71.88% 71.14% 65.57% 66.12%\nAverage\n73.22% 71.62% 70.11% 67.61% 67.28%\n* As reported by [9]. ** As reported by [1]. *** No embeddings were available for isiNdebele. \n**** The model was unable to produce scores for Sepedi.\n118 PROCEEDINGS OF THE FEDCSIS. ONLINE, 2021\nlanguage.  This  is  a  significant  improvement  for  NER\nresearch on the SA languages.\nThe  comparative  analysis  identified  the  Sesotho  (st)\nlanguage  as  the  lowest-performing  language  across  the\nstudies,  albeit  the  CRF  model  has  an  F-score  of  73%,\nmaking it an outlier. If the Sesotho (st) language is excluded\nfrom  the  evaluation,  then  the  metric  scores  for  the\ntransformer models begin to look much different.\nFor example, the highest average recall score of 72.48%\nby [1] belonged to the bi-LSTM-aux model, yet, the XLM-\nR\nLarge\n model, with Sesotho excluded, was able to produce an\naverage  recall  score  of  73.15%.  Similarly,  with  Sesotho\nexcluded, the average F-score and precision score were 71%\nand 69%, respectively, which are close to the high scores of\nthe previous studies.\nThis  study  reveals  that  the  NN  transformer  models\nperform fairly well on low-resource SA languages with NER\nsequence tagging, and Afrikaans (af) outperforms the other\nlanguages using these models. During the NN transformer\nmodel experiments, the disjunctive languages had a higher\ntoken  count,  while  conjunctive  languages  had  a  higher\nphrase type count (see Table I). However, there is no distinct\nperformance difference between individual disjunctive and\nconjunctive languages both during the XLM-R experiments\nand  when  compared  to  the  other  NN  and  ML  models.\nNonetheless,  except  for  the  CRF  model,  conjunctive\nlanguages  had  a  higher  F-score  average  than  disjunctive\nlanguages, even with the disjunctive Sesotho (st) language\nexcluded.\nThe Sesotho (st) language is a clear outlier in this study,\nwith the CRF baseline model F-score being 33% more than\nthe XLM-R models and 18% more than the bi-LSTM-aux\nmodels.  Interestingly,  while  both  the  isiXhosa  (xh)  and\nAfrikaans (af) languages were included in the pre-training of\nthe  XLM-R  model  (see  Section  III)  isiXhosa  (xh)\nunderperformed when compared to the CRF and bi-LSTM-\naux models. This finding suggests including a language in\nthe  XLM-R  model  pre-training  does  not  guarantee  good\nperformance  during  evaluation.  It  is  posited  that  the\nexperiment results could be improved upon. For instance,\nadditional fine-tuning of the hyper-parameters for each NN\nmodel  can  be  done  per  language,  given  the  available\nresources.  Further,  in  agreement  with  [9],  the  annotation\nquality  could  be  a  contributor  to  the  performance  of  the\nmodels.\nVI. L IMITATIONS  AND  F URTHER  R ESEARCH\nThe limitations of this research are the lack of resource\ncapacity  to  apply additional  hyperparameter  optimizations\non the transformer models per language. Additionally, the  \nnamed entities of the corpora would need to be investigated\nand  re-evaluated.  It  is  posited,  that  the  quality  of  the\nannotations could be improved upon, and the dataset could\nbe re-evaluated using an updated list of named entities.\nAdditional  research,  therefore,  could  implement  the\ntransformer models with discrete fine-tuning parameters per\nlanguage  to  produce  higher  F-scores.  In  addition,  the\ntransformer  models  could  be used  to evaluate  other NLP\nsequence  tagging and  sequence-to-sequence  tasks  such  as\nPOS tagging, Phrase chunking, and MT on the low-resource\nSA  languages.  Finally,  sequence  tagging  tasks  could  be\nevaluated  using  a  linear-complexity  recurrent  transformer\nvariant.\nR EFERENCES\n[1] M. Loubser, and M. J. Puttkammer, “Viability of neural net works for\ncore  technologies  for  resource-scarce  languages”.  Information ,\nSwitzerland, 2020. https://doi.org/10.3390/info11010041\n[2] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F .\nGuzmán,  E.  Grave,  M.  Ott,  L.  Zettlemoyer,  and  V.  Stoyanov,\nUnsupervised Cross-lingual Representation Learning at Scale , 2020.\nhttps://doi.org/10.18653/v1/2020.acl-main.747\n[3] B. Plank, A. Søgaard, and Y. Goldberg, “Multilingual part-of-spee ch\ntagging  with  bidirectional  long  short-term  memory  models  and\nauxiliary  loss”.  54th  Annual  Meeting  of  the  Association  for\nComputational  Linguistics,  ACL  2016  -  Short  Papers ,  2016.\nhttps://doi.org/10.18653/v1/p16-2067\n[4] G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and C.\nDyer,  “Neural  architectures  for  named  entity  recognition”.  2016\nConference of the North  American  Chapter  of the Association  for\nComputational Linguistics: Human Language Technologies, NAACL\nHLT  2016  -  Proceedings  of  the  Conference ,  2016.\nhttps://doi.org/10.18653/v1/n16-1030\n[5] J. Lafferty, A. McCallum, and C. N. F. Pereira, “Condit ional Random\nFields: Probabilistic Models for Segmenting and Labeling Sequenc e\nData”.  ICML  ’01:  Proceedings  of  the  Eighteenth  International\nConference  on  Machine  Learning ,  2001\nhttps://doi.org/10.29122/mipi.v11i1.2792\n[6] E.  D.  Liddy,  “Natural  Language  Processing.  In  Encyclopedia  of\nLibrary and Information Science”. In  Encyclopedia of Library and\nInformation Science , 2001.\n[7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. J ones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin,  “Attention is all you need ”.\nAdvances in Neural Information Processing Systems , 2017.\n[8] M. A. Hedderich, D. Adelani, D. Zhu, J. Alabi, U. Markus, and D.\nKlakow, Transfer Learning and Distant Supervision for Multilingual\nTransformer  Models:  A  Study  on  African  Languages ,  2020.\nhttps://doi.org/10.18653/v1/2020.emnlp-main.204\n[9] R. Eiselen, “Government domain named entity recognition for Sout h\nAfrican languages”. Proceedings of the 10th International Conference\non Language Resources and Evaluation, LREC 2016 , 2016.\n[10] T.  Pires,  E.  Schlinger,  and  D.  Garrette,  “How  multilingual  is\nmultilingual  BERT?”  ACL  2019  -  57th  Annual  Meeting  of  the\nAssociation  for  Computational  Linguistics,  Proceedings  of  the\nConference , 2020.  https://doi.org/10.18653/v1/p19-1493\n[11] A.  Conneau,  and  G.  Lample,  “Cross-lingual  language  model\npretraining”.  Advances  in  Neural  Information  Processing  Systems ,\n2019.\n[12] M. Sokolova, and G. Lapalme, “A systematic analysis of per formance\nmeasures  for  classification  tasks”.  Information  Processing  and\nManagement , 45 (4), 2009. https://doi.org/10.1016/j.ipm.2009.03.002\nRIDEW AAN HANSLO: EV ALUA TION OF NEURAL NETWORK TRANSFORMER MODELS FOR NAMED-ENTITY RECOGNITION 119",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7304226160049438
    },
    {
      "name": "Transformer",
      "score": 0.71528160572052
    },
    {
      "name": "Named-entity recognition",
      "score": 0.6109370589256287
    },
    {
      "name": "Artificial neural network",
      "score": 0.559581995010376
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5020267963409424
    },
    {
      "name": "Natural language processing",
      "score": 0.485825777053833
    },
    {
      "name": "Speech recognition",
      "score": 0.35627782344818115
    },
    {
      "name": "Engineering",
      "score": 0.12825322151184082
    },
    {
      "name": "Electrical engineering",
      "score": 0.08926030993461609
    },
    {
      "name": "Voltage",
      "score": 0.062167972326278687
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ]
}