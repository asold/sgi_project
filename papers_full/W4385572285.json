{
  "title": "Small Pre-trained Language Models Can be Fine-tuned as Large Models via Over-Parameterization",
  "url": "https://openalex.org/W4385572285",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2323364911",
      "name": "Ze-Feng Gao",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2102854352",
      "name": "Kun Zhou",
      "affiliations": [
        "Renmin University of China",
        "Beijing Institute of Big Data Research"
      ]
    },
    {
      "id": "https://openalex.org/A2103800083",
      "name": "Peiyu Liu",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2307999729",
      "name": "Wayne Xin Zhao",
      "affiliations": [
        "Renmin University of China",
        "Beijing Institute of Big Data Research"
      ]
    },
    {
      "id": "https://openalex.org/A3212238123",
      "name": "Ji-Rong Wen",
      "affiliations": [
        "Renmin University of China",
        "Beijing Institute of Big Data Research"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W204991446",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3005296017",
    "https://openalex.org/W3207779995",
    "https://openalex.org/W3015609966",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4302028573",
    "https://openalex.org/W2258054274",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4289436733",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W3176468671",
    "https://openalex.org/W4289293816",
    "https://openalex.org/W2045596450",
    "https://openalex.org/W3103754749",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3171463583",
    "https://openalex.org/W1798945469",
    "https://openalex.org/W3103454714",
    "https://openalex.org/W4296406182",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2899748887",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2805003733",
    "https://openalex.org/W3034230673",
    "https://openalex.org/W2707890836",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4298392601",
    "https://openalex.org/W3104263050",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W3104371525",
    "https://openalex.org/W2024165284",
    "https://openalex.org/W1963826206",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3126074026",
    "https://openalex.org/W3113412678",
    "https://openalex.org/W2404400936",
    "https://openalex.org/W4226375166",
    "https://openalex.org/W4283700140",
    "https://openalex.org/W1993482030",
    "https://openalex.org/W3034665240"
  ],
  "abstract": "By scaling the model size, large pre-trained language models (PLMs) have shown remarkable performance in various natural language processing tasks, mostly outperforming small PLMs by a large margin.However, due to the high computational cost, the huge number of parameters also restricts the applicability of large PLMs in real-world systems. In this paper, we focus on scaling up the parameters of PLMs only during fine-tuning, to benefit from the over-parameterization, while without increasing the inference latency. Given a relatively small PLM, we over-parameterize it by employing a matrix product operator, an efficient and almost lossless decomposition method to factorize its contained parameter matrices into a set of higher-dimensional tensors.Considering the efficiency, we further propose both static and dynamic strategies to select the most important parameter matrices for over-parameterization.Extensive experiments have demonstrated that our approach can significantly boost the fine-tuning performance of small PLMs and even help small PLMs outperform 3× parameterized larger ones.Our code is publicly available at https://github.com/zfgao66/OPF.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 3819–3834\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nSmall Pre-trained Language Models Can be Fine-tuned as Large Models\nvia Over-Parameterization\nZe-Feng Gao1 , Kun Zhou2,3 , Peiyu Liu1,3 , Wayne Xin Zhao1,3∗and Ji-Rong Wen1,2,3\n1Gaoling School of Artificial Intelligence, Renmin University of China\n2School of Information, Renmin University of China\n3Beijing Key Laboratory of Big Data Management and Analysis Methods\n{zfgao,francis_kun_zhou,liupeiyustu,jrwen}@ruc.edu.cn, batmanfly@gmail.com\nAbstract\nBy scaling the model size, large pre-trained\nlanguage models (PLMs) have shown remark-\nable performance in various natural language\nprocessing tasks, mostly outperforming small\nPLMs by a large margin. However, due to the\nhigh computational cost, the huge number of\nparameters also restricts the applicability of\nlarge PLMs in real-world systems. In this paper,\nwe focus on scaling up the parameters of PLMs\nonly during fine-tuning, to benefit from the\nover-parameterization, while without increas-\ning the inference latency . Given a relatively\nsmall PLM, we over-parameterize it by employ-\ning a matrix product operator, an efficient and\nalmost lossless decomposition method to factor-\nize its contained parameter matrices into a set\nof higher-dimensional tensors. Considering the\nefficiency, we further propose both static and\ndynamic strategies to select the most important\nparameter matrices for over-parameterization.\nExtensive experiments have demonstrated that\nour approach can significantly boost the fine-\ntuning performance of small PLMs and even\nhelp small PLMs outperform3×parameterized\nlarger ones. Our code is publicly available at\nhttps://github.com/zfgao66/OPF.\n1 Introduction\nDue to the remarkable performance, fine-\ntuning large-scale pre-trained language mod-\nels (PLMs) (Devlin et al., 2019; Liu et al., 2019;\nRaffel et al., 2020; Zhao et al., 2023) has become\nthe de facto method in the natural language pro-\ncess (NLP) field. With the help of large-scale\npre-trained data and parameters, these large-scale\nPLMs are able to process an extensive range\nof world knowledge (Roberts et al., 2020; Jiang\net al., 2020) and generalize well on a variety of\ntasks (Brown et al., 2020; Lester et al., 2021). Fol-\nlowing this direction, more data and more param-\neters have been widely explored in improving the\n∗Corresponding author.\nmodel capacity of PLMs in recent years (Raffel\net al., 2020; Chowdhery et al., 2022; Chen et al.,\n2022), leading to the expansion of PLM parameters\nfrom millions to billions.\nDespite the effectiveness, the huge model size\nalso limits the use of large PLMs in both research\nand industry communities. Specifically, the com-\nputational cost of pre-training and the efficiency\nof utilizing large PLMs are often unaffordable\nfor researchers as well as resource-limited appli-\ncations. Therefore, as a compromise, a number\nof studies (Gururangan et al., 2020; Chang et al.,\n2020; Zhang et al., 2020) focus on pre-training rel-\natively smaller language models (e.g., BERT-base-\nuncased) on the domain-specific or task-specific\ncorpus. However, small PLMs are not highly\nover-parameterized as larger models. As a re-\nsult, their generalization capabilities are relatively\nweaker (Brown et al., 2020), leading to a sub-\noptimal fine-tuning performance on downstream\ntasks.\nIn order to narrow the performance gap between\nsmall and large PLMs, this work seeks to over-\nparameterize small PLMs as large models during\nfine-tuning, in pursuit of improving their general-\nization capability. In the essential form, PLMs are\nbased on the Transformer architecture (Vaswani\net al., 2017), and most of their parameters are\nstored as matrices. According to the matrix de-\ncomposition techniques (Tucker, 1966; Henry and\nHofrichter, 1992; Oseledets, 2011) (e.g., Singular\nValue Decomposition), each matrix can be factor-\nized as the multiplication of a set of matrices. In\nthis way, the total number of parameters would\nbe enlarged during fine-tuning. While, after con-\nvergence, the factorized matrices can be merged\nto re-organize the parameter matrix of the small\nPLMs. Such a paradigm effectively harnesses the\nadvantages of over-parameterization, without in-\ncurring any additional inference latency for small\nPLMs during the fine-tuning process.\n3819\nAlthough it is promising to incorporate the\nmatrix decomposition to over-parameterize small\nPLMs, there are two major concerns remaining to\nbe investigated. First, the potential information\nloss caused by the matrix decomposition strategy\nshould be greatly reduced, since small computa-\ntion errors might be exponentially accumulated\nand propagated in the stacked multiple Transformer\nlayer of PLMs. Second, small PLMs are comprised\nof multiple parameter matrices, although they may\nnot always fulfill pivotal roles in fine-tuning var-\nious downstream tasks (V oita et al., 2019; Zhang\net al., 2022). Thus, it is costly and unnecessary to\nover-parameterize all of them during fine-tuning.\nTherefore, there is a need to select the proper ma-\ntrix decomposition method and leverage it to over-\nparameterize these selected (more important) pa-\nrameter matrices.\nTo address the above issues, we introduce the\ntechnique of matrix product operator (MPO) (Gao\net al., 2020) as the matrix decomposition strategy.\nMPO has been widely used in the quantum many-\nbody physics area, as it can efficiently factorize\nany matrix with arbitrary dimensions into a set of\nhigher-dimensional tensors with arbitrary scales,\nand the factorized tensors can reconstruct the origi-\nnal matrix in almost lossless condition (Liu et al.,\n2021a; Gao et al., 2022b). Such merits make MPO\na potentially good method for over-parameterizing\nsmall PLMs during fine-tuning. Based on MPO,\nwe also devise static and dynamic strategies to\nadaptively select important parameter matrices for\nover-parameterization. Specially, the static strat-\negy estimates the importance of each parameter\nmatrix based on the variation of the loss values\nafter removing it from a fine-tuned model (V oita\net al., 2019) and then over-parameterizes the top-N\nimportant ones. The dynamic strategy computes\nthe variation of gradients within several fine-tuning\nsteps, which is the approximation of the above loss\nvariation (Hou et al., 2020), which can dynamically\nguide the matrix over-parameterization process dur-\ning fine-tuning.\nTo this end, in this paper, we propose a general\nOver-Parameterization Framework, namely OPF\nto improve the fine-tuning performance of small\nPLMs. Given the pre-trained parameter matrices\nof a small PLM, we first utilize the static or dy-\nnamic strategies to select the most important ones\nand then over-parameterize them by the MPO de-\ncomposition. Such a framework does not affect the\nfine-tuning process, hence it is general to various\nsmall PLMs and NLP tasks. We conduct extensive\nexperiments on the GLUE benchmark (Wang et al.,\n2019), a widely-used natural language understand-\ning benchmark. Experimental results show that our\nOPF can boost the performance of small PLMs on\nGLUE significantly, e.g., improving BERT-base by\n+2.64 in average, improving T5-base by +2.41 in\naverage. Besides, our approach also helps small\nPLMs outperform 3 ×parameterized ones, e.g.,\nBERT-base+Ours (83.68)v.s. BERT-large (83.60)\nin average metrics on GLUE.\n2 Related Work\nPre-trained Language Models. Pre-trained lan-\nguage models (PLM) (Devlin et al., 2019; Liu et al.,\n2019; Zhao et al., 2023) have yielded state-of-the-\nart performance on a wide range of natural lan-\nguage processing tasks. Based on the Transformer\narchitecture (Vaswani et al., 2017), BERT (Devlin\net al., 2019) incorporated the “pre-training + fine-\ntuning” paradigm and has significantly improved\nthe performance on a variety of NLP benchmarks,\ne.g., GLUE (Wang et al., 2019). Following this way,\nthe T5 model (Raffel et al., 2020) and RoBERTa\nmodel (Liu et al., 2019) leveraged more data, more\nparameters and more pre-training steps, further im-\nproving the fine-tuning performance. Moreover,\nGPT-3 (Brown et al., 2020) showed that scaling\nup language models can greatly improve few-shot\nperformance. In our approach, we improve the per-\nformance of PLMs by just scaling up the model\nduring fine-tuning, which would not increase the\ninference latency.\nOver-parameterization in Neural Network.\nOver-parameterization has shown the superiority\non providing better model initialization (Arpit and\nBengio, 2019), improving model convergence (Du\net al., 2019; Allen-Zhu et al., 2019b; Gao et al.,\n2022a) and generalization (Allen-Zhu et al., 2019a).\nAfter the lottery theory hypothesis (Frankle and\nCarbin, 2019) was introduced, a surge of works\npointed out that over-parameterization might be\nhelpful to enhance the training efficiency (Malach\net al., 2020; Pensia et al., 2020) and improve\nthe model performance (Chen et al., 2020; Brix\net al., 2020; Prasanna et al., 2020). Among\nthem, Liu et al. (2021b) employed in-time over-\nparameterization to narrow the performance gap be-\ntween sparse and dense training. Our study aimed\nto use the over-parametrization strategy to better\n3820\ninspire the potentiality of PLMs, enhancing their\nfine-tuning performance.\nTensor Decomposition in Neural Network.\nTensor decomposition methods have been widely\napplied in a neural network for efficient training\nand inference, e.g., model compression (Gao et al.,\n2020; Sun et al., 2020; Gao et al., 2022c) and\nlightweight fine-tuning (Liu et al., 2021a). There\nare a surge of typical applications using the ten-\nsor decomposition methods on the parameter ma-\ntrices of deep models to compress the linear lay-\ners (Novikov et al., 2015) and convolutional ker-\nnels (Garipov et al., 2016). Besides, existing works\nalso apply the MPO method for the lightweight fine-\ntuning of ALBERT (Liu et al., 2021a) and the effi-\ncient expansion for the MoE framework (Gao et al.,\n2022b). Unlike existing methods, our approach fo-\ncuses on the property that tensor decomposition can\nbe used to map parameters from low-level spaces to\nhigh-dimensional spaces for over-parameterizing\nPLMs during fine-tuning, making PLMs benefit\nfrom more parameters.\n3 Preliminary\nTensor. A tensor Ti1,i2,...,im can be viewed as an\narray with m indices, where {i1,i2,...,i m}de-\nnotes the dimensions of the mindices, respectively.\nIn this way, a vector (i.e., v) and a matrix (i.e., W)\ncan be regarded as a 1-order tensor and 2-order\ntensor, respectively.\nTensor Product. Suppose {ψ1,...,ψ p} and\n{ϕ1,...,ϕ q}are the orthonormal basis of tensors\nT(1) and T(2), respectively. The tensor product\ncan be derived by contraction of T(1) and T(2),\ndenoted as ⊗. Formally, the tensor contraction of\nT(1) = ∑p\ni=1 aiψi1 and T(2) = ∑q\nj=1 bjϕi2 is\ndefined as follow:\nT(1) ⊗T(2) =\n{ p∑\ni=1\naiψi1\n}\n⊗\n\n\n\nq∑\nj=1\nbjϕi2\n\n\n\n=\np∑\ni=1\nq∑\nj=1\naibjψi1 ⊗ϕi2 . (1)\nTensor Decomposition Tensor decomposition\ncan be seen as the inverse operation of the ten-\nsor product. A widely-used way is the singular\nvalue decomposition (SVD) algorithm. Given a\nTensor T ∈Ri1×···×im, the mtimes SVD opera-\ntion can decompose this tensor into mlocal tensors\nTransformer Layer 1\nTransformer Layer N\n…\nFeed Forward \nNetwork                                  \n                                  \n2 ∈ ℝ3072×768=0.95\n=0.1\nUnselected Structure\n…\nMPO Decomposition\nSVD\n() () ()()\n…\nMulti-Head Attension\nEmbedding Layer\nPLM (e.g. BERT)\n1 ∈ ℝ768×3072\n\n\nReshape\n()\n ()\n ()\nReshape\nParameter Matrix Seleciton\nFigure 1: The overview of over-parameter fram-\nwork (OPF) in fine-tuning PLMs. IW denotes the es-\ntimated important score of parameter matrices. We\npresent an illustrative example of how a parameter ma-\ntrix W is selected for over-parameterization and is de-\ncomposed into a set of high-order tensors {T(k)}m\nk=1.\n{T(k)}m\nk=1. Conversely, the decomposed tensors\ncan also reconstruct the original tensor by sequen-\ntially performing the tensor product operator. The\ndetails of tensor decomposition are shown in Sup-\nplementary Materials A.1\n4 Approach\nIn this part, we describe our proposed over-\nparameterization framework for improving the fine-\ntuning performance of small PLMs. We first\ngive an overview of our approach, and then in-\ntroduce the details of matrix decomposition and\nover-parameterized matrices selection strategies.\n4.1 Overview\nExisting work mostly requires compressing a large\nPLM into a small one for benefiting from over-\nparameterization (Sun et al., 2019; Shen et al.,\n2020). Different from them, our approach can scale\nup the number of parameters of arbitrary small\nPLMs during fine-tuning without using large ones.\nTo achieve it, we leverage a matrix decomposi-\ntion method, to factorize the important parameter\nmatrices from the small PLM into a sequence of\nhigh-order tensors. These high-order tensors can\nbe utilized to reconstruct the original matrix, while\ngreatly increasing the number of trainable parame-\nters during fine-tuning. After reconstruction, in the\ninference phase, the number of parameters in the\nfine-tuned PLM will remain the same as the orig-\ninal one, without increasing the inference latency\nand model size.\nIn our proposed OPF, we incorporate an MPO-\n3821\nbased matrix decomposition strategy to scale up\nthe parameter matrices in PLMs and devise both\nstatic and dynamic selection strategies to determine\nimportant matrices for over-parameterization (Sec-\ntion 4.2). During fine-tuning, the static strategy\nfirst decides the important parameter matrices from\nthe PLM based on the variation of training loss\nafter removing each matrix and then relies on MPO\nto over-parameterize the selected top-N ones. The\ndynamic strategy computes the variation of gradi-\nents to estimate the importance of each matrix once\na few steps and dynamically selects important ma-\ntrices for over-parameterization (Section 4.3). The\noverview of our approach is presented in Figure 1.\nWe also present a detailed algorithm for our OPF\nin Algorithm 1.\n4.2 Over-parameterizing PLMs via Matrix\nProduct Operator\nTo make small PLMs benefit from the over-\nparameterization during fine-tuning, our approach\nemploys Matrix Product Operator (MPO), a matrix\ndecomposition technique to expand the number of\nmodel parameters. In this part, we first introduce\nthe details of the MPO method and then describe\nhow to adapt it for over-parameterizing PLMs.\nMatrix Product Operator. MPO is an efficient\nalgorithm that can factorize a parameter matrix\nW ∈RI×J into a sequential product of multiple\ntensors (Gao et al., 2020), denoted as:\nMPO(W) =T(1) ⊗···⊗T (m), (2)\nwhere {T(k)}m\nk=1 are the set of 4-order tensors\nwith size [dk−1,ik,jk,dk], in which ∏m\nk=1 ik = I,∏m\nk=1 jk = J, and dk is calculated by:\ndk = min(\nk∑\nl=1\nil ×jl,\nm∑\nl=k\nil ×jl). (3)\nGiven the parameter matrix W, the tensor sizes\n{dk}m\nk=1, {ik}m\nk=1 and {jk}m\nk=1, MPO can be re-\ngarded as a determined mapping process from W\nto multiple high-order tensors {T(k)}m\nk=1. Con-\ncretely, the MPO process consists of m-turn iter-\native matrix reshaping and SVD decomposition\noperations (Henry and Hofrichter, 1992), where\nthe parameter matrix will gradually shrink and the\ndecomposed tensor will be generated one by one.\nIn the k-th turn, given the output parameter ma-\ntrix Wk−1 from the last turn, we first reshape it\ninto a new matrix W\n′\nk−1 whose first dimension\nis dk−1 ×ik ×jk. Then, we perform the SVD\ndecomposition on it as:\nUλV⊤= SVD (W\n′\nk−1) (4)\nwhere U and V are complex unitary matrices, λis\na rectangular diagonal matrix with non-negative\nreal numbers on the diagonal. Following trun-\ncated SVD methods (Henry and Hofrichter, 1992;\nHansen et al., 1992), we extract the firstdk columns\nof U corresponding to the dk largest singular val-\nues to compose the decomposed tensor T(k), and\nreshape it to the size [dk−1,ik,jk,dk]. Besides, we\nadopt λV⊤as the output parameter matrix Wk for\nthe decomposition in the following turns. After m-\nturn iterations, we can obtain the decomposed mul-\ntiple high-order tensors {T(k)}m\nk=1, and the con-\ntraction of these tensors in order would reconstruct\nthe original parameter matrix W in almost lossless\ncondition (Gao et al., 2020) (See Algorithm 2 in\nAppendix A.1).\nOver-parameterizing PLMs. Based on the\nMPO method, we aim to expand the parame-\nter scale of small PLMs during fine-tuning, for\nbenefiting from over-parameterization. Gener-\nally, PLMs are based on the Transformer archi-\ntecture (Vaswani et al., 2017), consisting of an em-\nbedding layer, stacked multi-head attention layers,\nand feed-forward networks. These modules con-\ntain necessary parameter matrices that have been\npre-trained on large-scale corpus, e.g., the query\nprojection matrices in the multi-head attention lay-\ners. Therefore, we can utilize the MPO method to\ndecompose part of the parameter matrices into mul-\ntiple tensors as Eq. (2). After the MPO decomposi-\ntion, the parameter number of the matrix W would\nbe increased according to the values of {dk}m\nk=1,\n{ik}m\nk=1 and {jk}m\nk=1. The detailed added parame-\nter number Padd can be calculated as follows:\nPadd =\nm∑\nk=1\nikjkdk−1dk −\nm∏\nk=1\nikjk. (5)\nAccording to Eq. (8), {dk}m\nk=1 are determined by\n{ik; jk}m\nk=1. Hence we can adjust the values of\n{ik; jk}m\nk=1 to control the number of added param-\neters by the MPO decomposition strategy. There-\nfore, during fine-tuning, we can adopt MPO on\nseveral selected parameter matrices from the PLM\nto generate their corresponding multiple tensors.\nIn this way, we can scale up the total parameter\n3822\nthe number of the PLM. After fine-tuning the over-\nparameterized PLM to convergence, we will per-\nform tensor contraction on these decomposed ten-\nsors, to reconstruct the parameter matrices of the\nPLM. This new PLM owns the same parameter\nnumber and inference latency as the original one\nand has benefited from over-parameterization dur-\ning fine-tuning.\n4.3 Over-parameterized Matrices Selection\nDespite the efficiency and flexibility of the MPO\nmethod, it is still costly to utilize it for over-\nparameterizing all the parameter matrices in small\nPLMs. To concentrate the benefits of over-\nparameterization on the most important parame-\nters, we only select the most important parameter\nmatrices from PLMs for decomposition. In partic-\nular, we propose a static selection strategy as well\nas a dynamic selection strategy, which pre-selects\nthe important parameter matrices or dynamically\nchooses the ones during fine-tuning, respectively.\nStatic Selection Strategy. The proposed static\nselection strategy requires to pre-compute the\nimportance scores of all parameter matrices be-\nfore fine-tuning and then leverages MPO to over-\nparameterize the top- N ones. After that, the ar-\nchitecture of the over-parameterized PLM would\nbe static during fine-tuning. Inspired by network\npruning methods (Molchanov et al., 2017; V oita\net al., 2019), we utilize the change of the training\nloss LW after removing each parameter matrix W,\nto measure the importance scores since important\nparameters would play a key role to predict the\ncorrect label (V oita et al., 2019). Therefore, the\nimportance score IW of a parameter matrix W can\nbe computed as:\nIW = |LW −LW=0|, (6)\nwhere LW=0 denotes the value of loss after ze-\nroing W and the symbol ||represents the opera-\ntion of taking the absolute value. To calculate the\nloss, we need to fine-tune a small PLM using the\nsame pre-trained parameter as ours before. Gener-\nally, the parameter matrices from different modules\nof the PLM ( e.g., multi-head attention layer and\nfeed-forward network) may have different sizes and\nfunctions, making it inappropriate to directly com-\npare them. Thus, we first categorize all parameter\nmatrices by module, where each group contains\none module for Llayers. Then we pick the top-N\nones from each group for over-parameterization.\nAlgorithm 1Fine-tuning a PLM with our OPF.\nInput: Parameters matrices set of a PLM {W}.\n1: Divide {W}into several groups by module.\n2: if is Static Strategy then\n3: Fine-tuning the PLM until converged.\n4: Compute IW for {W}using Eq. (6).\n5: Sort {W}in each group according to IW.\n6: Perform MPO on the top- N matrices.\n7: Train the other PLM until converged.\n8: else\n9: Define S = {}\n10: while Len(S) <N do\n11: Train the PLM for tsteps.\n12: Compute IW for {W}using Eq. (7).\n13: Sort {W}in each group according to IW.\n14: Add top- nmatrices into S, and perform MPO.\n15: end while\n16: Continually train the PLM until converged.\n17: end if\nDynamic Selection Strategy Our proposed dy-\nnamic selection strategy aims to dynamically cal-\nculate the importance scores and choose the im-\nmediate important parameter matrices for over-\nparameterization during fine-tuning. Such a way\ncan dynamically capture the importance of change\nw.r.t.the optimization of the whole PLM. Follow-\ning Hou et al. (2020), we perform the first-order\nTaylor expansion on Eq. (6) to obtain the approxi-\nmation of the importance score as:\nIW = |LW−(LW−∂L\n∂W(W−0)+RW=0)|≈| ∂L\n∂WW|,\n(7)\nwhere once the remaining part RW=0 is omitted,\nthe important score can be estimated by the abso-\nlute values of the gradients of the parameter matrix.\nIn practice, we accumulate the absolute values of\nthe gradients for all the parameter matrices during\nfine-tuning. We dynamically calculate the impor-\ntance score using Eq. (7) and over-parameterize\nthe top-nparameter matrices from the categorized\ngroups once tsteps. The above process will be per-\nformed multiple times until N parameter matrices\nfrom each group have been selected.\n5 Experiments\nIn this section, we first set up the experiments, then\nreport the results and give a detailed analysis.\n5.1 Experimental Setup\nDatasets. To verify the effectiveness of our ap-\nproach, we conduct experiments on the Gen-\neral Language Understanding Evaluation (GLUE)\nbenchmark (Wang et al., 2019), which consists of 8\ndatasets (MNLI, QQP, QNLI, RTE, MRPC, CoLA,\n3823\nSST-2, STS-B) to systematically evaluate the abili-\nties of similarity matching, sentiment classification,\nlinguistic acceptability estimation and natural lan-\nguage inference. Since the labels of their original\ntest sets are not available, we randomly split their\noriginal validation sets in half, and use one half\nas the validation set and the other half as the test\nset. For the evaluation metrics, following existing\nworks (Gao et al., 2022b), we use Matthews corre-\nlation for CoLA, Spearman correlation for SST-B,\nF1 for MRPC, and accuracy for other tasks. We\nalso compute the average score across all tasks.\nBaseline Methods. We implement our approach\non the following PLMs, BERT (Devlin et al.,\n2019), T5 (Raffel et al., 2020) and BART (Lewis\net al., 2020). BERT is a widely-used PLM based\non the bidirectional Transformer architecture. We\nselect the publicly released BERT-small, BERT-\nmedium (Liu et al., 2021b), BERT-base and BERT-\nlarge (Devlin et al., 2019) for comparison. T5\nand BART adopt the sequence-to-sequence Trans-\nformer architecture, and we choose their base and\nlarge versions. Besides, we also compare our ap-\nproach with SVD (Henry and Hofrichter, 1992), a\nclassic matrix decomposition method that can also\nbe used for over-parameterizing PLMs. Concretely,\nwe leverage SVD to replace MPO in our frame-\nwork and perform over-parameterization on all the\nparameter matrices of the PLM during fine-tuning.\n5.2 Main Experimental Results\nIn this part, we report and analyze the experimental\nresults on BERT, T5 and BART.\nEvaluation on BERT. We present the results on\nBERT in Table 1. First, we can see that the BERT\nmodels with more parameters perform consistently\nbetter than smaller ones, i.e., BERT-large > BERT-\nbase > BERT-medium > BERT-small. It demon-\nstrates that more parameters are helpful for PLMs\nto achieve better performance, showing the effec-\ntiveness of over-parameterization. Second, after\ncombining PLMs with the over-parameterization\nmethods, their performances are most improved.\nAlthough these methods just increase the model pa-\nrameters during fine-tuning, they can also benefit\nfrom over-parameterization to improve the general-\nization capacity. Between the two matrix decom-\nposition methods, we observe that SVD mostly\nunderperforms MPO. As SVD just performs the\nmatrix decomposition once in the 2D space based\non the singular value, it is hard to greatly increase\nthe number of the model parameters as our ap-\nproach (e.g., 34M v.s. 81M in BERT-small). As\na comparison, MPO can factorize the matrix into\narbitrary scales by increasing the order, making it\nmore proper for over-parameterization.\nFinally, by comparing our approach with dif-\nferent matrix selection strategies, we can see that\nthe dynamic strategy mostly outperforms the static\none, under the setting of the same parameter scale.\nThe reason may be that the dynamic strategy can\nestimate the importance of immediate parameter\nmatrices w.r.t. the training steps. Such a way is\nable to adapt to the change of parameter impor-\ntance during fine-tuning, and better guides the over-\nparameterization. Surprisingly, by using our frame-\nwork with the dynamic strategy, the BERT-base\nmodel can be fine-tuned to achieve comparable\nperformance as the BERT-large model, where the\nnumber of its parameters is just increased into a\nsimilar scale during fine-tuning.\nEvaluation on T5 and BART.We show the re-\nsults on T5 and BART in Table 2. Similar to\nBERT, we can also see that the large models consis-\ntently outperform base models, and our proposed\nover-parameterization method narrows this perfor-\nmance gap. It indicates that our approach is general\nto different model architectures and pre-training\ntasks and can benefit from over-parameterization\nto improve the fine-tuning performance of differ-\nent PLMs. Besides, the performance of T5 is im-\nproved more than BART under a similar parameter-\nincreasing rate, and the over-parameterized T5-\nbase model also achieves comparable performance\nwith T5-large. A possible reason is that T5 has been\npre-trained using a much large corpus C4 (Raffel\net al., 2020), and over-parameterization can better\ninspire its potentiality during fine-tuning.\n5.3 Further Analysis\nNext, we continue to investigate our proposed ap-\nproach in a more detailed analysis.\nPerformance Comparisonw.r.t. Parameter In-\ncreasing Rate. During fine-tuning, our approach\ncan increase the number of model parameters\nfor improving the over-parameterization of PLMs.\nAs our approach is a general and flexible way\nto increase the model parameters into arbitrary\nscales, here we investigate how the performance\nchanges w.r.t. a different number of increased\nmodel parameters. Based on BERT-base and BERT-\nmedium, we expand their parameter scales after\n3824\nDatasets MNLI QNLI SST-2 RTE QQP CoLA STS-B MRPC Avg. #To (M) #To (M)\nAcc. Acc. Acc. Acc. Acc. Mcc. Spear. F1 Train Test\nBERT-small\nNone 77.60 86.40 89.70 61.80 87.00 27.80 77.00 83.40 73.84 28 28\n+OPF-SVD 77.73 86.06 89.04 62.31 88.10 27.90 79.31 83.25 74.21 34 28\n+OPF-MPOS 77.76 86.37 89.27 63.54 88.11 28.14 85.46 83.61 75.28 81 28\n+OPF-MPOD 77.75 86.10 89.77 63.55 88.99 28.19 86.27 83.91 75.57 81 28\nBERT-medium\nNone 80.00 87.70 89.60 62.20 87.90 38.00 78.40 86.60 76.30 41 41\n+OPF-SVD 80.77 87.50 89.68 62.45 89.35 39.16 79.61 87.35 76.98 46 41\n+OPF-MPOS 80.58 87.55 90.13 62.73 89.36 42.22 87.53 85.81 78.24 129 41\n+OPF-MPOD 80.61 88.24 90.37 62.82 89.84 44.56 87.89 86.08 78.90 129 41\nBERT-base\nNone 83.60 90.50 92.50 66.40 89.30 52.10 85.80 88.10 81.04 109 109\n+OPF-SVD 83.62 90.59 92.54 66.79 89.31 55.21 88.45 87.88 81.80 134 109\n+OPF-MPOS 83.78 90.87 92.55 68.87 89.30 56.12 88.53 88.40 82.30 341 109\n+OPF-MPOD 84.08 91.54 92.52 72.32 89.40 60.62 89.03 89.95 83.68 341 109\nBERT-large\nNone 85.70 92.70 93.90 70.10 90.10 60.50 86.50 89.30 83.60 335 335\n+OPF-SVD 85.33 91.78 93.22 71.48 90.12 56.82 88.04 88.74 83.19 410 335\n+OPF-MPOS 85.90 92.73 93.69 72.64 90.60 63.56 89.03 91.01 84.90 828 335\n+OPF-MPOD 85.96 92.85 93.82 72.94 90.69 62.63 89.63 91.08 84.95 828 335\nTable 1: Performance comparison using BERT on GLUE benchmark (in percent). “# To (M)-Train” and “#\nTo (M)-Test” denote the number (in millions) of total parameters during training and test, respectively. “+OPF-SVD”\nrepresents the use of SVD as the model over-parameterization method, whilst “+OPF-MPOS ” and “+OPF-MPOD ”\nsignify the use of MPO decomposition as the over-parameterization method with static and dynamic matrix selection\nstrategy, respectively. The best performance in each group is highlighted in bold. For all the results, we report the\nmean values of five runs using different random seeds.\n1 2 3 4\nScale Factor\n0.8\n0.85\n0.9\nSpear.\nBERT-base\nBERT-medium\n(a) Performance on STS-B.\n1 2 3 4\nScale Factor\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\nMcc.\nBERT-base\nBERT-medium (b) Performance on CoLA.\nFigure 2: Comparison of different scale factors of\nparameter number after over-parameterizing BERT-\nmedium and BERT-base in STS-B and CoLA tasks.\nover-parameterizing from 1×to 4×, reporting the\nperformance on STS-B and CoLA tasks. As shown\nin Figure 2, we can see that the model performance\nis consistently improving w.r.t. the increasing of\nparameter scales. Comparing the improved per-\nformance between BERT-base and BERT-medium,\nBERT-medium has gained more boost. It indicates\nthat a small PLM is much more hungry for more pa-\nrameters. Besides, after reaching the 4×parameter\nscale, the improvement becomes relatively smaller.\nIt shows that the 4×parameter scale seems to be\nthe limit that can significantly improve the model\nperformance via over-parameterization.\n3 6 9 12\nN\n0.66\n0.68\n0.7\n0.72\n0.74Acc.\nBERT-base\nBERT-base+ours\n(a) Performance w.r.tN.\n1 2 4 8\nn\n0.66\n0.68\n0.7\n0.72\n0.74Acc.\nBERT-base\nBERT-base+ours (b) Performance w.r.tn.\nFigure 3: Comparison of the different total parameter\nmatrices selection number N and the selection number\nnat one time in each parameter matrix. We conduct\nexperiments on RTE using BERT-base.\nHyper-parameters Tuning. For our OPF using\nthe dynamic strategy, the numbers of total selected\nparameter matrices N and the selection number\nat one time n in each parameter matrix group\nare important hyper-parameters that require tun-\ning. Larger N means that more parameter matrices\nare selected and over-parameterized and larger n\ndenotes that more matrices are over-parameterized\nat one time. To investigate the effect of their values\non the model performance, we conduct experiments\non the CoLA task using BERT-base as the back-\nbone. As shown in Figure 3, we can see that the\n3825\nDatasets MNLI QNLI SST-2 RTE QQP CoLA STS-B MRPC Avg. #To (M) #To (M)\nAcc. Acc. Acc. Acc. Acc. Mcc. Spear. F1 Train Test\nT5-Base\nNone 87.78 93.82 94.72 71.74 91.11 53.49 91.16 89.16 84.12 220 220\n+OPF-MPOS 87.95 93.27 92.88 74.64 89.89 62.72 91.21 90.76 85.42 663 220\n+OPF-MPOD 88.78 93.91 95.14 77.42 91.08 63.51 91.11 91.30 86.53 663 220\nT5-large\nNone 89.32 94.03 96.20 83.94 91.54 55.10 91.90 90.15 86.51 770 770\n+OPF-MPOS 88.15 93.98 96.21 83.98 89.88 66.38 91.91 92.38 87.86 1426 770\n+OPF-MPOD 88.91 94.11 96.05 84.12 91.67 66.51 91.85 92.41 88.20 1426 770\nBART-base\nNone 85.78 93.15 92.54 69.31 91.00 44.72 91.08 90.58 82.27 140 140\n+OPF-MPOS 85.84 93.62 93.58 67.57 91.16 45.78 91.07 90.32 82.36 418 140\n+OPF-MPOD 85.89 93.94 93.81 71.56 90.64 46.75 91.11 90.31 83.07 418 140\nBART-large\nNone 88.60 93.98 95.76 79.92 91.08 59.56 91.23 90.14 86.28 407 407\n+OPF-MPOS 88.75 94.21 95.18 79.81 90.67 61.69 91.15 90.16 86.45 1198 407\n+OPF-MPOD 89.09 94.12 95.35 82.31 91.16 62.55 91.08 91.31 87.12 1198 407\nTable 2: Performance comparison using T5 and BART on GLUE benchmark (in percent). “# To (M)-Train” and “#\nTo (M)-Test” denote the number (in millions) of total parameters during training and test, respectively. The best\nperformance in each group is highlighted in bold. For all the results, we report the mean values of five runs using\ndifferent random seeds.\nLearning Rate 5e-6 1e-5 3e-5 5e-5 1e-4\nRTE 71.08 72.24 72.12 72.31 70.25\nCoLA 59.86 60.44 60.54 60.61 59.31\nSTS-B 88.32 88.89 89.01 88.95 88.14\nTable 3: Comparison of different learning rates on RTE,\nCoLA and STS-B tasks using our approach on BERT-\nbase (in percent).\nperformance steadily improves as N increases and\neventually reaches a plateau as a result. The reason\nmay be that over-parameterizing too few matrices is\nnot able to sufficiently over-parameterize the PLM.\nBesides, we can see that too large nwould degrade\nthe performance. A possible reason is that too large\nnwill over-parameterize too many parameter ma-\ntrices at one time, causing the dynamic strategy\nto degrade into the static one. Whereas, we can\nsee that our approach consistently outperforms the\nbaseline method. It shows that our approach is not\nvery sensitive to the above hyper-parameters.\nSensitivity Analysis. As our approach is based\non the matrix decomposition method to over-\nparameterize the PLM, once a small error arises\nduring performing decomposition, it would accu-\nmulate into an extremely large value that may ruin\nthe PLM. To avoid it, our approach incorporates\nthe MPO method, which can factorize the param-\neter matrix in almost lossless conditions. Such\na way could stabilize the performance of our ap-\nproach and make it less sensitive to perturbation\non hyper-parameters. To validate it, we select a\ncommonly-used hyper-parameter, the learning rate\nto evaluate the sensitivity of our approach on RTE,\nCoLA and STS-B tasks using BERT-base, and re-\nport the performance change w.r.t.tuning it in the\nset {5e-6, 1e-5, 3e-5, 5e-5, 1e-4}in Table 3. We\ncan observe that the performance of our approach\nconsistently stables around certain values, i.e., 72.0\nfor RTE, 60.0 for CoLA, and 88.5 for STS-B. It\nindicates that our approach is not sensitive to the\nlearning rate during fine-tuning. Besides, setting\nthe learning rate to a commonly-used value 3e-\n5 is enough for our approach to achieving good\nperformance, no longer requiring time-consuming\nparameter tuning.\n6 Conclusion\nIn this paper, we proposed OPF, a novel over-\nparameterization framework to scale up the number\nof parameters for PLMs just during fine-tuning, for\nbenefiting from more parameters. In our OPF, we\nincorporated the matrix product operator method,\nwhich decomposes the parameter matrices in PLMs\ninto high-order tensors for increasing the param-\neter number, and also devised the static and dy-\nnamic strategies to select the most important param-\neter matrices for over-parameterization. Extensive\nexperiments have demonstrated that our OPF ap-\nproach can boost the performance of small PLMs\n3826\nsignificantly, and even help small PLMs outper-\nform big ones.\nIn future work, we will investigate more efficient\nand effective tensor decomposition methods for\nPLM over-parameterization. In addition, we will\nalso apply OPF to other important backbone mod-\nels in computer vision and multimodal domains.\nLimitations\nFurther research is needed to understand the ro-\nbustness of our over-parameterization framework\nproperly. The results given in this study are con-\nstrained by the natural language processing tasks\nand datasets used for evaluation. Even though we\nemploy standard classifications from the literature,\nthe choice of downstream tasks and datasets is still\nsubjective. Furthermore, due to computing limita-\ntions, we could not investigate the scaling behavior\nof the Large PLMs. Additional study is needed in\nthis area. In addition, as our approach is based on\nPLMs that may learn biased information from pre-\ntrained corpus, a potential risk is that our approach\nmay also be affected by it and generates improper\ntexts.\nAcknowledgments\nThis work was partially supported by National Nat-\nural Science Foundation of China under Grants\nNo. 62206299 and 62222215, Beijing Outstand-\ning Young Scientist Program under Grant No.\nBJJWZYJH012019100020098 and CCF-Zhipu AI\nLarge Model Fund. Xin Zhao is the corresponding\nauthor.\nReferences\nZeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang.\n2019a. Learning and generalization in overparam-\neterized neural networks, going beyond two layers.\nAdvances in neural information processing systems,\n32.\nZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. 2019b.\nA convergence theory for deep learning via over-\nparameterization. In International Conference on\nMachine Learning, pages 242–252. PMLR.\nDevansh Arpit and Yoshua Bengio. 2019. The benefits\nof over-parameterization at initialization in deep relu\nnetworks. arXiv preprint arXiv:1901.03611.\nChristopher Brix, Parnia Bahar, and Hermann Ney. 2020.\nSuccessfully applying the stabilized lottery ticket hy-\npothesis to the transformer architecture. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 3909–3915, On-\nline. Association for Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nWei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yim-\ning Yang, and Sanjiv Kumar. 2020. Pre-training tasks\nfor embedding-based large-scale retrieval. In 8th\nInternational Conference on Learning Representa-\ntions, ICLR 2020, Addis Ababa, Ethiopia, April 26-\n30, 2020. OpenReview.net.\nTianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia\nLiu, Yang Zhang, Zhangyang Wang, and Michael\nCarbin. 2020. The lottery ticket hypothesis for pre-\ntrained bert networks. Advances in neural informa-\ntion processing systems, 33:15834–15846.\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Pier-\ngiovanni, Piotr Padlewski, Daniel Salz, Sebastian\nGoodman, Adam Grycner, Basil Mustafa, Lucas\nBeyer, et al. 2022. Pali: A jointly-scaled mul-\ntilingual language-image model. arXiv preprint\narXiv:2209.06794.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nAndrzej Cichocki, Rafal Zdunek, Anh Huy Phan, and\nShun-ichi Amari. 2009. Nonnegative Matrix and\nTensor Factorizations - Applications to Exploratory\nMulti-way Data Analysis and Blind Source Separa-\ntion. Wiley.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nSimon S. Du, Xiyu Zhai, Barnabás Póczos, and Aarti\nSingh. 2019. Gradient descent provably optimizes\nover-parameterized neural networks. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nJonathan Frankle and Michael Carbin. 2019. The lottery\nticket hypothesis: Finding sparse, trainable neural\nnetworks. In 7th International Conference on Learn-\ning Representations, ICLR 2019, New Orleans, LA,\nUSA, May 6-9, 2019. OpenReview.net.\n3827\nTianxiang Gao, Hailiang Liu, Jia Liu, Hridesh Ra-\njan, and Hongyang Gao. 2022a. A global conver-\ngence theory for deep relu implicit networks via\nover-parameterization. In The Tenth International\nConference on Learning Representations, ICLR 2022,\nVirtual Event, April 25-29, 2022. OpenReview.net.\nZe-Feng Gao, Song Cheng, Rong-Qiang He, Zhi-Yuan\nXie, Hui-Hai Zhao, Zhong-Yi Lu, and Tao Xiang.\n2020. Compressing deep neural networks by ma-\ntrix product operators. Physical Review Research,\n2(2):023300.\nZe-Feng Gao, Peiyu Liu, Wayne Xin Zhao, Zhong-Yi\nLu, and Ji-Rong Wen. 2022b. Parameter-efficient\nmixture-of-experts architecture for pre-trained lan-\nguage models. In Proceedings of the 29th Inter-\nnational Conference on Computational Linguistics,\npages 3263–3273, Gyeongju, Republic of Korea. In-\nternational Committee on Computational Linguistics.\nZe-Feng Gao, Xingwei Sun, Lan Gao, Junfeng Li, and\nZhong-Yi Lu. 2022c. Compressing lstm networks by\nmatrix product operators.\nTimur Garipov, Dmitry Podoprikhin, Alexander\nNovikov, and Dmitry Vetrov. 2016. Ultimate ten-\nsorization: compressing convolutional and fc layers\nalike. arXiv preprint arXiv:1611.03214.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360.\nPer Christian Hansen, Takashi Sekii, and Hiromoto\nShibahashi. 1992. The modified truncated svd\nmethod for regularization in general form. SIAM\nJournal on Scientific and Statistical Computing ,\n13(5):1142–1150.\nER Henry and J Hofrichter. 1992. [8] singular value de-\ncomposition: Application to analysis of experimental\ndata. Methods in enzymology, 210:129–192.\nFrank L Hitchcock. 1927. The expression of a tensor or\na polyadic as a sum of products. Journal of Mathe-\nmatics and Physics, 6(1-4):164–189.\nLu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao\nChen, and Qun Liu. 2020. Dynabert: Dynamic BERT\nwith adaptive width and depth. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nTamara G Kolda and Brett W Bader. 2009. Ten-\nsor decompositions and applications. SIAM review,\n51(3):455–500.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, pages 7871–7880.\nAssociation for Computational Linguistics.\nPeiyu Liu, Ze-Feng Gao, Wayne Xin Zhao, Zhi-Yuan\nXie, Zhong-Yi Lu, and Ji-Rong Wen. 2021a. En-\nabling lightweight fine-tuning for pre-trained lan-\nguage model compression based on matrix product\noperators. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguis-\ntics and the 11th International Joint Conference on\nNatural Language Processing, ACL/IJCNLP 2021,\n(Volume 1: Long Papers), Virtual Event, August 1-6,\n2021, pages 5388–5398. Association for Computa-\ntional Linguistics.\nShiwei Liu, Lu Yin, Decebal Constantin Mocanu,\nand Mykola Pechenizkiy. 2021b. Do we actually\nneed dense over-parameterization? in-time over-\nparameterization in sparse training. In International\nConference on Machine Learning, pages 6989–7000.\nPMLR.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nEran Malach, Gilad Yehudai, Shai Shalev-Schwartz,\nand Ohad Shamir. 2020. Proving the lottery ticket\nhypothesis: Pruning is all you need. In International\nConference on Machine Learning, pages 6682–6691.\nPMLR.\nPavlo Molchanov, Stephen Tyree, Tero Karras, Timo\nAila, and Jan Kautz. 2017. Pruning convolutional\nneural networks for resource efficient inference. In\n5th International Conference on Learning Repre-\nsentations, ICLR 2017, Toulon, France, April 24-\n26, 2017, Conference Track Proceedings. OpenRe-\nview.net.\nAlexander Novikov, Dmitry Podoprikhin, Anton Os-\nokin, and Dmitry P. Vetrov. 2015. Tensorizing neural\nnetworks. In Advances in Neural Information Pro-\ncessing Systems 28: Annual Conference on Neural In-\nformation Processing Systems 2015, December 7-12,\n2015, Montreal, Quebec, Canada, pages 442–450.\nIvan V Oseledets. 2011. Tensor-train decomposition.\nSIAM Journal on Scientific Computing, 33(5):2295–\n2317.\n3828\nAnkit Pensia, Shashank Rajput, Alliot Nagle, Harit Vish-\nwakarma, and Dimitris Papailiopoulos. 2020. Opti-\nmal lottery tickets via subset sum: Logarithmic over-\nparameterization is sufficient. Advances in Neural\nInformation Processing Systems, 33:2599–2610.\nSai Prasanna, Anna Rogers, and Anna Rumshisky. 2020.\nWhen BERT plays the lottery, all tickets are winning.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020, pages 3208–\n3229. Association for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W Mahoney, and Kurt\nKeutzer. 2020. Q-bert: Hessian based ultra low\nprecision quantization of bert. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 34, pages 8815–8821.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for bert model com-\npression. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4323–4332.\nXingwei Sun, Ze-Feng Gao, Zhong-Yi Lu, Junfeng Li,\nand Yonghong Yan. 2020. A model compression\nmethod with matrix product operators for speech\nenhancement. IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, 28:2837–2847.\nLedyard R Tucker. 1966. Some mathematical notes\non three-mode factor analysis. Psychometrika,\n31(3):279–311.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nOn the importance of pre-training compact models.\narXiv preprint arXiv:1908.08962.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lifting,\nthe rest can be pruned. In Proceedings of the 57th\nConference of the Association for Computational Lin-\nguistics, ACL 2019, Florence, Italy, July 28- August\n2, 2019, Volume 1: Long Papers, pages 5797–5808.\nAssociation for Computational Linguistics.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th In-\nternational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nQingru Zhang, Simiao Zuo, Chen Liang, Alexander\nBukharin, Pengcheng He, Weizhu Chen, and Tuo\nZhao. 2022. Platon: Pruning large transformer mod-\nels with upper confidence bound of weight impor-\ntance. In International Conference on Machine\nLearning, pages 26809–26823. PMLR.\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Wein-\nberger, and Yoav Artzi. 2021. Revisiting few-sample\nBERT fine-tuning. In 9th International Conference\non Learning Representations, ICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021. OpenReview.net.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and William B Dolan. 2020. Dialogpt: Large-\nscale generative pre-training for conversational re-\nsponse generation. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics: System Demonstrations, pages 270–278.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.\nA survey of large language models. arXiv preprint\narXiv:2303.18223.\n3829\nAppendices\nA More Details about Tensors\nA.1 Tensor and Matrix Product Operator\nAs introduced in (Cichocki et al., 2009), a tensor\ncan be defined as follows.\nTensor. Let D1,D2...,DM ∈M denote the in-\ndex upper bounds. A tensor T ∈ RD1,...,DM\nof order M is an M-way array where ele-\nments T[d1,d2,...,d M ] are indexed by dm ∈\n{1,2,...,D M }for 1 ≤m≤M.\nMatrix Product Operator. The k-th or-\nder (Kolda and Bader, 2009) and k∈{1,...,D }.\nThe bond dimension dk is defined by:\ndk = min\n( k∏\nm=1\nim ×jm,\nn∏\nm=k+1\nim ×jm\n)\n. (8)\nFrom Eq. (8), we can see that is going to be large\nin the middle and small on both sides. Algorithm 2\npresents a thorough algorithm for MPO decompo-\nsition.\nAlgorithm 2MPO decomposition procedure.\nInput: matrix W ∈RI×J, the number of local tensor m\nOutput : local tensor set {T(k)}m\nk=1\n1: for k= 1,...,m −1 do\n2: W[dk−1 ×ik ×jk,−1] ←Reshape(W[I,J])\n3: UλV⊤←SVD (W)\n4: T(k)[dk−1,ik,jk,dk] ←Reshape(U)\n5: Calculate W = λV⊤\n6: end for\n7: Let T(m) ←W\n8: Normalization\n9: return local tensor set{T(k)}m\nk=1\nThe MPO representation of W is obtained by\nfactorizing it into a sequential product of local ten-\nsors. The algorithm has been depicted in Section\n4.2 of the main text. With the MPO decomposition\ntechnique, we can get local tensor as follows:\nWi1···in,j1···jn = T(1)[i1,j1] ···T (m)[im,jm] (9)\nwhere T(k)[jk,ik] is a Dk−1 ×Dk matrix with\nDk the virtual basis dimension on the bond link-\ning T(k) and T(k+1) with D0 = Dm = 1. With\nEq. (9) we can decompose an original matrix W to\na sequential product of the derived local tensors.\nA.2 Theorem\nTheorem 1.Suppose that the tensorW(k) of matrix\nW that is satisfy\nW = W(k) + E(k),D(W(k)) =dk,\nwhere ||E(k)||2\nF = ϵ2\nk,k = 1,...,d −1. (10)\nThen MPO(W) with the k-th bond dimension dk\nupper bound of truncation error satisfy:\n||W −MPO (W)||F ≤\n√\nd−1∑\nk=1\nϵ2\nk (11)\nProof. The proof is by induction. For n = 2\nthe statement follows from the properties of the\nSVD. Consider an arbitrary n> 2. Then the first\nunfolding W(1) is decomposed as\nW(1) = U1λ1V1 +E(1) = U1B(1) +E(1), (12)\nwhere U1 is of size r1 ×i1 ×j1 and ||E(1)||2\nF =\nϵ2\n1. The matrix B1 is naturally associated with\na (n−1)-dimensional tensor B(1) with elements\nB(1)(α,i2,j2,...,i n,jn), which will be decom-\nposed further. This means that B1 will be approx-\nimated by some other matrix ˆB1. From the prop-\nerties of the SVD it follows that UT\n1 E(1) = 0, and\nthus\n||W −B(1)||2\nF\n= ||W1 −U1 ˆB1||2\nF\n= ||W1 −U1( ˆB1 + B1 −B1)||2\nF\n= ||W1 −U1B1||2\nF + ||U1( ˆB1 −B1)||2\nF (13)\nand since U1 has orthonormal columns,\n||W −B(1)||2\nF ≤ϵ2\n1 + ||B1 − ˆB1||2\nF . (14)\nand thus it is not difficult to see from the orthonor-\nmality of columns of U1 that the distance of the\nk-th unfolding (k = 2,...,d k −1) of the (d−1)-\ndimensional tensor B(1) to the dk-th rank matrix\ncannot be larger than ϵk. Proceeding by induction,\nwe have\n||B1 − ˆB1||2\nF ≤\nd−1∑\nk=2\nϵ2\nk, (15)\ncombine with Eq. (14), this complets the proof.\n3830\nExperiments N n Feed-forward Network Multi-head Attention LR\nBERT-small\nOPF+SVD - - T32,32\n32,16 (D) T16,32\n32,16 (D) 3e-5\nOPF+MPOS 4 4 T32,1,1,1,1,1,1,1,1,1,32\n32,1,1,1,1,1,1,1,1,1,16 (D) T16,1,1,1,1,1,1,1,1,1,32\n32,1,1,1,1,1,1,1,1,1,16 (D) 3e-5\nOPF+MPOD 4 2 T32,1,1,1,1,1,1,1,1,1,32\n32,1,1,1,1,1,1,1,1,1,16 (D) T16,1,1,1,1,1,1,1,1,1,32\n32,1,1,1,1,1,1,1,1,1,16 (D) 3e-5\nBERT-medium\nOPF+SVD - - T32,32\n32,16 (D) T16,32\n32,16 (D) 3e-5\nOPF+MPOS 12 12 T32,1,1,1,1,1,1,1,1,1,32\n32,1,1,1,1,1,1,1,1,1,16 (D) T16,1,1,1,1,1,1,1,1,1,32\n32,1,1,1,1,1,1,1,1,1,16 (D) 3e-5\nOPF+MPOD 12 2 T32,1,1,1,1,1,1,1,1,1,32\n32,1,1,1,1,1,1,1,1,1,16 (D) T16,1,1,1,1,1,1,1,1,1,32\n32,1,1,1,1,1,1,1,1,1,16 (D) 3e-5\nBERT-base\nOPF+SVD - - T32,24\n64,48 (D) T24,32\n32,24 (D) 3e-5\nOPF+MPOS 8 8 T32,1,1,1,1,1,1,1,1,1,24\n64,1,1,1,1,1,1,1,1,1,48 (D) T32,1,1,1,1,1,1,1,1,1,24\n64,1,1,1,1,1,1,1,1,1,48 (D) 3e-5\nOPF+MPOD 8 2 T32,1,1,1,1,1,1,1,1,1,24\n64,1,1,1,1,1,1,1,1,1,48 (D) T32,1,1,1,1,1,1,1,1,1,24\n64,1,1,1,1,1,1,1,1,1,48 (D) 3e-5\nBERT-large\nOPF+SVD - - T32,32\n64,64 (D) T32,32\n32,32 (D) 3e-5\nOPF+MPOS 24 24 T32,1,1,1,1,1,1,1,1,1,32\n64,1,1,1,1,1,1,1,1,1,64 (D) T32,1,1,1,1,1,1,1,1,1,32\n64,1,1,1,1,1,1,1,1,1,64 (D) 3e-5\nOPF+MPOD 24 4 T32,1,1,1,1,1,1,1,1,1,32\n64,1,1,1,1,1,1,1,1,1,64 (D) T32,1,1,1,1,1,1,1,1,1,32\n64,1,1,1,1,1,1,1,1,1,64 (D) 3e-5\nTable 4: The experiments setting in various BERT models.“LR” denote the learning rate.\nB Additional Experimental Details\nExperimental Details in Pre-trained Language\nModeling In this paper, the MPO decomposition\nis proposed for enlarging model parameters. In\norder to show the process of incorporating several\nMPO structures into BERT, T5, and BART respec-\ntively. Moreover, we denote an MPO, defined by\nEq. (2), as:\nTj1,j2,...,jm\ni1,i2,...,im (D) (16)\nBERT-small and BERT-medium models were re-\nleased by Google Research in (Turc et al., 2019),\nwhich have 4 and 8 Transformer layers respectively,\nand each of the layers has 512 hidden nodes. We\nperform the pre-compute stage, which would com-\npute the importance scores of all parameter matri-\nces before fine-tuning and then leverages MPO to\nover-parameterize the top-N ones. The important\nscore can be computed by Eq. (6) and Eq. (7). In\nour main experiments, the detail hyper-parameter\nsetting was shown in Table 4.\nWe have conducted sufficient trials for both the\nT5 and the BART models using the following par-\nticular experimental parameter configurations in\nTable 5.\nHardware We trained our model on one machine\nwith 8 NVIDIA V100 GPUs. For our base mod-\nels, we adopt all these models released by Hug-\ngingface 1. The hyperparameters are described in\nTable 6.\nOptimizer We used the Adam optimizer and vary\nthe learning rate over the course of training. The\nvary formula (Vaswani et al., 2017) follows in our\nwork. We also used the warmup_steps= 1000.\nDetails of Fine-tuning DatasetsGLUE bench-\nmark covers multiple datasets (MNLI, QNLI, SST-\n2, RTE, QQP, CoLA, STS-B, MRPC)2.\nDetails of Evaluation Metrics Following Gao\net al. (2022b), the metrics that we use for the\nGLUE benchmark are Matthew’s correlation for\nCoLA (Mcc.), Spearman for STS-B (Spear.), F1\nfor MRPC, and accuracy (Acc.) for the remaining\ntasks. We compute and report the average scores\nfor all of the aforementioned metrics based on all of\nthe test samples taken into consideration. Since the\noriginal test sets are not accessible, we divide the\noriginal validation set in half and use one half for\nvalidation and the other for the test for datasets with\nfewer than 10,000 samples (RTE, MRPC, STS-B,\nCoLA) (Zhang et al., 2021).\n1https://huggingface.co/\n2In line with Raffel et al. (2020), we do not test WNLI due\nto its adversarial character with respect to the training set.\n3831\nExperiments N n Feed-forward Network Multi-head Attention LR\nT5-base\nOPF+MPOS 8 8 T32,1,1,1,1,1,1,1,1,1,24\n64,1,1,1,1,1,1,1,1,1,48 (D) T32,1,1,1,1,1,1,1,1,1,24\n64,1,1,1,1,1,1,1,1,1,48 (D) 3e-5\nOPF+MPOD 8 2 T32,1,1,1,1,1,1,1,1,1,24\n64,1,1,1,1,1,1,1,1,1,48 (D) T32,1,1,1,1,1,1,1,1,1,24\n64,1,1,1,1,1,1,1,1,1,48 (D) 3e-5\nT5-large\nOPF+MPOS 16 16 T32,1,1,1,1,1,1,1,1,1,32\n64,1,1,1,1,1,1,1,1,1,64 (D) T32,1,1,1,1,1,1,1,1,1,32\n64,1,1,1,1,1,1,1,1,1,64 (D) 3e-5\nOPF+MPOD 16 4 T32,1,1,1,1,1,1,1,1,1,32\n64,1,1,1,1,1,1,1,1,1,64 (D) T32,1,1,1,1,1,1,1,1,1,32\n64,1,1,1,1,1,1,1,1,1,64 (D) 3e-5\nBART-base\nOPF+MPOS 8 8 T32,1,1,1,1,1,1,1,1,1,24\n64,1,1,1,1,1,1,1,1,1,48 (D) T32,1,1,1,1,1,1,1,1,1,24\n64,1,1,1,1,1,1,1,1,1,48 (D) 3e-5\nOPF+MPOD 8 2 T32,1,1,1,1,1,1,1,1,1,24\n64,1,1,1,1,1,1,1,1,1,48 (D) T32,1,1,1,1,1,1,1,1,1,24\n64,1,1,1,1,1,1,1,1,1,48 (D) 3e-5\nBART-large\nOPF+MPOS 16 16 T32,1,1,1,1,1,1,1,1,1,32\n64,1,1,1,1,1,1,1,1,1,64 (D) T32,1,1,1,1,1,1,1,1,1,32\n64,1,1,1,1,1,1,1,1,1,64 (D) 3e-5\nOPF+MPOD 16 4 T32,1,1,1,1,1,1,1,1,1,32\n64,1,1,1,1,1,1,1,1,1,64 (D) T32,1,1,1,1,1,1,1,1,1,32\n64,1,1,1,1,1,1,1,1,1,64 (D) 3e-5\nTable 5: The experiments setting in T5 and BART models.“LR” denotes the learning rate.\nModel dhead dff L N sl\nBERT-small 512 2048 4 128\nBERT-medium 512 2048 8 128\nBERT-base 768 3072 12 128\nBERT-large 1024 4096 24 128\nT5-base 768 3072 12 128\nT5-large 1024 4096 24 128\nBART-base 768 3072 12 128\nBART-large 1024 4096 24 128\nTable 6: The hyperparameter in experiments of the main\ntext mentioned models. “L” denotes the number of\nTransformer layers. “Nsl” denotes the sequence length.\nC Additional Discussion\nDifferent Tensor Decompostion In the field of\nmathematics, the MPO-based approximation may\nbe seen as an alternative form of the low-rank ap-\nproximation approach. Now we will evaluate it in\nlight of many other low-rank approximation tech-\nniques, such as SVD (Henry and Hofrichter, 1992),\nCPD (Hitchcock, 1927), and Tucker decomposi-\ntion (Tucker, 1966).\nWe present the categorization of these methods\nin Table 7. Because the work of low-rank decom-\nposition only needs to be done once, and it does\nnot take a long time, thus we mainly focus on the\nforward propagation time in practical use. In point\nof fact, each of the techniques may either be based\non a tensor-based decomposition (that is, a list of\ntensors for factorization) or a matrix-based decom-\nCategory Method Inference Time\nMPO MPO (m> 2) O(mID 3 )\nMPO (m=2) (SVD) O(2ID3 )\nTucker Tucker (D> 1) O(mID + Dm )\nTucker (D=1) (CP) O(mID 2 )\nTable 7: The amount of time and complexity that various\nlow-rank approximation algorithms need for inference.\nHere, mdenotes the number of the tensors, I denotes\nmax({ik}m\nk=1) means the largest ik in input list, and D\ndenotes max({D′\nk}m\nk=0) means the largest dimension\nD′\nk in the truncated dimension list.\nposition, and we quantify the amount of time each\napproach requires using standard parameters. In-\ndeed, MPO and Tucker are examples of two differ-\nent classes of low-rank approximation algorithms.\nIn most cases, the capacity of the algorithm will\nrise in proportion to the value of m(more tensors).\nWhen mis more than three, the temporal complex-\nity of MPO is lower than that of Tucker decompo-\nsition. It is clear that SVD may be thought of as\na special example of MPO when the dimension of\nthe tensor is equal to two, and that CPD is a partic-\nular case of Tucker when the super-diagonal matrix\nis the core tensor. Both of these relationships can\nbe observed here.\n3832\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection-Limitations\n□\u0013 A2. Did you discuss any potential risks of your work?\nSection-Limitations\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection-Abstract; Section 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 5.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 5.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNot applicable. Left blank.\nC □\u0013 Did you run computational experiments?\nSection-Experiments( Section 5)\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection-Experiments( Section 5)\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n3833\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection-Experiments( Section 5.1); Appendix B\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 5.2, we report the mean of 5 runs using different random seeds.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n3834",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7927954196929932
    },
    {
      "name": "Parameterized complexity",
      "score": 0.72356116771698
    },
    {
      "name": "Scaling",
      "score": 0.5615231394767761
    },
    {
      "name": "Inference",
      "score": 0.5452665686607361
    },
    {
      "name": "Matrix decomposition",
      "score": 0.540629506111145
    },
    {
      "name": "Factorization",
      "score": 0.4592442810535431
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4588177800178528
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.4173767566680908
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4012521803379059
    },
    {
      "name": "Machine learning",
      "score": 0.36137425899505615
    },
    {
      "name": "Algorithm",
      "score": 0.35382822155952454
    },
    {
      "name": "Mathematics",
      "score": 0.10967493057250977
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Eigenvalues and eigenvectors",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}