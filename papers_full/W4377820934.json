{
  "title": "Knowledge Acquired by Foundation Models",
  "url": "https://openalex.org/W4377820934",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5025445408",
      "name": "Gerhard Paaß",
      "affiliations": [
        "Fraunhofer Institute for Intelligent Analysis and Information Systems"
      ]
    },
    {
      "id": "https://openalex.org/A5080184077",
      "name": "Sven Giesselbach",
      "affiliations": [
        "Fraunhofer Institute for Intelligent Analysis and Information Systems"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2997255270",
    "https://openalex.org/W3037636427",
    "https://openalex.org/W3153873661",
    "https://openalex.org/W3168194750",
    "https://openalex.org/W3213868621",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3211841533",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3034830866",
    "https://openalex.org/W2799424953",
    "https://openalex.org/W3202712981",
    "https://openalex.org/W3159900299",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2964895772",
    "https://openalex.org/W3159959439",
    "https://openalex.org/W3211686893",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W3083410900",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W3156170450",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W2995856824",
    "https://openalex.org/W3107806144",
    "https://openalex.org/W3209540659",
    "https://openalex.org/W6635469476",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2804796373",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W6727099177",
    "https://openalex.org/W2997359684",
    "https://openalex.org/W2897042519",
    "https://openalex.org/W2945067664",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3176495666",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3035507081",
    "https://openalex.org/W2997789497",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3194676777",
    "https://openalex.org/W3009786257",
    "https://openalex.org/W6796788747",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W3152698349",
    "https://openalex.org/W2928107702",
    "https://openalex.org/W2972987451",
    "https://openalex.org/W3035091181",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W2950246755",
    "https://openalex.org/W6811284106",
    "https://openalex.org/W2991223644",
    "https://openalex.org/W4289828103",
    "https://openalex.org/W3100279624"
  ],
  "abstract": "Abstract During pre-training, a Foundation Model is trained on an extensive collection of documents and learns the distribution of words in correct and fluent language. In this chapter, we investigate the knowledge acquired by PLMs and the larger Foundation Models. We first discuss the application of Foundation Models to specific benchmarks to test knowledge in a large number of areas and examine if the models are able to derive correct conclusions from the content. Another group of tests assesses Foundation Models by completing text and by applying specific probing classifiers that consider syntactic knowledge, semantic knowledge, and logical reasoning separately. Finally, we investigate if the benchmarks are reliable and reproducible, i.e. whether they actually test the targeted properties and yield the same performance values when repeated by other researchers.",
  "full_text": "Chapter 4 \nKnowledge Acquired by Foundation \nModels \nAbstract During pre-training, a Foundation Model is trained on an extensive \ncollection of documents and learns the distribution of words in correct and ﬂuent \nlanguage. In this chapter, we investigate the knowledge acquired by PLMs and the \nlarger Foundation Models. We ﬁrst discuss the application of Foundation Models to \nspeciﬁc benchmarks to test knowledge in a large number of areas and examine if \nthe models are able to derive correct conclusions from the content. Another group \nof tests assesses Foundation Models by completing text and by applying speciﬁc \nprobing classiﬁers that consider syntactic knowledge, semantic knowledge, and \nlogical reasoning separately. Finally, we investigate if the benchmarks are reliable \nand reproducible, i.e. whether they actually test the targeted properties and yield the \nsame performance values when repeated by other researchers. \nKeywords Knowledge in foundation models · Common Sense knowledge · \nLogical coherence · Benchmark collections · Reproducibility \nDuring pre-training, Pre-trained Language Models (PLMs) and the larger Foun-\ndation Models are trained on an extensive collection of documents and learn the \ndistribution of words in correct and ﬂuent language. During ﬁne-tuning, the models \nare adapted to a speciﬁc task using the knowledge from the pre-training and \nrequiring only a small set of manually labeled ﬁne-tuning data. In this chapter, we \ninvestigate the knowledge acquired by these models by different types of tests: \n• We ﬁrst assess PLMs and Foundation Models by speciﬁc benchmarks to test \nknowledge in a large number of areas and examine if the models are able to \nderive correct conclusions from the content (Sect.4.1). Usually these benchmark \ncollections have an aggregated performance measure averaging over different \ntests. Benchmark tests can be accomplished by ﬁne-tuning models to perform \nspeciﬁc classiﬁcation tasks or by few-shot querying Foundation Models. \n• Then we assess Foundation Models by completing text and by applying speciﬁc \nprobing classiﬁers without adapting model parameters (Sect. 4.2). We separately \nconsider syntactic knowledge, semantic knowledge and logical reasoning and \n© The Author(s) 2023 \nG. Paaß, S. Giesselbach, Foundation Models for Natural Language Processing , \nArtiﬁcial Intelligence: Foundations, Theory, and Algorithms, \nhttps://doi.org/10.1007/978-3-031-23190-2_4\n161\n162 4 Knowledge Acquired by Foundation Models\ndemonstrate the achievements and deﬁcits in different areas and for different \nmodel architectures. \n• Finally, we investigate if the benchmarks are reliable, i.e. actually test the targeted \nproperties (Sect. 4.3). Moreover, we analyze if published benchmark results are \nreproducible and yield the same performance values if they are repeated by other \nresearchers. \n4.1 Benchmark Collections \nIn order to arrive at quantitative measures of common sense knowledge and \ncommonsense reasoning, the community has compiled a number of benchmarks. \nThese allow a standardized comparison of different aspects of natural language \nunderstanding and provide comparable scores for the strength and weaknesses \nof different PLMs. Benchmarks have been a key driver for the development of \nlanguage models. A comprehensive collection of benchmarks and the corresponding \nleaderboards are provided by PapersWithCode [\n45]. A survey of actual benchmarks \nis given by Storks et al. [62]. \nA fair comparison of model architectures requires that the number of parameters, \nthe size of the training data, and the computing effort for training are similar. This \nhas been extensively discussed in Sect.\n3.5.1. Therefore, many authors conduct \nextensive ablation studies to adjust their training resources to a standard, e.g. to \nBERT as a “benchmark model”. This is really important, as it helps the reader to get \nan intuition for the impact of pre-training resources. Nevertheless, comparability is \noften hampered by two problems: \n1. Some training datasets, e.g. the BooksCorpus of BERT, are not publicly avail-\nable. \n2. These comparisons do not show the performance of a model when the size of \ndata, the number of parameters, or the computing effort are increased. \nTherefore, statements like “Model architecture A is superior to model architecture \nB on performing task X.” in general are not valid, but have to be qualiﬁed [ 2], e.g. \n“Model architecture A is superior to model architecture B on performing task X, \nwhen pre-trained on a small/large corpus of low/high quality data from domain Y \nwith computing effort Z.” \n4.1.1 The GLUE Benchmark Collection \nTo test the ability of PLMs to capture the content of a document, the GLUE \n(Sect.\n2.1.5) set of benchmarks has been developed. This is a collection of 9 \nbenchmarks testing different aspects of Natural Language Understanding (NLU). \nThe joint performance is measured by a single score, which has the value 87.1 for\n4.1 Benchmark Collections 163\nhuman annotators. The tasks are described in detail by examples in Table 2.1.I t \nturns out that variants of BERT ﬁne-tuned to the different GLUE-tasks can yield \nbetter results than people. The results are determined for the large variants of the \nmodels and shown in Table \n4.1. \nIn the past years GLUE was routinely employed to demonstrate the NLU \ncapabilities of PLMs. Currently, the best average value of 91.4 after ﬁne-tuning was \nreached by DeBERTaV3 [\n18] (Sect. 3.1.1). It uses separate embeddings for content \nand position and employs a corresponding disentangled attention mechanism. There \nare only three tasks where PLMs are worse than humans, but only by a small margin. \nNote that ensembles of several models often yield slightly better results. Nangia et \nal. [\n42] also measures the performance of human teams of 5 people. The numbers \nare not comparable as cases were excluded when the teams arrived at split judgment. \nNewer models such as PaLM use SuperGLUE instead of GLUE because GLUE is \nconsidered too simple. \n4.1.2 SuperGLUE: An Advanced Version of GLUE \nDue to the progress in the last years, PLMs have reached human performance \nin most tasks and the GLUE is no longer able to discriminate between models. \nTherefore, the authors of GLUE proposed a more demanding test suite called \nSuperGLUE [\n68] as an advanced version of GLUE with eight challenging tasks. \nThe tasks are similar to GLUE with longer contexts to consider. \n• BoolQ is a QA-task with questions collected from Google search and yes/no \nanswers. \n• CB is a textual entailment task. \n• COPA is a causal reasoning task in which a system must determine either the \ncause or effect of a given premise from two possible choices. \n• MultiRC is a QA task where each instance consists of a context passage, a \nquestion about that passage, and a list of possible answers. \n• In ReCoRD each example consists of a news article and an article in which one \nentity is masked out. The system must predict the masked entity from a list of \npossible entities. \n• RTE requires detecting whether a hypothesis is implied by a premise. \n• WiC is a word sense disambiguation task, where for two given sentences the \nsystem has to determine if a polysemous word is used with the same sense in \nboth sentences. \n• WSC is the Winograd Schema Challenge, where the system has to determine the \ncorrect noun phrase represented by a pronoun. \nThe performance again is measured by a single average score with a value of 89.8 \nfor human annotators [\n66].\n164 4 Knowledge Acquired by Foundation Models\nTable 4.1 Results for the GLUE benchmark for four different models and human annotators. The best value of a PLM for each task is printed in bold [ 18, \np. 7]. Human scores better than all model scores are underlined \nCoLA QQP MNLI m SST-2 STS-B QNLI RTE WNLI MRPC \nMcc Acc Acc Acc Corr Acc Acc Acc F1 \nModel Grammar Paraphr. Entail Sentim. Similar Question Entail Coref Paraphr. Avg \nHuman [42] 66.4 80.4 92.0 97.8 92.7 91.2 93.6 95.9 86.3 87.1 \nBERT.LARGE 60.6 91.3 86.6 93.2 90.0 92.3 70.4 65.1 88.0 84.1 \nRoBERTa.LARGE 68.0 92.2 90.2 96.4 92.4 93.9 86.6 89.9 90.9 88.8 \nXLNET.LARGE 69.0 92.3 90.8 97.0 92.5 94.9 85.9 92.5 90.8 89.2 \nDeBERTaV3.LARGE 75.3 93.0 91.8 96.9 93.0 96.0 92.7 – 92.2 91.4\n4.1 Benchmark Collections 165\nGPT-3 [ 7] is a huge language model (Sect. 3.1.2), which can be instructed to \nperform a task without ﬁne-tuning (Sect. 3.2). With this few-shot learning GPT-\n3 achieved an average SuperGLUE score of only 71.8 as shown in Table 4.2. \nObviously ﬁne-tuning the speciﬁc tasks seems to be important. Recently a ﬁne-tuned \nDeBERTa ensemble (Sect. 3.1.1) surpassed human performance on SuperGLUE \nwith an average score of 90.3. The most difﬁcult task is a comparison of word \nsenses in two sentences (WiC), where an accuracy of about 77% can be reached. \nThe autoregressive LM PaLM 540B was ﬁne-tuned on SuperGLUE and achieved \nan average of 90.4% on the test set [ 9, p. 13]. The best average of 91.2% \nwas obtained by the ST-MoE 32B mixture-of-experts model (Sect. 3.5.2) with 269B \nparameters [73]. This shows that Foundation Models are able to analyze complex \ntext semantics. \nGLUE and SuperGLUE have been criticized, as the answers of the posed \nproblems always can be reduced to a classiﬁcation task and the systems do not \nhave to formulate an answer in natural language. In addition, it turns out that the \nperformance of PLMs is not very stable. It has been shown that the prediction of \ncurrent models often change in an inconsistent way, if some words are replaced [\n51]. \nIf, for instance, in a sentiment analysis the input “I love the ﬂight” is classiﬁed as \npositive, then “I didn’t love the ﬂight” should not be classiﬁed as neutral. Ribeiro \net al. [ 51] show that inconsistencies like this often occur. They developed the \nCheckList system (Sect. 4.3.1), which automatically generates test examples for \nprobing a model. \n4.1.3 Text Completion Benchmarks \nThe task of an autoregressive language models is the reliable generation of the \nnext word in a text. This has to obey grammatical correctness as well as semantic \nconsistency. The LAMBADA benchmark [\n44] is a good test to demonstrate this \nability. It consists of about 10,000 passages from the BooksCorpus containing \nunpublished novels. The task is to predict the missing last word of the last sentence \nof each passage. Examples were ﬁltered by humans to ensure that models need to \ntake into account the full passage of at least 50 tokens to induce the ﬁnal word. \nAn example is the passage “Both its sun-speckled shade and the cool grass \nbeneath were a welcome respite after the stiﬂing kitchen, and I was glad to relax \nagainst the tree’s rough, brittle bark and begin my breakfast of buttery, toasted \nbread and fresh fruit. Even the water was tasty, it was so clean and cold. It almost \nmade up for the lack of .”, where “coffee” is the missing target word to be \npredicted. Examples which could be easily predicted by simpler language models \nwere omitted. Examples were only selected, if the target word could be predicted by \nhumans from the full passage but not from the last sentence. \nThe GPT-3175B autoregressive language model [48] predicted the last word with \n76.2% [7, p. 12]. PaLM540B with few-shot instructions could increase the accuracy \nto 89.7 [ 9, p. 79]. This means that in nearly nine of ten cases, the predicted word \nwas exactly the missing word in the test data.\n166 4 Knowledge Acquired by Foundation Models\nTable 4.2 Results for the SuperGLUE benchmark on the test set for human annotators and ﬁve different models. The best value for each task is printed in \nbold and human values better than the model values are underlined. For GPT-3 few-shot values (FS) are reported, ﬁne-tuned otherwise \nBoolQ CB COPA MultiRC ReCoRD RTE WiC WNLI \nAcc Acc/F1 Acc F1a/EM F1/EM F1/EM Acc Acc \nModel QA y/n Entail Cause QA mult. Entities Entail WSD Coref Avg \nHuman [68] 89.0 95.8/98.9 100.0 81.8/51.9 91.7/91.3 93.6 80.0 100.0 89.8 \nBERT336M [68] 77.4 83.6/75.7 70.6 70.0/24.0 72.0/71.3 71.6 69.5 64.3 69.0 \nGPT-3270B FS [7] 76.4 75.6/52.0 92.0 75.4/30.5 91.1/90.2 69.0 49.4 80.1 71.8 \nDeBERTA Ens. [19] 90.4 94.9/97.2 98.4 88.2/63.7 94.5/94.1 93.2 77.5 95.9 90.3 \nPaLM540B [9] 91.9 94.4/96.0 99.0 88.7/63.6 94.2/93.3 95.9 77.4 95.9 90.4 \nST-MoE32B [73] 92.4 96.9/98.0 99.2 89.6/65.8 95.1/94.4 93.5 77.7 96.6 91.2\n4.1 Benchmark Collections 167\nAnother relevant benchmark for language modeling is WikiText-103 [38]o f2 8 k \narticles from Wikipedia with 103M tokens. If large Foundation Models are applied \nto this corpus the following perplexities result: GPT-2 1.7B 17.5 [ 48], Megatron-\nLM 10.8 [ 58], Gopher280B 8.1 [49, p. 61]. Recently a small Retro 1.8B model with \nretrieval could reduce this perplexity to 3.9 [ 49, p. 12]. Note that there might \nbe a partial overlap of Wikitext 103 with Retro’s training data not caught by \ndeduplication. \n4.1.4 Large Benchmark Collections \nRecently large autoregressive language models like GPT-3, Gopher, and PaLM have \nbeen developed, which are trained on extremely large document collections with \nhundreds of billions of tokens. The models should perform well across a wide range \nof tasks. Therefore, instead of the limited GLUE benchmarks a large number of \nbenchmarks covering many aspects of possible applications are used to evaluate \ntheir performance. \nA frequent opinion is that current benchmarks are insufﬁcient and “saturate”, \n“have artifacts”, and are “overﬁtted by researchers”. Bowman et al. [ 5] argue that \n“evaluation for many natural language understanding (NLU) tasks is broken”. They \ncomplain that there are systems at the top of the leaderboards which fail in simple \ntest cases (cf. [\n51]). As a consequence they formulate four requirements on new \nbenchmarks: \n• A model should only reach good performance on the benchmark if it also has a \ngood performance on actual applications. \n• The annotation of benchmarks should be accurate and not ambiguous (e.g. 36% \nof the answers in Natural Questions are ambiguous). \n• The benchmarks should be large and challenging enough to detect relevant \nperformance differences between models. \n• Benchmarks should reveal plausibly harmful social biases in systems, and should \nnot encourage the creation of biases. \nThey summarize some promising developments that could support these challenges, \nincluding data collection involving both crowdworkers and domain experts, and \nlarger-scale data validation. \nTo address this criticism, two comprehensive collections of benchmarks have \nbeen deﬁned. The Massive Multitask Language Understanding (MMLU) bench-\nmark [\n20] emulates human exams with multiple choice questions, each with four \nresponses. In addition to logical and mathematical reasoning it tests a model’s ability \nacross a wide range of academic subjects from computer science to history and law. \nThe other collection is the BIG-bench collaborative benchmark [\n1, 60], designed \nto evaluate language interpretation aspects like reading comprehension, question \nanswering, world understanding, etc. Both benchmark collections include more than \na hundred tasks.\n168 4 Knowledge Acquired by Foundation Models\nTable 4.3 Groups of evaluation benchmarks for Gopher and related models [ 49,p .8 ] \nTask group #T a s k s Examples \nLanguage modeling 20 WikiText-103, The Pile: PG-19, arXiv, FreeLaw, . . . \nReading comprehension 3 RACE-m, RACE-h, LAMBADA \nFact checking 3 FEVER (2-way & 3-way), MultiFC \nQuestion answering 3 Natural questions, TriviaQA, TruthfulQA \nCommon sense 4 HellaSwag, Winogrande, PIQA, SIQA \nMassive multitask language \nunderstanding (MMLU) [20] \n57 High school chemistry, astronomy, clinical \nknowledge, social science, math, . . . \nBIG-bench [60] 62 Causal judgement, epistemic reasoning, temporal \nsequences, logic, math, code, social reasoning, . . . \nThe Gopher model with 280B parameters together with alternatives like GPT-3, \nJurassic-1, and Megatron-Turing NLG (all discussed in Sect. 3.1.2) were tested on \nthese and other benchmarks. Note that this was done with a total of 152 benchmarks \ndescribed in Table 4.3. Gopher shows an improvement on 100 of 124 tasks (81%) \ncompared to the previous SOTA scores. In language modeling (next word prediction) \nGopher improves SOTA for 10 of 19 benchmarks. Note that all benchmark results \nwere not obtained after ﬁne-tuning but by zero-shot or few-shot learning. \nThe distribution Gopher accuracies for thematic groups are shown in Fig. 4.1. \nGopher is able to increase S OTA for 4 out of 7 math tasks, 5 out of 9 common \nsense tasks, 9 out of 12 logical reasoning tasks, 22 out of 24 fact checking \nand general knowledge tasks, all 24 STEM (Science Technology Engineering \nMathematics) and medicine tasks, all 15 humanities and ethics task, and 10 out \nof 11 reading comprehension tasks. The average accuracies for common sense and \ngeneral knowledge are about 50%, indicating that some knowledge exists but can \nbe improved. Among these tests were benchmarks on logical reasoning, which, \nfor instance, include “Formal Fallacies Syllogisms Negation” or “Logical Fallacy \nDetection”. Only two of the 19 benchmarks achieved an accuracy of more than \n60% [\n49, p. 58], indicating that even for this large model correct reasoning is a \nmajor obstacle. Obviously this spectrum of evaluation gives a deep insight into the \ncapabilities of the compared models. It can be expected that the new Retro model \n(Sect.\n6.2.3), which performs retrieval during language generation, will improve \nthese results. \nThe PaLM autoregressive language model with 580B parameters [ 9, p. 15] \nrecently was evaluated with the BIG-bench benchmark. On the 150 tasks, PaLM \nwith 5-shot prompts achieved an normalized average score of 46%, which was better \nthan the average human score of 39%. However, the best human experts have a score \nof about 77%. The detailed results for the different BIG benchmark areas are not yet \navailable. On a subset of 58 BIG-tasks, which were also used by prior models, PaLM \nobtained a 5-shot normalized score of about 55%, again above the human average \nof 49%, outperforming Chinchilla (47%) and Gopher (30%). GPT-3 achieved a 1-\nshot performance of 16% on the 58 tasks. In general Foundation Models like Gopher \nand PaLM with several hundred billion parameters have ‘dramatically better’ results\n4.1 Benchmark Collections 169\nFig. 4.1 Accuracies in percent of different groups covering 152 different benchmarks evaluated \nfor the Gopher model [49, p. 57]. The 25% and 75% percentiles are given by the box, and the inner \nline is the median. The outside lines indicate variability outside the upper and lower quartiles \non BIG than smaller models, even if the model architecture is not fundamentally \ndifferent [1]. In this respect Foundation Models show a qualitatively new behavior. \nResearchers at Google have proposed to use the BIG-bench benchmark with \ncurrently 200 tasks as a replacement for the Turing test for “intelligence” [ 61]. \nIn this way the knowledge of an AI-System can be checked at a large scale. \nRecently, a Google engineer published a dialog [\n31] with the LaMDA language \nmodel (Sect. 6.6.3). In his view this indicates that LaMDA is “sentient”. However, \nthis aspect of human intelligence is not checked by knowledge and reasoning tests \nsuch as BIG and requires the development of new types of tests. \n4.1.5 Summary \nBenchmark collections are a popular way to demonstrate the superiority of a Pre-\ntrained Language Model for speciﬁc tasks. To show the merits of an architecture, \nhowever, also the number of parameters, the size of training data, and the computing\n170 4 Knowledge Acquired by Foundation Models\neffort has to be reported and compared, because these numbers also affect the model \nperformance. \nThe GLUE benchmark collection of nine language understanding tasks has \ndemonstrated the considerable progress of PLMs during the last years. It tests the \nability of PLMs to detect paraphrases, coreference relations, logical entailments \nand grammatical correctness. Meanwhile, the average accuracy exceeds the average \nhuman performance. The similar, more challenging SuperGLUE benchmark suite \nhas been introduced, where human performance is also surpassed. For autoregres-\nsive language models the LAMBADA benchmark requires an impressive ability to \ndetermine the most probable last word of a paragraph. Current models like PaLM \nare able to predict the last word with an accuracy of nearly 90% demonstrating its \nability to capture the ﬂow of arguments. \nFoundation Models are usually tested by extensive standardized test collections \ncovering many aspects like common sense knowledge, emotional intelligence, logi-\ncal reasoning, or social sciences. Recent Foundation Models like Gopher and PaLM, \nwith several hundred billion parameters, have been able to achieve performance \nbetter than that the human average and ‘dramatically better’ than smaller models. \nHowever, these models still have a lower accuracy than human experts. Although the \nbenchmarks are very expressive, they do not take into account the societal impact of \nthe models and are unable to detect features like self-awareness and sentience. \n4.2 Evaluating Knowledge by Probing Classiﬁers \nIn this section, we examine the extent to which PLMs acquire different types of \nknowledge. We discuss the covered knowledge for the small BERT model and later \nreview the improvements for foundation models such as GPT-3 and PaLM. First, \nwe consider their syntactic knowledge of correct language. Then, we investigate \nhow much common sense knowledge is represented by PLMs. Finally, we explore \nwhether the output produced by PLMs is logically consistent. \n4.2.1 BERT’s Syntactic Knowledge \nWe discuss the syntactic knowledge incorporated in PLMs using BERT as an exam-\nple. In the course of pre-training BERT is able to capture syntactic knowledge [ 54]. \nEmbeddings can encode information about parts of speech, syntactic phrases and \nsyntactic roles. Probing classiﬁers can predict part-of-speech tags and supersense \ninformation with an accuracy of 85% [ 33]. Obviously, this information has to be \nencoded in BERT’s ﬁnal embeddings. BERT also has knowledge of subject-verb \nagreement [17] and semantic roles [ 14]. It is also possible to extract dependency \ntrees and syntactic constituency trees from BERT [ 21, 23, 27]. While probing \nindicates that the information can be extracted from the representation, it can be\n4.2 Evaluating Knowledge by Probing Classiﬁers 171\nshown [13] that in some cases the features are not used for prediction. According to \nan empirical evaluation PLMs encode linguistic information with phrase features in \nthe bottom layers, syntactic features in the middle layers and semantic features in \nthe top layers [\n23]. \nHowever, BERT’s syntactic knowledge is incomplete and there is, for example, \nevidence that BERT often does not capture negations. For instance, BERT.LARGE is \nable to determine the correct supersense, e.g. “bird” in the masked sentence “A robin \nis a [MASK]” with high probability [ 14]. On the other hand, the model predicts \n“robin”, “bird”, “penguin”, “man”, “ﬂy” with maximum probabilities for the mask \nin “A robin is not a [MASK]”, effectively ignoring the negation. \nSome benchmarks described in Sect.4.1 check the syntactic knowledge of PLMs. \nAn example is the GLUE’s CoLA task testing the grammatical correctness of \nsentences, which is the most difﬁcult task of GLUE where the best models only yield \nabout 75% correct answers (Table \n4.1). SuperGLUE (Sect. 4.1.2) is a benchmark, \nwhich requires syntactic knowledge, e.g. for the textual entailment task COPA and \nthe coreference resolution task WSC. While the ﬁne-tuned BERT gets an average \nscore of 69.0 the ﬁne-tuned PaLM\n540B achieves an average of 91.4 (Table 4.2). \nLarge foundation models such as PaLM, which has more than 1000 times as many \nparameters as BERT, are obviously able to capture syntactical knowledge much \nbetter than the ‘small’ BERT. \n4.2.2 Common Sense Knowledge \nWorld knowledge, also called common sense knowledge, consists of facts about \nour every day world, such as “ﬁre is hot” . A simple method of checking world \nknowledge is to query BERT with cloze statements, for example, “Einstein was \nborn in [MASK]”. BERT acquires some semantic knowledge about semantic roles \nand encodes information about entity types and relations [ 54]. For instance, in \nthe sentence “to tip a [MASK]” the token “waiter” gets a high probability for \nthe position of [MASK]. Petroni et al. [ 46] and Zhou et al. [ 72] experimented \nwith such queries and concluded that BERT contains world knowledge competitive \nwith traditional supervised information extraction methods. It has been shown that \nBERT’s contextual embeddings make up clusters corresponding to word senses [\n56]. \nThis explains why BERT is quite capable of word sense disambiguation (Fig.2.10). \nPetroni et al. [ 46] remark that certain types of factual knowledge are learned \nmuch more easily than others by the standard language model pre-training \napproaches. They state that without ﬁne-tuning, BERT contains relational \nknowledge competitive with traditional NLP methods that have some access to \noracle knowledge. In addition, BERT also does remarkably well on open-domain \nquestion answering against a supervised baseline. These capabilities of BERT are a \ngreat achievement. \nThe language model GPT-3 has one hundred times more parameters than BERT \nand a dramatically better common sense knowledge. This, for example, can be seen\n172 4 Knowledge Acquired by Foundation Models\nfrom its answers (A) to the questions (Q): “Q: Are there any animals with three \nlegs?” “A: No, there are no animals with three legs.” or “Q: Which is heavier, a \nfootball player or a car?” “A: A car is heavier than a football player.” [29]. In an \ninitial experiment eighty persons were asked to assess, if short 200 word articles \nwere written by humans or GPT-3. The persons judged incorrectly 48% of the time, \ndoing only slightly better than random guessing [7]. \nHowever, the semantic knowledge of PLMs is not perfect. BERT, for instance, \nhas difﬁculties with the representation of numbers and often has problems with \nthe replacement of named entities (NEs), e.g. person names or location names. \nFor example, replacing names in the coreference task changes 85% of coreference \nassignments of expressions that refer to the same entity [\n3]. Obviously the pre-\ntrained version of BERT struggles to generalize the relations involving one named \nentity to other named entities of the same type. Moreover, BERT has problems to \ntransfer knowledge based on the roles or types of objects. In addition, it is possible \nto mislead BERT by adding some content to a cloze query. An example is the word \n“Talk” in “Talk? Birds can [MASK]”. A human would ignore “Talk?” and use his \nworld knowledge to generate a result like “ﬂy”. In contrast, PLMs can be misled \nand produce the wrong answer “talk” for the mask [26]. \nA related phenomenon is the invariance to paraphrases. Elazar et al. [ 12] \ngenerate a high-quality set of 328 paraphrases to express 38 relations. Examples \nare \n“X originally aired on [MASK]” and “X premiered on [MASK]”, which should \ngive the same prediction for [MASK],i f “X” is replaced by some TV series like \n“Seinfeld”. Although the models in about 60% of the cases have access to the \nrequired knowledge to ﬁll the mask correctly, BERT .LARGE yields a consistency in \nparaphrases in only 48.7% of the cases. This indicates that not every fact present in \nthe training data is encoded in the parameters and that the model does not always \ndetect the equivalence of paraphrases. The model variants RoBERTa and ALBERT \nachieve a lower consistency, although they are superior to BERT in other tasks. \nIt is instructive to consider the inﬂuence of word order on the performance of \nBERT. Word order is taken into account by speciﬁc position embeddings, which \nare added to the token embeddings. It turns out, however that masked language \nmodels like BERT still achieve a high accuracy, if word positions are permuted. For \npre-training Sinha et al. [\n59] perform sentence permutations, where each word in a \nsentence is randomly placed at a different position. The model was ﬁne-tuned on \nGLUE, a set of classiﬁcation tasks for natural language understanding (Sect. 2.1.5). \nIf we ignore the CoLA-task, which checks linguistic acceptability, the model on \naverage only looses 3.4% accuracy if the word order is permuted compared to the \noriginal RoBERTa results (88.7% on average). The authors conclude that BERT-like \nmodels achieve high performance on downstream tasks almost entirely by exploiting \nhigher-order word co-occurrence statistics. \nAnother aspect of common sense knowledge is time. When a PLM is applied \nto new documents it often does not know the meaning of new named entities and \nconcepts [\n30]. Often, the model cannot infer the time and region of a document \nand may not be able to correctly combine facts from documents that originate \nfrom different time periods or geographical regions. A benchmark for assessing\n4.2 Evaluating Knowledge by Probing Classiﬁers 173\nthe temporal reasoning capabilities of PLMs in dialogs shows that BERT and T5 \nhave major deﬁcits on this task [ 47]. In summary it can be expected that the \nnew Retro (Sect. 6.2.3) or WebGPT (Sect. 6.2.3) models, which perform retrieval \nduring language generation, will considerably mitigate the problems discussed in \nthis section. \nTo be able to check a multitude of different knowledge types in a standardized \nway large benchmarks like BIG-bench have been developed (Sect. 4.1.4). It com-\nprises benchmarks on common sense, emotional intelligence, ethics, fact checking, \ngeneral knowledge, humanities, mathematics, medicine, reading comprehension, \nscience and social sciences. Figure \n4.1 shows the performance of the Gopher model \nwith 280B parameters on these benchmark groups. On most groups more than \n50% accuracy was achieved. The PaLM model with 540B parameters was able \nto improve these performance ﬁgures. On about \n.2/3 of these tasks PaLM using \n5-shot prompts achieves a better performance than average humans [ 9,p .1 7 ] . \nThis indicates that PaLM has a much better common sense knowledge than earlier \nmodels. Nevertheless, PaLM surpasses the performance of human experts only in a \nsmall fraction of cases suggesting further headroom for improvement. \nAn interesting idea is to use large pre-trained multilingual language models \nas a multilingual knowledge base [ 25]. The authors evaluate this for mBERT \n(Sect. 3.3.1), a standard BERT model, which has been pre-trained with the MLM \nloss on non-parallel Wikipedia texts from 104 languages. The authors ﬁnd that \ncorrect entities can be retrieved for many languages. However, there is a clear \nperformance gap between English and, e.g., Japanese and Thai. This suggests that \nmBERT does not store knowledge about entities in a language-independent way. It \nwould be revealing if these experiments could be repeated with up-to-date language \nmodels like PaLM. \n4.2.3 Logical Consistency \nA set of statements is logically inconsistent if they cannot all be true at the same \ntime. As an example consider the statements “John is Tom’s father. Tom is the \ndaughter of John.” Sometimes, BERT is unable to reason, i.e. logically connect \ndifferent pieces of knowledge. It reproduces, for instance, the relations that persons \ncan walk into houses, and that houses are big, but it cannot infer that houses are \nbigger than persons [\n15, 52]. However, semantic knowledge problems tend to be \nsmaller for models with more parameters. \nRichardson et al. [ 52] formulated nine different types of simple sentence pairs \ncontaining e.g. negations, quantiﬁers, comparatives, etc. These sentences express \nlogical entailment, contradiction or neutrality. In addition, they also employ chains \nof hypernomy, e.g. poodle . ≤ dog . ≤ mammal . ≤ animal, and use these relations \nto generate new sentences expressing the corresponding logical properties. It turns \nout that BERT ﬁne-tuned with the ‘logical tasks’ SNLI and MNLI predicts correct \nstatements only with 47.3% accuracy of the cases.\n174 4 Knowledge Acquired by Foundation Models\nRibeiro et al. [51] propose to generate a large number of simple examples to test \nrelations by a CheckList procedure described in Sect. 4.3.1. It tests, for instance, \nwhether negating a positive sentiment expression leads to a negative sentiment \nrating. For more than half of the tests with commercial and open-source models \nthey observed failure rates of more than 50%. \nEven the larger model GPT-3 is not perfect, e.g. it incorrectly answers some \ncommon sense physics questions like “If I put cheese into the fridge, will it \nmelt?” [7]. In addition, it has difﬁculties with logical reasoning, e.g. to determine \nif one sentence implies another. If a question is not covered in its training material, \nGPT-3 compiles the most probable answer and sometimes this is wrong, e.g. “Q: \nHow many eyes does the sun have?” “A: The sun has one eye.” or “Q: Who was \npresident of the United States in 1600?” “A: Queen Elizabeth I was president of \nthe United States in 1600.” [29]. As another example consider the following input \n“You poured yourself a glass of cranberry, but then absentmindedly, you poured \nabout a teaspoon of grape juice into it. It looks OK. You try snifﬁng it, but you \nhave a bad cold, so you can’t smell anything. You are very thirsty. So you ...”.T h e \ncontinuation generated by GPT-3 is “drink it. You are now dead.”. GPT-3 assumes \nwrongly that “grape juice” is a poison and drinking it will kill you [36]. \nImproving Logical Consistency \nPLMs can improve logical reasoning capabilities if they are trained with appro-\npriately generated textual expressions. By ﬁne-tuning a BERT model with created \nsentences containing negations, hypernomy, etc., and testing with other generated \nsentences, Richardson et al. [\n52] achieve an accuracy of 98%. This approach is \nsimilar to the data generation strategy proposed in Sect. 3.6.6. \nSimilarly, Clark et al. [ 10] generate datasets of the form (context, statement, \nanswer), where context contains different logical facts and rules, statement is a \nlogical question to prove and answer is either T or F. Facts, rules, and the question \nstatements are then expressed in (synthetic) English. The problems require simulta-\nneous consideration of a number of different statements to reach a conclusion, from \ndepth 0 (simple lookup) to depth 5. During ﬁne-tuning on this data, RoBERTa was \ntrained to answer these questions as true or false. On the test data RoBERTa is able \nto answer the questions with 99% accuracy. If the facts and rules are paraphrased the \naccuracy drops to 66%. However, by training on paraphrased rules the model again \nreaches an accuracy beyond 90%. Clark et al. [\n10] suggest that by this approach \nthe transformer can be considered as a “soft theorem prover” able to work with \nstatements in language. \nIt is possible to combine the implicit, pre-trained knowledge of an LM and \nexplicit statements in natural language. Talmor et al. [64] show that models trained \nwith such datasets can perform inferences involving implicit world knowledge and \ntaxonomic knowledge (e.g. the WordNet hierarchy) . In addition, inference patterns \nprovided by examples are used by the model to solve logical problems.\n4.2 Evaluating Knowledge by Probing Classiﬁers 175\nThere were a number of prior approaches to combine logical reasoning with \nneural networks. If a neural network provides probabilities for logical facts, then \nwe can use a probabilistic reasoning system to enforce additional constraints. \nExamples are DeepProblog [\n35] that incorporates Deep Learning by means of \nneural predicates, i.e. statements whose probability is determined by a neural \nnetwork. An alternative is probabilistic soft logic (PSL)[\n28], which allows ﬁrst \norder probabilistic reasoning in relational domains. However, PLMs do not directly \nprovide probabilities for facts. There have been approaches to translate natural \nlanguage sentences to logical statements and apply logical reasoning. However, this \n“semantic parsing” [\n24] was not very successful. \nA number of researchers have developed methods for neural theorem proving. \nThis work combines symbolic and neural methods to reason about results derived \nfrom language. Examples are e.g. Minervini et al. [\n39], which jointly embed logical \npredicates and text in a shared space by using an end-to-end differentiable model, \nor Weber et al. [\n70] which combine a Prolog prover with a language model to apply \nrule-based reasoning to natural language. The DeepCTRL approach [57]i n t e g r a t e s \nrules with Deep Learning. It has a rule encoder which allows to control the strengths \nof the rules at inference. It can be applied to domains like healthcare, physical \nmodels or accounting, where obeying rules is essential. \nA simple but effective way to improve logical consistency is to increase the \nnumber of model parameters creating a Foundation Model. A large fraction of the \ntasks in the BIG-bench benchmark [\n1, 60] is devoted to checking logical consistency, \ne.g. the benchmark groups with analogical reasoning and logical reasoning. Gopher \n(Sect.\n3.1.2) is a language model with 280B parameters. It was applied to about \n150 benchmarks, among them 19 logical reasoning tasks. In all but 4 benchmarks it \ncould increase SOTA indicating that larger PLMs have better reasoning capabilities. \nNevertheless, the average accuracy was only about 50%. It was not yet evaluated \nwhether the recent Retro (Sect. 6.2.3) language model with retrieval of additional \ntext documents is able to improve these results. \nPaLM (Sect. 3.1.2) is an even larger language model with 540B parameters. \nOn the SuperGLUE logical tasks CB, COPA, RTE, it can drastically increase the \nscores compared to BERT, e.g. for COPA from 70.6 to 99.2 (Table 4.2). It has been \nevaluated on hundreds of benchmarks including those used for Gopher. It uses a \nnew prompt technique to pose logical questions, where examples are presented to \nthe system together with thought chains partitioning a reasoning task into smaller \nproblems (Sect. 3.6.4). Two examples are shown in Fig. 2.21. Note that k-shot \nreasoning only requires a single sequence of k thought chain prompts to be provided \nfor the training examples. The model then generates a thought chain for each test \nexample. This can be used for error analysis and explaining the model behavior. \nUsing this technique, PaLM is able to match or surpass the performance level of \nan average human asked to solve the task. As an example consider the StrategyQA \nbenchmark [\n16], which contains questions like “Did Aristotle use a laptop?” .F o r \nthis question the model has to collect facts on the lifespan of Aristotle and the year, \nwhen the ﬁrst laptop was invented to arrive at the answer “No”. Without thought \nchain prompts PaLM reached 69%, while the use of thought chain prompts could\n176 4 Knowledge Acquired by Foundation Models\nimprove the prior S OTA from 70% to 73.9%. As a comparison, average humans \nachieve 62.9%, while expert humans have an accuracy of 90%. \nThere are other ways to improve learning with such intermediate outputs. Wang \net al. [ 69] sample multiple chains of thought exploiting the diversity of reasoning \npaths and then return the most consistent ﬁnal answer in the set. Since it is expensive \nto obtain chains-of-thought for a large number of examples, Zelikman et al. [ 71] \ngenerate explanations for a large dataset by bootstrapping a model in the few-shot \nsetting and only retaining chains-of-thought that lead to correct answers. \n4.2.4 Summary \nPre-trained PLMs have a huge number of parameters and are able to represent \nan enormous amount of syntactic and factual knowledge. This knowledge can \nbe elicited by probing classiﬁers, the prediction of masked words, by generating \nanswers to inputs, or by solving benchmark tasks. \nAs far as syntactic knowledge is concerned, Foundation Models like GPT-3 \nproduce almost error-free text and ‘know’ a lot about syntactic rules. One problem \nis to adequately reﬂect the effect of negations. \nEven smaller models like BERT are capable of producing a lot of common-\nsense knowledge. Here, the effect of substituting names or using paraphrases is \nproblematic. Larger language models like GPT-3 are more robust, and the recently \nproposed language models with retrieval (WebGPT, Retro) are able to include \nrelevant external documents for the current task. This information can reduce errors \nconsiderably. However, there is no comprehensive evaluation yet. One problem is \nthe correct temporal and spatial location of information. Here, smaller models like \nBERT and T5 have large deﬁcits. Foundation Models meanwhile surpass the average \nhuman score in 2/3 of the BIG-bench tests on common sense knowledge. They can \neven be used as a multilingual knowledge base, since models like PaLM cover many \nlanguages. \nLogical consistency of inferences is a problem, and the PLMs often associate \nanswers that are plausible but wrong. The models are only able to make logical \ninferences for relationships mentioned in the training text, and they are often \nincapable of making abstractions and generalizing an observed relationship to \nother objects or entities of the same type. Logical consistency can be improved \nby generating additional training texts containing assumptions and valid logical \nconsequences resulting from them. The direct inclusion of logical reasoning systems \nin Foundation Models was not very successful. The PaLM language model with \n540B parameters achieved a fundamental improvement of the accuracy of logical \nreasoning through the use of thought chain prompts. Here in a few-shot prompt a \nlogical derivation is broken down into smaller logical substeps . At present, it is \nnot clear, to what extent language models with retrieval can reduce the still existing \ndeﬁcits in logical reasoning.\n4.3 Transferability and Reproducibility of Benchmarks 177\n4.3 Transferability and Reproducibility of Benchmarks \nIn this section, we consider whether benchmarks actually evaluate the properties \nthey are supposed to test. We also discuss the extent to which they are reproducible. \n4.3.1 Transferability of Benchmark Results \nOn a number of benchmarks, the performance of human annotators is exceeded \nby Foundation Models. This is an indication that the model has learned valuable \ncontents about language. However, Ribeiro et al. [\n51] argue that this can be \nmisleading, because the test sets often do not cover the right content. While \nperformance on held-out test data is a useful measure, these datasets are often not \ncomprehensive. Hence, there is the danger of overestimating the usability of the \nmodel in real applications. \nBenchmarks May Not Test All Aspects \nOn the MRPC task of the GLUE benchmark for detecting paraphrases RoBERTa, \nBERT\n.LARGE, and humans have F1 scores of 90.9% [ 34], 89.3% [ 42] and 86.3% \nrespectively. Therefore, both models perform better than humans. To test whether \nthe models respect basic logical relationships, Ribeiro et al. [51] propose to generate \na large number of simple examples using a CheckList procedure. This approach is \nsimilar to testing software by systematically generating a large variety of inputs in \nunit tests. \nThe following scheme, for instance, can be used to check the effect of a \nnegation in a sentiment classiﬁcation task “I . <negation.> . <positive_verb. > the \n. <thing. >”. It generates sentences like “I didn’t love the food” or “I don’t enjoy \nsailing”. The authors formulate minimum functionality tests, which are useful to \ncheck if the model actually detected the reason of an outcome or used some \nunjustiﬁed association. In addition, they utilize invariance tests to ﬁnd out, if neutral \nperturbations or paraphrases change the result. Finally, they create directional \nexpectation tests, where a modiﬁcation is known to change the result in an expected \nway. \nFor MPRC it turned out that the failure rates of RoBERTa and BERT on these \n23 test templates are larger than 50% for 11 and 14 of the templates respectively. \nTherefore, the “superhuman” performance of the two models should be taken with \na grain of salt. \nThe authors also tested ﬁve current PLMs: BERT .BASE, RoBERTa .BASE, \nMicrosoft’s Text Analytics, Google Cloud’s Natural Language, and Amazon’s \nComprehend. They report the results of 17 tests for sentiment classiﬁcation, where \nmost problems occurred with negations. For instance, the following example \n“I \nthought the plane would be awful, but it wasn’t.” was misclassiﬁed by most models.\n178 4 Knowledge Acquired by Foundation Models\nAlso very difﬁcult is the detection of paraphrases with 23 tests templates. Here \nRoBERTa had for 11 and BERT for 14 of the test templates a failure rate of more \nthan 50%. A similar failure rate was observed for reading comprehension when \ntest cases were generated with logical templates. These results indicate that the \nexamples in the original test sets of the benchmarks are too easy. \nTo increase robustness of PLMs it is possible to generate adversarial examples \n[8, 65]. The authors discuss methods that augment training data with adversarial \nexamples as well as methods that produce certiﬁcates of robustness. They also \ninvestigate methods to avoid spurious correlations, i.e. predictive patterns that work \nwell on a speciﬁc dataset but do not hold in general. \nTalman et al. [ 63] checked, if the results for benchmarks may be transferred \nto similar datasets. They trained six PLMs on different benchmarks for natural \nlanguage inference (NLI) containing sentence pairs manually labeled with the labels \nentailment, contradiction, and neutral. While six models perform well when the test \nset matches the training set, accuracy is signiﬁcantly lower when a test set from \nanother benchmark is used. BERT\n.BASE, for instance, yields a test accuracy of 90.4% \nfor SNLI, which drops on average 21.2% for the test sets of the other benchmarks. \nThe reason behind this drop is a slightly different deﬁnition of the task as well as \nsmall differences in the documents domains. Obviously, it cannot be expected that \nthe performance of PLMs can simply be transferred to new data. \nLogical Reasoning by Correlation \nThe Winograd schema challenge (WNLI) was developed by Levesque et al. [32] and \nis part of the GLUE benchmark collection. The test consists of a pair of sentences \ndiffering by exactly one word, each followed by a question [\n41], e.g. \n• The sports car passed the mail truck because it was going faster. Question: Which \nwas going faster, the sports car or the mail truck? \n• The sports car passed the mail truck because it was going slower. Question: \nWhich was going slower, the sports car or the mail truck? \nIn this pair of sentences, the difference of one word changes which thing or person \na pronoun refers to. Answering these questions correctly seems to require common \nsense reasoning and world knowledge. In addition, the authors have designed the \nquestions to be “Google-proof”: The system should not be able to use a web search \n(or anything similar) to answer the questions. GPT-3 reaches a value of 88.6% using \nfew-shot prompts without ﬁne-tuning [\n7] and DeBERTa managed an accuracy of \n95.6% after ﬁne-tuning [19]. This accuracy roughly equals human performance. \nAs Mitchell [ 41] argues, this does not necessarily mean that neural network \nlanguage models have attained human-like understanding. For a number of question \npairs it seems possible to answer the question by some sort of correlation instead \nof actual world knowledge. If pre-trained on a large corpus the model will learn \nthe high correlation between \n“sports car” and “fast” and between “mail truck” and \n“slow” for the above example. Therefore, it can give the correct answer on the\n4.3 Transferability and Reproducibility of Benchmarks 179\ncoreference of “it” based on those correlations alone and not by recourse to any \nunderstanding. It turns out that many of the Winograd schema challenge question \nfollow this pattern. A similar argument states [6, 37] that a model might heuristically \naccept a hypothesis by assuming that the premise entails any hypothesis whose \nwords all appear in the premise. This means that the model can give the right answer \nwithout ‘understanding’ the situation in question. \nTo reduce the deﬁcits of the Winograd schema challenge a much larger Wino-\ngrande benchmark [55] was created using crowdsourcing. The researchers discarded \nsentences which could be answered by exploiting intuition and correlation. They \nused the embeddings created by RoBERTa (Sect.\n3.1.1) to determine if these embed-\ndings strongly indicated the correct response option. In this case they discarded the \nquestion pair and ﬁnally ended up with 44k sentences. An example for a question \npair without correlation problems is: \n• The trophy doesn’t ﬁt into the brown suitcase because it’s too large. (it: trophy) \n• The trophy doesn’t ﬁt into the brown suitcase because it’s too small. (it: suitcase) \nWhile humans reach an accuracy of 94%, the best PLMs, standard models like \nRoBERTa only reached 79.1% accuracy. Recently, T5-XXL achieved an accuracy \nof about 91% [\n43] and the ST-MoE-32B mixture-of-experts model [ 73] with 269B \nparameters (Sect. 3.5.2) obtained 96.1%, drastically reducing the errors. It appears \nthat in most cases the latter models are able to perform ‘reasoning’ without simply \ncorrelating statements. \n4.3.2 Reproducibility of Published Results in Natural \nLanguage Processing \nMany publications in NLP claim that their model achieves S OTA for some bench-\nmark. Examples are the GLUE benchmark [ 67] for language understanding and \nthe SQuAD data [ 50] for reading comprehension. There are two main problems \nwith this approach. First it is difﬁcult to assess, if the results are reproducible and \nsigniﬁcant. As Crane [\n11] demonstrates, there are usually a number of unreported \nconditions that affect the reproducibility of the result. An example is the random ini-\ntialization of the network parameters. The resulting variance is often larger than the \nreported improvement in S\nOTA scores. However, the variance resulting from these \nphenomena is usually not reported. Other effects are the underlying programming \nframeworks and libraries, which change over time. Often the hyperparameters and \nthe details of preprocessing and model conﬁguration are not communicated. \nTo document the model architecture, the training and evaluation process of \na model, Mitchell et al. [ 40] proposed the description of relevant facts and \nhyperparameters in a model card. After a short high-level description of the model \nand its purpose the model card should contain nine different sections [40]: \n1. Basic information about the model, \n2. Intended uses and scope limitations,\n180 4 Knowledge Acquired by Foundation Models\n3. Model performance across a variety of relevant factors, \n4. Performance metrics, \n5. Evaluation data, \n6. Training data, \n7. Evaluation results according to the chosen metrics. \n8. Ethical consideration, risks and harms. \n9. Caveats and recommendations. \nMore details are given by huggingface [ 22]. Even if models still can be published \nwithout a model card, the explicit documentation of the model can only beneﬁt \nfuture users. Therefore, model cards should be provided if possible. For most recent \nmodels, a model card is provided even if the model is not open-source. \nA survey on reproducibility in NLP is given by Belz et al. [ 4]. They note \nthat the performance results often depend on seemingly small differences in \nmodel parameters and settings, for example minimum counts for rare word or the \nnormalization of writing. The authors state in their study on repeated experiments \nthat only 14% of the 513 reported scores were the same. An annoying fraction \nof 59% of the scores were worse than the published numbers. Therefore, the \nexperimental results published in papers should be treated with caution. \nAnother issue is the question of what causes an increase in performance. As we \nhave discussed above, a growth in the number of parameters and in the computing \neffort regularly leads to better results for PLMs (Sect.\n3.5.1). As a consequence, it \nis often not clear, whether the architectural changes to a model yield the improved \nperformance or just the number of additional parameters or the larger training set \n[\n53]. \nObviously a ﬁrst place in a leaderboard can be achieved with a larger model \nand more computing effort. This, however, “is not research news” according to \nRogers [\n53]. In addition, these results are often not reproducible: Who can afford to \nretrain GPT-3 for 4.6 million dollars. As a consequence, the development of smaller \nbut more innovative models is less rewarding, as it is difﬁcult to beat the bigger \nmodel. Only if the authors of a new model can show that their architecture is better \nthan the previous S\nOTA model with the same number of parameters and compute \nbudget, they can claim to have made a valuable contribution. Rogers [53] proposes \nto provide a standard training corpus for a leaderboard and limit the amount of \ncomputation effort to that of a strong baseline model. As an alternative the size of \nthe training data and the computational effort should be reported and taken into \naccount in the ﬁnal score. \nAvailable Implementations \n• There are model codes and trained models for RoBERTa and ELECTRA at \nHugging Face \nhttps://huggingface.co/transformers/. \n• The code for DeBERTa is available at https://github.com/microsoft/DeBERTa \nand Hugging Face. \n• The Checklist code is at https://github.com/marcotcr/checklist.\nReferences 181\n4.3.3 Summary \nThe transferability of benchmark results to real applications is not always granted. \nEven if a PLM is better than humans at logical reasoning on the test set, it may not \nbe able to classify generated logical reasoning chains correctly. This indicates that \nthe test set does not cover the full spectrum of possible examples. It is common for \nperformance to be lower on related benchmarks because the domain or the deﬁnition \nof the task may deviate. \nThere are cases where a logical conclusion is obtained not by logical deduc-\ntion, but by a simple correlation of antecedent and consequent. This could be \ndemonstrated for the Winograd task of the GLUE benchmark. To avoid this type of \n‘reasoning’ a new variant task called Winogrande was developed where correlations \nare unrelated to the reasoning task. Meanwhile, a Foundation Model with 269B \nparameters was also able to solve this task better than humans. \nA survey on the reproducibility of results in NLP demonstrated that the published \nperformance often depends on a number of unreported effects, such as random \nnumber initialization. Often the variability of such effects is larger than the reported \nimprovement. Therefore, it is necessary to report the variance caused by these \neffects. In addition, the details of the model architecture, its training and evaluation \nshould be documented in a model card. In about 500 repeated experiments, an \nirritating rate of about 60% of ﬁnal scores were lower than reported. Note that \nimprovements due to more parameters, more training data, or higher computational \neffort are not indicative of a better model architecture. \nReferences \n1. S. Aarohi and R. Abhinav. BIG-bench · Google, June 20, 2022. URL : https://github. \ncom/google/BIG-bench/blob/936c4a5876646966344349b28ae187c556938ec4/docs/paper/ \nBIGbench.pdf (visited on 06/20/2022). \n2. M. Aßenmacher and C. Heumann. “On the Comparability of Pre-Trained Language Models”. \n2020. arXiv: 2001.00781. \n3. S. Balasubramanian, N. Jain, G. Jindal, A. Awasthi, and S. Sarawagi. “What’s in a Name? \nAre BERT Named Entity Representations Just as Good for Any Other Name?” 2020. arXiv: \n2007.06897. \n4. A. Belz, S. Agarwal, A. Shimorina, and E. Reiter. “A Systematic Review of Reproducibility \nResearch in Natural Language Processing”. Mar. 21, 2021. arXiv: 2103.07929 [cs]. \n5. S. R. Bowman and G. E. Dahl. “What Will It Take to Fix Benchmarking in Natural Language \nUnderstanding?” 2021. arXiv: 2104.02145. \n6. R. Branco, A. Branco, J. António Rodrigues, and J. R. Silva. “Shortcutted Commonsense: Data \nSpuriousness in Deep Learning of Commonsense Reasoning”. In: Proc. 2021 Conf. Empir. \nMethods Nat. Lang. Process. EMNLP 2021. Online and Punta Cana, Dominican Republic: \nAssociation for Computational Linguistics, Nov. 2021, pp. 1504–1521. \nhttps://doi.org/10. \n18653/v1/2021.emnlp-main.113. \n7. T. B. Brown et al. “Language Models Are Few-Shot Learners”. 2020. arXiv: 2005.14165. \n8. K.-W. Chang, H. He, R. Jia, and S. Singh. “Robustness and Adversarial Examples in Natural \nLanguage Processing”. In: Proc. 2021 Conf. Empir. Methods Nat. Lang. Process. Tutor.\n182 4 Knowledge Acquired by Foundation Models\nAbstr. Punta Cana, Dominican Republic & Online: Association for Computational Linguis-\ntics, Nov. 2021, pp. 22–26. URL : https://aclanthology.org/2021.emnlp-tutorials.5 (visited on \n11/24/2021). \n9. A. Chowdhery et al. “PaLM: Scaling Language Modeling with Pathways”. Apr. 5, 2022. arXiv: \n2204.02311 [cs]. \n10. P. Clark, O. Tafjord, and K. Richardson. “Transformers as Soft Reasoners over Language”. \n2020. arXiv: 2002.05867. \n11. M. Crane. “Questionable Answers in Question Answering Research: Reproducibility and \nVariability of Published Results”. In: Trans. Assoc. Comput. Linguist. 6 (2018), pp. 241–252. \n12. Y . Elazar, N. Kassner, S. Ravfogel, A. Ravichander, E. Hovy, H. Schütze, and Y . Goldberg. \n“Measuring and Improving Consistency in Pretrained Language Models”. May 29, 2021. \narXiv: 2102.01017. \n13. Y . Elazar, S. Ravfogel, A. Jacovi, and Y . Goldberg. “Amnesic Probing: Behavioral Explanation \nwith Amnesic Counterfactuals”. In: Trans. Assoc. Comput. Linguist. 9 (2021), pp. 160–175. \n14. A. Ettinger. “What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics \nfor Language Models”. In: Trans. Assoc. Comput. Linguist. 8 (2020), pp. 34–48. \n15. M. Forbes, A. Holtzman, and Y . Choi. “Do Neural Language Representations Learn Physical \nCommonsense?” 2019. arXiv: 1908.02899. \n16. M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant. “Did Aristotle Use a Laptop? \nA Question Answering Benchmark with Implicit Reasoning Strategies”. In: Trans. Assoc. \nComput. Linguist. 9 (2021), pp. 346–361. \n17. Y . Goldberg. “Assessing BERT’s Syntactic Abilities”. 2019. arXiv: 1901.05287. \n18. P. He, J. Gao, and W. Chen. “Debertav3: Improving Deberta Using Electra-Style Pre- Training \nwith Gradient-Disentangled Embedding Sharing”. 2021. arXiv: 2111.09543. \n19. P. He, X. Liu, J. Gao, and W. Chen. “DeBERTa: Decoding-enhanced BERT with Disentangled \nAttention”. Jan. 11, 2021. arXiv: 2006.03654. \n20. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. \n“Measuring Massive Multitask Language Understanding”. 2020. arXiv: 2009.03300. \n21. J. Hewitt and C. D. Manning. “A Structural Probe for Finding Syntax in Word Representa-\ntions”. In: Proc. 2019 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. \nVol. 1 Long Short Pap. 2019, pp. 4129–4138. \n22. huggingface. Building a Model Card - Hugging Face Course. 2022. URL : https://huggingface. \nco/course/chapter4/4 (visited on 08/07/2022). \n23. G. Jawahar, B. Sagot, and D. Seddah. “What Does BERT Learn about the Structure of \nLanguage?” In: 2019. \n24. A. Kamath and R. Das. “A Survey on Semantic Parsing”. 2018. arXiv: 1812.00978. \n25. N. Kassner, P. Dufter, and H. Schütze. “Multilingual LAMA: Investigating Knowledge in \nMultilingual Pretrained Language Models”. 2021. arXiv: 2102.00894. \n26. N. Kassner and H. Schütze. “Negated and Misprimed Probes for Pretrained Language Models: \nBirds Can Talk, but Cannot Fly”. 2019. arXiv: 1911.03343. \n27. T. Kim, J. Choi, D. Edmiston, and S.-g. Lee. “Are Pre-Trained Language Models Aware of \nPhrases? Simple but Strong Baselines for Grammar Induction”. 2020. arXiv: 2002.00737. \n28. B. Kirsch, S. Giesselbach, T. Schmude, M. V ölkening, F. Rostalski, and S. Rüping. “Using \nProbabilistic Soft Logic to Improve Information Extraction in the Legal Domain”. In: (2020). \n29. K. Lacker. Giving GPT-3 a Turing Test. July 6, 2020. URL : https://lacker.io/ai/2020/07/06/ \ngiving-gpt-3-a-turing-test.html (visited on 12/03/2020). \n30. A. Lazaridou et al. “Mind the Gap: Assessing Temporal Generalization in Neural Language \nModels”. In: Adv. Neural Inf. Process. Syst. 34 (2021). \n31. B. Lemoine. Is LaMDA Sentient? – An Interview. Medium. June 11, 2022. URL : https:// \ncajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917 (visited on \n06/24/2022). \n32. H. Levesque, E. Davis, and L. Morgenstern. “The Winograd Schema Challenge”. In: Thirteen. \nInt. Conf. Princ. Knowl. Represent. Reason. 2012.\nReferences 183\n33. N. F. Liu, M. Gardner, Y . Belinkov, M. E. Peters, and N. A. Smith. “Linguistic Knowledge and \nTransferability of Contextual Representations”. 2019. arXiv: 1903.08855. \n34. Y . Liu et al. “Roberta: A Robustly Optimized Bert Pretraining Approach”. 2019. arXiv: \n1907.11692. \n35. R. Manhaeve, S. Dumancic, A. Kimmig, T. Demeester, and L. De Raedt. “Deepproblog: Neural \nProbabilistic Logic Programming”. In: Adv. Neural Inf. Process. Syst. 2018, pp. 3749–3759. \n36. G. Marcus and E. Davis. GPT-3: Commonsense Reasoning. Aug. 1, 2020. URL : https://cs.nyu. \nedu/faculty/davise/papers/GPT3CompleteTests.html (visited on 02/15/2021). \n37. R. T. McCoy, E. Pavlick, and T. Linzen. “Right for the Wrong Reasons: Diagnosing Syntactic \nHeuristics in Natural Language Inference”. June 24, 2019. arXiv: 1902.01007 [cs]. \n38. S. Merity, C. Xiong, J. Bradbury, and R. Socher. “Pointer Sentinel Mixture Models”. 2016. \narXiv: 1609.07843. \n39. P. Minervini, M. Bošnjak, T. Rocktäschel, S. Riedel, and E. Grefenstette. “Differentiable \nReasoning on Large Knowledge Bases and Natural Language”. In: Proc. AAAI Conf. Artif. \nIntell. V ol. 34. 04. 2020, pp. 5182–5190. \n40. M. Mitchell et al. “Model Cards for Model Reporting”. In: Proc. Conf. Fairness Account. \nTranspar. Jan. 29, 2019, pp. 220–229. \nhttps://doi.org/10.1145/3287560.3287596.a r X i v : \n1810.03993 [cs]. \n41. M. Mitchell. What Does It Mean for AI to Understand? Quanta Magazine. Dec. 16, 2021. URL : \nhttps://www.quantamagazine.org/what-does-it-mean-for-ai-to-understand-20211216/ (visited \non 01/03/2022). \n42. N. Nangia and S. R. Bowman. “Human vs. Muppet: A Conservative Estimate of Human \nPerformance on the GLUE Benchmark”. June 1, 2019. arXiv: 1905.10425 [cs]. \n43. openai.Submissions – WinoGrande: Adversarial Winograd Schema Challenge at Scale Leader-\nboard. Jan. 5, 2022. URL : https://leaderboard.allenai.org/winogrande/submissions/public (vis-\nited on 01/05/2022). \n44. D. Paperno et al. “The LAMBADA Dataset: Word Prediction Requiring a Broad Discourse \nContext”. June 20, 2016. arXiv: 1606.06031 [cs]. \n45. Paperswithcode. Browse State-of-the-Art in AI. 2019. URL : https://paperswithcode.com/sota. \n46. F. Petroni, T. Rocktäschel, P. Lewis, A. Bakhtin, Y . Wu, A. H. Miller, and S. Riedel. “Language \nModels as Knowledge Bases?” 2019. arXiv: 1909.01066. \n47. L. Qin, A. Gupta, S. Upadhyay, L. He, Y . Choi, and M. Faruqui. “TIMEDIAL: Temporal \nCommonsense Reasoning in Dialog”. In: Proc. 59th Annu. Meet. Assoc. Comput. Linguist. 11th \nInt. Jt. Conf. Nat. Lang. Process. Vol. 1 Long Pap. ACL-IJCNLP 2021. Online: Association for \nComputational Linguistics, Aug. 2021, pp. 7066–7076. \nhttps://doi.org/10.18653/v1/2021.acl-\nlong.549. \n48. A. Radford, J. Wu, D. Amodei, D. Amodei, J. Clark, M. Brundage, and I. Sutskever. “Better \nLanguage Models and Their Implications”. In: OpenAI Blog (2019). \nURL : https://openai. \n%20com/blog/better-language-models. \n49. J. W. Rae et al. “Scaling Language Models: Methods, Analysis & Insights from Training \nGopher”. In: ArXiv Prepr. ArXiv211211446 (Dec. 8, 2021), p. 118. \n50. P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. “Squad: 100,000+ Questions for Machine \nComprehension of Text”. 2016. arXiv: 1606.05250. \n51. M. T. Ribeiro, T. Wu, C. Guestrin, and S. Singh. “Beyond Accuracy: Behavioral Testing of \nNLP Models with CheckList”. 2020. arXiv: 2005.04118. \n52. K. Richardson, H. Hu, L. Moss, and A. Sabharwal. “Probing Natural Language Inference \nModels through Semantic Fragments”. In: Proc. AAAI Conf. Artif. Intell. V ol. 34. 05. 2020, \npp. 8713–8721. \n53. A. Rogers. How the Transformers Broke NLP Leaderboards. Hacking semantics. June 30, \n2019. URL : https://hackingsemantics.xyz/2019/leaderboards/ (visited on 12/15/2021). \n54. A. Rogers, O. Kovaleva, and A. Rumshisky. “A Primer in {Bertology}: What We Know about \nHow {BERT} Works”. In: Trans. Assoc. Comput. Linguist. 8 (2021), pp. 842–866. \n55. K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y . Choi. “WinoGrande: An Adversarial \nWinograd Schema Challenge at Scale”. In: Commun. ACM 64.9 (2021), pp. 99–106.\n184 4 Knowledge Acquired by Foundation Models\n56. F. Schmidt and T. Hofmann. “BERT as a Teacher: Contextual Embeddings for Sequence- Level \nReward”. 2020. arXiv: 2003.02738. \n57. S. Seo, S. Arik, J. Yoon, X. Zhang, K. Sohn, and T. Pﬁster. “Controlling Neural Networks with \nRule Representations”. In: Adv. Neural Inf. Process. Syst. 34 (2021). \n58. M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. “Megatron-\nLm: Training Multi-Billion Parameter Language Models Using Model Parallelism”. In: arXiv \n(2019), arXiv-1909. \n59. K. Sinha, R. Jia, D. Hupkes, J. Pineau, A. Williams, and D. Kiela. “Masked Language Modeling \nand the Distributional Hypothesis: Order Word Matters Pre-training for Little”. Apr. 14, 2021. \narXiv: 2104.06644. \n60. J. Sohl-Dickstein. BIG-bench. Google, Dec. 16, 2021. URL : https://github.com/google/ \nBIGbench (visited on 12/16/2021). \n61. M. Sparkes. Google Wants to Challenge AI with 200 Tasks to Replace the Turing Test.N e w \nScientist. June 14, 2022. URL : https://www.newscientist.com/article/2323685-google-wantsto-\nchallenge-ai-with-200-tasks-to-replace-the-turing-test/ (visited on 06/26/2022). \n62. S. Storks, Q. Gao, and J. Y . Chai. “Commonsense Reasoning for Natural Language Under-\nstanding: A Survey of Benchmarks, Resources, and Approaches”. 2019. arXiv: 1904.01172. \n63. A. Talman and S. Chatzikyriakidis. “Testing the Generalization Power of Neural Network \nModels Across NLI Benchmarks”. May 31, 2019. arXiv: 1810.09774. \n64. A. Talmor, O. Tafjord, P. Clark, Y . Goldberg, and J. Berant. “Teaching Pre-Trained Models to \nSystematically Reason over Implicit Knowledge”. 2020. arXiv: 2006.06609. \n65. TrustworthyAI, director. CVPR 2021 Tutorial on ”Practical Adversarial Robustness in Deep \nLearning: Problems and Solutions”. June 28, 2021. URL : https://www.youtube.com/watch?v= \nZmkU1YO4X7U (visited on 02/26/2022). \n66. Wang. SuperGLUE Benchmark. SuperGLUE Benchmark. 2021. URL : https://super. \ngluebenchmark.com/ (visited on 02/23/2021). \n67. A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. “Glue: A Multi-Task \nBenchmark and Analysis Platform for Natural Language Understanding”. Feb. 22, 2019. arXiv: \n1804.07461. \n68. A. Wang et al. “Superglue: A Stickier Benchmark for General-Purpose Language Understand-\ning Systems”. In: Adv. Neural Inf. Process. Syst. 2019, pp. 3266–3280. \n69. X. Wang et al. “Self-Consistency Improves Chain of Thought Reasoning in Language Models”. \nApr. 6, 2022. arXiv: 2203.11171 [cs]. \n70. L. Weber, P. Minervini, J. Münchmeyer, U. Leser, and T. Rocktäschel. “Nlprolog: Rea-\nsoning with Weak Uniﬁcation for Question Answering in Natural Language”. 2019. arXiv: \n1906.06187. \n71. E. Zelikman, Y . Wu, and N. D. Goodman. “STaR: Bootstrapping Reasoning With Reasoning”. \nMar. 27, 2022. arXiv: 2203.14465 [cs]. \n72. X. Zhou, Y . Zhang, L. Cui, and D. Huang. “Evaluating Commonsense in Pre-Trained Language \nModels.” In: AAAI. 2020, pp. 9733–9740. \n73. B. Zoph et al. “Designing Effective Sparse Expert Models”. 2022. arXiv: 2202.08906.\nReferences 185\nOpen Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 \nInternational License ( http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate \ncredit to the original author(s) and the source, provide a link to the Creative Commons license and \nindicate if changes were made. \nThe images or other third party material in this chapter are included in the chapter’s Creative \nCommons license, unless indicated otherwise in a credit line to the material. If material is not \nincluded in the chapter’s Creative Commons license and your intended use is not permitted by \nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder.",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.8956334590911865
    },
    {
      "name": "Computer science",
      "score": 0.7009018063545227
    },
    {
      "name": "Natural language processing",
      "score": 0.5711946487426758
    },
    {
      "name": "Test (biology)",
      "score": 0.5357956290245056
    },
    {
      "name": "Artificial intelligence",
      "score": 0.525519609451294
    },
    {
      "name": "Machine learning",
      "score": 0.35830748081207275
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210144576",
      "name": "Fraunhofer Institute for Intelligent Analysis and Information Systems",
      "country": "DE"
    }
  ]
}