{
    "title": "Language model acceptability judgements are not always robust to context",
    "url": "https://openalex.org/W4385569782",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5109576489",
            "name": "Koustuv Sinha",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5048080768",
            "name": "Jon Gauthier",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5020998070",
            "name": "Aaron Mueller",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5087402268",
            "name": "Kanishka Misra",
            "affiliations": [
                "Purdue University West Lafayette"
            ]
        },
        {
            "id": "https://openalex.org/A5082485513",
            "name": "Keren Fuentes",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5090215557",
            "name": "Roger Lévy",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5062696000",
            "name": "Adina Williams",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1991294536",
        "https://openalex.org/W4385573607",
        "https://openalex.org/W3034510440",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W3198507920",
        "https://openalex.org/W4307536702",
        "https://openalex.org/W2888922637",
        "https://openalex.org/W2020046942",
        "https://openalex.org/W1988727136",
        "https://openalex.org/W2929581986",
        "https://openalex.org/W3106531402",
        "https://openalex.org/W4386576705",
        "https://openalex.org/W4303648884",
        "https://openalex.org/W3172415559",
        "https://openalex.org/W2549835527",
        "https://openalex.org/W4312107601",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W4285594979",
        "https://openalex.org/W2516090925",
        "https://openalex.org/W3103816537",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4285174271",
        "https://openalex.org/W3173777717",
        "https://openalex.org/W3017374003",
        "https://openalex.org/W4221154823",
        "https://openalex.org/W3156470785",
        "https://openalex.org/W3035599593",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4385574286",
        "https://openalex.org/W2986128786",
        "https://openalex.org/W2962926715",
        "https://openalex.org/W3105882417",
        "https://openalex.org/W2735930646",
        "https://openalex.org/W3034995113",
        "https://openalex.org/W3035064549"
    ],
    "abstract": "Koustuv Sinha, Jon Gauthier, Aaron Mueller, Kanishka Misra, Keren Fuentes, Roger Levy, Adina Williams. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 6043–6063\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nLanguage model acceptability judgements are not always robust to context\nKoustuv Sinha ∗,∞ Jon Gauthier ∗,1\nAaron Mueller †,3 Kanishka Misra †,2 Keren Fuentes ∞\nRoger Levy 1 Adina Williams ∞\n∞Meta AI; 1MIT 2Purdue University 3Johns Hopkins\n∗, †Equal contributions\nkoustuvs@meta.com, jon@gauthiers.net\nAbstract\nTargeted syntactic evaluations of language mod-\nels ask whether models show stable prefer-\nences for syntactically acceptable content over\nminimal-pair unacceptable inputs. Our best\nsyntactic evaluation datasets, however, provide\nsubstantially less linguistic context than mod-\nels receive during pretraining. This mismatch\nraises an important question: how robust are\nmodels’ syntactic judgements across different\ncontexts? In this paper, we vary the input con-\ntexts based on: length, the types of syntac-\ntic phenomena it contains, and whether or not\nthere are grammatical violations. We find that\nmodel judgements are generally robust when\nplaced in randomly sampled linguistic contexts,\nbut are unstable when contexts match the test\nstimuli in syntactic structure. Among all tested\nmodels (GPT-2 and five variants of OPT), we\nfind that model performance is affected when\nwe provided contexts with matching syntactic\nstructure: performance significantly improves\nwhen contexts are acceptable, and it signifi-\ncantly declines when they are unacceptable.\nThis effect is amplified by the length of the con-\ntext, except for unrelated inputs. We show that\nthese changes in model performance are not ex-\nplainable by acceptability-preserving syntactic\nperturbations. This sensitivity to highly spe-\ncific syntactic features of the context can only\nbe explained by the models’ implicit in-context\nlearning abilities.\n1 Introduction\nThe unprecedented progress in the development\nof neural large language models (LLMs; Devlin\net al., 2019; Radford et al., 2019; Brown et al.,\n2020; Zhang et al., 2022) has been accompanied by\na comparable proliferation of methods that aim to\nbetter understand and characterize models’ linguis-\ntic capacities (Linzen et al., 2016; Ettinger et al.,\n2016; Alishahi et al., 2019; Hu et al., 2020; Jeretic\net al., 2020; Mueller et al., 2020, i.a.). Of the many\nmethods for this, the minimal-pair paradigm (MPP ),\nSubj. Verb. Agreement\n...\n...\n...\n...\nSpace of Candidate Prefixes\nMatched Mismatched\nTest Suite: Subject Verb Agreement\nsample\n= Island Effects\n= Filler-gaps\n= Binding\n... = ...\nacceptable unacceptable\n= Wikipedia\nFigure 1: We measure the impact of different contexts\non the performance of an LM on linguistic acceptability\ntasks by prefixing sentences (here, sourced from subject-\nverb agreement challenge sets) from a diverse sources.\nEach block represents a sentence: Red striped blocks\nare unacceptable sentences within a given task, while\ngreen solid ones are acceptable. We also vary the diver-\nsity of prefixes by sampling them from tasks/datasets\ndifferent from the test suite (indicated by shape).\nwhich is methodologically standard in linguistics,\nhas emerged as a popular approach to evaluate mod-\nels’ knowledge of linguistic phenomena in an unsu-\npervised manner (Marvin and Linzen, 2018; Kann\net al., 2019; Warstadt et al., 2019, 2020a; Misra\net al., 2023). Under the MPP , models are presented\nwith datasets containing pairs of minimally differ-\ning text sequences—usually differing in word order\nor in a few tokens—one of which is deemed by hu-\nmans to be acceptable and the other unacceptable.\nDrawing on the LLMs’ trained ability to produce\nprobabilities over token sequences, we can evalu-\nate them according to the MPP by testing whether\nmodels assign relatively greater probability to the\nacceptable sequence.\nStudies that employ MPP datasets generally com-\npare the probability of two stand-alone text se-\nquences without any explicit linguistic context.\nHowever, this is not a naturalistic or realistic ap-\n6043\nproach: utterances usually occur in some linguistic\ncontext, where the context itself could affect lin-\nguistic preferences. The syntactic priming litera-\nture investigates the effect of linguistic contexts to\nsome extent, but mostly in a constrained setting\nwith only one or a small number of context sen-\ntences (van Schijndel and Linzen, 2018; Prasad\net al., 2019). The interaction of context with min-\nimal pair accuracies remains underexplored for\nmulti-sentence contexts, despite the fact that multi-\nsentence inputs are more likely for many NLP\ntasks—especially with the rise of prompting and\nin-context learning (Brown et al., 2020; Schick and\nSchütze, 2021b). Furthermore, Transformer-based\nlanguage models are typically trained on large se-\nquences, where masked tokens are predicted given\na completely full context window, consisting of\nmany sentences. It is unclear how to evaluate MPP\nby utilizing this context window, given recent re-\nsearch that has raised questions about the sentence\nrepresentations acquired in long-form input (Sinha\net al., 2022; Haviv et al., 2022).\nWe evaluate the sensitivity of LLMs’ acceptabil-\nity preferences in a more realistic evaluation setting,\nwith one or more additional sentences in the input\ncontext. We focus on LLM sensitivity to three par-\nticular features of the context: (1) the length of\nthe input sequence, (2) the similarity of the context\nto the minimal pair being judged, and (3) whether\nthe context itself contains acceptability violations.\nFigure 1 illustrates our method at a high level: For\na given MPP dataset (BLiMP, Warstadt et al. 2020a\nand SyntaxGym, Hu et al. 2020), we generate new\nminimal pair test examples for a given syntactic\nphenomenon by artificially simulating a long con-\ntext window. Specifically, we prepend the given\ntest example pair with sentences drawn by the axis\nof similarity, from unrelated (Wikipedia), minimal-\npair sentences from different (mismatched) or the\nsame (matched) syntactic phenomena in the MPP\ndataset. We also introduce violations in the context\nby drawing unacceptable counterparts of the above\nsimilarity scale from the MPP dataset.\nWe find that the model’s judgements are highly\nrobust to the presence of unrelated Wikipedia sen-\ntences in the context, regardless of the size of the\nprefix. However, we observe strong sensitivity to\nmatched context manipulations. As the context\nlength increases, acceptable matched contexts im-\nprove the models’ judgements significantly. Con-\nversely, we observe a strong opposite effect of ex-\nposing the model to longer and longer prefixes\ncontaining acceptability violations: models’ judge-\nments degrade drastically, performing far below\nchance. This sensitivity is specific to the partic-\nular type of syntactic structural similarity of the\ncontext: we do not see the same degree of improve-\nment/degradation in prediction behavior for con-\ntexts consisting of mismatched sentences of valid\nor violated syntactic structures.\nTo better understand our results, we performed\nseveral exploratory analyses. To determine whether\nthe results are an effect of the acceptability judge-\nment task, we replicated our experiments for an-\nother task, that of stereotypicality judgements (Nan-\ngia et al., 2020), and found largely concurring re-\nsults. We also investigated the syntactic overlap be-\ntween the context and the test pair, and observe only\nminor effects on the judgements with phenomena-\npreserving syntactic perturbations. Our results,\ntherefore, can only be explained by the model\ndisplaying some kind of implicit, instruction-free,\nin-context learning ability, and they invite further\nscrutiny of and investigation into long-form sen-\ntence understanding capabilities of LLMs.\n2 Background\nSequence Length and Out-of-domain General-\nization. When evaluating language models’ lin-\nguistic abilities in particular, one ought to addition-\nally consider the domain of the test data fed into the\nmodel, as it can have large consequences for model\nperformance if it mismatches from the model train-\ning data. Length mismatches are quite common in\nNLP datasets. For example, MPP test sequences are\nconsiderably shorter than that of the inputs LLMs\ntypically receive during pre-training (≈512–1024\ntokens)—the test pairs in standard MPP datasets for\nthe linguistic acceptability task, for example, are\n≈4–30 tokens in the case of BLiMP. It is also rela-\ntively well established that mismatching sequence\nlengths between (pre-)training and testing scenar-\nios can affect performance (Hupkes et al., 2020;\nNewman et al., 2020; Varis and Bojar, 2021; Hup-\nkes et al., 2022), raising the question: how much\ndoes test sequence length impact our measurements\nof model performance onMPP datasets? We contex-\ntualize LLMs’ performance on acceptability judge-\nments against work in length extrapolation, and an-\nalyze generalization during test time to both shorter\nand longer sequences.\n6044\nPriming Language Models. Recent work has\nexplored the effects of providing additional linguis-\ntic context to LLMs by “priming” or prepending\ntheir inputs with words/sentences.1 For instance,\nMisra et al. (2020) and Kassner and Schütze (2020)\nshow LLMs’ behave in ways that are reminiscent\nof semantic priming, assigning greater probabili-\nties to words that were semantically related to their\nwords/sentence prefixes. More recently, Sinclair\net al. (2022) used a priming paradigm to measure\nthe probability assigned by LLMs to sentences pre-\nfixed with well-formed but structurally different\nsentences. They found that several autoregressive\nLLMs assign greater probability to sentences that\nare similar in structure to their prefixes across a\nnumber of diverse constructions, thereby demon-\nstrating a pattern analogous to what is known\nin psycholinguistics as structural priming (Bock,\n1986; Pickering and Ferreira, 2008). Together with\nthe findings of van Schijndel and Linzen (2018);\nPrasad et al. (2019), these works suggests that\nLLMs may represent at least some of the relevant\nstructural similarities between sentences, and that\ntheir word predictions could reflect an expectation\nof repeating structures. While these methods do\nnot focus on length per se, their manipulation of\nthe input context is necessarily accompanied by an\nincrease in length. This leaves open the question\nas to how structural properties of the context may\ninteract with varying levels of input lengths.\nIn-context Learning. A practical application of\nthe priming paradigm is that it can be used to elicit\nlearning behavior in LLMs. That is, LLMs can be\nprimed using labelled task demonstrations (Brown\net al., 2020), instructions/explanations (Lampinen\net al., 2022, though see Webson and Pavlick., 2022),\nor a combination of the two (Wei et al., 2022; Ko-\njima et al., 2022) as supervision for tasks such as\nsentiment analysis or reasoning. This suggests that\nLLMs seem to be able to extract higher-level infor-\nmation from their context when processing a new\ntest example from a supervised task. Our approach\ncontributes to this body of work by testing whether\nLLMs can also extract more abstract features, such\nas grammaticality or stereotypicality, given enough\npriming examples.\n1This is related to but differs from the operationalization\nof priming as finetuning/adaptation as developed by van Schi-\njndel and Linzen (2018); Prasad et al. (2019).\n3 Approach\nTerminology. We follow standard practice in\nMPP , where we evaluate the preference (P) of a\nlanguage model M towards acceptable sentence\n(x) over its unacceptable counterpart (x′), with re-\nspect to log-likelihood, and compute the value over\nthe full evaluation dataset D. Dtypically consists\nof several test suites, each of which instantiates\na particular linguistic phenomenon. We denote\nthe particular test suite under evaluation as the tar-\nget suite: S ⊂D. Each target suite consists of\nkpairs of acceptable and unacceptable sentences,\n(x,x′)k\ni=1 ∈S, and may have multiple conditions.\nWithin each target suite, we compute the accept-\nability judgements on one or more experimental\nconditions, comparing a given LM’s log-likelihood\npreference Pfor the acceptable and unacceptable\nsentence in each condition. The accuracy (A) over\na test pair from a single condition is defined as:\nA(xi,x′\ni) = 1 [P(xi) >P(x′\ni)], (1)\nwhere 1 is the indicator function which returns 1\nif the inequality is satisfied and 0 otherwise. De-\npending on the dataset, it can have either one or\nmultiple conditions evaluated for each test item.\nTo simulate increasing length of input, we\nprepend a prefix sequence cto both xand x′, and\ncompute the preferences over the concatenated se-\nquence, P([c,xi]) and P([c,x′\ni]), where ccan be\narbitrarily long.\nDatasets. We focus on the standard targeted syn-\ntactic evaluation datasets of BLiMP (Warstadt et al.,\n2020a, licensed CC-BY) and SyntaxGym (Hu et al.,\n2020, MIT license). BLiMP is a large-scale MPP\ndataset consisting of 67 different subsets of 1000\nEnglish sentence pairs each. Each BLiMP subset\ntargets a particular linguistic paradigm that belongs\nto 12 different overarching linguistic phenomena—\nfor instance, subject-verb agreement, argument\nstructure, etc. SyntaxGym is a syntactic evaluation\nbenchmark designed with more stringent evalua-\ntion criteria. For 34 different linguistic phenom-\nena, the SyntaxGym benchmark defines test items\nwith two to four different conditions, consisting\nof minimal structural variations on the same sen-\ntence which render the sentence either grammati-\ncal or ungrammatical. Model log-likelihoods are\nmeasured at a critical region within each sentence,\nrather than across the whole sentence, and models\nare expected to produce log-likelihoods that satisfy\n6045\nmultiple inequalities across all conditions. Syntax-\nGym is smaller than BLiMP (with about 20 items\nper phenomenon on average) and all of the exam-\nples are hand-written. We adapt 23 of the 34 test\nparadigms in SyntaxGym whose structure was com-\npatible with the prefixing analyses of this paper.2\nThese two datasets offer complementary value to\nthe analyses in this paper: BLiMP’s large scale\nallows us to make general conclusions about the\naverage effect of prefix interventions, while Syn-\ntaxGym’s stringent evaluation allows us to verify\nthat the effects are sustained under more rigorous\nexperimental conditions.\nTo better understand whether our results are spe-\ncific to syntactic evaluation MPP datasets, we also\nreplicate a portion of our experiments using the\nCrowS-Pairs dataset for stereotype evaluation (Nan-\ngia et al., 2020, licensed CC-BY-SA). CrowS-Pairs\nexamples fall into 9 bias types (e.g., race, gen-\nder, age) and consist of minimal pairs with one\nstereotypical sentence and one less stereotypical\nsentence about a historically disadvantaged group.\nWe view the bias types in CrowS-Pairs as analo-\ngous to particular linguistic test suites in BLiMP\nor SyntaxGym for the purposes of our replication:\nwe re-code “less-stereotypical” as “acceptable” and\n“more-stereotypical” as “unacceptable”.3 More dis-\ncussion of the dataset and further methodological\ninformation is provided in Appendix A.\nMethod. We compute the log-likelihood of the\ngiven input using the minicons library (Misra,\n2022),4 which is based on huggingface (Wolf\net al., 2020). For each dataset D, we first compute\nthe baseline acceptability accuracy according to\nEquation 1. Next, we re-evaluate the acceptability\naccuracy as we steadily increase the token length\nof the input. Following prior work on priming (§2),\nwe analyze how prepending the test examples with\nadditional context affects a given model’s accept-\nability judgements.\nTo increase the token length while maintaining\nthe MPP formulation, we introduce a context cby\nprepending the same sequence to each target x\nand x′ in S. To construct a context c, we sam-\n2See Appendix F for more technical details on the Syntax-\nGym analysis.\n3Our definition of “unacceptable” for the CrowS-Pairs\ndoes not imply grammatically ill-formed, but instead it implies\nsocially inappropriate. We are aware that recoding in this way\ndoes some terminological violence to the well established\npsycholinguistic term (un-)acceptable (c.f. Chomsky 1965;\nSchütze 1996), which we chose to do for reasons of space.\n4https://github.com/kanishkamisra/minicons\nple from several possible sources (acceptable sen-\ntences, unacceptable sentences, and control sen-\ntences) discussed below. We also gradually in-\ncrease the length of the contextcby sampling multi-\nple sentences from a known set, and concatenating\nthem with periods and spaces as delimiters.\nNext, we recompute the log-likelihood over\nthe stimuli ( x or x′) by conditioning on c, i.e.,\nP([c,xi]) = log p(xi |c).5 For each item pair\n(xi,x′\ni) in target suite S ∈D, we first sample ac-\nceptable sentences to construct context c as fol-\nlows:\n• Matched: Contexts are sampled from the same\ntest suite (or bias type) as the target suite S:\nx,c ∈S |x̸= c.\n• Mismatched: Contexts are sampled outside\nthe target suite (or bias type) S: x ∈S,c ∈\nD|c /∈S.\nFor each x ∈ S, we construct the context c by\nsampling N sentences (without replacement) from\neach group, concatenating them, until the input\nreaches 1000 tokens.6\nTraditionally, most work on priming has only\nconsidered grammatically acceptable sentences as\nthe context. While there has been some work on\nsyntactic priming in humans showing they can\nbe primed with ungrammatical sentences to pro-\nduce other ungrammatical sentences (Kaschak and\nGlenberg, 2004; Pickering and Garrod, 2017; Yang\nand Stocco, 2019), there is little evidence in the\nNLP literature about how a model would react to\ngrammatically unacceptable sentences in the in-\nput. Therefore, we perform our evaluation on both\nacceptable prefixes (c∈x) and unacceptable pre-\nfixes (c ∈x′), drawn from the same phenomena\n(matched, c ∈S) or from a different phenomena\n(mismatched, c /∈S).\nFor evaluation, we compute the ∆ accuracy of\nacceptability judgements for each model:\n1\n|D|\n|D|∑\ni\nA([c,xi],[c, ˆxi]) − 1\n|D|\n|D|∑\ni\nA(xi, ˆxi),\n(2)\nwhere |D|is the total number of samples in a given\ndataset (D). Taking this difference allows us to\nquantify the precise contribution (in terms of the\n5Since c is held constant for every item, the difference\nin the conditional measure is equivalent to that in the full\nsequence log-likelihood.\n6Since GPT and OPT models have a context window of\n1024 tokens, we investigate 1000 tokens as an approximate.\n6046\nBLiMP SyntaxGym CrowS-Pairs\nGPT2OPT (6.7B)\n0 200 400 600 800 0 50 100 150 200 250 0 200 400 600 800\n0 200 400 600 800 0 50 100 150 200 250 0 200 400 600 800\n-0.8\n-0.6\n-0.4\n-0.2\n0.0\n0.2\n-0.8\n-0.6\n-0.4\n-0.2\n0.0\n0.2\nInput Length\nΔ Accuracy\nPrefix Strategy\nAcc. (Matched)\nAcc. (Mismatched)\nUnacc. (Matched)\nUnacc. (Mismatched)\nWiki (Mismatched)\nFigure 2: Prefixing type affects model performance (∆ Accuracy) for GPT2 and OPT (6.7B) on BLiMP, SyntaxGym\nand CrowS-Pairs datasets. Longer prefixes tend to elicit performance enhancement, an effect which is modulated by\nwhether the prefixes are acceptable, and whether the prefixes match the test suite/bias.\ngain or loss in accuracy of the LM on the accept-\nability task) of the priming contexts (c), which are\nheld constant for a given pair of test samples. It\nfurther allows us to report a unified measure across\nour systematic manipulations of the context.\nModels. We study autoregressive language mod-\nels at varying sizes—we consider GPT2 (small,\n124M parameters) (Radford et al., 2018), and a sub-\nset of the OPT family (125M, 350M, 1.3B, 2.7B\nand 6.7B parameters; Zhang et al. 2022).\nControl. While we define matched and mis-\nmatched with respect to the phenomena or bias\ntype provided by the dataset (target suite, S), we\nare still in the regime of in-distribution prefix sen-\ntences, as the context is drawn from the same MPP\ndataset. By design, these sentences are lexically\nconstrained, and constructed to be as simple as pos-\nsible while still testing for the relevant phenomena.\nTo simulate an out-of-distribution context relative\nto the BLiMP/SyntaxGym test examples, we sam-\nple prefix sentences from a completely unrelated\nWikipedia domain, the WikiText-103 test set (Mer-\nity et al., 2017).\nRegression Analysis. We define and test our\nclaims about the effect of length on acceptability\nwith a mixed-effects logistic regression for each\ncombination of model and dataset. The regression\npredicts a model’s acceptability judgement accu-\nracy for a given phenomenon as a function of the\nthree previously introduced properties of the prefix\nc: its length, whether it is matched or mismatched,\nand its acceptability. The model includes a three-\nway interaction term and all lower-order terms for\nthese variables, with sum-coded categorical vari-\nables and log-transformed prefix lengths, along\nwith a random intercept term for the phenomenon\n(controlling for variation in baseline accuracies per\nphenomenon).\n4 Main Results\nFigure 2 presents the summary results of our prefix-\ning manipulation, charting models’ accuracy on\nMPP evaluations as a function of the prefix (1)\nlength (x-axis), (2) acceptability (teal and orange vs.\nred and purple), and (3) whether it is drawn from\na domain that is matched (teal and purple), mis-\nmatched (orange and red), or unrelated Wikipedia\n(light green). We further explore the main qualita-\ntive findings in the following paragraphs, plotting\nresults on the BLiMP dataset for simplicity. De-\ntailed results on SyntaxGym and CrowS-Pairs are\n6047\navailable in Appendix F and A, respectively.\nModel acceptability judgements are largely robust\nacross lengths—for unrelated, control prefixes.\nWe first investigate the impact of increasing con-\ntext length on model acceptability judgement per-\nformance. We start with the control case defined\nin §3, simulating lengthy context windows with no\nother notable grammatical properties by drawing\nsentences from Wikipedia, an out-of-distribution\ntext domain for the target MPP datasets. As we in-\ncrease the context length, acceptability judgement\nresults do not significantly change (Figure 3, long\ndashed lines), suggesting that LMs, in general, are\nvery robust to unrelated changes in their context\nwindow. Quantitatively, no main effect of prefix\nlength is significant ( p >0.2 for all models) for\nWikipedia sentences.\nThe length of the context matters when the prefix\nis related to the acceptability task. We next inves-\ntigate the effect of long context on acceptability by\ndrawing prefixes that are in-distribution (from the\nsame MPP dataset). As prefix length grows, model\nperformance on average changes monotonically\nfrom baseline accuracy (Figure 2: rising for ac-\nceptable sentences, falling for unacceptable ones).\nWhen the prefix consists of acceptable sentences\n(teal, orange) for example, ∆ accuracy increases\nup to 10–20 percentage points for all datasets, and\nmostly uniformly across all model sizes. However,\nunacceptable prefixes (purple, red) elicit the op-\nposite effect: ∆ accuracy falls as context context\nlength grows (Figure 2, dashed lines).\nScale amplifies this effect only for unacceptable\nsentences (Figure 3). For example, OPT 6.7B suf-\nfers the largest degradation of acceptability task\naccuracy with increasing length of ungrammatical\ncontext, compared to GPT2. Surprisingly, GPT2 re-\ncovers some percentage of the degradation on very\nlong sequences, while also showing attenuated the\nmatched gains. We speculate that this effect de-\nrives from a relative weakness of GPT2 to learn\nin-context, as it is trained on markedly less data\n(8B tokens, as estimated by Warstadt et al.) than\nmodels from the OPT family (180B tokens).\nQuantitatively, this interaction between prefix\nlength and acceptability is highly significant for all\nmodels and evaluations (p< 0.002 for all models\non BLiMP and SyntaxGym). Overall, we observe\nlength can influence LM’s acceptability judgement\nperformance for in-distribution contexts, and more\n-0.4\n-0.3\n-0.2\n-0.1\n0.0\n0.1\n0 200 400 600 800\nInput Length\nΔ Accuracy\nModel\nGPT2\nOPT (125M)\nOPT (350M)\nOPT (1.3B)\nOPT (2.7B)\nOPT (6.7B)\nPrefix Type\nAcceptable\nUnacceptable\nWiki\nFigure 3: Interaction of length and prefix type on\nBLiMP (collapsed across match/mismatch). Across\nall tested models, accuracy improves for acceptable pre-\nfixes and worsens for unacceptable ones, as length in-\ncreases (p< 10−11 for all models). Shaded regions are\nthe 95% confidence interval.\nso when the contexts contain acceptability viola-\ntions. One possible driver for these results could\nbe that longer contexts are more conducive to large\nLMs’ in-context learning abilities, and mimic their\nk-shot learning scenario. This would mean that the\nlength of preceding context matters only insofar as\nlength is a proxy for the number of acceptable (or\nunacceptable, with an opposite effect) matched pre-\nfixes in the context (see §5 for a related analysis).\nMatched context impacts acceptability judge-\nments more than mismatched contexts. We now\ndig into the interaction between length and accept-\nability, investigating whether the magnitude of the\neffect is modulated by whether the phenomena are\nmatched or not. In case of BLiMP, the average\neffect of acceptable prefixes is ≤12 ∆ accuracy\npoints (Figure 3). However, matched prefixes drive\nthis improvement more (∆ ≥15) than mismatched\nones ( ∆ ≤ 5) (Figure 4, left subfigure). Con-\nversely, while the average effect of unacceptable\nprefixes is between 30–40 ∆ accuracy points (Fig-\nure 3), this too is more heavily impacted by the\neffect of matched prefixes (50 ≤∆ ≤80) than by\nmismatched ones (∆ ≤20) (Figure 4, right sub-\nfigure). These effects manifest quantitatively in a\nthree-way interaction between prefix (un-)accept-\nability, (mis-)match, and length (p< 0.007 for all\nmodels on BLiMP and SyntaxGym).\nThe effects of unacceptable prefixes are ampli-\nfied substantially when they are consistent—i.e.,\nwhen they violate the grammatical rules (of En-\nglish) in the same way (matched), as opposed to\n6048\nAcceptable Unacceptable\n0 200 400 600 800 0 200 400 600 800\n-0.8\n-0.6\n-0.4\n-0.2\n0.0\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nInput Length\nΔ Accuracy\nModel\nGPT2\nOPT (125M)\nOPT (350M)\nOPT (1.3B)\nOPT (2.7B)\nOPT (6.7B) Prefix Suite\nMatched\nMismatched\nFigure 4: Interaction of length and prefix suite (matched\nversus mismatched) in two separate prefix types (accept-\nable/unacceptable) in BLiMP.\nin more diverse ways (mismatched). 7 These re-\nsults could explain why in-context learning ability\nworks: perhaps prepending contexts that are syntac-\ntic similarity can help the model learn or unlearn\nacceptability at a higher rate.\n5 Prefix Similarity Analysis\nWe have observed that length effects on accept-\nability judgements are conditional on the similar-\nity between the prefix phenomenon and the test\nphenomenon. However, we have only analyzed\nprefixes that are either very similar (i.e. contain\npredominantly the same abstract syntactic struc-\nture as the test sentence, matched prefixing), or\nare almost entirely unrelated (mismatched prefix-\ning, or unrelated prefixing such as Wikipedia). This\nleads us to wonder about the nature of the similarity\ndriving our results thus far: are the models respond-\ning to the presence of shared syntactic structure in\nthe prefix? Or are they responding to something\nmore shallow and brittle, such as the exact match\nin sentence templates between the prefix content\nand the test? If the former is true, we should see a\nsmooth relationship between prefix syntactic simi-\nlarity and length effects, such that slight changes in\nthe syntactic structure of the prefix content results\nin similarly slight modulations of length effects.\nIf models are using more shallow template-based\ncomparisons between the prefix content and the\ntest content, we might see a more discontinuous\nresponse, in which even small changes to prefix\ncontent result in large changes in length effects.\nTo test this, we narrow our focus to the top 20\n7Note, however, (i) we assumed that all Wikipedia sen-\ntences are acceptable, and (ii) we found that acceptable pre-\nfixes have a generally weaker effect on the acceptability task.\nWere we to test unacceptable Wikipedia sentences as well, we\nmight expect a small priming effect.\n-0.75\n-0.50\n-0.25\n0.00\n0.25\n0.50\n0 100 200 300 400 500 600\nInput Length\nΔ Accuracy\nPerturbation\nNone\nPrefix/suffix adv\nLong prefix adv\nAdd clause\nQuote\nAll\nPrefix Type\nAcceptable\nUnacceptable\nFigure 5: Perturbation analysis on OPT 6.7B, using\nBLiMP prefixes.\nBLiMP phenomena which responded most strongly\nto matched prefixing in our previous analyses.8 We\nperform controlled perturbations on each prefix\nsentence cthat preserve the presence of the origi-\nnal syntactic structure, but incorporate mild struc-\ntural variations or additions. These perturbations\nincrease prefix length and shift the position of cer-\ntain tokens (e.g., the main verb) increlative to their\ncounterparts in the test sentence. This enables us\nto test whether the models are merely learning to\nassociate fragile token-position pairings between\nthe prefix and test sentences, or whether they are\nrelying on relevant abstract syntactic information.\nWe leave the test sentence xunchanged.\nOur perturbations include the following, all of\nwhich preserve both the grammaticality and the rel-\nevant overarching syntactic structure of the BLiMP\nphenomena:\n• Prefix/suffix adverbs: add a single-word sen-\ntential adverb to the start or end of the sen-\ntence (e.g., “However, c.”).\n• Long prefix adverb: add an adverbial phrase\nto the start of the sentence (e.g., “First and\nforemost, c.”).\n• Add clause: Add a dependent clause to the\nstart or end of the sentence (e.g., “Regardless\nof what {NAME} thinks about it, c.”)\n• Quote: Embed the sentence in a quotation\n(e.g., “Yesterday, {NAME} said, ‘c.”’).\nWe also combine all of these strategies into a single\nlarge perturbation, referred to as All.9\n8We selected the phenomena which showed the greatest\nchange in accuracy (averaged across models) between their\nbaseline accuracy and their accuracy after matched prefixing at\nthe greatest lengths tested in the analysis (Appendix Table 1).\n9We exclude short prefix adverbs from the All perturbation\nin favor of long prefix adverbs. Combining these sometimes\n6049\nOur findings (Figure 5) show that minor per-\nturbation of the prefixes results in only very mi-\nnor reductions to length effects, suggesting that\nmatched prefixing effects do not require identically\nstructured prefixes. Increasingly aggressive pertur-\nbations result in increasing (if small) reductions\nto ∆ accuracy magnitudes, especially when using\nfewer prefixes. We correlate ∆ accuracies with the\nmean similarity of prefix sentences before and after\na perturbation, where similarity is an ordinal vari-\nable assigned to each perturbation based on how\nmany tokens it adds to the sentence; see App. B\nfor details. The Spearman rank-order correlation\n(ρs = 0.93, p<. 001) is significantly positive for\nacceptable prefixes; it is weaker but still signifi-\ncantly negative (ρs = −0.7, p<. 05) for unaccept-\nable prefixes. Thus, there is a smooth relationship\nbetween prefix similarity and length effects.\nThis perturbation analysis shows that model\njudgments are mostly robust to syntactic variations\nin the prefix content, with a smooth relationship\nbetween degrees of syntactic variation and model\nperformance. Appendix D investigates whether\nthese similarity effects can be described in terms\nof lexical overlap or matches in low-level syntactic\nfeatures between the prefix and test content; we\nfind no clear relationship between these low-level\nfeatures and models’ acceptability judgment perfor-\nmance. Taken together, these results suggest that\nthe changes we observe in models’ acceptability\njudgments are likely due to an abstract comparison\nbetween structural features of the prefix content\nand test content. In other words, language mod-\nels are sensitive to latent syntactic features, and\nthe syntactic similarity of the context to the test\nexamples.\n6 Discussion\nShort and single-sentence inputs may not be\nrepresentative of language models’ true abili-\nties. Our results have implications for interpret-\ning results from MPP benchmark datasets, as these\ndatasets often consist of shorter inputs that are not\nwhat many pre-trained language models expect,\ngiven that their pre-training procedures often entail\npacking many sentences into a single training ex-\nample (Brown et al., 2020; Liu et al., 2019). This\nstrengthens prior findings showing that reformat-\nting train and test inputs in a way that more closely\nresembles the pre-training setup can boost perfor-\nresults in unacceptable sentences.\nmance (Hupkes et al., 2020; Newman et al., 2020;\nVaris and Bojar, 2021; Chada and Natarajan, 2021).\nMore broadly, our work adds to the literature on\nprompt sensitivity in pre-trained language models,\nwhich found that LMs are sensitive to individual\nprompts (Kojima et al., 2022), and that the ordering\nof in-context examples (Lu et al., 2022) can greatly\naffect model performance. Smaller LMs are also\nsensitive to the choice of prompt and output verbal-\nizer (Schick and Schütze, 2021a; Gao et al., 2021),\nand we indeed observe that a variety of model sizes\nand prefixing strategies elicit prefix sensitivity. To\nour knowledge, our study is the first to consider\nstructural priming in concert with in-context learn-\ning; we have found quantitative, graded effects of\nstructural priming on string probabilities, subject\nto the length of the context.\nLanguage models are sensitive to latent syntactic\nfeatures, as well as syntactic similarities across\nmultiple sentences. Our analyses add to a lit-\nerature that has found that language models are\nsensitive to more than just lexical or surface-level\nsyntactic features (Warstadt et al., 2020b; Mueller\net al., 2022). Indeed, LMs are capable of lever-\naging abstract syntactic features, and are sensitive\nto latent syntactic similarities between the context\nand test examples. Strengthening this finding, we\nalso observe that models are capable of adapting\nto the structures of both acceptable and unaccept-\nable examples: LMs show marked improvements\non acceptability tasks when prefixed by matched\nacceptable sentences, and they also (more substan-\ntially) show the opposite behavior—preferring un-\nacceptable sentences—when prefixed by matched\nunacceptable sentences (§4). This shows that LMs\nare sensitive enough to sentence acceptability to be\nable to produce not just systematically grammati-\ncal outputs, but also systematically ungrammatical\noutputs. While this is not a practical application,\nit does demonstrate how well LMs capture this\nimportant linguistic feature. Furthermore, our per-\nturbation analysis demonstrated that this two-way\nadaptation was robust to irrelevant syntactic vari-\nations in the context (§5). The present work bol-\nsters the findings of other recent work that only\nexplores this behavior in the grammatical direction\n(Lampinen, 2022; Sinclair et al., 2022).\nOur finding of models’ reliance on abstract struc-\ntural features that are made available in their con-\ntext can be further strengthened by controlling for\nlexical exposure (Kim et al., 2022). That is, fu-\n6050\nture work can augment our contexts by replacing\nreal lexical items—especially content words—with\nnonsense words (e.g., wug, dax, etc.), following\nrecent works (Dasgupta et al., 2022; Misra et al.,\n2023, i.a.). Doing so would maintain the struc-\ntural features of the context while also more strictly\ncontrolling for superficial cues such as lexical over-\nlap or similarity, and would make our conclusions\nstronger.\n7 Conclusion\nIn this work, we study how robust the acceptabil-\nity judgements of autoregressive Transformer lan-\nguage models are to manipulations of the context.\nWe find that acceptability judgements are generally\nrobust when the test sentences are preceded by ran-\ndomly sampled linguistic contexts. However, when\nthe contexts contain syntactic structures closely\nmatching those in the test sentence, that can sig-\nnificantly improve or degrade the models’ perfor-\nmance. This effect is amplified as we lengthen the\ncontext provided to the model. Our results demon-\nstrate in-context learning in a highly specific way:\nmodels are sensitive to granular syntactic proper-\nties of the context when making predictions over\na target sentence, such that they can be driven to\nproduce both correct and reliably incorrect outputs.\nLimitations\nThe prefixes we use are semantically independent\nfrom the test sentences, and also semantically im-\nplausible when chained together. This is the oppo-\nsite of what we typically expect in natural language,\nwhere sentences follow from some pragmatically\nlicit prior context. While our findings are theo-\nretically relevant to any NLP task that leverages\nnatural language inputs, we may see qualitatively\ndifferent trends in more naturalistic settings.\nOur results are currently limited to English. Cer-\ntain languages have grammatical features (such as\ncase marking) that could strongly impact on lan-\nguage models’ acceptability judgments, and this\ncould affect the trends we have observed. Future\nwork should investigate similar phenomena across\nlanguages to ensure that these findings suitably\ngeneral.\nAcknowledgments\nWe thank the ACL reviewers for their insightful\nquestions and comments. We would also like to\nthank Marten van Schijndel, Allyson Ettinger, Ti-\nwalayo Eisape, Jennifer Hu, Peng Qian and Alex\nWarstadt for their feedback and comments on draft\nversions of this paper.\nReferences\nAfra Alishahi, Grzegorz Chrupała, and Tal Linzen. 2019.\nAnalyzing and interpreting neural networks for NLP:\nA report on the first BlackboxNLP workshop. Natu-\nral Language Engineering, 25(4):543–557.\nSu Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu,\nRobert Sim, and Hanna Wallach. 2021. Stereotyping\nNorwegian salmon: An inventory of pitfalls in fair-\nness benchmark datasets. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 1004–1015, Online. Association\nfor Computational Linguistics.\nJ. Kathryn Bock. 1986. Syntactic persistence in lan-\nguage production. Cognitive Psychology, 18(3):355–\n387.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language Models are Few-Shot\nLearners. Advances in Neural Information Process-\ning Systems, 33:1877–1901.\nRakesh Chada and Pradeep Natarajan. 2021. Few-\nshotQA: A simple framework for few-shot learning\nof question answering tasks using pre-trained text-to-\ntext models. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6081–6090, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nNoam Chomsky. 1965. Aspects of the theory of syntax.\nMIT Press, Cambridge, MA.\nIshita Dasgupta, Andrew K Lampinen, Stephanie CY\nChan, Antonia Creswell, Dharshan Kumaran,\nJames L McClelland, and Felix Hill. 2022. Lan-\nguage models show human-like content effects on\nreasoning. arXiv preprint arXiv:2207.07051.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAllyson Ettinger, Ahmed Elgohary, and Philip Resnik.\n2016. Probing for semantic evidence of composition\n6051\nby means of simple classification tasks. In Proceed-\nings of the 1st Workshop on Evaluating Vector-Space\nRepresentations for NLP, pages 134–139, Berlin, Ger-\nmany. Association for Computational Linguistics.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nAdi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer\nLevy. 2022. Transformer Language Models without\nPositional Encodings Still Learn Positional Informa-\ntion. ArXiv preprint, abs/2203.16634.\nJennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox,\nand Roger Levy. 2020. A systematic assessment\nof syntactic generalization in neural language mod-\nels. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n1725–1744, Online. Association for Computational\nLinguistics.\nDieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia\nBruni. 2020. Compositionality decomposed: How\ndo neural networks generalise? Journal of Artificial\nIntelligence Research, 67:757–795.\nDieuwke Hupkes, Mario Giulianelli, Verna Dankers,\nMikel Artetxe, Yanai Elazar, Tiago Pimentel, Chris-\ntos Christodoulopoulos, Karim Lasri, Naomi Saphra,\nArabella Sinclair, Denis Ulmer, Florian Schottmann,\nKhuyagbaatar Batsuren, Kaiser Sun, Koustuv Sinha,\nLeila Khalatbari, Maria Ryskina, Rita Frieske, Ryan\nCotterell, and Zhijing Jin. 2022. State-of-the-art gen-\neralisation research in nlp: a taxonomy and review.\narXiv preprint arXiv:2210.03050.\nPaloma Jeretic, Alex Warstadt, Suvrat Bhooshan, and\nAdina Williams. 2020. Are natural language infer-\nence models IMPPRESsive? Learning IMPlicature\nand PRESupposition. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 8690–8705, Online. Association\nfor Computational Linguistics.\nKatharina Kann, Alex Warstadt, Adina Williams, and\nSamuel R. Bowman. 2019. Verb argument structure\nalternations in word and sentence embeddings. In\nProceedings of the Society for Computation in Lin-\nguistics (SCiL) 2019, pages 287–297.\nMichael P. Kaschak and Arthur M. Glenberg. 2004.\nThis construction needs learned. Journal of Experi-\nmental Psychology: General, 133(3):450.\nNora Kassner and Hinrich Schütze. 2020. Negated and\nmisprimed probes for pretrained language models:\nBirds can talk, but cannot fly. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7811–7818, Online. Asso-\nciation for Computational Linguistics.\nNajoung Kim, Tal Linzen, and Paul Smolensky. 2022.\nUncontrolled lexical exposure leads to overestima-\ntion of compositional generalization in pretrained\nmodels. arXiv preprint arXiv:2212.10769.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large Lan-\nguage Models are Zero-Shot Reasoners. In Advances\nin Neural Information Processing Systems.\nAndrew Lampinen. 2022. Can language models han-\ndle recursively nested grammatical structures? a\ncase study on comparing models and humans. arXiv\npreprint arXiv:2210.15303.\nAndrew Lampinen, Ishita Dasgupta, Stephanie Chan,\nKory Mathewson, Mh Tessler, Antonia Creswell,\nJames McClelland, Jane Wang, and Felix Hill. 2022.\nCan language models learn from explanations in con-\ntext? In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2022 , pages 537–563,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn syntax-\nsensitive dependencies. Transactions of the Associa-\ntion for Computational Linguistics, 4:521–535.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretrain-\ning approach. Computing Research Repository ,\narXiv:1907.11692.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086–8098, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202,\nBrussels, Belgium. Association for Computational\nLinguistics.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In International Conference on Learning Repre-\nsentations.\nKanishka Misra. 2022. minicons: Enabling flexible be-\nhavioral and representational analyses of transformer\nlanguage models.\nKanishka Misra, Allyson Ettinger, and Julia Rayz. 2020.\nExploring BERT’s sensitivity to lexical cues using\ntests from semantic priming. In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2020,\npages 4625–4635, Online. Association for Computa-\ntional Linguistics.\n6052\nKanishka Misra, Julia Rayz, and Allyson Ettinger. 2023.\nCOMPS: Conceptual minimal pair sentences for test-\ning robust property knowledge and its inheritance in\npre-trained language models. In Proceedings of the\n17th Conference of the European Chapter of the As-\nsociation for Computational Linguistics, pages 2928–\n2949, Dubrovnik, Croatia. Association for Computa-\ntional Linguistics.\nAaron Mueller, Robert Frank, Tal Linzen, Luheng Wang,\nand Sebastian Schuster. 2022. Coloring the blank\nslate: Pre-training imparts a hierarchical inductive\nbias to sequence-to-sequence models. In Findings of\nthe Association for Computational Linguistics: ACL\n2022, pages 1352–1368, Dublin, Ireland. Association\nfor Computational Linguistics.\nAaron Mueller, Garrett Nicolai, Panayiota Petrou-\nZeniou, Natalia Talmina, and Tal Linzen. 2020.\nCross-linguistic syntactic evaluation of word predic-\ntion models. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5523–5539, Online. Association for Computa-\ntional Linguistics.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020. CrowS-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1953–1967, Online. As-\nsociation for Computational Linguistics.\nBenjamin Newman, John Hewitt, Percy Liang, and\nChristopher D. Manning. 2020. The EOS decision\nand length extrapolation. In Proceedings of the Third\nBlackboxNLP Workshop on Analyzing and Interpret-\ning Neural Networks for NLP, pages 276–291, On-\nline. Association for Computational Linguistics.\nMartin J. Pickering and Victor S. Ferreira. 2008. Struc-\ntural priming: A critical review. Psychological Bul-\nletin, 134(3):427–459.\nMartin J Pickering and Simon Garrod. 2017. Priming\nand Language Change, pages 173–90. Cambridge\nUniversity Press.\nGrusha Prasad, Marten van Schijndel, and Tal Linzen.\n2019. Using priming to uncover the organization of\nsyntactic representations in neural language models.\nIn Proceedings of the 23rd Conference on Computa-\ntional Natural Language Learning (CoNLL), pages\n66–76, Hong Kong, China. Association for Computa-\ntional Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving Language Under-\nstanding by Generative Pre-Training. OpenAI.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nModels are Unsupervised Multitask Learners. Ope-\nnAI.\nTimo Schick and Hinrich Schütze. 2021a. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255–269, Online. Association for Computa-\ntional Linguistics.\nTimo Schick and Hinrich Schütze. 2021b. It’s not just\nsize that matters: Small language models are also few-\nshot learners. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2339–2352, Online. Association\nfor Computational Linguistics.\nCarson T. Schütze. 1996. The empirical base of lin-\nguistics: Grammaticality judgments and linguistic\nmethodology. University of Chicago Press, Chicago,\nIL.\nArabella Sinclair, Jaap Jumelet, Willem Zuidema, and\nRaquel Fernández. 2022. Structural persistence in\nlanguage models: Priming as a window into abstract\nlanguage representations. Transactions of the Associ-\nation for Computational Linguistics, 10:1031–1050.\nKoustuv Sinha, Amirhossein Kazemnejad, Siva Reddy,\nJoelle Pineau, Dieuwke Hupkes, and Adina Williams.\n2022. The curious case of absolute position embed-\ndings. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2022, pages 4449–4472,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nMarten van Schijndel and Tal Linzen. 2018. A neural\nmodel of adaptation in reading. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 4704–4710, Brussels,\nBelgium. Association for Computational Linguistics.\nDusan Varis and Ondˇrej Bojar. 2021. Sequence length\nis a domain: Length-based overfitting in transformer\nmodels. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 8246–8257, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020a. BLiMP: A benchmark of linguis-\ntic minimal pairs for English. In Proceedings of the\nSociety for Computation in Linguistics 2020, pages\n409–410, New York, New York. Association for Com-\nputational Linguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nAlex Warstadt, Yian Zhang, Xiaocheng Li, Haokun Liu,\nand Samuel R. Bowman. 2020b. Learning which\nfeatures matter: RoBERTa acquires a preference for\n6053\nlinguistic generalizations (eventually). In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n217–235, Online. Association for Computational Lin-\nguistics.\nAlbert Webson and Ellie Pavlick. 2022. Do prompt-\nbased models really understand the meaning of their\nprompts? In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2300–2344, Seattle, United States.\nAssociation for Computational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nYuxue Cher Yang and Andrea Stocco. 2019. Syntactic\npriming depends on procedural, reward-based compu-\ntations: evidence from experimental data and a com-\nputational model. In Proceedings of the 17th Inter-\nnational Conference on Cognitive Modeling, pages\n307–313.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. OPT: Open pre-\ntrained transformer language models. arXiv preprint\narXiv:2205.01068.\nA Fairness Analysis\nDatasets. CrowS-Pairs (Nangia et al., 2020) con-\ntains 1508 sentence pairs denoting stereotypes\nabout nine types of demographics, including gen-\nder, age, nationality, etc. CrowS-Pairs differs from\nBLiMP and SyntaxGym in construction, since it\nwas crowdsourced using untrained English speak-\ners from Amazon Mechanical Turk. Despite this\ndifference, the resulting test pair sentences still only\nminimally differ from each other (except for some\ninstances where more than a few tokens differ due\nto annotation noise, see Blodgett et al. 2021 for\nFigure 6: Effects of length and stereotypical prefixes\nper model. Shaded regions indicate the 95% confidence\nintervals across 9 demographics.\nmore discussion). Thus, we can leverage CrowS-\nPairs in the similar MPP paradigm and test whether\nour results are specific to syntactic evaluation.\nSimilar to the approach in SyntaxGym, (Nangia\net al., 2020) propose to measure fairness in masked\nlanguage models by focusing only on the tokens\nwhich differ, computing the pseudo-log-likelihood\nof the sentences conditioned on those tokens. To\nmaximize the comparability of the CrowS-Pairs\nresults with our results on BLiMP/SyntaxGym,\nwe compute the conditional log-likelihood, as de-\nscribed in §3. We then compute the acceptability of\neach test pair as described in equation 2, where we\nrecode the definitions of unacceptable and accept-\nable items to stereotypical and antistereotypical,\nin-line with the definitions in this dataset. An ideal,\nfair model would show no special preference to-\nwards stereotypical sentences.\nMethod. We construct contexts using the same\napproach described in §3. In lieu of phenomena\nin SyntaxGym/BLiMP, Crows-Pairs dataset pro-\nvides test pairs over multiple demographies. Thus,\nfor a given test example, we construct matched\ncontexts by sampling from the same demographic\ncohort the test pair belongs to, and conversely sam-\nple from different demographic subset to construct\nmismatched context. We re-use the same control\nexperiment setup, i.e. sampling from Wikipedia for\nirrelevant contexts.\nAnalysis. Figure 6 compares stereotypical con-\ntexts from mismatched and matched demographics\nacross models of varying sizes. The results show\nthat mismatched contexts don’t show any signifi-\ncant impact on the fairness scores. Across all model\n6054\nFigure 7: Effects of length and anti-stereotypical pre-\nfixes per model. Shaded regions indicate the 95% confi-\ndence intervals across 9 demographics.\nFigure 8: Effects of length and antistereotypical\nmatched prefixes per model. Shaded regions indicate\nthe 95% confidence intervals across 9 demographics.\nsizes, we see a score decrease when prefixed with\nmatched stereotypical context. Meanwhile, Fig-\nure 7 shows that prefixing with an antistereotypical\ncontext improves the fairness scores. This raises\nthe question, can we prime models with antistereo-\ntypical contexts to reduce stereotypical bias? Fig-\nure 8 shows that prefixing with an antistereotypical\nmatched context can enable models to reach the\nideal score of an unbiased model, and even surpass\nit (i.e, making the model biased in the other direc-\ntion). However, it is worth noting that this does not\nnecessarily indicate that a model is unbiased, as\nthere is significant variation between demograph-\nics, and more detailed examination is needed to\nevaluate the effects per demographic cohort.\nB Prefix Similarity Analysis\nHere, we provide more detail to support the ex-\nperiment in §5. Specifically, we present the exact\nnumbers we use to compute the rank-order correla-\ntion coefficients, and describe the implications of\nthis finding for future work.\nTo compute the rank-order correlation, we first\nobtain mean accuracies across the 20 BLiMP phe-\nnomena that respond most strongly to matched pre-\nfixing. We do this for each perturbation strategy,\nas well as the non-perturbed matched prefixes. We\nthen take the mean across all prefixing lengths for\nOPT 6.7B (i.e., we convert each line in Figure 5\ninto a single number by taking the mean along the\nx-axis). This yields a metric that approximately\ncaptures how much of a priming effect a given pre-\nfixing strategy has for this model; we use this as\nour dependent variable.\nThe independent variable is the strength of the\nperturbation prefix. It is difficult to define how\nstrong a given perturbation is, as there are different\nnotions of linguistic similarity that can be contra-\ndictory; for instance, embedding a sentence cinto\na quote, as in “Yesterday, Sarah said ‘ c”’, does\nnot add many lexical items to the sentence, but it\nsignificantly modifies the syntactic structure of the\nsentence. In our case, we simply measure the token\nF1 score between the original prefix sentence and\na perturbed prefix sentence; this metric captures\nthe token similarity between the original and per-\nturbed sentences. Future work could consider more\nsophisticated similarity metrics, such as syntactic\nor semantic similarities.\nWe summarize these results in Figure 9. Note\nthe highly monotonic relationship when using ac-\nceptable prefixes, and the similarly (but slightly\nless) monotonic relationship with unacceptable pre-\nfixes. This visually displays the strong correlations\nwe found in §5.\nWhy are language models being more primeable\nwith longer contexts given more similar prefixes?\nPerhaps models can determine whether tokens are\nmeaningfully similar between multiple sentences\nin the same context; this would be expected given\nthe implications of the distributional hypothesis.\nAlternatively, the model could be effective at re-\nlating tokens that are similar in the pre-training\ncorpus, as long as their positions are within some\nlimited range of each other. Finally, perhaps the\nmodel is simply effective at ignoring (for exam-\nple) adverbs and adjuncts that are semantically or\n6055\n0.4 0.5 0.6 0.7 0.8 0.9 1.0\nToken F1\n0.24\n0.25\n0.26\n0.27\n0.28\n0.29Mean  Accuracy\nAll\nAdd clause\nQuote\nLong prefix adverb\nPrefix adverb\n2 suffix adverbs\n2 Pre-verb adverbs\nSuffix adverb\nNone\n0.4 0.5 0.6 0.7 0.8 0.9 1.0\nToken F1\n0.50\n0.48\n0.46\n0.44\n0.42\nMean  Accuracy\nAll\nAdd clause\nQuote\nLong prefix adverb\nPrefix adverb\n2 Pre-verb adverbs\n2 suffix adverbs\nSuffix adverb\nNone\nFigure 9: Token similarity before and after perturbing\nthe prefixes vs. ∆ accuracy across BLiMP phenomena\nfor OPT 6.7B. We show ∆ accuracies using acceptable\n(top) and unacceptable (bottom) prefixes.\nsyntactically irrelevant, and thus otherwise views\nthe perturbed prefixes and test sentences as more\nor less structurally identical. Our results cannot\ncurrently disambiguate between these possibilities,\nbut future work could investigate perturbed prefix-\ning in significantly more depth to better understand\nwhy we observe these effects and correlations.\nC Metric correlation analysis\nTo what degree can the priming effects discussed\nin this paper reveal facts about a model’s capacities\nnot already evident from existing MPP evaluations?\nTo test this, we evaluated (for every model and\ndataset) the correlation between a model’s baseline\nperformance and its performance with a maximal\namount of acceptable or unacceptable matched pre-\nfixes. For example, we evaluated the correlation\nbetween a model’s accuracy in un-prefixed BLiMP\nphenomena, and its accuracy on each of the phe-\nnomena after prefixing with the maximal amount\nof possible unacceptable prefixes (start vs. end of\ndashed purple line in Figure 2).\nFigure 10 and Figure 11 show the results of this\nanalysis. A single point in any of these scatter-\nplots indicates the relationship between a particular\nmodel’s performance on a particular suite at base-\nline (no prefix, x-axis) and its performance with\na maximal-length prefix, either acceptable (Fig-\nure 10) or unacceptable (Figure 11; y-axis). If our\nprefixing results reveal facts about model capaci-\nties not already present in MPP evaluations, then\nwe should see substantial variance in the y-axis\nnot explained by the x-axis on these plots. This\nis apparent in most of the plots, especially in the\nBLiMP evaluations (leftmost plots).\nWe also see variation among models: GPT2 has\na prefixing response which is relatively predictable\nfrom its baseline performance (correlation with\nacceptable prefixing effect, mean across datasets:\nr = 0.85; unacceptable: r = 0.79). In contrast,\nOPT 2.7B is far less predictable in its prefixing\nresponse (correlation with acceptable prefixing ef-\nfect, mean across datasets: r= 0.59; unacceptable:\nr= 0.52).\nOverall, this analysis suggests that there are non-\ntrivial variations in the way that models respond\nto these prefixing interventions which is not cap-\ntured by models’ baseline performance on matched\nstimuli. This suggests that prefixing reveals new\naspects of model capacity not exactly captured by\nexisting MPP evaluations.\nD BLiMP Phenomenon Similarities\nLength effects are conditional on the similarity of\nthe prefix to the target BLiMP phenomenon. Does\nsome specific kind of similarity (e.g., syntactic or\nlexical similarity) explain length effects? Perhaps\nthe prefix is syntactically priming the model for\nthe target sentence (Sinclair et al., 2022), in which\ncase we would expect the syntactic similarity of\nthe sentences to correlate smoothly with accuracy\nwhen using grammatical prefixes. Another pos-\nsibility is that a more spurious feature—such as\nlexical overlap—is responsible (Misra et al., 2020;\nKassner and Schütze, 2020). To test this, we can\ncorrelate syntactic similarity and lexical overlap\nwith accuracies on each example.\nTo measure lexical overlap, we use F1 scores\nto measure how many tokens 10 in the prefix and\ntest sentences are shared. To approximate syntactic\noverlap, we can compute the F1 score over depen-\ndency labels in two sentences, rather than across\ntokens. If multiple prefix sentences are present,\nwe can take the mean similarity with the target\nsentence across prefixes. Then, we compute the\npoint-biserial correlation11 (ρp) between the sim-\n10We tokenize the inputs using GPT2’s tokenizer before\ncomputing overlap.\n11The point-biserial correlation coefficient measures the\nstrength of the relationship between a continuous variable\n(e.g., our overlap metrics) and a binary variable (accuracy on\n6056\n0.25 0.50 0.75 1.00\nBaseline accuracy\n0.2\n0.4\n0.6\n0.8\n1.0\nMatched acceptable\nprefixing accuracy\nEvaluation = BLiMP\n0.3 0.4 0.5\nBaseline accuracy\nEvaluation = CrowS-Pairs\n0.25 0.50 0.75 1.00\nBaseline accuracy\nEvaluation = SyntaxGym\nAcceptable prefixing effect\nvs. baseline accuracy\nModel\nGPT2\nOPT (1.3B)\nOPT (125M)\nOPT (2.7B)\nOPT (350M)\nOPT (6.7B)\nFigure 10: Correlation between accuracy with maximum-length acceptable prefix and baseline accuracy, for each\nmodel and dataset.\n0.25 0.50 0.75 1.00\nBaseline accuracy\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nMatched unacceptable\nprefixing accuracy\nEvaluation = BLiMP\n0.3 0.4 0.5\nBaseline accuracy\nEvaluation = CrowS-Pairs\n0.25 0.50 0.75 1.00\nBaseline accuracy\nEvaluation = SyntaxGym\nUnacceptable prefixing effect\nvs. baseline accuracy\nModel\nGPT2\nOPT (1.3B)\nOPT (125M)\nOPT (2.7B)\nOPT (350M)\nOPT (6.7B)\nFigure 11: Correlation between accuracy with maximum-length unacceptable prefix and baseline accuracy, for each\nmodel and dataset.\nFigure 12: Token overlap (left) and dependency overlap (right) across BLiMP phenomena. We compute these using\na sample of 10,000 sentences from the target phenomenon and from the prefix phenomenon. The phenomena are\nordered alphabetically.\n6057\nilarity metric and accuracy on a given example,\naveraging similarities across prefix sentences. We\ncompute the correlation separately for each model\nsize and each prefixing strategy. Note that we only\nuse grammatical prefixes; thus, we expect positive\ncorrelations if priming explains the length effects.\nHowever, this instance-level analysis could be\nconfounded by the mixture of various phenom-\nena in the prefixes. The model could be sensi-\ntive to sentences from certain phenomena more\nthan others, or the varying lengths of sentences\nfrom each phenomenon. To more specifically mea-\nsure whether priming can explain our findings, we\nfocused on BLiMP and prefixed sentences from\none phenomenon at a time with a given test phe-\nnomenon; in other words, we sample mismatched\nprefixes, but controlling which phenomenon we\nsample from. Using this approach, we can capture\nhow structurally similar each BLiMP phenomenon\nis with each other BLiMP phenomenon, and how\nthis correlates with accuracies.\nHere, we present the lexical and syntactic simi-\nlarity across each pair of BLiMP phenomena (Fig-\nure 12).12 We find very low and non-significant\ncorrelations with dependency overlap and token\noverlap (ρp <0.05, p> 0.1) regardless of prefix-\ning strategy or model size. This could be evidence\nthat the model is more sensitive to the length of\nthe prefixes than any notion of syntactic or lexical\nsimilarity on this task. These are computed across\neach prefix and test phenomenon using a sample of\n10,000 test sentences and 10,000 prefix sentences\nfor each point in the confusion matrix. We find\nthat dependency overlap is generally higher than\ntoken overlap across inputs, perhaps unsurprisingly\ngiven that the size of the set of possible dependency\nlabels is much smaller than the size of the set of\npossible tokens in a given sentence.\nWe next try correlating these values with accura-\ncies on each BLiMP phenomenon as a function of\nthese phenomenon-level similarity metrics. Accu-\nracies with prefixes (and changes in accuracies after\nafter prefixing) for GPT2 are presented in Figure 13.\nEssentially, we are now measuring how similar the\ntrends are across a similarity confusion matrix and\nan accuracy confusion matrix. As we are now mea-\nsuring similarity across continuous variables, we\nan individual example).\n12For visual conciseness across confusion matrices, we use\nindices rather than individual phenomenon names. For each\nconfusion matrix in Figures 12 and 13, all phenomena are\npresented in alphabetical order.\ncompute the Spearman correlation ( ρs). We find\nthat correlations here are a bit stronger than when\nwe mix mismatched prefixes (ρs = 0.11 for depen-\ndency overlap, and ρs = 0.18 for token overlap,\np <0.001 for both). While the magnitude of the\ncorrelations is very low, these are still significant.\nThus, there is some relationship between the simi-\nlarity of the prefix and test sentence with accuracy,\nbut the relationship tends to be weak. Also, lexical\noverlap seems to be more strongly predictive of\naccuracies than structural similarities, indicating\nthat the model may indeed be more sensitive to spu-\nrious lexical similarities than any deeper abstract\nnotion of syntactic similarity between a prefix and\nthe test sentence. Nonetheless, this is still prelimi-\nnary evidence that priming effects do not explain\nmuch of the accuracy trends we observe with prefix-\ning; instead, perhaps length itself makes a stronger\ndifference than any specific notion of similarity\nbetween the prefix and test sentence.\nThis is preliminary evidence thatlexical overlap\nand low-level syntactic similarity effectspartially\nexplain accuracy increases with BLiMP prefix-\ning, but most of the trends we observe cannot\nbe explained by these effects alone. Perhaps this\nis because the model is more sensitive to multiple\nsimilarities simultaneously than any one isolated\ntype of similarity. Or, perhaps models are sensitive\nto some other latent feature that we did not analyze.\nNonetheless, it is difficult to draw strong conclu-\nsions from the lack of a strong correlation, and\ncorrelations alone cannot causally implicate simi-\nlarities in explaining our findings. Perhaps future\nwork could disambiguate the relationship between\nthese factors using causal methods.\nD.1 Suite-by-suite prefixing performance\nFigure 14 shows GPT2’s improvement in predic-\ntion accuracy on different SyntaxGym test suites\n(rows) after drawing as many acceptable prefix\nsentences as possible from another SyntaxGym\ntest suite (columns). The values are a percent-\nage increase in prediction accuracy, relative to\nGPT2’s baseline performance with no additional\ncontext. We see a substantial diversity in how dif-\nferent suites respond to prefixing of acceptable\nsentences. Some suites, such as an NPI licensing\nsuite (npi_src_any) and a filler-gap dependency\nsuite (fgd_subject), show across-the-board im-\nprovements in response to any prefixing at all. The\nsuites labeled reflexive_*_fem, which test under-\n6058\nFigure 13: Accuracies for GPT2 on individual BLiMP phenomena after prefixing 10 sentences from a single BLiMP\nphenomenon (left). Change in accuracy from no prefix to 10 prefixes on each BLiMP phenomenon (right). We\nexclude the diagonal in both cases, as we are interested in mismatched prefixing effects.\nstanding of feminine reflexive anaphor agreement,\ndemonstrate interesting unstable behavior: GPT2’s\npredictions degrade when these particular tests are\npreceded by grammatical sentences containing mas-\nculine reflexive anaphors (see e.g. the blue boxes\nin the row labeled reflexive_orc_fem, but the\nsame predictions are facilitated when preceded by\nfeminine reflexive anaphors.\nWe also provide a snapshot of the top 10 suites in\nBLiMP (Warstadt et al., 2020a) which get the best\nand worst changes in accuracy (∆ Accuracy), when\nprimed with acceptable (Table 1) and unacceptable\nprefixes (Table 2) respectively.\nE Margin Analysis\nHow confident are LMs as input length increases?\nThe results on length priming indicates that longer\nmatched acceptable prefixes tend to induce bet-\nter acceptability judgements to the target model.\nHowever, investigating the accuracies as com-\nputed in Equation 2 alone does not fully explain\nthe nuances of the model confidence. To under-\nstand how model confidence values themselves\ndiffer in acceptable/unacceptable target sentences,\nwe plot and investigate the perplexity margins in\nFigure 15. Specifically, we compute the differ-\nence in the model perplexities δ for each accept-\nable/unacceptable pair:\nδ(xi, ˆxi) = ppl(xi) −ppl( ˆxi), (3)\nWe observe the margins on BLiMP for a candi-\ndate model, OPT 6.7B in Figure 15, for grammati-\ncal, ungrammatical and Wikipedia prefixes. For all\ncases, δstarts from a high value for short sequences,\nand approaches zero as the context length increases.\nThere is a marked difference in δvalues compared\nto Wikipedia and BLiMP prefixes: Wikipedia pre-\nfixes appear to display a high value, suggesting\nhigh surprisals. The average δfor Wikipedia also\nremains higher than the baseline value (without any\npriming), while δis significantly lower for BLiMP\nprefixes. This behavior potentially explains why\nwe observe almost no change in the accuracy of\nWikipedia prefixes, as the margin remains high and\nstable with increasing length of tokens.\nWithin matched prefixes, we observe the δto be\nsignificantly lower for unacceptable prefixes com-\npared to the acceptable contexts, and it reduces with\nlength. This behavior partially explains why we\nobserve the trend of sharp decrease in acceptability\naccuracy for matched unacceptable prefixes, as the\nmonotonically decreasing δflips the acceptability\njudgement associations.\nF SyntaxGym Results\nWe run our prefixing evaluations for 23 of the 34\nSyntaxGym evaluations whose prediction struc-\ntures are compatible with this paper’s evaluation\nsetup – that is, where model success is a function\nof one or more differences in surprisal measured\nbetween two experimental conditions. These ap-\nplicable suites are shown in the axes of Figure 14.\nIn contrast to BLiMP, model surprisal is measured\nonly at a critical region, at which differing content\n6059\nfgd_object\nfgd_pp\nfgd_subject\nmvrr\nmvrr_mod\nnpi_orc_any\nnpi_orc_ever\nnpi_src_any\nnpi_src_ever\nnpz_obj\nnpz_obj_mod\nnumber_orc\nnumber_prep\nnumber_src\nreflexive_orc_fem\nreflexive_orc_masc\nreflexive_prep_fem\nreflexive_prep_masc\nreflexive_src_fem\nreflexive_src_masc\nsubordination_orc-orc\nsubordination_pp-pp\nsubordination_src-src\nPrefix suite\nfgd_object\nfgd_pp\nfgd_subject\nmvrr\nmvrr_mod\nnpi_orc_any\nnpi_orc_ever\nnpi_src_any\nnpi_src_ever\nnpz_obj\nnpz_obj_mod\nnumber_orc\nnumber_prep\nnumber_src\nreflexive_orc_fem\nreflexive_orc_masc\nreflexive_prep_fem\nreflexive_prep_masc\nreflexive_src_fem\nreflexive_src_masc\nsubordination_orc-orc\nsubordination_pp-pp\nsubordination_src-src T est suite\n4.3 2.6 4.3 4.3 4.3 4.3 4.3 4.3 4.3 4.3 4.3 4.3 4.3 4.3 4.3 4.3 4.3 4.3 4.3 2.5 4.3 4.3 4.3\n4.9 4.9 3.8 14.3 1.2 11.4 14.3 8.6 14.3 14.3 3.1 10.1 14.3 3.9 14.3 12.2 8.1 8.1 8.1 6.0 6.9 -2.7 1.8\n57.4 41.0 100.0 50.0 41.0 50.0 100.0 75.0 100.0 44.2 51.7 56.4 34.5 74.5 60.0 60.0 52.7 38.2 70.9 78.2 24.7 100.0 71.4\n12.7 27.3 11.1 27.3 14.8 1.1 -4.5 -2.7 1.1 24.3 22.9 -0.3 -1.9 -6.5 18.0 20.3 18.0 22.6 20.3 18.0 9.1 27.3 15.0\n22.4 33.3 20.6 22.2 18.0 3.3 13.3 6.7 6.7 33.3 33.3 11.5 2.8 4.4 28.5 28.5 30.9 28.5 30.9 26.1 17.6 33.3 33.3\n2.7 2.7 2.7 2.7 2.7 2.7 2.7 2.7 2.7 2.7 2.7 2.7 2.7 2.7 2.7 2.7 2.7 2.7 2.7 2.7 2.7 2.7 2.7\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n72.7 72.7 72.7 72.7 72.7 72.7 68.4 72.7 72.7 72.7 72.7 72.7 72.7 72.7 72.7 72.7 72.7 72.7 72.7 72.7 72.7 72.7 72.7\n8.6 8.6 8.6 8.6 8.6 8.6 8.6 8.6 8.6 8.6 8.6 8.6 8.6 8.6 8.6 8.6 8.6 8.6 8.6 8.6 8.6 8.6 8.6\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 -2.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n16.8 22.0 18.1 18.8 58.3 2.0 58.3 10.8 22.7 25.2 11.9 -10.8 -1.0 -1.6 -10.1 -3.7 20.9 18.0 7.0 7.0 -11.6 7.2 3.9\n3.8 10.1 12.6 26.7 -4.5 23.5 26.7 17.2 17.2 26.7 -6.2 19.8 19.8 19.8 8.2 10.5 3.6 11.3 12.8 12.8 -6.3 -6.2 -5.0\n19.2 26.7 16.6 21.4 14.2 20.3 23.5 23.5 26.7 6.2 10.2 -2.4 22.1 26.7 15.2 10.5 22.1 15.2 1.3 1.0 26.7 6.9 26.7\n21.1 14.2 40.7 26.7 -3.1 42.5 0.3 -26.1 -60.6 57.1 42.0 -42.9 -31.5 -51.5 95.8 -57.8 111.1 -51.5 72.7 -48.6 23.4 111.1 -20.5\n18.8 12.9 18.8 18.8 14.9 18.8 18.8 15.8 15.8 18.8 12.8 14.4 16.6 2.7 -11.5 16.6 18.8 18.8 -2.8 10.1 17.2 18.8 18.8\n164.8 177.1 171.4 107.8 94.7 42.5 -16.9 6.9 -40.6 185.0 112.9 -42.2 -42.2 -16.6 288.6 -35.8 271.4 15.5 254.1 -23.0 216.7 33.6 29.5\n17.5 20.9 26.7 24.0 16.3 23.5 26.7 20.3 26.7 26.7 13.6 15.2 8.2 5.9 -1.0 17.5 -10.2 1.3 3.6 19.8 26.7 11.9 26.7\n187.9 149.2 201.6 137.5 128.4 90.0 42.5 -5.0 -20.8 209.3 183.9 -40.1 2.7 -57.2 326.1 3.6 510.3 38.2 406.7 3.6 56.3 -1.0 -14.2\n23.1 24.6 13.1 22.0 58.3 34.6 38.5 30.6 58.3 58.3 33.8 26.7 41.1 6.5 -10.8 23.8 55.5 52.2 9.4 46.8 17.3 9.0 4.9\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 -1.3 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n0\n100\n200\n300\n400\n500\nRelative improvement in prediction accuracy after maximum prefixing\nFigure 14: Relative improvement (in percentage points) in accuracy on SyntaxGym test suite evaluations (rows)\nafter prefixing with sentences from other SyntaxGym test suites (columns) for GPT2.\nPhenomena GPT2 OPT 125M OPT 350M OPT 1.3B OPT 2.7B OPT 6.7B Mean ∆\nprinciple_A_reconstruction 0.528 (0.13) 0.62 (0.1) 0.699 (0.1) 0.599 (0.06) 0.585 (0.05) 0.585 (0.05) 0.603 (0.05)\nexistential_there_quantifiers_2 0.322 (0.07) 0.827 (0.04) 0.528 (0.04) 0.683 (0.02) 0.538 (0.03) 0.538 (0.02) 0.573 (0.15)\nsentential_subject_island 0.58 (0.19) 0.556 (0.14) 0.536 (0.18) 0.491 (0.11) 0.402 (0.1) 0.48 (0.11) 0.507 (0.06)\nwh_vs_that_with_gap_long_distance 0.457 (0.13) 0.48 (0.12) 0.465 (0.14) 0.481 (0.17) 0.446 (0.15) 0.514 (0.21) 0.474 (0.02)\nmatrix_question_npi_licensor_present 0.566 (0.01) 0.49 (0.03) 0.477 (0.03) 0.357 (0.01) 0.307 (0.01) 0.358 (0.01) 0.426 (0.09)\nwh_vs_that_with_gap 0.353 (0.09) 0.376 (0.07) 0.394 (0.09) 0.435 (0.12) 0.447 (0.13) 0.468 (0.14) 0.412 (0.04)\nleft_branch_island_echo_question 0.443 (0.21) 0.462 (0.17) 0.357 (0.17) 0.359 (0.14) 0.361 (0.12) 0.344 (0.1) 0.388 (0.05)\nonly_npi_scope 0.38 (0.05) 0.511 (0.02) 0.198 (0.02) 0.321 (0.02) 0.375 (0.02) 0.347 (0.02) 0.355 (0.09)\nnpi_present_1 0.327 (0.1) 0.337 (0.11) 0.236 (0.09) 0.267 (0.07) 0.305 (0.09) 0.352 (0.08) 0.304 (0.04)\ncomplex_NP_island 0.316 (0.12) 0.271 (0.09) 0.264 (0.1) 0.241 (0.1) 0.274 (0.09) 0.356 (0.1) 0.287 (0.04)\nTable 1: Top 10 phenomena in BLiMP (Warstadt et al., 2020a) with largest average∆ increase in acceptability over\nthe full context window, when primed with acceptable prefixes. The numbers in parenthesis reflect the standard\ndeviation over length of the context.\n6060\nPhenomena GPT2 OPT 125M OPT 350M OPT 1.3B OPT 2.7B OPT 6.7B Mean ∆\nonly_npi_licensor_present -0.693 (0.21) -0.726 (0.16) -0.934 (0.19) -0.953 (0.14) -0.945 (0.1) -0.961 (0.07) -0.869 (0.11)\nexistential_there_quantifiers_1 -0.783 (0.27) -0.871 (0.21) -0.869 (0.21) -0.856 (0.2) -0.911 (0.21) -0.906 (0.21) -0.866 (0.04)\nprinciple_A_case_1 -0.782 (0.42) -0.863 (0.35) -0.813 (0.33) -0.871 (0.34) -0.867 (0.35) -0.872 (0.34) -0.845 (0.03)\nsuperlative_quantifiers_2 -0.817 (0.1) -0.822 (0.07) -0.847 (0.06) -0.832 (0.05) -0.862 (0.05) -0.845 (0.04) -0.837 (0.02)\nsentential_negation_npi_licensor_present -0.637 (0.25) -0.733 (0.23) -0.882 (0.23) -0.907 (0.24) -0.904 (0.22) -0.911 (0.22) -0.829 (0.11)\nwh_questions_subject_gap -0.731 (0.32) -0.811 (0.27) -0.804 (0.29) -0.839 (0.28) -0.837 (0.27) -0.832 (0.27) -0.809 (0.04)\nwh_vs_that_no_gap_long_distance -0.715 (0.33) -0.806 (0.24) -0.742 (0.23) -0.806 (0.24) -0.852 (0.25) -0.889 (0.25) -0.801 (0.06)\nwh_questions_subject_gap_long_distance -0.782 (0.24) -0.802 (0.22) -0.781 (0.22) -0.828 (0.22) -0.784 (0.23) -0.833 (0.24) -0.801 (0.02)\nsuperlative_quantifiers_1 -0.685 (0.15) -0.746 (0.06) -0.832 (0.07) -0.836 (0.11) -0.849 (0.11) -0.806 (0.07) -0.792 (0.06)\nirregular_past_participle_adjectives -0.671 (0.33) -0.788 (0.23) -0.838 (0.24) -0.834 (0.23) -0.786 (0.22) -0.829 (0.23) -0.791 (0.06)\nTable 2: Top 10 phenomena in BLiMP (Warstadt et al., 2020a) with largest average∆ decrease in acceptability over\nthe full context window, when primed with unacceptable prefixes. The numbers in parenthesis reflect the standard\ndeviation over length of the context.\n0 200 400 600 800 1000\nNumber of tokens\n0\n2000\n4000\n6000\n8000\n10000Perplexity margin\nPerplexity Margin Analysis\ngrammatical\nungrammatical\nwiki\n0 200 400 600 800\n0\n500\n1000\n1500\nIn-Domain\nAcceptable\nUnacceptable\nFigure 15: Perplexity margins of Grammatical, Ungram-\nmatical and Wikipedia prefixes on BLiMP for OPT 6.7B\nmodel. The dashed lines represent the mean margin of\nthe baseline without any context.\n-0.3\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0 50 100 150 200 250\nInput Length\nΔ Accuracy\nModel\nGPT2\nOPT (125M)\nOPT (350M)\nOPT (1.3B)\nOPT (2.7B)\nOPT (6.7B)\nPrefix Type\nAcceptable\nUnacceptable\nWiki\nFigure 16: Interaction of length and prefix type on Syn-\ntaxGym (collapsed across match/mismatch). Shaded\nregions are the 95% confidence interval. Analogous to\nFigure 3 in the main text.\nAcceptable Unacceptable\n0 50 100 150 200 250 0 50 100 150 200 250\n-0.4\n-0.2\n0.0\n-0.1\n0.0\n0.1\n0.2\nInput Length\nΔ Accuracy\nModel\nGPT2\nOPT (125M)\nOPT (350M)\nOPT (1.3B)\nOPT (2.7B)\nOPT (6.7B) Prefix Suite\nMatched\nMismatched\nFigure 17: Interaction of length and prefix suite\n(matched versus mismatched) in two separate prefix\ntypes (acceptable/unacceptable) in SyntaxGym. Analo-\ngous to Figure 4 in the main text.\nbetween conditions render minimal-pair sentences\ngrammatical or ungrammatical. For example, the\nnumber_prep suite measures the surprisal differ-\nence at the underlined critical region between the\nfollowing four conditions:\n1. The farmer near the clerks knows many peo-\nple.\n2. * The farmer near the clerk know many peo-\nple.\n3. * The farmers near the clerk knows many peo-\nple.\n4. The farmers near the clerk know many people.\nIn this example test suite, model surprisals for\nthe word knows in sentence 3 must be higher than in\nsentence 1, and surprisals for the word know must\nbe higher in sentence 2 than in sentence 4. The\nfull list of included suites is visible in Figure 14.\nAdditional plots for SyntaxGym, analogous to Fig-\nure 3 and Figure 4, are provided at Figure 16 and\nFigure 17.\n6061\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitations section after the Conclusion.\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nUsed: yes (Section 3) Created: no.\n□\u0013 B1. Did you cite the creators of artifacts you used?\n3\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\n3\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\n3\nC □\u0013 Did you run computational experiments?\n3\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n3\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n6062\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nNot applicable. Left blank.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n3\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n3\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n6063"
}