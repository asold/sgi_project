{
  "title": "Scaling high-order character language models to gigabytes",
  "url": "https://openalex.org/W1966703356",
  "year": 2005,
  "authors": [
    {
      "id": "https://openalex.org/A2106434598",
      "name": "Bob Carpenter",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1972594981",
    "https://openalex.org/W2884262036",
    "https://openalex.org/W2157963512",
    "https://openalex.org/W2950186769",
    "https://openalex.org/W1641025018",
    "https://openalex.org/W2089319476",
    "https://openalex.org/W2041614298",
    "https://openalex.org/W2132957691",
    "https://openalex.org/W1597533204",
    "https://openalex.org/W2100155737",
    "https://openalex.org/W2129139611",
    "https://openalex.org/W2099111195",
    "https://openalex.org/W285471286",
    "https://openalex.org/W87844232",
    "https://openalex.org/W2102078257",
    "https://openalex.org/W2155520241",
    "https://openalex.org/W2057900969",
    "https://openalex.org/W2009944560",
    "https://openalex.org/W1992300521",
    "https://openalex.org/W1975879310",
    "https://openalex.org/W2135771747",
    "https://openalex.org/W2171457693",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2097927681"
  ],
  "abstract": "We describe the implementation steps required to scale high-order character language models to gigabytes of training data without pruning.Our online models build character-level PAT trie structures on the fly using heavily data-unfolded implementations of an mutable daughter maps with a long integer count interface.Terminal nodes are shared.Character 8-gram training runs at 200,000 characters per second and allows online tuning of hyperparameters.Our compiled models precompute all probability estimates for observed n-grams and all interpolation parameters, along with suffix pointers to speedup context computations from proportional to n-gram length to a constant.The result is compiled models that are larger than the training models, but execute at 2 million characters per second on a desktop PC.Cross-entropy on held-out data shows these models to be state of the art in terms of performance.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8330523371696472
    },
    {
      "name": "Language model",
      "score": 0.6445102691650391
    },
    {
      "name": "Speedup",
      "score": 0.556086003780365
    },
    {
      "name": "Character (mathematics)",
      "score": 0.4536858797073364
    },
    {
      "name": "Trie",
      "score": 0.44940704107284546
    },
    {
      "name": "Computation",
      "score": 0.4431915879249573
    },
    {
      "name": "Suffix",
      "score": 0.4416607916355133
    },
    {
      "name": "Theoretical computer science",
      "score": 0.4096481204032898
    },
    {
      "name": "Parallel computing",
      "score": 0.3843843936920166
    },
    {
      "name": "Programming language",
      "score": 0.3126429319381714
    },
    {
      "name": "Artificial intelligence",
      "score": 0.30925631523132324
    },
    {
      "name": "Data structure",
      "score": 0.24160298705101013
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ]
}