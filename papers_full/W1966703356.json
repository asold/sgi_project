{
    "title": "Scaling high-order character language models to gigabytes",
    "url": "https://openalex.org/W1966703356",
    "year": 2005,
    "authors": [
        {
            "id": "https://openalex.org/A2106434598",
            "name": "Bob Carpenter",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1972594981",
        "https://openalex.org/W2884262036",
        "https://openalex.org/W2157963512",
        "https://openalex.org/W2950186769",
        "https://openalex.org/W1641025018",
        "https://openalex.org/W2089319476",
        "https://openalex.org/W2041614298",
        "https://openalex.org/W2132957691",
        "https://openalex.org/W1597533204",
        "https://openalex.org/W2100155737",
        "https://openalex.org/W2129139611",
        "https://openalex.org/W2099111195",
        "https://openalex.org/W285471286",
        "https://openalex.org/W87844232",
        "https://openalex.org/W2102078257",
        "https://openalex.org/W2155520241",
        "https://openalex.org/W2057900969",
        "https://openalex.org/W2009944560",
        "https://openalex.org/W1992300521",
        "https://openalex.org/W1975879310",
        "https://openalex.org/W2135771747",
        "https://openalex.org/W2171457693",
        "https://openalex.org/W1934041838",
        "https://openalex.org/W2097927681"
    ],
    "abstract": "We describe the implementation steps required to scale high-order character language models to gigabytes of training data without pruning.Our online models build character-level PAT trie structures on the fly using heavily data-unfolded implementations of an mutable daughter maps with a long integer count interface.Terminal nodes are shared.Character 8-gram training runs at 200,000 characters per second and allows online tuning of hyperparameters.Our compiled models precompute all probability estimates for observed n-grams and all interpolation parameters, along with suffix pointers to speedup context computations from proportional to n-gram length to a constant.The result is compiled models that are larger than the training models, but execute at 2 million characters per second on a desktop PC.Cross-entropy on held-out data shows these models to be state of the art in terms of performance.",
    "full_text": null
}