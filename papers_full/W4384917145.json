{
  "title": "Inductive Biased Swin-Transformer With Cyclic Regressor for Remote Sensing Scene Classification",
  "url": "https://openalex.org/W4384917145",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2432954772",
      "name": "Siyuan Hao",
      "affiliations": [
        "Qingdao University of Science and Technology",
        "Qingdao University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1905209375",
      "name": "Nan Li",
      "affiliations": [
        "Qingdao University of Technology",
        "Qingdao University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2108919451",
      "name": "Yuanxin Ye",
      "affiliations": [
        "Southwest Jiaotong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3201623325",
    "https://openalex.org/W3092261202",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3112898771",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W6682751323",
    "https://openalex.org/W2963863119",
    "https://openalex.org/W6803432384",
    "https://openalex.org/W343636949",
    "https://openalex.org/W3128592650",
    "https://openalex.org/W6747899497",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W2326925005",
    "https://openalex.org/W6771917389",
    "https://openalex.org/W3022140654",
    "https://openalex.org/W2919352650",
    "https://openalex.org/W2890732922",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W2515866431",
    "https://openalex.org/W3153239684",
    "https://openalex.org/W2974770574",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W4308703323",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4295864484",
    "https://openalex.org/W3205886311",
    "https://openalex.org/W3123212500",
    "https://openalex.org/W3136416617",
    "https://openalex.org/W3044492802",
    "https://openalex.org/W3017622525",
    "https://openalex.org/W3119125170",
    "https://openalex.org/W3119762784",
    "https://openalex.org/W3120080208",
    "https://openalex.org/W4226291728",
    "https://openalex.org/W4210576848",
    "https://openalex.org/W4220815323",
    "https://openalex.org/W6802913470",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W4293824888",
    "https://openalex.org/W3124539583",
    "https://openalex.org/W6687157873",
    "https://openalex.org/W2992382424",
    "https://openalex.org/W2190186811",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2785325870",
    "https://openalex.org/W2154579312",
    "https://openalex.org/W3212076252",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3207506706",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4225916372",
    "https://openalex.org/W3105577662",
    "https://openalex.org/W1686810756"
  ],
  "abstract": "Convolutional neural networks (CNNs) have been widely used in remote sensing scene classification. However, the long-range dependencies of local features cannot be taken into account by CNNs. By contrast, a visual transformer (ViT) is good at capturing the long-range dependencies as it considers the global relationship of local features by introducing a self-attention mechanism. Although the ViT can obtain a good result when training on large-scale datasets, e.g., ImageNet, it is hard to be adapted to small-scale datasets (e.g., remote sensing image datasets). This is attributed to the fact that the ViT lacks the typical inductive bias capability. Therefore, we propose the inductive biased swin transformer with cyclic regressor used with random dense sampler (IBSwin-CR) to improve the training effect of the swin transformer on remote sensing image datasets, which builds upon three modules, i.e., inductive biased shifted window multihead self-attention (IBSW-MSA) module, random dense sampler, and a regressor with cyclic regression loss. We obtain the inductive bias information and the long-range dependencies of the attention map by the IBSW-MSA module. Moreover, the final feature map goes through a random dense sampler, in which the additional spatial information is learned. Finally, the network is normalized by a cross-entropy loss function and a cyclic regression loss function. The proposed IBSwin-CR model is evaluated on public datasets such as NWPU-RESISC45 dataset and Aerial Image Dataset, and the experimental results show that the proposed network can achieve better performance than other classification models, especially for the case with a small number of samples.",
  "full_text": "IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023 6265\nInductive Biased Swin-Transformer With Cyclic\nRegressor for Remote Sensing Scene Classiﬁcation\nSiyuan Hao , Member, IEEE,N a nL i, and Yuanxin Ye, Member, IEEE\nAbstract—Convolutional neural networks (CNNs) have been\nwidely used in remote sensing scene classiﬁcation. However, the\nlong-range dependencies of local features cannot be taken into\naccount by CNNs. By contrast, a visual transformer (ViT) is good\nat capturing the long-range dependencies as it considers the global\nrelationship of local features by introducing a self-attention mech-\nanism. Although the ViT can obtain a good result when training\non large-scale datasets, e.g., ImageNet, it is hard to be adapted\nto small-scale datasets (e.g., remote sensing image datasets). This\nis attributed to the fact that the ViT lacks the typical inductive\nbias capability. Therefore, we propose the inductive biased swin\ntransformer with cyclic regressor used with random dense sampler\n(IBSwin-CR) to improve the training effect of the swin transformer\non remote sensing image datasets, which builds upon three mod-\nules, i.e., inductive biased shifted window multihead self-attention\n(IBSW-MSA) module, random dense sampler, and a regressor with\ncyclic regression loss. We obtain the inductive bias information and\nthe long-range dependencies of the attention map by the IBSW-\nMSA module. Moreover, the ﬁnal feature map goes through a\nrandom dense sampler, in which the additional spatial information\nis learned. Finally, the network is normalized by a cross-entropy\nloss function and a cyclic regression loss function. The proposed\nIBSwin-CR model is evaluated on public datasets such as NWPU-\nRESISC45 dataset and Aerial Image Dataset, and the experimental\nresults show that the proposed network can achieve better perfor-\nmance than other classiﬁcation models, especially for the case with\na small number of samples.\nIndex Terms—Loss function, remote sensing image, scene\nclassiﬁcation, self-supervised learning (SSL), swin transformer.\nI. INTRODUCTION\nW\nITH the advancement of technology, the resolution of\nremote sensing images is getting increasingly high,\nand their semantic information is getting richer. At present,\nhigh-resolution remote sensing (HRRS) images can be used in\nvarious application scenarios, such as urban planning, natural\ndisaster detection, and environmental detection. For the remote\nsensing scene classiﬁcation task, it is important to interpret the\nspatial and contextual information contained in the images due\nManuscript received 3 February 2023; revised 27 April 2023 and 26 May 2023;\naccepted 21 June 2023. Date of current version 19 July 2023. This work was\nsupported in part by the National Natural Science Foundation of China under\nGrant 62171247 and Grant 41921781.(Siyuan Hao and Nan Li contributed\nequally to this work.) (Corresponding author: Siyuan Hao.)\nSiyuan Hao and Nan Li are with the College of Information and Control\nEngineering, Qingdao University of Technology, Qingdao 266520, China (e-\nmail: lemonbananan@163.com; 2500852429@qq.com).\nYuanxin Ye is with the Faculty of Geosciences and Environmental En-\ngineering, Southwest Jiaotong University, Chengdu 610031, China (e-mail:\nyeyuanxin@home.swjtu.edu.cn).\nDigital Object Identiﬁer 10.1109/JSTARS.2023.3290676\nto the complex structure and spatial layout of HRRS images. The\nchallenges of remote sensing scene classiﬁcation include: large\nintraclass diversity, high interclass similarity, large variance of\nobject/scene scales, and coexistence of multiple ground ob-\njects [1]. With the development of deep learning, convolutional\nneural networks (CNNs) and visual transformer (ViT)-based\nmethods can effectively solve the aforementioned challenges\nand achieve advanced classiﬁcation accuracy. However, both\nCNNs-based and ViT-based methods require a large amount of\ntraining data with labels, which is difﬁcult for remote sensing\nscene datasets.\nFor the purpose of improving the training performance of the\nswin transformer on small-scale datasets (e.g., remote sensing\nimage datasets), we employ a self-supervised learning (SSL)\nstrategy and design an SSL module named random dense sam-\npler, which trains a regressor to predict the relative distances of\nthe sample points after sampling, so as to learn additional infor-\nmation in the remote sensing image. The inspiration comes from\nthe natural language processing (NLP) ﬁeld and the computer\nvision (CV) ﬁeld. ELECTRA[2] is a representative method in\nthe ﬁeld of NLP, which is different from the traditional masked\nlanguage modeling [e.g., bidirectional encoder representation\nfrom transformers (BERT)]. The latter one takes patches as the\ninput, randomly masks them, and trains the network to recon-\nstruct the original patches, which usually requires a large amount\nof training data, and it is computational expensive. Instead,\nELECTRA corrupts the input through a random replacement\nof some patches, and a discriminative model is trained to let\nthe network predict whether each patch in the input has been\nreplaced or not. Clark et al.[2] show that ELECTRA is more\nefﬁcient, especially for small volume models or relatively small\ntraining samples. While in the ﬁeld of CV , the commonly used\nmethod of SSL is to predict the relative positions of randomly\nsampled points. Following these ideas, we designed a random\ndense sampler module to increase the training accuracy when\nonly a small amount of training data is available.\nSpeciﬁcally, the random dense sampler module works as fol-\nlows. First, we generate two index matrices based on two random\nposition matrices. Next, we densely sample the ﬁnal feature map\ntwice according to the two index matrices, and then, we feed\nthe two sampled matrices to the multilayer perception (MLP)\nto regress their corresponding positions. After processing, we\nget the prediction relative position matrix. We use the hybrid\nloss strategy that combines supervised learning and SSL to suit\nthe remote sensing datasets. Consequently, we propose a SSL\nloss strategy for the random dense sampler, which is the cyclic\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6266 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nregression loss function. It is inspired by the video alignment\ntask, while we convert the video frame alignment problem into\na distance problem for randomly sampled points. The cyclic\nregression loss function is designed by minimizing the distance\nbetween the prediction relative position and the actual relative\nposition. Finally, we combine the cyclic regression loss with the\ncross-entropy loss to train the network. However, visual images\ndiffer from remote sensing images in that in visual images, the\nobject is located in the center of the image. Remote sensing\nimages, on the other hand, contain many objects and the objects\nare not in the middle of the image. Therefore, it is necessary to\nimprove the methods from the ﬁeld of CV and apply them to\nremote sensing scene classiﬁcation[3].\nThus, based on the characteristics of remote sensing datasets,\nwe propose the IBSW-MSA module, which not only combines\nthe inductive bias capability of CNNs with the self-attention\nmechanism to extract both the local features and the global re-\nlationships of them in images, but also supplements the random\ndense sampler to make the network more adaptive to extract\nfeatures in remote sensing images. Inductive bias refers to the\nassumptions or constraints making machine learning algorithms\nlearn from the data. For CNNs, inductive bias is the assumption\nthat the input data have a certain structure that can be exploited\nfor better learning. Speciﬁcally, CNNs assume that the input\nimage has a spatial structure and that the nearby pixels in the\nimage are more likely to be related to each other than the distant\nones. This is achieved by using a convolutional layer, where the\nconvolutional ﬁlter slides over the image to detect local infor-\nmation. Moreover, by sharing weights over the entire image,\nthe parameter number is reduced, making it computationally\nmore efﬁcient. In addition, the CNNs assumes that the input\nimage is translation invariant, which means that the position\nof the object in the image does not affect the capacity of the\nnetwork to recognize it. This is achieved by using pooling layer,\nwhich reduce the spatial dimensionality of the feature map and\ncapture the most important information from the upper layer\nwhile discarding some details. Overall, the inductive bias of\nCNNs allows it to learn effectively from the input data by\nexploiting the spatial information in the image. In recent years,\nCNNs have been widely used for the remote sensing scene\nclassiﬁcation tasks. For example, Szegedy et al. [4] applied\nGoogLeNet and other CNN-based networks in the remote sens-\ning scene classiﬁcation task, in which they pointed out that the\nﬁne-tuned pretrained networks can avoid overﬁtting problems\nand accelerate convergence[5]. Kattenborn et al.[6] used deeper\nCNNs to extract various vegetation attributes in remote sensing\nimages, and they demonstrated that CNNs is superior to shallow\nnetworks in plant detection. In general, CNNs can achieve good\nperformance in the different domains. However, CNNs lack\nof interaction between local features in the image as a whole\nwhen stacking receptive ﬁelds. As an alternative to CNNs, a\ntransformer [7], can directly model the local features globally\nusing a self-attention mechanism.\nTransformer was ﬁrst proposed in 2017 in the NLP ﬁeld[7].\nIn recent years, the transformer has also been used in remote\nsensing scene classiﬁcation. For example, Zhang et al. [8]\nused a transformer in combination with CNNs, innovatively\nintegrated the self-attention mechanism in ResNet[9], proposed\naC N N→Convolution+Transformer →Transformer structure,\nand achieved good results in remote sensing scene classiﬁcation.\nThen, the improved transformer called ViT was designed by\nDosovitskiy et al. [10], and Bazi et al.[11] use the ViT for\nremote sensing scene classiﬁcation. Since the ViT has a self-\nattention mechanism for the whole image, the computational\ncomplexity is relatively large and the computational efﬁciency\nis low. To reduce the computational complexity and improve\nthe computational efﬁciency, Liu et al.[12] proposed the shift\nwindow transformer (swin transformer).\nAlthough the self-attention mechanism makes up for the lack\nof receptive ﬁelds in CNNs, a new shortcoming also emerges\ndue to the lack of typical CNNs inductive bias capability, that is\nto say, a large amount of training data is required to achieve good\nresults. Deng et al.[13] proposed the use of convolutional layers\nin combination with self-attention layers. The idea is to model\nlong-range dependencies while retain inductive bias ability to\nextract local features inside the image. The experimental results\nshow that the training accuracy is higher than that of classical\nCNNs, such as ResNet. However, this method uses two stream\nstructure and no process of information interaction during train-\ning, so that there are problems such as too many parameters and\ninsufﬁcient information interaction.\nIn this article, we take the inductive bias ability of CNNs\ninto account in the self-attention mechanism, and design the\nIBSW-MSA module, which embeds the inductive bias capabil-\nity of CNNs in the self-attention mechanism, which not only\navoids redundant parameters but also deeply interacts with the\ninformation obtained. IBSW-MSA module has the advantages\nof both the self-attention mechanism and the inductive bias\ncapability, and can extract the local features along with the global\nrelationship of local features. Moreover, the ability to extract\nadditional spatial information from remote sensing images by\nrandom dense sampler is improved. The proposed IBSW-MSA\nmodule has a positive impact on the training of the network\nwithout signiﬁcantly increasing the computation. At the same\ntime, the proposed IBSW-MSA module is more effective for\nthe task in which only a small amount of training data are\navailable.\nThe contributions can be summarized as follows.\n1) We have proposed an IBSW-MSA module that integrates\nCNN into the original SW-MSA module[12] and makes\nup for its inductive bias capability. Therefore, it is able to\nextract more informative features.\n2) We have proposed a random dense sampler module to\nlearn additional spatial information that works in a self-\nsupervised manner. A regressor is trained to predict the\ndistances of the dense sampled positions from their corre-\nsponding feature maps, and thus, it can strengthen the posi-\ntion information preserved in the feature maps. Besides, a\ncyclic regression loss function is introduced to process the\nresults obtained from the random dense sampler module,\nwhich can better regularize the whole network.\n3) IBSwin-CR model performs well not only on small-scale\ndatasets, but also on some medium-scale datasets. Exper-\nimental results show that our model can achieve higher\naccuracy on small datasets, regardless of whether it is\ntrained from scratch or ﬁne tuned from a pretrained model.\nHAO et al.: INDUCTIVE BIASED SWIN-TRANSFORMER WITH CYCLIC REGRESSOR 6267\nThe rest of this article is organized as follows. SectionII\nbrieﬂy reviews the related works on remote sensing scene classi-\nﬁcation and some transformer models. SectionIII introduces the\nproposed method. SectionIV evaluates the effectiveness of the\nproposed method using two remote sensing scene datasets and\nanalyzes the experimental results. Finally, SectionVI concludes\nthis article.\nII. RELATED WORK\nIn this section, we ﬁrst brieﬂy review CNN-based and\ntransformer-based methods in the domain of remote sensing,\nwhose development is described as follows. Moreover, the SSL\nand loss function are analyzed.\nA. CNNs for Remote Sensing\nAs an important tool for CV tasks, CNNs can be traced\nback to 1998, when the ﬁrst proposed neural network was\ncalled LeNet [14], but limited by the computer hardware at\nthat time, LeNet did not receive sufﬁcient attention. It was\nnot until the emergence of AlexNet[15] that CNNs became\nthe mainstream algorithm. Recently, deeper and more efﬁcient\nCNNs have been proposed, such as VGG[16], GoogLeNet[4],\nResNet [9], DenseNet [4], etc. The excellent performance of\nCNNs in different tasks, such as scene classiﬁcation, image\nsegmentation, and object detection, has attracted attention of\nthe researchers in the ﬁeld of remote sensing. For example,\nin 2015, Castelluccio et al.[5] ﬁne-tuned GoogLeNet for land\nclassiﬁcation tasks in remote sensing. CNNs can also be used\nas initial feature map extractors. For example, Zhang et al.[17]\ninnovatively used CNNs in combination with CapsNet, where\nCNNs are responsible for extracting image features which will\nlater be input to CapsNet to obtain the ﬁnal classiﬁcation results.\nSun et al. [18] pointed out that the traditional classiﬁcation\nmethods ignore the hierarchical structure of multilayer convo-\nlutional features, which brings some interference information.\nTherefore, GBNet was proposed, which not only quantitatively\nanalyzes the performance of each convolutional feature, and\nthus, selects the optimal combination of convolutional features,\nbut also eliminates the interference information by using the\ngated function. In the ﬁeld of hyperspectral, the typical small\nsample setup problem is also faced, so Makantasis et al.[19]\nproposed the Rank-R FNN, which solves the small sample setup\nproblem by normalizing the decomposition of its parameters.\nAlthough CNNs and its variants can signiﬁcantly improve the\naccuracy of remote sensing scene classiﬁcation, there are some\nlimitations: the receptive ﬁeld of the convolutional kernel limits\nthe network to learn the global features, i.e., long distance\ndependencies. This problem was later solved by the transformer.\nB. Transformer for Remote Sensing\nTransformer has been widely used in the ﬁeld of NLP since it\nwas proposed by Vaswani et al. in 2017[7]. It contains three main\nparts, which are encoder, decoder, and self-attention module.\nThen, BERT, which was a variant of the vanilla transformer, only\nuses the encoder part of the transformer as the main body[20].\nSimilarly, the generative pre-trained transformer[21], [22], [23]\nis stacked using the decoder part of the transformer. By 2021,\nmany variants of the transformer have been designed and widely\nused for CV tasks, such as classiﬁcation, semantic segmentation,\nobject detection, image generation, etc. Among them, the ViT\ncan learn the global features of the whole image by tokens,\nthus utilizing the long-range dependencies well, which solves\nthe limitations of CNNs. Therefore, many research groups use\nCNNs in combination with transformer. Wu et al.[24] combined\nCNNs with the ViT, redesigned the token and transformer block,\nand achieved superior performance compared to ResNet when\ntraining on ImageNet. While Yuan et al. [25] proposed the\nCeiT structure, extracting local shallow features with CNNs and\nusing the transformer to establish long-range dependencies. The\nexperts also brought these insights into the domain of remote\nsensing. Some works have applied the pure ViT to a wide range\nof scenarios. For instance, Bazi et al.[11] applied the ViT to the\nremote sensing classiﬁcation task. Bashmal et al.[26] also ap-\nplied the ViT to remote sensing scene classiﬁcation and explored\nthat the network still achieved good performance after pruning\nhalf of its layers. Kaselimi et al.[27] proposed ForestViT, which\ndoes not use any convolutional operations but achieves more\nsatisfactory experimental results than well-established convolu-\ntional structures such as VGG and DenseNet. Zhang et al.[8]\nproposed the remote sensing transformer (TRS), which inno-\nvatively integrated the self-attention mechanism into ResNet\nand improved the classiﬁcation accuracy with the novel design\nof multiheaded self-attention mechanism. SCViT proposed by\nLv et al.[28] fully considers the spatial information in remote\nsensing images and the contribution of classiﬁcation token in\ndifferent channels. Inspired by previous work, Deng et al.[13]\ncombined ViT and CNNs to propose CTNet, which can extract\nboth semantic features and local structural features from images,\nand ﬁnally, achieve high classiﬁcation performance. However,\nthe computational cost of the ViT is very high. In order to\nreduce the computational effort and complexity of operations,\nLiu et al.[12] improved the ViT by introducing the shift window\nmechanism, which is called swin transformer. After that, Hao\net al. [29] combined the swin transformer and sobel operator\nto improve the accuracy of remote sensing scene classiﬁcation.\nThe aforementioned improvements can achieve good results,\nhowever, they require large-scale datasets to train the model.\nC. SSL and Loss Functions\nSSL mainly uses pretext tasks to extract its supervised infor-\nmation in unsupervised data, breaking the reliance on manual la-\nbeling, and it can train networks efﬁciently even without labeled\ndata. SSL was ﬁrst applied in NLP ﬁeld, a typical pretext task\nis similar to a form of completion ﬁll-in-the-blank task, where a\nword is masked in the input sentence and the network is asked to\nmake a prediction of the missing word. For example, BERT[20]\nmasks the input patches randomly and trains the network to\nreconstruct the masked patches, which usually requires a large\namount of training data. Clark et al.[2] proposed ELECTRA to\nreduce the data requirements, in which a pretext task is designed\nto replace the input patches randomly, and a discriminative\n6268 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 1. Illustration of the difference between visual images and remote sensing\nscene images. (a) Visual image samples: Focused on the object. (b) Remote\nsensing image samples: Contain many objects, and the object is small as well\nas not in the image center.\nmodel is trained to make the discriminative network predict\nwhether each patch in the input has been replaced.\nIn the ﬁeld of CV , SSL strategies are divided into four main\ncategories: contrast SSL, noncontrast SSL, generative SSL, and\npredictive SSL. In this article, we mainly introduce predictive\nSSL, which makes predictions by learning semantic information\nand relevant features of regions in images. For example, by\nextracting random pairs of patches in each image and training\nthe network to predict the relative position of the second patch\nto the ﬁrst, Doersch et al.[30] improved the performance of\nthe model. Liu et al.[31] learned additional information in the\nimage by predicting the relative distances of sample points.\nThe other predictive pretext task is the image colorization pro-\nposed by Zhang et al.[32]. After graying the image, Zhang\net al. [32] trains an encoder to recolor the image by minimiz-\ning the mean square error of the reconstructed and original\nimages. Another well-known prediction task is RotNet[33],\nwhich is used to predict the angle of random image rotation\nby training the network to extract the angle information in the\nimage.\nHowever, different from visual images, which are generally\nfocused on the object, remote sensing images may contain\nobjects of varying sizes and shapes and contain a large amount\nof texture and structural information as shown in Fig.1. Speciﬁ-\ncally, in Fig.1(b), the image tag on the left side is airplane. It can\nbe seen that the object is small and not in the image center, and\nthere are residential buildings around it. The image tag on the\nright is baseball diamond, the object is still small and not in the\nmiddle of the image. There are basketball court, playground, and\nresidential buildings in the image. Therefore, SSL was adjusted\nfor application in remote sensing scene classiﬁcation to obtain\nbetter results in feature extraction. For example, Tao et al.[34]\nconducted a relative study on remote sensing scene classiﬁcation\nwith limited samples using pretext tasks such as image restora-\ntion and relative position prediction. While Zhao et al.[35]\nproposed to combine SSL and scene classiﬁcation tasks with a\nhybrid loss strategy of self-supervised and supervised training.\nThis improves the feature extraction and generalization abilities\nof the model.\nBased on the previous works, we designed an SSL module\nnamed random dense sampler. Through predicting the geometric\ndistance of the sampled position, a random dense sampler makes\nthe network learn additional information in the remote sensing\nimages. Inspired by [35], we decided to use the strategy of\nmixed loss function to combine SSL with supervised learning to\nimprove the feature extraction ability of the network. Besides,\nwe use the cyclic regression loss function as the self-supervised\nloss function.\nThe cyclic regression loss function is inspired by Dwibedi\net al. [36], who proposed a temporal cycle consistent learning\nstrategy in the video alignment problem, referring to ﬁnding\ncorresponding frames in multiple videos. More speciﬁcally,\nDwibedi et al.[36] built an embedded space to ﬁnd as many\ncorresponding video frames as possible between multiple video\nsequences. Different from ﬁnding corresponding video frames,\nwe need to compare the distance between the predicted rela-\ntive position and the actual relative position computed based\non the random dense sampler. Therefore, we propose a cyclic\nregression loss function, which obtains the distance from the\npredicted relative position and the actual relative position. The\nsmaller the distance between the predicted relative position and\nthe actual relative position, the more correspondence between\nthem, indicating that the network makes better predictions.\nIII. PROPOSED METHOD\nA. Overall Framework\nThe novelty of our proposed IBSwin-CR mainly lies in three\naspects: IBSW-MSA module, random dense sampler, and loss\nfunction. The diagram of the ﬁrst two modules is shown in Fig.2.\nWe can observe that the image ﬁrst passes through the W-MSA\nmodule, and the global relationship between the patches is\nextracted through the window multihead attention mechanism.\nNext, the features pass through our IBSW-MSA module, which\nnot only models the global information but also introduces the\ninductive biased information into the feature maps. Finally, the\nproposed random dense sampler is performed on the feature\nmaps. The sampler ﬁrst generates two random position matrices\nusing the random number generator, and then, samples the\nfeature map and regresses the prediction relative position matrix.\nIn addition to the aforementioned two modules, a new loss\nfunction is designed (i.e., the cyclic regression loss function),\nby minimizing the normalized square distance of the prediction\nrelative position and the actual relative position. Now, we would\nlike to provide the detailed descriptions as follows.\nB. IBSW-MSA Module\nIn this part, we introduce the IBSW-MSA module, which\nallows the network to retain both local features and the global re-\nlationship between them. We redesigned the transformer block.\nFig. 2 (top row) shows the structure of the IBSwin-CR trans-\nformer block, which consists of two parts, W-MSA and IBSW-\nMSA. The formulas of the W-MSA module and the IBSW-MSA\nmodule are shown as follows:\nX∗\ni−1 = W −MSA(LN(Xi−1)) +Xi−1\nXi = MLP(LN(X∗\ni−1)) +X∗\ni−1\nX∗\ni = IBSW −MSA(LN(Xi)) +Xi\nXi+1 = MLP(LN(X∗\ni )) +X∗\ni (1)\nHAO et al.: INDUCTIVE BIASED SWIN-TRANSFORMER WITH CYCLIC REGRESSOR 6269\nFig. 2. Overview of the IBSwin-CR framework. It is composed of four modules: (a) W-MSA module[12]; (b) IBSW-MSA module; (c) Random dense sampler,\nand cyclic regression loss function. The cyclic regression loss function is not marked in this ﬁgure; see SectionIII-D for details. The feature map processed by\nW-MSA is used as the input of IBSW-MSA, through which the additional local features along with the global relationship of them in the remote sensing image are\nextracted. The ﬁnal feature map is sent to the random dense sampler for processing, and the output is fed to the cyclic regression loss function, where additional\nspatial information is extracted from the image.\nFig. 3. Yellow box and the blue box indicate that the image regions in these\nwindows are neither contained in (a) nor (b). (a) Traditional windows. (b) Shifted\nwindows.\nwhere X∗\ni and Xi+1 represent the outputs of IBSW-MSA and\nMLP on the blocki. Each input in the formula is normalized by\nlayer norm (LN).\nFig. 3(a) shows the window partition mechanism in W-MSA,\nand it can be seen that the whole image is divided into four\nindependent windows. The self-attention calculation is only\nperformed within each individual window, and no information\nis transferred across different windows. To compensate this,\na window sliding mechanism is introduced. In Fig.3(b),t h e\nwindow slides ⌊M\n2 ⌋ units from the upper left corner, andM\nis the size of the window. By shifting the windows by⌊M\n2 ⌋\nunits, some regions that originally belong to different windows\nin (a) can be partitioned into the same window in (b).B y\nconcatenating (a) and (b), we have the SW-MSA module, which\nis originally introduced in[12]. The sliding operation allows\nmore interactions between different regions. However, this only\npartially solve the issue, and some regions remains independent\nand cannot be partitioned into the same window. These regions\nare shown in the yellow and blue windows in Fig.3. These\nwindows are neither included in(a) nor in(b).\nTo make up for the shortcomings mentioned previously, we\nintroduce inductive bias capability into the network. As shown in\nFig. 2(b), we designed a two-stream structure, the ﬁrst stream is\nthe original SW-MSA, and the other stream is IB branch, which\nis a light convolutional network. The IB branch is performed on\nthe feature maps before partition. By sliding the convolutional\nkernels across the entire feature maps, the regions in the blue\nand yellow windows can also be covered by the sliding kernels.\nFinally, we conduct an in-depth interaction of the dual stream\noutput. In this way, the IBSW-MSA module could capture both\nthe global relationship and ﬁne-grained local features without\nsigniﬁcantly increasing the parameters, as well as improve the\nperformance of the network on small and medium-sized datasets.\nC. Random Dense Sampler\nThe goal of our work is to encourage the network to obtain\ngood classiﬁcation performance for small- and medium-sized\ndatasets. Therefore, we propose an SSL module, which allows\nthe network to learn additional spatial information by randomly\nsampling multiple points and enforcing the network to predict\nthe relative distance between sampled points.\nThe details are shown in Fig.4. The top row shows the ﬂow\nchart of the classical classiﬁcation network, while the bottom\npart shows how the random dense sampler works.\nFirst, the random position matrices are generated by a random\nnumber generator. The matrix elements are random integers\nbetween 0 andk-1. Here,k is 7 and it represents the dimension\nof feature maps. Next, the index matrix is obtained by column\noperation: the second column of the random position matrix\nis multiplied byk and added to the ﬁrst column. And then, the\nﬁnal feature map will be densely sampled twice according to two\n6270 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 4. Flow diagram of the random dense sampler. First, we generate two random position matrices, and generate theprediction relative position matrix\nby MLP after dense sampling the feature map. On the other side, two random position matrices are correspondingly subtracted to generate theactual relative\nposition matrix. P rediction relative position matrix, which is normalized by softmax to obtain the predicted label. Extracting theactual relative\nposition matrixby column, and each column is the true label. The predicted label and true label are sent to the cyclic regression loss function, which will be\nintroduced in the next section.\nindex matrices to obtain sampled matrices, respectively. Finally,\nby concatenating the two sampled matrices and sending it to\nMLP, the prediction relative position matrix will be obtained.\nThe actual relative position matrix (random position matrices\nsubtraction) and the prediction relative position matrix that work\nas true label and predicted label will be fed into the subsequent\ncyclic regression loss function, thus helping the network to be\nregularized.\nD. Cyclic Regression Loss Function\nFor the issue of video alignment, Dwibed et al.[36] proposed\na time-cycle consistency SSL method, which aligns videos by\nﬁnding the closest video frame in an embedded space. The\nalignment issue is similar to our task. In our task, we expect\nto align the prediction relative position matrix with the actual\nrelative position matrix. Inspired by Dwibed et al.[36],w e\ndecide to apply Gaussian prior to the prediction relative position\nmatrix after random dense sampling, and constrain the network\nby minimizing the normalized square distance of the prediction\nrelative position and the actual relative position. We deﬁne the\nloss as\nLoss = |i−μ|2\nσ2 + αlogσ (2)\nwhere μ = ∑C\nj βj ∗j, σ2 = ∑C\nj βj ∗(j −μ)2, C stands for\nchannels, and j represents an integer from 1 to the channels.\nβ is the prediction relative position matrix. The 2-D matrix\nis converted into a 1-D by matrix multiplication, and we get\nthe predicted labelμ. Whilei is the true label.α is the weight\ncoefﬁcient, αis 0.001 in the experiment. We impose a Gaussian\nprior onβ and aim to minimize the squared distance|i−μ|2\nσ2 .\nAfter processing by the random dense sampler module, two\nloss are generated [seeLossin (3)]. We use hybrid loss strategy\nof self-supervised training and supervised training. Therefore,\nthe total loss function is expressed as\nLoss = |ix −μx|2\nσ2x\n+ αlogσx + |iy −μy|2\nσ2y\n+ αlogσy\nLtot = Lce + λLoss (3)\nwhere Ltot is the total loss function,Lce is the supervised learning\nloss strategy cross-entropy loss function, and Loss is the SSL loss\nstrategy cyclic regression loss function. Experiments show that\nwhen λ =0 .5, the network can achieve the best performance.\nIV . EXPERIMENTS\nIn this section, we evaluate our proposed model on two pub-\nlicly available remote sensing scene datasets. First, we describe\nthe datasets and the experimental setup. Next, the ablation\nexperiments are conducted to verify the effectiveness of our\nproposed module. Finally, our proposed method is compared\nwith other methods.\nA. Datasets\nTo demonstrate the effectiveness of our method, we make\nuse of two remote sensing scene datasets, namely, NWPU-\nRESISC45 dataset and Aerial Image Dataset (AID). These\ndatasets are described in detail as follows.\nHAO et al.: INDUCTIVE BIASED SWIN-TRANSFORMER WITH CYCLIC REGRESSOR 6271\nFig. 5. Example images of the NWPU dataset: (1) airplane; (2) airport; (3)\nbaseball diamond; (4) basket-ball court; (5) beach; (6) bridge; (7) chaparral;\n(8) church; (9) circular farmland; (10) cloud; (11) commercial area; (12) dense\nresidential; (13) desert; (14) forest; (15) freeway; (16) golf course; (17) ground\ntrack ﬁeld; (18) harbor; (19) industrial area; (20) intersection; (21) island;\n(22) lake;(23) meadow;(24) medium residential; (25) mobile home park; (26)\nmountain; (27) overpass; (28) palace; (29) parking lot; (30) railway; (31) railway\nstation; (32) rectangular farmland; (33) river; (34) roundabout; (35) runway;\n(36) sea ice; (37) ship; (38) snow berg; (39) sparse residential; (40) stadium;\n(41) storage tank; (42) tennis court; (43) terrace; (44) thermal power station;\nand (45) wetland.\nFig. 6. Example images of AID: (1) airport; (2) bare land; (3) baseball ﬁeld; (4)\nbeach; (5) bridge; (6) center; (7) church; (8) commercial; (9) dense residential;\n(10) desert; (11) farmland; (12) forest; (13) industrial; (14) meadow; (15)\nmedium residential; (16) mountain; (17) park; (18) parking; (19) playground;\n(20) pond; (21) port; (22) railway station; (23) resort; (24) river; (25) school;\n(26) sparse residential; (27) square; (28) stadium; (29) storage tanks; and (30)\nviaduct.\n1) NWPU-RESISC45 Dataset: As shown in Fig. 5,t h e\nNWPU dataset is a large-scale remote sensing dataset including\n45 scene categories with a total of 31 500 remote sensing scene\nimages created by Northwestern Polytechnic University. Each\nof these categories has 700 images, and the resolution of each\nimage is 256× 256.\n2) Aerial Image Dataset (AID):As shown in Fig.6,t h eA I D\nwas collected by Wuhan University and released in 2016. The\nAID is a multisource dataset consisting of different countries\nand regions with 10 000 remote sensing scene images covering\n30 scene classes. Each class has 200–400 images with image\nsize of 600× 600.\nB. Experimental Setup\nThe software environment is Ubuntu 18.04.6 LTS with the\ndeep learning framework Pytorch 1.8.0 and Python 3.8. All\nexperiments were run on a server conﬁgured with an Intel(R)\nTABLE I\nSYMBOL INDICATES ANDMODEL COMPOSITION\nXeon(R) Silver 4214R central processing unit at 2.40 GHz and\nfour NVIDIA Geforce RTX 3090 high-speed GPUs.\nWe use a swin transformer as the backbone of the network. In\naddition, we loaded the pretraining parameters of ImageNet21 K\nonto IBSwin-CR. The ofﬁcially provided pretraining parameters\nare available in[37]. We chose the Adamw optimizer with an\ninitial learning rate of 0.000125 and a weight decay of 0.05 to\noptimize the parameters of IBSwin-CR.\n1) Image Size:For the NWPU dataset, the image size is set to\n224 ×224. For the AID, when conducting ablation experiment,\nthe image size is set to 224 × 224. While compared with\nother methods in SectionIV-D, we resize the image to 384×\n384 pixels. We also use data augmentation techniques, such as\nrandom ﬂip, random rotate, random erase, to make image more\ndiverse.\n2) Training and Testing Ratios:We divide the dataset into\ntraining and testing samples according to the common division\nratios. The AID uses 20% and 50% of the training ratios, respec-\ntively. The NWPU dataset uses 10% and 20%, respectively.\n3) Evaluation Metrics: Common evaluation metrics for\nscene classiﬁcation include overall accuracy (OA) and confusion\nmatrix (CM). In order to avoid unequal distribution of image\nfeatures in the dataset and to improve the reliability of the\nclassiﬁcation results, all experiments are repeated ten times, and\nthe dataset is redivided randomly for each text. The experimental\nresults are denoted by the average OA with standard deviation\n(OA ± STD).\n4) Symbol Indicates: In order to verify the effectiveness of\nthe proposed method, we implemented the ablation study. The\nsymbol indicates are given in TableI, and their speciﬁc meaning\nis explained as follows.\n1) Vanilla swin: original swin transformer model.\n2) IBSwin: Vanilla swin transformer model, but in which the\nSW-MSA module is replaced by the proposed IBSW-MSA\nmodule.\n3) SW2Swin: Vanilla swin transformer model, but stack SW-\nMSA module two times.\n4) Vanilla Swin+CR: Vanilla swin transformer model opti-\nmized with the cyclic regression loss function and random\ndense sampler.\n5) IBSwin-CR: Our proposed model with the IBSW-MSA\nmodule, cyclic regression loss function, and random dense\nsampler.\n6) IBwin+CR: Replace two-stream IBSW-MSA module\n(SW-MSA branch + IB branch) with one single\n6272 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 7. For NWPU dataset, at training ratios of 10% and 20%, the accuracy of\ntraining methods from scratch is compared among different methods. IBSwin-\nCR always has the best performance among different training ratios.\nFig. 8. For AID, at training ratios of 20% and 50%, the accuracy of training\nmethods from scratch is compared among different methods. IBSwin-CR always\nhas the best performance among different training ratios.\nconvolutional IB-branch. And optimized with the cyclic\nregression loss function and random dense sampler.\nC. Ablation Study\nOur proposed IBSwin-CR model is an optimized swin trans-\nformer using the IBSW-MSA module, random dense sampler,\nand cyclic regression loss function, in which the random dense\nsampler provide the input for the cyclic regression loss function\nto better regularize the whole network.\nFirst, we would like to explore how the training accuracy is\naffected by these modules, especially for the case with a small\namount of training data. We selected the Swin-T with relatively\nsmall amount of parameters as the backbone of our model. In this\ntest, we trained all the models from scratch, and the experimental\nresults are given in Figs.7 and 8.\nFig. 7 shows the training accuracy histogram provided by\ndifferent models using different training ratios for the NWPU\ndataset. We can observe the following.\n1) When training epoch= 100 and training ratio= 10%, the\ntraining accuracy of the vanilla swin is 0.63% and 0.61%\nlower than those of IBSwin and IBSwin-CR, respectively,\nsuggesting that the IBSW-MSA module has a positive\nimpact on improving the training accuracy.\n2) When training epoch = 100 and training ratio= 10%,\nthe accuracy of vanilla swin is 1.1% higher than those\nof Vanilla Swin+CR. It seems that combine the cyclic\nregression loss function with the random dense sampler\n(CR) degrades the performance. However, this is not the\nfact, as the training epochs are not large enough to make\nthe network converge.\n3) While the training epoch is increased to 300 and training\nratio = 10%, both IBSwin and Vanilla Swin-CR outper-\nforms vanilla swin. This indicates that both the IB branch\nand CR can boost the performance. Besides, our proposed\nmodel, IBSwin-CR, has be the best performance, which\nindicates that the combination of IB branch and CR can\nfurther boost the performance.\nSo, we can draw a conclusion that IBSwin-CR can achieve\nideal training effect when few epochs are trained. When the\nnumber of training epoch goes up, the improvement is more\nobvious.\nIn the case of training ratio= 20%, we can observe that\n1) Similar to the cases mentioned previously in which train-\ning ratio = 10%, when the training epoch = 100, the\nmodels fail to converge. Thus, we focus on the cases in\nwhich the models converge (i.e., when the training epoch\n= 300).\n2) When the training epoch is increased to 300, the accuracy\nof SW2Swin is lower than the one of IB-Swin, which in-\ndicates that stacking an IB branch works more effectively\nthan stacking another SW branch. Both IB branch and SW\nbranch have similar amount of parameters. This indicates\nthat the performance gain is not obtained by increasing\nthe amount of parameters, but by the speciﬁc design of\nour IBSW-MSA module.\n3) Moreover, both IBSwin and Vanilla Swin-CR outperforms\nvanilla swin. Again, this observation indicates that both\nIB branch and CR could boost the performance. Besides,\nwhen combining IB branch and CR, our proposed model,\nIBSwin-CR, has the best performance.\n4) It is worth mentioning that the IBwin-CR model, in which\nthe SW-MSA is replaced by the IB branch, has inferior per-\nformance compared with our IBSwin-CR. Similar results\ncould be observed in SW2Swin model. These observations\nindicate that our IBSW-MSA module that integrates both\nIB branch and SW-MSA branch work more effectively\nthan the case which only uses IB branch or SW-MSA\nbranch.\nIn general, from the aforementioned observations, we can con-\nclude that the performance is gradually improved after adding\nCR and replacing SW-MSA with IBSW-MSA module, and\nthe improvement is up to 2.28% compared with vanilla swin.\nHAO et al.: INDUCTIVE BIASED SWIN-TRANSFORMER WITH CYCLIC REGRESSOR 6273\nBesides, we have also shown that the IBSW-MSA module works\nbetter than SW2-MSA and IB branch, which indicates that the\nperformance gain is not brought by the increased amount of\nparameters, but by the combination of IB branch and SW-MSA\nbranch.\nThe experimental results of AID is shown in Fig.8, which\nshows the training accuracy histogram for the AID using dif-\nferent training ratios for different models. We can observe the\nfollowing.\n1) When the training epoch= 100 and training ratio= 20%,\nthe training accuracy of vanilla swin is the lowest, which\nindicates that IBSW-MSA module and CR module either\nalone or in combination have a positive effect on training\nthe network from scratch on small datasets. Among them,\nIBSwin-CR improves the most.\n2) When the training epoch=100 and training ratio=50%,\nIBSwin-CR and IBSwin achieves accuracy beyond vanilla\nswin, which demonstrates the effectiveness of the IBSW-\nMSA module. In addition, our proposed model IBSwin-\nCR has the best performance, which indicates that the\ncombination of the IB branch and CR can further improve\nthe performance.\n3) When the training epoch= 300, Vanilla Swin+CR has the\nlowest training accuracy, which indicates that the method\nin the CV domain does not work well if applied directly to\nremote sensing datasets without adjustment. Meanwhile,\nboth IBSwin-CR and IBwin+CR accuracies are higher\nthan vanilla swin indicating that the CR module can be\nadapted to the remote sensing datasets and achieve good\nclassiﬁcation results with the pairing of the IBSW-MSA\nmodule.\nThe similar conclusion can be drawn that the training accuracy\nof the proposed IBSwin-CR is better than those of other models.\nFurthermore, from Figs.7 and 8, we further observed that the\nimpact of the IBSW-MSA module on accuracy does not only\nrely on increasing the number of parameters, but also on the\nspeciﬁc design. When the network convergences (i.e., training\nepoch = 300), we can have the following the experimental\nresults. For the NWPU dataset (see Fig.7), when training ratio\n= 20%, the accuracy of IBSwin is 1.38% higher than that of\nSW2Swin. For the AID (see Fig.8), the training accuracy of\nIBSwin is 0.39% higher than that of SW2Swin, with the training\nratio=20%. Moreover, when the training ratio is increased to\n50%, this improvement is more obvious. This is because our\nIBSW-MSA module does not obtain good performance by in-\ncreasing the amount of parameters, but by the speciﬁc design of\nour IBSW-MSA module. It is more effective to integrates both\nIB branch and SW-MSA branch than the case which only uses\nIB branch or SW-MSA branch.\nFinally, we plan to validate the effectiveness of the cyclic re-\ngression loss function. In this test, we use the training ratio=10%\nand training epoch= 300 on the NWPU dataset. The IBSW-\nMSA module, random dense sampler are used in both networks\nand the supervised learning loss function is kept constant [e.g.,\nLce in (3)]. By changing the SSL loss function, the validity of\nthe circular regression loss function is veriﬁed. After replacing\nFig. 9. Different SSL loss functions corresponding to the training accuracy\nare described in the NWPU dataset (training epoch= 300 and training ratio=\n10%). It can be observed that when the SSL loss strategy is our cyclic regression\nloss function, the training effect of the network is the best.\nthe SSL loss function with L1, L2, and cross-entropy loss,\nrespectively, the experimental results are shown in Fig.9.\nWith the same structure of the network (e.g., IBSW-MSA\nmodule and random dense sampler), we only change the loss\nfunction of SSL and feed the actual relative position matrix and\nthe prediction relative position matrix after dense sampling into\nL1 loss, L2 loss, cross-entropy loss, and cyclic regression loss,\nrespectively. It can be seen that when cross-entropy loss is used,\nthe training accuracy is the lowest, and the training accuracy of\nL1 loss and L2 loss is similar, higher than that of cross-entropy\nloss. Our cyclic regression loss works best and is much better\nthan other losses.\nAt the end of this section, we would like to verify the gener-\nalization ability of our method. We used the pretrained models,\nwhose parameters can be downloaded from [37]. Since the\narchitecture of the swin transformer (i.e., SW-MSA) is replaced\nby the proposed IBSW-MSA module, we cannot directly load the\nofﬁcial pretrained parameters. To this end, we partially loaded\nthe ofﬁcial pretrained parameters, and used the remote sensing\nimages to train IB branch from scratch. We still selected the\nSwin-T network as our backbone, and the image size is set to\n224 ×224, training epoch= 300. The training ratio is varied in\nthe set of 10%, 20% for NWPU dataset, and 20%, 50% for AID.\nFrom Fig. 10, when the training ratio= 10% and training\nepoch = 300, the training accuracy of IBSwin-CR is the highest,\nand it has a 0.04% accuracy higher compared with vanilla swin.\nWhen training ratio goes to 20%, this improvement can reach\n0.09%. However, the accuracy of other models is lower than\nthat of vanilla swin. For the AID, as shown in the Fig.11, when\nthe training ratio = 20%,50% and training epoch= 300, the\naccuracy of the vanilla swin is 0.03%, 0.3% lower than that of\nIBSwin-CR.\nWe found that compared with the models trained from scratch,\nthe performance of all the models except for IBSwin-CR is\nlower than vanilla swin. This is because vanilla swin already ob-\ntained relatively good parameters by loading the pretrained ones.\n6274 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 10. For NWPU dataset, at training ratio of 10% and 20%, training\n300 epochs by using the pre-training method. The accuracy of different methods\nis compared. IBSwin-CR always has the best performance.\nFig. 11. For AID, at training ratio of 20% and 50%, training 300 epochs by\nusing the pre-training method. The accuracy of different methods is compared.\nIBSwin-CR can always surpass the Vanilla Swin.\nSome modules in other models need to be trained from scratch,\nsuch that the performance might be decreased. However, our\nproposed IBSwin-CR can overcome this difﬁculty and still ob-\ntain good classiﬁcation performance, especially for the NWPU\ndataset.\nD. Comparison With Other Methods\nIn this section, to verify the advantage of our IBSwin-CR\nmodel, the state-of-the-art methods are shown in TablesII and\nIII. Some CNN-based methods and ViT-based methods are\nselected as the reference baselines, the details are shown as\nfollows.\nFor the AID, the CNN-based methods are VGGNet[38],\nGoogLeNet [38], ARCNet-VGG16 [39], EfﬁcientNet-B0-\naux [40], EfﬁcientNet-B3-aux [40],G B N e t[18], Inception-\nv3-CapsNet [17],A C N e t[41], KFBNet (DenseNet-121)[42],\nEFPN-DSE-TDFF [43], Xu’s method [44], and mmsCNN-\nHMM [45]. The ViT-based methods are V16_21 K [384×\n384] [11], CTNet [13], TRS [8],S C V i T[28], and consecutive\npretraining (CSPT)[46].\nTABLE II\nCOMPARISON OFCLASSIFICATION ACCURACIES (DENOTED BYOA ± STD)\nPROVIDED BYDIFFERENT METHODS USING DIFFERENT TRAINING RATIO FOR\nTHE AID\nFor the NWPU dataset, the CNN-based methods are\nVGGNet [38], GoogLeNet [38], EfﬁcientNet-B0-aux [40],\nEfﬁcientNet-B3-aux [40], Inception-v3-CapsNet[17],K F B N e t\n(DenseNet-121) [42], Contourlet CNN [47], ResNeXt-\n101+MTL [35], VGG MS2AP [48],A C N e t[41],X u ’ s\nmethod [44], and mmsCNN-HMM[45]. The ViT-based meth-\nods are V16_21 K [384× 384] [11], CTNet [13], TRS [8],\nSCViT [28], and CSPT[46].\nFrom Table II, we can see that the OA of the IBSwin-CR\nmodel is greater than those of all the other baselines. For\nthe case where training ratio is set to 20%, our proposed\nIBSwin-CR can achieve the highest accuracy (OA= 97.61%)\namong CNN-based methods, which is 2.11% higher than that\nof the second-best KFBNet. Among ViT-based methods, our\nIBSwin-CR is still the best one, with a 0.86% OA higher than\nCSPT. When the training ratio is set to 50%, the OA of our\nIBSwin-CR is 0.97% higher than the one of mmsCNN-HMM\nand 0.3% higher than TRS. Our IBSwin-CR always has the best\nperformance.\nFig. 12 shows the CM under the training ratio= 20% for\nthe AID. From it, our IBSwin-CR can generally achieve nearly\n100% OA for most categories. But there are still some categories\nwhich are hard to be classiﬁed, such as resort, school, and center.\nFor example, resorts are often misclassiﬁed as parks, while\ncenters are wrongly classiﬁed as squares. This is because in both\nthe resort and the park, the shape and layout of the buildings are\nvery similar. The reason why the center and square are easily\nconfused may be that their buildings and the environments are\nvery similar.\nHAO et al.: INDUCTIVE BIASED SWIN-TRANSFORMER WITH CYCLIC REGRESSOR 6275\nTABLE III\nCOMPARISON OFCLASSIFICATION ACCURACIES (DENOTED BYOA ± STD)\nPROVIDED BYDIFFERENT METHODS USING DIFFERENT TRAINING RATIO FOR\nTHE NWPU DATASET\nFig. 12. CM for AID under 20% training ratio, with true labels on the vertical\naxis and predicted values on the horizontal axis.\nIn Table III, we can observe that the OA of the IBSwin-\nCR model is the best. When training ratio is set to 10%, the\nbest model among CNN-based methods is mmsCNN-HMM,\nwhile our IBSwin-CR is 0.55% higher than its OA. Among the\nViT-based methods, our IBSwin-CR outperforms other models.\nSimilar observation can be seen when training ratio= 20%.\nIBSwin-CR model is 0.14% higher in accuracy than mmsCNN-\nHMM and 0.03% higher than CSPT. Fig.13 shows the CM\nunder the training ratio= 10% for the NWPU dataset. From it,\nwe can see that our IBSwin-CR model can achieve good results\nFig. 13. CM for NWPU dataset under 10% training ratio, with true labels on\nthe vertical axis and predicted values on the horizontal axis.\nTABLE IV\nCOMPUTATIONALCOST COMPARISON BETWEEN DIFFERENT METHODS FOR\nTHE NWPU DATASETUSING 10% TRAINING DATA\nfor most categories, indicating that our model also performs well\non some large datasets. However, there are still some categories\nthat are hard to tell apart, such as church, medium residential,\nand palace.\nE. Parameter Analysis of IBSwin-CR\nIn this section, we discuss the amount of parameters in\nIBSwin-CR. We choose to use the NWPU dataset in this\nsubsection. In TableIV, we report the number of parameters,\nGiga multiply accumulate operations per second (GMACs),\nthroughput, and accuracy. As shown in TableIV, ViT-B and\nSwin-B have fewer parameters, larger model throughput, but\ntheir performance is not very good. ViT-L and Swin-L have\nhigher accuracy, but also require a large number of parameters,\nsmaller model throughput, and more GMACs. The parameter\nquantity of our proposed IBSwin-CR is smaller than that of the\nSwin-L model, with a higher throughput and a higher accuracy.\nThe GMACs of the IBSwin-CR model is 14.2 G less than those\nof the Swin-L, which show that the computational cost is highly\ndecreased.\nV. DISCUSSION\nIn this article, we propose the IBSwin-CR model, which ﬁrst\nobtains local features and their global relationships in remote\nsensing images by inductive bias capability and self-attention\n6276 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nmechanism in the role of the IBSW-MSA module. Then, addi-\ntional spatial information in the image is obtained by predicting\nthe relative distances of the sampled points with the effect\nof the SSL module random dense sampler. Finally, we use a\ncombination of supervised learning loss strategy and SSL loss\nstrategy to constrain the network, i.e., by using a cross-entropy\nloss function and a cyclic regression loss function to train the\nnetwork. The proposed IBSwin-CR model is compared with the\nCNN-based methods and the ViT-based methods. The experi-\nmental results shown in SectionIV-D show that the proposed\nIBSwin-CR model is an effective strategy for remote sensing\nscene classiﬁcation.\nIn TablesII and III, the network with relatively better perfor-\nmance among the CNN-based methods is KFBNet, with OA\nof 95.50% and 93.08%, respectively. KFBNet is the use of\nthe key ﬁlter bank (KFB) to capture global information while\ncombining with CNN (obtaining local information) to improve\nthe classiﬁcation performance of the network. The emergence\nof the self-attention mechanism is a promising alternative to\nthe problem of global information capture. Therefore, many\nresearchers use CNNs in combination with the transformer\nto improve the classiﬁcation performance of the network. For\nexample, among the ViT-based methods, TRS and CTNet use\nthe aforementioned methods and achieve excellent results. The\nformer uses the CNN to process the image and feed it into\nMHSA-Bottleneck+CNN, ﬁnally a transformer is used to obtain\nthe classiﬁcation result with MLP. However, the latter uses a\ndual-stream structure of CNN and ViT streams, which inevitably\nbrings a large number of parameters and operations when the\nclassiﬁcation effect surpasses the former.\nAlso, we have discussed and analyzed SSL-based approaches,\nsuch as the CSPT model proposed by Zhang et al. [46].I t\ntakes into full consideration that remote sensing datasets are not\nsufﬁciently large and uniform and considers it unreasonable to\nﬁne-tune them directly in different downstream tasks. Therefore,\ninspired by the ﬁeld of NLP without stopping pretraining, the\nCSPT model is proposed, which can transfer large-scale knowl-\nedge to any domain.\nFor remote sensing datasets that are not sufﬁciently large,\nIBSwin-CR adopts an SSL strategy as well, using a random\ndense sampler to obtain additional spatial information by pre-\ndicting the relative distance of sampling points, and ﬁtting the\nsampling points by a circular regression loss function. For the\ncharacteristics that remote sensing datasets are different from\nnatural image datasets, we design the IBSW-MSA module,\nwhich combines the inductive biasing ability of the CNN with\nthe self-attention mechanism to process remote sensing images,\nand then, feed them into the random dense sampler. In addition,\nin contrast to CTNet and TRS, the IBSW-MSA module embeds\nthe CNN in a self-attention mechanism, which improves the\ninformation interaction capability without signiﬁcantly boosting\nthe parameters. As a result, the proposed IBSwin-CR is able to\ngenerate higher classiﬁcation accuracy than the aforementioned\nalgorithms.\nMeanwhile, since interpretability is an important evaluation\ncriterion for deep models, we visualized the class activation\nmapping (CAM). We visualized the IBSwin-CR network by\nFig. 14. Visualized CAM shows that IBSwin-CR has more powerful perfor-\nmance on remote sensing datasets. Here column (a) represents the original image,\ncolumn (b) represents the last layer of CAM of IBSwin-CR, and column (c)\nrepresents the last layer of CAM of the swin transformer.\nTABLE V\nCOMPARISON OFCLASSIFICATION RESULTS WITH 95% CONFIDENCE\nINTERV ALS IN THEAID AND NWPU DATASETS\nusing Score CAM and compared the visualization results with\nthe swin transformer to demonstrate the effectiveness of the\nproposed module. Experimental results shown in Fig.14 indicate\nthat IBSwin-CR has more powerful performance on remote\nsensing datasets, where column(a) represents the original image,\ncolumn (b) represents the CAM of the last layer of IBSwin-CR,\nand column (c) represents the CAM of the last layer of the\nswin transformer. In the airplane of(b), the traditional method\nignores the object in the image. Our method can not only\nidentify the airplane in the center of the image but also identify\npart of the wing in the lower right corner. In both bridge and\nship, the traditional method failed to completely identify the\nobject. The CAM of IBSwin-CR visualizes the effectiveness\nof the IBSW-MSA module, random dense sampler, and cyclic\nregression loss function.\nFinally, since conﬁdence intervals provide more information\nabout the accuracy of the point estimate (e.g., the mean) and the\nlevel of uncertainty around the estimate, a more comprehensive\ndescription of the uncertainty in the estimate helps the reader to\nassess the signiﬁcance of the results. We analytically compare\nmmsCNN-HMM in the CNN-based approach and CTNet in the\nViT-based approach. The comparison of the classiﬁcation results\nwith 95% conﬁdence intervals in the AID and NWPU datasets\nis shown in TableV. As shown in this table, the upper and lower\nlimits of the intervals of IBSwin-CR are higher than those of the\nother networks in the table for the training ratios of 20%, 50%\nHAO et al.: INDUCTIVE BIASED SWIN-TRANSFORMER WITH CYCLIC REGRESSOR 6277\nand 10%, 20% for both datasets, respectively, which indicates\nthat the classiﬁcation performance of IBSwin-CR is consistently\nbetter than that of the other networks of the table.\nHowever, the design of the IBSW-MSA module greatly\nlimits the network selection, so the future work focuses on\nimproving the generalizability of the IBSW-MSA module by\nintegrating the inductive bias capability with the self-attention\nmechanism or optimizing the module making it independent of\nSW-MSA. In addition, the random dense sampler learns spatial\ninformation by predicting the relative distance of the sampled\npoints. We consider that in the future, it is possible to try to\nlearn additional information in images by predicting the rotation\nangle. The cyclic regression loss function works better when\nconstraining the relative distance of sampling points, if we adjust\nthe strategy to predict the rotation angle, then the corresponding\nloss function needs to be reselected. These aforementioned will\nbe the focus of our future work.\nVI. CONCLUSION\nIn this article, we have proposed the IBSwin-CR model for\nsmall- and medium-scale datasets, which can achieve higher\ntraining accuracy and improve the discrimination ability of the\ntraditional swin transformer model. First, the local features along\nwith them global relationship in the image are obtained through\nour novel IBSW-MSA module. Second, the random dense sam-\npler module has been introduced to enable the network to obtain\nadditional spatial information in remote sensing images, and\nﬁnally, the entire network is regularized by a combination of\ncross-entropy loss and cyclic regression loss. In addition, the\nperformance of our IBSwin-CR model is better than those of\nother classiﬁcation models through the ablation study (regard-\nless of whether the model is trained from scratch or ﬁne-tuned\nfrom the pretrained parameters). In the future, we will explore\nhow to optimize other networks, and propose more convenient\nand general modules.\nREFERENCES\n[1] G. Cheng, X. Xie, J. Han, L. Guo, and G.-S. Xia, “Remote sensing\nimage scene classiﬁcation meets deep learning: Challenges, methods,\nbenchmarks, and opportunities,” IEEE J. Sel. Topics Appl. Earth Ob-\nserv. Remote Sens., vol. 13, pp. 3735–3756, Jun. 2020, doi:10.1109/JS-\nTARS.2020.3005403.\n[2] K. Clark, M. Luong, Q. V . Le, and C. D. Manning, “ELECTRA:\nPre-training text encoders as discriminators rather than generators,” in\nProc. 8th Int. Conf. Learn. Representations, Addis Ababa, Ethiopia,\nApr. 26–30, 2020. [Online]. Available: https://openreview.net/forum?id=\nr1xMH1BtvB\n[3] P. Berg, M.-T. Pham, and N. Courty, “Self-supervised learning for scene\nclassiﬁcation in remote sensing: Current state of the art and perspectives,”\nRemote Sens., vol. 14, no. 16, 2022, Art. no. 3995.\n[4] C. Szegedy et al., “Going deeper with convolutions,” inProc. IEEE Conf.\nComput. Vis. Pattern Recognit., 2015, pp. 1–9.\n[5] M. Castelluccio, G. Poggi, C. Sansone, and L. Verdoliva, “Land use\nclassiﬁcation in remote sensing images by convolutional neural networks,”\n2015, arXiv:1508.00092.\n[6] T. Kattenborn, J. Leitloff, F. Schiefer, and S. Hinz, “Review on convo-\nlutional neural networks (CNN) in vegetation remote sensing,”ISPRS J.\nPhotogrammetry Remote Sens., vol. 173, pp. 24–49, 2021.\n[7] A. Vaswani et al., “Attention is all you need,” in Proc. Adv. Neural\nInf. Process. Syst. 30: Annu. Conf. Neural Inf. Process. Syst., I. Guyon\net al., Eds. Long Beach, CA, USA, Dec. 4–9, 2017, pp. 5998–6008.\n[Online]. Available: https://proceedings.neurips.cc/paper/2017hash/3f5ee\n243547dee91fbd053c1c4a845aa-Abstract.html\n[8] J. Zhang, H. Zhao, and J. Li, “TRS: Transformers for remote sensing scene\nclassiﬁcation,” Remote Sens., vol. 13, no. 20, 2021, Art. no. 4143.\n[9] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016,\npp. 770–778.\n[10] A. Dosovitskiy et al., “An image is worth 16x16 words: Trans-\nformers for image recognition at scale,” in Proc. 9th Int. Conf.\nLearn. Representations, Austria, May 3–7, 2021. [Online]. Available:\nhttps://openreview.net/forum?id=YicbFdNTTy\n[11] Y . Bazi, L. Bashmal, M. M. A. Rahhal, R. A. Dayil, and N. A. Ajlan,\n“Vision transformers for remote sensing image classiﬁcation,”Remote\nSens., vol. 13, no. 3, 2021, Art. no. 516.\n[12] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using\nshifted windows,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2021,\npp. 10012–10022.\n[13] P. Deng, K. Xu, and H. Huang, “When CNNs meet vision transformer: A\njoint framework for remote sensing scene classiﬁcation,”IEEE Geosci.\nRemote Sens. Lett. , vol. 19, pp. 8020305-1–8020305-5, Sep. 2022,\ndoi: 10.1109/LGRS.2021.3109061.\n[14] Y . LeCun et al., “Handwritten digit recognition with a back-propagation\nnetwork,” in Adv. in Neural Inf. Process. Syst. 2, D. S. Touretzky, Ed.\nDenver, CO, USA, Nov. 27–30, 1989, pp. 396–404. [Online]. Available:\nhttp://papers.nips.cc/paper/293-handwritten-digitrecognition-with-a-back\n-propagation-network\n[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” in Proc. Adv. Neural Inf.\nProcess. Syst. 25, P. L. Bartlett, F. C. N. Pereira, C. J. C. Burges, L.\nBottou, and K. Q. Weinberger, Eds. Lake Tahoe, NV , USA, Dec. 3–6, 2012,\npp. pp.1106–1114.\n[16] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” inProc. 3rd Int. Conf. Learn. Representa-\ntions, Y . Bengio and Y . LeCun, Eds. San Diego, CA, USA, May 7–9, 2015.\n[Online]. Available: http://arxiv.org/abs/1409.1556\n[17] W. Zhang, P. Tang, and L. Zhao, “Remote sensing image scene clas-\nsiﬁcation using CNN-capsnet,” Remote Sens., vol. 11, no. 5, 2019,\nArt. no. 494.\n[18] H. Sun, S. Li, X. Zheng, and X. Lu, “Remote sensing scene classiﬁcation by\ngated bidirectional network,”IEEE Trans. Geosci. Remote Sens., vol. 58,\nno. 1, pp. 82–96, Jan. 2020.\n[19] K. Makantasis, A. Georgogiannis, A. V oulodimos, I. Georgoulas, A.\nDoulamis, and N. Doulamis, “Rank-R FNN: A tensor-based learn-\ning model for high-order data classiﬁcation,” IEEE Access,v o l .9 ,\npp. 58609–58620, 2021.\n[20] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training\nof deep bidirectional transformers for language understanding,” inProc.\nConf. North Amer. Chap. Assoc. Comput. Linguistics: Hum. Lang. Tech-\nnol., vol. 1, J. Burstein, C. Doran, and T. Solorio, Eds. Minneapolis, MN,\nUSA, Jun. 2–7, 2019, pp. 4171–4186, doi:10.18653/v1/n19-142.\n[21] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improv-\ning language understanding with unsupervised learning,” Tech. Rep.,\nOpenAI, 2018.\n[22] A. Radford et al., “Language models are unsupervised multitask learners,”\nOpenAI Blog, vol. 1, no. 8, p. 9, 2019.\n[23] T. B. Brown et al., “Language models are few-shot learners,” inProc. Adv.\nNeural Inf. Process. Syst. 33: Annu. Conf. Neural Inf. Process. Syst.,H .\nLarochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds. Dec.\n6–12, 2020, pp. 1877–1901.\n[24] H. Wu et al., “CvT: Introducing convolutions to vision transformers,” in\nProc. IEEE/CVF Int. Conf. Comput. Vis., 2021, pp. 22–31.\n[25] K. Yuan, S. Guo, Z. Liu, A. Zhou, F. Yu, and W. Wu, “Incorporating\nconvolution designs into visual transformers,” inProc. IEEE/CVF Int.\nConf. Comput. Vis., 2021, pp. 579–588.\n[26] L. Bashmal, Y . Bazi, and M. Al-Rahhal, “Deep vision transformers for\nremote sensing scene classiﬁcation,” inProc. IEEE Int. Geosci. Remote\nSens. Symp., 2021, pp. 2815–2818.\n[27] M. Kaselimi, A. V oulodimos, I. Daskalopoulos, N. Doulamis, and A.\nDoulamis, “A vision transformer model for convolution-free multilabel\nclassiﬁcation of satellite imagery in deforestation monitoring,” IEEE\nTrans. Neural Netw. Learn. Syst., vol. 34, no. 7, pp. 3299–3307, Jul. 2023,\ndoi: 10.1109/TNNLS.2022.3144791.\n[28] P. Lv, W. Wu, Y . Zhong, F. Du, and L. Zhang, “SCViT: A spatial-channel\nfeature preserving vision transformer for remote sensing image\nscene classiﬁcation,” IEEE Trans. Geosci. Remote Sens. , vol. 60,\npp. 4409512-1–4409512-12, Mar. 2022, doi:10.1109/TGRS.2022.3157671.\n[29] S. Hao, B. Wu, K. Zhao, Y . Ye, and W. Wang, “Two-stream swin\ntransformer with differentiable sobel operator for remote sensing image\nclassiﬁcation,” Remote Sens., vol. 14, no. 6, 2022, Art. no. 1507.\n6278 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\n[30] C. Doersch, A. Gupta, and A. A. Efros, “Unsupervised visual represen-\ntation learning by context prediction,” inProc. IEEE Int. Conf. Comput.\nVis., 2015, pp. 1422–1430.\n[31] Y . Liu, E. Sangineto, W. Bi, N. Sebe, B. Lepri, and M. Nadai, “Efﬁcient\ntraining of visual transformers with small datasets,” inProc. Annu. Conf.\nNeural Inf. Process. Syst., 2021, pp. 23818–23830.\n[32] R. Zhang, P. Isola, and A. A. Efros, “Colorful image colorization,” inProc.\nEur. Conf. Comput. Vis., 2016, pp. 649–666.\n[33] S. Gidaris, P. Singh, and N. Komodakis, “Unsupervised representation\nlearning by predicting image rotations,” inProc. 6th Int. Conf. Learn.\nRepresentations, Vancouver, BC, Canada, Apr. 30–May 3, 2018. [Online].\nAvailable: https://openreview.net/forum?id=S1v4N2l0\n[34] C. Tao, J. Qi, W. Lu, H. Wang, and H. Li, “Remote sensing image scene\nclassiﬁcation with self-supervised paradigm under limited labeled sam-\nples,”IEEE Geosci. Remote Sens. Lett., vol. 19, pp. 8004005-1–8004005-\n5, Dec. 2022, doi:10.1109/LGRS.2020.3038420.\n[35] Z. Zhao, Z. Luo, J. Li, C. Chen, and Y . Piao, “When self-supervised\nlearning meets scene classiﬁcation: Remote sensing scene classiﬁcation\nbased on a multitask learning framework,”Remote Sens., vol. 12, no. 20,\n2020, Art. no. 3276.\n[36] D. Dwibedi, Y . Aytar, J. Tompson, P. Sermanet, and A. Zisserman, “Tem-\nporal cycle-consistency learning,” inProc. IEEE/CVF Conf. Comput. Vis.\nPattern Recognit., 2019, pp. 1801–1810.\n[37] Z. Liu et al., “Swin transformer: Hierarchical vision transformer\nusing shifted windows,” in Proc. IEEE/CVF Int. Conf. Comput.\nVis., Montreal, QC, Canada, Oct. 10–17, 2021, pp. 9992–10 002,\ndoi: 10.1109/ICCV48922.2021.00986.\n[38] G.-S. Xia et al., “AID: A benchmark data set for performance evaluation\nof aerial scene classiﬁcation,”IEEE Trans. Geosci. Remote Sens., vol. 55,\nno. 7, pp. 3965–3981, Jul. 2017.\n[39] Q. Wang, S. Liu, J. Chanussot, and X. Li, “Scene classiﬁcation with\nrecurrent attention of VHR remote sensing images,”IEEE Trans. Geosci.\nRemote Sens., vol. 57, no. 2, pp. 1155–1167, Feb. 2019.\n[40] Y . Bazi, M. M. Al Rahhal, H. Alhichri, and N. Alajlan, “Simple yet\neffective ﬁne-tuning of deep CNNs using an auxiliary classiﬁcation loss\nfor remote sensing scene classiﬁcation,”Remote Sens., vol. 11, no. 24,\n2019, Art. no. 2908.\n[41] X. Tang, Q. Ma, X. Zhang, F. Liu, J. Ma, and L. Jiao, “Attention consistent\nnetwork for remote sensing scene classiﬁcation,” IEEE J. Sel. Topics\nAppl. Earth Observ. Remote Sens., vol. 14, pp. 2030–2045, Jan. 2021,\ndoi: 10.1109/JSTARS.2021.3051569.\n[42] F. Li, R. Feng, W. Han, and L. Wang, “High-resolution remote sensing\nimage scene classiﬁcation via key ﬁlter bank based on convolutional\nneural network,” IEEE Trans. Geosci. Remote Sens., vol. 58, no. 11,\npp. 8077–8092, Nov. 2020.\n[43] X. Wang, S. Wang, C. Ning, and H. Zhou, “Enhanced feature pyramid\nnetwork with deep semantic embedding for remote sensing scene classiﬁ-\ncation,”IEEE Trans. Geosci. Remote Sens., vol. 59, no. 9, pp. 7918–7932,\nSep. 2021.\n[44] C. Xu, G. Zhu, and J. Shu, “A lightweight and robust lie group-\nconvolutional neural networks joint representation for remote sensing\nscene classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 60, 2022,\nArt. no. 5501415, doi:10.1109/TGRS.2020.3048024.\n[45] X. Cheng and H. Lei, “Remote sensing scene image classiﬁcation based on\nmmsCNN–HMM with stacking ensemble model,”Remote Sens., vol. 14,\nno. 17, 2022, Art. no. 4423.\n[46] T. Zhang et al., “Consecutive pre-training: A knowledge transfer learning\nstrategy with relevant unlabeled data for remote sensing domain,”Remote\nSens., vol. 14, no. 22, 2022, Art. no. 5675.\n[47] M. Liu, L. Jiao, X. Liu, L. Li, F. Liu, and S. Yang, “C-CNN: Contourlet\nconvolutional neural networks,” inIEEE Trans. Neural Netw. Learn. Syst.,\nvol. 32, no. 6, pp. 2636–2649, Jun. 2020.\n[48] Q. Bi, H. Zhang, and K. Qin, “Multi-scale stacking attention pool-\ning for remote sensing scene classiﬁcation,”Neurocomputing, vol. 436,\npp. 147–161, 2021.\nSiyuan Hao (Member, IEEE) received the Ph.D. de-\ngree in information and communication engineering\nfrom the College of Information and Communications\nEngineering, Harbin Engineering University, Harbin,\nChina, in 2015.\nShe is currently an Associate Professor with the\nQingdao University of Technology, Qingdao, China,\nwhere she teaches remote sensing and electrical com-\nmunication. Her research interests include hyperspec-\ntral imagery processing and machine learning.\nNan Li received the B.S. degree in electronic in-\nformation engineering in 2021 from the Qingdao\nUniversity of Technology, Qingdao, China, where\nhe is currently working toward the M.S. degree in\ninformation and communication engineering.\nHis research interests include computer vision and\nremote sensing image processing.\nYuanxin Ye(Member, IEEE) received the B.S. degree\nin remote sensing science and technology from South-\nwest Jiaotong University, Chengdu, China, in 2008,\nand the Ph.D. degree in photogrammetry and remote\nsensing from Wuhan University, Wuhan, China, in\n2013.\nHe is currently an Associate Professor with the Fac-\nulty of Geosciences and Environmental Engineering,\nSouthwest Jiaotong University. His research interests\ninclude remote sensing image processing, registra-\ntion, change detection, and objection detection.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7404533624649048
    },
    {
      "name": "Transformer",
      "score": 0.6075019240379333
    },
    {
      "name": "Random forest",
      "score": 0.5763025879859924
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5756287574768066
    },
    {
      "name": "Entropy (arrow of time)",
      "score": 0.5350260734558105
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5329687595367432
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5205160975456238
    },
    {
      "name": "Regression",
      "score": 0.49599775671958923
    },
    {
      "name": "Inductive bias",
      "score": 0.48957833647727966
    },
    {
      "name": "Artificial neural network",
      "score": 0.47703737020492554
    },
    {
      "name": "Data mining",
      "score": 0.3389202058315277
    },
    {
      "name": "Mathematics",
      "score": 0.1517251431941986
    },
    {
      "name": "Multi-task learning",
      "score": 0.11906400322914124
    },
    {
      "name": "Statistics",
      "score": 0.08064737915992737
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I143413998",
      "name": "Qingdao University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I44468530",
      "name": "Qingdao University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4800084",
      "name": "Southwest Jiaotong University",
      "country": "CN"
    }
  ]
}