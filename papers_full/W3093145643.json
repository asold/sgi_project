{
  "title": "Audio–Visual Speech Recognition Based on Dual Cross-Modality Attentions with the Transformer Model",
  "url": "https://openalex.org/W3093145643",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2141670813",
      "name": "Yong-Hyeok Lee",
      "affiliations": [
        "Sogang University"
      ]
    },
    {
      "id": "https://openalex.org/A2125854053",
      "name": "Dong-Won Jang",
      "affiliations": [
        "Sogang University"
      ]
    },
    {
      "id": "https://openalex.org/A2306608294",
      "name": "Jae-Bin Kim",
      "affiliations": [
        "Sogang University"
      ]
    },
    {
      "id": "https://openalex.org/A3160666828",
      "name": "Rae‐Hong Park",
      "affiliations": [
        "Sogang University"
      ]
    },
    {
      "id": "https://openalex.org/A2782624347",
      "name": "Hyung-Min Park",
      "affiliations": [
        "Sogang University"
      ]
    },
    {
      "id": "https://openalex.org/A2141670813",
      "name": "Yong-Hyeok Lee",
      "affiliations": [
        "Sogang University"
      ]
    },
    {
      "id": "https://openalex.org/A2125854053",
      "name": "Dong-Won Jang",
      "affiliations": [
        "Sogang University"
      ]
    },
    {
      "id": "https://openalex.org/A2306608294",
      "name": "Jae-Bin Kim",
      "affiliations": [
        "Sogang University"
      ]
    },
    {
      "id": "https://openalex.org/A3160666828",
      "name": "Rae‐Hong Park",
      "affiliations": [
        "Sogang University"
      ]
    },
    {
      "id": "https://openalex.org/A2782624347",
      "name": "Hyung-Min Park",
      "affiliations": [
        "Sogang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2490695385",
    "https://openalex.org/W4206633762",
    "https://openalex.org/W4256112632",
    "https://openalex.org/W2164162335",
    "https://openalex.org/W2587942996",
    "https://openalex.org/W2060510034",
    "https://openalex.org/W2551572271",
    "https://openalex.org/W6683411478",
    "https://openalex.org/W2163808566",
    "https://openalex.org/W2151103935",
    "https://openalex.org/W2957285883",
    "https://openalex.org/W2076462394",
    "https://openalex.org/W2889624961",
    "https://openalex.org/W2963654155",
    "https://openalex.org/W2972756321",
    "https://openalex.org/W2901907199",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W3008191852",
    "https://openalex.org/W2963785710",
    "https://openalex.org/W6754392867",
    "https://openalex.org/W1503933356",
    "https://openalex.org/W2289925289",
    "https://openalex.org/W1736374775",
    "https://openalex.org/W2981857663",
    "https://openalex.org/W3035299099",
    "https://openalex.org/W3097385349",
    "https://openalex.org/W2981822321",
    "https://openalex.org/W3137384391",
    "https://openalex.org/W3099388488",
    "https://openalex.org/W2947476638",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2766219058",
    "https://openalex.org/W2963030892",
    "https://openalex.org/W6735927292",
    "https://openalex.org/W2963173190",
    "https://openalex.org/W7011482893",
    "https://openalex.org/W2117678320",
    "https://openalex.org/W6756040250",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W3102469722",
    "https://openalex.org/W3105120247"
  ],
  "abstract": "Since attention mechanism was introduced in neural machine translation, attention has been combined with the long short-term memory (LSTM) or replaced the LSTM in a transformer model to overcome the sequence-to-sequence (seq2seq) problems with the LSTM. In contrast to the neural machine translation, audio–visual speech recognition (AVSR) may provide improved performance by learning the correlation between audio and visual modalities. As a result that the audio has richer information than the video related to lips, AVSR is hard to train attentions with balanced modalities. In order to increase the role of visual modality to a level of audio modality by fully exploiting input information in learning attentions, we propose a dual cross-modality (DCM) attention scheme that utilizes both an audio context vector using video query and a video context vector using audio query. Furthermore, we introduce a connectionist-temporal-classification (CTC) loss in combination with our attention-based model to force monotonic alignments required in AVSR. Recognition experiments on LRS2-BBC and LRS3-TED datasets showed that the proposed model with the DCM attention scheme and the hybrid CTC/attention architecture achieved at least a relative improvement of 7.3% on average in the word error rate (WER) compared to competing methods based on the transformer model.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8104777336120605
    },
    {
      "name": "Speech recognition",
      "score": 0.7286696434020996
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5734514594078064
    },
    {
      "name": "Transformer",
      "score": 0.561067521572113
    },
    {
      "name": "Connectionism",
      "score": 0.461407333612442
    },
    {
      "name": "Word error rate",
      "score": 0.41852909326553345
    },
    {
      "name": "Artificial neural network",
      "score": 0.28552836179733276
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I148751991",
      "name": "Sogang University",
      "country": "KR"
    }
  ]
}