{
    "title": "A Multi-modal Pre-training Transformer for Universal Transfer Learning in Metal-Organic Frameworks",
    "url": "https://openalex.org/W4317393625",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5090245061",
            "name": "Yeonghun Kang",
            "affiliations": [
                "Kootenay Association for Science & Technology",
                "Korea Advanced Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5078836038",
            "name": "Hyunsoo Park",
            "affiliations": [
                "Kootenay Association for Science & Technology",
                "Korea Advanced Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5075317126",
            "name": "Berend Smit",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A5018605846",
            "name": "Jihan Kim",
            "affiliations": [
                "Kootenay Association for Science & Technology",
                "Korea Advanced Institute of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6795550632",
        "https://openalex.org/W3034739261",
        "https://openalex.org/W6779966058",
        "https://openalex.org/W6651636899",
        "https://openalex.org/W2992843173",
        "https://openalex.org/W3163360581",
        "https://openalex.org/W4233253307",
        "https://openalex.org/W2148931917",
        "https://openalex.org/W2899354085",
        "https://openalex.org/W3143460494",
        "https://openalex.org/W3048908832",
        "https://openalex.org/W3200152265",
        "https://openalex.org/W3118507387",
        "https://openalex.org/W1594869989",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6702248584",
        "https://openalex.org/W6629997333",
        "https://openalex.org/W6600336842",
        "https://openalex.org/W6600617704",
        "https://openalex.org/W6601125698",
        "https://openalex.org/W6602473000",
        "https://openalex.org/W2973153050",
        "https://openalex.org/W4213343138",
        "https://openalex.org/W2983028326",
        "https://openalex.org/W4307969679",
        "https://openalex.org/W2749580687",
        "https://openalex.org/W3046690401",
        "https://openalex.org/W3046114221",
        "https://openalex.org/W2001676859",
        "https://openalex.org/W3039136197",
        "https://openalex.org/W4238646451",
        "https://openalex.org/W2004550299",
        "https://openalex.org/W3197774601",
        "https://openalex.org/W2977837898",
        "https://openalex.org/W4233109831",
        "https://openalex.org/W2138085558",
        "https://openalex.org/W4245828386",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4285240556",
        "https://openalex.org/W1990633449",
        "https://openalex.org/W3157364410",
        "https://openalex.org/W1996369650",
        "https://openalex.org/W2053294416",
        "https://openalex.org/W4200128907",
        "https://openalex.org/W2019465613",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W4242469704",
        "https://openalex.org/W2756398738",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W4300108705",
        "https://openalex.org/W2949095042",
        "https://openalex.org/W4301676504",
        "https://openalex.org/W1552736770",
        "https://openalex.org/W3207468551",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3161898781",
        "https://openalex.org/W4307413333",
        "https://openalex.org/W2008151531",
        "https://openalex.org/W2907934912",
        "https://openalex.org/W3047876708",
        "https://openalex.org/W2766856748",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2084266203"
    ],
    "abstract": "Metal-organic frameworks (MOFs) are a class of crystalline porous materials that exhibit a vast chemical space due to their tunable molecular building blocks with diverse topologies. Given that an unlimited number of MOFs can, in principle, be synthesized, constructing structure-property relationships through a machine learning approach allows for efficient exploration of this vast chemical space, resulting in identifying optimal candidates with desired properties. In this work, we introduce MOFTransformer, a multi-model Transformer encoder pre-trained with 1 million hypothetical MOFs. This multi-modal model utilizes integrated atom-based graph and energy-grid embeddings to capture both local and global features of MOFs, respectively. By fine-tuning the pre-trained model with small datasets ranging from 5,000 to 20,000 MOFs, our model achieves state-of-the-art results for predicting across various properties including gas adsorption, diffusion, electronic properties, and even text-mined data. Beyond its universal transfer learning capabilities, MOFTransformer generates chemical insights by analyzing feature importance through attention scores within the self-attention layers. As such, this model can serve as a bedrock platform for other MOF researchers that seek to develop new machine learning models for their work.",
    "full_text": " 1\n 1 \nA Multi-modal Pre-training Transformer for 2 \nUniversal Transfer Learning in Metal-Organic 3 \nFrameworks 4 \nYeonghun Kang†,⊥, Hyunsoo Park†,⊥, Berend Smit‡, and Jihan Kim†,* 5 \n† Department of Chemical and Biomolecular Engineering, Korea Advanced Institute of Science 6 \nand Technology (KAIST), 291, Daehak-ro, Yuseong-gu, Daejeon 34141, Republic of Korea 7 \n‡ Laboratory of molecular simulation (LSMO), Institut des Sciences et Ingénierie Chimiques, 8 \nValais, Ecole Polytechnique Fédérale de Lausanne (EPFL), Rue de l’Industrie 17, CH-1951, 9 \nSion, Switzerland 10 \n⊥ These authors contributed equally to this work 11 \n*Correspondence to : jihankim@kaist.ac.kr 12 \n  13 \n 2\nABSTRACT 14 \nMetal-organic frameworks (MOFs) are a class of crystalline porous materials that exhibit a vast 15 \nchemical space due to their tunable molecular building blocks with diverse topologies. Given that 16 \nan unlimited number of MOFs can, in principle, be synthesized, constructing structure-property 17 \nrelationships through a machine le arning approach allows for effi cient exploration of this vast 18 \nchemical space, resulting in identifying optimal candidates with desired properties. In this work, 19 \nwe introduce MOFTransformer, a multi-model Transformer encoder pre-trained with 1 million 20 \nhypothetical MOFs. This multi-modal model utilizes integrated atom-based graph and energy-grid 21 \nembeddings to capture both local and global features of MOFs, respectively. By fine-tuning the 22 \npre-trained model with small datasets ranging from 5,000 to 20,000 MOFs, our model achieves 23 \nstate-of-the-art results for predicting across various properties including gas adsorption, diffusion, 24 \nelectronic properties, and even text-mined data. Beyond its universal transfer learning capabilities, 25 \nMOFTransformer generates chemical insights by analyzing feature importance through attention 26 \nscores within the self-attention layers. As such, this model can serve as a bedrock platform for 27 \nother MOF researchers that seek to develop new machine learning models for their work. 28 \n  29 \n 3\nIntroduction 30 \nMetal-organic frameworks (MOFs) are a class of crystalline porous materials used for various 31 \nenergy and environmental applications 1-4 due to their excellent properties such as large surface 32 \narea,5 high chemical/thermal stability, 6 and tunability. 7 Given that MOFs are composed of 33 \nthousands of tunable molecular building blocks (i.e., metal nodes and organic linkers), an infinite 34 \nnumber of MOFs can, in principle, be synthesize d taking into all the different combinations. To 35 \nefficiently explore this vast MOF search space, it is important to identify the structure-property 36 \nrelationship for a given application. One can then focus on MOFs that contain specific structures 37 \nthat can lead to user-desired properties. To gain information regarding this relationship, high-38 \nthroughput computational screening approaches has been primarily used by conducting 39 \nsimulations on a large dataset of MOF structures and retroactively identifying the 40 \nstructure/property relationship. 8-11 However, this can be a cumbersome process and more 41 \nimportantly, one would need to conduct independent computational screenings for each of the 42 \napplications, which requires a vast quantity of computational resources. 43 \nAn alternative way to discover the structure-property relationship is through a machine-learning 44 \n(ML) approach, and this methodology has gained a lot of traction lately. 12,13 In particular, 45 \ngeometric descriptors of MOF structures (e.g. void fraction and pore volume) have been used to 46 \naccurately predict various gas adsorption properties.14-16 Also, Bucior et al.17 developed a machine 47 \nlearning model using energy grid histograms as descriptors to predict gas uptake properties. For 48 \ndiffusion properties, Ibrahim et al. 18 developed a machine-learning model to predict N 2/O2 49 \nselectivity and diffusivity using geometric, atom-type, and chemical feature descriptors. For 50 \nelectronic properties, Rosen et al.19 demonstrated that a graph neural network facilitates capturing 51 \nthe underlying chemical features leading to accurate predictions in the band gap values for the 52 \n 4\nMOFs. Unfortunately, in all these previous studies, the developed machine-learning model cannot 53 \nbe readily transferred from one application to another. As such, one would need to restart the 54 \ntraining process and develop a new machine-learning model from scratch for every different 55 \napplication. 56 \nTo remedy this issue, one can utilize transfer learning, which incorporates knowledge from one 57 \nmachine learning application to another and, thereby, in principle, saving computational time for 58 \nsubsequent machine learning works. Although transfer learning has been applied in a few cases 59 \nfor MOFs, it is still limited to specific properties  (e.g. transfer knowledge from gas uptake to gas 60 \ndiffusivity or between different gas types), limiting their utility. 16,20 To make transferability a 61 \nfeasible solution, a universal transfer learning model that can be applied to all possible properties 62 \nneeds to be constructed. To achieve this, machine-learning models and descriptors should capture 63 \ntwo disparate types of features for MOFs: (1) local features (e.g., specific bonds and chemistry 64 \nmakeup of the building blocks) and (2) global features (e.g., geometric and topological descriptors). 65 \nAlthough both the local descriptors (e.g. CGCNN, 19,21 chemical descriptors, 18 RACs, 22,23 and 66 \nbuilding-block embedding.11,24,25) and the global features (e.g., geometric features calculated by 67 \nZEO++,26 the histograms of energy-grids.16,17) have been developed previously, as far as we know, 68 \nnone of these works have effectively captured both the local and global features to achieve 69 \nuniversal transfer learning. 70 \nWhen it comes to multi-modal learning that  takes in multiple inputs, the Transformer 71 \narchitecture27 (initially proposed for sequen ce data such as language models) has emerged as the 72 \ndominant modeling network. Given that the Transformer consists of self-attention layers, which 73 \nenables handling sequences of data in parallel, it  facilitates efficient training of neural networks 74 \nwith vast amounts of data. In 2019, Google introduced BERT, a pre-training Transformer encoder 75 \n 5\nin the language model,28 and demonstrated remarkable performance in transfer learning. By fine-76 \ntuning the pre-trained BERT model, it obtained state-of-the-art performance results for many 77 \nNatural Language Process (NLP) tasks such as question-answering and named entity recognition. 78 \nMoreover, for computer vision, various vision Transformer architectures have emerged as an 79 \nalternative solution to convol ution neural networks (CNNs). 29 Recently, the pre-trained 80 \nTransformers' transfer learning strategy has been expanded to multi-modal learning.30 And finally, 81 \nthe pre-trained multi-modal Transformers achieved state-of-the-art results in vision-language 82 \nmodels such as image captionin g and vision-question answering. 31-33 Due to its superior 83 \nperformance, the Transformer architectures have recently been adopted to predict various 84 \nproperties of MOFs.34,35 85 \nIn this work, for the first time in MOF rese arch, we introduce the multi-modal Transformer 86 \narchitecture (named “MOFTransformer”), which captures both the local and global features. Our 87 \nMOFTransformer was pre-trained with 1 million hypothetical MOFs (hMOFs). By fine-tuning the 88 \npre-trained MOFTransformer, it showcases ex cellent prediction capabilities across multiple 89 \ndifferent properties (e.g., gas uptake, gas diffusivity, electronic properties of MOFs, and text-90 \nmined data). Besides its superior performance, this architecture allows chemists to capture insights 91 \nfrom attention scores obtained by the attention layers of the MOFTransformer. As such, we believe 92 \nthat this model can serve as a bedrock architecture/model for future machine learning research for 93 \nthe MOF community. 94 \n95 \n 6\n 96 \nFigure 1.  (a) Overall schematics of MOFTransformer. The model takes both local and global 97 \nfeatures as inputs. In a pre-training step, it is trained with three pre-training tasks. In the fine-tuning 98 \nstep, the model is trained to predict desired properties of MOFs using the weights of the pre-trained 99 \nmodel as initial weights. (b) The architecture of the MOFTransformer. The input embedding takes 100 \natom-based graph embeddings and energy-grid embeddings that serve as local and global features, 101 \nrespectively.  102 \n 7\nResults 103 \nMOFTransformer 104 \nThe overall schematics of our MOFTransformer is shown in Figure 1(a). To build towards 105 \nuniversal transfer learning, both pre-training and fine-tuning strategies are implemented. The 106 \nobjective of pre-training is to allow the MOFTransformer to learn the essential characteristics of a 107 \nMOF. This pre-trained model serves as a starting point for all subsequent applications. Fine-tuning 108 \nrefers to the process of training the pre-trained models for the specific application at hand (e.g. gas 109 \nadsorption uptake prediction). Figure 1(b) shows the schematic of the MOFTransformer 110 \narchitecture, which is based on a multi-layer bidirectional Transformer encoder developed by 111 \nVaswani et al. 27 The MOFTransformer is a multi-modal Transformer that takes two types of 112 \nembedding as inputs, each representing the local and global features: (1) atom-based graph 113 \nembedding (2) energy-grid embedding. 114 \nPreviously, Xie et al. 21 devised crystal graph convolution neural networks (CGCNN) that 115 \ntransforms atoms (i.e., nodes), bonds (i.e., edges), and their features (i.e., the distance between 116 \natoms) into a vector space. Although CGCNN consists of convolutional layers and pooling layers 117 \nfrom the original paper, the atom-based graph embedding in the MOFTransformer uses output 118 \nvectors of the CGCNN without the pooling layers. It allows our model to deal with the atom-wise 119 \nfeatures without losing information. It should be noted that many atoms in the unit-cell of MOFs 120 \nhave the same embedding from the CGCNN, given that the CGCNN creates the embedding by 121 \ntaking atom types of nodes, distances, and atom types of the neighbor nodes (see Supplementary 122 \nFigure S1). We grouped the topologically identical atoms and defined these sets as unique atoms 123 \n(the details of the algorithm are explained in Supplementary Note S1). Removing the information 124 \n 8\nfrom the overlapping atoms enables efficient training and prevents significant memory issues that 125 \nfrequently appear when training with long sequences of inputs.  126 \nWhen it comes to the energy-grid embedding, the energy grids were calculated using a methane 127 \nmolecule probe that was selected due to its facility in modeling. Universal Force Field, 36 and 128 \nTraPPE37 were used to describe adsorbate-adsorbent van der Walls interactions in MOFs and the 129 \nmethane molecule, respectively. The 3D energy grids can be treated as 3D images, which means 130 \nthat the grid points and the energy values of the energy grids serve as pixels and 1-channel colors, 131 \nrespectively. Similar to the Vision Transformer,29 the MOFTransformer takes 1-dimensional (1D) 132 \npatches of the flattened 3D energy grids where (H, W, D) are the height, width, and depth of energy 133 \ngrids and (P, P, P) is the patch resolution, and N = HW D/P3 is the number of patches. Given that 134 \nthe energy grids were interpolated to 30 × 30 × 30 Å, the height H, weight W, and depth D are 30 135 \nÅ. The patch size P was set to 5 Å, so the number of patches N is 216. 136 \nThe MOFTransformer model is derived from the BERT-based model 28 (L=12, H=768, A=12), 137 \nwhere L is the number of blocks, H is the hidden size, and A is the number of self-attention heads. 138 \nSimilar to BERT’s class and separate tokens, the class token [CLS] and the separate token [SEP], 139 \nwhich are learnable embedding layers, are located at the first position and between the two types 140 \nof embedding, respectively (see Figure 1(b)). The [CLS] token is a head token of the Transformer 141 \nblocks and predicts desired properties by adding a single pooling layer for the pre-training and 142 \nfine-tuning tasks. Apart from these, a volume token [VOL], which is the normalized cell volume, 143 \nis added at the final position of the input embedding because the interpolation of the energy grids 144 \nleads to a loss of information regarding the volume  of the original energy grids. Finally, position 145 \nembedding and modal-type embedding, which are also learnable embedding layers, are added to 146 \nthe input embedding by the element-wise summati on. The position embedding is a vector that 147 \n 9\nencodes the position of the sequence, and the modal-type embedding encodes the two types of 148 \nembedding to 0 and 1. 149 \nUnderstanding MOF descriptors 150 \nIt is important to recognize how MOF descriptors (i.e., local features and global features) 151 \ninfluence the properties of MOFs. As shown in Figure 2, H 2 uptake, H2 diffusivity, and band gap 152 \nwere selected as case-study applications for MOFs that represent adsorption, diffusion, and 153 \nelectronic properties, respectively. Figure 2(a-c) shows the structure-property maps obtained from 154 \nthe molecular simulations for each of these applications. For H 2 uptake and diffusivity, the data 155 \nwas taken from our fine-tuning dataset (20,000 structures). The band gap values are obtained from 156 \nthe QMOF database (version 13) with the PBE functional that includes a total of 20,373 structures. 157 \nFrom Figure 2(a-b), it can be seen that the H 2 uptake and diffusivity increase with accessible 158 \nvolume fraction and are strongly dependent on th e MOF topology due to the correlation between 159 \ntopology and void fraction. Meanwhile, the band  gap exhibits no correlation with accessible 160 \nvolume fraction and topology, which is reasona ble given that electronic properties are more 161 \ndependent on local chemical features as opposed to global geometric features. 162 \nOn top of this, Figure 2(d-f) shows the correlation between the MOF properties and the types of 163 \nmetal atoms. It can be seen that the dependence on metal atoms is lowest for H 2 uptake while 164 \nhighest for the band gap energy. And similar trends can be found for the organic linkers (see 165 \nSupplementary Figure S2). Along with the aforementioned geometric analysis, Figure 2(d-f) 166 \nconfirms that adsorption and diffusion properties rely more on global features, while electronic 167 \nproperties rely more on local features. Apart from these, some properties like O2 diffusivity (which 168 \nis more dependent on electronic effects than H2 diffusivity) and CO2 Henry coefficient have more 169 \ncomplex correlations between features and properties (see Supplementary Figure S3). As such, 170 \n 10\nthis illustrates the importance of integrating both local and global features within the Transformer 171 \nto enable universal transferability across different applications. 172 \n  173 \n 11\n 174 \nFigure 2. (a-c) Scattered plots showing the relations hip between accessible volume fraction and 175 \nvarious properties (i.e., gas uptake, diffusivity, and bandgap). Gray dots represent the MOFs from 176 \neach database, and colored dots represent MOFs  with the top four topologies obtained from 177 \nMOFid.38 (d-f) The box plot of properties (adsorption,  diffusion, and band gap) for each metal 178 \ntype. The dark line in the center of the box represents the median.  179 \n\n 12\nPre-training Results  180 \nThe pre-training tasks play an essential role in  determining the effectiveness of the transfer 181 \nlearning performance. Three pre-training tasks were designed to capture the essential features of 182 \nthe MOFs: (1) MOF topology prediction (MTP), (2) void fraction prediction (VFP), and (3) metal 183 \ncluster/organic linker classification (MOC). For the MTP task, the model was trained to predict 184 \nthe 1,079 topologies of MOFs by adding a classification head, which consists of a single dense 185 \nlayer to the [CLS] token. The list of topologies is summarized in Supplementary Table S1. For the 186 \nVFP task, the model is trained to predict accessible void fraction calculated by ZEO++26 by adding 187 \nsingle dense layers to the [CLS] token. Finally, the MOC task was performed as it would enable 188 \nthe model to learn the features separately stemming from each metal node and organic linker. The 189 \nbinary classification (determining a given MOF atom as belonging to the metal or the organic 190 \nlinker) is conducted for the atom-wise features of atom-based embedding. The accuracies of MTP 191 \nand MOC were 0.97, 0.98 and the MAE of VFP was 0.01. 192 \nNext, we visualized the embedding vector of the pre-training model in a two-dimensional space 193 \nusing t-SNE, and PCA methods, as shown in Figure 3. Figure 3(a) shows a result of a t-SNE plot 194 \nfor the embedding vector of class tokens with the top 10 frequently appearing topologies in the 195 \ndataset. Figure 3(a) shows that MOFs with different topologies are clustered together and 196 \nsegregated from other MOFs, indicating that proper learning has occurred.  And the same pattern 197 \nof results was seen for all topologies (see Supplementary Figure S4). Furthermore, it is interesting 198 \nto note that the PCA plots exhibit the distribution of the embedding vector that gradually increases 199 \naccording to the void fraction, as shown in Figure 3(b). This indicates that the embedding vectors 200 \nare clustered with similar values of void fraction. These results demonstrate that the pre-training 201 \nmodel is successfully trained to capture the critical features of the MOFs. 202 \n 13\n203 \nFigure 3. (a) For the top 10 frequently appearing topologies, the t-SNE plot embeds the class 204 \ntokens of the pre-training model. (b) The class tokens of the pre-trained model are embedded by 205 \nthe PCA method, and a void fraction determines their colors. (c) Plots of MAE results of the fine-206 \ntuning model and three baseline models with datasets of H 2 uptake, H2 diffusivity, and band gap 207 \naccording to dataset size from 5,000 to 20,000. The baseline models are machine learning models 208 \nthat were respectively used to predict gas uptake, diffusivity, and band gap values. 209 \n  210 \n 14\nFine-tuning Results  211 \nFigure 3(c) shows the fine-tuning results for predicting H2 uptake (100 bar), H2 diffusivity, and 212 \nband gap, which were obtained from GCMC, MD, and DFT simulations, respectively. While 1 213 \nmillion hMOFs were used for the pre-training step , a relatively smaller number of MOFs (i.e., 214 \n5,000 to 20,000) were used for training during the fine-tuning stages. The performance of the fine-215 \ntuning is compared with the three baselin e models (i.e., the energy histogram, 17 descriptor-based 216 \nML model,18 and CGCNN19,21) as these have shown high performance in predicting gas uptake, 217 \ndiffusivity, and band gap, respectively. And from these comparisons, it can be seen that the 218 \nMOFTransformer outperforms all of these other models, demonstrating both its superior 219 \nperformance as well as transferable capabilities. It is worth noting that the MatErials Graph 220 \nNetwork (MEGNET) 39 outperforms the CGCNN in predicting the band gaps of MOFs 40. The 221 \nMEGNET utilizes global state attributes such as system temperature as well as atomic and bond 222 \nattributes as inputs. However, graph network models like CGCNN and MEGNET may have 223 \ndifficulty in effectively predicting properties that rely on global features such as gas uptake and 224 \ndiffusivity for MOFs. This is due to the larger crystal system of MOFs, which is characterized by 225 \na larger number of atoms and defined by metal clusters and organic linkers as nodes and edges, 226 \nrespectively. As a result, the MOFTransformer exhi bits strong performance in universal transfer 227 \nlearning for MOFs compared to graph network models. The ablation studies of the fine-tuning to 228 \ndemonstrate the effect of the data size on the pre-training tasks are explained in the Supplementary 229 \nNote S2.  230 \nTo demonstrate further transferability across different applications, the MOFTransformer was 231 \nfine-tuned for various properties summarized in Table 1. Table 1 shows a performance comparison 232 \nbetween our fine-tuned model and the machine-learning models used in other works. And it can 233 \n 15\nbe seen that the MOFTransformer model has either similar or higher performance (i.e., higher R 2 234 \nscore or lower MAE) across all properties. In particular, it is worth noting the robustness of our 235 \nmodel across different gas types, even though th e probe molecule used to generate energy grids 236 \nwas CH4. The reason is that overall shape of energy grids is relatively insensitive to the type of 237 \nprobe molecule which has little effect on our mode l to learn global features from energy-grid 238 \nembeddings. In addition, the MOFTransformer can accurately predict properties at ambient 239 \ncondition, given that N2, O2 uptake and diffusivity were calculated at 1 bar and 298 K. Moreover, 240 \nour model extends well to showcase lower MAE than the machine learning model using revised 241 \nautocorrelations (RAC)41 with geometric features as descriptors to predict solvent removal stability 242 \nand thermal stability collected by text-mining. It is worth highlighting that our model showcases 243 \nhigh performance in predicting the experimental data like text-minded data as well as the 244 \ncalculated properties. This result suggests that  one can easily obtain high-performance structure-245 \nproperties relationships by using our pre-trained model and fine-tuning it without needing to 246 \ndevelop a new model from scratch. 247 \n 248 \n 16\n 249 \nFigure 4. The schematics for attention score of atom-based embedding and energy-grid embedding 250 \nin IRMOF-1. (left) Repeating building blocks m odels in IRMOF-1 with atomic size proportional 251 \nto attention score. (right) Energy-grids that represent attention scores by color. The original form 252 \nof the IRMOF-1 is shown in the “base.” 253 \n  254 \n 17\nDiscussion 255 \nApart from the universal transfer learning, featur e importance and its interpretation can lead to 256 \na better understanding of the relationship between the MOF structures and their properties. Given 257 \nthat attention scores measure how much the model should pay attention to inputs when predicting 258 \ndesired properties, attention layers of the Transformer were assigned high attention scores to input 259 \nfeatures according to their importance. Fr om the fine-tuning models that predict H 2 uptake, H 2 260 \ndiffusivity, and band gap, feature importance analysis was implemented for IRMOF-1, which is 261 \none of the representative isoreticular MOFs. Figure 4 shows both the repeating building blocks 262 \nmodels, which represent the metal cluster and organic linker, of IRMOF-1 (representing local 263 \nfeatures) and the 6×6×6 patches of energy-grids (representing global features). The sizes of atoms 264 \nin the repeating building block models are scaled according to the attention scores obtained by the 265 \natom-based embeddings. And the colors of the patches are proportional to the attention scores 266 \nobtained from the energy-grid embeddings. As can be seen from Figure 4, the atom-based 267 \nembeddings are assigned with low attention scores (e.g. visualized by small atom sizes) when 268 \npredicting H2 uptake and diffusivity. On the other hand, the energy-grid embeddings are assigned 269 \nwith high attention scores, which is in accordance with the fact that H2 uptake and diffusivity rely 270 \nmore on the global features. Meanwhile, for the band gap prediction, there is a reversal in trend as 271 \nthe atom-based graph embeddings have higher attention scores compared to energy-grid 272 \nembeddings as the band gap is more dependent on the local features. The additional feature 273 \nimportance analysis for other properties (e.g. O2 diffusivity and CO2 Henry coefficient) were also 274 \nconducted (see Supplementary Figure S8). Note that the feature importance analysis via attention 275 \nscores is in line with previous findings and a chemist’s intuition. 276 \n 18\nBeyond the case study of IRMOF-1, we implemented an in-depth analysis of feature importance 277 \nfor the atom-based graph and the energy-grid embeddings for band gap and H2 uptake, respectively. 278 \nGiven that the band gap is defined by the difference between the conduction-band minimum (CBM) 279 \nand the valance-band maximum (VBM), one might think that the atoms that exhibit strong peaks 280 \nat the CBM and VBM play a critical role in determining its value. Interestingly, we identified that 281 \nthe atoms with peaks at the CBM and VBM strongly correlate with the atoms having high attention 282 \nscores. Figure 5(a) shows the repeating building blocks models of IRMOF-1, 2, 3, and Ni-IRMOF-283 \n1 and their density of state (DOS) plots. IRMOF-2 and IRMOF-3 are variants of the IRMOF-1 284 \nstructure with the BDC linker functionalized by −Br and −NH2. For IRMOF-2 and IMROF-3, the 285 \natoms that are part of the organic linkers (i.e., C, H, N, Br) have higher attention scores than those 286 \nfrom the metal clusters (i.e., Zn, O). Consistent with these results, the atoms of the organic linker 287 \nhave peaks at the CBM and VBM compared to those of the metal clusters. Meanwhile, for the Ni-288 \nIRMOF-1 (which has Ni instead of Zn compared to the IRMOF-1), the atoms that belong to the 289 \nmetal cluster have higher attention scores and stronger peaks at the CBM and VBM compared to 290 \nthe organic linkers. These tendencies are consistent with other examples that were randomly 291 \nselected in the QMOF database (see Supplementary Figure S9). Apart from these, we confirmed 292 \nthat the feature importance analysis could capture the underestimation of the band gap calculated 293 \nby the PBE functional (see Supplementary Note S3). Hence, these results demonstrate that the 294 \nfine-tuned model successfully learns the chemical features that are the more important when it 295 \ncomes to the band gap predictions. 296 \nWhen it comes to the energy-grid embeddings, one could argue that  the patches located near 297 \nthe metal atoms have an important ro le on determining the gas uptake 42 Indeed, from the fine-298 \ntuned model to predict H2 uptake, the 8 highest attention scores from the 6x6x6 energy-grid patches 299 \n 19\nof IRMOF-1 are located near the metal atoms as shown in Figure 5(b). The metal atoms can make 300 \nstronger bonding with adsorbates than other atoms such C, H, O, resulting in lower energy values 301 \nfor energy-grid patches near the metal atoms. Based on these observations, one can infer that the 302 \nenergy values of energy-grid patches can have an impact on determining attention score. Therefore, 303 \nwe plotted the relationship between the energy values of energy-grid points and the attention scores 304 \nfor each patch to further illustrate this rela tionship. The minimum energy values are normalized 305 \nby their corresponding structure (or unit cell), which is represented on the y-axis of Figure 5(c). 306 \nFigure 5(c) suggests that the energy-grid points with high attention scores tend to have relatively 307 \nlow energy values, as seen in the patches near the metal atoms. It is essential to highlight the fact 308 \nthat the scatter points within the high attentio n region (attention score > 0.008) exhibit a lower 309 \ndifference of energy than 20 kJ/mol. 310 \n 20\n 311 \nFigure 5. (a) Schematics of attention score for atom-based embedding, and density of state (DOS) 312 \nplots for IRMOF-1, 2, 3, and Ni-IRMOF-1. The atomic sizes of repeating building blocks model 313 \n 21\nare proportional to the attention score. E means the energy, and E f indicates the Fermi level. 314 \nPositive and negative values of DOS indicate spin-up and spin-down channels, respectively. (b) 315 \nSchematic of high attention score patches of energy-grid embedding for IRMOF-1. (c) Scattered 316 \nplot for the difference of minimum energy between patch and unit cell according to energy-grid 317 \nembedding. Ep,min refers the minimum energy of the patch, and Eu,min refers to the minimum energy 318 \nof the unit cell. The red line (x = 0.008) distinguishes between high and low attention regions. 319 \n  320 \n 22\nConclusions 321 \nFor the first time, we introduced a multi-modal pre-trained Transformer to capture both local 322 \nand global features of MOFs. The model facilita tes capturing the chemistry of metal nodes and 323 \norganic linkers from the CGCNN and the information on geometric and topological features such 324 \nas pore volume and topology from the energy grid s. By fine-tuning the MOFTransformer model, 325 \nour model outperforms all of the other state-of-the-art machine learning model across various 326 \ndifferent properties, showing its universal transf erability as well as superior performance.  327 \nMoreover, the model can provide insights by analyzing the feature importance from attention 328 \nscores obtained from attention layers of the fine-tuned model. We believe that this model can be 329 \nused as a bedrock model for other MOF researchers who wish to start their machine learning work 330 \nand, as such, can help accelerate materials discovery and research within the field of porous 331 \nmaterials. 332 \n  333 \n 23\nMethods 334 \nConstruction of hMOFs 335 \nThe hMOFs used to train our MOFTransformer were constructed using PORMAKE,11 a Python 336 \nlibrary that can generate MOFs by combining building blocks with different topologies. These 337 \nbuilding blocks and the topologies were obtained from ToBaCCo, 43 CoREMOF (with all of the 338 \nsolvents removed), 44 and RCSR database. 45 Altogether, 1 million and 20,000 hMOFs were 339 \ngenerated for the pre-training, and fine-tuning dataset, respectively, and the details of building 340 \nhMOFs are explained in Supplementary Note S4. All of the generated structures were 341 \ngeometrically optimized using the LAMMPS46 package with the UFF force field.36 342 \nComputational details for molecular simulation  343 \nFor the fine-tuning dataset, H2 uptake and diffusivity (or diffusion coefficient) were selected to 344 \nrepresent adsorption and diffusive properties. H 2 was selected to enable facile calculation while 345 \nbeing different from the guest molecule (i.e., methane) used for the energy grid construction. The 346 \ncalculations were conducted using the RASPA package. 47 For the H 2 molecule, a united atom 347 \nmodel was adopted. Also, the pseudo-Feynmna-Hibbs model was used to express the H2 behavior 348 \nat low temperature, which leads to fitting the Lenard-Jones (LJ) potentials to Feynman-Hibbs 349 \npotential at T = 77 K. 48,49 Except for the H 2 molecules, the UFF force field was used with the 350 \nLorentz-Berthelot mixing rule and a cutoff distance of 12.8 Å.  351 \nFor H2 uptake calculation, the GCMC calculation was performed at 100 bar and 77 K for 10k 352 \nproduction cycles with 5k cycles used for the in itialization. Diffusivity (or diffusion coefficient) 353 \nwas calculated at infinite dilution at 77 K using the MD simulation. Given that the intermolecular 354 \ninteractions of the H2 atoms are ignored for the infinite dilution simulation, it may sometimes lead 355 \nto the initial configurations of the H2 atoms captured within the small pores of MOFs. The initial 356 \n 24\nconfigurations were obtained from the MC simulation without infinite dilution for 5k cycles to 357 \nprevent this from happening. Then, the MD simulations were conducted by NVT ensemble with 1 358 \nfs time step. 18,50 The simulations were run for 3 million cycles, with 1k cycles used for the 359 \ninitialization and 10k cycles for equilibration. The guest molecules' mean-squared displacement 360 \n(MSD) was computed every 10k cycles, and the diffusion coefficient was obtained using the slope 361 \nof the MSD through Einstein’s relation.51 362 \nPre-training and Fine-tuning 363 \nIn the pre-training step, AdamW 52 optimizer with a learning rate of 10 −4 and weight decay of 364 \n10−2 was used in all three tasks. The model was trained with a batch size of 1,024 during 100 365 \nepochs. The pre-training dataset was randomly split into training, validation, test sets with the 366 \nnumber of 800,000, 126,611, 100,000, respectively. The learning rate was warmed up during the 367 \nfirst 5 % of the total epoch and then was linearly decayed to zero for the remaining epochs.  368 \n For fine-tuning, the MOFTransformer is trained to predict the desired properties with the model 369 \ninitialized by the converged weights from the pre-trained model. By adding a single dense layer to 370 \nthe class token, all model weights are fine-tuned to predict desired properties of MOFs. Given that 371 \nthe relatively small datasets are used during the fine-tuning step, the model was trained with a 372 \nbatch size of 32 during 20 epochs whose optimizer and learning rates are the same as those of the 373 \npre-training step. The fine-tuning dataset was randomly split into training, validation, test sets in a 374 \nratio of 0.8:0.1:0.1. For scaling the target properties, the standardization method was adopted. 375 \nTraining details of the three baseline models fo r comparison of the fine-tuning models are 376 \nexplained in Supplementary Note S5. 377 \n  378 \n 25\nConflicts of interest 379 \nThere are no conflicts to declare. 380 \nAuthor Contributions 381 \nY.K and H.P contributed equally to this work. Y.K and H.P developed MOFTransformer and 382 \nwrote the manuscript with J.K. The manuscript was written through the contributions of all authors. 383 \nAll authors have given approval for the final version of the manuscript. 384 \nData availability 385 \nData used in this work are available via Figshare (10.6084/m9.figshare.21155506). It provides 386 \nthe pre-trained model and the atom-based graph embeddings and the energy-grid embeddings used 387 \nas inputs of the MOFTransformer for CoREMOF, QMOF database .as well as fine-tuning data. In 388 \naddition, The UFF-optimized CIF files of hypotheti cal MOFs used in this work are available via 389 \nFigshare (10.6084/m9.figshare.21810147) 390 \nCode availability 391 \nThe MOFTransformer library is available at https://github.com/hspark1212/MOFTransformer. 392 \nDocuments for the library is available at ht tps://hspark1212.github.io/MOFTransformer which 393 \nprovides up-to-date documentation for pre-training, fine-tuning, and feature importance analysis 394 \nwith the MOFTransformer. For the sake of reprod ucibility, all results in this manuscript are 395 \nobtained from a 1.0.1 version of MOFTransformer library, which is available at 396 \nhttps://pypi.org/project/moftransformer/1.0.1. 397 \nAcknowledgements  398 \n 26\nH. P., Y. K., and J. K. acknowledge funding from National Research Foundation of Korea (NRF) 399 \nunder Project Number 2021M3A7C208974513. This work was supported by the National 400 \nSupercomputing Center with supercomputing re sources including technical support (KSC-2021-401 \nCRE-0460). BS is supported by the PrISMa Project, which is funded through the ACT programme 402 \n(Accelerating CCS Technologies, Horizon2020 Project No 294766). Financial contributions made 403 \nfrom: BEIS together with extra funding from NERC and EPSRC, UK; RCN, Norway; SFOE, 404 \nSwitzerland and US-DOE, USA, are gratefully acknowledged. Additional financial support from 405 \nTOTAL and Equinor, is also gratefully acknowledged. 406 \n  407 \n 27\nReferences 408 \n1 Freund, R.  et al. The current status of MOF and COF applications. Angewandte Chemie 409 \nInternational Edition 60, 23975-24001 (2021). 410 \n2 Kumar, S.  et al. Green synthesis of metal–organic frameworks: A state-of-the-art review 411 \nof potential environmental and medical applications. Coordination Chemistry Reviews 412 \n420, 213407 (2020). 413 \n3 Qian, Q.  et al. MOF-based membranes for gas separations. Chemical reviews 120, 8161-414 \n8266 (2020). 415 \n4 Lee, J.  et al. Metal–organic framework materials as catalysts. Chemical Society Reviews 416 \n38, 1450-1459 (2009). 417 \n5 Deng, H.  et al. Large-pore apertures in a series of metal-organic frameworks. science 418 \n336, 1018-1023 (2012). 419 \n6 Ding, M., Cai, X. & Jiang, H.-L. Improving MOF stability: approaches and applications. 420 \nChemical Science 10, 10209-10230 (2019). 421 \n7 Wang, C., Liu, D. & Lin, W. Metal–organic frameworks as a tunable platform for 422 \ndesigning functional molecular materials. Journal of the American Chemical Society 135, 423 \n13222-13234 (2013). 424 \n8 Colón, Y. J. & Snurr, R. Q. High-throughput computational screening of metal–organic 425 \nframeworks. Chemical Society Reviews 43, 5735-5749 (2014). 426 \n9 Boyd, P. G.  et al. Data-driven design of metal–organic frameworks for wet flue gas CO2 427 \ncapture. Nature 576, 253-256 (2019). 428 \n10 Daglar, H. & Keskin, S. Recent advanc es, opportunities, and challenges in high-429 \nthroughput computational screening of MOFs for gas separations. Coordination 430 \nChemistry Reviews 422, 213470 (2020). 431 \n11 Lee, S.  et al. Computational screening of trillions of metal–organic frameworks for high-432 \nperformance methane storage. ACS Applied Materials & Interfaces 13, 23647-23654 433 \n(2021). 434 \n12 Altintas, C., Altundal, O. F., Keskin, S. & Yildirim, R. Machine learning meets with 435 \nmetal organic frameworks for gas storage and separation. Journal of Chemical 436 \nInformation and Modeling 61, 2131-2146 (2021). 437 \n13 Chong, S., Lee, S., Kim, B. & Kim, J. Applications of machine learning in metal-organic 438 \nframeworks. Coordination Chemistry Reviews 423, 213487 (2020). 439 \n14 Ahmed, A. & Siegel, D. J. Predicting hydrogen storage in MOFs via machine learning. 440 \nPatterns 2, 100291 (2021). 441 \n15 Simon, C. M.  et al. The materials genome in action: identifying the performance limits 442 \nfor methane storage. Energy & Environmental Science 8, 1190-1199 (2015). 443 \n16 Lim, Y. & Kim, J. Application of transfer learning to predict diffusion properties in 444 \nmetal–organic frameworks. Molecular Systems Design & Engineering (2022). 445 \n17 Bucior, B. J.  et al. Energy-based descriptors to rapidly predict hydrogen storage in metal–446 \norganic frameworks. Molecular Systems Design & Engineering 4, 162-174 (2019). 447 \n18 Orhan, I. B., Daglar, H., Keskin, S., Le, T. C. & Babarao, R. Prediction of O2/N2 448 \nSelectivity in Metal–Organic Frameworks via High-Throughput Computational 449 \nScreening and Machine Learning. ACS Applied Materials & Interfaces 14, 736-749 450 \n(2021). 451 \n 28\n19 Rosen, A. S.  et al. Machine learning the quantum-chemical properties of metal–organic 452 \nframeworks for accelerated materials discovery. Matter 4, 1578-1597 (2021). 453 \n20 Ma, R., Colon, Y. J. & Luo, T. Transfer learning study of gas adsorption in metal–454 \norganic frameworks. ACS applied materials & interfaces 12, 34041-34048 (2020). 455 \n21 Xie, T. & Grossman, J. C. Crystal graph convolutional neural networks for an accurate 456 \nand interpretable prediction of material properties. Physical review letters 120, 145301 457 \n(2018). 458 \n22 Moosavi, S. M.  et al. Understanding the diversity of the metal-organic framework 459 \necosystem. Nature communications 11, 1-10 (2020). 460 \n23 Nandy, A.  et al. MOFSimplify, machine learning models with extracted stability data of 461 \nthree thousand metal–organic frameworks. Scientific Data 9, 1-11 (2022). 462 \n24 Yao, Z.  et al. Inverse design of nanoporous crystalline reticular materials with deep 463 \ngenerative models. Nature Machine Intelligence 3, 76-86 (2021). 464 \n25 Lim, Y., Park, J., Lee, S. & Kim, J. Finely tuned inverse design of metal–organic 465 \nframeworks with user-desired Xe/Kr selectivity. Journal of Materials Chemistry A 9, 466 \n21175-21183 (2021). 467 \n26 Willems, T. F., Rycroft, C. H., Kazi, M., Meza, J. C. & Haranczyk, M. Algorithms and 468 \ntools for high-throughput geometry-based analysis of crystalline porous materials. 469 \nMicroporous and Mesoporous Materials 149, 134-141 (2012). 470 \n27 Vaswani, A.  et al. Attention is all you need. Advances in neural information processing 471 \nsystems 30 (2017). 472 \n28 Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. Bert: Pre-training of deep 473 \nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 474 \n(2018). 475 \n29 Dosovitskiy, A.  et al. An image is worth 16x16 words: Transformers for image 476 \nrecognition at scale. arXiv preprint arXiv:2010.11929 (2020). 477 \n30 Hu, R. & Singh, A. in Proceedings of the IEEE/CVF International Conference on 478 \nComputer Vision.  1439-1449. 479 \n31 Zhou, L.  et al. in Proceedings of the AAAI Conference on Artificial Intelligence.  13041-480 \n13049. 481 \n32 Li, L. H., Yatskar, M., Yin, D., Hsieh, C.-J. & Chang, K.-W. Visualbert: A simple and 482 \nperformant baseline for vision and language. arXiv preprint arXiv:1908.03557 (2019). 483 \n33 Kim, W., Son, B. & Kim, I. in International Conference on Machine Learning.  5583-484 \n5594 (PMLR). 485 \n34 Cao, Z., Magar, R., Wang, Y. & Farimani, A. B. MOFormer: Self-Supervised 486 \nTransformer model for Metal-Organic Framework Property Prediction. arXiv preprint 487 \narXiv:2210.14188 (2022). 488 \n35 Chen, P., Jiao, R., Liu, J., Liu, Y. & Lu, Y. Interpretable Graph Transformer Network for 489 \nPredicting Adsorption Isotherms of Metal–Organic Frameworks. Journal of Chemical 490 \nInformation and Modeling 62, 5446-5456 (2022). 491 \n36 Rappé, A. K., Casewit, C. J., Colwell, K., G oddard III, W. A. & Skiff, W. M. UFF, a full 492 \nperiodic table force field for molecular mechanics and molecular dynamics simulations. 493 \nJournal of the American chemical society 114, 10024-10035 (1992). 494 \n37 Martin, M. G. & Siepmann, J. I. Transferable potentials for phase equilibria. 1. United-495 \natom description of n-alkanes. The Journal of Physical Chemistry B 102, 2569-2577 496 \n(1998). 497 \n 29\n38 Bucior, B. J.  et al. Identification schemes for metal–organic frameworks to enable rapid 498 \nsearch and cheminformatics analysis. Crystal Growth & Design 19, 6682-6697 (2019). 499 \n39 Chen, C., Ye, W., Zuo, Y., Zheng, C. & Ong, S. P. Graph networks as a universal 500 \nmachine learning framework for molecules and crystals. Chemistry of Materials 31, 501 \n3564-3572 (2019). 502 \n40 Nandy, A., Duan, C. & Kulik, H. J. Using Machine Learning and Data Mining to 503 \nLeverage Community Knowledge for the Engineering of Stable Metal–Organic 504 \nFrameworks. Journal of the American Chemical Society 143, 17535-17547 (2021). 505 \n41 Janet, J. P. & Kulik, H. J. Resolving transition metal chemical space: Feature selection 506 \nfor machine learning and structure–property relationships. The Journal of Physical 507 \nChemistry A 121, 8939-8954 (2017). 508 \n42 Koizumi, K., Nobusada, K. & Boero, M. Hydrogen storage mechanism and diffusion in 509 \nmetal–organic frameworks. Physical Chemistry Chemical Physics 21, 7756-7764 (2019). 510 \n43 Colón, Y. J., Gomez-Gualdron, D. A. & Snurr, R. Q. Topologically guided, automated 511 \nconstruction of metal–organic frameworks and their evaluation for energy-related 512 \napplications. Crystal Growth & Design 17, 5801-5810 (2017). 513 \n44 Chung, Y. G.  et al. Advances, updates, and analytics for the computation-ready, 514 \nexperimental metal–organic framework database: CoRE MOF 2019. Journal of Chemical 515 \n& Engineering Data 64, 5985-5998 (2019). 516 \n45 O’Keeffe, M., Peskov, M. A., Ramsden, S. J. & Yaghi, O. M. The reticular chemistry 517 \nstructure resource (RCSR) database of, and symbols for, crystal nets. Accounts of 518 \nchemical research 41, 1782-1789 (2008). 519 \n46 Plimpton, S. Fast parallel algorith ms for short-range molecular dynamics. Journal of 520 \ncomputational physics 117, 1-19 (1995). 521 \n47 Dubbeldam, D., Calero, S., Ellis, D. E. & Snurr, R. Q. RASPA: molecular simulation 522 \nsoftware for adsorption and diffusion in flexible nanoporous materials. Molecular 523 \nSimulation 42, 81-101 (2016). 524 \n48 Feynman, R. P., Hibbs, A. R. & Styer, D. F. Quantum mechanics and path integrals.  525 \n(Courier Corporation, 2010). 526 \n49 Fischer, M., Hoffmann, F. & Fröba, M. Pr eferred hydrogen adsorption sites in various 527 \nMOFs—a comparative computational study. ChemPhysChem 10, 2647-2657 (2009). 528 \n50 Daglar, H., Erucar, I. & Keskin, S. Exploring the performance limits of MOF/polymer 529 \nMMMs for O2/N2 separation using computational screening. Journal of Membrane 530 \nScience 618, 118555 (2021). 531 \n51 Ewald, P. P. Die Berechnung optisch er und elektrostatischer Gitterpotentiale. Annalen 532 \nder physik 369, 253-287 (1921). 533 \n52 Loshchilov, I. & Hutter, F. Decoupled weight decay regularization. arXiv preprint 534 \narXiv:1711.05101 (2017). 535 \n 536 \n  537 \n 30\nTable 1. A table of fine-tuning results with the publicly accessible databases of MOFs that include 538 \nthe properties calculated by GCMC, MD, and even text-mining data. The results of the machine 539 \nlearning models used in the paper on the databases are summarized to compare the performance. 540 \nProperty MOFTransformer Original paper \nNumber \nof data \nRemarks Ref \nN2 uptake R2 : 0.78 R2 : 0.71 5,286 CoREMOF 18 \nO2 uptake R2 : 0.83 R2 : 0.74 5,286 CoREMOF 18 \nN2 diffusivity R2 : 0.77 R2 : 0.76 5,286 CoREMOF 18 \nO2 diffusivity R2 : 0.78 R2 : 0.74 5,286 CoREMOF 18 \nCO2 henry coefficient MAE : 0.30 MAE : 0.42 8,183 CoREMOF 22 \nSolvent removal  \nstability classification \nACC : 0.76 ACC : 0.76 2,148 Text-mining \ndata \n40 \nThermal  \nstability regression \nR2 : 0.44 \n(MAE : 45°C) \nR2 : 0.46 \n(MAE : 44°C) \n3,098 Text-mining \ndata \n40 \n 541 "
}