{
    "title": "DT-Net: Joint Dual-Input Transformer and CNN for Retinal Vessel Segmentation",
    "url": "https://openalex.org/W4387435931",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5113144423",
            "name": "Wenran Jia",
            "affiliations": [
                "Shijiazhuang Tiedao University"
            ]
        },
        {
            "id": "https://openalex.org/A5041368982",
            "name": "Simin Ma",
            "affiliations": [
                "Shijiazhuang Tiedao University"
            ]
        },
        {
            "id": "https://openalex.org/A5101869541",
            "name": "Peng Geng",
            "affiliations": [
                "Shijiazhuang Tiedao University"
            ]
        },
        {
            "id": "https://openalex.org/A5100740929",
            "name": "Yan Sun",
            "affiliations": [
                "Zhangjiakou Academy of Agricultural Sciences"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6798888984",
        "https://openalex.org/W3127722248",
        "https://openalex.org/W6790556335",
        "https://openalex.org/W2809924268",
        "https://openalex.org/W6788020346",
        "https://openalex.org/W4286491042",
        "https://openalex.org/W4220738217",
        "https://openalex.org/W4304585001",
        "https://openalex.org/W4288040442",
        "https://openalex.org/W6848533490",
        "https://openalex.org/W6776948885",
        "https://openalex.org/W3035665735",
        "https://openalex.org/W4313407500",
        "https://openalex.org/W4309338752",
        "https://openalex.org/W6768973508",
        "https://openalex.org/W6767184660",
        "https://openalex.org/W6787034427",
        "https://openalex.org/W6639824700",
        "https://openalex.org/W6640054144",
        "https://openalex.org/W6761332760",
        "https://openalex.org/W4285093832",
        "https://openalex.org/W6806425126",
        "https://openalex.org/W4226373761",
        "https://openalex.org/W6788488406",
        "https://openalex.org/W6785680109",
        "https://openalex.org/W6767405928",
        "https://openalex.org/W2996290406",
        "https://openalex.org/W6771796109",
        "https://openalex.org/W2898910301",
        "https://openalex.org/W6736170873",
        "https://openalex.org/W6781832940",
        "https://openalex.org/W6756800942",
        "https://openalex.org/W3127751679",
        "https://openalex.org/W6795435739",
        "https://openalex.org/W6792155083",
        "https://openalex.org/W6805164356",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W3083569822",
        "https://openalex.org/W6762205418",
        "https://openalex.org/W6682401072",
        "https://openalex.org/W6606705549",
        "https://openalex.org/W2072130234",
        "https://openalex.org/W3025177399",
        "https://openalex.org/W6796825131",
        "https://openalex.org/W2058333183",
        "https://openalex.org/W6802101518",
        "https://openalex.org/W3178963004",
        "https://openalex.org/W4223582100",
        "https://openalex.org/W4200617893",
        "https://openalex.org/W4289779066",
        "https://openalex.org/W6852591437",
        "https://openalex.org/W2981413347",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W3045188463",
        "https://openalex.org/W3101507774",
        "https://openalex.org/W3169847394",
        "https://openalex.org/W4200015473",
        "https://openalex.org/W2970005570",
        "https://openalex.org/W2966926453",
        "https://openalex.org/W4313588240",
        "https://openalex.org/W3188260903",
        "https://openalex.org/W2979375128",
        "https://openalex.org/W2150769593",
        "https://openalex.org/W3204685218",
        "https://openalex.org/W2971013993",
        "https://openalex.org/W3112701542",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W4366124349",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3112712298",
        "https://openalex.org/W3013766724",
        "https://openalex.org/W3128890743",
        "https://openalex.org/W2601564443",
        "https://openalex.org/W2927980542",
        "https://openalex.org/W2145305441",
        "https://openalex.org/W3121038455",
        "https://openalex.org/W4321232185",
        "https://openalex.org/W4206179752",
        "https://openalex.org/W2395611524",
        "https://openalex.org/W3082160919"
    ],
    "abstract": "Retinal vessel segmentation in fundus images plays an essential role in the screening, diagnosis, and treatment of many diseases. The acquired fundus images generally have the following problems: uneven illumination, high noise, and complex structure. It makes vessel segmentation very challenging. Previous methods of retinal vascular segmentation mainly use convolutional neural networks on U Network (U-Net) models, and they have many limitations and shortcomings, such as the loss of microvascular details at the end of the vessels. We address the limitations of convolution by introducing the transformer into retinal vessel segmentation. Therefore, we propose a hybrid method for retinal vessel segmentation based on modulated deformable convolution and the transformer, named DT-Net. Firstly, multi-scale image features are extracted by deformable convolution and multi-head self-attention (MHSA). Secondly, image information is recovered, and vessel morphology is refined by the proposed transformer decoder block. Finally, the local prediction results are obtained by the side output layer. The accuracy of the vessel segmentation is improved by the hybrid loss function. Experimental results show that our method obtains good segmentation performance on Specificity (SP), Sensitivity (SE), Accuracy (ACC), Curve (AUC), and F1-score on three publicly available fundus datasets such as DRIVE, STARE, and CHASE_DB1.",
    "full_text": "This work is licensed under a Creative Commons Attribution 4.0 International License,\nwhich permits unrestricted use, distribution, and reproduction in any medium, provided the\noriginal work is properly cited.\nechT PressScience\nDOI:10.32604/cmc.2023.040091\nARTICLE\nDT-Net: Joint Dual-Input Transformer and CNN for Retinal Vessel\nSegmentation\nWenran Jia1, Simin Ma1, Peng Geng1 and Yan Sun2,*\n1SchoolofInformationScienceandTechnology,ShijiazhuangTiedaoUniversity,Shijiazhuang,China\n2SchoolofMathematicsandInformationScience,ZhangjiakouUniversity,Zhangjiakou,China\n*CorrespondingAuthor:YanSun.Email:sunyan7322@163.com\nReceived:04March2023 Accepted:28June2023 Published:08October2023\nABSTRACT\nRetinal vessel segmentation in fundus images plays an essential role in the screening, diagnosis, and treatment\nof many diseases. The acquired fundus images generally have the following problems: uneven illumination, high\nnoise,andcomplexstructure.Itmakesvesselsegmentationverychallenging.Previousmethodsofretinalvascular\nsegmentation mainly use convolutional neural networks on U Network (U-Net) models, and they have many\nlimitations and shortcomings, such as the loss of microvascular details at the end of the vessels. We address the\nlimitationsofconvolutionbyintroducingthetransformerintoretinalvesselsegmentation.Therefore,wepropose\nahybridmethodforretinalvesselsegmentationbasedonmodulateddeformableconvolutionandthetransformer,\nnamed DT-Net. Firstly, multi-scale image features are extracted by deformable convolution and multi-head self-\nattention (MHSA). Secondly, image information is recovered, and vessel morphology is refined by the proposed\ntransformerdecoderblock.Finally,thelocalpredictionresultsareobtainedby thesideoutputlayer.Theaccuracy\nof the vessel segmentation is improved by the hybrid loss function. Experimental results show that our method\nobtainsgoodsegmentationperformanceonSpecificity(SP),Sensitivity(SE),Accuracy(ACC),Curve(AUC),and\nF1-scoreonthreepubliclyavailablefundusdatasetssuchasDRIVE,STARE,andCHASE_DB1.\nKEYWORDS\nRetinalvesselsegmentation;deformableconvolution;multi-scale;transformer;hybridlossfunction\n1 Introduction\nThe vessels in fundus images are currently the only microvascular system that can be directly\nvisualized non-invasively and painlessly. The pathological characteristics of related diseases can be\nobserved by the morphology and changing information of retinal vessels. For example, diabetic\npatients are prone to retinopathy, macular degeneration, and blindness [1–3]. The retinal vessels of\nhypertensive patients have higher curvature and narrowing, which can easily lead to retinal hemorrhage\n[4]. Therefore, visualizing the distribution and details of retinal vessels can help doctors diagnose\ndiseases more efficiently [5]. However, retinal vessels have the following problems: complex and diverse\nstructures, tiny vessels, low contrast, and easy confusion with the background. A significant amount\n3394 CMC, 2023, vol.76, no.3\nof time and effort is required to segment the vessels manually. Therefore, an automatic retinal vessel\nsegmentation method is essential to assist doctors in diagnosing diseases quickly.\nArtificial intelligence technology shortens the distance between human life and computer, and the\nmethod based on deep learning is applied to various tasks. For example, Sultan et al. used deep learning\nto deal with the segmentation task of high-resolution aerial images [6,7]. Liu et al. [8] and Qin et al. [9]\napplied deep learning to image fusion. Jin et al. [10] applied deep learning to classification tasks to\nprovide accurate ERM automatic grading for clinical practice. Deep learning is also widely used in\nthe task of medical image segmentation [11,12] and other fields [13,14]. Among them, convolutional\nneural networks (CNN) have made great progress in location-sensitive tasks [15–17]. In recent years,\nU-Net [18] based on the full convolution network (FCN) [19] has been widely used in medical image\nsegmentation [20–24]. However, U-Net is difficult to deal with irregular and tiny vessels. M-Net [25]i s\nan improved U-Net framework that uses the image pyramid mechanism to realize multi-level receptive\nfields and can learn image features at different scales. However, the feature filtering is not realized in\nthe hopping connection in the M-Net model. ResU-Net [26] is derived from the U-Net architecture.\nIt uses residual blocks to replace convolutional layers and increases the depth of the model to get\nmore vessel features. But the contrast-limited adaptive histogram equalization (CLAHE) operation\nincreases the noise of the image. UNet++ [27] redesigns the skip connection part of U-Net, which\nis the aggregation of features of different semantic scales in the decoder. But it consumes too much\nmemory and takes a lot of time on small datasets. IterNet [28] is an encoder-decoder model like U-Net,\nwhich adopts U-Net as the basic module to improve the connectivity of vessel segmentation results by\nexpanding the depth of the model through multiple iterations.\nBased on U-Net, Deformable U-Net (DUNet) [29] adds deformable convolution [30] to adaptively\nadjust the receptive field according to the size and shape of vessels and improve segmentation accuracy\nand noise immunity. MAU-Net [31] uses modulated deformable convolution [32] as an encoding\nand decoding unit and uses position and channel attention block to realize vascular segmentation.\nRecently, the transformer has been successfully applied to the field of computer vision. Inspired by\nthis, TransUNet uses a hybrid of CNN and transformer as an encoder and uses a skip connection and\ndecoder for medical image segmentation [33]. The encoder, bottleneck, and decoder of Swin-Unet [34]\nuse the Swin-transformer block [35] to realize medical image segmentation. FAT-Net [36] implements\na dual encoder, including both CNN and transformer branches, to achieve skin lesion segmentation.\nAlthough it can get better performance, these models based on the transformer are both complicated\nand time-consuming, which will influence the practicability to some extent.\nThese segmentation methods have the following problems: (1) The method used can only extract\nthe local information in the image and can not deal with the global features. (2) The accuracy of\nsegmentation is low. (3) The structural information in the vascular image can not be obtained well.\nGiven the above problems, we use deformable convolution to extract complex and variable structural\ninformation, which has better learning ability than ordinary convolution. In addition, we use the\ntransformer to capture long-term dependencies through a self-attention mechanism and help CNN\novercome its inherent spatially induced biases [37]. Therefore, a segmentation network based on a\ncombination of deformable convolution [32] and transformer is proposed to solve the challenging task\nof retinal vessel segmentation. The proposed network uses convolution to extract local features and\nthe transformer to construct long-term dependencies. And pre-training on large-scale datasets is not\nrequired and achieves better results on small-scale datasets. Our main contributions are summarized\nas follows:\nCMC, 2023, vol.76, no.3 3395\n(1) We propose an end-to-end deep learning network named DT-Net, which is very effective for\nretinal vessel segmentation. The network takes into account multi-scale input, structural information,\nand long-term dependency, and provides more powerful technical support for clinical diagnosis and\nprocessing.\n(2) Combine deformable convolution with transformer. Deformable convolution can extract\nstructural information in retinal vessels. Transformer makes up for the defect that CNN can only\nobtain local information and enhances the extraction ability of feature information to achieve a better\nsegmentation effect.\n(3) A dual-input MHSA algorithm was proposed to extract multi-scale image information of\nfundus vascular images with different resolutions. The output of multi-scale image information is fused\nby skip connection to compensate for the information loss in feature extraction. A mixed loss function\nwas used to improve the accuracy of retinal vessel segmentation.\n(4) We conducted experiments on DRIVE, STARE, and CHASE_DB1 with accuracy rates of\n96.31%, 97.03%, and 97.37%, respectively. The experimental results showed that our segmentation\nperformance was superior to other methods.\nThe remainder of this paper is organized as follows:Section 2describes our proposed approach in\ndetail. Section 3presents the fundus dataset, preprocessing methods, and experimental results. Finally,\nwe conclude with a summary and outlook inSection 4.\n2 Method\n2.1 Network Architecture\nThe architecture of the proposed DT-Net is shown inFig. 1. It consists of four main parts: encoder,\ndecoder, multi-scale input and side output. We improve on U-Net, one of the simplest and most\npopular architectures in medical image segmentation. Firstly, because the information obtained by the\nsingle-scale input of U-Net is limited, we use multi-scale layers to construct image pyramid input, and\naverage pooling is used on the retinal images of sizeH×W to obtain the multi-scale image information,\nenhancing each layer’s feature information. Secondly, a hybrid block is used in the encoder to extract\nvessel features with irregular shapes and sizes. The encoders are connected through max pooling, which\nhalves the size of the feature map and generates hierarchical features at different levels. Except for\nthe first layer, each layer is inputted by the maximum pool map of the upper layer and the feature\nmap of this layer. The high-level features can correctly identify the coarse vessel information, and the\nlow-level features can accurately obtain the tiny vessel information. Among them, the hybrid block\ncombines modulated deformable convolution and MHSA. The segmentation effect is improved by\nusing deformable convolution for local feature extraction and using MHSA to learn global features.\nIn self-attention, relative position encoding is used to learn the content-position relationship in images.\nThis paper uses a novel decoder structure to fuse the high-resolution and low-resolution informa-\ntion. The decoder uses dual-input MHSA to obtain low-resolution and high-resolution information\nand then passes through the residual block [38] to achieve feature reuse, alleviate the problem of\ngradient disappearance, prevent the occurrence of overfitting, and improve segmentation capabilities.\nThe blue-shaded part at the bottom ofFig. 1is the structure of the residual block. Finally, multi-scale\nfeatures are fused using image information at different scales. This structure of first down-sampling\nand then up-sampling reduces the risk of overfitting to a certain extent. In the side output path,\nthe feature map is spatially up-sampling, and then 1× 1 convolution is performed to compress the\n3396 CMC, 2023, vol.76, no.3\nnumber of channels to 2, which is convenient for direct comparison with ground truth and outputs\nthe corresponding probability value of each pixel of the image.\nFigure 1:DT-Net network architecture diagram\n2.2 Deformable Convolution\nMost of the original CNN extract feature information at a fixed position in an image based\non a fixed receptive field structure, and cannot adaptively generate deformable receptive fields and\nconvolution kernel shapes according to different image features [39]. However, the vessel structure is\nirregular and complex, and the introduction of deformable convolution can enhance the construction\nability of retinal vessel geometric deformation. On the basis of the traditional convolution, the\ndeformable convolution increases the direction vector of the convolution kernel to make the shape\nof the convolution kernel closer to the feature. A learnable offset is introduced into the deformable\nconvolution. Offset learning is the use of interpolation algorithm, through back propagation learning.\nThe effective receptive field can more accurately cover the actual shape of the vessel to learn more\nfeatures. Therefore, deformable convolution is used in this paper to enhance the generalization ability\nof the adaptability to different position information of the image and the mapping ability during the\nconvolution process. The deformable convolution formula is as follows:\ny (p) =\nN∑\ni=1\nwi · x (p + pi + /Delta1pi) · /Delta1mi (1)\nLet N denotes the sampling position of a given standard convolution kernel, andwi and pi denote\nthe weight of thei-th position and the preset offset, respectively.x(p) and y(p) denote the features at\nposition p on the input and output feature mapsx and y, respectively. Where/Delta1pi and /Delta1mi are the\nlearnable offset and adjustment factor at thei-th position, and the adjustment factor/Delta1mi ∈ [0, 1] and\n/Delta1pi is an arbitrary value. It can be found that the deformable convolution learns the offset and the\nweight of the sampling points, which can effectively capture the structural details of tiny vessels and\nthus achieve more accurate feature extraction.\nCMC, 2023, vol.76, no.3 3397\n2.3 Multi-Head Self-Attention Mechanism\nMHSA is an attention mechanism that pays more attention to the internal structure, inherently\nhas a global receptive field, and is good at capturing long-distance dependencies. The input feature\nmap can be expressed asX ∈ R\nH×W×C,w h e r eH, W, C are the height, width, and number of channels,\nrespectively. The self-attention calculation formula is as follows:\nAttention (Q, K, V) = softmax\n(QKT\n√\nd\n)\nV = AV (2)\nwhere three 1× 1 convolutions are used to projectX for query, key and value embedding:Q, K, V in\nRH×W×d,w h e r ed is the dimension of the embedding for each head. The attention matrixA works well\nfor feature aggregation, where each row value corresponds to the similarity of a given element inQ\nrelative to all elements inK.\nBecause the image is highly structured data, in the local characteristics of high resolution, in\naddition to the border area, most of the pixels with similar features. Therefore, computing the attention\namong all pixels is very inefficient and redundant. So, we propose an efficient self-attention for the\ntask of vessel segmentation, as shown inFig. 2. The proposed self-attention decoder architecture is\nused to recover detailed information from the skip connections of the encoder, wherex is the image\nfeature of the previous layer in the decoder, and then the 1× 1 convolution is performed to obtain a\nlow-resolution image of sizeH\nl × Wl × d characteristics, y is the image feature from the same layer in\nthe encoder, and then a high-resolution feature of sizeHh × Wh × d is obtained by 1× 1 convolution.\nThen the dot product and soft-max are performed, and the pairwise attention matrix between the input\nunits. Finally, image features of sizeH\nh ×Wh ×d are obtained. For positional encoding, standard self-\nattention blocks lose their positional information and are ineffective for construction highly structured\nimage content [40]. The sinusoidal embedding in the convolution layer in the previous research does\nnot have the property of translation, so the 2-dimensional relative position coding is used by adding\nthe information of relative heightR\nh and widthRw. Relative position coding is used before soft-max\noperation, and the attention logit isqkT × qrT.\nFigure 2:MHSA decoder\n2.4 Loss Function\nLoss function has a significant influence on deep learning training tasks. Most existing methods\nuse only a single loss function to evaluate the network performance. Image segmentation tasks usually\nuse cross-entropy as a loss function, and the ratio of foreground and background pixels in the retinal\nimage is severely imbalanced, resulting in the features of retinal vessels cannot be effectively learned by\n3398 CMC, 2023, vol.76, no.3\nthe model. In the binary segmentation task, the Dice loss function can alleviate the above problems,\nand its essence is to measure the degree of overlap between two samples However, adjusting the weight\nof the network according to a single loss function can easily lead to the loss of the feature information\nof the middle and lower layers of the network. Mixed loss can effectively help the model training and\nenhance segmentation quality. Therefore, the network is trained using a hybrid loss function. Compare\nthe output with the ophthalmologist’s criteria and calculate the loss between them:\nL\n(n) = ω · Lbce + (1 − ω) · Ldice (3)\nwhere ω is the weighting factor for balancing different losses. The binary cross entropy (BCE) loss\nencourages the segmentation model to independently predict the correct class label at each pixel\nposition. Dice loss can alleviate the imbalance of class to some extent. The BCE loss function and\nDice loss function are defined as follows:\nL\nbce =− 1\nK\nK∑\ni=1\n(\ngilog\n(\npi\n)\n+ (1 − gi) log (1 − pi)\n)\n(4)\nLdice = 1 − 2 ∑ K\ni=1 pigi + ϵ∑ K\ni=1 p2\ni + ∑ K\ni=1 g2\ni + ϵ\n(5)\nwhere K represents the number of pixels in a given image, andpi ∈ [0, 1], gi ∈ [0, 1] represent the\npredicted probability and label probability of thei-th pixel, respectively. The parameterε is a Laplace\nsmoothing factor, which avoids numerical problems and speeds up the convergence of the training\nprocess.\n3 Experimental\n3.1 Experimental Details\nIn this paper, we run all experiments based on Windows 10, Intel Core i5-10400F CPU, GeForce\nGTX 1080ti GPU, Python 3.7 language, and PyTorch deep learning framework. The parameters in the\nnetwork are optimized using the Adam optimizer with an initial learning rate of 0.0005 and a weight\ndecay of 0.001. To dynamically adjust the training process, a cosine annealing strategy is utilized to\nupdate the learning rate. The proposed DT-Net framework is trained for 200 epochs with a batch\nsize of 2.Fig. 3shows the loss function curves of the proposed method for training and verifying\ndatasets relative to iteration on three datasets: DRIVE [41], STARE [42], and CHASE_DB1 [43].\nThe horizontal coordinate of the image is the iteration period “Epoch”, and the ordinate is the loss\nvalue “Loss”. Legend “training” means training, and Legend “validation” means validation. When\nthe proposed method is trained on three datasets, the training and validation losses converge rapidly\nwithin 50 epochs, flatten out within 150 epochs, and then reach a stable value.\nFigure 3:Loss function curvesvs. iterations for the training and validation datasets\nCMC, 2023, vol.76, no.3 3399\n3.2 Data and Preprocessing\nWe use the public and widely used standard datasets DRIVE, STARE, and CHASE_DB1 as the\ntraining and testing datasets of the proposed network. Sample images for the three datasets are shown\nin Fig. 4, along with their ground truth vessel segmentation masks and field-of-view (FOV) masks.\nThe DRIVE dataset consists of 40 fundus images with an image resolution of 584× 565, and it is\nspecified that the training set and the test set both contain 20 images. The STARE dataset consists of\n20 fundus images with an image resolution of 700× 605, including 10 retinal images with pathological\nfeatures. This dataset can evaluate the ability of the model to segment abnormal fundus images. The\nCHASE_DB1 dataset consists of 28 retinal images of 14 children with an image resolution of 960×\n999. Since the STARE and CHASE_DB1 datasets do not officially specify the training set and test set,\nwe use the first 10 images of the STARE dataset for model training according to DUNet [29], and the\nremaining 10 images are used for model performance evaluation. For the CHASE_DB1 datasets, we\nfollow a common protocol [44], selecting the first 20 images for model training and the remaining\n8 images for model performance evaluation. The three datasets contain the results of the manual\nsegmentation of retinal images by two experienced ophthalmologists. We used the segmentation results\nof the first ophthalmologist as the ground truth for network training [45] and also as the standard\nsegmentation results for network model and algorithm evaluation.\nFigure 4:Sample images from DRIVE, STARE, and CHASE_DB1. (A) Original image; (B) ground\ntruth; (C) field-of-view masks\n3400 CMC, 2023, vol.76, no.3\nRetinal images often contain noise and uneven illumination, so all images from these three\ndatasets undergo four kinds of preprocessing for image enhancement before the training and testing\ndatasets of the network. The preprocessing process of the fundus images is shown inFig. 5. Firstly, the\ncolor images are converted into grayscale images, simplifying the subsequent preprocessing steps and\nreducing the computation during training. Secondly, each pixel in the grayscale image is normalized\nto reduce the data dimension and speed up the convergence. Then, the CLAHE method [46]i su s e d\nto suppress image noise and enhance the details of the vessel and the contrast between the vessel\nand background. Finally, nonlinear transformation and gamma correction are performed to solve\nthe image quality problem caused by the brightness of the input images, enhance the contrast, make\nthe vessel in the darker area clearer, and improve the image quality. After the above processing,\nit can be found that the distinction between the retinal vessel and the background is significantly\nimproved at this time, which is conducive to feature extraction in the training process and enhances\nthe segmentation quality of the retinal vessel.\nFigure 5:Image preprocessing of fundus images, from left to right, are original image, image graying,\nimage normalization, gradient histogram equalization, gamma correction, original images patches,\nand ground truth patches\nDue to the limited number of images in the fundus datasets, patch processing is adopted to expand\nthe datasets to reduce the effect of overfitting. In the training process, each image of the pre-processed\ndatasets is first randomly cropped into a patch of size 64× 64 for the training of the network. The\ncorresponding patches are also extracted from the ground truth to ensure the original images and the\nground truth. In the experiments, 90% of the extracted patches are used for training and the remaining\n10% are used for validation.Fig. 5shows some image patches and the corresponding ground truth of\nthe fundus images.\n3.3 Evaluation Index\nSimilar to most methods of retinal image segmentation, we will compare the proposed DT-Net\nmethod with other algorithms and evaluate it through the following indicators: Accuracy (ACC),\nSpecificity (SP), Sensitivity (SE), F1-score, and Area Under Receiver Operating Characteristic (ROC)\nCurve (AUC). Acc is used to evaluate the overall segmentation performance of the model. The larger\nthe ACC, the more accurate the segmentation. The specific mathematical expression is as follows:\nACC = TP + TN\nTP + FN + FP + TN (6)\nSP is an important metric of retinal vessel segmentation. It is the ratio of correct negative\npredictions to the total number of negative predictions. It mainly evaluates the ability to recognize\nbackground in retinal images. The better the SP value, the lower the false positive rate (FPR). The\nspecific mathematical expression is as follows:\nSP = TN\nTN + FP (7)\nCMC, 2023, vol.76, no.3 3401\nSE mainly evaluates the ability to recognize retinal vessels (positive) in retinal images. It is the ratio\nof correct positive predictions to the total number of positive predictions in the predicted results. The\nspecific mathematical expression is as follows:\nSE = TP\nTP + FN (8)\nF1-score for evaluation of segmentation results and the similarity criteria of ophthalmology. The\nlarger the value is, the closer the algorithm segmentation result is to the gold standard, and the\nsegmentation effect. The specific mathematical expression is as follows:\nF1 = 2TP\n2TP + FP + FN (9)\nAmong them, true positive (TP) means correctly identified vessel pixels, true negative (TN) means\ncorrectly identified non-vessel pixels, false positive (FP) means non-vessel pixels identified as vessels,\nand false negative (FN) means vessel pixels identified as non-vessel.\nIn addition, we also introduce AUC to evaluate the segmentation performance of the model.\nAUC is a professional metric of retinal vessel segmentation. The closer its value is to 1, the better\nthe performance of the algorithm and the stronger the robustness. The ROC curve describes the\nrelationship between the true position rate and the false position rate under the different classification\nthresholds. The closer the value of the area under the AUC is to 1, the better the algorithm performs\nand the more robustness.\n3.4 Ablation Experiment\nTo further verify the effectiveness of the proposed network for vessel segmentation, we conduct\nablation experiments on the DRIVE dataset. The prediction results of the network are compared\nin terms of five performance metrics: ACC, SE, SP, AUC, and F1-score. To more clearly see the\nimprovement of the accuracy of retinal vessel segmentation by each module proposed in the model, the\nsegmentation performance of different methods is shown inTable 1. M0 uses a hybrid loss function\nbased on U-Net. M1 adds multiple inputs and side outputs based on M0. M2 adds the encoder hybrid\nblock based on M1. M3 adds the transformer decoder block based on M1. M4 adds the encoder hybrid\nblock and transformer decoder block based on M1.\nTable 1: Ablation experimental results of vessel segmentation on DRIVE dataset\nMethod ACC SE SP AUC F1-score\nM0 95.58% 74.68% 98.62% 96.70% 81.13%\nM1 95.63% 81.24% 97.73% 97.89% 82.56%\nM2 96.25% 83.75% 97.99% 98.40% 84.57%\nM3 96.26% 85.49% 97.76% 98.19% 84.86%\nM4 96.31% 86.36% 98.84% 98.43% 84.88%\nAs shown in Table 1, when multi-scale input and side output is added to M0, each index is\nsignificantly improved, and the segmentation performance of the network is improved. After adding\nthe hybrid block to the network, AUC and F1-score in M2 are 0.51% and 2.01% higher than M1,\nrespectively, which verifies the effectiveness of the hybrid block. In M3, SE, AUC, and F1-score\n3402 CMC, 2023, vol.76, no.3\nare 4.25%, 0.3%, and 2.3% higher than M1, respectively. It is shown that the proposed MHSA\ndecoder block is effective in retinal vessel segmentation and enhances the performance of retinal vessel\nsegmentation.\nWe can see from the last row ofTable 1that the values of SE, AUC, and F1-score of the proposed\nnetwork are increased from 74.68%, 96.70%, 81.13% of M0 to 86.36%, 98.43%, 84.88%, respectively.\nExperiments show that using either the hybrid block of the encoder or the attention block of the\ndecoder can improve the segmentation performance of the network, which shows their rationality and\neffectiveness. Therefore, the proposed method has advantages in retinal vessel segmentation.\nAblation experiments are performed by setting different loss functions to verify which loss func-\ntion is more suitable for the proposed method. The effects of varying loss functions on performance\nindexes are shown inTable 2. First, “DT+BCE” uses a BCE loss function to train the network.\n“DT+Dice” uses the Dice loss function. “DT+BCE+Dice” combines the BCE loss function and the\nDice loss function. The results inTable 2show that almost all the metrics are improved with the help of\nthe hybrid loss, which proves that the hybrid loss contributes to enhancing the accuracy of the model.\nTable 2: Loss function ablation experiment\nMethod ACC SE SP AUC F1-score\nDT+BCE 96.22% 85.88% 97.67% 98.18% 84.79%\nDT+Dice 96.21% 86.28% 97.58% 96.62% 84.82%\nDT+BCE+Dice 96.31% 86.36% 98.84% 98.43% 84.88%\nThe learning rate, as an essential parameter in the process of model training, controls the learning\nprogress of the network model. To explore the influence of different learning rates on segmented\nimages,Table 3shows the segmentation results when the learning rate of this method is 0.0001, 0.0003,\n0.0005, 0.0007, 0.0009, and 0.0011. When the learning rate is set to 0.0005, the best performance is\nachieved for all metrics. When the learning rate increases or decreases, both F1-score and AUC will\ndecrease.\nTable 3: Segmentation results with different learning rates\nlearning rate ACC SE SP AUC F1-score\n0.0001 96.18% 85.14% 97.73% 98.19% 84.56%\n0.0003 96.20% 86.39% 97.65% 98.11% 84.73%\n0.0005 96.31% 86.36% 98.84% 98.43% 84.88%\n0.0007 96.15% 85.58% 97.62% 98.09% 84.01%\n0.0009 95.66% 85.88% 97.19% 98.21% 83.59%\n0.0011 96.28% 87.74% 98.03% 98.08% 83.81%\n3.5 Comparison with Other Methods\nWe chose five other retinal vessel segmentation methods to compare with our method to\nprove the advantages of the proposed method. The five methods are U-Net [18], DUNet [24], GT\nU-Net [47], Attention Residual U Network (AReN-UNet) [48], and Multistage Dual-Path Interactive\nCMC, 2023, vol.76, no.3 3403\nRefinement Network (Multistage DPIRef-Net) [49]. For a better comparison with these methods, these\nmodels are trained using the same experimental settings as in this paper. In addition, we compare the\nperformance with UNet++ [27], Li et al. proposed mothed [44], D-MNet [50], WA-Net [51], and\nTUnet-LBF [52]. Among them, GT U-Net modifies both the encoder and decoder to achieve good\nperformance on tooth segmentation. The AReN-UNet proposes a novel cascaded network driven\nby integrating attention and residual modules. It improves convergence and stability and reduces\nvessel breakdown in the vessel map. Multistage DPIRef-Net uses multi-stage dual-path interaction\nto refine the network, retain the vascular branch edges, suppress the false positive rate, and accurately\nsegment the arteriovenous vascular map of the retinal surface. The Multistage DPIRef-Net is a vessel\nsegmentation architecture with a single encoder and dual decoder. It requires annotated labels of\narteries and veins, while our network uses only one kind of annotated label. Therefore, to be fair,\nwe use the experimental data of this paper in the same experimental setting, i.e., using one kind of\nannotated label. The modified network mainly consists of one backbone network and three single-path\nstages. The D-MNet uses deformable convolution and attention modules to improve the accuracy and\nconnectivity of vessel segmentation.\nTables 4–6 evaluate the different vessel segmentation methods in the DRIVE, STARE, and\nCHASE_DB1 datasets. Because there are more background pixels than vessel pixels in the fundus\nimages, AUC and F1-score metrics are more suitable for evaluating the vessel segmentation method.\nIn Table 4, compared with the maximum of existing methods, our proposed method performs better\non the DRIVE dataset. There is a 2.83% increase in SE, 0.2% in SP, 0.22% in AUC, and 1.22% in\nF1-score, respectively. The highest SE and SP of the proposed model means that retinal vessels can\nbe identified more accurately, and noise information can be suppressed. This is because the MHSA\nmechanism focuses on capturing global vessel details. We can observe fromTable 5that the proposed\nmethod achieves the best performance for ACC, SP, AUC, and F1-score on STARE datasets compared\nto other methods. The proposed method performs better, indicating that the framework is effective for\nvessel segmentation. Since there are many lesion images in the STARE dataset, the SE is not optimal\nall metrics of our method are highest on the CHASE_DB1 dataset except the SE and ACC metric.\nOn the whole, our method performs well. As seen fromTables 4–6, the proposed method has the\nhighest F1-score metrics on the three datasets compared to the maximum value of each metric of\nthe other methods, with an increase of 1.85%, 3.61%, and 0.26%, respectively. This indicates that the\nproposed method can distinguish retinal vessel pixels and background pixels effectively and accurately.\nIn general, compared with these methods, the proposed method can segment retinal vessels more\naccurately and has good prospects for application in clinical medical imaging diagnosis.\nTable 4: Comparison of the proposed method with existing methods in the DRIVE dataset\nMethod Y ear ACC SE SP AUC F1-score\nU-Net 2015 95.58% 74.91% 98.59% 96.83% 81.19%\nDU-Net 2019 95.49% 75.84% 98.35% 97.59% 81.05%\nUNet++ 2020 95.35% 74.73% 98.35% 97.13% 80.35%\nGT U-Net 2021 96.22% 80.58% 98.64% 97.96% 83.66%\nAReN-UNet 2021 96.60% 83.53% 98.12% 98.21% 82.87%\nLi et al. 2021 95.68% 79.21% 98.10% 98.06% –\nMultistage DPIRef-Net 2022 95.64% 80.53% 98.39% 97.62% 81.61%\n(Continued)\n3404 CMC, 2023, vol.76, no.3\nTable 4 (continued)\nMethod Y ear ACC SE SP AUC F1-score\nD-MNet 2022 95.39% 73.68% 97.12% 97.93% 82.13%\nWA-Net 2022 95.75% 79.66% 98.10% 97.84% 82.69%\nTUnet_LBF 2023 96.50% 81.40% 98.09% 89.75% –\nProposed 2023 96.31% 86.36% 98.84% 98.43% 84.88%\nTable 5: Comparison of the proposed method with existing methods in the STARE dataset\nMethod Y ear ACC SE SP AUC F1-score\nU-Net 2015 95.81% 77.62% 97.90% 97.12% 79.28%\nDU-Net 2019 96.15% 71.43% 99.04% 97.23% 76.55%\nUNet++ 2020 96.05% 77.76% 98.32% 97.40% 81.32%\nGT U-Net 2021 95.87% 70.49% 98.49% 97.03% 77.81%\nAReN-UNet 2021 96.73% 81.15% 98.78% 98.59% 82.11%\nLi et al. 2021 96.78% 83.52% 98.23% 98.75% –\nMultistage DPIRef-Net 2022 96.59% 80.04% 98.55% 97.64% 80.66%\nD-MNet 2022 96.43% 84.35% 97.79% 98.55% 82.74%\nWA-Net 2022 96.55% 77.67% 98.77% 96.65% 81.76%\nTUnet_LBF 2023 96.81% 80.04% 98.52% 89.28% –\nProposed 2023 97.03% 83.52% 99.37% 99.00% 86.37%\nTable 6: Comparison of the proposed method with existing methods in the CHASE_DB1 dataset\nMethod Y ear ACC SE SP AUC F1-score\nU-Net 2015 95.96% 73.38% 97.72% 96.54% 79.10%\nDU-Net 2019 96.69% 81.53% 98.30% 98.40% 80.43%\nUNet++ 2020 95.06% 63.61% 98.94% 97.04% 73.90%\nGT U-Net 2021 95.96% 72.72% 99.05% 96.11% 80.27%\nAReN-UNet 2021 97.51% 83.18% 98.41% 98.45% 81.95%\nLi et al. 2021 96.35% 78.18% 98.19% 98.10% –\nMultistage DPIRef-Net 2022 96.45% 81.18% 98.52% 97.79% 80.75%\nD-MNet 2022 95.87% 85.43% 96.93% 98.06% 79.01%\nWA-Net 2022 98.41% 80.42% 98.26% 96.53% 80.98%\nTUnet_LBF 2023 97.08% 83.54% 98.06% 90.80% –\nProposed 2023 97.37% 84.87% 99.21% 98.48% 82.74%\nTo further observe the segmentation results of the models, partial segmentation results on the three\ndatasets are given for visual comparison, as shown inFigs. 6–8. It can be seen that the DT-Net model\nCMC, 2023, vol.76, no.3 3405\nproduces more details of the vessel segmentation. Compared with U-Net, DT-Net can detect more\nvessels. Compared with DUNet and GT U-Net, DT-Net can detect some details of missing vessels and\nthus complete segmentation more efficiently. Compared with AReN-UNet and Multistage DPIRef-\nNet, DT-Net has better vessel continuity. We can observe that the DT-Net is superior to the other five\nmethods on three datasets. As can be seen inFigs. 6–8, the segmentation effect of the proposed method\nis very close to the standard of ophthalmologist manual segmentation, obtaining more continuous\nvessels. It can successfully segment the continuous tiny vessels and has good generalization ability\nwhen segmenting different datasets. This proves the network can reduce background noise, enhance\ncontrast, and preserve irregular vessels well. Visualization further illustrates the importance of multi-\nscale contextual information and capturing long-term dependencies in retinal vessel segmentation.\nThis suggests that the proposed method can segment retinal vessel images, help specialized physicians\nin disease diagnosis, and reduce the workload of clinical medical specialists. In addition, we use the\nROC curve to evaluate the model, shown inFig. 9. The closer the ROC curve is to the upper left\nboundary, the more accurate the network is.Fig. 10shows a locally enlarged view of the tiny vessel in\nthe segmentation result. This is because, in retinal images, tiny vessels are not significantly different\nfrom the image background. Therefore, to help the network pay attention to essential features and\nsuppress unnecessary features, we use the MHSA mechanism in both the encoder and decoder. It\ncan be seen fromFig. 10that the proposed algorithm has good robustness to the intersection of the\nvessel and tiny vessel areas with low contrast and maintains the degree and connectivity of thick and\nthin vessels, and the segmentation results of the lesion region are relatively close to the standard\nsegmentation. The reliability and robustness of this algorithm for retinal vessel segmentation are\nverified. The above experimental results can prove that the performance of the proposed model is\ngenerally better, it can more accurately identify vessels and backgrounds, and segment tiny vessels\nbetter.\nFigure 6: The segmentation results of different models on DRIVE datasets. (A) Original images;\n(B) ground truth images; (C) U-Net; (D) DUNet; (E) GT U-Net; (F) AReN-UNet; (G) multistage\nDPIRef-Net; (H) ours\n3406 CMC, 2023, vol.76, no.3\nFigure 7: The segmentation results of different models on STARE datasets. (A) Original images;\n(B) ground truth images; (C) U-Net; (D) DUNet; (E) GT U-Net; (F) AReN-UNet; (G) multistage\nDPIRef-Net; (H) ours\nFigure 8:The segmentation results of different models on CHASE_DB1 datasets. (A) Original images;\n(B) ground truth images; (C) U-Net; (D) DUNet; (E) GT UNet; (F) AReN-UNet; (G) multistage\nDPIRef-Net; (H) ours\nFigure 9:The ROC curves of the DT-Net model on different datasets. (A) DRIVE dataset. (B) STARE\ndataset. (C) CHASE DB1 dataset\nCMC, 2023, vol.76, no.3 3407\nFigure 10:Partial increased view of different models on different datasets. From top to bottom are the\nfundus images from the DRIVE, STARE, and CHASE DB1 datasets. From left to right: (A) original\nimages; (B) partial views; (C) ground truth; (D) U-Net; (E) DUNet; (F) GT U-Net; (G) AReN-UNet;\n(H) multistage DPIRef-Net; (I) ours\n4 Discussion\nIn our work, we propose a hybrid convolution and transformer network evolved from the classical\nmodel U-Net, which aims to aggregate multi-scale feature information at different resolutions to\nachieve accurate and efficient vessel segmentation. The fundus image is full of noise and low contrast.\nTherefore, we first preprocess to improve image contrast and suppress the background noise of the\nsource image. To fully use multi-scale information, DT-Net uses multi-scale images as input, and then\nwe introduce deformable convolution to change the convolution kernel according to the actual shape\nof blood vessels to obtain more accurate structural information. Meanwhile, the MHSA mechanism is\nused to capture the distant relationship of fundus images, making up for the defect that CNN cannot\nextract global features.\nIn addition, the proposed network is verified by ablative experiments. The ACC and AUC indexes\nof the network improved significantly after the addition of a mixing block to the encoder, and the\nSE and F1-scoring indexes of the network improved significantly after the addition of a transformer\ndecoder block. Of course, the current study of DT-Net proposed has the following shortcomings: (1)\nDue to the similarity between the background and blood vessels in the datasets, our method cannot\nachieve the best performance in each index; (2) They inevitably lose some container details due to\nconstant up-sampling. In the future, we will introduce more advanced methods, such as the encoding\npattern in Swin-Unet, to preserve more details in the original image and make our model perform\nbetter on various metrics.\n3408 CMC, 2023, vol.76, no.3\n5 Conclusion\nWe propose a network named DT-Net for fundus blood vessel segmentation. The performance\nof this method is mainly improved by the introduction of variable convolution and multiple self-\nattention mechanisms, which not only extract the structural information easily ignored in fundus blood\nvessel images but also effectively extract information at different scales. And the DT-Net presented\nin the DRIVE, STARE, and CHASE_DB1 datasets is significantly improved. Experimental results\nshow that this method can better process different fundus data sets, has better generalization ability,\nand provides more accurate segmentation results for medical diagnosis and treatment. In terms of\nsegmentation results, our model can segment more vascular details and has better connectivity.\nAcknowledgement: We thank the High-Performance Computing Center of the Shijiazhuang Tiedao\nUniversity for their support.\nFunding Statement: This work was supported in part by the National Natural Science Foundation\nof China under Grant 61972267; the National Natural Science Foundation of Hebei Province\nunder Grant F2018210148; the University Science Research Project of Hebei Province under Grant\nZD2021334.\nAuthor Contributions:WJ and SM design of study, analysis of data. WJ and SM conducted experiments\nand drafted the manuscript. PG and SM revised and edited the manuscript. YS and PG polished the\nmanuscript. All authors reviewed the results and approved the final version of the manuscript.\nAvailability of Data and Materials:Publicly available datasets were analyzed in this study. This data\ncan be found here: DRIVE:https://drive.grand-challenge.org; STARE: https://cecas.clemson.edu/~\nahoover/stare/; CHASE_DB1:https://blogs.kingston.ac.uk/retinal/chasedb1.\nConflicts of Interest:The authors declare that they have no conflicts of interest to report regarding the\npresent study.\nReferences\n[1] A. Bilal, G. M. Sun and S. Mazhar, “Diabetic retinopathy detection using weighted filters and classification\nusing CNN,” inIEEE Int. Conf. on Intelligent Technologies, Hubli, India, pp. 1–6, 2021.\n[2] A. Bilal, G. M. Sun, Y . Li, S. Mazhar and A. Q. Khan, “Diabetic retinopathy detection and classification\nusing mixed models for a disease grading database,”IEEE Access, vol. 9, pp. 23544–23553, 2021.\n[3] A. Bilal, G. M. Sun and S. Mazhar, “Survey on recent developments in automatic detection of diabetic\nretinopathy,”Journal Français d’Ophtalmologie, vol. 44, no. 3, pp. 420–440, 2021.\n[4] K. Kipli, M. E. Hoque, L. T. Lim, M. H. Mahmood, S. K. Sahariet al.,“A review on the extraction of\nquantitative retinal microvascular image feature,”Computational and Mathematical Methods in Medicine,\nvol. 2018, no. 4, pp. 1–21, 2018.\n[5] R. Xu, T. T. Liu, X. C. Y e, F . Liu, L. Linet al., “Joint extraction of retinal vessels and centerlines\nbased on deep semantics and multi-scaled cross-task aggregation,”IEEE Journal of Biomedical and Health\nInformatics, vol. 25, no. 7, pp. 2722–2732, 2021.\n[6] K. Sultan, A. Louai and B. Saleh, “DSMSA-Net: Deep spatial and multi-scale attention network for road\nextraction in high spatial resolution satellite images,”Arabian Journal for Science and Engineering, vol. 48,\npp. 1907–1920, 2022.\n[7] K. Sultan, A. Louai and B. Sale, “An encoder-decoder deep learning framework for building footprints\nextraction from aerial imagery,”Arabian Journal for Science and Engineering, vol. 48, no. 2, pp. 1723–1284,\n2023.\nCMC, 2023, vol.76, no.3 3409\n[ 8 ] H .R .L i u ,M .Z .L i u ,D .F .L i ,W .F .Z h e n g ,L .R .Y i net al.,“Recent advances in pulse-coupled neural\nnetworks with applications in image processing,”Electronics, vol. 11, no. 20, pp. 3264, 2022.https://doi.\norg/10.3390/electronics11203264\n[ 9 ] X .M .Q i n ,Y .X .B a n ,P .W u ,B .Y a n g ,S .L i uet al.,“Improved image fusion method based on sparse\ndecomposition,” Electronics, vol. 11, no. 15, pp. 2321, 2022.\n[10] K. Jin, Y . Yan, S. Wang, C. Yang, M. Chenet al.,“iERM: An interpretable deep learning system to classify\nepiretinal membrane for different optical coherence tomography devices: A multi-center analysis,”Journal\nof Clinical Medicine, vol. 12, no. 2, pp. 400, 2023.\n[11] E. Shibuya and K. Hotta, “Feedback U-Net for cell image segmentation,” inIEEE/CVF Conf. on Computer\nVision and Pattern Recognition Workshop, Seattle, WA, USA, pp. 974–975, 2020.\n[12] S. A. Taghanaki, K. Abhishek, J. P . Cohen, J. Cohen-Adad and G. Hamarneh, “Deep semantic segmenta-\ntion of natural and medical images: A review,”Artificial Intelligence Review, vol. 54, no. 1, pp. 137–138,\n2021.\n[13] H. Dai, G. Huang, H. Zeng and R. Yu, “Haze risk assessment based on improved PCA-MEE and ISPO-\nLightGBM model,”Systems, vol. 10, no. 6, pp. 263, 2022.\n[14] H. Zeng, B. Shao, H. Dai, Y . Yan and N. Tian, “Prediction of fluctuation loads based on GARCH family-\nCatBoost-CNNLSTM,” Energy, vol. 263, no. 6, pp. 126125, 2023.\n[15] Y . Gao, C. Liu and L. Zhao, “Multi-resolution path CNN with deep supervision for intervertebral\ndisc localization and segmentation,” inInt. Conf. on Medical Image Computing and Computer-Assisted\nIntervention, Shenzhen, China, pp. 309–317, 2019.\n[16] N. Tajbakhsh, L. Jeyaseelan, Q. Li, J. N. Chiang, Z. Wuet al.,“Embracing imperfect datasets: A review\nof deep learning solutions for medical image segmentation,”Medical Image Analysis, vol. 63, pp. 101693,\n2020.\n[17] F . Isensee, P . F . Jaeger, S. A. A. Kohl, J. Petersen and K. H. Maier-Hein, “nnU-Net: A self-configuring\nmethod for deep learning-based biomedical image segmentation,”Nature Methods, vol. 18, no. 2, pp. 203–\n211, 2021.\n[18] O. Ronneberger, P . Fischer and T. Brox, “U-Net: Convolutional networks for biomedical image segmenta-\ntion,” Medical Image Computing and Computer-Assisted Intervention, vol. 9351, pp. 234–241, 2015.\n[19] J. Long, E. Shelhamer, T. Darrell and M. Intelligence, “Fully convolutional networks for semantic\nsegmentation,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 39, no. 4, pp. 640–\n651, 2017.\n[20] Y . Weng, T. Zhou, Y . Li and X. Qiu, “NAS-Unet: Neural architecture search for medical image segmen-\ntation,” inIEEE Conf. on Computer Vision and Pattern Recognition, Boston, Massachusetts, USA, pp.\n44247–44257, 2019.\n[21] A. Bilal, L. C. Zhu, A. N. Deng, H. H. Lu and N. Wu, “AI-based automatic detection and classification of\ndiabetic retinopathy using U-net and deep learning,”Symmetry, vol. 14, no. 7, pp. 1427, 2022.\n[22] A. Bilal, G. M. Sun, S. Mazhar, A. Imran and J. Latif, “A transfer learning and U-net-based automatic\ndetection of diabetic retinopathy from fundus images,”Computer Methods in Biomechanics and Biomedical\nEngineering: Imaging & Visualization, vol. 10, no. 6, pp. 663–674, 2022.\n[23] A. Bilal, G. M. Sun, S. Mazhar and A. Imran, “Improved grey wolf optimization-based feature selection\nand classification using CNN for diabetic retinopathy detection,”Evolutionary Computing and Mobile\nSustainable Networks, vol. 116, pp. 1–14, 2022.\n[24] F . Zhao, Z. Wu, L. Wang, W . Lin and G. Li, “Spherical deformable U-Net: Application to cortical surface\nparcellation and development prediction,”Computer Methods and Programs in Biomedicine, vol. 40, no. 4,\npp. 1217–1228, 2021.\n[25] H. Fu, J. Cheng, Y . Xu, D. Wong, J. Liuet al.,“Joint optic disc and cup segmentation based on multi-\nlabel deep network and polar transformation,”IEEE Transactions on Medical Imaging, vol. 37, no. 7, pp.\n1597–1605, 2018.\n[26] D. Li, D. A. Dharmawan, B. P . Ng and S. Rahardja, “Residual U-Net for retinal vessel segmentation,” in\nIEEE Int. Conf. on Image Processing, Taipei, Taiwan, pp. 1425–1429, 2019.\n3410 CMC, 2023, vol.76, no.3\n[27] Z. Zhou, M. Siddiquee, N. Tajbakhsh and J. Liang, “UNet++: Redesigning skip connections to exploit\nmultiscale features in image segmentation,”IEEE Transactions on Medical Imaging, vol. 39, no. 6, pp. 1856–\n1867, 2020.\n[28] L. Li, M. Verma, Y . Nakashima, H. Nagahara and R. Kawasaki, “IterNet: Retinal image segmentation\nutilizing structural redundancy in vessel networks,” inIEEE Winter Conf. on Applications of Computer\nVision, Snowmass Village, USA, pp. 3656–3665, 2020.\n[29] Q. Jin, Z. Meng, T. D. Pham, Q. Chen, L. Weiet al.,“DUNet: A deformable network for retinal vessel\nsegmentation,” Knowledge-Based Systems, vol. 178, pp. 149–162, 2019.\n[ 3 0 ] J .D a i ,H .Q i ,Y .X i o n g ,Y .L i ,G .Z h a n get al.,“Deformable convolutional networks,” inIEEE Int. Conf.\non Computer Vision, Venice, Italy, pp. 764–773, 2017.\n[31] H. Li, Y . K. Wang, C. Wan, J. X. Shen, Q. L. Yuet al.,“MAU-Net: A retinal vessels segmentation method,”\nin Annual Int. Conf. of the IEEE Engineering in Medicine Biology Society, Montreal, QC, Canada, pp. 1958–\n1961, 2020.\n[32] X. Zhu, H. Hu, S. Lin and J. Dai, “Deformable convNets v2: More deformable, better results,” in\nIEEE/CVF Conf. on Computer Vision and Pattern Recognition, Seattle, USA, pp. 9300–9308, 2018.\n[33] J. Chen, Y . Lu, Q. Yu, X. Luo and Y . Zhou, “TransUNet: Transformers make strong encoders for medical\nimage segmentation,” arXiv:2102.04306, 2021.\n[34] H. Cao, Y . Wang, J. Chen, D. Jiang, X. Zhanget al.,“Swin-Unet: Unet-like pure transformer for medical\nimage segmentation,” inComputer Vision—ECCV 2022 Workshops, Cham, pp. 205–218, 2023.\n[35] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Weiet al.,“Swin transformer: Hierarchical vision transformer using\nshifted windows,” inIEEE/CVF Int. Conf. on Computer Vision, Montreal, QC, Canada, pp. 9992–10002,\n2021.\n[36] H. Wu, S. Chen, G. Chen, W . Wang, B. Leiet al.,“FAT-Net: Feature adaptive transformers for automated\nskin lesion segmentation,”Medical Image Analysis, vol. 76, pp. 102327, 2022.\n[37] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn and N. Houlsby, “An image is worth 16× 16\nwords: Transformers for image recognition at scale,” inInt. Conf. on Learning Representations, Vienna,\nAustria, pp. 1–12, 2021.\n[38] K. He, X. Zhang, S. Ren and J. Sun, “Deep residual learning for image recognition,” inIEEE Conf. on\nComputer Vision and Pattern Recognition, Las Vegas, NV , USA, pp. 770–778, 2016.\n[ 3 9 ] F .C h e n ,F .W u ,J .X u ,G .G a o ,Q .G eet al.,“Adaptive deformable convolutional network,”Neurocomputing,\nvol. 453, pp. 853–864, 2021.\n[40] I. Bello, B. Zoph, Q. Le, A. Vaswani and J. Shlens, “Attention augmented convolutional networks,” in\nIEEE/CVF Int. Conf. on Computer Vision, Seoul, Korea (South), pp. 3286–3295, 2019.\n[41] J. Staal, M. D. Abramoff, M. Niemeijer, M. A. Viergever and B. V . Ginneken, “Ridge-based vessel\nsegmentation in color images of the retina,”IEEE Transactions on Medical Imaging,v o l .2 3 ,n o .4 ,p p .\n501–509, 2004.\n[42] A. Hoover, V . Kouznetsova and M. Goldbaum, “Locating blood vessels in retinal images by piecewise\nthreshold probing of a matched filter response,”IEEE Transactions on Medical Imaging, vol. 19, no. 3, pp.\n203–210, 2000.\n[43] C. G. Owen, A. R. Rudnicka, R. Mullen, S. A. Barman, D. Monekossoet al., “Measuring retinal\nvessel tortuosity in 10-year-old children: Validation of the computer-assisted image analysis of the retina\n(CAIAR) program,”Investigative Ophthalmology & Visual Science, vol. 50, no. 5, pp. 2004–2010, 2009.\n[44] X. Li, Y . Jiang, M. Li and S. Yin, “Lightweight attention convolutional neural network for retinal vessel\nimage segmentation,”IEEE Transactions on Industrial Informatics, vol. 17, no. 3, pp. 1958–1967, 2021.\n[45] H. S. Wu, W . Wang, J. F . Zhong, B. Y . Lei, Z. K. Wenet al.,“SCS-Net: A scale and context sensitive network\nfor retinal vessel segmentation,”Medical Image Analysis, vol. 70, no. 1, pp. 102025, 2021.\n[46] S. M. Pizer, E. P . Amburn, J. D. Austin, R. Cromartie, A. Geselowitzet al., “Adaptive histogram\nequalization and its variations,”Computer Vision, Graphics, and Image Processing, vol. 39, no. 3, pp. 355–\n368, 1987.\nCMC, 2023, vol.76, no.3 3411\n[47] Y . Li, S. Wang, J. Wang, G. Zeng, W . Liuet al.,“GT U-Net: A U-Net like group transformer network for\ntooth root segmentation,” inInt. Workshop on Machine Learning in Medical Imaging, Virtual, pp. 386–395,\n2021.\n[48] A. A. Rahman, B. Biswal, P . P . Geetha, S. Hasan and M. V . S. Sairam, “Robust segmentation of vascular\nnetwork using deeply cascaded AReN-UNet,”Biomedical Signal Processing and Control, vol. 69, no. 3, pp.\n102953, 2021.\n[49] G. Pavani, B. Biswal and T. K. Gandhi, “Multistage DPIRef-Net: An effective network for semantic\nsegmentation of arteries and veins from retinal surface,”Neuroscience Informatics, vol. 2, no. 4, pp. 100074,\n2022.\n[50] X. Deng and J. Y e, “A retinal blood vessel segmentation based on improved D-MNet and pulse-coupled\nneural network,”Biomedical Signal Processing and Control, vol. 73, pp. 103467, 2022.\n[51] D. E. Alvarado-Carrillo and O. S. Dalmau-Cedeno, “Width attention based convolutional neural network\nfor retinal vessel segmentation,”Expert Systems with Applications, vol. 209, no. C, pp. 11, 2022.\n[52] H. Y . Zhang, W . H. Ni, Y . Luo, Y . N. Feng, R. X. Songet al.,“TUnet-LBF: Retinal fundus image fine\nsegmentation model based on transformer Unet network and LBF,”Computers in Biology and Medicine,\nvol. 159, pp. 106937, 2023."
}