{
  "title": "Humor Detection: A Transformer Gets the Last Laugh",
  "url": "https://openalex.org/W2970252517",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2970139245",
      "name": "Orion Weller",
      "affiliations": [
        "Brigham Young University",
        "Laboratoire d'Informatique de Paris-Nord"
      ]
    },
    {
      "id": "https://openalex.org/A2688037721",
      "name": "Kevin Seppi",
      "affiliations": [
        "Laboratoire d'Informatique de Paris-Nord",
        "Brigham Young University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2138825839",
    "https://openalex.org/W4394643672",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2038712753",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2885586371",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W96086407",
    "https://openalex.org/W2470673105",
    "https://openalex.org/W2936405048",
    "https://openalex.org/W2804900514",
    "https://openalex.org/W172035031",
    "https://openalex.org/W2033175753",
    "https://openalex.org/W2090915937",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2251785914",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963045354"
  ],
  "abstract": "Orion Weller, Kevin Seppi. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",
  "full_text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing, pages 3621–3625,\nHong Kong, China, November 3–7, 2019.c⃝2019 Association for Computational Linguistics\n3621\nHumor Detection: A Transformer Gets the Last Laugh\nOrion Weller\nComputer Science Department\nBrigham Young University\norionw@byu.edu\nKevin Seppi\nComputer Science Department\nBrigham Young University\nkseppi@byu.edu\nAbstract\nMuch previous work has been done in attempt-\ning to identify humor in text. In this pa-\nper we extend that capability by proposing a\nnew task: assessing whether or not a joke is\nhumorous. We present a novel way of ap-\nproaching this problem by building a model\nthat learns to identify humorous jokes based\non ratings gleaned from Reddit pages, consist-\ning of almost 16,000 labeled instances. Using\nthese ratings to determine the level of humor,\nwe then employ a Transformer architecture\nfor its advantages in learning from sentence\ncontext. We demonstrate the effectiveness of\nthis approach and show results that are com-\nparable to human performance. We further\ndemonstrate our model’s increased capabili-\nties on humor identiﬁcation problems, such as\nthe previously created datasets for short jokes\nand puns. These experiments show that this\nmethod outperforms all previous work done on\nthese tasks, with an F-measure of 93.1% for\nthe Puns dataset and 98.6% on the Short Jokes\ndataset.\n1 Introduction\nRecent advances in natural language processing\nand neural network architecture have allowed for\nwidespread application of these methods in Text\nSummarization (Liu et al., 2018), Natural Lan-\nguage Generation (Bahuleyan, 2018), and Text\nClassiﬁcation (Yang et al., 2016). Such advances\nhave enabled scientists to study common language\npractices. One such area, humor, has garnered fo-\ncus in classiﬁcation (Zhang and Liu, 2014; Chen\nand Soo, 2018), generation (He et al., 2019; Vali-\ntutti et al., 2013), and in social media (Raz, 2012).\nThe next question then is, what makes a joke\nhumorous? Although humor is a universal con-\nstruct, there is a wide variety between what each\nindividual may ﬁnd humorous. We attempt to fo-\ncus on a subset of the population where we can\nquantitatively measure reactions: the popular Red-\ndit r/Jokes thread. This forum is highly popu-\nlar - with tens of thousands of jokes being posted\nmonthly and over 16 million members. Although\nlarger joke datasets exist, the r/Jokes thread is un-\nparalleled in the amount of rated jokes it contains.\nTo the best of our knowledge there is no compa-\nrable source of rated jokes in any other language.\nThese Reddit posts consist of the body of the joke,\nthe punchline, and the number of reactions or up-\nvotes. Although this type of humor may only be\nmost enjoyable to a subset of the population, it is\nan effective way to measure responses to jokes in\na large group setting.1\nWhat enables us to perform such an analysis\nare the recent improvements in neural network ar-\nchitecture for natural language processing. These\nbreakthroughs started with the Convolutional Neu-\nral Network (LeCun et al., 1998) and have recently\nincluded the inception (Bahdanau et al., 2015) and\nprogress of the Attention mechanism (Luong et al.,\n2015; Xu et al., 2015), and the Transformer archi-\ntecture (Vaswani et al., 2017).\n2 Related Work\nIn the related work of joke identiﬁcation, we ﬁnd a\nmyriad of methods employed over the years: sta-\ntistical and N-gram analysis (Taylor and Mazlack,\n2004), Regression Trees (Purandare and Litman,\n2006), Word2Vec combined with K-NN Human\nCentric Features (Yang et al., 2015), and Convo-\nlutional Neural Networks (Chen and Soo, 2018).\nThis previous research has gone into many set-\ntings where humor takes place. Chen and Soo\n(2018) studied audience laughter compared to tex-\ntual transcripts in order to identify jokes in con-\nversation, while much work has also gone into us-\n1See the thread (of varied and not safe for work content)\nat this link. We do not endorse these jokes.\n3622\nBody Punchline Score\nMan, I was so tired last night; I had a dream I was a\nmufﬂer...\nand I woke up ex-\nhausted\n276\nI told my teenage niece to go get me a newspaper...\nShe laughed at me, and said, ”Oh uncle you’re so\nold. Just use my phone.”\nSo I slammed her\nphone against the\nwall to kill a spider.\n28315\nTable 1: Example format of the Reddit Jokes dataset\ning and creating datasets like the Pun of the Day\n(Yang et al., 2015), 16000 One-liners (Mihalcea\nand Strapparava, 2005), and even Ted Talks (Chen\nand Soo, 2018).\n3 Data\nWe gathered jokes from a variety of sources, each\ncovering a different type of humor. These datasets\ninclude jokes of multiple sentences (the Short\nJokes dataset), jokes with only one sentence (the\nPuns dataset), and more mixed jokes (the Reddit\ndataset). We have made our code and datasets\nopen source for others to use. 2\n3.1 Reddit\nOur Reddit data was gathered using Reddit’s pub-\nlic API, collecting the most recent jokes. Every\ntime the scraper ran, it also updated the upvote\nscore of the previously gathered jokes. This data\ncollection occurred every hour through the months\nof March and April 2019. Since the data was al-\nready split into body and punchline sections from\nReddit, we created separate datasets containing the\nbody of the joke exclusively and the punchline of\nthe joke exclusively. Additionally, we created a\ndataset that combined the body and punchline to-\ngether.\nSome sample jokes are shown in Table 1, above.\nThe distribution of joke scores varies wildly, rang-\ning from 0 to 136,354 upvotes. We found that\nthere is a major jump between the 0-200 upvote\nrange and the 200 range and onwards, with only\n6% of jokes scoring between 200-20,000. We used\nthis natural divide as the cutoff to decide what\nqualiﬁed as a funny joke, giving us 13884 not-\nfunny jokes and 2025 funny jokes.\n3.2 Short Jokes\nThe Short Jokes dataset, found on Kaggle, con-\ntains 231,657 short jokes scraped from various\njoke websites with lengths ranging from 10 to 200\n2Our code and datasets are publicly available at this link.\ncharacters. The previous work by Chen and Soo\n(2018) combined this dataset with the WMT162\nEnglish news crawl. Although their exact com-\nbined dataset is not publicly available, we used\nthe same method and news crawl source to cre-\nate a similar dataset. We built this new Short Jokes\ndataset by extracting sentences from the WMT162\nnews crawl that had the same distribution of words\nand characters as the jokes in the Short Jokes\ndataset on Kaggle 3. This was in order to match\nthe two halves (jokes and non-jokes) as closely as\npossible.\n3.3 Pun of the Day\nThis dataset was scraped by Yang et al. (2015) and\ncontains 16001 puns and 16002 not-punny sen-\ntences. We gratefully acknowledge their help in\nputting together and giving us use of this dataset.\nThese puns were constructed from the Pun of\nthe Day website while the negative samples were\ngathered from news websites.\n4 Methods\nIn this section we will discuss the methods and\nmodel used in our experiments.\n4.1 Our Model\nWe have chosen to use the pre-trained BERT (De-\nvlin et al., 2018) as the base of our model. BERT\nis a multi-layer bidirectional Transformer encoder\nand was initially trained on a 3.3 billion word cor-\npus. The model can be ﬁned-tuned with another\nadditional output layer for a multitude of other\ntasks. We chose to use this Transformer based\nmodel as our initial platform because of its success\nat recognizing and attending to the most important\nwords in both sentence and paragraph structures.\nIn Figure 1, originally designed by Vaswani\net al. (2017), we see the architecture of a Trans-\nformer model: the initial input goes up through\nan encoder, which has two parts: a multi-headed\n3The Short Jokes dataset from Kaggle is available here.\n3623\nFigure 1: Transformer Model Architecture\nself attention layer, followed by a feed-forward\nnetwork. It then outputs the information into the\ndecoder, which includes the previously mentioned\nlayers, plus an additional masked attention step.\nAfterwords, it is transformed through a softmax\ninto the output. This model’s success is in large\npart due to the Transformer’s self-attention layers.\nWe chose a learning rate of 2e-05 and a max\nsequence length of 128. We trained the model for a\nmaximum of 7 epochs, creating checkpoints along\nthe way.\n4.2 Training\nSince our data was unbalanced we decided to up-\nsample the humorous jokes in training. We split\nthe dataset into a 75/25 percent split, stratifying\nwith the labels. We then upsampled the minority\nclass in the training set until it reached an even 50\npercent. This helped our model learn in a more\nbalanced way despite the uneven amount of non-\nhumorous jokes. Our validation and test sets were\ncomposed of the remaining 25%, downsampling\nthe data into a 50/50 class split so that the accuracy\nmetric could be balanced and easily understood.\nTo show how our model compares to the pre-\nvious work done, we also test on the Short Joke\nand Pun datasets mentioned in the Data section.\nFor these datasets we will use the metrics (Accu-\nracy, Precision, Recall, and F1 Score) designated\nin Chen and Soo (2018) as a comparison. We use\nMethod Body Punchline Full\nCNN 0.651 0.684 0.688\nTransformer 0.661 0.692 0.724\nHuman (General) 0.493 0.592 0.663\nTable 2: Results of Accuracy on Reddit Jokes dataset\nthe same model format as previously mentioned,\ntrained on the Reddit dataset. We then immedi-\nately apply the model to predict on the Short Joke\nand Puns dataset, without further ﬁne-tuning, in\norder to compare the model. However, because\nboth the Puns and Short Joke datasets have large\nand balanced labels, we do so without the upsam-\npling and downsampling steps used for the Reddit\ndataset.\n5 Experiments\nIn this section we will introduce the baselines and\nmodels used in our experiments.\n5.1 Baselines\nIn order to have fair baselines, we used the fol-\nlowing two models: a CNN with Highway Lay-\ners as described by Chen and Soo (2018) and de-\nveloped by Srivastava et al. (2015), and human\nperformance from a study on Amazon’s Mechan-\nical Turk. We wanted to have the general pop-\nulation rate these same jokes, thus showing the\ndifference between a general audience and a spe-\nciﬁc subset of the population, in particular, Red-\ndit r/Jokes users. Since the Reddit users obvi-\nously found these jokes humorous, this experiment\nwould show whether or not a more general popu-\nlation agreed with those labels.\nWe had 199 unique participants rate an average\nof 30 jokes each with the prompt ”do you ﬁnd this\njoke humorous?” If the participant was evaluating\na sample from a body or punchline only dataset we\nprefaced our question with a sentence explaining\nthat context, for example: ”Below is the punch-\nline of a joke. Based on this punchline, do you\nthink you would ﬁnd this joke humorous?” Taking\nthese labels, we used the most frequently chosen\ntag from a majority vote to calculate the percent-\nages found in the Human section of Table 2.\n5.2 Results\nIn Table 2, we see the results of our experiment\nwith the Reddit dataset. We ran our models on\n3624\nPrevious Work: Accuracy Precision Recall F1\nWord2Vec+HCF 0.797 0.776 0.836 0.705\nCNN 0.867 0.880 0.859 0.869\nCNN+F 0.892 0.886 0.907 0.896\nCNN+HN 0.892 0.889 0.903 0.896\nCNN+F+HN 0.894 0.866 0.940 0.901\nOur Methods: Accuracy Precision Recall F1\nTransformer 0.930 0.930 0.931 0.931\nTable 3: Comparison of Methods on Pun of the Day Dataset. HCF represents Human Centric Features, F for\nincreasing the number of ﬁlters, and HN for the use of highway layers in the model. See (Chen and Soo, 2018;\nYang et al., 2015) for more details regarding these acronyms.\nthe body of the joke exclusively, the punchline ex-\nclusively, and both parts together (labeled full in\nour table). On the full dataset we found that the\nTransformer achieved an accuracy of 72.4 percent\non the hold out test set, while the CNN was in the\nhigh 60’s. We also note that the general human\nclassiﬁcation found 66.3% of the jokes to be hu-\nmorous.\nIn order to understand what may be happening\nin the model, we used the body and punchline only\ndatasets to see what part of the joke was most im-\nportant for humor. We found that all of the models,\nincluding humans, relied more on the punchline\nof the joke in their predictions (Table 2). Thus,\nit seems that although both parts of the joke are\nneeded for it to be humorous, the punchline car-\nries higher weight than the body. We hypothesize\nthat this is due to the variations found in the dif-\nferent joke bodies: some take paragraphs to set up\nthe joke, while others are less than a sentence.\nOur experiment with the Short Jokes dataset\nfound the Transformer model’s accuracy and F1\nscore to be 0.986. This was a jump of 8 percent\nfrom the most recent work done with CNNs (Ta-\nble 4).\nThe results on the Pun of the Day dataset are\nshown in Table 3 above. It shows an accuracy\nof 93 percent, close to 4 percent greater accuracy\nthan the best CNN model proposed. Although the\nCNN model used a variety of techniques to extract\nthe best features from the dataset, we see that the\nself-attention layers found even greater success in\npulling out the crucial features.\n6 Discussion\nConsidering that a joke’s humor value is subjec-\ntive, the results on the Reddit dataset are surpris-\nMethod Accuracy Precision Recall F1\nCNN+F+HN 0.906 0.902 0.946 0.924\nTransformer 0.986 0.986 0.986 0.986\nTable 4: Results on Short Jokes Identiﬁcation\ning. The model has used the context of the words\nto determine, with high probability, what an av-\nerage Reddit r/Jokes viewer will ﬁnd humorous.\nWhen we look at the general population’s opinion\nas well, we ﬁnd a stark difference between their\npreferences and those of the Reddit users (Table\n2). We would hypothesize that our model is learn-\ning the speciﬁc type of humor enjoyed by those\nwho use the Reddit r/Jokes forum. This would\nsuggest that humor can be learned for a speciﬁc\nsubset of the population.\nThe model’s high accuracy and F1 scores on the\nShort Jokes and Pun of the Day dataset show the\neffectiveness of the model for transfer learning.\nThis result is not terribly surprising. If the model\ncan ﬁgure out which jokes are funny, it seems to be\nan easier task to tell when something isn’t a joke\nat all.\nAlthough these results have high potential,\ndeﬁning the absolute truth value for a joke’s humor\nis a challenging, if not impossible task. However,\nthese results indicate that, at least for a subset of\nthe population, we can ﬁnd and identify jokes that\nwill be most humorous to them.\n7 Conclusion\nIn this paper, we showed a method to deﬁne the\nmeasure of a joke’s humor. We explored the\nidea of using machine learning tools, speciﬁcally\na Transformer neural network architecture, to dis-\ncern what jokes are funny and what jokes are not.\nThis proposed model does not require any human\n3625\ninteraction to determine, aside from the text of the\njoke itself, which jokes are humorous. This archi-\ntecture can predict the level of humor for a speciﬁc\naudience to a higher degree than a general audi-\nence consensus. We also showed that this model\nhas increased capability in joke identiﬁcation as\na result, with higher accuracy and F1 scores than\nprevious work on this topic.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural Machine Translation by Jointly\nLearning to Align and Translate. International Con-\nference on Learning Representations.\nHareesh Bahuleyan. 2018. Natural Language Genera-\ntion with Neural Variational Models. arXiv e-prints,\npage arXiv:1808.09012.\nPeng-Yu Chen and V on-Wun Soo. 2018. Humor recog-\nnition using deep learning. Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. North American Chapter of the Asso-\nciation for Computational Linguistics.\nHe He, Nanyun Peng, and Percy Liang. 2019. Pun\ngeneration with surprise. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 1734–1744, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nYann LeCun, Leon Bottou, Y Bengio, and Patrick\nHaffner. 1998. Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE ,\n86:2278 – 2324.\nPeter J. Liu, Mohammad Saleh, Etienne Pot, Ben\nGoodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. 2018. Generating Wikipedia by Summariz-\ning Long Sequences. International Conference on\nLearning Representations.\nMinh-Thang Luong, Hieu Pham, and Christopher D.\nManning. 2015. Effective Approaches to Attention-\nbased Neural Machine Translation. Proceedings of\nthe 2015 Conference on Empirical Methods in Natu-\nral Language Processing, pages pages 1412 – 1421.\nRada Mihalcea and Carlo Strapparava. 2005. Mak-\ning computers laugh: Investigations in automatic\nhumor recognition. In Proceedings of the Confer-\nence on Human Language Technology and Empiri-\ncal Methods in Natural Language Processing , HLT\n’05, pages 531–538, Stroudsburg, PA, USA. Associ-\nation for Computational Linguistics.\nAmruta Purandare and Diane Litman. 2006. Hu-\nmor: Prosody analysis and automatic recognition\nfor f*r*i*e*n*d*s*. Proceedings of the 2006 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 208–215.\nYishay Raz. 2012. Automatic humor classiﬁcation\non twitter. Proceedings of the 2012 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies: Student Research Workshop, pages 66–70.\nRupesh Kumar Srivastava, Klaus Greff, and J ¨urgen\nSchmidhuber. 2015. Highway Networks. arXiv e-\nprints, page arXiv:1505.00387.\nJulia M. Taylor and Lawrence J. Mazlack. 2004. Com-\nputationally recognizing wordplay in jokes. In Pro-\nceedings of CogSci 2004.\nAlessandro Valitutti, Hannu Toivonen, Antoine\nDoucet, and Jukka M Toivanen. 2013. let every-\nthing turn well in your wife: Generation of adult\nhumor using lexical constraints. In Proceedings\nof the 51st Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short\nPapers), volume 2, pages 243–248.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. 31st Conference on Neural Information\nProcessing Systems, pages 17–21.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,\nAaron Courville, Ruslan Salakhutdinov, Richard\nZemel, and Yoshua Bengio. 2015. Show, Attend and\nTell: Neural Image Caption Generation with Visual\nAttention. arXiv e-prints, page arXiv:1502.03044.\nDiyi Yang, Alon Lavie, Chris Dyer, and Eduard Hovy.\n2015. Humor recognition and humor anchor extrac-\ntion. Proceedings of the 2015 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 2367–2376.\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,\nAlexander J. Smola, and Eduard H. Hovy. 2016. Hi-\nerarchical attention networks for document classiﬁ-\ncation. HLT-NAACL.\nRenxian Zhang and Naishi Liu. 2014. Recognizing hu-\nmor on twitter. In Proceedings of the 23rd ACM In-\nternational Conference on Conference on Informa-\ntion and Knowledge Management, CIKM ’14, pages\n889–898, New York, NY , USA. ACM.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6463316082954407
    },
    {
      "name": "Computer science",
      "score": 0.5969119071960449
    },
    {
      "name": "Natural language processing",
      "score": 0.3330548405647278
    },
    {
      "name": "Engineering",
      "score": 0.22487956285476685
    },
    {
      "name": "Electrical engineering",
      "score": 0.18870827555656433
    },
    {
      "name": "Voltage",
      "score": 0.05505180358886719
    }
  ]
}