{
  "title": "polyBERT: a chemical language model to enable fully machine-driven ultrafast polymer informatics",
  "url": "https://openalex.org/W4383955629",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3096356045",
      "name": "Christopher Kuenneth",
      "affiliations": [
        "University of Bayreuth",
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2443284501",
      "name": "Rampi Ramprasad",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3096356045",
      "name": "Christopher Kuenneth",
      "affiliations": [
        "Georgia Institute of Technology",
        "University of Bayreuth"
      ]
    },
    {
      "id": "https://openalex.org/A2443284501",
      "name": "Rampi Ramprasad",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3100445615",
    "https://openalex.org/W3096386116",
    "https://openalex.org/W2754122850",
    "https://openalex.org/W2050445699",
    "https://openalex.org/W3142241411",
    "https://openalex.org/W3095106934",
    "https://openalex.org/W3095028680",
    "https://openalex.org/W3165831121",
    "https://openalex.org/W2984886967",
    "https://openalex.org/W4310806566",
    "https://openalex.org/W3024556032",
    "https://openalex.org/W3091459767",
    "https://openalex.org/W3199836559",
    "https://openalex.org/W3197690017",
    "https://openalex.org/W3112510144",
    "https://openalex.org/W2952832141",
    "https://openalex.org/W2315837940",
    "https://openalex.org/W1988037271",
    "https://openalex.org/W2415372084",
    "https://openalex.org/W1513260206",
    "https://openalex.org/W2791355014",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3093934881",
    "https://openalex.org/W2973114758",
    "https://openalex.org/W4206252531",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2947423323",
    "https://openalex.org/W3010145447",
    "https://openalex.org/W4297630484",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2606780347",
    "https://openalex.org/W4210261876",
    "https://openalex.org/W4320915618",
    "https://openalex.org/W4293232267",
    "https://openalex.org/W4213077304",
    "https://openalex.org/W4214868967",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W2022476850",
    "https://openalex.org/W2904181725",
    "https://openalex.org/W2951450280",
    "https://openalex.org/W2883528235",
    "https://openalex.org/W2978032524",
    "https://openalex.org/W3026577685",
    "https://openalex.org/W2976224320",
    "https://openalex.org/W3002410303",
    "https://openalex.org/W4297632148",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W4289677850",
    "https://openalex.org/W4288253152",
    "https://openalex.org/W2786672974",
    "https://openalex.org/W3176390156",
    "https://openalex.org/W4287642541",
    "https://openalex.org/W4288090629",
    "https://openalex.org/W6931921727",
    "https://openalex.org/W3114168740",
    "https://openalex.org/W3103092523",
    "https://openalex.org/W3155755963",
    "https://openalex.org/W4383955629"
  ],
  "abstract": "Abstract Polymers are a vital part of everyday life. Their chemical universe is so large that it presents unprecedented opportunities as well as significant challenges to identify suitable application-specific candidates. We present a complete end-to-end machine-driven polymer informatics pipeline that can search this space for suitable candidates at unprecedented speed and accuracy. This pipeline includes a polymer chemical fingerprinting capability called polyBERT (inspired by Natural Language Processing concepts), and a multitask learning approach that maps the polyBERT fingerprints to a host of properties. polyBERT is a chemical linguist that treats the chemical structure of polymers as a chemical language. The present approach outstrips the best presently available concepts for polymer property prediction based on handcrafted fingerprint schemes in speed by two orders of magnitude while preserving accuracy, thus making it a strong candidate for deployment in scalable architectures including cloud infrastructures.",
  "full_text": "Article https://doi.org/10.1038/s41467-023-39868-6\npolyBERT: a chemical language model\nto enable fully machine-driven ultrafast\npolymer informatics\nChristopher Kuenneth1,2 &R a m p iR a m p r a s a d1\nPolymers are a vital part of everyday life. Their chemical universe is so large\nthat it presents unprecedented opportunities as well as signiﬁcant challenges\nto identify suitable application-speciﬁc candidates. We present a complete\nend-to-end machine-driven polymer informatics pipeline that can search this\nspace for suitable candidates at unprecedented speed and accuracy. This\npipeline includes a polymer chemicalﬁngerprinting capability called polyBERT\n(inspired by Natural Language Processing concepts), and a multitask learning\napproach that maps the polyBERTﬁngerprints to a host of properties. poly-\nBERT is a chemical linguist that treats the chemical structure of polymers as a\nchemical language. The present approachoutstrips the best presently avail-\nable concepts for polymer propertyprediction based on handcraftedﬁnger-\nprint schemes in speed by two orders of magnitude while preserving accuracy,\nthus making it a strong candidate for deployment in scalable architectures\nincluding cloud infrastructures.\nPolymers are an integral part of our everyday life and instrumental in\nthe progress of technologies for future innovations1.T h es h e e rm a g -\nnitude and diversity of the polymer chemical space provide opportu-\nnities for crafting polymers that accurately match application\ndemands, yet also come with the challenge of efﬁciently and effectively\nbrowsing this gigantic space. The nascent ﬁeld of polymer\ninformatics\n2–5 allows access to the depth of the polymer universe and\ndemonstrates the potency of machine learning (ML) models to over-\ncome this challenge. ML frameworks have enabled substantial pro-\ngress in the development of polymer property predictors\n6–10 and\nsolving inverse problems in which polymers that meet speciﬁcp r o p -\nerty requirements are either identiﬁed from candidate sets11,12,o ra r e\nfreshly designed using genetic13,14 or generative15–17 algorithms.\nAn essential step in polymer informatics pipelines is the conver-\nsion of polymer chemical structures to numerical representations that\nare often calledﬁngerprints, features, or descriptors (see blue boxes in\nFig. 1a). Past handcraftedﬁngerprinting approaches\n18–22 utilize che-\nminformatics tools that numerically encode key chemical and struc-\ntural features of polymers. Although such handcraftedﬁngerprints\nbuild on invaluable intuition and experience, they are tedious to\ndevelop, involve complex computations that often consume most of\nthe time during model training and inference, and lack generalization\nto all polymer chemical classes (i.e., new features may have to be\nadded to the catalog of features in an ad hoc manner). ML pipelines\nthat use handcraftedﬁngerprints are thus prone to errors during the\nexploration of new polymer chemical classes. Also, handcraftedﬁn-\ngerprints present barriers for the development and deployment of\nfully machine-driven ipipelines, which are suited for scalability in cloud\ncomputing and high-throughput environments.\nOne way to overcome the previously mentioned limitations is to\nreplace handcraftedﬁngerprints with fully machine-crafted“Trans-\nformer” ﬁngerprints (see right pipeline of Fig.1a). Transformers\n23 were\nrecently developed in theﬁeld of Natural Language Processing (NLP)\nand have swiftly become the gold standard in ML language modeling.\nIn this work, we envision Simpliﬁed Molecular-Input Line-Entry System\n(SMILES)\n24 strings that have been used to represent polymers as the\n“chemical language” of polymers. We use millions of Polymer SMILES\n(PSMILES) strings for training a language model called polyBERT to\nbecome an expert— a linguist— of the polymer chemical language. In\ncombination with multitask deep neural networks\n6,7,p o l y B E R Te n a b l e s\nReceived: 24 October 2022\nAccepted: 28 June 2023\nCheck for updates\n1School of Materials Science and Engineering, Georgia Institute of Technology, Atlanta, GA 30332, USA.2Faculty of Engineering Science, University of\nBayreuth, 95447 Bayreuth, Germany. e-mail: rampi.ramprasad@mse.gatech.edu\nNature Communications|         (2023) 14:4099 1\n1234567890():,;\n1234567890():,;\na fully end-to-end machine-driven polymer informatics pipeline that\nuses and unleashes the true power of artiﬁcial intelligence methods.\nMultitask deep neural networks harness inherent correlations in multi-\nﬁdelity and multi-property data sets, scale effortlessly in cloud com-\nputing environments, and generalize to multiple prediction tasks.\nRecent studies\n25–27 demonstrated the beneﬁts of using Transfor-\nmers in the molecule chemical space. For example, Wang et al.26 have\ntrained a BERT28 model (the most common general language model)\nwith a data set of molecule SMILES strings. Using BERT’s latent space\nrepresentations of molecules asﬁngerprints, the authors show that\ntheir approach outperforms otherﬁngerprinting methods (including\nﬁngerprints of an unsupervised recurrent neural network and a graph\nneural network). Similarly, Schwaller et al.29,30 have developed a\nTransformer model to predict retrosynthesis pathways of molecules\nfrom reactants and reagents that outperforms known algorithms in the\nreaction prediction literature. A very recent study by Xu et al.\n31 (per-\nformed almost at the same time as us, as can be conﬁrmed by both our\narXiv submissions) used a RoBERTa32 model (an evolution of the BERT\nTransformer model) for polymer property predictions. Their training\nstrategy ﬁrst involves the pretraining (unsupervised training) of the\nRoBERTa\n32 model using 5 million polymers and then aﬁnetuning step\n(supervised training) to directly predict polymer properties. Although\ntheir work uses much smaller datasets than ours both for unsupervised\nand supervised training tasks, theyﬁnd that theirﬁnetuned RoBERTa\nmodel outperforms graph neural networks, long short-term memory\nand other models; we do note that this recent work does not make\ndirect comparisons of their Transformer-based model with the current\nstate-of-the-art in hand-craftedﬁngerprinting and multi-task learning\n7\n(which we do in the present contribution).\nAnother promising neural network architecture, namely, graph\nneural networks33, which treats chemical structures as graphs, has\nbeen applied to the molecule and polymer chemical space in the past.\nIn contrast to Transformers, graph neural networks represent atoms as\nnodes and bonds as edges of a graph, so as to encode immediate and\nextended connectivities between atoms. As a consequence, graph\nneural networks are not directly based on PSMILES strings like Trans-\nformers, but depend on an initial set of feature vectors (such as atom\ntypes, implicit valence, etc.) that need to be computed for and\nassigned to each node. For example, Park et al.\n34 compared predictions\nof a graph convolutional network and the popular extended-\nconnectivity circularﬁngerprint\n19 for thermal and mechanical poly-\nmer properties, ﬁnding a similar prediction performance for both\nmodels. Similarly, Gurnani et al.35 used multitask graph neural net-\nworks to predict polymer properties, but introduced edges between\nthe heavy boundary atoms to incorporate the recurrent topology of\npolymer chains. Their combined approach of graph neural networks\nand multitask learning outperforms predictions based on the con-\nventional handcrafted Polymer Genomeﬁngerprint\n8,20 in almost all\ncases. In a similar manner, Aldeghi and Coley36 introduced low-weight\nedges between polymer chains to enable predictions for alternating,\nrandom, and block copolymers, and termini chemical groups. We also\nnote that unlike Transformers graph neural networks are usually\ntrained end-to-end, i.e., their latent space representations (ﬁnger-\nprints) are learned under supervision with polymer properties. The\nFig. 1 | Polymer informatics with polyBERT. aPrediction pipelines. The left\npipeline shows the prediction using handcraftedﬁngerprints using cheminfor-\nmatics tools, while the right pipeline (present work) portrays a fully end-to-end\nmachine-driven predictor using polyBERT. Property symbols are deﬁned in Table1.\nID1 and ID3 are copolymers, and ID2 is a homopolymer.c\n1 and c2 are the fractions of\nthe ﬁrst and second comonomer in the polymer. The symbolsTg, Tm, Td, E, ϵb,a n d\nσb stand for glass transition temperature, melting temperature, degradation tem-\nperature, Young’s modulus, elongation at break, and tensile strength at break,\nrespectively.b polyBERT is a polymer chemical language linguist. polyBERT\ncanonicalizes, tokenizes, and masks Polymer Simpliﬁed Molecular-Input Line-Entry\nSystem (PSMILES) strings before passing them to the DeBERTa model. Each of the\n12 Transfomer encoders has 12 attention heads. A last dense layer with a softmax\nactivation functionﬁnds the masked tokens. polyBERTﬁngerprints (dashed arrow)\nare the averages over the token dimension (sentence average) of the last Trans-\nformer encoder.c 100 million hypothetical PSMILES strings. First, 13 766 known\n(i.e., previously synthesized) polymers are decomposed to 4424 fragments using\nthe Breaking Retrosynthetically Interesting Chemical Substructures (BRICS)\n40\nmethod. Second, re-assembling the BRICS fragments in many different ways gen-\nerates 100 million hypothetical polymers by randomly and enumeratively com-\nbining the fragments.\nArticle https://doi.org/10.1038/s41467-023-39868-6\nNature Communications|         (2023) 14:4099 2\nconsequence of this is that in the case of Transformer-based approa-\nches, the learnedﬁngerprint is independent of the polymer properties\n(and so can be determined once and for all), where as graph neural\nnetwork architectures are typically constructed such that the learned\nrepresentations depend on the speciﬁc property under consideration.\nWe note that self-supervised graph neural networks\n37,38 have recently\nbeen developed that learn the molecule graph through atom, bond,\nand subgraph masking, an approach similar to Transformers.\nThis work has several critical ingredients. First, we generate a data\nset of 100 million hypothetical polymers by enumeratively combining\nchemical fragments extracted from a list of more than 13 000 synthe-\nsized polymers. Next, we train polyBERT, a DeBERTa\n39-based encoder-\nonly Transformer, using this hypothetical polymer data set to become\na polymer chemical linguist. During training, polyBERT learns to\ntranslate input PSMILES strings to numerical representations that we\nuse as polymerﬁngerprints. Finally, we map the polyBERTﬁngerprints\nto about 3 dozen polymer properties using our multitask ML frame-\nwork to yield fully machine-driven ultrafast polymer property pre-\ndictors. For benchmarking, the performance (both accuracy and\nspeed) of this new end-to-end property prediction pipeline is com-\npared with the state-of-the-art handcrafted Polymer Genome\n8 (PG)\nﬁngerprint based pipeline pioneered previously. Using the ultrafast\npolyBERT polymer informatics pipeline, we are in a position to predict\nthe properties of the 100 million hypothetical polymers intending to\nﬁnd property boundaries of the polymer universe. This work con-\ntributes to expediting the discovery, design, development, and\ndeployment of polymers by harnessing the true power of language,\ndata, and artiﬁcial intelligence models.\nResults\nData sets\nFigure 1c sketches the two-step process for fabricating 100 million\nhypothetical PSMILES strings. We use the Breaking Retrosynthetically\nInteresting Chemical Substructures (BRICS)\n40 method (as imple-\nmented in RDKit41) to decompose previously synthesized 13,766\npolymers (all monomers of the data set outline in Table1, see below)\ninto 4424 unique chemical fragments. Random and enumerative\ncompositions of these fragments yield 100 million hypothetical\nPSMILES strings that weﬁrst canonicalize (see“Methods” section) and\nthen use for training polyBERT. The hypothetical PSMILES strings are\nchemically valid polymers but, mostly, have never been synthesized\nbefore.\nOnce polyBERT has completed its unsupervised learning task\nusing the 100 million hypothetical PSMILES strings, multitask super-\nvised learning maps polyBERT polymerﬁngerprints to multiple prop-\nerties to produce property predictors. We use the property data set in\nTable 1 for training the property predictors. The data set contains\n28,061 (≈80%) homopolymer and 7456 (≈20%) copolymer (total of\n35,517) data points of 29 experimental and computational polymer\nproperties that pertain to 11,145 different monomers and 1338 distinct\ncopolymer chemistries, respectively. Each of the 7456 copolymer data\npoints involves two distinct comonomers at various compositions. Our\ncopolymer data points are for random copolymers, which are ade-\nquately handled by our adoptedﬁngerprinting strategy (see“Meth-\nods” section). Alternating copolymers are treated as homopolymers\nwith appropriately deﬁned repeat units forﬁngerprinting purposes.\nOtherﬂavors of copolymers may also be encoded by adding additional\nﬁngerprint components. All data points in the data set have been used\nin past studies\n6,7,11,42–49 and were produced using computational\nmethods or obtained from literature and other public sources. Sup-\nplementary Figs. S3–S8 show histograms for each property.\npolyBERT\npolyBERT iteratively ingests 100 million hypothetical PSMILES strings\nto learn the polymer chemical language, as sketched in Fig.1b. Using\n100 million PSMILES strings is the latest example of training a\nchemistry-related language model with a large data set and follows the\ntrend of growing data sets in this discipline, with ChemBERTa using 10\nmillion, SMILES-BERT using 18.7 million, and ChemBERTa-2 using 77\nmillion SMILES strings.\n50 polyBERT is a DeBERTa39 model (as imple-\nmented in Huggingface’s Transformer Python library51) with a sup-\nplementary three-stage preprocessing unit for PSMILES strings. We\nchose the DeBERTa model as the foundation of polyBERT because it\noutperformed other BERT-like models (BERT\n28,R o B E R T a32,a n d\nDistilBERT52) in our tests (see Supplementary Discussion) and stan-\ndardized performance task39. First, polyBERT transforms a input\nPSMILES string into its canonical form (e.g.,[*]CCOCCO[*] to [*]\nCOC[*])u s i n gt h ecanonicalize_psmilesPython package devel-\noped in this work. Details can be found in the Methods section. Sec-\nond, polyBERT tokenizes canonical PSMILES strings using the\nSentencePiece\n53 tokenizer and a total of 265 tokens. The tokens include\ncommon PSMILES characters such as the uppercased and lowercased\n118 elements of the periodic table of elements, numbers ranging from\n0 to 9, and special characters like[*], (, ), =, among others. This\nensures that the tokenizer covers the entire PSMILES strings vocabu-\nlary and is a similar approach to that in ref.50. A full token list can be\nfound at the GitHub repository (see the Data and Code Availability\nsection). Third, polyBERT masks 15% (default parameter for masked\nlanguage models) of the tokens to create a self-supervised training\ntask. In this training task, polyBERT is taught to predict the masked\ntokens using the non-masked surrounding tokens by adjusting the\nweights of the Transformer encoders (ﬁll-in-the-blanks task). We use\n80 million PSMILES strings for training and 20 million PSMILES strings\nfor validation. The validation F1-score is > 0.99. This exceptionally\ngood F1-score indicates that polyBERTﬁnds the masked tokens in\nalmost all cases. The total CO\n2 emissions for training polyBERT on our\nhardware are estimated to be 12.6 kgCO2eq (see CO2 Emission and\nTiming section).\nT h et r a i n i n gw i t h8 0m i l l i o nP S M I L E Ss t r i n g sr e n d e r sp o l y B E R Ta n\nexpert polymer chemical linguist who knows grammatical and syn-\ntactical rules of the polymer chemical language. polyBERT learns pat-\nterns and relations of tokens via the multi-head self-attention\nmechanism and fully connected feed-forward network of the Trans-\nformer encoders\n23. The attention mechanism instructs polyBERT to\ndevote more focus to a small but essential part of a PSMILES string.\npolyBERT’s learned latent spaces after each encoder block are\nnumerical representations of the input PSMILES strings. The polyBERT\nﬁngerprint is the average over the token dimension (sentence average)\nof the last latent space (dotted line in Fig.1b). We use the Python\npackage SentenceTransformers\n54 for extracting and computing poly-\nBERT ﬁngerprints.\nFingerprints\nFor acquiring analogies and juxtaposing chemical relevancy, we com-\npare polyBERTﬁngerprints with the handcrafted Polymer Genome8\n(PG) ﬁngerprints that numerically encode polymers at three different\nlength scales. A description of PG ﬁngerprints can be found in\n“Methods” section. The PGﬁngerprint vector for the data set in this\nwork has 945 components and is sparsely populated (93.9% zeros). The\nreason for this ultra sparsity is that many PGﬁngerprint components\ncount chemical groups in polymers\n8.A ﬁngerprint component of zero\nindicates that a chemical group is not present. In contrast, polyBERT\nﬁngerprint vectors have 600 components and are fully dense (0%\nzeros). Fully dense and lower-dimensional ﬁngerprints are often\nadvantageous for ML models whose computation time scales super-\nlinear (Oðn\nsÞ,s>1) with the data set size (n)s u c ha sG a u s s i a np r o c e s so r\nkernel ridge techniques. Moreover, in the case of neural networks,\nsparse and high-dimensional input vectors can cause unnecessary high\nmemory load that reduces training and inference speed. We note that\nthe dimensionality of polyBERTﬁngerprints is a parameter that can be\nArticle https://doi.org/10.1038/s41467-023-39868-6\nNature Communications|         (2023) 14:4099 3\nchosen arbitrarily to yield the best training result. A summary of the\nkey ﬁgures can be found in Supplementary Table S4.\nFigure 2 shows Uniform Manifold Approximation and Projection\n(UMAP)55 plots for all homo- and copolymer chemistries in Table1.T h e\ncolored triangles in theﬁrst column indicate the coordinates of three\nselected polymers for polyBERT and PGﬁngerprints. We observe for\nbothﬁngerprint types that the orange and blue triangles are very close,\nwhile the green triangle is separate. We also note that polymers cor-\nresponding to the orange and blue triangles, namely poly(but-1-ene)\nand poly(pent-1-ene), have similar chemistry (different by only one\ncarbon atom), but poly(4-vinylpyridine) represented by a green trian-\ngle, is different. This chemically intuitive positioning ofﬁngerprints\nsuggests the chemical relevancy ofﬁngerprint distances. The cosine\nﬁngerprint distances reported in Supplementary Fig. S1 allow for the\nsame conclusion.\nThe second, third, and fourth columns of Fig.2 display the same\nUMAP plots as in theﬁrst column. Colored dots indicate the property\nvalues ofT\ng, Td,a n dEgc, while light gray dots show polymerﬁnger-\nprints with unknown property values. We observe localized clusters of\nsimilar color in each plot pertaining to polymers of similar properties.\nAlthough thisﬁnding is not surprising for the PGﬁngerprint because it\nrelies on handcrafted chemical features that purposely position similar\npolymers next to each other, it is remarkable for polyBERT. With no\nchemical information and purely based on training on a massive\namount of PSMILES strings, polyBERT has learned polymerﬁnger-\nprints that match chemical intuition. This again shows that polyBERT\nﬁngerprints have chemical pertinence and their distances measure\npolymer similarity (e.g., using the cosine distance metric).\npolyBERT learns chemical motifs and relations in the PSMILES\nstrings using the Transformer encoders, each of which includes an\nTable 1 | Training data set for the property predictors. The properties are sorted into categories, showed at the top of each\nblock. The data set contains 29 properties (dielectric constantskf are available at 9 different frequenciesf). HP and CP stand\nfor homopolymer and copolymer, respectively\nProperty Symbol Unit Source a Data range Data points\nHP CP All\nThermal\nGlass transition temp. Tg K Exp. [8e+01, 9e+02] 5183 3312 8495\nMelting temp. Tm K Exp. [2e+02, 9e+02] 2132 1523 3655\nDegradation temp. Td K Exp. [3e+02, 1e+03] 3584 1064 4648\nThermodynamic & physical\nHeat capacity cp Jg−1K−1 Exp. [8e-01, 2e+00] 79 79\nAtomization energy Eat eV atom−1 DFT [-7e+00, -5e+00] 390 390\nLimiting oxygen index Oi % Exp. [1e+01, 7e+01] 101 101\nCrystallization tendency (DFT) Xc % DFT [1e-01, 1e+02] 432 432\nCrystallization tendency (exp.) Xe % Exp. [1e+00, 1e+02] 111 111\nDensity ρ gc m−3 Exp. [8e-01, 2e+00] 910 910\nElectronic\nBand gap (chain) Egc eV DFT [2e-02, 1e+01] 4224 4224\nBand gap (bulk) Egb eV DFT [4e-01, 1e+01] 597 597\nElectron afﬁnity Eea eV DFT [4e-01, 5e+00] 368 368\nIonization energy Ei eV DFT [4e+00, 1e+01] 370 370\nElectronic injection barrier Eib eV DFT [2e+00, 7e+00] 2610 2610\nCohesive energy density δ cal cm−3 Exp. [2e+01, 3e+02] 294 294\nOptical & dielectric\nRefractive index (DFT) nc DFT [1e+00, 3e+00] 382 382\nRefractive index (exp.) ne Exp. [1e+00, 2e+00] 516 516\nDielec. constant (DFT) kc DFT [3e+00, 9e+00] 382 382\nDielec. constant at freq.fb kf Exp. [2e+00, 1e+01] 1187 1187\nMechanical\nYoung’sm o d u l u s E MPa Exp. [2e-02, 4e+03] 592 322 914\nTensile strength at yield σy MPa Exp. [3e-05, 1e+02] 216 78 294\nTensile strength at break σb MPa Exp. [5e-03, 2e+02] 663 318 981\nElongation at break ϵb Exp. [3e-01, 1e+03] 868 260 1128\nPermeability\nO2 gas permeability μO2\nbarrer Exp. [5e-06, 1e+03] 390 210 600\nCO2 gas permeability μCO2\nbarrer Exp. [1e-06, 5e+03] 286 119 405\nN2 gas permeability μN2\nbarrer Exp. [3e-05, 5e+02] 384 99 483\nH2 gas permeability μH2\nbarrer Exp. [2e-02, 5e+03] 240 46 286\nHe gas permeability μHe barrer Exp. [5e-02, 2e+03] 239 58 297\nCH4 gas permeability μCH4\nbarrer Exp. [4e-04, 2e+03] 331 47 378\n28,061 7456 35,517\naExperiments (Exp.); density functional theory (DFT).\nbf 2 1:78,2,3,4,5,6,7,9,15fg is the log10(frequency in Hz); e.g.,k3 is the dielectric constant at a frequency of 1 kHz.\nArticle https://doi.org/10.1038/s41467-023-39868-6\nNature Communications|         (2023) 14:4099 4\nattention and feed-forward network layer (see Fig.1b). Figure3a–c\ndisplays the normalized attention maps summed over all 12 attention\nheads and 12 encoders of polyBERT for the same PSMILES strings as\nin Fig.2. Large dots indicate high attention scores, while small dots\nshow weak attention scores. The attention scores can be interpreted\nas the importance of knowing the position and type of another token\n(or chemical motif) and its impact on the current token’s latent\nspace. The[CLS], _,a n d[SEP] tokens are auxiliary tokens. Theﬁrst\ntwo tokens indicate the beginning of PSMILES strings and the last\ntoken shows the end of PSMILES strings. We notice high attention\nscores for the[CLS], _,a n dﬁrst [*] tokens in all panels a to c that\nimply the connection of the auxiliary tokens to the beginning of\nPSMILES strings. Also, we observe at least intermediate attention\nscores to next and next-to-next neighbors (ﬁrst and second off-\nFig. 2 | Two-dimensional Uniform Manifold Approximation and Projection55\n(UMAP) plots of theﬁngerprints.Panel a shows polyBERT and\npanel b shows Polymer Genomeﬁngerprints for all homo- and copolymer che-\nmistries in Table1. The triangles (blue, orange, and green) in theﬁrst column\nindicate ﬁngerprint positions in the UMAP spaces of three selected polymers. The\ncolored dots in columns two, three, and four indicate property values ofTg, Td,a n d\nEgc, which stand for the glass transition temperature, degradation temperature, and\nband gap (chain), respectively. Light gray dots show polymers with unknown\nproperty values. The Polymer Simpliﬁed Molecular-Input Line-Entry System\n(PSMILES) strings[*]CC([*])CC, [*]CC([*])CCC,a n d[*]CC([*])c1ccncc1\ndenote poly(but-1-ene), poly(pent-1-ene), and poly(4-vinylpyridine), respectively.\nFig. 3 | Attention maps and neuron activation for three polymers.Panels a–c\nshow the normalized attention maps summed over all 12 attention heads and 12\nencoders of polyBERT. Panelsd–f show the factorized neurons activations in the\nfeed-forward network layers\n56. The Polymer Simpliﬁed Molecular-Input Line-Entry\nSystem (PSMILES) strings[*]CC([*])CC, [*]CC([*])CCC,a n d[*]CC([*])\nc1ccncc1 denote poly(but-1-ene), poly(pent-1-ene), and poly(4-vinylpyridine),\nrespectively.\nArticle https://doi.org/10.1038/s41467-023-39868-6\nNature Communications|         (2023) 14:4099 5\ndiagonal elements) for all tokens highlighting the importance of\nclosely bonded neighbors for the polyBERT ﬁngerprint. Another\ngeneral trend is large attention scores between the second[*]\ntokens and multiple neighbor tokens across all panels. Moreover, in\nFig. 3c, we ﬁnd large attention scores for thecn token up to the\nfourth orﬁfth neighbor tokens that indicate a strong impact ofcn to\nthe latent spaces and polyBERTﬁngerprint, which is expected due to\nthe different nature of the nitrogen atom.\nFigure 3d–f shows the non-negative matrix factorizations (4\ncomponents) of the neuron activations in the feed-forward neural\nnetwork layers\n56 of polyBERT (see Fig.1b) for the same polymers as in\npanels a to c. The neurons in the feed-forward network layers account\nfor more than 60% of the parameters. Each of the four components\nrepresent a set of distinct neurons that is active for speciﬁct o k e n s( x -\naxes). For example, the fourth set of neurons is active if polyBERT\npredicts latent spaces for the auxiliary tokens. The third set of neurons\nﬁre in the case of theﬁrst twoC tokens and theﬁrst set of neurons are\nactive for side chainc or C atoms, except in the case of thecn token,\nwhich has its own set of neurons (second set of neurons). In total, the\nattention layers incorporate positional and relational knowledge and\nthe feed-forward neural network layers disable and enable certain\nroutes through polyBERT. Both factors modulate the polyBERT\nﬁngerprints.\nNot surprisingly, the computations of polyBERT and PGﬁnger-\nprints scale nearly linearly with the number of PSMILES strings\nalthough their performance (i.e., pre-factor) can be quite different, as\nshown in the log-log scaled Fig.4. The computation of polyBERT (GPU)\nis over two orders of magnitude (215 times) faster than computing PG\nﬁngerprints. polyBERTﬁngerprints may be computed on CPUs and\nGPUs. Because of the presently large efforts in industry to develop\nfaster and better GPUs, we expect the computation of polyBERTﬁn-\ngerprint to become even faster in the future. Time is very important for\nhigh-throughput polymer informatics pipelines that identify polymers\nfrom large candidate sets\n11.W i t ha ne s t i m a t eo f0 . 3 0m s / P S M I L E Sf o r\nthe multitask deep neural networks (see Property Prediction section),\nthe total time using the polyBERT-based pipeline to predict 29 polymer\nproperties sums to 1.06 ms/polymer/GPU.\nProperty prediction\nFor benchmarking the property prediction accuracy of polyBERT and\nPG ﬁngerprints, we train multitask deep neural networks for each\nproperty category deﬁned in Table 1. In our previous study7,w e\nobserved that these property categories resulted in the development of\nmodels exhibiting superior performance. Multitask deep neural net-\nworks have demonstrated best-in-class results for polymer property\npredictions\n6,7,11, while being fast, scalable, and readily amenable if more\ndata points become available. Unlike single-task models, multitask\nmodels simultaneously predict numerous properties (tasks) and har-\nness inherent but hidden correlations in data to improve their perfor-\nmance. Such correlation exists, for instance, betweenT\ng andTm,b u tt h e\nexact correlation varies across speciﬁc polymer chemistries. Multitask\nmodels learn and improve from these varying correlations in data. The\ntraining protocol of the multitask deep neural networksfollows state-\nof-the-art methods involving ﬁve-fold cross-validation and a con-\nsolidating meta learner that forecasts theﬁnal property values based\nupon the ensemble of cross-validation predictors. More details about\nmultitask deep neural networks are provided in the Methods section.\nTheir training process is outlined in Supplementary Figure S2.\nFigure5a shows the coefﬁcient of determination (R\n2)a v e r a g e sa n d\nstandard deviations across theﬁve validation data sets of the cross-\nvalidation process for 29 polymer properties. The averages are inde-\npendent of the data set splits, while the standard deviations show the\nvariance of the prediction performance for the different splits. Smaller\nstandard deviations indicate data sets with homogeneously dis-\ntributed data points in the learning space. Large standard deviations\nstem from inhomogeneously distributed data points of usually smaller\ndata sets. Cross-validation is shown to establish an independence of\nthe data set splits for polymer predictions\n11. Root-Mean-Square Error\n(RMSE) andR2 values for the cross-validation and meta learner models\ncan be found in Supplementary Table S1–S3 for all polymers, homo-\npolymers, and copolymers, respectively. Weﬁnd the prediction accu-\nracy to be better for thermal and mechanical properties of copolymers\n(relative to that for homopolymers) and slightly worse for the gas\npermeabilities, similar to previousﬁndings\n6.O v e r a l l ,P Gp e r f o r m sb e s t\n(R2 = 0.81) but is very closely followed by polyBERT (R2 =0 . 8 0 ) . T h i s\noverall performance order of theﬁngerprint types is persistent with\nthe category averages and properties, except forXc, Xe,a n dϵb,w h e r e\npolyBERT slightly outperforms PGﬁngerprints. We note that polyBERT\nand PGﬁngerprints are both practical routes for polymer featurization\nbecause their R2 values lie close together and are generally high.\npolyBERT ﬁngerprints have the accuracy of the handcrafted PGﬁn-\ngerprints but are over two orders of magnitude faster (see Fig.4).\nFigure5b shows highR2 values for each meta learner (one for each\ncategory), suggesting an exceptional prediction performance across\nall properties. We train the meta learners on unseen 20% of the data set\nand validate using 80% of the data set (also used for cross-validation).\nThe reported validationR\n2 values thus only partly measure the gen-\neralization performance with respect to the full data set. Meta learners\ncan be conceived as taking decisive roles in selecting the best values\nfrom the predictions of theﬁve cross-validation models. We use the\nmeta learners for all property predictions in this work. Supplementary\nFigs. S9–S14 show the meta learners’parity plots.\nThe ultrafast and accurate polyBERT-based polymer informatics\npipeline allows us to predict all 29 properties of the 100 million\nhypothetical polymers that were originally created to train polyBERT.\nFigure5c shows the minimum, mean, and maximum for each property.\nHistograms are given in Supplementary Figs. S15–S20. Given the vast\nsize of our data set and consequent chemical space of the 100 million\nhypothetical polymers, the minimum and maximum values can be\ninterpreted as potential boundaries of the total polymer property\nspace. In addition, a data set of this magnitude presents numerous\nopportunities for obtaining fascinating insights and practical applica-\ntions. For example, it can be utilized in future studies to establish\nstandardized benchmarks for testing and evaluating ML models in the\ndomain of polymer informatics. The data set may also reveal structure-\nproperty information that provides guidance for design rules, helps to\nFig. 4 | Computation times of polymerﬁngerprints.The ﬁngerprints are com-\nputed on one CPU core (Intel(R) Xeon(R) CPU E5-2667), except for polyBERT (GPU)\nﬁngerprints that are computed on one GPU (Quadro GP100). The computation\ntimes per Polymer Simpliﬁed Molecular-Input Line-Entry System (PSMILES) string,\nin the order of the legend, are 33.39, 0.76, and 163.59 ms/PSMILES (computed for\n10\n4 PSMILES), respectively.\nArticle https://doi.org/10.1038/s41467-023-39868-6\nNature Communications|         (2023) 14:4099 6\nidentify unexplored areas to search for new polymers, or facilitates\ndirect selection of polymers with speciﬁc properties through nearest\nneighbor searches, as evidenced in a recent study11.Ap o s s i b l ef u t u r e\nevolution of the data set may also contain subspaces of distinct poly-\nmer classes, such as biodegradable or low-carbon polymer classes.\nHowever, these aspects are beyond the scope of this study. The data\nset with 100 million hypothetical polymers including the predictions of\n29 properties is available for academic use. The total CO\n2 emissions for\npredicting 29 properties of 100 million hypothetical polymers are\nestimated to be 5.5 kgCO\n2eq (see CO2 Emission and Timing section).\nOther advantages of polyBERT: beyond speed and accuracy\nThe feed-forward network (last layer in Fig.1b), which predicts masked\ntokens during the self-supervised training of polyBERT, enables the\nmapping of numerical latent spaces (i.e.,ﬁngerprints) to PSMILES\nstrings. However, because we average over the token dimension of the\nlast latent space to compute polyBERT ﬁngerprints, we cannot\nunambiguously map the currentﬁngerprints back to PSMILES strings.\nAm o d iﬁed future version of polyBERT that provides PSMILES strings\nencoding and ﬁngerprint decoding could involve inserting a\ndimensionality-reducing layer after the last Transformer encoder.\nFingerprint decoders are important elements of design informatics\npipelines that invert the prediction pipeline to meet property speciﬁ-\ncations. We note that the current choice of computing polyBERTﬁn-\ngerprints as pooling averages stems from basic dimensionality\nreduction considerations that require no modiﬁcation of the DeBERTa\narchitecture.\nA second advantage of the polyBERT approach is interpretability.\nAnalyzing the chemical relevancy of polyBERTﬁngerprints (as dis-\ncussed in the Fingerprints section) in greater detail can reveal chemical\nfunctions and interactions of structural parts of the polymers. As illu-\nstrated with the examples of the three polymers in Fig.3, deciphering\nFig. 5 |Coefﬁcient of determination(R2) performance values for polyBERT (PB)\nand Polymer Genome (PG)ﬁngerprints.Panel a shows R2 averages of theﬁve\ncross-validation validation data sets along with standard deviations (1σ)a n d\npanelb showsR2 values of the meta learner’s test data set. The category-averagedR2\nvalues are stated in the last rows of each block, while overallR2 values are given in\nthe very last block. The properties gas permeabilities (μx) and elongation at break\n(ϵb) are trained on log base 10 scale (x7!log10ðx +1 Þ). TheR2 values are reported on\nthis scale. Panelc shows the minimum, mean, and maximum of polyBERT-based\nproperty predictions for 100 million hypothetical polymers.Tg, Tm,a n dTd stand for\nglass transition, melting, and degradation temperature.cp, Eat, Oi, Xc, Xe,a n dρ\nstand for heat capacity, atomization energy, limiting oxygen index, crystallization\ntendency (DFT), crystallization tendency (exp.), and density.Egc, Egb, Eea, Ei, Eib,a n d\nδ stand for band gap (chain), band gap (bulk), electron afﬁnity, ionization energy,\nelectronic injection barrier, and cohesive energy density.nc, ne, kc,a n dkf stand for\nrefractive index (DFT), refractive index (exp.), dielectric constant (DFT), and\ndielectric constant at freq.f 2 1:78,2,3,4,5,6,7,9,15fg . E, σ\ny, σb,a n dϵb stand for\nYoung’s modulus, tensile strength at yield, tensile strength at break, and elongation\nat break.μO2\n,μCO2\n,μN2\n,μH2\n,μHe,a n dμCH4\nstand for O2,C O2,N 2,H 2, He, and CH4 gas\npermeability. Plain numbers of this Figure can be found in Supplementary Tables S1\nand S5.\nArticle https://doi.org/10.1038/s41467-023-39868-6\nNature Communications|         (2023) 14:4099 7\nand visualizing the attention layers of the Transformer encoders can\nreveal such information. Saliency methods57 may also be used to\ndirectly explain the relationships between structural parts of the\nPSMILES strings (inputs) and polymer properties (outputs).\nYet another advantage of the polyBERT approach is its coverage\nof the entire chemical space. Molecule SMILES strings are a subset of\npolymer SMILES strings and differ by only two stars ([*])s y m b o l st h a t\nindicate the two endpoints of the polymer repeat unit. polyBERT has\nno intrinsic limitations or functions that obstruct predictingﬁnger-\nprints for molecule SMILES strings. Ourﬁrst experiments show con-\nsistent and well-conditionedﬁngerprints for molecule SMILES strings\nusing polyBERT that required only minimal changes in the canonica-\nlization routine.\nDiscussion\nHere, we show a generalizable, ultrafast, and accurate polymer infor-\nmatics pipeline that is seamlessly scalable on cloud hardware and\nsuitable for high-throughput screening of huge polymer spaces.\npolyBERT, which is a Transformer-based NLP model modiﬁed for the\npolymer chemical language, is the critical element of our pipeline.\nAfter training on 100 million hypothetical polymers, the polyBERT-\nbased informatics pipeline arrives at a representation of polymers and\npredicts polymer properties over two orders of magnitude faster but\nat the same accuracy as the best pipeline based on handcrafted PG\nﬁngerprints.\nThe total polymer universe is gigantic, but currently limited by\nexperimentation, manufacturing techniques, resources, and econom-\nical aspects. Contemplating different polymer types such as homo-\npolymers, copolymer, and polymer blends, novel undiscovered\npolymer chemistries, additives, and processing conditions, the num-\nber of possible polymers in the polymer universe is truly limitless.\nSearching this extraordinarily large space enabled by property pre-\ndictions is limited by the prediction speed. The accurate prediction of\n29 properties for 100 million hypothetical polymers in a reasonable\ntime demonstrates that polyBERT is an enabler to extensive explora-\nt i o n so ft h i sg i g a n t i cp o l y m e ru n i v e r s ea ts c a l e .p o l y B E R Tp a v e st h e\npathway for the discovery of novel polymers 100 times faster (and\npotentially even faster with newer GPU generations) than state-of-the-\nart informatics approaches– but at the same accuracy as slower\nhandcrafted ﬁngerprinting methods— by leveraging Transformer-\nbased ML models originally developed for NLP. polyBERTﬁnger-\nprints are dense and chemically pertinent numerical representations\nof polymers that adequately measure polymer similarity. They can be\nused for any polymer informatics task that requires numerical\nrepresentations of polymers such as property predictions (demon-\nstrated here), polymer structure predictions, ML-based synthesis\nassistants, etc. polyBERT ﬁngerprints have a huge potential to\naccelerate past polymer informatics pipelines by replacing the\nhandcrafted ﬁngerprints with polyBERTﬁngerprints. polyBERT may\nalso be used to directly design polymers based onﬁngerprints (that\ncan be related to properties) using polyBERT’s decoder that has been\ntrained during the self-supervised learning. This, however, requires\nretraining and structural updates to polyBERT and is thus part of a\nfuture work.\nMethods\nPSMILES canonicalization\nThe string representations of homopolymer repeat units in this work\nare PSMILES strings. PSMILES strings follow the SMILES\n24 syntax deﬁ-\nnition but use two stars to indicate the two endpoints of the polymer\nrepeat unit (e.g.,[*]CC[*]for polyethylene). The raw PSMILES syntax\nis non-unique; i.e., the same polymer may be represented using many\nPSMILES strings; canonicalization is a scheme to reduce the different\nPSMILES strings of the same polymer to a singel unique canonicalized\nPSMILES string. polyBERT requires canonicalized PSMILES strings\nbecause polyBERT ﬁngerprints change with different writings of\nPSMILES strings. In contrast, PGﬁngerprints are invariant to the way of\nwriting PSMILES strings and, thus, do not require canonicalization.\nFigure 6 s h o w st h r e ev a r i a n c e so fP S M I L E Ss t r i n g st h a tl e a v et h e\npolymer unchanged. The translational variance of PSMILES strings\nallows to move the repeat unit window of polymers (cf., white and red\nbox). The multiplicative variance permits to write polymers as multi-\nples of the repeat unit (e.g., twofold repeat unit of Nylon 6), while the\npermutational variance stems from the SMILES syntax deﬁnition\n24 and\nallows syntactical permutations of PSMILES strings that leave the\npolymer unchanged.\nFor this work, we developed the canonicalize_psmiles\nPython package thatﬁnds the canonical form of PSMILES strings in\nfour steps; (i) itﬁnds the shortest PSMILES string by searching and\nremoving repetition patterns, (ii)it connects the polymer endpoints to\ncreate a periodic PSMILES string, (iii) it canonicalizes the periodic\nPSMILES string using RDKit’s\n41 canonicalization routines, (iv) it breaks\nthe periodic PSMILES string to create the canonical PSMILES string.\nThe canonicalize_psmilespackage is available athttps://github.\ncom/Ramprasad-Group/canonicalize_psmiles.\nPolymer ﬁngerprinting\nFingerprinting converts geometric and chemical information of poly-\nmers (based upon the PSMILES string) to machine-readable numerical\nrepresentations in the form of vectors. These vectors are the polymer\nﬁngerprints and can be used for property predictions, similarity sear-\nches, or other tasks that require numerical representations of\npolymers.\nWe compare the polyBERTﬁngerprints, developed in this work,\nwith the handcrafted Polymer Genome (PG) polymerﬁngerprints. PG\nﬁngerprints capture key features of polymers at three hierarchical\nlength scales\n8,20. At the atomic scale (1st level), PGﬁngerprints track the\noccurrence of aﬁxed set of atomic fragments (or motifs)21.T h eb l o c k\nscale (2nd level) uses the Quantitative Structure-Property Relationship\n(QSPR)ﬁngerprints18,44 for capturing features on larger length-scales as\nimplemented in the cheminformatics toolkit RDKit41. The chain scale\n(3rd level)ﬁngerprint components deal with“morphological descrip-\ntors” such as the ring distance or length of the largest side-chain44.T h e\nPG ﬁngerprints are developed within the Ramprasad research group\nand used, for example, athttps://PolymerGenome.org.M o r ed e t a i l s\ncan be found in References8,44.\nN\nO\nTranslation\nN\nO\nNylon 6\n[*]NCCCCCC(=O)[*]\n[*]CCC(=O)NCCC[*]\n[*]NCCCCCC(=O)NCCCCCC(=O)[*]\nMultiplication:\nH\nH\n[*]NCCCCCC(NCCCCCC([*])=O)=O\nMultiplication and permutation: \nFig. 6 | Translational, multiplicative, and permutational variances of Polymer\nSimpliﬁed Molecular-Input Line-Entry System (PSMILES) strings.The gray and\nred boxes represent the smallest repeat unit of poly(hexano-6-lactam) (Nylon 6).\nThe red box can be translated to match the black box. The dashed boxes show the\nsecond smallest repeat unit (two-fold repeat unit) of Nylon 6.\nArticle https://doi.org/10.1038/s41467-023-39868-6\nNature Communications|         (2023) 14:4099 8\nAs discussed recently6,11, we sum the composition-weighted\npolymer ﬁngerprints to compute copolymer ﬁngerprints\nF = PN\ni Fici,w h e r eN is the number of comonomers in the copolymer,\nFi the ith comonomer ﬁngerprint vector, andci the fraction of theith\ncomonomer. This approach renders copolymerﬁngerprints invariant\nto the order in which one may sort the comonomers and satisﬁes the\ntwo main demands of uniqueness and invariance to different (but\nequivalent) periodic unit speciﬁcations. While the current ﬁnger-\nprinting scheme is most appropriate for random copolymers, other\ncopolymer ﬂavors may be encoded by adding additionalﬁngerprint\ncomponents. Contrary to homopolymerﬁngerprints, copolymerﬁn-\ngerprints may not be interpretable (e.g., the composition-weighted\nsum of theﬁngerprint component“length of largest side-chain” of two\nhomopolymers has no physical meaning).\nMultitask neural networks\nMultitask deep neural networks simultaneously learn multiple poly-\nmer properties to utilize inherent correlations of properties in data\nsets. The training protocol of the concatenation-conditioned multi-\ntask predictors follows state-of-the-art techniques involvingﬁve-fold\ncross-validation and a meta learner that forecasts theﬁnal property\nvalues based upon the ensemble of cross-validation predictors\n6,7,11.\nSupplementary Figure S2 details this process. After shufﬂing, we split\nthe data set into two parts and use 80% for theﬁve cross-validation\nmodels and for validating the meta learners. 20% of the data set is\nused for training the meta learners. We use the Hyperband method\n58\nof the Python package KerasTuner59 to fully optimize all hyperpar-\namters of the neural networks, including the number of layers,\nnumber of nodes, dropout rates, and activation functions. The\nHyperband methodﬁnds the best set of hyperparameters by mini-\nmizing the Mean Squared Error (MSE) loss function. We perform data\nset stratiﬁcation of all splits based on the polymer properties. The\nmultitask deep neural networks are implemented using the Python\nAPI of TensorFlow\n60.\nCO2 emission and timing\nExperiments were conducted using a private infrastructure, which has\nan estimated carbon efﬁciency of 0.432 kgCO2eq kWh−1. A total of 31 h\nof computations were performed on four Quadro-GP100-16GB (ther-\nmal design power of 235 W) for training polyBERT. Total emissions are\nestimated to be 12.6 kgCO\n2eq. About 8 h of computations on four\nGPUs were necessary for training the cross-validation and meta learner\nmodels with an estimated emission of 3.3 kgCO\n2eq for polyBERT and\nPolymer Genomeﬁngerprints, respectively. The total emissions for\npredicting 29 properties for 100 million hypothetical polymers are\nestimated to be 5.5 kgCO\n2eq, taking a total of 13.5 h. Estimations were\nconducted using the Machine Learning Impact calculator presented\nin ref.61.\nReporting summary\nFurther information on research design is available in the Nature\nPortfolio Reporting Summary linked to this article.\nData availability\nThe data set of 100 million hypothetical polymers with 29 predicted\nproperties is available for academic use athttps://doi.org/10.5281/\nzenodo.7766806.\nCode availability\nThe polyBERT code is available for academic use athttps://github.\ncom/Ramprasad-Group/polyBERTand Zenodo62.T h et r a i n e dp o l y -\nBERT model is available at https://huggingface.co/kuelumbus/\npolyBERT. The Python package for canonicalizing PSMILES strings is\navailable at https://github.com/Ramprasad-Group/canonicalize_\npsmiles. polyBERT-based property predictions will be made\naccessible through the polymer informatics platform Polymer Genome\nat https://PolymerGenome.org.\nReferences\n1. Plastics Europe. https://plasticseurope.org/knowledge-hub/\nplastics-the-facts-2021/.\n2. Batra, R., Song, L. & Ramprasad, R.Emerging materials intelligence\necosystems propelled by machine learning.Nat. Rev. Mater.6,\n655– 678 (2021).\n3. Chen, L. et al. Polymer informatics: Current status and critical next\nsteps. Mater. Sci. Eng.: R: Rep.144, 100595 (2021).\n4. Audus, D. J. & de Pablo, J. J. Polymer informatics: Opportunities and\nchallenges.ACS Macro Lett.6,1 0 7 8– 1082 (2017).\n5. Adams, N. & Murray-Rust, P. Engineering polymer informatics:\nTowards the computer-aided design of polymers.Macromol. Rapid\nCommun. 29,6 1 5– 632 (2008).\n6. Kuenneth, C., Schertzer, W. & Ramprasad, R. Copolymer infor-\nmatics with multitask deep neural networks.Macromolecules54,\n5957– 5961 (2021).\n7. Kuenneth, C. et al. Polymer informatics with multi-task learning.\nPatterns 2, 100238 (2021).\n8. Doan Tran, H. et al. Machine-learning predictions of polymer\nproperties with polymer genome.J. Appl. Phys.128, 171104 (2020).\n9. Chen, G., Tao, L. & Li, Y. Predicting polymers’ glass transition tem-\nperature by a chemical language processing model.Polymers13,\n1898 (2021).\n10. Pilania, G., Iverson, C. N., Lookman, T. & Marrone, B. L. Machine-\nlearning-based predictive modeling of glass transition tempera-\ntures: A case of polyhydroxyalkanoate homopolymers and copo-\nlymers. J. Chem. Inf. Model.59,5 0 1 3– 5025 (2019).\n11. Kuenneth, C. et al. Bioplastic design using multitask deep neural\nnetworks.Commun. Mater.3, 96 (2022).\n12. Barnett, J. W. et al. Designing exceptional gas-separation polymer\nmembranes using machine learning.Sci. Adv. 6(2020) https://doi.\norg/10.1126/sciadv.aaz4301.\n13. Kim, C., Batra, R., Chen, L., Tran, H. & Ramprasad, R. Polymer design\nusing genetic algorithm and machine learning.C o m p u t .M a t e r .S c i .\n186,1 1 0 0 6 7( 2 0 2 1 ) .\n14. Kern, J., Chen, L., Kim, C. & Ramprasad, R. Design of polymers for\nenergy storage capacitors using machine learning and evolutionary\nalgorithms.J. Mater. Sci.56, 19623– 19635 (2021).\n15. Gurnani, R. et al. polyG2G: A novel machine learning algorithm\napplied to the generative design of polymer dielectrics.Chem.\nMater. 33,7 0 0 8– 7016 (2021).\n16. Batra, R. et al. Polymers for extreme conditions designed using\nsyntax-directed variational autoencoders.Chem. Mater.\n32,\n10489– 10500 (2020).\n17. Wu, S. et al. Machine-learning-assisted discovery of polymers with\nhigh thermal conductivity using a molecular design algorithm.npj\nComput. Mater.5,6 6( 2 0 1 9 ) .\n1 8 . L e ,T . ,E p a ,V .C . ,B u r d e n ,F .R .&W i n k l e r ,D .A .Q u a n t i t a t i v e\nstructure-property relationshipm o d e l i n go fd i v e r s em a t e r i a l s\nproperties.Chem. Rev.112, 2889– 2919 (2012).\n19. Rogers, D. & Hahn, M. Extended-connectivityﬁngerprints.J. Chem.\nInf. Model.50,7 4 2– 754 (2010).\n20. Mannodi-Kanakkithodi, A., Pilania, G., Huan, T. D., Lookman, T. &\nRamprasad, R. Machine learning strategy for accelerated design of\npolymer dielectrics.Sci. Rep.6,2 0 9 5 2( 2 0 1 6 ) .\n21. Huan, T. D., Mannodi-Kanakkithodi, A. & Ramprasad, R. Accelerated\nmaterials property predictions and design using motif-basedﬁn-\ngerprints.Phys. Rev. B92, 014106 (2015).\n2 2 . M o r i w a k i ,H . ,T i a n ,Y . - S . ,K a w a s h i t a ,N .&T a k a g i ,T .M o r d r e d :a\nmolecular descriptor calculator.J. Cheminf.10,4( 2 0 1 8 ) .\n23. Vaswani, A. et al. Attention is all you need. arXiv (2017),https://doi.\norg/10.48550/arXiv.1706.03762.\nArticle https://doi.org/10.1038/s41467-023-39868-6\nNature Communications|         (2023) 14:4099 9\n24. Weininger, D. SMILES, a chemical language and information sys-\ntem. 1. Introduction to methodology and encoding rules.J. Chem.\nInf. Model.28,3 1– 36 (1988).\n25. Chithrananda, S., Grand, G., Ramsundar, B. ChemBERTa: Large-\nscale self-supervised pretraining for molecular property prediction.\narXiv (2020),https://doi.org/10.48550/arXiv.2010.09885.\n2 6 . W a n g ,S . ,G u o ,Y . ,W a n g ,Y . ,S u n ,H . ,H u a n g ,J .S M I L E S - B E R T .P r o -\nceedings of the 10th ACM International Conference on Bioinfor-\nmatics, Computational Biology and Health Informatics. New York,\nNY, USA; pp 429– 436, (2019).https://doi.org/10.1145/3307339.\n3342186.\n27. Li, J. & Jiang, X. Mol-BERT: An effective molecular representation\nwith BERT for molecular property prediction.Wirel Commun.\nMobile Comput.2021,1 – 7 (2021).\n28. Devlin, J., Chang, M.-W., Lee, K., Toutanova, K. BERT: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv\n(2018), https://doi.org/10.48550/arXiv.1810.04805.\n29. Schwaller, P. et al. Molecular transformer: A model for uncertainty-\ncalibrated chemical reaction prediction.ACS Central Sci.5,\n1572– 1583 (2019).\n30. Schwaller, P. et al. Predicting retrosynthetic pathways using\ntransformer-based models and a hyper-graph exploration strategy.\nChem. Sci.11, 3316– 3325 (2020).\n31. Xu, C., Wang, Y., Farimani, A. B. TransPolymer: a transformer-based\nlanguage model for polymer property predictions. arXiv (2022),\nhttps://doi.org/10.48550/arXiv.2209.01307.\n32. Liu, Y. et al. RoBERTa: A robustly optimized BERT pretraining\napproach. arXiv (2019),https://doi.org/10.48550/arXiv.1907.11692.\n33. Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., Dahl, G. E.\nNeural message passing for quantum chemistry. arXiv (2017),\nhttps://doi.org/10.48550/arXiv.1704.01212.\n34. Park, J. et al. Prediction and interpretation of polymer properties\nusing the graph convolutional network.ACS Polymers Au2,\n213– 222 (2022).\n35. Gurnani, R., Kuenneth, C., Toland, A. & Ramprasad, R. Polymer\ninformatics at scale with multitask graph neural networks.Chem.\nMater. 35,1 5 6 0– 1567 (2023).\n36. Aldeghi, M. & Coley, C. W. A graph representation of molecular\nensembles for polymer property prediction.Chem. Sci.13,\n10486– 10498 (2022).\n37. Fang, X. et al. Geometry-enhanced molecular representation\nlearning for property prediction.Nat. Mach. Intell.4,1 2 7– 134 (2022).\n38. Wang, Y., Wang, J., Cao, Z. & Barati Farimani, A. Molecular con-\ntrastive learning of representations via graph neural networks.Nat.\nMach. Intell.4,2 7 9\n– 287 (2022).\n39. He, P., Liu, X., Gao, J., Chen, W. DeBERTa: Decoding-enhanced BERT\nwith disentangled attention. arXiv (2020),https://doi.org/10.\n48550/arXiv.2006.03654.\n40. Degen, J., Wegscheid-Gerlach, C., Zaliani, A. & Rarey, M. On the art\nof compiling and using’Drug-Like’ chemical fragment spaces.\nChemMedChem3,1 5 0 3– 1507 (2008).\n41. Landrum, G. others, RDKit: Open-source cheminformatics. (2006).\n42. Jha, A., Chandrasekaran, A., Kim, C. & Ramprasad, R. Impact of\ndataset uncertainties on machine learning model predictions: the\nexample of polymer glass transition temperatures.Model. Simul.\nMater. Sci. Eng.27,0 2 4 0 0 2( 2 0 1 9 ) .\n43. Kim, C., Chandrasekaran, A., Jha, A. & Ramprasad, R. Active-\nlearning and materials design: theexample of high glass transition\ntemperature polymers.MRS Commun.9,8 6 0– 866 (2019).\n44. Kim, C., Chandrasekaran, A., Huan, T. D., Das, D. & Ramprasad, R.\nPolymer genome: A data-powered polymer informatics platform for\nproperty predictions.J. Phys. Chem. C122,1 7 5 7 5– 17585 (2018).\n45. Patra, A. et al. A multi-ﬁdelity information-fusion approach to\nmachine learn and predict polymer bandgap.C o m p u t .M a t e r .S c i .\n172, 109286 (2020).\n46. Chen, L. et al. Frequency-dependent dielectric constant prediction\nof polymers using machine learning.npj Comput. Mater.6,\n61 (2020).\n47. Venkatram, S., Kim, C., Chandrasekaran, A. & Ramprasad, R. Critical\nassessment of the hildebrand and hansen solubility parameters for\npolymers.J. Chem. Inf. Model.59,4 1 8 8– 4194 (2019).\n48. Zhu, G. et al. Polymer genome-based prediction of gas perme-\nabilities in polymers.J. Polym. Eng.40,4 5 1– 457 (2020).\n49. PolyInfo. https://polymer.nims.go.jp/en/.\n50. Ahmad, W., Simon, E., Chithrananda, S., Grand, G., Ramsundar, B.\nChemBERTa-2: Towards Chemical Foundation Models.arXiv(2022),\nhttps://doi.org/10.48550/arXiv.2209.01712.\n51. Wolf, T. et al. Transformers: State-of-the-Art Natural Language\nProcessing. Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System Demonstrations.\nStroudsburg, PA, USA; pp 38– 45, (2020),https://doi.org/10.18653/\nv1/2020.emnlp-demos.6.\n52. Sanh, V., Debut, L., Chaumond, J., Wolf, T. DistilBERT, a distilled\nversion of BERT: smaller, faster, cheaper and lighter.arXiv (2019),\nhttps://doi.org/10.48550/arXiv.1910.01108.\n53. Kudo, T., Richardson, J. SentencePiece: A simple and language\nindependent subword tokenizer and detokenizer for Neural Text\nProcessing.arXiv (2018), https://doi.org/10.48550/arXiv.\n1808.06226.\n54. Reimers, N., Gurevych, I. Sentence-BERT: Sentence Embeddings\nusing Siamese BERT-Networks.arXiv (2019), https://doi.org/10.\n48550/arXiv.1908.10084.\n55. McInnes, L., Healy, J., Melville, J. UMAP: Uniform Manifold\nApproximation and Projection for Dimension Reduction.arXiv\n(2018), https://doi.org/10.48550/arXiv.1802.03426.\n56. Alammar, J. Ecco: An Open Source Library for the Explainability of\nTransformer Language Models. Proceedings of the 59th Annual\nMeeting of the Association for Computational Linguistics and the\n11th International Joint Conference on Natural Language Proces-\nsing: System Demonstrations. Stroudsburg, PA, USA; pp 249– 257,\n(2021), https://doi.org/10.18653/v1/2021.acl-demo.30.\n57. Bastings, J., Filippova, K. The elephant in the interpretability room:\nWhy use attention as explanation when we have saliency methods?\narXiv (2020), https://doi.org/10.48550/arXiv.2010.05607.\n5 8 . L i ,L . ,J a m i e s o n ,K . ,D e S a l v o ,G . ,R o s t a m i z a d e h ,A .&T a l w a l k a r ,A .\nHyperband: A novel bandit-based approach to hyperparameter\noptimization.J. Mach. Learn. Res.18,1 – 52 (2016).\n59. O’Malley, T., Bursztein, E., Long, J., Chollet, F., Jin, H., Invernizzi, L.\nKeras Tuner. (2019);https://github.com/keras-team/keras-tuner.\n60. Martin, A. et al. TensorFlow: Large-scale machine learning on het-\nerogeneous systems. (2015);https://www.tensorﬂow.org/.\n61. Lacoste, A., Luccioni, A., Schmidt, V., Dandres, T. Quantifying the\ncarbon emissions of machine learning. arXiv (2019),https://doi.org/\n10.48550/arXiv.1910.09700.\n62. Kuenneth, C., Ramprasad, R. polyBERT: A chemical language model\nto enable fully machine-driven ultrafast polymer informatics.\nZenodo (2023), https://doi.org/10.5281/zenodo.7969082.\nAcknowledgements\nC.K. thanks the Alexander von Humboldt Foundation forﬁnancial sup-\nport. We acknowledge funding from the Ofﬁce of Naval Research\nthrough a Multidisciplinary University Research Initiative grant (N00014-\n17-1-2656) and the National Science Foundation (#1941029).\nAuthor contributions\nC .K .d e s i g n e d ,t r a i n e da n de v a l u a t e dt h em a c h i n el e a r n i n gm o d e l sa n d\ndrafted this paper. The work was conceived and guided by R. R. All\nauthors discussed results and commented on the manuscript.\nArticle https://doi.org/10.1038/s41467-023-39868-6\nNature Communications|         (2023) 14:4099 10\nCompeting interests\nR.R. is the founder of the company Matmerize, Inc., that intends to\nprovide polymer informatics services. A provisional patent has been\nﬁled by the Georgia Tech Research Corporation, Atlanta, GA; Inventors:\nRampi Ramprasad, and Christopher Kuenneth; Application Number: 63/\n374,761; Status: patent pending; Aspect covered in the patent applica-\ntion: transformer based informatics pipeline for polymer representation\nand property predictions.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41467-023-39868-6.\nCorrespondenceand requests for materials should be addressed to\nRampi Ramprasad.\nPeer review information: Nature Communicationsthanks the anon-\nymous, reviewer(s) for their contribution to the peer review of this work.\nAp e e rr e v i e wﬁle is available.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jur-\nisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons license, and indicate if\nchanges were made. The images or other third party material in this\narticle are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons license and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this license, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2023\nArticle https://doi.org/10.1038/s41467-023-39868-6\nNature Communications|         (2023) 14:4099 11",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8105484843254089
    },
    {
      "name": "Scalability",
      "score": 0.6660181879997253
    },
    {
      "name": "Pipeline (software)",
      "score": 0.6518160104751587
    },
    {
      "name": "Cloud computing",
      "score": 0.5578253269195557
    },
    {
      "name": "Chemical space",
      "score": 0.4633999168872833
    },
    {
      "name": "Fingerprint (computing)",
      "score": 0.4521508514881134
    },
    {
      "name": "Informatics",
      "score": 0.4467789828777313
    },
    {
      "name": "Software deployment",
      "score": 0.425345778465271
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3662129342556
    },
    {
      "name": "Data science",
      "score": 0.3640592694282532
    },
    {
      "name": "Software engineering",
      "score": 0.14892196655273438
    },
    {
      "name": "Database",
      "score": 0.13872173428535461
    },
    {
      "name": "Drug discovery",
      "score": 0.13730069994926453
    },
    {
      "name": "Bioinformatics",
      "score": 0.1341596245765686
    },
    {
      "name": "Programming language",
      "score": 0.1333717405796051
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    }
  ]
}