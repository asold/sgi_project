{
  "title": "Comparison of Neural Language Modeling Pipelines for Outcome Prediction From Unstructured Medical Text Notes",
  "url": "https://openalex.org/W4210277770",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3127522577",
      "name": "Cherubin Mugisha",
      "affiliations": [
        "University of Aizu"
      ]
    },
    {
      "id": "https://openalex.org/A2119206154",
      "name": "Incheon Paik",
      "affiliations": [
        "University of Aizu"
      ]
    },
    {
      "id": "https://openalex.org/A3127522577",
      "name": "Cherubin Mugisha",
      "affiliations": [
        "University of Aizu"
      ]
    },
    {
      "id": "https://openalex.org/A2119206154",
      "name": "Incheon Paik",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2902433112",
    "https://openalex.org/W6676053923",
    "https://openalex.org/W3127747328",
    "https://openalex.org/W3181361218",
    "https://openalex.org/W2625625371",
    "https://openalex.org/W2095603117",
    "https://openalex.org/W4247264423",
    "https://openalex.org/W2023894087",
    "https://openalex.org/W3044427673",
    "https://openalex.org/W2283041611",
    "https://openalex.org/W2966351171",
    "https://openalex.org/W2395172628",
    "https://openalex.org/W2889565651",
    "https://openalex.org/W6634358093",
    "https://openalex.org/W2922594471",
    "https://openalex.org/W2983587580",
    "https://openalex.org/W3024836211",
    "https://openalex.org/W2738242267",
    "https://openalex.org/W2927032858",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W6761672038",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W2900049381",
    "https://openalex.org/W6738346508",
    "https://openalex.org/W6607783245",
    "https://openalex.org/W2114422718",
    "https://openalex.org/W3035511434",
    "https://openalex.org/W2601221586",
    "https://openalex.org/W2146958003",
    "https://openalex.org/W2795959543",
    "https://openalex.org/W2948909602",
    "https://openalex.org/W1980867644",
    "https://openalex.org/W2149684865",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W6682691769",
    "https://openalex.org/W6685812147",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2517194566",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W2937845937",
    "https://openalex.org/W3104523752",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2985962305",
    "https://openalex.org/W3102251128"
  ],
  "abstract": "Machine learning techniques and algorithm-based approaches are becoming more and more vital to support clinical decision-making. In the medical area, natural language processing (NLP) techniques have shown the ability to extract useful information from electronic health records. On the one hand, statistic, semantic, and contextualized word embedding-based models and on the other hand preprocessing approaches are the keys to a better representation of a document. Using narratives from the Intensive Care Unit, we elaborated a comparison of the most used methods and preprocessing approaches to tackle an outcome prediction problem and guide researchers into NLP pipelines in the medical area. We used real data from Medical Information Mart for Intensive Care-III (MIMIC-III). We selected all notes related to patients with pneumonia. We conducted a deep analysis on text preprocessing tasks producing three datasets: raw data with minor preprocessing, meticulous preprocessing, and extreme preprocessing filtering only medical-related terminologies using Named Entity Recognition algorithms. We then used these three sets in five models, of which two are based on the traditional noncontextual word embedding techniques and three use contextualized word embedding based on a transformer. We demonstrated that transformer-based models outperform other word embedding models and a profound preprocessing yielded an accuracy of 98.2 F1-score. These results show the highly competitive ability of NLP predictive models against other models that use medical data. With an appropriate NLP pipeline, the information contained in medical narratives can be used to draw up a patient profile, and admission notes can help to ascertain a mortality risk of a patient admitted to the Intensive Care Unit.",
  "full_text": "Received January 11, 2022, accepted January 25, 2022, date of publication January 31, 2022, date of current version February 15, 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.3148279\nComparison of Neural Language Modeling\nPipelines for Outcome Prediction From\nUnstructured Medical Text Notes\nCHERUBIN MUGISHA\n, (Member, IEEE), AND INCHEON PAIK\n, (Senior Member, IEEE)\nSchool of Computer Science and Engineering, The University of Aizu Tsuruga, Ikki-machi, Aizu-Wakamatsu, Fukushima 965-8580, Japan\nCorresponding author: Incheon Paik (paikic@u-aizu.ac.jp)\nABSTRACT Machine learning techniques and algorithm-based approaches are becoming more and more\nvital to support clinical decision-making. In the medical area, natural language processing (NLP) techniques\nhave shown the ability to extract useful information from electronic health records. On the one hand,\nstatistic, semantic, and contextualized word embedding-based models and on the other hand preprocessing\napproaches are the keys to a better representation of a document. Using narratives from the Intensive Care\nUnit, we elaborated a comparison of the most used methods and preprocessing approaches to tackle an\noutcome prediction problem and guide researchers into NLP pipelines in the medical area. We used real\ndata from Medical Information Mart for Intensive Care-III (MIMIC-III). We selected all notes related to\npatients with pneumonia. We conducted a deep analysis on text preprocessing tasks producing three datasets:\nraw data with minor preprocessing, meticulous preprocessing, and extreme preprocessing ﬁltering only\nmedical-related terminologies using Named Entity Recognition algorithms. We then used these three sets in\nﬁve models, of which two are based on the traditional noncontextual word embedding techniques and three\nuse contextualized word embedding based on a transformer. We demonstrated that transformer-based models\noutperform other word embedding models and a profound preprocessing yielded an accuracy of 98.2 F1-\nscore. These results show the highly competitive ability of NLP predictive models against other models that\nuse medical data. With an appropriate NLP pipeline, the information contained in medical narratives can\nbe used to draw up a patient proﬁle, and admission notes can help to ascertain a mortality risk of a patient\nadmitted to the Intensive Care Unit.\nINDEX TERMS BERT, medical notes, NER, NLP, outcome prediction, pre-trained language models, text\npreprocessing, word embedding.\nI. INTRODUCTION\nPneumonia is an infectious disease of the lungs affecting alve-\noli and caused by bacteria, fungi, or viruses. Pneumonia can\nrange in seriousness from mild to life-threatening. It remains\nthe commonest infective reason for admission to intensive\ncare as well as being the most common secondary infection\nacquired while in the Intensive Care Unit (ICU) [1], [2].\nElectronic Health Records (EHRs) are health-related infor-\nmation on an individual created in a health care organization.\nEHR systems contain structured data such as demographics,\nvital signs, laboratory test results, medications, and proce-\ndures. They also have unstructured medical or nonmedical\ndata in a free format such as imaging reports or care-provider\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Vishal Srivastava.\nnotes [3]. In medical assessment, it is common and practical\nto use all types of data to understand the status of a patient\nor to predict his outcome. However, for caregivers, medical\nnotes are of paramount importance.\nWithin a hospitalization or a clinical visit, a patient might\nhave several note documents which can constitute a rich and\nlong clinical history. Clinical notes provide a deep under-\nstanding of a patient’s illness because they describe symp-\ntoms, clinical history, reasons for admission, and details of\nany intervention made by a multidisciplinary team [4]. With\nthe medical texts representing 80% of the EHR data [5],\nadmission constitute an extensive informative source used by\ndoctors to draw a patient’s proﬁle within the ﬁrst 24 hours\nof admission. It is then crucial to be able to use his history\nand admission description to predict what is likely to happen\nduring his stay.\nVOLUME 10, 2022 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 16489\nC. Mugisha, I. Paik: Comparison of Neural Language Modeling Pipelines for Outcome Prediction\nIn recent years, machine learning algorithms have been\nincreasingly used to predict the outcome by using structured\nor unstructured medical data. However, using free-text notes\nto achieve such tasks may encounter a lot of challenges. Even\nthough there are standards [6] when taking medical notes,\nmost of the texts are biased by internal and conventional\nwriting methods that make generalization harder for resulting\nmodels in a different environment. In addition, medical text\nnotes can be too long to be handled by conventional natural\nlanguage processing (NLP) models. Consequently, achieving\ngood results requires a better algorithm that preprocesses the\ndata and models the determinants of health condition for an\noverall understanding of a patient’s status.\nAlthough there are different studies on medical text clas-\nsiﬁcation, not many have demonstrated a clear statistical\ncomparison of NLP pipelines to guide researchers on the\nselection of methods ensuring the best results.\nOur main insight was that admission narrative notes have\nthe potential to predict outcome only if, in the NLP pipeline,\nwe can ﬁnd the best combination of preprocessing methods,\ndocument representation, and learning models.\nThe question of this research is what combination of NLP\nmodels and preprocessing methods are appropriate to unlock\nthe information from medical narratives.The present study\naims to use medical pneumonia patients notes, written by\na multidisciplinary team of care-providers, to investigate\namong the dynamic word embeddings and static models to\nassess and compare their performance on the outcome pre-\ndiction of an ICU hospitalization. We evaluate the resulting\nmodels using admission notes taken within 24 hours.\nII. RELATED WORK\nA. TRADITION LINEAR MODELS\nPrediction of prognosis to inform decision-making in the\nICU has a long history. Traditionally, statistical meth-\nods were widely used to evaluate the survival rate of a\npatient using domain experts features. Linear models such\nas Kaplan–Meier (KM) estimator were the most popular\ncombination with Cox proportional hazards regression to\nhandle regression problems [7]. Using advanced methods\nbased on logistic regression, Simpliﬁed Acute Physiology\nScore (SAPS) and the Acute Physiologic Assessment and\nChronic Health Evaluation (APACHE) demonstrated a clear\nimprovement in assessing the disease severity of a hospi-\ntalized patient. However, those models use predetermined\nfeatures, which greatly limits their applicability in a real\nsituation.\nIn recent years, those traditional methods have been sur-\npassed by more modern and accurate algorithms mainly using\ndata-driven models based on machine learning architectures.\nSince the linguistic string project [8] in analyzing clinical\ndocuments, most of the work has been conducted around gen-\neral medical management, treatment, test and results, patient\nstate, and patient behavior using medical text.\nHowever, it has been more challenging to demonstrate the\nreal usability of nonmedical data. The nature of medical text\ndata requires a combination of steps to unlock the information\nembedded in a clinical text (e.g., disease, treatment, patient\nstatus) by transforming the text into structured medical data.\nThe automation of this process and the efﬁciency of models\nto perform tasks such as clinical text classiﬁcation has been\ninvestigated [4].\nB. ML MODELS AND TEXT PREPROCESSING\nCurrently, ML techniques have shown a major improvement\nfor prediction in the general domain and particularly in the\nmedical domain. They can perform better using either struc-\ntured and unstructured data or even both through an ensemble\nof machine learning processes [3], [9]. An NLP task starts\nwith a preprocessing stage, to extract useful information and\nstructure the raw text into a format that can make use of\nautomated computing power.\nA review [10] conducted on 67 publications from 2000 to\n2015 has shown that extracting information from the EHR\nnarratives can improve case detection for classiﬁcation tasks.\nWhile this process can use different techniques of information\nextraction, 67% of the studies incorporated rule-based, 24%\nused keywords, and only 9% include machine learning in\ntheir approach. However, this trend has changed and a recent\nreview [11] shows that machine learning-based methods are\nmore used. The medical data transformation stage is chal-\nlenging for messy medical notes because preprocessing can\nclean out signiﬁcant information that is clinically important\nto predict the outcome accurately [12], [13].\nTo process medical text, Authors have been using the Uni-\nﬁed Medical Language System (UMLS) in order to reduce\nambiguity from abbreviations and conventional annotations.\nHowever, according to Liu et al. [14], 31% of UMLS abbre-\nviations have multiple meanings. This can be resolved by\ncomputing the proximity of the abbreviation to its expanded\nform and replacing it with the most suitable term. This\nabbreviation disambiguation pipeline has brought a lot of\ncontroversy among the community and led to the creation of\nseveral resources for different data.\nIn NLP, deep learning methods such as Long Short-Term\nMemory (LSTM) and its variants use a preprocessing pipeline\nthat includes a ﬁltering process based on predeﬁned con-\ntrolled vocabulary terms before transforming data into train-\ning vectors [15], [16].In this recent study [17], the authors\npropose an online medical pre-diagnosis support in which\nsemantic and sequential features are extracted from a patient’s\ninputs using a CNN-RNN-based architecture model to predict\na diagnosis.\nGeraci et al. proposed a neural network to extract phe-\nnotype information from electronic medical record(EMR)\ntext notes [18]. Their goal was to identify suitable candi-\ndates for medical research using doctors’ narratives within a\nsupervised learning process. They extract useful information\nthrough a Document Term Matrix (DTM) using the TF-IDF\nalgorithm. Their results show that NLP can help to identify\ncriteria for models to perform better for a task such as classiﬁ-\ncation. Wang et al. [19] illustrated a paradigm of clinical text\n16490 VOLUME 10, 2022\nC. Mugisha, I. Paik: Comparison of Neural Language Modeling Pipelines for Outcome Prediction\nclassiﬁcation using deep representation and weak supervi-\nsion. Their work demonstrated that it could be possible to use\na deep neural network like CNN and outperform traditional\nNLP rule-based algorithms. Their approach also compared\nthe importance of using word embeddings over count vector\nalgorithms such as TF-IDF. However, their method has limi-\ntations because the model is trained from scratch, it requires\na lot of training data. The input size is also dictated by the\nword embedding methods, which are usually not suitable for\nlong text like medical narratives.\nAuthors have tried to tackle this problem in recent litera-\nture by using more sophisticated NLP models. For example,\nmodel like BERT (Bidirectional Encoder Representations\nfrom Transformers) [20]have shown impressive results from\nan architecture of a multilayer encoder models, to learn words\nand documents representation more deeply. In the medical\narea, researchers have been trying to leverage the knowledge\nfrom general documents to pretrain speciﬁc-domain models\nfor higher performance for medical-related tasks [21], [22].\nDespite that evolution in NLP, we still have many pub-\nlications that use either advanced machine learning models\nor archaic models. Although most of these studies claim to\nhave achieved the best scores in various tasks by using very\ndifferent approaches, we can only wonder if there is no room\nfor improvement. To our knowledge, no other research trying\nto elucidate a fair comparison of those methods has been\npublished.\nIII. MATERIAL AND METHODS\nPrediction task has been a great topic for academic research.\nFinding a correlation between the massive amount of clinical\ndata and the outcome has the potential to understand life\nbetter and the factors involved in its end. In the ICU, time\ncould be the determinant, and to obtain more valuable and\neasily interpretable information, medical notes are a good\nalternative to identifying key problems of a patient when other\nsources are not available.\nText narratives contain a concise description of a patient\nthat can inform a caregiver about the status of the patient\nas soon as he is admitted to the ICU. Nonetheless, those\nnarratives could incorporate more information than neces-\nsary, such as duplications of the structured data or repetition\nfrom different contributors, making it hard to be modeled for\nprognosis or prediction.\nMachine learning and NLP have shown an incredible abil-\nity to learn from data; no matter how messy they are, there is\nalways a potential to obtain an output from a model. However,\nbuilding up a consistent and valuable model requires a deep\nunderstanding of the data to know what preprocessing steps\nare needed and what model architecture is more suitable for\nthose data.\nA. STUDY WORKFLOW DIAGRAM\nThis research was conducted in several steps consisting of\nthree main processes illustrated in Fig. 1. A summarized\nworkﬂow for the study is as follows:\nTABLE 1. Dataset size description.\n1) Step1: Data sampling and selection from a large dataset\nof MIMIC notes.\n2) Step2: Data preprocessing through a light(A), thor-\nough(B), and extreme(C) cleaning performed by\nextracting medical entities, producing three separate\ndatasets.\n3) Step3: Use of various embeddings to train different\nNLP classiﬁer models.\nDetailed descriptions of each of these steps are provided in\nthe following sections.\nB. DATA DESCRIPTION\nTo initiate this research, we need to use real medical text\nto challenge our approach and models to real-life data.\nWe extracted our narratives from the Medical Information\nMart for Intensive Care-III (MIMIC-III) [23] data using SQL\nqueries. MIMIC is a publicly available multiparameter mon-\nitoring system used in the ICU for 11 years. It contains\nstructured information such as physiological medical data\nand unstructured data such as text notes taken by differ-\nent healthcare actors. The overall mortality of patients in\nthe MIMIC-III database is 23.2%. To narrow our scope of\nresearch, we had to ﬁnd a disease with enough data and a\nhigher mortality rate for balanced learning for our models.\nTherefore, using only the admission diagnostic instead of\nICD9 codes, ’’Pneumonia’’ as the main disease comes ﬁrst\nwith 2059 cases and a mortality rate of 29%. These patients\nhave a total of 85085 notes taken by physicians, nurses,\nradiologists, and nutritionists, making an average of 41 notes\nper admission. Our cohort of patients comprised only adults,\nall over 15 years old.\nWe associated with each sequence of notes a label from the\npatient outcome. This binary sequence labeling considered all\nnotes for discharged patients as a negative class represented\nby ’’0’’ and for those who deceased in hospital as a positive\nclass represented by ’’1’’. In addition, in case a patient has\nmultiple admissions, they were considered as new. We, there-\nfore, included the new admission data and used admission\nID instead of patient ID to corroborate our queries. All these\nexploratory data analyses were done using python libraries\nand their statistics are reported in Table 1, and more details\non the data sets sampling are given in section IV-D.\nC. DATA CLEANING\nData cleaning refers to steps that we took to standardize our\ndata and to remove text and characters that are not relevant to\nbe left with a clean text dataset that is ready to be analyzed.\nVOLUME 10, 2022 16491\nC. Mugisha, I. Paik: Comparison of Neural Language Modeling Pipelines for Outcome Prediction\nFIGURE 1. Study framework.α, β, andγ illustrate different embeddings for each model.α used Global Vectors for the three datasets,β used\nCountVector and TF-IDF from the NER dataset, whileγ used BERT embeddings for all three datasets.\nAuthors have suggested many methods of text data clean-\ning. Most of the NLP cleaning tasks are based on basic rules\nsuch as converting text to lower case, regular expression and\nword replacement, punctuation, and nonalphanumeric char-\nacter removal. Advanced preprocessing will include more\ntasks such as stop-words and tokenization, stemming and\nlemmatization, word tagging, or Named Entity Recognition\n(NER). All of these will depend on the data, whether it has a\ndictionary or not, or simply the NLP task we want to perform.\nFor data privacy, an illustration sample on the cleaning results\nis provided as a reference in Fig. 2.\nTo create our three datasets and analyze in detail the impact\nof each cleaning approach, especially for medical notes,\nwe proceed as follows:\n1) MINOR CLEANING\nThis cleaning task follows the basic rules of the NLP clean-\ning task utilizing the natural language toolkit (NLTK). For\ncase sensitivity, we converted all text into lower case and\nused regular expressions to remove punctuation extra white\nspaces, line breaks, and nonregular expressions. In addition,\nWe utilized the Stop-words dictionary to ﬁlter out irrelevant\nentities.\n2) THOROUGH CLEANING\nTo take our cleaning process even further, and harmonize\nclinical abbreviations and acronyms, we manually built up\na matching dictionary of 80 terms. Using the UMLS [24],\n[25] with its metathesaurus inventory, we selected the top\nused acronyms in medical notes presented in these stud-\nies [26], [27] and added predominant risk factors for pneumo-\nnia such as acute respiratory distress syndrome (ARDS) and\nacute respiratory failure (ARF) [28], [29]. We also removed\nde-identiﬁcation characters and harmonized typos and con-\nventional spellings (e.g., pt, dr, W/O). These two cleansing\nprocesses are suitable for the emergent bidirectional models\nbecause their tokenization uses a word-piece technique and\ndoes not need a deep cleaning.\n16492 VOLUME 10, 2022\nC. Mugisha, I. Paik: Comparison of Neural Language Modeling Pipelines for Outcome Prediction\nFIGURE 2. Illustration of the three types of data cleaning. This table\nshows an example of the transformation of the raw data (first column),\nthrough a cleaning process, producing three different datasets A,B and C.\n3) NAMED ENTITY RECOGNITION\nNarratives can be very long and full of information that it\ncould be necessary to ﬁlter out domain-related data. NER has\nshown the ability to process data semantically by identifying\nand categorizing key information (entities) in text [30]–[32].\nTraditionally, dictionary NER-based models have been used\nfor text data mining, and recently, deep learning-based mod-\nels have shown outstanding progress leveraging pretrained\nlanguage models. For our case, we addressed this step as\nsentence-level biomedical information extraction tasks. The\nbiomedical language representation model for biomedical\ntext mining (BioBERT) [21] is a domain-speciﬁc language\nmodel that has been trained on medical text data. BioBERT\nNER (BERN) [33] is one of its modules for recognizing\nbiomedical entities and discovering new entities. We used\nBERN to extract entities related to disease, drugs/chemicals,\ngenes/proteins, and species. However, the resulting entities\nare independent of each other, so they can only be used by\nnonsequential models and their association would be consid-\nered as correlated features.\nIV. MODELS\nIn this study, we used several methods, from the tradi-\ntional to the recent NLP models. NLP in the medical area\nhas been using count-vector-based models, word-embedding-\nbased models, and transformers-based models. To make a fair\ncomparison, we conducted this study using all of them as we\nprepared data accordingly.\nA. FEATURE EXTRACTION\nTo convert the text into a numeric format, comprehensible\nby computers, the narratives need to be encoded. We used\nvarious encoding techniques such as:\n1) BAG OF WORDS (BOW)\nBOW is a statistical representation of words and sentences\nand their compositionality [34]. Boosted by the success of\ntext classiﬁcation [34], [35], BOW has become one of the\nmost used methods to classify text and documents using\nkeywords. Although this method can help to classify text,\nit represents words in a singular dimension vector. It does\nnot carry any semantic or syntactic meanings. To use these\nmethods, we hypothesized that discriminative terms could\nbe highlighted by a term frequency counter. Counter Vector-\nizer is a low-level one-hot encoder that transforms a given\ntext into a vector based on the frequency of each term that\noccurs in the entire document. This representation can be\nefﬁcient if each class has particular discerning words. Term\nFrequency Inverse Document Frequency (TFIDF) measures\nthe relevancy of a given term by multiplying its frequency by\nthe logged inverse document frequency of that term across the\nentire corpus.\nxij =TFij ∗log(|J|/TFij) (1)\nwhere TFij is the term frequency and J the number of doc-\numents in the corpus. TFIDF is more efﬁcient since it nor-\nmalizes the count by scaling up rare terms and diminishes\nthe weight of frequent words like ’’patient’’ in our case.\nWe utilized Count vectorizer and TFIDF encoder for dataset\nwhere the order of words are ignored or broken (NER) and\nwe additionally varied the vocabulary size between 1000 and\n5000 words.\n2) GloVe\nTo take advantage of the multidimensions of text data,\nwe must use a model that vectorizes the text from a large\nnumber of precise syntactic and semantic word relation-\nships. Global Vectors for Word Representation(GloVe) [36]\nrepresents words as real-valued vectors in a vector space\nof relatively low dimensions compared with its vocabu-\nlary size. This means that words will be close in a vector\nspace only if their semantic and syntactic meanings are also\nrelatively close and vice versa. In contrast with precedent\nembeddings such as Word to Vector(Word2Vec) [37], fre-\nquency of co-occurrences within context windows is vital as\nsemantic information and should be carried on. In our case,\nwe assumed that frequent co-occurrence words determine the\noutcome; besides, global corpus statistics are already incor-\nporated in the embeddings. We downloaded the pre-trained\nword vectors and we used it as our word embedding to train\na sequence model.\n3) BERT EMBEDDINGS\nBERT is a contextualized word vector representation. BERT\nembeddings create different vectors for a word used in\ndifferent contexts. It utilizes the transformer encoder to rep-\nresent a word in a higher-dimensional space, capturing rela-\ntions between distant words more efﬁciently than traditional\nbidirectional encoders. Using a vocabulary size of more than\n30000 tokens, BERT can encode any words or subwords\nusing its position in the input sequence. A text represen-\ntation by BERT prepends to each sequence a [CLS] token\nthat can be used for a classiﬁcation task [20]. Utilizing\nsubwords has an advantage especially in biomedical text\nbecause we can encode more properly uncommon medical\nterminologies.\nVOLUME 10, 2022 16493\nC. Mugisha, I. Paik: Comparison of Neural Language Modeling Pipelines for Outcome Prediction\nB. LEARNING MODELS\n1) RECURRENT NETWORKS\nIn deep learning, a recurrent neural network (RNN) is a\ntraditional reference for time series data such as sound and\nmonitoring data in medical scenarios [38]. Conventional\nRNNs are slow and for long sequences, backpropagation\nin time tends to either vanish or explode [39]. Variants of\nRNN such as LSTMs have been developed to overcome\nRNN issues for long sequential inputs such as text but also\nto learn a bidirectional dependency via an attention mech-\nanism using BiLSTM [40]. These models use an encoder\npart for a classiﬁcation task where several recurrent cells\nhandle each element as an input vector and propagate it\nforward.\nht =f (W (hh)ht−1 +W (hx)xt ) (2)\nHidden states ht are computed by applying some weights\nw(hh) on the previous input vector xi where i is the order of\nthe input words. For the output, a decoder part will calculate\na vector where each value will represent a probability score\nfor each class.\nyt =SoftMax(W sht ) (3)\nHere, ht represents the output of the encoder for an input i\nand W s is the respective weight applied by the decoder before\nfeeding to a SoftMax function [41].\n2) TRANSFORMER\nRNNs and LSTMs are slow because data need to be passed\nsequentially. Transformer-based models can use the advance-\nment of the computation technology by parallelizing the\nprocess and learning faster. Leveraging the knowledge from\nmodels like BERT, authors proposed multiple variants of\nlanguage models dedicated to medical text. Among them,\nfor this study, we speciﬁcally utilized ClinicalBERT and\nBioBERT. Even though these models are domain-speciﬁc-\nbased models for biomedicals, they were pretrained on dif-\nferent data. BioBERT initialized its weights from BERT and\nused PubMed abstracts and Central full-text articles while\nClinicalBERT leveraged BioBERT weights and pretrained\nusing MIMIC medical notes. To use these models, we lever-\naged their weights and ﬁne-tuned each to predict an outcome\nthrough a classiﬁcation. As for a classiﬁcation task, even\nthough the [CLS] token can be used alone for a classiﬁcation,\nthe authors of [20] recommend trying different approaches.\nFor our case, we averaged the four last layers by vector-wise\nsummation to obtain a sentence embedding vector as an input\nto train a logistic regression classiﬁer computing a binary\nprobability.\nP(outcome =1|hn) =ArgMax(Whn), (4)\nwhere hn is the averaged output of n hidden layers and W is\nthe parameter matrix of the classiﬁer.\nC. EXPERIMENT DESIGN\nUsing free-text narratives from the MIMIC-III database,\nwe propose a model that predicts a binary outcome utilizing\nthe NLP process from the cleaning stage to the prediction.\nAs shown in Fig. 1, our experiment was conducted in three\nmain steps. The ﬁrst is data gathering and selection. As we\ndescribed, only patients with pneumonia as the primary dis-\nease were chosen for our experiment.\nThe second step is related to data preprocessing. To prepare\ndata for models and improve its quality, this study proposes\nthree cleaning methods that lead to three different datasets.\nWe generated sets with minor cleaning, thorough cleaning,\nand medical entities extraction(NER). For simplicity, we will\ncall these sets A, B, and C.\nThe third step is about optimizing the NLP machine learn-\ning models. This research demonstrates the performances\nof different machine learning algorithms to use static and\ncontextualized word embeddings.\n1) STATIC WORD EMBEDDINGS\nThey map each word to a single vector. Moreover, these vec-\ntors are dense and have much lower dimensionality than the\nsize of the vocabulary. For this reason, we utilize two different\nvectorizations; a very simple document vectorization using\ncount vectorization and TF-IDF. Such models ignore the\nmeaning and context of a word in a document, for example,\nthe word ‘‘pneumonia’’ will have the same value as ‘‘cancer’’\nas long as they have the same number of occurrences in\nthe document. These methods should then be applied to a\nbag of words that do not have any sequential relationship\nsuch as entities extracted by NER methods.The idea is to\nuse medical terms with low frequency by n-gram (n =1)\nvectors to achieve better discrimination for our classiﬁcation.\nWe passed the resulting vectors to a logistic regression model\nfor a classiﬁcation. We set the maximum number of iterations\nto 5000, the solver to liblinear and kept other parameters to\ntheir default values. As for the more advanced static word\nembedding method, we conducted this study with Global\nvectors for word representation (GloVe) [36] that has been\npretrained on a Wikipedia dataset and has approximately\n6000 words represented each by a vector size of 300. These\npretrained word embeddings vectors include the sequential\ndimension, which can be well learned by sequential models\nsuch as LSTM and BiLSTM.\n2) DYNAMIC WORD EMBEDDINGS\nTo analyze the importance of contextualized word embed-\ndings for medical free text, we relied on 12 layers of language\nrepresentation models (BERT, BioBERT, and ClinicalBERT).\nTo handle long narratives, we shrunk them into small sen-\ntences of 380 words, to leave some room for the tokenization\nprocess that will output 512 tokens.\nD. EXPERIMENT SETTING\nTo evaluate the effectiveness of using text notes to predict\noutcomes, we hypothesized that we should make predictions\n16494 VOLUME 10, 2022\nC. Mugisha, I. Paik: Comparison of Neural Language Modeling Pipelines for Outcome Prediction\nTABLE 2. Evaluation and comparison of BOW-based models. A logistic\nregression classifier used a unigram representation from count-vectorizer\nand TF-IDF of 1000 and 5000 vocabulary size, respectively.\nas soon as a patient is admitted to the ICU. We then decided to\nsample our test set by utilizing only admission notes. Within\nthe database, no tags were available to determine admission\nnarratives among others. Using SQL queries, we ﬁltered from\nthe database on this criterion: an admission note =Unique\nper admission ID & taken by nurse & the ﬁrst taken within\n24 hours. This was sampled as a test set and we divided\nthe rest into 90% for training and 10% for the validation set\nin order to have a separate validation set. The training and\nvalidation sets comprised progress, nursing, and procedure\nnotes. The contextualized encoding has a limitation in terms\nof sentence lengths. However, medical narratives are usually\ntoo long, without any indication of which part contains the\nmost useful information. This constrained us to truncate each\nlong note to a size of 380 words to be used in all embeddings.\nThat transformation changed our dataset from 85,085 long\nnotes to 1,101,524 notes of a maximum size of 380 words.\nOn average, each note produced almost 12 small consecutive\nnotes, which we labeled as their original narratives. The dis-\ntribution of our datasets will be described later in the results\n(3. Although we trained these notes separately, we averaged\nthe predicted classes to calculate the loss.\nV. RESULTS\nFor a fair comparison, given the low mortality rate in the\ndata (29%), we calculated the accuracy in terms of sensitivity\nand speciﬁcity respectively by recall and precision metrics.\nAs an overall evaluation metric, we reported the F1-scores\nand balanced accuracy ( 1\n2 ∗TP\nP ∗TN\nN ) as well as Matthews\ncorrelation coefﬁcient (MCC) within the Table 3. In contrast,\nwe evaluated each model with different datasets resulting\nfrom our cleaning methods. This evaluation was conducted on\nthe aforementioned test set made from admission notes. The\nscores demonstrate how well each model with a particular\npreprocessing can predict the outcome using the information\ndescribed by admission notes.\nFrom the very basic BOW, Table 2 shows that vectorization\nfrom NER leads to better results than a thorough cleaning\nprocess by all metrics. Between Count-vectorizer and TF-\nIDF, the latter performs better with accuracy, recall, and\nF1-score of 0.801, 0.993, and 0.889, respectively.\nTable 3 shows a deep comparison of the static and con-\ntextualized embeddings as well as the accuracy of models to\npredict the outcome. For LSTM and BiLSTM, we performed\na k =10-fold cross-validation, using the training set and we\ntrained both models for 10 epochs. Contextualized embed-\ndings demonstrated a higher ability to understand the medical\nnarratives than the static embeddings with a difference of\n6% between their respective best scores. For static embed-\nding, BiLSTM shows a better F1-score of 92.01% using the\nB dataset, however, using independent entities from the C\ndataset, BOW outperforms LSTM with an F1-score of 88.9%\nfrom TF-IDF against 54.6% from LSTM.\nA. PERFORMANCE ON THE BEST-PERFORMING MODELS\nFig. 3 reports values obtained from the training of contex-\ntualized word representation using different word-piece tok-\nenizations and embeddings. Extensive training of 50 epochs\non the B dataset shows that BioBERT and ClinicalBERT have\na more stable logarithmic training loss curve while BERT\nneeds more training epochs. This is also illustrated by the\nvalidation (Fig. 3b) and the test (Fig. 3c), where BioBERT\nand ClinicalBERT performed similarly but BERT needed\nmore than 20 epochs to gain stability.\nHowever, even though BERT was pretrained on the gen-\neral corpus, they all showed good discriminatory power as\ndescribed in Table 3 with F1-scores of 98.2%, 97.4%, and\n98.2%. Although these scores seem to be close, the precision\nscores of 98.1%, 96.7%, and 97.4% show a clearly inferior\nability for BioBERT to handle unbalanced data. The best-\nperforming model made use of the B dataset demonstrating\nthe importance of a deeper cleansing process before train-\ning a contextualized model. Improvements of 9.79%, 7.7%,\nand 4.3% for MCC scores were observed among BERT,\nBioBERT, and ClinicalBERT, respectively.\nB. ADDITIONAL ANALYSIS\nWord and document embedding are starting points to repre-\nsent any knowledge behind the input text. To understand how\neach model and embedding have a different interpretation of\nthe medical notes, we tried to elucidate that contrast with\nvector similarity. Once we have vectorized our narratives,\nstatistical similarity methods can be used. However, on one\nhand, multidimensional vectorization such as BERT cannot\nbe properly reshaped into two dimensions without losing\ntheir important information. On the other hand, the lack of\nsemantic and contextual information for BOW-based models\nconstitutes a handicap to initiate any clustering behavior from\nthe beginning of the NLP pipeline.\nAs shown in Fig. 4, using cosine similarity distance,\nit was clear that there is no evidence of cluster between\nfeatures representing the two classes. With random samples,\nwe ordered six positive and six negative notes and we tested\nthe effect of using cosine similarity to look for any similarity\nbetween our two classes. None of the noncontextualized mod-\nels demonstrated such ability. However, using Uniform Man-\nifold Approximation and Projection (UMAP), we reduced the\ndimension of contextualized embedding on 100 notes for each\nVOLUME 10, 2022 16495\nC. Mugisha, I. Paik: Comparison of Neural Language Modeling Pipelines for Outcome Prediction\nTABLE 3. Results from global vectors and BERT-based vectorization.\nFIGURE 3. BERT-based models training. Each model was trained for 50 epochs using a validation set of 20% to control the over-fitting, this shows a\ncomparison of the training, validation, and accuracy of BERT, BioBERT, and ClinicalBERT.\nFIGURE 4. Cosine similarity for noncontextualized models. This shows a sample of 12 inputs, where the first 6 are notes of discharged patients and\nothers are for deceased patients.\nclass and the Fig. 5 shows clear clusters of the two classes\nbefore the learning and classiﬁcation processes.\nTo analyze the certainty of advanced models through\nthe prediction probabilities, we utilized the same random\n200 samples. We extracted the logits from the last layer of\neach model, before being transferred to the activation func-\ntion. Fig. 6 shows a distribution set of unnormalized scores\nθ from the three models, and each class is represented by\n100 consecutive samples. It demonstrates that BioBERT and\nClinicalBERT have a constant better distance score between\nthe two classes than BERT. This means that if a score is\nclose to 0, the probability for that sequence to fall under a\ncertain class is around 0.5 which can be translated as a low\nconﬁdence score for that prediction.\nVI. DISCUSSION\nWith this research, pneumonia patients were selected among\nICU EHR data. Through the NLP pipeline, we aim to demon-\nstrate the ability to predict outcomes using the narratives\ntaken during a patient’s stay by comparing the existing NLP\napproaches. Cleaning thoroughly improves the contextual\nunderstanding of the inputs as demonstrated by Clinical-\nBERT with an MCC score of 4.3%.\nBERT-based domain-speciﬁc models can perform slightly\nbetter than the general BERT but the difference resides\nmainly on the convergence time between the training and\nthe validation process. In case we want to use independent\nentities, such as NER, BOW models are more suitable to\nhandle prediction tasks because only term frequency over\ninverse document frequency is more relevant. Nonetheless,\nthe preprocessing methods may change with a different EHR,\nthat utilizes a different notation. Besides the performance of a\nprediction model, it should be interpretable. For our case, the\naccuracy of the prediction should be quantiﬁed by diagnosis,\ndrugs, bioinformation, or even demographics. However, the\nhigh dimensionality of modern NLP models contains abstract\n16496 VOLUME 10, 2022\nC. Mugisha, I. Paik: Comparison of Neural Language Modeling Pipelines for Outcome Prediction\nFIGURE 5. Contextualized embeddings clustering. This figures show that dimension reduction using Uniform Manifold Approximation and\nProjection (UMAP) can easily find similarities between embedding vectors from the two classes.\nFIGURE 6. Logits from 200 input samples represented byy =θpos −θneg.\nOn the left are sentences from deceased patients while on the right are\nfor discharged patients. The evaluation is on the y-axis. This shows the\npolarity of each model’s output.\nfeatures for which we have no words or mental concepts.\nTo visualize which feature or medical terminology activated\nthe model along with the input text does not necessary have\na meaning for us. Therefore, we judged that interpreting the\noutcomes from the narratives is beyond the scope of this\nresearch.\nThere are also some limitations of this study. First, we lim-\nited this analysis to pneumonia patients who stayed in the\nICU, and our prediction test used admission notes. NLP\nmodels require a lot of data to be generalizable and avoid\nover-ﬁtting the models. Therefore, our guarantee for repro-\nducibility using different domain data is limited.\nA second limitation is common to EHR-driven prediction\nmodels for supervised learning. It is rare to have sufﬁcient\nbalance between classes, however for our case, the minority\nclass represented by 29% of the data did not give an alarming\nfalse-negative for high-dimensional models and its precision\nscore was as high as the majority class.\nVII. CONCLUSION\nThrough this paper, we present a deep comparison of Neural\nLanguage modeling pipelines for outcome prediction from\nmedical text notes using pneumonia patients. We compare\nthe performance of medical notes preprocessing, their rep-\nresentation as well as a supervised learning mechanism to\npredict outcomes of ICU admissions. We demonstrated that\ntext preprocessing is of paramount importance as the ﬁrst\nstep of the pipeline. A light preprocessing will not achieve\nresults as good as a deeper processing. Replacing medical\njargon and abbreviation terms to harmonize the data will have\na high positive impact. For example, changing ‘‘dx PE’’ to\n‘‘diagnosis Pulmonary Embolism’’ will allow models to add\nmore weight to each of those tokens as related to pneumonia\nand appears multiple times. However, extreme processing\nsuch as NER will limit the applicable models besides cutting\noff some useful information from the text. The choice of\nthe embeddings depends mostly on the input type, size, and\ndomain. Current NLP models, utilizing transformers as the\nfundamental structure, understand the medical text better at\nthe expense of prediction interpretability. A meticulous data\ncleaning, and subword level representation from a medical\ndomain embedding and a ﬁne-tuned transformer-based model\nyielded better optimal results.\nREFERENCES\n[1] A. C. Morris, ‘‘Management of pneumonia in intensive care,’’ J. Emer-\ngency Crit. Care Med. , vol. 2, p. 101, Dec. 2018. [Online]. Available:\nhttps://jeccm.amegroups.com/article/view/4830\n[2] I. Rudan, K. O’Brien, H. Nair, L. Liu, E. Theodoratou, S. Qazi,\nI. Luksic, C. Walker, R. Black, H. Campbell, and G. Reference, ‘‘Epi-\ndemiology and etiology of childhood pneumonia in 2010: Estimates\nof incidence, severe morbidity, mortality, underlying risk factors and\ncausative pathogens for 192 countries,’’ J Glob Health, vol. 3, May 2013,\nArt. no. 10401.\n[3] C. Mugisha and I. Paik, ‘‘Pneumonia outcome prediction using structured\nand unstructured data from EHR,’’ in Proc. IEEE Int. Conf. Bioinf. Biomed.\n(BIBM), Dec. 2020, pp. 2640–2646.\n[4] I. Li, J. Pan, J. Goldwasser, N. Verma, W. P. Wong, M. Y . Nuzumlalı,\nB. Rosand, Y . Li, M. Zhang, D. Chang, R. A. Taylor, H. M. Krumholz,\nand D. Radev, ‘‘Neural natural language processing for unstructured data\nin electronic health records: A review,’’ 2021, arXiv:2107.02975.\n[5] B. Shickel, P. J. Tighe, A. Bihorac, and P. Rashidi, ‘‘Deep EHR: A sur-\nvey of recent advances in deep learning techniques for electronic health\nrecord (EHR) analysis,’’ IEEE J. Biomed. Health Inform., vol. 22, no. 5,\npp. 1589–1604, Sep. 2018.\n[6] R. Mann and J. Williams, ‘‘Standards in medical record keeping,’’ Clin.\nMed. London, U.K., vol. 3, pp. 329–332, Jul. 2003.\n[7] N. R. Council, Biomedical Models and Resources: Current Needs\nFuture Opportunities. Washington, DC, USA: National Academies Press,\n1998. [Online]. Available: https://www.nap.edu/catalog/6066/biomedical-\nmodels-and-resources-curren%t-needs-and-future-opportunities\nVOLUME 10, 2022 16497\nC. Mugisha, I. Paik: Comparison of Neural Language Modeling Pipelines for Outcome Prediction\n[8] J. L. Schlossberg, ‘‘Book review: Medical language processing: Computer\nmanagement of narrative data by naomi sager, carol friedman, and margaret\nS. Lyman (Addison–Wesley 1987),’’ ACM SIGCHI Bull., vol. 20, no. 1,\npp. 70–71, Jul. 1988, doi: 10.1145/49103.1046397.\n[9] N. El-Rashidy, S. El-Sappagh, T. Abuhmed, S. Abdelrazek, and\nH. M. El-Bakry, ‘‘Intensive care unit mortality prediction: An improved\npatient-speciﬁc stacking ensemble model,’’ IEEE Access, vol. 8,\npp. 133541–133564, 2020.\n[10] E. Ford, J. A. Carroll, H. E. Smith, D. Scott, and J. A. Cassell, ‘‘Extracting\ninformation from the text of electronic medical records to improve case\ndetection: A systematic review,’’ J. Amer. Med. Inform. Assoc., vol. 23,\nno. 5, pp. 1007–1015, Sep. 2016.\n[11] S. Sheikhalishahi, R. Miotto, J. T. Dudley, A. Lavelli, F. Rinaldi, and\nV . Osmani, ‘‘Natural language processing of clinical notes on chronic dis-\neases: Systematic review,’’ JMIR Med. Informat., vol. 7, no. 2, Apr. 2019,\nArt. no. e12239.\n[12] B. A. Goldstein, A. M. Navar, M. J. Pencina, and J. P. A. Ioannidis,\n‘‘Opportunities and challenges in developing risk prediction models with\nelectronic health records data: A systematic review,’’ J. Amer. Med. Inform.\nAssoc., vol. 24, no. 1, pp. 198–208, Jan. 2017.\n[13] V . Osmani, L. Li, M. Danieletto, B. Glicksberg, J. Dudley, and O. Mayora,\n‘‘Automatic processing of electronic medical records using deep learning,’’\nin Proc. 12th EAI Int. Conf. Pervasive Comput. Technol. Healthcare,\nMay 2018, pp. 251–257.\n[14] H. Liu, Y . Lussier, and C. Friedman, ‘‘A study of abbreviations in\nthe UMLS,’’ in Proc. AMIA. Annu. Symp. AMIA Symp., Feb. 2001,\npp. 393–397.\n[15] G. Maragatham and S. Devi, ‘‘LSTM model for prediction of heart failure\nin big data,’’ J. Med. Syst., vol. 43, no. 5, pp. 1–13, May 2019.\n[16] R. AlSaad, Q. Malluhi, I. Janahi, and S. Boughorbel, ‘‘Interpreting patient-\nspeciﬁc risk prediction using contextual decomposition of BiLSTMs:\nApplication to children with asthma,’’ BMC Med. Informat. Decis. Making,\nvol. 19, no. 1, pp. 1–11, Dec. 2019.\n[17] X. Zhou, Y . Li, and W. Liang, ‘‘CNN-RNN based intelligent recommenda-\ntion for online medical pre-diagnosis support,’’ IEEE/ACM Trans. Comput.\nBiol. Bioinf., vol. 18, no. 3, pp. 912–921, May 2021.\n[18] J. Geraci, P. Wilansky, V . de Luca, A. Roy, J. L. Kennedy, and J. Strauss,\n‘‘Applying deep neural networks to unstructured text notes in electronic\nmedical records for phenotyping youth depression,’’ Evidence Based Men-\ntal Health, vol. 20, no. 3, pp. 83–87, Aug. 2017.\n[19] Y . Wang, S. Sohn, S. Liu, F. Shen, L. Wang, E. J. Atkinson, S. Amin, and\nH. Liu, ‘‘A clinical text classiﬁcation paradigm using weak supervision and\ndeep representation,’’ BMC Med. Informat. Decis. Making, vol. 19, no. 1,\npp. 1–13, Dec. 2019.\n[20] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ 2018,\narXiv:1810.04805.\n[21] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang,\n‘‘BioBERT: A pre-trained biomedical language representation model for\nbiomedical text mining,’’ Bioinformatics, vol. 36, no. 4, pp. 1234–1240,\n2020.\n[22] K. Huang, J. Altosaar, and R. Ranganath, ‘‘ClinicalBERT: Modeling clin-\nical notes and predicting hospital readmission,’’ 2019, arXiv:1904.05342.\n[23] A. Johnson, T. Pollard, L. Shen, L.-W. Lehman, M. Feng, M. Ghassemi,\nB. Moody, P. Szolovits, L. Celi, and R. Mark, ‘‘MIMIC-III, a freely acces-\nsible critical care database,’’ Sci. Data, vol. 3, May 2016, Art. no. 160035.\n[24] O. Bodenreider, ‘‘The uniﬁed medical language system (UMLS): Inte-\ngrating biomedical terminology,’’ Nucleic acids Res., vol. 32, p. D267,\nFeb. 2004.\n[25] L. V . Grossman, E. G. Mitchell, G. Hripcsak, C. Weng, and D. K. Vawdrey,\n‘‘A method for harmonization of clinical abbreviation and acronym sense\ninventories,’’J. Biomed. Informat., vol. 88, pp. 62–69, Dec. 2018.\n[26] G. Finley, S. Pakhomov, R. McEwan, and G. Melton, ‘‘Towards compre-\nhensive clinical abbreviation disambiguation using machine-labeled train-\ning data,’’ AMIA Annu. Symp. Proc., vol. 2016, pp. 560–569, Feb. 2017.\n[27] H. Xu and P. Stetson, ‘‘A study of abbreviations in clinical notes,’’ in Proc.\nAMIA. Annu. Symp. AMIA Symp. AMIA Symp., Feb. 2007, pp. 821–825.\n[28] J. Rello, A. Rodriguez, A. Torres, J. Roig, J. Violán, J. Garnacho-Montero,\nM. Torre, J. Sirvent, and M. Bodí, ‘‘Implications of copd in patients\nadmitted to the intensive care unit by community-acquired pneumonia,’’\nEur. respiratory J., Off. J. Eur. Soc. Clin. Respiratory Physiol., vol. 27,\npp. 1210–1216, Jul. 2006.\n[29] H. Çelikhisar, G. Ilkhan, and C. Arabaci, ‘‘Prognostic factors in elderly\npatients admitted to the intensive care unit with community-acquired pneu-\nmonia,’’Aging Male, vol. 23, pp. 1425–1431, Jun. 2020.\n[30] R. S. Puttini, A. A. Toffanello, R. M. Chaim, G. Alves, J. M. P. Rotzsch,\nE. O. Carvalho, E. Ishikawa, A. P. Araujo, and E. C. Oliveira, ‘‘Semantic\nframework for electronic health records,’’ in Proc. IEEE 11th Int. Conf.\nSemantic Comput. (ICSC), Jan. 2017, pp. 334–337.\n[31] H. Sun, K. Depraetere, J. D. Roo, G. Mels, B. D. Vloed, M. Twagiru-\nmukiza, and D. Colaert, ‘‘Semantic processing of EHR data for clinical\nresearch,’’J. Biomed. Informat., vol. 58, pp. 247–259, Dec. 2015.\n[32] W. Sun, Z. Cai, Y . Li, F. Liu, S. Fang, and G. Wang, ‘‘Data processing\nand text mining technologies on electronic medical records: A review,’’\nJ. Healthcare Eng., vol. 2018, pp. 1–9, Apr. 2018.\n[33] D. Kim, J. Lee, C. H. So, H. Jeon, M. Jeong, Y . Choi, W. Yoon,\nM. Sung, and J. Kang, ‘‘A neural named entity recognition and multi-\ntype normalization tool for biomedical text mining,’’ IEEE Access, vol. 7,\npp. 73729–73740, 2019.\n[34] Y . Zhang, R. Jin, and Z. Zhou, ‘‘Understanding bag-of-words model:\nA statistical framework,’’ Int. J. Mach. Learn. Cybern., vol. 1, nos. 1–4,\npp. 43–52, Dec. 2010.\n[35] T. Joachims, ‘‘Text categorization with support vector machines: Learning\nwith many relevant features,’’ in Proc. Eur. Conf. Mach. Learn. Springer,\n1998, pp. 137–142.\n[36] J. Pennington, R. Socher, and C. D. Manning, ‘‘Glove: Global vectors for\nword representation,’’ in Proc. Conf. Empirical Methods Natural Lang.\nProcess. (EMNLP), 2014, pp. 1532–1543.\n[37] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, ‘‘Distributed\nrepresentations of words and phrases and their compositionality,’’ in Proc.\nAdv. Neural Inf. Process. Syst., vol. 26, Dec. 2013, pp. 1–9.\n[38] E. Choi, T. Bahadori, and J. Sun, ‘‘Doctor AI: Predicting clinical events\nvia recurrent neural networks,’’ Nov. 2015, arXiv:1511.05942.\n[39] S. Hochreiter and J. Schmidhuber, ‘‘Long short-term memory,’’ Neural\nComput., vol. 9, no. 8, pp. 1735–1780, 1997.\n[40] P. Zhou, W. Shi, J. Tian, Z. Qi, B. Li, H. Hao, and B. Xu, ‘‘Attention-based\nbidirectional long short-term memory networks for relation classiﬁcation,’’\nin Proc. ACL, Berlin, Germany, Aug. 2016, pp. 207–212. [Online]. Avail-\nable: https://www.aclweb.org/anthology/P16-2034\n[41] I. Sutskever, O. Vinyals, and Q. Le, ‘‘Sequence to sequence learning with\nneural networks,’’ inProc. Adv. Neural Inf. Process. Syst., vol. 4, Sep. 2014,\npp. 3104–3112.\nCHERUBIN MUGISHA (Member, IEEE) rece-\nived the bachelor’s degree in computer science\nfrom the Université Lumière de Bujumbura,\nBurundi, in 2013, and the M.Sc. degree in com-\nputer science from The University of Aizu, Japan,\nin 2020, where he is currently pursuing the Ph.D.\ndegree. His research interests include algorithms\nfor multimodal machine learning methods inte-\ngrating NLP, structured and unstructured data and\nits application to medical data. He is the President\nof the IEEE University of Aizu Student Branch.\nINCHEON PAIK(Senior Member, IEEE) received\nthe M.E. and Ph.D. degrees in electronic engi-\nneering from Korea University, in 1987 and 1992,\nrespectively. He is currently a Professor with The\nUniversity of Aizu, Japan. His research interests\ninclude semantic web, web services and their com-\nposition, data mining, deep learning, and big data\nscience infrastructure. He is a member of ACM,\nIEICE, and IPSJ.\n16498 VOLUME 10, 2022",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8126339912414551
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6954442858695984
    },
    {
      "name": "Preprocessor",
      "score": 0.6714644432067871
    },
    {
      "name": "Word embedding",
      "score": 0.6397648453712463
    },
    {
      "name": "Natural language processing",
      "score": 0.5740268230438232
    },
    {
      "name": "Language model",
      "score": 0.48771294951438904
    },
    {
      "name": "Machine learning",
      "score": 0.4765932261943817
    },
    {
      "name": "Sentiment analysis",
      "score": 0.440607488155365
    },
    {
      "name": "F1 score",
      "score": 0.41446760296821594
    },
    {
      "name": "Transformer",
      "score": 0.41184136271476746
    },
    {
      "name": "Embedding",
      "score": 0.3941988945007324
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I141591182",
      "name": "University of Aizu",
      "country": "JP"
    }
  ],
  "cited_by": 24
}