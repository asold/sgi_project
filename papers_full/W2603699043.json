{
    "title": "Generative Knowledge Transfer for Neural Language Models",
    "url": "https://openalex.org/W2603699043",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5100705459",
            "name": "Sungho Shin",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5057914051",
            "name": "Kyuyeon Hwang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5113491293",
            "name": "Wonyong Sung",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2157331557",
        "https://openalex.org/W2027595342",
        "https://openalex.org/W2263232528",
        "https://openalex.org/W2110287632",
        "https://openalex.org/W2950602864",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2103763702",
        "https://openalex.org/W1602122992",
        "https://openalex.org/W2294370754",
        "https://openalex.org/W2134797427",
        "https://openalex.org/W104184427",
        "https://openalex.org/W196214544",
        "https://openalex.org/W2150355110",
        "https://openalex.org/W1810943226",
        "https://openalex.org/W2165698076",
        "https://openalex.org/W2511741338",
        "https://openalex.org/W2136933783",
        "https://openalex.org/W1690739335",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2263253503",
        "https://openalex.org/W2394569251",
        "https://openalex.org/W1873763122",
        "https://openalex.org/W2963684088",
        "https://openalex.org/W2949382160",
        "https://openalex.org/W889023230",
        "https://openalex.org/W6908809",
        "https://openalex.org/W2024490156",
        "https://openalex.org/W1850742715",
        "https://openalex.org/W2057653135",
        "https://openalex.org/W1534477342",
        "https://openalex.org/W2051267297"
    ],
    "abstract": "In this paper, we propose a generative knowledge transfer technique that trains an RNN based language model (student network) using text and output probabilities generated from a previously trained RNN (teacher network). The text generation can be conducted by either the teacher or the student network. We can also improve the performance by taking the ensemble of soft labels obtained from multiple teacher networks. This method can be used for privacy conscious language model adaptation because no user data is directly used for training. Especially, when the soft labels of multiple devices are aggregated via a trusted third party, we can expect very strong privacy protection.",
    "full_text": "Generative Knowledge Transfer for Neural Language Models\nSungho Shin1 Kyuyeon Hwang1 Wonyong Sung1\nAbstract\nIn this paper, we propose a generative knowl-\nedge transfer technique that trains an RNN based\nlanguage model (student network) using text and\noutput probabilities generated from a previously\ntrained RNN (teacher network). The text genera-\ntion can be conducted by either the teacher or the\nstudent network. We can also improve the per-\nformance by taking the ensemble of soft labels\nobtained from multiple teacher networks. This\nmethod can be used for privacy conscious lan-\nguage model adaptation because no user data is\ndirectly used for training. Especially, when the\nsoft labels of multiple devices are aggregated via\na trusted third party, we can expect very strong\nprivacy protection.\n1. Introduction\nNeural network based language models (LMs) are used in\nmany ﬁelds such as speech recognition, chatbot, sentence\ncompletion and machine translation (Mikolov et al., 2010;\nSerban et al., 2015; Spithourakis et al., 2016; Mirowski &\nVlachos, 2015; Cho et al., 2014). Training such LMs re-\nquires a large amount of training data. A straight-forward\nway of gathering a large amount of training data is to col-\nlect user data through mobile or the internet connected de-\nvices. However, since device users are increasingly reluc-\ntant to leak their privacy, it becomes important to collect\ndata while protecting the privacy. Even after training the\nLM once, it needs to be updated for the purpose of user\nadaptation or adding new expression. However, it is not\ndesired to use the user data directly.\nInstead of collecting sensitive user data, the model pa-\nrameters of a neural network adapted to a user can be\nused for training a new model by the knowledge transfer\nmethod (Hinton et al., 2014). However, this approach can\nalso cause an unwanted privacy violation by an adversary\n1Department of Electrical and Computer Engineering, Seoul\nNational University, Seoul, 08826 Korea. Correspondence to:\nSungho Shin <sungho.develop@gmail.com>, Wonyong Sung\n<wysung@snu.ac.kr>.\nthrough a machine learning model attack. If the adversary\ncan access the machine learning model, the output of the\nmodel can be used to restore the face of the individual\nused in the training (Fredrikson et al., 2015). In the case\nof text data, similar attacks are possible because an LM\ncan be used as a text generator for generating the sensi-\ntive user data used for training the model (Sutskever et al.,\n2011; Graves, 2013). Therefore, even the model parame-\nters trained with sensitive data should not allow direct ac-\ncess to the adversary.\nEnsemble of knowledge aggregation can increase the se-\ncurity of personal data. Ensemble methods combine the\nresults of multiple classiﬁers to improve the performance\nof machine learning algorithms (Dietterich, 2000). Sev-\neral studies trained private classiﬁers to produce a ﬁnal\ndistributable classiﬁer in various ways, such as averaging\nthe model parameters of teacher networks (Pathak et al.,\n2010), training hard labels by voting the ensemble of all the\nteachers (Papernot et al., 2016), or using soft labels (Hamm\net al., 2016). In this process, the ﬁnal classiﬁer mixes ran-\ndom noise, such as Laplacian or Gaussian noise, to hide\ninformation about a speciﬁc person and achieve strong dif-\nferential privacy (Dwork et al., 2006; 2014).\nIn this paper, we propose a method that efﬁciently trans-\nfers personal text data information for training a recurrent\nneural network (RNN) based LM while minimizing privacy\ninfringement. This method sends the soft labels generated\nby the teacher networks instead of sending personal data or\nmodel parameters, and the soft outputs obtained from the\nindividual users are aggregated for training a student net-\nwork by a reliable third party. The proposed GKT trains\nthe student network using only the generated data and la-\nbels without the original training data, by operating RNN\nLMs as a generative model. The text generation can be\nconducted by either the teacher or the student network.\nThis paper is composed as follows. Section 2 introduces\nthe related work and Section 3 describes the proposed GKT.\nSection 4 describes the GKT by ensemble of multiple LMs,\nSection 5 shows the experimental results, and Section 6\nconcludes this paper.\narXiv:1608.04077v3  [cs.LG]  28 Feb 2017\nGenerative Knowledge Transfer for Neural Language Models\n(a) Teacher-driven generative knowledge transfer (TDGKT)\n(b) Student-driven generative knowledge transfer (SDGKT)\nFigure 1.Two different schemes of GKT. Text sequence genera-\ntion (green lines) can be produced by the teacher (TDGKT) or the\nstudent network (SDGKT).\n2. Related Work on Knowledge Transfer\nUtilizing the knowledge contained in the previously trained\nnetworks has been of much interest for the application to\nnetwork compression or pretraining. In an early work for\nnetwork compression, a previously trained model is used\nto label a large unlabeled dataset for producing a much\nlarger training set (Bucilu et al., 2006). Another related\nwork is a knowledge transfer through the hidden Markov\nmodel (HMM) (Pasa et al., 2014). An HMM is trained us-\ning the original data, and then the generated sequence from\nthe HMM is used for pretraining of an RNN, which is then\nﬁne-tuned using the original data. In the Hinton’s knowl-\nedge distillation (Hinton et al., 2014), the output probabili-\nties of a well-trained network are used as the soft target for\ntraining a small network. In FitNet, a thick-shallow model\nis transformed to a thin-deep model (Romero et al., 2015).\nThey employ a guided layer in the student network that\ncan be pretrained from the teacher’s corresponding hidden\nlayer, and ﬁne-tuned using knowledge distillation. Also,\nthe model change from a fully connected deep neural net-\nwork (FCDNN) to an RNN or the opposite direction has\nbeen tried (Tang et al., 2016; Chan et al., 2015).\nThe main difference between the previous works and ours\nis the use of the original training data. The previous works\ntry to improve the performance of training by generating\nmore data using the developed model, but the original data\nis also used. Our study conducts training only by using\nthe trained network. Thus, the proposed approach can be\nconsidered pure knowledge transfer.\n3. Generative Knowledge Transfer for RNN\nLMs\nIn this section, we explain the generative knowledge trans-\nfer (GKT) for training RNN LMs. We employ teacher-\nstudent training scheme. The previously trained network\nis referred to as the teacher network, and the network that\nlearns from the teacher network is called the student net-\nOriginal data\nFREEMANS HASN’T DECIDED WHETHER TO APPEAL THE RULING\nFREEMAN WIGTON AND TABOR\nFREEMAN WIGTON TABOR AND MILKEN AND FOR DRESEL COULDN’T BE REACHED\nGenerated data\nFREEMAN HAS HAD A HORRIBLE IMPACT ON PLAYBOY MAGAZINE’S ALLEGED HIS \nOWN EARLY\nFREEMAN ALSO ACTED AS ADVERSARY AT ONE MAJOR STUDY THAT CITED THE \nSIZE OF HIS WORK IN THE SOUTH\nFREEMAN WIGTON TURNED HIS HANDS WITH TWENTY PEOPLE TO RESPOND TO \nHIS OPINIONS\nFigure 2.Examples of original text and generated text from the\nteacher network. The ﬁrst word is a name ”FREEMAN”, which\nis included in the original training data.\nwork. The generative knowledge transfer (GKT) proposed\nin this paper train the student network from a teacher net-\nwork or networks with only the generated data. For RNN\nLMs, the GKT can be classiﬁed as the teacher-driven GKT\n(TDGKT) and the student-driven GKT (SDGKT) as shown\nin Figure 1. Note that soft labels are always created by the\nteacher network.\nThe proposed method consists of three steps; (1) train the\nteacher network using original training data, (2) generate a\ntext sequence and soft labels to train the student network,\n(3) train the student network using the generated text data\nand soft labels.\n3.1. Teacher-driven Generative Knowledge Transfer\n(TDGKT)\nIn TDGKT, a student network is trained by creating text\nusing a teacher network as shown in Figure 1a. In our\nstudy, we use a character-level language model (CLM)\nwhich is easy to handle out of vocabulary (OOV) words\nand has a simpler structure than the word-level language\nmodel (WLM). Using a CLM, we can predict the next out-\nput probabilities as follows (Sutskever et al., 2011):\nPθteacher (xteacher\nt+1 |xteacher\n1:t ) =yteacher\nt , (1)\nwhere θteacher is the trained teacher model, t is the time\nstep, x is the input data sequences, and yt is the model\noutput at time step t (after applying a softmax activa-\ntion function). The teacher network can generate the\ntext sequence by random sampling from the distribution\nPθteacher (xteacher\nt+1 |xteacher\n1:t ).\nHowever, since the generated text sequence is quite differ-\nent from the original text sequence as shown in Figure 2,\nthe performance of the student network using only the gen-\nerated text is not as good as that of the original training\ndata. We solve this problem by using softmax output prob-\nabilities as soft training labels for the student network. A\nsmall example that proves the importance of soft labels in\nGKT training is given in Section 5.1.\nGenerative Knowledge Transfer for Neural Language Models\n3.2. Student-driven Generative Knowledge Transfer\n(SDGKT)\nThe SDGKT, which is shown in Figure 1b, generates the\ninput data sequence from the student network, whereas the\nsoft labels are from the teacher network as in the TDGKT.\nPθstudent (xstudent\nt+1 |xstudent\n1:t ) =ystudent\nt\nPθteacher (xteacher\nt+2 |xstudent\n1:t+1 ) =yteacher\nt+1\n(2)\nNote that the text sequence is sequentially sampled from\nthe output of the student network, Pθstudent (xstudent\nt+1 |\nxstudent\n1:t ). Therefore, the student network needs to be pre-\ntrained in order to generate a suitable text sequence unlike\nTDGKT.\nIn SDGKT training, there is no concept of ‘epoch’ because\nthe student network can generate data inﬁnitely. Instead,\nwe introduced the concept of ‘cycle’ and ‘lot’. The stu-\ndent network generates a text sequence with the size of a\n‘lot’, feeds it to the teacher network, and trains the stu-\ndent network with the obtained soft labels. A single repeti-\ntion of this process is called a ‘single-cycle’, and repeating\nmore than two times is called a ‘multi-cycle’. If the ‘lot’ is\n5M characters and the ‘cycle’ is 10, it means that the stu-\ndent network has generated 50M characters and the train-\ning has been repeated 10 times each with 5M characters.\nNote that the sequence is generated by the updated student\nnetwork, thus the generated sequence will gradually follow\nthe distributions of the original training data learned by the\nteacher network.\nThe ‘multi-cycle’ training plays a very important role. For\nexample, the student network can only generate the words\nin the dataset that have been used for pretraining. There-\nfore, if the OOV words that are not in the dataset used\nfor pretraining the student network were in the dataset that\ntrained the teacher network, the student network can not\nlearn it properly 1. However, if the parameters of the stu-\ndent network are updated multiple times during the train-\ning, the student network can progressively generate words\nfrom OOV words starting from the ﬁrst character, eventu-\nally learning the whole word corresponding to OOV words.\nFor example, assume that the word JANUARY is in the data\nthat trained the teacher network, but it is not in the data that\npretrained the student network. In this case, the student net-\nwork gradually generates a portion of the word JANUARY\nas “JAN→JANU →JANUA→JANUAR→JANUARY”\nas the cycle increases during SDGKT training. Thus, we\ncan learn OOV words completely after a few cycles.\n1However, student network can learn the OOV words in cer-\ntain conditions. This situation is shown in Section 5\n4. Application: Learning Language Models\nwhile Preserving User Privacy\nThe GKT described in Section 3 assumes a single teacher\nnetwork. In many cases, however, it is necessary to col-\nlect data from many users and use them for training. This\napproach not only helps to improve the performance be-\ncause it collects data from multiple people, but it also plays\na critical role in preserving privacy. The generative knowl-\nedge transfer with multiple teachers proposed in this paper\nis described in Figure 3. This ﬁgure shows SDGKT-based\nmultiple teacher networks, which can be similarly conﬁg-\nured for TDGKT with weaker privacy protection. In this\nproposed method, the pretrained student network using the\npublic (or in-house) data generates the text sequence and\nfeeds it to the teacher networks for obtaining the soft labels.\nThe soft labels generated by all teacher networks are ag-\ngregated together and then applied to the student network.\nWhen the soft labels are aggregated using a trusted third\nparty, the privacy can be greatly increased. In this section,\nthe student network is on the main server and the teacher\nnetworks are in the user devices.\nThe proposed method consists of 4 steps as shown in Fig-\nure 3. In the ﬁrst step, the LM in the main sever (student\nnetwork) is trained using public data or in-house data. The\nsecond step distributes the LM trained in the ﬁrst step to all\nthe participating user devices. In the third step, each user\ndevice ﬁne-tunes (or overﬁts) its LM with its own private\ndata. The last step is to train the LM on the main server\nusing the ensemble of soft targets obtained from the user\ndevices. At this time, the LM on the main server gener-\nates the text and sends it to the user devices, where the text\nis applied as the input to the LMs in the devices. In this\ncase, each device generates the soft labels using the given\ntext and sends it to the trusted third party. The trusted third\nparty aggregates the soft labels2 and sends them to the main\nserver for updating the LM.\nWith this procedure, the main server, which is a service\nprovider, cannot access the original soft labels from each\nuser device, thus it is difﬁcult to infringe on the privacy\nof the individual users. The trusted third party does not\nknow any information about the input sequence, and thus\nit cannot violate the privacy of users. In other words, even\nthe trusted third party that collects soft labels does not have\nthe whole information about the individual data.\nTherefore, even if an external hacking attack exposes data\nfrom either the trusted third party or the main server, it is\ndifﬁcult for the adversary to utilize the information. Unless\n2We can also apply differential privacy mechanism (Dwork\net al., 2014) by mixing random noise during the aggregation of\nsoft labels. However, we do not inject random noise because it is\nout of the scope of this paper.\nGenerative Knowledge Transfer for Neural Language Models\nFigure 3.Overview of our system for learning the language trends with preserving privacy. (1) The teacher and the student networks\nare trained using public or in-house data; (2) the teacher networks are ﬁne-tuned (or overﬁtted) using their private data; (3) the student\nnetwork generates text sequence and feeds it to the teacher networks to obtain the ensemble of soft labels; (4) the student network is\ntrained using the aggregated soft labels and the generated sequence.\nboth the main server and the trusted third party are intruded,\nthe privacy of individual users can be kept.\nThe training method with an ensemble of soft labels was\npreviously proposed for private learning in Hamm et al.\n(2016). However, GKT training does not require auxiliary\nunlabeled data, so it has the advantage that no additional\nhuman intervention is required. The information gathered\nfrom many user devices also allows the server (student net-\nwork) to adapt the LM for learning new words, expressions,\nor trendy dialogue styles.\n5. Experimental Results\nThe RNN-based CLM is used to utilize the OOV words\ngeneration capability of the model. The RNN is based\non a deep long short-term memory (LSTM) network and\nNxM LSTM means M LSTM layers each containing N\nmemory cells (Hochreiter & Schmidhuber, 1997; Graves\net al., 2013). The input of this CLM is a 30-dimensional\nvector that is one hot encoded for representing alphabets\n(A∼Z) and four special symbols ( space, < eos >, ’, .).\nThe output vector represents the probabilities of characters\nand symbols, and is also represented as a 30-dimensional\nvector. The CLMs are trained using the Wall Street Jour-\nnal LM training text with non-verbalized punctuation (Paul\n& Baker, 1992). The RNN training employs the truncated\nbackpropagation through time (BPTT) algorithm (Werbos,\n1990) with Adadelta (Zeiler, 2012) and Nesterov momen-\ntum (Sutskever et al., 2013). We report the performance\nusing bits per character (BPC), which is the standard per-\nTable 1.BPCs of the teacher and student networks for the simple\nTDGKT example. The original WSJ text data is used for training.\nStudent Teacher\nSize of network 256x2 512x2 1024x4\nBPC 1.275 1.148 1.101\nformance measure for CLMs.\n5.1. A Simple TDGKT Example\nWe ﬁrst examine the effect of the soft labels in GKT train-\ning as mentioned in Section 3.1. Table 1 shows the BPCs of\nteacher and student networks, where the original WSJ text\ndata is used for training. The 256x2 and the 512x2 conﬁg-\nurations are chosen as the student networks and 1024x4 as\nthe teacher networks in this experiment.\n5.1.1. C ONVERGENCE SPEED OF CONVENTIONAL AND\nGKT T RAINING\nIn order to evaluate the performance of the proposed train-\ning method, we estimate the difference between the hard\nand soft targets, and also the effects of using the original\ntraining data and the teacher generated sequences. Thus,\nthere are four different experiments. The ﬁrst one uses the\nhard target with the original training data. The second one\nemploys the soft target with the original training data. The\nthird method uses the hard target with the teacher gener-\nated sequences. The fourth one utilizes the soft target with\nGenerative Knowledge Transfer for Neural Language Models\n0 5 10 15 20 25\n1.3\n1.35\nNumber of frames trained (·5 · 108)\nBits per Character\nOS (215M) + HT(baseline)OS (50M) + HTOS (215M) + STTeacher GS + HTTeacher GS (10M) + STTeacher GS (50M) + STTeacher GS (100M) + STTeacher GS (215M) + STTeacher GS (250M) + ST\n(a) 256x2\n0 5 10 15 20\n1.15\n1.2\n1.25\n1.3\n1.35\nNumber of frames trained (·5 · 108)\nBits per Character\nOS (215M) + HT(baseline)OS (50M) + HTOS (215M)+ STTeacher GS (10M) + STTeacher GS (50M) + STTeacher GS (100M) + STTeacher GS (150M) + STTeacher GS (215M) + STTeacher GS (250M) + ST\n(b) 512x2\nFigure 4.Convergence curves in terms of BPC on the validation\nset in the ﬁxed learning rate of 1e-5. “OS” is original training\nsequence. “HT” and “ST” are hard and soft targets. “Teacher G”\nis the generated sequences from teacher network and the number\nin the parenthesis are the sizes of the sequences for performance\nevaluation.\nthe teacher generated sequences. The ﬁrst one represents\nthe conventional training method, while the fourth one is\nthe GKT. The experiments are conducted to know the per-\nformance of the student network when the amount of the\ntraining data generated by the teacher network is limited,\nsuch as 10M, 50M,100M, 150M, 215M, and 250M char-\nacters. Note that the original data have 215M characters.\nAlthough it is possible to generate an inﬁnite length of data\nusing the teacher network, we try to know the efﬁciency\nof teacher-student knowledge transfer by limiting the data\nsize. The training is conducted employing many epochs\nuntil the performance saturates.\nFigure 4 shows the training curves of the 256x2 and 512x2\nLSTM RNNs for the four different training methods. As\nfor the reference, the training result using the original data,\nWSJ LM training text, and hard target is given. Also, the\ntraining result that uses the generated sequence but not the\nsoft target is also shown. Although the generated data of\n900M characters is used for this hard target based teacher-\nstudent transfer learning, the result is not sufﬁcient in Fig-\nure 4a. The BPC only reaches to 1.311 with the valida-\ntion set. The remaining ﬁve graphs show the training re-\nsults using the generated sequence and the soft target of\nthe teacher network, where the data size is intentionally\nlimited. We can ﬁnd that almost comparable performance\ncan be achieved by applying the proposed method when\nthe generated sequence size is 100 M characters, which is\nsmaller than that of the original data size. The learning\nspeed is even faster than that using the original data. This\nspeed-up is due to the knowledge transfer effect (Hinton\net al., 2014). Finally, the Figure 4a also shows the training\nresult that utilizes both the original data and the soft target,\nwhich apparently shows the best results.\nIn Figure 4b, the data size is deliberately limited to 10M,\n50M, 100M, 150M, 215M, and 250M characters in 512x2\nLSTM network which has a higher capacity than the 256x2\nLSTM network. The comparable performance can be\nachieved when the generated sequence size is 215 M char-\nacters, but it needs more training frames to reach the base-\nline performance. With the 250M generated characters, the\nconvergence speed is even faster than that using the origi-\nnal data with the hard targets, and almost reaches the per-\nformance of the original data and the soft targets.\nThe ﬁnal training results are reported in Table 2. The base-\nline which is trained using the original sequence and the\nhard target is 1.329 and 1.275 with 50M and 215M charac-\nters, respectively. The proposed method shows better BPC\nin 50 M characters and achieves the same BPC with 215M\ncharacters. Even slightly better BPC is observed in 250M\ncharacters, which is the same result with the original se-\nquence and the soft target training. The 512x2 network is\nalso trained fairly well with the TDGKT. However, when\nwe compare the 256x2 and the 512x2 networks, the gener-\native transfer for the latter network seems less satisfactory.\nThe 512x2 network also shows better BPC in 50M charac-\nters, but slightly worse result even if the 250M characters\nare need for the training.\n5.2. Privacy Conscious Language Model Adaptation\nusing Multiple Teacher Devices\nThis sub-section shows the experimental setup and results\nof the multiple teacher device based training proposed\nin Section 4.\n5.2.1. E XPERIMENTAL SETUP\nData partitioning: The WSJ corpus used in this paper\nconsists of about 215M characters. About 98% of the\ndata is designated as the training set (210M characters)\nand the remaining 2% data is divided into the validation\n(2M characters) and the test sets (2M characters). Private\ndata for training the user LM was created by intention-\nally removing some words from the WSJ corpus. To be\nspeciﬁc, sentences containing vocabulary for the months\n(JANUARY , FEBRUARY , ..., and DECEMBER), days of\nthe week (MONDAY , TUESDAY , ..., and SUNDAY), sea-\nGenerative Knowledge Transfer for Neural Language Models\nTable 2.Comparison of TDGKT according to the amount of generated data. The BPCs on the test set are measured.\nTDGKT (GS+ST) OS+HT (Baseline) OS+ST\nSize 10M 50M 100M 150M 215M 250M 50M 215M 215M\n256x2 1.371 1.288 1.283 1.283 1.275 1.272 1.329 1.275 1.272\n512x2 1.314 1.196 1.177 1.168 1.165 1.151 1.274 1.148 1.146\nsons (SPRING, SUMMER, AUTUMN, and WINTER)\nwere separated from the WSJ corpus. The size of the sep-\narated private text data is about 29 M characters, which is\nabout 14% of the original corpus size. As a result, the re-\nmaining public (or in-house) data has 181M characters. For\nperformance evaluation, the validation and the test sets are\nclassiﬁed into three kinds as follows.\n1. Valid private, Testprivate: the data consisting of sentences\ncontaining the private words in the validation and the\ntest sets, respectively.\n2. Valid public, Testpublic: the data consisting of sentences\nthat do not contain the private words in the validation\nand the test sets, respectively.\n3. Valid full, Testfull: the data consisting of the original\nvalidation and test sets, respectively.\nTraining the main server and user device LMs: The\nmain server is trained with the public data (or in-house\ndata), which do not includes words for representing\nmonths, days of the week, and seasons. Before the training\nof the user device with the private data, we considered two\ndifferent network parameter initialization. The ﬁrst is to\nrandomly initialize the user device LM weight parameters,\nand the second is kind of a transfer learning (Pan & Yang,\n2010), where the weight parameters of the user device LM\nis copied from the main server LM, which was trained with\npublic (or in-house) data. For the latter method, the size of\nthe main server LM and that of the user device LMs must be\nthe same size and structure, so the LM for all user devices\nand the main server adopt 256x2 LSTM RNNs in this ex-\nperiment. Table 3 shows the results of the user device LM\ntrained using private data in both initial states and also the\nperformance when trained using all data (private + public).\nThe dataset that each user device LM uses during training\nis summarized as follows:\n1. T full: train with the public and private data simultane-\nously\n2. T transfer: pretrain with the public data and then retrain\n(ﬁne-tune) with the private data\n3. T private: train with the private data only\nTable 3.BPC on the validation and the test set for a user device\nLM with the standard training.\nFull Private Public\nTfull\nValid 1.271 1.151 1.285\nTest 1.275 1.167 1.288\nTtransfer\nValid 1.395 1.131 1.426\nTest 1.397 1.147 1.427\nTprivate\nValid 1.486 1.177 1.522\nTest 1.490 1.198 1.524\n5.2.2. E VALUATION OF SDGKT WITH A SINGLE USER\nDEVICE LM\nWe ﬁrst evaluate SDGKT in Figure 1b with a small user de-\nvice LM (teacher network)3. Speciﬁcally, two experiments\nare conducted to examine the inﬂuence of the ‘cycle’ men-\ntioned in Section 3.2. In the ﬁrst experiment, the student\nnetwork is trained with 200M characters of generated data\nwith a single-cycle of SDGKT. On the other hand, in the\nsecond experiment, SDGKT is performed through multi-\nple cycles, where 5M training characters are generated per\ncycle (‘lot’). Figure 5 and Figure 6 show the convergence\ncurve of SDGKT by employing three different types of user\ndevice LMs.\nFrom the results of the single-cycle experiment in Figure 5,\nwe observed that the OOV words can be learned only when\n‘Ttransfer’ is employed as a teacher network in the user de-\nvice. However, in the multi-cycle training results in Fig-\nure 6, the OOV words can be learned with all the three types\nof teacher networks, and the training speed is much faster\ncompared to the single-cycle experiment. This is because\nthe OOV words cannot appear in the generated training text\nwith SDGKT in the initial cycle of the training. Therefore,\nOOV words such as ‘JANUARY’ or ‘DECEMBER’ are\ndifﬁcult to be learned with the single-cycle training. Even\nif the soft labels are generated from the teacher network,\nwhich is aware of the OOV words, the soft labels generated\nin the initial cycle only contains the information of single\ncharacter prediction probabilities from the given generated\ntext. However, in the multi-cycle experiments, the partial\nOOV words such as “JANU, JANUA, and JUNUAR” grad-\nually appear in the generated text as the cycle progresses,\n3This experiment is the simple example of SDGKT mentioned\nin Section 3.2.\nGenerative Knowledge Transfer for Neural Language Models\nTable 4.BPC on the test sets with the single-cycle and multi-cycle\nSDGKT methods using a single teacher network.\n# of cycles Test full Testprivate Testpublic\nTfull\nSingle 1.356 1.396 1.351\nMulti 1.376 1.330 1.381\nTtransfer\nSingle 1.352 1.153 1.375\nMulti 1.390 1.148 1.419\nTprivate\nSingle 1.447 1.381 1.455\nMulti 1.479 1.286 1.501\nand as a result, all the three student networks are able to\nlearn OOV words.\nNevertheless, even in the single-cycle experiment the\n‘Ttransfer’ was able to teach OOV words to the student net-\nwork. Interestingly, the similar observation was reported\nby Hinton et al. (2014) with feedforward deep neural net-\nworks. In this experiment, knowledge transfer was per-\nformed between two digit classiﬁcation networks, where\nit was shown that even if images of the number 3 was not\ngiven to the teacher network during the knowledge transfer,\nthe student network was able to correctly classify the image\nof the number 3 to some extent. However, further analysis\nis required to reach the general explanation for this obser-\nvation in the case of RNNs.\nAs the training progresses, the BPC measured on the pri-\nvate validation set gradually improves with the sacriﬁce of\nthe BPC on the validation set of the public data. This is\nbecause the student network in the main server, which is\ntrained with the public data, becomes adapted to the dis-\ntribution of the private training data that is used for train-\ning the user device LM. As a result, the student network\nis ﬁne-tuned to the user data without accessing this data\ndirectly. Table 4 shows the results on the three test sets\nwith the three types of teacher networks. From the table, it\nis observed that the user device LM trained with ‘T transfer’\nmethod shows the best performance on the private test data,\nwhich means that the transfer learning method is suitable\nfor training the user device LM if the gradual adaptation\nof the main server LM to the private user data is the main\nconcern. Therefore, we only employ ‘T transfer’ for training\nthe user device LMs in the following multi user device ex-\nperiments in Section 5.2.3.\n5.2.3. GKT WITH THE ENSEMBLE OF USER DEVICE\nLMS\nIn this section, experiments are conducted with multiple\nuser LMs for language model adaptation while minimiz-\ning privacy infringement. For the experiments, we divide\nthe private data into 100 sets, where each set is exclusively\ngiven for training a single user device LM. As a result, each\nuser device LM is trained with only about 300K characters\n0 50 100 150 200\n1.1\n1.2\n1.3\n1.4\n1.5\nNumber of frames trained (·5 · 106)\nBits per Character\n(a) Tfull\n0 50 100 150 200\n1.1\n1.2\n1.3\n1.4\n1.5\nNumber of frames trained (·5 · 106)\nBits per Character\nValidfull(Student)\nValidprivate\nValidpublic\nValidfull(Teacher)\nValidprivate\nValidpublic\n(b) Ttransfer\n0 50 100 150 200\n1.1\n1.2\n1.3\n1.4\n1.5\nNumber of frames trained (·5 · 106)\nBits per Character\n(c) Tprivate\nFigure 5.Convergence curves in terms of BPC on validation set\nfor the single-cycle training. Dotted lines represent the BPC of\nthe teacher networks which are reported in Table 3, and solid lines\nshow the BPC of the student networks with SDGKT training.\nof private data. In the following experiments, we set the\nnumber of user devices to 10, 50, and 100. The soft la-\nbels, which are delivered to the main server LM, are ob-\ntained by averaging the output soft target values generated\nfrom the user device LMs by employing the trusted third\nparty. In brief, the main server LM is trained with the gen-\nerated text sequence as the input and the ensemble of the\ncorresponding soft targets as the target. Table 5 shows the\naveraged BPC over the user device LMs and the ensem-\nble BPC, which is obtained from the aggregated outputs\nfrom the user device LMs. Table 6 shows the training re-\nsult of the main server LM with the SDGKT and TDGKT\nschemes. Note that ‘T’ indicates the TDGKT scheme,\nand ‘S’ denotes the SDGKT scheme, which are described\nin Figure 1. By comparing the results with the ones in Ta-\nble 5, we can see that BPC of the main server LM is bet-\nter than the average BPC of the user device LMs since the\nmain server LM is trained with the ensemble of the user\nGenerative Knowledge Transfer for Neural Language Models\n0 50 100 150 200\n1.1\n1.2\n1.3\n1.4\n1.5\nNumber of frames trained (·5 · 106)\nBits per Character\nValidfull(Student)\nValidprivate\nValidpublic\nValidfull(Teacher)\nValidprivate\nValidpublic\n(a) Tfull\n0 50 100 150 200\n1.1\n1.2\n1.3\n1.4\n1.5\nNumber of frames trained (·5 · 106)\nBits per Character\n(b) Ttransfer\n0 50 100 150 200\n1.1\n1.2\n1.3\n1.4\n1.5\nNumber of frames trained (·5 · 106)\nBits per Character\n(c) Tprivate\nFigure 6.Convergence curves in terms of BPC on validation set\nfor the multi-cycle training. Dotted lines represent the BPC of the\nteacher networks which are reported in Table 3, and solid lines\nshow the BPC of the student networks with SDGKT training.\ndevice LMs. As mentioned in Section 4, our proposed sys-\ntem employs SDGKT over TDGKT for stronger privacy\nprotection, however, the result of the TDGKT training is\nalso reported for comparison. In the case of the TDGKT,\nthe training text is generated by a randomly selected user\ndevice and broadcasted to the other user devices. As can\nbe seen in Table 6, both the TDGKT and SDGKT method\nshowed very similar performance.\nThe BPC of the LM utilizing the private data improves\nwhen increasing the number of devices. The performance\nwith 50 devices was slightly better than that with 10 de-\nvices, whereas it is not much different from that with 100\ndevices. Compared with the performance of the server\nLM, which is pretrained using only public data, the BPC\nof the server LM on the private test data is improved after\nSDGKT. On the other hand, the BPC is slightly degraded\non the public test data. This is because the server LM be-\ncomes adapted to the words and grammars in the private\nTable 5.BPC on the test set for teacher networks. ‘A’ is average\nof the teachers BPCs, and ‘E’ is BPC of the teachers ensemble.\n# of Teachers Test full Testprivate Testpublic\n10 A 1.345 1.269 1.358\nE 1.323 1.246 1.335\n50 A 1.347 1.269 1.359\nE 1.321 1.242 1.333\n100 A 1.346 1.269 1.359\nE 1.321 1.242 1.333\nTable 6.BPC on the test set for main server LM. ‘T’ and ‘S’ in-\ndicate the TDGKT and the SDGKT. The BPC of the initial main\nserver LM is 1.328, 1.496, and 1.301 for Test full, Testprivate, and\nTestpublic data respectively.\n# of Teachers Test full Testprivate Testpublic\n10 T 1.325 1.252 1.336\nS 1.324 1.250 1.336\n50 T 1.323 1.247 1.335\nS 1.323 1.248 1.334\n100 T 1.323 1.247 1.334\nS 1.322 1.247 1.334\nuser data by slightly forgetting the pretrained expressions\nthat is not frequently used in the user data. Note that as in\nthe single user device experiments, the server LM can fol-\nlow the language trends by gradually learning OOV words\nsuch as newly coined words or trendy expressions.\n6. Concluding Remarks\nThroughout the paper, generative knowledge transfer\n(GKT) techniques are proposed for RNN LMs, where\nknowledge transfer from the teacher network to the stu-\ndent network is performed with the text data generated by\none of the networks. In teacher-driven GKT (TDGKT), the\ntraining text is generated by the teacher network, whereas\nin student-driven GKT (SDGKT), the text generation is\nperformed by the student network. Although the train-\ning text is generated by the student network in SDGKT,\nwe showed that it is able to transfer the knowledge of the\nOOV words, which only the teacher network is aware of,\nto the student network by employing multi-cycle SDGKT.\nAlso, the SDGKT provides strong privacy protection when\napplied to the presented privacy-preserving LM adaptation\ntask between the main server and the multiple user devices.\nThe experimental results show that SDGKT allows efﬁcient\ntransfer of the knowledge contained in the private user text\ndata, such as newly coined words or trendy expressions, to\nthe RNN LM in the main server without direct access to the\nuser data, thereby preserving the privacy of the users.\nGenerative Knowledge Transfer for Neural Language Models\nReferences\nBucilu, Cristian, Caruana, Rich, and Niculescu Mizil,\nAlexandru. Model compression. In Proceedings of the\n12th ACM SIGKDD international conference on Knowl-\nedge discovery and data mining , pp. 535–541. ACM,\n2006.\nChan, William, Ke, Nan Rosemary, and Lane, Ian. Trans-\nferring knowledge from a RNN to a DNN.arXiv preprint\narXiv:1504.01483, 2015.\nCho, Kyunghyun, Van Merri ¨enboer, Bart, Gulcehre,\nCaglar, Bahdanau, Dzmitry, Bougares, Fethi, Schwenk,\nHolger, and Bengio, Yoshua. Learning phrase represen-\ntations using rnn encoder-decoder for statistical machine\ntranslation. arXiv preprint arXiv:1406.1078, 2014.\nDietterich, Thomas G. Ensemble methods in machine\nlearning. In International workshop on multiple clas-\nsiﬁer systems, pp. 1–15. Springer, 2000.\nDwork, Cynthia, McSherry, Frank, Nissim, Kobbi, and\nSmith, Adam. Calibrating noise to sensitivity in private\ndata analysis. In Theory of Cryptography Conference ,\npp. 265–284. Springer, 2006.\nDwork, Cynthia, Roth, Aaron, et al. The algorithmic\nfoundations of differential privacy. Foundations and\nTrends in Theoretical Computer Science , 9(3–4):211–\n407, 2014.\nFredrikson, Matt, Jha, Somesh, and Ristenpart, Thomas.\nModel inversion attacks that exploit conﬁdence informa-\ntion and basic countermeasures. In Proceedings of the\n22nd ACM SIGSAC Conference on Computer and Com-\nmunications Security, pp. 1322–1333. ACM, 2015.\nGraves, Alex. Generating sequences with recurrent neural\nnetworks. arXiv preprint arXiv:1308.0850, 2013.\nGraves, Alex, Mohamed, Abdel-rahman, and Hinton, Ge-\noffrey. Speech recognition with deep recurrent neu-\nral networks. In Acoustics, speech and signal process-\ning (icassp), 2013 ieee international conference on , pp.\n6645–6649. IEEE, 2013.\nHamm, Jihun, Cao, Paul, and Belkin, Mikhail. Learn-\ning privately from multiparty data. In Proceedings of\nthe 33th International Conference on Machine Learning\n(ICML-16), 2016.\nHinton, Geoffrey, Vinyals, Oriol, and Dean, Jeff. Distilling\nthe knowledge in a neural network. In Deep Learning\nand Representation Learning Workshop, NIPS, 2014.\nHochreiter, Sepp and Schmidhuber, J ¨urgen. Long short-\nterm memory. Neural computation , 9(8):1735–1780,\n1997.\nMikolov, Tomas, Karaﬁ ´at, Martin, Burget, Lukas, Cer-\nnock`y, Jan, and Khudanpur, Sanjeev. Recurrent neu-\nral network based language model. In Interspeech, vol-\nume 2, pp. 3, 2010.\nMirowski, Piotr and Vlachos, Andreas. Dependency re-\ncurrent neural language models for sentence completion.\narXiv preprint arXiv:1507.01193, 2015.\nPan, Sinno Jialin and Yang, Qiang. A survey on transfer\nlearning. IEEE Transactions on knowledge and data en-\ngineering, 22(10):1345–1359, 2010.\nPapernot, Nicolas, Abadi, Mart´ın, Erlingsson, ´Ulfar, Good-\nfellow, Ian, and Talwar, Kunal. Semi-supervised knowl-\nedge transfer for deep learning from private training\ndata. arXiv preprint arXiv:1610.05755, 2016.\nPasa, Luca, Testolin, Alberto, and Sperduti, Alessandro. A\nHMM-based pre-training approach for sequential data.\nIn ESANN. Citeseer, 2014.\nPathak, Manas, Rane, Shantanu, and Raj, Bhiksha. Mul-\ntiparty differential privacy via aggregation of locally\ntrained classiﬁers. In Advances in Neural Information\nProcessing Systems, pp. 1876–1884, 2010.\nPaul, Douglas B and Baker, Janet M. The design for the\nWall Street Journal-based CSR corpus. In Proceed-\nings of the workshop on Speech and Natural Language ,\npp. 357–362. Association for Computational Linguistics,\n1992.\nRomero, Adriana, Ballas, Nicolas, Kahou,\nSamira Ebrahimi, Chassang, Antoine, Gatta, Carlo,\nand Bengio, Yoshua. Fitnets: Hints for thin deep nets.\nIn Proceedings of the 3th International Conference on\nLearning Representations (ICLR), 2015.\nSerban, Iulian V , Sordoni, Alessandro, Bengio, Yoshua,\nCourville, Aaron, and Pineau, Joelle. Building end-to-\nend dialogue systems using generative hierarchical neu-\nral network models. arXiv preprint arXiv:1507.04808 ,\n2015.\nSpithourakis, Georgios P, Augenstein, Isabelle, and Riedel,\nSebastian. Numerically grounded language mod-\nels for semantic error correction. arXiv preprint\narXiv:1608.04147, 2016.\nSutskever, Ilya, Martens, James, and Hinton, Geoffrey E.\nGenerating text with recurrent neural networks. In Pro-\nceedings of the 28th International Conference on Ma-\nchine Learning (ICML-11), pp. 1017–1024, 2011.\nSutskever, Ilya, Martens, James, Dahl, George E, and Hin-\nton, Geoffrey E. On the importance of initialization and\nmomentum in deep learning. ICML (3), 28:1139–1147,\n2013.\nGenerative Knowledge Transfer for Neural Language Models\nTang, Zhiyuan, Wang, Dong, and Zhang, Zhiyong. Recur-\nrent neural network training with dark knowledge trans-\nfer. In 2016 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pp. 5900–\n5904. IEEE, 2016.\nWerbos, Paul J. Backpropagation through time: what it\ndoes and how to do it. Proceedings of the IEEE, 78(10):\n1550–1560, 1990.\nZeiler, Matthew D. Adadelta: an adaptive learning rate\nmethod. arXiv preprint arXiv:1212.5701, 2012."
}