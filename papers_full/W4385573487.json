{
    "title": "Context Limitations Make Neural Language Models More Human-Like",
    "url": "https://openalex.org/W4385573487",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2899299319",
            "name": "Tatsuki Kuribayashi",
            "affiliations": [
                "Tohoku University"
            ]
        },
        {
            "id": "https://openalex.org/A2589102869",
            "name": "Yohei Oseki",
            "affiliations": [
                "The University of Tokyo",
                "RIKEN"
            ]
        },
        {
            "id": "https://openalex.org/A2805747697",
            "name": "Ana Brassard",
            "affiliations": [
                "RIKEN",
                "Tohoku University"
            ]
        },
        {
            "id": "https://openalex.org/A2084773436",
            "name": "Kentaro Inui",
            "affiliations": [
                "RIKEN",
                "Tohoku University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2062988874",
        "https://openalex.org/W2933138175",
        "https://openalex.org/W3175306105",
        "https://openalex.org/W3033254023",
        "https://openalex.org/W2139450036",
        "https://openalex.org/W1604686258",
        "https://openalex.org/W2108010971",
        "https://openalex.org/W3001497429",
        "https://openalex.org/W4231427720",
        "https://openalex.org/W2107265154",
        "https://openalex.org/W2741831486",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2164418233",
        "https://openalex.org/W1975387939",
        "https://openalex.org/W2151073408",
        "https://openalex.org/W3037273551",
        "https://openalex.org/W2123862679",
        "https://openalex.org/W4242745329",
        "https://openalex.org/W1951724000",
        "https://openalex.org/W3007362922",
        "https://openalex.org/W3183164279",
        "https://openalex.org/W3099434687",
        "https://openalex.org/W2170619726",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W2102709979",
        "https://openalex.org/W4302556686",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2913125520",
        "https://openalex.org/W2795342569",
        "https://openalex.org/W4301893732",
        "https://openalex.org/W3184030040",
        "https://openalex.org/W2226734577",
        "https://openalex.org/W2970648593",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W3171953676",
        "https://openalex.org/W3176464186",
        "https://openalex.org/W3213051760",
        "https://openalex.org/W2153791616",
        "https://openalex.org/W2977268464",
        "https://openalex.org/W2130914914",
        "https://openalex.org/W1973926448",
        "https://openalex.org/W2054125330",
        "https://openalex.org/W2962941914",
        "https://openalex.org/W4252057175",
        "https://openalex.org/W2141440284",
        "https://openalex.org/W3036487253",
        "https://openalex.org/W2166544825",
        "https://openalex.org/W2117823388",
        "https://openalex.org/W2471684646",
        "https://openalex.org/W3100748148",
        "https://openalex.org/W2572891740"
    ],
    "abstract": "Language models (LMs) have been used in cognitive modeling as well as engineering studies—they compute information-theoretic complexity metrics that simulate humans' cognitive load during reading.This study highlights a limitation of modern neural LMs as the model of choice for this purpose: there is a discrepancy between their context access capacities and that of humans.Our results showed that constraining the LMs' context access improved their simulation of human reading behavior.We also showed that LM-human gaps in context access were associated with specific syntactic constructions; incorporating syntactic biases into LMs' context access might enhance their cognitive plausibility.",
    "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10421–10436\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nContext Limitations Make Neural Language Models More Human-Like\nTatsuki Kuribayashi1,2 Yohei Oseki3,4 Ana Brassard1,4 Kentaro Inui1,4\n1Tohoku University 2Langsmith Inc. 3University of Tokyo 4RIKEN\n{kuribayashi, inui}@tohoku.ac.jp\noseki@g.ecc.u-tokyo.ac.jp ana.brassard@riken.jp\nAbstract\nLanguage models (LMs) have been used in cog-\nnitive modeling as well as engineering studies—\nthey compute information-theoretic complexity\nmetrics that simulate humans’ cognitive load\nduring reading. This study highlights a lim-\nitation of modern neural LMs as the model\nof choice for this purpose: there is a discrep-\nancy between their context access capacities\nand that of humans. Our results showed that\nconstraining the LMs’ context access improved\ntheir simulation of human reading behavior. We\nalso showed that LM-human gaps in context\naccess were associated with specific syntactic\nconstructions; incorporating syntactic biases\ninto LMs’ context access might enhance their\ncognitive plausibility.1\n1 Introduction\nIn computational psycholinguistics, human read-\ning behavior has been compared with various\ncomplexity metrics to understand human sentence\nprocessing (Crocker, 2007). Having historically\nstarted from simple measures such as word length,\nsurprisal ( −log p(word|context)) computed by\nlanguage models (LMs) has become a common\nchoice (Levy, 2008; Smith and Levy, 2013). On\ntop of this, the next question arises—which model\nimplementation and/or algorithm can compute sur-\nprisal that successfully simulates human behavior?\nIn this line of research, modern neural LMs such\nas Transformer (Vaswani et al., 2017) have been\nanalyzed with respect to their cognitive plausibil-\nity (Wilcox et al., 2020; Merkx and Frank, 2021;\nKuribayashi et al., 2021).\nDespite their use in cognitive modeling, such\nmodern LM architectures (e.g., self-attention) are,\narguably, an unnatural choice when it comes to\nhuman cognitive constraints; modern LM architec-\ntures assume powerful, parallel access to a vast\n1Our codes are available at /gtbhttps://github.\ncom/kuribayashi4/context_limitation_cognitive_\nmodeling\n23571020full wordsinput length\nLSTM-xs-WikiGPT2-xs-WikiGPT2-md-WikiGPT2-smGPT2-mdGPT2-lgGPT2-xl\n235710full wordsinput length\nLSTM-xs-WikiGPT2-xs-WikiGPT2-md-Wiki\nEnglish\nJapanese\nPPPPPP\nFigure 1: Relationship between psychometric predictive\npower (PPP) of language models (LMs) and their con-\ntext access constraints. LMs with less context access\nbetter simulate human reading behavior (higher PPP).\nThe marker color/shape indicates LM settings; colored\nareas present one standard deviation of PPP.\nnumber of context tokens, while humans might\nhave limited and selective context access (Hawkins,\n1994; Gibson, 1998, 2000; Lewis et al., 2006).\nSearching for a computational model that better\nsimulates human sentence processing than previ-\nously examined ones, we hypothesized that intro-\nducing such context limitations can improve LMs’\nestimation of human cognitive load.\nSpecifically, as a starting point, we applied an\nn-gram-ification trick to neural LMs mimicking\nloading for long context access ( locality effects)\nand compared their surprisal with human reading\nbehavior data. Despite the simple context limita-\ntion design, our experiments with 280 settings (40\nLM settings×7 noise patterns) showed that the ad-\nvantage of a shorter context was consistent among\n10421\nNeural model architecture Data size Training\nsteps\nTokenization Syntactic supervi-\nsion\nContext\nlength\nGoodkind and Bicknell (2018); Aurnham-\nmer and Frank (2019); Wilcox et al. (2020);\nHao et al. (2020); Merkx and Frank (2021);\nOh et al. (2021); Kuribayashi et al. (2021)\nWilcox\net al. (2020);\nKuribayashi\net al. (2021)\nKuribayashi\net al. (2021)\nWilcox et al.\n(2020); Oh\net al. (2021)\nHale et al. (2018);\nYoshida et al.\n(2021)\nThis\nwork\nTable 1: Related studies exploring psychometric predictive power of neural models while separately controlling a\nspecific factor of their configuration.\nLMs and typologically different languages (Fig-\nure 1). This showed that constraining the modern\nLM’s context access is key to increasing their simi-\nlarity to the model of human reading.\nFurthermore, expecting that humans’ context\nlimitations might be more complex than simple\ndistance-based erasure, we conducted exploratory\nanalysis of in which constructions longer/shorter\ncontexts were beneficial. We found that the con-\ntext limitation (dis)advantages were allocated in\nspecific syntactic constructions, suggesting that,\nto build more cognitively plausible LMs, adding\nsyntactic biases in their context access could be\nbeneficial. From a psycholinguistic view, our re-\nsults empirically highlight the memory account of\nhuman sentence processing during naturalistic read-\ning (Futrell et al., 2020a).\n2 Background\n2.1 Human sentence processing\nHumans incrementally process text and exhibit dif-\nferent processing costs (e.g., reading times) for\ndifferent tokens. Psycholinguistic theories on such\nprocessing costs are divided between expectation-\nbased and memory-based perspectives.\nExpectation-based theories claim that hu-\nmans predict upcoming words during incremen-\ntal sentence processing (Clark, 2013). Re-\ncent studies have extensively analyzed this\nexpectation-based aspect by comparing surprisal,\n−log p(word|context), to human reading behav-\nior (Hale, 2001; Levy, 2008; Wilcox et al., 2020).\nOn the other hand, memory-based theories have\nasserted that human sentence processing is con-\nstrained by a limited cognitive resource (Gibson,\n2000; Lewis and Vasishth, 2005; Lewis et al., 2006).\nCross-linguistic studies have reported that differ-\nent languages incur different memory decay during\nreading (Konieczny, 2000; Vasishth et al., 2010;\nHusain et al., 2014; Frank et al., 2016). Notably,\nmemory efficiency is also considered in exploring\nthe design (e.g., word order) and evolution of nat-\nural language (Greenberg, 1963; Chomsky, 2005;\nGibson et al., 2019; Hahn et al., 2020a,b).\nRecently, Futrell and Levy (2017) and Futrell\net al. (2020a) have proposed integrating the two\ntheories through the concept of lossy-context sur-\nprisal—next-word probabilities calculated with\nnoisy context should better predict human reading\nbehavior than with complete context. These studies\nhave focused on its theoretical aspects and explain-\ning a specific phenomenon (e.g., verb forgetting);\non top of this, our study demonstrates the theory’s\nbroad benefit in modeling naturalistic reading data.\nNotably, such a simulation of human cognitive\nload also contributes to achieving text readability\nassessment (Ambati et al., 2016). Furthermore,\nhuman-like agents are necessary in in silico simu-\nlation studies on language evolution (Galke et al.,\n2022; Rita et al., 2020; Ueda and Washio, 2021).\n2.2 Cognitive plausibility of LMs\nSurprisal from certain LMs could predict human\nreading behavior well; thus, what type of LM does\nbetter simulate human reading behavior? LM-\nbased analyses have typically explored inductive\nbiases, such as LM architecture (Table 1). We focus\non context limitation as an alternative factor.\nStudies comparing the cognitive plausibility of\nLM architectures also addressed, albeit implic-\nitly, context access abilities (Aurnhammer and\nFrank, 2019; Merkx and Frank, 2021). For exam-\nple, simple recurrent neural networks assume rela-\ntively weak context access, whereas Transformer\nLMs (Vaswani et al., 2017) assume stronger ac-\ncess than those (Michaelov et al., 2021; Merkx\nand Frank, 2021). In addition, studies contrasted\ncount-based n-gram LMs and neural LMs (Wilcox\net al., 2020; Hao et al., 2020; Goodkind and Bick-\nnell, 2018); however, (i) count-based versus neural-\nbased estimation and (ii) partial versus full context\naccess were not distinguished. By contrast, we\nfixed the architectures and investigated the exact\neffect of context access with input deletion.\n10422\n3 Methods\nWe investigate how human-like neural LMs be-\ncome with more or less context at their input.\nSpecifically, we measured the psychometric pre-\ndictive power (PPP) of lossy-context LM surprisal\nfor gaze duration modeling. In the following sec-\ntions, we describe each measure in detail.\n3.1 Psychometric predictive power\nIn this study, the cognitive plausibility of a model\nθ is measured via the similarity between its sur-\nprisal and human gaze duration across words based\non surprisal theory (Smith and Levy, 2013; Levy,\n2008). Here, the surprisal of a word computed by\na model θ, −log pθ(word|context), is compared\nwith the corresponding word’s gaze duration.\nSpecifically, we measured the psychometric\npredictive power (PPP) of surprisal values by\nfitting two nested linear mixed-effects regression\nmodels that predict gaze duration, one with sur-\nprisal features and the other without. Here, the per-\ntoken difference in their log-likelihoods (∆LogLik;\nLogLik with surprisal minus LogLik without sur-\nprisal) denotes PPP, following Goodkind and Bick-\nnell (2018). The larger the PPP ( ∆LogLik), the\nmore useful the surprisal for modeling gaze dura-\ntion, i.e., the model computes surprisal well corre-\nlating with human behavior. See Appendix A for\ndetailed features used in regression modeling.\n3.2 Lossy-context surprisal\nInstead of the full-context surprisal, we inves-\ntigate the PPP of surprisal conditioned by lim-\nited context −log pθ(word|lossy_context) to ex-\nplore the cognitive plausibility of context-limited\nLMs (Futrell et al., 2020a). The lossy-context sur-\nprisal of the symbol wi given its preceding context\nc<i = [w0,··· ,wi−1] is defined as follows:\nIlossy(wi,c<i,f)\n= −log pθ(wi|<s> ◦f([w0,··· ,wi−1])) , (1)\nwhere θdenotes left-to-right LMs, <s> denotes the\nbeginning of a sequence, ◦is a concatenation func-\ntion, and f represents a noise function. The noise\nfunction controls the LMs’ access to contextual\ninformation by deleting the input of LMs with a\nparticular pattern. For example, if f is leaving only\nthe last two symbols,Ilossy corresponds to surprisal\nfrom 3-gram LMs, and if f is an identity function,\nIlossy corresponds to unmodified surprisal.\nData sentLen contextLen wordLen\nDC 17.8 ±11.7 13.0 ±10.1 1.3 ±0.7\nBE 7.9 ±5.3 6.0 ±4.6 3.4 ±2.3\nTable 2: Statistics of all the sentences in each corpus.\nThe values present mean±standard deviation; sentLen\ndenotes the number of words in a sentence, contextLen\ndenotes the number of preceding words within the same\nsentence for each word, and wordLen denotes the num-\nber of subwords in each word.\nGaze duration is typically annotated in larger\nspans such as words, while LMs’ input is at the\nsmaller levels (i.e., subwords). The lossy-context\nsurprisal of a span s = [wl,wl+1,··· ,wm] (0 ≦\nl<m ) was calculated as the cumulative surprisal\nof its constituent subwords:\nIlossy(s,c<l,f) =\nm∑\nj=l\nIlossy(wj,c<j,f) . (2)\nN-gram surprisal. As a starting point, based\non the assumption about human working memory\nthat distant context is hard to access (Lewis et al.,\n2006), we explored surprisal given by LMs condi-\ntioned on n−1 preceding words (not subwords);\nhenceforth, this surprisal is referred to as n-gram\nsurprisal (a special case of lossy-context surprisal).\nIn Appendix B, we also explored a probabilistic ver-\nsion of the noise inspired by Futrell et al. (2020a),\nyielding consistent conclusions with our experi-\nments using n-gram surprisal.\n3.3 Gaze duration\nGaze duration data were modeled by lossy-context\nsurprisal. To explore the cross-linguistic consis-\ntency of our results, we used two typologically\ndifferent languages, English and Japanese; their\ndifference is introduced in the later paragraph.\nData. For English, we used the Dundee Corpus\n(DC) (Kennedy et al., 2003). As its Japanese coun-\nterpart, we used BCCWJ-EyeTrack (BE) (Asa-\nhara et al., 2016). In both corpora, first-pass\ngaze duration information was used. The aver-\nage sentence/context lengths are shown in Table 2.\nNote that while the English gaze duration annota-\ntion is typically attached to space-separated words,\nJapanese gaze duration annotation is attached to\neach phrasal unit (bunsetsu; henceforth, “word”);\nJapanese “words” contain more subwords than En-\nglish words. Following Goodkind and Bicknell\n10423\n(2018), we excluded outliers such as words with\nspecial characters (details in Appendix C). We used\n212,649 data points from DC and 9,217 from BE.\nCross-linguistic analysis. English and Japanese\nsentence structures differ in their branching direc-\ntions; while English word order (SVO) has mixed\ndirectionalities of head-initial and head-final de-\npendencies, Japanese word order (SOV) strongly\nprefers head-final, left-branching constructions.\nThe dependency structures of the sentence “the dog\nwagging its tail eats fish on the desk.” in English\nand Japanese are contrasted below:2\n(1) The dog wagging its tail ate fish on the desk.\n(2) 尻尾を 振る 犬が机の上で魚を食べた。\ntail wagging dog on desk fish ate\nSuch an asymmetry of structure has been re-\nported to incur different memory biases for sen-\ntence processing (Konieczny, 2000; Nakatani and\nGibson, 2008; Vasishth et al., 2010; Futrell et al.,\n2020a). Thus, we included typologically different\nlanguages in our experiments.\n4 Language models\nWe used two types of neural LMs for lossy-context\n(n-gram) surprisal computation: (i) Wiki-LMs and\n(ii) pretrained OpenAI GPT-2s (Radford et al.,\n2019). Their hyperparameters are shown in Ap-\npendix D. Notably, using neural LMs makes the\ncomparison of long-context and short-context LMs\ncomputationally tractable.3\n4.1 Wiki-LMs\nModel settings. We used three variants of unidi-\nrectional neural LM architectures: LSTM-xs-Wiki\n(27M parameters) (Hochreiter and Schmidhuber,\n1997), GPT2-xs-Wiki (29M), and GPT2-md-Wiki\n(335M) (Vaswani et al., 2017). We trained each\nLM with three different random seeds using the\nFairseq toolkit (Ott et al., 2019).\nIn both English and Japanese settings, the input\nis split into subwords with byte-pair encoding (Sen-\n2For simplicity, some functional words (e.g., “the,” “を”)\nare merged into a single node.\n3For example, if we use count-based LMs (Heafield et al.,\n2013), even a single 5-gram Japanese LM took 27GB in model\nsize.\n... <b> _was _also _the _first _hotel _in _Westchester\n_County . <b> _on _4 _March _1990 _with _a _concert\n_performed _by _Ell a _Fitzgerald _at _the _Royal _Al-\nbert _Hal <b> _the _Har row <b> _On _the _night _of\n_the _31 _May _/ _1 _June _1941 _he <b> ...\nTable 3: An example of the modified training data,\nwhere sub-sequences (with the same color) sampled\nfrom the original corpus were randomly patched. The\nspecial token ( <b>) indicates the break of contextual\ndependence between before and after.\nnrich et al., 2016).4 Specifically, for the Japanese\ndata, we adopted two-stage segmentation to ensure\nthat multiple subwords compose a Japanese word\ndefined in a commonly used corpus (e.g., BE). 5\nThat is, text was segmented in advance into mor-\nphemes (Maekawa et al., 2014), and then a subword\ntokenizer was applied to the morpheme-separated\ntexts. Details are in Appendix D.\nTraining data. For English, the training data\nwere approximately 4M sentences from the\nWikiText-103 dataset (Stephen et al., 2016), and\nfor Japanese, the data were 4M sentences from\nWikipedia and news articles (approximately 0.5GB\ndata size in both English and Japanese). The sen-\ntence order was shuffled, and duplicated sentences\nwere excluded.\nMitigating training-inference mismatches.\nDuring n-gram surprisal computation, LMs\nmust predict the upcoming words with limited\ncontext from the middle of a sentence, while such\na prediction is rarely enforced during ordinal\ndocument-level training. Such a training-inference\nmismatch could lead to confusion on whether our\nresults stem from the LM-human gap or biases\nfrom the training/inference mismatch.\nTo handle such a potential mismatch, we mod-\nified the LM training data to make the language\nmodeling task more like n-gram one. Specifically,\nwe randomly split original sentences into smaller\nchunks of various lengths and randomly patched\nthem by inserting a special token <b> in between\nthe chunks (see Appendix E for the detailed pro-\ncess). Table 3 shows an example. In this modi-\n4Note that there is some debate on the cognitive plausibility\nof subwords (Oh et al., 2021; Anonymous, 2022); we consider\nthis issue to be out of the scope of this study.\n5This procedure is common practice in Japanese NLP. See\nhttps://github.com/himkt/awesome-bert-japanese.\nWe used Mecab (Kudo, 2006) with a unidic dictionary\n(https://unidic.ninjal.ac.jp/) for morphological\nanalysis.\n10424\nInput length\nLang. Model full 20 10 7 5 3 2 ∆\nEn\nGPT2-xl 5.6 5 .7 5 .6 5 .8 6 .2 6 .8 7.4† 1.8\nGPT2-lg 5.9 6 .0 6 .0 6 .0 6 .4 7 .1 7.5† 1.6\nGPT2-md 5.8 5 .9 5 .9 5 .9 6 .2 6.8 6.7† 0.9\nGPT2-sm 6.9 7 .0 6 .9 6 .9 7 .1 7 .5 7.6† 0.7\nGPT2-md-Wiki 5.9 5 .9 5 .9 6 .0 6 .1 6 .3 6.5† 0.6\nGPT2-xs-Wiki 7.1 7 .1 7 .1 7 .1 7 .1 7.0 7.1 0.0\nLSTM-xs-Wiki 7.4 7 .4 7 .4 7 .3 7 .4 7 .4 7.5 0.1\nJa\nGPT2-md-Wiki 8.1 8 .1 8 .3 8 .6 8 .4 9 .3 10.0† 1.9\nGPT2-xs-Wiki 10.7 10 .7 10 .8 11 .0 10 .8 12 .5 12.8† 2.1\nLSTM-xs-Wiki 10.6 10 .6 10 .6 10 .7 10 .6 11 .8 11.9† 1.3\nTable 4: Average PPP ofn-gram surprisal; for example, the input length of 2 corresponds to the PPP of surprisal\ncomputed by the neural LMs that take only the 2-gram context as input. For readability, values are multiplied by\n1000. The 2-gram PPP with †is significantly higher than its corresponding full-context PPP. The ∆ column shows\nthe PPP gain from the full context to 2-gram context surprisal in each LM setting.\nfied corpus, LMs must predict upcoming words by\nseverely limited usable context especially in the\ndata points immediately after the special tokens.\nWhen computing n-gram surprisal, the <b> token\nis set instead of <s> in Eq. 1. Note that this modifi-\ncation does not change the total corpus size.\nWe trained the Wiki-LMs using this modified\ndata. In Section 5.1, we ablated the effect of this\ntraining modification and showed that such careful\ntraining makes the short-context advantage clearer.\n4.2 Pretrained GPT-2s\nTo investigate large-scale LMs typically devel-\noped in NLP, we additionally used four variants\nof pretrained English OpenAI GPT-2s (Radford\net al., 2019): GPT2-sm (117M params.), GPT2-md\n(345M), GPT2-lg (774M), and GPT2-xl (1558M).\nThe input was split into subwords by their pre-\ntrained tokenizer with a vocabulary size of 50,257.\nThe training data were 40GB of web texts. The\npotential training-inference mismatch is not han-\ndled in the GPT-2 experiments due to the high re-\ntraining cost; this point is partially addressed in\nSection 5.1. Note that we did not use Japanese ver-\nsions of pretrained GPT-2s since available models6\nhave a tokenizer that is inconsistent with the BE\nannotation; 16.4% of word boundaries in the BE\nwere not separated by their pretrained tokenizer.\n6https://huggingface.co/rinna/\njapanese-gpt2-small\n5 Experiments\nOur experiments demonstrate how limiting con-\ntext access improved the PPP in LMs, i.e., their\nsurprisal becomes a more effective predictor for\nhuman gaze duration (Section 5.1). As described\nin Section 3.2, we applied distance-based noise\nto the input (i.e., computing n-gram surprisal). A\npotential training-inference mismatch bias is han-\ndled (Section 5.2). Furthermore, we explored the\nconnection to existing studies (Section 7.2).\nSettings. We measured the PPP of 40 LMs ({3\nWiki-LMs}×{3 seeds}×{2 languages}×{2 train-\ning settings}+{4 OpenAI GPT-2s}) with seven\nnoise patterns. Specifically, we explored the\n{2,3,5,7,10,20}-grams and full settings for each\nLM, where full refers to using the entire sentence\n(w0 to wi−1) as context. Note that only intra-\nsentential context is used as the main focus of this\nstudy is sentence-level syntactic processing.\n5.1 PPP and input length\nShorter context improved or did not decrease\nhuman likeness. The PPP of n-gram surprisal\nin relation to input length nis shown in Figure 1\nand Table 4. The English results show that using a\nshorter context improved (OpenAI GPT-2s) or did\nnot hurt (Wiki-LMs) their human likeness. Notably,\nwe also conducted experiments using probabilis-\ntic versions of the noise in Appendix B, yielding\nconsistent results.\nNote that the tininess of the values in Figure\n1 and Table 4 (e.g., 0.0074 v.s. 0.0056 in GPT2-\n10425\nLSTM-xs-WikiGPT2-xs-WikiGPT2-md-WikiGPT2-smGPT2-mdGPT2-lgGPT2-xl\nEnglish\nJapanese(nearly zero)\nΔPPPΔPPPsmall large\n　\nFigure 2: Increase in PPP (from the full-gram to 2-gram\nsettings) in each model type (ordered by their parameter\nsize). The bar colors correspond to those in Figure 1.\nxl) does not imply that the difference is valueless,\nbut this is just because the score is divided by\nthe number of data points (e.g., 212,649 in the\nDundee corpus) to facilitate inter-corpora compar-\nison. As a statistical test, we compared the by-\ntoken squared residual errors from 2-gram models\nwith those from full-context models using paired\npermutation tests (p=0.05). The short context, 2-\ngram models had significantly smaller fitting errors\nthan the full context models (p< 0.001) in using\nrelatively large LMs (GPT2-md-Wiki, GPT2-sm,\nGPT2-md, GPT2-lg, and GPT2-xl); smaller LMs\n(LSTM-xs-Wiki, and GPT2-xs-Wiki) have no sig-\nnificant differences (p∼0.4).\nNotably, we also observed that larger GPT-2s\nhave less human-like behavior in the full setting\n(right-most column in Table 4). This trend was\nweakened by introducing our context limitation.\nCross-linguistic consistency. Figure 1 and Ta-\nble 4 also show the cross-linguistic generality of\nthe short context advantage. The short context was\nmore clearly favored in Japanese than in English.\nUsing the same method as the English experiments,\nwe performed the significance tests; 2-gram models\nexhibited smaller fitting errors (p ∼0.001) in all\nthe Japanese LM settings. The language-dependent\ndifferences are further investigated in Section 6.\nThe larger the LM, the greater the increase\nin PPP when limiting context access. Figure 2\nshows the PPP increase in each LM class by context\nlimitation (PPP at 2-gram minus PPP at full-gram).\nThe bars were ordered by the model parameter size\n(small − →large). We found a clear trend that larger\nLMs become human-like by a larger margin be-\ncause of context limitation; larger full-context LMs\ndeviate more from human-like context access.\nWe statistically tested whether the gain by con-\ntext limitation (full-context v.s. bigram) was larger\nin the largest LMs (GPT2-md in Japanese and\nGPT2-xl in English) than in the smallest LMs\n(LSTM-xs). Specifically, we compared the by-\ntoken decrease in squared residual errors; the large\nmodel exhibited a larger error decrease than the\nsmall model (p= 0.024 <0.05 in Japanese, and\np< 0.001 in English). In addition, the rank corre-\nlation between model size and PPP gain by context\nlimitation was 0.50 in Japanese and 0.96 in English.\nGeneral effectiveness of surprisal. Note that, in\nall the LMs, the PPP scores (equivalent to∆logLik)\nwere significantly higher than 0 with the chi-square\ntest (p< 10−31 even in the worst case); surprisal\nwas an effective factor as existing studies reported.\nOn top of this, we newly showed that their effect\nsize differs due to the context limitation levels.\n5.2 Does the potential training-inference\nmismatch bias our results?\nVanilla LMs slightly underestimate the short-\ncontext advantage. We additionally trained Wiki-\nLMs (LSTM-xs-Wiki, GPT2-xs-Wiki, and GPT2-\nsm-Wiki) without the data modification handling\nthe training-inference gap (Section 4.1) (hence-\nforth; vanilla LMs). Figure 3 shows the results\nof the models with and without the training modi-\nfication. The vanilla LMs slightly underestimated\nthe short-context advantage; the PPP of 2-gram\nsurprisal improved when we adopted the modified\ntraining. That is, mitigating the train-inference\ngap made clearer the trend that context limitation\nincreases PPP. Carefully training n-gram neural\nLMs could be a way to create more human-like\ncomputational model.\n6 Analyses\nIn our experiments, we merely deleted distant con-\ntexts regardless of linguistic factors. However, this\ndesign is somewhat counter-intuitive in the sense\nthat humans are assumed to completely forget the\ndistant context during reading. To gain insights\ninto a more sophisticated noise design to fill the\nLM-human gap, we observed in which construc-\ntions longer/shorter contexts improved simulation\nof human gaze duration.\n6.1 Settings\nQuantifying long context effect. To quantify the\nlong context advantage for each data point, we\n10426\n23571020full words 235710full words\nEnglish Japanese\nPPP2-gram–PPPfull model type\nLSTM-xs-Wiki\nLSTM-xs-Wiki(vanilla)\nGPT2-xs-WikiGPT2-md-Wiki\nGPT2-xs-Wiki(vanilla)GPT2-md-Wiki (vanilla)\n(vanilla)\ninput length input length\nmodel typesmall largesmall large\nFigure 3: Reproduction of Figures 1 and 2 using LMs without training setting modifications (Section 5.1). The\nresults from Wiki-LMs with the modification (colored) and without the modification (gray) are overlayed. In the\nline charts, the X-axis indicates input length, and the Y-axis indicates PPP. The bottom bar charts show the increase\nin PPP (from full-gram to 2-gram setting) of the modified LMs.\ncompared the squared residual (fitting) errors of\nthe regression models we used to compute PPP in\nSection 5. Note that the larger the squared residual\nerror is, the worse the model fit with the target\nvariable (gaze duration).\nSpecifically, we contrasted the two regression\nmodels with different context access: (i) the model\nwith 2-gram surprisal, and (ii) the model with full\ncontext surprisal. For each data point d, we mea-\nsured the effectiveness of long context (ELC) in\nexplaining gaze duration. Specifically, the differ-\nence between the squared residual errors by the\nregression models with 2-gram surprisal r2(d) and\nfull surprisal rfull(d) was computed:\nELC(d) = r2(d) −rfull(d) . (3)\nHere, a high ELC value indicates that reading\ntimes on dwere better simulated with long context\n(rfull(d) ↓); worse simulated with short context\n(r2(d) ↑). The aim of this section is to find the data\npoints with a high ELC value. In the following\nanalyses, we used all the models from Section 5.1,\nand ELC scores for each data point were averaged\nacross all the LMs.\nDependency structure. Human context access\nhas typically been discussed with respect to syntac-\ntic structure (Gibson, 1998; Demberg and Keller,\n2008); we first explored the interactions between\ncontext limitation advantage and syntactic depen-\ndencies. We analyzed two syntactic factors: (i)\ndependency locality and (ii) dependency type,\nwhere the dependency locality of a token denotes\nhow far its syntactically related preceding items\n(i.e., with a direct dependency) are placed on aver-\nage. An example is as follows:\n(3) The boy over there had a cap.\n3,nsubj\nHere, the dependency locality of “had” is three;\nnote that the dependency direction was disregarded.\nIn the following analyses, we only used data\npoints with potential long context access, i.e., those\nin the latter part of a sentence.7 After this filtering,\nthe average dependency locality score was 2.5 and\n2.6 in the DC and BE, respectively. Manual linguis-\ntic annotations were used in our analyses (Barrett\net al., 2015; Omura and Asahara, 2018).\n6.2 Results\nDependency locality. We first grouped the data\npoints by their dependency locality and calculated\nthe average ELC scores for each group. Figure 4a\nshows the results. Surprisingly, in the English data,\nthere is no advantage in considering the long con-\ntext for tokens with long dependencies. By con-\ntrast, in the Japanese data, long context access con-\ntributed to simulating reading time for tokens with\na moderate (two or three) dependency length, but\nnot for long dependency locality. These imply that\nthe solution is more complex than simply using\nlong context for words with long dependency.\n713th/7th or later words of a sentence in the DC/BE data\n(20,554 words/4,051 words) were used, based on the median\nof the word position in sentences (12 and 6 in the DC and BE\ncorpus, respectively).\n10427\nDependency type. Does long context matter in\nspecific syntactic constructions? We categorized\nthe data points by their dependency type to their\npreceding syntactically related items and calculated\nthe averaged ELC score for each group.8\nFigure 4b shows that different dependency types\nare associated with different ELC scores. For ex-\nample, the discourse type in English have relatively\nlarger ELC scores; long context input is necessary\nto simulate its gaze duration. Figure 4b also sug-\ngests that such context-favoring (with high ELC)\ndependency types are different between English\nand Japanese. These findings imply that the LM-\nhuman context access gap occurred in specific\nsyntactic constructions in each language.\nOne-way ANOV A revealed that the average ELC\nscores for each dependency type significantly var-\nied (p = 0.029 < 0.05 in English, p = 0.038 <\n0.05 in Japanese), suggesting that the variation\nof the ELC score is related to certain construc-\ntions. More specifically, we compared the ELC\ndistribution between the categories with the high-\nest and lowest average ELC scores (discourse vs.\ncop in English, and advcl vs. obl) using an un-\npaired t-test. The test exhibited a significant dif-\nference ( p = 0 .012 < 0.05 in English, and\np = 0.019 < 0.05 in Japanese). Note that if the\ntest is repeated for other dependency-length/type\npairs, multiple comparison problems should occur;\nsome counteractions, such as Bonferroni correc-\ntion should be applied, and a more conservative\nconclusion can be reached.\n7 Discussion\n7.1 Interpretations of the main results\nWe observed that simply deleting distant context\nimproved LMs’ PPP—as context decreased, LMs\nbecame more human-like. We finally discuss sev-\neral potential interpretations of our results.\nOne interpretation is that our results supported\nthe dominance of short context access in human\nsentence processing. In this sense, our findings\nemphasized that explicitly incorporating principles\nfrom the memory-based account of human sentence\nprocessing is still necessary for simulating human\nsentence processing despite the success of modern\nLMs in cognitive modeling (Wilcox et al., 2020;\nSchrimpf et al., 2020). Notably, there are several\nother theories on human working memory; sparse\n8Here, we only focus on the dependencies with more than\nfour distances to explore potential long context access.\nJapaneseEnglishELC\n(a) Relationship between dependency locality and the ELC\nscores. The X-axis corresponds to dependency locality (e.g.,\nthe group “3” denotes the data points with the locality score\nof three). The Y-axis denotes the ELC score for each group.\nELC JapaneseEnglish\n(b) Relationship between dependency type and the ELC scores.\nThe X-axis corresponds to the dependency type. The Y-axis\ndenotes the average ELC score for each group. Dependency\ntypes for which there are more than 100 long dependencies\n(locality>4) were included.\nFigure 4: Relationships between syntactic factors and\nthe ELC scores.\nallocation of elements incurring memory load (Gib-\nson, 2000), hierarchical memory operations (van\nSchijndel et al., 2013), and cue-based memory re-\ntrieval (Lewis and Vasishth, 2005). Incorporating\nthese perspectives into context-limited LMs could\nbe an interesting future direction.\nAnother possibility is that identifying the cause\nof the LM-human gap as context limitations is\nover-claiming; our study alone did not rule out\nsome potentially confounding factors. For exam-\nple, increasing the softmax temperature when LMs\ncompute the next-word distribution may induce a\nsimilar effect to our context limitation with respect\nto that both modifications make LMs less confident\nabout the upcoming word (if temperature matters,\nthe linear relationship between surprisal and cogni-\ntive load may be doubted first). Further exploring\nsuch factors will be an important investigation.\nThere is also a possibility that the eye move-\nment data only reflected local, shallow aspects\nin human sentence processing . Similarly, Gau-\nthier and Levy (2019) obtained somewhat counter-\nintuitive results implying that word order is not im-\nportant information in sentence processing—bag-\nof-words (i.e., not word-order-aware) models fit\nfMRI data surprisingly well. They concluded that\ntheir results may stem from shortcomings of the\n10428\nmeasurement method along with the possibility of\nhumans’ good-enough processing. Exploring the\nadvantage of context limitation in various types of\nreading behavior data and/or using other text mate-\nrials (e.g., including more complex constructions)\nis also a line of future research.\nIs the 2-gram advantage counter-intuitive? If\nthe dominance of short context access in human\nreading is accepted once, some readers might be\nconfused that the 2-gram context access sounds too\nsevere. Again, as a strong word frequency effect\ndoes not deny context-dependent processing, our\nresults also did not decline long context access and\ndid not claim that the human language processing\nmodel is 2-gram LMs. Exploring the interactions\nof short- and long-context effects should be an in-\nteresting investigation.\nNevertheless, such severe memory limitations\nduring reading might be consistent with the\nmemory-based explanation for the linguistic univer-\nsals in sentence structures such as the preferences\ntoward consistent head directions, specific word\norder (e.g., short-before-long order), and projective\nstructures (Futrell et al., 2020b). Such phenom-\nena are typically explained by the humans’ prefer-\nences toward short dependencies; here, those are\nsometimes a matter of severe choices, such as the\npreference for an average dependency length of 1\nover 2 (Intuitively, Example (4) is preferred over\nExamples (5) and (6)):\n(4) A B C D E avg. dep len.=1.25\n2 1 1 1\n(5) A B C D E avg. dep len.=2.25\n4 31 1\n(6) A B C D E avg. dep len.=1.50\n2 2 1 1\nIf one reasons these principles to the constraints\nof humans’ cognitive resources, perhaps it makes\nsense that humans conduct syntactic processing\nwith such a severe working memory that the imme-\ndiately preceding word/phrase highly explains the\ncognitive load to the upcoming word.\n7.2 PPP and next-word prediction accuracy\nLastly, we discuss the connection to reports on\ncognitive modeling with LMs—better next-word\nprediction ability of LMs indicates their better\nPPP (Fossum and Levy, 2012; Goodkind and Bick-\nLSTM-xs-WikiGPT2-xs-WikiGPT2-md-WikiGPT2-mdGPT2-lgGPT2-xl\nGPT2-sm\nContext lengthshortlong\nModel\nFigure 5: Relationship between PPP and perplexity\n(PPL) drawn using the English LMs targeted in Sec-\ntion 5.1. Each point corresponds to each configuration\nof the n-gram surprisal computation; marker color and\nshape present the LM architectures, and larger markers\ncorrespond to longer context access.\nnell, 2018; Wilcox et al., 2020). 9 Our results in\nSection 5.1 might be conflicting with them; LMs\nwith relatively worse prediction accuracy (less con-\ntext access) exhibit better PPP. See Appendix F for\nnext-word prediction accuracy of LMs.\nResults. In fact, there is no clear relationship be-\ntween PPP and next-word prediction accuracy (per-\nplexity; PPL) of the LMs used in Section 5.1 (Fig-\nure 5; Appendix G exhibits Japanese results). The\nresults show that LMs with nearly the same next-\nword prediction accuracy could show different PPP\nvalues. Furthermore, Pearson’s correlation between\nPPP and PPL was even positive (r= 0.15). These\nobservations corroborated the conclusion that the\nPPL alone is not a good indicator of PPP; differ-\nent means of controlling PPL (e.g., context length\nvs. other factors existing studies focused on) could\nshow different PPP-PPL relationships.\n8 Conclusions\nThere has been little investigation of the cognitive\nplausibility of context-limited modern LMs. Our\nexperiments using the input-controlled neural LMs\nhave shown that short context LMs simulate human\nreading behavior surprisingly well, emphasizing\nthe LM-human gap in context access. Further anal-\nysis has shown that the gap could be associated\nwith specific syntactic constructions; injecting syn-\ntactic bias into LMs’ context access could be one\nway to make LMs more human-like. This study\nhas also asserted that the use of a modern LM pop-\nular in NLP as-is is not always a natural choice in\ncognitive modeling.\n9There are also some counter-arguments (Hao et al., 2020;\nOh et al., 2021; Kuribayashi et al., 2021; Anonymous, 2022).\n10429\nLimitations\nAs discussed in Section 7, this study alone could\nnot comprehensively explain the cause of the LM-\nhuman discrepancies. Nevertheless, our observa-\ntion itself could advance the step toward under-\nstanding the relationship between human sentence\nprocessing and computational models typically de-\nveloped in NLP, which is a central theme in the long\nhistory of artificial intelligence and the cognitive\nscience of language.\nThis study was scientifically motivated to under-\nstand humans and language; this could sound like\nless impact on engineering-oriented efforts (e.g.,\nsolving real-world problems accurately). However,\nsimulating human cognitive load during reading is\ndirectly associated with automatic text readability\nassessment. In addition, our study implies that hu-\nman sentence processing could be performed with\nmore efficient context access than modern LMs.\nThis encourages the development of language pro-\ncessing models with increased efficiency; this is\nrelated to the sustainability issues, such as the envi-\nronmental impact of creating gigantic NLP models.\nEthical considerations\nThis study explored the relationship between the\nLM-computed complexity measures and human\nreading behaviors. Human subjects’ privacy infor-\nmation in the eye-tracking data was anonymized.\nWe did not find any other ethical concerns; as a\nsomewhat minor point, the LMs used in our exper-\niments might be biased by the data we used (i.e.,\nWikipedia and Web data), although these follow\nthe commonly used settings in the NLP research.\nAcknowledgements\nWe would like to thank Jun Suzuki and Ryo\nYoshida for their valuable advice and appreciate\nthe members of the Tohoku NLP Group for their\nconstructive comments. This work was supported\nby Grant-in-Aid for JSPS Fellows Grant Num-\nber JP20J22697, JST PRESTO Grant Number JP-\nMJPR21C2, and JST CREST Grant Number JP-\nMJCR20D2, Japan.\nReferences\nBharat Ram Ambati, Siva Reddy, and Mark Steedman.\n2016. Assessing relative sentence complexity using\nan incremental ccg parser. In Proceedings of NAACL,\npages 1051–1057.\nAnonymous. 2022. Why does surprisal from smaller\nGPT-2 models provide better fit to human reading\ntimes? OpenReview.\nMasayuki Asahara, Hajime Ono, and Edson T\nMiyamoto. 2016. Reading-Time Annotations\nfor “Balanced Corpus of Contemporary Written\nJapanese”. In Proceedings of COLING, pages 684–\n694.\nC Aurnhammer and S L Frank. 2019. Comparing Gated\nand Simple Recurrent Neural Network Architectures\nas Models of Human Sentence Processing. In Pro-\nceedings of CogSci, pages 112–118.\nMaria Barrett, Željko Agi, and Anders Søgaard. 2015.\nThe Dundee Treebank. In Fourteenth International\nWorkshop on Treebanks and Linguistic Theories ,\npages 242–248.\nDouglas Bates, Martin Mächler, Ben Bolker, and Steve\nWalker. 2015. Fitting linear mixed-effects models\nusing lme4. Journal of Statistical Software, Articles,\n67(1):1–48.\nNoam Chomsky. 2005. Three factors in language design.\nLinguist. Inq., 36(1):1–22.\nAndy Clark. 2013. Whatever next? predictive brains,\nsituated agents, and the future of cognitive science.\nBehav. Brain Sci., 36(3):181–204.\nMatthew W Crocker. 2007. Computational psycholin-\nguistics. The Handbook of Computational Linguis-\ntics and Natural Language Processing.\nVera Demberg and Frank Keller. 2008. Data from eye-\ntracking corpora as evidence for theories of syntac-\ntic processing complexity. Journal of Cognition ,\n109(2):193–210.\nVictoria Fossum and Roger Levy. 2012. Sequential vs.\nHierarchical Syntactic Models of Human Incremen-\ntal Sentence Processing. In Proceedings of CMCL,\npages 61–69.\nStefan L Frank, Thijs Trompenaars, and Shravan Va-\nsishth. 2016. Cross-linguistic differences in pro-\ncessing double-embedded relative clauses: Working-\nmemory constraints or language statistics? Cogn.\nSci., 40(3):554–578.\nRichard Futrell, Edward Gibson, and Roger P. Levy.\n2020a. Lossy-Context Surprisal: An Information-\nTheoretic Model of Memory Effects in Sentence Pro-\ncessing. Journal of Cognitive Science.\nRichard Futrell and Roger Levy. 2017. Noisy-context\nsurprisal as a human sentence processing cost model.\nIn Proceedings of EACL, pages 688–698.\n10430\nRichard Futrell, Roger P. Levy, and Edward Gibson.\n2020b. Dependency locality as an explanatory prin-\nciple for word order. Journal of Language.\nLukas Galke, Yoav Ram, and Limor Raviv. 2022. Emer-\ngent communication for understanding human lan-\nguage evolution: What’s missing? Emergent Com-\nmunication Workshop at ICLR 2022.\nJon Gauthier and Roger Levy. 2019. Linking artificial\nand human neural representations of language. In\nProceedings of EMNLP-IJCNLP, pages 529–539.\nEdward Gibson. 1998. Linguistic complexity: Local-\nity of syntactic dependencies. Journal of Cognition,\n68(1):1–76.\nEdward Gibson. 2000. The dependency locality the-\nory: A distance-based theory of lingustic complexity.\nImage, language, brain, 2000:95–126.\nEdward Gibson, Richard Futrell, Steven P Piantadosi,\nIsabelle Dautriche, Kyle Mahowald, Leon Bergen,\nand Roger Levy. 2019. How efficiency shapes human\nlanguage. Trends Cogn. Sci., 23(5):389–407.\nAdam Goodkind and Klinton Bicknell. 2018. Predictive\npower of word surprisal for reading times is a linear\nfunction of language model quality. In Proceedings\nof CMCL, pages 10–18.\nJoseph H Greenberg. 1963. Some universals of gram-\nmar with particular reference to the order of meaning-\nful elements. http://tushik.org/wp-content/\nuploads/GRE-order.pdf. Accessed: 2022-10-18.\nMichael Hahn, Judith Degen, and Richard Futrell.\n2020a. Modeling word and morpheme order in natu-\nral language as an efficient tradeoff of memory and\nsurprisal. Psychological Review.\nMichael Hahn, Dan Jurafsky, and Richard Futrell.\n2020b. Universals of word order reflect optimiza-\ntion of grammars for efficient communication. Proc.\nNatl. Acad. Sci. U. S. A., 117(5):2347–2353.\nJohn Hale. 2001. A Probabilistic Earley Parser as a\nPsycholinguistic Model. In Proceedings of NAACL,\npages 159–166.\nJohn Hale, Chris Dyer, Adhiguna Kuncoro, and\nJonathan R. Brennan. 2018. Finding Syntax in Hu-\nman Encephalography with Beam Search. In Pro-\nceedings of ACL, pages 2727–2736.\nYiding Hao, Simon Mendelsohn, Rachel Sterneck,\nRandi Martinez, and Robert Frank. 2020. Probabilis-\ntic predictions of people perusing: Evaluating metrics\nof language model performance for psycholinguistic\nmodeling. In Proceedings of CMCL, pages 75–86,\nOnline.\nJohn A Hawkins. 1994. A performance theory of order\nand constituency. Cambridge University Press.\nKenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark,\nand Philipp Koehn. 2013. Scalable modified Kneser-\nNey language model estimation. In Proceedings of\nACL, pages 690–696.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nShort-Term Memory. Journal of Neural Computa-\ntion, 9(8):1735–1780.\nSamar Husain, Shravan Vasishth, and Narayanan Srini-\nvasan. 2014. Strong expectations cancel locality ef-\nfects: evidence from hindi. PLoS One, 9(7):e100986.\nAlan Kennedy, Robin Hill, and Joël Pynte. 2003. The\ndundee corpus. In Proceedings of the 12th European\nconference on eye movement.\nLars Konieczny. 2000. Locality and Parsing Complex-\nity. Journal of Psycholinguistic Research, 29(6):627–\n645.\nTaku Kudo. 2006. MeCab: Yet Another Part-of-speech\nand Morphological Analyzer. http://mecab. source-\nforge. jp.\nTatsuki Kuribayashi, Yohei Oseki, Takumi Ito, Ryo\nYoshida, Masayuki Asahara, and Kentaro Inui. 2021.\nLower perplexity is not always human-like. In Pro-\nceedings of ACL-IJCNLP, pages 5203–5217.\nRoger Levy. 2008. Expectation-based syntactic compre-\nhension. Journal of Cognition, 106(3):1126–1177.\nRichard L Lewis and Shravan Vasishth. 2005. An\nactivation-based model of sentence processing as\nskilled memory retrieval. Cogn. Sci., 29(3):375–419.\nRichard L Lewis, Shravan Vasishth, and Julie A\nVan Dyke. 2006. Computational principles of work-\ning memory in sentence comprehension. Trends\nCogn. Sci., 10(10):447–454.\nKikuo Maekawa, Makoto Yamazaki, Toshinobu\nOgiso, Takehiko Maruyama, Hideki Ogura, Wakako\nKashino, Hanae Koiso, Masaya Yamaguchi, Makiro\nTanaka, and Yasuharu Den. 2014. Balanced corpus of\ncontemporary written japanese. Language Resources\nand Evaluation, 48(2):345–371.\nDanny Merkx and Stefan L. Frank. 2021. Human sen-\ntence processing: Recurrence or attention? In Pro-\nceedings of CMCL, pages 12–22.\nJames A Michaelov, Megan D Bardolph, Seana Coulson,\nand Benjamin Bergen. 2021. Different kinds of cog-\nnitive plausibility: why are transformers better than\nRNNs at predicting N400 amplitude? Proceedings\nof CogSci, 43(43).\nKentaro Nakatani and Edward Gibson. 2008. Distin-\nguishing theories of syntactic expectation cost in sen-\ntence comprehension: Evidence from Japanese. Jour-\nnal of Linguistics, 46(1):63–87.\nByung-Doh Oh, Christian Clark, and William Schuler.\n2021. Surprisal estimators for human reading times\nneed character models. In Proceedings of ACL-\nIJCNLP, pages 3746–3757.\n10431\nMai Omura and Masayuki Asahara. 2018. UD-Japanese\nBCCWJ: Universal Dependencies annotation for the\nBalanced Corpus of Contemporary Written Japanese.\nIn Proceedings of the Second Workshop on Universal\nDependencies (UDW 2018), pages 117–125.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for se-\nquence modeling. In Proceedings of NAACL (Demon-\nstrations), pages 48–53.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners.\nMathieu Rita, Rahma Chaabouni, and Emmanuel\nDupoux. 2020. “lazimpa”: Lazy and impatient\nneural agents learn to communicate efficiently. In\nProceedings of CoNLL, pages 335–343.\nMartin Schrimpf, Idan Blank, Greta Tuckute, Carina\nKauf, Eghbal A Hosseini, Nancy Kanwisher, Joshua\nTenenbaum, and Evelina Fedorenko. 2020. The\nneural architecture of language: Integrative reverse-\nengineering converges on a model for predictive pro-\ncessing. bioRxiv.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural Machine Translation of Rare Words\nwith Subword Units. In Proceedings of ACL, pages\n1715–1725.\nNathaniel J. Smith and Roger Levy. 2013. The effect\nof word predictability on reading time is logarithmic.\nJournal of Cognition, 128(3):302–319.\nMerity Stephen, Xiong Caiming, Bradbury James,\nSocher Richard, et al. 2016. Pointer sentinel mix-\nture models. In Proceedings of ICLR.\nRyo Ueda and Koki Washio. 2021. On the relationship\nbetween zipf’s law of abbreviation and interfering\nnoise in emergent languages. In Proceedings of ACL\n(SRW), pages 60–70.\nMarten van Schijndel, Andy Exley, and William Schuler.\n2013. A model of language processing as hierarchic\nsequential prediction. Top. Cogn. Sci., 5(3):522–540.\nShravan Vasishth, Katja Suckow, Richard L Lewis, and\nSabine Kern. 2010. Short-term forgetting in sentence\ncomprehension: Crosslinguistic evidence from verb-\nfinal structures. Lang. Cogn. Process., 25(4):533–\n567.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. NIPS, pages 5998–6008.\nEthan Gotlieb Wilcox, Jon Gauthier, Jennifer Hu, Peng\nQian, and Roger Levy. 2020. On the Predictive\nPower of Neural Language Models for Human Real-\nTime Comprehension Behavior. In Proceedings of\nCogSci, pages 1707–1713.\nRyo Yoshida, Hiroshi Noji, and Yohei Oseki. 2021.\nModeling human sentence processing with left-\ncorner recurrent neural network grammars. In Pro-\nceedings of EMNLP, pages 2964–2973.\n10432\nA Psychometric predictive power and\nregression models\nPsychometric predictive power refers to the sim-\nilarity between (lossy-context) surprisal and hu-\nman gaze duration, calculated using a linear mixed-\neffects regression (Bates et al., 2015). First, gaze\nduration (GD) is modeled by the following formula:\nGD ∼ surprisal + surprisal_prev_1\n+ surprisal_prev_2 + freq ∗ length\n+ freq_prev_1 ∗ length_prev_1\n+ screenN + lineN + segmentN\n+ (1|article) + (1|subj) .\n(4)\nTable 5 shows the descriptions for the factors\nused in the above formulation. Then, a base-\nline regression model without the surprisal,\nsurprisal_prev_1, and surprisal_prev_2\nterms from Eq. 4 is trained additionally. We calcu-\nlated the per-token average of the log-likelihood\ndifference (∆LogLik) between the two regression\nmodels.\nB Probabilistic erasure noise\nFutrell and Levy (2017) and Futrell et al. (2020a)\nsuggested that a linear probabilistic erasure noise\n(LPEN), where more distant items are more likely\nto disappear as opposed to a constant cutoff point\nwith n-grams, might be a plausible design of input\nlimitations. We examined whether such a proba-\nbilistic nature of noise design substantially affects\nour conclusions. Within our experimental settings,\nthere is no substantial difference in the results re-\ngardless of the probabilistic nature of noise.\nMethods. To implement LPEN, we erased the\nj-th nearest word in the context with a proba-\nbility of min(j ∗a,1), here a > 0. We ini-\ntially observed that erasing too close context hin-\ndered human-like behavior; we also introduced\nan always-present portion of the context ( l near-\nest words) and applied noise only on farther\nwords. That is, the probabilistic erasure noise\nis only applied to [ w0,··· ,wi−l−1]. Assuming\na = 0 .25, wi−l−1 is then erased with a proba-\nbility of 0.25, wi−l−2 is erased with a probabil-\nity of 0.5, and so on, while the l nearest words\nto the target are left intact. We compared the\nPPP of surprisal with l ∈{2,3,5,7,10,20}and\na∈{0.5,0.25,0.125,0.0625}.\nContext limitation did not change or improved\nPPP. The results are shown in Figure 6. The\nFactor Type Description\nsurprisal num (lossy-context) surprisal cal-\nculated by LMs\nGD num reading time (first pass gaze\nduration)\narticle factor article ID\nscreenN int screen display order\nlineN int the serial number of line the\nsegment is displayed\nsegmentN int the serial number of seg-\nment in a screen\nsentN int the serial number of sen-\ntence the segment belongs\nto\ntokenN int the position of segment in\nsentence\nlength int number of characters\nfreq num geometric mean of the fre-\nquencies of subword con-\nstituents in a segment\nsubj factor participant ID\nTable 5: Factor names and their description.\ntrends were similar to those using discrete con-\ntext noise (Figure 1): (i) context limitation did not\nchange or improved PPP and (ii) larger LMs have\nlarger PPP gain due to context limitation.\nC Exclusion criteria for eye movement\ndata\nWe excluded outliers following Goodkind and Bick-\nnell (2018). Specifically, we excluded the data\npoints meeting any of the following criteria in the\nEnglish experiments, and those meeting (a), (c), or\n(e) in the Japanese experiments:\n(a) has zero gaze duration or beyond three standard\ndeviations\n(b) contains punctuation\n(c) contains numeric characters\n(d) the next segment has punctuation or numeric\ncharacters\n(e) is the first segment in a line\n(f) is the last segment in a line\nWe included data points meeting (b) and (f) in the\nJapanese data out of concern that excluding them\ndisregards the data points for the main verb, regard-\ning the verb-final Japanese construction (punctu-\nation is included in a bunsetsu). Note that in the\nJapanese data, the first/end word in a line corre-\nspond to first/end word in a sentence (sentences\nare presented line by line.). Similarly, (d) substan-\ntially reduces the Japanese data points, and the\ninter-segment-level influence of special symbols\n10433\nLSTM-xs-WikiGPT2-xs-WikiGPT2-md-WikiGPT2-smGPT2-mdGPT2-lgGPT2-xl LSTM-xs-WikiGPT2-xs-WikiGPT2-md-Wikiinput lengthshortlong input lengthshortlong\nEnglish Japanese\nFigure 6: Relationship between the strength of input noise (X-axis) and PPP (Y-axis) under the probabilistic\nerasure noise settings. The results with various noise settings ( L×Awhere L = {2,3,5,7,10,20}and A =\n{0.5,0.25,0.125,0.0625}) are summarized with respect to average input length in each setting. The marker color\nand shape correspond to LM architectures.\nwould be less likely than in English considering\nthat bunsetsu is a relatively larger unit than the\nEnglish word.\nD Hyperparameters of LMs\nTable 6 shows the hyperparameters of Wiki-\nLMs. Training each LM took approximately\nthree days on four GPUs (NVIDIA V100).\nThe hyperparameters of OpenAI GPT-2s could\nbe shown in https://huggingface.co/docs/\ntransformers/model_doc/gpt2.\nAs for the subword tokenization used in Wiki-\nLMs, we set character coverage to 0.9995. V ocabu-\nlary size was set to 32,000 in English and 100,000\nin Japanese, taking the rich characters and mor-\nphemes in Japanese into consideration. Note that\nthis difference results in Japanese Wiki-LMs hav-\ning more parameters than English LMs. In the\nJapanese settings, LSTM-xs-Wiki has 54M (27M\nin English), GPT2-xs-Wiki has 55M (29M in En-\nglish), and GPT2-md-Wiki has 404M (335M in\nEnglish) parameters.\nE Mitigating training-inference gaps\nAs introduced in Section 4.1, we modified the\ntraining data to augment the data points, where\nLMs must predict the upcoming tokens from the\nmiddle of a sentence with severely limited con-\ntext. Specifically, we first split each ( i-th) sen-\ntence, si = [wi\n0,wi\n1,··· ,wi\nn], in training data into\ntwo sub-sequences: [<s>, wi\n0,··· ,wi\nk−1] and [<b>,\nwi\nk,··· ,wi\nn]. Here, the breakpoint ki for the i-\nth sentence is sampled from the uniform distri-\nbution U(0,|si|). When k = 0 , the former sub-\nsequence is [ <s>]. Then, the sub-sequences ob-\ntained from the whole corpus were randomly con-\ncatenated to create the modified training data (e.g.,\n[···, <b>, wc\nkc+1,··· ,wc\nn, <b>, wa\nk+1,··· ,wa\nn,\n<s>, wl\n0,··· ,wl\nk, ···]). This modified data has\ntwo characteristics: (i) there is no dependency be-\ntween before and after the special tokens (<s> and\n<b>), and (ii) to the <b> token, uniform prior about\nthe token position within the sentence is set. It\nis expected that training LMs with this data and\ncomputing the next word prediction (e.g., with the\nquery of [<b>, w]) enables them to adequately com-\npute the upcoming token (i) with severely limited\ncontext and (ii) without any token position prior\nfor <b>.\nConcretely, the lossy-context surprisal is com-\nputed by the modified LMs as follows:\nIlossy(wi,c<i,f)\n= −log pθ(wi|<b> ◦f([w0,··· ,wi−1])) , (5)\nOnly when the <b> token corresponds to the\nsentence initial position, <s> was set instead of\n<b>.\nF Perplexity of n-gram LMs\nPerplexity (PPL): Perplexity (PPL), the in-\nverse geometric mean of next-word probabilities\np(wi|w<i) in a text that consists of N symbols\n(w1,w2,··· ,wN), is a typical evaluation metric\nfor the next-word prediction accuracy. Given a LM\nθand context noise design f, the perplexity under\nlossy-context settings is calculated as follows:\n10434\nEnglish Japanese\n23571020full words 235710full words\nFigure 7: Relationship between the perplexity of n-gram LMs and input length. A monotonic relationship, the\nlonger context LMs use, the lower perplexity LMs exhibit, is observed. The colored areas show a 95% confidence\ninterval. The PPL was computed at the subword level; here, directly comparing the scale of Y-axis across languages\nis non-sense due to their different segmentation (e.g., vocabulary size).\nPPL =\nN∏\ni=0\npθ(wi|BOS ◦f(c<i))−1\nN . (6)\nLow PPL indicates that the model and the con-\ntext noise yielded accurate predictions about the\nupcoming signal.\nRelationship between perplexity and context\nlength: Figure 7 shows the relationship between\nthe perplexity of n-gram LMs and their average\ncontext length. The PPL values are computed with\nthe texts in the eye movement data. A monotonic\nrelationship, the longer context LMs use, the lower\nperplexity LMs exhibit, is observed. This ensures\nthat LMs with long context actually exploit the in-\nformation in the added context to accurately predict\nthe upcoming symbols.\nG Next-word prediction accuracy and\nPPP in Japanese\nFigure 8 shows the relationship between PPL and\nPPP in the Japanese experiments. Similar to the\nresults of Section 5.1, LMs with a similar PPL\nvalue exhibited different PPP (e.g., results around\nPPL=60).\nLSTM-xs-WikiGPT2-xs-WikiGPT2-md-WikiContext lengthshortlong\nModel\nFigure 8: Relationship between PPP and perplexity\n(PPL) drawn using the Japanese LMs targeted in Sec-\ntion 5.1. Each point corresponds to each configuration\nof the n-gram surprisal computation; marker color and\nshape present the LM architectures, and larger markers\ncorrespond to longer context access.\n10435\nFairseq\nmodel\narchitecture transformer_lm _gpt2_small\nadaptive softmax cut off 50,000, 140,000\nshare-decoder-input-output-embed True\nembed_dim 1,024\nffn_embed_dim 4,096\nlayers 24\nheads 16\ndropout 0.1\nattention_dropout 0.1\nOptimizer\nalgorithm AdamW\nlearning rates 5e-4\nbetas (0.9, 0.98)\nweight decay 0.01\nclip norm 0.0\nLearning rate\nscheduler\ntype inverse_sqrt\nwarmup updates 4,000\nwarmup init lrarning rate 1e-7\nTraining batch size 61,440 tokens\nsample-break-mode none\n(a) GPT2-md-Wiki.\nFairseq\nmodel\narchitecture transformer_lm_gpt\nadaptive softmax cut off 50,000, 140,000\nshare-decoder-input-output-embed True\nembed_dim 384\nffn_embed_dim 2,048\nlayers 8\nheads 6\ndropout 0.1\nattention_dropout 0.1\nOptimizer\nalgorithm AdamW\nlearning rates 5e-4\nbetas (0.9, 0.98)\nweight decay 0.01\nclip norm 0.0\nLearning rate\nscheduler\ntype inverse_sqrt\nwarmup updates 4,000\nwarmup init learning rate 1e-7\nTraining batch size 61,440 tokens\nsample-break-mode none\n(b) GPT2-xs-Wiki.\nFairseq\nmodel\narchitecture lstm_lm\nadaptive softmax cut off 50,000, 140,000\nshare-decoder-input-output-embed True\nembed_dim 400\nhiden_size 1,024\nlayers 2\ndropout 0.1\nOptimizer\nalgorithm AdamW\nlearning rates 1e-3\nbetas (0.9, 0.98)\nweight decay 0.01\nclip norm 0.0\nLearning\nrate\nscheduler\ntype inverse_sqrt\nwarmup updates 4,000\nwarmup init learning rate 1e-7\nTraining batch size 20,480 tokens\nsample-break-mode none\n(c) LSTM-xs-Wiki.\nTable 6: Hyperparameters of the LMs.\n10436"
}