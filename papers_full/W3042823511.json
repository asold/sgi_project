{
  "title": "Feature Pyramid Transformer",
  "url": "https://openalex.org/W3042823511",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5101818657",
      "name": "Dong Zhang",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5042324027",
      "name": "Hanwang Zhang",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A5035112538",
      "name": "Jinhui Tang",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5100377147",
      "name": "Meng Wang",
      "affiliations": [
        "Hefei University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5024965898",
      "name": "Xian‚ÄêSheng Hua",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5101633158",
      "name": "Qianru Sun",
      "affiliations": [
        "Singapore Management University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2598915960",
    "https://openalex.org/W3106250896",
    "https://openalex.org/W2890782586",
    "https://openalex.org/W2981899103",
    "https://openalex.org/W2963840672",
    "https://openalex.org/W2109255472",
    "https://openalex.org/W2598666589",
    "https://openalex.org/W2340897893",
    "https://openalex.org/W2890166761",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2964080601",
    "https://openalex.org/W8437397",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2940262938",
    "https://openalex.org/W2550553598",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963857746",
    "https://openalex.org/W2991471181",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W2606722458",
    "https://openalex.org/W2963311325",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2799166040",
    "https://openalex.org/W2962731754",
    "https://openalex.org/W2963382180",
    "https://openalex.org/W2960251737",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2955813853",
    "https://openalex.org/W2037227137",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2737258237",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963911037",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2953212265",
    "https://openalex.org/W2966112933",
    "https://openalex.org/W2630837129",
    "https://openalex.org/W2963815618",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W2162915993",
    "https://openalex.org/W2963727650",
    "https://openalex.org/W2412782625"
  ],
  "abstract": "Feature interactions across space and scales underpin modern visual recognition systems because they introduce beneficial visual contexts. Conventionally, spatial contexts are passively hidden in the CNN's increasing receptive fields or actively encoded by non-local convolution. Yet, the non-local spatial interactions are not across scales, and thus they fail to capture the non-local contexts of objects (or parts) residing in different scales. To this end, we propose a fully active feature interaction across both space and scales, called Feature Pyramid Transformer (FPT). It transforms any feature pyramid into another feature pyramid of the same size but with richer contexts, by using three specially designed transformers in self-level, top-down, and bottom-up interaction fashion. FPT serves as a generic visual backbone with fair computational overhead. We conduct extensive experiments in both instance-level (i.e., object detection and instance segmentation) and pixel-level segmentation tasks, using various backbones and head networks, and observe consistent improvement over all the baselines and the state-of-the-art methods.",
  "full_text": "Feature Pyramid Transformer\nDong Zhang1 Hanwang Zhang2 Jinhui Tang1‚ãÜ Meng Wang3\nXiansheng Hua4 Qianru Sun5\n{dongzhang, jinhuitang}@njust.edu.cn hanwangzhang@ntu.edu.sg eric.mengwang@gmail.com\nxiansheng.hxs@alibaba-inc.com qianrusun@smu.edu.sg\n1School of Computer Science and Engineering, Nanjing University of Science and Technology\n2Nanyang Technological University 3Hefei University of Technology\n4Damo Academy, Alibaba Group 5Singapore Management University\nAbstract. Feature interactions across space and scales underpin mod-\nern visual recognition systems because they introduce beneÔ¨Åcial visual\ncontexts. Conventionally, spatial contexts are passively hidden in the\nCNN‚Äôs increasing receptive Ô¨Åelds or actively encoded by non-local con-\nvolution. Yet, the non-local spatial interactions are not across scales, and\nthus they fail to capture the non-local contexts of objects (or parts) re-\nsiding in diÔ¨Äerent scales. To this end, we propose a fully active feature\ninteraction across both space and scales, called Feature Pyramid Trans-\nformer (FPT). It transforms any feature pyramid into another feature\npyramid of the same size but with richer contexts, by using three specially\ndesigned transformers in self-level, top-down, and bottom-up interaction\nfashion. FPT serves as a generic visual backbone with fair computational\noverhead. We conduct extensive experiments in both instance-level (i.e.,\nobject detection and instance segmentation) and pixel-level segmentation\ntasks, using various backbones and head networks, and observe consistent\nimprovement over all the baselines and the state-of-the-art methods 1.\nKeywords: Feature pyramid; Visual context; Transformer; Object de-\ntection; Instance segmentation; Semantic segmentation\n1 Introduction\nModern visual recognition systems stand in context. Thanks to the hierarchical\nstructure of Convolutional Neural Network (CNN), as illustrated in Fig. 1 (a),\ncontexts are encoded in the gradually larger receptive Ô¨Åelds (the green dashed\nrectangles) by pooling [1,2], stride [3] or dilated convolution [4]. Therefore, the\nprediction from the last feature map is essentially based on the rich contexts ‚Äî\neven though there is only one ‚Äúfeature pixel‚Äù for a small object, e.g., mouse, its\nrecognition will be still possible, due to the perception of larger contexts, e.g.,\ntable and computer [5,6].\nScale also matters ‚Äî the mouse recognition deserves more feature pixels, not\nonly the ones from the last feature map, which easily overlooks small objects. A\n‚ãÜ Corresponding author.\n1 Code is open-sourced at https://github.com/ZHANGDONG-NJUST\narXiv:2007.09451v1  [cs.CV]  18 Jul 2020\n2 D. Zhang, H. Zhang, et al.\npredict\npredict\ndesk\nchair\ncomputer\n computer\ndesk\npredict\nmouse\n(a) CNN (b) Feature \nPyramid\n(d) Non-local Spatial \nInteraction\n(c) Cross-scale \nInteraction\n(e) Feature Pyramid \nTransformer\npredict\ncomputer\nchair\ndesk\npredict\nFig. 1.The evolution of feature interaction across space and scale in feature pyramid\nfor visual context. Transparent cubes: feature maps. Shadedpredict: task-speciÔ¨Åc head\nnetworks. The proposed Feature Pyramid Transformer is inspired by the evolution.\nconventional solution is to pile an image pyramid for the same image [7], where\nthe higher/lower levels are images of lower/higher resolutions. Thus, objects of\ndiÔ¨Äerent scales are recognized in their corresponding levels, e.g., mouse in lower\nlevels (high resolution) and table in higher levels (low resolution). However,\nthe image pyramid multiplies the time-consuming CNN forward pass as each\nimage requires a CNN for recognition. Fortunately, CNN oÔ¨Äers an in-network\nfeature pyramid[8], i.e., lower/higher-level feature maps represent higher/lower-\nresolution visual content without computational overhead [9,10]. As shown in\nFig. 1 (b), we can recognize objects of diÔ¨Äerent scales by using feature maps of\ndiÔ¨Äerent levels, i.e., small objects ( computer) are recognized in lower-levels and\nlarge objects ( chair and desk) are recognized in higher-levels [11,12,13].\nSometimes the recognition ‚Äî especially forpixel-level labeling such as seman-\ntic segmentation ‚Äî requires to combine the contexts from multiple scales [14,15].\nFor example in Fig. 1 (c), to label pixels in the frame area of the monitor, per-\nhaps the local context of the object itself from lower levels is enough; however,\nfor the pixels in the screen area, we need to exploit both of the local context and\nthe global context from higher levels, because the local appearance of monitor\nscreen is close to TV screen, and we should use scene context such as keyboard\nand mouse to distinguish between the two types.\nThe spirit of the above non-local context is recently modeled in a more ex-\nplicit and active fashion ‚Äî as opposed to the above passive feature map pile ‚Äî\nby using the non-local convolution [16] and self-attention [17,18]. Such spatial\nfeature interaction is expected to capture the reciprocal co-occurring patterns\nof multiple objects [19,11]. As shown in Fig. 1 (d), it is more likely that there is\na computer on the desk rather than on road, thus, the recognition of either is\nhelpful to the other.\nThe tale of context and scale should continue, and it is our key motivation.\nIn particular, we are inspired by the omission of the cross-scale interactions\n(Fig. 1 (c)) in the non-local spatial interactions (Fig. 1 (d)). Moreover, we be-\nlieve that the non-local interaction per se should happen in the corresponding\nscales of the interacted objects (or parts), but not just in one uniform scale\nas in existing methods [17,16,19]. Fig. 1 (e) illustrates the expected non-local\nFeature Pyramid Transformer 3\n(a) Feature Pyramid (b) Feature Pyramid Transformer (c) Transformed Feature Pyramid\nHead \nNetwork\nb\na\nc\nre-arranged\nfeatures\nconcatenated\nfeatures\nc\nb\na\ntransformed\nfeatures\nSelf-transformer\n Grounding transformer\n Rendering transformer \nConv\nConv\nConv\nFig. 2.Overall structure of our proposed FPT network. DiÔ¨Äerent texture patterns in-\ndicate diÔ¨Äerent feature transformers, and diÔ¨Äerent color represents feature maps with\ndiÔ¨Äerent scales. ‚ÄúConv‚Äù denotes a 3 √ó3 convolution with the output dimension of\n256. Without loss of generality, the top/bottom layer feature maps has no render-\ning/grounding transformer.\ninteractions across scales: the low-level mouse is interacting with the high-level\ncomputer, which is interacting with desk at the same scale.\nTo this end, we propose a novel feature pyramid network called Feature\nPyramid Transformer (FPT) for visual recognition, such as instance-level\n(i.e., object detection and instance segmentation) and pixel-level segmentation\ntasks. In a nutshell, as illustrated in Fig. 2, the input of FPT is a feature pyramid,\nand the output is a transformed one, where each level is a richer feature map\nthat encodes the non-local interactions across space and scales. Then, the feature\npyramid can be attached to any task-speciÔ¨Åc head network. As its name implies,\nFPT‚Äôs interaction adopts the transformer-style [17,18]. It has the neat query,\nkey and value operation (cf. Section 3.1) that is shown eÔ¨Äective in selecting\ninformative long-range interaction, which tailors our goal: non-local interaction\nat proper scales. In addition, the computation overhead (cf. Section 4.1) can be\nalleviated by using TPUs like any other transformer models [20].\nOur technical contributions, as illustrated in the FPT breakdown in Fig. 2,\nare the designs of three transformers: 1) Self-Transformer (ST). It is based on\nthe classic non-local interaction within the same level feature map [16], and the\noutput has the same scale as its input. 2) Grounding Transformer (GT). It\nis in a top-down fashion, and the output has the same scale as the lower-level\nfeature map. Intuitively, we ground the ‚Äúconcept‚Äù of the higher-level feature\nmaps to the ‚Äúpixels‚Äù of the lower-level ones. In particular, as it is unnecessary\nto use the global information to segment objects, and the context within a lo-\ncal region is empirically more informative, we also design a locality-constrained\nGT for both eÔ¨Éciency and accuracy of semantic segmentation. 3) Rendering\nTransformer (RT). It is in a bottom-up fashion, and the output has the same\nscale as the higher-level feature map. Intuitively, we render the higher-level ‚Äúcon-\ncept‚Äù with the visual attributes of the lower-level ‚Äúpixels‚Äù. Note that this is a\nlocal interaction as it is meaningless to render an ‚Äúobject‚Äù with the ‚Äúpixels‚Äù of\n4 D. Zhang, H. Zhang, et al.\nanother distant one. The transformed feature maps of each level (the red, blue\nand green) are re-arranged to its corresponding map size and then concatenated\nwith the original map, before feeding into the conv-layer that resize them to the\noriginal ‚Äúthickness‚Äù.\nExtensive experiments show that FPT can greatly improve conventional de-\ntection/segmentation pipelines by the following absolute gains: 1) 8.5% box-AP\nfor object detection and 6.0% mask-AP for instance segmentation over baseline\non the MS-COCO [21] test-dev; 2) for semantic segmentation, 1.6% and 1.2%\nmIoU on Cityscapes [22] and PASCAL VOC 2012 [23] test sets, respectively;\n1.7% and 2.0% mIoU on ADE20K [24] and LIP [25] validation sets, respectively.\n2 Related Work\nFPT is generic to apply in a wide range of computer vision tasks. This paper\nfocuses on two instance-level tasks: object detection, instance segmentation, and\none pixel-level task: semantic segmentation. Object detection aims to predict a\nbounding box for each object and then assigns the bounding box a class label [6],\nwhile instance segmentation is additionally required to predict a pixel-level mask\nof the object [26]. Semantic segmentation aims to predict a class label to each\npixel of the image [27].\nFeature pyramid. The in-network feature pyramid ( i.e., the Bottom-up Fea-\nture Pyramid (BFP) [12]) is one of the most commonly used methods, and has\nbeen shown useful for boosting object detection [9], instance segmentation [13]\nand semantic segmentation [28]. Another popular method of constructing fea-\nture pyramid uses feature maps of the scale while processing the maps through\npyramidal pooling or dilated/atrous convolutions. For example, atrous spatial\npyramid pooling [14] and pyramid pooling module [1,15] leverages output feature\nmaps of the last convolution layer in the CNN backbone to build the four-level\nfeature pyramid, in which diÔ¨Äerent levels have the same resolution but diÔ¨Äer-\nent information granularities. Our approach is based on the existing BFP (for\nthe instance-level) and unscathed feature pyramid [29] (for the pixel-level). Our\ncontribution is the novel feature interaction approach.\nFeature interaction. An intuitive approach to the cross-scale feature interac-\ntion is gradually summing the multi-scale feature maps, such as Feature Pyramid\nNetwork (FPN) [12] and Path Aggregation Network (PANet) [13]. In particular,\nboth FPN and PANet are based on BFP, where FPN adds a top-down path to\npropagate semantic information into low-level feature maps, and PANet adds\na bottom-up path augmentation on the basis of FPN. Another approach is to\nconcatenate multi-scale feature maps along the channel dimension. The spe-\nciÔ¨Åc examples for semantic segmentation are DeepLab [30] and pyramid scene\nparsing network [15]. Besides, a more recent work proposed the ZigZagNet [31]\nwhich exploits the addition and convolution to enhance the cross-scale feature\ninteraction. Particularly, for the within-scale feature interaction, some recent\nworks exploited non-local operation [16] and self-attention [17] to capture the\nco-occurrent object features in the same scene. Their models were evaluated in\nFeature Pyramid Transformer 5\na wide range of visual tasks [11,32,19,33]. However, we argue that the non-local\ninteraction performed in just one uniform scale feature map is not enough to\nrepresent the contexts. In this work, we aim to conduct the non-local interaction\nper se in the corresponding scales of the interacted objects (or parts).\n3 Feature Pyramid Transformer\nGiven an input image, we can formally extract a feature pyramid, where the\nÔ¨Åne-/coarse-grained feature maps are in low/high levels, respectively. Without\nloss of generality, we express a low-level Ô¨Åne-grained feature map as Xf and a\nhigh-level coarse-grained feature map as Xc. Feature Pyramid Transformer\n(FPT) enables features to interact across space and scales. It speciÔ¨Åcally includes\nthree transformers: self-transformer (cf. Section 3.2), grounding transformer (cf.\nSection 3.3) and rendering transformer (cf. Section 3.4). The transformed feature\npyramid is in the same size but with richer contexts than the original.\n3.1 Non-Local Interaction Revisited\nA typical non-local interaction [16] operates onqueries(Q), keys(K) and values(V)\nwithin a single feature map X, and the output is the transformed version ÀúX with\nthe same scale as X. This non-local interaction is formulated as:\nInput: q i,kj,vj\nSimilarity: si,j = Fsim(qi,kj)\nWeight: wi,j = Fnom(si,j)\nOutput: ÀúXi = Fmul(wi,j,vj),\n(1)\nwhere qi = fq(Xi) ‚ààQ is the ith query; kj = fk(Xj) ‚ààK and vj = fv(Xj) ‚ààV\nare the jth key/value pair; fq(¬∑), fk(¬∑) and fv(¬∑) denote the query, key and\nvalue transformer functions [18,17], respectively. Xi and Xj are the ith and jth\nfeature positions in X, respectively. Fsim is the similarity function (default as\ndot product); Fnom is the normalizing function (default as softmax); Fmul is the\nweight aggregation function (default as matrix multiplication); and ÀúXi is the ith\nfeature position in the transformed feature map ÀúX.\n3.2 Self-Transformer\nSelf-Transformer (ST) aims to capture the co-occurring object features on\none feature map. As illustrated in Fig. 3 (a), ST is a modiÔ¨Åed non-local interac-\ntion [16] and the output feature map ÀÜX has the same scale as its inputX. A main\ndiÔ¨Äerence with [17,16] is that we deploy the Mixture of Softmaxes (MoS) [34]\nas the normalizing function Fmos, which turns out to be more eÔ¨Äective than the\nstandard Softmax [19] on images. SpeciÔ¨Åcally, we Ô¨Årst divide qi and kj into N\n6 D. Zhang, H. Zhang, et al.\nparts. Then, we calculate a similarity score sn\ni,j for every pair, i.e., qi,n, kj,n,\nusing Fsim. The MoS-based normalizing function Fmos is as follows:\nFmos(sn\ni,j) =\nN‚àë\nn=1\nœÄn\nexp(sn\ni,j)‚àë\nj exp(sn\ni,j), (2)\nwhere sn\ni,j is the similarity score of the nth part. œÄn is the nth aggregating\nweight that is equal to Softmax\n(\nwT\nn ¬Øk\n)\n, where wn is a learnable linear vector\nfor normalization and ¬Øk is the arithmetic mean of all positions of kj. Based on\nFmos, we then can reformulate Eq. 1 to elaborate our proposed ST as follows:\nInput: q i,kj,vj,N\nSimilarity: sn\ni,j = Fsim(qi,n,kj,n)\nWeight: wi,j = Fmos(sn\ni,j)\nOutput: ÀÜXi = Fmul(wi,j,vj),\n(3)\nwhere ÀÜXi is the ith transformed feature position in ÀÜX.\n3.3 Grounding Transformer\nGrounding Transformer (GT) can be categorized as a top-down non-local in-\nteraction [16], which grounds the ‚Äúconcept‚Äù in the higher-level feature maps Xc\nto the ‚Äúpixels‚Äù in the lower-level feature maps Xf . The output ÀÜXf has the same\nscale as Xf . Generally, image features at diÔ¨Äerent scales extract diÔ¨Äerent seman-\ntic or contextual information or both [8,28]. Moreover, it has been empirically\nshown that the negative value of the euclidean distanceFeud is more eÔ¨Äective in\ncomputing the similarity than dot productwhen the semantic information of two\nfeature maps is diÔ¨Äerent [35]. So we prefer to use Feud as the similarity function,\nwhich is expressed as:\nFeud(qi,kj) = ‚àí||qi ‚àíkj||2, (4)\nwhere qi = fq(Xf\ni ) and kj = fk(Xc\nj); Xf\ni is the ith feature position in Xf , and\nXc\nj is the jth feature position in Xc. We then replace the similarity function in\nEq. 3 with Feud, and get the formulation of the proposed GT as follows:\nInput: q i,kj,vj,N\nSimilarity: sn\ni,j = Feud(qi,n,kj,n)\nWeight: wi,j = Fmos(sn\ni,j)\nOutput: ÀÜXf\ni = Fmul(wi,j,vj),\n(5)\nwhere vj = fv(Xc\nj); ÀÜXf\ni is the ith transformed feature position in ÀÜXf . Based on\nEq. 5, each pair of qi and kj with a closer distance will be given a larger weight\nas in [17,16]. Compared to the results of dot product, using Feud brings clear\nimprovements in the top-down interactions2.\n2 More details are given in Section Aof the supplementary.\nFeature Pyramid Transformer 7\n(c) Locality-constrained GT\n (d) Rendering  Transformer\nStride conv\nùêê\nùêï\nùê∫ùê¥ùëÉ(ùêä)\n(a) Self-Transformer\n (b) Conventional \nCross-scale Interaction\nFig. 3.Self-Transformer(ST), Conventional Cross-Scale Interaction in existing meth-\nods, Locality-constrained Grounding Transformer (GT), and Rendering Transformer.\nThe red grid in low-level is a query position; grids in high-level are the key and the\nvalue positions (within a local square area in (b)); Q are the high-level feature maps,\nK and V are the low-level feature maps. Grey square is the down-sampled V.\nIn feature pyramid, high-/low-level feature maps contain much global/local\nimage information. However, for semantic segmentation by cross-scale feature\ninteractions, it is unnecessary to use global information to segment two objects\nin an image. The context within a local region around the query position is em-\npirically more informative. That is why the conventional cross-scale interaction\n(e.g., summation and concatenation) is eÔ¨Äective in existing segmentation meth-\nods [30,15]. As shown in Fig. 3 (b), they are essentially the implicit local style.\nHowever, our default GT is the global interaction.\nLocality-constrained Grounding Transformer. We therefore introduce a\nlocality-constrained version of GT called Locality-constrained GT (LGT) for se-\nmantic segmentation, which is an explicit local feature interaction. As illustrated\nin Fig. 3 (c), each qi (i.e., the red grid on the low-level feature map) interacts\nwith a portion of kj and vj (i.e., the blue grids on the high-level feature map)\nwithin the local square area where the center coordinate is the same with qi and\nthe side length is square size. Particularly, for positions of kj and vj that exceed\nthe index, we use 0 value instead.\n3.4 Rendering Transformer\nRendering Transformer(RT) works in a bottom-up fashion, aiming to render\nthe high-level ‚Äúconcept‚Äù by incorporating the visual attributes in the low-level\n‚Äúpixels‚Äù. As illustrated in Fig. 3 (d), RT is a local interaction where the local is\ndue to the fact that it is meaningless to render an ‚Äúobject‚Äù with the features or\nattributes from another distant one.\nIn our implementation, RT is not performed by pixel but the entire feature\nmaps. SpeciÔ¨Åcally, the high-level feature map is deÔ¨Åned as Q; the low-level fea-\nture map is deÔ¨Åned asK and V. To highlight the rendering target, the interaction\nbetween Q and K is conducted in a channel-wise attention manner [36]. K Ô¨Årst\ncomputes a weight w for Q through Global Average Pooling (GAP) [37]. Then,\nthe weighted Q (i.e., Qatt) goes through a 3 √ó3 convolution for reÔ¨Ånement [38].\nV goes through a 3 √ó3 convolution with stride to reduce the feature scale (the\n8 D. Zhang, H. Zhang, et al.\ngray square in Fig. 3 (d)). Finally, the reÔ¨Åned Qatt and the down-sampled V\n(i.e., Vdow) are summed-up, and processed by another 3 √ó3 convolution for\nreÔ¨Ånement. The proposed RT can be formulated as follows:\nInput: Q ,K,V\nWeight: w = GAP(K)\nWeight Query: Qatt = Fatt(Q,w)\nDown-sampled Value: Vdow = Fsconv(V)\nOutput: ÀÜX\nc\n= Fadd(Fconv(Qatt),Vdow),\n(6)\nwhere Fatt(¬∑) is an outer productfunction; Fsconv(¬∑) is a 3 √ó3 stride convolution,\nin particular, where stride= 1 if the scales of Q and V are equal; Fconv(¬∑) is a\n3 √ó3 convolution for reÔ¨Ånement; Fadd(¬∑) is the feature map summation function\nwith a 3 √ó3 convolution; and ÀÜX\nc\ndenotes the output feature map of RT.\n3.5 Overall Architecture\nWe build speciÔ¨Åc FPT networks for tackling object detection [11,12], inatance\nsegmentation [26,13], and semantic segmentation [14,15]. Each FPT network\nis composed of four components: a backbone for feature extraction; a feature\npyramid construction module; our proposed FPT for feature transformer; and a\ntask-speciÔ¨Åc head network. In the following, we detail the proposed architectures.\nFPT for object detection and instance segmentation. We follow [12,13]\nto deploy the ResNet as the backbone, and pre-train it on the ImageNet [39].\nBFP [12] is used as the pyramid construction module. Then the proposed FPT\nis applied to BFP, for which the number of divided parts of Nis set to 2 for ST\nand 4 for GT3. Then, the transformed feature maps (by FPT) are concatenated\nwith the original maps along the channel dimension. The concatenated maps go\nthrough a 3√ó3 convolution to reduce the feature dimension into 256. On the top\nof the output feature maps, we apply the head networks for handling speciÔ¨Åc\ntasks, e.g., the Faster R-CNN [6] head for object detection and the Mask R-\nCNN [26] head for instance segmentation. To enhance the feature generalization,\nwe apply the DropBlock [40] to each output feature map. We set the drop block\nsize as 5 and the feature keep probability as 0 .9.\nFPT for semantic segmentation. We use dilated ResNet-101 [4] as the back-\nbone (pre-trained on the ImageNet) following [14,41]. We then apply the Un-\nscathed Feature Pyramid (UFP) as the feature pyramid construction module,\nwhich basically contains a pyramidal global convolutional network [29] with the\ninternal kernel size of 1, 7, 15 and 31, and each scale with the output dimension\nof 256. Then, the proposed FPT (including LGT) is applied to UFP with the\nsame number of divided parts Nas in the instance-level tasks. In particular, the\nsquare size of LGT is set to 5. On the top of the transformed feature pyramid,\nwe apply the semantic segmentation head network, as in [14,19]. We also deploy\nthe DropBlock [40] on the output feature maps with the drop block size as 3 and\nthe feature keep probability as 0 .9.\n3 More details are given in Section B of the supplementary.\nFeature Pyramid Transformer 9\n4 Experiments\nOur experiments were conducted on three interesting and challenging tasks: i.e.,\ninstance-level object detection and segmentation, and pixel-level semantic seg-\nmentation. In each task, we evaluated our approach with careful ablation studies,\nextensive comparisons to the state-of-the-arts and representative visualizations.\n4.1 Instance-Level Recognition\nDataset. Experiments on object detection and instance segmentation were con-\nducted on MS-COCO 2017 [21] which has 80 classes and includes 115k, 5k and\n20k images for training, validation and test, respectively.\nBackbone. In the ablation study, ResNet-50 [42] was used as the backbone. To\ncompare to state-of-the-arts, we also employed ResNet-101 [42], Non-local Net-\nwork (NL-ResNet-101) [16], Global Context Network (GC-ResNet-101) [43] and\nAttention Augmented Network (AA-ResNet-101) [44] as the backbone networks.\nSetting. As in [12,13], the backbone network was pre-trained on the Ima-\ngeNet [39], then the whole network was Ô¨Åne-tuned on the training data while\nfreezing the backbone parameters. For fair comparisons, input images were re-\nsized into 800 pixels/1,000 pixels for the shorter/longer edge [31].\nTraining details. We adopted SGD training on 8 GPUs with the Synchronized\nBatch Norm (SBN) [41]. Each mini-batch involved one image per GPU and 512\nRegion of Interest (ROI) per image. The positive-to-negative ratio was set to 1 :\n3. The weight decay and momentum were set to 0.0001 and 0.9, respectively. For\nobject detection, the learning rate was 0 .05 in the Ô¨Årst 80k iterations, and 0 .005\nin the remaining 20k iterations. For instance segmentation, the learning rate was\n0.05 for the Ô¨Årst 120k iterations, and 0 .005 in the remaining 40k iterations. An\nend-to-end region proposal network was used to generate proposals, as in [16].\nComparison methods. We compared our FPT to the state-of-the-art cross-\nscale feature pyramid interactions including FPN [12], Bottom-up Path Aggrega-\ntion (BPA) in PANet [13], and Bi-direction Feature Interaction (BFI) in ZigZa-\ngNet [31]. We also reported the experimental results of using the Augmented\nHead (AH) [13] and Multi-scale Training (MT) [13], where the AH speciÔ¨Åcally\nincludes the adaptive feature pooling, fully-connected fusion, and heavier head.\nMetrics. We evaluated the model performance using the standard Average Pre-\ncision (AP), AP50, AP75, APS, APM and APL.\nAblation study. Our ablation study aims to (1) evaluate the performance of\nthree individual transformers (in our FPT) and combinations, for which the base\npyramid method BFP [12] is the baseline (in Table 1), and (2) investigate the\neÔ¨Äects of SBN [41] and DropBlock [40] on our FPT (in Table 2).\nComparing to the baseline. Table 1 show that three transformers bring con-\nsistent improvements over the baseline. For example, ST, GT and RT respec-\ntively brings 0 .4%, 3.5% and 3 .1% improvements for the bounding box AP in\nobject detection. The improvements are higher as 0 .7%, 4.0% and 3 .2% for the\nmask AP in instance segmentation. The gain by ST is not as much as the gains\n10 D. Zhang, H. Zhang, et al.\nBFPSTGTRT AP AP 50 AP75 APS APM APL ParamsGFLOPs\n‚úì \u0017 \u0017 \u0017 31.6 29.9 54.1 50.7 35.9 34.7 16.1 14.2 32.5 31.6 48.8 48.5 34.6 M 172.3\n‚úì ‚úì \u0017 \u0017 32.0 30.6 54.9 51.4 36.9 35.5 16.5 15.1 34.0 32.1 49.1 49.7 55.0 M 248.2\n‚úì \u0017 ‚úì \u0017 35.1 33.9 55.2 52.4 38.1 37.7 17.4 16.9 36.3 33.3 50.3 51.7 63.9 M 265.1\n‚úì \u0017 \u0017 ‚úì 34.7 33.1 55.5 52.0 37.5 37.7 17.0 15.3 36.6 34.9 52.0 52.1 39.8 M 187.9\n‚úì ‚úì ‚úì \u0017 35.7 34.6 55.7 54.1 38.3 37.9 18.0 17.4 36.5 34.0 52.1 50.5 82.5 M 322.9\n‚úì ‚úì \u0017 ‚úì 35.9 34.4 56.8 55.1 38.8 38.0 19.1 17.9 37.0 34.8 53.1 52.2 61.2 M 256.7\n‚úì \u0017 ‚úì ‚úì 36.9 35.1 56.6 54.5 38.2 38.5 18.8 17.7 37.7 35.3 54.3 53.2 69.6 M 281.6\n‚úì ‚úì ‚úì ‚úì38.0 36.8 57.1 55.9 38.9 38.6 20.5 18.8 38.1 35.3 55.7 54.288.2 M 346.2\nimprovements ‚Üë 6.4‚Üë 6.9‚Üë 3.0‚Üë 5.2‚Üë 3.0‚Üë 3.9‚Üë 4.4‚Üë 4.6‚Üë 5.6‚Üë 3.7‚Üë 6.9‚Üë 5.7\nTable 1.Ablation study on MS-COCO 2017 val set [21]. ‚ÄúBFP‚Äù is Bottom-up Feature\nPyramid [12]; ‚ÄúST‚Äù is Self-Transformer; ‚ÄúGT‚Äù is Grounding Transformer; ‚ÄúRT‚Äù is\nRendering Transformer. Results on the left and right of the dashed are of bounding\nbox detection and instance segmentation.\nFPT SBN DropBlock AP AP 50 AP75 APS APM APL\n‚úì \u0017 \u0017 37.2 35.9 56.0 54.3 37.7 36.9 19.0 17.2 37.7 34.8 53.1 51.3\n‚úì ‚úì \u0017 37.8 36.5 56.7 55.2 38.4 38.2 19.6 18.0 37.9 35.1 54.0 52.1\n‚úì \u0017 ‚úì 37.5 36.2 56.5 54.8 38.0 37.3 19.5 17.8 37.8 35.0 53.8 51.9\n‚úì ‚úì ‚úì 38.0 36.8 57.1 55.9 38.9 38.6 20.5 18.8 38.1 35.3 55.7 54.2\nTable 2. Ablation study of SBN [41] and DropBlock [40] on the MS-COCO 2017 val\nset [21]. Results on the left and right of dashed lines are respectively for bounding box\ndetection and instance segmentation.\nby the other two transformers. An intuitive reason is that, compared to self-\ninteraction (i.e., ST), the cross-scale interactions (i.e., GT and RT) capture more\ndiverse and richer inter-object contexts to achieve better object recognition and\ndetection performances, which is consistent with the conclusion of instance-level\nrecognition works [45,46]. The middle blocks in Table 1 show that the combina-\ntion of transformers improves the performance over individuals in most of cases.\nIn particular, the full combination of ST, GT and RT results the best perfor-\nmance, i.e., 38.0% bounding box AP (6.4% higher than BFP) on object detection\nand 36.8% mask AP (6 .9% higher than BFP) on instance segmentation.\nEÔ¨Äects of SBN and DropBlock. Table 2 shows that both SBN and Drop-\nBlock improve the model performance of FPT. Their combination yields 0 .8%\nof improvement for the bounding box AP in object detection, and 0 .9% for the\nmask AP in instance segmentation.\nModel eÔ¨Éciency4. We reported the model Parameters (Params) and GFLOPs\nwith the Mask R-CNN [26]. Adding +ST, +GT and +RT to the baseline respec-\ntively increase Params by 0.59√ó, 0.85√óand 0.15√ó(with mask AP improvements\nof 0.7%, 4.0%, 3.2%). Correspondingly, GFLOPs are increased by 0 .44√ó, 0.54√ó\nand 0.09√ó. Compared to related works [31,12,16], these are relatively fair over-\n4 More details are given in the Section C of the supplementary.\nFeature Pyramid Transformer 11\nInput FPN BPA BFI FPT (Ours) Ground Truth\n86.7% 90.0% 88.1%\n75.5%\n92.3%\n76.3% 78.7% 86.7%\n88.3% 89.6% 88.9% 92.5%\nFig. 4.Visualization results in instance segmentation. The red rectangle highlights the\nbetter predicted area of FPT. Samples are from MS-COCO 2017 validation set [21].\nThe value on each image represents the corresponding segmentation mIoU.\nheads on average. For example, the classical Non-local [16] has 0.24√óParams for\ninstance segmentation with only 0 .9% mask AP improvements on ResNet-50.\nComparing to the state-of-the-arts. The top three blocks in Table 3 show\nthat applying the cross-scale interaction methods, e.g., FPN [12], BPA [13],\nBFI [31] and FPT, results consistent improvements over the baseline BFP [12].\nIn particular, our FPT achieves the highest gains, i.e., 8.5% AP in object de-\ntection and 6.0% mask AP in instance segmentation, with the ResNet-101 back-\nbone [42]. Besides, the consistent improvements are also achieved on the stronger\nNL-, GC- and AA- ResNet-101, and validate that BFP+FPT can generalize\nwell to stronger backbones, which make more senses in the age of results5. At\nlast but not the least, two bottom blocks in Table 3 show that adding eÔ¨Écient\ntraining strategies such as AH, MT, and both (denoted as ‚Äú[all]‚Äù in Table 3)\nto BFP+FPT yields performance boosts. For example, BFP+FPT [all] (with\nResNet-101) achieves a higher bounding box AP in object detection and the\nsame mask AP in instance segmentation, compared to the best performance of\nBFP+FPT (with stronger GC-ResNet-101). Besides, BFP+FPT [all] achieves\nthe average 1 .5% AP in object detection and 2 .1% mask AP in instance seg-\nmentation (over BFP+BFI) using ResNet-101, which further veriÔ¨Åes the robust\nplug-and-play ability of our FPT. The visualization results in instance segmen-\ntation are given in Fig. 4. Compared to other feature interaction methods, the\nresults of FPT show more precise predictions for both small ( e.g., bottle) and\nlarge objects ( e.g., bicycle). Moreover, it shows the gracile parts in the object\n(e.g., the horse legs) are also well predicted using our FPT.\n4.2 Experiments on Pixel-Level Recognition\nDataset. Our pixel-level segmentation experiments were conducted on four\nbenchmarks: (1) Cityscapes [22] contains 19 classes, and includes 2,975, 500 and\n1,525 images for training, validation and test, respectively; (2) ADE20K [24]\n5 More results are given in Section D of the supplementary.\n12 D. Zhang, H. Zhang, et al.\nMethods Backbone AP AP 50 AP75 APS APM APL\nBFP [12]\nResNet-101 33.1 32.6 53.8 51.7 34.6 33.3 12.6 11.4 35.3 34.4 49.5 48.9\nNL-ResNet-101 34.4 33.7 54.3 53.6 35.8 33.9 15.1 13.7 37.1 36.0 50.7 49.7\nGC-ResNet-101 35.0 34.2 55.8 54.1 36.5 35.3 14.8 13.9 38.6 37.3 50.9 50.5\nAA-ResNet-101 33.8 32.8 54.2 52.3 35.4 33.8 13.0 12.3 35.5 34.5 50.0 49.0\nBFP+FPN [12] ResNet-101 36.2 35.7 59.1 58.0 39.0 37.8 18.2 15.5 39.0 38.1 52.4 49.2\nBFP+BPA [13] ResNet-101 37.3 36.3 60.4 59.0 39.9 38.3 18.9 16.3 39.7 39.0 53.0 50.5\nBFP+BFI [31] ResNet-101 39.5 - - - - - - - - - - -\nBFP+FPT\nResNet-101 41.6 38.6 60.9 58.2 44.0 43.3 23.4 19.0 41.5 39.2 53.1 50.8\nNL-ResNet-101 42.0 39.5 62.1 60.7 46.5 45.4 25.1 20.8 42.6 41.0 53.7 53.0\nGC-ResNet-101 42.5 40.3 62.0 61.0 46.1 45.8 25.3 21.1 42.7 41.8 53.1 52.7\nAA-ResNet-101 42.1 40.1 61.5 60.1 46.5 45.2 25.2 20.6 42.6 41.2 53.5 52.0\nBFP+FPT [AH] ResNet-101 41.1 40.0 62.0 59.9 46.6 45.5 24.2 20.5 42.1 41.0 53.3 52.5\nBFP+FPT [MT] ResNet-101 41.2 39.8 62.1 60.1 46.0 45.1 24.1 20.9 41.9 40.8 53.2 51.9\nBFP+FPN [12] [all] ResNet-101 37.9 36.3 59.6 58.8 40.1 39.1 19.5 16.7 41.0 40.3 53.5 51.1\nBFP+BPA [13] [all] ResNet-101 39.0 37.7 60.8 59.4 41.7 40.1 20.2 18.5 41.5 40.1 54.1 52.4\nBFP+BFI [31] [all] ResNet-101 40.1 38.2 61.2 60.0 42.6 42.4 21.9 19.6 42.4 40.8 54.3 52.5\nBFP+FPT [all] ResNet-101 42.6 40.3 62.4 61.1 46.9 45.924.9 21.3 43.041.2 54.5 53.3\nTable 3.Experimental results on MS-COCO 2017 test-dev [21]. ‚ÄúAH‚Äù is Augmented\nHead, and ‚ÄúMT‚Äù is Multi-scale Training [13]; ‚Äúall‚Äù means that both the AH and MT\nare used. Results on the left and right of the dashed are of bounding box detection and\ninstance segmentation. ‚Äú-‚Äù means that there is no reported result in its paper.\nhas 150 classes, and uses 20 k, 2 k, and 3 k images for training, validation and\ntest, respectively; (3) LIP [25] contains 50 ,462 images with 20 classes, and in-\ncludes 30,462, 10kand 10kimages for training, validation and test, respectively;\n(4) PASCAL VOC 2012[23] contains 21 classes, and includes 1 ,464, 1,449 and\n1,456 images for training, validation and test, respectively.\nBackbone. We used dilated ResNet-101 [4] as the backbone as in [19].\nSetting. We Ô¨Årst pre-trained the backbone network on the ImageNet [39], then\nÔ¨Åne-tuned the whole network on the training data while Ô¨Åxing the parameters\nof backbone as in [41]. Before input, we cropped the image into 969 √ó969 for\nCityscapes, 573 √ó573 for LIP, and 521 √ó521 for PASCAL VOC 2012. Because\nimages in ADE20K are of various sizes, we cropped the shorter-edge images to\nan uniform size {269,369,469,569}as that in [32].\nTraining details. We followed [32] to use the learning rate scheduling lr =\nbaselr√ó(1‚àí iter\ntotaliter\n)power. On Cityscapes, LIP and PASCAL VOC 2012, the base\nlearning rate was 0 .01, and the power is 0 .9. The weight decay and momentum\nwere set to 0 .0005 and 0 .9, respectively. On ADE20K, the base learning rate\nwas 0.02 and the power was 0 .9. The weight decay and momentum were 0 .0001\nand 0.9, respectively. We trained models on 8 GPUs with SBN [41]. The model\nwas trained for 120 epochs on Cityscapes and ADE20K, 50 on LIP, and 80 on\nPASCAL VOC 2012. For data augmentation, the training images were Ô¨Çipped\nleft-right and randomly scaled between a half and twice as in [19].\nComparison methods. Our FPT was applied to the feature pyramids con-\nstructed by three methods: UFP [29], PPM [1,15] and ASPP [14]. Based on\nFeature Pyramid Transformer 13\nMethods Tra.mIoU Val.mIoU Params GFLOPs\nUFP [29] 86.0 79.1 71.3 M 916.1\nUFP+ST [29] 86.9 80.7 91.2 M 948.4\nUFP+LGT [29] 86.5 80.3 102.8 M 1008.3\nUFP+RT [29] 86.3 80.1 77.4 M 929.3\nUFP+LGT+ST [29] 87.2 80.9 121.3 M 1052.6\nUFP+RT+ST [29] 87.0 80.8 96.2 M 985.2\nUFP+LGT+RT [29] 86.6 80.4 107.0 M 1014.8\nUFP+LGT+ST+RT [29] 87.4 81.7 127.2 M 1063.9\nthe improvement ‚Üë 1.4 ‚Üë 2.6\nTable 4. Ablation study on the Cityscapes validation set [22]. ‚ÄúLGT‚Äù is Locality-\nconstrained Grounding Transformer; ‚ÄúRT‚Äù is Rendering Transformer; ‚ÄúST‚Äù is Self-\nTransformer. ‚Äú+‚Äù means building the method on the top of UFP.\nMethods Backbone Cityscapes ADE20K LIP PASCAL VOC 2012\nbaseline ResNet-101 65.3 40.9 42.7 62.2\nCFNet [19] ResNet-101 80.6 44.9 54.6 84.2\nAFNB [33] ResNet-101 81.3 45.2 - -\nHRNet [47] HRNetV2-W48 81.6 44.7 55.9 84.5\nOCNet [32] ResNet-101 81.7 45.5 54.7 84.3\nGSCNN [48] Wide-ResNet-101 82.8 - 55.2 -\nPPM [15]+OC [32] ResNet-101 79.9 43.7 53.0 82.9\nASPP [14]+OC [32] ResNet-101 80.0 44.1 53.3 82.7\nUFP [29]+OC [32] ResNet-101 80.6 44.7 54.5 83.2\nPPM [15]+FPT ResNet-101 80.4( ‚Üë 0.5) 44.8(‚Üë 1.1) 54.2(‚Üë 1.2) 83.2( ‚Üë 0.3)\nASPP [14]+FPT ResNet-101 80.7( ‚Üë 0.7) 45.2(‚Üë 1.1) 54.4(‚Üë 1.1) 83.1( ‚Üë 0.4)\nUFP [29]+FPT ResNet-101 82.2(‚Üë 1.6)45.9(‚Üë 1.2)56.2(‚Üë 1.7) 85.0(‚Üë 1.8)\nTable 5.Comparisons with state-of-the-art on test sets of Cityscapes [22] and PASCAL\nVOC 2012 [23], validation sets of ADE20K [24] and LIP [25]. Results in this table refer\nto mIoU; ‚Äú-‚Äù means that there is no reported result in its paper. The best and second\nbest models under each setting are marked with corresponding formats.\neach of these methods, we compared our FPT to the state-of-the-art pixel-level\nfeature pyramid interaction method, i.e., Object Context Network (OCNet) [32].\nMetrics. We used the standard mean Intersection of Union (mIoU) as a uniform\nmetric. We showed the results of ablation study by reporting the mIoU of training\nset (i.e., Tra.mIoU) and validation set (i.e., Val.mIoU) on the Cityscapes.\nAblation study. Results are given in Table 4. UFP is the baseline. Applying\nour transformers ( i.e., +ST, +LGT and +RT) to UFP respectively achieves\nthe improvements of 0 .9%, 0.5% and 0 .3% Tr.mIoU, and the more impressive\n1.6%, 1.2% and 1.0% Val.mIoU. Moreover, any component combinations of our\ntransformers yields concretely better results than using individual ones. Our\nbest model achieves 1 .4% and 2.6% improvements (over UFP) for Tr.mIoU and\nVal.mIoU, respectively.\nModel eÔ¨Éciency. In Table 4, we reported the model Params and GFLOPs.\nIt is clear that using our transformers increases a fair computational overhead.\n14 D. Zhang, H. Zhang, et al.\nInput OCNetBaseline Ground TruthFPT (Ours)\n73.1% 86.5% 90.4%\n77.1% 86.7% 92.6%\n73.3% 87.0% 93.5%\nFig. 5.Visualization results in segmentation. The red rectangle highlights the better\npredicted area of FPT. Samples are from the validation set of PASCAL VOC 2012\n[23]. The value on each image represents the corresponding segmentation mIoU.\nFor example, +ST, +LGT and +RT respectively add Params 0.28√ó, 0.44√óand\n0.09√ó, and increase GFLOPs by 0 .04√ó, 0.10√óand 0.01√ó, compared to UFP.\nComparing to the state-of-the-arts. From Table 5, we can observe that\nour FPT can achieve a new state-of-the-art performance over all the previ-\nous methods based on the same backbone ( i.e., ResNet-101). It obtains im-\npressive improvements as 1.6%, 1.2%, 1.7% and 1 .8% mIoU on Cityscapes [22],\nADE20K [24], LIP [25] and PASCAL VOC 2012 [23], respectively. Besides,\ncompared to OCNet, FPT obtains gain by 0.9%, 1.1%, 1.3% and 0.8% mIoU\nin these four datasets on average. In Fig. 5, we provide the qualitative results\nof our method 6. Compared to the baseline [29] and OCNet [32], the results of\nFPT show more precise segmentation on boundaries, particularly for smaller\nand thinner objects, e.g., the guardrail, person‚Äôs leg and bird. Moreover, FPT\ncan also achieve more integrated segmentation on some larger objects, e.g., the\nhorse, person and sofa.\n5 Conclusion\nWe proposed an eÔ¨Écient feature interaction approach called FPT, composed\nof three carefully-designed transformers to respectively encode the explicit self-\nlevel, top-down and bottom-up information in the feature pyramid. Our FPT\ndoes not change the size of the feature pyramid, and is thus generic and easy\nto plug-and-play with modern deep networks. Our extensive quantitative and\nqualitative results on three challenging visual recognition tasks showed that FPT\nachieves consistent improvements over the baselines and the state-of-the-arts,\nvalidating its high eÔ¨Äectiveness and strong application capability.\n6 More visualization results are given in the Section E of the supplementary.\nFeature Pyramid Transformer 15\nAcknowledgements\nWe would like to thank all the anonymous reviewers for their constructive com-\nments. This work was partially supported by the National Key Research and\nDevelopment Program of China under Grant 2018AAA0102002, the National\nNatural Science Foundation of China under Grant 61925204, the China Scholar-\nships Council under Grant 201806840058, the Singapore Ministry of Education\n(MOE) Academic Research Fund (AcRF) Tier 1 grant, and the NTU-Alibaba\nJRI.\nReferences\n1. He, K., Zhang, X., Ren, S., Sun, J.: Spatial pyramid pooling in deep convolutional\nnetworks for visual recognition. TPAMI 37(9) (2015) 1904‚Äì1916\n2. Lazebnik, S., Schmid, C., Ponce, J.: Beyond bags of features: Spatial pyramid\nmatching for recognizing natural scene categories. In: CVPR. (2006)\n3. Springenberg, J.T., Dosovitskiy, A., Brox, T., Riedmiller, M.: Striving for simplic-\nity: The all convolutional net. In: ICLR. (2015)\n4. Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. In:\nICLR. (2016)\n5. Girshick, R.: Fast r-cnn. In: ICCV. (2015)\n6. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object\ndetection with region proposal networks. In: NeurIPS. (2015)\n7. Adelson, E.H., Anderson, C.H., Bergen, J.R., Burt, P.J., Ogden, J.M.: Pyramid\nmethods in image processing. RCA Engineer 29(6) (1984) 33‚Äì41\n8. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks.\nIn: ECCV. (2014)\n9. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C.:\nSsd: Single shot multibox detector. In: ECCV. (2016)\n10. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: UniÔ¨Åed,\nreal-time object detection. In: CVPR. (2016)\n11. Hu, H., Gu, J., Zhang, Z., Dai, J., Wei, Y.: Relation networks for object detection.\nIn: CVPR. (2018)\n12. Lin, T.Y., Doll¬¥ ar, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature\npyramid networks for object detection. In: CVPR. (2017)\n13. Liu, S., Qi, L., Qin, H., Shi, J., Jia, J.: Path aggregation network for instance\nsegmentation. In: CVPR. (2018)\n14. Chen, L.C., Papandreou, G., SchroÔ¨Ä, F., Adam, H.: Rethinking atrous convolution\nfor semantic image segmentation. In: arXiv. (2017)\n15. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In:\nCVPR. (2017)\n16. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: CVPR.\n(2018)\n17. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. In: NeurIPS. (2017)\n18. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-\nto-end object detection with transformers. In: ECCV. (2020)\n19. Zhang, H., Zhang, H., Wang, C., Xie, J.: Co-occurrent features in semantic seg-\nmentation. In: CVPR. (2019)\n16 D. Zhang, H. Zhang, et al.\n20. Jouppi, N.P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., Bates,\nS., Bhatia, S., Boden, N., Borchers, A., et al.: In-datacenter performance analysis\nof a tensor processing unit. In: ISCA. (2017)\n21. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll¬¥ ar, P.,\nZitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV. (2014)\n22. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,\nFranke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene\nunderstanding. In: CVPR. (2016)\n23. Everingham, M., Eslami, S.A., Van Gool, L., Williams, C.K., Winn, J., Zisserman,\nA.: The pascal visual object classes challenge: A retrospective. IJCV 111(1) (2015)\n98‚Äì136\n24. Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing\nthrough ade20k dataset. In: CVPR. (2017)\n25. Gong, K., Liang, X., Zhang, D., Shen, X., Lin, L.: Look into person: Self-supervised\nstructure-sensitive learning and a new benchmark for human parsing. In: CVPR.\n(2017)\n26. He, K., Gkioxari, G., Doll¬¥ ar, P., Girshick, R.: Mask r-cnn. In: ICCV. (2017)\n27. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic\nsegmentation. In: CVPR. (2015)\n28. Zhang, Z., Zhang, X., Peng, C., Xue, X., Sun, J.: Exfuse: Enhancing feature fusion\nfor semantic segmentation. In: ECCV. (2018)\n29. Peng, C., Zhang, X., Yu, G., Luo, G., Sun, J.: Large kernel matters‚Äìimprove\nsemantic segmentation by global convolutional network. In: CVPR. (2017)\n30. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab:\nSemantic image segmentation with deep convolutional nets, atrous convolution,\nand fully connected crfs. TPAMI 40(4) (2017) 834‚Äì848\n31. Lin, D., Shen, D., Shen, S., Ji, Y., Lischinski, D., Cohen-Or, D., Huang, H.: Zigza-\ngnet: Fusing top-down and bottom-up context for object segmentation. In: CVPR.\n(2019)\n32. Yuan, Y., Wang, J.: Ocnet: Object context network for scene parsing. In: arXiv.\n(2018)\n33. Zhu, Z., Xu, M., Bai, S., Huang, T., Bai, X.: Asymmetric non-local neural networks\nfor semantic segmentation. In: ICCV. (2019)\n34. Yang, Z., Dai, Z., Salakhutdinov, R., Cohen, W.W.: Breaking the softmax bottle-\nneck: A high-rank rnn language model. In: ICLR. (2018)\n35. Zhang, Y., Hare, J., Pr¬® ugel-Bennett, A.: Learning to count objects in natural\nimages for visual question answering. In: ICLR. (2018)\n36. Chen, L., Zhang, H., Xiao, J., Nie, L., Shao, J., Liu, W., Chua, T.S.: Sca-cnn:\nSpatial and channel-wise attention in convolutional networks for image captioning.\nIn: CVPR. (2017)\n37. Lin, M., Chen, Q., Yan, S.: Network in network. In: ICLR. (2014)\n38. Yu, C., Wang, J., Peng, C., Gao, C., Yu, G., Sang, N.: Learning a discriminative\nfeature network for semantic segmentation. In: CVPR. (2018)\n39. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale\nhierarchical image database. In: CVPR. (2009)\n40. Ghiasi, G., Lin, T.Y., Le, Q.V.: Dropblock: A regularization method for convolu-\ntional networks. In: NeurIPS. (2018)\n41. Zhang, H., Dana, K., Shi, J., Zhang, Z., Wang, X., Tyagi, A., Agrawal, A.: Context\nencoding for semantic segmentation. In: CVPR. (2018)\n42. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: CVPR. (2016)\nFeature Pyramid Transformer 17\n43. Cao, Y., Xu, J., Lin, S., Wei, F., Hu, H.: Gcnet: Non-local networks meet squeeze-\nexcitation networks and beyond. In: ICCV. (2019)\n44. Irwan, B., Barret, Z., Ashish, V., Jonathon, S., Quoc, V.L.: Attention augmented\nconvolutional networks. In: ICCV. (2019)\n45. Zhou, Y., Zhu, Y., Ye, Q., Qiu, Q., Jiao, J.: Weakly supervised instance segmen-\ntation using class peak response. In: CVPR. (2018)\n46. Zhu, L., Wang, T., Aksu, E., Kamarainen, J.K.: Portrait instance segmentation\nfor mobile devices. In: ICME. (2019)\n47. Sun, K., Zhao, Y., Jiang, B., Cheng, T., Xiao, B., Liu, D., Mu, Y., Wang, X., Liu,\nW., Wang, J.: High-resolution representations for labeling pixels and regions. In:\narXiv. (2019)\n48. Takikawa, T., Acuna, D., Jampani, V., Fidler, S.: Gated-scnn: Gated shape cnns\nfor semantic segmentation. In: ICCV. (2019)\n18 D. Zhang, H. Zhang, et al.\nSupplementary Materials\nThese materials include the details of the eÔ¨Äectiveness of Feud (Section A), an\nadditional study on hyperparameters (Section B), FPT complexity analysis (Sec-\ntion C), more quantitative result comparisons (Section D), and more qualitative\nresults (Section E).\nA EÔ¨Äectiveness of Feud\nIn Section 3.3 of the main paper, we propose to use the negative value of eu-\nclidean distance Feud [35] (instead of the conventionalFsim [17]) as the similarity\nfunction in Grounding Transformer (GT). In this section, we show the eÔ¨Äective-\nness of Feud. In details, the Mixture of Softmaxes (MoS) [34] is deployed as the\nnormalizing function, and the number of the divided parts N is set to 4. Ta-\nble S1 shows that Feud surpasses the classic softmax-based Fsim in both cases\nof GT (with and without MoS). In particular, Feud with MoS achieves the best\nperformance. There are at most 3.1% box AP and 3.3% mask AP improvements\nfor object detection and instance segmentation, respectively.\nMethods AP AP 50 AP75 APS APM APL\nBFP [12] 31.6 29.9 54.1 50.7 35.9 34.7 16.1 14.2 32.5 31.6 48.8 48.5\n+ Fsim [17] 32.2 30.5 54.2 50.9 35.6 34.5 16.3 14.5 32.1 31.2 49.0 48.6\n+ Feud [35] 33.7 32.1 54.5 52.1 36.0 34.9 16.9 15.6 33.0 31.8 49.4 48.7\n+ Fsim [17] + MoS [34] 32.6 31.1 54.3 51.2 35.8 34.8 16.4 15.3 32.6 31.5 49.1 48.5\n+ Feud [35] + MoS [34]34.7 33.2 55.4 53.2 37.0 36.1 17.8 16.2 33.9 32.0 50.3 49.1\nTable S1.Comparing Feud with Fsim on validation set of MS-COCO 2017 [21]. The\nbackbone is ResNet-50 [42]. ‚ÄúBFP‚Äù is the bottom-up feature pyramid (BFP) [12].\nResults on the left and right of the dashed are respectively from bounding box detection\nand instance segmentation.\nB Hyperparameters\nB.1 N in ST\nIn Section 3.2, we use MoS [34] as the normalizing function. In this section, we\ninvestigate the inÔ¨Çuence of N(in MoS) on Self-Transformer (ST). In particular,\nno MoS [34] means N=1, i.e., the classical softmax [16]. In Table S2, we can see\nthat N=2 brings the best performance in all cases.\nB.2 N in GT\nIn this section, we investigate the inÔ¨Çuence of N(in MoS) on GT. As shown in\nTable S3, we can see that using N=4 achieves the best performance for both\nobject detection and instance segmentation.\nFeature Pyramid Transformer 19\nN AP AP 50 AP75 APS APM APL\nBFP [12] 31.6 29.9 54.1 50.7 35.9 34.7 16.1 14.2 32.5 31.6 48.8 48.5\n1 (w/o MoS [34]) 31.7 30.0 54.3 50.5 36.0 34.9 16.3 14.3 32.6 31.9 48.5 48.4\n2 31.8 30.2 54.6 51.1 36.5 35.1 16.8 14.6 33.2 32.0 49.8 49.3\n4 31.1 29.3 54.0 50.5 35.9 34.6 16.4 14.1 32.8 31.4 49.1 48.3\n6 30.6 28.7 53.6 49.7 35.7 34.0 16.1 13.7 32.2 30.9 48.8 48.0\n8 30.0 28.1 53.1 49.5 35.3 33.7 15.9 13.3 31.8 30.5 48.2 47.5\nTable S2.The inÔ¨Çuence of N on ST. Experiments are carried out on validation set\nof MS-COCO 2017 [21]. The backbone is ResNet-50 [42]. ‚ÄúBFP‚Äù is the bottom-up\nfeature pyramid (BFP) [12]. ‚Äúw/o MoS‚Äù means that these results are obtained without\nMoS [34]. Results on the left and right of the dashed are of bounding box detection\nand instance segmentation.\nN AP AP 50 AP75 APS APM APL\nBFP [12] 31.6 29.9 54.1 50.7 35.9 34.7 16.1 14.2 32.5 31.6 48.8 48.5\n1 (w/o MoS [34]) 33.7 32.1 54.5 52.1 36.0 34.9 16.9 15.6 33.0 31.8 49.4 48.6\n2 34.3 32.7 55.1 52.8 36.5 35.4 17.3 15.9 33.5 31.6 49.9 48.8\n4 34.7 33.2 55.4 53.2 37.0 36.1 17.8 16.2 33.9 32.0 50.3 49.1\n6 33.4 32.9 54.3 52.7 36.4 35.6 17.3 15.7 33.5 31.5 50.0 48.7\n8 32.5 32.3 53.3 52.0 35.9 35.0 17.0 15.2 32.9 30.7 49.5 48.4\nTable S3.The inÔ¨Çuence of N on GT. Experiments are carried out on validation set\nof MS-COCO 2017 [21]. The backbone is ResNet-50 [42]. ‚ÄúBFP‚Äù is the bottom-up\nfeature pyramid (BFP) [12]. ‚Äúw/o MoS‚Äù means that these results are obtained without\nMoS [34]. Results on the left and right of the dashed are of bounding box detection\nand instance segmentation.\nB.3 square size in LGT\nIn Section 3.3, we introduce LGT for semantic segmentation. In this section, we\ninvestigate the inÔ¨Çuence of the side length square size (of local square area) on\nLGT. We use MoS [34] with N=4 as the normalizing function. We report the\nstandard mean Intersection of Union (mIoU) on the training set (i.e., Tra.mIoU)\nas well as the validation set ( i.e., Val.mIoU) of Cityscapes [22], in Table S4. We\ncan see that LGT with square size=5 achieves the best performance.\nB.4 DropBlock in Instance-level Tasks\nIn Section 3.5, we apply the DropBlock [40] to each transformed feature map, to\nalleviate the over-Ô¨Åtting problem. In this section, we investigate the inÔ¨Çuence of\ntwo hyper-parameters (i.e., the drop block size block sizeand the keep probabil-\nity keep prob of each feature position) of DropBlock [40] on instance-level tasks\n(i.e., object detection and instance segmentation). The MoS [34] is applied in GT\nwith its N=4, and in ST with its N=2. In Table S5, we Ô¨Ånd that block size=5\nand keep prob=0.9 result the best performance.\n20 D. Zhang, H. Zhang, et al.\nsquare size Method Tra.mIoU (%) Val.mIoU (%)\n- backbone + UFP 86.0 79.1\n1 backbone + UFP + LGT 86.1 79.5\n3 backbone + UFP + LGT 86.2 79.9\n5 backbone + UFP + LGT 86.3 80.0\n7 backbone + UFP + LGT 86.1 79.8\n9 backbone + UFP + LGT 85.8 79.6\nTable S4.The inÔ¨Çuence of square size of LGT on the pixel-level semantic segmentation\ntask. The backbone is the dilated ResNet-101 [4]. Experiments are carried out on\ntraining set and the validation set of Cityscapes [22]. ‚ÄúUFP‚Äù is the unscathed feature\npyramid.\nSettings block size=1 block size=3 block size=5 block size=7\nkeep prob=0.1 30.7 29.7 31.4 30.7 31.9 30.1 30.0 29.8\nkeep prob=0.3 32.1 30.9 32.9 31.6 33.2 31.1 31.8 30.8\nkeep prob=0.5 33.2 31.0 34.2 33.5 35.5 34.7 33.7 33.4\nkeep prob=0.7 33.8 32.4 35.6 34.9 36.1 35.7 35.8 34.6\nkeep prob=0.9 34.2 33.8 36.6 35.1 38.0 36.8 36.6 35.3\nTable S5.The inÔ¨Çuence of block sizeand keep probof DropBlock [40] on the instance-\nlevel tasks (i.e., object detection and instance segmentation). The backbone is ResNet-\n50 [42]. Results on the left and right of the dashed are AP of bounding box detection\nand mask AP of instance segmentation.\nB.5 DropBlock in Pixel-level Task\nIn this section, we investigate the inÔ¨Çuence of two hyper-parameters ( i.e., the\ndrop block sizeblock sizeand the keep probabilitykeep probof each feature posi-\ntion) of DropBlock [40] on the pixel-level semantic segmentation. The MoS [34] is\napplied in GT with its N=4, and in ST with its N=2. The square size of LGT is\nset to 5. We report mIoU on the validation set (i.e., Val.mIoU) of Cityscapes [22]\nin Table S6. We Ô¨Ånd that using block size=3 and keep prob=0.9 achieves the\nbest performance.\nC FPT Complexity\nIn Section 4.1.1, we report the model eÔ¨Éciency. In this section, we supplement\nthe details of model Parameters (Params) and FLOPs using Mask R-CNN [26]\nhead. In Table S7, we compare our FPT and its components ( i.e., ST, GR,\nand RT) to the non-local operation on the validation set of MS-COCO 2017 for\ninstance segmentation [21]. The implementation detail of the non-local operation\nis the same as that in [16].\nFrom Table S7, we can Ô¨Ånd that for the non-local operation the average\nincreases of the model Params and FLOPs required by AP at per improved\nFeature Pyramid Transformer 21\nSettings block size=1 block size=3 block size=5 block size=7\nkeep prob=0.1 80.8 81.0 80.8 80.4\nkeep prob=0.3 81.0 81.1 81.0 80.7\nkeep prob=0.5 81.2 81.4 81.2 80.9\nkeep prob=0.7 81.4 81.6 81.3 81.1\nkeep prob=0.9 81.3 81.7 81.5 81.4\nTable S6.The inÔ¨Çuence of block size and keep prob of DropBlock [40] on the pixel-\nlevel semantic segmentation. The backbone is the dilated ResNet-101 [4]. Experiments\nare carried out on validation set of Cityscapes [22]. Results in this table refer to the\nmIoU on the validation set ( i.e., Val.mIoU).\nMethods AP AP 50 AP75 APS APM APL Params FLOPs\nBFP [12] 29.9 50.7 34.7 14.2 31.6 48.5 1 √ó 1√ó\n+ non-local [16] 30.8 (‚Üë0.9) 52.4 (‚Üë1.7) 35.5 (‚Üë0.8) 15.2 (‚Üë1.0) 32.5 (‚Üë0.9) 49.5 (‚Üë1.0) 1.24√ó 1.24√ó\n+ ST 30.6 ( ‚Üë0.7) 51.4 (‚Üë0.7) 35.5 (‚Üë0.8) 15.1 (‚Üë0.9) 32.1 (‚Üë0.5) 49.7 (‚Üë1.2) 1.59√ó 1.44√ó\n+ GT 33.9 ( ‚Üë4.0) 52.4 (‚Üë1.7) 37.7 (‚Üë3.0) 16.9 (‚Üë2.7) 33.3 (‚Üë1.7) 51.7 (‚Üë3.2) 1.85√ó 1.54√ó\n+ RT 33.1 ( ‚Üë3.2) 52.0 (‚Üë1.3) 37.7 (‚Üë3.0) 15.3 (‚Üë1.1) 34.9 (‚Üë3.3) 52.1 (‚Üë3.6) 1.15√ó 1.09√ó\n+ FPT 36.8 ( ‚Üë6.9) 55.9 (‚Üë5.2) 38.6 (‚Üë3.9) 18.8 (‚Üë4.6) 35.3 (‚Üë3.7) 54.2 (‚Üë5.7) 2.54√ó 2.01√ó\nTable S7.Model complexity analysis on validation set of MS-COCO 2017 [21] for in-\nstance segmentation. The backbone is ResNet-50 [42]. ‚ÄúBFP‚Äù is the bottom-up feature\npyramid (BFP) [12].\npoint are 0 .27√óand 0 .27√ó, respectively. In contrast, the average increases in\nour FPT are lower (better) as 0 .21√óand 0.15√ó, respectively.\nD More Quantitative Result Comparisons\nD.1 Results on Stronger Backbones\nIn addition to ResNet [42], we also employ the Non-local ResNet [16], the Global\nContext Network (GC-ResNet) [43], and the Attention Augmented Network\n(AA-ResNet) [44] as backbone networks in the instance-level recognition. In\nthis section, we report more quantitative results on these backbones in Table S8.\nIn Table S8, we can observe that BFP+FPT still achieves the better perfor-\nmance than BFP+FPN, BFP+BPA and BFP+BFI on the stronger backbone\nnetworks (i.e., NL-, GC-, and AA-ResNet). In particular, BFP+FPT achieves up\nto 40.8% bounding box AP (and 38.7% mask AP), while BFP+FPN, BFP+BPA\nand BFP+BFI can achieve 37.9% bounding box AP (and 36.8% mask AP),\n38.8% bounding box AP (and 37.6% mask AP), and 39.0% bounding box AP\n(and 37.9% mask AP), respectively.\nD.2 Results on Deeper Backbones\nIn this section, we report more result comparisons on the deeper backbone\nnetwork (i.e., ResNet-152) in Table S9. We can observe that BFP + FPT on\n22 D. Zhang, H. Zhang, et al.\nMethods Backbone AP AP 50 AP75 APS APM APL\nBFP+FPN [12] NL-ResNet 37.2 36.4 60.1 59.2 40.0 38.5 19.0 16.7 37.8 37.1 51.1 49.9\nBFP+BPA [13] NL-ResNet 38.5 37.6 60.9 59.5 41.6 39.2 20.5 18.1 39.5 38.7 51.9 51.0\nBFP+BFI [31] NL-ResNet 38.9 37.8 61.2 59.7 41.5 39.5 20.2 18.6 39.7 38.9 51.5 50.5\nBFP+FPT NL-ResNet 40.1 38.0 62.9 60.7 42.0 40.6 21.4 19.1 40.8 39.9 53.0 51.8\nBFP+FPN [12] GC-ResNet 37.7 36.8 60.4 59.5 40.1 38.8 19.2 17.2 38.5 37.5 51.3 50.5\nBFP+BPA [13] GC-ResNet 38.8 37.4 61.2 59.8 41.9 40.3 20.8 18.5 39.7 38.9 52.2 51.5\nBFP+BFI [31] GC-ResNet 39.0 37.7 62.0 60.2 42.3 40.7 21.1 18.9 40.2 39.1 52.0 51.8\nBFP+FPT GC-ResNet 40.4 38.5 63.3 61.0 43.5 41.9 22.6 19.7 41.1 40.5 53.4 52.3\nBFP+FPN [12] AA-ResNet 37.9 36.7 60.7 59.6 40.3 38.4 19.6 17.5 38.6 37.3 51.8 50.1\nBFP+BPA [13] AA-ResNet 38.5 37.5 61.7 59.3 41.8 40.1 20.4 18.2 39.8 38.5 52.7 51.7\nBFP+BFI [31] AA-ResNet 38.9 37.9 62.1 60.1 42.3 40.7 21.0 18.5 39.5 38.9 52.2 51.3\nBFP+FPT AA-ResNet 40.8 38.7 63.8 61.3 43.7 41.5 22.7 19.4 41.5 40.8 53.3 52.0\nTable S8. Combining FPN/BPA/BFI and NL-ResNet/GC-ResNet/AA-ResNet on\nvalidation set of MS-COCO 2017 [21]. The base is ResNet-50. Results on the left\nand right of the dashed are respectively from bounding box detection and instance\nsegmentation.\nMethods Backbone AP AP 50 AP75 APS APM APL Params\nBFP+ FPT ResNet-50 38.0 36.8 57.1 55.9 38.9 38.6 20.5 18.8 38.1 35.3 55.7 54.2 88.2 M\nBFP+ FPN [12] ResNet-101 36.2 35.7 59.1 58.0 39.0 37.8 18.2 15.5 39.0 38.1 52.4 49.2 88.0 M\nBFP+ BPA [13] ResNet-101 37.3 36.3 60.4 58.7 39.9 38.3 18.9 16.3 39.7 39.0 53.0 50.5 88.4 M\nBFP [12] ResNet-152 35.8 34.6 55.7 53.8 37.8 35.6 15.3 14.3 35.2 33.2 51.5 45.8 89.3 M\nBFP+ FPN [12] ResNet-152 38.3 37.1 60.2 58.5 39.7 38.0 19.0 16.1 39.6 38.9 53.0 50.1 91.2 M\nTable S9. Result comparisons on diÔ¨Äerent backbones. Experiments are carried out\non validation set of MS-COCO 2017 [21]. ‚ÄúBFP‚Äù is the bottom-up feature pyramid\n(BFP) [12]. Results on the left and right of the dashed are of bounding box detection\nand instance segmentation.\nResNet-50 achieves 38.0%/36.8% AP, which surpasses BFP + FPN [12] and\nBFP + BPA [13] ( i.e., 36.2%/35.7% AP and 37.3% /36.3% AP) on ResNet-\n101 under the similar number of parameters (88 M). Compared to results on\nResNet-152, FPT can still surpass BFP (35.8%/34.6% AP) with fewer param-\neters. Although BFP+FPN on ResNet-152 can slightly outperform FPT on\nResNet-50, it has more parameters.\nE More Qualitative Results\nThis section supplements to the visualization results given in Section 4.1.1 and\nSection 4.2.2 (of the main paper). The results of object detection, instance seg-\nmentation and semantic segmentation are visualized respectively in Fig. S1,\nFig. S2 and Fig. S4. The samples for object detection and instance segmen-\ntation are from the test set of MS-COCO 2017 [21]. As we can see in Fig. S1\nand Fig. S2 that most of our predictions are of high quality, e.g., small objects\nFeature Pyramid Transformer 23\nsuch as persons and sheep in the distance are correctly detected. The samples for\nsemantic segmentation are from the validation set of PSACAL VOC2012 [23].\nThe demonstration in Fig. S4 validates that FPT achieves precise segmentation\nof the thinner objects, e.g., the biker‚Äôs foot, the cat‚Äôs tail, the man in the dis-\ntance and the woman‚Äôs arm. Moreover, FPT enhances the segmentation quality\nof larger objects, e.g., the sofa, the bottle, and the dining-table.\nIn Fig. S3, we additionally present the failure examples for object detection\nand instance segmentation. One possible reason for these failure results is that\nthe background of these objects is not annotated in the ground truth, for example\nthere is no category information for ‚Äúmirror‚Äù and ‚Äúpainting‚Äù in the MS-COCO\n2017 [21] dataset. Hence, objects in these backgrounds can easily be recognized\nas the real ones, e.g., the cat in the painting, the bike on the wall, and the man\nin the mirror.\n24 D. Zhang, H. Zhang, et al.\nFig. S1. More object detection results.Samples are from test set of MS-COCO\n2017 [21].\nFeature Pyramid Transformer 25\ninput FPN BPA BFI FPT\nFig. S2. More instance segmentation results.Samples are from test set of MS-\nCOCO 2017 [21]. The red rectangle highlights the better predicted areas of FPT.\nFig. S3. Failure examples of object detection and instance segmentation.\nSamples are from test set of MS-COCO 2017 [21].\n26 D. Zhang, H. Zhang, et al.\ninput OCNetbaseline Ground TruthFPT\nFig. S4. Semantic segmentation results.Samples are from val set of PSACAL\nVOC 2012 [23]. The yellow rectangle highlights the better predicted areas of FPT.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6837154626846313
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6759732961654663
    },
    {
      "name": "Segmentation",
      "score": 0.6646793484687805
    },
    {
      "name": "Pyramid (geometry)",
      "score": 0.6631442308425903
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.5449676513671875
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5359419584274292
    },
    {
      "name": "Transformer",
      "score": 0.5139195919036865
    },
    {
      "name": "Pixel",
      "score": 0.4641512632369995
    },
    {
      "name": "Computer vision",
      "score": 0.42705950140953064
    },
    {
      "name": "Mathematics",
      "score": 0.14697256684303284
    },
    {
      "name": "Engineering",
      "score": 0.08673524856567383
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I36399199",
      "name": "Nanjing University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I172675005",
      "name": "Nanyang Technological University",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I16365422",
      "name": "Hefei University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I45928872",
      "name": "Alibaba Group (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I79891267",
      "name": "Singapore Management University",
      "country": "SG"
    }
  ],
  "cited_by": 7
}