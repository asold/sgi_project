{
    "title": "Sort by Structure: Language Model Ranking as Dependency Probing",
    "url": "https://openalex.org/W4287855067",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4287857853",
            "name": "Max Müller- Eberstein",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287857854",
            "name": "Rob Goot",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1970522973",
            "name": "Barbara Plank",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3034763191",
        "https://openalex.org/W2952370363",
        "https://openalex.org/W3090789254",
        "https://openalex.org/W3008110149",
        "https://openalex.org/W3099878876",
        "https://openalex.org/W2946359678",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3154147337",
        "https://openalex.org/W331019419",
        "https://openalex.org/W3035261420",
        "https://openalex.org/W3104235057",
        "https://openalex.org/W3154806625",
        "https://openalex.org/W2011301426",
        "https://openalex.org/W3212892226",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3133604157",
        "https://openalex.org/W4288351520",
        "https://openalex.org/W2889197485",
        "https://openalex.org/W2963809228",
        "https://openalex.org/W4287633642",
        "https://openalex.org/W3003257820",
        "https://openalex.org/W2613364385",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2250263931",
        "https://openalex.org/W2970862333",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W4287993739",
        "https://openalex.org/W2805981949",
        "https://openalex.org/W3034776473",
        "https://openalex.org/W3098749165",
        "https://openalex.org/W4287727940",
        "https://openalex.org/W2552110825",
        "https://openalex.org/W4205424278",
        "https://openalex.org/W1707848225",
        "https://openalex.org/W3155312918",
        "https://openalex.org/W2282211856"
    ],
    "abstract": "Making an informed choice of pre-trained language model (LM) is critical for performance, yet environmentally costly, and as such widely underexplored. The field of Computer Vision has begun to tackle encoder ranking, with promising forays into Natural Language Processing, however they lack coverage of linguistic tasks such as structured prediction. We propose probing to rank LMs, specifically for parsing dependencies in a given language, by measuring the degree to which labeled trees are recoverable from an LM’s contextualized embeddings. Across 46 typologically and architecturally diverse LM-language pairs, our probing approach predicts the best LM choice 79% of the time using orders of magnitude less compute than training a full parser. Within this study, we identify and analyze one recently proposed decoupled LM—RemBERT—and find it strikingly contains less inherent dependency information, but often yields the best parser after full fine-tuning. Without this outlier our approach identifies the best LM in 89% of cases.",
    "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1296 - 1307\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\n/sort-amount-downSort by Structure: Language Model Ranking as Dependency Probing\nMax Müller-Eberstein☼ and Rob van der Goot☼ and Barbara Plank☼⛰\n☼ Department of Computer Science, IT University of Copenhagen, Denmark\n⛰Center for Information and Language Processing (CIS), LMU Munich, Germany\nmamy@itu.dk, robv@itu.dk, bplank@cis.lmu.de\nAbstract\nMaking an informed choice of pre-trained lan-\nguage model (LM) is critical for performance,\nyet environmentally costly, and as such widely\nunderexplored. The field of Computer Vi-\nsion has begun to tackle encoder ranking, with\npromising forays into Natural Language Pro-\ncessing, however they lack coverage of linguis-\ntic tasks such as structured prediction. We\npropose probing to rank LMs, specifically for\nparsing dependencies in a given language, by\nmeasuring the degree to which labeled trees\nare recoverable from an LM’s contextualized\nembeddings. Across 46 typologically and ar-\nchitecturally diverse LM-language pairs, our\nprobing approach predicts the best LM choice\n79% of the time using orders of magnitude less\ncompute than training a full parser. Within this\nstudy, we identify and analyze one recently pro-\nposed decoupled LM—RemBERT—and find it\nstrikingly contains less inherent dependency in-\nformation, but often yields the best parser after\nfull fine-tuning. Without this outlier our ap-\nproach identifies the best LM in 89% of cases.\n1 Introduction\nWith the advent of massively pre-trained language\nmodels (LMs) in Natural Language Processing\n(NLP), it has become crucial for practitioners to\nchoose the best LM encoder for their given task\nearly on, regardless of the rest of their proposed\nmodel architecture. The greatest variation of LMs\nlies in the language or domain-specificity of the\nunlabelled data used during pre-training (with ar-\nchitectures often staying identical).\nTypically, better expressivity is expected from\nlanguage/domain-specific LMs (Gururangan et al.,\n2020; Dai et al., 2020) while open-domain settings\nnecessitate high-capacity models with access to as\nmuch pre-training data as possible. This tradeoff is\ndifficult to navigate, and given that multiple special-\nized LMs (or none at all) are available, practitioners\noften resort to an ad-hoc choice. In absence of im-\nmediate performance indicators, the most accurate\nchoice could be made by training the full model\nusing each LM candidate, however this is often\ninfeasible and wasteful (Strubell et al., 2019).\nRecently, the field of Computer Vision (CV) has\nattempted to tackle this problem by quantifying\nuseful information in pre-trained image encoders\nas measured directly on labeled target data without\nfine-tuning (Nguyen et al., 2020; You et al., 2021).\nWhile first forays for applying these methods to\nNLP are promising, some linguistic tasks differ\nsubstantially: Structured prediction, such as pars-\ning syntactic dependencies, is a fundamental NLP\ntask not covered by prior encoder ranking methods\ndue to its graphical output. Simultaneously, perfor-\nmance prediction in NLP has so far been studied as\na function of dataset and model characteristics (Xia\net al., 2020; Ye et al., 2021) and has yet to examine\nhow to rank large pools of pre-trained LMs.\nGiven the closely related field of probing, in\nwhich lightweight models quantify task-specific\ninformation in pre-trained LMs, we recast its ob-\njective in the context of performance prediction\nand ask: How predictive is lightweight probing at\nchoosing the best performing LM for dependency\nparsing? To answer this question, we contribute:\n• An efficient encoder ranking method for struc-\ntured prediction using dependency probing\n(Müller-Eberstein et al., 2022; DEPPROBE ) to\nquantify latent syntax (Section 2).\n• Experiments across 46 typologically and ar-\nchitecturally diverse LM + target language\ncombinations (Section 3).1\n• An in-depth analysis of the surprisingly low\ninherent dependency information in Rem-\nBERT (Chung et al., 2021) compared to its\nhigh fine-tuned performance (Section 4).\n1Code at https://personads.me/x/naacl-2022-code.\n1296\nB\nL\nroot\npunct\nobj\nnsubj\namod\ndet\nroot\nnsubj\ndet\namod\nobj\npunct\nDEPPROBE\nLM Encoder\nfrozen weights\nFigure 1: Visualization ofDEPPROBE . Relational and\nstructural subspaces Land Bare combined to extract\nlabeled, directed trees from embeddings.\n2 Methodology\nProbing pre-trained LMs is highly related to en-\ncoder ranking in CV where the ease of recover-\nability of class-differentiating information is key\n(Nguyen et al., 2020; You et al., 2021). This ap-\nproach is more immediate than existing NLP per-\nformance prediction methods which rely on fea-\nturized representations of source and target data\nwithout actively ranking encoders (Xia et al., 2020;\nYe et al., 2021). As most experiments in NLP are\nconducted using a limited set of LMs—often a sin-\ngle model—without strong prior motivations, we\nsee LM ranking as a critical task on its own.\nWhile probes for LMs come in many forms, they\nare generally characterized as lightweight, min-\nimal architectures intended to solve a particular\ntask (Hall Maudslay et al., 2020). While non-linear\nmodels such as small multi-layer perceptrons are of-\nten used (Tenney et al., 2019), there have been crit-\nicisms given that their performance highly depends\non the complexity of their architecture (Hewitt and\nLiang, 2019; V oita and Titov, 2020). As such,\nwe rely on linear probes alone, which have the\nbenefit of being extremely lightweight, closely re-\nsembling existing performance prediction methods\n(You et al., 2021), and allow for statements about\nlinear subspaces contained in LM latent spaces.\nDEPPROBE (Müller-Eberstein et al., 2022; vi-\nsualized in Figure 1) is a linear formulation for\nextracting fully labeled dependency trees based on\nthe structural probe by Hewitt and Manning (2019).\nGiven contextualized embeddings of dimensional-\nity d, a linear transformationB ∈Rb×dwith b≪d\n(typically b= 128) maps them into a subspace in\nwhich the Euclidean distance between embeddings\ncorresponds to the number of edges between the\nrespective words in the gold dependency graph.\nIn our formulation, we supplement a linear trans-\nformation L ∈Rl×d (with l = number of depen-\ndency relations) which maps each embedding to a\nsubspace in which the magnitude of each dimen-\nsion corresponds to the likelihood of a word and its\nhead being governed by a certain relation.\nBy computing the minimum spanning tree in B\nand then finding the word with the highest root\nlikelihood in L, we can determine the direction-\nality of all edges as pointing away from the root.\nAll remaining edges are labeled according to the\nmost likely non-root class in L, resulting in a fully\ndirected and labeled dependency tree.\nNote that this approach differs substantially from\nprior approaches which yield undirected and/or\nunlabeled trees (Hewitt and Manning, 2019; Kul-\nmizev et al., 2020) or use pre-computed edges and\nnon-linear classifiers (Tenney et al., 2019). DEP-\nPROBE efficiently computes the full target metric\n(i.e. labeled attachment scores) instead of approxi-\nmate alternatives (e.g. undirected, unlabeled attach-\nment scores or tree depth correlation).\n3 Experiments\nSetup We investigate the ability of DEPPROBE\nto select the best performing LM for dependency\nparsing across nine linguistically diverse treebanks\nfrom Universal Dependencies (Zeman et al., 2021;\nUD) which were previously chosen by Smith et al.\n(2018) to reflect diverse writing systems and mor-\nphological complexity (see Appendix A).\nFor each target language, we employ three multi-\nlingual LMs—mBERT (Devlin et al., 2019), XLM-\nR (Conneau et al., 2020), RemBERT (Chung et al.,\n2021)—as well as 1–3 language-specific LMs re-\ntrieved by popularity from HuggingFace’s Model\nHub (Wolf et al., 2020), resulting in a total of 46\nLM-target pair setups (see Appendix C).\nFor each combination, we train a DEPPROBE\nto compute labeled attachment scores (LAS), hy-\npothesizing that LMs from which trees are most\naccurately recoverable also perform better in a fully\ntuned parser. To evaluate the true downstream per-\nformance of a fully-tuned model, we further train\na deep biaffine attention parser (BAP; Dozat and\nManning, 2017) on each LM-target combination.\nCompared to full fine-tuning, DEPPROBE only op-\ntimizes the matrices Band L, resulting in the ex-\ntraction of labeled trees with as few as 190k instead\nof 583M trainable parameters for the largest Rem-\nBERT model (details in Appendix B).\nWe measure the predictive power of probing for\nfully fine-tuned model performance using the Pear-\nson correlation coefficientρas well as the weighted\n1297\n10 20 30 40 50 60 70\nDepProbe (LAS)\n75\n80\n85\n90\n95BAP (LAS)\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmBERT\nXLM-R\nRemBERT\n*BERT\n*RoBERTA\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAR\nEN\nFI\nGRC\nHE\nKO\nRU\nSV\nZH\nFigure 2: LAS of DEPPROBE in relation to full\nBAP across nine language targets (dev) using language-\nspecific and multilingual LM encoders of different ar-\nchitecture types (exact scores in Appendix C).\nKendall’s τw (Vigna, 2015). The latter metric cor-\nresponds to a correlation coefficient in [−1,1] and\nsimultaneously defines the probability of choosing\nthe better LM given a pair as τw+1\n2 , allowing us to\nquantify the overall quality of a ranking.\nResults Comparing the LAS of DEPPROBE ’s\nlightweight predictions against full BAP fine-\ntuning in Figure 2, we see a clear correlation as\nthe probe correctly predicts the difficulty of pars-\ning languages relative to each other and also ranks\nmodels within languages closely according to their\nfinal performance. With a τw of .58 between scores\n(p< 0.001), this works out to DEPPROBE select-\ning the better performing final model given any\ntwo models 79% of the time. Additionally, LAS is\nslightly more predictive of final performance than\nunlabeled, undirected attachment scores (UUAS)\nwith τw = .57 to which prior probing approaches\nare restricted (see Appendix C).\nGiven a modest ρ of .32 ( p < 0.05), we sur-\nprisingly also observe a single strong outlier to\nthis pattern, namely the multilingual RemBERT\n(Chung et al., 2021) decoupled LM architecture.\nWhile DEPPROBE consistently ranks it low as it\ncannot extract dependency parse trees as accurately\nas from the BERT and RoBERTa-based architec-\ntures, RemBERT actually performs best on four\nout of the nine targets when fully fine-tuned in\nBAP. Excluding monolingual LMs, it further out-\nperforms the other multilingual LMs in seven out\nof nine cases. As it is a more recent and distinc-\ntive architecture with many differences to the most\ncommonly-used contemporary LMs, we analyze\npotential reasons for this discrepancy in Section 4.\nExcluding RemBERT as an outlier, we find sub-\n0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32\nLayer\n20\n30\n40\n50\n60UUAS & RelAcc & LAS\nUUAS\nRelAcc\nLAS\nFigure 3: Dependency Information per RemBERT\nLayer via DEPPROBE ’s structural, relational and pars-\ning accuracy (UUAS, RelAcc, LAS) on EN-EWT (dev).\nstantially higher correlation among all other mod-\nels: ρ= .78 and τw = .78 (p< 0.001). This means\nthat among these models, fully fine-tuning the LM\nfor which DEPPROBE extracts the highest scores,\nyields the better final performance 89% of the time.\nIn practice, learning DEPPROBE ’s linear trans-\nformations while keeping the LM frozen is multiple\norders of magnitude more efficient than fully train-\ning a complex parser plus the LM’s parameters.\nAs such, linear probing offers a viable method for\nselecting the best encoder in absence of qualita-\ntive heuristics or intuitions. This predictive perfor-\nmance is furthermore achievable in minutes com-\npared to hours and at a far lower energy budget (see\nAppendices B and C).\n4 Probing Decoupled LMs\nConsidering DEPPROBE ’s high predictive perfor-\nmance across LMs with varying architecture types,\nlanguages/domains and pre-training procedures,\nwe next investigate its limitations: Specifically,\nwhich differences in RemBERT (Chung et al.,\n2021) lead to it being measured as an outlier with\nseemingly low amounts of latent dependency infor-\nmation despite reaching some of the highest scores\nafter full fine-tuning. The architecture has 32 lay-\ners and embeddings with d= 1152, compared to\nmost models’ 12 layers and d = 768. It accom-\nmodates these size and depth increases within a\nmanageable parameter envelope by using smaller\ninput embeddings with din = 256. While choosing\ndifferent d for the input and output embeddings\nis not possible in most prior models due to both\nembedding matrices being coupled, RemBERT de-\ncouples them, leading to a larger parameter budget\nand less overfitting on the masked language model-\ning pre-training task (Chung et al., 2021).\n1298\n0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32\nLayer\nAR\nEN\nFI\nGRC\nHE\nKO\nRU\nSV\nZH\nFigure 4: Per-language α of RemBERT Layersfor\nDEPPROBE across all layer weights (dark >light).\nMODEL AR EN FI GRC HE KO RU SV ZH\nmBERT 65 74 65 46 69 58 68 65 58\n±.08 ±.09 ±.35 ±.14 ±.23 ±.18 ±.31 ±.12 ±.17\nXLM-R 60 70 66 53 60 49 57 51 51\n±.14 ±.08 ±.18 ±.19 ±.20 ±.08 ±.34 ±.24 ±.53\nRemBERT58 56 52 54 52 46 49 43 39\n±.12 ±.22 ±.15 ±.18 ±.05 ±.14 ±.04 ±.08 ±.24\nTable 1: LAS ofBAP Trained on Frozen LMs.A\nbiaffine attention parsing head is trained on top of frozen\nmBERT, XLM-R and RemBERT for each of the nine\ntarget languages (±standard deviation).\nLayer-wise Probing Prior probing studies have\nfound dependency information to be concentrated\naround the middle layers of an LM (Hewitt and\nManning, 2019; Tenney et al., 2019; Fayyaz et al.,\n2021). Using EN-EWT (Silveira et al., 2014),\nwe evaluate whether this holds for RemBERT’s\nnew architecture. Figure 3 confirms that both de-\npendency structural and relational information are\nmost prominent around layer 17 of 32 as indi-\ncated by UUAS and relation classification accuracy\n(RelAcc) respectively. Combining the structural\nand relational information in DEPPROBE similarly\nleads to a peak of the LAS at the same layer while\ndecreasing with further distance from the center.\nAcross all target languages, we next investigate\nwhether probing a sum over the embeddings of all\nlayers weighted by α ∈R32 can boost extraction\nperformance in RemBERT. The heavier weighting\nof middle layers by α, visible in Figure 4, reaf-\nfirms a concentration of dependency information\nin the center. Contrasting probing work on prior\nmodels (Tenney et al., 2019; Kulmizev et al., 2020),\nusing all layers does not increase the retrievable de-\npendencies, with LAS differences ±1 point. This\nfurther confirms that there is not a lack of depen-\ndency information in any specific layer, but that\nthere is less within the encoder as a whole.\nFrozen Parsing Our probing results show that\nlinear subspaces in RemBERT contain less depen-\ndency information than prior LMs. However, DEP-\nPROBE ’s parametrization is kept intentionally sim-\nple and may therefore not be capturing non-linearly\nrepresented information that is useful during later\nfine-tuning. To evaluate this hypothesis, we train\na full biaffine attention parsing head, but keep the\nunderlying LM encoder frozen. This allows us to\nquantify the performance gains which come from\ninherent dependency information versus later task-\nspecific fine-tuning.\nTable 1 confirms our findings from DEPPROBE\nand shows that despite RemBERT outperforming\nmBERT and XLM-R when fully fine-tuned, it\nhas substantially lower LAS across almost all lan-\nguages when no full model fine-tuning is applied.\nThis leads us to conclude that there indeed is less in-\nherent dependency information in the newer model\nand that most performance gains must be occurring\nduring task-specific full fine-tuning.\nGiven that DEPPROBE extracts dependency\nstructures reliably from LM architectures with dif-\nferent depths and embedding dimensionalities (e.g.\nRoBERTalarge with 24 layers and d= 1024 versus\nRuBERTtiny with 3 layers and d= 312) as well as\nvarying tokenization, optimization and pre-training\ndata, the key difference in RemBERT appears to\nbe embedding decoupling. The probe’s linear for-\nmulation is not the limiting factor as the non-linear,\nbiaffine attention head also produces less accurate\nparses when the LM’s weights are frozen. Our\nanalyses thus suggest that RemBERT’s decoupled\narchitecture contains less dependency information\nout-of-the-box, but follows prior patterns such as\nconsolidating dependency information towards its\nmiddle layers and serving as strong initialization\nfor parser training.\nLastly, RemBERT’s larger number of tunable\nparameters compared to all other LM candidates\nmay provide it further capacity, especially after full\nfine-tuning. As our probing methods are deliber-\nately applied to the frozen representations of the en-\ncoder, it becomes especially important to consider\nthe degree to which these embeddings may change\nafter updating large parts of the model. Taking\nthese limitations into account, the high correlations\nwith respect to encoder ranking nonetheless enable\na much more informed selection of LMs from a\nlarger pool than was previously possible.\n5 Conclusion\nTo guide practitioners in their choice of LM en-\ncoder for the structured prediction task of depen-\ndency parsing, we leveraged a lightweight, linear\n1299\nDEPPROBE to quantify the latent syntactic infor-\nmation via the labeled attachment score. Evaluat-\ning 46 pairs of multilingual/language-specific LMs\nand nine typologically diverse target treebanks, we\nfound DEPPROBE to not only be efficient in its\npredictions, with orders of magnitude fewer train-\nable parameters, but to also be accurate 79–89%\nof the time in predicting which LM will outper-\nform another when used in a fully tuned parser.\nThis allows for a substantially faster iteration over\npotential LM candidates, saving hours worth of\ncompute in practice (Section 3).\nOur experiments further revealed surprising in-\nsights on the newly proposed RemBERT architec-\nture: While particularly effective for multilingual\ndependency parsing when fully fine-tuned, it con-\ntains substantially less latent dependency informa-\ntion relative to prior widely-used models such as\nmBERT and XLM-R. Among its architectural dif-\nferences, we identified embedding decoupling to\nbe the most likely contributor, while added model\ncapacity during fine-tuning may also improve final\nperformance. Our analyses showed that despite\ncontaining less dependency information overall,\nRemBERT follows prior findings such as structure\nand syntactic relations being consolidated towards\nthe middle layers. Given these consistencies, per-\nformance differences between decoupled LMs may\nbe predictable using probes, but in absence of simi-\nlar multilingual LMs using decoupled embeddings\nthis effect remains to be studied (Section 4).\nOverall, the high efficiency and predictive power\nof ranking LM encoders via linear probing as well\nas the ease with which they can be analyzed—even\nwhen they encounter their limitations—offers im-\nmediate benefits to practitioners who have so far\nhad to rely on their own intuitions when making\na selection. This opens up avenues for future re-\nsearch by extending these methods to more tasks\nand LM architectures in order to enable better in-\nformed modeling decisions.\nAcknowledgements\nWe would like to thank the NLPnorth group for\ninsightful discussions on this work, in particular\nElisa Bassignana and Mike Zhang. Thanks also to\nITU’s High-performance Computing team. Finally,\nwe thank the anonymous reviewers for their helpful\nfeedback. This research is supported by the Inde-\npendent Research Fund Denmark (Danmarks Frie\nForskningsfond; DFF) grant number 9063-00077B.\nReferences\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020.\nAraBERT: Transformer-based model for Arabic lan-\nguage understanding. In Proceedings of the 4th Work-\nshop on Open-Source Arabic Corpora and Process-\ning Tools, with a Shared Task on Offensive Language\nDetection, pages 9–15, Marseille, France. European\nLanguage Resource Association.\nPavel Blinov. 2021. RoBERTa-base Russian.\nhttps://huggingface.co/blinoff/\nroberta-base-russian-v0 . Accessed 4th\nJanuary, 2022.\nJayeol Chun, Na-Rae Han, Jena D. Hwang, and Jinho D.\nChoi. 2018. Building Universal Dependency tree-\nbanks in Korean. In Proceedings of the Eleventh In-\nternational Conference on Language Resources and\nEvaluation (LREC 2018), Miyazaki, Japan. European\nLanguage Resources Association (ELRA).\nHyung Won Chung, Thibault Fevry, Henry Tsai, Melvin\nJohnson, and Sebastian Ruder. 2021. Rethinking em-\nbedding coupling in pre-trained language models. In\nInternational Conference on Learning Representa-\ntions.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, and\nZiqing Yang. 2021. Pre-training with whole word\nmasking for Chinese BERT. IEEE/ACM Transac-\ntions on Audio, Speech, and Language Processing ,\n29:3504–3514.\nXiang Dai, Sarvnaz Karimi, Ben Hachey, and Cecile\nParis. 2020. Cost-effective selection of pretraining\ndata: A case study of pretraining BERT on social\nmedia. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1675–1681,\nOnline. Association for Computational Linguistics.\nDavid Dale. 2021. RuBERT-tiny: A small and fast\nBERT for Russian. https://habr.com/ru/\npost/562064/. Accessed 4th January, 2022.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\n1300\nTimothy Dozat and Christopher D. Manning. 2017.\nDeep biaffine attention for neural dependency pars-\ning. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings. Open-\nReview.net.\nHanne Eckhoff, Kristin Bech, Gerlof Bouma, Kristine\nEide, Dag Haug, Odd Einar Haugen, and Marius Jøh-\nndal. 2018. The PROIEL treebank family: a standard\nfor early attestations of Indo-European languages.\nLanguage Resources and Evaluation, 52(1):29–65.\nMohsen Fayyaz, Ehsan Aghazadeh, Ali Modarressi, Ho-\nsein Mohebbi, and Mohammad Taher Pilehvar. 2021.\nNot all models localize linguistic knowledge in the\nsame place: A layer-wise probing on BERToids’ rep-\nresentations. In Proceedings of the Fourth Black-\nboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP , pages 375–388, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nJan Haji ˇc, Otakar Smrž, Petr Zemánek, Petr Pajas,\nJan Šnaidauf, Emanuel Beška, Jakub Krácmar, and\nKamila Hassanová. 2009. Prague Arabic dependency\ntreebank 1.0.\nRowan Hall Maudslay, Josef Valvoda, Tiago Pimentel,\nAdina Williams, and Ryan Cotterell. 2020. A tale\nof a probe and a parser. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 7389–7395, Online. Association\nfor Computational Linguistics.\nCharles R. Harris, K. Jarrod Millman, Stéfan J. van der\nWalt, Ralf Gommers, Pauli Virtanen, David Cour-\nnapeau, Eric Wieser, Julian Taylor, Sebastian Berg,\nNathaniel J. Smith, Robert Kern, Matti Picus,\nStephan Hoyer, Marten H. van Kerkwijk, Matthew\nBrett, Allan Haldane, Jaime Fernández del Río, Mark\nWiebe, Pearu Peterson, Pierre Gérard-Marchant,\nKevin Sheppard, Tyler Reddy, Warren Weckesser,\nHameer Abbasi, Christoph Gohlke, and Travis E.\nOliphant. 2020. Array programming with NumPy.\nNature, 585(7825):357–362.\nJohn Hewitt and Percy Liang. 2019. Designing and in-\nterpreting probes with control tasks. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2733–2743, Hong Kong,\nChina. Association for Computational Linguistics.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for finding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4129–4138, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJ. D. Hunter. 2007. Matplotlib: A 2d graphics environ-\nment. Computing in Science & Engineering, 9(3):90–\n95.\nKiyoung Kim. 2020. Pretrained language models for ko-\nrean. https://github.com/kiyoungkim1/\nLMkor.\nJohn Koutsikakis, Ilias Chalkidis, Prodromos Malaka-\nsiotis, and Ion Androutsopoulos. 2020. Greek-bert:\nThe greeks visiting sesame street. In 11th Hellenic\nConference on Artificial Intelligence , SETN 2020,\npage 110–117, New York, NY , USA. Association for\nComputing Machinery.\nArtur Kulmizev, Vinit Ravishankar, Mostafa Abdou,\nand Joakim Nivre. 2020. Do neural language mod-\nels show preferences for syntactic formalisms? In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4077–\n4091, Online. Association for Computational Lin-\nguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nMartin Malmsten, Love Börjeson, and Chris Haffenden.\n2020. Playing with words at the national library of\nsweden – making a swedish bert.\nRyan McDonald, Joakim Nivre, Yvonne Quirmbach-\nBrundage, Yoav Goldberg, Dipanjan Das, Kuzman\nGanchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar\nTäckström, Claudia Bedini, Núria Bertomeu Castelló,\nand Jungmee Lee. 2013a. Universal Dependency an-\nnotation for multilingual parsing. In Proceedings\nof the 51st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers),\npages 92–97, Sofia, Bulgaria. Association for Com-\nputational Linguistics.\nRyan McDonald, Joakim Nivre, Yvonne Quirmbach-\nBrundage, Yoav Goldberg, Dipanjan Das, Kuzman\nGanchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar\nTäckström, Claudia Bedini, Núria Bertomeu Castelló,\nand Jungmee Lee. 2013b. Universal Dependency an-\nnotation for multilingual parsing. In Proceedings\nof the 51st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers),\n1301\npages 92–97, Sofia, Bulgaria. Association for Com-\nputational Linguistics.\nMax Müller-Eberstein, Rob van der Goot, and Barbara\nPlank. 2022. Probing for labeled dependency trees.\nComputing Research Repository, arxiv:2203.12971.\nVersion 1.\nCuong Nguyen, Tal Hassner, Matthias Seeger, and\nCedric Archambeau. 2020. Leep: A new measure\nto evaluate transferability of learned representations.\nIn International Conference on Machine Learning,\npages 7294–7305. PMLR.\nSungjoon Park, Jihyung Moon, Sungdong Kim, Won Ik\nCho, Jiyoon Han, Jangwon Park, Chisung Song, Jun-\nseong Kim, Yongsook Song, Taehwan Oh, Joohong\nLee, Juhyun Oh, Sungwon Lyu, Younghoon Jeong,\nInkwon Lee, Sangwoo Seo, Dongjun Lee, Hyunwoo\nKim, Myeonghwa Lee, Seongbo Jang, Seungwon Do,\nSunkyoung Kim, Kyungtae Lim, Jongwon Lee, Kyu-\nmin Park, Jamin Shin, Seonghyun Kim, Lucy Park,\nAlice Oh, Jungwoo Ha, and Kyunghyun Cho. 2021.\nKlue: Korean language understanding evaluation.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. PyTorch:\nAn imperative style, high-performance deep learning\nlibrary. In Advances in Neural Information Process-\ning Systems 32, pages 8024–8035. Curran Associates,\nInc.\nSampo Pyysalo, Jenna Kanerva, Anna Missilä, Veronika\nLaippala, and Filip Ginter. 2015. Universal Depen-\ndencies for Finnish. In Proceedings of the 20th\nNordic Conference of Computational Linguistics\n(NODALIDA 2015), pages 163–172, Vilnius, Lithua-\nnia. Linköping University Electronic Press, Sweden.\nAli Safaya, Moutasem Abdullatif, and Deniz Yuret.\n2020. KUISAIL at SemEval-2020 task 12: BERT-\nCNN for offensive speech identification in social me-\ndia. In Proceedings of the Fourteenth Workshop on\nSemantic Evaluation, pages 2054–2059, Barcelona\n(online). International Committee for Computational\nLinguistics.\nSber Devices. 2021. ruRoBERTa-large. https:\n//huggingface.co/sberbank-ai/\nruRoberta-large. Accessed 4th January,\n2022.\nAmit Seker, Elron Bandel, Dan Bareket, Idan\nBrusilovsky, Refael Shaked Greenfeld, and Reut Tsar-\nfaty. 2021. Alephbert: A hebrew large pre-trained\nlanguage model to start-off your hebrew nlp applica-\ntion with.\nMo Shen, Ryan McDonald, Daniel Zeman, and\nPeng Qi. 2016. UD_Chinese-GSD. https://\ngithub.com/UniversalDependencies/\nUD_Chinese-GSD.\nNatalia Silveira, Timothy Dozat, Marie-Catherine\nde Marneffe, Samuel Bowman, Miriam Connor, John\nBauer, and Chris Manning. 2014. A gold standard\ndependency corpus for English. In Proceedings of\nthe Ninth International Conference on Language\nResources and Evaluation (LREC’14), pages 2897–\n2904, Reykjavik, Iceland. European Language Re-\nsources Association (ELRA).\nPranaydeep Singh, Gorik Rutten, and Els Lefever. 2021.\nA pilot study for BERT language modelling and mor-\nphological analysis for ancient and medieval Greek.\nIn Proceedings of the 5th Joint SIGHUM Workshop\non Computational Linguistics for Cultural Heritage,\nSocial Sciences, Humanities and Literature , pages\n128–137, Punta Cana, Dominican Republic (online).\nAssociation for Computational Linguistics.\nAaron Smith, Miryam de Lhoneux, Sara Stymne, and\nJoakim Nivre. 2018. An investigation of the inter-\nactions between pre-trained word embeddings, char-\nacter models and POS tags in dependency parsing.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2711–2720, Brussels, Belgium. Association for Com-\nputational Linguistics.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3645–3650, Florence, Italy. Asso-\nciation for Computational Linguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim, Ben-\njamin Van Durme, Sam Bowman, Dipanjan Das, and\nEllie Pavlick. 2019. What do you learn from con-\ntext? probing for sentence structure in contextualized\nword representations. In International Conference\non Learning Representations.\nRob van der Goot, Ahmet Üstün, Alan Ramponi,\nIbrahim Sharaf, and Barbara Plank. 2021. Massive\nchoice, ample tasks (MaChAmp): A toolkit for multi-\ntask learning in NLP. In Proceedings of the 16th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics: System Demon-\nstrations, pages 176–197, Online. Association for\nComputational Linguistics.\nSebastiano Vigna. 2015. A weighted correlation index\nfor rankings with ties. In Proceedings of the 24th\ninternational conference on World Wide Web, pages\n1166–1176.\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma,\nJuhani Luotolahti, Tapio Salakoski, Filip Ginter, and\nSampo Pyysalo. 2019. Multilingual is not enough:\nBERT for finnish. CoRR, abs/1912.07076.\n1302\nPauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt\nHaberland, Tyler Reddy, David Cournapeau, Ev-\ngeni Burovski, Pearu Peterson, Warren Weckesser,\nJonathan Bright, Stéfan J. van der Walt, Matthew\nBrett, Joshua Wilson, K. Jarrod Millman, Nikolay\nMayorov, Andrew R. J. Nelson, Eric Jones, Robert\nKern, Eric Larson, C J Carey, ˙Ilhan Polat, Yu Feng,\nEric W. Moore, Jake VanderPlas, Denis Laxalde,\nJosef Perktold, Robert Cimrman, Ian Henriksen, E. A.\nQuintero, Charles R. Harris, Anne M. Archibald, An-\ntônio H. Ribeiro, Fabian Pedregosa, Paul van Mul-\nbregt, and SciPy 1.0 Contributors. 2020. SciPy 1.0:\nFundamental Algorithms for Scientific Computing in\nPython. Nature Methods, 17:261–272.\nElena V oita and Ivan Titov. 2020. Information-theoretic\nprobing with minimum description length. In Pro-\nceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP) ,\npages 183–196, Online. Association for Computa-\ntional Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nMengzhou Xia, Antonios Anastasopoulos, Ruochen Xu,\nYiming Yang, and Graham Neubig. 2020. Predicting\nperformance for natural language processing tasks.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 8625–\n8646, Online. Association for Computational Lin-\nguistics.\nZihuiwen Ye, Pengfei Liu, Jinlan Fu, and Graham Neu-\nbig. 2021. Towards more fine-grained and reliable\nNLP performance prediction. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 3703–3714, Online. Association for Computa-\ntional Linguistics.\nKaichao You, Yong Liu, Jianmin Wang, and Ming-\nsheng Long. 2021. Logme: Practical assessment\nof pre-trained models for transfer learning. In In-\nternational Conference on Machine Learning, pages\n12133–12143. PMLR.\nDaniel Zeman, Joakim Nivre, Mitchell Abrams, Elia\nAckermann, Noëmi Aepli, Hamid Aghaei, Željko\nAgi´c, Amir Ahmadi, Lars Ahrenberg, Chika Kennedy\nAjede, Gabriel˙e Aleksandraviˇci¯ut˙e, Ika Alfina, Lene\nAntonsen, Katya Aplonova, Angelina Aquino, Car-\nolina Aragon, Maria Jesus Aranzabe, Bilge Nas\nArıcan, ⁀Hórunn Arnardóttir, Gashaw Arutie, Jes-\nsica Naraiswari Arwidarasti, Masayuki Asahara,\nDeniz Baran Aslan, Luma Ateyah, Furkan Atmaca,\nMohammed Attia, Aitziber Atutxa, Liesbeth Au-\ngustinus, Elena Badmaeva, Keerthana Balasubra-\nmani, Miguel Ballesteros, Esha Banerjee, Sebastian\nBank, Verginica Barbu Mititelu, Starkaður Barkar-\nson, Rodolfo Basile, Victoria Basmov, Colin Batch-\nelor, John Bauer, Seyyit Talha Bedir, Kepa Ben-\ngoetxea, Gözde Berk, Yevgeni Berzak, Irshad Ah-\nmad Bhat, Riyaz Ahmad Bhat, Erica Biagetti, Eck-\nhard Bick, Agn˙e Bielinskien˙e, Kristín Bjarnadóttir,\nRogier Blokland, Victoria Bobicev, Loïc Boizou,\nEmanuel Borges Völker, Carl Börstell, Cristina\nBosco, Gosse Bouma, Sam Bowman, Adriane Boyd,\nAnouck Braggaar, Kristina Brokait˙e, Aljoscha Bur-\nchardt, Marie Candito, Bernard Caron, Gauthier\nCaron, Lauren Cassidy, Tatiana Cavalcanti, Gül¸ sen\nCebiro˘glu Eryi ˘git, Flavio Massimiliano Cecchini,\nGiuseppe G. A. Celano, Slavomír ˇCéplö, Nesli-\nhan Cesur, Savas Cetin, Özlem Çetino ˘glu, Fabri-\ncio Chalub, Shweta Chauhan, Ethan Chi, Taishi\nChika, Yongseok Cho, Jinho Choi, Jayeol Chun,\nJuyeon Chung, Alessandra T. Cignarella, Silvie\nCinková, Aurélie Collomb, Ça˘grı Çöltekin, Miriam\nConnor, Marine Courtin, Mihaela Cristescu, Phile-\nmon Daniel, Elizabeth Davidson, Marie-Catherine\nde Marneffe, Valeria de Paiva, Mehmet Oguz De-\nrin, Elvis de Souza, Arantza Diaz de Ilarraza,\nCarly Dickerson, Arawinda Dinakaramani, Elisa\nDi Nuovo, Bamba Dione, Peter Dirix, Kaja Do-\nbrovoljc, Timothy Dozat, Kira Droganova, Puneet\nDwivedi, Hanne Eckhoff, Sandra Eiche, Marhaba\nEli, Ali Elkahky, Binyam Ephrem, Olga Erina,\nTomaž Erjavec, Aline Etienne, Wograine Evelyn,\nSidney Facundes, Richárd Farkas, Jannatul Fer-\ndaousi, Marília Fernanda, Hector Fernandez Alcalde,\nJennifer Foster, Cláudia Freitas, Kazunori Fujita,\nKatarína Gajdošová, Daniel Galbraith, Marcos Gar-\ncia, Moa Gärdenfors, Sebastian Garza, Fabrício Fer-\nraz Gerardi, Kim Gerdes, Filip Ginter, Gustavo\nGodoy, Iakes Goenaga, Koldo Gojenola, Memduh\nGökırmak, Yoav Goldberg, Xavier Gómez Guino-\nvart, Berta González Saavedra, Bernadeta Grici ¯ut˙e,\nMatias Grioni, Loïc Grobol, Normunds Gr ¯uz¯ıtis,\nBruno Guillaume, Céline Guillot-Barbance, Tunga\nGüngör, Nizar Habash, Hinrik Hafsteinsson, Jan Ha-\njiˇc, Jan Haji ˇc jr., Mika Hämäläinen, Linh Hà M ˜y,\nNa-Rae Han, Muhammad Yudistira Hanifmuti, Sam\nHardwick, Kim Harris, Dag Haug, Johannes Hei-\nnecke, Oliver Hellwig, Felix Hennig, Barbora Hladká,\nJaroslava Hlaváˇcová, Florinel Hociung, Petter Hohle,\nEva Huber, Jena Hwang, Takumi Ikeda, Anton Karl\nIngason, Radu Ion, Elena Irimia, O. lájídé Ishola,\nKaoru Ito, Siratun Jannat, Tomáš Jelínek, Apoorva\nJha, Anders Johannsen, Hildur Jónsdóttir, Fredrik\nJørgensen, Markus Juutinen, Sarveswaran K, Hüner\nKa¸ sıkara, Andre Kaasen, Nadezhda Kabaeva, Syl-\nvain Kahane, Hiroshi Kanayama, Jenna Kanerva,\nNeslihan Kara, Boris Katz, Tolga Kayadelen, Jes-\nsica Kenney, Václava Kettnerová, Jesse Kirchner,\nElena Klementieva, Elena Klyachko, Arne Köhn,\nAbdullatif Köksal, Kamil Kopacewicz, Timo Korki-\nakangas, Mehmet Köse, Natalia Kotsyba, Jolanta\nKovalevskait˙e, Simon Krek, Parameswari Krishna-\n1303\nmurthy, Sandra Kübler, O ˘guzhan Kuyrukçu, Aslı\nKuzgun, Sookyoung Kwak, Veronika Laippala,\nLucia Lam, Lorenzo Lambertino, Tatiana Lando,\nSeptina Dian Larasati, Alexei Lavrentiev, John Lee,\nPhuong Lê H`ông, Alessandro Lenci, Saran Lertpra-\ndit, Herman Leung, Maria Levina, Cheuk Ying Li,\nJosie Li, Keying Li, Yuan Li, KyungTae Lim, Bruna\nLima Padovani, Krister Lindén, Nikola Ljubeši ´c,\nOlga Loginova, Stefano Lusito, Andry Luthfi, Mikko\nLuukko, Olga Lyashevskaya, Teresa Lynn, Vivien\nMacketanz, Menel Mahamdi, Jean Maillard, Aibek\nMakazhanov, Michael Mandl, Christopher Manning,\nRuli Manurung, Bü¸ sra Mar¸ san, C˘at˘alina M ˘ar˘an-\nduc, David Mare ˇcek, Katrin Marheinecke, Héctor\nMartínez Alonso, Lorena Martín-Rodríguez, An-\ndré Martins, Jan Mašek, Hiroshi Matsuda, Yuji\nMatsumoto, Alessandro Mazzei, Ryan McDonald,\nSarah McGuinness, Gustavo Mendonça, Tatiana\nMerzhevich, Niko Miekka, Karina Mischenkova,\nMargarita Misirpashayeva, Anna Missilä, C ˘at˘alin\nMititelu, Maria Mitrofan, Yusuke Miyao, AmirHos-\nsein Mojiri Foroushani, Judit Molnár, Amirsaeid\nMoloodi, Simonetta Montemagni, Amir More, Laura\nMoreno Romero, Giovanni Moretti, Keiko Sophie\nMori, Shinsuke Mori, Tomohiko Morioka, Shigeki\nMoro, Bjartur Mortensen, Bohdan Moskalevskyi,\nKadri Muischnek, Robert Munro, Yugo Murawaki,\nKaili Müürisep, Pinkey Nainwani, Mariam Nakhlé,\nJuan Ignacio Navarro Horñiacek, Anna Nedoluzhko,\nGunta Nešpore-B¯erzkalne, Manuela Nevaci, Luong\nNguy˜ên Thi., Huy `ên Nguy ˜ên Thi. Minh, Yoshihiro\nNikaido, Vitaly Nikolaev, Rattima Nitisaroj, Alireza\nNourian, Hanna Nurmi, Stina Ojala, Atul Kr. Ojha,\nAdédayo. Olúòkun, Mai Omura, Emeka Onwueg-\nbuzia, Petya Osenova, Robert Östling, Lilja Øvre-\nlid, ¸ Saziye Betül Özate¸ s, Merve Özçelik, Arzu-\ncan Özgür, Balkız Öztürk Ba¸ saran, Hyunji Hay-\nley Park, Niko Partanen, Elena Pascual, Marco\nPassarotti, Agnieszka Patejuk, Guilherme Paulino-\nPassos, Angelika Peljak-Łapi ´nska, Siyao Peng,\nCenel-Augusto Perez, Natalia Perkova, Guy Per-\nrier, Slav Petrov, Daria Petrova, Jason Phelan, Jussi\nPiitulainen, Tommi A Pirinen, Emily Pitler, Bar-\nbara Plank, Thierry Poibeau, Larisa Ponomareva,\nMartin Popel, Lauma Pretkalni n, a, Sophie Prévost,\nProkopis Prokopidis, Adam Przepiórkowski, Ti-\nina Puolakainen, Sampo Pyysalo, Peng Qi, An-\ndriela Rääbis, Alexandre Rademaker, Mizanur Ra-\nhoman, Taraka Rama, Loganathan Ramasamy, Car-\nlos Ramisch, Fam Rashel, Mohammad Sadegh Ra-\nsooli, Vinit Ravishankar, Livy Real, Petru Rebeja,\nSiva Reddy, Mathilde Regnault, Georg Rehm, Ivan\nRiabov, Michael Rießler, Erika Rimkut˙e, Larissa Ri-\nnaldi, Laura Rituma, Putri Rizqiyah, Luisa Rocha,\nEiríkur Rögnvaldsson, Mykhailo Romanenko, Rudolf\nRosa, Valentin Ro s, ca, Davide Rovati, Olga Rud-\nina, Jack Rueter, Kristján Rúnarsson, Shoval Sadde,\nPegah Safari, Benoît Sagot, Aleksi Sahala, Shadi\nSaleh, Alessio Salomoni, Tanja Samardži´c, Stephanie\nSamson, Manuela Sanguinetti, Ezgi Sanıyar, Dage\nSärg, Baiba Saul¯ıte, Yanin Sawanakunanon, Shefali\nSaxena, Kevin Scannell, Salvatore Scarlata, Nathan\nSchneider, Sebastian Schuster, Lane Schwartz,\nDjamé Seddah, Wolfgang Seeker, Mojgan Seraji,\nSyeda Shahzadi, Mo Shen, Atsuko Shimada, Hi-\nroyuki Shirasu, Yana Shishkina, Muh Shohibussirri,\nDmitry Sichinava, Janine Siewert, Einar Freyr Sig-\nurðsson, Aline Silveira, Natalia Silveira, Maria Simi,\nRadu Simionescu, Katalin Simkó, Mária Šimková,\nKiril Simov, Maria Skachedubova, Aaron Smith, Is-\nabela Soares-Bastos, Shafi Sourov, Carolyn Spadine,\nRachele Sprugnoli, Stein⁀hór Steingrímsson, Antonio\nStella, Milan Straka, Emmett Strickland, Jana Str-\nnadová, Alane Suhr, Yogi Lesmana Sulestio, Umut\nSulubacak, Shingo Suzuki, Zsolt Szántó, Chihiro\nTaguchi, Dima Taji, Yuta Takahashi, Fabio Tam-\nburini, Mary Ann C. Tan, Takaaki Tanaka, Dipta\nTanaya, Samson Tella, Isabelle Tellier, Marinella\nTestori, Guillaume Thomas, Liisi Torga, Marsida\nToska, Trond Trosterud, Anna Trukhina, Reut Tsar-\nfaty, Utku Türk, Francis Tyers, Sumire Uematsu, Ro-\nman Untilov, Zdeˇnka Urešová, Larraitz Uria, Hans\nUszkoreit, Andrius Utka, Sowmya Vajjala, Rob\nvan der Goot, Martine Vanhove, Daniel van Niekerk,\nGertjan van Noord, Viktor Varga, Eric Villemonte\nde la Clergerie, Veronika Vincze, Natalia Vlasova,\nAya Wakasa, Joel C. Wallenberg, Lars Wallin, Abi-\ngail Walsh, Jing Xian Wang, Jonathan North Wash-\nington, Maximilan Wendt, Paul Widmer, Sri Hartati\nWijono, Seyi Williams, Mats Wirén, Christian Wit-\ntern, Tsegay Woldemariam, Tak-sum Wong, Alina\nWróblewska, Mary Yako, Kayo Yamashita, Naoki\nYamazaki, Chunxiao Yan, Koichi Yasuoka, Marat M.\nYavrumyan, Arife Betül Yenice, Olcay Taner Yıldız,\nZhuoran Yu, Arlisa Yuliawati, Zdenˇek Žabokrtský,\nShorouq Zahra, Amir Zeldes, He Zhou, Hanzhi Zhu,\nAnna Zhuravleva, and Rayan Ziane. 2021. Universal\ndependencies 2.9. LINDAT/CLARIAH-CZ digital\nlibrary at the Institute of Formal and Applied Linguis-\ntics (ÚFAL), Faculty of Mathematics and Physics,\nCharles University.\nAppendices\nA Treebanks\nTARGET LANG FAMILY SIZE\nAR-PADT Arabic Afro-Asiatic 7.6k\nEN-EWT English Indo-European 16.6k\nFI-TDT Finnish Uralic 15.1k\nGRC-PROIEL Ancient Greek Indo-European 17.1k\nHE-HTB Hebrew Afro-Asiatic 6.2k\nKO-GSD Korean Korean 6.3k\nRU-GSD Russian Indo-European 5k\nSV-Talbanken Swedish Indo-European 6.0k\nZH-GSD Chinese Sino-Tibetan 5.0k\nTable 2: Target Treebanksbased on Smith et al. (2018)\nwith language family ( FAMILY ) and total number of\nsentences (SIZE ).\nTable 2 lists the nine target treebanks based on\nthe set by Smith et al. (2018): AR-PADT (Haji ˇc\net al., 2009), EN-EWT (Silveira et al., 2014), FI-\n1304\nTDT (Pyysalo et al., 2015), GRC-PROIEL (Eck-\nhoff et al., 2018), HE-HTB (McDonald et al.,\n2013a), KO-GSD (Chun et al., 2018), RU-GSD\n(McDonald et al., 2013b), SV-Talbanken (McDon-\nald et al., 2013a), ZH-GSD (Shen et al., 2016). We\nuse these treebanks as provided in Universal Depen-\ndencies v2.9 (Zeman et al., 2021). DEPPROBE and\nBAP are trained on each target’s respective train-\ning split and are evaluated on the development split\nas this work aims to analyze general performance\npatterns instead of state-of-the-art performance.\nB Experiment Setup\nDEPPROBE is implemented in PyTorch v1.9.0\n(Paszke et al., 2019) and uses language models\nfrom the Transformers library v4.13.0 and the as-\nsociated Model Hub (Wolf et al., 2020). Following\nthe structural probe by Hewitt and Manning (2019),\neach token which is split by the LM encoder into\nmultiple subwords is mean-pooled. Similarly, we\nfollow the original hyperparameter settings and\nset the structural subspace dimensionality to b=\n128 and use embeddings from the middle layer of\neach LM (Hewitt and Manning, 2019; Tenney et al.,\n2019; Fayyaz et al., 2021). The structural loss is\ncomputed based on the absolute difference of the\nEuclidean distance between transformed word em-\nbeddings and the number of edges separating the\nwords in the gold tree (see Hewitt and Manning,\n2019 for details). The relational loss is computed\nusing cross entropy between the logits and gold\nhead-child relation. Optimization uses AdamW\n(Loshchilov and Hutter, 2018) with a learning rate\nof 10−3 which is reduced by a factor of 10 each\ntime the loss plateaus. Early stopping is applied\nafter three epochs without improvement and a max-\nimum of 30 total epochs. With the only trainable\nparameters being the matricesBand L, the model’s\nfootprint ranges between 51k and 190k parameters.\nBAP For the biaffine attention parser (Dozat and\nManning, 2017) we use the implementation in the\nMaChAmp framework v0.3 (van der Goot et al.,\n2021) with the default training schedule and hyper-\nparameters. The number of trainable parameters\ndepends on the LM encoder’s size and ranges be-\ntween 14M and 583M.\nAnalyses For our analyses in Sections 3 and 4 we\nfurther make use of numpy v1.21.0 (Harris et al.,\n2020), SciPy v1.7.0 (Virtanen et al., 2020) and\nMatplotlib v3.4.3 (Hunter, 2007).\nTraining Details Models are trained on an\nNVIDIA A100 GPU with 40GBs of VRAM and\nan AMD Epyc 7662 CPU. BAP requires around 1\nh (±30 min). DEPPROBE can be trained in around\n15 min (±5 min) with the embedding forward op-\neration being most computationally expensive. The\nmodels use batches of size 32 and are initialized\nusing the random seeds 692, 710 and 932.\nReproducibility In order to ensure reproducibil-\nity and comparability with future work, we re-\nlease our code and token-level predictions at\nhttps://personads.me/x/naacl-2022-code.\nC Detailed Results\nTables 3–11 list exact LAS and standard deviations\nfor each experiment in Section 3’s Figure 2 in ad-\ndition to the HuggingFace Model Hub IDs of the\nLMs used in each of the 46 setups as well as their\nnumber of layers, embedding dimensionality dand\ntotal number of parameters. In addition, Figure\n5 shows UUAS for all setups, equivalent to only\nprobing structurally (Hewitt and Manning, 2019)\nfor unlabeled, undirected dependency trees.\n30 40 50 60 70 80\nDepProbe (UUAS)\n75\n80\n85\n90\n95BAP (LAS)\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nmBERT\nXLM-R\nRemBERT\n*BERT\n*RoBERTA\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAR\nEN\nFI\nGRC\nHE\nKO\nRU\nSV\nZH\nFigure 5: UUAS ofDEPPROBE in relation toBAP\nacross nine language targets (dev) using language-\nspecific and multilingual LM encoders of different ar-\nchitecture types.\n1305\nMODELS SOURCE LAYERS EMBd PARAMS BAP D EPPROBE\nbert-base-multilingual-casedDevlin et al. (2019) 12 768 178M 83.5 ±0.2 54.8±0.6\nxlm-roberta-base Conneau et al. (2020) 12 768 278M 85.2 ±0.1 57.2±0.1\ngoogle/rembert Chung et al. (2021) 32 1152 576M 85.4 ±0.2 20.7±0.1\naubmindlab/bert-base-arabertv02Antoun et al. (2020) 12 768 135M 85.8 ±0.1 59.0±0.1\nasafaya/bert-base-arabic Safaya et al. (2020) 12 768 111M 84.9 ±0.1 57.0±0.2\nTable 3: LAS on AR-PADT (Dev)using BAP and D EPPROBE with different LMs (±standard deviation).\nMODELS SOURCE LAYERS EMBd PARAMS BAP D EPPROBE\nbert-base-multilingual-casedDevlin et al. (2019) 12 768 178M 90.0 ±0.1 64.5±0.3\nxlm-roberta-base Conneau et al. (2020) 12 768 278M 91.7 ±0.2 64.8±0.1\ngoogle/rembert Chung et al. (2021) 32 1152 576M 92.2 ±0.0 41.6±0.3\nbert-base-uncased Devlin et al. (2019) 12 768 109M 91.2 ±0.1 63.4±0.3\nroberta-large Liu et al. (2019) 24 1024 355M 92.3 ±0.2 59.9±0.2\nTable 4: LAS on EN-EWT (Dev)using BAP and D EPPROBE with different LMs (±standard deviation).\nMODELS SOURCE LAYERSEMBd PARAMSBAP D EPPROBE\nbert-base-multilingual-casedDevlin et al. (2019) 12 768 178M 89.1 ±0.2 54.5±0.4\nxlm-roberta-base Conneau et al. (2020) 12 768 278M 92.4 ±0.1 62.4±0.2\ngoogle/rembert Chung et al. (2021) 32 1152 576M 93.1 ±0.1 30.8±0.1\nTurkuNLP/bert-base-finnish-uncased-v1Virtanen et al. (2019) 12 768 125M 93.4 ±0.1 68.9±0.3\nTurkuNLP/bert-base-finnish-cased-v1Virtanen et al. (2019) 12 768 125M 93.4 ±0.1 67.5±0.4\nTable 5: LAS on FI-TDT (Dev)using BAP and D EPPROBE with different LMs (±standard deviation).\nMODELS SOURCE LAYERSEMBd PARAMSBAP D EPPROBE\nbert-base-multilingual-casedDevlin et al. (2019) 12 768 178M 73.1 ±0.1 41.6±0.5\nxlm-roberta-base Conneau et al. (2020) 12 768 278M 85.0 ±0.2 51.1±0.2\ngoogle/rembert Chung et al. (2021) 32 1152 576M 87.7 ±0.1 15.3±0.1\npranaydeeps/Ancient-Greek-BERTSingh et al. (2021) 12 768 113M 87.3 ±0.1 60.0±0.0\nnlpaueb/bert-base-greek-uncased-v1Koutsikakis et al. (2020) 12 768 113M 84.6 ±0.3 53.9±0.1\nTable 6: LAS on GRC-PROIEL (Dev)using BAP and D EPPROBE with different LMs (±standard deviation).\nMODELS SOURCE LAYERS EMBd PARAMS BAP D EPPROBE\nbert-base-multilingual-casedDevlin et al. (2019) 12 768 178M 86.7 ±0.2 60.2±0.6\nxlm-roberta-base Conneau et al. (2020) 12 768 278M 88.8 ±0.1 59.2±0.3\ngoogle/rembert Chung et al. (2021) 32 1152 576M 90.5 ±0.1 11.6±0.4\nonlplab/alephbert-base Seker et al. (2021) 12 768 126M 89.6 ±0.1 61.4±0.2\nTable 7: LAS on HE-HTB (Dev)using BAP and D EPPROBE with different LMs (±standard deviation).\nMODELS SOURCE LAYERS EMBd PARAMS BAP D EPPROBE\nbert-base-multilingual-casedDevlin et al. (2019) 12 768 178M 83.8 ±0.2 46.6±0.2\nxlm-roberta-base Conneau et al. (2020) 12 768 278M 86.1 ±0.1 49.4±0.3\ngoogle/rembert Chung et al. (2021) 32 1152 576M 86.1 ±0.2 15.9±0.3\nklue/bert-base Park et al. (2021) 12 768 111M 86.8 ±0.0 51.0±0.1\nklue/roberta-large Park et al. (2021) 24 1024 337M 88.1 ±0.3 48.8±0.5\nkykim/bert-kor-base Kim (2020) 12 768 118M 86.8 ±0.1 46.9±0.4\nTable 8: LAS on KO-GSD (Dev)using BAP and D EPPROBE with different LMs (±standard deviation).\n1306\nMODELS SOURCE LAYERS EMBd PARAMS BAP D EPPROBE\nbert-base-multilingual-casedDevlin et al. (2019) 12 768 178M 89.1 ±0.1 60.7±0.1\nxlm-roberta-base Conneau et al. (2020) 12 768 278M 90.0 ±0.2 59.9±1.1\ngoogle/rembert Chung et al. (2021) 32 1152 576M 90.8 ±0.0 26.0±0.2\ncointegrated/rubert-tiny Dale (2021) 3 312 11M 76.7 ±0.1 41.5±0.6\nsberbank-ai/ruRoberta-largeSber Devices (2021) 24 1024 355M 90.3 ±0.3 63.2±0.4\nblinoff/roberta-base-russian-v0Blinov (2021) 12 768 124M 75.8 ±0.0 15.6±0.2\nTable 9: LAS on RU-GSD (Dev)using BAP and D EPPROBE with different LMs (±standard deviation).\nMODELS SOURCE LAYERS EMBd PARAMS BAP D EPPROBE\nbert-base-multilingual-casedDevlin et al. (2019) 12 768 178M 87.5 ±0.1 55.5±0.2\nxlm-roberta-base Conneau et al. (2020) 12 768 278M 90.2 ±0.1 59.1±0.2\ngoogle/rembert Chung et al. (2021) 32 1152 576M 91.3 ±0.3 31.7±0.3\nKB/bert-base-swedish-casedMalmsten et al. (2020) 12 768 125M 90.8 ±0.1 61.7±0.2\nTable 10: LAS on SV-Talbanken (Dev)using BAP and D EPPROBE with different LMs (±standard deviation).\nMODELS SOURCE LAYERS EMBd PARAMS BAP D EPPROBE\nbert-base-multilingual-casedDevlin et al. (2019) 12 768 178M 84.6 ±0.4 49.1±0.4\nxlm-roberta-base Conneau et al. (2020) 12 768 278M 85.5 ±0.3 30.3±0.1\ngoogle/rembert Chung et al. (2021) 32 1152 576M 85.3 ±0.2 5.2±0.1\nbert-base-chinese Devlin et al. (2019) 12 768 102M 85.8 ±0.1 46.4±0.1\nhfl/chinese-bert-wwm-extCui et al. (2021) 12 768 102M 86.0 ±0.3 45.8±0.3\nhfl/chinese-roberta-wwm-extCui et al. (2021) 12 768 102M 85.9 ±0.3 47.7±0.4\nTable 11: LAS on ZH-GSD (Dev)using BAP and D EPPROBE with different LMs (±standard deviation).\n1307"
}