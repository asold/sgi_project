{
  "title": "Fine-Tuning Large Language Models for Ontology Engineering: A Comparative Analysis of GPT-4 and Mistral",
  "url": "https://openalex.org/W4407704384",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5099293417",
      "name": "Dimitrios Doumanas",
      "affiliations": [
        "University of the Aegean"
      ]
    },
    {
      "id": "https://openalex.org/A4285914892",
      "name": "Andreas Soularidis",
      "affiliations": [
        "University of the Aegean"
      ]
    },
    {
      "id": "https://openalex.org/A1979061195",
      "name": "Dimitris Spiliotopoulos",
      "affiliations": [
        "University of Peloponnese"
      ]
    },
    {
      "id": "https://openalex.org/A308868717",
      "name": "Costas Vassilakis",
      "affiliations": [
        "University of Peloponnese"
      ]
    },
    {
      "id": "https://openalex.org/A2075035682",
      "name": "Konstantinos Kotis",
      "affiliations": [
        "University of the Aegean"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4406870075",
    "https://openalex.org/W4399876390",
    "https://openalex.org/W4397029707",
    "https://openalex.org/W4396790435",
    "https://openalex.org/W4404782801",
    "https://openalex.org/W4404869895",
    "https://openalex.org/W4391697658",
    "https://openalex.org/W2794347841",
    "https://openalex.org/W4404261122",
    "https://openalex.org/W2018504982",
    "https://openalex.org/W4297678719",
    "https://openalex.org/W6632488602"
  ],
  "abstract": "Ontology engineering (OE) plays a critical role in modeling and managing structured knowledge across various domains. This study examines the performance of fine-tuned large language models (LLMs), specifically GPT-4 and Mistral 7B, in efficiently automating OE tasks. Foundational OE textbooks are used as the basis for dataset creation and for feeding the LLMs. The methodology involved segmenting texts into manageable chapters, generating question–answer pairs, and translating visual elements into description logic to curate fine-tuned datasets in JSONL format. This research aims to enhance the models’ abilities to generate domain-specific ontologies, with hypotheses asserting that fine-tuned LLMs would outperform base models, and that domain-specific datasets would significantly improve their performance. Comparative experiments revealed that GPT-4 demonstrated superior accuracy and adherence to ontology syntax, albeit with higher computational costs. Conversely, Mistral 7B excelled in speed and cost efficiency but struggled with domain-specific tasks, often generating outputs that lacked syntactical precision and relevance. The presented results highlight the necessity of integrating domain-specific datasets to improve contextual understanding and practical utility in specialized applications, such as Search and Rescue (SAR) missions in wildfire incidents. Both models, despite their limitations, exhibited potential in understanding OE principles. However, their performance underscored the importance of aligning training data with domain-specific knowledge to emulate human expertise effectively. This study, based on and extending our previous work on the topic, concludes that fine-tuned LLMs with targeted datasets enhance their utility in OE, offering insights into improving future models for domain-specific applications. The findings advocate further exploration of hybrid solutions to balance accuracy and efficiency.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5643449425697327
    },
    {
      "name": "Ontology",
      "score": 0.41627609729766846
    },
    {
      "name": "Philosophy",
      "score": 0.06894585490226746
    },
    {
      "name": "Epistemology",
      "score": 0.04059708118438721
    }
  ]
}