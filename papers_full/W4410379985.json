{
  "title": "On bullshit, large language models, and the need to curb your enthusiasm",
  "url": "https://openalex.org/W4410379985",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2865057863",
      "name": "Daniel W. Tigard",
      "affiliations": [
        "Northeastern University",
        "University of San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2865057863",
      "name": "Daniel W. Tigard",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4248611638",
    "https://openalex.org/W4221052430",
    "https://openalex.org/W2786522954",
    "https://openalex.org/W4234116076",
    "https://openalex.org/W2977589706",
    "https://openalex.org/W2996884718",
    "https://openalex.org/W2021641437",
    "https://openalex.org/W4403144637",
    "https://openalex.org/W2916945592",
    "https://openalex.org/W4229621810",
    "https://openalex.org/W4212784659",
    "https://openalex.org/W4380319827",
    "https://openalex.org/W4399465031",
    "https://openalex.org/W4385576721",
    "https://openalex.org/W1965082696",
    "https://openalex.org/W7036054368",
    "https://openalex.org/W4388793704",
    "https://openalex.org/W2800072875",
    "https://openalex.org/W4210739840",
    "https://openalex.org/W4318240966",
    "https://openalex.org/W4384303327",
    "https://openalex.org/W4401142165",
    "https://openalex.org/W3013207429",
    "https://openalex.org/W3173585017",
    "https://openalex.org/W4220882633",
    "https://openalex.org/W6825850876",
    "https://openalex.org/W2076774342",
    "https://openalex.org/W4308391526",
    "https://openalex.org/W2106175837",
    "https://openalex.org/W3173032389",
    "https://openalex.org/W3195810573",
    "https://openalex.org/W4312035934",
    "https://openalex.org/W4367671325",
    "https://openalex.org/W2792732529",
    "https://openalex.org/W2973907375",
    "https://openalex.org/W4405759969",
    "https://openalex.org/W2588627057",
    "https://openalex.org/W4225479391",
    "https://openalex.org/W2955653438"
  ],
  "abstract": "Abstract Amidst all the hype around artificial intelligence (AI), particularly regarding large language models (LLMs), generative AI and chatbots like ChatGPT, a surge of headlines is instilling caution and even explicitly calling “bullshit” on such technologies. Should we follow suit? What exactly does it mean to call bullshit on an AI program? When is doing so a good idea, and when might it not be? With this paper, I aim to provide a brief guide on how to call bullshit on ChatGPT and related systems. In short, one must understand the basic nature of LLMs, how they function and what they produce, and one must recognize bullshit. I appeal to the prominent work of the late Harry Frankfurt and suggest that recent accounts jump too quickly to the conclusion that LLMs are bullshitting. In doing so, I offer a more level-headed approach to calling bullshit, and accordingly, a way of navigating some of the recent critiques of generative AI systems.",
  "full_text": "ORIGINAL RESEARCH\nAI and Ethics (2025) 5:4863–4873\nhttps://doi.org/10.1007/s43681-025-00743-3\ngenerator” [31]. Similarly, Michael Townsen Hicks, James \nHumphries and Joe Slater argue that ChatGPT is a “bullshit \nmachine” [ 19]. S. Shyam Sundar and Mengqi Liao call \n“bullshit” upon learning that ChatGPT misunderstands \nSundar’s own theory [ 43]. And even before the release of \nChatGPT, Blaise Aguera y Arcas of Google reported that \nLaMDA, Google’s LLM chatbot, “is indeed…bullshitting” \n[55].\nConsidering the escalation of headlines and the exper -\ntise of those who call bullshit (not to mention the open let -\nter warning of AI’s existential threat, signed by hundreds \nof industry leaders in May 2023 1), it might seem that we \nshould follow suit. Perhaps it would be a good idea for all \nof us—whether academics studying AI, students seeking \nwriting support, or professionals toying with its potential \nto offload work—to call bullshit on ChatGPT and related \nsystems. But what exactly does it mean to ‘call bullshit’ on \nan AI program? When is doing so a good idea, and when \nmight it not be? With this paper, I provide a brief guide, \nnamely two key steps, on how to call bullshit on ChatGPT \nand other generative AI systems. First, one must understand \nthe basic nature of LLMs, how they function and what they \nproduce. To help take this first step, in Sect. 2, I will appeal \n1 For the Statement on AI Risk and list of current signatories, see the \nCenter for AI Safety:  h t t p s :   /  / w w  w . s  a f   e .  a  i / s t  a t  e m e  n t  - o n  - a i - r i s k. Relat-\nedly, Kevin Roose [34] reports on the threat of AI.\n1 Introduction\nAmidst all the hype around artificial intelligence (AI), par -\nticularly regarding large language models (LLMs), genera -\ntive AI and chatbots like ChatGPT, a surge of headlines is \ninstilling suspicion and serving as a cautionary guide against \nthe deceptive potential of such technologies. For example, \nrecent reports have revealed the folly of a lawyer who relied \nupon ChatGPT and submitted a brief with fabricated court \ndecisions [52]. In Texas, a professor reportedly failed stu -\ndents after misusing ChatGPT in an attempt to identify pla-\ngiarized essays [25]. More recently, a wellness chatbot was \nshut down after giving users harmful advice about weight \nloss [27]. “The bottom line,” as technology reporter Cade \nMetz states, “Don’t believe everything a chatbot tells you” \n[29].\nElsewhere, the statements are even more blunt. In an AI-\nfocused blog, computer scientists Arvind Narayanan and \nSayash Kapoor write frankly that “ChatGPT is a bullshit \n \r Daniel W. Tigard\ndtigard@sandiego.edu\n1 University of San Diego, San Diego, USA\n2 Institute for Experiential AI, Northeastern University, \nBoston, USA\nAbstract\nAmidst all the hype around artificial intelligence (AI), particularly regarding large language models (LLMs), generative AI \nand chatbots like ChatGPT, a surge of headlines is instilling caution and even explicitly calling “bullshit” on such tech -\nnologies. Should we follow suit? What exactly does it mean to call bullshit on an AI program? When is doing so a good \nidea, and when might it not be? With this paper, I aim to provide a brief guide on how to call bullshit on ChatGPT and \nrelated systems. In short, one must understand the basic nature of LLMs, how they function and what they produce, and \none must recognize bullshit. I appeal to the prominent work of the late Harry Frankfurt and suggest that recent accounts \njump too quickly to the conclusion that LLMs are bullshitting. In doing so, I offer a more level-headed approach to calling \nbullshit, and accordingly, a way of navigating some of the recent critiques of generative AI systems.\nKeywords Artificial intelligence · Large language models · Frankfurt · Bullshit · Truth · Human-AI interaction\nReceived: 24 January 2025 / Accepted: 21 April 2025 / Published online: 14 May 2025\n© The Author(s) 2025\nOn bullshit, large language models, and the need to curb your \nenthusiasm\nDaniel W. Tigard1,2\n1 3\nAI and Ethics (2025) 5:4863–4873\nto the work of cognitive roboticist and senior researcher at \nDeepMind Murray Shanahan. Second, one must fully rec -\nognize bullshit. To help take this second step, in Sect. 3, I \nwill discuss the popularized theory developed by the late \nHarry Frankfurt. To be sure, previous efforts to call bullshit \non LLMs, such as the blunt statements above, have likewise \ninvoked Frankfurt’s classic essay. But as I suggest, many \nof these recent accounts jump too quickly to the conclusion \nthat LLMs are bullshitting. By contrast, the account devel -\noped here puts forward a more nuanced view. If the func -\ntioning and outputs of LLMs fit reasonably well into our \nunderstanding of bullshit, then it may be fair to call bullshit \non ChatGPT and similar programs. But if our target does not \nfit the theory, as it will often appear, then we should exer -\ncise restraint against the temptation to colorfully repudiate a \nfashionable technology.\nThe next question, one that has received less attention in \nthe nascent literature on bullshit and LLMs, concerns why \nwe should or should not call bullshit on generative AI. By \nway of introduction, I want to offer a preemptive qualifica -\ntion, which I will expand upon below. That is, the follow -\ning discussion might help to make sense of AI-generated \ncontent as bullshit; however, very often, AI systems should \nnot be understood as bullshitters. Following the theoretical \ninsights drawn from Frankfurt, in Sect. 4, I focus on highly \npractical considerations, namely cases where the outputs \nof LLMs have some kind of meaning and provide tangible \nbenefits.2 Taken together, my account provides theoretically \ngrounded reasons, as well as independent practical consid -\nerations, which both point toward exercising some degree \nof restraint against calling bullshit on AI. Although a popu-\nlarized and rich theory like Frankfurt’s could allow us to \njustify calling bullshit, we might need to curb our enthusi -\nasm. What I offer here is, in a sense, a more level-headed \napproach, a way of navigating—and perhaps only cau -\ntiously supporting—some of the recent critiques of genera -\ntive AI systems. In Sect. 5, I consider some objections, and \nin Sect. 6, I conclude by highlighting that while there may \nbe reasons to thwart some human-AI interactions, we also \nsee theoretical and practical reasons for remaining open-\nminded and, at times, not calling bullshit.\n2 The functioning and outputs of LLMs\nAs prefaced, for this first step, one must understand the \nfunctioning and the outputs of LLMs, at least on a rudi -\nmentary level. A recent article by Shanahan [ 39], “Talking \nAbout Large Language Models”, provides a highly fitting \n2 Indeed, while it may often be fair to call bullshit on ChatGPT, I find \nit important to recognize—and have previously defended [ 48]—the \npotential positive value of chatbots, at least in some settings.\nentry point for this task. To begin my appeal to Shanahan’s \nwork, it is worth noting that his article aims to mitigate \nthe growing trend of anthropomorphizing technological \nentities.3 While it is sometimes quite natural to attribute \nhuman-like characteristics, behaviors, or mental states to \nnon-human entities, it is often important to retain an active \nawareness and critical perspective on these processes. \nIndeed, anthropomorphism toward technology may occur \nquite subtly and inadvertently, and it may lead the public to \nadopt mistaken perceptions of AI and its capacities [ 35]. In \nShanahan’s words, the trend toward anthropomorphizing AI \nis “amplified by the natural tendency to use philosophically \nloaded terms, such as ‘knows’, ‘believes’, and ‘thinks’, \nwhen describing these systems” [39, p. 1]. As Shanahan and \nothers advise, we should be wary of our practices and per -\nceptions that frame LLMs as human-like, since we might \nallow ourselves and others to be fooled into thinking we are \ninteracting with a fellow human, when in fact AI systems \nare very different from humans.4\nTo some extent, it seems understandable to fall into the \nperception of AI as human-like. After all, much of the recent \nprogress in AI is owed to the fact that algorithms are getting \nbetter at replicating some distinctly human capacities, such \nas the ability to predict future events. Consider, for example, \nalgorithmic prediction of activities of daily living in homes \n[54] or algorithms’ success in assisting with the detection of \nindicators of depression among youth [ 8].5 In terms of lan -\nguage generation, successful prediction is seen in models, \nwhich have been trained with enormous quantities of text, \nconsistently predicting what comes next in a natural conver-\nsation. As Shanahan [39, p. 2] explains, “LLMs are genera-\ntive mathematical models of the statistical distribution of \ntokens in the vast public corpus of human-generated text, \nwhere the tokens in question include words, parts of words, \nor individual characters including punctuation marks.” The \nmodel’s functioning, then, can be described as a continu -\nous series of predictions regarding which individual token \nis likely to come next. But here is where we begin to see \nsome crucial differences between how LLMs function and \nhow we function when generating language. That is, when \nwe speak, clearly we are doing something more, or at least \nsomething other than merely calculating the statistics of \neach piece of our sentences. Accordingly, while it may be \n3 The concern for anthropomorphizing technology, particularly AI \nand robotics, has received notable attention in recent years. See e.g., \nDuffy [12], Darling [11], Watson [51], and Nyholm [33].\n4 For related concerns surrounding deception and technology, see \nBryson [6], van Wynsberghe and Robbins [ 50], Danaher [ 10], and \nNatale [32].\n5 Despite such advancements in machine learning, some researchers \nremain skeptical of the idea that, given enough data, everything is \npredictable. For discussion on the limits of prediction, see Kapoor \nand Narayanan [20].\n1 3\n4864\nAI and Ethics (2025) 5:4863–4873\nunderstandable to see an LLM as human-like, their func -\ntioning is unlike ours in important respects.\nTurning to the outputs, we can again notice important \ndifferences between human- and AI-generated language. \nUnlike AI systems, when we generate language, we mean \nsomething; we communicate thoughts, beliefs, perceptions \nof the world, and so on. 6 For LLMs, because their func -\ntioning is a matter of mathematical calculations, what they \nproduce is an amalgamation of the statistical results. LLMs \ndo not mean anything, at least in the current state of devel -\nopment, and the statistical compilation of linguistic symbols \ncannot be said to represent thoughts or beliefs, and so forth. \nShanahan illustrates the present point with three examples:\nSuppose we give an LLM the prompt ‘The first person \nto walk on the Moon was’, and suppose it responds \nwith ‘Neil Armstrong’… we might give an LLM the \nprompt ‘Twinkle twinkle’, to which it will most likely \nrespond ‘little star’… prompt it with the words ‘After \nthe ring was destroyed, Frodo Baggins returned to’, to \nwhich it responds ‘the Shire’. [39, p. 2]\nIn all three cases, Shanahan explains, we are seeing some -\nthing very similar happening. For each prompt, the model \nproduces the most likely words that follow from the given \ninputs. In Shanahan’s words [39, p. 2], it is as if we are ask-\ning the LLM: “Given the statistical distribution of words in \nthe public corpus, what words are most likely to follow” the \ngiven sequence? As its output in each case, the LLM pro -\nduces the most likely words that follow the given prompts, \nand we then interpret these symbols in terms of truth or \naccuracy.\nTo the LLM, in the three cases, we are not really ask -\ning about the first person on the Moon, or a nursery rhyme, \nor what happened to Frodo—we are not asking content-\nrelated questions. We are simply requesting it to complete \nthe sequence, and to an LLM, there may well be identifiable \nsequences and corresponding sets of statistically likely con-\ntinuations of those sequences. To us, however, when accu -\nrately completed, each one of the three sequences is readily \ndistinguishable from the others. The first means something \nand can be said to be true in reality; the third means some -\nthing and can be said to be true in a fictional world; and \nthe second, while it might have a sort of artistic meaning, \nit cannot properly be said to be true. Importantly, “each of \n6 Although it is tempting, this is not the place for me to weigh in \non whether AI could ever produce thought or understanding. Instead, \nI refer interested readers to Searle’s famous Chinese Room thought \nexperiment and the wealth of related literature. Also, it should be \nnoted that nothing substantive turns on this point here. For present \npurposes, the claim to be made is simply that, for the time being, lan-\nguage generation looks very different between humans and AI (even \nif the latter was thought to produce some kind of understanding).\nthese examples presents a different sort of relationship to \ntruth,” Shanahan says, but “these distinctions are invisible \nat the level of what the LLM itself…actually does” (ibid.). \nAs a result, here is where many researchers, commentators, \nand perhaps some everyday users, are often inclined to call \nbullshit on LLMs.\n3 On bullshit and on bullshitters\nAmong the most fundamental and most propagated com -\nponents of Frankfurt’s theory of bullshit is the idea that \nbullshit is produced without a concern for truth. Much like \nthe notion of “hot air”, bullshit is meaningless communica-\ntion, “emptied of all informative content” [ 15, p. 127]. In a \ncentral passage, Frankfurt states, it is “this lack of connec -\ntion to a concern with truth—this indifference to how things \nreally are—that I regard as of the essence of bullshit” [ 15, \np. 125]. To make clear this essential characteristic, a great \ndeal of Frankfurt’s writing “On Bullshit” is devoted to ana-\nlyzing the distinction between a lie and bullshit. The former \nhas meaning and is produced “with a sharp focus,” he says. \nTelling a lie is “designed to insert a particular falsehood at a \nspecific point in a set or system of beliefs, in order to avoid \nthe consequences of having that point occupied by the truth” \n[15, p. 129]. Because of the need to carefully craft the com-\nmunication at stake, to tell a lie, a person is “inescapably \nconcerned with truth-values… he must think he knows what \nis true,” for otherwise he cannot reasonably hope to lead his \naudience away from the truth [ 15, p. 130]. By contrast, in \nbullshitting, a person “ignores these demands altogether. He \ndoes not reject the authority of the truth, as the liar does, and \noppose himself to it. He pays no attention to it at all” and for \nthese reasons, as Frankfurt eloquently states, “bullshit is a \ngreater enemy of the truth than lies are” [15, p. 132].\nConsidering this framing, it is not hard to see why many \ncontemporary writers liken the features of LLMs to bullshit. \nThose features, such as LLMs’ functioning and outputs, \nclearly lack a connection to a concern with truth, which ful-\nfills Frankfurt’s essential characteristic of bullshit. Indeed, \nthe very idea of being concerned  seems to suggest that \none cares for something [cf. 40]. Undoubtedly, LLMs do \nnot care in any way, and so, do not care for truth or to lead \nanyone away from it. As a mathematical model designed to \ncalculate the statistical distributions of letters and words, the \nmodel itself, we can say, pays no attention to the truth. Fur-\nther, as Shanahan helps to make clear, not only is the distinc-\ntion between truth and falsity invisible to the functioning of \nLLMs, but similarly, the distinction between sequences that \nconvey any truth-value and those that do not is, so to speak, \nignored by LLMs. After all, to an LLM, “twinkle twinkle \nlittle star” is just as true as “The first person to walk on the \n1 3\n4865\nAI and Ethics (2025) 5:4863–4873\ncommunication, bullshitters nonetheless have distinct pur -\nposes. In a particularly telling passage, Frankfurt indicates a \ncharacteristic intention of bullshitters:\nHis eye is not on the facts at all… except insofar as \nthey may be pertinent to his interest in getting away \nwith what he says. He does not care whether the things \nhe says describe reality correctly. He just picks them \nout, or makes them up, to suit his purpose .” [15, p. \n131, italics added]\nIt seems clear, then, that a more nuanced application of \nFrankfurt’s theory shows LLMs to be unlike bullshitters. \nBullshitters have distinct interests and purposes, even if \nwhat they produce in their attempt to fulfill their interests \ndoes not serve the purpose of meaningful communication. \nConsider that, early on, Frankfurt describes advertising \nand politics as presenting us with paradigmatic instances \nof bullshit. In these domains, we see techniques of “pub -\nlic opinion polling, of psychological testing, and so forth” \nwhich all have as a common baseline aim the goal of “trying \nto get away with something” [ 15, p. 122]. Again, the com -\nparison with lying is informative, namely the idea that liars \naim to misrepresent either the state of affairs in question or \nthe beliefs of the speaker. Bullshitters too have aims. But \nwhat the bullshitter “does necessarily attempt to deceive \nus about is his enterprise,” Frankfurt says. The bullshitter \nhides “that the truth-values of his statements are of no cen -\ntral interest to him” [15, p. 130].\nHere again, those who invoke Frankfurt in order to frame \nLLMs as bullshitters might retort that the theory does fit the \ntechnology, since, just like Frankfurtian bullshitters, LLMs \nhave no interest in truth-values. And granted, the outputs of \nLLMs may well resemble, and often be indistinguishable \nfrom, bullshit as a product. But by now, I hope, we can see \nthat there is a perhaps subtle but important distinction when \nour inquiry properly accounts for the producers of bullshit. \nSpecifically, we can grant that LLMs in their functioning \nhave no interest in truth-values—or in anything else for that \nmatter—and that their outputs reflect a worrisome indiffer -\nence to how things really are. Bullshitters, however, have \ninterests. They are trying to hide something about them -\nselves, namely their indifference. And while LLMs might in \neffect hide their indifference, surely they were not trying to \ndo so. While they might in effect get away with something, \nlike fooling their users, surely they do not have an inter-\nest in getting away with something. 7 Thus, when looking \n7 It might be thought that, if not the LLM itself, perhaps the company \nbehind the LLM—say, Open AI—has an interest and is indeed try -\ning to get away with something. In this way, the LLM—in this case \nChatGPT—could be framed as a sort of puppet and perhaps thought \nof as an “indirect” bullshitter. That is, ChatGPT might be bullshitting \nMoon was Neil Armstrong.” In these ways, it can be said \nthat the outputs of LLMs fit Frankfurt’s theory and, accord-\ningly, it seems fair to call bullshit. As tech writer Gary Mar-\ncus writes of LaMDA, “literally everything that the system \nsays is bullshit” [24, italics in original].\nHowever, there is more to Frankfurt’s theory than the \nidea of bullshit as communication which lacks a connec -\ntion to a concern for truth. Although the other components \nin his analysis are not as widely propagated, they are no \ndoubt fundamental to fully understanding his theory. But \nwhen properly taken into account, the full theory does not \nmatch up so neatly with some characterizations of LLMs. \nWhat I have in mind here are Frankfurt’s remarks concern -\ning the character and the intentions of those who tend to \nproduce bullshit. Consider Frankfurt’s example when dis -\ncussing Max Black’s work, namely a “Fourth of July ora -\ntor, who goes on bombastically about ‘our great and blessed \ncountry, whose Founding Fathers under divine guidance…” \n[15, p. 121]. To be clear, Black’s [4] account concerned the \nnature of humbug, and Frankfurt’s appeal to it is an effort to \nfind potential relations to bullshit. Frankfurt indeed notes \nthat orators producing humbug are not lying and that they \ndo not care what their audiences think about the content of \nthe communication. And these points are consistent with the \nresulting characterization of bullshit as lacking connection \nto a concern for truth. Crucially, however, Frankfurt then \npoints out that “the orator intends these statements to con -\nvey a certain impression of himself” and that “What he cares \nabout is what people think of him” (ibid., italics in original). \nHere, even at this early point in Frankfurt’s work, we start to \nsee how LLMs themselves, apart from their outputs, might \nnot be accurately portrayed in terms of bullshit.\nGranted, Frankfurt parts ways with Black’s account and \nthe relationship between humbug and bullshit. Nonethe -\nless, his analysis soon returns to an unmistakable emphasis \non the character and intentions of those who bullshit. The \nbullshitter, Frankfurt says, “is faking things” [ 15, p. 129]. \nIn this way, bullshitting is closer to bluffing than to lying, \nand bluffing “is more especially a matter not of falsity but \nof fakery” [ 15, p. 128]. When someone fakes something, \nFrankfurt explains, the product may be indistinguishable \nfrom the real thing, and thus, in order to understand fak -\nery—and its counterpart, bullshit—we must attend not \nonly to what something is like, but to “how it was made” \n(ibid.). In this way, if we wish to invoke Frankfurt to help us \nunderstand LLMs, we must focus on the system itself and \nnot only on its outputs, for indeed the outputs may often \nbe indistinguishable from human-generated language. How-\never, turning our attention to LLMs themselves, and main -\ntaining a fine-tuned grasp of the theory, reveals a crucial \nmismatch. On Frankfurt’s account, while the production of \nbullshit is indeed disconnected from purposeful, truth-apt \n1 3\n4866\nAI and Ethics (2025) 5:4863–4873\nmy discussion, the argument I want to put forward can be \nsummarized as follows.\n(1) If a conversational interaction with an AI chatbot \nposes some degree of tangible benefit to the human \nuser without risking harm to others, all things consid -\nered, we should refrain from thwarting the human–AI \ninteraction.\n(2) Conversational interactions with AI chatbots can, at \nleast sometimes, pose some degree of tangible benefit \nto human users without risking harm to others.\n(3) Thus, at least sometimes, we should refrain from thwart-\ning the human–AI interaction.\nHere I am assuming that calling bullshit—namely on an \nAI chatbot, but directed at a human user—would count as \nthwarting the conversational interaction. In other words, the \nconsequent of premise 1 and the conclusion can be thought \nto entail the claim that we should refrain from calling \nbullshit on the AI chatbot . Framing the argument in terms \nof “thwarting” human–AI interactions, rather than “calling \nbullshit,” I find, better captures and accounts for a range \nof cases where we might suspect someone of falling prey \nto bullshit (believing in AI-produced content) but benefit -\nting from an interaction they see as somehow meaningful \n(namely an exchange with an utterly disinterested system). \nThat is, following my discussion of Frankfurt, I want to \nconsider situations that seem to involve bullshit (communi-\ncation without concern for truth) but do not involve bullshit-\nters (orators with purposes and interests in getting away \nwith something).10 And while debate might ensue over the \nmanner in which someone calls bullshit or otherwise inter -\nvenes, or over the possibility of conversational interactions \ncontinuing undeterred amidst such interruptions, I will leave \nthese details aside and focus on what I take to be the weight-\nier notions at stake, premises 1 and 2.\nBefore turning to the ways in which conversational inter-\nactions with AI chatbots could pose benefits, consider first \nthe conditional claim stated in premise 1. Although objec -\ntions can clearly be raised, which I will explore below, the \nidea might initially come across as rather uncontroversial. \nAfter all, all I am claiming in this first premise is that if \nsome interaction benefits a human without risking harm \nto others, we should not put a stop to the interaction and \n10 Cases will surely arise, and likely already abound, where the deci -\nsion of a third party (the potential bullshit-accuser, so to speak) is not \nmerely whether or not to call bullshit. Rather, they may be more inter-\nested in how to help someone engaged with an AI chatbot– for exam -\nple, by educating or simply urging caution. I take it that aside from \ncalling bullshit, there are surely a range of additional processes and \ninterventions by which one could thwart a human–AI interaction. For \nvaluable comments and encouragement to make these clarifications, I \nthank an anonymous reviewer.\nbeyond the widely-cited thought that bullshit is a lack of \nconcern for truth, and when fully investigating Frankfurt’s \nideas on bullshit and on bullshitters , it appears that LLMs \ndo not accurately fit the theory.\n4 When not to call bullshit\nSo, when is calling bullshit on an LLM a good idea, and \nwhen might it not be? To summarize what I hope to have \nestablished so far, the outputs of LLMs can be character -\nized as bullshit on Frankfurt’s popularized theory. An LLM \nitself, however, is not a bullshitter, even if it is a producer \nof bullshit.8 As outlined above, a key component of Frank -\nfurt’s theory is overlooked when critiques of LLMs aim not \nso much at the outputs but at the system itself. ChatGPT \nand related programs are not bullshitters, since they have no \ninterests of their own; the models themselves (in their pres-\nent state, anyway) are not trying to hide or get away with \nsomething. Here we see at least one reason, albeit a theoreti-\ncally grounded reason, to not call bullshit on LLMs, namely \nbecause the theory often invoked in order to do so simply \ndoes not fit the target. Accordingly, while it might be easy \nto cite Frankfurt for these purposes, and doing so may seem \nto lend a degree of philosophical credence to one’s charges, \nthose who wish to characterize LLMs as bullshitters face the \ntask of specifying an alternative theory of bullshit.9\nAside from this theory-laden reason to not call bullshit \non LLMs, it seems to me that there may also be some strong \nindependent practical reasons to not do so. In this section, I \nwill explore reasons bearing practical significance, specifi -\ncally some considerations that pertain to the potential bene-\nfits to be achieved within human–AI interaction. To preface \non behalf of Open AI. While plausible in terms of extending Frank -\nfurt’s theory, this line of thought would presumably be committed to \nshowing that the primary orator, Open AI, is indifferent to truth-val -\nues and has an interest in hiding its indifference. Again, while plau -\nsible, showing that the company behind an LLM is itself the primary \nbullshitter is a task I must leave aside here. For discussion on this \npoint, I thank Nick Riggle.\n8 Hicks, Humphries, and Slater [ 19] offer a comparable argument. \nThey claim that ChatGPT is a soft bullshitter , in the sense that—\nassuming it has no intentions—bullshit is produced “without the \nintention to mislead” [ 19, p. 5]. Although somewhat consistent \nwith this important point, my account parts ways by recommending \nagainst any attribution of bullshit due to concerns over anthropomor-\nphization, as I explain below in response to possible objections. But \nalso, in the highly practical approach taken in the present section, I \nfind it useful to question when it may or may not  be appropriate to \ncall bullshit, or even “soft” bullshit, on AI.\n9 For a notable example of critique of Frankfurt’s theory, see Cohen \n[7]. For recent expansions of Frankfurt, specifically into the realm of \nargumentation, see Gascón [ 17], and into rhetoric and writing com -\nposition, see Kellogg [21].\n1 3\n4867\nAI and Ethics (2025) 5:4863–4873\nunderstanding of how to properly give credit to generative \nAI. In this way, where a user receives a tangible benefit \nfrom AI in terms of developing or improving their research \nagenda, even where no risk of harm is present, we (perhaps \nthe researcher’s institution, their funder, or interested col -\nleagues, among others) might think we should thwart the \nhuman–AI interaction, say, by not promoting or rewarding \nthe resulting product.\nAgain, it is tempting to push back on premise 1 in the way \nI have suggested. But it is also worth reflecting upon what \nthis line of thought appears to be committed to and what \nit would be forced to give up. In terms of the underlying \ncommitments of this position, it seems that something like \nacademic integrity, honesty, authenticity, and related values, \nmay be at the heart of the demand to utilize only those con-\ntributions which can be properly credited. Considering that \nthese sorts of values have a noticeably deontological tone, \nthe conundrum at stake here, for some, may boil down to \na fundamental clash between ethical standpoints: broadly \nabsolutist versus broadly consequentialist. Regarding the \nformer stance, it can be expected that some will be of the \nmind that tangible benefits to an AI user are not enough to \nground an obligation to refrain from thwarting the interac -\ntion. For some institutions, this sort of outlook could trans -\nlate to something like a prohibition on the use of generative \nAI, whether in terms of manuscript authorship, grant writ -\ning, or other creative activities. 13 Adherence to values like \nintegrity and authenticity may be of the utmost importance, \nand in order to preserve such values—at least in these sorts \nof cases—some may well see the need to thwart interactions \nwith generative AI systems.14\nNotice also what the absolutist position is committed to \ngiving up, namely the tangible benefits to the immediate \nuser and, depending upon the domain of application, any \ndownstream benefits to others. While values related to integ-\nrity and authenticity are understandably esteemed and likely \nuncompromisable in academic and creative endeavors, giv-\ning up all advancements resulting from contributions from \nAI would be a high price to pay. We only need to briefly \nconsider the developments due to AI in areas like medi -\ncine to see strong support for the consequential goods to \nbe achieved by human–AI interaction, even where we have \nnot yet settled upon standards for properly crediting AI. 15 \ninspired by AI, very generally speaking, to explore implications of the \nuse of AI.\n13 For related discussion concerning regulatory recommendations, \nsee, e.g. Frosio [16] and Hacker et al. [18].\n14 Similarly, we can imagine this same stance being adopted in terms \nof friendship or in clinical settings. Authenticity and related values, \nfor some, are more important than the goods to be achieved through \nhuman–AI interaction.\n15 A wealth of emerging literature is showing increasing support for \nthe benefits of medical AI, such as new methods for diagnosis [ 9] or \nthereby to the benefits being accrued. The idea would \nlikely appear straightforward enough in cases of human-\nto-human interaction: a heartening conversation between \nfriends, a patient’s consultation with a physician, and count-\nless other examples. To put the weight on cases of AI in \nacademic settings, an issue which many readers will find \nfamiliar, consider an exchange between two colleagues, in \nwhich ideas for an important research project undertaken by \none colleague are substantially improved and developed as \na result of input from the other colleague. For the sake of \nthe example, assume that both parties to the exchange are \nparticipating willingly. Assume also that, in the process of \ncarrying on with the interaction, neither one is neglecting \noverriding duties; hence my use of the all-things-considered \nclause. As we can imagine, it would be off-putting for a third \nperson to thwart the continuation of this fruitful interaction. \nIndeed, it seems safe to say in these sorts of cases that third \nparties should not thwart such interactions, particularly by \ncalling bullshit.\nHowever, once we apply the human-to-human analogy to \nthe premise at stake, it might seem that there are important \ndifferences between the contribution of a human interlocu -\ntor and that of an AI program. Among others, one difference \nthat could be highlighted, at least in the sort of case I have \nimagined, is that researchers can give credit—acknowledg-\nments, co-authorship, and so forth—where they have ben -\nefited from the input of their human colleagues. By contrast, \nit is currently unclear how one can or should give credit to \nan LLM, even where its contributions to an interaction were \nsimilarly fruitful as those of a human colleague. 11 But does \nthe current lack of clarity in this regard mean that it is not \nthe case that we should refrain from thwarting human–AI \ninteractions where such interactions pose tangible benefits \nand no risks of harm? Admittedly, it is tempting to affirm \nprecisely this point. Indeed, there are likely already cases \nof the sort I am imagining—research projects where one \nperson benefits from a conversational interaction with \nAI—where it is reasonable to think we should thwart, or \nperhaps should have  thwarted, the human–AI interaction. \nWhy exactly might this be the case? For one, it might be \nthought, again because norms surrounding AI authorship \nare presently unclear, that researchers should utilize only \nthose contributions which can be properly credited to their \nsources.12 And we simply do not yet have an established \n11 In this way, a great deal of research and development in the law, \nparticularly surrounding patents and intellectual property, is clearly \nneeded in order to accommodate the use of generative AI. For one \nset of proposals in this domain, see Abbott [ 1]. Additionally, for dis-\ncussion on authorship and copyrightability of AI-generated works, \nsee Samuelson [ 36], Bridy [ 5], and Lee [ 22, 23]. As Samuelson [ 37] \nwrites, “no consensus has emerged” on AI authorship.\n12 At this point I feel obligated to note that no assistance from an AI \nprogram was employed in authoring this manuscript, apart from being \n1 3\n4868\nAI and Ethics (2025) 5:4863–4873\nattention will be needed to work toward easing the variety \nof concerns, such as security of sensitive information, lack \nof regulation, and potential to reduce established care prac-\ntices and exacerbate health inequalities [ 14]. But progress \nin these areas is feasible, with some reason for optimism \nsuggested by the rise of frameworks for embedding ethics \n[28, 47, 53]. And while some authors worry, further, that AI \nlacks moral agency and will never be able to provide robust \nexplanations or authentic dialogue [ 38], conversational AI \ndoes not need full agential capacities in order to satisfy \ndemands of answerability [ 45, 46] or to successfully serve \ntherapeutic purposes [14, 48].\nSecond, beyond healthcare settings, consider the every -\nday use of chatbots in language translation. Again, it can be \nassumed, of course, that in some cases there will be no tan-\ngible benefit. The model may provide formulations which \nfail to fully capture the intended sentiment in the original \nlanguage, or perhaps a user is simply playing around, as it \nwere, without putting the translation into use. Nonetheless, \neffective communication between humans who would oth -\nerwise be incapable of communicating can be enabled by \ninteractions with AI systems, and it is hard to deny that this \nis a tangible benefit. Similarly, consider the power of chat -\nbots in providing programming assistance, translating users’ \nnatural language and commands into publishable code, like \nHTML. Such assistance stands to save countless hours of \nhuman labor and could be harnessed in building further \nbenefits in terms of personal achievement and perhaps web-\nservices that promote the interests of many others.\nUndoubtedly, I have painted the use of generative AI \nwith rose-colored glasses here and, once again, in no way \ndo I maintain that the use of such programs will always—or \neven consistently—provide tangible benefits. 18 However, \nI believe the foregoing discussion provides sufficient sup -\nport for what I already took to be a modest claim in premise \n2, namely that at least sometimes  conversational interac -\ntions with AI can pose a degree of tangible benefit to their \nusers without risking harm to others. In affirming this idea, \ntogether with the conditional statement established in prem-\nise 1, we see that at least sometimes we should refrain from \nthwarting human–AI interactions. Sometimes, we should \nnot call bullshit.\n18 Plenty of counterexamples can be imagined here, many of which \nwill involve some sort of deception or the now widely discussed pro -\npensity for LLMs to “hallucinate”. In conclusion, below, I readily \nacknowledge that we would have good reason to call bullshit where \nwe find that the potential harms clearly outweigh the potential benefits.\nIn any case, it seems clear from the conundrum between \nabsolutist values and consequential goods that progress is \nurgently needed in developing patent regulations, intellec -\ntual property law, and academic standards on how to prop -\nerly credit AI programs when they play instrumental roles \nin producing what may turn out to be widespread benefits. \nFortunately for present purposes, my support for premise 1 \ncan rest upon limiting its scope to domains where absolutist \nvalues are not wholly compromised but might be comfort -\nably sidelined where less stringent, and upon the assump -\ntion that in time we will eventually settle upon standards \nand laws detailing how one must give credit to AI for its \ncontributions. Forthcoming regulations could help to sat -\nisfy the concerns related to absolutist values, such that this \nposition would either no longer need to retain the option of \nthwarting human–AI interactions or could ensure that such \ninteractions are thwarted only in some domains.\nIn moving to my defense of premise 2, I want to fur -\nther emphasize the qualification that at least sometimes  \nconversational interactions with AI chatbots can pose a \ndegree of tangible benefit without risking harm. Clearly, \nsome human–AI interactions will turn out to be downright \nharmful; we can easily imagine cases where chatbots pro -\nduce toxic responses [ 41] or even teach users how to build \nbiological weapons [3].16 What I am claiming here is simply \nthat in some cases, users will benefit from conversational AI \ninteraction—and in tangible ways, in the sense that the ben-\nefit can be plainly observed as such by the user and perhaps \nalso by others.\nFirst, as I have already alluded to, healthcare contexts \nprovide ample space to explore the benefits of AI. But what \nI want to narrow in on is the idea, reflected in both premises, \nthat personal interactions with chatbots stand to benefit the \nimmediate user. Support for this scenario can be discerned \nfrom considering the use of AI in psychotherapy. Nicole \nMartinez-Martin and Karola Kreitmair [ 26], for example, \nsuggest that because a vast majority of Americans own \nsmartphones, we see opportunities to provide novel men -\ntal health care services in the form of digital psychotherapy \napps, which is particularly encouraging simply for the sake \nof expanding access to such care.17 Indeed, there is reason to \nthink that AI-based psychotherapy can help individual users, \nfor example, by reducing stigma and corresponding reluc -\ntance to participate in mental health services [2]. Substantial \nan improved quality of patient-physician interaction [49]. Nonetheless, \nmajor opposition to such use of AI remains.\n16 Consider also the harms of LLMs degrading science by generating \nfindings without regard for facts. With this concern in mind, Mittel -\nstadt, Wachter and Russell [30] propose the use of LLMs as ‘zero-shot \ntranslators’, converting accurate material from one form to another.\n17 Fiske et al. [14] similarly note the potential therapeutic benefits of \nvarious emerging technologies, such as social robots in caring for indi-\nviduals with dementia, autism, or sexual disorders.\n1 3\n4869\nAI and Ethics (2025) 5:4863–4873\nfrom a great deal of uncertainty regarding the cornerstones of \ntheir theorizing, namely the consequences. We simply can -\nnot know the future, and so, calculations of future goods to \nbe realized or bad outcomes to be avoided are bound to be \nunreliable and thereby constitute a poor basis for guiding our \nactions. In the same way, my account falls prey to uncertainty \nwhen I claim, in premise 2, that conversational AI interac -\ntions can, at least sometimes, pose tangible benefits without \nrisking harm. We may be left to wonder: When exactly, and \nhow could we know in advance? Again though, the claim is \nconstructed modestly, so as to need only a small number of \ncases, really just one, in order to affirm the possibility. But \nwe still might not know exactly which cases will bring about \ntangible benefits without risking any harm. Accordingly, for \nthose who default on a highly precautionary stance, consider-\ning that the use of AI might generally be seen as risky, it may \nwell seem that we should very often, and perhaps always, \nthwart human–AI interactions. In response, I admittedly can-\nnot hope here to definitively quell a longstanding and very \nreasonable objection to consequentialism. However, I can \nand do hope that if we admit the plausibility of premise 2, \nthat there are some cases where AI chatbots help one without \nharming others, then the door is open to all but the highly \nprecautious among us. Naturally, we cannot be certain as to \nwhich cases will prove beneficial—which patients will finally \nfind relief in AI-based mental health counseling, which nov-\nice programmers will build valuable products and services \nwith AI-assisted coding, and so on. But the uncertainty over \nwhere the benefits will accrue should not force us to forego \nall possibilities of benefiting. It may well be in a limited num-\nber of cases and in limited domains, but the promise of help-\ning many, at least for now, seems to warrant an open mind and \ncalls for informed policymaking in order to ease the potential \nharms.\nFinally, and again among other things, one might take \nissue with my qualified application of Frankfurt. Specifi -\ncally, it might be thought that Frankfurt himself would agree \nwith the suggestion that his account helps to characterize \nLLMs, even if he had other things to say about the qualities \nof bullshitters’ characters and intentions, which LLMs can-\nnot possess. As tech journalist Clive Thompson writes, “We \ncan see, at a glance, how applicable [Frankfurt’s] concept of \nbullshit is to ChatGPT—and indeed to the whole realm of \n‘AI-generated prose’ apps that are now on offer” [ 44]. Per-\nhaps it is no problem that numerous writers call bullshit on \nLLMs and that they cite Frankfurt in doing so. It might also \nbe said that my interpretation of bullshitters having interests \nand purpose is too literal, and that there are cases, more gen-\nerally speaking, where someone—or even something—has \nan interest in a very broad sense, such as having an end such \nthat its fulfillment would bestow a benefit. On a classical \nAristotelian teleology, for example, we loosely say things \n5 Objections considered\nBefore concluding, it is worth considering several lines of \nobjection which may plausibly be raised in response to the \naccount I have offered here. Some will likely have been \nlurking for many readers, and especially considering the \nnovelty of the issues surrounding generative AI, many more \nobjections could arise. That being said, the following is a \nsmall sample of challenges, along with some thoughts in \nresponse.\nFirst, it might be said that my main argument—perhaps \nbecause it is formulated quite modestly—faces no legitimate \nopposition, that I have fabricated a sort of strawman by sup-\nporting an echo-chamber of those who obviously seek only \nbenefits and wish to avoid the harms of AI. Indeed, it seems \nthat even those who are highly critical of developing AI and \nrobotic technologies for sensitive domains like healthcare \n[e.g. 42] might agree with the thought that we should refrain \nfrom thwarting AI interactions if chatbots pose only benefits \nwithout risking harm. However, the risk of harm is so high \nand so widespread—say, via deception—that the cases in \nwhich AI chatbots pose benefits without at least some risk \nof harm are negligible. And so, in this way, one could affirm \npremises 1 and 2, and even agree in principle with my con -\nclusion, all while thinking that the conclusion simply finds \nno application. Nonetheless, it appears that my account does \nface probable dissidence, and for that matter contributes \nto what is in fact a live debate concerning how we should \nrespond to the rise of generative AI. Consider again the surge \nof headlines and blogposts with which I began. While some \nwriters adopt a level-headed approach—for example, advis-\ning not to believe “ everything a chatbot tells you” [ 29]—\nothers are prone to making bold, exceptionless suggestions. \nFor instance, in commenting on the hype surrounding Blake \nLeMoine, the Google engineering who claimed LaMDA to \nbe sentient, Marcus wrote “The sooner we all realize that \nLamda’s utterances are bullshit—just games with predic -\ntive word tools, and no real meaning… the better off we’ll \nbe” [24, italics added]. Such bold positions surely play an \nimportant role in how we collectively deal with the emer -\ngence of AI systems today, but sweeping statements such \nas these are often far from accurate. Even if most users of \ngenerative AI should acquire a basic understanding of how \nthey function and what they produce, it is certainly not the \ncase that we should all realize that the outputs are bullshit. \nConsidering again the possibility that some users might ben-\nefit therapeutically, for example, it is misleading at best to \nthink AI’s utterances cannot have any meaning and that we \nwould all be better off with this realization.\nSecond, objections could be raised concerning my reli -\nance upon overtly consequentialist considerations. In par -\nticular, it is often thought that consequentialist theories suffer \n1 3\n4870\nAI and Ethics (2025) 5:4863–4873\nexactly when doing so might be a good idea.20 In conclusion, \na brief picture can be suggested, although not fully argued, \nnamely by adjusting the key terms of my first premise. That is, \nif a conversational interaction with an AI chatbot risks some \ndegree of tangible harm to the human user without posing \nbenefits to others, all things considered, we should proceed \nwith thwarting the human–AI interaction. In other words, in \ncases where AI interactions cause only harm, we see good \nreason to interrupt the interaction—and such interruptions \nmight be carried out by calling bullshit, namely since the \nindividuals harmed may already hold mistaken perceptions \nof AI’s capacities as being human-like. In such cases, perhaps \nwe could effectively invoke anthropomorphic notions, like \nbullshitting, if only momentarily to help break the spell.\nLLMs may well deceive us, but unlike Frankfurtian \nbullshitters, they do not necessarily attempt to deceive us  \nand do not have interests. Indeed, it is precisely this sort of \nattribution of human-like qualities to AI systems that Shana-\nhan and others advise us against. In this way, when contem-\nporary tech writers rely on Frankfurt in order to call bullshit \non LLMs, what we see is yet another attribution of human-\nlike characteristics and behaviors to non-human entities. \nThe outputs of ChatGPT and related models may well be \nbullshit, but the system itself cannot accurately be seen as a \nbullshitter, at least not on the eloquent theory put forward by \nFrankfurt. Further, as I have argued, there will also be strong \npractical reasons to refrain from thwarting some human–AI \ninteractions. Accordingly, at times, we may need to exercise \nrestraint in the face of any temptation to explicitly repudi -\nate such fashionable technology. Otherwise, those who wish \nto frame LLMs as bullshitters are tasked with specifying \nan alternative theory or with somehow convincing us that \nAI systems have interests of their own. And this latter task \nitself might start to resemble bullshit.\nAcknowledgements This paper was initially conceived in a Spring \n2023 seminar on ChatGPT and AI Ethics at the University of San \nDiego. I’m grateful to all participants for their contributions, to Darby \nVickers for co-instructing, and to Ron Kaufmann for helping to launch \nthe seminar. For conversations and correspondence that helped to pro-\npel the paper’s development, I thank David Lawrence, Nick Riggle, \nand colleagues at Northeastern University’s Institute for Experiential \nAI, especially Cansu Canca and Tomo Lazovich. Excerpts were pre -\nsented at the 2024 Neuroscience, Ethics and Technology conference, \nhosted by the Neuroethics Network at UC Berkeley, and the 2025 \nCentral Division meeting of the American Philosophical Association. I \n20 While a fuller, positive account cannot be offered here, the ground-\nwork for one can be seen with a careful application of Frankfurt’s rich \ntheory, as suggested above. That is, generative AI may well produce \nits content without a concern for truth, but even as producers of such \nbullshit, the models cannot—and, in the interest of avoiding decep -\ntion, should not—be characterized as bullshitters. But of course, this \nthought along with many, much more foundational beliefs will need \ndrastic reassessment if AI systems ever truly develop interests and pur-\nposes of their own. Again, I thank an anonymous reviewer for thought-\nful comments.\nlike acorns have an interest in becoming trees, tadpoles have \nan interest in becoming frogs, and so on.\nIn response I want to note, first, that even on a very general \ninterpretation of having interests, we would nonetheless have \nreason to give pause to the thought that LLMs are bullshit -\nters, since they certainly do not have this sort of naturalis -\ntic telos and do not really have an interest in anything, even \nbroadly speaking. Also, considering that bullshitters are said \nto have interests specifically in getting away with something, \nFrankfurt gives us good reason to favor the more literal read-\ning of having interests and thus the more restrained applica-\ntion of his theory to LLMs. Further and more importantly, \nI want to briefly return to a point I introduced at the outset. \nIn short, it is all too easy and perhaps understandable for us \nto anthropomorphize interactive technological entities. But I \ntake it as sound advice that we will usually want to quell these \ntendencies. Anthropomorphism toward technology could fos-\nter deception and mistaken perceptions of AI and its capaci-\nties, presumably even where the anthropomorphic notion we \napply is intended to stave off the idea that AI has human-like \ncapacities. As Shanahan warned, the anthropomorphic trends \ntoward AI are “amplified by the natural tendency to use \nphilosophically loaded terms, such as ‘knows’, ‘believes’, \nand ‘thinks’, when describing these systems.” In this way, \ncharacterizing an LLM as a bullshitter, or even referring to \nthe outputs as bullshit, seems to run precisely against some \nof the well-supported recommendations on how we should \nlive with these novel systems. 19 Thus, I stand by my more \nnuanced application of Frankfurt’s concept, since it appears \nconsistent with the full reading of his theory and because \nproceeding more freely with charges of AI bullshitting falls \ndirectly into the deceptive practices he would have advised \nagainst. Such advice should be heeded by contemporary tech \nwriters. Although it might be intended to help us avoid the \nharms of AI, calling bullshit on LLMs may be contributing \nto the problem.\n6 Conclusion\nAs the use of AI increases, and hopefully as public under -\nstanding of its uses and misuses likewise increases, we would \ndo well to become more attentive to the issues surrounding \nwhen and how to call bullshit. Admittedly, amidst my effort \nto say when not to call bullshit on AI, I have not specified \n19 A similar point is eloquently raised by Sarah Fisher: “we commit \nan anthropomorphizing category error in describing large language \nmodels as ‘bullshitters’” [ 13, p. 4]. Although a fuller discussion is \nmerited, Fisher’s article was published after initial submission of this \nmanuscript. At present it can be said that despite some similarities, \nmy account suggests restraint on calling bullshit in any way, while \nFisher aims to articulate newfound conceptions of bullshit—“mindless \nbullshitting”—in order to proceed with applying them to LLMs.\n1 3\n4871\nAI and Ethics (2025) 5:4863–4873\n12. Duffy, B.R.: Anthropomorphism and the social robot. Robot. \nAuton. Syst. 42(3–4), 177–190 (2003)\n13. Fisher, S.A.: Large language models and their big bullshit poten-\ntial. Eth. Inf. Technol. (2024).  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 0 7  / s 1  0 6 7 6 - 0 2 \n4 - 0 9 8 0 2 - 5\n14. Fiske, A., Henningsen, P., Buyx, A.: Your robot therapist will see \nyou now: ethical implications of embodied artificial intelligence \nin psychiatry, psychology, and psychotherapy. J. Med. Internet \nRes. 21(5), e13216 (2019)\n15. Frankfurt, H.: The Importance of What We Care About: Philo -\nsophical Essays. Cambridge University Press, Cambridge (1988)\n16. Frosio, G. Should We Ban Generative AI, Incentivise it or Make \nit a Medium for Inclusive Creativity? A Research Agenda for EU \nCopyright Law (Edward Elgar, forthcoming)\n17. Gascón, J.Á.: Argumentative bullshit. Inf. Logic 41(3), 289–308 \n(2021)\n18. Hacker, P., Engel, A., Mauer, M.: Regulating ChatGPT and other \nlarge generative AI models. In: Proceedings of the 2023 ACM \nConference on Fairness, Accountability, and Transparency, pp. \n1112–1123 (2023)\n19. Hicks, M. T., Humphries, J., Slater J.: ChatGPT is bullshit. Ethics \nand Information Technology (2024)\n20. Kapoor, S., Narayanan, A.: Leakage and the reproducibility crisis \nin ML-based science. arXiv preprint arXiv:2207.07048 (2022)\n21. Kellogg, D.: The bullshit revival. Pedagogy 6(3), 553–558 (2006)\n22. Lee, J.A., Hilty, R., Liu, K.C. (eds.): Artificial Intelligence and \nIntellectual Property. Oxford University Press, Oxford (2021)\n23. Lee, E.: Prompting progress: authorship in the Age of AI. Florida \nLaw Rev. (2024).  h t t p  s : /  / d o i  . o  r g /  1 0 . 2  1 3 9  / s s  r n . 4 6 0 9 6 8 7\n24. Marcus, G.: Nonsense on stilts. Marcus on AI.  h t t p  s : /  / g a r  y m  a r c  u \ns . s  u b s  t a c  k . c  o m /  p / n o  n s  e n s e - o n - s t i l t s (2022)\n25. Marcus, J.: Texas professor misuses ChatGPT and fails most of \nclass for AI plagiarism. Independent.  h t t p  s : /  / w w w  . i  n d e  p e n d  e n t  . c \no  . u k  / n e  w s / w  o r  l d /  a m e r  i c a  s / c  h a t  g p t  - a i -  p l  a g i  a r i s  m - t  e x a  s - a - a n d - m - \nb 2 3 4 1 2 3 8 . h t m l. (2023)\n26. Martinez-Martin, N., Kreitmair, K.: Ethical issues for direct-to-\nconsumer digital psychotherapy apps: addressing accountability, \ndata protection, and consent. JMIR Mental Health 5(2), e9423 \n(2018)\n27. McCarthy, L.: A Wellness Chatbot is Offline After Its ‘Harmful’ \nFocus on Weight Loss. The New York Times.  h t t p  s : /  / w w w  . n  y t i  m \ne s .  c o m  / 2 0  2 3 /  0 6 /  0 8 / u  s /  a i -  c h a t  b o t  - t e  s s a  - e a  t i n g  - d  i s o  r d e r  s - a  s s o  c i a t i \no n . h t m l (2023)\n28. McLennan, S., Fiske, A., Tigard, D., Müller, R., Haddadin, S., & \nBuyx, A. Embedded ethics: a proposal for integrating ethics into \nthe development of medical AI. BMC Medical Ethics, 23 (1), 6 \n(2022)\n29. Metz, C.: Why Chatbots Sometimes Act Weird and Spout Non -\nsense. The New York Times.  h t t p  s : /  / w w w  . n  y t i  m e s .  c o m  / 2 0  2 3 /  0 2 \n/  1 6 / t  e c  h n o  l o g y  / c h  a t b  o t s - e x p l a i n e d . h t m l (2023)\n30. Mittelstadt, B., Wachter, S., Russell, C.: To protect science, we \nmust use LLMs as zero-shot translators. Nat. Hum. Behav.Behav. \n7, 1–3 (2023)\n31. Narayanan, A., Kapoor, S.: ChatGPT is a bullshit generator. But \nit can still be amazingly useful. AI Snake Oil.  h t t p  s : /  / a i s  n a  k e o  i l . s  \nu b s  t a c  k . c  o m /  p / c h  a t  g p t  - i s -  a - b  u l l  s h i t - g e n e r a t o r - b u t (2022)\n32. Natale, S.: AI, human-machine communication and deception. In: \nThe Sage Handbook of Human-Machine Communication. Eds. \nAndrea Guzman, Rhonda McEwen and Steve Jones. London: \nSage, pp. 401–08 (2023)\n33. Nyholm, S.: Humans and Robots: Ethics, Agency, and Anthropo-\nmorphism. Rowman & Littlefield Publishers, Lanham (2020)\n34. Roose, K.: AI Poses ‘Risk of Extinction,’ Industry Leaders Warn. \nThe New York Times (30 May 2023):  h t t p  s : /  / w w w  . n  y t i  m e s .  c o m  / \n2 0  2 3 /  0 5 /  3 0 / t  e c  h n o  l o g y  / a i  - t h  r e a t - w a r n i n g . h t m l (2023)\nthank all participants at those events and want to express a special debt \nof gratitude to the late Thomasine Kushner.\nAuthor contribution DT is the sole author of the manuscript.\nFunding Open access funding provided by SCELC, Statewide Cali -\nfornia Electronic Library Consortium\nData availability No datasets were generated or analysed during the \ncurrent study.\nDeclarations\nConflict of interest The authors declare no competing interests.\nOpen Access   This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this licence, visit  h t t p  : / /  c r e a  t i  v e c  o m m o  n s .  o \nr g  / l i c e n s e s / b y / 4 . 0 /.\nReferences\n1. Abbott, R.: The Reasonable Robot: Artificial Intelligence and The \nLaw. Cambridge University Press, Cambridge (2020)\n2. Aktan, M.E., Turhan, Z., Dolu, I.: Attitudes and perspectives \ntowards the preferences for artificial intelligence in psychother -\napy. Comput. Hum. Behav. 133, 107273 (2022)\n3. Batalis, S.: Can Chatbots help you build a bioweapon? Foreign \nPolicy.  h t t p  s : /  / f o r  e i  g n p  o l i c  y . c  o m /  2 0 2  3 / 1  1 / 0 5  / a  i - a  r t i fi   c i  a l -  i n t  e l l  \ni g e n  c e  - c h  a t b o  t - b  i o w  e a p  o n -  v i r u  s -  b a c t e r i a - g e n e t i c - e n g i n e e r i n g / \n(2023)\n4. Black, M.: The Prevalence of Humbug. Cornell University Press, \nIthaca (1985)\n5. Bridy, A.: Coding creativity: copyright and the artificially intel -\nligent author. Stan. Tech. L. Rev. 5 (2012)\n6. Bryson, J.J.: Patiency is not a virtue: the design of intelligent \nsystems and systems of ethics. Ethics Inf. Technol. 20(1), 15–26 \n(2018)\n7. Cohen, G.A.: Deeper into bullshit. In: Buss, S., Overton, L. (eds.) \nThe Contours of Agency: Essays on Themes from Harry Frank -\nfurt, pp. 321–339. The MIT Press, Cambridge (2002)\n8. Cohen, J.R., Thakur, H., Young, J.F., Hankin, B.L.: The devel -\nopment and validation of an algorithm to predict future depres -\nsion onset in unselected youth. Psychol. Med. 50(15), 2548–2556 \n(2020)\n9. Cohn, J. The Robot Will See You Now. The Atlantic (2013)\n10. Danaher, J.: Robot betrayal: a guide to the ethics of robotic decep-\ntion. Ethics Inf. Technol. 22(2), 117–128 (2020)\n11. Darling, K.: Extending legal protection to social robots: the \neffects of anthropomorphism, empathy, and violent behavior \ntowards robotic objects. In: Calo, R., Michael Froomkin, A., Kerr, \nI. (eds.) Robot Law. Edward Elgar, Cheltenham (2016)\n1 3\n4872\nAI and Ethics (2025) 5:4863–4873\n47. Tigard, D. W. Embedded ethics as preparatory regulation of tech-\nnology: A new solution to the Collingridge dilemma? In D. Law-\nrence & S. Morley (Eds), Novel beings: Regulatory approaches \nfor a future of new intelligent life.  Edward Elgar Publishing \n(2022)\n48. Tigard, D. W. Toward relational diversity for AI in psychother -\napy. Am. J. Bioeth. 23(5), 64-66 (2023)\n49. Topol, E.: Deep Medicine: How Artificial Intelligence Can Make \nHealthcare Human Again. Hachette UK, Paris (2019)\n50. Van Wynsberghe, A., Robbins, S.: Critiquing the reasons for \nmaking artificial moral agents. Sci. Eng. Ethics 25(3), 719–735 \n(2019)\n51. Watson, D.: The rhetoric and reality of anthropomorphism in arti-\nficial intelligence. Mind. Mach. 29(3), 417–440 (2019)\n52. Weiser, B.: Here’s what happens when your lawyer uses Chat -\nGPT. The New York Times.  h t t p  s : /  / w w w  . n  y t i  m e s .  c o m  / 2 0  2 3 /  0 5 /  2 \n7 / n  y r  e g i  o n / a  v i a  n c a  - a i  r l i  n e - l  a w  s u i t - c h a t g p t . h t m l (2023)\n53. Willem, T., Fritzsche, M. C., Zimmermann, B. M., Sierawska, A., \nBreuer, S., Braun, M.,... & Buyx, A. Embedded Ethics in Prac -\ntice: A Toolbox for Integrating the Analysis of Ethical and Social \nIssues into Healthcare AI Research. Sci. Eng. Ethics  31(1), 1-22 \n(2025)\n54. Wu, S., Rendall, J.B., Smith, M.J., Zhu, S., Xu, J., Wang, H., Qin, \nP.: Survey on prediction algorithms in smart homes. IEEE Inter -\nnet Things J. 4(3), 636–644 (2017)\n55. y Arcas, B.A.: Do large language models understand us? Daeda -\nlus 151(2), 183–197 (2022)\nPublisher's Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional affiliations.\n35. Salles, A., Evers, K., Farisco, M.: Anthropomorphism in AI. \nAJOB Neurosci. 11(2), 88–95 (2020)\n36. Samuelson, P.: Allocating ownership rights in computer-gener -\nated works. Univ. Pittsburgh Law Rev. 47, 1185 (1986)\n37. Samuelson, P.: AI authorship? Commun. ACM 63(7), 20–22 \n(2020)\n38. Sedlakova, J., Trachsel, M.: Conversational artificial intelligence \nin psychotherapy: A new therapeutic tool or agent? Am. J. Bioeth. \n23(5), 4–13 (2023)\n39. Shanahan, M.: Talking about large language models. arXiv pre -\nprint arXiv:2212.03551 (2022)\n40. Shoemaker, D.W.: Caring, identification, and agency. Ethics \n114(1), 88–118 (2003)\n41. Si, W.M., Backes, M., Blackburn, J., De Cristofaro, E., String -\nhini, G., Zannettou, S., Zhang, Y .: Why so toxic? measuring and \ntriggering toxic behavior in open-domain chatbots. In: Proceed -\nings of the 2022 ACM SIGSAC Conference on Computer and \nCommunications Security, pp. 2659–2673 (2022)\n42. Sparrow, R., Sparrow, L.: In the hands of machines? The future of \naged care. Mind. Mach. 16, 141–161 (2006)\n43. Sundar, S., Liao, M.: Calling BS on ChatGPT: Reflections on AI \nas a Communication Source. J. Commun. Monogr. 25(2), 165–\n180 (2023)\n44. Thompson, C. On Bullshit, and AI-Generated Prose. Medium.  h t t \np  s : /  / c l i  v e  t h o  m p s o  n . m  e d i  u m .  c o m  / o n -  b u  l l s  h i t -  a n d  - a i  - g e  n e r  a t e d  - p  r \no s e - 6 1 1 a 0 f 8 9 9 c 5 (2022)\n45. Tigard, D. W. Artificial moral responsibility: How we can and \ncannot hold machines responsible. Cambridge Quarterly of \nHealthcare Ethics, 30(3), 435-447 (2021)\n46. Tigard, D. W. Technological answerability and the severance \nproblem: Staying connected by demanding answers. Sci. Eng. \nEthics 27(5), 59 (2021)\n1 3\n4873",
  "topic": "Enthusiasm",
  "concepts": [
    {
      "name": "Enthusiasm",
      "score": 0.9552580714225769
    },
    {
      "name": "Linguistics",
      "score": 0.36461758613586426
    },
    {
      "name": "Computer science",
      "score": 0.3205159306526184
    },
    {
      "name": "Psychology",
      "score": 0.2448156476020813
    },
    {
      "name": "Philosophy",
      "score": 0.23408693075180054
    },
    {
      "name": "Social psychology",
      "score": 0.07017165422439575
    }
  ],
  "institutions": [],
  "cited_by": 1
}