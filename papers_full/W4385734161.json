{
  "title": "LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models",
  "url": "https://openalex.org/W4385734161",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2234347598",
      "name": "Yen Ting Lin",
      "affiliations": [
        "National Taiwan University"
      ]
    },
    {
      "id": "https://openalex.org/A4208538295",
      "name": "Yun-Nung Chen",
      "affiliations": [
        "National Taiwan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093632820",
    "https://openalex.org/W1591706642",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W3176264844",
    "https://openalex.org/W2944870985",
    "https://openalex.org/W3036394672",
    "https://openalex.org/W2977070604",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W3177219653",
    "https://openalex.org/W3210011271",
    "https://openalex.org/W4289447057",
    "https://openalex.org/W2584220694",
    "https://openalex.org/W3117574848",
    "https://openalex.org/W2972664115",
    "https://openalex.org/W4205686084",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W2962786758",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2963903950",
    "https://openalex.org/W4362598574",
    "https://openalex.org/W2665731731",
    "https://openalex.org/W3105218667",
    "https://openalex.org/W2963825865",
    "https://openalex.org/W3104405162",
    "https://openalex.org/W4285188834",
    "https://openalex.org/W3114032074",
    "https://openalex.org/W4361806892",
    "https://openalex.org/W2998494704",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3034808773",
    "https://openalex.org/W4283317842",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3094313828",
    "https://openalex.org/W2951583236",
    "https://openalex.org/W4319793767"
  ],
  "abstract": "We propose LLM-Eval, a unified multi-dimensional automatic evaluation method for open-domain conversations with large language models (LLMs). Existing evaluation methods often rely on human annotations, ground-truth responses, or multiple LLM prompts, which can be expensive and time-consuming. To address these issues, we design a single prompt-based evaluation method that leverages a unified evaluation schema to cover multiple dimensions of conversation quality in a single model call. We extensively evaluate the performance of LLM-Eval on various benchmark datasets, demonstrating its effectiveness, efficiency, and adaptability compared to state-of-the-art evaluation methods. Our analysis also highlights the importance of choosing suitable LLMs and decoding strategies for accurate evaluation results. LLM-Eval offers a versatile and robust solution for evaluating open-domain conversation systems, streamlining the evaluation process and providing consistent performance across diverse scenarios.",
  "full_text": "Proceedings of the 5th Workshop on NLP for Conversational AI (NLP4ConvAI 2023), pages 47–58\nJuly 14, 2023 ©2023 Association for Computational Linguistics\nLLM-E VAL: Unified Multi-Dimensional Automatic Evaluation for\nOpen-Domain Conversations with Large Language Models\nYen-Ting Lin Yun-Nung Chen\nNational Taiwan University, Taipei, Taiwan\n{ytl, y.v.chen}@ieee.org\nAbstract\nWe propose LLM-E VAL, a unified multi-\ndimensional automatic evaluation method for\nopen-domain conversations with large language\nmodels (LLMs). Existing evaluation methods\noften rely on human annotations, ground-truth\nresponses, or multiple LLM prompts, which\ncan be expensive and time-consuming. To ad-\ndress these issues, we design a single prompt-\nbased evaluation method that leverages a uni-\nfied evaluation schema to cover multiple dimen-\nsions of conversation quality in a single model\ncall. We extensively evaluate the performance\nof LLM-E VAL on various benchmark datasets,\ndemonstrating its effectiveness, efficiency, and\nadaptability compared to state-of-the-art evalu-\nation methods. Our analysis also highlights the\nimportance of choosing suitable LLMs and de-\ncoding strategies for accurate evaluation results.\nLLM-E VAL offers a versatile and robust solu-\ntion for evaluating open-domain conversation\nsystems, streamlining the evaluation process\nand providing consistent performance across\ndiverse scenarios.\n1 Introduction\nEffective evaluation of open-domain conversation\nsystems is a critical yet challenging problem in\nnatural language processing research (Smith et al.,\n2022). Accurate and consistent evaluation meth-\nods are essential for understanding and improv-\ning the performance of dialogue systems. Tradi-\ntional automatic evaluation metrics, such as BLEU\n(Papineni et al., 2002) and ROUGE (Lin, 2004),\nare insufficient for capturing the nuances of natu-\nral language conversations (Liu et al., 2016; De-\nriu et al., 2021), leading to the development of\nvarious advanced metrics (Tao et al., 2018; Ghaz-\narian et al., 2019; Sai et al., 2020; Huang et al.,\n2020; Mehri and Eskenazi, 2020b; Phy et al., 2020;\nZhang et al., 2021a; Li et al., 2021; Fu et al., 2023;\nLiu et al., 2023). However, most existing meth-\nods require annotation data , human references, or\nLLM-Eval\n{evaluation schema}\nScore the following dialogue response gener-\nated on a continuous scale from 0.0 to 5.0.\nContext:\n: My cat likes to eat cream.\n: Be careful not to give too much, \nthough.\nDialogue response :\n: Don't worry, I only give a little bit \nas a treat.\nAppropriateness: 3.0\nContent: 2.5\nGrammer: 4.0\nRelevence: 2.0\nFigure 1: An illustration of our proposed LLM-E VAL\nframework, which leverages a unified multi-dimensional\nevaluation schema and a single prompt to efficiently\nevaluate open-domain conversations with large language\nmodels.\nmultiple prompts, which could be expensive, time-\nconsuming, or prone to errors.\nIn this paper, we address the problem of eval-\nuating open-domain conversation systems with a\nfocus on large language models (LLMs) (Figure\n1). Our goal is to develop an efficient and accurate\nevaluation method that covers multiple dimensions\nof conversation quality, such as content, grammar,\nrelevance, and appropriateness, without requiring\nhuman references or multiple prompts. We build\nupon recent advances in LLMs (Brown et al., 2020;\n47\nBai et al., 2022; OpenAI, 2023), and propose a uni-\nfied multi-dimensional evaluation method called\nLLM-E VAL.\nExisting evaluation methods have demonstrated\npromising results in various aspects of dialogue\nevaluation. However, they often rely on human\nannotations (Mehri and Eskenazi, 2020b; Phy et al.,\n2020), ground-truth responses (Ghazarian et al.,\n2020; Zhang et al., 2020a), or multiple LLM infer-\nences (Fu et al., 2023; Liu et al., 2023), limiting\ntheir efficiency and adaptability in practical scenar-\nios. We aim to bridge this gap by proposing LLM-\nEVAL, a single-prompt-based evaluation method\nthat leverages a unified evaluation schema to cover\nmultiple dimensions of conversation quality in a\nsingle model call.\nIn LLM-E VAL, we design a natural language\ninstruction that defines the evaluation task and de-\nsired criteria, as well as a format instruction that\nspecifies the structure and range of scores for each\ndimension. The single prompt is created by con-\ncatenating the dialogue context, reference (if avail-\nable), and generated response, and then fed to a\nlarge language model, which outputs scores for\neach dimension based on the defined schema.\nWe extensively evaluate the performance of\nLLM-E VAL on a variety of benchmark datasets,\ncovering diverse dialogue systems and evaluation\ndimensions. Our experiments demonstrate that\nLLM-E VAL consistently outperforms most base-\nlines and state-of-the-art evaluation methods in\nterms of correlation with human judgments. The\nproposed method is also robust and versatile, adapt-\ning to different scoring ranges and evaluation sce-\nnarios.\nIn summary, our main contributions are 3-fold:\n• We propose LLM-E VAL, a unified multi-\ndimensional automatic evaluation method for\nopen-domain conversations with large lan-\nguage models, which streamlines the evalu-\nation process by using a single prompt and a\nunified evaluation schema.\n• We extensively evaluate the performance\nof LLM-E VAL on a variety of benchmark\ndatasets, demonstrating its effectiveness and\nefficiency in comparison with state-of-the-art\nevaluation methods.\n• We provide an in-depth analysis of the impact\nof different LLMs and decoding methods on\nthe performance of LLM-E VAL, highlighting\nthe importance of choosing suitable LLMs\nand decoding strategies for accurate evalua-\ntion results.\n2 Related Work\nMulti-Dimensional Metrics Multi-dimensional\nevaluation metrics have been proposed to assess\nvarious aspects of dialogue quality, such as content,\ngrammar, relevance, and appropriateness. Exam-\nples include USR (Mehri and Eskenazi, 2020b),\nwhich trains multiple models to measure qualities\nlike fluency, relevance, and knowledge condition-\ning, and GRADE (Huang et al., 2020), which mod-\nels topic transition dynamics in dialogue history\nusing a graph representation. FlowScore (Li et al.,\n2021) leverages dynamic information flow in di-\nalog history to measure dialogue quality. Unlike\nthese approaches, LLM-E VAL employs a single\nprompt-based evaluation method that leverages a\nunified evaluation schema, streamlining the eval-\nuation process and providing a more efficient and\nadaptable solution.\nUnsupervised Metrics Unsupervised evaluation\nmetrics aim to assess the quality of dialogue re-\nsponses without requiring human annotations. No-\ntable unsupervised methods include DEB (Sai et al.,\n2020), which fine-tunes BERT with an NSP ob-\njective on a dataset with relevant and adversarial\nirrelevant responses, and FED (Mehri and Eske-\nnazi, 2020a), an unsupervised method that mea-\nsures dialogue quality using features derived from\nresponse embeddings and language model prob-\nabilities. In contrast, LLM-E VAL leverages the\npower of large language models to provide a uni-\nfied multi-dimensional evaluation, achieving better\nperformance and adaptability compared to existing\nunsupervised methods.\nLarge Language Models for Evaluation Re-\ncent works have explored using large language\nmodels for dialogue evaluation. GPTScore (Fu\net al., 2023) employs models like GPT-3 to assign\nhigher probabilities to quality content, using mul-\ntiple prompts for a multi-dimensional assessment.\nChen et al. (2023) explores using ChatGPT and\nInstructGPT to evaluate text quality without refer-\nences, and compares different paradigms of using\nLLMs, including generating explicit scores, using\nmodel confidence to determine implicit scores, and\ndirectly comparing pairs of texts. G-EV AL (Liu\net al., 2023), a framework that leverages LLMs\n48\nwith chain-of-thoughts (CoT)(Wei et al., 2022) and\na form-filling paradigm. G-EV AL with GPT-4\nas the backbone model achieves a high correla-\ntion with human judgments on a summarization\ntask. However, both GPTScore and G-EV AL re-\nquire multiple prompts or complex scoring func-\ntions that use probabilities of output tokens and\ntheir weighted summation as the final score, which\ncan be inefficient or time-consuming. LLM-E VAL\naddresses these issues by using a single prompt\nand a unified evaluation schema, offering a more\nefficient and adaptable evaluation method for open-\ndomain conversations. Additionally, LLM-E VAL\nprovides multi-dimensional evaluation scores in a\nsingle model call, further streamlining the evalua-\ntion process.\n3 Methodology\nLLM-E VAL is an efficient prompt-based evalua-\ntor tailored for open-domain conversations with\nlarge language models. It encompasses a single\nprompt that addresses the evaluation task, desired\nevaluation criteria, and a unified multi-dimensional\nevaluation schema. This method eradicates the ne-\ncessity for numerous LLMs inferences or intricate\nscoring functions (Fu et al., 2023; Liu et al., 2023),\nwhile still delivering a comprehensive assessment\nof the generated text.\nUnified Evaluation Schema The evaluation\nschema is a natural language instruction that de-\nfines the task and the desired evaluation criteria.\nIt is designed to cover multiple dimensions of the\nevaluation, such as content, grammar, relevance,\nand appropriateness. The schema is provided as\na format instruction, which specifies the structure\nand the range of the scores for each dimension. For\nexample, the evaluation schema can be:\nHuman: The output should be format-\nted as a JSON instance that conforms\nto the JSON schema below. ... Here is\nthe output schema: {\"properties\": {\"con-\ntent\": {\"title\": \"Content\", \"description\":\n\"content score in the range of 0 to 100\",\n\"type\": \"integer\", \"grammar\": ...}\nSingle Prompt for Evaluation The single\nprompt is designed to include the necessary dia-\nlogue context and the target response that needs\nto be evaluated, along with the evaluation schema.\nThe prompt is concatenated with the dialogue con-\ntext, the reference (if available), and the generated\nresponse, and then fed to the large language model\nto output a score for each evaluation dimension,\nbased on the defined schema. For example, the\nprompt for evaluating a dialogue response with\nhuman reference can be:\nContext: {context}\nReference: {reference}\nDialogue response: {response}\nEfficient Evaluation By using a single prompt\nwith a unified evaluation schema, LLM-E VAL can\nefficiently obtain multi-dimensional scores for the\nresponses without the need for multiple prompts.\nThe large language model is called only once, and it\ndirectly provides the evaluation scores for each di-\nmension based on the defined schema. For instance,\ngiven a dialogue context, reference, and generated\nresponse, the LLM-E VAL method would produce\nan example output that looks like this:\nOutput: {\"appropriateness\": 3.0, \"con-\ntent\": 2.5, \"grammar\": 4.0, \"relevance\":\n2.0}\nThis output showcases the multi-dimensional\nevaluation of the generated response, with each\ndimension receiving a score based on the prede-\nfined schema. The scores help in understanding\nthe quality of the response in terms of appropri-\nateness, content, grammar, and relevance, while\nstill maintaining the efficiency of the evaluation\nprocess by requiring just a single call to the large\nlanguage model. For a detailed description of the\nprompt templates used in our experiments with\nLLM-E VAL, please refer to Appendix A.\n4 Experiments\n4.1 Datasets and Benchmarks\nOur proposed LLM-E VAL method is assessed on\nan array of datasets spanning diverse dialogue sys-\ntems and evaluation dimensions. We provide a\nconcise overview of the datasets and their features\nin this section. The datasets include human annota-\ntions, where each entry comprises a dialogue con-\ntext, a generated response, and associated scores. A\nground-truth human reference may also be present.\nFor data lacking human reference, we only evaluate\nreference-free metrics.\nDSTC10 Hidden Set The DSTC10 hidden set\n(Zhang et al., 2021b) is a multi-dimensional evalua-\ntion dataset that includes JSALT (Kong-Vega et al.,\n49\n2018), NCM, ESL (Vinyals and Le, 2015; Sedoc\net al., 2019; Lee et al., 2020), Topical-DSTC10\n(Gopalakrishnan et al., 2019) and Persona-DSTC10\n(Zhang et al., 2018). JSALT contains human-\ngenerated dialogue segments from EmpatheticDi-\nalogues (Rashkin et al., 2019) and TopicalChat\n(Gopalakrishnan et al., 2019). NCM and ESL are\ndatasets with pairwise comparisons between sys-\ntem responses, collected from an English learn-\ning website and hand-crafted prompts. Topical-\nDSTC10 and Persona-DSTC10 are newly created\ndatasets that include responses from various dia-\nlogue systems, such as LSTM Seq2Seq, HRED,\nVHRED, BlenderBot, DialoGPT, T5, and GPT-3.\nOverall Scores with Human Reference\nTopicalChat-USR evaluates response quality in\nknowledge-grounded dialogues, emphasizing\ntopical understanding. PersonaChat-USR measures\nresponse quality in personalized conversations,\nhighlighting the incorporation of speaker personas\n(Mehri and Eskenazi, 2020b). ConvAI2-GRADE\nexamines the quality of chit-chat dialogue systems,\nfocusing on engaging and contextually relevant\nresponses. DailyDialog-GRADE investigates re-\nsponse quality in everyday conversational contexts.\nEmpatheticDialogue-GRADE assesses the quality\nof empathetic responses in dialogue systems\n(Huang et al., 2020). DSTC6 evaluates end-to-end\nconversation modeling with human-generated\nresponses (Hori and Hori, 2017).\nOverall Scores without Human Reference\nDailyDialog-PredictiveEngagement evaluates en-\ngagement in dialogue systems without relying on\nhuman references (Ghazarian et al., 2020). FED is\nan unsupervised method that measures the quality\nof dialogue responses without using human ref-\nerences (Mehri and Eskenazi, 2020a). DSTC9\nfocuses on the end-to-end evaluation of context-\naware dialogue systems without human references\n(Mehri et al., 2022).\nWe compare the performance of LLM-E VAL\nwith existing evaluation methods on these datasets\nto demonstrate its effectiveness and efficiency in\nevaluating open-domain conversations. The evalu-\nation results are presented in terms of correlation\nwith human judgments, using Pearson’s correlation\ncoefficient (r) and Spearman’s correlation coeffi-\ncient (ρ).\n4.2 LLM-E VAL Configurations\nWe evaluateLLM-E VAL under different settings to\ndemonstrate its effectiveness and adaptability. The\nconfigurations are as follows:\nLLM-E VAL 0-5 The evaluation scores for each\ndimension are in the range of 0 to 5 with one dec-\nimal place, which is more close to common 1-5\nLikert scale used in human evaluation.\nLLM-E VAL 0-100 The evaluation scores for\neach dimension are in the range of 0 to 100 as inte-\ngers, providing a finer-grained scale for evaluation.\nThe evaluation schema prompt for both config-\nurations remains the same, with only the range of\nscores differing between them. We test the LLM-\nEVAL method with and without human references\nfor each configuration if applicable.\nUnless specified otherwise, throughout our ex-\nperiments and evaluations, we employ the An-\nthropic Claude API with the claude-v1.3 model\nand use greedy decoding, which selects the token\nwith the highest probability at each time step during\nthe generation process.\n4.3 Baseline Evaluation Metrics\nWe compare LLM-E VAL with several state-of-the-\nart evaluation metrics, including both traditional\nand LLM-based approaches.\n• Deep-AM-FM measures dialog quality with\nAdequacy Metric (AM) and Fluency Met-\nric (FM), utilizing BERT embeddings and\nlanguage model probabilities (Zhang et al.,\n2020a).\n• DSTC10 Team 1 boosted DyanEval’s (Zhang\net al., 2021a) turn-level evaluation perfor-\nmance by integrating auxiliary objectives and\ncombining USL-H(Phy et al., 2020), DEB\n(Sai et al., 2020), and an improved DyanEval,\nwith weights based on input dialogue data\ncharacteristics (Zhang et al., 2021b).\n• MME-CRS introduces the Multi-Metric Eval-\nuation, consisting of 5 parallel sub-metrics to\nassess dialogue quality across fluency, rele-\nvance, engagement, specificity, and topic co-\nherence. The approach utilizes Correlation\nRe-Scaling to model sub-metric relationships\n(Zhang et al., 2022).\n• BERTScore computes the F1 score by match-\ning token embeddings in human references\nand system responses (Zhang et al., 2020b).\n50\nSpearman ρ(%) JSALT ESL NCM TopicalChat-DSTC10 PersonaChat-DSTC10 AvgAPP APP APP APP CON GRA REL APP CON GRA REL\nDeep-AM-FM 5.1 32.3 16.5 18.2 9.4 17.9 26.2 21.0 14.7 19.1 24.1 18.4\nDSTC10 Team 1 27.7 42.0 29.9 29.7 7.0 11.6 37.0 38.6 19.3 18.6 44.5 30.2\nMME-CRS 11.7 41.4 29.9 32.6 17.2 9.0 44.8 45.6 32.5 22.0 54.8 31.0\nwithout human reference\nLLM-E VAL 0-5 23.2 51.8 34.4 38.6 20.6 33.2 42.8 48.2 36.9 34.5 52.1 37.8\nLLM-E VAL 0-100 27.3 50.5 34.2 38.6 21.3 32.7 41.1 47.6 37.8 30.2 51.9 37.6\nwith human reference\nLLM-E VAL 0-5 25.4 51.8 32.5 38.0 21.5 31.2 42.2 47.9 36.0 30.6 49.1 36.9\nLLM-E VAL 0-100 25.7 51.9 30.8 38.2 21.6 30.0 40.2 45.4 34.8 28.6 49.3 36.0\nTable 1: Spearman correlation coefficients between human ratings and automatic metrics across multiple dimensions\n(APP for Appropriateness, CON for Content, GRA for Grammar, and REL for Relevance) for DSTC10 hidden test\ndatasets with human reference. Each team is represented by the best submission on 5 test datasets. The best score\nfor each column is highlighted in bold. The second best is underlined. Note that the last column is averaged over 11\ndimension-wise correlation scores of all five datasets.\nr/ ρ(%) TopicalChat PersonaChat ConvAI2 DD ED DSTC6 Average\nBLEU-4 21.6 / 29.6 13.5 / 9.0 0.3 / 12.8 7.5 / 18.4 -5.1 / 0.2 13.1 / 29.8 8.5 / 16.6\nROUGE-L 27.5 / 28.7 6.6 / 3.8 13.6 / 14.0 15.4 / 14.7 2.9 / -1.3 33.2 / 32.6 16.5 / 15.4\nBERTScore 29.8 / 32.5 15.2 / 12.2 22.5 / 22.4 12.9 / 10.0 4.6 / 3.3 36.9 / 33.7 20.3 / 19.0\nDEB 18.0 / 11.6 29.1 / 37.3 42.6 / 50.4 33.7 / 36.3 35.6 / 39.5 21.1 / 21.4 30.0 / 32.8\nGRADE 20.0 / 21.7 35.8 / 35.2 56.6 / 57.1 27.8 / 25.3 33.0 / 29.7 11.9 / 12.2 30.9 / 30.2\nUSR 41.2 / 42.3 44.0 / 41.8 50.1 / 50.0 5.7 / 5.7 26.4 / 25.5 18.4 / 16.6 31.0 / 30.3\nUSL-H 32.2 / 34.0 49.5 / 52.3 44.3 / 45.7 10.8 / 9.3 29.3 / 23.5 21.7 / 17.9 31.3 / 30.5\nwithout human reference\nLLM-E VAL 0-5 55.7 / 58.3 51.0 / 48.0 59.3 / 59.6 31.8 / 32.2 42.1 / 41.4 43.3 / 41.1 47.2 / 46.8\nLLM-E VAL 0-100 49.0 / 49.9 53.3 / 51.5 61.3 / 61.8 34.6 / 34.9 43.2 / 42.3 44.0 / 41.8 47.6 / 47.0\nwith human reference\nLLM-E VAL 0-5 56.5 / 59.4 55.4 / 53.1 43.1 / 43.8 32.0 / 32.2 40.0 / 40.1 47.0 / 45.5 45.7 / 45.7\nLLM-E VAL 0-100 55.6 / 57.1 53.8 / 52.7 45.6 / 45.9 33.4 / 34.0 43.5 / 43.2 49.8 / 49.9 47.0 / 47.1\nTable 2: Correlation coefficients (Pearson rand Spearman ρ) between human ratings and automatic metrics in\nterms of overall scores for datasets with human reference. We use the following abbreviations: TopicalChat\n(TopicalChat-USR), PersonaChat (PersonaChat-USR), ConvAI2 (ConvAI2-GRADE), DD (DailyDialog-GRADE),\nED (EmpatheticDialogue-GRADE). The best score for each column is highlighted in bold. The second best is\nunderlined.\n• DEB constructs a dialog dataset with relevant\nand adversarial irrelevant responses, then fine-\ntunes BERT with an NSP objective (Sai et al.,\n2020).\n• GRADE models topic transition dynamics\nin dialog using a graph representation of the\ndialog history (Huang et al., 2020).\n• USR trains several models to measure differ-\nent qualities of dialogs, including fluency, rel-\nevance, and knowledge conditioning (Mehri\nand Eskenazi, 2020b).\n• USL-H combines three models trained with\ndifferent objectives (VUP, NSP, MLM) to eval-\nuate response validity, sensibleness, and like-\nlihood (Phy et al., 2020).\n• DynaEval leverages a graph structure to\nmodel dialog-level interactions between user\nand system (Zhang et al., 2021a).\n• FlowScore models dynamic information flow\nin dialog history and measures dialog qual-\nity using DialoFlow representations (Li et al.,\n2021).\n• GPTScore evaluates text using models like\nGPT-3, assigning higher probabilities to qual-\nity content through multiple prompts for a\nmulti-dimensional assessment. However, it\nmay not be as effective asLLM-E VAL, which\nonly requires a single prompt (Fu et al., 2023).\n• Traditional Metrics: We also include classic\nmetrics such as BLEU (Papineni et al., 2002)\nand ROUGE (Lin, 2004), which have known\nlimitations in dialogue evaluation.\n4.4 Results of DSTC10 Hidden Set\nThe results of our proposed LLM-E VAL method\non the DSTC10 hidden set are presented in Table\n51\nr/ ρ(%) DailyDialog-PE FED DSTC9 AverageTurn-Level Turn-Level Dialog-Level Dialog-Level\nDynaEval 16.7 / 16.0 31.9 / 32.3 50.3 / 54.7 9.3 / 10.1 27.1 / 28.3\nUSL-H 68.8 / 69.9 20.1 / 18.9 7.3 / 15.2 10.5 / 10.5 26.7 / 28.6\nFlowScore - -6.5 / -5.5 -7.3 / -0.3 14.7 / 14.0 0.3 / 2.7\nGPTScore - - / 38.3 - / 54.3 - - / 46.3\nLLM-E VAL 0-5 71.0 / 71.3 60.4 / 50.9 67.6 / 71.4 15.9 / 16.5 53.7 / 52.5\nLLM-E VAL 0-100 71.4 / 71.0 59.7 / 49.9 64.4 / 70.4 16.1 / 18.6 52.9 / 52.5\nTable 3: Correlation coefficients (Pearson rand Spearman ρ) between human ratings and automatic metrics in terms\nof overall scores for datasets without human reference. The best score for each column is highlighted in bold. The\nsecond best is underlined.\n1. We compare the performance of LLM-E VAL\nwith other participating teams and baselines in the\nDSTC10 challenge. The evaluation is performed in\nterms of Spearman correlation coefficients between\nhuman ratings and automatic metrics across multi-\nple dimensions, including Appropriateness (APP),\nContent (CON), Grammar (GRA), and Relevance\n(REL).\nThe results show that LLM-E VAL consistently\noutperforms most of the baselines and even the\nbest performing team in DSTC10 across different\ndimensions and datasets. In particular, LLM-E VAL\nwith a 0-5 score range achieves the highest average\nSpearman correlation coefficient of 0.378 among\nall the methods without human reference.\nWhen comparing the two LLM-E VAL configura-\ntions, both 0-5 and 0-100 settings demonstrate com-\npetitive performance, with the 0-5 configuration\nslightly outperforming the 0-100 configuration in\nboth cases with or without human reference. This\nindicates that the LLM-E VAL method is robust\nand versatile in evaluating open-domain conversa-\ntions, as it can adapt to different scoring ranges\nand consistently outperform all baselines and the\nbest performing team in DSTC10 across various\ndimensions and datasets.\n4.5 Overall Scores with Human Reference\nThe results of LLM-E VAL on datasets with over-\nall scores and human references are presented in\nTable 2. We compare the performance of LLM-\nEVAL with other top-performing evaluation meth-\nods (Yeh et al., 2021), such as BLEU, ROUGE,\nBERTScore, DEB, GRADE, USR, and USL-H.\nThe meta-evaluation is performed in terms of Pear-\nson correlation coefficient (r) and Spearman cor-\nrelation coefficient (ρ) between human ratings and\nautomatic metrics.\nFor the DailyDialog-GRADE, ConvAI2-\nGRADE, and EmpatheticDialogue-GRADE\ndatasets, we use the \"Relevance\" dimension for\nevaluation, while for the DSTC6 dataset, we use\nthe “Overall” score. For TopicalChat-USR and\nPersonaChat-USR, we predict all the \"Engaging,\nMaintains Context, Natural, Overall, Understand-\nable, Uses Knowledge\" dimensions in the original\nannotations but only use the \"Overall\" score for\nmeta-evaluation.\nLLM-E VAL consistently outperforms most of\nthe baselines across the datasets and correlation\ncoefficients, with LLM-Eval 0-100 configuration\nachieving the highest average correlation coeffi-\ncient across all datasets.\nThe consistent performance of both configura-\ntions across different datasets and dimensions in-\ndicates that LLM-E VAL is a reliable and effective\nevaluation tool for open-domain conversations with\nhuman references. Its ability to adapt to different\nscoring ranges while maintaining competitive per-\nformance against state-of-the-art evaluation meth-\nods showcases the versatility and robustness of the\nLLM-E VAL approach.\n4.6 Overall Scores without Human Reference\nTable 3 presents the performance of LLM-E VAL\non datasets without human references, comparing\nit with other high-performing evaluation methods\nsuch as DynaEval, USL-H, and FlowScore.\nFor the evaluation of DailyDialog-\nPredictiveEngagement and DSTC9 datasets,\nwe utilize the \"Overall\" score. In the FED\ndataset, we predict \"Correctness, Engagement,\nFluency, Interestingness, Overall, Relevance,\nSemantically Appropriateness, Specificity, and\n52\nSpearman ρ(%) Topical-DSTC10 Persona-DSTC10 AverageAPP CON GRA REL APP CON GRA REL\nDeep-AM-FM 18.2 9.4 17.9 26.2 21.0 14.7 19.1 24.1 18.9\nDSTC10 Team 1 29.7 7.0 11.6 37.0 38.6 19.3 18.6 44.5 25.8\nMME-CRS 32.6 17.2 9.0 44.8 45.6 32.5 22.0 54.8 32.3\nwithout human reference\nLLM-E VAL 0-5\nAnthropic Claude 38.6 20.6 33.2 42.8 48.2 36.9 34.5 52.1 38.4\nAnthropic Claude top_p= 0.9 31.9 16.9 30.2 38.5 39.4 30.2 28.9 46.3 32.8\nOpenAI ChatGPT 35.7 18.4 33.1 37.3 43.5 33.4 30.1 48.8 35.0\nOpenAI GPT-3.5 29.3 16.9 20.9 37.1 36.5 30.2 21.7 45.2 29.7\nLLM-E VAL 0-100\nAnthropic Claude 38.6 21.3 32.7 41.1 47.6 37.8 30.2 51.9 37.7\nAnthropic Claude top_p= 0.9 30.1 15.6 27.3 37.7 36.2 27.9 25.9 45.4 30.8\nOpenAI ChatGPT 36.2 16.7 33.4 36.0 44.0 31.7 31.4 48.1 34.7\nOpenAI GPT-3.5 28.2 13.9 23.5 34.0 34.8 24.7 21.7 42.9 28.0\nwith human reference\nLLM-E VAL 0-5\nAnthropic Claude 38.0 21.5 31.2 42.2 47.9 36.0 30.6 49.1 37.1\nAnthropic Claude-instant 26.5 14.3 30.1 27.0 33.4 30.5 25.8 35.2 27.9\nOpenAI ChatGPT 34.0 18.9 30.3 35.1 39.4 30.0 25.6 40.9 31.8\nOpenAI GPT-3.5 30.0 17.3 21.2 38.8 37.9 28.8 20.8 45.1 30.0\nLLM-E VAL 0-100\nAnthropic Claude 38.2 21.6 30.0 40.2 45.4 34.8 28.6 49.3 36.0\nAnthropic Claude-instant 28.0 14.3 32.1 34.0 37.5 31.1 32.0 40.8 31.2\nOpenAI ChatGPT 34.6 20.6 31.1 35.4 39.7 31.3 23.8 44.1 32.6\nOpenAI GPT-3.5 12.4 20.8 30.5 37.8 26.6 20.7 24.0 40.0 26.6\nTable 4: Spearman correlation coefficients between human ratings and LLM-E VAL with different configurations\nacross multiple dimensions (APP for Appropriateness, CON for Content, GRA for Grammar, andREL for Relevance)\nfor Topical-DSTC10 and Persona-DSTC10. The best score for each column is highlighted in bold. The second best\nis underlined.\nUnderstandability\" dimensions for turn-based\nevaluation, and \"Coherence, Consistency, Topic\nDepth, Diversity, Error Recovery, Flexibility,\nInformativeness, Inquisitiveness, Likability,\nOverall, and Understandability\" dimensions for\ndialogue-based evaluation. Nonetheless, only the\n\"Overall\" score is used for meta-evaluation in each\nscenario.\nBoth LLM-E VAL configurations, 0-5 and 0-100,\nconsistently display strong performance across the\ndatasets, highlighting their resilience and flexibil-\nity. The method’s capacity to accommodate differ-\nent scoring ranges while maintaining competitive-\nness against state-of-the-art evaluation techniques\ndemonstrates LLM-E VAL’s adaptability and ro-\nbustness. This establishes its value as an efficient\nand versatile evaluation solution in reference-free\nsettings.\n5 Analysis\n5.1 Different LLMs\nIn this section, we analyze the performance of\nLLM-E VAL when using different large language\nmodels for evaluation. Table 4 presents the Spear-\nman correlation coefficients between human rat-\nings and LLM-E VAL with various model con-\nfigurations and scoring ranges for the Topical-\nDSTC10 and Persona-DSTC10 datasets. We com-\npare the performance of LLM-E VAL when us-\ning different LLMs, such as Anthropic Claude,\nOpenAI ChatGPT , Anthropic Claude-instant ,\nand OpenAI GPT-3.5 1.\nAmong these models, Claude and ChatGPT are\noptimized for chat applications, while GPT-3.5 is\nnot. We observe that both Claude and ChatGPT\ngenerally achieve better performance across all di-\nmensions when compared to GPT-3.5. This sug-\ngests that using dialogue-optimized LLMs in the\nLLM-E VAL method leads to more accurate evalua-\ntion results in the context of open-domain conver-\nsations.\nMoreover, when comparing the Claude and\nChatGPT models, both models demonstrate com-\npetitive performance across different evaluation\ndimensions, with Claude slightly outperforming\nChatGPT in certain configurations.\n1Anthropic Claude (claude-v1.3), OpenAI ChatGPT\n(gpt-3.5-turbo-0301), Anthropic Claude-instant (claude-\ninstantv1.0), and OpenAI GPT-3.5 (text-davinci-003).\n53\nWe also analyze the performance of\nClaude-instant, a smaller version of Claude.\nAlthough it is not as competitive as its larger\ncounterpart, it still achieves reasonable perfor-\nmance in some cases. This implies that smaller\nmodels, while not optimal, can still be employed\nfor LLM-E VAL to a certain extent, possibly\nproviding a more resource-efficient option in\nspecific scenarios.\nIn conclusion, our analysis demonstrates that\ndialogue-optimized LLMs, such as Claude and\nChatGPT, yield better performance in the LLM-\nEVAL method for open-domain conversation eval-\nuation. Although smaller models like Anthropic\nClaude-instant may not achieve the best perfor-\nmance, they can still be considered for resource-\nlimited scenarios. Overall, the choice of LLMs\nin LLM-E VAL plays a crucial role in obtaining\naccurate evaluation results.\n5.2 Decoding Methods\nIn our experiments, we employ greedy decoding for\ngenerating responses using the Anthropic API with\nthe claude-v1.3 model. Greedy decoding selects\nthe token with the highest probability at each time\nstep during the generation process. However, other\ndecoding methods, such as nucleus sampling could\nbe employed in the LLM-E VAL method to explore\ntheir impact on the evaluation results.\nNucleus sampling, also known as top-p sam-\npling, samples tokens from the top- p most prob-\nable tokens at each time step, where p is a pre-\ndefined probability threshold. This method intro-\nduces some randomness into the generation pro-\ncess and could lead to more diverse and creative\nresponses.\nComparing the performance of Claude and\nClaude top_p = 0.9 in Table 4, we observe that\ngreedy decoding generally achieves better perfor-\nmance across all evaluation dimensions. This find-\ning suggests that using greedy decoding with the\nLLM-E VAL method provides more accurate and\nconsistent evaluation results compared to nucleus\nsampling.\nOne possible reason for this difference in perfor-\nmance is that greedy decoding tends to generate\nmore coherent and focused responses due to its de-\nterministic nature. In contrast, nucleus sampling\nintroduces randomness into the generation process,\nwhich may result in less focused or less relevant\nresponses, affecting the evaluation scores. Con-\nsequently, greedy decoding appears to be a more\nsuitable choice for the LLM-E VAL method.\n6 Conclusion\nIn this paper, we introduced LLM-E VAL, a unified\nmulti-dimensional automatic evaluation method for\nopen-domain conversations with large language\nmodels. The proposed method employs a single\nprompt along with a unified evaluation schema that\ncovers multiple dimensions of evaluation, such as\ncontent, grammar, relevance, and appropriateness.\nThis approach streamlines the evaluation process\nand eliminates the need for multiple prompts. Ex-\nperiments on various datasets demonstrated the ef-\nfectiveness and efficiency of LLM-E VAL, consis-\ntently outperforming most baselines and state-of-\nthe-art evaluation methods.\nAs future work, we plan to explore reinforce-\nment learning from LLMs feedback and investigate\nLLM-in-the-loop evaluation strategies as an alter-\nnative to human-in-the-loop methods. This will\nfurther enhance the applicability and performance\nof the LLM-E VAL method in various dialogue sys-\ntem evaluation scenarios.\nLimitations\nAlthough LLM-E VAL has shown promising results\nin assessing open-domain conversations, it is cru-\ncial to acknowledge its limitations.\nFirstly, the performance of our method relies\nheavily on the large language models underlying it,\nwhich may exhibit biases or generate unexpected\noutputs. If the language model misinterprets the\nevaluation schema or prompt instructions, it could\nlead to inaccurate evaluation scores.\nSecondly, the choice of LLM significantly in-\nfluences the evaluation results, as demonstrated in\nour analysis. While dialogue-optimized LLMs pro-\nduce better performance, this selection may limit\nLLM-E VAL’s applicability for particular tasks or\ndialogue systems.\nThirdly, our approach employs single-number\nscoring for each evaluation dimension, which may\nfail to capture the subtleties of human judgments,\nparticularly for subjective aspects like engagement,\ncreativity, or humor.\nLastly, the effectiveness of LLM-E VAL hinges\non the quality and clarity of the prompts and evalu-\nation schemas. Creating such prompts and schemas\nmay require domain expertise and knowledge of\nLLM behavior, posing challenges for non-experts.\n54\nTo overcome these limitations, future research\ncan focus on exploring alternative prompt designs,\nrefining evaluation schemas, and expanding the\nmethod to cover a wider range of evaluation dimen-\nsions and dialogue system types.\nEthics Statement\nWe acknowledge that there are potential ethical\nconcerns associated with the use of large language\nmodels in our evaluation method.\nA primary concern is the biases present in large\nlanguage models. These biases are introduced dur-\ning training, as the models learn from textual data\nthat may contain biased information, stereotypes,\nor misinformation. When using these biased mod-\nels for evaluation, it is possible that the evaluation\nscores produced by LLM-E VAL may reflect and\nperpetuate these biases, potentially leading to bi-\nased evaluations of dialogue system outputs. This\ncould, in turn, affect the development of future\ndialogue systems by encouraging biased behavior.\nTo mitigate this concern, researchers and devel-\nopers should be cautious when interpreting the\nevaluation results obtained through LLM-E VAL\nand consider potential biases in the large language\nmodels used. Moreover, future work could explore\ntechniques to debias language models or employ al-\nternative evaluation schemas that actively account\nfor biases in the evaluation process.\nAcknowledgements\nWe thank the reviewers for their insightful com-\nments. This work was financially supported by the\nYoung Scholar Fellowship Program by the National\nScience and Technology Council (NSTC) in Tai-\nwan, under Grants 111-2222-E-002-013-MY3 and\n111-2628-E-002-016.\nReferences\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan,\nNicholas Joseph, Saurav Kadavath, Jackson Kernion,\nTom Conerly, Sheer El Showk, Nelson Elhage, Zac\nHatfield-Dodds, Danny Hernandez, Tristan Hume,\nScott Johnston, Shauna Kravec, Liane Lovitt, Neel\nNanda, Catherine Olsson, Dario Amodei, Tom B.\nBrown, Jack Clark, Sam McCandlish, Chris Olah,\nBenjamin Mann, and Jared Kaplan. 2022. Train-\ning a helpful and harmless assistant with rein-\nforcement learning from human feedback. CoRR,\nabs/2204.05862.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nYi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and\nRuifeng Xu. 2023. Exploring the use of large lan-\nguage models for reference-free text quality eval-\nuation: A preliminary empirical study. CoRR,\nabs/2304.00723.\nJan Deriu, Álvaro Rodrigo, Arantxa Otegi, Guillermo\nEchegoyen, Sophie Rosset, Eneko Agirre, and Mark\nCieliebak. 2021. Survey on evaluation methods for\ndialogue systems. Artif. Intell. Rev., 54(1):755–810.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire. CoRR,\nabs/2302.04166.\nSarik Ghazarian, Johnny Wei, Aram Galstyan, and\nNanyun Peng. 2019. Better automatic evaluation\nof open-domain dialogue systems with contextual-\nized embeddings. In Proceedings of the Workshop\non Methods for Optimizing and Evaluating Neural\nLanguage Generation, pages 82–89, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nSarik Ghazarian, Ralph M. Weischedel, Aram Galstyan,\nand Nanyun Peng. 2020. Predictive engagement:\nAn efficient metric for automatic evaluation of open-\ndomain dialogue systems. In The Thirty-Fourth AAAI\nConference on Artificial Intelligence, AAAI 2020, The\nThirty-Second Innovative Applications of Artificial\nIntelligence Conference, IAAI 2020, The Tenth AAAI\nSymposium on Educational Advances in Artificial In-\ntelligence, EAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 7789–7796. AAAI Press.\nKarthik Gopalakrishnan, Behnam Hedayatnia, Qinglang\nChen, Anna Gottardi, Sanjeev Kwatra, Anu\nVenkatesh, Raefer Gabriel, and Dilek Hakkani-Tür.\n2019. Topical-chat: Towards knowledge-grounded\nopen-domain conversations. In Interspeech 2019,\n20th Annual Conference of the International Speech\nCommunication Association, Graz, Austria, 15-19\nSeptember 2019, pages 1891–1895. ISCA.\nChiori Hori and Takaaki Hori. 2017. End-to-end\nconversation modeling track in DSTC6. CoRR,\nabs/1706.07440.\n55\nLishan Huang, Zheng Ye, Jinghui Qin, Liang Lin, and\nXiaodan Liang. 2020. GRADE: Automatic graph-\nenhanced coherence metric for evaluating open-\ndomain dialogue systems. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 9230–9240,\nOnline. Association for Computational Linguistics.\nNaomi Kong-Vega, Mingxin Shen, Mo Wang, and\nLuis Fernando D’Haro. 2018. Subjective annotation\nand evaluation of three different chatbots WOCHAT:\nshared task report. In 9th International Workshop on\nSpoken Dialogue System Technology, IWSDS 2018,\nSingapore, April 18-20, 2018 , volume 579 of Lec-\nture Notes in Electrical Engineering, pages 371–378.\nSpringer.\nSeolhwa Lee, Heuiseok Lim, and João Sedoc. 2020.\nAn evaluation protocol for generative conversational\nsystems. CoRR, abs/2010.12741.\nZekang Li, Jinchao Zhang, Zhengcong Fei, Yang Feng,\nand Jie Zhou. 2021. Conversations are not flat: Mod-\neling the dynamic information flow across dialogue\nutterances. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 128–138, Online. Association for Computa-\ntional Linguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nChia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-\nworthy, Laurent Charlin, and Joelle Pineau. 2016.\nHow NOT to evaluate your dialogue system: An\nempirical study of unsupervised evaluation metrics\nfor dialogue response generation. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2122–2132, Austin,\nTexas. Association for Computational Linguistics.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. G-eval:\nNLG evaluation using GPT-4 with better human\nalignment. CoRR, abs/2303.16634.\nShikib Mehri and Maxine Eskenazi. 2020a. Unsuper-\nvised evaluation of interactive dialog with DialoGPT.\nIn Proceedings of the 21th Annual Meeting of the\nSpecial Interest Group on Discourse and Dialogue,\npages 225–235, 1st virtual meeting. Association for\nComputational Linguistics.\nShikib Mehri and Maxine Eskenazi. 2020b. USR: An\nunsupervised and reference free evaluation metric\nfor dialog generation. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 681–707, Online. Association for\nComputational Linguistics.\nShikib Mehri, Yulan Feng, Carla Gordon, Seyed Hos-\nsein Alavi, David Traum, and Maxine Eskenazi. 2022.\nInteractive evaluation of dialog track at DSTC9. In\nProceedings of the Thirteenth Language Resources\nand Evaluation Conference, pages 5731–5738, Mar-\nseille, France. European Language Resources Asso-\nciation.\nOpenAI. 2023. GPT-4 technical report. CoRR,\nabs/2303.08774.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nVitou Phy, Yang Zhao, and Akiko Aizawa. 2020. Decon-\nstruct to reconstruct a configurable evaluation metric\nfor open-domain dialogue systems. In Proceedings of\nthe 28th International Conference on Computational\nLinguistics, pages 4164–4178, Barcelona, Spain (On-\nline). International Committee on Computational Lin-\nguistics.\nHannah Rashkin, Eric Michael Smith, Margaret Li, and\nY-Lan Boureau. 2019. Towards empathetic open-\ndomain conversation models: A new benchmark and\ndataset. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5370–5381, Florence, Italy. Association for\nComputational Linguistics.\nAnanya B. Sai, Akash Kumar Mohankumar, Siddhartha\nArora, and Mitesh M. Khapra. 2020. Improving di-\nalog evaluation with a multi-reference adversarial\ndataset and large scale pretraining. Transactions of\nthe Association for Computational Linguistics, 8:810–\n827.\nJoão Sedoc, Daphne Ippolito, Arun Kirubarajan, Jai\nThirani, Lyle Ungar, and Chris Callison-Burch. 2019.\nChatEval: A tool for chatbot evaluation. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics (Demonstrations), pages 60–65, Minneapo-\nlis, Minnesota. Association for Computational Lin-\nguistics.\nEric Smith, Orion Hsu, Rebecca Qian, Stephen Roller,\nY-Lan Boureau, and Jason Weston. 2022. Human\nevaluation of conversations is an open problem: com-\nparing the sensitivity of various methods for eval-\nuating dialogue agents. In Proceedings of the 4th\nWorkshop on NLP for Conversational AI, pages 77–\n97, Dublin, Ireland. Association for Computational\nLinguistics.\nChongyang Tao, Lili Mou, Dongyan Zhao, and Rui Yan.\n2018. RUBER: an unsupervised method for auto-\nmatic evaluation of open-domain dialog systems. In\nProceedings of the Thirty-Second AAAI Conference\n56\non Artificial Intelligence, (AAAI-18), the 30th inno-\nvative Applications of Artificial Intelligence (IAAI-\n18), and the 8th AAAI Symposium on Educational\nAdvances in Artificial Intelligence (EAAI-18), New\nOrleans, Louisiana, USA, February 2-7, 2018, pages\n722–729. AAAI Press.\nOriol Vinyals and Quoc V . Le. 2015. A neural conver-\nsational model. CoRR, abs/1506.05869.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,\nand Denny Zhou. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nNeurIPS.\nYi-Ting Yeh, Maxine Eskenazi, and Shikib Mehri. 2021.\nA comprehensive assessment of dialog evaluation\nmetrics. In The First Workshop on Evaluations and\nAssessments of Neural Conversation Systems, pages\n15–33, Online. Association for Computational Lin-\nguistics.\nChen Zhang, Yiming Chen, Luis Fernando D’Haro,\nYan Zhang, Thomas Friedrichs, Grandee Lee, and\nHaizhou Li. 2021a. DynaEval: Unifying turn and\ndialogue level evaluation. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 5676–5689, Online. Association\nfor Computational Linguistics.\nChen Zhang, Luis Fernando D’Haro, Rafael E. Banchs,\nThomas Friedrichs, and Haizhou Li. 2020a. Deep\nAM-FM: toolkit for automatic dialogue evaluation.\nIn Conversational Dialogue Systems for the Next\nDecade - 11th International Workshop on Spoken\nDialogue Systems, IWSDS 2020, Madrid, Spain, 21-\n23 September, 2020, volume 704 of Lecture Notes in\nElectrical Engineering, pages 53–69. Springer.\nChen Zhang, João Sedoc, Luis Fernando D’Haro,\nRafael E. Banchs, and Alexander Rudnicky. 2021b.\nAutomatic evaluation and moderation of open-\ndomain dialogue systems. CoRR, abs/2111.02110.\nPengfei Zhang, Xiaohui Hu, Kaidong Yu, Jian Wang,\nSong Han, Cao Liu, and Chunyang Yuan. 2022.\nMME-CRS: multi-metric evaluation based on correla-\ntion re-scaling for evaluating open-domain dialogue.\nCoRR, abs/2206.09403.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you\nhave pets too? In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2204–2213,\nMelbourne, Australia. Association for Computational\nLinguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020b. Bertscore: Eval-\nuating text generation with BERT. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nA Prompt Templates\nBelow are the prompt templates used in our experi-\nments with LLM-E VAL. They provide examples\nof the natural language instructions used to define\nthe evaluation task and desired criteria, as well as\nthe format instructions that specify the structure\nand range of scores for each dimension.\nA.1 Evaluation Schema\nThe evaluation schema used inLLM-E VAL is a nat-\nural language instruction that defines the task and\nthe desired evaluation criteria. It covers multiple di-\nmensions of evaluation, such as content, grammar,\nrelevance, and appropriateness. An example of\nthe format instruction specifying the structure and\nrange of scores for each dimension is as follows:\nHuman: The output should be formatted as a\nJSON instance that conforms to the JSON\nschema below.\nAs an example, for the schema {\"properties\":\n{\"foo\": {\"title\": \"Foo\", \"description\": \"a\nlist of strings\", \"type\": \"array\", \"items\":\n{\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\nthe object {\"foo\": [\"bar\", \"baz\"]} is a\nwell-formatted instance of the schema.\nThe object {\"properties\": {\"foo\": [\"bar\",\n\"baz\"]}} is not well-formatted.\nHere is the output schema:\n{\"properties\": {\"content\": {\"title\":\n\"Content\", \"description\": \"content score\nin the range of 0 to 100\", \"type\":\n\"integer\"}, \"grammar\": {\"title\": \"Grammar\",\n\"description\": \"grammar score in the range\nof 0 to 100\", \"type\": \"integer\"}, \"relevance\":\n{\"title\": \"Relevance\", \"description\":\n\"relevance score in the range of 0 to 100\",\n\"type\": \"integer\"}, \"appropriateness\":\n{\"title\": \"Appropriateness\", \"description\":\n\"appropriateness score in the range of 0 to\n100\", \"type\": \"integer\"}}, \"required\":\n[\"content\", \"grammar\", \"relevance\",\n\"appropriateness\"]}\nA.2 Reference-based Turn-level Evaluation\nFor reference-based turn-level evaluation, the sin-\ngle prompt is designed to include the necessary\ndialogue context, the reference, and the target re-\nsponse that needs to be evaluated, along with the\nevaluation schema. An example prompt template\nfor evaluating a dialogue response with a human\nreference is:\n57\n{evaluation_schema}\nScore the following dialogue response\ngenerated on a continuous scale from\n{score_min} to {score_max}.\nContext: {context}\nReference: {reference}\nDialogue response: {response}\nA.3 Reference-free Turn-level Evaluation\nFor reference-free turn-level evaluation, the single\nprompt includes the dialogue context and the target\nresponse that needs to be evaluated, without requir-\ning a human reference. The evaluation schema is\nalso included in the prompt. An example prompt\ntemplate for evaluating a dialogue response without\na human reference is:\n{evaluation_schema}\nScore the following dialogue response\ngenerated on a continuous scale from\n{score_min} to {score_max}.\nContext: {context}\nDialogue response: {response}\nA.4 Dialogue-level Evaluation\nFor dialogue-level evaluation, the single prompt\nis designed to cover the entire dialogue instead of\nindividual turns. The evaluation schema is also in-\ncluded in the prompt. An example prompt template\nfor evaluating a dialogue is:\n{evaluation_schema}\nScore the following dialogue generated\non a continuous scale from {score_min}\nto {score_max}.\nDialogue: {dialog}\n58",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8260802030563354
    },
    {
      "name": "Conversation",
      "score": 0.721973180770874
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6301143169403076
    },
    {
      "name": "Schema (genetic algorithms)",
      "score": 0.5918501615524292
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.49977946281433105
    },
    {
      "name": "Ground truth",
      "score": 0.49755099415779114
    },
    {
      "name": "Adaptability",
      "score": 0.4842919707298279
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46521973609924316
    },
    {
      "name": "Unified Modeling Language",
      "score": 0.45760393142700195
    },
    {
      "name": "Process (computing)",
      "score": 0.45024797320365906
    },
    {
      "name": "Machine learning",
      "score": 0.34330153465270996
    },
    {
      "name": "Natural language processing",
      "score": 0.32071223855018616
    },
    {
      "name": "Programming language",
      "score": 0.10446354746818542
    },
    {
      "name": "Software",
      "score": 0.07598218321800232
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I16733864",
      "name": "National Taiwan University",
      "country": "TW"
    }
  ],
  "cited_by": 56
}