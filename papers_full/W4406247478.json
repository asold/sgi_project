{
  "title": "Patient- and clinician-based evaluation of large language models for patient education in prostate cancer radiotherapy",
  "url": "https://openalex.org/W4406247478",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A4289894506",
      "name": "Trapp, Christian",
      "affiliations": [
        "Ludwig-Maximilians-Universität München",
        "LMU Klinikum"
      ]
    },
    {
      "id": null,
      "name": "Schmidt-Hegemann, Nina",
      "affiliations": [
        "Ludwig-Maximilians-Universität München",
        "LMU Klinikum"
      ]
    },
    {
      "id": null,
      "name": "Keilholz, Michael",
      "affiliations": [
        "Ludwig-Maximilians-Universität München",
        "LMU Klinikum"
      ]
    },
    {
      "id": null,
      "name": "Brose, Sarah Frederike",
      "affiliations": [
        "Ludwig-Maximilians-Universität München",
        "LMU Klinikum"
      ]
    },
    {
      "id": null,
      "name": "Marschner, Sebastian N.",
      "affiliations": [
        "LMU Klinikum",
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A2752032179",
      "name": "Schönecker, Stephan",
      "affiliations": [
        "Ludwig-Maximilians-Universität München",
        "LMU Klinikum"
      ]
    },
    {
      "id": null,
      "name": "Maier, Sebastian H.",
      "affiliations": [
        "Ludwig-Maximilians-Universität München",
        "LMU Klinikum"
      ]
    },
    {
      "id": null,
      "name": "Dehelean, Diana-Coralia",
      "affiliations": [
        "Ludwig-Maximilians-Universität München",
        "LMU Klinikum"
      ]
    },
    {
      "id": null,
      "name": "Rottler, Maya",
      "affiliations": [
        "LMU Klinikum",
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": null,
      "name": "Konnerth, Dinah",
      "affiliations": [
        "Ludwig-Maximilians-Universität München",
        "LMU Klinikum"
      ]
    },
    {
      "id": "https://openalex.org/A2416935876",
      "name": "Belka Claus",
      "affiliations": [
        "Ludwig-Maximilians-Universität München",
        "Deutschen Konsortium für Translationale Krebsforschung",
        "LMU Klinikum"
      ]
    },
    {
      "id": "https://openalex.org/A3206954408",
      "name": "Corradini Stefanie",
      "affiliations": [
        "LMU Klinikum",
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": null,
      "name": "Rogowski, Paul",
      "affiliations": [
        "Ludwig-Maximilians-Universität München",
        "LMU Klinikum"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4386477610",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4387356888",
    "https://openalex.org/W6884310970",
    "https://openalex.org/W4321366933",
    "https://openalex.org/W4319341091",
    "https://openalex.org/W4385299173",
    "https://openalex.org/W4353016766",
    "https://openalex.org/W4387166123",
    "https://openalex.org/W4386593417",
    "https://openalex.org/W4384564606",
    "https://openalex.org/W3215379495",
    "https://openalex.org/W4323928271",
    "https://openalex.org/W4388765353",
    "https://openalex.org/W4388565819",
    "https://openalex.org/W4200535791",
    "https://openalex.org/W4390616164",
    "https://openalex.org/W1967390364",
    "https://openalex.org/W4366447635",
    "https://openalex.org/W4385723558",
    "https://openalex.org/W4385380523",
    "https://openalex.org/W4390611084",
    "https://openalex.org/W4400061171",
    "https://openalex.org/W4388343109",
    "https://openalex.org/W4399557121",
    "https://openalex.org/W4393529891",
    "https://openalex.org/W4401798900",
    "https://openalex.org/W4400080951",
    "https://openalex.org/W4390906300",
    "https://openalex.org/W4376130416",
    "https://openalex.org/W4382929886",
    "https://openalex.org/W4396899513",
    "https://openalex.org/W4386245324",
    "https://openalex.org/W4386913852",
    "https://openalex.org/W4221106857",
    "https://openalex.org/W4385242971",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W4365459718",
    "https://openalex.org/W4396672582",
    "https://openalex.org/W4392711451",
    "https://openalex.org/W4403645549"
  ],
  "abstract": null,
  "full_text": "ORIGINAL ARTICLE\nhttps://doi.org/10.1007/s00066-024-02342-3\nStrahlentherapie und Onkologie (2025) 201:333–342\nPatient- and clinician-based evaluation of large language models for\npatient education in prostate cancer radiotherapy\nChristian Trapp1 · Nina Schmidt-Hegemann 1 · Michael Keilholz 1 · Sarah Frederike Brose 1 ·\nSebastian N. Marschner 1 · Stephan Schönecker 1 · Sebastian H. Maier 1 · Diana-Coralia Dehelean 1 ·\nMaya Rottler1 · Dinah Konnerth 1 · Claus Belka 1,2,3 · Stefanie Corradini 1 ·P a u lR o g o w s k i1\nReceived: 30 July 2024 / Accepted: 18 November 2024 / Published online: 10 January 2025\n© The Author(s) 2025\nAbstract\nBackground This study aims to evaluate the capabilities and limitati ons of large language models (LLMs) for providing\npatient education for men undergoing ra diotherapy for localized prostate cancer, incorporating assessments from both\nclinicians and patients.\nMethods Six questions about deﬁnitive radiotherapy for prostat e cancer were designed based on common patient inquiries.\nThese questions were presented to different LLMs [ChatGPT-4, ChatGPT-4o (both OpenAI Inc., San Francisco, CA, USA),\nGemini (Google LLC, Mountain View, CA, USA), Copilot (Microsoft Corp., Redmond, W A, USA), and Claude (Anthropic\nPBC, San Francisco, CA, USA)] via the respective web in terfaces. Responses were evaluated for readability using the\nFlesch Reading Ease Index. Five radiation oncologists assessed the responses for relevance, correctness, and completeness\nusing a ﬁve-point Likert scale. Additionally, 35 prostate cancer patients evaluated the responses from ChatGPT-4 for\ncomprehensibility, accuracy, rel evance, trustworthiness, an d overall informativeness.\nResults The Flesch Reading Ease Index indicated that the responses from all LLMs were relatively difﬁcult to understand.\nAll LLMs provided answers that clinicians found to be generally relevant and correct. The answers from ChatGPT-4,\nChatGPT-4o, and Claude AI were also found to be complete. However, we found signiﬁcant differences between the per-\nformance of different LLMs regarding relevance and complete ness. Some answers lacked detail or contained inaccuracies.\nPatients perceived the information as easy to understand and relevant, with most expressing conﬁdence in the informa-\ntion and a willingness to use ChatGPT-4 for future medical ques tions. ChatGPT-4’s responses helped patients feel better\ninformed, despite the initially standardized information provided.\nConclusion Overall, LLMs show promise as a tool for patient education in prostate cancer radiotherapy. While improve-\nments are needed in terms of accuracy an d readability, positive feedback from clinic ians and patients suggests that LLMs\ncan enhance patient understanding and engage ment. Further research is essential to fully realize the potential of artiﬁcial\nintelligence in patient education.\nKeywords Artiﬁcial intelligence · Radiation oncology · ChatGPT · Patient information · AI\nAbbreviations\nAI Artiﬁcial intelligence\nASL Average sentence lengths\nASW Average number of syllables per word\n/envelopebackDr. med. Christian Trapp\nchristian.trapp@med.uni-muenchen.de\n1 Department of Radiation Oncology, University Hospital,\nLMU Munich, Marchioninistr. 15, 81377 Munich, Germany\n2 Bavarian Cancer Research Center (BZKF), Munich, Germany\n3 German Cancer Consortium (DKTK), Partner Site Munich,\nMunich, Germany\nChatGPT Chat Generative Pre-Trained Transformer\nLLM Large language model\nRI Readability index\nBackground\nIn recent years, the advancemen t of artiﬁcial intelligence\n(AI) and particularly the release of Chat Generative Pre-\nTrained Transformer (ChatGPT) by OpenAI (OpenAI Inc.,\nSan Francisco, CA, USA) in November 2022 has garnered\nwidespread global interest [ 1]. ChatGPT is a large language\nmodel (LLM) combined with a cloud-based generative AI\nK\n334 Strahlentherapie und Onkologie (2025) 201:333–342\nTable 1 Questions and responses from ChatGPT-4 (OpenAI Inc., San Francisco, CA, USA)\nQuestion ChatGPT-4’s response\nQuestion 1\nWhat is radiation\ntherapy for prostate\ncancer, and how is it\nperformed?\nResponse 1\nRadiation therapy for prostate cancer is a treatment in which high-energy rays are used to destroy cancer cells or slow\ndown their growth. There are two main types of radiation therapy:\n– External-beam radiation therapy (EBRT): In this approach, beams are directed from outside the body through the\nskin to target the prostate. Modern techniques such as intensity-modulated radiation therapy (IMRT) or image-\nguided radiation therapy (IGRT) enable precise targeting to spare surrounding healthy tissue.\n– Brachytherapy (internal radiation therapy): In this method, small radioactive seeds are implanted directly into the\nprostate, emitting radiation from within. The implementation depends on the type of radiation therapy but gener-\nally involves a series of sessions over several weeks\nQuestion 2\nWhat are the advan-\ntages of radiation\ntherapy compared to\nsurgery for localized\nprostate cancer?\nResponse 2\nThe advantages of radiation therapy may include:\n– Noninvasive: In contrast to surgery, radiation therapy does not require incisions or anesthesia.\n– Preservation of physical integrity: The prostate is preserved, which can be psychologically advantageous for some\nmen.\n– Shorter recovery time: The recovery time after radiation therapy is typically shorter than after surgery.\n– Lower risk of immediate severe complications: Like any surgical procedure, there are risks, but these risks tend to\nbe lower with radiation therapy\nQuestion 3\nWhat are the possible\nside effects of radiation\ntherapy for prostate\ncancer?\nResponse 3\nPossible side effects of radiation therapy may include:\n– Acute side effects: These may involve skin irritations in the treated area, fatigue, frequent urination, painful urina-\ntion, rectal discomfort, or minor bleeding.\n– Long-term side effects: Potential long-term issues coul d include erec tile dysfuncti on, urinary inc ontinence, and\nchanges in bowel habits, such as more frequent bowel movements or rectal bleeding\nQuestion 4\nHow does radiation\ntherapy affect the qual-\nity of life of patients\nduring and after treat-\nment?\nResponse 4\nRadiation therapy can impact quality of life both during and after treatment. Many patients can continue with their\nnormal activities during treatment but may experience fatigue and the acute side effects mentioned above. After treat-\nment, some side effects may persist or develop over time, affecting the overall quality of life. However, every patient\nis unique, and many men lead a full and active life after radiation therapy\nQuestion 5\nWhat precautions or\npreparations are nec-\nessary before radiation\ntherapy?\nResponse 5\nBefore radiation therapy, it may be necessary to:\n– Conduct a planning CT (computed tomography) to precisely determine the location of the prostate.\n– Follow a speciﬁc diet or empty the bowel to facilitate treatment.\n– Fill the bladder to position the prostate optimally for radiation.\n– Possibly implant markers into the prostate (especially in brachytherapy)\nQuestion 6\nWhat follow-up care is\nrequired after radiation\ntherapy?\nResponse 6\nPost-treatment care following radiation therapy may involve:\n– Regular follow-up appointments to assess the effectiveness of the treatment and monitor potential long-term ef-\nfects.\n– Periodic PSA (prostate-speciﬁc antigen) tests to monitor the treatment’s efﬁcacy.\n– Monitoring and, if necessary, treating side effects.\n– Support through nutr itional counseling, ps ychological assistance, and potentially physical therapies.\n– It is crucial to communicate with your treatment team to receive individualized recommendations based on your\nspeciﬁc situation\nchatbot service [ 2]. It generates human-like responses to\nwritten prompts using deep learning algorithms trained on\nextensive text datasets. Based on learned patterns and rules,\nthe model selects the most probable response in terms of co-\nherence and appropriateness regarding the context of input\ntext.\nIn healthcare, ChatGPT, among other LLMs, has shown\nconsiderable promise in clini cal, research, and educational\napplications [ 2]. Proposed clinical uses include optimizing\nradiology reports, creating patient discharge summaries, of-\nfering guidance on antimicrobial use, and supporting clin-\nical decision-making [ 3–6]. Furthermore, by delivering in-\nformation to personalized questions, ChatGPT may con-\ntribute to a more informed and empowered patient com-\nmunity [ 7–10]. However, ChatGPT lacks critical thinking\nand relies on data only up to 2023, potentially leading to\nmisinformation or information harmful to users [ 11].\nProstate cancer ranks as the most prevalent cancer among\nmen, with a lifetime prevalence of approximately 13% [ 12].\nRadiotherapy is an important component in the treatment\nof prostate cancer, offering res ults comparable to surgery in\nthe deﬁnitive setting [ 13]. Even though patients generally\nexpress satisfaction with radiotherapy care [ 14], technolog-\nical innovations, such as mobile health, have already proven\nbeneﬁcial and feasible for enhancing radiotherapy care [ 15].\nGiven the signiﬁcant implicati ons of a prostate cancer diag-\nK\nStrahlentherapie und Onkologie (2025) 201:333–342 335\nnosis and the pivotal role of intricate technologies like ra-\ndiotherapy, patients seek comprehensive and readily avail-\nable information online [ 16]. Tools like ChatGPT could\npossibly offer this information. However, the use of Chat-\nGPT for patient education in radiation oncology is still in\nits early stages and its potentia l beneﬁts for prostate cancer\npatients undergoing radiothera py remain largely theoretical\ndue to the lack of signiﬁcant studies [ 17].\nTherefore, this study aims to evaluate the capabilities\nand limitations of employing ChatGPT and other LLMs\nfor patient education in men treated with radiotherapy for\nlocalized prostate cancer. Alongside a clinician-led evalua-\ntion assessing response relevance, accuracy, and complete-\nness of information from different LLMs, we conducted\na patient-led assessment to inv estigate ChatGPT-4’s perfor-\nmance in this educational context.\nMethods\nStudy design\nWe designed six questions regarding deﬁnitive radiother-\napy of prostate cancer based on questions frequently asked\nby patients during consultations in our own clinical ex-\nperience, covering general inf ormation about radiotherapy,\nthe comparison with surgery, adverse effects, inﬂuence on\nquality of life, the preparations before radiation treatment,\nand follow-up. The questions were presented to ChatGPT-4\n(version as of 3 November 2023) via the web interface pro-\nvided by OpenAI using the prompt: “I am a patient with\nprostate cancer and seek answers to the following ques-\ntions.” Table 1 shows the questions and the responses given\nby ChatGPT-4. The individual responses were preceded by\nthe sentence “I am sorry to hear about your diagnosis. I hope\nthe following information can help answer some of your\nquestions.” by ChatGPT-4. Additionally, the chatbot ended\nits response with the sentence: “It is important that you dis-\ncuss all your questions and concerns with your oncologist,\nwho can provide you with individual advice based on your\nspeciﬁc situation.” Questions and responses were provided\nin German and translated for this publication. To ensure the\nconsistency of the responses, the questions were posed to\nall LLMs ﬁve times on the same day using the same prompt,\neach time in a new chat, to eliminate any potential memory\nor learning effects. The authors CT and PR then compared\nthe ﬁve answers, but no relevant differences were found.\nAs part of the reviewing process, we asked the same\nquestions in September 2024 to other popular LLMs [Gem-\nini (version 1.5 Flash, Google LLC, Mountain View, CA,\nUSA), Copilot (Microsoft Corp., Redmond, W A, USA) and\nClaude (version 3.5 Sonnet, Anthropic PBC, San Francisco,\nCA, USA) each in the version as of 10 September 2024] as\nwell as to the now improved and freely accessible successor\nversion of ChatGPT, ChatGPT-4o (version as of 10 Septem-\nber 2024). Since including all the responses would exceed\nthe scope of this manuscript, the complete answers from all\nother LLMs can be found in the Appendix.\nThe quality of the responses was evaluated via the fol-\nlowing various methods:\nEvaluation of readability\nFirstly, readability was examined using the Flesch Reading\nEase Index, adapted for German texts according to Amstad\n[18, 19]. The Flesch Reading Ease Index serves as a nu-\nmerical measure of a text’s readability. The index is de-\nrived using the following formula. RI = 180 – ASL – (58.5 ×\nASW), where ASL is the average sentence length, ASW\nis the average number of syllables per word, and RI is the\nreadability index. The higher the value, the more easily un-\nderstandable the text is. For example, a text with values\nfrom 0 to 30 is deemed difﬁcult and primarily understand-\nable by academics, whereas a t ext with values from 60 to\n70 is classiﬁed as moderately difﬁcult and understandable\nfor 13- to 15-year-old students.\nClinicians’ evaluation\nSecondly, the responses from all LLMs were evaluated in-\ndependently by ﬁve radiation oncologists (CT, NSH, SM,\nSTS, PR) with 7–12 years of experience. Evaluation was\nperformed for each response regarding relevance, correct-\nness, and completeness using an ordinal ﬁve-point Likert\nscale, where a score of 1 indicated no agreement and a score\nof 5 indicated complete agreement with the statements that\nthe responses were relevant, correct, or complete, respec-\ntively. Respondents also had the opportunity to supplement\ntheir responses with additional comments.\nPatients’ evaluation\nLastly, the question-and-answer pairs from ChatGPT-4\nwere provided to 35 consecutive prostate cancer patients at\ntheir ﬁrst follow-up appointment 3 months after deﬁnitive\nradiotherapy. The patients were asked to evaluate ChatGPT-\n4’s performance in terms of c omprehensibility, accuracy,\nrelevance, and trustworthiness using an ordinal ﬁve-point\nLikert scale. Furthermore, patients were asked whether\nthe provided information would have helped them to feel\nbetter informed and whether they would use ChatGPT-4\nfor future medical questions.\nA graphical depiction of the study design is shown in\nFig. 1.\nK\n336 Strahlentherapie und Onkologie (2025) 201:333–342\nFig. 1 Depiction of the study\nworkﬂow\nEthical approval\nThe local ethics committee of the LMU approved the study\nprotocol in August 2023 (approval number 23-0742), and\nthe study was conducted in accord ance with the Declaration\nof Helsinki. All patients provided signed written consent to\nparticipating in the study.\nStatistical analysis\nData are reported using descriptive statistics with me-\ndian and mean values and standa rd deviations. Descrip-\ntive statistical analyses we re performed using Microsoft\nOfﬁce Excel (version 2016; Microsoft Corp, Redmond,\nW A, USA), while the comparison of clinicians’ evaluation\nacross the different LLMs was conducted using IBM SPSS\n(version 28; IBM Corp, Armonk, NY , USA). To compare\nthe LLMs regarding relevance, correctness, and complete-\nness, we used the Kruskal–Wallis test and the respective\nDunn–Bonferroni post-hoc tests.\nResults\nEvaluation of readability\nThe calculated Flesch Reading Ease Index values for the\nresponses from the different LLMs are shown in Table 2.\nThe results indicate that the answers from all LLMs were\nconsidered rather difﬁcult, with the answers from Gem-\nini being the least difﬁcult and the results from ChatGPT-\nTable 2 Flesch Reading Ease Index of the answers from the different large language models\nLLM ChatGPT-4 ChatGPT-4o Gemini Copilot Claude AI\nFlesch Reading Ease Index 29 24 39 28 31\n4o being the most difﬁcult. To evaluate whether ChatGPT\ncan generate more easily understandable responses upon re-\nquest, ChatGPT-4o was also prompted to provide answers\nin simpler language. As a result, the Flesch Reading Index\nimproved from 24 to 44.\nClinicians’ evaluation\nThe review of the responses from all LLMs by ﬁve radia-\ntion oncologists is shown in Figs. 2, 3,a n d 4 and Tables 3\nand 4. Overall, the reviewers agreed that the responses from\nall LLMs were relevant (range 4.2–4.7) and correct (range\n3.8–4.5). Furthermore, reviewers agreed that answers from\nChatGPT-4, ChatGPT-4o, an d Claude AI were also com-\nplete (4.0, 3.9, and 4.2, respectively), while they were neu-\ntral for the answers from Copilot and Gemini (3.2 and\n2.8, respectively). All in all, there were signiﬁcant differ-\nences between the LLMs regarding relevance and complete-\nness. However, the pairwise comparisons between the single\nLLMs showed a signiﬁcant difference only between Claude\nAI and Gemini regarding completeness.\nThe detailed ratings of the single responses from the\ndifferent LLMs can be found in the Appendix. When re-\nviewing the individual responses, it is noticeable that none\nof the LLMs provided an answer that was rated as irrelevant\nby any of the reviewers.\nRegarding correctness, there were a few responses (es-\npecially to questions 5 and 6 on the topics of preparation\nand follow-up) that were rated as incorrect by some review-\ners. ChatGPT-4o was the only LLM without any incorrect\nrating by any of the reviewers. ChatGPT-4 (question 5),\nK\nStrahlentherapie und Onkologie (2025) 201:333–342 337\n10% 10% 10% 13%\n27%\n30%\n20%\n57% 47%\n73%\n67%\n70%\n33%\n40%\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nChatGPT-4 ChatGPT-4o Claude AI Copilot Gemini\nRelevance\n3 4 5\nFig. 2 Clinicians’ overall rating of the answers regarding their rele-\nvance. A ﬁve-point Likert scale was used, with 1 representing “very\nirrelevant” and 5 representing “very relevant”\n3% 3% 3%\n10%\n10%\n7%\n17%\n37%33% 47%\n37%\n40%\n20%\n53%\n47%\n60%\n40%\n33%\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nChatGPT-4 ChatGPT-4o Claude AI Copilot Gemini\nCorrectness\n2 3 4 5\nFig. 3 Clinicians’ overall rating of the answers regarding their correct-\nness. A ﬁve-point Likert scale was used, with 1 representing “very cor-\nrect” and 5 representing “very incorrect”\nClaude AI (question 5), and Copilot (question 6) provided\none answer that was rated as incorrect by one reviewer and\nGemini provided 3 answers that were rated as incorrect by\none (question 3 and 5) or two reviewers (question 6). As\nincorrect facts in the responses, the following were noted:\nthe claim that small tattoos are applied to the body be-\nfore radiation therapy (Claude AI), that regular imaging is\npart of follow-up care (Copilot, Gemini), that transrectal\n7%\n13%\n3% 7%\n20%\n27%17%\n13% 10%\n47%\n43%\n33%\n73%\n40%\n30%\n23%\n37%\n10%\n43%\n3%\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nChatGPT-4 ChatGPT-4o Claude AI Copilot GeminiCompleteness\n1 2 3 4 5\nFig. 4 Clinicians’ overall rating of the answers regarding their com-\npleteness. A ﬁve-point Likert scale was used, with 1 representing “very\ncomplete” and 5 representing “very incomplete”\nultrasound is typically performed before radiation therapy\n(Gemini), and that marker implantation is primarily used\nin brachytherapy (ChatGPT-4). Nevertheless, when look-\ning at the overall rating of these answers, they were rated\nas correct or at least as neutral (range 2.6–4.0).\nRegarding completeness, there were a few responses that\nwere rated as incomplete by some reviewers. Gemini and\nCopilot provided mostly answers that were found to be in-\ncomplete by one or two of the reviewers (questions 1–3\nand 5–6). However, the overall rating of these answers was\ncomplete or at least neutral (range 2.6–3.4), with the excep-\ntion of Gemini’s answer to question 5 (preparations) which\nwas rated as incomplete (2.4 , SD 0.5), because it was the\nonly answer not mentioning bladder and bowel preparation.\nFurthermore, ChatGPT-4 (questions 1 and 2), ChatGPT-4o\n(question 2), and Claude AI (question 2) provided a few\nresponses that were rated as incomplete by one to three re-\nviewers but were rated overall as complete or neutral (range\n2.8–3.6).\nPatients’ evaluation\nThirty-ﬁve consecutive patients treated with deﬁnitive ra-\ndiotherapy for prostate cancer between July 2023 and April\n2024 at the University Hospital of Munich (LMU) were\nqueried at their ﬁrst follow-up examination 3 months after\nradiotherapy between November 2023 and June 2024.\nThe median age of the patients was 73 years (range\n57–85 years), and the treatment consisted of moderately\nhypofractionated radiation therapy of the prostate, ultrahy-\npofractionated radiation therapy of the prostate, and mod-\nK\n338 Strahlentherapie und Onkologie (2025) 201:333–342\nTable 3 Clinicians’ overall\nrating of the answers from the\ndifferent large language models\nregarding their relevance,\ncorrectness, and completeness\nRelevance (mean, SD) Correctness (mean, SD) Completeness (mean, SD)\nChatGPT-4 4.7 (0.4) 4.4 (0.8) 4.0 (1.0)\nChatGPT-4o 4.6 (0.6) 4.4 (0.6) 3.9 (0.6)\nClaude AI 4.6 (0.7) 4.5 (0.7) 4.2 (0.9)\nCopilot 4.2 (0.6) 4.2 (0.8) 3.2 (0.8)\nGemini 4.3 (0.7) 3.8 (1.0) 2.8 (0.9)\nSigniﬁcance p = 0.021 p = 0.176 p = 0.002\nA ﬁve-point Likert scale was used, with 1 representing “very irrelevant”, “very incorrect” or “very incom-\nplete” and 5 representing “very relevant”, “very correct” or “very complete”\nTable 4 Results of the\nDunn–Bonferroni post-hoc\ntests\nRelevance Correctness Completeness\nCopilot vs. Gemini p =1 . 0 p =1 . 0 p =1 . 0\nCopilot vs. Claude AI p = 0.628 p =1 . 0 p = 0.093\nCopilot vs. ChatGPT-4o p = 0.408 p =1 . 0 p = 0.520\nCopilot vs. ChatGPT-4 p = 0.060 p = 0.966 p = 0.412\nGemini vs. Claude AI p =1 . 0 p = 0.238 p = 0.008\nGemini vs. ChatGPT-4o p = 0.677 p =1 . 0 p = 0.073\nGemini vs. ChatGPT-4 p = 0.114 p =1 . 0 p = 0.054\nClaude AI vs. ChatGPT-4o p =1 . 0 p =1 . 0 p =1 . 0\nClaude AI vs. ChatGPT-4 p =1 . 0 p =1 . 0 p =1 . 0\nChatGPT-4o vs. ChatGPT-4 p =1 . 0 p =1 . 0 p =1 . 0\nTable 5 Patients’ ratings of the statements\nStatement\nnumber\nStatement Mean\n(SD)\nRatings on Likert scale, n (%)\n5 4 321\nStatement 1 The information provided by ChatGPT was easy\nto understand\n4.5 (0.60) 19 (54) 14 (40) 2 (6) 0 0\nStatement 2 The information provided by ChatGPT was clear\nand did not contain medical terms that were\ndifﬁcult to understand\n4.4 (0.90) 20 (57) 10 (29) 4 (11) 0 1 (3)\nStatement 3 The information provided by ChatGPT was\naccurate and relevant to the topic of prostate\nradiotherapy\n4.3 (0.75) 17 (49) 14 (40) 3 (9) 1 (3) 0\nStatement 4 The information provided by ChatGPT matches\nmy experiences\n4.3 (0.71) 16 (46) 16 (46) 2 (6) 1 (3) 0\nStatement 5 I have conﬁdence in the information I received\nfrom ChatGPT\n4.2 (0.86) 16 (46) 11 (31) 7 (20) 1 (3) 0\nStatement 6 The information provided by ChatGPT would\nhave helped me feel more informed about\nprostate radiotherapy\n4.2 (0.96) 16 (46) 12 (34) 4 (11) 3 (9) 0\nStatement 7 I would use the ChatGPT search for future medi-\ncal questions as well\n4.0 (0.88) 10 (29) 16 (46) 8 (23) 0 1 (3)\nA “5” on the ﬁve-point Likert scale means that a statement is felt to be true, while a “1” means that a statement is perceived as not true\nerately hypofractionated radiation therapy of the prostate\nplus radiation therapy of the lymphatic drainage pathways\nin 80%, 11%, and 9%, respectively.\nThe patients’ evaluations of seven statements regard-\ning the information provided by ChatGPT-4 are shown in\nTable 5 and Fig. 5. In general, the patients agreed that\nthe information was easy to understand (94%) and did\nnot contain medical terms that were difﬁcult to understand\n(86%). Furthermore, the vast majority of patients agreed\nthat the information was accurate and relevant (89%) and\nmatched their personal experience (91%). Most patients\n(76%) agreed that they have conﬁdence in the information\nreceived from ChatGPT-4. Fur thermore, 80% said that they\nwould have felt better informed with the information pro-\nvided by ChatGPT-4. Consequently, most patients (77%)\nwould use ChatGPT-4 for future medical questions. How-\nK\nStrahlentherapie und Onkologie (2025) 201:333–342 339\nFig. 5 Patients’ overall evalua-\ntion of the statements. A ﬁve-\npoint Likert scale was used, with\n1 representing “not true” and\n5 representing “true”\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nThe informa/g415on\nprovided by\nChatGPT was\neasy to\nunderstand.\nThe informa/g415on\nprovided by\nChatGPT\nmatches my\nexperiences.\nThe informa/g415on\nprovided by\nChatGPT was\nclear and did not\ncontain medical\nterms that were\ndiﬃcult to\nunderstand.\nThe informa/g415on\nprovided by\nChatGPT was\naccurate and\nrelevant to the\ntopic of prostate\nradiotherapy.\nThe informa/g415on\nprovided by\nChatGPT would\nhave helped me\nfeel more\ninformed about\nprostate\nradiotherapy.\nI have\nconﬁdence in\nthe informa/g415on I\nreceived from\nChatGPT.\nI would use the\nChatGPT search\nfor future\nmedical\nques/g415ons as\nwell.\n5 4 3 2 1\never, 26% were neutral or did not agree regarding the last\nstatement.\nDiscussion\nThis study investigated the beneﬁts of employing LLMs for\npatient education in men undergoing radiotherapy for lo-\ncalized prostate cancer. Clinicians evaluated the quality of\nresponses from ﬁve different LLMs to typical patient ques-\ntions regarding deﬁnitive radiotherapy for prostate cancer.\nMoreover, the responses of ChatGPT-4 were also evalu-\nated by patients. Clinicians’ evaluations underscored the\nrelevance, correctness, and completeness of most of the\nresponses. However, speciﬁc responses were critiqued for\nlacking certain details or containing inaccuracies as evi-\ndenced by the incorrect statem ents presented in the results\nsection. This is particularly relevant in oncology, where\nmisinformation, even at low rates, can have severe con-\nsequences for patients. All in all, we found signiﬁcant dif-\nferences between the performance of the different LLMs\nregarding relevance and compl eteness, while there were no\nsigniﬁcant differences regarding correctness.\nThese ﬁndings align with existing data, despite method-\nological differences in the conducted studies. The appro-\npriateness and accuracy of in formation given by ChatGPT\nto uro-oncological topics were generally rated as moderate\nto high [ 20–24]. This was also observed for queries related\nto radiotherapy [ 25–27]. The study of Alasker et al. com-\npared ChatGPT-3.5 with ChatGPT-4 and Google Bard, the\npredecessor of Google Gemini, regarding their responses to\nprostate cancer questions and found, overall, accurate, com-\nprehensive, and easily readable responses. Similar to our\nstudy, the Google LLM provided easier-to-read responses\n[28]. In another study evaluating LLM responses to prostate\ncancer questions there were si gniﬁcant differences between\nChatGPT-3.5, Microsoft Copilot, and Google Gemini [ 29].\nHowever, several authors found that the quality of LLMs’\nresponses declined with increasing speciﬁcity and complex-\nity of the questions [ 22, 23, 30]. This decline is attributable\nto the models being trained on general internet texts rather\nthan on specialized medical data, resulting in a lack of spe-\ncialized knowledge. Additionally, ChatGPT uses real-time\ninformation from the internet only upon speciﬁc request\nwhen generating responses, and information is otherwise\nbased on data from up to 2023. Another criticism of LLMs\nis the occurrence of so-called “hallucinations” (also referred\nto as “fact fabrication” to avoid inappropriate anthropo-\nmorphisms) [ 2]. In such cases, the LLMs invent incorrect\ninformation and present it as factual truth. In our study, in-\naccuracies were primarily due to imprecise or insufﬁciently\ndifferentiated responses from the LLMs. For example, stat-\ning that marker implantation particularly increases targeting\naccuracy in brachytherapy. While this is true for EBRT, it\nis incorrect for brachytherapy. We would not classify this\nas a “hallucination.” Nevertheless, the response is incorrect\nand could cause confusion for patients.\nFurthermore, because the LLMs generate responses\nanew each time, the answers for the same prompt are\npotentially not identical. However, in our queries, the dif-\nferences between responses from the LLMs were only\nmarginal and were not deemed relevant by CT and PR.\nNonetheless, other studies have noted signiﬁcant variability\nin responses between iterations [ 31, 32].\nAs far as we know, the performance of LLMs has been\nevaluated in previous studies exclusively by clinicians or\ninvestigators. In our study, we also examined the patients’\nperspective. In summary, patients found the information\nK\n340 Strahlentherapie und Onkologie (2025) 201:333–342\nto be relevant to their experiences and accurate regarding\nprostate radiotherapy. Most p atients expressed conﬁdence\nin the information received and stated that it would have\nhelped them to feel more informed about their treatment.\nHowever, prior to their treatment, all patients were informed\nabout the therapy using a standardized information sheet.\nTherefore, the information from ChatGPT-4 was not new;\nrather, the different and more active engagement with the\ninformation might have led to the patients feeling better\ninformed.\nAdditionally, most patients indicated a willingness to use\nChatGPT for future medical inquiries. However, 26% were\nneutral or did not agree regarding this statement. Interest-\ningly, 94% found ChatGPT’s responses to be easy to un-\nderstand. This contrasts with our readability analysis, which\nrevealed that the text generated by ChatGPT may be chal-\nlenging for some readers. This was also demonstrated by\nother studies [ 20, 27]. A possible reason for this ﬁnding\ncould be that we surveyed patients after their treatment,\nwhen they were already familiar with the topic and termi-\nnology. In this context it is also noteworthy that responses\nprovided by the LLMs may also vary depending on the\nprompts. For example, the responses’ quality may increase\nwhen asking the LLM to also take into account results of\nan internet search, as is possi ble for ChatGPT, Gemini, and\nCopilot. Another possibility to enhance utility of the LLMs\nin this context can be the use of modiﬁed prompts. For\nexample, LLMs can be asked to answer in a very com-\nplete or accurate way or to direct the responses to a spe-\nciﬁc audience. Hershenhouse et al. prompted ChatGPT to\nrephrase the answers for medical laypersons, which resulted\nin more readable answers r egarding prostate cancer [ 33]. In\nour study, prompting ChatGPT-4 to answer the questions\nin an easy-to-understand way also improved the readability\nof its responses.\nAlongside the positive aspects of using LLMs for pa-\ntient information, it is crucial to recognize their limitations,\nincluding the risk of incorr ect answers already mentioned\nabove. Ethical concerns as well as security and privacy is-\nsues are frequently raised [ 23, 34–36]. Another common\ncriticism of LLMs is their lack of human touch and empa-\nthy [ 37]. However, a study comparing responses from clini-\ncians and ChatGPT to patient questions posted in an online\nforum found the chatbot’s answers to be signiﬁcantly more\nempathetic [ 38]. Furthermore, ChatGPT does not necessar-\nily aim to replace the clinician, as prophesied and feared in\nsome articles [ 5, 39]. Instead, it could serve as a source of\nsupplementary information before or after a medical con-\nsultation, as was the case in the setting of our study.\nOur study has some limitations that warrant considera-\ntion. Firstly, questions were formulated by the study team\nand not by actual patients, potentially limiting the represen-\ntation of diverse clinical scen arios and increasing the risk of\ninaccurate responses due to patients providing incomplete\nor incorrect information. Secondly, queries and responses\nwere in German, which may affect the generalizability of\nﬁndings, as the performance of ChatGPT could vary across\nother languages. Thirdly, there are no standardized and val-\nidated criteria to assess the accuracy and reliability of AI-\ngenerated responses. Fourthly, our study initially focused on\nChatGPT because it stood as the most widely used LLM\nin practice and had consistently demonstrated superior re-\nsponse quality compared to other LLMs for medical topics\n[16, 40, 41]. A few months later, we added the comparison\nwith other LLMs as part of the reviewing process, so that\nthe timing of the questions to ChatGPT-4 does not coincide\nwith the questions to the other LLMs. All in all, it is worth\nnoting that a study like this can only make a statement at\na speciﬁc point in time. In the dynamic development of\nLLMs, results can change quickly due to new versions or\nmodels.\nConclusion\nLarge language models show promise as a valuable tool\nfor patient education in prostate cancer radiotherapy. While\nimprovements are needed to ensure accuracy and readabil-\nity, the overall positive feedback from clinicians and pa-\ntients suggests that LLMs like ChatGPT have the potential\nto enhance patient understanding and engagement in their\ntreatment journey. Continue d research and development in\nthis area are essential for harnessing the full potential of AI\nin patient education.\nSupplementary Information The online version of this article ( https://\ndoi.org/10.1007/s00066-024-02342-3) contains supplementary mate-\nrial, which is available to authorized users.\nFunding Open Access funding enabled and organized by Projekt\nDEAL.\nDeclarations\nConﬂict of interest C. Trapp, N. Schmidt-Hegemann, M. Keilholz,\nS.F. Brose, S.N. Marschner, S. Schönecker, S.H. Maier, D.-C. Dehe-\nlean, M. Rottler, D. Konne rth, C. Belka, S. Corradini, and P . Rogowski\ndeclare that they have no competing interests.\nEthical standards All procedures performed in studies involving hu-\nman participants or on human tissue were in accordance with the eth-\nical standards of the institutional and/or national research committee\nand with the 1975 Helsinki declaration and its later amendments or\ncomparable ethical standards. Informed consent was obtained from all\nindividual participants included in the study.\nOpen Access This article is licensed under a Creative Commons At-\ntribution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long as\nyou give appropriate credit to the original author(s) and the source, pro-\nvide a link to the Creative Commons licence, and indicate if changes\nK\nStrahlentherapie und Onkologie (2025) 201:333–342 341\nwere made. The images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included\nin the article’s Creative Commons licence and your intended use is not\npermitted by statutory regulation or ex ceeds the perm itted use, you will\nneed to obtain permission directly from the copyright holder. To view\na copy of this licence, visit http://creativecommons.org/licenses/by/4.\n0/.\nReferences\n1. Temsah MH, Altamimi I, Jamal A, Alhasan K, Al-Eyadhy A (2023)\nChatGPT Surpasses 1000 Publications on PubMed: Envisioning the\nRoad Ahead. Cureus 15(9):e44769. https://doi.org/10.7759/cureus.\n44769\n2. Thirunavukarasu AJ, Ting DSJ, Elangovan K, Gutierrez L, Tan TF,\nTing DSW (2023) Large language models in medicine. Nat Med\n29(8):1930–1940. https://doi.org/10.1038/s41591-023-02448-8\n3. Jeblick K, Schachtner B, Dexl J et al (2023) ChatGPT makes\nmedicine easy to swallow: an exploratory case study on simpliﬁed\nradiology reports. Eur Radiol 34(5):2817–2825. https://doi.org/10.\n1007/s00330-023-10213-1\n4. Patel SB, Lam K (2023) ChatGPT: the future of discharge sum-\nmaries? Lancet 5(3):21–23. https://doi.org/10.1016/S2589-7500\n5. Howard A, Hope W, Gerada A (2023) ChatGPT and antimicrobial\nadvice: the end of the consulting infection doctor? Lancet Infect Dis\n23(4):405–406. https://doi.org/10.1016/S1473-3099(23)00113-5\n6. Rao A, Kim J, Kamineni M et al (2023) Evaluating ChatGPT as an\nAdjunct for Radiologic Decision-Making. medRxiv 7:2023.02.02.\n23285399. https://doi.org/10.1101/2023.02.02.23285399\n7. ye SC, Li H, Liu XL et al (2023) Appropriateness and Comprehen-\nsiveness of Using ChatGPT for Perioperative Patient Education in\nThoracic Surgery in Different Language Contexts: Survey Study.\nInteract J Med Res 12:e46900. https://doi.org/10.2196/46900\n8. Yeo YH, Samaan JS, Ng WH et al (2023) Assessing the perfor-\nmance of ChatGPT in answering questions regarding cirrhosis and\nhepatocellular carcinoma. Clin Mol Hepatol 29(3):721–732. https://\ndoi.org/10.3350/cmh.2023.0089\n9. Alan R, Alan BM (2023) Utilizing ChatGPT-4 for Providing In-\nformation on Periodontal Disease to Patients: A DISCERN Qual-\nity Analysis. Cureus 15(9):e46213. https://doi.org/10.7759/cureus.\n46213\n10. Nielsen JPS, von Buchwald C, Grønhøj C (2023) Validity of the\nlarge language model ChatGPT (GPT 4) as a patient information\nsource in otolaryngology by a variety of doctors in a tertiary otorhi-\nnolaryngology department. Acta Otolaryngol 143(9):779–782.\nhttps://doi.org/10.1080/00016489.2023.2254809\n11. Whiles BB, Bird VG, Canales BK, DiBianco JM, Terry RS (2023)\nCaution! AI Bot Has Entered the Patient Chat: ChatGPT Has Limi-\ntations in Providing Accurate Urologic Healthcare Advice. Urology\n180:278–284. https://doi.org/10.1016/j.urology.2023.07.010\n12. Erdmann F, Spix C, Katalinic A et al (2021) Krebs in Deutschland\nfür 2017/2018. Robert Koch Institut https://doi.org/10.25646/8353\n13. Hamdy FC, Donovan JL, Lane JA et al (2023) Fifteen-Year Out-\ncomes after Monitoring, Surgery, or Radiotherapy for Prostate\nCancer. N Engl J Med 388(17):1547–1558. https://doi.org/10.\n1056/NEJMoa2214122\n14. Fabian A, Rühle A, Domschikowski J et al (2024) Satisfaction with\nradiotherapy care among cancer patients treated in Germany-sec-\nondary analysis of a large multicenter study. Strahlenther Onkol\n200(6):487–496. https://doi.org/10.1007/s00066-023-02176-5\n15. Schunn FA, El Shaﬁe RA, Kronsteiner D et al (2024) Oncologic\ntreatment support via a dedicated mobile app: a prospective feasibil-\nity evaluation (OPTIMISE-1). Strahlenther Onkol 200(6):475–486.\nhttps://doi.org/10.1007/s00066-023-02166-7\n16. Jia X, Pang Y , Liu LS (2021) Online health information seeking\nbehavior: A systematic review. Healthcare 9(12):1740. https://doi.\norg/10.3390/healthcare9121740\n17. Moll M, Heilemann G, Georg D, Kauer-Dorner D, Kuess P (2024)\nThe role of artiﬁcial intelligence in informed patient consent\nfor radiotherapy treatments—a case report. Strahlenther Onkol\n200(6):544–548. https://doi.org/10.1007/s00066-023-02190-7\n18. Flesch R (1948) A new readab ility yardstick. J Appl Psychol\n32(3):221–233. https://doi.org/10.1037/h0057532\n19. Amstad T (1978) Wie verständlich sind unsere Zeitungen? Studen-\nten-Schreib-Service\n20. Zhu L, Mou W, Chen R (2023) Can the ChatGPT and other large\nlanguage models with internet-connected database solve the ques-\ntions and concerns of patient with prostate cancer and help democ-\nratize medical knowledge? J Transl Med 21(1):269. https://doi.org/\n10.1186/s12967-023-04123-5\n21. Musheyev D, Pan A, Loeb S, Kabarriti AE (2024) How Well Do\nArtiﬁcial Intelligence Chatbots R espond to the Top Search Queries\nAbout Urological Malignancies? Eur Urol 85(1):13–16. https://doi.\norg/10.1016/j.eururo.2023.07.004\n22. Cocci A, Pezzoli M, Re LM et al (2024) Qua lity of information\nand appropriateness of ChatGPT outputs for urology patients.\nProstate Cancer Prostatic Dis 27(1):103–108. https://doi.org/10.\n1038/s41391-023-00705-y\n23. Ozgor F, Caglar U, Halis A et al (2024) Urological Cancers and\nChatGPT: Assessing the Quality of Information and Possible Risks\nfor Patients. Clin Genitourin Cancer 22(2):454–457.e4. https://doi.\norg/10.1016/j.clgc.2023.12.017\n24. Collin H, Ke ogh K, Basto M, Loeb S, Roberts MJ (2024) ChatGPT\ncan help guide and empower patients after prostate cancer diagno-\nsis. Prostate Cancer Prostatic Dis. https://doi.org/10.1038/s41391-\n024-00864-6\n25. Dennstädt F, Hastings J, Putora PM et al (2023) Exploring Capa-\nbilities of Large Language Models such as ChatGPT in Radiation\nOncology. Adv Radiat Oncol 9(3):101400. https://doi.org/10.1016/\nj.adro.2023.101400\n26. Pandey VK, Munshi A, Mohanti BK, Bansal K, Rastogi K (2024)\nEvaluating ChatGPT to test its robustness as an interactive infor-\nmation database of radiation oncology and to assess its responses\nto common queries from radiotherapy patients: A single institution\ninvestigation. Cancer/Radiotherapie 28(3):258–264. https://doi.org/\n10.1016/j.canrad.2023.11.005\n27. Yalamanchili A, Sengupta B, S ong J et al (2024) Qu ality of Large\nLanguage Model Responses to Radiation Oncology Patient Care\nQuestions. Jama Netw Open 7(4):e244630. https://doi.org/10.1001/\njamanetworkopen.2024.4630\n28. Alasker A, Alsalamah S, Alshathri N et al (2024) Performance of\nlarge language models (LLMs) in providing prostate cancer infor-\nmation. BMC Urol 24(1):177. https://doi.org/10.1186/s12894-024-\n01570-0\n29. Geanta M, Badescu D, Chirca N et al (2024) The Emerging\nRole of Large Language Models in Improving Prostate Can-\ncer Literacy. Bioengineering 11(7):654. https://doi.org/10.3390/\nbioengineering11070654\n30. Lombardo R, Gallo G, Stira J et al (2024) Qua lity of informa-\ntion and appropriateness of Open AI outputs for prostate cancer.\nProstate Cancer Prostatic Dis. https://doi.org/10.1038/s41391-024-\n00789-0\n31. Tung JYM, Lim DYZ, Sng GGR (2023) Potential safety concerns\nin use of the artiﬁcial intelligence chatbot ‘ChatGPT’ for perioper-\native patient communication. BJU Int 132(2):157–159. https://doi.\norg/10.1111/bju.16042\n32. Coskun B, Ocakoglu G, Yetemen M, Kaygisiz O (2023) Can Chat-\nGPT, an Artiﬁcial Intelligence La nguage Model, Provide Accurate\nK\n342 Strahlentherapie und Onkologie (2025) 201:333–342\nand High-quality Patient Information on Prostate Cancer? Urology\n180:35–58. https://doi.org/10.1016/j.urology.2023.05.040\n33. Hershenhouse JS, Mokhtar D, Eppler MB et al (2024) Accuracy,\nreadability, and understandability of large l anguage models for\nprostate cancer information to the public. Prostate Cancer Prostatic\nDis. https://doi.org/10.1038/s41391-024-00826-y\n34. Jeyaraman M, K SP , Jeyaraman N et al (2023) ChatGPT in Medical\nEducation and Research: A Boon or a Bane? Cureus 15(8):e44316.\nhttps://doi.org/10.7759/cureus.44316\n35. Adhikari K, Naik N, Hameed BZ, Raghunath SK, Somani BK\n(2024) Exploring the Ethical, Legal, and Social Implications of\nChatGPT in Urology. Curr Urol Rep 25(1):1–8. https://doi.org/10.\n1007/s11934-023-01185-2\n36. Naik N, Hameed BMZ, Shetty DK et al (2022) Legal and Ethical\nConsideration in Artiﬁcial Intelligence in Healthcare: Who Takes\nResponsibility? Fr ont Surg 14(9):862322. https://doi.org/10.3389/\nfsurg.2022.862322\n37. Wang C, Liu S, Yang H et al (2023) Ethical Considerations of Us-\ning ChatGPT in Health Care. J Med Internet Res 11(25):e48009.\nhttps://doi.org/10.2196/48009\n38. Ayers JW, Poliak A, Dredze M et al (2023) Comparing Physi-\ncian and Artiﬁcial Intelligence Chatbot Responses to Patient\nQuestions Posted to a Public Social Media Forum. JAMA In-\ntern Med 183(6):589–596. https://doi.org/10.1001/jamainternmed.\n2023.1838\n39. Zhou Z, Wang X, Li X, Liao L (2023) Is ChatGPT an Evidence-\nbased Doctor? Eur Urol 84(3):355–356. https://doi.org/10.1016/j.\neururo.2023.03.037\n40. D’Anna G, V an Cauter S, Thurnher M, V an Goethem J, Haller S\n(2024) Can large language models pass ofﬁcial high-grade exams\nof the European Society of Neuroradiology courses? A direct com-\nparison between OpenAI chatGPT 3.5, OpenAI GPT 4 and Google\nBard. Neuroradiology 66(8):1245–1250. https://doi.org/10.1007/\ns00234-024-03371-6\n41. Abbas A, Rehman MS, Rehman SS (2024) Comparing the Perfor-\nmance of Popular Large Language Models on the National Board\nof Medical Examiners Sample Questions. Cureus 16(3):e55991.\nhttps://doi.org/10.7759/cureus.55991\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\nK",
  "topic": "Readability",
  "concepts": [
    {
      "name": "Readability",
      "score": 0.8408076763153076
    },
    {
      "name": "Medicine",
      "score": 0.7754533886909485
    },
    {
      "name": "Relevance (law)",
      "score": 0.5256513357162476
    },
    {
      "name": "Likert scale",
      "score": 0.5248346924781799
    },
    {
      "name": "Radiation therapy",
      "score": 0.4802008867263794
    },
    {
      "name": "Prostate cancer",
      "score": 0.47351765632629395
    },
    {
      "name": "Family medicine",
      "score": 0.409122109413147
    },
    {
      "name": "Medical physics",
      "score": 0.3329145908355713
    },
    {
      "name": "Cancer",
      "score": 0.3242719769477844
    },
    {
      "name": "Surgery",
      "score": 0.1985095739364624
    },
    {
      "name": "Internal medicine",
      "score": 0.1882258951663971
    },
    {
      "name": "Psychology",
      "score": 0.14524933695793152
    },
    {
      "name": "Computer science",
      "score": 0.091143399477005
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Developmental psychology",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I3018771216",
      "name": "LMU Klinikum",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I8204097",
      "name": "Ludwig-Maximilians-Universität München",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4391767962",
      "name": "Deutschen Konsortium für Translationale Krebsforschung",
      "country": null
    }
  ]
}