{
  "title": "Ertim at SemEval-2023 Task 2: Fine-tuning of Transformer Language Models and External Knowledge Leveraging for NER in Farsi, English, French and Chinese",
  "url": "https://openalex.org/W4385571071",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2805332030",
      "name": "Kévin Deturck",
      "affiliations": [
        "Teem Photonics (France)"
      ]
    },
    {
      "id": "https://openalex.org/A861488291",
      "name": "Pierre Magistry",
      "affiliations": [
        "Teem Photonics (France)"
      ]
    },
    {
      "id": "https://openalex.org/A5094087207",
      "name": "Bénédicte Diot Parvaz-Ahmad",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2151996825",
      "name": "Ilaine Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A696992000",
      "name": "Damien Nouvel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4370144368",
      "name": "Hugo Lafayette",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3204526376",
    "https://openalex.org/W4294294857",
    "https://openalex.org/W4385572425",
    "https://openalex.org/W2760505947",
    "https://openalex.org/W4287760320",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W4287854446",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4389519502"
  ],
  "abstract": "Transformer language models are now a solid baseline for Named Entity Recognition and can be significantly improved by leveraging complementary resources, either by integrating external knowledge or by annotating additional data. In a preliminary step, this work presents experiments on fine-tuning transformer models. Then, a set of experiments has been conducted with a Wikipedia-based reclassification system. Additionally, we conducted a small annotation campaign on the Farsi language to evaluate the impact of additional data. These two methods with complementary resources showed improvements compared to fine-tuning only.",
  "full_text": "Proceedings of the The 17th International Workshop on Semantic Evaluation (SemEval-2023), pages 2211–2215\nJuly 13-14, 2023 ©2023 Association for Computational Linguistics\nErtim at SemEval-2023 Task 2:\nFine-tuning of Transformer Language Models and External Knowledge\nLeveraging for NER in Farsi, English, French and Chinese\nKévin Deturck, Pierre Magistry,\nBénédicte Diot-Parvaz Ahmad, Ilaine Wang, Damien Nouvel\nERTIM Inalco / 2 rue de Lille, 75007 Paris\nHugo Lafayette\nKairntech / 29 Chemin du Vieux Chêne, 38240 Meylan\nfirst.last@{inalco,kairntech}.fr\nAbstract\nTransformer language models are now a solid\nbaseline for Named Entity Recognition and can\nbe significantly improved by leveraging com-\nplementary resources, either by integrating ex-\nternal knowledge or by annotating additional\ndata. In a preliminary step, this work presents\nexperiments on fine-tuning transformer mod-\nels. Then, a set of experiments has been con-\nducted with a Wikipedia-based reclassification\nsystem. Additionally, we conducted a small\nannotation campaign on the Farsi language to\nevaluate the impact of additional data. These\ntwo methods with complementary resources\nshowed improvements compared to fine-tuning\nonly.\n1 Introduction\nEntity recognition and linking has now become a\nstandard task for NLP, either as a goal in itself or as\na preprocessing step for other goals. As for many\nother NLP tasks, developing an accurate model for\na specific dataset (by language or domain) is still\na challenge that heavily relies on resources, either\npre-computed in a language model or as an external\ndataset.\nAs the NLP research lab of Inalco, Ertim was\nparticularly interested in MultiCoNER II because\nit gave us a good opportunity to:\n• carry out a novel evaluation of HuggingFace\nmodels on the complex NER task proposed\nin MultiCoNER II, through the diversity of\nmore or less endowed languages (Fetahu et al.,\n2023a)\n• experiment the integration of external knowl-\nedge as a postprocessing step to improve the\nclassification of named entities\n• conduct experiments on NER in Farsi, which\ncoincided with our work for a project (VITAL,\nsee the Acknowledgements section)\nWe report our official results (Fetahu et al.,\n2023b) in Table 1. Due to lack of time, some of\nour experiments could not be submitted as official\nruns, this paper provides additional information\nand results about these experiments.\nLang. Rank F1\nclean\nF1\nnoisy\nF1\noverall\nEN 20/34 61.85 52.78 59.03\nFR 9/17 69.73 58.6 66.3\nFA 12/14 53.77 - 53.77\nZH 11/22 64.26 44.38 59.45\nTable 1: Ertim’s rankings for the 4 tracks undertaken\n2 Fine-tuning of HuggingFace Language\nModels\nWe present experiments based on the fine-tuning of\ntransformer language models for the MultiCoNER\nII task on specific languages.\n2.1 Official runs\nIn this section, we present our runs based on the\nfine-tuning of HuggingFace language models that\nhave been officially retained for the MultiCoNER\nII final rankings. All these runs have been done\nby using the Spacy framework. We used models\nrespectively pre-trained on our target languages and\nfine-tuned them by using the MultiCoNER IItrain\nand dev datasets in the corresponding languages.\nHuggingFace Model Prec. Rec. F1\nroberta-base\n(Liu et al., 2019) 59.13 59.54 59.03\nTable 2: \"en\" track, fine-grained macro average perfor-\nmance on test dataset\nWe logically found out two classes of results: the\nbest ones are on highly endowed languages (En-\nglish and French) whereas the lowest ones are on\nthe less supported languages (Chinese and Farsi).\n2211\nHuggingFace Model Prec. Rec. F1\nflaubert-large-cased\n(Le et al., 2020) 65.03 64.58 63.75\nflaubert-base-uncased\n(Le et al., 2020) 67.44 66.08 66.30\nTable 3: \"fr\" track, fine-grained macro average perfor-\nmance on test dataset\nHuggingFace Model Prec. Rec. F1\nbert-base-chinese\n(Devlin et al., 2019) 56.95 52.71 53.14\nTable 4: \"zh\" track, fine-grained macro average perfor-\nmance on test dataset\nHuggingFace Model Prec. Rec. F1\nbert-base-parsbert-\nuncased\n(Farahani et al., 2021)\n51.56 58.53 53.77\nTable 5: \"fa\" track, fine-grained macro average perfor-\nmance on test dataset\nComparing French and English, the best perfor-\nmance on French may be explained by the volume\nand diversity of the corpus used for pre-training the\nFlaubert models, which are Bert models for French,\nlike roberta-base for English.\nIt is also interesting to note the crucial role of\ncase in the datasets used for training and evaluation.\nThe particularity of the datasets provided for Multi-\nCoNER is that all corpora were uncased, including\nthose of languages with writing systems that dis-\ntinguish lowercase and uppercase letters. In Table\n3, we can see that the flaubert-base-uncased\nmodel outperforms flaubert-large-cased; this\nis remarkable considering that the uncased version\nof flaubert has a much smaller training corpus.\n2.2 Additional experiments\nWe conducted additional experiments on the fine-\ntuning of HuggingFace language models by us-\ning the MultiCoNER II train and dev sets.\nThese experiments were performed independently\nof the official runs, by using HuggingFace’s\ntransformers library. It required aligning labels\nwith offset_mapping and checking the BIO for-\nmat. The models used are listed in Table 6. Some\nof them are multilingual and were tested on a spe-\ncific language.\nFigure 1 reports F1 results by distinguishing\nlanguages among line charts and models by dif-\nAll (inc. English)\ndistilbert-base-uncased\nbert-base-uncased\nbert-large-uncased\nxlm-roberta-large\nFrench\ncamembert-base\ncamembert-large\n(Martin et al., 2020)\nFarsi bert-base-parsbert-uncased\n(Farahani et al., 2021)\nTable 6: Pre-computed HuggingFace models\n2 4 6 80\n0.2\n0.4\n0.6\n0.8\n1\nepochs\nEN F1\n2 4 6 80\n0.2\n0.4\n0.6\n0.8\n1\nepochs\nFR F1\n2 4 6 80\n0.2\n0.4\n0.6\n0.8\n1\nepochs\nFA F1\n2 4 6 80\n0.2\n0.4\n0.6\n0.8\n1\nepochs\nZH F1\ndistilbert-base-uncased bert-base-uncased\nbert-large-uncased xlm-roberta-large\ncamembert-base camembert-large\nbert-base-parsbert-uncased bert-base-chinese\nFigure 1: Results per language and epoch\nferent plots in each of them. Most models re-\nquired more than five epochs to obtain the best\nF1 results, which are very different among lan-\nguages. Dedicated language models (Bert for En-\nglish, CamemBERT for French, ParsBERT for\nFarsi and bert-base-chinese) outperform mul-\ntilingual ones by about 5pts of F1 score in English,\n3pts in French, 10pts in Farsi and 30pts in Chinese.\n3 Entity Classification Leveraging\nWikipedia Articles\n3.1 Method\nWe used as a resource the Wikipedia dump\ndatabase made publicly available online by the\nWikimedia Foundation 1. We downloaded the\ndump containing only textual data, for example\n\"wikipedia_zh_all_nopic\", and then used it as a\n1https://dumps.wikimedia.org/other/kiwix/zim/\nwikipedia/\n2212\nlocal database, with the Kiwix2 tool.\nAssuming a mention has been detected, the aim\nis to create a model of the categories to be pre-\ndicted from the Wikipedia resources. Our ap-\nproach consists in training a classifier based on\nthe TF-IDF representation of word bigrams from\nthe first paragraph and the descriptive field head-\nings of Wikipedia pages. This representation is\nmapped with entity categories in both train and\ndev datasets.\nThe classification system is implemented as a\nKeras model with two input layers and one output\nlayer. The input layers are dense with relu activa-\ntion of 256 neurons and a dropout of 0.5. The out-\nput layer is dense with softmax activation. Once\nwe have generated the Wikipedia-based model, we\ncan apply it in order to perform a reclassification\nof the named entities previously identified by fine-\ntuned transformer models.\nFor each named entity detected, we send a query\nto the Kiwix server to obtain the corresponding\nWikipedia page. In cases in which there is no\nWikipedia page, we do not modify the previous\ncategory. Thus, our approach does not detect new\nnamed entities, the goal of this Wikipedia-based\nsystem is only to improve a previous categorisation.\nIf a Wikipedia page is available for the named\nentity to be reclassified, we generate a represen-\ntation of the named entity, similar to the ones of\nthe pages used for training the model. Then, we\ncategorise the named entity with the class predicted\nby the classification model.\n3.2 Official Results\nNb. of page\nper entity Precision Recall F1\n1 61.72 58.94 59.45\n3 62.09 58.49 59.20\nTable 7: Fine-grained macro average performance on\nChinese test dataset with our Wikipedia-based reclassi-\nfication system and the bert-base-chinese model\nWith our Wikipedia-based reclassifier system,\nwe submitted two runs on Chinese using the\nbert-base-chinese transformer model that we\npreviously tested alone. The difference between\nthe two runs is the number of Wikipedia pages in-\ncluded for each entity: for the first run, we only\nuse the first page, for the second run, we use the\n2https://www.kiwix.org\nfirst three pages in order to bypass the possible\ndisambiguation page.\nWe found out that the two tests with the reclas-\nsification system significantly improve the perfor-\nmance compared to using thebert-base-chinese\nmodel alone (see section 2.2). Also, taking into ac-\ncount more Wikipedia pages slightly improves the\nprecision, which was the goal since we wanted to\navoid the disambiguation page that could lead to a\nfalse categorisation. However, we have to counter-\nact the decline in recall induced by this method.\n4 Annotation of an Additional Farsi\nDataset\n4.1 Creation of Additional Farsi Annotations\nThe Farsi language is a relatively poorly endowed\nlanguage, especially in comparison with English\nand French. The goal motivating this part of our\nwork is to evaluate the impact of additional anno-\ntated data on the performance of the best model\nwe obtained by using only the MultiCoNER II\ndataset, i.e. bert-baseparsbert-uncased (see\nsection 2.2).\nWe recruited a Farsi speaker as an annotator.\nShe worked on a news dataset in Farsi including\nmore than 10 news agencies from 2009 to 2021\n(Alimoradi, 2021). We asked the annotator to work\nwith all of the 36 annotation categories proposed\nfor the MultiCoNER II task. We used an annotation\nplatform developed by Kairntech (Nibart, 2022).\nAs we did not have guidelines regarding the cat-\negories, we provided the annotator with the Mul-\ntiCoNER II Farsi dev file so that she could see\nand learn from annotation examples. We also pro-\nvided the annotator with the annotation guidelines\nregarding some of the coarse-grain categories from\nthe paper that described the previous MultiCoNER\nedition (Derczynski et al., 2017; Malmasi et al.,\n2022a,b).\nFigure 2: Category distribution from session 1\n2213\nFigure 3: Category distribution from session 2\nTwo 50-minute annotation sessions were organ-\nised. As a result of the first session, the annota-\ntor produced 116 annotated entities from 10 doc-\numents. As a result of the second session, she\nproduced 122 annotated entities from 9 documents.\nWe present the category distribution in the addi-\ntional annotations from session 1 in Figure 2 and\nsession 2 in Figure 3.\nWe note that the categories’ distribution between\nthe two sessions are significantly different, which\nis positive for the categorical diversity of the ad-\nditional annotations. In addition, only about one\nthird of all proposed MultiCoNER II labels were\nidentified in the new dataset, which leads us to hy-\npothesise that the performance improvements with\nthe additional annotations would be variable across\nthe labels.\n4.2 Experimental Results with the Additional\nAnnotations\nWe carried out a first experiment which consisted\nin training bert-base-parsbert-uncased on the\nMultiCoNER II train set plus the additional anno-\ntations, and evaluating on the dev set. As we had\npreviously assumed from the unbalanced distribu-\ntion of categories in the additional annotations, we\nnotice that the results are variable across categories.\nWe then decided to experiment on a re-\nstricted version of the additional annotations\nin order to filter out categories that per-\nformed less well. We have retained 15 cate-\ngories out of 36: PublicCorp, PrivateCorp,\nPolitician, Facility, HumanSettlement,\nAthlete, MedicalProcedure, Software,\nAnatomicalStructure, Disease, ORG,\nOtherPROD, Artist, CarManufacturer, and\nClothing. The corresponding results are\npresented in Table 8.\nHuggingFace model Precision Recall F1\nbert-base-parsbert-\nuncased 59.73 58.99 58.04\nTable 8: Fine-grained macro average performance on\n\"fa\" test dataset with filtered additional annotations\nThe filtered additional annotations allowed to im-\nprove the F1 score by 4.2pts compared to training\nbert-base-parsbert-uncased only on the Mul-\ntiCoNER II train set (see section 2.2). This indi-\ncates that even modest in terms of quantity and cat-\negories covered, additional annotations are signifi-\ncantly beneficial to the fine-tuning of a transformer\nFarsi model for NER. We consider that those im-\nprovements may be viewed as few-shot settings\nthat leverage on the generalisation capacities of the\nunderlying large language model.\n5 Conclusion\nOur work is based on three complementary direc-\ntions: the fine-tuning of HuggingFace transformer\nlanguage models, a reclassification system based\non Wikipedia, and an effort to provide additional\nannotations. The first axis allowed us to highlight\nkey results regarding the use of different models on\na variety of languages. The second and third axes\ncorrespond to the original contributions of our work\nabout integrating complementary data sources.\nWith our Wikipedia-based reclassification sys-\ntem, we found that using the information available\nin Wikipedia could correct the classification from a\nfine-tuned transformer model. We have shown the\nsignificant contribution of additional annotations\nto the fine-tuning of transformer language model,\neven if these annotations are modest in terms of\nvolume and categorical coverage. We consider shar-\ning as open-source the annotated data, the models\nproduced and our experimental codes.\nAs a follow-up to this work, it would be interest-\ning to evaluate the evolution of pre-trained model\nperformance according to the quantity and variety\nof annotations we include. Also, our Wikipedia-\nbased reclassification system has a lot of room for\nimprovement, especially when it comes to handling\nmentions with no Wikipedia page available and\ncases of ambiguity. Overall, this work would be\nall the more interesting and robust when applied\nto more languages. We consider experimenting\nfurther with Spacy as it is a user-friendly wrapper,\nincluding Pytorch and HuggingFace components.\n2214\nAcknowledgements\nThis work was conducted thanks to the DGA\nRAPID VITAL funding.\nReferences\nSaied Alimoradi. 2021. Persian News Dataset. https:\n//saied71.github.io/RohanAiLab/.\nLeon Derczynski, Eric Nichols, Marieke Van Erp, and\nNut Limsopatham. 2017. Results of the WNUT2017\nshared task on novel and emerging entity recognition.\nIn Proceedings of the 3rd Workshop on Noisy User-\ngenerated Text, pages 140–147.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. pages 4171–4186.\nMehrdad Farahani, Mohammad Gharachorloo, Marzieh\nFarahani, and Mohammad Manthouri. 2021. Pars-\nbert: Transformer-based Model for Persian Language\nUnderstanding. Neural Processing Letters, 53:3831–\n3847.\nBesnik Fetahu, Zhiyu Chen, Sudipta Kar, Oleg\nRokhlenko, and Shervin Malmasi. 2023a. Multi-\nCoNER v2: a Large Multilingual dataset for Fine-\ngrained and Noisy Named Entity Recognition.\nBesnik Fetahu, Sudipta Kar, Zhiyu Chen, Oleg\nRokhlenko, and Shervin Malmasi. 2023b. SemEval-\n2023 Task 2: Fine-grained Multilingual Named En-\ntity Recognition (MultiCoNER 2). In Proceedings of\nthe 17th International Workshop on Semantic Evalua-\ntion (SemEval-2023). Association for Computational\nLinguistics.\nHang Le, Loïc Vial, Jibril Frej, Vincent Segonne, Max-\nimin Coavoux, Benjamin Lecouteux, Alexandre Al-\nlauzen, Benoît Crabbé, Laurent Besacier, and Didier\nSchwab. 2020. FlauBERT: Unsupervised Language\nModel Pre-training for French. In Proceedings of\nThe 12th Language Resources and Evaluation Con-\nference, pages 2479–2490, Marseille, France. Euro-\npean Language Resources Association.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. abs/1907.11692.\nShervin Malmasi, Anjie Fang, Besnik Fetahu, Sudipta\nKar, and Oleg Rokhlenko. 2022a. MultiCoNER:\nA Large-scale Multilingual Dataset for Complex\nNamed Entity Recognition. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, Gyeongju, Republic of Korea.\nShervin Malmasi, Anjie Fang, Besnik Fetahu, Sudipta\nKar, and Oleg Rokhlenko. 2022b. SemEval-2022\nTask 11: Multilingual Complex Named Entity Recog-\nnition (MultiCoNER). In Proceedings of the 16th\nInternational Workshop on Semantic Evaluation\n(SemEval-2022). Association for Computational Lin-\nguistics.\nLouis Martin, Benjamin Muller, Pedro Javier Ortiz\nSuárez, Yoann Dupont, Laurent Romary, Éric Ville-\nmonte de la Clergerie, Djamé Seddah, and Benoît\nSagot. 2020. CamemBERT: a Tasty French Lan-\nguage Model. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics.\nVincent Nibart. 2022. Découvrir et enrichir des con-\nnaissances à partir de l’analyse de documents grâce à\nl’intelligence artificielle: l’exemple de la plateforme\nkairntech. I2D–Information, données & documents,\npages 38–43.\n2215",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8380856513977051
    },
    {
      "name": "Transformer",
      "score": 0.7722041606903076
    },
    {
      "name": "Natural language processing",
      "score": 0.6515282988548279
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5592893362045288
    },
    {
      "name": "Language model",
      "score": 0.5409672260284424
    },
    {
      "name": "Annotation",
      "score": 0.5281422138214111
    },
    {
      "name": "Baseline (sea)",
      "score": 0.4897928237915039
    },
    {
      "name": "Task (project management)",
      "score": 0.4527977406978607
    },
    {
      "name": "Named-entity recognition",
      "score": 0.42264145612716675
    },
    {
      "name": "Voltage",
      "score": 0.08413159847259521
    },
    {
      "name": "Engineering",
      "score": 0.07159373164176941
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 3
}