{
  "title": "Exploring Gameplay With AI Agents",
  "url": "https://openalex.org/W2901293537",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2595479373",
      "name": "Fernando de Mesentier Silva",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2466838332",
      "name": "Igor Borovikov",
      "affiliations": [
        "Electronic Arts (United States)"
      ]
    },
    {
      "id": null,
      "name": "John Kolen",
      "affiliations": [
        "Electronic Arts (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2638146196",
      "name": "Navid Aghdaie",
      "affiliations": [
        "Electronic Arts (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3107414603",
      "name": "Kazi Zaman",
      "affiliations": [
        "Electronic Arts (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2595479373",
      "name": "Fernando de Mesentier Silva",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2466838332",
      "name": "Igor Borovikov",
      "affiliations": []
    },
    {
      "id": null,
      "name": "John Kolen",
      "affiliations": [
        "Electronic Arts (United States)",
        "iSign Solutions (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2638146196",
      "name": "Navid Aghdaie",
      "affiliations": [
        "iSign Solutions (United States)",
        "Electronic Arts (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3107414603",
      "name": "Kazi Zaman",
      "affiliations": [
        "iSign Solutions (United States)",
        "Electronic Arts (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6677525955",
    "https://openalex.org/W2646036860",
    "https://openalex.org/W2591367559",
    "https://openalex.org/W2794586780",
    "https://openalex.org/W2750737324",
    "https://openalex.org/W2810835168",
    "https://openalex.org/W2896685958",
    "https://openalex.org/W6688863588",
    "https://openalex.org/W6602324621",
    "https://openalex.org/W6732291021",
    "https://openalex.org/W2212995356",
    "https://openalex.org/W2058552503",
    "https://openalex.org/W2218319575",
    "https://openalex.org/W788042400",
    "https://openalex.org/W1980956916",
    "https://openalex.org/W2228991311",
    "https://openalex.org/W2018839436",
    "https://openalex.org/W6732528621",
    "https://openalex.org/W2395774634",
    "https://openalex.org/W2160237541",
    "https://openalex.org/W2111393845",
    "https://openalex.org/W2575813499",
    "https://openalex.org/W2579414847",
    "https://openalex.org/W4300526394",
    "https://openalex.org/W2892327512",
    "https://openalex.org/W57172406",
    "https://openalex.org/W2220072944"
  ],
  "abstract": "The process of play testing a game is subjective, expensive and incomplete. In this paper, we present a play-testing approach that explores the game space with automated agents and collects data to answer questions posed by the designers. Rather than have agents interacting with an actual game client, this approach recreates the bare bone mechanics of the game as a separate system. Our agent is able to play in minutes what would take testers days of organic gameplay. The analysis of thousands of game simulations exposed imbalances in game actions, identified inconsequential rewards and evaluated the effectiveness of optional strategic choices. Our test case game, The Sims Mobile, was recently released and the findings shown here influenced design changes that resulted in improved player experience.",
  "full_text": "Exploring Gameplay with AI Agents\nFernando de Mesentier Silva\nNew York University\nGame Innovation Lab\nBrooklyn, NY 11201\nIgor Borovikov\nElectronic Arts\n209 Redwood Shores Pkwy\nRedwood City, CA 94065\nJohn Kolen\nElectronic Arts\n209 Redwood Shores Pkwy\nRedwood City, CA 94065\nNavid Aghdaie\nElectronic Arts\n209 Redwood Shores Pkwy\nRedwood City, CA 94065\nKazi Zaman\nElectronic Arts\n209 Redwood Shores Pkwy\nRedwood City, CA 94065\nAbstract\nThe process of playtesting a game is subjective, expensive\nand incomplete. In this paper, we present a playtesting ap-\nproach that explores the game space with automated agents\nand collects data to answer questions posed by the design-\ners. Rather than have agents interacting with an actual game\nclient, this approach recreates the bare bone mechanics of\nthe game as a separate system. Our agent is able to play in\nminutes what would take testers days of organic gameplay.\nThe analysis of thousands of game simulations exposed im-\nbalances in game actions, identiﬁed inconsequential rewards\nand evaluated the effectiveness of optional strategic choices.\nOur test case game, The Sims Mobile, was recently released\nand the ﬁndings shown here inﬂuenced design changes that\nresulted in improved player experience.\nIntroduction\nPlayer engagement is a crucial part of any game. Immersing\nthe players in the game experience not only results in longer\nplaying sessions, it also keeps them interested in coming\nback to the game. In contrast, experiencing inconsistent be-\nhaviors or an unnatural cycle of actions can result in early\nchurn from the game.\nTo ensure the game provides players with the intended ex-\nperiences, designers conduct playtesting sessions. Playtest-\ning consists of having a group of players experimenting\nwith the game during development. Through it, designers\nnot only want to measure the engagement of players, they\nalso want to understand how interactions reﬂect in their sys-\ntem. One main point of playtesting is to discover elements\nand states that result in undesirable outcomes. As a game\ngoes through the various stages of development, it is essen-\ntial to continuously iterate and improve. Relying exclusively\non playtesting conducted by humans can be costly. Em-\nploying automated agents could reduce development costs\nthrough faster play sessions and the thorough exploration of\nthe game space in much shorter time. These beneﬁts can be-\ncome even more valuable as a game grows in size, increas-\ning the space of possible actions. Automated agents are also\nCopyright c⃝ 2018, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\ncapable of playing the game trying to mimic the same deci-\nsions multiple times in order to generate statistically signiﬁ-\ncant results.\nUnexpected scenarios are not the only focus of playtests.\nDuring iterating on the game mechanics and looking for\nan ideal tunning, playtesting is used to gauge the impact\nof changes made to the game. Determining if changes had\nthe desired impact on the gameplay or comparing progres-\nsion over different builds of the game can also be made eas-\nier with the use of AI Agents. Agents can explore different\nroutes through the game and statistical results can be used to\ncompare the evolution of gameplay over different iterations.\nIn this paper, we present our work on using AI agents to\nfacilitate the process of playtesting a game. In the next sec-\ntion, we discuss related work. We then present The Sims\nMobile, the focus of our game testing efforts. After this in-\ntroduction, we elaborate our approach and the justiﬁcations\nfor choosing it over a simpler, more straight forward, strat-\negy of implementing the agent on the game client. We then\nshow four use cases that were created following questions\nfrom the designers of the game. We later compare the suc-\ncess of our chosen approach in contrast with the game client\napproach. Lastly, we discuss our results, present our conclu-\nsions, and discuss future work.\nRelated Work\nThe concept of using agents to help playtest games has\nbeen previously explored by several researchers. De Mesen-\ntier Silva et al. investigated how using agents to automate\nplaytesting could help designers of contemporary board\ngames (de Mesentier Silva et al. 2017a; 2017b; 2018a). With\na focus on the game Ticket to Ride, their work presents four\nheuristic-driven agents tailor made to play the game. They\npresent analyses that originate from the data gather by sim-\nulating the game with their agents and could help designers\nﬁne tune the game. The cases shown are interesting and in-\nformative, but it is hard to gauge with those were scenarios\nraised during development. Our approach was built to an-\nswer designer questions.\nWhile we propose to use agents as a tool to help design-\ners, other approaches to algorithmically address balance ex-\nist in the literature. Hom et al. presented AI techniques to\nProceedings of the Fourteenth Artificial Intelligence and  \nInteractive Digital Entertainment Conference (AIIDE 2018)\n159\noptimize balance in abstract board games (Hom and Marks\n2007). Using a genetic algorithm, rules were searched in or-\nder to optimize for balance, represented by the number of\ndraws and the advantage of going of going ﬁrst. Krucher\nin turn looked to balance a collectible card game (Krucher\n2015). By rating and modifying cards, AI agents would de-\ncide actions to take. Cards would be automatically modi-\nﬁed following gameplay. Jaffe et al. analyze the contribu-\ntion of several features in the game (Jaffe et al. 2012). These\nfeatures are automatically measured and tracked on a edu-\ncational card game. Dormans evaluates the economy of a\ngame and its impact on strategy (Dormans 2011). With the\nuse of a framework designed to track the ﬂow of resources\nenabling simulation and balance of games before they are\nbuilt. Mahlmann et al. focuses on the card game Dominion,\ndiscussing how to achieve a balanced card set (Mahlmann,\nTogelius, and Yannakakis 2012). By playing the game with\nthree different agents, intersections were found in the win-\nning sets of different agents. These cards were deemed to\nbe part of a set that contributed to a more balanced game,\nregardless of strategy. This work successfully demonstrates\nagent-based game balance evaluation.\nAnother dimension of research focuses on investigating\napproaches where AI and Machine Learning can play the\nrole of a co-designer, making suggestions during develop-\nment. This effort is called mixed initiative design (Yan-\nnakakis, Liapis, and Alexopoulos 2014). Liapis et al. pre-\nsented a tool for creating real time strategy game maps (Li-\napis, Yannakakis, and Togelius 2013). The tool, called Sen-\ntient Sketchbook, would make suggestions on how to change\nthe proposed map design in order to optimize for a number\nof different features. Smith et al. presents a 2D platformer\nlevel designing framework (Smith, Whitehead, and Mateas\n2010). Users can create and manipulate key elements in the\nlevel and an algorithm proceeds to complete the level, guar-\nanteeing its playability. Shaker et al. discusses Ropossum, a\nlevel design tool for the Cut the Rope game (Shaker, Shaker,\nand Togelius 2013). The work can address level playability,\nﬁnish a level with incomplete design or generate a new one.\nOther approaches have also touched on the contributions\nAI can make for the game design process. Browne et al.\ngenerates entirely new abstract games by means of evolu-\ntionary algorithms (Browne and Maire 2010). Games gen-\nerated are measured in terms of predeﬁned features of qual-\nity (Browne 2008). The most interesting designs generated\nwere later then published. Salge et al. relates a games’ de-\nsign to the concept of relevant information with the use of\nan adaptive AI (Salge and Mahlmann 2010). Smith et al.\naddresses the behavior emanating from a design by hav-\ning an engine capable of recording play traces (Smith, Nel-\nson, and Mateas 2010). Nelson discusses alternate strate-\ngies to gather information from games, other than empir-\nical playtesting (Nelson 2011). Nielsen et al. relates qual-\nity in a game to the performance of multiple general game\nplaying algorithms (Nielsen et al. 2015). Isaksen automates\nplaytesting, but with the goal of exploring the space of possi-\nble games represented from the concept of the game Flappy\nBird. Variants with different game feel and difﬁculty are\ndeemed interested and further explored (Isaksen et al. 2015;\nIsaksen, Gopstein, and Nealen 2015). De Mesentier Silva\net al. searches the space of possible strategies for simple\nand effective introductory heuristics for playing blackjack\nand HULHE Poker (de Mesentier Silva et al. 2016; 2018b;\n2018c).\nThe Sims Mobile\nThe Sims Mobile is a mobile entry from the very popular\ngame franchise The Sims. Gameplay focuses on “emulat-\ning life”: players create avatars, called Sims, and conduct\nthem through everyday activities. Common Sims actions can\ntake range from cooking meals to going on dates with other\navatars. The Sims Mobile is a “sandbox” game, where there\nis not a predetermined goal to achieve and instead players\ncraft their own experience.\nAn important game mechanic revolves around managing\nyour resource pool. Resources are required to perform ac-\ntions. Each Sim has their own pool of resources. In addition\nto resources, actions also require time to be performed. Af-\nter selecting an action for their avatar, such Sim is locked\nfor the duration of it. Actions have a cool down restriction,\nmeaning the same Sim can only repeat this action after such\ncool down is over. Lastly, resources are both recovered over\ntime, as well as through the execution of speciﬁc actions.\nAnother core gameplay elements are events. To improve\ntheir relationships Sims have to complete speciﬁc in-game\nevents. During an event, players have a set amount of time\nto complete speciﬁc actions in order to succeed. Each ac-\ntion rewards event experience (event XP) and the total XP\nacquired by the end measures the success of the Sim.\nWe conducted the reported work on development builds\nbefore public release. The game received numerous updates\nsince then, until it reached the current published product.\nApproach\nIn order to playtest using AI agents, we need a model of the\ngame mechanics. We originally attempted to implement AI\nagents controlling the actual game client. A series of lim-\nitations impeded this approach. First, the game mechanics\ncould only be driven as fast as the client allowed. Since hu-\nman gameplay was the only use case, it was built to fast\nenough to respond to human ﬁnger tapping, but no faster.\nSecond, neither graphics nor animation could be turned off,\nskipped, or otherwise bypassed. Rendering animations and\nuser interface elements for our agent wasted computation\ntime. Finally, the in-games menus could not be turned off as\nwell, so the agents had to navigate them as part of their logic.\nIt was also impossible to fast forward the time spent waiting\nfor actions to be performed. These reasons, combined with\nhaving to rewrite the agent every time a new build of the\ngame is released, forced us to search for a new approach.\nThe Sims Mobile uses a collection of JSON ﬁles to store\ngame parameters subject to tuning. With the tunning ﬁles,\nwe decided to re-implement the game mechanics as an appli-\ncation separate from the game client. This approach brought\nseveral advantages: complete control over the game state,\ngraphics and UI avoidance, and the ability to advance the\ngame clock when necessary. With the simulation of game\n160\nmechanics, due to all the advantages listed above, gameplay\nruns at a much faster speed. The increase in speed is roughly\nthousand fold for executing in-game actions. Despite hav-\ning to re-implement the mechanics, we only need a slice of\nthe game. Starting with the core system, we added more me-\nchanics as needed for the analysis in question.\nFast simulation allowed for the application of search-\nbased AI techniques. The system runs about a thousand ac-\ntions every second, making it suited for future lookahead\ntechniques such as A*. Given that all the agents in this pa-\nper are solving shortest path problems, A* would provide\nthe globally optimal solution (and we exclude sub-optimal\nhuman-like behavior). This approach proved to be a more\npowerful technique than what we could explore by driving\nthe game client. In the results section, we look further into\nthe comparison between the two approaches.\nUsing the A* Algorithm\nOur experiments were agreed upon in consultation with the\ngame designers with whom we identiﬁed clear goals for the\nagents. Requirements to reach these goals are explicit and\nthe rewards for each action usually have some direct impact\non reaching the conditions. For this reason, we decided to\nuse the A* algorithm to play the game. The challenge is then\nto build a heuristic that can target the gameplay style that we\nare looking for in an experiment.\nAlthough the elements that inﬂuence the goal are clear,\nbuilding a heuristic from them is not. Weighting the different\ncomponents to achieve the desired outcome is not simple.\nThe parts have to be managed delicately and minor changes\ncan result in different strategies.\nThe experiment guides the heuristic function construc-\ntion. We selected the experiments after meeting with the\ngame testing team. They propose questions about the game\ntuning, such as imbalance or possible exploits. We then\nwrite a heuristic aimed at exploring the issues raised. This\nmeans heuristics are frequently changed or re-written. These\nheuristics target different in-game activities, and for each\ncase, various elements are used to create effective functions.\nOur experiments can require thousands of actions to be\ncompleted. To reduce the amount of time the agent spends\nto pick an action, we limit the amount of nodes A* can\nsearch. For our experiments, we limit the search to 2000\nnodes for every action. Consequently A* might not ﬁnd the\noptimal action to take. However, from testing we concluded\nthat searching 2000 nodes achieved the desired results in all\nexperiments, and had fast computational speed, with each\ndecision taking at most one second.\nSome actions in the game have extremely similar rewards.\nDifference in actions might be only in the cool down time.\nWhen applying the heuristic, we could have multiple game\nstates with the same evaluation. When nodes have the same\npriority, we randomly choose which one to evaluate.\nUse Cases\nIn this session, we detail four different use cases that were\nassessed by having an A* agent playtest the game. First, we\nstate the question raised by the designers. Then we proceed\nto give a detailed description of the in-game representation\nof the use case. Finally, we present the results and discuss\nhow they inﬂuenced design decisions.\nUse Case: Relationship imbalance\nQuestion: Is there a signiﬁcant imbalance when comparing\ndifferent relationship categories?\nFor two Sims to develop a relationship, they need to pur-\nsue one of three mutually exclusive categories: Friendship,\nRomance, and Rivalry. Each relationship type requires com-\npletion of ﬁve events in a speciﬁc order, e.g., Friendship pro-\ngresses through Friends, Close Friends, Great Friends, Best\nFriends, and ﬁnally, BFFs. An imbalance between categories\nimplies that players need to put signiﬁcantly more effort to\npursue one category compared to another. The metric for this\ndifference is the number of the actions required to complete\nall the events for the relationship category.\nOur experiment measures how many actions are needed\nto complete the events in each category. Our objective is to\ncompare the number of events between categories consid-\nering their order, e.g., compare the ﬁrst events of category\nA with the ﬁrst event of B and C. For this experiment, our\nheuristic rewards the agent for acquiring relationship expe-\nrience points and for successfully completing relationship\nevents. Our simulation stopped when the Sim completed the\nﬁfth, and last, event of any relationship category.\nThere are aspects that make exploring relationships\nunique compared to other experiments. The players do not\nselect their relationship category by navigating a menu, like\nthey do for careers and hobbies. Instead, the category of the\nﬁrst event chosen leads the relationship to that category. An-\nother feature unique to relationships are actions with delayed\neffect dependent on the category. We do not include those in\nour heuristic for this experiment, causing the A* to reach lo-\ncal optimum depending on the randomization. To overcome\nsuch, we run the experiment 1000 times and take the aver-\nage of those runs. Repeated sampling allowed the use of the\nsame heuristic to explore all categories of relationship. Since\nthe ﬁrst event for all categories are very similar in require-\nments, whichever is ﬁrst on the list of actions is selected.\nThis randomization guarantees that we have a signiﬁcant\namount of samples for each category after 1000 runs.\nFigure 1 shows the result of the experiment. While the\nagent achieved Friendship and Rivalry events with only\nthree or four appointments, the second event of the Romantic\ncategory, required thirteen appointments. This outlier show-\ncases that more effort was needed to progress in Roman-\ntic relationships. Following this ﬁnd, designers analyzed the\ntuning data for the second event of each category and dis-\ncovered the second event for the Romantic relationship to\nrequire about two times more experience than any other sec-\nond event. The game team adjusted the number of appoint-\nments for this event over the next iterations of the game.\nUse Case: Time needed to progress in a career\nQuestion: How many actions are needed to progress in the\ncareers?\nPlayers can pick one career for each of their Sims. Char-\nacters progress in their career by completing events speciﬁc\n161\nFigure 1: Bar chart showing the average number of actions (appointments) needed to complete each event. Events are color\ncoded to reﬂect the relationship category they belong to. The number on the top of the bar shows the rounded value of each\naverage. The black lines on top of the bars represent the variance. The Sweet Hearts (the second event of the Romantic category)\nappears as an outlier, requiring twice as many actions as any other event. This turned out not to be the tuning the game designers\ndesired and was changed over the next iterations of the game.\nFigure 2: A bar chart comparing the number of actions (ap-\npointments) required to reach the goal in each career. Barista\nis much smaller because it can only go up to level 5, while\nthe others are going up to level 10.\nto that profession. Each career has several levels to repre-\nsent the progress made and the actions of the events reward\ncareer experience points that are used to reach higher lev-\nels. Each career has a maximum level to reach and may have\ndifferent experience point requirements for their levels.\nAnswering this question would provide guidance to the\ngame team on the relative balance of different careers. To\nperform this experiment, we examine the ﬁrst four careers\nthat are available to the players: Barista, Culinary, Fashion,\nand Medical. While the maximum level for Barista is 5, the\nothers can reach level 20. For our experiment, we stopped\nwhen reaching the maximum level for Barista or level 10 on\nthe others. We capped these careers as it provided sufﬁcient\ndata to establish the speed of progression in the early game.\nThe heuristic to progress in careers is similar to the re-\nlationship heuristic. Career level, career experience points\nand completing career events are direct rewards we look for.\nThe Barista and Culinary careers also have a small extra me-\nchanic: Sims need to perform an action to generate an object,\nsuch as coffee and tea for Barista and cooked dishes for Culi-\nnary, which then enable them to perform a second action that\nrewards experience points. For that reason we factor those\nobjects into our heuristic as well.\nUnlike relationships, players do not execute a character\naction to choose their career. They instead select from a\nmenu, an action requiring none of the Sim’s resources. We\nrun separate experiments for each career, by assigning our\ndesired career directly to the Sim at the start of the simula-\ntion. The same heuristic was used across all careers.\nFigure 2 shows the results of our experiment. Barista has\nless than a third of the actions required to complete the ca-\nreer, but it only goes up to level 5. Meanwhile, Culinary,\nFashion and Medical all go up to level 10, but have a dif-\nference in number of necessary actions. The game team de-\ncided that these values represented their original design in-\ntentions and choose not to make any changes.\nIncorrect tuning discovered When running experiments\nfor the Barista career, we noticed that agents made unex-\n162\nFigure 3: Line plot showing the relation between the amount\nof experience required to achieve each step of the event and\nthe amount of career experience it rewards for reaching it.\nEach step in the event is marked by a red star. This plot evi-\ndences that reaching the second step takes more than double\nthe effort of the ﬁrst one for the exact same reward.\nCareer Actions Reduction (%) ρ / Action Save\nBarista 5% 56.7\nCulinary 26% 45.8\nFashion 20% 52.6\nTable 1: Table showing how objects affect career progres-\nsion. Here, ρ denotes in-game resources available to all play-\ners. The “Actions Reduction (%)” column shows the per-\ncentage of fewer actions required to achieve level threshold.\nThe “ρ / Action Save” column shows the ratio between the\namount of resources used, for all objects affecting that ca-\nreer, and how many actions less they would need to take.\nThe Medical career objects are not availabe below level 10.\npected decisions in the case of one speciﬁc event. Usually\nthe agent would progress the event until it was completed.\nFor this speciﬁc event, however, it decided to do a hand-\nful of actions and then fast-forward the game time until\nthe event timed out. Instead of receiving full reward for the\nevent, it did only enough actions to receive the smallest re-\nward the event could give. When investigating the tuning of\nsuch event, we pinpointed the cause of this behavior. Fig-\nure 3 shows the cost-reward relation of this event. Reaching\nthe second step of the event would have the agent do at least\ntwice as many actions when compared to stopping at the ﬁrst\nstep, but it would still generate the same reward. When pre-\nsented with this novel strategy, the game team traced it to an\nerror in the tuning parameters and ﬁxed it.\nUse Case: Effect of objects on careers\nQuestion: How does object acquisition impact career\nprogress?\nWhen progressing through a career, players can acquire\nCareer Evt Actions Tot. Actions Sessions\nBarista A 75 82 2\nBarista B 347 381 24\nCulinary A 298 327 8\nCulinary B 1506 1643 94\nTable 2: Table showing the comparison of the number of ac-\ntions and sessions needed for Barista and Culinary careers\non build A and build B. The Evt Actions column refers to\nhow many event related actions were taken. The Tot. Ac-\ntions column shows the total number of actions taken during\nthe experiment. The Sessions column shows the number of\nsessions necessary to ﬁnish the experiment.\nobjects speciﬁc to that career. These objects unlock object-\nspeciﬁc new event actions that have higher utility for faster\nprogression than the regular actions. The objects become\navailable as the Sim progresses through the career levels.\nFor this experiment, we wanted to gauge the impact of ob-\njects in the career progress. While we used the same heuris-\ntic for exploring careers, the simulation would now award\nobjects to the agent the moment they were available without\nan exchange of resources. We then compared it to the num-\nber of actions needed to complete the same career goals we\nset out before, with no objects to use.\nTable 1 shows the results of the experiment. Objects\nmade a bigger impact on the Fashion and Culinary careers,\nwhile having little impact on the Barista. The Medical career\nshows no impact, since objects are only available past level\n10. We also displayed the ratio of the amount of resources\nthe players would have to exchange to acquire the objects\nby the amount of actions they would save. Designers ana-\nlyzed the ﬁndings and changed the tuning of the object’s to\nincrease their impact and make them more accessible.\nUse Case: Comparing the off-time between builds\nQuestion: Did gameplay change between builds?\nWith major changes between game iterations, signiﬁcant\nimpact on the gameplay can be felt. The question “how\nmuch impact did the changes have” is on everyone’s mind.\nUsing our AI agent to play both builds of the game, we can\ncreate metrics for comparison. Since we are not simulating\nthe full game experience, our approach can only make punc-\ntual statements, but those can point to the impact of changes.\nFor our experiment, we compare progression through the\nBarista and Culinary careers between two signiﬁcantly dif-\nferent builds named A and B for simplicity. The resource\nmanagement system is the key change between these builds,\nas a consequence we expect a big impact in the gameplay\nand the goals our agents want to achieve.\nTable 2 shows the results of our experiments. The dif-\nference in the number of actions performed between builds\nA and B is evident. For both careers, build B requires over\nfour times as many actions to reach the target goals. We also\ncompared builds in terms of sessions. A session is a period\nthat starts when players log into the game and ends when no\nmore actions are available. We saw a signiﬁcant increase in\n163\nFigure 4: Bar chart comparing career progress in each ap-\nproach. Numbers are the average amount of actions (ap-\npointments) to reach the goal. Our approach ﬁnds a path\ncloser to optimal play in all but one career.\nthe number of sessions from the build A to B, but the length\nof the session also changed. While in build A, players had\nto wait in average six hours between sessions, in build B\nthe average wait time drops to about 45 minutes. This way,\nplaying build A allows faster progression, but build B incen-\ntivizes players to keep checking the game throughout the day\nto steadily improve their progress. The two different builds\nprovide very different experiences. It is up to the designers\nto decide which experience they prefer.\nComparing Approaches\nEarlier, we identiﬁed some of the limitations of running an\nagent on the game client. We now compare the results of the\ntwo different approaches, simulation based and game client\nbased, using the career experiment as the basis of our com-\nparison. We are looking to compare how many actions each\napproach needs to achieve the career goal. The agents of\neach approach use different algorithms: while the simulation\nuses A*, the game client approach uses Softmax.1\nFigure 4 shows the comparison between the two ap-\nproaches. The simulation based approach reaches a style of\ngameplay closer to optimal in all but one case. Optimal in\nthis case would be using the minimum amount of actions.\nThe Barista stands out for the large difference. For the med-\nical career, the 2000 node A* cutoff could be moving the\nalgorithm into a local optimal, while the game client Soft-\nmax was closer to optimal play.\nWe can also compare the number of simulations needed\nfor signiﬁcant results. We ran 2000 simulations for each ap-\nproach, and the conclusion is obvious. The simulation with\nA* agent achieves a deterministic playstyle, having no vari-\nance. In contrast, the game client Softmax agent has high\nvariance requiring numerous simulations for convergence.\nFigure 5 compares the variance between the two approaches.\nA single run of A* already achieves our goals, which bal-\nances the time spent re-implementing the mechanics.\n1Namely, we used Softmax over utility of valid actions, trained\nwith stochastic gradient ascent to optimize linear weights of the\naction parameters. During the model execution, lower temperature\nreduced the variance of the results.\nFigure 5: Comparing the variance between the two ap-\nproaches. While A* on the simulation approach achieves\ndeterministic play and has no variance, the Softmax game\nclient approach has high variance and needs a considerable\nnumber of simulations to converge.\nDiscussion and Conclusion\nWe illustrated the advantages of AI-based playtesting in\ngame development and how it can help designers to val-\nidate their work. We have shown the limitations of trying\nto implement an AI agent on the game client and proposed\nthe approach of building a simulator of the game mechan-\nics. Our approach gives us full control over the experiments\nand avoids the difﬁculties of coupling with an instrumented\ngame client. We have also highlighted the power of this tech-\nnique with four use cases proposed by the game team and\nwhich results were later presented to the designers, inform-\ning decisions they took to make changes.\nBecause most games keep their data in a standard format,\nit is becoming easier to write a simulation outside of the\ngame client. These tools may come in existence even before\na playable game client is built, even as the game is being de-\nsigned. AI can then have an even bigger impact in playtest-\ning, assisting from early stages in the game development,\nhelping speed up the production cycle, saving time and ef-\nforts while achieving more balanced gameplay.\nWe have shown that our approach produces overall\nstronger results by empowering search-based algorithms. A\nbasic algorithm such as A* achieves more precise results\nthan agents running within the limitations of the game client.\nA* delivered convincing results, but for the price of de-\nveloping and tuning the heuristic function. The Monte Carlo\nTree Search algorithm could be a viable alternative elimi-\nnating this overhead of making a new heuristic in favor of\na custom win condition for each experiment. MCTS Agents\nwere proven successful at gameplaying, and we believe it\nwould be no different for a game such as The Sims Mobile.\nReferences\nBrowne, C., and Maire, F. 2010. Evolutionary game design.\nIEEE Transactions on Computational Intelligence and AI in\nGames 2(1):1–16.\n164\nBrowne, C. 2008. Automatic generation and evaluation of\nrecombination games. Phd thesis, Queensland University of\nTechnology.\nde Mesentier Silva, F.; Isaksen, A.; Togelius, J.; and Nealen,\nA. 2016. Generating heuristics for novice players.\n2016 IEEE Conference on Computational Intelligence and\nGames.\nde Mesentier Silva, F.; Lee, S.; Togelius, J.; and Nealen, A.\n2017a. Ai as evaluator: Search driven playtesting of modern\nboard games. In AAAI 2017 Workshop on What’s Next for\nAI in Games.\nde Mesentier Silva, F.; Lee, S.; Togelius, J.; and Nealen, A.\n2017b. Ai-based playtesting of contemporary board games.\nIn Foundations of Digital Games 2017. ACM.\nde Mesentier Silva, F.; Lee, S.; Togelius, J.; and Nealen, A.\n2018a. Evolving maps and decks for ticket to ride. The\n9th Workshop on Procedural Content Generation at Foun-\ndations of Digital Games.\nde Mesentier Silva, F.; Togelius, J.; Lantz, F.; and Nealen,\nA. 2018b. Generating beginner heuristics for simple texas\nhold’em. The Genetic and Evolutionary Computation Con-\nference (GECCO).\nde Mesentier Silva, F.; Togelius, J.; Lantz, F.; and Nealen, A.\n2018c. Generating novice heuristics for post-ﬂop poker. In\nComputational Intelligence and Games (CIG), 2018 IEEE\nConference on, 1–8. IEEE.\nDormans, J. 2011. Simulating mechanics to study emer-\ngence in games. Artiﬁcial Intelligence in the Game Design\nProcess 2(6.2):5–2.\nHom, V ., and Marks, J. 2007. Automatic design of balanced\nboard games. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence and Interactive Digital Entertainment\n(AIIDE), 25–30.\nIsaksen, A.; Gopstein, D.; Togelius, J.; and Nealen, A. 2015.\nDiscovering unique game variants. In Computational Cre-\nativity and Games Workshop at the 2015 International Con-\nference on Computational Creativity.\nIsaksen, A.; Gopstein, D.; and Nealen, A. 2015. Exploring\ngame space using survival analysis. Foundations of Digital\nGames (FDG).\nJaffe, A.; Miller, A.; Andersen, E.; Liu, Y .-E.; Karlin, A.;\nand Popovic, Z. 2012. Evaluating competitive game balance\nwith restricted play. In Artiﬁcial Intelligence and Interactive\nDigital Entertainment (AIIDE).\nKrucher, J. 2015. Algorithmically balancing a collectible\ncard game. Bachelor’s thesis, ETH Zurich.\nLiapis, A.; Yannakakis, G. N.; and Togelius, J. 2013. Sen-\ntient sketchbook: Computer-aided game level authoring. In\nFoundations of Digital Games (FDG), 213–220.\nMahlmann, T.; Togelius, J.; and Yannakakis, G. N. 2012.\nEvolving card sets towards balancing dominion. In Evolu-\ntionary Computation (CEC), 2012 IEEE Congress on, 1–8.\nIEEE.\nNelson, M. J. 2011. Game metrics without players: Strate-\ngies for understanding game artifacts. In Proceedings of the\nFirst Workshop on AI in the Game-Design Process, 14–18.\nNielsen, T. S.; Barros, G. A.; Togelius, J.; and Nelson, M. J.\n2015. General video game evaluation using relative algo-\nrithm performance proﬁles. In European Conference on\nthe Applications of Evolutionary Computation, 369–380.\nSpringer.\nSalge, C., and Mahlmann, T. 2010. Relevant information\nas a formalised approach to evaluate game mechanics. In\nComputational Intelligence and Games (CIG), 2010 IEEE\nSymposium on, 281–288. IEEE.\nShaker, N.; Shaker, M.; and Togelius, J. 2013. Ropossum:\nAn authoring tool for designing, optimizing and solving cut\nthe rope levels. InArtiﬁcial Intelligence and Interactive Dig-\nital Entertainment (AIIDE).\nSmith, A. M.; Nelson, M. J.; and Mateas, M. 2010. Lu-\ndocore: A logical game engine for modeling videogames.\nIn Proceedings of the 2010 IEEE Conference on Computa-\ntional Intelligence and Games, 91–98. IEEE.\nSmith, G.; Whitehead, J.; and Mateas, M. 2010. Tanagra:\nA mixed-initiative level design tool. In Proceedings of the\nFifth International Conference on the Foundations of Digital\nGames, 209–216. ACM.\nYannakakis, G. N.; Liapis, A.; and Alexopoulos, C. 2014.\nMixed-initiative co-creativity. In Proceedings of the 9th\nConference on the Foundations of Digital Games.\n165",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6095114350318909
    },
    {
      "name": "Human–computer interaction",
      "score": 0.5936506986618042
    },
    {
      "name": "Space (punctuation)",
      "score": 0.5211836695671082
    },
    {
      "name": "Process (computing)",
      "score": 0.5186182856559753
    },
    {
      "name": "Game design",
      "score": 0.5103771090507507
    },
    {
      "name": "Game mechanics",
      "score": 0.4577552080154419
    },
    {
      "name": "Test (biology)",
      "score": 0.4423682987689972
    },
    {
      "name": "Game play",
      "score": 0.42225009202957153
    },
    {
      "name": "Multimedia",
      "score": 0.3493928909301758
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210113251",
      "name": "Electronic Arts (United States)",
      "country": "US"
    }
  ],
  "cited_by": 11
}