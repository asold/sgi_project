{
  "title": "Extracting Information from Brazilian Legal Documents with Retrieval Augmented Generation",
  "url": "https://openalex.org/W4403396872",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5114265085",
      "name": "Isabella V. de Aquino",
      "affiliations": [
        "Universidade Federal de Santa Catarina"
      ]
    },
    {
      "id": "https://openalex.org/A2288817093",
      "name": "Matheus M. dos Santos",
      "affiliations": [
        "Universidade Federal de Santa Catarina"
      ]
    },
    {
      "id": "https://openalex.org/A2275791825",
      "name": "Carina F. Dorneles",
      "affiliations": [
        "Universidade Federal de Santa Catarina"
      ]
    },
    {
      "id": "https://openalex.org/A4281349779",
      "name": "Jônata T. Carvalho",
      "affiliations": [
        "Universidade Federal de Santa Catarina"
      ]
    },
    {
      "id": "https://openalex.org/A5114265085",
      "name": "Isabella V. de Aquino",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2288817093",
      "name": "Matheus M. dos Santos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2275791825",
      "name": "Carina F. Dorneles",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281349779",
      "name": "Jônata T. Carvalho",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2996274640",
    "https://openalex.org/W2986692942",
    "https://openalex.org/W6713635746",
    "https://openalex.org/W2117350473",
    "https://openalex.org/W2022760666",
    "https://openalex.org/W4389984066",
    "https://openalex.org/W4378469337",
    "https://openalex.org/W4404534210",
    "https://openalex.org/W6858023062",
    "https://openalex.org/W4309217888",
    "https://openalex.org/W4318586794",
    "https://openalex.org/W1539986401",
    "https://openalex.org/W4391876619",
    "https://openalex.org/W4393002641",
    "https://openalex.org/W3137695810",
    "https://openalex.org/W3096266342",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4284676849",
    "https://openalex.org/W2091894391",
    "https://openalex.org/W2084458362"
  ],
  "abstract": "Extracting information from unstructured data is a challenge that has drawn increasing attention over time due to the exponential growth of stored digital data in modern society. Large Language Models (LLMs) have emerged as powerful tools that benefit from this abundance and have shown remarkable capabilities in Natural Language Processing tasks. Nonetheless, these models still encounter limitations on extraction tasks. Retrieval Augmented Generation (RAG) is a novel approach that combines classic retrieval techniques and LLMs to address some of these limitations. This paper proposes a workflow that allows the assessment of RAG experimental setups, including the multiple possibilities of parameters and LLMs, to extract structured data from Brazilian legal documents. We validated our proposal with experiments using forty legal documents and the extraction of two target variables. The best results obtained with our workflow showed an average extraction accuracy of 90\\%, significantly outperforming a regular expression strategy, with 58.75\\% average accuracy. Furthermore, our results show that each extracted variable potentially holds an optimal combination of parameters, highlighting the context-dependency of each extraction and, therefore, the proposed workflow's usefulness.",
  "full_text": "Extracting Information from Brazilian Legal Documents with\nRetrieval Augmented Generation\nIsabella V . de Aquino1, Matheus M. dos Santos1, Carina F. Dorneles1, Jˆonata T. Carvalho1\n1Department of Informatics and Statistics\nFederal University of Santa Catarina (UFSC)\nP.O. Box 5064 – 88.040-370 – Florian´opolis – SC – Brazil\nisabella.aquino@grad.ufsc.br, matheus.m.santos@posgrad.ufsc.br\n{carina.dorneles,jonata.tyska}@ufsc.br\nAbstract. Extracting information from unstructured data is a challenge that has\ndrawn increasing attention over time due to the exponential growth of stored\ndigital data in modern society. Large Language Models (LLMs) have emerged\nas powerful tools that benefit from this abundance and have shown remarkable\ncapabilities in Natural Language Processing tasks. Nonetheless, these models\nstill encounter limitations on extraction tasks. Retrieval Augmented Genera-\ntion (RAG) is a novel approach that combines classic retrieval techniques and\nLLMs to address some of these limitations. This paper proposes a workflow\nthat allows the assessment of RAG experimental setups, including the multiple\npossibilities of parameters and LLMs, to extract structured data from Brazilian\nlegal documents. We validated our proposal with experiments using forty legal\ndocuments and the extraction of two target variables. The best results obtained\nwith our workflow showed an average extraction accuracy of 90%, significantly\noutperforming a regular expression strategy, with 58.75% average accuracy.\nFurthermore, our results show that each extracted variable potentially holds\nan optimal combination of parameters, highlighting the context-dependency of\neach extraction and, therefore, the proposed workflow’s usefulness.\n1. Introduction\nThe increasing digitization of judicial and administrative processes worldwide has led to\nmassive production and storage of legal documents. These documents are commonly un-\nstructured, complex and contain crucial information for lawyers, judges, and prosecutors.\nExtracting this information typically requires extensive human annotation and manage-\nment in external systems such as relational databases. In line with this scenario, several\nefforts have been made to handle and process legal documents in various countries, for\ninstance, explored in [Bach and et al. 2019] for extracting references from Vietnamese\nlegal documents, and in [Vianna and et al. 2022] for examining the processing and sum-\nmarization of Portuguese legal documents.\nIn particular, the Brazilian public legal sector is an example of an organization\ndealing with great amounts of documents; almost 200,0001 public procurement processes\nwere successfully contracted from 2020 to 2023 by the Brazilian Federal Government,\nin which each one of them requires thorough documentation to formalize every step of\nthe process. As a result, retrieving and extracting specific information, such as legal\nprocesses, contract identifiers, and involved municipalities from these numerous complex\ndocuments, poses a demanding task if done manually.\nMoving forward, information extraction (IE) is an extensively researched sub-\nject in legal domains to overcome the presented challenges and has been applied\nand evaluated through multiple approaches, such as traditional pattern matching\n1https://portaldatransparencia.gov.br/licitacoes\nCompanion Proceedings of the 39 th Brazilian Symposium on Data Bases October 2024 – Florian´ opolis, SC, Brazil\n280\n[Cheng and et al. 2009]. Likewise, the work presented in [Kowsrihawat and et al. 2015]\nachieves expressive results in extracting variables in legal documents through a proposed\nframework utilizing regular expressions. Overall, IE in legal domains is a rapidly evolv-\ning field with the potential to transform how legal professionals work and automating\ninformation extraction can provide valuable insights to legal entities and potentially aid\nbroader analyses to detect and prevent fraud and corruption.\nThen, Large Language Models have emerged offering the promise of understand-\ning and generating human-like text at scale and in the legal domain [Katz and et al. 2023].\nHowever, despite their impressive performance and variety of applications, LLMs still\nface inherent limitations when extracting structured information from unstructured data\nsources. LLMs knowingly struggle with domain-specific or knowledge-intensive tasks\n[Kandpal and et al. 2023], have their performance degraded when dealing with relevant\ninformation in the middle of long contexts [Liu and et al. 2023] and tend to produce ”hal-\nlucinations” [Huang and et al. 2023] when searching for information beyond their train-\ning data.\nIn response to these challenges, Retrieval Augmented Generation (RAG) has\nemerged as a promising approach for enhancing the capabilities of LLMs in information\nextraction tasks [Gao and et al. 2024]. By combining classic retrieval techniques with\nLLMs, RAG systems enable the retrieval of relevant information from external sources\nduring text generation, thereby mitigating domain-specific and context window limita-\ntions of LLMs and improving the accuracy and coherence of the generated text.\nThis paper proposes a workflow that leverages LLMs within RAG pipelines to\nextract structured information from Brazilian legal documents related to fraud in public\nprocurement processes. However, RAG is a data-driven general framework, and its setup\ncan be demanding once several different parameter types are required to be set before-\nhand. Our objective is to demonstrate the effectiveness of RAG in overcoming the chal-\nlenges associated with information extraction from complex, domain-specific documents\nand propose a workflow that evaluates and indicates the best RAG parameter configura-\ntions for extracting a given variable. We extracted and evaluated two different variables\nof interest in forty different Brazilian legal documents utilizing our workflow. The results\nshowed the proposed methodology’s effectiveness, which achieved an average accuracy\nof 90%, outperforming a baseline strategy based on regular expressions, which achieved\n58.5%.\n2. Related Work\nInformation extraction (IE) has become a significant explored subject\n[Doan and et al. 2006], keeping pace with the rapid increase of unstructured data\navailability in today’s data-driven world. IE tasks permeate various aspects of in-\nformation and its forms of representation and structure, including visual aspects\n[Sarkhel and et al. 2021], and consider different languages [Zhu and et al. 2012]. It\ntraditionally can be done by applying various approaches, such as the ones focused on\nannotating [Boisen and et al. 2000] or filtering [Wachsmuth and et al. 2013].\nOn that matter, Artificial Intelligence and NLP accompany IE advancements and\nresearch; [Han and et al. 2023] analyze and evaluate IE using ChatGPT, ranking LLMs\nencountered limitations, while [Wei and et al. 2024] explores IE systems chatting with\nChatGPT in zero-shot settings.\nFinally, IE is highly useful in legal applications, which commonly deal with ex-\npressive volumes of unstructured information. [Bhattacharya and et al. 2019] automati-\ncally identifies rhetorical roles in Indian legal cases. Then, [Pereira and et al. 2024] in-\ntroduces basic information extraction from Brazilian audit court documents integrating\nCompanion Proceedings of the 39 th Brazilian Symposium on Data Bases October 2024 – Florian´ opolis, SC, Brazil\n281\nLLMs in a retrieval-augmented generation workflow.\nGiven the foregoing, extracting information from legal documents commonly en-\ncounters difficulties, such as formatting and structure variability, complicating pattern-\nmatching strategies. As for NLP-driven strategies, sole LLMs are greatly affected by\nirrelevant and longer context, a big aspect of legal documents. Our work addresses these\nbottlenecks by enabling contextualization of variables and overcoming the need to feed\nentire documents into prompts with RAG, highlighting the usefulness of the paper.\n3. Method\nOur method is structured around a main RAG pipeline for the extraction, executing mul-\ntiple times iteratively across multiple possible parameter configurations of RAG parame-\nters. This approach facilitates comprehensive comparisons across various parameter sets,\npotentially identifying an optimal configuration for extracting a given variable. The list\nbelow outline these parameters, grouping them by types:\n• Generation: Parameters related to the generation step of RAG. The following parame-\nter can be tested: Large Language Models (LLMs);\n• Chunking: Parameters related to the documents chunking strategy. The following\nparameters can be tested: chunk size, which is the size of each of the split chunks of\ntext; chunk overlap, which is the size of text overlap between adjacent chunks; and\nsplitting strategy, which is usually the text splitter used to execute the chunking;\n• Embeddings: Parameters related to the embeddings to be generated. The following\nparameter can be tested: embedding model, used to generate the embeddings from the\ndocuments’ chunks, e.g. BERT models;\n• Retrieval: Parameters related to the retrieval step of RAG. The following parameters\ncan be tested: vector database, responsible to store and retrieve the embeddings and\nTop K value, which is the K-amount of retrieved chunks to serve as context on the\nextraction.\nCreate\nEmbeddings\n2\nExtract \nvariable\n3\nSampled\nDocuments\nFor each \ndocument Evaluate \nresponses\n4\nFor each \nRAG\nparameter\ncon\u0000guration\nRaw\ndocuments\nAnnotating \n(for n variables)\nSTART\nEND\nAssets\nGeneration Chunking Embeddings Retrieval\nLarge\nLanguage\nModel\nChunk Size,\nChunk Overlap,\nSpli\u0000ing Strategy\nEmbedding\nModel\nVector Database,\nTop K\nRAG parameter types\n1\nSampling\nSampled\ndocuments\nBest RAG parameter con\u0000guration \nand perfomance estimation \nfor extracting variable i\n5...n-th variable\nAnnotated \ndocuments\n(per n variable)\nFigure 1. Main workflow overview.\nFigure 1 depicts the overview of the proposed workflow, which will cover all com-\nbinations of parameter settings possible to extract the same chosen variable and compare\nthe results among each other. A parameter configuration is a unique set of values for each\nof the RAG parameter types previously described due to iterating through all the available\nparameter options. Our proposal is based on the following steps:\n• Step 1 establishes the beginning of our proposed workflow, initiated by sampling the\ndocuments available. We ensure all of them will have an expected value to be extracted\nfor a given query and manually annotate every sampled document with its expected\nvalues. This step is the base of our further evaluation assessment step, represented by\nstep number 4.\nCompanion Proceedings of the 39 th Brazilian Symposium on Data Bases October 2024 – Florian´ opolis, SC, Brazil\n282\n• Step 2 iterates all possible parameter configuration combinations within the selected\noptions for each parameter type. Step 2 represents the embeddings creation for every\nsampled PDF document using the current configuration for chunking and embeddings.\nThese embeddings will be used during the extraction step in the workflow.\n• Step 3 constitutes the main RAG pipeline. It will retrieve the most relevant embeddings\ngenerated in the previous step related to a given query. It will insert them as the context\nin a prompt template and return direct responses containing or not the answer for the\ntask.\n• Step 4 evaluates all the extracted responses by comparing them directly to the forego-\ning annotated values, labeling as correct the responses that contain the exact expected\ninformation for a query.\n• Step 5 outputs the best results parameter configurations and performance estimation for\neach extracted variable.\nLastly, a key aspect of this process is the iterative nature of the main pipeline ex-\necution. Various parameters are systematically altered based on a range of parameters\nvalues to be tested. This approach enables the evaluation across configurations and iden-\ntification of the potential most suitable parameter setting for extracting a certain variable\nfrom the documents. It also aids decision-making in selecting from the potential options\nthat can be applied to general RAG pipeline parameters by offering comparative results\nfor each configuration and highlighting the best-obtained ones. In our proposed work-\nflow, any of the previously stated parameters can be iteratively tested and analyzed by\ndetermining which options for each parameter type will be covered.\n4. Experimental Evaluation\nWe analyzed and ran our experiments with forty selected Brazilian legal documents pro-\nvided by Santa Catarina Government Agency for Law Enforcement and Prosecution of\nCrimes (MPSC), with an average of 26 pages and 60,000 characters each. Moreover, as\nmentioned in Section 3, while our proposed workflow allows the variation of any of the\ngeneral RAG parameters, our experiments focused on alternating the parameters Large\nLanguage Models (Llama-2-7b, Llama-2-13b and Mistral-7B-v0.2), Chunk Size (128,\n256, 512), Chunk Overlap Size (20, 50, 100, 200) and Top K (1, 3, 5, 8, 10, 12) and\nmaintaining BERT Model, Splitting Strategy and Vector database as fixed parameters.\n4.1. Data Preparation\nThe first step in the experiment, equivalent to step number 1 in Figure 1, is to prepare the\navailable documents dataset to be used. This step is divided into two sub-steps: sampling\nand annotating. In the first sub-step, we filtered a smaller sample of the legal documents\nprovided by MPSC, ensuring that all documents contained our study’s analyzed variables.\nThen, to evaluate the accuracy of the experiments, the second sub-step was to manually\nannotated each selected document, mapping useful information including the variables\nto be evaluated. The annotations will be used to directly compare the model’s responses\nto the constructed prompts, thereby assessing the accuracy of each extraction on every\nexperiment.\n4.2. Embeddings Creation\nEmbeddings were generated from the pieces of text parsed and chunked previously uti-\nlizing BERTimbau Large [Souza and et al. 2020], a BERT model pre-trained in Brazilian\nPortuguese, representing Step 2 in Figure 1. They were then stored in a vector database\nto manipulate and retrieve these embeddings. Chroma2 was used as our option for vector\ndatabase, a commonly chosen option for general RAG pipelines, highlighted for being\nopen-source.\n2https://github. com/chroma-core/chroma\nCompanion Proceedings of the 39 th Brazilian Symposium on Data Bases October 2024 – Florian´ opolis, SC, Brazil\n283\n4.3. Extracting variables\nWith the embeddings stored in the database, the next step was to embed the user’s query,\nretrieve the most similar embeddings through a similar search, and finally use them as\ncontext on the prompt final form, represented by the template illustrated in Figure 2.\nThese steps correspond to the main RAG pipeline, identified by step 3 in Figure 1.\n## Instructions\nYou are a helpful AI assistant and provide the response in Portuguese to the question based on the provided context.\nUse the following chunks of context to answer the question at the end. If it is not possible to answer the question from the\ncontext, just answer that you didn't find the answer .\n## Context built with Top K relevant retrieved chunks\nCONTEXT: [RETRIEVED CHUNKS]\n>>>QUESTION<<<: [USER QUERY]\n>>>ANSWER<<<:\nFigure 2. English translation of prompt template used in every experiment.\nThe LLMs options used in our experiments were the Llama-2 family chat mod-\nels [Touvron and et al. 2023] and Mistral-7B-Instruct [Jiang and et al. 2023], all of them\nloaded locally with a RTX 3090 as the main GPU. This setup ensured privacy to handle\nthe legal document’s sensitive information, however, limited the involved models used in\nour study, making it impossible to handle bigger models on the current analysis.\n4.4. Evaluation Metrics\nWhile several aspects of evaluation around RAG can be measured [Gao and et al. 2024],\nour work primarily concentrates on direct accuracy assessment. We specifically examine\nwhether the generated response by the model precisely matches the annotated value as-\nsociated with a particular document. This evaluation occurs in step 4 in Figure 1, which\nwill divide the quantity of successfully matched extracted values by its annotation value\nby the total amount of documents.\n4.5. Evaluated extracted variables\nAs previously stated, our work focuses on extracting two variables: public procurement\nprocess identifiers and municipalities of irregularity. The public procurement process\nidentifier is a string that identifies a certain public procurement process for a munici-\npality, and it is consistently presented in the format X/YYYY , where ’X’ represents any\nnumerical sequence and ’YYYY’ denotes a four-digit year. The municipality of irregu-\nlarity refers to the name of the municipality where fraud was committed through public\nprocurement processes. Both variables have 145 associated experiments, one for every\nunique configuration possible interchanging the parameters detailed earlier in this Sec-\ntion.\n5. Results and Discussion\nAs our baseline, we built a regular expression that looked for matches using the men-\ntioned formats. When comparing the mode of the matches, extracting the variable with\na regular expression reached a maximum of 35% accuracy against the best accuracy of\n88% using our workflow when extracting public process identifiers and a maximum accu-\nracy of 82.5% compared with our best accuracy of 93% when extracting municipalities.\nThis comparison is visible in Figure 3, where the best obtained accuracies through our\nproposed method overcomes expressively the result obtained by the regular expression,\nwhen extracting public procurement process identifiers. For extracting municipalities, our\nexperiments still bests the regular expression results by 10.5%. These results underscore\nthe effectiveness of our method, overcoming regular expressions by contextualizing the\nCompanion Proceedings of the 39 th Brazilian Symposium on Data Bases October 2024 – Florian´ opolis, SC, Brazil\n284\n/uni00000030/uni0000004c/uni00000056/uni00000057/uni00000055/uni00000044/uni0000004f/uni00000010/uni0000001a/uni00000025/uni00000010/uni0000002c/uni00000051/uni00000056/uni00000057/uni00000055/uni00000058/uni00000046/uni00000057/uni00000010/uni00000059/uni00000013/uni00000011/uni00000015/uni0000004f/uni0000004f/uni00000044/uni00000050/uni00000044/uni00000010/uni00000015/uni00000010/uni00000014/uni00000016/uni00000045/uni00000010/uni00000046/uni0000004b/uni00000044/uni00000057/uni00000010/uni0000004b/uni00000049/uni0000004f/uni0000004f/uni00000044/uni00000050/uni00000044/uni00000010/uni00000015/uni00000010/uni0000001a/uni00000045/uni00000010/uni00000046/uni0000004b/uni00000044/uni00000057/uni00000010/uni0000004b/uni00000049/uni00000035/uni00000048/uni0000004a/uni00000058/uni0000004f/uni00000044/uni00000055/uni00000003/uni00000028/uni0000005b/uni00000053/uni00000055/uni00000048/uni00000056/uni00000056/uni0000004c/uni00000052/uni00000051\n/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f\n/uni00000013/uni00000008\n/uni00000014/uni00000013/uni00000008\n/uni00000015/uni00000013/uni00000008\n/uni00000016/uni00000013/uni00000008\n/uni00000017/uni00000013/uni00000008\n/uni00000018/uni00000013/uni00000008\n/uni00000019/uni00000013/uni00000008\n/uni0000001a/uni00000013/uni00000008\n/uni0000001b/uni00000013/uni00000008\n/uni0000001c/uni00000013/uni00000008\n/uni00000014/uni00000013/uni00000013/uni00000008/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000025/uni00000048/uni00000056/uni00000057/uni00000003/uni00000026/uni00000052/uni00000051/uni00000049/uni0000004c/uni0000004a/uni00000058/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni0000003a/uni00000052/uni00000055/uni00000056/uni00000057/uni00000003/uni00000026/uni00000052/uni00000051/uni00000049/uni0000004c/uni0000004a/uni00000058/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n(a) Public Procurement Process Id.\n/uni00000030/uni0000004c/uni00000056/uni00000057/uni00000055/uni00000044/uni0000004f/uni00000010/uni0000001a/uni00000025/uni00000010/uni0000002c/uni00000051/uni00000056/uni00000057/uni00000055/uni00000058/uni00000046/uni00000057/uni00000010/uni00000059/uni00000013/uni00000011/uni00000015/uni0000004f/uni0000004f/uni00000044/uni00000050/uni00000044/uni00000010/uni00000015/uni00000010/uni00000014/uni00000016/uni00000045/uni00000010/uni00000046/uni0000004b/uni00000044/uni00000057/uni00000010/uni0000004b/uni00000049/uni0000004f/uni0000004f/uni00000044/uni00000050/uni00000044/uni00000010/uni00000015/uni00000010/uni0000001a/uni00000045/uni00000010/uni00000046/uni0000004b/uni00000044/uni00000057/uni00000010/uni0000004b/uni00000049/uni00000035/uni00000048/uni0000004a/uni00000058/uni0000004f/uni00000044/uni00000055/uni00000003/uni00000028/uni0000005b/uni00000053/uni00000055/uni00000048/uni00000056/uni00000056/uni0000004c/uni00000052/uni00000051\n/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f\n/uni00000013/uni00000008\n/uni00000014/uni00000013/uni00000008\n/uni00000015/uni00000013/uni00000008\n/uni00000016/uni00000013/uni00000008\n/uni00000017/uni00000013/uni00000008\n/uni00000018/uni00000013/uni00000008\n/uni00000019/uni00000013/uni00000008\n/uni0000001a/uni00000013/uni00000008\n/uni0000001b/uni00000013/uni00000008\n/uni0000001c/uni00000013/uni00000008\n/uni00000014/uni00000013/uni00000013/uni00000008/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000025/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni0000003a/uni00000052/uni00000055/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c (b) Municipality of Irregularity\nFigure 3. Best and Worst results per model vs Regular Expression\nvariables on the prompt fed to the LLMs. The built regular expressions for public pro-\ncurement process identifier and municipality of irregularity are \\b\\d+\\/\\d{4}\\b and\nMunic´ıpio de ([A-Z][a-z]+(?:\\s[A-Z][a-z]+)*), respectively.\nThen, Figure 4 illustrates the Top K evolution and its impact on obtained accu-\nracies on extracting both variables on fixed chunk sizes and chunk overlap. It suggests\nthat the increase of Top k values directly impacts on the accuracy in pipelines with small\nchunks, increasing the probability of the retriever returning the accurate answer among\nthe available embeddings.\n/uni00000014 /uni00000016 /uni00000018 /uni0000001b /uni00000014/uni00000013/uni00000014/uni00000015\n/uni00000037/uni00000052/uni00000053/uni00000003/uni0000002e\n/uni00000016/uni00000013/uni00000008\n/uni00000017/uni00000013/uni00000008\n/uni00000018/uni00000013/uni00000008\n/uni00000019/uni00000013/uni00000008\n/uni0000001a/uni00000013/uni00000008\n/uni0000001b/uni00000013/uni00000008\n/uni0000001c/uni00000013/uni00000008/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000030/uni0000004c/uni00000056/uni00000057/uni00000055/uni00000044/uni0000004f/uni00000010/uni0000001a/uni00000025/uni00000010/uni0000002c/uni00000051/uni00000056/uni00000057/uni00000055/uni00000058/uni00000046/uni00000057/uni00000010/uni00000059/uni00000013/uni00000011/uni00000015\n/uni0000004f/uni0000004f/uni00000044/uni00000050/uni00000044/uni00000010/uni00000015/uni00000010/uni00000014/uni00000016/uni00000045/uni00000010/uni00000046/uni0000004b/uni00000044/uni00000057/uni00000010/uni0000004b/uni00000049\n/uni0000004f/uni0000004f/uni00000044/uni00000050/uni00000044/uni00000010/uni00000015/uni00000010/uni0000001a/uni00000045/uni00000010/uni00000046/uni0000004b/uni00000044/uni00000057/uni00000010/uni0000004b/uni00000049\n(a) Public Procurement Process Id.\n/uni00000014 /uni00000016 /uni00000018 /uni0000001b /uni00000014/uni00000013/uni00000014/uni00000015\n/uni00000037/uni00000052/uni00000053/uni00000003/uni0000002e\n/uni00000018/uni00000013/uni00000008\n/uni00000019/uni00000013/uni00000008\n/uni0000001a/uni00000013/uni00000008\n/uni0000001b/uni00000013/uni00000008\n/uni0000001c/uni00000013/uni00000008/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000030/uni0000004c/uni00000056/uni00000057/uni00000055/uni00000044/uni0000004f/uni00000010/uni0000001a/uni00000025/uni00000010/uni0000002c/uni00000051/uni00000056/uni00000057/uni00000055/uni00000058/uni00000046/uni00000057/uni00000010/uni00000059/uni00000013/uni00000011/uni00000015\n/uni0000004f/uni0000004f/uni00000044/uni00000050/uni00000044/uni00000010/uni00000015/uni00000010/uni00000014/uni00000016/uni00000045/uni00000010/uni00000046/uni0000004b/uni00000044/uni00000057/uni00000010/uni0000004b/uni00000049\n/uni0000004f/uni0000004f/uni00000044/uni00000050/uni00000044/uni00000010/uni00000015/uni00000010/uni0000001a/uni00000045/uni00000010/uni00000046/uni0000004b/uni00000044/uni00000057/uni00000010/uni0000004b/uni00000049 (b) Municipality of Irregularity\nFigure 4. Top K evolution with fixed chunk size as 128 and chunk overlap as 20.\n6. Conclusion and Future Works\nIn conclusion, legal documents are often extensive and irregularly structured, and ex-\ntracting relevant and structured data from these documents still poses a significant chal-\nlenge. In this paper, we presented and evaluated a promising approach utilizing retrieval-\naugmented generation to extract two different variables of interest, obtaining an average\naccuracy of 90%, which overcame pattern matching measured accuracies in both scenar-\nios. Our work addresses a common bottleneck for traditional extraction techniques —\ncontextualization, and is part of a new paradigm of zero-shot IE, not requiring training or\nfinetuning any models, representing a step forward on IE in legal domains.\nFinally, our future works will focus on overcoming dataset and hardware limita-\ntions, in order to evaluate more expressive samples and include more robust Large Lan-\nguage Models. It will also be centralized in formalizing the proposed method as a RAG\nparameter evaluator framework for any type RAG pipeline for any system.\nCompanion Proceedings of the 39 th Brazilian Symposium on Data Bases October 2024 – Florian´ opolis, SC, Brazil\n285\nReferences\nBach and et al. (2019). Reference extraction from vietnamese legal documents. SoICT\n’19, page 486–493, New York, NY , USA. Association for Computing Machinery.\nBhattacharya, P. and et al. (2019). Identification of rhetorical roles of sentences in indian\nlegal judgments.\nBoisen, S. and et al. (2000). Annotating resources for information extraction. InProceed-\nings of the Second International Conference on Language Resources and Evaluation\n(LREC’00), Athens, Greece. European Language Resources Association (ELRA).\nCheng and et al. (2009). Information extraction from legal documents. In 2009 Eighth\nInternational Symposium on Natural Language Processing.\nDoan, A. and et al. (2006). Managing information extraction: state of the art and research\ndirections. In Proceedings of the 2006 ACM SIGMOD International Conference on\nManagement of Data, SIGMOD ’06’, page 799–800, New York, NY , USA. Associa-\ntion for Computing Machinery.\nGao, Y . and et al. (2024). Retrieval-augmented generation for large language models: A\nsurvey.\nHan, R. and et al. (2023). Is information extraction solved by chatgpt? an analysis of\nperformance, evaluation criteria, robustness and errors.\nHuang, L. and et al. (2023). A survey on hallucination in large language models: Princi-\nples, taxonomy, challenges, and open questions.\nJiang, A. Q. and et al. (2023). Mistral 7b.\nKandpal, N. and et al. (2023). Large language models struggle to learn long-tail knowl-\nedge.\nKatz, D. M. and et al. (2023). Natural language processing in the legal domain.\nKowsrihawat and et al. (2015). An information extraction framework for legal docu-\nments: A case study of thai supreme court verdicts. In 2015 12th International Joint\nConference on Computer Science and Software Engineering (JCSSE), pages 275–280.\nIEEE.\nLiu, N. F. and et al. (2023). Lost in the middle: How language models use long contexts.\nPereira, J. and et al. (2024). Inacia: Integrating large language models in brazilian audit\ncourts: Opportunities and challenges. Digit. Gov.: Res. Pract.\nSarkhel, R. and et al. (2021). Improving information extraction from visually rich docu-\nments using visual span representations. Proc. VLDB Endow., 14(5):822–834.\nSouza, F. and et al. (2020). BERTimbau: pretrained BERT models for Brazilian Por-\ntuguese. In 9th Brazilian Conference on Intelligent Systems, BRACIS, Rio Grande do\nSul, Brazil, October 20-23 (to appear).\nTouvron, H. and et al. (2023). Llama 2: Open foundation and fine-tuned chat models.\nVianna and et al. (2022). Organizing portuguese legal documents through topic discovery.\nIn Proceedings of the 45th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval , SIGIR ’22, page 3388–3392, New York, NY ,\nUSA. Association for Computing Machinery.\nWachsmuth, H. and et al. (2013). Information extraction as a filtering task. In Proceed-\nings of the 22nd ACM International Conference on Information & Knowledge Manage-\nment, CIKM ’13, page 2049–2058, New York, NY , USA. Association for Computing\nMachinery.\nCompanion Proceedings of the 39 th Brazilian Symposium on Data Bases October 2024 – Florian´ opolis, SC, Brazil\n286\nWei, X. and et al. (2024). Chatie: Zero-shot information extraction via chatting with\nchatgpt.\nZhu, W. and et al. (2012). Cross language information extraction for digitized textbooks\nof specific domains. In 2012 IEEE 12th International Conference on Computer and\nInformation Technology, pages 1114–1118.\nCompanion Proceedings of the 39 th Brazilian Symposium on Data Bases October 2024 – Florian´ opolis, SC, Brazil\n287",
  "topic": "Information retrieval",
  "concepts": [
    {
      "name": "Information retrieval",
      "score": 0.7139758467674255
    },
    {
      "name": "Computer science",
      "score": 0.6945584416389465
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4104125",
      "name": "Universidade Federal de Santa Catarina",
      "country": "BR"
    }
  ]
}