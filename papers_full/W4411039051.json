{
  "title": "Across Generations, Sizes, and Types, Large Language Models Poorly Report Self-Confidence in Gastroenterology Clinical Reasoning Tasks",
  "url": "https://openalex.org/W4411039051",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A4281154951",
      "name": "Nariman Naderi",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A3031925457",
      "name": "Seyed Amir Ahmad Safavi-Naini",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2114695148",
      "name": "Thomas Savage",
      "affiliations": [
        "California University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A2184043697",
      "name": "Mohammad Amin Khalafi",
      "affiliations": [
        "Shahid Beheshti University of Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A1964490877",
      "name": "Peter B. Lewis",
      "affiliations": [
        "First Technical University",
        "University of Ontario Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2182611525",
      "name": "zahra Atf",
      "affiliations": [
        "First Technical University",
        "University of Ontario Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2419298921",
      "name": "Girish Nadkarni",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2036204426",
      "name": "ali soroush",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4406152279",
    "https://openalex.org/W4378509483",
    "https://openalex.org/W4381797997",
    "https://openalex.org/W4383468784",
    "https://openalex.org/W4400612054",
    "https://openalex.org/W4405630972",
    "https://openalex.org/W4402502387",
    "https://openalex.org/W4401498253",
    "https://openalex.org/W4403362497",
    "https://openalex.org/W4403160758",
    "https://openalex.org/W4400025475"
  ],
  "abstract": "<title>Abstract</title> This study evaluated confidence calibration across 48 large language models (LLM) using 300 gastroenterology board exam style questions. Regardless of response accuracy, all models demonstrated poor certainty estimation. Even the best-calibrated systems (o1 preview, GPT-4o, Claude-3.5-Sonnet) showed substantial overconfidence (Brier scores 0.15-0.2, AUROC ~0.6). Most concerning, models maintained high certainty regardless of question difficulty or their actual knowledge limitations. This metacognitive deficiency poses significant challenges for safe clinical implementation of current LLMs in gastroenterology.",
  "full_text": "Page 1/18\nAcross Generations, Sizes, and Types, Large\nLanguage Models Poorly Report Self-Con\u0000dence in\nGastroenterology Clinical Reasoning Tasks\nNariman Naderi \nIcahn School of Medicine at Mount Sinai\nSeyed Amir Ahmad Safavi-Naini \nIcahn School of Medicine at Mount Sinai\nThomas Savage \nUniversity of Pennsylvania\nMohammad Amin Khala\u0000 \nShahid Beheshti University of Medical Sciences\nPeter Lewis \nOntario Tech University\nZahra Atf \nOntario Tech University\nGirish Nadkarni \nIcahn School of Medicine at Mount Sinai\nAli Soroush \nIcahn School of Medicine at Mount Sinai\nShort Report\nKeywords: Large Language Models, Metacognition, Arti\u0000cial Intelligence, Gastroenterology, Uncertainty\nQuanti\u0000cation\nPosted Date: June 4th, 2025\nDOI: https://doi.org/10.21203/rs.3.rs-6725427/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: Competing interest reported. The authors declare no competing \u0000nancial\ninterests or personal relationships that could have in\u0000uenced the work reported in this study. NN: none;\nPage 2/18\nSAASN: none; TS: none; MK: none; PL: none; ZA: none; GN: is a founder of Renalytix, Pensieve, and Verici\nand provides consultancy services to AstraZeneca, Reata, Renalytix, and Pensieve. He also has equity in\nRenalytix, Pensieve, and Verici.; AS: is on the advisory board and has equity in Virgo Surgical Solutions;\nPage 3/18\nAbstract\nThis study evaluated con\u0000dence calibration across 48 large language models (LLM) using 300\ngastroenterology board exam style questions. Regardless of response accuracy, all models demonstrated\npoor certainty estimation. Even the best-calibrated systems (o1 preview, GPT-4o, Claude-3.5-Sonnet)\nshowed substantial overcon\u0000dence (Brier scores 0.15-0.2, AUROC ~0.6). Most concerning, models\nmaintained high certainty regardless of question di\u0000culty or their actual knowledge limitations. This\nmetacognitive de\u0000ciency poses signi\u0000cant challenges for safe clinical implementation of current LLMs in\ngastroenterology.\nIntroduction\nLarge Language Models (LLMs) are rapidly transforming healthcare, but their tendency to present\nincorrect information with convincing terminology, termed “hallucinations”, poses substantial safety\nconcerns when used in clinical settings 1,2. In high-stakes medical environments, such misinformation\ncan lead to misdiagnosis, inappropriate treatment selection, or failure to identify critical patient\nconditions 3,4. Communicating uncertainty is therefore essential for providing clinicians with reliable\nindicators of when model outputs should be treated with caution. \nVarious approaches have been developed to quantify LLM uncertainty, ranging from intrinsic methods\nanalyzing model internals (token probabilities, attention patterns) to extrinsic techniques (ensemble\ndisagreement, calibration layers, surrogate models) 5–7. However, many of these methods require\nsubstantial computational resources or specialized expertise to implement and interpret. Among these\nuncertainty quanti\u0000cation techniques, self-reported con\u0000dence through natural language offers unique\npractical advantages for clinical implementation. Unlike complex technical approaches that require\nspecialized expertise to interpret, self-reported con\u0000dence provides immediately interpretable outputs in\nthe same natural language format as the provided clinical information 8–11. This approach requires\nminimal technical overhead and is intuitive to understand, making it particularly appealing for real-world\nhealthcare applications where simplicity and interpretability are paramount. However, the effectiveness of\nLLM self-reported con\u0000dence depends critically on their language-based metacognition, or the ability to\naccurately monitor and evaluate one's own knowledge boundaries and reasoning processes. \nPrior research suggests LLM self-reported con\u0000dence assessments are poorly calibrated expressions,\ndemonstrating a lack of meaningful metacognitive capabilities 12,13. However, prior studies examined this\ncon\u0000dence-accuracy gap have primarily examined general clinical reasoning tasks and a limited subset of\nmodel deployment conditions.  Our systematic analysis across model architectures, parameter scales,\nand training methodologies examines whether con\u0000dence miscalibration represents a universal limitation\nor varies meaningfully across model families. These insights can inform the development of more\ntrustworthy clinical AI systems and identify whether certain models and environments inherently promote\nbetter self-reported con\u0000dence.\nPage 4/18\nWe evaluated self-reported con\u0000dence for 48 commercial and open-source LLMs across local, web, and\nAPI-based environments using the 2022 American College of Gastroenterology self-assessment\nexamination containing 300 board exam-style multiple-choice questions. Gastroenterology was selected\nas our test domain primarily because the senior author (AS) is a practicing gastroenterologist, providing\ndirect clinical insight into the impact of LLM con\u0000dence miscalibration. Subspecialty domains like\ngastroenterology also present unique challenges for clinical reasoning that make them ideal test\nenvironments since they require integration of diverse knowledge sources to formulate diagnoses and\ninvolve procedures with signi\u0000cant risks, where diagnostic or treatment errors can lead to serious patient\nharm. We used standardized board exam-style questions because they offer an objective benchmark for\nevaluating model performance across a range of clinically relevant scenarios. \nWe employed a systematic approach where models were instructed to select the correct answer choice\nto each board exam question and explicitly report their con\u0000dence on a 0-10 scale (from least to most\ncon\u0000dent). Building on our established methodology, we optimized model parameters including prompt\ninstructions, temperature settings, and token limit to maximize response accuracy.14 A semi-automated\nextraction pipeline with human veri\u0000cation (99% accuracy, Supplementary Figure S1) was used to process\nthe responses and con\u0000dence scores for subsequent analysis. \nWe extracted 13,362 answers and 12,307 con\u0000dence scores (Figure 1). The difference between these\ncounts resulted primarily from non-compliance with prompt instructions (n=846) or from reasoning\nmodels that exhausted their token limits because of their internal reasoning dialogues (n=209)\n(Supplementary Figure S2). Mean con\u0000dence scores ranged from 7.99 (95% CI: 7.89-8.09) for Claude-3-\nOpus to 9.58 (95% CI: 9.45-9.71) for Mistral-7b, while accuracy varied substantially from 30.3% (Llama3-\n8b-Q8) to 81.5% (o1 preview) (Table 1). All models demonstrated systematic overcon\u0000dence, with\naverage con\u0000dence consistently exceeding average accuracy (Figure 2).\nWe also observed a substantial overlap in con\u0000dence distributions between correct and incorrect\nresponses, indicating limited discriminative capacity (Figure 3). This means models expressed high\ncertainty regardless of whether their answers were right or wrong—a critical safety issue in clinical\nsettings. We quanti\u0000ed this observation through discrimination metrics. Even the best-performing model\n(o1 mini) achieved an Area Under the Receiver Operating Characteristic (AUROC) of only 0.626, well below\nthe 0.7 threshold typically considered meaningful for clinical applications (Table 1; Supplementary Figure\nS3). This pattern was consistent across all model families. \nCalibration measurements reinforced these \u0000ndings. Only 5 of the 48 models demonstrated better-than-\nrandom performance. o1-preview showed the best calibration (Brier score: 0.157), followed by Claude-3.5-\nSonnet (0.202) and GPT-4o (0.206) (Supplementary Figure S4). The calibration curves in Figure 4\nand Supplementary Figure S5 visually con\u0000rm this systematic overcon\u0000dence trend. Expected Calibration\nError measurements reinforced these \u0000ndings, with even the best models (o1-preview: 0.100, Claude-3.5-\nSonnet: 0.122) showing meaningful deviations from ideal calibration (Supplementary Figure S6). \nPage 5/18\nPerhaps most alarming for clinical applications, we found that models maintained high con\u0000dence levels\neven as their accuracy signi\u0000cantly decreased on more challenging questions (Figure 5). This relationship\nwas universally observed, as even the best-calibrated models (Figure 5a-c) showed the same\novercon\u0000dence on di\u0000cult questions as poorly calibrated ones (Figure 5d-f). We also investigated\nwhether question length affected con\u0000dence assessments, \u0000nding that con\u0000dence scores remained\nremarkably stable regardless of text complexity and had no meaningful relationship with actual\nperformance (Supplementary Figure S7). This suggests models lack the awareness to recognize that\nlonger, potentially more complex questions could reduce their response certainty.\nLooking at differences between model families, we observed generational improvements in self-assessed\ncon\u0000dence performance. Newer versions consistently outperformed their predecessors. For example, o1\nshowed better calibration than GPT-4o, which in turn outperformed GPT-4 (Table 1). Commercial models\ngenerally demonstrated superior uncertainty estimation compared to equivalent open-source alternatives,\nthough this pattern had notable exceptions. We also found that quantization, while enabling deployment\non less powerful hardware, typically degraded calibration quality (as seen when comparing Llama 3 8B\nwith its quantized counterpart). Additional analysis of middle-performing models further con\u0000rmed these\ntrends (Supplementary Figure S5).\nOur \u0000ndings con\u0000rm previous research highlighting the limitations of LLM self-reported con\u0000dence. 15,16\nWe extend those \u0000ndings with three additional contributions. First, we present the most comprehensive\ncross-architectural evaluation to date, testing 48 LLMs—from 7 B to 175 B parameters—across\ncommercial, open-source, and quantized deployments. Second, by using gastroenterology board-style\nquestions, we deliver key domain-speci\u0000c insights. Third, we show quantitatively that all models suffer a\ncommon metacognitive de\u0000ciency, in which even the best-calibrated LLMs remain systematically\novercon\u0000dent, regardless of question di\u0000culty. This pervasive overcon\u0000dence transcends architecture,\nscale, and deployment environment, pointing to a fundamental limitation of current neural language\nmodels. \nWhile we observed that newer model generations (for example, o1 and Claude 3.5) achieve modestly\nbetter calibration metrics, 15 this improved calibration tracks closely with higher overall accuracy (Table\n1). It remains unclear whether these gains re\u0000ect genuine uncertainty awareness or simply byproducts of\nstronger performance 17. In contrast, across all models, we observed high, unvarying self-reported\ncon\u0000dence scores, irrespective of question di\u0000culty (Figure 5), model generation (Figure 2), or\ncorrectness (Figure 3). This core observation suggests that con\u0000dence outputs are no more than\nstatistically likely text, not true re\u0000ections of internal uncertainty. In other words, LLMs are reciting the\nmost probable “con\u0000dence score” token, rather than expressing insight into their own knowledge\nboundaries. Future efforts should explore architectural innovations or training objectives that explicitly\nfoster genuine metacognitive capabilities—such as self-explanation or introspective feedback loops—\nrather than relying on incremental prompt or calibration tweaks.\nPage 6/18\nSeveral limitations temper our conclusions. While our use of multiple-choice gastroenterology board\nexam style questions offers a clear, objective benchmark, this approach may not generalize to open-\nended clinical reasoning or to other medical specialties. Our standardized prompt engineering approach,\nwhich was designed to maximize accuracy, could itself bias models toward overcon\u0000dent “expert”\nlanguage. Finally, the ACG self-assessment questions are proprietary and only accessible to paying\nsubscribers or via direct request for the structured data. We cannot rule out that LLMs may have seen\nthese questions or similar content during their pretraining.\nDespite these caveats, our \u0000ndings highlight critical AI safety gaps: LLMs uniformly overestimate their\ncertainty, poorly discriminate correct from incorrect answers, and fail to adjust con\u0000dence for harder\nquestions. In high-stakes clinical settings, reliable expressions of decision certainty are essential for safe\nhuman–AI collaboration. While newer LLM generations offer incremental calibration improvements, none\napproach the reliability required for autonomous decision support. Addressing this con\u0000dence-accuracy\ngap will be vital to protect patient safety and foster appropriate clinician trust as we integrate LLMs into\nhealthcare work\u0000ows.\nMethods\nReference dataset\nThe 2022 American College of Gastroenterology (ACG) self-assessment consists of 300 questions, of\nwhich 138 contain images. These questions were developed by a committee of gastroenterologists to\nre\u0000ect the knowledge, skills, and attitudes required for excellent patient care, covering a broad range of\ntopics, including liver, colon, esophagus, pancreaticobiliary, and endoscopy. The questions were designed\nto assess higher-order thinking skills and were primarily case based. They were validated through\nstatistical analysis of test-takers' performance, with an average correctness rate of 74.52% ± 19.49% on\nthe 2022 assessment, indicating a moderate level of di\u0000culty. Only the text portions of the questions and\nanswers were used in this study’s analyses. Questions were categorized by length (token count), di\u0000culty\n(percentage of correct answers by test-takers), and patient care phase (treatment, diagnosis, or\ninvestigation). Additional details are provided in the Supplementary Section 1.\nResponse Generation and Con\u0000dence Score Elicitation\nFor response generation and con\u0000dence score elicitation, we built upon our established methodology,12\nusing 60 questions from the 2023 self-assessment exam and GPT-3.5 to select the model settings\n(temperature, maximum input, and output token count), prompt structure, and output format of all\nmodels. The con\u0000guration that maximized response accuracy was a temperature of 1, maximum token\ncount of input token count + 512 output tokens, structured output approach, and prompt (Fig. 1). Among\nthe various prompt engineering techniques evaluated, the following were identi\u0000ed as having a positive\nimpact on the outcomes: expert mimicry, contextual embedding, Answer and Justify, Chain of Thought,\ncon\u0000dence scoring, and direct questioning. OpenAI Web interface, OpenAI API, Claude Web interface,\nPage 7/18\nClaude API, Gemini Web interface, Poe Web interface, Firework API, and locally hosted hardware\ncon\u0000gurations such as RTX4090Ti and H100 systems were used for response generation and con\u0000dence\nscore elicitation.\nOutput Parsing\nTo e\u0000ciently extract response and con\u0000dence data from the LLM outputs, we developed a structured\noutput pipeline using GPT-4o (Fig. 1). Our hybrid methodology combined regex-based rules to reduce the\nnumber of input tokens and LLM-based extraction to effectively parse the key portions of the LLM\noutputs. Speci\u0000cally, the pipeline identi\u0000ed sentences containing \"con\u0000d\" for further LLM-based parsing\nto either extract the certainty score (0–10) or de\u0000ne the score as \"not_mentioned.” Sentences classi\u0000ed\nas \"not_mentioned\" in the \u0000rst pass are passed through the LLM-based parsing step a second time to\nmaximize the extraction performance. The complete output parsing methodology is described in\nSupplementary Section 1. To validate the output parsing pipeline, we compared it against manually\nextracted con\u0000dence scores from \u0000ve randomly selected questions per model, achieving 98.8% accuracy\n(Supplementary Figure S1).\nBecause some models did not reliably generate con\u0000dence scores, we excluded those that were missing\ncon\u0000dence scores for more than 50% of the questions (Medicine-Chat Q8, OpenBioLLM-7B Q8, Qwen\nQwq-32b, and GPT-3.5 Turbo). Supplementary Figure S2 describes the distribution of missing con\u0000dence\nscores, with 30 models having missing con\u0000dence scores. Supplementary Figure S8 illustrates a strati\u0000ed\nanalysis of response accuracy by con\u0000dence score missingness for models with missing scores for more\nthan one-third of the questions.\nStatistical Analysis\nWe evaluated each model’s performance from two perspectives: discrimination, the ability to distinguish\nbetween correct and incorrect responses, and calibration, the alignment between predicted con\u0000dence\nand actual accuracy.\nDiscrimination was quanti\u0000ed using AUROC. Speci\u0000cally, we designated each response as 1 (positive) if it\nwas labeled “correct” and 0 (negative) otherwise. The con\u0000dence scores of the model ranged from 0 to 10\nand served as the continuous predictor variable. We employed the roc_auc_score function from\nsklearn.metrics to calculate the AUROC. In practical terms, AUROC measures how well con\u0000dence scores\ncan separate correct from incorrect answers, with 0.5 indicating random performance and 1.0 indicating\nperfect discrimination. Conceptually, this involves varying the decision threshold over all possible\ncon\u0000dence values, thereby classifying the responses as positive or negative at each threshold.\nCalibration was evaluated using calibration plots, Brier score, and ECE. Calibration plots were generated\nby normalizing predicted con\u0000dence scores to a 0–1 scale, binning them into 0.1 intervals, and plotting\nthe mean predicted con\u0000dence against the observed accuracy in each bin. Bins containing fewer than\nthree predictions were excluded to ensure the reliability of the results. Bootstrap resampling (n = 1,000\niterations per bin) was used to derive 95% con\u0000dence intervals for each calibration point.\nPage 8/18\nThe combination of these metrics provided comprehensive assessment of model uncertainty estimation.\nThe ECE complements the Brier score by directly quantifying the aggregate discrepancy between\npredicted probabilities and observed outcomes across bins, whereas the Brier score measures the mean\nsquared error between predictions and true labels. As a result, the Brier score re\u0000ects both calibration\n(how closely predicted probabilities match observed frequencies) and re\u0000nement (the sharpness of\npredictions), whereas ECE focuses more directly on calibration quality. Calculating both metrics provides\na more comprehensive evaluation of model performance, capturing not only how well models are\ncalibrated, but also the overall predictive accuracy of their probability estimates.\nOur development and analysis were performed using Python 3.10. LLM answers were generated and\nextracted using the Openai Python library, Ollama application (v0.4), LM studio, and Langchain (v0.2 and\nv0.3). Statistical analyses were conducted using SciPy (v1.13.1) and Scikit-learn (v1.5.1), with data\nmanipulation and visualization implemented through Pandas (v2.2.2), Matplotlib (v3.9.2), and Seaborn\n(v0.13.2). Additional methodological details and code are available in our repository (see Code\navailability).\nDeclarations\nCon\u0000ict of Interest Declaration: NN: none; SAASN: none; TS: none; MK: none; PL: none; ZA: none; GN: is a\nfounder of Renalytix, Pensieve, and Verici and provides consultancy services to AstraZeneca, Reata,\nRenalytix, and Pensieve. He also has equity in Renalytix, Pensieve, and Verici.; AS: is on the advisory board\nand has equity in Virgo Surgical Video Solutions;\nEthical considerations\nThis study did not require ethical approval, as it did not involve human subjects or human data. We\nensured data protection by con\u0000rming that the utilized LLM services did not retain or use our queries for\nmodel training purposes.\nData availability\nThe data supporting this study's \u0000ndings were obtained from the American College of Gastroenterology\n(ACG) under license agreement. While these data are not publicly available owing to licensing restrictions,\nthey may be obtained from the authors with the ACG’s permission upon reasonable request. ACG self-\nassessment questions and answers are accessible to members throughhttps://education.gi.org/.\nThe datasets of models con\u0000dence score and correctness which is used in current study are available in\nthe narimannr2x/con\u0000dence_scoring repository, https://github.com/narimannr2x/con\u0000dence_scoring.\nCode availability\nThe underlying code for this study is available at https://github.com/narimannr2x/con\u0000dence_scoring.\nAcknowledgements\nPage 9/18\nThis study was supported by the American Gastroenterological Association AGA-Amgen Fellowship-to-\nFaculty Transition Award (AGA2023-32-06) for AS. The funding source had no role in the study design,\ndata collection, analysis, interpretation, or manuscript preparation.\nWe thank the American College of Gastroenterology for providing their question bank, the Hugging Face\nteam for their accessible AI infrastructure, and the theBloke account on Hugging Face for providing\nquantized versions of open-source LLMs. ChatGPT was used to assist with English language editing\nduring manuscript preparation. The authors reviewed and edited all AI-assisted content and maintained\nfull responsibility for the manuscript's content.\nAuthor contributions\nNN: Conceptualization, Methodology, Formal Analysis, Investigation, Data Curation, Writing Original Draft,\nProgramming, Data Curation; SAASN: Methodology, Investigation, Validation, Review & Editing, Project\nAdministration; AS: Methodology, Investigation, Supervision, Validation; TS: Investigation, Validation; GN:\nSupervision; ZA:  Investigation; PL: Investigation; MK:Investigation. \nCompeting Interests\nThe authors declare no competing \u0000nancial interests or personal relationships that could have in\u0000uenced\nthe work reported in this study. NN: none; SAASN: none; TS: none; MK: none; PL: none; ZA: none; GN: is a\nfounder of Renalytix, Pensieve, and Verici and provides consultancy services to AstraZeneca, Reata,\nRenalytix, and Pensieve. He also has equity in Renalytix, Pensieve, and Verici.; AS: is on the advisory board\nand has equity in Virgo Surgical Solutions;\nReferences\n1. Liévin, V., Hother, C. E., Motzfeldt, A. G. & Winther, O. Can large language models reason about\nmedical questions? Patterns 5, (2024).\n2. Singhal, K. et al. Toward expert-level medical question answering with large language models. Nat.\nMed. 1–8 (2025) doi:10.1038/s41591-024-03423-7.\n3. Haltaufderheide, J. & Ranisch, R. The ethics of ChatGPT in medicine and healthcare: a systematic\nreview on Large Language Models (LLMs). Npj Digit. Med. 7, 1–11 (2024).\n4. McKenna, N. et al. Sources of Hallucination by Large Language Models on Inference Tasks. Preprint\nat https://doi.org/10.48550/arXiv.2305.14552 (2023).\n5. Xiong, M. et al. Can LLMs Express Their Uncertainty? An Empirical Evaluation of Con\u0000dence\nElicitation in LLMs. Preprint at https://doi.org/10.48550/arXiv.2306.13063 (2024).\n\u0000. Duan, J. et al. Shifting Attention to Relevance: Towards the Predictive Uncertainty Quanti\u0000cation of\nFree-Form Large Language Models. Preprint at https://doi.org/10.48550/arXiv.2307.01379 (2024).\n7. Wu, J., Yu, Y. & Zhou, H.-Y. Uncertainty Estimation of Large Language Models in Medical Question\nAnswering. Preprint at https://doi.org/10.48550/arXiv.2407.08662 (2024).\nPage 10/18\n\u0000. Tian, K. et al. Just Ask for Calibration: Strategies for Eliciting Calibrated Con\u0000dence Scores from\nLanguage Models Fine-Tuned with Human Feedback. in Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing (eds. Bouamor, H., Pino, J. & Bali, K.) 5433–5442\n(Association for Computational Linguistics, Singapore, 2023). doi:10.18653/v1/2023.emnlp-\nmain.330.\n9. Yang, D., Tsai, Y.-H. H. & Yamada, M. On Verbalized Con\u0000dence Scores for LLMs. Preprint at\nhttps://doi.org/10.48550/arXiv.2412.14737 (2024).\n10. Ni, S., Bi, K., Yu, L. & Guo, J. Are Large Language Models More Honest in Their Probabilistic or\nVerbalized Con\u0000dence? Preprint at https://doi.org/10.48550/arXiv.2408.09773 (2024).\n11. Omar, M., Agbareia, R., Glicksberg, B. S., Nadkarni, G. N. & Klang, E. Benchmarking the Con\u0000dence of\nLarge Language Models in Clinical Questions. 2024.08.11.24311810 Preprint at\nhttps://doi.org/10.1101/2024.08.11.24311810 (2024).\n12. Savage, T. et al. Large language model uncertainty proxies: discrimination and calibration for medical\ndiagnosis and treatment. J. Am. Med. Inform. Assoc. JAMIA 32, 139–149 (2025).\n13. Griot, M., Hemptinne, C., Vanderdonckt, J. & Yuksel, D. Large Language Models lack essential\nmetacognition for reliable medical reasoning. Nat. Commun. 16, 642 (2025).\n14. Safavi-Naini, S. A. A. et al. Vision-Language and Large Language Model Performance in\nGastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized Models. Preprint at\nhttps://doi.org/10.48550/arXiv.2409.00084 (2024).\n15. Omar, M., Agbareia, R., Glicksberg, B. S., Nadkarni, G. N. & Klang, E. Benchmarking the Con\u0000dence of\nLarge Language Models in Clinical Questions. 2024.08.11.24311810 Preprint at\nhttps://doi.org/10.1101/2024.08.11.24311810 (2024).\n1\u0000. Vashurin, R. et al. Benchmarking Uncertainty Quanti\u0000cation Methods for Large Language Models\nwith LM-Polygraph. Preprint at https://doi.org/10.48550/arXiv.2406.15627 (2024).\n17. Yang, D., Tsai, Y.-H. H. & Yamada, M. On Verbalized Con\u0000dence Scores for LLMs. Preprint at\nhttps://doi.org/10.48550/arXiv.2412.14737 (2024).\nTable\nTable 1. LLM accuracy, discrimination, calibration, and con\u0000dence scores were sorted from best\ncalibration (lowest Brier score) to worst for each model family.\nPage 11/18\nModelfamily Model nameandparameter(quantization)\nDateaccessed Calibration DiscriminationAccuracy Self-reportedcon\u0000dencescore\n      Brierscore ECE AUROC Percent Mean(95CI)\nLlama              \n  Llama-3.3-70b December2024 0.260 0.199 0.563 65.66 8.46 (8.36-8.56)\n  Llama 3.1405B August2024 0.273 0.211 0.592 64 8.47 (8.38-8.57)\n  Llama3.2-90BDecember2024 0.302 0.269 0.600 60.00 8.49 (8.34-8.62)\n  Llama 3.170B August2024 0.313 0.283 0.538 58.19 8.51 (8.39-8.62)\n  Llama 3 70B May 2024 0.334 0.301 0.572 54.66 8.38 (8.28-8.48)\n  Llama 3 8B May 2024 0.422 0.450 0.478 43.33 8.54 (8.41-8.68)\n  Llama-3.2-11b December2024 0.400 0.390 0.519 48.65 8.59 (8.46-8.69)\n  Llama 3.1 8BAugust2024 0.433 0.441 0.512 43.14 8.67 (8.54-8.80)\n  Llama-3.2-3bDecember2024 0.465 0.487 0.534 35.66 8.32 (8.18-8.45)\n  Llama 2 70B April 2024 0.481 0.493 0.529 37.71 8.70 (8.58-8.81)\n  Llama-3.2-1bDecember2024 0.500 0.511 0.455 30.61 8.13 (7.96-8.31)\n  Llama 2 13B(Q5) April 2024 0.525 0.546 0.5 35.16 8.98 (8.92-9.04)\n  Llama 3 8B(Q8) April 2024 0.527 0.613 0.472 30.35 8.65 (8.28-9.02)\n  Llama 2 7B April 2024 0.528 0.587 0.47 30.87 8.66 (8.47-8.84)\n  Llama 2 13B April 2024 0.531 0.558 0.52 33.11 8.89 (8.82-8.95)\n  Llama 2 7B(Q8) April 2024 0.559 0.582 0.458 32.45 9.07 (8.98-9.15)\nPage 12/18\nQwen              \n  Qwen-2.5-72bSeptember2024 0.326 0.304 0.549 61.48 8.39(8.15-8.63)\n  Qwen-2-72B September2024 0.364 0.360 0.583 57.00 9.10(8.98-9.20)\nPhi              \n  Phi-3 Medium14B (Q6) April 2024 0.389 0.377 0.588 48.66 8.57 (8.48-8.67)\n  Phi-3 3BFP16 April 2024 0.458 0.464 0.486 43.79 8.96 (8.84-9.07)\n  Phi-3.5-4b December2024 0.558 0.578 0.465 31.86 8.96 (8.90-9.02)\nGoogle              \n  GeminiAdvancedWeb\nMarch-April 2024 0.297 0.247 0.561 58.49 8.20 (8.07-8.33)\n  Gemma 227B July 2024 0.374 0.352 0.557 50 8.52 (8.41-8.63)\n  Gemma 2 9B(Q8) July 2024 0.397 0.392 0.543 45.33 8.40 (8.30-8.50)\n  Gemma 2 9BJuly 2024 0.398 0.390 0.592 44.59 8.33 (8.20-8.45)\n  Gemini Web March2024 0.421 0.420 0.563 44.44 8.61 (8.53-8.70)\nMistral              \n  Mistral Large April 2024 0.282 0.224 0.602 60.53 8.13 (7.98-8.28)\n  Mixtral 8x7B April 2024 0.359 0.336 0.547 54.33 8.79 (8.72-8.87)\n  Mistral v2 Q8 April 2024 0.506 0.527 0.554 39.06 9.11 (8.90-9.32)\n  Mistral 7B April 2024 0.547 0.551 0.519 40.66  \nClaude              \n  Claude 3.5Sonnet July 2024 0.207 0.122 0.6 74 8.60 (8.54-8.67)\n  Claude 3Opus March-April 2024 0.229 0.150 0.575 70.35 8.54 (8.44-8.63)\nPage 13/18\n  Claude 3Opus Web March-April 2024 0.246 0.154 0.578 65.66 7.99 (7.89-8.09)\n  Claude 3Sonnet Web March-April 2024 0.326 0.284 0.551 55.33 8.37 (8.29-8.45)\n  Claude 3Sonnet March-April 2024 0.361 0.336 0.559 51.17 8.48 (8.39-8.58)\n  Claude 3Haiku March-April 2024 0.373 0.357 0.522 53.76 8.88 (8.80-8.96)\n  Claude 3Haiku Web March-April 2024 0.398 0.385 0.523 50 8.85 (8.80-8.90)\nGPT              \n  o1 preview September2024 0.157 0.100 0.576 81.57 9.15 (9.10-9.20)\n  GPT-4o May 2024 0.208 0.148 0.604 74 8.86 (8.80-8.92)\n  GPT-4 Web March2024 0.267 0.221 0.588 66.22 8.79 (8.70-8.87)\n  GPT-4 March2024 0.278 0.237 0.605 66.53 9.02 (8.92-9.13)\n  o1 Mini September2024 0.278 0.257 0.626 66.33 9.20 (9.12-9.27)\n  GPT-4o Mini July 2024 0.342 0.309 0.572 56.61 8.75 (8.67-8.83)\n  GPT-3.5 Web March2024 0.394 0.375 0.546 47.66 8.56 (8.48-8.63)\nFigures\nPage 14/18\nFigure 1\nSummary illustration of pipeline for con\u0000dence score extraction from raw textual responses.\nPage 15/18\nFigure 2\nAverage accuracy versus average con\u0000dence scores for LLMs with more than 150 valid samples. The\ndashed red line indicates perfect calibration, that is, the alignment of the average accuracy and average\ncon\u0000dence score. Models above this line are overcon\u0000dent, whereas those below are under-con\u0000dent. A\nsubset of the data was magni\u0000ed for clarity purposes.\nPage 16/18\nFigure 3\nLeft panel: Overall distribution of self-reported con\u0000dence scores and mean response accuracy (stars) for\neach model. Right panel: Distribution of self-reported con\u0000dence scores for each model strati\u0000ed by\nresponse accuracy.\nPage 17/18\nFigure 4\nCalibration curves for the top six (a-f) and bottom three (g-i) models. Con\u0000dence scores were binned into\nintervals of 0.1 across the range of 0 to 1, with the mean normalized con\u0000dence score for each bin\nplotted against the corresponding observed accuracy. The dashed line represents perfect calibration.\nPage 18/18\nFigure 5\nSub\u0000gures (a) to (f) illustrate the smoothed trends in accuracy and con\u0000dence scores of LLMs as a\nfunction of question di\u0000culty, de\u0000ned as the percentage of test-takers answering correctly. The questions\nwere grouped into bins at 5% intervals to facilitate visualization. Across all models, con\u0000dence scores\nremained relatively stable despite increasing question di\u0000culty and lower model response accuracy.\nFigures (a)–(c) highlight the three models with the lowest Brier scores (highest calibration), whereas\nFigures (d)–(f) display the three models with the highest Brier scores (lowest calibration).\nSupplementary Files\nThis is a list of supplementary \u0000les associated with this preprint. Click to download.\nSupplementary.pdf",
  "topic": "Confidence interval",
  "concepts": [
    {
      "name": "Confidence interval",
      "score": 0.4770083427429199
    },
    {
      "name": "Self-confidence",
      "score": 0.42397987842559814
    },
    {
      "name": "Psychology",
      "score": 0.4160383641719818
    },
    {
      "name": "Internal medicine",
      "score": 0.3378172516822815
    },
    {
      "name": "Computer science",
      "score": 0.33758729696273804
    },
    {
      "name": "Medicine",
      "score": 0.32060354948043823
    },
    {
      "name": "Social psychology",
      "score": 0.16153272986412048
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98704320",
      "name": "Icahn School of Medicine at Mount Sinai",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I36788626",
      "name": "California University of Pennsylvania",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I58048189",
      "name": "Shahid Beheshti University of Medical Sciences",
      "country": "IR"
    },
    {
      "id": "https://openalex.org/I39470171",
      "name": "Ontario Tech University",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210161785",
      "name": "First Technical University",
      "country": "NG"
    }
  ],
  "cited_by": 1
}