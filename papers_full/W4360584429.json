{
  "title": "METER: A Mobile Vision Transformer Architecture for Monocular Depth Estimation",
  "url": "https://openalex.org/W4360584429",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2114535847",
      "name": "Lorenzo Papa",
      "affiliations": [
        "Sapienza University of Rome"
      ]
    },
    {
      "id": "https://openalex.org/A2110921893",
      "name": "Paolo Russo",
      "affiliations": [
        "Sapienza University of Rome"
      ]
    },
    {
      "id": "https://openalex.org/A39074145",
      "name": "Irene Amerini",
      "affiliations": [
        "Sapienza University of Rome"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3118635606",
    "https://openalex.org/W3128121047",
    "https://openalex.org/W3212645988",
    "https://openalex.org/W4214520160",
    "https://openalex.org/W3173727695",
    "https://openalex.org/W6810661541",
    "https://openalex.org/W4226254592",
    "https://openalex.org/W2967115342",
    "https://openalex.org/W6802648153",
    "https://openalex.org/W3190492058",
    "https://openalex.org/W125693051",
    "https://openalex.org/W2115579991",
    "https://openalex.org/W6685261749",
    "https://openalex.org/W2962741876",
    "https://openalex.org/W2962928205",
    "https://openalex.org/W6757246177",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2961343177",
    "https://openalex.org/W2787091153",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3118453581",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3200000156",
    "https://openalex.org/W4295934562",
    "https://openalex.org/W6811234694",
    "https://openalex.org/W6798891274",
    "https://openalex.org/W6810242703",
    "https://openalex.org/W3025045095",
    "https://openalex.org/W3140854437",
    "https://openalex.org/W2964052474",
    "https://openalex.org/W2963267406",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W3175059103",
    "https://openalex.org/W2967733054",
    "https://openalex.org/W6737664043",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W2963911235",
    "https://openalex.org/W2520707372",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W4225949693",
    "https://openalex.org/W4297775537",
    "https://openalex.org/W2081900960",
    "https://openalex.org/W3186318242",
    "https://openalex.org/W4386723894",
    "https://openalex.org/W4226191818",
    "https://openalex.org/W4289082871",
    "https://openalex.org/W4286910290",
    "https://openalex.org/W1522301498"
  ],
  "abstract": "Depth estimation is a fundamental knowledge for autonomous systems that need\\nto assess their own state and perceive the surrounding environment. Deep\\nlearning algorithms for depth estimation have gained significant interest in\\nrecent years, owing to the potential benefits of this methodology in overcoming\\nthe limitations of active depth sensing systems. Moreover, due to the low cost\\nand size of monocular cameras, researchers have focused their attention on\\nmonocular depth estimation (MDE), which consists in estimating a dense depth\\nmap from a single RGB video frame. State of the art MDE models typically rely\\non vision transformers (ViT) architectures that are highly deep and complex,\\nmaking them unsuitable for fast inference on devices with hardware constraints.\\nPurposely, in this paper, we address the problem of exploiting ViT in MDE on\\nembedded devices. Those systems are usually characterized by limited memory\\ncapabilities and low-power CPU/GPU. We propose METER, a novel lightweight\\nvision transformer architecture capable of achieving state of the art\\nestimations and low latency inference performances on the considered embedded\\nhardwares: NVIDIA Jetson TX1 and NVIDIA Jetson Nano. We provide a solution\\nconsisting of three alternative configurations of METER, a novel loss function\\nto balance pixel estimation and reconstruction of image details, and a new data\\naugmentation strategy to improve the overall final predictions. The proposed\\nmethod outperforms previous lightweight works over the two benchmark datasets:\\nthe indoor NYU Depth v2 and the outdoor KITTI.\\n",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1\nMETER: a mobile vision transformer\narchitecture for monocular depth estimation\nLorenzo Papa Student Member IEEE , Paolo Russo and Irene Amerini Member IEEE\nAbstract—Depth estimation is a fundamental knowledge for\nautonomous systems that need to assess their own state and\nperceive the surrounding environment. Deep learning algorithms\nfor depth estimation have gained significant interest in recent\nyears, owing to the potential benefits of this methodology in\novercoming the limitations of active depth sensing systems.\nMoreover, due to the low cost and size of monocular cameras,\nresearchers have focused their attention on monocular depth\nestimation (MDE), which consists in estimating a dense depth\nmap from a single RGB video frame. State of the art MDE\nmodels typically rely on vision transformers (ViT) architectures\nthat are highly deep and complex, making them unsuitable for\nfast inference on devices with hardware constraints.\nPurposely, in this paper, we address the problem of exploiting\nViT in MDE on embedded devices. Those systems are usually\ncharacterized by limited memory capabilities and low-power\nCPU/GPU. We propose METER, a novel lightweight vision\ntransformer architecture capable of achieving state of the art\nestimations and low latency inference performances on the con-\nsidered embedded hardwares: NVIDIA Jetson TX1 and NVIDIA\nJetson Nano. We provide a solution consisting of three alternative\nconfigurations of METER, a novel loss function to balance pixel\nestimation and reconstruction of image details, and a new data\naugmentation strategy to improve the overall final predictions.\nThe proposed method outperforms previous lightweight works\nover the two benchmark datasets: the indoor NYU Depth v2 and\nthe outdoor KITTI.\nIndex Terms—Deep learning, embedded device, monocular\ndepth estimation, vision transformer\nI. I NTRODUCTION\nAcquiring accurate depth information from a scene is a\nfundamental and important challenge in computer vision, as\nit provides essential knowledge in a variety of vision applica-\ntions, such as augmented reality, salient object detection, visual\nSLAM, video understanding, and robotics [1]–[3]. Depth data\nis usually captured with active depth sensors as LiDARs, depth\ncameras, and other specialised sensors capable of perceiving\nsuch information by perturbing the surrounding environment,\ne.g. through time-of-flight or structured light technologies.\nThese sensors have several disadvantages, including unfilled\ndepth maps and restricted depth ranges, as well as being\ndifficult to integrate into low-power embedded devices. In\naddition, we also need to consider the power consumption in\nthe case of hardwares with low-resource constraints.\nOn the contrary, passive depth sensing systems based on\ndeep learning (DL) could potentially overcome all the active\nThe authors are with the Department of Computer, Control and Man-\nagement Engineering, Sapienza University of Rome, Italy (e-mail: [papa,\npaolo.russo, amerini]@diag.uniroma1.it).\nManuscript received Month Day, 2022; revised Month Day, 2022.\ndepth sensor limitations. Moreover, in some settings such as\nindoor or hostile environments, where the use of small robots\nand drones could introduce additional constraints, the presence\nof a single RGB camera offers an effective and low-cost\nalternative to such traditional setups. The monocular depth\nestimation (MDE) task consists in the prediction of a dense\ndepth map from a video frame with the use of DL algorithms,\nwhere the estimation is computed for each pixel.\nRecent MDE models aim at enabling depth perception using\nsingle RGB images on deep vision transformer (ViT) architec-\ntures [4]–[6], which are generally unsuitable for fast inference\non low-power hardwares. Instead, well-established convolu-\ntional neural networks (CNN) architectures [7], [8] have been\nsuccessfully exploited on embedded devices with the goal of\nachieving accurate and low latency inferences. However, ViT\narchitectures demonstrate the advantage of a global processing\nby obtaining significant performance improvements over fully-\nCNNs. In order to balance computational complexity and hard-\nware constraints, we propose to integrate the two architectures\nby fusing transformers blocks and convolutional operations, as\nsuccessfully exploited in classification and object detection [9],\n[10] tasks.\nThis paper presents METER, a MobilE vision TransformER\narchitecture for MDE that achieves state of the art results with\nrespect to previous lightweight models over two benchmark\ndatasets, i.e. NYU Depth v2 [11] and KITTI [12]. METER\ninference speed will be evaluated on two embedded hard-\nwares, the 4GB NVIDIA Jetson TX1 and the 4GB NVIDIA\nJetson Nano. To improve the overall estimation performances,\nwe focus on three fundamental components: a specific loss\nfunction, a novel data augmentation policy and a custom\ntransformer architecture. The loss function is composed of four\nindependent terms (quantitative and similarity measurements)\nto balance the architecture reconstruction capabilities while\nhighlighting the image high-frequency details. Moreover, the\ndata augmentation strategy employs a simultaneous random\nshift over both the input image and the dense ground truth\ndepth map to increase model resilience to tiny changes of\nillumination and depth values.\nThe proposed network exploits a hybrid encoder-decoder\nstructure characterized by a ViT encoder, which was inspired\nby [9] due to its fast inference performances. We focus on the\ntransformer structure in order to identify and to improve the\nblocks with the highest computational cost while optimizing\nthe model to extract robust features. In addition, we designed\na novel lightweight CNN decoder to limit the amount of\nCopyright © 20xx IEEE. Personal use of this material is permitted. However, permission to use this material for any other purposes must be obtained from\nthe IEEE by sending an email to pubs-permissions@ieee.org.\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3260310\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2\nRGB\nInput\nGT\nDepth\nPred\nDepth\nKITTI NYU Depth v2\nFig. 1: METER depth map predictions (third-row) over the KITTI and NYU Depth v2 datasets. GT depth maps are resized\nto match METER output resolution. The depth maps are converted in RGB format with a perceptually uniform colormap\n(Plasma-reversed) extracted from the ground truth (second-row), for a better view.\noperations while improving the reconstruction process. Fur-\nthermore, we propose three different METER configurations;\nfor each variant, we reduce the number of trainable parameters\nat the expense of a slight increase of the final estimation error.\nFigure 1 shows several METER depth estimations for both\nindoor and outdoor environments.\nMoreover, to the best of our knowledge, METER is the\nfirst model for the MDE task that integrates the advantage\nof ViT architectures in such lightweight DL structures under\nlow-resource hardware constraints. The main contributions of\nthe paper are summarized as follows:\n• We propose a novel lightweight ViT architecture for\nmonocular depth estimation able to infer at high fre-\nquency on low-resource (4GB) embedded devices.\n• We introduce a novel data augmentation method and loss\nfunction to boost the model estimation performances.\n• We show the effectiveness and robustness of METER\nwith respect to related state of the art MDE methods\nover two benchmark datasets, i.e. NYU Depth v2 [11]\nand KITTI [12].\n• We validate the models through quantitative and qualita-\ntive experiments, data augmentation strategies and a loss\nfunction components, highlighting their effectiveness.\nThis paper is organized as follows: Section II reviews some\nprevious works related to the topics of interest. Section III\ndescribes the proposed method and the overall architecture\nin detail. Experiments and hyper-parameters are discussed in\nSection IV, while Section V reports the results and a quan-\ntitative analysis of METER with respect to other significant\nworks. Some final considerations and future applications are\nprovided in Section VI.\nII. R ELATED WORKS\nIn this section, we report state of the art related works\non monocular depth estimation, grouped as follows: fully\nCNN-based methods are covered in Section II-A, ViT-based\napproaches in Section II-B and lightweight (CNN) MDE\nmethods in Section II-C.\nA. CNN-based MDE methods\nFully convolutional neural networks based on encoder-\ndecoder structures are commonly used for dense prediction\ntasks such as depth estimation and semantic segmentation. In\nthe seminal work of Eigen et al. [13] it is presented a CNN\nmodel to handle the MDE task by employing two stacked deep\nnetworks to extract both global and local informations. Cao et\nal. present [14] and [15] two works based on deep residual\nnetworks to solve the MDE defined as a classification task,\nrespectively, over absolute and relative depth maps. Alhashim\net al. [16] propose DenseDepth, a network which exploits\ntransfer learning to produce high-resolution depth maps. The\narchitecture is composed of a standard encoder-decoder with a\npre-trained DenseNet-169 [17] as backbone and a specifically\ndesigned decoder. Gur et al. [18] present a variant of the\nDeepLabV3+ [19] model where the encoder is composed of a\nResNet [20] and of an atrous spatial pyramidal pooling while\nintroducing a Point Spread Function convolutional layer to\nlearn depth informations from defocus cues. Recently, Song\net al. [21] propose LapDepth, a Laplacian pyramid-based\narchitecture composed of a pretrained ResNet-101 encoder and\na Laplacian pyramid decoder that combined the reconstructed\ncoarse and fine scales to predict the final depth map.\nHowever, those methods, which often rely on deep pre-\ntrained encoders and high-resolution images as input, are\nunsuitable for inferring on low-resource hardwares. In contrast,\nwe propose a lightweight architecture that takes advantage\nof transformers blocks to balance global feature extraction\ncapabilities and the overall computational complexity of con-\nvolutional operations.\nB. ViT-based MDE methods\nVision Transformers [22] gain popularity for their accuracy\ncapabilities thanks to the attention mechanism [23] that simul-\ntaneously extract information from the input pixels and their\ninter-relation, outperforming the translation-invariant property\nof convolution. In dense prediction tasks, ViT architectures\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3260310\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3\nshare the same encoder-decoder structure that has significantly\ncontributed to face many CNN vision-related problems. Bhat\net al. [5] have been the first to handle the MDE task with\nViT architectures by proposing Adabins: it uses a minimized\nversion of a vision transformer structure to adaptively calculate\nbins width. Ranftl et al. [4] investigate the application of\nViT proposing DPT, a model composed of a transformer-\nCNN encoder and a fully-convolutional decoder. The authors\nshow that ViT encoders provide finer-grade predictions with\nrespect to standard CNNs, especially when instantiated with a\nlarge amount of training data. Yun et al. [24] improves 360◦\nmonocular depth estimation methods with a joint supervised\nand self-supervised learning strategies taking advantage of\nnon-local DPT. Recently, Li et al. [25] design MonoIndoor++,\na framework that takes in account the main challenges of\nindoor scenarios. Kim et al. [26] propose GLPDepth, a global-\nlocal transformer network to extract meaningful features at\ndifferent scales and a Selective Feature Fusion CNN block\nfor the decoder. The authors also integrate a revisited ver-\nsion of CutDepth data augmentation method [27] which is\nable to improve the training process on the NYU Depth v2\ndataset without needing additional data. Li et al. propose\nDepthFormer [6] and BinsFormer [28], where the first one is\ncomposed of a fully-transformer encoder and a convolutional\ndecoder interleaved by an interaction module to enhance\ntransformer encoded and CNN decoded features. Differently,\nin BinsFormer the idea of the authors is to use a multi-scale\ntransformer decoder to generate adaptive bins and to recover\nspatial geometry information from the encoded features.\nInstead of following the recent trend of high-capacity mod-\nels, we propose a novel lightweight ViT architecture that is\nable to achieve accurate, low latency depth estimations on\nembedded devices.\nC. Lightweight MDE methods\nThe models reported so far are not suitable for embedded\ndevices due to their size and complexity. For this reason, devel-\noping lightweight architectures could be a solution to perform\ninference on constrained hardwares as shown in [29], [30].\nTo provide a clearer overview of those approaches we also\nprovide the frames per second (fps) published in the original\npapers that focus on inference frequency, remarking that they\nare not comparable due to the different tested hardwares. Poggi\net al. [31] propose PyD-Net, a pyramidal network to infer\non CPU devices. The authors use the pyramidal structure to\nextract features from the input image at different levels, which\nare afterwards upsampled and merged to refine the output\nestimation. Such model achieves less than 1 fps on an ARM\nCPU and almost 8 fps on an Intel i7 CPU. Spek et al. [32]\npresent CReaM, a fully convolutional architecture obtained\nthrough a knowledge-transfer learning procedure. The model\nis able to achieve real-time frequency performances (30 fps) on\nthe 8GB NVIDIA Jetson TX2 device. Wofk et al. [8] develop\nFastDepth, an encoder-decoder architecture characterized by\na MobileNet [33] pre-trained network as backbone, and a\ncustom decoder. Furthermore, the authors show that pruning\nthe trained model guarantees a boost of inference frequency\nat the expense of a small increment of the final estimation\nerror. FastDepth achieves 178 fps on the 8GB NVIDIA Jetson\nTX2 device. Recently, Yucel et al. [34] propose a small\nnetwork composed by the MobileNet v2 [33] as encoder and\nFBNet x112 [35] as decoder, trained on an altered knowledge\ndistillation process; the model achieves 37 fps on smartphone\nGPU. Papa et al. [7] design SPEED, a separable pyramidal\npooling architecture characterized by an improved version of\nthe MobileNet v1 [36] as an encoder and a dedicated decoder.\nThis architecture exploits the use of depthwise separable\nconvolutions, achieving real-time frequency performances on\nthe embedded 4GB NVIDIA Jetson TX1 and 6 fps on the\nGoogle Dev Board Edge TPU.\nAs previously mentioned, all those lightweight MDE works\nare designed over fully-convolutional architectures. In contrast\nto previous methodologies, METER exploits a lightweight\ntransformer module in three different configurations, achieving\nstate of the art results over the standard evaluation metrics.\nIII. P ROPOSED METHOD\nThis section outlines the design of METER, the proposed\nlightweight monocular depth estimator. In particular, in Sec-\ntion III-A, we provide a detailed architecture analysis for both\nencoder and decoder modules, in Section III-B we describe\nthe proposed loss function and in Section III-C the employed\naugmentation policy.\nA. METER architecture\nThe vision transformer architecture has demonstrated out-\nstanding performances in a variety of computer vision tasks,\nusually relying on deep and heavy structures. On the other\nhand, to reduce the computational cost of such models,\nlightweight CNN usually relies on convolutional operations\nwith small kernels (i.e. 3x3, 1x1) or on particular techniques\nsuch as depthwise separable convolution [37]. Based on those\nstatements, we design an hybrid lightweight ViT characterized\nby convolutions with small kernels and as few transformers\nblocks as possible reducing the computational impact in the\noverall structure. Motivated by this, in the following, we\npresent METER: a MobilE vision TrasformER architecture\ncharacterized by a lightweight encoder-decoder model de-\nsigned to infer on embedded devices. METER encoder re-\ndesign computational demanding operations of [9] to improve\nthe inference performances while maintaining the feature\nextraction capabilities. The high-level features extracted from\nthe encoder are then fed into the decoder through the skip-\nconnections to recover the image details. The proposed fully\nconvolutional decoder has been structured to upsample the\ncompact set of encoder high-level features while enhancing\nthe reconstruction of the image details to obtain the desired\noutput depth map (i.e. a per-pixel distance map). A graphical\noverview of the architecture is reported in Figure 2 while\nthe number of channels employed in the different METER\nconfigurations, METER S, METER XS, and METER XXS\nare reported in Table I. The number of trainable parameters of\nthe three proposed networks consist of 3.29M, 1.45M, and\n0.71M, respectively.\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3260310\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4\nH\nC\nW\nH\nC\nW\nSkip connection\nInput/Output featuresConvolution MobileNetV2 blockMETER blockConvolutional block Transformer blockTrasposed Convolution Upsampling block\n(H↓, W↓, C6) \n(H↑, W↑, C9) (H↑, W↑, C8) (H↑, W↑, C7) (H↓, W↓, C5) (H↓, W↓, C4) (H↓, W↓, C3) (H↓, W↓, C2) \nW↑\nH↑\nC↓\n(H, W, C10) (H, W, C1) \nFig. 2: Overview of METER encoder-decoder network structure. The processing flow, i.e. the sequence of operations and the\nskip-connection, is represented with a blue dashed arrow. The (H, W, C) format refers to the input-output spatial dimensions,\nwhile the ↑ and ↓ refers to the feature resolution upsampling and downsampling.\nTABLE I: Number of channels ( Ci) used in METER config-\nurations.\nChannels METER S METER XS METER XXS\nC1 16 16 16\nC2 32 32 16\nC3 64 48 24\nC4 128 80 64\nC5 160 96 80\nC6 320 192 160\nC7 128 128 64\nC8 64 64 32\nC9 32 32 16\nC10 16 16 8\nMETER encoder exploits a modified version of MobileViT\nnetwork due to its light structure demonstrated in [9]. As can\nbe noticed in Figure 2, METER presents a hybrid network\ncomposed of convolutional MobileNetV2 blocks (red) and\ntransformers blocks (green). The MobileViT blocks with the\nhighest computational cost, i.e. the ones composed of cascaded\ntransformers and convolution operations, have been identi-\nfied and replaced with new modules (METER blocks). Such\nmodules are able to guarantee low latency inference while\ntuning the entire structure to minimize the final estimation\nerror. Along the lines of [9], we propose three variants of\nthe same encoder architecture with decreasing complexity and\ncomputational cost namely S, XS, and XXS .\nThe proposed METER block (green in Figure 2) is com-\nposed by three feature extraction operations, two Convolu-\ntional blocks composed by a 3 × 3 convolution and a point-\nwise one (purple) and a second 1 × 1 convolution (yellow)\ninterleaved by a single transformer block (blue). Such module\ncomputes an unfold operation to apply the transformer atten-\ntion on the flattened input patches while reconstructing output\nfeature map with an opposite folding operation, as described\nin [9]. Moreover, in order to apply an attention mechanism to\nthe encoded features, the input of METER block (gray) has\nbeen concatenated with the output of the transformer and fed\nto the previous 1 × 1 convolution layer. When compared with\nMobileViT architecture, characterized by four convolutions\noperations and a number of cascaded transformers blocks, the\nproposed design allows to reduce the computational cost of\nthe overall model while producing an accurate estimation of\nthe depth (as will be shown in Section V-B).\nFinally, we halved the number of output encoder features\n(channel C6) and we replaced the MobileViT SiLU non\nlinearity function with the ReLU. Despite the fact that SiLU\nactivation function is differentiable at every point 1, it does\nnot ensure better performance, likely due to the depth-data\ndistribution.\nMETER decoder is designed with a fully convolutional struc-\nture to enhance the estimation accuracy and the reconstruction\ncapabilities while keeping a limited number of operations. As\ncan be seen in Figure 2, the decoder consists of a sequence\nof three cascaded upsampling blocks (light blue) and two\nconvolutional layers (yellow) located at the beginning and at\nthe end of the model. Each upsampling block is composed\nby a sequence of upsampling, skip-connection and feature\nextraction operations. The upsampling operation is performed\nby a transposed convolutional layer (orange) which doubles the\nspatial resolution of the input. Then, a Convolutional block\n(purple) is used for feature extraction; the skip-connection\n(dashed blue arrow) linking METER encoder-decoder modules\nallows to recover image details from the encoded feature maps.\nB. The balanced loss function\nThe standard monocular depth estimation formulation con-\nsider as loss function the per-pixel difference between the\nith ground truth pixel yi and the predicted one ˆyi. However,\n1Unlike the SiLU, the ReLU activation function is non-differentiable at\nzero.\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3260310\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5\nas reported in literature [16], [38], [39] several modifications\nhave been proposed to improve the convergence speed and\nthe overall depth estimation performances. In particular, the\naddition of different loss components focuses on refinement\nof fine details in the scenes, like object contours.\nDerived from [38], [39], we propose a balanced loss\nfunction (BLF) to weight the reconstruction loss through the\nLdepth(yi, ˆyi) and LSSIM (yi, ˆyi) components with the high-\nfrequency features taken into account by the Lgrad(yi, ˆyi)\nand the Lnorm(yi, ˆyi) losses. The BLF L(yi, ˆyi) mathematical\nformulation is reported in Equation 1, where λ1, λ2, λ3 are\nused as scaling factors.\nL(yi, ˆyi) =Ldepth + λ1Lgrad + λ2Lnorm + λ3LSSIM (1)\nIn detail, the loss Ldepth(yi, ˆyi) in Equation 2 is the point-\nwise L1 loss computed as the per-pixel absolute difference\nbetween the ground truth yi and the predicted image ˆyi.\nLdepth(yi, ˆyi) = 1\nn\nnX\ni=1\n|yi − ˆyi| (2)\nThe Lgrad(yi, ˆyi) and the Lnorm(yi, ˆyi) losses reported\nrespectively in Equation 3 and Equation 4 are designed to\npenalize the estimation errors around the edges and on small\ndepth details. The Lgrad(yi, ˆyi) loss computes the Sobel\ngradient function to extract the edges and objects boundaries.\nLgrad(yi, ˆyi) = 1\nn\nnX\ni=1\n(∇x(|yi − ˆyi|) +∇y(|yi − ˆyi|)) (3)\nWe report with ∇ the spatial derivative of the absolute\nestimation error with respect to the x and y axes.\nThe Lnorm(yi, ˆyi) loss, reported in Equation 4, calculates\nthe cosine similarity [40] between the ground truth and the\nprediction.\nLnorm(yi, ˆyi) = 1\nn\nnX\ni=1\n \n1 − ⟨nˆyi , nyi ⟩p\n⟨nˆyi , nˆyi ⟩\np\n⟨nyi , nyi ⟩\n!\n(4)\nWe identify with ⟨nyi , nˆyi ⟩ the inner product of the surface\nnormal vectors nyi and nˆyi computed for each depth map i.e.\nnz = [−∇x(z), −∇y(z), 1]T with z = [yi, ˆyi].\nThe last component LSSIM (yi, ˆyi) loss, Equation 5, is\nbased on the mean structural similarity ( SSIM ) [41]. Sim-\nilarly to [16], [39] we add this function to improve the depth\nreconstruction and the overall final estimation.\nLSSIM (yi, ˆyi) = 1− SSIM (yi, ˆyi) (5)\nIn conclusion, the proposed BLF balances the image re-\nconstruction Ldepth, the image similarity LSSIM , the edge\nreconstruction Lgrad and the edge similarity Lnorm losses.\nThe impact of each loss will be quantitatively evaluated in\nSection V-C.\nC. The data augmentation policy\nDeep learning architectures and especially Vision Trans-\nformer need a large amount of input data to avoid overfitting\nof the given task. Those models are typically trained on large-\nscale labelled datasets in a supervised learning strategy [4].\nHowever, gathering annotated images is time-consuming and\nlabour-intensive; as result, the data augmentation (DA) tech-\nnique is a typical solution for expanding the dataset by creating\nnew samples. In the MDE task, the use of DA techniques\ncharacterized by geometric and photometric transformations\nare a standard practice [5], [16]. However, not all the geometric\nand image transformations would be appropriate due to the\nintroduced distortions and aberrations in the image domain,\nwhich are also reflected on the ground-truth depth maps.\nWith METER we propose a data augmentation policy based\non commonly used DA operations while introducing a novel\napproach named shifting strategy . In particular we consider\nas default augmentation policy the use of the vertical flip,\nmirroring, random crop and channels swap of the input image\nas in [16] to make the network invariant to specific color\ndistributions. The key idea is to combine the default augmen-\ntation policy with the shifting strategy augmentation, based\non two simultaneous transformations applied respectively to\nthe input image and to the ground truth depth map. The first\none applies a color ( C) shift to the RGB input images, while\nthe second one is a depth-range ( D) shift, which consists of\nadding a small, random positive or negative value to the depth\nground truth. The mathematical formulation of the computed\ntransformations are following reported; we refer with rgbun\nand rgbaug respectively the unmodified and the augmented\ninput for RGB images and with dun and daug the unmodified\nand the augmented depth map.\nThe C shift augmentation, applied on RGB images, is\ncomposed of two consecutive steps. In the first operation we\napply a gamma-brightness transformation ( rgbgb), as reported\nin Equation 6, where β and γ are respectively the brightness\nand gamma factors that are randomly chosen into a value range\nexperimentally defined between [0.9, 1.1].\nrgbgb = β ∗ (rgbun)γ (6)\nThen, the color augmentation transformation reported in\nEquation 7 is applied, where I is an identity matrix of H ×W\nresolution and η is a scaling factor that is randomly chosen\ninto a value range empirically set between [0.9, 1.1].\nrgbaug = rgbgb ∗ (IH×W ∗ η) (7)\nThe D shift augmentation, Equation 8, is made up of a\nrandom positive or negative value summed to the ground-\ntruth depth maps ( dun). The random value, with a range of\n[−10, +10] centimeters for the indoor dataset and [−10, +10]\ndecimeters for the outdoor one, is uniformly applied to the\nwhole depth map.\ndaug = dun + SH×W (8)\nIn Figure 3 we report a sample frame before and after the\napplication of the proposed strategy with the minimum and\nthe maximum shift values. To emphasise the impact of the\nD shift, we focus on a narrow portion of the original depth\nmap (in a distance range between 150 and 300 centimeters) by\napplying a perceptually uniform colormap and highlighting the\nminimum and maximum depth intervals through the associated\ncolor bars. The reported frames show that the depth with\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3260310\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6\nOriginal β, γ, η = 0.9 \nS = -10cm\nβ, γ, η = 1.1 \nS = +10cm\n(C Shift)\n(D Shift)\nFig. 3: Illustration of an augmented sample with the proposed\nshifting strategy. The shifting factors ( β, γ, η, and S) are set\nas their maximum and minimum values, i.e. {0.9, −10} and\n{1.1, +10} respectively. The min/max depth ranges for the\nregions of interest are given through the respective colored\nbars.\nthe positive displacement ( +10 centimeters) has a lighter\ncolormap, while the depth with the negative displacement\n(−10 centimeters) has a darker one; this effect is emphasised\nby the colormap of the original distance distribution.\nThe introduced depth-range shift augmentation, along with\nthe color and brightness shift and the commonly used trans-\nformations, leads to better final estimations as will be shown\nin Section V-D providing also invariance to color and illumi-\nnation changes.\nIV. E XPERIMENTAL SETUP\nThis section gives a detailed description of the exper-\nimental setup, including training hyper-parameters, bench-\nmark datasets and evaluation metrics respectively in Sec-\ntions IV-A, IV-B, and IV-C.\nA. Training hyper-parameters\nMETER has been implemented using PyTorch 2 deep learn-\ning API, randomly initializing the weights of the architectures.\nAll the models have been trained from scratch using the\nAdamW optimizer [42] with β1 = 0.9, β2 = 0.999, weight\ndecay wd = 0.01 and an initial learning rate of 0.001 with\na decrement of 0.1 every 20 epochs. We use a batch size of\n128 for a total of 60 epochs. For the balanced loss function\nwe empirically choose the scaling factors λ1 = 0 .5 and\nλ2, λ3 = {1, 10, 100} depending on the unity of measure\nused for the predicted depth map, i.e. meters, decimeters or\ncentimeters. We apply a probability of 0.5 for all the random\ntransformations set in the data augmentation policy.\n2Code and corresponding pre-trained weights are made publicly available\nat the following GitHub repository:\nhttps://github.com/lorenzopapa5/METER\nB. Benchmark datasets\nThe datasets used to show the performance of METER\nare NYU Depth v2 [11] and KITTI [12], two popular MDE\nbenchmark datasets for indoor and outdoor scenarios.\nNYU Depth v2 dataset provides RGB images and corre-\nsponding depth maps in several indoor scenarios captured at\na resolution of 640 × 480 pixels. The depth maps have a\nmaximum distance of 10 meters. The dataset contains 120K\ntraining samples and 654 testing samples; we used for training\nthe 50K subset as performed by previous works [5], [16].\nThe input images have been downsampled at a resolution of\n256 × 192.\nKITTI dataset provides stereo RGB images and correspond-\ning 3D laser scans in several outdoor scenarios. The RGB\nimages are captured at a resolution of 1241 ×376 pixels. The\ndepth maps have a maximum distance of 80 meters. We train\nour network at a input resolution of 636 × 192 on Eigen et.\nal [13] split; it is composed of almost 23K training and 697\ntesting samples. Similarly to [21], due to the low density depth\nmaps, we evaluate the compared models in the cropped area\nwhere point-cloud measurement are reported.\nC. Performance evaluation\nWe quantitatively evaluate the performance of METER\nusing common metrics [13] in the monocular depth estimation\ntask: the root-mean-square error (RMSE, in meters [m]), the\nrelative error (REL), and the accuracy value δ1, respectively\nreported in Equations 9, 10, and 11. We remind that yi is\nthe ground truth depth map for the ith pixel while ˆyi is the\npredicted one, n is the total number of pixels for each depth\nimage, and thr is a threshold commonly set to 1.25.\nRMSE =\ns\n1\n|n|\nX\ni∈n\n||yi − ˆyi||2 (9)\nREL = 1\n|n|\nX\ni∈n\n|yi − ˆyi|\nyi\n(10)\nδ1 = 1\n|n|\nX\ni∈n\nmax\n\u0012yi\nˆyi\n, ˆyi\nyi\n\u0013\n< thr (11)\nMoreover, we compare the different models through the\nnumber of multiply-accumulate (MAC) operations and train-\nable parameters. METER has been tested on the low-resource\nembedded 4GB NVIDIA Jetson TX1 3 and the 4GB NVIDIA\nJetson Nano 4 that have a power consumption of 10W and\n5W respectively. Those devices are equipped with an ARM\nCPU and a 256-core NVIDIA Maxwell GPU 5 for the TX1\nand a 128-core for the Nano. The inference speed reported in\nSection V are computed as frame-per-second (fps) on a single\nimage averaged over the entire test dataset.\n3https://developer.nvidia.com/embedded/jetson-tx1\n4https://developer.nvidia.com/embedded/jetson-nano\n5https://developer.nvidia.com/maxwell-compute-architecture\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3260310\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7\nRGB\nInput\nGT\nDepth\nPred\nDepth\nDiff \nMap\nMETER SMETER S METER XS METER XXS METER XS METER XXS\nKITTI NYU Depth v2\nFig. 4: A graphical comparison among METER (S, XS, XXS) configurations. For a better visualization, we apply to depth\nimages and difference maps uniform colormaps with the same depth range. Precisely, in the ground truth (GT) and predicted\ndepth maps (Pred) a lower color intensity corresponds to further distances, while in the difference map (Diff = |GT − Pred|)\na lower color intensity corresponds to a smaller error.\nTABLE II: Comparison with state of the art lightweight methods on the two benchmark datasets. The best scores are in bold\nand second best are underlined; the - represents a value which is not reported in the original paper.\nModels\nNYU KITTI\nRMSE↓ REL↓ δ1 ↑ MAC RMSE↓ REL↓ δ1 ↑ MAC Parameters\n[m] [G] [m] [G] [M]\nCReaM [32] 0.687 0.190 0.704 - - - - - -\nPyD-Net (50) [31] - - - - 6.253 0.262 0.759 - 1.9\nPyD-Net (200) [31] - - - - 6.030 0.153 0.789 - 1.9\nFastDepth [8] 0.579 - 0.772 3.210 - - - - 3.9\nM.Net v2 + FBNet [34] 0.564 - 0.790 - - - - - 2.6\nSPEED [7] 0.566 0.158 0.783 0.552 5.191 0.181 0.770 1.403 2.6\nMETER S 0.471 0.134 0.831 0.975 4.603 0.126 0.829 2.432 3.3\nMETER XS 0.522 0.154 0.793 0.579 4.671 0.128 0.827 1.444 1.4\nMETER XXS 0.580 0.174 0.744 0.186 5.157 0.156 0.782 0.464 0.7\nV. R ESULTS\nIn this section, we report the results obtained with METER\non the two evaluated datasets, NYU Depth v2 and KITTI, de-\nscribed in the previous Section IV-B. In Section V-A METER\nis compared with lightweight, state of the art related works in\nterms of the metrics described in Section IV-C; then, we report\nmultiple ablation studies to emphasize the individual contri-\nbution of each METER component. In particular, Section V-B\nis related to the architecture structure, while Sections V-C\nand V-D analyze respectively the effect of each element of the\nproposed balanced loss function and of the shifting strategy\nused for data augmentation. Finally, in SectionV-E, we provide\nan example of METER application in a real-case scenario.\nA. Comparison with state of the art methods\nIn this section, METER is compared with state of the art\nlightweight models as [7], [8], [31], [32], [34], which are\ndesigned to infer at high speed on embedded devices while\nkeeping a small memory footprint (lower than 3GB). This\nchoice is due to the limited amount of available memory in\nthe chosen platforms. Usually a portion of available RAM is\nreserved for the operating system, thus lowering the overall\namount of available space for the model allocation. In par-\nticular METER and its variants allocate less than 2.1GB of\navailable memory, a value that does not saturate the hardware’s\nmemory and which gives the opportunity to perform other\noperations on the same device. Moreover, for each compared\narchitecture we also report the number of trainable parameters\n(in million [M]) and the number of Multiply-And-Accumulate\n(MAC) operations (in giga [G]).\nThe results can be found in Table II; as can be noticed, ME-\nTER outperforms all the other methods on both the datasets.\nWhen compared with [7], METER S achieves a boost of\n17%, 15%, and 6% respectively for the RMSE, REL and δ1\nmetrics over NYU Depth v2 dataset and of 11%, 30% and\n7% over KITTI. As before, METER XS achieves superior\nperformances, with a boost of 9%, 10% and 5% over NYU\nDepth v2 dataset and of 10%, 29% and 7% over KITTI.\nThe last configuration, METER XXS, can still obtain good\npredictions compared with state of the art models while using\njust 0.7M trainable parameters and 0.186G MAC operations.\nMoreover, in order to assess the frequency performances\nof such architectures, we choose as baseline models SPEED,\ndue to its accuracy, and FastDepth, which is one of the most\npopular technique. When tested on the NVIDIA Jetson TX1,\nsuch models achieve 30.9 fps and 18.8 fps, while METER S,\nXS and XXS achieve respectively 16.3 fps, 18.3 fps and 25.8\nfps. From these results we can remark that our most accurate\nmodel shows similar fps values with respect to FastDepth with\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3260310\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8\nTABLE III: Comparison between the MobileViT [9] and METER encoders over different activation functions (ReLU, SiLU)\nkeeping METER decoder fixed. The fps are measured on the two benchmark hardwares, the NVIDIA Jetson TX1 and the\nNVIDIA Jetson Nano. In bold the best results for each configuration in terms of RMSE, REL and δ1.\nEncoders\nNYU KITTI\nRMSE↓ REL↓ δ1 ↑ TX1↑ Nano↑ MAC RMSE↓ REL↓ δ1 ↑ TX1↑ Nano↑ MAC Parameters\n[m] [fps] [fps] [G] [m] [fps] [fps] [G] [M]\nMobileViT S 0.549 0.168 0.763 13.3 10.5 1.222 4.673 0.128 0.825 5.1 4.1 3.046 5.9\nMobilViT ReLU S 0.521 0.153 0.790 13.3 10.5 1.222 4.789 0.140 0.815 5.1 4.1 3.046 5.9\nMETER SiLU S 0.496 0.145 0.811 16.3 12.0 0.975 4.692 0.134 0.825 5.9 4.8 2.432 3.3\nMETER S 0.471 0.134 0.831 16.3 12.0 0.975 4.603 0.126 0.829 5.9 4.8 2.432 3.3\nMobileViT XS 0.572 0.171 0.754 13.5 13.3 0.815 4.734 0.133 0.822 5.9 5.1 2.032 2.8\nMobilViT ReLU XS 0.547 0.158 0.780 13.5 13.3 0.815 4.797 0.137 0.819 5.9 5.1 2.032 2.8\nMETER SiLU XS 0.539 0.156 0.787 18.3 15.6 0.579 4.727 0.133 0.821 7.2 6.0 1.444 1.4\nMETER XS 0.522 0.154 0.793 18.3 15.6 0.579 4.671 0.128 0.827 7.2 6.0 1.444 1.4\nMobileViT XXS 0.615 0.195 0.715 17.4 16.9 0.472 5.211 0.187 0.761 14.3 10.7 1.180 1.8\nMobilViT ReLU XXS 0.588 0.176 0.737 17.4 16.9 0.472 5.210 0.171 0.763 14.3 10.7 1.180 1.8\nMETER SiLU XXS 0.596 0.180 0.728 25.8 23.2 0.186 5.208 0.165 0.763 20.4 15.1 0.464 0.7\nMETER XXS 0.580 0.174 0.744 25.8 23.2 0.186 5.157 0.156 0.782 20.4 15.1 0.464 0.7\nTABLE IV: Comparison between lightweight decoder architectures keeping METER S encoder fixed. The best scores are in\nbold.\nDecoders\nNYU KITTI\nRMSE↓ REL↓ δ1 ↑ TX1↑ Nano↑ MAC RMSE↓ REL↓ δ1 ↑ TX1↑ Nano↑ MAC Parameters\n[m] [fps] [fps] [G] [m] [fps] [fps] [G] [M]\nNNDSConv5 [8] 0.596 0.174 0.685 15.4 11.5 0.869 5.737 0.164 0.677 5.5 4.7 2.166 3.1\nNNConv5 [8] 0.562 0.167 0.761 14.6 11.3 1.141 4.895 0.139 0.818 5.6 4.5 2.845 3.6\nMDSPP [7] 0.581 0.169 0.694 15.1 11.7 1.004 5.167 0.157 0.760 5.7 4.7 2.503 3.4\nMETER S 0.471 0.134 0.831 16.3 12.0 0.975 4.603 0.126 0.829 5.9 4.8 2.432 3.3\na sensible lower estimation error, while the lightweight XXS\nvariant exhibits comparable estimation performance and fps\nwith respect to SPEED.\nRegarding MAC operations, it is possible to see that SPEED\nMAC value is on par with METER XS, while FastDepth MAC\nis sensible higher than all METER architectures.\nFurthermore, a qualitative analysis between the proposed\nvariants of METER is reported in Figure 4 over an indoor\nand outdoor scenarios. The estimated depths and their asso-\nciated difference (Diff) maps, which are per-pixel differences\nbetween the ground truth depth maps (GT Depth) and the\npredicted (Pred Depth) ones, show how the estimation error\nis distributed along the frame. Precisely, we notice an error\nincrement fairly distributed over the frame as the model\ntrainable parameters of the model are reduced.\nB. Ablation study: the encoder-decoder architecture\nIn this subsection we compare the performances of the\nencoder and the decoder components of METER; results are\nreported in Table III and Table IV, respectively. In particular,\nthe first analysis highlights the contribution of the novel\nMETER block for each configuration (S, XS, and XXS) while\nkeeping METER decoder fixed. The second analysis focuses\non the use of alternative decoders with respect to the default\nMETER decoder, such as NNDSConv5, NNConv5 [8] and\nMDSPP [7] using METER S encoder since it is the encoder\nthat shows the best performances in the evaluated metrics.\nEncoder architectures are compared in Table III, reporting\na one-to-one comparison between METER encoder and the\nMobileViT; evaluating the effects of two different activation\nfunctions (ReLU, SiLU). From the obtained results, we high-\nlight that METER encoder (in bold) achieves better depth\nestimation in all the proposed variants, as well as when\ncompared with the same activation function, using fewer train-\nable parameters and a reduced number of MAC operations.\nIn particular, when compared with the MobileViT, METER\nachieves an average improvement of 10%, 14% and 6% on\nRMSE, REL, and δ1 metrics in the indoor dataset and of 2%,\n7% and 2% respectively on the outdoor dataset. Based on those\nfindings, the overall estimation contribution of the proposed\nencoder over the three configurations is equivalent to 7%,\nwhich almost 3% is due to the use of ReLU activation function\nwith respect to SiLU. Moreover, regarding MAC operations\nwe obtain a reduction of 20%, 29%, and 60% with respect\nto the corresponding MobileViT variants (S, XS, XXS), while\nthe fps improvements are respectively 16% fps, 22% fps, and\n32% on the NVIDIA Jetson TX1 and of 16% fps, 15% fps,\nand 28% fps over the NVIDIA Jetson Nano.\nIn light of the previous experiments, we can state that all\nMETER variants show good accuracy and frequency perfor-\nmances on the NYU Depth v2, while in the case of KITTI\ndataset METER XXS variant should be preferred in order to\nget a reasonable inference speed. Focusing on the timings, the\nMETER XXS variant shows the fastest inference speed, with\nreasonable results also on high resolution images of KITTI\ndataset, avoiding the needing of cropping or downscaling the\noriginal images.\nDecoders architectures are reported in Table IV, comparing\nMETER decoder and those of other lightweight models; we\nused the METER S encoder as baseline. METER decoder\nachieves an RMSE improvement of 16% and 19% on NYU\nDepth v2 dataset and of 6% and 11% on KITTI dataset with\nrespect to NNConv5 and MDSPP models. Furthermore, we\ncompare METER decoder with the NNDSConv5 [8], a variant\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3260310\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9\nLdepth + λ1Lgrad + λ2LnormLdepth Ldepth + λ3LSSIM Ldepth + λ1Lgrad + λ2Lnorm + λ3LSSIM  \nRGB\nInput\nGT\nDepth\nPred\nDepth\nDiff \nMap\nFig. 5: Qualitative comparison of a predicted frame taking into account different loss components. For a better visualization, we\napply to the depth images and to the difference maps uniform colormaps with the same depth range. Precisely, in the ground\ntruth (GT) and predicted depth maps (Pred) a lower color intensity corresponds to further distances, while in the difference\nmap (Diff = |GT − Pred|) a lower color intensity corresponds to a smaller error.\nTABLE V: The effect of each balanced loss function components on the METER S over the considered metrics. The best\nscores are in bold.\nLoss Components\nNYU KITTI\nRMSE↓ REL↓ δ1 ↑ RMSE↓ REL↓ δ1 ↑[m] [m]\nLdepth 0.582 0.185 0.736 5.637 0.183 0.741\nLdepth + λ3LSSIM 0.544 0.161 0.774 5.526 0.198 0.731\nLdepth + λ1Lgrad + λ2Lnorm 0.522 0.153 0.792 5.285 0.166 0.744\nLdepth + λ1Lgrad + λ2Lnorm + λ3LSSIM (BLF) 0.471 0.134 0.831 4.603 0.126 0.829\nof the NNConv5 that takes advantage of depthwise separable\nconvolution to reduce the computational cost. Our encoder-\ndecoder architecture is able to achieve higher speed and a\nsignificant improvement in all the estimation metrics with\ncomparable MAC operations with respect to NNDSConv5.\nFinally, when compared with the NNConv5 decoder, ranked\nsecond in our analysis, the proposed structure is able to achieve\nan overall improvement equal to 12% over the two scenarios.\nMoreover, it can be noticed that the decoder has little influence\non the inference frequency; however, METER decoder still\nshows the best fps on the two hardwares (e.g. 11% of METER\nS compared to NNConv5 on the TX1 hardware and NYU\nDepth v2 dataset). The overall MAC operations decrement\nwith respect to NNConv5 and MDSPP decoders is equal to\n15% on the same configuration as before, suggesting that the\noptimized METER decoder is able to produce more accurate\nestimations while using less operations.\nC. Ablation study: loss function\nIn this subsection we analyze the impact of the different\ncomponents of the proposed balanced loss function introduced\nin Section III-B. METER S architecture is used as a baseline\nmodel. The quantitative and qualitatively comparisons are\nprovided in Table V and Figure 5 respectively, while Figure 6\nshows the converging trends of each introduced component,\nreferring to Ldepth (blue), Lgrad (orange), Lnorm (green) and\nLSSIM (red).\nThe curves shape show that the initial loss contribution is\nmostly attributed to the Ldepth and Lgrad, while the contribu-\nFig. 6: Plot of the individual loss components composing the\nbalanced loss function in the first ten epochs, i.e. almost 3600\niterations.\ntions of the LSSIM and Lnorm penalize from start to finish\nstructural and high-level details prediction errors.\nThe Ldepth component showed to be fundamental for the\ntraining convergence, thus it is applied on every experiment\nof Table V. The obtained results demonstrate that each loss\ncomponent is crucial to get the final METER performance,\nbalancing the reconstruction of the entire image and of edges\ndetails. In fact, the loss formulation in the second row focuses\nonly on the overall image, failing at reaching satisfying results.\nAt the same time, the third row shows a typical loss exploited\nin [38] focusing on edge details but not taking into account\nthe image structure similarity, thus producing an unbalanced\nloss achieving a worse result with respect to the proposed\none, which is able to obtain the lowest estimation error by\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3260310\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10\nTABLE VI: Comparison between different augmentation strategies. The default policy comprises the flip, random crop and\nchannel swap while the others represent the different components of the shifting strategy described in Section III-C. The\nreference model is METER S. The best scores are in bold.\nAugmentation Components\nNYU KITTI\nRMSE↓ REL↓ δ1 ↑ RMSE↓ REL↓ δ1 ↑[m] [m]\ndefault 0.511 0.143 0.813 4.839 0.128 0.826\ndefault + C shift 0.506 0.143 0.815 4.897 0.136 0.810\ndefault + D shift 0.585 0.144 0.805 4.938 0.141 0.804\ndefault + C shift + D shift (our) 0.471 0.134 0.831 4.603 0.126 0.829\nbalancing all the components. In detail, the BLF achieves an\nimprovement of 10%, 12%, and 5% for RMSE, REL and δ1\nmetrics on NYU dataset, and a boost of 13%, 24%, and 10%\nover the KITTI dataset compared to [38].\nMoreover, to better show the qualitative contribution of\neach loss component, provided in Figure 5 the estimated\ndepth under the four analyzed configurations given an input\nsample from KITTI dataset. Based on such example, we can\nobserve a similar behaviour to the one found in Figure 6\nand Table V: the Ldepth component is fundamental for a\ncorrect image reconstruction while the weighted addition of\nspecific loss components ( λ1Lgrad, λ2Lnorm, λ3LSSIM ) can\nquantitatively and qualitatively improve the final estimation.\nThis improvement may also be noticed by observing the\npredicted frames from left to right, where the object details\nand the overall estimation increase significantly as difference\nmaps darken.\nTherefore, we can conclude that the proposed balanced\nloss function can successfully enhance the training process,\nwhile each component can effectively contribute to more\naccurate estimations, hence enhancing the entire framework.\nPrecisely, the overall quantitative contribution of the balanced\nloss function over the two scenarios is equal to 25% when\ncompared with Ldepth, and 13% with respect to the loss\nformulation used in [38].\nD. Ablation study: data augmentation\nIn this ablation study, we evaluate the performances of\nthe proposed data augmentation strategy in comparison with\nstandard MDE data augmentation. We report in Table VI the\nquantitative results of shifting strategy (C shift, D shift) and\nthe default DA (flip, random crop and channel swap) and the\ncombinations of the two. The proposed shifting strategy (last\nrow) achieves, on METER S architecture, an improvement of\n8%, 6%, and 2% over the RMSE, REL and δ1 on the NYU\nDepth v2 dataset, and of 6%, 2%, and 1% over the KITTI\ndataset. On the other hand, the single use of the C shift or\nD shift with the default augmentation does not lead to an\nimprovement in the final estimation, resulting in equivalent or\nslightly worst final prediction. Then, the overall improvement\nof the shifting strategy over the two scenarios is equal to 4%\nwith respect to the default data augmentation policy.\nE. Real-case scenario\nOne of the main objectives of exploring lightweight deep\nlearning solutions is to close the gap between computer vision\nRGB Input Ref Depth Pred Depth\nBC\nA\nFig. 7: METER application in a real-case scenario. Missing\ndepth measurements of reference (Ref) depth are shown as\nyellow pixels. A uniform colormap, with the same depth range,\nhas been applied to the depth maps. Points A (armchair), B\n(box), and C (curtain) on the RGB frame indicates object used\nfor quantitative comparison.\nand practical applications, where the proposed models may be\nintegrated as perception systems, such as robotic systems, thus\ntaking into account possible hardware limitations. Therefore,\nin this subsection, we present an example of a real-case\napplication in which METER is used to estimate the depth\nscene obtained from a generic camera image. We used a\nKinetic V2 to measure the reference depth of the scene. The\nextracted acquisition is reported in Figure 7.\nQualitatively comparing the reference depth and the es-\ntimated one, we can notice a less sharp prediction, which\ncan be mainly attributed to the lower working resolution that\nensures high frame rates on edge devices. However, the object\nshapes are still adequately defined, and the overall estimation\nis visually comparable with the reference frame.\nMoreover, in order to perform a quantitative analysis, we\ncompute the average error of three salient objects that appear\nin the input frame (RGB Input), which are point A for the\narmchair, point B for the box and point C for the curtain.\nThe estimation error for the first two points (A and B) is\nalmost equal to 0.79m, respectively. The obtained value is\nrelated to the fact that we are working in a challenging\nopen-set scenario with different statistics with respect to the\ntraining set. On the other hand, by analyzing point C, we can\nidentify one of the main drawbacks of active depth sensing,\ni.e. missing or incorrect depth measurements under particular\nlighting conditions. In this scenario, although the estimated\ndepth error is unknown, most likely due to the intense light\nsource directed towards the camera sensor, our model can still\ncorrectly identify and estimate the area as a single surface.\nVI. C ONCLUSIONS\nIn this work, we propose METER, a MDE architecture\ncharacterized by a novel lightweight vision transformer model,\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3260310\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11\na multi-component loss function and a specific data augmenta-\ntion policy. Our method exploits a lightweight encoder-decoder\narchitecture characterized by a transformer METER block,\nwhich is able to improve the final depth estimation with a small\nnumber of computed operations, and a fast upsampling block\nemployed in the decoder. METER achieves high inference\nspeed over low-resource embedded hardwares such as the\nNVIDIA Jetson TX1 and the NVIDIA Jetson Nano. Moreover,\nMETER architecture in its three configurations is able to\noutperform previous state of the art lightweight related works.\nThanks to the obtained performances on inference frequency\nand accuracy in the estimation, such proposed architectures\ncan be good candidate to work on multiple MDE scenarios\nand real-world embedded applications. Precisely, METER\nS outperforms the accuracy of state of the art lightweight\nmethods over the two datasets, METER XS represents the\nbest trade-off between inference speed and estimation error,\nand METER XXS reaches a high inference frequency, up to\n25.8 fps, on the two hardwares at the cost of a small increment\nin the estimation error.\nThe obtained results and the limited MAC operations of\nthe proposed network demonstrate that our framework could\nbe valuable in a variety of resource-constrained applications,\nsuch as autonomous systems, drones, and IoT. Moreover, we\nalso test METER in a real-case scenario with a frame captured\nby a generic camera achieving a reasonable estimation error.\nFinally, METER architecture could be a valuable starting\npoint for future studies, in order to get real-time inference\nfrequency on high resolution images, as well as building\ntransformer architectures to take advantage of the attention\nmechanism both in encoder and decoder structures.\nACKNOWLEDGMENTS\nThis work was partially supported by the Sapienza Uni-\nversity of Rome project 2022-2024 “A Novel Vision-based\ndetection system for the control of the ectoparasitic mite\nVarroa destructor in honey bee colonies”.\nREFERENCES\n[1] Y . Ming, X. Meng, C. Fan, and H. Yu, “Deep learning for\nmonocular depth estimation: A review,” Neurocomputing, vol. 438,\npp. 14–33, 2021. [Online]. Available: https://www.sciencedirect.com/\nscience/article/pii/S0925231220320014\n[2] R. Xiaogang, Y . Wenjing, H. Jing, G. Peiyuan, and G. Wei, “Monocular\ndepth estimation based on deep learning:a survey,” in 2020 Chinese\nAutomation Congress (CAC), 2020, pp. 2436–2440.\n[3] Z. Liu, Y . Tan, Q. He, and Y . Xiao, “Swinnet: Swin transformer drives\nedge-aware rgb-d and rgb-t salient object detection,” IEEE Transactions\non Circuits and Systems for Video Technology, vol. 32, no. 7, pp. 4486–\n4497, 2022.\n[4] R. Ranftl, A. Bochkovskiy, and V . Koltun, “Vision transformers for dense\nprediction,” in Proceedings of the IEEE/CVF International Conference\non Computer Vision , 2021, pp. 12 179–12 188.\n[5] S. Farooq Bhat, I. Alhashim, and P. Wonka, “Adabins: Depth estimation\nusing adaptive bins,” in 2021 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR) , 2021, pp. 4008–4017.\n[6] Z. Li, X. Wang, X. Liu, and J. Jiang, “Binsformer: Revisiting adaptive\nbins for monocular depth estimation,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2204.00987\n[7] L. Papa, E. Alati, P. Russo, and I. Amerini, “Speed: Separable pyramidal\npooling encoder-decoder for real-time monocular depth estimation on\nlow-resource settings,” IEEE Access, vol. 10, pp. 44 881–44 890, 2022.\n[8] D. Wofk, F. Ma, T.-J. Yang, S. Karaman, and V . Sze, “Fastdepth: Fast\nmonocular depth estimation on embedded systems,” in 2019 Interna-\ntional Conference on Robotics and Automation (ICRA), 2019, pp. 6101–\n6108.\n[9] S. Mehta and M. Rastegari, “Mobilevit: Light-weight, general-purpose,\nand mobile-friendly vision transformer,” 2021. [Online]. Available:\nhttps://arxiv.org/abs/2110.02178\n[10] Y . Chen, X. Dai, D. Chen, M. Liu, X. Dong, L. Yuan, and Z. Liu,\n“Mobile-former: Bridging mobilenet and transformer,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2022, pp. 5270–5279.\n[11] P. K. Nathan Silberman, Derek Hoiem and R. Fergus, “Indoor segmen-\ntation and support inference from rgbd images,” in ECCV, 2012.\n[12] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:\nThe kitti dataset,” International Journal of Robotics Research (IJRR) ,\n2013.\n[13] D. Eigen, C. Puhrsch, and R. Fergus, “Depth map prediction from\na single image using a multi-scale deep network,” in Proceedings of\nthe 27th International Conference on Neural Information Processing\nSystems - Volume 2, ser. NIPS’14. Cambridge, MA, USA: MIT Press,\n2014, p. 2366–2374.\n[14] Y . Cao, Z. Wu, and C. Shen, “Estimating depth from monocular images\nas classification using deep fully convolutional residual networks,” IEEE\nTransactions on Circuits and Systems for Video Technology , vol. 28,\nno. 11, pp. 3174–3182, 2018.\n[15] Y . Cao, T. Zhao, K. Xian, C. Shen, Z. Cao, and S. Xu, “Monocular\ndepth estimation with augmented ordinal depth relationships,” IEEE\nTransactions on Image Processing , pp. 1–1, 2018.\n[16] I. Alhashim and P. Wonka, “High quality monocular depth estimation\nvia transfer learning,” 2018. [Online]. Available: https://arxiv.org/abs/\n1812.11941\n[17] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely\nconnected convolutional networks,” in 2017 IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , 2017, pp. 2261–2269.\n[18] S. Gur and L. Wolf, “Single image depth estimation trained via depth\nfrom defocus cues,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2019, pp. 7683–7692.\n[19] L.-C. Chen, Y . Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-\ndecoder with atrous separable convolution for semantic image segmen-\ntation,” 02 2018.\n[20] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in 2016 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2016, pp. 770–778.\n[21] M. Song, S. Lim, and W. Kim, “Monocular depth estimation using\nlaplacian pyramid-based depth residuals,” IEEE Transactions on Circuits\nand Systems for Video Technology, vol. 31, no. 11, pp. 4381–4393, 2021.\n[22] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:\nTransformers for image recognition at scale,” 2020. [Online]. Available:\nhttps://arxiv.org/abs/2010.11929\n[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proceedings\nof the 31st International Conference on Neural Information Processing\nSystems, ser. NIPS’17. Red Hook, NY , USA: Curran Associates Inc.,\n2017, p. 6000–6010.\n[24] I. Yun, H.-J. Lee, and C. E. Rhee, “Improving 360 monocular depth es-\ntimation via non-local dense prediction transformer and joint supervised\nand self-supervised learning,” in Proceedings of the AAAI Conference\non Artificial Intelligence , vol. 36, no. 3, 2022, pp. 3224–3233.\n[25] R. Li, P. Ji, Y . Xu, and B. Bhanu, “Monoindoor++:towards better practice\nof self-supervised monocular depth estimation for indoor environments,”\nIEEE Transactions on Circuits and Systems for Video Technology , pp.\n1–1, 2022.\n[26] D. Kim, W. Ga, P. Ahn, D. Joo, S. Chun, and J. Kim, “Global-local\npath networks for monocular depth estimation with vertical cutdepth,”\n2022. [Online]. Available: https://arxiv.org/abs/2201.07436\n[27] Y . Ishii and T. Yamashita, “Cutdepth: Edge-aware data augmentation in\ndepth estimation,” ArXiv, vol. abs/2107.07684, 2021.\n[28] Z. Li, Z. Chen, X. Liu, and J. Jiang, “Depthformer: Exploiting long-\nrange correlation and local information for accurate monocular depth\nestimation,” 2022. [Online]. Available: https://arxiv.org/abs/2203.14211\n[29] M. V ´estias, R. Duarte, J. Sousa, and H. Neto, “Moving deep learning\nto the edge,” Algorithms, vol. 13, p. 125, 05 2020.\n[30] L. Alzubaidi, J. Zhang, A. J. Humaidi, A. Q. AlDujaili, Y . Duan, O. Al-\nShamma, J. Santamar ´ıa, M. A. Fadhel, M. AlAmidie, and L. Farhan,\n“Review of deep learning: concepts, cnn architectures, challenges,\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3260310\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12\napplications, future directions,” Journal of Big Data , vol. 8, no. 1, pp.\n1–74, 2021.\n[31] M. Poggi, F. Aleotti, F. Tosi, and S. Mattoccia, “Towards real-time\nunsupervised monocular depth estimation on cpu,” in 2018 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS) ,\n2018, pp. 5848–5854.\n[32] A. Spek, T. Dharmasiri, and T. Drummond, “Cream: Condensed real-\ntime models for depth prediction using convolutional neural networks,”\n2018.\n[33] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “Mo-\nbilenetv2: Inverted residuals and linear bottlenecks,” in 2018 IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2018, pp.\n4510–4520.\n[34] M. K. Y ¨ucel, V . Dimaridou, A. Drosou, and A. Sa`a-Garriga, “Real-time\nmonocular depth estimation with sparse supervision on mobile,” in 2021\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\nWorkshops (CVPRW), 2021, pp. 2428–2437.\n[35] B. Wu, X. Dai, P. Zhang, Y . Wang, F. Sun, Y . Wu, Y . Tian, P. Vajda,\nY . Jia, and K. Keutzer, “Fbnet: Hardware-aware efficient convnet design\nvia differentiable neural architecture search,” in 2019 IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition (CVPR), 2019, pp.\n10 726–10 734.\n[36] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efficient\nconvolutional neural networks for mobile vision applications,” 2017.\n[Online]. Available: https://arxiv.org/abs/1704.04861\n[37] F. Chollet, “Xception: Deep learning with depthwise separable convolu-\ntions,” in Proceedings of the IEEE conference on computer vision and\npattern recognition, 2017, pp. 1251–1258.\n[38] J. Hu, M. Ozay, Y . Zhang, and T. Okatani, “Revisiting single image\ndepth estimation: Toward higher resolution maps with accurate object\nboundaries,” in 2019 IEEE Winter Conference on Applications of Com-\nputer Vision (WACV), 2019, pp. 1043–1051.\n[39] C. Godard, O. M. Aodha, and G. J. Brostow, “Unsupervised monocular\ndepth estimation with left-right consistency,” in 2017 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) , 2017, pp. 6602–\n6611.\n[40] J. W. Foreman, “Data smart: Using data science to transform information\ninto insight,” 2013.\n[41] Z. Wang, A. Bovik, H. Sheikh, and E. Simoncelli, “Image quality assess-\nment: from error visibility to structural similarity,” IEEE Transactions\non Image Processing , vol. 13, no. 4, pp. 600–612, 2004.\n[42] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\n2014. [Online]. Available: https://arxiv.org/abs/1412.6980\nLORENZO PAPA is a Ph.D. student in Com-\nputer Science Engineering. He collaborates with the\nAlcorLab in the DIAG department, University of\nRome Sapienza, Italy. He received the B.S. degree\nin Computer and Automation Engineering and the\nM.S. degree in Artificial Intelligence and Robotics\nfrom the University of Rome La Sapienza, Italy,\nin 2019 and 2021, respectively. His main research\ninterests are Deep Learning, Computer Vision and\nCyber Security.\nPAOLO RUSSO is an Assistant Researcher at Al-\ncorLab in DIAG department, University of Rome\nSapienza, Italy. He received the B.S. degree in\nTelecommunication Engineering from Universit `a\ndegli studi di Cassino, Italy, in 2008, and the M.S.\ndegree in Artificial Intelligence and Robotics from\nUniversity of Rome La Sapienza, Italy, in 2016. He\nreceived Ph.D. degree in Computer Science from\nUniversity of Rome La Sapienza in 2020. From\n2018 to 2019, he has been a researcher at Italian\nInstitute of Technology (IIT) in Tourin, Italy. His\nmain research interests are Deep Learning, Computer Vision, Generative\nAdversarial Networks, Reinforcement Learning.\nIRENE AMERINI (M’17) received the Laurea\ndegree in computer engineering and the Ph.D. degree\nin computer engineering, multimedia, and telecom-\nmunication from the University of Florence, Italy,\nin 2006 and 2010, respectively. She is currently an\nAssistant Professor with the Department of Com-\nputer, Control, and Management Engineering A.\nRuberti, Sapienza Univeristy of Rome, Italy. She was\na Visiting Scholar with Binghamton University, NY ,\nUSA, in 2010, and a Visiting Research Fellow of\nCharles Sturt University, Australia, in 2018, with a\nfellowship offered by the Australian Government Department of Education\nand Training, through the Endeavour Scholarship & Fellowship Program.\nHer main research activities include digital image processing, multimedia\ncontent security technologies, secure media, and multimedia forensics. She is a\nmember of the IEEE Information Forensics and Security Technical Committee\nand the EURASIP TAC Biometrics, Data Forensics, and Security and the\nIAPR TC6 - Computational Forensics Committee. She has received the Italian\nHabilitation for an Associate Professor in telecommunications and computer\nscience. She is a Guest Editor of several international journals. She is an\nAssociate Editor of IEEE ACCESS, Journal of Electronic Imaging and Journal\nof Information Security and Applications.\nThis article has been accepted for publication in IEEE Transactions on Circuits and Systems for Video Technology. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2023.3260310\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7945576906204224
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6014143228530884
    },
    {
      "name": "Monocular",
      "score": 0.597054123878479
    },
    {
      "name": "Deep learning",
      "score": 0.5155367851257324
    },
    {
      "name": "Computer vision",
      "score": 0.4918546676635742
    },
    {
      "name": "Transformer",
      "score": 0.4688127040863037
    },
    {
      "name": "Frame rate",
      "score": 0.4507826864719391
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4368535876274109
    },
    {
      "name": "Inference",
      "score": 0.43444395065307617
    },
    {
      "name": "Pixel",
      "score": 0.42619168758392334
    },
    {
      "name": "RGB color model",
      "score": 0.4167421758174896
    },
    {
      "name": "Real-time computing",
      "score": 0.41487106680870056
    },
    {
      "name": "Engineering",
      "score": 0.12122243642807007
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I861853513",
      "name": "Sapienza University of Rome",
      "country": "IT"
    }
  ],
  "cited_by": 33
}