{
  "title": "Transformers to Learn Hierarchical Contexts in Multiparty Dialogue for Span-based Question Answering",
  "url": "https://openalex.org/W3034520363",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2304771752",
      "name": "Changmao Li",
      "affiliations": [
        "Emory University"
      ]
    },
    {
      "id": "https://openalex.org/A2125846880",
      "name": "Jinho D. Choi",
      "affiliations": [
        "Emory University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W2888302696",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2557764419",
    "https://openalex.org/W2963963993",
    "https://openalex.org/W2964120615",
    "https://openalex.org/W2558203065",
    "https://openalex.org/W2612228435",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2979928633",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2970049541",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W2964223283",
    "https://openalex.org/W2912904516",
    "https://openalex.org/W2990138404",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4288614645",
    "https://openalex.org/W2963339397"
  ],
  "abstract": "We introduce a novel approach to transformers that learns hierarchical representations in multiparty dialogue. First, three language modeling tasks are used to pre-train the transformers, token- and utterance-level language modeling and utterance order prediction, that learn both token and utterance embeddings for better understanding in dialogue contexts. Then, multi-task learning between the utterance prediction and the token span prediction is applied to fine-tune for span-based question answering (QA). Our approach is evaluated on the FriendsQA dataset and shows improvements of 3.8% and 1.4% over the two state-of-the-art transformer models, BERT and RoBERTa, respectively.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5709–5714\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n5709\nTransformers to Learn Hierarchical Contexts in Multiparty Dialogue\nfor Span-based Question Answering\nChangmao Li\nDepartment of Computer Science\nEmory University\nAtlanta, GA, USA\nchangmao.li@emory.edu\nJinho D. Choi\nDepartment of Computer Science\nEmory University\nAtlanta, GA, USA\njinho.choi@emory.edu\nAbstract\nWe introduce a novel approach to transformers\nthat learns hierarchical representations in mul-\ntiparty dialogue. First, three language model-\ning tasks are used to pre-train the transformers,\ntoken- and utterance-level language modeling\nand utterance order prediction, that learn both\ntoken and utterance embeddings for better un-\nderstanding in dialogue contexts. Then, multi-\ntask learning between the utterance prediction\nand the token span prediction is applied to ﬁne-\ntune for span-based question answering (QA).\nOur approach is evaluated on the FRIENDS QA\ndataset and shows improvements of 3.8% and\n1.4% over the two state-of-the-art transformer\nmodels, BERT and RoBERTa, respectively.\n1 Introduction\nTransformer-based contextualized embedding ap-\nproaches such as BERT (Devlin et al., 2019), XLM\n(CONNEAU and Lample, 2019), XLNet (Yang\net al., 2019), RoBERTa (Liu et al., 2019), and\nAlBERT (Lan et al., 2019) have re-established the\nstate-of-the-art for practically all question answer-\ning (QA) tasks on not only general domain datasets\nsuch as SQUAD (Rajpurkar et al., 2016, 2018),MS\nMARCO (Nguyen et al., 2016), TRIVIA QA (Joshi\net al., 2017), NEWS QA (Trischler et al., 2017), or\nNARRATIVE QA (Koisk et al., 2018), but also multi-\nturn question datasets such as SQA (Iyyer et al.,\n2017), QUAC (Choi et al., 2018), COQA (Reddy\net al., 2019), or CQA (Talmor and Berant, 2018).\nHowever, for span-based QA where the evidence\ndocuments are in the form of multiparty dialogue,\nthe performance is still poor even with the latest\ntransformer models (Sun et al., 2019; Yang and\nChoi, 2019) due to the challenges in representing\nutterances composed by heterogeneous speakers.\nSeveral limitations can be expected for language\nmodels trained on general domains to process dia-\nlogue. First, most of these models are pre-trained\non formal writing, which is notably different from\ncolloquial writing in dialogue; thus, ﬁne-tuning for\nthe end tasks is often not sufﬁcient enough to build\nrobust dialogue models. Second, unlike sentences\nin a wiki or news article written by one author with\na coherent topic, utterances in a dialogue are from\nmultiple speakers who may talk about different top-\nics in distinct manners such that they should not be\nrepresented by simply concatenating, but rather as\nsub-documents interconnected to one another.\nThis paper presents a novel approach to the latest\ntransformers that learns hierarchical embeddings\nfor tokens and utterances for a better understand-\ning in dialogue contexts. While ﬁne-tuning for\nspan-based QA, every utterance as well as the ques-\ntion are separated encoded and multi-head atten-\ntions and additional transformers are built on the\ntoken and utterance embeddings respectively to pro-\nvide a more comprehensive view of the dialogue\nto the QA model. As a result, our model achieves\na new state-of-the-art result on a span-based QA\ntask where the evidence documents are multiparty\ndialogue. The contributions of this paper are:1\n• New pre-training tasks are introduced to improve\nthe quality of both token-level and utterance-level\nembeddings generated by the transformers, that\nbetter suit to handle dialogue contexts (§2.1).\n• A new multi-task learning approach is proposed\nto ﬁne-tune the language model for span-based\nQA that takes full advantage of the hierarchical\nembeddings created from the pre-training (§2.2).\n• Our approach signiﬁcantly outperforms the pre-\nvious state-of-the-art models using BERT and\nRoBERTa on a span-based QA task using dia-\nlogues as evidence documents (§3).\n1All our resources including the source codes and the dataset\nwith the experiment split are available at\nhttps://github.com/emorynlp/friendsqa\n5710\nTransformer Encoder (TE)\nSoftmax\new\n11es\n1 /uni22EFe/uni03BC\nijew\n1n/uni22EFec\no/uni03BC\nij\n/uni22EF/uni22EF[CLS] s1 w11 w1n /uni03BCij /uni22EF/uni22EFsm wm1 wmn\n/uni22EFes\nm ew\nm1 ew\nmn/uni22EF\nTransformer Encoder (TE)\nSoftmax\n/uni22EF/uni22EF[CLSi] si wi1 /uni03BCij win\n/uni22EFew\ninew\ni1es\ni e/uni03BC\nik/uni22EFec\ni\no/uni03BC\nij\nTransformer \nEncoder \n(TE)\n/uni22EF\n[CLS1] s1 w11 w1n\n[CLSm] sm wm1 wmn\n/uni22EF\n/uni22EF\n/uni22EF\n/uni22EF\n[CLSi] /uni22EF\nsi wi1 /uni22EFwin /uni22EF/uni22EF\nTL2 TL1Softmaxo/uni03BD tc\n1 tc\nm/uni22EFtc\ni /uni22EF\nec\n1\nec\nm /uni22EF/uni22EF\nec\ni\new\n11es\n1\nes\nm ew\nm1\new\n1n\n/uni22EFew\nmn\n/uni22EF\nes\ni ew\ni1 /uni22EFew\nin /uni22EF/uni22EF\n/uni22EF\n/uni22EF\n(a) Token-level MLM (§2.1.1)\nTransformer Encoder (TE)\nSoftmax\new\n11es\n1 /uni22EFe/uni03BC\nijew\n1n/uni22EFec\no/uni03BC\nij\n/uni22EF/uni22EF[CLS] s1 w11 w1n /uni03BCij /uni22EF/uni22EFsm wm1 wmn\n/uni22EFes\nm ew\nm1 ew\nmn/uni22EF\nTransformer Encoder (TE)\nSoftmax\n/uni22EF/uni22EF[CLSi] si wi1 /uni03BCij win\n/uni22EFew\ninew\ni1es\ni e/uni03BC\nik/uni22EFec\ni\no/uni03BC\nij\nTransformer \nEncoder \n(TE)\n/uni22EF\n[CLS1] s1 w11 w1n\n[CLSm] sm wm1 wmn\n/uni22EF\n/uni22EF\n/uni22EF\n/uni22EF\n[CLSi] /uni22EF\nsi wi1 /uni22EFwin /uni22EF/uni22EF\nTL2 TL1Softmaxo/uni03BD tc\n1 tc\nm/uni22EFtc\ni /uni22EF\nec\n1\nec\nm /uni22EF/uni22EF\nec\ni\new\n11es\n1\nes\nm ew\nm1\new\n1n\n/uni22EFew\nmn\n/uni22EF\nes\ni ew\ni1 /uni22EFew\nin /uni22EF/uni22EF\n/uni22EF\n/uni22EF (b) Utterance-level MLM (§2.1.2)\nTransformer Encoder (TE)\nSoftmax\new\n11es\n1 /uni22EFe/uni03BC\nijew\n1n/uni22EFec\no/uni03BC\nij\n/uni22EF/uni22EF[CLS] s1 w11 w1n /uni03BCij /uni22EF/uni22EFsm wm1 wmn\n/uni22EFes\nm ew\nm1 ew\nmn/uni22EF\nTransformer Encoder (TE)\nSoftmax\n/uni22EF/uni22EF[CLSi] si wi1 /uni03BCij win\n/uni22EFew\ninew\ni1es\ni e/uni03BC\nik/uni22EFec\ni\no/uni03BC\nij\nTransformer \nEncoder \n(TE)\n/uni22EF\n[CLS1] s1 w11 w1n\n[CLSm] s/uni2032 m w/uni2032 m1 w/uni2032 mn\n/uni22EF\n/uni22EF\n/uni22EF\n/uni22EF\n[CLSi] /uni22EF\ns/uni2032 i w/uni2032 i1 /uni22EFw/uni2032 in /uni22EF/uni22EF\nTL2 TL1Softmaxo/uni03BD tc\n1 tc\nm/uni22EFtc\ni /uni22EF\nec\n1\nec\nm /uni22EF/uni22EF\nec\ni\new\n11es\n1\nes\nm ew\nm1\new\n1n\n/uni22EFew\nmn\n/uni22EF\nes\ni ew\ni1 /uni22EFew\nin /uni22EF/uni22EF\n/uni22EF\n/uni22EF\n(c) Utterance order prediction (§2.1.3)\nFigure 1: The overview of our models for the three pre-training tasks (Section 2.1).\n2 Transformers for Learning Dialogue\nThis section introduces a novel approach for pre-\ntraining (Section 2.1) and ﬁne-tuning (Section 2.2)\ntransformers to effectively learn dialogue contexts.\nOur approach has been evaluated with two kinds\nof transformers, BERT (Devlin et al., 2019) and\nRoBERTa (Liu et al., 2019), and shown signiﬁcant\nimprovement to a question answering task (QA) on\nmultiparty dialogue (Section 3).\n2.1 Pre-training Language Models\nPre-training involves 3 tasks in sequence, the token-\nlevel masked language modeling (MLM; §2.1.1),\nthe utterance-level MLM ( §2.1.2), and the utter-\nance order prediction ( §2.1.3), where the trained\nweights from each task are transferred to the next\ntask. Note that the weights of publicly available\ntransformer encoders are adapted to train the token-\nlevel MLM, which allows our QA model to han-\ndle languages in both dialogues, used as evidence\ndocuments, and questions written in formal writing.\nTransformers from BERT and RoBERTa are trained\nwith static and dynamic MLM respectively, as de-\nscribed by Devlin et al. (2019); Liu et al. (2019).\n2.1.1 Token-level Masked LM\nFigure 1(a) illustrates the token-level MLM model.\nLet D= {U1,...,U m}be a dialogue where Ui =\n{si,wi1,...,w in}is the i’th utterance inD, si is\nthe speaker of Ui, and wij is the j’th token in Ui.\nAll speakers and tokens in Dare appended in order\nwith the special token CLS, representing the entire\ndialogue, which creates the input string sequence\nI = {CLS}⊕U1 ⊕... ⊕Un. For every wij ∈I, let\nIµ\nij = (I\\{wij}) ∪{µij}, where µij is the masked\ntoken substituted in place of wij. Iµ\nij is then fed\ninto the transformer encoder (TE), which generates\na sequence of embeddings {ec}⊕E1 ⊕... ⊕Em\nwhere Ei = {es\ni,ew\ni1,..,e w\nin}is the embedding list\nfor Ui, and (ec,es\ni,ew\nij,eµ\nij) are the embeddings of\n(CLS,si,wij,µij) respectively. Finally, eµ\nij is fed\ninto a softmax layer that generates the output vector\noµ\nij ∈R|V|to predict µij, where V is the set of all\nvocabularies in the dataset.2\n2.1.2 Utterance-level Masked LM\nThe token-level MLM (t-MLM) learns attentions\namong all tokens in Dregardless of the utterance\nboundaries, allowing the model to compare every\ntoken to a broad context; however, it fails to catch\nunique aspects about individual utterances that can\nbe important in dialogue. To learn an embedding\nfor each utterance, the utterance-level MLM model\nis trained (Figure 1(b)). Utterance embeddings can\nbe used independently and/or in sequence to match\ncontexts in the question and the dialogue beyond\nthe token-level, showing an advantage in ﬁnding\nutterances with the correct answer spans (§2.2.1).\n2n: the maximum number of words in every utterance,\nm: the maximum number of utterances in every dialogue.\n5711\nTransformer \nEncoder \n(TE)\nTL1TL2\nec\nq\nec\n1\nec\nm /uni22EF\neq\n1 eq\n2 /uni22EFeq\nn\new\n11es\n1 /uni22EF\nes\nm ew\nm1\new\n1n\n/uni22EFew\nmn\n/uni22EF/uni22EF\n/uni22EF\nSoftmaxSoftmax\ne/uni007Ct /uni2208/uni211Dd\nE /uni2208/uni211Dn×d\nou /uni2208/uni211Dm+1\nol/uni007Cr /uni2208/uni211Dn+1\n/uni22EF\n[CLS1] s1 w11 w1n\n[CLSm] sm wm1 wmn\n/uni22EF\n/uni22EF\n[CLSq] q1 q2 qn\n/uni22EF\n/uni22EF\n/uni22EF\nMHA\ntc\nq tc\n1 tc\nm/uni22EF Softmax Eq\n1 Eq\n2 Eq\nm/uni22EF\nor\n1 or\n2 or\nm/uni22EFo/uni2113\n1 o/uni2113\n2 o/uni2113\nm/uni22EFou\nFigure 2: The overview of our ﬁne-tuning model exploiting multi-task learning (Section 2.2).\nFor every utterance Ui, the masked input sequence\nIµ\nij = {CLSi}⊕{(Ui\\{wij}) ∪µij}is generated.\nNote that CLSi now represents Ui instead of Dand\nIµ\nij is much shorter than the one used for t-MLM.\nIµ\nij is fed into TE, already trained by t-MLM, and\nthe embedding sequence Ei = {ec\ni,es\ni,ew\ni1,..,e w\nin}\nis generated. Finally, ec\ni, instead of eµ\nij, is fed into\na softmax layer that generates oµ\nij to predict µij.\nThe intuition behind the utterance-level MLM is\nthat once ec\ni learns enough contents to accurately\npredict any token inUi, it consists of most essential\nfeatures about the utterance; thus, ec\ni can be used\nas the embedding of Ui.\n2.1.3 Utterance Order Prediction\nThe embedding ec\ni from the utterance-level MLM\n(u-MLM) learns contents within Ui, but not across\nother utterances. In dialogue, it is often the case\nthat a context is completed by multiple utterances;\nthus, learning attentions among the utterances is\nnecessary. To create embeddings that contain cross-\nutterance features, the utterance order prediction\nmodel is trained (Figure 1(c)). Let D= D1 ⊕D2\nwhere D1 and D2 comprise the ﬁrst and the second\nhalves of the utterances in D, respectively. Also,\nlet D′= D1 ⊕D′\n2 where D′\n2 contains the same set\nof utterances as D2 although the ordering may be\ndifferent. The task is whether or not D′preserves\nthe same order of utterances as D.\nFor each Ui ∈D′, the input Ii = {CLSi}⊕Ui is\ncreated and fed intoTE, already trained by u-MLM,\nto create the embeddings Ei = {ec\ni,es\ni,ew\ni1,..,e w\nin}.\nThe sequence Ec = {ec\n1,...,e c\nn}is fed into two\ntransformer layers, TL1 and TL2, that generate the\nnew utterance embedding list Tc = {tc\n1,...,t c\nn}.\nFinally, Tcis fed into a softmax layer that generates\noν ∈R2 to predict whether or not D′is in order.\n2.2 Fine-tuning for QA on Dialogue\nFine-tuning exploits multi-task learning between\nthe utterance ID prediction (§2.2.1) and the token\nspan prediction (§2.2.2), which allows the model to\ntrain both the utterance- and token-level attentions.\nThe transformer encoder (TE) trained by the utter-\nance order prediction (UOP) is used for both tasks.\nGiven the question Q= {q1,...,q n}(qi is the i’th\ntoken in Q) and the dialogue D= {U1,...,U m},\nQand all U∗are fed into TE that generates Eq =\n{ec\nq,eq\n1,..,e q\nn}and Ei = {ec\ni,es\ni,ew\ni1,..,e w\nin}for Q\nand every Ui, respectively.\n2.2.1 Utterance ID Prediction\nThe utterance embedding list Ec = {ec\nq,ec\n1,..,e c\nn}\nis fed into TL1 and TL2 from UOP that generate\nTc = {tc\nq,tc\n1,..,t c\nn}. Tc is then fed into a softmax\nlayer that generates ou ∈Rm+1 to predict the ID of\nthe utterance containing the answer span if exists;\notherwise, the 0’th label is predicted, implying that\nthe answer span for Qdoes not exist in D.\n2.2.2 Token Span Prediction\nFor every Ei, the pair (E′\nq,E′\ni) is fed into the multi-\nhead attention layer, MHA, where E′\nq = Eq \\{ec\nq}\nand E′\ni = Ei \\{ec\ni}. MHA (Vaswani et al., 2017)\nthen generates the attended embedding sequences,\nTa\n1 ,...,T a\nm, where Ta\ni = {ts\ni,tw\ni1,..,t w\nin}. Finally,\neach Ta\ni is fed into two softmax layers, SL and SR,\nthat generate oℓ\ni ∈Rn+1 and or\ni ∈Rn+1 to predict\nthe leftmost and the rightmost tokens in Ui respec-\ntively, that yield the answer span for Q. It is possi-\nble that the answer spans are predicted in multiple\nutterances, in which case, the span from the utter-\nance that has the highest score for the utterance ID\nprediction is selected, which is more efﬁcient than\nthe typical dynamic programming approach.\n5712\n3 Experiments\n3.1 Corpus\nDespite of all great work in QA, only two datasets\nare publicly available for machine comprehension\nthat take dialogues as evidence documents. One is\nDREAM comprising dialogues for language exams\nwith multiple-choice questions (Sun et al., 2019).\nThe other is FRIENDS QA containing transcripts\nfrom the TV showFriends with annotation for span-\nbased question answering (Yang and Choi, 2019).\nSince DREAM is for a reading comprehension task\nthat does not need to ﬁnd the answer contents from\nthe evidence documents, it is not suitable for our\napproach; thus, FRIENDS QA is chosen.\nEach scene is treated as an independent dialogue\nin FRIENDS QA. Yang and Choi (2019) randomly\nsplit the corpus to generate training, development,\nand evaluation sets such that scenes from the same\nepisode can be distributed across those three sets,\ncausing inﬂated accuracy scores. Thus, we re-split\nthem by episodes to prevent such inﬂation. For ﬁne-\ntuning (§2.2), episodes from the ﬁrst four seasons\nare used as described in Table 1. For pre-training\n(§2.1), all transcripts from Seasons 5-10 are used\nas an additional training set.\nSet D Q A E\nTraining 973 9,791 16,352 1 - 20\nDevelopment 113 1,189 2,065 21 - 22\nEvaluation 136 1,172 1,920 23 - *\nTable 1: New data split for FriendsQA. D/Q/A: # of\ndialogues/questions/answers, E: episode IDs.\n3.2 Models\nThe weights from the BERTbase and RoBERTabase\nmodels (Devlin et al., 2019; Liu et al., 2019) are\ntransferred to all models in our experiments. Four\nbaseline models, BERT, BERTpre, RoBERTa, and\nRoBERTapre, are built, where all models are ﬁne-\ntuned on the datasets in Table 1 and the *pre mod-\nels are pre-trained on the same datasets with the\nadditional training set from Seasons 5-10 ( §3.1).\nThe baseline models are compared to BERTour and\nRoBERTAour that are trained by our approach.3\n3.3 Results\nTable 2 shows results achieved by all the models.\nFollowing Yang and Choi (2019), exact matching\n(EM), span matching (SM), and utterance match-\ning (UM) are used as the evaluation metrics. Each\n3Detailed experimental setup are provided in Appendices.\nmodel is developed three times and their average\nscore as well as the standard deviation are reported.\nThe performance ofRoBERTa* is generally higher\nthan BERT* although RoBERTabase is pre-trained\nwith larger datasets including CC-N EWS (Nagel,\n2016), OPEN WEBTEXT (Gokaslan and Cohen,\n2019), and STORIES (Trinh and Le, 2018) than\nBERTbase such that results from those two types of\ntransformers cannot be directly compared.\nModel EM SM UM\nBERT 43.3(±0.8) 59.3(±0.6) 70.2(±0.4)\nBERTpre 45.6(±0.9) 61.2(±0.7) 71.3(±0.6)\nBERTour 46.8(±1.3) 63.1(±1.1) 73.3(±0.7)\nRoBERTa 52.6(±0.7) 68.2(±0.3) 80.9(±0.8)\nRoBERTapre 52.6(±0.7) 68.6(±0.6) 81.7(±0.7)\nRoBERTaour 53.5(±0.7) 69.6(±0.8) 82.7(±0.5)\nTable 2: Accuracies ( ±standard deviations) achieved\nby the BERT and RoBERTa models.\nThe *pre models show marginal improvement over\ntheir base models, implying that pre-training the\nlanguage models on FRIENDS QA with the original\ntransformers does not make much impact on this\nQA task. The models using our approach perform\nnoticeably better than the baseline models, showing\n3.8% and 1.4% improvements on SM from BERT\nand RoBERTa, respectively.\nType Dist. EM SM UM\nWhere 18.16 66.1(±0.5) 79.9(±0.7) 89.8(±0.7)\nWhen 13.57 63.3(±1.3) 76.4(±0.6) 88.9(±1.2)\nWhat 18.48 56.4(±1.7) 74.0(±0.5) 87.7(±2.1)\nWho 18.82 55.9(±0.8) 66.0(±1.7) 79.9(±1.1)\nHow 15.32 43.2(±2.3) 63.2(±2.5) 79.4(±0.7)\nWhy 15.65 33.3(±2.0) 57.3(±0.8) 69.8(±1.8)\nTable 3: Results from theRoBERTaour model by differ-\nent question types.\nTable 3 shows the results achieved byRoBERTaour\nw.r.t. question types. UM drops signiﬁcantly for\nWhy that often spans out to longer sequences and\nalso requires deeper inferences to answer correctly\nthan the others. Compared to the baseline models,\nour models show more well-around performance\nregardless the question types.4\n3.4 Ablation Studies\nTable 4 shows the results from ablation studies to\nanalyze the impacts of the individual approaches.\nBERTpre and RoBERTapre are the same as in Ta-\nble 2, that are the transformer models pre-trained by\n4Question type results for all models are in Appendices.\n5713\nthe token-level masked LM (§2.1.1) and ﬁne-tuned\nby the token span prediction (§2.2.2). BERTuid and\nRoBERTauid are the models that are pre-trained by\nthe token-level masked LM and jointly ﬁne-tuned\nby the token span prediction as well as the utter-\nance ID prediction (UID: §2.2.1). Given these two\ntypes of transformer models, the utterance-level\nmasked LM (ULM: §2.1.2) and the utterance order\nprediction (UOP: §2.1.3) are separately evaluated.\nModel EM SM UM\nBERTpre 45.6(±0.9) 61.2(±0.7) 71.3(±0.6)\n⊕ULM 45.7(±0.9) 61.8(±0.9) 71.8(±0.5)\n⊕ULM⊕UOP 45.6(±0.9) 61.7(±0.7) 71.7(±0.6)\nBERTuid 45.7(±0.8) 61.1(±0.8) 71.5(±0.5)\n⊕ULM 46.2(±1.1) 62.4(±1.2) 72.5(±0.8)\n⊕ULM⊕UOP 46.8(±1.3) 63.1(±1.1) 73.3(±0.7)\nRoBERTapre 52.6(±0.7) 68.6(±0.6) 81.7(±0.7)\n⊕ULM 52.9(±0.8) 68.7(±1.1) 81.7(±0.6)\n⊕ULM⊕UOP 52.5(±0.8) 68.8(±0.5) 81.9(±0.7)\nRoBERTauid 52.8(±0.9) 68.7(±0.8) 81.9(±0.5)\n⊕ULM 53.2(±0.6) 69.2(±0.7) 82.4(±0.5)\n⊕ULM⊕UOP 53.5(±0.7) 69.6(±0.8) 82.7(±0.5)\nTable 4: Results for the ablation studies. Note that the\n*uid⊕ULM ⊕UOP models are equivalent to the*our mod-\nels in Table 2, respectively.\nThese two dialogue-speciﬁc LM approaches, ULM\nand UOP, give very marginal improvement over the\nbaseline models, that is rather surprising. However,\nthey show good improvement when combined with\nUID, implying that pre-training language models\nmay not be enough to enhance the performance by\nitself but can be effective when it is coupled with\nan appropriate ﬁne-tuning approach. Since both\nULM and UOP are designed to improve the quality\nof utterance embeddings, it is expected to improve\nthe accuracy for UID as well. The improvement\non UM is indeed encouraging, giving 2% and 1%\nboosts to BERTpre and RoBERTapre, respectively\nand consequently improving the other two metrics.\n3.5 Error Analysis\nAs shown in Table 3, the major errors are from the\nthree types of questions, who, how, and why; thus,\nwe select 100 dialogues associated with those ques-\ntion types that our best model, RoBERTaour, incor-\nrectly predicts the answer spans for. Speciﬁc exam-\nples are provided in Tables 12, 13 and 14 ( §A.3).\nFollowing Yang et al. (2019), errors are grouped\ninto 6 categories, entity resolution, paraphrase and\npartial match, cross-utterance reasoning, question\nbias, noise in annotation, and miscellaneous.\nTable 5 shows the errors types and their ratios with\nrespect to the question types. Two main error types\nare entity resolution and cross-utterance reasoning.\nThe entity resolution error happens when many of\nthe same entities are mentioned in multiple utter-\nances. This error also occurs when the QA system\nis asked about a speciﬁc person, but predicts wrong\npeople where there are so many people appearing in\nmultiple utterances. The cross-utterance reasoning\nerror often happens with the why and how ques-\ntions where the model relies on pattern matching\nmostly and predicts the next utterance span of the\nmatched pattern.\nError Types Who How Why\nEntity Resolution 34% 23% 20%\nParaphrase and Partial Match14% 14% 13%\nCross-Utterance Reasoning 25% 28% 27%\nQuestion Bias 11% 13% 17%\nNoise in Annotation 4% 7% 9%\nMiscellaneous 12% 15% 14%\nTable 5: Error types and their ratio with respect to the\nthree most challenging question types.\n4 Conclusion\nThis paper introduces a novel transformer approach\nthat effectively interprets hierarchical contexts in\nmultiparty dialogue by learning utterance embed-\ndings. Two language modeling approaches are pro-\nposed, utterance-level masked LM and utterance\norder prediction. Coupled with the joint inference\nbetween token span prediction and utterance ID\nprediction, these two language models signiﬁcantly\noutperform two of the state-of-the-art transformer\napproaches, BERT and RoBERTa, on a span-based\nQA task called FriendsQA . We will evaluate our\napproach on other machine comprehension tasks\nusing dialogues as evidence documents to further\nverify the generalizability of this work.\nAcknowledgments\nWe gratefully acknowledge the support of the AWS\nMachine Learning Research Awards (MLRA). Any\ncontents in this material are those of the authors\nand do not necessarily reﬂect the views of them.\nReferences\nEunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-\ntau Yih, Yejin Choi, Percy Liang, and Luke Zettle-\nmoyer. 2018. Quac: Question answering in context.\n5714\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing.\nAlexis CONNEAU and Guillaume Lample. 2019.\nCross-lingual language model pretraining. In\nH. Wallach, H. Larochelle, A. Beygelzimer,\nF. d'Alch´e-Buc, E. Fox, and R. Garnett, editors, Ad-\nvances in Neural Information Processing Systems\n32, pages 7057–7067. Curran Associates, Inc.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL’19, pages 4171–4186.\nAaron Gokaslan and Vanya Cohen. 2019. OpenWeb-\nText Corpus.\nMohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017.\nSearch-based neural structured learning for sequen-\ntial question answering. In Proceedings of the\n55th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) ,\npages 1821–1831, Vancouver, Canada. Association\nfor Computational Linguistics.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers).\nTom Koisk, Jonathan Schwarz, Phil Blunsom, Chris\nDyer, Karl Moritz Hermann, Gbor Melis, and Ed-\nward Grefenstette. 2018. The narrativeqa reading\ncomprehension challenge. Transactions of the As-\nsociation for Computational Linguistics, 6:317328.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv, 1907.11692.\nSebastian Nagel. 2016. News Dataset Available.\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\n2016. MS MARCO: A human generated machine\nreading comprehension dataset. In Proceedings\nof the Workshop on Cognitive Computation: Inte-\ngrating neural and symbolic approaches 2016 co-\nlocated with the 30th Annual Conference on Neu-\nral Information Processing Systems (NIPS 2016),\nBarcelona, Spain, December 9, 2016.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you dont know: Unanswerable ques-\ntions for squad. Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers).\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing.\nSiva Reddy, Danqi Chen, and Christopher D. Manning.\n2019. Coqa: A conversational question answering\nchallenge. Transactions of the Association for Com-\nputational Linguistics, 7:249266.\nKai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi,\nand Claire Cardie. 2019. DREAM: A Challenge\nData Set and Models for Dialogue-Based Reading\nComprehension. Transactions of the Association for\nComputational Linguistics, 7:217–231.\nAlon Talmor and Jonathan Berant. 2018. The web as\na knowledge-base for answering complex questions.\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers).\nTrieu H. Trinh and Quoc V . Le. 2018. A Sim-\nple Method for Commonsense Reasoning. arXiv,\n1806.02847.\nAdam Trischler, Tong Wang, Xingdi Yuan, Justin Har-\nris, Alessandro Sordoni, Philip Bachman, and Ka-\nheer Suleman. 2017. Newsqa: A machine compre-\nhension dataset. Proceedings of the 2nd Workshop\non Representation Learning for NLP.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of the 31st International\nConference on Neural Information Processing Sys-\ntems, NIPS’17, pages 6000–6010, USA. Curran As-\nsociates Inc.\nZhengzhe Yang and Jinho D. Choi. 2019. FriendsQA:\nOpen-domain question answering on TV show tran-\nscripts. In Proceedings of the 20th Annual SIGdial\nMeeting on Discourse and Dialogue, pages 188–197,\nStockholm, Sweden. Association for Computational\nLinguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. In H. Wallach,\nH. Larochelle, A. Beygelzimer, F. d'Alch ´e-Buc,\nE. Fox, and R. Garnett, editors, Advances in Neu-\nral Information Processing Systems 32, pages 5754–\n5764. Curran Associates, Inc.",
  "topic": "Utterance",
  "concepts": [
    {
      "name": "Utterance",
      "score": 0.9269512891769409
    },
    {
      "name": "Security token",
      "score": 0.8336524367332458
    },
    {
      "name": "Transformer",
      "score": 0.831206202507019
    },
    {
      "name": "Computer science",
      "score": 0.8003795146942139
    },
    {
      "name": "Question answering",
      "score": 0.777090311050415
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5420587062835693
    },
    {
      "name": "Language model",
      "score": 0.53493732213974
    },
    {
      "name": "Span (engineering)",
      "score": 0.49687841534614563
    },
    {
      "name": "Natural language processing",
      "score": 0.4952920973300934
    },
    {
      "name": "Speech recognition",
      "score": 0.33322474360466003
    },
    {
      "name": "Engineering",
      "score": 0.07528233528137207
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Civil engineering",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I150468666",
      "name": "Emory University",
      "country": "US"
    }
  ],
  "cited_by": 24
}