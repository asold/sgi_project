{
  "title": "Romanization-based Large-scale Adaptation of Multilingual Language Models",
  "url": "https://openalex.org/W4389520101",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5090787541",
      "name": "Sukannya Purkayastha",
      "affiliations": [
        "Technical University of Darmstadt",
        "Hess (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5037310413",
      "name": "Sebastian Ruder",
      "affiliations": [
        "DeepMind (United Kingdom)",
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5024983536",
      "name": "Jonas Pfeiffer",
      "affiliations": [
        "DeepMind (United Kingdom)",
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5027450194",
      "name": "Iryna Gurevych",
      "affiliations": [
        "Technical University of Darmstadt",
        "Hess (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5014866912",
      "name": "Ivan Vulić",
      "affiliations": [
        "University of Cambridge"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3168641112",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W4386566878",
    "https://openalex.org/W2964114970",
    "https://openalex.org/W3214578205",
    "https://openalex.org/W2552110825",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W3101567131",
    "https://openalex.org/W3154311556",
    "https://openalex.org/W3099793224",
    "https://openalex.org/W3018647120",
    "https://openalex.org/W4285596748",
    "https://openalex.org/W2804670900",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W644009243",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W4386576703",
    "https://openalex.org/W3199212312",
    "https://openalex.org/W3173954987",
    "https://openalex.org/W4230579319",
    "https://openalex.org/W3164045210",
    "https://openalex.org/W4385570699",
    "https://openalex.org/W3035579820",
    "https://openalex.org/W4385572376",
    "https://openalex.org/W3169244955",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W2804732445",
    "https://openalex.org/W3208076653",
    "https://openalex.org/W3155682407",
    "https://openalex.org/W4287890953",
    "https://openalex.org/W2742113707",
    "https://openalex.org/W3099771192",
    "https://openalex.org/W2329003026",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "Large multilingual pretrained language models (mPLMs) have become the de facto state of the art for cross-lingual transfer in NLP. However, their large-scale deployment to many languages, besides pretraining data scarcity, is also hindered by the increase in vocabulary size and limitations in their parameter budget. In order to boost the capacity of mPLMs to deal with low-resource and unseen languages, we explore the potential of leveraging transliteration on a massive scale. In particular, we explore the UROMAN transliteration tool, which provides mappings from UTF-8 to Latin characters for all the writing systems, enabling inexpensive romanization for virtually any language. We first focus on establishing how UROMAN compares against other language-specific and manually curated transliterators for adapting multilingual PLMs. We then study and compare a plethora of data- and parameter-efficient strategies for adapting the mPLMs to romanized and non-romanized corpora of 14 diverse low-resource languages. Our results reveal that UROMAN-based transliteration can offer strong performance for many languages, with particular gains achieved in the most challenging setups: on languages with unseen scripts and with limited training data without any vocabulary augmentation. Further analyses reveal that an improved tokenizer based on romanized data can even outperform non-transliteration-based methods in the majority of languages.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7996–8005\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nRomanization-based Large-scale Adaptation\nof Multilingual Language Models\nSukannya Purkayastha1, Sebastian Ruder2, Jonas Pfeiffer2, Iryna Gurevych1, Ivan Vuli´c3\n1 Ubiquitous Knowledge Processing Lab,\nDepartment of Computer Science and Hessian Center for AI (hessian.AI),\nTechnical University of Darmstadt\n2 Google DeepMind\n3 Language Technology Lab, University of Cambridge\nwww.ukp.tu-darmstadt.de\nAbstract\nLarge multilingual pretrained language models\n(mPLMs) have become the de facto state of\nthe art for cross-lingual transfer in NLP. How-\never, their large-scale deployment to many lan-\nguages, besides pretraining data scarcity, is also\nhindered by the increase in vocabulary size and\nlimitations in their parameter budget. In or-\nder to boost the capacity of mPLMs to deal\nwith low-resource and unseen languages, we ex-\nplore the potential of leveraging transliteration\non a massive scale. In particular, we explore\nthe UROMAN transliteration tool, which pro-\nvides mappings from UTF-8 to Latin characters\nfor all the writing systems, enabling inexpen-\nsive romanization for virtually any language.\nWe first focus on establishing how UROMAN\ncompares against other language-specific and\nmanually curated transliterators for adapting\nmultilingual PLMs. We then study and com-\npare a plethora of data- and parameter-efficient\nstrategies for adapting the mPLMs to roman-\nized and non-romanized corpora of 14 diverse\nlow-resource languages. Our results reveal that\nUROMAN -based transliteration can offer strong\nperformance for many languages, with partic-\nular gains achieved in the most challenging\nsetups: on languages with unseen scripts and\nwith limited training data without any vocabu-\nlary augmentation. Further analyses reveal that\nan improved tokenizer based on romanized data\ncan even outperform non-transliteration-based\nmethods in the majority of languages.\n1 Introduction\nMassively multilingual language models (mPLMs)\nsuch as mBERT (Devlin et al., 2019) and XLM-R\n(Conneau et al., 2020) have become the driving\nforce for a variety of applications in multilingual\nNLP (Ponti et al., 2020; Hu et al., 2020; Moghe\net al., 2023). However, guaranteeing and main-\ntaining strong performance for a wide spectrum\nof low-resource languages is difficult due to two\ncrucial problems. The first issue is the vocabulary\nFigure 1: Romanization across different languages.\nsize, as the vocabulary is bound to increase with the\nnumber of languages added if per-language perfor-\nmance is to be maintained (Hu et al., 2020; Artetxe\net al., 2020; Pfeiffer et al., 2022). Second, pretrain-\ning mPLMs with a fixed model capacity improves\ncross-lingual performance up to a point after which\nit starts to decrease; this is the phenomenon termed\nthe curse of multilinguality (Conneau et al., 2020).\nTransliteration refers to the process of convert-\ning language represented in one writing system\nto another (Wellisch et al., 1978). Latin script-\ncentered transliteration or romanization is the most\ncommon form of transliteration (Lin et al., 2018;\nAmrhein and Sennrich, 2020; Demirsahin et al.,\n2022) as the Latin/Roman script is by far the most\nwidely adopted writing script in the world (Daniels\nand Bright, 1996; van Esch et al., 2022).1 Adapt-\ning mPLMs via transliteration can address the two\naforementioned critical issues. 1) Since the Latin\nscript covers a dominant portion of the mPLM’s\nvocabulary (e.g., 77% in case of mBERT, see Ács),\n‘romanizing’ the remaining part of the vocabulary\nmight mitigate the vocabulary size issue and boost\nvocabulary sharing. 2) Since no new tokens are\nadded during the romanization process, reusing\npretrained embeddings from the mPLM’s embed-\nding matrix helps reuse the information already\npresent within the mPLM, thereby allocating the\nmodel’s parameter budget more efficiently.\nHowever, the main drawback of transliteration\nseems to be the expensive process of creating ef-\nfective language-specific transliterators, as they\n1According to Encyclopedia Britannica, up to 70% of the\nworld population is employing the Latin script.\n7996\ntypically require language expertise to curate dic-\ntionaries that map tokens from one language and\nscript to another. Therefore, previous attempts at\nmPLM adaptation to unseen languages via translit-\neration (Muller et al., 2021; Chau and Smith, 2021;\nDhamecha et al., 2021; Moosa et al., 2023) were\nconstrained to a handful of languages due to the\nlimited availability of language-specific transliter-\nators, or were applied only to languages that have\n‘language siblings’ with developed transliterators.\nIn this work, unlike previous work, we propose\nto use and then evaluate the usefulness of a uni-\nversal romanization tool, UROMAN (Hermjakob\net al., 2018), for quick, large-scale and effective\nadaptation of mPLMs to low-resource languages.\nThe UROMAN tool disposes of language-specific\ncurated dictionaries and maps any UTF-8 charac-\nter to the Latin script, increasing the portability of\nromanization, with some examples in Figure 1.\nWe analyze language adaptation on a massive\nscale via UROMAN -based romanization on a set of\n14 diverse low-resource languages. We conduct ex-\nperiments within the standard parameter-efficient\nadapter-based cross-lingual transfer setup on two\ntasks: Named Entity Recognition (NER) on the\nWikiANN dataset (Pan et al., 2017; Rahimi et al.,\n2019), and Dependency Parsing (DP) with Univer-\nsal Dependencies v2.7 (Nivre et al., 2020). Our\nkey results suggest that UROMAN -based transliter-\nation can offer strong performance on par or even\noutperforming adaptation with language-specific\ntransliterators, setting up the basis for wider use of\ntransliteration-based mPLM adaptation techniques\nin future work. The gains with romanization-based\nadaptation over standard adaptation baselines are\nparticularly pronounced for languages with unseen\nscripts (∼8-22 performance points) without any\nvocabulary augmentation.2\n2 Background\nWhy UROMAN -Based Romanization? UROMAN -\nbased romanization is not always fully reversible,\nand its usage for transliteration has thus been lim-\nited in the literature. However, due to its high\nportability, UROMAN can help scale the process of\ntransliteration massively and as such benefit low-\nresource scenarios and wider adaptation of mPLMs.\nThe main idea, as hinted in §1, is to (learn to) map\n2Code and data available at https://github.\ncom/UKPLab/emnlp23_romanization_based_\nadaptation\nany UTF-8 character to the Latin script, without the\nuse of any external language-specific dictionaries\n(see Hermjakob et al. (2018) for technical details).\nCross-Lingual Transfer to Low-Resource Lan-\nguages. Parameter-efficient and modular fine-\ntuning methods (Pfeiffer et al., 2023) such as\nadapters (Houlsby et al., 2019; Pfeiffer et al.,\n2020b) have been used for cross-lingual trans-\nfer, putting a particular focus on enabling trans-\nfer to low-resource languages and scenarios, in-\ncluding languages with scripts ‘unseen’ by the\nbase mPLM (Pfeiffer et al., 2021). Adapters are\nsmall lightweight components stitched into the base\nmPLM, and then trained for particular languages\nand tasks while keeping the parameters of the orig-\ninal mPLM frozen. This circumvents the issues\nof catastrophic forgetting and interference (Mc-\nCloskey and Cohen, 1989) within the mPLM, and\nallows for extending its reach also to unseen lan-\nguages (Pfeiffer et al., 2021; Ansell et al., 2021).\nFor our main empirical analyses, we adopt a\nstate-of-the-art modular method for cross-lingual\ntransfer: MAD-X (Pfeiffer et al., 2020b). In short,\nMAD-X is based on language adapters (LA), task\nadapters (TA), and invertible adapters (INV). While\nLAs are trained for specific languages relying on\nmasked language modeling, TAs are trained with\nhigh-resource languages relying on task-annotated\ndata and task-specific objectives. At inference, the\nsource LA is replaced with the target LA while\nthe TA is kept. In order to do parameter-efficient\nlearning for the token-level embeddings across dif-\nferent languages and to deal with the vocabulary\nmismatch between source and target languages,\nPfeiffer et al. (2020b) also propose INV adapters:\nthey are placed on top of the embedding layer and\ntheir inverses precede the output embedding layer.3\nWe adopt the better-performing MAD-X 2.0 setup\n(Pfeiffer et al., 2021) where the adapters in the last\nTransformer layer are dropped at inference.4\n3 Experiments and Results\nAs the main means of analyzing the impact of\ntransliteration in general and UROMAN -based ro-\nmanization in particular, we train different variants\nof language adapters within the MAD-X frame-\nwork, based on transliterated and non-transliterated\n3They are trained together with the LAs while the rest of\nthe mPLM is kept frozen.\n4We refer the reader to the original papers for further tech-\nnical details regarding the MAD-X framework.\n7997\nLanguage Family Script # Sentences\nBhojpuri (bh) Indo-European Devanagari 35,983\nBuryat (bxr) Mongolic Cyrillic 41,692\nErzya (myv) Uralic Cyrilic 42,575\nMeadow Mari (mhr) Uralic Cyrillic 144,529\nMin Dong (cdo) Sino-Tibetan Chinese 33,978\nMingrelian (xmf) Kartvelian Georgian 63,032\nSindhi (sd) Indo-European Arabic 86,176\nSorani Kurdish (ckb) Indo-European Arabic 459,475\nUyghur (ug) Turkic Arabic 149,813\nAmharic (am) Afro-Asiatic Ge’ez 88,320\nDivehi (dv) Indo-European Thaana 34,779\nKhmer (km) Austroasiatic Khmer 139,704\nSinhala (si) Indo-European Sinhala 219,866\nTibetan (bo) Sino-Tibetan Tibetan 131,362\nTable 1: Languages with their ISO 639-3 codes used in\nour evaluation, along with their script, language family,\nand number of sentences available for pretraining. The\ndashed line separates languages with unseen scripts,\nplaced in the bottom part of the table.\nversions of target language data, outlined here.\nVariants with Non-Transliterated Data. For the\nNon-TransLA+INV variant, we train LAs and INV\nadapters together. This variant serves to examine\nthe extent to which mPLMs can adapt to unseen\nlanguages without any vocabulary extension.5 We\ncompare this to Non-TransLA+EmbLex , which trains\na new tokenizer for the target language (Pfeiffer\net al., 2021): the so-called ‘lexically overlapping’\ntokens are initialized with mPLM’s trained embed-\ndings, while the remaining embeddings are initial-\nized randomly. All these embeddings (EmbLex) are\nfine-tuned along with LAs.\nVariants with Transliterated Data.We evaluate a\nTransLA+INV variant, which uses the same setup as\nNon-TransLA+INV but now with transliterated data.\nWe again note that in this efficient setup, we do not\nextend the vocabulary size, and use the fewest train-\nable parameters. In the TransLA+mPLMft variant,\nwe train LAs along with fine-tuning the pretrained\nembeddings of mPLM (mPLMft). This further en-\nhances the model capacity by fine-tuning the em-\nbedding layer instead of using invertible adapters.6\nFor both variants, transliterated data can be pro-\nduced via different transliterators: (i) language-\nspecific ones; (ii) the ones from ‘language siblings’\n(e.g., using a Georgian transliterator for Mingre-\nlian), or (iii) UROMAN .\n5Since LAs without INV typically perform worse than with\nINV (Pfeiffer et al., 2020b), also confirmed in our preliminary\nexperiments, we do not ablate to the setup without INV .\n6We do not have this setup for non-transliterated data since,\nfor languages with unseen scripts, most of the tokens are re-\nplaced by the generic ‘UNK’ token, and fine-tuning embed-\ndings hardly benefit downstream performance.\n3.1 Experimental Setup\nData, Languages and Tasks. Following Pfeiffer\net al. (2021), we select mBERT as our base mPLM.\nWe experiment with 14 typologically diverse low-\nresource languages that are not part of mBERT’s\npretraining corpora, with 5/14 languages written\nin distinct scripts (see Table 1 for details). For LA\ntraining, we use Wikipedia dumps for the target\nlanguages, which we also transliterate (using dif-\nferent transliterators). Evaluation is conducted on\ntwo standard cross-lingual transfer tasks in zero-\nshot setups: 1) the WikiAnn NER dataset (Pan\net al., 2017) with the train, dev, and test splits from\n(Rahimi et al., 2019); 2) for dependency parsing,\nwe rely on the UD Dataset v2.7 (Nivre et al., 2020).\nLAs and TAs. English is the source language in all\nexperiments, and is used for training TAs. The En-\nglish LA is obtained directly from Adapterhub.ml\n(Pfeiffer et al., 2020a), LAs and embeddings (when\nneeded) are only trained for target languages.\nFinally, for the Non-TransLA+EmbLex variant, we\ntrain a WordPiece tokenizer on the target language\ndata with a vocabulary size of 10K.\nTraining of Language and Task Adapters. We\ntrain all the language adapters for 50 epochs or\n∼ 50K update steps based on the corpus size. The\nbatch size is set to 64 and the learning rate is1e−4.\nWe train English task adapters following the\nsetup from (Pfeiffer et al., 2020b). For NER, we di-\nrectly obtain the task adapter from Adapterhub.ml\nwhich is trained with a learning rate of 1e − 4 for\n10 epochs. For DP, we train a Transformer-based\n(Glavaš and Vuli´c, 2021) biaffine attention depen-\ndency parser (Dozat and Manning, 2017). We use\na learning rate of 5e − 4 and train for 10 epochs as\nin (Pfeiffer et al., 2021).\nAll the reported results in both tasks (NER and\nDP) are reported as averages over 6 random seeds.\nAll the models have been trained on A100 or V100\nGPUs. None of the training methods consumed\nmore than 36 hours. As the main means of analyz-\ning the impact of transliteration in general andURO -\nMAN -based romanization in particular, we train\ndifferent variants of language adapters within the\nMAD-X framework, based on transliterated and\nnon-transliterated versions of target language data,\noutlined here.\n3.2 Results and Discussion\nUROMAN versus Other Transliterators and\nTransliteration Strategies. In order to estab-\n7998\nTask Transliterator am ar ka ru hi sd avg\nNER (Macro F1)UROMAN 25.6 24.8 61.4 66.5 48.6 35.3 43.7Other 25.5 23.7 57.3 63.9 56.7 35.9 43.8\nUD (UAS / LAS)UROMAN 36.1 / 6.6 33.0 / 19.8 - 47.3 / 32.4 33.8 /17.8 - 37.5/19.1Other 29.9 / 5.4 32.6 / 19.9 - 45.0 / 19.9 33.2 / 17.9 - 35.2 / 15.8\nTable 2: Comparison of UROMAN with language-specific transliterators.\nSeen Script Unseen Script\nMethod bh cdo ckb mhr sd ug xmf am bo dv km si avg\nUROMAN32.59 27.34 67.73 64.68 35.33 28.10 52.58 25.69 35.95 29.99 41.76 31.83 26.89BORROW53.42 (hi)- 12.46 (ar) 45.86 (ru) 16.79 (ar) 12.85 (ar) 24.77 (ru) - - - - - -RAND 25.42 19.51 53.55 42.02 27.20 25.18 35.82 18.00 18.95 21.19 32.75 20.01 21.59\nTable 3: Comparison of various transliteration strategies on the NER task (Macro-F1).\nSeen Script Unseen Script\nVariant bh cdo sd xmf mhr ckb ug am bo dv km si avg\nNon-TransLA+INV 55.14 24.19 31.31 49.7470.31 45.54 33.53 3.26 19.86 18.72 13.81 23.14 18.39TransLA + INV 32.59 27.34 35.33 52.5864.68 67.73 28.10 25.69 35.95 29.99 41.76 31.83 26.89Non-TransLA+EmbLex 60.00 28.91 42.47 51.99 61.0579.12 50.42 47.60 40.9631.21 53.94 45.89 49.01TransLA + mPLMft 49.05 36.92 39.16 57.99 69.8573.92 33.43 37.09 33.8240.40 52.39 45.24 47.44\nTable 4: Results (Macro-F1 scores) on WikiAnn NER averaged over 6 random seeds.\nSeen Script Unseen Script\nVariant bh myv ug bxr am avg\nNon-TransLA+INV 28.46/11.5345.28 / 26.2733.44/15.28 39.75/19.7719.08 / 1.85 33.20 / 10.81TransLA + INV 25.12 / 10.1745.74/26.6432.30 / 15.10 37.92 / 17.2336.07/7.58 35.43 /12.41Non-TransLA+EmbLex 26.68 / 10.1048.34/25.3441.20 /22.81 39.51/ 16.02 36.47 / 8.39 38.44 / 12.20TransLA + mPLMft 28.04/11.1341.97 / 20.2950.89/ 16.56 35.03 /20.29 39.10/9.00 39.01 /14.65\nTable 5: Results (UAS / LAS scores) in the DP task with UD, averaged over 6 random seeds.\nlish the utility of UROMAN as a viable translit-\nerator, especially for low-resource languages, we\ncompare its performance with transliteration op-\ntions using the TransLA + INV setup as the most ef-\nficient scenario. First, we compare UROMAN with\nlanguage-specific transliterators available for se-\nlected languages: amseg (Yimam et al., 2021) for\nAmharic, ai4bharat-transliteration (Madhani et al.,\n2022) for Hindi and Sindhi, lang-trans for Ara-\nbic, and transliterate for Russian and Georgian 7.\nThe transliterators used in this work are outlined\nin Table 6. The results are provided in Table 2.\nOn average, UROMAN performs better or compa-\nrable to the language-specific transliterators. This\nprovides justification to use UROMAN for massive\ntransliteration at scale.\nSecond, we compare UROMAN to two other\ntransliteration strategies. (i) BORROW refers to\nborrowing transliterators from languages within\nthe same language family and written in the same\nscript.8 Since building transliterators are costly,\nthis gives us an estimate of whether it is possible to\nrely on the related transliterators when we do not\nhave a language-specific one at hand. (ii) RAND\n7For reproducibility, the links to the language-specific\ntransliterators are available in Appendix A\n8E.g., a Hindi transliterator can be borrowed for Bhojpuri\nsince the two are related and written in Devanagari.\nrefers to a random setting where we associate any\nnon-ASCII character with any ASCII character,\ngiving us an estimate of whether we actually need\nknowledge of the language to build transliterators.\nThe results are provided in Table 3: UROMAN is\nlargely and consistently outperforming both BOR-\nROW and RAND, where the single exception is\nBORROW (from Hindi to Bhojpuri). Surprisingly,\nRAND also yields reasonable performance and\non average even outperforms the Non-TransLA+INV\nvariant with non-transliterated data (21.59 vs 18.39\nin Table 4 later). This provides further evidence\ntowards the utility of transliteration in general and\nUROMAN -based romanization in particular to assist\nand improve language adaptation.\nPerformance on Low-Resource Languages is\nsummarized in Table 4 and Table 5.9 We note that\nTransLA+INV outperforms Non-TransLA+INV for all\nthe languages with unseen scripts, and achieves\nthat with huge margins (∼ 8-22 points for NER and\n∼ 17 points in UAS scores). We observe similar\ntrends for some of the languages with seen scripts\nsuch as Min Dong (cdo), Sindhi (sd), Mingrelian\n9We also compare the performance of these methods with\nthe standard cross-lingual transfer setup for finetuning an out-\nof-the-box mBERT for languages with unseen scripts in Ap-\npendix B. All the adapter-based methods massively outper-\nform an out-of-the-box mBERT in this scenario.\n7999\n100 1000 10000 Full\nNumber of sentences for MLM\n0\n10\n20\n30\n40\n50F1\nVariant\nTransLA+INV\nNon-TransLA+EmbLex\nScript\nAll\nUnseen\nFigure 2: Sample efficiency in the NER task.\n100 1000 10000 Full\nNumber of sentences for MLM\n20\n30UAS TransLA+INV\nNon-TransLA+EmbLex\nFigure 3: Sample efficiency in the DP task.\n(xmf) on NER tasks and Erzya (myv) on DP. The\nless efficient TransLA+mPLMft , as expected, further\nimproves the performance for all the languages\nexcept for Tibetan (bo). 10 Non-TransLA+EmbLex ,\nhowever, now outperforms UROMAN -based meth-\nods for a majority of the languages. This ob-\nservation can be attributed to various factors re-\nlated to mBERT’s tokenizer, and we provide an\nin-depth analysis later in Appendix C. Nonetheless,\nwe observe strong and competitive performance\nof TransLA + mPLMft in both tasks, again indicating\nthat more attention should be put on transliteration-\nbased language adaptation in future work.\nSample Efficiency. Finally, we simulate a few-shot\nsetup to study the effectiveness of using transliter-\nated versus non-transliterated data in data-scarce\nscenarios. For NER, we evaluate performance on\nall the languages and on languages with unseen\nscripts; for DP, we evaluate on all the languages.\nFigure 2 indicates that TransLA+INV on average per-\nforms better than all the other methods at sample\nsizes 100 (i.e., 100 sentences in the target language)\nand 1, 000. However, from 10, 000 sentences on-\nward, Non-TransLA+EmbLex takes the lead. We ob-\nserve similar trends in the DP task (see Fig 3).\nThis establishes the utility of transliteration for (ex-\ntremely) low-resource scenarios.\n10For Tibetan, longer words are composed using shorter\nwords separated by tsek (“.”) which is not a valid space de-\nlimiter for the mBERT tokenizer; the number of produced\nsubwords is thus much higher than for the other languages.\n4 Conclusion\nIn this work, we have systematically analyzed and\nconfirmed the potential of romanization, imple-\nmented via the UROMAN tool, to help with adap-\ntation of multilingual pretrained language models.\nGiven (i) its broad applicability and (ii) strong per-\nformance overall and for languages with unseen\nscripts, we hope our study will inspire more work\non transliteration-based adaptation.\nLimitations\nIn this paper, we work with UROMAN (Hermjakob\net al., 2018) which is an unsupervised romaniza-\ntion tool. While it is an effective tool for roman-\nization at scale, it still has potential drawbacks.\nSince it is only based on lexical substitution, its\ntransliterations may not semantically or phoneti-\ncally align with the source content and may dif-\nfer from transliterations preferred by native speak-\ners. Moreover, UROMAN is not invertible—as\nwe have highlighted—and may thus be less ap-\npealing when text in the original script needs to\nbe exactly reproduced. Our proposed method,\nwhile it is parameter-efficient and effective—\nparticularly for low-resource languages—still\nunderperforms language-specific tokenizer-based\nnon-transliteration methods. Future work may fo-\ncus on developing an improved and more efficient\ntokenizer for transliteration-based methods as we\nhighlight in the Appendix.\nWhile there is now a growing body of available\nevaluation resources for low-resource languages\n(Ebrahimi et al., 2022; Mhaske et al., 2023; Winata\net al., 2023, among others), our final selection of\ntasks, resources and languages has been driven and\nconstrained by the specific concrete goal of our\nshort paper: studying and evaluating if and how\ntransliteration/romanization can help with adapta-\ntion of languages with scripts unseen by the pre-\ntrained multilingual language model. We thus\nclosely follow the experimental setup of Pfeiffer\net al. (2021) which used the same set of tasks and\nlanguages with unseen scripts.\nFinally, romanization can be seen as a step to-\nwards providing more universal, or rather language-\nagnostic, input text representation. Full-fledged\ncomparisons against other approaches that aim\nto strike language independence at the input or\nfeature level, such as byte-level models (e.g.,\nByT5 (Xue et al., 2022)) and pixel-based mod-\nels (e.g., PIXEL (Rust et al., 2023)) go beyond\n8000\nthe scope of this particular work, but we point\nout to this as a very interesting future research av-\nenue. Moreover, the integration of these language-\nagnostic representations with ‘romanization’-based\napproaches might yield additional benefits, and\nshould also be attested in future research.\nAcknowledgements\nThis work has been funded by the German Re-\nsearch Foundation (DFG) as part of the Research\nTraining Group KRITIS No. GRK 2222. The work\nof Ivan Vuli ´c has been supported by a personal\nRoyal Society University Research Fellowship (no\n221137; 2022-).\nWe thank Indraneil Paul, Yongxin Huang, Ivan\nHabernal, and Massimo Nicosia for their valuable\nfeedback and suggestions on a draft of this paper.\nReferences\nChantal Amrhein and Rico Sennrich. 2020. On Roman-\nization for model transfer between scripts in neural\nmachine translation. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n2461–2469, Online. Association for Computational\nLinguistics.\nAlan Ansell, Edoardo Maria Ponti, Jonas Pfeiffer, Se-\nbastian Ruder, Goran Glavaš, Ivan Vuli´c, and Anna\nKorhonen. 2021. MAD-G: Multilingual adapter gen-\neration for efficient cross-lingual transfer. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2021, pages 4762–4781, Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 4623–4637, Online. Association\nfor Computational Linguistics.\nEthan C. Chau and Noah A. Smith. 2021. Specializing\nmultilingual language models: An empirical study.\nIn Proceedings of the 1st Workshop on Multilingual\nRepresentation Learning, pages 51–61, Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nPeter T. Daniels and William Bright. 1996. The world’s\nwriting systems. Oxford University Press.\nIsin Demirsahin, Cibu Johny, Alexander Gutkin, and\nBrian Roark. 2022. Criteria for useful automatic\nRomanization in South Asian languages. In Pro-\nceedings of the Thirteenth Language Resources and\nEvaluation Conference, pages 6662–6673, Marseille,\nFrance. European Language Resources Association.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nTejas Dhamecha, Rudra Murthy, Samarth Bharad-\nwaj, Karthik Sankaranarayanan, and Pushpak Bhat-\ntacharyya. 2021. Role of Language Relatedness in\nMultilingual Fine-tuning of Language Models: A\nCase Study in Indo-Aryan Languages. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 8584–8595,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nTimothy Dozat and Christopher D. Manning. 2017.\nDeep biaffine attention for neural dependency pars-\ning. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings. Open-\nReview.net.\nAbteen Ebrahimi, Manuel Mager, Arturo Oncevay,\nVishrav Chaudhary, Luis Chiruzzo, Angela Fan, John\nOrtega, Ricardo Ramos, Annette Rios, Ivan Vladimir\nMeza Ruiz, Gustavo Giménez-Lugo, Elisabeth\nMager, Graham Neubig, Alexis Palmer, Rolando\nCoto-Solano, Thang Vu, and Katharina Kann. 2022.\nAmericasNLI: Evaluating zero-shot natural language\nunderstanding of pretrained multilingual models in\ntruly low-resource languages. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n6279–6299, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nGoran Glavaš and Ivan Vuli´c. 2021. Is supervised syn-\ntactic parsing beneficial for language understanding\ntasks? an empirical investigation. In Proceedings\nof the 16th Conference of the European Chapter of\nthe Association for Computational Linguistics: Main\nVolume, pages 3090–3104, Online. Association for\nComputational Linguistics.\nUlf Hermjakob, Jonathan May, and Kevin Knight. 2018.\nOut-of-the-box universal Romanization tool uroman.\nIn Proceedings of ACL 2018, System Demonstrations,\npages 13–18, Melbourne, Australia. Association for\nComputational Linguistics.\n8001\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In Pro-\nceedings of the 36th International Conference on Ma-\nchine Learning, ICML 2019, 9-15 June 2019, Long\nBeach, California, USA, volume 97 of Proceedings\nof Machine Learning Research , pages 2790–2799.\nPMLR.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual gener-\nalisation. In Proceedings of the 37th International\nConference on Machine Learning, ICML 2020, 13-18\nJuly 2020, Virtual Event, volume 119 of Proceedings\nof Machine Learning Research , pages 4411–4421.\nPMLR.\nYing Lin, Cash Costello, Boliang Zhang, Di Lu, Heng\nJi, James Mayfield, and Paul McNamee. 2018. Plat-\nforms for non-speakers annotating names in any lan-\nguage. In Proceedings of ACL 2018, System Demon-\nstrations, pages 1–6, Melbourne, Australia. Associa-\ntion for Computational Linguistics.\nYash Madhani, Sushane Parthan, Priyanka A. Bedekar,\nRuchi Khapra, Vivek Seshadri, Anoop Kunchukut-\ntan, Pratyush Kumar, and Mitesh M. Khapra. 2022.\nAksharantar: Towards building open transliteration\ntools for the next billion users. arXiv preprint.\nMichael McCloskey and Neal J. Cohen. 1989. Catas-\ntrophic interference in connectionist networks: The\nsequential learning problem. volume 24 of Psychol-\nogy of Learning and Motivation, pages 109–165. Aca-\ndemic Press.\nArnav Mhaske, Harshit Kedia, Sumanth Doddapaneni,\nMitesh M. Khapra, Pratyush Kumar, Rudra Murthy,\nand Anoop Kunchukuttan. 2023. Naamapadam: A\nlarge-scale named entity annotated data for Indic lan-\nguages. In Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 10441–10456, Toronto,\nCanada. Association for Computational Linguistics.\nNikita Moghe, Evgeniia Razumovskaia, Liane Guillou,\nIvan Vuli´c, Anna Korhonen, and Alexandra Birch.\n2023. Multi3NLU++: A multilingual, multi-intent,\nmulti-domain dataset for natural language under-\nstanding in task-oriented dialogue. In Findings of\nthe Association for Computational Linguistics: ACL\n2023, pages 3732–3755, Toronto, Canada. Associa-\ntion for Computational Linguistics.\nIbraheem Muhammad Moosa, Mahmud Elahi Akhter,\nand Ashfia Binte Habib. 2023. Does transliteration\nhelp multilingual language modeling? In Findings\nof the Association for Computational Linguistics:\nEACL 2023, pages 670–685, Dubrovnik, Croatia. As-\nsociation for Computational Linguistics.\nBenjamin Muller, Antonios Anastasopoulos, Benoît\nSagot, and Djamé Seddah. 2021. When being un-\nseen from mBERT is just the beginning: Handling\nnew languages with multilingual language models.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 448–462, Online. Association for Computa-\ntional Linguistics.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Jan Haji ˇc, Christopher D. Manning, Sampo\nPyysalo, Sebastian Schuster, Francis Tyers, and\nDaniel Zeman. 2020. Universal Dependencies v2:\nAn evergrowing multilingual treebank collection. In\nProceedings of the Twelfth Language Resources and\nEvaluation Conference, pages 4034–4043, Marseille,\nFrance. European Language Resources Association.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel Noth-\nman, Kevin Knight, and Heng Ji. 2017. Cross-lingual\nname tagging and linking for 282 languages. In Pro-\nceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 1946–1958, Vancouver, Canada. As-\nsociation for Computational Linguistics.\nJonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James\nCross, Sebastian Riedel, and Mikel Artetxe. 2022.\nLifting the curse of multilinguality by pre-training\nmodular transformers. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3479–3495, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nJonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya\nKamath, Ivan Vuli´c, Sebastian Ruder, Kyunghyun\nCho, and Iryna Gurevych. 2020a. AdapterHub: A\nframework for adapting transformers. In Proceedings\nof the 2020 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations,\npages 46–54, Online. Association for Computational\nLinguistics.\nJonas Pfeiffer, Sebastian Ruder, Ivan Vuli ´c, and\nEdoardo Maria Ponti. 2023. Modular deep learning.\narXiv preprint.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020b. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654–7673, Online. Association for Computa-\ntional Linguistics.\nJonas Pfeiffer, Ivan Vuli´c, Iryna Gurevych, and Sebas-\ntian Ruder. 2021. UNKs everywhere: Adapting mul-\ntilingual language models to new scripts. In Proceed-\nings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 10186–10203,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\n8002\nEdoardo Maria Ponti, Goran Glavaš, Olga Majewska,\nQianchu Liu, Ivan Vuli´c, and Anna Korhonen. 2020.\nXCOPA: A multilingual dataset for causal common-\nsense reasoning. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 2362–2376, Online. As-\nsociation for Computational Linguistics.\nAfshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Mas-\nsively multilingual transfer for NER. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 151–164, Florence,\nItaly. Association for Computational Linguistics.\nPhillip Rust, Jonas F. Lotz, Emanuele Bugliarello, Eliz-\nabeth Salesky, Miryam de Lhoneux, and Desmond\nElliott. 2023. Language modelling with pixels. In\nThe Eleventh International Conference on Learning\nRepresentations, ICLR 2023, Kigali, Rwanda, May\n1-5, 2023. OpenReview.net.\nPhillip Rust, Jonas Pfeiffer, Ivan Vuli´c, Sebastian Ruder,\nand Iryna Gurevych. 2021. How good is your tok-\nenizer? on the monolingual performance of multilin-\ngual language models. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 3118–3135, Online. Association\nfor Computational Linguistics.\nDaan van Esch, Tamar Lucassen, Sebastian Ruder, Isaac\nCaswell, and Clara Rivera. 2022. Writing system and\nspeaker metadata for 2,800+ language varieties. In\nProceedings of the Thirteenth Language Resources\nand Evaluation Conference, pages 5035–5046, Mar-\nseille, France. European Language Resources Asso-\nciation.\nHans H Wellisch, Lee Breuer, Richard Foreman, and\nRobert Wilson. 1978. The conversion of scripts, its\nnature, history, and utilization. New York; Toronto:\nWiley.\nGenta Indra Winata, Alham Fikri Aji, Samuel Cahyawi-\njaya, Rahmad Mahendra, Fajri Koto, Ade Romad-\nhony, Kemal Kurniawan, David Moeljadi, Radi-\ntyo Eko Prasojo, Pascale Fung, Timothy Baldwin,\nJey Han Lau, Rico Sennrich, and Sebastian Ruder.\n2023. NusaX: Multilingual parallel sentiment dataset\nfor 10 Indonesian local languages. In Proceedings\nof the 17th Conference of the European Chapter of\nthe Association for Computational Linguistics, pages\n815–834, Dubrovnik, Croatia. Association for Com-\nputational Linguistics.\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-\nRfou, Sharan Narang, Mihir Kale, Adam Roberts,\nand Colin Raffel. 2022. ByT5: Towards a token-free\nfuture with pre-trained byte-to-byte models. Transac-\ntions of the Association for Computational Linguis-\ntics, 10:291–306.\nSeid Muhie Yimam, Abinew Ali Ayele, Gopalakrish-\nnan Venkatesh, Ibrahim Gashaw, and Chris Bie-\nmann. 2021. Introducing various semantic models\nfor amharic: Experimentation and evaluation with\nmultiple tasks and datasets. Future Internet, 13(11).\nJudit Ács. Exploring bert’s vocabulary. Blog Post.\n8003\nA Transliterators in Evaluation\nBesides UROMAN , we also employ various\nlanguage-specific transliterators which are publicly\navailable. We list them in Table 6.\nB Performance comparison of mBERT\nWe adapt the standard cross-lingual transfer setup\nfor mBERT. The model is finetuned on the task\ndata for a source language (high-resource) and is\nused to perform inference on the target language\n(low-resource). We report the performance com-\nparison of the standard cross-lingual transfer setup\nfor mBERT on the NER task for languages with\nunseen scripts with the adapter-based methods in\nTable 7. We observe that the adapter-based meth-\nods outperform mBERT by huge margins.\nC Further Analyses\nFollowing previous work (Ács; Rust et al., 2021;\nMoosa et al., 2023), we further analyze tokeniza-\ntion quality of the mBERT tokenizer using the fol-\nlowing established metrics: 1) % of “UNK”s mea-\nsures the % of “UNK” tokens produced by the\ntokenizer, and our aim is to compare their rate\nbefore and after transliteration; 2) fertility mea-\nsures the number of subwords that are produced\nper tokenized word; 3) proportion of continued\nsubwords measures the proportion of words for\nwhich the tokenized word is split across at least\ntwo subwords (denoted by the symbol ##).\nFrom the results summarized in Figure 4, it is\napparent that transliteration drastically reduces %\nof UNKs. However, mBERT’s tokenizer under-\nperforms as compared to monolingual tokenizers\nbased on fertility and the proportion of continued\nsubwords (Rust et al., 2021). Transliteration per-\nforms better for some languages where the quality\nof the mBERT tokenizer is similar to the monolin-\ngual tokenizer such as for dv, km, and cdo. On the\nother hand, transliteration methods perform worse\non languages where the quality of the underlying\nmBERT tokenizer is relatively poor.\nIn order to test the hypothesis that the tokenizer\nquality might be the principal reason for the perfor-\nmance gap for the transliteration-based methods in\ncomparison to the non-transliteration based meth-\nods, we carried out an additional experiment. For\nthe experiment, we adapt the Non-TransLA+EmbLex\nto operate on transliterated data, and call this vari-\nant TransLA+EmbLex . Here, we train a new tokenizer\non the transliterated data and initialize lexically\noverlapping embeddings with mBERT’s pretrained\nembeddings.\nWe plot the performance in Figure 5. The new\nmethod, TransLA+EmbLex now outperforms the non-\ntransliteration-based variant on 8/12 languages and\nalso on average. Consequently, this validates our\nhypothesis and is in line with the previous work\n(Moosa et al., 2023). However, we found a drop\nin performance in the case of mhr (-10.71) and\ncdo (-10.14) when compared to Trans LA + mPLMft .\nThese drops may be attributed to the lower degree\nof lexical overlap with mBERT’s vocabulary, and\nconsequently a higher number of randomly initial-\nized embeddings for those target languages.\n8004\nTransliterator Used for languages Available at\nUROMAN All github.com/isi-nlp/uroman\namseg am pypi.org/project/amseg/\ntransliterate ru, ka pypi.org/project/transliterate/\nai4bharat-transliteration hi, sd pypi.org/project/ai4bharat-transliteration/\nlang-trans ar pypi.org/project/lang-trans/\nTable 6: Transliterators used in this work.\n(a)\n (b)\n (c)\nFigure 4: Tokenizer quality analysis. a) % of UNKs before and after transliteration, b) Fertility, and c) Proportion of\ncontinued subwords for mBERT vs monolingual tokenizer.\nUnseen Scripts\nVariant am bo dv km si\nmBERT 0.91 17.40 1.30 10.71 2.50Non-TransLA+INV 3.26 19.86 18.72 13.81 23.14TransLA + INV 25.69 35.95 29.99 41.76 31.83Non-TransLA+EmbLex 47.60 40.9631.2153.94 45.89TransLA + mPLMft 37.09 33.8240.4052.39 45.24\nTable 7: Performance Comparison of mBERT with\nadapter-based finetuning methods for unseen scripts on\nthe NER task.\nFigure 5: Comparison of Non-Trans LA+EmbLex with\nTransLA+EmbLex on NER (left) and DP (right).\n8005",
  "topic": "Romanization",
  "concepts": [
    {
      "name": "Romanization",
      "score": 0.874360203742981
    },
    {
      "name": "Computer science",
      "score": 0.8615109920501709
    },
    {
      "name": "Transliteration",
      "score": 0.7770898342132568
    },
    {
      "name": "Natural language processing",
      "score": 0.7031777501106262
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6230854988098145
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.5101637840270996
    },
    {
      "name": "Vocabulary",
      "score": 0.4606613516807556
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4532545208930969
    },
    {
      "name": "Machine translation",
      "score": 0.43705514073371887
    },
    {
      "name": "Scripting language",
      "score": 0.4295969307422638
    },
    {
      "name": "Language model",
      "score": 0.4228357970714569
    },
    {
      "name": "Focus (optics)",
      "score": 0.41802603006362915
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.41378363966941833
    },
    {
      "name": "Linguistics",
      "score": 0.20439007878303528
    },
    {
      "name": "Programming language",
      "score": 0.09317123889923096
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I31512782",
      "name": "Technical University of Darmstadt",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I16718484",
      "name": "Hess (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210090411",
      "name": "DeepMind (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    }
  ]
}