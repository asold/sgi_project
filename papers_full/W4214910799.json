{
  "title": "Homo–Heterogenous Transformer Learning Framework for RS Scene Classification",
  "url": "https://openalex.org/W4214910799",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2096061300",
      "name": "Jing-Jing Ma",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2280604264",
      "name": "Mingteng Li",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A1978905009",
      "name": "Xu Tang",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2130877461",
      "name": "Xiangrong Zhang",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2099661209",
      "name": "Fang Liu",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2166558591",
      "name": "Licheng Jiao",
      "affiliations": [
        "Xidian University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2515866431",
    "https://openalex.org/W2592962403",
    "https://openalex.org/W2070452328",
    "https://openalex.org/W4230714977",
    "https://openalex.org/W2767814567",
    "https://openalex.org/W3022140654",
    "https://openalex.org/W3181391941",
    "https://openalex.org/W3042481550",
    "https://openalex.org/W2590149631",
    "https://openalex.org/W3129169341",
    "https://openalex.org/W4205485113",
    "https://openalex.org/W4206470192",
    "https://openalex.org/W2161969291",
    "https://openalex.org/W2153425333",
    "https://openalex.org/W2132633146",
    "https://openalex.org/W1980038761",
    "https://openalex.org/W6675402254",
    "https://openalex.org/W2077689834",
    "https://openalex.org/W1526295910",
    "https://openalex.org/W2140494000",
    "https://openalex.org/W2125283600",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2962217138",
    "https://openalex.org/W3044492802",
    "https://openalex.org/W2886702754",
    "https://openalex.org/W2945385604",
    "https://openalex.org/W3043183554",
    "https://openalex.org/W2939570633",
    "https://openalex.org/W3168997536",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3204166336",
    "https://openalex.org/W3204650100",
    "https://openalex.org/W3203480968",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W2253590344",
    "https://openalex.org/W3023931079",
    "https://openalex.org/W2948329096",
    "https://openalex.org/W2974770574",
    "https://openalex.org/W3120080208",
    "https://openalex.org/W3113299742",
    "https://openalex.org/W3015063979",
    "https://openalex.org/W2783165089",
    "https://openalex.org/W3138758234",
    "https://openalex.org/W2999338704",
    "https://openalex.org/W3045603631",
    "https://openalex.org/W3088971399",
    "https://openalex.org/W3125493368",
    "https://openalex.org/W3039010446",
    "https://openalex.org/W2944971001",
    "https://openalex.org/W3119125170",
    "https://openalex.org/W2974373385",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W6779879114",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6794345597",
    "https://openalex.org/W6790690058",
    "https://openalex.org/W3112157188",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W3128592650",
    "https://openalex.org/W2118246710",
    "https://openalex.org/W3188086824",
    "https://openalex.org/W3119804198",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2919352650",
    "https://openalex.org/W2829067510",
    "https://openalex.org/W2969689033",
    "https://openalex.org/W3155649272",
    "https://openalex.org/W2992382424",
    "https://openalex.org/W2940939359",
    "https://openalex.org/W3155804464",
    "https://openalex.org/W3196798006",
    "https://openalex.org/W4214636423",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3104355817",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W3100892511",
    "https://openalex.org/W3105577662",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W3101627915"
  ],
  "abstract": "Remote sensing (RS) scene classification plays an essential role in the RS community and has attracted increasing attention due to its wide applications. Recently, benefiting from the powerful feature learning capabilities of convolutional neural networks (CNNs), the accuracy of the RS scene classification has significantly been improved. Although the existing CNN-based methods achieve excellent results, there is still room for improvement. First, the CNN-based methods are adept at capturing the global information from RS scenes. Still, the context relationships hidden in RS scenes cannot be thoroughly mined. Second, due to the specific structure, it is easy for normal CNNs to exploit the heterogenous information from RS scenes. Nevertheless, the homogenous information, which is also crucial to comprehensively understand complex contents within RS scenes, does not get the attention it deserves. Third, most CNNs focus on establishing the relationships between RS scenes and semantic labels. However, the similarities between them are not considered deeply, which are helpful to distinguish the intra-/interclass samples. To overcome the limitations mentioned previously, we propose a homo&#x2013;heterogenous transformer learning (HHTL) framework for the RS scene classification in this article. First, a patch generation module is designed to generate homogenous and heterogenous patches. Then, a dual-branch feature learning module (FLM) is proposed to mine homogenous and heterogenous information within RS scenes simultaneously. In the FLM, based on vision transformer, not only the global information but also the local areas and their context information can be captured. Finally, we design a classification module, which consists of a fusion submodule and a metric-learning module. It can integrate homo&#x2013;heterogenous information and compact/separate samples from the same/different RS scene categories. Extensive experiments are conducted on four public RS scene datasets. The encouraging results demonstrate that our HHTL framework can outperform many state-of-the-art methods. Our source codes are available at the below website.",
  "full_text": "IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022 2223\nHomo–Heterogenous Transformer Learning\nFramework for RS Scene Classiﬁcation\nJingjing Ma, Member , IEEE, Mingteng Li, Xu Tang , Senior Member , IEEE,\nXiangrong Zhang , Senior Member , IEEE,F a n gL i u, Member , IEEE, and Licheng Jiao , Fellow, IEEE\nAbstract—Remote sensing (RS) scene classiﬁcation plays an\nessential role in the RS community and has attracted increasing\nattention due to its wide applications. Recently, beneﬁting from\nthe powerful feature learning capabilities of convolutional neural\nnetworks (CNNs), the accuracy of the RS scene classiﬁcation has\nsigniﬁcantly been improved. Although the existing CNN-based\nmethods achieve excellent results, there is still room for improve-\nment. First, the CNN-based methods are adept at capturing the\nglobal information from RS scenes. Still, the context relationships\nhidden in RS scenes cannot be thoroughly mined. Second, due\nto the speciﬁc structure, it is easy for normal CNNs to exploit\nthe heterogenous information from RS scenes. Nevertheless, the\nhomogenous information, which is also crucial to comprehensively\nunderstand complex contents within RS scenes, does not get the\nattention it deserves. Third, most CNNs focus on establishing the\nrelationships between RS scenes and semantic labels. However,\nthe similarities between them are not considered deeply, which\nare helpful to distinguish the intra-/interclass samples. To over-\ncome the limitations mentioned previously, we propose a homo–\nheterogenous transformer learning (HHTL) framework for the RS\nscene classiﬁcation in this article. First, a patch generation module\nis designed to generate homogenous and heterogenous patches.\nThen, a dual-branch feature learning module (FLM) is proposed to\nmine homogenous and heterogenous information within RS scenes\nsimultaneously. In the FLM, based on vision transformer, not only\nthe global information but also the local areas and their context\ninformation can be captured. Finally, we design a classiﬁcation\nmodule, which consists of a fusion submodule and a metric-learning\nManuscript received December 1, 2021; revised January 25, 2022; accepted\nFebruary 25, 2022. Date of publication March 3, 2022; date of current version\nMarch 21, 2022. This work was supported in part by the National Natural\nScience Foundation of China under Grant 62171332, in part by the Key\nResearch and Development Program of Shaanxi under Grant 2021GY-035\nand Grant 2019ZDLGY03-08, in part by the Fund of National Key Labo-\nratory of Science and Technology on Remote Sensing Information and Im-\nagery Analysis, Beijing Research Institute of Uranium Geology under Grant\n6142A010301, in part by China Postdoctoral Science Foundation Funded Project\nunder Grant 2017M620441, in part by Key Laboratory Program under Grant\nHLJGXQ20210701008, and in part by Open Research Fund in 2021 of Jiangsu\nKey Laboratory of Spectral Imaging Intelligent Sense under Grant JSGP202101.\n(Corresponding author: Xu Tang.)\nJingjing Ma, Mingteng Li, Xu Tang, Xiangrong Zhang, and Licheng Jiao\nare with the Key Laboratory of Intelligent Perception and Image Under-\nstanding of Ministry of Education, School of Artiﬁcial Intelligence, Xidian\nUniversity, Xi’an 710071, China (e-mail: jjma@ieee.org; 1983864344@qq.\ncom; tangxu128@gmail.com; xrzhang@mail.xidian.edu.cn; lchjiao@mail.\nxidian.edu.cn).\nFang Liu is with the Key Laboratory of Intelligent Perception and Systems\nfor High-Dimensional Information of Ministry of Education, School of Com-\nputer Science and Engineering, Nanjing University of Science and Technology,\nNanjing 210094, China (e-mail: liufang_cs@njust.edu.cn).\nThis article has supplementary downloadable material available at https://doi.\norg/10.1109/JSTARS.2022.3155665, provided by the authors.\nDigital Object Identiﬁer 10.1109/JSTARS.2022.3155665\nmodule. It can integrate homo–heterogenous information and com-\npact/separate samples from the same/different RS scene categories.\nExtensive experiments are conducted on four public RS scene\ndatasets. The encouraging results demonstrate that our HHTL\nframework can outperform many state-of-the-art methods. Our\nsource codes are available at the below website.\nIndex Terms —Homo–heterogenous transformer, metric\nlearning, remote sensing (RS) scene classiﬁcation.\nI. I NTRODUCTION\nW\nITH the rapid development of remote sensing (RS) data\nacquisition technologies, a large number of RS images\nare produced every day. They contain a lot of useful information,\nand how to use this much data to study our planet stands in\nthe way for researchers. As a basic task in the RS community,\nRS scene classiﬁcation receives growing attention recently. By\nassigning semantic labels to different scenes according to their\ncontents [1], it can be used in a wide range of applications,\nsuch as urban planning [2], land resource management [3],\nagriculture [4], forestry [5], etc.\nDuring the last decades, an ocean of successful methods\nhas been proposed for RS scene classiﬁcation [6]–[12]. Before\nthe prosperity of deep learning (DL) techniques, most of the\nproposed methods contain a feature extractor and a classiﬁer.\nThe feature extractor focuses on ﬁnding a proper feature space\nfor RS scenes, while the classiﬁer aims at grouping scenes into\ndifferent semantic classes. For feature extractors, hand-crafted\nvisual features are popular since they are easy to accomplish and\nstable in performance. Those feature descriptors can be divided\ninto low- and midlevel representations. The popular low-level\nfeatures are histogram of oriented gradients (HOG) [13], scale-\ninvariant feature transform (SIFT) [14], and local binary pattern\n(LBP) [15]. They are good at capturing key points, texture,\nand shape information from RS scenes. The famous midlevel\nfeatures are bag-of-visual-word (BoVW) [16], vector of locally\naggregated descriptors (VLAD) [17], and so on. They usually\nencode low-level features in a particular manner to construct\nthe new ones [18], [19]. For classiﬁers, the statistic theory and\nmachine learning-based tools, such as support vector machine\n(SVM) [20] and decision tree [21], are popular due to their\nstrict mathematical deduction and complete theoretical foun-\ndation. Although the combinations of low-/midlevel features\n[Online]. Available: http://github.com/TangXu-Group/Remote-Sensing-\nImages-Classiﬁcation/tree/main/HHTL\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2224 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nand classical classiﬁers complete RS scene classiﬁcation tasks\nwell, they still cannot get what we expect due to the complex\ncontents within RS scenes, especially with the spatial resolution\nincreases.\nWhen DL techniques boom, they bring many opportunities to\nthe RS scene classiﬁcation. Among diverse DL networks, convo-\nlutional neural networks (CNNs) are the most eye catching. Due\nto the hierarchical structure and speciﬁc learning manner, CNNs\ncan learn semantic information from RS scenes and accomplish\nthe classiﬁcation at the same time, which not only improves\nthe feature representation but also simpliﬁes the categorization\nprocess. Many general-purpose CNNs have been proposed, such\nas VGG-Net [22] and Residual Network (ResNet) [23], and\ntheir pretrained versions achieve cracking results in RS scene\nclassiﬁcation tasks [24], [25]. Also, abundant RS-oriented CNNs\nhave been developed, and they perform perfectly in both RS\nscene classiﬁcation, and other RS applications [26]–[28].\nAlthough CNN-based models have signiﬁcantly improved RS\nscene classiﬁcation accuracy, some limitations are still needed to\nbe noticed. First, they are good at exploiting global information\nfrom RS scenes. However, the context information hidden in\nRS scenes cannot be fully captured, which is essential to deﬁne\nthe semantic categories of different RS scenes. Second, since\nthe land covers in RS images are diverse in type and huge in\nvolume, the regions covered by the normal convolutional kernels\n(square shape) contain many various land covers. Therefore,\nordinary CNNs pay more attention to the regions with het-\nerogenous information instinctively [29], [30]. Nevertheless, it\nis not easy for CNNs to explore the information within homoge-\nnous regions, which can help models further understand the\ncontents of different scenes. Third, most CNNs aim to bridge\nthe corresponding relations between RS scenes and semantic\nlabels through a mass of training samples. However, distances\nbetween inter-/intraclass samples, which are crucial in pattern\nrecognition, are not considered.\nIn this article, we propose a new method for the RS scene\nclassiﬁcation, named homo–heterogenous transformer learning\n(HHTL) framework, to overcome limitations mentioned previ-\nously. It contains a patch generation module (PGM), a feature\nlearning module (FLM), and a classiﬁcation module. The PGM\nconsists of a heterogenous patch generation submodule (HPGM)\nand an adaptive homogenous patch generation submodule (AH-\nPGM). They aim to divide the RS scenes into heterogenous and\nhomogenous patches, respectively. The FLM is a dual-channel\nnetwork embedded by vision transformer [31]. On the one hand,\nboth global and local context information can be learned by\nvision transformer effectively. On the other hand, the knowledge\nwithin homogenous and heterogenous patches can be mined\nsimultaneously. In the classiﬁcation module, we introduce the\nmetric learning strategy into our model and develop a new\nloss function to integrate the contributions of homogenous and\nheterogenous features and compact/disperse the intra-/interclass\nsamples.\nThe CNN and transformer have their own characteristics.\nIn this article, we select transformer instead of normal CNNs\nas the feature extractor. The exact reasons are summarized as\nfollows. First, the transformer is a model that relies on the\nattention mechanism to model global dependencies between\ninput and output [32]. It can seem like the enhanced CNN.\nDue to the speciﬁc structure of CNNs, which is stacked by\nmultiple convolutional kernels, they pay more attention to the\nlocal information within RS scenes [33]–[35]. In other words,\na convolution layer only models the relationships between\nneighboring pixels. The transformer can not only capture local\ninformation from the RS scene by linear mapping each patch\nbut also explore the global context knowledge hidden in the RS\nscene by the self-attention mechanism [31]. A transformer layer\ndirectly models the relationships between all pixels, thereby\nextracting long-range information within RS scenes. Second,\nconvolution can be regarded as a kind of template matching,\nwhere the same template is used for ﬁltering at different positions\nin the image [36]. The summation method of the convolution\nkernel is static. However, the transformer is an adaptive ﬁlter.\nThe weight of the template is determined by the relationship\nbetween pixels, and the summation method is dynamic. This\nadaptive computing module could mine more useful information\nfrom complex RS scenes. Finally, once CNN is trained, all\nparameters are ﬁxed for different input remote sensing scenes.\nThus, the weights of different scenes are the same. Nevertheless,\nthe transformer models the correlation between patches within\nthe input remote sensing scene, and then, calculates the features.\nAs the obtained correlations are different, different RS scenes\ncan be weighted dynamically. In summary, the CNN is local and\nstatic, and the transformer is global and dynamic, and they have\ndifferent advantages.\nThe major contributions of this article are summarized as\nfollows.\n1) We develop an end-to-end network based on the vision\ntransformer to accomplish RS scene classiﬁcation tasks.\nThe homogenous and heterogenous information within RS\nscenes can be mined simultaneously, and local knowledge\nand global contextual relations can also be exploited from\nRS scenes. Therefore, the complex contents of RS scenes\ncan be fully interpreted.\n2) A new loss function named homo–heterogenous classi-\nﬁcation loss (HHCL) is proposed in the classiﬁcation\nmodule. On the one hand, it can enhance the feature learn-\ning ability of the homo–heterogenous transformer. On the\nother hand, both the contributions of homo–heterogenous\nfeatures and the distance relationships between inter-\n/intraclass samples are simultaneously considered.\n3) Extensive experiments are conducted on four benchmark\ndatasets, and the encouraging results demonstrate that the\nproposed method is effective in the RS scene classiﬁcation.\nThe rest of this article is organized as follows. The literature\nrelated to CNN-based methods and attention-based methods for\nthe RS scene classiﬁcation is reviewed in Section II. Then, the\nproposed method is introduced in Section III. The experimental\nresults and discussion are given in Section IV. Finally, Section V\nconcludes this article.\nII. R ELATED WORK\nIn this section, we divide the existing RS scene classiﬁcation\nmethods into two groups for the brief review, i.e., CNN-based\nand attention-based methods.\nMA et al.: HOMO–HETEROGENOUS TRANSFORMER LEARNING FRAMEWORK FOR REMOTE SENSING SCENE CLASSIFICATION 2225\nA. CNN-Based Methods for RS Scene Classiﬁcation\nDue to the powerful feature learning capability, a variety\nof CNN-based methods have been proposed for the RS scene\nclassiﬁcation and they have achieved remarkable performance.\nNogueira et al. [37] ﬁne-tuned six popular CNNs to extract\nfeatures from RS scenes directly, and the classiﬁcation results\nbased on those deep features are competitive. Li et al. [38]\nembedded transfer learning into the CNN to reduce the inﬂu-\nence of overﬁtting problem and improve the classiﬁcation ac-\ncuracy with limited labeled samples. To elaborately explore the\ncomplementarity of multilayer features for scene classiﬁcation,\nLu et al.[39] proposed an end-to-end feature aggregation CNN\n(FACNN). In FACNN, a supervised feature encoding module\nis proposed to encode the different convolutional features from\nthe CNN. Another model, named gated bidirectional network,\nwas presented in [40] to fuse the contributions of multilayer\nfeatures. Therefore, more complementary information can be\nexploited for RS scene classiﬁcation tasks. In general, CNN-\nbased methods learn RS images’ features from the global aspect.\nHowever, the local regions that are also important to the RS scene\nclassiﬁcation cannot be fully considered. To overcome this lim-\nitation, Wang et al.[41] proposed an enhanced feature pyramid\nnetwork with deep semantic embedding to extract multiscale\nmultilevel features for scene classiﬁcation. In addition, a joint\nglobal and local feature representation method was introduced\nin [42] for the RS scene classiﬁcation, in which a dual-stream\nnetwork is constructed to capture the global and local features\nfrom RS scenes, respectively. In this network, to locate the most\nimportant areas in RS scenes, a weakly supervised key area\ndetection strategy is designed. To further study the complex\ncontents within RS scenes, some speciﬁc networks have been\nproposed. For example, He et al.[24] developed a kip-connected\ncovariance (SCCov) network, which embeds the skip connection\nand covariance pooling into the normal CNN to classify RS\nscenes. Wang et al.[43] introduced a multigranularity canonical\nappearance pooling to learn multigrained features. It adopts\nthe Siamese style architecture to learn transformation-invariant\nfeatures for the RS scene classiﬁcation.\nIn addition, some models that combine CNNs with other\ntechniques to further improve the performance of RS scene\nclassiﬁcation. For example, Cheng et al. [44] integrated the\nmetric learning and CNN to boost the performance of the RS\nscene classiﬁcation. Through adding the distance constraint into\nthe hierarchical learning process, the obtained features are more\ndiscriminative. A contextual information-preserved architecture\nlearning (CIPAL) framework was presented in [45] for the\nRS scene classiﬁcation, in which the architecture learning and\nchannel compression mechanisms are combined to mine the\ncontextual information and reduce the memory consumption\nsimultaneously. Considering the scene classiﬁcation problem\nfrom the perspective of multiple instance learning, Li et al.[46]\nproposed a multiple instance CNN (MI-CNN) to acquire more\nrobust scene representations, where an instance-level classiﬁer\nis developed that is sensitive to the discriminative local patches.\nInspired by the potential of relation inference of the graph convo-\nlutional network (GCN), a new model that combines the CNN\nand GCN was developed for the RS scene classiﬁcation [47],\nwhich can learn both the global-based visual features and the\nobject-based location features. Although the aforementioned\nmethods have achieved excellent results in remote sensing scene\nclassiﬁcation, they rely on large amounts of training data. Con-\nsidering the scale of the RS scene is limited, the few-shot\nclassiﬁcation of the RS scene has attracted the attention of\nresearchers [48]. Zhang et al. [49] proposed a metalearning\nmethod for few-shot classiﬁcation of RS scenes, including a\nfeature extraction module and a metalearning module, in which\nthe CNN as a feature extractor to learn a representation and the\nclassiﬁer is optimized in the metric space by the cosine distance.\nB. Attention-Based Methods for the RS Scene Classiﬁcation\nCNN-based methods have achieved remarkable performance\non the RS scene classiﬁcation because their strong capacity\nof global information exploration. However, most of them are\nnot good at capturing the local knowledge from RS scenes. To\novercome this limitation, scholars introduce attention mecha-\nnism into their models. Those attention-based models can mine\nsalient regions from RS scenes, which are conducive to enhance\nfeatures’ discrimination.\nFor clarity, we divide the existing attention-based methods\ninto two groups, i.e., regular-attention- and transformer-based\napproaches. In the ﬁrst group, the methods aim to capture\nthe discriminative local areas from RS scenes. For instance,\nLi et al.[50] adopted an augmentation attention mechanism to\ndevelop a classiﬁer for completing RS scene classiﬁcation tasks.\nAugmentation operations over attention feature maps are used\nto ensure the model to exploit discriminative regions as much\nas possible. Guo et al. [51] proposed a global-local attention\nnetwork (GLANet) to capture both global and local information\nfor the RS scene classiﬁcation. It concatenates global and local\nfeatures to ensure the discrimination of feature representation.\nTang et al.[52] developed an attention consistent network (AC-\nNet) for the RS scene classiﬁcation. First, intermediate feature\nmaps are learned by VGG-Net [22]. Second, a parallel-attention\nmodel is designed to mine the local information from feature\nmaps. Third, an attention consistent model is developed to\nunify the salient regions. Finally, the learned features are used\nto classify RS scenes. An unsupervised deep feature learning\nmethod was proposed for the RS scene classiﬁcation [53], which\nintegrates the attention mechanism into GANs to enhance the\nrepresentation power of the discriminator.\nIn the second group, the deep classiﬁcation networks are con-\nstructed based on the transformer, which was proposed in [32]\nand it achieves excellent performance in many computer vision\ntasks [54]–[59]. The transformer can capture long-range context\ninformation through multihead attention [60] and can be seen\nas an enhanced version of the attention mechanism. Recently,\nDosovitskiy et al.[31] applied a fully-transformer model, named\nvision transformer (ViT), to image classiﬁcation. Instead of\nusing images directly, ViT splits images into ﬁxed-size patches\nﬁrst. Then, by mining the relations between image patches,\nthe effective visual features can be learned. Based on ViT, a\nknowledge distillation token was added along with the class\ntoken [61]. By introducing a knowledge distillation strategy,\nViT can achieve a better performance on small-scale datasets\n2226 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nFig. 1. Overall structure of the proposed HHTL framework, which consists of a PGM, an FLM, and a classiﬁcation module.\nwithout pretraining. To model surrounding tokens for capturing\nlocal structure and simplifying backbone, Yuan et al.[62] pro-\nposed a tokens-to-token vision transformer (T2T-ViT), including\na T2T module and an efﬁcient backbone with a deep-narrow\nstructure. In addition, a convolutional vision transformer is de-\nveloped in [63], which can simultaneously extract local features\nand contextual information from images to build discriminative\nfeature representations. In the RS community, the number of\ntransformer-based methods is few. A recent work directly used\nvision transformer for the RS scene classiﬁcation [64], and trans-\nformer’s potential in the RS scene classiﬁcation task is being\ndeveloped. In this article, we explore an HHTL framework with\nmetric learning to capture global and local context discriminative\ninformation for the RS scene classiﬁcation.\nIII. P ROPOSED METHOD\nA. Overall Framework\nThe architecture of the proposed HHTL framework is il-\nlustrated in Fig. 1, which consists of a PGM, a FLM, and a\nclassiﬁcation module. The PGM aims at generating multiple\ntypes of patches, which includes HPGM and AHPGM. The goal\nof the HPGM is to divide RS scenes into heterogeneous patches\ndirectly, while the target of the AHPGM is to obtain homogenous\npatches using the superpixel segmentation method. The FLM\nis proposed to extract complete features from heterogenous\nand homogenous patches. It can learn global and local context\ninformation within RS scenes. For the classiﬁcation module, a\nnew loss function is developed. It can compact/disperse intra-\n/interclass samples and integrate the homo–heterogeneous fea-\ntures so that the discriminative feature representation for the\nRS scene classiﬁcation can be obtained. Now, let us discuss the\nproposed HHTL in detail.\nB. Patch Generation Module (PGM)\nConsidering the complex contents within RS scenes, we de-\nvelop the PGM to generate diverse patches for mining the RS\nscenes’ information comprehensively. There are two submod-\nules in the PGM, including HPGM and AHPGM.\nIn the HPGM, we divide a scene into a set of nonoverlapping\npatches with ﬁxed size directly. Thereby, for each input scene\nI ∈ RH×W×C (where C, H, and W indicate the number of\nchannels, spatial height, and spatial width), we can get a set\nof heterogenous patches Phe = {pi\nhe}N\ni=1, where pi\nhe ∈ RS×S×C\ndenotes the ith heterogenous patch, N = HW/S2 is the number\nof patches, and S is the patch size. To reduce memory and time\ncosts, S equals 32 in this article.\nApart from the heterogenous information, homogenous\nknowledge is also essential to understand complex contents\nwithin RS scenes. Here, the AHPGM is proposed to adap-\ntively generate homogenous patches with uniform size from\nan input RS scene. The ﬂowchart of the AHPGM is shown in\nFig. 2. First, the simple linear iterative clustering (SLIC) [65]\nalgorithm is used to segment the input RS scene into a set of\nsuperpixels. Second, each superpixel corresponds to a ﬁxed\nvalue in the mask so that we can take it out according to the\nvalue of the mask. Third, we select the maximum and minimum\nvalues of the x- and y-axes in the RS scene plane to get a\nquadrangular area for each superpixel. Here, the coordinates\nof the four vertices of the quadrangular area are (xmin,ymin),\n(xmin,ymax),(xmax,ymin),and (xmax,ymax). Then, we can get\ncentroid coordinates (cx,cy) of the quadrangular area. Fourth,\nthe 16 pixels around the centroid are captured and cropped from\nthe scene to generate a uniform rectangular patch. Note that if\nthe vertex is at the boundary of the RS scene and the coordinate\ndistance between the vertex and centroid pixels is less than 16,\nMA et al.: HOMO–HETEROGENOUS TRANSFORMER LEARNING FRAMEWORK FOR REMOTE SENSING SCENE CLASSIFICATION 2227\nFig. 2. Flowchart of the AHPGM.\nwe take the vertex coordinates as the starting point and select\n16 pixels to get the new centroid coordinates, thereby, obtaining a\npatch with uniform size. Finally, the input RS scene is adaptively\ndivided into a sequence of homogenous patches Pho = {pi\nho}N\ni=1,\nwhere pi\nho ∈ RS×S×C denotes the ith homogenous patch. Note\nthat, to avoid the overlapping issue, we process each superpixel\nindependently. In other words, we pick a superpixel out and\nadjust its shape individually. If the size of a superpixel is less\nthan 32 ×32, the expanded part would be ﬁlled with zero.\nC. Feature Learning Module\nThe information within Pho and Phe is complementary, which\nis critical to fully understand the complex contents of the RS\nscenes. Here, we propose FLM to learn homogenous and het-\nerogenous features simultaneously. As shown in Fig. 1, the FLM\nis a dual-branch network that does not share parameters, and the\nstructure of each branch is the same to each other. Therefore, we\nonly introduce the heterogenous feature learning branch in the\nfollowing for clarity.\nThe feature extractor of the heterogenous feature learning\nbranch can be any deep learning model. In this article, due\nto the powerful feature learning ability of vision transformer,\nthe Vision Transformer-Base 32 (ViT-B 32) [31] model is used\nto extract features. In ViT-B ˜32, the layer numbers of the\ntransformer encoder and the number of self-attention heads\nin each multihead self-attention (MSA) are set to 12 and 12,\nrespectively. First,Phe are input to heterogenous feature learning\nbranch and are mapped into a set of d-dimensional heterogenous\npatch embedding through a trainable linear projection. Next, the\nheterogenous patch embeddings are concatenated with a learn-\nable class embedding he-token, which is a specialized vector\nto perform scene classiﬁcation task. Then, to retain positional\ninformation of the heterogenous patches in the original scene, a\nlearnable position embedding is added. Afterwards, the desired\nheterogenous embedded patches z0 are formed as follows:\nz0 =[ he-token; p1\nheE; p2\nheE; ... ; pN\nhe E;]+ Epos (1)\nwhere E ∈ R(S2.C)×d is the patch embedding projection, and\nEpos ∈ R(N+1)×d denotes the position embedding. Finally,\nthe heterogenous embedded patches z0 are input to the He-\ntransformer encoder to learn heterogenous features.\nAs a core part of the heterogenous feature learning branch,\nthe He-transformer encoder can learn both global information\nFig. 3. Architecture of the transformer encoder. As a core part of the FLM,\nthe transformer encoder consists of (a) He-transformer encoder and (b) Ho-\ntransformer encoder, and can learn heterogenous information and homogenous\ninformation from embedded patches simultaneously.\nand local areas’ contextual relations from heterogenous em-\nbedded patches. The framework of the He-transformer encoder\nis illustrated in Fig. 3(a), which is composed of L identical\nlayers. Each layer has two sublayers, e.g., an MSA block and a\nmultilayer perceptron (MLP) block. Layernorm (LN) is applied\nbefore every block, and residual connections after every block.\nThe MLP block consists of two linear transformations with a\nGaussian error linear unit (GELU) activation in between. Thus,\nthe output of the lth layer can be written as follows:\nzMSA\nl = MSA (LN (zl−1)) +zl−1,l =1 ...,L (2)\nzl = MLP\n(\nLN\n(\nzMSA\nl\n))\n+ zMSA\nl ,l =1 ...,L (3)\nwhere zl is the encoded scene representation. We use the ﬁrst\nelement of the last encoder layer z0\nL to generate the heterogenous\nfeature representation he-token 0, which can be formulated as\nhe-token0= LN\n(\nz0\nL\n)\n. (4)\nThe homogenous feature representation ho-token 0 can be ob-\ntained in the same manner. Note that, similar to heterogenous\npatches, the position embedding of homogenous patches is\nsame for different scenes. For special cases, if the number of\nhomogenous patches generated from the image is different from\nthe length of the position embedding, we will adjust the number\nof patches, and the expanded patches will be ﬁlled with zero.\nThe key component of the transformer encoder is MSA,\nwhich aims to explore the context information among patch em-\nbeddings from different representation subspaces and different\npositions. Before explaining MSA in detail, we ﬁrst introduce\nthe self-attention head function. The self-attention head function\n2228 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nis actually the scaled dot-product attention mechanism, which is\napt at establishing the relationship between the elements in the\ninput patches sequence. Its formulation is\nAttention(Q,K,V )= softmax\n(QKT\n√dk\n)\nV (5)\nwhere {Q,K,V } indicates an input data, √dk is a scaling\nfactor, and softmax (·) denotes the softmax function. The MSA\nperforms h self-attention heads in parallel, and the outputs are\nconcatenated and projected to the ﬁnal values. For input zl−1,\nthe formulation of the MSA is\nMSA(zl−1)= Concat (head1,..., headh) WO (6)\nwhere W O represents the learned matrices and head i is the\nith self-attention head function. The deﬁnition of head i can be\ndescribed as\nheadi = Attention(zl−1WQ\ni ,zl−1WK\ni ,zl−1WV\ni ) (7)\nwhere WQ\ni ,WK\ni , and WV\ni ∈ Rd×d/h are the learned parameter\nmatrices.\nD. Classiﬁcation Module\nThe classiﬁcation module consists of a fusion submodule\nand a metric-learning submodule. In the fusion submodule,\nthe learned heterogenous features he-token 0 and homogenous\nfeatures ho-token 0 form the FLM are ﬁrst transformed into\nclassiﬁcation score he-class and ho-class through an MLP head\nlayer. Second, to better integrate heterogenous information and\nhomogenous information, he-class and ho-class are added to get\na fusion classiﬁcation score S as\nS = he-class + ho-class\n2 . (8)\nFinally, a softmax loss function Lsoftmax is used to enhance\nthe fusion classiﬁcation score S. Lsoftmax aims to minimize\nthe classiﬁcation error between the model outputs and their\ncorresponding labels, and is deﬁned as\nLsoftmax = − 1\nN\nN∑\n1\n⟨yi,log(si)⟩ (9)\nwhere ⟨.,.⟩denotes inner product of two vectors, N is the size\nof minibatch, si denotes the probability, and yi is the real scene\nlabel.\nIn the metric-learning submodule, a dual cosine contrastive\nloss Ldccl is proposed to reduce the interclass similarity and\nenhance the intraclass similarity in the feature space, and its\ndeﬁnition is\nLdccl = Lhe\ndccl + Lho\ndccl (10)\nLhe\ndccl = 1\nN2\nN∑\ni\n⎛\n⎝\nN∑\nj:yi=yj\n(1−cossim(he-token0i,he-token0j)\n⎞\n⎠\n+\nN∑\nj:yi̸=yj\nmax((cossim(he-token0i,he-token0j)−α),0))\n(11)\nFig. 4. Illustration of feature space. (a) Original feature space. (b) FLM feature\nspace.\nLho\ndccl = 1\nN2\nN∑\ni\n( N∑\nj:yi=yj\n(1 −cossim(ho-token0i,ho-token0j)\n)\n+\nN∑\nj:yi̸=yj\nmax((cossim(ho-token0i,ho-token0j)−α),0))\n(12)\nwhere Lhe\ndccl and Lho\ndccl are used to optimize the heterogenous and\nhomogenous branch of the FLM, respectively, N is the size of\na batch, cossim (.,.) is the cosine similarity of two features, α\nis used to constrain the interclass distances, and he-token 0 and\nho-token0 are preprocessed with l2 normalization to have unit\nnorm.\nAfter using Ldccl, the distances between samples from the\nsame/different semantic classes can be reduced/increased. The\nschematic illustration of Ldccl is exhibited in Fig. 4.\nIn summary, a joint loss function named HHCL function is de-\nﬁned to enhance the discriminability of the homo–heterogenous\ntransformer feature representations. Its deﬁnition is\nLhhcl = Lsoftmax + λLdccl (13)\nwhere λ is a hyperparameter to control the importance of metric\nembedded term in the loss function.\nIV . E XPERIMENT\nA. Dataset Introduction\nTo evaluate the performance of the proposed HHTL frame-\nwork, we conduct experiments on four public RS scene datasets,\nincluding the UC Merced (UCM) Land Use dataset [16],\nAerial Image dataset (AID) [1], NWPU-RESISC45 (NWPU)\ndataset [2], and RSSDIVCS dataset [66], [67].\nThe UCM dataset was published by University of California\nMerced and has 2100 RS scene images. These RS images are\ndivided into 21 scene categories, and each class consists of 100\nscene images with a size of 256 ×256 pixels. The spatial resolu-\ntion is 0.3-m per pixel in the red, green blue (RGB) color space.\nSome images of this dataset are shown in Fig. 5. The AID dataset\nwas proposed by Wuhan University and has 10 000 RS images\nwith 600 ×600 pixels. These images are split into 30 scene\ncategories, and the number of images in each category varies\nfrom 220 to 420. The spatial resolution changes from about 8 m\nMA et al.: HOMO–HETEROGENOUS TRANSFORMER LEARNING FRAMEWORK FOR REMOTE SENSING SCENE CLASSIFICATION 2229\nFig. 5. Examples of the UCM dataset.\nFig. 6. Examples of the AID dataset.\nto 0.5 m per pixel. Some RS images of this dataset are shown\nin Fig. 6. The NWPU dataset was constructed by Northwestern\nPolytechnical University and has 3 1500 RS scene images in the\nRGB color space. It has 45 scene categories, and each category\ncontains 700 scene images with a size of 256 ×256 pixels,\nand the spatial resolution changes from about 0.2 m to 30 m.\nThis dataset is a large-scale scene classiﬁcation dataset with\nlarge image variations, and some examples of this dataset are\nshown in Fig. 7. The RSSDIVCS dataset is a large-scale RS\nscene classiﬁcation dataset, including the instance-level visual\nimages and class-level semantic representations. It has 70 scene\ncategories, and each category contains 800 scene images with a\nsize of 256 ×256 pixels. Examples of this dataset are shown in\nFig. 8.\nB. Dataset Settings and Evaluation Metrics\nTo obtain objective results, we repeat the experiments ﬁve\ntimes by randomly selecting training/testing samples. Then, the\naverage results and their standard deviations are reported. For\nUCM, we choose 50% and 80% scenes randomly to train our\nFig. 7. Examples of the NWPU dataset.\nFig. 8. Examples of the RSSDIVCS dataset.\nmodel. For AID, the training data ratios are set to 20% and 50%.\nFor NWPU, the training–testing ratios are set to 10%–90% and\n20%–80%. For RSSDIVCS, the training ratio is set to 10%.\nWe adopt two widely used evaluation metrics, including\noverall accuracy (OA) and confusion matrix (CM). The OA is\ndeﬁned as the number of correctly classiﬁed images divided\nby the total number of testing images. It reﬂects the model’s\noverall classiﬁcation performance. The CM is used to analyze\nthe detailed classiﬁcation information of each scene class, which\ncan intuitively reﬂect whether the prediction is correct or not and\nthe degree of confusion between different scene classes. In our\nCMs, the rows represent true classes, while the columns denote\nthe predicted classes.\nC. Experimental Settings\nAll experiments are implemented using Pytorch frame-\nwork [68] on a workstation with Intel Xeon CPU E5-2650,\nNVIDIA Titan Xp, and 256-G RAM. The parameters of\neach branch of the LFM are initialized from the ofﬁcial Vi-\nsion Transformer-Base ˜32 (ViT-B ˜32) model (pretrained on\n2230 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nImagenet-21k [69], and then, ﬁne tuned on Imagenet2012 [70]).\nWe employ the stochastic gradient descent (SGD) optimizer with\na momentum of 0.9 to train our model. The learning rate is\ninitialized as 0.03, and a cosine decay learning rate scheduler\nwith a linear warm-up is adopted. The epoch number and batch\nsize equal 300 and 32. For the AHPGM, we use SLIC to segment\nan RS image into 144 superpixels. In addition, the horizontal\nand vertical ﬂipping are used for data augmentation, and all\ninput scenes are resized into 384 ×384. Our model has two free\nparameters, i.e., the constant margin α and the hyperparameter\nλ [see (13)]. They are set to be different values for different sce-\nnarios, including various datasets and training sets. Also, their\ninﬂuence is discussed in Section IV-G. Note that the selection of\nSGD is not our limitation. Some other optimizers (e.g., AdamW)\ncan also be used. From analyzing the experimental results, we\ncan ﬁnd that the performance differences between HHTLs based\non two optimizers are small. The details can be found in the\nsupplementary material.\nD. Experimental Results and Comparisons\nTo comprehensively evaluate the classiﬁcation performance\nof the proposed HHTL framework, we compare our method\nwith some state-of-the-art CNN-based and transformer-based\nmethods. The CNN-based methods are GoogLeNet [1], [2],\nVGGNet-16 [1], [2], VGG-16-CapsNet [71], SCCov [24],\nVGG-VD16+MSCP+MRA [72], GBNet+global feature [40],\nMIDC-Net ˜ CS [73], EFPN-DSE-TDFF [41], DFAGCN [74],\nEfﬁcientNet-B0-aux [75], SF-CNN with VGGNet [76], MG-\nCAP (Sqrt-E) [43], ACNet [52], ACR-MLFF [77], and MSA-\nNetwork [78]. The transformer-based methods are T2T-ViT-\n12 [62], pooling-based vision transformer-small (PiT-S) [79],\nand pyramid vision transformer-medium (PVT-Medium) [80].\nNote that, the results of CNN-based methods are from the\noriginal literature. Meanwhile, the transformer-based methods\nare accomplished by ourselves. Except the image size, other\nexperimental settings are the same as ours.\n1) Results on UCM Dataset:For UCM, the value of αis set to\n0.1/0.3 when training ratios is 50%/80%, and the value of λ is set\nto 0.5 for two training ratios. The results of different methods are\nsummarized in Table I. From the observation of results, we can\nﬁnd the following points. First, all of the results are acceptable\ndue to deep neural networks’ energetic feature learning capac-\nity. Among the compared methods, the strongest and weakest\nCNN-based models are EfﬁcientNet-B0-aux and GoogLeNet,\nwhile the best and worst transformer-based methods are PVT-\nMedium and T2T-ViT-12. Second, the performance of the con-\nventional CNN models (e.g., GoogLeNet and VGGNet-16) is\nweaker than that of the transformer-based networks. This is\nbecause that the global information mining capacity of the\ntransformer is higher than that of normal CNNs. Nevertheless,\nwhen some RS-oriented schemes are embedded into CNNs,\nthe CNN-based models’ behavior is increased distinctly. For\nexample, MG-CAP (Sqrt-E) outperforms all of the transformer-\nbased models. Third, our HHTL framework achieves the best\nperformance in all cases because the dual-channel FLM and\nthe proposed new loss function of our method can comprehen-\nsively learn information within RS scenes and compact/disperse\nTABLE I\nOVERALL ACCURACIES AND STANDARD DEVIATIONS (%) OF THE PROPOSED\nHHTL FRAMEWORK AND THE COMPARISON METHODS ON THE UCM DATASET\nintra-/interclass samples, respectively. Compared with the best\nCNN- and transformer-based models, the improvements in OA\nobtained by the HHTL are 0.86% (over EfﬁcientNet-B0-aux)\nand 2.6% (over PVT-Medium) when the training ratio is 50%.\nWhen the training ratio equals 80%, the enhancements are 0.44%\n(EfﬁcientNet-B0-aux) and 1% (PVT-Medium). The encourag-\ning results discussed previously demonstrate that our HHTL\nframework is effective for the UCM dataset.\nApart from OA, we also report CMs of our method under\nthe different training ratios in Fig. 9. From the observation,\nwe can see that most scenes are categorized correctly. For\nexample, when the training ratio is 80%, the incorrect predicted\nresults only appear in “Forest” and “Sparse Residential.” These\nencouraging results demonstrate the effectiveness of our method\nagain.\n2) Results on AID Dataset: For AID, the values of α and\nλ are set to be 0.9 and 0.05 under the training ratio of 20%,\nwhile under the training ratio of 50%, their values are equal\nto 0.5 and 0.05. The results of different methods are shown\nin Table II. Similar to UCM, HTTL achieves the best perfor-\nmance in any case. Its OA values are as high as 95.62% and\n96.88% when the training ratios are 20% and 50%. Compared\nwith the weakest model (GoogLeNet), the increases in OA\nobtained by the HHTL under two training ratios are 12.18%\nand 10.49%, respectively. Interestingly, there is a distinct perfor-\nmance gap between conventional CNN models (i.e., GoogLeNet\nand VGGNet-16) and other RS-oriented CNN-based networks.\nThis factor demonstrates that mining the characteristics of RS\nimages (such as the multiscale property) plays a vital role in\nscene classiﬁcation, especially when the archive is complex in\nthe scene and large in volume. Also, although transformer-based\nmethods are general-purposed, their behavior is still assertive\ndue to the forceful feature learning capacity of the transformer\nmodel. These encouraging results mentioned previously conﬁrm\nthat the HHTL is helpful to the AID dataset.\nBesides, CMs of our HHTL framework under different train-\ning ratios are displayed in Fig. 10. Although the RS scenes\nMA et al.: HOMO–HETEROGENOUS TRANSFORMER LEARNING FRAMEWORK FOR REMOTE SENSING SCENE CLASSIFICATION 2231\nFig. 9. Confusion matrices of the proposed HHTL framework under the training ratio of (a) 50% and (b) 80% on the UCM dataset. The scene category\ncorresponding to each number can be found in Fig. 5.\nFig. 10. Confusion matrices of the proposed HHTL framework under the training ratio of (a) 20% and (b) 50% on the AID dataset. The scene category\ncorresponding to each number can be found in Fig. 6.\nin AID are large in scale and diverse in the category, our\nHHTL still achieves competitive classiﬁcation results in most\ncases. In detail, as shown in Fig. 10(a), 26 of the 30 categories\nhave classiﬁcation accuracies over 90%. Besides, as shown in\nFig. 10(b), 28 of the 30 categories have classiﬁcation accuracies\nover 90%, and 24 categories have classiﬁcation accuracies over\n95%. Nevertheless, the performance of the HHTL is not as good\nas what we expect in some scenes, such as “Resort” and “Park.”\nThe reason behind this is the high interclass similarities between\nRS images within these two categories. As exhibited in Fig. 6,\nthe main contents within “Resort” and “Park” are green plants,\nbare soil, and a handful of manual buildings. Therefore, the\nfeatures learned by the HHTL are not discriminative enough\nto distinguish them. Developing an effective scheme for this\nscenario is one of our future work.\n3) Results on NWPU Dataset: For NWPU, the values of\nα and λ are set to be 0.7 and 0.5 for two training ratios\n(e.g., 10% and 20%), and the results of different methods are\nshown in Table III. Like to UCM and AID, we can ﬁnd that\nthe performance of our HHTL framework is best. Compared\nwith other methods, when 10% scenes are used for train-\ning, the enhancements in OA obtained by our HHTL frame-\nwork are 15.88% (over GoogLeNet), 15.6% (over VGGNet-\n16), 6.99% (over VGG-16-CapsNet), 2.77% (over SCCov),\n4% (over VGG-VD16+MSCP+MRA), 5.95% (over MIDC-\nNet CS), 0.98% (over ACNet), 2.11% (over EfﬁcientNet-\nB0-aux), 2.18% (over SF-CNN with VGGNet), 2.06% (over\nACR-MLFF), 1.69% (over MSA-Network), 1.24% (over MG-\nCAP(Sqrt-E)), 7.16% (over T2T-ViT-12), 6.22% (over PiT-S),\nand 4.67% (over PVT-Medium). When 20% scenes are used for\n2232 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nTABLE II\nOVERALL ACCURACIES AND STANDARD DEVIATIONS (%) OF THE PROPOSED\nHHTL FRAMEWORK AND THE COMPARISON METHODS ON THE AID DATASET\nTABLE III\nOVERALL ACCURACIES AND STANDARD DEVIATIONS (%) OF THE PROPOSED\nHHTL FRAMEWORK AND THE COMPARISON METHODS ON THE\nNWPU DATASET\ntraining, the enhancements in OA obtained by our HHTL frame-\nwork are 15.73% (over GoogLeNet), 14.42% (over VGGNet-\n16), 5.03% (over VGG-16-CapsNet), 2.11% (over SCCov),\n3.4% (over VGG-VD16+MSCP+MRA), 6.22% (over MIDC-\nNet CS), 1.79% (over ACNet), 4.92% (over DFAGCN), 1.32%\n(over EfﬁcientNet-B0-aux), 1.66% (over SF-CNN with VG-\nGNet), 1.76% (over ACR-MLFF), 0.69% (over MSA-Network),\n1.26% (over MG-CAP(Sqrt-E)), 4.78% (over T2T-ViT-12),\n4.3% (over PiT-S), and 2.82% (over PVT-Medium). The ex-\nperimental results discussed previously verify the effectiveness\nof our method for the RS scene classiﬁcation task again.\nFurthermore, CMs of HHTL framework under the training\nratios of 10% and 20% are exhibited in Fig. 11. From observing\nCMs, we can ﬁnd that our method obtains satisfactory classiﬁca-\ntion results on most categories. Taking Fig. 11(b) as an example,\n39 of the 45 categories have classiﬁcation accuracies over 90%.\nTABLE IV\nOVERALL ACCURACIES AND STANDARD DEVIATIONS (%) OF THE PROPOSED\nHHTL FRAMEWORK AND THE COMPARISON METHODS ON THE\nRSSDIVCS DATASET\nHowever, the HHTL performs weakly in some categories, such\nas “Church” and “Place.” How to enhance HHTL’s behavior in\nthese content-similar categories is our future work.\n4) Results on RSSDIVCS Dataset:RSSDIVCS is the largest\ndataset among the testing data. For RSSDIVCS, the value of\nα is set to 0.9 and the value of λ is set to 1.0. The results of\ndifferent methods are summarized in Table IV, in which we can\nﬁnd that our HHTL framework achieves the best classiﬁcation\nresults. Compared with other methods, the improvements in\nOA obtained by our HHTL framework are 3.93% (EFPN-DSE-\nTDFF), 2.89% (GBNet+global feature), 2.02% (ACNet), 3.95%\n(T2T-ViT-12), 3.6% (PiT-S), and 2.42% (PVT-Medium). These\npositive results prove that our method is suitable for the RS scene\nclassiﬁcation task even though the dataset is large and complex.\nIn addition, the CM of our HHTL framework is reported in\nFig. 12. From observing the ﬁgure, we can ﬁnd that the HHTL is\neffective for most scene categories. For some scene categories,\nsuch as “Sea” and “Golf Course,” the classiﬁcation accuracy\nobtained by our model is as high as 100%. However, the HHTL\nis not good at distinguishing “Palace,” “Church,” and “Square”\nscenes due to the similar semantic distribution among them.\nThere is another point we want to touch on. Compared with\nother methods, the advantage of our HHTL is not signiﬁcant.\nThe reasons can be summarized as follows. First, the vision\ntransformer [31] embedded in our model is the original version,\nwhich directly comes from the natural language processing\ncommunity [32]. In other words, the selected transformer is\nnot speciﬁcally designed for visual tasks. We believe if we\nchoose some new transformers that are developed for computer\nvision tasks [57]–[59], the performance of the HHTL would be\nincreased distinctly. To ensure the this, we replace the primary vi-\nsion transformer [31] with the swin transformer [57] to construct\nthe HHTL model. Then, it is applied to different RS datasets to\ncount the performance. From observing these results, we can\nﬁnd that the swin transformer-based HHTL model outperforms\nthe vision transformer-based HHTL in all cases. The details of\nthis experiment have been added in the supplementary material.\nSecond, the strong CNN-based methods are delicately developed\nbased on basic CNNs. Some of them add multiscale, spatial\nattention and other operations to improve the learning ability of\nthe network, and others use feature aggregation and channel at-\ntention to enhance the classiﬁcation performance. Nevertheless,\nour HHTL does not include that advanced tricks. In the future,\nwe will try to introduce some pointed operations into our HHTL\nto improve its performance on the RS scene classiﬁcation.\nMA et al.: HOMO–HETEROGENOUS TRANSFORMER LEARNING FRAMEWORK FOR REMOTE SENSING SCENE CLASSIFICATION 2233\nFig. 11. Confusion matrices of the proposed HHTL framework under the training ratio of (a) 10% and (b) 20% on the NWPU dataset. The scene category\ncorresponding to each number can be found in Fig. 7.\nFig. 12. Confusion matrix of the proposed HHTL framework under the training ratio of 10% on the RSSDIVCS dataset. The scene category corresponding\nto each number can be found in Fig. 8.\n2234 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nFig. 13. 2-D visualization of feature embedding under Softmax loss and HHCL on the UCM dataset using t-SNE. (a) Training ratio is 50%. (b) Training rati o\nis 80%. The scene category corresponding to each number can be found in Fig. 5.\nFig. 14. 2-D visualization of feature embedding under Softmax loss and HHCL on the AID dataset using t-SNE. (a) Training ratio is 20%. (b) Training rati o\nis 50%. The scene category corresponding to each number can be found in Fig. 6.\nFig. 15. 2-D visualization of feature embedding under Softmax loss and HHCL on the NWPU dataset using t-SNE. (a) Training ratio is 10%. (b) Training rat io\nis 20%. The scene category corresponding to each number can be found in Fig. 7.\nE. Feature Structure\nApart from displaying the numerical results, we study the\nstructure of the learned deep features to estimate if the proposed\nHHTL framework is effective or not for scene classiﬁcation\ntasks. Here, we use the t-distributed stochastic neighbor em-\nbedding ( t-SNE) algorithm [81] to reduce the dimension of\nthe homo–heterogenous feature embeddings generated by our\nHHTL framework, and then, visualize their projections in a\n2-D space. In addition, to testify the usefulness of the new loss\nfunction (HHCL), we replace the HHCL with the softmax loss\nfunction and draw the feature distributions for reference. From\nobserving Figs. 13–16, we can ﬁnd the features’ structure is clear\nno matter which loss function is adopted, which demonstrates\nthe effectiveness of HHTL. Furthermore, compared with HHTL\nembedded with the softmax loss function, the feature clusters\nobtained by the HHTL embedded with our loss function are\nseparable obviously, and the relative distances between them\nare distinct. This fact conﬁrms the usefulness of our new loss\nfunction.\nF . Ablation Study\nAs mentioned in Section III-A, our model consists of three\nkey components: PGM, FLM, and HHCL. The PGM contains\nthe AHPGM and HPGM, which are used to generate homo-\nand heterogenous patches. The FLM is a dual-channel network\nembedded with the basic transformer model, which aims at\nmapping diverse RS patches into effective embeddings. The\nHHCL is our new loss function that focuses on fusing homo-\nand heterogenous embeddings and compacting/separating intra-\n/interclass samples. To analyze their contributions to HHTL,\nrespectively, we construct the following nets.\n1) Net-0: HPGM + heterogenous transformer + softmax loss.\n2) Net-1: HPGM + heterogenous transformer + AHPGM +\nhomogenous transformer + softmax loss.\n3) Net-2: HPGM + heterogenous transformer + AHPGM +\nhomogenous transformer + HHCL.\nHere, Net-0 is a single-vision transformer, Net-1 is a dual-\nbranch network, and Net-2 is our HHTL framework. Note that\nthe softmax loss function is used to train Net-0 and Net-1.\nMA et al.: HOMO–HETEROGENOUS TRANSFORMER LEARNING FRAMEWORK FOR REMOTE SENSING SCENE CLASSIFICATION 2235\nFig. 16. 2-D visualization of feature embedding under Softmax loss and HHCL on the RSSDIVCS dataset using t-SNE. The scene category corresponding\nto each number can be found in Fig. 8.\nTABLE V\nOVERALL ACCURACIES AND STANDARD DEVIATIONS (%) OF DIFFERENT\nMETHODS ON FOUR DATASETS\nBesides, the experimental settings remain the same as men-\ntioned in Section IV-C. The results of different nets counted\non four datasets with varying ratios of training are summarized\nin Table V. We can easily ﬁnd that the performance of all nets is\ngood, and each component of the HHTL plays a positive role in\nRS scene classiﬁcation tasks. In detail, since Net-0 only learns\nheterogenous information from RS scenes, its performance is\nthe weakest among the three nets. Second, as exploiting the\nhomogenous information, Net-1’s behavior becomes stronger.\nThird, after adding HHCL into Net-1, the new model (Net-2)\nachieves the best performance. These positive results demon-\nstrate the following:\n1) homo- and heterogenous patches can provide complemen-\ntary information for the RS scene classiﬁcation;\n2) the FLM can fully capture the complex contents within\nRS scenes; and\n3) the HHCL can further enhance the feature learning ability\nof the FLM so that the obtained features are more discrim-\ninative.\nG. Sensitivity Analysis\nThere are two free parameters in the HHTL, i.e., α and λ. α\nis a margin to constrain the interclass distances, and λ is used to\ncontrol the relative importance of terms in Ldccl. To study their\ninﬂuence on our model, we vary their values alternately. The\nrange of α is {0.1, 0.3, 0.5, 0.7, 0.9}, and λ can be varied in\n{0.01, 0.05, 0.1, 0.5, 1.0}. When one parameter is changed, the\nother one is ﬁxed as its optimal values. The results counted on\nfour datasets are exhibited in Fig. 17.\n1) UCM Dataset: For α, the optimal value of α is different\nunder different scenarios. The HHTL achieves the best perfor-\nmance when α =0 .1 under the training ratio of 50%, and the\nbest performance appears at α =0 .3 under the training ratio of\n80%. For λ, we ﬁnd 0.5 to be the best value of λ for our method\non the UCM dataset.\n2) AID Dataset: Similar to the UCM, the optimal value of\nα is different under two training ratios. When 20% scenes are\nused to train our network, the optimal value of α is 0.9. Under\nthe training ratio of 50%, our method has a solid and stable\nperformance when α ∈ [0.3,0.7]. The best performance of our\nmethod appears at α =0 .5.F o r λ, the best value of λ is 0.05\nwhen 20% or 50% scenes are selected for training. Both too\nlarge or too small values of λ will reduce the ﬁnal performance.\nThese results indicate that the suitable α and λ are essential to\ntraining our model on the AID dataset.\n3) NWPU Dataset: The performance of our HHTL frame-\nwork is not sensitive to two free parameters under the different\ntraining ratios. For α, when the training ratio is 10%, the OAs\nwill improve with the increase of α and the peak value can be\nreached at α =0 .7. When the training ratio is 20%, the optimal\nvalue of α is the same. Thus, we set α =0 .7 for the NWPU\ndataset. For λ, our method achieves the highest performance\nwhen λ is set to 0.5. When the value of λ is bigger than 0.5 or\nsmaller than 0.5, the classiﬁcation performance will be slightly\nreduced.\n4) RSSDIVCS Dataset: Similar to NWPU, the performance\nof our HHTL framework is not sensitive to two free parameters.\nFor α, our method achieves the highest performance when λ is\nset to 0.9. For λ, our method has a stable performance and the\nbest performance appears at λ =1 .0.\nAccording to the experimental results of four datasets, we\nfound that the best performance of HHTL appears at λ ∈\n{0.05,0.5,1.0}. Besides, for α, the best performance of HHTL\nappears at α ≥ 0.5 for AID, NWPU, and RSSDIVCS. As for\nUCM, different values of α have a slight impact on perfor-\nmance. Therefore, when there is a new RS scene dataset, we\nsuggest the optimal values of α and β can be tuned around\nα ∈{ 0.5,0.7,0.9} and λ ∈{ 0.05,0.5,1.0}, respectively.\nApart from the free parameters, the homogenous and het-\nerogenous patch size would impact our HHTL. As said in\nSection III-B, we set the sizes of different patches to be 32 ×32\n2236 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nFig. 17. OAs of our HHTL framework on UCM, AID, NWPU, and RSSDIVCS datasets under different values of (a) α and (b) λ.\nTABLE VI\nTRAINING AND TESTING TIME OF DIFFERENT METHODS ON FOUR DATASETSUNDER DIFFERENT TRAINING RATIOS\nFig. 18. OAs of our HHTL framework on different patch sizes.\nin this article. To verify the impact of different patch sizes on the\nmodel performance, we conduct experiments to train our HHTL\nwith patch sizes of 16 (i.e., ViT-B ˜16 [31]) and 32 (i.e., ViT-\nB˜32 [31]), respectively. Here, for the sake of fairness, we train\ntwo networks with same experimental settings as the contents\ndiscussed in Section IV-C. The experimental results are shown\nin Fig. 18. From observing the results, we can ﬁnd that the per-\nformance of our HHTL with different patch sizes is close. Com-\npared to ViT-B ˜32, ViT-B˜16 has more parameters and larger\ncomputational complexity. Considering the tradeoff between\nthe performance and the computation complexity, we choose\nViT-B˜32 as feature extractor and the patch size is set to 32.\nH. Time Costs\nIn this section, we quantitatively study the time costs of\nour method. The compared methods’ time consumption is also\ncounted for reference. All methods are trained on four datasets\nwith different training ratios. The training and testing time is\nreported in Table VI. According to the results, we can easily\nﬁnd that the time costs of all methods are acceptable. Meanwhile,\ncompared with other methods, our approach needs more time to\ncomplete the training and inference. The reasons can be summa-\nrized as follows. First, the HHTL is a dual-branch network that\nrequires more time to process the input data, and the compared\nmethods are single-branch network. Second, the transformers\nembedded in the HHTL have MSA blocks. They focus on\nexploiting the long-range context information from RS scenes\nby measuring relationships between all RS scene patches. This\nwould increase the calculated amount of the HHTL. Fortunately,\nthe most time-consuming stage (training) can be accomplished\nofﬂine, and the classiﬁcation accuracy of our HHTL is as high\nas we expect.\nV. C ONCLUSION\nThis article proposes an HHTL framework to accomplish the\nRS scene classiﬁcation task. To comprehensively interpret the\ncomplex content of RS scenes, instead of learning RS scenes\ndirectly, we divide them into diverse image patches. Also, a dual-\nbranch FLM is proposed to learn homogenous and heterogenous\ninformation from RS scenes simultaneously. By embedding a\nvision transformer into our FLM, the contents within RS patches\nand the context information between different patches can be\nfurther captured. Furthermore, tanking the similarities between\ninter-/intraclass RS scenes, an HHCL function is designed under\nthe metric learning paradigm. After using the HHCL, samples\nfrom the same/different RS scene class are compacted/separated.\nMA et al.: HOMO–HETEROGENOUS TRANSFORMER LEARNING FRAMEWORK FOR REMOTE SENSING SCENE CLASSIFICATION 2237\nThe extensive experimental results counted on four widely used\nRS scene datasets illustrate that our method is helpful to RS\nscene classiﬁcation tasks. We will focus on designing a more ef-\nﬁcient transformer to further improve the classiﬁcation accuracy\nand decrease computational costs in future work.\nREFERENCES\n[1] G.-S. Xia et al., “AID: A benchmark data set for performance evaluation\nof aerial scene classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 55,\nno. 7, pp. 3965–3981, Jul. 2017.\n[2] G. Cheng, J. Han, and X. Lu, “Remote sensing image scene classiﬁ-\ncation: Benchmark and state of the art,” Proc. IEEE, vol. 105, no. 10,\npp. 1865–1883, Oct. 2017.\n[3] G. Cheng, J. Han, L. Guo, Z. Liu, S. Bu, and J. Ren, “Effective and\nefﬁcient midlevel visual elements-oriented land-use classiﬁcation using\nVHR remote sensing images,” IEEE Trans. Geosci. Remote Sens., vol. 53,\nno. 8, pp. 4238–4249, Aug. 2015.\n[4] N. Zhu et al., “Deep learning for smart agriculture: Concepts, tools,\napplications, and opportunities,” Int. J. Agricultural Biol. Eng., vol. 11,\nno. 4, pp. 32–44, 2018.\n[5] X. Zou, M. Cheng, C. Wang, Y . Xia, and J. Li, “Tree classiﬁcation in\ncomplex forest point clouds based on deep learning,” IEEE Geosci. Remote\nSens. Lett., vol. 14, no. 12, pp. 2360–2364, Dec. 2017.\n[6] G. Cheng, X. Xie, J. Han, L. Guo, and G.-S. Xia, “Remote sensing\nimage scene classiﬁcation meets deep learning: Challenges, methods,\nbenchmarks, and opportunities,” IEEE J. Sel. Topics Appl. Earth Observ.\nRemote Sens., vol. 13, pp. 3735–3756, Jun. 2020.\n[7] Q. Bi, K. Qin, H. Zhang, and G.-S. Xia, “Local semantic enhanced Con-\nvNet for aerial scene recognition,” IEEE Trans. Image Process., vol. 30,\npp. 6498–6511, Jul. 2021.\n[8] C. Liu, J. Ma, X. Tang, F. Liu, X. Zhang, and L. Jiao, “Deep hash learning\nfor remote sensing image retrieval,” IEEE Trans. Geosci. Remote Sens.,\nvol. 59, no. 4, pp. 3420–3443, Apr. 2020.\n[9] X. Tang, L. Jiao, and W. J. Emery, “SAR image content retrieval based on\nfuzzy similarity and relevance feedback,” IEEE J. Sel. Topics Appl. Earth\nObserv. Remote Sens., vol. 10, no. 5, pp. 1824–1842, May 2017.\n[10] D. Guo, Y . Xia, and X. Luo, “Self-supervised GANs with similarity loss\nfor remote sensing image scene classiﬁcation,” IEEE J. Sel. Topics Appl.\nEarth Observ. Remote Sens., vol. 14, pp. 2508–2521, Feb. 2021.\n[11] X. Wang, L. Duan, C. Ning, and H. Zhou, “Relation-attention networks\nfor remote sensing scene classiﬁcation,” IEEE J. Sel. Topics Appl. Earth\nObserv. Remote Sens., vol. 15, pp. 422–439, Dec. 2021.\n[12] W. Chen, S. Ouyang, W. Tong, X. Li, X. Zheng, and L. Wang, “GCSANet:\nA global context spatial attention deep learning network for remote sensing\nscene classiﬁcation,”IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens.,\nvol. 15, pp. 1150–1162, Jan. 2022.\n[13] N. Dalal and B. Triggs, “Histograms of oriented gradients for human\ndetection,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,v o l .1 ,\n2005, pp. 886–893.\n[14] Y . Yang and S. Newsam, “Comparing SIFT descriptors and Gabor texture\nfeatures for classiﬁcation of remote sensed imagery,” in Proc. 15th IEEE\nInt. Conf. Image Process., 2008, pp. 1852–1855.\n[15] J. Ren, X. Jiang, and J. Yuan, “Learning LBP structure by maximizing\nthe conditional mutual information,” Pattern Recognit., vol. 48, no. 10,\npp. 3180–3190, 2015.\n[16] Y . Yang and S. Newsam, “Bag-of-visual-words and spatial extensions\nfor land-use classiﬁcation,” in Proc. 18th SIGSPATIAL Int. Conf. Adv.\nGeographic Inf. Syst., 2010, pp. 270–279.\n[17] R. Arandjelovic and A. Zisserman, “All about VLAD,” in Proc. IEEE\nConf. Comput. Vis. Pattern Recognit., 2013, pp. 1578–1585.\n[18] Y . Yang and S. Newsam, “Spatial pyramid co-occurrence for image clas-\nsiﬁcation,” in Proc. Int. Conf. Comput. Vis., 2011, pp. 1465–1472.\n[19] Y . Zhong, Q. Zhu, and L. Zhang, “Scene classiﬁcation based on the\nmultifeature fusion probabilistic topic model for high spatial resolution\nremote sensing imagery,” IEEE Trans. Geosci. Remote Sens., vol. 53,\nno. 11, pp. 6207–6222, Nov. 2015.\n[20] L. Wang, Support V ector Machines: Theory and Applications, vol. 177.\nCham, Switzerland: Springer, 2005.\n[21] S. R. Safavian and D. Landgrebe, “A survey of decision tree classi-\nﬁer methodology,” IEEE Trans. Syst., Man, Cybern., vol. 21, no. 3,\npp. 660–674, May/Jun. 1991.\n[22] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” 2014, arXiv:1409.1556.\n[23] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016,\npp. 770–778.\n[24] N. He, L. Fang, S. Li, J. Plaza, and A. Plaza, “Skip-connected covariance\nnetwork for remote sensing scene classiﬁcation,” IEEE Trans. Neural\nNetw. Learn. Syst., vol. 31, no. 5, pp. 1461–1474, May 2020.\n[25] M. Liu, L. Jiao, X. Liu, L. Li, F. Liu, and S. Yang, “C-CNN: Contourlet\nconvolutional neural networks,” IEEE Trans. Neural Netw. Learn. Syst.,\nvol. 32, no. 6, pp. 2636–2649, Jun. 2020.\n[26] X. Tang, X. Zhang, F. Liu, and L. Jiao, “Unsupervised deep feature learning\nfor remote sensing image retrieval,” Remote Sens., vol. 10, no. 8, 2018,\nArt. no. 1243.\n[27] Y . Gu, Y . Wang, and Y . Li, “A survey on deep learning-driven remote\nsensing image scene understanding: Scene classiﬁcation, scene retrieval\nand scene-guided object detection,” Appl. Sci., vol. 9, no. 10, 2019,\nArt. no. 2110.\n[28] X. Tang et al., “Hyperspectral image classiﬁcation based on 3-D octave\nconvolution with spatial-spectral attention network,” IEEE Trans. Geosci.\nRemote Sens., vol. 59, no. 3, pp. 2430–2447, Mar. 2021.\n[29] X. Liu, C. Deng, J. Chanussot, D. Hong, and B. Zhao, “StfNet: A\ntwo-stream convolutional neural network for spatiotemporal image fu-\nsion,” IEEE Trans. Geosci. Remote Sens., vol. 57, no. 9, pp. 6552–6564,\nSep. 2019.\n[30] Z. Li, F. Liu, W. Yang, S. Peng, and J. Zhou, “A sur-\nvey of convolutional neural networks: Analysis, applications, and\nprospects,” IEEE Trans. Neural Netw. Learn. Syst., to be published,\ndoi: 10.1109/TNNLS.2021.3084827.\n[31] A. Dosovitskiy et al., “An image is worth 16 ×16 words: Transformers for\nimage recognition at scale,” 2020, arXiv:2010.11929.\n[32] A. Vaswani et al., “Attention is all you need,” Adv. Neural Inf. Process.\nSyst., vol. 30, pp. 5998–6008, 2017.\n[33] Y . Zhang, H. Liu, and Q. Hu, “Transfuse: Fusing transformers and CNNs\nfor medical image segmentation,” in Proc. Int. Conf. Med. Image Comput.\nComput.-Assist. Interv., 2021, pp. 14–24.\n[34] Y . Zhang et al. , “A multi-branch hybrid transformer networkfor\ncorneal endothelial cell segmentation,” in Proc. Int. Conf. Med.\nImage Comput. Comput.-Assist. Interv. , Cham, Switzerland, 2021,\npp. 99–108.\n[35] J. M. J. Valanarasu, P. Oza, I. Hacihaliloglu, and V . M. Patel, “Medical\ntransformer: Gated axial-attention for medical image segmentation,” in\nProc. Int. Conf. Med. Image Comput. Comput.-Assist. Interv.,C h a m ,\nSwitzerland, 2021, pp. 36–46.\n[36] H. Hu, Z. Zhang, Z. Xie, and S. Lin, “Local relation networks for image\nrecognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019,\npp. 3464–3473.\n[37] K. Nogueira, O. A. Penatti, and J. A. Dos Santos, “Towards better exploit-\ning convolutional neural networks for remote sensing scene classiﬁcation,”\nPattern Recognit., vol. 61, pp. 539–556, 2017.\n[38] W. Li et al., “Classiﬁcation of high-spatial-resolution remote sensing\nscenes method using transfer learning and deep convolutional neural\nnetwork,”IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 13,\npp. 1986–1995, May 2020.\n[39] X. Lu, H. Sun, and X. Zheng, “A feature aggregation convolutional neural\nnetwork for remote sensing scene classiﬁcation,” IEEE Trans. Geosci.\nRemote Sens., vol. 57, no. 10, pp. 7894–7906, Oct. 2019.\n[40] H. Sun, S. Li, X. Zheng, and X. Lu, “Remote sensing scene classiﬁcation by\ngated bidirectional network,” IEEE Trans. Geosci. Remote Sens., vol. 58,\nno. 1, pp. 82–96, Jan. 2020.\n[41] X. Wang, S. Wang, C. Ning, and H. Zhou, “Enhanced feature pyramid\nnetwork with deep semantic embedding for remote sensing scene classiﬁ-\ncation,”IEEE Trans. Geosci. Remote Sens., vol. 59, no. 9, pp. 7918–7932,\nSep. 2021.\n[42] Q. Wang, W. Huang, Z. Xiong, and X. Li, “Looking closer at the\nscene: Multiscale representation learning for remote sensing image scene\nclassiﬁcation,” IEEE Trans. Neural Netw. Learn. Syst., to be published,\ndoi: 10.1109/TNNLS.2020.3042276.\n[43] S. Wang, Y . Guan, and L. Shao, “Multi-granularity canonical appearance\npooling for remote sensing scene classiﬁcation,” IEEE Trans. Image Pro-\ncess., vol. 29, pp. 5396–5407, Apr. 2020.\n[44] G. Cheng, C. Yang, X. Yao, L. Guo, and J. Han, “When deep learning meets\nmetric learning: Remote sensing image scene classiﬁcation via learning\ndiscriminative CNNs,” IEEE Trans. Geosci. Remote Sens., vol. 56, no. 5,\npp. 2811–2821, May 2018.\n[45] J. Chen et al., “Contextual information-preserved architecture learning for\nremote-sensing scene classiﬁcation,” IEEE Trans. Geosci. Remote Sens.,\nvol. 60, pp. 1–14, Mar. 2021.\n2238 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\n[46] Z. Li, K. Xu, J. Xie, Q. Bi, and K. Qin, “Deep multiple instance convo-\nlutional neural networks for learning robust scene representations,” IEEE\nTrans. Geosci. Remote Sens., vol. 58, no. 5, pp. 3685–3702, May 2020.\n[47] J. Liang, Y . Deng, and D. Zeng, “A deep neural network combined CNN\nand GCN for remote sensing scene classiﬁcation,” IEEE J. Sel. Topics\nAppl. Earth Observ. Remote Sens., vol. 13, pp. 4325–4338, Jul. 2020.\n[48] H. Li et al., “RS-MetaNet: Deep metametric learning for few-shot remote\nsensing scene classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 59,\nno. 8, pp. 6983–6994, Aug. 2021.\n[49] P. Zhang, Y . Bai, D. Wang, B. Bai, and Y . Li, “Few-shot classiﬁcation\nof aerial scene images via meta-learning,” Remote Sens., vol. 13, no. 1,\np. 108, 2021.\n[50] F. Li, R. Feng, W. Han, and L. Wang, “An augmentation attention mecha-\nnism for high-spatial-resolution remote sensing image scene classiﬁca-\ntion,” IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 13,\npp. 3862–3878, 2020.\n[51] Y . Guo, J. Ji, X. Lu, H. Huo, T. Fang, and D. Li, “Global-local attention\nnetwork for aerial scene classiﬁcation,” IEEE Access, vol. 7, pp. 67200–\n67212, 2019.\n[52] X. Tang, Q. Ma, X. Zhang, F. Liu, J. Ma, and L. Jiao, “Attention consistent\nnetwork for remote sensing scene classiﬁcation,” IEEE J. Sel. Topics Appl.\nEarth Observ. Remote Sens., vol. 14, pp. 2030–2045, 2021.\n[53] Y . Yu, X. Li, and F. Liu, “Attention GANs: Unsupervised deep feature\nlearning for aerial scene classiﬁcation,” IEEE Trans. Geosci. Remote Sens.,\nvol. 58, no. 1, pp. 519–531, Jan. 2020.\n[54] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S.\nZagoruyko, “End-to-end object detection with transformers,” in Proc. Eur .\nConf. Comput. Vis., 2020, pp. 213–229.\n[55] M. Chen et al., “Generative pretraining from pixels,” in Proc. Int. Conf.\nMach. Learn, 2020, pp. 1691–1703.\n[56] F. Yang, H. Yang, J. Fu, H. Lu, and B. Guo, “Learning texture transformer\nnetwork for image super-resolution,” in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., 2020, pp. 5791–5800.\n[57] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using\nshifted windows,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2021,\npp. 10012–10022.\n[58] X. Chu et al., “Twins: Revisiting the design of spatial attention in vision\ntransformers,”Adv. Neural Inf. Process. Syst., vol. 34, 2021.\n[59] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y . Wang, “Transformer in\ntransformer,”Adv. Neural Inf. Process. Syst., vol. 34, 2021.\n[60] N. Chen, S. Watanabe, J. A. Villalba, P. Zelasko, and N. Dehak, “Non-\nautoregressive transformer for speech recognition,” IEEE Signal Process.\nLett., vol. 28, pp. 121–125, Dec. 2020.\n[61] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H.\nJégou, “Training data-efﬁcient image transformers & distillation through\nattention,” in Proc. Int. Conf. Mach. Learn., 2021, pp. 10347–10357.\n[62] L. Yuan et al., “Tokens-to-token ViT: Training vision transformers from\nscratch on imagenet,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2021,\npp. 558–567.\n[63] H. Wu et al., “CVT: Introducing convolutions to vision transformers,” in\nProc. IEEE/CVF Int. Conf. Comput. Vis., 2021, pp. 22–31.\n[64] Y . Bazi, L. Bashmal, M. M. A. Rahhal, R. A. Dayil, and N. A. Ajlan,\n“Vision transformers for remote sensing image classiﬁcation,” Remote\nSens., vol. 13, no. 3, p. 516, 2021.\n[65] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk, “SLIC\nsuperpixels compared to state-of-the-art superpixel methods,” IEEE Trans.\nPattern Anal. Mach. Intell., vol. 34, no. 11, pp. 2274–2282, Nov. 2012.\n[66] Y . Li, D. Kong, Y . Zhang, Y . Tan, and L. Chen, “Robust deep alignment\nnetwork with remote sensing knowledge graph for zero-shot and gen-\neralized zero-shot remote sensing image scene classiﬁcation,” ISPRS J.\nPhotogramm. Remote Sens., vol. 179, pp. 145–158, 2021.\n[67] Y . Li, Z. Zhu, J.-G. Yu, and Y . Zhang, “Learning deep cross-modal embed-\nding networks for zero-shot remote sensing image scene classiﬁcation,”\nIEEE Trans. Geosci. Remote Sens., vol. 59, no. 12, pp. 10590–10603,\nDec. 2021.\n[68] A. Paszke et al., “Pytorch: An imperative style, high-performance deep\nlearning library,” Adv. Neural Inf. Process. Syst., vol. 32, 2019.\n[69] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet:\nA large-scale hierarchical image database,” in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit., 2009, pp. 248–255.\n[70] O. Russakovsky et al., “ImageNet large scale visual recognition chal-\nlenge,”Int. J. Comput. Vis., vol. 115, no. 3, pp. 211–252, 2015.\n[71] W. Zhang, P. Tang, and L. Zhao, “Remote sensing image scene classiﬁca-\ntion using CNN-CapsNet,” Remote Sens., vol. 11, no. 5, p. 494, 2019.\n[72] N. He, L. Fang, S. Li, A. Plaza, and J. Plaza, “Remote sensing scene\nclassiﬁcation using multilayer stacked covariance pooling,” IEEE Trans.\nGeosci. Remote Sens., vol. 56, no. 12, pp. 6899–6910, Dec. 2018.\n[73] Q. Bi, K. Qin, Z. Li, H. Zhang, K. Xu, and G.-S. Xia, “A multiple-instance\ndensely-connected ConvNet for aerial scene classiﬁcation,” IEEE Trans.\nImage Process., vol. 29, pp. 4911–4926, Mar. 2020.\n[74] K. Xu, H. Huang, P. Deng, and Y . Li, “Deep feature aggregation frame-\nwork driven by graph convolutional network for scene classiﬁcation in\nremote sensing,” IEEE Trans. Neural Netw. Learn. Syst., to be published,\ndoi: 10.1109/TNNLS.2021.3071369.\n[75] Y . Bazi, M. M. Al Rahhal, H. Alhichri, and N. Alajlan, “Simple yet\neffective ﬁne-tuning of deep cnns using an auxiliary classiﬁcation loss\nfor remote sensing scene classiﬁcation,” Remote Sens., vol. 11, no. 24,\n2019, Art. no. 2908.\n[76] J. Xie, N. He, L. Fang, and A. Plaza, “Scale-free convolutional neural\nnetwork for remote sensing scene classiﬁcation,” IEEE Trans. Geosci.\nRemote Sens., vol. 57, no. 9, pp. 6916–6928, Sep. 2019.\n[77] X. Wang, L. Duan, A. Shi, and H. Zhou, “Multilevel feature fusion net-\nworks with adaptive channel dimensionality reduction for remote sensing\nscene classiﬁcation,” IEEE Geosci. Remote Sens. Lett., vol. 19, Apr. 2021,\nArt. no. 8010205.\n[78] G. Zhang et al., “A multiscale attention network for remote sensing scene\nimages classiﬁcation,” IEEE J. Sel. Topics Appl. Earth Observ. Remote\nSens., vol. 14, pp. 9530–9545, Sep. 2021.\n[79] B. Heo, S. Yun, D. Han, S. Chun, J. Choe, and S. J. Oh, “Rethinking\nspatial dimensions of vision transformers,” in Proc. IEEE/CVF Int. Conf.\nComput. Vis., 2021, pp. 11936–11945.\n[80] W. Wang et al., “Pyramid vision transformer: A versatile backbone for\ndense prediction without convolutions,” in Proc. IEEE/CVF Int. Conf.\nComput. Vis., 2021, pp. 568–578.\n[81] L. Van der Maaten and G. Hinton, “Visualizing data using t-SNE,” J. Mach.\nLearn. Res., vol. 9, no. 11, pp. 2579–2605, 2008.\nJingjing Ma (Member, IEEE) received the B.S. and\nPh.D. degrees in electronic circuit and system from\nXidian University, Xi’an, China, in 2004 and 2012,\nrespectively.\nShe is currently an Associate Professor with the\nKey Laboratory of Intelligent Perception and Image\nUnderstanding, Ministry of Education, Xidian Uni-\nversity. Her current research interests include com-\nputational intelligence and image understanding.\nMingteng Li received the B.E. degree in computer\nscience and technology from the Qingdao University\nof Science and Technology, Qingdao, China, in 2020.\nHe is currently working toward the M.S. degree with\nthe School of Artiﬁcial Intelligence, Xidian Univer-\nsity, Xi’an, China.\nHis research interests include machine learning,\nvision transformer, and remote sensing image pro-\ncessing.\nXu Tang (Senior Member, IEEE) received the B.Sc.,\nM.Sc., and Ph.D. degrees in electronic circuit and\nsystem from Xidian University, Xi’an, China, in 2007,\n2010, and 2017, respectively.\nFrom 2015 to 2016, he was a Joint Ph.D. along with\nProf. W. J. Emery with the University of Colorado\nat Boulder, Boulder, CO, USA. He is currently an\nAssociate Professor with the Key Laboratory of Intel-\nligent Perception and Image Understanding, Ministry\nof Education, Xidian University. His current research\ninterests include remote sensing image content-based\nretrieval and reranking, hyperspectral image processing, remote sensing scene\nclassiﬁcation, object detection, etc.\nMA et al.: HOMO–HETEROGENOUS TRANSFORMER LEARNING FRAMEWORK FOR REMOTE SENSING SCENE CLASSIFICATION 2239\nXiangrong Zhang (Senior Member, IEEE) received\nthe B.S. and M.S. degrees in pattern recognition from\nthe School of Computer Science, Xidian University,\nXi’an, China, in 1999 and 2003, respectively, and the\nPh.D. degree from the School of Electronic Engineer-\ning, Xidian University, in 2006.\nShe is a Professor with the Key Laboratory of\nIntelligent Perception and Image Understanding of\nthe Ministry of Education, Xidian University. From\nJanuary 2015 to March 2016, she was a Visiting\nScientist with the Computer Science and Artiﬁcial\nIntelligence Laboratory, Massachusetts Institute of Technology, Cambridge,\nMA, USA. Her research interests include pattern recognition, machine learning,\nand remote sensing image analysis and understanding.\nFang Liu (Member, IEEE) was born in China, in\n1990. She received the B.S. degree in information and\ncomputing science from Henan University, Kaifeng,\nChina, in 2012, and the Ph.D. degree in intelligent in-\nformation processing from Xidian University, Xi’an,\nChina, in 2018.\nShe is currently an Associate Professor with\nthe Nanjing University of Science and Technology,\nNanjing, China. Her research interests include deep\nlearning, object detection, polarimetric SAR image\nclassiﬁcation, and change detection.\nLicheng Jiao (Fellow, IEEE) received the B.S. degree\nin high voltage from Shanghai Jiao Tong University,\nShanghai, China, in 1982, and the M.S. and Ph.D.\ndegrees in electronic engineering from Xi’an Jiaotong\nUniversity, Xi’an, China, in 1984 and 1990, respec-\ntively.\nFrom 1984 to 1986, he was an Assistant Professor\nwith the Civil Aviation Institute of China, Tianjin,\nChina. From 1990 to 1991, he was a Postdoctoral\nFellow with the Key Laboratory for Radar Signal\nProcessing, Xidian University, Xi’an, where he is\ncurrently the Director with the Key Laboratory of Intelligent Perception and\nImage Understanding of Ministry of Education of China. He has authored or\nco-authored more than 200 scientiﬁc articles. His research interests include\nsignal and image processing, nonlinear circuits and systems theory, wavelet\ntheory, natural computation, and intelligent information processing.\nDr. Jiao is also a member of the IEEE Xian Section Executive Committee\nand an Executive Committee member of the Chinese Association of Artiﬁcial\nIntelligence. He is the Chairman of the Awards and Recognition Committee.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8616912364959717
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7038047909736633
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6653294563293457
    },
    {
      "name": "Exploit",
      "score": 0.5983490943908691
    },
    {
      "name": "Deep learning",
      "score": 0.5383424162864685
    },
    {
      "name": "Feature learning",
      "score": 0.47189009189605713
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.46452438831329346
    },
    {
      "name": "Focus (optics)",
      "score": 0.44261863827705383
    },
    {
      "name": "Transformer",
      "score": 0.43459463119506836
    },
    {
      "name": "Feature extraction",
      "score": 0.4253343641757965
    },
    {
      "name": "Machine learning",
      "score": 0.332683801651001
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I149594827",
      "name": "Xidian University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I36399199",
      "name": "Nanjing University of Science and Technology",
      "country": "CN"
    }
  ]
}