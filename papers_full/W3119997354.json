{
  "title": "Transformers in Vision: A Survey",
  "url": "https://openalex.org/W3119997354",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2483644475",
      "name": "Khan, Salman",
      "affiliations": [
        "Australian National University",
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A4227314091",
      "name": "Naseer, Muzammal",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence",
        "Australian National University"
      ]
    },
    {
      "id": "https://openalex.org/A4222938126",
      "name": "Hayat, Munawar",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A4225782226",
      "name": "Zamir, Syed Waqas",
      "affiliations": [
        "Inception Institute of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A4202207810",
      "name": "Khan, Fahad Shahbaz",
      "affiliations": [
        "Linköping University"
      ]
    },
    {
      "id": "https://openalex.org/A2743042772",
      "name": "Shah, Mubarak",
      "affiliations": [
        "University of Central Florida"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6777047548",
    "https://openalex.org/W6793119350",
    "https://openalex.org/W6776048684",
    "https://openalex.org/W2171943915",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W6765307894",
    "https://openalex.org/W6761837902",
    "https://openalex.org/W3112516115",
    "https://openalex.org/W3179517581",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W6786585107",
    "https://openalex.org/W3111566807",
    "https://openalex.org/W3177313544",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W3009561768",
    "https://openalex.org/W3168489096",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W6761628794",
    "https://openalex.org/W3157528469",
    "https://openalex.org/W4287251194",
    "https://openalex.org/W4214490042",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3044220283",
    "https://openalex.org/W3174738881",
    "https://openalex.org/W6795736418",
    "https://openalex.org/W3111551570",
    "https://openalex.org/W4214614183",
    "https://openalex.org/W3119866685",
    "https://openalex.org/W3091401866",
    "https://openalex.org/W3094751268",
    "https://openalex.org/W4214588794",
    "https://openalex.org/W3035060554",
    "https://openalex.org/W6790690058",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3128723389",
    "https://openalex.org/W2944828972",
    "https://openalex.org/W6772853553",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3190965961",
    "https://openalex.org/W3134144764",
    "https://openalex.org/W3172801447",
    "https://openalex.org/W2911779594",
    "https://openalex.org/W2926645869",
    "https://openalex.org/W3159619744",
    "https://openalex.org/W6955071965",
    "https://openalex.org/W4300852401",
    "https://openalex.org/W2890052321",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2147800946",
    "https://openalex.org/W3113320078",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W3139445856",
    "https://openalex.org/W3175466730",
    "https://openalex.org/W3171547673",
    "https://openalex.org/W2762941833",
    "https://openalex.org/W6791276965",
    "https://openalex.org/W3112160422",
    "https://openalex.org/W3173151551",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W6796750486",
    "https://openalex.org/W3194042166",
    "https://openalex.org/W3023306062",
    "https://openalex.org/W3158846111",
    "https://openalex.org/W6795475546",
    "https://openalex.org/W6790441406",
    "https://openalex.org/W3049455300",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W2173520492",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W6600983433",
    "https://openalex.org/W4214893857",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W2899335602",
    "https://openalex.org/W2975357369",
    "https://openalex.org/W6637162671",
    "https://openalex.org/W6783267081",
    "https://openalex.org/W6763442200",
    "https://openalex.org/W3163465952",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W6792240715",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3035251378",
    "https://openalex.org/W3011199263",
    "https://openalex.org/W3170713111",
    "https://openalex.org/W6779163297",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3190216403",
    "https://openalex.org/W3112776202",
    "https://openalex.org/W6786708909",
    "https://openalex.org/W3166368936",
    "https://openalex.org/W6791943378",
    "https://openalex.org/W6798047393",
    "https://openalex.org/W6794295097",
    "https://openalex.org/W3136416617",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3212228063",
    "https://openalex.org/W2766091292",
    "https://openalex.org/W3139633126",
    "https://openalex.org/W6795463671",
    "https://openalex.org/W6757937817",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2785366763",
    "https://openalex.org/W2963916161",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W3164540605",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2950761309",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W3153842237",
    "https://openalex.org/W2799087757",
    "https://openalex.org/W2405756170",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2489434015",
    "https://openalex.org/W2964134613",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2962931121",
    "https://openalex.org/W2961193895",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W2888101838",
    "https://openalex.org/W2964037671",
    "https://openalex.org/W3034588855",
    "https://openalex.org/W2971155163",
    "https://openalex.org/W2999219213",
    "https://openalex.org/W2799120945",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2980088508",
    "https://openalex.org/W3139049060",
    "https://openalex.org/W2784025607",
    "https://openalex.org/W3172345956",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W3171206729",
    "https://openalex.org/W2747898905",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W2995460200",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2995253937",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3202742610",
    "https://openalex.org/W2337252826",
    "https://openalex.org/W2973857456",
    "https://openalex.org/W3102129360",
    "https://openalex.org/W3211432419",
    "https://openalex.org/W3199093552",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W2968880719",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W2895748257",
    "https://openalex.org/W2990289029",
    "https://openalex.org/W3166398787",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W3173220247",
    "https://openalex.org/W3159778524",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3022265721",
    "https://openalex.org/W3203444029",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W3096739052",
    "https://openalex.org/W3176153963",
    "https://openalex.org/W3177220026",
    "https://openalex.org/W3098085362",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W2109586012",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W3000775737",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W3147387781",
    "https://openalex.org/W3036139671",
    "https://openalex.org/W2185175083",
    "https://openalex.org/W2564591810",
    "https://openalex.org/W3141023492",
    "https://openalex.org/W3164208409",
    "https://openalex.org/W3163747765",
    "https://openalex.org/W3040304705",
    "https://openalex.org/W2593116425",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W3132890542",
    "https://openalex.org/W2963109634",
    "https://openalex.org/W3168101492",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W3172942063",
    "https://openalex.org/W2963121255",
    "https://openalex.org/W3166942762",
    "https://openalex.org/W2340897893",
    "https://openalex.org/W2895240252",
    "https://openalex.org/W2964153729",
    "https://openalex.org/W2964048159",
    "https://openalex.org/W3126337037",
    "https://openalex.org/W3179869055",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2753798143",
    "https://openalex.org/W2937843571",
    "https://openalex.org/W2962964995",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2963420272",
    "https://openalex.org/W3120885796",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W2904946678",
    "https://openalex.org/W2737258237",
    "https://openalex.org/W2952355681",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W2326925005",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W1920022804",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3023371261",
    "https://openalex.org/W3160566314",
    "https://openalex.org/W2795840542",
    "https://openalex.org/W3164024107",
    "https://openalex.org/W3123615524",
    "https://openalex.org/W3175515048",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W2795061970",
    "https://openalex.org/W3170778815",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W3168294587",
    "https://openalex.org/W3122542623",
    "https://openalex.org/W3125056032",
    "https://openalex.org/W2951004968",
    "https://openalex.org/W2963372104",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W3156109214",
    "https://openalex.org/W2951528897",
    "https://openalex.org/W2964024144",
    "https://openalex.org/W3021293129",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W3146091044",
    "https://openalex.org/W2994759459",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W3137963805",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2963115613",
    "https://openalex.org/W3108995912",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W3047848469",
    "https://openalex.org/W3211490618",
    "https://openalex.org/W2619947201",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W2949517790",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W2557283755",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2982723417",
    "https://openalex.org/W2969262604",
    "https://openalex.org/W2963163163",
    "https://openalex.org/W3034312118",
    "https://openalex.org/W3171087525",
    "https://openalex.org/W2963636093",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2944006115",
    "https://openalex.org/W3034609440",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W2885820039",
    "https://openalex.org/W2962746461",
    "https://openalex.org/W2962749806",
    "https://openalex.org/W3199245537",
    "https://openalex.org/W2619697695",
    "https://openalex.org/W3034499084",
    "https://openalex.org/W2321533354",
    "https://openalex.org/W2971074500",
    "https://openalex.org/W3173365702",
    "https://openalex.org/W2101032778",
    "https://openalex.org/W3131149871",
    "https://openalex.org/W3105479157",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W3012126539",
    "https://openalex.org/W2190691619",
    "https://openalex.org/W2941531368",
    "https://openalex.org/W2976847563",
    "https://openalex.org/W3139980562",
    "https://openalex.org/W3173053527",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W2487442924",
    "https://openalex.org/W2962742544",
    "https://openalex.org/W2097073572",
    "https://openalex.org/W3035574324",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W2866634454",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W3161838454",
    "https://openalex.org/W2891158090",
    "https://openalex.org/W2970869018",
    "https://openalex.org/W3106250896",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2251512949",
    "https://openalex.org/W2118858186",
    "https://openalex.org/W2954930822",
    "https://openalex.org/W3035170495",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963978393",
    "https://openalex.org/W2781228439",
    "https://openalex.org/W3101065397",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W2932077855",
    "https://openalex.org/W3113067059",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2963826423",
    "https://openalex.org/W2998108143",
    "https://openalex.org/W3099495704",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W3147852756",
    "https://openalex.org/W2412589713",
    "https://openalex.org/W3204563069",
    "https://openalex.org/W2963037581",
    "https://openalex.org/W2963966654",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W24089286"
  ],
  "abstract": "Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks, e.g., Long short-term memory. Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text, and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization), and three-dimensional analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges toward the application of transformer models in computer vision.",
  "full_text": "1\nTransformers in Vision: A Survey\nSalman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir,\nFahad Shahbaz Khan, and Mubarak Shah\nAbstract—Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their\napplication to computer vision problems. Among their salient beneﬁts, Transformers enable modeling long dependencies between input\nsequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory\n(LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited\nas set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos,\ntext and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge\ndatasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to\nprovide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to\nfundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional feature\nencoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classiﬁcation,\nobject detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual\nreasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image\nsuper-resolution, image enhancement, and colorization) and 3D analysis (e.g., point cloud classiﬁcation and segmentation). We\ncompare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental\nvalue. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further\ninterest in the community to solve current challenges towards the application of transformer models in computer vision.\nIndex Terms—Self-attention, transformers, bidirectional encoders, deep neural networks, convolutional networks, self-supervision.\n!\n1 I NTRODUCTION\nT\nRANSFORMER models [1] have recently demonstrated\nexemplary performance on a broad range of language\ntasks e.g., text classiﬁcation, machine translation [2] and\nquestion answering. Among these models, the most popular\nones include BERT (Bidirectional Encoder Representations\nfrom Transformers) [3], GPT (Generative Pre-trained Trans-\nformer) v1-3 [4]–[6], RoBERTa (Robustly Optimized BERT\nPre-training) [7] and T5 (Text-to-Text Transfer Transformer)\n[8]. The profound impact of Transformer models has become\nmore clear with their scalability to very large capacity mod-\nels [9], [10]. For example, the BERT-large [3] model with\n340 million parameters was signiﬁcantly outperformed by\nthe GPT-3 [6] model with 175 billion parameters while the\nlatest mixture-of-experts Switch transformer [10] scales up\nto a whopping 1.6 trillion parameters!\nThe breakthroughs from Transformer networks in Nat-\nural Language Processing (NLP) domain has sparked great\ninterest in the computer vision community to adapt these\nmodels for vision and multi-modal learning tasks (Fig. 1).\n• S. Khan, M. Naseer and F. S. Khan are with the MBZ University of\nArtiﬁcial Intelligence, Abu Dhabi, UAE.\nE-mail: ﬁrstname.lastname@mbzuai.ac.ae\n• M. Hayat is with the Faculty of IT, Monash University, Clayton VIC\n3800, Australia.\n• S. W. Zamir is with the Inception Institute of Artiﬁcial Intelligence, Abu\nDhabi, UAE.\n• S. Khan and M. Naseer are also with the CECS, Australian National\nUniversity, Canberra ACT 0200, Australia.\n• F. S. Khan is also with the Computer Vision Laboratory, Link¨ oping\nUniversity, Sweden.\n• M. Shah is with the Center for Research in Computer Vision, University\nof Central Florida, Orlando, FL 32816, United States.\nManuscript received March, 2021.\nHowever, visual data follows a typical structure (e.g., spatial\nand temporal coherence), thus demanding novel network\ndesigns and training schemes. As a result, Transformer mod-\nels and their variants have been successfully used for image\nrecognition [11], [12], object detection [13], [14], segmenta-\ntion [15], image super-resolution [16], video understanding\n[17], [18], image generation [19], text-image synthesis [20]\nand visual question answering [21], [22], among several\nother use cases [23]–[26]. This survey aims to cover such\nrecent and exciting efforts in the computer vision domain,\nproviding a comprehensive reference to interested readers.\nTransformer architectures are based on a self-attention\nmechanism that learns the relationships between elements\nof a sequence. As opposed to recurrent networks that pro-\ncess sequence elements recursively and can only attend to\nshort-term context, Transformers can attend to complete\nsequences thereby learning long-range relationships. Al-\nthough attention models have been extensively used in\nboth feed-forward and recurrent networks [27], [28], Trans-\nformers are based solely on the attention mechanism and\nhave a unique implementation (i.e., multi-head attention)\noptimized for parallelization. An important feature of these\nmodels is their scalability to high-complexity models and\nlarge-scale datasets e.g., in comparison to some of the other\nalternatives such as hard attention [29] which is stochastic in\nnature and requires Monte Carlo sampling for sampling at-\ntention locations. Since Transformers assume minimal prior\nknowledge about the structure of the problem as compared\nto their convolutional and recurrent counterparts [30]–[32],\nthey are typically pre-trained using pretext tasks on large-\nscale (unlabelled) datasets [1], [3]. Such a pre-training avoids\ncostly manual annotations, thereby encoding highly expres-\narXiv:2101.01169v5  [cs.CV]  19 Jan 2022\n2\nFig. 1: Statistics on the number of times keywords such as BERT, Self-Attention, and Transformers appear in the titles of Peer-\nreviewed and arXiv papers over the past few years (in Computer Vision and Machine Learning). The plots show consistent growth\nin recent literature. This survey covers recent progress on Transformers in the computer vision domain.\nsive and generalizable representations that model rich rela-\ntionships between the entities present in a given dataset. The\nlearned representations are then ﬁne-tuned on the down-\nstream tasks in a supervised manner to obtain favorable\nresults.\nThis paper provides a holistic overview of the trans-\nformer models developed for computer vision applications.\nWe develop a taxonomy of the network design space and\nhighlight the major strengths and shortcomings of the ex-\nisting methods. Other literature reviews mainly focus on\nthe NLP domain [33], [34] or cover generic attention-based\napproaches [27], [33]. By focusing on the newly emerging\narea of visual transformers, we comprehensively organize\nthe recent approaches according to the intrinsic features of\nself-attention and the investigated task. We ﬁrst provide an\nintroduction to the salient concepts underlying Transformer\nnetworks and then elaborate on the speciﬁcs of recent vision\ntransformers. Where ever possible, we draw parallels be-\ntween the Transformers used in the NLP domain [1] and the\nones developed for vision problems to ﬂash major novelties\nand interesting domain-speciﬁc insights. Recent approaches\nshow that convolution operations can be fully replaced\nwith attention-based transformer modules and have also\nbeen used jointly in a single design to encourage symbiosis\nbetween the two complementary set of operations. This sur-\nvey ﬁnally details open research questions with an outlook\ntowards the possible future work.\n2 F OUNDATIONS\nThere exist two key ideas that have contributed towards\nthe development of conventional transformer models. (a)\nThe ﬁrst one is self-attention, which allows capturing ‘long-\nterm’ dependencies between sequence elements as com-\npared to conventional recurrent models that ﬁnd it chal-\nlenging to encode such relationships. (b) The second key\nidea is that of pre-training1 on a large (un)labelled corpus in\na (self)supervised manner, and subsequently ﬁne-tuning to\nthe target task with a small labeled dataset [3], [7], [38]. Be-\nlow, we provide a brief tutorial on these two ideas (Sec. 2.2\nand 2.1), along with a summary of seminal Transformer\n1. Several recent Vision Transformers demonstrate that the model\ncan be learned end-to-end on ImageNet-1K without any dedicated pre-\ntraining phase [35]–[37]. However, the performance generally remains\nlower than the pre-trained counter-parts.\nFig. 2: An example self-attention block used in the vision\ndomain [39]. Given the input sequence of image features, the\ntriplet of (key, query, value) is calculated followed by attention\ncalculation and applying it to reweight the values. A single\nhead is shown here and an output projection ( W) is ﬁnally\napplied to obtain output features with the same dimension as\nthe input. Figure adapted from [39].\nnetworks (Sec. 2.3 and 2.4) where these ideas have been\napplied. This background will help us better understand\nthe forthcoming Transformer based models used in the\ncomputer vision domain (Sec. 3).\n2.1 Self-Attention in Transformers\nGiven a sequence of items, self-attention estimates the rel-\nevance of one item to other items (e.g., which words are\nlikely to come together in a sentence). The self-attention\nmechanism is an integral component of Transformers, which\nexplicitly models the interactions between all entities of a\nsequence for structured prediction tasks. Basically, a self-\nattention layer updates each component of a sequence by\naggregating global information from the complete input\nsequence. Lets denote a sequence ofn entities (x1, x2, ···xn)\nby X ∈Rn×d, where d is the embedding dimension to rep-\nresent each entity. The goal of self-attention is to capture the\ninteraction amongst all n entities by encoding each entity\nin terms of the global contextual information. This is done\nby deﬁning three learnable weight matrices to transform\nQueries ( WQ ∈Rd×dq ), Keys ( WK ∈Rd×dk ) and Values\n(WV ∈Rd×dv ), where dq = dk. The input sequence X is\nﬁrst projected onto these weight matrices to get Q = XWQ,\nK = XWK and V = XWV. The output Z ∈Rn×dv of the\nself attention layer is,\nZ = softmax\n(\nQKT\n√\ndq\n)\nV.\n3\nFig. 3: Architecture of the Transformer Model [1]. The model was ﬁrst developed for the language translation task where an input\nsequence in one language is required to be converted to the output sequence in another language. The Transformer encoder\n(middle row) operates on the input language sequence and converts it to an embedding before passing it on to the encoder blocks.\nThe Transformer decoder (bottom row) operates on the previously generated outputs in the translated language and the encoded\ninput sequence from the middle branch to output the next word in the output sequence. The sequence of previous outputs (used\nas input to the decoder) is obtained by shifting the output sentence to the right by one position and appending start-of-sentence\ntoken at the beginning. This shifting avoids the model to learn to simply copy the decoder input to the output. The ground-truth\nto train the model is simply the output language sequence (without any right shift) appended with an end-of-sentence token. The\nblocks consisting of multi-head attention (top row) and feed-forward layers are repeated N times in both the encoder and decoder.\nFor a given entity in the sequence, the self-attention basi-\ncally computes the dot-product of the query with all keys,\nwhich is then normalized using softmax operator to get the\nattention scores. Each entity then becomes the weighted sum\nof all entities in the sequence, where weights are given by\nthe attention scores (Fig. 2 and Fig. 3, top row-left block).\nMasked Self-Attention: The standard self-attention\nlayer attends to all entities. For the Transformer model [1]\nwhich is trained to predict the next entity of the sequence,\nthe self-attention blocks used in the decoder are masked to\nprevent attending to the subsequent future entities. This is\nsimply done by an element-wise multiplication operation\nwith a mask M ∈Rn×n, where M is an upper-triangular\nmatrix. The masked self-attention is deﬁned by,\nsoftmax\n(\nQKT\n√\ndq\n◦M\n)\n,\nwhere ◦denotes Hadamard product. Basically, while pre-\ndicting an entity in the sequence, the attention scores of the\nfuture entities are set to zero in masked self-attention.\nMulti-Head Attention:In order to encapsulate multiple\ncomplex relationships amongst different elements in the\nsequence, the multi-head attention comprises multiple self-\nattention blocks ( h = 8 in the original Transformer model\n[1]). Each block has its own set of learnable weight ma-\ntrices {WQi , WKi , WVi }, where i = 0···(h−1). For an\ninput X, the output of the h self-attention blocks in multi-\nhead attention is then concatenated into a single matrix\n[Z0, Z1, ···Zh−1] ∈ Rn×h·dv and projected onto a weight\nmatrix W ∈Rh·dv×d (Fig. 3, top row).\nThe main difference of self-attention with convolution\noperation is that the ﬁlters are dynamically calculated in-\nstead of static ﬁlters (that stay the same for any input) as in\nthe case of convolution. Further, self-attention is invariant\nto permutations and changes in the number of input points.\nAs a result, it can easily operate on irregular inputs as op-\nposed to standard convolution that requires grid structure.\nFurthermore, it has been shown in the literature how self-\nattention (with positional encodings) is theoretically a more\nﬂexible operation which can model the behaviour of convo-\nlutional models towards encoding local features [40]. Cor-\ndonnier et al. [41] further studied the relationships between\nself-attention and convolution operations. Their empirical\nresults conﬁrm that multi-head self-attention (with sufﬁcient\nparameters) is a more generic operation which can model\nthe expressiveness of convolution as a special case. In fact,\nself-attention provides the capability to learn the global as\nwell as local features, and provide expressivity to adaptively\nlearn kernel weights as well as the receptive ﬁeld (similar to\ndeformable convolutions [42]).\n2.2 (Self) Supervised Pre-training\nSelf-attention based Transformer models generally operate\nin a two-stage training mechanism. First, pre-training is\nperformed on a large-scale dataset (and sometimes a com-\nbination of several available datasets [22], [43]) in either a\nsupervised [11] or a self-supervised manner [3], [44], [45].\nLater, the pre-trained weights are adapted to the down-\nstream tasks using small-mid scale datasets. Examples of\ndownstream tasks include image classiﬁcation [46], ob-\nject detection [13], zero-shot classiﬁcation [20], question-\nanswering [10] and action recognition [18]. The effective-\nness of pre-training for large-scale Transformers has been\nadvocated in both the language and vision domains. For\n4\nexample, Vision Transformer model (ViT-L) [11] experiences\nan absolute 13% drop in accuracy on ImageNet test set\nwhen trained only on ImageNet train set as compared to the\ncase when pretrained on JFT dataset [47] with 300 million\nimages.\nSince acquiring manual labels at a massive scale is cum-\nbersome, self-supervised learning has been very effectively\nused in the pre-training stage. The self-supervision based\npre-training stage training has played a crucial role in un-\nleashing the scalability and generalization of Transformer\nnetworks, enabling training even above a trillion parame-\nter networks (e.g., the latest Switch Transformer [10] from\nGoogle). An extensive survey on SSL can be found in [48],\n[49]. As nicely summarized by Y. LeCun [50], the basic\nidea of SSL is to ﬁll in the blanks , i.e., try to predict the\noccluded data in images, future or past frames in temporal\nvideo sequences or predict a pretext task e.g., the amount\nof rotation applied to inputs, the permutation applied to\nimage patches or the color of a gray-scale image. Another\neffective way to impose self-supervised constraints is via\ncontrastive learning. In this case, nuisance transformations\nare used to create two types of modiﬁed versions of the same\nimage i.e., without changing the underlying class semantics\n(e.g., image stylizing, cropping) and with semantic changes\n(e.g., replacing an object with another in the same scene, or\nchanging the class with minor adversarial changes to the\nimage). Subsequently, the model is trained to be invariant to\nthe nuisance transformations and emphasize on modeling\nminor changes that can alter semantic labels.\nSelf-supervised learning provides a promising learning\nparadigm since it enables learning from a vast amount of\nreadily available non-annotated data. In the SSL based pre-\ntraining stage, a model is trained to learn a meaningful\nrepresentation of the underlying data by solving a pretext\ntask. The pseudo-labels for the pretext task are automati-\ncally generated (without requiring any expensive manual\nannotations) based on data attributes and task deﬁnition.\nTherefore, the pretext task deﬁnition is a critical choice in\nSSL. We can broadly categorize existing SSL methods based\nupon their pretext tasks into (a) generative approaches which\nsynthesize images or videos (given conditional inputs), (b)\ncontext-based methods which exploit the relationships be-\ntween image patches or video frames, and (c) cross-modal\nmethods which leverage from multiple data modalities.\nExamples of generative approaches include conditional gen-\neration tasks such as masked image modeling [43] and\nimage colorization [51], image super-resolution [52], image\nin-painting [53], and GANs based methods [54], [55]. The\ncontext-based pretext methods solve problems such as a\njigsaw puzzle on image patches [56]–[58], masked object\nclassiﬁcation [22], predict geometric transformation such as\nrotation [46], [59], or verify temporal sequence of video\nframes [60]–[62]. Cross-modal pretext methods verify the\ncorrespondence of two input modalities e.g., text & image\n[63], audio & video [64], [65] or RGB & ﬂow [66].\n2.3 Transformer Model\nThe architecture of the Transformer model proposed in [1]\nis shown in Fig. 3. It has an encoder-decoder structure. The\nencoder ( middle row) consists of six identical blocks (i.e.,\nN=6 in Fig. 3), with each block having two sub-layers: a\nmulti-head self-attention network, and a simple position-\nwise fully connected feed-forward network. Residual con-\nnections [67] alongside layer normalization [68] are em-\nployed after each block as in Fig. 3. Note that, different from\nregular convolutional networks where feature aggregation\nand feature transformation are simultaneously performed\n(e.g., with a convolution layer followed by a non-linearity),\nthese two steps are decoupled in the Transformer model\ni.e., self-attention layer only performs aggregation while the\nfeed-forward layer performs transformation. Similar to the\nencoder, the decoder (bottom row) in the Transformer model\ncomprises six identical blocks. Each decoder block has three\nsub-layers, ﬁrst two (multi-head self-attention, and feed-\nforward) are similar to the encoder, while the third sub-\nlayer performs multi-head attention on the outputs of the\ncorresponding encoder block, as shown in Fig. 3.\nThe original Transformer model in [1] was trained for\nthe Machine Translation task. The input to the encoder is\na sequence of words (sentence) in one language. Positional\nencodings are added to the input sequence to capture the\nrelative position of each word in the sequence. Positional\nencodings have the same dimensions as the input d = 512,\nand can be learned or pre-deﬁned e.g., by sine or cosine\nfunctions. Being an auto-regressive model, the decoder of\nthe Transformer [1] uses previous predictions to output the\nnext word in the sequence. The decoder, therefore, takes\ninputs from the encoder as well as the previous outputs\nto predict the next word of the sentence in the translated\nlanguage. To facilitate residual connections the output di-\nmensions of all layers are kept the same i.e., d = 512.\nThe dimensions of query, key and value weight matrices\nin multi-head attention are set to dq = 64, dk = 64, dv = 64.\n2.4 Bidirectional Representations\nThe training strategy of the original Transformer model [1]\ncould only attend to the context on the left of a given word\nin the sentence. This is limiting, since for most language\ntasks, contextual information from both left and right sides\nis important. Bidirectional Encoder Representations from\nTransformers (BERT) [3] proposed to jointly encode the right\nand left context of a word in a sentence, thus improving\nthe learned feature representations for textual data in an\nself-supervised manner. To this end, BERT [3] introduced\ntwo pretext tasks to pre-train the Transformer model [1] in\na self-supervised manner: Masked Language Model and Next\nSentence Prediction. For adapting the pre-trained model for\ndownstream tasks, a task-speciﬁc additional output module\nis appended to the pre-trained model, and the full model\nis ﬁne-tuned end-to-end. Here, we brieﬂy touch upon the\npretext tasks. (1) Masked Language Model (MLM) - A\nﬁxed percentage (15%) of words in a sentence are randomly\nmasked and the model is trained to predict these masked\nwords using cross-entropy loss. In predicting the masked\nwords, the model learns to incorporate the bidirectional\ncontext. (2) Next Sentence Prediction (NSP) -Given a pair\nof sentences, the model predicts a binary label i.e., whether\nthe pair is valid from the original document or not. The\ntraining data for this can easily be generated from any\nmonolingual text corpus. A pair of sentences A and B is\n5\nFig. 4: A taxonomy of self-attention design space . Existing approaches based on self-attention explore single-head or multi-head\n(transformer) designs for vision tasks. We note that interesting efforts have been made to utilize knowledge from convolution\nbased architectures to improve ViTs (e.g., multi-scale and hybrid designs). We categorize the upcoming sections of this survey\naccording to the types of self-attention block ( left tree diagram) as well as the prominent tasks in computer vision ( right).\nformed, such that B is the actual sentence (next to A) 50% of\nthe time, and B is a random sentence for other 50% of the\ntime. NSP enables the model to capture sentence-to-sentence\nrelationships which are crucial in many language modeling\ntasks such as Question Answering and Natural Language\nInference.\n3 S ELF -ATTENTION & TRANSFORMERS IN VISION\nWe broadly categorize vision models with self-attention\ninto two categories: the models which use single-head self-\nattention (Sec. 3.1), and the models which employ multi-\nhead self-attention based Transformer modules into their\narchitectures (Sec. 3.2). Below, we ﬁrst discuss the ﬁrst\ncategory of single-head self-attention based frameworks,\nwhich generally apply global or local self-attention within\nCNN architectures, or utilize matrix factorization to enhance\ndesign efﬁciency and use vectorized attention models. We\nthen discuss the Transformer-based vision architectures in\nSec. 3.2.\n3.1 Single-head Self-Attention\n3.1.1 Self-Attention in CNNs\nInspired by non-local means operation [69] which was\nmainly designed for image denoising, Wang et al. [70] pro-\nposed a differentiable non-local operation for deep neural\nnetworks to capture long-range dependencies both in space\nand time in a feed-forward fashion. Given a feature map,\ntheir proposed operator [70] computes the response at a\nposition as a weighted sum of the features at all positions\nin the feature map. This way, the non-local operation is\nable to capture interactions between any two positions in\nthe feature map regardless of the distance between them.\nVideos classiﬁcation is an example of a task where long-\nrange interactions between pixels exist both in space and\ntime. Equipped with the capability to model long-range\ninteractions, [70] demonstrated the superiority of non-local\ndeep neural networks for more accurate video classiﬁcation\non Kinetics dataset [71].\n(a) Non-local block [70]\n (b) Criss-cross attention [72]\nFig. 5: Comparison of two different self-attention approaches:\nNon-local self-attention block [70] and Criss-cross self-attention\nmodule [72]. Figure is from [72].\nAlthough the self-attention allows us to model full-\nimage contextual information, it is both memory and com-\npute intensive. As shown in Fig. 5(a), in order to encode\nglobal context for a given pixel location, non-local block [70]\ncomputes a dense attention map (in green). The non-local\nblock [70] has a high complexity of O(N2), where N de-\nnotes the number of input feature maps. To reduce this\ncomputational burden, Huang et al. [72] propose the criss-\ncross attention module that for each pixel position generates\na sparse attention map only on the criss-cross path, as illus-\ntrated in Fig. 5(b). Further, by applying criss-cross attention\nrecurrently, each pixel position can capture context from all\nother pixels. Compared to non-local block, the criss-cross\nuses 11 × lesser GPU memory, and has a complexity of\nO(2\n√\nN). State-of-the-art results are reported [72] for the\nsemantic and instance segmentation tasks on several bench-\nmark datasets including Cityscapes [73], ADE20K [74],\nCOCO [75], LIP [76] and CamVid [77].\nAnother shortcoming of the convolutional operator\ncomes from the fact that after training, it applies ﬁxed\nweights regardless of any changes to the visual input. Hu\net al. [78] proposed local relation networks to adaptively\ncompose pixels in a local window. They introduced a new\ndifferentiable layer that adapts its weight aggregation based\non the compositional relations (similarity) between pix-\nels/features within a local window. Such adaptive weight\naggregation introduces geometric priors into the network\nwhich are important for the recognition tasks [78]. Convo-\nlution is considered to be a top-down operator as it remains\n6\nﬁxed across positions while a non-local operation such as\nintroduced in [69] is a bottom-up method as it aggregates\ninput features over the full image. The local relation layer\nbelongs to the category of bottom-up methods but it is\nrestricted to a ﬁxed window size e.g., 7x7 neighborhood.\nBello et al. [79] explore the possibility of employing\nself-attention as an alternative to convolutional operators.\nThey employ the relative position encoding [80] in two\ndimensions to develop a new self-attention mechanism that\nmaintains translation equivariance, a desirable property for\nhandling images. Although this self-attention provides com-\npetitive results as a stand-alone computational primitive,\nthe best performance is obtained in combination with the\nconvolutional operations. Authors show that attention aug-\nmentation leads to systematic performance gains in image\nclassiﬁcation and object detection for different architectures.\n3.1.2 Self-Attention as Stand-alone Primitive\nAs discussed above, convolutional layers possess transla-\ntion equivariance but can not scale with a large receptive\nﬁeld, therefore can not capture long-range interactions [81].\nOn the other hand, global attention [1] which attend to\nall spatial locations of the input can be computationally\nintensive and is preferred on down-sampled small images,\nimage patches [11] or augmenting the convolutional features\nspace [79]. Ramachandran et al. [81] proposed to replace\nconvolutional layers in deep neural networks with a local\nself-attention layer which can be applied to small or large\ninputs without increasing the computational cost. At a basic\nlevel, the proposed self-attention layer [81] considers all\npixel positions in a speciﬁc window size around a given\npixel, compute queries, keys and value vectors for these\npixels, and then aggregates the spatial information within\nthis window. The value vectors are aggregated after pro-\njecting the softmax score of queries and keys. This process\nis repeated for all given pixels and the response is concate-\nnated to produce the output pixel. ResNet models with local\nself-attention layer can solve ImageNet and COCO object\ndetection with fewer parameters as compared to ResNet\nmodels based on convolutional layers [81].\nZhao et al. [82] note that a traditional convolution\noperator performs feature aggregation and transformation\njointly (by applying a ﬁlter and then passing it through\na non-linearity). In contrast, they propose to perform fea-\nture aggregation separately with self-attention followed by\ntransformation using an element-wise perceptron layer. For\nfeature aggregation, they propose two alternate strategies:\n(a) pairwise self-attention and (b) patch-wise self-attention.\nThe pairwise self-attention is permutation and cardinality\ninvariant operation, while the patch-wise self-attention does\nnot have such invariance properties (similar to convolu-\ntion). Both pairwise and patch-wise self-attentions are im-\nplemented as a vector attention [82] that learns weights for\nboth the spatial and channel dimensions. This provides an\nalternate approach for attention that is conventionally per-\nformed using scalar weights (by taking a dot-product). The\npairwise self-attention is a set operator that computes a vec-\ntor attention keeping in view the relationships of a particular\nfeature with its neighbors in a given local neighborhood.\nIn contrast, patch-wise self-attention is a generalization of\nthe convolution operator (not a set operator) and looks at\nall the feature vectors in the local neighbourhood when\nderiving the attention vectors. Authors show that with con-\nsiderably fewer parameters, self-attention networks (SAN)\ncan beat ResNet baselines on the ImageNet dataset. They\nfurther show robustness against adversarial perturbations\n[83], [84] and generalization to unseen transformations [85].\nThis behaviour is due to the dynamic nature of attention\nthat makes it difﬁcult for the adversary to calculate useful\nfooling directions.\n3.2 Multi-head Self-Attention (Transformers)\nUnlike the approaches discussed in Sec. 3.1 which insert\nself-attention as a component in CNN inspired architectures,\nVision Transformer (ViTs) [11] adapts the architecture of [1]\n(see Fig. 3), which cascades multiple Transformer layers.\nViTs have gained signiﬁcant research attention, and a num-\nber of recent approaches have been proposed which build\nupon ViTs. Below, we discuss these methods by categorizing\nthem into: uniform scale ViTs having single-scale features\nthrough all layers (Sec. 3.2.1), multi-scale ViTs that learn\nhierarchical features which are more suitable for dense\nprediction tasks (Sec. 3.2.2), and hybrid designs having\nconvolution operations within ViTs (Sec. 3.2.3).\n3.2.1 Uniform-scale Vision Transformers\nThe original Vision Transformer [11] model belongs to this\nfamily, where the multi-head self-attention is applied to a\nconsistent scale in the input image where the spatial scale is\nmaintained through the network hierarchy. We name such\nmodels as the uniform-scale ViTs, as described below.\nVision Transformer (ViT) [11] (Fig. 6) is the ﬁrst work\nto showcase how Transformers can ‘altogether’ replace\nstandard convolutions in deep neural networks on large-\nscale image datasets. They applied the original Transformer\nmodel [1] (with minimal changes) on a sequence of image\n’patches’ ﬂattend as vectors. The model was pre-trained\non a large propriety dataset (JFT dataset [47] with 300\nmillion images) and then ﬁne-tuned to downstream recog-\nnition benchmarks e.g., ImageNet classiﬁcation. This is an\nimportant step since pre-training ViT on a medium-range\ndataset would not give competitive results, because the\nCNNs encode prior knowledge about the images (inductive\nbiases e.g., translation equivariance) that reduces the need of\ndata as compared to Transformers which must discover such\ninformation from very large-scale data. Notably, compared\nto the iGPT [19] model that also applied Transformers to\nfull-sized images but performs training as a generative task,\nViT pre-trains the model with a supervised classiﬁcation\ntask (although a self-supervision variant is also explored\nwhich results in a less performance).\nThe DeiT [12] is the ﬁrst work to demonstrate that\nTransformers can be learned on mid-sized datasets (i.e., 1.2\nmillion ImageNet examples compared to 300 million images\nof JFT [11] used in ViT [11]) in relatively shorter training\nepisodes. Besides using augmentation and regularization\nprocedures common in CNNs, the main contribution of\nDeiT [12] is a novel native distillation approach for Trans-\nformers which uses a CNN as a teacher model (RegNetY-\n16GF [86]) to train the Transformer model. The outputs\nfrom the CNN aid the Transformer in efﬁciently ﬁguring\n7\n  \nFig. 6: An overview of Vision Transformer (on the left) and the\ndetails of Transformer encoder (on the right). The architecture\nresembles Transformers used in the NLP domain and the image\npatches are simply fed to the model after ﬂattening. After\ntraining, the feature obtained from the ﬁrst token position is\nused for classiﬁcation. Image obtained from [11].\nout useful representations for input images. A distillation\ntoken is appended with the input patch embeddings and\nthe class token. The self-attention layers operate on these\ntokens to learn their inter-dependencies and outputs the\nlearned class, patch, and distillation tokens. The network is\ntrained with a cross-entropy loss deﬁned on the output class\ntoken and a distillation loss to match the distillation token\nwith the teacher output. Both soft and hard label choices\nwere explored for distillation, where the hard distillation\nwas found to perform better. Interestingly, the learned class\nand distillation tokens do not exhibit a high correlation indi-\ncating their complementary nature. The learned representa-\ntions compare favorably well against top-performing CNN\narchitectures such as EfﬁcientNet [87] and also generalize\nwell for a number of downstream recognition tasks.\nToken to Token (T2T) ViT [35] recursively combines\nneighboring tokens into a single token to reduce tokens\nlength and aggregate spatial context. Transformer in Trans-\nformer [88] computes attention at two levels: patch-level\n(as done is standard ViTs [11]) and local sub-patch-level\n(e.g.by subdividing a 16 ×16 patch into four 4 ×4 blocks,\nand computing attention amongst these blocks). In token\nlabelling ViT [89], all patch tokens contribute towards loss\ncalculation, different from regular ViTs that only use clas-\nsiﬁcation token in the loss. This process includes auxiliary\nsupervision where each image-patch (token) is labeled using\na pre-trained CNN model. Similar to CutMix augmentation\n[90], tokens from different images are mixed as an augmen-\ntation strategy, and the model is trained using the standard\nclassiﬁcation loss and auxiliary token-label loss. Their model\ndemonstrates excellent performance specially for smaller\nsized models.\nThe quadratic complexity of self-attention hinders its\napplicability to longer sequences (high-resolution images).\nCross-Covariance Image Transformers (XCiT) [91] incor-\nporate attention across feature-channels instead of to-\nkens, i.e., their cross-covariance attention is given by\nVsoftmax\n(\nKT QT\n√τ\n)\n. The proposed cross-covariance atten-\ntion has linear complexity (since it depends upon feature\ndimension instead of the number of tokens). XCiT can\ntherefore handle large resolution images and demonstrate\nexcellent performance across different vision tasks i.e., self-\nsupervised and fully supervised image classiﬁcation and\ndense prediction (detection, segmentation). DeepViT [92]\nobserves that the similarity between attention maps of\ndeeper layer is high and hinders scaling models depth.\nThey propose to re-attend the attention maps in a multi-\nhead block instead of simple aggregation of these attention\nmaps, and show consistent gains over standard multi-head\nself attention based ViTs.\n3.2.2 Multi-scale Vision Transformers\nIn standard ViTs, the number of the tokens and token feature\ndimension are kept ﬁxed throughout different blocks of\nthe network. This is limiting, since the model is unable\nto capture ﬁne spatial details at different scales. Initial\nTransformer based dense prediction methods (e.g., DETR\n[13]) therefore have a convolutional backend. Multi-stage\nhierarchical design for ViTs, where number of tokens is\ngradually reduced while the token feature dimension is\nprogressively increased, has been shown to produce ef-\nfective features for dense prediction tasks [36], [93]–[96].\nThese models generally also perform well for recognition\ntasks. These architectures mostly sparsify tokens by merg-\ning neighboring tokens and projecting them to a higher\ndimensional feature space. Examples of multi-stage ViTs\ninclude Pyramid ViT [93], [97], Twins [37], CoaT [98], Swin\nTransformer [36], Convolutional vision Transformer (CvT)\n[96], Shufﬂe Transformer [95], CrossFormer [99], RegionViT\n[100] and Focal Transformer models [94]. Some of them are\nhybrid designs (with both convolution and self-attention\noperations, see Sec. 3.2.3), while others only employ pure\nself-attention based design (discussed next).\nPyramid ViT (PVT) [93] is the ﬁrst hierarchical design\nfor ViT, and proposes a progressive shrinking pyramid\nand spatial-reduction attention. PVTv2 [97] and SegFormer\n[101] improve original PVT [93] by introducing overlapping\npatch embedding, depth-wise convolution, and efﬁcient\nattention. Swin Transformer [36] has a multi-stage hierar-\nchical architecture which computes attention within a local\nwindow, by partitioning the window into multiple sub-\npatches. To capture interactions between different windows\n(image locations), window partitioning is gradually shifted,\nalong the hierarchy of the network, to capture overlapping\nregions. Focal Transformer models [94] is another hierar-\nchical design, where focal self-attention is introduced to\nsimultaneously capture global and local relationships. Simi-\nlarly, CrossFormer [99] has a hierarchical pyramid structure,\nand introduces cross-scale embedding module, along-with\nlong short distance attention and dynamic position bias\nto faithfully capture both local and global visual cues.\nRegionViT [100] proposes a regional-to-local attention to\nencode hierarchical features. Multi-Scale Vision Longformer\n[102] also considers a local context in self-attention, but\nemploys the efﬁcient Longformer [103] design for self-\nattention. CrossViT [104] encodes multi-scale features with\ntwo branches (each with multiple transformer blocks), by\nseparately processesing smaller and larger image patches.\nThe information from these two multi-scale bracnches is\nthen fused together using a cross-attention module.\n8\n3.2.3 Hybrid ViTs with Convolutions\nConvolutions do an excellent job at capturing low-level local\nfeatures in images, and have been explored in multiple hy-\nbrid ViT designs, specially at the beginning to “patchify and\ntokenize” an input image. For example, Convolutional vi-\nsion Transformer (CvT) [96] incorporate convolution based\nprojection to capture the spatial structure and low-level\ndetails, for tokenization of image patches. CvT has a hier-\narchical design, where number of tokens is progressively re-\nduced while the token-width is increased, thus imitating the\nimpact of spatial downsampling as in CNNs. Convolution\nenhanced image Transformers [105] employ convolutions\nbased image-to-token module to extract low-level features.\nCompact Convolutional Transformer (CCT) [106] introduces\na new sequence pooling scheme, and incorporates convolu-\ntional blocks (conv-pool-reshape) for tokenization. CCT can\nbe trained from scratch on smaller datasets, e.g., CIFAR10\nwith ∼95% accuracy, which is a remarkable property not\npossible with the traditional ViTs.\nLocalViT [107] introduces depthwise convolutions to en-\nhance local features modeling capability of ViTs. LeViT [108]\n(name inspired from LeNet [109]) applies a four-layered\nCNN block (with 3 ×3 convolutions) at the beginning with\nprogressively increasing channels (3,32,64,128,256). For a\n3×224×224 input image, the resulting 256×14×14 output\nfrom the CNN block becomes input to a hierarchical ViT.\nBy virtue of its design, LeViT is 5×faster than EfﬁcientNet\n[87] on CPU, at inference. ResT [110] is another hierarchical\narchitecture which applies a CNN block at the beginning for\npatch-embedding. It incorporates depth-wise convolutions\nand adaptive position encoding to tackle varying image\nsizes. A recent approach NesT [111] proposes a simple\ntechnique to introduce hierarchy in ViTs. NesT divides an\nimage into non-overlapping blocks (each block is further\nsplit into patches). It ﬁrst separately applies local self-\nattention on patches within each block, and then enables\nglobal interaction between blocks by aggregating them into\nan image space and applying convolution operation, fol-\nlowed by downsampling. The number of blocks is gradually\nreduced along the hierarchy of the model, while number\nof local-patches is kept ﬁxed. This simple scheme performs\nfavorably compared with more sophisticated designs [36],\n[97], and enables training NesT on smaller datasets (e.g.,\nCIFAR-10) from scratch.\nDepthwise Convolution and self-Attention Networks\n(CoAtNets) [112] introduce a relative attention mod-\nule (which combines depthwise convolutions and self-\nattention), and vertically stack convolution and attention\nlayers. CoAtNets demonstrate an impressive 86% Ima-\ngeNet top-1 accuracy without extra data (i.e. trained only\non ImageNet-1k). Shufﬂe Transformer [95] performs self-\nattention within a window and has depth-wise convolutions\nbetween the window-based multi-head self-attention and\nMLP . It introduces a shufﬂe operation to build stronger\ncross-patch connections. Co-scale conv-attentional image\nTransformers (CoaT) [98], is a hybrid hierarchical pyramid\ndesign, with serial and parallel blocks, where the serial\nblock is similar to standard transformer block except for\nthe attention layer replaced with depthwise convolution.\nThe parallel blocks is applied on the output of serial blocks\nand encodes relationships between tokens at multiple scales\nusing cross-attention. Twins [37] builds upon PVT [93] (an\nattention only pyramid design), by replacing the absolute\nposition embedding in PVT with relative conditional po-\nsition embedding [113], and incorporating the separable\ndepth-wise convolutions instead of the standard spatial\nattention, to capture local and global context of the image. In\nthis sense, the hybrid designs tend to combine the strengths\nof both convolution and transformer models. TransCNN\n[114] propose a hierarchical multi-head self attention block,\nwhich ﬁrst learns interactions within small grids (tokens)\nusing self-attention, and then gradually merges the smaller\ngrids into larger grids. The proposed block can then be\nplugged into existing CNN architectures.\n3.2.4 Self-Supervised Vision Transformers\nContrastive learning based self-supervised approaches,\nwhich have gained signiﬁcant success for CNN based vision\ntasks, have also been investigated for ViTs. Chen et al. [115]\nevaluate different self-supervised frameworks and propose\npractical strategies including MoCo v3 (extended from\nv1/v2 [116], [117]) for stabilized training of self-supervised\nViTs. Xieet al. [118] combine MoCo v2 [117] and BYOL [119]\nto train DeiT [12] and SwinTransformer [36]. They demon-\nstrate generalization of self-supervised SwinTransformer\nfor dense prediction tasks of detection and segmentation.\nSelf distillation with no labels (DINO) [120] demonstrate\nthat self-supervised ViTs can automatically segment the\nbackground pixels of an image, even though they were\nnever trained using pixel-level supervision, a phenomena\notherwise not observed in CNNs or fully supervised ViTs.\nEfﬁcient self-supervised vision transformer (EsViT) [121]\npropose a multi-stage design, where neighboring tokens are\ngradually merged along the hierarchy of the network, and\nuse DINO for self-supervision. Apart from standard image-\nlevel self-supervision as in DINO, they incorporate addi-\ntional patch-level self-supervision in which correspondence\nis promoted between similar patches within augmented\nversions of an image. EsViT demonstrates excellent perfor-\nmance under self-supervision settings, and its off-the-shelf\nfeatures transfer better than supervised SwinTransformer on\n17 out of 18 evaluated datasets.\n3.3 Transformers for Object Detection\nTransformers based modules have been used for object\ndetection in the following manner: (a) Transformer back-\nbones for feature extraction, with a R-CNN based head\nfor detection (see Sec. 3.2.2), (b) CNN backbone for visual\nfeatures and a Transformer based decoder for object detec-\ntion [13], [14], [122], [123] (see Sec. 3.3.1, and (c) a purely\ntransformer based design for end-to-end object detection\n[124] (see Sec. 3.3.2).\n3.3.1 Detection Transformers with CNN Backbone\nDetection Transformer (DETR) [13] treats object detection\nas a set prediction task i.e., given a set of image features,\nthe objective is to predict the set of object bounding boxes.\nThe Transformer model enables the prediction of a set of\nobjects (in a single shot) and also allows modeling their\nrelationships. DETR adapts a set loss function which allows\n9\nFig. 7: Detection Transformer (DETR) [13] treats the object\ndetection task as a set prediction problem and uses the Trans-\nformer network to encode relationships between set elements.\nA bipartite set loss is used to uniquely match the box predic-\ntions with the ground-truth boxes (shown on the right two\ncolumns). In case of no match, a ’ no object ’ class prediction\nis selected. Its simple design with minimal problem-speciﬁc\nmodiﬁcations can beat a carefully built and popular Faster R-\nCNN model. Figure from [13].\nbipartite matching between predictions and ground-truth\nboxes. The main advantage of DETR is that it removes\nthe dependence on hand-crafted modules and operations,\nsuch as the RPN (region proposal network) and NMS (non-\nmaximal suppression) commonly used in object detection\n[125]–[129]. In this manner, the dependence on prior knowl-\nedge and careful engineering design is relaxed for complex\nstructured tasks like object detection.\nGiven spatial feature maps from the CNN backbone, the\nencoder ﬁrst ﬂattens the spatial dimensions (see Fig. 7). This\ngives a sequence of features d ×n, where d is the feature\ndimension and n = h ×w with h, wbeing the height\nand width of the spatial feature maps. These features are\nthen encoded and decoded using multi-head self-attention\nmodules as in [1]. The main difference in the decoding\nstage is that all boxes are predicted in parallel while [1]\nuses an RNN to predict sequence elements one by one.\nSince the encoder and decoder are permutation invariant,\nlearned positional encodings are used as the object queries\nby the decoder to generate different boxes. Note that the\nspatial structure in a CNN detector (e.g., Faster R-CNN)\nautomatically encodes the positional information. DETR\nobtains performance comparable to the popular Faster R-\nCNN model [125] which is an impressive feat given its\nsimple design. The DETR has also been extended to inter-\nesting applications in other domains, e.g., Cell-DETR [130]\nextends it for instance segmentation of biological cells. A\ndedicated attention branch is added to obtain instance-wise\nsegmentations in addition box predictions that are enhanced\nwith a CNN decoder to generate accurate instance masks.\nThe DETR [13] model successfully combines convolu-\ntional networks with Transformers [1] to remove hand-\ncrafted design requirements and achieves an end-to-end\ntrainable object detection pipeline. However, it struggles\nto detect small objects and suffers from slow convergence\nand a relatively high computational cost [14]. DETR maps\nimages to features space before using the Transformer for\nthe relation modeling. Thus, the computational cost of self-\nattention grows quadratically with the spatial size of the\nfeature map i.e., O(H2W2C), where H and W represent\nthe height and width of the feature map. This inherently\nputs a limitation on the use of multi-scale hierarchical\nfeatures [131] in DETR training framework which is ulti-\nmately important to detect small objects. Furthermore, at the\nbeginning of training, the attention module simply projects\nuniform attention to all the locations of the feature map and\nFig. 8: Axial attention module [133] that sequentially applies\nmulti-head axial attention operations along height and width\naxes. Image from [133].\nrequires a large number of training epochs to tune attention\nweights to converge to meaningfully sparse locations. This\napproach contributes to a slow convergence rate of DETR.\nTo mitigate the above-mentioned issues, [14] proposed a\ndeformable attention module to process the feature maps.\nInspired from deformable convolutions [42], deformable\nattention module [14] only attends to sparse set of elements\nfrom the whole feature map regardless of its spatial size.\nThis further allows cross-scale aggregation of feature maps\nwith the help of multi-scale attention modules without\nincreasing the computational cost signiﬁcantly. Deformable\nDETR not only performs better but its training time also\nremains 10 × lower than the original DETR model [14].\nAnchor DETR [122] replaces the learnable query tokens in\n[13] with anchor-point based queries, such that each query\nfocuses on predicting the object near the anchor point. The\nanchor points can be ﬁxed on 2D grid, or learned from\nuniformly distributed points. Anchor DETR [122] requires\n10 ×fewer training epochs with comparable performance.\nPix2Seq [123] is a generic Transformer-based framework,\nwithout any specialized task-speciﬁc modules, and learns\nto directly produce a sequence of tokens with object de-\nscriptions (bounding-boxes and class-labels). A quantization\nand serialization scheme ﬁrst converts bounding boxes and\nclass-labels into a sequence of discrete tokens. A generic\nTransformer based encoder-decoder network is then used\nto generate these tokens in an auto-regressive manner con-\nditioned on previous predictions and image features.\n3.3.2 Detection with Pure Transformers\nYou Only Look at One Sequence (YOLOS) [124] is a sim-\nple, attention-only architecture directly built upon the ViT\n[1], [132]. It replaces the class-token in ViT with multiple\nlearnable object query tokens, and the bipartite matching\nloss is used for object detection similar to [13]. YOLOS\ndemonstrates the ﬂexibility of ViTs to object detection, in a\npure sequence-to-sequence learning manner, with minimal\nimage related 2D inductive biases. In similar spirit, PVT [93]\nis combined with DETR [13] to perform object detection\nwith an end-to-end transformer pipeline. We note that it is\nfeasible to combine other recent ViTs with transformer based\ndetection heads as well to create pure ViT based designs\n[124], and we hope to see more such efforts in future.\n3.4 Transformers for Segmentation\nSelf-attention can be leveraged for dense prediction tasks\nlike image segmentation that requires modeling rich interac-\ntions between pixels. Below, we discuss axial self-attention\n10\n[133], a cross-modal approach [15] that can segment regions\ncorresponding to a given language expression, and ViTs\nbased segmentation architectures [101], [134], [135].\nPanoptic segmentation [136] aims to jointly solve the\notherwise distinct tasks of semantic segmentation and in-\nstance segmentation by assigning each pixel a semantic label\nand an instance id. Global context can provide useful cues\nto deal with such a complex visual understanding task.\nSelf-attention is effective at modeling long-range contextual\ninformation, albeit applying it to large inputs for a dense\nprediction task like panoptic segmentation is prohibitively\nexpensive. A naive solution is to apply self-attention either\nto downsampled inputs or to limited regions around each\npixel [81]. Even after introducing these constraints, the self-\nattention still has quadratic complexity and sacriﬁces the\nglobal context. To tackle these issues, Wang et al. [133]\npropose the position-sensitive axial-attention where the 2D\nself-attention mechanism is reformulated as two 1D axial-\nattention layers, applied to height-axis and width-axis se-\nquentially (see Fig. 8). The axial-attention is compute efﬁ-\ncient and enables models to capture the full-image context.\nIt achieves competitive performance for the panoptic seg-\nmentation task on COCO [75], Mapillary Vistas [137], and\nCityscapes [73] benchmarks and for the image classiﬁcation\non ImageNet dataset [138].\nCross-modal Self-attention (CMSA) [15] encodes long-\nrange multi-modal dependencies between linguistic and\nvisual features for referring image segmentation task, that aims\nto segment entities in an image referred by a language\ndescription.For this purpose, a set of cross-modal features is\nobtained by concatenating image features with each word\nembedding and the spatial coordinate features. The self-\nattention operates on these features and generates attention\nover the image corresponding to each word in the sentence.\nThe segmentation network then performs self-attention at\nmultiple spatial levels and uses a gated multi-level fusion\nmodule to reﬁne segmentation masks via information ex-\nchange across multi-resolution features. A binary CE loss is\nused to train the overall model that achieves good improve-\nments on UNC [139], G-Ref [140] and ReferIt [141] datasets.\nWhile the segmentation approaches discussed above in-\nsert self-attention in their CNN based architectures, some\nrecent works have proposed transformer based encoder-\ndecoder architectures. Segmentation Transformer (SETR)\n[134] has a ViT encoder, and two decoder designs based\nupon progressive upsampling, and multi-level feature ag-\ngregation. SegFormer [101] has a hierarchical pyramid ViT\n[93] (without position encoding) as an encoder, and a simple\nMLP based decoder with upsampling operation to get the\nsegmentation mask. Segmenter [135] uses ViT encoder to\nextract image features, and the decoder is a mask Trans-\nformer module which predicts segmentation masks, using\nlearnable mask tokens and image-patch tokens as inputs.\nThe authors also propose a baseline linear decoder which\nprojects the patch-embeddings to classiﬁcation space, thus\nproducing coarse patch-level labels.\n3.5 Transformers for Image and Scene Generation\nHere, we discuss Transformer-based architectures [23],\n[142]–[146] for image synthesis, which is interesting from\nFig. 9: (a) Self-attention block in Image Transformer [142].\nGiven one channel for a pixel q, the block attends to the mem-\nory of previous synthesized pixels ( mi), followed by a feed-\nforward sub-network. Positional encodings pi are added in the\nﬁrst layer. (b) The operation performed in Local Self-Attention\n(example of a 2D case is shown). The image is partitioned into\na grid of spatial blocks known as query blocks. In the self-\nattention operation, each pixel in a query block attends to all\npixels in the memory block (shown in cyan rectangle). White\ngrid locations show masked inputs that have zero-contribution\ntowards the self-attention.\nthe perspective of generative modeling and learning unsu-\npervised representations for down-stream tasks.\nParmar et al. [142] develop an image generation model\nthat can sequentially predict each pixel of an output image\ngiven its previously generated pixels (Fig. 9). Their approach\nmodels the joint distribution of the image pixels by factor-\nizing it as a product of pixel-wise conditional distributions.\nPreviously developed auto-regressive models for this task,\nsuch as the PixelCNN [147], suffer from a limited receptive\nﬁeld which hinders in modeling long term relationships in\nan image e.g., part relationships or occlusions. Using self-\nattention, [142] enhances the receptive ﬁeld without incur-\nring a high computational cost ( e.g., effective receptive ﬁeld\nup to 256 pixels can be achieved as compared to 25 pixels\nof PixelCNN [147]). The generative pipeline was also tested\non conditional generation tasks e.g., image super-resolution,\nimage completion, and denoising.\nInspired by the success of GPT model [5] in the lan-\nguage domain, image GPT (iGPT) [143] demonstrated that\nsuch models can be directly used for image generation\ntasks, and to learn strong features for downstream vision\ntasks (e.g., image classiﬁcation). Speciﬁcally, iGPT trains\nGPT v2 model [5] on ﬂattened image sequences (1D pixel\narrays) and shows that it can generate plausible image\noutputs without any external supervision. The generated\nsamples depict the model’s ability to understand spatial\nrelationships between pixels and high-level attributes such\nas object classes, texture, and scale. Notably, the design\n11\ndoes not use any image-speciﬁc knowledge in the design\n(e.g., the 2D position embeddings used in Image Trans-\nformer [142]). The features learned with iGPT’s unsuper-\nvised training mechanism compete impressively against\nother unsupervised approaches, achieving state-of-the-art\nperformance on CIFAR-10/100 [148] and STL [149] datasets\nwhile performing comparably to SimCLR (a contrastive\nlearning approach) [150] on ImageNet dataset. This is an\nastounding result, since the iGPT architecture is exactly the\nsame as used for language modeling tasks, and therefore it\ndoes not incorporate any prior domain-speciﬁc knowledge.\nNotably, the competing unsupervised CNN based solutions\nwidely adopt such priors in the form of architectural design,\nattention mechanisms, loss functions, and regularization\n[117], [151]–[154]. However, on the downside, iGPT has a\nhigh compute cost e.g., iGPT-L version has roughly 36×\nhigh training cost compared to MoCo [117] which is a\nstate of the art self-supervised feature learning approach.\nFor this reason, the training was generally limited to low-\nresolution of ≤64 ×64, while convolutional architectures\ncan effectively learn from high-resolution inputs.\nTransformers typically incur a high compute cost when\napplied on high-dimensional sequences. To overcome this\nlimitation, Esser et al. [144] proposed to include inductive\nbiases (commonly used in the CNNs) alongside Transform-\ners to improve their efﬁciency. Speciﬁcally, local connectivity\nand spatial invariance biases inbuilt in the CNN structure\nare leveraged by learning a rich dictionary of visual patterns\n(using a Generative Adversarial approach). A Transformer\nis then used to learn the long-range interactions between\nthe dictionary items to generate the outputs. In turn, they\ndevelop a conditional image generation model capable of\nproducing very high-resolution images (up to megapixel\nrange) using Transformers. This is the ﬁrst work that\ndemonstrates the application of Transformers to generate\nsuch high-resolution images.\nGenerative Adversarial Networks (GANs) [54] with\nCNNs as default backbone have been very successful for\nvisually appealing image synthesis [155]–[157]. TransGAN\n[145] builds a strong GAN model, free of any convolution\noperation, with both generator and discriminator based\nupon the Transformer model [1]. The architecture of both\ngenerator and discriminator is based upon the encoder in\noriginal Transformer model [1]. For memory efﬁciency, the\ngenerator contains multiple stages, with up-sampling mod-\nules in-between, which gradually increase the resolution\nof feature maps (input sequence length) while reducing\nthe embedding dimension. The discriminator of TransGAN\ntakes ﬂattened image-patches as tokens similar to [132].\nAuthors introduce different training techniques including\ndata augmentation, training with an auxiliary task and\ninjecting locality to self-attention to scale-up their model\nfor high quality image synthesis [144]. The TransGAN\nmodel achieves state-of-the-art results in terms of Inception\nScore and Fr ´echet Inception Distance (FID) on STL-10 and\nperforms favorably compared with their CNN-based GAN\ncounterparts on other datasets.\nUnlike previous image generation methods [142]–[144],\nwhich directly predict image outputs, [23] learns to generate\nparameters of 3D objects to be placed in a given scene.\nSpeciﬁcally, SceneFormer [23] studies the 3D room layout\nconditioned scene generation task. Given the empty room\nshape, [23] can propose new object conﬁgurations in the\nroom while maintaining realism. Remarkably, the model\ndoes not use any appearance information and only learns to\ngenerate new scenes by modeling the inter-object relation-\nships using self-attention in Transformers. Similar to how\na Transformer operates on a sentence, it is applied to a\nsequence of objects to predict the next suitable object in a\nscene. Speciﬁcally, the size, pose, location, and category of\nthe next object is predicted by the Transformer model. A\nstart token indicates the initiation of inference and the num-\nber of output token indicate the objects generated by the\nmodel in a sequence. The authors also explore generating\nnew scenes given a textual description of the room layout.\nThe independence from the appearance makes the approach\nefﬁcient, enabling interactive scene generation.\nThe task of generating realistic images from text is inter-\nesting and practically valuable ( e.g., for artistic content cre-\nation), but at the same time highly challenging. Prior text-to-\nimage synthesis approaches [158]–[161] are mostly based on\nGANs [54]. Although these methods produce encouraging\nresults, they are far from being photo-realistic. Ramesh et\nal. [20] recently proposed DALL·E which is a Transformer\nmodel capable of generating high-ﬁdelity images from a\ngiven text description. DALL·E model has 12 billion param-\neters and it is trained on a large set of text-image pairs taken\nfrom the internet. Before training, images are ﬁrst resized\nto 256 ×256 resolution, and subsequently compressed to\na 32 ×32 grid of latent codes using a pre-trained discrete\nvariational autoencoder [162], [163]. DALL·E takes as input\na single stream of 1280 tokens (256 for the text and 1024\nfor the image), and is trained to generate all other tokens\nautoregressively (one after another). It provides ﬂexibility\nto generate images either from scratch (Fig. 10a) or by\nextending existing images (Fig. 10b), while staying faithful\nto the text caption.\nThe authors demonstrate the effectiveness of DALL·E by\ncreating images from text describing a wide variety of real\nand ﬁctional concepts. While generating images purely from\ntextural captions, DALL·E shows impressive performance at\ncontrolling multiple objects and their attributes (Fig. 10c),\nrendering certain viewpoint (Fig. 10d), capturing object’s\ninternal structure (Fig. 10e), and combining unrelated ob-\njects (Fig. 10f). Furthermore, DALL·E can perform image-to-\nimage translation (Fig. 10g) guided by the input text.\n3.6 Transformers for Low-level Vision\nAfter witnessing the success of Transformer models in high-\nlevel vision problems, numerous Transformer-based meth-\nods have been proposed for low-level vision tasks, including\nimage super-resolution [16], [19], [164], denoising [19], [165],\nderaining [19], [165], and colorization [24]. Image restoration\nrequires pixel-to-pixel correspondence from the input to the\noutput images. One major goal of restoration algorithms\nis to preserve desired ﬁne image details (such as edges\nand texture) in the restored images. CNNs achieve this by\nemploying a single-scale architecture design that does not\ninvolve any downsampling operation. Since the computa-\ntional complexity of self-attention in Transformer models\nincreases quadratically with number of image patches, it is\n12\n(a)\n (b)\n (c)\n (d)\n (e)\n (f)\n (g)\nFig. 10: Images generated by DALL·E [20] from the following text prompts. (a) An armchair in the shape of an avocado. (b) A photo\nof San Francisco’s golden gate bridge.Given a part of the image (in green box), DALL·E performs the image completion. (c) An emoji\nof a baby penguin wearing a blue hat, red gloves, green shirt, and yellow pants. (d) An extreme close-up view of a capybara sitting in a ﬁeld.\n(e) A cross-section view of a pomegranate. (f) A penguin made of watermelon. (g) The exact same cat on the top as a sketch on the bottom.\ninfeasible to develop Transformer model that can operate on\nsingle-scale feature processing pipeline. Consequently, these\nTransformer-based image restoration models make use of\nvarious strategies to reduce the computational burden, such\nas computing attention on local image windows [164], per-\nforming spatial reduction attention [166], and employing\nencoder-decoder design [19], [165]. Here, we brieﬂy discuss\na few image restoration Transformer models.\n3.6.1 Transformers for Image Processing Tasks\nTop performing algorithms for high-level computer vision\ntasks such as object detection and semantic segmentation\noften employ backbone models that are pre-trained on large-\nscale datasets e.g., ImageNet. In contrast, algorithms for low-\nlevel vision tasks such as image denoising, super-resolution,\nand deraining are directly trained on task-speciﬁc data,\nthereby suffer from these limitations:(i) small number of im-\nages available in task-speciﬁc datasets ( e.g., the commonly\nused DIV2K dataset for image super-resolution contains\nonly 2000 images), (ii) the model trained for one image\nprocessing task does not adapt well to other related tasks.\nChen et al. [19] propose a pre-trained model based on\nTransformer architecture, named as Image Processing Trans-\nformer (IPT). It is capable of performing various image\nrestoration tasks such as super-resolution, denoising, and\nderaining. The overall architecture of IPT consists of multi-\nheads and multi-tails to deal with different tasks separately,\nand a shared encoder-decoder Transformer body. Since ex-\nploiting Transformers at full potential requires training on\nlarge-scale data, [19] takes the clean (ground-truth) images\nfrom the ImageNet benchmark and synthesize their de-\ngraded versions for different tasks. For example, bicubic in-\nterpolation is used for generating low-resolution images, ad-\nditive white Gaussian noise is added to prepare noisy data,\nand hand-crafted rain streaks are applied to obtain rainy\nimages. In total, 10 million images are used to pre-train the\nIPT model. During training, each task-speciﬁc head takes as\ninput a degraded image and generates visual features. These\nfeature maps are divided into small crops and subsequently\nﬂattened before feeding them to the Transformer encoder\n(whose architecture is the same as [1]). The outputs of the\nencoder along with the task-speciﬁc embeddings are given\nas input to the Transformer decoder. The features from the\ndecoder output are reshaped and passed to the multi-tail\nthat yields restored images. The IPT model is optimized\nwith L1 loss. Experimental results show that the pre-trained\nIPT model, when ﬁne-tuned for a speciﬁc low-level vision\ntask, can provide signiﬁcant performance gains over the\nstate-of-the-art methods [167]–[169].\n3.6.2 Transformers for Super-Resolution\nRecent years have seen major performance breakthroughs\nfor super-resolution (SR) due to convolutional neural net-\nworks (CNNs). Principally, the quality of super-resolved\nimages generated by CNNs is dependent on the choice of\noptimization objective. While the SR methods [167], [170]–\n[173] that are based on pixel-wise loss functions ( e.g., L1,\nMSE, etc.) yield impressive results in terms of image ﬁ-\ndelity metrics such as PSNR and SSIM, they struggle to\nrecover ﬁne texture details and often produce images that\nare overly-smooth and perceptually less pleasant. Further,\nperceptual SR approaches [52], [174]–[177], in addition to\nper-pixel loss, employ adversarial loss [54] and perceptual\nloss [178] based on deep features extracted from pre-trained\nCNNs. While these methods generate images that are sharp,\nvisually pleasant, and perceptually plausible, they show a\nsubstantial decrease in reconstruction accuracy measured in\nPSNR/SSIM. Moreover, the perceptual SR algorithms have a\ntendency to hallucinate fake textures and cause artifacts. The\nabove mentioned SR approaches follow two distinct (but\nconﬂicting) research directions: one maximizing the recon-\nstruction accuracy and the other maximizing the perceptual\nquality, but never both.\nTo alleviate the trade-off between perceptual reproduc-\ntion and accurate reproduction, Yang et al. [16] propose a\nTransformer network (TTSR) for super-resolution. During\ntraining, TTSR uses paired LR-HR images, as well as ref-\nerence (Ref) images with similar content as of LR images.\nTTSR learns to search relevant regions in the Ref image and\ntransfers rich textures to help super-resolving the input LR\nimage. The texture Transformer module of TTSR method\n(see Fig. 11) consists of four core components: (1) Learnable\ntexture extractor: takes as input LR ↑, Ref↓↑, and Ref images,\nand generates texture features query (Q), key (K), and value\n(V), respectively. Here,↑denotes bicubic upsampling opera-\ntion, and ↓↑represents bicubic down-sampling followed by\nan upsampling operation. (2) Relevance embedding: ﬁrst un-\nfolds Q and K into patches and then computes the similarity\nof each patch in Q with each patch in K in order to generate\nhard and soft attention maps. (3) Hard-attention: transfers\nHR texture features from V to (LR features) Q using the hard\nattention map. (4) Soft-attention: further enhances relevant\nfeatures while suppressing less relevant ones.\nWhile TTSR [16] method deals with reference-based\nimage super-resolution, most of the research is conducted\n13\nFig. 11: Diagram of the texture Transformer module.Q (query),\nK (key) and V (value) represent texture features extracted from\na (bicubic upsampled) low-resolution image, a sequentially\ndown/upsampled reference image, and an original reference\nimage, respectively. The relevance embedding aims to estimate\nsimilarity between low-resolution and reference images. H\nand S respectively denote hard and soft attentions computed\nfrom relevance embedding. T indicates high-resolution texture\nfeatures that are then transferred to the features F of low-\nresolution image. Figure is from [16].\non single image super-resolution problem in which only\nLR-HR paired images are available. Since the computa-\ntional complexity of the original self-attention operation\nis prohibitively high for high-resolution images, recently\na few efﬁcient transformer models have been proposed\nthat employ window-based attention (SwinIR [164]) and\nspatial resolution reduction operation in attention module\n(ESRT [166]) to perform super-resolution.\n3.6.3 Colorization Transformer\nGiven a grayscale image, colorization seeks to produce the\ncorresponding colorized sample. It is a one-to-many task as\nfor a given grayscale input, there exist many possibilities\nin the colorized output space. The challenging nature of\nthis task requires probabilistic models capable of produc-\ning multiple colorized output samples. Colorization Trans-\nformer [24] is a probabilistic model based on conditional\nattention mechanism [179]. It divides the image colorization\ntask into three sub-problems and proposes to solve each\ntask sequentially by a different Transformer network. The\nauthors ﬁrst train a Transformer network to map a low-\nresolution grey-scale image to a 3-bit low-resolution col-\nored image. Low-resolution images in turn allow training\nof larger models. The 3-bit low-resolution colored image\nis then upsampled to an 8-bit RGB sample by another\nTransformer network in the second stage of training. Finally,\na third stage Transformer is trained to increase the spatial\nresolution of the 8-bit RGB sample produced by the second-\nstage Transformer. Self-attention used in the colorization\nTransformer is based on row/column attention layers intro-\nduced in [179]. These layers capture the interaction between\neach pixel of an input image while being computation-\nally less costly. The row-wise attention layer applies self-\nattention to all pixels in a given row, while the column-wise\nattention layer considers pixels only in a given column of\nan image. This work [24] is the ﬁrst successful application\nof Transformers trained to colorize grey-scale images at high\n(256×256) resolution.\n3.7 Transformers for Multi-Modal Tasks\nTransformer models have also been extensively used for\nvision-language tasks such as visual question answering\n(VQA) [183], visual commonsense reasoning (VSR) [184],\ncross-modal retrieval [185] and image captioning [29]. Sev-\neral works in this direction target effective vision-language\npre-training (VLP) on large-scale multi-modal datasets to\nlearn generic representations that effectively encode cross-\nmodality relationships ( e.g., grounding semantic attributes\nof a person in a given image). These representations can\nthen be transferred to downstream tasks, often obtaining\nstate of the art results. Notably, several of these models\nstill use CNNs as vision backbone to extract visual features\nwhile Transformers are used mainly used to encode text\nfollowed by the fusion of language and visual features.\nSuch models generally apply the vanilla multi-layer Trans-\nformer [1] with multi-modal inputs and do not introduce\nfundamental changes to the core attention block. However,\ntheir main distinction is in the conﬁguration of Transformers\nand the loss functions, based on which we categorize them\ninto: (a) Multi-stream Transformers (see Sec. 3.7.1) and (b)\nSingle-stream Transformers (see Sec. 3.7.2). The single-stream\ndesigns feed the multi-modal inputs to a single Transformer\nwhile the multi-stream designs ﬁrst use independent Trans-\nformers for each modality and later learn cross-modal repre-\nsentations using another Transformer (see Fig. 12). Besides\nthese vision language pretraining methods, we also explain\nvisual grounding approaches towards the end of this section\n(see Sec. 3.7.3).\n3.7.1 Multi-stream Transformers\nVision and Language BERT (ViLBERT) [63] was the ﬁrst\nextension of the BERT model to the multi-modal domain.\nThe goal was to learn representations that can jointly model\nimages and natural language. For this purpose, ViLBERT\ndeveloped a two-stream architecture where each stream is\ndedicated to model the vision or language inputs (Fig. 12-h).\nThe architecture of both parallel streams is a series of Trans-\nformer blocks similar to the BERT model. Subsequently, co-\nattentional Transformer layers are applied to learn cross-\nmodal relationships. The co-attentional framework is very\nsimple. Query, key, and value matrices are computed for\neach modality in the standard way [1] and then key-value\npairs for one modality are passed on to the other modality’s\nattention head.\nViLBERT applies VLP on a set of proxy tasks deﬁned on\nthe Conceptual Concepts dataset (with 3.3M images with\nweak captions) and later ﬁne-tune the model on down-\nstream tasks such as VQA. The pre-training phase oper-\nates in a self-supervised manner, i.e., pretext tasks are cre-\nated without manual labeling on the large-scale unlabelled\ndataset. These pretext tasks include predicting whether the\ntext and image inputs are related and predicting the seman-\ntics of masked image regions and textual inputs (e.g., similar\n14\nFig. 12: An overview of Transformer models used for multi-modal tasks in computer vision. The Transformer designs in this\ncategory can be grouped into single-stream (UNITER [43], OSCAR [44], VideoBERT [17], Unicoder-VL [180], VisualBERT [63] and\nVL-BERT [22]) and dual-stream architectures (LXMERT [21], ViLBERT [181] and PEMT [182]). A key distinction between models\nis the choice of loss functions. While most of the multi-modal methods are focused on images as visual data, VideoBERT [17] and\nPEMT [182] are designed to work on video streams and leverage unique modalities e.g., audio signals in videos [182].\nto reconstructing masked words in text in the BERT model\n[3]). This way, the model learns the inherent structure in\nthe data during pre-training and also models cross-domain\nassociations. With evaluations on several tasks, [17] demon-\nstrated that a two-stream model can perform better than a\nsingle-stream model that uses shared parameters to model\nboth language and vision domains [17].\nSimilar to ViLBERT [181], Learning Cross-Modality En-\ncoder Representations from Transformers (LXMERT) [21]\nalso uses a two-stream architecture based on BERT frame-\nwork. The main difference lies in the object-relationship\nencoder that is used to model the visual features instead\nof simple image-level features used in ViLBERT. The infor-\nmation in two streams is then fused across modalities using\ncross-attention blocks similar to [181].\nCompared to two pre-texts tasks used for VLP in [181],\nLXMERT uses ﬁve pre-training tasks including masked ob-\nject and language prediction, cross-modality matching, and\nvisual question answering (Fig. 12-g). The pre-trained model\nis ﬁne-tuned on the VQA task, however, a high similarity\nbetween pre-training and ﬁne-tuned tasks raises questions\non the generalizability of the learned representations to new\ntasks. To this end, the authors conducted generalization\nexperiments on Visual Reasoning for Real (NLVR) task [186]\ndemonstrating impressive improvements on novel tasks.\nLee et al. [182] note that the multi-modal representation\nlearning approaches like VideoBERT [17] and ViLBERT [181]\ngenerally keep the language processing part ﬁxed to a pre-\ntrained model ( e.g., BERT [3]) to reduce training complex-\nity. For the ﬁrst time in the literature, they propose to\nlearn an end-to-end multi-modal bidirectional Transformer\nmodel called PEMT on audio-visual data from unlabeled\nvideos. First, short-term ( e.g., 1-3 seconds) video dynamics\nare encoded using CNNs, followed by a modality-speciﬁc\nTransformer (audio/visual) to model long-term dependen-\ncies ( e.g., 30 seconds). A multi-modal Transformer is then\napplied to the modality-speciﬁc Transformer outputs to ex-\nchange information across visual-linguistic domains. How-\never, learning such a model in a naive form would incur\nhuge memory requirements. To reduce parametric complex-\nity, the parameters are shared across layers within each\nTransformer which leads upto 80% parameter reduction.\nThe Transformer is trained using a contrastive learning ap-\nproach based on a content-aware negative sampling (Fig. 12-\ni). Speciﬁcally, the model uses the features obtained from\nCNNs learned during the training phase to select negative\n15\nsamples that are visually similar to the positive instances.\nThis work also compares various fusion strategies adopted\nin earlier works such as early (VideoBERT [17] and VL-\nBERT [22]), mid-level (ViL-BERT [181] and LXMERT [21])\nand late fusion mechanisms and shows that the mid-level\nfusion is the optimal choice. The proposed model is pre-\ntrained on Kinetics-700 [187] dataset and later ﬁne-tuned on\ndownstream video classiﬁcation tasks such as short video\nclassiﬁcation on UCF101 [188], audio classiﬁcation on ESC50\n[189] and long-term action recognition on Charades [190]\nand Kinetics-Sounds [65] datasets.\nTan and Bansal [191] introduce the concept of ‘ vokens’\n(images related to language tokens extracted from sen-\ntences). The vokens (visualized tokens) provide visual su-\npervision to the language model to learn better features. The\nmotivation is that humans learn languages by correlating\nvisual information with semantic concepts. In a similar spirit\nto other self-supervised language representation learning\nmethods [3], [181], they learn representations by deﬁning\nan auxiliary task of voken-prediction task. Since the exist-\ning datasets encode limited visually grounded tokens, they\npropose a vokenization method to map language tokens to\nvisual vokens, as illustrated in Fig. 13. The approach uses\nlanguage-based retrieval for such a mapping and transfers\na model trained on a small labeled dataset (MS-COCO) to a\nlarge dataset (Wikipedia). Furthermore, it was ensured that\nthe sentence-wide context is considered to obtain the token-\nvoken mapping. The resulting model trained using gener-\nated tokens outperforms the state of the art BERT model on\na diverse set of NLP tasks. In this sense, the proposed model\ndoes not evaluate vision tasks, however, uses vision as a\nuseful grounding cue to train the language model, hence we\ninclude it in the multi-modal representation learning group.\nVision-and-Language Navigation (VLN) aims to predict\na navigation plan on a map based on the vision and\nlanguage inputs. Transformer models were used earlier in\n[192], [193] for VLN task. These works ﬁrst pre-train a cross-\nmodal Transformer using self-supervision on vision and\nlanguage pairs and subsequently ﬁne-tune on the speciﬁc\nVLN tasks. While these works learn attention between im-\nage region and language, Chen et al. [194] propose to learn\ncross-modal attention between language inputs and spatial\ntopological maps (to represent an agent’s environment as\na graph whose nodes denote places and the edges denote\ntheir connectivity). Given the topological map and natural\nlanguage inputs, a VLN task using the Transformer model\nbears resemblance to sequence prediction in NLP . Specif-\nically, at each time instance, the cross-modal Transformer\npredicts a single node of the topological map in the nav-\nigation plan. The individual language and map encodings\nare ﬁrst processed using uni-modal encoders and later a\ncross-modal encoder (similar to LXMERT [21]) is applied\nto aggregate information across modalities. To denote posi-\ntions in the map, a learned trajectory position encoding is\nappended with the map features. Based on this Transformer\nsetup, [194] reports a full navigation system that can freely\nexplore the environment and intelligently plan its actions.\nCLIP [195] is a contrastive approach to learn image rep-\nresentations from text, with a learning objective which max-\nimizes similarity of correct text-image pairs embeddings in\na large batch size. Speciﬁcally, given a batch of N image-\ntext pairs, CLIP learns a multi-modal embedding space, by\njointly training an image-encoder and a text-encoder, such\nthat the cosine similarity of the valid N image-text pairs is\nmaximized, while the remaining N2 −N pairs is minimized.\nThe authors consider ResNet-50 [67] and Vision Transformer\n(ViT) [132] for encoding images. The modiﬁed Transformer\nmodel [1] as in [5] is employed for encoding text. CLIP is\ntrained on a large corpus of 400 million image-text pairs and\ndemonstrates excellent zero-shot transfer capabilities. At\ninference, the names of classes are used as input to the text-\nencoder, and similarity of the encoded image is computed\nwith all encoded texts (classes) to ﬁnd the image-text pair\nwith highest match. The CLIP achieves an astounding zero-\nshot classiﬁcation accuracy of 75% on ImageNet, without us-\ning an supervision from ImageNet training set. The authors\nfurther demonstrate zero-shot transfer capabilities of the\nCLIP model on 30 different computer vision benchmarks.\nNote that CLIP with ResNet took 18 days to train on 592\nV100 GPUs while CLIP with ViT took 12 days on 256 V100\nGPUs. This highlights the computational cost of CLIP .\n3.7.2 Single-stream Transformers\nDifferent from two-stream networks like ViLBERT [181]\nand LXMERT [21], VisualBERT [63] uses a single stack of\nTransformers to model both the domains (images and text).\nThe input sequence of text ( e.g., caption) and the visual\nfeatures corresponding to the object proposals are fed to\nthe Transformer that automatically discovers relations be-\ntween the two domains. Notably, VisualBERT architecture is\nsomewhat similar to VideoBERT [17] (explained in Sec. 3.8),\nbut instead of only focusing on cooking videos, Visual-\nBERT evaluates on various visual-linguistic tasks (e.g., VCR,\nNLVR, VQA, and visual grounding). The VisualBERT model\nﬁrst applies task-agnostic pre-training using two objectives\n(Fig. 12-e). The ﬁrst objective simply attempts to predict\nmissing text tokens using the image features and remaining\ntextual tokens. The second objective attempts to differentiate\nbetween the true and false caption of a given image. After\ntask-agnostic pre-training, the authors propose to perform\ntask-speciﬁc pre-training to bridge the domain gap before\nthe ﬁnal ﬁne-tuning to the downstream task.\nSu et al. [22] propose a multi-modal pre-training ap-\nproach to learn features that are generalizable to multi-\nmodal downstream tasks such as Visual Commonsense\nReasoning and Visual Question Answering. This endeavor\nrequires adequately aligning the visual and linguistic cues\nso that an effective composite representation is learned. To\nthe end, [22] builds on the BERT model and inputs both\nthe visual and language features. The language features\ncorrespond to the token in the input sentence and the visual\nfeatures correspond to the region of interest (RoI) from\nthe input image (obtained via a standard Faster R-CNN).\nSpeciﬁcally, the model is pre-trained on both the visual-\nlingual dataset (Conceptual Captions [196]) as well as the\nlanguage-only datasets (e.g., Wikipedia). The loss function is\nidentical to BERT, where the model is trained to predict the\nmasked out words or visual ROIs (Fig. 12-f). In contrary to\nother works such as UNITER [43], VL-BERT claims that the\nvisual-linguistic matching tasks are not useful during pre-\ntraining, which is in contrast to evidence from later efforts\n16\n[180]. Their results on several multi-modal tasks show their\nbeneﬁt over the language-only pre-training ( e.g., in BERT).\nUniversal Encoder for Vision and Language (Unicoder-\nVL) [180] learns multi-modal representations using large-\nscale image-caption pairs. The language and image inputs\nare fed to a single Transformer model (with multiple suc-\ncessive encoders) to learn joint embeddings. To this end,\nit uses masked word prediction, masked object classiﬁca-\ntion, and visual-linguistic matching as self-supervision tasks\nduring pre-training (Fig. 12-d). Notably, the visual-linguistic\nmatching is carried out only at the global level (i.e., image-\nsentence alignment). The model is evaluated on image-\ntext retrieval, zero-shot learning, and visual commonsense\nreasoning where it performs better than the previous models\nsuch as ViLBERT [181] and VisualBERT [63]. This shows\nthe signiﬁcance of rich self-supervised tasks and advocates\nfor a uniﬁed Transformer architecture to learn multi-modal\nfeatures in a common framework.\nThe Uniﬁed Vision-Language Pre-training (VLP) [197]\nmodel uses a single Transformer network for both encod-\ning and decoding stages. This stands in contrast to BERT\ninspired VLP models [17], [22], [63], [198] which use in-\ndependent encoder and decoder networks. Joint modeling\nof encoding and decoding stages allows the Uniﬁed VLP\nmodel to perform well for both image captioning and visual-\nquestion answering tasks, when ﬁne-tuned on these individ-\nual tasks. The intuition for shared modeling of encoding and\ndecoding stage stems from the need to better share cross-\ntask information during pre-training. The uniﬁed model\nconsists of a stack of 12 Transformer blocks, each with a self-\nattention layer followed by a feed-forward module. The self-\nsupervised objectives used for pre-training include masked\nvision-language predictions. Here, the authors explore two\nvariants i.e., bidirectional and sequence-to-sequence predic-\ntion of masked works where different context encodings are\nused for both types of objectives. The proposed approach is\nevaluated on COCO Captions, Flick 30K Captions and VQA\n2.0 and obtains encouraging results compared to previous\nmethods on image captioning and VQA [199].\nUniversal image-text representation (UNITER) [43] per-\nforms pre-training on four large-scale visual-linguistic\ndatasets (MS-COCO [75], Visual Genome [200], Conceptual\nCaptions [196] and SBU Captions [201]). The learned repre-\nsentations transfer well on downstream tasks such as VQA,\nMulti-modal retrieval, Visual Commonsense reasoning, and\nNLVR. In order to emphasize on learning the relationships\nbetween visual and language domains, [43] speciﬁcally de-\nsigns pre-training tasks to predict masked visual or text\nregion conditioned on the other domain input, and align\nlanguage and visual inputs on both the global (image-text)\nand local (word-region) levels (Fig. 12-a). These tasks are\nbeside the conventional masked language modeling task\nused in BERT and explicitly include ﬁne-grained word-\nregion alignment alongside conditional masking of inputs\nthat were not considered in the earlier works such as VL-\nBERT [22], Visual-BERT [63], Vilbert [181] and Unicoder-\nVL [180]. Common to the other approaches, they adopt the\nTransformer architecture proposed in BERT that operates\non both the visual and language embeddings. In contrast\nto applying independent Transformers to the language and\nvisual inputs (as in ViLBERT [181] and LXMERT [21]),\nFig. 13: Visualized tokens (Vokens) [191]: A language model\nis visually supervised using closely related images that leads\nto better feature representations from the pretrained model.\nFigure from [191].\nUNITER adopts a single Transformer applied to the textual\nand image inputs like [22], [63], [180].\nVisualBert [63], Uniter [43], VL-BERT [22], VilBERT [181],\nand Unicoder-VL [180] models for VLP concatenate im-\nage and text features and leave it to the self-attention to\nautomatically discover cross-modal relationships. This can\ncomplicate the visual grounding of semantic concepts in an\nimage. To address this problem, Object-Semantics Aligned\nPre-Training (Oscar) [44] ﬁrst uses an object detector to\nobtain object tags (labels), which are then subsequently used\nas a mechanism to align relevant visual features with the\nsemantic information (Fig. 12-b). The motivation is that the\ntextual content generally pertains to major objects in the\nimage, therefore by explicitly adding those image labels to\nthe input, visual features can be better attended. Similar to\nBERT [3], Oscar uses a Masked Token Loss for VLP , where\ndifferent tokens in the textual input and image tags are ran-\ndomly masked and the model predicts these missing tokens.\nFurther, it also uses a contrastive loss that discriminates\nbetween the original and noisy/fake image-tag pairs. The\nrepresentations thus learned are ﬁne-tuned on VQA, cross-\nmodality retrieval, natural language reasoning, and image\ncaptioning tasks to obtain better performances compared to\nVLP methods that do not use object tags. The recent VinVL\n[202] approach extends Oscar for the object detection task\nand learns object instance-centered relationships between\nvisual and language domains using an adapted pretraining\nscheme. The model is trained on a collection of datasets\n(MS-COCO, OpenImages, Visual Genome and Objects365)\nand was demonstrated to precisely relate semantic attributes\nwith the visual information and provided better transfer-\nability to the downstream visual comprehension tasks.\n3.7.3 Transformers for Visual Grounding\nModulated DETR (MDETR) [203] has a CNN and BERT\nbackbone to extract features from image and text inputs,\nrespectively. The visual and text features are then separately\nlinearly projected to a shared space, concatenated and fed to\na transformer model (with an architecture similar to DETR)\nto predict the bounding boxes for objects corresponding to\nthe queries in the grounding text. The model is trained by\n17\nusing a loss which predicts a uniform distribution over all\nrelevant text query tokens speciﬁc to the predicted bounding\nboxes. An additional contrastive loss term ensures corre-\nspondence between visual and text embedding. TransVG\n[204] is a simple design, where visual and text features are\nfused together in a transformer module, and the bounding-\nbox corresponding to the query is directly regressed us-\ning a learnable token (input to the Transformer module,\nalong-with visual and text features). Referring Transformer\n[205] is also a simple one stage design where the text\nand image features are fused in a Transformer encoder,\nand the Transformer based decoder then directly regresses\nbounding boxes or segmentation masks. Visual Grounding\nwith Transformer [206] has an encoder-decoder architecture,\nwhere visual tokens (features extracted from a pretrained\nCNN model) and text tokens (parsed through an RNN\nmodule) are processed in parallel with two distinct branches\nin the encoder, with cross-modality attention to generate\ntext-guided visual features. The decoder then computes\nattention between the text queries and visual features and\npredicts query-speciﬁc bounding boxes.\n3.8 Video Understanding\nExisting approaches for audio-video data analysis generally\nlearn representations on short-length videos (up to a few\nseconds long), that allow them to encode only short-range\ndependencies [1], [32]. Long-range dependency modeling is\ndesirable in various uni-modal and multi-modal learning\ntasks such as activity recognition [71], [187], [207]–[209].\nBelow, we explain recent approaches that seek to resolve this\nchallenge using the expressivity of Transformer networks.\nIt is important to note that several of these works [17],\n[18], [182], [210] still employ (pretrained) CNNs to encode\nimage/frame-level features in the videos on top of which\nTransformers are applied to model wide context. A few\nexceptions include [209], [211]–[213] which obtain frame-\nlevel features also using the ViT based backbones.\n3.8.1 Joint Video and Language Modeling\nThe VideoBERT [17] model leverages Transformer networks\nand the strength of self-supervised learning to learn effec-\ntive multi-modal representations. Speciﬁcally, VideoBERT\nuses the prediction of masked visual and linguistic tokens as\na pretext task (Fig. 12-c). This allows modeling high-level se-\nmantics and long-range temporal dependencies, important\nfor video understanding tasks. Given a video, [17] converts\nspeech to text using off-the-shelf speech recognition systems\nand applies vector quantization (clustering) to obtain visual\nfeatures from pre-trained video classiﬁcation models. The\nBERT model is then directly applied to these concatenated\nsequences of language and visual tokens to learn their\njoint distribution. The model can be trained with only-text,\nvideo-only, and video+text domains. The resulting model\nshowcases interesting capabilities for cross-modal predic-\ntions such as video generation from a given textual input\n(e.g., captions or cooking recipe) and (video-based) future\nforecasting. The video+text model uses a visual-linguistic\nalignment task to learn cross-modality relationships. The\ndeﬁnition of this pre-text task is simple, given the latent\nstate of the [cls] token, the task is to predict whether the\nsentence is temporally aligned with the sequence of visual\ntokens. Further, the learned representations are shown to be\nvery useful for downstream tasks such as action classiﬁca-\ntion, zero-shot classiﬁcation, and video captioning.\nZhou et al. [210] explore Masked Transformers for dense\nvideo captioning. This requires generating language de-\nscriptions for all events occurring in a video. Existing works\non this problem generally operate sequentially i.e., ﬁrst\ndetect events and then generate captions in separate sub-\nblocks. [210] proposes a uniﬁed Transformer network to\ntackle both tasks jointly, thereby seamlessly integrating the\nmulti-modal tasks of event detection and captioning. First, a\nvideo encoder is used to obtain frame-wise representations\nfollowed by two decoder blocks focused on proposing the\nvideo events and the captions. Since untrimmed videos are\nconsidered, a masking network is used in the captioning\ndecoder to focus on describing a single event proposal.\nRemarkably, [210] was the ﬁrst approach to target dense\nvideo captioning using non-recurrent models and used self-\nattention in the encoder(applied on CNN derived features)\nto model broad range context between video frames. Ex-\nperiments on ActivityNet Captions [214] and YouCookII\n[215] datasets showed good improvements over previous\nrecurrent network and two-stage based approaches.\n3.8.2 Video Action Recognition\nThe traditional CNN based methods in video classiﬁcation\ngenerally perform 3D spatio-temporal processing over lim-\nited intervals to understand videos. Neimark et al. [211]\npropose Video Transformer Network (VTN) that ﬁrst ob-\ntains frame-wise features using 2D CNN and apply a Trans-\nformer encoder (Longformer [103]) on top to learn temporal\nrelationships. Longformer is an attractive choice to process\nlong sequences (with an arbitrary length n) due to its O(n)\ncomplexity. The classiﬁcation token is passed through a\nfully connected layer to recognize actions or events. The\nadvantage of using Transformer encoder on top of spatial\nfeatures is two fold: (a) it allows processing a complete video\nin a single pass, and (b) considerably improves training and\ninference efﬁciency by avoiding the expensive 3D convolu-\ntions. This makes VTN particularly suitable for modeling\nlong videos where interactions between entities are spread\nthroughout the video length. Their experiments on Kinetics-\n400 dataset [71] with various backbones (ResNet [67], ViT\n[11] and DeiT [12]) shows competitive performance.\nGirdhar et al. [18] use a variant of Transformer archi-\ntecture to aggregate person-speciﬁc contextual cues in a\nvideo for action classiﬁcation and localization. Initially, the\nmodel uses a Faster-RCNN [125] style processing where a\nbackbone model generates features that are forwarded to the\nRegion Proposal Network to obtain object proposals. Then\nRoI pooling is applied to generate object-speciﬁc features.\nMulti-head self-attention [1] is then applied on top of the\nobject features as a cascade of self-attention layers. In each\nTransformer unit, a particular person feature is treated as\nthe ‘query’ (Q), while the features from the neighboring\nvideo clip are used as ‘key’ (K) and ‘value’ (V). The location\ninformation is explicitly encoded in the input feature map\nfrom which K, V and Q are derived, thus incorporating\nthe positional information in the self-attention. For a given\n400×400×64 video clip, the key and value tensors are of size\n18\n16×25×25×128, while the query is 128 dimensional vector.\nAlthough [18] uses only RGB stream, additional modalities\nlike optical ﬂow and audio signal (as in competing works)\nwould further increase the compute complexity. Further, the\nTransformer model was found to be sub-optimal for action\nlocalization, perhaps due to its tendency to incorporate\nglobal information. Therefore, it is important to achieve\nthe right trade-off between the global and local context\nfor problems that demand precise delineation ( e.g., action\nlocalization and segmentation).\nHuman action recognition based on skeleton representa-\ntion requires understanding relationships between different\njoints of a body in a given frame as well as between different\nframes of a video. Plizzari et al. [216] proposed a two-stream\nTransformer network to model such relationships. They\nintroduced spatial self-attention (SSA) to model relations\nbetween different body-joints (Fig. 14a) while temporal self-\nattention (TSA) to capture long-range inter-frame depen-\ndencies (Fig. 14b). They ﬁrst used a small residual network\nto extract features from skeleton data and then used SSA\nand TSA modules to process those feature maps. SSA ﬁnds\nthe correlation between each pair of joints independently,\nwhile TSA focuses on how features of a certain joint change\nbetween frames along the temporal dimension. The purpose\nof SSA is to discover relationships among the surrounding\njoints in the same way as the Transformer relates different\nwords in a phrase. On the other hand, TSA ﬁnds long-range\nrelations between frames, similar to how relations among\nphrases are built in NLP . The two streamed model achieves\nstate-of-the-art results on NTU-RGB+D 60 [217] and NTU-\nRGB+D 120 [218] datasets.\nMultiscale Vision Transformers (MViT) [219] build a\nfeature hierarchy by progressively expanding the channel\ncapacity and reducing the spatio-temporal resolution in\nvideos. They introduce multi-head pooling attention to\ngradually change the visual resolution in their pyramid\nstructure. TimeSFormer [213] extends ViTs [132] to videos,\nby considering the video as a sequence of patches ex-\ntracted from individual frames. To capture spatio-temporal\nrelationships, they propose divided attention i.e., spatial\nand temporal attentions are separately applied within each\nblock. TimeSFormer demonstrates SoTA performance on\naction recognition, and can be applied to clips over one\nminute. Another notable pure-transformer based model is\nthe Video Vision Transformer (ViViT) [212]. First, the spatio-\ntemporal tokens are extracted and then efﬁcient factorised\nversions of self-attention are applied to encode relationships\nbetween tokens. However, they require initialization with\nimage-pretrained models to effectively learn the ViT models.\nThere has also been concurrent work on learning sound\npretrained models using self-supervised learning with ViTs.\nAn important recent effort is the long-short contrastive\nlearning (LSTCL) framework [220], which reconstructs rep-\nresentations from different time-scales (narrow and broad)\nas auxiliary learning tasks and demonstrates good down-\nstream performance.\n3.8.3 Video Instance Segmentation\nThe Video Instance Segmentation Transformer (VisTR) [209]\nmodel extends DETR [13] for video object instance seg-\nmentation (VIS) task. Local features are obtained using a\n(a) Spatial Self-Attention\n(b) Temporal Self-Attention\nFig. 14: Spatial/Temporal Attention for Skeleton Data Repre-\nsentations. Relationships between body-joints and inter-frame\ndependencies are modeled using two dedicated self-attention\nmodules. Figure is from [216].\nbackbone CNN on a collection of video frames. An encoder\nand a decoder Transformer is used similar to DETR to\nframe the instance segmentation problem as a sequence to\nsequence prediction task. The input frame-level features are\nconcatenated to form clip representations and the Trans-\nformer outputs instance predictions in a order that is consis-\ntent across frames. This integrates the object detection and\ntracking with-in a single uniﬁed architecture. The predicted\noutputs are matched with the ground-truth using bipartitie\nmatching. Similar to Mask R-CNN [127], a separate head is\nused to predict the instance mask based on self-attention\nand 3D convolutions. The overall results are competitive\namong the single model approaches on YouTube VIS dataset\n[221], but performs somewhat lower compared to more\ncomplex CNN-based models such as MaskProp [222].\n3.9 Transformers in Low-shot Learning\nIn the few-shot learning settings, a support set is provided\nat the inference to adapt to a novel set of categories. Trans-\nformer models have been used to learn set-to-set mappings\non this support set [26] or learn the spatial relationships\nbetween a given input query and support set samples [25].\nIn terms of absolute performance, the patch-wise spatial\nself-attention between query and support set images excels\ncompared to an image level association learned in [26].\nHowever, the patch-wise attention computation is computa-\ntionally expensive. We elaborate on these approaches below.\nDoersch et al. [25] explore the utility of self-supervision\nand Transformer model for few-shot ﬁne-grained classiﬁca-\ntion, where distribution mismatch exists between training\n19\nFig. 15: An overview of FEAT [26]. Compared to the con-\nventional instance embedding methods in FSL that keep the\nembedding function same for all tasks (a), FEAT uses a set-\nto-set function to adapt the embedding function to each FSL\ntask (b). It evaluates several set-to-set functions and found the\nTransformer module to be the most suitable choice for FSL.\nFigure from [26].\nand evaluation phases. They develop Cross-Transformer\nmodel to relate a given query image with the few-examples\navailable in the support set. To this end, the Transformer\nﬁnds spatially similar regions in the query and support\nset images, and the corresponding features are then used\nto obtain class decisions for the query. The queries in the\nTransformer architecture are derived from the grid features\nobtained using the query image. Similarly, grid features\nfrom the support images are used to construct keys and\nvalues which are in turn used to derive attended outputs.\nThis approach, besides a contrastive self-supervision based\ntraining mechanism, leads to the best performance on the\nchallenging Meta-dataset [223].\nYe et al. [26] propose to adapt the few-shot embeddings\nlearned on the base classes to the few-shot target classes\nduring inference using a Transformer module. This leads\nto task-speciﬁc embeddings that perform better on the\ndiscriminative tasks such as few-shot classiﬁcation. While\nmany other set-to-set functions are also evaluated, such as\nGraph convolutional networks [224], Bidirectional LSTMs\n[32] and DeepSets [225], the best performance is achieved\nwith the Transformer-based mapping. This is attributed to\nthe better contextualization, task interpolation and extrap-\nolation capability of Transformers and their permutation\ninvariance while maintaining a relatively lower parameter\ncomplexity. The Transformer architecture in [26] follows the\nstandard model [1]. The embeddings are adapted using\na contrastive loss function for preserving discriminative\nproperties (Fig. 15). The resulting model achieves strong\nperformance on inductive, transductive, and generalized\nFSL tasks.\nLiu et al. [226] learn a multi-head self-attention based\nmodule, to integrate the visual representation learned by the\nmodels trained on different domains present in the meta-\ndataset [223]. The Universal Representation Transformer\n(URT) layer dynamically re-weights the representations\nfrom different domain-speciﬁc backbones, and proves very\neffective in handling few shot tasks across a variety of data\ndistributions.\n3.10 Transformers for Clustering\nClustering aims to discover structure in the data by group-\ning similar data points together. It has numerous applica-\ntions such as data visualization and interpretation, anomaly\ndetection, and open-set categorization. Neural networks\nhave been developed for set prediction problems [225],\n[227], however, the setpoints are processed individually\nwhich can lose information about inter-point relationships.\nRecent works employ Transformers that operate on set\ninputs called the Set Transformers (ST) [228] for amortized\nclustering. Amortized clustering is a challenging problem\nthat seeks to learn a parametric function that can map an\ninput set of points to their corresponding cluster centers. Lee\net al. [228] propose to learn such a mapping function using\na Transformer architecture comprising of multi-head self-\nattention blocks [1]. The Transformer model is permutation\ninvariant by design and allows encoding both pair-wise and\nhigher-order relationships between the input points. How-\never, a full Transformer would lead to a high computational\ncost of O(n2) in each self-attention layer, where n is the\nnumber of points in the set. ST reduces this cost to O(mn)\nby using an Induced Self-Attention Block that uses a low-\nrank projection ( H ∈Rm) to allow operating on large sets.\nThe model was trained to learn optimal parameters that\nmaximize the likelihood of a mixture of Gaussians (MoGs).\nThus MoG parameters are estimated by the ST given a set\nof data points. Beyond amortized clustering, ST is a generic\nframework which can handle other set-input problems such\nas counting unique elements in an input set, multi-instance\nlearning, set anomaly detection, and 3D point-cloud clas-\nsiﬁcation. More recently, [229] improves [228] by taking a\nsequential approach to cluster generation, thereby allowing\nassignment to a variable number of clusters.\n3.11 Transformers for 3D Analysis\nGiven the irregular (variable number of points) and permu-\ntation invariant nature of 3D point cloud representations,\nTransformers provide a promising mechanism to encode\nrich relationships between 3D data points. To this end,\nrecent works [230], [231] are motivated by the capability of\nTransformers to learn set-functions. Speciﬁcally, [230] intro-\nduced a Point Transformer which uses vector attention to\nlearn weights for each channel, while [231] suggest an alter-\nnate design where local 3D structure is explicitly encoded.\nThe non-local nature of Transformers is exploited in [45]\ntowards an accurate human pose and mesh reconstruction\nalgorithm. We discuss these approaches below.\nSelf-attention being a set-operator is ideally suited for\nprocessing point clouds, a 3D data representation that de-\nmands invariance to number of points and their permuta-\ntions. Zhao et al. [230] propose a point Transformer layer that\napplies self-attention in the local neighborhood of 3D points.\nThe proposed layer builds on vectorized self-attention net-\nwork (SAN) [82] where attention weights are represented\nwith vectors.Furthermore, a positional encoding is added\nboth to the attention vector and transformed features (value\nvectors) to represent location information. The point Trans-\nformer layer is sandwiched between two linear layers to\ncreate a point Transformer block that is stacked multiple\ntimes in the developed network architecture. Their design\n20\nalso included transition down/up blocks to reduce/increase\nthe number of points in the input (in a typical encoding-\ndecoding pipeline style). The resulting architecture shows\npromising results on the 3D classiﬁcation and segmentation\ntasks.\nThe Point Cloud Transformer (PCT) [231] is a parallel\nwork to [230] and motivated by the permutation invariance\nproperty of Transformers. However, compared to [230], it\nis more directly based on the conventional Transformer\narchitecture [1] and does not involve vector attention. The\nkey modiﬁcations include a 3D coordinate-based position\nencoding, an offset attention module, and a neighbor em-\nbedding that encodes local 3D structure in point-clouds.\nSpeciﬁcally, the offset attention layer calculates the dif-\nference between the self-attended features and the input\nfeatures using element-wise subtraction. The local neighbor\nembedding simply ﬁnds self-attention relationships among\na group of points instead of individual 3D points. Explicitly\nincorporating local neighbourhood information makes this\na more efﬁcient architecture compared to [230]. The method\nshows promising performance on 3D shape classiﬁcation,\nnormal estimation and segmentation tasks on ModelNet40\n[232] and ShapeNet [233] datasets.\nThe Mesh Transformer (METRO) [45] model targets 3D\nhuman pose and mesh reconstruction from a single 2D im-\nage. A key challenge here is to faithfully learn the non-local\ninteractions between body-joints and mesh vertices ( e.g.,\nhand and foot). The expressivity of Transformer network\nis used to jointly model vertex to vertex relationships in a\nmesh as well as the vertex to body-joint relationships. The\nself-attention mechanism can attend to any combination of\nvertices in the mesh, thereby encoding non-local relation-\nships. The multi-layer Transformer architecture sequentially\nperforms dimensionality reduction to map the 2D image to\n3D mesh. Position encoding is performed using the 3D coor-\ndinates (x,y,z) of each vertex and each body-joint. Similar to\nmasked language modeling in NLP , METRO uses masked\nvertex modeling (MVM) which randomly masks some per-\ncentage of input queries (see Fig. 16). The Transformer is\ntasked with regressing all the joints and vertices which helps\nencode inter-dependencies between them. METRO obtains\nstate-of-the-art results on human mesh reconstruction on\nHuman3.6M [234] and 3DPW [235] datasets. Since the ap-\nproach does not depends on a parametric mesh model, it\ngeneralizes well to other reconstruction tasks such as 3D\nhand reconstruction [236]. Overall, this is the ﬁrst effort\nto employ Transformers for 3D human reconstruction tasks\nand leads to fairly good results.\n4 O PEN CHALLENGES & FUTURE DIRECTIONS\nDespite excellent performance from Transformer models\nand their interesting salient features (Table 1), there ex-\nist several challenges associated with their applicability to\npractical settings (Table 2). The most important bottlenecks\ninclude requirement for large-amounts of training data and\nassociated high computational costs. There have also been\nsome challenges to visualize and interpret Transformer\nmodels. In this section, we provide an overview of these\nchallenges, mention some of the recent efforts to address\nthose limitations and highlight the open research questions.\n4.1 High Computational Cost\nAs discussed in Sec. 1, a strength of Transformer models\nis their ﬂexibility to scale to high parametric complexity.\nWhile this is a remarkable property that allows training\nenormous sized models, this results in high training and\ninference cost (a detailed comparison between CNN and\nViTs is shown in Table 3). As an example, the BERT [3]\nbasic model (with 109 million parameters) took around 1.89\npeta-ﬂop days2 for training, while the latest GPT3 [6] model\n(175 billion parameters) took around 3640 peta-ﬂop days\nfor training (a staggering ∼1925× increase). This comes\nwith a huge price tag, e.g., according to one estimate [237],\nGPT3 training might have cost OpenAI 4.6 million USD.\nAdditionally, these large-scale models require aggressive\ncompression (e.g., distillation) to make them feasible for real-\nworld settings.\nAn empirical study on the scalability of Vision Trans-\nformers for number of parameters (ranging from ﬁve million\nto two billion), size of the training datasets (ranging from 30\nmillion to three billion training images), and compute bud-\nget (1-10000 TPU core-days) is presented in [238]. From this\nstudy, We can draw the following conclusions (a) scaling up\non compute, model and size of training samples improves\nperformance (b) only large models (with more parameters)\ncan beneﬁt from more training data, and the performance\nof smaller models platueas quickly and can not leverage\nfrom additional data. This indicates that large scale models\nhave the capacity to further enhance their representation\nlearning capabilities. However, with the current designs,\nscaling upon Transformer models is expensive and compute\nprohibitive, thus necessitating the need for efﬁcient designs.\nIn the language domain, recent works focus on reducing\nthe high complexity of Transformer models (basically aris-\ning from the self-attention mechanism [1] where a token’s\nrepresentation is updated by considering all tokens from the\nprevious layer). For example, [103], [245] explore selective\nor sparse attention to previous layer tokens while updating\neach next layer token. Linformer [38] reduces complexity of\nstandard self-attention operation from O(n2) to O(n) (both\nin time and memory requirements). The main idea is to\nshow that a low-rank matrix is sufﬁcient to model the self-\nattention mechanism. The Reformer model [246] employed\nlocally-sensitive hashing (LSH) to minimize the complexity\nof self-attention from O(n2) to O(nlog(n)). In similar pur-\nsuit, the recent Lambda Networks propose to model local\ncontext as a linear function which helps reduce complexity\nof self-attention [247]. These linear function lambdas are\napplied to the input query to model contextual relationships\nbetween pixels.\nVyas et al. [248] developed an efﬁcient cluster attention\nto deal with large input sequences that approximates the\noriginal self-attention. The cluster attention groups queries\ninto clusters and then computes attention between cluster\ncenters (instead of attention between all the queries that\nleads to quadratic complexity). The main idea is that the\nqueries close in the Euclidean space should have similar\nattention distributions. With a ﬁxed number of clusters, this\nintuition helps reduce the quadratic complexity to linear\n2. A peta-ﬂop day is a measure of computation and equals to per-\nforming 1015 neural net operations per second for one complete day.\n21\nTask Method Design Highlights(focus on differences\nwith the standard form)\nInput Data TypeLabel Type Loss\nImage\nClassiﬁcation\nViT [11] Directly adopted NLP Transformer En-\ncoder for images, Mechanism to linearly\nembed image patches with positional em-\nbedding suitable for the Encoder.\n2D Image Class labels Cross-entropy\nDeiT [12] Transformer as s student while CNN as\na teacher, Distillation tokens to produce\nestimated labels from teacher, Attention\nbetween class and distillation tokens.\n2D Image Class labels Cross-entropy,\nDistillation loss\nbased on\nKL-divergence\nCLIP [195] Jointly train image and text encoders on\nimage-text pairs, to maximize similarity of\nvalid pairs and minimize otherwise\n2D Images & texts Image-text\npairs\nSymmetric\ncross-entropy\nObject\nDetection\nDETR [13] Linear projection layer to reduce CNN\nfeature dimension, Spatial positional em-\nbedding added to each multi-head self-\nattention layer of both encoder and de-\ncoder. Object queries (output positional\nencoding) added to each multi-head self-\nattention layer of decoder.\n2D Image Class labels Hungarian loss\nbased on bipartite\nmatching between\npredicted and\nground truths\nD-DETR [14] Deformable Transformer consists of de-\nformable attention layers to introduce\nsparse priors in Transformers, Multi-scale\nattention module.\n2D Image Class labels Hungarian loss\nLow Shot\nLearning\nCT [25] Self-supervised pretraining, Query-\naligned class prototypes that provide\nspatial correspondence between the\nsupport-set images and query image.\n2D Image Pretraining\nwithout\nlabels and\nfew-shot\nlearning with\nClass labels\nNormalized\nCross-entropy\nImage\nColorization\nColTran [24] Conditional Row/column multi-head at-\ntention layers, Progressive multi-scale col-\norization scheme.\n2D Image 2D Image Negative\nlog-likelihood of the\nimages\nAction\nRecognition\nST-TR [216] Spatial and Temporal self-attention to op-\nerates on graph data such as joints in skele-\ntons.\nSkeleton Action\nClasses\nCross-entropy\nSuper-\nresolution\nTTSR [16] Texture enhancing Transformer module,\nRelevance embeddings to compute the rel-\nevance between the low-resolution and\nreference image.\n2D Image 2D Image Reconstruction loss,\nPerceptual loss\ndeﬁned on\npretrained VGG19\nfeatures.\nMulti-Model\nLearning\nOscar [44] Transformer layer to jointly process triplet\nrepresentation of image-text [words, tags,\nfeatures], Masked tokens to represent text\ndata.\n2D Image Captions,\nClass labels,\nObject tags\nNegative\nlog-likelihood of\nmasked tokens,\nContrastive binary\ncross-entropy\n3D Classiﬁca-\ntion/Segmentation\nPT [230] Point Transformer block, Transition down\nblock to reduce cardinality of the point set,\nTransition up for dense prediction tasks.\nCAD models, 3D\nobject part\nsegmentation\nObject and\nshape\ncategories\nCross-entropy\n3D Mesh\nReconstruction\nMETRO [45] Progressive dimensionality reduction\nacross Transformer layers, Positional\nEncoding with 3D joint and 3D vertex\ncoordinates, Masked vertex/joint\nmodeling.\n2D Image 3D Mesh +\nHuman Pose\nL1 loss on mesh\nvertices and joints in\n3D and 2D\nprojection.\nVision and\nLanguage\nNavigation\nChenet al.[194] Uni-modal encoders on language and map\ninputs followed by a cross-modal trans-\nformer, Trajectory position encodings in\nthe map encoder.\nInstruction text +\nRGBD panorama +\nTopological\nEnvironment Map\nNavigation\nPlan\nCross-entropy over\nnodes and[stop]\naction\nReferring\nImage\nSegmentation\nCMSA [15] Multimodal feature, Cross-modal self-\nattention on multiple levels and their fu-\nsion using learned gates.\n2D Image +\nLanguage expression\nSegmentation\nmask\nBinary cross-entropy\nloss\nVideo\nClassiﬁcation\nLeeet al.[182] Operates on real-valued audio-visual sig-\nnals instead of tokens, Contrastive learn-\ning for pre-training, End-to-end multi-\nmodal transformer learning.\nAudio-Visual Activity\nlabels\nContrastive InfoNCE\nloss and Binary\ncross-entropy\nTABLE 1: A summary of key design choices adopted in different variants of transformers for a representative set of\ncomputer vision applications. The main changes relate to speciﬁc loss function choices, architectural modiﬁcations, different\nposition embeddings and variations in input data modalities.\n22\nTask Method Metric Dataset Performance Highlights Limitations\nImage\nClassiﬁca-\ntion\nViT [11]\nICLR’21 Top-1 Acc. ImageNet 88.55 a)First application of Transformer\n(global self-attention) directly on\nimage patches,b)Convolution-free\nnetwork architecture,c) Outper-\nforms CNN models such as ResNet.\na)Requires training on large-scale\ndatae.g., 300-Million images,b)\nRequires careful transfer learning\nto the new task,c) Requires large\nmodel with 632-Million parameters\nto achieve SOTA results.\nDeiT [12]\narXiv’20 Top-1 Acc. ImageNet 83.10 a) Successfully trains Transformer\non ImageNet only,b) Introduces\nattention-based distillation method.\nc) Produces competitive perfor-\nmance with small (86-Million pa-\nrameters) Transformers.\na) Requires access to pretrained\nCNN based teacher model thus per-\nformance depends on the quality of\nthe teacher model.\nSwin-T [36]\narXiv’21 Top-1 Acc. ImageNet 84.5 a)Provides a general purpose back-\nbone for different vision tasks e.g.,\nclassiﬁcation, detection and seg-\nmentationb)A hierarchical design\nusing shifted-windows operation.\na) Hard to train from scratch on\nsmaller datasetsb)Quadratic com-\npute complexity inherent to the\nself-attention operation.\nLow-Shot\nLearning\nCT [25]\nNeurIPS’20Top-1 Acc.ImageNet\nCOCO\n62.25\n60.35 a) Self-supervised pre-training\nmechanism that does not need\nmanual labels, b) Dynamic\ninference using Transformer\nachieving stat-of-the-art results.\nProposed algorithm is limited in its\ncapacity to perform on datasets that\nlack spatial details such as texture.\nObject\nDetection\nDETR [13]\nECCV’20 AP COCO 44.9 a)Use of Transformer allows end-\nto-end training pipeline for object\ndetection,b)Removes the need for\nhand-crafted post-processing steps.\na)Performs poorly on small objects,\nb) Requires long training time to\nconverge.\nD-DETR [14]\nICLR’21 AP COCO 43.8 a)Achieves better performance on\nsmall objects than DETR [13],b)\nFaster convergence than DETR [13]\nObtain SOTA results with 52.3 AP\nbut with two stage detector design\nand test time augmentations.\nImage\nColoriza-\ntion\nColTran [24]\nICLR’21 FID ImageNet 19.71 a) First successful application of\nTransformer to image colorization,\nb)Achieves SOTA FID score.\na) Lacks end-to-end training,b)\nlimited to images of size 256×256.\nAction\nRecogni-\ntion\nST-TR [216]\narXiv’20 Top-1 Acc. NTU\n60/120\n94.0/84.7a)Successfully applies Transformer\nto model relations between body\njoints both in spatial and temporal\ndomain,b)Achieves SOTA results.\nProposed Transformers do not pro-\ncess joints directly rather operate on\nfeatures extracted by a CNN, thus\nthe overall model is based on hand-\ncrafted design.\nSuper-\nResolution\nTTSR [16]\nCVPR’20\nPSNR/\nSSIM\nCUFED5\nSun80\nUrban100\nManga109\n27.1 / 0.8\n30.0 / 0.81\n25.9 / 0.78\n30.1 / 0.91\na) Achieves state-of-the-art super-\nresolution by using attention,b)\nNovel Transformer inspired archi-\ntectures that can process multi-scale\nfeatures.\na)Proposed Transformer does not\nprocess images directly but features\nextracted by a convolution based\nnetwork,b)Model with large num-\nber of trainable parameters, andc)\nCompute intensive.\nMulti-\nModel\nLearning\nViLBERT\n[181]\nNeurIPS’19\nAcc./\nmAP (R@1)\nVQA [183]/\nRetrieval\n[239]\n70.6/ 58.2a) Proposed Transformer architec-\nture can combine text and visual\ninformation to understand inter-\ntask dependencies,b)Achieves pre-\ntraining on unlabelled dataset.\na) Requires large amount of data\nfor pre-training,b) Requires ﬁne\ntuning to the new task.\nOscar [44]\nECCV’20\nAcc./\nmAP (R@1) VQA [240]/\nCOCO 80.37/57.5a)Exploit novel supervisory signal\nvia object tags to achieve text and\nimage alignment,b)Achieves state-\nof-the-art results.\nRequires extra supervision through\npre-trained object detectors thus\nperformance is dependent on the\nquality of object detectors.\nUNITER [43]\nECCV’20\nAcc./\nAvg.\n(R@1/5/10)\nVQA [183]/\nFlickr30K\n[241]\n72.47/83.72Learns ﬁne-grained relation align-\nment between text and images\nRequires large multi-task datasets\nfor Transformer training which lead\nto high computational cost.\n3D\nAnalysis\nPoint Trans-\nformer [230]\narXiv’20\nTop-1 Acc.\nIoU\nModelNet40\n[232] 92.8\n85.9\na)Transformer based attention ca-\npable to process unordered and un-\nstructured point sets,b)Permuta-\ntion invariant architecture.\na) Only moderate improvements\nover previous SOTA,b)Large num-\nber of trainable parameters around\n6×higher than PointNet++ [242].\nMETRO [45]\narXiv’20\nMPJPE\nPA-MPJPE\nMPVE\n3DPW\n[235]\n77.1\n47.9\n88.2\na)Does not depend on parametric\nmesh models so easily extendable\nto different objects,b) Achieves\nSOTA results using Transformers.\nDependent on hand-crafted net-\nwork design.\nTABLE 2: A summary of advantages and limitations of different Transformers based methods in different Tasks. (CT: Cross\nTransformers, AP: Average Precision, mAP: mean AP , IoU: Intersection over Union, FID: Fr´echet inception distance, MPJPE:\nMean Per Joint Position Error, MPVE: Mean Per Vertex Error).\n23\nFig. 16: Mesh Transformer architecture. The joint and vertex queries are appended with positional embeddings and passed\nthrough multiple self-attention layers to jointly regress 3D coordinates of joints and mesh vertices. Figure is from [45].\nMethod #Param (M) GFLOPs Top-1 Acc (%)\nResNet18 [67]⋆ 11.7 1.8 69.8\nEfﬁcientNet-B3 [87]⋆ 12.0 1.8 81.6\nDeiT-T [12] 5.7 1.3 72.2\nT2T-ViTt-7 [35] 5.0 1.3 71.7\nLocalViT-T [107] 5.9 1.3 74.8\nCrossViT-T [104] 6.9 1.6 73.4\nPVTv1-T [93] 13.2 1.9 75.1\nResT-Lite [110] 10.5 1.4 77.2\nCaiT-XXX-24 [243] 12.0 2.5 77.6\nPVTv2-B1 [97] 13.1 2.1 78.7\nLv-ViT-T [89] 8.5 – 79.1\nRegionViT-T [100] 13.8 2.4 80.4\nResNet50 [67]⋆ 25.6 4.1 76.1\nResNeXt50-32x4d [244]⋆ 25.0 4.3 77.6\nRegNetY-4G [86]⋆ 21.0 4.0 80.0\nEfﬁcientNet-B4 [87]⋆ 19.0 4.2 82.9\nDeiT-S [12] 22.1 4.6 79.9\nPVTv1-S [93] 24.5 3.8 79.8\nLocalViT-S [107] 22.4 4.6 80.8\nCrossViT-S [104] 26.7 5.6 81.0\nTNT-S [88] 23.8 5.2 81.3\nSwin-T [36] 29.0 4.5 81.3\nNesT-T [111] 17.0 5.8 81.5\nT2T-ViTt-14 [35] 21.5 5.2 81.5\nCvT-13 [96] 20.0 4.5 81.6\nResT-B [110] 30.3 4.3 81.6\nTwins-SVT-S [37] 24.0 2.8 81.7\nPVTv2-B2-Li [97] 22.6 3.9 82.1\nRegionViT-S [100] 30.6 5.6 82.5\nLv-ViT-S [89] 26.0 6.6 83.3\nMethod #Param (M) GFLOPs Top-1 Acc (%)\nResNet101 [67]⋆ 44.7 7.9 77.4\nResNeXt101-32x4d [244]⋆ 44.2 8.0 78.8\nRegNetY-8G [86]⋆ 39.0 8.0 81.7\nEfﬁcientNet-B5 [87]⋆ 30.0 9.9 83.6\nCvT-21 [96] 32.0 7.1 82.5\nCaiT-S-24 [243] 32.2 9.4 82.7\nT2T-ViTt-19 [35] 39.0 9.8 81.4\nPVTv1-M [93] 44.2 6.7 81.2\nPVTv2-B3 [97] 45.2 6.9 83.2\nNesT-S [111] 38.0 10.4 83.3\nResNet152 [67]⋆ 60.2 11.6 78.3\nCaiT-S-36 [243] 48.0 13.9 83.3\nT2T-ViTt-24 [35] 64.0 15.0 82.2\nPVTv1-L [93] 61.4 9.8 81.7\nTNT-B [88] 66.0 14.1 82.8\nSwin-S [36] 50.0 8.7 83.0\nTwins-SVT-B [37] 56.0 8.3 83.2\nRegionViT-B [100] 72.7 13.0 83.3\nPVTv2-B4 [97] 62.6 10.1 83.6\nResNeXt101-64x4d [244]⋆ 83.5 15.6 79.6\nRegNetY-16G [86]⋆ 84.0 16.0 82.9\nEfﬁcientNet-B6 [87]⋆ 43.0 19.0 84.0\nNesT-B [111] 68.0 17.9 83.8\nViT-B/16 [11] 86.6 17.6 79.8\nDeiT-B/16 [12] 86.6 17.6 81.8\nSwin-B [36] 88.0 15.4 83.3\nTwins-SVT-L [37] 99.2 14.8 83.7\nPVTv2-B5 [97] 82.0 11.8 83.8\nLv-ViT-M [89] 56.0 16.0 84.1\nTABLE 3: A Comparative analysis between different vision transformer and CNN models in terms of their parameter\ncomplexity and top-1 (%) accuracy on ImageNet validation set. For a direct comparison, we consider models that are\ntrained on ImageNet from scratch on input of size 224x224. ⋆ denotes pure CNN-based methods.\ncomplexity of O(nc) with respect to the input sequence\nlength n (where c is the number of clusters). We refer\ninterested readers to a survey on efﬁcient Transformers in\nNLP [34].\nSimilar to the NLP domain, computer vision models\nalso suffer from the high computational cost of Transformer\nmodels. For example, image generators that are based\non sequence-based Transformers ( e.g., iGPT) have a high\ncompute cost limiting their applicability to high-resolution\ninputs. The time and memory cost of core self-attention\noperation in Transformers increases quadratically with the\nnumber of patches, i.e. O(n2), for n image patches (in some\napplications, e.g., low-level vision, n = H ×W where\nH, Wdenote the height and width of the image). This is a\nmajor drawback of existing Transformers that hinders their\napplication to most tasks involving high-resolution (HR)\nimages, such as object detection and segmentation (in high-\nlevel vision), and super-resolution, deblurring, denoising,\netc. (in low-level vision). Numerous methods have been\nproposed that make special design choices to perform self-\nattention more ‘efﬁciently’, for instance employing pool-\ning/downsampling in self-attention [97], [219], [249], local\nwindow-based attention [36], [250], axial-attention [179],\n[251], low-rank projection attention [38], [252], [253], ker-\n24\nnelizable attention [254], [255], and similarity-clustering\nbased methods [246], [256]. However, almost all of these\napproaches either come with a trade-off between complexity\nand accuracy, require special hardware speciﬁcations or are\nstill not applicable to very large images. Therefore, there\nis a pressing need to develop an efﬁcient self-attention\nmechanism that can be applied to HR images on resource-\nlimited systems without compromising accuracy. It will be\ninteresting to explore how existing models can be extended\nto high-dimensional cases e.g., using a multi-scale trans-\nformer design with a somewhat local context modeling. By\ninducing inductive biases based on our understanding of\nthe visual learning tasks (e.g., spatial relationships in the\nlocal neighbourhood), the high computational cost can be\nreduced. Similarly, using sparse attention maps modeled\nwith low-rank factorization in the matrices can also help\ntowards reducing the computational cost [211].\n4.2 Large Data Requirements\nSince Transformer architectures do not inherently encode\ninductive biases (prior knowledge) to deal with visual data,\nthey typically require large amount of training to ﬁgure\nout the underlying modality-speciﬁc rules. For example, a\nCNN has inbuilt translation invariance, weight sharing, and\npartial scale invariance due to pooling operations or multi-\nscale processing blocks. However, a Transformer network\nneeds to ﬁgure out these image-speciﬁc concepts on its own\nfrom the training examples. Similarly, relationships between\nvideo frames need to be discovered automatically by the\nself-attention mechanism by looking at a large database\nof video sequences. This results in longer training times,\na signiﬁcant increase in computational requirements, and\nlarge datasets for processing. For example, the ViT [11]\nmodel requires hundreds of millions of image examples to\nobtain reasonable performance on the ImageNet benchmark\ndataset. The question of learning a Transformer in a data-\nefﬁcient manner is an open research problem and recent\nworks report encouraging steps towards its resolution. For\nexample, DeiT [12] uses a distillation approach to achieve\ndata efﬁciency while T2T (Tokens-to-Token) ViT [35] models\nlocal structure by combining spatially close tokens together,\nthus leading to competitive performance when trained only\non ImageNet from scratch (without pre-training). By incor-\nporating CNNs like feature hierarchies in ViTs to effectively\ncapture local image cues, ViTs (e.g., CCT [106], NesT [111])\ncan be trained from scratch even on small-scale datasets\n(e.g., CIFAR-10). Another approach to data efﬁcient training\nof ViTs is proposed in et al. [257]. The authors show that\nby smoothing the local loss surface using sharpness-aware\nminimizer (SAM) [258], ViTs can be trained with simple\ndata augmentation scheme (random crop, and horizontal\nﬂip) [259], instead of employing compute intensive strong\ndata augmentation strategies, and can outperform their\ncounterpart ResNet models.\n4.3 Vision Tailored Transformer Designs\nWe note that most of the existing works focused on vision\ntasks tend to directly apply NLP Transformer models on\ncomputer vision problems. These include architectures de-\nsigned for image recognition [11], video understanding [17]\nand especially multi-modal processing [181]. Although the\ninitial results from these simple applications are quite en-\ncouraging and motivate us to look further into the strengths\nof self-attention and self-supervised learning, current archi-\ntectures may still remain better tailored for language prob-\nlems (with a sequence structure) and need further intuitions\nto make them more efﬁcient for visual inputs. For example,\nvector attention from [82] is a nice work in this direction\nwhich attempts to speciﬁcally tailor self-attention operation\nfor visual inputs via learning channel-wise attentions. Simi-\nlarly, [260] uses a Jigsaw puzzle based self-supervision loss\nas a parallel branch in the Transformers to improve person\nre-identiﬁcation. A recent work [35] rearranges the spa-\ntially close tokens to better model relationships in spatially\nproximal locations. Token distillation [12] from pre-trained\nCNN models has also been used as a remedy to inject\ndomain biases in the representations. One may argue that\nthe architectures like Transformer models should remain\ngeneric to be directly applicable across domains, we notice\nthat the high computational and time cost for pre-training\nsuch models demands novel design strategies to make their\ntraining more affordable on vision problems.\n4.4 Neural Architecture Search for ViTs\nWhile Nerual Architecuter Search (NAS) has been well\nexplored for CNNs to ﬁnd an optimized architecture, it\nis relatively less explored in Transformers (even for lan-\nguage transformers [261], [262]). Chen et al. [263] propose a\none-shot NAS for vision transformers, called AutoFormer.\nBossNAS [264] searches for a hybrid architecture (CNN\nand Transformer). Another recent effort studies the trade-\noff between global and local information in Transformers in\nthe context of vision applications [265]. It will be insightful\nto further explore the domain-speciﬁc design choices (e.g.,\nthe contrasting requirements between language and vision\ndomains) using NAS to design more efﬁcient and light-\nweight models similar to CNNs [87].\n4.5 Interpretability of Transformers\nThrough an extensive set of carefully designed experiments,\nNaseer et al. [266] investigate multiple intriguing properties\nof ViTs in terms of their generalization and robustness. They\nshow that, compared with CNNs, ViTs demonstrate strong\nrobustness against texture changes and severe occlusions,\ne.g.ViTs retain upto 60% top-1 accuracy on ImageNet once\n80% of the image content is randomly occluded. Given the\nstrong performance of Transformer architectures, it is inter-\nesting and critical to interpret their decisions, e.g., by visual-\nizing relevant regions in an image for a given classiﬁcation\ndecision. The main challenge is that the attention originating\nin each layer, gets inter-mixed in the subsequent layers in a\ncomplex manner, making it difﬁcult to visualize the relative\ncontribution of input tokens towards ﬁnal predictions. This\nis an open problem, however, some recent works [267]–[269]\ntarget enhanced interpretability of Transformers and report\nencouraging results. Attention roll-out and attention ﬂow\nmethods were proposed in [268] to estimate the accurate at-\ntentions. However, this method functions in an ad-hoc man-\nner and makes simplistic assumptions e.g., input tokens are\n25\nlinearly combined using attention weights across the layers.\nChefer et al. [269] note that the attention scores obtained di-\nrectly via the self-attention process (encoding relationships\nbetween tokens) or reassignments in [268] do not provide an\noptimal solution. As an alternative, they propose to assign\nand propagate relevancy scores in the Transformer network\nsuch that the sum of relevancy is constant throughout the\nnetwork. Their design can handle both the positive and\nnegative attributions experienced in the self-attention layer.\nThe proposed framework has an added advantage of being\nable to provide class-speciﬁc visualizations. Despite these\nseminal works, visualizing and interpreting Transformers\nis an unsolved problem and methods are needed to obtain\nspatially precise activation-speciﬁc visualizations. Further\nprogress in this direction can help in better understanding\nthe Transformer models, diagnosing any erroneous behav-\niors and biases in the decision process. It can also help us\ndesign novel architectures that can help us avoid any biases.\n4.6 Hardware Efﬁcient Designs\nLarge-scale Transformer networks can have intensive power\nand computation requirements, hindering their deployment\non edge devices and resource-constrained environments\nsuch as internet-of-things (IoT) platforms. Some recent ef-\nforts have been reported to compress and accelerate NLP\nmodels on embedded systems such as FPGAs [270]. Li et\nal. [270] used an enhanced block-circulant matrix-based rep-\nresentation to compress NLP models and proposed a new\nField Programmable Gate Array (FPGA) architecture design\nto efﬁciently manage resources for high throughput and low\nlatency. They could achieve 27x, 3x and 81x improvements\nin performance (throughput measured in FPS), reduced\npower consumption, and energy efﬁciency relative a CPU\nfor RoBERTa model [7]. Towards this goal, [262] proposed\nto design Hardware-Aware Transformers (HAT) using neu-\nral architecture search strategies [271]–[273]. Speciﬁcally, a\nSuperTransformer model is ﬁrst trained for performance\napproximation which can estimate a model’s performance\nwithout fully training it. This model comprises the largest\npossible model in the search space while sharing weights\nbetween common parts. Eventually, an evolutionary search\nis performed considering the hardware latency constraints\nto ﬁnd a suitable SubTransformer model for a target hard-\nware platform (e.g., IoT device, GPU, CPU). However, such\nhardware efﬁcient designs are currently lacking for the\nvision Transformers to enable their seamless deployment\nin resource-constrained devices. Further, the search cost of\nthe evolutionary algorithms remains signiﬁcant with the\nassociated impact of CO2 emissions on the environment.\n4.7 Towards Integrating All Modalities\nSince Transformers provide a uniﬁed design to process\ndifferent modalities, recent efforts also focus on proposing\nmore generic general purpose reasoning systems based on\nTransformers. Inspired by the biological systems that can\nprocess information from a diverse range of modalities,\nPerceiver model [274] aims to learn a uniﬁed model that\ncan process any given input modality without making\ndomain-speciﬁc architectural assumptions. In order to scale\nto high-dimensional inputs, Perceiver uses an asymmetric\ncross attention method to distill input information into low-\ndimensional latent bottleneck features. Once the features are\ndistilled in a compact and ﬁxed-dimensional form, regular\nTransformer blocks are applied in the latent space. The\noriginal Perceiver model shows performance competitive to\nResNets and ViTs on image classiﬁcation and can process 3D\ndata, audio, images, video or their combinations. However,\nthis model can only generate ﬁxed outputs e.g., class prob-\nabilities. A recent improvement called Perceiver IO [275]\naims to learn models with both ﬂexible inputs as well as\narbitrary sized outputs. This allows application to problems\nwhich demand structured outputs such as natural language\ntasks and visual comprehension. While these models avoid\nmodality dependent architectural choices, the learning itself\nstill involves modality dependent choices e.g., speciﬁc aug-\nmentations or positional encodings. An interesting and open\nfuture direction is to achieve total modality-agnosticism in\nthe learning pipeline.\n5 C ONCLUSION\nAttention has played a key role in delivering efﬁcient\nand accurate computer vision systems, while simultane-\nously providing insights into the function of deep neu-\nral networks. This survey reviews the self-attention ap-\nproaches and speciﬁcally focuses on the Transformer and bi-\ndirectional encoding architectures that are built on the prin-\nciple of self-attention. We ﬁrst cover fundamental concepts\npertaining to self-attention architectures and later provide\nan in-depth analysis of competing approaches for a broad\nrange of computer vision applications. Speciﬁcally, we in-\nclude state of the art self-attention models for image recog-\nnition, object detection, semantic and instance segmentation,\nvideo analysis and classiﬁcation, visual question answering,\nvisual commonsense reasoning, image captioning, vision-\nlanguage navigation, clustering, few-shot learning, and 3D\ndata analysis. We systematically highlight the key strengths\nand limitations of the existing methods and particularly\nelaborate on the important future research directions. With\nits speciﬁc focus on computer vision tasks, this survey pro-\nvides a unique view of the recent progress in self-attention\nand Transformer-based methods. We hope this effort will\ndrive further interest in the vision community to leverage\nthe potential of Transformer models and improve on their\ncurrent limitations e.g., reducing their carbon footprint.\nACKNOWLEDGMENTS\nThe authors would like to thank Tim Prangemeier (TU Darmstadt), Lu-\nowei Zhou (Microsoft Research), Jason Corso (University of Michigan),\nPichao Wang (Alibaba Group), Yuqing Wang (Meituan), Alex Meinke\n(Uni-Tuebingen), Irwan Bello (Google Brain) and Manoj Kumar (Google\nBrain) for their helpful feedback on the survey. We would also like to\nthank Mohamed Afham for his help with a ﬁgure.\nREFERENCES\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin NeurIPS, 2017.\n[2] M. Ott, S. Edunov, D. Grangier, and M. Auli, “Scaling neural\nmachine translation,” in WMT, 2018.\n26\n[3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” arXiv preprint arXiv:1810.04805, 2018.\n[4] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Im-\nproving language understanding by generative pre-training,”\ntech. rep., OpenAI, 2018.\n[5] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n“Language models are unsupervised multitask learners,” tech.\nrep., OpenAI, 2019.\n[6] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan,\nP . Dhariwal, A. Neelakantan, P . Shyam, G. Sastry, A. Askell,\net al. , “Language models are few-shot learners,” arXiv preprint\narXiv:2005.14165, 2020.\n[7] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer, and V . Stoyanov, “RoBERTa: A ro-\nbustly optimized bert pretraining approach,” arXiv preprint\narXiv:1907.11692, 2019.\n[8] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY. Zhou, W. Li, and P . J. Liu, “Exploring the limits of transfer\nlearning with a uniﬁed text-to-text transformer,” arXiv preprint\narXiv:1910.10683, 2019.\n[9] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang,\nM. Krikun, N. Shazeer, and Z. Chen, “Gshard: Scaling giant\nmodels with conditional computation and automatic sharding,”\narXiv preprint arXiv:2006.16668, 2020.\n[10] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling\nto trillion parameter models with simple and efﬁcient sparsity,”\narXiv preprint arXiv:2101.03961.\n[11] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\net al., “An image is worth 16x16 words: Transformers for image\nrecognition at scale,” arXiv preprint arXiv:2010.11929, 2020.\n[12] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\nH. J ´egou, “Training data-efﬁcient image transformers & distilla-\ntion through attention,” arXiv preprint arXiv:2012.12877, 2020.\n[13] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,”\narXiv preprint arXiv:2005.12872, 2020.\n[14] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable\nDETR: Deformable transformers for end-to-end object detection,”\narXiv preprint arXiv:2010.04159, 2020.\n[15] L. Ye, M. Rochan, Z. Liu, and Y. Wang, “Cross-modal self-\nattention network for referring image segmentation,” in CVPR,\n2019.\n[16] F. Yang, H. Yang, J. Fu, H. Lu, and B. Guo, “Learning texture\ntransformer network for image super-resolution,” in CVPR, 2020.\n[17] C. Sun, A. Myers, C. Vondrick, K. Murphy, and C. Schmid,\n“VideoBERT: A joint model for video and language represen-\ntation learning,” in ICCV, 2019.\n[18] R. Girdhar, J. Carreira, C. Doersch, and A. Zisserman, “Video\naction transformer network,” in CVPR, 2019.\n[19] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu,\nC. Xu, and W. Gao, “Pre-trained image processing transformer,”\narXiv preprint arXiv:2012.00364, 2020.\n[20] A. Ramesh, M. Pavlov, G. Goh, and S. Gray, “DALL·E: Creating\nimages from text,” tech. rep., OpenAI, 2021.\n[21] H. Tan and M. Bansal, “LXMERT: Learning cross-modality en-\ncoder representations from transformers,” in EMNLP-IJCNLP,\n2019.\n[22] W. Su, X. Zhu, Y. Cao, B. Li, L. Lu, F. Wei, and J. Dai, “VL-BERT:\nPre-training of generic visual-linguistic representations,” arXiv\npreprint arXiv:1908.08530, 2019.\n[23] X. Wang, C. Yeshwanth, and M. Nießner, “SceneFormer:\nIndoor scene generation with transformers,” arXiv preprint\narXiv:2012.09793, 2020.\n[24] M. Kumar, D. Weissenborn, and N. Kalchbrenner, “Colorization\ntransformer,” in ICLR, 2021.\n[25] C. Doersch, A. Gupta, and A. Zisserman, “CrossTransformers:\nspatially-aware few-shot transfer,” NeurIPS, 2020.\n[26] H.-J. Ye, H. Hu, D.-C. Zhan, and F. Sha, “Few-shot learning via\nembedding adaptation with set-to-set functions,” in CVPR, 2020.\n[27] S. Chaudhari, G. Polatkan, R. Ramanath, and V . Mithal,\n“An attentive survey of attention models,” arXiv preprint\narXiv:1904.02874, 2019.\n[28] A. de Santana Correia and E. L. Colombini, “Attention, please!\nasurvey of neural attention models in deep learning,” arXiv\npreprint arXiv:2103.16775, 2021.\n[29] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell: A\nneural image caption generator,” in CVPR, 2015.\n[30] Y. Bengio, I. Goodfellow, and A. Courville, Deep learning . MIT\npress, 2017.\n[31] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature,\n2015.\n[32] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”\nNeural computation, 1997.\n[33] D. Hu, “An introductory survey on attention mechanisms in nlp\nproblems,” in IntelliSys, 2019.\n[34] Y. Tay, M. Dehghani, D. Bahri, and D. Metzler, “Efﬁcient trans-\nformers: A survey,” arXiv preprint arXiv:2009.06732, 2020.\n[35] L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, F. E. Tay, J. Feng, and\nS. Yan, “Tokens-to-token vit: Training vision transformers from\nscratch on imagenet,” arXiv preprint arXiv:2101.11986, 2021.\n[36] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,\n“Swin transformer: Hierarchical vision transformer using shifted\nwindows,” arXiv preprint arXiv:2103.14030, 2021.\n[37] X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia,\nand C. Shen, “Twins: Revisiting the design of spatial attention\nin vision transformers,” arXiv preprint arXiv:2104.13840, 2021.\n[38] S. Wang, B. Li, M. Khabsa, H. Fang, and H. Ma, “Linformer: Self-\nattention with linear complexity,”arXiv preprint arXiv:2006.04768,\n2020.\n[39] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, “Self-\nattention generative adversarial networks,” in International con-\nference on machine learning, pp. 7354–7363, PMLR, 2019.\n[40] J. P ´erez, J. Marinkovi ´c, and P . Barcel´o, “On the turing complete-\nness of modern neural network architectures,” in International\nConference on Learning Representations, 2018.\n[41] J.-B. Cordonnier, A. Loukas, and M. Jaggi, “On the relationship\nbetween self-attention and convolutional layers,” in International\nConference on Learning Representations, 2019.\n[42] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei,\n“Deformable convolutional networks,” in Proceedings of the IEEE\ninternational conference on computer vision, pp. 764–773, 2017.\n[43] Y.-C. Chen, L. Li, L. Yu, A. El Kholy, F. Ahmed, Z. Gan, Y. Cheng,\nand J. Liu, “UNITER: Universal image-text representation learn-\ning,” in ECCV, 2020.\n[44] X. Li, X. Yin, C. Li, P . Zhang, X. Hu, L. Zhang, L. Wang, H. Hu,\nL. Dong, F. Wei, et al. , “Oscar: Object-semantics aligned pre-\ntraining for vision-language tasks,” in ECCV, 2020.\n[45] K. Lin, L. Wang, and Z. Liu, “End-to-end human pose\nand mesh reconstruction with transformers,” arXiv preprint\narXiv:2012.09760, 2020.\n[46] S. Gidaris, P . Singh, and N. Komodakis, “Unsupervised represen-\ntation learning by predicting image rotations,” in ICLR, 2018.\n[47] “Revisiting the unreasonable effectiveness of data.” https://ai.\ngoogleblog.com/2017/07/revisiting-unreasonable-effectiveness.\nhtml. Accessed: 2020-12-31.\n[48] L. Jing and Y. Tian, “Self-supervised visual feature learning with\ndeep neural networks: A survey,” TP AMI, 2020.\n[49] X. Liu, F. Zhang, Z. Hou, Z. Wang, L. Mian, J. Zhang, and\nJ. Tang, “Self-supervised learning: Generative or contrastive,”\narXiv preprint arXiv:2006.08218, 2020.\n[50] “Aaai 2020 keynotes turing award winners event.” https://www.\nyoutube.com/watch?v=UX8OubxsY8w. Accessed: 2020-12-31.\n[51] R. Zhang, P . Isola, and A. A. Efros, “Colorful image colorization,”\nin ECCV, 2016.\n[52] C. Ledig, L. Theis, F. Husz ´ar, J. Caballero, A. Cunningham,\nA. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, et al., “Photo-\nrealistic single image super-resolution using a generative adver-\nsarial network,” in CVPR, 2017.\n[53] D. Pathak, P . Krahenbuhl, J. Donahue, T. Darrell, and A. Efros,\n“Context encoders: Feature learning by inpainting,” in CVPR,\n2016.\n[54] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-\nFarley, S. Ozair, A. Courville, and Y. Bengio, “Generative adver-\nsarial nets,” in NeurIPS, 2014.\n[55] D. Lin, K. Fu, Y. Wang, G. Xu, and X. Sun, “MARTA GANs:\nUnsupervised representation learning for remote sensing image\nclassiﬁcation,” GRSL, 2017.\n[56] U. Ahsan, R. Madhok, and I. Essa, “Video jigsaw: Unsupervised\nlearning of spatiotemporal context for video action recognition,”\nin WACV, 2019.\n[57] M. Noroozi and P . Favaro, “Unsupervised learning of visual\nrepresentations by solving jigsaw puzzles,” in ECCV, 2016.\n27\n[58] D. Kim, D. Cho, D. Yoo, and I. S. Kweon, “Learning image\nrepresentations by completing damaged jigsaw puzzles,” WACV,\n2018.\n[59] L. Jing, X. Yang, J. Liu, and Y. Tian, “Self-supervised spatiotempo-\nral feature learning via video rotation prediction,” arXiv preprint\narXiv:1811.11387, 2018.\n[60] H.-Y. Lee, J.-B. Huang, M. Singh, and M.-H. Yang, “Unsupervised\nrepresentation learning by sorting sequences,” in ICCV, 2017.\n[61] I. Misra, C. L. Zitnick, and M. Hebert, “Shufﬂe and learn: unsu-\npervised learning using temporal order veriﬁcation,” in ECCV,\n2016.\n[62] D. Wei, J. J. Lim, A. Zisserman, and W. T. Freeman, “Learning\nand using the arrow of time,” in CVPR, 2018.\n[63] L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang,\n“VisualBERT: A simple and performant baseline for vision and\nlanguage,” in Arxiv preprint arXiv:1908.03557, 2019.\n[64] B. Korbar, D. Tran, and L. T., “Cooperative learning of audio and\nvideo models from self-supervised synchronization,” in NeurIPS,\n2018.\n[65] R. Arandjelovic and A. Zisserman, “Look, listen and learn,” in\nICCV, 2017.\n[66] N. Sayed, B. Brattoli, and B. Ommer, “Cross and learn: Cross-\nmodal self-supervision,” in GCPR, 2018.\n[67] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in CVPR, 2016.\n[68] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,”\narXiv preprint arXiv:1607.06450, 2016.\n[69] A. Buades, B. Coll, and J.-M. Morel, “A non-local algorithm for\nimage denoising,” in CVPR, 2005.\n[70] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural\nnetworks,” in CVPR, 2018.\n[71] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vi-\njayanarasimhan, F. Viola, T. Green, T. Back, P . Natsev, et al. ,\n“The kinetics human action video dataset,” arXiv preprint\narXiv:1705.06950, 2017.\n[72] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei, and W. Liu,\n“CCNet: Criss-cross attention for semantic segmentation,” in\nICCV, 2019.\n[73] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,\nR. Benenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes\ndataset for semantic urban scene understanding,” in CVPR, 2016.\n[74] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba,\n“Scene parsing through ade20k dataset,” in CVPR, 2017.\n[75] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P . Perona, D. Ramanan,\nP . Doll´ar, and C. L. Zitnick, “Microsoft COCO: Common objects\nin context,” in ECCV, 2014.\n[76] X. Liang, K. Gong, X. Shen, and L. Lin, “Look into person: Joint\nbody parsing & pose estimation network and a new benchmark,”\nTP AMI, 2018.\n[77] G. J. Brostow, J. Fauqueur, and R. Cipolla, “Semantic object\nclasses in video: A high-deﬁnition ground truth database,” Pat-\ntern Recognition Letters, 2009.\n[78] H. Hu, Z. Zhang, Z. Xie, and S. Lin, “Local relation networks for\nimage recognition,” in ICCV, 2019.\n[79] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V . Le, “Attention\naugmented convolutional networks,” in ICCV, 2019.\n[80] P . Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with rela-\ntive position representations,” in NAACL, 2018.\n[81] N. Parmar, P . Ramachandran, A. Vaswani, I. Bello, A. Levskaya,\nand J. Shlens, “Stand-alone self-attention in vision models,” in\nNeurIPS, 2019.\n[82] H. Zhao, J. Jia, and V . Koltun, “Exploring self-attention for image\nrecognition,” in CVPR, 2020.\n[83] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Good-\nfellow, and R. Fergus, “Intriguing properties of neural networks,”\narXiv preprint arXiv:1312.6199, 2013.\n[84] M. M. Naseer, S. H. Khan, M. H. Khan, F. S. Khan, and F. Porikli,\n“Cross-domain transferability of adversarial perturbations,” in\nNeurIPS, 2019.\n[85] M. Naseer, K. Ranasinghe, S. Khan, F. S. Khan, and F. Porikli, “On\nimproving adversarial transferability of vision transformers,”\narXiv preprint arXiv:2106.04169, 2021.\n[86] I. Radosavovic, R. P . Kosaraju, R. Girshick, K. He, and P . Doll ´ar,\n“Designing network design spaces,” in CVPR, 2020.\n[87] M. Tan and Q. V . Le, “EfﬁcientNet: Rethinking model scaling for\nconvolutional neural networks,” in ICML, 2019.\n[88] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y. Wang, “Transformer\nin transformer,” arXiv preprint arXiv:2103.00112, 2021.\n[89] Z. Jiang, Q. Hou, L. Yuan, D. Zhou, Y. Shi, X. Jin, A. Wang, and\nJ. Feng, “All tokens matter: Token labeling for training better\nvision transformers,” arXiv preprint arXiv:2104.10858, 2021.\n[90] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo, “Cutmix:\nRegularization strategy to train strong classiﬁers with localizable\nfeatures,” in Proceedings of the IEEE/CVF International Conference\non Computer Vision, pp. 6023–6032, 2019.\n[91] A. El-Nouby, H. Touvron, M. Caron, P . Bojanowski, M. Douze,\nA. Joulin, I. Laptev, N. Neverova, G. Synnaeve, J. Verbeek, and\nH. Jegou, “Xcit: Cross-covariance image transformers,” 2021.\n[92] D. Zhou, B. Kang, X. Jin, L. Yang, X. Lian, Z. Jiang, Q. Hou, and\nJ. Feng, “Deepvit: Towards deeper vision transformer,” 2021.\n[93] W. Wang, E. Xie, X. Li, D.-P . Fan, K. Song, D. Liang, T. Lu, P . Luo,\nand L. Shao, “Pyramid vision transformer: A versatile back-\nbone for dense prediction without convolutions,” arXiv preprint\narXiv:2102.12122, 2021.\n[94] J. Yang, C. Li, P . Zhang, X. Dai, B. Xiao, L. Yuan, and J. Gao, “Focal\nself-attention for local-global interactions in vision transformers,”\n2021.\n[95] Z. Huang, Y. Ben, G. Luo, P . Cheng, G. Yu, and B. Fu, “Shufﬂe\ntransformer: Rethinking spatial shufﬂe for vision transformer,”\n2021.\n[96] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang,\n“Cvt: Introducing convolutions to vision transformers,” arXiv\npreprint arXiv:2103.15808, 2021.\n[97] W. Wang, E. Xie, X. Li, D.-P . Fan, K. Song, D. Liang, T. Lu, P . Luo,\nand L. Shao, “Pvtv2: Improved baselines with pyramid vision\ntransformer,” 2021.\n[98] W. Xu, Y. Xu, T. Chang, and Z. Tu, “Co-scale conv-attentional\nimage transformers,” 2021.\n[99] W. Wang, L. Yao, L. Chen, D. Cai, X. He, and W. Liu, “Cross-\nformer: A versatile vision transformer based on cross-scale atten-\ntion,” arXiv preprint arXiv:2108.00154, 2021.\n[100] C.-F. Chen, R. Panda, and Q. Fan, “Regionvit: Regional-to-local\nattention for vision transformers,” 2021.\n[101] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and\nP . Luo, “Segformer: Simple and efﬁcient design for semantic\nsegmentation with transformers,” 2021.\n[102] P . Zhang, X. Dai, J. Yang, B. Xiao, L. Yuan, L. Zhang, and J. Gao,\n“Multi-scale vision longformer: A new vision transformer for\nhigh-resolution image encoding,” ICCV 2021, 2021.\n[103] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-\ndocument transformer,” arXiv preprint arXiv:2004.05150, 2020.\n[104] C.-F. Chen, Q. Fan, and R. Panda, “Crossvit: Cross-attention\nmulti-scale vision transformer for image classiﬁcation,” arXiv\npreprint arXiv:2103.14899, 2021.\n[105] K. Yuan, S. Guo, Z. Liu, A. Zhou, F. Yu, and W. Wu, “Incorporat-\ning convolution designs into visual transformers,” arXiv preprint\narXiv:2103.11816, 2021.\n[106] A. Hassani, S. Walton, N. Shah, A. Abuduweili, J. Li, and H. Shi,\n“Escaping the big data paradigm with compact transformers,”\n2021.\n[107] Y. Li, K. Zhang, J. Cao, R. Timofte, and L. V . Gool, “Localvit:\nBringing locality to vision transformers,” 2021.\n[108] B. Graham, A. El-Nouby, H. Touvron, P . Stock, A. Joulin,\nH. J´egou, and M. Douze, “Levit: a vision transformer in convnet’s\nclothing for faster inference,” 2021.\n[109] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,\nW. Hubbard, and L. D. Jackel, “Backpropagation applied to\nhandwritten zip code recognition,” Neural computation , vol. 1,\nno. 4, pp. 541–551, 1989.\n[110] Q. Zhang and Y. Yang, “Rest: An efﬁcient transformer for visual\nrecognition,” arXiv preprint arXiv:2105.13677, 2021.\n[111] Z. Zhang, H. Zhang, L. Zhao, T. Chen, and T. Pﬁster, “Aggre-\ngating nested transformers,” in arXiv preprint arXiv:2105.12723 ,\n2021.\n[112] Z. Dai, H. Liu, Q. V . Le, and M. Tan, “Coatnet: Marrying convo-\nlution and attention for all data sizes,” 2021.\n[113] X. Chu, Z. Tian, B. Zhang, X. Wang, X. Wei, H. Xia, and C. Shen,\n“Conditional positional encodings for vision transformers,” 2021.\n[114] Y. Liu, G. Sun, Y. Qiu, L. Zhang, A. Chhatkuli, and L. Van Gool,\n“Transformer in convolutional neural networks,” arXiv preprint\narXiv:2106.03180, 2021.\n28\n[115] X. Chen, S. Xie, and K. He, “An empirical study of training self-\nsupervised visual transformers,” arXiv e-prints , pp. arXiv–2104,\n2021.\n[116] X. Chen, H. Fan, R. Girshick, and K. He, “Improved base-\nlines with momentum contrastive learning,” arXiv preprint\narXiv:2003.04297, 2020.\n[117] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum\ncontrast for unsupervised visual representation learning,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 9729–9738, 2020.\n[118] Z. Xie, Y. Lin, Z. Yao, Z. Zhang, Q. Dai, Y. Cao, and H. Hu,\n“Self-supervised learning with swin transformers,” arXiv preprint\narXiv:2105.04553, 2021.\n[119] J.-B. Grill, F. Strub, F. Altch ´e, C. Tallec, P . H. Richemond,\nE. Buchatskaya, C. Doersch, B. A. Pires, Z. D. Guo, M. G. Azar,\net al. , “Bootstrap your own latent: A new approach to self-\nsupervised learning,” arXiv preprint arXiv:2006.07733, 2020.\n[120] M. Caron, H. Touvron, I. Misra, H. J´egou, J. Mairal, P . Bojanowski,\nand A. Joulin, “Emerging properties in self-supervised vision\ntransformers,” arXiv preprint arXiv:2104.14294, 2021.\n[121] C. Li, J. Yang, P . Zhang, M. Gao, B. Xiao, X. Dai, L. Yuan,\nand J. Gao, “Efﬁcient self-supervised vision transformers for\nrepresentation learning,” arXiv preprint arXiv:2106.09785, 2021.\n[122] Y. Wang, X. Zhang, T. Yang, and J. Sun, “Anchor detr: Query\ndesign for transformer-based detector,” 2021.\n[123] T. Chen, S. Saxena, L. Li, D. J. Fleet, and G. Hinton, “Pix2seq: A\nlanguage modeling framework for object detection,” 2021.\n[124] Y. Fang, B. Liao, X. Wang, J. Fang, J. Qi, R. Wu, J. Niu, and W. Liu,\n“You only look at one sequence: Rethinking transformer in vision\nthrough object detection,” 2021.\n[125] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: To-\nwards real-time object detection with region proposal networks,”\nTP AMI, 2016.\n[126] R. Girshick, “Fast R-CNN,” in ICCV, 2015.\n[127] K. He, G. Gkioxari, P . Doll´ar, and R. Girshick, “Mask R-CNN,” in\nICCV, 2017.\n[128] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only\nlook once: Uniﬁed, real-time object detection,” in CVPR, 2016.\n[129] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and\nA. C. Berg, “SSD: Single shot multibox detector,” in ECCV, 2016.\n[130] T. Prangemeier, C. Reich, and H. Koeppl, “Attention-based trans-\nformers for instance segmentation of cells in microstructures,” in\n2020 IEEE International Conference on Bioinformatics and Biomedicine\n(BIBM), pp. 700–707, IEEE, 2020.\n[131] T.-Y. Lin, P . Doll ´ar, R. Girshick, K. He, B. Hariharan, and\nS. Belongie, “Feature pyramid networks for object detection,” in\nCVPR, 2017.\n[132] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:\nTransformers for image recognition at scale,” 2020.\n[133] H. Wang, Y. Zhu, B. Green, H. Adam, A. Yuille, and L.-C.\nChen, “Axial-DeepLab: Stand-alone axial-attention for panoptic\nsegmentation,” arXiv preprint arXiv:2003.07853, 2020.\n[134] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu,\nJ. Feng, T. Xiang, P . H. S. Torr, and L. Zhang, “Rethinking\nsemantic segmentation from a sequence-to-sequence perspective\nwith transformers,” 2021.\n[135] R. Strudel, R. Garcia, I. Laptev, and C. Schmid, “Segmenter:\nTransformer for semantic segmentation,” 2021.\n[136] A. Kirillov, K. He, R. Girshick, C. Rother, and P . Doll´ar, “Panoptic\nsegmentation,” in CVPR, 2019.\n[137] G. Neuhold, T. Ollmann, S. Rota Bulo, and P . Kontschieder, “The\nmapillary vistas dataset for semantic understanding of street\nscenes,” in ICCV, 2017.\n[138] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,\n“ImageNet: A large-scale hierarchical image database,” in CVPR,\n2009.\n[139] L. Yu, P . Poirson, S. Yang, A. C. Berg, and T. L. Berg, “Modeling\ncontext in referring expressions,” in ECCV, 2016.\n[140] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and\nK. Murphy, “Generation and comprehension of unambiguous\nobject descriptions,” in CVPR, 2016.\n[141] S. Kazemzadeh, V . Ordonez, M. Matten, and T. Berg, “Refer-\nitgame: Referring to objects in photographs of natural scenes,”\nin EMNLP, 2014.\n[142] N. Parmar, A. Vaswani, J. Uszkoreit, Ł. Kaiser, N. Shazeer, A. Ku,\nand D. Tran, “Image transformer,” in ICML, 2018.\n[143] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and\nI. Sutskever, “Generative pretraining from pixels,” inICML, 2020.\n[144] P . Esser, R. Rombach, and B. Ommer, “Taming transformers for\nhigh-resolution image synthesis,” arXiv:2012.09841, 2020.\n[145] Y. Jiang, S. Chang, and Z. Wang, “Transgan: Two transformers\ncan make one strong gan,” 2021.\n[146] A. K. Bhunia, S. Khan, H. Cholakkal, R. M. Anwer, F. S.\nKhan, and M. Shah, “Handwriting transformers,” arXiv preprint\narXiv:2104.03964, 2021.\n[147] A. Van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals,\nA. Graves, et al. , “Conditional image generation with pixelcnn\ndecoders,” in NeurIPS, 2016.\n[148] A. Krizhevsky, “Learning multiple layers of features from tiny\nimages,” tech. rep., 2009.\n[149] A. Coates, A. Ng, and H. Lee, “An analysis of single-layer\nnetworks in unsupervised feature learning,” in AISTATS, 2011.\n[150] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple\nframework for contrastive learning of visual representations,”\narXiv preprint arXiv:2002.05709, 2020.\n[151] P . Bachman, R. Hjelm, and W. Buchwalter, “Learning represen-\ntations by maximizing mutual information across views,” in\nNeurIPS, 2019.\n[152] O. J. H ´enaff, A. Srinivas, J. De Fauw, A. Razavi, C. Doersch, S. Es-\nlami, and A. v. d. Oord, “Data-efﬁcient image recognition with\ncontrastive predictive coding,” arXiv preprint arXiv:1905.09272 ,\n2019.\n[153] Y. Tian, D. Krishnan, and P . Isola, “Contrastive multiview cod-\ning,” arXiv preprint arXiv:1906.05849, 2019.\n[154] S. Khan, H. Rahmani, S. A. A. Shah, and M. Bennamoun, “A\nguide to convolutional neural networks for computer vision,”\nSynthesis Lectures on Computer Vision, 2018.\n[155] A. Radford, L. Metz, and S. Chintala, “Unsupervised represen-\ntation learning with deep convolutional generative adversarial\nnetworks,” arXiv preprint arXiv:1511.06434, 2015.\n[156] C. Gao, Y. Chen, S. Liu, Z. Tan, and S. Yan, “Adversarialnas: Ad-\nversarial neural architecture search for gans,” inCVPR, pp. 5680–\n5689, 2020.\n[157] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila,\n“Analyzing and improving the image quality of stylegan,” in\nCVPR, pp. 8110–8119, 2020.\n[158] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee,\n“Generative adversarial text to image synthesis,” in ICML, 2016.\n[159] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, and D. N.\nMetaxas, “StackGAN: Text to photo-realistic image synthesis\nwith stacked generative adversarial networks,” in ICCV, 2017.\n[160] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, and D. N.\nMetaxas, “StackGAN++: Realistic image synthesis with stacked\ngenerative adversarial networks,” TP AMI, 2018.\n[161] T. Xu, P . Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, and\nX. He, “AttnGAN: Fine-grained text to image generation with\nattentional generative adversarial networks,” in CVPR, 2018.\n[162] D. P . Kingma and M. Welling, “Auto-encoding variational bayes,”\narXiv preprint arXiv:1312.6114, 2013.\n[163] A. Razavi, A. van den Oord, and O. Vinyals, “Generating diverse\nhigh-ﬁdelity images with vq-vae-2,” in NeurISP, 2019.\n[164] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte,\n“Swinir: Image restoration using swin transformer,” in ICCVW,\n2021.\n[165] Z. Wang, X. Cun, J. Bao, and J. Liu, “Uformer: A general\nu-shaped transformer for image restoration,” arXiv preprint\narXiv:2106.03106, 2021.\n[166] Z. Lu, H. Liu, J. Li, and L. Zhang, “Efﬁcient transformer for single\nimage super-resolution,” arXiv preprint arXiv:2108.11084, 2021.\n[167] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y. Fu, “Image\nsuper-resolution using very deep residual channel attention net-\nworks,” in ECCV, 2018.\n[168] T. Dai, J. Cai, Y. Zhang, S. Xia, and L. Zhang, “Second-order\nattention network for single image super-resolution,” in CVPR,\n2019.\n[169] B. Niu, W. Wen, W. Ren, X. Zhang, L. Yang, S. Wang, K. Zhang,\nX. Cao, and H. Shen, “Single image super-resolution via a holistic\nattention network,” in ECCV, 2020.\n[170] B. Lim, S. Son, H. Kim, S. Nah, and K. Mu Lee, “Enhanced deep\nresidual networks for single image super-resolution,” inCVPRW,\n2017.\n29\n[171] Y. Tai, J. Yang, and X. Liu, “Image super-resolution via deep\nrecursive residual network,” in CVPR, 2017.\n[172] W. Han, S. Chang, D. Liu, M. Yu, M. Witbrock, and T. Huang,\n“Image super-resolution via dual-state recurrent networks,” in\nCVPR, 2018.\n[173] Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu, “Residual dense\nnetwork for image restoration,” TP AMI, 2020.\n[174] X. Wang, K. Yu, S. Wu, J. Gu, Y. Liu, C. Dong, Y. Qiao, and\nC. Change Loy, “ESRGAN: enhanced super-resolution generative\nadversarial networks,” in ECCVW, 2018.\n[175] S.-J. Park, H. Son, S. Cho, K.-S. Hong, and S. Lee, “SRFEAT: Single\nimage super-resolution with feature discrimination,” in ECCV,\n2018.\n[176] M. S. Sajjadi, B. Scholkopf, and M. Hirsch, “EnhanceNet: Single\nimage super-resolution through automated texture synthesis,” in\nICCV, 2017.\n[177] C. Ledig, L. Theis, F. Husz ´ar, J. Caballero, A. Cunningham,\nA. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, et al., “Photo-\nrealistic single image super-resolution using a generative adver-\nsarial network,” in CVPR, 2017.\n[178] J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for real-\ntime style transfer and super-resolution,” in ECCV, 2016.\n[179] J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Salimans, “Ax-\nial attention in multidimensional transformers,” arXiv preprint\narXiv:1912.12180, 2019.\n[180] G. Li, N. Duan, Y. Fang, M. Gong, D. Jiang, and M. Zhou,\n“Unicoder-VL: A universal encoder for vision and language by\ncross-modal pre-training.,” in AAAI, 2020.\n[181] J. Lu, D. Batra, D. Parikh, and S. Lee, “Vilbert: Pretraining task-\nagnostic visiolinguistic representations for vision-and-language\ntasks,” in NeurIPS, 2019.\n[182] S. Lee, Y. Yu, G. Kim, T. Breuel, J. Kautz, and Y. Song, “Param-\neter efﬁcient multimodal transformers for video representation\nlearning,” arXiv preprint arXiv:2012.04124, 2020.\n[183] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra,\nC. Lawrence Zitnick, and D. Parikh, “VQA: Visual question\nanswering,” in ICCV, 2015.\n[184] R. Zellers, Y. Bisk, A. Farhadi, and Y. Choi, “From recognition to\ncognition: Visual commonsense reasoning,” in CVPR, 2019.\n[185] K.-H. Lee, X. Chen, G. Hua, H. Hu, and X. He, “Stacked cross\nattention for image-text matching,” in ECCV, 2018.\n[186] A. Suhr, S. Zhou, A. Zhang, I. Zhang, H. Bai, and Y. Artzi,\n“A corpus for reasoning about natural language grounded in\nphotographs,” arXiv preprint arXiv:1811.00491, 2018.\n[187] J. Carreira, E. Noland, C. Hillier, and A. Zisserman, “A short\nnote on the kinetics-700 human action dataset,” arXiv:1907.06987,\n2019.\n[188] K. Soomro, A. R. Zamir, and M. Shah, “UCF101: A dataset of 101\nhuman actions classes from videos in the wild,” arXiv preprint\narXiv:1212.0402, 2012.\n[189] J. F. Gemmeke, D. P . Ellis, D. Freedman, A. Jansen, W. Lawrence,\nR. C. Moore, M. Plakal, and M. Ritter, “Audio set: An ontology\nand human-labeled dataset for audio events,” in ICASSP, 2017.\n[190] G. A. Sigurdsson, G. Varol, X. Wang, A. Farhadi, I. Laptev, and\nA. Gupta, “Hollywood in homes: Crowdsourcing data collection\nfor activity understanding,” in ECCV, 2016.\n[191] H. Tan and M. Bansal, “Vokenization: Improving language un-\nderstanding with contextualized, visual-grounded supervision,”\nin EMNLP, 2020.\n[192] W. Hao, C. Li, X. Li, L. Carin, and J. Gao, “Towards learning\na generic agent for vision-and-language navigation via pre-\ntraining,” in CVPR, 2020.\n[193] A. Majumdar, A. Shrivastava, S. Lee, P . Anderson, D. Parikh,\nand D. Batra, “Improving vision-and-language navigation with\nimage-text pairs from the web,” arXiv preprint arXiv:2004.14973 ,\n2020.\n[194] K. Chen, J. K. Chen, J. Chuang, M. V ´azquez, and S. Savarese,\n“Topological planning with transformers for vision-and-\nlanguage navigation,” arXiv preprint arXiv:2012.05292, 2020.\n[195] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agar-\nwal, G. Sastry, A. Askell, P . Mishkin, J. Clark, et al. , “Learning\ntransferable visual models from natural language supervision,”\nImage, vol. 2, p. T2, 2021.\n[196] P . Sharma, N. Ding, S. Goodman, and R. Soricut, “Conceptual\ncaptions: A cleaned, hypernymed, image alt-text dataset for\nautomatic image captioning,” in ACL, 2018.\n[197] L. Zhou, H. Palangi, L. Zhang, H. Hu, J. Corso, and J. Gao,\n“Uniﬁed vision-language pre-training for image captioning and\nvqa,” in AAAI, vol. 34, pp. 13041–13049, 2020.\n[198] C. Sun, F. Baradel, K. Murphy, and C. Schmid, “Learning video\nrepresentations using contrastive bidirectional transformer,”\narXiv preprint arXiv:1906.05743, 2019.\n[199] C. Alberti, J. Ling, M. Collins, and D. Reitter, “Fusion of detected\nobjects in text for visual question answering,” in EMNLP, 2019.\n[200] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz,\nS. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma, et al. , “Visual\ngenome: Connecting language and vision using crowdsourced\ndense image annotations,” IJCV, 2017.\n[201] V . Ordonez, G. Kulkarni, and T. L. Berg, “Im2text: Describing\nimages using 1 million captioned photographs,” inNeurIPS, 2011.\n[202] P . Zhang, X. Li, X. Hu, J. Yang, L. Zhang, L. Wang, Y. Choi,\nand J. Gao, “Vinvl: Revisiting visual representations in vision-\nlanguage models,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 5579–5588, 2021.\n[203] A. Kamath, M. Singh, Y. LeCun, I. Misra, G. Synnaeve, and\nN. Carion, “Mdetr–modulated detection for end-to-end multi-\nmodal understanding,” arXiv preprint arXiv:2104.12763, 2021.\n[204] J. Deng, Z. Yang, T. Chen, W. Zhou, and H. Li, “Transvg: End-to-\nend visual grounding with transformers,” 2021.\n[205] M. Li and L. Sigal, “Referring transformer: A one-step approach\nto multi-task visual grounding,” arXiv preprint arXiv:2106.03089 ,\n2021.\n[206] Y. Du, Z. Fu, Q. Liu, and Y. Wang, “Visual grounding with\ntransformers,” arXiv preprint arXiv:2105.04281, 2021.\n[207] S. Ging, M. Zolfaghari, H. Pirsiavash, and T. Brox, “COOT: Co-\noperative hierarchical transformer for video-text representation\nlearning,” arXiv preprint arXiv:2011.00597, 2020.\n[208] H. Seong, J. Hyun, and E. Kim, “Video multitask transformer\nnetwork,” in ICCV Workshops, pp. 0–0, 2019.\n[209] Y. Wang, Z. Xu, X. Wang, C. Shen, B. Cheng, H. Shen, and H. Xia,\n“End-to-end video instance segmentation with transformers,”\narXiv preprint arXiv:2011.14503, 2020.\n[210] L. Zhou, Y. Zhou, J. Corso, R. Socher, and C. Xiong, “End-to-\nend dense video captioning with masked transformer,” in CVPR,\n2018.\n[211] D. Neimark, O. Bar, M. Zohar, and D. Asselmann, “Video trans-\nformer network,” arXiv preprint arXiv:2102.00719, 2021.\n[212] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lu ˇci´c, and\nC. Schmid, “Vivit: A video vision transformer,” arXiv preprint\narXiv:2103.15691, 2021.\n[213] G. Bertasius, H. Wang, and L. Torresani, “Is space-time attention\nall you need for video understanding?,” in Proceedings of the\nInternational Conference on Machine Learning (ICML) , July 2021.\n[214] R. Krishna, K. Hata, F. Ren, L. Fei-Fei, and J. Carlos Niebles,\n“Dense-captioning events in videos,” in ICCV, pp. 706–715, 2017.\n[215] L. Zhou, C. Xu, and J. Corso, “Towards automatic learning of\nprocedures from web instructional videos,” in AAAI, vol. 32,\n2018.\n[216] C. Plizzari, M. Cannici, and M. Matteucci, “Spatial tempo-\nral transformer network for skeleton-based action recognition,”\narXiv preprint arXiv:2008.07404, 2020.\n[217] A. Shahroudy, J. Liu, T.-T. Ng, and G. Wang, “NTU RGB+D: A\nlarge scale dataset for 3d human activity analysis,” in CVPR,\n2016.\n[218] J. Liu, A. Shahroudy, M. Perez, G. Wang, L.-Y. Duan, and A. C.\nKot, “NTU RGB+D 120: A large-scale benchmark for 3d human\nactivity understanding,” TP AMI, 2019.\n[219] H. Fan, B. Xiong, K. Mangalam, Y. Li, Z. Yan, J. Malik, and\nC. Feichtenhofer, “Multiscale vision transformers,” 2021.\n[220] J. Wang, G. Bertasius, D. Tran, and L. Torresani, “Long-short tem-\nporal contrastive learning of video transformers,” arXiv preprint\narXiv:2106.09212, 2021.\n[221] L. Yang, Y. Fan, and N. Xu, “Video instance segmentation,” in\nICCV, pp. 5188–5197, 2019.\n[222] G. Bertasius and L. Torresani, “Classifying, segmenting, and\ntracking object instances in video with mask propagation,” in\nCVPR, pp. 9739–9748, 2020.\n[223] E. Triantaﬁllou, T. Zhu, V . Dumoulin, P . Lamblin, U. Evci, K. Xu,\nR. Goroshin, C. Gelada, K. Swersky, P .-A. Manzagol,et al., “Meta-\ndataset: A dataset of datasets for learning to learn from few\nexamples,” in ICLR, 2020.\n30\n[224] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with\ngraph convolutional networks,” arXiv preprint arXiv:1609.02907 ,\n2016.\n[225] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhut-\ndinov, and A. J. Smola, “Deep sets,” in NeurIPS, 2017.\n[226] L. Liu, W. Hamilton, G. Long, J. Jiang, and H. Larochelle, “A\nuniversal representation transformer layer for few-shot image\nclassiﬁcation,” 2020.\n[227] H. Edwards and A. Storkey, “Towards a neural statistician,”arXiv\npreprint arXiv:1606.02185, 2016.\n[228] J. Lee, Y. Lee, J. Kim, A. Kosiorek, S. Choi, and Y. W. Teh,\n“Set transformer: A framework for attention-based permutation-\ninvariant neural networks,” in ICML, 2019.\n[229] J. Lee, Y. Lee, and Y. W. Teh, “Deep amortized clustering,” arXiv\npreprint arXiv:1909.13433, 2019.\n[230] H. Zhao, L. Jiang, J. Jia, P . Torr, and V . Koltun, “Point trans-\nformer,” arXiv preprint arXiv:2012.09164, 2020.\n[231] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin,\nand S.-M. Hu, “Pct: Point cloud transformer,” arXiv preprint\narXiv:2012.09688, 2020.\n[232] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao,\n“3D ShapeNets: A deep representation for volumetric shapes,” in\nCVPR, 2015.\n[233] A. X. Chang, T. Funkhouser, L. Guibas, P . Hanrahan, Q. Huang,\nZ. Li, S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, and\nF. Yu, “ShapeNet: An information-rich 3d model repository,”\narXiv preprint arXiv:1512.03012, 2015.\n[234] C. Ionescu, D. Papava, V . Olaru, and C. Sminchisescu, “Hu-\nman3.6M: Large scale datasets and predictive methods for 3D\nhuman sensing in natural environments,” TP AMI, 2013.\n[235] T. von Marcard, R. Henschel, M. J. Black, B. Rosenhahn, and\nG. Pons-Moll, “Recovering accurate 3d human pose in the wild\nusing imus and a moving camera,” in ECCV, 2018.\n[236] C. Zimmermann, D. Ceylan, J. Yang, B. Russell, M. Argus, and\nT. Brox, “FreiHAND: A dataset for markerless capture of hand\npose and shape from single rgb images,” in ICCV, 2019.\n[237] “OpenAI’s GPT-3 language model: A technical overview.” https:\n//lambdalabs.com/blog/demystifying-gpt-3/. Accessed: 2020-\n12-31.\n[238] X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer, “Scaling vision\ntransformers,” 2021.\n[239] P . Young, A. Lai, M. Hodosh, and J. Hockenmaier, “From image\ndescriptions to visual denotations: New similarity metrics for\nsemantic inference over event descriptions,” TACL, 2014.\n[240] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh,\n“Making the v in vqa matter: Elevating the role of image un-\nderstanding in visual question answering,” in CVPR, 2017.\n[241] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hock-\nenmaier, and S. Lazebnik, “Flickr30k entities: Collecting region-\nto-phrase correspondences for richer image-to-sentence models,”\nin ICCV, 2015.\n[242] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “PointNet++: Deep\nhierarchical feature learning on point sets in a metric space,”\nNeurIPS, 2017.\n[243] H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve, and\nH. J´egou, “Going deeper with image transformers,”arXiv preprint\narXiv:2103.17239, 2021.\n[244] S. Xie, R. Girshick, P . Doll ´ar, Z. Tu, and K. He, “Aggregated\nresidual transformations for deep neural networks,” in CVPR,\n2017.\n[245] R. Child, S. Gray, A. Radford, and I. Sutskever, “Generating long\nsequences with sparse transformers,” arXiv:1904.10509, 2019.\n[246] N. Kitaev, Ł. Kaiser, and A. Levskaya, “Reformer: The efﬁcient\ntransformer,” in ICLR, 2020.\n[247] I. Bello, “Lambdanetworks: Modeling long-range interactions\nwithout attention,” in International Conference on Learning Repre-\nsentations, 2021.\n[248] A. Vyas, A. Katharopoulos, and F. Fleuret, “Fast transformers\nwith clustered attention,” NeurIPS, 2020.\n[249] Y.-H. Wu, Y. Liu, X. Zhan, and M.-M. Cheng, “P2t: Pyramid\npooling transformer for scene understanding,” arXiv preprint\narXiv:2106.12011, 2021.\n[250] A. Vaswani, P . Ramachandran, A. Srinivas, N. Parmar, B. Hecht-\nman, and J. Shlens, “Scaling local self-attention for parameter\nefﬁcient visual backbones,” in Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition , pp. 12894–12904,\n2021.\n[251] X. Dong, J. Bao, D. Chen, W. Zhang, N. Yu, L. Yuan, D. Chen,\nand B. Guo, “Cswin transformer: A general vision trans-\nformer backbone with cross-shaped windows,” arXiv preprint\narXiv:2107.00652, 2021.\n[252] Y. Xiong, Z. Zeng, R. Chakraborty, M. Tan, G. Fung, Y. Li, and\nV . Singh, “Nystr\\” omformer: A nystr\\” om-based algorithm for\napproximating self-attention,” in AAAI, 2021.\n[253] Y. Tay, D. Bahri, D. Metzler, D. Juan, Z. Zhao, and C. Zheng,\n“Synthesizer: Rethinking self-attention in transformer models,”\nin ICML, 2021.\n[254] H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A. Smith, and\nL. Kong, “Random feature attention,” in ICLR, 2021.\n[255] K. Choromanski, V . Likhosherstov, D. Dohan, X. Song, A. Gane,\nT. Sarlos, P . Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al. ,\n“Rethinking attention with performers,” in ICLR, 2021.\n[256] Y. Tay, D. Bahri, L. Yang, D. Metzler, and D.-C. Juan, “Sparse\nsinkhorn attention,” in ICML, 2020.\n[257] X. Chen, C.-J. Hsieh, and B. Gong, “When vision transformers\noutperform resnets without pretraining or strong data augmen-\ntations,” arXiv preprint arXiv:2106.01548, 2021.\n[258] P . Foret, A. Kleiner, H. Mobahi, and B. Neyshabur, “Sharpness-\naware minimization for efﬁciently improving generalization,”\narXiv preprint arXiv:2010.01412, 2020.\n[259] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna,\n“Rethinking the inception architecture for computer vision,” in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 2818–2826, 2016.\n[260] S. He, H. Luo, P . Wang, F. Wang, H. Li, and W. Jiang, “Transreid:\nTransformer-based object re-identiﬁcation,” arXiv:2102.04378,\n2021.\n[261] D. R. So, C. Liang, and Q. V . Le, “The evolved transformer,” 2019.\n[262] H. Wang, Z. Wu, Z. Liu, H. Cai, L. Zhu, C. Gan, and S. Han, “Hat:\nHardware-aware transformers for efﬁcient natural language pro-\ncessing,” 2020.\n[263] M. Chen, H. Peng, J. Fu, and H. Ling, “Autoformer:\nSearching transformers for visual recognition,” arXiv preprint\narXiv:2107.00651, 2021.\n[264] C. Li, T. Tang, G. Wang, J. Peng, B. Wang, X. Liang, and\nX. Chang, “Bossnas: Exploring hybrid cnn-transformers with\nblock-wisely self-supervised neural architecture search,” arXiv\npreprint arXiv:2103.12424, 2021.\n[265] B. Chen, P . Li, C. Li, B. Li, L. Bai, C. Lin, M. Sun, W. Ouyang,\net al., “Glit: Neural architecture search for global and local image\ntransformer,” arXiv preprint arXiv:2107.02960, 2021.\n[266] M. Naseer, K. Ranasinghe, S. Khan, M. Hayat, F. S. Khan, and\nM.-H. Yang, “Intriguing properties of vision transformers,”arXiv\npreprint arXiv:2105.10497, 2021.\n[267] E. Voita, D. Talbot, F. Moiseev, R. Sennrich, and I. Titov, “Ana-\nlyzing multi-head self-attention: Specialized heads do the heavy\nlifting, the rest can be pruned,” arXiv preprint arXiv:1905.09418 ,\n2019.\n[268] S. Abnar and W. Zuidema, “Quantifying attention ﬂow in trans-\nformers,” arXiv preprint arXiv:2005.00928, 2020.\n[269] H. Chefer, S. Gur, and L. Wolf, “Transformer interpretability\nbeyond attention visualization,” arXiv preprint arXiv:2012.09838 ,\n2020.\n[270] B. Li, S. Pandey, H. Fang, Y. Lyv, J. Li, J. Chen, M. Xie, L. Wan,\nH. Liu, and C. Ding, “FTRANS: energy-efﬁcient acceleration of\ntransformers using fpga,” in ISLPED, 2020.\n[271] G. Bender, P .-J. Kindermans, B. Zoph, V . Vasudevan, and Q. Le,\n“Understanding and simplifying one-shot architecture search,”\nin ICML, 2018.\n[272] Z. Guo, X. Zhang, H. Mu, W. Heng, Z. Liu, Y. Wei, and J. Sun,\n“Single path one-shot neural architecture search with uniform\nsampling,” arXiv preprint arXiv:1904.00420, 2019.\n[273] H. Pham, M. Y. Guan, B. Zoph, Q. V . Le, and J. Dean, “Efﬁcient\nneural architecture search via parameter sharing,” in ICML, 2018.\n[274] A. Jaegle, F. Gimeno, A. Brock, A. Zisserman, O. Vinyals, and\nJ. Carreira, “Perceiver: General perception with iterative atten-\ntion,” arXiv preprint arXiv:2103.03206, 2021.\n[275] A. Jaegle, S. Borgeaud, J.-B. Alayrac, C. Doersch, C. Ionescu,\nD. Ding, S. Koppula, D. Zoran, A. Brock, E. Shelhamer, et al. ,\n“Perceiver io: A general architecture for structured inputs &\noutputs,” arXiv preprint arXiv:2107.14795, 2021.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.741474986076355
    },
    {
      "name": "Transformer",
      "score": 0.64666748046875
    },
    {
      "name": "Segmentation",
      "score": 0.5532912611961365
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5199043154716492
    },
    {
      "name": "Scalability",
      "score": 0.4688728153705597
    },
    {
      "name": "Image processing",
      "score": 0.4569951891899109
    },
    {
      "name": "Computer vision",
      "score": 0.3761197030544281
    },
    {
      "name": "Machine learning",
      "score": 0.32622072100639343
    },
    {
      "name": "Engineering",
      "score": 0.13062000274658203
    },
    {
      "name": "Database",
      "score": 0.11653080582618713
    },
    {
      "name": "Image (mathematics)",
      "score": 0.11202353239059448
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}