{
  "title": "SymbolicGPT: A Generative Transformer Model for Symbolic Regression",
  "url": "https://openalex.org/W3175526646",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4306705246",
      "name": "Valipour, Mojtaba",
      "affiliations": []
    },
    {
      "id": null,
      "name": "You, Bowen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4298468252",
      "name": "Panju, Maysum",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2754134202",
      "name": "Ghodsi, Ali",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1979769287",
    "https://openalex.org/W3157497780",
    "https://openalex.org/W3113059633",
    "https://openalex.org/W2962858226",
    "https://openalex.org/W2530064072",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W3101699553",
    "https://openalex.org/W2129202132",
    "https://openalex.org/W2991933648",
    "https://openalex.org/W3215776097",
    "https://openalex.org/W2995359496",
    "https://openalex.org/W3118689318",
    "https://openalex.org/W3006755302",
    "https://openalex.org/W3082465854",
    "https://openalex.org/W2091338396",
    "https://openalex.org/W2026482919",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2194775991"
  ],
  "abstract": "Symbolic regression is the task of identifying a mathematical expression that best fits a provided dataset of input and output values. Due to the richness of the space of mathematical expressions, symbolic regression is generally a challenging problem. While conventional approaches based on genetic evolution algorithms have been used for decades, deep learning-based methods are relatively new and an active research area. In this work, we present SymbolicGPT, a novel transformer-based language model for symbolic regression. This model exploits the advantages of probabilistic language models like GPT, including strength in performance and flexibility. Through comprehensive experiments, we show that our model performs strongly compared to competing models with respect to the accuracy, running time, and data efficiency.",
  "full_text": "SymbolicGPT: A Generative Transformer Model for\nSymbolic Regression\nMojtaba Valipour∗\nUniversity of Waterloo\nmojtaba.valipour@uwaterloo.ca\nBowen You†\nUniversity of Waterloo\nbyyou@uwaterloo.ca\nMaysum Panju†\nUniversity of Waterloo\nmhpanju@uwaterloo.ca\nAli Ghodsi∗†\nUniversity of Waterloo\nali.ghodsi@uwaterloo.ca\nAbstract\nSymbolic regression is the task of identifying a mathematical expression that best\nﬁts a provided dataset of input and output values. Due to the richness of the space of\nmathematical expressions, symbolic regression is generally a challenging problem.\nWhile conventional approaches based on genetic evolution algorithms have been\nused for decades, deep learning-based methods are relatively new and an active\nresearch area. In this work, we present SymbolicGPT, a novel transformer-based\nlanguage model for symbolic regression3. This model exploits the advantages of\nprobabilistic language models like GPT, including strength in performance and\nﬂexibility. Through comprehensive experiments, we show that our model performs\nstrongly compared to competing models with respect to the accuracy, running time,\nand data efﬁciency.\n1 Introduction\nDeep learning and neural networks have earned an esteemed reputation for being capable tools for\nsolving a wide variety of problems over countless application domains. Notably, deep language\nmodels have made an enormous impact in the ﬁeld of linguistics and natural language processing.\nWith the advances in technology like Generative Pre-trained Transformers, or GPT [17], the scope of\nproblems now accessible to automated methods continues to grow. It is particularly interesting when\nlanguage models are used for tasks that, at ﬁrst glance, do not seem to have any relationship with\nlanguage at all.\nSymbolic regression, the problem of ﬁnding a mathematical equation to ﬁt a set of data, is one\nsuch task. The objective of symbolic regression is to obtain a closed-form symbolic mathematical\nexpression to describe the relationship between speciﬁed predictor and response variables, where the\nmathematical expression is allowed to be ﬂexible without being restricted to a particular structure\nor family. More precisely, the goal in symbolic regression is to recover a mathematical function\nf in terms of the input variables x = [x1 ...x d]⊤, given a set of datapoint vectors of the form\nD= {(xi,yi)}n\ni=1, such that f(xi) =yi for all i. Here, x1,...,x d,yi are scalars and xi ∈Rd.\nBy not imposing any structural constraints on the shape of the desired equation, symbolic regression\nis a much more difﬁcult task compared to other kinds of regression, such as linear regression or\nmultinomial regression, as the search space of candidate expressions is so much larger.\n∗David R. Cheriton School of Computer Science\n†Department of Statistics and Actuarial Science\n3Code and results available at https://git.uwaterloo.ca/data-analytics-lab/symbolicgpt2\nPreprint. Under review.\narXiv:2106.14131v1  [cs.LG]  27 Jun 2021\nThe most common approach for symbolic regression is based on genetic programming, where\nnumerous candidate parse trees are generated, evaluated, combined, and mutated in an evolutionary\nway until a tree is produced that models an expression that ﬁts the dataset up to a required accuracy\nlevel. In essence, it is a search strategy over the vast space of mathematical expressions, seeking the\nformula that would optimize an objective function.\nIn this typical framework, which applies not only to genetic methods but also many deep-learning-\nbased approaches to symbolic regression, the goal is to identify a mathematical expression that ﬁts\nmost optimally given a single input dataset. This dataset is the basis over which all the training occurs.\nConsequently, when presented with any new dataset (as a fresh instance of the task of symbolic\nregression), the entire training procedure must begin again from scratch.\nIn this work, we explore an alternative approach to symbolic regression by considering it as a task in\nlanguage modelling. Symbolic mathematics behaves as a language in its own right, with well-formed\nmathematical expressions treated as valid “sentences” in this language. It is natural, therefore, to\nconsider using deep language models to address tasks involving symbolic mathematics.\nWe can frame the regression problem as an exercise in captioning. Each instance takes input in the\nform of a cloud of points in Rd+1, with each point consisting of dcomponents corresponding to x\nand a single component for the associated yvalue. The instance returns a statement in the language\nof symbolic mathematics to describe the point set. By training a model to correctly “caption” datasets\nwith the equations underlying them, we will have a system for performing symbolic regression\nquickly and accurately.\nBased on this idea, we present SymbolicGPT, a method that makes use of deep language models for\nsymbolic regression. SymbolicGPT employs a framework that represents a major shift from the way\nsymbolic regression is conventionally performed. We move the task of symbolic regression from\nbeing a strictly quantitative problem into a language one. Effectively, we propose a system that not\nonly learns the language of symbolic mathematics, but also the underlying relationship between point\nclouds and mathematical expressions that deﬁne them.\nAs part of SymbolicGPT, we make use of a T-net model [16] to represent the input point cloud in\nan order-invariant way. This allows us to obtain vector embeddings of the entire input dataset for\nsymbolic regression instances without depending on the number of points in the dataset or the order\nin which they are given.\nA major advantage of SymbolicGPT is that we are no longer training a model to learn an equation\nfor an individual dataset in each instance of symbolic regression. Instead, we train a single language\nmodel once, and use that trained model to rapidly solve instances of symbolic regression as individual\ncaptioning tasks. We will show that SymbolicGPT not only presents a running time speed boost of an\norder of magnitude or more, but also provides competent performance in accurately reconstructing\nmathematical equations to ﬁt numerical datasets, presenting a new frontier for language models and a\nnovel direction for approaching symbolic regression.\n2 Related Work\nTraditionally, the problem of symbolic regression has been tackled with methods based on genetic\nalgorithms [12, 1, 20, 13, 22]. In this framework, the task is seen as a search space optimization\nproblem where symbolic expressions are candidates and the expression with the greatest ﬁtness, or\nﬁtting accuracy on the training data, is obtained through a process of mutation and evolution. Although\nthis approach has shown success in practice, it is computationally expensive, highly randomized,\nrequires instance-based training, and struggles with learning equations containing many variables\nand constants.\nMore recently, newer approaches to symbolic regression have arisen that make use of neural networks.\nThe EQL (Equation Learner) model [11, 19] is an example of performing symbolic regression by\ntraining a neural network that represents a symbolic expression. This method, and others based on it\n[5, 8], take advantage of advances in deep learning as an alternative to genetic approaches. However,\nthey still approach symbolic regression as an instance-based problem, training a model from scratch\ngiven every new input dataset for a regression task.\n2\nA recent study [2] presents a novel, language-based method for handling symbolic regression as a\nmachine translation task, similar to the approach used by [10] for performing symbolic integration\nand solving differential equations. Given an input dataset, the algorithm treats the input as a text string\nand passes it through a trained sequence-to-sequence LSTM to produce an output text string that is\nparsed as the desired symbolic expression. Although this method overcomes the cost of per-instance\ntraining, its interpretation of the input dataset as a textual string limits its usability, as the input data\nmust follow speciﬁc constraints, such as ﬁtting a 1-dimensional mesh of ﬁxed size. Consequently,\nthis method can only be used in one-dimensional space. However, in most problems, more than one\nvariable is involved and we need to ﬁnd a multivariate function. In this work, we propose a method\nthat removes such limitations on the structure of input data. This can be applied easily to symbolic\nregression problems in high-dimensional spaces and when many variables are involved.\nAnother active area of research is to use deep reinforcement learning methods to tackle this problem\n[8, 15]. The method presented by Petersen et. al. [ 15] uses a hybrid approach between traditional\ngenetic algorithms and deep learning methods. Here, the authors use deep RNNs to generate samples\nof candidate skeletons. As an example, if the function was f(x) = x2 + 1, the corresponding\nskeleton would be C1x2 + C2. As in [ 9], numerical optimization is then used to optimize for the\nconstants of each candidate skeleton. A reinforcement learning algorithm is applied to train the RNN\nto generate better skeletons at every iteration. However, this method still relies on the iterative nature\nof traditional genetic algorithms as well as numerical optimization. This results in a computationally\nintensive process in order to generate a prediction for each equation.\n3 Method\nOur model for symbolic regression, SymbolicGPT, consists of three main stages: obtaining an\norder-invariant embedding of the input dataset using a T-net [ 16], obtaining a skeleton equation\nusing a GPT language model [18], and optimizing constant values to ﬁll in the equation skeleton. In\naddition to discussing each of these steps, we also present the method for generating our equation\ndatasets.\n3.1 Equation Generation\nTo train our language model, we need a large dataset of solved instances of symbolic regression. This\ndataset is a collection of input-label pairs where each input is in the form of a numerical dataset, itself\na set of input and output pairs {(x,y)}, and the corresponding label is a string encoding the symbolic\nexpression governing the relationship between variables in the numerical dataset.\nIn order to ensure that the language model is able to generalize to unseen equations, having good\ntraining data is key. It is necessary to train the model over a wide, diverse set of training equations in\norder to prevent the language model from overﬁtting.\nThere are a number of different ways to randomly sample symbolic mathematical expressions. One\napproach, as used in [3], is to consider symbolic expressions as constructed by rules in a context-free\ngrammar, and randomly sampling from rules until reaching a string containing only terminal values.\nAnother approach, taken in [10], uses parse-tree representations of symbolic formulas, presenting a\nmethod that samples uniformly from all trees of nnodes and then ﬁlling in nodes with valid operators\nor variable values.\nFor our training dataset, we use an approach similar to the latter, where we start with a blank parse\ntree and then “decorate” the nodes with choices of operators and variables. In contrast with [10], we\ndo not constrain our parse trees by the number of nodes, but by the number of levels. This enables\nmore control over the maximum level of complexity in the equations used in our training set, as the\nnumber of levels in the parse tree corresponds to the number of potential function nestings, a measure\nof how complex an equation can be.\nWe begin by ﬁxing k, the maximum number of levels in the parse tree for the equations we wish\nto encounter in our training set. We also begin with a pre-speciﬁed number of variables, d, and a\npre-selected set of operators, P = {u1,...,u m}, that are allowed to appear in any training equation.\nThen, for each data-equation pair in our training set, we generate a perfectly balanced binary tree of\ndepth k, having 2k−1 −1 internal nodes and 2k−1 leaf nodes. These nodes originally start off empty\nto form the template of a symbolic expression.\n3\nThe template is ﬁlled in by randomly selecting valid choices to occupy each node in the parse tree.\nFor leaf nodes, each node is randomly assigned with a variable from the set{x1,...,x d}. For interior\nnodes, operators from the set P are randomly chosen. Once ﬁlled in, the parse tree can naturally be\ninterpreted as a symbolic expression. For nodes ﬁlled in by binary operators, both of their child nodes\nare used as input; in the case of unary operators, only the left child is used as input, and the right\nchild is ignored. Importantly, the unary operator “id(·)”, which returns its input argument unchanged,\nis included in P, which effectively allows for equations with shallower or unbalanced parse trees to\nstill be represented using this template.\nAdditionally, to ensure that the equations generated are not all too complex, we introduce “terminal”\nnodes in which children of the terminal nodes are discarded. This ensures that we obtain a diverse set\nof equations within the training set.\nAs a ﬁnal step for the equation generation procedure, constants are incorporated into the equation\nby inserting them at nodes in the parse tree. Given a speciﬁed value r∈[0,1] and constant bounds\ncmin and cmax, for each node in the tree, a random real-valued constant is selected between cmin\nand cmax and, with probability r, is inserted as a multiplicative factor the subtree rooted at that node.\nSimilarly, a second random constant is selected between cmin and cmax and, with probability r, is\ninserted as an additive bias to the subtree rooted at that node. By varying the constant ratio r, the\nequations can be customized to include many constants, few constants, or none at all.\nOnce an equation is generated, an input dataset for symbolic regression can be produced by evaluating\nthe symbolic expression at ndifferent vectors x randomly sampled from some region of interest in\nRd. The label value for the symbolic regression instance would be the symbolic expression. This\nprocess can be repeated many times to construct the training set by which our SymbolicGPT model\nwill learn how to perform symbolic regression.\n3.2 Order-Invariant Embedding\nOnce the training set of input data and output equations is generated, it is used to train our model for\ntranslating numerical datasets into equation strings.\nThe ﬁrst step is to convert the input datasetD= {(xi,yi)}n\ni=1 ⊂Rd+1 into a single vector embedding\nwD ∈Re. For the conversion to be useful, it must have two properties. First, it should not strictly\ndepend on the number of points in the input dataset, n. In practice, the datasets provided as input to a\nsymbolic regression solver may have varying sizes, and we do not want our method to be restricted to\ncases with a ﬁxed number of input points.\nSecond, the conversion method should not be sensitive to the order in which the points of the\ndataset are given. The input to a symbolic regression instance is a collection of datapoints, rather\nthan a sequence, and the optimal symbolic expression to ﬁt the dataset should not depend on the\norder in which the points are listed. Thus, the vector embedding of the dataset should be similarly\norder-invariant.\nOur approach for converting the dataset D into a vector embedding is to use a T-net, a kind of\ndeep network that makes use of a global max-pooling layer to provide order-invariance over its\narbitrarily-sized input [16]. The T-net takes as input the dataset D, consisting of ndatapoints over\ndvariables, represented in matrix format as X ∈Rn×(d+1), where ncan be any number and d, the\nnumber of allowable variables, is ﬁxed in advance. Any symbolic regression instance with fewer than\ndvariables can be padded with 0 values, bringing the total number of variables up to d.\nThe matrix X is ﬁrst normalized using a learnable normalization layer in order to regulate extreme\nvalues from the input. The normalized input points are then passed through three stages of MLP\nnetworks. Within each stage, each of the nrows of X are passed individually, albeit in parallel,\nthrough a single fully connected layer, where weights are shared between the networks for all points\nfor that stage. The ﬁrst stage results in npoints encoded in e-dimensional space; the second stage\ntakes them into 2edimensions, and the output after the third stage are npoints having 4edimensions\neach.\nThe next layer in the T-net is a global max pool, which reduces the n×4eoutput of the previous step\ndown to a 1 ×4e-dimensional vector. The max-pooling eliminates the dependence on both nand the\norder of the input points, achieving both goals needed for our vector embedding. Finally, the output\nof the global max-pool is passed through two more fully connected layers, resulting in a single output\n4\nToken Embedding Wt\nE\nPoint Embedding WD\nINPUT: X(eq) = Sequence of Tokens e.g. <SOS>\nPositional Embedding Wp\nEmbedding WD+Wp+X(eq)Wt\nINPUT: X(points)= (n= #points, d=#variables)\nn×(d+1)\n...nShared MLPShared MLP W(d+1)×eShared MLP\n...Shared MLPShared MLP Shared MLPn We×2e\n...Shared MLPShared MLP Shared MLPn W2e×4e\nTransformer Block\nRepeat \n× 8\nMasked Multi-Headed Self-Attention\nPointwise Feed Forward\nAdd & Layer Norm\nTransposed Embedding Wt\nSoftmax\nOutput: Probabilities Over Tokens (Symbols in Equations)\nGPT:T-Net:\nT\nh\nhWt\nT\nAdd & Layer Norm\nLearn an Order-Invariant Representation for Points\nGlobal Max Pool on Dimn\nFully Connected (MLP)\nFully Connected  (MLP)\nn×(d+1)\nn×(d+1)\nn×e\nn×2e\nn×4e\n1×4e\n1×2e\n1×e\nBatch Norm\nW4e×2e\nW2e×e\nFigure 1: The architecture of SymbolicGPT. The left box illustrates the structure of our order-invariant\nT-net for obtaining a vector representation of the input dataset, and the right box shows the structure\nof the GPT language model for producing symbolic equation skeletons.\nvector wD, an e-dimensional embedding of the input dataset. The overall structure of the T-net is\nshown in the left part of Figure 1.\n3.3 Generative Model Architecture\nThe main component of SymbolicGPT is the deep network for producing symbolic equations, as\nimplemented using a GPT language model [17, 18, 4]. This framework takes in two pieces of input:\nthe order-invariant embedding of the point cloud wD as produced by the T-net, representing the input\ndataset, and a sequence of tokens, X(eq), used to initialize the output formula string. In the typical\nregression case where no information is provided about the output symbolic expression in advance,\nthis token sequence would be the singleton Start-of-Sequence token ⟨SOS⟩, although in general it\ncan be any desired preﬁx of the output equation. The input token sequence is tokenized at a character\nlevel and encoded as the matrix Wt using a trainable embedding as part of the GPT model.\nThe ﬁrst step in the GPT model is to combine the two inputs wD and Wt together, along with the\npositional embedding matrix Wp. Based on empirical support, we chose to obtain the combined\nembedding by taking the sum Wp + WD + XeqWt, where WD is the dataset representation vector\nwD expanded to ﬁt a matrix matching the dimensions of the other embeddings.\nThe combined vector is then passed through l= 8successive transformer blocks, using the standard\nformat of GPT models [18]. Each transformer block is a sequential combination of a masked multi-\nheaded self-attention layer and a pointwise feed-forward network, with all blocks feeding into a\ncentral residual pathway, similar to ResNets [7].\nAfter llayers of the transformer block, the resulting output vector h is passed through a ﬁnal decoder\nin the form of a linear projection into a vanilla softmax classiﬁer. The projection uses the transposed\ntoken embedding matrixW⊤\nt to map the hidden state vector back into the space of tokens for symbolic\nexpressions. The result of the softmax is a probability vector over tokens in the symbolic equation,\nwhich can be sampled to produce the best equation to describe the input dataset. We use top- k\nsampling with k= 40for our experiments.\nAlthough the symbolic equation used to generate the data can contain constant values, we do not train\nthe GPT model to recover these values exactly. Instead, constant values in the equation are masked\nby ⟨C⟩tokens during the training phase, and the output of the GPT model is a “skeleton equation”\nwhich leaves these constant placeholders in the output string. This is because it is unnecessary to\nburden the language model with the additional task of learning precise constant values, as this can be\neasily handled as a separate step.\n5\n3.4 Learning Constants\nOnce the GPT model predicts a skeleton equation, we learn values of constants to decorate the\nskeleton as a post-processing step. This division of tasks is a common approach for string-based\nregression methods [9, 2].\nTo learning the values of constants in the symbolic equation, we employ BFGS optimization, similar\nto [2], using an implementation from SciPy [21]. The learned constant values then replace the ⟨C⟩\nplaceholder tokens in the skeleton equation, resulting in the ﬁnal symbolic expression to represent\nthe given symbolic regression task.\n3.5 Evaluation Metric\nNormally, regression tasks use the mean square error as a metric for measuring the predictive accuracy\nof an equation. For data following equations with large values, however, this can be problematic, as\nthe residuals can grow very large even when the predicted equation is very close to the true underlying\none. To resolve this issue normalize by ∥y + ϵ∥2 where ϵis used to avoid division by zero and ∥·∥2\nis the Euclidean norm. Then the normalized mean square error, MSEN , is given by\nMSEN (y,ˆy) = 1\nn\nn∑\ni=1\n(yi −ˆyi)2\n∥y + ϵ∥2\n3.6 Strengths and Advantages\nOur method exhibits the following strengths and advantages.\n3.6.1 One-Time Training\nIn contrast with most approaches for symbolic regression, our method does not start training from\nscratch given every new problem instance. All of the model training is performed as a one-time\nprocedure that takes place before the GPT transformer is ever used. Thus, SymbolicGPT enjoys all\nof the beneﬁts of allowing a pretrained model, similar to popular frameworks like BERT [6], which\ncan make use of massive neural networks because the model can be trained ofﬂine in advance.\nAfter the model is trained, every instance of symbolic regression can be solved rapidly as a problem\nin inference. The running time is dependent only on the initial step of reading in the input dataset,\nobtaining the T-Net embedding, and the ﬁnal step of optimizing constant values.\n3.6.2 GPT Technology\nOur approach to symbolic regression is based on a probabilistic language model as implemented by\nGPT. As state-of-the-art language models continue to evolve, our method is expected to organically\nimprove accordingly, with no extra effort in design or implementation, by simply replacing the GPT\narchitecture with any newer and more powerful alternative.\n3.6.3 Scalability\nOur approach addresses two of the main problems with traditional methods. First, our model is\nable to scale to multiple variables. Iterative methods that choose the best candidate equation at each\niteration struggle as the dimension dof the inputs increase since the search space of functions grow\nexponentially with respect to d. By passing in the data points directly as inputs, the model is able to\ninfer the dimension and produce equations accordingly. Second, our model is able to scale in terms\nof the speed in which we generate predictions. Existing methods that train from scratch for each\nregression instance are slow compared to our model. These methods incrementally update their model\nbased on computing many candidate equations. For string-based methods, the constants within these\ncandidate equations would need to be optimized as well. This results in a bottleneck in terms of the\nnumber of constant optimizations that need to be performed to perform inference. SymbolicGPT only\nperforms this constant optimization once which results in signiﬁcantly faster inference times. We\nshow empirically that SymbolicGPT produces superior results using signiﬁcantly less computation\ntime in Section 4.\n6\n4 Experiments and Results\nTo test our model, we implemented SymbolicGPT and trained it in a number of different settings,\nwhich we detail below. In all cases, we trained SymbolicGPT over 4 epochs using a batch size of\n64. The embedding size for the T-net vector representation is e= 512, and the maximum equation\noutput length was capped at 200 tokens.\nTraining and inference for the SymbolicGPT model were performed using an Intel(R) Core(TM)\ni9-9900K CPU @ 3.60GHz with a single NVIDIA GeForce RTX 2080 11 GB GPU and 32.0 GB\nRam. It is noteworthy that our performance scores were achieved using only a single GPU, and\nscaling up is expected to improve training and inference times even further.\nOur experimental framework consists of a large-scale comparison test where we test our model on\n1000 different, randomly generated instances of symbolic regression and evaluate performance based\non MSEN . We repeat this test on four different settings, based on the choice of the dimension d:\ndatasets with one input variable, two variables, three variables, and a random selection between one\nand ﬁve variables. This last setting will be referred to as the “general” experiment.\nIn each experimental setting, SymbolicGPT was trained using 10,000 randomly generated symbolic\nregression instances belonging to the associated dimensional conﬁguration, each consisting of an\ninput dataset and an equation label. A further 1000 dataset-equation pairs were generated and used for\nvalidation, and 1000 new dataset-equation pairs were generated for a test set he training and validation\ndatasets used values of x ∈[−3.0,3.0]d , and test datasets took values of x ∈([−5,−3] ∪[3,5])d.\nThe one, two, and three variables datasets contained 30, 200, and 500 points, respectively. The\nnumber of points in the general dataset withd∈{1,2,..., 5}was a randomly selected integer between\n10 and 200.\nThe parse tree templates, as described in Section 3.1, contained a maximum depth of k= 4levels\nand allowable operators coming from the set\nP = {id(·),add(·,·),mul(·,·),sin(·),pow(·,·),cos(·),sqrt(·),exp(·),div(·,·),sub(·,·),log(·)}\nConstant values selected from the interval [−2.1,2.1] were randomly inserted using a constant ratio\nr= 0.5.\nWe compared our methods with three existing models for nonlinear regression:\n1. Deep Symbolic Regression (DSR): We use the method in [15] to represent the most recent\ndevelopments in deep learning methods for symbolic regression. The DSR algorithm\ncan be very effective for simple equations, but includes a constant optimization step that\nis extremely expensive for larger conﬁgurations, computationally. In order to complete\nexperiments within a reasonable running time, we limited the population size to be 1000\nand trained for a maximum of 10 epochs.\n2. Genetic Programming ( GP): We chose to use Python’s GPLearn package to represent\ngenetic evolution algorithms for symbolic regression. We use two models with different\nconﬁgurations for this experiment. We refer to GP as the model with a population size\nof 1000 and 10 generations. GP Maxis the model with a population size of 5000 and 20\ngenerations.\n3. Neural Network (MLP): We use a standard Multilayer Perceptron to act as a non-symbolic,\nnonlinear regressor to use as a baseline for comparison, as implemented in the Python\npackage Scikit-Learn [14].\nFor each method, we evaluated its performance on 1000 test instances of symbolic regression in each\nof the four experiment settings, using MSEN as the ﬁtness metric. We summarized the results in the\ncumulative distribution plots of Figure 2, showing the proportion of the test cases that attained error\nless than any given threshold value. Methods corresponding to curves positioned higher in the plot\nachieved higher accuracy on more test equations, and hence are better regressors. However, the most\nimportant region of the plot is the far left side, as the number of test cases that achieved the lowest\npossible error is an indication of how often the method would ﬁnd a highly accurate ﬁtting equation.\nSome visualized examples of predictions generated by SymbolicGPT are presented in Figure 4.\nTo give an indication of the speedy performance of SymbolicGPT, we measured and compared the\naverage running time to solve an instance of symbolic regression in the general experiment setting.\n7\n0\n20\n40\n60\n80\n100\nOne Variable Two Variables\nSymbolicGPT\nDSR\nGP\nGP Max\nMLP\n10\n 5\n 0 5 10\n0\n20\n40\n60\n80\n100\nThree Variables\n10\n 5\n 0 5 10\nOne to Five Variables\nLog Normalized MSE\nProportion (%)\nFigure 2: Cumulative log MSEN over all methods and experiments. Each curve shows the proportion\nof test cases that attained an error score less than every given threshold. SymbolicGPT ﬁnds better\nﬁtting equations for more test cases than DSR and ﬁnds more highly accurate equations (with\nlog MSEN <−10) than any other method tested.\nExperiment GP GP Max DSR SymbolicGPT\nGeneral 48.0 ±26.7 84 .8 ±25.8 78 .8 ±42.8 5.0 ±12.0\nOne variable 44.6 ±33.0 82 .1 ±32.1 15 .1 ±2.1 1.1 ±0.9\nTwo variables 47.3 ±29.4 100 .8 ±31.9 76 .7 ±39.8 3.5 ±9.0\nThree variables 60.0 ±32.9 109 .5 ±32.4 73 .3 ±56.4 10.3 ±26.2\nTable 1: Average running times (in seconds) for an instance of symbolic regression during each of\nthe four experiments.\nThe mean running times, along with standard deviations, are shown in Table 1. In order to make a\nfair comparison between running times, all experiments were performed using the same computer\nspeciﬁcations. We excluded the MLP regressor from this experiment in order to compare between\nstrictly symbolic regression methods. The results show that SymbolicGPT requires signiﬁcantly less\ntime to solve an instance of symbolic regression compared with other methods, often by an order of\nmagnitude or more, due to most of the computation being shifted to the ofﬂine step of setting up the\npre-trained model.\nWe also ran each of the four regression algorithms on symbolic regression instances with the varying\nnumber of input datapoints, in order to gauge the data efﬁciency of each of the methods. The results\nof this experiment are shown in Figure 3. As the plot indicates, all methods improve performance (by\nreducing regression error) as more training points are provided; however, SymbolicGPT consistently\nachieves lower error scores than all other methods, regardless of how many data points are available\nin the symbolic regression instances. In particular, SymbolicGPT, when given datasets of just 50\npoints, achieves lower regression error than the other algorithms do on instances with up to 500\npoints. This is in spite of the fact that the SymbolicGPT model was trained only on datasets of 500\npoints each: the robustness to differently sized input dataset instances is possibly a consequence of\nthe order-invariant T-embeddings used in the model.\n8\n0 100 200 300 400 500\nNumber of Points\n5\n4\n3\n2\n1\n0\n1\nMean Log Normalized Square Error\nModel\nGP\nDSR\nMLP\nSymbolicGPT\nFigure 3: The effect of the number of points on the performance of the model.\n1.1\n1.2\n1.3\n1.4\n40\n20\n0\n20\n40\n1.2\n1.3\n1.4\n1.5\n1.6\nTrue\nPredicted\n20000\n15000\n10000\n5000\n0\n2000\n0\n2000\n100\n0\n100\n5\n 0 5\n5\n0\n5\n10\n15\n5\n 0 5\n1.5\n1.0\n0.5\n5\n 0 5\n2.6\n2.4\n2.2\nFigure 4: Graphical representations of selected equations of one input variable. The solid blue curves\nare the graphs of the true underlying equations; the orange dotted curves are the predicted functions\nas generated by SymbolicGPT.\n9\n5 Conclusions\nIn this work, we have presented a method that pushes the boundaries of language models and\napproaches the problem of symbolic regression from a new and powerful direction. We have\nemployed language models in a novel way and with a novel approach, combining them with symbolic\nmathematics and order-invariant representations of point clouds. Our approach eliminates the\nper-instance computation expense of most regression methods, and resolves the input restrictions\nimposed by other language-based regression models. Moreover, our method is fast, scalable, and\nperforms competently on several kinds of symbolic regression problems when compared with existing\napproaches.\nReferences\n[1] D. A. Augusto and H. J. Barbosa. Symbolic regression via genetic programming. In Neural\nNetworks, 2000. Proceedings. Sixth Brazilian Symposium on, pages 173–178. IEEE, 2000.\n[2] L. Biggio, T. Bendinelli, A. Lucchi, and G. Parascandolo. A seq2seq approach to symbolic\nregression. In Learning Meets Combinatorial Algorithms at NeurIPS2020, 2020.\n[3] J. Brence, L. Todorovski, and S. Džeroski. Probabilistic grammars for equation discovery.\nKnowledge-Based Systems, page 107077, 2021.\n[4] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020.\n[5] G. Chen. Learning symbolic expressions via gumbel-max equation learner network. arXiv\npreprint arXiv:2012.06921, 2020.\n[6] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[7] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 770–\n778, 2016.\n[8] S. Kim, P. Y . Lu, S. Mukherjee, M. Gilbert, L. Jing, V .ˇCeperi´c, and M. Soljaˇci´c. Integration\nof neural network-based symbolic regression in deep learning for scientiﬁc discovery. IEEE\nTransactions on Neural Networks and Learning Systems, 2020.\n[9] M. Kommenda, B. Burlacu, G. Kronberger, and M. Affenzeller. Parameter identiﬁcation\nfor symbolic regression using nonlinear least squares. Genetic Programming and Evolvable\nMachines, pages 1–31, 2019.\n[10] G. Lample and F. Charton. Deep learning for symbolic mathematics. International Conference\non Learning Representations, 2020.\n[11] G. S. Martius and C. Lampert. Extrapolation and learning equations. In 5th International\nConference on Learning Representations, ICLR 2017-Workshop Track Proceedings, 2017.\n[12] B. McKay, M. J. Willis, and G. W. Barton. Using a tree structured genetic algorithm to\nperform symbolic regression. In Genetic Algorithms in Engineering Systems: Innovations and\nApplications, 1995. GALESIA. First International Conference on (Conf. Publ. No. 414), pages\n487–492. IET, 1995.\n[13] A. Murari, E. Peluso, M. Gelfusa, I. Lupelli, M. Lungaroni, and P. Gaudio. Symbolic regression\nvia genetic programming for data driven derivation of conﬁnement scaling laws without any\nassumption on their mathematical form. Plasma Physics and Controlled Fusion, 57(1):014008,\n2014.\n10\n[14] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel,\nP. Prettenhofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,\nM. Perrot, and Édouard Duchesnay. Scikit-learn: Machine learning in python. Journal of\nMachine Learning Research, 12(85):2825–2830, 2011. URL http://jmlr.org/papers/\nv12/pedregosa11a.html.\n[15] B. K. Petersen, M. L. Larma, T. N. Mundhenk, C. P. Santiago, S. K. Kim, and J. T. Kim.\nDeep symbolic regression: Recovering mathematical expressions from data via risk-seeking\npolicy gradients. In International Conference on Learning Representations, 2021. URL\nhttps://openreview.net/forum?id=m5Qsh0kBQG.\n[16] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d\nclassiﬁcation and segmentation. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 652–660, 2017.\n[17] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding\nby generative pre-training. preprint, 2018.\n[18] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[19] S. Sahoo, C. Lampert, and G. Martius. Learning equations for extrapolation and control. In\nInternational Conference on Machine Learning, pages 4442–4450. PMLR, 2018.\n[20] M. Schmidt and H. Lipson. Distilling free-form natural laws from experimental data. Science,\n324(5923):81–85, 2009.\n[21] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski,\nP. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett, J. Wilson, K. J. Millman,\nN. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, ˙I. Polat, Y . Feng,\nE. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. A. Quintero,\nC. R. Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, and SciPy 1.0\nContributors. SciPy 1.0: Fundamental Algorithms for Scientiﬁc Computing in Python. Nature\nMethods, 17:261–272, 2020. doi: 10.1038/s41592-019-0686-2.\n[22] Y . Wang, N. Wagner, and J. M. Rondinelli. Symbolic regression in materials science. MRS\nCommunications, 9(3):793–805, 2019.\n11",
  "topic": "Symbolic regression",
  "concepts": [
    {
      "name": "Symbolic regression",
      "score": 0.9137536287307739
    },
    {
      "name": "Computer science",
      "score": 0.6973935961723328
    },
    {
      "name": "Probabilistic logic",
      "score": 0.5975427627563477
    },
    {
      "name": "Transformer",
      "score": 0.5911494493484497
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5566928386688232
    },
    {
      "name": "Machine learning",
      "score": 0.5399560928344727
    },
    {
      "name": "Regression",
      "score": 0.5327335000038147
    },
    {
      "name": "Generative grammar",
      "score": 0.5032042860984802
    },
    {
      "name": "Exploit",
      "score": 0.49510660767555237
    },
    {
      "name": "Generative model",
      "score": 0.4577760398387909
    },
    {
      "name": "Regression analysis",
      "score": 0.4502356946468353
    },
    {
      "name": "Genetic programming",
      "score": 0.3592730760574341
    },
    {
      "name": "Mathematics",
      "score": 0.18552586436271667
    },
    {
      "name": "Statistics",
      "score": 0.13465651869773865
    },
    {
      "name": "Engineering",
      "score": 0.0814875066280365
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I151746483",
      "name": "University of Waterloo",
      "country": "CA"
    }
  ]
}