{
    "title": "Explainable automated debugging via large language model-driven scientific debugging",
    "url": "https://openalex.org/W4405556610",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2110737808",
            "name": "Sung-Min Kang",
            "affiliations": [
                "Korea Advanced Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2096510293",
            "name": "Bei Chen",
            "affiliations": [
                "Microsoft Research Asia (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2122406214",
            "name": "Shin Yoo",
            "affiliations": [
                "Korea Advanced Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A4212328323",
            "name": "Jian-Guang Lou",
            "affiliations": [
                "Microsoft Research Asia (China)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3043635118",
        "https://openalex.org/W2964061924",
        "https://openalex.org/W2138428785",
        "https://openalex.org/W2981731882",
        "https://openalex.org/W2621101275",
        "https://openalex.org/W6600050674",
        "https://openalex.org/W2466388701",
        "https://openalex.org/W2963998044",
        "https://openalex.org/W6776935626",
        "https://openalex.org/W6601899773",
        "https://openalex.org/W3208407575",
        "https://openalex.org/W2998011150",
        "https://openalex.org/W1989678938",
        "https://openalex.org/W3195128168",
        "https://openalex.org/W2867448323",
        "https://openalex.org/W6738317309",
        "https://openalex.org/W4384345778",
        "https://openalex.org/W2964322208",
        "https://openalex.org/W4243127898",
        "https://openalex.org/W2156723666",
        "https://openalex.org/W3151195892",
        "https://openalex.org/W2084429940",
        "https://openalex.org/W2467903332",
        "https://openalex.org/W2039179155",
        "https://openalex.org/W4401906817",
        "https://openalex.org/W2958754741",
        "https://openalex.org/W2154157725",
        "https://openalex.org/W3104012431",
        "https://openalex.org/W2953998535",
        "https://openalex.org/W2151674625",
        "https://openalex.org/W2787871251",
        "https://openalex.org/W2373227884",
        "https://openalex.org/W2944697551",
        "https://openalex.org/W1990785546",
        "https://openalex.org/W4284674057",
        "https://openalex.org/W6851092083",
        "https://openalex.org/W2148854374",
        "https://openalex.org/W2808429234",
        "https://openalex.org/W6635750829",
        "https://openalex.org/W2015265939",
        "https://openalex.org/W2065224323",
        "https://openalex.org/W3105943882",
        "https://openalex.org/W4308643065",
        "https://openalex.org/W4308643319",
        "https://openalex.org/W4402457546",
        "https://openalex.org/W6630224890",
        "https://openalex.org/W2518136680",
        "https://openalex.org/W2952920225",
        "https://openalex.org/W6600696048",
        "https://openalex.org/W3098154879",
        "https://openalex.org/W1574060188",
        "https://openalex.org/W4297666478"
    ],
    "abstract": "Abstract Automated debugging techniques have the potential to reduce developer effort in debugging. However, while developers want rationales for the provided automatic debugging results, existing techniques are ill-suited to provide them, as their deduction process differs significantly froof human developers. Inspired by the way developers interact with code when debugging, we propose Automated Scientific Debugging ( AutoSD ), a technique that prompts large language models to automatically generate hypotheses, uses debuggers to interact with buggy code, and thus automatically reach conclusions prior to patch generation. In doing so, we aim to produce explanations of how a specific patch has been generated, with the hope that these explanations will lead to enhanced developer decision-making. Our empirical analysis on three program repair benchmarks shows that AutoSD performs competitively with other program repair baselines, and that it can indicate when it is confident in its results. Furthermore, we perform a human study with 20 participants to evaluate AutoSD -generated explanations. Participants with access to explanations judged patch correctness more accurately in five out of six real-world bugs studied. Furthermore, 70% of participants answered that they wanted explanations when using repair tools, and 55% answered that they were satisfied with the Scientific Debugging presentation.",
    "full_text": "Empirical Software Engineering (2025) 30:45\nhttps://doi.org/10.1007/s10664-024-10594-x\nExplainable automated debugging via large language\nmodel-driven scientiﬁc debugging\nSungmin Kang 1 · Bei Chen 2 · Shin Yoo 1 · Jian-Guang Lou 2\nAccepted: 12 November 2024 / Published online: 18 December 2024\n© The Author(s) 2024\nAbstract\nAutomated debugging techniques have the potential to reduce developer effort in debugging.\nHowever, while developers want rationales for the provided automatic debugging results,\nexisting techniques are ill-suited to provide them, as their deduction process differs sig-\nniﬁcantly froof human developers. Inspired by the way developers interact with code when\ndebugging, we propose Automated Scientiﬁc Debugging ( AutoSD), a technique that prompts\nlarge language models to automatically generate hypotheses, uses debuggers to interact with\nbuggy code, and thus automatically reach conclusions prior to patch generation. In doing so,\nwe aim to produce explanations of how a speciﬁc patch has been generated, with the hope\nthat these explanations will lead to enhanced developer decision-making. Our empirical anal-\nysis on three program repair benchmarks shows that AutoSDperforms competitively with\nother program repair baselines, and that it can indicate when it is conﬁdent in its results.\nFurthermore, we perform a human study with 20 participants to evaluate AutoSD-generated\nexplanations. Participants with access to explanations judged patch correctness more accu-\nrately in ﬁve out of six real-world bugs studied. Furthermore, 70% of participants answered\nthat they wanted explanations when using repair tools, and 55% answered that they were\nsatisﬁed with the Scientiﬁc Debugging presentation.\nCommunicated by: Andrea Stocco, Matteo Biagiola, Vincenzo Riccio, Foutse Khomh, Nicolás Cardozo,\nDongwan Shin\nSungmin Kang participated in this work during an internship at Microsoft Research Asia.\nThis article belongs to the Topical Collection: Special Issue on Innovations in Software System Testing with\nDeep Learning\nB Shin Y oo\nshin.yoo@kaist.ac.kr\nSungmin Kang\nsungmin.kang@kaist.ac.kr\nBei Chen\nbeichen1019@126.com\nJian-Guang Lou\njlou@microsoft.com\n1 KAIST, Daejeon, South Korea\n2 Microsoft Research Asia, Beijing, China\n123\n45 Page 2 of 28 Empirical Software Engineering (2025) 30 :45\nKeywords Automated Program Repair · Machine Learning\n1 Introduction\nAutomated debugging techniques, such as Fault Localization (FL) or Automated Program\nRepair (APR), aim to help developers by automating the debugging process in part. Due\nto the signiﬁcant amount of developer effort that goes into debugging (Zeller 2009), auto-\nmated debugging is a research topic of signiﬁcant interest (Liu et al. 2020): many papers are\npublished every year (Monperrus 2020), and the ﬁeld is mature enough to see adoption by\nindustry (Kirbas et al. 2021; Marginean et al. 2019).\nRegarding the practical adoption of these techniques, a body of literature surveying devel-\noper expectations on automated debugging has consistently highlighted that, as much as\nstrong performance on software engineering tasks is important, so is supporting information\nthat helps developers judge the results. For example, Kochhar et al. ( 2016) perform a study\nof developer expectations on fault localization, and ﬁnd that more than 85% of developers\nagree that the ability to provide rationale is important. Further, Kirbas et al. ( 2021) note that\nsome developers responded negatively to automated program repair results, citing that they\nwould come “out of the blue”. Such ﬁndings suggest that strong automated debugging results\nmay not be acceptable on their own, and may need supporting information that helps explain\nthe results.\nDespite the consistent request for explainable processes for automated results, to the best\nof our knowledge explainable automated debugging techniques can be difﬁcult to come\nby. For example, in the living review of APR compiled by Monperrus updated in August\n2022 (Monperrus 2020), the word ‘explain’ appears only in one position paper (Monperrus\n2019), revealing that the critical research on how to explain repair suggestions to develop-\ners is under-explored. We argue that this is in part because existing automated debugging\ntechniques reason in starkly different ways to humans. Whereas existing automated debug-\nging techniques will reduce a search space (Jiang et al. 2018) and try multiple solutions to\nﬁnd results that are correlated with the location and ﬁx of a bug (Moon et al. 2014), human\ndevelopers will generally utilize debuggers and print statements to interact with the buggy\ncode, understand its behavior and in turn make a patch based on such observations (Siegmund\net al. 2014). That is, the reasoning traces (Lim et al. 2009) of existing automated debugging\nprocesses are so different from those of developers, that suggesting them may contribute little\nto the understanding of a generated patch.\nAs a step towards automated debugging techniques that can generate explanations that\nhelp developers, we propose AutoSD, which represents a novel pipeline to combine bug-\nrelated information into generate legible explanations for bugs along with patches. To do\nso, AutoSDleverages Large Language Models (LLMs) and a debugger interface to auto-\nmatically emulate the Scientiﬁc Debugging (SD) process for developers proposed by Zeller\n(2009), which suggests to formulate and verify hypotheses about the bug in the process of\ndebugging. In line with this, AutoSDprompts an LLM to automatically generate hypotheses\nabout what is causing the bug, along with a debugger script that would test the hypotheses.\nAutoSDthen executes the suggested debugger command and provides the LLM with the\nresult; based on this, the LLM ﬁnally decides whether the hypothesis was met, and predicts\nif the debugging process is done, or additional investigation is required. The intermediate\ndebugging text generated as a result can naturally be presented as an explanation describing\nhow AutoSDreached its conclusion. Emulating Scientiﬁc Debugging has ideal properties for\n123\nEmpirical Software Engineering (2025) 30 :45 Page 3 of 28 45\nexplainable debugging: notably, as existing work identiﬁes that developers use the principles\nof Scientiﬁc Debugging to debug even without formal training (Siegmund et al. 2014), the\nexplanations could help inform or augment the thought process of developers.\nWe empirically evaluate AutoSDby ﬁrst evaluating it on three program repair bench-\nmarks. Our results indicate AutoSDcan achieve competitive repair results to non-explainable\nAPR techniques. In terms of practical usage, precision is an important factor (Xiong et al.\n2017); we ﬁnd that for cases when AutoSDindicates it had collected enough information for\ndebugging, repair performance is in fact higher, potentially reducing developer inspection\neffort. As language models become more capable, the repair performance of AutoSDrapidly\nincreases as well, demonstrating the potential of AutoSD. We further perform a user study\non Python developers involving 20 participants, including six professional developers, under\na realistic APR application setting: reviewing patches for acceptance. Our results demon-\nstrate that the debugging traces generated by AutoSDenhance developer accuracy in terms\nof assessing whether the patch is correct for 83% of the real-world bugs studied, while the\namount of time in which developers judged the patch was roughly constant; these results sug-\ngest that humans beneﬁt from the automatically generated patch explanations of AutoSD.\nFurthermore, 70% of participants responded that they would see explanations as an impor-\ntant factor when using APR tools, and 55% were satisﬁed with the Scientiﬁc Debugging\nformulation of AutoSD.\nOur demonstration that LLMs can emulate loosely-structured workﬂows such as Sci-\nentiﬁc Debugging, which were originally intended for humans, and consequently improve\nexplainability, has a greater implication for many software engineering processes as well.\nOur work can also be seen as an combining recent advances in LLM research, in which\nLLMs are combined with traditional tool use, with existing software engineering processes -\nAutoSDcombines tools that developers often use (symbolic debuggers), which subsequently\nallows LLMs to debug in a human-like manner. This opens the possibility that other software\nprocesses designed for humans in mind, such as test scripts for manual testers (Haas et al.\n2021) or code review processes (Rigby and Bird 2013), may also be performed in an explain-\nable manner, improving the usability of automated software engineering tools in multiple\nareas.\nOverall, our contribution may be summarized as:\n– We identify that explainable automated debugging may be achieved by LLMs emulating\ndeveloper processes, and as a demonstration propose AutoSD, which uses LLMs to\nemulate Scientiﬁc Debugging (Zeller 2009);\n– We perform a comprehensive set of empirical analyses on three APR benchmarks, demon-\nstrating that AutoSDcan achieve signiﬁcant APR performance while also generating\nexplanations and indicating conﬁdence in its results as a natural byproduct of its patch-\ngeneration process;\n– We conduct a developer study on AutoSD, based on a realistic scenario of patch review,\nand demonstrate explanations from AutoSDcan aid developers in decision-making;\n– We further solicit feedback from users regarding repair explanations, presenting a guide-\nline for future improvement of AutoSDexplanations.\nThe remainder of the paper is organized as follows. We introduce the technical background\nto our work in Section 2, and our technique AutoSDin Section 3. The evaluation setup\nand research questions are provided in Section 4, and the empirical results based on these\nexperiments are presented in Section 5. Threats and limitations are discussed in Sections 6,\nand 7 concludes.\n123\n45 Page 4 of 28 Empirical Software Engineering (2025) 30 :45\n2 Background\nThis section provides the motivation and background for our work.\n2.1 Explainable Automated Debugging\nAutomated debugging has a long history, with research often being done on the topics of\nfault localization (Moon et al. 2014;J o n e se ta l .2002;L ie ta l . 2019) and automated program\nrepair (Gazzola et al. 2019). As described before, while the technical complexity and perfor-\nmance of automated debugging techniques has been increasing (Jiang et al. 2023b), including\nthe use of LLMs for APR (Jiang et al. 2023a; Xia et al. 2022), empirical work on explaining\nresults for developer consumption has been difﬁcult to identify. In addition to Monperrus’\nliving review on APR having only one paper mentioning explanations (Monperrus 2020),\nWinter et al. ( 2022) ﬁnd 17 human studies evaluating APR, of which none involved explana-\ntions directly from an APR tool; Kochhar et al. ( 2016) survey fault localization techniques\nat the time, and ﬁnd two techniques that could provide explanations of their results (Sun and\nKhoo 2013; Mariani et al. 2011); unfortunately, both papers did not have human studies.\nThis contrasts to the growing body of literature showing that, to adopt automated debug-\nging techniques in practice, ‘explanations’ for the results would be welcome. Developers have\nstated their desire for explanations in multiple occasions: along with the ﬁndings of Kochhar\net al. ( 2016) mentioned earlier, a developer study on expectations for APR by Noller et al.\n(2022) notes that “the most commonly mentioned helpful output from an APR tool is an\nexplanation ... including its root cause ”. Developer expectation is particularly important\nbecause when automated debugging has been adopted by industry, automatically generated\npatches are consistently reviewed by developers. At Meta, the APR system is connected to\nthe internal code review platform (Marginean et al. 2019); at Bloomberg, Kirbas et al. ( 2021)\nwrite that “Bloomberg’s view was that full automation was far from ideal”, and they subject\nAPR patches to be reviewed by a software engineer. This is also reﬂected in Noller et al.’s\nresults that “full developer trust requires a manual patch review”.\nA promising way to present developers with explanations could be to show the reasoning\ntrace (Lim et al. 2009) of a tool, i.e. how an automated debugging tool came to recommend a\ncertain line for FL or a certain patch for APR. Unlike post-hoc explanation techniques such as\ncommit message generation (Jiang et al. 2017), reasoning traces can answer critical questions\nthat a developer may have, such as ‘why this patch?’; indeed, research in Human-Computer\nInteractions (HCI) have indicated that explanations should strive to be capable of answering\nwhy an approach gave a certain result (Lim et al. 2009).\nWe suggest that ideally, such explanations and reasoning traces would provide two critical\nfactors: (i) new information about the situation, so that developers can learn something real\nabout the situation by reading the explanation, and (ii) a new perspective about the situation,\nso that developers can interpret the new information that is presented by the explanation.\nIn fact, prior research on developer debugging practice supports the need of each of these\ncomponents. As Böhme et al. ( 2017) note, developers spend a signiﬁcant amount of time\ngathering information about the bug, such as internal program values. Meanwhile, it is also\napparent from the literature that developers are not consumers of raw data; instead, they will\nformulate higher-level ‘hypotheses’ on why the bug is happening to interpret the data, as\nevidenced by a multitude of prior work (Siegmund et al. 2014; Alaboudi and LaToza 2020;\nLayman et al. 2013). As such, providing both information and perspective is important in\ngenerating an explanation seeks to be helpful for developers.\n123\nEmpirical Software Engineering (2025) 30 :45 Page 5 of 28 45\nHowever, current automated debugging techniques are ill-suited to generate helpful expla-\nnations for their results under these criteria, as they fail to provide at least one of these factors.\nUsing a common classiﬁcation of APR techniques (Goues et al. 2019)a sa ne x a m p l e\n1,\ngenerate-and-validate (G&V) techniques (Gazzola et al. 2019) (which includes learning-\nbased techniques (Jiang et al. 2018; Xia and Zhang 2022; Zhu et al. 2021)) will generate\nvariants of the buggy code until a test passes. As their deduction process is simply enumerat-\ning changes and trying them one by one, the process runs without regard to any ‘hypothesis’.\nSemantics-based APR techniques such as Mechtaev et al. ( 2016) use variable values as\ninputs to Satisﬁability Modulo Theory (SMT) solvers to more effectively search within a\npatch space; thus they are not inherently identifying any ‘hypothesis’ either. This is not to\nsay these techniques are ineffective at ﬁxing bugs - numerous work on APR shows that\nexisting APR techniques can ﬁx a wide array of bugs. Rather, we argue that because their\nreasoning trace is so different from humans, it is difﬁcult to make a satisfactory explanation\nof their results. On the other hand, one way to make satisfactory explanations would be to\ndevelop an automated debugging technique that deduces in a similar way to humans, to make\nthe decision-making process transparent (Dam et al. 2018). While technically similar, the\ngeneration of explanations is what distinguishes our technique from ChatRepair (Xia and\nZhang 2023): while we allow LLMs to interact with a debugger in a formatted way that\nimitates human debugging, in the hopes of actually generating explanations helpful to devel-\nopers, ChatRepair provides test execution result feedback to the LLM, and is thus closer to\nan extension of the generate-and-validate concept (Martinez and Monperrus 2019)i nA P R .\n2.2 Scientific Debugging\nTo align APR reasoning traces more closely to those of human developers, we must know\nhow developers debug in practice. Previous work on developer debugging patterns provide\nglimpses into how debugging is actually done.\nEarly work on developer debugging found that there was a “gross descriptive model” that\ndevelopers followed, in which developers formulated hypotheses, then veriﬁed whether the\nhypotheses are true (Gould 1975). A formal version of this process was named Scientiﬁc\nDebugging by Zeller ( 2009), who advocated for developers to maintain a debugging log\nconsisting of an iteration of the following items:\n– Hypothesis: a tentative description that explains the bug and is consistent with the known\nobservations;\n– Prediction: an expected outcome if the hypothesis is true;\n– Experiment: a means of verifying the prediction;\n– Observation: the result of an experiment;\n– Conclusion: a judgement of the hypothesis, based on the observation.\nSiegmund et al. ( 2014) found that even without formal training in debugging techniques,\nall developers surveyed would roughly follow the ‘hypothesis formulation, then veriﬁca-\ntion’ process of scientiﬁc debugging, where the developer will formulate a hypothesis about\nwhat the bug is, then observe actual execution results via debuggers or logging to verify the\nhypothesis. Thus, Scientiﬁc Debugging can be seen as a formal way of describing the domi-\nnant developer thought process when ﬁxing bugs, and thus we seek to emulate this process\nto make an explanation when generating APR results.\n1 The explainability of FL is discussed in the Appendix.\n123\n45 Page 6 of 28 Empirical Software Engineering (2025) 30 :45\n2.3 Large Language Models\nIn this paper, we seek to emulate the Scientiﬁc Debugging process via Large Language\nModels (LLMs). We believe LLMs are capable of emulating Scientiﬁc Debugging for the\nfollowing reasons. First, they have shown increasingly strong performance on question-\nanswering benchmarks that involve reasoning (Brown et al. 2020; OpenAI 2023), which also\nmakes it possible that they would be capable of predicting whether a hypothesis is met, and\nwhich hypothesis to investigate next. While it would be difﬁcult to manually gather a large\namount of data that contains debugging traces in the Scientiﬁc Debugging format, LLMs\nhave also been demonstrated to be capable of few-shot or zero-shot problem solving: that is,\ngiven a few examples or simply a description of the task to be solved in the form of a natural-\nlanguage prompt, they are capable of doing the task (Brown et al. 2020). This capability\nimproves with Reinforcement Learning with Human Feedback (RLHF) training (Ouyang\net al. 2022), which the main LLM of our task (ChatGPT of OpenAI) was trained on. Finally,\nthe interaction with code that Scientiﬁc Debugging asks for requires the use of external tools.\nWhen using ‘Chain-of-Thought’ (CoT) prompting (Wei et al. 2022), LLMs appear capable of\nusing the results of external tools to improve their performance as well (Yao et al. 2022;G a o\net al. 2022). As a result, we believe that LLMs are well-positioned to emulate the Scientiﬁc\nDebugging process, and thus generate reasoning traces complete with actual execution results\nthat would provide an intelligible guide to developers as to how the patch occurred.\n3 Automated Scientiﬁc Debugging\nThe overall process of our approach is presented in Fig. 1. To start, the prompt contain-\ning relevant information is generated (Fig. 1 A ): this consists of a detailed explanation\nof what Scientiﬁc Debugging is, and a description of the debugging problem itself, so\nthat AutoSDcan proceed with the following steps. With the initial prompt prepared,\nAutoSDgenerates a hypothesis on what is wrong with the code or how it can be ﬁxed,\nalong with the concrete experiment that would validate such a hypothesis, using an LLM\n(Fig. 1\nB ). The experiment script will be passed to a background debugger/code executor\nprocess, which runs the script and returns the actual result (Fig. 1 C ). Based on the observed\ninformation, AutoSDdecides whether the hypothesis was veriﬁed or not using an LLM (Fig.\n1 D ); depending on the conclusion, AutoSDeither starts with a new hypothesis or opts\nto terminate the debugging process and generate a ﬁx. When the interaction with the code\nis over, AutoSDgenerates a bug ﬁx based on the gathered information (Fig. 1\nE ). Unlike\nother automated program repair techniques we are aware of, as a result of steps ( B - D )\nAutoSDcan provide a rationale of how a particular ﬁx was generated, which can then be\nprovided to the developer upon request.\n3.1 Constructing the Input Prompt\nTo construct the initial prompt, as in the example presented in Fig. 1 A ,w eﬁ r s tm a n u a l l y\nwrote a detailed description of Scientiﬁc Debugging that explains what hypotheses, predic-\ntions, experiments, observations, and conclusions are, along with multiple examples for each\ncategory, so that the LLM can generate an intelligible reasoning trace. The full description\ncan be found in the Appendix; here, we describe the aspects of the description critical for the\npipeline of AutoSDin detail. For one, concrete examples of experiments are provided, to\nallow the LLM to predict appropriate experiment scripts: composite debugger commands\n123\nEmpirical Software Engineering (2025) 30 :45 Page 7 of 28 45\nFig. 1 The pipeline and a real example run of AutoSD, with annotations in black boxes and lightly edited\nfor clarity. Given a detailed description of the scientiﬁc debugging concept and a description of the bug (A),\nAutoSDwill generate a hypothesis about what the bug is and construct an experiment to verify, using an LLM\n(B), actually run the experiment using a debugger or code execution (C), and decide whether the hypothesis is\ncorrect based on the experiment result using an LLM (D). The hypothesize-observe-conclude loop is repeated\nuntil the LLM concludes the debugging or an iteration limit is reached; ﬁnally, a ﬁx is generated (E), with an\nexplanation (white boxes from (1) to (9)) that the developer may view. The experiments generated in (1) and\n(4) are valid Python debugger commands, with b signifying the setting of a breakpoint, c signifying running\nthe code until a breakpoint, and p being a command to print the value of an expression. The experiment in (7)\nis a valid code-editing DSL command, as described in the main text\n(consisting of setting a breakpoint, running code, and printing a value) and a Domain-\nSpeciﬁc Language (DSL) that we deﬁne to allow edit-and-execute commands are given. The\nBackus-Naur form deﬁnition of the DSL is provided in Fig. 2. The prompt explains the DSL,\nspeciﬁcally that the following commands are available: REPLACE(line, old_expr,\nnew_expr) that changes an expression at line, ADD(line, new_expr) that adds\na new statement above line,a n d DEL(line, old_expr) that allows deletion of any\nexpression within a line, including the entire line. Multiple commands can be joined with\nthe AND connector, and ﬁnally the bug-revealing test can be executed after modiﬁcation via\nthe RUN command. In addition to experiment commands, the prompt instructs to predict the\n<DEBUGGING DONE> token (<DONE> for short in the rest of the paper) if enough informa-\ntion to discern the patch has been gathered, so that we can gauge how conﬁdent AutoSDis\nin its patch. The prompt is detailed enough so that our default LLM, ChatGPT, can fol-\nlow the instructions zero-shot, i.e., without a concrete demonstration of the full process. On\nFig. 2 Deﬁnition of the code-editing DSL used in our experiments\n123\n45 Page 8 of 28 Empirical Software Engineering (2025) 30 :45\nthis description of scientiﬁc debugging, we add the bug-speciﬁc information: concretely, the\nbuggy function/method, the test that reveals the bug, the error message when the bug is exe-\ncuted, and if available a bug report. We add this information as we believe such information\nwould be necessary, if not sufﬁcient, for a human to debug an issue, and thus would likely\nalso help an automated technique to predict appropriate hypotheses and ultimately succeed\nin debugging.\n3.2 Hypothesize-Observe-Conclude\nWith the initial prompt, AutoSDstarts iterating over the ‘hypothesize-observe-conclude’\nloop depicted in Fig. 1 ( B - D ). The result of each process is appended to the prompt to\nallow incremental hypothesis prediction; i.e. when generating the conclusion in 3 , the LLM\nwould predict it based on the concatenation of the initial prompt, 1 ,a n d 2 .W ed e s c r i b e\neach iteration of the loop as a step: for example, Figure 1 1 - 3 would make up one step.\nUnlike previous automated debugging techniques that will make statistical ‘guesses’ on what\npatches are likely through search space reduction or neural networks, by going through the\nhypothesize-observe-conclude loop, AutoSDcan demonstrate to developers what observa-\ntions led to speciﬁcally this patch. In turn, this allows developers to scrutinize the generation\nprocess, which can ease developer veriﬁcation efforts and lead to greater developer trust in\nthe results.\nHypothesize. Here, we lead the language model to generate a hypothesis by appending\nthe token Hypothesis: to the prompt, so that the language model generates a hypothesis\nabout the bug. We observe that the Prediction: and Experiment: line headers are\nalso generated in turn by the LLM, due to the detailed description of the scientiﬁc debugging\nprocess provided by the prompt. The important aspect for the next step is the Experiment\ncommand, where the language model either generates a debugger command that can be\nexecuted by a debugger, or a custom code modiﬁcation-and-execution script so that the\nlanguage model can ‘test’ a certain change. As the document is in Markdown format, the\nExperiment script is wrapped in backticks ( `); this script is extracted from the LLM\noutput to get concrete code execution results in the next step. Examples can be seen in Fig.\n1\n1 , 4 ,a n d 7 - note that AutoSDalso localizes the fault as a part of the hypothesizing\nprocess, thus making fault localization explainable as well.\nObserve. The generated experiment script is passed to a background process based on\ntraditional software engineering tools that provides real execution results back to the language\nmodel, so that we can ground the generation process of AutoSDon real results, and also\nbuild credibility for developer presentation. The model can either (i) invoke a composite\ndebugger command by setting a breakpoint and printing a value, or (ii) modify the code and\nrun the failing test with the aforementioned DSL. When executing a debugger command,\nit is executed via the command-line interface of the language-appropriate debugger, and\nthe output from the last subcommand of the composite command (assumed to be a print\ncommand) is returned, as in Fig. 1\n2 and 5 . When the breakpoint is within a loop, the\ndebugger collects values at different timesteps of execution and returns them together, e.g.\n‘At each loop execution, the expression was: [v1, v2, ...]’, up to a maximum of 100 values.\nMeanwhile, upon test execution from a edit-and-execute DSL command, if an exception is\nraised, the exception type and message are returned as the observation; otherwise, the result\n‘[No exception triggered]’ is appended, as in Fig. 1\n8 . As described in earlier sections and in\nour results, this step anchors the explanations in actual execution results, which existing patch\nexplanation techniques such as commit message generation (or the baseline of asking an LLM\nfor a patch explanation) are categorically incapable of. As we demonstrate in Section 5.5,t h i s\n123\nEmpirical Software Engineering (2025) 30 :45 Page 9 of 28 45\nincorporation improved developer trust in the explanations (80% of developers responded\npositively that the incorporation of execution results enhanced their trust in the explanation)\nwhile preventing the LLM from hallucinating explanations that may mislead developers.\nConclude. Based on the observation, AutoSDinvokes the LLM to check whether the\nhypothesis and the observation are consistent, by having the LLM predict if the hypothesis\nis rejected (e.g.\n3 ), supported (e.g. 6 ), or undecided due to an unexpected observation.\nWe have the LLM generate the conclusion to maximize ﬂexibility in value interpretation,\nas the LLM will generate complex hypotheses or predictions at times that are difﬁcult to\nautomatically resolve based on debugger or test execution output. As described earlier, the\nLLM may predict a separate <DONE> token at this step if it predicts the debugging process is\ncomplete; in such cases, AutoSDwould have greater conﬁdence in its output. An example is\ns h o w ni nF i g .1\n9 : on the information that the previously failing test now passes, the LLM\nconcludes that debugging is done. If the <DONE> token is predicted, AutoSDproceeds to\ngenerate a ﬁx as in Section 3.3; otherwise the loop restarts with hypothesizing based on the\nnewly available information until a maximum iteration limit s is reached. If <DONE> is not\npredicted until then, AutoSDis failing to identify the cause of the bug, and we may be more\nskeptical of the generated patch.\n3.3 Fix Suggestion\nWhen AutoSDhas completed its interaction with the code, either by predicting <DONE> or\nby reaching the maximum iteration limit s, the conclusions to each of the hypotheses are\nassessed, and rejected hypotheses are automatically removed from the prompt prior to patch\ngeneration, as this empirically improved program repair performance in our experiments.\nEven if rejected hypotheses are not involved when making the ﬁx itself, rejected hypotheses\ncan still be presented to the developer as context for successful hypotheses. We subsequently\nprompt the LLM to generate a ﬁx using the available information by appending the words\n“The repaired code (full method, without comments) is:\\n ```” .\nThis prompt leads the LLM to generate repaired code, based on the information available\nfrom the problem description and the code interaction, as in Fig. 1\n10. We ask the LLM to\ngenerate code without comments to ease the parsing of the generated results and to help it\nfocus on generating the ﬁx itself. Identically to other APR techniques, a patch is ultimately\ngenerated; what makes AutoSDunique is that it can show its intermediate reasoning steps\n(\n1 - 9 )a sa n explanation that can help the developer understand where a patch comes\nfrom.\n4 Evaluation Setup\nHere we describe the setup for our empirical evaluation.\n4.1 Research Questions\nRQ1: How well does A UTO SDperform repair? While the main focus of our work is\nto generate a reasoning chain for automated debugging results, good performance in the\ndebugging task itself is also important (Kochhar et al. 2016; Noller et al. 2022). We thus seek\nto answer whether AutoSDachieves performance competitive to prior APR techniques, and\nwhen compared to prompting the same underlying LLM as AutoSDto immediately predict\n123\n45 Page 10 of 28 Empirical Software Engineering (2025) 30 :45\na ﬁx (this baseline is referred to as LLM- Base in the rest of the paper). To clarify, LLM-\nBase has the same initial input as AutoSD, but predicts ﬁxes without interacting with\nthe code. We aim to demonstrate that the explainability of AutoSDdoes not come with a\nsigniﬁcant performance cost, even as prior reviews on explainable AI describe a tradeoff\nbetween interpretability and performance (Arrieta et al. 2020). We evaluate AutoSDon the\nAlmost-Right HumanEval benchmark we construct to mitigate data leakage concerns, and\nthe Defects4J v1.2 and 2.0 benchmarks (Just et al. 2014) consisting of real-world bugs.\nRQ2: How does the debugger inﬂuence the behavior of A\nUTO SD? Given that hypoth-\nesis veriﬁcation is a critical aspect of AutoSD, we evaluate whether the performance of\nAutoSDis better when it indicates that debugging is done via the <DONE> token, which\nindicates the external observations match the generated hypotheses enough for the LLM to\nbe conﬁdent that debugging is over. If AutoSDcan indicate when it is likely to be correct, this\ncould help developers make decisions about how to think about the automatically generated\nﬁx, as developer inspection time spent on reviewing patches could be reduced by omitting\npatches that AutoSDis not conﬁdent in. As such a property would likely aid developer adop-\ntion of AutoSD,w ee v a l u a t et ow h a te x t e n t<DONE> predicts better performance. Based\non our these experiments, we further evaluate the performance of AutoSDwhen debuggers\nare not used, and observations are ‘hallucinated’ by the LLM instead of obtained via actual\ncode execution. We evaluate whether under this setting, the <DONE> token continues to be\na marker of strong performance.\nRQ3: How does the choice of LLM inﬂuence the performance of A\nUTO SD? We\nevaluate the performance of AutoSDas we vary the LLM that is used. While we empirically\nfound the best performance when using the ChatGPT model, and thus used it as the default\nsetting throughout the rest of the paper, by varying the size of the language model and plotting\nthe performance, we investigate automated repair performance as models improve in terms\nof parameter size and training sophistication.\nRQ4: How do developers beneﬁt from A\nUTO SDexplanations? Via our human study,\nwe evaluate whether developers beneﬁt materially from automatically generated explana-\ntions by AutoSD, i.e. regardless of their opinion towards explanations. In our human study,\nparticipants are given the buggy code, a bug-revealing test, a candidate patch, and half of\nthe time an explanation, and asked to determine whether the patch correctly addresses the\nissue that the test reveals. We measure the time and accuracy of developers when deciding\nwhether a patch is correct, along with developer answers to the question ‘did the explanation\nhelp you make the decision?’. We thus hope to evaluate whether developers beneﬁt by being\nprovided explanations.\nRQ5: How do developers feel towards A\nUTO SDexplanations? We evaluate whether\nthe explanations of AutoSDare acceptable to developers by asking them six questions on\nwhether they would want to use APR, whether they would want explanations when using\nAPR, and whether AutoSDand each element of its explanation were satisfactory. Unlike\nRQ4, which evaluates the material beneﬁt developers derive from each explanation, this RQ\nfocuses on developer opinion. We thus hope to measure whether developers are willing to\nuse explanations, distinctly from whether their productivity increases from explanations. We\nadditionally perform interviews to identify what developers liked about the explanations of\nAutoSD, and what could improve.\nRQ6: What do A\nUTO SDexplanations look like? We provide examples of liked and\ndisliked patch attempts and their corresponding explanations in this research question as\nfurther context, along with a breakdown of common failure causes by analyzing a random\nsample of 25 cases in which all hypotheses generated by AutoSDwere classiﬁed as incorrect\nby itself.\n123\nEmpirical Software Engineering (2025) 30 :45 Page 11 of 28 45\n4.2 Environment\n4.2.1 Evaluating Explainable APR Performance\nTo empirically evaluate AutoSD, we use four program repair benchmarks. First, the widely-\nused Defects4J benchmarks (Just et al. 2014) version 1.2 and 2.0, which have been used by\nprior work as a standard benchmark to compare APR techniques (Liu et al. 2020), are used\nto evaluate program repair performance, namely how many bugs are ﬁxed by the technique.\nMeanwhile, we use the BugsInPy benchmark (Widyasari et al. 2020) (abbreviated to BIP in\nour paper) for the sake of getting real-world Python bugs to evaluate in our human study,\nbut we do not report the program repair performance of AutoSDon BIP as many of its bugs\nneeded additional environment setup not described in the README which makes setting it\nup so that tests and debuggers execute correctly is difﬁcult, whereas execution is critical to\nthe operation of AutoSD.\nWe additionally construct the Almost-Right HumanEval (ARHE) dataset based on the\nHumanEval Python single-function synthesis benchmark by Chen et al. ( 2021), and use it\nfor both our human study and performance evaluation. We construct it in the hope that it will\nbe free from data contamination concerns, as HumanEval was explicitly made by Chen et\nal. to avoid data contamination when evaluating their LLM, and was also used to evaluate\nthe recent GPT-4 model (OpenAI 2023); constructing such a dataset is particularly important\ngiven that Lee et al. ( 2024) ﬁnd that many bugs from Defects4J are included in the training\ndata of the open-source LLM StarCoder. The ARHE dataset was built by mutating the human\nsolutions in the HumanEval benchmark so that exactly one test fails, making bugs that cause\nthe code to be ‘almost’ right. To do this, we ﬁrst generate all possible mutants using each\nmutation operator, then select the mutants that result in exactly one test failure, ﬁltering out\nmutants that cause more tests to fail. We do this to construct a repair dataset - whereas ﬁxing\na mutant that causes many tests to fail is perhaps akin to ﬁxing the entire functionality of the\ncode, ﬁxing a mutant that causes one test to fail is closer to what we would consider to be\nrepair as one must preserve the existing correct functionality while rectifying the erroneous\nbehavior as indicated by the failing test. Through this procedure, we end up with 200 bugs to\nevaluate with using seven mutators, as is presented in Table 1, and compare them to mutators\nin PIT (Coles et al. 2016), a widely used mutation testing tool. ‘Integer Literal Changer’ will\nchange literal 0 constants to 1 constants, and vice versa, which shows similar behavior to the\n‘Inline Constant Mutator’ of PIT. ‘If Remover’ will remove the then-block or else-block of an\nif statement; if it has no remaining children, the if statement itself will be removed, similarly\nto ‘Remove Conditionals Mutator’ of PIT. ‘String Literal Changer’ will make a string literal\nTable 1 ARHE benchmark\nbreakdown Mutator Number\nInteger Literal Changer ◦ 45\nIf Remover □ 24\nString Literal Changer /Delta1 63\nOperator Changer ◦ 40\nBinary Operator Remover □ 24\nAugmented Assignment Changer ◦ 3\nIf Negator ◦ 1\nReversible mutators are marked with ◦, irreversible mutators are marked\nwith □ , and occasionally reversible mutators are marked with /Delta1\n123\n45 Page 12 of 28 Empirical Software Engineering (2025) 30 :45\nempty, lower-case, or upper-case; making the string literal an empty string is not reversible,\nbut whether the lower-casing or upper-casing can be applied in the reverse to get the original\ncode differs from problem to problem. The generation of empty strings is similar to the\n‘Empty returns Mutator’ of PIT. ‘Operator Changer’ will change pluses to minuses, along\nwith similar operations, similarly to the ‘Math Mutator’ of PIT. ‘Binary Operator Remover’\nwill remove a binary operator and only leave one of the operands, similarly to the ‘Arithmetic\nOperator Deletion Mutator’ of PIT. ‘Augmented Assignment Changer’ will change += to -=,\nvice versa, etc., similarly to the ‘Increments Mutator’ of PIT. ‘If negator’ will add a not to\nan if condition, similarly to the ‘Negate Conditionals Mutator’ of PIT. Mutators were added\niteratively until the ARHE dataset contained 200 bugs, which we deemed to be a reasonable\nnumber for APR evaluation.\nWhen using this dataset, we additionally compare against a template-based APR baseline\nthat has the reverse mutators of those used to construct the dataset, and randomly applies\nthem to the buggy code. This baseline is used for this benchmark as we seek to demonstrate\nthat it is not trivial to repair the bugs in this benchmark just because the mutators that were\nused to cause the bugs were simple. We run this baseline 100 times as it is stochastic. Note\nthat 90 bugs of ARHE are created by deletion or string mutation, and consequently are not\nreversible by the baseline: all the remaining mutations are reversible and therefore can be\nﬁxed by our template-based baseline given sufﬁcient time. In Table 1, the 24 bugs from If\nRemover and 24 bugs from Binary Operator Remover are not reversible; furthermore, we\nmanually determine that 42 of the 63 String Literal Changer bugs are not reversible, making\nfor a total of 90 bugs that cannot be repaired by applying the same mutation set.\nRegarding speciﬁc APR parameters, for each dataset we provide AutoSDwith the buggy\nmethod and generate 10 patches, to match the settings in the large-scale empirical work by\nJiang et al. ( 2023a), who evaluate the repair performance of multiple large language models\nand more traditional learning-based APR techniques. Their evaluation setup of generating\n10 patches was motivated by Noller et al. ( 2022), who note that developers are willing to\nreview up to ten patches, and thus provides a practical basis for comparing APR techniques\nacceptable to developers. We note our setting assumes less exact information and is thus\nmore realistic: Jiang et al. evaluate with perfect statement-level FL, whereas AutoSDuses\nperfect method-level FL and the bug report, and thus needs to also identify which statement\nis faulty within the method based on available information. When evaluating the generated\npatches, we run the tests provided by each dataset for each bug; a ﬁx that makes all tests\npass is deemed a plausible patch, and plausible patches are manually inspected to see if\nthey are semantically equivalent with the developer patch. Semantically equivalent ﬁxes are\ndeemed correct; semantic equivalence is determined by going through the generated patches\nand ﬁnding counterexamples where the behavior of the developer patch and generated patch\nwould diverge. This is important to ensure that the LLM is not simply making all tests pass\nand thus being potentially misleading, but actually correctly ﬁxing the bug. If at least one\nof the 10 generated patches are correct or plausible, the bug is deemed correctly ﬁxed or\nplausibly ﬁxed, respectively.\nAutoSDrequires the use of an LLM and a debugger. For the LLMs, we experiment\nwith the CodeGen (Nijkamp et al. 2022), Codex (Chen et al. 2021) (code-davinci-002), and\nChatGPT (a sibling model to InstructGPT (Ouyang et al. 2022)) LLMs, with the ChatGPT\nLLM being the default model. LLM output is sampled using a temperature of 0.7. Different\ndebuggers are used depending on the target language; we use the jdb tool for the Java\nbenchmarks (Defects4J v1.2 and v2.0) and the pdb tool for the Python benchmarks (ARHE\nand BugsInPy). The maximum iteration limit, s,i ss e tt o3 .\n123\nEmpirical Software Engineering (2025) 30 :45 Page 13 of 28 45\n4.2.2 Human Study Parameters\nTo approximate the real-world impact of AutoSD, we perform a human study by asking\nparticipants to review patches, based on the real-world applications of APR (Marginean et al.\n2019; Kirbas et al. 2021). We speciﬁcally sampled 12 bugs where AutoSDmade a patch that\ncaused the initially failing test to pass: a random sample of six such bugs from the ARHE\ndataset (which had complete documentation), and six real-world bugs from the BugsInPy\nPython dataset (Widyasari et al. 2020). In our preliminary studies, we found that reviewing\n12 patches could take a long time, so we divided the 12 bugs into two groups of six (each\ncontaining three ARHE and three BugsInPy bugs) and randomly assigned participants to\nsolve code review problems from one of the groups, so that each participant would see six\nbugs. A scheme of the code review screen that was presented to participants is shown in Fig. 3\n(a); a screenshot of the the survey website which corresponds to our schematic is shown in\nFig. 3 (b). Our human study received IRB review exemption (IRB-23-054); our study was\nconducted on the basis of Ko et al. ( 2015), who recommend randomization and institutional\noversight on study design.\nFor each code review problem, participants are provided with the buggy code, the bug-\nrevealing (failing) test, along with the patch; they are provided with the explanation in a\nrandomly selected three of the six cases. Each step of the explanation has a header, which\nis a summary of the hypothesis explaining the bug; the header is color-coded based on the\npredicted conclusion, with supported/rejected/undecided hypotheses being green/red/yellow,\nrespectively, as in Fig. 3. Each header can be clicked to reveal the full reasoning process\nof AutoSDas depicted in Fig. 1. Participants are asked three questions for each patch:\n(Q1) whether the patch is a correct patch, where they may answer yes, no, or unsure (as\na proxy for checking correctness during the code review process (Sadowski et al. 2018));\n(Q2) a short justiﬁcation of their decision in Q1, to ﬁlter potential bad-faith answers; and\n(Q3) when an explanation is available, whether the explanation was helpful in making their\ndecision, to measure the differing impact of explanations for different patches. Based on\nthe developer patch for each bug, the authors determined which patches were accurate;\ndeveloper assessment accuracy was calculated based on whether developers under a certain\ncondition (i.e., with or without explanations) had the same answer to Q1 when compared to\nthe developer-patch determined patch correctness.\nTo recruit participants, we advertised the task to both undergraduate and graduate students\nwith at least 1 year of Python experience, as well as professional developers at a company\nthat specializes in software testing techniques. Overall, we recruit 20 participants: eight\nundergraduate and six graduate students, as well as six professional developers whose career\nspan from 3 to 10 years. As a result of dividing the participants into two groups, each\ndebugging problem was inspected by 10 people. Participants start with a brieﬁng of what\nFig. 3 Human Study Screen Scheme and Screenshot. For a larger version of the screenshot, see our Appendix\n123\n45 Page 14 of 28 Empirical Software Engineering (2025) 30 :45\nTable 2 Repair results on the\nARHE benchmark Result Template-based LLM- Base AutoSD\nPlausible 85.77 ± 4.20 179 189\nCorrect - 177 187\nThe template-based performance is based on 100 reruns, and shows the\nmean and standard deviation repair performance\nthey should do in the study, solve an example code review problem as practice, and then solve\nsix code review problems in 30-40 minutes in a randomized order. The six code review tasks\ncontain 2 correct and 1 incorrect patches for ARHE and BugsInPy benchmarks, respectively.\nAfter conducting a post-questionnaire about their demographics and overall satisfaction with\nexplanations, we perform an interview that lasted about 5 minutes on their impression of the\ntool for qualitative analysis.\n5 Experimental Results\nWe present the results of empirical evaluation below.\n5.1 RQ1: How Well Does AUTO SDPerform Repair?\nIn Table 2, we present the APR performance of AutoSDon the ARHE benchmark when\ncompared with LLM- Base and the template-based baseline. Note that the template-\nbased baseline shows signiﬁcantly weaker repair performance than both LLM- Base and\nAutoSDwhen evaluated under the same conditions; as a result, we did not assess correctness\nfor the thousands of patches generated, as the upper bound of correctness is the plausible patch\ncount. Additionally, the performance of LLM- Base and AutoSDare similar, demonstrating\nAutoSDretains the repair performance of the LLM while simultaneously being capable of\ngenerating explanations.\nIn Table 3, we present the APR performance of AutoSDon the Defects4J benchmarks\nwhen compared against LLM- Base and the best-performing techniques from the empirical\nstudy by Jiang et al. ( 2023a): Recoder, a DL-based APR technique (Zhu et al. 2021)w h i c h\na custom neural architecture for repair that utilizes ASTs, and ﬁnetuned InCoder (Fried\net al. 2022), a language model from Facebook, which was ﬁnetuned to predict the ﬁxed\nline given the exact buggy line, i.e. perfect statement-level FL results, and thus uses more\nexact information than AutoSD.W eﬁ n dt h a t AutoSDagain shows competitive performance\nwhen compared to other baselines, even those that have more speciﬁc information provided.\nAs an additional reference point, when compared against the repair results of Codex on\nDefects4J presented by Xia et al. ( 2022) and ChatRepair (Xia and Zhang 2023), which\ngenerate 200 patch candidates (unlike our 10) on both benchmarks under the ‘patch function’\nsetting, we ﬁnd that AutoSDoutperforms or matches their performance with substantially\nTable 3 Correct repair results on the Defects4J benchmarks\nBenchmark Recoder InCoder Codex* ChatRepair* LLM- Base AutoSD\nD4J v1.2 24 41 63 76 87 76\nD4J v2.0 11 28 45 48 110 113\nResults for Recoder and InCoder are from Jiang et al. ( 2023a). , while results from Codex and ChatRepair are\nfrom Xia et al. ( 2022) and Xia and Zhang ( 2023), and use 200 patch generations instead of 10\n123\nEmpirical Software Engineering (2025) 30 :45 Page 15 of 28 45\nless patch generation, while assuming the same FL conditions as our setup. We additionally\nanalyzed why AutoSDunderperformed LLM- Base on the Defects4J v1.2 dataset. We ﬁnd\nthat AutoSDshowed roughly the same performance as the baseline for every project in the\nbenchmark except Closure, which is a JavaScript compiler and hence deals with complex\nobjects with nested reference structures. Consequently, AutoSDseemed to have difﬁculty\nmaking proper judgments on whether the hypothesis was actually met on the basis of these\ncomplex values, and these incorrect decisions ultimately led to inaccurate patch generation. It\nis noteworthy that such complex values would also require signiﬁcant efforts for the developer\nto understand as well. Thus, we argue that there is a need to research how to present values\nfor both LLM and developer consumption.\nAnswer to RQ1: AutoSDis capable of operating at a competitive level of program repair\nperformance when compared to a diverse set of baselines on three repair benchmarks.\n5.2 RQ2: How Does the Debugger Influence the Behavior of AUTO SD?\nThis RQ ﬁrst investigates whether the conﬁdence in a result indicated by the prediction of the\n<DONE> token actually correlates with better performance. The results are presented in Fig. 4.\nFor Defects4J, as it was infeasible to manually label all 1045 plausible patches generated\nfor the dataset, we sampled 100 patches with and without <DONE> to get results. As the\nﬁgure shows, for both the ARHE and Defects4J datasets, AutoSDshows a higher precision\nwhen the <DONE> token is generated as part of a conclusion, indicating that AutoSDcan\nindeed signal when it is likely to generate a plausible or correct patch. Furthermore, for\nbugs where a plausible patch was generated and the <DONE> token was predicted, 89%\nwere correctly ﬁxed, while for bugs with plausible patches but without <DONE> predictions\n82% were correctly ﬁxed. These results indicate that AutoSDcan indicate when its output is\nlikely to help developers based on its interaction, and thus aid developer decision-making and\npotentially reduce developer inspection cost when processing automated debugging results. It\nis noteworthy that this is a natural property of the patch generation process of AutoSDitself,\nand did not require a separate patch correctness detector (Xiong et al. 2018) being added\nspeciﬁcally for this purpose.\nFig. 4 <DONE> &p e r f\n123\n45 Page 16 of 28 Empirical Software Engineering (2025) 30 :45\nWe also investigate the performance when the debugger/code execution results are also\npredicted by the LLM, instead of being obtained via concrete execution, for the ARHE dataset;\nwould the <DONE> token still predict good performance? In this ‘debugger hallucination’\nscenario, <DONE>-predicted solutions were actually 11%p less likely to be plausible; this\nis in contrast to using actual code execution results, where <DONE>-predicted solutions are\n12.4%p more likely to be plausible. Furthermore, individual runs became much less likely to\nbe plausible: while 73% of the individual AutoSDruns would yield a plausible patch, only\n63% would when the debugger was ablated. Thus, incorporating code execution contributes\nto the reliability of AutoSD; we later demonstrate in RQ5 that developers found real code\nexecution results useful as well.\nAnswer to RQ2: AutoSDcan indicate when its answers are more likely to be correct with\nthe <DONE> token, which we also use to verify the utility of debugger use.\n5.3 RQ3: How Does the Choice of LLM Influence the Performance of AUTO SD?\nIn Fig. 5, we depict the performance of AutoSDas different underlying LLMs are used, with\nthe x axis showing different LLMs roughly sorted in terms of number of parameters and the\ntechnical advancement of training, and the y axis showing the performance of AutoSDwhen\nusing the LLM on the ARHE benchmark. The performance of AutoSDis depicted along with\nthe performance of simply querying the LLM to ﬁx the bug. As shown, the performance of\nAutoSDrapidly improves and ultimately becomes comparable to the performance of LLM-\nBase, suggesting that AutoSDshows better performance when using stronger language\nmodels; for smaller models such as CodeGen-6B, repair itself fails in a zero-shot setting,\nas in our experiments it would simply return the original buggy code. (We conﬁrm that the\nmodel implementation works by also evaluating in a few-shot setting for CodeGen-6B; it\ncould ﬁx 44 bugs in that case.) Indeed, it appears that CodeGen-6B is incapable of running\nAutoSD, as it failed to match the provided format for Scientiﬁc Debugging in 68% of all\ncases; meanwhile, that was only the case in 0.7% of runs when using ChatGPT. Thus, we may\nspeculate that as language models improve, the performance of AutoSDwill also become\nstronger.\nAnswer to RQ3: Under our experimental setup, as the underlying language model\nimproves, the performance of AutoSDalso increases.\n5.4 RQ4: How do Developers Benefit from AUTO SDExplanations?\nIn this section, we evaluate whether developers beneﬁt from explanations in a way that is\nunlikely to be swayed by a participant’s opinion about explanations. The results of measuring\nthe code review time, accuracy, and whether the explanation was rated as helpful in making\nthe decision are presented in Fig. 6.\nFirst, looking at the amount of time that it took to solve the code review problems, we ﬁnd\nthat the time it took to solve a problem was generally similar between the case where there\nwas no explanation and when there was an explanation. There is no case where the difference\nis statistically signiﬁcant, despite the explanations of AutoSDproviding more information\nthan the case without explanations, and thus potentially requiring more processing time from\ndevelopers.\nRegarding the accuracy with and without explanations, participants were more accurate\nwhen solving the same problems with explanations than without explanations in seven cases,\n123\nEmpirical Software Engineering (2025) 30 :45 Page 17 of 28 45\nCodeGen-6B Codex ChatGPT\nModel\n0\n50\n100\n150\n200Plausible in ARHE\nARHE performance by model size\nAutoSD\nLLM-Base\nFig. 5 Model Size\nwith ﬁve of them being concentrated in the real-world BugsInPy benchmark. Using the\nMann-Whitney U Test as suggested by Arcuri and Briand (Arcuri and Briand 2011), we\nfound explanations to have a statistically signiﬁcant effect on developer accuracy assess-\nment ( p < 0.05). These results demonstrate that AutoSDcould have a positive impact on\nreal-world developer productivity when using APR, as the judgment quality improved when\nevaluating real-world bugs while requiring roughly the same amount of developer time.\nMeanwhile, there are two cases where the use of explanations lead to a drop in accuracy:\nARHE105 and BIP003. For BIP003, we found that the respondents became more cautious\nafter looking at the explanation, and answered that they needed more information to judge\nit. Meanwhile, for ARHE105 the participants who answered incorrectly accepted the rea-\nsoning of AutoSDwithout signiﬁcant scrutiny. While this was a somewhat rare incidence\nthat happened in one of the 12 randomly sampled problems, it highlights the need of fur-\nther research to identify potentially misleading reasoning. Additionally, developer accuracy\nimproved with explanations on the two incorrect patches from BIP (BIP002 and BIP004)\nmeaning developers are not blindly accepting patches with explanations.\nOn whether the participants found the explanations helpful in their decision-making, in\neight of the twelve questions developers noted that the explanations were actually helpful\nwhen coming to their conclusion, underscoring the psychological beneﬁt that providing\nexplanations for patches holds.\nAnswer to RQ4: When exposed to explanations generated by AutoSD, human participants\ncould process patches in roughly the same time, while achieving a higher accuracy in ﬁve\nof the six of the real-world bugs. They also rate the explanations as helpful in two-thirds\nof all bugs.\nFig. 6 Developer performance on code review tasks with and without explanations from AutoSD, and expla-\nnation ratings\n123\n45 Page 18 of 28 Empirical Software Engineering (2025) 30 :45\n5.5 RQ5: How do Developers Feel Towards AUTO SDExplanations?\nThe results of our post-questionnaire are presented in Fig. 7. To our surprise, there was a\ndiscrepancy in satisfaction of AutoSDbetween students and professional developers: while\nmore than half of the students were satisﬁed with AutoSD, only one of the six developers\nwere satisﬁed. We use these differing results as an opportunity to discuss the strengths and\npotential improvements of AutoSD-generated explanations.\nWhat did students ﬁnd appealing about the explanations of AutoSD? Ten out of the 14\nstudent participants noted that they ‘missed’ the explanations when they were not available.\nWhen asked why they wanted to see the explanations in these cases, and how they used\nexplanations when they were available, students described a wide range of thought processes\nthat were aided by the existence of explanations. One common pattern was to think through\nthe patch by oneself, then comparing one’s internal thoughts to the provided explanation;\none participant referred to the explanation as useful because it could function as a ‘rubber\nduck’.\n2 Another common usage of explanations was to look at the explanation to discern\nwhere to focus effort on, and thus guide the direction of judgment. Other students would use\nthe explanation to gain a better understanding of what the code was intended to do. We thus\nargue that a strength of AutoSD-generated explanations is that they can accommodate a\ndiverse set of thought processes , potentially aiding a wide range of developers.\nMeanwhile, another usage pattern was to look at the experiments and observations within\nthe explanations to get a concrete idea of what the values are at certain points, and use those\nvalues to build a mental model of how the bug happened. This points to another strength of\nAutoSD, which is that it incorporates actual values in its explanations :i nF i g . 7 (a), we\nnote that more than 90% of students thought that the addition of execution results improved\ntheir trust in the explanations. Critically, these results justify AutoSD’s architectural choice\nof interacting with code: while the interaction may cause program repair to take a greater\namount of time, it provides reliable elements in the explanation that improve developer trust\nof the explanations.\nOn the other hand, professional developers showed a more mixed attitude towards the\nexplanations of AutoSD. It is noteworthy that developers are not opposed to explanations\nthemselves: half agreed or strongly agreed that explanations would be important when using\nan APR tool (Fig. 7 (b)), highlighting the importance of the problem. When asked why\nthey found the explanations of AutoSDleft more to be desired, one suggestion was that\nthe current explanations would be more useful if they were connected with “business logic”\nor speciﬁcations, a suggestion echoed by one of the student participants as well. The pro-\nfessional developers argued that without such connections, the explanations needed to be\nveriﬁed rigorously and even after that were of limited value. Thus one potential direction of\nimprovement would be to integrate explanations with existing development artifacts like\nspeciﬁcation documents .\nAnother common suggestion was to improve the interface of the tool: developers noted that\nthey might use the tool if it was attached to an IDE, and that the explanations were too wordy.\nThis feedback suggests that to improve developer satisfaction, we may consider integrating\nexplanations to platforms that developers frequent (as also suggested by Kochhar et al.\n(2016)), and further study the speciﬁcs of explanations that developers ﬁnd satisfactory.\nLooking at the overall statistics, we ﬁnd that 70% of participants agreed that explanations\nwere an important factor when using program repair, and 55% found the scientiﬁc debug-\n2 See https://en.wikipedia.org/wiki/Rubber_duck_debugging.\n123\nEmpirical Software Engineering (2025) 30 :45 Page 19 of 28 45\nFig. 7 Human study post-questionnaire results by group\nging details (Expl. Details Satisfaction of Fig. 7) satisfactory, showing that a majority of\nparticipants agreed with the overall motivation and formulation of AutoSD.\nAnswer to RQ5: While the explanations of AutoSDare capable of accommodating diverse\nthought processes and improving developer trust by using concrete execution results, they\ncould be further improved by enhancing the interface and by linking to speciﬁcations.\n5.6 RQ6: What do AUTO SDExplanations Look Like?\nWhat do the explanations generated by AutoSDlook like? In addition to the example embed-\nded in Fig. 1, we provide two reasoning traces generated by AutoSDthat were liked (BIP006\n- 75% liked) and disliked (BIP002 - 16% liked) in the human study from the real-world\nBugsInPy problems. On the left of Fig. 8, we show a liked explanation, along with a con-\ndensed failing test and the generated ﬁx. Looking at the patch, the developer will see that a\n.lower() call was added; without an explanation, this ﬁx can appear spurious. In contrast,\nby providing a rationale on why AutoSDfocused on this area, participants could swiftly\nidentify whether this ﬁx was related to the test. For example, Student-6 said “I ﬁrst looked\nat the explanation, which helped me identify which part of the code to look at”. The subse-\nquent experiment conﬁrms that an uppercase ‘Chunked’ header was within the program\nstate, which is the source of the bug. These execution results helped participants understand\nthe bugs, e.g. Student-11 who noted that “expression values were useful in making deci-\nsions”. Overall, this patch was correct, and the explanation aided developer comprehension\nand built trust. While we provide a simple example from the human study, we also note that\nAutoSDworks on more complex bugs as demonstrated in Section 5.1, and provide additional\nexamples in the Appendix.\nAttempts at hypothesizing can fail as well. The right side of Fig. 8 depicts an case where\nAutoSDfails to validate any hypotheses. While AutoSDinitially generates a hypothesis\nabout appending in the wrong order, the line that is suggested in the experiment is actually\n123\n45 Page 20 of 28 Empirical Software Engineering (2025) 30 :45\nFig. 8 Example successful and unsuccessful repairs and explanations of AutoSDfrom the human study\nnot covered; as a result, the debugger provides feedback that the breakpoint was not covered.\nThis is one of the most common failure causes - our analysis on 25 cases where all hypotheses\nwere rejected revealed that in 13 of the 25 cases, breakpoints suggested by AutoSDwere never\nhit, and consequently AutoSDcould not get results for generated experiments. In BIP002’s\ncase, instead of looking for new breakpoints that could be covered by the test, the LLM starts\nsuggesting that the test is wrong. Ultimately, while a ﬁx is generated, the explanation has little\nconnection to the patch, and as a result the human study participants rated the explanation as\nunhelpful; the patch itself is plausible but incorrect as well. Nonetheless, the example also\nillustrates how bad explanations can still lead to better decision-making: developers may see\nthat the foundations of the patch are weak, and be (rightly) more suspicious about the patch.\nIn this context, it is noteworthy that developers who saw the explanation of BIP002 more\naccurately assessed it (Fig. 6). Other failure modes include generating an invalid experiment\nexpression (2/25) or adding multiple print commands in the experiment script when the\ninfrastructure of AutoSDonly allows one print command, causing inaccurate hypothesis\nrejection (2/25).\nWe additionally present examples from the more complex bugs of the Defects4J dataset\nin Fig. 9. In the left case, AutoSDhypothesizes that the bug is happening when the current\ntoken is END_OBJECT, and generates an experiment to conﬁrm that this is the case. As this\nis actually the case, it proceeds to search for what behavior would lead to the failing test to\npass in Attempt 2. Combining these two steps together, it generates a patch identical (in this\nmethod) to the developer patch, and that makes all tests in the test suite pass. Meanwhile, on\nthe right, another example of failing to identify the right breakpoint is provided. In this case,\nthe same hypothesis and experiments are parroted, leading to no improvement.\nAnswer to RQ6: AutoSDcan generate helpful explanations on its patches, but the reason-\ning process may fail as well. A common failure cause is an inability to identify the right\nbreakpoints.\n6 Discussion\nThis section provides threats and limitations of our work.\n123\nEmpirical Software Engineering (2025) 30 :45 Page 21 of 28 45\nFig. 9 Example AutoSDruns from Defects4J bugs\n6.1 Threats to Validity\nInternal Validity concerns whether the analysis supports claims about cause and effect.\nPotential threats include incorrect implementations, inaccurate patch correctness assessment,\nand the risk of biased responses in our human study. To mitigate the impact of the ﬁrst\ntwo concerns, we plan to make our implementation and repair results publicly available\nfor scrutiny. For our human study, in addition to gathering developer sentiment about the\ngenerated explanations (which included occasional negative feedback), we also ﬁnd that\nparticipant accuracy improved in ﬁve of the six BugsInPy problems, which is a result difﬁcult\nto be due to bias. In addition to these issues, we note that AutoSDcan at times generate\nmisleading explanations; however, the explanations are still grounded in actual execution\nresults, and thus more reliable than techniques that do not use such results at all, such as\ncommit message generation techniques or LLM-generated explanations without such results.\nExternal Validity concerns whether the results presented in this paper may generalize to\nother results. A particular concern when using large language models is that their training data\nmay include segments of the evaluation data. To mitigate this issue, we newly constructed\nthe ARHE dataset for repair and evaluated AutoSDon that benchmark. Furthermore, our\nexplanations were likely never within the training data, as developers usually describe code\nwith less of a structure than Scientiﬁc Debugging prescribes, even if they think along the\nlines of it.\n6.2 Limitations\nAutoSDhas a number of limitations that we would like to highlight. First, to enable multi-\nstep interaction with code, both the language model and debugger must be invoked multiple\ntimes, which increases the repair time of the technique; in our experiments, AutoSDtook on\naverage 4.66 times longer to generate a patch when compared to LLM- Base . Speciﬁcally,\nto generate ten patches for a single bug, AutoSDwould on average take 9 minutes and 22\nseconds, while LLM- Base would take 1 minutes and 59 seconds. Nonetheless, given the\nsigniﬁcant developer demand for explanations of automatically generated patches as shown\nin Fig. 7, we believe that the additional cost needed to build explanations for patches is\njustiﬁed. We note that this execution time is primarily due to the interaction with external\n123\n45 Page 22 of 28 Empirical Software Engineering (2025) 30 :45\ntools, and is not not signiﬁcantly impacted by the length of the prompts itself; when removing\nthe bug report, the execution time was practically the same, while the performance dropped by\n39%, indicating that providing additional information can help without signiﬁcant execution\ncost. Second, as a step towards explainable automatic debugging, we evaluated in the setting\nwhere method-level FL was done, and AutoSDwould then perform statement-level FL in an\nexplainable manner. Our main focus in this paper was to establish that AutoSDcan generate\nexplanations that aid developers in practice; we hope to work on explainable method-level\nFL in future work. On a related note, our technique can only handle single-method bugs\nas of now; incorporating a wider range of information to handle more complex bugs is\nalso an interesting research direction. In this work, we experimented on benchmarks for\nPython and Java debugging problems. Further experimentation is required to discern how\nwell AutoSDwould perform with other languages and benchmarks. Nonetheless, because\nthe prompt of AutoSDis presented in a zero-shot manner, one would only need to change the\ndebugger examples when the language changes. Indeed, aside for the Java and Python-speciﬁc\ndebugger command examples, our starting prompt did not change when using ARHE and\nDefects4J. Finally, the generated explanation may occasionally lend credibility to incorrect\npatches; by allowing our technique to indicate its conﬁdence in its output and demonstrating\nthat conﬁdence is correlated with correctness, we take the ﬁrst steps to address this issue.\nFurthermore, our explanation includes concrete code execution results, aiding developer\ndecision-making (Fig. 7).\n6.3 Future Work\nThe limitations presented in our work also provide a few directions that AutoSDcould\nimprove to further its cause of helping developers in automatically debugging issues. As\ndescribed in RQ5 (Section 5.5), professional developers pointed out problems with the current\nexplanation format, such as that it was too long or lacked connection with speciﬁcation arti-\nfacts. These results (i) provide valuable feedback on what types of explanations are appealing\nto developers, and (ii) suggest that AutoSDcould be improved by incorporating text sum-\nmarization techniques (Allahyari et al. 2017) (while taking care to preserve the actual value\naspects, which improved developer trust), and by improving the user interface via links to\nconcrete artifacts. Thanks to the incorporation of chain-of-thought (Wei et al. 2022) prompt-\ning in APR as well, AutoSDcan also beneﬁt from the large volume of recent work that aims\nto improve the performance of CoT prompting. Of particular interest is using techniques\nsuch as reﬂection (Shinn et al. 2023) or tree-of-thought (Yao et al. 2023), which could help\nimprove the reasoning process of AutoSDand reduce failures such as those presented in RQ6\n(Section 5.6) by allowing the LLM to ‘correct’ its unhelpful reasoning process. Finally, while\nin this work 10 patches were generated per bug, which is likely the limit of what developers\nare willing to manually inspect (Noller et al. 2022), as a future direction one could improve\nthe performance of AutoSDby sampling more patches and ranking them using techniques\nsuch as self-consistency (Wang et al. 2023).\n6.4 Implications\nOur work has a few implications for future work. One is that explainable software engineer-\ning tasks, once considered difﬁcult due to the signiﬁcant difﬁculty of dealing with natural\nlanguage, has been made signiﬁcantly easier as LLMs are ﬂuent in natural language. In our\nwork, we also present criteria for explanations, namely that explanations should provide\n123\nEmpirical Software Engineering (2025) 30 :45 Page 23 of 28 45\nboth new information and a new perspective, which was highly rated by developers as well;\nthese guidelines should help researchers develop novel explainable automated debugging\ntechniques going forward. Another potential implication of this work is that we have demon-\nstrated LLMs can follow development processes that were designed for humans in mind. This\nindicates the potential for research on how LLMs could ﬁt naturally into human workﬂow,\nwhich was highlighted as important by prior work (Winter et al. 2022).\n7 Conclusion\nIn this paper, we summarize the importance of explanations for automated debugging results\nas revealed by prior studies, and the lack of automated techniques capable of providing\nadequate explanations for humans. We argue this is due to a lack of automated debugging\ntechniques that deduce in a human way, and bridge this gap between automatic and manual\ndebugging practices by using LLMs to emulate the Scientiﬁc Debugging process. We demon-\nstrate that AutoSDis capable of achieving competitive repair performance when compared to\nother repair baselines, while having favorable properties for practical use such as an indication\nof conﬁdence in the output. The repair performance of AutoSDalso improves as language\nmodels become more capable, suggesting the performance and availability of explanations\nmay improve as language models get better. Finally, our human study reveals that the auto-\nmatically generated explanations could improve developer assessment of patches, with a\nmajority of students also expressing that they ‘missed’ the explanations when they were not\navailable. The interviews we performed show that the explanations AutoSDgenerates could\naid a wide range of developer thought patterns, and that they could be improved via tighter\nintegration into the development process, such as making connections to written speciﬁ-\ncation. Overall, we believe that the rapid improvement in language model capabilities can\nbe harnessed to signiﬁcantly ease developer use of automated techniques, and we hope to\ndevelop more human-friendly automated debugging techniques as future work.\nSupplementary Information The online version contains supplementary material available at https://doi.\norg/10.1007/s10664-024-10594-x .\nAcknowledgements Sungmin Kang and Shin Y oo have been supported by National Research Foundation\nof Korea (NRF) funded by the Korean Government MSIT (RS-2023-00208998), as well as the Institute\nof Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea\ngovernment (MSIT) (RS-2022-00155958, RS-2021-II211001, and RS-2022-II220995).\nFunding Open Access funding enabled and organized by KAIST.\nData Availability Our code and experimental results can be found at https://doi.org/10.6084/m9.ﬁgshare.\n27861528.v3.\nDeclarations\nConﬂicts of Interest The authors declare that Shin Y oo is a member of the EMSE Editorial board. All co-authors\nhave seen and agree with the contents of the manuscript and there is no ﬁnancial interest to report.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,\nand indicate if changes were made. The images or other third party material in this article are included in the\narticle’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is\nnot included in the article’s Creative Commons licence and your intended use is not permitted by statutory\n123\n45 Page 24 of 28 Empirical Software Engineering (2025) 30 :45\nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.\nTo view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ .\nReferences\nAlaboudi A, LaToza TD (2020) Using hypotheses as a debugging aid. In: 2020 IEEE Symposium on Visual\nLanguages and Human-Centric Computing (VL/HCC), IEEE, pp 1–9\nAllahyari M, Pouriyeh S, Asseﬁ M, Safaei S, Trippe ED, Gutierrez JB, Kochut K (2017) Text summarization\ntechniques: A brief survey. International Journal of Advanced Computer Science and Applications 8(10).\nhttps://doi.org/10.14569/IJACSA.2017.081052\nArcuri A, Briand L (2011) A practical guide for using statistical tests to assess randomized algorithms in\nsoftware engineering. In: Proceedings of the 33rd International Conference on Software Engineering,\nAssociation for Computing Machinery, New Y ork, NY , USA, ICSE ’11, pp 1–10. https://doi.org/10.\n1145/1985793.1985795\nArrieta AB, Díaz-Rodríguez N, Del Ser J, Bennetot A, Tabik S, Barbado A, García S, Gil-López S, Molina D,\nBenjamins R et al (2020) Explainable artiﬁcial intelligence (xai): Concepts, taxonomies, opportunities\nand challenges toward responsible ai. Information Fusion 58:82–115\nBöhme M, Soremekun EO, Chattopadhyay S, Ugherughe EJ, Zeller A (2017) How developers debug software\n- the dbgbench dataset. In: 2017 IEEE/ACM 39th International Conference on Software Engineering\nCompanion (ICSE-C), pp 244–246, https://doi.org/10.1109/ICSE-C.2017.94\nBrown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P , Neelakantan A, Shyam P , Sastry G, Askell\nA et al (2020) Language models are few-shot learners. Adv Neural Inf Process Syst 33:1877–1901\nChen M, Tworek J, Jun H, Y uan Q, Pinto HPdO, Kaplan J, Edwards H, Burda Y , Joseph N, Brockman G et al\n(2021) Evaluating large language models trained on code. arXiv:2107.03374\nColes H, Laurent T, Henard C, Papadakis M, V entresque A (2016) Pit: A practical mutation testing tool for\njava (demo). In: Proceedings of the 25th International Symposium on Software Testing and Analysis,\nAssociation for Computing Machinery, New Y ork, NY , USA, ISSTA 2016, pp 449–452\nDam HK, Tran T, Ghose A (2018) Explainable software analytics. In: Proceedings of the 40th International\nConference on Software Engineering: New Ideas and Emerging Results, Association for Computing\nMachinery, New Y ork, NY , USA, ICSE-NIER ’18, pp 53–56\nFried D, Aghajanyan A, Lin J, Wang S, Wallace E, Shi F, Zhong R, Yih Wt, Zettlemoyer L, Lewis M (2022)\nIncoder: A generative model for code inﬁlling and synthesis. arXiv preprint arXiv:2204.05999\nGao L, Madaan A, Zhou S, Alon U, Liu P , Y ang Y , Callan J, Neubig G (2022) Pal: Program-aided language\nmodels. arXiv:2211.10435\nGazzola L, Micucci D, Mariani L (2019) Automatic software repair: A survey. IEEE Trans Software Eng\n45(1):34–67. https://doi.org/10.1109/TSE.2017.2755013\nGoues CL, Pradel M, Roychoudhury A (2019) Automated program repair. Commun ACM 62(12):56–65\nGould JD (1975) Some psychological evidence on how people debug computer programs. Int J Man Mach\nStud 7(2):151–182. https://doi.org/10.1016/S0020-7373(75)80005-8 ,U R L https://www.sciencedirect.\ncom/science/article/pii/S0020737375800058\nHaas R, Elsner D, Juergens E, Pretschner A, Apel S (2021) How can manual testing processes be optimized?\ndeveloper survey, optimization guidelines, and case studies. In: Proceedings of the 29th ACM Joint\nMeeting on European Software Engineering Conference and Symposium on the Foundations of Software\nEngineering, Association for Computing Machinery, New Y ork, NY , USA, ESEC/FSE 2021, pp 1281–\n1291\nJiang J, Xiong Y , Zhang H, Gao Q, Chen X (2018) Shaping program repair space with existing patches and\nsimilar code. Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing\nand Analysis\nJiang N, Liu K, Lutellier T, Tan L (2023a) Impact of code language models on automated program repair.\n2302.05020\nJiang N, Lutellier T, Lou Y , Tan L, Goldwasser D, Zhang X (2023b) Knod: Domain knowledge distilled tree\ndecoder for automated program repair. 2302.01857\nJiang S, Armaly A, McMillan C (2017) Automatically generating commit messages from diffs using neu-\nral machine translation. In: 2017 32nd IEEE/ACM International Conference on Automated Software\nEngineering (ASE), pp 135–146, https://doi.org/10.1109/ASE.2017.8115626\nJones JA, Harrold MJ, Stasko J (2002) Visualization of test information to assist fault localization. In: Pro-\nceedings of the 24th International Conference on Software Engineering, Association for Computing\nMachinery, New Y ork, NY , USA, ICSE ’02, pp 467–477\n123\nEmpirical Software Engineering (2025) 30 :45 Page 25 of 28 45\nJust R, Jalali D, Ernst MD (2014) Defects4j: A database of existing faults to enable controlled testing studies for\njava programs. In: Proceedings of the 2014 International Symposium on Software Testing and Analysis,\nAssociation for Computing Machinery, New Y ork, NY , USA, ISSTA 2014, pp 437–440. https://doi.org/\n10.1145/2610384.2628055\nKirbas S, Windels E, McBello O, Kells K, Pagano M, Szalanski R, Nowack V , Winter ER, Counsell S, Bowes D,\nHall T, Haraldsson S, Woodward J (2021) On the introduction of automatic program repair in bloomberg.\nIEEE Softw 38(4):43–51. https://doi.org/10.1109/MS.2021.3071086\nKo AJ, LaToza TD, Burnett MM (2015) A practical guide to controlled experiments of software engineering\ntools with human participants. Empirical Softw Engg 20(1):110–141. https://doi.org/10.1007/s10664-\n013-9279-3\nKochhar PS, Xia X, Lo D, Li S (2016) Practitioners’ expectations on automated fault localization. In: Proceed-\nings of the 25th International Symposium on Software Testing and Analysis, Association for Computing\nMachinery, New Y ork, NY , USA, ISSTA 2016, pp 165–176, https://doi.org/10.1145/2931037.2931051\nLayman L, Diep M, Nagappan M, Singer J, Deline R, V enolia G (2013) Debugging revisited: Toward under-\nstanding the debugging needs of contemporary software developers. In: 2013 ACM / IEEE International\nSymposium on Empirical Software Engineering and Measurement, pp 383–392, https://doi.org/10.1109/\nESEM.2013.43\nLee J, Kang S, Y oon J, Y oo S (2024) The github recent bugs dataset for evaluating llm-based debugging\napplications. In: 2024 IEEE Conference on Software Testing, V eriﬁcation and V alidation (ICST), IEEE\nComputer Society, Los Alamitos, CA, USA, pp 442–44 https://doi.org/10.1109/ICST60714.2024.00049,\nURL https://doi.ieeecomputersociety.org/10.1109/ICST60714.2024.00049\nLi X, Li W, Zhang Y , Zhang L (2019) Deepﬂ: Integrating multiple fault diagnosis dimensions for deep fault\nlocalization. In: Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing\nand Analysis, Association for Computing Machinery, New Y ork, NY , USA, ISSTA 2019, pp 169–180\nLim BY , Dey AK, Avrahami D (2009) Why and why not explanations improve the intelligibility of context-\naware intelligent systems. In: Proceedings of the SIGCHI Conference on Human Factors in Computing\nSystems, Association for Computing Machinery, New Y ork, NY , USA, CHI ’09, pp 2119–2128\nLiu K, Wang S, Koyuncu A, Kim K, Bissyandé TF, Kim D, Wu P , Klein J, Mao X, Traon YL (2020) On the\nefﬁciency of test suite based program repair: A systematic assessment of 16 automated repair systems\nfor java programs. In: Proceedings of the ACM/IEEE 42nd International Conference on Software Engi-\nneering, Association for Computing Machinery, New Y ork, NY , USA, ICSE ’20, pp 615–627, https://\ndoi.org/10.1145/3377811.3380338\nMarginean A, Bader J, Chandra S, Harman M, Jia Y , Mao K, Mols A, Scott A (2019) Sapﬁx: Automated\nend-to-end repair at scale. In: 2019 IEEE/ACM 41st International Conference on Software Engineering:\nSoftware Engineering in Practice (ICSE-SEIP), pp 269–278, https://doi.org/10.1109/ICSE-SEIP .2019.\n00039\nMariani L, Pastore F, Pezze M (2011) Dynamic analysis for diagnosing integration faults. IEEE Trans Software\nEng 37(4):486–508. https://doi.org/10.1109/TSE.2010.93\nMartinez M, Monperrus M (2019) Astor: Exploring the design space of generate-and-validate program repair\nbeyond genprog. J Syst Softw 151:65–80\nMechtaev S, Yi J, Roychoudhury A (2016) Angelix: Scalable multiline program patch synthesis via symbolic\nanalysis. In: 2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE), pp 691–\n701,https://doi.org/10.1145/2884781.2884807\nMonperrus M (2019) Explainable software bot contributions: Case study of automated bug ﬁxes. In: 2019\nIEEE/ACM 1st International Workshop on Bots in Software Engineering (BotSE), IEEE Computer\nSociety, Los Alamitos, CA, USA, pp 12–1 https://doi.org/10.1109/BotSE.2019.00010 ,U R L https://doi.\nieeecomputersociety.org/10.1109/BotSE.2019.00010\nMonperrus M (2020) The Living Review on Automated Program Repair, URL https://hal.archives-ouvertes.\nfr/hal-01956501, working paper or preprint\nMoon S, Kim Y , Kim M, Y oo S (2014) Ask the mutants: Mutating faulty programs for fault localization.\nIn: 2014 IEEE Seventh International Conference on Software Testing, V eriﬁcation and V alidation, pp\n153–162,https://doi.org/10.1109/ICST.2014.28\nNijkamp E, Pang B, Hayashi H, Tu L, Wang H, Zhou Y , Savarese S, Xiong C (2022) Codegen: An open large\nlanguage model for code with multi-turn program synthesis. arXiv preprint\nNoller Y , Shariffdeen R, Gao X, Roychoudhury A (2022) Trust enhancement issues in program repair. In:\nProceedings of the 44th International Conference on Software Engineering, Association for Computing\nMachinery, New Y ork, NY , USA, ICSE ’22, pp 2228–2240, https://doi.org/10.1145/3510003.3510040\nOpenAI (2023) Gpt-4 technical report. 2303.08774\n123\n45 Page 26 of 28 Empirical Software Engineering (2025) 30 :45\nOuyang L, Wu J, Jiang X, Almeida D, Wainwright CL, Mishkin P , Zhang C, Agarwal S, Slama K, Ray\nA, et al. (2022) Training language models to follow instructions with human feedback. arXiv preprint\narXiv:2203.02155\nRigby PC, Bird C (2013) Convergent contemporary software peer review practices. In: Proceedings of the\n2013 9th Joint Meeting on Foundations of Software Engineering, Association for Computing Machinery,\nNew Y ork, NY , USA, ESEC/FSE 2013, pp 202–212, https://doi.org/10.1145/2491411.2491444\nSadowski C, Söderberg E, Church L, Sipko M, Bacchelli A (2018) Modern code review: A case study at google.\nIn: Proceedings of the 40th International Conference on Software Engineering: Software Engineering\nin Practice, Association for Computing Machinery, New Y ork, NY , USA, ICSE-SEIP ’18, pp 181–190,\nhttps://doi.org/10.1145/3183519.3183525\nShinn N, Cassano F, Labash B, Gopinath A, Narasimhan K, Y ao S (2023) Reﬂexion: Language agents with\nverbal reinforcement learning. 2303.11366\nSiegmund B, Perscheid M, Taeumel M, Hirschfeld R (2014) Studying the advancement in debugging practice\nof professional software developers. In: 2014 IEEE International Symposium on Software Reliability\nEngineering Workshops, pp 269–274, https://doi.org/10.1109/ISSREW.2014.36\nSun C, Khoo SC (2013) Mining succinct predicated bug signatures. In: Proceedings of the 2013 9th Joint\nMeeting on Foundations of Software Engineering, Association for Computing Machinery, New Y ork,\nNY , USA, ESEC/FSE 2013, pp 576–586\nWang X, Wei J, Schuurmans D, Le QV , Chi EH, Narang S, Chowdhery A, Zhou D (2023) Self-consistency\nimproves chain of thought reasoning in language models. In: The Eleventh International Conference on\nLearning Representations, URL https://openreview.net/forum?id=1PL1NIMMrw\nWei J, Wang X, Schuurmans D, Bosma M, hsin Chi EH, Le Q, Zhou D (2022) Chain of thought prompting\nelicits reasoning in large language models. ArXiv abs/2201.11903\nWidyasari R, Sim SQ, Lok C, Qi H, Phan J, Tay Q, Tan C, Wee F, Tan JE, Yieh Y , Goh B, Thung F, Kang\nHJ, Hoang T, Lo D, Ouh EL (2020) Bugsinpy: A database of existing bugs in python programs to enable\ncontrolled testing and debugging studies. In: Proceedings of the 28th ACM Joint Meeting on Euro-\npean Software Engineering Conference and Symposium on the Foundations of Software Engineering,\nAssociation for Computing Machinery, New Y ork, NY , USA, ESEC/FSE 2020, pp 1556–1560\nWinter ER, Nowack V , Bowes D, Counsell S, Hall T, Haraldsson S, Woodward J, Kirbas S, Windels E,\nMcBello O, Atakishiyev A, Kells K, Pagano M (2022) Towards developer-centered automatic program\nrepair: Findings from bloomberg. In: Proceedings of the 30th ACM Joint European Software Engineering\nConference and Symposium on the Foundations of Software Engineering, Association for Computing\nMachinery, New Y ork, NY , USA, ESEC/FSE 2022, pp 1578–1588, https://doi.org/10.1145/3540250.\n3558953\nXia CS, Zhang L (2022) Less training, more repairing please: Revisiting automated program repair via zero-\nshot learning. In: Proceedings of the 30th ACM Joint European Software Engineering Conference and\nSymposium on the Foundations of Software Engineering, Association for Computing Machinery, New\nY ork, NY , USA, ESEC/FSE 2022, pp 959–971\nXia CS, Zhang L (2023) Keep the conversation going: Fixing 162 out of 337 bugs for $0.42 each using chatgpt.\narXiv preprint arXiv:2304.00385\nXia CS, Wei Y , Zhang L (2022) Practical program repair in the era of large pre-trained language models. arXiv\npreprint arXiv:2210.14179\nXiong Y , Wang J, Y an R, Zhang J, Han S, Huang G, Zhang L (2017) Precise condition synthesis for program\nrepair. In: 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE), pp 416–426\nXiong Y , Liu X, Zeng M, Zhang L, Huang G (2018) Identifying patch correctness in test-based program\nrepair. In: Proceedings of the 40th International Conference on Software Engineering, Association for\nComputing Machinery, New Y ork, NY , USA, ICSE ’18, pp 789–799, https://doi.org/10.1145/3180155.\n3180182\nY ao S, Zhao J, Y u D, Du N, Shafran I, Narasimhan K, Cao Y (2022) React: Synergizing reasoning and acting\nin language models. arXiv preprint arXiv:2210.03629\nY ao S, Y u D, Zhao J, Shafran I, Grifﬁths TL, Cao Y , Narasimhan K (2023) Tree of thoughts: Deliberate\nproblem solving with large language models. 2305.10601\nZeller A (2009) Why programs fail: a guide to systematic debugging. Elsevier\nZhu Q, Sun Z, Y a Xiao, Zhang W, Y uan K, Xiong Y , Zhang L (2021) A Syntax-Guided Edit Decoder for\nNeural Program Repair. Association for Computing Machinery, New Y ork, NY , USA, pp 341–353\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and\ninstitutional afﬁliations.\n123\nEmpirical Software Engineering (2025) 30 :45 Page 27 of 28 45\nSungmin Kang received the Ph.D. degree under the supervision of\nProf. Shin Y oo. He is a postdoctoral researcher with Korea Advanced\nInstitute of Science and Technology. His research interests include\napplication of large language models to software engineering prob-\nlems, as well as a theoretic analyses of probabilistic techniques to\nsoftware engineering problems.\nBei Chen is a senior researcher at Microsoft. She received her Ph.D.\ndegree from the Department of Computer Science and Technology at\nTsinghua University, Beijing, China, in 2017. She is mainly working\non pre-trained language models and multimodal understanding, and\ntheir applications in code intelligence. She has published above 40\npapers in top conferences, including ICLR, NeurIPS, ACL, EMNLP ,\nKDD, AAAI, IJCAI, etc.\nShin Yoo received the Ph.D. degree under the supervision of Prof.\nMark Harman from King’s College London, in 2009. He is a Tenured\nAssociate Professor with Korea Advanced Institute of Science and\nTechnology, and worked as a Lecturer of software engineering with\nthe Centre for Research on Evolution, Search, and Testing, Univer-\nsity College London from 2012 to 2015 before joining KAIST. His\nresearch interests include search based software engineering, software\ntesting for AI systems, and the use of AI/ML for software testing as\nwell as automated debugging.\n123\n45 Page 28 of 28 Empirical Software Engineering (2025) 30 :45\nJian-Guang LOU is now a senior principal research manager at\nMicrosoft. His main research interests include data mining and AI for\nsoftware engineering, performance analysis and diagnosis of online\nservices, chatbot and agent based on large language models. Many of\nhis research results have been applied and deployed to the large-scale\nonline services in Microsoft.\n123"
}