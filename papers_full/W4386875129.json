{
    "title": "An Unified Search and Recommendation Foundation Model for Cold-Start Scenario",
    "url": "https://openalex.org/W4386875129",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2522100568",
            "name": "Yuqi Gong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2949962805",
            "name": "Xichen Ding",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5087981910",
            "name": "Yehui Su",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2227544295",
            "name": "Kaiming Shen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2111194444",
            "name": "Zhongyi Liu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2116104147",
            "name": "Guan-Nan Zhang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2740070748",
        "https://openalex.org/W2913340405",
        "https://openalex.org/W2951661358",
        "https://openalex.org/W4285294723",
        "https://openalex.org/W6637618735",
        "https://openalex.org/W4385568359",
        "https://openalex.org/W4290943593",
        "https://openalex.org/W4306317361",
        "https://openalex.org/W4224313506",
        "https://openalex.org/W2809290718",
        "https://openalex.org/W3209943551",
        "https://openalex.org/W4220907840",
        "https://openalex.org/W4377130978",
        "https://openalex.org/W3087931390",
        "https://openalex.org/W3203749733",
        "https://openalex.org/W3002414756",
        "https://openalex.org/W4212836229",
        "https://openalex.org/W3104763847",
        "https://openalex.org/W2810397803",
        "https://openalex.org/W2884789176",
        "https://openalex.org/W1731081199",
        "https://openalex.org/W4280586754",
        "https://openalex.org/W4303443398",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4362515116",
        "https://openalex.org/W3123878450",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4297969478",
        "https://openalex.org/W3208380962"
    ],
    "abstract": "In modern commercial search engines and recommendation systems, data from\\nmultiple domains is available to jointly train the multi-domain model.\\nTraditional methods train multi-domain models in the multi-task setting, with\\nshared parameters to learn the similarity of multiple tasks, and task-specific\\nparameters to learn the divergence of features, labels, and sample\\ndistributions of individual tasks. With the development of large language\\nmodels, LLM can extract global domain-invariant text features that serve both\\nsearch and recommendation tasks. We propose a novel framework called S\\\\&amp;R\\nMulti-Domain Foundation, which uses LLM to extract domain invariant features,\\nand Aspect Gating Fusion to merge the ID feature, domain invariant text\\nfeatures and task-specific heterogeneous sparse features to obtain the\\nrepresentations of query and item. Additionally, samples from multiple search\\nand recommendation scenarios are trained jointly with Domain Adaptive\\nMulti-Task module to obtain the multi-domain foundation model. We apply the\\nS\\\\&amp;R Multi-Domain foundation model to cold start scenarios in the\\npretrain-finetune manner, which achieves better performance than other SOTA\\ntransfer learning methods. The S\\\\&amp;R Multi-Domain Foundation model has been\\nsuccessfully deployed in Alipay Mobile Application's online services, such as\\ncontent query recommendation and service card recommendation, etc.\\n",
    "full_text": "An Unified Search and Recommendation Foundation Model for\nCold-Start Scenario\nYuqi Gong\nAnt Group\nBeijing, China\ngongyuqi.gyq@antgroup.com\nXichen Ding\nAnt Group\nBeijing, China\nxichen.dxc@antgroup.com\nYehui Su\nAnt Group\nBeijing, China\nsuyehui.syh@antgroup.com\nKaiming Shen\nAnt Group\nBeijing, China\nkaiming.skm@antgroup.com\nZhongyi Liu\nAnt Group\nHangzhou, China\nzhongyi.lzy@antgroup.com\nGuannan Zhang\nAnt Group\nHangzhou, China\nzgn138592@antgroup.com\nABSTRACT\nIn modern commercial search engines and recommendation sys-\ntems, data from multiple domains is available to jointly train the\nmulti-domain model. Traditional methods train multi-domain mod-\nels in the multi-task setting, with shared parameters to learn the\nsimilarity of multiple tasks, and task-specific parameters to learn\nthe divergence of features, labels, and sample distributions of indi-\nvidual tasks. With the development of large language models, LLM\ncan extract global domain-invariant text features that serve both\nsearch and recommendation tasks. We propose a novel framework\ncalled S&R Multi-Domain Foundation, which uses LLM to extract\ndomain invariant features, and Aspect Gating Fusion to merge the\nID feature, domain invariant text features and task-specific het-\nerogeneous sparse features to obtain the representations of query\nand item. Additionally, samples from multiple search and recom-\nmendation scenarios are trained jointly with Domain Adaptive\nMulti-Task module to obtain the multi-domain foundation model.\nWe apply the S&R Multi-Domain foundation model to cold start\nscenarios in the pretrain-finetune manner, which achieves better\nperformance than other SOTA transfer learning methods. The S&R\nMulti-Domain Foundation model has been successfully deployed in\nAlipay Mobile Application‚Äôs online services, such as content query\nrecommendation and service card recommendation, etc.\nCCS CONCEPTS\n‚Ä¢ Computing methodologies‚ÜíMachine learning algorithms;\n‚Ä¢ Information systems‚ÜíData mining; Retrieval models and\nranking.\nKEYWORDS\nsearch and recommendation, LLM, multi-domain recommendation\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nCIKM ‚Äô23, October 21‚Äì25, 2023, Birmingham, United Kingdom\n¬© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0124-5/23/10. . . $15.00\nhttps://doi.org/10.1145/3583780.3614657\nACM Reference Format:\nYuqi Gong, Xichen Ding, Yehui Su, Kaiming Shen, Zhongyi Liu, and Guannan\nZhang. 2023. An Unified Search and Recommendation Foundation Model for\nCold-Start Scenario. In Proceedings of the 32nd ACM International Conference\non Information and Knowledge Management (CIKM ‚Äô23), October 21‚Äì25, 2023,\nBirmingham, United Kingdom. ACM, New York, NY, USA, 7 pages. https:\n//doi.org/10.1145/3583780.3614657\n1 INTRODUCTION\nModern commercial recommendation systems and search engines\nare widely used in online service platforms such as YouTube, Tik-\nTok, Taobao, Alipay, etc. Search and recommendation can facilitate\nusers‚Äô behaviors to browse instant videos, buy products, use ser-\nvices, and make payments using E-wallets. The similarity between\nSearch (S) and Recommendation (R) makes jointly modeling S&R a\npromising research topic. Some work [17, 18] propose to enhance\nrecommendation by learning from a unified sequence of search\nand recommendation behaviors. Others [1, 14] propose to improve\npersonalized search by adding users‚Äô multi-interests in recommen-\ndation. Methods to model search and recommendation tasks jointly\nare also proposed in [21‚Äì24, 26]. JSR framework in [23] simultane-\nously learns retrieval and recommendation models with shared item\nset and optimizes a joint loss function. Researchers in [22] applied\ntwo-level transformer encoders, text encoder to learn documents\nand queries, session encoder to model the integrated sequence of\nsearch and browsing behaviors. [26] applied GNN to learn node em-\nbedding of user&item, and treat search query as a special attribute\nof edges in the graph. In industrial scenarios, there are several\nbenefits to model S and R jointly. First, there are multiple search\nand recommendation scenarios in a single mobile application. The\ntraining data collected in a single domain can‚Äôt fully reflect users‚Äô\ncomplete intents and is sub-optimal compared to modeling them\njointly. Secondly, majority of items are shared between search and\nrecommendation. Once users have impressions of any products,\nvideos, and services in a recommendation scenario, they should\nbe able to retrieve the item in Search later for repurchase or reuse\npurposes. Despite the similarity between Search and Recommen-\ndation, there are also difficulties in modeling them jointly, such\nas the data imbalance issue of multiple domains, the heteroge-\nneous issue of different item sets (videos, products, and services),\nthe negative transferring issue. With the latest developments in\nlarge language models (LLM) [2, 4, 5, 20, 27], the pretrain-finetune\nframework [7, 13, 18, 22, 26] greatly improves the performance of\narXiv:2309.08939v1  [cs.IR]  16 Sep 2023\nCIKM ‚Äô23, October 21‚Äì25, 2023, Birmingham, United Kingdom Yuqi Gong et al.\ndownstream tasks. We are inspired by the strong expressive power\nof natural language features and propose to build the Search and\nRecommendation Foundation model on top of LLMs, which ex-\ntract low-level domain-invariant text features of the query (Q) and\nitem (I). The major difference between our S&R foundation model\nand traditional multi-domain multi-task models is how we use the\ndomain-invariant text features to help constrain the divergence of\ndifferent tasks, which alleviates data imbalance, negative transfer-\nring, and item heterogeneous issues. To summarize, our proposed\nS&R Foundation model has the following key contributions:\n‚Ä¢We apply LLMs in S&R Multi-Domain Foundation model,\nand extract domain invariant text features to help mitigate\nthe negative transferring and item heterogeneous issues in\nthe multi-domain settings.\n‚Ä¢We novelly proposed the Aspect Gating Fusion (Domain-\nSpecific Gating) to fuse the ID feature, text features from\nLLMs, and sparse features. The Domain Adaptive Multi-Task\nmodule is also used to extract the domain-specific query and\nitem towers‚Äô representations.\n‚Ä¢For the cold start of new scenarios, we have conducted ex-\ntensive experiments both offline and online, to show the\neffectiveness of supervised fine-tuning of our S&R Founda-\ntion model in downstream tasks, which is now fully deployed\nonline and serving in Alipay‚Äôs mobile application.\n2 PROPOSED MODEL\n2.1 Problem Formulation\nGiven a set of ùêæ search and recommendation tasks {ùê∑ùëò}ùêæ\nùëò=1, ùê∑ùëò\ndenotes the dataset for the ùëò-th task. We let U= {ùë¢1,ùë¢2,...,ùë¢ ùëÅ}\ndenote the user set, I= {ùëñ1,ùëñ2,...,ùëñ ùëÄ}denote the item set and\nQ= {ùëû1,ùëû2,...,ùëû ùëá}denote the search query set. In real-world sce-\nnarios, items in search and recommendation usually come from\ndifferent domains and are heterogeneous. Some items are shared\nacross multiple domains and some items belong to each specific\ndomain. And we let I= I1 ‚à™I2 ‚à™... ‚à™Iùêæ denote the union of all\nitems in ùêæ domains, which contains ùëÄ items in total. We aim to\njointly train a search and recommendation (S&R) foundation model\nùëÄùëÜ&ùëÖ\nùêπùëúùë¢ùëõùëëùëéùë°ùëñùëúùëõ in the multi-task setting and predict the probability of\nuser ùë¢ùëô click the item ùëñùëô given input query ùëûùëô as ùëù(ùë¶ùëêùë°ùëü\nùëô = 1|ùë¢ùëô,ùëûùëô,ùëñùëô).\nAnd for search scenarios, additional query-item relevance score is\nalso predicted as ùëù(ùë¶ùë†ùëñùëö\nùëô = 1|ùëûùëô,ùëñùëô). For cold start of a new search or\nrecommendation scenario ùê∑‚àó, we restore parameters of embedding\ntables and partial network structures from the pretrained S&R foun-\ndation model ùëÄùëÜ&ùëÖ\nùêπùëúùë¢ùëõùëëùëéùë°ùëñùëúùëõ , and then apply supervised fine-tuning\non the downstream tasks, such as click through rate (CTR) predic-\ntion, query-item relevance prediction, etc. For the search task ùê∑ùëÜ\nùëò,\nwe letùê∑ùëÜ\nùëò = {ùë•ùëô = (ùë¢ùëô,ùëûùëô,ùëñùëô),ùë¶ùëô}ùëô, which denotes the search ranking\ntask given the triple input of (user, query, item) as(ùë¢ùëô,ùëûùëô,ùëñùëô). For the\nrecommendation task ùê∑ùëÖ\nùëò, we set search query set Q as emptyset ‚àÖ\nin ùê∑ùëÖ\nùëò = {ùë•ùëô = (ùë¢ùëô,ùëûùëô = ‚àÖ,ùëñùëô),ùë¶ùëô}ùëô.\n2.2 S&R Multi-Domain Foundation Model\nAs illustrated in Figure 1, the S&R Multi-Domain Foundation model\nhas three main components: the User-Query-Item encoding mod-\nule, the Aspect Gating Fusion module, and the Domain-Adaptive\nMulti-Task module. Firstly, raw features of user, query and item\npass through the embedding layers, and we extract the ID embed-\nding, token-level text embedding and sparse features‚Äô embedding.\nWe apply LLM to extract domain-invariant text features of query\nand item towers, which minimize the divergence of features‚Äô distri-\nbution cross multiple domains. Secondly, the Aspect Gating Fusion\nmodule is designed to merge different groups of ID, text, sparse\nfeatures‚Äô embedding. The fusion network is to balance the relative\nimportance of ID, text, and sparse features. Very few training sam-\nples contain ID features of cold start items and can‚Äôt represent them\nwell, and generic text features play more important role. Finally, we\nfeed the concatenated embedding of user, query and item towers to\nthe Domain Adaptive MTL module. The module has two outputs\nrepresenting the click through rate (CTR) prediction task and the\nquery-item relevance prediction task. The final loss function is the\nsum of CTR prediction loss Lùëêùë°ùëü, relevance prediction loss Lùë†ùëñùëö\nand domain adaptive regularization Lùëüùëíùëî.\n2.2.1 User Query and Item Encoding. We extract three towers for\nuser, query and item respectively. For the user tower, ùëíùêºùê∑ùë¢ ‚ààRùê∑\ndenotes user id embedding. ùëíùëÅùêª\nùë¢ = [ùë•1,...,ùë• ùë†,...,ùë• ùëÅùêª]denotes\nthe unified sequence of both search and recommendation clicks\nin chronological order. Each behavior ùë•ùë† is encoded as multiple\nlayers of MLPs with inputs of ID feature, sparse feature of be-\nhavior type S or R, and other sparse features of attributes, ùë•ùë† =\nùêπùê∂(ùëíùêºùê∑ùë† ‚äïùëíùë°ùë¶ùëùùëí\nùë† ‚äïùëíùëéùë°ùë°ùëüùë† ). For the query (ùëÑ) and item (ùêº) features,\nwe extract both domain-invariant text features, such as tokens in\nsearch query and items‚Äô title, and the domain-specific sparse fea-\ntures. The tokens of ùëÑand ùêº go through the same tokenizer and we\nget the tokenized id sequences as integer tensorsùëíùëáùëúùëòùëíùëõùëû and ùëíùëáùëúùëòùëíùëõ\nùëñ .\nùëíùëáùëúùëòùëíùëõùëû = [ùëí1ùëû,ùëí2ùëû,...,ùëí ùêøùëû\nùëû ]‚àà Rùêøùëû√óùê∑ denotes the query‚Äôs tokenized\nid tensor of length ùêøùëû, and ùëíùëáùëúùëòùëíùëõ\nùëñ = [ùëí1\nùëñ,ùëí2\nùëñ,...,ùëí ùêøùëñ\nùëñ ]‚àà Rùêøùëñ√óùê∑ de-\nnotes the item‚Äôs tokenized id tensor of length ùêøùëñ. For ID feature, we\nalso embed the search query as ID feature ùëíùêºùê∑ùëû ‚ààRùê∑, and item ID\nas ùëíùêºùê∑\nùëñ ‚ààRùê∑. For the sparse features, we embed sparse features of\nùëÑ as ùëíùëÜùëû and sparse features of ùêº as ùëíùëÜ\nùëñ . Finally, we get the feature\ngroups of query tower as ùëíùëû = [ùëíùêºùê∑ùëû ,ùëíùëáùëúùëòùëíùëõùëû ,ùëíùëÜùëû]and the feature\ngroups of item tower as ùëíùëñ = [ùëíùêºùê∑\nùëñ ,ùëíùëáùëúùëòùëíùëõ\nùëñ ,ùëíùëÜ\nùëñ ].\nLLM as Domain-Invariant Feature Extractor\nWe apply the pretrained Large Language Model, such as BERT\n[6], GPT [2], ChatGLM [8, 25], to extract domain-invariant text fea-\ntures on both query tower and item tower, represented asùúôùëôùëö(ùëÑ)=\nùúôùëôùëö(ùëíùëáùëúùëòùëíùëõùëû ) ‚ààRùêøùëû√óùê∑ and ùúôùëôùëö(ùêº)= ùúôùëôùëö(ùëíùëáùëúùëòùëíùëõ\nùëñ ) ‚ààRùêøùëñ√óùê∑. Af-\nter mean pooling of the encoding layer, followed by shared lin-\near projection, we get the domain-invariant text representation\nof query and item as ùê∏ùëôùëö(ùëÑ) = ùëäùëôùëö √óMEAN(ùúôùëôùëö(ùëíùëáùëúùëòùëíùëõùëû )) ‚àà\nùëÖùêª,ùê∏ùëôùëö(ùêº)= ùëäùëôùëö √óMEAN(ùúôùëôùëö(ùëíùëáùëúùëòùëíùëõ\nùëñ )) ‚ààùëÖùêª. ùëäùëôùëö ‚ààRùêª√óùê∑\ndenotes the linear projection layer shared between query tower\nand item tower, ùêª denotes the hidden size of the learned repre-\nsentations. The language models‚Äô representation is useful for cold\nstart scenarios, especially when we have few training samples to\nupdate the ID feature of new item ùëñ‚àóand new search query ùëû‚àó.\nWe also apply linear projections ùëäùëû\nùêºùê∑,ùëäùëñ\nùêºùê∑ ‚àà Rùêª√óùê∑ to ID fea-\nture of query and item tower, and get the ID representation of\nquery and item as ùê∏ùêºùê∑(ùëÑ),ùê∏ùêºùê∑(ùêº)‚àà Rùêª. For the sparse features we\nhave separate network (usually multiple layers of MLPs) to encode\nAn Unified Search and Recommendation Foundation Model for Cold-Start Scenario CIKM ‚Äô23, October 21‚Äì25, 2023, Birmingham, United Kingdom\nUser\nEmb Table\nTransformer\nUser Encoder\nID\nQuery Encoder\nQuery\nEmb Table\nItem Encoder\nLLM Sparse\nFusion\nID\nItem\nEmb Table\nLLM Sparse\nFusion\nEmb\nOutput A Output B\nDomain\nEmbedding\nDomain\nDivergence\nAspect Gating\nFusion\nDomain K\nDomain 2\nDomain 1\nExpert 1 Expert 2 Expert n\nID Sequence Sparse\nDomain Gating\n[CLS] Gating\nMean Gating\nDomain\nAdaptive MTL\nUser-Query-\nItem Encoding\nFigure 1: SR Multi-Domain Foundation Model Architecture\nquery and item as ùê∏ùëÜ(ùëÑ),ùê∏ùëÜ(ùêº)‚àà Rùêª. Finally, we get the feature\ngroups of query tower as [ùê∏ùêºùê∑(ùëÑ),ùê∏ùëôùëö(ùëÑ),ùê∏ùëÜ(ùëÑ)]and item tower\nas [ùê∏ùêºùê∑(ùêº),ùê∏ùëôùëö(ùêº),ùê∏ùëÜ(ùêº)].\n2.2.2 Aspect Gating Fusion. After low level networks ùêø0 (embed-\nding tables) and ùêø1 (feature encoding layers) in Figure 1, we fuse\ndifferent aspects of query and item as in literature [12]. Each aspect\nùê∏ùëé represents some fine-grained properties of query and item, such\nas ID, text and sparse features. Adenotes the set of aspects we\nextract from query and item. In S&R scenarios, we set |A|= 3 as\nID, text and sparse attributes. Final representations are fused as\nweighted sum of different aspects‚Äô representations.\nùê∏(ùëÑ)=\n‚àëÔ∏Å\nùëé\nùë§ùëé(ùëÑ)ùê∏ùëé(ùëÑ),ùê∏(ùêº)=\n‚àëÔ∏Å\nùëé\nùë§ùëé(ùêº)ùê∏ùëé(ùêº) ‚àÄùëé ‚àà|A|\nThe weight vector ùë§(ùëÑ),ùë§(ùêº)‚àà R|A| are outputs of a gating\nnetwork, and we have different strategies to design the network.\n‚Ä¢Mean-Gating StrategySimply mean pooling of different\naspects of query and item features as ùë§ùëé = 1\n|A|.\n‚Ä¢[CLS]-Gating StrategyWe use randomly initialized em-\nbedding ùê∏ùê∂ùêøùëÜ(ùëÑ),ùê∏ùê∂ùêøùëÜ(ùêº)‚àà Rùêª to represent classification\ntoken [CLS] of query and item respectively.\nùë§ùëé = ùëíùê∏ùê∂ùêøùëÜùê∏ùëé\n√ç\nùëé‚àà|A| ùëíùê∏ùê∂ùêøùëÜùê∏ùëé\n‚ààR|A|\n.\n‚Ä¢Domain-Gating Strategy\nWe design the domain gating strategy from the intuition\nthat the fusion network has different weights when merging\ndifferent aspects of query and item. To model the differ-\nences across domains, we randomly initialize the domain\nembedding ùê∏ùê∑ = [ùê∏ùê∑1 ,ùê∏ùê∑2 ,...,ùê∏ ùê∑ùêæ] ‚ààRùêæ√óùêª as the rep-\nresentations of different domains. And the domain-specific\ngating is calculated as\nùë§ùëé = ùëíùê∏ùê∑ùëòùê∏ùëé\n√ç\nùëé‚àà|A| ùëíùê∏ùê∑ùëòùê∏ùëé\n‚ààR|A|\n.\n2.2.3 Domain Adaptive Multi-Task Learning. The input to the Do-\nmain Adaptive Multi-Task module is the concatenation of represen-\ntations of user, query and item towers asx = ùê∏(ùëà)‚äïùê∏(ùëÑ)‚äïùê∏(ùêº). For\nmulti-domain setting, a series of multi-task and multi-domain mod-\nels are proposed, such as SharedBottom[ 3], MMoE[15], PLE[19],\nSTAR[16], SAMD[11], etc. These models use shared structures (Ex-\nperts or MLP layers) to model the similarity among different tasks or\ndomains, and use individual structures to learn the domain-specific\nproperties. The difficulty of training the multi-domain models is\nthe domain shift phenomena. For the k-th domainùê∑ùëò, the marginal\ndistribution of input feature ùëù(xùëò)and the conditional distribu-\ntion of predicting output ùë¶ùëò as ùëù(ùë¶ùëò|xùëò)has divergence from other\ndomains. The well studied MTL models handle the divergence of\nconditional distribution. We propose to add a Domain Adaptive\nLayer to the input features xùëñ, which maps the inputs from mul-\ntiple domains to a common vector space. We reuse the randomly\ninitialized domain embedding ùê∏ùê∑ = [ùê∏ùê∑1 ,ùê∏ùê∑2 ,...,ùê∏ ùê∑ùêæ]‚àà Rùêæ√óùêª\nin section 2.2.2 and concatenate the domain embedding ùê∏ùê∑ùëò to\nfeature vector xùëñ of instances from the k-th domain ùê∑ùëò, followed\nby domain-specific linear transformation ùëäùëò. Suppose xùëñ and xùëó\ndenote two instances from different domains in the same training\nbatch, we can get the domain-adaptive representation ÀÜxùëñ,ÀÜxùëó as\nCIKM ‚Äô23, October 21‚Äì25, 2023, Birmingham, United Kingdom Yuqi Gong et al.\nÀÜxùëñ = ùëäùëñ(xùëñ ‚äïùê∏ùê∑ùëñ),ÀÜxùëó = ùëäùëó(xùëó ‚äïùê∏ùê∑ùëó)\nWe apply domain adaptation [ 9] techniques to constrain the\ndivergence of distributions from domains ùëñ and ùëó as ùëù(ÀÜxùëñ)and\nùëù(ÀÜxùëó)as Lùëüùëíùëî = √ç\nùëñ,ùëó‚àà{1,2,...,ùêæ}ùëë(ùëù(ÀÜxùëñ)||ùëù(ÀÜxùëó)). In terms of diver-\ngence measurement, we compared different metrics such as Jensen-\nShannon Divergence (symmetric KL Divergence), Maximum Mean\nDiscrepancy (MMD) [10] in the experiment section. And we find\nthe Jensen-Shannon Divergence achieves the best performance as\nLùëüùëíùëî =\n‚àëÔ∏Å\nùëñ,ùëó‚àà{1,2,...,ùêæ}\nJS(ùëù(ÀÜxùëñ)||ùëù(ÀÜxùëó))\n.\nFinally, on top of the Domain Adaptive Layer we stack the stan-\ndard Multi-Task module, such as MMoE to extract outputs and\npredict two objectives, CTR prediction ùë¶ùëêùë°ùëü and query-item rele-\nvance prediction ùë¶ùë†ùëñùëö.\nCTR PredictionClick-Through Rate (CTR) Prediction is a com-\nmon task in both search and recommendation scenarios. We apply\na unified scoring function ùë¶ùëêùë°ùëü\nùëô = ùëìùúÉ(ùë¢ùëô,ùëûùëô,ùëñùëô)in the S&R founda-\ntion framework to predict CTR with the triple inputs of user, item\nand query as (ùë¢,ùëû,ùëñ ). For search tasks, users have explicit search\nquery ùëû. And for recommendation tasks users don‚Äôt have explicit\nintentions. So we set ùëû= ‚àÖas the default embedding in the unified\nscoring function.\nQuery-Item Relevance PredictionQuery-Item Relevance Pre-\ndiction is a common task in search scenarios, which predicts the\nrelevance score of query-item pair of (ùëû,ùëñ)and train a function\nùë¶ùë†ùëñùëö\nùëô = ùëìùúô(ùëûùëô,ùëñùëô)to represent query-item pair‚Äôs relevance score.\nThe relevance prediction is usually a classification task.\n2.2.4 Loss of S&R Foundation model. We train the S&R foundation\nmodel in multi-domain multi-task settings, using datasets from ùêæ\ndomains. Each domain calculates either or both of two objectives\nof CTR prediction ùë¶ùëêùë°ùëü\nùëô = ùëìùúÉ(ùë¢ùëô,ùëûùëô,ùëñùëô)and relevance prediction\nùë¶ùë†ùëñùëö\nùëô = ùëìùúô(ùëûùëô,ùëñùëô), depending on whether the task is search or rec-\nommendation. The final objective function consists of three parts,\nthe loss for CTR prediction Lùëêùë°ùëü, the loss for relevance prediction\nLùë†ùëñùëö, and the loss for domain adaptive regularizer Lùëüùëíùëî.\nL= Lùëêùë°ùëü +Lùë†ùëñùëö +Lùëüùëíùëî\nLùëêùë°ùëü =\n‚àëÔ∏Å\nùëò‚ààùêæ\n‚àëÔ∏Å\nùëô‚ààùëÅùëêùë°ùëü\nùëò\nLùëêùëí(ùëìùúÉ(ùë¢ùëô,ùëûùëô,ùëñùëô);ùë¶ùëêùë°ùëü\nùëô )\nLùë†ùëñùëö =\n‚àëÔ∏Å\nùëò‚ààùêæ\n‚àëÔ∏Å\nùëô‚ààùëÅùë†ùëñùëö\nùëò\nLùëêùëí(ùëìùúô(ùëûùëô,ùëñùëô);ùë¶ùë†ùëñùëö\nùëô )\n2.3 Supervised Fine-Tuning Downstream Tasks\nThe pretrained S&R foundation model can benefit downstream\ntasks in the pretrain-finetune manner. The downstream model re-\nstores parameters from the foundation model, freezes part of the\nparameters and finetunes the remaining layers. We experiment dif-\nferent ways of freeze-finetune split. Firstly, the freeze-finetune split\nis between level ùêø0 and ùêø1 as in Figure 1. The pretrained embedding\nin level ùêø0 is freezed and the remaining layers from ùêø1 to ùêøùëõ are\nfinetuned. Secondly, the freeze-finetune split is between level ùêø1\nand ùêø2. The embedding in level ùêø0 as well as the parameters of\nencoding layers in level ùêø1 are freezed, and the parameters from\nlevel ùêø2 to ùêøùëõ are finetuned. Given dataset of new downstream task\nùê∑‚àó = {(ùë¢‚àó\nùëô,ùëû‚àó\nùëô,ùëñ‚àó\nùëô),ùë¶‚àó\nùëô }, the domain embedding ùê∏ùê∑‚àó ‚ààRùêª is ran-\ndomly initialized and finetuned. In the experiment section, we thor-\noughly tested the performance of different ways of freeze-finetune\nsplit. We also compared the performance of pretrain-finetuning S&R\nFoundation model ùëÄùëÜ&ùëÖ\nùêπùëúùë¢ùëõùëëùëéùë°ùëñùëúùëõ with the performance of training\nsingle domain model without transfer learning.\n3 EXPERIMENT\nTo test the effectiveness of our proposed S&R Multi-Domain Foun-\ndation model, we want to answer the following questions:\n‚Ä¢RQ1: Whether our joint S&R Multi-Domain Foundation\nmodel can achieve SOTA performance compared to other\nmulti-domain and multi-task models?\n‚Ä¢RQ2: In terms of query and item towers‚Äô representations,\nwhat‚Äôs the performance of the domain-invariant text fea-\ntures extracted by LLM and Aspect Gating Fusion network\ncompared to other methods?\n‚Ä¢RQ3: Whether S&R Multi-Domain Foundation and Super-\nvised Finetuning can help benefit cold start scenarios?\n3.1 Experimental Settings\n3.1.1 Dataset. We conducted extensive experiments of S&R Foun-\ndation model on real-world datasets, including 7 industrial datasets\nof Alipay Search Ranking and Query Recommendation. The sta-\ntistics are summarized in table 1. ùëÜ denotes the search dataset, in\nwhich users have explicit search query, such as Query-Item Rele-\nvance Prediction, Content Search Ranking, etc. And ùëÖdenotes the\nrecommendation dataset, in which users don‚Äôt have explicit intent\nof search query. There are also some tasks between Search and\nRecommendation, which we classify as S/R, such as Query Suggest\nCTR Prediction, in which users have explicit query, and at the same\ntime the task is a CTR prediction task to make recommendation of\nquery suggestions to users.\n3.1.2 Comparison Methods. S&R Foundation ModelWe com-\npared our proposed S&R Multi-Domain Foundation model with\nSOTA multi-domain and multi-task models, such as Shared Bot-\ntom MTL [ 3], Multi-Gate Mixture of Experts (MMoE) [ 15], PLE\n[19], etc. For ablation study, we designed separate experiments to\nevaluate different modules of the framework, including the User-\nQuery-Item encoding module, Aspect Gating Fusion module and\nDomain Adaptive Multi Task module. The experiment of S&R Multi-\nDomain Foundation (MLP) denotes the concatenated user-query-\nitem representations are followed by multiple MLP layers. And\nthe experiment of S&R Multi-Domain Foundation-MMoE-DA-JS\ndenotes the representations are followed by a Domain Adaptive\nLayer (JS-Divergence) and MMoE multi-task module.\nDomain-Invariant Text Features and Aspect Gating Fu-\nsion To prove the effectiveness of adding domain-invariant text\nfeatures in S&R Foundation model, we have conducted experiments\nand ablation studies on different query and item token encoding\nmethods on Alipay Content Query Recommendation dataset of\ntasks 4 in table 1. In the baseline method, we intentionally leave out\nthe token-embedding of text features and only use ID and sparse fea-\ntures. We also compared randomly initialized token embedding with\nAn Unified Search and Recommendation Foundation Model for Cold-Start Scenario CIKM ‚Äô23, October 21‚Äì25, 2023, Birmingham, United Kingdom\nTable 1: Statistics of Alipay Search Ranking and Query Recommendation Datasets.\nID Dataset S/R Train Eval Test #Query #Item\nTask 1 Query-Item Relevance Prediction S 76.2M 12.7M 12.7M 40K 40K\nTask 2 Query Suggest CTR Prediction S/R 145.4M 23.5M 23.5M 0.84M 0.16M\nTask 3 Background Word Query Recommendation CTR Prediction R 146.2M 24.3M 24.3M - 65K\nTask 4 Content Query Recommendation CTR Prediction R 0.76M 0.09M 0.09M - 4.6K\nTask 5 People Also Ask DeepSuggest S/R 2.4M 0.38M 0.38M 0.41M 25K\nTask 6 Service Card Recommendation S/R 1.01M 0.17M 0.17M 1.3K 1.6K\nTask 7 Content Search Ranking S 6.13M 1.03M 1.03M 0.27M 0.14M\nTable 2: Performance of S&R Multi-Domain Foundation Model.\nMethod Task1 Task2 Task3 Task4 Task5 Task6 Task7\nS&R Multi-Domain Shared Bottom [3] 0.6483 0.8993 0.7829 0.6575 0.8511 0.8015 0.8561\nS&R Multi-Domain MMoE [15] 0.6482 *0.9003 0.7812 0.6650 0.8463 0.7942 0.8599\nS&R Multi-Domain PLE [19] *0.7006 0.8981 0.7815 0.6682 0.8487 0.7978 0.8620\nS&R Multi-Domain Foundation (MLP) 0.6827 0.8974 0.7784 0.6683 0.8462 0.7926 0.8629\nS&R Multi-Domain Foundation-MMoE-DA-MMD 0.6874 0.8942 *0.7971 0.6793 0.8564 0.8203 0.8569\nS&R Multi-Domain Foundation-MMoE-DA-JS 0.6942 0.8973 0.7912 *0.6979 *0.8703 *0.8312 *0.8692\nAbsolute Improvement +0.0459 -0.0020 +0.0083 +0.0404 +0.0192 +0.0297 +0.0131\nTable 3: Comparison of Query and Item Token Encoding Methods after Fine-tuning Task 4.\nID Token Embedding Query/Item Encoder Finetune AUC\n1 Baseline: Without Token Emb - - 0.7524\n2 Randomly Initialized Mean Pooling - 0.7551\n3 Randomly Initialized Transfomer(L=1) True 0.7544\n4 Randomly Initialized Transfomer(L=6) True 0.7559\n5 SR Foundation (LM=Transformer) Transfomer(L=1) ùêø0,ùêø1:True 0.7562\n6 SR Foundation (LM=Transformer) Transfomer(L=1) ùêø0:False,ùêø1:True 0.7531\n7 SR Foundation (LM=Transformer) Transfomer(L=1) ùêø0,ùêø1:False 0.7574\n8 SR Foundation (LM=BERT) BERT BASE(L=12) True 0.7563\n9 SR Foundation (LM=BERT) BERT BASE(L=12) False *0.7580\n10 SR Foundation (LM=ChatGLM 6B) ChatGLM 6B Pretrained LLM [8, 25] False 0.7518\n11 SR Foundation (LM=ChatGLM 6B) ChatGLM 6B Pretrained LLM [8, 25] + prompt False 0.7503\n12 SR Foundation (LM=ChatGLM2 6B) ChatGLM2 Pretrained LLM [8, 25] False 0.7502\nAbsolute Improvement - - +0.0056\n30\n 20\n 10\n 0 10 20 30\n10\n0\n10\n20\nS&R Foundation-MLP\ntask 1\ntask 2\ntask 3\ntask 4\ntask 5\ntask 6\ntask 7\n60\n 40\n 20\n 0 20 40\n60\n40\n20\n0\n20\n40\nS&R Foundation-MMoE-DA-MMD\ntask 1\ntask 2\ntask 3\ntask 4\ntask 5\ntask 6\ntask 7\n30\n 20\n 10\n 0 10 20 30\n30\n20\n10\n0\n10\n20\n30\n40\nS&R Foundation-MMoE-DA-JS\ntask 1\ntask 2\ntask 3\ntask 4\ntask 5\ntask 6\ntask 7\nFigure 2: Visualization of SR Foundation Model‚Äôs Domain-Adaptive Layers\nTable 4: Comparison of Aspect Gating Fusion on Task 4.\nMethod AUC Absolute Gain\nMean-Pooling 0.7385 -\n[CLS]-Gating 0.7515 +0.0130\nDomain-Gating *0.7524 +0.0139\nTable 5: Comparison of Cold Start Scenarios Task 4 and 6.\nService Card Rec Content Query Rec\nSingle Domain 0.8229 0.7295\nSR Fdt->Finetune 0.8446 0.7574\nAbsolute Improvement +0.0216 +0.0279\nCIKM ‚Äô23, October 21‚Äì25, 2023, Birmingham, United Kingdom Yuqi Gong et al.\n1 2 3 4 5 6 7\nDays\n0.024\n0.026\n0.028\n0.030\n0.032\n0.034PVCTR\nOnline A/B T esting Service Card Recommendation PVCTR\nBaseline\nSR Foundation-Finetune\nFigure 3: Online AB Testing PVCTR Performance of Service\nCard Recommendation.\nembedding restored from the pretrained S&R foundation model\nunder different configurations. For the encoders, we compared\nmean pooling, randomly initialized Transformer, BERT, ChatGLM-\n6B[8, 25], and ChatGLM+prompt, etc. In methods 10-12, we adopt\nChatGLM-6B and ChatGLM2-6B to encode the text features of\nquery and items. The implementation details are: we utilized the en-\ncoders of ChatGLM-6B and ChatGLM2-6B to convert the input text\nfeatures and corresponding prompts into 4096-dimensional vectors,\nwhich are followed by 2 MLP dense layers and further reduced to\n32-dimensional vectors. The prompt function used in our approach\nis defined as ùëìùëùùëüùëúùëöùëùùë° (ùëã)= ‚ÄùExtract keywords from sentence [X]‚Äù.\nTo compare different Aspect Gating Fusion methods, e.g. Mean-\nPooling, [CLS]-Gating, Domain-Gating, we conducted ablation stud-\nies and the results are listed in table 4.\nSupervised Fine-Tuning on Cold Start ScenariosFor cold\nstart scenarios, we compared the performance of supervised fine-\ntuning (SFT) the foundation model using downstream dataset, with\nthe method of training single domain model on several tasks, in-\ncluding Service Card Recommendation (task 6) and Content Query\nRecommendation (task 4) as in table 5.\n3.2 Experimental Results\n3.2.1 S&R Multi-Domain Foundation model. To compare the per-\nformance of different multi-domain models, we report AUC perfor-\nmance on 7 search and recommendation datasets in table 2. All the\nexperimented models share same input features, User-Query-Item\nEncoding module, and Domain-Gating as Aspect Gating Fusion\nstrategy. The baseline for the multi-task learning (MTL) module\nis the shared bottom model. MMoE-DA-MMD and MMoE-DA-JS\nrepresent models that utilize Maximum Mean Discrepancy (MMD)\nand Jensen‚ÄìShannon Divergence (JS-Divergence) to constrain the\ndistributions of domain adaptive layers respectively. The asterisk (*)\ndenotes the best performance achieved in each task, and the abso-\nlute improvement represents the absolute improvement of MMoE-\nDA-JS method compared to baseline. MMoE-DA-JS achieved best\nperformance on 4 tasks: 4, 5, 6, 7 with AUC improvement of +0.0404,\n+0.0192, +0.0297, +0.0131 respectively. The domain-adaptive layer\nconstrains the embedding representations from different domains\nin the common vector space. The t-SNE visualization of S&R Foun-\ndation model‚Äôs domain-adaptive layers is depicted in Figure 2. The\nembedding depicted in the first subplot \"S&R Foundation MLP\" is\nscattered, and the embedding in the third subplot \"S&R Foundation-\nMMoE-DA-JS\" is coherently aligned.\n3.2.2 Domain-Invariant Text Features and Aspect Gating Fusion.\nWe report the performance of different methods to encode domain-\ninvariant text features and freeze-finetune split in table 3 on task 4\nContent Query Recommendation. Our proposed method of restor-\ning pretrained parameters from BERT BASE (12 layers Transformer)\nin S&R Foundation, freezing the parameters of the encoder and\nfinetuning the remaining networks achieves the best AUC perfor-\nmance 0.7580, which is 0.0056 absolute gain over baseline model.\nComparing different freeze-finetune split (methods 5-7), we can see\nthat freezing pretrained parameters in level ùêø0 and ùêø1 (method 7)\nachieves better performance than other split methods (method 5/6),\nwhich is 0.0043 absolute gain in AUC. As for the ablation studies\nof Aspect Gating Fusion in table 4, the baseline is to simply mean\npooling three aspects: ID, text and sparse features. We can see the\nDomain-Gating achieves best AUC performance 0.7524, which is\n0.0139 absolute gain over mean-pooling method.\n3.2.3 Supervised Finetuning in Cold Start Scenarios. To prove the\neffectiveness of finetuning our pretrained S&R Foundation model,\nwe compared cold start performance of two scenarios, Service Card\nRecommendation (task 6) and Content Query Recommendation\n(task 4). They are new scenarios and we only collected a few samples\nin a short period of time. The samples are splitted as we leave out\nlast one day‚Äôs collected data for testing, and use the remaining data\nfor fine-tuning the S&R Foundation. We also train the single domain\nmodel as the baseline. From table 5, we can see the fine-tuned S&R\nFoundation model achieves +0.0216 AUC improvement over single\ndomain model on task 6 and +0.0279 AUC improvement on task 4.\n3.2.4 Online AB Testing. To further prove the effectiveness of on-\nline performance in cold start scenario, we deployed the fine-tuned\nS&R Foundation model online in Service Card Recommendation\nscenario, and compared with baseline, which is the single domain\nDNN model. The results of the AB Testing from day 1 to day 7\nare depicted in Figure 3. The key performance measurement of the\ncold start scenario is PVCTR (Page View Click Through Rate). And\nwe observed that the fine-tuned S&R Foundation model achieved\n+17.54% relative gain in PVCTR over baseline. The online AB Test-\ning results showed that our method achieved better performance\nthan baseline consistently in cold start scenario.\n4 CONCLUSION\nIn this paper, we study the problem of training search and rec-\nommendation tasks jointly as the S&R Multi-Domain Foundation\nmodel, and use domain adaptation techniques to benefit cold start\nscenario. Our proposed model learns user, query and item repre-\nsentations, applies LLM to encode domain invariant text features\nand Aspect Gating Fusion to merge ID, text and sparse features. We\nalso conducted extensive experiments on finetuning the foundation\nmodels in cold start scenarios, which achieves better performance\nthan the single domain model. The fine-tuned S&R Multi-Domain\nFoundation model has been successfully deployed online in Alipay‚Äôs\nmultiple search and recommendation scenarios.\nAn Unified Search and Recommendation Foundation Model for Cold-Start Scenario CIKM ‚Äô23, October 21‚Äì25, 2023, Birmingham, United Kingdom\nREFERENCES\n[1] Qingyao Ai, Yongfeng Zhang, Keping Bi, Xu Chen, and W. Bruce Croft. 2017.\nLearning a Hierarchical Embedding Model for Personalized Product Search. In\nProceedings of the 40th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (Shinjuku, Tokyo, Japan) (SIGIR ‚Äô17) . As-\nsociation for Computing Machinery, New York, NY, USA, 645‚Äì654. https:\n//doi.org/10.1145/3077136.3080813\n[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.\nIn Advances in Neural Information Processing Systems 33: Annual Conference on\nNeural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual, Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina\nBalcan, and Hsuan-Tien Lin (Eds.).\n[3] Rich Caruana. 1997. Multitask Learning. Mach. Learn. 28, 1 (1997), 41‚Äì75.\n[4] Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. M6-\nRec: Generative Pretrained Language Models are Open-Ended Recommender\nSystems. CoRR abs/2205.08084 (2022).\n[5] Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. M6-\nRec: Generative Pretrained Language Models are Open-Ended Recommender\nSystems. arXiv:2205.08084 [cs.IR]\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language Technologies, NAACL-HLT\n2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , Jill\nBurstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computa-\ntional Linguistics, 4171‚Äì4186.\n[7] Xichen Ding, Jie Tang, Tracy Liu, Cheng Xu, Yaping Zhang, Feng Shi, Qixia\nJiang, and Dan Shen. 2019. Infer Implicit Contexts in Real-Time Online-to-\nOffline Recommendation. In Proceedings of the 25th ACM SIGKDD International\nConference on Knowledge Discovery Data Mining (Anchorage, AK, USA) (KDD\n‚Äô19). Association for Computing Machinery, New York, NY, USA, 2336‚Äì2346.\nhttps://doi.org/10.1145/3292500.3330716\n[8] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and\nJie Tang. 2022. GLM: General Language Model Pretraining with Autoregressive\nBlank Infilling. In Proceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) . 320‚Äì335.\n[9] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo\nLarochelle, Fran√ßois Laviolette, Mario Marchand, and Victor Lempitsky. 2016.\nDomain-Adversarial Training of Neural Networks. J. Mach. Learn. Res. 17, 1 (jan\n2016), 2096‚Äì2030.\n[10] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Sch√∂lkopf, and\nAlexander Smola. 2012. A Kernel Two-Sample Test. J. Mach. Learn. Res. 13, null\n(mar 2012), 723‚Äì773.\n[11] Zhaoxin Huan, Ang Li, Xiaolu Zhang, Xu Min, Jieyu Yang, Yong He, and\nJun Zhou. 2023. SAMD: An Industrial Framework for Heterogeneous Multi-\nScenario Recommendation. In Proceedings of the 29th ACM SIGKDD Confer-\nence on Knowledge Discovery and Data Mining (Long Beach, CA, USA) (KDD\n‚Äô23). Association for Computing Machinery, New York, NY, USA, 4175‚Äì4184.\nhttps://doi.org/10.1145/3580305.3599955\n[12] Weize Kong, Swaraj Khadanga, Cheng Li, Shaleen Kumar Gupta, Mingyang\nZhang, Wensong Xu, and Michael Bendersky. 2022. Multi-Aspect Dense Retrieval.\nIn Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and\nData Mining (Washington DC, USA) (KDD ‚Äô22) . Association for Computing\nMachinery, New York, NY, USA, 3178‚Äì3186. https://doi.org/10.1145/3534678.\n3539137\n[13] Ningning Li, Qunwei Li, Xichen Ding, Shaohu Chen, and Wenliang Zhong. 2022.\nPrototypical Contrastive Learning and Adaptive Interest Selection for Candidate\nGeneration in Recommendations. In Proceedings of the 31st ACM International\nConference on Information and Knowledge Management (Atlanta, GA, USA)(CIKM\n‚Äô22). Association for Computing Machinery, New York, NY, USA, 4183‚Äì4187.\nhttps://doi.org/10.1145/3511808.3557674\n[14] Jiongnan Liu, Zhicheng Dou, Qiannan Zhu, and Ji-Rong Wen. 2022. A Category-\nAware Multi-Interest Model for Personalized Product Search. In Proceedings\nof the ACM Web Conference 2022 (Virtual Event, Lyon, France) (WWW ‚Äô22) .\nAssociation for Computing Machinery, New York, NY, USA, 360‚Äì368. https:\n//doi.org/10.1145/3485447.3511964\n[15] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H. Chi. 2018.\nModeling Task Relationships in Multi-task Learning with Multi-gate Mixture-\nof-Experts. In Proceedings of the 24th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, KDD 2018, London, UK, August 19-23, 2018 .\nACM, 1930‚Äì1939.\n[16] Xiang-Rong Sheng, Liqin Zhao, Guorui Zhou, Xinyao Ding, Binding Dai, Qiang\nLuo, Siran Yang, Jingshan Lv, Chi Zhang, Hongbo Deng, and Xiaoqiang Zhu.\n2021. One Model to Serve All: Star Topology Adaptive Recommender for Multi-\nDomain CTR Prediction. In CIKM ‚Äô21: The 30th ACM International Conference on\nInformation and Knowledge Management, Virtual Event, Queensland, Australia,\nNovember 1 - 5, 2021 , Gianluca Demartini, Guido Zuccon, J. Shane Culpepper,\nZi Huang, and Hanghang Tong (Eds.). ACM, 4104‚Äì4113.\n[17] Zihua Si, Xueran Han, Xiao Zhang, Jun Xu, Yue Yin, Yang Song, and Ji-Rong\nWen. 2022. A Model-Agnostic Causal Learning Framework for Recommendation\nUsing Search Data. In Proceedings of the ACM Web Conference 2022 (Virtual Event,\nLyon, France) (WWW ‚Äô22) . Association for Computing Machinery, New York,\nNY, USA, 224‚Äì233. https://doi.org/10.1145/3485447.3511951\n[18] Zihua Si, Zhongxiang Sun, Xiao Zhang, Jun Xu, Xiaoxue Zang, Yang Song, Kun\nGai, and Ji-Rong Wen. 2023. When Search Meets Recommendation: Learning Dis-\nentangled Search Representation for Recommendation. In Proceedings of the 46th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval (Taipei, Taiwan) (SIGIR ‚Äô23) . Association for Computing Machinery,\nNew York, NY, USA, 1313‚Äì1323. https://doi.org/10.1145/3539618.3591786\n[19] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progres-\nsive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for\nPersonalized Recommendations. In RecSys 2020: Fourteenth ACM Conference on\nRecommender Systems, Virtual Event, Brazil, September 22-26, 2020 . ACM, 269‚Äì278.\n[20] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur√©lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lam-\nple. 2023. LLaMA: Open and Efficient Foundation Language Models. CoRR\nabs/2302.13971 (2023).\n[21] Jun Xu, Xiangnan He, and Hang Li. 2018. Deep Learning for Matching in Search\nand Recommendation. In The 41st International ACM SIGIR Conference on Re-\nsearch & Development in Information Retrieval (Ann Arbor, MI, USA) (SIGIR\n‚Äô18). Association for Computing Machinery, New York, NY, USA, 1365‚Äì1368.\nhttps://doi.org/10.1145/3209978.3210181\n[22] Jing Yao, Zhicheng Dou, Ruobing Xie, Yanxiong Lu, Zhiping Wang, and Ji-Rong\nWen. 2021. USER: A Unified Information Search and Recommendation Model\nBased on Integrated Behavior Sequence. In Proceedings of the 30th ACM Inter-\nnational Conference on Information and Knowledge Management (Virtual Event,\nQueensland, Australia) (CIKM ‚Äô21) . Association for Computing Machinery, New\nYork, NY, USA, 2373‚Äì2382. https://doi.org/10.1145/3459637.3482489\n[23] Hamed Zamani and W. Bruce Croft. 2018. Joint Modeling and Optimization of\nSearch and Recommendation. In Proceedings of the First Biennial Conference on\nDesign of Experimental Search & Information Retrieval Systems, Bertinoro, Italy,\nAugust 28-31, 2018 (CEUR Workshop Proceedings, Vol. 2167) , Omar Alonso and\nGianmaria Silvello (Eds.). CEUR-WS.org, 36‚Äì41.\n[24] Hamed Zamani and W. Bruce Croft. 2020. Learning a Joint Search and Recom-\nmendation Model from User-Item Interactions. In WSDM ‚Äô20: The Thirteenth\nACM International Conference on Web Search and Data Mining, Houston, TX, USA,\nFebruary 3-7, 2020 , James Caverlee, Xia (Ben) Hu, Mounia Lalmas, and Wei Wang\n(Eds.). ACM, 717‚Äì725.\n[25] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding,\nZhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open\nbilingual pre-trained model. arXiv preprint arXiv:2210.02414 (2022).\n[26] Kai Zhao, Yukun Zheng, Tao Zhuang, Xiang Li, and Xiaoyi Zeng. 2022. Joint\nLearning of E-Commerce Search and Recommendation with a Unified Graph\nNeural Network. In Proceedings of the Fifteenth ACM International Conference on\nWeb Search and Data Mining (Virtual Event, AZ, USA) (WSDM ‚Äô22) . Association\nfor Computing Machinery, New York, NY, USA, 1461‚Äì1469. https://doi.org/10.\n1145/3488560.3498414\n[27] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang,\nYushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang,\nZikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A Survey of Large\nLanguage Models. CoRR abs/2303.18223 (2023)."
}