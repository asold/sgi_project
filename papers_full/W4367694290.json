{
  "title": "TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation",
  "url": "https://openalex.org/W4367694290",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2360607594",
      "name": "Bao, Keqin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2226268113",
      "name": "Zhang Ji-zhi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1999911198",
      "name": "Zhang Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1462929032",
      "name": "Wang Wenjie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3132652808",
      "name": "Feng, Fuli",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2329433866",
      "name": "He, Xiangnan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4362707064",
    "https://openalex.org/W4280586754",
    "https://openalex.org/W3156939347",
    "https://openalex.org/W4376633008",
    "https://openalex.org/W4365211555",
    "https://openalex.org/W4288254514",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4360612299",
    "https://openalex.org/W2137063737",
    "https://openalex.org/W4290944002",
    "https://openalex.org/W2902040508",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4293790978",
    "https://openalex.org/W4385430559",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W4377866047",
    "https://openalex.org/W1985854669",
    "https://openalex.org/W4389520758",
    "https://openalex.org/W2949448917",
    "https://openalex.org/W4361193179",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3004578093",
    "https://openalex.org/W4384625746",
    "https://openalex.org/W3206127589",
    "https://openalex.org/W4224313077",
    "https://openalex.org/W4284709839",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W2155912844",
    "https://openalex.org/W3156622960",
    "https://openalex.org/W4362706980",
    "https://openalex.org/W4224317071",
    "https://openalex.org/W2750004028",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4299286960",
    "https://openalex.org/W3100480425",
    "https://openalex.org/W3133849783",
    "https://openalex.org/W4285069854",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3097021616",
    "https://openalex.org/W4319653585",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W4312205996",
    "https://openalex.org/W3034896171",
    "https://openalex.org/W2900841426",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2171279286",
    "https://openalex.org/W4367191003",
    "https://openalex.org/W2990221466",
    "https://openalex.org/W2966483207",
    "https://openalex.org/W4389158474",
    "https://openalex.org/W2963367478",
    "https://openalex.org/W2783272285",
    "https://openalex.org/W2964296635",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W2054141820",
    "https://openalex.org/W4313680149",
    "https://openalex.org/W4385638369",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4226155321",
    "https://openalex.org/W4384648324",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4323572061"
  ],
  "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across diverse domains, thereby prompting researchers to explore their potential for use in recommendation systems. Initial attempts have leveraged the exceptional capabilities of LLMs, such as rich knowledge and strong generalization through In-context Learning, which involves phrasing the recommendation task as prompts. Nevertheless, the performance of LLMs in recommendation tasks remains suboptimal due to a substantial disparity between the training tasks for LLMs and recommendation tasks, as well as inadequate recommendation data during pre-training. To bridge the gap, we consider building a Large Recommendation Language Model by tunning LLMs with recommendation data. To this end, we propose an efficient and effective Tuning framework for Aligning LLMs with Recommendation, namely TALLRec. We have demonstrated that the proposed TALLRec framework can significantly enhance the recommendation capabilities of LLMs in the movie and book domains, even with a limited dataset of fewer than 100 samples. Additionally, the proposed framework is highly efficient and can be executed on a single RTX 3090 with LLaMA-7B. Furthermore, the fine-tuned LLM exhibits robust cross-domain generalization. Our code and data are available at https://github.com/SAI990323/TALLRec.",
  "full_text": "TALLRec: An Effective and Efficient Tuning Framework to Align\nLarge Language Model with Recommendation\nKeqin Bao*\nbaokq@mail.ustc.edu.cn\nUniversity of Science and Technology\nof China\nChina\nJizhi Zhang*\ncdzhangjizhi@mail.ustc.edu.cn\nUniversity of Science and Technology\nof China\nChina\nYang Zhang\nzy2015@mail.ustc.edu.cn\nUniversity of Science and Technology\nof China\nChina\nWenjie Wang\nwenjiewang96@gmail.com\nNational University of Singapore\nSingapore\nFuli Feng‚Ä†\nfulifeng93@gmail.com\nUniversity of Science and Technology\nof China\nChina\nXiangnan He‚Ä†\nxiangnanhe@gmail.com\nUniversity of Science and Technology\nof China\nChina\nABSTRACT\nLarge Language Models (LLMs) have demonstrated remarkable per-\nformance across diverse domains, thereby prompting researchers to\nexplore their potential for use in recommendation systems. Initial at-\ntempts have leveraged the exceptional capabilities of LLMs, such as\nrich knowledge and strong generalization through In-context Learn-\ning, which involves phrasing the recommendation task as prompts.\nNevertheless, the performance of LLMs in recommendation tasks\nremains suboptimal due to a substantial disparity between the train-\ning tasks for LLMs and recommendation tasks, as well as inadequate\nrecommendation data during pre-training. To bridge the gap, we\nconsider building a Large Recommendation Language Model by\ntunning LLMs with recommendation data. To this end, we propose\nan efficient and effective Tuning framework for Aligning LLMs with\nRecommendations, namely TALLRec. We have demonstrated that\nthe proposed TALLRec framework can significantly enhance the rec-\nommendation capabilities of LLMs in the movie and book domains,\neven with a limited dataset of fewer than 100 samples. Additionally,\nthe proposed framework is highly efficient and can be executed on\na single RTX 3090 with LLaMA-7B. Furthermore, the fine-tuned\nLLM exhibits robust cross-domain generalization. Our code and\ndata are available at https://github.com/SAI990323/TALLRec.\nCCS CONCEPTS\n‚Ä¢ Information systems ‚ÜíRecommender systems.\nKEYWORDS\nRecommendation, Instruction Tuning, Large Language Models\n*The two authors contributed equally to this work and are listed alphabetically. ‚Ä†The\ncorresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nRecSys ‚Äô23, September 18‚Äì22, 2023, Singapore, Singapore\n¬© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0241-9/23/09. . . $15.00\nhttps://doi.org/10.1145/3604915.3608857\nACM Reference Format:\nKeqin Bao*, Jizhi Zhang*, Yang Zhang, Wenjie Wang, Fuli Feng‚Ä†, and Xi-\nangnan He‚Ä†. 2023. TALLRec: An Effective and Efficient Tuning Frame-\nwork to Align Large Language Model with Recommendation. In Seven-\nteenth ACM Conference on Recommender Systems (RecSys ‚Äô23), September\n18‚Äì22, 2023, Singapore, Singapore. ACM, New York, NY, USA, 8 pages. https:\n//doi.org/10.1145/3604915.3608857\n1 INTRODUCTION\nLarge Language Models (LLMs) have exhibited remarkable profi-\nciency in generating text that closely resembles human language\nand in performing a wide range of tasks [69], including Natural Lan-\nguage Processing [4, 32, 54], Robotics [10, 43, 56], and Information\nRetrieval [1, 24, 25, 48]. Prior research has also demonstrated the\nknowledge-rich and compositional generalization capabilities of\nLLMs [36, 41, 53]. Only given appropriate instructions, these mod-\nels are able to learn how to solve unseen tasks and inspire their own\nknowledge to achieve a high level of performance [33]. The afore-\nmentioned capabilities of LLM present promising opportunities\nto address the current challenges requiring strong generalization\nand rich knowledge in the recommendation field. In this light, it\nis valuable to explore the integration of LLMs into recommender\nsystems, which has received limited attention in prior research.\nIn recent initial attempts [13, 47], achieving the target relies on\nIn-context Learning [3], which is typically implemented through the\nofficial OpenAI API [2]. They regard the LLM as a toolformer [42]\nof traditional recommendation models (such as MF [27] and Light-\nGCN [16]), i.e., the LLM is used for re-ranking the candidate items\nfiltered by these models. However, these approaches only reach a\ncomparable performance with traditional models [13, 47]. Worse\nstill, using only In-context Learning may fail to make recommen-\ndations. As shown in Figure 1, we find that ChatGPT either refuses\nto answer or always gives positive predictions (likes). Therefore, it\nis critical to further explore an appropriate way for more effective\nleverage of LLMs in the recommendation.\nWe postulate that the failure of using only In-context Learn-\ning is because of two reasons: 1) LLMs may not align well with\nthe recommendation task due to the huge gap between language\nprocessing tasks for training LLMs and recommendation. Besides,\nthe recommendation-oriented corpus is very limited during the\ntraining phase of LLMs. 2) The effect of LLMs is restricted by the\narXiv:2305.00447v3  [cs.IR]  17 Oct 2023\nRecSys ‚Äô23, September 18‚Äì22, 2023, Singapore, Singapore Keqin Bao*, Jizhi Zhang*, Yang Zhang, Wenjie Wang, Fuli Feng ‚Ä†, and Xiangnan He‚Ä†\n100%\nInstruction\nLLM e.g. \nChatGPT\nRatio\n100%\n0%\n53%\n47%\nGround Truth\n0%\nChatGPT\nFail\nLLM Fails !\nRec\nAUC\n0.5\n0.75\nAlpaca\n0.46\nDavinci\n002 (GPT3)\n0.49\nChatGPT\n0.5\nDavinci\n003 (GPT3)\n0.53\nLLMs on Movie Rec \nAUC\n0.5\n0.75\nAlpaca\n0.53\nDavinci\n002 (GPT3)\n0.46\nChatGPT\n0.5\nDavinci\n003 (GPT3)\n0.46\nLLMs on Book Rec \nHistorical Sequence\nLike\nDislike\nItem\nRecommend\nRec Task Sample\n‚Ä¶\nD\nifficult to \ndetermine \nwhether they \nwill like\n‚Ä¶\n(\nrefuse\nto \nanswer)\nRefuse\nFigure 1: Illustration of LLMs for the recommendations. Given users‚Äô interaction history, LLMs predict whether a user will like\na new item through In-context Learning. However, the representative LLMs, e.g., ChatGPT, either refuse to answer or always\ngive positive predictions (likes) on Movie and Book recommendation tasks. If we ignore the refused answers and calculate AUC\non the remaining samples, we find that LLMs perform similarly with random guessing (AUC=0.5). Refer to Section 3 for more\nexperimental details.\nunderlying recommendation models, which may fail to include\ntarget items in their candidate lists due to their limited capacity.\nTherefore, we consider building a Large Recommendation Language\nModel (LRLM) to bridge the gap between LLMs and the recommen-\ndation task and better stimulate the recommendation capabilities\nof LLMs in addition to In-context Learning.\nToward this goal, we focus on tuning LLMs with the recommen-\ndation task. Considering that instruction tuning is core to letting\nthe LLM learn to solve different tasks and have strong generaliza-\ntion ability [22, 23, 37], we propose a lightweight tuning framework\nto adapt LLMs for recommendations, named TALLRec. Elaborately,\nTALLRec structures the recommendation data as instructions and\ntunes the LLM via an additional instruction tuning process. More-\nover, given that LLM training necessitates a substantial amount\nof computing resources, TALLRec employs a lightweight tuning\napproach to efficiently adapt the LLMs to the recommendation task.\nSpecifically, we apply the TALLRec framework on the LLaMA-\n7B model [46] with a LoRA [21] architecture, which ensures the\nframework can be deployed on an Nvidia RTX 3090 (24GB) GPU.\nFurthermore, to investigate the minimal computational resources\nrequired, we do experiments in a few-shot setting, utilizing only\na limited number of tuning examples. We conduct detailed exper-\niments in knowledge-rich recommendation scenarios of movies\nand books, where the tuned LLaMA-7B model outperforms tra-\nditional recommendation models and In-context Learning with\nGPT3.5, a much stronger LLM than LLaMA-7B. The results validate\nthe efficiency and robustness of our framework: 1) our TALLRec\nframework can quickly inspire the recommendation capability of\nLLMs in the few-shot setting. and 2) LLMs trained via the TALL-\nRec framework have a strong generalization ability across different\ndomains (e.g., movie ‚Üíbook).\nIn total, our contributions are summarized as follows:\n‚Ä¢We study a new problem in recommendation ‚Äî aligning\nthe LLMs with the recommendation, where we reveal the\nTable 1: A tuning sample for a translation task.\nInstruction Input\nTask Instruction: Translate from English to Chinese.\nTask Input: Who am I ?\nInstruction Output\nTask Output: Êàë ÊòØ Ë∞Å ?\nlimitations of In-context Learning-based approaches and\nunderscore the significance of instruction tuning.\n‚Ä¢We introduce a new TALLRec framework to build Large Rec-\nommendation Language Models, which enables the effective\nand efficient tuning of LLMs for recommendation with low\nGPU costs and few tuning samples.\n‚Ä¢We conduct extensive experiments, validating the effective-\nness and efficiency of the proposed framework, and uncov-\nering its exceptional robustness with seamless navigation\nacross different domains.\n2 TALLREC\nIn this section, we first introduce the preliminary knowledge for\ntuning LLMs and our task formulation, and then present the pro-\nposed TALLRec framework.\n2.1 Preliminary\n‚Ä¢Instruction Tuning is a crucial technique to train LLMs with\nhuman-annotated instructions and responses [36]. Generally, in-\nstruction tuning has four steps (see the example in Table 2). Specifi-\ncally, Step 1: Define a task and articulate a ‚ÄúTask Instruction‚Äù using\nnatural language, which usually encompasses a clear definition of\nthe task, as well as specific solutions to address it.Step 2: Formulate\nand construct the input and output of the task in natural language,\ndenoted as ‚ÄúTask Input ‚Äù and ‚ÄúTask Output ‚Äù. Step 3: Integrate the\n‚ÄúTask Instruction‚Äù and ‚ÄúTask Input ‚Äù together to form the ‚ÄúInstruction\nTALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation RecSys ‚Äô23, September 18‚Äì22, 2023, Singapore, Singapore\nTable 2: A tuning sample for rec-tuning.\nInstruction Input\nTask Instruction:\nGiven the user‚Äôs historical interactions, please determine\nwhether the user will enjoy the target new movie by\nanswering \"Yes\" or \"No\".\nTask Input:\nUser‚Äôs liked items: GodFather.\nUser‚Äôs disliked items: Star Wars.\nTarget new movie: Iron Man\nInstruction Output\nTask Output: No.\nInput‚Äù, and take the ‚ÄúTask Output ‚Äù as the corresponding ‚ÄúInstruc-\ntion Output ‚Äù, for each tuning sample. Step 4: Instruction tuning\non LLMs based on the formatted pairs of ‚Äú Instruction Input ‚Äù and\n‚ÄúInstruction Output ‚Äù.\n‚Ä¢Rec-tuning Task Formulation. We aim to utilize LLM, denoted\nas M, to construct an LRLM, which can predict whether a new\nitem will be enjoyed by a user. To achieve this objective, we do rec-\nommendation tuning (rec-tuning) on LLMs with recommendation\ndata. As shown in Table 2, we format recommendation data into\na pattern of instruction tuning. We begin by composing a ‚Äú Task\nInstruction‚Äù that directs the model to determine whether the user\nwill like the target item based on their historical interactions, and\nto respond with a binary answer of ‚ÄúYes‚Äù or ‚ÄúNo‚Äù. To format the\n‚ÄúTask Input ‚Äù, we categorize the user‚Äôs historically interacted items\ninto two groups based on ratings: the user‚Äôs liked items and dis-\nliked items, where items are sequentially ranked by interaction\ntime and represented by textual descriptions ( e.g., title and brief\nintroduction). Besides, ‚ÄúTask Input ‚Äù also includes a target new item\nthat the user has never seen. Lastly, we merge ‚Äú Task Instruction‚Äù\nand ‚ÄúTask Input ‚Äù to create a ‚Äú Instruction Input ‚Äù, and then set the\nexpected ‚ÄúInstruction Output ‚Äù as ‚ÄòYes‚Äù or ‚ÄúNo‚Äù for rec-tuning.\n2.2 TALLRec Framework\nIn this subsection, we introduce the TALLRec framework, which\naims to facilitate the effective and efficient alignment of LLMs with\nrecommendation tasks, particularly in low GPU memory consump-\ntion settings. Specifically, we first present two TALLRec tuning\nstages with lightweight implementation, followed by the backbone\nselection. As shown in Figure 2, TALLRec comprises two tuning\nstages: alpaca tuning and rec-tuning. The former stage is the com-\nmon training process of LLM that enhances LLM‚Äôs generalization\nability, while the latter stage emulates the pattern of instruction\ntuning and tunes LLMs for the recommendation task.\n‚Ä¢TALLRec Tuning Stages. For alpaca tuning, we employ the\nself-instruct data made available by Alpaca [45] to train the LLM.\nSpecifically, we utilize the conditional language modeling objective\nduring the alpaca tuning, as exemplified in the Alpaca repository1.\n1https://github.com/tloen/alpaca-lora.\nFormally,\nmax\nŒ¶\n‚àëÔ∏Å\n(ùë•,ùë¶ )‚ààZ\n|ùë¶|‚àëÔ∏Å\nùë°=1\nlog (ùëÉŒ¶ (ùë¶ùë° |ùë•, ùë¶<ùë° )), (1)\nwhere ùë• and ùë¶ represent the ‚ÄúInstruction Input ‚Äù and ‚ÄúInstruction\nOutput‚Äù in the self-instruct data, respectively,ùë¶ùë° is the ùë°-th token\nof the ùë¶, ùë¶<ùë° represents the tokens before ùë¶ùë° , Œ¶ is the original\nparameters of M, and Zis the training set. For rec-tuning, we can\nleverage the rec-tuning sample as described in Table 2 to tune the\nLLM, similar to alpaca tuning.\n‚Ä¢Lightweight Tuning. However, directly tuning the LLM is com-\nputationally intensive and time-consuming. As such, we propose to\nadopt a lightweight tuning strategy to execute both alpaca tuning\nand rec-tuning. The central premise of lightweight tuning is that\ncontemporary language models may possess an excessive number\nof parameters, and their information is concentrated on a low in-\ntrinsic dimension [21]. Consequently, we can achieve comparable\nperformance to that of the entire model by tuning only a small sub-\nset of parameters [20, 28, 31]. Specifically, we employ LoRA [21],\nwhich involves freezing the pre-trained model parameters and in-\ntroducing trainable rank decomposition matrices into each layer\nof the Transformer architecture to facilitate lightweight tuning.\nTherefore, by optimizing rank decomposition matrices, we can effi-\nciently incorporate supplementary information while maintaining\nthe original parameters in a frozen state. In total, the final learning\nobjective can be computed as:\nmax\nŒò\n‚àëÔ∏Å\n(ùë•,ùë¶ )‚ààZ\n|ùë¶|‚àëÔ∏Å\nùë°=1\nlog (ùëÉŒ¶+Œò (ùë¶ùë° |ùë•, ùë¶<ùë° )), (2)\nwhere Œò is the LoRA parameters and we only update LoRA param-\neters during the training process. Through LoRA, we can complete\ntraining with only one-thousandth of the original LLM parameters\nto complete the training process [21]. ‚Ä¢Backbone Selection. At\npresent, there are large amounts of LLMs released, such as GPT\nseries, PaLM, CHinchilla, and LLaMA [3, 4, 18, 46]. Among these,\na considerable number of LLMs (such as PaLM and Chinchilla)\ndo not provide access to their model parameters or APIs, render-\ning them challenging to utilize for research or other applications.\nAdditionally, data security concerns are significant issues in the\nrecommendation field. Consequently, the utilization of third-party\nAPIs (such as ChatGPT and text-davinci-003) to leverage LLMs\nnecessitates further discussion. To replicate the issues that require\nconsideration in real-world recommendation scenarios, we intend\nto simulate the practical utilization of a public LLM and update\nits parameters for recommendation purposes. After careful con-\nsideration, we have opted to conduct experiments using LLMs-\nLLaMA, which is presently the best-performing open-source LLM,\nand whose training data is also publicly available [46].\n3 EXPERIMENTS\nIn this section, we conduct experiments to answer the following\nresearch questions:\n- RQ1: How does TALLRec perform compared with current LLM-\nbased and traditional recommendation models?\nRecSys ‚Äô23, September 18‚Äì22, 2023, Singapore, Singapore Keqin Bao*, Jizhi Zhang*, Yang Zhang, Wenjie Wang, Fuli Feng ‚Ä†, and Xiangnan He‚Ä†\nLLM\nAlpaca\nTuning\nRec\n-\nTuning\nLLM\nLoRA\nInput\nOutput\nLightweight Tuning\nTALLRec\nFramework\nRec\n-\nTuning\nSamples\nInstruction\nInput\nOutput\nK\nFigure 2: Illustration of the TALLRec framework constructed by alpaca tuning and rec-tuning two stages. During rec-tuning,\nwe use the rec-tuning samples with instruction input and output constructed from recommendation data. Notably, we employ\nlightweight tuning technology to enhance the efficiency of our TALLRec framework.\n- RQ2: How do the different components in TALLRec affect its\neffectiveness?\n- RQ3: How does TALLRec perform under cross-domain recom-\nmendation?\n‚Ä¢Dataset. We conduct experiments on two datasets. The statistics\nand more details can be found in our released data.\n- Movie.\nThis is a processed dataset from MovieLens100K [ 12] , which\ncomprises user ratings on movies and comprehensive textual\ndescriptions of movies such as ‚Äútitle‚Äù and ‚Äúdirector‚Äù. Because\nwe conduct experiments in a few-shot training setting that re-\nquires limited tuning samples, we process the original dataset\nby sampling the most recent 10,000 interactions and split them\ninto training, validation, and testing sets with a ratio of 8:1:1. To\nconstruct a rec-tuning sample, 10 interactions prior to the target\nitem are retained as historical interactions. Following [ 16, 66],\nwe only treat the interactions with ratings > 3 as ‚Äúlikes‚Äù, and\nthose with ratings ‚â§3 as ‚Äúdislike‚Äù.\n- Book. This is a book recommendation dataset processed from\nBookCrossing [71]. The BookCrossing dataset has user ratings (1-\n10) and textual descriptions of books, such as the information of\n‚ÄòBook-Author‚Äô and ‚ÄòBook-Title‚Äô. For each user, we randomly select\nan item interacted by this user as the prediction target, and sam-\nple 10 interacted items as historical interactions2. Subsequently,\nwe partition constructed rec-tuning samples into training, vali-\ndation, and testing sets with the same ratio of 8:1:1. Additionally,\nwe binarize the ratings according to a threshold of 5.\n‚Ä¢Few-shot Training Setting. We adopt a few-shot training set-\nting, where only a limited number of samples are randomly selected\nfrom the training set for model training. It is referred to as ‚Äòùêæ-shot‚Äô\ntraining setting, where ùêæ represents the number of training sam-\nples used. By setting an extremely small value for K, such as 64, we\ncould test whether a method can rapidly acquire recommendation\ncapability from LLMs with severely limited training data.\n‚Ä¢Baseline. We compare TALLRec against both LLM-based and tra-\nditional recommendation methods. 1) Existing LLM-based meth-\nods adopt In-context Learning to directly generate recommenda-\ntions [13, 47]. For a fair comparison, we align these methods with\nTALLRec by using the same instructions. Specifically, we perform\n2BookCrossing lacks interaction timestamps, thus we can only construct historical\ninteraction by random sampling.\nIn-context Lerning on different LLMs: 1) Alpaca-LoRA, 2) Text-\nDavinvi-002, 3) Text-Daviniv-003, and 4) ChatGPT. Alpaca-LoRA is a\nmodel for reproducing Alpaca results of the LLaMA model by using\nLoRA and alpaca tuning. The latter three are GPT series models\nfrom OpenAI.\n2) Traditional methods. Since our approach utilizes histori-\ncal interactions to predict the subsequent interaction, similar to\nthe sequential recommendation, we consider the following sequen-\ntial models: (i) GRU4Rec [ 17] is an RNN-based sequential rec-\nommender, which utilizes GRU to encode historical interactions.\n(ii) Caser [ 44] utilizes CNN to encode historical interaction se-\nquences. (iii) SASRec [ 26] is a classic transformer-based sequen-\ntial recommender. (iv) DROS [ 60] is a state-of-the-art sequential\nrecommender model, which harnesses distributionally robust op-\ntimization for robust recommendations. We use the version im-\nplemented by GRU4Rec, provided by the authors.3 The sequential\nmodels above rely on item ID features without considering textual\ndescriptions of items. However, in our setting, we assume item\ntext descriptions are available for LLM tuning. To ensure fair com-\nparisons, we further consider comparing the following variants of\nGRU4Rec and DROS: (v) GRU-BERT is a variant of GRU4Rec that\nincorporates a pre-trained BERT [ 7] to encode text descriptions.\nSpecifically, BERT encodes text descriptions and outputs a CLS\nembedding, which is then concatenated with the original ID embed-\ndings of GRU4Rec as the item representations. (vi) DROS-BERT\nis integrated with BERT, similar to GRU-BERT.\n‚Ä¢Evaluation Metric. Since TALLRec aims to predict user prefer-\nence over a given target item, i.e., a binary classification problem,\nwe adopt a popular evaluation metric used in recommendation:\nArea Under the Receiver Operating Characteristic (AUC).\n‚Ä¢Implementation Details. To ensure uniform sequence lengths,\nwe use the user‚Äôs last interacted item to pad the historical interaction\nsequences with lengths < the threshold, 10. For all methods, we\noptimize parameters using Adam with MSE loss and a learning rate\nof 1e-3. We search the weight decay of all methods in {1e-3, 1e-4, 1e-\n5, 1e-6, 1e-7}. Following [60], regarding specific hyperparameters of\nbaselines, we adhered to their original settings. For GRU-BERT and\nDROS-BERT, we utilize BERT released by Hugging Face 4, while\nsetting the number of GRU layers to4 and the hidden size to 1024 for\naligning with BERT‚Äôs embedding size. Lastly, we run all methods\n3https://github.com/YangZhengyi98/DROS.\n4https://huggingface.co/bert-base-uncased.\nTALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation RecSys ‚Äô23, September 18‚Äì22, 2023, Singapore, Singapore\nTable 3: Performance comparison between conventional sequential recommendation baselines and TALLRec under different\nfew-shot training settings. The reported result is the AUC multiplied by 100, with boldface indicating the highest score. ‚Ä°:\nsignificantly better than all baselines with t-test ùëù<0.01.\nFew-shot GRU4Rec Caser SASRec DROS GRU-BERT DROS-BERT TALLRec\nmovie\n16 49.07 49.68 50.43 50.76 50.85 50.21 67.24‚Ä°\n64 49.87 51.06 50.48 51.54 51.65 51.71 67.48‚Ä°\n256 52.89 54.20 52.25 54.07 53.44 53.94 71.98‚Ä°\nbook\n16 48.95 49.84 49.48 49.28 50.07 50.07 56.36\n64 49.64 49.72 50.06 49.13 49.64 48.98 60.39‚Ä°\n256 49.86 49.57 50.20 49.13 49.79 50.20 64.38‚Ä°\nFigure 3: Figure (a) shows the performance comparison between LLM-based baselines (zero-shot setting) and ours TALLRec,\nwhere the TALLRec is trained on only 64 rec-tuning samples ( i.e., in the 64-shot training setting). Figure (b) demonstrates the\nperformance tendency of TALLRec‚Äôs variants and conventional sequential recommendation methods w.r.t. the number of\ntraining samples used, ranging from 1 to 256. TALLRec has three variants: ‚ÄúAT‚Äù for alpaca tuning only, ‚ÄúRT‚Äù for rec-tuning\nonly, and ‚ÄúTALLRec‚Äù for the full version.\nfive times with different random seeds and report the averaged\nresults.\n3.1 Performance Comparison (RQ1)\nWe aim to investigate the recommendation performance of various\nmethods under the few-shot training setting, which enables us to\nevaluate how LLMs can be quickly adjusted for recommendation\nwith limited rec-tuning samples. The evaluation results against\ntraditional methods are presented in Table 3, while the comparison\nagainst LLM-based methods is depicted in Figure 3 (a).\nFrom the figure and table, we draw the following observations:\n1) Our method significantly outperforms both traditional and LLM-\nbased methods, verifying the superiority of tuning LLM via our\nTALLRec framework. TALLRec successfully unlocks the knowl-\nedge and generalization capabilities of LLMs for recommenda-\ntions. 2) LLM-based methods perform similarly to random guessing\n(AUC‚âà0.5). However, the LLMs trained via TALLRec achieves sig-\nnificant improvements. These results demonstrate a considerable\ngap between recommendation and language tasks, showing the\nimportance of using recommendation data for rec-tuning on LLMs.\n3) Traditional recommender methods consistently yield inferior\nperformance under our few-shot training settings, implying that\ntraditional methods are incapable of quickly learning the recom-\nmendation capability with limited training samples. 4) GRU-BERT\nand DROS-BERT do not show significant improvement over their\nbackend models, GRU4Rec and DROS. This indicates that purely\nadding textual descriptions cannot enhance the traditional recom-\nmender models in the few-shot training setting.\n3.2 Ablation Study (RQ2)\nTo demonstrate the effectiveness of alpaca tuning and rec-tuning\nin TALLRec, we conduct ablation studies with varying ùêæ under the\nùêæ-shot training setting. Specifically, we compare the performance\nof TALLRec with that of two variants, ‚ÄúAT‚Äù and ‚ÄúRT‚Äù, where ‚ÄúAT‚Äù\nonly conducts the alpaca tuning, while ‚ÄúRT‚Äù solely implements\nrec-tuning. By varying ùêæ, we further investigate the impact of the\nnumber of training samples.\nWe summarize the results in Figure 3 (b), from which we have\nthe following observations: 1) The performance of ‚ÄúAT‚Äù signifi-\ncantly declines compared to that of ‚ÄúRT‚Äù and TALLRec, indicating\nthe essential effect of rec-tuning, which effectively inspires the\nLLM‚Äôs recommendation capability 2) With limited rec-tuning sam-\nples (‚â§ 128), TALLRec generally outperforms ‚ÄúRT‚Äù, confirming\nthat alpaca tuning can enhance the LLM‚Äôs generalization ability\non new tasks, especially when the training data in the new tasks\nare insufficient. As the quantity of rec-tuning samples grows, the\nresults of TALLRec and ‚ÄúRT‚Äù become closer. This makes sense, as\nthe significance of generalization abilities derived from other tasks\ndiminishes when there is an ample amount of training data for\nthe new tasks. 3) With the increase of rec-tuning sample number,\nRecSys ‚Äô23, September 18‚Äì22, 2023, Singapore, Singapore Keqin Bao*, Jizhi Zhang*, Yang Zhang, Wenjie Wang, Fuli Feng ‚Ä†, and Xiangnan He‚Ä†\nFigure 4: Cross-domain generalization performance of LRLMs trained via TALLRec using Book data (TALLRec (Book)), Movie\ndata (TALLRec (Movie)), and both (TALLRec (Both)). The left figure shows the testing results on the Movie dataset with varying\nnumbers of rec-tuning samples, while the right figure shows the testing results on the Book dataset.\nTALLRec consistently performs better than the baselines. It is at-\ntributed to rec-tuning, which can utilize limited samples to inspire\nthe LLM‚Äôs recommendation capability.\n3.3 Cross-domain Generalization Analyses\n(RQ3)\nTo further investigate the generalization ability of TALLRec, we con-\nduct experiments on cross-domain recommendations. Specifically,\nwe tune TALLRec with different rec-tuning samples, including 1)\n‚ÄúTALLRec (Book)‚Äù, only using the samples from the Book dataset; 2)\n‚ÄúTALLRec (Movie)‚Äù, solely using samples from the Movie dataset;\nand 3) ‚ÄúTALLRec (Both)‚Äù, tuned with both the Book and Movie\nsamples. We vary ùêæ in {16,64,258}under the few-shot training\nsetting, and evaluate the models on the testing sets of Book and\nMovie, respectively. The results are summarized in Figure 4, from\nwhich we can find: 1) TALLRec demonstrates remarkable cross-\ndomain generalization ability. For instance, after tuning only on\nmovie samples, ‚ÄúTALLRec (Movie)‚Äù exhibits good performance on\nBook data, comparable to ‚ÄúTALLRec (Book)‚Äù. This is impressive and\nsuggests that TALLRec has cross-domain generalization ability in-\nstead of only fitting a single domain like traditional recommenders.\n2) ‚ÄúTALLRec (Both)‚Äù surpasses ‚ÄúTALLRec (Movie)‚Äù and ‚ÄúTALLRec\n(Book)‚Äù on two testing sets when the number of rec-tuning samples\nexceeds 64. This finding indicates that TALLRec can seamlessly\nintegrate data from different domains to enhance its generalization\nperformance. In future work, it is promising to pre-train TALL-\nRec with large-scale recommendation data from heterogeneous\ndomains.\n4 RELATED WORK\n‚Ä¢LMs for Recommendation. There have been several attempts\nto integrate language models (LMs) with recommendation systems.\nDespite the incorporation of LMs [14, 29], some attempts persist in\nutilizing traditional user/item IDs to represent users/items. Thereby,\nthey disregard the semantic understanding capabilities of LMs, such\nas reviews, which other work has incorporated the language infor-\nmation as part of the users/items embedding [19]. In addition, other\nmethods either utilize an undisclosed model that already possesses\npreliminary recommendation capabilities [6]. or employ small mod-\nels to train on large-scale downstream task data [ 68]. Moreover,\nthe aforementioned models are also limited to small models, while\nthis paper is orthogonal about how to adapt large language mod-\nels to recommendation tasks. In recommendation systems, there\nis currently little research on applying LLMs in recommendation\nscenarios. Those works utilize the interaction ability of GPT3.5\nseries models and apply In-context Learning [ 13, 47]. In detail,\nChat-Rec [13] endeavors to harness the interaction capabilities of\nChatGPT and link the ChatGPT with traditional recommendation\nmodels (e.g. MF [27], LightGCN [16]) to formulate a conversational\nrecommendation system. NIR [47] shares a similar concept with\nChat-Rec, which employs conventional recommendation models\nto generate candidate items subjected to a three-stage multi-step\nprompting process for re-ranking.\n‚Ä¢Sequential Recommendation. Our setup is close to the sequen-\ntial recommendation, which aims to infer the user‚Äôs next interaction\nbased on users‚Äô historical interaction sequences [11, 50]. In the early\ntime, the Markov chain plays an important role in sequential recom-\nmendation [15, 34, 40, 49]. Recently, deep learning-based methods\nhave become mainstream. Extensive work using different kinds of\nneural network structures, like RNN [ 5, 9, 17], CNN [44, 59, 62],\nand attention [26, 58, 65], to model the user interaction sequences.\nHowever, limited by only using IDs to represent users and items,\nsuch work cannot fastly adapt and generalize to new scenarios.\nThus, some works focus on the generalization ability of sequential\nrecommendation models by pre-training [35, 61], data augmenta-\ntion [38, 39, 51, 57], debiasing [8, 52, 67, 70], and robust optimiza-\ntion [55, 60]. However, they ignore the strong generalization ability\nof existing LLMs, leading to inadequate exploration.\n5 CONCLUSION\nWith the advancement of LLMs, people are gradually recognizing\ntheir potential in recommendation systems [30, 63, 64]. In this work,\nwe explored the feasibility of using LLMs for the recommendation.\nOur initial findings reveal that even the existing best LLM models do\nnot perform well in recommendation tasks. To address this issue, we\nproposed a TALLRec framework that can efficiently align LLM with\nrecommendation tasks through two tuning stages: alpaca tuning\nand rec-tuning. Our experimental results demonstrate that the\nLLMs trained using our TALLRec framework outperform traditional\nmodels and exhibit strong cross-domain generalization abilities.\nMoving forward, we plan to explore more efficient methods to\nactivate the recommendation ability of larger models and tune\nLLMs to handle multiple recommendation tasks simultaneously.\nTALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation RecSys ‚Äô23, September 18‚Äì22, 2023, Singapore, Singapore\nACKNOWLEDGMENTS\nThis work is supported by the National Natural Science Foundation\nof China (62272437), and the CCCD Key Lab of Ministry of Culture\nand Tourism.\nREFERENCES\n[1] Qingyao Ai, Ting Bai, Zhao Cao, Yi Chang, Jiawei Chen, Zhumin Chen, Zhiyong\nCheng, Shoubin Dong, Zhicheng Dou, Fuli Feng, et al. 2023. Information Retrieval\nMeets Large Language Models: A Strategic Report from Chinese IR Community.\narXiv preprint arXiv:2307.09751 (2023).\n[2] Greg Brockman, Mira Murati, Peter Welinder, and OpenAI. 2022. OpenAI API.\n[3] Tom Brown, Benjamin Mann, Nick Ryder, et al . 2020. Language models are\nfew-shot learners. NeurIPS 33 (2020), 1877‚Äì1901.\n[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint arXiv:2204.02311 (2022).\n[5] Qiang Cui, Shu Wu, Qiang Liu, Wen Zhong, and Liang Wang. 2020. MV-RNN: A\nMulti-View Recurrent Neural Network for Sequential Recommendation. IEEE\nTrans. Knowl. Data Eng. 32, 2 (2020), 317‚Äì331.\n[6] Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. M6-\nRec: Generative Pretrained Language Models are Open-Ended Recommender\nSystems. arXiv preprint arXiv:2205.08084 (2022).\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nNAACL. Association for Computational Linguistics, 4171‚Äì4186.\n[8] Sihao Ding, Fuli Feng, Xiangnan He, Jinqiu Jin, Wenjie Wang, Yong Liao, and\nYongdong Zhang. 2022. Interpolative Distillation for Unifying Biased and Debi-\nased Recommendation. In SIGIR ‚Äô22, July 11 - 15, 2022 . ACM, 40‚Äì49.\n[9] Tim Donkers, Benedikt Loepp, and J√ºrgen Ziegler. 2017. Sequential user-based\nrecurrent neural network recommendations. In Proceedings of the eleventh ACM\nconference on recommender systems . 152‚Äì160.\n[10] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, et al. 2023. PaLM-E: An Embodied\nMultimodal Language Model. abs/2303.03378 (2023). arXiv:2303.03378\n[11] Hui Fang, Danning Zhang, Yiheng Shu, and Guibing Guo. 2020. Deep Learning\nfor Sequential Recommendation: Algorithms, Influential Factors, and Evaluations.\nACM Trans. Inf. Syst. 39, 1 (2020), 10:1‚Äì10:42.\n[12] F.Maxwell and Joseph A. Konstan. 2015. The MovieLens Datasets: History and\nContext.. In ACM Transactions on Interactive Intelligent Systems (TiiS) .\n[13] Yunfan Gao et al. 2023. Chat-REC: Towards Interactive and Explainable LLMs-\nAugmented Recommender System. arXiv preprint:2303.14524 (2023).\n[14] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022.\nRecommendation as language processing (rlp): A unified pretrain, personalized\nprompt & predict paradigm (p5). In Proceedings of the 16th ACM Conference on\nRecommender Systems . 299‚Äì315.\n[15] Ruining He and Julian McAuley. 2016. Fusing similarity models with markov\nchains for sparse sequential recommendation. In 2016 IEEE 16th international\nconference on data mining (ICDM) . IEEE, 191‚Äì200.\n[16] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng\nWang. 2020. Lightgcn: Simplifying and powering graph convolution network for\nrecommendation. In SIGIR‚Äô20. 639‚Äì648.\n[17] Bal√°zs Hidasi et al. 2016. Session-based Recommendations with Recurrent Neural\nNetworks. In ICLR.\n[18] Jordan Hoffmann et al. 2022. Training compute-optimal large language models.\narXiv preprint arXiv:2203.15556 (2022).\n[19] Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong\nWen. 2022. Towards Universal Sequence Representation Learning for Recom-\nmender Systems. InProceedings of the 28th ACM SIGKDD Conference on Knowledge\nDiscovery and Data Mining . 585‚Äì593.\n[20] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, et al . 2019. Parameter-\nEfficient Transfer Learning for NLP. In Proceedings of the 36th International\nConference on Machine Learning, ICML 2019, 9-15 June 2019 , Vol. 97. 2790‚Äì2799.\n[21] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large\nLanguage Models. In The Tenth International Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022 .\n[22] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, et al . 2022. OPT-IML:\nScaling Language Model Instruction Meta Learning through the Lens of General-\nization. arXiv preprint arXiv:2212.12017 (2022).\n[23] Joel Jang, Seungone Kim, Seonghyeon Ye, Doyoung Kim, Lajanugen Logeswaran,\nMoontae Lee, Kyungjae Lee, and Minjoon Seo. 2023. Exploring the Bene-\nfits of Training Expert Language Models over Instruction Tuning. (2023).\narXiv:2302.03202\n[24] Vitor Jeronymo, Luiz Henrique Bonifacio, Hugo Abonizio, Marzieh Fadaee,\nRoberto de Alencar Lotufo, Jakub Zavrel, and Rodrigo Frassetto Nogueira. 2023.\nInPars-v2: Large Language Models as Efficient Dataset Generators for Informa-\ntion Retrieval. abs/2301.01820 (2023). arXiv:2301.01820\n[25] Matthew Jin, Syed Shahriar, Michele Tufano, Xin Shi, Shuai Lu, Neel Sundaresan,\nand Alexey Svyatkovskiy. 2023. InferFix: End-to-End Program Repair with LLMs.\n(2023). arXiv:2303.07263\n[26] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom-\nmendation. In ICDM. 197‚Äì206.\n[27] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization tech-\nniques for recommender systems. Computer 42, 8 (2009), 30‚Äì37.\n[28] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for\nParameter-Efficient Prompt Tuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing, EMNLP,7-11 November, 2021 .\nAssociation for Computational Linguistics, 3045‚Äì3059.\n[29] Lei Li, Yongfeng Zhang, and Li Chen. 2023. Personalized prompt learning for\nexplainable recommendation. ACM Transactions on Information Systems 41, 4\n(2023), 1‚Äì26.\n[30] Ruyu Li, Wenhao Deng, Yu Cheng, Zheng Yuan, Jiaqi Zhang, and Fajie Yuan.\n2023. Exploring the Upper Limits of Text-Based Collaborative Filtering Using\nLarge Language Models: Discoveries and Insights.arXiv preprint arXiv:2305.11700\n(2023).\n[31] Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous\nPrompts for Generation. In ACL/IJCNLP 2021, August 1-6, 2021 . Association for\nComputational Linguistics, 4582‚Äì4597.\n[32] Percy Liang, Rishi Bommasani, Tony Lee, et al . 2022. Holistic evaluation of\nlanguage models. arXiv preprint arXiv:2211.09110 (2022).\n[33] Xi Victoria Lin, Todor Mihaylov, et al. 2021. Few-shot learning with multilingual\nlanguage models. arXiv preprint arXiv:2112.10668 (2021).\n[34] Tariq Mahmood and Francesco Ricci. 2007. Learning and adaptivity in interactive\nrecommender systems. In Proceedings of the ninth international conference on\nElectronic commerce . 75‚Äì84.\n[35] Yabo Ni, Dan Ou, Shichen Liu, et al. 2018. Perceive Your Users in Depth: Learning\nUniversal User Representations from Multiple E-commerce Tasks. In KDD 2018,\nAugust 19-23, 2018 . ACM, 596‚Äì605.\n[36] Long Ouyang et al. 2022. Training language models to follow instructions with\nhuman feedback. NeurIPS 35 (2022), 27730‚Äì27744.\n[37] Baolin Peng et al. 2023. Instruction Tuning with GPT-4. (2023). arXiv:2304.03277\n[38] Ruihong Qiu, Zi Huang, Hongzhi Yin, and Zijian Wang. 2021. Contrastive Learn-\ning for Representation Degeneration Problem in Sequential Recommendation.\n(2021). arXiv:2110.05730\n[39] Ruihong Qiu, Zi Huang, Hongzhi Yin, and Zijian Wang. 2022. Contrastive Learn-\ning for Representation Degeneration Problem in Sequential Recommendation. In\nWSDM ‚Äô22: The Fifteenth ACM International Conference on Web Search and Data\nMining, February 21 - 25, 2022 . ACM, 813‚Äì823.\n[40] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factor-\nizing personalized markov chains for next-basket recommendation. InProceedings\nof the 19th international conference on World wide web . 811‚Äì820.\n[41] Victor Sanh, Albert Webson, Colin Raffel, et al. 2021. Multitask prompted training\nenables zero-shot task generalization. arXiv:2110.08207 (2021).\n[42] Timo Schick, Jane Dwivedi-Yu, Roberto Dess√¨, Roberta Raileanu, Maria Lomeli,\nLuke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Lan-\nguage models can teach themselves to use tools. arXiv preprint arXiv:2302.04761\n(2023).\n[43] Dhruv Shah, Blazej Osinski, Brian Ichter, and Sergey Levine. 2022. LM-Nav:\nRobotic Navigation with Large Pre-Trained Models of Language, Vision, and\nAction. In CoRL 2022, 14-18 December 2022 (Proceedings of Machine Learning\nResearch, Vol. 205) . PMLR, 492‚Äì504.\n[44] Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommendation\nvia convolutional sequence embedding. In WSDM. 565‚Äì573.\n[45] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos\nGuestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An\nInstruction-following LLaMA model.\n[46] Hugo Touvron et al. 2023. LlaMA: Open and efficient foundation language models.\narXiv preprint arXiv:2302.13971 (2023).\n[47] Lei Wang et al. 2023. Zero-Shot Next-Item Recommendation using Large Pre-\ntrained Language Models. arXiv preprint arXiv:2304.03153 (2023).\n[48] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query Expansion with\nLarge Language Models. (2023). arXiv:2303.07678\n[49] Pengfei Wang, Jiafeng Guo, Yanyan Lan, Jun Xu, Shengxian Wan, and Xueqi\nCheng. 2015. Learning hierarchical representation model for nextbasket rec-\nommendation. In Proceedings of the 38th International ACM SIGIR conference on\nResearch and Development in Information Retrieval . 403‚Äì412.\n[50] Shoujin Wang, Liang Hu, Yan Wang, Longbing Cao, Quan Z. Sheng, and\nMehmet A. Orgun. 2019. Sequential Recommender Systems: Challenges, Progress\nand Prospects. In IJCAI 2019, Macao, China, August 10-16, 2019 . ijcai.org, 6332‚Äì\n6338.\n[51] Ziyang Wang, Huoyu Liu, Wei Wei, Yue Hu, Xian-Ling Mao, Shaojian He, Rui\nFang, and Dangyang Chen. 2022. Multi-level Contrastive Learning Framework\nfor Sequential Recommendation. (2022). arXiv:2208.13007\n[52] Zhenlei Wang, Shiqi Shen, Zhipeng Wang, Bo Chen, Xu Chen, and Ji-Rong Wen.\n2022. Unbiased Sequential Recommendation with Latent Confounders. In WWW\nRecSys ‚Äô23, September 18‚Äì22, 2023, Singapore, Singapore Keqin Bao*, Jizhi Zhang*, Yang Zhang, Wenjie Wang, Fuli Feng ‚Ä†, and Xiangnan He‚Ä†\n‚Äô22: The ACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022 .\nACM, 2195‚Äì2204.\n[53] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian\nLester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models\nare zero-shot learners. arXiv preprint arXiv:2109.01652 (2021).\n[54] Jason Wei, Yi Tay, Rishi Bommasani, et al . 2022. Emergent abilities of large\nlanguage models. arXiv preprint arXiv:2206.07682 (2022).\n[55] Hongyi Wen, Xinyang Yi, Tiansheng Yao, Jiaxi Tang, Lichan Hong, and Ed H. Chi.\n2022. Distributionally-robust Recommendations for Improving Worst-case User\nExperience. In WWW ‚Äô22: The ACM Web Conference 2022, Virtual Event, Lyon,\nFrance, April 25 - 29, 2022 . ACM, 3606‚Äì3610.\n[56] Ted Xiao, Harris Chan, Pierre Sermanet, Ayzaan Wahid, Anthony Brohan, Karol\nHausman, Sergey Levine, and Jonathan Tompson. 2022. Robotic Skill Acquisition\nvia Instruction Augmentation with Vision-Language Models. abs/2211.11736\n(2022). arXiv:2211.11736\n[57] Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Jiandong Zhang, Bolin\nDing, and Bin Cui. 2022. Contrastive Learning for Sequential Recommendation.\nIn 38th IEEE International Conference on Data Engineering, ICDE 2022, Kuala\nLumpur, Malaysia, May 9-12, 2022 . IEEE, 1259‚Äì1273.\n[58] Chengfeng Xu, Jian Feng, Pengpeng Zhao, Fuzhen Zhuang, Deqing Wang, Yanchi\nLiu, and Victor S. Sheng. 2021. Long- and short-term self-attention network for\nsequential recommendation. Neurocomputing 423 (2021), 580‚Äì589.\n[59] An Yan, Shuo Cheng, Wang-Cheng Kang, Mengting Wan, and Julian J. McAuley.\n2019. CosRec: 2D Convolutional Neural Networks for Sequential Recommenda-\ntion. In CIKM 2019, November 3-7, 2019 . ACM, 2173‚Äì2176.\n[60] Zhengyi Yang et al. 2023. A Generic Learning Framework for Sequential Recom-\nmendation with Distribution Shifts. (2023).\n[61] Fajie Yuan, Xiangnan He, Alexandros Karatzoglou, and Liguang Zhang. 2020.\nParameter-Efficient Transfer from Sequential Behaviors for User Modeling and\nRecommendation. In SIGIR 2020, July 25-30, 2020 . ACM, 1469‚Äì1478.\n[62] Fajie Yuan, Alexandros Karatzoglou, Ioannis Arapakis, Joemon M. Jose, and\nXiangnan He. 2019. A Simple Convolutional Generative Network for Next Item\nRecommendation. In WSDM 2019, February 11-15, 2019 . ACM, 582‚Äì590.\n[63] Zheng Yuan, Fajie Yuan, Yu Song, Youhua Li, Junchen Fu, Fei Yang, Yunzhu\nPan, and Yongxin Ni. 2023. Where to Go Next for Recommender Systems? ID-\nvs. Modality-based Recommender Models Revisited. In Proceedings of the 46th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023 , Hsin-Hsi Chen, Wei-Jou (Ed-\nward) Duh, Hen-Hsen Huang, Makoto P. Kato, Josiane Mothe, and Barbara Poblete\n(Eds.). ACM, 2639‚Äì2649. https://doi.org/10.1145/3539618.3591932\n[64] Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan\nHe. 2023. Is chatgpt fair for recommendation? evaluating fairness in large lan-\nguage model recommendation. In Proceedings of the 17th ACM Conference on\nRecommender Systems (RecSys ‚Äô23) . Association for Computing Machinery.\n[65] Tingting Zhang, Pengpeng Zhao, Yanchi Liu, et al. 2019. Feature-level Deeper\nSelf-Attention Network for Sequential Recommendation. In Proceedings of the\nTwenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019,\nAugust 10-16, 2019 . ijcai.org, 4320‚Äì4326.\n[66] Yang Zhang et al. 2023. Reformulating CTR Prediction: Learning Invariant Feature\nInteractions for Recommendation. (2023).\n[67] Yang Zhang, Fuli Feng, Xiangnan He, Tianxin Wei, Chonggang Song, Guohui\nLing, and Yongdong Zhang. 2021. Causal Intervention for Leveraging Popularity\nBias in Recommendation. In SIGIR ‚Äô21, July 11-15, 2021 . ACM, 11‚Äì20.\n[68] Zizhuo Zhang and Bang Wang. 2023. Prompt Learning for News Recommendation.\narXiv preprint arXiv:2304.05263 (2023).\n[69] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang,\nYushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang,\nZikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A Survey of Large\nLanguage Models. CoRR abs/2303.18223 (2023). https://doi.org/10.48550/arXiv.\n2303.18223 arXiv:2303.18223\n[70] Yu Zheng et al. 2021. Disentangling User Interest and Conformity for Recom-\nmendation with Causal Embedding. In WWW ‚Äô21 . 2980‚Äì2991.\n[71] Cai-Nicolas Ziegler, Sean M. McNee, Joseph A. Konstan, and Georg Lausen. 2005.\nImproving Recommendation Lists through Topic Diversification. In Proceedings\nof the 14th International Conference on World Wide Web (WWW ‚Äô05) . Association\nfor Computing Machinery, 22‚Äì32.",
  "topic": "Generalization",
  "concepts": [
    {
      "name": "Generalization",
      "score": 0.7074607610702515
    },
    {
      "name": "Computer science",
      "score": 0.6751407384872437
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6571218967437744
    },
    {
      "name": "Code (set theory)",
      "score": 0.516205370426178
    },
    {
      "name": "Task (project management)",
      "score": 0.488784521818161
    },
    {
      "name": "Bridge (graph theory)",
      "score": 0.4723426103591919
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.41646263003349304
    },
    {
      "name": "Artificial intelligence",
      "score": 0.32791459560394287
    },
    {
      "name": "Engineering",
      "score": 0.12322530150413513
    },
    {
      "name": "Medicine",
      "score": 0.11826357245445251
    },
    {
      "name": "Geography",
      "score": 0.08684366941452026
    },
    {
      "name": "Programming language",
      "score": 0.07513502240180969
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Internal medicine",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    }
  ],
  "cited_by": 13
}