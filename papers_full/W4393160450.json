{
  "title": "Auto-Prox: Training-Free Vision Transformer Architecture Search via Automatic Proxy Discovery",
  "url": "https://openalex.org/W4393160450",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5016625901",
      "name": "Zimian Wei",
      "affiliations": [
        "National University of Defense Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5063431385",
      "name": "Peijie Dong",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5101263846",
      "name": "Zheng Hui",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A5040367049",
      "name": "Anggeng Li",
      "affiliations": [
        "Huawei Technologies (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5101983249",
      "name": "Lujun Li",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5091320085",
      "name": "Menglong Lu",
      "affiliations": [
        "National University of Defense Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5053995594",
      "name": "Hengyue Pan",
      "affiliations": [
        "National University of Defense Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5100440903",
      "name": "Dongsheng Li",
      "affiliations": [
        "National University of Defense Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6608315494",
    "https://openalex.org/W4297847691",
    "https://openalex.org/W6641363171",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W6848948516",
    "https://openalex.org/W3177313544",
    "https://openalex.org/W3126536942",
    "https://openalex.org/W4361230789",
    "https://openalex.org/W4385000754",
    "https://openalex.org/W3174738881",
    "https://openalex.org/W3135593154",
    "https://openalex.org/W6790690058",
    "https://openalex.org/W3162847008",
    "https://openalex.org/W3172801447",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W3139445856",
    "https://openalex.org/W3136062890",
    "https://openalex.org/W4286588134",
    "https://openalex.org/W4312590184",
    "https://openalex.org/W7048135330",
    "https://openalex.org/W4390874023",
    "https://openalex.org/W4312767675",
    "https://openalex.org/W4312717156",
    "https://openalex.org/W6794906783",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4393160566",
    "https://openalex.org/W6779348065",
    "https://openalex.org/W2134273960",
    "https://openalex.org/W2533598788",
    "https://openalex.org/W3179495697",
    "https://openalex.org/W6803843097",
    "https://openalex.org/W3034877463",
    "https://openalex.org/W2783873922",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W4286750734",
    "https://openalex.org/W6757204547",
    "https://openalex.org/W6811274459",
    "https://openalex.org/W3211190672",
    "https://openalex.org/W4389821034",
    "https://openalex.org/W3166395393",
    "https://openalex.org/W3177462300",
    "https://openalex.org/W4312734855",
    "https://openalex.org/W4313047844",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W4313009170",
    "https://openalex.org/W2962847160",
    "https://openalex.org/W4389072615",
    "https://openalex.org/W4377130896",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W4312617935",
    "https://openalex.org/W4386071640",
    "https://openalex.org/W3010604601",
    "https://openalex.org/W3104688113",
    "https://openalex.org/W2894740066",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W4229725817",
    "https://openalex.org/W4287324314",
    "https://openalex.org/W206690697",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3204801262",
    "https://openalex.org/W4317941616",
    "https://openalex.org/W4313007769",
    "https://openalex.org/W3172099915",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W4378506785",
    "https://openalex.org/W4287257983",
    "https://openalex.org/W4283712666"
  ],
  "abstract": "The substantial success of Vision Transformer (ViT) in computer vision tasks is largely attributed to the architecture design. This underscores the necessity of efficient architecture search for designing better ViTs automatically. As training-based architecture search methods are computationally intensive, there’s a growing interest in training-free methods that use zero-cost proxies to score ViTs. However, existing training-free approaches require expert knowledge to manually design specific zero-cost proxies. Moreover, these zero-cost proxies exhibit limitations to generalize across diverse domains. In this paper, we introduce Auto-Prox, an automatic proxy discovery framework, to address the problem. First, we build the ViT-Bench-101, which involves different ViT candidates and their actual performance on multiple datasets. Utilizing ViT-Bench-101, we can evaluate zero-cost proxies based on their score-accuracy correlation. Then, we represent zero-cost proxies with computation graphs and organize the zero-cost proxy search space with ViT statistics and primitive operations. To discover generic zero-cost proxies, we propose a joint correlation metric to evolve and mutate different zero-cost proxy candidates. We introduce an elitism-preserve strategy for search efficiency to achieve a better trade-off between exploitation and exploration. Based on the discovered zero-cost proxy, we conduct a ViT architecture search in a training-free manner. Extensive experiments demonstrate that our method generalizes well to different datasets and achieves state-of-the-art results both in ranking correlation and final accuracy. Codes can be found at https://github.com/lilujunai/Auto-Prox-AAAI24.",
  "full_text": "Auto-Prox: Training-Free Vision Transformer Architecture Search via Automatic\nProxy Discovery\nZimian Wei1, Peijie Dong2, Zheng Hui3, Anggeng Li4, Lujun Li5*, Menglong Lu1, Hengyue Pan1*,\nDongsheng Li1*\n1 National University of Defense Technology\n2The Hong Kong University of Science and Technology (Guangzhou)\n3Columbia University\n4Huawei\n5The Hong Kong University of Science and Technology\n{weizimian16,lumenglong,hengyuepan,dsli}@nudt.edu.cn,{lilujunai,dongpeijie98}@gmail.com\nZh2483@columbia.edu,anggeng.li@outlook.com\nAbstract\nThe substantial success of Vision Transformer (ViT) in com-\nputer vision tasks is largely attributed to the architecture de-\nsign. This underscores the necessity of efficient architecture\nsearch for designing better ViTs automatically. As training-\nbased architecture search methods are computationally in-\ntensive, there’s a growing interest in training-free methods\nthat use zero-cost proxies to score ViTs. However, existing\ntraining-free approaches require expert knowledge to manu-\nally design specific zero-cost proxies. Moreover, these zero-\ncost proxies exhibit limitations to generalize across diverse\ndomains. In this paper, we introduce Auto-Prox, an auto-\nmatic proxy discovery framework, to address the problem.\nFirst, we build the ViT-Bench-101, which involves differ-\nent ViT candidates and their actual performance on multiple\ndatasets. Utilizing ViT-Bench-101, we can evaluate zero-cost\nproxies based on their score-accuracy correlation. Then, we\nrepresent zero-cost proxies with computation graphs and or-\nganize the zero-cost proxy search space with ViT statistics\nand primitive operations. To discover generic zero-cost prox-\nies, we propose a joint correlation metric to evolve and mu-\ntate different zero-cost proxy candidates. We introduce an\nelitism-preserve strategy for search efficiency to achieve a\nbetter trade-off between exploitation and exploration. Based\non the discovered zero-cost proxy, we conduct a ViT archi-\ntecture search in a training-free manner. Extensive experi-\nments demonstrate that our method generalizes well to dif-\nferent datasets and achieves state-of-the-art results both in\nranking correlation and final accuracy. Codes can be found\nat https://github.com/lilujunai/Auto-Prox-AAAI24.\nIntroduction\nRecently, Vision Transformer (ViT) (Dosovitskiy et al.\n2020a) has achieved remarkable performance in image clas-\nsification (Liang et al. 2022; Jiang et al. 2021; Chen, Fan,\nand Panda 2021), object detection (Wu et al. 2022), semantic\nsegmentation (Dong et al. 2021), and other computer vision\ntasks (Li et al. 2021b; Liu et al. 2021). Despite these ad-\nvancements, the manual trial-and-error method of designing\n*Corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Kendall (KT) & Spearman (SP) ranking correla-\ntions of zero-cost proxies on AutoFormer (Left) and PiT\n(Right) search space for four datasets including CIFAR-\n100, Flowers, Chaoyang, and ImageNet. Results demon-\nstrate that our proposed Auto-Prox significantly outperforms\nSynflow (Tanaka et al. 2020) and TF-TAS (Zhou et al. 2022).\nViT architectures becomes impractical given the expanding\nneural architecture design spaces and intricate application\nscenarios (Liu et al. 2023; Li et al. 2023b; Li and Jin 2022;\nLi et al. 2023a, 2022d,c; Li 2022; Shao et al. 2023). Neural\nArchitecture Search (NAS) aims to address this issue by au-\ntomating the design of neural network architectures. Tradi-\ntional training-based architecture search methods (Xie et al.\n2019; Wei et al. 2023; Hu et al. 2021; Dong et al. 2022;\nChen et al. 2022; Dong, Li, and Wei 2023; Dong et al. 2023;\nLu et al. 2024; Zimian Wei et al. 2024) involve training and\nevaluating numerous candidate ViTs, which can be compu-\ntationally expensive and time-consuming. Therefore, there\nis a need for a more efficient architecture search of ViT.\nRecent training-free NAS methods, such as NWOT (Mel-\nlor et al. 2021) and TF-TAS (Zhou et al. 2022), have received\ngreat research interest due to their meager costs. These\nmethods utilize hand-crafted zero-cost proxies (Tanaka et al.\n2020; Chen, Gong, and Wang 2020), which are conditional\non the model’s parameters or gradients, to predict the ac-\ntual accuracy ranking without the expensive training pro-\ncess. However, there are still some drawbacks limiting their\nbroader application: (1) Dependency on expert knowledge\nand extensive tuning. Lots of traditional zero-cost proxies\nare transferred from different areas with extensive expert in-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n15814\ntuition and time-consuming tuning processes. In addition,\nthese hand-crafted zero-cost proxies can be influenced by\nhuman biases and limited by the designer’s experience. (2)\nGenerality and flexibility. Hand-crafted zero-cost proxies\nmay perform well on the specific problem but can not gen-\neralize well to new or unseen datasets or tasks (see Fig-\nures 1). These zero-cost proxies usually have fixed formu-\nlas, and some do not associate with input or labels of the\ntarget dataset. i.e., scores of the same architecture on differ-\nent datasets are the same, which does not match the facts.\nThus, two problems are naturally raised: (1) How to effi-\nciently discover the proxies without expert knowledge? (2)\nHow to reduce the gap between fixed zero-cost proxy and\nvariable tasks?\nFor the first problem, we propose Auto-Prox, a from-\nscratch automatic proxy search framework, as an alternative\nto traditional manual designs. Unlike hand-crafted zero-cost\nproxies, Auto-Prox mitigates human bias and automates the\nexploration of more expressive and efficient zero-cost prox-\nies. First, we establish the ViT-Bench-101 dataset, which\ncomprises diverse ViT architectures and their corresponding\nperformance on multiple datasets. ViT-Bench-101 provides\na benchmark for evaluating the score-accuracy correlations\nof different zero-cost proxies. We then define the zero-cost\nproxy search space, which includes ViT’s weights and gra-\ndient statistics as candidate inputs, potential unary and bi-\nnary mathematical operations, and computation graphs as\nrepresentations of zero-cost proxies. In our exploration of\nthe proxy search space, we introduce an elitism preserva-\ntion strategy to enhance search efficiency. This strategy in-\nvolves judgment in the evolutionary search to preserve high-\nperforming zero-cost proxies.\nFor the second problem, we propose a joint correlation\nmetric, designed as the objective for the evolutionary proxy\nsearch. Instead of solely optimizing the automatic proxy\nbased on high score-accuracy correlation within a single\ndataset, this metric captures the weighted average of corre-\nlations spanning multiple datasets. Based on the joint corre-\nlation metric, Auto-Prox improves generalization, allowing\nfor discovering zero-cost proxies that perform well on new\nor unseen datasets or tasks.\nWe conduct extensive experiments on CIFAR-100, Flow-\ners, Chaoyang (Zhu et al. 2021), and ImageNet-1K to val-\nidate the superiority of our proposed method. For small\ndatasets, except for ImageNet-1K, we focus on distillation\naccuracy instead of vanilla accuracy for ViTs, in contrast\nto traditional NAS experiments. The experiments demon-\nstrate that our Auto-Prox can achieve better distillation accu-\nracy than other zero-cost proxies when searched in the same\nsearch spaces. Moreover, Auto-Prox obtains state-of-the-art\nranking correlation across multiple datasets, significantly\nsurpassing existing training-free NAS approaches (see Fig-\nure 1) without prior knowledge.\nMain Contributions:\n• We focus on a training-free architecture search for ViTs\nacross multiple datasets. We build ViT-Bench-101 and\ndiscover the failures in the generalization of existing\ntraining-free methods.\n• We propose a from-scratch proxy search framework,\nAuto-Prox, designed to eliminate the need for manual\nintervention and enhance generalizability. We propose\na joint correlation metric to evolve different zero-cost\nproxy candidates and an elitism-preserve strategy to im-\nprove search efficiency.\n• We experimentally validate that Auto-Prox achieves\nstate-of-the-art performance across multiple datasets and\nsearch spaces, advancing the broader application of ViTs\nin vision tasks.\nRelated Work\nVision Transformer (ViT) (Dosovitskiy et al. 2020a) has\nshown remarkable performance in various visual recogni-\ntion tasks due to its ability to capture long-range depen-\ndencies. Recently, researchers have developed several au-\ntomated techniques to discover more effective ViT archi-\ntectures. For instance, AutoFormer (Chen et al. 2021a) uti-\nlizes a one-shot NAS framework for ViT-based architec-\nture search, while BossNAS (Li et al. 2021a) incorporates\na hybrid CNN-transformer search space along with a self-\nsupervised training scheme. ViTAS (Su et al. 2021b) has\ndeveloped a cyclic weight-sharing mechanism for token em-\nbeddings of ViTs to stabilize the training of Superformer and\nprevent catastrophic failures. Despite significant progress\nin ViT architecture search, the aforementioned one-shot-\nbased methods are still computationally demanding. TF-\nTAS (Zhou et al. 2022) stands as the first method to con-\nduct a training-free architecture search for ViTs, assessing\nViT by merging two theoretical perspectives: synaptic diver-\nsity from multi-head self-attention layers (MSA) and synap-\ntic saliency from multi-layer perceptrons (MLP). However,\ncurrent training-free ViT architecture search methods strug-\ngle to generalize across various domains. In this paper,\nwe introduce an automated approach to pinpoint excellent\nproxies for ViTs across diverse datasets. Comparing with\nEZNAS: Auto-Prox differs notably from EZNAS (Akhauri\net al. 2022) from the following aspects: (1) EZNAS is only\ndesigned for CNNs, but our approach is customized for\nViTs. (2) EZNAS simply follows the search strategy from\nAutoML-Zero. In contrast, we propose an elitism-preserve\nstrategy that significantly improves search efficiency and re-\nsults (see Figure 5). (3) Our proposed Joint Correlation Met-\nric enhances ranking consistency across multiple datasets,\nencompassing both small and large-scale datasets, such as\nImageNet-1K. In contrast, EZNAS’s testing is limited to\nsmall datasets.\nMethodology\nIn this section, we first introduce the search space of our au-\ntomatic zero-cost proxy. We then delve into the details of\nthe joint correlation metric and the elitism-preserve strategy\nduring the evolutionary process. Subsequently, we give an\nanalysis of the searched zero-cost proxy. Finally, we illus-\ntrate the training-free ViT search process.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n15815\nCifar100 Flowers Chaoyang ImageNet\nJoint Correlation Metric\nH\nMul\nInput\nUnary OP\nSearch Space\nCifar-100\n Flowers Chaoyang ImageNet\nMultiple Datasets Inputs\nPow\nLog Exp\nAdd\nNo\nOutput\nAbs Norm\nInvert No\nAdd\nOutput\nPow\nLog Exp\nAdd\nNo\nOutput\nPow\nLog Exp\nAdd\nNo\nOutput\nAbs Pow\nLog Exp\nAdd\nOutput\nPow\nLog Exp\nAdd\nNo\nOutput\nLast Population\nNext Population\nBinary OP\nPow\nLog Exp\nAdd\nAbs\nOutput\nChild\nBest JCM as parent\nElitism-Preserve Strategy\nPow\nLog Exp\nAdd\nNo\nOutput\nParent\nJCM(P)=0.43 JCM(C)=0.56\nJCM=0.36 JCM=0.43JCM=0.29\nJCM=0.36 JCM=0.43JCM=0.56Input\nFigure 2: Illustration of the Auto-Prox search process. First, we devise a comprehensive search space, incorporating primitive\noperations, ViT statistics, and computation graphs to represent zero-cost proxies. We then randomly sample candidate zero-\ncost proxies to initialize the population and evaluate their ranking consistency using the Joint Correlation Metric across four\ndatasets. Based on the JCM score, we pick up promising ones as parents and perform mutation to generate a new population.\nSubsequently, we perform the elitism-preserve strategy to prevent the deterioration of the population.\nSearch Space of Automatic Zero-cost Proxy\nTo guarantee the effectiveness and flexibility of the zero-cost\nproxy search space, we begin by revisiting the formulations\nof existing zero-cost proxies. Using this foundational under-\nstanding, we design a search space that encompasses eight\ninput candidates, and 56 primitive operations. This compre-\nhensive search space allows us to explore a wide range of\nzero-cost proxies, uncovering potential ones that previous\nhand-crafted approaches may have overlooked.\nReview of Existing Zero-cost Proxies To investigate the\ndesign of zero-cost proxies, we summarize the existing\nones in Table 1. Among these zero-cost proxies, Fisher\n(Theis et al. 2018), SNIP (Lee, Ajanthan, and Torr 2018),\nPlain (Mozer and Smolensky 1988), and SynFlow (Tanaka\net al. 2020) are conducted on ReLU-Conv2D-BatchNorm2D\nblocks in CNNs. In contrast, TF-TAS (Zhou et al. 2022) is\nbased on transformer layers in ViTs. The inputs of these\nzero-cost proxies derive from the following types of net-\nwork statistics: Activation (A), Gradient (G), and Weight\n(W). Specifically, Fisher computes the sum over all gradi-\nents of the activations ∂L\n∂z in the network, which can be used\nfor channel pruning. SNIP, employing weightθ and gradient\n∂L\n∂θ as inputs, computes a saliency metric at initialization us-\ning a single mini-batch of data. This metric approximates the\nchange in loss when a specific parameter is removed. Syn-\nFlow, also using weight θ and gradient ∂R\n∂θ as inputs, intro-\nduces a modified version of synaptic saliency scores to pre-\nvent layer collapse during parameter pruning. TF-TAS con-\nsiders the weights θl, θk and their gradients ∂L\n∂θl\n, ∂L\n∂θk\nfrom\nmulti-head self-attention (MSA) and multi-layer perceptron\n(MLP) as inputs, respectively. ∥∥n is the Nuclear-norm.\nInput Proxy Formula\nA&G Fisher P\nz∈A\n\u0000∂L\n∂z z\n\u00012\nW&G SNIP\n\f\f(∂L\n∂θ ) ⊙ θ\n\f\n\f\nW&G Plain (∂L\n∂θ ) ⊙ θ\nW&G SynFlow (∂R\n∂θ ) ⊙ θ, R = 1T \u0000Q\nθi∈W |\nθi|\n\u0001\n1\nW&G TF-TAS P\nθl\n\r\n\r\r( ∂L\n∂θl\n)\n\r\r\r\nn\n⊙ ∥θl∥n + P\nθk\n( ∂L\n∂θk\n) ⊙ θk\nTable 1: Overview of existing zero-cost proxies. A, W, and\nG refer to Activation, Weight, and Gradient.\nViT Statistics as Input The input subsection in Table 1\nexplores the input choices for constructing zero-cost prox-\nies, in which activations (A), gradients(G), and weights(W)\nare identified as the most informative and effective options.\nActivation provides insight into the data distribution and in-\nternal network representations, while gradients highlight the\nsensitivity of weights to the loss functions, and weights re-\nflect the significance of network parameters. Building on this\nobservation, we register the activation, weights, and corre-\nsponding gradients from each transformer layer as potential\ninputs of our zero-cost proxy. For modules in the multi-head\nself-attention (MSA), we collect their weights and gradi-\nents at initialization using a mini-batch of data. Regarding\nmulti-layer perception (MLP), we consider weights, gradi-\nents, activation, and activation gradients as possible inputs.\nTo differentiate between these inputs, we employ symbols\nlike F1 and F1g. As depicted in Figure 3, for a transformer\nlayer, there are a total of eight potential inputs in the zero-\ncost proxy search space.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n15816\nConcat\nLinear\nScaled Dot-Product Attention\nLinear\nV K Q\nW\nF2 F2g\nW\n G\nF3 F3g\nLinear\nGELU\nLinear\nA\n G\nF1 F1g\nW\n G\nF4 F4g\nTransformer Encoder\nMulti-Head \nSelf-Attention \nLayer Norm\nLayer Norm\nMLP\n Embedded Patches \nG\nLinear\n Linear\nFigure 3: ViT statistics of the zero-cost proxy search space,\nincluding activation (A), gradient (G), and weights (W) from\nMSA and MLP modules.\nPrimitive Operations To efficiently aggregate informa-\ntion from different types of inputs, we search among dif-\nferent primitive operations to produce the final scalar out-\nput. In the context of the zero-cost proxy, primitive opera-\ntions are used to process ViT statistics, resulting in the zero-\ncost proxy score for performance evaluation. We consider\ntwo types of primitive operations, including unary opera-\ntions (operations with only one operand) and binary opera-\ntions (operations with two operands). Inspired by AutoML-\nbased methods (Li et al. 2022a; Real et al. 2020; Dong et al.\n2023), we provide a total of 24 unary operations and four\nbinary operations to form the zero-cost proxy search space.\nSince the intermediate variables can be scalar or matrix, the\ntotal number of operations is 56.\nZero-cost Proxy as Computation Graph. The automatic\nzero-cost proxy is represented as a computation graph, in\nwhich the input nodes are ViT statistics, and the intermediate\nnodes are primitive operations. The graph’s output yields the\nproxy score used for ViT ranking. There are four main types\nof computation graphs, namely Linear, Tree, Graph (DAG),\nand unstructured memory-based structures (such as Automl-\nzero (Real et al. 2020)). The expressiveness of the compu-\ntation graph increases from Linear to unstructured memory-\nbased structures, but the valid structure in the search space\ndecreases. To balance the trade-off between expressiveness\nand validity, we employ an expression tree to represent the\nautomatic zero-cost proxy. Based on previous works such as\nSNIP (Lee, Ajanthan, and Torr 2018), and TF-TAS (Zhou\net al. 2022), most proxies typically require two types of in-\nputs. Therefore, we build an expression tree with two inputs\n(see Figure 2). The expression tree is applied to every trans-\nformer layer of ViT, with the final proxy score derived by\naveraging the outputs from all transformer layers.\nEvolving Automatic Proxy on Multiple Datasets\nFigure 2 presents the automatic proxy search. At initializa-\ntion, we randomly sample a population ofN candidate zero-\ncost proxies from the search space, ensuring that they are\nAlgorithm 1: Evolutionary Search for Auto-Prox\nInput: Search space S, population P, max iteration T , sam-\nple ratio r, sampled pool R, topk k, margin m.\nOutput: Auto-prox with best JCM.\n1: P0 := Initialize population(Pi);\n2: Sample pool R := ∅;\n3: for i = 1,2, . . . ,T do\n4: Clear sample pool R := ∅;\n5: Randomly select R ∈ P;\n6: Candidates Gik := GetTopk(R, k);\n7: Parent Gp\ni := RandomSelect(Gik);\n8: Mutate Gm\ni := MUTATE(Gp\ni );\n9: // Elitism-Preserve Strategy.\n10: if JCM(Gm\ni ) − JCM(Gp\ni ) ≥ m then\n11: Append Gm\ni to P;\n12: else\n13: Go to line 8;\n14: end if\n15: Remove the zero-cost proxy with the lowest JCM.\n16: end for\nvalid primitively. Each of these proxies is then evaluated us-\ning the proposed Joint Correlation Metric, which serves as\nthe fitness measure in our evolutionary search process. In\neach iteration of the evolutionary process, the top-k candi-\ndates with the highest JCM scores are selected. From this\nsubset, a parent is randomly picked for mutation. When con-\nducting mutation, a random point in the computation graph\nis chosen and mutated using new inputs or primitive oper-\nations. To guarantee the quality of mutated zero-cost prox-\nies and stave off degradation, we’ve introduced the Elitism-\nPreserve Strategy. This strategy involves judgment and se-\nlectively retaining only valid and promising zero-cost prox-\nies for the next generation. We repeat this process for T it-\nerations to identify the target zero-cost proxy. In the follow-\ning, we introduce details of the Joint Correlation Metric and\nElitism-Preserve Strategy.\nJoint Correlation Metric (JCM) Instead of measuring\nranking correlation in just one dataset, we propose the Joint\nCorrelation Metric (JCM), which measures the generaliza-\ntion of proxies Q among multiple datasets. We use M\ndatasets {Di}M\ni=0 and the weight {αi}M\ni=0 to measure the\ncorresponding importance of datasets. JCM is formulated as\nthe weighted sum of the ranking correlation (Kendall’s Tau\nτ) as follows:\nJCM (Q) = 1\nM\nMX\ni\nαi × τ(Di, Q) (1)\nwhere τ is the ranking correlation between the zero-cost\nproxy score and actual accuracy on dataset Di.\nElitism-Preserve Strategy To prevent population deteri-\noration and premature convergence, the proposed Elitism-\nPreserve Strategy involves comparing the performance of\nthe parent and newly generated offspring to measure the va-\nlidity of the mutation. If the offspring is invalid or if its per-\nformance is lower than that of the parent, the mutation is\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n15817\nRandom initialized Candidate ViT\nsearch\nBest architecture\nScore:      73.5              48.24                       39.03   \nMHA Choice\nMLP Choice\nMHA Choice\nMLP Choice\nMHA Choice\nMLP Choice Population of ViT\n……\nInput\nAuto-Prox\nFigure 4: Illustration of the training-free ViT search process. Left: The search space of ViT, where multi-self attention and\nmulti-layer perceptron are mutable. Right: Traverse the search space and select the best ViT with the highest Auto-Prox score.\nconsidered as deterioration, and therefore new offspring are\ngenerated as a replacement. Algorithm 1 presents the de-\ntailed process.\nAnalysis of the Searched Zero-cost Proxy\nAuto-Prox needs ViT statistics as its input. To eliminate\nthe potential bias introduced by ViTs from a specific de-\nsign space, we sampled ViT-accuracy configurations from\ntwo distinct design spaces of ViT-Bench-101 and performed\nzero-cost proxy searches on each. As a result, we discovered\ntwo separate zero-cost proxies, AutoProxA and AutoProxP,\nwhich are used independently to score ViTs sampled from\ntheir corresponding search spaces. It is important to note that\nAuto-Prox is an automated, from-scratch method, making it\nversatile across different ViT search spaces and datasets. Be-\nlow, we present the formulas for the searched proxies within\nthe AutoFormer (AutoProxA) and PiT search spaces (Auto-\nProxP):\nAutoProxA =|∂L\n∂θl\n| + 1\nn + ϵ\nnX\ni=1\nsigmoid\n\u0012 ∂L\n∂θk\n\u0013\n(2)\nAutoProxP =∥sigmoid (θl)∥F − log\n\u0012 exp (∥θk∥)P\ni exp (∥θk∥)\n\u0013\n(3)\nwhere θl is the weight parameter matrix of QKV layers in\nthe multi-head self-attention (MSA) module, ∂L\n∂θl\nis the cor-\nresponding gradient matrix. θk is the weight parameter ma-\ntrix of linear layers in the MLP module. ∂L\n∂θk\nrepresents the\ncorresponding gradient matrix. n is the number of elements.\nϵ is the constant set as 1e-9. P\ni means the sum of elements\nin the i-th dimension. ∥∥F means the Frobenius-norm.\nBy analyzing the formulas, we found that high-\nperforming ViTs correlate positively with the following fac-\ntors: (1) A larger norm of weight parameters or gradients in\nQKV layers of the MSA module, which approximately indi-\ncates the diversity of MSA (Dong, Cordonnier, and Loukas\n2021). (2) More salient weight parameters in linear layers\nof the MLP module, implying a significant impact on per-\nformance. Moreover, we compare the searched proxies with\nexisting TF-TAS (Zhou et al. 2022), which is hand-crafted\nand is included in our zero-cost proxy search space. Table 3\nand Table 2 have demonstrated the superiority of Auto-Prox,\nwhich significantly outperforms the TF-TAS method and\nmeanwhile enjoys more search efficiency, further emphasiz-\ning the advantages of automatic searching.\nTraining-free ViT Search\nOnce we have obtained a good zero-cost proxy through the\nevolutionary search, we utilize it to perform a training-free\nViT search. Figure 4 presents the training-free ViT search\nprocess. Specifically, we randomly explore a large number\nof candidate architectures from the ViT search space. Then,\nwe evaluate each ViT architecture and select the best one\nbased on the searched zero-cost proxy (Auto-Prox) score.\nWithout the need for costly and time-consuming training,\nthe ViT search process is highly efficient.\nExperiments\nViT-Bench-101\nViT-Bench-101 provides ground-truth accuracy of ViTs on\nboth tiny datasets and large-scale datasets. For the tiny\ndatasets, we employ CIFAR-100 (Krizhevsky 2009), Flow-\ners (Nilsback and Zisserman 2008), and Chaoyang (Zhu\net al. 2021), while for the large-scale datasets, we focus\non ImageNet-1K. Motivated by findings from (Li et al.\n2022b), which show that ViTs achieve significant gains\non tiny datasets when distilled from an efficient CNN\nteacher network, we include distillation accuracy for ViTs\non these datasets using a given teacher. Specifically, for\nthe smaller datasets excluding ImageNet-1K, ViT-Bench-\n101 offers both distillation accuracy and vanilla accuracy\nfor ViTs sampled from AutoFormer and PiT search spaces.\nThis supports the evaluation of zero-cost proxies based on\nthe score-accuracy correlation in different scenarios, with or\nwithout distillation. Regarding the ImageNet-1K dataset, we\nfollow the demonstration in (Chen et al. 2021a; Zhou et al.\n2022), showing that the sub-nets with inherited weights from\nthe pre-trained AutoFormer supernet can achieve perfor-\nmance comparable to the same network when retrained from\nscratch. Thus, we sample ViTs from the AutoFormer search\nspace and collect their performance by inheriting weights\nfrom the publicly available supernet.\nImplementation Details\nEvolutionary Zero-cost Proxy Search We partition the\nwhole ViT-Bench-101 dataset into a validation set (60%)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n15818\nSearch Space Proxy CIFAR-100 Flowers Chaoyang\nKendall Spearman Pearson Kendall Spearman Pearson Kendall Spearman Pearson\nAutoFormer\nGraSP 0.84±0.73 1.35 ±0.92 0.82 ±1.58 0.82±1.58 -7.33±0.10 -4.14±0.84 -4.42±0.38 -6.53±0.36 6.26±0.72\nSynFlow 37.66±0.63 52.89±1.11 52.01±0.74 62.59±0.01 82.13±0.01 62.26±7.71 27.87±0.75 39.30±1.51 41.08±1.14\nTENAS -30.03±0.28 -43.27±0.46 -42.66±0.26 -53.55±0.05 -73.79±0.11 -54.17±8.31 -27.81±0.11 -39.69±0.24 -40.79±0.18\nNWOT 54.65±0.22 63.11±0.26 60.01±0.17 68.16±0.03 82.06±1.07 53.91±7.46 27.29±0.25 38.96±0.53 40.57±0.56\nTF-TAS 35.89±0.26 50.84±0.58 50.51±0.46 63.90±0.04 83.28±0.09 62.80±8.58 27.14±0.35 38.52±0.68 40.57±0.40\nOurs 55.67±0.74 63.87±1.04 60.56±0.93 69.19±2.03 83.65±0.93 79.52±7.15 33.76±0.46 41.76±0.66 42.63±0.45\nPiT\nGraSP -42.02±0.58 -58.71±0.74 -31.53±0.09 -50.66±0.07 -69.64±0.09 -40.90 ±0.57 -16.00±0.61 -22.94±1.33 -19.07±0.56\nSynFlow 69.79±1.16 87.05±0.77 70.80±0.08 62.22±2.38 79.98±2.16 71.66±0.99 30.96±4.38 42.66±8.46 39.24±5.89\nTENAS -2.13±0.30 -3.21±0.74 -1.68±0.17 -2.86±0.63 -4.23±1.46 -3.33±1.49 -3.34±0.03 -5.04±0.07 -3.55±0.13\nNWOT -2.61±0.01 -4.13±0.01 -0.52±1.04 2.67±0.23 3.69±0.49 0.71±0.46 4.73±0.16 6.87±0.31 4.43±0.26\nTF-TAS 63.83±0.06 82.20±0.04 58.37±1.84 64.48±0.08 82.91±0.06 67.23±0.79 37.99±1.54 52.92±2.54 42.68±0.62\nOurs 82.07±0.33 95.12±0.09 73.25±1.30 79.25±0.94 92.94±0.33 79.53±0.14 41.67±4.45 55.09±7.50 49.17±3.94\nTable 2: Ranking correlation results (%) on CIFAR-100, Flowers, and Chaoyang. Auto-Prox achieves the highest ranking\ncorrelation with distillation accuracy on all three datasets of the ViT-Bench-101, demonstrating superior generalization.\nSearch Space Proxy\nCIFAR-100 Flowers Chaoyang\nParam(M) Dis.Acc(%) Search\nCost Param(M) Dis.Acc(%) Search\nCost Param(M) Dis.Acc(%) Search\nCost\nAutoFormer\nRandom 8.30 76.72 N/A 6.18 67.64 N/A 6.54 84.20 N/A\nGraSP 5.77 77.53 2.62 h 5.29 66.08 2.71 h 6.12 84.81 1.52 h\nSynFlow 9.52 77.83 1.91 h 8.13 68.61 1.85 h 5.82 83.87 1.77 h\nTENAS 5.40 75.54 4.83 h 5.25 66.69 4.82 h 5.16 84.53 4.80 h\nNWOT 8.36 76.79 3.24 h 8.87 69.18 3.24 h 5.53 84.71 3.07 h\nTF-TAS 5.25 75.72 1.94 h 5.80 67.57 1.87 h 5.60 84.57 1.92 h\nOurs 9.11 78.26 0.70 h 9.80 69.71 0.85 h 8.97 84.85 0.85 h\nPiT\nRandom 5.33 75.84 N/A 4.88 65.30 N/A 5.24 82.94 N/A\nGraSP 4.53 76.03 1.24 h 3.72 66.58 1.85 h 4.63 83.87 0.86 h\nSynFlow 11.05 77.13 1.08 h 5.23 68.12 0.99 h 4.93 83.73 0.70 h\nTENAS 6.93 76.09 5.14 h 4.26 68.03 5.14 h 6.76 83.64 5.07 h\nNWOT 5.21 76.64 3.02 h 10.77 67.72 3.09 h 6.37 83.31 3.08 h\nTF-TAS 16.07 77.06 1.21 h 10.30 68.21 0.95 h 4.32 84.34 0.71 h\nOurs 6.22 77.26 0.51 h 6.20 68.85 0.39 h 4.49 84.53 0.31 h\nTable 3: Comparing the distillation performance on three datasets of ViTs sampled from Autoformer and PiT search spaces,\nAuto-Prox achieves competitive results with the lowest search cost (measured on a single NVIDIA A40 GPU).\nfor proxy searching and a test set (40%) for proxy evalu-\nation. There is no overlap between these two sets. In the\nevolutionary search process, we employ a population size of\nP = 20, and the total number of iterations T is set to 200.\nTo evaluate zero-cost proxy candidates, we randomly sam-\nple 100 ViT ground-truth configurations from ViT-Bench-\n101 and measure the ranking consistency between its zero-\ncost proxy score and actual accuracy. We then calculate the\nJoint Correlation Metric based on the ranking consistencies\non multiple datasets as the fitness function. When conduct-\ning mutation, the probability of mutation for a single node in\na zero-cost proxy representation is set to 0.5. The margin m\nin the Elitism-Preserve Strategy is 0.1. The zero-cost proxy\nsearch process is conducted on a single NVIDIA A40 GPU\nand occupies the memory of only one ViT.\nTraining-free ViT Search Based on the Auto-Prox score,\nthe ViT search process is efficient since gradient back-\npropagation is not included. we randomly sample 400 ViTs\nfrom AutoFormer (Chen et al. 2021a) and PiT (Zhou et al.\n2022) search spaces. The parameter intervals of ViTs from\nAutoFormer and PiT search spaces in our experiments are\nModels Param Acc Gd.\nDeit-Ti (Touvron\net al. 2021) 5.7 72.2 -\nTNT-Ti (Han\net al. 2021) 6.1 73.9 -\nViT-Ti\n(Dosovitskiy et al. 2020b) 5.7 74.5 -\nPVT-Tiny\n(Wang et al. 2021) 13.2 75.1 -\nViTAS-C (Su\net al. 2021a) 5.6 74.7 32\nAutoFormer-Ti\n(Chen et al. 2021b) 5.7 74.7 24\nTF-TAS-Ti (Zhou\net al. 2022) 5.9 75.3 0.5\nAuto-Prox (Ours) 6.4 75.6 0.1\nTable 4: ImageNet results on the AutoFormer search space.\nParam, Acc, and Gd. refer to parameter(M), accuracy(%),\nand GPU Days.\n4 ∼ 9 M and 2 ∼ 25 M, respectively. The final accuracy\nof the ViTs with the highest zero-cost proxy scores is re-\nported as the results of the ViT search process. The hyperpa-\nrameters for ViT retraining and building ViT-Bench-101 are\nadopted from (Li et al. 2022b).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n15819\nMethod Kendall Spearman Pear\nson\nSnip 14.61±1.55 30.62±6.03 49.45±10.68\nSynFlow 14.81±2.33 27.69±7.25 44.19±10.35\nNWOT 13.34±0.08 19.79±1.53 38.39±9.94\nTF-TAS 14.52±1.74 29.93±6.38 48.70±11.04\nAuto-Prox (ours) 25.44±0.90 37.10±1.86 51.84±10.07\nTable 5: Ranking Correlation Results (%) for ImageNet-1K\ndataset in ViT-Bench-101.\nFigure 5: Left: Comparison of naive evolutionary search,\nrandom search, and evolutionary search with Elitism-\nPreserve Strategy, which are denoted as ’Naive’, ’Random’,\nand ’EPS’. Right: Evolutionary process of ranking correla-\ntions on different datasets, and the proposed JCM.\nExperimental Results on Tiny Datasets\nTable 3 presents a comparison of distillation results obtained\nby using various zero-cost proxies on AutoFormer and PiT\nsearch spaces. In addition, we evaluate the ranking correla-\ntions of these zero-cost proxies under different experimental\nsettings using metrics such as Kendall’s tau (Abdi 2007),\nSpearman’s rho (Stephanou and Varughese 2021), and Pear-\nson’s correlation coefficient (Bowley 1928). As shown in\nboth Table 2 and Figure 6, Auto-Prox outperforms other\nexcellent zero-cost proxies. These findings demonstrate the\nimportance of using effective zero-cost proxies and the su-\nperiority of Auto-Prox in achieving higher performance in\nViT architecture search.\nExperimental Results on ImageNet-1K\nTo validate the effectiveness and superiority of our proposed\nAuto-Prox further, we evaluate its performance on the chal-\nlenging ImageNet-1K dataset, comparing its ranking cor-\nrelation and top-1 classification accuracy with other hand-\ncrafted and automatically searched ViT methods. The re-\nsults, as presented in Table 4, demonstrate that the optimal\nViT architecture searched by Auto-Prox outperforms both\nexcellent hand-crafted and other automatically searched ViT\nmethods, underscoring the superiority of our proposed ap-\nproach. Importantly, our approach strikes a good balance be-\ntween performance and search efficiency, requiring only 0.1\nGPU days for ViT architecture search, making it a practical\nand efficient approach for ViT architecture search. Further-\nmore, the ranking correlation results in Table 5 show that\nour proposed Auto-Prox performs better than other zero-cost\nproxies on the challenging ImageNet task, further validating\nthe effectiveness and generalization of our approach.\nFigure 6: Correlation of distillation accuracy and Auto-\nProx scores on AutoFormer search space (top left: CIFAR-\n100, top right: Flowers) and PiT search space (bottom left:\nCIFAR-100, bottom right: Flowers).\nAblation Study\nAs shown in Figure 5 (left), we observe that the proposed\nelitism-preserve strategy significantly enhances search effi-\nciency, leading to better results. The findings underscore the\nimportance of preserving the top-performing zero-cost prox-\nies during the evolutionary process to achieve better search\nefficiency. Moreover, Figure 5 (right) reveals that optimiz-\ning the Joint Correlation Metric facilitates the discovery of\nhigh-performing zero-cost proxies across multiple datasets.\nWe also conducted a zero-cost proxy search using vanilla\naccuracy provided by ViT-Bench-101.\nConclusion\nIn this paper, we present Auto-Prox, an automated proxy dis-\ncovery framework designed for generality across multiple\ndata domains. To facilitate the evaluation of zero-cost prox-\nies, we propose the ViT-Bench-101 dataset as a standardized\nbenchmark. We design a search space of automatic zero-\ncost proxies for ViTs and develop a joint correlation met-\nric to optimize genetic proxy candidates. Additionally, we\nintroduce an elitism preservation strategy to enhance search\nefficiency. With the searched proxy, we conduct a ViT ar-\nchitecture search in a training-free manner, achieving sig-\nnificant accuracy gains. Extensive experiments validate the\nefficiency and effectiveness of Auto-Prox across multiple\ndatasets and search spaces. We hope this elegant and practi-\ncal approach will inspire more investigation into the gener-\nalization of training-free ViT search methods.\nAcknowledgements\nThis work is supported by the National Natural Science\nFoundation of China (No.62025208), the Open Project of\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n15820\nKey Laboratory (2022-KJWPDL-06), and the Xiangjiang\nLaboratory Fund (No.22XJ01012).\nReferences\nAbdi, H. 2007. The Kendall rank correlation coefficient.\nEncyclopedia of measurement and statistics, 2: 508–510.\nAkhauri, Y .; Munoz, J.; Jain, N.; and Iyer, R. 2022. EZNAS:\nEvolving Zero-Cost Proxies For Neural Architecture Scor-\ning. Advances in Neural Information Processing Systems,\n35: 30459–30470.\nBowley, A. 1928. The standard deviation of the correlation\ncoefficient. Journal of the American Statistical Association,\n23(161): 31–34.\nChen, C.-F. R.; Fan, Q.; and Panda, R. 2021. Crossvit:\nCross-attention multi-scale vision transformer for image\nclassification. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision.\nChen, K.; Yang, L.; Chen, Y .; Chen, K.; Xu, Y .; and Li,\nL. 2022. GP-NAS-ensemble: a model for the NAS Perfor-\nmance Prediction. In CVPRW.\nChen, M.; Peng, H.; Fu, J.; and Ling, H. 2021a. Auto-\nformer: Searching transformers for visual recognition. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 12270–12280.\nChen, M.; Peng, H.; Fu, J.; and Ling, H. 2021b. Auto-\nFormer: Searching Transformers for Visual Recognition. In\nICCV.\nChen, W.; Gong, X.; and Wang, Z. 2020. Neural Architec-\nture Search on ImageNet in Four GPU Hours: A Theoreti-\ncally Inspired Perspective. In ICLR.\nDong, P.; Li, L.; and Wei, Z. 2023. Diswot: Student archi-\ntecture search for distillation without training. In CVPR.\nDong, P.; Li, L.; Wei, Z.; Niu, X.; Tian, Z.; and Pan, H. 2023.\nEmq: Evolving training-free proxies for automated mixed\nprecision quantization. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, 17076–17086.\nDong, P.; Niu, X.; Li, L.; Xie, L.; Zou, W.; Ye, T.; Wei, Z.;\nand Pan, H. 2022. Prior-Guided One-shot Neural Architec-\nture Search. arXiv preprint arXiv:2206.13329.\nDong, X.; Bao, J.; Chen, D.; Zhang, W.; Yu, N.; Yuan, L.;\nChen, D.; and Guo, B. 2021. CSWin Transformer: A Gen-\neral Vision Transformer Backbone with Cross-Shaped Win-\ndows. ArXiv, abs/2107.00652.\nDong, Y .; Cordonnier, J.-B.; and Loukas, A. 2021. Atten-\ntion is not all you need: Pure attention loses rank doubly\nexponentially with depth. In International Conference on\nMachine Learning, 2793–2803. PMLR.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020a. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020b. An image is worth\n16x16 words: Transformers for image recognition at scale.\nIn ICLR.\nHan, K.; Xiao, A.; Wu, E.; Guo, J.; Xu, C.; and Wang, Y .\n2021. Transformer in transformer. Advances in Neural In-\nformation Processing Systems.\nHu, Y .; Wang, X.; Li, L.; and Gu, Q. 2021. Improving one-\nshot NAS with shrinking-and-expanding supernet. Pattern\nRecognition.\nJiang, Z.-H.; Hou, Q.; Yuan, L.; Zhou, D.; Shi, Y .; Jin, X.;\nWang, A.; and Feng, J. 2021. All tokens matter: Token la-\nbeling for training better vision transformers. Advances in\nNeural Information Processing Systems.\nKrizhevsky, A. 2009. Learning multiple layers of features\nfrom tiny images. Technical report.\nLee, N.; Ajanthan, T.; and Torr, P. H. 2018. Snip: Single-\nshot network pruning based on connection sensitivity. arXiv\npreprint arXiv:1810.02340.\nLi, C.; Tang, T.; Wang, G.; Peng, J.; Wang, B.; Liang, X.;\nand Chang, X. 2021a. BossNAS: Exploring Hybrid CNN-\ntransformers with Block-wisely Self-supervised Neural Ar-\nchitecture Search. 2021 IEEE/CVF International Confer-\nence on Computer Vision (ICCV), 12261–12271.\nLi, H.; Fu, T.; Dai, J.; Li, H.; Huang, G.; and Zhu, X. 2022a.\nAutoloss-zero: Searching loss functions from scratch for\ngeneric tasks. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 1009–1018.\nLi, K.; Yu, R.; Wang, Z.; Yuan, L.; Song, G.; and Chen, J.\n2022b. Locality guidance for improving vision transform-\ners on tiny datasets. In European Conference on Computer\nVision, 110–127. Springer.\nLi, L. 2022. Self-Regulated Feature Learning via Teacher-\nfree Feature Distillation. In ECCV.\nLi, L.; Dong, P.; Li, A.; Wei, Z.; and Ya, Y . 2023a.\nKD-Zero: Evolving Knowledge Distiller for Any Teacher-\nStudent Pairs. In Thirty-seventh Conference on Neural In-\nformation Processing Systems.\nLi, L.; Dong, P.; Wei, Z.; and Yang, Y . 2023b. Automated\nKnowledge Distillation via Monte Carlo Tree Search. In\nICCV.\nLi, L.; and Jin, Z. 2022. Shadow knowledge distillation:\nBridging offline and online knowledge transfer.Advances in\nNeural Information Processing Systems.\nLi, L.; Shiuan-Ni, L.; Yang, Y .; and Jin, Z. 2022c. Boosting\nOnline Feature Transfer via Separable Feature Fusion. In\nIJCNN.\nLi, L.; Shiuan-Ni, L.; Yang, Y .; and Jin, Z. 2022d. Teacher-\nfree Distillation via Regularizing Intermediate Representa-\ntion. In IJCNN.\nLi, Y .; Zhang, K.; Cao, J.; Timofte, R.; and Gool, L. V .\n2021b. LocalViT: Bringing Locality to Vision Transform-\ners. ArXiv, abs/2104.05707.\nLiang, Y .; GE, C.; Tong, Z.; Song, Y .; Wang, J.; and Xie,\nP. 2022. EViT: Expediting Vision Transformers via Token\nReorganizations. In International Conference on Learning\nRepresentations.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n15821\nLiu, X.; Li, L.; Li, C.; and Yao, A. 2023. NORM: Knowl-\nedge Distillation via N-to-One Representation Matching.\narXiv preprint arXiv:2305.13803.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vision\ntransformer using shifted windows. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision.\nLu, L.; CHen, Z.; Lu, L., Xiaoyu; Rao, Y .; Li, L.; and Pang,\nS. 2024. UniADS: Universal Architecture-Distiller Search\nfor Distillation Gap. In AAAI.\nMellor, J.; Turner, J.; Storkey, A.; and Crowley, E. J. 2021.\nNeural architecture search without training. In ICML.\nMozer, M. C.; and Smolensky, P. 1988. Skeletonization: A\nTechnique for Trimming the Fat from a Network via Rele-\nvance Assessment. In NIPS.\nNilsback, M.-E.; and Zisserman, A. 2008. Automated flower\nclassification over a large number of classes. In 2008 Sixth\nIndian Conference on Computer Vision, Graphics & Image\nProcessing, 722–729. IEEE.\nReal, E.; Liang, C.; So, D. R.; and Le, Q. V . 2020.\nAutoML-Zero: Evolving Machine Learning Algorithms\nFrom Scratch. arXiv:2003.03384.\nShao, S.; Dai, X.; Yin, S.; Li, L.; Chen, H.; and Hu, Y . 2023.\nCatch-Up Distillation: You Only Need to Train Once for Ac-\ncelerating Sampling. arXiv preprint arXiv:2305.10769.\nStephanou, M.; and Varughese, M. 2021. Sequential esti-\nmation of Spearman rank correlation using Hermite series\nestimators. Journal of Multivariate Analysis, 186: 104783.\nSu, X.; You, S.; Xie, J.; Zheng, M.; Wang, F.; Qian, C.;\nZhang, C.; Wang, X.; and Xu, C. 2021a. Vision Transformer\nArchitecture Search. arXiv preprint arXiv:2106.13700.\nSu, X.; You, S.; Xie, J.; Zheng, M.; Wang, F.; Qian, C.;\nZhang, C.; Wang, X.; and Xu, C. 2021b. ViTAS: Vision\nTransformer Architecture Search. In European Conference\non Computer Vision.\nTanaka, H.; Kunin, D.; Yamins, D. L.; and Ganguli, S. 2020.\nPruning neural networks without any data by iteratively con-\nserving synaptic flow. NeurIPS.\nTheis, L.; Korshunova, I.; Tejani, A.; and Husz ´ar, F. 2018.\nFaster gaze prediction with dense networks and Fisher prun-\ning. ArXiv, abs/1801.05787.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J´egou, H. 2021. Training data-efficient image trans-\nformers & distillation through attention. In ICML. PMLR.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021. Pyramid Vision Trans-\nformer: A Versatile Backbone for Dense Prediction without\nConvolutions. In ICCV.\nWei, Z.; Pan, H.; Li, L.; Dong, P.; Tian, Z.; Niu, X.; and Li,\nD. 2023. TVT: Training-Free Vision Transformer Search on\nTiny Datasets. arXiv preprint arXiv:2311.14337.\nWu, K.; Zhang, J.; Peng, H.; Liu, M.; Xiao, B.; Fu, J.; and\nYuan, L. 2022. TinyViT: Fast Pretraining Distillation for\nSmall Vision Transformers. ArXiv, abs/2207.10666.\nXie, S.; Zheng, H.; Liu, C.; and Lin, L. 2019. SNAS:\nstochastic neural architecture search. In ICLR.\nZhou, Q.; Sheng, K.; Zheng, X.; Li, K.; Sun, X.; Tian, Y .;\nChen, J.; and Ji, R. 2022. Training-free Transformer Archi-\ntecture Search. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 10894–10903.\nZhu, C.; Chen, W.; Peng, T.; Wang, Y .; and Jin, M. 2021.\nHard Sample Aware Noise Robust Learning for Histopathol-\nogy Image Classification. IEEE Transactions on Medical\nImaging, 41(4): 881–894.\nZimian Wei, Z.; Li, L. L.; Dong, P.; Hui, Z.; Li, A.; Lu, M.;\nPan, H.; and Li, D. 2024. Auto-Prox: Training-Free Vision\nTransformer Architecture Search via Automatic Proxy Dis-\ncovery. In AAAI.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n15822",
  "topic": "Architecture",
  "concepts": [
    {
      "name": "Architecture",
      "score": 0.6863162517547607
    },
    {
      "name": "Proxy (statistics)",
      "score": 0.620074450969696
    },
    {
      "name": "Computer science",
      "score": 0.5010912418365479
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43940412998199463
    },
    {
      "name": "Machine learning",
      "score": 0.321260541677475
    },
    {
      "name": "Geography",
      "score": 0.15333274006843567
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I170215575",
      "name": "National University of Defense Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I78577930",
      "name": "Columbia University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210160618",
      "name": "Huawei Technologies (United Kingdom)",
      "country": "GB"
    }
  ],
  "cited_by": 15
}