{
    "title": "MM-SHAP: A Performance-agnostic Metric for Measuring Multimodal Contributions in Vision and Language Models &amp; Tasks",
    "url": "https://openalex.org/W4385571395",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2952608021",
            "name": "Letitia Parcalabescu",
            "affiliations": [
                "Heidelberg University",
                "Heidelberg University"
            ]
        },
        {
            "id": "https://openalex.org/A2113562383",
            "name": "Anette Frank",
            "affiliations": [
                "Heidelberg University",
                "Heidelberg University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2962862931",
        "https://openalex.org/W3016211260",
        "https://openalex.org/W3104379732",
        "https://openalex.org/W4281633937",
        "https://openalex.org/W3186479087",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W2277195237",
        "https://openalex.org/W4285169422",
        "https://openalex.org/W3011101596",
        "https://openalex.org/W3106784008",
        "https://openalex.org/W2886641317",
        "https://openalex.org/W2962707719",
        "https://openalex.org/W2487898712",
        "https://openalex.org/W2966715458",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W4387001223",
        "https://openalex.org/W2970726176",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2109586012",
        "https://openalex.org/W3100801259",
        "https://openalex.org/W2934842096",
        "https://openalex.org/W3090449556",
        "https://openalex.org/W2939888942",
        "https://openalex.org/W3176196997",
        "https://openalex.org/W2516809705",
        "https://openalex.org/W4312261477",
        "https://openalex.org/W2998356391",
        "https://openalex.org/W2962858109",
        "https://openalex.org/W2963518342",
        "https://openalex.org/W4300235091",
        "https://openalex.org/W3014290757",
        "https://openalex.org/W3163542683",
        "https://openalex.org/W3197394194",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W4386065345",
        "https://openalex.org/W4286432986",
        "https://openalex.org/W3212002421",
        "https://openalex.org/W2336525064",
        "https://openalex.org/W3184735396",
        "https://openalex.org/W4390873716",
        "https://openalex.org/W4288096733",
        "https://openalex.org/W4230405732",
        "https://openalex.org/W4386071707",
        "https://openalex.org/W2489434015"
    ],
    "abstract": "Vision and language models (VL) are known to exploit unrobust indicators in individual modalities (e.g., introduced by distributional biases) instead of focusing on relevant information in each modality. That a unimodal model achieves similar accuracy on a VL task to a multimodal one, indicates that so-called unimodal collapse occurred. However, accuracy-based tests fail to detect e.g., when the model prediction is wrong, while the model used relevant information from a modality.Instead, we propose MM-SHAP, a performance-agnostic multimodality score based on Shapley values that reliably quantifies in which proportions a multimodal model uses individual modalities. We apply MM-SHAP in two ways: (1) to compare models for their average degree of multimodality, and (2) to measure for individual models the contribution of individual modalities for different tasks and datasets.Experiments with six VL models – LXMERT, CLIP and four ALBEF variants – on four VL tasks highlight that unimodal collapse can occur to different degrees and in different directions, contradicting the wide-spread assumption that unimodal collapse is one-sided. Based on our results, we recommend MM-SHAP for analysing multimodal tasks, to diagnose and guide progress towards multimodal integration. Code available at https://github.com/Heidelberg-NLP/MM-SHAP.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 4032–4059\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nMM-SHAP: A Performance-agnostic Metric for Measuring\nMultimodal Contributions in Vision and Language Models & Tasks\nLetitia Parcalabescu and Anette Frank\nComputational Linguistics Department\nHeidelberg University\nAbstract\nVision and language models (VL) are known to\nexploit unrobust indicators in individual modal-\nities (e.g., introduced by distributional biases)\ninstead of focusing on relevant information\nin each modality. That a unimodal model\nachieves similar accuracy on a VL task to a\nmultimodal one, indicates that so-called uni-\nmodal collapse occurred. However, accuracy-\nbased tests fail to detect e.g., when the model\nprediction is wrong, while the model used rele-\nvant information from a modality. Instead, we\npropose MM-SHAP, a performance-agnostic\nmultimodality score based on Shapley values\nthat reliably quantifies in which proportions\na multimodal model uses individual modali-\nties. We apply MM-SHAP in two ways: (1)\nto compare models for their average degree\nof multimodality, and (2) to measure for indi-\nvidual models the contribution of individual\nmodalities for different tasks and datasets. Ex-\nperiments with six VL models – LXMERT,\nCLIP and four ALBEF variants – on four VL\ntasks highlight that unimodal collapse can oc-\ncur to different degrees and in different direc-\ntions, contradicting the wide-spread assump-\ntion that unimodal collapse is one-sided. Based\non our results, we recommend MM-SHAP for\nanalysing multimodal tasks, to diagnose and\nguide progress towards multimodal integration.\nCode available at https://github.com/\nHeidelberg-NLP/MM-SHAP.\n1 Introduction\nVision and language (VL) tasks are dominated by\ngeneral-purpose pretrained transformer-based VL\nmodels (Lu et al., 2019; Tan and Bansal, 2019;\nLi et al., 2019; Chen et al., 2020; Li et al., 2020,\n2021a). But we are only starting to understand why\nthese multimodal (MM) models work so well, and\nhow they utilise and fuse image and text modalities\n(Hessel and Lee, 2020; Cao et al., 2020). Even\nworse, these highly parametrised neural VL mod-\nels, pretrained on large amounts of data, tend to\nFigure 1: We display image-sentence alignment scores\n(ISA) and the textual degree T-SHAP that measures\nhow much models focus on text rather than the image\n(with 100−T-SHAP% the corresponding visual degree)\nfor 3 VL models. Blue/red highlights on text tokens and\nimage tokens (patches) contribute towards higher/lower\nISA. Note: CLIP’s ISA is an absolute score, while AL-\nBEF and LXMERT predict ISA probabilities. See Sec-\ntion 4.4 for more details on this figure; App. C for more\ndetailed analysis of this instance and more samples.\nexploit artefacts and statistical correlations in the\ndata (Shekhar et al., 2019; Kafle et al., 2019), show-\ning little to no evidence of detailed linguistic or\nvisual understanding (Milewski et al., 2022; Parcal-\nabescu et al., 2022; Thrush et al., 2022). Statistical\nbiases towards indicators in one modality – to the\n4032\ndetriment of others – can cause unimodal collapse\n(Parcalabescu et al., 2022), where seemingly MM\nmodels exploit one modality that exhibits biases,\nmeaning that the MM system effectively reduces\nto a unimodal model (Madhyastha et al., 2018) –\ne.g., if a model answers “How many...?” ques-\ntions with “two” – the most frequent answer seen\nin training (Goyal et al., 2017). Unimodal collapse\nis severe, as it leads to loss of system reliability. It\nalso shows that multimodal fusion is far from being\nsolved. Hence the importance of measuring multi-\nmodal degree – the degree to which modalities are\nused in model predictions – with reliable metrics.\nTo test for unimodal collapse, research has so far\nfocused on performance tests: a VL model is eval-\nuated on a MM task, but one modality crucial for\nsolving it correctly is missing, corrupted (Shekhar\net al., 2017) or permuted (Gat et al., 2021). These\ntests are indicative of unimodal collapse, but we\nargue that they are not appropriate to reliably mea-\nsure the contribution of each modality. Clearly,\naccuracy reflects whether a model prediction is\n(in)correct, but it may detect illicit cases where the\nmodel prediction is wrong, although it does use\ncrucial indicators in a given modality. Conversely,\na prediction might be correct, but may be derived\nfrom unrobust indicators. Fig. 1 shows very dif-\nferent SHAP-based contribution patterns of image\nregions and text tokens leading to model responses\nof different image-sentence alignment (ISA) scores\n(e.g., ALBEF caption vs. foil), while yielding same\nISA accuracy since both scores surpass the 0.5 clas-\nsification threshold.\nAs an alternative to accuracy-based methods, we\npropose MM-SHAP, a performance-agnostic met-\nric to quantify and interpret the contribution of\nindividual modalities in VL models. MM-SHAP is\nbased on Shapley values (Shapley, 1953), a theoret-\nically well-founded interpretability method from\ncooperative game theory. We apply MM-SHAP to\nquantify the contribution of specific parts of the\ninput towards model predictions.\nOur main contributions are:\ni) We propose MM-SHAP, a performance-agno-\nstic metric to measure the degree of contribu-\ntion of each modality in VL (but not limited to\nV&L), to measure the degree to which individ-\nual modalities contribute to MM model pre-\ndictions. We combine MM-SHAP with model\naccuracy to analyse the degree to which each\nmodality contributes to model predictions.\nii) We use MM-SHAP to 1) compare models in\nterms of their reliance on different modalities,\n2) compare the relevance of different modal-\nities for a given task and dataset, and to 3)\nzoom in at sample-level to determine the con-\ntribution of each modality and each token in\neach modality for a model prediction (Fig. 1).\niii) We conduct experiments with six VL models:\nLXMERT, CLIP and four ALBEF variants –\non four VL tasks: image-sentence alignment,\nVQA, GQA and on the more fine-grained\nV ALSE\n VL benchmark.\niv) We identify VL models that are balanced in\ntheir usage of two modalities (CLIP), models\nthat show a higher visual degree (LXMERT)\nor a stronger textual degree (ALBEF).\nv) We show that 1) fine-tuning a model can affect\nits MM degree and that 2) current VL models\ndo not all collapse towards the same modality,\nas reported in recent work (Frank et al., 2021;\nGat et al., 2021), but that directions can differ\nfrom model to model.\n2 Related Work\nTesting for unimodal collapse Strong prediction\nindicators in either modality can cause MM mod-\nels to ignore weaker indicators in another modality.\nPrior work has proposed ways to identify (and re-\nmove) such biases from data (Goyal et al., 2017).\nFoiling approachesintroduce mistakes in image\ndescriptions and test whether VL models notice the\ndiscrepancy between image and captions (Shekhar\net al., 2019; Parcalabescu et al., 2022), finding that\nmodels are surprisingly insensitive to such foils.\nGat et al. (2021), in a similar vein, exchange images\nwith other images or captions with other captions,\nexpecting that inputs with misleading information\nin one modality incur a decrease in model accuracy.\nThey use an observed decrease in task accuracy\nto calculate a perceptual score as a measure of the\nMM degree of a model. Their findings suggest\nthat across their tested VL models, textual input\nconsistently matters more than visual input.\nAblation methods remove information from ei-\nther modality and test whether the model can still\nsolve the task. Here, Frank et al. (2021) find that the\nvisual modality matters more than text: VL models\nsuffer from image parts removal when predicting\nmasked text, but can predict masked visual inputs\nwhen text input is ablated. This contradicts Gat\net al. (2021)’s finding, but their investigations have\n4033\nonly a single model in common, namely LXMERT.\nHence, the literature agrees that VL models are\nnot as cross-modal as expected – but disagree on\nwhether models rely more on the textual (Gat et al.,\n2021) or on the visual modality (Frank et al., 2021).\nWe argue that a reason for this discrepancy is that\nprior work computes MM scores based on model\nperformance. In our work we argue that methods\nfor measuring a model’s MM degree should not rely\non accuracy (see §3.1 for motivation). Instead, we\npropose an accuracy-agnostic method to measure\nthe MM degree of VL models, using the SHAP\n(Lundberg and Lee, 2017) interpretability method\nthat is theoretically suitable to define an MM score.\nInterpretability Methods for explaining predic-\ntions of neural models can be classified into two\ncategories: White-box methods, which require ac-\ncess to specific components of neural architectures\nand black-box methods, which are model-agnostic,\nrequiring only access to model inputs and outputs.\nNotable white-box methods are: Attention-based\nmethods, which correlate high attention weights\nwith high feature importance. But the equivalence\nof importance score and attention is debated and\nmust be considered with care (Jain and Wallace,\n2019; Wiegreffe and Pinter, 2019) (see App. D for\na detailed discussion on why attention is inappro-\npriate for defining an MM score). Layer-wise rele-\nvance propagation (Binder et al., 2016) or gradient-\nbased methods e.g., Grad-CAM (Selvaraju et al.,\n2017) can also be used to determine the importance\nof inputs, but can be deceived by small changes in\ninputs (adversarial attacks).\nNotable black-box methods are: LIME (Ribeiro\net al., 2016) and its multimodal adaptation DIME\n(Lyu et al., 2022) approximate the vicinity of the in-\nput with a linear function that is interpretable. But\ndepending on the choice of the size of the vicinity,\nLIME can lead to very disparate results. Methods\nlike RISE (Petsiuk et al., 2018) and SHAP (Lund-\nberg and Lee, 2017) compute importance scores by\nrandomly masking parts of the input and determin-\ning the effect this has on the output. SHAP exhibits\ngreat theoretical properties that enable us to define\na MM score, as we will motivate in §3.4.\n3 Quantifying Multimodal Contributions\n3.1 A case for a performance-agnostic score\nAs a community, we are interested in improving\nmodel performance, and thus need to evaluate mod-\nels using performance metrics such as accuracy.\nBut in this work we address a complementary ques-\ntion that is only indirectly related to performance.\nWe aim to measure how much a given modality\nmatters for model predictions. This is important\nfor model developers to know, to detectunimodal\ncollapse, and to find ways of preventing it.\nTo date, research tried to measure MM contribu-\ntions based on accuracy. Gat et al. (2021) and Frank\net al. (2021), e.g., rely on the difference between\na model’s accuracy with and without information\nfrom a modality, e.g., to define the importance of\nvision as V = Acc(vision, text) − Acc(∅, text).\nThis score works well if a MM model shows good\nperformance, but is problematic for wrong model\npredictions, since in such cases Acc(vision, text)\n= 0, and we expect Acc(∅, text) = 0 too, resulting\nin V = 0 (or another low value). But this does not\nnecessarily reflect reality: The model may well\nhave relied on the visual modality, but incorrectly.\nEven worse, accuracy-based methods that com-\npletely delete (Madhyastha et al., 2018) or ex-\nchange (Gat et al., 2021) information in one modal-\nity are ill-defined for image-sentence alignment\n(ISA): ISA asks a model to assess how well two\nmodalities align, with the rationale that alignment\nis given if the given modalities (e.g., image and\ntext) contain relevant information that indicates\nalignment by ’being about the same things or facts’.\nIn case the information conveyed in two modal-\nities is not about the same (type of) things (e.g.,\na picture of a dog paired with a caption talking\nabout a cat), the modalities do not align. How-\never, metrics that measure the importance of vi-\nsion V by the impact of deleting it, as V =\nAcc(vision, text) − Acc(∅, text), are ill-defined\nfor unaligned image-sentence pairs: A model that\nuses both modalities to correctly predict misalign-\nment (Acc(vision, text) = 1), will also predict a\nmismatch when the visual information is deleted or\nexchanged, yielding Acc(∅, text) = 1. This results\nin V = 0, signalling that no visual importance is\nmeasured, which is ill-founded in this case. Hence,\naccuracy-based scores that rely on deletion of sin-\ngle modalities are unable to measure multimodal\ndegree on ISA – an important pretraining task for\nVL models – or on zero-shot ISA benchmark tasks\nsuch as V ALSE\n (Parcalabescu et al., 2022).\nWe argue for using accuracy-agnostic meth-\nods to measure a model’s multimodal degree and\npropose MM-SHAP ,a metric that avoids the pit-\n4034\nfalls of performance-based metrics. We move\nfrom Acc(vision, text) to measuring the rela-\ntive contribution of vision and text by measur-\ning Contribution(vision, text) for a given model\nprediction. We compute the Contribution func-\ntion using Shapley values, which quantify a to-\nken’s contribution to a model prediction, indepen-\ndently of whether the prediction is correct. Impor-\ntantly, our performance-agnostic way of measuring\na model’s MM degree in terms of contributions of\ntokens – within or across modalities – will make\nit possible to clearly separate accuracy-based per-\nformance analysis from the study of relative contri-\nbutions of modalities in MM systems. This allows\nus to measure MM degree in situations where ac-\ncuracy cannot: e.g., when model accuracy is low –\nas in out-of-domain or zero-shot settings.\n3.2 Background on Shapley Values\nShapley values1 were first introduced in a game\ntheoretical setting to estimate fair rewards among\ncooperative players (Shapley, 1953). For machine\nlearning, the outcome of a game is the model’s pre-\ndiction, the players are parts of the input and are\nassigned Shapley values that represent the impor-\ntance of each player (Lundberg and Lee, 2017).\nWe compute Shapley values for pretrained trans-\nformer-based VL models at prediction time. Their\ninput consists of n input tokens (image and text\ntokens alike). We create subsets S ⊆ {1, . . . , n}\nof tokens forming a coalition towards the model\nprediction val(S). Tokens not being part of the sub-\nset are masked. val(∅) is the output of the model\nwhen all tokens are masked. The Shapley value for\na token j follows formula (1):\nϕj =\nX\nS⊆{1,...,n}\\{j}\nval(S ∪ {j}) − val(S)\nγ (1)\nHere, γ = (n−1)!\n|S|!(n−|S|−1|)! is the normalising fac-\ntor that accounts for all possible combinations of\nchoosing subset S. When masking p tokens, the\ncoalition possibilities grow exponentially (2p). We\nthus approximate the Shapley values with Monte\nCarlo, by randomly sub-sampling 2p+ 1coalitions.\nThe Shapley value of a token measures its con-\ntribution towards the model prediction (e.g., the\nprobability of image-sentence alignment) and can\nbe positive (increases the model prediction) or neg-\native (decreases it) or zero (no effect). Shapley\n1We refer to Molnar (2022) for a gentle introduction into\nShapley Values.\nvalues exhibit four defining properties of a fair pay-\nout, which are all beneficial for model interpretabil-\nity: (1) Efficiency: the contributions of all players\nsum up to the model outcome; (2) Symmetry: any\ntwo players that contribute equally are assigned the\nsame payout; (3) Dummy: a non-contributing part\nis assigned zero value and (4) Additivity, enabling\nus to simply average the Shapley Values to deter-\nmine the overall player contributions in a game\nwith combined payouts (e.g., the two halves of a\nsoccer match, or ensembling of decision trees).\nMost importantly, Shapley values are not based\non model accuracy or performance, but solely on\nthe model’s input and its prediction, e.g., the prob-\nability for an image and a caption to match. This is\nan important property for our MM score, since its\nobjective is to quantify how much inputs of either\nmodality matter for prediction – even if the cooper-\nation between (multimodal) inputs is not sufficient\nto reach success, i.e., yielding the correct outcome.\n3.3 MM-SHAP\nFor a pretrained VL transformer with nT text to-\nkens and nI image tokens, Eq. 2 defines the textual\ncontribution ΦT and the image contribution ΦI to-\nwards a prediction as the sum of (absolute) Shapley\nValues (Eq. 1) of all textual resp. visual tokens:\nΦT =\nnTX\nj\n|ϕj| ; Φ I =\nnIX\nj\n|ϕj| (2)\nWe consider the magnitude and not the sign of a\ntoken contribution2, as we are interested in mea-\nsuring whether a token is active in a modality –\nirrespective of the direction it pushes the prediction\ninto. Eq. 3 defines MM-SHAP as a proportion of\nmodality contributions, allowing us to determine\na model’s textual degree T-SHAP and its visual\ndegree V-SHAP:\nT-SHAP = ΦT\nΦT + ΦI\n; V-SHAP = ΦI\nΦT + ΦI\n(3)\nWe can extend MM-SHAP to any number of moda-\nlities. Here we only use image and text.\nWhen generating coalitions, i.e., subsets of to-\nkens from which to compute Shapley Values, we\ndo not distinguish image and text tokens, because\nMM-SHAP aims to fairly distribute potential token\ncontributions first and to aggregate them modality-\nwise in a 2 nd step with Eq. 2. To mask tokens,\n2Contributions can be positive (increase the model predic-\ntion) or negative (decrease it) or zero (no effect), see §3.2.\n4035\nwe replace text tokens with the [MASK] token; for\nimages we set pixel values of image patches to zero.\nWe ensure similar text and image sequence lengths\nby using more and smaller patches for longer text,\nand vice versa – resulting in 16 image patches for\nthe majority of samples in our data. See App. A.\n3.4 Why SHAP enables a MM score\nOur aim for MM-SHAP is to estimate the propor-\ntion to which text and vision are used by VL models\n(x% visual and y% textual). Defining an MM score\nis nontrivial, since it should not be based on accu-\nracy, see §3.1. An MM score should rely on a mea-\nsure of how much tokens contribute to the output\nvalue computed by the model. Most interpretablity\nmethods do not directly answer this question of\nhow much models use certain features, but use\nproxies such as gradients or attention. Moreover,\ntheir explanations cannot be added modality-wise\nin a meaningful way, to define a relative contribu-\ntion per modality (Cf. App. D for a discussion on\nattention). Luckily, Shapley values compute fair\npayouts to players (tokens), depending on their con-\ntribution to achieving the total payout (the model’s\nprediction). Their theoretically founded properties\n– e.g. fair payout between tokens and modalities,\nor in-sample and between-sample additivity, as de-\ntailed in §3.2 – allow us to aggregate intra-modal\ntoken-level contributions to compute an MM score.\nGrounding our MM score in Shapley values\nbears further advantages, which we discuss next.\n3.5 Ways of using MM-SHAP\nSample-level MM-SHAP, being based on the\ncontributions of individual image and text tokens,\nis a sample-level score (Fig. 1). It enables fine-\ngrained analyses of the relevance of tokens from a\nsingle or various modalities, for each instance.\nDataset and model level We can average sample-\nlevel MM-SHAP scores into dataset-level scores,\nthanks to the additivity property of Shapley values.\nHence it can help analyse a model across various\ndatasets, or compare distinct models on a certain\ndataset to gain insights of models, datasets / tasks.\nMeasuring fine-tuning effects An accuracy-\nbased MM score is limited when model perfor-\nmance on a task is very low, since the differences\nbetween a model’s accuracy with correct vs. per-\nmuted inputs are small in such cases (Cf. §3.1).\nSince MM-SHAP is based on actual model predic-\ntions and not on model performance, we can apply\nMM-SHAP for models with low performance. E.g.,\nwe can compare a pretrained model’s MM score to\na fine-tuned version of it that may have lost gen-\neral task abilities (thus showing low accuracy) after\nspecialising for another task; or we can measure\nthe effectiveness of targeted interventions in fine-\ntuning to increase a model’s reliance on modalities.\nFuture work could apply MM-SHAP on models\naccepting different or a wider range of modalities,\nfor tracing a model’s MM-SHAP evolution in pre-\ntraining, or on data cleaning, by identifying groups\nof samples with very unbalanced MM degree – es-\npecially when the accuracy on those samples is\nhigh and the model may rely on unimodal cues.\n4 Multimodal Contributions across\nModels and Datasets\nWe use MM-SHAP to study MM contributions for\ndifferent i) model types, ii) datasets and iii) tasks.\nIn doing so we iv) re-evaluate prior findings on vi-\nsual vs. textual unimodal collapse and v) showcase\nMM-SHAP’s abilities for interpreting predictions\nfor individual samples, for error analysis.\nWe evaluate pretrained VL models with MM-\nSHAP and complement our analysis by measuring\nthe model’s task accuracy. We compare MM-SHAP\nto a 50% T-SHAP – 50% V-SHAP baseline and\ngauge how much the model tends towards the tex-\ntual or visual modality. We hypothesise that in\naverage, V&L should contribute equally when the\nmodel predicts whether the contents of the modali-\nties are aligned (image-sentence alignment).\nWe test on matching image-captions, but also\non cases with discrepancies between modalities.\nWe break down our incongruity tests into high dis-\ncrepancy (cases of completely mismatching image-\ncaptions, Tab. 1), andlow discrepancy (cases where\na single word or phrase incurs a mismatch, Tab. 2).\n4.1 Tasks\nVisual Question Answering (VQA) is a task\nwhere transformer-based VL models have consis-\ntently increased SOTA performance. We use the\nVQA v2.0 (Goyal et al., 2017) and GQA (Hudson\nand Manning, 2019) datasets for our experiments.\nImage-sentence alignment (ISA) VL models\nare typically pretrained on predicting an image-\nsentence alignment score. We assess their MM\ncontributions in their “comfort zone” by letting\nthem predict the alignment of images and captions,\nin contrast to misalignment to random captions.\n4036\nWe test on 1,500 samples from the MSCOCO val-\nidation set (Lin et al., 2014), and on uncommon\nimage-caption pairs composed from questions and\nanswers from the VQA and GQA validation sets.\nISA on fine-grained VL phenomena In ISA\ntasks, models are typically confronted with highly\ndiscrepant negative samples (non-matching image–\ncaptions). To evaluate VL models in a more fine-\ngrained manner, we examine their MM score on the\nV ALSE\n benchmark (Parcalabescu et al., 2022),\nwhere foiled captions were created by altering\nphrases pertaining to 6 specific linguistic phenom-\nena: existence, counting, plurality, spatial rela-\ntions, actions, and coreference, such that image\nand foiled caption do not match. For completeness,\nwe also test on noun phrase foils as introduced in\nthe FOILit! dataset (Shekhar et al., 2017).\n4.2 Models\nLXMERT by Tan and Bansal (2019) is a dual-\nstream transformer that combines V&L in early\nfusion using cross-modal attention layers between\nimage and language encoders. It was pretrained on\nMSCOCO (Lin et al., 2014) images and captions,\nand on VQA v2.0 and GQA images, questions and\nanswers. Its objectives were (i) multimodal masked\nword and object prediction, (ii) ISA, and (iii) VQA\nobjectives. For experiments on ISA, VQA and\nGQA, we use the corresponding heads and task-\nspecific checkpoints.3\nCLIP by Radford et al. (2021) processes image\nand text with two separate transformer encoders.\nThe resulting image and text representations are\ncombined in late fusion by cross-product. CLIP\nwas trained for ISA in low discrepancy mode on\n400M image-text pairs to predict high scores for\npaired image-text examples and low scores when\nimage-text samples are not paired in the dataset.\nWith this simple contrastive learning objective,\nCLIP shows zero-shot capabilities in e.g. object\nclassification, OCR, or activity recognition (Rad-\nford et al., 2021). In our work, we test CLIP 4\non ISA and V ALSE\n , using the model’s image-\ntext alignment score to assess whether it predicts a\nhigher image-text similarity for correct pairs or for\nfoiled image-caption pairs.\nALBEF by Li et al. (2021b) combines vision\nand language with early and late fusion. As in\n3github.com/huggingface/transformers\n4github.com/openai/CLIP\nCLIP, transformer image and text encoders are trai-\nned to map the two modalities to a common space.\nCross-modal transformer layers further combine\nthe two with (i) MM masked word prediction and\n(ii) ISA objectives. It was pretrained on Concep-\ntual Captions (Sharma et al., 2018), SBU Captions\n(Ordonez et al., 2011), MSCOCO (Lin et al., 2014)\nand Visual Genome (Krishna et al., 2017).\nTo analyse how the MM contributions are af-\nfected by fine-tuning, we compare 4 ALBEF5 mod-\nels fine-tuned on (1) image retrieval on MSCOCO,\n(2) image retrieval on Flickr30k (Plummer et al.,\n2015), (3) visual grounding on RefCOCO+ (Yu\net al., 2016) and (4) VQA (Goyal et al., 2017).\n4.3 Metrics\nWe use accuracy to assess model performances,\nand MM-SHAP to measure the proportion to\nwhich the different modalities contribute.\nWith MM-SHAP (def. in §3.3) we aim to\nanalyse the MM contributions in terms of visual\n(V-SHAP) and textual ( T-SHAP) degree. As in\nour case of two modalities they are complemen-\ntary (V-SHAP = 100 − T-SHAP), we only report\nT-SHAP (in %). We distinguish T-SHAPc for tex-\ntual degree in image-caption pairs and T-SHAPf\nfor image-foil pairs. As the results are very similar,\nwe refer to Table 3 App. B for T-SHAPf results.\nWhen evaluating VQA and GQA performance,\naccuracy measures the proportion of correct ans-\nwers given pairs of images and questions. For ISA,\nwe measure the overall accuracy acc of models to\nclassify foils and captions. We fan outacc into cap-\ntion accuracy accc (for correctly predicting match-\ning images and captions) and foil accuracy accf\n(for correctly predicting mismatching images and\nfoils). Pairwise accuracy accr measures the pro-\nportion of samples where the ISA score is higher\nfor a correct image-text pair compared to its image-\nfoil counterpart. accr is more permissive than acc:\nit does not require the ISA score to surpass a classi-\nfication threshold (of 0.5), but only that image-foil\npairs are ranked lower than the ground truth pairs.\n4.4 Experiments and Results\nWe test all VL models from §4.2 without further\ntuning and assess their task accuracy and MM-\nSHAP scores in three settings: i) for VQA on the\nVQA and GQA datasets; for ISA ii) with high\ndiscrepancy image-caption pairs (from MSCOCO,\n5github.com/salesforce/ALBEF\n4037\nVQA, GQA) and iii) with low discrepancy pairs\nfrom V ALSE\n ; finally iv) we showcase sample-\nlevel analyses using MM-SHAP. Table 1 shows\nresults on VQA, GQA and ISA; Table 2 for\nV ALSE\n . MM-SHAP varies between samples\nwith a stdev. of ∼12% across our experiments.\nHigh discrepancy ISA (Table 1) shows thataccr\nscores for ISA on MSCOCO, VQA, GQA are high\nfor all models. This is expected as they have been\npretrained for ISA – only ALBEF vqa stands out:\nit lost its ISA performance by fine-tuning on VQA.\nLXMERT has highest accr for ISA on VQA and\nGQA, since for its last 10 epochs it was trained on\nthese datasets.\nFor ISA, we observe the models scattering\naround the hypothesised 50% balance forT-SHAP,\nwith CLIP being the most balanced one, especially\non MSCOCO. This is expected since CLIP is a\ntwo-branch model where the two modalities com-\nmunicate in late fusion, in other words, CLIP keeps\nall information from the textual and visual branches\nseparate until the very end. By contrast, LXMERT\nhas a low textual degree of only 35.5%, while AL-\nBEF models are more textual.\nGiven highly diverging foil pairs, T-SHAPc and\nT-SHAPf differ prominently: LXMERT moves\nfrom weak to higher textual degree (35.5 to 62.8%)\nand inversely for ALBEF mscoco (63.4 to 54.3%).\nCanonical VL tasks Results on VQA and GQA\nin Table 1 – with ALBEF fine-tuned for VQA and\nLXMERT fine-tuned on VQA and GQA6 – show\nhigh model accuracy. T-SHAP is higher for VQA\n(51.5%) than for ISA (45.7% accc), which is inter-\nesting, since LXMERT was more visually focused\non ISA. It seems like ALBEF vqa’s and LXMERT’s\ntraining on VQA increases the impact of the textual\nmodality to the detriment of the visual one. This\naligns with earlier findings that in VQA tasks, lin-\nguistic indicators (e.g., “How many...?”) give away\nthe most likely answer (two) (Goyal et al., 2017).\nLow discrepancy ISA on V ALSE\n in Table 2.\nFor T-SHAPc we bold-face high deviations from\nthe 50% T-SHAP baseline (values > 61% and\n<40%). We note that the scores do not deviate\nmuch from the baseline. CLIP is the multimodally\nmost balanced model, with an average T-SHAPc\nof 50.7% across all instruments, which is expected,\n6We do not test CLIP and the other ALBEF models on\nVQA because they do not have corresponding VQA heads.\nas argued for high discrepancy ISA above. By con-\ntrast, LXMERT skews towards the visual modality\nwith an average T-SHAPc of 41.9%, while ALBEF\nfocuses on text – its variants showing T-SHAPc\nvalues of 56% to 62%. This is consistent with our\nresults for high discrepancy ISA in Table 1.\nAccuracy vs. MM-SHAP On V ALSE\n , accura-\ncies do not correlate with MM-SHAP (see App. B.1\nfor details). This suggests that MM-SHAP com-\nplements accuracy in assessing MM contributions.\nAs Parcalabescu et al. (2022) observe, models are\nbetter with some instruments (noun phrases, exis-\ntence) as opposed to others (actions, coreference).\nOur work adds the multimodal score MM-SHAP as\na new dimension of analysis. Some models exhibit\nstrong divergences in T-SHAP across phenomena:\nLXMERT is strongly visually focused for plurality,\nspatial relations, noun phrases; also ALBEF’s tex-\ntual bias is especially strong for these phenomena.\nModel bias For overall ISA results on V ALSE\n ,\nTable 2 shows that despite varying model accura-\ncies (stdev. for accr across phenomena ±11-15%),\nMM-SHAP is relatively stable (±1-5% stdev.) even\nwhen data distributions differ: E.g.,counting adver-\nsarial contains foils in number ranges 0 to 3, while\nfor captions numbers are higher than 4. The piece\nserves as a sanity check for biased models that may\nprefer the more frequently found small numbers.\nFor LXMERT and ALBEF refcoco accr drops for\ncounting small numbers to counting adversarial\n(encircled numbers in Tab. 2) from 69.2% to 42.6%\nfor LXMERT and from 70.7% to 45.7% for AL-\nBEF – while T-SHAPc stays remarkably constant\n(47.3% to 46.4% and 55.1% to 55.8%). Even for\nphenomena that suffer from plausibility bias (Par-\ncalabescu et al., 2022), T-SHAP varies little, while\naccuracies differ. Stable MM-SHAP scores high-\nlight our MM score’s ability to measure how much\nthe input modalities matter for model predictions\n– irrespective of their correctness –, complement-\ning accuracy. Further results in App. B.2 compare\nmodel performances on foils vs. captions, support-\ning MM-SHAP’s stability while accuracy varies.\nFine-tuning effects For the four fine-tuned AL-\nBEF models evaluated on V ALSE\n , we observe\nthat i) the models fine-tuned for image retrieval\n(mscoco, flickr) are good at predicting ISA (73.6%\naccr for ALBEF mscoco) but not those for VQA\n(ALBEF vqa 50.7%) and referring expressions (AL-\nBEF refcoco 66.0%). This is expected, since ISA\n4038\nVisual Question Answering Image-sentence alignment\nVQA GQA MSCOCO VQA GQA\nModel acc T acc T accc accf accr Tc Tf accc accf accr Tc Tf accc accf accr Tc Tf\nRandom 0.0 50.0 0.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nLXMERT 72.5 51.5 60.3 57.8 71.8 99.1 99.3 35.5 62.8 66.6 95.9 95.2 45.7 57.5 41.8 96.5 89.9 47.5 59.8\nCLIP - - - - - - 99.5 50.3 52.9 - - 94.0 48.4 47.6 - - 83.4 47.0 46.0\nA mscoco - - - - 95.9 99.6 99.8 63.4 54.3 28.0 99.9 91.0 60.3 59.2 13.1 99.7 83.6 58.3 57.2\nA flickr - - - - 97.3 99.4 99.7 61.1 56.6 42.4 99.2 91.8 61.3 60.2 23.4 99.5 84.1 58.7 58.1\nA refcoco - - - - 92.3 99.3 99.7 56.6 58.9 49.8 99.1 90.0 57.8 58.6 25.0 98.4 85.6 58.2 59.3\nA vqa 76.0 66.7 - - 99.9 0.0 33.4 64.1 62.8 100.0 0.0 60.2 58.2 60.0 100.0 0.0 52.6 61.7 62.4\nTable 1: Task accuracy and MM score on canonical tasks. T is T-SHAP (in %). V-SHAP = 100 − T-SHAP. accr\nis pairwise ranking accuracy, counting predictions as correct if p(caption, img) > p(random, img). A stands for\nALBEF fine-tuned for different tasks: image retrieval on MSCOCO and Flickr30k; visual grounding on RefCOCO+\nand VQA. Overall foil task performance is the mean of accc and accf (equal nb. of samples, all pairs).\nMetric Model Existence Plurality Counting Sp.rel.‡ Action Coreference Foil-it! Avg. MM\nquantifiers number bal.† sns.† adv.† relations repl.† swap† std.† clean nouns ±stdev. skew\nRandom 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 ±0\naccr\nCLIP 66.9 56.2 62.1 62.5 57.5 64.3 75.6 68.6 52.1 49.7 88.8 64.0 ±11\nLXMERT 78.6 64.4 62.2 69.2 42.6 60.2 54.8 45.8 46.8 44.2 87.1 59.6 ±15\nA mscoco 78.6 80.1 71.8 74.3 68.9 74.6 79.8 62.6 62.2 59.6 97.0 73.6±11\nA flickr 80.6 78.9 71.0 73.6 64.3 73.3 82.4 55.5 59.9 57.7 96.6 72.1 ±12\nA refcoco 73.1 69.0 67.9 70.7 45.7 68.6 79.9 58.9 52.7 43.3 96.5 66.0±15\nA vqa 40.8 63.3 49.0 49.2 23.2 61.9 51.7 52.0 55.9 43.3 67.2 50.7±12\nacc\nLXMERT 55.8 55.1 52.0 55.4 49.4 50.7 51.1 48.5 49.8 49.0 70.8 53.4 ±6\nA mscoco 56.7 60.2 55.4 53.9 56.0 52.3 63.7 54.0 52.7 52.0 76.3 57.6 ±7\nA flickr 55.6 56.3 53.8 53.3 55.4 52.3 64.9 48.9 50.0 50.0 70.5 55.5 ±6\nA refcoco 53.4 56.3 51.1 51.1 48.4 51.1 63.1 51.2 50.7 49.3 77.4 54.8 ±8\nA vqa 52.8 50.0 50.0 50.0 51.1 53.5 50.0 50.0 51.4 50.0 53.7 51.1 ±1\nT-SHAP\nc\nCLIP 44.7 52.3 51.5 51.8 52.1 50.9 50.0 49.7 52.1 52.6 49.9 50.7±2 bal.\nLXMERT 51.7 37.1 46.5 47.3 46.4 36.6 42.1 42.2 38.2 37.2 36.1 41.9±5 vis.\nA mscoco 56.7 63.5 58.3 58.0 59.5 64.1 61.7 61.5 61.9 61.4 63.9 60.9±3 txt.\nA flickr 59.5 61.7 59.6 59.8 59.5 61.6 59.8 58.9 60.9 61.9 63.5 60.6±1 txt.\nA refcoco 53.3 57.2 55.4 55.1 55.8 57.0 54.5 54.4 57.9 58.9 56.8 56.0±2 txt.\nA vqa 64.6 63.6 62.5 61.4 63.4 63.0 59.3 60.3 63.6 63.1 62.1 62.4±2 txt.\nTable 2: Performance and MM scores of VL models on the V ALSE\n benchmark. We bold-face high accuracies and\nmultimodally unbalanced models on tasks. accr: the pairwise ranking accuracy, considering predictions as correct\nif p(caption, img) > p(foil, img). acc: Overall ISA accurracy. A stands for different fine-tunings of ALBEF:\nimage retrieval on MSCOCO and Flickr30k, visual grounding on RefCOCO+ and VQA. †bal. Counting balanced.\n†sns. Counting small numbers. adv. Counting adversarial. repl. Action replacement. swap. Actant swap. ‡ Sp.rel.\nSpatial relations. †std. Coreference standard. MM skew: Modality on which a model relies more: bal. balanced,\nvis. visual, txt. textual. We refer to Table 3 in App. B for more fanned out results.\nand image retrieval are very similar tasks. Inter-\nestingly, not only accuracy, but also the MM score\nchanges, making ALBEF vqa more focused on text\n(62.4% avg. T-SHAPc across V ALSE) compared\nto referring expressions (ALBEF refcoco 56.0%).\nNotably, MM-SHAP being accuracy-agnostic, we\ncan compute indicative scores even when a fine-\ntuned model fails the task completely, like ALBEF\nvqa that always predicts the foil class on captions.\nSample-level analysis Fig. 1 shows ISA predic-\ntions of CLIP, ALBEF mscoco and LXMERT, and\ntheir T-SHAP values for caption and foil. LX-\nMERT correctly predicts high ISA between image\nand caption (left), although the regions contribut-\ning most (in blue) are not all reasonable, since the\n‘phone’ token is not correctly grounded. ALBEF\nmscoco and CLIP also assign very high ISA scores,\nwhile using well-justified image regions for thumb\nand phone. On the foil (right), LXMERT’s con-\ntributing tokens change, with the phone region in\nthe image mistakenly contributing to a high ISA.\nFavourably for ALBEF, the ‘keyboard’ text token\ncontributes towards lowering the ISA, unlike for\nCLIP and LXMERT, where the ‘keyboard’ token\nraises the ISA. For more examples see App. C. We\nalso showcase how attention does not reflect neg-\native impact of tokens on a model’s prediction –\nwhich is very important in e.g., assessing the im-\npact of foil words – in App. D.2 and Fig. 10.\n4.5 Comparison to other MM metrics\nWe can only compare to other MM scores for VQA,\nbecause accuracy-based MM scores that delete in-\nformation cannot apply to ISA (as argued in §3.1).\nUnsurprisingly LXMERT’s accuracy when delet-\n4039\ning the image is 31%; when deleting the text it is\n8%, since excluding the image should negatively\naffect accuracy more than excluding text in VQA,\nwhere at least the answer type can be better inferred\nfrom the text (should be numeral for “How many”).\nBut this ablation tells us more about the task defi-\nnition than a model’s reliance on modalities.\nThe Perceptual Score (Gat et al., 2021) computes\nthe per-sample difference between the model’s ac-\ncuracy when working with the correct image and\ntext as input and with a random image or text.\nLXMERT’s Perceptual Score (Gat et al., 2021) is\n32.5 visual, 41.6 textual (relying more on text), but\nwe argued in §3.1 that does not reflect cases where\na model makes a wrong prediction because it failed\nto interpret the right cues correctly. MM-SHAP\nrates LXMERT vqa as balanced (51.5% T-SHAP).\n4.6 On the need of a MM score\nOur experiments show that a models’ reliance on\na modality can vary with each task, dataset and\ninstance. While prior work found that the models\nthey analysed all prefer a single modality that they\nrely on most, our analyses show that different VL\nmodels behave differently on the same task: AL-\nBEF is rather textual, CLIP balanced, LXMERT\nshows higher visual degree.\nFor LXMERT, we side with Frank et al. (2021),\nwho found it to have a higher visual preference –\nthis aligns with our analysis yielding a T-SHAP\nof 41.9%. We therefore disagree with Gat et al.\n(2021), who found a preference towards text.\nClearly, we do not assume that a MM model\nmust rely equally on multiple modalities, but there\nare cases where unimodal collapse is unwanted,\ni.e., a model gives the right answer for the wrong\nreason in tasks such as VQA. MM-SHAP helps\nidentify how much models rely on each modality.\n5 Conclusions and Future Work\nWe present MM-SHAP, a performance-agnostic\nmetric that measures the MM degree of VL models\nat dataset and sample level. Our results show that\non the same task, dataset, and on specific instances,\ndifferent types of models rely on modalities to dif-\nferent degrees and in different directions. Using\nMM-SHAP we are the first to quantify changes in\na model’s MM degree through fine-tuning. Our\nanalyses show that degrees of MM contributions\ncan be orthogonal to task performance, supporting\nthe need for performance-agnostic metrics. MM-\nSHAP is applicable to further modalities. It enables\nmodel-based data cleaning and thus, dataset bias\nremoval. Finally, it can serve as a diagnostic tool\nfor improving MM fusion methods.\nMM-SHAP can be used for testing true model\nunderstanding at a global and at instance level, and\nwhether a model is giving the right answer for\nthe right reasons, at corpus – and instance-level –\nwhich is not guaranteed for performance-dependent\nmetrics. It can help us track MM contributions dur-\ning (pre-)training and towards assessing and even-\ntually predicting how much a model needs to rely\non how many and which modalities in a given task\nor instance case – and how to explain this. We\nhence believe that many future research questions\nwill profit from our MM score as an unbiased MM\ncontribution metric, with AI research advancing\nto include more and more modalities beyond vi-\nsion and language (Girdhar et al., 2023): acoustics,\nhaptics, emotion, and more (cf. Parcalabescu et al.\n(2021b)).\nLimitations\nThis work focused on assessing multimodal degree\nfor recent English VL models. The following limi-\ntations can be relevant for future work.\nWe only evaluated a limited number of models\nin a zero-shot setting using their image-sentence\nalignment and VQA heads. Future work might be\ninterested in assessing more models and tracking\nthe evolution of MM-SHAP scores during model\npretraining and finetuning.\nThis work applied MM-SHAP to VL encoders.\nWe leave it for future work to investigate autore-\ngressive (decoder-only) VL models. In the time it\ntook to review and publish this work, we already\nencountered efforts to apply Shapley Values for\ninterpreting VL models in Cafagna et al. (2023).\nWe only applied ML-SHAP to VL models. Fu-\nture work might be interested in models working\nwith other or additional modalities beyond vision\nand language.\nComputing all possible coalitions between in-\nput tokens for Shapley Values is infeasible because\ntheir number is exponential in the number of tokens\n(2p). Therefore we perform Monte Carlo approxi-\nmation by randomly sub-sampling2p+1 coalitions.\nThis results in approximate MM-SHAP scores per\nsample. We argue that as an alternative, one can\nsimply increase the number of sampled coalitions\nfor more exact measurements (as we did 10-fold\n4040\nfor Fig. 1 and the examples in Appendix C) – at\nthe cost of increasing the environmental footprint.\nBut it is not necessary to increase the number of\nsamples when estimating MM-SHAP at dataset\nlevel, because the number of coalitions has very\nlittle effect on a data-set wide range – given that\napproximation fluctuations average out.\nTo compute MM-SHAP at data-set level, one\nneeds to run models in inference mode2p+1 times,\nwhere p is the number of tokens to mask (around\n40 in average for MSCOCO-sized captions). On\nan NVIDIA Titan X GPU, computing MM-SHAP\nfor one image-caption pair can take 2 seconds\nfor ALBEF, 3 seconds for CLIP. LXMERT is the\nmost expensive and needs 15 seconds, because it\ncomputes image features with a CNN backbone for\nevery masking configuration.\nEthical Considerations\nThis paper uses publicly available datasets and\nmodels and therefore could carry on their potential\nbiases (Meister et al., 2022; Garcia et al., 2023) and\nimperfections. However, the method presented in\nthis paper enables model and dataset interpretation\nand we hope that it can help future work locate\nharmful biases.\nAcknowledgements\nThe authors would like to thank the anonymous\nreviewers, Albert Gatt and Emanuele Bugliarello\nfor their useful suggestions. Thanks go to Nils\nTrost for assisting with the visualisations.\nThe authors acknowledge support by the state\nof Baden-Württemberg through bwHPC and the\nGerman Research Foundation (DFG) through grant\nINST 35/1597-1 FUGG.\nReferences\nAlexander Binder, Grégoire Montavon, Sebastian La-\npuschkin, Klaus-Robert Müller, and Wojciech Samek.\n2016. Layer-wise relevance propagation for neural\nnetworks with local renormalization layers. In Inter-\nnational Conference on Artificial Neural Networks,\npages 63–71. Springer.\nMichele Cafagna, Lina M Rojas-Barahona, Kees van\nDeemter, and Albert Gatt. 2023. Interpreting vision\nand language generative models with semantic visual\npriors. arXiv preprint arXiv:2304.14986.\nJize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-Chun\nChen, and Jingjing Liu. 2020. Behind the scene: Re-\nvealing the secrets of pre-trained vision-and-language\nmodels. In European Conference on Computer Vi-\nsion, pages 565–580. Springer.\nHila Chefer, Shir Gur, and Lior Wolf. 2021a. Generic\nattention-model explainability for interpreting bi-\nmodal and encoder-decoder transformers. In Pro-\nceedings of the IEEE/CVF International Conference\non Computer Vision, pages 397–406.\nHila Chefer, Shir Gur, and Lior Wolf. 2021b. Trans-\nformer interpretability beyond attention visualization.\nIn Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 782–\n791.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El\nKholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. Uniter: Universal image-text\nrepresentation learning. In ECCV.\nIan Covert, Scott M Lundberg, and Su-In Lee. 2020.\nUnderstanding global feature contributions with ad-\nditive importance measures. Advances in Neural\nInformation Processing Systems, 33:17212–17223.\nStella Frank, Emanuele Bugliarello, and Desmond\nElliott. 2021. Vision-and-language or vision-for-\nlanguage? on cross-modal influence in multimodal\ntransformers. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 9847–9857, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nNoa Garcia, Yusuke Hirota, Yankun Wu, and Yuta\nNakashima. 2023. Uncurated image-text datasets:\nShedding light on demographic bias. In Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 6957–6966.\nItai Gat, Idan Schwartz, and Alex Schwing. 2021. Per-\nceptual score: What data modalities does your model\nperceive? Advances in Neural Information Process-\ning Systems, 34.\nRohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Man-\nnat Singh, Kalyan Vasudev Alwala, Armand Joulin,\nand Ishan Misra. 2023. Imagebind: One em-\nbedding space to bind them all. arXiv preprint\narXiv:2305.05665.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017. Making the v in vqa\nmatter: Elevating the role of image understanding\nin visual question answering. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, pages 6904–6913.\nJack Hessel and Lillian Lee. 2020. Does my multimodal\nmodel learn cross-modal interactions? it’s harder to\ntell than you might think! In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 861–877, Online.\nAssociation for Computational Linguistics.\n4041\nDrew A Hudson and Christopher D Manning. 2019.\nGqa: A new dataset for real-world visual reasoning\nand compositional question answering. In Proceed-\nings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 6700–6709.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot Explanation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3543–3556, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nThéo Jaunet, Corentin Kervadec, Romain Vuillemot,\nGrigory Antipov, Moez Baccouche, and Christian\nWolf. 2021. Visqa: X-raying vision and language\nreasoning in transformers. IEEE Transactions on Vi-\nsualization and Computer Graphics, 28(1):976–986.\nKushal Kafle, Robik Shrestha, and Christopher Kanan.\n2019. Challenges and prospects in vision and lan-\nguage research. Frontiers in Artificial Intelligence,\n2:28.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al.\n2017. Visual genome: Connecting language and vi-\nsion using crowdsourced dense image annotations.\nInternational journal of computer vision, 123(1):32–\n73.\nGen Li, Nan Duan, Yuejian Fang, Ming Gong, and\nDaxin Jiang. 2020. Unicoder-vl: A universal encoder\nfor vision and language by cross-modal pre-training.\nIn The Thirty-Fourth AAAI Conference on Artificial\nIntelligence, AAAI 2020, The Thirty-Second Innova-\ntive Applications of Artificial Intelligence Conference,\nIAAI 2020, The Tenth AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence, EAAI 2020,\nNew York, NY, USA, February 7-12, 2020 , pages\n11336–11344. AAAI Press.\nJunnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,\nShafiq Joty, Caiming Xiong, and Steven Chu Hong\nHoi. 2021a. Align before fuse: Vision and language\nrepresentation learning with momentum distillation.\nAdvances in Neural Information Processing Systems,\n34.\nJunnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,\nShafiq Joty, Caiming Xiong, and Steven Chu Hong\nHoi. 2021b. Align before fuse: Vision and language\nrepresentation learning with momentum distillation.\nAdvances in Neural Information Processing Systems,\n34.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A sim-\nple and performant baseline for vision and language.\nIn Arxiv.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C. Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In Computer Vision –\nECCV 2014, pages 740–755, Cham. Springer Inter-\nnational Publishing.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.\n2019. Vilbert: Pretraining task-agnostic visiolinguis-\ntic representations for vision-and-language tasks. In\nAdvances in Neural Information Processing Systems,\npages 13–23.\nScott M Lundberg and Su-In Lee. 2017. A unified ap-\nproach to interpreting model predictions. Advances\nin neural information processing systems, 30.\nYiwei Lyu, Paul Pu Liang, Zihao Deng, Ruslan\nSalakhutdinov, and Louis-Philippe Morency. 2022.\nDime: Fine-grained interpretations of multimodal\nmodels via disentangled local explanations. In Pro-\nceedings of the 2022 AAAI/ACM Conference on AI,\nEthics, and Society, pages 455–467.\nPranava Swaroop Madhyastha, Josiah Wang, and Lu-\ncia Specia. 2018. Defoiling foiled image captions.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 2 (Short Papers), pages 433–438, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nNicole Meister, Dora Zhao, Angelina Wang, Vikram V\nRamaswamy, Ruth Fong, and Olga Russakovsky.\n2022. Gender artifacts in visual datasets. arXiv\npreprint arXiv:2206.09191.\nVictor Milewski, Miryam de Lhoneux, and Marie-\nFrancine Moens. 2022. Finding structural knowl-\nedge in multimodal-BERT. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n5658–5671, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nChristoph Molnar. 2022. Interpretable Machine Learn-\ning, 2 edition.\nVicente Ordonez, Girish Kulkarni, and Tamara Berg.\n2011. Im2text: Describing images using 1 million\ncaptioned photographs. Advances in neural informa-\ntion processing systems, 24.\nLetitia Parcalabescu, Michele Cafagna, Lilitta Murad-\njan, Anette Frank, Iacer Calixto, and Albert Gatt.\n2022. V ALSE: A task-independent benchmark for\nvision and language models centered on linguistic\nphenomena. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 8253–8280, Dublin,\nIreland. Association for Computational Linguistics.\nLetitia Parcalabescu, Albert Gatt, Anette Frank, and\nIacer Calixto. 2021a. Seeing past words: Testing\nthe cross-modal capabilities of pretrained V&L mod-\nels on counting tasks. In Proceedings of the 1st\nWorkshop on Multimodal Semantic Representations\n4042\n(MMSR), pages 32–44, Groningen, Netherlands (On-\nline). Association for Computational Linguistics.\nLetitia Parcalabescu, Nils Trost, and Anette Frank.\n2021b. What is multimodality? In Proceedings\nof the 1st Workshop on Multimodal Semantic Repre-\nsentations (MMSR), pages 1–10, Groningen, Nether-\nlands (Online). Association for Computational Lin-\nguistics.\nVitali Petsiuk, Abir Das, and Kate Saenko. 2018. RISE:\nrandomized input sampling for explanation of black-\nbox models. CoRR, abs/1806.07421.\nBryan A Plummer, Liwei Wang, Chris M Cervantes,\nJuan C Caicedo, Julia Hockenmaier, and Svetlana\nLazebnik. 2015. Flickr30k entities: Collecting\nregion-to-phrase correspondences for richer image-\nto-sentence models. In Proceedings of the IEEE\ninternational conference on computer vision, pages\n2641–2649.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning, pages 8748–8763. PMLR.\nMarco Ribeiro, Sameer Singh, and Carlos Guestrin.\n2016. “why should I trust you?”: Explaining the pre-\ndictions of any classifier. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Demon-\nstrations, pages 97–101, San Diego, California. As-\nsociation for Computational Linguistics.\nRamprasaath R Selvaraju, Michael Cogswell, Abhishek\nDas, Ramakrishna Vedantam, Devi Parikh, and\nDhruv Batra. 2017. Grad-cam: Visual explanations\nfrom deep networks via gradient-based localization.\nIn Proceedings of the IEEE international conference\non computer vision, pages 618–626.\nL. S. Shapley. 1953. 17. A Value for n-Person Games,\npages 307–318. Princeton University Press.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic im-\nage captioning. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2556–2565,\nMelbourne, Australia. Association for Computational\nLinguistics.\nRavi Shekhar, Sandro Pezzelle, Yauhen Klimovich, Au-\nrélie Herbelot, Moin Nabi, Enver Sangineto, and Raf-\nfaella Bernardi. 2017. FOIL it! find one mismatch\nbetween image and language caption. In Proceed-\nings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 255–265, Vancouver, Canada. Association for\nComputational Linguistics.\nRavi Shekhar, Ece Takmaz, Raquel Fernández, and Raf-\nfaella Bernardi. 2019. Evaluating the representa-\ntional hub of language and vision models. In Pro-\nceedings of the 13th International Conference on\nComputational Semantics - Long Papers, pages 211–\n222, Gothenburg, Sweden. Association for Computa-\ntional Linguistics.\nHao Tan and Mohit Bansal. 2019. LXMERT: Learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5100–5111, Hong Kong, China. Association for Com-\nputational Linguistics.\nTristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet\nSingh, Adina Williams, Douwe Kiela, and Candace\nRoss. 2022. Winoground: Probing vision and lan-\nguage models for visio-linguistic compositionality.\nIn Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 5238–\n5248.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention is not\nnot explanation. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 11–20, Hong Kong, China. Association for\nComputational Linguistics.\nLicheng Yu, Patrick Poirson, Shan Yang, Alexander C\nBerg, and Tamara L Berg. 2016. Modeling context\nin referring expressions. In European Conference on\nComputer Vision, pages 69–85. Springer.\n4043\nA Experimental Details\nMasking VL models predict their outputs (such\nas ISA) on full and uncorrupted image and text in-\nputs. To compute Shapley values and with them the\nMM-SHAP score, we create coalitions by mask-\ning image and text tokens. For masking text, we\nreplace the text tokens with the [MASK] token.\nFor masking images we mask out image patches\nsetting pixel values to zero. The patches are the\nregions for which we compute Shapley values, as\nvisualised in Figures 2 to 9. By masking these\npatches, the SHAP algorithm can estimate how\nthe prediction of the model changes in all possible\ncombinations of their presence or absence.\nAfter zero-ing out the patches, the models work\nas usual: LXMERT with the Faster-RCNN back-\nbone computes image features and extracts image\ntokens. Working on the image level has the up-\nside that no neighborhood information can leak\ninto each image token: If we were to mask out\non feature-level of the Faster-RCNN, i.e., on rect-\nangular regions, the other regions would possibly\n“know about” the other regions due to the hierar-\nchical structure of the CNN. For CLIP, the CLIP\nimage encoder works as usual: It works internally\nwith 32x32 patches of images in which we have\nalready zeroed out information.\nTherefore this masking procedure has the upside\nof being directly applicable to different types of\nVL model architectures, since some apply trans-\nformers directly on the image (CLIP and ALBEF),\nwhile others compute image tokens (features) with\na different CNN-based backbone (LXMERT).\nFor computing Shapley values, we aim for a\nbalance between text and image sequence length\nto make MM-SHAP adaptable to variable caption\nlengths and variable image sizes. Therefore we\nuse the text length to dynamically determine patch\nsizes: For longer text, we use more and smaller\npatches and for shorter text, less but bigger patches.\nIn the majority of our experiments, we have 16\nimage patches. We illustrate the image tiling in the\ntop right of Figures 2 to 9.\nThis masking procedure has several advantages:\ni) It adapts to variable caption lengths and variable\nimage sizes, and ii) it directly applies to differ-\nent types of VL model architectures, since some\napply transformers directly on the image (CLIP\nand ALBEF), while others compute image tokens\n(features) with a different CNN-based backbone\n(LXMERT).\nSpecial tokens When computing token-wise con-\ntributions, we do not take [SEP] and [CLS] tokens\ninto account (i.e., they are always assigned zero\ncontribution), since their functionality is to aggre-\ngate cross-modal information, e.g. for classifica-\ntion, and hence they cannot be attributed to one\nmodality exclusively.\nB Additional results\nDue to space constraints, we could not include full\ndetailed results on V ALSE\n in 2. Here, we present\nTable 3, which is an extended version of Table 2\nincluding the MM-SHAP scores for foils too, rather\nthan just the captions. It also includes fanned out\naccuracies over matching image-captions accc and\nmismatching image-foils accf .\nB.1 Correlation between accuracy and\nMM-SHAP\nFor each model and instrument on V ALSE\n , we\ncomputed the Spearman correlation coefficient be-\ntween the sample’s accuracy and textual degree.\nThe correlations are very low, e.g., the correla-\ntion between accc and T-SHAPc is around 0.02 for\nmost instruments and models, rising to 0.12 in rare\ncases. This low correlation between accuracy and\nMM-SHAP indicates that they are not measuring\nthe same aspect: accuracy measures the models’\nperformance while MM-SHAP measures the de-\ngree to which a modality was used – independently\nof the success of its use.\nB.2 MM-SHAP difference between captions\nand foils\nWe do not find notable differences between foils\nand captions on V ALSE\n in terms of MM-SHAP\n(cf. Table 3), while we find clear differences in\naccuracies between accc and accf , since they mea-\nsure the model’s preference towards one side in the\nbinary classification. Similar MM-SHAP scores\nbetween captions and foils speak for their ability to\ncapture how the model’s input matters for the pre-\ndiction, independently on which class the decision\nfalls onto. A notable exception is the difference be-\ntween T-SHAPc and T-SHAPf for LXMERT and\nALBEF refoco on Foil-it! (underlined numbers in\nTable 3).\nC Sample-level Analyses with MM-SHAP\nSEE FIGURES ON FOLLOWING PAGES!\n4044\nMetric Model Existence Plurality Counting Sp.rel.‡ Action Coreference Foil-it! Avg. MM\nquantifiers number bal.† sns.† adv.† relations repl.† swap† std.† clean nouns ±stdev. skew\nRandom 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 ±0\naccr\nCLIP 66.9 56.2 62.1 62.5 57.5 64.3 75.6 68.6 52.1 49.7 88.8 64.0 ±11\nLXMERT 78.6 64.4 62.2 69.2 42.6 60.2 54.8 45.8 46.8 44.2 87.1 59.6 ±15\nA mscoco 78.6 80.1 71.8 74.3 68.9 74.6 79.8 62.6 62.2 59.6 97.0 73.6±11\nA flickr 80.6 78.9 71.0 73.6 64.3 73.3 82.4 55.5 59.9 57.7 96.6 72.1 ±12\nA refcoco 73.1 69.0 67.9 70.7 45.7 68.6 79.9 58.9 52.7 43.3 96.5 66.0±15\nA vqa 40.8 63.3 49.0 49.2 23.2 61.9 51.7 52.0 55.9 43.3 67.2 50.7±12\nacc\nLXMERT 55.8 55.1 52.0 55.4 49.4 50.7 51.1 48.5 49.8 49.0 70.8 53.4 ±6\nA mscoco 56.7 60.2 55.4 53.9 56.0 52.3 63.7 54.0 52.7 52.0 76.3 57.6 ±7\nA flickr 55.6 56.3 53.8 53.3 55.4 52.3 64.9 48.9 50.0 50.0 70.5 55.5 ±6\nA refcoco 53.4 56.3 51.1 51.1 48.4 51.1 63.1 51.2 50.7 49.3 77.4 54.8 ±8\nA vqa 52.8 50.0 50.0 50.0 51.1 53.5 50.0 50.0 51.4 50.0 53.7 51.1 ±1\naccc\nLXMERT 41.6 68.0 50.9 50.0 61.5 73.1 35.8 36.8 81.2 80.8 72.3 59.3 ±17\nA mscoco 18.4 93.2 26.7 23.7 34.6 95.9 66.2 64.9 87.0 89.4 96.1 63.3 ±32\nA flickr 28.7 94.0 43.1 41.2 50.8 96.8 65.1 64.2 91.5 96.2 97.5 69.9 ±26\nA refcoco 33.7 89.8 41.8 31.0 57.2 93.1 72.5 75.0 81.4 90.4 92.7 69.0 ±24\nA vqa 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ±0\naccf\nLXMERT 70.1 42.2 53.0 60.8 37.3 28.4 66.4 60.2 18.4 17.3 69.3 47.6 ±20\nA mscoco 91.5 27.1 82.0 87.2 80.9 9.2 61.7 42.3 16.1 12.5 52.1 51.1 ±32\nA flickr 82.4 18.5 66.4 70.9 58.6 7.1 63.3 38.8 8.2 4.8 42.4 41.9 ±28\nA refcoco 71.3 19.4 62.0 72.9 41.8 10.5 53.2 29.7 18.4 8.7 61.19 40.8 ±25\nA vqa 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 ±0\nT-SHAP\nc\nCLIP 44.7 52.3 51.5 51.8 52.1 50.9 50.0 49.7 52.1 52.6 49.9 50.7±2 bal.\nLXMERT 51.7 37.1 46.5 47.3 46.4 36.6 42.1 42.2 38.2 37.2 36.1 41.9±5 vis.\nA mscoco 56.7 63.5 58.3 58.0 59.5 64.1 61.7 61.5 61.9 61.4 63.9 60.9±3 txt.\nA flickr 59.5 61.7 59.6 59.8 59.5 61.6 59.8 58.9 60.9 61.9 63.5 60.6±1 txt.\nA refcoco 53.3 57.2 55.4 55.1 55.8 57.0 54.5 54.4 57.9 58.9 56.8 56.0±2 txt.\nA vqa 64.6 63.6 62.5 61.4 63.4 63.0 59.3 60.3 63.6 63.1 62.1 62.4±2 txt.\nT-SHAP\nf\nCLIP 45.2 53.0 50.8 51.7 51.1 51.0 48.3 48.2 52.4 52.1 50.0 50.3 ±2 bal.\nLXMERT 52.3 39.4 48.2 48.8 45.8 36.5 43.9 42.7 39.1 38.6 45.0 43.7±5 vis.\nA mscoco 57.2 62.8 57.7 56.0 57.0 64.6 61.9 63.2 61.9 61.8 65.8 60.9±3 txt.\nA flickr 56.1 61.9 57.8 57.8 58.5 62.5 59.3 61.9 61.1 62.1 61.7 60.1±2 txt.\nA refcoco 56.1 58.5 56.2 55.6 57.8 57.6 55.5 56.9 58.4 58.4 61.3 57.5±2 txt.\nA vqa 64.0 64.7 61.9 60.9 61.2 63.2 59.9 60.1 63.4 62.4 62.2 62.2 ±2 txt.\nTable 3: Performance and multimodal score of VL models on the instruments of the V ALSE\n benchmark. We\nbold-face high accuracies and multimodally unbalanced models on tasks. accr is the pairwise ranking accuracy,\nconsidering predictions as correct if p(caption, img) > p(foil, img). Overall foil task performance acc is the\nmean of accc and accf (equal number of samples, all pairs). A stands for ALBEF models fine-tuned on different\ntasks and datasets: image retrieval on MSCOCO and Flickr30k, visual grounding on RefCOCO+ and VQA. †bal.\nCounting balanced. †sns. Counting small numbers. adv. Counting adversarial. repl. Action replacement. swap.\nActant swap. ‡ Sp.rel. Spatial relations. †std. Coreference standard. MM skew: Modality on which a model\nrelies more: bal. balanced, vis. visual, txt. textual. We test CLIP in pairwise ranking mode only (CLIP works\ncontrastively).\nFigures 2 to 9 contain sample-level visualisa-\ntions for each model for images and i) captions that\nmatch and ii) foils / random captions that show low\n/ high discrepancy mismatch with the images, as\nintroduced in Section 4.4:\n• There is low discrepancy between images and\nfoils obtained from V ALSE\n targeting spe-\ncific linguistic phenomena, with only a phrase\ndiffering between the caption and the foil. We\nselected examples for different phenomena:\nFigure 2 (noun phrase), 3 (action replacement,\neasy example), 4 (counting), 5 (positive ex-\nistence), 6 (negative existence), 9 (action re-\nplacement, hard example).\n• There is high discrepancy between\nMSCOCO images and randomly chosen\ncaptions in terms of low ISA between image\nand random caption – Figures 7 (easier\nexample) and 8 (harder example).\nIn Figure 2 we reiterate Figure 1 from the main\npaper with more detail:\n• CLIP correctly predicts a foil in the pairwise\naccuracy setting, since the ISA score for the\ncaption (30.3) is higher than for the foil (29.9),\nbut fails to identify that “keyboard” should not\ncontribute towards a high ISA. It successfully\npredicts caption alignment, but seems to mis-\nunderstand the meaning of the word “shines”\nand its instantiation in the image.\n• ALBEF mscoco is the only model to predict\nISA (99.4%) on the caption with coherent –\nbut mostly textual – indicators. It fails on foil\nprediction, still relying on the same textual\nindicators, and on the visual side focuses on\n4045\ncounter-evidence regions, erroneously taking\nthem as positive support for ISA.\n• LXMERT predicts correct ISA for the caption\n(99.5% ISA), using few relevant textual tokens\nas indicators, and possibly useful support-\ning visual tokens (focuses the fingers of the\ntwo hands). It fails to detect the foil (99.4%\nISA which is higher than a 50% classification\nthreshold and just slightly below the ISA for\nthe caption): counterevidence from textual to-\nkens is out-weighted by a single strong indica-\ntor (thumb); visual tokens confirm ISA despite\nfocusing on counterevidence (the phone).\nOn the following pages we present Figures 4 to 9\nwith more samples and their analyses.\nWe sampled the instances based on the following\ncriteria: i) low / high discrepancy; ii) interesting\nV ALSE\n instruments; iii) easier (no cluttering, no\ndark spots, no blur) and iv) harder examples (e.g.,\nhard to recognise the statue as such in Figure 9).\nThrough Fig. 4 to 9, we observe some patterns:\nModel performance does not tell much about the\nmultimodal degree. A correct ISA score (high\nfor the caption, low for the random caption/foil) is\nnot always accompanied by a sensible contribution\npattern in terms of Shapley values as seen for exam-\nple in Figures 2 and 4 for CLIP and LXMERT. The\nShapley values computed on the image and text\nside deliver much better intuition about what was\nsuccessfully aligned and what was not grounded\ncorrectly. Among all models, LXMERT seems to\nbe most affected by high discrepancy between per-\nformance and image and text token contributions.\nEasy examples deliver more robust contribu-\ntion patterns. On easy examples (Figures 3 and\n4), where the model generally performs well, we\ncan see how in the low discrepancy cases where\ncaption and foil differ in only one word, the one\nword difference does not change the contribution\npatterns much. In contrast, low discrepancy hard\nexamples (Figures 8 – unusual bed and bedroom\narrangement and 9 – hard to recognise the goat as a\nstatue without world knowledge) deliver different\npatterns on caption and foil, indicating confusion\nfrom the models.\nPositive existence is easier than negative exis-\ntence. When comparing Figures 5 and 6 we get\nsome insight into how the models’ image-sentence\nFigure 2: Low discrepancy noun phrase foil: Image-\nsentence alignment score (ISA) of the six VL models\nwith their textual degree T-SHAP (in %). Each text\nand image token (image patch) is colour-coded: Blue\ntokens contribute to a high ISA, while red ones lower\nthe ISA. The visual degree is 100 −T-SHAP. Note that\nthe ISA of CLIP is an absolute score, while ALBEF and\nLXMERT predict ISA probabilities. With\n we mark\ncorrect ISA and highlight the correct / foil token that\ncontributes in the right direction for aligning the image\nand the caption. With\n , we mark incorrect ISA and\nwrong contribution directions.\nalignment pretraining objective affects their be-\nhaviour:\nFor positive existence, where the caption indi-\ncates that an object is present in the image – as\nin Fig. 5: There are children. – is better handled by\nthe models, delivering more sensible patterns for\nimage-caption pairs. The contribution patterns on\nthe negated version of the existence sentence – the\nfoil There are no children. – show that some mod-\n4046\nels handled the negation correctly (CLIP, LXMERT,\nALBEF mscoco and refcoco), while the rest do not.\nNegative existence, where the caption indicates\nthat an object is not present in the image – as\nseen in Fig. 6: There are no humans in the picture.\n– seems more difficult to align, since the objects\nare not present in the image and to assign a high\nISA for text mentions that cannot be located, the\nmodel needs to understand the negation. The foil,\nchanging the sentence to affirmative – There are\nhumans in the picture. – turns the instance into a\nmuch simpler case of no image-sentence alignment,\nas is often seen during pretraining. Unsurprisingly,\nall models correctly predict a low ISA in Figure 6.\nCounting is hard. In Figure 4 for the counting\nfoils in V ALSE\n , CLIP is the only model that as-\nsigns higher ISA for the image-caption pair and not\nto the image-foil pair. Overall, the contribution pat-\nterns look scattered: High visual contributions in\nthe image indicate that the models align the plane\nobject to its mention in the sentence, but we see\nconfused textual contributions from the mentioned\nnumber of planes (0 or 4) in the text. This is unsur-\nprising, given the low performance of VL models\nin counting as highlighted by Parcalabescu et al.\n(2021a).\nD Why not to use Attention for defining a\nMultimodality Score\nD.1 Requirements for a MM Score\nFor defining a multimodality score that aims at\nquantifying each modality’s contribution to any\nmodel prediction, we need an interpretability\nmethod that has crucial properties to do so. With\nthe properties of efficiency, symmetry, dummy vari-\nable, additivity (see §3.2), Shapley values provide\nimportant ingredients for sample-based explana-\ntions that can be aggregated in a straightforward\nway into dataset-level explanations for machine\nlearning methods (Covert et al., 2020). Other inter-\npretability methods lack the robustness and theoret-\nical foundation to produce a multimodality score\nthat is comparable to the one proposed in our work.\nIn particular, attention – while being widely used\nfor generating visually appealing heat-maps – does\nnot fulfil the condition of delivering a fair pay-\nout (like Shapley values do) and it is questionable\nhow much high/low attention scores correlate with\nhigh/low contributions of input features for system\npredictions (Jain and Wallace, 2019; Wiegreffe and\nPinter, 2019).7 Attention linearly combines input\nfeatures and determines how much of each token\nis mixed with every other token. But it does not\nnecessarily mean that a low attention value cannot\nhave a large impact on the decision of the model.\nIn other words, a pinch of salt is enough to make\nfood taste good: Even if the attention score for\nsalt is low, its contribution to the taste of the food\n(captured by Shapley values) is high.\nAttention is present in transformers in multiple\nlayers and to complicate the matter even further,\neach attention layer contains multiple attention\nheads. Hence, to visualise attention we need a\ncarefully designed interface, as proposed, e.g., by\nJaunet et al. (2021) https://visqa.liris.\ncnrs.fr/ to keep a reasonable overview of all\nattention values. When integrating the multiple at-\ntention values and propagating them back to the\ninput to assign relevancy values for image and text\ntokens, research strives to generate simple expla-\nnations that represent the most important tokens\nand tend to inhibit the rest, as can be seen on the\nprogress from Chefer et al. (2021b) to Chefer et al.\n(2021a) (cf. Figure 4 in Chefer et al. (2021a)).\nD.2 Measuring negative contribution\nWhile Shapley values estimate both the positive and\nthe negative contributions of input tokens towards\nthe model prediction – which is relevant for foil\nwords –, attention (Chefer et al., 2021a) allows for\npositive-only relevance assessments.\nIn Figures 10 and 11, we have visualised CLIPs\nattention-based relevancy for the image-caption\nand foil examples shown in Figures 2 to 7 using\nthe method of Chefer et al. (2021a). On the image\nside, we observe little to no changes in the atten-\ntion visualisation, when comparing image-caption\nto image-foil pairs (cf. Figure 10). Even more,\non the text side, both the correct and the foil word\ncarry relatively similar attention scores, with no\nindication whether this contributes positively or\nnegatively towards the model prediction. Shapley\nvalues however, are sensitive to foil words and we\ncan visualise whether the word contributes towards\nraising the ISA (high image-sentence match) or\nlowering the ISA (e.g., Figure 3).\nBesides the problematic interpretation of atten-\ntion as feature contribution and the many ways of\naggregating and propagating the different attention\n7Arguably this may be the case when attention weights are\nhigh, but it is clearly not the case when attention weights are\nlow.\n4047\nvalues to the input, another problem with attention\nis that it is unclear how to disentangle and aggre-\ngate the textual self-attention, visual self-attention,\ntext-to-image attention and image-to-text attention\ninto a single multimodality score that assesses the\ndegree to which a given modality contributes to-\nwards the model prediction.\nAll things considered, we argue that attention\nis not well-suited as a basis for a multimodality\nscore we aim for in this work, but that Shapley\nvalues – as presented in this paper – are, thanks\nto their theoretical properties (efficiency, symme-\ntry, dummy variable, additivity) and their property\nof being model-agnostic measurements of input\nfeature contributions.\nSEE FIGURES ON FOLLOWING PAGES!\n4048\nFigure 3: Low discrepancy (V ALSE\n action replacement): Image-sentence alignment score (ISA) of the six VL\nmodels with their textual degree T-SHAP (in %). Each text and image token (image patch) is colour-coded: Blue\ntokens contribute to a high ISA, while red ones lower the ISA. The visual degree is 100 − T-SHAP. Note that the\nISA of CLIP is an absolute score, while ALBEF and LXMERT predict ISA probabilities. With\n we mark correct\nISA and an highlight the correct / foil token that contributes in the right direction for aligning the image and the\ncaption. With\n , we mark incorrect ISA and wrong contribution directions.\n4049\nFigure 4: Low discrepancy (V ALSE\n counting): Image-sentence alignment score (ISA) of the six VL models\nwith their textual degree T-SHAP (in %). Each text and image token (image patch) is colour-coded: Blue tokens\ncontribute to a high ISA, while red ones lower the ISA. The visual degree is 100 − T-SHAP. Note that the ISA of\nCLIP is an absolute score, while ALBEF and LXMERT predict ISA probabilities. With\n we mark correct ISA\nand an highlight the correct / foil token that contributes in the right direction for aligning the image and the caption.\nWith\n , we mark incorrect ISA and wrong contribution directions.\n4050\nFigure 5: Low discrepancy (V ALSE\n existence positive): Image-sentence alignment score (ISA) of the six VL\nmodels with their textual degree T-SHAP (in %). Each text and image token (image patch) is colour-coded: Blue\ntokens contribute to a high ISA, while red ones lower the ISA. The visual degree is 100 − T-SHAP. Note that the\nISA of CLIP is an absolute score, while ALBEF and LXMERT predict ISA probabilities. With\n we mark correct\nISA and an highlight the correct / foil token that contributes in the right direction for aligning the image and the\ncaption. With\n , we mark incorrect ISA and wrong contribution directions.\n4051\nFigure 6: Low discrepancy (V ALSE\n existence negative – harder phenomenon than positive existence): Image-\nsentence alignment score (ISA) of the six VL models with their textual degreeT-SHAP (in %). Each text and image\ntoken (image patch) is colour-coded: Blue tokens contribute to a high ISA, while red ones lower the ISA. The visual\ndegree is 100 − T-SHAP. Note that the ISA of CLIP is an absolute score, while ALBEF and LXMERT predict ISA\nprobabilities. With\n we mark correct ISA and an highlight the correct / foil token that contributes in the right\ndirection for aligning the image and the caption. With\n , we mark incorrect ISA and wrong contribution directions.\n4052\nFigure 7: High discrepancy (MSCOCO): Image-sentence alignment score (ISA) of the six VL models with their\ntextual degree T-SHAP (in %). Each text and image token (image patch) is colour-coded: Blue tokens contribute\nto a high ISA, while red ones lower the ISA. The visual degree is 100 − T-SHAP. Note that the ISA of CLIP is\nan absolute score, while ALBEF and LXMERT predict ISA probabilities. With\n we mark correct ISA and an\nhighlight one important token that contributes in the right direction for aligning the image and the caption. With\n ,\nwe mark incorrect ISA and wrong contribution directions.\n4053\nFigure 8: High discrepancy (MSCOCO) hard example where the models have trouble recognising the bed: Image-\nsentence alignment score (ISA) of the six VL models with their textual degreeT-SHAP (in %). Each text and image\ntoken (image patch) is colour-coded: Blue tokens contribute to a high ISA, while red ones lower the ISA. The visual\ndegree is 100 − T-SHAP. Note that the ISA of CLIP is an absolute score, while ALBEF and LXMERT predict ISA\nprobabilities. With\n we mark correct ISA and highlight one important token that contributes in the right direction\nfor aligning the image and the caption. With\n , we mark incorrect ISA and wrong contribution directions.\n4054\nFigure 9: Low discrepancy (V ALSE\n action replacement) – hard example where models and humans have trouble\nrecognising the goat as a statue): Image-sentence alignment score (ISA) of the six VL models with their textual\ndegree T-SHAP (in %). Each text and image token (image patch) is colour-coded: Blue tokens contribute to a\nhigh ISA, while red ones lower the ISA. The visual degree is 100 − T-SHAP. Note that the ISA of CLIP is an\nabsolute score, while ALBEF and LXMERT predict ISA probabilities. With\n we mark correct ISA and highlight\nthe correct / foil token that contributes in the right direction for aligning the image and the caption. With\n , we\nmark incorrect ISA and wrong contribution directions.\n4055\nFigure 10: Low discrepancy . CLIP results of attention-based relevance visualisation, using\nthe method of Chefer et al. (2021a) https://huggingface.co/spaces/PaulHilders/\nCLIPGroundingExplainability. Red means high relevancy, blue is zero relevancy and there is\nno negative relevancy (while Shapley values do allow for positive and negative contributions). Note that the\nheat-maps give the impression that the relevance irradiates from single spots. This is an artefact from the\nvisualisation since the model works with 32x32 pixel patches and it is these patches that each have a relevance\nscore. For reference: the images are around 500 pixels in height and width.\n4056\nFigure 11: High discrepancy . CLIP results of attention-based relevance visualisation, using\nthe method of Chefer et al. (2021a) https://huggingface.co/spaces/PaulHilders/\nCLIPGroundingExplainability. Red means high relevancy, blue is zero relevancy and there is\nno negative relevancy (while Shapley values do allow for positive and negative contributions). Note that the\nheat-maps give the impression that the relevance irradiates from single spots. This is an artefact from the\nvisualisation since the model works with 32x32 pixel patches and it is these patches that each have a relevance\nscore. For reference: the images are around 500 pixels in height and width.\n4057\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitations (section 6)\n□\u0013 A2. Did you discuss any potential risks of your work?\nEthical Considerations (section 7)\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract, section 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n4\n□\u0013 B1. Did you cite the creators of artifacts you used?\n4\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\n4\nC □\u0013 Did you run computational experiments?\n6, 4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n6\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n4058\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nNot applicable. Left blank.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n5\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n4\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n4059"
}