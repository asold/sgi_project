{
    "title": "GridFormer: Point-Grid Transformer for Surface Reconstruction",
    "url": "https://openalex.org/W4393149117",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2096179091",
            "name": "Shengtao Li",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2105102234",
            "name": "Ge Gao",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2104360253",
            "name": "Yudong Liu",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2479163717",
            "name": "Yu-Shen Liu",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2133243365",
            "name": "Ming Gu",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2096179091",
            "name": "Shengtao Li",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2105102234",
            "name": "Ge Gao",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2104360253",
            "name": "Yudong Liu",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2479163717",
            "name": "Yu-Shen Liu",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2133243365",
            "name": "Ming Gu",
            "affiliations": [
                "Tsinghua University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6771217994",
        "https://openalex.org/W4224943919",
        "https://openalex.org/W3110366555",
        "https://openalex.org/W3174178900",
        "https://openalex.org/W6809564636",
        "https://openalex.org/W6687484953",
        "https://openalex.org/W4312261929",
        "https://openalex.org/W2905188570",
        "https://openalex.org/W3200254434",
        "https://openalex.org/W3009443497",
        "https://openalex.org/W2531409750",
        "https://openalex.org/W2342277278",
        "https://openalex.org/W2594519801",
        "https://openalex.org/W6763283787",
        "https://openalex.org/W3006956544",
        "https://openalex.org/W6753343845",
        "https://openalex.org/W6793697131",
        "https://openalex.org/W6853435903",
        "https://openalex.org/W6648630804",
        "https://openalex.org/W4221153094",
        "https://openalex.org/W3098125324",
        "https://openalex.org/W2229412420",
        "https://openalex.org/W4377371825",
        "https://openalex.org/W2211722331",
        "https://openalex.org/W2904383797",
        "https://openalex.org/W2971547687",
        "https://openalex.org/W6811122175",
        "https://openalex.org/W2909750748",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W6796823060",
        "https://openalex.org/W6774877540",
        "https://openalex.org/W3014169829",
        "https://openalex.org/W2948913018",
        "https://openalex.org/W3121736960",
        "https://openalex.org/W3197617697",
        "https://openalex.org/W4221144437",
        "https://openalex.org/W6762903858",
        "https://openalex.org/W4310998470",
        "https://openalex.org/W2945957791",
        "https://openalex.org/W4281704462",
        "https://openalex.org/W6788305448",
        "https://openalex.org/W3035507572",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W4312950511",
        "https://openalex.org/W4289549306",
        "https://openalex.org/W2560609797",
        "https://openalex.org/W4386066503",
        "https://openalex.org/W3184957317",
        "https://openalex.org/W2971278627",
        "https://openalex.org/W4394671432",
        "https://openalex.org/W2963627347",
        "https://openalex.org/W3214168277",
        "https://openalex.org/W3035291735",
        "https://openalex.org/W4233857083",
        "https://openalex.org/W2981978060",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2993195924",
        "https://openalex.org/W3153465022",
        "https://openalex.org/W4312907000",
        "https://openalex.org/W2944579304",
        "https://openalex.org/W4386076153",
        "https://openalex.org/W4312597904",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2963926543",
        "https://openalex.org/W4226106508",
        "https://openalex.org/W4312616477",
        "https://openalex.org/W4386076147",
        "https://openalex.org/W3117476483",
        "https://openalex.org/W2963121255",
        "https://openalex.org/W3119331429",
        "https://openalex.org/W2962849139",
        "https://openalex.org/W4287864845",
        "https://openalex.org/W3034395814",
        "https://openalex.org/W1992642990"
    ],
    "abstract": "Implicit neural networks have emerged as a crucial technology in 3D surface reconstruction. To reconstruct continuous surfaces from discrete point clouds, encoding the input points into regular grid features (plane or volume) has been commonly employed in existing approaches. However, these methods typically use the grid as an index for uniformly scattering point features. Compared with the irregular point features, the regular grid features may sacrifice some reconstruction details but improve efficiency. To take full advantage of these two types of features, we introduce a novel and high-efficiency attention mechanism between the grid and point features named Point-Grid Transformer (GridFormer). This mechanism treats the grid as a transfer point connecting the space and point cloud. Our method maximizes the spatial expressiveness of grid features and maintains computational efficiency. Furthermore, optimizing predictions over the entire space could potentially result in blurred boundaries. To address this issue, we further propose a boundary optimization strategy incorporating margin binary cross-entropy loss and boundary sampling. This approach enables us to achieve a more precise representation of the object structure. Our experiments validate that our method is effective and outperforms the state-of-the-art approaches under widely used benchmarks by producing more precise geometry reconstructions. The code is available at https://github.com/list17/GridFormer.",
    "full_text": "GridFormer: Point-Grid Transformer for Surface Reconstruction\nShengtao Li1,2, Ge Gao1,2*, Yudong Liu1,2, Yu-Shen Liu2, Ming Gu1,2\n1Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China\n2School of Software, Tsinghua University, Beijing, China\nlist21@mails.tsinghua.edu.cn, gaoge@tsinghua.edu.cn,\nliuyd23@mails.tsinghua.edu.cn, liuyushen@tsinghua.edu.cn, guming@tsinghua.edu.cn\nAbstract\nImplicit neural networks have emerged as a crucial tech-\nnology in 3D surface reconstruction. To reconstruct contin-\nuous surfaces from discrete point clouds, encoding the in-\nput points into regular grid features (plane or volume) has\nbeen commonly employed in existing approaches. However,\nthese methods typically use the grid as an index for uni-\nformly scattering point features. Compared with the irreg-\nular point features, the regular grid features may sacrifice\nsome reconstruction details but improve efficiency. To take\nfull advantage of these two types of features, we introduce\na novel and high-efficiency attention mechanism between\nthe grid and point features named Point-Grid Transformer\n(GridFormer). This mechanism treats the grid as a trans-\nfer point connecting the space and point cloud. Our method\nmaximizes the spatial expressiveness of grid features and\nmaintains computational efficiency. Furthermore, optimizing\npredictions over the entire space could potentially result in\nblurred boundaries. To address this issue, we further pro-\npose a boundary optimization strategy incorporating margin\nbinary cross-entropy loss and boundary sampling. This ap-\nproach enables us to achieve a more precise representation\nof the object structure. Our experiments validate that our\nmethod is effective and outperforms the state-of-the-art ap-\nproaches under widely used benchmarks by producing more\nprecise geometry reconstructions. The code is available at\nhttps://github.com/list17/GridFormer.\nIntroduction\nPerceiving and modeling the surrounding world are essen-\ntial tasks in 3D computer vision. Point clouds obtained from\nvarious sensors allow us to capture discrete spatial informa-\ntion about 3D surfaces directly. Surface reconstruction plays\na vital role in converting this discrete representation into a\ncontinuous one. Recently, learning-based approaches have\ngained significant popularity in reconstructing point clouds.\nThese implicit methods employ a conversion process that\ntransforms the input point cloud into a global feature to rep-\nresent the spatial continuous field of a 3D shape. However,\nbridging the gap between the continuous space and the dis-\ncrete point cloud poses challenges, resulting in varying re-\nconstruction outcomes across different representations.\n*Corresponding author: Ge Gao\nCopyright ¬© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n(a) Ground Truth (b) POCO\n(c) ALTO (d) Ours\nFigure 1: Visualization of the complex scene reconstruction\nresults on the Synthetic Rooms dataset (Peng et al. 2020).\nOur method can produce high-fidelity reconstructions com-\npared with the point-based method POCO (Boulch and Mar-\nlet 2022) and the grid-based method ALTO (Wang et al.\n2023).\nTypically, these methods encode the point cloud using ei-\nther a single global feature or regular local grid features. The\nregular grid features are learned by uniformly distributing\nthe point-wise features across each grid. While regular grid\nfeatures capture information averagely from every point of\nthe space, they may overlook the shape details in the point\ncloud. Some other methods will attach the features to the\ninput points (Boulch and Marlet 2022; Zhang, Nie√üner, and\nWonka 2022) or move the regular grid features close to the\nsurface (Li et al. 2022). These irregular features can repre-\nsent the 3D shape more accurately. However, connecting the\nirregular features with the space can be difficult. Also, the\ntarget occupancy function is not differentiable or even not\ncontinuous at the zero level. This intrinsic property increases\nthe error bound and makes training more difficult.\nTo address these challenges, we propose a novel and\nhighly efficient attention mechanism that bridges the space\nand point cloud by treating the grid as a transfer point. Fig-\nure 2 shows the difference between our approach and the\nprevious techniques based on point or grid structures. The\npoint-based method can effectively obtain the shape infor-\nmation from the point cloud, but the irregular structure of\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n3163\n(a) (b) (c) (d)\nInput Points Grid Points Query Points\nFigure 2: Comparisons between our GridFormer and other\nmethods. The colorized arrows in (a), (c), and (d) repre-\nsent learnable weights for scattering point or grid features.\n(a) The point-based approach expresses the query point fea-\nture by aggregating the nearby point features with learnable\nweights. (b) The grid-based approach learns the grid features\nby uniformly scattering the point features. The decoder ag-\ngregates the grid features by the weights calculated by bilin-\near or trilinear interpolation. (c) The attention-based decoder\nin ALTO (Wang et al. 2023) makes the weights between the\nquery and grid points learnable. (d) Our point-grid trans-\nformer learns the weights between the input and grid fea-\ntures. This enables our method to approximate (a) through\ngrid points while maintaining high efficiency.\nthe points significantly decreases efficiency. Our approach\nleverages the concept of point-grid attention to model the\ngrid feature. This enables our network to learn the relation-\nship between the input and grid features, which can implic-\nitly bridge the space and the points. And the visual recon-\nstruction differences between these methods can be found in\nFigure 1.\nIn particular, apart from employing uniform sampling, we\nhave devised a two-stage training strategy. It incorporates\nmargin binary cross-entropy loss and boundary sampling to\nnarrow down the error bound caused by the discontinuity\nproperty, ensuring a more precise reconstruction result.\nOur contributions can be listed as follows:\n‚Ä¢ We introduce the Point-Grid Transformer (GridFormer)\nfor surface reconstruction. Our method significantly im-\nproves the spatial expressiveness of grid features for\nlearning implicit neural fields.\n‚Ä¢ We design a two-stage training strategy incorporating\nmargin binary cross-entropy loss and boundary sampling.\nThis strategy enhances our model‚Äôs predictive capability\nby yielding a more precise occupancy field near the sur-\nface.\n‚Ä¢ Both object-level and scene-level reconstruction experi-\nments validate our method, demonstrating its effective-\nness and ability to produce accurate reconstruction re-\nsults.\nRelated Work\n3D Representations\nExplicit Representations. V oxels are amongst the most\nwidely used shape representations (Maturana and Scherer\n2015; Choy et al. 2016). However, as the resolution in-\ncreases, the memory consumed by voxels increases dramat-\nically. Different from voxels, point clouds represent a 3D\nshape as a set of discrete points (Qi et al. 2016, 2017). These\npoints are irregularly distributed in space and lack continu-\nous topological relationships. Hence post-processing steps\n(Kazhdan and Hoppe 2013) are needed to extract continu-\nous surface. Meshes (Gkioxari, Malik, and Johnson 2019;\nPan et al. 2019) avoid the complexity brought by voxels and\ncan better represent the topological structure. However, gen-\nerating mesh directly from the neural network is also more\ncomplicated. Most meshed-based methods require deform-\ning geometric primitives (Groueix et al. 2018a) or templates\nof fixed topology (Groueix et al. 2018b).\nNeural Implicit Representations. Implicit representation\ncharacterizes the whole space by predicting each point as in-\nside, outside, or on the surface. It relies on a neural network\nacting as the function to model the binary occupancy field\n(Chen and Zhang 2019; Mescheder et al. 2019; Sitzmann,\nZollhoefer, and Wetzstein 2019) or distance field (Gropp\net al. 2020; Park et al. 2019; Atzmon and Lipman 2020;\nTakikawa et al. 2021) and then uses the marching cubes\n(Lorensen and Cline 1987) algorithm to extract the mesh.\nThe implicit function typically receives a query point with a\nfeature and outputs the corresponding occupancy or distance\nvalue. A single global feature has been applied to represent\ndifferent shapes initially but it cannot capture local details.\nTo resolve this, several works explored capturing local fea-\ntures both in the 2D image field (Saito et al. 2019, 2020;\nXu et al. 2019) and in the 3D point cloud field (Chibane,\nAlldieck, and Pons-Moll 2020; Peng et al. 2020; Chen, Liu,\nand Han 2022; Baorui et al. 2022).\nTo obtain more faithful geometric features, some subse-\nquent works also explored multi-resolution (Takikawa et al.\n2021; Chen et al. 2021; Huang et al. 2023), irregular or dy-\nnamic feature representations (Li et al. 2022; Boulch and\nMarlet 2022; Zhang, Nie√üner, and Wonka 2022). Indeed,\ncompared to the regular gird features (Peng et al. 2020; Tang\net al. 2021; Lionar et al. 2021), the irregular feature repre-\nsentation is more compact and better suited to capture the\ndetails. Some other works (Baorui et al. 2021; Ben-Shabat,\nHewa Koneputugodage, and Gould 2022; Ma et al. 2023)\nuse the gradient or divergence to constrain the implicit fields\nfor better reconstruction. Due to the intrinsic properties of\nthe occupancy field, we adopt the margin to optimize our\nestimated occupancy function.\nTransformers for Point Cloud\nTransformer was first applied in NLP tasks (Vaswani et al.\n2017) and has made a great success. It relies on a self-\nattention mechanism to capture the relationships between\ndifferent words. This practical approach has also brought\nabout innovations in other fields. Recently several works,\nlike Point Transformer (Zhao et al. 2021) and Point Cloud\nTransformer (Guo et al. 2021), have explored the application\nof transformers in point cloud processing. However, due to\nthe discrete distribution of point clouds, many of these ap-\nproaches rely on the k-nearest neighbor (kNN) search to find\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n3164\n(b) 2D Point-Grid Transformer Layer\nz\ny\nx\nz\ny\nx\n(R, 32) (R, 64) (R/2, 128) (R/4, 256) (R/2, 128) (R, 64) (R, 32)\nz\ny\nx\nUniformly\nScattering\nQuery Point q\nOccupancy\nProbability\nPoint-Grid\nTransformer Layer MLP Upsampling Downsampling\nPoint Features\nGrid (Plane/V olume) Features\n(a) GridFormer\nBilinear or Trilinear\nInterpolation\nq\nInput\nPoint Features Aggregation Plane Features Aggregation Point Features Sampling\nCNN\nValues\nKeys\nPos Enc\nQueries\nSoftmax\n(c) Detailed Structure of the Point-Grid Attention\nFigure 3: Overview of our method. (a) The architecture of GridFormer. (b) The 2D plane point-grid transformer layer in which\nthe colorized arrows represent learnable weights. (c) The detailed structure of the point-grid attention mechanism for point\nfeatures aggregation. ‚ÄòPos Enc‚Äô denotes position encoding.\nthe nearest neighboring points. As the size of the point cloud\nincreases, the kNN search becomes more complex and com-\nputationally expensive. Fast Point Transformer (Park et al.\n2022) designs a lightweight self-attention layer that uses the\nvoxel hashing-based architecture to boost computational ef-\nficiency. Our method also utilizes grids for the aggregation\nof point features. But our innovation lies in utilizing the fixed\ngrid not only for acceleration but also to connect the space\nand the point cloud.\nMethod\nOur method aims to establish an efficient attention mecha-\nnism for connecting the space and the point cloud. Figure\n3(a) shows an overview of our network structure. Based on\nour point-grid transformer layer, the model constructs a con-\ntinuous occupancy function o : R3 ‚Üí [0, 1]. For a point\ncloud P = {pi | pi ‚àà R3}, we first learn the per-point fea-\ntures by applying a small point-wise multi-layer perception\n(MLP). Then the grid (plane or volume) features are initial-\nized by scattering the point features uniformly. The U-Net-\nlike network is constructed based on the point-grid trans-\nformer layer. Each layer takes in point and grid features. In\nthe following sections, we will elaborate on the point-grid\ntransformer layer, the multi-resolution decoder, and our sub-\nsequent optimization strategy.\nPoint-Grid Transformer Layer\nGiven the point cloud P, we define the fp, fg, fq as the fea-\ntures of input, grid, and query points, and the p, g, qrepre-\nsent the corresponding points. œï and œà represent the MLP\nand convolutional neural network (CNN), respectively. The\nsteps of the point-grid transformer layer, as illustrated in Fig-\nure 3(b) and (c), will be described in detail below.\nPosition Encoding. We convert the point clouds from the\nglobal coordinate system to the local coordinate system of\nthe grid where the point is situated. Then we use an MLP\nœïpos with two linear layers and one ReLU nonlinearity func-\ntion. It takes the localized points as input and outputs the\nposition encoding, as denoted by\nfpos = œïpos(p ‚àí ‚åä(p √ó r)‚åã/r), (1)\nwhere r represents the plane or volume resolution.\nPoint Features Aggregation. This section will describe\nthe point-grid attention mechanism used to aggregate the\npoint features. We leverage the point transformer mecha-\nnism in (Zhao et al. 2021) to learn the weights between the\npoint and grid features. The detailed structure is shown in\nFigure 3(c). A small CNN œàk is applied to the grid fea-\ntures to learn the keys, and two MLP networks œïq and œïv\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n3165\nare utilized to learn the queries and values of the point fea-\ntures, respectively. We also add position encoding in both the\npoint-grid attention generation branch and the feature trans-\nformation branch following (Zhao et al. 2021). The point-\ngrid weights can be represented as follows:\nœâij = œïw(œàk(fgi ) ‚àí œïq(fpj ) +fpos). (2)\nHere the point pj is situated in the same grid Ngi as the\ngrid point gi. The softmax function is applied to normalize\nthe weights in the same grid. Then we aggregate the point\nfeatures by our learned point-grid weights:\nfgi =\nX\npj‚ààNgi\nœâij(œïv(fpj ) +fpos). (3)\nGrid Features Aggregation. To increase the receptive\nfield of each grid, we further aggregate the neighbor grid\nfeatures by a small CNN. Considering our motivation and\nthe lightweight network design, we replace the CNN with\nthe depth-wise convolutional network (Chollet 2017) in the\nlast three point-grid transformer layers directly connected to\nthe decoder. The depth-wise convolution convolves each in-\nput channel with a different kernel which acts as the shared\nweights between the grid features.\nPoint Features Sampling. Merely updating the point fea-\ntures from fpj to œïv(fpj ) +fpos will disregard the point\nfeatures within the neighborhood. To address this issue, we\nopt to sample the point features from the hybrid grid features\nusing bilinear or trilinear interpolation denoted as S. Simul-\ntaneously, considering that the grid features inherently con-\ntain position encoding, we omit this component during this\nstage. The revised point features can be mathematically ex-\npressed as\nfpi = œïv(fpj ) +S(fg). (4)\nThroughout this entire stage, the grid features only serve\nthe purpose of computing the keys. Consequently, we imple-\nment a skip connection for both the point and grid features\nwithin one attention layer.\nMulti-resolution Decoder\nFor any given query pointq, we can sample the query feature\nby bilinear or trilinear interpolation, leveraging our learned\ngrid features as shown in Figure 2(d). The weights in the\ninterpolation are fixed for a certain query point since they\nare computed by the relative distance. An improved method\nis learning the weights also through the attention mechanism\nbetween the query and grid points like the Figure 2(c). How-\never, In the feature transfer chain from the input point cloud\nto the grid points, then to the query points, we already make\nthe weights of the front half part learnable. At the same time,\nthe number of query points will increase rapidly when we\nneed a high-resolution reconstruction result. An attention-\nbased decoder will consume more time to decode the occu-\npancy value for each query point.\nBased on the aforementioned factors, we still keep the in-\nterpolation method to sample the query features. We opt for\ngrid features f = (f1, f2, f3) from two different resolutions\nas illustrated in Figure 3(a). We scale the different feature\nùëü\n1 + ùëí‚àíùëö ‚àí 1(1 + ùëíùëö )‚àí1\n0 1\nFigure 4: Illustration of boundary optimization.\ndimensions to 32 through a shallow MLP to keep the same\nsettings as other methods. We use the same network oŒ∏ for\nthe decoder as (Peng et al. 2020), which takes in the accu-\nmulated feature fq ‚àà Fand the query point q ‚àà R3 and\noutputs the occupancy:\noŒ∏ : R3 √ó F ‚Üí[0, 1]. (5)\nBoundary Optimization\nIn this section, we propose boundary optimization for the\noccupancy function. According to the definition of the occu-\npancy function o : R3 ‚Üí [0, 1], it is not continuous and not\ndifferential on the surface. Based on these intrinsic features,\nwe adopt the margin binary cross-entropy loss to finetune\nour model with the boundary sampling.\nBoundary Sampling. The uniform sampling strategy has\nbeen proven to be the most suitable training strategy in\n(Mescheder et al. 2019) as other alternative sampling strate-\ngies tend to introduce bias to the model. But for a precise re-\nconstruction, it is required to have accurate predictions near\nthe surface. Hence, we divide the whole training procedure\ninto two stages. At the first stage, we use uniform sampling\nfor training until the model converges. At the second stage,\nwe switch to boundary sampling. To ensure fairness, we ex-\ntract boundary regions from the original training data rather\nthan resampling the query points.\nSince both the noise levels and densities of the point\nclouds are different, extracting the region based on the input\npoint cloud is insufficient. We extract the boundary points\nbased on the ground-truth occupancy labels the same as\n(Tang et al. 2022). A point is a boundary point only if at\nleast one of its neighboring points lies on the opposite side\nof the surface. A fixed radius r is set to search for the oppo-\nsite points shown in Figure 4.\nMargin Binary Cross-entropy Loss. At the first stage,\nwe minimize the binary cross-entropy loss between the pre-\ndicted ÀÜoq and the ground-truth occupancy valuesoq with uni-\nformly sampled points q ‚àà R3:\nL(ÀÜoq, oq) =‚àí[oq ¬∑ log(ÀÜoq) + (1‚àí oq) ¬∑ log(1 ‚àí ÀÜoq)] (6)\nwhere oq is calculated by applying a sigmoid layer to the\noutput of the network o(q):\noq = 1\n1 +e‚àío(q) . (7)\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n3166\nMethods 3000 Pts, œÉ =\n0.005 1000 Pts, œÉ =\n0.005 300 Pts, œÉ =\n0.005\nIoU ‚Üë CD ‚Üì NC ‚Üë FS ‚Üë IoU ‚Üë CD ‚Üì NC ‚Üë FS ‚Üë IoU ‚Üë CD ‚Üì NC ‚Üë FS ‚Üë\nONet (Mescheder\net al. 2019) 0.761 0.87\n0.891 0.785 0.772 0.81\n0.894 0.801 0.778 0.80\n0.895 0.806\nConvONet (Peng et al. 2020) 0.884 0.44\n0.938 0.942 0.859 0.50\n0.929 0.918 0.821 0.59\n0.907 0.883\nPOCO (Boulch and Marlet 2022) 0.926 0.30\n0.950 0.984 0.884 0.40\n0.928 0.950 0.808 0.61\n0.892 0.869\nALTO (Wang et al. 2023) 0.930 0.30\n0.952 0.980 0.905 0.35\n0.940 0.964 0.863 0.47\n0.922 0.924\nOurs 0.936 0.28\n0.956 0.985 0.913 0.33\n0.946 0.970 0.866 0.46\n0.925 0.926\nTable 1: Comparison on the ShapeNet dataset with different point density levels. ‚ÄòPts‚Äô denotes input points andœÉ is the standard\ndeviation of the Gaussian noise.\nGT Input ConvONet POCO ALTO NKSR Ours\nFigure 5: Object-level reconstruction results on the\nShapeNet dataset. All the methods are trained and tested on\n3000 noisy points.\nA margin m is added directly to the output according to\nthe ground-truth label l ‚àà [0, 1]. Consequently, the new out-\nput is defined as:\noq = 1\n1 +e‚àí(o(q)‚àím√ó(l√ó2‚àí1)) . (8)\nAs shown in Figure 4, the margin binary cross-entropy loss\ncan make the predicted occupancy values as close to 0 or 1\nas possible.\nImplementation Details\nWe implement our model in Pytorch (Paszke et al. 2019) and\nuse the Adam optimizer (Kingma and Ba 2014). The learn-\ning rate is 10‚àí4 at the first stage, and 10‚àí6 at the finetune\nstage. The depth of our U-Net-like encoder is 4, and we do\nnot downsample or upsample the grid features in the two top\nlevels the same as (Wang et al. 2023). The radiusr to search\nfor the opposite points is set to 0.08 and the margin m is set\nto 2.0. At reference time, we apply Multiresolution IsoSur-\nface Extraction (MISE) (Mescheder et al. 2019) to obtain the\nmesh.\nExperiments\nDatasets, Metrics, and Baselines\nShapeNet. We use ShapeNet (Chang et al. 2015)\nfor object-level reconstruction evaluation. ShapeNet pre-\nprocessed by ONet (Mescheder et al. 2019) contains water-\ntight meshes of shapes in 13 classes, with train/val splits and\n8500 objects for testing. To comprehensively evaluate our\nmethod, we leverage two different settings for a fair com-\nparison. Following ALTO (Wang et al. 2023), we sample dif-\nferent densities of points and add Gaussian noise with zero\nmean and standard deviation of 0.005. To evaluate the effect\nof noise, we follow NKSR (Huang et al. 2023) to sample\npoints with different noise levels.\nScene-Level Datasets. For our scene-level reconstruction,\nwe use the Synthetic Rooms dataset (Peng et al. 2020) and\nScanNet-v2 (Dai et al. 2017). The Synthetic Rooms dataset\ncomprises 5000 synthetic room scenes containing randomly\nplaced walls, floors, and ShapeNet objects. We utilize the\nsame train/validation/test division as previously established.\nThe ScanNet-v2 dataset includes 1513 scans of real-world\nenvironments featuring a diverse selection of room types.\nThe meshes provided in ScanNet-v2 are not watertight, so\nmodels are trained using the Synthetic Rooms dataset and\nthen tested on ScanNet-v2. This enables the evaluation of\nthe generalization performance of our method.\nEvaluation Metrics. Following ConvONet (Peng et al.\n2020), we use the volumetric IoU, Chamfer-L 1 dis-\ntance √ó102 (CD), normal consistency (NC), and F-Score\n(Tatarchenko* et al. 2019) with threshold value 1% (FS)\nmetrics for our evaluation. Other used metrics also include\nthe Chamfer-L2 distance (L2-CD). Please refer to the sup-\nplementary of (Peng et al. 2020) for the mathematical de-\ntails.\nBaselines. To evaluate the validity of our attention mech-\nanism, the baselines used for comparison include ONet\n(Mescheder et al. 2019), ConvONet (Peng et al. 2020),\nPOCO (Boulch and Marlet 2022), and ALTO (Wang et al.\n2023). In addition to these, we also include SPSR (Kazh-\ndan and Hoppe 2013), SAP (Peng et al. 2021), and NKSR\n(Huang et al. 2023). Please note that NKSR utilized point\nnormals in most of their experiments. To ensure fairness in\nour comparison, we only evaluate their results from training\nwithout point normals.\nObject-level Reconstruction\nWe first evaluate our method on the task of single-object\nreconstruction. The quantitative results of different density\nlevels are shown in Table 1. Our method performs better\nthan the point-based and other grid-based methods. We also\nfind that when the points are too sparse (300 points), the im-\nprovement will become smaller because learning the weight\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n3167\n(a) Ground Truth (b) Input Points (c) ConvONet (d) POCO (e) ALTO (f) Ours\nFigure 6: Scene-level comparisons on the Synthetic Rooms dataset. Our method preserves most of the details of the furniture.\nMethods\n1000 Pts\n3000 Pts 3000 Pts\nœÉ = 0.0 œÉ =\n0.005 œÉ = 0.025\nIoU ‚Üë CD ‚Üì IoU ‚Üë CD ‚Üì IoU ‚Üë CD ‚Üì\nConvONet 0.823 0.61\n0.880 0.44 0.787 0.73\nSAP 0.908 0.34\n0.911 0.33 0.829 0.53\nPOCO 0.927 0.30\n0.926 0.30 0.817 0.58\nALTO 0.940 0.29\n0.931 0.30 0.839 0.51\nNKSR 0.934 0.26 0.926 0.27 0.829 0.50\nOurs 0.946 0.28 0.936 0.28 0.844 0.50\nTable 2: Comparison on the ShapeNet dataset with different\nnoise levels.\nfor a single point is meaningless. This also verifies the effec-\ntiveness of our attention mechanism. We also evaluate the\neffect of noise following the setting of NKSR (Huang et al.\n2023) in Table 2. Qualitative comparisons are provided in\nFigure 5. Compared with other baselines, our method can\ncapture more details, and the overall topology of the shape\nis more unified and consistent.\nScene-level Reconstruction\nWe report numerical comparisons in Table 3 with previous\npoint-based and grid-based methods. The comparison veri-\nfies the validity of our proposed mechanism on the scenes.\nWe further present the visual comparison in Figure 6, which\nshows that our method achieves more detailed reconstruc-\ntion results, particularly for finer structures.\nReal-world Generalization\nWe also explore the generalization performance of our\nmethod under the ScanNet-v2 dataset in Table 4. Figure 7\nillustrates that our method can reconstruct a smoother and\nmore complete surface than the other methods. At the same\ntime, we also find that since the Synthetic Rooms dataset\nonly contains regularly generated walls and floors, which are\ndifferent from the real-scanned rooms as shown in Figure 6\nand Figure 7. This issue causes all methods to try to com-\nplete the irregular walls and floors to some extent, like the\nsecond group of reconstruction results in Figure 7. The com-\npleted parts will greatly influence the metrics. For a more\nMethods IoU ‚Üë CD ‚Üì NC ‚Üë FS ‚Üë\nONet 0.475 2.03\n0.783 0.541\nSPSR - 2.23\n0.866 0.810\nConvONet 0.849 0.42\n0.915 0.964\nDP-ConvONet 0.800 0.42\n0.912 0.960\nPOCO 0.884 0.36\n0.919 0.980\nALTO 0.914 0.35\n0.921 0.981\nOurs 0.918 0.34\n0.926 0.983\nTable 3: Comparison on the Synthetic Rooms dataset.\nMethods CD ‚Üì NC ‚Üë FS ‚Üë\nConvONet 0.80 0.816\n0.810\nDP-ConvONet 1.35 0.769\n0.682\nPOCO 0.74 0.813\n0.816\nALTO 0.79 0.802\n0.809\nOurs 0.71 0.822\n0.846\nTable 4: Comparison of the generalization performance on\nthe ScanNet dataset. All methods are trained on the Syn-\nthetic Rooms dataset and tested on ScanNet-v2 without\nfloors.\naccurate comparison, we remove the floors following the\nmethod used in ConvONet (Peng et al. 2020) but keep the\nwalls since they are randomly seated throughout the scene.\nAblation Study\nGrid Representation. We report the effect of our point-\ngrid attention for different representations (triplane and vol-\nume) in table 5 on the Synthetic Rooms dataset. All methods\nuse the same decoder without attention.\nGrid Downsampling. We experiment without downsam-\npling in our U-Net-like encoder and the results are shown\nin Table 6. It takes less time to encode the input points but\nthe reconstruction results are significantly inferior. Thus we\nconsider it worthy to consume a little bit more time for a\nbetter result.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n3168\n(a) Ground Truth (b) Input Points (d) ConvONet (d) POCO (d) ALTO (d) Ours\nFigure 7: Generalization performance on the ScanNet dataset. These methods are trained on the Synthetic Rooms dataset and\ntested on the ScanNet-v2 dataset. The red squares show all the methods will complete the floor to some extent.\nMethod IoU ‚Üë CD ‚Üì NC ‚Üë FS ‚Üë\nConvONet\n(3 ‚àó 1282) 0.805 0.44\n0.903 0.948\nConvONet (643) 0.849 0.42\n0.915 0.964\nALTO (3‚àó 1282) 0.834 0.43\n0.906 0.960\nALTO (643) 0.903 0.36\n0.920 0.981\nOurs (\n3 ‚àó 1282) 0.835 0.40\n0.900 0.949\nOurs (643) 0.918 0.34\n0.926 0.983\nTable 5: Ablation on the network framework.\nIOU ‚Üë CD ‚Üì NC ‚Üë Time\n(s) ‚Üì\nw/o Do\nwn. 0.897 0.38\n0.949 0.39\nw/ Down. 0.942 0.30\n0.962 0.41\nTable 6: Ablation on the gird downsampling. ‚ÄòDown.‚Äô means\ngrid downsampling. ‚ÄòTime‚Äô refers to the encoding time.\n(a) Input Points (b) w/o Opt.\n0.01\n0\n(c) w/ Opt.\nFigure 8: The visual effect of boundary optimization.\nBoundary Optimization. We further explore the effect\nof our boundary optimization. Table 7 shows that the op-\ntimization works under different noise and density levels.\nWe also visualize the distance from reconstruction results to\nthe ground-truth meshes in Figure 8. The proposed boundary\noptimization can effectively help reduce the error bound.\n3000 Pts\n3000 Pts 1000 Pts 1000 Pts\nœÉ = 0.005 œÉ =\n0.025 œÉ = 0 œÉ = 0.005\nw/o Opt. 0.211 0.881\n0.253 0.371\nw/ Opt. 0.201 0.718\n0.247 0.328\nTable 7: Ablation study on boundary optimization in terms\nof L2-CD (√ó104). ‚ÄòOpt.‚Äô means optimization.\nMethod GPU Memory\n(MiB) Inference Time (s)\nConvONet 1957 0.43\nPOCO 6540 9.30\nAL\nTO 3257 7.96\nOurs 1915 5.61\nTable 8: GPU memory and runtime comparisons on\nShapeNet chairs.\nConclusion\nWe proposed the Point-Grid Transformer (GridFormer) us-\ning a novel point-grid attention mechanism between the\npoint and grid features. It is valid both for object-level and\nscene-level reconstruction and reconstructs a smoother sur-\nface on the unseen dataset. Compared with the attention-\nbased decoder used in the point-based and other grid-based\nmethods, our attention-based encoder costs less time and\nGPU memory (Table 8) and achieves comparable or even\nbetter results. Our introduced boundary optimization strat-\negy can reduce the error between the estimated and ground-\ntruth occupancy functions and help extract a more accurate\nsurface. In future work, exploring how to dynamically divide\nthe grid to achieve the attention mechanism between differ-\nent resolutions may apply this mechanism to more scenarios.\nAcknowledgments\nThe corresponding author is Ge Gao. This work was sup-\nported by the National Key Research and Development Pro-\ngram of China (2021YFB1600303).\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n3169\nReferences\nAtzmon, M.; and Lipman, Y . 2020. SAL: Sign Agnostic\nLearning of Shapes From Raw Data. In IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR).\nBaorui, M.; Yu-Shen, L.; Matthias, Z.; and Zhizhong, H.\n2022. Surface Reconstruction from Point Clouds by Learn-\ning Predictive Context Priors. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR).\nBaorui, M.; Zhizhong, H.; Yu-Shen, L.; and Matthias, Z.\n2021. Neural-Pull: Learning Signed Distance Functions\nfrom Point Clouds by Learning to Pull Space onto Surfaces.\nIn International Conference on Machine Learning (ICML).\nBen-Shabat, Y .; Hewa Koneputugodage, C.; and Gould, S.\n2022. DiGS: Divergence guided shape implicit neural rep-\nresentation for unoriented point clouds. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 19323‚Äì19332.\nBoulch, A.; and Marlet, R. 2022. POCO: Point Convo-\nlution for Surface Reconstruction. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 6302‚Äì6314.\nChang, A. X.; Funkhouser, T.; Guibas, L.; Hanrahan, P.;\nHuang, Q.; Li, Z.; Savarese, S.; Savva, M.; Song, S.; Su,\nH.; Xiao, J.; Yi, L.; and Yu, F. 2015. ShapeNet: An\nInformation-Rich 3D Model Repository. Technical Report\narXiv:1512.03012 [cs.GR], Stanford University ‚Äî Prince-\nton University ‚Äî Toyota Technological Institute at Chicago.\nChen, C.; Liu, Y .-S.; and Han, Z. 2022. Latent Partition\nImplicit with Surface Codes for 3D Representation. In Eu-\nropean Conference on Computer Vision (ECCV).\nChen, Z.; and Zhang, H. 2019. Learning Implicit Fields for\nGenerative Shape Modeling. In2019 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR).\nChen, Z.; Zhang, Y .; Genova, K.; Funkhouse, T.; Fanello,\nS.; Bouaziz, S.; Haene, C.; Du, R.; Keskin, C.; and Tang, D.\n2021. Multiresolution Deep Implicit Functions for 3D Shape\nRepresentation. In 2021 IEEE/CVF International Confer-\nence on Computer Vision, ICCV . IEEE.\nChibane, J.; Alldieck, T.; and Pons-Moll, G. 2020. Implicit\nFunctions in Feature Space for 3D Shape Reconstruction\nand Completion. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR). IEEE.\nChollet, F. 2017. Xception: Deep learning with depthwise\nseparable convolutions. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, 1251‚Äì\n1258.\nChoy, C. B.; Xu, D.; Gwak, J.; Chen, K.; and Savarese, S.\n2016. 3D-R2N2: A Unified Approach for Single and Multi-\nview 3D Object Reconstruction. In Proceedings of the Eu-\nropean Conference on Computer Vision (ECCV).\nDai, A.; Chang, A. X.; Savva, M.; Halber, M.; Funkhouser,\nT.; and Nie√üner, M. 2017. Scannet: Richly-annotated 3d\nreconstructions of indoor scenes. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 5828‚Äì5839.\nGkioxari, G.; Malik, J.; and Johnson, J. 2019. Mesh r-cnn.\nIn Proceedings of the IEEE/CVF international conference\non computer vision, 9785‚Äì9795.\nGropp, A.; Yariv, L.; Haim, N.; Atzmon, M.; and Lipman,\nY . 2020. Implicit Geometric Regularization for Learning\nShapes. In Proceedings of Machine Learning and Systems\n2020, 3569‚Äì3579.\nGroueix, T.; Fisher, M.; Kim, V . G.; Russell, B.; and Aubry,\nM. 2018a. AtlasNet: A Papier-M ÀÜach¬¥e Approach to Learn-\ning 3D Surface Generation. In Proceedings IEEE Conf. on\nComputer Vision and Pattern Recognition (CVPR).\nGroueix, T.; Fisher, M.; Kim, V . G.; Russell, B. C.; and\nAubry, M. 2018b. 3D-CODED: 3D Correspondences by\nDeep Deformation. In European Conference on Computer\nVision.\nGuo, M.-H.; Cai, J.-X.; Liu, Z.-N.; Mu, T.-J.; Martin, R. R.;\nand Hu, S.-M. 2021. PCT: Point cloud transformer. Compu-\ntational Visual Media, 7(2): 187‚Äì199.\nHuang, J.; Gojcic, Z.; Atzmon, M.; Litany, O.; Fidler, S.; and\nWilliams, F. 2023. Neural Kernel Surface Reconstruction.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 4369‚Äì4379.\nKazhdan, M.; and Hoppe, H. 2013. Screened poisson sur-\nface reconstruction. ACM Transactions on Graphics (ToG),\n32(3): 1‚Äì13.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980.\nLi, T.; Wen, X.; Liu, Y .-S.; Su, H.; and Han, Z. 2022. Learn-\ning Deep Implicit Functions for 3D Shapes with Dynamic\nCode Clouds. In IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition.\nLionar, S.; Emtsev, D.; Svilarkovic, D.; and Peng, S. 2021.\nDynamic Plane Convolutional Occupancy Networks. In\nWinter Conference on Applications of Computer Vision\n(WACV).\nLorensen, W. E.; and Cline, H. E. 1987. Marching Cubes:\nA High Resolution 3D Surface Construction Algorithm.\nIn Proceedings of the 14th Annual Conference on Com-\nputer Graphics and Interactive Techniques, SIGGRAPH\n‚Äô87, 163‚Äì169. New York, NY , USA: Association for Com-\nputing Machinery. ISBN 0897912276.\nMa, B.; Zhou, J.; Liu, Y .-S.; and Han, Z. 2023. Towards Bet-\nter Gradient Consistency for Neural Signed Distance Func-\ntions via Level Set Alignment. In Conference on Computer\nVision and Pattern Recognition (CVPR).\nMaturana, D.; and Scherer, S. 2015. V oxNet: A 3D Con-\nvolutional Neural Network for real-time object recognition.\nIn 2015 IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS).\nMescheder, L.; Oechsle, M.; Niemeyer, M.; Nowozin, S.;\nand Geiger, A. 2019. Occupancy Networks: Learning 3D\nReconstruction in Function Space. In 2019 IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR).\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n3170\nPan, J.; Han, X.; Chen, W.; Tang, J.; and Jia, K. 2019. Deep\nMesh Reconstruction from Single RGB Images via Topol-\nogy Modification Networks. In Proceedings of the IEEE\nInternational Conference on Computer Vision, 9964‚Äì9973.\nPark, C.; Jeong, Y .; Cho, M.; and Park, J. 2022. Fast Point\nTransformer. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n16949‚Äì16958.\nPark, J. J.; Florence, P.; Straub, J.; Newcombe, R.; and Love-\ngrove, S. 2019. DeepSDF: Learning Continuous Signed Dis-\ntance Functions for Shape Representation. In The IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR).\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga,\nL.; Desmaison, A.; Kopf, A.; Yang, E.; DeVito, Z.; Raison,\nM.; Tejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.; Bai,\nJ.; and Chintala, S. 2019. PyTorch: An Imperative Style,\nHigh-Performance Deep Learning Library. In Wallach, H.;\nLarochelle, H.; Beygelzimer, A.; d'Alch ¬¥e-Buc, F.; Fox, E.;\nand Garnett, R., eds., Advances in Neural Information Pro-\ncessing Systems, volume 32. Curran Associates, Inc.\nPeng, S.; Jiang, C. M.; Liao, Y .; Niemeyer, M.; Pollefeys,\nM.; and Geiger, A. 2021. Shape As Points: A Differentiable\nPoisson Solver. In Advances in Neural Information Process-\ning Systems (NeurIPS).\nPeng, S.; Niemeyer, M.; Mescheder, L.; Pollefeys, M.; and\nGeiger, A. 2020. Convolutional Occupancy Networks. In\nEuropean Conference on Computer Vision (ECCV).\nQi, C. R.; Su, H.; Mo, K.; and Guibas, L. J. 2016. Point-\nNet: Deep Learning on Point Sets for 3D Classification and\nSegmentation. arXiv preprint arXiv:1612.00593.\nQi, C. R.; Yi, L.; Su, H.; and Guibas, L. J. 2017. Point-\nNet++: Deep Hierarchical Feature Learning on Point Sets in\na Metric Space. arXiv preprint arXiv:1706.02413.\nSaito, S.; ; Huang, Z.; Natsume, R.; Morishima, S.;\nKanazawa, A.; and Li, H. 2019. PIFu: Pixel-Aligned Im-\nplicit Function for High-Resolution Clothed Human Digiti-\nzation. arXiv preprint arXiv:1905.05172.\nSaito, S.; Simon, T.; Saragih, J.; and Joo, H. 2020. PI-\nFuHD: Multi-Level Pixel-Aligned Implicit Function for\nHigh-Resolution 3D Human Digitization. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition.\nSitzmann, V .; Zollhoefer, M.; and Wetzstein, G. 2019. Scene\nRepresentation Networks: Continuous 3D-Structure-Aware\nNeural Scene Representations. In Wallach, H.; Larochelle,\nH.; Beygelzimer, A.; d'Alch¬¥e-Buc, F.; Fox, E.; and Garnett,\nR., eds., Advances in Neural Information Processing Sys-\ntems, volume 32. Curran Associates, Inc.\nTakikawa, T.; Litalien, J.; Yin, K.; Kreis, K.; Loop, C.;\nNowrouzezahrai, D.; Jacobson, A.; McGuire, M.; and Fidler,\nS. 2021. Neural Geometric Level of Detail: Real-time Ren-\ndering with Implicit 3D Shapes.\nTang, J.; Lei, J.; Xu, D.; Ma, F.; Jia, K.; and Zhang, L.\n2021. SA-ConvONet: Sign-Agnostic Optimization of Con-\nvolutional Occupancy Networks. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision.\nTang, L.; Zhan, Y .; Chen, Z.; Yu, B.; and Tao, D. 2022. Con-\ntrastive Boundary Learning for Point Cloud Segmentation.\nIn 2022 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 8479‚Äì8489.\nTatarchenko*, M.; Richter*, S. R.; Ranftl, R.; Li, Z.; Koltun,\nV .; and Brox, T. 2019. What Do Single-view 3D Reconstruc-\ntion Networks Learn?\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nWang, Z.; Zhou, S.; Park, J. J.; Paschalidou, D.; You, S.;\nWetzstein, G.; Guibas, L.; and Kadambi, A. 2023. ALTO:\nAlternating Latent Topologies for Implicit 3D Reconstruc-\ntion. In Proceedings IEEE Conf. on Computer Vision and\nPattern Recognition (CVPR).\nXu, Q.; Wang, W.; Ceylan, D.; Mech, R.; and Neumann,\nU. 2019. DISN: Deep Implicit Surface Network for High-\nquality Single-view 3D Reconstruction. In Wallach, H.;\nLarochelle, H.; Beygelzimer, A.; d'Alch ¬¥e-Buc, F.; Fox, E.;\nand Garnett, R., eds., Advances in Neural Information Pro-\ncessing Systems, volume 32. Curran Associates, Inc.\nZhang, B.; Nie√üner, M.; and Wonka, P. 2022. 3DILG: Irreg-\nular Latent Grids for 3D Generative Modeling. In Oh, A. H.;\nAgarwal, A.; Belgrave, D.; and Cho, K., eds., Advances in\nNeural Information Processing Systems.\nZhao, H.; Jiang, L.; Jia, J.; Torr, P. H.; and Koltun, V . 2021.\nPoint transformer. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, 16259‚Äì16268.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n3171"
}