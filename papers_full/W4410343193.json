{
  "title": "A framework to assess clinical safety and hallucination rates of LLMs for medical text summarisation",
  "url": "https://openalex.org/W4410343193",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2037946636",
      "name": "Elham Asgari",
      "affiliations": [
        "Nicolaus Copernicus University",
        "Guy's and St Thomas' NHS Foundation Trust"
      ]
    },
    {
      "id": "https://openalex.org/A2918950482",
      "name": "Nina Montana Brown",
      "affiliations": [
        "Nicolaus Copernicus University"
      ]
    },
    {
      "id": "https://openalex.org/A2602099610",
      "name": "Magda Dubois",
      "affiliations": [
        "Nicolaus Copernicus University"
      ]
    },
    {
      "id": "https://openalex.org/A2329729278",
      "name": "SALEH kHALIL",
      "affiliations": [
        "Nicolaus Copernicus University"
      ]
    },
    {
      "id": "https://openalex.org/A5093539167",
      "name": "Jasmine Balloch",
      "affiliations": [
        "Nicolaus Copernicus University"
      ]
    },
    {
      "id": "https://openalex.org/A2776633113",
      "name": "Joshua Au Yeung",
      "affiliations": [
        "Nicolaus Copernicus University"
      ]
    },
    {
      "id": "https://openalex.org/A2406048682",
      "name": "Dominic Pimenta",
      "affiliations": [
        "Nicolaus Copernicus University"
      ]
    },
    {
      "id": "https://openalex.org/A2037946636",
      "name": "Elham Asgari",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2918950482",
      "name": "Nina Montana Brown",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2602099610",
      "name": "Magda Dubois",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2329729278",
      "name": "SALEH kHALIL",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5093539167",
      "name": "Jasmine Balloch",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2776633113",
      "name": "Joshua Au Yeung",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2406048682",
      "name": "Dominic Pimenta",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4387500346",
    "https://openalex.org/W2114014706",
    "https://openalex.org/W4392668324",
    "https://openalex.org/W4323350039",
    "https://openalex.org/W4319301505",
    "https://openalex.org/W4392193048",
    "https://openalex.org/W4391494845",
    "https://openalex.org/W4388444944",
    "https://openalex.org/W4391158659",
    "https://openalex.org/W1994256263",
    "https://openalex.org/W2941604507",
    "https://openalex.org/W2166750785",
    "https://openalex.org/W4386576626",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4389072658",
    "https://openalex.org/W4388585881",
    "https://openalex.org/W4386942825",
    "https://openalex.org/W4390602555",
    "https://openalex.org/W4394759276",
    "https://openalex.org/W4392711449",
    "https://openalex.org/W4285242550",
    "https://openalex.org/W2152772232",
    "https://openalex.org/W4378770578",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4395483680",
    "https://openalex.org/W4391995913",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W4392077089",
    "https://openalex.org/W4391766565",
    "https://openalex.org/W4286905034",
    "https://openalex.org/W4313197536",
    "https://openalex.org/W4382930233",
    "https://openalex.org/W4386120650",
    "https://openalex.org/W4402909345",
    "https://openalex.org/W2963926728",
    "https://openalex.org/W3186721116",
    "https://openalex.org/W4318350718",
    "https://openalex.org/W4287213686",
    "https://openalex.org/W3116079511",
    "https://openalex.org/W2995225687",
    "https://openalex.org/W4385436582",
    "https://openalex.org/W4399803256",
    "https://openalex.org/W4404787535",
    "https://openalex.org/W4402348202",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4399911714",
    "https://openalex.org/W4386528723",
    "https://openalex.org/W4395049441",
    "https://openalex.org/W4393971165",
    "https://openalex.org/W4380353763",
    "https://openalex.org/W4404986534"
  ],
  "abstract": null,
  "full_text": "npj |digital medicine Article\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-025-01670-7\nA framework to assess clinical safety and\nhallucination rates of LLMs for medical\ntext summarisation\nCheck for updates\nElham Asgari1,2 , Nina Montaña-Brown1, Magda Dubois1, Saleh Khalil1, Jasmine Balloch1,\nJoshua Au Yeung1 & Dominic Pimenta1\nIntegrating large language models (LLMs) into healthcare can enhance workﬂow efﬁciency and patient\ncare by automating tasks such as summarising consultations. However, theﬁdelity between LLM\noutputs and ground truth information is vital to prevent miscommunication that could lead to\ncompromise in patient safety. We propose a framework comprising (1) an error taxonomy for\nclassifying LLM outputs, (2) an experimental structure for iterative comparisons in our LLM document\ngeneration pipeline, (3) a clinical safety framework to evaluate the harms of errors, and (4) a graphical\nuser interface, CREOLA, to facilitate these processes. Our clinical error metrics were derived from 18\nexperimental conﬁgurations involving LLMs for clinical note generation, consisting of 12,999 clinician-\nannotated sentences. We observed a 1.47% hallucination rate and a 3.45% omission rate. By reﬁning\nprompts and workﬂows, we successfully reduced major errors below previously reported human note-\ntaking rates, highlighting the framework’s potential for safer clinical documentation.\nO n eo ft h em o s ta p p e a l i n ga p p l i c a t i o n so fL L M si nh e a l t h c a r ei sf o r\nadministrative tasks1. Clinicians devote a substantial amount of time to\ndocumentation2, and prolonged interaction with electronic health records,\nwhere clinical documentation is logged, has been demonstrated to raise\ncognitive load and lead to burnout3. In fact, the use of LLMs for clinical\ndocumentation, especially clinical note generation 4 or consultation\nsummarisation5,6,i sa na c t i v ea r e ao fr e s e a r c h .\nHowever, LLMs are known to produce errors in many settings, from\ndocument summarisation7, to general reasoning tasks as well as more\nclinically relevant tasks 8. These errors can be categorised as\n“hallucinations”9: known as an event where LLMs generate information that\nis not present in the input data, or omissions: the event where LLMs miss\nrelevant information from the original document. Errors in clinical doc-\numentation generation can lead to inaccurate recording and communica-\ntion of facts\n10,11. Inaccuracies in the document summarisation task can\nintroduce misleading details8 into transcribed conversations or summaries,\npotentially delaying diagnoses12 and causing unnecessary patient anxiety.\nThe problem of hallucinations poses a signiﬁcant challenge to date1,13.\nThe occurrence of hallucinations has previously been attributed to the data\nquality during model training\n14,15, the type of model training methodology16\nand prompting strategies17.\nRecent work has established thathallucination may be an intrinsic,\ntheoretical property of all LLMs9. Consequently, there is a growing body of\nwork focused on the technical evaluation of LLM accuracy and the detection\nand mitigation of hallucinations in LLMs18. However, the prevalence, cau-\nsation, and evaluation of hallucinations in a clinical context, as well as their\nsubsequent impact on clinical safety, remains an open question.\nClinical documentation can be variable in quality19,20, and studies\nestimate that human-generated clinical notes have, on average, at least 1\nerror and 4 omissions21. Given the increased usage of LLMs for clinical\ndocumentation22,23, several methods have been proposed for evaluating\nclinical documentation generated using LLMs.\nRelevant clinical evaluation frameworks typically include categorising\nclinical errors for downstream analysis. Typically, these differ from tradi-\ntional natural language processing (NLP) taxonomies16, which have sepa-\nrated hallucination types into distinct categories, for example, into\n“intrinsic” and “extrinsic”\n24, “factuality” and “faithfulness”16, “factual\nmirage” and “silver lining”25 errors. The differences between general and\nclinical taxonomies arise from the necessity of increased granularity of\nclinical error types, which are not captured by the broader, general methods.\nFor example, Tierney et al.26 propose using a modiﬁed version of the\nPhysician Documentation Quality Instrument-9, accounting for halluci-\nnations and bias, while Abacha et al.\n23 propose evaluating clinical note\nquality using automated metrics. However, these relevant clinical categor-\nisations have not assessed the implications of the mistakes for\ndownstream harm.\nAutomated metrics, such as Recall-oriented Understudy for Gisting\nEvaluation (ROUGE)\n27, Bilingual Evaluation Understudy (BLEU)28 and\n1Tortus AI, London, UK.2Guy’s and St Thomas NHS Trust, London, UK.e-mail: asgelham@gmail.com\nnpj Digital Medicine|           (2025) 8:274 1\n1234567890():,;\n1234567890():,;\nBidirectional Encoder Representations from Transformers (BERT)-score29,\nwhile useful for comparing model-generated text with expert-written\nexamples, exhibit signiﬁcant limitations when applied to the evaluation of\nhealthcare-related content. These metrics, primarily focused on surface-\nlevel textual similarity, fail to capture the semantic nuances, contextual\ndependencies, and domain-speciﬁc knowledge crucial for accurate medical\ndiscourse30.T h i sd eﬁciency is particularly problematic in healthcare settings,\nwhere understanding complex medical concepts (e.g., symptoms, diagnoses,\ntreatments) and their interrelationshi p si sp a r a m o u n tf o rp a t i e n tw e l l - b e i n g\nand effective decision-making.\nDespite the exponential growth in benchmarks for model reasoning\nabilities31, the evaluation of LLMs on clinical tasks has typically been carried\nout via “question-answering” (QA) benchmarks5,8,32. These tasks assess\nmodels’accuracy over various clinical questions, typically derived from\nlicensing exams. While these methods offer insights into the factual\nknowledge and reasoning abilities of LLMs, they do not assess clinical or\nmedical capabilities such as medical text summarisation.\nSinghal et al.\n33 have outlined the challenges of evaluating LLMs in various\nmedical contexts, including medical exams, research and consumer queries.\nThey have proposed a human evaluation model for the answers provided by\nd i f f e r e n tL L M st h a tc h e c k so nf a c t u a l i ty, precision, possible harm and bias.\nOther evaluation factors such as fairness, transparency, trustworthiness and\naccountability have been suggested in using LLMs in healthcare34.I nam o r e\nrecent study, Tang et al. assessed humanevaluation based on metrics such as\ncoherence, factual consistency, comprehensiveness and potential harm.\nInterestingly, they assessed the clinician’s preference for different outputs35.\nMore recently, Tam et al.36 have introduced QUEST as a framework for\nh u m a ne v a l u a t i o no fL L M si nh e a l t h c a r efollowing a comprehensive literature\nreview on the topic. QUEST includesﬁve principles for human evaluation of\nLLMs, including Quality of information, Understanding and reasoning,\nExpression style and persona, Safety and harm, and Trust and conﬁdence.\nMultiple benchmarks have been proposed to evaluate model sum-\nmarisation capabilities in the biomedical domain, including over biomedical\nliterature37–40, medical forum conversations41, and radiology reports22,42.\nHowever, these benchmarks do not capture the nuances of patient-facing\nclinical interactions, where LLM-documentation holds most promise.\nRecently, Umapathi et al.\n43 have assessed models’tendency towards\nhallucination. They reported that LLMs were signiﬁcantly variable in their\naccuracy depending on the prompts used. However, the MedHALT bench-\nmark is limited to assessing LLM’s reasoning capabilities over the medical\ndomain in a QA format. Most relevantly, Moramarco et al.21 benchmark\nBART models on the PriMock dataset andﬁnd that they produce 3.9 errors\nand 6.6 omissions on average per note. However, they did not assess the\nmodel’s impact or human errors on patient safety as part of their study.\nThis study aims to contribute to the ongoing effort to ensure clinical\nsafety in using LLMs for note generation by introducing a framework which\nhas four components: (1) a clinicallyand technically-informed error tax-\nonomy to classify LLM outputs, (2) an experiment structure to compre-\nhensively and iteratively compare outputs within our LLM document\ngeneration pipeline, (3) a clinical safety framework to assess potential harms\nFig. 1 | Our workﬂow for the assessment of LLM\noutput using CREOLA platform.This diagram\nillustrates the process we followed using the avail-\nable dataset for various experiments, including the\ninput from clinicians for labelling and consolidating\nreviews, followed by a safety analysis.\nhttps://doi.org/10.1038/s41746-025-01670-7 Article\nnpj Digital Medicine|           (2025) 8:274 2\nof errors in LLM outputs, and (4) an encompassing graphical user interface\n(GUI), CREOLA, to perform and assess all previous steps. Figure1 shows\nour workﬂow based on the framework. We present ourﬁndings and insights\nfrom applying our framework, which, to our knowledge, represents the\nlargest manual evaluation of LLM clinical note generation to date.\nO u ro b j e c t i v ei st op r o m o t et h ee fﬁcient, reliable, and conﬁdent use of\nLLMs for clinical documentation, thus supporting healthcare providers in\ndelivering high-quality care and overall reducing the administrative work-\nload for clinicians.\nResults\nDataset\nWe conducted a series of 18 experiments, each consisting of 25 primary care\nconsultation transcripts from the PriMock dataset44 For each transcript, we\nFig. 2 | Incidence of hallucinations, their types, the section of the note they appear\nin, and clinical risk.The ﬁgure illustrates the occurrence of hallucinations in the\ngenerated sentences based on different prompts (a) and their corresponding\npercentages (b). It also highlights the type and clinical severity of hallucinations (c)\nand the speciﬁc sections of the note where they appeared (d).\nFig. 3 | Number of omitted sentences based on\ndifferent prompts.The ﬁgure illustrates the num-\nber of omitted sentences using different prompts (a)\nand their respective percentage (b).\nhttps://doi.org/10.1038/s41746-025-01670-7 Article\nnpj Digital Medicine|           (2025) 8:274 3\ngenerated paired clinical documentation using an LLM, resulting in 450\nconsultation transcript-note pairs.This was a total of 49,590 transcript\nsentences and 12,999 clinical note sentences that were manually evaluated\nand labelled for any hallucinations or omissions.\nExperiments\nOur experiments were guided by our framework, which provides a sys-\ntematic approach to evaluate LLM outputs quantitatively. Using a baseline\nLLM prompt and workﬂow, we generated 25 transcript-note pairs. We\nrecruited 50 medical doctors for manual evaluation. For each paired tran-\nscript-note, we had two clinician reviewers evaluate each sentence in the\nclinical note to ensure it was evidencedin the transcript; sentences that were\nnot evidenced were labelled as hallucinations. We also highlighted each\nsentence in the transcript for review, checking if it was present in the output\nnote if it was clinically relevant; and if not, they were labelled as an omission.\nIf the hallucination or omission could change the diagnosis or management\nof the patient (if left uncorrected), it was marked as‘Major’,o t h e r w i s e ,t h e y\nw o u l db el a b e l l e da s‘minor’. In cases of discrepancy between the two\nreviewers, consolidation was performed by a senior clinician with over 20\nyears of clinical experience.\nWe additionally identiﬁed the speciﬁc sections of the notes where the\nhallucinations occur (main history, examination, discussion, symptoms\nassessment, and plan). The result of each experiment informed our sub-\nsequent experiment approach to analyse how prompt/ workﬂow/ engi-\nneering changes affect the halluci natory potential for clinical note\ngeneration.\nAll experiments were conducted on CREOLA, our in-house platform,\ndesigned to allow clinicians to identify and label relevant hallucinations and\nomissions in clinical text. Using this platform, we were able to implement\nour framework to quantify and track changes in our prompts and model\nconﬁgurations and iteratively modify our approach to ensure the safe\nintegration of LLM-generated summaries into clinical practice.\nHallucinations\nOf 12,999 sentences in 450 clinical notes, 191 sentences had hallucinations\n(1.47%), of which 84 sentences (44%) were major (could impact patient\ndiagnosis and management if left uncorrected). Of the hallucination types,\n82 (43%) were fabricated, 56 (30%) were negations, 33 (17%) were con-\ntextual, and 20 (10%) were related to causality.\nMajor hallucinations occurred in all sections, but most commonly in\nPlan (21%), Assessment (10.5%), and Symptoms (5.2%) sections. The most\ncommon hallucination type were fabrications and primarily appeared in the\nplanning section of the clinic note (Fig.2). Examples of the various hallu-\ncination types are available in supplementary materials.\nOmissions\nOf the 49,590 sentences from our consultation transcripts, 1712 sentences\nwere omitted (3.45%), of which 286 (16.7%) of which were classiﬁed as\nmajor and 1426 (83.3%) as minor. Figure3 shows the number of omissions\nand the percentage of omitted sentences based on different prompts.\nGrading hallucinations and omissions by clinical safety impact\nInspired by protocols in medical device certiﬁcations, we applied the clinical\nsafety assessment of our framework described in the methods section. We\nclassiﬁed the clinical risk (Major or Minor) and evaluated the risk severity of\nall identiﬁed major hallucinations as depicted in Fig.4. We also determined\nFig. 4 | Severity of risk in major hallucinations.We assessed the clinical risk\nresulting from major hallucinations based on our suggested framework.\nFig. 5 | Hallucination risk assessment.We have categorised the clinician severity of hallucination risks based on their type (a) and the section of the clinical notes where they\noccurred (b).\nhttps://doi.org/10.1038/s41746-025-01670-7 Article\nnpj Digital Medicine|           (2025) 8:274 4\nthe risk of hallucinations based on their type and where in the sentence they\noccurred (Fig.5).\nWe assessed the clinical risk resulting from major hallucinations based\non our suggested framework inspired by protocols in medical device\ncertiﬁcations.\nWe conducted the risk assessment on the omissions (Fig.6a) and\ndetailed the note section that would likely have been affected (Fig.6b). Major\nomissions were most common in current issues, followed by PMFS, and\nInfo and Plan sections (55%, 35%, and 10%, respectively).\nExamples of hallucinations and omissions are available in the sup-\nplementary materials.\nIterative experiments can signiﬁcantly reduce hallucination and\nomission rates in LLM-generated clinical notes\nThrough a series of 18 iterative experiments, we tested a combination of\nprompting and workﬂow strategies, including structured prompting, ato-\nmisation, function calls and JSON-based outputs, an additional LLM revi-\nsion step, and templating (SOAP -Subjective, Objective, Assessment, plan-\nnote), which are explained in more detail in the methods section.\nModifying the prompt from the baseline used in Experiment 1 to\ninclude a style update used in Experiment 8, resulted in a reduction of both\nmajor and minor omissions. Althoughthere was a slight increase in hal-\nl u c i n a t i o n si nE x p e r i m e n t8 ,these were mostly minor. Figure7 illustrates\nthe number of hallucinations and omissions recorded in Experiments 1\nand 8.\nWe then compared the outputs using various structured prompts in\nExperiments 3 and 8 illustrated in Table1.\nThe change in the prompt from Experiments 3 to 8 reduced the inci-\ndence of major hallucinations by 75% (from 4 to 1), major omissions by 58%\n(24 to 10), and minor omissions by 35% (114 to 74) (Fig.8). By following\nstructured prompting, including a style update and instructing the model to\noutput the status“unknown”for instances where information was missing\nfrom the transcript, we signiﬁcantly improved performance.\nIn a subsequent experiment (Experiment 5), we found that incorpor-\nating a chain-of-thought prompt (Table1, supplementary material), to\nextract facts from the transcript— a process referred to as atomisation—\nbefore generating the clinical note, led to an increase in major hallucinations\nand omissions. Figure9 shows the comparison between omissions and\nhallucinations between this experiment and our base Experiment (Experi-\nment 1).\nWe also compared the output of Experiment 5 to Experiment 3, which\nused structured prompts. We found that major hallucinations increased\nfrom 4 to 25, minor hallucinations from 5 to 29, major omissions from 24 to\n47, and minor omissions from 114 to 188 (Fig.10). This result precluded the\nnew change from being evaluated for clinical safety, as the increase in\nhallucinations and omissions was considered too large to be considered\nuseful.\nWe found that using prompts with function calling was useful in\nensuring that the outputs adhered to a speciﬁc structure required for dif-\nferent electronic health records. Utilising our framework, we iteratively\nFig. 7 | A comparison of omissions and halluci-\nnations between Experiment 1 and Experiment 8.\nComparing omissions (a) and hallucinations (b)\nusing the base prompt from Experiment 1 and the\nupdated style prompt from Experiment 8. Each\nconnected dot represents the change in hallucina-\ntions or omissions for a given document.\nFig. 6 | Severity of clinical risk for major omissions\nand the section of the clinical note where they\nmost occurred.This ﬁgure illustrates the clinical\nrisk severity due to major omissions (a) and indi-\ncates the sections of the notes where they are most\nlikely to occur (b).\nhttps://doi.org/10.1038/s41746-025-01670-7 Article\nnpj Digital Medicine|           (2025) 8:274 5\nimproved the performance of the structured notes across several experi-\nments (6, 9, 10, and 11). From theﬁrst to the last iteration (Experiments 6 to\n11), we made meaningful improvements to the prompts, including\ninstructions on adherence to subheadings and the addition of a writing style\nguidance (e.g., a list of writing rules to follow). Table2 shows the prompts\nused in the four function call experiments.\nAs a result of these changes, we eliminated major omissions com-\npletely, decreasing them from 61 to 0,and reduced minor omissions by 58%,\nfrom 130 to 54. Additionally, we lowered the total number of hallucinations\nb y2 5 % ,r e d u c i n gt h e mf r o m4t o3( F i g .11).\nWe then examined the two best-performing experiments with the\nfewest hallucinations and omissions (Experiments 8 and 11) for the type of\nhallucinations they produced and where they were more likely to appear in\nthe sentence (Fig.12).\nExperiment 11 did not have any major omissions, and the risk\nassessment of major omissions for Experiment 8 is shown in Fig.13.\nDiscussion\nOur study supported that hallucinations and omissions may be intrinsic\ntheoretical properties of current LLMs9. LLMs can output unfactual or\nunfaithful text with high degrees of conﬁdence45 which can be particularly\nd a n g e r o u si nah i g h - s t a k e se n v i r o n m ent such as healthcare. Our framework\nquantiﬁes the clinical impact and implications that LLM omissions and\nhallucinations may lead to if unchecked or uncorrected; only then can\nTable 1 | Prompt changes that led to decreased hallucinations and omissions\nExperiment 3 Experiment 8\nYou are a highly accurate medical ofﬁce assistant drafting\ndocumentation for a physician.\nEvery decision you take is life or death and must be 100%\naccurate. DO NOT ADD any\ncontent that isn’t speciﬁcally mentioned IN THE TRANSCRIPT.\nFrom the attached transcript\ngenerate a clinical note based on the below template format for\nthe physician to review,\ninclude all the relevant information and do not include any\ninformation that isn’t explicitly\nmentioned in the transcript. If nothing is mentioned just return\n[NOT MENTIONED].\nIt is vital that all the information in the note is as accurate as\npossible. Avoid repeating the\nsame information in different sections where possible. Write the\nnote from the perspective\nof the physician. DO NOT add associate or relate causes for\nmedical conditions unless\nexplicitly speciﬁed by the Physician. See below for a template to\noutline the structure of the\noutput and style preferences to follow.\nYou are a medical ofﬁce assistant drafting documentation for a physician. DO NOT ADD any content\nthat isn’t speciﬁcally mentioned IN THE TRANSCRIPT. From the attached transcript generate a SOAP\nnote based on the below template format for the physician to review, include all the relevant information\nand do not include any information that isn’t explicitly mentioned in the transcript. If nothing is\nmentioned just return [NOT MENTIONED].\nIt is VITAL that all the information in the note is as accurate as possible. Avoid repeating the same\ninformation in different sections where possible. Write the note from the perspective of the physician.\nOnly include any section of the template if there is information from the transcript, otherwise omit it.\nTemplate:\nReferral Reason/reason for appointment:\nHistory\n- Allergies\n- Medications\n- History of presenting complaint\n- Past Medical History\n- Family/Social History\n- Sensitive information\nObservations:\n- Examinationﬁndings\n- Investigation results\n- Impression or clinical assessment\nPlan:\n- Planned investigations\n- Follow up\n- New prescribed medication or therapies\n- Communication, reassurance & patient understanding of care\n- Actions for referrer/GP\nClarity:\n- Explained medical terms\nTemplate for Clinical SOAP Note Format:\nSubjective:\n- HPI: [include here any mentioned symptoms, chronological narrative of patients complaints,\ninformation obtained from other sources(always identify source if not the patient).]\n- Past medical history. [include here all of the patients past conditions, treatments and encounters, also\ninclude relevant social history here including smoking, alcohol, drug use and occupation/travel\nhistory]\n- Review of systems [include here any additioinal symptoms in other organs that is relevant to the initial\npresentation]\n- Current medications [list medicines out each on a seperate line, in a standard format where the\ninformation is mentioned: [DRUG NAME][DRUG DOSE][DRUG FREQUENCY][INDICATION]\nObjective:\n- Vital signs [including any mentioned blood pressure, pulse rate, oxygen saturation, temperature]\n- Physical exam [the examinationﬁndings from the physical exam, if mentioned]\n- Test Results [include in this section any lab test results or imaging reports] Assessment / Problem List:\n- Assessment: [A one sentence description of the patient and major problem as described >by the\nphysician, including the diagnosis the physician has identiﬁed]\n- Problem list: [A numerical list of clinical problems arising from this encounter and active ongoing\nmedical problems the patient has. Present each problem as [Condition][Status:active/suspected/\nconﬁrmed/past/unknown], list each problem on a separate line, leave status as unknown if not\nmentioned in the transcript]\nPlan:\n[include here any management plan mentioned in the transcript, including patient education,\nprescriptions, tests, referrals or other plans.]\nFollow-up: [include here any plan mentioned to see the patient again, or to be discharged.]\nStyle preferences:\n- Write from the perspective of the physician (ﬁrst person)\n- Be ultra concise\n- Use bullet points and broken sentences\nPlease adhere to the following style guidelines:\n- Write from the perspective of the physician (ﬁrst person)\n- Be ultra-concise\n- Be ultra-precise, do not use generalising terms\n- Be highly detailed\n- Include ALL important negations in the relevant sections (e.g. the patient has no fever) the clinician has\nelicited as well as all positiveﬁndings.\n- Use bullet points and single words, not sentences.\n- Always list medications in a list in the following format for each one: medicine, dose, frequency,\nindication\n- Always document if drug allergies are present or not\n- Examinationﬁndings always refer to a physical exam, only include signs here, not symptoms\n- Preserve quantities if mentioned in the text\nhttps://doi.org/10.1038/s41746-025-01670-7 Article\nnpj Digital Medicine|           (2025) 8:274 6\nclinical safety be meaningfully addressed. Once LLM errors are identiﬁed\nand quantiﬁed, we can make iterations on LLM prompts, workﬂows or\nengineering design to reduce or eliminate these errors. By concentrating on\nminimising errors that could signiﬁcantly impact patient care, we can align\nLLMs with clinical safety standards and regulations.\nTo our knowledge, we have conducted the largest manual evaluation\non the task of LLM clinical note generation to date. To implement our\nframework at scale we built CREOLA, an in-house platform, designed to\nenable clinicians to identify and labelrelevant hallucinations and omissions\nin clinical text and inform future experiments. However, similar publicly\navailable platforms (e.g. labelbox, explosion AI, autoblocks etc.) may be used\nf o rt h i st a s k .E x p e r i m e n t sw i t h i nC R E O L Ac a nv a l i d a t eo rd i s c r e d i ta r c h i -\ntectures and prompt approaches in a safe sandbox environment before\nclinical deployment.\nOur experiment results show thatomissions are more common than\nhallucinations (3.45% to 1.47%, respectively). However, hallucinations were\nmuch more likely to be classiﬁed as a“major”error compared to omissions\n(44% to 16.7%, respectively). This means that hallucinations are more likely\nto lead to downstream harm and impact clinical care if left uncorrected.\nHallucinations occurred in all sections, but most commonly (20%) in the\nPlan section of the clinical note. This is an importantﬁnding as this section\noften contains direct instructions or actions to colleagues or patients that can\nimpact clinical safety. The most concerning hallucinations were the nega-\ntion type (30% of total hallucinations). These mostly appeared in the\nplanning section and contradicted what was said during the consultation.\nNegation hallucinations can lead to signiﬁcant confusion and harm, and\nwithout the full context of the consultation, readers may struggle to discern\nwhich negation is true and which is false.\nBy designing prompts that addressed speciﬁc aspects of the notes (base,\ntemplate and style), we were able to focus our iterations to achieve the best\nresults. However, it is important to note that summarisation tasks require\nthe ablation of certain data from the original text to make it a concise,\nrelevant, and useful summary artefact. Optimal omission rates depend\nsigniﬁcantly on the context, the quality of input and the reviewer receiving\noutput. Our framework allows us to focus on the clinical impact of omis-\nsions and hallucinations quantitatively. Overall, our hallucination rates are\nsimilar to those reported in the literature for generalist tasks\n46.F o rt h e\nclinical summarisation task, Experiment 8 achieved 1 major hallucination\nand 10 major omissions, whilst experiment 11 achieved 2 major halluci-\nnations and 0 major omissions over 25 notes. These results are highly\nencouraging, as our iterative experiment process has resulted in fewer errors\nper note than those reported in the clinical literature. Moramarco et al.\n21\nreported 3.9 errors and 6.6 omissions per note as produced by a BART\nmodel and 1 error and 4 omissions per human-written note. This\nimprovement is likely due to the large parameter sizes of modern large\nlanguage models in combination with our framework. Although this rate is\nsubject to change depending on the text and experiment, our results suggest\nthat we can achieve state-of-the-art, sub-human clinical error rates by\ncarefully engineering and subsequently validating LLMs to produce safe\noutputs.\nOur study is limited in several ways. Firstly, the sample size of medical\ntranscripts used was relatively small; the sample size was chosen to balance\nthe trade-off of annotation volume required for the comparison of different\nexperiments against sample size and number of experiments performed.\nAdditionally, we only evaluated one LLM (GPT-4), selected due to its\nFig. 9 | A comparison of omissions and halluci-\nnations between Experiment 1 and Experiment 5.\nComparing omissions (a) and hallucinations (b)i n\nour base experiment (Experiment 1) with Experi-\nment 5 where the transcript was broken down into\nconcise facts before being passed to the LLM for note\ngeneration. Each connected dot represents the\nchange in hallucinations or omissions for a given\ndocument.\nFig. 8 | Comparison of hallucination and omission counts between two experi-\nments, assessing differences in prompt engineering effect on the quality of\noutputs. The ﬁgure shows the changes in the counts of hallucinations (a) and\nomissions (b) between Experiments 3 and 8 which have resulted from the changes in\nthe prompts.\nFig. 10 | Comparison of hallucination and omission counts between two\nexperiments, assessing the efﬁcacy of a data-extraction intermediate step versus a\nnormal note-generation step.The ﬁgure compares the number of hallucinations (a)\nand omissions (b) between Experiment 3, where the standard note generation step\nwas used, and experiment 5, where data extraction was performed before note\ngeneration.\nhttps://doi.org/10.1038/s41746-025-01670-7 Article\nnpj Digital Medicine|           (2025) 8:274 7\nTable 2 | Prompts used in experiments 6, 9, 10 and 11\nEperiment 6 Experiment 9\nPlease use the function below to generate a customised output. provide your output\nin json format.break\nFor each parameter value you provide, make sure to include all properties deﬁned in\nthe schema. If a parameter is an array please try to separate ideas into separate\nitems.\nYou are a highly accurate medical ofﬁcer drafting documentation for a physician.\nYou will receive a transcript of a medical consultation between a patient and a\nclinician. Your task is to identify the patient’s key problems in that encounter, and\nthen extract information provided to you in the format outlined in a JSON schema for\neach individual problem. A problem is a single discrete issue for the patient,\nencompassing presenting complaint, associated symptoms, and relevant history -\ne.g. shortness of breath, needs new housing, recent bereavement etc.\nIt is VITAL that you include all properties mentioned in the schema, if there is aﬁeld\nthat is not mentioned in the transcript just write [NOT MENTIONED]. Only include\ninformation strictly mentioned in the transcript. Failure to do so may cause harm to\nthe patient. DO NOT duplicate information in more than one problem.\nAim to capture all problems mentioned in the transcript into their own array items.\nPlease use the function below to generate a customised output. provide your output\nin json format.\nFor each parameter value you provide, make sure to include all properties deﬁned in\nthe schema. If a parameter is an array please try to separate ideas into separate\nitems.\nYou are a highly accurate medical ofﬁcer drafting documentation for a physician.\nYou will receive a transcript of a medical consultation between a patient and a\nclinician. Your task is to identify the patient’s key problems in that encounter, and\nthen extract information provided to you in the format outlined in a JSON schema for\neach individual problem. A problem is a single discrete issue for the patient,\nencompassing presenting complaint, associated symptoms, and relevant history -\ne.g. shortness of breath, needs new housing, recent bereavement etc.\nIt is VITAL that you include all properties mentioned in the schema, if there is aﬁeld\nthat is not mentioned in the transcript just write [NOT MENTIONED]. Only include\ninformation strictly mentioned in the transcript. Failure to do so may cause harm to\nthe patient. DO NOT duplicate information in more than one problem.\nFor each parameter you provide in the tool call, please adhere to the following style\nguidelines:\n- Write from the perspective of the physician (ﬁrst person)\n- Be ultra-concise\n- Be ultra-precise, do not use generalising terms\n- Be highly detailed\n- Include ALL important negations in the relevant sections (e.g. the patient has no\nfever) the clinician has elicited as well as all positiveﬁndings\n- Use bullet points and single words, not sentences\n- Always list medications in a list in the following format for each one: medicine,\ndose, frequency, indication\n- Always document if drug allergies are present or not\n- Examinationﬁndings always refer to a physical exam, only include signs here, not\nsymptoms\n- Preserve quantities if mentioned in the text\n- Avoid repeating the same information in different sections where possible\nExperiment 10 Experiment 11\nPlease use the function below to generate a customised output. provide your output\nin json format.\nFor each parameter value you provide, make sure to include all properties deﬁned in\nthe schema. If a parameter is an array please try to separate ideas into separate\nitems.\nYou are a highly accurate medical ofﬁcer drafting documentation for a physician.\nYou will receive a transcript of a medical consultation between a patient and a\nclinician. Your task is to identify the patients key problems in that encounter, and\nthen extract information provided to you in the format outlined in a JSON schema for\neach individual problem. A problem is a single discrete issue for the patient,\nencompassing presenting complaint, associated symptoms, and relevant history -\ne.g. shortness of breath, needs new housing, recent bereavement etc.\nIt is VITAL that you include all properties mentioned in the schema, if there is aﬁeld\nthat is not mentioned in the transcript just write [NOT MENTIONED]. Only include\ninformation strictly mentioned in the transcript. Failure to do so may cause harm to\nthe patient. DO NOT duplicate information in more than one problem.\nFor each parameter you provide in the tool call, please adhere to the following style\nguidelines:\n- Write from the perspective of the physician (ﬁrst person)\n- Be ultra-concise\n- Be ultra-precise, do not use generalising terms\n- Be highly detailed\n- Include ALL important negations in the relevant sections (e.g. the patient has no\nfever) the clinician has elicited as well as all positiveﬁndings\n- Use bullet points and single words, not sentences\n- Always list medications in a list in the following format for each one: medicine,\ndose, frequency, indication\n- Always document if drug allergies are present or not\n- Examinationﬁ\nndings always refer to a physical exam, only include signs here, not\nsymptoms\n- Preserve quantities if mentioned in the text\n- Avoid repeating the same information in different sections where possible\nPlease use the function below to generate a customised output. provide your output\nin JSON format.\nFor each parameter value you provide, make sure to include all properties deﬁned in\nthe schema. If a parameter is an array please try\nto separate ideas into separate items.\nYou are a highly accurate medical ofﬁcer drafting documentation for a physician.\nYou will receive a transcript of a medical consultation\nbetween a patient and a clinician. Your task is to identify the patients key clinical\nproblems (the presenting complaint or complaints) in\nthat encounter, and then extract information provided to you in the format outlined in\na JSON schema for each individual problem. A\nproblem is a single discrete issue for the patient, encompassing presenting\ncomplaint, associated symptoms, and relevant history -\ne.g. shortness of breath, needs new housing, recent bereavement etc.\nIt is VITAL that you include all properties mentioned in the schema, if there is aﬁeld\nthat is not mentioned in the transcript just write\n[NOT MENTIONED]. Only include information strictly mentioned in the transcript.\nFailure to do so may cause harm to the patient. DO\nNOT duplicate information in more than one problem.\nFor each parameter you provide in the tool call, please adhere to the following style\nguidelines:\n- Write from the perspective of the physician (ﬁrst person)\n- Be ultra-concise\n- Be ultra-precise, do not use generalising terms\n- Be highly detailed\n- Include ALL important negations in the relevant sections (e.g. the patient has no\nfever) the clinician has elicited as well as all positiveﬁndings.\n- Use bullet points and single words, not sentences.\n- Always list medications in a list in the following format for each one: medicine,\ndose, frequency, indication\n- Always document if drug allergies are present or not\n- Examinationﬁndings always refer to a physical exam, only include signs here, not\nsymptoms\n- Preserve quantities if mentioned in the text\n- Avoid repeating the same information in different sections where possible.\n- Ensure the output only has the headings Description of Problem, History,\nExamination and Comments - and no other headings\nhttps://doi.org/10.1038/s41746-025-01670-7 Article\nnpj Digital Medicine|           (2025) 8:274 8\nestablished performance in text summarisation at the time of our experi-\nments. The ongoing development and enhancement of open-source large\nlanguage models (LLMs) are likely to boost their application in medicine\n47.\nRecently, Zhang et al. reviewed howﬁne-tuning open-source LLMs such as\nPRIMERA, LongT5, and Llama-2 can enhance their ability to summarise\nmedical evidence effectively48. Furthermore, our experiments use a direct\nprompting scheme (Supplementary Data 1). Newer methods such as (but\nnot limited to) Retrieval-Augmented Generation (RAG)\n49,C h a i no f\nThought (CoT)50, or the use of knowledge graphs51 have recently been used\nto enhance the performance of LLMs.For example, by equipping LLMs with\ndomain-speciﬁc knowledge, RAG enables the models to generate more\nprecise and pertinent results52,53, whilst CoT generally enhances model\nreasoning abilities. A straightforward extension of this work is using this\nframework over different experimental conﬁgurations, such as using dif-\nferent models or prompting techniques, and comparing the impact on\nreported performance to clinical safety metrics.\nFinally, using human annotators toevaluate large amounts of data is\nexpensive and unsustainable. In the long run, the automated evaluation of\nmodel output54 is a consequential future direction which will enable the\nscalable assessment of a larger volume of information, with clinicians\nFig. 11 | Number of hallucinations and omissions\nin the function call experiments.This ﬁgure illus-\ntrates how the change in prompts within the func-\ntion call experiments can change the number of\nmajor and minor hallucinations and omissions in\nthe output.\nFig. 12 | Clinical risk assessment for hallucinations in experiments 8 and 11.The ﬁgures illustrate the type and severity of clinical risk resulting from major hallucinations\nin Experiments 8 (a) and 11 (c) and the section of the notes they occur (b and d).\nhttps://doi.org/10.1038/s41746-025-01670-7 Article\nnpj Digital Medicine|           (2025) 8:274 9\nremaining in the loop by“supervising”evaluator models via the inspection\nof a sub-sample of the outputs.‘LLM-as-a-Judge’, which refers to an LLM\ntasked with evaluating, scoring, or assessing the quality, correctness, or\nappropriateness of outputs, has recently been described\n55 and its applic-\nability has been discussed56. Utilising the capabilities of LLMs for initial\nscreening can signiﬁcantly reduce the time and resource demands on\nhuman evaluators. This can make the CREOLA framework more scalable\nand efﬁcient over time. This hybrid approach aligns with ongoing\nadvancements in AI and has the potential to maintain rigorous oversight\nwhile ensuring scalability.\nIn this work, we present a framework for the clinical safety assessment\no fL L M si nc l i n i c a ld o c u m e n t a t i o nscenarios. Using the CREOLA platform,\nwe analyse the impact of prompting techniques on the safety of LLM out-\nputs. Our iterative modiﬁcation process allows us to reach new low hallu-\ncination and omission rates - our best-performing experiments outperform\npreviously reported model and human error rates - facilitating conﬁdent\ndeployment of our solutions to end clinical users. Additionally, CREOLA\nprovides a sandbox environment which buffers users and patients from\nharm in case the iteration leads to higher clinical error rates. The addition of\nclinical safety assessment to prompt evaluation creates a valuable framework\nfor implementing note summarisation too l si nc l i n i c a lp r a c t i c e .W ep r o p o s e\nthat our suggested framework, which combines the assessment of halluci-\nnations and omissions with an evaluation of their impact on clinical safety,\ncan serve as a governance and clinical safety assessment template for various\norganisations. This approach aimsto empower clinicians to become key\nstakeholders in the deployment of large LLMs in clinical settings.\nMethods\nWe propose a multi-component framework to evaluate hallucinations and\nomissions in clinical documentation generated by LLMs. Central to our\napproach is the concept of“clinician in the loop”. Given their expertise,\nclinicians are uniquely positioned to identify clinical errors made by the\nmodels, making their involvement essential. A specialised annotation\nplatform (CREOLA) was developed to facilitate clinician labelling for each\nexperimental dataset.\nExperimental design: Our experiments systematically assessed how\nvarious prompting techniques and workﬂow structures inﬂuenced the\naccuracy and reliability of clinical notes derived from primary care con-\nsultation transcripts. Typical parameters varied in our experiments included\nthe complexity and speciﬁcity of prompts (such as the addition of structured\nsections, negations, or perspective changes), as well as the number of LLM\ncalls, such as introducing an additional revision step through a secondary\nLLM call. For consistency and reproducibility, we used OpenAI’sG P T - 4\n(GPT-4-32k-0613), setting the seed to 210, temperature to 0, and a top-p\nvalue of 0.95 to accommodate clinical language complexities.\nTable 3 | The likelihood of a hazard occurring\nLikelihood Category Interpretation\nVery high Certain or almost certain; highly likely to occur\nHigh Not certain but very possible: reasonably expected to\noccur in the majority of cases\nMedium Possible\nLow Could occur but in the great majority of occasions will not\nVery low Negligible or nearly negligible possibility of occurring\nTable 4 | Guidance for assessing the level of harm\nConsequence classiﬁcation Interpretation Number of patients\naffected\nCatastrophic Death Multiple\nPermanent life-changing incapacity and any condition for which the prognosis is death or permanent life-\nchanging incapacity, severe injury or severe incapacity from which recovery is not expected in the short term\nMultiple\nMajor Death Single\nPermanent life-changing incapacity and any condition for which the prognosis is death or permanent life-\nchanging incapacity, severe injury or severe incapacity from which recovery is not expected in the short term\nSingle\nSevere injury or severe incapacity from which recovery is expected in the short term Multiple\nSevere psychological trauma Multiple\nConsiderable Severe injury or severe incapacity from which recovery is expected in the short term Single\nSevere psychological trauma Single\nMinor injury or injuries from which recovery is not expected in the short term Multiple\nSigniﬁcant psychological trauma Multiple\nSigniﬁcant Minor injury or injuries from which recovery is not expected in the short term Single\nSigniﬁcant psychological trauma Single\nMinor injury from which recovery is expected in the short term Multiple\nMinor psychological upset; inconvenience Multiple\nMinor Minor injury from which recovery is expected in the short term; Minor psychological upset; inconvenience;\nany negligible severity\nSingle\nFig. 13 | Assessment of clinical risk resulting from major omissions.The ﬁgure\nillustrates the clinical risk from major omissions in Experiment 8 and the section of\nthe clinical notes where it has occurred.\nhttps://doi.org/10.1038/s41746-025-01670-7 Article\nnpj Digital Medicine|           (2025) 8:274 10\nAll prompts used are detailed in Supplementary Materials, Table4.T o\nachieve a meaningful clinical comparison of efﬁcacy and safety in a data-\ndriven way, our framework relies on the deﬁnition of a‘baseline’experiment\nagainst which to compare results. The baseline experiment must use the\nsame input data points as the new experiment. To clearly attribute an\nexperiment’s results to a speciﬁc change, we aim only to alter one parameter\nfrom the baseline experiment conﬁguration at a time.\nWe used different methodologies toassess and improve model output\nas described below:\nModel Improvements:This outlines modiﬁcations to individual LLM\nc a l l sw i t h i no u rw o r kﬂow while preserving the same overall structure.\nCommon modiﬁcations include the prompt,the model used for the call, or\nthe model hyperparameters, such as maximum output tokens (Table1).\nWorkﬂow Improvements: We implemented changes to explore new\nmethods for generating a speciﬁc type of output. For instance, in our clinical\nnote generation based on a transcript, we decided to extract a list of facts\nfrom the transcript before making a single call to the LLM for theﬁnal note\n(Supplementary Table 1). We then evaluated how this approach affected the\nfrequency of hallucinations and omissions (Figs.9 and 10). Additionally, we\nincluded an extra LLM call in some experiments to improve the quality of\nthe output (Supplementary Fig. 1).\nClinician vs LLM generated notes: Several members of our clinical\nteam were tasked with creating notes based on consultation transcripts. We\nthen utilised the framework to identifyany hallucinations and omissions in\nthese notes, which allowed us to compare the clinician-created notes with\nthose generated by the language model. Results shown in supplementary\nFig. 3, Experiment 17.\nSummary of experimental approaches in LLM-based note\nsummarisation\nOur study evaluated various approaches to prompt design and output\nstructuring for LLM-based clinical notes. The experiments were designed to\niteratively reﬁne the model’s ability to produce accurate, structured, and\nclinically relevant summaries. The key methodological approaches across\nour 18 experiments are summarised below:\nBaseline prompts (Experiments 1 and 2). These were the initial prompts\nwe had in our product prior to adopting the framework. We used these as a\nbenchmark against which the later prompts were compared.\nStructured prompts (Experiments 3, 7, 8, and 4). Prompts were\norganised into three components: base (context and goal setting), tem-\nplate (content and structure), and style (formatting). Experiment 3 was a\ncustomisation experiment to test a new prompt structure (base, template,\nstyle preferences) for generating custom notes based on a transcript.\nExperiment 7 introduced aﬁrst-person perspective in generated notes.\nExperiment 8 reﬁned the style section by incorporating negations and an\n“unknown” category for problems not explicitly mentioned in tran-\nscripts. Experiment 4 tested an enhancement to the baseline SOAP\n(Subjective, Objective, Assessment, Plan) note by improving medication\nrecord representation.\nAtomisation (Experiment 5). This method used a chain-of-thought\nprompt to extract atomic facts from transcripts to ensure the precise\norganisation of clinical details. The approach facilitated structured\nextraction, breaking down information into fundamental components.\nFunction calls & JSON-based output (Experiments 6, 9, 10, 11).\nLLMs were instructed to generate responses in structured JSON format\ninstead of free text. This structured format was optimised for integration\nwith primary care electronic health record systems. Successive experi-\nments reﬁned style handling, negation accuracy, and clinical speciﬁcity,\nprogressively reducing hallucinations.\nStructured prompt+ LLM revision step (Experiments 14 and 15).W e\nadded a second LLM pass to review and reﬁne outputs based on struc-\ntured prompting. Experiment 14 built on Experiment 11 with a revision\nstep to improve SOAP notes and introduce an“unknown” option for\nmissing details. Experiment 15 applied this process to a‘Bad SOAP’note,\ncontaining hallucinations and omissions, to evaluate how well errors\ncould be mitigated.\nNew note generation approach (Experiment 16). A novel template-\ndriven method was introduced for generating customised outputs.\nHowever, comparison with baseline results (Experiment 8) revealed an\nincrease in major hallucinations and minor omissions, highlighting\npotential trade-offs.\nClinician vs. LLM comparison (Experiment 17). In this experiment,\nclinicians manually created notes based on medical transcripts. These\nnotes were then assessed for hallucinations and omissions, providing a\nbenchmark against LLM-generated content. Interestingly,ﬁndings sug-\ngested slightly more hallucinations in clinician-written notes but fewer\nomissions, highlighting key differences in human vs. LLM-generated\nsummaries.\nExperiment 18. In this experiment, we assessed the performance of the\nnotes within the publicly available ACI Bench dataset.\nHallucination and Omission Taxonomy: We follow the conventional\nAI literature and taxonomise LLM errors into two types 1) hallucinations,\nwhich are instances of text unsupported by the associated clinical doc-\numentation, and 2) omissions\n21, which are instances where relevant details\nare missed in the supporting evidence. Furthermore, inspired by protocols\nin medical device certiﬁcations57,58, we categorise errors as either‘major’or\n‘minor’, where major errors can impact on the diagnosis or the management\nof the patient if not corrected.\nTo make our categories more granular, we propose to divide halluci-\nnations into four categories: (1) fabrication, occurring when the model\nproduced information that was not evidenced in the text, (2) negation,\noccurring when the model output negates a clinically relevant fact, (3)\ncausality, occurring when a model speculates the cause of a given condition\nw i t h o u te x p l i c i ts u p p o r tf r o mt h et e xt, and (4) contextual, occurring when\nthe model mixes topics otherwise not related to the given context.\nIn the case of omissions, we further divide them into sections: (1)\ncurrent issues, occurring when details about the current presentation were\nTable 5 | Calculating the likelihood of an error occurring in the\ntext output\nPer 25 examples Possibility\nVery High 22.5 90%\nHigh 15 60%\nMedium 7 10-60%\nLow 2.5 10%\nVery Low 0.5 1%\nFig. 14 | Risk estimation based on the likelihood and consequence of harm\noccurrence. This ﬁgure illustrates the scoring of clinical risk based on the likelihood\nof an incident occurring and the severity of harm it may cause.\nhttps://doi.org/10.1038/s41746-025-01670-7 Article\nnpj Digital Medicine|           (2025) 8:274 11\nFig. 15 | The annotation user interface is used to identify hallucinations (a) and\nomissions (b) and categorise them into major and minor categories.To facilitate\nclinician review, the closest sentence matches (highlighted in yellow) for each por-\ntion of the text under review were extracted from the counterpart document. In the\ncase of hallucinations, portions of text in the note were compared to the consultation\n(a), whereas for omissions, portions of the transcript were compared against the\nnote (b).\nhttps://doi.org/10.1038/s41746-025-01670-7 Article\nnpj Digital Medicine|           (2025) 8:274 12\nomitted, (2) PMFS (past medical history, medication history, family and\nsocial history), occurring when details about the past medical history,\nmedications including allergies, family and social history, including drink-\ning and smoking, were omitted, and 3) information and plan: when dis-\ncussions and explanations of the condition and management plans were\nomitted. Examples of each of the sub-categories are provided in the Sup-\nplementary Materials.\nExperimental Structure and Annotation Protocol: Here, we deﬁne a\nprocess to assess how model parameters affect the model outputs and\nclinical safety. To do this, we deﬁne “experiments”, which are parametrised\nby (1) the number of data points processed by the LLM, (2) the type of data\nthe LLM will ingest, (3) the model conﬁguration (type of model, random\nseed, temperature,… ), (4) the prompt used to obtain an LLM output, and (5)\nthe number of clinicians which must review the data point for clinical errors.\nG i v e na ne x p e r i m e n tc o nﬁguration, we extract model outputs from the\ninput data and store the results in a database with the associated experiment\nmetadata. We task annotators to classify whether given sub-sections of the\noutput contain hallucinations or omissions according to our taxonomy, and\nexplain in free text the reason for classiﬁcation. The annotators were\nvolunteer doctors who were paid £5 per note for annotations. Recognising\nthe subjectivity inherent in annotation, we require annotation by at least two\nclinicians for each input-output pair.This step is followed by a consolidation\nstep, i.e. a detailed review by our internal team of senior clinicians, ensuring a\nconsistent evaluation of all annotations.\nClinical Safety Assessment: Recognising that safety assessment is a\ncrucial part of using any medical technology, we designed a safety evaluation\nframework of the LLM outputs based on the framework used for evaluating\na medical device\n57,58. Overall, this assessment involves estimating the like-\nlihood of an error happening (Table3) in conjunction with the potential\nimpact of the error on the clinical outcome if it does occur. Table4 shows the\nclassiﬁcation of the level of harm, and Fig.14 presents the estimation of risk\nbased on the likelihood and consequences of an event.\nTo maintain consistency in assessingthe likelihood of hallucinations\nand omissions in each experiment, we created a percentage-based metric for\ntheir occurrence across experiments, as detailed in Supplementary data 1.\n‘Very High’likelihood represents scenarios where errors were very common\n(>90%), whereas‘Very Low’likelihood was associated with situations where\nerrors were rare (<1%). For the‘Medium’likelihood category, which covers\n10–60%, we used a broader range to accommodate the output variability\nand the understanding that some errors may be less predictable or depend\non context (Table5).\nCREOLA, Clinical Review of LLMs and AI: We combine the experi-\nment design, hallucination and omission taxonomy, and clinical safety\nevaluation in a platform we denote CREOLA, short for Clinical Review of\nLLMs and A1 (pays tribute to Creola Katherine Johnson\n59, a pioneering\nhuman computer at NASA. Just as human computers were integral to the\nsafe landing of Apollo moon missions, clinicians play a vital role in safely\nintegrating AI technologies into clinical practice).\nThe platform is used to identify resultant changes in generated clinical\ndocumentation arising from changes to processes in LLM architecture. As\nillustrated in the“experimental structure”, these changes could involve— but\nare not limited to— t h et y p eo fm o d e lu s e do rp r o m p t su s e dt oo b t a i n\noutputs. The platform was hosted as a Streamlit web application (https://\ncreola.tortus.ai/); the annotation user interface is displayed in Fig.15.\nAnnotator recruitment: As outlinedearlier, our framework requires\nannotators to review model outputs. Clinicians are uniquely skilled in cri-\ntically assessing the veracity of clinical facts in the text. Therefore, we ask\nclinicians to annotate errors for our experiments. Annotators could register\nto contribute to the annotation through the CREOLA platform. To ensure\nannotators had a good understanding ofthe process, one-to-one tuition was\ninitially provided by the study team. As the number of annotators grew, a\nshort online course was developed to explain the annotation process, fol-\nlowed by a questionnaire to ensure a comprehensive understanding of the\nmaterial. The annotators were only able to participate if they completed the\nquestionnaire correctly. The annotators could contact the study teams with\nany questions through the CREOLA platform in order to ensure any pro-\nblems in the platform were dealt with promptly.\nDate availability\nWe have used data from Primock and ACI bench which are publicly\navailable clinical transcripts andnotes (references in the main text)\nCode availability\nWe have added all our prompts used inthe supplementary materials of the\narticle. As explained in the methods section, we used OpenAI’sG P T - 4\n(GPT-4-32k-0613) as the LLMfor all our experiments.\nReceived: 19 August 2024; Accepted: 22 April 2025;\nReferences\n1. Clusmann, J. et al. The future landscape of large language models in\nmedicine. Commun. Med.3, 141 (2023).\n2. Becker, G. et al. Four minutes for a patient, twenty seconds for a\nrelative— an observational study at a university hospital.BMC Health\nServ. Res.10, 94 (2010).\n3. Asgari, E. et al. Impact of electronic health record use on cognitive\nload and burnout among clinicians: narrative review.JMIR Med. Inf.\n12, e55499 (2024).\n4. Ali, S. R., Dobbs, T. D., Hutchings, H. A. & Whitaker, I. S. Using\nChatGPT to write patient clinic letters.Lancet Digit Health5,\ne179–e181 (2023).\n5. Patel, S. B. & Lam, K. ChatGPT: the future of discharge summaries?.\nLancet Digit Health5, e107–e108 (2023).\n6. Van Veen, D. et al. Adapted large language models can outperform\nmedical experts in clinical text summarization.Nat. Med.30,\n1134–1142 (2024).\n7. Zhang, T. et al. Benchmarking large language models for news\nsummarization. Trans. Assoc. Comput Linguist12,3 9–57 (2024).\n8. Ahmad, M. A., Yaramis, I. & Roy, T. D. Creating Trustworthy LLMs:\nDealing with Hallucinations in Healthcare AI.Computer Science:\nComputation and Language.https://doi.org/10.48550/arXiv.2311.\n01463 (2023).\n9. Xu, Z., Jain, S. & Kankanhalli, M. Hallucination is Inevitable: An Innate\nLimitation of Large Language Models. Computer Science:\nComputation and Language. https://doi.org/10.48550/arXiv.2401.\n11817 (2025).\n10. Kripalani, S. et al. Deﬁcits in communication and information transfer\nbetween hospital-based and primary care physicians.JAMA 297, 831\n(2007).\n11. Adane, K., Gizachew, M. & Kendie, S. The role of medical data in\nefﬁcient patient care delivery: a review.Risk Manag Health. Policy12,\n67–73 (2019).\n12. Schiff, G. D. Diagnostic error in medicine.Arch. Intern Med.169, 1881\n(2009).\n13. Kumar, S., Balachandran, V., Njoo, L., Anastasopoulos, A. & Tsvetkov,\nY. Language generation models can cause harm: so what can we do\nabout it? An actionable survey. In:Proceedings of the 17th\nConference of the European Chapter of the Association for\nComputational Linguistics3299–3321 (Association for Computational\nLinguistics, Stroudsburg, PA, USA, 2023).https://doi.org/10.18653/\nv1/2023.eacl-main.241.\n14. Bender, E. M., Gebru, T., McMillan-Major, A. & Shmitchell, S. On the\nDangers of Stochastic Parrots. In:Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Transparency610\n–623\n(ACM, New York, NY, USA, 2021).https://doi.org/10.1145/3442188.\n3445922.\n15. Rando, J. & Tramèr, F. Universal jailbreak backdoors from poisoned\nhuman feedback.Computer science: Artiﬁcial intelligence.https://\ndoi.org/10.48550/arXiv.2311.14455 (2024).\nhttps://doi.org/10.1038/s41746-025-01670-7 Article\nnpj Digital Medicine|           (2025) 8:274 13\n16. Huang, L. et al. A Survey on Hallucination in Large Language Models:\nPrinciples, Taxonomy, Challenges, and Open Questions.Computer\nscience: Computation and language.https://doi.org/10.48550/arXiv.\n2311.05232 (2024).\n17. Rawte, V. et al. Exploring the Relationship between LLM\nHallucinations and Prompt Linguistic Nuances: Readability,\nFormality, and Concreteness.Computer Science: Artiﬁcial\nIntelligence. https://doi.org/10.48550/arXiv.2309.11064 (2023).\n18. Tonmoy, S. M. T. I. et al. A Comprehensive Survey of Hallucination\nMitigation Techniques in Large Language Models.Computer\nScience, Computation and Language.https://doi.org/10.48550/arXiv.\n2401.01313 (2024).\n19. Overhage, J. M., Qeadan, F., Choi, E. H. E., Vos, D. & Kroth, P. J.\nExplaining variability in electronic health record effort in primary care\nambulatory encounters.Appl Clin. Inf.15, 212–219 (2024).\n20. Shahbodaghi, A., Moghaddasi, H., Asadi, F. & Hosseini, A.\nDocumentation errors and deﬁciencies in medical records: a\nsystematic review.J. Health Manag26, 351–368 (2024).\n21. Moramarco, F. et al. Human Evaluation and Correlation with\nAutomatic Metrics in Consultation Note Generation. In:Proceedings\nof the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers)5739–5754 (Association for\nComputational Linguistics, Stroudsburg, PA, USA, 2022).https://doi.\norg/10.18653/v1/2022.acl-long.394.\n22. Demner-Fushman, D. et al. Preparing a collection of radiology\nexaminations for distribution and retrieval.J. Am. Med. Inform. Assoc.\n23, 304–310 (2016).\n23. Abacha, A. Ben, Yim, W., Michalopoulos, G. & Lin, T. An Investigation\nof Evaluation Metrics for Automated Medical Note Generation.\nComputer Science: Computation and Language.https://doi.org/10.\n48550/arXiv.2305.17364 (2023).\n24. Ji, Z. et al. Survey of hallucination in natural language generation.ACM\nComput. Surv.55,1 –38 (2023).\n25. Huang, Y., Tang, K., Chen, M. & Wang, B. A Comprehensive Survey on\nEvaluating Large Language Model Applications in the Medical\nIndustry. Computer Science: Computation and Language.https://doi.\norg/10.48550/arXiv.2404.15777 (2024).\n26. Tierney, A. A. et al. Ambient Artiﬁcial Intelligence Scribes to Alleviate\nthe Burden of Clinical Documentation.NEJM Catal.5 (2024).\n27. Chin-Yew Lin. ROUGE: A Package for Automatic Evaluation of\nSummaries. In Text Summarization Branches Out, 74–81, Barcelona,\nSpain. Association for Computational Linguistics.https://\naclanthology.org/W04-1013/ (2004).\n28. Papineni, K., Roukos. S., Ward. T. & Zhu. W. Bleu: A method for\nautomatic evaluation of machine translation. In:Proceedings of the\n40th Annual Meeting of the Association for Computational Linguistics,\n311–318. Philadelphia, Pennsylvania, USA.https://aclanthology.org/\nP02-1040/ (2002).\n29. Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q. & Artzi, Y.\nBERTScore: Evaluating Text Generation with BERT.\nComputer\nScience: Computation and Language.https://doi.org/10.48550/arXiv.\n1904.09675 (2020).\n30. Moreno, A. C. & Bitterman, D. S. Toward Clinical-Grade Evaluation of\nLarge Language Models.Int. J. Radiat. Oncol.*Biol.*Phys. 118,\n916–920 (2024).\n31. Minaee, S. et al. Large Language Models: A Survey.Computer\nScience: Computation and Language.https://doi.org/10.48550/arXiv.\n2402.06196 (2025).\n32. Evans, O. et al. Truthful AI: Developing and governing AI that does not\nlie. Computer Science: Computers and Society.https://doi.org/10.\n48550/arXiv.2110.06674 (2021).\n33. Singhal, K. et al. Large Language Models Encode Clinical Knowledge.\nComputer Science: Computation and Language.https://doi.org/10.\n48550/arXiv.2212.13138 (2022).\n34. Reddy, S. Evaluating large language models for use in healthcare: a\nframework for translational value assessment.Inf. Med Unlocked41,\n101304 (2023).\n35. Tang, L. et al. Evaluating large language models on medical evidence\nsummarization. NPJ Digit Med.6, 158 (2023).\n36. Tam, T. Y. C. et al. A framework for human evaluation of large language\nmodels in healthcare derived from literature review.NPJ Digit Med.7,\n258 (2024).\n37. Cohan, A. et al.A Discourse-Aware Attention Model for Abstractive\nSummarization of Long Documents(2018).\n38. Gupta, V., Bharti, P., Nokhiz, P. & Karnick, H. SumPubMed:\nsummarization dataset of pubmed scientiﬁc articles. In:Proceedings\nof the 59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on Natural\nLanguage Processing: Student Research Workshop292–303\n(Association for Computational Linguistics, Stroudsburg, PA, USA,\n2021). https://doi.org/10.18653/v1/2021.acl-srw.30.\n39. Luo, Z., Xie, Q. & Ananiadou, S. CitationSum: Citation-aware Graph\nContrastive Learning for Scientiﬁc Paper Summarization.Computer\nScience: Information Retrieval.https://doi.org/10.48550/arXiv.2301.\n11223 (2023).\n40. DeYoung, J., Beltagy, I., van Zuylen, M., Kuehl, B. & Wang, L. L. MS2:\nMulti-Document Summarization of Medical Studies.Computer\nScience: Computation and Language.https://doi.org/10.48550/arXiv.\n2104.06486 (2021).\n41. Song, Y., Tian, Y., Wang, N. & Xia, F. Summarizing Medical Conversations\nvia Identifying Important Utterances. In:Proceedings of the 28th\nInternational Conference on Computational Linguistics717–729\n(International Committee on Computational Linguistics, Stroudsburg, PA,\nUSA, 2020).https://doi.org/10.18653/v1/2020.coling-main.63.\n42. Johnson, A. E. W. et al. MIMIC-CXR, a de-identiﬁed publicly available\ndatabase of chest radiographs with free-text reports.Sci. Data6\n, 317\n(2019).\n43. Pal, A., Umapathi, L. K. & Sankarasubbu, M. Med-HALT: Medical\nDomain Hallucination Test for Large Language Models.Computer\nScience: Computation and Language.https://doi.org/10.48550/arXiv.\n2307.15343 (2023).\n44. Kor ﬁatis, A. P. and Moramarco. F. and Sarac. R. and Savkov. A.\nPriMock57: A Dataset of Primary Care Mock Consultations. https://\nGithub.Com/Babylonhealth/Primock57 (2022).\n45. Farquhar, S., Kossen, J., Kuhn, L. & Gal, Y. Detecting hallucinations in\nlarge language models using semantic entropy.Nature 630, 625–630\n(2024).\n46. Simon Hughes, M. B. Vectara Hallucination Leaderboard comparing\nLLM performance at maintaining factual consistency when\nsummarizing a set of facts.Vectara, Inc. https://huggingface.co/\nspaces/vectara/leaderboard (2023).\n47. Riedemann, L., Labonne, M. & Gilbert, S. The path forward for large\nlanguage models in medicine is open.NPJ Digit Med.7, 339 (2024).\n48. Zhang, G. et al. Closing the gap between open source and commercial\nlarge language models for medical evidence summarization.NPJ Digit\nMed. 7, 239 (2024).\n49. Lewis, P. et al. Retrieval-augmented generation for knowledge-\nintensive NLP tasks.Computer Science: Computation and Language.\nhttps://doi.org/10.48550/arXiv.2005.11401 (2021).\n50. Wei, J. et al. Chain-of-thought prompting elicits reasoning in large\nlanguage models.Computer Science: Computation and Language.\nhttps://doi.org/10.48550/arXiv.2201.11903 (2023).\n51. Jia, M., Duan, J., Song, Y. & Wang, J. medIKAL: integrating knowledge\ngraphs as assistants of LLMs for enhanced clinical diagnosis on\nEMRs. Computer Science: Computation and Language. https://doi.\norg/10.48550/arXiv.2406.14326 (2025).\n52. Wang, Y., Ma, X. & Chen, W. Augmenting black-box LLMs with\nmedical textbooks for clinical question answering.Computer\nhttps://doi.org/10.1038/s41746-025-01670-7 Article\nnpj Digital Medicine|           (2025) 8:274 14\nScience: Computation and Language.https://doi.org/10.48550/arXiv.\n2309.02233 (2024).\n53. Gilbert, S., Kather, J. N. & Hogan, A. Augmented non-hallucinating\nlarge language models as medical information curators.NPJ Digit\nMed. 7, 100 (2024).\n54. Desmond, M., Ashktorab, Z., Pan, Q., Dugan, C. & Johnson, J. M.\nEvaluLLM: LLM assisted evaluation of generative outputs. In:\nCompanion Proceedings of the 29th International Conference on\nIntelligent User Interfaces30–32 (ACM, New York, NY, USA, 2024).\nhttps://doi.org/10.1145/3640544.3645216.\n55. Zheng, L. et al. Judging LLM-as-a-Judge with MT-Bench and Chatbot\nArena. Computer Science: Computation and Language.https://doi.\norg/10.48550/arXiv.2306.05685 (2023).\n56. Gu, J. et al. A survey on LLM-as-a-Judge.Computer Science:\nComputation and Language.https://doi.org/10.48550/arXiv.2411.\n15594 (2024).\n57. International Organization for Standardization.https://Www.Iso.Org/\nStandard/59752.Html.\n58. International Organization for Standardization.https://Www.Iso.Org/\nStandard/72704.Html.\n59. https://en.wikipedia.org/wiki/Katherine_Johnson. Katherine\nJohnson.\nAcknowledgements\nThe authors would like to thank the many physicians who helped with our\nCREOLA experiments and the annotation of the clinical transcripts and\nnotes. No funding has been received for this study.\nAuthor contributions\nD.P., E.A., M.D., N.M., S.K. and J.B. contributed to the concept, design and\nexecution of the study. M.D. and S.K. built the CREOLA platform and with\nNM designed the various prompts and experiments. MD analysed all the\nresults and prepared theﬁgures. E.A. and N.M. wrote, reviewed and revised\nthe paper. E.A. and D.P. designed the clinical safety framework, reviewed all\nthe annotations and scored the impact of errors on clinical safety. JAY\nprovided expert advice on paper writing and direction and contributed to\nsigniﬁcant paper revision and rewriting. E.A., J.A.Y., M.D., N.M., S.K., J.B.\nand D.P. contributed to the review and revision of the paper.\nCompeting interests\nD.M. is the CEO of the company Tortus AI and all authors were employees of\nTortus AI at the time of the writing the paper (M.D., N.M., S.K. and J.B. as full\ntime and E.A. part time, J.A.Y. full time employee at the time of revision). The\nauthors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41746-025-01670-7\n.\nCorrespondenceand requests for materials should be addressed to\nElham Asgari.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed material. You\ndo not have permission under this licence to share adapted material\nderived from this article or parts of it. The images or other third party\nmaterial in this article are included in the article’s Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use,\nyou will need to obtain permission directly from the copyright holder. To\nview a copy of this licence, visithttp://creativecommons.org/licenses/by-\nnc-nd/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s41746-025-01670-7 Article\nnpj Digital Medicine|           (2025) 8:274 15",
  "topic": "Psychology",
  "concepts": [
    {
      "name": "Psychology",
      "score": 0.4515969157218933
    },
    {
      "name": "Psychiatry",
      "score": 0.3222353756427765
    }
  ]
}