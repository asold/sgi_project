{
  "title": "Adapting a Swin Transformer for License Plate Number and Text Detection in Drone Images",
  "url": "https://openalex.org/W4362653066",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2508147504",
      "name": "Srikanta Pal",
      "affiliations": [
        "National University of Ireland, Maynooth"
      ]
    },
    {
      "id": "https://openalex.org/A4316647652",
      "name": "Ayush Roy",
      "affiliations": [
        "Indian Statistical Institute"
      ]
    },
    {
      "id": "https://openalex.org/A191983659",
      "name": "Shivakumara Palaiahnakote",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2024204244",
      "name": "Umapada Pal",
      "affiliations": [
        "Indian Statistical Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2508147504",
      "name": "Srikanta Pal",
      "affiliations": [
        "National University of Ireland, Maynooth"
      ]
    },
    {
      "id": "https://openalex.org/A4316647652",
      "name": "Ayush Roy",
      "affiliations": [
        "Indian Statistical Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2024204244",
      "name": "Umapada Pal",
      "affiliations": [
        "Indian Statistical Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3207420819",
    "https://openalex.org/W2738072026",
    "https://openalex.org/W6803338719",
    "https://openalex.org/W4312316831",
    "https://openalex.org/W2142159465",
    "https://openalex.org/W4224262990",
    "https://openalex.org/W1998384060",
    "https://openalex.org/W6847997455",
    "https://openalex.org/W4206679962",
    "https://openalex.org/W4214922754",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4224329610",
    "https://openalex.org/W4226323731",
    "https://openalex.org/W3204678987",
    "https://openalex.org/W4313174565",
    "https://openalex.org/W3198649044",
    "https://openalex.org/W1609473077",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3157738407",
    "https://openalex.org/W3148848505",
    "https://openalex.org/W3211423998",
    "https://openalex.org/W3012003609",
    "https://openalex.org/W4212991279",
    "https://openalex.org/W3154506171",
    "https://openalex.org/W4313224863",
    "https://openalex.org/W3181016597",
    "https://openalex.org/W4312923907",
    "https://openalex.org/W3034514377",
    "https://openalex.org/W4312988011",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3212451712",
    "https://openalex.org/W4291675528",
    "https://openalex.org/W4287186597"
  ],
  "abstract": "The use of drones and unmanned aerial vehicles has significantly increased in various real-world applications such as monitoring illegal car parking, tracing vehicles, controlling traffic jams, and chasing vehicles. However, accurate detection of license plate numbers in drone images becomes complex and challenging due to variations in height distances and oblique angles during image capturing, unlike most existing methods that focus on normal images for text/license plate number detection. To address this issue, this work proposes a new model for license plate number detection in drone images using Swin transformer. The Swin transformer is chosen due to its special properties such as higher accuracy, efficiency, and fewer computations, making it suitable for license plate number/text detection in drone images. To further improve the performance of the proposed model under adverse conditions such as degradations, poor quality, and occlusion, the proposed work incorporates a maximally stable extremal region-based regional proposal network to represent text data in the images. Experimental results on both normal license plates and drone images demonstrate the superior performance of the proposed model over state-of-the-art methods. Received: 23 November 2022 | Revised: 28 March 2023 | Accepted: 4 April 2023 Conflicts of Interest Palaiahnakote Shivakumara is an Editor-in-Chief and Umapada Pal is an Advisory Board Member for Artificial Intelligence and Applications, and were not involved in the editorial review or the decision to publish this article. The authors declare that they have no conflicts of interest to this work.",
  "full_text": "Received: 23 November 2022 | Revised: 28 March 2023 | Accepted: 4 April 2023 | Published online: 6 April 2023\nRESEARCH ARTICLE\nAdapting a Swin Transformer for\nLicense Plate Number and Text\nD e t e c t i o ni nD r o n eI m a g e s\nSrikanta Pal1, Ayush Roy2 , Palaiahnakote Shivakumara3,* and Umapada Pal2\n1Maynooth International Engineering College, Maynooth University, Ireland\n2Computer Vision and Pattern Recognition Unit, Indian Statistical Institute, India\n3Faculty of Computer Science and Information Technology, University of Malaya, Malaysia\nAbstract: The use of drones and unmanned aerial vehicles has significantly increased in various real-world applications such as monitoring illegal\ncar parking, tracing vehicles, controlling traffic jams, and chasing vehicles. However, accurate detection of license plate numbers in drone images\nbecomes complex and challenging due to variations in height distances and oblique angles during image capturing, unlike most existing methods\nthat focus on normal images for text/license plate number detection. To address this issue, this work proposes a new model for license plate number\ndetection in drone images using Swin transformer. The Swin transformer is chosen due to its special properties such as higher accuracy, efficiency,\nand fewer computations, making it suitable for license plate number/text detection in drone images. To further improve the performance of the\nproposed model under adverse conditions such as degradations, poor quality, and occlusion, the proposed work incorporates a maximally stable\nextremal region-based regional proposal network to represent text data in the images. Experimental results on both normal license plates and drone\nimages demonstrate the superior performance of the proposed model over state-of-the-art methods.\nKeywords: MSER, deep learning, Swin transformer, text detection, license plate number detection\n1. Introduction\nText and license plate number detection is important for several\nreal-world surveillance applications, where text detection facilitates\ntext recognition to understand images and videos. Some examples of\nreal-world cases include automatic driving without a pilot, machine\ntranslation, human–computer interactions, etc. In these applications,\nthere are some challenges like arbitrary orientation, arbitrarily\nshaped text, low resolution, complex background, font variations,\netc. for achieving better detection results (Mittal et al., 2022;\nNadanwar et al., 2022). However, most of these challenges are\naddressed adequately by the existing methods using different deep\nlearning-based approaches. But in the case of surveillance\napplications, drones have been used for monitoring and tracking\nvehicles, traffic jams, illegal parking, toll fee collection, etc. In\nthese situations, due to variations in the heights and oblique\nangles of drone cameras, captured images suffer from severe\ndegradation, poor quality, occlusion, inadequate information, etc.\nIt is visible in Figure1(a), where partial license plate number is\nvisible, quality differs from one license plate number to another\ndue to distance variations between the camera and cars and the\neffectiveness of perspective distortion due to oblique angle.\nIn contrast to drone images in Figure1(a), normal images shown\nin Figure 1(b) do not suffer much from degradation. Since these\nchallenges are different from normal scene images, the past methods\nmay not be effective for drone images. It is evident from the results\nof the state-of-the-art methods (Liao et al.,2022; Zhang et al.,2020;\nZhu et al., 2021) and the proposed method on drone and normal\nscene images shown in Figure 1(a) and (b), respectively. The\nmethods (Liao et al.,2022; Zhang et al.,2020; Zhu et al.,2021)\nused a deep learning-based approach for addressing challenges of\nscene text detection, misses characters in the case of drone images.\nOn the other hand, the same methods works well for normal scene\ntext images. As a result, one can infer that the existing methods are\nnot effective for drone images. At the same time, the results of the\nproposed method shown in Figure 1(a) and (b) show that the\nproposed method is capable of handling both drone and normal\nscene images. Therefore, there is a need for addressing the above\nchallenges to achieve better results for drone images.\nPrevious studies have attempted to address the challenges of\ndrone images. For instance, Kim et al. (2022)p r o p o s e dam e t h o d\nfor rescuing missing people by designing a web server that receives\ndrone images and uses visual content to detect missing people.\nMohite et al. (2022) developed hyperspectral imaging techniques to\ndetect crop water stress from images captured by drones using\nvisual spectral analysis. Chowdhury et al. (2022) explored gradient\nvector flow to detect dominant points in palm tree images captured\nby drones to detect crown-shaped regions and count the number of\n*Corresponding author: Palaiahnakote Shivkumara, Faculty of Computer\nScience and Information Technology, University of Malaya, Malaysia. Email:\nshiva.um@edu.my\nArtificial Intelligence and Applications\n2023, Vol. 1(3) 129–138\nDOI: 10.47852/bonviewAIA3202549\n© The Author(s) 2023. Published by BON VIEW PUBLISHING PTE. LTD. This is an open access article under the CC BY License (https://creativecommons.org/\nlicenses/by/4.0/).\n129\npalm trees in drone images. Dwivedi et al. (2022) developed a model\nfor estimating crop area and extraction in images captured by drones\nbased on object-based image analysis instead of pixel-based image\nanalysis to monitor agriculture. However, the scope of these\nmethods is limited to general images and may not be effective for\ntext detection in drone images, including license plate numbers.\nTherefore, this work aims to develop a new method for detecting\nlicense plate numbers and text in both drone and normal images.\nThe remarkable success of deep learning discussed in the\npast (Zhang et al.,2022a, 2022b) for the classification of objects\nand complex scene images motivated us to explore transformer\narchitectures for license plate number detection in drone images.\nThis is because transformer (Liu et al.,2021) has the ability to cope\nwith the challenges posed by multiple adverse factors, and they\nperform better than conventional deep learning approaches; hence,\nwe explore addressing the challenges of drone images as well as\nnormal scene images in this work. To reduce the effect of\nnonuniform quality of license plate numbers due to the presence of\nmultiple vehicles in the same image, the proposed work adapts\nmaximally stable extremal regions (MSERs) (G´omez & Karatzas,\n2014) based regional proposal network (RPN) to detect text\ncandidates in the input images. This step helps the Swin transformer\nto perform better detection irrespective of drone and normal scene\nimages.\nThe main contributions are as follows: (i) exploring Swin\ntransformer for addressing challenges of both drone and normal\nscene images is new compared to the state-of-the-art methods and\n(ii) the use of the combination of MSER and RPN for reducing\nthe effect of background complexity and the effect of nonuniform\nquality is new compared to the existing methods.\nThis paper is structured as follows: Section 2 provides a\nsuccinct overview of related works, while Section3 delves into\nthe proposed method. The experimental findings are presented in\nSection 4, and the paper concludes with Section5.\nFigure 1\nChallenges faced during license plate number detection in drone images\n(a)\n(b)\nArtificial Intelligence and Applications Vol. 1 Iss. 3 2023\n130\n2. Literature Review\nBroadly speaking, the methods for detecting text in scene\nimages can be classified into two categories: those designed for\nscene text detection and those developed specifically for detecting\nlicense plate numbers. Therefore, we review the same in this section.\nZhu et al. (2021) employed the Fourier contour embedding\ntechnique to identify text in scene images. The approach aims to\ncreate an effective text representation that can handle diverse\ngeometric variations, which is achieved by leveraging the Fourier\ndomain instead of spatial information. However, the method may\nnot perform optimally in detecting text with arbitrary shapes.\nSimilarly, Zhang et al. (2020) proposed a text detection approach\nfor scene images using a deep relational reasoning graph (DRRG)\nnetwork that employs a graph convolutional network.\nMost text detection methods typically use segmentation as a\nstrategy to improve their performance, but their effectiveness is\nreliant on complex postprocessing procedures. To mitigate the\nimpact of these complicated postprocessing steps, Liao et al. (2022)\ndeveloped a novel approach for text detection in scene images that\nleverages differential binarization and an adaptive scale fusion\ntechnique. This method aims to overcome the limitations of\nsegmentation-based methods by integrating binarization and\nsegmentation steps to achieve more efficient text detection. Since\nthe scope of the method is limited to scene text images, the method\nmay not be extended for document layout analysis. To address this\nchallenge, Long et al. (2022) tackled this issue by creating an end-\nto-end unified model that addresses both scene text detection and\ndocument layout analysis challenges. Since scene text detection is a\ncomponent of document image analysis, it is reasonable to expect\nthat a text detection method that performs well in scenes would also\nexcel in document layout analysis. The proposed approach can\nsimultaneously detect scene text and group text into clusters.\nAlthough various methods have been proposed for text detection\nin scene images, they are often sensitive to noise and low-contrast\nimages. To address this issue, Soni et al. ( 2022) introduced a\nsupervised attention network that learns multiscale edge semantics\nand pixel-wise spatial structure information to detect text masks in\nedge-fainted noisy scene images. However, many existing methods\nprioritize accuracy over efficiency. To achieve both accuracy and\nefficiency, Wang et al. (2022) developed an end-to-end approach for\nspotting arbitrarily shaped text in scene images using kernels that\ndescribe the text shape and distinguish it from adjacent text. While\nmost methods require a large number of training samples, Dai\net al. ( 2021) proposed a scale-aware data augmentation-based\ntechnique that generates synthetic samples, reducing the dependency\non real samples for accurate scene text detection. Nonetheless, these\nmethods may not perform well on images containing deformed text.\nTo address this challenge, Ma et al. (2022) proposed a text attention\nnetwork that obtains super-resolution images, significantly improving\ntext detection performance, especially for low-contrast and spatially\ndeformed text in scene images.\nWhile most text detection methods use training and testing data\nfrom the same distribution to achieve optimal results, this is not\nalways feasible for real-world applications. To address this issue,\nZheng (2022) proposed a scene text detection method using cross-\ndomain data and a domain adaptation strategy that involves both\nlow-level and high-level alignment models for feature extraction.\nAdditionally, transformer-based methods have been introduced to\nreduce computational complexity, improve text detection\nperformance, and reduce the reliance on the number of training\nsamples. For instance, Zeng & Song (2022) developed a Swin\ntransformer with a feature pyramid network for scene text\ndetection in circuit cabinet wiring images. The proposed approach\nleverages global self-attention context at each level of feature\npyramid networks and integrates features from all levels to\neffectively detect text in the images.\nTo summarize, while the existing methods have effectively\naddressed many challenges of scene text detection, they have\nnot been specifically designed to detect text in drone images.\nDrone-captured images present unique challenges, such as occlusion,\ndistortion, degradations, nonuniform illumination, and multiple\ntext instances in the same image (such as license plate numbers\nof multiple vehicles in the same image), which may limit the\neffectiveness of the discussed methods. Furthermore, these methods\nare primarily focused on scene text images and may not be well-\nsuited for detecting license plate numbers.\nSeveral methods have been developed recently for detecting\nlicense plate numbers in different situations. For instance, Bagi\net al. (2021) proposed a method for multilingual-oriented scene\ntext and traffic sign detection in adverse meteorological\nconditions. However, this approach does not primarily focus\non license plate number detection. To improve the performance of\nlicense plate detection in adverse conditions, Lee et al. (2022)\ndeveloped an information maximization-based method that uses\nscene text detectors for detecting license plate numbers. Srilekha\net al. ( 2022) developed a method for license plate number\ndetection and nonhelmet rider identification using a combination\nof Yolov2 and an optical character recognizer. Gizatullin\net al. (2022) used an image weight model for license plate number\ndetection that involves multiple-scale wavelet transforms and\nmorphological gradient for improving performance. Kim\net al. (2021) developed a deep learning-based model for recognizing\nlicense plate numbers in CCTV images, which includes a super-\nresolution technique using a generative adversarial network.\nHowever, these methods do not address the specific challenges\nof license plate number detection in drone images. These\nchallenges include occlusion, distortion, degradations, nonuniform\nillumination, and multiple text instances in the same image.\nTo summarize, the existing methods for license plate number\ndetection have not addressed the challenge of detecting license\nplate numbers in drone images. However, Jain et al. ( 2022)\nproposed a method that adapted the Yolov5 architecture for\nnumber plate detection in drone applications, but it is not\neffective for images with cluttered backgrounds and multiple\nadverse factors. There is a need for a more robust model that can\nhandle the challenges of drone images with high accuracy and\nminimal computations. Thus, the proposed work introduces the\nSwin transformer for detecting license plate numbers and text in\nscene images.\n3. Proposed Method\nThe aim of this study is to develop a method for detecting\nlicense plate numbers and text in drone images. Unlike existing\nmethods that only focus on text detection in normal scene images,\nthe proposed method considers both license plate numbers and\ntext images for detection. However, detecting text in drone images\nis challenging due to degradations, occlusion, and distortion\ncaused by varying height distances and oblique angles. To address\nthis, the proposed method uses the Swin transformer for license\nplate number detection, inspired by its ability to extract context\nand semantic features with high accuracy and fewer computations.\nTo deal with the complex background of drone images, the\nproposed method uses a combination of MSERs and RPN to\nextract text components. The extracted features from the Swin\nArtificial Intelligence and Applications Vol. 1 Iss. 3 2023\n131\ntransformer and MSER-based RPN are fused for license plate number\ndetection. The pipeline of the proposed method can be seen in Figure2.\nIn the figure, the input image is fed to the backbone network\n(ResNet50) followed by Swin transformer layers. The feature maps\nof the backbone are then used in the Swin transformers. The\nanchor-free RPN comprises MSER-based text region detections that\nare projected on the feature map of the Swin transformer blocks.\nRegion of Interest (ROI) pooling is performed to maintain a fixed\nfeature size for input to the dense layers. The dense layers include\n64 units in dense layer 1, and 32 units each in dense layers 2 and 3,\nwhile dense layer 4 has 16 units, all activated using the RELU\nactivation function. The BBOX regressor has four units, which are\nthe four coordinates of the bounding boxes and are activated using\nthe linear activation function.\n3.1. MSER-based RPN for text component\ndetection\nAs explained in the previous section, we propose a novel\napproach that combines MSER-based RPN to identify text\ncandidate components in the images. For a given input image, the\nproposed work employs MSER, which outputs candidate\ncomponents as shown in Figure 3, whereas for the input image\nshown in (a), the MSER outputs candidate components by\ndiscarding nontext components as shown in Figure 3(b). Since\nMSER is sensitive to background components, it detects some of\nthe nontext components as candidate components. Therefore, the\nproposed work obtains Canny edge image for the input image as\nshown in Figure 3(c), where edges are representing prominent\nFigure 2\nThe block diagram of the proposed model\nFigure 3\nIllustrating the steps for text component detection\nArtificial Intelligence and Applications Vol. 1 Iss. 3 2023\n132\ninformation (edges of text). The RPN (Chen et al.,2017) is used to\nfuse the output of the Canny edge image and MSER making it anchor\nfree. It uses merged information from the Canny edge detector and\nthe MSER region proposals (the edges obtained by the Canny edge\ndetector are used as a boundary for the MSER regions and the\nbackground information is removed) (Tabassum & Dhondse,\n2015). Furthermore, to reduce the effect of false positives, the\nproposed work performs stroke width transform (SWT) (Epshtein\net al.,2010) over the results of the fused step. The SWT considers\nthe boundary pixels to estimate the stroke width, and we believe\nthat the stroke width of every character is almost similar. Based\non this observation, the proposed work fixes certain thresholds to\nremove nontext candidates. This step helps us to eliminate most\nof the nontext candidates as shown in Figure3(d), where it can be\nseen that most of the text candidates are retained and most of the\nnontext candidates are removed. The result of SWT is called text\ncomponents detection. It is noted from the results (see\nFigure 3(d)) that the output still contains nontext components.\nThis is because of variations in the foreground and background.\nNote that this step does not remove nontext components at the\ncost of text components. Therefore, the steps retain all the text\ncomponents. Since most of the background components are\nremoved, the complexity of the text detection is also reduced. The\nfive reduced text candidates having the highest IoU score are\nconsidered. The proposed regions are ROI pooled with the\nfeature map of the Swin transformer network. This leads to\nobtaining good results by the Swin transformer irrespective of\nthe challenges of drone images, which is the advantage of\nthe introduction of MSER-based RPN. Another challenge of the\ndrone dataset is the arbitrary angles as well as the varying scales\nof texts presented which creates difficulties for segmentation-\nbased methods. MSERs are immune to affine transformations and\nperform multiscale detection. This provides an edge over other\ndetection models and performs superiorly.\n3.2. Swin transformer\nIn this study, the Swin transformer is selected for license plate\nnumber and text detection in drone images, as it is well-suited for\nrepresenting data and extracting high-level features. While the\nvision transformer (Dosovitskiy et al., 2021) and data efficient\ntransformer (Touvron et al., 2021) are designed for specific\nobjectives, such as visual information and data collection,\nrespectively, the Swin transformer (Liu et al.,2021) is capable of\nglobal and local self-attention, allowing it to extract context features\nglobally and locally. This property is particularly useful for\ndifferentiating text components from nontext components, making it\nan ideal choice for the proposed work’s objectives. Figure4 shows\nthe complete architecture of the Swin transformer blocks.\nFigure 5 represents two consecutive Swin transformer blocks.\nThe input to the first block is the encoded features z (after patch\npartition) which are passed on to the layer normalization, followed\nby the weighted multihead self-attention (MSA) layer. The output\nfrom the multilayer perceptron (MLP) is fed to the next block.\nInstead of W-MSA, the shifted window MSA is used for\ncomputational efficiency. The SW-MSA procedure is shown\nwhere the map is shifted by two units for performing the attention\nmechanism. To fill up the empty space, either padding is used or\na more sophisticated approach of cyclic shift is applied and\nindicated using the green arrows.\nThe images are first divided into patches by a patch partition\nlayer (e.g., H, W, three-dimensional image is divided into H/4,\nW/4, 48). These partitioned patches are passed on a linear\nembedding layer to project it into a dimension of C (H/4,\nW/4, C). In between the stages, (between two subsequent blocks)\npatch merging is done to reduce the number of patches (H/8,\nW/8, 2C) resulting in lower dimensional concatenated features.\nSwin transformer uses a shifted window attention mechanism to\neffectively reduce the computational burden.\nΩ MSAðÞ ¼ 4hwC\n2 þ 2 hwðÞ 2C (1)\nEquation (1) represents MSA, whereh is the height of the image,w is\nthe width of the image, and C is the dimension of the embedding\nvector of the image.\nΩ WMSAðÞ ¼ 4hwC\n2 þ 2M2hwC (2)\nEquation (2) is the window attention mechanism where MSA is\napplied not on the entire image but rather on a local window of\nnonoverlapping patches (window dimension is 7 ×7). A cyclic\nshift approach is adopted which introduces connections between\nneighboring overlapping windows like the convolutional neural\nnetworks as shown in Figure 5. This cross-window MSA\nincreases the accuracy while reducing computation by eliminating\nredundant calculations. Each transformer block consists of linear\nregularization, MSA layer (number of attention heads is 8), and\ntwo-layer MLP with GELU activation function. After these\nblocks, a patch merging layer is used. The input image of\ndimension (128 × 128) is passed on to the embedding layer for\nFigure 4\nSwin transformer network architecture for license plate number and text detection in drone images\nArtificial Intelligence and Applications Vol. 1 Iss. 3 2023\n133\npositional encoding of the patches (each patch is of size 4× 4) due to\nthe transformer’s immunity to permutational changes. The addition\nof features of Blocks 2 and 3 before feeding them to Block 4\n(similarly the features of Blocks 4 and 5 are added together)\nincreases the accuracy of the model. The effect of the Swin\ntransformer is illustrated in Figure 6 for the image shown in\nFigure 3(d), where one can see that the proposed model detects\nlicense plate numbers properly for all the vehicles in the drone image.\n4. Experimental Results\nTo evaluate the proposed method on both drone images and\nnormal scene images, we collected car images from Kaggle1.T h i s\ndataset provides 432 images of cars with license plate number\nground truth in the PASCAL VOC format. For the same dataset, our\ncollected drone images are added to evaluate and validate the\neffectiveness of the proposed method for license plate number\ndetection in drone and normal scene images. In total, the dataset\ncontains 1142 images for experimentation, which include low\nresolution, degraded, good quality, poor quality, partially occluded\nlicense plate number images, and images with tiny text. Sample\nimages of our dataset are shown in Figure7(a) and (b), respectively,\nfor normal images collected from the Kaggle dataset and drone\ndatasets, where one can see the complexity of license plate number\ndetection varies from one image to another. When we look at the\nsample images of the Kaggle and our datasets shown in Figure7(a)\nand (b), the presence of multiple vehicles and background\ncomplexities is almost similar. However, the height distance varies\nmuch in the case of our dataset compared to the Kaggle dataset. To\nshow that the proposed model works well for different situations,\nsuch as good-quality and poor-quality images, our dataset includes\nimages of the Kaggle dataset for experimentation. Therefore, overall,\nwe believe that the diversified images of our dataset reflect real\nscenarios. All the images of our dataset are resized to 256× 256\ndimensions and normalized (intensity of image normalized by\ndividing it by 255) before feeding the model.\nTo demonstrate the effectiveness of our proposed method, we\nconducted a comparative study with the state-of-the-art techniques\nthat use powerful deep learning models and are robust to\nchallenges similar to drone images. These methods include the\ndifferential binarization network (Liao et al., 2022), DRRG\nFigure 6\nText detection result of the proposed method for the images in Figure3(d)\nFigure 5\nIllustration of two consecutive Swin transformer blocks and cyclic shift mechanism\n1https://www.kaggle.com/datasets/andrewmvd/car-plate-detection\nArtificial Intelligence and Applications Vol. 1 Iss. 3 2023\n134\nnetwork (Zhang et al., 2020), and Fourier contour embedding\nnetwork (Zhu et al., 2021). To ensure a fair comparison, we\nretrained these methods on our dataset and used a 70:30% split\nfor training and testing data. We maintained a consistent\nexperimental setup for all the experiments, which involved using\nan HP Laptop 15s-eq0xxx with an AMD Ryzen 5 3500U\nprocessor, 8GB RAM, and 2GB RADEON AMDA graphics card.\nTo evaluate and compare the performance of the proposed and\nexisting methods, we use the commonly used metrics of precision,\nrecall, and F1-score, which are defined in Equations ( 3), (4),\nand (5), respectively. These standard evaluation measures have also\nbeen used in the previous studies (Liao et al.,2022; Zhang et al.,\n2020; Zhu et al.,2021). We follow the same evaluation scheme as\nin these studies for calculating the metrics. By using these\nmeasures, we can assess the effectiveness of each method in terms\nof both accuracy and completeness of the license plate number\ndetection. Similar to the traditional precision, recall, and F1-score,\nthe measures are defined as follows for evaluating the performance\nof the methods. The pixels that are inside the ground truth\nbounding box are defined as true positives. The pixels that are\ninside the predicted box but are outside the ground truth bounding\nbox are defined as false positives. The pixels that are outside the\nground truth bounding box are defined as true negatives. The pixels\nthat are outside the predicted bounding box but are inside the\nground truth bounding box are defined as false negatives.\nP ¼\narea ground truthðÞ \\ area predicted boxðÞ\narea predicted boxðÞ (3)\nR ¼ area ground truthðÞ \\ area predicted boxðÞ\narea ground truthðÞ (4)\nF1 ¼ 2 /C2 P /C2 RðÞ\nP þ RðÞ (5)\n4.1. Ablation study\nThe adapted Swin transformer uses ResNet50 as backbone and\nthe combination of MSER with RPN to improve the performance of\nlicense plate number detection in drone images. To validate the\neffectiveness of the above two key steps, we conducted the\nfollowing experiments using our drone images dataset. The results\nare reported in Table 1. (i) Use the baseline RestNet101 for\nlicense plate number detection by feeding images as input. This is\nto test the effectiveness of the Swin transformer. (ii) In the same\nway, supply of input images to the ResNet50 instead of the\nRestNet101 for license plate number detection. (iii) Use of the\nResNet50 as a backbone for Swin transformer without MSER+\nRPN. This is to test the effectiveness of the MSER+ RPN. (iv)\nThe proposed method that considers the ResNet50 as a backbone\nto Swin transformer and the steps of MSER + RPN. Table 1\nshows that the baseline architecture of the ResNet50 is better than\nthe baseline architecture of the ResNet101 in terms of F-measure.\nIn this case, the precision increases for the ResNet50 while recall\ndecreases for the ResNet50 compared to the ResNet101.\nTherefore, the ResNet50 is good for reducing the number of false\npositives while the ResNet101 is good for detecting text instances\nin the images. Since the precision of the ResNet50 gained more\nFigure 7\nExamples of normal and drone license plate number images\nTable 1\nAssessing the efficacy of key steps in the proposed\nmethod for license plate detection\nExperiments Methods Precision Recall F1-score\n(i) Baseline ResNet101 42.4 33.6 37.4\n(ii) Baseline ResNet50 48.6 31.9 38.5\n(iii) Proposed method\nwithout MSER-RPN\n50.3 48.6 49.4\n(iv) Proposed method\nwith MSER-RPN\n79.8 77.9 78.9\nArtificial Intelligence and Applications Vol. 1 Iss. 3 2023\n135\nthan 6% over the ResNet101, recall of the ResNet101 gained more\nthan 2% over the recall of the ResNet50. In addition, overall, the F1-\nscore performance is better for the ResNet50 compared to the\nResNet101. Thus, one can infer that the ResNet50 is effective for\nlicense plate number detection in drone images. However, when\nwe compare the performance of baseline architectures and the\nproposed method, one can conclude that baseline architectures are\nnot capable of achieving the best results for drone images.\nThe results (iii) and (iv) reveal a significant difference in the\nperformance of the proposed method with and without MSER+\nRPN. Specifically, the proposed method without MSER+ RPN\nexhibits inferior performance in comparison to the proposed\nmethod with MSER + RPN. This highlights the importance of\nMSER + RPN in improving the accuracy of the proposed method\nfor detecting license plate numbers in drone images.\n4.2. Experiments on license plate number detection\nIn order to evaluate the efficacyof the proposed method, qualitative\nresults obtained from sample images of Kaggle and drone datasets are\ndepicted in Figure 8(a) and (b), respectively. As observed from\nFigure 8, the proposed method is ableto accurately detect license\nplate numbers in all images, evenin the presence of multiple adverse\nfactors. These results demonstrate the method’s effectiveness in\ndetecting license plate numbers in both normal and drone images.\nSimilar conclusions can be drawn from the quantitative results\np r e s e n t e di nT a b l e2, which indicate that the proposed method\nachieves the highest recall and F1-score when compared to existing\nmethods. The poor performance of existing methods can be attributed\nto their lack of suitability for drone images, as they were developed\nexclusively for text detection in scene images.\nWhen we compare the results of the existing methods (Liao et al.,\n2022; Zhang et al., 2020; Zhu et al., 2021)r e p o r t e di nT a b l e2,\nthe performance of the FCE (Zhang et al.,2020) is better than\nDRRG (Zhu et al.,2021). This is due to the model in Zhang et al. (2020)\nusing the frequency domain to represent text instances while the model\n(Zhu et al., 2021) uses the spatial domain for representing text\ninstances. It is true that the frequency domain can represent\ncomplicated shapes accurately compared to the spatial domain.\nHowever, the postprocessing steps used in Zhang et al. (2020)a n dZ h u\net al. (2021) to improve the detection performance are not robust, and\nhence, the models (Zhang et al.,2020; Zhu et al.,2021) report poor\nprecision compared to the model (Liao et al., 2022). In Liao\net al. (2022), the method uses optimized postprocessing steps to\novercome the limitations of the steps used in Zhang et al. (2020)a n dZ h u\net al. (2021). However, overall, the existing models (Liao et al.,2022;\nZhang et al.,2020; Zhu et al.,2021) report poor results compared to\nt h ep r o p o s e dm o d e li nt e r m so fr e c a l la n dF - s c o r e .\nFrom these results, it can be inferred that methods developed for\ntext detection in normal scene images may not perform well when\napplied to drone images. Conversely, the proposed method is\ncapable of effectively detecting license plate numbers in both\ntypes of images, thereby demonstrating its versatility and\napplicability across various settings. This is because of the\ncontribution of MSER-based RPN for text component detection and\nthe advantage of the Swin transformer. However, the method (Liao\net al., 2022) reports the highest precision compared to the other\nexisting method and the proposed method. This is because the\nperformance of the method depends on postprocessing unlike\nother existing methods, and it is an end-to-end model for scene\ntext detection. In the case of our method, the use of Canny edge\nFigure 8\nQualitative results of the proposed method for license plate number detection on different datasets\nTable 2\nThe performance of the proposed method and existing\ntechniques for license plate detection in both normal and drone\nimages in (%)\nMethods Precision Recall F1-score\nDBNet++ (Liao et al.,2022) 90.97 61.04 73.06\nDRRG (Zhang et al.,2020) 64.96 54.88 59.50\nFCENet (Zhu et al.,2021) 90.96 66.00 76.50\nProposed 79.86 77.99 78.91\nArtificial Intelligence and Applications Vol. 1 Iss. 3 2023\n136\nimages and MSER step sometimes detects nontext components as\ntext components. The reason is Canny and MSER are sensitive to\ncomplex backgrounds and degradations in the images, and hence,\nthe step introduces spurious edges for the complex background.\nAs a consequence, the proposed method may generate a higher\nnumber of false positives, which in turn can result in poor precision.\n4.3. Limitations\nAs mentioned earlier, our proposed method may fail when the\ninput images are hazy, have poor resolution, or are noisy in nature.\nThis is demonstrated in sample images in Figure9, where the method\nmisses characters or does not correctly identify bounding boxes. This\nmay be due to the sensitivity of the Canny edge detector used in the\nRPN to noise, resulting in inaccurate bounding box predictions.\nTherefore, there is room for improvement by replacing the step of\ntext component detection with a new deep learning model or end-\nto-end transformer, which we plan to investigate in future work.\nThe processing time for license plate number detection using\nour proposed model is 7.2 FPS, which may not be optimal.\nThis is due to the large number of parameters and computations\ninvolved in region proposal calculation. However, the\nprocessing time is affected by various factors, including system\nconfiguration, programming, and platform. Our focus in this work\nis to address the problem of drone images rather than achieving\nthe lowest processing time. In future work, we aim to develop a\nsystem that can be used in real-time environments.\n5. Conclusion and Future Work\nThe proposed method in this study combines MSER and Swin\ntransformers to detect license plate numbers in both normal and\ndrone images. The MSER and RPN are used for detecting text\ncomponents in drone images despite the challenges that come\nwith them. The Swin transformer is adapted to detect license plate\nnumbers in both drone and normal images. Experimental results\non our dataset, which includes license plate images from\nboth normal and drone scenes, demonstrate that the proposed\nmethod outperforms the state-of-the-art methods in terms of recall\nand F1-score. However, severe degradations in images can\ncause the performance of the proposed method to deteriorate.\nNonetheless, this issue falls outside the scope of this study. To\ntackle such challenges in the future, the step of text component\ndetection could be replaced with a new transformer.\nConflicts of Interest\nPalaiahnakote Shivakumara is an Editor-in-Chief and Umapada\nPal is an Advisory Board Member forArtificial Intelligence and\nApplications, and were not involved in the editorial review or the\ndecision to publish this article. The authors declare that they have\nno conflicts of interest to this work.\nReferences\nBagi, R., Dutta, T., Nigam, N., Verma, D., & Gupta, H. P. (2021).\nMet-MLTS: Leveraging smartphones for end-to-end spotting\nof multi-lingual oriented scene texts and traffic signs in\nadverse meteorological conditions. IEEE Transactions on\nIntelligent Transportation Systems, 23(8), 12801–12810.\nChen, X., Guo, Q., Li, S., & Zhang, J.(2017). Holistic vertical regional\nproposal network for scene text detection. In 2017 2nd\nInternational Conference onImage, Vision and Computing,7 2–77.\nChowdhury, P. N., Shivakumara, P., Nandanwar, L., Samiron, F.,\nPal, U., & Lu, T. (2022). Oil palm tree counting in drone\nimages. Pattern Recognition Letters, 153,1 –9.\nDai, P., Li, Y., Zhang, H., Li, J., & Cao, X. (2021). Accurate text detection\nvia scale-aware data augmentation and shape similarity constraint.\nIEEE Transactions on Multimedia, 24, 1883–1895.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai,\nX., Unterthiner, T., ..., & Houlsby, N. (2021). An image is\nworth 16× 16 words: Transformers for image recognition at\nscale. In Proceedings of International Conference on\nLearning Representations,1 –21.\nDwivedi, A. K., Singh, A. K., & Singh, D. (2022). An object-based\nimage analysis of multispectral satellite and drone images for\nprecision agriculture monitoring. In2022 IEEE International\nGeoscience and Remote Sensing Symposium, 4899–4902.\nFigure 9\nSome failure cases of the proposed method\nArtificial Intelligence and Applications Vol. 1 Iss. 3 2023\n137\nEpshtein, B., Ofek, E., & Wexler, Y. (2010). Detecting text in natural\nscenes with stroke width transform. In2010 IEEE Computer\nSociety Conference on Computer Vision and Pattern\nRecognition, 2963–2970.\nGizatullin, Z. M., Lyasheva, M. M.,Shleymovich, M. P., & Lyasheva,\nS. A. (2022). Automatic car license plate detection based on the\nimage weight model. In2022 IEEE Conference of Russian Young\nResearchers in Electrical and Electronic Engineering, 1346–1349.\nG´omez, L., & Karatzas, D. (2014). MSER-based real-time text\ndetection and tracking. In 2014 IEEE 22nd International\nConference on Pattern Recognition, 3110–3115.\nJain, S., Patel, S., Mehta, A., & Verma, J. P. (2022). Number plate\ndetection using drone surveillance. In2022 IEEE 9th Uttar\nPradesh Section International Conference on Electrical,\nElectronics and Computer Engineering,1 –6.\nKim, S. S., Jung, H. T., Lee, S. J., Park, J. H., Yu, S. H., & Go, J. H.\n(2022). A study of real-time 4K drone images visualization\nto rescue for missing people based on web. In 2022\nIEEE 13th International Conference on Information and\nCommunication Technology Convergence, 1594–1596.\nKim, T. G., Yun, B. Y., Kim, T. H., Lee, J. Y., Park, K. H., Jeong, Y.,\n& Kim, H. D. (2021). Recognition of vehicle license plate\nbased on image processing.Applied Science, 11(14), 6292.\nLee, Y., Jeon, J., Ko, Y., Jeon, M., & Pedrycz, W. (2022). License plate\ndetection via information maximization.IEEE Transactions on\nIntelligent Transportation Systems, 23(9), 14908–14921.\nLiao, M., Zou, Z., Wan, Z., Yao, C., & Bai, X. (2022). Real-time\nscene text detection with differentiable binarization and\nadaptive scale fusion. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 45(1), 919–931.\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., & Guo, B.\n(2021). Swin transformer: Hierarchical vision transformer using\nshifted windows. In2021 IEEE/CVF International Conference\non Computer Vision, 10012–10022.\nLong, S., Qin, S., Panteleev, D., Bissacco, A., Fujii, Y., & Raptis, M.\n(2022). Towards end-to-end unified scene text detection and\nlayout analysis. In 2022 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 1039–1049.\nMa, J., Liang, Z., & Zhang, L. (2022). A text attention network for\nspatial deformation robust scene text image super-resolution.\nIn 2022 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 5901–5910.\nMittal, A., Shivakumara, P., Pal, U., Lu, T., & Blumenstein, M.\n(2022). A new method for detection and prediction of\noccluded text in natural scene images. Signal Processing:\nImage Communication, 100, 116512.\nMohite, J., Sawant, S., Agrawal, R., Pandit, A., & Pappula, S. (2022).\nDetection of crop water stress in maize using drone based\nhyperspectral imaging. In 2022 IEEE International\nGeoscience and Remote Sensing Symposium, 5957–5960.\nNadanwar, L., Shivakumara, P., Ramachandra, R., Lu, T., Pal, U.,\n& Antonacopoulos, A. (2022). A new deep wavefront-based\nmodel for text localization in 3D video.IEEE Transactions\non Circuits and Systems for Video Technology , 32(6),\n3375–3389.\nSoni, A., Dutta, T., Nigam, N., Verma, D., & Gupta, H. P. (2022).\nSupervised attention network for arbitrarily-shaped text\ndetection in edge-fainted noisy scene images. IEEE\nTransactions on Computational Social Systems, 1179–1188.\nSrilekha, B., Kiran, K.V. D., & Pradyala, V. V. P. (2022). Detection\nof license plate numbers and identification of non-helmet rider\nusing Yolov2 and OCR method. In2022 IEEE International\nConference on Electronics and Renewable Systems ,\n1539–1549.\nTabassum, A., & Dhondse, S. (2015). Text Detection Using MSER and\nStroke Width Transform. In 2015 IEEE Fifth International\nConference on Communication Systems and Network\nTechnologies, 568–571.https://doi.org/10.1109/CSNT.2015.154.\nTouvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., &\nJégou, H. (2021). Training data-efficient image transformers\n& distillation through attention. InInternational Conference\non Machine Learning, 10347– 10357.\nWang, W., Xie, E., Li, X., Liu, X., Liang, D., Yang, Z.,::: , & Shen,\nC. (2022). PAN++: Towards efficient and accurate end-to-end\nspotting of arbitrarily-shaped text. IEEE Transactions on\nAnalysis, Pattern and Intelligence, Machine, 44(9), 5349–5367.\nZeng, C., & Song, C. (2022). Swin transformer with feature pyramid\nnetworks for scene text detection of secondary circuits cabinet\nwiring. In2022 IEEE 4th International Conference on Power,\nIntelligent Computing and Systems, 255–258.\nZhang, R., Xu, L., Yu, A., Shi, Y., Mu, C., & Xu, M. (2022a).\nDeep-IRTarget: An automatic target detector in infrared\nimagery using dual-domain feature extraction and allocation.\nIEEE Transactions on Multimedia, 24, 1735–1749.\nZhang, R., Yang, S., Zhang, Q., Xu, L., He, Y., & Zhang, F. (2022b).\nGraph-based few-shot learning with transformed feature\npropagation and optimal class allocation. Neurocomputing,\n470,2 4 7–256.\nZhang, S. X., Zhu, X., Hou, J. B., Liu, C., Yang, C., Wang, H., &\nYin, X. C. (2020). Deep relational reasoning graph\nnetwork for arbitrary shape text detection. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n9699– 9708.\nZheng, J. (2022). Multiple-level alignment for cross-domain scene text\ndetection. In 2022 IEEE 2nd International Conference on\nConsumer Electronics and Computer Engineering,6 7 1–175.\nZhu, Y., Chen, J., Liang, L., Kuang, Z., Jin, L., & Zhang, W. (2021).\nFourier contour embedding for arbitrary-shaped text detection.\nIn IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 3123–3131.\nHow to Cite:Pal, S., Roy, A., Shivakumara, P., & Pal, U. (2023). Adapting a Swin\nTransformer for License Plate Number and Text Detection in Drone Images.\nArtificial Intelligen ce and Applications 1(3), 129 – 138, https://doi.org/\n10.47852/bonviewAIA3202549\nArtificial Intelligence and Applications Vol. 1 Iss. 3 2023\n138",
  "topic": "Drone",
  "concepts": [
    {
      "name": "Drone",
      "score": 0.8933005332946777
    },
    {
      "name": "License",
      "score": 0.6034663915634155
    },
    {
      "name": "Transformer",
      "score": 0.5523114204406738
    },
    {
      "name": "Computer science",
      "score": 0.5144925117492676
    },
    {
      "name": "Computer vision",
      "score": 0.4611080288887024
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4366602897644043
    },
    {
      "name": "Computer security",
      "score": 0.3697059154510498
    },
    {
      "name": "Engineering",
      "score": 0.2567096948623657
    },
    {
      "name": "Electrical engineering",
      "score": 0.232585608959198
    },
    {
      "name": "Operating system",
      "score": 0.10414281487464905
    },
    {
      "name": "Biology",
      "score": 0.05285745859146118
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    }
  ]
}