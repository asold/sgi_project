{
  "title": "Non-autoregressive Transformer-based End-to-end ASR using BERT",
  "url": "https://openalex.org/W3156902660",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Yu, Fu-Hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2749272385",
      "name": "Chen Kuan Yu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3129868806",
    "https://openalex.org/W3097882114",
    "https://openalex.org/W3128340799",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2972818416",
    "https://openalex.org/W2924690340",
    "https://openalex.org/W3033017368",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W648786980",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W1828163288",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W3097625183",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W2766219058",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963242190",
    "https://openalex.org/W3095358777",
    "https://openalex.org/W2976556660",
    "https://openalex.org/W2982095018",
    "https://openalex.org/W2991509857",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2963970792",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2939111082",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W2979636403",
    "https://openalex.org/W3014413043",
    "https://openalex.org/W2970597249"
  ],
  "abstract": "Transformer-based models have led to significant innovation in classical and practical subjects as varied as speech processing, natural language processing, and computer vision. On top of the Transformer, attention-based end-to-end automatic speech recognition (ASR) models have recently become popular. Specifically, non-autoregressive modeling, which boasts fast inference and performance comparable to conventional autoregressive methods, is an emerging research topic. In the context of natural language processing, the bidirectional encoder representations from Transformers (BERT) model has received widespread attention, partially due to its ability to infer contextualized word representations and to enable superior performance for downstream tasks while needing only simple fine-tuning. Motivated by the success, we intend to view speech recognition as a downstream task of BERT, thus an ASR system is expected to be deduced by performing fine-tuning. Consequently, to not only inherit the advantages of non-autoregressive ASR models but also enjoy the benefits of a pre-trained language model (e.g., BERT), we propose a non-autoregressive Transformer-based end-to-end ASR model based on BERT. We conduct a series of experiments on the AISHELL-1 dataset that demonstrate competitive or superior results for the model when compared to state-of-the-art ASR systems.",
  "full_text": "NON-AUTOREGRESSIVE TRANSFORMER-BASED END-TO-END ASR USING BERT \nFu-Hao Yu, and Kuan-Yu Chen \nNational Taiwan University of Science and Technology, Taiwan \n \n \nABSTRACT \nTransformer-based models have led to significant innovation \nin classical and practical subjects as varied as speech \nprocessing, natural language processing, and computer vision. \nOn top of the Transformer, attention- based end- to-end \nautomatic speech recogni tion (ASR) models have recently \nbecome popular. Specifically, non- autoregressive modeling, \nwhich boasts fast inference and performance comparable to \nconventional autoregressive methods, is an emerging \nresearch topic. In the context of natural language processing, \nthe bidirectional encoder representations from Transformers \n(BERT) model has received widespread attention, partially \ndue to its ability to infer contextualized word representations \nand to enable superior performance for downstream tasks \nwhile need ing only simple fine -tuning. Motivated by the \nsuccess, we intend to view  speech recognition as a \ndownstream task of BERT, thus an ASR system  is expected \nto be  deduced by performing fine-tuning. Consequently, to \nnot only inherit the advantages of non-autoregressive ASR \nmodels but also enjoy the benefits of a pre -trained language \nmodel (e.g., BERT ), we propose a non- autoregressive \nTransformer-based end-to-end ASR model based on BERT . \nWe conduct a series of experiments on the AISHELL -1 \ndataset that  demonstrate competitive or superior results for \nthe model when compared to state-of-the-art ASR systems. \nIndex Termsâ€” Transformer, speech recognition, non-\nautoregressive, BERT \n1. INTRODUCTION \nThe central objective of an ASR system is to convert a given \nspeech signal to its corresponding sequence of words. The \nimportance of the technique stems from the fact that it is a \nkey technology for humans to communicate with machines, a \nresearch topic that has attracted interest for several decades. \nConventionally, ASR systems are usually composed of an \nacoustic model, a language model, a pronunciation lexicon, \nand a search algorithm. With the recent success of deep \nlearning, much research has been devoted  to assembling the \nvarious modules in an end- to-end manner, which not only \neliminates the need for task -specific engineering but also \nprevents error propagation. Classic representations include \nthe connectionist temporal classification (CTC)  method [1], \nthe RNN transducer  model [2], the listen, attend, and spell \n(LAS) model [3], and the hybrid CTC/attention architecture \n[4], to name a few. Specifically, because of the reliability and \ncapability of Transformer [5], more and more end- to-end \nASR models are Transformer-based.  \nThe spectrum of research on end -to-end ASR models can \nbe categorized into autoregressive (AR) and non -\nautoregressive (NAR) models. Both employ an acoustic \nencoder to extract high-level representations from the speech \nsignal; their major difference lies in the prediction philosophy \nto generate the corresponding word sequence for an input \nspeech signal. AR methods predict each token conditioned on \nall of the previously generated tokens and the acoustic -level \nstatistics distilled by the encode r. However, careful design \ncan usually yield strong recognition results, but the modeling \nconstitutes a decoding bottleneck. To mitigate the flaws of \nAR models, NAR models assume token -wise independence \nat the decoding stage to facilitate parallel implemen tations, \nusually resulting in fast inference and performance \ncomparable to that of AR methods. Hence, NAR research has \nattracted much attention recently. \nDeep learning is the focus of much recent research and \nmany experiments in various applications because of its \nremarkable performance [6 â€“8]. For the natural language \nprocessing (NLP) field, language representation methods can \nbe viewed as pioneering studie s [9, 10]. The line of research \nhas lately involved manipulating large and deep neural \nnetwork architectures typically composed of a stack of \nTransformers [11, 12â€“15, 16]. The major innovation of these \nWe have extended the paper to a journal\nï¹¢\n, and the framework is named as a pre-trained language model-based NAR ASR model, denoted as PLM-NAR-\nASR in short, to reveal the generaization. A series of experiments is likewise carried out with different settings to analyze the performance of the proposed \nframework. The source code is available at https://github.com/s920128/NAR-BERT-ASR \n \nï¹¢\nF. -H. Yu, K. - Y. Chen and K. - H. Lu, \"Non- Autoregressive ASR Modeling Using Pre -Trained Language Models for Chinese Speech Recognit ion,\" in IEEE/ACM \nTransactions on Audio, Speech, and Language Processing, vol. 30, pp. 1474- 1482, 2022, doi: 10.1109/TASLP.2022.3166400. \n \nmodels is capturing interactions between tokens via se lf-\nattention [5] and updating model parameters by optimizing \nthe masked language model objective [17, 18], which \nmodulates the constraint of the conventional left -to-right \nautoregressive objective [5]. One particularly successful \napproach is the bidirectional encoder representations from \nTransformers (BERT) model, whose success comes from a \nframework in which pre-training is followed by task-specific \nfine-tuning. Since BERT is trained  on a set of  extremely \nlarge-scale unsupervised data, it has learned general-purpose \nlanguage knowledge . In the past years, BERT and variants \nsuch as XLNet [18] and RoBERTa [19] have dominated NLP.  \nOn one hand, in view of the successes of BERT in various \nNLP-related tasks, we intend to reformulate speech \nrecognition as a downst ream task of BERT, thus an ASR \nsystem is expected to  enjoy the benefits of a pre -trained \nlanguage model and to be deduced by performing fine-tuning. \nOn the other hand, to speed up the decoding process  while \nretain competitive recognition performance, the paper strives \nto a novel non- autoregressive ASR model. Con sequently, \ngiven the potentials of pre-trained language model and NAR \nASR modeling, a  non-autoregressive Transformer -based \nASR model based on BERT (NAR-BERT-ASR) is proposed. \nBecause of the NAR modeling, the model trains and decodes \nin parallel, making inference faster than in classic \nautoregressive modeling. Also, as NAR -BERT-ASR is \ncoupled tightly with BERT, the capabilities and abilities of a \npre-trained language model can be injected deeply into t he \nASR system. As a result, the NAR- BERT-ASR achieves \ncompetitive or superior results when compared to mature \nand/or state-of-the-art ASR models. \n2. RELATED WORK \nGenerally, autoregressive (AR)  ASR models decode each \ntoken conditioned on all of the previously generated tokens \nand the high- level acoustic representations distilled by the \nencoder. For a given speech utterance ğ‘‚ğ‘‚, the objective of AR \nmodeling is to generate an output token sequence ğ‘Šğ‘Š by \nreferring to ğ‘ƒğ‘ƒ(ğ‘Šğ‘Š|ğ‘‚ğ‘‚). According to the chain rule, a common \nsimplification is to decompose ğ‘ƒğ‘ƒ(ğ‘Šğ‘Š|ğ‘‚ğ‘‚)  into a series of \nconditional probabilities:\n \nğ‘Šğ‘Šâˆ— = argmax\nğ‘Šğ‘Š\nğ‘ƒğ‘ƒ(ğ‘Šğ‘Š|O) = ğ‘ƒğ‘ƒ(ğ‘¤ğ‘¤1|ğ‘‚ğ‘‚) ï¿½ğ‘ƒğ‘ƒ(ğ‘¤ğ‘¤ğ‘–ğ‘–|ğ‘‚ğ‘‚, ğ‘Šğ‘Š<ğ‘–ğ‘–)\nğ¿ğ¿\nğ‘–ğ‘–=2\n,   (1) \nwhere ğ‘Šğ‘Š<ğ‘–ğ‘– = {ğ‘¤ğ‘¤1, â€¦ , ğ‘¤ğ‘¤ğ‘–ğ‘–âˆ’1}  denotes the partial token \nsequence before ğ‘¤ğ‘¤ğ‘–ğ‘–. Although this design accounts for both \nacoustic- and text -level information, it makes AR ASR \nsystems difficult to speed up decoding.  In addition, ground -\ntruth history tokens are fed to the decoder to predict  the next \ntoken during training, while  previous predictions from the \ndecoder should be used as a condition to predict the upcoming \ntoken during inference. Thereby, scheduled sampling [20] is \nused to relax the mismatch between training and inference. \nAlso note that the left-to-right nature of AR models limits the \nefficiency of parallel computation, making it diffic ult to \nspeed up inference. Representative models include the RNN \ntransducer model [2], the listen, attend, and spell (LAS) \nmodel [3],  the speech T ransformer [16], and the hybrid \nCTC/attention architecture model [4]. \nOrthogonal to AR models are non- autoregressive (NAR) \nASR models, which assume that each token is conditionally \nindependent of other tokens given a speech signal. That is, for \na given speech utterance ğ‘‚ğ‘‚, the NAR objective is\n \nğ‘Šğ‘Šâˆ— = argmax\nğ‘Šğ‘Š\nğ‘ƒğ‘ƒ(ğ‘Šğ‘Š|O) = ï¿½ğ‘ƒğ‘ƒ(ğ‘¤ğ‘¤ğ‘–ğ‘–|ğ‘‚ğ‘‚)\nğ¿ğ¿\nğ‘–ğ‘–=1\n.                       (2) \nWithout the dependency between the current token and \nprevious tokens, NAR models generate tokens in parallel to \nachieve a lower latency than AR models. Also, there is no \ntraining-inference mismatch in NAR models. Despite the \napparent simplicity of NAR mode ls in comparison to AR \nmodels, an increasing number of NAR -based ASR models \nhave demonstrated fast inference and competitive \nperformance compared to conventional AR methods. The \nlisten and fill in missing letter (LFML) model [12], the listen \nattentively an d spell once (LASO) model [13, 14], and the \nMask CTC method [15] are all celebrated methods. \nTransformer has been widely used in domains as varied \nas speech processing, natural language processing, and \ncomputer vision. The BERT model and its variants, all \nTransformer-based, constitute a new NLP paradigm. The \nself-supervised training objective ensures that t hese models \ncan learn from a huge amount of unlabeled text data, and are \nthus capable of learning enriched language representations \nthat are useful on a wide range of NLP tasks. To leverage the \npower of BERT for ASR, pioneer studies concentrate on \ntransferring the knowledge from BERT in a teacher -student \nmanner [14, 21] or using BERT as a strong language model \nto rescore candidate sentences [36, 37] . Empir ical results, \nhowever, suggest only limited progress. \n3. PROPOSED ASR MODEL \n3.1. Architecture \nIn this section, we introduce NAR-BERT-ASR, a novel non-\nautoregressive Transformer -based end -to-end ASR model \nusing BERT. As usual, the model consists of two components: \nan acoustic representation encoder and a text generation \ndecoder. The acoustic extract or is inspired by the speech \nTransformer [16] and LASO  [13, 14 ]; a pre -trained BERT \nmodel is placed on top of the encoder to be the decoder. \nFigure 1 depicts the architecture of the proposed ASR system. \nGiven a speech corpus containing ğ‘ğ‘ training examples  \nğ‘«ğ‘«ğ´ğ´ğ´ğ´ğ´ğ´ = âŒ©ğ‘‚ğ‘‚(ğ‘—ğ‘—), ğ‘Šğ‘Š(ğ‘—ğ‘—)âŒªğ‘—ğ‘—=1\nğ‘ğ‘ , where the speech signal ğ‘‚ğ‘‚  is \nquantized into a series of ğ‘‡ğ‘‡  acoustic feature vectors \n{ğ‘œğ‘œ1, â€¦ , ğ‘œğ‘œğ‘‡ğ‘‡}  and ğ‘Šğ‘Š  denotes a sequence of ğ¿ğ¿  tokens \n{ğ‘¤ğ‘¤1, â€¦ , ğ‘¤ğ‘¤ğ¿ğ¿}, we employ an acoustic encoder to produce high-\nlevel acoustic representations . Initially, for each speech \nsignal, two convolutional neural network (CNN) layers with \n2D filters and subsampling are introduced to capture short -\nterm temporal variations and to refine the input acoustic \nfeature frames. The stride of each CNN layer is set  to 2, \nreducing the number of acoustic representations to a quarter \nof ğ‘‡ğ‘‡. We add position embeddings to encode the absolute \nposition of each acoustic representation within an utterance, \nand stack six Transformers to model long-term dependencies \namong all the acoustic statistics. After preliminary results, we \nmade two adjustments in the Transformer implementation: \nwe use gated linear units (GLUs) [22] instead of ReLU [14]  \nas the activation function, and we use a pre -norm structure \nrather than the conventional post -norm structure to stabilize \ntraining [23]. The multi- head attention mechanism, layer \nnormalization, residual dropout, and position- wise \nfeedforward network are all unchanged  [5]. After the \nTransformer sequence, we infer a set of high -level acoustic \nfeatures ï¿½â„\n1\nğ‘ğ‘, â€¦ , â„ğ‘‡ğ‘‡ 4â„\nğ‘ğ‘ ï¿½ âˆˆ â„ğ‘‘ğ‘‘ğ‘šğ‘šÃ—(ğ‘‡ğ‘‡ 4â„ ) , where ğ‘‘ğ‘‘ğ‘šğ‘š  is a pre -\ndefined hyperparameter. \nAs the time resolution of text and speech is at different \nscales, an important step of the proposed NAR -BERT-ASR \nis automatically determining the nonlinear alignment \nbetween an acoustic feature sequence and its corresponding \ntext tokens. To make this idea work, a set of query vectors \n{ğ‘ğ‘\n1, â€¦ , ğ‘ğ‘ğ¿ğ¿â€²} âˆˆ â„ğ‘‘ğ‘‘ğ‘šğ‘šÃ—ğ¿ğ¿â€²\n is initialized using sinusoidal \nfunctions [5]. The set of query vectors â€”actually the \nconventional position embeddings â€”and the high- level \nacoustic feature vectors ï¿½â„1\nğ‘ğ‘, â€¦ , â„ğ‘‡ğ‘‡ 4â„\nğ‘ğ‘ ï¿½  are fed into a \nTransformer, where the former is used to query the latter. The \nquery vectors provide position-dependent information which \ncan be interpreted as a set of ordered anchors; hence the \nprocess is designed to roughly organize the acoustic vectors \nand to generate ğ¿ğ¿\nâ€²  position-aware features. Subsequently, \nthree Transformers, whose queries are output from the \nprevious Transformer, while the keys and values are identical \nto ï¿½â„\n1\nğ‘ğ‘, â€¦ , â„ğ‘‡ğ‘‡ 4â„\nğ‘ğ‘ ï¿½, are used to iteratively refine the statistics to \nobtain a set of position -aware acoustic embeddings \nï¿½â„1\nğ‘ğ‘, â€¦ , â„ğ¿ğ¿â€²\nğ‘ğ‘ ï¿½ âˆˆ â„ğ‘‘ğ‘‘ğ‘šğ‘šÃ—ğ¿ğ¿â€²\n. At the last stage of the acoustic \nencoder, the long- term dependencies among ï¿½â„1\nğ‘ğ‘, â€¦ , â„ğ¿ğ¿â€²\nğ‘ğ‘ ï¿½ are \nagain modeled by going through six Transformers  and a \nsimple feedforward network is then used to scale the vector \ndimension from ğ‘‘ğ‘‘ğ‘šğ‘š to ğ‘‘ğ‘‘ğ‘›ğ‘›, yielding a set of final acoustic \nembeddings ï¿½â„1\nğ‘“ğ‘“, â€¦ , â„ğ¿ğ¿â€²\nğ‘“ğ‘“ ï¿½ âˆˆ â„ğ‘‘ğ‘‘ğ‘›ğ‘›Ã—ğ¿ğ¿â€²\n. In other words, t he \nsimple projection layer , which can have varying degrees of \ncomplexity, is design to bridge the dimension mismatch \nbetween the encoder and the decoder. \nIn order to harvest  the power of BERT for ASR , we \nemploy it as the decoder . As we believe that each final \nacoustic embedding â„\nğ‘™ğ‘™\nğ‘“ğ‘“ carries specific phonetic information, \nwe regard the inferred acoustic embeddings ï¿½â„1\nğ‘“ğ‘“, â€¦ , â„ğ¿ğ¿â€²\nğ‘“ğ‘“ ï¿½ as \ntoken embeddings for BERT. Hence, a pre -trained BERT \nmodel is stacked upon the set of acoustic embeddings as a \ndecoder. For the BERT -based decoder, the input vector \nsequence is a series of composite vectors that are sums of \ntoken embeddings, i.e., ï¿½â„\n1\nğ‘“ğ‘“, â€¦ , â„ğ¿ğ¿â€²\nğ‘“ğ‘“ ï¿½, with their corresponding \nposition embeddings and segment  embeddings as in \nconventional BERT. A position- wise feedforward network \nwith softmax activation is positioned on top of the BERT \nmodel. Consequently, speech recognition is framed as \nposition-wise classification in a non -autoregressive manner. \nIt is worthwhile to note that the design of the ASR \narchitecture provides a  natural way to  deploy a pre-trained \nlanguage model for an ASR system. \n3.2. Training Recipe and Settings \nSince NAR-BERT-ASR consists of an acoustic encoder to be \ntrained from scratch and a decoder initialized by a pre-trained \nBERT model, the training process starts from pretraining the \nacoustic encoder to balance the whole system. To ensure that \nthe final aco ustic embeddings (i.e., ï¿½â„\n1\nğ‘“ğ‘“, â€¦ , â„ğ¿ğ¿â€²\nğ‘“ğ‘“ ï¿½) contain \nspecific phonetic information and can be used as token \n \nFigure 1: Architecture of proposed non-\nautoregressive Transformer-based end-to-end ASR \n \nMulti-Head \nAttention\nK QV\nDropout\nLayerNorm\nFeed \nForward \nDropout\nAttention \nBlock\nLayerNorm\nK QV\nAttention \nBlock\nK QV\nCNN\nLayerNorm\nAttention \nBlock\nLayerNorm\nK QV\nPositional\nEncoding\nAttention \nBlock\nLayerNorm\nK QV\nTransformer\nEncoder\nCNN\nFeed \nForward \nDecoder\nFeed Forward + Softmax\nBERT\nToken \nEmbedding\nSegment \nEmbedding\nPosition \nEmbedding\nembeddings for the decoder, we stack a position- wise \nfeedforward network whose parameters are initialized by the \ntoken embedding layer from BERT on top of the acoustic \nencoder. Softmax activa tion is used to create a probability \ndistribution over tokens. Except for the last layer (i.e., the \nposition-wise feedforward network), the model parameters \nğœƒğœƒ\nğ‘’ğ‘’ğ‘›ğ‘›ğ‘’ğ‘’  of the acoustic encoder are trained to minimize the \nnegative log-likelihood loss: \nâ„’ğ‘’ğ‘’ğ‘›ğ‘›ğ‘’ğ‘’ = 1\nğ‘ğ‘ğ¿ğ¿â€² ï¿½ï¿½log ğ‘ƒğ‘ƒï¿½ğ‘¤ğ‘¤ğ‘–ğ‘–\n(ğ‘—ğ‘—)ï¿½ğ‘‚ğ‘‚(ğ‘—ğ‘—), ğœƒğœƒğ‘’ğ‘’ğ‘›ğ‘›ğ‘’ğ‘’ï¿½\nğ¿ğ¿â€²\nğ‘–ğ‘–=1\nğ‘ğ‘\nğ‘—ğ‘—=1\n.                    (3) \nAfter pre-training, we discard the position- wise feedforward \nnetwork and the acoustic encoder obtains its own \ninitialization. Likewise, the entire model parameters are all \nthen fine-tuned end-to-end by minimizing the negative log-\nlikelihood loss.\n \nBecause the pre-trained BERT model is employed as the \ndecoder, we use the BERT lexicon, which is composed of \nwordpieces, for the proposed NAR -BERT-ASR. All the \ntranscriptions in the training set ar e tokenized by the BERT \ntokenizer, and the special tokens [CLS] and [SEP] are added \nat the beginning and end of each ground- truth text. Note that \nthe hyperparameter ğ¿ğ¿\nâ€² can be decided by counting the lengths \nin the training set. However, because during tra ining, ğ¿ğ¿â€² is \nalways larger or equal to the length of ground -truth text, the \n[PAD] token is added at the end of the text to match the length. \nAt the inference stage, since speech recognition is \nreformulated as position- wise classification, the output \nsequence is easily generated by concatenating the tokens with \nhighest probability at each position individually: \nğ‘Šğ‘Šâˆ— = argmax\nğ‘Šğ‘Š\nï¿½ğ‘ƒğ‘ƒ(ğ‘¤ğ‘¤ğ‘–ğ‘–|ğ‘‚ğ‘‚)\nğ¿ğ¿â€²\nğ‘–ğ‘–=1\n.                                                (4) \nAlthough one difficulty in the decoding process is the \nindependent a ssumption between tokens, several models \naccount for this by integrating an extra language model with \na beam search algorithm. It should be emphasized that one \nimportant innovation of the proposed NAR -BERT-ASR is \nthat the text- level relationships are well- modeled naturally \nowing to the BERT -based decoder, allowing for a simple \ninference process. \nIn sum, the proposed NAR-BERT-ASR not only inherits \nthe advantages of non-autoregressive ASR modeling but also \nexplicitly leverages the power  of BERT. Consequently, it \nfacilitates parallel implementation, and provides a systematic \nand theoretically sound way to inject a pre -trained language \nmodel into an ASR system. \n \n1 https://huggingface.co/bert-base-chinese \n4. EXPERIMENTS \n4.1. Experimental Setup \nThe experiments in this study were conducted on the \nAISHELL-1 dataset  [34], which contains 178 hours of \nMandarin speech. Table 1 shows some basic statistics of the \ncollection. All the experiments used 80-dimensional log Mel-\nfilter bank features with 3 -dimensional pitch features, \ncomputed with a 25ms window size and shifted every 10ms. \nFor the acoustic encoder, each CNN layer was composed of \n32 filters with a kernel size of 3, a  stride step of 2, and the \nReLU activation function. The hyperparameter ğ‘‘ğ‘‘\nğ‘šğ‘š was set to \n256, and ğ‘‘ğ‘‘ğ‘›ğ‘› was set to 768 to ensure consistency with the pre-\ntrained BERT model . For the Transformer, each multi -head \nattention had 8 heads, and the hidden size of the feedforward \nneural network was set to 2048. Although the average length \nof the training transcriptions was 14.4, in this study ğ¿ğ¿\nâ€² was set \nto 60. For the decoder, we directly used bert -base-chinese1, \nwith a vocabulary size of 21128, from Huggingfaceâ€™s \nTransformers library  [24]. The NAR-BERT-ASR model  is \ntrained on an NVIDIA Tesla V100 GPU and is implemented \nwith PyTorch [39] and ESPnet toolkit  [40]. The pre-trained \nBERT model consisted of 12 Transformers. For training, the \nnumber of training epochs for both the acoustic encoder pre -\ntraining and the whole -model fine -tuning was 130. Each \ntraining batch contained 100 seconds of speech data with \ntheir corresponding text utt erances, and the gradient was \naccumulated 12 times for each update. For the optimizer we \nused Adam, and to determine the learning rate we used the \nNoam scheduler, with 12000 warm-up steps. The dropout rate \nwas set to 0.1 to avoid overfitting, and the label  smoothing \nfactor was set to 0.1. The SpecAugment technique [25]  was \nused for data augmentation. We averaged the parameters of \nthe last 10 epochs as our final model. \n \nTable 1: Statistics of the AISHELL-1 corpus \n  Training \nSet \nDev. \nSet \nTest \nSet \n#Utterances 120,098 14,326 7,176 \n#Hours 150 18 10 \n#Speakers 340 40 20 \nDuration (Sec.) \nMin. 1.2 1.6 1.9 \nMax. 14.5 12.5 14.7 \nAvg. 4.5 4.5 5.0 \n#Tokens/Sentence \nMin. 1.0 3.0 3.0 \nMax. 44.0 35.0 21.0 \nAvg. 14.4 14.3 14.0 \n \n \n4.2. Experimental Results \nWe classify current ASR systems into traditional hybrid \nsystems, autoregressive (AR) models, and non-\nautoregressive (NAR) models. At first glance, as shown in \nTable 2, most AR and NAR models outperform traditional \nhybrid models. Comparing the AR models wit h the NAR \nmethods, we observe that NAR methods have caught up with \nclassic AR models in terms of efficiency and effectiveness, \nwhich demonstrates the potential of NAR modeling.  \nIn the second set of experiments, we compared the \nproposed NAR-BERT-ASR with state-of-the-art systems, as \nsummarized in Table 2. We offer several observations. First, \nNAR-BERT-ASR outperforms all of the systems discussed \nin this study, which suggests the feasibility of the proposed \nASR model, which integrates a pre-trained BERT model with \na carefully designed acoustic encoder. Second , obviously, \nNAR models usually do not incorporate a language model in \ndecoding, because using a language model no longer makes \nthe model end -to-end and almost practical language models \nare autoregressive. Orthogonally, language model is almost a \nstandard component of AR models, since  empirical results \nindicate that language regularities and constrains are \nimportant clues for ASR. However, the tradeoff of the \nreduced recognition error rate is an increase in computational \noverhead. If we all agree upon that  language model can \nprovide useful information for ASR, NAR -BERT-ASR \n \n2 https://storage.googleapis.com/bert_models/ \n2018_11_03/chinese_L-12_H-768_A-12.zip \ncreates a potential way to install a language model in an end-\nto-end non-autoregressive ASR system. Consequently, NAR-\nBERT-ASR not only can deliver fast inference,  but also \nachieves superior results than some mature and/or state -of-\nthe-art ASR models.  \nThird, note that LASO can be seen as an important baseline, \nbecause it was the inspiration for the acoustic encoder of \nNAR-BERT-ASR. For improved performance, LASO uses \nthe teacher -student learning [14, 35] to transfer knowledge \nfrom BERT 2 ; in Table 2  this is denoted as â€œLASO with \nBERTâ€. From the results, LASO with BERT can give about \n11% relative improvement than LASO. When compared with \nthe proposed ASR model , NAR-BERT-ASR achieves over \n26% and 16% relative improvements than LASO and LASO \nwith BERT , respectively, on the test set.  The experiments \nreveal that the pre- trained language model (i.e., BERT) can \nindeed promote the recognition results, and the performance \ngap between LASO with BERT and NAR -BERT-ASR is \nbecause the latter is designed to tightly couple with BERT. In \nother words, during inference, NAR -BERT-ASR still \nexplicitly and directly  leverages the benefits from a pre -\ntrained language model , while LASO with BERT only \nimplicitly uses language knowledge transferred from BERT. \nBesides, LASO with BERT need s to bear additional \ncomputation costs to compute auxiliary ground-truths and \nTable 2: Character error rate (CER) on AISHELL-1 for NAR-BERT-ASR vs. state-of-the-art approaches. Real-\ntime factor (RTF) is computed as the ratio of the total inference time to the total duration of the test set.  \nModel Dev. Test LM Params RTF \nTraditional hybrid ASR      \nKaldi nnet3 - 8.6 ïƒ¼   \nKaldi chain - 7.5 ïƒ¼   \nAutoregressive ASR      \nBERT-ASR [32] 54.6 58.8    \nLAS [26] - 8.7 ïƒ¼ 156M  \nSA-T [28] 8.3 9.3 ïƒ¼   \nESPnet (Transformer) 6.0 6.7 ïƒ¼ 58M 0.4200 \nUnsupervised pre-training [30] - 6.7    \nSAN-M [29] 5.7 6.5    \nCAT [31] - 6.3 ïƒ¼   \nNon-autoregressive ASR      \nCTC [33] 7.8 8.7    \nLFML [12] 6.2 6.7 ïƒ¼   \nLASO [14] 5.9 6.9  105.8M  \nLASO (our implementation) 5.9 6.7   0.0033 \nNAR-Transformer [33] 5.6 6.3    \nLASO with BERT [14] 5.3 6.1  105.8M  \nNAR-BERT-ASR 4.6 5.1  119.5M 0.0057 \n \nlosses during training [14]. In a nutshell, NAR-BERT-ASR is \na preferable vehicle for utilizing BERT in NAR ASR system. \nFourth, although BERT -ASR, an ASR system obtained \npurely by finetuning a pre-trained BERT model3, achieves a \nmediocre result and remains several challenges [32], it is the \ncornerstone of the proposed NAR -BERT-ASR model. \nTaking this idea further, we leverage a more sophisticated \nacoustic encoder to distill high-level acoustic representations \nand also introduce an attention mechanism to eliminate the \nneed for alignment information. Finally, in terms of the \ndecoding speed measured in RTF, NAR -BERT-ASR is at  \nleast 70 times faster than the classical autoregressive model \n(i.e., ESPnet (Transformer)). LASO achieves slightly faster \ninference speed than NAR -BERT-ASR, while the latter can \ndeliver remarkable improvements than LASO  and LASO \nwith BERT. Major reason for the difference  of RTFs is the \nmodel architecture, where LASO ha s 13 Transformers and \nNAR-BERT-ASR consists of 28 Transformers. Hence, NAR-\nBERT-ASR suffers from the computational  barrier made by \nthe self-attention mechanism, which is a major component of \nTransformer. One of the emerg ent challenges is to speed up \nthe inference process, and we leave as our future work. Based \non the experiments, NAR-BERT-ASR, the end result, is thus \na systematic and theoretically sound way to inject a pre -\ntrained language model into a non-autoregressive ASR \nsystem. \n4.3. Ablation Studies and Discussions \nIn Table 3 , we further analyze different configurations of \nNAR-BERT-ASR. First, the performance of NAR -BERT-\nASR is boosted further by increasing the number of training \nepochs during whole -model fine -tuning, although the \nimprovement is not significant. Second, we seek to better \nunderstand the effect pre -training has on performance for \nboth the acoustic encoder and decoder. The results show a \nlarge performance degradation if the whole model â€”both \nencoder and decoder â€”is trained from scratch. Furthermore, \nwe found no good way to build a successful model by \nrandomly initializing the acoustic encoder while copying the \nmodel parameters of the decoder from a pre- trained BERT \nmodel (â€œWithout acoustic encoder pre -trainingâ€). One \npossible explanation for the two results  is large diff erence \nbetween the time resolution of text and that of speech. This is \naddressed with a carefully designed acoustic encoder, which \nis thus more difficult to train than the decoder; to put it simply, \nthe acoustic encoder pre- training is an important step for \ntraining. Orthogonally, respectable performance is achieved \nwhen only the decoder is initialized randomly (â€œWithout \ndecoder pre-trainingâ€). In addition, an interesting observation \nis the results  of NAR -BERT-ASR without decoder pre -\ntraining are almost equ ivalent to LASO (cf. Table 2). A \n \n3 https://github.com/ymcui/Chinese-BERT-wwm \npossible reason may be the fact that they have similar model \narchitectures, and the model parameters are also close. \nTherefore, without the pre -trained BERT model,  NAR-\nBERT-ASR can only achieve comparable results to LASO.  \nBesides, w hen compared the without  decoder pre -training \nwith the original setting, we can conclude that the pre-trained \nBERT model brings 21% relative improvements for the \nproposed NAR -BERT-ASR. It is worthy to note again that \nNAR-BERT-ASR makes an effectiv e and  efficient way to \nleverage the power of BERT for ASR, thus the experimental \nresults can show attractive improvements than practical state-\nof-the-art ASR models. \n5. CONCLUSION \nWe propose NAR -BERT-ASR, a novel Transformer -based  \nnon-autoregressive end-to-end ASR model based on BERT, \nwhich not only enjoys the benefits of a pre -trained language \nmodel but also inherits the strengths of a NAR model. In the \nexperiments, the proposed model achieves promising results \ncompared to several state- of-the-art methods. In the future, \nwe will continue improving the architecture and exploring \ndifferent training objectives fo r NAR-BERT-ASR. We also \nplan to replace BERT with other pre-trained language models \n(e.g., RoBERTa, XLNet and Albert) , and evaluate the \nproposed ASR model on other benchmark corpora. \n6. ACKNOWLEDGMENTS \nThis work was supported by the Ministry of \nScience and  Technology of Taiwan under Grant \nMOST 110- 2636-E-011-003 (Young Scholar \nFellowship Program). We thank National Center \nfor High -performance Computing (NCHC) for \nproviding computational and storage resources. \n \n \nTable 3: Character error rate (CER) on \nAISHELL-1 for different configurations of NAR-\nBERT-ASR \nConfiguration Dev. Test \nOriginal settings 4.6 5.1 \nFinetuning: 140 epochs 4.5 5.0 \nWithout acoustic encoder pre-training n/a \nWithout decoder pre-training 5.9 6.5 \nEntire model trained from scratch 15.4 18.7 \n \n \n7. REFERENCES \n[1] A. Graves et al. , â€œConnectionist temporal classification: \nlabelling unsegmented sequence data with recurrent neural \nnetworks,â€ in Proceedings of the 23rd international conference \non Machine learning, 2006, pp. 369-376. \n[2] A. Graves, â€œSequence transduction with recurrent neural \nnetworks,â€ arXiv preprint arXiv:1211.3711, 2012. \n[3] W. Chan et al., â€œListen, attend and spell: A neural network for \nlarge vocabulary conversational speech recognition ,â€ in 2016 \nIEEE International Conference on Acoustics, Speech and \nSignal Processing (ICASSP), 2016: IEEE, pp. 4960-4964.  \n[4] S. Watanabe et al., â€œHybrid CTC/attention architecture for end-\nto-end speech recognition,â€ IEEE Journal of Selected Topics in \nSignal Processing, vol. 11, no. 8, pp. 1240-1253, 2017.  \n[5] A. N. Gomez et al., â€œAttention is all you need,â€ in Proceedings \nof Advances in Neural Information Processing Systems  \n(NeurIPS), 2017, pp. 5998-6008. \n[6] Y. Liu, â€œFine-tune BERT for extractive summarization,â€ arXiv \npreprint arXiv:1903.10318, 2019. \n[7] V. Karpukhin et al., â€œDense passage retrieval for open-domain \nquestion answering,â€ in Proceedings of the 2020 Conference \non Empirical Methods in Natural Language Processing \n(EMNLP), 2020, pp. 6769-6781. \n[8] L. H. Li et al., â€œVisualbert: A simple and performant baseline \nfor visio n and language,â€ arXiv preprint arXiv:1908.03557, \n2019. \n[9] T. Mikolov et al., â€œEfficient estimation of word representations \nin vector space,â€ in Proceedings of International Conference \non Learning Representations (ICLR), 2013. \n[10] J. Pennington et al. , â€œGlove: Global vectors for word \nrepresentation,â€ in Proceedings of the 2014 conference on \nempirical methods in natural language processing (EMNLP) , \n2014, pp. 1532-1543.  \n[11] J. Gu et al., â€œNon-autoregressive neural machine translation ,â€ \nin Proceedings of International Conference on Learning \nRepresentations (ICLR), 2018. \n[12] N. Chen  et al. , â€œListen and fill in the missing letters: Non -\nautoregressive transformer for speech recognition ,â€ arXiv \npreprint arXiv:1911.04908, 2019. \n[13] Y. Bai  et al. , â€œListen attentively, and spe ll once: Whole \nsentence generation via a non -autoregressive architecture for \nlow-latency speech recognition ,â€ in Proceedings of \nINTERSPEECH, 2020, pp. 3381-3385. \n[14] Y. Bai  et al. , â€œFast end -to-end speech recognition via non-\nautoregressive models and cross-modal knowledge transferring \nfrom BERT,â€ arXiv preprint arXiv:2102.07594, 2021. \n[15] Y. Higuchi et al., â€œMask CTC: Non-autoregressive end-to-end \nASR with CTC and mask predict ,â€ in Proceedings of \nINTERSPEECH, 2020, pp. 3655-3659. \n[16] L. Dong et al., â€œSpeech-transformer: a no-recurrence sequence-\nto-sequence model for speech recognition,â€ in 2018 IEEE \nInternational Conference on Acoustics, Speech and Signal \nProcessing (ICASSP), 2018: IEEE, pp. 5884-5888.  \n[17] J. Devlin et al. , â€œBERT: Pre -training of deep bidirectional \ntransformers for language understanding,â€ in Proceedings of \nthe 2019 Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Human Language \nTechnologies(NAACL-HLT), 2019, pp. 4171â€“4186. \n[18] Z. Yang et al., â€œXLNet: Generalized autoregressive pretraining \nfor language understanding,â€ in Proceedings of Advances in \nNeural Information Processing Systems (NeurIPS), 2019, pp. \n5754-5764. \n[19] Y. Liu et al., â€œRoBERTa: A robustly optimized bert pretraining \napproach,â€ arXiv preprint arXiv:1907.11692, 2019. \n[20] S. Bengio et al., â€œScheduled sampling for sequence prediction \nwith recurrent neural networks,â€ in Proceedings of Advances in \nNeural Information Processing Systems (NeurIPS), 2015, pp. \n1171â€“1179. \n[21] H. Futami  et al. , â€œDis tilling the knowledge of BERT for \nsequence-to-sequence ASR ,â€ in Proceedings of \nINTERSPEECH, 2020, pp. 3635-3639. \n[22] Y. N. Dauphin  et al. , â€œLanguage modeling with gated \nconvolutional networks,â€ in International conference on \nmachine learning, 2017: PMLR, pp. 933-941.  \n[23] T. Q. Nguyen and J. Salazar, â€œ Transformers without tears: \nImproving the normalization of self -attention,â€ arXiv preprint \narXiv:1910.05895, 2019. \n[24] T. Wolf et al., â€œTransformers: State-of-the-art natural language \nprocessing,â€ in Proceedings of the 2020 Conference on \nEmpirical Methods in Natural Language Processing: System \nDemonstrations, 2020, pp. 38-45.  \n[25] D. S. Park  et al., â€œSpecaugment: A simple data augmentation \nmethod for automatic speech recognition,â€ in  Proceedings of \nINTERSPEECH, 2019, pp. 2613-2617. \n[26] C. Shan  et al. , â€œComponent fusion: Learning replaceable \nlanguage model component for end-to -end speech recognition \nsystem,â€ in ICASSP 2019-2019 IEEE International Conference \non Acoustics, Speech and Signal Processing (ICASSP) , 2019: \nIEEE, pp. 5361-5635.  \n[27] S. Karita et al., â€œA comparative study on transformer vs rnn in \nspeech applications ,â€ in 2019 IEEE Automatic Speech \nRecognition and Understanding Workshop (ASRU) , 2019: \nIEEE, pp. 449-456. \n[28] Z. Tian et al., â€œSelf-attention transducers for end-to-end speech \nrecognition,â€ in Proceedings of INTERSPEECH, 2019, pp. \n4395-4399. \n[29] Z. Gao et al., â€œSan-m: Memory equipped self-attention for end-\nto-end speech recognition,â€ in Proceedings of INTERSPEECH, \n2020, pp. 6-10. \n[30] Z. Fan  et al. , â€œUnsupervised pre -training for sequence to \nsequence speech recognition,\" arXiv preprint \narXiv:1910.12418, 2019. \n[31] K. An  et al. , â€œCAT: crf- based ASR toolkit,â€ arXiv preprint \narXiv:1911.08747, 2019. \n[32] W.-C. Huang et al., â€œSpeech recognition by simply fine-tuning \nBERT,â€ arXiv preprint arXiv:2102.00291, 2021. \n[33] X. Song  et al. , â€œNon-autoregressive transformer ASR with \nCTC-enhanced decoder input,â€ arXiv preprint \narXiv:2010.15025, 2020. \n[34] H. Bu  et al. , â€œAishell -1: An open -source mandarin speech \ncorpus and a spe ech recognition baseline ,â€ in 2017 20th \nConference of the Oriental Chapter of the International \nCoordinating Committee on Speech Databases and Speech I/O \nSystems and Assessment (O-COCOSDA), 2017: IEEE, pp. 1-5. \n[35] G. Hinton et al., â€œDistilling the knowledge in a neural network,â€ \narXiv preprint arXiv:1503.02531, 2015. \n[36] J. Shin et al., â€œEffective sentence scoring method using BERT \nfor speech recognition,â€ in Proc. ACML , 2019, vol. 101, pp. \n1081â€“1093.  \n[37] J. Salazar, et al. , â€œMasked language model scoring,â€ in Proc. \nACL, July 2020, pp. 2699â€“ 2712.  \n[38] Z. Tian et al., â€œSpike-triggered nonautoregressive transformer \nfor end -to-end speech recognition,â€ in  Proceedings of \nINTERSPEECH, 2020. \n[39] A. Paszke et al ., â€œPytorch: An imperative style, \nhighperformance deep learning library ,â€ in Proceeding s of \nNIPS, 2019, pages 8024â€“8035. \n[40] S. Watanabe et al ., â€œEspnet: End-to -end speech processing \ntoolkit,â€ in Proceedings of Interspeech, 2018. \n ",
  "topic": "Autoregressive model",
  "concepts": [
    {
      "name": "Autoregressive model",
      "score": 0.8760315179824829
    },
    {
      "name": "Transformer",
      "score": 0.8165833950042725
    },
    {
      "name": "Computer science",
      "score": 0.7467113733291626
    },
    {
      "name": "Inference",
      "score": 0.6482526063919067
    },
    {
      "name": "Language model",
      "score": 0.6446007490158081
    },
    {
      "name": "End-to-end principle",
      "score": 0.6426083445549011
    },
    {
      "name": "Encoder",
      "score": 0.5586698651313782
    },
    {
      "name": "Speech recognition",
      "score": 0.5575324296951294
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4450089633464813
    },
    {
      "name": "Natural language processing",
      "score": 0.34495121240615845
    },
    {
      "name": "Engineering",
      "score": 0.13831248879432678
    },
    {
      "name": "Mathematics",
      "score": 0.0669940710067749
    },
    {
      "name": "Voltage",
      "score": 0.060898393392562866
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Econometrics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": []
}