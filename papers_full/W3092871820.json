{
    "title": "Text Classification Using Label Names Only: A Language Model Self-Training Approach",
    "url": "https://openalex.org/W3092871820",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2103641016",
            "name": "Meng Yu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1901863177",
            "name": "Zhang, Yunyi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2054901068",
            "name": "Huang Jia-Xin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2746807616",
            "name": "Xiong Chenyan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2604355407",
            "name": "Ji, Heng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1851812986",
            "name": "Zhang Chao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2125747881",
            "name": "Han, Jiawei",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2970200208",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W3104717349",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W3034588688",
        "https://openalex.org/W3105538385",
        "https://openalex.org/W2113459411",
        "https://openalex.org/W746911252",
        "https://openalex.org/W2964074409",
        "https://openalex.org/W2108281845",
        "https://openalex.org/W2470673105",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3004119480",
        "https://openalex.org/W2963216553",
        "https://openalex.org/W2963413667",
        "https://openalex.org/W2144415203",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2533513334",
        "https://openalex.org/W2250966211",
        "https://openalex.org/W2120779048",
        "https://openalex.org/W2964071174",
        "https://openalex.org/W2889577585",
        "https://openalex.org/W2963012544",
        "https://openalex.org/W2890931111",
        "https://openalex.org/W1832693441",
        "https://openalex.org/W2285986798",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W3100474067",
        "https://openalex.org/W2551773530",
        "https://openalex.org/W2953369973",
        "https://openalex.org/W2777746208",
        "https://openalex.org/W2942203175",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3035542229",
        "https://openalex.org/W3035055211",
        "https://openalex.org/W2963804400",
        "https://openalex.org/W2971869958",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2061873838",
        "https://openalex.org/W2166706824",
        "https://openalex.org/W1552847225",
        "https://openalex.org/W3105705953",
        "https://openalex.org/W3034999214"
    ],
    "abstract": "Current text classification methods typically require a good number of human-labeled documents as training data, which can be costly and difficult to obtain in real applications. Humans can perform classification without seeing any labeled examples but only based on a small set of words describing the categories to be classified. In this paper, we explore the potential of only using the label name of each class to train classification models on unlabeled data, without using any labeled documents. We use pre-trained neural language models both as general linguistic knowledge sources for category understanding and as representation learning models for document classification. Our method (1) associates semantically related words with the label names, (2) finds category-indicative words and trains the model to predict their implied categories, and (3) generalizes the model via self-training. We show that our model achieves around 90% accuracy on four benchmark datasets including topic and sentiment classification without using any labeled documents but learning from unlabeled data supervised by at most 3 words (1 in most cases) per class as the label name.",
    "full_text": "Text Classiﬁcation Using Label Names Only: A Language Model\nSelf-Training Approach\nYu Meng1, Yunyi Zhang1, Jiaxin Huang1, Chenyan Xiong2,\nHeng Ji1, Chao Zhang3, Jiawei Han1\n1University of Illinois at Urbana-Champaign, IL, USA\n2Microsoft Research, W A, USA 3Georgia Institute of Technology, GA, USA\n1{yumeng5, yzhan238, jiaxinh3, hengji, hanj}@illinois.edu\n2chenyan.xiong@microsoft.com 3chaozhang@gatech.edu\nAbstract\nCurrent text classiﬁcation methods typically\nrequire a good number of human-labeled doc-\numents as training data, which can be costly\nand difﬁcult to obtain in real applications. Hu-\nmans can perform classiﬁcation without see-\ning any labeled examples but only based on\na small set of words describing the categories\nto be classiﬁed. In this paper, we explore\nthe potential of only using the label name of\neach class to train classiﬁcation models on un-\nlabeled data, without using any labeled doc-\numents. We use pre-trained neural language\nmodels both as general linguistic knowledge\nsources for category understanding and as rep-\nresentation learning models for document clas-\nsiﬁcation. Our method (1) associates semanti-\ncally related words with the label names, (2)\nﬁnds category-indicative words and trains the\nmodel to predict their implied categories, and\n(3) generalizes the model via self-training. We\nshow that our model achieves around 90% ac-\ncuracy on four benchmark datasets including\ntopic and sentiment classiﬁcation without us-\ning any labeled documents but learning from\nunlabeled data supervised by at most 3 words\n(1 in most cases) per class as the label name1.\n1 Introduction\nText classiﬁcation is a classic and fundamental task\nin Natural Language Processing (NLP) with a wide\nspectrum of applications such as question answer-\ning (Rajpurkar et al., 2016), spam detection (Jin-\ndal and Liu, 2007) and sentiment analysis (Pang\net al., 2002). Building an automatic text classiﬁca-\ntion model has been viewed as a task of training\nmachine learning models from human-labeled doc-\numents. Indeed, many deep learning-based clas-\nsiﬁers including CNNs (Kim, 2014; Zhang et al.,\n2015) and RNNs (Tang et al., 2015a; Yang et al.,\n1Source code can be found at https://github.com/\nyumeng5/LOTClass.\n2016) have been developed and achieved great\nsuccess when trained on large-scale labeled doc-\numents (usually over tens of thousands), thanks\nto their strong representation learning power that\neffectively captures the high-order, long-range se-\nmantic dependency in text sequences for accurate\nclassiﬁcation.\nRecently, increasing attention has been paid to\nsemi-supervised text classiﬁcation which requires a\nmuch smaller amount of labeled data. The success\nof semi-supervised methods stems from the usage\nof abundant unlabeled data: Unlabeled documents\nprovide natural regularization for constraining the\nmodel predictions to be invariant to small changes\nin input (Chen et al., 2020; Miyato et al., 2017;\nXie et al., 2019), thus improving the generalization\nability of the model. Despite mitigating the annota-\ntion burden, semi-supervised methods still require\nmanual efforts from domain experts, which might\nbe difﬁcult or expensive to obtain especially when\nthe number of classes is large.\nContrary to existing supervised and semi-\nsupervised models which learn from labeled docu-\nments, a human expert will just need to understand\nthe label name (i.e., a single or a few representative\nwords) of each class to classify documents. For\nexample, we can easily classify news articles when\ngiven the label names such as “sports”, “business”,\nand “politics” because we are able to understand\nthese topics based on prior knowledge.\nIn this paper, we study the problem of weakly-\nsupervised text classiﬁcation where only the label\nname of each class is provided to train a classiﬁer\non purely unlabeled data. We propose a language\nmodel self-training approach wherein a pre-trained\nneural language model (LM) (Devlin et al., 2019;\nPeters et al., 2018; Radford et al., 2018; Yang et al.,\n2019) is used as both the general knowledge source\nfor category understanding and feature represen-\ntation learning model for classiﬁcation. The LM\narXiv:2010.07245v1  [cs.CL]  14 Oct 2020\ncreates contextualized word-level category super-\nvision from unlabeled data to train itself, and then\ngeneralizes to document-level classiﬁcation via a\nself-training objective.\nSpeciﬁcally, we propose the LOTClass model\nfor Label-Name-Only Text Classiﬁcation built in\nthree steps: (1) We construct a category vocabu-\nlary for each class that contains semantically corre-\nlated words with the label name using a pre-trained\nLM. (2) The LM collects high-quality category-\nindicative words in the unlabeled corpus to train\nitself to capture category distinctive information\nwith a contextualized word-level category predic-\ntion task. (3) We generalize the LM via document-\nlevel self-training on abundant unlabeled data.\nLOTClass achieves around 90% accuracy on\nfour benchmark text classiﬁcation datasets, AG\nNews, DBPedia, IMDB and Amazon corpora, with-\nout learning from any labeled data but only using\nat most 3 words (1 word in most cases) per class\nas the label name, outperforming existing weakly-\nsupervised methods signiﬁcantly and yielding even\ncomparable performance to strong semi-supervised\nand supervised models.\nThe contributions of this paper are as follows:\n• We propose a weakly-supervised text classiﬁca-\ntion model LOTClass based on a pre-trained\nneural LM without any further dependencies 2.\nLOTClass does not need any labeled documents\nbut only the label name of each class.\n• We propose a method for ﬁnding category-\nindicative words and a contextualized word-level\ncategory prediction task that trains LM to predict\nthe implied category of a word using its con-\ntexts. The LM so trained generalizes well to\ndocument-level classiﬁcation upon self-training\non unlabeled corpus.\n• On four benchmark datasets, LOTClass outper-\nforms signiﬁcantly weakly-supervised models\nand has comparable performance to strong semi-\nsupervised and supervised models.\n2 Related Work\n2.1 Neural Language Models\nPre-training deep neural models for language\nmodeling, including autoregressive LMs such as\n2Other semi-supervised/weakly-supervised methods usu-\nally take advantage of distant supervision like Wikipedia\ndump (Chang et al., 2008), or augmentation systems like\ntrained back translation models (Xie et al., 2019).\nELMo (Peters et al., 2018), GPT (Radford et al.,\n2018) and XLNet (Yang et al., 2019) and autoen-\ncoding LMs such as BERT (Devlin et al., 2019) and\nits variants (Lan et al., 2020; Lewis et al., 2020; Liu\net al., 2019b), has brought astonishing performance\nimprovement to a wide range of NLP tasks, mainly\nfor two reasons: (1) LMs are pre-trained on large-\nscale text corpora, which allow the models to learn\ngeneric linguistic features (Tenney et al., 2019) and\nserve as knowledge bases (Petroni et al., 2019);\nand (2) LMs enjoy strong feature representation\nlearning power of capturing high-order, long-range\ndependency in texts thanks to the Transformer ar-\nchitecture (Vaswani et al., 2017).\n2.2 Semi-Supervised and Zero-Shot Text\nClassiﬁcation\nFor semi-supervised text classiﬁcation, two lines\nof framework are developed to leverage unlabeled\ndata. Augmentation-based methods generate new\ninstances and regularize the model’s predictions\nto be invariant to small changes in input. The\naugmented instances can be either created as real\ntext sequences (Xie et al., 2019) via back transla-\ntion (Sennrich et al., 2016) or in the hidden states\nof the model via perturbations (Miyato et al., 2017)\nor interpolations (Chen et al., 2020). Graph-based\nmethods (Tang et al., 2015b; Zhang et al., 2020)\nbuild text networks with words, documents and la-\nbels and propagate labeling information along the\ngraph via embedding learning (Tang et al., 2015c)\nor graph neural networks (Kipf and Welling, 2017).\nZero-shot text classiﬁcation generalizes the clas-\nsiﬁer trained on a known label set to an unknown\none without using any new labeled documents.\nTransferring knowledge from seen classes to un-\nseen ones typically relies on semantic attributes and\ndescriptions of all classes (Liu et al., 2019a; Pushp\nand Srivastava, 2017; Xia et al., 2018), correlations\namong classes (Rios and Kavuluru, 2018; Zhang\net al., 2019) or joint embeddings of classes and\ndocuments (Nam et al., 2016). However, zero-shot\nlearning still requires labeled data for the seen label\nset and cannot be applied to cases where no labeled\ndocuments for any class is available.\n2.3 Weakly-Supervised Text Classiﬁcation\nWeakly-supervised text classiﬁcation aims to cat-\negorize text documents based only on word-level\ndescriptions of each category, eschewing the need\nof any labeled documents. Early attempts rely on\ndistant supervision such as Wikipedia to interpret\nthe label name semantics and derive document-\nconcept relevance via explicit semantic analy-\nsis (Gabrilovich and Markovitch, 2007). Since\nthe classiﬁer is learned purely from general knowl-\nedge without even requiring any unlabeled domain-\nspeciﬁc data, these methods are called dataless\nclassiﬁcation (Chang et al., 2008; Song and Roth,\n2014; Yin et al., 2019). Later, topic models (Chen\net al., 2015; Li et al., 2016) are exploited for seed-\nguided classiﬁcation to learn seed word-aware top-\nics by biasing the Dirichlet priors and to infer pos-\nterior document-topic assignment. Recently, neu-\nral approaches (Mekala and Shang, 2020; Meng\net al., 2018, 2019) have been developed for weakly-\nsupervised text classiﬁcation. They assign docu-\nments pseudo labels to train a neural classiﬁer by\neither generating pseudo documents or using LMs\nto detect category-indicative words. While achiev-\ning inspiring performance, these neural approaches\ntrain classiﬁers from scratch on the local corpus\nand fail to take advantage of the general knowledge\nsource used by dataless classiﬁcation. In this pa-\nper, we build our method upon pre-trained LMs,\nwhich are used both as general linguistic knowl-\nedge sources for understanding the semantics of\nlabel names, and as strong feature representation\nlearning models for classiﬁcation.\n3 Method\nIn this section, we introduce LOTClass with\nBERT (Devlin et al., 2019) as our backbone model,\nbut our method can be easily adapted to other pre-\ntrained neural LMs.\n3.1 Category Understanding via Label Name\nReplacement\nWhen provided label names, humans are able to\nunderstand the semantics of each label based on\ngeneral knowledge by associating with it other cor-\nrelated keywords that indicate the same category.\nIn this section, we introduce how to learn a cate-\ngory vocabulary from the label name of each class\nwith a pre-trained LM, similar to the idea of topic\nmining in recent studies (Meng et al., 2020a,b).\nIntuitively, words that are interchangeable most\nof the time are likely to have similar meanings. We\nuse the pre-trained BERT masked language model\n(MLM) to predict what words can replace the la-\nbel names under most contexts. Speciﬁcally, for\neach occurrence of a label name in the corpus, we\nfeed its contextualized embedding vector h ∈Rh\nproduced by the BERT encoder to the MLM head,\nwhich will output a probability distribution over\nthe entire vocabulary V, indicating the likelihood\nof each word wappearing at this position:\np(w|h) =Softmax (W2 σ(W1h + b)) , (1)\nwhere σ(·) is the activation function; W1 ∈Rh×h,\nb ∈Rh, and W2 ∈R|V |×h are learnable param-\neters that have been pre-trained with the MLM\nobjective of BERT.\nTable 1 shows the pre-trained MLM prediction\nfor the top words (sorted by p(w |h)) to replace\nthe original label name “sports” under two different\ncontexts. We observe that for each masked word,\nthe top-50 predicted words usually have similar\nmeanings with the original word, and thus we use\nthe threshold of 50 words given by the MLM to\ndeﬁne valid replacement for each occurrence of the\nlabel names in the corpus. Finally, we form the cat-\negory vocabulary of each class using the top-100\nwords ranked by how many times they can replace\nthe label name in the corpus, discarding stopwords\nwith NLTK (Bird et al., 2009) and words that ap-\npear in multiple categories. Tables 2, 3, 4 and 9\n(Table 9 is in Appendix A) show the label name\nused for each category and the obtained category\nvocabulary of AG News, IMDB, Amazon and DB-\nPedia corpora, respectively.\n3.2 Masked Category Prediction\nLike how humans perform classiﬁcation, we want\nthe classiﬁcation model to focus on category-\nindicative words in a sequence. A straightforward\nway is to directly highlight every occurrence of the\ncategory vocabulary entry in the corpus. However,\nthis approach is error-prone because: (1) Word\nmeanings are contextualized; not every occurrence\nof the category keywords indicates the category.\nFor example, as shown in Table 1, the word “sports”\nin the second sentence does not imply the topic\n“sports”. (2) The coverage of the category vocabu-\nlary is limited; some terms under speciﬁc contexts\nhave similar meanings with the category keywords\nbut are not included in the category vocabulary.\nTo address the aforementioned challenge, we\nintroduce a new task, Masked Category Predic-\ntion (MCP), as illustrated in Fig. 1, wherein a pre-\ntrained LM creates contextualized word-level cat-\negory supervision for training itself to predict the\nimplied category of a word with the word masked.\nTo create contextualized word-level category su-\npervision, we reuse the pre-trained MLM method in\nSentence Language Model Prediction\nThe oldest annual US team sports competition that\nincludes professionals is not in baseball, or football or\nbasketball or hockey. It’s in soccer.\nsports, baseball, handball, soccer,\nbasketball, football, tennis, sport,\nchampionship, hockey, . . .\nSamsung’s new SPH-V5400 mobile phonesports a built-in\n1-inch, 1.5-gigabyte hard disk that can store about 15 times\nmore data than conventional handsets, Samsung said.\nhas, with, features, uses, includes,\nhad, is, contains, featured, have,\nincorporates, requires, offers, . . .\nTable 1: BERT language model prediction (sorted by probability) for the word to appear at the position of “sports”\nunder different contexts. The two sentences are from AG News corpus.\nLabel Name Category Vocabulary\npolitics\npolitics, political, politicians, government, elections, politician, democracy,\ndemocratic, governing, party, leadership, state, election, politically, affairs, issues,\ngovernments, voters, debate, cabinet, congress, democrat, president, religion, . . .\nsports\nsports, games, sporting, game, athletics, national, athletic, espn, soccer, basketball,\nstadium, arts, racing, baseball, tv, hockey, pro, press, team, red, home, bay, kings,\ncity, legends, winning, miracle, olympic, ball, giants, players, champions, boxing, . . .\nbusiness\nbusiness, trade, commercial, enterprise, shop, money, market, commerce, corporate,\nglobal, future, sales, general, international, group, retail, management, companies,\noperations, operation, store, corporation, venture, economic, division, ﬁrm, . . .\ntechnology\ntechnology, tech, software, technological, device, equipment, hardware, devices,\ninfrastructure, system, knowledge, technique, digital, technical, concept, systems,\ngear, techniques, functionality, process, material, facility, feature, method, . . .\nTable 2: The label name used for each class of AG News dataset and the learned category vocabulary.\nSection 3.1 to understand the contextualized mean-\ning of each word by examining what are valid re-\nplacement words. As shown in Table 1, the MLM\npredicted words are good indicators of the original\nword’s meaning. As before, we regard the top-50\nwords given by the MLM as valid replacement of\nthe original word, and we consider a word w as\n“category-indicative” for classcw if more than 20\nout of 50 w’s replacing words appear in the cate-\ngory vocabulary of class cw. By examining every\nword in the corpus as above, we will obtain a set of\ncategory-indicative words and their category labels\nSind as word-level supervision.\nFor each category-indicative wordw, we mask it\nout with the [MASK] token and train the model\nto predict w’s indicating category cw via cross-\nentropy loss with a classiﬁer (a linear layer) on\ntop of w’s contextualized embeddingh:\nLMCP = −\n∑\n(w,cw)∈Sind\nlog p(cw |hw), (2)\np(c|h) =Softmax (Wch + bc) , (3)\nwhere Wc ∈RK×h and bc ∈RK are learnable\nparameters of the linear layer (Kis the number of\nclasses).\nWe note that it is crucial to mask out the category-\nindicative word for category prediction, because\nthis forces the model to infer categories based on\nthe word’s contexts instead of simply memoriz-\ning context-free category keywords. In this way,\nthe BERT encoder will learn to encode category-\ndiscriminative information within the sequence into\nthe contextualized embedding h that is helpful for\npredicting the category at its position.\n3.3 Self-Training\nAfter training the LM with the MCP task, we pro-\npose to self-train the model on the entire unlabeled\ncorpus for two reasons: (1) There are still many\nunlabeled documents not seen by the model in the\nMCP task (due to no category keywords detected)\nthat can be used to reﬁne the model for better gen-\neralization. (2) The classiﬁer has been trained on\ntop of words to predict their categories with them\nmasked, but have not been applied on the [CLS]\ntoken where the model is allowed to see the entire\nLabel Name Category Vocabulary\ngood\ngood, excellent, fair, wonderful, sound, high, okay, positive, sure, solid, quality,\nsmart, normal, special, successful, quick, home, brilliant, beautiful, tough, fun,\ncool, amazing, done, interesting, superb, made, outstanding, sweet, happy, old, . . .\nbad\nbad, badly, worst, mad, worse, sad, dark, awful, rotten, rough, mean, dumb,\nnegative, nasty, mixed, thing, much, fake, guy, ugly, crazy, german, gross, weird,\nsorry, like, short, scary, way, sick, white, black, shit, average, dangerous, stuff, . . .\nTable 3: The label name used for each class of IMDB dataset and the learned category vocabulary.\nLabel Name Category Vocabulary\ngood\ngood, excellent, ﬁne, right, fair, sound, wonderful, high, okay, sure, quality, smart,\npositive, solid, special, home, quick, safe, beautiful, cool, valuable, normal,\namazing, successful, interesting, useful, tough, fun, done, sweet, rich, suitable, . . .\nbad\nbad, terrible, horrible, badly, wrong, sad, worst, worse, mad, dark, awful, mean,\nrough, rotten, much, mixed, dumb, nasty, sorry, thing, negative, funny, far, go, crazy,\nweird, lucky, german, shit, guy, ugly, short, weak, sick, gross, dangerous, fake, . . .\nTable 4: The label name used for each class of Amazon dataset and the learned category vocabulary.\nsequence to predict its category.\nThe idea of self-training (ST) is to iteratively\nuse the model’s current prediction P to compute\na target distribution Qwhich guides the model for\nreﬁnement. The general form of ST objective can\nbe expressed with the KL divergence loss:\nLST = KL(Q∥P) =\nN∑\ni=1\nK∑\nj=1\nqij log qij\npij\n, (4)\nwhere N is the number of instances.\nThere are two major choices of the target distri-\nbution Q: Hard labeling and soft labeling. Hard\nlabeling (Lee, 2013) converts high-conﬁdence pre-\ndictions over a threshold τ to one-hot labels, i.e.,\nqij = 1 (pij >τ ), where 1 (·) is the indicator func-\ntion. Soft labeling (Xie et al., 2016) derives Q\nby enhancing high-conﬁdence predictions while\ndemoting low-conﬁdence ones via squaring and\nnormalizing the current predictions:\nqij =\np2\nij/fj\n∑\nj′\n(\np2\nij′ /fj′\n), fj =\n∑\ni\npij, (5)\nwhere the model prediction is made by applying the\nclassiﬁer trained via MCP (Eq. (3)) to the [CLS]\ntoken of each document, i.e.,\npij = p(cj |hdi:[CLS]). (6)\nIn practice, we ﬁnd that the soft labeling strategy\nconsistently gives better and more stable results\nthan hard labeling, probably because hard labeling\ntreats high-conﬁdent predictions directly as ground-\ntruth labels and is more prone to error propagation.\nAnother advantage of soft labeling is that the target\ndistribution is computed for every instance and no\nconﬁdence thresholds need to be preset.\nWe update the target distribution Qvia Eq. (5)\nevery 50 batches and train the model via Eq. (4).\nThe overall algorithm is shown in Algorithm 1.\nAlgorithm 1: LOTClass Training.\nInput: An unlabeled text corpus D; a set of\nlabel names C; a pre-trained neural\nlanguage model M.\nOutput: A trained model M for classifying\nthe Kclasses.\nCategory vocabulary ←Section 3.1;\nSind ←Section 3.2;\nTrain M with Eq. (2);\nB ←Total number of batches;\nfor i←0 to B−1 do\nif imod 50 = 0then\nQ←Eq. (5);\nTrain M on batch iwith Eq. (4);\nReturn M;\nBERT Encoder\n(Pre-trained, not ﬁne-tuned, as general knowledge)\nsports\nsports\nteam\nteam\nUS\nUS\ncompetition\ncompetition\nMLM Probable Words (Top 50):\nsports, baseball, handball, soccer…\nCategory 2 Vocabulary:\nsports, soccer, game, baseball, sport…\nCategory 1 Vocabulary:\npolitics, political, politicians, government…\nCategory 3 Vocabulary:\nbusiness, trade, commercial, enterprise…\nBERT Encoder\n(Pre-trained, ﬁne-tuned, as classiﬁcation model)\nteam\nteam\nUS\nUS\ncompetition\ncompetition\nMCP\nContextualized EmbeddingsInput Tokens\n Neural Network Modules\nWord-Level \nCategory Prediction:\n[0, 1, 0]\n> 20/50 matched\n···\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\n···\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\n···\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\n···\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\n···\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\n···\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\n···\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\n···\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\n[CLS]\n<latexit sha1_base64=\"hLexQ2ue+kyjKjaV8rYXfkjAhMU=\">AAAB9XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMdALh48RDQP2KxhdjKbDJl9MNOrhiX/4cWDIl79F2/+jZNkD5pY0FBUddPd5SdSaLTtb6uwsrq2vlHcLG1t7+zulfcPWjpOFeNNFstYdXyquRQRb6JAyTuJ4jT0JW/7o/rUbz9wpUUc3eE44V5IB5EIBKNopPsu8idEzNz69a036ZUrdtWegSwTJycVyNHolb+6/ZilIY+QSaq169gJehlVKJjkk1I31TyhbEQH3DU0oiHXXja7ekJOjNInQaxMRUhm6u+JjIZaj0PfdIYUh3rRm4r/eW6KwaWXiShJkUdsvihIJcGYTCMgfaE4Qzk2hDIlzK2EDamiDE1QJROCs/jyMmmdVR2TzM15pWbncRThCI7hFBy4gBpcQQOawEDBM7zCm/VovVjv1se8tWDlM4fwB9bnD6/uko4=</latexit><latexit sha1_base64=\"hLexQ2ue+kyjKjaV8rYXfkjAhMU=\">AAAB9XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMdALh48RDQP2KxhdjKbDJl9MNOrhiX/4cWDIl79F2/+jZNkD5pY0FBUddPd5SdSaLTtb6uwsrq2vlHcLG1t7+zulfcPWjpOFeNNFstYdXyquRQRb6JAyTuJ4jT0JW/7o/rUbz9wpUUc3eE44V5IB5EIBKNopPsu8idEzNz69a036ZUrdtWegSwTJycVyNHolb+6/ZilIY+QSaq169gJehlVKJjkk1I31TyhbEQH3DU0oiHXXja7ekJOjNInQaxMRUhm6u+JjIZaj0PfdIYUh3rRm4r/eW6KwaWXiShJkUdsvihIJcGYTCMgfaE4Qzk2hDIlzK2EDamiDE1QJROCs/jyMmmdVR2TzM15pWbncRThCI7hFBy4gBpcQQOawEDBM7zCm/VovVjv1se8tWDlM4fwB9bnD6/uko4=</latexit><latexit sha1_base64=\"hLexQ2ue+kyjKjaV8rYXfkjAhMU=\">AAAB9XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMdALh48RDQP2KxhdjKbDJl9MNOrhiX/4cWDIl79F2/+jZNkD5pY0FBUddPd5SdSaLTtb6uwsrq2vlHcLG1t7+zulfcPWjpOFeNNFstYdXyquRQRb6JAyTuJ4jT0JW/7o/rUbz9wpUUc3eE44V5IB5EIBKNopPsu8idEzNz69a036ZUrdtWegSwTJycVyNHolb+6/ZilIY+QSaq169gJehlVKJjkk1I31TyhbEQH3DU0oiHXXja7ekJOjNInQaxMRUhm6u+JjIZaj0PfdIYUh3rRm4r/eW6KwaWXiShJkUdsvihIJcGYTCMgfaE4Qzk2hDIlzK2EDamiDE1QJROCs/jyMmmdVR2TzM15pWbncRThCI7hFBy4gBpcQQOawEDBM7zCm/VovVjv1se8tWDlM4fwB9bnD6/uko4=</latexit><latexit sha1_base64=\"hLexQ2ue+kyjKjaV8rYXfkjAhMU=\">AAAB9XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMdALh48RDQP2KxhdjKbDJl9MNOrhiX/4cWDIl79F2/+jZNkD5pY0FBUddPd5SdSaLTtb6uwsrq2vlHcLG1t7+zulfcPWjpOFeNNFstYdXyquRQRb6JAyTuJ4jT0JW/7o/rUbz9wpUUc3eE44V5IB5EIBKNopPsu8idEzNz69a036ZUrdtWegSwTJycVyNHolb+6/ZilIY+QSaq169gJehlVKJjkk1I31TyhbEQH3DU0oiHXXja7ekJOjNInQaxMRUhm6u+JjIZaj0PfdIYUh3rRm4r/eW6KwaWXiShJkUdsvihIJcGYTCMgfaE4Qzk2hDIlzK2EDamiDE1QJROCs/jyMmmdVR2TzM15pWbncRThCI7hFBy4gBpcQQOawEDBM7zCm/VovVjv1se8tWDlM4fwB9bnD6/uko4=</latexit>\n[CLS]\n<latexit sha1_base64=\"hLexQ2ue+kyjKjaV8rYXfkjAhMU=\">AAAB9XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMdALh48RDQP2KxhdjKbDJl9MNOrhiX/4cWDIl79F2/+jZNkD5pY0FBUddPd5SdSaLTtb6uwsrq2vlHcLG1t7+zulfcPWjpOFeNNFstYdXyquRQRb6JAyTuJ4jT0JW/7o/rUbz9wpUUc3eE44V5IB5EIBKNopPsu8idEzNz69a036ZUrdtWegSwTJycVyNHolb+6/ZilIY+QSaq169gJehlVKJjkk1I31TyhbEQH3DU0oiHXXja7ekJOjNInQaxMRUhm6u+JjIZaj0PfdIYUh3rRm4r/eW6KwaWXiShJkUdsvihIJcGYTCMgfaE4Qzk2hDIlzK2EDamiDE1QJROCs/jyMmmdVR2TzM15pWbncRThCI7hFBy4gBpcQQOawEDBM7zCm/VovVjv1se8tWDlM4fwB9bnD6/uko4=</latexit><latexit sha1_base64=\"hLexQ2ue+kyjKjaV8rYXfkjAhMU=\">AAAB9XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMdALh48RDQP2KxhdjKbDJl9MNOrhiX/4cWDIl79F2/+jZNkD5pY0FBUddPd5SdSaLTtb6uwsrq2vlHcLG1t7+zulfcPWjpOFeNNFstYdXyquRQRb6JAyTuJ4jT0JW/7o/rUbz9wpUUc3eE44V5IB5EIBKNopPsu8idEzNz69a036ZUrdtWegSwTJycVyNHolb+6/ZilIY+QSaq169gJehlVKJjkk1I31TyhbEQH3DU0oiHXXja7ekJOjNInQaxMRUhm6u+JjIZaj0PfdIYUh3rRm4r/eW6KwaWXiShJkUdsvihIJcGYTCMgfaE4Qzk2hDIlzK2EDamiDE1QJROCs/jyMmmdVR2TzM15pWbncRThCI7hFBy4gBpcQQOawEDBM7zCm/VovVjv1se8tWDlM4fwB9bnD6/uko4=</latexit><latexit sha1_base64=\"hLexQ2ue+kyjKjaV8rYXfkjAhMU=\">AAAB9XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMdALh48RDQP2KxhdjKbDJl9MNOrhiX/4cWDIl79F2/+jZNkD5pY0FBUddPd5SdSaLTtb6uwsrq2vlHcLG1t7+zulfcPWjpOFeNNFstYdXyquRQRb6JAyTuJ4jT0JW/7o/rUbz9wpUUc3eE44V5IB5EIBKNopPsu8idEzNz69a036ZUrdtWegSwTJycVyNHolb+6/ZilIY+QSaq169gJehlVKJjkk1I31TyhbEQH3DU0oiHXXja7ekJOjNInQaxMRUhm6u+JjIZaj0PfdIYUh3rRm4r/eW6KwaWXiShJkUdsvihIJcGYTCMgfaE4Qzk2hDIlzK2EDamiDE1QJROCs/jyMmmdVR2TzM15pWbncRThCI7hFBy4gBpcQQOawEDBM7zCm/VovVjv1se8tWDlM4fwB9bnD6/uko4=</latexit><latexit sha1_base64=\"hLexQ2ue+kyjKjaV8rYXfkjAhMU=\">AAAB9XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMdALh48RDQP2KxhdjKbDJl9MNOrhiX/4cWDIl79F2/+jZNkD5pY0FBUddPd5SdSaLTtb6uwsrq2vlHcLG1t7+zulfcPWjpOFeNNFstYdXyquRQRb6JAyTuJ4jT0JW/7o/rUbz9wpUUc3eE44V5IB5EIBKNopPsu8idEzNz69a036ZUrdtWegSwTJycVyNHolb+6/ZilIY+QSaq169gJehlVKJjkk1I31TyhbEQH3DU0oiHXXja7ekJOjNInQaxMRUhm6u+JjIZaj0PfdIYUh3rRm4r/eW6KwaWXiShJkUdsvihIJcGYTCMgfaE4Qzk2hDIlzK2EDamiDE1QJROCs/jyMmmdVR2TzM15pWbncRThCI7hFBy4gBpcQQOawEDBM7zCm/VovVjv1se8tWDlM4fwB9bnD6/uko4=</latexit>\nsports\n[MASK]\n<latexit sha1_base64=\"DTQHaOsJ5gwF+zQectHbaT/ajcg=\">AAAB+HicbVBNS8NAEJ3Ur1o/WvXoJVgETyURQY8VL4IIFe0HpKFstpt26WYTdidiDf0lXjwo4tWf4s1/47bNQasPBh7vzTAzL0gE1+g4X1ZhaXllda24XtrY3NouV3Z2WzpOFWVNGotYdQKimeCSNZGjYJ1EMRIFgrWD0cXUb98zpXks73CcMD8iA8lDTgkaqVcpd5E9IGLmXZ/fXvmTXqXq1JwZ7L/EzUkVcjR6lc9uP6ZpxCRSQbT2XCdBPyMKORVsUuqmmiWEjsiAeYZKEjHtZ7PDJ/ahUfp2GCtTEu2Z+nMiI5HW4ygwnRHBoV70puJ/npdieOZnXCYpMknni8JU2Bjb0xTsPleMohgbQqji5labDokiFE1WJROCu/jyX9I6rrkmmZuTat3J4yjCPhzAEbhwCnW4hAY0gUIKT/ACr9aj9Wy9We/z1oKVz+zBL1gf38I1kxM=</latexit><latexit sha1_base64=\"DTQHaOsJ5gwF+zQectHbaT/ajcg=\">AAAB+HicbVBNS8NAEJ3Ur1o/WvXoJVgETyURQY8VL4IIFe0HpKFstpt26WYTdidiDf0lXjwo4tWf4s1/47bNQasPBh7vzTAzL0gE1+g4X1ZhaXllda24XtrY3NouV3Z2WzpOFWVNGotYdQKimeCSNZGjYJ1EMRIFgrWD0cXUb98zpXks73CcMD8iA8lDTgkaqVcpd5E9IGLmXZ/fXvmTXqXq1JwZ7L/EzUkVcjR6lc9uP6ZpxCRSQbT2XCdBPyMKORVsUuqmmiWEjsiAeYZKEjHtZ7PDJ/ahUfp2GCtTEu2Z+nMiI5HW4ygwnRHBoV70puJ/npdieOZnXCYpMknni8JU2Bjb0xTsPleMohgbQqji5labDokiFE1WJROCu/jyX9I6rrkmmZuTat3J4yjCPhzAEbhwCnW4hAY0gUIKT/ACr9aj9Wy9We/z1oKVz+zBL1gf38I1kxM=</latexit><latexit sha1_base64=\"DTQHaOsJ5gwF+zQectHbaT/ajcg=\">AAAB+HicbVBNS8NAEJ3Ur1o/WvXoJVgETyURQY8VL4IIFe0HpKFstpt26WYTdidiDf0lXjwo4tWf4s1/47bNQasPBh7vzTAzL0gE1+g4X1ZhaXllda24XtrY3NouV3Z2WzpOFWVNGotYdQKimeCSNZGjYJ1EMRIFgrWD0cXUb98zpXks73CcMD8iA8lDTgkaqVcpd5E9IGLmXZ/fXvmTXqXq1JwZ7L/EzUkVcjR6lc9uP6ZpxCRSQbT2XCdBPyMKORVsUuqmmiWEjsiAeYZKEjHtZ7PDJ/ahUfp2GCtTEu2Z+nMiI5HW4ygwnRHBoV70puJ/npdieOZnXCYpMknni8JU2Bjb0xTsPleMohgbQqji5labDokiFE1WJROCu/jyX9I6rrkmmZuTat3J4yjCPhzAEbhwCnW4hAY0gUIKT/ACr9aj9Wy9We/z1oKVz+zBL1gf38I1kxM=</latexit><latexit sha1_base64=\"DTQHaOsJ5gwF+zQectHbaT/ajcg=\">AAAB+HicbVBNS8NAEJ3Ur1o/WvXoJVgETyURQY8VL4IIFe0HpKFstpt26WYTdidiDf0lXjwo4tWf4s1/47bNQasPBh7vzTAzL0gE1+g4X1ZhaXllda24XtrY3NouV3Z2WzpOFWVNGotYdQKimeCSNZGjYJ1EMRIFgrWD0cXUb98zpXks73CcMD8iA8lDTgkaqVcpd5E9IGLmXZ/fXvmTXqXq1JwZ7L/EzUkVcjR6lc9uP6ZpxCRSQbT2XCdBPyMKORVsUuqmmiWEjsiAeYZKEjHtZ7PDJ/ahUfp2GCtTEu2Z+nMiI5HW4ygwnRHBoV70puJ/npdieOZnXCYpMknni8JU2Bjb0xTsPleMohgbQqji5labDokiFE1WJROCu/jyX9I6rrkmmZuTat3J4yjCPhzAEbhwCnW4hAY0gUIKT/ACr9aj9Wy9We/z1oKVz+zBL1gf38I1kxM=</latexit>\nFigure 1: Overview of Masked Category Prediction (MCP). The Masked Language Model (MLM) head ﬁrst\npredicts what are probable words to appear at each token’s position. A token is considered as “category-indicative”\nif its probable replacement words highly overlap with the category vocabulary of a certain class. The MCP head is\ntrained to predict the implied categories of the category-indicative words with them masked.\nDataset Classiﬁcation Type # Classes # Train # Test\nAG News News Topic 4 120,000 7,600\nDBPedia Wikipedia Topic 14 560,000 70,000\nIMDB Movie Review Sentiment 2 25,000 25,000\nAmazon Product Review Sentiment 2 3,600,000 400,000\nTable 5: Dataset statistics. Supervised models are trained on the entire training set. Semi-supervised models use\n10 labeled documents per class from the training set and the rest as unlabeled data. Weakly-supervised models are\ntrained by using the entire training set as unlabeled data. All models are evaluated on the test set.\n4 Experiments\n4.1 Datasets\nWe use four benchmark datasets for text classi-\nﬁcation: AG News (Zhang et al., 2015), DBPe-\ndia (Lehmann et al., 2015), IMDB (Maas et al.,\n2011) and Amazon (McAuley and Leskovec, 2013).\nThe dataset statistics are shown in Table 5. All\ndatasets are in English language.\n4.2 Compared Methods\nWe compare LOTClass with a wide range of\nweakly-supervised methods and also state-of-the-\nart semi-supervised and supervised methods. The\nlabel names used as supervision on each dataset\nfor the weakly-supervised methods are shown in\nTables 2, 3, 4 and 9. (Table 9 can be found in Ap-\npendix A.) Fully supervised methods use the entire\ntraining set for model training. Semi-supervised\nmethod UDA uses 10 labeled documents per class\nfrom the training set and the rest as unlabeled data.\nWeakly-supervised methods use the training set as\nunlabeled data. All methods are evaluated on the\ntest set.\nWeakly-supervised methods:\n• Dataless (Chang et al., 2008): Dataless classi-\nﬁcation maps label names and each document\ninto the same semantic space of Wikipedia con-\ncepts. Classiﬁcation is performed based on vec-\ntor similarity between documents and classes us-\ning explicit semantic analysis (Gabrilovich and\nMarkovitch, 2007).\n• WeSTClass (Meng et al., 2018): WeSTClass\ngenerates pseudo documents to pre-train a CNN\nclassiﬁer and then bootstraps the model on unla-\nbeled data with self-training.\n• BERT w. simple match : We treat each docu-\nment containing the label name as if it is a labeled\ndocument of the corresponding class to train the\nBERT model.\n• LOTClass w/o. self train : This is an ablation\nversion of our method. We train LOTClass\nonly with the MCP task, without performing self-\ntraining on the entire unlabeled data.\nSemi-supervised method:\n• UDA (Xie et al., 2019): Unsupervised data aug-\nmentation is the state-of-the-art semi-supervised\ntext classiﬁcation method. Apart from using a\nsmall amount of labeled documents for super-\nvised training, it uses back translation (Sennrich\net al., 2016) and TF-IDF word replacing for aug-\nmentation and enforces the model to make con-\nsistent predictions over the augmentations.\nSupervised methods:\n• char-CNN (Zhang et al., 2015): Character-level\nCNN was one of the state-of-the-art supervised\ntext classiﬁcation models before the appearance\nof neural LMs. It encodes the text sequences into\ncharacters and applies 6-layer CNNs for feature\nlearning and classiﬁcation.\n• BERT (Devlin et al., 2019): We use the pre-\ntrained BERT-base-uncased model and ﬁne-tune\nit with the training data for classiﬁcation.\n4.3 Experiment Settings\nWe use the pre-trained BERT-base-uncased model\nas the base neural LM. For the four datasets AG\nNews, DBPedia, IMDB and Amazon, the maximum\nsequence lengths are set to be 200, 200, 512 and\n200 tokens. The training batch size is 128. We\nuse Adam (Kingma and Ba, 2015) as the optimizer.\nThe peak learning rate is 2e−5 and 1e−6 for\nMCP and self-training, respectively. The model is\nrun on 4 NVIDIA GeForce GTX 1080 Ti GPUs.\n4.4 Results\nThe classiﬁcation accuracy of all methods on the\ntest set is shown in Table 6.LOTClass consistently\noutperforms all weakly-supervised methods by a\nlarge margin. Even without self-training, LOT-\nClass’s ablation version performs decently across\nall datasets, demonstrating the effectiveness of our\nproposed category understanding method and the\nMCP task. With the help of self-training, LOT-\nClass’s performance becomes comparable to state-\nof-the-art semi-supervised and supervised models.\nHow many labeled documents are label names\nworth? We vary the number of labeled docu-\nments per class on AG News dataset for training\nSupervised BERTand show its corresponding per-\nformance in Fig. 2(a). The performance of LOT-\nClass is equivalent to that of Supervised BERT\nwith 48 labeled documents per class.\n4.5 Study of Category Understanding\nWe study the characteristics of the method intro-\nduced in Section 3.1 from the following two as-\npects. (1) Sensitivity to different words as label\nnames. We use “commerce” and “economy” to\nreplace “business” as the label name onAG News\ndataset. Table 7 shows the resulting learned cat-\negory vocabulary. We observe that despite the\nchange in label name, around half of terms in the\nresulting category vocabulary overlap with the orig-\ninal one (Table 2 “business” category); the other\nhalf also indicate very similar meanings. This guar-\nantees the robustness of our method since it is the\ncategory vocabulary rather than the original label\nname that is used in subsequent steps. (2) Ad-\nvantages over alternative solutions. We take the\npre-trained 300-d GloVe (Pennington et al., 2014)\nembeddings and use the top words ranked by co-\nsine similarity with the label names for category\nvocabulary construction. On Amazon dataset, we\nuse “good” and “bad” as the label names, and the\ncategory vocabulary built by LOTClass (Table 4)\naccurately reﬂects the sentiment polarity, while the\nresults given by GloVe (Table 8) are poor—some\nwords that are close to “good”/“bad” in the GloVe\nembedding space do not indicate sentiment, or even\nthe reversed sentiment (the closest word to “bad” is\n“good”). This is because context-free embeddings\nonly learn from local context windows, while neu-\nral LMs capture long-range dependency that leads\nto accurate interpretation of the target word.\n4.6 Effect of Self-Training\nWe study the effect of self-training with two sets of\nexperiments: (1) In Fig. 2(b) we show the test accu-\nracy and self-training loss (Eq. (4)) when training\nLOTClass on the ﬁrst 1,000 steps (batches) of un-\nlabeled documents. It can be observed that the loss\ndecreases within a period of 50 steps, which is the\nupdate interval for the target distribution Q—when\nthe self training loss approximates zero, the model\nhas ﬁt the previous Qand a new target distribution\nis computed based on the most recent predictions.\nWith the model reﬁning itself on unlabeled data iter-\natively, the performance gradually improves. (2) In\nFig. 2(c) we show the performance of LOTClass\nvs. BERT w. simple match with the same self-\nSupervision Type Methods AG News DBPedia IMDB Amazon\nWeakly-Sup.\nDataless (Chang et al., 2008) 0.696 0.634 0.505 0.501\nWeSTClass (Meng et al., 2018) 0.823 0.811 0.774 0.753\nBERT w. simple match 0.752 0.722 0.677 0.654\nLOTClass w/o. self train 0.822 0.860 0.802 0.853\nLOTClass 0.864 0.911 0.865 0.916\nSemi-Sup. UDA (Xie et al., 2019) 0.869 0.986 0.887 0.960\nSupervised char-CNN (Zhang et al., 2015) 0.872 0.983 0.853 0.945\nBERT (Devlin et al., 2019) 0.944 0.993 0.945 0.972\nTable 6: Test accuracy of all methods on four datasets.\nLabel Name Category Vocabulary\ncommerce\ncommerce, trade, consumer, retail, trading, merchants, treasury, currency, sales,\ncommercial, market, merchant, economy, economic, marketing, store, exchange,\ntransactions, marketplace, businesses, investment, markets, trades, enterprise, . . .\neconomy\neconomy, economic, economies, economics, currency, trade, future, gdp, treasury,\nsector, production, market, investment, growth, mortgage, commodity, money,\nmarkets, commerce, economical, prosperity, account, income, stock, store, . . .\nTable 7: Different label names used for class “business” ofAG News dataset and the learned category vocabulary.\nLabel Name Category Vocabulary\ngood\ngood, better, really, always, you, well, excellent, very, things, think, way, sure,\nthing, so, n’t, we, lot, get, but, going, kind, know, just, pretty, i, ’ll, certainly, ’re,\nnothing, what, bad, great, best, something, because, doing, got, enough, even, . . .\nbad\nbad, good, things, worse, thing, because, really, too, nothing, unfortunately, awful,\nn’t, pretty, maybe, so, lot, trouble, something, wrong, got, terrible, just, anything,\nkind, going, getting, think, get, ?, you, stuff, ’ve, know, everything, actually, . . .\nTable 8: GloVe 300-d pre-trained embedding for category understanding onAmazon dataset.\ntraining strategy. BERT w. simple matchdoes not\nseem to beneﬁt from self-training as our method\ndoes. This is probably because documents con-\ntaining label names may not be actually about the\ncategory (e.g., the second sentence in Table 1); the\nnoise from simply matching the label names causes\nthe model to make high-conﬁdence wrong predic-\ntions, from which the model struggles to extract\ncorrect classiﬁcation signals for self-improvement.\nThis demonstrates the necessity of creating word-\nlevel supervision by understanding the contextu-\nalized word meaning and training the model via\nMCP to predict the category of words instead of\ndirectly assigning the word’s implied category to\nits document.\n5 Discussions\nThe potential of weakly-supervised classiﬁca-\ntion has not been fully explored. For the sim-\nplicity and clarity of our method, (1) we only use\nthe BERT-base-uncased model rather than more ad-\nvanced and recent LMs; (2) we use at most3 words\nper class as label names; (3) we refrain from using\nother dependencies like back translation systems\nfor augmentation. We believe that the performance\nwill become better with the upgrade of the model,\nthe enrichment in inputs and the usage of data aug-\nmentation techniques.\nApplicability of weak supervision in other NLP\ntasks. Many other NLP problems can be for-\nmulated as classiﬁcation tasks such as named en-\n20 40 60 80 100\n# Labeled Documents / Class\n0.76\n0.80\n0.84\n0.88\nTest Acc.\n(20, 0.822)\n(50, 0.867)\nSup. BERT\nLOTClass\n(a) Supervised BERT: Test acc. vs.\nnumber of labeled documents.\n0 200 400 600 800\nSteps\n0.83\n0.84\n0.85\n0.86\nTest Acc.\n0.00\n0.05\n0.10\n0.15\n0.20\nLoss\nAcc.\nLoss\n(b) LOTClass: Test accuracy and self-\ntraining loss.\n0 200 400 600 800\nSteps\n0.75\n0.80\n0.85\n0.90\nTest Acc.\nLOTClass\nBERT w. simple match\n(c) LOTClass vs. BERT w. sim-\nple match during self-training.\nFigure 2: (On AG News dataset.) (a) The performance of LOTClass is close to that of Supervised BERT with\n48 labeled documents per class. (b) The self-training loss of LOTClass decreases in a period of 50 steps; the\nperformance of LOTClass gradually improves. (c) BERT w. simple match does not beneﬁt from self-training.\ntity recognition and aspect-based sentiment analy-\nsis (Huang et al., 2020). Sometimes a label name\ncould be too generic to interpret ( e.g., “person”,\n“time”, etc). To apply similar methods as intro-\nduced in this paper to these scenarios, one may\nconsider instantiating the label names with more\nconcrete example terms like speciﬁc person names.\nLimitation of weakly-supervised classiﬁcation.\nThere are difﬁcult cases where label names are\nnot sufﬁcient to teach the model for correct classi-\nﬁcation. For example, some review texts implicitly\nexpress sentiment polarity that goes beyond word-\nlevel understanding: “I ﬁnd it sad that just because\nEdward Norton did not want to be in the ﬁlm or\nhave anything to do with it, people automatically\nthink the movie sucks without even watching it or\ngiving it a chance. ”Therefore, it will be interesting\nto improve weakly-supervised classiﬁcation with\nactive learning where the model is allowed to con-\nsult the user about difﬁcult cases.\nCollaboration with semi-supervised classiﬁca-\ntion. One can easily integrate weakly-supervised\nmethods with semi-supervised methods in differ-\nent scenarios: (1) When no training documents\nare available, the high-conﬁdence predictions of\nweakly-supervised methods can be used as ground-\ntruth labels for initializing semi-supervised meth-\nods. (2) When both training documents and la-\nbel names are available, a joint objective can be\ndesigned to train the model with both word-level\ntasks (e.g., MCP) and document-level tasks (e.g.,\naugmentation, self-training).\n6 Conclusions\nIn this paper, we propose the LOTClass model\nbuilt upon pre-trained neural LMs for text classi-\nﬁcation with label names as the only supervision\nin three steps: Category understanding via label\nname replacement, word-level classiﬁcation via\nmasked category prediction, and self-training on\nunlabeled corpus for generalization. The effective-\nness of LOTClass is validated on four benchmark\ndatasets. We show that label names is an effective\nsupervision type for text classiﬁcation but has been\nlargely overlooked by the mainstreams of litera-\nture. We also point out several directions for future\nwork by generalizing our methods to other tasks or\ncombining with other techniques.\nAcknowledgments\nResearch was sponsored in part by US DARPA\nKAIROS Program No. FA8750-19-2-1004 and So-\ncialSim Program No. W911NF-17-C-0099, Na-\ntional Science Foundation IIS 19-56151, IIS 17-\n41317, IIS 17-04532, IIS 16-18481, and III-\n2008334, and DTRA HDTRA11810026. Any opin-\nions, ﬁndings, and conclusions or recommenda-\ntions expressed herein are those of the authors\nand should not be interpreted as necessarily repre-\nsenting the views, either expressed or implied, of\nDARPA or the U.S. Government. The U.S. Gov-\nernment is authorized to reproduce and distribute\nreprints for government purposes notwithstanding\nany copyright annotation hereon. We thank anony-\nmous reviewers for valuable and insightful feed-\nback.\nReferences\nSteven Bird, Ewan Klein, and Edward Loper. 2009.\nNatural language processing with Python: analyz-\ning text with the natural language toolkit. ” O’Reilly\nMedia, Inc.”.\nMing-Wei Chang, Lev-Arie Ratinov, Dan Roth, and\nVivek Srikumar. 2008. Importance of semantic rep-\nresentation: Dataless classiﬁcation. In AAAI.\nJiaao Chen, Zichao Yang, and Diyi Yang. 2020. Mix-\ntext: Linguistically-informed interpolation of hid-\nden space for semi-supervised text classiﬁcation. In\nACL.\nXingyuan Chen, Yunqing Xia, Peng Jin, and John A.\nCarroll. 2015. Dataless text classiﬁcation with de-\nscriptive lda. In AAAI.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT.\nEvgeniy Gabrilovich and Shaul Markovitch. 2007.\nComputing semantic relatedness using wikipedia-\nbased explicit semantic analysis. In IJCAI.\nJiaxin Huang, Yu Meng, Fang Guo, Heng Ji, and Ji-\nawei Han. 2020. Weakly-supervised aspect-based\nsentiment analysis via joint aspect-sentiment topic\nembedding. In EMNLP.\nNitin Jindal and Bing Liu. 2007. Review spam detec-\ntion. In WWW.\nYoon Kim. 2014. Convolutional neural networks for\nsentence classiﬁcation. In EMNLP.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR.\nThomas Kipf and Max Welling. 2017. Semi-\nsupervised classiﬁcation with graph convolutional\nnetworks. In ICLR.\nZhen-Zhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learn-\ning of language representations. In ICLR.\nDong-Hyun Lee. 2013. Pseudo-label : The simple and\nefﬁcient semi-supervised learning method for deep\nneural networks. In Workshop on challenges in rep-\nresentation learning, ICML.\nJens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,\nDimitris Kontokostas, Pablo N. Mendes, Sebastian\nHellmann, Mohamed Morsey, Patrick van Kleef,\nS¨oren Auer, and Christian Bizer. 2015. Dbpedia -\na large-scale, multilingual knowledge base extracted\nfrom wikipedia. Semantic Web, 6:167–195.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2020.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. In ACL.\nChenliang Li, Jian Xing, Aixin Sun, and Zongyang Ma.\n2016. Effective document labeling with very few\nseed words: A topic model approach. In CIKM.\nHongmei Liu, Xiaotong Zhang, Lu Fan, Xuandi Fu,\nQimai Li, Xiao ming Wu, and Albert Y . S. Lam.\n2019a. Reconstructing capsule networks for zero-\nshot intent classiﬁcation. In EMNLP.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv, abs/1907.11692.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn ACL.\nJulian J. McAuley and Jure Leskovec. 2013. Hidden\nfactors and hidden topics: understanding rating di-\nmensions with review text. In RecSys ’13.\nDheeraj Mekala and Jingbo Shang. 2020. Contextu-\nalized weak supervision for text classiﬁcation. In\nACL.\nYu Meng, Jiaxin Huang, Guangyuan Wang, Zihan\nWang, Chao Zhang, Yu Zhang, and Jiawei Han.\n2020a. Discriminative topic mining via category-\nname guided text embedding. In WWW.\nYu Meng, Jiaming Shen, Chao Zhang, and Jiawei Han.\n2018. Weakly-supervised neural text classiﬁcation.\nIn CIKM.\nYu Meng, Jiaming Shen, Chao Zhang, and Jiawei Han.\n2019. Weakly-supervised hierarchical text classiﬁ-\ncation. In AAAI.\nYu Meng, Yunyi Zhang, Jiaxin Huang, Yu Zhang, Chao\nZhang, and Jiawei Han. 2020b. Hierarchical topic\nmining via joint spherical tree and text embedding.\nIn KDD.\nTakeru Miyato, Andrew M. Dai, and Ian J. Goodfel-\nlow. 2017. Adversarial training methods for semi-\nsupervised text classiﬁcation. In ICLR.\nJinseok Nam, Eneldo Loza Menc ´ıa, and Johannes\nF¨urnkranz. 2016. All-in text: Learning document,\nlabel, and word representations jointly. In AAAI.\nBo Pang, Lillian Lee, and Shivakumar Vaithyanathan.\n2002. Thumbs up? sentiment classiﬁcation using\nmachine learning techniques. In EMNLP.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In EMNLP.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In NAACL-HLT.\nFabio Petroni, Tim Rockt ¨aschel, Patrick Lewis, Anton\nBakhtin, Yuxiang Wu, Alexander H. Miller, and Se-\nbastian Riedel. 2019. Language models as knowl-\nedge bases? In EMNLP.\nPushpankar Kumar Pushp and Muktabh Mayank\nSrivastava. 2017. Train once, test anywhere:\nZero-shot learning for text classiﬁcation. ArXiv,\nabs/1712.05972.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100, 000+ questions for\nmachine comprehension of text. In EMNLP.\nAnthony Rios and Ramakanth Kavuluru. 2018. Few-\nshot and zero-shot multi-label learning for structured\nlabel spaces. In EMNLP.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving neural machine translation models\nwith monolingual data. In ACL.\nYangqiu Song and Dan Roth. 2014. On dataless hierar-\nchical text classiﬁcation. In AAAI.\nDuyu Tang, Bing Qin, and Ting Liu. 2015a. Docu-\nment modeling with gated recurrent neural network\nfor sentiment classiﬁcation. In EMNLP.\nJian Tang, Meng Qu, and Qiaozhu Mei. 2015b. Pte:\nPredictive text embedding through large-scale het-\nerogeneous text networks. In KDD.\nJian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun\nYan, and Qiaozhu Mei. 2015c. Line: Large-scale\ninformation network embedding. In WWW.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R. Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R. Bowman, Dipan-\njan Das, and Ellie Pavlick. 2019. What do you learn\nfrom context? probing for sentence structure in con-\ntextualized word representations. In ICLR.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS.\nCongying Xia, Chenwei Zhang, Xiaohui Yan,\nYi Chang, and Philip S. Yu. 2018. Zero-shot user\nintent detection via capsule neural networks. In\nEMNLP.\nJunyuan Xie, Ross B. Girshick, and Ali Farhadi. 2016.\nUnsupervised deep embedding for clustering analy-\nsis. In ICML.\nQizhe Xie, Zihang Dai, Eduard H. Hovy, Minh-Thang\nLuong, and Quoc V . Le. 2019. Unsupervised data\naugmentation. ArXiv, abs/1904.12848.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In NeurIPS.\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,\nAlexander J. Smola, and Eduard H. Hovy. 2016. Hi-\nerarchical attention networks for document classiﬁ-\ncation. In NAACL-HLT.\nWenpeng Yin, Jamaal Hay, and Dan Roth. 2019.\nBenchmarking zero-shot text classiﬁcation:\nDatasets, evaluation and entailment approach.\nIn EMNLP.\nJingqing Zhang, Piyawat Lertvittayakumjorn, and Yike\nGuo. 2019. Integrating semantic knowledge to\ntackle zero-shot text classiﬁcation. In NAACL-HLT.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In NIPS.\nYu Zhang, Yu Meng, Jiaxin Huang, Frank F. Xu, Xuan\nWang, and Jiawei Han. 2020. Minimally supervised\ncategorization of text with metadata. In SIGIR.\nA Label Names Used and Category\nVocabulary Obtained for DBPedia\nWe show the label names used forDBPedia corpora\nand the obtained category vocabulary in Table 9. In\nmost cases, only one word as the label name will\nbe sufﬁcient; however, sometimes the semantics\nof the label name might be too general so we in-\nstead use 2 or 3 keywords of the class to represent\nthe label name. For example, we use “school” and\n“university” to represent the class “educational insti-\ntution”; we use “river”, “lake” and “mountain” to\nrepresent the class “natural place”; we use “book”,\n“novel” and “publication” to represent the class “pa-\nper work”.\nLabel Name Category Vocabulary\ncompany\ncompanies, co, ﬁrm, concern, subsidiary, brand, enterprise, division, partnership,\nmanufacturer, works, inc, cooperative, provider, corp, factory, chain, limited,\nholding, consortium, industry, manufacturing, entity, operator, product, giant . . .\nschool\nuniversity\nacademy, college, schools, ecole, institution, campus, university, secondary,\nform, students, schooling, standard, class, educate, elementary, hs, level,\nstudent, tech, academic, universities, branch, degree, universite, universidad, . . .\nartist\nartists, painter, artistic, musician, singer, arts, poet, designer, sculptor, composer,\nstar, vocalist, illustrator, architect, songwriter, entertainer, cm, painting,\ncartoonist, creator, talent, style, identity, creative, duo, editor, personality, . . .\nathlete\nathletes, athletics, indoor, olympian, archer, events, sprinter, medalist, olympic,\nrunner, jumper, swimmer, competitor, holder, mile, ultra, able, mark, hurdles,\nrelay, amateur, medallist, footballer, anchor, metres, cyclist, shooter, athletic, . . .\npolitics\npolitics, political, government, politicians, politician, elections, policy, party,\naffairs, legislature, politically, democracy, democratic, governing, history,\nleadership, cabinet, issues, strategy, election, religion, assembly, law, . . .\ntransportation\ntransportation, transport, transit, rail, travel, trafﬁc, mobility, bus, energy,\nrailroad, communication, route, transfer, passenger, transported, traction,\nrecreation, metro, shipping, railway, security, transports, infrastructure, . . .\nbuilding\nbuildings, structure, tower, built, wing, hotel, build, structures, room,\ncourthouse, skyscraper, library, venue, warehouse, block, auditorium, location,\nplaza, addition, museum, pavilion, landmark, ofﬁces, foundation, headquarters, . . .\nriver\nlake\nmountain\nriver, lake, bay, dam, rivers, water, creek, channel, sea, pool, mountain,\nstream, lakes, ﬂow, reservoir, hill, ﬂowing, mountains, basin, great, glacier,\nﬂowed, pond, de, valley, peak, drainage, mount, summit, brook, mare, head, . . .\nvillage\nvillage, villages, settlement, town, east, population, rural, municipality, parish,\nna, temple, commune, pa, ha, north, pre, hamlet, chamber, settlements, camp,\nadministrative, lies, township, neighbourhood, se, os, iran, villagers, nest, . . .\nanimal\nanimal, animals, ape, horse, dog, cat, livestock, wildlife, nature, lion, human,\nowl, cattle, cow, wild, indian, environment, pig, elephant, fauna, mammal,\nbeast, creature, australian, ox, land, alligator, eagle, endangered, mammals, . . .\nplant\ntree\nshrub, plants, native, rose, grass, herb, species, jasmine, race, vine, hybrid,\nbamboo, hair, planted, ﬁre, growing, ﬂame, lotus, sage, iris, perennial, variety,\npalm, cactus, trees, robert, weed, nonsense, given, another, stand, holly, poppy, . . .\nalbum\nlp, albums, cd, ep, effort, recording, disc, compilation, debut, appearance,\nsoundtrack, output, genus, installation, recorded, anthology, earth, issue, imprint, ex,\nera, opera, estate, single, outing, arc, instrumental, audio, el, song, offering, . . .\nﬁlm\nﬁlms, comedy, drama, directed, documentary, video, language, pictures,\nminiseries, negative, movies, musical, screen, trailer, acting, starring, ﬁlmmaker,\nﬂick, horror, silent, screenplay, box, lead, ﬁlmmaking, second, bond, script, . . .\nbook\nnovel\npublication\nnovel, books, novels, mystery, memoir, fantasy, ﬁction, novelist, reader, read, cycle,\nromance, writing, written, published, novella, play, narrative, trilogy, manga,\nautobiography, publication, literature, isbn, write, tale, poem, year, text, reading, . . .\nTable 9: The label name used for each class of DBPedia dataset and the learned category vocabulary."
}