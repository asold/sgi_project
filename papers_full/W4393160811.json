{
  "title": "Bootstrapping Large Language Models for Radiology Report Generation",
  "url": "https://openalex.org/W4393160811",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A209039771",
      "name": "Chang Liu",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2970383525",
      "name": "Yuanhe Tian",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2012784458",
      "name": "Weidong Chen",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2103229335",
      "name": "Yan Song",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2152909678",
      "name": "Yongdong Zhang",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2745461083",
    "https://openalex.org/W3173688449",
    "https://openalex.org/W3096799362",
    "https://openalex.org/W2152772232",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3104027471",
    "https://openalex.org/W6784955093",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2964195337",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W3174714208",
    "https://openalex.org/W3166142651",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W4285108627",
    "https://openalex.org/W3116079511",
    "https://openalex.org/W4366327761",
    "https://openalex.org/W3107083523",
    "https://openalex.org/W4296122936",
    "https://openalex.org/W4307006119",
    "https://openalex.org/W2970601365",
    "https://openalex.org/W4318718936",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4250244075",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2963466845",
    "https://openalex.org/W4287120153",
    "https://openalex.org/W4380715596",
    "https://openalex.org/W4385573131",
    "https://openalex.org/W4388651010",
    "https://openalex.org/W4389072945",
    "https://openalex.org/W4386076522",
    "https://openalex.org/W4392044798",
    "https://openalex.org/W4394659038",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W3098325931",
    "https://openalex.org/W4366330503",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4366850747",
    "https://openalex.org/W4388482211",
    "https://openalex.org/W2575842049",
    "https://openalex.org/W4304206659",
    "https://openalex.org/W2803411968",
    "https://openalex.org/W4386003857",
    "https://openalex.org/W4307073150",
    "https://openalex.org/W3104609094",
    "https://openalex.org/W2560313346",
    "https://openalex.org/W4386065580",
    "https://openalex.org/W2116492146",
    "https://openalex.org/W3177048142"
  ],
  "abstract": "Radiology report generation (RRG) aims to automatically generate a free-text description from a specific clinical radiograph, e.g., chest X-Ray images. Existing approaches tend to perform RRG with specific models trained on the public yet limited data from scratch, where they often lead to inferior performance owing to the problem of inefficient capabilities in both aligning visual and textual features and generating informative reports accordingly. Currently, large language models (LLMs) offered a promising solution to text generation with their power in learning from big data, especially for cross-modal scenarios such as RRG. However, most existing LLMs are pre-trained on general data, and suffer from the same problem of conventional approaches caused by knowledge gap between general and medical domain if they are applied to RRG. Therefore in this paper, we propose an approach to bootstrapping LLMs for RRG with a in-domain instance induction and a coarse-to-fine decoding process. Specifically, the in-domain instance induction process learns to align the LLM to radiology reports from general texts through contrastive learning. The coarse-to-fine decoding performs a text elevating process for those reports from the ranker, further enhanced with visual features and refinement prompts. Experimental results on two prevailing RRG datasets, namely, IU X-Ray and MIMIC-CXR, demonstrate the superiority of our approach to previous state-of-the-art solutions. Further analyses illustrate that, for the LLM, the induction process enables it to better align with the medical domain and the coarse-to-fine generation allows it to conduct more precise text generation.",
  "full_text": "Bootstrapping Large Language Models for Radiology Report Generation\nChang Liu1, Yuanhe Tian2, Weidong Chen1, Yan Song1∗, Yongdong Zhang1\n1University of Science and Technology of China\n2University of Washington\nlc980413@mail.ustc.edu.cn, yhtian@uw.edu, chenweidong@ustc.edu.cn,\nclksong@gmail.com, zhyd73@ustc.edu.cn\nAbstract\nRadiology report generation (RRG) aims to automatically\ngenerate a free-text description from a specific clinical radio-\ngraph, e.g., chest X-Ray images. Existing approaches tend\nto perform RRG with specific models trained on the public\nyet limited data from scratch, where they often lead to infe-\nrior performance owing to the problem of inefficient capa-\nbilities in both aligning visual and textual features and gen-\nerating informative reports accordingly. Currently, large lan-\nguage models (LLMs) offered a promising solution to text\ngeneration with their power in learning from big data, es-\npecially for cross-modal scenarios such as RRG. However,\nmost existing LLMs are pre-trained on general data, and suf-\nfer from the same problem of conventional approaches caused\nby knowledge gap between general and medical domain if\nthey are applied to RRG. Therefore in this paper, we pro-\npose an approach to bootstrapping LLMs for RRG with a\nin-domain instance induction and a coarse-to-fine decoding\nprocess. Specifically, the in-domain instance induction pro-\ncess learns to align the LLM to radiology reports from general\ntexts through contrastive learning. The coarse-to-fine decod-\ning performs a text elevating process for those reports from\nthe ranker, further enhanced with visual features and refine-\nment prompts. Experimental results on two prevailing RRG\ndatasets, namely, IU X-Ray and MIMIC-CXR, demonstrate\nthe superiority of our approach to previous state-of-the-art\nsolutions. Further analyses illustrate that, for the LLM, the\ninduction process enables it to better align with the medical\ndomain and the coarse-to-fine generation allows it to conduct\nmore precise text generation.\n1 Introduction\nMedical imaging plays an important role in clinical di-\nagnosis and treatment recommendation, where physicians\nnormally write reports according to the syndromes of pa-\ntients that are reflected in images so as to form profes-\nsional records. As a special type of medical images, radio-\ngraphs are essential in evaluating patients’ medical condi-\ntion with analyzing internal structures of their bodies, and\nhave been widely used in orthopedics, dentistry, cardiology,\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n*Corresponding author.\nCode and models of our approach are available at https://\ngithub.com/synlp/R2-LLM.\nand pulmonology, etc. Yet, writing radiology reports is a\ntime-consuming job, and always error-prone for inexperi-\nenced radiologists, which motivate a series of studies (Jing,\nXie, and Xing 2018; Chen et al. 2020, 2021; Qin and Song\n2022; Tanida et al. 2023; Liu, Tian, and Song 2023) on auto-\nmatic reports generation. They have achieved great success\non this topic that has emerged as an attractive research di-\nrection in both artificial intelligence and clinical medicine.\nAforementioned approaches for radiology report gener-\nation (RRG) employ the encoder-decoder architecture and\nmainly focus on improving the capabilities of cross-modal\nalignment and text generation, which are fundamentally re-\nstricted when they are learned from a rather small and fixed\nset of radiograph-report pairs. Consider recent large lan-\nguage models (LLMs) illustrate their superiority in gener-\nating high-quality text with few examples, it is expected\nto apply LLMs in RRG by fine-tuning with limited data. 1\nHowever, in doing so, one faces a challenging barrier of do-\nmain variance since ready-to-use LLMs are often pre-trained\non general data, which causes a series of problems such as\nill-representing visual and textual features from in-domain\ndata, generating texts without domain characteristics, etc.\nTherefore, domain adaptation and refined text generation are\nexpected on LLMs for in-domain applications, thus requir-\ning particular LLM optimization processes for RRG.\nIn this paper, we propose an approach to bootstrapping\nLLM for RRG, with two components designed for domain\nadaptation and task-specific generation, namely, in-domain\ninstance induction and coarse-to-fine decoding. Specifically,\nin-domain instance induction adapts the LLM with learn-\ning on radiology reports-alike data, equipped with two parts,\nrelated instance retrieval and contrastive semantic ranking.\nRelated instance retrieval provides a series of reports with\nranked semantic relations to the input radiograph, and these\nreports are used in contrastive semantic ranking as related\ninstances, comparing with less correlated instances from\nother medical sources. The LLM in this induction process\nis then learned to generate reports similar to high-ranked in-\nstances than lower ones, so as to fast align with in-domain\nand task-specific data. The coarse-to-fine decoding process\n1In practice, fine-tuning LLMs for in-domain tasks still requires\na rather large amount of labeled data for better performance when\nLLMs are not pre-trained on the data from particular domains.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18635\nRelated Instance Retrieval (RIR) Contrastive Semantic Ranking (CSR)\nIn-domain Instance Induction (I3)\nRelated Instance Retrieval (RIR) Contrastive Semantic Ranking (CSR)\nIn-domain Instance Induction (I3)\nVisual EncodingVisual Encoding Coarse-to-Fine Decoding (C2FD)Coarse-to-Fine Decoding (C2FD)\nImage\nRefinement Prompt\nMiniGPT-4\nText\nGenerator\nMiniGPT-4\nText\nGenerator\nMiniGPT-4\nText\nGenerator\n“The heart size is midly enlarged. No pleural \neffusion or pneumothorax is seen. There are no \nacute bony findings...”\nGeneration Prompt\nImage Encoder\nText Encoder\nTraining\n Data\nFinal Report\nPublic\nMedical Data\nSampling “The heart is normal. \nThere is a small left \npleural effusion...”\nMiniGPT-4\nText\nGenerator\nMiniGPT-4\nText\nGenerator\nMiniGPT-4\nText\nGenerator\nIntermediate Report\nMiniGPT-4\nVisual\nEncoder\nMiniGPT-4\nVisual\nEncoder\nUnrelated Instances Unrelated Instances \n...\nReference InstancesReference Instances\nRanked Instances\nRetrieve\nRetrieve\nSampling\nFigure 1: The overall architecture of our proposed approach based on MiniGPT-4 for RRG. The approach consists of three\nmain components, namely, visual encoding, in-domain instance induction (I3), and coarse-to-fine decoding (C2FD), which are\nrepresented in red, yellow, and green backgrounds, respectively. The dashed blue box cross visual encoding and C2FD refers to\nMiniGPT-4. The dashed orange box in the left part of in-domain instance induction stands for the toolkit to retrieve instances\nfrom different sources, where the darker colored instances in the output are more related to the input radiograph than the lighter\nones. Note that the text generator (LLM) of MiniGPT-4 is shared in I3 and C2FD, indicated by a gray dashed line.\noptimizes the LLM to refine intermediate reports generated\nfrom the induction process to the precise final reports. Ex-\nperimental results and analyses on two RRG benchmark\ndatasets, i.e., IU X-R AY (Demner-Fushman et al. 2016) and\nMIMIC-CXR (Johnson et al. 2019) demonstrate the supe-\nriority of our approach, which outperforms strong baselines\nand achieves state-of-the-art performance on both datasets.\n2 The Approach\nThe overall architecture built upon MiniGPT-4 (Zhu et al.\n2023)2 is illustrated in Figure 1, with three main compo-\nnents, namely, visual encoding, in-domain instance induc-\ntion (I3), and coarse-to-fine decoding (C2FD).\nThe visual encoding process employs an encoderfve (i.e.,\nMiniGPT-4 visual encoder) to extract latent representations\nof an input radiograph I. Then, the in-domain instance in-\nduction process uses a text generator ftg (i.e., MiniGPT-4\ntext generator, namely, Vicuna (Chiang et al. 2023)) to gen-\nerate intermediate reports based on the resulted radiograph\nrepresentation from the visual encoder and a generation\nprompt pg, where in training stage, related and unrelated\nreports are retrieved and sampled to provide a set of ranked\ninstances, for the induction process to update the generator\nwith domain-specific knowledge. Finally, the coarse-to-fine\ndecoding process uses the same text generatorftg to produce\n2LLaV A (Liu et al. 2023) is also replaceable and claims similar\nresults in our experiments. We choose MiniGPT-4 because it more\nfocuses on generation tasks that are more relevant to the RRG task,\nwhereas LLaV A is specialized in vision-language understanding\ntasks according to how they are applied in their original settings.\nthe final report bY based on the resulted intermediate reports,\nalong with the radiograph representations and a refinement\nprompt pr. Thus, RRG in our approach is formalized as\nbY = ftg (fve(I), pr, ftg (fve(I), pg)) (1)\nwhere bY is the final report and I is the input radiograph. In\ntraining, the model is optimized based on the in-domain in-\nstance induction loss LI3 and the cross-entropy loss LC2FD\nfrom the generated final reports bY and the gold standard Y∗,\ntherefore resulting the final loss L as\nL = β1LI3 + β2LC2FD (2)\nwhere β1 and β2 are hyper-parameters to balance the contri-\nbution of the losses. In the following text, we introduce each\ncomponent according to the aforementioned processing se-\nquence in details, including visual encoding, in-domain in-\nstance induction, and coarse-to-fine decoding, respectively.\n2.1 Visual Encoding\nThe MiniGPT-4 visual encoder fve consists of three mod-\nules, namely, the vision transformer fv (Dosovitskiy et al.\n2021), the Q-Former fq (Li et al. 2023), and a linear pro-\njection layer, formulated as fve(·) = Linear(fq(fv(·))).\nTherefore, radiographs are firstly fed into the vision trans-\nformer model fv, with their features hv extracted through\nhv = fv(I) (3)\nand are further processed by the Q-Former fq to transfer vi-\nsual representations into textual semantic space by\nhq = fq(hv) (4)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18636\nwhere hq represents the output features. Finally, a linear pro-\njection layer is used to further project hq into the latent rep-\nresentations v to align hq to the dimension of hidden states\nin the text generator through\nv = Linear(hq) (5)\nwhere v is used in I3 and C2FD as visual features to guide\nthe generation process of intermediate and final reports.\n2.2 In-domain Instance Induction\nOnce the visual features are extracted, the next step is to gen-\nerate a corresponding radiology report through a text gener-\nator, i.e., an LLM in our approach. In doing so, it still strug-\ngles to produce highly patternized radiology reports with\nprofessional medical terminology if the generator is trained\nin the general domain, where misalignment occurs between\nthe general and medical semantic spaces. However, domain\nadaptation for LLMs is not a trivial task, especially when in-\ndomain training data is limited. Inspired by studies (Ouyang\net al. 2022; Bai et al. 2022; Touvron et al. 2023a) that op-\ntimize LLMs by learning from ranked texts, we propose a\nnovel method, in-domain instance induction, to bootstrap\nLLMs with effective domain adaptation based on two se-\nquentially connected components, namely, related instance\nretrieval and contrastive semantic ranking, whose details are\npresented in the following texts.\nRelated Instance Retrieval (RIR) The first component\nserves as the data provider that offers a series of in-domain\nreferences for LLM domain adaptation. In RIR, we retrieve\nin-domain data from two sources, namely, the training data\nof RRG and public medical corpora, to provide task-specific\nand ranking-support references, respectively, where each\nsource contributes M instances (2M instances in total). This\ntwo-source design allows the text generator to not only learn\ndomain information by being optimized on in-domain data,\nbut also be equipped with task-specific information by learn-\ning to distinguish related instances from others. Specifically,\nin the retrieval process from the training data, we randomly\nsample a set of radiology reports R = {R1, ··· , RN } with\na retrieval size of N. Next, we utilize an image encoder f′\nve\nand its corresponding text encoder f′\nte from an off-the-shelf\ntoolkit (i.e., MedCLIP (Wang et al. 2022b)) to project the\ninput radiograph I and the sampled reports R into the same\nsemantic space through\nvr = f′\nve(I) (6)\nand\n{u1, ··· , uN } = f′\nte(R1, ··· , RN ) (7)\nwhere vr and {u1, ··· , uN } are visual and text representa-\ntions, respectively. Afterwards, for each text representation\nui, we compute its cosine similarity score ci with vr and\nselect the top M ones based on ci, then retrieve their cor-\nresponding reports from the training data. For the rest M\ninstances from public medical corpora, we randomly sam-\nple them out and compute their cosine similarity scores with\nvr following the same process as that we do for ci. Fi-\nnally, we merge the two aforementioned instance lists into\nS = [ S1 ··· S2M ], with the first M instances from public\nmedical corpora, and the rest M ones from the training data.\nDATASET\nIU X-RAY MIMIC-CXR\nTRAIN VAL TEST TRAIN VAL TEST\nIMAGE 5.2K 0.7K 1.5K 369.0K 3.0K 5.2K\nREPORT 2.8K 0.4K 0.8K 222.8K 1.8K 3.3K\nPATIENT 2.8K 0.4K 0.8K 64.6K 0.5K 0.3K\nAVG. LEN. 37.6 36.8 33.6 53.0 53.1 66.4\nTable 1: The statistics of the two benchmark datasets w.r.t.\ntheir training, validation, and test sets, including the num-\nbers of images, reports, and patients, and the averaged word-\nbased length (AVG. LEN.) of all reports in each category.\nContrastive Semantic Ranking (CSR)With the retrieved\ninstance list S, LLM learns to rank them in a contrastive\nmanner by learning over and comparing the representations\nof LLM output bZ and S. Firstly, by using the visual fea-\nture v of the input radiograph obtained from Eq. (5) and the\ngeneration prompt pg, we generate the intermediate report\nbZ = bz1 ··· bzNz with Nz tokens through\nbZ = ftg(v, pg) (8)\nwhere the representation on of the n-th token bzn is extracted\nfrom the last layer3 of ftg by\non = ftg(v, pg; bz1 ··· bzn−1) (9)\nThen, we compute the mean pooling of all on and use the\nresulting vector o to represent bZ. For each instance Sm in\nS, we perform a similar process as that we do forbZ to obtain\nits representation o′\nm, with the representation of each token\nin Sm computed through\no′\nm,n = ftg(v, pg; sm,1 ··· sm,n−1) (10)\nwhere sm,1 ··· sm,n−1 are the n-1 tokens prior to the current\ntoken as that we do in Eq. (9). Afterwards, to facilitate the\nlearning process in a contrastive manner, we construct M\ninstance pairs SP = [(S1, S1+M ), ··· , (SM , S2M )], where\nin each pair, the former comes from public medical corpora\nand the latter from the training data, which guarantees con-\ntrast between unrelated and related instances in each pair.\nFinally, we compute LI3 through a pairwise optimization\nLI3 = 1\nM\nMX\nm=1\nh\n∥o − o′\nm+M ∥ − ∥o − o′\nm∥ + α\ni\n(11)\nwhere ∥ · ∥computes the Euclidean norm of a vector, α is a\npositive real number that controls the margin. For further ex-\nplanation, Eq. (11), ftg learns to update the LLM by gener-\nating intermediate reports that are closer to the high-ranking\ninstance Sm+M in every pair (S m, Sm+M ), so that ftg is\ngradually aligned to both the medical domain as well as the\nreports related to the RRG task.\n3For the sake of simplicity, in Eq. (9) and (10), we still use ftg\nto represent representation computation, which is actually different\nfrom the generation since the last linear projection layer is omitted.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18637\nDATA MODEL NLG METRICS CE METRICS\nBL-1 BL-2 BL-3 BL-4 MTR RG-L A VG. ∆ P R F1\nIU\nX-R AY\nMINI GPT-4 0.207 0.068 0.022 0.007 0.091 0.161 - - - -\n+ I3 0.277 0.163 0.122 0.083 0.107 0.181 47.2% - - -\n+ C2FD 0.263 0.154 0.113 0.075 0.101 0.169 28.6% - - -\n+ I3+C2FD ( THIS WORK ) 0.326∗ 0.204∗ 0.154∗ 0.119∗ 0.132∗ 0.234∗ 57.5% - - -\nMINI GPT-4 (FT) 0.389 0.262 0.181 0.134 0.169 0.308 - - - -\n+ I3 0.458 0.296 0.210 0.158 0.183 0.357 12.8% - - -\n+ C2FD 0.437 0.289 0.205 0.151 0.178 0.336 9.4% - - -\n+ I3+C2FD ( THIS WORK ) 0.499∗ 0.323∗ 0.238∗ 0.184∗ 0.208∗ 0.390∗ 22.0% - - -\nMIMIC\n-CXR\nMINI GPT-4 0.135 0.048 0.014 0.003 0.064 0.104 - 0.126 0.103 0.113\n+ I3 0.212 0.120 0.075 0.031 0.106 0.175 58.2% 0.157 0.113 0.131\n+ C2FD 0.178 0.084 0.051 0.017 0.097 0.163 48.7% 0.144 0.109 0.124\n+ I3+C2FD ( THIS WORK ) 0.266∗ 0.152∗ 0.093∗ 0.060∗ 0.109∗ 0.210∗ 64.9% 0.228∗ 0.147∗ 0.179∗\nMINI GPT-4 (FT) 0.323 0.184 0.110 0.069 0.122 0.220 - 0.335 0.246 0.284\n+ I3 0.358 0.203 0.137 0.091 0.139 0.256 14.9% 0.383 0.279 0.323\n+ C2FD 0.345 0.191 0.128 0.079 0.134 0.243 9.2% 0.354 0.251 0.294\n+ I3+C2FD ( THIS WORK ) 0.402∗ 0.262∗ 0.180∗ 0.128∗ 0.175∗ 0.291∗ 31.5% 0.465∗ 0.482∗ 0.473∗\nTable 2: NLG and CE evaluations of different models on the test sets of IU X-R AY and MIMIC-CXR datasets. “MiniGPT-\n4 (FT)” denotes the model with MiniGPT-4 pre-trained on MIMIC-CXR training data. “BL” is the abbreviation of BLEU;\n“MTR” and “RG-L” denote METEOR and ROUGE-L, respectively. The average improvements over all NLG metrics compared\nto MiniGPT-4 and MiniGPT-4 (FT) correspondingly are also presented in the “A VG. ∆” column, respectively. “∗” marks the\nresults where the improvements are statistically significant over all baselines at p ≤ 0.05 level.\n2.3 Coarse-to-Fine Decoding\nAlthough I3 offers a rather strong learning process to adapt\nLLM to the medical domain, it is still limited for the LLM to\ngenerate precise reports for the RRG task, since only coarse\nimage-text alignment and low-level text references are pro-\nvided. To further facilitate the generation ability of LLM for\nRRG, we propose to enhance the LLM with a coarse-to-fine\ndecoding process. Specifically, the text generator 4 (i.e., the\nLLM, which is the same as the one in I3) takes the visual\nrepresentation v, the refinement prompt pr, and the inter-\nmediate report bZ to generate the final report bY by\nbY = ftg(v, pr; bZ) (12)\nAfterwards, we compute the cross-entropy loss LC2FD\nbased on the generated and the gold standard reports for each\ninput radiograph and combine it with LI3 based on Eq. (2)\nto jointly optimize the LLM accordingly during training.\n3 Experiment Settings\n3.1 Datasets\nWe conduct our experiments on two conventional bench-\nmark datasets, i.e., IU X-R AY (Demner-Fushman et al.\n2016) from Indiana University and MIMIC-CXR (Johnson\net al. 2019) from the Beth Israel Deaconess Medical Cen-\nter. The former dataset is relatively small with 7,470 chest\nX-Ray images and 3,955 radiology reports. The latter one\n4It is worth noting that, the text generator learns to generate in-\ntermediate and final reports with different inputs. Compared with\nintermediate report generation, final report generation has an addi-\ntional input (i.e., the intermediate report bZ) and a different prompt.\nis the largest public radiology dataset with 473,057 chest\nX-Ray images and 206,563 reports. We follow the experi-\nmental setup of previous studies (Li et al. 2018; Chen et al.\n2020, 2021; Qin and Song 2022) by selecting findings sec-\ntions and excluding samples without such sections for both\ndatasets. We follow the dataset split in Li et al. (2018) for\nIU X-R AY and the official split of MIMIC-CXR. Table 1\nreports the statistics of all datasets in terms of the numbers\nof radiographs, reports, patients, and average report length\naccording to each split of the datasets.\n3.2 Baselines and Evaluation Metrics\nTo verify our proposed approach, we try three baselines: the\nfirst uses the visual encoder and text generator to directly\ngenerate final reports, which is equivalent to the standard\nMiniGPT-4; the second and third baselines add I3 and C2FD\non top of the first baseline, denoted as “+I3” and “+C2FD”,\nrespectively. Note that we have two groups of such baselines\naccording to whether MiniGPT-4 is fine-tuned on RRG data,\nwith details of its fine-tune illustrated in the next subsection.\nFollowing previous studies (Chen et al. 2020, 2021),\nwe evaluate different approaches with two types of assess-\nments, namely, natural language generation (NLG) metrics\nand clinical efficacy (CE) metrics. NLG metrics consist of\nBLEU (Papineni et al. 2002), METEOR (Denkowski and\nLavie 2011) and ROUGE-L (Lin 2004). For CE metrics,\nCheXpert (Irvin et al. 2019) is utilized to label the gener-\nated reports and compare the results with ground truths in 14\ndifferent categories related to thoracic diseases and support\ndevices, with precision, recall, and F1 used for evaluation.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18638\nDATA MODEL NLG METRICS CE METRICS\nBL-1 BL-2 BL-3 BL-4 MTR RG-L P R F1\nIU\nX-R AY\nST (Vinyals et al. 2015) 0.216 0.124 0.087 0.066 - 0.306 - - -\nATT2IN (Rennie et al. 2017) 0.224 0.129 0.089 0.068 - 0.308 - - -\nADAATT (Lu et al. 2017) 0.220 0.127 0.089 0.068 - 0.308 - - -\nCOATT (Jing, Xie, and Xing 2018) 0.455 0.288 0.205 0.154 - 0.369 - - -\nHRGR (Li et al. 2018) 0.438 0.298 0.208 0.151 - 0.322 - - -\nCMAS -RL (Jing, Wang, and Xing 2019) 0.464 0.301 0.210 0.154 - 0.362 - - -\nR2G EN (Chen et al. 2020) 0.470 0.304 0.219 0.165 - 0.371 - - -\nCA (Liu et al. 2021b) 0.492 0.314 0.222 0.169 0.193 0.381 - - -\nCMCL (Liu, Ge, and Wu 2021) 0.473 0.305 0.217 0.162 0.186 0.378 - - -\nPPKED (Liu et al. 2021a) 0.483 0.315 0.224 0.168 - 0.376 - - -\nR2G ENCMN (Chen et al. 2021) 0.475 0.309 0.222 0.170 0.191 0.375 - - -\nR2G ENRL (Qin and Song 2022) 0.494 0.321 0.235 0.181 0.201 0.384 - - -\nXRAYGPT (7B) (Thawkar et al. 2023) 0.177 0.104 0.047 0.007 0.105 0.203 - - -\nOURS (14.2B) 0.499∗ 0.323∗ 0.238∗ 0.184∗ 0.208∗ 0.390∗ - - -\nMIMIC\n-CXR\nST (Vinyals et al. 2015) 0.299 0.184 0.121 0.084 0.124 0.263 0.249 0.203 0.204\nATT2IN (Rennie et al. 2017) 0.325 0.203 0.136 0.096 0.134 0.276 0.322 0.239 0.249\nADAATT (Lu et al. 2017) 0.299 0.185 0.124 0.088 0.118 0.266 0.268 0.186 0.181\nTOPDOWN (Anderson et al. 2018) 0.317 0.195 0.130 0.092 0.128 0.267 0.320 0.231 0.238\nR2G EN (Chen et al. 2020) 0.353 0.218 0.145 0.103 0.142 0.277 0.333 0.273 0.276\nCA (Liu et al. 2021b) 0.350 0.219 0.152 0.109 0.151 0.283 - - -\nCMCL (Liu, Ge, and Wu 2021) 0.344 0.217 0.140 0.097 0.133 0.281 - - -\nPPKED (Liu et al. 2021a) 0.360 0.224 0.149 0.106 0.149 0.284 - - -\nR2G ENCMN (Chen et al. 2021) 0.353 0.218 0.148 0.106 0.142 0.278 0.334 0.275 0.278\nR2G ENRL (Qin and Song 2022) 0.381 0.232 0.155 0.109 0.151 0.287 0.342 0.294 0.292\nITA (Wang et al. 2022a) 0.395 0.253 0.170 0.121 0.147 0.284 - - -\nWARM START (Aaron Nicolson 2022) 0.392 0.245 0.169 0.124 0.153 0.285 0.359 0.412 0.384\nRGRG (Tanida et al. 2023) 0.373 0.249 0.175 0.126 0.168 0.264 0.461 0.475 0.447\nXRAYGPT (7B) (Thawkar et al. 2023) 0.128 0.045 0.014 0.004 0.079 0.111 - - -\nMED-PALM (562B) (Tu et al. 2023) 0.317 - - 0.115 - 0.275 - - 0.378\nOURS (14.2B) 0.402∗ 0.262∗ 0.180∗ 0.128∗ 0.175∗ 0.291∗ 0.465∗ 0.482∗ 0.473∗\nTable 3: Comparisons of our approach with previous studies on the test sets of IU X-R AY and MIMIC-CXR with respect to\nNLG and CE metrics. The best and the second-best results are highlighted in boldface and underlines, respectively. For LLM-\nbased methods (i.e., X RAYGPT, M ED-PALM, and O URS ), we illustrate the number of parameters in parentheses. “ ∗” marks\nthe results where the improvements are statistically significant over all baselines at p ≤ 0.05 level.\n3.3 Implementation Details\nWe use MiniGPT-4 with its default hyper-parameter settings\n(i.e., ViT-G version of vision transformer from EV A-CLIP\n(Fang et al. 2022) with 40 encoding layers, Q-Former (Li\net al. 2023) with 12 layers, a linear projection layer corre-\nsponding to Eq. (5), and Vicuna (Chiang et al. 2023) (13B)\nas the text generator with its default 40 transformer lay-\ners). In order to obtain an enhanced baseline, we fine-tune\nMiniGPT-4 on the training set of public radiology bench-\nmark MIMIC-CXR following the standard fine-tuning pro-\ncess, with the resulted model marked as MiniGPT-4 (FT) in\nfollowing texts. For the public medical data used in I3, we\nrandomly sampled 3, 000 medical documents from PubMed\ndataset5. In addition, motivated by the practice of optimiz-\ning LLaMA-2 (Touvron et al. 2023b) by learning to rank\ninstances in a binary form, we only retrieve a small number6\nof related instances in I3, i.e., M = 3, which refers to that\nthere are three instances retrieved from the training set and\n5https://pubmed.ncbi.nlm.nih.gov/pubmed\n6We try M ∈ [1, 5] and adopt M = 3 with the best results.\nthree from the public medical corpora, respectively. For our\nfull model, we train them on the training set of IU X-R AY\nand MIMIC-CXR with different hyper-parameter settings\nand use the one with the highest performance on the valida-\ntion set. The batch sizes for IU X-R AY and MIMIC-CXR\nare set to 12. The weights to balance I3 and C2FD loss in\nEq. (2) are set to β1 = 1 and β2 = 1, respectively. In train-\ning, we only update parameters in the linear projection layer\nin the visual encoder and Vicuna through AdamW (Kingma\nand Ba 2015) with learning rate set to1×10−6. Note that in\ninference, we only use visual encoding, intermediate report\ngeneration in I3 without RIR and other parts of CSR, and\nC2FD, to generate reports.\n4 Results and Analysis\n4.1 Overall Results\nExperiment results of different models on the two bench-\nmark datasets are reported in Table 2, with several observa-\ntions. First, under different settings with original MiniGPT-\n4 and its fine-tuned version, our approach with I3 (i.e.,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18639\nNL\nG METRICS\nBL\n-1 BL-2 BL-3 BL-4 MTR RG-L\nI3\n(TD) 0.339 0.182\n0.120 0.084 0.126 0.238\nI3 (PMD) 0.310 0.167\n0.112 0.074 0.109 0.215\nTable 4: I3 performance with only using training data (TD)\nor public medical data (PMD) as the retrieval source.\nFigure 2: The curves of L2 distance between the represen-\ntation of the reports from different models (i.e., MiniGPT-4\n(FT)+I3 and MiniGPT-4 (FT)) and that of the top-ranked or\nbottom-ranked instances against training steps.\n“+I3”) and C2FD (i.e., “+C2FD”) consistently outperforms\nall baselines on the test set of both datasets under all evalua-\ntion metrics, which demonstrates the effectiveness of our ap-\nproach given the baselines, especially the MiniGPT-4 (FT),\nhave already achieved outstanding performance. Second, the\nablation of either I3 or C2FD leads to inferior performance\ncompared with the full model, which presents that both com-\nponents play essential roles in this task. Third, in most cases,\nmodel with I3 achieves better performance than the one with\nC2FD. The performance gap is much more significant when\nthe models are equipped with the original MiniGPT-4 than\nthe fine-tuned one, which indicates the power of I3 to bridge\nthe domain gap and improve RRG, especially when the gap\nis rather large under the original MiniGPT-4 setting.\nWe further compare our approach (i.e., MiniGPT-4\n(FT)+I3+C2FD) with existing state-of-the-art methods on\nthe same datasets, where results are reported in Table 3\non both NLG metrics and CE metrics. Overall, our ap-\nproach significantly outperforms all existing studies on both\nNLG and CE metrics, which further confirms the validity of\nour approach to enhance LLMs for RRG by learning from\nranked instances and generating the final reports in a coarse-\nto-fine manner. Notably, our approach outperforms exist-\ning methods with medical domain LLMs (i.e, X RAYGPT\n(Thawkar et al. 2023) and M ED-PALM7 (Tu et al. 2023))\n7MED-PALM does not release the model weights and its RRG\ntest set. Therefore, for fair comparisons, we approximate their set-\ntings to randomly curated 10 groups of test instances with the same\nsize (i.e., 246 cases) as that used in M ED-PALM. Our approach\nNL\nG METRICS\nBL\n-1 BL-2 BL-3 BL-4 MTR RG-L\nINT\nER. 0.364 0.211\n0.142 0.093 0.145 0.261\nFINAL 0.402 0.262\n0.180 0.128 0.175 0.291\nTable 5: Comparison of intermediate (I NTER .) and final re-\nports that input to and output from C2FD w.r.t. NLG metrics.\nwith significant improvements.\n4.2 Analysis\nIn this section, we analyze the effect of different components\nof our approach. Specifically for I3, we explore it separately\nwith its two components, namely, RIR and CSR. Then, we\ninvestigate how C2FD performs and finally present the con-\ntribution of different components through a case study.\nEffect of Related Instance Retrieval To investigate the\neffect of instance retrieval methods, we try alternative set-\ntings that only use the training data or public medical cor-\npora as the retrieval sources in I3, as a comparison to that\nin our main experiment. Table 4 compares the results on\nMIMIC-CXR test set, where I3 (TD) and I3 (PMD) de-\nnote the aforementioned two settings, respectively. By com-\nparing the results in Table 4 to “MiniGPT (FT)+I3” reported\nin Table 2, it is observed that I3 in our approach outper-\nforms both I3 (TD) and I3 (PMD) with rather large mar-\ngin, which confirms the validity of our design because single\nretrieval source contributes less differentiated instances for\neffectively ranking, therefore affects domain and task adap-\ntation of the LLM to RRG. Moreover, the performance com-\nparison between I3 (TD) and I3 (PMD) verifies that in-\nstances from the training data are more important than those\nones irrelevant to the task, which further illustrates the fea-\nsibility and practicability of tuning LLM to RRG by our ap-\nproach (especially using I3) with employing limited data.\nEffect of Contrastive Semantic Ranking We analyze\nthe changing of L2 distance between the representations\nof the intermediate reports and the ranked instances (i.e.,\n∥o − o′\nm∥) during training, so as to analyze how CSR\nperforms. Figure 2 presents the curve of distance with re-\nspect to the training steps, where the red nodes stand for\nthe distances between the intermediate report generated by\nMiniGPT-4 (FT)+I3 and the top-ranked instances, and green\nones for that between the same report and bottom-ranked in-\nstances, respectively. As comparing references, blue/yellow\nnodes indicate the distances between the report generated by\nMiniGPT-4 (FT) and the top-/bottom-ranked instances, re-\nspectively. It is observed that, with more training steps, the\ndistance (green dots) between the reports from MiniGPT-\n4+I3 and the top-ranked instances decreases and that (red\ndots) between the same report and the bottom-ranked in-\nstances increases, which indicates that our approach suc-\nceeds to enable the generator to produce texts that are closer\nto good references and separated from irrelevant informa-\ntion. On the contrary, MiniGPT-4 (FT) without the assis-\nachieves performance similar to the results reported in Table 3.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18640\nMiniGPT-4 (FT)+I3: Lung volumes are normal. The heart size is within normal limits. The thoracic aorata is widened \nbut elongated. No local abnormality is identified. The cardiac and mediastinal silhouettes are unremarkable. No \nabnormalities in local contours are identified. No pleural effusion and pneumothorax is seen.\nMiniGPT-4 (FT): The given radiology image is a frontal chest radiograph of an adult male. The image is oblique, with \nno abnormalities visible. The lungs are well aerated and the heart is in its normal position. The mediastinum is not \nenlarged and there is no evidence of any infiltrates or consolidation. \nMiniGPT-4 (FT)+C2FD: Frontal chest radiograph is presented. The image is normal and unremarkable, with no \nevidence of vascular abnormalities. The chest radiograph shows moderate opacities in both lungs, consistent with \nchronic bronchitis. The heart is mildy enlarged. Mediastinum contours are clear and there is no pleural effusion or \nfocal consolidation.\nMiniGPT-4 (FT)+I3+C2FD: There are low lung volumes. Heart size is mildly enlarged. Mediastinal contours are \nunremarkable. Patchy atelectasis are seen at the lung bases. There is mild pulmonary vascular congestion without \novert interstitial edema. No pleural effusion or pneumothorax is present. There are no acute osseous abnormalities.\nGold Standard: Lung volumes are \nlow.  Mediastinal and hilar contours\nare unremarkable. The heart is mildly \nenlarged. Streaky opacities in the lung\nbases likely reflect areas of \natelectasis. No pleural effusion or \npneumothorax is present. There are \nno acute osseous abnormalities.\nMiniGPT-4: Key Findings:\n* A large mass in the right lung field, likely a malignancy\n* Irregularly shaped mass with a irregular border...\nOriginal Image\nFigure 3: An illustration of the reports generated by different models with a given input radiograph, where the gold standard is\nalso presented for reference. Medical terms shared by the gold standard and model outputs are highlighted in the same color.\nCorrect and incorrect content in the generated reports are highlighted in green and red background, respectively.\ntance of I3 is unable to distinguish related reports from the\nunrelated ones, which is not capable of aligning LLMs to the\ntask and thus obtains inferior performance.\nEffect of Coarse-to-Fine Decoding To investigate how\nC2FD performs, we run our full model and extract the re-\nports input (intermediate) to and output (final) from C2FD.\nTable 5 compares NLG results of those reports on MIMIC-\nCXR test set, where significant improvements are observed\non final reports, confirming the effectiveness of C2FD in re-\nfining the intermediate reports with precise and coherent el-\nevation when an appropriate model design is applied.\nCase Study To further qualitatively investigate how our\napproach bootstraps LLMs for RRG, we perform a case\nstudy on the output reports of different models with the same\ninput chest X-ray image chosen from MIMIC-CXR. Figure\n3 shows the results, with several observations from differ-\nent perspectives drawn as follows. The original MiniGPT-4\nfails to handle RRG and generates reports that contain few\nmedical terms. On the contrary, MiniGPT-4 (FT) generates\na much better report with more relevant medical terms (e.g.,\n“heart”, “mediastinum”) compared to the original MiniGPT-\n4. However, the generated reports still include irrelevant\ndescriptions (e.g., “This image is oblique.”) owing to less\nalignment to the task, which is also found in MiniGPT-\n4 (FT)+I3 although it produces reports that are well patt-\nernized and better aligned to the medical domain. MiniGPT-\n4 (FT)+C2FD produces more informative and precise re-\nports than that without C2FD, where some missing diag-\nnoses from MiniGPT-4 (FT) are resolved by the C2FD pro-\ncess (e.g., “There is no pleural effusion”). Finally, MiniGPT-\n4 (FT)+I3+C2FD generates the best report over all other\nmodels, whose problems are all alleviated to some extent,\nthus confirms the superiority of the proposed approach.\n5 Related Work\nClinical medicine has raised increasing attention nowadays\n(Wu et al. 2019; Tian et al. 2020; Song et al. 2020). Partic-\nularly, RRG is a challenging application, requiring to gen-\nerate long text in the medical domain. In doing so, some\nstudies try to leverage useful visual and textual features to\nimprove RRG, e.g., regional visual features (Tanida et al.\n2023), report templates (Li et al. 2018), and structure-level\ndescriptions (Wang et al. 2022a). From another aspect, some\nstudies focus on improving cross-modal alignment through\nco-attention (Jing, Xie, and Xing 2018), memory networks\n(Chen et al. 2020, 2021), and reinforcement learning (Qin\nand Song 2022), to better matching different information to\nguide the generation process. Recently, with the extraordi-\nnary generation ability of LLMs (Touvron et al. 2023a; Gan\net al. 2023; Yuanhe Tian 2023), recent studies have applied\nLLMs to multimodal scenarios (Li et al. 2023; Zhu et al.\n2023; Liu et al. 2023), including applications in the medi-\ncal domain (Thawkar et al. 2023; Tu et al. 2023). Compared\nwith them, our approach offers an alternative solution to im-\nprove RRG, where we start from general domain LLM (i.e.,\nMiniGPT-4) and design special learning processes to boot-\nstrap it for RRG with limited in-domain data for domain\nadaptation and generation optimization.\n6 Conclusion\nIn this paper, we propose to bootstrap LLMs for RRG, where\nI3 and C2FD are proposed to align LLMs with the medical\ndomain and improve report generation, respectively. Experi-\nment results demonstrate our superiority to current state-of-\nthe-art models on IU X-Ray and MIMIC-CXR with analysis\nfurther conducted to verify its validity. So that the fine-tuned\nLLM and our designed components in this work offer a new\nreference framework for future RRG studies. Notably, our\napproach provides a practical paradigm of adapting general\ndomain LLMs to applications in specific domains, which re-\nveals the potential of extending the approach to other tasks.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18641\nReferences\nAaron Nicolson, B. K., Jason Dowling. 2022. Improv-\ning Chest X-Ray Report Generation by Leveraging Warm-\nStarting. arXiv:2201.09405.\nAnderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.;\nGould, S.; and Zhang, L. 2018. Bottom-up and Top-down\nAttention for Image Captioning and Visual Question An-\nswering. In CVPR, 6077–6086.\nBai, Y .; Jones, A.; Ndousse, K.; Askell, A.; Chen, A.; Das-\nSarma, N.; Drain, D.; Fort, S.; Ganguli, D.; Henighan,\nT.; Joseph, N.; Kadavath, S.; Kernion, J.; Conerly, T.; El-\nShowk, S.; Elhage, N.; Hatfield-Dodds, Z.; Hernandez, D.;\nHume, T.; Johnston, S.; Kravec, S.; Lovitt, L.; Nanda, N.;\nOlsson, C.; Amodei, D.; Brown, T.; Clark, J.; McCandlish,\nS.; Olah, C.; Mann, B.; and Kaplan, J. 2022. Training a\nHelpful and Harmless Assistant with Reinforcement Learn-\ning from Human Feedback. arXiv:2204.05862.\nChen, Z.; Shen, Y .; Song, Y .; and Wan, X. 2021. Cross-\nmodal Memory Networks for Radiology Report Generation.\nIn Proceedings of the 59th Annual Meeting of the Associ-\nation for Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Processing\n(Volume 1: Long Papers), 5904–5914. Online.\nChen, Z.; Song, Y .; Chang, T.-H.; and Wan, X. 2020. Gen-\nerating Radiology Reports via Memory-driven Transformer.\nIn Proceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP), 1439–1449.\nOnline.\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;\nZheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; Stoica,\nI.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot\nImpressing GPT-4 with 90%* ChatGPT Quality.\nDemner-Fushman, D.; Kohli, M.; Rosenman, M.; Shooshan,\nS.; Rodriguez, L.; Antani, S.; Thoma, G.; and McDonald, C.\n2016. Preparing A Collection of Radiology Examinations\nfor Distribution and Retrieval.Journal of the American Med-\nical Informatics Association, 23(2): 304–310.\nDenkowski, M.; and Lavie, A. 2011. METEOR 1.3: Au-\ntomatic Metric for Reliable Optimization and Evaluation of\nMachine Translation Systems. In Proceedings of the Sixth\nWorkshop on Statistical Machine Translation, 85–91. Edin-\nburgh, Scotland.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. ICLR, 1–21.\nFang, Y .; Wang, W.; Xie, B.; Sun, Q.; Wu, L.; Wang, X.;\nHuang, T.; Wang, X.; and Cao, Y . 2022. EV A: Exploring the\nLimits of Masked Visual Representation Learning at Scale.\narXiv preprint arXiv:2211.07636.\nGan, R.; Wu, Z.; Sun, R.; Lu, J.; Wu, X.; Zhang, D.; Pan,\nK.; Yang, P.; Yang, Q.; Zhang, J.; and Song, Y . 2023. Ziya2:\nData-centric Learning is All LLMs Need. arXiv preprint\narXiv:2311.03301.\nIrvin, J.; Rajpurkar, P.; Ko, M.; Yu, Y .; Ciurea-Ilcus, S.;\nChute, C.; Marklund, H.; Haghgoo, B.; Ball, R.; Shpan-\nskaya, K.; Seekins, J.; Mong, D. A.; Halabi, S. S.; Sandberg,\nJ. K.; Jones, R.; Larson, D. B.; Langlotz, C. P.; Patel, B. N.;\nLungren, M. P.; and Ng, A. Y . 2019. CheXpert: A Large\nChest Radiograph Dataset with Uncertainty Labels and Ex-\npert Comparison. arXiv:1901.07031.\nJing, B.; Wang, Z.; and Xing, E. 2019. Show, Describe and\nConclude: On Exploiting the Structure Information of Chest\nX-ray Reports. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics, 6570–\n6580. Florence, Italy.\nJing, B.; Xie, P.; and Xing, E. 2018. On the Automatic\nGeneration of Medical Imaging Reports. In Proceedings\nof the 56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), 2577–2586.\nMelbourne, Australia.\nJohnson, A. E. W.; Pollard, T. J.; Greenbaum, N. R.; Lun-\ngren, M. P.; ying Deng, C.; Peng, Y .; Lu, Z.; Mark, R. G.;\nBerkowitz, S. J.; and Horng, S. 2019. MIMIC-CXR-JPG, A\nLarge Publicly Available Database of Labeled Chest Radio-\ngraphs. arXiv:1901.07042.\nKingma, D.; and Ba, J. 2015. Adam: A Method for Stochas-\ntic Optimization. In International Conference on Learning\nRepresentations (ICLR), 1–15. San Diega, CA, USA.\nLi, J.; Li, D.; Savarese, S.; and Hoi, S. 2023. BLIP-2: Boot-\nstrapping Language-image Pre-training with Frozen Image\nEncoders and Large Language Models. arXiv:2301.12597.\nLi, Y .; Liang, X.; Hu, Z.; and Xing, E. P. 2018. Hybrid\nRetrieval-generation Reinforced Agent for Medical Image\nReport Generation. In NeurIPS, 1537–1547.\nLin, C.-Y . 2004. ROUGE: A Package for Automatic Evalu-\nation of Summaries. In Text Summarization Branches Out,\n74–81. Barcelona, Spain.\nLiu, C.; Tian, Y .; and Song, Y . 2023. A Systematic Review of\nDeep Learning-based Research on Radiology Report Gener-\nation. arXiv:2311.14199.\nLiu, F.; Ge, S.; and Wu, X. 2021. Competence-based Mul-\ntimodal Curriculum Learning for Medical Report Genera-\ntion. In Proceedings of the 59th Annual Meeting of the As-\nsociation for Computational Linguistics and the 11th Inter-\nnational Joint Conference on Natural Language Processing\n(Volume 1: Long Papers), 3001–3012. Online.\nLiu, F.; Wu, X.; Ge, S.; Fan, W.; and Zou, Y . 2021a. Explor-\ning and Distilling Posterior and Prior Knowledge for Radi-\nology Report Generation. arXiv:2106.06963.\nLiu, F.; Yin, C.; Wu, X.; Ge, S.; Zhang, P.; and Sun, X.\n2021b. Contrastive Attention for Automatic Chest X-ray Re-\nport Generation. In Findings of the Association for Compu-\ntational Linguistics: ACL-IJCNLP 2021, 269–280. Online.\nLiu, H.; Li, C.; Wu, Q.; and Lee, Y . J. 2023. Visual Instruc-\ntion Tuning. arXiv:2304.08485.\nLu, J.; Xiong, C.; Parikh, D.; and Socher, R. 2017. Knowing\nWhen to Look: Adaptive Attention via A Visual Sentinel for\nImage Captioning. arXiv:1612.01887.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18642\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright,\nC. L.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray,\nA.; Schulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens,\nM.; Askell, A.; Welinder, P.; Christiano, P.; Leike, J.; and\nLowe, R. 2022. Training Language Models to Follow In-\nstructions with Human Feedback. arXiv:2203.02155.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBLEU: A Method for Automatic Evaluation of Machine\nTranslation. In Proceedings of the 40th Annual Meeting\nof the Association for Computational Linguistics, 311–318.\nPhiladelphia, Pennsylvania, USA.\nQin, H.; and Song, Y . 2022. Reinforced Cross-modal Align-\nment for Radiology Report Generation. In Findings of the\nAssociation for Computational Linguistics: ACL 2022, 448–\n458. Dublin, Ireland.\nRennie, S. J.; Marcheret, E.; Mroueh, Y .; Ross, J.; and Goel,\nV . 2017. Self-critical Sequence Training for Image Caption-\ning. arXiv:1612.00563.\nSong, Y .; Tian, Y .; Wang, N.; and Xia, F. 2020. Summariz-\ning Medical Conversations via Identifying Important Utter-\nances. In Proceedings of the 28th International Conference\non Computational Linguistics, 717–729.\nTanida, T.; M¨uller, P.; Kaissis, G.; and Rueckert, D. 2023. In-\nteractive and Explainable Region-guided Radiology Report\nGeneration. In CVPR, 7433–7442.\nThawkar, O.; Shaker, A.; Mullappilly, S. S.; Cholakkal, H.;\nAnwer, R. M.; Khan, S.; Laaksonen, J.; and Khan, F. S.\n2023. XrayGPT: Chest Radiographs Summarization using\nMedical Vision-Language Models. arXiv:2306.07971.\nTian, Y .; Shen, W.; Song, Y .; Xia, F.; He, M.; and Li, K.\n2020. Improving Biomedical Named Entity Recognition\nwith Syntactic Information. BMC Bioinformatics, 21: 1471–\n2105.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lam-\nple, G. 2023a. LLaMA: Open and Efficient Foundation Lan-\nguage Models. arXiv:2302.13971.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; Bikel, D.; Blecher, L.; Ferrer, C. C.; Chen, M.; Cucu-\nrull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller, B.;\nGao, C.; Goswami, V .; Goyal, N.; Hartshorn, A.; Hosseini,\nS.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V .; Khabsa, M.;\nKloumann, I.; Korenev, A.; Koura, P. S.; Lachaux, M.-A.;\nLavril, T.; Lee, J.; Liskovich, D.; Lu, Y .; Mao, Y .; Martinet,\nX.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y .; Poul-\nton, A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten,\nA.; Silva, R.; Smith, E. M.; Subramanian, R.; Tan, X. E.;\nTang, B.; Taylor, R.; Williams, A.; Kuan, J. X.; Xu, P.; Yan,\nZ.; Zarov, I.; Zhang, Y .; Fan, A.; Kambadur, M.; Narang,\nS.; Rodriguez, A.; Stojnic, R.; Edunov, S.; and Scialom, T.\n2023b. Llama 2: Open Foundation and Fine-tuned Chat\nModels. arXiv:2307.09288.\nTu, T.; Azizi, S.; Driess, D.; Schaekermann, M.; Amin, M.;\nChang, P.-C.; Carroll, A.; Lau, C.; Tanno, R.; Ktena, I.;\nMustafa, B.; Chowdhery, A.; Liu, Y .; Kornblith, S.; Fleet,\nD.; Mansfield, P.; Prakash, S.; Wong, R.; Virmani, S.; Sem-\nturs, C.; Mahdavi, S. S.; Green, B.; Dominowska, E.; y Ar-\ncas, B. A.; Barral, J.; Webster, D.; Corrado, G. S.; Ma-\ntias, Y .; Singhal, K.; Florence, P.; Karthikesalingam, A.; and\nNatarajan, V . 2023. Towards Generalist Biomedical AI.\narXiv:2307.14334.\nVinyals, O.; Toshev, A.; Bengio, S.; and Erhan, D.\n2015. Show and Tell: A Neural Image Caption Generator.\narXiv:1411.4555.\nWang, L.; Ning, M.; Lu, D.; Wei, D.; Zheng, Y .; and Chen,\nJ. 2022a. An Inclusive Task-aware Framework for Radiol-\nogy Report Generation. In Medical Image Computing and\nComputer Assisted Intervention – MICCAI 2022, 568–577.\nCham. ISBN 978-3-031-16452-1.\nWang, Z.; Wu, Z.; Agarwal, D.; and Sun, J. 2022b. Med-\nCLIP: Contrastive Learning from Unpaired Medical Images\nand Text. In Proceedings of the 2022 Conference on Empir-\nical Methods in Natural Language Processing, 3876–3887.\nAbu Dhabi, United Arab Emirates.\nWu, Z.; Song, Y .; Huang, S.; Tian, Y .; and Xia, F.\n2019. WTMED at MEDIQA 2019: A Hybrid Approach to\nBiomedical Natural Language Inference. In Proceedings of\nthe 18th BioNLP Workshop and Shared Task, 415–426. Flo-\nrence, Italy.\nYuanhe Tian, Y . S. J. Z. Y . Z., Ruyi Gan. 2023. ChiMed-\nGPT: A Chinese Medical Large Language Model with Full\nTraining Regime and Better Alignment to Human Prefer-\nences. arXiv preprint arXiv:2311.06025.\nZhu, D.; Chen, J.; Shen, X.; Li, X.; and Elhoseiny, M. 2023.\nMiniGPT-4: Enhancing Vision-language Understanding\nwith Advanced Large Language Models. arXiv:2304.10592.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18643",
  "topic": "Bootstrapping (finance)",
  "concepts": [
    {
      "name": "Bootstrapping (finance)",
      "score": 0.8898944854736328
    },
    {
      "name": "Computer science",
      "score": 0.5300191640853882
    },
    {
      "name": "Natural language processing",
      "score": 0.34283339977264404
    },
    {
      "name": "Medical physics",
      "score": 0.33523333072662354
    },
    {
      "name": "Radiology",
      "score": 0.32623058557510376
    },
    {
      "name": "Medicine",
      "score": 0.27466005086898804
    },
    {
      "name": "Econometrics",
      "score": 0.16855093836784363
    },
    {
      "name": "Mathematics",
      "score": 0.15266981720924377
    }
  ]
}