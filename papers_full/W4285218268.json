{
  "title": "Scaling Language Model Size in Cross-Device Federated Learning",
  "url": "https://openalex.org/W4285218268",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2846056713",
      "name": "Jae Ro",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2894777247",
      "name": "Theresa Breiner",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2757956876",
      "name": "Lara McConnaughey",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2119090176",
      "name": "Mingqing Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3202348671",
      "name": "Ananda Suresh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2110183797",
      "name": "Shankar Kumar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2907465900",
      "name": "Rajiv Mathews",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2786602455",
    "https://openalex.org/W2989787172",
    "https://openalex.org/W3038022836",
    "https://openalex.org/W3187964316",
    "https://openalex.org/W3103682594",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3103802018",
    "https://openalex.org/W2943845043",
    "https://openalex.org/W2783522756",
    "https://openalex.org/W4304764781",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3035453001",
    "https://openalex.org/W4299283926",
    "https://openalex.org/W3035003500",
    "https://openalex.org/W2146502635",
    "https://openalex.org/W3198769601",
    "https://openalex.org/W2617766261",
    "https://openalex.org/W3094793624",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W3183188446",
    "https://openalex.org/W4287663285",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W3161629591",
    "https://openalex.org/W4294106961",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W4297685247",
    "https://openalex.org/W3175729092",
    "https://openalex.org/W3006726390",
    "https://openalex.org/W2769644379",
    "https://openalex.org/W2784621220",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4220826035",
    "https://openalex.org/W4289147229",
    "https://openalex.org/W4318619660",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W3198442913",
    "https://openalex.org/W4289107582",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2950346378",
    "https://openalex.org/W2900120080",
    "https://openalex.org/W4297687186",
    "https://openalex.org/W3016234571",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3160766462",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W3047989515",
    "https://openalex.org/W2985986882",
    "https://openalex.org/W2530417694",
    "https://openalex.org/W2975043678",
    "https://openalex.org/W2963240019",
    "https://openalex.org/W3035046187",
    "https://openalex.org/W3193348889"
  ],
  "abstract": "Jae Ro, Theresa Breiner, Lara McConnaughey, Mingqing Chen, Ananda Suresh, Shankar Kumar, Rajiv Mathews. Proceedings of the First Workshop on Federated Learning for Natural Language Processing (FL4NLP 2022). 2022.",
  "full_text": "Proceedings of the First Workshop on Federated Learning for Natural Language Processing (FL4NLP 2022), pages 6 - 20\nMay 27, 2022 ©2022 Association for Computational Linguistics\nScaling Language Model Size in Cross-Device Federated Learning\nJae Hun Ro∗ and Theresa Breiner and Lara McConnaughey\nMingqing Chen and Ananda Theertha Sureshand Shankar Kumar and Rajiv Mathews\nGoogle\n∗jaero@google.com\nAbstract\nMost studies in cross-device federated learn-\ning focus on small models, due to the server-\nclient communication and on-device compu-\ntation bottlenecks. In this work, we lever-\nage various techniques for mitigating these\nbottlenecks to train larger language models\nin cross-device federated learning. With sys-\ntematic applications of partial model train-\ning, quantization, efﬁcient transfer learning,\nand communication-efﬁcient optimizers, we\nare able to train a 21M parameter Transformer\nthat achieves the same perplexity as that of\na similarly sized LSTM with ∼10×smaller\nclient-to-server communication cost and 11%\nlower perplexity than smaller LSTMs com-\nmonly studied in literature.\n1 Introduction\nFederated learning is a distributed training tech-\nnique, where a model is trained on data dis-\ntributed across clients or edge devices without user-\ngenerated data ever leaving the device, providing an\nadditional layer of privacy and security (Koneˇcn`y\net al., 2016b,a; McMahan et al., 2017). We refer\nreaders to (Li et al., 2020; Kairouz et al., 2021) for\na detailed literature survey on federated learning.\nFederated learning has been used in several applica-\ntions including virtual keyboard applications (Hard\net al., 2018), keyword spotting (Hard et al., 2020),\nand healthcare (Brisimi et al., 2018).\nLanguage models (LM) have many uses in\nlanguage-based applications including virtual key-\nboard (Chen et al., 2019; Zhang et al., 2021) and\nautomatic speech recognition (Kannan et al., 2018;\nVariani et al., 2020; Gruenstein et al., 2021). Re-\ncently, there has been increased interest in training\nprogressively larger and deeper LMs with impres-\nsive quality improvements in downstream tasks,\nincluding question answering, text classiﬁcation,\nand text summarization (Devlin et al., 2019; Dai\net al., 2019; Yang et al., 2019; Irie et al., 2019; Ka-\nplan et al., 2020). These models tend to be variants\nof the Transformer (Vaswani et al., 2017).\nFederated learning is typically studied in two\nscenarios: cross-silo, where the number of clients\nis small, and cross-device, where the number of\nclients can be in the order of millions (Hard et al.,\n2018). In this work we focus on cross-device,\nwhere devices are typically edge devices such as\ncell phones, with limited computation and commu-\nnication capabilities. Hence, the major benchmark\nLMs tend to be very limited in size (McMahan\net al., 2017, 2018; Caldas et al., 2019a; Reddi et al.,\n2020; Sim et al., 2021) because memory, compu-\ntation, and communication are critical bottlenecks\n(Kairouz et al., 2021). In particular, previous works\nthat train federated LMs in production settings have\nused coupled input forget gate (CIFG) long short-\nterm memory (LSTM) models with fewer than 4\nmillion parameters (Hard et al., 2018; Chen et al.,\n2019; Ramaswamy et al., 2020). These resource\nconstraints have motivated research into various\nefﬁcient algorithms for training larger models with\nfederated learning (Koneˇcn`y et al., 2016b; Hamer\net al., 2020). However, most of these techniques are\nstill evaluated on relatively small models compared\nto their server-based counterparts. In this work,\nwe systematically evaluate multiple strategies for\nmitigating communication and computation costs\nof training larger LMs to determine if the impres-\nsive quality gains from larger models can also be\nachieved in cross-device federated learning.\nWhile there are previous works on efﬁcient\nTransformers (Tay et al., 2020, 2021), we forgo\nthese efﬁcient variants as they may actually\nbe more inefﬁcient when sequences are short\n(Katharopoulos et al., 2020; Choromanski et al.,\n2021). Additionally, Lin et al. (2020); Liu and\nMiller (2020); Hilmkil et al. (2021) trained large\nTransformer models in the cross-silo setting, where\ndevices have more resources, whereas we focus on\nthe resource-constrained cross-device setting.\n6\nRecent large LMs, such as GPT-3 (Brown et al.,\n2020), contain hundreds of billions of parameters,\nwhich is substantially bigger than the memory lim-\nits of edge devices. Therefore in this work, we\nconsider large models to be at most 25 million pa-\nrameters, which is still considerably larger than\nexisting models trained on-device.\nThe rest of the paper is organized as follows. In\nSection 2, we overview our contributions. In Sec-\ntion 3, we detail the dataset and models. We then\nanalyze techniques to reduce the per-round cost\nin Section 4, and the number of communication\nrounds in Section 5. Finally in Section 6, we com-\nbine techniques and demonstrate that large Trans-\nformers can be trained using many fewer rounds\nand signiﬁcantly lower communication and compu-\ntation cost.\n2 Our contributions\nWe explore two regimes: small models typically\nstudied in cross-device federated learning with\nfewer than 5M parameters and new larger models\nwith at most 25M parameters. We study two archi-\ntectures: CIFG-LSTM (Hochreiter and Schmidhu-\nber, 1997), or LSTM for simplicity, (Hard et al.,\n2018) and Transformer (Vaswani et al., 2017). Our\ncontributions are the following:\n•We are the ﬁrst to investigate Transformer\nLMs with 25M parameters for cross-device\nfederated learning, which we ﬁnd outperform\nLSTMs of similar size.\n•We demonstrate that large models substan-\ntially outperform small models on standard\ntasks but at much higher communication and\ncomputation costs, requiring 4×the commu-\nnication cost per round.\n•We investigate quantization and partial model\ntraining to address the per round communica-\ntion and computation cost. With quantization,\nwe achieve similar perplexity with half the\ndownload cost and one quarter of the upload\ncost, reducing total communication cost by\n62.5%. Partial model training can further re-\nduce the upload cost by 60%.\n•We study transfer learning as a method of re-\nducing the number of communication rounds\nand show that centralized pretraining on a suit-\nable alternate corpus reduces the total commu-\nnication rounds by 3×.\n•We show that the combination of above tech-\nniques can be used to train a Large Trans-\nformer with the same perplexity as that of a\nsimilarly sized LSTM with∼10×the smaller\nclient-to-server communication cost.\n3 Dataset and models\nIn this section, we describe the models and dataset\nused in the rest of the paper. We train on\nthe Stack Overﬂow federated dataset from TFF\n(2018), which contains posts from the public forum\ngrouped by username. Following trends in training\nTransformers, we use sentence-piece (Kudo and\nRichardson, 2018) for sub-word tokenization with\na vocabulary size of 4K. The sentence-piece model\nis computed based on the entire Stack Overﬂow\ntraining corpus in an ofﬂine process on server. Dur-\ning federated learning, this ﬁxed sentence-piece\nmodel is transmitted to each client to encode the\nlocal text data. Doing so provides greater coverage\nfor cross-dataset applications as well as potential\ndownstream speech applications such as ASR (Li\net al., 2021; Sim et al., 2021). We measure per-\nformance on next-subword prediction using test\nperplexity. See Appendix A for descriptive dataset\nstatistics. All experiments were implemented using\nJAX (Bradbury et al., 2018) and FedJAX (Ro et al.,\n2021) federated simulation libraries.\nWe ﬁrst did a hyperparameter search for each\nmodel and size (≤5M and ≤25M), with FedAdam\n(Reddi et al., 2020), or FedAvg for simplicity, with\n200 clients per round for 3K rounds, resulting in\nfour models: Small LSTM (4.7M), Large LSTM\n(18.8M), Small Transformer (4.1M), and Large\nTransformer (21M).\nFigure 1: Test perplexity over communication rounds\nfor each class and size of model.\nWe then trained the chosen architectures with\n800 clients per round for 10K rounds in Figure 1.\nAs expected, the larger variants signiﬁcantly out-\nperform their smaller counterparts with the Large\nTransformer achieving the best perplexity. How-\never, the larger models are more expensive to train\n7\nper round and although the Large Transformer\nachieves the best perplexity, it only surpasses the\nLarge LSTM after 4K rounds. Next, we focus\non techniques to reduce this cost per round and\nnumber of rounds. For more details about the ar-\nchitecture search, the selected models, and their\nperformance, see Appendix A.\n4 Cost per round\nThe larger models have 18.8M and 21M param-\neters (150MB and 168MB, at 32 bits per param-\neter) which need to be downloaded, trained, and\nuploaded at each round, a strain on both commu-\nnication and computation on device. There are\noften strict time or transfer byte limits for each\nround of training, which can prohibit some devices\nfrom training these models due to slower trans-\nfer/processing speeds (Kairouz et al., 2021). We\nshow that we can signiﬁcantly reduce these costs by\npartial model training and quantization techniques.\nPartial model training: Training only a subset\nof the model can reduce the computational cost of\ntraining and has been examined in both federated\n(Caldas et al., 2019b; Yang et al., 2021) and non-\nfederated (Kovaleva et al., 2019) settings. Addition-\nally, reducing the number of trainable parameters\ncan also decrease communication cost since only\nthe trainable parameters need to be uploaded.\nFigure 2: Test perplexity as a function of number of\ntrainable variables.\nWe follow the Partial Variable Training (PVT)\nper client per round strategy (Yang et al., 2021)\nas it only freezes a subset of the original model\nand can be applied generally to multiple model\narchitecture types. For more experiment details, see\nAppendix B. We report test perplexity as a function\nof number of trainable variables in Figure 2. Large\nLSTM seems to be able to handle more aggressive\nparameter freezing compared to Large Transformer\nin terms of quality regression. However, training\nonly 40% of variables for the Large Transformer\n(6.3M) achieves better performance than the full\nLarge LSTM (18.8M).\nQuantization: To reduce communication costs,\nvarious quantization strategies can decrease the\nnumber of bits required to represent model pa-\nrameters (Bernstein et al., 2018; Reisizadeh et al.,\n2020; Gandikota et al., 2021; Vargaftik et al., 2021).\nWe examine stochastic k-level uniform quantiza-\ntion (Alistarh et al., 2017; Suresh et al., 2017) as\nit can be applied to model parameters on down-\nload (server-to-client) and model updates on upload\n(client-to-server) communication with adjustable\nlevels of compression, and compare with TernGrad,\nan upload technique (Wen et al., 2017).\nWe focus analysis on larger models which are\nmore affected by quantization. The LSTM ap-\npears more \"quantizable\" during download than\nthe Transformer, with less regression in Figure 3.\nThe perplexity of the Transformer with 16 down-\nload bits matches that of the baseline Transformer\nand with 12 bits its perplexity is close to that of the\nLSTM.\nFigure 3: Test perplexity over communication rounds\nfor varying download quantization levels, with upload\nquantization ﬁxed to8 bits. Dashed line shows the base-\nline without quantization.\nFor both the models, 8 bit upload matches the\ncorresponding baselines, or even 6 bits for the\nLSTM in Figure 4. TernGrad, requiring log2(3)\nbits, outperforms the 4 bit in the Transformer but\nnot for the LSTM in Figure 5. More details are in\nAppendix C.\nFigure 4: Test perplexity over communication rounds\nfor varying upload quantization levels, with download\nquantization ﬁxed to 16 bits. TernGrad is comparable\nto uniform with about 1.6 bits. Dashed line shows the\nbaseline without quantization.\n8\nFigure 5: Test set perplexity versus total communica-\ntion cost (download + upload) in a single round of\ntraining, for each quantization algorithm. Uniform set-\ntings include points for varying quantization bits.\n5 Number of communication rounds\nTransfer learning: Transfer learning leverages\npretrained models to improve model quality\n(Houlsby et al., 2019). By pretraining, the number\nof communication rounds required for model con-\nvergence can be signiﬁcantly reduced (Stremmel\nand Singh, 2020).\nWe use two datasets for pretraining: a large cor-\npus of digitized books (Zhang et al., 2021) and\nthe One Billion Word Benchmark (LM1B) (Chelba\net al., 2014). After pretraining using synchronous\nSGD for 30M steps, we ﬁnetune on Stack Over-\nﬂow using FedAvg. For additional details, see Ap-\npendix D. We report results for each of the pretrain-\ning datasets and random initialization in Figure 6.\nBooks consistently outperforms LM1B for both\nthe LSTM and Transformer. Pretraining greatly\nbeneﬁts the Large Transformer compared to the\nLarge LSTM, reducing the number of rounds\nneeded to reach the ﬁnal 10K without pretraining\nby 4K rounds. Furthermore, at round 2K, the Large\nTransformer already outperforms the Large LSTM,\nmaking the number of rounds needed for training\nsimilar to that of smaller models used in mobile\nkeyboard prediction (Hard et al., 2018).\nFigure 6: Test perplexity over communication compar-\ning pretraining corpora. Dashed line is the ﬁnal per-\nplexity reached by the randomly initialized model.\nDifferent optimizers: Since the introduction of\nFedAvg, several variations continue to be devel-\noped (Li et al., 2018; Hamer et al., 2020; Reddi\nFigure 7: Test perplexity over communication rounds\nfor each model and algorithm.\nFigure 8: Test perplexity over total uploaded gigabytes\nper client for each class of model.\net al., 2020). Speciﬁcally, we examine MimeLite\n(Karimireddy et al., 2020) and FedProx (Li et al.,\n2018) as they have been shown to reduce the to-\ntal amount of rounds required for provable con-\nvergence. However, in Figure 7, FedProx and\nMimeLite do not improve convergence speed over\nFedAvg. More details can be found in Appendix E.\n6 Combination of techniques\nWe experiment with combining partial model train-\ning, quantization, and transfer learning to train efﬁ-\ncient larger models. For these experiments, we\ntrain on just 40% of trainable parameters with\nPVT and warm start after pretraining on the Books\ncorpus. Combining download quantization with\nthese techniques did not perform as well, so we\nonly apply 8 bit uniform quantization on upload,\nwhich is the tightest communication bottleneck\n(Statista.com (2021) reports that mobile upload\nspeeds worldwide are over 4×slower than down-\nload as of May 2021). For the full experiment\ndetails, refer to Appendix F. We report the test\nperplexity in terms of total upload communication\ncost in Figure 8. Restricting for small upload costs\n(<200GB), the efﬁcient models outperform all oth-\ners with the efﬁcient Large Transformer yielding\nthe best perplexity. Furthermore, the efﬁcient Large\nTransformer also achieves the same perplexity as\nthe Large LSTM with no efﬁcient techniques.\n9\n7 Conclusion\nWe systematically studied several techniques for ad-\ndressing the communication and computation bot-\ntlenecks of federated learning. We further demon-\nstrated that these techniques, individually or in\ncombination, can scale to larger models in cross-\ndevice federated learning. Extending this study to\nother architectures and efﬁcient strategies remains\nan interesting open question.\nReferences\nDan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka,\nand Milan V ojnovic. 2017. Qsgd: Communication-\nefﬁcient sgd via gradient quantization and encoding.\nAdvances in Neural Information Processing Systems,\n30.\nJeremy Bernstein, Yu-Xiang Wang, Kamyar Aziz-\nzadenesheli, and Animashree Anandkumar. 2018.\nsignsgd: Compressed optimisation for non-convex\nproblems. In International Conference on Machine\nLearning, pages 560–569. PMLR.\nJames Bradbury, Roy Frostig, Peter Hawkins,\nMatthew James Johnson, Chris Leary, Dougal\nMaclaurin, George Necula, Adam Paszke, Jake\nVanderPlas, Skye Wanderman-Milne, and Qiao\nZhang. 2018. JAX: composable transformations of\nPython+NumPy programs.\nTheodora S Brisimi, Ruidi Chen, Theofanie Mela, Alex\nOlshevsky, Ioannis Ch Paschalidis, and Wei Shi.\n2018. Federated learning of predictive models from\nfederated electronic health records. International\njournal of medical informatics, 112:59–67.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nSebastian Caldas, Sai Meher Karthik Duddu, Peter Wu,\nTian Li, Jakub Koneˇcný, H. Brendan McMahan, Vir-\nginia Smith, and Ameet Talwalkar. 2019a. Leaf: A\nbenchmark for federated settings.\nSebastian Caldas, Jakub Kone ˇcny, H. Brendan McMa-\nhan, and Ameet Talwalkar. 2019b. Expanding the\nreach of federated learning by reducing client re-\nsource requirements.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nT. Brants, Phillip Todd Koehn, and Tony Robinson.\n2014. One billion word benchmark for measuring\nprogress in statistical language modeling. ArXiv,\nabs/1312.3005.\nMingqing Chen, Ananda Theertha Suresh, Rajiv Math-\news, Adeline Wong, Cyril Allauzen, Françoise Bea-\nufays, and Michael Riley. 2019. Federated learn-\ning of n-gram language models. In Proceedings\nof the 23rd Conference on Computational Natural\nLanguage Learning (CoNLL), pages 121–130, Hong\nKong, China. Association for Computational Lin-\nguistics.\n10\nKrzysztof Marcin Choromanski, Valerii Likhosherstov,\nDavid Dohan, Xingyou Song, Andreea Gane, Tamas\nSarlos, Peter Hawkins, Jared Quincy Davis, Afroz\nMohiuddin, Lukasz Kaiser, David Benjamin Be-\nlanger, Lucy J Colwell, and Adrian Weller. 2021.\nRethinking attention with performers. In Interna-\ntional Conference on Learning Representations.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJohn Duchi, Elad Hazan, and Yoram Singer. 2011.\nAdaptive subgradient methods for online learning\nand stochastic optimization. Journal of Machine\nLearning Research, 12(61):2121–2159.\nVenkata Gandikota, Daniel Kane, Raj Kumar Maity,\nand Arya Mazumdar. 2021. vqsgd: Vector quantized\nstochastic gradient descent. In International Confer-\nence on Artiﬁcial Intelligence and Statistics , pages\n2197–2205. PMLR.\nAlex Gruenstein, Anmol Gulati, Arun Narayanan,\nBo Li, Cal Peyser, Chung-Cheng Chiu, Cyril\nAllauzen, David Johannes Rybach, Diamantino A.\nCaseiro, Ehsan Variani, Emmanuel Guzman,\nIan Carmichael McGraw, James Qin, Jiahui\nYu, Michael D. Riley, Pat Rondon, Qiao Liang,\nQuoc-Nam Le-The, Rami Botros, Ruoming Pang,\nSepand Mavandadi, Shuo yiin Chang, Tara N\nSainath, Trevor Deatrick Strohman, W. Ronny\nHuang, Wei Li, Yanzhang (Ryan) He, Yonghui\nWu, and Yu Zhang. 2021. An efﬁcient streaming\nnon-recurrent on-device end-to-end model with\nimprovements to rare-word modeling.\nJenny Hamer, Mehryar Mohri, and Ananda Theertha\nSuresh. 2020. Fedboost: A communication-efﬁcient\nalgorithm for federated learning. In International\nConference on Machine Learning, pages 3973–3983.\nPMLR.\nAndrew Hard, Kurt Partridge, Cameron Nguyen, Ni-\nranjan Subrahmanya, Aishanee Shah, Pai Zhu, Igna-\ncio Lopez Moreno, and Rajiv Mathews. 2020. Train-\ning keyword spotting models on non-iid data with\nfederated learning. In Interspeech.\nAndrew Hard, Kanishka Rao, Rajiv Mathews,\nFrançoise Beaufays, Sean Augenstein, Hubert\nEichner, Chloé Kiddon, and Daniel Ramage. 2018.\nFederated learning for mobile keyboard prediction.\narXiv preprint arXiv:1811.03604.\nAgrin Hilmkil, Sebastian Callh, Matteo Barbieri,\nLeon René Sütfeld, Edvin Listo Zec, and Olof Mo-\ngren. 2021. Scaling federated learning for ﬁne-\ntuning of large language models. In International\nConference on Applications of Natural Language to\nInformation Systems, pages 15–23. Springer.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for NLP.\nIn Proceedings of the 36th International Conference\non Machine Learning , volume 97 of Proceedings\nof Machine Learning Research , pages 2790–2799.\nPMLR.\nKazuki Irie, Albert Zeyer, Ralf Schlüter, and Hermann\nNey. 2019. Language modeling with deep trans-\nformers. Interspeech 2019.\nPeter Kairouz et al. 2021. Advances and open\nproblems in federated learning. Foundations and\nTrends R⃝in Machine Learning, 14(1).\nAnjuli Kannan, Yonghui Wu, Patrick Nguyen, Tara N.\nSainath, ZhiJeng Chen, and Rohit Prabhavalkar.\n2018. An analysis of incorporating an external lan-\nguage model into a sequence-to-sequence model. In\n2018 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , pages 1–\n5828.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario\nAmodei. 2020. Scaling laws for neural language\nmodels.\nSai Praneeth Karimireddy, Martin Jaggi, Satyen Kale,\nMehryar Mohri, Sashank J Reddi, Sebastian U Stich,\nand Ananda Theertha Suresh. 2020. Mime: Mim-\nicking centralized stochastic algorithms in federated\nlearning. arXiv preprint arXiv:2008.03606.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and Francois Fleuret. 2020. Transformers are\nrnns: Fast autoregressive transformers with linear at-\ntention. In ICML 2020: 37th International Confer-\nence on Machine Learning , volume 1, pages 5156–\n5165.\nJakub Koneˇcn`y, H Brendan McMahan, Daniel Ramage,\nand Peter Richtárik. 2016a. Federated optimization:\nDistributed machine learning for on-device intelli-\ngence. arXiv preprint arXiv:1610.02527.\n11\nJakub Koneˇcn`y, H Brendan McMahan, Felix X Yu, Pe-\nter Richtárik, Ananda Theertha Suresh, and Dave\nBacon. 2016b. Federated learning: Strategies for im-\nproving communication efﬁciency. arXiv preprint\narXiv:1610.05492.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof BERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4365–4374, Hong Kong, China. Association for\nComputational Linguistics.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nBo Li, Anmol Gulati, Jiahui Yu, Tara N. Sainath,\nChung-Cheng Chiu, Arun Narayanan, Shuo-Yiin\nChang, Ruoming Pang, Yanzhang He, James Qin,\nWei Han, Qiao Liang, Yu Zhang, Trevor Strohman,\nand Yonghui Wu. 2021. A better and faster end-\nto-end model for streaming ASR. In IEEE Inter-\nnational Conference on Acoustics, Speech and Sig-\nnal Processing, ICASSP 2021, Toronto, ON, Canada,\nJune 6-11, 2021, pages 5634–5638. IEEE.\nTian Li, Anit Kumar Sahu, Ameet Talwalkar, and Vir-\nginia Smith. 2020. Federated learning: Challenges,\nmethods, and future directions. IEEE Signal Pro-\ncessing Magazine, 37(3):50–60.\nTian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar San-\njabi, Ameet Talwalkar, and Virginia Smith. 2018.\nFederated optimization in heterogeneous networks.\narXiv preprint arXiv:1812.06127.\nTao Lin, Lingjing Kong, Sebastian U Stich, and Martin\nJaggi. 2020. Ensemble distillation for robust model\nfusion in federated learning. Advances in Neural In-\nformation Processing Systems, 33:2351–2363.\nDianbo Liu and Tim Miller. 2020. Federated pretrain-\ning and ﬁne tuning of bert using clinical notes from\nmultiple silos. arXiv preprint arXiv:2002.08562.\nBrendan McMahan, Eider Moore, Daniel Ramage,\nSeth Hampson, and Blaise Aguera y Arcas. 2017.\nCommunication-efﬁcient learning of deep networks\nfrom decentralized data. In Artiﬁcial Intelligence\nand Statistics, pages 1273–1282. PMLR.\nH. Brendan McMahan, Daniel Ramage, Kunal Talwar,\nand Li Zhang. 2018. Learning differentially private\nrecurrent language models.\nSwaroop Ramaswamy, Om Thakkar, Rajiv Math-\news, Galen Andrew, H. Brendan McMahan, and\nFrançoise Beaufays. 2020. Training production lan-\nguage models without memorizing user data.\nSashank Reddi, Zachary Charles, Manzil Zaheer,\nZachary Garrett, Keith Rush, Jakub Koneˇcný, Sanjiv\nKumar, and H. Brendan McMahan. 2020. Adaptive\nfederated optimization.\nAmirhossein Reisizadeh, Aryan Mokhtari, Hamed Has-\nsani, Ali Jadbabaie, and Ramtin Pedarsani. 2020.\nFedpaq: A communication-efﬁcient federated learn-\ning method with periodic averaging and quantiza-\ntion. In Proceedings of the Twenty Third Inter-\nnational Conference on Artiﬁcial Intelligence and\nStatistics, volume 108 of Proceedings of Machine\nLearning Research, pages 2021–2031. PMLR.\nJae Hun Ro, Ananda Theertha Suresh, and Ke Wu.\n2021. Fedjax: Federated learning simulation with\njax. arXiv preprint arXiv:2108.02117.\nKhe Chai Sim, Angad Chandorkar, Fan Gao, Mason\nChua, Tsendsuren Munkhdalai, and Françoise Beau-\nfays. 2021. Robust Continuous On-Device Personal-\nization for Automatic Speech Recognition. In Proc.\nInterspeech 2021, pages 1284–1288.\nStatista.com. 2021. Average mobile and ﬁxed broad-\nband download and upload speeds worldwide as of\nMay 2021. Accessed September 26, 2021.\nJoel Stremmel and Arjun Singh. 2020. Pretraining fed-\nerated text models for next word prediction. CoRR,\nabs/2005.04828.\nAnanda Theertha Suresh, Felix X Yu, Sanjiv Kumar,\nand H Brendan McMahan. 2017. Distributed mean\nestimation with limited communication. In Pro-\nceedings of the 34th International Conference on\nMachine Learning-Volume 70 , pages 3329–3337.\nJMLR. org.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang\nShen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu\nYang, Sebastian Ruder, and Donald Metzler. 2021.\nLong range arena : A benchmark for efﬁcient trans-\nformers. In International Conference on Learning\nRepresentations.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020. Efﬁcient transformers: A survey.\nTFF. 2018. Tensorﬂow federated.\nShay Vargaftik, Ran Ben-Basat, Amit Portnoy, Gal\nMendelson, Yaniv Ben-Itzhak, and Michael Mitzen-\nmacher. 2021. DRIVE: One-bit distributed mean es-\ntimation. In Advances in Neural Information Pro-\ncessing Systems.\nEhsan Variani, David Rybach, Cyril Allauzen, and\nMichael Riley. 2020. Hybrid autoregressive trans-\nducer (hat).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\n12\nWei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yan-\ndan Wang, Yiran Chen, and Hai Li. 2017. Terngrad:\nTernary gradients to reduce communication in dis-\ntributed deep learning. CoRR, abs/1705.07878.\nTien-Ju Yang, Dhruv Guliani, Françoise Beaufays, and\nGiovanni Motta. 2021. Partial variable training for\nefﬁcient on-device federated learning.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems , volume 32. Curran\nAssociates, Inc.\nHao Zhang, You-Chi Cheng, Shankar Kumar,\nMingqing Chen, and Rajiv Mathews. 2021.\nPosition-invariant truecasing with a word-and-\ncharacter hierarchical recurrent neural network.\nArXiv, abs/2108.11943.\nBarret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui,\nHanxiao Liu, Ekin Dogus Cubuk, and Quoc Le.\n2020. Rethinking pre-training and self-training. In\nNeurIPS.\n13\nAppendix\nA Dataset and models\nFigure 9: Stack Overﬂow train split sub-word statistics.\nTable 1: Selected architectures for each model and size range. The values in [ ] are the possible hyperparameter\nvalues searched over. Layer Size refers to the LSTM layer dimension and MLP layer dimension for Transformer\nand # Layers refers to number of LSTM layers and number of Transformer blocks.\nModel # Parameters Embedding Size Layer Size # Layers\n[128,256,512,1024] [512 ,1024,2048] [1 ,2,4,6,8]\nSmall LSTM 4.7M 256 2048 1\nSmall Transformer 4.1M 128 2048 6\nLarge LSTM 18.8M 1024 2048 1\nLarge Transformer 21.0M 512 2048 6\nTable 2: Test metrics after 10K rounds of training for each class of model and number of clients per round. The\nresults in bold indicate the best for each size range.\nModel # Clients Perplexity\nSmall LSTM 200 35 .31\nSmall LSTM 400 34 .93\nSmall LSTM 800 34.80\nSmall Transformer 200 40 .18\nSmall Transformer 400 39 .38\nSmall Transformer 800 38 .66\nLarge LSTM 200 30 .97\nLarge LSTM 400 30 .79\nLarge LSTM 800 30 .83\nLarge Transformer 200 30 .64\nLarge Transformer 400 29 .81\nLarge Transformer 800 29.15\nFor the baseline architecture search, Table 1 details the selected architectures as well as the search\nranges for each dimension. The ﬁnal hyperparameters were selected based on the test perplexity after 3K\nrounds of training using FedAvg with 200 clients per round. From here on, we ﬁx the Adam optimizer\nwith β1 at 0.9, β2 at 0.999, and epsilon at 1e−8. Additionally, based on the distribution of average\nsequence lengths across Stack Overﬂow clients in Figure 9, we ﬁx the max sequence length for training\nand evaluation to 30.\nTable 2 contains the results for each selected model after10K rounds of training using FedAvg with\n200, 400, and 800 clients per round. As expected, the best results are achieved by using 800 clients per\nround. Thus, from here on, we report results for 800 clients per round only. For these experiments, we\n14\nTable 3: Selected hyperparameters for each model and size range. The values in[ ] are the possible hyperparameter\nvalues searched over. Batch Size, # Examples, and Clipnorm here apply to the client local SGD steps. LR is\nlearning rate.\nModel Batch Size # Examples Clipnorm Client LR Server LR\n[8,16] [1200 ,1600] [0 .0,16.0] [0 .01,0.1,0.5,1.0,2.0] [0 .001,0.01]\nSmall LSTM 16 1200 16 .0 1 .0 0 .001\nSmall Transformer 16 1200 0 .0 0 .1 0 .001\nLarge LSTM 16 1200 16 .0 1 .0 0 .001\nLarge Transformer 16 1200 0 .0 0 .5 0 .001\nFigure 10: Test set perplexity as a function of number of gradient computations for comparing the centralized and\nfederated averaging baselines.\nalso search over client learning rate, client batch size, client max number of examples (with client number\nof epochs ﬁxed to 1), client ℓ2 norm for clipping, and server learning rate. The search ranges as well as\nselected values for each model are detailed in Table 3. For all following experiments, we ﬁx client batch\nsize to 16 and client max number of examples to 1200 since the larger batch size consistently performed\nthe best and Figure 9 shows that 1200 sequences is more than enough to cover the vast majority of clients\nwith the number of epochs ﬁxed at 1. We also search over the same ranges for all following experiments\nwhere applicable for consistency.\nAs an additional baseline comparison, we also train each model using synchronous SGD to observe\nmodel quality in terms of number of gradient computations. These centralized baselines provide a rough\nestimate of an upper bound on model quality for federated learning. To produce a reasonable comparison\nbetween the federated and centralized experiments, we compare by number of gradient computations.\nWe approximate the number of gradient steps taken for federated learning with 200 clients per round for\n10K communication rounds. We train the centralized models using the Adam optimizer and run periodic\nevaluation on the test set at the same frequency as the federated experiments. We report and compare ﬁnal\nmetrics between centralized training and federated averaging on the test set in Figure 10. Observing the\ntest perplexity over gradient steps, it is evident that the relative rankings of the models remain consistent\nbetween centralized and federated baselines. Additionally, by 10K rounds, the large federated models\nseem to approach somewhat close in perplexity to their centralized counterparts.\nB Partial model training\nIn our experiments with PVT, we vary the percentage of trainable variables from10% to 90% in increments\nof 10. As before, we search over the hyperparameters in Table 3 and ﬁnd them to be mostly consistent\nwith baseline other than client learning rate. Following Yang et al. (2021), we use the per client per round\n(PCPR) conﬁguration, where the frozen variables vary from round to round and from client to client, as\nthis was shown to achieve the highest accuracy. Speciﬁcally, we only freeze subsets of the multiplicative\nvectors and matrices of the original model. This corresponds to the embedding and weights of the LSTM,\nand for the Transformer, the weights of the MLP layer, attention matrices, layer normalization in each\n15\nTable 4: Test perplexity after 10K communication rounds of training for each class of model and PVT % of\ntrainable variables.\nModel Trainable % # Parameters Perplexity\nSmall LSTM 100% 4 .7M 34.80\nSmall Transformer 100% 4 .1M 38.66\nLarge LSTM 100% 18 .8M 30.83\nLarge LSTM 40% 7 .5M 31.53\nLarge LSTM 20% 3 .8M 32.93\nLarge Transformer 100% 21 .0M 29.15\nLarge Transformer 40% 8 .4M 30.45\nLarge Transformer 20% 4 .2M 32.61\nFigure 11: Test perplexity over communication rounds for the large models with select percentages of trainable\nvariables denoted by X% with 100% indicating all trainable variables are trained (i.e. baseline).\nblock, and embedding. We also note though that although overall the number of trainable variables might\naverage to the desired percentage (e.g. 10%), for certain architectures, like LSTM, that don’t have that\nmany freezable variables (only one layer’s weight matrix and embedding matrix), the number of trained\nvariables will be much more variable from round to round. On the other hand, for architectures, like\nTransformer, that have more freezable variables (6 blocks’ weight matrices and attention matrices and\nembeddings), the number of trained is much more consistent between rounds.\nWe report test set perplexity over communication rounds for the large architectures and varying degrees\nof PVT in Figure 11 with the number of clients per round set to 800. Looking at Table 4, it is evident\nthat both large models can handle some percentage of partial freezing up until a certain point and that\nthe Large Transformer with only 40% of trainable variables can reach a similar perplexity as the Large\nLSTM with 100% trainable variables by 10K rounds or so. However, training for the full 10K rounds can\nbe a communication bottleneck so PVT would need to be combined with another technique to reduce the\nnumber of rounds needed.\nC Quantization\nIn stochastic k-level uniform quantization (Suresh et al., 2017), values in each layer are converted into one\nof kevenly distributed values between the layer min and max, stochastically assigned to the closest target\nvalue either above or below the real value. The lower the kvalue, the more the data is being compressed,\nas the number of bits used to store the value equals log2(k). For download quantization, we explore k\nvalues corresponding to between 8 and 28 bits. For upload quantization, which can be a larger bottleneck\nin edge devices (Statista.com, 2021), we explore kvalues corresponding to between 1 and 28 bits. On\nupload, we also try applying zero-centering during uniform quantization as well as trying the TernGrad\n(Wen et al., 2017) algorithm, which quantizes values in each vector vinto only one of three values, 0 and\n±max(|v|), corresponding to log2(3) (∼1.585) bits per parameter. While TernGrad is designed to use L\ninﬁnity clipping (ℓ∞), we experiment with and without this for completeness.\n16\nFigure 12: Test set perplexity over communication rounds for varying upload quantization levels, with download\nquantization ﬁxed to 16 bits. The dotted line shows baseline perplexity achieved after 10K rounds without any\nquantization.\nWhile ℓ∞clipping did make a signiﬁcant difference in the TernGrad experiment for Transformers,\nperforming much better with it than without, it did not have a large effect on the TernGrad performance in\nthe LSTM in Figure 12. TernGrad and its counterpart uniform quantization to ∼1.585 bits performed\nthe same, as long as ℓ∞clipping was applied. It is clear from the uniform 2-bit experiments as well that\nℓ∞clipping is important when quantizing into these lower number of bits; the 2-bit experiment without\nclipping performs much worse than the Terngrad without clipping, although enabling clipping allows\n2-bit to perform slightly better than Terngrad’slog2(3) bits with clipping. Zero-centering did not seem to\naffect upload behavior much for either model, marginally improving the LSTM and marginally degrading\nthe Transformer.\nWe explore the patterns of communication cost for each experiment setting in Figure 5. We calculate\nthe approximate download and upload MB for each experiment by multiplying the model’s number of\nparameters by the number of download or upload bits to get total bits transported.\nExamining Figure 5, we note the baseline points for each set of experiments as the lowest and rightmost,\ngetting the best perplexity but also highest communication cost. Starting from there, we see trends of no\nperplexity degradation as we apply conservative quantization to the Large LSTM and Transformer settings\nand move left in the plot. We then reach an elbow in the points for each setting right around where the\nTerngrad point is, from which point perplexity degrades drastically without much communication cost\nsavings as the points head up in two lines as upload quantization is reduced, with one line corresponding\nto experiments with download 16 bits and the other to download 12 bits. While the Terngrad point for\nthe Large Transformer falls at the outermost point in the \"elbow\" and therefore gives the best tradeoff\nfor cost versus perplexity, there is one uniform quantization point that does better than the Large LSTM\nTerngrad, which is download 12 bits and upload 6 bits. It makes sense that this does well as we saw that\nthe LSTM was able to use these settings without much regression from the baseline performance, while\nthe Transformer could only quantize to 16 download bits and 8 upload bits without regressions.\n17\nTable 5: Selected hyperparameters for each centrally trained model and dataset. The values in [ ] are the possible\nhyperparameter values searched over.\nModel Dataset Clipnorm Learning Rate\n[0,16] [1 e−5,5e−5,1e−4,\n5e−4,1e−3,5e−3,1e−2]\nSmall LSTM Book 16.0 5 e−5\nSmall LSTM LM1B 0.0 5 e−5\nLarge LSTM Book 0.0 5 e−5\nLarge LSTM LM1B 0.0 5 e−5\nSmall Transformer Book 0.0 1 e−4\nSmall Transformer LM1B 16.0 1 e−4\nLarge Transformer Book 16.0 5 e−5\nLarge Transformer LM1B 16.0 5 e−5\nD Transfer learning\nTo ﬁnd the best models pretrained on the Books and LM1B datasets, we train for30M steps of synchronous\nSGD searching over learning rate and clip norm. Like our other centrally trained models, the batch size is\nﬁxed to 16 and Adam is used with β1 at 0.9, β2 at 0.999, and epsilon at 1e−8. See Table 5 for the selected\nhyperparameters.\nNext we warmstart each models with the parameters from the best corresponding pretrained centralized\nmodel and train using FedAvg for 10K rounds. We sweep over clip norm and client learning rate. See\nTable 6 for the selected hyperparameters. Clip norm is omitted in Table 6, since for all hyperparameter\nsweeps 16 was the best value. The Book dataset outperforms the LM1B dataset in all model architectures\nacross LSTM and Transformer. Investigating the difference between the two datasets and their similarities\nto the Stackoverﬂow dataset to determine why Books always outperformed LM1B remains an interesting\nopen question.\nE Different optimizers\nIn an effort to improve communication efﬁciency of the larger language models, we examine two\ncommunication-efﬁcient federated algorithms: MimeLite and FedProx. By comparing the speed and point\nof convergence of these algorithms in number of rounds, we can determine if the overall communication\ncost of training can be decreased. As before, we ﬁx the model architectures for each class of model and\nconduct a basic search over learning hyperparameters using the same common search space as Table 3 with\nthe addition of the following algorithm speciﬁc hyperparameter sweeps. For MimeLite, we use Adagrad\n(Duchi et al., 2011) for the base optimizer as this setup was shown to perform the best by Karimireddy\net al. (2020) for Stack Overﬂow. For the MimeLite Adagrad base optimizer, we sweep over base learning\nrates of [0.01,0.03,0.1,0.3,1.0] and epsilons of [1e−1,1e−3,1e−5,1e−7] and ﬁx the server learning rate\nto 1.0. For FedProx, we sweep over µvalues of [0,0.1,0.01,0.001,0.0001] which controls the weight of\nthe L2 squared norm.\nWe report test perplexity over 10K federated training rounds with 800 clients per round in Figure 7\nand Table 7. While FedProx does slightly outperform FedAvg, it does not signiﬁcantly alter the speed of\ntraining in terms of number of communication rounds. Thus, we chose to continue using FedAvg in the\ncombination experiments for consistency across experiments and more accurate comparisons.\nF Combination of techniques\nFor the combination experiments, we conducted a joint search over a smaller range of hyperparameters for\neach technique to keep the total search space reasonable. For PVT, we restricted the possible percentages\nto 20%, 30%, and 40% of trainable variables as those were shown to yield good performance while\ncutting model size to less than half the original size. For uniform quantization, we restricted the search of\n18\nTable 6: Test set metrics after 10K communication rounds of training for each class of model and pretrain dataset.\nThe client learning rate listed is the best performing learning rate found from a hyperparameter sweep.Reported ∆\nmetrics are the change in quality relative to Table 2.\nModel Dataset # Clients Client Learning Rate ∆ Perplexity\n[0.01, 0.1, 0.5, 1.0, 2.0]\nSmall LSTM Book 200 1 .0 0 .24\nSmall LSTM Book 400 0 .5 1 .09\nSmall LSTM Book 800 0 .5 1 .66\nSmall LSTM LM1B 200 1 .0 0 .53\nSmall LSTM LM1B 400 0 .5 1 .72\nSmall LSTM LM1B 800 0 .5 2 .36\nLarge LSTM Book 200 0 .5 0 .59\nLarge LSTM Book 400 0 .1 0 .79\nLarge LSTM Book 800 0 .5 0 .94\nLarge LSTM LM1B 200 0 .5 0 .91\nLarge LSTM LM1B 400 0 .1 1 .09\nLarge LSTM LM1B 800 0 .5 1 .3\nSmall Transformer Book 200 0 .1 0 .35\nSmall Transformer Book 400 0 .1 1 .83\nSmall Transformer Book 800 0 .1 3 .34\nSmall Transformer LM1B 200 0 .1 0 .42\nSmall Transformer LM1B 400 0 .1 1 .97\nSmall Transformer LM1B 800 0 .1 3 .49\nLarge Transformer Book 200 0 .5 −1.92\nLarge Transformer Book 400 0 .1 −0.76\nLarge Transformer Book 800 0 .1 −0.04\nLarge Transformer LM1B 200 0 .1 −1.81\nLarge Transformer LM1B 400 0 .1 −0.64\nLarge Transformer LM1B 800 0 .1 0 .14\nupload to 6 or 8 bits and download to 16 or 32 bits since the Transformer was shown to be able to handle\naggressive upload quantization but required more care on download quantization. Finally, for transfer\nlearning, we warmstarted after pretraining on the Books corpus. As in previous experiments, we also\nsearch over the common hyperparameter space deﬁned in Table 3, where applicable.\nSimilar to previous experiments, we use 800 clients per round and train for 10K rounds with FedAvg.\nFigure 13 and Table 8 contain the results for the large models with and without the efﬁcient techniques\napplied. We apply two levels of quantization on download, 16 and 32 bits, and observe that the Large\nLSTM is more amenable to download quantization compared to the Large Transformer as the regression\nbetween the two levels is much smaller for the LSTM than the Transformer. However, the Transformer with\n16 bit download quantization still outperforms all efﬁcient LSTMs though it requires more communication\nrounds to do so than the efﬁcient Transformer with 32 bits for download. For the remaining analysis, we\nfocus on the efﬁcient Transformer using 32 bits for download. It is clear that for the Large Transformer,\napplying efﬁcient techniques yields better quality in earlier communication rounds. Although there are\nregressions in the ﬁnal model quality after 10K rounds of training, this could be attributed to previously\nobserved issues with increased amounts of labeled data diminishing the value pretraining (Zoph et al.,\n2020). However, the Efﬁcient Large Transformer still reaches the same ﬁnal perplexity as the Large\nLSTM which had no efﬁcient techniques applied. Furthermore, when considered in terms of actual\ncommunication cost, as is done in Figure 8, the efﬁcient models yield much better performance at smaller\ntotal communication costs.\n19\nTable 7: Test perplexity after 10K communication rounds of training for each class of model and federated algo-\nrithm.\nModel Algorithm Perplexity\nSmall LSTM FedAvg 34.80\nSmall LSTM MimeLite 34.81\nSmall LSTM FedProx 34.66\nSmall Transformer FedAvg 38.66\nSmall Transformer MimeLite 39.88\nSmall Transformer FedProx 38.57\nLarge LSTM FedAvg 30.83\nLarge LSTM MimeLite 31.00\nLarge LSTM FedProx 30.76\nLarge Transformer FedAvg 29.15\nLarge Transformer MimeLite 30.39\nLarge Transformer FedProx 29.04\nTable 8: Test perplexity and total communication costs in gigabytes after 10K communication rounds of training\nfor each class of model and setup. If the number of download bits is unspeciﬁed, the standard 32 bits was used.\nModel Download Cost (GB) Upload Cost (GB) \nSmall LSTM 188 188 34 .80\nSmall Transformer 164 164 38 .66\nLarge LSTM 752 752 30 .83\nLarge Transformer 840 840 29 .15\nEfﬁcient Large LSTM (download 32 bits) 752 75 32 .57\nEfﬁcient Large Transformer (download 32 bits) 840 84 30 .83\nEfﬁcient Large LSTM (download 16 bits) 376 75 32 .76\nEfﬁcient Large Transformer (download 16 bits) 420 84 32 .32\nFigure 13: Test perplexity over communication rounds for the large models with and without efﬁcient techniques\napplied.\nPerplex.\n20",
  "topic": "Scaling",
  "concepts": [
    {
      "name": "Scaling",
      "score": 0.58984375
    },
    {
      "name": "Computer science",
      "score": 0.5866047143936157
    },
    {
      "name": "Chen",
      "score": 0.5224612951278687
    },
    {
      "name": "Natural language processing",
      "score": 0.4002669155597687
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37133699655532837
    },
    {
      "name": "Mathematics",
      "score": 0.09619909524917603
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": []
}