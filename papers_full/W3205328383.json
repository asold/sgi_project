{
    "title": "NormFormer: Improved Transformer Pretraining with Extra Normalization",
    "url": "https://openalex.org/W3205328383",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4226508944",
            "name": "Shleifer, Sam",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3088846263",
            "name": "Weston, Jason",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226508943",
            "name": "Ott, Myle",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2970971581",
        "https://openalex.org/W2964583233",
        "https://openalex.org/W3016635207",
        "https://openalex.org/W3130602232",
        "https://openalex.org/W2963631907",
        "https://openalex.org/W2998617917",
        "https://openalex.org/W3034772996",
        "https://openalex.org/W3200543719",
        "https://openalex.org/W2890894339",
        "https://openalex.org/W2952509486",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2996908057",
        "https://openalex.org/W2946609015",
        "https://openalex.org/W3146097248",
        "https://openalex.org/W2466175319",
        "https://openalex.org/W3093960091",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W3129576130",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2462831000",
        "https://openalex.org/W3006439205",
        "https://openalex.org/W3118895645",
        "https://openalex.org/W3165647589",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2799054028",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W2963112338",
        "https://openalex.org/W2591954064",
        "https://openalex.org/W3035618017",
        "https://openalex.org/W3010768098",
        "https://openalex.org/W3098903812",
        "https://openalex.org/W3168171623",
        "https://openalex.org/W3035691519"
    ],
    "abstract": "During pretraining, the Pre-LayerNorm transformer suffers from a gradient magnitude mismatch: gradients at early layers are much larger than at later layers. These issues can be alleviated by our proposed NormFormer architecture, which adds three normalization operations to each layer: a Layer Norm after self attention, head-wise scaling of self-attention outputs, and a Layer Norm after the first fully connected layer. The extra operations incur negligible compute cost (+0.4% parameter increase), but improve pretraining perplexity and downstream task performance for both causal and masked language models ranging from 125 Million to 2.7 Billion parameters. For example, adding NormFormer on top of our strongest 1.3B parameter baseline can reach equal perplexity 24% faster, or converge 0.27 perplexity better in the same compute budget. This model reaches GPT3-Large (1.3B) zero shot performance 60% faster. For masked language modeling, NormFormer improves fine-tuned GLUE performance by 1.9% on average. Code to train NormFormer models is available in fairseq https://github.com/pytorch/fairseq/tree/main/examples/normformer .",
    "full_text": "Under review as a conference paper at ICLR 2022\nNORM FORMER : I MPROVED TRANSFORMER\nPRETRAINING WITH EXTRA NORMALIZATION\nSam Shleifer Jason Weston Myle Ott\nFacebook AI Research∗\nABSTRACT\nDuring pretraining, the Pre-LayerNorm transformer suffers from a gradient mag-\nnitude mismatch: gradients at early layers are much larger than at later layers.\nThese issues can be alleviated by our proposed NormFormer architecture, which\nadds three normalization operations to each layer: a Layer Norm after self at-\ntention, head-wise scaling of self-attention outputs, and a Layer Norm after the\nﬁrst fully connected layer. The extra operations incur negligible compute cost\n(+0.4% parameter increase), but improve pretraining perplexity and downstream\ntask performance for both causal and masked language models ranging from 125\nMillion to 2.7 Billion parameters. For example, adding NormFormer on top of\nour strongest 1.3B parameter baseline can reach equal perplexity 24% faster, or\nconverge 0.27 perplexity better in the same compute budget. This model reaches\nGPT3-Large (1.3B) zero shot performance 60% faster. For masked language mod-\neling, NormFormer improves ﬁne-tuned GLUE performance by 1.9% on average.\nCode to train NormFormer models is available in fairseq.\n1 I NTRODUCTION\nThe original transformer architecture (Vaswani et al., 2017) applies Layer Normalization (Ba et al.,\n2016) after each sublayer’s residual connection (“Post-LN”) in order to reduce the variance of the\ninputs to the following sublayer, i.e.:\nPostLN(x) = LayerNorm(x+ Sublayer(x)),\nwith\nLayerNorm(x) = x−E[x]√\nVar[x] +ϵ\n·γ+ β,\nwhere γ and β are trainable parameters, and ϵis a small constant. Recent work has observed that\nPost-LN transformers tend to have larger magnitude gradients in later layers compared to earlier\nlayers (Xiong et al., 2020) and has advocated moving the LayerNorm operation to the beginning of\neach sublayer (“Pre-LN”; see Figure 1, left), i.e.:\nPreLN(x) =x+ Sublayer(LayerNorm(x)).\nIn practice Pre-LN transformers can be trained with larger learning rates, shorter learning rate\nwarmup and often yield improved performance compared to Post-LN transformers (Xiong et al.,\n2020), so most recent, large pretrained language models tend to use Pre-LN transformers (Baevski\n& Auli, 2019; Radford et al., 2019; Raffel et al., 2020; Brown et al., 2020; Lieber et al., 2021).\nIn this work we show that, while Pre-LN improves stability over Post-LN, it has the opposite\nside effect: gradients at earlier layers tend to be larger than gradients at later layers. We propose\nNormFormer, which alleviates the gradient magnitude mismatch by adding 3 normalization oper-\nations to each layer (see Figure 1, middle). These operations reduce gradients to early layers and\nincrease gradients to later layers, bringing their magnitudes closer together.\n∗Jason implemented residual scaling and helped with writing. Myle helped with writing and hardware\nissues. Thanks to Tim Dettmers for giving us early access to the Adam8Bit Optimizer, and to Naman Goyal,\nXian Li, Susan Zhang, Zoe Shleifer and Oﬁr Press for valuable comments. Correspondence to sshleifer@\nfb.com.\n1\narXiv:2110.09456v2  [cs.CL]  1 Nov 2021\nUnder review as a conference paper at ICLR 2022\nFigure 1: Left: a baseline Pre-LayerNorm transformer layer. Center: NormFormer, with the three\nproposed additions in bold. Right: a single attention head with our proposedHeadScale operation\napplied prior to the output projection with trainable parameters γi. * When applied, residual scaling\nimpacts the second residual connection in each layer.\nCompared to compute-matched, well-tuned Pre-LN baselines, NormFormer models reach target\npretraining perplexities faster and achieve better pretraining perplexities and downstream task per-\nformance.\nThe rest of this paper is organized as follows: Section 2 describes the proposed modiﬁcations, Sec-\ntion 3 shows pretraining and downstream task performance for fully trained NormFormer models\nagainst well-tuned, compute-matched baselines. Section 4 shows the gradient mismatch introduced\nby Pre-LN and howNormFormer alleviates it. Section 4.2 analyzes residual scaling, a related tech-\nnique proposed to stabilize Post-LN architectures (Xiong et al., 2020; Zhu et al., 2021). Section 5\nshows that removing any of the added operations degrades performance and that NormFormer\nimproves over the baseline at a wide range of hyperparameter conﬁgurations.\n2 A PPROACH\n2.1 N ORM FORMER\nNormFormer includes three modiﬁcations to the Pre-LN transformer: First, we apply head-wise\nscaling inside the attention module and add two additional LayerNorm operations: one after the\nattention module and a second after the ﬁrst fully connected layer. The modiﬁcations introduce a\nsmall number of additional learnable parameters, which provide a cost-effective way for each layer\nto change the magnitude of its features, and therefore the magnitude of the gradients to subsequent\ncomponents. The changes are visualized in Figure 1 and described below.\nScaling Attention Heads The standard multi-head attention operation is deﬁned as:\nMultiHeadAttention(Q,K,V ) = Concat(h1,..., hn)WO\nhi = Attention(QWQ\ni ,KW K\ni ,VW V\ni )\nAttention(Q,K,V ) = softmax\n(QKT\n√dk\n)\nV,\nwhere nis the number of heads, iis the attention head index, dk is the dimensionality of the keys\nand WO,WQ\ni ,WK\ni ,WV\ni are learned projection matrices for the output, query, key and value, re-\nspectively.\nWe propose scaling the output of each attention head via learned scalar coefﬁcients γi:\nHeadScaleMHA(Q,K,V ) = Concat(γ1h1,...,γ nhn)WO\nwhere γare learnable parameters initialized to 1.\n2\nUnder review as a conference paper at ICLR 2022\nModel Size GPT-3 Paper Baseline NormFormer\n125M 6e-4 3e-3 3e-3\n355M 3e-4 1e-3 1e-3\n1.3B 2e-4 6e-4 6e-4\nTable 1: Searching for learning rates on our dataset results in higher values than reported in Brown\net al. (2020), providing stronger baselines to compare to our NormFormer architecture.\nAdditional Layer Normalization and Putting it All TogetherIn the Pre-LN transformer each\nlayer lmodiﬁes an input xl as follows:\nxPreLN\nl+1 = FFN(MHA(xl))\nwhere MHA( x) =x+ MultiHeadAttention(LN(x),LN(x),LN(x))\nFFN(x) =x+ σ(LN(x)W1 + b1)W2 + b2\nLN(x) = LayerNorm(x)\nIn this work σis the GELU non-linear activation introduced in Hendrycks & Gimpel (2016).\nOur overall method, NormFormer, instead modiﬁes each input xl as:\nxNormFormer\nl+1 = NormFFN(NormScaledMHA(xl))\nwhere NormScaledMHA( x) =x+ LN(HeadScaleMHA(LN(x),LN(x),LN(x)))\nNormFFN(x) = x+ LN(σ(LN(x)W1 + b1))W2 + b2\nwhere bolded operations are newly introduced.\n2.2 E XPERIMENTS\nCausal Language Models We pretrain causal LMs (CLM) that roughly match the “Small” (125M\nparameter), “Medium” (355M), “Large” (1.3B) and “XL” (2.7B) sizes from Brown et al. (2020).\nOur model architecture differs from Brown et al. (2020) in two ways: (1) we use only dense at-\ntention, while they alternate between dense and locally banded sparse attention; (2) we train our\nmodels with sinusoidal positional embeddings, following Shortformer (Press et al., 2020b), since\nearly experiments found this to produce comparable results with fewer learned parameters.\nWe train the baseline models for 300 billion tokens. We train NormFormer models for an equiv-\nalent number of GPU hours, which typically results in 2-6% fewer steps and tokens due to the\nadditional overhead of the normalization operations.\nOn our dataset, we ﬁnd that the learning rates proposed in GPT-3 are suboptimally low. 1\nFor both baseline and NormFormer at each size besides 2.7B, we tune the learning rate\nby training models for 50,000 steps and selecting the best performing learning rate among:\n{1e−4,6e−4,3e−4,6e−4,1e−3,3e−3}. The learning rates we obtained from this process, shown\nin Table 1, are 3-5 times larger than those used in the GPT-3 paper. Additionally, we have veriﬁed\nthat the baseline and NormFormer both perform worse at the full training budget with the GPT-3\nlearning rates than with the higher learning rates. Other hyperparameters do not differ from GPT-3.2\nResidual Scaling Standard Post-LN transformers simply sum the previous output (residual) with\nthe new output. Recent work attempts to stabilize Post-LN architectures by weighting the residual\nconnection for each layer (Zhu et al., 2021; Liu et al., 2020). We thus experiment with scaling the\nresidual in each embedding dimension via learned scalar coefﬁcients (λresid)i:\nResScale(x) =λresid ◦x+ Sublayer(LayerNorm(x))\n1The difference in optimal learning rates may be due partly to architectural differences between our baseline\nand GPT-3 (e.g., not using locally banded sparse attention).\n2See Table 2.1 in Brown et al. (2020).\n3\nUnder review as a conference paper at ICLR 2022\nwhere ◦is elementwise multiplication, and λresid are learned parameters initialized to 1.\nWhile this can be applied at any normalization layer, we ﬁnd it it most effective for normalizing the\nfeedforward network (FFN) submodule for the smaller sized language models. In this setting,\nNormFFN(x) =λresid ◦ x+ LN(σ(LN(x)W1 + b1))W2 + b2\nFor 1.3B parameter models and larger, scaling residuals hurts performance (see discussion in Sec-\ntion 4.2), so ResScale is not used in our 1.3B and 2.7B CLM results.\nLarge scale experiments We also train three large-scale models with 2.7B parameters. Our ﬁrst\nbaseline is a replicated version of GPT-3-2.7B with GELU activations, the published learning rate\n(1.6e-4) and the same number of training steps and tokens (286K steps; 300B tokens). This model\nslightly exceeds the reference zero shot performance (Brown et al., 2020). Next, we train two\nvariants of GPT3-2.7B with Relu2 activations (So et al., 2021), but use slightly fewer training steps\n(20% less) for compute efﬁciency. The ﬁrst of these uses the baseline learning rate (1.6e-4) and the\nsecond uses NormFormer-2.7B with a higher learning rate of 6e-4. We note that training baseline\n2.7B CLMs (i.e., without NormFormer modiﬁcations) with a higher 6e-4 learning rate diverged\nand failed to train. However, as opposed to the smaller architectures, we did not exhaustively tune\nthe learning rate, so it is possible that an intermediate value would perform better.\nZero Shot Evaluation In addition to validation perplexity, we evaluate CLMs on a subset of the\ntasks that GPT3 evaluated on in a zero-shot setting (Brown et al., 2020), with the same prompts.\nWe select WinoGrande (Sakaguchi et al., 2020), StoryCloze (Mostafazadeh et al., 2016), Open-\nBookQA (Mihaylov et al., 2018), HellaSwag (Zellers et al., 2019) and PIQA (Bisk et al., 2020)\nbecause GPT3 showed strong performance on these tasks at small scale, as well as consistently\nimproving performance with scale.\nMasked Language Models (MLM) We adopt the RoBERTa-base, Pre-LN architecture and hy-\nperparameters used in Liu et al. (2019). For the baseline, we pretrain for 2 million batches of 1\nmillion tokens, about 1\n4 of the training budget of the original roberta-base. NormFormer runs\nthrough 1.92 million batches in the same amount of time.\nFine-Tuning We ﬁne-tune both the baseline MLM and NormFormer with learning rates\n1e−5,1e−4,3e−4,1e−3,3e−3,6e−3 and report the best performance on the validation set for\neach GLUE task (Wang et al., 2019), following Liu et al. (2019). Other ﬁne-tuning hyperparam-\neters match those used for roberta-base in Liu et al. (2019).\nPretraining data We pretrain all models on a collection of English language text including the\nEnglish portion of the CC100 corpus (Conneau et al., 2020) as well as the data from Liu et al. (2019),\nconsisting of BookCorpus (Zhu et al., 2019), English Wikipedia and ﬁltered subsets of Common\nCrawl. We encode our data with the byte-level Byte Pair Encoding (BPE) vocabulary from Liu\net al. (2019), originally introduced in Radford et al. (2019). The combined dataset contains around\n450GB of uncompressed text and 110B BPE tokens. We hold out 40M BPE tokens from this data\nas a validation set on which we report pretraining perplexities.\nImplementation details We train our causal and masked language models infairseq (Ott et al.,\n2019; Paszke et al., 2019). Although NormFormer introduces fewer than 0.07% additional parame-\nters, it slows individual training updates and increases memory usage between 2% (2.7B model) to\n6% (125M model) due to the FFN LNs. Accordingly, we compare NormFormer to baseline models\ntrained for an equal amount of GPU time, i.e., controlling for compute rather than the number of\ntraining updates. Finally, we note that the HeadScale operation can be moved outside the self\nattention module to allow the use of the very efﬁcient pytorch F.multihead attention. This\nchange reduces overhead without noticeable performance degradation.\n3 R ESULTS\nWe report pretraining perplexities for CLMs and MLMs as a function of training wall-time (GPU\ndays) in Figure 2. We observe that NormFormer trains signiﬁcantly faster and achieves better vali-\n4\nUnder review as a conference paper at ICLR 2022\n|θ| LR Relu2 λresid Steps PPL HS PI WG SC OB Avg\nRandom Baseline - - - - - - 25.0 50.0 50.0 50.0 25.0 40.0\nGPT3-125M (paper) 124.4 6e-4 - - 572K - 33.7 64.6 52.0 63.3 35.6 49.8\nGPT3-125M (replicated) 124.4 6e-4 - - 572K 21.11 33.7 66.5 52.2 66.1 35.4 50.8\nGPT3-125M (High LR) 124.4 3e-3 - - 572K 21.09 35.3 67.5 50.5 66.3 35.0 50.9\nNormFormer-125M 124.5 3e-3 - - 540K 20.34 34.9 67.1 52.3 66.3 38.0 51.7\nNormFormer-125M 124.5 3e-3 - 539K 20.11 34.9 65.9 53.4 67.5 40.0 52.3\nGPT3-355M (paper) 354.7 3e-4 - - 572K - 43.6 70.2 52.1 68.5 43.2 55.5\nGPT3-355M (replicated) 354.7 3e-4 - - 572K 15.41 46.1 70.8 54.6 71.1 41.2 56.8\nGPT3-355M (High LR) 354.7 1e-3 - - 572K 14.85 48.4 71.7 53.8 73.3 43.4 58.1\nNormFormer-355M 355.0 1e-3 - - 552K 14.54 49.7 71.8 56.0 73.8 43.6 59.0\nNormFormer-355M 355.0 1e-3 - 550K 14.52 49.7 72.0 56.7 73.2 43.8 59.1\nGPT3-1.3B (paper) 1313.5 2e-4 - - 286K - 54.7 75.1 58.0 73.4 46.8 61.6\nGPT3-1.3B (replicated) 1313.5 2e-4 - - 286K 12.56 58.5 74.6 58.1 76.8 49.4 63.5\nGPT3-1.3B (High LR) 1313.5 6e-4 - - 286K 12.21 57.5 74.3 59.3 76.3 50.8 63.6\nNormFormer-1.3B 1314.0 6e-4 - - 275K 11.94 60.5 74.5 60.1 77.5 50.8 64.7\nGPT3-2.7B (paper) 2648.7 1.6e-4 - - 286K - 62.8 75.6 62.3 77.2 53.0 66.2\nGPT3-2.7B (replicated) 2648.7 1.6e-4 - - 286K 10.92 65.9 76.6 61.4 78.2 49.6 66.3\nNormFormer-2.7B 2649.5 6e-4 - 277K 10.55 68.1 78.1 64.4 79.4 53.4 68.7\nGPT3-2.7B-Relu 2648.7 1.6e-4 - 230K 10.99 65.9 76.1 63.2 79.3 49.4 66.8\nGPT3-2.7B-Relu 2648.7 6e-4 - 28K diverged\nNormFormer-2.7B 2649.5 6e-4 - 222K 10.73 67.4 77.2 64.4 78.9 52.6 68.1\nTable 2: Zero-Shot Accuracy for Causal LMs for the following tasks: HS: HellaSwag, PI: PIQA,\nWG: WinoGrande, SC: StoryCloze, OB: OpenBookQA. PPL is validation perplexity during pretrain-\ning. GPT-3 (paper)results taken from Brown et al. (2020). Horizontal lines group compute-matched\nruns. High LR corresponds to using a larger learning rate than reported in Brown et al. (2020).λresid\nindicates whether residual scaling was used. λresid did not help at 1.3B scale, as shown in 2, but\nthat run is not compute matched so it is not included here. Model size (|θ|) is reported in millions of\nparameters.\nFigure 2: Pretraining perplexity on held-out validation data for Causal and Masked Language Mod-\nels as a function of training compute (GPU days). The blue stars show the point where a model\nmatches the baseline’s lowest perplexity.\ndation perplexities for a given training compute budget. The blue stars mark the ﬁrst validation step\nwhere NormFormer matches the baseline’s lowest perplexity and shows that NormFormer matches\nPre-LN models while needing only 60% and 57% as much compute for CLM and MLM models,\nrespectively. This is particularly impressive since NormFormer models take 2-6% longer for each\ntraining step and thus see less data than Pre-LN models in this comparison. The left side blue line\nin Figure 2 shows the failed attempt to add ResScale to NormFormer-1.3B.\nWe observe a similar trend on downstream tasks. In Table 2 we report zero shot accuracy for causal\nLMs using the tasks and prompts from Brown et al. (2020). NormFormer outperforms GPT-3 at all\nsizes. The gains from Normformer extra parameters operations outpace the gains from normal\n5\nUnder review as a conference paper at ICLR 2022\nModel Size λresid PPL CoLA MNLI MRPC QNLI QQP RTE SST-2 Avg\nBaseline 125.42 - 3.42 74.3 85.9 84.6 91.6 90.7 66.4 92.9 83.77\nNormFormer 125.50 - 3.31 82.6 86.3 86.0 91.9 91.3 67.9 93.8 85.69\nNormFormer 125.51 3.29 80.9 86.2 85.3 91.5 91.2 62.8 94.2 84.59\nTable 3: Masked LM: Pretraining validation perplexity (PPL) and ﬁne-tuned performance on GLUE\ntasks for Pre-LN and NormFormer models. Note that models are trained for an equal amount of\ncompute, which is less than the publicly-released roberta-base models.\nFigure 3: Average L1 norm of gradients to the second fully connected weight for layers 0,1,6,10 and\n11, early in training.\nscaling laws. Changing the hidden dimension of a 125M parameter model from 768 to 780, for\nexample, results in a 127 million parameter model that is only 0.08 perplexity better than the baseline\nwhereas NormFormer-125M adds only 100,000 parameters and is 0.83 perplexity better than the\nbaseline.\nFor MLM models, we report ﬁne-tuned accuracy on GLUE in Table 3. We again ﬁnd that Norm-\nFormer MLM models outperform their Pre-LN counterparts on every task (rows 1 vs 2). Adding\nResScale improves improves pre-training performance marginally (3.29 valid PPL vs 3.31), but\nthe gains to do not translate to ﬁnetuned performance.\n4 A NALYSIS\n4.1 A NALYSIS OF GRADIENT NORMS BY LAYER\nWe begin by examining the magnitude of the gradients at different layers for Post-LN, Pre-LN and\nNormFormer models, since large magnitude differences in gradients across layers can destabilize\ntraining, particularly when training in mixed precision (Micikevicius et al., 2018). Figure 3 shows\nthe average L1 norm of the gradients to the second fully connected weight in various layers for a 12\nlayer, 125M parameter CLM model at the beginning of training. As reported in past work (Xiong\net al., 2020), we observe that the gradients to later layers in Post-LN models are much larger than\nfor earlier layers, and that the gradients to early layers quickly vanish in the early stages of training.\nPre-LN models have the opposite behavior, with early layers instead receiving signiﬁcantly larger\ngradients than later layers. NormFormer brings the average gradient norms closer together for\ndifferent layers in the network.\nIn Figure 4 we present the distribution of scaling parameters learned by NormFormer models.\nFor the FFN LN, the γ parameters are smaller for earlier layers, reducing the magnitude of the\ninputs to early fully connected parameters, thereby decreasing the magnitude of their gradients.\nThe post attention LN, in the middle of Figure 4, all layers have γ coefﬁcients below 1, indicating\n6\nUnder review as a conference paper at ICLR 2022\nFigure 4: Distribution of learned scaling parameters in three of the added operations. For FFN LN,\nearlier layers receive downscaled inputs, keeping their gradients in the same range as the gradients\nof later layers. This plot is discussed in detail in Section 4.\nFigure 5: LR Stability Test: learning rate starts from 0 and linearly increases by5e-5 at each train-\ning step until training destabilizes. NormFormer reaches a higher learning rate before destabilizing.\nEach data point is the median of 3 runs with a different random seed.\ndownscaling.3 The HeadScale γ parameters, shown in the rightmost plot in Figure 4 vary more\nthan the others, and have no relationship with depth in the network. We interpret this as evidence\nthat the HeadScale parameters dynamically increase the importance of well initialized attention\nheads, as suggested in Chen et al. (2021).\nOne result of reducing the gradient mismatch, besides better perplexities and downstream task per-\nformance, is the ability to train stably with larger learning rates. To measure the stability of an\narchitecture, we train it on a learning rate schedule with a very large peak learning rate, so that the\nlearning rate increases a little each step until the loss explodes. Figure 5 shows that NormFormer\nmodels can survive for more updates in this environment than the baseline. For the baseline 125M\nmodel (the left most blue dot), the loss eventually explodes, with the activations from multiplying\nthe query and key features at layer 0 overﬂowing the FP16 range. The down scaling of the attention\noutputs allows NormFormer to avoid this issue and remain stable with larger learning rates. Figure 5\nalso shows that λresid reduces the stability improvement at all sizes.\n3The downscaling is also apparent in Figure 7 in the Appendix, which plots the change in grad norm for\neach operation at each layer. It shows that adding extra normalization reduces the gradient norm for all attention\nparameters at every layer. Only FFN parameters at later layers, have increased gradient norms.\n7\nUnder review as a conference paper at ICLR 2022\n4.2 R ESIDUAL SCALING\nBy comparing adjacent NormFormer-125M and NormFormer-355M rows in Table 2 we can see\nthat adding ResScale to NormFormer improves perplexity and zero shot performance for small\nscale CLMs. For 125M parameter MLM, ResScale improves pre-training perplexity marginally,\nbut hurts ﬁne-tuned performance. At 1.3 billion parameter scale, however, adding ResScale to\nNormFormer does not improve performance (Figure 2). Although it’s not included in our tables,\nwe ﬁnd that ResScale without NormFormer is stronger than the baseline at small scale, but not\nlarge scale. This suggests that the negative result is caused by scale, rather than interaction with\nNormFormer.\nFigure 6 shows the Avg. λresid weights at each layer of different sized CLMs. We can see that at\n125M and 355M parameters, the weights in the later layers are lower, indicating down weighting\nof the residual connection, whereas at the largest scale, 1.3B, the weights are larger deeper into the\nnetwork.\nAdding the λresid parameters to the other (earlier) residual connection in each layer, or using a\nscalar instead of a vector for each λresid, does not ﬁx the large scale issue, but hurts small scale\nperformance marginally.\nFigure 6: λresid weights at each layer of different sized CLMs in theNormFormer+λresid setting.\nDepth is layer number / total layers.\n5 A BLATIONS\nThis section provides evidence that removing any of our additions to the transformer block degrades\nperformance on language modeling tasks, and that our additions improve language modeling per-\nformance across a wide range of hyperparameter settings. Experiments use 125M parameter CLMs,\nand are run with the default hyperparameters given in Table 7 in the appendix for 470 V100 Hours\n(100,000 updates for the baseline) unless otherwise mentioned.\nRemoving any of the added operations hurts performanceTable 4 shows that none of the\nfour introduced operations can be removed without degrading performance. Rows 2-5 remove each\noperation one at a time. In all cases perplexity increases, with the removal of HeadScale being\nthe most damaging and the removal of the Post-Attn LN being the least damaging. In Row 6 ( + 3\nMore LN) we try to introduce more normalization inside self attention, applying LN to the query,\nkey and value features in addition to our 3 other operations, for a total of 6 new operations. In this\nsetting, every other parameterized operation inside the transformer layer is an LN. We ﬁnd that this\ndoes not change perplexities at a ﬁxed number of updates, but reduces training speed by another\n5%. This result suggests that there is not much upside to adding even more normalization on top of\nNormFormer.\n8\nUnder review as a conference paper at ICLR 2022\nArchitecture Valid PPL\nNormFormer+ResScale 15.88\n- Post-Attn LN 15.92\n- FFN LN 16.14\n- Head Scale 16.22\n- Res Scale 16.20\n+ 3 More LN 15.88\nBaseline 16.37\nTable 4: 125M parameter Language Modeling Validation perplexities after 470 V100 Hours of\npretraining. Removing any of our proposed additions degrades performance (Rows 2-5). Adding\nmore normalization inside the Multi Headed Attention (Row 6) does not impact perplexity at a ﬁxed\nnumber of updates, but reduces throughput such that the model can only complete 87,500 updates vs.\n92,500 for Rows 1-5 and 100,000 for Row 7. Note that these PPL scores are not directly comparable\nto other tables – they use a different validation set.\nOther Experiments Replacing the FFN LN with the FFNGeGlu proposed in Shazeer (2020),\nwhich includes scaling but no normalization, degraded performance in our 125M parameter CLM\nsetting, the only place we tried it. We also ﬁnd that the LN variant proposed in Raffel et al. (2020),\nwhich removes the bias and the mean substraction from the normalization, performs equally well\nto our LN and has fewer trainable parameters, but is about 2x slower than the FusedLayerNorm\nimplementation we use. We therefore do not adopt it.\nDing et al. (2021) propose related stabilization strategies for text to image generation tasks with\nlarger models including a downscaled embedding gradient, a layer norm after the ﬁnal fully con-\nnected layer, and the same post-attention LN. We ﬁnd that, besides the post attention LN, these\ntechniques do not help in our setting.\nTable 5 in the appendix shows language modeling perplexities for 7 different hyperparameter con-\nﬁgurations, separated by horizontal lines. NormFormer outperforms the baseline in all settings.\n6 R ELATED WORK\nLayer normalization (Ba et al., 2016) is an important component of the transformer architecture.\nXiong et al. (2020) shows that for Post-LN: gradients are too big for later layers and solves this\nproblem with Pre-LN. We build on the Pre-LN architecture to make it even more stable and efﬁcient.\nPress et al. (2020a) proposes an architecture where instead of interleaving attention and feed forward\nsublayers, the attention all happens ﬁrst. This increases the number of late FFN parameters, rather\nthan increasing their importance and gradient norm, as our FFN LN does, and does not impact\nstability.\nOur HeadScale operation is related to that used in Chen et al. (2021), but used differently.\nWhereas that work prunes attention heads with low γ parameters, we use the γ parameters to im-\nprove pretraining performance.\nThese approaches are also related to techniques for initializing neural networks: GradInit (Zhu\net al., 2021) introduces a set of scalars and biases for initialization based on a variance heuristic, and\nAdmin (Liu et al., 2020) applies a similar heuristic in proﬁling and initialization stages. These works\nalso use variants of our ResScale operation, which we ﬁnd helpful at small scale and harmful at\nlarge scale.\nSimilarly, some other approaches targeted initialization as well, in particular ReZero (Bachlechner\net al., 2020), FixUp (Huang et al., 2020) and LookLinear (Balduzzi et al., 2017). We note that\nDALL-E (Ramesh et al., 2021) also added a per residual scaling factor (only during backprop). Our\napproach, in contrast, only has new learnable parameters without variance heuristics, and has no\nextra stages or changes in initialization.\n9\nUnder review as a conference paper at ICLR 2022\n7 C ONCLUSION\nWe identify a mismatch in the gradients of Pre-LN transformer weights: earlier layers receive much\nlarger gradients than later layers, while the optimal scaling of residuals is larger at earlier layers than\nat later layers. We propose NormFormer, which alleviates these issues by adding 3 extra opera-\ntions to each transformer layer. These modiﬁcations help the gradient mismatch for fully connected\nparameters and improve validation perplexity and downstream task performance for both causal and\nmasked language models. None can be removed without degrading performance back towards the\nbaseline, and adding more normalization – at least of the types we have tried – does not improve\nperformance. Since NormFormer primarily addresses the gradient mismatch by increasing the gra-\ndients to the last FFN layers while decreasing the gradient magnitudes in other parts of the network,\nfuture work could examine whether all 3 operations need to be added to every layer. Additionally,\nthe small computational overhead associated with NormFormer could be alleviated by fusing the\nFFN LN with the preceding fully connected layer, with or without the mean centering and bias,\nwhich do not appear to improve pretraining perplexity. In general, we have shown that adding small\nnumbers of learnable parameters in the right places in our architectures can alleviate certain issues\nin current state of the art networks. Future work should ascertain if there are additional similarly\nefﬁcient modiﬁcations that can bring gains, while helping us understand current deﬁciencies further.\n10\nUnder review as a conference paper at ICLR 2022\n8 A PPENDIX\nLearning Rate Setting Changes Valid PPL\nBaseline 0.001 - 16.80\nNormFormer 0.001 - 16.33\nBaseline 0.003 - 16.37\nNormFormer 0.003 - 15.88\nBaseline 0.006 - 16.58\nNormFormer 0.006 - 16.22\nBaseline 0.003 Longer Warmup 16.50\nNormFormer 0.003 Longer Warmup 16.06\nBaseline 0.003 GPT3 16.29\nNormFormer 0.003 GPT3 15.88\nBaseline 0.003 Clip Grad Norms at 0.1 16.46\nNormFormer 0.003 Clip Grad Norms at 0.1 16.14\nTable 5: Longer Warmup: increase LR Warmup to 6,000 steps (from 500). GPT3: increase sequence\nlength to 2048, increase dropout to 0.1, increase training budget to 1,000 V100 hours. Grad Clip:\nclip gradient norms at 0.1. NormFormer outperforms the baseline in all settings.\nWikitext103 Table 6 shows that NormFormer can also provide gains on top of a well tuned lan-\nguage model in settings with much less data. We simply add our three operations to the architecture\nand hyperparameters of Baevski & Auli (2019). Convergence perplexity improves, and we reach the\nbaseline perplexity in 70% as many steps. In this setting, NormFormer does not improve in the\nlast 30% of training, which suggests that with more tuning the perplexity gap could be widened.\nSteps to Final PPL PPL\nBaseline 100% 18.70\nNormFormer 70% 18.65\nTable 6: Wikitext 103 results following Baevski & Auli (2019).Steps to Final PPL: at what\npercentage of the 280K steps did the model reach 18.70 perplexity. PPL: Best Perplexity\nLearning Rate 0.003\nBatch Size 524K Tokens\nParameters 124M+\nLayers 12\nLayer Dimension 768\nDropout 0\nLR Warmup Updates 500\nLR Scheduler Linear Decay\nSequence Length 1024\nTrain Budget 470 V100 Hours\nTable 7: Hyperparameters for ablations in Tables 4 and 7. This train budget allows the baseline\nmodel to run for 100,000 updates.\n11\nUnder review as a conference paper at ICLR 2022\nFigure 7: Change in grad norm with each operation of NormFormer compared to the baseline.\nNorms are the average between step 950 and 1000, normalized to control for different losses. 2.0\non the Y axis means the gradient to a parameter is twice as large as the baseline, on average. The\nNormFormer increases the norm to fully connected parameters in later layers, while reducing the\ngradient norm to attention parameters at all layers. The results are discussed in detail in Section 4.\nREFERENCES\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016.\nThomas Bachlechner, Bodhisattwa Prasad Majumder, Huanru Henry Mao, Garrison W Cottrell,\nand Julian McAuley. Rezero is all you need: Fast convergence at large depth. arXiv preprint\narXiv:2003.04887, 2020.\nAlexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In\nInternational Conference on Learning Representations, 2019. URL https://openreview.\nnet/forum?id=ByxZX20qFQ.\nDavid Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams.\nThe shattered gradients problem: If resnets are the answer, then what is the question? In Interna-\ntional Conference on Machine Learning, pp. 342–350. PMLR, 2017.\nYonatan Bisk, Rowan Zellers, Ronan Le bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about\nphysical commonsense in natural language. Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, 34(05):7432–7439, Apr. 2020. doi: 10.1609/aaai.v34i05.6239. URL https://\nojs.aaai.org/index.php/AAAI/article/view/6239.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-\nwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-\ndlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot\nlearners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-\nvances in Neural Information Processing Systems , volume 33, pp. 1877–1901. Curran Asso-\nciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nXiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Zhangyang Wang, and Jingjing Liu. Earlybert:\nEfﬁcient bert training via early-bird lottery tickets, 2021.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek,\nFrancisco Guzm ´an, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Un-\nsupervised cross-lingual representation learning at scale. 2020.\n12\nUnder review as a conference paper at ICLR 2022\nMing Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou,\nZhou Shao, Hongxia Yang, and Jie Tang. Cogview: Mastering text-to-image generation via\ntransformers, 2021.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415, 2016.\nXiao Shi Huang, Felipe Perez, Jimmy Ba, and Maksims V olkovs. Improving transformer opti-\nmization through better initialization. In International Conference on Machine Learning , pp.\n4475–4483. PMLR, 2020.\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and evalua-\ntion. Technical report, AI21 Labs, August 2021.\nLiyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the difﬁ-\nculty of training transformers. arXiv preprint arXiv:2004.08249, 2020.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed\nprecision training. In International Conference on Learning Representations, 2018.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor con-\nduct electricity? a new dataset for open book question answering. In Proceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing , pp. 2381–2391,\nBrussels, Belgium, October-November 2018. Association for Computational Linguistics. doi:\n10.18653/v1/D18-1260. URL https://www.aclweb.org/anthology/D18-1260.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Van-\nderwende, Pushmeet Kohli, and James Allen. A corpus and cloze evaluation for deeper under-\nstanding of commonsense stories. In Proceedings of the 2016 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies , pp.\n839–849, San Diego, California, June 2016. Association for Computational Linguistics. doi:\n10.18653/v1/N16-1098. URL https://www.aclweb.org/anthology/N16-1098.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\nand Michael Auli. FAIRSEQ : A fast, extensible toolkit for sequence modeling. 2019.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep\nlearning library. pp. 8024–8035, 2019.\nOﬁr Press, Noah A. Smith, and Omer Levy. Improving transformer models by reordering their\nsublayers, 2020a.\nOﬁr Press, Noah A Smith, and Mike Lewis. Shortformer: Better language modeling using shorter\ninputs. arXiv preprint arXiv:2012.15832, 2020b.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. Technical report, OpenAI, 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. 21:1–67, 2020.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021.\n13\nUnder review as a conference paper at ICLR 2022\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An ad-\nversarial winograd schema challenge at scale. Proceedings of the AAAI Conference on Artiﬁ-\ncial Intelligence, 34(05):8732–8740, Apr. 2020. doi: 10.1609/aaai.v34i05.6399. URL https:\n//ojs.aaai.org/index.php/AAAI/article/view/6399.\nNoam Shazeer. Glu variants improve transformer, 2020.\nDavid R. So, Wojciech Ma´nke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V . Le. Primer:\nSearching for efﬁcient transformers for language modeling, 2021.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, 2017.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang,\nYanyan Lan, Liwei Wang, and Tie-Yan Liu. On layer normalization in the transformer architec-\nture, 2020.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a\nmachine really ﬁnish your sentence? In Proceedings of the 57th Annual Meeting of the Associ-\nation for Computational Linguistics , pp. 4791–4800, Florence, Italy, July 2019. Association for\nComputational Linguistics. doi: 10.18653/v1/P19-1472. URL https://www.aclweb.org/\nanthology/P19-1472.\nChen Zhu, Renkun Ni, Zheng Xu, Kezhi Kong, W Ronny Huang, and Tom Goldstein. Gra-\ndinit: Learning to initialize neural networks for stable and efﬁcient training. arXiv preprint\narXiv:2102.08098, 2021.\nYukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching\nmovies and reading books. arXiv preprint arXiv:1506.06724, 2019.\n14"
}