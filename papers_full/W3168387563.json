{
  "title": "Layered gradient accumulation and modular pipeline parallelism: fast and efficient training of large language models",
  "url": "https://openalex.org/W3168387563",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4303088100",
      "name": "Lamy-Poirier, Joel",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2903697572",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3119866685",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2902280036",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2991040477",
    "https://openalex.org/W2763421725",
    "https://openalex.org/W3129831491",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3153553004",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2951781666",
    "https://openalex.org/W2338908902",
    "https://openalex.org/W2900167092",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W3134376292",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W1877037013",
    "https://openalex.org/W2963702144",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3127394918"
  ],
  "abstract": "The advent of the transformer has sparked a quick growth in the size of language models, far outpacing hardware improvements. (Dense) transformers are expected to reach the trillion-parameter scale in the near future, for which training requires thousands or even tens of thousands of GPUs. We investigate the challenges of training at this scale and beyond on commercially available hardware. In particular, we analyse the shortest possible training time for different configurations of distributed training, leveraging empirical scaling laws for language models to estimate the optimal (critical) batch size. Contrary to popular belief, we find no evidence for a memory wall, and instead argue that the real limitation -- other than the cost -- lies in the training duration. In addition to this analysis, we introduce two new methods, \\textit{layered gradient accumulation} and \\textit{modular pipeline parallelism}, which together cut the shortest training time by half. The methods also reduce data movement, lowering the network requirement to a point where a fast InfiniBand connection is not necessary. This increased network efficiency also improve on the methods introduced with the ZeRO optimizer, reducing the memory usage to a tiny fraction of the available GPU memory.",
  "full_text": "arXiv:2106.02679v1  [cs.LG]  4 Jun 2021\nLayered gradient accumulation and modular pipeline parall elism:\nfast and eﬃcient training of large language models\nJoel Lamy-Poirier∗\nJune 8, 2021\nAbstract\nThe advent of the transformer has sparked a quick growth in th e size of language models, far outpacing\nhardware improvements. (Dense) transformers are expected to reach the trillion-parameter scale in the\nnear future, for which training requires thousands or even t ens of thousands of GPUs. We investigate\nthe challenges of training at this scale and beyond on commer cially available hardware. In particular, we\nanalyse the shortest possible training time for diﬀerent co nﬁgurations of distributed training, leveraging\nempirical scaling laws for language models to estimate the o ptimal (critical) batch size. Contrary to\npopular belief, we ﬁnd no evidence for a memory wall, and inst ead argue that the real limitation — other\nthan the cost — lies in the training duration.\nIn addition to this analysis, we introduce two new methods, layered gradient accumulationand mod-\nular pipeline parallelism, which together cut the shortest training time by half. The m ethods also reduce\ndata movement, lowering the network requirement to a point w here a fast InﬁniBand connection is not\nnecessary. This increased network eﬃciency also improve on the methods introduced with the ZeRO\noptimizer, reducing the memory usage to a tiny fraction of th e available GPU memory.\n1 Introduction\nLarge scale language models are rapidly changing the ﬁeld of natural language processing. Transformers\n[31] have risen as the preferred architecture for language models , being simpler, more scalable and more\nperformant than alternatives based on recurrent neural netwo rks. Although transformers come in numerous\nﬂavors and variations [2, 6, 8, 14, 21, 22, 32, . . . ], a growing body o f work suggests that the model performance\nis mainly driven by scale. Furthermore, few-shot learning capabilities have been observed in GPT-3 (175\nbillion parameters) [2]. This opens the ﬁeld of natural language proce ssing to a wide range of new applications\nwhere ﬁne-tuning is either impossible or impractical. These few-shot learning capabilities have not been\nobserved in smaller models, so they are believed to emerge only at the scale of GPT-3, reinforcing the need\nfor extremely large language models.\nWhile large language models present exciting new capabilities, they also pose a diﬃcult and costly\nengineering challenge. For example, simply storing the training state of GPT-3 takes about 2 terabytes of\nmemory, and storing its intermediate activations and gradients tak es several more terabytes. This is dozens\nof times more memory than available on the largest GPU to date, the 8 0 GB NVIDIA A100. Besides memory,\ntraining also requires a gigantic amount of computing power. GPT-3 n eeds about 3600 petaﬂop-day to train,\nor 30 years on the same device.\nTo speed up training of large models, it is necessary to parallelize the t raining process by distributing\nthe load across multiple GPUs. The leading method is 3d parallelism (see f or example [27]), which combines\nthe three common form of parallelism: data, pipeline and tensor para llelism. Together with methods such\nas mixed precision training and activation checkpointing, 3d parallelism allows quickly training large models\nwith up hundreds of billions of parameters but is not so fast beyond t hat scale. For example, GPT-3 was\ntrained in a matter of days, while the trillion-parameter version of Me gatron-LM would need more than\nthree months to train [18]. For larger models, training times are in the order of years or worse.\n∗ ServiceNow, joel.lamy-poirier@servicenow.com\n1\nRecently, several memory optimizations have been suggested for large models, aiming in particular to\nsimplify ﬁne-tuning. When compared to training from scratch, ﬁne- tuning requires much less computational\npower and can be done with a limited number of GPUs, but doing so crea tes a memory bottleneck. The ZeRO\nfamily of methods addresses this bottleneck by partitioning the tra ining state [23], aggressively oﬄoading\nmemory [25, 24], and breaking down individual operations [24]. These m ethods together shatter any memory\nconstraint.\nWhile a lot of attention has been given to the memory usage, much less eﬀort has been dedicated to\nreducing the training time. To that end, we investigate various para llel training conﬁgurations and strategies\nfor large and dense transformers, with the goal of minimizing the tr aining time on existing hardware. We\nexplicitly integrate the concept of critical batch size [9, 15, 26] in our analysis, leveraging the empirical\nscaling laws found in [12]. The critical batch size provides an upper bo und on the eﬃcient scaling of the\nbatch size, and by extension dictates how many GPUs can be used fo r training. To our knowledge, our\nanalysis is the ﬁrst to directly integrate the critical batch size and t he resulting parallelism bounds for large\nlanguage models. Our analysis can be applied to a wide range of scales, from the tiniest thousand-parameter\ntransformers up to the quadrillion parameter scale and beyond.\nOur analysis shows that memory is not a limiting factor even past the t rillion-parameter scale. Instead,\nwe ﬁnd a computational bottleneck caused by the limitations of distr ibuted training. Due to constraints\nfrom the critical batch size and network connectivity, 3d parallelism can eﬃciently use a limited number\nof GPUs, and this upper bound does not increase nearly as fast as t he amount of computation needed to\ntrain larger models. We ﬁnd a minimum training time of about two weeks f or a trillion-parameter model,\nand this bound scales worse than linearly with respect to the model s ize. As a result, training times are\nin the order of months or years above the trillion-parameter scale. We also ﬁnd that the ZeRO family of\nmethods counter-productive for the training time, largely becaus e of frequent data transfers in micro-batched\napproaches — including pipeline parallelism — which prevent eﬃcient 3d pa rallelism.\nWe introduce two closely related methods which signiﬁcantly improve t he training eﬃciency, while also\nreducing the memory usage and network requirement. The ﬁrst on e, layered gradient accumulation , uses\na bandwidth-eﬃcient scheduling for gradient accumulation, which ma kes it easier to overlap the gradient\nreduction with the computation. It also reconciles gradient accumu lation with the methods introduced in\nthe ZeRO optimizer, avoiding frequent data transfers when partit ioning the training state or when oﬄoading\nit to CPU memory.The second method, modular pipeline parallelism , uses a modular split of the layers to\nimprove the eﬃciency of pipeline-parallel training by minimizing the pipelin e “bubble”, which otherwise\nlimits the eﬃciency of pipeline parallelism. It also builds upon layered grad ient accumulation, enabling its\nbeneﬁts in the pipeline-parallel case. In particular, it allows partition ing the training state in the fastest\n3d parallel settings, reducing the memory usage to a minimum and pre venting a trade-oﬀ between memory\nusage and the bubble reduction of modular pipeline parallelism. These m ethods together allow training\nat least twice as fast as previous methods, for example reducing th e minimum training time to one week\nfor a trillion-parameter model. While we focus on large transformers , layered gradient accumulation and\nmodular pipeline parallelism are not speciﬁc to transformers or even la rge models. In fact, the improved\ncommunication overlap is particularly useful for smaller models, espe cially over slower networks.\nConcurrent to our work, two papers appeared that show overlap with our results. The latest version of\nMegatron-LM [18] suggests a breakdown of the layers similar to mod ular pipeline parallelism, however the\nauthors suggest a diﬀerent scheduling aimed at reducing the activa tion memory. Our method instead focuses\non a network-eﬃcient method, which leads to an increased eﬃciency beneﬁt from the new layer breakdown\nand when combined with a training state partition also leads to a lower m emory usage. In section 8.2,\nwe investigate disk oﬄoading in a way similar to ZeRO-Inﬁnity [24]. Our m ethods show improved results,\nfurther reducing the requirements for oﬄoading and enabling oﬄoa d even on slow hard drives. However,\nwe also little use for this extra space, as the memory usage tends to remain reasonable in most scenarios.\nInstead, we suggest leveraging the results to improve checkpoint ing methods.\nThis paper is structured as follow. In section 2 we describe the exist ing approaches and provide some\nadditional context for the paper. In sections 3 and4 we introduce the main new methods. In section 5, 6\nand 7, we analyze the impact of our method on training speed and mem ory usage, then generalize to a wide\nrange of scales. In section 8 we investigate some additional concer ns that arise in realistic training scenarios.\nFinally, in section 9 we discuss the implications and limitations of our resu lts.\n2\n2 Background and related work\nIn this section we describe the optimizations and distributed training methods relevant to our analysis.\n2.1 Critical batch size\nIn stochastic gradient descent, the gradient of the loss with resp ect to the model parameters is estimated\nfrom a micro-batch. When the micro-batch is small, adding more samp les improves this estimate and allow\ntraining with a proportionally lower number of steps, keeping the tot al amount of computation unchanged.\nHowever, past a certain point, the estimate becomes accurate en ough and adding more samples no longer\nreduces the required number of steps. The critical batch size [9, 15, 26] provides an upper limit for the small\nmicro-batch regime, and can obtained by estimating the variation (n oise) of the gradients between individual\nsamples, relative to the true gradients [15].\n2.2 Mixed precision training\nDeep neural networks do not require a high numerical accuracy, a nd to some extent can be trained with as\nlittle as 16 bits of precision. The state-of-the-art approach in tha t regard is mixed precision training [16],\nin which the bulk of the computation is done in half-precision, while the w eights are stored and updated\nin single-precision. Half-precision however has a limited exponent ran ge, and the gradients need to be\ndynamically scaled to limit the risk of underﬂows and overﬂows. The pr oblem can also be solved with the\nbﬂoat16 data format, which has a wider exponent range and is available in rece nt devices such as the NVIDIA\nA100.\n2.3 Communication overlap\nIn addition to the computation itself, data transfers present a sig niﬁcant optimization challenge. They should\npreferably be done in parallel with the bulk of the computation, othe rwise the computational cores stay idle\nduring the transfer. As the computation and network communicat ions mostly use separate resources, they\ncan in principle be overlapped with near-perfect eﬃciency, so the runtime is determined by the slo west of\nthe operations. The operation is said to be compute-bound or data-bound depending on which one ﬁnishes\nlast, with the former scenario being preferable here. In practice, there is a small overhead to overlapping the\noperations, but we ignore it for the purpose of this paper.\nThe threshold for a compute-bound operation is described throug h the concept of arithmetic intensity .\nFor an operation which requires both computation and data transf er, the arithmetic intensity is deﬁned as\nthe ratio between the amount of computation and data transfer. In the case of perfect overlap, the operation\nis compute-bound if the arithmetic intensity is higher than what the h ardware can support. For example, A\nNVIDIA A100 has a peak computational power of 312 teraﬂops for half-precision and a memory bandwidth\nof 2039 GB/s, for an arithmetic intensity threshold of 143 ﬂops/B. A binary addition with an arithmetic\nintensity of 1/6 lies deeply in the memory-bound region, while the multip lication of two 1024x1024 matrices\nhas an arithmetic intensity of 341 and is compute-bound.\n2.4 Distributed training\nThere are two main forms of parallelism, data parallelism in which each de vice performs the same computa-\ntion on a subset of the data, and model parallelism in which the model a nd computation are split between\ndevices.\nIn data parallelism , the input batch is split between the nb devices. Each device independently processes\nits assigned input, then the resulting gradients are reduced (summed) between the devices, and ﬁnally each\nindependently updates all the weights. All the network communicat ion happens in the gradient reduction,\nwhich for the most part can be overlapped with the backward pass, and this overlap can be made compute-\nbound with large enough micro-batches. Data parallelism is the most u sed method for parallel training,\nbecause of its simplicity and wide availability. However, as each device n eeds to store a copy of the model,\nthe memory usage is excessive for larger models. This can be addres sed by partitioning the model, as\nsuggested in [23]. In this partitioned case, the model is split between the devices, each being responsible\n3\nfor storing and updating an equal share of the weights 1. The weight tensors are reconstructed on each\ndevice as needed, and the reduced gradients are kept only as requ ired. The partition increases the network\ncommunication by 50%. Data parallelism scales up to the critical batch size bc, but in general a minimum\nmicro-batch size is required for eﬃcient communication overlap, red uces the limit to a fraction of bc.\nModel parallelism comes in two main forms, depending on how the model is split. The ﬁrst form is\npipeline parallelism , where each of the nl devices is responsible for a subset of the layers. The input is\npipelined through the devices, and parallel computation is achieved b y feeding micro-batches sequentially\nto the model [11]. Each device processes all the micro-batches for a given batch, then updates the weights.\nThis however leads to a “bubbling” eﬀect in which some of the devices s tay idle due to input starving (see\nthe upper part of ﬁgure 3) 2. For an input split into nµ micro-batches, this bubble increases the training time\nby a factor nl− 1\nnµ\n. Pipeline parallelism is also limited in magnitude, as it cannot grow beyond t he number of\nlayers in the model.\nThe other form of model parallelism, tensor parallelism , involves slicing tensors so that each device\nis responsible for evaluating a slice of the activations using its corres ponding slice of the weights 3. Tensor\nparallelism is more complex than the two other forms of parallelism and g enerally depends on the internal\ndetails of the model. It also requires a lot more network communicatio n and is in general only feasible\nwith the fastest interconnects such as NVLink. In practice this limit s tensor parallelism to 16 GPUs, the\nmaximum that can be connected with NVLink.\nAlthough data, pipeline and tensor parallelism are individually limited, the y can be combined together\ninto (up to) 3d parallelism . The dimensions combine in a multiplicative way, for a total of ngpu = nbnlna\ndevices. In 3d parallelism, the input batch size is split both between th e data-parallel instances 4, and between\nthe nµ ≥ nl sequential micro-batches. For a batch of size b, this implies a micro-batch size bµ = b/(nbnµ ).\nBecause of the limitation from the critical batch size (see section 2) , this implies a competition between\ndata and pipeline parallelism, but the combined method generally allows a higher degree of parallelism than\nwith either method in isolation. This is because pipeline parallelism increas es the arithmetic intensity for\nthe gradient reduction, reducing the minimum micro-batch size 5. However, pipeline parallelism provides no\nsuch beneﬁt with a partitioned training state, where the network o perations need to be repeated for each\nmicro-batch.\n2.5 Memory optimizations\nAs the model grows in size, so does its memory usage, and substant ial memory optimizations are needed to\navoid running out of memory. The bulk of the memory usage falls in two categories: the training state and\nthe activation memory. The training state consists of the model parameters and the optimizer state, which\nfor Adam includes the running average and variance of the paramet ers. For the purpose of this paper, the\nparameter gradients are also included in the training state. The activation memory consists of the layer\nactivations and their gradients. Both categories can use substan tial amounts of memory for larger models,\nbut various techniques have been developed to reduce memory con sumption.\nThe training state is proportional to the model size. At 80 GB, a NVI DIA A100 can store at most 20\nbillion parameters in single precision, or 6.7 billions with ADAM. As the train ing size is ﬁxed, additional\nmemory is needed to ﬁt larger models, and distributed training provid es such memory. As described in\n1In this paper we consider a partition of the whole training st ate, which in the terminology of [23] corresponds to the thir d\nstage of ZeRO-DP.\n2The network bubble can be avoided with asynchronous training [19, 17], where the weight updates are interleaved with the\ncomputation. However, this leads to gradients being comput ed on stale (outdated) weights, which can be harmful to training.\nAlthough training is technically done with sequential batc hes rather than micro-batches, staleness adds to the gradient noise\nand reduces the critical batch size proportionally to the de lay [7, 10, 29]. Consequently, the limitations of 3d paralle lism (see\nbelow) are the same as with synchronous training.\n3The method is often referred to as “model parallelism” in the literature, but the term may also refer to pipeline parallel ism,\nso here tensor parallelism is used instead, and the more general term “model parallelis m” is reserved for the combination of\nboth methods.\n4The term instance is used to designate a slice of the cluster i n a speciﬁc parallelism dimension. For example, a data paral lel\ninstance refers to the nlna devices handling the same sequence of micro-batches.\n5Gradient compression methods such as 1-bit Adam [30] also reduce the network communication, potentially pr oviding the\nsame beneﬁt without pipeline parallelism. However, compre ssion methods cannot eﬃciently be used with a partitioned tr aining\nstate. Together with the absence of pipeline parallelism, t his makes the size of the training state diﬃcult to manage for larger\nmodels.\n4\nsection 2.4, the training state is split by construction in the model-pa rallel directions and can be partitioned\nin the data parallel direction. The training state can also be oﬄoaded to CPU memory; however, this\nrequires additional data transfers and may create a communicatio n bottleneck.\nWith a training state partition or oﬄoading, the model weights are st ored in another device and must be\nrestored to the before usage, so additional buﬀers are required . Similarly, the gradients need to be created\non a buﬀer before being moved to the appropriate place. While these parameter and gradient buﬀers are\nsmall compared to the whole training state, they dominate the GPU m emory usage for larger models once\nall the optimizations are taken into account. These buﬀers are to s ome extent also required in the absence\nof partition or oﬄoading, as for example the parameters need to be converted to 16 bits before usage.\nActivation memory is also a concern, and naive implementations can ea sily require hundreds of gigabytes\neven at the billion-parameter scale. The most signiﬁcant reduction in activation memory is obtained with\nactivation checkpointing , in which the bulk of the activations are dropped during the forward pass [3] 6.\nA subset of the activations is kept as checkpoints and are used to recalculate the remaining activations\nduring the backward pass. This lowers the activation memory by a sig niﬁcant factor, reducing it to the\nlayer activations , i.e., the activation memory required between the activation checkp oint, and the activation\ncheckpoints themselves, which can in many cases be oﬄoaded to CPU memory. The method comes at the\ncost of a 33% increase in computation, but there is no alternative fo r larger models.\nAs the activation memory is proportional to the micro-batch size, a n obvious memory optimization\nis to lower it, down to a single sample if possible. While single-sample micro- batches are typically not\nrecommended, they can run eﬃciently for larger models for which th e computation kernels are big enough.\nHowever small micro-batches come with a lower arithmetic intensity w ith respect to the model weights,\npotentially creating a bottleneck in the case of data parallelism or mem ory oﬄoad. The layered gradient\naccumulation and modular pipeline parallelism methods introduced in this paper are designed to prevent\nsuch bottlenecks, and to allow running eﬃciently with a micro-batch s ize of one.\nOn a side note, gradient accumulation allows increasing the batch size without running out of memory, by\nprocessing multiple micro-batches sequentially between weight upda tes. This is especially useful with data\nparallelism over a slow network, as larger batches reduce the frequ ency of the gradient reduction. However,\nthis leads to ineﬃcient communication overlap as the gradient reduct ion is concentrated in the last micro-\nbatch. The method is also counter-productive for optimizing the tr aining time, since the micro-batches are\nprocessed sequentially instead of in parallel. The layered gradient accumulation is designed to improve the\ncommunication overlap, while pipeline parallelism allows processing the mic ro-batches in parallel.\nIn addition to the training state and activation memory, there can b e some additional memory usage or\nhidden memory costs. Many libraries for distributed training, includin g Pytorch DistributedDataParallel, use\na bucketing strategy for network operations, combining several tensors in a temporary network buﬀer . This\nis preferable for small tensors, as it avoids the overhead or runnin g the individual operations sequentially,\nbut comes at the cost of additional memory usage and data moveme nt. In ZeRO-R it is suggested to use a\nconstant size buﬀer to keep the memory overhead bounded [23]. Ho wever, in the present case, the parameter\nand gradient buﬀers already play a role similar to the buckets, and in-place network operations can be done\nat no extra memory cost.\nEven when enough memory is available, memory fragmentation can still cause the device to run out\nof memory. For larger models, memory fragmentation becomes a pr oblem because the allocations involve\nvery large contiguous chunks of memory, and there is an overlap be tween short-lived activations and their\ngradients, and longer-lived tensors such as the activation checkp oints and parameter gradients. A solution\nto this is to pre-allocate contiguous buﬀers for the memory whenev er possible. Pre-allocation also allows\ncombining tensors into a single contiguous (fused) chunk of memory , which not only ensures optimal allocation\nbut also allows running data operations on a fused tensor. In [23] it is suggested to pre-allocate the activation\ncheckpoints and parameter gradient. This leaves the layer activat ions and gradients as the main source of\nfragmentation, which can still be signiﬁcant because of the size imba lance between the various activations,\nwhich creates large memory gaps. For example, we observed an ove rhead of up to 40% for a single transformer\nlayer in the PyTorch implementation. However, it is possible to reduce this overhead to almost zero with a\nmemory-eﬃcient implementation, so in this paper we assume the memo ry fragmentation cost to be minimal.\n6The method is also known as gradient checkpointing in the literature, even though it does not involve any gradie nt.\n5\nreduce\n0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3Compute\nNetwork\n(available)\ntime\nStandard gradient accumulation\nreduce\n0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3Compute\nNetwork\n(available)\nLayered gradient accumulation\nidle layer 0 layer 1 layer 2 layer 3\nFigure 1: Computation and network scheduling example with data par allelism for standard gradient ac-\ncumulation (top) and layered gradient accumulation (bottom). The colors represent the diﬀerent layers,\nwith diﬀerent shades for the forward and backward pass (lengths not to scale), and the numbers indicate the\nmicro-batch index. The layered version reduces the network requ irement by spreading the gradient reduction\nover most of the backward pass.\n3 Layered gradient accumulation\nIn layered gradient accumulation , we split the input into micro-batches exactly as in standard gradien t\naccumulation, but we process all the micro-batches for a given laye r before proceeding to the next one.\nWe take such layers as the intervals between activation checkpoint s, so that we can drop the intermediate\nactivations between the micro-batches.\nThe layered gradient accumulation method is advantageous from a d ata perspective. With data par-\nallelism, it allows an eﬃcient overlap of the gradient reduction with the b ackward pass, unlike traditional\ngradient accumulation which only allows overlapping with the last micro- batch. This is illustrated in ﬁgure\n1. With a state partition, layered gradient accumulation greatly red uces the bandwidth requirement by\neliminating redundancies in the parameter restoration and gradient reduction operations, as illustrated in\nﬁgure 2. Similarly, layered gradient accumulation helps with oﬄoading b y reducing the amount of data\nmovement. All the activation checkpoints must be kept, which in the presence of pipeline parallelism is\nalready a requirement 7, but otherwise may cause an increase in the activation checkpoint m emory.\nNote that layered gradient accumulation generally cannot be eﬃcien tly combined with standard pipeline\nparallelism. Indeed, a given pipeline-parallel instance must process e very micro-batch for all every layer\nother than the last before it can pass an output to the next instan ce. This is addressed with the modular\npipeline parallelism.\n4 Modular pipeline parallelism\nIn pipeline parallelism, the layers are generally split into contiguous chu nks. For a network with dl layers\nsplit into nl devices, ﬁrst instance gets the layers 1 to dl/nl, the second gets the layers dl/nl + 1 to 2 dl/nl,\netc. However, while this “naive” splitting minimizes pipeline-parallel net work operations, it also maximizes\nthe bubbling eﬀect. In modular pipeline parallelism, the layers are inste ad split in a modular fashion, so\nthat the ﬁrst instance gets the layers 1, nl + 1, etc., the second gets the layers 2, nl + 2, etc., and so on.\nThe computation is scheduled as with layered gradient accumulation, i.e., a given instance processes all\nmicro-batches for a given layer, then goes on to the next layer for which the input should be ready, etc.\nIn the modular formulation, a micro-batch reaches the last instanc e after being processed on nl − 1 layers\nrather than dl(1 − 1/nl), reducing the bubbling overhead by a factor dl/nl. This makes it possible to reduce\nthe bubbling to almost zero without increasing the number of micro-b atches nµ . Additionally, the gradient\nreduction is spread more evenly over the backward pass, dividing th e network overhead by a factor dl/nl.\n7Some approaches perform better with a memory-eﬃcient sched uling [17, 18], but in any case the activation checkpoints do\nnot grow too big.\n6\n0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3Compute\nNetwork\n(available)\ntime\nStandard gradient accumulation\nrestore restore restore restore reduce restore reduce restore reduc e restore reduce\n0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3Compute\nNetwork\n(available)\nLayered gradient accumulation\nidle layer 0 layer 1 layer 2 layer 3\nFigure 2: Computation and network scheduling example with state pa rtition or oﬄoad, for standard gradient\naccumulation (top) and layered gradient accumulation (bottom). A mixed buﬀering method is assumed (see\nappendix C.2). The standard version involves frequent context sw itches with respect to the weights, which\nleads to unreasonable bandwidth requirements. The layered versio n on the other hand maximizes the reuse\nof the restored weights, so requires the same bandwidth as withou t gradient accumulation.\nModular pipeline parallelism comes with an increased pipeline parallel netw ork cost since data needs to be\ntransferred after each layer, but for large models this cost rema ins far below the data parallel network usage.\nThis data transfer can be overlapped with computation provided th ere are slightly more micro-batches than\npipeline-parallel instances.\n5 Methodology\nIn the following sections we analyse the resource usage and training time for large language models, focusing\non the impact of the methods introduced in this paper, as well as 3d p arallelism and training state parti-\ntioning. We present some example and relevant results, leaving the d etailed computation to the appendix C.\nWe assume the computation is done with a scalable cluster with up to 16 A100 GPUs per node, supporting\nboth InﬁniBand and NVLink. See appendix A for additional details on t he hardware.\nModel We consider a transformer encoder following the original approach [31], up to computationally\nunimportant modiﬁcations. Transformer encoders are for examp le used in the BERT model [6] and its\nderivatives, and our results generalize straightforwardly to deco der-based models such as the GPT family\n[20, 21, 2]. For simplicity, we restrict to the transformer part of th e model, and ignore other components\nsuch as the tokenizer, the embedding layer and the language model head.\nA transformer encoder consists of dl identical layers, each composed of a multi-head attention module\nfollowed by a non-linearity. The former consists of da attention heads of size dh, for a layer width dm = da× dh,\nwhile the latter consists of a two-layer dense feedforward networ k with intermediate size dI = nI × dm. Each\nlayer holds pl ≈ (4 + 2 nI )d2\nm parameters, for a total of p ≈ (4 + 2 nI )d2\nm × dl parameters.\nFor concreteness, we consider the X [x] family of models described in appendix B, which allows extrap-\nolation to wide range of scales. The exact conﬁguration however ma kes little diﬀerence, so our results\nstraightforwardly generalize to most conﬁgurations.\nTraining We use mixed precision training, and activation checkpoints at the ou tput of each transformer\nlayer. We overlap communication when possible and oﬄoad the training state and activation checkpoints\nwhen needed (and possible). We use the mixed gradient buﬀering method for the parameter and gradient\nbuﬀers, as described in appendix C.2. This method is not necessarily t he most memory eﬃcient but combines\nwell with the bandwidth-eﬃcient methods introduced here and is suﬃ cient for all practical purposes. The\nAdam optimizer is assumed.\nWe consider distributed training with up to three parallelism dimensions . For tensor parallelism, we follow\nthe transformer-speciﬁc method proposed in [27]. This approach m inimizes the network communication,\n7\nreduce\n0 1 2 3 0 1 2 3\nreduce\n0 1 2 3 0 1 2 3\nreduce\n0 1 2 3 0 1 2 3\nreduce\n0 1 2 3 0 1 2 3\nDevice 0\nDevice 1\nDevice 2\nDevice 3\ntime\nStandard pipeline\nidle layers 0-3 layers 4-7 layers 8-11 layers 12-15\nreduce\n0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 3 2 1 0 3 2 1 0 3 2 1 0 3 2 1 0\nreduce\n0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 3 2 1 0 3 2 1 0 3 2 1 0 3 2 1 0\nreduce\n0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 3 2 1 0 3 2 1 0 3 2 1 0 3 2 1 0\nreduce\n0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 3 2 1 0 3 2 1 0 3 2 1 0 3 2 1 0\nDevice 0\nDevice 1\nDevice 2\nDevice 3\nModular pipeline\nidle layers 0-3 layers 4-7 layers 8-11 layers 12-15\nFigure 3: Computation and network scheduling example with standar d and modular pipelines (no state\npartition). The modular version signiﬁcantly reduces the idle time fro m the bubbling eﬀect and makes the\ngradient reduction much easier to overlap.\nwhich however cannot be overlapped with computation.\nWe investigate three training strategies. In the baseline, we use standard data and pipeline parallelism\n(without partition), as described in section 2.4. In the partitioned approach, we also partition the training\nstate in the data-parallel direction. In the improved approach, we implement layered gradient accumulation\nand modular pipeline parallelism. Unless speciﬁed otherwise, we partitio n the training state in the improved\napproach, as it is preferable to do so in most cases.\nOptimal conﬁguration We select the distributed training conﬁguration as follow, with the go al of opti-\nmizing the training speed without wasting too much computational po wer. We train at or slightly below the\ncritical batch size, as training above it is ineﬃcient. The selection is ba sed on the resource usage described\nin appendix C.\nIn the baseline, we select the highest possible pipeline parallelism (if app licable), as it reduces memory\nusage while also increasing eﬃciency due to the faster gradient redu ction. We select a micro-batch count\nslightly above nl to ensure overlap of the pipeline-parallel computation and the smalle st micro-batch size\nthat does not cause a communication bottleneck. With pipeline paralle lism, we impose a maximum overhead\nof 25% from the gradient reduction, although it is only constraining f or smaller models and slow networks.\nWhen oﬄoading is needed, the micro-batch size is also constrained by the CPU-GPU transfer rate, and there\nmay be an additional bottleneck in the PCI-express conection which is shared between CPU and Inﬁniband\ntransfers.\nIn the partitioned approach, we do not consider pipeline parallelism as it leads to worse results. Instead,\nwe maximize the data parallelism degree nb. Due to the increased network communication, the gradient\nreduction is more constraining on the micro-batch size than in the ba seline approach, but oﬄoading is in\ngeneral not constraining at all due to the small size of the oﬄoaded state.\nIn the improved approach, we can set the micro-batch size to one, and avoid a communication bottleneck\nwith a high enough count nµ . We maximize nb, then nl under these considerations, so that the bubbling\noverhead is minimized. It is in general preferable not to overlap the p ipeline-parallel communication, as nµ\nand nl are both small and rounding up to a single extra micro-batch would sig niﬁcantly reduce the training\nspeed.\n8\nTable 6.1: Fastest training conﬁguration for X 160 for selected training methods\nParallelism Method Oﬄoad b b µ nµ ngpu nb nl na Eﬃciency Time\nNone Baseline ✓ 2416 4 604 1 1 1 1 1.00 630 y\nData Baseline ✓ 2415 5 1 483 483 1 1 1.00 1.3 y\nData Partitioned ✓ 2415 5 1 483 483 1 1 1.00 1.3 y\nData + pipe Baseline ✓ 2412 4 201 480 3 160 1 0.56 2.4 y\nData + pipe Improved ✗ 2415 1 5 2415 483 5 1 0.94 100 d\nData + tensor Baseline ✓ 2415 5 1 7728 483 1 16 0.93 32 d\nData + tensor Partitioned ✗ 2415 5 1 7728 483 1 16 0.93 32 d\n3d Baseline ✗ 2408 1 172 35840 14 160 16 0.48 13 d\n3d Improved ✗ 2415 1 5 38640 483 5 16 0.88 6.8 d\nTable 6.2: Memory usage breakdown for the same training conﬁgura tions.\nParallelism Method State Checkpoint Buﬀers Activations Oﬄ oadable Non-oﬄoadable\nNone Baseline 14.1 K 47.2 K 43.9 24.9 61.2 K 68.8\nData Baseline 14.1 K 97.7 43.9 31.1 14.2 K 75.1\nData Partitioned 29.1 97.7 43.9 31.1 127 75.1\nData + pipe Baseline 87.9 98.1 43.9 24.9 186 68.8\nData + pipe Improved 5.82 19.5 43.9 6.23 25.4 50.2\nData + tensor Baseline 879 6.10 2.75 1.95 885 4.69\nData + tensor Partitioned 1.82 6.10 2.75 1.95 7.92 4.69\n3d Baseline 5.49 1.31 2.75 0.389 6.81 3.14\n3d Improved 0.364 1.22 2.75 0.389 1.58 3.14\nIn all cases, tensor parallelism allows a near-perfect split of both th e memory and computation. We\nimpose a maximum overhead of 25%, which for large models (above ∼ 50 billion parameters) allows the\npractical limit na = 16. At extreme scales ( 25 trillion parameters) it becomes possible t o eﬃciently use\ntensor parallelism over InﬁniBand with na > 16.\nResource usage The computation, memory and bandwidth requirements are evaluat ed in appendix C.\nTo obtain a training time estimate, we compare the available and requir ed computational power, taking\ninto account the measurable overheads from the pipeline bubble and non-overlapped data transfers. We do\nnot however consider the eﬃciency of the computational kernels, the data transfers and the communication\noverlap, so the training times are likely underestimated. On the othe r hand, the memory usage is expected\nto be accurate for the given training conﬁgurations.\n6 Trillion parameter example\nTo investigate the requirements for training at the trillion-paramet er scale, we consider the 1.26 trillion\nparameter model X 160. This model consists of 160 transformer layers with 80 heads of siz e 320, for a width\nof 25600. The sequence length is 2560, and the critical batch size is estimated to be around 2420. Training\nfor 100 k steps requires 6 .24 × 1024 ﬂoating point operations, or 72 exaﬂop/s ·day, which corresponds to 231\nk GPU-days on A100s at perfect eﬃciency.\nAlthough the computational requirement strongly hints that a larg e cluster is necessary, for comparison\npurpose we investigate various distributed training scenarios, with the training conﬁguration selected as\nfollow. The resulting conﬁgurations are summarized in table 6.1, toge ther with the expected computational\neﬃciency and training time. We ﬁnd both data and tensor parallelism to be necessary (and together suﬃcient)\nto train in a reasonable amount of time. However, modular pipeline par allelism stands out with both a high\nGPU count and near-optimal eﬃciency, allowing to train the model in a week. It also out-performs the\nbaseline in the absence of tensor parallelism, but the training time rem ains above three months.\nA breakdown of the memory usage for each conﬁguration is shown in table 6.2. All methods are possible\nfrom a memory perspective, assuming a minimal memory fragmentat ion overhead. However, the baseline\n9\nTable 6.3: Selected training conﬁguration for X 160 for the speciﬁed training times of one and six months\nParallelism Method b n a ngpu Oﬄoadable Non-oﬄoadable Eﬃciency Time\nData + tensor Partitioned 2415 16 7728 7.92 4.69 0.93 32 d\n3d Baseline 2416 16 10240 10.1 3.14 0.73 31 d\n3d Improved 2220 4 7400 7.76 12.5 0.97 32 d\nData + tensor Partitioned 1660 8 1328 35.0 9.38 0.97 180 d\nPipe + tensor Baseline 2416 8 1280 47.9 6.27 0.91 199 d\n3d Improved 792 2 1320 22.4 25.1 0.97 180 d\nData + pipe Improved 1572 1 1310 34.2 50.2 0.98 180 d\n3d Improved 102 16 1360 11.8 3.14 0.91 186 d\napproach requires an impractical amount of oﬄoaded memory witho ut pipeline parallelism. The improved\nmethod has the lowest memory footprint of 4.72 GB, which is 17 times le ss than the memory available in an\n80 GB A100.\nSmaller clusters Although training scales to nearly 40000 GPUs, it may be diﬃcult to ﬁnd a cluster of\nthat size, so there is a strong case for training with fewer GPUs ove r a longer period. There are a variety\nof viable strategies for smaller clusters, allowing trade-oﬀs betwee n eﬃciency, memory usage, and the choice\nof parallelism methods. In table 6.3 we provide some example conﬁgura tions with high eﬃciency for the\ntarget training times of one and six months. With these time constra ints, it is possible to train with clusters\nof sizes 7400 and 1300 respectively, which makes training available to a marginally wider community. The\nmemory usage is increased when compared with the larger clusters b ut remains far below what is available.\nAmong the methods considered, ours is the most eﬃcient, although the margin becomes negligible for longer\ntraining times. It is also far more ﬂexible, as shown for example in the la st two entries. For the six-month\ntraining it is the only one able to train without tensor parallelism (with oﬄ oad). It is also able to train with\na much lower batch size, which amounts to an extra eﬃciency gain as t here is an implicit cost to training\nwith a high batch size.\n7 Scaling analysis and practical limits\nWe analyze how the memory usage and training time scale with the mode l size, using the X [x] model family\ndescribed in appendix C.2. We consider the same three training strat egies as before — baseline, partitioned\nand improved — but restrict to the fastest conﬁguration for each .\nFigure 4 shows the memory usage and training time as a function of th e model size at various scales. The\nimproved method outperforms the others at most scales, althoug h it becomes identical to the partitioned\napproach above the quadrillion parameter scale, as pipeline parallelism is no longer necessary.\nFocusing on the improved method, we ﬁnd that 80 GB is enough to tra in models up to 280 trillion\nparameters, with oﬄoading being required above 90 trillion. However , such model would take up to four\nyears to train (and more when taking the real computation eﬃcienc y into account), which is unreasonably\nlong as for instance waiting for the next generation of hardware wo uld likely be faster. To obtain better\nscaling limits, we consider a reasonable threshold of one month, and a more generous threshold of one year.\nThese thresholds respectively result in limits of about 4.5 and 50 trillion parameters, with total memory\nusages of 13 and 62 GB.\nThe above results show a strong limitation on the model size due to th e training time. It may however\nbe possible to do better with existing GPUs. While data and pipeline para llelism are fundamentally limited\nby the critical batch size, tensor parallelism is limited by a computer de sign choice which limits the node\nsize to 16. The fully connected NVSwitch topology in DGX and HGX node s is convenient in the general\ncase, but tensor parallelism only requires a ring topology, which is eas y to scale to an arbitrary size. This\nmeans larger nodes (or separate nodes connected with NVLink) sh ould be possible, although it may require\na certain amount of engineering. For this reason, we consider the s cenario where the node size limitation is\nremoved, with the results being shown in ﬁgure 5. In this case, ther e is enough memory for models up to 100\n10\n1010 1012 1014 1016\nParameters\n10−1\n100\n101\n102\nNon-offloadable (GB)\n2d Partiti ned\n3d Baseline / Impr ved\n80 GB (A100)\n1010 1012 1014 1016\nParameters\n10−1\n100\n101\n102\n103\nOffl adable (GB)\n3d Baseline\n2d Partiti ned\n3d Impr ved\n80 GB (A100)\n1 TB ( ffl ad)\n1010 1012 1014 1016\nParameters\n10−2\n10−1\n100\n101\n102\n103\n104\n105\nTraining time (days)\n3d Baseline\n2d Partiti ned\n3d Impr ved\nOne m nth\nOne year\nFigure 4: Approximate minimal memory usage and training time for sele cted training methods, with a\nmaximum node size of 16.\n1010 1012 1014 1016\nParameters\n10−1\n100\n101\n102\nNon-offloadable (GB)\n2d Partitioned\n3d Baseline / Improved\n80 GB (A100)\n10 10 10 12 10 14 10 16\nParameters\n10 −1\n100\n101\n102\n103\nOffloadable (GB)\n3d Baseline\n2d Partitioned\n3d Im roved\n80 GB (A100)\n1 TB (offload)\n1010 1012 1014 1016\nParameters\n10−2\n10−1\n100\n101\n102\n103\n104\nTraining time (days)\n3d Baseline\n2d Partitioned\n3d Im roved\nOne month\nOne year\nFigure 5: Approximate minimal memory usage and training time for sele cted training methods.\nquadrillion parameters, while the training time reduces the limit to 40 tr illion (one month) or 900 trillion\nparameters (one year).\nThere is no memory wall While memory usage is not a problem until astronomical scales, we can go\nfurther and show memory is never a problem. For this, we suppose at some point in the future it become s\npossible to train a given model in a ﬁxed time of one month, either using faster devices or faster network\nconnections. Then we measure how much memory would be needed, in relation to the computing power.\nFor simplicity, we assume that the conﬁguration still uses as much da ta and pipeline parallelism as possible,\nand scales only with tensor parallelism. The results are shown in ﬁgure 6, which shows that the memory\nrequirement decreases with the model size. In fact, memory is only an issue at smaller scales, which may in\npart explain the perceived memory problem, but it is already possible t o train those models much faster with\na minimal amount of memory (dotted line). These results are not par ticularly surprising, since the highest\nmemory scaling comes from the state which is proportional to the mo del size, while the computation has a\nworse scaling due to the increased input size.\n11\n109 1010 1011 1012 1013 1014 1015 1016\nParameters\n10−4\n10−3\n10−2\n10−1\n100\nNon-offloadable (GB/tflops)\n2d Partitioned\n3d Baseline / Improved\nA100 80 GB\n10 9 10 10 10 11 10 12 10 13 10 14 10 15 10 16\nParameters\n10 −4\n10−3\n10−2\n10−1\n100\nOffloadable (GB/tflops)\n3d Baseline\n2d Pa titioned\n3d Imp oved\nA100 80 GB\nFigure 6: Required memory to compute ratio for training a transfor mer in one month, as a function of the\nmodel size. The dotted line shows a faster training scheme which is alr eady possible on A100s.\n8 Additional training considerations\nWhile we assumed so far the existence of a suﬃciently large supercom puter, in practice this may be diﬃcult\nto achieve. Training a large language model is very expensive, and se tting up such supercomputer exclusively\nfor this purpose signiﬁcantly increases the cost. For this reason, it is preferable to leverage data centers where\nthe nodes can quickly switch to and from other workloads so that th ey are used without interruption. In this\nscenario, the number of available nodes may vary during training, fo r example if higher priority workloads\nneed to be run or some nodes need to be removed for maintenance. This means training should preferably\nbe elastic, i.e., it should support variations in the cluster size. Even in a ﬁxed clus ter, the risk of hardware\nfailure is high due to the large number of components, and handling th ese failures requires some form of\nelasticity. While a complete analysis of elastic training is outside the sco pe of this paper, in this section we\ninvestigate some concerns and potential optimizations related to t he optimizations presented in this paper.\nWe assume the nodes are located in the same data center and prefe rably connected through InﬁniBand,\nalthough in section 8.3 we investigate the eﬀect of training over a slow er Ethernet connection. We also\nassume the various parallelism and network bounds presented in this paper are respected, which can be done\nby scaling the data parallelism degree down from the maximum allowed va lue. When the state is partitioned,\nthis presents additional challenges since the partitioning changes d uring training, but we show in section 8.3\nthat partitioning actually helps with elasticity. The varying partition may also increase the memory us age,\nbut there is plenty of room for it.\n8.1 Don’t decay the learning rate, increase the cluster size\nThe analysis so far wrongly assumed that the critical batch size is co nstant during training. During early\ntraining the gradients contain a strong model-improving signal, but a s the model is trained this signal\nbecomes less important relative to the noise, which increases the nu mber of samples required to obtain an\naccurate estimate, i.e., the critical batch size. The “unique” critica l batch size considered so far corresponds\nto the value during late training, and using it implies a wasteful training above the critical batch size during\ntraining. Instead, the critical batch size can be evaluated dynamic ally during training, and the batch size\nadjusted accordingly [15, 28]. In the present context, this means the maximum cluster size varies during\ntraining, and when elastic training is possible it should be adjusted dyn amically. Doing so reduces the cost\nof training without signiﬁcantly aﬀecting the training time.\n8.2 Oﬄoading revisited: real-time checkpoints\nIn the previous sections we found oﬄoading to be possible, but rare ly necessary from a memory perspective.\nHere we take another look at oﬄoading, this time with the objective o f keeping a copy of the weights in a\n12\n10 10 10 12 10 14 10 16\nParameters\n10 3\n10 4\n10 5\n10 6\n10 7\n10 8\nArithmetic intensity (flop/B )\n1010 1012 1014 1016\nParameters\n10−2\n10 21\n10 0\n10 1\n10 2\n10 3\nMinimum ban 0i −h (GB/s)\nState, Baseline\nState, Partitioned/Improved\nCheckpoint offload\nCPU -GPU limit\nDisk (NVMe)\nDisk (Hard drive)\nEthernet (25 Gb/s)\nFigure 7: Required memory to compute ratio for training a transfor mer in one month, as a function of the\nmodel size. The dotted line shows a faster training scheme which is alr eady possible on A100s.\nsecure and accessible location, a practice also known as “saving a ch eckpoint”. Saving checkpoints typically\ntakes a long time during which the cluster is idle, often taking longer th an training multiple batches. This\nmeans checkpoints cannot be saved often, and failures lead to a sig niﬁcant loss of progress. There is also a\nsigniﬁcant downtime in elastic training, since a variation of the cluster means a checkpoint must be saved,\nthen loaded in the new nodes. This downtime is problematic when runnin g in a data center with a lot of\nactivity, where nodes are added and removed frequently.\nFigure 7 shows the arithmetic intensity and bandwidth requirement f or oﬄoading the model. In the\npartitioned case, we ﬁnd that the state can not only be oﬄoaded to CPU, but it can also easily be oﬄoaded\non fast SSDs (as suggested in [24]), possibly on a remote location via E thernet, and for larger models even\nhard drives are fast enough. This allows keeping an up-to-date cop y of the weights in a secure external\nlocation accessible by all nodes at a negligible cost, which reduces the potential cost of failure to a single\nbatch8. The overhead of modifying the cluster size can also be reduced to a lmost zero by loading the weights\non the ﬂy, even if a new partition needs to be made.\nWe can go further by considering activation checkpoints, which as ﬁ gure 7 shows need a higher bandwidth\nthan the state, but for the larger models can also be saved to a fas t remote storage. This reduces the potential\nloss from a crash to a single layer and allows swapping nodes in the middle of a batch at nearly zero cost.\n8.3 Ethernet is enough\nAs data centers may not be equipped with a fast InﬁniBand connect ion, we evaluate the possibility of training\nover Ethernet. We assume the nodes are equipped with a 400 Gb/s E thernet connection, which amounts to\n25 Gb/s per GPU. The analysis is shown in ﬁgure 8. For larger models, t he slower connection makes little\ndiﬀerence, provided pipeline parallelism is used. However, for smaller m odels, a higher degree of pipeline\nparallelism is required to reduce the network usage, which makes it ha rder to mitigate the bubble in the\nimproved case. For the trillion-parameter model, this slows down tra ining by about 4%, but the eﬀect is\nmore important at smaller scales. Despite this and the communication overhead from the state partition,\nthe improved method outperforms the baseline at smaller scales bec ause of the improved communication\noverlap. For the smallest models, the partition can be avoided to fur ther reduce the training time at a\nminimal memory cost (dotted line).\n8This can also be done in the non-partitioned by sharing the lo ad of saving the checkpoint across the data-parallel instan ces,\nbut all nodes still need to load the entire checkpoint.\n13\n1010 1012 1014 1016\nParameters\n10−1\n100\n101\n102\nNon-offloadable (GB)\n2d Partiti ned\n3d Baseline / Impr ved\nInfiniband\n80 GB (A100)\n1010 1012 1014 1016\nParameters\n10−1\n100\n101\n102\n103\nOffl adable (GB) 3d Baseline\n2d Partiti ned\n3d Impr ved\nN  partiti n\nInfiniband\n80 GB (A100)\n1 TB ( ffl ad)\n1010 1012 1014 1016\nParameters\n10−2\n10−1\n100\n101\n102\n103\n104\n105\nTraining time (days)\n3d Baseline\n2d Partiti ned\n3d Impr ved\nN  partiti n\nInfiniband\nOne m nth\nOne year\nFigure 8: Approximate minimal memory usage and training time for sele cted training methods, with a 25\nGb/s Ethernet connection.\n9 Conclusions and future work\nWe showed that 3d parallelism is necessary for training large language models, although it remains limited\nby fundamental and practical bounds aﬀecting the training time. W ith a combination of layered gradient\naccumulation, modular pipeline parallelism and training state partition, it is possible to nearly achieve these\nbounds while using only a small fraction of the available memory. Given a suﬃciently large cluster, this\nallow eﬃciently training models above the trillion-parameter scale in a re asonable time.\nOur approach is also not particularly demanding on the (inter-node) network, being able to perform\nrelatively well with only an Ethernet connection. It also helps with the training of smaller models, which\nbeneﬁt from the improved communication overlap. We expect our me thods to allow training models such\nas BERT noticeably faster than with existing state-of-the-art me thods, in a matter of minutes. However,\nfurther research would be needed to make accurate predictions f or models of that size, as there may be an\nadditional overhead due the small size of the computational kerne ls.\n9.1 Fine-tuning and inference\nThe recent interest in extremely large models has been in large part f uelled by their potential to learn\nnew tasks on the ﬂy, potentially eliminating the need for ﬁne-tuning. When ﬁne-tuning is still needed, it\nrequires much less computing power than training from scratch, bu t the computational challenge remains to\nsome extent. For example, ﬁne-tuning a trillion-parameter model f or 1000 steps requires over 2000 GPU-\ndays, which remains too high for many researchers. Fine-tuning ca n be done on smaller clusters than those\nconsidered in this paper, which may bring back the need for oﬄoading the training state. In that regard,\nour approach improves over existing method such as the ZeRO family . The reduced need for data transfers\nallows for an easier oﬄoad, while also reducing the activation memory a nd the network requirement. Note\nthat ﬁne-tuning models above the trillion-parameter scale brings ba ck the need for 3d parallelism and large\nclusters, assuming a pre-trained model is available to begin with.\nInference represents an additional challenge, because it also nee ds to be optimized for low latency. Data\nand pipeline parallelism do not help in that regard, and tensor parallelism is limited by the node size. This sets\na lower bound of several seconds per trillion parameters, dependin g on the sequence length and computational\nprecision. Consequently, the largest language models may not be su itable for real-time applications until\nwith current hardware. As with ﬁne-tuning, oﬄoading may also be ne cessary for inference, which can run\nwith a minimal number of devices.\n14\n9.2 Computational wall\nBeyond the trillion-parameter scale, our results show an increasing ly high lower bound on the training time\nwhich makes it impossible to train arbitrarily large models. For example, a 50 trillion parameter model\ntakes at least a year to train, while a quadrillion parameter model wou ld need decades. As hardware speed\nis currently increasing at a much slower pace than the model size (an d the computational requirement), this\ncomputational wall will not be addressed by hardware speed alone. Instead, a dedicated eﬀort is needed\nto either reduce the computational requirement or provide more c omputational power through distributed\ntraining improvements.\nSparse models There is a signiﬁcant research eﬀort dedicated to reducing the com putational requirement\nof language models through sparsity, with some promising results. F or example, mixture of experts methods\nhave been shown to improve the model performance for a ﬁxed com putational budget [8, 13], while sparse\nmatrix multiplications enable training on much larger sequences [1, 4, 33]. While we leave a det ailed analysis\nfor future work, we expect sparsity to increase the network and memory usage due to the reduced arithmetic\nintensity, which should not be problematic unless pushed to the extr eme.\nHardware implications Recent trends in hardware have focused on large GPUs with as much memory\nas possible. This was motivated in good part by the requirements of s maller models as well as simplicity\nconsiderations, as large devices reduce the need for complex para llelism methods and aggressive memory\noptimizations. However, this focus on size increases costs, and sp lits the limited memory bandwidth between\nlarge numbers of computational cores, causing memory bottlenec ks.\nThe situation is diﬀerent for larger models, for which simplicity is not an option. In this case 3d parallelism\nbecomes necessary, and the unavoidable memory optimizations brin g down the memory usage to a minimum.\nInstead, the real challenge for large models lies in the cost and dura tion of training. As emphasised by our\nresults, the key to a faster training is in large scale tensor parallelism , which needs many GPUs to be\nconnected through a high bandwidth, low latency interconnect. Th e size of the GPUs is not particularly\nimportant, and in fact, (a lot of) smaller devices may be preferable a s they are less aﬀected by bandwidth\nlimitations. The GPUs only need a small amount of fast memory, as long as there is a larger amount of\nexternal storage for ﬁne-tuning and inference.\n9.3 Scaling concerns\nWhile we focused on how to train a very large transformer, we did not attempt to determine whether\none should do it, or even can aﬀord it. Training models at the trillion-parameter s cale requires thousands\nor preferably tens of thousands of GPUs, which implies astronomica l costs far beyond the means of most\nresearchers. Training at this scale also has a signiﬁcant environmen tal impact, both from the electricity\nusage and the amount of hardware being used.\nIn addition to the cost, language models are challenging from an ethic al standpoint, and the scale of\nthe model does not help on that regard (see for example [2]). Lang uage models tend to be highly biased\nand unfair, a behavior learned from the training data. Due to their p erformance, they can also be used to\nimpersonate real humans, enabling a wide range of unethical applica tions. While it is impossible to prevent\nsuch misuse in the long term, we hope that large language models beco me more accessible to the research\ncommunity in the near future, so that mitigating solutions can be fou nd as soon as possible.\nAcknowledgements\nWe would like to thank Harm de Vries for providing extensive support w hile writing the paper, Eric Robert\nand Simon B´ elanger for supporting the research project, and Na than Schucher for providing additional\nfeedback.\n15\nReferences\n[1] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer : The long-document transformer, 2020.\n[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, S andhini Agarwal, Ariel Herbert-\nVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Rames h, Daniel M. Ziegler, Jeﬀrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateus z Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radf ord, Ilya Sutskever, and Dario\nAmodei. Language models are few-shot learners, 2020.\n[3] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Trainin g deep nets with sublinear memory\ncost, 2016.\n[4] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Gen erating long sequences with sparse\ntransformers, 2019.\n[5] NVIDIA Corporation. https://www.nvidia.com/en-us/data-center/. Accessed: 2020-04-01.\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutano va. Bert: Pre-training of deep\nbidirectional transformers for language understanding, 2019.\n[7] John C. Duchi, Sorathan Chaturapruek, and Christopher R´ e. Asynchronous stochastic convex opti-\nmization, 2015.\n[8] William Fedus, Barret Zoph, and Noam Shazeer. Switch transform ers: Scaling to trillion parameter\nmodels with simple and eﬃcient sparsity, 2021.\n[9] Noah Golmant, Nikita Vemuri, Zhewei Yao, Vladimir Feinberg, Amir Gh olami, Kai Rothauge,\nMichael W. Mahoney, and Joseph Gonzalez. On the computational in eﬃciency of large batch sizes\nfor stochastic gradient descent, 2018.\n[10] Robert Hannah and Wotao Yin. On unbounded delays in asynchro nous parallel ﬁxed-point algorithms,\n2017.\n[11] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia X u Chen, Dehao Chen, HyoukJoong\nLee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. Gpipe : Eﬃcient training of giant\nneural networks using pipeline parallelism, 2019.\n[12] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Ben jamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeﬀrey Wu, and Dario Amodei. Scaling laws for ne ural language models, 2020.\n[13] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant mod els with conditional computation\nand automatic sharding, 2020.\n[14] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abde lrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence- to-sequence pre-training for natural\nlanguage generation, translation, and comprehension, 2019.\n[15] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Te am. An empirical model of large-\nbatch training, 2018.\n[16] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamo s, Erich Elsen, David Garcia, Boris\nGinsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision train-\ning, 2018.\n[17] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memory-eﬃcient\npipeline-parallel dnn training, 2021.\n16\n[18] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patric k LeGresley, Mostofa Patwary, Vijay Kor-\nthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, B ryan Catanzaro, Amar Phanishayee,\nand Matei Zaharia. Eﬃcient large-scale language model training on g pu clusters, 2021.\n[19] Feng Niu, Benjamin Recht, Christopher Re, and Stephen J. Wrig ht. Hogwild!: A lock-free approach to\nparallelizing stochastic gradient descent, 2011.\n[20] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutske ver. Improving language understand-\ning by generative pre-training, 2018.\n[21] Alec Radford, Jeﬀ Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners, 2019.\n[22] Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Shar an Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learnin g with a uniﬁed text-to-text\ntransformer, 2020.\n[23] Samyam Rajbhandari, Jeﬀ Rasley, Olatunji Ruwase, and Yuxion g He. Zero: Memory optimizations\ntoward training trillion parameter models, 2020.\n[24] Samyam Rajbhandari, Olatunji Ruwase, Jeﬀ Rasley, Shaden Sm ith, and Yuxiong He. Zero-inﬁnity:\nBreaking the gpu memory wall for extreme scale deep learning, 2021 .\n[25] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunj i Ruwase, Shuangyan Yang, Minjia\nZhang, Dong Li, and Yuxiong He. Zero-oﬄoad: Democratizing billion-s cale model training, 2021.\n[26] Christopher J. Shallue, Jaehoon Lee, Joseph Antognini, Jasch a Sohl-Dickstein, Roy Frostig, and\nGeorge E. Dahl. Measuring the eﬀects of data parallelism on neural n etwork training, 2019.\n[27] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGr esley, Jared Casper, and Bryan Catan-\nzaro. Megatron-lm: Training multi-billion parameter language models u sing model parallelism, 2020.\n[28] Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V . Le. Don’t decay the learning rate,\nincrease the batch size, 2018.\n[29] Sebastian U. Stich, Amirkeivan Mohtashami, and Martin Jaggi. Cr itical parameters for scalable dis-\ntributed learning with large batches and asynchronous updates, 2 021.\n[30] Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhan dari, Conglong Li, Xiangru Lian,\nJi Liu, Ce Zhang, and Yuxiong He. 1-bit adam: Communication eﬃcient large-scale training with\nadam’s convergence speed, 2021.\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need, 2017.\n[32] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhu tdinov, and Quoc V. Le. Xlnet:\nGeneralized autoregressive pretraining for language understand ing, 2020.\n[33] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ains lie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big b ird: Transformers for\nlonger sequences, 2021.\nA Hardware details\nThis appendix summarizes the hardware speciﬁcations relevant to t his paper. For more details, see the\noﬃcial documentation [5]\nFor the purpose of this paper, the computation is assumed to be do ne on NVIDIA A100 80 GB GPUs.\nSuch device has a theoretical peak fp16 compute cgpu = 312 Tﬂop/s, although this value is diﬃcult to reach\nin practice. The device has mgpu = 80 GB of memory, with a peak bandwidth βgpu = 2039 GB/s. Each\n17\nTable A.1: Bandwidth and arithmetic intensity with respect to a A100 G PU for the available network\ninterconnects and data storages.\nNetwork\nBandwidth Arithmetic intensity threshold\nInput + Output @ 312 Tﬂop/s\nGB/s ﬂops/B\nGPU memory 2039 143\nNVLINK 600 484\nPCI-express 63 4.61 k\nInﬁniBand (200 Gb/s) 50 5.81 k\nCPU-GPU 31.5 9.22 k\nEthernet (25 Gb/s) 6.25 46.5 k\nDisk (NVMe) 3.2 90.8 k\nDisk (Hard drive) 0.1 2.91 M\nGPU is connected to the rest of the computer with a PCIe 4.0 x16 con nector and can be connected to oher\nGPUs with up to 12 NVLink interconnects, for a total bandwidth of 3 00 GB/s in each direction. When\ncombined with NVSwitch, NVLink allows up to 16 GPUs to be fully connect ed, each pairwise connection\nbeing able to fully utilize the NVLink bandwidth.\nAlthough we do not assume a speciﬁc computer architecture, we us e a 16-GPU DGX or HGX A100 is as\na reference 9. The GPUs are fully connected through NVSwitch, and each pair is co nnected to a PCI-express\nswitch which also connects to a pair of 200 Gb/s InﬁniBand interconn ects and to one of the CPUs. While there\nis a total of 16 InﬁniBand connectors, each GPU can only eﬃciently u se one connector due to the PCI-express\nbottleneck. The CPUs are bundled with a large amount of memory, an Ethernet connection, additional\noptional InﬁniBand connectors, and other standard computer c omponents. The Ethernet connection scales\nto at least 400 Gb/s, which amounts to 25 Gb/s per GPU 10.\nThe PCI-express conﬁguration of the HGX server creates a bott leneck on the CPU side, since a single\nPCIe 4.0 x16 connects the CPU to two GPUs and two InﬁniBand interc onnects, eﬀectively dividing the\nmaximum CPU-GPU throughput by half and preventing the CPUs to eﬃ ciently connect with InﬁniBand.\nThis is problematic in the oﬄoading scenarios where data is transferr ed through the CPUs. The problem\ncould be ﬁxed with improved computer designs but remains an importa nt practical limitation.\nThe various bandwidths and arithmetic intensities are summarized in t able A.1.\nB Transformer scaling\nTransformers are largely insensitive to the exact values of the hyp erparameters [12], assuming they fall\nwithin a reasonable range, so the driving factor when scaling the mod el is computational eﬃciency. The\nintermediate size factor nI should be kept constant, and we use the common value nI = 4. The sequence\nlength and head size should be proportional to keep the intermediat e activations balanced. The number\nof heads can scale independently but has an impact on parallelism and t he allowed values of the sequence\nlength. In this paper we assume a mild scaling ds ∼ d1/ 2\nm , which we achieve with the reasonable relative\nscaling ds = 8 dh = 32 da. These ratios lead to values comparable to what is found in the literat ure, and\nmatch BERT for da = 16, but diﬀer slightly from GPT-3 which gives more weight to the head count.\nThe layer count scaling is relevant to parallelism, as deeper networks are suitable to pipeline parallelism,\nwhile wider ones enable more tensor parallelism. The impact is minimal give n that both are limited by\nother factors, but wider networks are slightly preferable as they lead to more eﬃcient tensor parallelism.\nIn terms of memory, deep and thin networks reduce the size of the buﬀers and activations, at the cost of\n9As no detailed architecture could be found, we extrapolated some details from other DGX and HGX nodes. At the time of\nwriting this paper, there does not appear to be a 16 GPU versio n for the DGX A100.\n10In a data parallel scenario, network operations use a ring to pology which eﬀectively makes the entire Ethernet bandwidt h\navailable to a single device. We ignore this fact for simplic ity, and in any case, this does not happen with tensor paralle lism\nwhere all devices communicate with other nodes.\n18\nTable B.1: X [x] model conﬁguration examples for a wide range of scales and compar ison to some existing\nlarge transformer. Parameter counts exclude the embedding laye r and language model head.\nModel p b c (b) ds da dh dm dl\nX2 488 130 32 1 4 4 2\nBERT 301 M 751 (256) 512 16 64 1024 24\nX32 403 M 826 512 16 64 1024 32\nMegatron-LM 8.15 B 1130 (512) 1024 32 96 3072 72\nX64 12.9 B 1310 1024 32 128 4096 64\nT-NLG 17.0 B 1440 (512) 1024 28 152 4256 78\nGPT-3 174 B 1560 2048 96 128 12288 96\nX108 176 B 1860 1728 54 216 11664 108\nX160 1.26 T 2420 2560 80 320 25600 160\nX[x] 12x5 + 13x3 82.0x2/ 3 16x 1\n2 x 2x x 2 x\nlarger activation checkpoints. As parallelism is much more important t han memory, we select a mild scaling\ndl = √ dm.\nThe resulting model family X [x] is parametrized by a single variable x:\nda = 1\n2x, d h = 2 x, d l = x,\nds = 16 x, d m = x2, d I = 4 x2. (1)\nTable B.1 shows some examples of X [x] models and a comparison to other large language models.\nIn [12] it was found empirically that the critical batch size scales appr oximately as p1/ 3, when measured\nin tokens. To obtain a numerical value, we assume that GPT-3 was tr ained at the critical batch size (3.2 M\ntokens). This results in the empirical formula\nbc ≈ 573 p1/ 3\nds\n≈ 82.0x2/ 3. (2)\nAlthough equation 2 is approximate and was not demonstrated to sc ale for the whole parameter range studied\nin this paper, we take it as the true value of the critical batch size fo r numerical estimations 11.\nC Resource usage\nC.1 Computation\nIn a transformer, as with nearly all deep learning models, the bulk of the computation is performed in\nthe matrix multiplications. These appear in the weight multiplications in t he dense layers, and in the self-\nattention mechanism, but the self-attention matrix multiplications a re much smaller in general and can be\nneglected. For the forward pass, this leads to a computational co st of two ﬂoating point operations for each\ninput token and parameter, or 2 bdsp ﬂops per batch. In the backward pass, the parameter and layer g radient\ncomputation each require a similar amount of computation, to which is added the activation re-computation,\nfor a total of three times the forward pass computation. Summing up, each batch requires 8 bdsp ﬂops of\ncomputation, or 8bdsp\nngpu\nfor each device.\n11The dependence on the sequence length was not demonstrated i n [12], and we make the reasonable approximation that the\ncritical batch size measured in tokens does not depend on the sequence length.\n19\nTable C.1: Operation sequences and their resource usage for layer buﬀering methods. All values are relative\nto the double buﬀered forward pass.\nStream 1 Stream 2 Parameter Gradient Computation Network Arith metic\nbuﬀers buﬀers Intensity\nForward\nActivations(i − 1) Restore( i) 2 0 1 1 1\nActivations(i) Restore( i + 1) 2 0 1 1 1\nBackward\nGradients(i − 1) Restore( i) 2 1 2 1 2\nActivations(i) Reduce( i − 1) 1 1 1 1 1\nGradients(i) Restore( i + 1) 2 1 2 1 2\nActivations(i + 1) Reduce( i) 1 1 1 1 1\nC.2 Parameter and gradient buﬀering\nTo determine the memory usage, we need to determine the size and lif etime of the parameter and gradient\nbuﬀers. Each parameter is used in both the forward and backward pass, so should be restored at least\ntwice. A convenient choice is to deﬁne a buﬀer for all the parameter s in a layer, i.e., between two activation\ncheckpoints, and similarly for the gradients. To allow overlapping the communication, two parameter buﬀers\nare needed, but one gradient buﬀer is suﬃcient. This mixed buﬀering method (as opposed to a strict single\nor double buﬀering) is summarized in table C.1.\nNote that in the absence of gradient accumulation, it is possible to ac hieve a lower memory usage with\nbuﬀers deﬁned at the sub-layer level, possibly even splitting the par ameter within a single operation as\nsuggested in [24]. Doing so requires restoring the parameters an ex tra time in the backward pass, but this\ncan be done without increasing the network requirement by leverag ing the arithmetic intensity imbalance\n(see table C.1). However, with gradient accumulation, the network operations need to happen for each\nmicro-batch, which leads to an excessive network requirement and defeats the purpose of layered gradient\naccumulation. In any case, we ﬁnd mixed buﬀering suﬃcient for all re alistic scenarios.\nC.3 Memory\nAs described in section 2.5, the memory usage breaks down into four categories: the training state, the\nactivation checkpoints, the parameter and gradient buﬀers, and the layer activations. The ﬁrst two can be\noﬄoaded to CPU memory, but not the latter two which ultimately dete rmine how big the model can grow\nfrom a memory perspective.\nWith the Adam optimiser, the training state consists of the model pa rameters as well as their running\nmean and variance, all stored with single precision, for a total of 12 p bytes. The gradients would take an\nextra 2 p bytes, but we can e this to a negligible amount by updating the weights as soon as possible. In the\nnon-partitioned case, the state is split across the model-parallel in stances ( 12p\nnlna\nbytes per device), while in\nthe partitioned case it is split across all devices ( 12p\nngpu\nbytes each).\nIn the mixed buﬀering method, two parameter and one gradient buﬀ ers are needed, each being the size\nof a single transformer layer. The buﬀers are split in the tensor par allel dimension for a total of 6pl\nna\nbytes\nper GPU. Note that while the buﬀers are much smaller than the trainin g state, they do not shrink much\nwith parallelism so can still be important.\nThe checkpoints are assumed to match the output of each transf ormer, which works relatively well in\npractice, for a total of 2 bdsdmdl. They split naturally over the data and pipeline parallel dimensions and\ncan also be partitioned in the tensor parallel dimension, for a memory usage of 2bdsdmdl\nngpu\nbytes per device.\nThis formula is not optimal, as for example some intermediate computa tions can be combined together\ninto fused kernels, but it is suﬃcient for the purpose of this paper a s the activation memory remains low\nenough. The activation memory is split between the micro-batches a nd the tensor-parallel instances, for a\ntotal of bdsm0\nnbnµ na\n20\nC.4 Network\n3d parallelism involves three kinds of network communication, one for each parallelism dimension. This\nsection aims at evaluating the bandwidth requirement for each, as w ell as the associated arithmetic intensity.\nFor the communication to be overlapped perfectly, the arithmetic in tensity νop for the computation with\nrespect to the network transfer needs to be higher than the arit hmetic intensity νnet implied by the GPU\nand the network, or\nνop ≥ νnet, (3)\nWhen eﬃcient overlap is not possible, there is a relative overhead νnet\nνop\n. This overhead is expected to remain\nwithin a chosen threshold ǫ, resulting in the condition\nǫ ν op ≥ νnet. (4)\nC.4.1 Data parallel\nIn the non-partitioned case, the gradient reduction involves a sca tter-reduce and an all-gather, which are\nboth identical from a computational perspective and are generally implemented with bandwidth-optimal ring\nmethods. In an all-gather for instance, each device receives all th e gradients except for the ones it already\nhas and sends the same amount of data. For the gradient reductio n, this results in a bandwidth usage of\n8(nb− 1)p\nngpu\n≈ 8p\nnlna\n. The reduction is overlapped with the backward pass for the last mic ro-batch, except for\nboundary eﬀects at the ﬁrst and layers. When comparing with the b ackward pass compute of 6bdsp\nnµ nbnlna\nresulting in an arithmetic intensity\nνbase\nb ≈ 3bds\n4nbnµ\n, (5)\nassuming dn/nl is large enough. This means the overlap gets worse with data parallelis m and micro-batching,\nand by extension with pipeline parallelism. In the latter case case this is due to poor communication overlap,\nand for dl = nl there is no overlap at all. Because of that, the non-overlapped sce nario is more appropriate,\nwith\nνpipe\nb ≈ bds\nnb\n. (6)\nIn the partitioned case, there is an extra all-gather in the forward pass, and the network operations need\nto be done for each micro-batch, resulting in 3\n2 nµ times the network bandwidth when compared with the\nnon-partitioned case. The lowest arithmetic intensity is in the forwa rd pass, with the value\nνbase-part\nb ≈ bds\n2nbnµ\n. (7)\nThis is only 33% lower than without the partition, but in the micro-batc hed case the overlap is with all the\nmicro-batches rather than the last one, so the non-overlapped s cenario does not apply.\nWith layered gradient accumulation, the network usage remains the same, but can be overlapped with\nthe entire backward pass even in the micro-batched case (again ex cluding boundary eﬀects). This remains\ntrue with pipeline parallelism, in the recommended setting where dl\nnl\nis not too small. This results in the\narithmetic intensity\nνimpr\nb ≈ 3bds\n4nb\n, (8)\nor\nνimpr-part\nb ≈ bds\n2nb\n, (9)\ndepending on whether the state is partitioned.\n21\nC.4.2 Pipeline parallel\nWith pipeline parallelism, each instance needs to receive its inputs and s end its outputs, and the potential\nbottleneck is in the forward pass. In the baseline, each micro-batc h the forward pass involves 4bdsdm\nnbna\nbytes\nof communication and 2bdsp\nngpu\nﬂops of computation, for an arithmetic intensity of\nνbase\nl ≈ (2 + nI )dmdl\nnl\n. (10)\nWith modular pipeline parallelism, the number of transfers is multiplied by dl\nnl\n, for an intensity\nνimpr\nl ≈ (2 + nI )dm, (11)\nwhich is high enough for large models.\nThe data transfer can be overlapped with the computation, but it is diﬃcult to do so with the minimal\nnumber of micro-batches nµ = nl. This can be addressed by adding a small number of extra micro-bat ches,\napproximately given by νl\nνnet\nnµ , or more is the network speed ﬂuctuates.\nC.4.3 Tensor parallel\nFollowing the approach of [27], tensor parallelism requires two all-redu ce operations for each transformer\nlayer in the forward pass, which are not overlapped with computatio n. An extra two all-reduces are needed\nin the gradient computation, for a total of six (when including the ac tivation re-computation). For a given\nlayer, this implies a network usage of 24bdsdm(na− 1)\nnbna\nbytes compared with a compute of 8bdspl\nnbna\nﬂop, for an\nintensity\nνa ≈ (4 + 2 nI )dm\n3(na − 1) . (12)\nC.5 CPU-GPU transfers\nOﬄoading the training state and activation checkpoints requires lar ge data transfers between the CPU and\nGPU memory. For the state oﬄoading, the computation for a given la yer is overlapped with the transfer\nof the parameters and the gradients for a given layer or its partitio n. In the baseline, the transfer needs to\nhappen for each micro-batch, while with layered gradient accumulat ion it happens once for all micro-batches.\nThe bottleneck is in the forward pass, with four possible values depe nding on the scenario:\nνbase\ns ≈ bds\nnµ nb\n, ν base− part\ns ≈ bds\nnµ\nνimpr\ns ≈ bds\nnb\n, ν impr− part\ns ≈ bds (13)\nFor checkpoint oﬄoad the computation is similar to the pipeline-paralle l network transfer, with half the\ncomputation, for an intensity\nνc ≈ (4 + 2 nI )dm (14)\n22",
  "topic": "Pipeline (software)",
  "concepts": [
    {
      "name": "Pipeline (software)",
      "score": 0.7980512380599976
    },
    {
      "name": "Parallelism (grammar)",
      "score": 0.7958128452301025
    },
    {
      "name": "Modular design",
      "score": 0.7928266525268555
    },
    {
      "name": "Computer science",
      "score": 0.7121880054473877
    },
    {
      "name": "Parallel computing",
      "score": 0.702829122543335
    },
    {
      "name": "Training (meteorology)",
      "score": 0.4921781122684479
    },
    {
      "name": "Programming language",
      "score": 0.31356674432754517
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Meteorology",
      "score": 0.0
    }
  ]
}