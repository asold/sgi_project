{
    "title": "A Unified Transformer-based Framework for Duplex Text Normalization",
    "url": "https://openalex.org/W3194932072",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4222670224",
            "name": "Lai, Tuan Manh",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1999911198",
            "name": "Zhang Yang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222949136",
            "name": "Bakhturina, Evelina",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222949138",
            "name": "Ginsburg, Boris",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2604355407",
            "name": "Ji, Heng",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2339294230",
        "https://openalex.org/W2895605766",
        "https://openalex.org/W2747175885",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2924677654",
        "https://openalex.org/W3008118770",
        "https://openalex.org/W3161730088",
        "https://openalex.org/W2016589492",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W3158005503",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2103085228",
        "https://openalex.org/W1570629387",
        "https://openalex.org/W3154516348",
        "https://openalex.org/W2742947407",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W2163489213",
        "https://openalex.org/W2806253224",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3160789530",
        "https://openalex.org/W3176328530",
        "https://openalex.org/W3150776450",
        "https://openalex.org/W2546744831",
        "https://openalex.org/W2945656493"
    ],
    "abstract": "Text normalization (TN) and inverse text normalization (ITN) are essential preprocessing and postprocessing steps for text-to-speech synthesis and automatic speech recognition, respectively. Many methods have been proposed for either TN or ITN, ranging from weighted finite-state transducers to neural networks. Despite their impressive performance, these methods aim to tackle only one of the two tasks but not both. As a result, in a complete spoken dialog system, two separate models for TN and ITN need to be built. This heterogeneity increases the technical complexity of the system, which in turn increases the cost of maintenance in a production setting. Motivated by this observation, we propose a unified framework for building a single neural duplex system that can simultaneously handle TN and ITN. Combined with a simple but effective data augmentation method, our systems achieve state-of-the-art results on the Google TN dataset for English and Russian. They can also reach over 95% sentence-level accuracy on an internal English TN dataset without any additional fine-tuning. In addition, we also create a cleaned dataset from the Spoken Wikipedia Corpora for German and report the performance of our systems on the dataset. Overall, experimental results demonstrate the proposed duplex text normalization framework is highly effective and applicable to a range of domains and languages",
    "full_text": "A UNIFIED TRANSFORMER-BASED FRAMEWORK FOR DUPLEX TEXT\nNORMALIZATION\nTuan Manh Lai 2, Yang Zhang 1, Evelina Bakhturina 1, Boris Ginsburg 1, Heng Ji 2\n1 NVIDIA\n2 University of Illinois at Urbana-Champaign\nABSTRACT\nText normalization (TN) and inverse text normalization (ITN)\nare essential preprocessing and postprocessing steps for text-\nto-speech synthesis and automatic speech recognition, respec-\ntively. Many methods have been proposed for either TN or\nITN, ranging from weighted ﬁnite-state transducers to neural\nnetworks. Despite their impressive performance, these meth-\nods aim to tackle only one of the two tasks but not both. As\na result, in a complete spoken dialog system, two separate\nmodels for TN and ITN need to be built. This heterogene-\nity increases the technical complexity of the system, which in\nturn increases the cost of maintenance in a production setting.\nMotivated by this observation, we propose a uniﬁed frame-\nwork for building a single neural duplex system that can si-\nmultaneously handle TN and ITN. Combined with a simple\nbut effective data augmentation method, our systems achieve\nstate-of-the-art results on the Google TN dataset for English\nand Russian. They can also reach over 95% sentence-level\naccuracy on an internal English TN dataset without any ad-\nditional ﬁne-tuning. In addition, we also create a cleaned\ndataset from the Spoken Wikipedia Corpora for German and\nreport the performance of our systems on the dataset 1. Over-\nall, experimental results demonstrate the proposed duplex text\nnormalization framework is highly effective and applicable to\na range of domains and languages 2.\nIndex Terms— Text Normalization, Transformer Models\n1. INTRODUCTION\nText normalization (TN) is the process of converting written\ntext to its spoken form. For example, 72 people were found\nshould be verbalized as seventy two people were found (Fig.\n1). TN is usually the ﬁrst preprocessing step in a text-to-\nspeech (TTS) system [1]. On the other hand, inverse text nor-\nmalization (ITN) refers to the inverse process, which trans-\nforms spoken-domain text into its written form [2]. ITN is\ntypically an important post-processing step of an automatic\nspeech recognition (ASR) system. A challenge in TN/ITN is\n1 The German dataset will be released upon the publication of this work\n2 We have integrated our framework into NeMo, Nvidia’s open-source\nConversational AI toolkit https://github.com/NVIDIA/NeMo.\nFig. 1. Text normalization and its inverse problem.\nthe variety of semiotic classes [3, 4]. Semiotic class denotes\nthings like numbers, dates, times, etc., whose written forms\nare typically different from spoken forms. Another challenge\nis that there is low tolerance towards unrecoverable errors in\na production setting. In the context of TN, acceptable errors\nare less severe errors that typically involve picking the wrong\nform of a word while otherwise preserving the original mean-\ning (e.g., 35 mins → thirty ﬁve minute). In contrast, unrecov-\nerable errors are errors where a totally different meaning is\nbeing conveyed (e.g., 35 mins → forty ﬁve minutes).\nTraditional approaches to TN/ITN typically use hand-\nwritten grammars in the form of weighted ﬁnite-state trans-\nducers (WFST) [5, 6, 7] to handle semiotic classes such as\ndate (e.g., May 24 ↔ May twenty fourth ) or numbers (e.g.,\n72 ↔ seventy two ). While WFST-based approaches work\nreasonably well and have previously been adopted by sev-\neral production systems [6, 4, 8], they are expensive to scale\nacross different languages. Due to the long tail of special\ncases, constructing WFST-based grammars typically requires\nextensive linguistic knowledge and manual effort to design\nhandcrafted rules.\nWith the rise of neural networks (NN) in natural lan-\nguage processing (NLP) [9, 10, 11], several deep learning\nmodels have recently been introduced for either TN or ITN\n[4, 12, 1, 13]. For example, [14] uses recurrent NN-s to learn\na TN function from a large corpus of written text aligned to its\nspoken form. [15] explores the use of convolutional NN-s for\nTN. In [8], the author uses a sequence-to-sequence (seq2seq)\nmodel for TN with byte pair encoding (BPE) as sub-word\nunits. More recently, [2] investigates the use of Transformer-\nbased models for ITN. While these neural-based methods\narXiv:2108.09889v1  [cs.CL]  23 Aug 2021\nFig. 2. Overview of our duplex text normalization framework. In this example, the input to our system is a written text, and it\nneeds to perform TN. Therefore, the task indicator is set to be TN.\ncan achieve impressive performance, they are not designed to\nwork with TN and ITN simultaneously. As a result, to build\na complete spoken dialog system with both TTS and ASR\ncapabilities, two separate models for TN and ITN need to\nbe implemented. This heterogeneity increases the technical\ncomplexity of the system, which in turn increases the cost of\nmaintenance in a production setting.\nIn this work, we pursue an ambitious goal of address-\ning TN and ITN jointly. We propose a uniﬁed framework\nfor building a single duplex system that can simultaneously\nhandle the two tasks. Combined with a simple but effec-\ntive data augmentation method, our Transformer-based sys-\ntems achieve impressive results on both public and internal\ndatasets. When trained on the public Google TN dataset [16],\nour systems achieve state-of-the-art results for both English\nand Russian. They can also reach over 95% sentence-level\naccuracy on an internal English TN dataset without any ad-\nditional ﬁne-tuning. In addition, we also create a cleaned\ndataset from the Spoken Wikipedia Corpora [17] for German\nand report the performance of our systems on the dataset.\nOverall, our proposed framework is highly effective and ap-\nplicable to a range of domains and languages.\n2. METHODS\nInspired by previous work [4], our framework consists of\ntwo main components (Figure 2). Given an input sentence, a\nTransformer-based tagger is ﬁrst used to identify all thesemi-\notic spans in the input (i.e., numbers, times, dates, monetary\namounts, etc.) (Section 2.1). After that, a Transformer-based\nTag Description\n{B,I}-TASK The task indicator.\n{B,I}-SAME A span that should be kept the same.\n{B,I}-PUNCT A punctuation.\n{B,I}-TRANSFORM A semiotic span\nTable 1. The label set that the tagger uses. The preﬁx B-\nindicates the beginning. Any other token after the ﬁrst is given\nthe preﬁx I-.\nnormalizer is used to convert the semiotic spans into their\nappropriate forms (Section 2.2). For TN/ITN, typically, most\nof the tokens in the input can be kept the same. Tokens that\nneed to be transformed belong to a small set of semiotic\nclasses (e.g., measure, money, cardinal number, date, or time)\n[3]. Because of the tagger, the seq2seq normalizer only needs\nto work with few input spans. The normalizer does not have\nto transform the entire original input.\nOur framework uniﬁes TN and ITN by using task-\nindicating preﬁxes. To allow duplex mode, we append a\ntask indicator preﬁx to each input to indicate whether the\ninput is for TN or ITN [18]. This approach is conceptually\nsimple, easy to implement, and effective. A single duplex\nsystem is also easier to maintain than two separate systems\n(one for TN, one for ITN).\n2.1. Transformer-Based Tagger\nGiven an original input sequence T = (t1, ..., tn) consisting\nof n tokens, we ﬁrst append a task indicator tokent0 to the be-\nginning of the sequence to indicate whether the model needs\nto do TN or ITN (i.e., t0 ∈ {TN, ITN}). Therefore, the actual\ninput sequence to our tagger is (t0, t1, ..., tn). The role of the\ntagger is to predict a sequence of labels(y0, y1, ..., yn), where\nyi is the label corresponding to tokenti. Table 1 describes the\nlabel set that the tagger uses.\nOur tagger ﬁrst forms a contextualized representation for\neach input token using a Transformer encoder such as BERT\n[19] or RoBERTa [20]. Let X = (x0, ...,xn) be the output\nof the Transformer encoder, where xi ∈ Rd. We then feed\nthe representations into a softmax layer to classify over the\ntagging labels (Table 1):\noi = softmax(Wxi + b) ∀xi ∈ X (1)\nwhere W ∈ R8×d and b ∈ R8 are trainable parameters. oi ∈\nR8 is a vector containing the predicted logits for token ti. To\ntrain the tagger, we use the cross-entropy loss function.\n2.2. Transformer-Based Normalizer\nLet S = {s1, ..., sm} be the set of all (predicted) semiotic\nspans in the input sequence T. Here, m denotes the number\nof semiotic spans. The role of the normalizer is to transform\neach semiotic span into its appropriate form (e.g., its spoken\nform if T is a piece of written text and the task is TN). For\nsemiotic span si, the actual input to the normalizer includes\nthe task indicator, the left context of si, the textual content of\nsi, and the right context of si (Figure 2). We use two special\ntokens (denoted as <m> and </m> in Figure 2) to separate\neach semiotic span from its context. Since the surrounding\ncontext of a semiotic span may contain another semiotic span,\nthe two special tokens are needed to highlight the span that the\nnormalizer needs to pay the most attention to.\nOur normalizer model is based on the standard encoder-\ndecoder Transformer architecture [21]. First, an input se-\nquence of tokens is mapped into a sequence of input embed-\ndings, which is then passed into the encoder. The encoder\nconsists of a stack of Transformer layers that map the se-\nquence of input embeddings into a sequence of feature vec-\ntors. The decoder is also Transformer-based. It produces\nan output sequence in auto-regressive manner: at each out-\nput time-step, the decoder attends to the encoder’s output se-\nquence and to its previous outputs to predict the next output\ntoken. The normalizer model is trained using standard max-\nimum likelihood, i.e., using teacher forcing [22] and a cross-\nentropy loss.\nTo make the normalizer more robust against the tagger’s\npotential errors, we train the normalizer with not only correct\nsemiotic spans but also with some other more “noisy” spans\n(Fig. 3). For example, let’s consider the sentence “remind\nme at 4 pm today please”. In addition to the semiotic span\n“4 pm” (and its context), we also use other spans such as “at\n4 pm today” (and their contexts) as training examples for the\nnormalizer (for this augmented case, the target output should\nFig. 3. Data augmentation strategy for the normalizer.\nComponent Language Model\nEnglish Tagger distilroberta-base\nEnglish Normalizer t5-base\nRussian Tagger cointegrated/rubert-tiny\nRussian Normalizer cointegrated/rut5-base\nGerman Tagger bert-base-german-cased\nGerman Normalizer google/mt5-base\nTable 2. The pretrained language models we use in this work.\nThe models are referred by their names in HuggingFace’s\nTransformers library.\nbe “at four p m today”). This way even if the tagger makes\nsome errors, there will still be some chance that the ﬁnal out-\nput is still correct.\n3. EXPERIMENTS AND RESULTS\nData and Experimental Setup For English and Russian, we\nuse the standard Google TN dataset for training [16]. For Ger-\nman, we create a cleaned dataset from the Spoken Wikipedia\nCorpora [17]. We use pretrained Transformer-based language\nmodels from HuggingFace’s library 3. For English, we use\na distilled version of RoBERTa for the tagger [20] and T5-\nbase for the normalizer [18]. For Russian, we use a distilled\nversion of the multilingual version of BERT for the tagger\n[19], and a distilled version of the multilingual mT5 for the\nnormalizer [24]. For German, we use a German version of\nBERT for the tagger [19], and we use the multilingual mT5\nfor the normalizer [24]. Table 2 summarizes the list of lan-\nguage models we use. The TN framework is open-sourced in\nNeMo 4, where one can ﬁnd an implementation details and\ntraining hyper-parameters.\nComparison with Previous Methods. Table 3 summa-\nrizes the performance of our systems and compares them\nto other baselines. The duplex systems trained with data\naugmentation outperform the baselines on all languages. Fur-\nthermore, the duplex systems achieve results comparable to\n3 https://github.com/huggingface/transformers\n4 https://github.com/NVIDIA/NeMo\nEnglish Russian German\nTN ITN TN ITN TN ITN\nOur models\nDuplex System 98.36† 93.17† 96.21† 85.67† 94.34† 87.71†\nSimplex TN-only System 98.34 - 96.30† - 93.28† -\nSimplex ITN-only System - 93.07† - 85.55† - 87.04†\nOur models w/o data augmentation\nDuplex System 98.25 93.02 95.07 79.55 91 .37 86.07\nSimplex TN-only System 98.41 - 94.93 - 89.36 -\nSimplex ITN-only System - 92.97 - 79.50 - 83.79\nBaseline models\nRNN-based Sliding Window Model [4] 97.75 - 95.46 - - -\nTransformer-based Seq2Seq Model [4] 96.53 - 93.35 - - -\nNeMo’s WFSTs [23] 80.19 75.65 - 50.96 - 51.73\nTable 3. Results on the Google dataset (English, Russian) and the Spoken Wikipedia Corpus (German). Sentence-level accuracy\nscores (%) are shown. Duplex systems are trained using both TN and ITN instances. Simplex systems are trained using either\nTN instances or ITN instances (but not both). We use the symbol † to indicate the cases where data augmentation improves\nperformance.\nModels TN Accuracy\nDuplex System 98.34\nDuplex System (w/o augmentation) 96.58\nSimplex TN-only System 97.01\nSimplex TN-only System (w/o augmentation) 96.39\nTable 4. The performance of our English systems on NLU\nAssistant, an internal English TN dataset. Sentence-level ac-\ncuracy scores (%) are shown.\nor even better than the simplex systems. Finally, using data\naugmentation consistently improves the performance except\nonly when training an English simplex TN-only system.\nResults on Internal Dataset. We evaluate some of our\nEnglish systems on NLU Assistant, an internal English TN\ndataset (Table 4). The dataset consists of about 2100 utter-\nances between humans and automated personal assistants.\nSome example utterances are “set the alarm at 10 am” and\n“show me the weather on 27/03/2017”. All the systems can\nreach over 95% sentence-level accuracy on NLU Assistant.\nNote that we only train the systems on the Google dataset and\ndo not ﬁnetune them on NLU Assistant. While the instances\nin the Google dataset come from Wikipedia, the instances in\nNLU Assistant are from the conversational domain. These\nresults demonstrate the generalizability of our systems.\nError Analysis. We have manually analyzed the errors made\nby our English duplex system for TN. Among 7551 test in-\nstances, our model makes mistakes in 124 cases (1.64%).\nHowever, 113 of the cases are acceptable errors, and only\n11 cases (0.146%) are unrecoverable errors. Among the 11\nInput: ... PMID 10667370 ...\nOutput: ... p m i d one million sixty six thousand seven hun-\ndred seventy ...\nCategory: Number\nInput: ... discussion on Gizmodo.com ...\nOutput: ... discussion on gi zi z modo dot com ...\nCategory: URL\nInput: ... Highights of the ASAPS 2013 ...\nOutput: ... Highights of the a a p a s twenty thirteen ...\nCategory: Miscellaneous\nTable 5. Some of the unrecoverable TN errors.\nunrecoverable errors, seven are related to URLs, three are\nrelated to numbers, and one is miscellaneous (Table 5).\n4. CONCLUSION\nThis paper introduces a novel uniﬁed framework for build-\ning duplex systems that can simultaneously handle both di-\nrect and and inverse text normalization. Experimental results\non both public and internal datasets demonstrate the effective-\nness of our framework. Our best systems achieve state-of-the\nart results on the Google TN dataset. An interesting future di-\nrection is to investigate semi-supervised learning techniques\nto reduce the amount of data required for training our systems.\nAnother direction is to build a single multilingual duplex sys-\ntem that simultaneously handles multiple languages.\n5. REFERENCES\n[1] Shubhi Tyagi, A. Bonafonte, Jaime Lorenzo-Trueba,\nand Javier Latorre, “Proteno: Text normalization with\nlimited data for fast deployment in text to speech sys-\ntems,” in NAACL, 2021.\n[2] Monica Sunkara, Chaitanya P. Shivade, S. Bodapati, and\nKatrin Kirchhoff, “Neural inverse text normalization,”\nin ICASSP, 2021.\n[3] Paul Taylor, Text-to-Speech Synthesis, Cambridge Uni-\nversity Press, 2009.\n[4] Hao Zhang, Richard Sproat, Axel H Ng, Felix Stahlberg,\nXiaochang Peng, Kyle Gorman, and Brian Roark, “Neu-\nral models of text normalization for speech applica-\ntions,” Computational Linguistics, vol. 45, no. 2, pp.\n293–337, 2019.\n[5] B. Roark, R. Sproat, C. Allauzen, M. Riley, J. S.\nSorensen, and T. Tai, “The OpenGrm open-source\nﬁnite-state grammar software libraries,” in ACL, 2012.\n[6] P. Ebden and R. Sproat, “The Kestrel TTS text normal-\nization system,” Natural Language Engineering , vol.\n21, pp. 333 – 353, 2014.\n[7] K. Sodimana, P. D. Silva, R. Sproat, T. Wattanavekin,\nA. Gutkin, and K. Pipatsrisawat, “Text normalization\nfor Bangla, Khmer, Nepali, Javanese, Sinhala and Sun-\ndanese text-to-speech systems,” in SLTU, 2018.\n[8] C. Mansﬁeld, M. Sun, Y . Liu, A. Gandhe, and\nB. Hoffmeister, “Neural text normalization with sub-\nword units,” in NAACL, 2019.\n[9] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Re-\ncent trends in deep learning based natural language pro-\ncessing,” IEEE Computational Intelligence Magazine ,\nvol. 13, no. 3, pp. 55–75, 2018.\n[10] Tuan Lai, Heng Ji, Trung Bui, Quan Hung Tran,\nFranck Dernoncourt, and Walter Chang, “A context-\ndependent gated module for incorporating symbolic\nsemantics into event coreference resolution,” arXiv\npreprint arXiv:2104.01697, 2021.\n[11] Tuan Lai, Heng Ji, ChengXiang Zhai, and Quan Hung\nTran, “Joint biomedical entity and relation extraction\nwith knowledge-enhanced collective inference,” arXiv\npreprint arXiv:2105.13456, 2021.\n[12] L. Huang, S. Zhuang, and K. Wang, “A text normal-\nization method for speech synthesis based on local at-\ntention mechanism,” IEEE Access, vol. 8, pp. 36202–\n36209, 2020.\n[13] W. Jiang, J. Li, M. Chen, J. Ma, S. Wang, and Xiao\nJ, “Improving neural text normalization with partial pa-\nrameter generator and pointer-generator network,” in\nICASSP, 2021.\n[14] R. Sproat and Navdeep Jaitly, “RNN approaches to text\nnormalization: A challenge,” ArXiv:1611.00068, 2016.\n[15] S. Yolchuyeva, G. N ´emeth, and B. Gyires-T ´oth, “Text\nnormalization with convolutional neural networks,” In-\nternational Journal of Speech Technology , vol. 21, pp.\n589–600, 2018.\n[16] R. Sproat and Navdeep Jaitly, “An RNN model of text\nnormalization,” in INTERSPEECH, 2017.\n[17] A. K ¨ohn, F. Stegen, and T. Baumann, “Mining the spo-\nken Wikipedia for speech data and beyond,” in LREC,\n2016.\n[18] C. Raffel, N. M. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y . Zhou, W. Li, and P. J. Liu, “Exploring\nthe limits of transfer learning with a uniﬁed text-to-text\ntransformer,” ArXiv:1910.10683, 2020.\n[19] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT:\nPre-training of deep bidirectional transformers for lan-\nguage understanding,” in NAACL-HLT, 2019.\n[20] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen,\nO. Levy, M. Lewis, L. Zettlemoyer, and V . Stoyanov,\n“RoBERTa: A robustly optimized BERT pretraining ap-\nproach,” ArXiv:1907.11692, 2019.\n[21] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin,\n“Attention is all you need,”ArXiv:1706.03762, 2017.\n[22] R. J. Williams and D. Zipser, “A learning algorithm\nfor continually running fully recurrent neural networks,”\nNeural Computation, vol. 1, pp. 270–280, 1989.\n[23] Yang Zhang, Evelina Bakhturina, Kyle Gorman, and\nBoris Ginsburg, “Nemo inverse text normalization:\nFrom development to production,” arXiv preprint\narXiv:2104.05055, 2021.\n[24] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou,\nA. Siddhant, A. Barua, and C. Raffel, “mT5: A mas-\nsively multilingual pre-trained text-to-text transformer,”\nin NAACL, 2021."
}