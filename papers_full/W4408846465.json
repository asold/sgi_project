{
  "title": "Mist: Efficient Distributed Training of Large Language Models via Memory-Parallelism Co-Optimization",
  "url": "https://openalex.org/W4408846465",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2615993823",
      "name": "Zhu, Zhanda",
      "affiliations": [
        "Vector Institute",
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A4222150914",
      "name": "Giannoula, Christina",
      "affiliations": [
        "University of Toronto",
        "Vector Institute"
      ]
    },
    {
      "id": "https://openalex.org/A4307008080",
      "name": "Andoorveedu, Muralidhar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288528005",
      "name": "Su, Qidong",
      "affiliations": [
        "University of Toronto",
        "Vector Institute"
      ]
    },
    {
      "id": "https://openalex.org/A3176074116",
      "name": "Mangalam, Karttikeya",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2305034455",
      "name": "Zheng Bojian",
      "affiliations": [
        "University of Toronto",
        "Vector Institute"
      ]
    },
    {
      "id": "https://openalex.org/A3190148322",
      "name": "Pekhimenko, Gennady",
      "affiliations": [
        "University of Toronto",
        "Vector Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4307007621",
    "https://openalex.org/W4394998532",
    "https://openalex.org/W4220741164",
    "https://openalex.org/W3206636350",
    "https://openalex.org/W4285225959",
    "https://openalex.org/W1710734607",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W4318541647",
    "https://openalex.org/W2008875774",
    "https://openalex.org/W3132107458",
    "https://openalex.org/W4318541673",
    "https://openalex.org/W2489773810",
    "https://openalex.org/W2122691518",
    "https://openalex.org/W4394998892",
    "https://openalex.org/W2979044977",
    "https://openalex.org/W4327694855",
    "https://openalex.org/W4320523450",
    "https://openalex.org/W4394922822",
    "https://openalex.org/W4385585365",
    "https://openalex.org/W2612076670",
    "https://openalex.org/W4310282800",
    "https://openalex.org/W2969388332",
    "https://openalex.org/W3204998121",
    "https://openalex.org/W3012514909",
    "https://openalex.org/W3129831491",
    "https://openalex.org/W3205803342",
    "https://openalex.org/W3081168214",
    "https://openalex.org/W2489529491",
    "https://openalex.org/W3157919170",
    "https://openalex.org/W4388979610",
    "https://openalex.org/W4395117922",
    "https://openalex.org/W4301361180",
    "https://openalex.org/W2883830791",
    "https://openalex.org/W4386768656",
    "https://openalex.org/W3043619075",
    "https://openalex.org/W3145595827",
    "https://openalex.org/W4206623734",
    "https://openalex.org/W398859631",
    "https://openalex.org/W3037639655",
    "https://openalex.org/W4229024507",
    "https://openalex.org/W4205983429",
    "https://openalex.org/W2734941459",
    "https://openalex.org/W2914209329",
    "https://openalex.org/W4394907705",
    "https://openalex.org/W3101104221"
  ],
  "abstract": "Various parallelism, such as data, tensor, and pipeline parallelism, along with memory optimizations like activation checkpointing, redundancy elimination, and offloading, have been proposed to accelerate distributed training for Large Language Models. To find the best combination of these techniques, automatic distributed training systems are proposed. However, existing systems only tune a subset of optimizations, due to the lack of overlap awareness, inability to navigate the vast search space, and ignoring the inter-microbatch imbalance, leading to sub-optimal performance. To address these shortcomings, we propose Mist, a memory, overlap, and imbalance-aware automatic distributed training system that comprehensively co-optimizes all memory footprint reduction techniques alongside parallelism. Mist is based on three key ideas: (1) fine-grained overlap-centric scheduling, orchestrating optimizations in an overlapped manner, (2) symbolic-based performance analysis that predicts runtime and memory usage using symbolic expressions for fast tuning, and (3) imbalance-aware hierarchical tuning, decoupling the process into an inter-stage imbalance and overlap aware Mixed Integer Linear Programming problem and an intra-stage Dual-Objective Constrained Optimization problem, and connecting them through Pareto frontier sampling. Our evaluation results show that Mist achieves an average of 1.28$\\times$ (up to 1.73$\\times$) and 1.27$\\times$ (up to 2.04$\\times$) speedup compared to state-of-the-art manual system Megatron-LM and state-of-the-art automatic system Aceso, respectively.",
  "full_text": "Mist: Efficient Distributed Training of Large Language\nModels via Memory-Parallelism Co-Optimization\nZhanda Zhu\nUniversity of Toronto, Vector\nInstitute, CentML\nzhanda.zhu@mail.utoronto.ca\nChristina Giannoula\nUniversity of Toronto, Vector\nInstitute, CentML\nchristina.giann@gmail.com\nMuralidhar Andoorveedu\nCentML\nmurali@centml.ai\nQidong Su\nUniversity of Toronto, Vector\nInstitute, CentML\nqdsu@cs.toronto.edu\nKarttikeya Mangalam\nSigIQ.ai\nmangalam@sigiq.ai\nBojian Zheng\nUniversity of Toronto, Vector\nInstitute, CentML\nbojian@cs.toronto.edu\nGennady Pekhimenko\nUniversity of Toronto, Vector\nInstitute, CentML\npekhimenko@cs.toronto.edu\nAbstract\nVarious parallelism, such as data, tensor, and pipeline par-\nallelism, along with memory optimizations like activation\ncheckpointing, redundancy elimination, and offloading, have\nbeen proposed to accelerate distributed training for Large\nLanguage Models. To find the best combination of these tech-\nniques, automatic distributed training systems are proposed.\nHowever, existing systems only tune a subset of optimiza-\ntions, due to the lack of overlap awareness, inability to navi-\ngate the vast search space, and ignoring the inter-microbatch\nimbalance, leading to sub-optimal performance. To address\nthese shortcomings, we propose Mist, a memory, overlap,\nand imbalance-aware automatic distributed training system\nthat comprehensively co-optimizes all memory footprint\nreduction techniques alongside parallelism. Mist is based\non three key ideas: (1) fine-grained overlap-centric schedul-\ning, orchestrating optimizations in an overlapped manner,\n(2) symbolic-based performance analysis that predicts run-\ntime and memory usage using symbolic expressions for fast\ntuning, and (3) imbalance-aware hierarchical tuning, decou-\npling the process into an inter-stage imbalance and overlap\naware Mixed Integer Linear Programming problem and an\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for profit or commercial advantage and that\ncopies bear this notice and the full citation on the first page. Copyrights\nfor components of this work owned by others than the author(s) must\nbe honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific\npermission and/or a fee. Request permissions from permissions@acm.org.\nEuroSys â€™25, March 30-April 3, 2025, Rotterdam, Netherlands\nÂ© 2025 Copyright held by the owner/author(s). Publication rights licensed\nto ACM.\nACM ISBN 979-8-4007-1196-1/25/03\nhttps://doi.org/10.1145/3689031.3717461\nintra-stage Dual-Objective Constrained Optimization prob-\nlem, and connecting them through Pareto frontier sampling.\nOur evaluation results show that Mist achieves an average\nof 1.28Ã—(up to 1.73Ã—) and 1.27Ã—(up to 2.04Ã—) speedup com-\npared to state-of-the-art manual system Megatron-LM and\nstate-of-the-art automatic system Aceso, respectively.\nCCS Concepts:â€¢ Computing methodologies â†’Distributed\ncomputing methodologies; Machine learning.\nKeywords: LLM, Systems for Machine Learning, Distributed\ntraining\nACM Reference Format:\nZhanda Zhu, Christina Giannoula, Muralidhar Andoorveedu, Qi-\ndong Su, Karttikeya Mangalam, Bojian Zheng, and Gennady Pekhi-\nmenko. 2025. Mist: Efficient Distributed Training of Large Language\nModels via Memory-Parallelism Co-Optimization. In Twentieth Eu-\nropean Conference on Computer Systems (EuroSys â€™25), March 30-\nApril 3, 2025, Rotterdam, Netherlands. ACM, New York, NY, USA,\n19 pages. https://doi.org/10.1145/3689031.3717461\n1 Introduction\nLarge Language Models (LLMs) have gained high interest and\nshow remarkable capabilities in various fields like question\nanswering, summarization, problem solving, and more [5, 23,\n60]. However, their significantly increased sizes and dataset\nrequirements have escalated computational and memory de-\nmands. For instance, training LLaMa-3.1-405B [ 50] uses a\ncumulative 30.84M GPU hours of computation on NVIDIA\nH100 GPUs [51]. While most companies and researchers can-\nnot afford to pre-train such LLMs, continuous pre-training\nor supervised fine-tuning still costs over 2000 NVIDIA H100\nGPU hours per 1B tokens [51]. Therefore, efficient distributed\ntraining techniques have been proposed [6, 12, 40â€“42, 45, 46,\n49, 53, 68, 73, 76, 77, 80, 82, 85, 87, 89] to improve system\nperformance during training, since even minor reductions in\narXiv:2503.19050v1  [cs.DC]  24 Mar 2025\nEuroSys â€™25, March 30-April 3, 2025, Rotterdam, Netherlands Z. Zhu, C. Giannoula, M. Andoorveedu, Q. Su, K. Mangalam, B. Zhang, G. Pekhimenko\nthe training time are translated to significant financial and\nenvironmental benefits [22, 52, 74].\nPrior works [ 35, 49, 55, 57, 68, 73, 89] propose various\nparallelization techniques for distributed training. Data Par-\nallelism (DP) [1, 42] splits input data across devices, with\neach device processing a portion of the data, while main-\ntaining a full copy of the LLM model. Tensor Parallelism\n(TP) [39, 57, 73] partitions the parameters of each layer across\ndevices, however introducing inter-device communication\nover activations to ensure computation correctness. Pipeline\nParallelism (PP) [32, 35, 43, 55] divides the model into stages,\nhowever a large number of stages may lead to performance\ninefficiencies, e.g., pipeline bubbles where devices are idle\nduring training. We henceforth refer to the number of par-\ntitions in DP, TP and PP methods as DP size, TP size, and\nPP size, respectively. Gradient accumulation techniques are\nusually applied, dividing a global batch into multiple micro-\nbatches, to reduce the memory pressure of each microbatch\nand facilitate pipeline parallelism [ 73]. To alleviate mem-\nory pressure in devices, memory footprint reduction tech-\nniques [3, 17, 26, 31, 36, 38, 63, 66, 67, 69, 70, 87, 88] have also\nbeen proposed. Activation checkpointing (CKPT) [17, 36, 38,\n88] reduces memory footprint during training by recomput-\ning activations during backpropagation. ZeRO [66, 87] elimi-\nnates model states redundancy by partitioning the optimizer\nstates, gradients, or weights across devices. Higher ZeRO\nlevels partition more model states, thereby providing larger\nmemory footprint savings, however increasing the inter-\ndevice communication. Offloading [ 26, 31, 63, 67, 69, 70]\ntemporarily transfers unused tensors from the GPU to the\nCPU, freeing GPU memory however increasing the data\ntransfer costs. These memory optimizations often require\noverlapping data transfers with GPU computation to reduce\nperformance overhead [63, 69, 87].\nIn this work, we observe that memory footprint reduction\ntechniques, although they have been primarily designed to\nalleviate memory pressure, they can significantly enhance\nperformance, since they assist in balancing trade-offs be-\ntween runtime overhead and memory footprint reduction.\nFor instance, applying offloading optimization can free up\nsome memory in GPU devices, which can then be leveraged\nto reduce the TP or PP size, thereby reducing communication\noverheads or pipeline bubbles during training. Generally, ex-\nploiting memory footprint reduction techniques to release\nsome memory footprint in GPU devices can be leveraged\nto: 1) reduce the TP size, thus mitigating communication\noverheads; 2) reduce the PP size, thus eliminating pipeline\nbubbles; and (3) increase the batch size, improving kernel effi-\nciency. Conversely, applying less aggressively memory foot-\nprint reduction optimizations results in higher GPU memory\nusage, which increases the partitioning across devices, thus\nincurring higher performance overheads related to paral-\nlelism. Therefore, additional GPU memory can be gained\nby applying more aggressive memory footprint reduction\ntechniques, which come with some added overhead. This\nmemory can then be used to reduce the overhead of other\noptimizations. As long as the benefit from reducing the over-\nhead outweighs the additional cost incurred by the memory\nfootprint reduction techniques, overall training efficiency\nimproves. See detailed motivational examples in Section 3.1.\nOverall, distributed training constitutes an optimization\nproblem that can be formulated as choosing the best combi-\nnation of all available techniques (both parallelism and mem-\nory footprint reduction techniques) to maximize training effi-\nciency, while keeping the memory usage lower than the avail-\nable hardware memory capacity. Manual distributed training\nmethods such as Megatron-LM [ 73] and DeepSpeed [ 68],\namong others [67, 69, 87], are developed to provide some of\nthe above optimizations. However, these manual methods re-\nquire users to specify configurations, i.e., the combination of\nparallelism and memory footprint reduction techniques, for\noptimal performance. This can be quite challenging even for\nexperienced users and takes lots of engineering efforts [89].\nMoreover, as model sizes and device counts increase, tun-\ning complexity increases exponentially [89]. To address this\nissue, automatic distributed training systems have been pro-\nposed [12, 40, 45, 46, 49, 53, 76, 77, 80, 82, 85, 89]. Given LLM\nmodel and GPU hardware, they construct the search space of\nvarious configurations of the their supported optimizations\nand automatically find optimal combination of them.\nWe extensively examine distributed training frameworks\nand identify a key shortcoming: they fail to comprehensively\nco-optimize memory footprint reduction techniques along-\nside parallelism, since they only focus on a subset of the\navailable search space. Specifically, these systems are still in-\nefficient in optimizing training performance, because they (i)\neither tune parallelism configuration with a fixed pre-defined\nmemory optimization [49, 53, 89], (ii) support only one spe-\ncific optimization like activation checkpointing [40, 46, 76],\nor (iii) make strong assumptions, such as applying the same\nmemory footprint strategy across all pipeline stages [ 85].\nThese constraints lead to a reduced search space and sub-\noptimal performance, as demonstrated in Section 3.1.\nWe analyze how prior works tune the training configu-\nrations and find that co-optimizing all available memory\nfootprint reduction techniques and parallelism is challeng-\ning in these prior existing systems, because they suffer from\nthree limitations. First, existing automatic systems do not\noverlap communication with computation beyond the basic\ngradient all-reduce, thus missing important opportunities\nfor training efficiency. This can cause severe performance\ndegradation (See Figure 12), which becomes even more se-\nvere when all memory optimizations are involved. Second,\nthey are not able to efficiently explore the exploded search\nspace when co-tuning all optimizations. When more memory\noptimization techniques are incorporated, the search space\nincreases significantly, and existing systems fail to find the\nbest configuration in such a huge search space. Third, they\nMist: Efficient Distributed Training of Large Language Models via Memory-Parallelism ... EuroSys â€™25, March 30-April 3, 2025, Rotterdam, Netherlands\nuse the averaged microbatch time to model the pipeline paral-\nlelism performance, implicitly assuming uniform microbatch\nexecution time within a pipeline stage. However, we find\nthat this is not the case, since first and last microbatches\nincur higher communication costs (especially when ZeRO\nand offloading are involved), as we explain in Section 3.2.\nTo tackle the aforementioned limitations, we propose\nMist, a memory, overlap, and imbalance-aware automatic\ndistributed training system that co-optimizes memory foot-\nprint reduction techniques with parallelism. Mist consists of\nthree key ideas: (1) fine-grained overlap-centric schedul-\ning, which carefully orchestrates the implementation of both\nmemory footprint reduction techniques and parallelism to\nmaximize the computation-communication overlapping; (2)\nsymbolic-based efficient performance analysis , which\nenables fast exploration of the exploded search space of var-\nious configurations by efficiently predicting runtime and\nmemory usage via symbolic expressions and batched value\nsubstitutions; and (3) imbalance-aware hierarchical tun-\ning, which takes into account the microbatch variability and\noverlap opportunities in pipeline parallelism, decouples the\noptimization process into an inter-stage Mixed Integer Lin-\near Programming (MILP) problem and an intra-stage Dual-\nObjective Constrained Optimization problem, and connects\nthem via Pareto frontier sampling. This third key idea ad-\ndresses both the search space explosion and the microbatch\nimbalance in pipeline parallelism.\nWe extensively evaluate Mist on a wide variety of LLMs,\nincluding GPT-3 [9], LlaMa [79], and Falcon [2], across di-\nverse training configurations, i.e., varying the global batch\nsize, model size, and using different kernel implementations\nsuch as FlashAttention [ 19]) and hardware setups (up to\n32 NVIDIA L4 [ 59] and 32 NVIDIA A100 GPUs [ 58]), and\ndemonstrate that Mist significantly outperforms prior state-\nof-the-art works [46, 68, 73]. Our evaluation results show\nthat Mist achieves an average of 1.28 Ã—(up to 1.73 Ã—) and\n1.27Ã—(up to 2.04Ã—) speedup compared to the state-of-the-art\nmanual implementation Megatron-LM and the state-of-the-\nart automatic system Aceso, respectively, across different\nGPU, model, and training configurations.\nTo sum up, we make the following contributions:\nâ€¢We identify the need of comprehensively co-optimizing\nmemory footprint reduction techniques alongside paral-\nlelism and propose Mist, a highly efficient easy-to-use\nautomatic distributed training framework for LLMs.\nâ€¢We propose and implement a symbolic analysis system\nthat generates symbolic expressions for workload charac-\nteristics to quickly explore the exploded search space. We\ndesign a scheduling method that maximizes computation-\ncommunication overlap by carefully coordinating memory\noptimization and parallelism techniques. We introduce a\ntuning method that decouples the optimization process\ninto two stages and connects them through Pareto frontier\nFigure 1. An illustration of optimization configurations.\nsampling, addressing microbatch variability and leverag-\ning overlap opportunities in pipeline parallelism.\nâ€¢We evaluate Mist using various large-scale LLMs in both\nNVLink systems (NVIDIA A100 GPUs [58]) and PCIe sys-\ntems (NVIDIA L4 GPUs [ 59]) and compare it to multi-\nple strong baselines. Mist significantly outperforms prior\nworks, up to 2.04Ã—, under various training configurations.\n2 Background\nLLMs [9, 78] require excessive computation and memory,\nleading to significant costs and energy consumption [ 51].\nConsequently, distributed training, i.e., scaling hardware and\nsplitting the model and/or the input data, is the typical solu-\ntion to train LLMs [68, 73]. To efficiently conduct distributed\ntraining, various parallelism techniques [35, 42, 73] and GPU\nmemory footprint reduction methods [17, 26, 63, 66] have\nbeen developed. As shown in Figure 1, different parallelism\nand memory optimizations can be applied in combination.\n2.1 Parallelism in Distributed Training\nData Parallelism. To scale training,Data Parallelism (DP) [1,\n42] distributes input data across GPUs, with each GPU pro-\ncessing its data independently using a model replica. It in-\nvolves only an all-reduce of gradients per iteration, but re-\nquires the entire model to fit within each GPUâ€™s memory.\nTensor Parallelism. Tensor Parallelism (TP) [57, 73] parti-\ntions the parameters in each layer and conducts all-reduce\nover activations in the forward pass and gradients in the\nbackward pass to maintain computation correctness.\nPipeline Parallelism. Pipeline Parallelism (PP) [25, 35, 37,\n43, 55, 56] partitions the model into stages, using p2p com-\nmunication between stages to pass data through the pipeline.\nAlthough it only involves small communication overhead\nto transfer intermediate tensors, the dependency between\nstages introduces pipeline bubbles, which causes efficiency\nto suffer as a result of the device idle time.\n2.2 GPU Memory Footprint Reduction Techniques\nActivation Checkpointing. Activation checkpointing (CKPT)\n(also known as recomputation) [3, 17, 36, 38, 88] discards cer-\ntain activations in the forward pass, while stashing others.\nLater in the backward pass, the discarded activations are\nrecomputed from the stashed activations, and are then used\nfor gradient computation. This method reduces the memory\nEuroSys â€™25, March 30-April 3, 2025, Rotterdam, Netherlands Z. Zhu, C. Giannoula, M. Andoorveedu, Q. Su, K. Mangalam, B. Zhang, G. Pekhimenko\nTable 1. Comparison of distributed training systems. P, G,\nO, and A under offloading denote parameter, gradient, op-\ntimizer states, activation offloading, respectively. Circle for\noptimizations represents functionality support and granu-\nlarity. Circle for tuning represents whether the system can\ntune all optimizations it supports.\nParallelism Offloading ZeRO-\n2/3\nAuto-Tuning\nCapability DP TP PP P G O A\nMegatron-LM [73] âœ“ âœ“ âœ“ # # # # âœ— #\nDeepSpeed [68] âœ“ âœ“ âœ“ # # # #âœ“ #\nZeRO-Offload [69] âœ“ âœ“ âœ— # # G # #âœ“ #\nZeRO-Infinity [67] âœ“ âœ“ âœ— G # G # G # G #âœ“ #\nAlpa [89] âœ“ âœ“ âœ“ # # # #âœ“ G #\nSlapo [12] âœ“ âœ“ âœ“ # # # #âœ“ G #\nAdaPipe [76] âœ“ âœ“ âœ“ # # # # âœ— G #\nYuan et al. [85] âœ“ âœ“ âœ“ # # # G #âœ— G #\nAceso [46] âœ“ âœ“ âœ“ # # # # âœ—  \nMist âœ“ âœ“ âœ“     âœ“  \nneeded for the saved activations, at the cost of recomputing\ndiscarded activations in the backward pass.\nRedundancy Optimization. Zero Redundancy Optimizer\n(ZeRO) reduces the memory usage by eliminating redundant\ncopies of optimizer states, gradients, and model weights\nacross data-parallel devices [66, 87]. ZeRO operates in three\nmodes: ZeRO-1 (shards optimizer only), ZeRO-2 (shards op-\ntimizer and gradients), and ZeRO-3 (shards optimizer, gradi-\nents, and weights). While ZeRO-1 introduces no additional\ncommunication, ZeRO-2/3 incur communication overhead\ndue to all-gathering and reduce-scattering operations.\nOffloading. Offloading [26, 31, 63, 67, 70, 81] (also known as\nswapping) involves transferring model states or activations\nbetween the GPU devices and the host CPU. This technique\nhelps manage GPU memory constraints by temporarily of-\nfloading data, allowing the GPU to accommodate other tasks.\nThe efficiency of swapping significantly depends on overlap-\nping, which allows memory transfers to be hidden outside\nthe critical path.\n3 Limitation of Existing Systems\n3.1 The Need for Comprehensive Co-Optimization\nDistributed training optimization problem involves finding\nthe optimal configuration of parallelism strategies and mem-\nory reduction methods to maximize performance, given\nspecific hardware, model, and global batch size. Current\ndistributed training systems, however, lack the ability to\ncomprehensively co-optimize memory footprint optimiza-\ntions alongside parallelism [ 46, 85, 89]. As detailed in Ta-\nble 1, manual methods, such as ZeRO-Infinity [67], support\na broader range of memory optimizations but only allow\ncoarse-grained configuration (enabling or disabling offload-\ning) and lack automatic tuning. Automatic methods, such as\nAlpa [89], AdaPipe [76], and Aceso [12], either support fewer\n1 3 1 5 3 7 5\n1 3 1 5 3 7 5 7\n1, 2 1, 2\n1, 2 1, 2\n#CKPT Reduction: GPU 1/2: 16 -> 8; GPU 3/4: 8 -> 0\nExtra Optimizer Offloading\nFull \nCKPT\nGPU1\nTuned \nCKPT\nTuned \nZeRO\nTuned \nOffloading\nNo \nCKPT OOM\nGPU2\n11\n22\n4 2 6 4 8 6 82\nGPU3\nGPU4\n11 3 3 5 5 7 7\n22 4 4 6 6 8 8\n2 4 2 6 4 8 6 8\n3, 4 3, 4\n5, 6 5, 6\n7, 8 7, 8\n3, 4 3, 4\n5, 6 5, 6\n7, 8 7, 8\nGPU1\nGPU2\nGPU3\nGPU4\nGPU1\nGPU2\nGPU3\nGPU4\nGPU1\nGPU2\nGPU3\nGPU4\ni FWD with \nAll-gathering\ni Data\nIndex\ni FWD i BWD with \nReduce-Scattering\ni BWD\n33\n44\n55\n66\n77\n88\n7\nReduce PP size, increase micro batch size\n1, 2 1, 2\n#CKPT Reduction: 32 -> 28\nAll \nTuned\n3, 4 3, 4\n5. 6 5, 6\n7, 8 7, 8\nGPU1\nGPU2\nGPU3\nGPU4\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nSpeedup\n1.16Ã— \nSpeedup\n1.22Ã— \nSpeedup\n1.25Ã— \nSpeedup\n1.30Ã— \nFigure 2. Motivational examples of tuning parallelism with\nmemory optimizations for GPT-3-2.7B on 4 NVIDIA L4 GPUs\nwith ğ‘†ğ‘’ğ‘ = 4096,ğµğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ = 8. Parallelism is always tuned.\noptimizations or can only tune a subset of the optimizations\nthey provide. This limited search space leads to sub-optimal\nconfigurations and reduced performance.\nTo illustrate these trade-offs of different optimizations and\nthe impact of co-optimization, we present a motivational\nexample of training GPT-3-2.7B on four NVIDIA L4 GPUs\nwith a global batch size of 8. We manually enumerate all\nğ·ğ‘ƒ, ğ‘‡ğ‘ƒ, ğ‘ƒğ‘ƒ, and micro batch sizeğ‘configurations, and when\nco-tuning memory optimizations, we also enumerate their\ncombinations with the parallelism configurations.\nAs shown in Figure 2(a), without memory optimization, all\nparallelism plans result in out-of-memory (OOM) errors. In\nFigure 2(b), applying full CKPT (all layers being recomputed,\nas in Megatron-LM [73] and Alpa [89]) reduces memory us-\nage by recomputing activations, avoiding OOM. The best\nparallelism strategy found isğ·ğ‘ƒ=2, ğ‘ƒğ‘ƒ=2, ğ‘=1. In Figure 2(c),\nif activation checkpointing is tuned (as in Aceso [ 46] and\nAdapipe [76]), the number of recomputed layers is reduced\nfrom 16 to 8 on the first two GPUs, and from 16 to 0 on the\nother two, reducing recomputation. During tuning, although\nanother strategy (ğ·ğ‘ƒ=1, ğ‘ƒğ‘ƒ=4, ğ‘=1) fully eliminates recom-\nputation by using the extra memory from the increased PP\nsize, the added pipeline bubbles outweigh the benefits of re-\nduced recomputation, causing it to under-perform compared\nto ğ‘ƒğ‘ƒ=2. In Figure 2(d), tuning ZeRO (as in DeepSpeed [68])\nenables ğ·ğ‘ƒ=4, ğ‘ƒğ‘ƒ=1, ğ‘=2 with ZeRO-2, preventing OOM by\nMist: Efficient Distributed Training of Large Language Models via Memory-Parallelism ... EuroSys â€™25, March 30-April 3, 2025, Rotterdam, Netherlands\nS1 S2 S3 S4 S5 S6 S7 S8\n0\n50\n100\n150\n200\n250\n300\n350Latency per Sample (ms)\nPipeline Bottleneck\nPreLayer\nNon-CKPT\nCKPT\nPostLayer\n(a) Runtime of tuning\nparallelism with CKPT\nS1 S2 S3 S4\n0\n2\n4\n6\n8\n10Number of Layers\n#Layers\n#CKPT\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nOffloading Ratio\nOO\nAO\n(b) Configurations of\ncomprehensive tuning\nS1 S2 S3 S4\n0\n50\n100\n150\n200\n250\n300\n350Latency per Sample (ms)\nPipeline Bottleneck\nMem Opt Overhead\n(c) Runtime of\ncomprehensive tuning\nFigure 3. Motivational example of showing the speedup\nsource of comprehensive co-optimization for GPT-3-7B on 8\nNVIDIA L4 GPUs with ğ‘†ğ‘’ğ‘ = 2048, ğµğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ = 512.\nsharding gradients. Similarly, in Figure 2(e), tuning offload-\ning enables the same parallelism with an optimizer offloading\nratio of 0.325, avoiding OOM. In both cases, reduced pipeline\nbubbles and improved kernel efficiency (from the increased\nbatch size) outweigh the memory optimization overhead,\nincreasing training efficiency. These examples show that tun-\ning each memory optimization with parallelism improves\ntraining performance, achieving speedups of 1.22Ã—, 1.25Ã—\nand 1.16Ã—for CKPT, ZeRO, and offloading tuning, respective,\ncompared to the full CKPT strategy.\nBuilding upon these findings, we co-optimize all memory\noptimizations with parallelism and identify an even better\nstrategy: ğ·ğ‘ƒ=4, ğ‘ƒğ‘ƒ=1, ğ‘=2 with ZeRO-2 and adjusted activa-\ntion checkpointing (recomputed layers reduced from 32 to\n28), which reduces pipeline bubbles (compared to activation\ncheckpointing tuning only) and recomputation (compared to\nZeRO tuning only), leading to a 1.30Ã—speedup while main-\ntaining memory savings.\nTo further demonstrate the benefits of comprehensive co-\noptimization, we consider an example of training GPT-3-7B\non eight NVIDIA L4 GPUs with a global batch size of 512.\nWhen only activation checkpointing is tuned, the best paral-\nlelism strategy identified is ğ·ğ‘ƒ=1, ğ‘ƒğ‘ƒ=8, ğ‘=1, which causes\nsevere pipeline imbalance and hardware idling, as shown in\nFigure 3(a). However, by comprehensively co-optimizing all\ntechniques, we find a better strategy: ğ·ğ‘ƒ=2, ğ‘ƒğ‘ƒ=4, ğ‘=2, with\nadjusted activation checkpointing and optimized offloading\nratios, detailed in Figure 3(b), where ğ‘‚ğ‘‚ and ğ´ğ‘‚stand for\noptimizer and activation offloading, respectively. This con-\nfiguration uses offloading to gain GPU memory, which is\nthen used to reduce PP size from 8 to 4 and eliminate re-\ncomputation for the last two stages. As shown in Figure 3(c),\nco-optimization reduces pipeline stages and device idle time,\nimproving overall performance despite some offloading over-\nhead, as the optimizer offloading overhead is amortized over\nmultiple micro-batches and activation offloading can overlap\nwith computation. Comprehensive co-optimization yields\na 1.22Ã—speedup over tuning only parallelism and a 1.11Ã—\naâ€™ a b\naâ€™ a\nc d e f g h\nb c d e f g h\naâ€™ a b c d e f g h\naâ€™ a b c d e f g h\naâ€™\naâ€™\naâ€™\naâ€™\nStage 1\nStage 2\nStage 3\nStage 4\nğ‘¡\n4\nğº âˆ’ 1 â‹… ğ‘¡\n3\nğ‘¡\n3\nğ‘¡\n2\nğ‘¡\n1\nğ‘‘\n2\nâˆ’ ğ‘¡\n1\nâˆ’ ğ‘¡\n2\nFigure 4. Illustration of pipeline parallelism overlap oppor-\ntunity and inter-microbatch imbalance. aâ€²is the extra com-\nmunication happened in the first microbatch.\nspeedup over tuning parallelism with activation checkpoint-\ning, demonstrating significant performance gains.\nHowever, existing systems lack support for comprehensive\nco-optimization. For instance, Aceso [46] does not support\nZeRO or offloading, Slapo [12] only tunes activation check-\npointing within a fixed parallelism plan, and AdaPipe [76]\nfocuses solely on pipeline parallelism and activation check-\npointing. It limits their ability to fully leverage the trade-offs\nbetween memory reduction and runtime overhead, leading\nto suboptimal performance.\n3.2 Why Existing Auto Systems Fail to Co-Optimize?\nWe summarize the key shortcomings of existing systems in\nachieving comprehensive co-optimization:\nShortcoming #1: Lack of overlap awareness. Existing\nautomatic distributed training methods fail to account for\ncomputation-communication overlap beyond basic gradi-\nent synchronization overlap. This results in significant per-\nformance degradation, as seen in our experiments where\nAceso underperforms manual implementation Megatron-LM\n(with overlap) in 6 out of 10 cases despite a larger search\nspace (See Figure 12). Moreover, techniques like ZeRO and\noffloading add extra communication overheads, requiring\noverlap with computation or pipeline bubbles to maintain\nefficiency [66, 67]. In Figure 3(b), Stage 2 shows a 13% over-\nhead if activation offloading is not overlapped, and for Stage\n3, offloading optimizer states for a 7B model with a ğ‘ƒğ‘ƒ = 4\ntakes 7 seconds, resulting in a 40% overhead with a batch\nsize of 64. Additionally, computation-communication inter-\nference results in inaccurate performance predictions. For\ninstance, we observe a 7.7% performance degradation for the\nlinear layer in attention module of the motivational example\nwhen it is executed concurrently with all-reduce operations,\nwhich becomes worse when CPU-GPU communication is\nalso involved. Ignoring overlap leads to mis-estimating opti-\nmization configurations and results in sub-optimal strategies.\nShortcoming #2: Unable to navigate the exploded search\nspace. Co-optimizing memory footprint reduction tech-\nniques with parallelism significantly expands the search\nspace, making it difficult to efficiently find the best com-\nbination. For example, Alpa takes over 40 hours to find the\nbest parallelization strategy for GPT-3-39B on 64 GPUs [89].\nAs depicted in Figure 5, simultaneously tuning parallelism\nand memory optimizations further dramatically increases\nthe search space and complexity. Even after applying search\nspace pruning methods, such as inter and intra-stage tuning\nEuroSys â€™25, March 30-April 3, 2025, Rotterdam, Netherlands Z. Zhu, C. Giannoula, M. Andoorveedu, Q. Su, K. Mangalam, B. Zhang, G. Pekhimenko\n16 32 48 64 80\n#Layers\n100\n1025\n1050\n1075\n10100\n10125\n10150\n#Configs (log scale)\nDP+TP\n+PP (cont.)\n+ZeRO (cont.)\n+CKPT (cont.)\n+OO (cont.)\n+GO (cont.)\n+PO (cont.)\n+AO (cont.)\nFigure 5. Growth in the number of configurations within the\nsearch space as each optimization is incrementally added.\ndecoupling, the search space remains significantly larger\nthan what existing performance predictors can efficiently\nhandle [21, 34, 48, 71, 84]. For example, Proteus, a fast simulation-\nbased tool that supports the prediction of performance in\nparallelization and recomputation, requires around 6 sec-\nonds to simulate one optimization configuration for GPT-2\non 32 GPUs [21]. Despite its speed, this kind of tool is still\nimpractical for effectively exploring the vast search space\npresented by our problem.\nShortcoming #3: Inaccurate performance prediction\ndue to the lack of inter-microbatch imbalance aware-\nness. Existing automatic distributed training systems often\nsuffer from inaccurate performance predictions when new\nmemory optimizations are involved due to their inability to\naccount for inter-microbatch variability. Automatic paral-\nlelism planners [46, 89] typically assume uniform microbatch\nexecution times within a pipeline stage. However, the first\nand last microbatches take longer due to extra operations like\nparameter all-gathering, gradient reduce-scattering, and op-\ntimizer offloading, as shown in Figure 4. In the motivational\nexample of tuning GPT-3-7B, simply averaging microbatch\ntimes leads to performance prediction error of up to 21.55%,\ndepending on the number of microbatches, which may cause\nup to a twofold performance slowdown. Our evaluation, as\nshown in Figure 13, ignoring inter-microbatch imbalance\nleads to about a 9% slowdown compared to optimal solutions.\nThese inaccuracies undermine the effectiveness of tuning\nprocess, leading to the selection of sub-optimal strategies.\n3.3 Why Simple Heuristics Can Not Address it?\nWhy manual frameworks with simple heuristics cannot ad-\ndress co-optimization? Manual frameworks struggle to de-\nliver optimal performance in comprehensive co-optimization\nscenarios, because the best performance can only be achieved\nwith fine-grained configuration for each stage, including\nlayer assignments, DP and TP sizes, recomputed layers, and\noffloading ratios for each type of model states. The vast and\nexponentially large search space makes manual exploration\nimpractical, leading users to rely on simple heuristics. For\nexample, a recent study [ 85] uses a heuristic that applies\nuniform checkpointed layers and activation offloading ra-\ntios across all pipeline stages to reduce tuning search space.\nHowever, pipeline parallelism inherently exhibits memory\nand computation imbalances, making uniform strategies sub-\noptimal. As shown in Figure 3(b), heterogeneous optimiza-\ntions per stage are selected. In our motivational examples,\nuniform heuristics results in 26% and 20% performance degra-\ndation for the 2.7B and 7B models, respectively, compared\nto the optimal strategies achieved through comprehensive\nco-optimization. As workloads scale, the complexity of tun-\ning grows, making fully automated systems increasingly\nessential to efficiently handle the expanded search space and\ndeliver optimal performance [46, 76, 89].\nOur Goal is to develop a fully automated distributed train-\ning optimization system for LLMs that addresses all the short-\ncomings above and comprehensively co-optimize memory foot-\nprint reduction techniques alongside parallelism.\n4 Mist: Overview and Key Ideas\nTo accelerate distributed training, we introduce Mist, a mem-\nory, overlap, and imbalance aware automatic distributed\ntraining system that comprehensively co-optimizes mem-\nory footprint reduction techniques with parallelism. Overall,\nMist proposes three key ideas:\n1. Fine-Grained Overlap-Centric Scheduling and In-\nterference Modeling. Mist proposes an overlap-centric\nscheduling approach that carefully orchestrates parallelism\nand memory optimizations to maximize the overlap of com-\nputation and communication. By optimizing the order and\ngranularity of these techniques, Mist mitigates memory op-\ntimization overhead while maintaining manageable tuning\ncomplexity. Additionally, data-driven interference model-\ning accurately predicts performance when computation and\ncommunication kernels run concurrently. This approach ad-\ndresses Shortcoming #1, the lack of overlap awareness.\n2. Symbolic-Based Efficient Performance Prediction.\nBuilding upon the scheduling strategy, Mist introduces a\nsymbolic analysis system to significantly enhance the per-\nformance prediction efficiency. Unlike traditional methods\nthat require repeated simulations for each optimization con-\nfiguration [21, 46, 89], Mist symbolizes the model and opti-\nmizations, requiring only a single simulation pass to predict\nruntime and memory usage in the form of symbolic expres-\nsions. Subsequent predictions are simplified to value substitu-\ntions in these expressions, dramatically reducing redundant\nsimulation and allowing for rapid batched evaluation of mul-\ntiple configurations. This approach effectively mitigates the\nShortcoming #2 search space explosion.\n3. Imbalance-Aware Hierarchical Tuning via Pareto\nFrontier Sampling. Finally, Mist proposes an imbalance-\naware hierarchical auto-tuner that decouples tuning into\nintra-pipeline-stage and inter-pipeline-stage processes, while\nstill considering the microbatch imbalances and overlap op-\nportunities in PP. To find the best intra-stage configurations,\nour intra-stage tuning uses a dual-objective approach to\nbalance time between imbalanced and stable microbatches.\nMist: Efficient Distributed Training of Large Language Models via Memory-Parallelism ... EuroSys â€™25, March 30-April 3, 2025, Rotterdam, Netherlands\nModel Info Inputs InfoHardware Info\nOverlap-\nCentric\n Schedule\n(Â§5.1)\nSymbolic Tracer\nMemory Analyzer Overlap- & Interference-\nAware Runtime Analyzer\nSymbolic Performance Analyzer\nImbalance-Aware Hierarchical Tuning (Â§5.3)\nIntra-Stage Tuning\n(Batched Value Subs)\nInter-Stage Tuning\n(MILP)\nExecution Engine\nAuto Pipelining Mem Buffer\nOptimization\nOverlapped\nOffloading Engine\nb, s, h, d, v = symbols(\"b s d h v\", (4, 2048, 32, 128, 50304))\nSymbolic Shape Annotation\nPeak Mem Expression Latency Func\nIntra-Stage Plan Candidates:\nDP & TP & ZeRO &Offloading   \nInter-Stage Plans:          \nPipeline Partition & CKPT.\nSymbolic Shaped Model\nGPT(n_embd=h*d, n_head=h, ...)\nSymbolic Shaped Tensors\ntorch.randint(0, v, (b, s))\nSymbolic Comp. Graph\nSymbolic-Based Efficient Performance Analysis (Â§5.2)\nFigure 6. High-level System Overview of Mist.\nInter-stage tuning formulates an MILP problem to determine\nthe best pipeline partitioning using data points sampled from\nthe Pareto frontier of intra-stage tuning. This approach pre-\nserves a pruned search space from hierarchical tuning (help-\ning to solve Shortcoming #2) while addressing the lack of\ninter-microbatch imbalance awareness (Shortcoming #3).\nSystem Overview. Figure 6 presents the high-level overview\nof Mist. The model and its input data are annotated with\nsymbolic shapes and traced to generate a symbolic shaped\ncomputational graph. This graph, on top of the Overlap-\nCentric Scheduling, is analyzed by our symbolic performance\nanalyzer to derive the peak memory expression and run-\ntime function whose inputs are optimization-related sym-\nbols. During intra-stage tuning, Mist evaluates these sym-\nbolic expressions with specific optimization values in batches.\nThis evaluation identifies a Pareto-optimal set of parallelism\nand memory optimization plans for each potential inter-\nstage candidate, utilizing our Symbolic-Based Efficient Per-\nformance Prediction strategy to efficiently navigate the ex-\ntensive search space. Subsequently, the inter-stage tuning\nphase formulates a MILP problem using stable microbatch\ntimes (ğ‘‡ğ‘ ğ‘¡ğ‘ğ‘ğ‘™ğ‘’) and their deviations ( ğ‘‡ğ‘‘ğ‘’ğ‘™ğ‘¡ğ‘) sampled from\nintra-stage tuning results. This determines the optimal pipeline\npartitioning and combination of (ğ‘‡ğ‘ ğ‘¡ğ‘ğ‘ğ‘™ğ‘’, ğ‘‡ğ‘‘ğ‘’ğ‘™ğ‘¡ğ‘), addressing\nboth inter-microbatch imbalances and inter-stage imbalances\nin pipeline parallelism. Once the optimal tuning plans are\nidentified, Mist employs an orchestrated execution engine to\nexecute them, including automatic pipeline transformation,\nTable 2. Optimization variables in the schedule template.\nName Value Type Meaning\nğº Integer Gradient accumulation steps\nğ‘† Integer Number of pipeline stages\nğ¿ğ‘– Integer Number of layers in stage ğ‘–\nğ‘ğ‘– Integer Micro batch size for stage ğ‘–\nğ·ğ‘ƒğ‘– Integer DP size for stage ğ‘–\nğ‘‡ğ‘ƒğ‘– Integer TP size for stage ğ‘–\nğ‘ğ‘’ğ‘…ğ‘‚ğ‘– One-Hot [0-3] ZeRO level for stage ğ‘–\nğ¶ğ¾ğ‘ƒğ‘‡ğ‘– Integer Number of recomputed layers for stage ğ‘–\nğ‘Šğ‘‚ğ‘– Float [0, 1] Weight offloading ratio for stage ğ‘–\nğºğ‘‚ğ‘– Float [0, 1] Gradient offloading ratio for stage ğ‘–\nğ‘‚ğ‘‚ğ‘– Float [0, 1] Opt states offloading ratio for stage ğ‘–\nğ´ğ‘‚ğ‘– Float [0, 1] Activation offloading ratio for stage ğ‘–\noverlapped offloading communication, and memory buffer\noptimizations.\n5 Mist: Design Details\n5.1 Fine-Grained Overlap-Centric Scheduling\nMist coordinates memory optimizations and parallelism with\ntwo primary objectives: balancing optimization effectiveness\nwith manageable tuning complexity and maximizing overlap\nopportunities to reduce runtime overhead.\nOptimizations and Granularity. Mist comprehensively\nsupports various parallelism techniques such as DP, TP, and\nPP, alongside memory optimizations like fine-grained of-\nfloading, flexible activation checkpointing, and different lev-\nels of ZeRO optimization. Based on the observation that all\ntransformers are identical and share computational proper-\nties within a pipeline stage, Mist adopts stage-wise tuning\ngranularity, meaning all layers within the same pipeline stage\nuse the same parallelism and memory optimization config-\nurations to balance optimization effectiveness and search\nspace. Specifically, for gradient accumulation steps ğº and\nthe number of pipeline stages ğ‘†, the combination of (ğ¿ğ‘–, ğ‘ğ‘–,\nğ·ğ‘ƒğ‘–, ğ‘‡ğ‘ƒğ‘–, ğ‘ğ‘’ğ‘…ğ‘‚ğ‘–, ğ¶ğ¾ğ‘ƒğ‘‡ğ‘–, ğ‘Šğ‘‚ğ‘–, ğºğ‘‚ğ‘–, ğ‘‚ğ‘‚ğ‘–, ğ´ğ‘‚ğ‘–) defines the\nconfiguration for pipeline stage ğ‘–(see Table 2 for detailed ex-\nplanations of the symbols). Notably, swapping strategies are\nrepresented as floating-point ratios, enabling fine-grained\ncontrol of memory offloading and enhancing computation-\ncommunication overlap potential.\nOverlapped Schedule. Building upon these optimization\nstrategies, the overlapped schedule optimizes the execution\norder of memory optimizations and parallelism techniques\nto maximize hardware utilization while maintaining a low\nGPU memory footprint. As depicted in Figure 7, computation,\nGPU-GPU communication, and CPU-GPU communication\nare overlapped. As shown in â·, during the forward pass, the\ncomputation of layer ğ‘˜ overlaps with the activation swap-\nping out of layerğ‘˜âˆ’1, and the swapping in and all-gathering\nof parameters for layer ğ‘˜ +1. Similarly, as shown in â¸, in\nthe backward pass, the computation of layer ğ‘˜overlaps with\nEuroSys â€™25, March 30-April 3, 2025, Rotterdam, Netherlands Z. Zhu, C. Giannoula, M. Andoorveedu, Q. Su, K. Mangalam, B. Zhang, G. Pekhimenko\nCPU â†’ GPUGi Oi Gi+1Oi+1 Gi+2Oi+2 Gi+3Oi+3 Pi Pi+1 Pi+2 Pi+3Pi Gi Ai Pi-1Gi-1Ai-1Pi-2Gi-2Ai-2 â€¦GPU Computeâ€¦Si Fi-1 Si+1Fi Si+2Fi+1Si+3Fi-1 Fi Fi+1â€¦ Bi+1 Bi Bi-1 â€¦GPU âŸ·GPUUi Ui+1 Ui+2 Ui Ui+1 Ui+2 Ri+2Ui Ri+1Ui-1 Ri Ui-2 Ri-1â€¦GPU â†’ CPUOi-1Pi-1Ai-2 Oi Pi Ai-1 Oi+1Pi+1Ai Oi+2Pi+2Ai+1Ai-2 Ai-1 Ai Ai+1Gi+2 Gi+1 Gi Gi-1\nâ¶Forward PassoftheFirstMicrobatchâ·StableForward Passâ¸BackwardPass\nOi OptimizerStates(incl.FP32Params)\nPi FP16Params\nGi Gradients\n Ai Activations\nFi ForwardExec(incl.TP)\nBi BackwardExec(incl.TP)\nUi ParameterAll-gathering\nRi GradientReduce-scatteringTime\ni LayerIndex\n Si OptStep\nFigure 7. Overlap Schedule Template of Mist\nthe gradient reduction and the swapping-out of the previ-\nous backward layer ğ‘˜ +1, along with the swapping in of\nparameters, gradients and activations, and all-gathering of\nparameters for the next layer ğ‘˜ âˆ’1. This overlap ensures\nthat the computation in layer ğ‘˜ is not stalled by data move-\nment or pre-fetching, leading to better hardware utilization.\nAdditionally, Mist supports inter-stage overlap by hiding\ncommunications that are independent of previous stages\nwithin pipeline bubbles, as shown in Figure 10.\nOptimizer Step Decoupling and Repositioning. Further-\nmore, in scenarios involving ZeRO optimization and offload-\ning, a monolithic optimizer step can lead to increased peak\nmemory usage and redundant communication. Specifically,\nto perform an optimizer step in the mixed-precision opti-\nmizer, the following tensors must be in the GPU device at the\nsame time: FP16 parameters, FP16 gradients, FP32 optimizer\nstates, and FP32 master parameters [66]. When offloading is\napplied, peak memory during the optimizer step may exceed\nthat of the backward pass, as only partial layer states reside\nin GPU memory during the forward and backward computa-\ntions, while a monolithic optimizer step requires all of them\nfor the whole model. Additionally, optimizer steps require\nrematerializing all states through offloading or all-gathering,\nwhich also occur during the forward and backward passes,\nintroducing redundant communication. To address these is-\nsues, Mist decouples the optimizer step into multiple steps,\nrepositioning each layerâ€™s optimizer step immediately before\nits first forward pass. Moreover, to eliminate synchronization\nbetween pipeline stages for nan and inf checks, the validate-\nand-update method from zero-bubble pipeline parallelism\ncan be employed, delaying synchronization and reverting\noptimizer states if necessary [64].\n5.2 Symbolic-Based Efficient Performance Analysis\nPerformance modeling is crucial for optimizing distributed\ntraining as it enables efficient configuration exploration. Ex-\nisting systems rely on simulation-based performance predic-\ntion, running a concrete simulation for each configuration to\nestimate computation, communication, and memory usage.\nAs shown in Figure 8, a traditional simulator initializes a\nGPT model with a concrete parallelism configuration of e.g.,\nâ€¦\nâ€¦\nFigure 8. Comparison of the symbolic performance analyzer\nwith the traditional analyzer.\n(ğ·ğ‘ƒ=2, ğ‘‡ğ‘ƒ=8, ğ‘ƒğ‘ƒ=2) on 32 GPUs, applies memory optimiza-\ntions, and simulates execution to measure performance and\npeak memory usage. Although each simulation is efficient,\nit still takes about 6 seconds per configuration [ 21]. This\ncost makes exhaustive search impractical for our combined\nsearch space. We now demonstrate how Mistâ€™s symbolic-\nbased performance analysis helps to accelerate the perfor-\nmance prediction and thus facilitates the traversal over a\nhuge search space.\n5.2.1 Symbolic Analysis System Mist overcomes these\nlimitations by employing symbolic-based performance mod-\neling. Symbolic analysis refers to techniques used to ana-\nlyze systems by reasoning about symbolic representations\nof data or computations rather than concrete values, used\nin compiler optimizations [10] and circuit designs [27]. Mist\nproposes a symbolic analysis system for LLMs, supporting\nsymbolic execution, tracing, and analysis. As shown in Fig-\nure 9, users define the model and inputs, simply replacing\nthe concrete dimensions and optimizations with symbols.\nThen all operations are executed with the information of\nsymbolic shapes. On top of it, Mist traces the computational\ngraph and performs static analysis to derive symbolic ex-\npressions for execution time and memory usage. Instead\nof repeatedly simulating different configurations, Mist only\nperforms a single symbolic simulation pass and later sub-\nstitutes values into these expressions to quickly evaluate\ndifferent configurations. This approach enables batched eval-\nuation and compilation optimization, making performance\nprediction over 105Ã—faster than traditional analyzers.\nMist: Efficient Distributed Training of Large Language Models via Memory-Parallelism ... EuroSys â€™25, March 30-April 3, 2025, Rotterdam, Netherlands\n1 from mist import global_symbol_manager as gsm\n2 # Define symbols\n3 b, s, h, d, tp = gsm.symbols(\"b s h d tp\", (4, 128, 12, 64, 8),\ninteger=True, positive=True)\n4 # Initialize the model configuration using symbolic parameters\n5 config = GPT2Config(n_embd=h*d, n_head=h, tp=tp, ...)\n6 # Construct the GPT-2 model with symbolic configuration\n7 model = GPT2LMHeadModel(config)\n8 # Create symbolic input tensors\n9 input_ids = torch.randint(0, V, (b, s), dtype=torch.long)\n10 # Execute the model with symbolic inputs\n11 logits = model(input_ids).logits\n>>> logits\nsymbolic_tensor((b, s, V), concrete_shape=(4, 128, 50257), ...)\nFigure 9. Example of defining symbolic model configura-\ntions and inputs, followed by symbolic execution.\nFor memory analysis, Mist uses liveness analysis on the\nsymbolic computational graph. It tracks live tensors during\nexecution and determines peak memory usage by identi-\nfying the maximum memory allocation at any point. To\nsupport pipeline parallelism, Mist performs intra-layer and\ninter-layer analysis: the intra-layer pass extracts memory\nstatistics (e.g., layer states, saved activations, and intermedi-\nate tensors), while the inter-layer pass combines this data to\ngenerate stage-wise symbolic memory expressions, enabling\nefficient estimation across configurations.\nFor runtime analysis, direct symbolic representation is im-\npractical due to the complex behavior of various GPU kernels.\nInstead, Mist profiles operator execution dynamically. Com-\nputation is estimated using a operator computation database,\nwhich benchmarks new operators or unseen input shapes\non the current hardware and stores results for future use.\nCommunication is modeled symbolically by dividing com-\nmunicated bytes by the bandwidth, and overlap is managed\nvia interference modeling, which we introduce below.\nThe design of our symbolic analysis system is far from\ntrivial and addresses several significant challenges, making\nit both powerful and widely applicable. First, large models\nmust be run across multiple GPUs due to memory capacity\nissues, but direct analysis on multi-GPU setups is inefficient.\nWe solve this by using the idea of fake tensors and meta\ndevices, where tensor shapes are represented symbolically\nbut not materialized physically, allowing analysis without\nneeding actual hardware. Second, backward pass memory\nanalysis is difficult due to the absence of an explicit compu-\ntational graph. We generate a fake backward graph using\ngradient function properties to track memory during back-\npropagation. Third, supporting custom kernels like FlashAt-\ntention [19] and communication operations required custom\nsymbolic representations, ensuring flexibility. Beyond opti-\nmization, our symbolic analysis system offers clear insights\ninto workloads, making it easier to understand how specific\nparameters and optimizations affect performance, which can\nbe valuable for both practical use and educational purposes.\nAlgorithm 1: Batched Interference Estimation\nData: ğ¶,ğº2ğº,ğ¶2ğº,ğº2ğ¶, ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘ \nResult: Total latency vectorğ‘‡\n1 Function PredINTF(ğ¶,ğº2ğº,ğ¶2ğº,ğº2ğ¶,ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘  ):\n2 ğ‘‹ â†[ğ¶,ğº2ğº,ğ¶2ğº,ğº2ğ¶]ğ‘‡ // Stack features\n3 ğ‘‡ â†ğ‘ğ‘’ğ‘Ÿğ‘œğ‘ ğ¿ğ‘–ğ‘˜ğ‘’ (ğ¶) // Initialize output\n4 for ğ‘›= 4 downto 2 // The num of concurrent ops\n5 do\n6 for ğ‘– = 0 to \u00004\nğ‘›\n\u0001 âˆ’1 // Enumerate all combinations\n7 do\n// Index pre-defined masks and factors\n8 ğ‘šğ‘ğ‘ ğ‘˜ â†ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥ğ‘ƒğ‘Ÿğ‘’ğ‘‘ğ‘’ğ‘“ğ‘–ğ‘›ğ‘’ğ‘‘ğ‘€ğ‘ğ‘ ğ‘˜ (ğ‘›,ğ‘–)\n9 ğ‘“ğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿğ‘  â†ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥ğ¹ğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿğ‘  (ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘ ,ğ‘›,ğ‘– )\n10 Update(ğ‘‹,ğ‘‡,ğ‘šğ‘ğ‘ ğ‘˜,ğ‘“ğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿğ‘  )\n11 end\n12 end\n13 ğ‘‡ += sum(ğ‘‹,axis = âˆ’1) // Sum remaining time\n14 return ğ‘‡\n15 Function Update(ğ‘‹,ğ‘‡,ğ‘šğ‘ğ‘ ğ‘˜,ğ‘“ğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿğ‘  ):\n16 ids â†{ğ‘— |\u0000ğ‘‹ğ‘— â‰  0\u0001 matches ğ‘šğ‘ğ‘ ğ‘˜}\n17 if ids = âˆ…then return\n18 scaled â†ğ‘‹[ids]Ã— ğ‘“ğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿğ‘ \n19 ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘™ğ‘ğ‘ â†min(ğ‘ ğ‘ğ‘ğ‘™ğ‘’ğ‘‘, axis = âˆ’1)\n20 ğ‘‹[ids]â†( ğ‘ ğ‘ğ‘ğ‘™ğ‘’ğ‘‘ âˆ’ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘™ğ‘ğ‘ )/ğ‘“ğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿğ‘ \n21 ğ‘‡[ids]+= ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘™ğ‘ğ‘\n22 return\n5.2.2 Interference Model To be overlap aware, we inte-\ngrate an interference model within the symbolic analysis\nsystem. Runtime prediction is much more challenging when\noverlap is involved, such as computation, NCCL (GPU â†”\nGPU communication), D2H (GPUâ†’CPU communication),\nand H2D (CPUâ†’GPU communication) running simultane-\nously. Mist provides an interference model that predicts the\nimpact of up to four different types of kernels running si-\nmultaneously. Instead of using machine learning models like\nXGBoost [15], which may overfit in this case, we develop a\nmathematical model with fewer parameters and clearer intu-\nition. In this model, each possible combination of co-running\nkernels is assigned a set of slowdown factors that quantify\nthe effect of the execution for each participant.\nAlgorithm 1 implements batched interference estimation,\nwhich iteratively applies slowdown factors to update execu-\ntion times. For each concurrency level (ğ‘›= 4 to 2 operations),\nit iterates through all \u00004\nğ‘›\n\u0001 combinations, retrieves predefined\nmasks and factors, and invokesUpdate. The Update function\nscales execution time by their respective slowdown factors,\ncomputes the scaled overlapping, and updates remaining exe-\ncution times accordingly. By progressively resolving interfer-\nence through successive reductions, the algorithm eliminates\nconcurrent operations until only a single component remains.\nA data-driven approach is used to fit the model, where dif-\nferent shapes and combinations of concurrent kernels are\nsampled and benchmarked, and the resulting runtime data\nis used to train the slowdown factors.\nEuroSys â€™25, March 30-April 3, 2025, Rotterdam, Netherlands Z. Zhu, C. Giannoula, M. Andoorveedu, Q. Su, K. Mangalam, B. Zhang, G. Pekhimenko\naâ€™ a b\naâ€™ a\nc d e f g h\nb c d e f g h\naâ€™ a b c d e f g h\naâ€™ a b c d e f g h\nStage 1\nStage 2\nStage 3\nStage 4\nğ‘¡4\nğº âˆ’1 â‹…ğ‘¡3\nğ‘¡3\nğ‘¡2\nğ‘¡1\nğ‘‘3 âˆ’ğ‘¡1 âˆ’ğ‘¡2\nFigure 10. Illustration of runtime of a pipeline considering\ninter-microbatch imbalance. aâ€™ is the extra communication\noverhead only involved in the first and last microbatches.\n5.3 Imbalance-Aware Hierarchical Tuning via\nPareto Frontier Sampling\nOur tuning problem is defined as, given a model, a global\nbatch sizeğµ, and a device mesh(ğ‘,ğ‘€ ), Mistâ€™s auto-tuner out-\nputs the best training plan including the gradient accumula-\ntion steps ğº, layer partitions ğ‘ƒğ‘ƒ for different pipeline stages,\nand combination of [ğ‘, ğ·ğ‘ƒ, ğ‘‡ğ‘ƒ, ğ‘ğ‘’ğ‘…ğ‘‚, ğ¶ğ¾ğ‘ƒğ‘‡, ğ‘Šğ‘‚, ğºğ‘‚, ğ‘‚ğ‘‚,\nğ´ğ‘‚] for each pipeline stage, as detailed in Section 5.1.\nTo efficiently find the best strategy in a huge search space,\nMist adopts the idea of hierarchical tuning, decoupling the\nwhole tuning process into intra-stage tuning and inter-stage\ntuning [89]. Intra-stage tuning aims at finding the best op-\ntimization plans for all possible pipeline partitioning candi-\ndates, while inter-stage tuning is used to find the best stage\npartitioning and device assignment. Compared to existing\nautomatic parallelization methods, Mist offers two key im-\nprovements: imbalance and overlap awareness.\nInter-Stage Tuning. Inter-stage tuning finds the best layer\npartition and device assignment, as well as the best number\nof layers being recomputed. Unlike previous methods that\ntreat all microbatches as the same [46, 89], Mist discovers that\nusing either averaged runtime across different microbatches\nor simply applying the stable microbatch to tune leads to\nsub-optimal results. The former approximation might lead\nto bottleneck drifting, and the latter one fails to consider\nthe extra overhead of optimizations specifically for the first\nor last micro batch. As Figure 10 shows, Mist considers the\ninter-microbatch imbalance and proposes a new objective as\nmin\n1â‰¤ğ‘–â‰¤ğ‘†\nğ‘™ğ‘–,ğ‘ğ‘˜ğ‘ğ‘¡ğ‘–,\n(ğ‘›ğ‘–,ğ‘šğ‘– )\n(\n(ğºâˆ’1)Â· max\n1â‰¤ğ‘–â‰¤ğ‘†\n{ğ‘¡ğ‘–}+\nğ‘†âˆ‘ï¸\nğ‘–=1\nğ‘¡ğ‘– +max\n1â‰¤ğ‘–â‰¤ğ‘†\n \nğ‘‘ğ‘– âˆ’\nâˆ‘ï¸\n1â‰¤ğ‘—<ğ‘–\nğ‘¡ğ‘–\n!)\n(1)\nwhere ğ‘† means the number of stages, ğ‘™ğ‘– denotes the number\nof layers, ğ‘ğ‘˜ğ‘ğ‘¡ğ‘– denotes the number of checkpointed layers,\nand (ğ‘›ğ‘–,ğ‘šğ‘–)denotes the device assignment, for stage ğ‘–. For\nsimplicity, we define any microbatch that is neither first nor\nlast as a stable microbatch. ğ‘¡ğ‘– means the stable microbatch\nruntime of stage ğ‘–, and ğ‘‘ğ‘– means the runtime delta of the first\nand last microbatches compared to ğ‘¡ğ‘–. The first term ensures\nthat Mist correctly identifies the pipeline bottleneck, while\nthe second and third terms account for inter-stage and inter-\nmicrobatch imbalances, respectively. The third term also\nconsiders the overlap opportunities of hiding communication\nindependent of previous stages in the pipeline bubbles.\nObjective 1 can be solved given (ğ‘¡ğ‘–, ğ‘‘ğ‘–) according to ğ‘™ğ‘–, ğ‘ğ‘–,\nand (ğ‘›ğ‘–,ğ‘šğ‘–). However,ğ‘¡ğ‘– and ğ‘‘ğ‘– are correlated within a stage.\nFor instance, if optimizer offloading is applied aggressively,\nthe runtime of the first microbatch significantly increases\nand the runtime of the stable microbatches reduces because\nof the less intensive memory pressure. This suggests that (ğ‘¡ğ‘–,\nğ‘‘ğ‘–) form pairs along a Pareto frontier within the stage. Thus,\nwe transform the decision variables and obtain:\nmin\n1â‰¤ğ‘–â‰¤ğ‘†\nğ‘™ğ‘–,ğ‘“ğ‘–,(ğ‘›ğ‘–,ğ‘šğ‘– )\n(\n(ğºâˆ’1)Â· max\n1â‰¤ğ‘–â‰¤ğ‘†\n{ğ‘¡ğ‘–}+\nğ‘†âˆ‘ï¸\nğ‘–=1\nğ‘¡ğ‘– +max\n1â‰¤ğ‘–â‰¤ğ‘†\n \nğ‘‘ğ‘– âˆ’\nâˆ‘ï¸\n1â‰¤ğ‘—<ğ‘–\nğ‘¡ğ‘–\n!)\n(2)\n(ğ‘¡ğ‘–,ğ‘‘ğ‘–)= ğ¼ğ‘›ğ‘¡ğ‘Ÿğ‘ğ‘†ğ‘¡ğ‘ğ‘”ğ‘’ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘œ (ğ‘–,ğ‘™ğ‘–,(ğ‘›ğ‘–,ğ‘šğ‘–))[ğ‘“ğ‘–] (3)\nwhere ğ‘“ğ‘– is the sampled index from the intra-stage Pareto\nfrontier introduced in the next section. We directly combine\nthe checkpointing tuning into the Pareto frontier as it also\nserves as a trade-off between ğ‘¡ğ‘– and ğ‘‘ğ‘–. Objective (2) can\nbe reformulated into an MILP problem and solved by the\noff-the-shelf solver [28].\nIntra-Stage Tuning. As Objective (4) shows, given the\nstage partitioning, device assignment, gradient accumula-\ntion steps, and memory budget, intra-stage tuning finds the\nbest data and tensor parallelism, and memory optimization\ncombinations to maximize the throughput and sample the\nPareto frontier.\nmin\nğ‘,ğ‘§,ğ‘œ\nğ›¼Â·ğºÂ·ğ‘¡ğ‘,ğ‘§,ğ‘œ +(1 âˆ’ğ›¼)Â·ğ‘‘ğ‘,ğ‘§,ğ‘œ\nğ‘–.ğ‘’. max\n\u0010\nğ‘€ğ‘’ğ‘š(ğ‘“ğ‘¤ğ‘‘ )\nğ‘ğ‘’ğ‘ğ‘˜ ,ğ‘€ğ‘’ğ‘š(ğ‘ğ‘¤ğ‘‘)\nğ‘ğ‘’ğ‘ğ‘˜\n\u0011\nâ‰¤ğ‘€ğ‘’ğ‘šğµğ‘¢ğ‘‘ğ‘”ğ‘’ğ‘¡\n(4)\nwhere a series of ğ›¼ âˆˆ[0,1]are sampled uniformly to con-\nstruct a Pareto frontier efficiently. And the stable microbatch\ntime ğ‘¡ and delta time ğ‘‘ of a certain gradient accumulation\nstep ğº and strategy tuple ( parallelism ğ‘, ZeRO config ğ‘§, and\noffloading configs ğ‘œ) can be obtained from the interference\nmodel. The parallelism strategy ğ‘ includes ğ‘, ğ·ğ‘ƒ, ğ‘‡ğ‘ƒ. The\noffloading configuration ğ‘œ consists of ğ‘‚ğ‘‚, ğºğ‘‚, ğ‘Šğ‘‚, and ğ´ğ‘‚.\nğ‘¡ğ‘,ğ‘§,ğ‘œ = I\n\u0010\nğ‘ğ‘ ğ‘¡ğ‘ğ‘ğ‘™ğ‘’\nğ‘,ğ‘§,ğ‘œ ,ğ‘›ğ‘ğ‘ğ‘™ğ‘ ğ‘¡ğ‘ğ‘ğ‘™ğ‘’\nğ‘,ğ‘§,ğ‘œ ,ğ‘‘2â„ğ‘ ğ‘¡ğ‘ğ‘ğ‘™ğ‘’\nğ‘,ğ‘§,ğ‘œ ,â„2ğ‘‘ğ‘ ğ‘¡ğ‘ğ‘ğ‘™ğ‘’\nğ‘,ğ‘§,ğ‘œ\n\u0011\n(5)\nğ‘‘ğ‘,ğ‘§,ğ‘œ = I\n\u0010\nğ‘ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡\nğ‘,ğ‘§,ğ‘œ ,ğ‘›ğ‘ğ‘ğ‘™ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡\nğ‘,ğ‘§,ğ‘œ ,ğ‘‘2â„ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡\nğ‘,ğ‘§,ğ‘œ ,â„2ğ‘‘ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡\nğ‘,ğ‘§,ğ‘œ\n\u0011\nâˆ’ğ‘¡ğ‘,ğ‘§,0 (6)\nwhere Iis the interference model proposed before, ğ‘ means\nthe GPU computation time, ğ‘›ğ‘ğ‘ğ‘™ means the GPU-GPU com-\nmunication time, ğ‘‘2â„means the device to host copy time,\nand â„2ğ‘‘means the host to device copy time. The superscript\nğ‘ ğ‘¡ğ‘ğ‘ğ‘™ğ‘’ indicates the time of a stable micro batch, while ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡\nindicates the time of the first micro batch.\nAll the statistics of runtime and memories are reported\nby our symbolic analyzer. With the help of our symbolic-\nbased performance analyzer, querying single datapoints is\nextremely fast. Thus, to get the best strategy, we simply\nsearch in a brute-force way, which would not miss any opti-\nmization possibilities, ensuring the optimal solution.\nMist: Efficient Distributed Training of Large Language Models via Memory-Parallelism ... EuroSys â€™25, March 30-April 3, 2025, Rotterdam, Netherlands\nTable 3. Hardware Specifications.\nPlatform GPU GPU# Mem. PCIe Spec NVLink Interconnect\nGCP L4 [2, 4, 8, 16, 32] 24GB Gen3@16x âœ— 100Gbps\nAWS A100 [2, 4, 8, 16, 32] 40GB Gen4@16x âœ“ 400Gbps\nTable 4. Workload Specifications.\nGPU Models Param# (billion) Global Batch Size Seq Len\nL4 GPT, Llama, Falcon [1.3, 2.6, 6.7, 13, 22] [32, 64, 128, 256, 512] 2048\nA100 GPT, Llama, Falcon [1.3, 2.6, 6.7, 13, 22] [32, 64, 128, 256, 512] 4096\n6 Evaluation\nWe prototype Mist with âˆ¼27K LoC in Python. To support all\noptimizations, we have implemented it from scratch based\non PyTorch [61], supporting symbolic torch tracing and exe-\ncution, model automatic pipelining, overlapped offloading\nand ZeRO execution, and memory buffer optimizations.\nWe evaluate Mist on various training configurations with\ndifferent hardware, models, and hyper-parameters to demon-\nstrate its ability to effectively find the optimal combination\nof memory optimizations and parallelism. Our results show\nthat Mist constantly outperforms state-of-the-art distributed\ntraining systems. We use training throughput (samples per\nsecond) as our primary metric. Since all optimizations ap-\nplied by Mist are lossless, the fidelity of computation is pre-\nserved, ensuring the model convergence is not affected. Addi-\ntionally, we provide speedup breakdown, sensitivity studies,\nprediction accuracies, tuning time, and case studies to ex-\nplore the sources of our speedup and provide insights.\n6.1 Methodology\nHardware Settings. To fully study the capabilities of Mist,\nwe evaluate its training performance on both PCIe and NVLink\nsystems, as they offer different combinations of hardware\nresources. We conduct our major experiments on up to 32\nNVIDIA L4 GPUs [59] and 32 NVIDIA A100 GPUs [58]. De-\ntailed hardware specifications are shown in Table 3.\nWorkloads Setting. We selected three representative types\nof LLMs, GPT-3[9], LLaMa [23, 78, 79], and Falcon [2]. All\nare transformer-based models with some variation in their\ncomponents. GPT-3 consists of typical transformer decoder\nlayers[9]. LLaMa integrates techniques like pre-RMSNorm [86],\ngated functions [72], rotary embedding [75], among others,\nto improve performance on tasks involving long-range de-\npendencies [23, 78, 79]. Falcon adopts parallel attention and\nMLP layers inspired by GPT-J [ 14] and GPT-NeoX [8], re-\nducing the number of all-reduce operations associated with\ntensor parallelism from two to one per layer [2]. Following\ncommon practice, we scale the number of GPUs and the\nglobal batch size with the size of the model. To minimize the\nimpact of different frameworks and kernel implementations,\nwe set the dropout ratio to zero and disable all biases in the\nlinear layers. Table 4 shows the workloads specifications.\nBaselines. We compare Mist with three state-of-the-art\ndeep learning distributed training systems: (1) Megatron-\nLM [73] (core_r0.4.0), (2) DeepSpeed [68] (v0.12.6), and\n(3) Aceso [ 46]. Megatron-LM and DeepSpeed are state-\nof-the-art manual implementations. Since they do not sup-\nport automatic tuning, to achieve the best performance, we\nperform a grid search over all possible optimization com-\nbinations for single-node distributed training. For multi-\nnode cases, we benchmark the best strategies that Mist finds\nwithin the same search space as Megatron-LM and Deep-\nSpeed. Aceso is the state-of-the-art automatic distributed\nstrategy tuner with search space larger than others [12, 76],\nwhich can automatically find the best combinations of par-\nallelism and activation checkpointing plans. We follow its\nartifact to get its numbers.\nIn the common practice of training LLMs, FlashAtten-\ntion [18, 19], a vital kernel for performing the fast and memory-\nefficient attention mechanism, is applied by default to reduce\nmemory usage and achieve the best performance. When\nFlashAttention is enabled, we only compare with Megatron-\nLM and DeepSpeed since the Aceso does not support it. We\ncompare all three baselines on L4 GPUs. For A100 GPUs,\nwe only compare Mist with state-of-the-art manual and au-\ntomatic methods, Megatron-LM and Aceso as DeepSpeed\ngenerally underperforms Megatron-LM in our experiments.\nWe attempted to compare with Alpa [89], but it fails to\nfind any feasible solutions on L4 GPUs for our workloads.\nOur conjecture is that Alpa only considers memory usage\nin the Inter-Op pass by compiling the searched strategy and\nrunning it, while its memory-unaware Intra-Op pass likely\ncauses OOM errors for all proposed strategies.\n6.2 End-to-End Training Performance\nSpeedup in Real-World Scenarios. Figure 11 compares\nthe end-to-end throughput of various distributed training\nframeworks in real-world scenarios where FlashAttention [18,\n19] is enabled. We make two key observations. First, Megatron-\nLM outperforms DeepSpeed in most cases. This is mainly\ndue to the fact that the parallelization plans that work in\nMegatron-LM cause out-of-memory issues in DeepSpeed,\nforcing DeepSpeed to choose sub-optimal parallelization\nstrategies. Second, Mist consistently outperforms other dis-\ntributed training frameworks, achieving an average speedup\nof 1.32Ã—(up to 1.59Ã—) over Megatron-LM on L4 GPUs, 1.51Ã—\non average (up to 1.67Ã—) over DeepSpeed on L4 GPUs, and\n1.34Ã—on average (up to 1.72Ã—) over Megatron-LM on A100\nGPUs. Specifically for the GPT-3 model, which is the most\nheavily optimized model in other frameworks, Mist achieves\n1.22Ã—speedup on average (up to 1.32 Ã—) on L4 GPUs, and\n1.20Ã—speedup on average (up to 1.32Ã—) on A100 GPUs, com-\npared with Megatron-LM. The higher speedup for LLaMa\nmodel mainly comes from the better RMSNorm kernel im-\nplementation and efficient rotary embedding implementa-\ntion [19]. Overall, we conclude that Mist achieves the best\nEuroSys â€™25, March 30-April 3, 2025, Rotterdam, Netherlands Z. Zhu, C. Giannoula, M. Andoorveedu, Q. Su, K. Mangalam, B. Zhang, G. Pekhimenko\n1.3B 2.7B 7B 13B 22B\n0.0\n1.0\n2.0\n3.0\n4.0Throughput\n1.00Ã—\n1.00Ã—\n1.00Ã—\n1.00Ã—\n1.00Ã—\n0.95Ã—\n0.91Ã—\n0.74Ã—\n0.80Ã—\n0.80Ã—\n1.21Ã—\n1.20Ã—\n1.15Ã—\n1.26Ã—\n1.32Ã—\n(a) GPT3 - L4 GPUs\nMegatron-LM DeepSpeed Mist\n1.3B 2.7B 7B 13B 22B\n0.0\n1.0\n2.0\n3.0\n4.0Throughput\n1.00Ã—\n1.00Ã—\n1.00Ã—\n1.00Ã—\n1.00Ã—\n1.11Ã—\n0.97Ã—\n0.80Ã—\n0.86Ã—\n0.90Ã—\n1.59Ã—\n1.42Ã—\n1.33Ã—\n1.34Ã—\n1.48Ã—\n(b) Llama2 - L4 GPUs\n1.3B 2.7B 7B 13B 22B\n0.0\n1.0\n2.0\n3.0\n4.0Throughput\n1.00Ã—\n1.00Ã—\n1.00Ã—\n1.00Ã—\n1.00Ã—\n1.05Ã—\n0.94Ã—\n0.74Ã—\n0.81Ã—\n0.81Ã—\n1.42Ã—\n1.29Ã—\n1.20Ã—\n1.27Ã—\n1.36Ã—\n(c) Falcon - L4 GPUs\n1.3B 2.7B 7B 13B 22B\n0.0\n2.5\n5.0\n7.5\n10.0Throughput\n1.00Ã—\n1.00Ã—\n1.00Ã—\n1.00Ã—\n1.00Ã—\n1.25Ã—\n1.23Ã—\n1.32Ã—\n1.12Ã—\n1.10Ã—\n(d) GPT3 - A100 GPUs\n1.3B 2.7B 7B 13B 22B\n0.0\n2.5\n5.0\n7.5\n10.0Throughput\n1.00Ã—\n1.00Ã—\n1.00Ã—\n1.00Ã—\n1.00Ã—\n1.73Ã—\n1.54Ã—\n1.59Ã—\n1.32Ã—\n1.35Ã—\n(e) Llama2 - A100 GPUs\n1.3B 2.7B 7B 13B 22B\n0.0\n2.5\n5.0\n7.5\n10.0Throughput\n1.00Ã—\n1.00Ã—\n1.00Ã—\n1.00Ã—\n1.00Ã—\n1.43Ã—\n1.40Ã—\n1.39Ã—\n1.19Ã—\n1.20Ã—\n(f) Falcon - A100 GPUs\nFigure 11. End-to-end training throughput (samples/sec) on L4 GPUs and A100 GPUs, with FlashAttention enabled. Sequence\nlengths are 2048 for L4 GPUs and 4096 for A100 GPUs. The numbers of GPUs are 2, 4, 8, 16, 32, respectively.\n1.3B 2.7B 7B 13B 22B\n0.0\n0.8\n1.6\n2.4\n3.2Throughput\n1.00Ã—\n1.00Ã—\n1.00Ã—\n1.00Ã—\n1.00Ã—\n0.90Ã—\n0.91Ã—\n0.62Ã—\n0.81Ã—\n0.91Ã—\n0.73Ã—\n0.95Ã—\n1.09Ã—\n1.04Ã—\n1.07Ã—\n1.11Ã—\n1.20Ã—\n1.20Ã—\n1.14Ã—\n1.20Ã—\n(a) L4 GPUs\n1.3B 2.7B 7B 13B 22B\n0.0\n1.5\n3.0\n4.5\n6.0Throughput\n1.00Ã—\n1.00Ã—\n1.00Ã—\n1.00Ã—\n1.00Ã—\n0.62Ã—\n0.89Ã—\n1.06Ã—\n0.90Ã—\n0.92Ã—\n1.26Ã—\n1.09Ã—\n1.15Ã—\n0.99Ã—\n1.05Ã—\n(b) A100 GPUs\nMegatron-LM DeepSpeed Aceso Mist\nFigure 12. End-to-end training throughput (samples/sec) on L4 GPUs and A100 GPUs, without FlashAttention. Sequence\nlengths are 2048 for L4 GPUs and 4096 for A100 GPUs. The numbers of GPUs are 2, 4, 8, 16, 32, respectively.\nperformance over prior state-of-the-art manual implementa-\ntions across various models and hardware.\nSpeedup compared with more baselines. Figure 12 com-\npares the throughput of the GPT-3 model with both manual\nand automatic parallelization frameworks without FlashAt-\ntention. Mist still consistently outperforms or is equal to\nall prior distributed training frameworks. Mist achieves an\naverage of 1.14Ã—speedup (up to 1.26Ã—speedup) compared to\nMegatron-LM and an average of 1.27Ã—speedup (up to 2.04Ã—\nspeedup) compared to Aceso. When training GPT-3 13B on\n16 A100 GPUs, Mist does not achieve better performance but\nstill gets almost the same results, because the naive strategy\nhappens to achieve the best trade-off among all resources.\nWe also find that Aceso does not consistently outperform\nMegatron-LM even though it has larger search space due\nto fine-grained activation checkpointing tuning. The root\ncause is that Aceso does not include sharded data parallelism\nin the search space and miss several essential opportunities\nfor communication-computation overlapping.\nDiscussion on the hardware. As shown in Figures 11\nand 12, Mist exhibits higher speedup on L4 GPUs than that on\nA100 GPUs, with following reasons. Large-scale distributed\ntraining tasks on L4 GPUs are often limited by smaller mem-\nory capacity and the restricted intra-node and inter-node\nbandwidth. In this scenario, Mist plays a crucial role in strik-\ning the best trade-off among various resources to enhance\nthe resource utilization. On the other hand, training tasks on\n0.8 0.9 1.0 1.1 1.2 1.3\nRelative Averaged Speedup\n+Imbalance-Aware Pipelining 1.28x\n+Offloading 1.19x\n+Flexible CKPT 1.12x\n+ZeRO-2/3 1.03x\n3D Parallelism 1.00x Megatron-LM\nFigure 13. Relative averaged speedup of tuning over differ-\nent search spaces for GPT model on 8, 16, and 32 L4 GPUs.\nA100 GPUs benefit from larger memory capacity and faster\nintra-node NVLink and inter-node InfiniBand connections,\nresulting in much higher resource utilization that approaches\nthe physical limits. This leaves less room for improvement.\n6.3 Speedup Breakdown\nTo understand how each key ideas of Mist contributes to the\nfinal performance, we evaluate it by incrementally enlarging\nthe search space proposed by Mist in Figure 13. We normal-\nized the throughput by the baseline search space of Megatron-\nLM. Three key conclusions are drawn: First, Mistâ€™s advantage\nis not from the better implementation; with the same search\nspace as Megatron-LM, Mist is slightly slower due to imple-\nmentation overhead supporting other optimizations. Second,\nactivation checkpointing tuning provides a 1.12Ã—speedup\non average, with offloading adding an extra 7% speedup,\nshowing their abilities of striking better trade-offs among\nMist: Efficient Distributed Training of Large Language Models via Memory-Parallelism ... EuroSys â€™25, March 30-April 3, 2025, Rotterdam, Netherlands\n32 48 64 80\nNumber of Layers\n0.0\n2.5\n5.0\n7.5\n10.0Throughput\n 1.00Ã—\n 1.00Ã—\n 1.00Ã—\n 1.00Ã—\n 1.15Ã—\n 1.08Ã—\n 1.12Ã—\n 1.00Ã—\n 1.20Ã—\n 1.17Ã—\n 1.17Ã—\n 1.21Ã—\n32 48 64 80\nNumber of Layers\n0.0\n2.5\n5.0\n7.5\n10.0Throughput\n 1.00Ã—\n 1.00Ã—\n 1.00Ã—\n 1.00Ã—\n 1.16Ã—\n 1.16Ã—\n 1.17Ã—\n 1.10Ã—\n 1.25Ã—\n 1.24Ã—\n 1.30Ã—\n 1.32Ã—\n3D Parallelism 3D+CKPT Tuning Mist\nFigure 14. Performance of GPT-3 with different number of\nlayers on 32 L4 GPUs. Left: without FlashAttention; Right:\nwith FlashAttention.\nresources. Third, inter-microbatch imbalance-awareness of-\nfers an extra 9% speedup upon all prior speedups, as it pro-\nvides accurate runtime predictions for pipeline parallelism.\nIn summary, all optimizations included in Mist are crucial\nfor improve the system performance.\nOne key insight we observe is that much of the speedup\ncomes from reducing activation checkpointing and elimi-\nnating pipeline bubbles. However, naively disabling check-\npointing often leads to OOM errors, and tuning it (as done in\nAceso) only partially solves the issue. Further improvements\nfrom increasing ZeRO levels or increasing offloading are es-\nsential, as they help to further reduce recomputation. As long\nas these overheads can be overlapped or amortized across\nmultiple microbatches, performance significantly improves.\nAdditionally, speedups may vary depending on hardware\nresources and workload intensity. On A100 GPUs with mod-\nerate workloads, most speedups come from activation check-\npointing tuning. However, when memory pressure is high,\ncombining it with offloading becomes important. For in-\nstance, training a 40B GPT-3 model on 32 A100 GPUs, Mist\nis expected to get 1.10 Ã—speedup compared to 1.04 Ã—with\nonly activation checkpointing tuning.\n6.4 Sensitivity Study\nTo comprehensively understand the robustness of Mist, we\nevaluate its performance with different model scales and\ndifferent global batch sizes.\nRobustness over Different Model Scales. As depicted in\nFigure 14, Mist consistently outperforms the baseline search\nspaces by up to 1.32 Ã—higher throughput, particularly at\n80 layers, regardless of whether FlashAttention is enabled.\nActivation checkpointing tuning is particularly effective for\nsmaller model sizes. However, as model size increases, the\nspeedup from checkpointing alone decreases. With the entire\nsearch space enabled, Mist maintains substantial speedups\nacross different model sizes.\nRobustness over Different Global Batch Sizes. As shown\nin Figure 15, Mist always achieves the best performance com-\npared to the baseline search space across different global\nbatch sizes. Notably, Imbalance-Aware Inter-Stage Tuning\nprovides an extra 1.13Ã—speedup on average. One concern\nis that with larger global batch sizes and potentially more\n256 512 1024 2048\nGlobal Batch Size\n0.0\n1.5\n3.0\n4.5Throughput\n 1.00Ã—\n 1.00Ã—\n 1.00Ã—\n 1.00Ã—\n 1.15Ã—\n 1.22Ã—\n 1.16Ã—\n 1.18Ã—\n 1.28Ã—\n 1.35Ã—\n 1.34Ã—\n 1.35Ã—\n3D Parallelism\nMist Without\nImbalance-Aware PP\nMist\nFigure 15. Performance of GPT-3 22B with different global\nbatch sizes on 32 L4 GPUs.\nAlpa Aceso3D Para.+CKPT+ZeRO +OO +AO +GO +WO\n100\n101\n102\n103\n104\nTime (Seconds)\n10106\n201\n92 102 99\n414 431 532 1083\nFigure 16. Tuning Time of the GPT-3 22B model on 32 GPUs.\nOrange bars are Mist with different optimizations applied.\nmicrobatches, the benefit of imbalance-aware inter-stage\ntuning might diminish, as treating all microbatches as the\nsame might seem sufficient. However, the sub-optimal strat-\negy produced by inaccurate predictions lead to a significant\nperformance gap due to the larger gradient accumulation\nsteps while Mistâ€™s inter-microbatch awareness avoids it.\n6.5 Tuning Time Comparison\nAs Figure 16 shows, to understand the tuning efficiency of\nMist, we evaluate the tuning time by enabling optimizations\none by one and compare it with the tuning time of Alpa [89]\nand Aceso [46]. For Alpa, we choose 6 data points because it\ndoesnâ€™t automatically tune the gradient accumulation steps\nand layer grouping size. We make three key observations:\nFirst, Mist helps to reduce the tuning time a lot compared\nto Alpa. Second, when Mist is configured to use a similar\nsearch space as Aceso, which is branded as an efficient dis-\ntributed training searching system, Mist can be faster. Third,\neven when Mist enables more optimization and greatly in-\ncreases the search space, the tuning time remains reasonable\ncompared to the significantly longer training time. More-\nover, since searching over different gradient accumulation\nsteps is independent, Mistâ€™s tuning can be parallelized across\ndifferent CPU cores or machines to make it faster.\n6.6 Accuracy of Symbolic Shape Analysis System\nThe effectiveness of Mistâ€™s tuning system relies heavily on\nthe performance prediction accuracy. Therefore, we sample\ndifferent strategies and benchmark the accuracy of the pre-\ndicted runtime and memory usage compared with the actual\nones. Mist consistently demonstrates a high prediction ac-\ncuracy for both runtime and memory usage. The averaged\nruntime error ratio is 1.79%, and average memory footprint\nerror ratio is 2.10%. For runtime, our analysis system focuses\nmore on the magnitude comparison to determine the best\nEuroSys â€™25, March 30-April 3, 2025, Rotterdam, Netherlands Z. Zhu, C. Giannoula, M. Andoorveedu, Q. Su, K. Mangalam, B. Zhang, G. Pekhimenko\nstrategy. As a result, some minor times, such as the optimizer\nstep time, are not included. To better understand runtime\naccuracy, we shift the predicted runtime so that the mean\nvalues of predicted and actual runtime match.\n7 Other Related Work\nDNN Performance Modeling. The closest works to ours\nin performance modeling are DistSim, Proteus, and dPRO\n[21, 34, 48], which use simulation-based approaches to pre-\ndict model performance. Mist differs in that we consider an\nexpanded search space including all memory optimizations\nin addition to parallelism strategies. This requires additional\nsophisticated modeling with respect to overlap and inter-\nference. Moreover, Mist employs a symbolic modeling first\napproach that is well-suited for the efficiency that the ex-\npanded search space demands. DistIR [71] is a work that also\nemploys an analytical-based approach, but employs a cost-\nmodel predictor of operator latencies. Habitat [84] profiles\noperators in tandem with methods to extrapolate perfor-\nmance to other GPUs for non-distributed scenarios.\nSymbolic Analysis. Symbolic analysis examines program\nbehavior through abstract representations of variables and\ncomputations rather than concrete values, enabling system-\natic exploration of optimization spaces. Foundational ap-\nplications in compilers leveraged symbolic techniques for\ndependency analysis and loop transformations via constraint\nsolving [10, 24], while analog circuit design adopted symbolic\nmethods for parameter space exploration [ 27, 29]. Unlike\nprior works targeting code-level optimizations or hardware\nverification, Mist adapts symbolic analysis in the area of per-\nformance estimation for deep learning models to efficiently\nexplore the joint space of parallelism strategies and memory\nreduction techniques in distributed training.\nAcceleration Techniques. Techniques like tensor compila-\ntion and kernel optimizations (e.g., TVM, Hidet, FlashAtten-\ntion) are largely different than Mist, since they mainly focus\non lower-level, static graph optimizations from the graph\nto hardware level [4, 16, 18â€“20, 30, 33, 44]. Additionally, ap-\nproaches such as gradient compression, quantization, and\nsparsity, and automatic mixed precision [ 7, 11, 13, 54, 83]\nare also orthogonal to Mist: they could be integrated into\nMistâ€™s schedule template as additional optimization tech-\nniques that can improve system performance and memory\nefficiency. Some of these may also be potentially lossy opti-\nmizations, i.e., downgrading the accuracy of models, whereas\nMist exclusively targets system-level improvements without\ncompromising training accuracy.\n8 Discussion\nIntegration of Other Techniques. Mist is extensible to ad-\nditional operators, parallelism strategies, and optimizations.\nNew acceleration or memory optimization techniques like\nquantization [54, 62] or compression [13, 47] are typically\nimplemented using native torch operators or customized\nkernels. Native torch operators (including communication\noperators) can be straightforwardly supported by Mist, and\ncustomized CUDA kernels can be easily incorporated by\nregistering them in the symbolic analysis system, as demon-\nstrated with FlashAttention [19]. Therefore, these optimiza-\ntions are directly reflected in the traced computational graph\nproduced by Mist and analyzed by Mist, thus they will be\nconfigured intelligently to achieve high performance.\nFuture Work. Mist relies on a symbolic computational\ngraph, making it less suited for highly dynamic workloads\nwhere a fixed computation graph is difficult to obtain. How-\never, for workloads like Mixture of Experts (MoE) with expert\nparallelism [65], where computation patterns are largely pre-\ndictable, data-dependent routing can be handled through\nmultiple simulations to obtain an average performance esti-\nmate. Another limitation is that, while Mist supports fine-\ngrained overlap of multiple operations, ensuring correctness\nremains challenging due to potential data races and value\ninconsistencies. Future work should focus on developing\nautomated mechanisms to manage overlap and prevent ex-\necution conflicts. Additionally, although Mist can analyze\narbitrary computational graphs, its efficient tuning algorithm\nassumes identical layers. Extending it to optimize models\nwith heterogeneous architectures is an important direction.\n9 Conclusion\nWe propose Mist, a memory, overlap, and imbalance aware\nmethod that enables efficient LLM training by co-optimizing\nall memory footprint reduction techniques and parallelism\nstrategies in a comprehensive manner. Mist contributes a\nfine-grained overlap-centric schedule template, an symbolic-\nbased efficient analysis system, and an imbalance-aware hier-\narchical auto-tuner to allow efficient optimization in a large\nsearch space over optimizations with complex interactions.\nAs a result, Mist achieves 1.27Ã—(up to 2.04Ã—) over state-of-\nthe-art distributed training systems such as Aceso. We hope\nthat Mist will be able to help democratize LLM training for\nmachine learning researchers and practitioners alike.\nAcknowledgments\nWe sincerely thank our shepherd, Zhaoguo Wang, and the\nanonymous reviewers for their valuable feedback. We also\nappreciate members of the EcoSystem Research Laboratory\nat the University of Toronto for their discussions and sugges-\ntions, with special thanks to Anand Jayarajan, Xin Li, Wei\nZhao, Yaoyao Ding, and Jiacheng Yang for their contributions.\nThe authors with the University of Toronto are supported\nby Vector Institute Research grants, the Canada Foundation\nfor Innovation JELF grant, NSERC Discovery grant, AWS\nMachine Learning Research Award (MLRA), Facebook Fac-\nulty Research Award, Google Scholar Research Award, and\nVMware Early Career Faculty Grant.\nMist: Efficient Distributed Training of Large Language Models via Memory-Parallelism ... EuroSys â€™25, March 30-April 3, 2025, Rotterdam, Netherlands\nReferences\n[1] MartÃ­n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis,\nJeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving,\nMichael Isard, et al. 2016. {TensorFlow}: a system for {Large-Scale}\nmachine learning. In 12th USENIX symposium on operating systems\ndesign and implementation (OSDI 16) . 265â€“283.\n[2] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessan-\ndro Cappelli, Ruxandra Cojocaru, MÃ©rouane Debbah, Ã‰tienne Goffinet,\nDaniel Hesslow, Julien Launay, Quentin Malartic, et al . 2023. The\nfalcon series of open language models. arXiv preprint arXiv:2311.16867\n(2023).\n[3] Muralidhar Andoorveedu, Zhanda Zhu, Bojian Zheng, and Gennady\nPekhimenko. 2022. Tempo: Accelerating Transformer-Based Model\nTraining through Memory Footprint Reduction. Advances in Neural\nInformation Processing Systems 35 (2022), 12267â€“12282.\n[4] Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh\nJain, Michael Voznesensky, Bin Bao, David Berard, Geeta Chauhan, An-\njali Chourdia, et al. 2024. PyTorch 2: Faster Machine Learning Through\nDynamic Python Bytecode Transformation and Graph Compilation.\n(2024). To appear at ASPLOS.\n[5] Anthropic. 2024. Claude. https://claude.ai/.\n[6] Sanjith Athlur, Nitika Saran, Muthian Sivathanu, Ramachandran Ram-\njee, and Nipun Kwatra. 2022. Varuna: scalable, low-cost training of\nmassive deep learning models. In Proceedings of the Seventeenth Euro-\npean Conference on Computer Systems . 472â€“487.\n[7] Youhui Bai, Cheng Li, Quan Zhou, Jun Yi, Ping Gong, Feng Yan,\nRuichuan Chen, and Yinlong Xu. 2021. Gradient Compression Super-\ncharged High-Performance Data Parallel DNN Training. InProceedings\nof the ACM SIGOPS 28th Symposium on Operating Systems Principles .\n[8] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao,\nLaurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason\nPhang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria\nReynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-\nNeoX-20B: An Open-Source Autoregressive Language Model. In Pro-\nceedings of the ACL Workshop on Challenges & Perspectives in Creating\nLarge Language Models . https://arxiv.org/abs/2204.06745\n[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D\nKaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish\nSastry, Amanda Askell, et al . 2020. Language models are few-shot\nlearners. Advances in neural information processing systems 33 (2020),\n1877â€“1901.\n[10] Cristian Cadar, Daniel Dunbar, Dawson R Engler, et al. 2008. Klee: unas-\nsisted and automatic generation of high-coverage tests for complex\nsystems programs.. In OSDI, Vol. 8. 209â€“224.\n[11] Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri\nRudra, and Christopher RÃ©. 2022. Pixelated Butterfly: Simple and\nEfficient Sparse training for Neural Network Models. In The Tenth\nInternational Conference on Learning Representations, ICLR .\n[12] Hongzheng Chen, Cody Hao Yu, Shuai Zheng, Zhen Zhang, Zhiru\nZhang, and Yida Wang. 2023. Slapo: A Schedule Language for\nProgressive Optimization of Large Deep Learning Model Training.\narXiv:2302.08005 [cs.LG]\n[13] Jianfei Chen, Lianmin Zheng, Zhewei Yao, Dequan Wang, Ion Stoica,\nMichael W Mahoney, and Joseph E Gonzalez. 2021. ActNN: Reducing\nTraining Memory Footprint via 2-Bit Activation Compressed Training.\nIn International Conference on Machine Learning (ICML) .\n[14] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde\nde Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas\nJoseph, Greg Brockman, et al. 2021. Evaluating large language models\ntrained on code. arXiv preprint arXiv:2107.03374 (2021).\n[15] Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree\nboosting system. In Proceedings of the 22nd acm sigkdd international\nconference on knowledge discovery and data mining . 785â€“794.\n[16] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie\nYan, Meghan Cowan, Haichen Shen, Leyuan Wang, Yuwei Hu, Luis\nCeze, Carlos Guestrin, and Arvind Krishnamurthy. 2018. TVM: an\nautomated end-to-end optimizing compiler for deep learning. In Pro-\nceedings of the 13th USENIX Conference on Operating Systems Design\nand Implementation .\n[17] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016.\nTraining deep nets with sublinear memory cost. arXiv preprint\narXiv:1604.06174 (2016).\n[18] Tri Dao. 2023. FlashAttention-2: Faster Attention with Better Paral-\nlelism and Work Partitioning. (2023).\n[19] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©.\n2022. FlashAttention: Fast and Memory-Efficient Exact Attention with\nIO-Awareness. In Advances in Neural Information Processing Systems .\n[20] Yaoyao Ding, Cody Hao Yu, Bojian Zheng, Yizhi Liu, Yida Wang, and\nGennady Pekhimenko. 2023. Hidet: Task-Mapping Programming Par-\nadigm for Deep Learning Tensor Programs. In Proceedings of the 28th\nACM International Conference on Architectural Support for Program-\nming Languages and Operating Systems, Volume 2 .\n[21] Jiangfei Duan, Xiuhong Li, Ping Xu, Xingcheng Zhang, Shengen Yan,\nYun Liang, and Dahua Lin. 2023. Proteus: Simulating the Performance\nof Distributed DNN Training. arXiv preprint arXiv:2306.02267 (2023).\n[22] facebookresearch/llama. 2023. llama/MODEL_CARD.md. https://\ngithub.com/facebookresearch/llama/blob/main/MODEL_CARD.md.\n[23] facebookresearch/llama. 2024. llama3. https://github.com/meta-llama/\nllama3.\n[24] Thomas Fahringer and Bernhard Scholz. 1997. Symbolic evaluation\nfor parallelizing compilers. In Proceedings of the 11th international\nconference on Supercomputing . 261â€“268.\n[25] Shiqing Fan, Yi Rong, Chen Meng, Zongyan Cao, Siyu Wang, Zhen\nZheng, Chuan Wu, Guoping Long, Jun Yang, Lixue Xia, et al . 2021.\nDAPPLE: A pipelined data parallel approach for training large models.\nIn Proceedings of the 26th ACM SIGPLAN Symposium on Principles and\nPractice of Parallel Programming . 431â€“445.\n[26] Yangyang Feng, Minhui Xie, Zijie Tian, Shuo Wang, Youyou Lu, and\nJiwu Shu. 2023. Mobius: Fine tuning large-scale models on commodity\ngpu servers. In Proceedings of the 28th ACM International Conference\non Architectural Support for Programming Languages and Operating\nSystems, Volume 2 . 489â€“501.\n[27] Henrik Floberg. 2012. Symbolic analysis in analog integrated circuit\ndesign. Vol. 413. Springer Science & Business Media.\n[28] John Forrest and Robin Lougee-Heimer. 2005. CBC user guide. In\nEmerging theory, methods, and applications . INFORMS, 257â€“277.\n[29] Georges Gielen, Piet Wambacq, and Willy M Sansen. 1994. Sym-\nbolic analysis methods and applications for analog circuits: A tutorial\noverview. Proc. IEEE 82, 2 (1994), 287â€“304.\n[30] Google. 2022. XLA. https://www.tensorflow.org/xla.\n[31] Cong Guo, Rui Zhang, Jiale Xu, Jingwen Leng, Zihan Liu, Ziyu Huang,\nMinyi Guo, Hao Wu, Shouren Zhao, Junping Zhao, et al. 2024. GMLake:\nEfficient and Transparent GPU Memory Defragmentation for Large-\nscale DNN Training with Virtual Memory Stitching. arXiv preprint\narXiv:2401.08156 (2024).\n[32] Chaoyang He, Shen Li, Mahdi Soltanolkotabi, and Salman Avestimehr.\n2021. Pipetransformer: Automated elastic pipelining for distributed\ntraining of transformers. arXiv preprint arXiv:2102.03161 (2021).\n[33] Horace He and Shangdi Yu. 2023. Transcending Runtime-Memory\nTradeoffs in Checkpointing by being Fusion Aware. Proceedings of\nMachine Learning and Systems (2023).\n[34] Hanpeng Hu, Chenyu Jiang, Yuchen Zhong, Yanghua Peng, Chuan\nWu, Yibo Zhu, Haibin Lin, and Chuanxiong Guo. 2022. dpro: A generic\nperformance diagnosis and optimization toolkit for expediting dis-\ntributed dnn training. Proceedings of Machine Learning and Systems\n(2022).\nEuroSys â€™25, March 30-April 3, 2025, Rotterdam, Netherlands Z. Zhu, C. Giannoula, M. Andoorveedu, Q. Su, K. Mangalam, B. Zhang, G. Pekhimenko\n[35] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao\nChen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui\nWu, et al. 2019. Gpipe: Efficient training of giant neural networks\nusing pipeline parallelism. Advances in neural information processing\nsystems 32 (2019).\n[36] Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter\nAbbeel, Joseph Gonzalez, Kurt Keutzer, and Ion Stoica. 2020. Check-\nmate: Breaking the memory wall with optimal tensor rematerialization.\nProceedings of Machine Learning and Systems 2 (2020), 497â€“511.\n[37] Chiheon Kim, Heungsub Lee, Myungryong Jeong, Woonhyuk Baek,\nBoogeon Yoon, Ildoo Kim, Sungbin Lim, and Sungwoong Kim. 2020.\ntorchgpipe: On-the-fly pipeline parallelism for training giant models.\narXiv preprint arXiv:2004.09910 (2020).\n[38] Marisa Kirisame, Steven Lyubomirsky, Altan Haan, Jennifer Bren-\nnan, Mike He, Jared Roesch, Tianqi Chen, and Zachary Tatlock. 2020.\nDynamic Tensor Rematerialization. In International Conference on\nLearning Representations .\n[39] Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence\nMcAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catan-\nzaro. 2023. Reducing activation recomputation in large transformer\nmodels. Proceedings of Machine Learning and Systems 5 (2023).\n[40] Zhiquan Lai, Shengwei Li, Xudong Tang, Keshi Ge, Weijie Liu, Yabo\nDuan, Linbo Qiao, and Dongsheng Li. 2023. Merak: An efficient dis-\ntributed dnn training framework with automated 3d parallelism for\ngiant foundation models. IEEE Transactions on Parallel and Distributed\nSystems 34, 5 (2023), 1466â€“1478.\n[41] Dacheng Li, Hongyi Wang, Eric Xing, and Hao Zhang. 2022. Amp:\nAutomatically finding model parallel strategies with heterogeneity\nawareness. Advances in Neural Information Processing Systems 35\n(2022), 6630â€“6639.\n[42] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis,\nTeng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania,\net al. 2020. Pytorch distributed: Experiences on accelerating data\nparallel training. arXiv preprint arXiv:2006.15704 (2020).\n[43] Zhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao Zhang,\nDawn Song, and Ion Stoica. 2021. Terapipe: Token-level pipeline\nparallelism for training large-scale language models. In International\nConference on Machine Learning . PMLR, 6543â€“6552.\n[44] Bin Lin, Ningxin Zheng, Lei Wang, Shijie Cao, Lingxiao Ma, Quanlu\nZhang, Yi Zhu, Ting Cao, Jilong Xue, Yuqing Yang, et al. 2023. Efficient\nGPU Kernels for N: M-Sparse Weights in Deep Learning. Proceedings\nof Machine Learning and Systems (2023).\n[45] Zhiqi Lin, Youshan Miao, Quanlu Zhang, Fan Yang, Yi Zhu, Cheng\nLi, Saeed Maleki, Xu Cao, Ning Shang, Yilei Yang, et al . 2024.\n{nnScaler}:{Constraint-Guided}Parallelization Plan Generation for\nDeep Learning Training. In 18th USENIX Symposium on Operating\nSystems Design and Implementation (OSDI 24) . 347â€“363.\n[46] Guodong Liu, Youshan Miao, Zhiqi Lin, Xiaoxiang Shi, Saeed Maleki,\nFan Yang, Yungang Bao, and Sa Wang. 2024. Aceso: Efficient Parallel\nDNN Training through Iterative Bottleneck Alleviation. InProceedings\nof the Nineteenth European Conference on Computer Systems . 163â€“181.\n[47] Xiaoxuan Liu, Lianmin Zheng, Dequan Wang, Yukuo Cen, Weize Chen,\nXu Han, Jianfei Chen, Zhiyuan Liu, Jie Tang, Joey Gonzalez, et al .\n2022. Gact: Activation compressed training for generic network ar-\nchitectures. In International Conference on Machine Learning . PMLR,\n14139â€“14152.\n[48] Guandong Lu, Runzhe Chen, Yakai Wang, Yangjie Zhou, Rui Zhang,\nZheng Hu, Yanming Miao, Zhifang Cai, Li Li, Jingwen Leng, and\nMinyi Guo. 2023. DistSim: A performance model of large-scale hybrid\ndistributed DNN training. In Proceedings of the 20th ACM International\nConference on Computing Frontiers .\n[49] Wenyan Lu, Guihai Yan, Jiajun Li, Shijun Gong, Yinhe Han, and Xi-\naowei Li. 2017. Flexflow: A flexible dataflow accelerator architecture for\nconvolutional neural networks. In 2017 IEEE International Symposium\non High Performance Computer Architecture (HPCA) . IEEE, 553â€“564.\n[50] meta llama. 2024. llama3.1.https://ai.meta.com/blog/meta-llama-3-1/ .\n[51] meta llama/llama3. 2024. llama/MODEL_CARD.md. https://github.\ncom/meta-llama/models/llama3_1/blob/main/MODEL_CARD.md.\n[52] meta llama/llama3. 2024. llama/MODEL_CARD.md. https://github.\ncom/meta-llama/models/llama3/blob/main/MODEL_CARD.md.\n[53] Xupeng Miao, Yujie Wang, Youhe Jiang, Chunan Shi, Xiaonan Nie,\nHailin Zhang, and Bin Cui. 2023. Galvatron: Efficient Transformer\nTraining over Multiple GPUs Using Automatic Parallelism.Proc. VLDB\nEndow. 16, 3 (2023), 470â€“479. https://doi.org/10.14778/3570690.3570697\n[54] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory F. Diamos,\nErich Elsen, David GarcÃ­a, Boris Ginsburg, Michael Houston, Oleksii\nKuchaiev, Ganesh Venkatesh, and Hao Wu. 2018. Mixed Precision\nTraining. In 6th International Conference on Learning Representations,\nICLR.\n[55] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri,\nNikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei\nZaharia. 2019. PipeDream: Generalized pipeline parallelism for DNN\ntraining. In Proceedings of the 27th ACM Symposium on Operating\nSystems Principles . 1â€“15.\n[56] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and\nMatei Zaharia. 2021. Memory-efficient pipeline-parallel dnn training.\nIn International Conference on Machine Learning . PMLR, 7937â€“7947.\n[57] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGres-\nley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi\nKashinkunti, Julie Bernauer, Bryan Catanzaro, et al. 2021. Efficient\nlarge-scale language model training on gpu clusters using megatron-\nlm. In Proceedings of the International Conference for High Performance\nComputing, Networking, Storage and Analysis . 1â€“15.\n[58] NVIDIA. [n. d.]. NVIDIA A100 GPUs. https://www.nvidia.com/en-\nus/data-center/a100/.\n[59] NVIDIA. 2023. NVIDIA L4 GPUs. https://www.nvidia.com/en-us/data-\ncenter/l4/.\n[60] openai. 2022. ChatGPT. https://openai.com/chatgpt/.\n[61] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James\nBradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia\nGimelshein, Luca Antiga, et al . 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. Advances in neural informa-\ntion processing systems 32 (2019).\n[62] Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze\nLiu, Yifan Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, et al. 2023. Fp8-\nlm: Training fp8 large language models.arXiv preprint arXiv:2310.18313\n(2023).\n[63] Xuan Peng, Xuanhua Shi, Hulin Dai, Hai Jin, Weiliang Ma, Qian Xiong,\nFan Yang, and Xuehai Qian. 2020. Capuchin: Tensor-based gpu memory\nmanagement for deep learning. In Proceedings of the Twenty-Fifth\nInternational Conference on Architectural Support for Programming\nLanguages and Operating Systems . 891â€“905.\n[64] Penghui Qi, Xinyi Wan, Guangxing Huang, and Min Lin. 2023. Zero\nBubble Pipeline Parallelism. arXiv preprint arXiv:2401.10241 (2023).\n[65] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang,\nReza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yux-\niong He. 2022. Deepspeed-moe: Advancing mixture-of-experts infer-\nence and training to power next-generation ai scale. In International\nconference on machine learning . PMLR, 18332â€“18346.\n[66] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.\n2020. Zero: Memory optimizations toward training trillion param-\neter models. In SC20: International Conference for High Performance\nComputing, Networking, Storage and Analysis . IEEE, 1â€“16.\n[67] Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith,\nand Yuxiong He. 2021. Zero-infinity: Breaking the gpu memory wall\nfor extreme scale deep learning. In Proceedings of the International\nConference for High Performance Computing, Networking, Storage and\nAnalysis. 1â€“14.\nMist: Efficient Distributed Training of Large Language Models via Memory-Parallelism ... EuroSys â€™25, March 30-April 3, 2025, Rotterdam, Netherlands\n[68] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He.\n2020. DeepSpeed: System Optimizations Enable Training Deep Learn-\ning Models with Over 100 Billion Parameters. In Proceedings of the\n26th ACM SIGKDD International Conference on Knowledge Discovery &\nData Mining .\n[69] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji\nRuwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He.\n2021. {ZeRO-Offload}: Democratizing {Billion-Scale}model train-\ning. In 2021 USENIX Annual Technical Conference (USENIX ATC 21) .\n551â€“564.\n[70] Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar, and\nStephen W Keckler. 2016. vDNN: Virtualized deep neural networks\nfor scalable, memory-efficient neural network design. In 2016 49th An-\nnual IEEE/ACM International Symposium on Microarchitecture (MICRO) .\nIEEE, 1â€“13.\n[71] Keshav Santhanam, Siddharth Krishna, Ryota Tomioka, Andrew\nFitzgibbon, and Tim Harris. 2021. Distir: An intermediate representa-\ntion for optimizing distributed neural networks. In Proceedings of the\n1st Workshop on Machine Learning and Systems .\n[72] Noam Shazeer. 2020. Glu variants improve transformer. arXiv preprint\narXiv:2002.05202 (2020).\n[73] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley,\nJared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-\nbillion parameter language models using model parallelism. arXiv\npreprint arXiv:1909.08053 (2019).\n[74] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy\nand policy considerations for deep learning in NLP. arXiv preprint\narXiv:1906.02243 (2019).\n[75] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yun-\nfeng Liu. 2024. Roformer: Enhanced transformer with rotary position\nembedding. Neurocomputing 568 (2024), 127063.\n[76] Zhenbo Sun, Huanqi Cao, Yuanwei Wang, Guanyu Feng, Shengqi\nChen, Haojie Wang, and Wenguang Chen. 2024. AdaPipe: Optimizing\nPipeline Parallelism with Adaptive Recomputation and Partitioning. In\nProceedings of the 29th ACM International Conference on Architectural\nSupport for Programming Languages and Operating Systems, Volume 3 .\n86â€“100.\n[77] Jakub Tarnawski, Deepak Narayanan, and Amar Phanishayee. 2021.\nPiper: Multidimensional Planner for DNN Parallelization. In Neu-\nral Information Processing Systems . https://api.semanticscholar.org/\nCorpusID:244711821\n[78] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-\nAnne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric\nHambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971 (2023).\n[79] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Alma-\nhairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal\nBhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and\nfine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).\n[80] Colin Unger, Zhihao Jia, Wei Wu, Sina Lin, Mandeep Baines, Carlos\nEfrain Quintero Narvaez, Vinay Ramakrishnaiah, Nirmal Prajapati, Pat\nMcCormick, Jamaludin Mohd-Yusof, et al. 2022. Unity: Accelerating\n{DNN}training through joint optimization of algebraic transforma-\ntions and parallelization. In 16th USENIX Symposium on Operating\nSystems Design and Implementation (OSDI 22) . 267â€“284.\n[81] Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuai-\nwen Leon Song, Zenglin Xu, and Tim Kraska. 2018. Superneurons: Dy-\nnamic GPU memory management for training deep neural networks.\nIn Proceedings of the 23rd ACM SIGPLAN symposium on principles and\npractice of parallel programming . 41â€“53.\n[82] Minjie Wang, Chien-chin Huang, and Jinyang Li. 2019. Supporting\nvery large models using automatic dataflow graph partitioning. In\nProceedings of the Fourteenth EuroSys Conference 2019 . 1â€“17.\n[83] Zhuang Wang, Xinyu Wu, Zhaozhuo Xu, and TS Ng. 2023. Cupcake:\nA Compression Scheduler for Scalable Communication-Efficient Dis-\ntributed Training. Proceedings of Machine Learning and Systems 5\n(2023).\n[84] Geoffrey X Yu, Yubo Gao, Pavel Golikov, and Gennady Pekhimenko.\n2021. Habitat: A {Runtime-Based}computational performance predic-\ntor for deep neural network training. In2021 USENIX Annual Technical\nConference (USENIX ATC 21) .\n[85] Tailing Yuan, Yuliang Liu, Xucheng Ye, Shenglong Zhang, Jianchao\nTan, Bin Chen, Chengru Song, and Di Zhang. 2024. Accelerating\nthe Training of Large Language Models using Efficient Activation\nRematerialization and Optimal Hybrid Parallelism. In 2024 USENIX\nAnnual Technical Conference (USENIX ATC 24) . USENIX Association,\nSanta Clara, CA, 545â€“561. https://www.usenix.org/conference/atc24/\npresentation/yuan\n[86] Biao Zhang and Rico Sennrich. 2019. Root mean square layer nor-\nmalization. Advances in Neural Information Processing Systems 32\n(2019).\n[87] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang,\nMin Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al.\n2023. Pytorch FSDP: experiences on scaling fully sharded data parallel.\narXiv preprint arXiv:2304.11277 (2023).\n[88] Bojian Zheng, Nandita Vijaykumar, and Gennady Pekhimenko. 2020.\nEcho: Compiler-based GPU memory footprint reduction for LSTM\nRNN training. In 2020 ACM/IEEE 47th Annual International Symposium\non Computer Architecture (ISCA) . IEEE, 1089â€“1102.\n[89] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng\nChen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo,\nEric P Xing, et al. 2022. Alpa: Automating inter-and {Intra-Operator}\nparallelism for distributed deep learning. In 16th USENIX Symposium\non Operating Systems Design and Implementation (OSDI 22) . 559â€“578.\nEuroSys â€™25, March 30-April 3, 2025, Rotterdam, Netherlands Z. Zhu, C. Giannoula, M. Andoorveedu, Q. Su, K. Mangalam, B. Zhang, G. Pekhimenko\nA Artifact Appendix\nA.1 Abstract\nMist is an automatic distributed training configuration op-\ntimizer designed to tune the optimal configuration for the\ncombination of parallelism strategies and memory footprint\nreduction techniques. This artifact includes the source code\nof the Mist prototype along with instructions for evaluating\nits functionality and reproducing key results.\nA.2 Description & Requirements\nA.2.1 How to access The Mist system is available at the\nfollowing repositories: https://github.com/dazz993/mist and\nZenodo (DOI: 10.5281/zenodo.14873554)\nA.2.2 Hardware dependencies Experiments are conducted\non up to four GCP L4 machines, each equipped with 8 Ã—\nNVIDIA L4 GPUs, and up to four AWS p4d.24xlarge ma-\nchines, each with 8Ã—NVIDIA A100 40GB GPUs. For artifact\nevaluation, an ideal testbed consists of a machine with 8 Ã—\nGPUs, each with approximately 24GB of memory, as we can\nprovide configurations suitable for direct evaluation on such\nhardware. Unless explicitly stated otherwise, the following\nevaluation workflow assumes this testbed. A general evalua-\ntion methodology for other GPUs and multi-node evaluation\nis also provided in our repository.\nA.2.3 Software dependencies We provide a Docker im-\nage with NVIDIA GPU support for this artifact. The software\nenvironment includes CUDA 12.1, PyTorch v2.1.1, Megatron-\nLM (git-hash 38879f8), DeepSpeed (v0.12.6), and NCCL v2.18.6.\nA.2.4 Benchmarks None.\nA.3 Set-up\nTo install the artifact, users should clone the repository and\nbuild the Docker image. For users with GPUs other than L4\nGPUs (sm_89), the environment variableTORCH_CUDA_ARCH_LIST\nin the Dockerfile may require modification.\n1 git clone https://github.com/Dazz993/Mist.git\n2 cd Mist\n3 docker build -t mist -f Dockerfile .\nRun the Docker container, mounting the Mist repository\nhome to /workspace.\n1 docker run --gpus all -it --rm --privileged \\\n2 --ipc=host --shm-size=20G --ulimit memlock=-1 \\\n3 --name \"mist\" -v $(pwd):/workspace/ mist\n[Optional] To obtain stable results, especially on L4 machines,\nfix the GPU frequency accordingly:\n1 nvidia-smi -ac 6251,1050\nA.4 Evaluation workflow.\nWe provide scripts for reproducing single-node results. Multi-\nnode experiments require large-scale clusters and additional\nsetup and execution time, so end-to-end scripts are not pro-\nvided for artifact evaluation. However, instructions are avail-\nable in GitHub repository README.We recommend users\nto follow the README as it provides extra explana-\ntions.\nA.4.1 Major Claims\nâ€¢(C1): Mist achieves an average of 1.28Ã—(up to 1.73Ã—)\nspeedup compared to state-of-the-art manual system\nMegatron-LM. See Section 6.2 and Figure 11 and 12.\nThis claim is validated by E2.\nâ€¢(C2): Mist demonstrates efficient tuning speed even\nwith a large search space. See Section 16 and Figure 16.\nThis claim is validated by E3.\nA.4.2 Experiments\nâ€¢E1: Kick-the-Tries [10 human-minutes]. This experiment\nevaluates the functionalities of Mist on Large Language\nModel analysis, execution, and distributed training opti-\nmization. Detailed explanations and expected results are\nalso provided in the repository README.\n[Execution] Given a YAML configuration for running the\nGPT-3 1.3B model on two GPUs: test-small-base:\nâ€“ Run model performance analysis\n1 cd /workspace/benchmark/mist/analysis/\n2 python run.py --config-name test-small-base\nâ€“ Execute the model distributed training\n1 cd /workspace/benchmark/mist/exec/\n2 torchrun --nproc-per-node 2 \\\n3 benchmark_one_case.py \\\n4 --config-name test-small-base\nâ€“ Run model tuning (the hyperparameters in this config-\nuration file are specifically tuned for GCP L4 GPUs).\n1 cd /workspace/benchmark/mist/tune/\n2 python tune_one_case.py --config-name test-small-base \\\n3 +output_path=/workspace/benchmark/mist/tune/results/test-small-mist\nThen execute the optimized configuration:\n1 cd /workspace/benchmark/mist/exec/\n2 torchrun --nproc-per-node 2 \\\n3 benchmark_one_case.py \\\n4 --config-path /workspace/benchmark/mist/tune/results/ \\\n5 --config-name test-small-mist\n[Results] The executed commands output the analysis\nresults, execution time, and memory usage for the base\nconfiguration, as well as the execution time and memory\nusage for the optimized configurations.\nâ€¢E2: Run Single-Node Performance Evaluation [4 compute-\nhours]. This experiment evaluates the performance of\nMist on a single node for GPT and LLaMA models.\nMist: Efficient Distributed Training of Large Language Models via Memory-Parallelism ... EuroSys â€™25, March 30-April 3, 2025, Rotterdam, Netherlands\nFor the L4 machine, we provide pre-tuned configurations\nthat enable a quick assessment of Mistâ€™s speedup com-\npared to baseline models. Additionally, we provide a gen-\neral process for evaluating on a new cluster. For further\ndetails, refer to the GitHub repository README.\n[Execution]\nâ€“ Evaluate Mist performance. Results are summarized\nin /workspace/benchmark/mist/tuned_configs/l4-24gb/\ngpt/summary.json and corresponding LLaMA folder.\n1 cd /workspace/benchmark/mist/tuned_configs/\n2 bash run_single_node.sh\nâ€“ Evaluate Megatron-LM performance. Results are under\n/workspace/benchmark/megatron/results.\n1 cd /workspace/benchmark/megatron/\n2 bash scripts/tops/l4/gpt2/1_8xl4_node_1_pcie.sh\n3 bash scripts/tops/l4/llama/1_8xl4_node_1_pcie.sh\nâ€“ Evaluate DeepSpeed performance. Results are under\n/workspace/benchmark/deepspeed/results.\n1 cd /workspace/benchmark/deepspeed/\n2 bash scripts/tops/l4/gpt2/1_8xl4_node_1_pcie.sh\n3 bash scripts/tops/l4/llama/1_8xl4_node_1_pcie.sh\n[Results] We provide a python file to collect the results for\neasy comparison.\n1 cd /workspace/benchmark/\n2 python scripts/collect_single_node_results_v1.py\nIt provides tables with absolute throughput values or rel-\native throughput improvements. Here we provide an ex-\nample:\n1 +-----------------------+-------------+-------------+\n2 | SpeedUp | SpeedUp vs | SpeedUp vs |\n3 | | Megatron | DeepSpeed |\n4 +=======================+=============+=============+\n5 | gpt2-1.3b-flash_False | 1.175X | 1.418X |\n6 +-----------------------+-------------+-------------+\n7 | gpt2-2.7b-flash_False | 1.141X | 1.384X |\n8 +-----------------------+-------------+-------------+\n9 | gpt2-7.0b-flash_False | 1.222X | 2.053X |\n10 +-----------------------+-------------+-------------+\nâ€¢E3: Benchmarking Tuning Time [0.75 compute hours].\nThis experiment reproduces the tuning time results for\nMist, as shown in Figure 16.\nWe demonstrate how tuning time varies as the search\nspace expands incrementally. To evaluate this, we run a\nGPT-22B model on a 4 Ã— 8 GPU setup. Beyond examining\ntuning time, this experiment also provides insights into\nthe performance of large-scale distributed training, as\nwe can see the performance improvements when more\noptimizations are applied.\n[Execution]\n1 cd /workspace/benchmark/mist/benchmark-tuning-time\n2 python run.py --model=gpt2/22b -n 4 -m 8\nThe results are shown in /workspace/benchmark/mist/\nbenchmark-tuning-time/results/.../summary.json.\nA.5 Notes on Reusability\nMist provides a symbolic execution system that represents all\ntensors with symbolic dimensions. Additionally, it supports\ntracing, which generates a corresponding symbolic compu-\ntational graph. This feature can serve as an educational tool,\nhelping users understand shape propagation and how each\ninput dimension is utilized. Furthermore, it serves as a basis\nfor exploring performance estimation in future research.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8104687929153442
    },
    {
      "name": "Mist",
      "score": 0.7607574462890625
    },
    {
      "name": "Parallel computing",
      "score": 0.7027243375778198
    },
    {
      "name": "Parallelism (grammar)",
      "score": 0.6596417427062988
    },
    {
      "name": "Training (meteorology)",
      "score": 0.490054726600647
    },
    {
      "name": "Computer architecture",
      "score": 0.44184210896492004
    },
    {
      "name": "Distributed computing",
      "score": 0.35218721628189087
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Meteorology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I185261750",
      "name": "University of Toronto",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210127509",
      "name": "Vector Institute",
      "country": "CA"
    }
  ],
  "cited_by": 1
}