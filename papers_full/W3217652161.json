{
  "title": "Building astroBERT, a language model for Astronomy &amp; Astrophysics",
  "url": "https://openalex.org/W3217652161",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4223070328",
      "name": "Grezes, Felix",
      "affiliations": [
        "Adaptive Design Association"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3128285573"
  ],
  "abstract": "The existing search tools for exploring the NASA Astrophysics Data System (ADS) can be quite rich and empowering (e.g., similar and trending operators), but researchers are not yet allowed to fully leverage semantic search.<br> For example, a query for \"results from the Planck mission\" should be able to distinguish between all the various meanings of Planck (person, mission, constant, institutions and more) without further clarification from the user.<br> At ADS, we are applying modern machine learning and natural language processing techniques to our dataset of recent astronomy publications to train astroBERT, a deeply contextual language model based on research at Google.<br> Using astroBERT, we aim to enrich the ADS dataset and improve its discoverability, and in particular we are developing our own named entity recognition tool. We present here our preliminary results and lessons learned.",
  "full_text": "arXiv:2112.00590v1  [cs.CL]  1 Dec 2021\nBuilding astroBERT , a language model for Astronomy&\nAstrophysics\nFelix Grezes 1 , Sergi Blanco-Cuaresma, Alberto Accomazzi, Michael J. Kur tz,\nGolnaz Shapurian, Edwin Henneken, Carolyn S. Grant, Donna M . Thompson,\nRoman Chyla, Stephen McDonald, Timothy W . Hostetler,\nMatthew R. T empleton, Kelly E. Lockhart, Nemanja Martinovi c, Shinyi Chen,\nChris T anner, and Pavlos Protopapas.\n1Harvard-Smithsonian Center for Astrophysics, Cambridge, MA, USA;\nfelix.grezes@cfa.harvard.edu\nAbstract. The existing search tools for exploring the NASA Astrophysi cs Data Sys-\ntem (ADS) can be quite rich and empowering (e.g., similar and trending operators),\nbut researchers are not yet allowed to fully leverage semant ic search. For example,\na query for \"results from the Planck mission\" should be able t o distinguish between\nall the various meanings of Planck (person, mission, consta nt, institutions and more)\nwithout further clariﬁcation from the user. At ADS, we are ap plying modern machine\nlearning and natural language processing techniques to our dataset of recent astronomy\npublications to train astroBER T , a deeply contextual langu age model based on research\nat Google. Using astroBER T , we aim to enrich the ADS dataset a nd improve its discov-\nerability, and in particular we are developing our own named entity recognition tool.\nW e present here our preliminary results and lessons learned .\n1. Introduction\nThe NASA ADS ( https://ui.adsabs.harvard.edu) has been publicly servicing\nthe astrophysical research community since Kurtz et al. (19 93). It maintains a search-\nable bibliographic collection of more than 15 million recor ds. The database can be\nexplored via complex queries using ﬁlters (e.g. authors, ye ar range, text in title and /or\nbody , refereed or not) combined with Boolean operators to ex plore the citations and ref-\nerences, and more advanced operators such as paper similari ty , co-reads, and reviews.\nAs powerful as the existing tools are, they do not make use of r ecent advances\nin natural language processing and semantic search. Ideall y , if a user wanted to ﬁlter\nby papers that use data from the ESA Planck space observatory , a simple query of\n\"Planck mission\" should su ﬃce, without requiring manual input of speciﬁc keywords\nsuch as year:2009-2022 or NOT author:\"Planck, Max\". Such queries highlight\nthe inherent ambiguity of natural language and how remarkab ly ﬂexible humans are at\nunderstanding semantics from contextual clues. Currently , ADS handles many of the\nmost common ambiguities with carefully crafted rules (e.g. using SIMBAD objects).\nT o automate the process of identifying, disambiguating, an d tagging named en-\ntities within the ADS database, we are building astroBERT, a language model for\nastronomical content. This astroBERT is based on Google’s B ERT deep neural net-\nwork transformer architecture (Devlin et al. 2018), inspir ed by the AllenAI’s SciBERT\n1\n2 Grezes, and the ADS team\n(Beltagy et al. 2019) e ﬀorts, built using tools from Huggingface (W olf et al. 2019), and\nis trained is trained using a large collection of recent astr onomy ADS papers.\nOur preliminary results show that astroBERT outperforms BE RT on the named\nentity recognition task on ADS data. Based on this encouragi ng result, we can now\nwork on improving and integrating astroBERT into ADS servic es, and providing access\nto it for the astrophysics community .\n2. Related W ork: BERT and SciBERT\nDevlin et al. (2018) introduced the Bidirectional Encoder R epresentations from Trans-\nformers (BERT) language model, based on the Transformer tec hnology (V aswani et al.\n2017). It was trained with text from Wikipedia and the Brown C orpus, and then ﬁne-\ntuned to achieve state-of-the-art performance on 11 NLP tas ks, including sentiment\nanalysis, semantic role labeling, sentence classiﬁcation and word disambiguation.\nBuilding o ﬀthe success of BERT , Beltagy et al. (2019) created SciBERT , F o-\ncusing on scientiﬁc papers from the Semantic Scholar corpus , SciBERT is part of a\ngrowing list of models that adapt BERT to speciﬁc domains and tasks. Its state-of-the-\nart performance on a wide range of scientiﬁc domain NLP tasks motivates our work on\nastroBERT . The SciBERT paper shows that pre-training BERT w ith domain-speciﬁc\nlanguage data improves its performance when compared to the original BERT .\n3. T echnical Details\nThe work presented here was implemented using the open-sour ce python-based Hug-\ngingface (W olf et al. 2019) library , which provides easy acc ess to state-of-the-art NLP\nmodels, and tools. Runtimes are reported as run on a machine w ith two NVIDIA V100\nGPUs, two 12-core Intel Xeon Gold 6246 CPUs, and 768GB of RAM. Unless speciﬁed,\nall parameters were left to Huggingface v4.10.2 defaults .\n3.1. Data Preparation\nIn order to train our language model, our ﬁrst task was to buil d a large dataset of clean\nEnglish language text from astronomy papers. From the ADS da tabase, we selected\npapers for which the source was provided by the publishers in XML format. The ﬁnal\ndataset is made up of 395,499 documents (16GB on disk). Per best practices, this was\nevenly split into training, evaluation and testing dataset s. Despite our e ﬀorts to select\nthe best quality data sources, we required to execute a clean -up to remove unprintable\nunicode characters and rare unicode characters (those appe aring 50 times or less). W e\nalso replaced accented characters by the non-accented coun terparts when possible. In\nfuture re-iterations we will consider a more exhaustive cle aning to ﬁlter out meaningless\nstrings representing data tables, mathematical formulas, and appendices of raw data.\n3.2. T okenizer\nAn important part of the text processing pipeline is the toke nizer, which converts text\ndata into inputs compatible with the model T o stick as close a s possible to the original\nBERT design choices. we trained a W ordPiece tokenizer maxim um vocabulary size of\n30,000 tokens. Due to the extensive use of accronyms in the as trophysics literature, and\nBuilding astroBERT 3\nits tendency to correspond to English words (e.g., AURA, BLA ST , CLUSTER, INTE-\nGRAL, WISE), we decided to keep the case information in our mo del. the tokenizer\ndid not automatically convert every character to the same ca se, however accents were\nremoved to simplify downstream tasks. Training the tokeniz er over the entire data took\napproximately 45 minutes . The astroBERT vocabulary overlaps with that of BERT\nand SciBERT by 24.5% and 35.3% respectively (over total number of tokens), while\nBERT and SciBERT overlapped by 27.0%. The total size of the da ta corresponds to\n3,819,322,591 tokens, or 2,977,635,680 words and 121,207,934 sentences w hen parsed\nby NL TK (Bird et al. 2009) .\n3.3. The MLM, NSP and NER T asks\nFollowing the methodology of the BERT paper, astroBERT is pr e-trained on two tasks\nthat do not require hand labeled data: ﬁrst the masked langua ge model (MLM) task in\nwhich the model predicts tokens that were randomly masked by using the contextual\ntext; and second MLM jointly with the next sentence predicti on (NSP) task in which\nthe model predicts if a sentence is subsequent to another in t he original document. The\noriginal random masking and shu ﬄing probabilities were kept unchanged (imitating\nthe original BERT setup).\nFollowing astroBERT pre-training, we transferred the mode ls to our ﬁrst down-\nstream task: named entity recognition (NER), in which the mo del tries to identify or-\nganizations. Using a list of 908 astronomy organization acr onyms and their full names,\nwe scanned the acknowledgement sections of 44K astronomy ma nuscripts and built a\ndataset of 1856 sentences with 6279 annotated organization s. The models were evalu-\nated using 10-fold cross-validation.\n4. T raining astroBERT and Preliminary Results\nW e trained astroBERT(MLM) for 15 epochs on the MLM task, and a stroBERT(NSP\n+MLM) for 3 epochs on the joint NSP and MLM tasks. astroBERT(ML M) was ini-\ntialized with BERT weights, and astroBERT(NSP +MLM) with the trained astroBERT\n(MLM). Training astroBERT takes approximately 8 hours per epoch for the MLM\ntask, and 22 hours for the joint NSP +MLM task. Training was done using mixed ﬂoat-\ning point precision, which has been shown to reduce memory co nsumption and lead\nto faster learning (Micikevicius et al. 2017). The NER model s were ﬁne-tuned for 3\nepochs, each epoch taking approximately 90 seconds.\nIn table 1 we report perplexity scores our astroBERT , BERT , a nd SciBERT mod-\nels on our cleaned ADS dataset, as well across various standa rd English texts dataset\nprovided by Huggingface. Because these models use di ﬀerent vocabularies, scores\nshould be compared within model and across datasets, and not across models. These\nscores show that BERT , which is trained on Wikipedia English , can be re-trained into\nastroBERT to performs best on ADS text. SciBERT , which was tr ained on data from\nSemantic Scholar (not tested here) performs similarly on al l 3 scientiﬁc text datasets.\nAs expected, adding the NSP task to astroBERT(NSP +MLM) training causes it to per-\nform worse in terms of perplexity on the MLM task.\nHowever as shown in table 2, in which we report the average f1- scores and stan-\ndard deviations across cross-validation runs for BERT and a stroBERT after ﬁne-tuning,\nastroBERT(NSP+MLM) slightly outperforms astroBERT(MLM) on the NER task, a nd\nboth outperform the original BERT model.\n4 Grezes, and the ADS team\nT able 1. Comparisons of perplexity scores across BER T model s\nBER T SciBER T astroBER T astroBER T\n(MLM) (NSP +MLM)\nADS 38.4 4.05 4.16 5.71\nwikitext-2-raw-v1 29.7 8.64 18.5 25.9\nptb_text_only 82.8 9.63 20.0 27.4\nscientiﬁc_papers (arxiv) 41.3 4.23 4.82 6.87\nscientiﬁc_papers (pubmed) 34.0 3.96 8.41 12.6\nT able 2. Comparisons of NER scores across BER T models on ADS d ata\nBER T astroBER T astroBER T\n(MLM) (NSP +MLM)\nf1-score 0.859 0.893 0.902\nstandard deviation 0.014 0.028 0.014\n5. Future W ork\nWith astroBERT showing encouraging results on the prelimin ary ADS data, we are\ncurrently building a more complete NER dataset covering mor e named entities and\npapers. Our goal is to extend our services to automatically i dentify entities such as\nfacilities, and allow ADS users to unambiguously ﬁnd papers that mention them.\nBeyond improving ADS services, astroBERT will be made publi cly available to\nthe astrophysics community through the Huggingface Model H ub , which provides free\naccess to many state-of-the-art language models.\nReferences\nBeltagy, I., Lo, K., & Cohan, A. 2019, arXiv e-prints. 1903.10676\nBird, S., Klein, E., & Loper, E. 2009, Natural language proce ssing with Python: analyzing text\nwith the natural language toolkit (\" O’Reilly Media, Inc.\")\nDevlin, J., Chang, M.-W ., Lee, K., & T outanova, K. 2018, arXi v e-prints. 1810.04805\nKurtz, M. J., Karakashian, T ., Grant, C. S., Eichhorn, G., Mu rray, S. S., W atson, J. M., Ossorio,\nP . G., & Stoner, J. L. 1993, in Astronomical Data Analysis Sof tware and Systems II,\nedited by R. J. Hanisch, R. J. V . Brissenden, & J. Barnes, vol. 52 of Astronomical\nSociety of the Paciﬁc Conference Series, 132\nMicikevicius, P ., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Hous-\nton, M., Kuchaiev, O., V enkatesh, G., & Wu, H. 2017, arXiv e-p rints. 1710.03740\nV aswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., &\nPolosukhin, I. 2017, in Advances in neural information proc essing systems, 5998\nW olf, T ., Debut, L., Sanh, V ., Chaumond, J., Delangue, C., Mo i, A., Cistac, P ., Rault, T ., Louf,\nR., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P . , Ma, C., Jernite, Y ., Plu, J.,\nXu, C., Le Scao, T ., Gugger, S., Drame, M., Lhoest, Q., & Rush, A. M. 2019, arXiv\ne-prints. 1910.03771",
  "topic": "Discoverability",
  "concepts": [
    {
      "name": "Discoverability",
      "score": 0.8630266189575195
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.703149676322937
    },
    {
      "name": "Computer science",
      "score": 0.5964016914367676
    },
    {
      "name": "Planck",
      "score": 0.47156378626823425
    },
    {
      "name": "Information retrieval",
      "score": 0.39820143580436707
    },
    {
      "name": "World Wide Web",
      "score": 0.33805516362190247
    },
    {
      "name": "Astronomy",
      "score": 0.2770090103149414
    },
    {
      "name": "Physics",
      "score": 0.21272048354148865
    },
    {
      "name": "Artificial intelligence",
      "score": 0.19782662391662598
    }
  ]
}