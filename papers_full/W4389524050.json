{
  "title": "Large Language Models are Temporal and Causal Reasoners for Video Question Answering",
  "url": "https://openalex.org/W4389524050",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3210892978",
      "name": "Dohwan Ko",
      "affiliations": [
        "Korea University"
      ]
    },
    {
      "id": "https://openalex.org/A2152348793",
      "name": "Ji Lee",
      "affiliations": [
        "Korea University"
      ]
    },
    {
      "id": "https://openalex.org/A2114596947",
      "name": "Woo Young Kang",
      "affiliations": [
        "Kao Corporation (Japan)",
        "Brain (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2514455387",
      "name": "Byungseok Roh",
      "affiliations": [
        "Kao Corporation (Japan)",
        "Brain (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2239728134",
      "name": "Hyunwoo Kim",
      "affiliations": [
        "Korea University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4229005866",
    "https://openalex.org/W2765716052",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4313447121",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2964220823",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W4312480274",
    "https://openalex.org/W4385572738",
    "https://openalex.org/W3197457832",
    "https://openalex.org/W3206166878",
    "https://openalex.org/W2998166190",
    "https://openalex.org/W4385571357",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4361229539",
    "https://openalex.org/W4295101003",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W4318718936",
    "https://openalex.org/W4363671827",
    "https://openalex.org/W4313071966",
    "https://openalex.org/W4385571894",
    "https://openalex.org/W4287125738",
    "https://openalex.org/W3106534186",
    "https://openalex.org/W3213562172",
    "https://openalex.org/W4283066680",
    "https://openalex.org/W4386076661",
    "https://openalex.org/W4376167553",
    "https://openalex.org/W4386043768",
    "https://openalex.org/W4389519587",
    "https://openalex.org/W4385569780",
    "https://openalex.org/W2963541336",
    "https://openalex.org/W4386076010",
    "https://openalex.org/W4385571189",
    "https://openalex.org/W3175961224",
    "https://openalex.org/W4380135847",
    "https://openalex.org/W4312246181",
    "https://openalex.org/W4386071468",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W4385567227",
    "https://openalex.org/W3196930868",
    "https://openalex.org/W4382461806",
    "https://openalex.org/W4378465237",
    "https://openalex.org/W2955124656",
    "https://openalex.org/W4385570088",
    "https://openalex.org/W4385570446",
    "https://openalex.org/W4361229284",
    "https://openalex.org/W3174564426",
    "https://openalex.org/W3177934633",
    "https://openalex.org/W4323927057",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W4376312579",
    "https://openalex.org/W4385572645",
    "https://openalex.org/W4385570982",
    "https://openalex.org/W4385570140",
    "https://openalex.org/W4289421997",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4385571219",
    "https://openalex.org/W4365460762",
    "https://openalex.org/W3211777899",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W2606982687",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W4310921506",
    "https://openalex.org/W4280631063",
    "https://openalex.org/W4367692219"
  ],
  "abstract": "Large Language Models (LLMs) have shown remarkable performances on a wide range of natural language understanding and generation tasks. We observe that the LLMs provide effective priors in exploiting linguistic shortcuts for temporal and causal reasoning in Video Question Answering (VideoQA). However, such priors often cause suboptimal results on VideoQA by leading the model to over-rely on questions, i.e., linguistic bias, while ignoring visual content. This is also known as ‘ungrounded guesses’ or ‘hallucinations’. To address this problem while leveraging LLMs’ prior on VideoQA, we propose a novel framework, Flipped-VQA, encouraging the model to predict all the combinations of ⟨V, Q, A⟩ triplet by flipping the source pair and the target label to understand their complex relationships, i.e., predict A, Q, and V given a VQ, VA, and QA pairs, respectively. In this paper, we develop LLaMA-VQA by applying Flipped-VQA to LLaMA, and it outperforms both LLMs-based and non-LLMs-based models on five challenging VideoQA benchmarks. Furthermore, our Flipped-VQA is a general framework that is applicable to various LLMs (OPT and GPT-J) and consistently improves their performances. We empirically demonstrate that Flipped-VQA not only enhances the exploitation of linguistic shortcuts but also mitigates the linguistic bias, which causes incorrect answers over-relying on the question. Code is available at https://github.com/mlvlab/Flipped-VQA.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4300–4316\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nLarge Language Models are Temporal and Causal Reasoners\nfor Video Question Answering\nDohwan Ko1∗ Ji Soo Lee1∗ Wooyoung Kang2 Byungseok Roh2 Hyunwoo J. Kim1†\n1Department of Computer Science and Engineering, Korea University 2Kakao Brain\n{ikodoh, simplewhite9, hyunwoojkim}@korea.ac.kr\n{edwin.kang, peter.roh}@kakaobrain.com\nAbstract\nLarge Language Models (LLMs) have shown\nremarkable performances on a wide range of\nnatural language understanding and genera-\ntion tasks. We observe that the LLMs pro-\nvide effective priors in exploiting linguistic\nshortcuts for temporal and causal reasoning in\nVideo Question Answering (VideoQA). How-\never, such priors often cause suboptimal results\non VideoQA by leading the model to over-\nrely on questions, i.e., linguistic bias, while\nignoring visual content. This is also known as\n‘ungrounded guesses’ or ‘hallucinations’. To\naddress this problem while leveraging LLMs’\nprior on VideoQA, we propose a novel frame-\nwork, Flipped-VQA, encouraging the model to\npredict all the combinations of ⟨V , Q, A⟩triplet\nby flipping the source pair and the target label\nto understand their complex relationships, i.e.,\npredict A, Q, and V given a VQ, V A, and QA\npairs, respectively. In this paper, we develop\nLLaMA-VQA by applying Flipped-VQA to\nLLaMA, and it outperforms both LLMs-based\nand non-LLMs-based models on five challeng-\ning VideoQA benchmarks. Furthermore, our\nFlipped-VQA is a general framework that is\napplicable to various LLMs (OPT and GPT-J)\nand consistently improves their performances.\nWe empirically demonstrate that Flipped-VQA\nnot only enhances the exploitation of linguistic\nshortcuts but also mitigates the linguistic bias,\nwhich causes incorrect answers over-relying\non the question. Code is available at https:\n//github.com/mlvlab/Flipped-VQA.\n1 Introduction\nLarge Language Models (LLMs) have exhibited\nan impressive ability to free-form generation tasks\nand multi-choice question-answering tasks in natu-\nral language processing (Chung et al., 2023; Jiang\net al., 2023; Fei et al., 2023; Cai et al., 2022; Chen\net al., 2023; Saha et al., 2022). These LLMs have\n∗Equal contribution.\n†Corresponding author.\n(a)\n (b)\nFigure 1: LLMs’ temporal and causal reasoning abil-\nity. (a) An example of a causal question that LLMs\ncorrectly answer without visual content. (b) Compari-\nson of LLaMA 33B (QA) and OPT 125M (VQA).\nachieved human-level performance on a wide range\nof challenging tasks like professional & academic\nQA (Hendrycks et al., 2021), science QA (Clark\net al., 2018), mathematics QA (Cobbe et al., 2021),\ncode generation (Chen et al., 2021), and common-\nsense reasoning (Zellers et al., 2019; Sakaguchi\net al., 2021) since they are pretrained with large-\nscale corpora (e.g., CommonCrawl, Bookcorpus,\nand Wikipedia) which entail massive human knowl-\nedge. With such pretraining data, usually compris-\ning a series of contexts, LLMs are trained to predict\nthe next token given the preceding tokens. There-\nfore, LLMs learn to predict the next context given\na series of contexts during pretraining, so they im-\nplicitly learn temporal and causal reasoning ability.\nTo assess LLMs’ temporal and causal reason-\ning ability, we explore a popular multi-modal\nunderstanding task, Video Question Answering\n(VideoQA), which requires the model to predict the\ncorrect answer (A) given a video (V) and question\n(Q) pair. Recent challenging VideoQA benchmarks\ndemand the model to answer the question which\nasks temporal and causal relationships, e.g., the\nnext event of a video or the reason why a scene\nhappens. We observe that LLMs effectively handle\n4300\nsuch challenging VideoQA benchmarks by lever-\naging their strong prior knowledge of temporal and\ncausal reasoning learned from the pretraining phase.\nFor example, in Fig. 1a, LLMs correctly answer\ncausal questions solely based on the text question\nand options without referring to the visual content\nby exploiting linguistic shortcut. Also, Fig. 1b\nshows that a language-only QA model equipped\nwith a larger language model, e.g., LLaMA 33B,\ndenoted by QA 33B outperforms a VideoQA model\nwith OPT 125M trained with full ⟨V , Q, A⟩on\ncausal and temporal questions by a large margin of\n13%. Although LLMs’ prior knowledge is effective\nfor addressing complex temporal and causal ques-\ntions, this sometimes leads to suboptimal answers\nwhen the model overly depends on inaccurate lin-\nguistic prior, i.e., linguistic bias, while ignoring\nthe visual content. This is known as the ‘hallucina-\ntion problem’ of visual question-answering models\nequipped with LLMs (Alayrac et al., 2022). Al-\nthough in the literature linguistic shortcut and lin-\nguistic bias are interchangeably used, in this paper,\nwe use the former specifically when the linguistic\nprior is correct and the latter otherwise.\nHere, we propose a novel learning framework,\nFlipped-VQA, predicting all the combinations of\n⟨V , Q, A⟩triplet by flipping the source pair and\nthe target label, i.e., VQ →A (main task), V A\n→Q, and QA →V (auxiliary tasks). In other\nwords, to understand complex relationships be-\ntween the video, question, and answer, LLMs are\nasked to additionally predict the question given a\nvideo-answer pair and the video given a question-\nanswer pair by leveraging their knowledge of tem-\nporal and causal reasoning. In our experiments, we\ndevelop LLaMA-VQA by applying Flipped-VQA\nto LLaMA (Touvron et al., 2023) and it outper-\nforms other baselines on five challenging VideoQA\nbenchmark datasets: NExT-QA (Xiao et al., 2021),\nSTAR (Wu et al., 2021), DramaQA (Choi et al.,\n2021), VLEP (Lei et al., 2020), and TVQA (Lei\net al., 2018) with a small number of learnable\nparameters (only 0.06% of total model parame-\nters). Furthermore, Flipped-VQA improves the\nperformance of GPT-J (Wang and Komatsuzaki,\n2021) and OPT (Zhang et al., 2022), implying\nthat our framework is generally applicable to other\ndecoder-only LLMs. We empirically demonstrate\nthat Flipped-VQA encourages LLMs to exploit lin-\nguistic shortcuts by leveraging their prior knowl-\nedge and also mitigates linguistic bias which causes\nincorrect answer over-relying on the question.\nTo sum up, our contributions are as follows:\n• We investigate that pretrained LLMs’ knowl-\nedge is a strong prior for temporal and causal\nreasoning on challenging VideoQA.\n• We propose a novel framework, Flipped-VQA,\nto efficiently fine-tune LLMs on VideoQA by\nreasoning and understanding the complex re-\nlationships of ⟨V , Q, A⟩triplet, using LLMs’\nprior knowledge of temporal and causal rea-\nsoning. Flipped-VQA requires LLMs to per-\nform three tasks: VQ →A, V A→Q, and\nQA →V , and we combine these objectives as\nLLMs’ language generation objective.\n• LLaMA-VQA trained by Flipped-VQA, out-\nperforms the baselines on five challenging\nVideoQA benchmark datasets. Also, our ex-\nperiments demonstrate that Flipped-VQA is\ngenerally applicable to various decoder-only\nLLMs and consistently improves their perfor-\nmances.\n• Our extensive analyses show that Flipped-\nVQA is effective in exploiting linguistic short-\ncuts to answer the question based on LLMs’\nprior knowledge and alleviating the linguis-\ntic bias by increasing the utilization of visual\ncontents.\n2 Related works\nLLMs for temporal and causal reasoning. Ex-\nposed to a wide range of corpora during pretrain-\ning, LLMs perform diverse reasoning tasks (Liu\net al., 2023; Wang et al., 2023c; Ozturkler et al.,\n2023; Ho et al., 2022; Li et al., 2022; Wang et al.,\n2023c; Kıcıman et al., 2023). Particularly, a line\nof works (Zhang et al., 2023a; Li et al., 2023e; Ka-\nmalloo et al., 2023; Wang et al., 2023b; Tan et al.,\n2023) focuses on the temporal and causal reason-\ning skills of LLMs. For temporal reasoning, Zhang\nand Choi (2021) assesses LLMs by asking open-\ndomain time-sensitive questions in both closed and\nopen settings. Similarly, Hobbhahn et al. (2022)\ninvestigates the ability through querying subject\nand relation pairs of different time periods. Further-\nmore, some works have also explored the causal\ncapabilities by evaluating whether LLMs can under-\nstand the causal implications given in the sentence.\nLong et al. (2023) uses simple causal graphs and de-\ntermines if LLMs can understand the relationship\n4301\nFigure 2: Illustration of LLMs with Flipped-VQA. Flipped-VQA consists of three objectives: Lvqa, Lvaq, and Lqav.\nLvqa is a common objective, which predicts the answer given a video-question pair, for VideoQA. Likewise,Lvaq and\nLqav are the objectives for question and video prediction by leveraging LLMs’ knowledge. In other words, for each\nobjective, VQ, V A, and QA pair is used as prefix tokens to predict A, Q, and V , respectively. Trainable parameters\ninterleaved in LLMs stand for adapter tokens as in LLaMA-Adapter. Our framework employs only a relatively small\nnumber of trainable parameters on LLMs, e.g., 4.5M trainable parameters among the total parameters of LLaMA\n7B (0.06%).\nbetween nodes. In this work, we further examine\nthe temporal and causal reasoning skills of LLMs\nexpanded to the multi-modal setting of challenging\nVideoQA.\nLLMs for multi-modal understanding. Vari-\nous lines of work attempt to incorporate different\nmodalities into LLMs to leverage the models’ gen-\neration power and knowledge in performing multi-\nmodal downstream tasks. There exist various tech-\nniques (Hu et al., 2021; Jia et al., 2022; Ju et al.,\n2022) in the literature to bridge the gap between dif-\nferent modalities. For instance, Flamingo (Alayrac\net al., 2022) ingests visual content into the frozen\nChinchilla (Hoffmann et al., 2022) through a Per-\nceiver Resampler. LLaMA-Adapter (Zhang et al.,\n2023c) fine-tunes LLaMA by applying linear pro-\njection along with the adaption of prompts to in-\ncorporate the visual information. Recently, there\nhave been several approaches to develop a LLMs-\nbased video chat model trained on massive multi-\nmodal instruction tuning dataset that enables com-\nprehensive understanding across different modali-\nties (Zhang et al., 2023b; Li et al., 2023b,d). Specif-\nically, VideoChat (Zhang et al., 2023b) proposes\nvideo-centric instruction dataset that primarily em-\nphasizes spatio-temporal reasoning and causal rela-\ntionships present in the video.\nVideo Question Answering (VideoQA).VideoQA\naims to answer natural language questions given\na video that requires multi-modal understanding\nand reasoning skills on different semantic levels.\nPrevious VideoQA benchmarks (Xu et al., 2017;\nJang et al., 2017) target short videos and ask ques-\ntions based on visual facts such as location and\nobjects/attributes. In contrast, more recent bench-\nmarks (Xiao et al., 2021; Lei et al., 2020, 2018;\nWu et al., 2021; Choi et al., 2021) tend to tackle\ntemporal and causal questions referencing a longer\nvideo. Specifically, NExT-QA (Xiao et al., 2021)\nrequires uncovering the cause/intention of a cer-\ntain event (e.g., Why did ... ) or reasoning about\nsubsequent actions in the video (e.g., What/How ...\ndo/react after ...)1. In this work, we address these\nchallenging VideoQA benchmarks through LLMs’\ntemporal and causal reasoning abilities.\n3 Method\nWe present Flipped-VQA, a simple yet effec-\ntive framework for Video Question Answering\n(VideoQA) that leverages the LLMs’ prior knowl-\n1Further details with examples of NExT-QA are provided\nin Sec. D.\n4302\nedge of temporal and causal reasoning. In addi-\ntion to the target task of predicting an answer A\ngiven a video V and a question Q ( i.e., VQ →\nA), our framework flips the role of inputs and\noutputs, requiring the model to predict V given\nQA and Q given V A. We apply our framework\nto LLaMA (Touvron et al., 2023) and develop\nLLaMA-VQA but it is applicable to any decoder-\nonly LLMs. In this section, we first describe the\noverall architecture of LLaMA-VQA and then in-\ntroduce our objective. The overall architecture is\nillustrated in Fig. 2.\n3.1 LLaMA-VQA\nLLaMA-VQA is built on LLaMA with a few addi-\ntional learnable parameters. First, LLaMA-VQA\nadopts a learnable linear layer f to project the vi-\nsual embeddings, extracted from the frozen visual\nencoder CLIP ViT/L14 (Radford et al., 2021), to\nLLaMA’s text token embedding space, see Fig. 2.\nSpecifically, given a raw video xv, a sequence of\nvisual tokens is calculated as v = [v1,...,v Nv ] =\nf(CLIP(xv)) ∈RNv×D, where Nv is the number\nof video frames and Dis a feature dimension. Sec-\nond, as in LLaMA-Adapter (Zhang et al., 2023c),\nwe additionally adopt several trainable adapter\ntokens p = [p1,...,p Np ] which are prepended\nto the key and value of each self-attention layer,\nwhere Np is the number of adapter tokens. Fur-\nther descriptions of LLaMA-Adapter are provided\nin Sec. C. So the number of trainable parameters\nof LLaMA-VQA 7B is 4.5M, only 0.06% of to-\ntal parameters of LLaMA 7B. With such a few\ntrainable parameters, LLaMA-VQA effectively pre-\nserves LLMs’ prior knowledge and leverages it in\nexploiting linguistic shortcuts for VideoQA.\nThe question q = [q1,...,q Nq ] ∈RNq×D and\nanswer a = [a1,...,a Na ] ∈RNa×D tokens are ex-\ntracted from raw question xq and answer xa texts\nby a tokenizer, where Nq and Na are the numbers\nof question and answer tokens respectively. The\ninput prompt with visual tokens for LLaMA-VQA\nis provided in Tab. 1, wherev and q serve as prefix\ntokens, see Sec. B for further prompt details. For\nsimplicity, we omit the tokens for the prompt tem-\nplate (e.g., ‘Video:’ and ‘ Question:’) and only\nconsider content tokens (e.g., ‘<v1>’ and ‘<ques-\ntion>’) in our equations. Note that q ∈RNq×D rep-\nresents only the question tokens and choice tokens\nare omitted in following notations. Then, token\nsequences v, q, and a are concatenated and fed to\n[SOS] Video: <v1> <v2> ··· <vNv >\nQuestion: <question>\nChoices:\n(A) <option 1>\n(B) <option 2>\n(C) <option 3>\n(D) <option 4>\n(E) <option 5>\nAnswer: The answer is <answer> [EOS]\nTable 1: Input Prompt of LLaMA-VQA.\nLLaMA, and the output feature is calculated as:\n[hv,hq,ha] =LLaMA([v,q,a],p), (1)\nwhere hv is a sequence of output features, i.e.,\nhv = [hv\n1,...,h v\nNv ], and hq,ha are similarly de-\nfined.\n3.2 Flipped-VQA\nTo utilize LLMs’ temporal and causal reasoning\nabilities, we here present Flipped-VQA, consisting\nof three objectives, for reasoning the complex re-\nlationship between video, question, and answer of\nVideoQA.\nVQ →A. Predicting an answer given a video-\nquestion pair is the primary task of VideoQA. Its\nobjective function is formulated as:\nLvqa = −log P(a|v,q)\n= −\nNa−1∑\nt=0\nlog P(at+1|v,q,a≤t),\n(2)\nwhere v and q are given as prefix to generate the an-\nswer a. Note that P(a1|v,q,a≤0) :=P(a1|v,q).\nThen, the probability in Eq. (2) is calculated as:\nP(at+1|v,q,a≤t) =Softmax(Linear(ha\nt )). (3)\nAt the inference phase, the model predicts the an-\nswer as:\nˆa = arg max\na∈A\nP(a|v,q), (4)\nwhere Ais a set of candidate answers, i.e., choices.\nWe now flip the role of inputs and outputs and\ndefine two auxiliary tasks: question generation and\nvideo prediction.\nV A→Q. Similar to Lvqa, we also encourage the\nmodel to generate the question from the video and\n4303\nanswer as:\nLvaq = −log P(q|v,a)\n= −\nNq−1∑\nt=0\nlog P(qt+1|v,a,q≤t),\n(5)\nwhere P(qt+1|v,a,q≤t) = Softmax(Linear(hq\nt )).\nBy Eq. (5), LLaMA-VQA has to generate the ques-\ntion which derives the answer from the video, lever-\naging its prior knowledge of temporal and causal\nreasoning.\nQA →V .Another flipped task is video prediction\ngiven a question and an answer. It is formulated as:\nLqav = −log P(v|q,a)\n= −\nNv−1∑\nt=0\nlog P(vt+1|q,a,v≤t).\n(6)\nIn contrast to the text generation loss in Eq. (2)\nand Eq. (5), which selects a token among the fixed\nvocabulary set (discrete space), it is too challeng-\ning to generate a video. So we instead adopt In-\nfoNCE (Oord et al., 2018) to maximize the mu-\ntual information between the input frame feature\nvt+1 ∈RD and the output feature ( hv\nt ∈RD) of\nLLaMA-VQA. Then, the likelihood in Eq. (6) is\ncalculated as:\nP(vt+1|q,a,v≤t) = exp(vt+1⊤hv\nt )∑Nv\ni=1 exp(vi⊤hv\nt )\n, (7)\nwhere hv\n0 is the token representation right before\nthe start of visual tokens. This encourages the\nmodel to predict the order of video frames given\npreceding frames, i.e., next frame prediction, by an-\nalyzing the question and answer with LLMs’ prior\nknowledge. This formulation enables video predic-\ntion via a unified text-generation-based QA model\nwith minimum modification.\nWe combine all three objectives, which are\nLLMs’ language generation losses and its variant.\nFinally, we train LLaMA-VQA with the following\nloss:\nLFlipped-VQA = Lvqa + Lvaq + Lqav. (8)\nWe accumulate gradients of three different objec-\ntives and then update the learnable parameters.\nRemarks. We observe that the objectives of the\nprimary task and auxiliary tasks can be inter-\npreted as learning posterior and likelihood, respec-\ntively. For instance, by the Bayes rule, we have\nP(a|v,q) ∝ P(q|v,a)P(a|v). Hence, learn-\ning likelihood P(q|v,a) via the question gener-\nation given a video and an answer benefits the pri-\nmary task of predicting the answer given a video\nand a question, which is the posterior probability\nP(a|v,q). Similarly, the same argument holds for\nvideo prediction; P(a|v,q) ∝P(v|q,a). These\nrelationships explain why training a VQA model\nwith flipped tasks boosts the performance of the\ntarget task. More detailed discussion is provided in\nSec. E.\n4 Experiments\nWe verify the effectiveness of our framework to\nleverage the powerful prior knowledge induced by\nan LLM. For a thorough analysis, our framework\nis applied to various LLMs: LLaMA (7B, 13B,\nand 33B), OPT (125M ∼6.7B), GPT-J (6B). We\nconduct experiments and analyses to answer the\nfollowing research questions:\nQ1. Do LLMs possess the knowledge of temporal\nand causal reasoning?\nQ2. Is Flipped-VQA effective for dealing with\nchallenging VideoQA?\nQ3. How does Flipped-VQA alleviate linguistic\nbias?\nDatasets. We experiment on five multiple-choice\nVideoQA benchmark datasets (NExT-QA, STAR,\nDramaQA, VLEP, and TVQA) which require chal-\nlenging temporal and causal reasoning abilities.\nFurther experimental settings and implementation\ndetails are provided in Sec. A and Sec. B.\n4.1 Temporal and causal reasoning of LLMs\nWe investigate LLMs’ strong prior of temporal and\ncausal reasoning to answer Q1 by comparing our\nframework with both LLMs-based and non-LLMs-\nbased models for VideoQA.\nComparison of various sizes of LLMs. We first\nconduct the experiment on various LLMs sizes to\nverify the effectiveness of LLMs’ temporal and\ncausal reasoning ability on VideoQA in Fig. 3.\nNote that Flipped-VQA is not applied in this experi-\nment to show that LLMs already possess strong rea-\nsoning ability themselves. In Fig. 3a, we evaluate\nvarious sizes of LLMs trained with entire⟨V , Q, A⟩\ntriplets and the result shows that the performances\non causal and temporal questions are dramatically\nimproved as the model size increases. On the other\nhand, the performance gain of descriptive questions\nis relatively smaller than causal and temporal ques-\n4304\nModels Language Model# trainableparams NExT-QA STAR DramaQAVLEPTVQACau. Tem. Des.Tot. Int. Seq. Pre. Fea.Tot. Tot. Tot. Tot.\nHGA (Jiang and Han, 2020)GRU - 46.8 52.1 59.350.4 - - - - - - - -FrozenBiLM (Yang et al., 2022)DeBERTa 30M - - - - - - - - - - - 82.0MERLOT (Zellers et al., 2021)RoBERTa 223M - - - - - - - - - 81.4 68.4 78.7HCRN (Le et al., 2021) LSTM 44M 45.9 49.3 53.748.2 - - - - - - - -SPCRL (Kim et al., 2021)BERT - - - - - - - - - - 81.0 - 76.2VGT (Xiao et al., 2022) BERT - 53.4 56.4 69.556.9 - - - - - - - -AIO (Wang et al., 2023a) - 110M 48.0 48.6 63.250.647.5 50.8 47.8 44.147.5 - - -VidL (Cheng et al., 2023)BERT 25M - - - - - - - - - - - 79.0ATP (Buch et al., 2022)CLIP text encoder- 53.1 50.2 66.854.350.6 52.9 49.4 40.648.4 - - -MIST (Gao et al., 2023) - - 54.6 56.6 66.957.255.6 54.2 54.2 44.553.9 - - -HiTeA (Ye et al., 2022) BERT - 62.4 58.3 75.663.1 - - - - - - - -InternVideo (Wang et al., 2022)CLIP text encoder1.3B 62.5 58.575.863.262.7 65.6 54.9 51.958.7 - 63.9 57.2\nLLaMA-VQA(Ours) LLaMA 4.5M 72.7 69.2 75.872.066.2 67.9 57.2 52.765.4 84.1 71.0 82.2\nTable 2: Comparison on five challenging VideoQA benchmarks with non-LLMs-based models. NExT-QA\ninvolves causal, temporal, and descriptive question types. STAR contains four question types: interaction, sequence,\nprediction, and feasibility. Total accuracy is highlighted in grey.\n(a) VQA\n (b) QA\nFigure 3: Performances of LLMs on three question\ntypes of NExT-QA. Performances of various sizes of\nOPT (125M ∼6.7B) and LLaMA (7B ∼33B) are re-\nported. A VideoQA approach with a larger language\nmodel achieves a better performance in both VQA\nand QA settings. Surprisingly, the QA approach with\nLLaMA (33B) outperforms VQA models with OPT\n(125M ∼6.7B) in temporal and causal reasoning.\ntions. The performance gap between descriptive\nand causal questions has decreased from 17.2% on\n125M to 1.1% on 33B.\nAlso, to verify the LLMs’ prior knowledge of\ntemporal and causal reasoning, we evaluate LLMs\ntrained with only ⟨Q, A⟩pairs by forcing the model\nto solely rely on the question. In Fig. 3b, with only\nlinguistic information (i.e., question), the perfor-\nmance of causal questions is lower than descriptive\nquestions on 125M, but it significantly improves as\nthe model size increases and outperforms the de-\nscriptive question accuracy on 33B by a margin of\n6.6%. Without visual content, this model already\noutperforms MIST (Gao et al., 2023) in Tab. 2, a\nnon-LLMs-based model, in terms of causal and\ntemporal question types. These results suggest that\nlarger LLMs possess more powerful prior of causal\nand temporal reasoning obtained during pretrain-\ning, and such prior plays a significant role in ex-\nploiting linguistic shortcuts for complex VideoQA.\nModels LLMs# totalparams# trainableparams NExT-QACau. Tem. Des.Tot.\nBLIP-2 (Li et al., 2023a)FlanT512.1B 188M70.1 65.2 80.170.1SeViLA (Yu et al., 2023)FlanT512.1B 188M74.2 69.481.373.8\nLLaMA-VQALLaMA7B 4.5M 72.7 69.2 75.872.0LLaMA13B 6M 75.3 71.7 75.974.2LLaMA33B 9.2M 76.2 72.678.875.5\nTable 3: Comparison with LLM-based models.\nComparison with non-LLMs-based models. In\nTab. 2, we then show the results of LLaMA-VQA\nin comparison to non-LLMs-based models on five\nchallenging VideoQA benchmark datasets. Our\nLLaMA-VQA outperforms all the baselines across\nvarious datasets by a significant margin, especially\non causal and temporal questions. For example,\nin NExT-QA, the performance gain in descrip-\ntive questions type is marginal compared to In-\nternVideo, but it yields more than 10% improve-\nments in both temporal and causal questions. Also,\nin STAR, LLaMA-VQA surpasses MIST on all\nquestion types resulting in an 11.5% increase in\ntotal accuracy. Particularly, the performance on\nsequence questions, which ask for temporal rea-\nsoning about consecutive actions, is improved by\na large margin of 13.7%. These results highlight\nthat LLMs-based LLaMA-VQA achieves remark-\nable capability, especially on temporal and causal\nreasoning questions compared to non-LLMs-based\nmodels, by mainly leveraging its pretrained prior\n(introducing only 4.5M learnable parameters).\nComparison with LLMs-based models. We also\nexplore larger LLaMA-VQA (∼33B) and compare\nthem with LLMs-based models on NExT-QA in\nTab. 3. Our LLaMA-VQA 33B outperforms BLIP-\n2 and SeViLA in terms of the total accuracy only\nwith 9.2M trainable parameters. Specifically, the\nperformance gain on causal and temporal questions\n4305\nLLMsSizesEpochs ObjectivesNExT-QA STAR DramaQALvqa Lvaq Lqav\nOPT 6.7B\n15 ✔ 57.0 57.7 73.05 ✔ 57.2 56.6 73.25 ✔ ✔ 60.9 60.0 76.95 ✔ ✔ ✔ 62.3 63.3 78.7\nGPT-J6B\n15 ✔ 62.8 59.3 80.75 ✔ 62.6 59.3 80.15 ✔ ✔ 64.2 60.1 81.15 ✔ ✔ ✔ 67.1 63.7 82.7\nLLaMA7B\n15 ✔ 67.1 60.6 82.45 ✔ 68.7 60.9 82.65 ✔ ✔ 71.2 6.1 83.35 ✔ ✔ ✔ 72.0 65.4 84.1\nTable 4: Comparison of various LLMs and objectives.\nTotal accuracy is reported.\nis 2% and 3.2% compared to SeViLA. On the other\nhand, the accuracy of LLaMA-VQA on descriptive\nquestions is lower than baselines since they were\nfurther pretrained with large-scale image-caption\npair datasets, which boosts the descriptiveness ca-\npability of visual content. Finally, as the model\nsize of LLaMA-VQA increases (7B →33B), the\nperformance on causal and temporal questions is\nincreased by 3.5% and 3.4%, respectively implying\nthat larger LLMs have more powerful temporal and\ncausal reasoning capabilities.\n4.2 Flipped-VQA on challenging VideoQA\nWe here discuss Q2 by analyzing the effectiveness\nof Flipped-VQA on challenging VideoQA.\nAblation studies of Flipped-VQA. Tab. 4 shows\nthe ablation studies of Flipped-VQA on various\nLLMs (OPT, GPT-J, and LLaMA). Compared to\nthe baseline LLMs with Lvqa, introducing a ques-\ntion generation objective Lvaq improves the perfor-\nmances by 3.7%, 1.6%, and 2.5% in NExT-QA on\nOPT, GPT-J, and LLaMA, respectively. This result\ndemonstrates that generating intricate questions of\nNExT-QA given a video-answer pair encourages\nLLMs to leverage their temporal and causal reason-\ning knowledge to predict the answer. In addition,\nfurther improvement is observed by adapting video\npredicting objective Lqav that helps to understand\nthe order of visual contents based on the question\nand answer. The accuracy of GPT-J is increased\nby a margin of 2.9%, 3.6%, and 1.6% respectively\non NExT-QA, STAR, and DramaQA. Overall, each\ncomponent of Flipped-VQA improves the perfor-\nmance across various LLMs, implying that Flipped-\nVQA is an effective training objective for LLMs to\ndeal with challenging VideoQA by leveraging their\npretrained knowledge.\nUnlike solely using Lvqa to perform the main\n(a) Causal question\n (b) Temporal question\nFigure 4: Examples of question generation.\ntask, Flipped-VQA accumulates gradients from\nthree different objectives. So we conduct an ad-\nditional experiment by increasing the gradient ac-\ncumulation steps three times more than the base-\nline, i.e., we additionally train Lvqa for 15 epochs\nwhile the others are trained for 5 epochs. In Tab. 4,\nthe performance of Lvqa with 15 epochs is on par\nwith the one with 5 epochs across various LLMs\nand datasets. This suggests that the performance\ngain of Flipped-VQA does not come from the in-\ncreased gradient accumulation or training sched-\nule, but comes from the capability of LLMs’ prior\nknowledge exploited by Lvaq and Lqav.\nQualitative results of generated questions. Fig. 4\nillustrates examples of questions generated by\nLLaMA-VQA conditioned on the video and an-\nswer with the objective of Lvaq, in the NExT-QA\nvalidation set. We observe that the majority of ⟨V ,\nQ, A⟩triplets with generated questions are plau-\nsible enough to answer, while the generated ques-\ntions depict different aspects from the video than\nthe originals. For instance of the causal question\nin Fig. 4a, LLaMA-VQA combines the visual con-\ntent, a man waterskiing, with the answer “hold on\nto long rope” and expresses as the man is “main-\ntain[ing] balance”. Note that the idea of “keep[ing]\ncontact” in the original question aligns with the\nidea of “maintain[ing] his balance”, so there is no\ndifficulty for the model to answer the generated\nquestion based on the given video. Hence it shows\nhow LLMs are using their pretrained knowledge\nto generate the question appropriate to the given\n4306\nExploiting linguistic shortcutMitigating linguistic bias\nP(ˆYA|V,Q=Y⏐⏐⏐ˆYA|Q=Y)P(ˆYA|V,Q=Y⏐⏐⏐ˆYA|Q̸=Y)\nFlipped-VQA✖ ✔ ✖ ✔\nRatio 82.7% 87.3% 50.7% 53.8%\nTable 5: Ratio of the number of samples. ˆYA|Q de-\nnotes the prediction of the model trained with only ⟨Q,\nA⟩. ˆYA|Q,V stands for the prediction of LLaMA-VQA\neither trained with or without Flipped-VQA. Y is a\nground truth.\nvideo and answer.\nMore interestingly, LLaMA-VQA is capable of\nunderstanding intricate interactions present in the\nvideo. For the temporal question in Fig. 4b, unlike\nthe original question that asks for the action after\nthe girl “stop[s] hitting the board”, the generated\nquestion asks for the action after “look[ing] down”.\nThis reveals that LLaMA-VQA understands the in-\nteraction between objects and the sequential events\nin the video, and thus it can generate temporal ques-\ntions adequate to the answer. These results suggest\nthat LLaMA-VQA successfully understands the\ncomplex relationship between the video, question,\nand answer with Flipped-VQA by leveraging its\nprior knowledge of temporal and causal reasoning.\n4.3 Flipped-VQA for mitigating linguistic bias\nWe observed that Flipped-VQA is crucial to ad-\ndress linguistic bias. So we finally analyze how\nFlipped-VQA alleviates linguistic bias (Q3) by pro-\nviding detailed quantitative and qualitative analy-\nsis.\nQuantitative results of bias mitigation. Although\nLLMs’ strong prior of temporal and causal reason-\ning is beneficial to exploit linguistic shortcuts for\nchallenging questions, this sometimes leads to sub-\noptimal results by forcing the model to overly rely\non the text question while ignoring the visual con-\ntent. This problem, linguistic bias, is commonly\nobserved in visual question answering (Niu et al.,\n2021; Ramakrishnan et al., 2018; Cadene et al.,\n2019). Our extensive analyses show that Flipped-\nVQA mitigates the linguistic bias while effectively\nleveraging linguistic shortcut. We first analyze how\neffectively LLaMA-VQA exploits linguistic short-\ncuts. Specifically, Tab. 5 shows that Flipped-VQA\nimproves the accuracy of LLaMA-VQA on NExT-\nQA by 4.6% when the linguistic prior is correct. It\nis measured by the following equation:\nP\n(\nˆYA|V,Q = Y\n⏐⏐⏐ˆYA|Q = Y\n)\n. (9)\n(a) Causal question\n (b) Temporal question\nFigure 5: Examples of alleviation on linguistic bias.\nHere, the correctness of linguistic prior is de-\nfined as the accuracy of the QA model that pre-\ndicts answers solely based on the language, i.e.,\nP( ˆYA|Q = Y). Secondly, we analyze how effec-\ntively LLaMA-VQA mitigates linguistic bias by\nthe following metric:\nP\n(\nˆYA|V,Q = Y\n⏐⏐⏐ˆYA|Q ̸= Y\n)\n. (10)\nThis measures the accuracy of the VQA model\nwhen the linguistic prior is wrong, i.e., ˆYA|Q ̸= Y.\nTab. 5 shows that Flipped-VQA improves the accu-\nracy on the samples with linguistic bias (inaccurate\nlinguistic priors) by 3.1%. Our in-depth analysis of\nattention and embeddings also shows that Flipped-\nVQA encourages LLMs to leverage more visual\ncontent and better align the visual embedding space\nwith LLMs’ text embedding space, see Sec. F for\ndetails.\nQualitative results of bias mitigation. We here\nfurther analyze the effectiveness of Flipped-VQA\nin mitigating linguistic bias with qualitative results.\nGiven incorrect linguistic prior, i.e., ˆYA|Q ̸= Y, in\nother words, when the prediction by the language-\nonly model is wrong, our model trained with\nFlipped-VQA better rejects the wrong linguistic\nbias than the one trained without Flipped-VQA. For\nexample in Fig. 5a the language-only model out-\nputs “man draws on ground” for the causal question\n“Why are there lines everywhere on the ground?”.\nLLaMA-VQA trained without Flipped-VQA fails\nto reject the wrong linguistic prior and chooses the\n4307\nplausible-sounding answer based on the common\nknowledge in the pretrained language model with-\nout referring to visual content. This can be viewed\nas the hallucination and ungrounded guess problem\nobserved in Alayrac et al. (2022). On the other\nhand, LLaMA-VQA trained with Flipped-VQA\nrefers to the visual content that depicts a plane\nleaving traces on the snow as it is taking off and\nsuccessfully predicts the actual cause “plane take\noff”. The enhanced attention between the answer\nand visual tokens supports that Flipped-VQA en-\ncourages the model to refer to visual content, see\nSec. F for more details.\nSimilarly, LLMs’ temporal prior occasionally\ndisrupts identifying the action followed by an event.\nFor the temporal question in Fig. 5b, the act of\nwiping off follows the act of “pick[ing] up the\ntowel” in general. Hence, the language-only model\nˆYA|Q and LLaMA-VQA ˆYA|V,Q without Flipped-\nVQA predict “rub the baby’s face.” In contrast,\nthe proposed method with Flipped-VQA accurately\npredicts “keeps it back” by referring to the video.\nThese results demonstrate that Flipped-VQA en-\ncourages the answers grounded on visual informa-\ntion and mitigates linguistic bias.\n5 Conclusion\nIn this paper, we investigate the large language\nmodels’ (LLMs’) temporal and causal reasoning\nabilities on the challenging multi-modal Video\nQuestion Answering (VideoQA) task. We observe\nthat the larger LLMs possess more powerful prior\nknowledge of temporal and causal reasoning in ad-\ndressing complex VideoQA. Moreover, we propose\na novel framework, Flipped-VQA, that effectively\nleverages the LLMs’ knowledge of temporal and\ncausal reasoning on understanding the complex\n⟨V , Q, A⟩triplet by introducing three generative\nobjectives: Lvqa, Lvaq, and Lqav. Our in-depth anal-\nyses show that Flipped-VQA not only enhances the\nexploitation of linguistic shortcuts but also miti-\ngates linguistic bias that causes hallucination and\nungrounded guess problems.\nAcknowledgments. This work was partly sup-\nported by ICT Creative Consilience program (IITP-\n2023-2020-0-01819) supervised by the IITP, the\nNational Research Foundation of Korea (NRF)\ngrant funded by the Korea government (MSIT)\n(NRF-2023R1A2C2005373), and KakaoBrain cor-\nporation.\nLimitations\nWe propose a Flipped-VQA which can be widely\nadaptable to decoder-only LLMs by improv-\ning their performances on challenging VideoQA.\nFlipped-VQA effectively leverages LLMs’ prior\nknowledge of temporal and causal reasoning with\ngenerative objectives of next token prediction. Ex-\ntending it to encoder-decoder LLMs with an ob-\njective other than the next token prediction can be\ninteresting. Also, although the number of trainable\nparameters of LLaMA-VQA is only 4.5M ∼9.2M,\nthe total number of parameters is inherently large\n(7B ∼33B), which mainly comes from backbone\nLLMs, LLaMA. This leads to massive memory\nusage during training/fine-tuning and inference.\nReferences\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,\nAntoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm\nReynolds, et al. 2022. Flamingo: a visual language\nmodel for few-shot learning. In NeurIPS.\nShyamal Buch, Cristóbal Eyzaguirre, Adrien Gaidon,\nJiajun Wu, Li Fei-Fei, and Juan Carlos Niebles. 2022.\nRevisiting the\" video\" in video-language understand-\ning. In CVPR.\nRemi Cadene, Corentin Dancette, Matthieu Cord, Devi\nParikh, et al. 2019. Rubi: Reducing unimodal biases\nfor visual question answering. In NeurIPS.\nShanqing Cai, Subhashini Venugopalan, Katrin\nTomanek, Ajit Narayanan, Meredith R Morris, and\nMichael P Brenner. 2022. Context-aware abbrevi-\nation expansion using large language models. In\nNAACL.\nJiangjie Chen, Wei Shi, Ziquan Fu, Sijie Cheng, Lei Li,\nand Yanghua Xiao. 2023. Say what you mean! large\nlanguage models speak too positively about negative\ncommonsense knowledge. In ACL.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nFeng Cheng, Xizi Wang, Jie Lei, David Crandall, Mo-\nhit Bansal, and Gedas Bertasius. 2023. Vindlu: A\nrecipe for effective video-and-language pretraining.\nIn CVPR.\nSeongho Choi, Kyoung-Woon On, Yu-Jung Heo, Ah-\njeong Seo, Youwon Jang, Minsu Lee, and Byoung-\nTak Zhang. 2021. Dramaqa: Character-centered\nvideo story understanding with hierarchical qa. In\nAAAI.\n4308\nJohn Joon Young Chung, Ece Kamar, and Saleema\nAmershi. 2023. Increasing diversity while main-\ntaining accuracy: Text data generation with large\nlanguage models and human interventions. In ACL.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. arXiv\npreprint arXiv:1803.05457.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nYu Fei, Yifan Hou, Zeming Chen, and Antoine Bosselut.\n2023. Mitigating label biases for in-context learning.\nIn ACL.\nDifei Gao, Luowei Zhou, Lei Ji, Linchao Zhu, Yi Yang,\nand Mike Zheng Shou. 2023. Mist: Multi-modal\niterative spatial-temporal transformer for long-form\nvideo question answering. In CVPR.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding. In ICLR.\nNamgyu Ho, Laura Schmid, and Se-Young Yun. 2022.\nLarge language models are reasoning teachers. In\nACL.\nMarius Hobbhahn, Tom Lieberum, and David Seiler.\n2022. Investigating causal understanding in llms. In\nNeurIPS Workshop.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\nlarge language models. In ICLR.\nYunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim,\nand Gunhee Kim. 2017. Tgif-qa: Toward spatio-\ntemporal reasoning in visual question answering. In\nCVPR.\nMenglin Jia, Luming Tang, Bor-Chun Chen, Claire\nCardie, Serge Belongie, Bharath Hariharan, and Ser-\nNam Lim. 2022. Visual prompt tuning. In ECCV.\nDongfu Jiang, Xiang Ren, and Bill Yuchen Lin. 2023.\nLlm-blender: Ensembling large language models\nwith pairwise ranking and generative fusion. In ACL.\nPin Jiang and Yahong Han. 2020. Reasoning with het-\nerogeneous graph alignment for video question an-\nswering. In AAAI.\nYao Jin, Guocheng Niu, Xinyan Xiao, Jian Zhang,\nXi Peng, and Jun Yu. 2023. Knowledge-constrained\nanswer generation for open-ended video question an-\nswering. In AAAI.\nChen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and\nWeidi Xie. 2022. Prompting visual-language models\nfor efficient video understanding. In ECCV.\nEhsan Kamalloo, Nouha Dziri, Charles LA Clarke, and\nDavood Rafiei. 2023. Evaluating open-domain ques-\ntion answering in the era of large language models.\nIn ACL.\nEmre Kıcıman, Robert Ness, Amit Sharma, and Chen-\nhao Tan. 2023. Causal reasoning and large language\nmodels: Opening a new frontier for causality. arXiv\npreprint arXiv:2305.00050.\nSeonhoon Kim, Seohyeong Jeong, Eunbyul Kim, Inho\nKang, and Nojun Kwak. 2021. Self-supervised pre-\ntraining and contrastive representation learning for\nmultiple-choice video qa. In AAAI.\nDohwan Ko, Ji Soo Lee, Miso Choi, Jaewon Chu,\nJihwan Park, and Hyunwoo J Kim. 2023. Open-\nvocabulary video question answering: A new bench-\nmark for evaluating the generalizability of video ques-\ntion answering models. In ICCV.\nThao Minh Le, Vuong Le, Svetha Venkatesh, and\nTruyen Tran. 2021. Hierarchical conditional relation\nnetworks for multimodal video question answering.\nIJCV.\nJie Lei, Tamara L Berg, and Mohit Bansal. 2023. Re-\nvealing single frame bias for video-and-language\nlearning. In ACL.\nJie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg.\n2018. Tvqa: Localized, compositional video ques-\ntion answering. In EMNLP.\nJie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal.\n2020. What is more likely to happen next? video-\nand-language future event prediction. In EMNLP.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\n2023a. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large lan-\nguage models. In ICML.\nKunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wen-\nhai Wang, Ping Luo, Yali Wang, Limin Wang, and\nYu Qiao. 2023b. Videochat: Chat-centric video un-\nderstanding. arXiv preprint arXiv:2305.06355.\nKunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan\nHe, Limin Wang, and Yu Qiao. 2023c. Unmasked\nteacher: Towards training-efficient video foundation\nmodels. In ICCV.\nLei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi\nWang, Shuhuai Ren, Mukai Li, Yazheng Yang,\nJingjing Xu, Xu Sun, et al. 2023d. M3it: A large-\nscale dataset towards multi-modal multilingual in-\nstruction tuning. arXiv preprint arXiv:2306.04387.\n4309\nXiang Lorraine Li, Adhiguna Kuncoro, Jordan Hoff-\nmann, Cyprien de Masson d’Autume, Phil Blunsom,\nand Aida Nematzadeh. 2022. A systematic investiga-\ntion of commonsense knowledge in large language\nmodels. In EMNLP.\nXingxuan Li, Liying Cheng, Qingyu Tan, Hwee Tou\nNg, Shafiq Joty, and Lidong Bing. 2023e. Un-\nlocking temporal question answering for large lan-\nguage models using code execution. arXiv preprint\narXiv:2305.15014.\nHanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji\nZhou, and Yue Zhang. 2023. Evaluating the logical\nreasoning ability of chatgpt and gpt-4. arXiv preprint\narXiv:2304.03439.\nStephanie Long, Tibor Schuster, Alexandre Piché, Ser-\nviceNow Research, et al. 2023. Can large lan-\nguage models build causal graphs? arXiv preprint\narXiv:2303.05279.\nIlya Loshchilov and Frank Hutter. 2017. Decou-\npled weight decay regularization. arXiv preprint\narXiv:1711.05101.\nYulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu,\nXian-Sheng Hua, and Ji-Rong Wen. 2021. Counter-\nfactual vqa: A cause-effect look at language bias. In\nCVPR.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.\nRepresentation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748.\nBatu Ozturkler, Nikolay Malkin, Zhen Wang, and Nebo-\njsa Jojic. 2023. Thinksum: Probabilistic reasoning\nover sets using large language models. In ACL.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In ICML.\nSainandan Ramakrishnan, Aishwarya Agrawal, and Ste-\nfan Lee. 2018. Overcoming language priors in visual\nquestion answering with adversarial regularization.\nIn NeurIPS.\nSwarnadeep Saha, Peter Hase, Nazneen Rajani, and\nMohit Bansal. 2022. Are hard examples also harder\nto explain? a study with human and model-generated\nexplanations. In EMNLP.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2021. Winogrande: An adver-\nsarial winograd schema challenge at scale. Commu-\nnications of the ACM.\nQingyu Tan, Hwee Tou Ng, and Lidong Bing. 2023.\nTowards benchmarking and improving the temporal\nreasoning capability of large language models. In\nACL.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\n6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/\nmesh-transformer-jax.\nJinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge,\nKevin Qinghong Lin, Satoshi Tsutsui, Xudong Lin,\nGuanyu Cai, Jianping Wu, Ying Shan, et al. 2023a.\nAll in one: Exploring unified video-language pre-\ntraining. In CVPR.\nLei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu,\nYunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.\n2023b. Plan-and-solve prompting: Improving zero-\nshot chain-of-thought reasoning by large language\nmodels. In ACL.\nYi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun\nHuang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu,\nZun Wang, et al. 2022. Internvideo: General video\nfoundation models via generative and discriminative\nlearning. arXiv preprint arXiv:2212.03191.\nYiming Wang, Zhuosheng Zhang, and Rui Wang. 2023c.\nElement-aware summarization with large language\nmodels: Expert-aligned evaluation and chain-of-\nthought method. In ACL.\nBo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenen-\nbaum, and Chuang Gan. 2021. Star: A benchmark for\nsituated reasoning in real-world videos. In NeurIPS.\nJunbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng\nChua. 2021. Next-qa: Next phase of question-\nanswering to explaining temporal actions. In CVPR.\nJunbin Xiao, Pan Zhou, Tat-Seng Chua, and Shuicheng\nYan. 2022. Video graph transformer for video ques-\ntion answering. In ECCV.\nDejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang\nZhang, Xiangnan He, and Yueting Zhuang. 2017.\nVideo question answering via gradually refined atten-\ntion over appearance and motion. In ACM Multime-\ndia.\nAntoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev,\nand Cordelia Schmid. 2021. Just ask: Learning to\nanswer questions from millions of narrated videos.\nIn ICCV.\nAntoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev,\nand Cordelia Schmid. 2022. Zero-shot video ques-\ntion answering via frozen bidirectional language mod-\nels. In NeurIPS.\nQinghao Ye, Guohai Xu, Ming Yan, Haiyang Xu,\nQi Qian, Ji Zhang, and Fei Huang. 2022. Hitea: Hier-\narchical temporal-aware video-language pre-training.\narXiv preprint arXiv:2212.14546.\n4310\nShoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit\nBansal. 2023. Self-chained image-language model\nfor video localization and question answering. arXiv\npreprint arXiv:2305.06988.\nWeijiang Yu, Haoteng Zheng, Mengfei Li, Lei Ji, Lijun\nWu, Nong Xiao, and Nan Duan. 2021. Learning from\ninside: Self-driven siamese sampling and reasoning\nfor video question answering. In NeurIPS.\nZhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yuet-\ning Zhuang, and Dacheng Tao. 2019. Activitynet-qa:\nA dataset for understanding complex web videos via\nquestion answering. In Proceedings of the AAAI Con-\nference on Artificial Intelligence.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really finish your sentence? In ACL.\nRowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu,\nJae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi.\n2021. Merlot: Multimodal neural script knowledge\nmodels. In NeurIPS.\nCheng Zhang, Stefan Bauer, Paul Bennett, Jiangfeng\nGao, Wenbo Gong, Agrin Hilmkil, Joel Jennings,\nChao Ma, Tom Minka, Nick Pawlowski, et al. 2023a.\nUnderstanding causality with large language mod-\nels: Feasibility and opportunities. arXiv preprint\narXiv:2304.05524.\nHang Zhang, Xin Li, and Lidong Bing. 2023b. Video-\nllama: An instruction-tuned audio-visual language\nmodel for video understanding. arXiv preprint\narXiv:2306.02858.\nMichael JQ Zhang and Eunsol Choi. 2021. Situatedqa:\nIncorporating extra-linguistic contexts into qa. In\nEMNLP.\nRenrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,\nShilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and\nYu Qiao. 2023c. Llama-adapter: Efficient fine-tuning\nof language models with zero-init attention. arXiv\npreprint arXiv:2303.16199.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nAppendix\nA Dataset details\nNExT-QA (Xiao et al., 2021) consists of three\ntypes of questions. Causal questions ask for the\nintentions of earlier actions or reasons for succeed-\ning ones. Temporal questions determine the rela-\ntionships between actions that are solely based on\nthe sequence of occurrence (e.g. what ... do af-\nter/before/while ...). Descriptive questions focus\non visible contents such as places, and objects/at-\ntributes. About 5K videos of average 44s and 48K\nQA pairs with five answer candidates are given.\nFurther examples of each question type are pro-\nvided in Sec. D.\nDramaQA (Choi et al., 2021) features video story\nunderstanding with hierarchical difficulty levels.\nThe level is determined by the required length of\nthe clip (shot or scene) and the number of logical\nreasoning steps to answer the question. The dataset\ncontains 24K video clips and 18K QA pairs with\nfive answer candidates. Average video lengths are\n3.7s for the shot and 91.3s for the scene.\nSTAR (Wu et al., 2021) is designed for situational\nreasoning with questions that tackle interaction, se-\nquence, prediction, and feasibility of events. There\nexist 60K QA pairs with four answer candidates\nand 22K video clips.\nVLEP (Lei et al., 2020) uses TV shows and\nYouTube Vlogs with an average of 6.1s that contain\nrich physical interactions and dialogues between\npeople. The challenge is to determine which of two\nfuture events is likely to occur in the given video\n(with dialogue). It is comprised of 29K QA pairs\nwith 10K video clips.\nTVQA (Lei et al., 2018) is built on long video clips\n(60-90s) of six different TV shows with various\nsocial interactions and activities. It provides dia-\nlogues for each video with 153K QA pairs and 22K\nvideo clips.\nB Implementation details\nTraining details. LLaMA-VQA is trained for\nfive epochs on all datasets with a batch size\nof 32. LLaMA-VQA 7B and 13B are trained\nwith 8 ×A6000 GPUs and LLaMA-VQA 33B\nis trained with 8 ×A100 GPUs. AdamW opti-\nmizer (Loshchilov and Hutter, 2017) is used with\nβ = (0.9,0.95). We search learning rate and\nweight decay in [0.05, 0.1] and [0.15, 0.25], respec-\ntively. Following LLaMA-Adapter, for each layer\nof LLMs, 10 adapter tokens are used, i.e.Np = 10.\nThe number of video frames Nv is set to 10. Each\nframe is resized by 224 ×224 and fed into CLIP\nVIT-L/14 (Radford et al., 2021) to extract frame\nfeatures. The total sequence length of the con-\ncatenated visual, question, and answer tokens,\nNv + Nq + Na, is 128, 128, 384, 256, and 512 for\nNExT-QA, STAR, DramaQA, VLEP, and TVQA\nrespectively. Each dataset optionally provides dia-\nlogues. We append dialogues as prefix tokens for\n4311\n[SOS] Video: <v1> <v2> ··· <vNv >\nQuestion: <question>\nChoices:\n(A) <option 1>\n(B) <option 2>\n(C) <option 3>\n(D) <option 4>\n(E) <option 5>\nAnswer: The answer is <answer> [EOS]\nTable 6: Input Prompt of VQ →A.\n[SOS] Video: <v1> <v2> ··· <vNv >\nChoices:\n(A) <option 1>\n(B) <option 2>\n(C) <option 3>\n(D) <option 4>\n(E) <option 5>\nAnswer: The answer is <answer> [EOS]\nQuestion: <question> [EOS]\nTable 7: Input Prompt of V A→Q.\nVLEP and TVQA. Lvaq is not applied in VLEP\nsince questions of all samples in VLEP are consis-\ntent to “Which event is more likely to happen right\nafter?”.\nPrompt details. The general input prompt of\nLLaMA-VQA is provided in Tab. 1. Also, Tab. 6,\nTab. 7, and Tab. 8 provides detailed input prompt of\neach task in Flipped-VQA, respectively. In those\ntables, non-prefix tokens, which the model needs\nto generate, are highlighted in red and the rest are\nprefix tokens.\nC LLaMA-Adapter\nLLaMA-Adapter (Zhang et al., 2023c) adopts a set\nof learnable adapter tokens p = [p1,...,p Np ] ∈\nRNp×D to efficiently fine-tune LLaMA (Touvron\net al., 2023), where Np is the number of adapter\ntokens and Dis a feature dimension. The adapter\ntokens are then concatenated as prefix tokens for\nthe key and value of each self-attention layer, for-\nmulated as:\nQ= Linearq([v,q,a]) ∈RN ,\nK = Lineark([p,v,q,a]) ∈RNp+N ,\nV = Linearv([p,v,q,a]) ∈RNp+N ,\n(11)\n[SOS] Question: <question>\nChoices:\n(A) <option 1>\n(B) <option 2>\n(C) <option 3>\n(D) <option 4>\n(E) <option 5>\nAnswer: The answer is <answer>\nVideo: <v1> <v2> ··· <vNv >\nTable 8: Input Prompt of QA →V .\nwhere N = Nv + Nq + Na. Note in our work,\naccording to each objective of Flipped-VQA, the\norder of v, q, and a is permuted. Then, the scaled\ndot-product between Qand Kis calculated as:\nS = QK⊤/\n√\nD∈RN×(Np+N). (12)\nSin Eq. (12) can be divided into two groups as:\nS = [SNp ,SN ]⊤, (13)\nwhere SNp ∈RN×Np is the attention scores of\nadapter tokens p and SN ∈RN×N is for v, q, a\ntokens.\nMoreover, to adjust the contribution of newly\nadopted tokens p at the beginning of training,\nLLaMA-Adapters introduce a zero-init attention\ngate gas:\nS = [Softmax(SNp ) ·g,Softmax(SN )]⊤, (14)\nwhere g is initialized to zero. Eq. (14) preserves\nthe LLMs’ knowledge at the beginning of training\nand gradually increases the influence of adapter\ntokens p. Finally, the output of LLaMA-Adapter is\nas follows:\nH = Linear(SV) ∈RN×D. (15)\nD NExT-QA\nIn NExT-QA, causal and temporal questions ac-\ncount for 48% and 29% respectively of the dataset.\nSpecifically, there exist three types of questions:\nCausal, Temporal, and Descriptive. In general,\nquestions and answers for temporal and causal\ntypes are longer than descriptive ones.\nCausal questions seek for event A which hap-\npens in advance of event B and is also responsible\nfor B’s occurrence in the video. These questions\nare broken down into asking “how” such an event\n4312\nFigure 6: Examples of NExT-QA.\noccurred or “why” the object acts in a certain way.\nFor instance, Fig. 6 (left) asks “Why did the short-\nest girl cover her mouth near the end of the video?”.\nTemporal questions are closely related to causal-\nity but require reasoning solely based on the se-\nquence of occurrence (present, previous, or next\nactions) and further ask to focus on interactions of\nmultiple objects. For example, a question regard-\ning the previous action asks “What does the man\nin red do before the man in white cut the ribbon?”\nin Fig. 6 (middle).\nLastly, descriptive questions tend to ask about\nthe video in general ( i.e., the places, objects/at-\ntributes, and main actions/events). For instance,\nFig. 6 (right) asks “What is the relationship be-\ntween the guy in red and the rest of the people in\nblack?”.\nE Discussion on Flipped-VQA\nThe primary task of VideoQA, predicting the an-\nswer given the video and question, can be rewritten\nas:\nP(a|v,q) =P(v|a,q)P(a|q)\nP(v|q) ∝P(v|a,q).\n(16)\nIn Eq. (16), we observe that the auxiliary task of\npredicting visual tokens v given q and a can be\nviewed as maximum likelihood estimation ( i.e.,\nP(v|a,q)) and the primary task as the maximum\na posterior (MAP) estimation (i.e., P(a|v,q)), re-\nspectively.\nSimilarly, the auxiliary task of predicting the\nquestion token q given a and v can be correlated\nwith the primary task as:\nP(a|v,q) =P(q|a,v)P(a|v)\nP(q|v) ∝P(q|a,v).\n(17)\nTherefore, by maximizing the likelihoods of\nauxiliary tasks, LFlipped-VQA in Eq. (8) aims to\nstrengthen the performance on the main task.\n(a) T-SNE\n (b) Attention score\nFigure 7: Visualization of T-SNE and attention score.\n(a) Each input token embeddings of visual v and ques-\ntion q is visualized. (b) Attention score between answer\nquery tokens and visual key tokens is visualized.\nF Embedding space alignment of\nFlipped-VQA\nTo bridge the visual encoder embedding space with\nLLMs text embedding space, we adopt a simple\nlinear projection layer f. We observe that without\nFlipped-VQA, f is not trained effectively as the\nvisual tokens v are just used as prefix tokens which\nare excluded from the generation target of LLMs.\nHowever, with Flipped-VQA which directly prop-\nagates the loss to the visual tokens (red arrows of\nLqav in Fig. 2), f is trained to align visual encoder\nembedding space with frozen LLMs text token em-\nbedding space. We visualize the embedding space\nof question tokens q and visual tokens v with and\nwithout Flipped-VQA in Fig. 7a, and show that\nthe embedding space of visual tokens with Flipped-\nVQA (orange) is closer to LLMs’ text embedding\nspace (red) compared to the one without Flipped-\nVQA (blue).\nFurthermore, in Fig. 7b, we plot the attention\nscore between the answer query tokens and visual\nkey tokens, i.e., measuring how much visual tokens\nv affect answer tokens a. As training proceeds,\nthe attention score of both LLaMA-VQA with and\nwithout Flipped-VQA gradually increases, repre-\nsenting that the model leverages more visual con-\ntent to answer the question. However, after the\nentire training iterations, the attention score with\n4313\nModels # external visual-textdata samplesWUPS\nHGA (Jiang and Han, 2020) 0 25.2\nKcGA (Jin et al., 2023) 0 28.2\nFlamingo 0-shot (Alayrac et al., 2022)2.1B 26.7\nFlamingo 32-shot (Alayrac et al., 2022)2.1B 33.5\nLLaMA-VQA(Ours) 0 34.3\nTable 9: Results of NExT-QA.\nModels # external visual-textdata samples Accuracy\nJustAsk (Yang et al., 2021) 69M 38.9\nSiaSamRea (Yu et al., 2021)5.6M 39.8\nMERLOT (Zellers et al., 2021)180M 41.4\nFrozenBiLM (Yang et al., 2022)10M 43.2\nSingularity (Lei et al., 2023)17M 44.1\nFrozenBiLM+ (Ko et al., 2023)10M 44.8\nUMT-L (Li et al., 2023c) 25M 47.9\nLLaMA-VQA(Ours) 0 48.6\nTable 10: Results of ActivityNet-QA.\nFlipped-VQA (red) is three times larger than the\none without Flipped-VQA (blue), indicating that\nFlipped-VQA plays a key role in transferring the\nvisual representation into the LLMs embedding\nspace and enhances the utilization of visual content\non LLMs. These results demonstrate that Flipped-\nVQA encourages text-only trained LLMs to un-\nderstand visual content by utilizing the strong rep-\nresentation power of a pretrained visual encoder,\neffectively aligning the visual embedding space\nwith the text embedding space.\nG Further quantitative results\nWe conduct an additional experiment on two\ngeneration-based VideoQA benchmark datasets:\nNExT-QA open-form generation (Xiao et al., 2021)\nand ActivityNet-QA (Yu et al., 2019). WUPS and\nAccuracy are used for evaluation metrics in NExT-\nQA open-form generation and ActivityNet-QA, re-\nspectively. In Tab. 9, our LLaMA-VQA outper-\nforms non-LLMs-based models HGA and KcGA\nby a large margin. Also, compared with LLMs-\nbased Flamingo which is further trained with 2.1B\nexternal visual-text pairs, the performance gain\nis 0.8%. Furthermore, in Tab. 10, LLaMA-VQA\nalso outperforms all the baselines although those\nuse large-scale visual-text data for extra training in\nActivityNet-QA.\nH Further qualitative results\nQuestion generation of Flipped-VQA. We here\nshow further qualitative examples of generated\nquestions by Lvaq in Fig. 8. For the example in\nthe middle of the first row, the generated question\nsuccessfully describes the video, where the man\nmoves across the muddy area, so the model can an-\nswer “jeep” based on this question. Also, LLaMA-\nVQA generates questions by referring to different\ntimestamps of the video. In the right example of\nthe first row, the original question asks the reason\nwhy the lioness bends its head in the middle of the\nvideo to lead to the answer “drink water”. However,\nthe lioness also touches the river to drink water at\nthe beginning of the video, and the generated ques-\ntion asks the reason for touching the river to obtain\nthe answer “drink water”. This suggests that Lvaq\nof Flipped-VQA helps to understand the complex\nrelationship between video, question, and answer\nwith LLMs’ prior knowledge.\nBias alleviation of Flipped-VQA. In addition to\nthe examples in Fig. 5, we provide further qualita-\ntive results that Flipped-VQA enables the allevia-\ntion of linguistic bias. For the left example of the\nfirst row in Fig. 9, the model trained with question-\nanswer pairs ( ˆYA|Q) predicts “microphone” for the\ntemporal question “What are the men holding on\nto when they speak?”, based on the prior knowl-\nedge that people usually use the microphone when\nthey are speaking. LLaMA-VQA trained without\nFlipped-VQA still fails to reject the wrong linguis-\ntic bias and predicts the same answer. On the other\nhand, LLaMA-VQA trained with Flipped-VQA ac-\ncurately outputs the answer “glasses” by taking\ninto account the video where the men are holding\nglasses, resulting in the reduction of hallucination\nproblems with the mitigation of linguistic bias.\n4314\nFigure 8: Examples of question generation.\n4315\nFigure 9: Examples of alleviation on linguistic bias.\n4316",
  "topic": "Question answering",
  "concepts": [
    {
      "name": "Question answering",
      "score": 0.7728734016418457
    },
    {
      "name": "Computer science",
      "score": 0.6086798906326294
    },
    {
      "name": "Code (set theory)",
      "score": 0.5064073801040649
    },
    {
      "name": "Language model",
      "score": 0.478603333234787
    },
    {
      "name": "Natural language processing",
      "score": 0.47740235924720764
    },
    {
      "name": "Prior probability",
      "score": 0.46884098649024963
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43407613039016724
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3406520485877991
    },
    {
      "name": "Psychology",
      "score": 0.2684378921985626
    },
    {
      "name": "Programming language",
      "score": 0.10788914561271667
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.07328054308891296
    },
    {
      "name": "Bayesian probability",
      "score": 0.0
    }
  ]
}