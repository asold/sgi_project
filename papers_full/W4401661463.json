{
  "title": "Mechanistic interpretability of large language models with applications to the financial services industry",
  "url": "https://openalex.org/W4401661463",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2587043598",
      "name": "Ashkan Golgoon",
      "affiliations": [
        "Discover Financial Services (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2793298144",
      "name": "Khashayar Filom",
      "affiliations": [
        "Discover Financial Services (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3101288926",
      "name": "Arjun Ravi Kannan",
      "affiliations": [
        "Discover Financial Services (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4367628394",
    "https://openalex.org/W3110749958",
    "https://openalex.org/W4234322080",
    "https://openalex.org/W4388994228",
    "https://openalex.org/W6839328737",
    "https://openalex.org/W3010694149",
    "https://openalex.org/W6843370592",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4308023630"
  ],
  "abstract": "Large Language Models such as GPTs (Generative Pre-trained Transformers) exhibit remarkable capabilities across a broad spectrum of applications. Nevertheless, due to their intrinsic complexity, these models present substantial challenges in interpreting their internal decision-making processes. This lack of transparency poses critical challenges when it comes to their adaptation by financial institutions, where concerns and accountability regarding bias, fairness, and reliability are of paramount importance. Mechanistic interpretability aims at reverse engineering complex AI models such as transformers. In this paper, we are pioneering the use of mechanistic interpretability to shed some light on the inner workings of large language models for use in financial services applications. We offer several examples of how algorithmic tasks can be designed for compliance monitoring purposes. In particular, we investigate GPT-2 Small's attention pattern when prompted to identify potential violation of Fair Lending laws. Using direct logit attribution, we study the contributions of each layer and its corresponding attention heads to the logit difference in the residual stream. Finally, we design clean and corrupted prompts and use activation patching as a causal intervention method to localize our task completion components further. We observe that the (positive) heads $10.2$ (head $2$, layer $10$), $10.7$, and $11.3$, as well as the (negative) heads $9.6$ and $10.6$ play a significant role in the task completion.",
  "full_text": "MECHANISTIC INTERPRETABILITY OF LARGE LANGUAGE MODELS WITH\nAPPLICATIONS TO THE FINANCIAL SERVICES INDUSTRY\nASHKAN GOLGOON∗,‡ , KHASHAYAR FILOM∗,† , ARJUN RAVI KANNAN∗,§\nAbstract. Large Language Models such as GPTs (Generative Pre-trained Transformers) exhibit remark-\nable capabilities across a broad spectrum of applications. Nevertheless, due to their intrinsic complexity,\nthese models present substantial challenges in interpreting their internal decision-making processes. This\nlack of transparency poses critical challenges when it comes to their adaptation by financial institutions,\nwhere concerns and accountability regarding bias, fairness, and reliability are of paramount importance.\nMechanistic interpretability aims at reverse engineering complex AI models such as transformers. In this\npaper, we are pioneering the use of mechanistic interpretability to shed some light on the inner workings of\nlarge language models for use in financial services applications. We offer several examples of how algorith-\nmic tasks can be designed for compliance monitoring purposes. In particular, we investigate GPT-2 Small’s\nattention pattern when prompted to identify potential violation of Fair Lending laws. Using direct logit\nattribution, we study the contributions of each layer and its corresponding attention heads to the logit dif-\nference in the residual stream. Finally, we design clean and corrupted prompts and use activation patching\nas a causal intervention method to localize our task completion components further. We observe that the\n(positive) heads10.2 (head 2, layer10), 10.7, and11.3, as well as the (negative) heads9.6 and 10.6 play a\nsignificant role in the task completion.\nKeywords:: Mechanistic Interpretability, Large Language Models (LLMs), Transformer Circuits,\nFinTech, Natural Language Processing.\nThe goal of this paper is to introduce the reader to the emerging field ofmechanistic interpretabilityand\nits potential applications to financial services, especially when it comes to understanding the inner workings\nof Large Language Models(LLMs) in financial services.\n1. Background on LLMs\nThe release of ChatGPT by OpenAI in late 2022 stunned the world as the chatbot set new milestones\nsurpassing all previous publicly available systems and triggered discussions about potential risks [1, 9, 12].\nAt the heart of the current large language model (LLM) revolution lies thetransformer architecture. Below,\nwe provide a short introduction on transformers, followed by LLMs.\n1.1. The transformer architecture.Transformers are deep feed-forward neural networks that leverage\nthe attention mechanism. They excel in sequence modeling tasks, especially in natural language processing\n(NLP) [26]. Before the advent of the transformer architecture in the landmark paper [38], recurrent neural\nDate: October 2024.\n∗Emerging Capabilities Research Group, Discover Financial Services Inc., Riverwoods, IL.\n‡Co-first author,ashkangolgoon@gmail.com.\n†Co-first author,khashayar.1367@gmail.com.\n§arjun.kannan@gmail.com.\n1\narXiv:2407.11215v2  [cs.LG]  16 Oct 2024\n2 GOLGOON, FILOM, RAVI KANNAN\nFigure 1. The transformer architecture. Picture adapted from [38].\nnetwork(RNN)architecturessuchas Long-Short Term Memory(LSTM)werecommonforNLPandsequence\nmodeling tasks. Such models rely on an internal hidden state and must process the data sequentially. On\nthe other hand, transformers, which are based on the attention mechanism, are superior in capturing long-\nterm dependencies. This is due to the non-sequential and parallel manner by which they process the data.\nTransformers, moreover, allowtransfer learning—they can be pre-trained on large corpora and thenfine-\ntuned for downstream tasks—a fact that facilitates developing new AI applications tailored for in-house\ndatasets based on the existingfoundation modelssuch as BERT or GPT-4.\nIn the context of sequential data, the goal can be to learn the probability distribution of the next token\nin sequence modeling tasks(e.g., language modeling), the probability distribution of a sequence conditioned\non another sequence insequence-to-sequence tasks(e.g., machine translation), or the probability score in a\ntext classification task(e.g., sentiment analysis). As alluded to earlier, the transformer architecture shines\nin such tasks via utilizing the attention mechanism. The distinctions made above between different tasks\nare reflected in the attention type—bidirectional/unmasked or unidirectional/masked attention—and the\npresence or absence ofencoder and decoder stacks in the network [26]. The difference between these two\nstacks is that the former maps into a latent space while the latter takes its inputs from a latent space. Figure\n1, from the original paper on transformers [38], illustrates an encoder-decoder transformer architecture.\nBelow, we briefly discuss the attention mechanism, followed by some other components appearing in that\nillustration, e.g., atokenization step in the context of language tasks.\nMECH. INTERP. FOR LLMS WITH APPLICATIONS TO FINANCIAL SERVICES 3\nThe attention mechanism is based onkey, query and value vectors—which are all learnable. In its simplest\nform, the current token, the one to be predicted, is mapped to a query vectorq; and the tokens in the context\nare mapped to key vectorskt and value vectorsvt (as t varies, different tokens in the context are captured).\nKey and query vectors are of the same dimension, saydattn, while the dimension of the value vectors may be\ndifferent. Indeed, that dimension coincides with the dimension of the output vector (a representation of token\nand context combined) because the output is a linear combination of value vectorsvt. The coefficients of this\ncombination are the entries ofsoftmax\n\u0010\nqTK√dattn\n\u0011\nwhere the softmax function is applied to a normalization\nof a matrix product involving the query vector and the matrixK formed by the key vectors kt. The\nattention mechanism just described was based on a single query; that is, we outlined a singleattention head.\nIn practice, transformers use amulti-head attention mechanism where multiple attention heads are run in\nparallel and their outputs are combined by concatenation and then projection. We refer the reader to [38]\nfor more details, or to [25, 26] for mathematically rigorous treatments of the attention mechanism.\nWe end this subsection by briefly mentioning some of the other components of a transformer model (cf.\nFigure 1). We follow [26] where precise pseudocodes for these components are presented:\n▶ Token Embedding) A vector representation of each vocabulary element (token) is learned.\n▶ Positional Embedding) As a remedy to the lack of recurrence or convolution in transformers, it is\nsuggested to inject information about the position of tokens via adding certain sinusoidal terms to\ninput embeddings [38].\n▶ MLP) Blocks of fully-connected feed-forward neural networks (multi-layer perceptrons) are occa-\nsionally used in a transformer.\n▶ Add & Norm Layers) Residual connectionsand layer normalization are used to help with the van-\nishing gradient problem during training, and to make the training faster and more stable.\n▶ Unembedding) The model learns to convert vector representations of tokens and their contexts to a\ndistribution over vocabulary elements.\n1.2. Large Language Models (LLMs).Recent transformer-based LLMs are systems pre-trained on an\nenormous amount of data with an astronomical numbers of parameters. The table below, adapted from\nsurvey [16], summarizes certain aspects of famous LLMs as of 2024, e.g., their type, number of parameters,\nnumber of tokens etc. We refer the interested reader to lecture notes [5] for an introduction to LLMs.\nThe exposition is aimed at mathematicians and physicists, and discusses the historical pretext and the\nphenomenology of language models among other topics.\nThe advent of LLMs has stimulated conversations and posed urgent questions about this disruptive\ntechnology, including on theemergent properties of LLMs [41], on theiralignment with human values [35],\non thebias present in their training data [10], on thehallucination problem [43], and finally, on the challenge\nof interpreting LLMs [36]. As a matter of fact, LLMs are expected to produce a rapidly growing array of risks\nwhich makes research on safety and governance mechanisms for them even more crucial [2]. In this paper,\nwe focus on the interpretability question, beginning with a short introduction to the field of mechanistic\ninterpretability in the next section.\n2. Mechanistic interpretability\nThenascentfieldofmechanisticinterpretabilityseeksto“reverseengineer”neuralnetworks. Thisendeavor\ncan be construed in parallel with understanding the compiled binary program run on a virtual machine where\nthebinarycodeandvirtualmachine/interpretercorrespondtotheparametersandthearchitectureofaneural\n4 GOLGOON, FILOM, RAVI KANNAN\nTable 1: An overview of popular large language models as of February 2024 (courtesy of [16]).\nnetwork. In this setting, variables/memory locations roughly correspond to neurons or other “independent\nunits” a neural network representation can be decomposed into [21]. For basic resources on the topic, we\nrefer the reader to the guide [18], the glossary [17], the TransformerLens library [19], or the course [14].\n2.1. Motivation. Despite their immense success in various tasks, the lack of transparency of LLMs presents\nchallenges such as hallucination, toxicity, unfairness, and misalignment with human values which can hinder\nsafe deployment of these models. Thus, there is an urgent need for a deeper understanding of the inner\nfunctioning of LLMs. Mechanistic interpretability is an important explanation technique used to this end.\nMECH. INTERP. FOR LLMS WITH APPLICATIONS TO FINANCIAL SERVICES 5\nFigure 2. Curve detector and line detector neurons in early layers form “full curve detector circuits”\nthat can be used to create complex shapes and geometry. The picture is based on experiments with\nthe Inception convolutional architecture. It is from [22] (taken from https://github.com/distillpub/\npost--circuits-zoom-in under the CC-BY 4.0 license).\nIn this paradigm, one strives to understand an LLM at the level of neurons,circuits, and attention heads,\ni.e., at a micro scale (as opposed torepresentation engineering[44, 45] where the explanation occurs at a\nmacro scale). In XAI (eXplainable AI) terminology, mechanistic interpretability can be described as a global,\npost-hoc, model-specific and white-box (i.e., needs access to model’s internals) approach [44]. Apart from\nhelping to address societal risks, insights from mechanistic interpretability can help with “editing” an LLM\nfor better performance. It can furthermore be utilized to explain training phenomena such asgrokking and\nmemorization; cf. [20].\n2.2. Circuits. By analogy with cellular biology, researchers have tried to understand complicated neural\nnetworks by “zooming in” and investigating “the fundamental units” building a network, and the connections\nbetween them [22]. These building blocks are called “features”. Each feature corresponds to a “direction”,\nmeaning a vector in the representation of a layer of the neural net. This can be an individual neuron\nor a linear combination of neurons in a layer. Features are connected to each other by weights to form\n“circuits”. More precisely, a circuit is a computational subgraph of the neural network where each edge\nconnects two neurons/directions in adjacent layers, and comes with weights which are the weights shared\nbetween them in the network. It is claimed (admittedly speculatively) in [22] that features are typically\nmeaningful, i.e., they correspond to articulable properties of the input; and the circuits correspond to\nmeaningful algorithms encoded in neural networks’ weights. These ideas are perhaps better understood in\nthe context of vision models—neurons may pick certain aspects of the input image and then group together\nto form more sophisticated detectors; see Figure 2.\nAbove, we presented a very brief account of features and circuits. Mechanistic interpretability is a rapidly\ngrowing field and many other aspects of these concepts have been studied.\n• An important hurdle to explaining neural networks through circuits is the existence ofpolysemantic\nneurons; that is, neurons that respond to/get activated by multiple unrelated inputs; cf. Figure 3.\nPolysemanticity can be due tosuperposition, meaning when a circuit spreads a feature across many\nneurons (which can be inevitable since there are more features than neurons); see [17, 18] for more\ndetails, and the work [37] on extractingmonosemantic features.\n6 GOLGOON, FILOM, RAVI KANNAN\nFigure 3. An illustration of polysemantic neurons in the context of vision models adapted from [22] (taken\nfrom https://github.com/distillpub/post--circuits-zoom-in under the CC-BY 4.0 license).\nFigure 4. A depiction of the residual stream in a transformer (courtesy of [6]).\n• Universality (or convergent learning) is the claim that analogous features and circuits form across\nmodels and tasks. This hypothesis, inspired by cell theory, clearly shapes the mechanistic inter-\npretability research by suggesting focusing on “universal” neurons, features or circuits. This line of\nresearch has been investigated in [3, 7].\nMECH. INTERP. FOR LLMS WITH APPLICATIONS TO FINANCIAL SERVICES 7\nFigure 5. Top, an illustration of QK (query-key) and OV (output-value) circuits (courtesy of [6]). Bottom,\nan illustration of induction heads’ behavior (courtesy of [23]).\nWe now delve into a more precise treatment of circuits following [6, 14]. In transformers, a layer’s output is\nadded to theresidual stream, i.e., the sum of outputs of the previous layers and the original embedding. The\nresidual stream can be considered as a “communication channel” between different transformer components\nsuchasMLPlayersandattentionheads; seeFigure4. Thefunctionofattentionheadscanthenbeunderstood\nas moving the information: reading from the residual stream of one token, and writing to the residual stream\nof another token. Following [14, chap. 1.1 §2, chap. 1.2 §4], and [6], we argue that each attention head\nconsists of two circuits (Figure 5 (top)):\n• A QK (query-key) circuit “determines where to move information to and from” [14, chap. 1.1 §2].\nFor attention headh, this can be captured by the matrix productWT\nE (Wh\nQ)TWh\nKWE.\n• An OV (output-value) circuit “determines what information to move” [14, chap. 1.1 §2]. For\nattention headh, this can be captured by the matrix productWU Wh\nOWh\nV WE.\nIn what appeared above, matricesWE and WU capture embedding and unembedding of tokens;Wh\nK, Wh\nQ\nand Wh\nV denote the key, query and value matrices of the attention headh; andWh\nO captures the attention\nhead’s output weights. A thorough study of these circuits for transformers with no more than two layers\n8 GOLGOON, FILOM, RAVI KANNAN\nFigure 6. In activation patching, a clean run (e.g.,”When Mary and John went to the store, John gave a\ndrink to ...”in the IOI task) is considered along with a corrupted run for which the answer is flipped (e.g.,\n”When Mary and John went to the store, Mary gave a drink to ...”). The clean run is captured on the left.\nOn the right, an activation from the corrupted run is patched in the corresponding one from the clean run.\n(Pictures courtesy of [14].)\nand no MLP layer can be found in [6]. Great insights can be gained from investigating these toy models:\nwhen there is one transformer layer, or none, there are concrete descriptions in terms of n-gram models,\nwhereas two-layer transformers are much more capable since now attention heads can compose (in three\ndifferent ways corresponding to keys, queries, and values), and this results in creation ofinduction heads\n(Figure 5 (bottom)). Going beyond toy examples, paper [23] studies induction heads for larger models: they\nare implemented by circuits consisting of attention heads in different layers that work together to copy or\ncomplete patterns, and thus, contributing toin-context learning.\nWe focus oncircuit discovery in the next subsection, especially the idea ofalgorithmic tasks which is\nessential to the financial services applications proposed in the last section.\n2.3. Circuit discovery.There is a growing body of literature on circuit discovery for transformers; cf. [4].\nAs a concrete example, the phenomenon of grokking, i.e., sudden transition from overfitting to generalization\nafter numerous training steps, can be explained through circuit formation. This is the content of paper [20]\nwhere grokking is reverse engineered for transformer models trained for simple arithmetic tasks. Also, see\n[8] where grokking and “emergent” abilities in LLMs are studied through the lens of circuits.\nFor our purposes, we shall follow the approach of [39] which is based on algorithmic tasks, an idea that\nwe believe carries over easily to financial services applications. The paper focuses on theIndirect Object\nIdentification (IOI) language task with the GPT-2 Small model ([27]), a language model comprised of 12\nlayers and about 80 million parameters. To give an example of this task, a sentence such as”When Mary\nand John went to the store, John gave a drink to ...”should be completed with the word”Mary” instead of\n”John”. In that paper, a circuit (here, meaning a computational subgraph) of GPT-2 Small, consisting of\n26 attention heads, is identified as being responsible for the IOI task. These are further broken down into 7\nMECH. INTERP. FOR LLMS WITH APPLICATIONS TO FINANCIAL SERVICES 9\ncategories each responsible for a human-understandable purpose, e.g., “duplicate token heads” that identify\ntokens already appeared in the sentence.\nAlgorithmic examples of the IOI instances are constructed as follows (see [39, Appendix E] and [40, IOI\nDataset]):\n• ABC-TEMPLATES: ”Afterwards [A], [B] and [C] went to the [PLACE]. [B] and [C] gave a [OBJECT]\nto [A]”,\n• BABA-TEMPLATES: ”When [B] and [A] got a [OBJECT] at the [PLACE], [B] decided to give the\n[OBJECT] to [A]”,\n• BABA-LONG-TEMPLATES: ”Then in the morning, [B] and [A] went to the [PLACE]. [B] gave a\n[OBJECT] to [A]”,\n• BABA-LATE-IOS: ”Afterwards, [B] and [A] went to the [PLACE]. [B] gave a [OBJECT] to [A]”,\n• BABA-EARLY-IOS: ”After the lunch [B] and [A] went to the [PLACE], and [B] gave a [OBJECT] to\n[A]”,\nsuch that each name was drawn from a list of 100 English first names. The place and the object were chosen\nfrom a hand-made list of 20 common words. Using these templates, a dataset of samples for IOI is created.\nNext, GPT-2 Small’s performance is evaluated on the prompted dataset and mechanistic interpretability of\nthe model pertaining to the IOI task is investigated.\nAfter this high-level overview of [39], we delve into the steps and tools involved in the circuit discovery\ncarried out therein following the exposition in [14, chap. 1.3].\n• Recall that the probabilities that a language model outputs for predicting the next token are obtained\nfrom applying a softmax function to tokens’logit values. The difference of logit values, e.g., the\nexpression logit(\"Mary\") − logit(\"John\") in the case of the IOI example above, can be utilized as a\nperformance metric.\n• The contribution of a layer (a transformer block), and the attention heads inside it, to the residual\nstream can be quantified by considering the logit difference after that layer as if the subsequent layers\nare ignored. Thisdirect logit attributiontechnique identifies heads that contribute the most to the\nresidual stream, and can be conducted with the TransformerLens library [19]. See [14, chap. 1.3 §2]\nfor more details.\n• A more sophisticated approach to end-to-end circuit discovery, which goes beyond just looking at\nwhat happens at the very end of a circuit, isactivation patching, as well as its more refined version\npath patching.\n▶ Let us begin with the activation patching tool, the idea that was first introduced in [15]. Two\ndifferent runs of the model are considered, a “clean” run and a “corrupted” one. The output\nanswers for them are correct and incorrect, respectively. We intervene on a specific activation\nfrom the corrupted run by replacing it with the corresponding activation from the clean run,\nand then measure the change of the output towards the correct answer. This is calleddenoising.\nAlternatively, innoising, one patches from the corrupted run into the clean run; cf. Figure 6.\nThrough trying many different activations and assessing how much they affect the corrupted\nrun, one can identify the activations that really matter. Finally, we point out that patching\ninto a transformer can be done in a number of different ways, for example, into MLP layers,\nattention heads, or the values of the residual stream. We refer the reader to [14, chap. 1.3 §3]\nfor more details.\n▶ The activation patching approach considers the alternative of swapping the contribution of an\nattention head to the residual stream with what it would have been under a different distribution\n10 GOLGOON, FILOM, RAVI KANNAN\nFigure 7. Thinking of nodes as attention heads and edges as representations of how two attention heads\nare tangled in the residual stream, the picture illustrates path patching where edges are replaced (unlike the\nactivation patching where nodes are replaced; cf. Figure 6). (Picture courtesy of [14].)\nwhile everything else remains the same. On the other hand, in path patching, one contemplates\nthe alternative of replacing the input to an attention head from another attention head with\nwhat it would have been under a different distribution; see Figure 7. The path-patching tool,\nthus, investigates the importance of particular paths between a model’s components (e.g., a\ncircuit formed by two attention heads) rather than individual model components (e.g., a single\nattention head). Again, a clean run and a corrupted run are involved. Looking at the IOI task\nagain, instead of replacing”When Mary and John went to the store, John gave a drink to ...”with\n”When Mary and John went to the store, Mary gave a drink to ...”, three random names are used\nas in”When [X] and [Y] went to the store, [Z] gave a drink to ...”—hence the information about\nduplicate tokens is erased. We refer the reader to [14, chap. 1.3 §4] for more details.\n• How does one assess if a discovered circuit is a reliable explanation for the model behavior? Paper\n[39] proposes three evaluation criteria for circuit analysis:\n– Faithfulness: can the circuit perform the task as well as the whole model?\n– Completeness: does the circuit contain all the nodes used to perform the task?\n– Minimality: are all the nodes in the circuit relevant to the task?\n3. Applications in financial services\nAfter the remarkable success of ChatGPT, there have been efforts to design domain-specific large lan-\nguage models tailored to specific sectors such as law, commerce, healthcare and finance (see [11]). Such\nmodels are constructed through pre-training and/or fine-tuning on targeted datasets to perform well-defined\ntasks specific to a particular domain. They can tackle data security issues and prevent AI hallucination in\nspecialized fields [24].\n3.1. LLM interpretability for financial services.In the financial domain, BloombergGPT is an LLM\nwith 50 billion parameters trained on a wide range of financial data [42]. BloombergGPT is not open\nMECH. INTERP. FOR LLMS WITH APPLICATIONS TO FINANCIAL SERVICES 11\nFigure 8. An illustration of the FinGPT ecosystem. Picture adapted from https://github.com/\nAI4Finance-Foundation/FinGPT (under the MIT license).\nsource, and thus, investigating it through the lens of mechanistic interpretability, e.g., circuit discovery, is\nnot practical. Moreover, frequently retraining an LLM model like BloombergGPT is costly. Therefore, there\nis a need for lightweight adaptations. One remarkable adaptation is the FinGPT project which facilitates\nprompt engineering and fine-tuning of open-source LLMs on financial data [13] for a variety of financial\napplications such as risk management, portfolio optimization, credit scoring, and fraud detection.\nIn the financial services industry, LLMs, and, more generally, NLP models, have been used for various\ncustomer-service related tasks. In what follows, we outline a few industry-wide compliance, legal and business\nuse cases.1\n• An existing large language model, such as BERT, GPT-2, or LLaMA-2, may be utilized, through\nfine-tuning, prompt engineering orRetrieval Augmented Generation(RAG) for in-house legal and\ncompliance applications such as processing legal documents and correspondence, conducting sen-\ntiment analysis to monitor compliance with (or potential violations of) a variety of regulations\noverseeing financial services. For instance:\n– Monitoring cases and complaints involving Unfair or Deceptive Acts or Practices (UDAAP)\n[29];\n– Detecting cases related to Telephone Consumer Protection Act (TCPA) [30], e.g., circumstances\nwhere customers requests involve TCPA regulations;\n1Disclaimer: The authors have neither developed/fine-tuned any large language model for these use cases nor have they used\nany customer data or internal data from Discover Financial Services for their experiments.\n12 GOLGOON, FILOM, RAVI KANNAN\n– Compliance monitoring for regulations concerning military lending protections (e.g., Military\nLending Act (MLA) [31] and Servicemembers Civil Relief Act (SCRA) [32]);\n– Compliancemonitoringforconsumerprivacyprotection(asrequiredbytheCaliforniaConsumer\nPrivacy Act (CCPA) [28]);\n– Compliance monitoring for fair lending (FL) laws (as required by the Equal Credit Opportunity\nAct (ECOA) [34] and Fair Housing Act (FHA) [33]).\n• An existing large language model can be used (through fine tuning, for example) to classify customer-\nagent transcripts with business-related tags such as “website/mobile”, “login issues”, “hard inquiry\",\n“balance too high” etc.\nNow, consider the problem of explaining an LLM model applied to a regulatory, legal, or business use case\nin financial services. Inspired by [39], the approach we adapt here is to design algorithmic tasks. In what\nfollows, we describe how financial algorithmic tasks can be designed by working on some examples. These\ntasks can often be decomposed into a set of rules. These rulesets are based on legal requirements and business\ncriteria and are often handcrafted by subject-matter experts. Below, we describe these rules for some of the\ncompliance tasks described above followed by some algorithmic examples that can be constructed.\n• TCPA rules) The TCPA regulation [30] prohibits unwanted communication with consumers. In\ncompliance with this, certain conversations should be classified as critical governed by TCPA. One\nmay design the following algorithmic examples for TCPA use cases in order to generate datasets to\nbe used for mechanistic interpretability investigations:\n▶ MARKETING-CALL-TEMPLATES: ”The agent reaches out to the customer regarding a new\n[FINANCIAL-PRODUCT]. The customer says I’m not interested. Please remove me from your\ncall list.”—[FINANCIAL-PRODUCT] consists of: credit card, auto loan, mortgage loan, checking\naccount, savings account.\n▶ COMMUNICATION-TEMPLATES:”I don’t want to receive any [WAY-of-COMMUNICATIONS]\nfrom you.”—[WAY-of-COMMUNICATIONS]is given by: calls, mails, text messages, messages,\nemails, notifications, communications, further communications, etc.\n▶ STOP-CONTACT-LIST-TEMPLATE:“Please add my [PROFILE] to the do-not-call lists. I don’t\nwant to be contacted anymore.”—[PROFILE] consists of the customer contact details such as:\nnumber, phone number, personal number, work number, email address, personal email, mail address,\netc.\n▶ DEBT-COLLECTION-CALL-TEMPLATE:“The agent notifies the customer regarding an\n[OVERDUE-BALANCE] on her account and offers a payment plan. The customer says I don’t want\nto receive calls about this anymore.”—[OVERDUE-BALANCE] is given by:missed payment, missed\nminimum payment, outstanding balance, overdue balance, unpaid balance, delinquent balance, etc.\n• UDAAP rules) UDAAP protects consumers against the risks of harm from unfair, deceptive, or\nabusive practices by financial institutions. Harm does not have to be monetary and can result from\nincreased difficulty of consumer understanding of the overall costs or risks of the product and the\npotential harm to the consumer associated with the product. Financial institutions monitor their\ncustomer-agent interactions. When the consumer suggests potential UDAAP (Unfair, Deceptive, or\nAbusive Acts or Practices) concerns, such interactions need to be identified for further review by a\nfinancial institution. One may use the following criteria to define UDAAP as an algorithmic financial\ntask:\nMECH. INTERP. FOR LLMS WITH APPLICATIONS TO FINANCIAL SERVICES 13\n– UDAAPKeywords: Unfair/notfair,deceptive/deceitful,abusive/abuse/abused,tricky/trick/tricked,\ncheating/cheat/cheated, misleading/mislead/misled, lying/lie/lied, taking/took advantage of me\nbecause I’m a [member of a vulnerable population](e.g., students and minority groups).\nTherefore, we design the following algorithmic examples for UDAAP circuit discovery:\n▶ UNFAIR-PRACTICES: The customer is interested in opening a checking account and asks\nabout overdraft protection. The agent mentions that the account comes with overdraft pro-\ntection but fails to clearly disclose the high fees associated with overdraft transactions. Ex-\nample: ”You are [UDAAP-BEHAVIOR]. You did not disclose the high fees associated with your\n[FINANCIAL-PRODUCT].”—[UDAAP-BEHAVIOR] can be replaced with the followings: unfair,\ndeceptive, deceitful, abusive, misleading, fraud, liars etc.\n▶ DECEPTIVE-PRACTICES: The customer is interested in a personal loan and the agent men-\ntions that they offer personal loans with APR as low as4.99%. Example: ”You are [UDAAP-\nBEHAVIOR]. You did not disclose that the rate4.99% only applies to applicants with excellent credit\nscores and my rate is significantly higher. Your loans also come with hidden origination fees that\nthe agent did not mention.\"\n▶ ABUSIVE-PRACTICES: The agent engages in aggressive debt collection by notifying the cus-\ntomer that he is 30 days overdue on his credit card payment and he must immediately pay\nhis debt, or the company will take legal actions against him. Example: \"You are [UDAAP-\nBEHAVIOR]. You are asking me to pay the full amount now and are threatening me that you will\ngarnish my wages and reporting me to credit bureaus.\"\n• Fair Lendingrules) These laws set forth prohibited bases (i.e., protected classes) against which a\nfinancial institution cannot discriminate.\n– Protected classes:\n∗ Race, national origin, age, disability, religion, sex, marital or familial status (i.e., having\nchildren).\n∗ An applicant’s receipt of income from public assistance or their good faith exercise of any\nright under the Consumer Credit Protection Act.\n∗ States may have broader protected classes under their respective fair lending laws, in-\ncluding but not limited to military status, ancestry, domestic violence victim status, or\npolitical affiliation.\n– Examples of Potential Discrimination:\n∗ ”I requested for a credit line increase multiple times, and you said no because I’m on unem-\nployment.”\n∗ ”Why are you denying all my requests for a payment plan. It’s because I’m a single mother.”\n∗ ”I don’t understand why I have all these late and hidden fees. I think you’re trying to take\nadvantage of me because I’m disabled.”\n∗ ”You are quoting me a much higher rate than my neighbor when we have the same credit\nscore and income. It’s because I’m old!”\nWe design the following algorithmic examples for Fair Lending circuit discovery:\n▶ ”I don’t understand why I have all these late fees. I think you’re trying to [UDAAP-VERB] me\nbecause I’m a [MEMBER-of-PROTECTED-CLASS].”—[UDAAP-VERB] is given by: trick, cheat,\ndeceit, abuse, mislead, defraud, taking advantage of.\n▶ Note that [MEMBER-of-PROTECTED-CLASS] can be an item fromelderly, single mother, dis-\nabled, minority, etc.\n∗ ”You are taking advantage of me because I’m a [MEMBER-of-PROTECTED-CLASS];”\n14 GOLGOON, FILOM, RAVI KANNAN\n∗ ”You are charging me a much higher rate than my neighbor. It’s because I’m a [MEMBER-\nof-PROTECTED-CLASS];”\n∗ ”I asked for a credit line increase, and you denied my request because I’m a [MEMBER-of-\nPROTECTED-CLASS].”\n▶ The customer is looking for a loan to expand their small business. The agent provides less favor-\nable loan terms to the business. Example:\"You are providing us with a less favorable loan terms\ncompared to other similar businesses with similar business plans and finances. Is this because we\nare a [MINORITY-OWNED] business?\"—[MINORITY-OWNED] can be given by:women-owned,\nblack-owned, LGBTQ+-owned, etc.\n3.2. Numerical Experiments. In this section, we design mechanistic interpretability examples for com-\npliance tasks in financial services. For the sake of simplicity, we choose GPT-2 Small [27] as the model to\nstudy mechanistically following the methods presented in [39]. For the rest of this section, we adapt the\nsteps presented in [14] for the IOI task and tailor it to the compliance tasks described in §3.1.\nWe focus on understanding financial language models tasks involving Fair Lending laws discussed before.\nOur goal is to analyze GPT-2 Small attention pattern in identifying potential violation of Fair Lending Laws\nrelated to the Equal Credit Opportunity Act (ECOA) [34] and the Fair Housing Act (FHA) [33]. In doing so,\nwe design prompts of the following format:\"The customer says on the phone that you are denying my request\nfor a payment plan because I’m on unemployment.\"; and then we ask\"Is this is an example of a Fair Lending\nviolation based on Equal Credit Opportunity Act (ECOA)?\". Next, we measure model performance by defining\nthe following Logit Difference2 metric: logit(\"Yes\") − logit(\"No\"). Furthermore, we define another metric\nwhere we compare the\"Yes\" and \"No\" token probabilities of the final output, i.e.,P(\"Yes\")/P(\"No\"). The\nprompts that we investigate are illustrated in Table 2. The corresponding logit differences and normalized\nprobability ratios for the prompts are presented in Table 3. The average logit difference and probability\nratio are0.81 and 2.26, respectively.\nNext, we study the model by performing direct logit attribution [39]. More specifically, we investigate\nthe contributions of each layer (and its corresponding attention heads) to the logit difference of the residual\nstream.\nFigure 9, illustrates the calculated logit difference by decomposing the residual stream after each layer\n(see logit lensfrom [19]).3 As one can see the logit difference generally keeps improving through the layers.\nThis is in contrast to the IOI task where one can localize the task completion to merely a few final layers,\ni.e., layers7, 8, and9 [14]. We believe that this is due to the fact that our compliance task at hand is far\nmore complex when compared to the IOI task which involves moving information around from the indirect\nobject and not the subject, requiring less information processing.\nWe may investigate the logit difference between adjacent residual streams (see Figure 10). One can realize\nthat unlike the IOI task, where only attention layers matter, MLP layers play a significant role in performing\nthe financial compliance task. In particular, MLP layers2 and 4 improve task completion greatly, while\nMLP layer10 and attention layer0 decrease performance.\nNext, we further decompose the output of each attention layer into the sum of the outputs of the corre-\nsponding attention heads (see Figure 11). As expected, there are quite a few attention heads that play a\n2Note that this choice of metric was inspired by [39] where they study the logit difference between the indirect object and\nthe subject tokens for the circuit discovery of the IOI task.\n3Recall from §2.3 that, one can interpret the result of residual stream decomposition and calculating logit differences as\nsimulating what would happen if subsequent layers are removed after each layer [14].\nMECH. INTERP. FOR LLMS WITH APPLICATIONS TO FINANCIAL SERVICES 15\nTable 2: An illustration of the Fair Lending rule financial task investigation. Prompts and the answers.\nTable 3: Final token logits, token probabilities, and token ranks for the\"Yes\" and \"No\" tokens, along with\nthe logit differences and the normalized probability ratios.\nsubstantial role in the compliance financial task completion. Of note, heads4 11.4, 8.9, and 6.3 contribute a\nlot positively, while heads 0.6, 11.0, and 10.7 contribute a lot negatively to the task completion (see Figure\n12).\nAs mentioned in §2.3, one of the limitations of the direct logit attribution discussed before is that it\nonly looks at the very end of the circuit which affects the logits directly [14, 39]. To obtain a more refined\nunderstanding, in what follows, we utilize the activation patching technique. In doing so, we design the\nso-called clean prompts (Figure 13), and corrupted prompts (Figure 14). The format for the clean prompts\nis given by:\"[A]’s credit score is 600. He was qualified for a loan of amount$80, 000. [B]’s credit score is 800.\n4Note we use the convention thatl.h specifies the head numberh in the layer numberl (indexing starts from0). There are\n144(12 × 12) heads in total in GPT-2 Small.\n16 GOLGOON, FILOM, RAVI KANNAN\nFigure 9. Calculated logit difference for the decomposed accumulated residual stream after each layer.\nn-pre denotes the residual stream at the start of layern, whilen-mid denotes the residual stream after the\nattention part of layern.\nFigure 10. Break down of logit differences from each layer between adjacent residual streams.\nMECH. INTERP. FOR LLMS WITH APPLICATIONS TO FINANCIAL SERVICES 17\nFigure 11. Attention heads heat map per layer illustrating logit difference from each head.\nFigure 12. Attention patterns for the top three positive and top three negative logit attribution heads.\n18 GOLGOON, FILOM, RAVI KANNAN\nFigure 13. Logit difference for \"Yes\" (ranked 1st) and \"No\" (ranked 2nd) answer tokens for the clean\nprompts.\nFigure 14. Logit difference for\"Yes\" (ranked 2nd) and\"No\" (ranked 1st) answer tokens for the corrupted\nprompts.\nShe was qualified for a loan of amount$40, 000. [B] and [A] have the same income. Is this a gender-based fair\nlending violation?\", where [A] and [B] are, respectively, predominately male-associated and female-associated\nnames. Similarly, the corrupted prompt is given by:\"[A]’s credit score is 600. He was qualified for a loan\nof amount $80, 000. [C]’s credit score is 800. He was qualified for a loan of amount $40, 000. [A] and [C]\nhave the same income. Is this a gender-based fair lending violation?\", where [A] and [C] are predominately\nmale-associated names. The clean run favors\"Yes\" over \"No\" as an output response, and thus, results in\npositive logit differences, while the corrupted run favors\"No\" over \"Yes\" as an output response resulting in\nnegative logit differences. Next, we run the model on our corrupted prompts and then intervene by patching\nin the activations from our clean prompts and monitor how the output response changes.\nFigure 15 depicts the results for residual stream patching at the start of each layer for all the61 token\npositions. A score closer to0 means the performance is closer to the one obtained on the corrupted input,\nwhile a score closer to1 means that the performance is closer to that of the clean input. We are trying to look\nfor activations that are sufficient to recover the correct response. One can make the following observations:\n(i) The computation is fairly localized to positions:22 (\"[B]\" in \"[B]’s credit score ...\"), 29 (\"She\" in \"She was\nqualified ...\"), 42 (\"[B]\" in \"[B] and [A] ...\"), 44 (\"[A]\" in\"[B] and [A] ...\"), 54 (\"gender\" in \"gender-based ...\"),\nMECH. INTERP. FOR LLMS WITH APPLICATIONS TO FINANCIAL SERVICES 19\nFigure 15. Residual stream patching at the onset of each layer for each token positions averaged over all4\nprompts. On the x-axis we only show the labels for the first prompt.\nFigure 16. Patching in residual stream after the attention layer (in the middle) and after the MLP layer\n(on the right).\nand END tokens. (ii) One can see that the information at token22 starts to be moved toEND token around\nlayers 5 and 6.\nInstead of just patching to the residual stream at the onset of each layer, we may use activation patching\nfor patching after the attention layer and after the MLP layer. Figure 16 illustrates the results. As one can\nsee layers8 and 10 contribute positively, while layer9 contributes negatively to the performance. Among\nMLP layers, we observe thatMLP-05 plays an important role, along withMLP-11.\nNext, we can further refine our analysis (see Figure 17) by patching in on individual attention heads for\nall layers and tokens. We see that the heads10.2, 10.7, and11.3, from later layers, and the heads0.4, 1.7,\n5Note that this is consistent with the observations from [39, 14], thatMLP-0 matters a lot. This behavior is often observed\nin GPT-2 Small.\n20 GOLGOON, FILOM, RAVI KANNAN\nFigure 17. Patching in individual attention heads output for all layers and sequence positions.\nFigure 18. Decomposing heads by patching on the query vectors, key vectors and value vectors.\nand 2.1, from earlier layers, have very large positive scores. On the other hand, the heads9.6 and 10.6, from\nlater layers, and the heads0.10 and 5.0, from earlier layers, have very large negative scores.\nFinally, instead of just patching on a head’s output, we decompose the attention heads by patching into\nthe building blocks of the attention mechanism, i.e., key vectors, query vectors, value vectors, and attention\npatterns (see Figure 18). We can deduce the following observations: (i) Some of the layer-0 heads such as\n0.5, 0.1 and 0.10 are very important because of their query and key vectors. (ii) The head10.2 is significantly\nimportant due to its value vector. (iii) Other important attention heads owing to their value vectors are9.6,\n9.9, 10.10, 11.3, and11.10. (iv) Thus, we can conclude that value patching has a more significant effect than\nkey (or query) patching for the important heads in later layers, i.e., layers9 − 11.\nIn summary, we investigated GPT-2 Small’s attention pattern for identifying potential violation of Fair\nLending laws. Employing the direct logit attribution technique, we analyzed the contributions of each layer to\nthe logit difference in the residual stream. In particular, we identified the heads11.4, 8.9, and6.3 (0.6, 11.0,\nand 10.7) that contribute a lot positively (negatively) to the compliance task completion. We performed a\ncasual intervention through utilizing the activation patching technique by designing the clean and corrupted\nprompts. We refined our analysis to the find which components of the attention mechanism are important\nMECH. INTERP. FOR LLMS WITH APPLICATIONS TO FINANCIAL SERVICES 21\nfor each attention head. Of note, we found that the head10.2 is very important due to its value vector,\nwhile some of the layer-0 heads such as0.5, 0.1, and0.10 are very important owing to their query and key\nvectors.\n4. Future Directions\nWe consider this paper to be the first step towards leveraging mechanistic interpretability techniques for\nunderstanding applications of LLMs in financial services. Some of the future research directions include: (i)\nConducting experiments with more powerful open-source LLMs such as Mistral 7B and Llama-2 7B. This\nis where one may leverage fine-tuned open-source models from FinGPT project [13]. (ii) Working on more\nalgorithmic examples of compliance tasks in financial services and preferably finding prompts that result in\nlarge logit differences. (iii) Perform path patching as a more sophisticated circuit discovery approach toward\nfinding an end-to-end circuit for performing a compliance financial task.\nAcknowledgement\nWe would like to thank Sharon O’Shea Greenbach (Sr. Counsel & Director, Regulatory Policy at DFS)\nand Dennis Lee (Chief IP Counsel & AGC Privacy & Security at DFS) for their helpful comments relevant\nto regulatory issues that arise in the financial industry and for carefully reviewing the manuscript. We\nalso thank Callum McDougall, Shervin Minaee, and Christopher Olah for granting permission to use some\nof the figures and tables from their papers. Ashkan Golgoon benefited from stimulating discussions with\nAmirhossein Tajdini. The views and opinions expressed in this paper are solely our own and do not represent\nor reflect those of our employer.\nReferences\n[1] D. Bartz. As ChatGPT’s popularity explodes, U.S. lawmakers take an interest. Reuters, https://www.reuters.com/\ntechnology/chatgpts-popularity-explodes-us-lawmakers-take-an-interest-2023-02-13 , 2023.\n[2] S. R. Bowman. Eight Things to Know about Large Language Models.arXiv e-prints, page arXiv:2304.00612, Apr. 2023.\n[3] B. Chughtai, L. Chan, and N. Nanda. A Toy Model of Universality: Reverse Engineering How Networks Learn Group\nOperations. arXiv e-prints, page arXiv:2302.03025, Feb. 2023.\n[4] A. Conmy, A. Mavor-Parker, A. Lynch, S. Heimersheim, and A. Garriga-Alonso. Towards automated circuit discovery for\nmechanistic interpretability.Advances in Neural Information Processing Systems, 36:16318–16352, 2023.\n[5] M. R. Douglas. Large Language Models.arXiv e-prints, page arXiv:2307.05782, July 2023.\n[6] N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, N. DasSarma,\nD. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown,\nJ. Clark, J. Kaplan, S. McCandlish, and C. Olah. A Mathematical Framework for Transformer Circuits.Transformer\nCircuits Thread, 2021.https://transformer-circuits.pub/2021/framework/index.html.\n[7] W. Gurnee, T. Horsley, Z. C. Guo, T. Rezaei Kheirkhah, Q. Sun, W. Hathaway, N. Nanda, and D. Bertsimas. Universal\nNeurons in GPT2 Language Models.arXiv e-prints, page arXiv:2401.12181, Jan. 2024.\n[8] Y. Huang, S. Hu, X. Han, Z. Liu, and M. Sun. Unified View of Grokking, Double Descent and Emergent Abilities: A\nPerspective from Circuits Competition.arXiv e-prints, page arXiv:2402.15175, Feb. 2024.\n[9] E. Klein. This Changes Everything. New York Times , https://www.nytimes.com/2023/03/12/opinion/\nchatbots-artificial-intelligence-future-weirdness.html , 2023.\n[10] Y. Li, M. Du, R. Song, X. Wang, and Y. Wang. A survey on fairness in large language models. arXiv preprint\narXiv:2308.10149, 2023.\n[11] Y. Li, S. Wang, H. Ding, and H. Chen. Large language models in finance: A survey. InProceedings of the fourth ACM\ninternational conference on AI in finance, pages 374–382, 2023.\n[12] T. Lieu. I’m a Congressman Who Codes. A.I. Freaks Me Out.New York Times, https://www.nytimes.com/2023/01/23/\nopinion/ted-lieu-ai-chatgpt-congress.html , 2023.\n22 GOLGOON, FILOM, RAVI KANNAN\n[13] X.-Y. Liu, G. Wang, H. Yang, and D. Zha. Data-centric FinGPT: Democratizing Internet-scale Data for Financial Large\nLanguage Models.NeurIPS Workshop on Instruction Tuning and Instruction Following, 2023.\n[14] C. McDougall. ARENA (Alignment Research Engineer Accelerator) 3.0 [accessed June 2024]. https:\n//arena3-chapter1-transformer-interp.streamlit.app/ ;https://arena-ch1-transformers.streamlit.app/;https:\n//github.com/callummcdougall/ARENA_2.0, 2024.\n[15] K. Meng, D. Bau, A. Andonian, and Y. Belinkov. Locating and editing factual associations in gpt.Advances in Neural\nInformation Processing Systems, 35:17359–17372, 2022.\n[16] S. Minaee, T. Mikolov, N. Nikzad, M. Chenaghlu, R. Socher, X. Amatriain, and J. Gao. Large Language Models: A Survey.\narXiv e-prints, page arXiv:2402.06196v2, Feb. 2024.\n[17] N. Nanda. A Comprehensive Mechanistic Interpretability Explainer & Glossary [accessed June 2024].https://neelnanda.\nio/glossary, Dec. 2022.\n[18] N. Nanda. Mechanistic Interpretability Quickstart Guide [accessed June 2024]. https://www.lesswrong.com/posts/\njLAvJt8wuSFySN975/mechanistic-interpretability-quickstart-guide , Jan. 2023.\n[19] N. Nanda and J. Bloom. TransformerLens.https://github.com/neelnanda-io/TransformerLens, 2022.\n[20] N. Nanda, L. Chan, T. Lieberum, J. Smith, and J. Steinhardt. Progress measures for grokking via mechanistic inter-\npretability.arXiv e-prints, page arXiv:2301.05217, Jan. 2023.\n[21] C. Olah. Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases. https://www.\ntransformer-circuits.pub/2022/mech-interp-essay , 2022.\n[22] C. Olah, N. Cammarata, L. Schubert, G. Goh, M. Petrov, and S. Carter. Zoom In: An Introduction to Circuits.Distill,\n2020. https://distill.pub/2020/circuits/zoom-in.\n[23] C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, T. Con-\nerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones, J. Kernion, L. Lovitt, K. Ndousse,\nD. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. In-context learning and induction heads.Trans-\nformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/\nindex.html.\n[24] S. Park. Real-world examples of ‘Domain-Specific LLMs’: Bring tailored AI to your business [accessed June 2024].https:\n//www.upstage.ai/feed/insight/examples-of-domain-specific-llms , Feb. 2024.\n[25] C. Penke. A mathematician’s introduction to transformers and large language models. Technical report, Jülich Supercom-\nputing Center, 2022.\n[26] M. Phuong and M. Hutter. Formal Algorithms for Transformers.arXiv e-prints, page arXiv:2207.09238v1, July 2022.\n[27] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners.\nOpenAI blog, 1(8):9, 2019.\n[28] California Civil Code. California Consumer Privacy Act.https://cppa.ca.gov/regulations/pdf/cppa_act.pdf, 2018.\n[29] Consumer Financial Protection Bureau (CFPB). Unfair, Deceptive, Or Abusive Acts Or Practices. https:\n//files.consumerfinance.gov/f/documents/cfpb_unfair-deceptive-abusive-acts-practices-udaaps_procedures_\n2023-09.pdf, Oct. 2012.\n[30] Federal Communications Commission (FCC). Telephone Consumer Protection Act.https://www.fcc.gov/sites/default/\nfiles/tcpa-rules.pdf, 1991.\n[31] Federal Deposit Insurance Corporation (FDIC). Military Lending Act. https://www.fdic.gov/resources/\nsupervision-and-examinations/consumer-compliance-examination-manual/documents/5/v-13-1.pdf , 2006.\n[32] U.S. Department of Justice (DoJ). Servicemembers Civil Relief Act. https://www.justice.gov/crt/\nservicemembers-civil-relief-act-summary , 1940.\n[33] U.S. Department of Justice (DoJ). Fair Housing Act.https://www.justice.gov/crt/fair-housing-act-1 , 1968.\n[34] U.S. Department of Justice (DoJ). Equal Credit Opportunity Act. https://www.govinfo.gov/content/pkg/\nUSCODE-2011-title15/html/USCODE-2011-title15-chap41-subchapIV.htm , 2011.\n[35] T. Shen, R. Jin, Y. Huang, C. Liu, W. Dong, Z. Guo, X. Wu, Y. Liu, and D. Xiong. Large language model alignment: A\nsurvey.arXiv preprint arXiv:2309.15025, 2023.\n[36] C. Singh, J. P. Inala, M. Galley, R. Caruana, and J. Gao. Rethinking interpretability in the era of large language models.\narXiv preprint arXiv:2402.01761, 2024.\n[37] A. Templeton, T. Conerly, J. Marcus, J. Lindsey, T. Bricken, B. Chen, A. Pearce, C. Citro, E. Ameisen, A. Jones,\nH. Cunningham, N. L. Turner, C. McDougall, M. MacDiarmid, C. D. Freeman, T. R. Sumers, E. Rees, J. Batson,\nA. Jermyn, S. Carter, C. Olah, and T. Henighan. Scaling monosemanticity: Extracting interpretable features from claude\n3 sonnet.Transformer Circuits Thread, 2024.\n[38] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all\nyou need.Advances in neural information processing systems, 30, 2017.\nMECH. INTERP. FOR LLMS WITH APPLICATIONS TO FINANCIAL SERVICES 23\n[39] K. R. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. Interpretability in the Wild: a Circuit for Indirect\nObject Identification in GPT-2 small. InNeurIPS ML Safety Workshop, 2022.\n[40] K. R. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. Redwood Research Easy-Transformer.https:\n//github.com/redwoodresearch/Easy-Transformer/, 2022.\n[41] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, et al.\nEmergent abilities of large language models.arXiv preprint arXiv:2206.07682, 2022.\n[42] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann, P. Kambadur, D. Rosenberg, and G. Mann.\nBloombergGPT: A Large Language Model for Finance.arXiv e-prints, page arXiv:2303.17564, Mar. 2023.\n[43] H. Ye, T. Liu, A. Zhang, W. Hua, and W. Jia. Cognitive mirage: A review of hallucinations in large language models.\narXiv preprint arXiv:2309.06794, 2023.\n[44] H. Zhao, F. Yang, B. Shen, H. Lakkaraju, and M. Du. Towards Uncovering How Large Language Model Works: An\nExplainability Perspective.arXiv e-prints, page arXiv:2402.10688, Feb. 2024.\n[45] A. Zou, L. Phan, S. Chen, J. Campbell, P. Guo, R. Ren, A. Pan, X. Yin, M. Mazeika, A.-K. Dombrowski, et al. Represen-\ntation engineering: A top-down approach to ai transparency.arXiv preprint arXiv:2310.01405, 2023.",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.9530254602432251
    },
    {
      "name": "Financial services",
      "score": 0.6791337728500366
    },
    {
      "name": "Business",
      "score": 0.44170263409614563
    },
    {
      "name": "Financial modeling",
      "score": 0.4102582633495331
    },
    {
      "name": "Computer science",
      "score": 0.3794441819190979
    },
    {
      "name": "Finance",
      "score": 0.3677715063095093
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2954137325286865
    }
  ],
  "institutions": [],
  "cited_by": 2
}