{
    "title": "In-context Learning of Large Language Models for Controlled Dialogue Summarization: A Holistic Benchmark and Empirical Analysis",
    "url": "https://openalex.org/W4389524299",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2112719261",
            "name": "Yuting Tang",
            "affiliations": [
                "Nanyang Technological University"
            ]
        },
        {
            "id": "https://openalex.org/A2891163760",
            "name": "Ratish Puduppully",
            "affiliations": [
                "Institute for Infocomm Research"
            ]
        },
        {
            "id": "https://openalex.org/A2313970301",
            "name": "Zhengyuan Liu",
            "affiliations": [
                "Institute for Infocomm Research"
            ]
        },
        {
            "id": "https://openalex.org/A2041529670",
            "name": "Nancy Chen",
            "affiliations": [
                "Institute for Infocomm Research"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3093517588",
        "https://openalex.org/W4389519092",
        "https://openalex.org/W2989743967",
        "https://openalex.org/W4377865110",
        "https://openalex.org/W4362707004",
        "https://openalex.org/W4319793767",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4308244616",
        "https://openalex.org/W4385572967",
        "https://openalex.org/W3111372071",
        "https://openalex.org/W3204515301",
        "https://openalex.org/W4380136141",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W4386290290",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W4312205996",
        "https://openalex.org/W4389009433",
        "https://openalex.org/W2973049837",
        "https://openalex.org/W4304208854",
        "https://openalex.org/W2963096510",
        "https://openalex.org/W3169117666",
        "https://openalex.org/W4361230777",
        "https://openalex.org/W4385567149"
    ],
    "abstract": "Large Language Models (LLMs) have shown significant performance in numerous NLP tasks, including summarization and controlled text generation. A notable capability of LLMs is in-context learning (ICL), where the model learns new tasks using input-output pairs in the prompt without any parameter update. However, the performance of LLMs in the context of few-shot abstractive dialogue summarization remains underexplored. This study evaluates various state-of-the-art LLMs on the SAMSum dataset within a few-shot framework. We assess these models in both controlled (entity control, length control, and person-focused planning) and uncontrolled settings, establishing a comprehensive benchmark in few-shot dialogue summarization. Our findings provide insights into summary quality and model controllability, offering a crucial reference for future research in dialogue summarization.",
    "full_text": "Proceedings of The 4th New Frontiers in Summarization Workshop, pages 56–67\nDecember 6, 2023 ©2023 Association for Computational Linguistics\nIn-context Learning of Large Language Models for\nControlled Dialogue Summarization:\nA Holistic Benchmark and Empirical Analysis\nYuting Tang†∗, Ratish Puduppully §‡, Zhengyuan Liu §‡, Nancy F. Chen §‡\n†Nanyang Technological University, Singapore §CNRS@CREATE, Singapore\n‡Institute for Infocomm Research (I2R), A*STAR, Singapore\nAbstract\nLarge Language Models (LLMs) have shown\nsignificant performance in numerous NLP\ntasks, including summarization and controlled\ntext generation. A notable capability of LLMs\nis in-context learning (ICL), where the model\nlearns new tasks using input-output pairs in the\nprompt without any parameter update. How-\never, the performance of LLMs in the context\nof few-shot abstractive dialogue summarization\nremains underexplored. This study evaluates\nvarious state-of-the-art LLMs on the SAMSum\ndataset within a few-shot framework. We as-\nsess these models in both controlled (entity con-\ntrol, length control, and person-focused plan-\nning) and uncontrolled settings, establishing\na comprehensive benchmark in few-shot dia-\nlogue summarization. Our findings provide in-\nsights into summary quality and model control-\nlability, offering a crucial reference for future\nresearch in dialogue summarization.\n1 Introduction\nAbstractive dialogue summarization aims to dis-\ntill human conversations into natural, concise, and\ninformative text, and is a challenging and interest-\ning task in text summarization (Chen and Yang,\n2020; Liu et al., 2021). The major challenges\ncome from several aspects: 1) it lacks large human-\nannotated datasets unlike document summarization\n(Feng et al., 2021), and 2) it requires responses\nto be not only fluent but also factually consistent\n(Liu and Chen, 2022; Wang et al., 2022). More-\nover, in practical use cases, users may impose addi-\ntional constraints on system outputs, and this task\nis known as controlled dialogue summarization,\nwhich requires models to be capable of coherent\nand flexible language generation.\nIn controlled dialogue summarization, users can\nspecify desired attributes (i.e., control signals) to\nguide the response of language models. Previ-\nous works have explored to incorporate control\n∗Work done during internship at I2R, A*STAR.\nsignals during pre-training (Keskar et al., 2019),\ntask-specific fine-tuning (Liu and Chen, 2021), and\nprompt tuning (Zhang et al., 2022b). Meanwhile,\nthe advancements in LLMs have unveiled new\nparadigms. For instance, instruction tuning, which\nenables models to understand users’ intent in nat-\nural language, is considered to be promising for\nconditional text generation (Zhang et al., 2023).\nAdditionally, the emergence of in-context learn-\ning (ICL) in LLMs has recently gained attention.\nThe ICL ability refers to learning from a few input-\noutput pairs written in the natural language form\n(also called demonstrations) (Dong et al., 2023).\nFollowed by demonstrations, a query question is\nappended at the end to form a complete prompt.\nCompared to the traditional supervised learning,\nICL requires no training and only a few annotated\nsamples. Motivated by the paradigm shift with\nLLMs and the challenges encountered in controlled\ndialogue summarization, this study answers the fol-\nlowing two key questions:\n• How is the quality of the dialogue summaries\ngenerated by LLMs via ICL?\n• How is the controllability of LLMs in dia-\nlogue summarization?\nWe comprehensively evaluate a range of recent\nLarge Language Models (LLMs) on the SAMSum\ndataset (Gliwa et al., 2019) using a few-shot frame-\nwork. Our assessment covers several controlled sce-\nnarios, including entity control, length control, and\nperson-focused planning, as well as uncontrolled\nsettings. We establish a comprehensive benchmark\nfor few-shot dialogue summarization in Section 2,\nand elaborate on the findings in Section 3. Specif-\nically, in our experiments, we observe that LLMs\ncan summarize dialogues reasonably given several\ndemonstrations, and LLaMA and Alpaca achieve\na factual consistency rate exceeding 90% in the\nautomatic evaluation. Moreover, adding control\n56\nModel Architecture Instruction-tuned Training Data\nOPT (Zhang et al., 2022a) Decoder-only RoBERTa + The Pile + Reddit\nOPT-IML (Iyer et al., 2022) Decoder-only ✓ OPT-IML Bench\nmT5 (Xue et al., 2021) Encoder-Decoder mC4\nCEREBRAS-GPT (Dey et al., 2023) Decoder-only The Pile\nLLaMA (Touvron et al., 2023) Decoder-only CommonCrawl + C4 + Github, etc.\nAlpaca (Taori et al., 2023) Decoder-only ✓ Instruct dataset generated by GPT-3\nBLOOM (BigScience Workshop, 2022) Decoder-only ROOTS\nTable 1: Summary of the experimented LLMs.\nSummary of SAMSum Dataset\nTraining Set 14,732 samples\nValidation Set 818 samples\nTesting Set 819 samples\nLanguage English\nAnnotation Method Manual\nTable 2: Data details of the SAMSum dataset.\nsignals in prompts (particularly keywords) can ef-\nfectively guide models to include key information\nin generated summaries.\n2 Our Experimental Setting of ICL\nDialogue Summarization\nIn this section, we describe how we establish the\nbenchmark of evaluating LLMs’ in-context learn-\ning for abstractive dialogue summarization.\n2.1 Selected Models & Prompt Template\nTo conduct an extensive comparison, we evaluate\nvarious models that differ in architectures, train-\ning corpora, and paradigms. Previous work shows\nwhen LLMs reach a certain parameter size, their\ndifferences in performance on dialogue summariza-\ntion become relatively small (Wang et al., 2023).\nTherefore, to balance the performance and infer-\nence latency, here we select models that are smaller\nthan a 10B parameter size. Details of the experi-\nmented models are shown in Table 1. For a repro-\nducible and fair comparison, consistent prompt tem-\nplates are employed across all models, as detailed\nin Appendix A. Moreover, considering the encoder-\ndecoder architecture of mT5, we follow the ap-\nproach of Puduppully et al. (2023) for prompting\nbidirectional LLMs, specifically by adding control\nkeywords and infilling text between them.\n2.2 Experimental Dataset\nAll models are evaluated using SAMSum (Gliwa\net al., 2019), a human-annotated dataset for abstrac-\ntive multi-turn dialogue summarization. Table 2\nlists some information about the dataset. We use\nsamples from the test set for model evaluation. For\nControl Signal Example: Length Control\nSummarize the conversation with the defined length:\nKevin: Hi, will you come to the workshop?\nElena: I have to, I will present a paper.\nKevin: Nice, I can’t wait!\nSummary with the length of 8 words: Elena will\npresent a paper at the workshop.\nSummarize the conversation with the defined length:\nJamilla: remember that the audition starts at 19:30.\nKiki: which station?\nJamilla: Antena 3\nYoyo: roger that\nSummary with the length of 9 words: <output>\nthe few-shot ICL inference, all demonstrations are\nrandomly sampled from the training set.\n2.3 ICL Inference Configuration\nIn this study, we consider two experiment settings:\nuncontrolled and controlled dialogue summariza-\ntion. An uncontrolled setting is identical to a tra-\nditional summarization task without control sig-\nnals. In contrast, a controlled setting involves user-\nprovided control signals as constraints to LLMs’\noutputs. Here we focus on three types of control\nsignals that are common and straightforward con-\ntrol aspects to users (He et al., 2022; Liu and Chen,\n2021; Wang et al., 2023):\n• Entity control: Given a set of user-specified\nkeywords or entities, the generated summary\nshould include them.\n• Length control: In this case, the user deter-\nmines the desired length for the summaries.\n• Personal named entity planning: This is a\nspecific form of entity control, where the user\nprovides models with a sequence of personal\nnamed entities, indicating person-focused per-\nspectives.\nControl Signal Setup: To quantitatively evaluate\nthe controllability of LLMs, we extract oracle con-\ntrol signals from human-annotated references (i.e.,\n57\nModel Size ROUGE-1 ROUGE-2 ROUGE-L Perplexity Factual Consistency(%)\nOPT 1.3B 30.7 6 .6 22 .6 64 .7 60 .2\nOPT-IML 1.3B 34.6 9 .9 27 .8 264.4 80 .9\nmT5-XL 3.7B 21.9 7 .4 21 .5 139 .3 48 .4\nCEREBRAS-GPT 6.7B 31.5 7 .4 22 .4 28.0 66.6\nLLaMA 7B 31.0 7 .3 22 .9 41 .1 94 .0\nAlpaca 7B 32.0 7 .1 23 .7 90 .8 97.3\nBLOOM 7B 32.1 7 .7 23 .2 38 .2 82 .1\nGPT3-davinci-003 175B 43.8 17 .0 39 .4 66 .6 -\nTable 3: Evaluation results in theuncontrolled setting. The ROUGE F-scores are reported. The optimal performance\nis highlighted in bold. GPT-3 serves as the factual consistency evaluator, so its factual consistency is excluded.\nModel Size ROUGE-1 ROUGE-2 ROUGE-L Perplexity Success Rate(%)\nOPT 1.3B 33.2 8 .2 24 .5 53 .2 65 .3 (↑ 14.8)\nOPT-IML 1.3B 37.8 11 .6 30 .5 294 .1 54 .5 (↑ 9.5)\nmT5-XL 3.7B 39.8 15 .2 34 .6 112.6 100.0\nCEREBRAS-GPT 6.7B 36.0 9 .7 26 .0 40.5 73.0 (↑ 16.9)\nLLaMA 7B 34.1 9 .3 25 .4 52 .8 62 .5 (↑ 14.0)\nAlpaca 7B 35.9 9 .6 27 .1 111 .9 63 .4 (↑ 12.3)\nBLOOM 7B 36.6 10 .2 27 .2 60 .1 71 .1 (↑ 17.0)\nGPT3-davinci-003 175B 48.8 22 .3 39 .1 112 .2 94 .0 (↑ 18.8)\nTable 4: Evaluation results in the entity control setting with 3 keywords. The ↑symbol denotes the change of the\nappearance likelihood of keywords compared to the uncontrolled setting.\ngold summaries), assuming the user provides the\nappropriate signals (He et al., 2022). For entity\ncontrol, the top k words in every gold summary\nwith the highest TF-IDF scores are extracted as key-\nwords. Considering the shorter lengths of the dia-\nlogue summaries, the range of k is set as {1, 2, 3}.\nTable 8 shows several generated examples of entity\ncontrol. For length control, the expected length\nis set equal to the length (number of words) of the\ngold summary. In personal named entity plan-\nning, the order of named entities 1 follows their\noccurrence in the gold summaries. The control sig-\nnals are included in the prompt, and the prompt\ntemplates are shown in Appendix A.\nDemonstration Selection: During few-shot in-\nference, the prompt includes several input-output\npairs followed by a query dialogue. We limit\nthe number of demonstrations to {1, 2, 3}due to\ncomputational constraints on the prompt’s length.\nDemonstrations are randomly selected from the\ntraining set but are kept consistent across all mod-\nels. Given the potential variance of ICL (Min et al.,\n2022), we repeat the generation process in 5 times\nusing different demonstrations and report the aver-\nage scores. The input-output pairs are concatenated\nwith the query dialogue to compose the prompt.\n1The personal named entities data is acquired from\nhttps://github.com/seq-to-mind/planning_dial_\nsumm/tree/main/data (Liu and Chen, 2021).\nEvaluation Metrics: Our evaluation has two pri-\nmary objectives: 1) to assess the quality of the\ngenerated summaries, and 2) to measure the con-\ntrollability of the models.\nFor assessing text-level quality automatically,\nwe employ the ROUGE metric (Lin, 2004), which\ngauges the correspondence between the generated\nsummaries and the reference (or gold) summaries.\nFollowing previous work (Fan et al., 2018), we\nalso calculate the perplexity of model generations\nusing GPT-2 (Radford et al., 2019), which serves\nas a measure of textual fluency.\nFactual consistency represents another essential\nfacet of quality. Since GPT-3 has demonstrated\nrobust performance across various evaluation tasks\n(Luo et al., 2023; Fu et al., 2023; Chia et al., 2023),\nwe utilize it as a binary natural language inference\nclassifier. This classifier assesses factual consis-\ntency by determining if the generated summary\naligns with the underlying dialogue. Further elabo-\nrations on this are provided in Appendix B.\nAdditionally, we perform automatic holistic eval-\nuations of writing quality, focusing on coherence\nand relevance. Following previous work (Chia\net al., 2023), where GPT-3 is used for automatic\nevaluations to benchmark instruction-tuned mod-\nels, we instruct GPT-3 to score the generated sum-\nmaries on a discrete scale of 1 to 5. To ensure com-\nparability, we adopt the same prompt templates as\nin Chia et al. (2023). The average scores are pre-\n58\nModel Size ROUGE-1 ROUGE-2 ROUGE-L Perplexity Length Deviation\nOPT 1.3B 30.7 6 .5 22 .2 54 .2 12 .4 (↓ 0.9)\nOPT-IML 1.3B 36.0 10 .4 28 .8 252.0 11 .7 (↓ 0.5)\nmT5-XL 3.7B 21.1 5 .6 18 .4 102 .4 10 .7 (↑ 0.7)\nCEREBRAS-GPT 6.7B 31.2 6 .7 22 .5 31.8 16.1 (↓ 1.4)\nLLaMA 7B 33.7 8 .2 24 .8 57 .8 12 .3 (↓ 2.2)\nAlpaca 7B 34.7 8 .2 26 .4 185 .3 7.2 (↓ 4.7)\nBLOOM 7B 32.9 7 .9 24 .3 45 .0 13 .1 (↓ 1.3)\nGPT3-davinci-003 175B 47.8 20 .1 38 .0 219 .0 7 .1 (↓ 12.6)\nTable 5: Evaluation results in thelength control setting. The ↑and ↓symbols denote the change of length deviations\ncompared to the uncontrolled setting.\nModel Size ROUGE-1 ROUGE-2 ROUGE-L Perplexity Success Rate (%)\nOPT 1.3B 30.5 7 .2 23 .6 57 .4 82 .7 (↑ 4.1)\nOPT-IML 1.3B 36.5 11 .1 29 .5 239.2 76 .1 (↑ 4.2)\nmT5-XL 3.7B 28.3 7 .9 24 .8 50 .1 100\nCEREBRAS-GPT 6.7B 32.8 8 .8 24 .2 31.1 88.0 (↑ 5.5)\nLLaMA 7B 33.3 8 .6 25 .2 51 .1 77 .8 (↑ 7.1)\nAlpaca 7B 33.8 8 .5 25 .8 102 .4 76 .6 (↑ 2.0)\nBLOOM 7B 33.4 9 .0 25 .2 43 .3 89.2 (↑ 6.4)\nGPT3-davinci-003 175B 47.3 21 .6 36 .7 65 .7 96 .8 (↑ 4.9)\nTable 6: Evaluation results in the person-focused planning setting. The ↑and ↓symbols denote the change of\nlength deviations compared to the uncontrolled setting.\nModel Size Consistency (%) Fluency Coherence Relevance\nOPT 1.3B 60.2 64 .7 3.5 3.2\nOPT-IML 1.3B 80.9 264 .4 3 .4 3 .2\nmT5-XL 3.7B 48.4 139 .3 3 .3 3 .0\nCEREBRAS-GPT 6.7B 66.6 28.0 3.4 3 .4\nLLaMA 7B 94.0 41 .1 3 .4 3.7\nAlpaca 7B 97.3 90.8 3.5 3.5\nBLOOM 7B 82.1 38 .2 3 .4 3 .5\nTable 7: Holistic evaluations on the writing quality. For each aspect, the best score is in bold.\nsented in Table 7, and a more detailed description\nis provided in Appendix C.\nRegarding the model controllability, the success\nrate of entity control is determined by the propor-\ntion of specified keywords present in the generated\nsummaries (He et al., 2022). This success rate\nmetric is also applicable to personal named entity\nplanning. Length controllability is evaluated by\nthe mean absolute length deviation, formulated as\n1\nN ·∑N\ni=1 |lgenerated −lgold|, where N represents the\nsize of the test set, and l denotes the text length.\n3 Our Findings\nIn this section, we analyze the experimental results\nby discussing about the two research problems.\n3.1 How is the quality of the dialogue\nsummaries generated by LLMs via ICL?\nThe experimented LLMs can generate reasonable\nsummaries via ICL inference, but perform differ-\nently. In the uncontrolled setting, as shown in Ta-\nble 3, OPT-IML 1.3B exhibits the highest ROUGE\nscores, despite having the fewest parameters. How-\never, ROUGE scores cannot reflect factual correct-\nness from the semantic view. In contrast, LLaMA\nand Alpaca tend to produce summaries that are the\nmore factually consistent (i.e., their consistency in\nGPT3’s assessment exceeds 90%). Considering\nother evaluation aspects, CEREBRAS-GPT consis-\ntently generates the most fluent sentences across\nall settings, and summaries generated by LLaMA\nobtain the highest score regarding relevance among\nall models (see Table 7).\nIn the controlled setting, as shown in Table 4,\nboth entity control and person-focused planning im-\nprove ROUGE scores, and the former brings a sub-\nstantial performance gain. Length control has a mi-\nnor impact on ROUGE scores, especially for mod-\nels without instruction-tuning. Moreover, mT5’s\nperformance is inferior to other models in non-\nentity control settings in terms of ROUGE scores\nand factual consistency. However, it achieves opti-\nmal ROUGE scores in entity control, as its archi-\ntecture ensures the inclusion of keywords.\n59\nGold Summary Model Uncontrolled setting Entity control\nThe audition starts at 7.30\nP.M. in Antena 3.\nBLOOM-7b1 Jamilla and Yoyo are going to\nan audition for a TV show. They\nare waiting for the station to\nstart broadcasting.\nJamilla and Kiki are going to an\naudition for a TV show. The sta-\ntion is Antena3.\nAdam and Karen are wor-\nried that May suffers from\ndepression. Karen will call\nher friend who is a psychol-\nogist and ask for advice.\nmT5-XL Melissa shows Brian what she\nbought and he finds it awesome.\nAdam and karen talk to their friend\nabout her depression. They talk to\ntheir friend’s psychologist and she\nagrees to help.\nMike suspects he might have\nhad an allergic reaction to\nsomething.\nLLaMA-7B Mike wakes up with a strange\nrash on his face. He has no clue\nwhat it is and asks his girlfriend\nEmma to help him figure it out.\nMike suspects that he is allergic to\nsomething, but he does not know\nwhat it is.\nFlo cannot get an appoint-\nment at the salon until the\n6th. Flo worries she’s going\nto be gray. Flo will have to\nget a touch-up kit at Tesco.\nCEREBRAS-6.7B Gina and Flo are having a con-\nversation about getting their hair\ndone at the same time.\nFlo is worried that she will be too\nlate for her appointment at the hair\nsalon. She will get a kit to get her\nhair touched up.\nTable 8: Comparisons of generated summaries between controlled and uncontrolled settings. Underlined keywords\nare the control signals included in the prompts.\n3.2 Can LLMs do controlled dialogue\nsummarization?\nExperimental results demonstrate that LLMs are\ncapable of achieving controlled dialogue summa-\nrization via ICL. Evaluation details of LLMs in\ncontrolled settings are presented in Table 4 for en-\ntity control with 3 keywords, Table 5 for length\ncontrol, and Table 6 for person-focused planning.\nTable 10 in Appendix provides a comprehensive\nevaluation with varying numbers of keywords.\nIn the entity control experiments, keywords are\nincluded in the prompts as constraints. All models\nexhibit improved ROUGE scores, and the likeli-\nhood of keywords appearing in the summaries is\nincreased, indicating that LLMs effectively utilize\nthe information from control signals. Notably, mT5\nachieves a 100% success rate, benefiting from its\nbi-directional encoding architecture. Examples pre-\nsented in Table 8 show how keywords can guide\nmodels to generate better summaries. Surprisingly,\nnon-instruction-tuned models like CEREBRAS-\nGPT and BLOOM demonstrate better controlla-\nbility than instruction-tuned models like Alpaca\nand OPT-IML in entity control.\nThe impact of length signals is relatively minor\ncompared to keyword signals on ROUGE scores.\nHowever, the length distribution with length sig-\nnals is more aligned with the actual length across\nmodels, except for mT5. Notably, Alpaca demon-\nstrates the best length controllability. We also find\nthat OPT-IML appears to have lower controllability\ncompared to its foundation model, OPT.\nModel Success Rate (%)\nOPT-IML-1.3B 19.0 (↑ 4.2)\nLLaMA-7B 10.1 (↑ 4.7)\nAlpaca-7B 7.8 (↑ 3.5)\nBLOOM-7B 28.3 (↑ 17.0)\nTable 9: The success rates of numerical keywords.\n3.3 Further Analysis\nNumerical keywords (e.g., time and quantity)\ntend to be left out by LLMs. Preliminary error\nanalysis shows a large portion of the missing key-\nwords in entity control contain numerical informa-\ntion. To verify that, the models are prompted with\nonly numerical keywords (e.g., time, date, quantity,\nand percent) extracted from gold summaries using\nSpaCy (Honnibal et al., 2020). The results in Table\n9 demonstrate a significant decrease in the success\nrates across all models. It implies that LLMs have\nsome intrinsic bias toward non-numerical content,\npotentially causing them to overlook crucial numer-\nical details within dialogues.\n4 Conclusion\nIn this study, we have benchmarked the in-context\nlearning performance of state-of-the-art LLMs in\ncontrolled and uncontrolled settings for abstractive\ndialogue summarization. We assessed their sum-\nmarization quality, factual consistency, and con-\ntrollability, while also conducting holistic evalua-\ntions and empirical analysis. We hope this study\nprovides insights for the follow-up research about\ndialogue summarization using LLMs.\n60\nLimitations\nOne limitation of this study is that only LLMs with\nless than 10B parameters are experimented with\ndue to hardware constraints. To address this is-\nsue, we release the evaluation codes, in order to\nfacilitate the follow-up research.\nMeanwhile, the control signals in this work are\noracle, which means we assume the user provides\nindicative keywords to be included in the summary.\nThere are automatic methods to extract keywords\nfrom dialogues (He et al., 2022), but it is not the\nfocus and therefore not discussed in this study.\nDue to time constraints, we adopted GPT-3 to\nconduct automatic qualitative evaluations. While\nGPT-based evaluations have proven to be compet-\nitive in some evaluation tasks, the necessity for\nhuman evaluations remains.\nAcknowledgments\nThis research was supported by the Agency for Sci-\nence, Technology and Research (A*STAR), Singa-\npore under its Industry Alignment Pre-Positioning\nFund (Grant No. H19/01/a0/023 - DCOF), and by\nthe National Research Foundation, Prime Minis-\nter’s Office, Singapore under its Campus for Re-\nsearch Excellence and Technological Enterprise\n(CREATE) programme. We also thank the anony-\nmous reviewers for their precious feedback to help\nimprove and extend this piece of work.\nReferences\nBigScience Workshop. 2022. BLOOM (revision\n4ab0472).\nJiaao Chen and Diyi Yang. 2020. Multi-view sequence-\nto-sequence models with conversational structure for\nabstractive dialogue summarization. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 4106–\n4118, Online. Association for Computational Lin-\nguistics.\nYew Ken Chia, Pengfei Hong, Lidong Bing, and Sou-\njanya Poria. 2023. Instructeval: Towards holistic\nevaluation of instruction-tuned large language mod-\nels. arXiv preprint arXiv:2306.04757.\nNolan Dey, Gurpreet Gosal, Zhiming, Chen, Hemant\nKhachane, William Marshall, Ribhu Pathria, Mar-\nvin Tom, and Joel Hestness. 2023. Cerebras-gpt:\nOpen compute-optimal language models trained on\nthe cerebras wafer-scale cluster.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong\nWu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and\nZhifang Sui. 2023. A survey on in-context learning.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018.\nHierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889–898, Melbourne, Australia. Association\nfor Computational Linguistics.\nXiachong Feng, Xiaocheng Feng, and Bing Qin.\n2021. A survey on dialogue summarization: Re-\ncent advances and new frontiers. arXiv preprint\narXiv:2107.03175.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire.\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Alek-\nsander Wawer. 2019. SAMSum corpus: A human-\nannotated dialogue dataset for abstractive summa-\nrization. In Proceedings of the 2nd Workshop on\nNew Frontiers in Summarization, pages 70–79, Hong\nKong, China. Association for Computational Linguis-\ntics.\nJunxian He, Wojciech Kryscinski, Bryan McCann,\nNazneen Rajani, and Caiming Xiong. 2022. CTRL-\nsum: Towards generic controllable text summariza-\ntion. In Proceedings of the 2022 Conference on Em-\npirical Methods in Natural Language Processing,\npages 5879–5915, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nMatthew Honnibal, Ines Montani, Sofie Van Lan-\ndeghem, and Adriane Boyd. 2020. spaCy: Industrial-\nstrength Natural Language Processing in Python.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\nTodor Mihaylov, Dániel Simig, Ping Yu, Kurt Shuster,\nTianlu Wang, Qing Liu, Punit Singh Koura, et al.\n2022. Opt-iml: Scaling language model instruction\nmeta learning through the lens of generalization.\nNitish Shirish Keskar, Bryan McCann, Lav R. Varshney,\nCaiming Xiong, and Richard Socher. 2019. Ctrl: A\nconditional transformer language model for control-\nlable generation.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nZhengyuan Liu and Nancy Chen. 2021. Controllable\nneural dialogue summarization with personal named\nentity planning. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 92–106, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nZhengyuan Liu and Nancy Chen. 2022. Entity-based de-\nnoising modeling for controllable dialogue summa-\nrization. In Proceedings of the 23rd Annual Meeting\nof the Special Interest Group on Discourse and Dia-\nlogue, pages 407–418, Edinburgh, UK. Association\nfor Computational Linguistics.\n61\nZhengyuan Liu, Ke Shi, and Nancy Chen. 2021.\nCoreference-aware dialogue summarization. In Pro-\nceedings of the 22nd Annual Meeting of the Special\nInterest Group on Discourse and Dialogue, pages\n509–519, Singapore and Online. Association for\nComputational Linguistics.\nZheheng Luo, Qianqian Xie, and Sophia Ananiadou.\n2023. Chatgpt as a factual inconsistency evaluator\nfor text summarization.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work?\nRatish Puduppully, Anoop Kunchukuttan, Raj Dabre,\nAi Ti Aw, and Nancy F. Chen. 2023. Decomposed\nprompting for machine translation between related\nlanguages using large language models.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nBin Wang, Zhengyuan Liu, and Nancy F Chen. 2023.\nInstructive dialogue summarization with query aggre-\ngations. arXiv preprint arXiv:2310.10981.\nBin Wang, Chen Zhang, Yan Zhang, Yiming Chen, and\nHaizhou Li. 2022. Analyzing and evaluating faith-\nfulness in dialogue summarization. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 4897–4908.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mt5: A massively multilingual\npre-trained text-to-text transformer.\nHanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou,\nand Dawei Song. 2023. A survey of controllable\ntext generation using transformer-based pre-trained\nlanguage models.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022a. Opt: Open\npre-trained transformer language models.\nYubo Zhang, Xingxing Zhang, Xun Wang, Si qing Chen,\nand Furu Wei. 2022b. Latent prompt tuning for text\nsummarization.\n62\nA Appendix: Prompt template\nThis section includes examples of the prompt tem-\nplates, which remain consistent across models.\nUncontrolled Setting\nSummarize the conversation:\nSelby: anybody for indian?\nTerri: yuo cooked?\nSelby: yessir\nTerri: sounds cool\nWinslow: gr8. ill be there too\nSummary: Selby invites Terri and Winslow for a\nhome-cooked Indian meal.\nSummarize the conversation:\nMarta: <file_gif>\nMarta: Sorry girls, I clicked something by accident\n:D\nAgnieszka: No problem :p\nWeronika: Hahaha\nAgnieszka: Good thing you didn’t send something\nfrom your gallery ;)\nSummary:\n(Last line of mT5)\nSummary: <extra_id_0>\nLength Control\nSummarize the conversation with the defined length:\nKevin: Hi, will you come to the workshop?\nElena: I have to, I will present a paper.\nKevin: Nice, I can’t wait!\nSummary with the length of 8 words: Elena will\npresent a paper at the workshop.\nSummarize the conversation with the defined length:\nJamilla: remember that the audition starts at 7.30\nP.M.\nKiki: which station?\nJamilla: Antena 3\nYoyo: roger that\nSummary with the length of 9 words:\n(Last line of mT5)\nSummary with the length of 9 words: <extra_id_0>\nEntity control (non-mT5)\nSummarize the conversation with keywords:\nKevin: Hi, will you come to the workshop?\nElena: I have to, I will present a paper.\nKevin: Nice, I can’t wait!\nSummary with keywords [’Elena’, ’workshop’]:\nElena will present a paper at the workshop.\nSummarize the conversation with keywords:\nJamilla: remember that the audition starts at 19:30.\nKiki: which station?\nJamilla: Antena 3\nYoyo: roger that\nSummary with keywords [’audition’, ’antena’]:\nEntity Control / Person-focused Planning\n(mT5)\nSummarize the conversation:\nSelby: anybody for indian?\nTerri: yuo cooked?\nSelby: yessir\nTerri: sounds cool\nWinslow: gr8. ill be there too\nSummary: Selby invites Terri and Winslow for a\nhome-cooked Indian meal. Selby invites Terri and\nWinslow for a home-cooked Indian meal.</s>\nSummarize the conversation:\nMarta: <file_gif>\nMarta: Sorry girls, I clicked something by accident\n:D\nAgnieszka: No problem :p\nWeronika: Hahaha\nAgnieszka: Good thing you didn’t send something\nfrom your gallery ;)\nSummary: <extra_id_0> marta <extra_id_1> file <ex-\ntra_id_2>\nPerson-focused Planning (non mT5)\nSummarize the conversation with the focus perspec-\ntives provided:\nHugo: Im off to Mazagan (Morocco) this weekend\nLacey: Oh so nice!\nHugo: Where are you these days? In Spain already?\nLacey: No. In Poland all the time. I study here\nSummary with the focus perspectives [’Hugo’,\n’Lacey’]: Hugo is off to Mazagan in Morocco this\nweekend. Lacey studies in Poland.\nSummarize the conversation with the focus perspec-\ntives provided:\nMiley: don’t want to go to work tomorrow!\nMiley: wanna sleep!!!\nMiley: need some rest :(\nAaron: know what u feelin\nAaron: really want some holiday already\nMiley: :(\nAaron: fighting girl!\nSummary with the focus perspectives [’Miley’]:\n63\nB Appendix: Utilizing GPT-3 for factual\nconsistency evaluation\nThis study employs text-davinci-003 model as the\nfactual consistency evaluator. The prompt template\nadopts a zero-shot chain-of-thought style because it\noutperforms direct prompting and exhibits greater\nstability compared to the few-shot setup, as sug-\ngested in Luo et al. (2023).\nAn illustrative example of the prompt and GPT-\n3’s response is provided below:\nPrompt for factual consistency classification\nDecide if the following summary is consistent with\nthe corresponding dialogue. Note that consistency\nmeans all information in the summary is supported\nby the dialogue.\nDialogue:\nWill: hey babe, what do you want for dinner tonight?\nEmma: gah, don’t even worry about it tonight\n...\nWill: Alright, love you.\nEmma: love you too.\nSummary: Emma doesn’t want to cook for Will\ntonight.\nExplain your reasoning step by step then answer (yes\nor no) the question:\nGPT-3’s response\nYes, the summary is consistent with the dialogue.\nStep 1: Will asks Emma what she wants for dinner.\nStep 2: Emma responds that she doesn’t want to\nworry about it, implying that she does not want to\ncook.\nStep 3: Will then asks what time she’ll be HOME,\nand Emma says soon.\nStep 4: Will then offers to pick her up to which\nEmma responds no, she’ll tell him when she gets\nhome.\nStep 5: Will then tells her he loves her and she\nresponds in kind.\nYes, the summary is consistent with the dialogue.\nC Appendix: Holistic Evaluation\nThe holistic evaluation of writing quality utilizes\nGPT-3. We follow the evaluation templates as well\nas the rubrics in Chia et al. (2023). See details of\nthe prompts for writing an evaluation of relevance\nand coherence on the next page.\nDue to the API cost, each model is evaluated on\n100 random samples from the test set.\nD Appendix: Impact of the number of\ndemonstrations\nICL’s performance instability is influenced by the\nselection and quantity of demonstrations. This\nstudy employs a random selection strategy for effi-\nciency and simplicity, with each test sample having\nfive sets of demonstrations. The average values are\nreported.\nTable 10 and Figure 1 show the metric trends\nfor k demonstrations, indicating that an increase\nin demonstrations may degrade performance and\nincrease instability in some models (e.g., Cerebras-\nGPT-6.7B, OPT-1.3B), possibly due to their small\nparameter sizes. Table 3 presents the optimal per-\nformance for k values in the set 1,2,3.\n64\nWriting evaluation on relevance\nText: Eric, Bella and Eric were talking about their boss and how he appreciated their decision of dismissing a potential\nclient. Eric and Bella were discussing the reasons why they dismissed the client, while Eric was asking Bella about her\nreaction to his boss’ reaction.\nPrompt: Summarize the following dialogue:\nEric: Hey Bella, What happened today in boss’s room?? Was he angry??\nBella: NO NO!!! He wasn’t angry at all.. He actually appreciated on our brave deccision to dismiss the request of client..\nEric: REALLY!! He appreciated this decision.. Bella: Yeah he really did.. I too was astounded by his reaction...\nEric: What could possibly lead to this?? I mean , they were potential clients...\nBella: What he told me was that he was looking forward to bring in new clients which were our current client’s\ncompetitor..\nEric: Oh that could possibly be the reason.Well anyways you got appreciation xD congo\nBella: hahaha Blessing in disguise xD\nHow relevant is the text to the prompt? Select a suitable option number between 1 and 5 based on the op-\ntions below.\n1. Inadequate: The text fails to provide any relevant information or insights related to the given prompt.\n2. Limited: The text may contain some relevant information, but significant gaps exist, and key aspects of the prompt are\nnot adequately covered.\n3. Satisfactory: The text covers the main aspects of the prompt and provides relevant information, but it lacks depth and\nmay not explore the topic in great detail.\n4. Proficient: The text provides a comprehensive response by addressing the key aspects of the prompt, offering relevant\nand well-supported information or arguments.\n5. Excellent: The text thoroughly and thoughtfully addresses the prompt, demonstrating a comprehensive understanding\nof the topic. It offers insightful and original ideas, supported by relevant arguments and information.\nWriting evaluation on coherence\nText: Eric, Bella and Eric were talking about their boss and how he appreciated their decision of dismissing a potential\nclient. Eric and Bella were discussing the reasons why they dismissed the client, while Eric was asking Bella about her\nreaction to his boss’ reaction.\nHow coherent is the text? Select a suitable option number between 1 and 5 based on the options below.\n1. Inadequate: The text lacks logical organization, making it difficult to follow. Ideas are disjointed and\nphrased awkwardly, requiring significant effort to understand.\n2. Limited: The text demonstrates some attempt at organization, but there are significant gaps in coherence. Ideas may\nbe loosely connected, and the arguments lack clarity.\n3. Satisfactory: The text generally follows a logical organization, but occasional disruptions or awkward phrasing may\noccur. There is an acceptable level of readability and understanding.\n4. Proficient: The text is clearly organized and easy to understand. Ideas and arguments flow smoothly, contributing to\neasy comprehension and a pleasant reading experience.\n5. Excellent: The text presents exceptionally coherent writing with a fluent and engaging flow of ideas, ensuring\neffortless comprehension and a delightful reading experience.\n65\nModel k ROUGE-1 ROUGE-2 ROUGE-L Succee Rate (%)\nOPT-1.3B 1 30.0 6.4 22.1 71.7 (↑ 10.7)\n2 32.1 7.7 23.7 68.7 (↑ 14.1)\n3 33.2 8.2 24.5 65.3 (↑ 14.8)\nOPT-IML-1.3B 1 36.5 11.0 30.0 61.9 (↑ 7.5)\n2 36.9 11.1 29.6 57.4 (↑ 8.7)\n3 37.8 11.6 30.5 54.5 (↑ 9.5)\nmT5-XL 1 32.3 11.0 27.5 100.0\n2 36.3 13.0 31.4 100.0\n3 39.8 15.2 34.6 100.0\nCerebras-GPT-6.7B 1 32.6 7.7 23.2 79.6 (↑ 13.9)\n2 33.9 8.6 24.5 74.9 (↑ 15.1)\n3 36.0 9.7 26.0 73.0 (↑ 16.9)\nLLaMA-7B 1 32.0 7.8 23.4 69.1 (↑ 12.6)\n2 33.6 8.8 24.8 65.2 (↑ 13.7)\n3 34.1 9.3 25.4 62.5 (↑ 14.0)\nAlpaca-7B 1 33.3 7.6 24.7 67.6 (↑ 8.5)\n2 35.2 8.9 26.3 65.4 (↑ 11.5)\n3 35.9 9.6 27.1 63.4 (↑ 12.3)\nBLOOM-7B 1 32.2 7.6 23.1 77.1 (↑ 12.2)\n2 34.9 9.2 25.7 73.0 (↑ 14.8)\n3 36.6 10.2 27.2 71.1 (↑ 17.0)\nTable 10: Evaluation results in the entity control setting with k keywords.\n66\n(a) OPT-1.3B\n(b) mT5-XL\n(c) BLOOM-7B\n(d) CEREBRAS-GPT-6.7B\nFigure 1: The line plots of evaluation metrics given k demonstrations in the uncontrolled setting. 95% confidence\ninterval is highlighted within the plots.\n67"
}