{
  "title": "HTS-AT: A hierarchical token-semantic audio transformer for sound classification and detection",
  "url": "https://openalex.org/W4221095487",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1968999463",
      "name": "Ke Chen",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2799145318",
      "name": "Xingjian Du",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2111377086",
      "name": "Bilei Zhu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2098040314",
      "name": "Zejun Ma",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A4208177027",
      "name": "Taylor Berg-Kirkpatrick",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2463114407",
      "name": "Shlomo Dubnov",
      "affiliations": [
        "University of California, San Diego"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1975817689",
    "https://openalex.org/W3097067478",
    "https://openalex.org/W2797583228",
    "https://openalex.org/W3047453285",
    "https://openalex.org/W3047386385",
    "https://openalex.org/W2593116425",
    "https://openalex.org/W4398958419",
    "https://openalex.org/W2973109987",
    "https://openalex.org/W2896382071",
    "https://openalex.org/W2994728585",
    "https://openalex.org/W3164279099",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3142837074",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3006275583",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W3119913666",
    "https://openalex.org/W3161820708",
    "https://openalex.org/W2946948417",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3161435013",
    "https://openalex.org/W4226151502",
    "https://openalex.org/W3005441616",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3178592608",
    "https://openalex.org/W3162999565",
    "https://openalex.org/W4310330865",
    "https://openalex.org/W4247124580",
    "https://openalex.org/W3198035615",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3205743929",
    "https://openalex.org/W3163379884",
    "https://openalex.org/W3094550259",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4287025446",
    "https://openalex.org/W3196974791",
    "https://openalex.org/W2963610932"
  ],
  "abstract": "Audio classification is an important task of mapping audio samples into their corresponding labels. Recently, the transformer model with self-attention mechanisms has been adopted in this field. However, existing audio transformers require large GPU memories and long training time, meanwhile relying on pretrained vision models to achieve high performance, which limits the model's scalability in audio tasks. To combat these problems, we introduce HTS-AT: an audio transformer with a hierarchical structure to reduce the model size and training time. It is further combined with a token-semantic module to map final outputs into class featuremaps, thus enabling the model for the audio event detection (i.e. localization in time). We evaluate HTS-AT on three datasets of audio classification where it achieves new state-of-the-art (SOTA) results on AudioSet and ESC-50, and equals the SOTA on Speech Command V2. It also achieves better performance in event localization than the previous CNN-based models. Moreover, HTS-AT requires only 35\\% model parameters and 15\\% training time of the previous audio transformer. These results demonstrate the high performance and high efficiency of HTS-AT.",
  "full_text": "HTS-AT: A HIERARCHICAL TOKEN-SEMANTIC AUDIO TRANSFORMER\nFOR SOUND CLASSIFICATION AND DETECTION\nKe Chen1, Xingjian Du2, Bilei Zhu2, Zejun Ma2, Taylor Berg-Kirkpatrick1, Shlomo Dubnov1\n1University of California San Diego 2AI Lab, Bytedance Inc.\nABSTRACT\nAudio classification is an important task of mapping au-\ndio samples into their corresponding labels. Recently, the\ntransformer model with self-attention mechanisms has been\nadopted in this field. However, existing audio transformers\nrequire large GPU memories and long training time, mean-\nwhile relying on pretrained vision models to achieve high\nperformance, which limits the model’s scalability in audio\ntasks. To combat these problems, we introduce HTS-AT:\nan audio transformer with a hierarchical structure to reduce\nthe model size and training time. It is further combined\nwith a token-semantic module to map final outputs into class\nfeaturemaps, thus enabling the model for the audio event\ndetection (i.e. localization in time). We evaluate HTS-AT\non three datasets of audio classification where it achieves\nnew state-of-the-art (SOTA) results on AudioSet and ESC-\n50, and equals the SOTA on Speech Command V2. It also\nachieves better performance in event localization than the\nprevious CNN-based models. Moreover, HTS-AT requires\nonly 35% model parameters and 15% training time of the\nprevious audio transformer. These results demonstrate the\nhigh performance and high efficiency of HTS-AT.\nIndex Terms— Audio Classification, Sound Event De-\ntection, Transformer, Token-Semantic Module\n1. INTRODUCTION\nAudio classification is an audio retrieval task which aims\nto learn a mapping from audio samples to their correspond-\ning labels. Depending on the audio categories, it involves\nsound event detection [1], music instrument classification [2],\namong others. It establishes a foundation for many down-\nstream applications including music recommendation [3],\nkeyword spotting [4], music generation [5, 6], etc.\nWith burgeoning research in the field of artificial in-\ntelligence, we have seen significant promising progress in\naudio classification. For data collections, many datasets\nwith different types of audio (e.g. AudioSet [7], ESC-50\n[8], Speech Command [4], etc.) provide platforms for the\ntraining and evaluation of models on different subtasks. For\nthe model design, the audio classification task is thriving\nbased on neural-network-based models. Convolutional neural\nnetworks (CNNs) have been widely used in this field, such\nas DeepResNet [9], TALNet [10], PANN [11], and PSLA\n[12]. These models leverage CNN to capture features on the\naudio spectrogram, and further improve their performance\nthrough the design of the depth and breadth of the network.\nRecently, by introducing the transformer structure [13] into\naudio classification, the audio spectrogram transformer (AST)\n[14] further achieves the best performance through the self-\nattention mechanism and the pretrained model from computer\nvision. In this paper, we take a further step on a transformer-\nbased audio classification model by first analyzing remaining\nproblems in the AST.\nFirst, since the transformer takes the audio spectrogram as\na complete sequential data, AST takes a long time to train and\nconsumes large GPU memories. In practice, it takes about one\nweek to train on the full AudioSet with four 12GB GPUs. One\nmethod to boost training speed is to use the ImageNet [15]\npretrained model in computer vision. However, this also lim-\nits the model to those pretrained hyperparameters, which re-\nduces its scalability in more audio tasks. Indeed, we find that\nwithout pretraining, AST can only achieve the baseline per-\nformance (mAP=0.366 on AudioSet), which raises our atten-\ntion to its learning efficiency on the audio data. Second, AST\nuses a class-token (CLS) to predict labels, making it unable\nto predict the start and end time of events in audio samples.\nMost CNN-based models naturally support the frame-level lo-\ncalization by empirically taking the penultimate layer’s output\nas a event presence map. This inspires us to design a module\nthat makes every output token of an audio transformer aware\nof the semantic meaning of events (i.e. a token-semantic mod-\nule [16]) for supporting more audio tasks (e.g. sound event\ndetection and localization).\nIn this paper, we propose HTS-AT 1, a hierarchical audio\ntransformer with a token-semantic module for audio classifi-\ncation. Our contributions of HTS-AT can be listed as:\n• HTS-AT achieves or equals SOTAs on AudioSet and ESC-\n50, and Speech Command V2 datasets. Moreover, the\nmodel without pretraining can still achieve the perfor-\nmance that is only 1%-2% lower than the best results.\n• HTS-AT takes fewer parameters (31M vs. 87M), fewer\nGPU memories, and less training time (80 hrs vs. 600 hrs)\nthan AST’s to achieve the best performance.\n1https://github.com/RetroCirce/HTS-Audio-Transformer\ntime frame\nfrequency\n...... \n...... \n... ... ... \ntime → frequency → window\n...... \nPatch-Embed\nPatch\nTokens\nSwin Transformer\nPatch-Merge\nSwin Transformer\nPatch-Merge\nSwin \nTransformer\nPatch-Merge\nSwin\nTransformer\nGroup 1 Group 2 Group 3 Group 4\nwindow\nattention\nPatch-Embed\nReshape\nToken-Semantic CNN\nSwin Transformer\nSwin Transformer\nSwin \nTransformer\nEvent Presence Map\navg-pool\nLabel Prediction\nEncode Audio Mel-Spectrogram HTS-AT Training Output\nLatent Tokens\nFig. 1: The model architecture of HTS-AT.\n• HTS-AT further enables the audio transformer to produce\nthe localization results of event only with weakly-labeled\ndata. And it achieves a better performance than the previ-\nous CNN-based model.\n2. PROPOSED MODEL\n2.1. Hierarchical Transformer with Window Attention\nA typical transformer structure consumes lots of GPU mem-\nories and training time, because the length of input tokens\nis too long and remains unchanged in all transformer blocks\nfrom beginning to end. As a result, the machine saves the out-\nput and its gradient of each block via large GPU memories,\nand spends much calculation time maintaining a large global\nself-attention matrix. To combat these problems, as depicted\nin Figure 1, we propose two key designs: a hierarchical trans-\nformer structure and a window attention mechanism.\n2.1.1. Encode the Audio Spectrogram\nIn the left of Figure 1, an audio mel-spectrogram is cut into\ndifferent patch tokens with a Patch-Embed CNN of kernel\nsize (P × P) and sent into the transformer in order. Dif-\nferent from images, the width and the height of an audio mel-\nspectrogram denote different information (i.e. the time and\nthe frequency bin). And the length of time is usually much\nlonger than that of frequency bins. Therefore, to better cap-\nture the relationship among frequency bins of the same time\nframe, we first split the mel-spectrogram intopatch windows\nw1, w2, ..., wn and then split the patches inside each window.\nThe order of tokens follows time→frequency→window as\nshown in Figure 1. With this order, patches with different\nfrequency bins at the same time frame will be organized adja-\ncently in the input sequence.\n2.1.2. Patch-Merge and Window Attention\nIn the middle of Figure 1, the patch tokens are sent into sev-\neral groups of transformer-encoder blocks. At the end of each\ngroup, we implement a Patch-Merge layer [17] to reduce the\nsequence size. This merge operation is applied by first reshap-\ning the sequence to its original 2D map( T\nP × F\nP , D), where D\nis the latent state dimension. Then it merges adjacent patches\nas ( T\n2P × F\n2P , 4D) and finally applies a linear layer to reduce\nthe latent dimension to ( T\n2P × F\n2P , 2D). As illustrated in Fig-\nure 1, the shape of the patch tokens is reduced by 8 times from\n( T\nP × F\nP , D) to ( T\n8P × F\n8P , 8D) after 4 network groups, thus\nthe GPU memory consumption is reduced exponentially after\neach group.\nFor each transformer block inside the group, we adopt a\nwindow attention mechanism to reduce the calculation. As\nshown in different color boxes in the middle right of Figure\n1, we first split the patch tokens (in 2D format) into non-\noverlapping (M ×M) attention windows aw1, aw2, ..., awk.\nThen we only compute the attention matrix inside each M ×\nM attention window. As a result, we have k window atten-\ntion (W A) matrices instead of a whole global attention (GA)\nmatrix. The computational complexities of these two mecha-\nnisms in one transformer block for f × t audio patch tokens\nwith the initial latent dimension D are:\nGA: O(ftD2 + (ft)2D) (1)\nW A: O(ftD2 + M2ftD) (2)\nwhere the window attention reduces the second complexity\nterm by ( ft\nM2 ) times. For audio patch tokens in a time-\nfrequency-window order, each window attention module will\ncalculate the relation in a certain range of continuous fre-\nquency bins and time frames. As the network goes deeper,\nthe Patch-Merge layer will merge adjacent windows, thus\nthe attention relation is calculated in a larger space. In the\ncode implementation, we use the swin transformer block with\na shifted window attention [17], a more efficient window\nattention mechanism. This also helps us to use the swin\ntransformer pretrained vision model in the experiment stage.\n2.2. Token Semantic Module\nThe existing AST uses a class-token (CLS) to predict the clas-\nsification label, which limits it from further indicating the\nstart and end times of events as realized in CNN-based mod-\nels. In the final layer output, each token contains information\nabout its corresponding time frames and frequency bins. We\nexpect to convert tokens into activation maps for each label-\nclass (i.e. aware of semantic meaning [16]). For strong-label\ndatasets, we can let the model directly calculate the loss in\nspecific time ranges. For weakly-labeled datasets, we can\nModel Pretrain #Params. mAP Ensemble-mAP\nBaseline [7] ✗ 2.6M 0.314 -\nDeepRes [9] ✗ 26M 0.392 -\nPANN [11] ✗ 81M 0.434 -\nPSLAP [12] ✓ 13.6M 0.444 0.474\nAST [14] ✗ 87M 0.366 -\nASTP [14] ✓ 87M 0.459 0.475 (0.485 2)\nHTS-ATH ✗ 28.8M 0.440 -\nHTS-ATHC ✗ 31M 0.453 -\nHTS-ATHCP ✓ 31M 0.471 0.487\nTable 1: The mAP results on AudioSet evaluation set.\nleverage the transformer to locate via its strong capability to\ncapture the relation. In HTS-AT, as shown in the right of\nFigure 1, we modify the output structure by adding a token-\nsemantic CNN layer after the final transformer block. It has\na kernel size (3, F\n8P ) and a padding size (1, 0) to integrate all\nfrequency bins and map the channel size 8D into the event\nclasses C. The output ( T\n8P , C) is regarded as a event pres-\nence map. Finally, we average the featuremap as the final\nvector (1, C) to compute the binary cross-entropy loss with\nthe groundtruth labels. Apart from the localization function-\nality, we also expect the token-semantic module to improve\nthe classification performance, as it considers the final output\nby directly grouping all tokens .\n3. EXPERIMENTS\nIn this section, we evaluate the performance of HTS-AT in\nfour datasets: the event classification on AudioSet [7], ESC-\n50 [8]; the keyword spotting on Speech Command V2 [4];\nand additionally, the event detection on DESED [18].\n3.1. Event Classification on AudioSet\n3.1.1. Dataset and Training Detail\nThe AudioSet contains over two million 10-sec audio sam-\nples labeled with 527 sound event classes. In this paper, we\nfollow the same training pipeline in [11, 12, 14] by using the\nfull-train set (2M samples) to train our model and evaluating\nit on the evaluation set (22K samples). All samples are con-\nverted to mono as 1 channel by 32kHz sampling rate. We\nuse 1024 window size, 320 hop size, and 64 mel-bins to com-\npute STFTs and mel-spectrograms. As a result, the shape of\nthe mel-spectrogram is(1024, 64) as we pad each 1000-frame\n(10-sec) sample with 24 zero-frames ( T=1024, F=64). The\nshape of the output featuremap is (1024, 527) (C=527). The\npatch size is 4 × 4, the patch window length is 256 frames,\nand the attention window size is 8 × 8. Since 8 is divisi-\nble by 64, the attention window in the first layer will not span\ntwo frames with a large time difference. The latent dimension\nsize is D=96 and the final output latent dimension is8D=768,\n2AST provides a second bigger ensemble result by using models with\ndifferent patch settings, which is partially comparable with our settings.\nModel ESC-50 Acc.(%) Model SCV2 Acc.(%)\nPANN [11] 90.5 RES-15 [21] 97.0\nAST [14] 95.6 ±0.4 AST [14] 98.1±0.05\nERANN [22] 96.1 KWT-2 [23] 97.3 ±0.03\nHTS-AT 97.0±0.2 HTS-AT 98.0±0.03\nTable 2: The accuracy score results on ESC-50 dataset and\nSpeech Command V2 (SCV2).\nwhich is consistent to AST. Finally, we set 4 network groups\nwith 2, 2, 6, 2 swin-transformer blocks respectively.\nWe follow [11, 12] to use the balance sampler, α = 0.5\nmix-up [19], spectrogram masking [20] with time-mask=128\nframes and frequency-mask=16 bins, and weight averag-\ning. The HTS-AT is implemented in Pytorch and trained\nvia the AdamW optimizer ( β1=0.9, β2=0.999, eps=1e-8, de-\ncay=0.05) with a batch size of 128 ( 32 × 4) in 4 NVIDIA\nTesla V-100 GPUs. We apply a warm-up schedule by setting\nthe learning rate as 0.05, 0.1, 0.2 in the first three epochs, then\nthe learning rate is halved every ten epochs until it returns to\n0.05. We use the mean average precision (mAP) to evaluate\nthe classification performance.\n3.1.2. Experimental Results\nIn Table 1, we compare our HTS-AT with different bench-\nmark models and three self-ablated variations: (1)H: only hi-\nerarchical structure; (2) HC : with hierarchical structure and\ntoken-semantic module; and (3) HCP : (2) with pretrained\nvision model (the full setting). Our best setting achieves a\nnew SOTA mAP 0.471 in a single model as a large increment\nfrom 0.459 by AST. We also ensemble six HTS-ATs with dif-\nferent training random seeds in the same settings to achieve\nthe mAP as 0.487, and outperforms AST’s 0.475 and 0.485.\nWe analyze our results in two facets.\nToken Semantic Module and Pretraining PSLA, AST\nand HTS-AT adopt the ImageNet-pretrained model, where\nPSLA uses the pretrained EfficientNet [25], AST uses DeiT\n[26], and our HTS-AT uses the swin-transformer in Swin-\nT/C24 setting3 for 256 ×256 images (256 ×256 = 1024×64\nas we could transfer the same size weights). We can see\nthat the unpretrained single HTS-AT can achieve an mAP as\n0.440. It is improved to 0.453 by the addition of token se-\nmantic module, 1.8% lower than 0.471. Finally the pretrained\nHTS-AT achieves the new best mAP as 0.471. However, the\nunpretrained single AST only reflects 0.366, 9.3% lower than\n0.459. These indicate that: (1) the pretrained model definitely\nimproves the performance by building a solid prior on pattern\nrecognition; and (2) HTS-AT shows a far better scalability\nto different hyperparameters than AST, since its unpretrained\nmodel can still achieve the third best performance.\nParameter Size and Training Time When comparing the\nparameter size of each model, the AST has 87M parame-\n3https://github.com/microsoft/Swin-Transformer\nModel Alarm Blender Cat Dishes Dog Shaver Frying Water Speech Cleaner Average\nPANN [11] 34.3 42.4 36.3 17.6 35.8 23.8 9.3 30.6 69.7 51.0 35.1\nHTS-AT 48.6 52.9 67.7 25.0 48.0 42.9 60.3 43.0 46.8 49.1 48.4\nHTS-AT - Ensemble 47.5 55.1 72.4 30.9 49.7 41.9 63.2 44.3 51.3 50.6 50.7\nZheng et al.* [24] 41.4 54.1 72.4 29.4 47.8 61.01 49.2 33.7 69.5 65.5 52.4\nKim et al.* [24] 34.7 59.8 71.6 40.4 47.3 26.2 61.8 32.8 64.9 66.7 50.6\nLu et al.* [24] 37.1 41.4 62.5 40.6 39.7 46.5 46.5 34.5 54.5 46.9 45.0\nTable 3: The event-based F1-scores of each class on the DESED test set. Models with* are from DCASE 2021 [24], which are\npartial references since they use extra training data and are evaluated on DESED test set and its another private subset.\nters. And HTS-AT is more lightweight with 31M parameters,\nwhich is even compatible with CNN-based models. As for the\nestimated training time, PANN takes about 72 hours to con-\nverge and HST-AT takes about 20 × 4 = 80 hours in V-100\nGPUs; and AST takes about150×4 = 600hours in 4 TITAN\nRTX GPUs4. The speed improvement corresponds to the less\ncalculation and GPU memory consumption of HTS-AT, as we\ncould feed 128 samples instead of only 12 samples in AST per\nbatch. Therefore, we conclude that HTS-AT consumes less\ntraining time and has fewer parameters than AST’s, which is\nmore efficient.\n3.2. Evaluations on ESC-50 and Speech Command V2\n3.2.1. Dataset and Training Detail\nThe ESC-50 dataset contains 2000 5-sec audio samples la-\nbeled with 50 environmental sound classes in 5 folds. We\ntrain the model for 5 times by selecting 4-fold (1600 sam-\nples) as training set and the left 1-fold (400 samples) as test\nset. And we repeat this experiment 3 times with different ran-\ndom seeds to get the mean performance and deviation. The\nSpeech Command V2 contains 105,829 1-sec spoken word\nclips labled with 35 common word classes. It contains 84843,\n9981, and 11005 clips for training, validation and evaluation.\nSimilarly, we train our HTS-AT for 3 times to obtain the pre-\ndiction results. We use the mean accuracy score (acc) for the\nevaluation on both datasets. For the data processing, we re-\nsample the ESC-50 samples into 32kHz and the Speech Com-\nmand clips 16kHz. And we follow the same setting of Au-\ndioSet to train the model.\n3.2.2. Experimental Results\nWe use our best AudioSet-pretrained HTS-AT to train on\nthese two dataset respectively and compare it with bench-\nmark models (also in AudioSet or extra data pretraining).\nSince 1-sec and 5-sec does not take the full 10-sec input\ntrained on AudioSet, we repeat the 1-sec and 5-sec by 10 and\n2 times to make it 10-sec. As shown in Table 2, the results\nshows that our HTS-AT achieves a new SOTA as 97.0% on\nESC-50 dataset and equals the SOTA 98.0% on Speech Com-\nmand V2. Our deviations are relatively smaller than AST’s,\nindicating that HTS-AT is more stable after convergence.\n4We make memories not exceed 12GB in V-100 in line with TITAN RTX.\n3.3. Localization Performance on DESED\nWe additionally evaluate HTS-AT’s capability to localize the\nsound event as start and end time in given audio samples.\nWe use the DESED test set [18], which contains 692 10-sec\ntest audio samples in 10 classes with the strong labels. We\nmainly compare our HTS-AT with PANN. We do not include\nAST and PSLA since AST does not directly support the event\nlocalization and the PSLA’s code is not published. We also\ncompare it partially with models in DCASE 2021 [24], nev-\nertheless they use extra training data and are evaluated on\nDESED test set and its another private subset. We use the\nevent-based F1-score on each class as the evaluation metric,\nimplemented by a Python library psds eval5.\nThe F1-scores on all 10 classes in the DESED by different\nmodels are shown in Table 3. We find that HTS-AT achieves\nbetter F1-scores on 8 classes and a better average F1-score\n50.7% than PANN. When compared among leaderboard mod-\nels, our model still achieves some highest scores of certain\nclasses. However, the F1-scores on Speech and Cleaner are\nrelatively low, indicating that there are still some improve-\nments for a better localization performance. From the above\nexperiments, we can conclude that HTS-AT is able to produce\nthe specific localization output via the token-semantic mod-\nule, which extends the functionality of the audio transformer.\n4. CONCLUSION AND FUTURE WORK\nIn this paper, we propose HTS-AT: a hierarchical token-\nsemantic transformer for audio classification. It achieves a\nnew SOTA on multiple datasets of different audio classifi-\ncation scenarios. Furthermore, the token-semantic module\nenables HTS-AT to locate the events start and end time. Ex-\nperiments show that HTS-AT is a high performance, high\nscalability, and lightweight audio transformer. In the future,\nwe notice that a partial strong labeled subset of AudioSet\nhas just been released [27], we decide to conduct a detail\nlocalization training and evaluation work by HTS-AT to fur-\nther explore its potential. Combining the audio classification\nmodel into more downstreaming tasks [28, 29] is also consid-\nered a future work.\n5https://github.com/audioanalytic/psds eval\n5. REFERENCES\n[1] Annamaria Mesaros, Toni Heittola, Tuomas Virtanen,\nand Mark D. Plumbley, “Sound event detection: A tuto-\nrial,” IEEE Signal Process. Mag. 2021.\n[2] Perfecto Herrera, Geoffroy Peeters, and Shlomo Dub-\nnov, “Automatic classification of musical instrument\nsounds,” Journal of New Music Research, 2010.\n[3] Ke Chen, Beici Liang, Xiaoshuan Ma, and Minwei Gu,\n“Learning audio embeddings with user listening data\nfor content-based music recommendation,” in ICASSP\n2021.\n[4] Pete Warden, “Speech commands: A dataset for\nlimited-vocabulary speech recognition,” CoRR, vol.\nabs/1804.03209, 2018.\n[5] Ke Chen, Cheng-i Wang, Taylor Berg-Kirkpatrick, and\nShlomo Dubnov, “Music sketchnet: Controllable mu-\nsic generation via factorized representations of pitch and\nrhythm,” in ISMIR 2020.\n[6] Hao-Wen Dong, Ke Chen, Julian J. McAuley, and Tay-\nlor Berg-Kirkpatrick, “Muspy: A toolkit for symbolic\nmusic generation,” in ISMIR 2020.\n[7] Jort F. Gemmeke, Daniel P. W. Ellis, and Dylan Freed-\nman et al., “Audio set: An ontology and human-labeled\ndataset for audio events,” in ICASSP 2017.\n[8] Karol J. Piczak, “ESC: dataset for environmental sound\nclassification,” in ACM MM 2015. ACM.\n[9] Logan Ford, Hao Tang, and Franc ¸ois Grondin et al.,\n“A deep residual network for large-scale acoustic scene\nanalysis,” in Interspeech 2019.\n[10] Yun Wang, Juncheng Li, and Florian Metze, “A com-\nparison of five multiple instance learning pooling func-\ntions for sound event detection with weak labeling,” in\nICASSP 2019.\n[11] Qiuqiang Kong, Yin Cao, and Turab Iqbal et al., “Panns:\nLarge-scale pretrained audio neural networks for audio\npattern recognition,” IEEE TASLP 2020.\n[12] Yuan Gong, Yu-An Chung, and James Glass, “Psla: Im-\nproving audio tagging with pretraining, sampling, label-\ning, and aggregation,” IEEE TASLP 2021.\n[13] Ashish Vaswani, Noam Shazeer, and Niki Parmar et al.,\n“Attention is all you need,” in NeurIPS 2017.\n[14] Yuan Gong, Yu-An Chung, and James Glass, “Ast: Au-\ndio spectrogram transformer,” in Interspeech 2021.\n[15] Jia Deng, Wei Dong, and Richard Socher et al., “Im-\nagenet: A large-scale hierarchical image database,” in\nCVPR 2009.\n[16] Wei Gao, Fang Wan, and Xingjia Pan et al., “Ts-cam:\nToken semantic coupled attention map for weakly su-\npervised object localization,” in ICCV 2021.\n[17] Ze Liu, Yutong Lin, and Yue Cao et al., “Swin trans-\nformer: Hierarchical vision transformer using shifted\nwindows,” CoRR, vol. abs/2103.14030, 2021.\n[18] Romain Serizel, Nicolas Turpault, Ankit Parag Shah,\nand Justin Salamon, “Sound event detection in synthetic\ndomestic environments,” in ICASSP 2020.\n[19] Hongyi Zhang, Moustapha Ciss ´e, Yann N. Dauphin, and\nDavid Lopez-Paz, “mixup: Beyond empirical risk min-\nimization,” in ICLR 2018.\n[20] Daniel S. Park et al., “Specaugment: A simple data aug-\nmentation method for automatic speech recognition,” in\nInterspeech 2019.\n[21] Roman Vygon and Nikolay Mikhaylovskiy, “Learn-\ning efficient representations for keyword spotting with\ntriplet loss,” in SPECOM 2021. Springer.\n[22] Yang Liu, Alexandras Neophytou, Sunando Sengupta,\nand Eric Sommerlade, “Cross-modal spectrum trans-\nformation network for acoustic scene classification,”\nin ICASSP 2021-2021 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2021, pp. 830–834.\n[23] Axel Berg, Mark O’Connor, and Miguel Tairum Cruz,\n“Keyword transformer: A self-attention model for key-\nword spotting,” in Interspeech 2021.\n[24] “Dcase 2021 challenge task 4: Sound event de-\ntection and separation in domestic environments,”\nhttp://dcase.community/challenge2021.\n[25] Mingxing Tan and Quoc V . Le, “Efficientnet: Rethink-\ning model scaling for convolutional neural networks,” in\nICML 2019. PMLR.\n[26] Hugo Touvron, Matthieu Cord, and Matthijs Douze\net al., “Training data-efficient image transformers & dis-\ntillation through attention,” in ICML 2021.\n[27] Shawn Hershey, Daniel P. W. Ellis, and Eduardo Fon-\nseca et al., “The benefit of temporally-strong labels in\naudio event classification,” in ICASSP 2021.\n[28] Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Tay-\nlor Berg-Kirkpatrick, and Shlomo Dubnov, “Zero-shot\naudio source separation through query-based learning\nfrom weakly-labeled data,” in AAAI 2022.\n[29] Ke Chen, Gus Xia, and Shlomo Dubnov, “Continu-\nous melody generation via disentangled short-term rep-\nresentations and structural conditions,” in IEEE ICSC\n2020.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8050611019134521
    },
    {
      "name": "Transformer",
      "score": 0.6919550895690918
    },
    {
      "name": "Security token",
      "score": 0.6900589466094971
    },
    {
      "name": "Scalability",
      "score": 0.622054934501648
    },
    {
      "name": "Speech recognition",
      "score": 0.5488529801368713
    },
    {
      "name": "Granularity",
      "score": 0.4272370934486389
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4141961634159088
    },
    {
      "name": "Database",
      "score": 0.09010908007621765
    },
    {
      "name": "Engineering",
      "score": 0.08746609091758728
    },
    {
      "name": "Computer network",
      "score": 0.07354140281677246
    },
    {
      "name": "Voltage",
      "score": 0.06676238775253296
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I36258959",
      "name": "University of California, San Diego",
      "country": "US"
    }
  ]
}