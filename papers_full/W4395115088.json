{
  "title": "A multi-channel spatial-temporal transformer model for traffic flow forecasting",
  "url": "https://openalex.org/W4395115088",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5101774246",
      "name": "Jianli Xiao",
      "affiliations": [
        "University of Shanghai for Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5113109501",
      "name": "Baichao Long",
      "affiliations": [
        "University of Shanghai for Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6811038593",
    "https://openalex.org/W3123909522",
    "https://openalex.org/W2579495707",
    "https://openalex.org/W4367595602",
    "https://openalex.org/W4315928962",
    "https://openalex.org/W6853526968",
    "https://openalex.org/W2074991364",
    "https://openalex.org/W2991103574",
    "https://openalex.org/W2613322775",
    "https://openalex.org/W6696968006",
    "https://openalex.org/W2073640212",
    "https://openalex.org/W2093961844",
    "https://openalex.org/W1988489815",
    "https://openalex.org/W2901504064",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6790076102",
    "https://openalex.org/W6806889972",
    "https://openalex.org/W6807079475",
    "https://openalex.org/W4313594206",
    "https://openalex.org/W4285891076",
    "https://openalex.org/W6801254957",
    "https://openalex.org/W4315647796",
    "https://openalex.org/W3136758818",
    "https://openalex.org/W6848164945",
    "https://openalex.org/W3207611353",
    "https://openalex.org/W4322755838",
    "https://openalex.org/W4296690332",
    "https://openalex.org/W6762978078",
    "https://openalex.org/W6757242954",
    "https://openalex.org/W6772631766",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W4313004671",
    "https://openalex.org/W2531563875",
    "https://openalex.org/W4242926283",
    "https://openalex.org/W2247875277",
    "https://openalex.org/W2996847713",
    "https://openalex.org/W2903871660",
    "https://openalex.org/W4225157025",
    "https://openalex.org/W4381384273"
  ],
  "abstract": null,
  "full_text": "A Multi-Channel Spatial-Temporal Transformer Model\nfor Traffic Flow Forecasting\nJianli Xiao∗, Baichao Long\nSchool of Optical-Electrical and Computer Engineering, University of Shanghai for\nScience and Technology, Shanghai, 200093, China\nAbstract\nTraffic flow forecasting is a crucial task in transportation management and\nplanning. The main challenges for traffic flow forecasting are that (1) as\nthe length of prediction time increases, the accuracy of prediction will de-\ncrease; (2) the predicted results greatly rely on the extraction of temporal\nand spatial dependencies from the road networks. To overcome the challenges\nmentioned above, we propose a multi-channel spatial-temporal transformer\nmodel for traffic flow forecasting, which improves the accuracy of the predic-\ntion by fusing results from different channels of traffic data. Our approach\nleverages graph convolutional network to extract spatial features from each\nchannel while using a transformer-based architecture to capture temporal de-\npendencies across channels. We introduce an adaptive adjacency matrix to\novercome limitations in feature extraction from fixed topological structures.\nExperimental results on six real-world datasets demonstrate that introducing\na multi-channel mechanism into the temporal model enhances performance\nand our proposed model outperforms state-of-the-art models in terms of ac-\ncuracy.\nKeywords:\nmulti-channel, graph convolutional network, Transformer, traffic flow\nforecasting\n∗Corresponding author.\nEmail address: audyxiao@sjtu.edu.cn (Jianli Xiao)\nPreprint submitted to Information Sciences May 13, 2024\narXiv:2405.06266v1  [cs.AI]  10 May 2024\n1. Introduction\nWith the rapid development of urbanization, urban traffic has become a\npressing issue that needs to be addressed. The increasing number of vehicles\nin cities has led to problems such as road congestion, traffic accidents, and\nenvironmental pollution becoming increasingly severe. Traffic flow forecast-\ning, an important component of intelligent transportation systems, provides\ntechnical support for alleviating traffic congestion, reducing traffic accidents,\nand lowering environmental pollution [1]. However, due to the complex spa-\ntial and temporal dependencies of traffic flow, traffic flow forecasting remains\na challenging problem. In recent years, most research has used deep learning\nto model traffic flow. Graph neural networks (GNNs) [2] and convolutional\nneural networks (CNNs) [3] are commonly used for modeling on the spatial\nscale. Recurrent neural networks (RNNs) [4] including its variants such as\nlong short-term memory (LSTM) [5] and gated recurrent unit (GRU) [6] are\ncommonly used for modeling on the temporal scale. With the popularity of\nTransformers, which have also been used for traffic flow forecasting tasks,\nbut the complex spatial-temporal correlations of traffic flow are still difficult\nto learn.\nTraffic flow forecasting models can be divided into three types: para-\nmetric models, non-parametric models, and hybrid models. Early statistical\nmodels such as auto-regressive integrated moving average models [7], grey\nprediction models [8], and Kalman filter models [9] are typical parameter\nmodels, which rely on static assumptions of the system and could not reflect\nthe nonlinearity and uncertainty of traffic data. Support vector regression\nmodels [10], K-nearest neighbor models [11], Bayesian models [12], and fuzzy-\nneural models [13] are classical non-parametric models. All of these models\nonly consider the temporal dependence of traffic flow and ignore the spatial\ndependence between roads. This makes traffic flow forecasting results uncon-\nstrained by the road network, and it is unable to accurately predict actual\ntraffic conditions. Hybrid models are composed of multiple models combined\nto solve multi-scale dependence problems. Due to the excellent ability of\ngraph convolutional networks (GCNs) [14] to extract spatial features, they\nare widely utilized in traffic flow forecasting tasks. Many researchers have\ncombined spatial models with temporal models to form hybrid models for\nspatial-temporal prediction tasks, which simultaneously consider both spa-\ntial and temporal dependencies [15].\nThe traffic flow in urban road networks exhibits randomness, uncertainty,\n2\nFigure 1: Visualization of spatial-temporal correlation: (a) distribution of urban road\nand traffic sensors, where red, yellow, and blue dots represent traffic sensors at different\nlocations with different traffic conditions; (b) time slices of road network from time t1 to\ntN, where the current traffic flow at each node is not only related to the traffic conditions\nin the previous slice of the day, but also related to the historical traffic flow at the same\ntime on previous days.\nand periodicity. This phenomenon is primarily influenced by factors such as\naccidents, changes in city planning, and fluctuations in population. The in-\nteraction of these factors leads to variations and fluctuations in traffic flow,\nwhich are not only related to adjacent time slices but also to previous daily,\nweekly, and monthly patterns at that time. As shown in Fig. 1(a), the red\ndot represents a node of school road. Due to students’ fixed arrival and\ndeparture times, the traffic conditions at the same location during peak pe-\nriod exhibit long-term similarities. Therefore, when predicting traffic flow, it\nis necessary to consider not only short-term temporal correlations but also\nlong-term temporal correlations, which means taking into account the im-\npact of multiple time channels on the results. The arrival and departure\ntimes of other schools within the road network are generally synchronized,\nso even if the distance between different schools is considerable, their traffic\nflow will exhibit a certain correlation. Therefore, when predicting traffic flow,\nit is necessary to consider not only spatial correlations in terms of distance\nbut also dynamic spatial correlations. This requires incorporating not only\nstatic graph structures but also dynamic graph structures in GCNs. As il-\nlustrated in Fig. 1(b), we can predict the traffic conditions at time tN based\non the traffic data from time t1 to tN−1 on Thursday, taking into account the\ntemporal dependence of consecutive time slices as mentioned earlier. Addi-\ntionally, we need to incorporate data from the same time slice on Wednesday,\n3\nTuesday, and Monday to predict the traffic conditions at this time, consid-\nering the temporal dependencies across different days. To achieve the afore-\nmentioned object, we propose a new Multi-Channel Spatial-Temporal Model\n(MC-STTM) for traffic flow forecasting. Data are extracted from multiple\nchannels and passed through the model for spatial-temporal feature traffic\nflow forecasting. For spatial-temporal feature extraction, GCNs and Trans-\nformers [16] are used. In GCNs, both the adjacency matrix generated from\nknown topological structures and the adaptive adjacency matrix are jointly\nutilized to extract spatial features of road networks, capturing spatial depen-\ndencies more accurately. In the position encoding of Transformer, not only\nall time intervals within a day are encoded, but each day of the week from\nMonday to Sunday is also encoded.\nIn summary, the distinction of our model from previous studies lies in\nthe introduction of a multi-channel mechanism to account for the impact\nof cyclical factors on traffic flow forecasting. At the spatial scale, we con-\nsider both static and dynamic spatial dependencies to fully capture spatial\ninformation. At the temporal scale, although the Transformer architecture\nis currently very popular, there are still some studies using other deep time\nseries models. In this paper, we compare the Transformer architecture with\nother deep time series models before and after the introduction of the multi-\nchannel mechanism as a way to justify our choice of Transformer. Previous\nstudies have focused on the combination of time series models with spatial\nmodels, and few have compared the models at different scales. Therefore,\nMC-STTM is proposed in this paper. We evaluate our approach on six pub-\nlic traffic datasets, and the experimental results demonstrate that our model\nachieves promising performance compared to state-of-the-art baseline mod-\nels. The main contributions of this paper are as follows:\n• We propose a multi-channel data fusion model for traffic flow fore-\ncasting, which models the traffic network by incorporating historical\ntraffic data from different channels, outperforming existing traffic flow\nforecasting models that do not leverage multi-channel data.\n• We utilize GCNs to extract spatial features, where the construction\nof the adjacency matrix differs from previous models. We combine\nan adaptive adjacency matrix with a known road network topology-\ngenerated adjacency matrix to further capture spatial dependencies.\n• We utilize Transformers to extract temporal features, which exhibit\n4\ncompetitive performance in long-term prediction compared to RNNs.\nWe also introduce different position encoding mechanisms in different\nchannels to further capture temporal dependencies.\n• We conduct comparative experiments on multiple datasets, including\nboth traffic flow datasets and speed datasets. The results show that\nour model outperforms baseline models.\nPaper Organization. In Section 2, we provide a detailed review of\nthe main developments in traffic flow forecasting. In Section 3, we present\nMC-STTM framework in detail. Subsequently, Section 4 outlines the exper-\nimental design and results. Lastly, in Section 5, we summarize our findings\nand conclusions.\n2. Related Works\nIn this section, we present a comprehensive review of the primary research\non traffic flow forecasting. The existing models for predicting traffic flow can\nbe broadly classified into three categories: temporal models that rely on\ntemporal-based scales, spatial models that rely on spatial-based scales, and\nhybrid models that combine both temporal and spatial scales.\n2.1. Temporal Models\nWith the rapid development of deep learning, traditional models are not\ncommonly used in existing traffic flow forecasting tasks. RNNs and their\nvariants, such as LSTM and GRU, have become commonly used time series\nmodels in recent years. Transformers are the most popular deep learning\nnetwork architecture in recent years, mainly used for sequence-to-sequence\nlearning tasks. In time series prediction, we can use Transformers to learn the\ncorresponding mapping relationships. Compared with RNNs, Transformers\nachieves good results in handling long-term dependencies and can better\ncapture relationships between sequences through position encoding and other\nmodels. Transformers can also be computed in parallel and can handle input\nsequences of any length, making the model more flexible. In summary, as the\ndevelopment of time series models has progressed, Transformers have become\nthe most advantageous time series network model architecture for traffic flow\nforecasting tasks.\n5\n2.2. Spatial Models\nTime series prediction models only consider temporal dependencies, ne-\nglecting spatial dependencies, which results in inaccurate traffic flow forecast-\ning that do not take into account the constraints imposed by road networks.\nSome scholars have introduced CNNs into spatial dependency models, which\nare highly effective in processing regular grid data. However, road network\nstructures are often irregular graph structures, and CNNs, which are inher-\nently suited to Euclidean spaces, have limitations when dealing with the\ncomplex topologies of traffic road networks. They are fundamentally un-\nable to describe spatial dependencies. Compared to CNNs, GCNs perform\nbetter in processing non-Euclidean structured data, as they can capture the\nstructural features of graph data. As the two models face different data struc-\ntures, GCNs are widely used for traffic flow forecasting on road networks and\nachieve good results. For graph models, we need to know sufficient informa-\ntion about the topology of road network graphs, such as adjacency matrices\nand feature matrices, meaning that adequate topological information about\nroad networks is needed to adopt spatial models. To solve the problem of\nincomplete topological information for spatial feature extraction, some schol-\nars have proposed models such as adaptive adjacency matrices. In summary,\nthe model of GCNs has become the most popular model for traffic flow fore-\ncasting tasks.\n2.3. Hybrid Models\nIn the current research field, hybrid methods combining time series and\nspatial dependency models have emerged as powerful methods for analyzing\nand forecasting traffic flow. This approach is particularly suited for extract-\ning temporal characteristics and spatial structural information from road\nnetwork data. In recent years, researchers have developed advanced models\nby integrating time series and spatial dependency models, achieving signif-\nicant progress in this field. Spatio-Temporal Graph Convolutional Network\n(STGCN) [17] is a typical model among these, employing a graph-based ap-\nproach instead of traditional convolutional and recurrent structures to han-\ndle the non-linearity and complexity of traffic flow. However, as Lu and Li\n[18] note, traditional GNNs have limitations in modeling the time-varying\nspatial-temporal correlations among sensors and often rely on predefined\ngraph structures, which are not always available or entirely reliable. To\novercome this challenge, Topologically Enhanced Spatial-Temporal Graph\nConvolutional Network [19] was designed to capture spatial dependencies in\n6\ndata more effectively. Another approach, Trajectory WaveNet [20], addresses\nthe shortcomings of graph-based methods in capturing complex dependen-\ncies between continuous road segments, such as prohibited left turns and\ndynamic spatial dependencies. Liu et al. [21] combines spatial-temporal\nsimilarity feature modules with spatial-temporal convolution modules that\nincorporate attention mechanisms. This combination effectively solves the\nchallenge of extracting spatial-temporal relations from complex traffic flows.\nZhang et al. [22] emphasize that simple and fixed spatial graph structures,\nrelying only on prior knowledge of the traffic network, can lead to poor\npredictive performance. Similarly, Zheng and Zhang [23] point out that\nstatic graphs cannot directly learn dynamic spatial-temporal dependencies\nacross time periods. Yang et al. [24] adopted a novel method combining\nspatial-temporal convolution and attention mechanisms to learn extensive\nspatial-temporal correlations. The proposed local spacetime neural network\nmethod aims to capture local traffic patterns without relying on a specific\nnetwork structure. Multi-Scale Temporal Dual Graph Convolution Network\n[25] combining gated spatial-temporal convolution and dual graph convo-\nlution modules, effectively extracting spatial-temporal correlations between\nnodes in traffic flow forecasting. It is a critical task that obtaining the graph\nsequence models by connecting graph models from different time periods, in\nthe field of graph theory [26]. Particularly in the domain of traffic flow fore-\ncasting, the construction of effective dynamic graph models has emerged as\na focal point of research. Multiview Spatial-Temporal Transformer Network\n[27] addresses the challenge of accurately predicting traffic flow by integrat-\ning multiple views for complex spatial-temporal feature learning. Although\ncertain models demonstrate high accuracy in short-term forecasting, its per-\nformance significantly diminishes in long-term forecasting, thereby limiting\nits overall effectiveness. In such conditions, certain models may only be suit-\nable for specific short-duration or long-duration tasks. Despite their overall\nutility, their performance in long-term forecasting tasks may still be notably\npoor, which is an important consideration in model selection and application.\nA key issue is the decline in accuracy for long-term forecasting. Spatial-\nTemporal Transformer Networks (STTN) [28] improve the accuracy of long-\nterm flow forecasting by combining dynamic directional spatial dependencies\nand long-range temporal dependencies. Fang et al. [29] introduced a LSTM\nwith attention mechanisms for short-term traffic flow forecasting, allocating\nweights to different inputs through the attention mechanism to focus on key\ninformation and improve forecasting accuracy. M´ endez et al. [30] presented\n7\na hybrid model combining CNNs and Bi-directional LSTMs to address long-\nterm traffic flow forecasting issues. Djenouri et al. [31] explored an innovative\ngraph convolutional neural network for urban traffic flow forecasting in edge\nIoT environments, employing a combined approach of graph optimization\nand forecasting.\nIn summary, researchers have developed various advanced spatial-temporal\nmodels that utilize techniques such as GNNs, RNNs, attention mechanisms,\nand Transformers to capture the complex spatial and temporal dependencies\nand correlations within traffic networks. Some models also incorporate exter-\nnal factors such as weather and road conditions to enhance the accuracy of\ntraffic flow forecasting. Previous work shows that obtaining both static and\ndynamic graph structures is crucial, and forecasting accuracy significantly de-\nclines with increased forecasting extent. Especially when considering urban\ntraffic and long-term forecasting simultaneously, the accuracy of these mod-\nels may decrease. Previous research mainly focused on graph structures and\nshort-term or long-term forecasting, whereas this paper considers both static\nand dynamic spatial dependencies and introduces a multi-channel mecha-\nnism, combined with the characteristics of Transformers, aiming to ensure\nthe accuracy of short-term forecasting while effectively capturing long-term\ntemporal dependencies.\n3. Proposed Model\nIn this section, we describe MC-STTM in detail. First, we introduce\nsome necessary symbols and definitions that will be used throughout the\npaper. Then, we describe the overall framework of our model, which is\nmainly composed of four parts: multi-channel mechanism, spatial-temporal\nblock, feature fusion layer, and prediction layer. Afterwards, we elaborate\non the main innovative parts of the framework: multi-channel mechanism,\nspatial block, and temporal Transformer.\n3.1. Problem Definition\nThe topological structure of the road network is represented as a graph\nG= ( V,Θ,A), where V = {V1,V2,··· ,VM }denotes the set of nodes cor-\nresponding to M roads or sensors in the road network. The set of edges is\ndenoted by Θ, and the spatial adjacency matrixA ∈RM×M contains the con-\nnectivity of the roads in the network (e.g., computed based on the Euclidean\ndistance between two points).\n8\nFigure 2: The framework of MC-STTM.\nXhour and Xday represent two channels, which are selected from historical\ntime slices at different time intervals. Xhour =\n[\nXt−p+1,Xt−p+2,··· ,Xt\n]\nrepresents recent traffic conditions, which exhibits temporal correlation over\na short period of time; Xday =\n[\nXt−ds,Xt−(d−1)s,··· ,Xt−s\n]\nrepresents long-\nterm traffic conditions, reflecting temporal correlation over a longer period\nof time; Y = [ Xt+1,Xt+2,··· ,Xt+q] represents future traffic conditions; p\nrepresents the number of historical time slices; d represents the number of\nhistorical days; s represents the number of time slices in a day (if the time\ninterval of each time slice is 5 minutes, then s = 288); q represents the\nprediction horizon, where q = 1 for single-step prediction and q > 1 for\nmulti-step prediction.\nThe task of traffic flow forecasting can be defined as follows: given the\nhistorical traffic conditions Xhour and Xday, and a traffic road network topol-\nogy G, learn a prediction model F to forecast Y. The expression can be\nrepresented as Eq. (1):\nY = F(G; Xhour; Xday) . (1)\n3.2. Model Framework\nAs depicted in Fig. 2, MC-STTM adopts a multi-channel mechanism\nfor input, where the features of the input are first dimensionally expanded\nand then subjected to stacked spatial-temporal blocks for feature extrac-\ntion. Subsequently, the feature fusion block is employed, followed by the\nprediction layer, which yields the final prediction result. The dimensional\nexpansion of the features can capture more information and relationships,\nthereby improving the model’s performance and generalization ability. Each\nspatial-temporal block consists of a spatial block and a temporal block that\njointly extract spatial-temporal features. The stacking of spatial-temporal\n9\nFigure 3: The architrctaure of spatial-temporal block.\nblocks forms a deep model, which facilitates better feature extraction. Fur-\nthermore, the feature fusion adopts a gate mechanism similar to GRU that\nnot only prevents gradient vanishing and explosion but also facilitates learn-\ning long-term dependencies. Finally, the prediction layer aggregates spatial-\ntemporal features through two 1D convolution operations, enabling traffic\nflow forecasting.\nMulti-Channel Mechanism. The model learns from historical data and the\ntopological structure of the road network Gto predict future traffic condi-\ntions, which represents a single-channel prediction process. According to Eq.\n(1), the so-called multi-channel refers to learning the model from historical\ndata at multiple scales and the topological structure of the road network G\nto predict future traffic conditions. We can select historical data at different\nintervals, such as time slices ranging from a few minutes to several days or\neven weeks. This mechanism can learn more features and improve the perfor-\nmance of the model. We select Xhour and Xday, where Xhour is characterized\nby short-term temporal correlations while Xday displays long-term temporal\ncorrelations.\nSpatial-Temporal Block. The future traffic condition of each node in a road\nnetwork is influenced by multiple factors, such as the traffic conditions of ad-\njacent or similar nodes, historical and forecast time steps (i.e., pand q), and\nspecial events (e.g., traffic accidents and extreme weather conditions). We\npropose a spatial-temporal block that not only uses the spatial feature ex-\ntractor GCN but also incorporates the Transformer, which show good perfor-\n10\nmance in long-term prediction. As shown in Fig. 2, multiple spatial-temporal\nblocks are stacked and connected by residual connections. The input to the\nk-th spatial-temporal block is Xk (Xk\nhour ∈RM×p×fd or Xk\nday ∈RM×d×fd )\nand Gk, a 3D tensor with three dimensions representing the number of nodes\nM, the length of historical time slices p or d, and the size of the feature\ndimension after expansion fd. The output of the k-th spatial-temporal block\nis Xk+1 (Xk+1\nhour ∈RM×p×fd or Xk+1\nday ∈RM×d×fd ). In fact, the input of the\nnext spatial-temporal block is jointly composed of the input and output of\nthe previous spatial-temporal block. To simplify the expression, this is not\nexplicitly reflected in the formula but can be clearly observed in Fig. 2.\nXk+1 = Fk\n(\nGk; Xk\n)\n. (2)\nTherefore, we can infer that the output of the final spatial-temporal\nblock is given by XST = Conv(Xk + Xk+1), where XST represent either\nXST\nhour ∈RM×p×fd or XST\nday ∈RM×d×fd . Finally, to achieve dimension align-\nment between XST\nhour and XST\nday, they are separately processed through a 1D\nconvolution, resulting in ˆXST\nhour ∈RM×p×fd and ˆXST\nday ∈RM×p×fd .\nFeature Fusion. A gate mechanism similar to GRU is utilized to fuse ˆXST\nhour\nand ˆXST\nday, with the gate g calculated from both inputs.\ng= sigmoid\n(\nf1\n(\nˆXST\nhour\n)\n+ f2\n(\nˆXST\nday\n))\n, (3)\nwhere f1 and f2 denote linear transformations that convert ˆXST\nhour and ˆXST\nday\ninto 1D vectors, which are then passed through a sigmoid function.\nˆXST\ngate = gˆXST\nhour + (1 −g) ˆXST\nday. (4)\nThe output ˆXST\ngate ∈RM×p×fd from the feature aggregation layer is used\nas input for the final prediction layer to generate the ultimate result.\nPrediction Layer. The prediction layer consists of two 1D convolutions, which\nperform single-step or multi-step forecasting on ˆXST\ngate based on pand q, and\nthe features of ˆXST\ngate are dimensionally reduced from fd to 1. The final\npredicted result is Y ∈RM×q×1.\nY = Conv\n(\nConv\n(\nˆXST\ngate\n))\n. (5)\n11\n3.3. Multi-Channel Mechanism\nIn this study, a multi-channel mechanism is employed to decompose his-\ntorical traffic data into recent patterns and daily patterns. The recent pattern\nis obtained by collecting the traffic flow at a frequency of p times per day,\nand the resulting time series is denoted as\n[\nXt−p+1,Xt−p+2,··· ,Xt\n]\n. The\ndaily pattern is obtained by considering the data from the previous d days,\nand the resulting time series is denoted as\n[\nXt−ds,Xt−(d−1)s,··· ,Xt−s\n]\n, the\nparameter srepresents the number of time slices within a day, and if the time\ninterval is 5 minutes, the value of s is 288. Simultaneously, if p= 12, d= 7,\nthen the recent pattern reflected the continuous traffic conditions within the\npast hour, while the daily pattern considered the daily cyclic components in\nthe traffic data (e.g., peak hours in the morning and evening, fixed school\ncommute times near schools, and fixed peak travel schedules near transporta-\ntion hubs).\n3.4. Spatial Block\nThe GCN is utilized in the current task to extract complex topological\ninformation from road networks, Given the structural information of road\nnetwork nodes, GCN serves as a fundamental approach for extracting node\nfeatures. A two-layered GCN can be formulated as Eq. (6) as follows:\n{\nH1 = σ\n(\nAXWH\n0\n)\nH2 = σ(AH1WH\n1 ) , (6)\nwhere σ represents the activation function, typically ReLU or tanh. X ∈\nRM×D denotes the node feature matrix. WH\n0 ∈RD×D and WH\n1 ∈RD×D\ndenote the model parameter. Matrix A ∈RM×M refers to the normalized\nadjacency matrix, and H1 ∈RN×D denotes the output of the first layer. The\nultimate output is a node feature representation matrix H2, based on the\ngiven structural information of the road network nodes.\nAn adaptive adjacency matrix Aadp, similar to that in [32], is introduced.\nThis adaptive adjacency matrix does not require any prior knowledge and\nis learned through gradient descent. By using this model, the model can\nlearn the hidden relationships between nodes within the road network. Col-\numn vectors Ec and row vectors Er with learnable parameters are randomly\ninitialized to generate the adaptive adjacency matrix as follows:\nAadp = softmax(ReLU(EcEr)) , (7)\n12\nwhere ReLU is used to eliminate weakly correlated nodes and softmax func-\ntion is applied for normalization.\nWe propose the following graph convolution layer:\nˆXk = AadpXkWk\n0 + A1\nfixed XkWk\n1 + A2\nfixed XkWk\n2, (8)\nwhere ˆXk represent either ˆXk\nhour or ˆXk\nday.The adjacency matrices A1\nfixed and\nA2\nfixed are built based on the physical distances between nodes in opposite\ndirections. The correlation between nodes decreases as their physical distance\nincreases. Eq. (8) allows us to extract not only the static spatial features\nof road networks from fixed graphs, but also the dynamic spatial features\nusing an adaptive adjacency matrix. This approach enhances the accuracy\nof feature extraction for road networks.\n3.5. Temporal Transformer\nAs shown in Fig. 3, a Transformer-based temporal block is presented for\ncapturing long-term temporal dependencies, which outperforms traditional\nRNNs and their variants. Firstly, position encoding is applied to each time\nstep. Differently from previous work, we perform position encoding not only\nfor all time slices within a day but also for weekdays and weekends within a\nweek. After position encoding, the feature vectors in the k-th ST block are\nmapped to query vector Qk, key vector Kk, and value vector Vk by three\nlearned linear transformations, similar to the original Transformer.\nQk = ´XkWk\nQ\nKk = ´XkWk\nK\nVk = ´XkWk\nV\n, (9)\nwhere ´Xk represent either ´Xk\nhour or ´Xk\nday. ´Xk is obtained after position encod-\ning ˆXk. The position encoding is performed separately for the 288 time slices\nwithin a day and the seven days of the week using one-hot encoding. Wk\nQ,\nWk\nK, and Wk\nV are learnable parameters. The softmax function is applied to\nconvert the results into attention scores.\nSk = softmax\n\nQkKkT\n√fd\n\n. (10)\nAfter computing Sk, we aggregate the result withVk through dot-product\nattention and then pass it through a feed-forward neural network to produce\n13\nthe final output of the temporal block.\nXk+1 = ReLU\n(\nReLU\n(\nMkW0\n)\nW1\n)\nW2 + Mk, (11)\nwhere Mk = SkVk + ´Xk. This temporal feature extraction process differs\nfrom recurrent neural networks and their variants in that it can process\nsequence data in parallel.\n4. Experiments\nIn this section, we conduct experiments on six real-world datasets to\naddress the following questions. Q1: Does the performance of deep learn-\ning models improve before and after introducing multi-channel mechanisms?\nQ2: How does the performance of our model compare to other advanced\ntraffic flow forecasting models? We also perform ablation experiments to\ndemonstrate the importance of each block.\n4.1. Experiment Setting\nTable 1: Dataset Description.\nDataset Nodes Time Slices Edges Time Range The selected features\nPEMS03 [33] 358 26208 547 9/1/2018-11/30/2018 flow\nPEMS04 [33] 307 16992 340 1/1/2018-2/28/2018 flow\nPEMS07 [33] 883 28224 866 5/1/2017-8/31/2017 flow\nPEMS08 [33] 170 17856 295 7/1/2017-8/31/2017 flow\nMETR-LA [34] 207 34272 1515 3/1/2012-6/30/2012 speed\nPEMS-BAY [34] 325 52116 2369 1/1/2017-5/31/2017 speed\nDataset. In our study, we conduct a comprehensive performance evaluation\nof our method using real-world datasets from six different regions, as shown in\nTable 1. The datasets are counted at a 5-min interval. These datasets vary in\naspects such as the number of nodes, time slices, edges (i.e., spatial distance\nrelationships between nodes), and the duration across time. Specifically, we\nutilize traffic flow data for experiments in the PEMS03, PEMS04, PEMS07,\nand PEMS08 datasets, while traffic speed data is used in the METR-LA\nand PEMS-BAY datasets. This diverse dataset selection, along with Fig.\n4 illustrating the sensor distribution for METR-LA, provides an in-depth\nunderstanding of dataset’s geographical and technical features, ensuring a\nthorough assessment of our model’s applicability and effectiveness.\n14\nFigure 4: Visualization of sensor distribution in the METR-LA dataset.\nEvalution metrics. The evaluation metrics used in this study are the Mean\nAbsolute Error (MAE), Mean Absolute Percentage Error (MAPE), and Root\nMean Square Error (RMSE).\nImplementation details. For the sake of fairness, we divide the datasets into\ntraining, validation, and test sets using the same ratio as the baseline, namely\n6:2:2. Our objective is to predict the traffic flow for the next hour based on\none hour of historical data. The hyperparameter settings for the baseline\nmodels are selected based on the default configurations provided in the pub-\nlicly available code. This approach ensures consistency and reproducibility\nin our experimental evaluation. In the multi-channel mechanism, we em-\nploy one week’s historical data from an additional channel. All experiments\nare conducted on the windows platform (CPU: Intel(R) Core(TM) i7-9700K\nCPU @ 3.60GHz; GPU: NVIDIA GeForce RTX 2080). We set the following\nhyperparameters: 4 heads for the multi-head attention mechanism, PyTorch\nas the deep learning framework, and MAE as the loss function. The max-\nimum number of epochs is set to 200, with a learning rate of 0.0001 and a\nbatch size of 64. Other parameters are kept at their default values. During\nthe training process, we automatically save the best model on the validation\nset. If the model did not improve for 15 consecutive epochs, we consider it\nto converge and stop the training process.\nBaselines. We compare MC-STTM against the following popular approaches\nfor traffic flow forecasting task:\n• HA: this approach leverages a weighted mean of historical data slices\nto prognosticate forthcoming data points.\n15\n• Temporal models: RNN; LSTM; GRU; Transformer.\n• T-GCN [15]: a spatial-temporal hybrid model that combines GCN\nand GRU.\n• STGCN [17]: it features a complete convolutional structure, enabling\nfaster training speed with lower model complexity.\n• Graph WaveNet[32]: it introduces an adaptive adjacency matrix,\nleading to significantly improved performance.\n• STTN [28]: the model performs well in long-term prediction, but has\nrelatively high model complexity.\n4.2. Performance Comparison before and after Introduction of Multi-Channel\nMechanism\nTo demonstrate that the performance of deep learning models can be im-\nproved by introducing multi-channel mechanisms, temporal models in base-\nlines are employed to introduce multi-channel mechanisms on different datasets.\nThe models prefixed with MC denote that they incorporate multi-channel\nmechanisms. Fig. 5 depicts the comparison of MAE in four real-world flow\ndatasets, before and after incorporating the multi-channel mechanism in the\neach temporal model, the solid line represents the performance curve with\nthe multi-channel mechanism, which exhibits noticeably lower MAE values\nthan the dashed line. Furthermore, this improvement in flow datasets is par-\nticularly prominent in long-term forecasting. We also conduct comparative\nexperiments on two real speed datasets. As shown in Fig. 6, we compare\nthe results of single-step prediction before and after introducing the multi-\nchannel mechanism in the METR-LA and PEMS-BAY datasets. The results\nindicate that the multi-channel mechanism improved the model performance\nin the speed datasets. Moreover, the improvement in performance is more\nsignificant in Transformer-based models compared to RNN-based models,\nwhich is one of the reasons for selecting the Transformer network architec-\nture in this paper.\nThe results indicate that the introduction of a multi-channel mechanism\nimproves the prediction performance of deep learning models across different\ndatasets, thus answering our research question Q1: the performance of deep\nlearning models will be significantly improved by introducing a multi-channel\nmechanism.\n16\nFigure 5: MAE comparison of flow prediction before and after introducing multi-channel\nmechanism in temporal models: (a) results on PEMS03; (b) results on PEMS04; (c) results\non PEMS07; (d) results on PEMS08.\n17\nFigure 6: MAE comparison of speed prediction before and after introducing multi-channel\nmechanism in temporal models: (a) results on METR-LA; (b) results on PEMS-BAY.\n4.3. Performance Comparison between MC-STTM and Baseline Model\nTo answer Q2, we compare the traffic flow forecasting performance of\nMC-STTM with baseline models on six different datasets. Currently, most\nmainstream traffic flow forecasting models are spatial-temporal approaches.\nBuilding upon existing research, MC-STTM innovates not only in terms of\nmulti-channel representation but also integrates both static and dynamic\nspatial dependencies as well as long-term temporal dependencies. The ex-\nperiment is a multi-step prediction, with steps 1 to 12 ranging from 5 min-\nutes to 60 minutes. Table 2 and 3 show a performance comparison between\nMC-STTM and baseline models, where bold indicates the best results and\nunderlined indicates the second-best.\nThe results indicate that MC-STTM outperforms baseline models on var-\nious datasets, particularly exhibiting superior performance in long-term pre-\ndiction. This validates the superiority of the Transformer in capturing tempo-\nral features over extended periods. Traffic flow prediction needs to account for\nexternal factors such as weather, road conditions, or special events that can\nimpact traffic flow. However, due to limited information in the dataset, we\nconduct robustness experiments specifically on the PEMS07 dataset, which\nhas the highest number of road network nodes. In the test set, we introduce\nGaussian noise with a mean of 0 and a standard deviation of 0.01 as inter-\nference. The experimental results before and after adding noise are shown in\nFig. 7. The results indicate that there is little difference in traffic flow pre-\ndictions before and after adding noise. Therefore, MC-STTM demonstrates\nstrong interference resistance capabilities.\n18\nTable 2: Performance comparison of flow prediction between MC-STTM and other baseline\nmodels.\nDataset Model 15min (step 3) 30min (step 6) 60min (step 12)\nMAE MAPE RMSE MAE MAPE RMSE MAE MAPE RMSE\nPEMS03\nHA 31.89 25.82 43.28 31.89 25.82 43.28 31.89 25.82 43.28\nGRU 22.02 20.26 35.54 24.73 22.25 38.74 31.60 26.21 47.20\nTGCN 17.24 16.33 25.36 20.42 20.30 29.80 26.76 29.25 38.12\nSTGCN 17.69 14.85 30.29 20.94 17.41 34.25 26.78 21.90 42.48\nGraph WaveNet 17.1714.55 25.76 20.46 16.98 30.95 29.18 23.94 43.36\nSTTN 19.18 18.84 28.70 20.29 19.38 30.41 24.39 23.01 35.94\nMC-STTM 16.56 14.07 24.74 18.35 15.21 27.53 22.30 17.88 33.26\nPEMS04\nHA 42.28 30.44 57.69 42.28 30.44 57.69 42.28 30.44 57.69\nGRU 24.35 23.91 37.76 26.82 26.60 40.51 34.22 38.22 49.38\nTGCN 21.20 15.76 31.65 25.67 20.49 37.37 34.91 31.22 49.02\nSTGCN 21.69 15.41 32.43 26.45 18.64 39.39 35.69 24.33 54.44\nGraph WaveNet 22.44 15.78 33.79 26.64 18.98 39.78 37.95 27.48 54.74\nSTTN 20.87 15.36 31.28 22.83 16.88 34.36 26.80 20.39 39.98\nMC-STTM 19.7415.47 29.65 21.7917.26 32.57 25.6320.90 37.59\nPEMS07\nHA 48.95 21.40 64.91 48.95 21.40 64.91 48.95 21.40 64.91\nGRU 35.45 19.93 58.82 39.84 22.33 62.87 49.04 27.16 72.57\nTGCN 25.50 11.51 37.70 34.34 16.84 49.54 49.81 27.15 67.75\nSTGCN 25.70 10.81 39.07 32.16 14.15 49.48 43.13 20.44 67.41\nGraph WaveNet 25.07 10.42 37.99 29.97 12.81 44.86 43.51 20.29 60.73\nSTTN 23.52 10.27 34.83 26.26 11.50 38.65 32.24 14.65 46.14\nMC-STTM 20.75 8.65 31.75 23.29 9.82 35.47 28.18 12.31 42.07\nPEMS08\nHA 34.09 19.59 45.58 34.09 19.59 45.58 34.09 19.59 45.58\nGRU 23.83 13.78 35.08 27.30 15.50 39.51 34.87 20.69 20.04\nTGCN 17.78 10.99 25.81 22.31 14.54 31.76 31.85 22.87 43.64\nSTGCN 16.96 9.65 25.27 21.23 12.06 31.92 29.04 16.17 42.06\nGraph WaveNet 17.79 10.34 26.74 21.45 12.30 32.03 31.04 18.15 43.75\nSTTN 18.28 10.41 26.66 19.04 10.87 28.20 22.60 12.92 33.47\nMC-STTM 15.86 9.03 23.48 17.19 9.92 25.73 20.11 10.04 25.98\n4.4. Ablation Experiments\nIn order to analyze the necessity of each block in MC-STTM, ablation\nexperiments are conducted in this section. As shown in Table 4, each com-\nponent contributes to the improvement of the model’s performance. The\nperformance metrics in the table represent the average values within one\nhour, aiming to evaluate the overall effectiveness of the model in multi-step\nprediction. Therefore, when modeling traffic data, it is necessary to consider\nboth the spatial scale and the temporal scale, namely the S-Block and T-\nBlock. At the spatial scale, the existing topological structure is undoubtedly\nimportant, but it is also essential to adopt dynamic modeling methods such\nas adaptive adjacency matrices to capture potential spatial correlations. Fi-\nnally, the multi-channel mechanism is one of the main innovations in this\npaper, playing a vital role in capturing long-term dependencies.\n19\nFigure 7: Visualization of performance comparison between before and after adding Gaus-\nsian noise in the PEMS07 dataset.\nFigure 8: Visualization of correlation between nodes: (a) partial results of the adaptive\nadjacency matrix in the PEMS03 dataset; (b) flow curves of corresponding nodes in the\nPEMS03 dataset.\n20\nTable 3: Performance comparison of speed prediction between MC-STTM and other base-\nline models.\nDataset Model 15min (step 3) 30min (step 6) 60min (step 12)\nMAE RMSE MAE RMSE MAE RMSE\nMETR-LA\nHA 7.41 12.77 7.41 12.77 7.41 12.77\nGRU 5.82 11.76 8.12 14.63 11.24 18.43\nTGCN 4.09 7.91 5.37 9.93 7.41 12.57\nSTGCN 6.02 11.50 8.67 15.46 9.57 17.03\nGraph WaveNet3.64 7.86 4.78 10.00 6.74 12.62\nSTTN 3.69 8.06 4.80 9.80 6.43 12.22\nMC-STTM 3.79 7.53 4.77 9.35 6.39 11.72\nPEMS-BAY\nHA 3.10 5.79 3.10 5.79 3.10 5.79\nGRU 5.79 11.79 8.37 15.60 10.91 19.53\nTGCN 1.82 3.23 2.45 4.31 3.36 5.75\nSTGCN 2.44 4.30 2.81 5.05 3.51 6.50\nGraph WaveNet 1.54 3.06 2.13 4.30 2.98 5.74\nSTTN 1.51 2.84 1.99 3.63 2.51 4.57\nMC-STTM 1.48 2.75 1.94 3.64 2.45 4.58\nTable 4: Results of the Ablation Experiments.\nPEMS03 PEMS04 PEMS07 PEMS08MAE MAPE RMSE MAE MAPE RMSE MAE MAPE RMSE MAE MAPE RMSENoAadp 21.89 18.21 32.94 24.52 19.70 36.33 26.44 11.59 39.69 19.34 11.35 28.85No Fixed Graph 21.71 19.34 32.55 29.60 30.20 41.61 24.50 10.58 36.89 19.63 11.53 28.83No S-Block 19.72 16.78 29.11 23.82 17.75 35.44 25.07 10.67 37.97 18.34 10.49 27.25No T-Block 19.50 16.48 29.37 26.40 20.43 38.84 26.94 12.23 39.25 19.88 12.02 29.17No Multi Channel 19.40 18.29 28.81 24.84 22.26 36.01 26.12 11.39 38.96 21.16 13.23 30.37Full Model 18.63 15.38 28.11 21.97 17.59 32.88 23.55 10.02 35.99 17.38 10.03 25.98\nThe adaptive adjacency matrix, as generated by Equation 7, comprises\nvalues that are learnable parameters of the model. These values are dynami-\ncally optimized with each training iteration to discover the optimal configura-\ntion. This adaptive approach ensures that the adjacency matrix continually\nadapts and refines its parameters throughout the training process, enhanc-\ning the model’s ability to capture complex relationships and dependencies in\nthe data. In PEMS03, three known nodes are selected that are not directly\nconnected in the road network. Through the learning of an adaptive adja-\ncency matrix, we can observe from Fig. 8(a) that node ID 318028 exhibits\na strong correlation with node ID 315837, while showing a relatively weak\ncorrelation with node ID 317885. Correspondingly, Fig. 8(b) reveals the\ntraffic flow variations of these three nodes over the course of a week, which\nclearly align with the learned correlations from the adjacency matrix. The\nadaptive adjacency matrix not only captures the static spatial relationships\n21\nFigure 9: Visualization of the road network in the METR-LA dataset.\npresent in the original road network, but also learns dynamic correlations.\nStatic spatial relationships refer to the proximity of adjacent nodes in space,\nwhile dynamic correlations reflect similar traffic patterns exhibited by re-\ngions containing nodes with similar properties, such as schools, post offices\nand newspapers across different areas. Despite being geographically distant,\nthese nodes exhibit similar peak traffic patterns.\nAs shown in Fig. 9, in the METR-LA dataset, we visualize the structure\nof its road network. Fig. 9(a) The distribution of node positions and the\ncorrelation between nodes in the original road network can be seen. The\nspatial position correlations are reflected in the connected roads (nodes are\nconnected by blue lines, and thicker lines indicate stronger correlation). Fig.\n9(b) The inherent road network correlation learned in the adaptive adjacency\nmatrix in this paper is relatively weak or even negligible, as the intrinsic\nstructure is introduced. Therefore, the visualization results on the map are\nnot obvious. Fig. 9(c) The unknown road network correlation learned in the\nadaptive adjacency matrix in this paper can be observed. Despite the long\ndistance between nodes in the road network, some nodes still show strong\ncorrelation. Fig. 9(d) The few nodes with the strongest correlations after\nfiltering. As shown in Fig. 10, it presents the speed curve diagrams of several\n22\nFigure 10: Visualization of the speed curve of selected nodes in Fig. 9(d).\nselected nodes from Fig. 9(d). It can be observed that the trends between\nnodes are very similar, even though the connected nodes in Fig. 9(d) are\nfar apart from each other. This further emphasizes the effectiveness of the\nadaptive adjacency matrix selected for feature learning in this paper.\n5. Conclusion\nIn this paper, we propose a novel multi-channel spatial-temporal Trans-\nformer model for traffic flow forecasting. Our model adopts a multi-channel\nmechanism, which not only learns the static spatial features of the road net-\nwork but also learns the dynamic spatial features, while introducing Trans-\nformers can better capture long-term dependencies. Experimental results on\nmultiple real-world datasets demonstrate that MC-STTM outperforms state-\nof-the-art models, confirming the effectiveness of our approach in capturing\nspatial-temporal correlations. Specifically, the performance of deep learning\nmodels can be significantly improved by introducing a multi-channel mech-\nanism. In the future, we will further explore how to capture more spatial-\ntemporal information from raw data in cases of data missing or multi-modal\ndata. On the other hand, with the development of multimodal data, future\nresearch will inevitably combine text information, image information, online\n23\ninformation, and other sources to predict real-time traffic conditions.\nCRediT authorship contribution statement\nThe authors confirm contribution to the paper as follows: data collection\nand experiments: Baichao Long; theory and method: Jianli Xiao. All authors\nreviewed the results and approved the final version of the manuscript.\nDeclaration of competing interes\nThe author(s) declared no potential conflicts of interest with respect to\nthe research, authorship, and/or publication of this article.\nData availability\nThe data used in this research is sourced from [33] and [34], which is\npublicly available.\nAcknowledgments\nThis work is supported in part by China NSFC Program under Grant\nNO.61603257.\nReferences\n[1] B. Medina-Salgado, E. Sanchez-DelaCruz, P. Pozos-Parra, and J.\nE.Sierra, “Urban Traffic Flow Prediction Techniques: A review,” Sus-\ntainable Computing: Informatics and Systems, vol. 35, pp. 100739,\nSeptember 2022.\n[2] W. Jiang and J. Luo, “Graph Neural Network for Traffic Forecasting:\nA Survey,” Expert Systems with Applications, vol. 207, pp. 117921, 30\nNovember 2022.\n[3] X. Ma, Z. Dai, Z. He, J. Ma, Y. Wang and Y. Wang, “Learning Traf-\nfic as Images: A Deep Convolutional Neural Network for Large-Scale\nTransportation Network Speed Prediction,” Sensors, vol. 17, no. 4, pp.\n818, 10 April 2017.\n24\n[4] W. Weng, J. Fan, H. Wu, Y. Hu, H. Tian, et al, “A Decomposition Dy-\nnamic Graph Convolutional Recurrent Network for Traffic Forecasting,”\nPattern Recognition, vol. 142, pp. 109670, October 2023.\n[5] W. Fang, W. Zhuo, Y. Song, J. Yan, T. Zhou, et al, “ ∆free -LSTM:\nAn Error Distribution Free Deep Learning for Short-Term Traffic Flow\nForecasting,” Neurocomputing, vol. 526, pp. 180-190, March 2023.\n[6] W. Zhang, R. Yao, X. Du, Y. Liu, R. Wang, et al, “Traffic Flow Predic-\ntion Under Multiple Adverse Weather Based on Self-Attention Mecha-\nnism and Deep Learning Models,” Physica A: Statistical Mechanics and\nits Applications, pp. 128988, 2023.\n[7] N. Zhang, Y. Zhang and H. Lu, “Seasonal Autoregressive Integrated\nMoving Average and Support Vector Machine Models: Prediction of\nShort-Term Traffic Flow on Freeways,” Transportation Research Record,\nvol. 2215, no. 1, pp. 85-92, 2011.\n[8] X. Xiao and H. Duan, “A New Grey Model for Traffic Flow Mechanics,”\nEngineering Applications of Artificial Intelligence, vol. 88, pp. 103350,\n2020.\n[9] S. V. Kumar, “Traffic Flow Prediction Using Kalman Filtering Tech-\nnique,” Procedia Engineering, vol. 187, pp. 582-587, 2017.\n[10] J. Ahn, E. Ko and E. Y. Kim, “Highway Traffic Flow Prediction Using\nSupport Vector Regression and Bayesian Classifier,” 2016 International\nconference on big data and smart computing (BigComp), pp. 239-244,\n2016.\n[11] L. Zhang, Q. Liu, W. Yang, N. Wei and D. Dong, “An Improved\nK-nearest Neighbor Model for Short-Term Traffic Flow Prediction,”\nProcedia-Social and Behavioral Sciences, vol. 96, pp. 653-662, 2013.\n[12] E. Castillo, J. M. Men´ endez and S. S´ anchez-Cambronero, “Predicting\nTraffic Flow Using Bayesian Networks,” Transportation Research Part\nB: Methodological, vol. 42, no. 5, pp. 482-509, 2008.\n[13] H. Yin, S. Wong, J. Xu and C. K. Wong, “Urban Traffic Flow Predic-\ntion Using a Fuzzy-Neural Approach,” Transportation Research Part C:\nEmerging Technologies, vol. 10, no. 2, pp. 85-98, 2002.\n25\n[14] T. N. Kipf and M. Welling, “Semi-Supervised Classification with Graph\nConvolutional Networks,” arXiv preprint arXiv:1609.02907, 2016.\n[15] L. Zhao, Y. Song, C. Zhang, Y. Liu, P. Wang, et al, “T-GCN: A Tempo-\nral Graph Convolutional Network for Traffic Prediction,” IEEE Trans-\nactions on Intelligent Transportation Systems, vol. 21, no. 9, pp. 3848-\n3858, 2019.\n[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, K. Jones, et al, “At-\ntention Is All You Need,” Advances in Neural Information Processing\nSystems, pp. 5998-6008, 2017.\n[17] B. Yu, H. Yin and Z. Zhu,“Spatio-Temporal Graph Convolutional Net-\nworks: A Deep Learning Framework for Traffic Forecasting,” arXiv\npreprint arXiv:1709.04875, 2017.\n[18] Y. J. Lu and C. T. Li, “AGSTN: Learning Attention-Adjusted Graph\nSpatio-Temporal Networks for Short-Term Urban Sensor Value Fore-\ncasting,” in IEEE International Conference on Data Mining, 2020.\n[19] M. A. Ali, S. Venkatesan, V. Liang and H. Kruppa, “TEST-GCN: Topo-\nlogically Enhanced Spatial-Temporal Graph Convolutional Networks for\nTraffic Forecasting,” in IEEE International Conference on Data Mining,\npp. 982-987, 2021.\n[20] B. Hui, D. Yan, H. Chen and W. S. Ku, “Trajectory WaveNet: A\nTrajectory-Based Model for Traffic Forecasting,” in IEEE International\nConference on Data Mining, pp. 1114-1119, 2021.\n[21] M. Liu, G. Liu and L. Sun, “Spatial-Temporal Dependence and Similar-\nity aware Traffic Flow Forecasting,” Information Sciences, vol. 625, pp.\n81-96, 2023.\n[22] Q. Zhang, C. Yin, Y. Chen and F. Su, “IGCRRN: Improved Graph\nConvolution Res-Recurrent Network for Spatio-Temporal Dependence\nCapturing and Traffic Flow Prediction,” Engineering Applications of\nArtificial Intelligence, vol. 114, pp. 105179, 2022.\n[23] Q. Zheng and Y. Zhang, “Dynamic Spatial-Temporal Adjacent Graph\nConvolutional Network for Traffic Forecasting,” IEEE Transactions on\nBig Data, 2022.\n26\n[24] S. Yang, J. Liu and K. Zhao “Space Meets Time: Local Spacetime\nNeural Network For Traffic Flow Forecasting,” in IEEE International\nConference on Data Mining, pp. 817-826, 2021.\n[25] X. Huang, J Wang, Y Lan, C. Jiang and X. Yuan, “MD-GCN: A Multi-\nScale Temporal Dual Graph Convolution Network for Traffic Flow Pre-\ndiction,” Sensors, vol. 23, no. 2, pp. 841, 2023.\n[26] J. Chen, K. Li, K. Li, P. S. Yu and Z. Zeng, “Dynamic Planning of\nBicycle Stations in Dockless Public Bicycle-Sharing System Using Gated\nGraph Neural Network,” ACM Transactions on Intelligent Systems and\nTechnology (TIST), vol. 12, no. 2, pp. 1-22, 2021.\n[27] B. Pu, J. Liu, Y. Kang, J. Chen and S. Y. Philip, “MVSTT: A Multiview\nSpatial-Temporal Transformer Network for Traffic-Flow Forecasting,”\nIEEE transactions on cybernetics, 2022.\n[28] M. Xu, W. Dai, C. Liu, X. Gao and G. J. Qi, “Spatial-Temporal\nTransformer Networks for Traffic Flow Forecasting,” arXiv preprint\narXiv:2001.02908, 2020.\n[29] W. Fang, W. Zhuo, J. Yan, Y. Song and D. Jiang, et al, “Attention Meets\nLong Short-Term Memory: A Deep Learning Network for Traffic Flow\nForecasting,” Physica A: Statistical Mechanics and its Applications, vol.\n587, pp. 126485, 2022.\n[30] M. M´ endez, M. G. Merayo, and M. N´ u˜ nez, “Long-Term Traffic Flow\nForecasting Using a Hybrid CNN-BiLSTM Model,” Engineering Appli-\ncations of Artificial Intelligence, vol. 121, pp. 106041, 2023.\n[31] Y. Djenouri, A. Belhadi, G. Srivastava and J. C. W. Lin, “Hybrid Graph\nConvolution Neural Network and Branch-and-Bound Optimization for\nTraffic Flow Forecasting,” Future Generation Computer Systems, vol.\n139, pp. 100-108, 2023.\n[32] Z. Wu, S. Pan, G Long, J. Jiang and C. Zhang, “Graph WaveNet for\nDeep Spatial-Temporal Graph Modeling,” in Proceedings of the 28th\nInternational Joint Conference on Artificial Intelligence, 10 August 2019.\n[33] S. Guo, Y. Lin, N. Feng, C. Song and H. Wan, “Attention Based Spatial-\nTemporal Graph Convolutional Networks for Traffic Flow Forecasting,”\n27\nin Proceedings of the AAAI conference on artificial intelligence, vol. 33,\nno. 01, pp. 922-929, 2019.\n[34] C. Song, Y. Lin, S. Guo and H. Wan, “Spatial-Temporal Syn-\nchronous Graph Convolutional Networks: A New Framework for Spatial-\nTemporal Network data forecasting,” in Proceedings of the AAAI con-\nference on artificial intelligence, vol. 34, no. 01, pp. 914-921, 2020.\n28",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6340823173522949
    },
    {
      "name": "Transformer",
      "score": 0.5170508027076721
    },
    {
      "name": "Electrical engineering",
      "score": 0.11808395385742188
    },
    {
      "name": "Engineering",
      "score": 0.11410155892372131
    },
    {
      "name": "Voltage",
      "score": 0.07834547758102417
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I148128674",
      "name": "University of Shanghai for Science and Technology",
      "country": "CN"
    }
  ]
}