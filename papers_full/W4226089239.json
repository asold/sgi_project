{
  "title": "Pretraining with Artificial Language: Studying Transferable Knowledge in Language Models",
  "url": "https://openalex.org/W4226089239",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3019104427",
      "name": "Ryokan Ri",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A1788978749",
      "name": "Yoshimasa Tsuruoka",
      "affiliations": [
        "The University of Tokyo"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W2079656678",
    "https://openalex.org/W3173660000",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W2251771443",
    "https://openalex.org/W3104570641",
    "https://openalex.org/W3156194904",
    "https://openalex.org/W3001434439",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W1973589994",
    "https://openalex.org/W3035422697",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3152698349",
    "https://openalex.org/W2963351454",
    "https://openalex.org/W2962911926",
    "https://openalex.org/W3105478763",
    "https://openalex.org/W2518186251",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W4319915529",
    "https://openalex.org/W2826721128",
    "https://openalex.org/W4231510805",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2963951265",
    "https://openalex.org/W2159637323",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3035137491",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4226405181",
    "https://openalex.org/W3035547806",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W2963846239",
    "https://openalex.org/W2995230342",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W3198802556",
    "https://openalex.org/W3018647120",
    "https://openalex.org/W3175606037",
    "https://openalex.org/W2142708806",
    "https://openalex.org/W3134307371",
    "https://openalex.org/W4236950558",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W2552110825",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1899794420"
  ],
  "abstract": "We investigate what kind of structural knowledge learned in neural network encoders is transferable to processing natural language.We design artificial languages with structural properties that mimic natural language, pretrain encoders on the data, and see how much performance the encoder exhibits on downstream tasks in natural language.Our experimental results show that pretraining with an artificial language with a nesting dependency structure provides some knowledge transferable to natural language.A follow-up probing analysis indicates that its success in the transfer is related to the amount of encoded contextual information and what is transferred is the knowledge of position-aware context dependence of language.Our results provide insights into how neural network encoders process human languages and the source of cross-lingual transferability of recent multilingual language models.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 7302 - 7315\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nPretraining with Artificial Language:\nStudying Transferable Knowledge in Language Models\nRyokan Ri and Yoshimasa Tsuruoka\nThe University of Tokyo\n7-3-1 Hongo, Bunkyo-ku, Tokyo, Japan\n{li0123,tsuruoka}@logos.t.u-tokyo.ac.jp\nAbstract\nWe investigate what kind of structural knowl-\nedge learned in neural network encoders is\ntransferable to processing natural language. We\ndesign artificial languages with structural prop-\nerties that mimic natural language, pretrain\nencoders on the data, and see how much per-\nformance the encoder exhibits on downstream\ntasks in natural language. Our experimental\nresults show that pretraining with an artificial\nlanguage with a nesting dependency structure\nprovides some knowledge transferable to natu-\nral language. A follow-up probing analysis in-\ndicates that its success in the transfer is related\nto the amount of encoded contextual informa-\ntion and what is transferred is the knowledge\nof position-aware context dependence of lan-\nguage. Our results provide insights into how\nneural network encoders process human lan-\nguages and the source of cross-lingual transfer-\nability of recent multilingual language models.\n1 Introduction\nPretrained language models (Devlin et al., 2019;\nYang et al., 2019; Raffel et al., 2020) have demon-\nstrated strong empirical performance not only\nwithin a language but also across languages. Lan-\nguage models pretrained with a mix of monolingual\ncorpora, such as multilingual BERT, exhibit a de-\ncent zero-shot cross-lingual transfer capability, i.e.,\na model fine-tuned in a single source language (L1)\ncan solve the task in another language (L2) (Con-\nneau et al., 2020a; Xue et al., 2021). Surprisingly,\nthe transfer happens without lexical overlaps be-\ntween L1 and L2 (Karthikeyan K and Roth, 2020;\nConneau et al., 2020b) or even without joint pre-\ntraining (Artetxe et al., 2020): an encoder only\npretrained on L1 can be transferred to L2 without\nany parameter updates. These results suggest that,\nwhether the encoder is trained on single or multiple\nlanguages, it learns some transferable knowledge\nabout language.\nFigure 1: Transfer from artificial language to natural\nlanguage. The artificial language encodes some struc-\ntural properties (e.g., token distributions, dependency\nstructures) and we study how the learning of such prop-\nerties can be transferred to natural language.\nHowever, the characteristics of such transferable\nknowledge are still underexplored. Recent stud-\nies with the probing methodology (Hupkes and\nZuidema, 2018; Conneau et al., 2018) have re-\nvealed that multilingual BERT captures language-\nindependent linguistic structures such as universal\ndependency relations (Chi et al., 2020) and subject-\nhood (Papadimitriou et al., 2021), but it remains\nunknown whether learning such linguistic proper-\nties actually contributes to the performance, and\nwhether there exists more abstract knowledge trans-\nferred across languages.\nIn this study, we try to shed light on these ques-\ntions with the framework of the Test for Inductive\nBias via Language Model Transfer (Papadimitriou\nand Jurafsky, 2020), focusing on designing arti-\nficial languages with natural-language-like struc-\ntural properties (Figure 1). We pretrain encoders\n7302\nwith artificial languages and transfer the encoders\nto natural language tasks with their parameters\nfrozen. This enables us to see how learning the\nspecific structural properties of the artificial lan-\nguage affects the downstream performance.\nSpecifically, we explore whether it is beneficial\nfor the encoder to know the following two char-\nacteristics of natural language: word distributions\nand latent dependency structures. We design arti-\nficial languages that represent such characteristics\nand perform an extensive study with different en-\ncoder architectures (LSTM and Transformer) pre-\ntraining objectives (causal and masked language\nmodelings).\nThe contribution is summarized as follows:\n• We first start by complementing the study\nin Papadimitriou and Jurafsky (2020). We\ntrain LSTM and Transformer encoders with\nthe sentence-level causal language modeling\ntask and evaluate the encoders in English. We\nshow that an artificial language that models\nsimple statistical dependency within a sen-\ntence provides decent transferable knowledge\non natural language modeling. Furthermore,\nwe find that the inductive bias of a nesting\nhead-to-tail dependency structure is more use-\nful than a flat one.\n• We then proceed to investigate transfer learn-\ning in masked language modeling (Devlin\net al., 2019), one of the current dominant pre-\ntraining paradigms. We evaluate pretrained\nTransformer encoders with dependency pars-\ning and confirm that the nesting dependency\nstructure is important to learn the structure of\nnatural language.\n• We hypothesize that the transfer performance\nof pretrained encoders is related to the way\nthe encoder preserves the input contextual in-\nformation in the output vectors. We perform a\nprobing experiment and find that the artificial\nlanguage with the nesting dependency struc-\nture trains encoders to encode the information\non adjacent tokens into the output vector of\neach token. We conclude this paper with the\nhypothesis that a part of transferable knowl-\nedge in language models could be explained\nby the knowledge of position-aware context\ndependence of language.\n2 Related Work\n2.1 Transferable Structural Knowledge in\nPretrained Encoders\nMultilingual language models trained with masked\nlanguage modeling objective (Devlin et al., 2019;\nDoddapaneni et al., 2021) have demonstrated a\nsurprisingly strong cross-lingual transfer capability\n(Liu et al., 2020), given the model is only trained\nwith a mix of monolingual corpora. This leads\nto several studies investigating the source of the\ncross-lingual capability of multilingual models.\nAn early common hypothesis was that the mod-\nels take advantage of a common word-piece vo-\ncabulary across languages (Wu and Dredze, 2019;\nPires et al., 2019), which provides cross-lingual\nalignment signals to learn useful multilingual rep-\nresentations. However, this hypothesis has been\nquestioned by recent studies (Karthikeyan K and\nRoth, 2020; Conneau et al., 2020b) which show\nthat shared word-pieces only play a minor role in\nthe performance. These studies suggest that the\nmodel can exploit abstract structures of languages\nto learn shared multilingual representations.\nAnother line of research suggests that the learn-\ning of transferable knowledge happens even in\nmonolingual pretraining. Artetxe et al. (2020)\nshowed that a Transformer encoder pretrained only\non L1 exhibits strong cross-lingual transfer perfor-\nmance simply by aligning the L2 embeddings to\nthe encoder. Papadimitriou and Jurafsky (2020)\npretrained LSTM encoders with natural languages\nand non-linguistic data (e.g., code, music, and arti-\nficial data) to demonstrate that the encoders achieve\nreasonable performance in Spanish language mod-\neling. These studies provide additional evidence\nfor the existence of transferable linguistic knowl-\nedge learned in the model.\nThen what is such knowledge? Probing studies\n(Hupkes and Zuidema, 2018; Conneau et al., 2018)\nhave revealed that the model captures language-\nindependent structures such as universal depen-\ndency relations (Chi et al., 2020) and subjecthood\n(Papadimitriou et al., 2021). However, the probing\nmethodology does not answer whether such lin-\nguistic knowledge contributes to the performance\nin cross-lingual transfer.\nIn this study, we shed light on this question\nby studying transfer learning from artificial lan-\nguage with the Test for Inductive Bias via Lan-\nguage Model Transfer (TILT) (Papadimitriou and\nJurafsky, 2020). This framework enables us to\n7303\nassess if abstract features generalizable to L2 (nat-\nural language) are encoded in L1. Here we explic-\nitly design artificial languages with some structural\nproperties as L1 to investigate their transferability.\n2.2 Studying Language Models with Artificial\nLanguage\nTo study the behavior of language models, sev-\neral studies have employed a specific type of ar-\ntificial language: artificial variants of natural lan-\nguages. A typical experimental framework is as\nfollows: (1) create an artificial language that dif-\nfers from a natural language in one linguistic prop-\nerty, such as word orders (Sinha et al., 2021b;\nDufter and Schütze, 2020; Sinha et al., 2021a),\nscripts (Karthikeyan K and Roth, 2020; Dufter and\nSchütze, 2020; Conneau et al., 2020b), or morphol-\nogy (Ravfogel et al., 2019); (2) train or evaluate\nthe natural/artificial language models and compare\nthe performance to analyze the model’s sensitivity\nto the linguistic property.\nHowever, this methodology is limited to study-\ning linguistic properties that are easily editable\nto create artificial variants and also offers limited\ncontrol over the experiments. To overcome this\nproblem, White and Cotterell (2021) created artifi-\ncial languages by defining their own probabilistic\ncontext-free grammars (PCFG). As the concurrent\nwork, Chiang and yi Lee (2022) trained Trans-\nformer encoders on artificial data with token de-\npendencies in the sequences and showed that they\nperform reasonably well on the GLUE benchmark\n(Wang et al., 2019). In this research, we design\nartificial languages with certain structural proper-\nties from scratch to study knowledge transferable\nto natural language.\n3 Approach\n3.1 Experimental Framework\nWe first describe the experimental framework used\nthroughout this paper, the Test for Inductive Bias\nvia Language Model Transfer (TILT) introduced by\nPapadimitriou and Jurafsky (2020). TILT consists\nof pretraining and transfer steps:\n1. Pretrain an encoder with a pretraining task in\nthe source language (L1). We explore pretrain-\ning with causal language modeling in §4 and\nmasked language modeling in §5.\n2. Transfer the encoder to the target language\n(L2) in a downstream task. As we are inter-\nested in structural prior knowledge learned\nin the encoder, we discard the learned L1\nword embeddings and initialize the embed-\nding layer with the L2 vocabulary. We then\ntrain the model with the encoder parameters\nfrozen and evaluate the task performance.\nTILT reveals how transferrable the computation\ninduced to solve the L1 pretraining task is to pro-\ncessing L2. In this study, we are interested in the\ntransferability of certain types of structures to nat-\nural language, and thus we primarily use hand-\ndesigned artificial languages with the structural\nproperties as L1 and natural language as L2.\n3.2 Designing Artificial Languages\nArtificial languages are designed to mimic a certain\nproperty of natural language. After providing a for-\nmal definition of artificial language, we introduce\nseveral languages used in this paper.\n3.2.1 Formulation of Artificial Language\nA artificial language refers to a set of a vocabu-\nlary and algorithms to generate sequential data for\npretraining. Each language has a sentence-length\ndistribution plen(l), token vocabulary {w|w ∈ V},\nand sentence-sampling function f(l) : l 7→ Vl.\nThe training data is generated sentence by sen-\ntence as follows: we first sample a sentence length\n(l ∼ plen(l)) and then sample a sequence of tokens\nof that length ([w1, ..., wl] ∼ f(l)).\nIn this study, the token vocabularyV simply con-\nsists of integers (or integers with a special symbol)\nand is not intended to correspond to a vocabulary\nof any natural language. Also the sentence-length\ndistribution plen(l) is fitted with a baseline dataset\nin each experiment. The focus is how to design the\nsentence-sampling function f(l). This determines\nwhat kind of characteristics we want to encode in\nthe artificial dataset.\n3.2.2 Modeling Word Distribution\nWords in natural language are distributed in non-\ntrivial fashions. We will study whether prior knowl-\nedge of token distribution facilitates learning from\nnatural language. We first present the simplest arti-\nficial language that serves as a baseline.\nUniform language samples each token in a sen-\ntence independently and uniformly. Specifically,\nthe probability of a token w being sampled is\np(w) = 1\n|V| . (1)\n7304\nHowever, this deviates from the token distri-\nbution of natural language. Natural language is\nempirically known to follow the Zipf’s law (Zipf,\n1949), i.e., the relation between the frequency of\na word and its rank is given by frequency(w) ∝\nrank(w)−α. The coefficient α is typically around\n1, although the coefficient shows some variation\naccording to the corpus domain (Zanette and Mon-\ntemurro, 2005).\nZipf language captures this property and samples\neach token w from the following probability distri-\nbution assuming α = 1:\np(w) ∝ 1\nrank(w). (2)\nThe two languages introduced so far generate to-\nkens in a sentence independently. However, words\nwithin a sentence of natural language are known to\nhave statistical dependencies, i.e., specific cooccur-\nrence patterns (Church and Hanks, 1989). Consider\nthe sentence “ The cat and dog are fighting over\nfood.” The words the and cat would cooccur much\nmore often than by chance because cat (noun) is\ndependent on the (determinant); so would dog and\ncat because they are topically related. The words in\na sentence are usually coherent according to some\nsyntactic and semantic dependencies.\nLog-linear language is designed to capture this\nproperty. Inspired by the log-linear model in Arora\net al. (2016), tokens in a sentence s are drawn from\nthe following probability distribution:\np(w|s) ∝ exp(⃗ cs · ⃗ vw), (3)\nwhere ⃗ cs is the discourse vector of the sentence and\n⃗ vw is the word vector of the tokenw. Intuitively, we\ncan imagine that the discourse vector represents the\ntopic of the sentence and determines the unigram\ndistribution over the vocabulary (Blei et al., 2003).\nSampling tokens this way, non-trivial cooccurrence\npatterns within sentences emerge in the language.\nWe speculate that pretraining with the Log-linear\nlanguage will endow the model with an inductive\nbias to aggregate the context in a sentence to predict\nthe identity or property of tokens, which is likely\nto benefit natural language processing.\nIn the experiments, the word vectors ⃗ vw are ini-\ntialized with the normal distribution, and the dis-\ncourse vector ⃗ cs is also drawn from the normal\ndistribution each time we generate a sentence. We\nset the dimension of the word and discourse vec-\ntor to 10 as we empirically find that this makes\nthe entire token distribution close to the Zipfian\ndistribution.\n3.2.3 Modeling Latent Dependency Structure\nSentences in natural language are known to have la-\ntent structures, which are often described in the\nform of trees (Chomsky, 1957) or dependency\ngraphs (Mel’ ˇcuk, 1988). Now we consider how\nto endow the sampled tokens with such structures.\nIn this study, we adopt a dependency-based la-\ntent structure. Words in sentences of natural lan-\nguage often have dependency relations and the exis-\ntence of a certain word can be predictive of another\nword (e.g., the verbam always cooccurs withI). We\nhypothesize that, pretrained on such data, language\nmodels may acquire inductive bias towards finding\nrelations between tokens in the input, which is pre-\nsumably important in processing natural language.\nInspired by Papadimitriou and Jurafsky (2020),\nwe design algorithms that generate structured sen-\ntences given a set of tokens sampled with any of\nthe strategies described in §3.2.2. The general idea\nis that half of the tokens (heads) in the vocabulary\nare all paired with another half of tokens (tails). A\npair of head and tail can be represented in right and\nleft brackets with the same integer (e.g., “<123”,\n“123>”). The pairs always appear together in a\nsentence and express simple dependency relations.\nAfter determining the sentence length l ∼ f(l),\nwe first sample l\n2 (rounded to an integer) pairs of\nhead and tail and then arrange them with one of the\nfollowing structures.\nFlat Dependency structure simply arranges the\ntokens randomly while keeping the right order of\nthe brackets (e.g., [“<5”, “<84”, “5>”, “<123”,\n“123>”, “ 84>”]). The dependency arcs are al-\nlowed to be crossed and thus often result in a non-\nprojective dependency structure.\nNesting Dependency language, by contrast, does\nnot allow any dependency arcs to be crossed, and\nthe brackets are nested hierarchically (e.g., [“<5”,\n“<84”, “84>”, “5>”, “<123”, “123>”]). The sen-\ntences are generated from the stack-based algo-\nrithm described in Appendix A.\nThese structures are similar to the Parenthe-\nsis languages used to study the inductive bias of\nlanguage models in Papadimitriou and Jurafsky\n(2020). However, our Dependency languages differ\nfrom them in how to represent the head and tail\ntokens. In the Parenthesis language, the head and\n7305\ntail are represented with the same token (e.g., [“5”,\n“84”, “84”, “5”, “123”, “123”]), which we argue\ndeviates from the dependency structure in natural\nlanguage, because in natural language, dependency\nrelations usually hold between different words (e.g.,\nI and am). We will show that this difference is in\nfact crucial and draw a different conclusion from\nPapadimitriou and Jurafsky (2020) on the impor-\ntance of the nested structure (§4.2).\n4 Causal Language Model Pretraining\nwith Artificial Language\nIn this section, we complement the study of Pa-\npadimitriou and Jurafsky (2020). While they stud-\nied the inductive bias learned in LSTM encoders\nwith some artificial languages, here we provide\nadditional studies with the newly introduced Log-\nlinear and Dependency artificial languages, and the\nTransformer encoder.\n4.1 Experimental Setups\nTask. We study sentence-level causal (left-to-right)\nlanguage modeling (CLM), where the model needs\nto predict the next word given the previous con-\ntext in the sentence. Note that, Papadimitriou and\nJurafsky (2020) experiment with language model-\ning across sentences, but we adopt sentence-level\nmodeling because we would like to focus on the\nlearning of sentence structures here. As we will see\nin §4.2, we observe the same tendency in regard to\nthe effect of artificial pretraining where we share\nthe setups. The task performance is measured by\nthe average perplexity scores for each token.\nModel. We study two encoder architectures:\nLSTM (Hochreiter and Schmidhuber, 1997) and\nTransformer (Vaswani et al., 2017). These archi-\ntectures are known to exhibit different abilities in\ncapturing the underlying hierarchical structure of\nsequential data (Tran et al., 2018).\nThe size of word embeddings is set to 300. For\nboth LSTM and Transformer encoders, the number\nof layers is set to 3, and the number of parameters\nis configured to be the same (6.9M parameters) to\nenable a fair comparison between architectures (for\nfurther details, see Appendix B).\nPretraining Data. We generate artificial corpora\nwith three unstructured languages, which randomly\narrange the tokens sampled from Uniform, Zipf,\nand Log-linear languages, and four structured lan-\nguages which combine the Zipf sampling strategy\nwith the structures of Flat Parenthesis, Nesting\nParenthesis, Flat Dependency, and Nesting Depen-\ndency.\nWe also experiment with natural language cor-\npora. We create training corpora from Wikipedia\ndumps of English, Japanese, and Spanish. The sen-\ntences are tokenized with the Moses tokenizer1\nfor English and Spanish and MeCab2 for Japanese.\nThe sentence lengths of artificial data were sam-\npled from the empirical distribution of the English\nWikipedia corpus. The size of the vocabulary|V | is\nset to 32,000 for both artificial and natural corpora,\nand out-of-vocabulary words in natural language\nare replaced with the OOV token. For each corpus,\nwe sample 12.8 M sentences and train the model\nwith one iteration over the corpus.\nEvaluation Data. We evaluate the pretrained en-\ncoders on the Penn Treebank (PTB) corpus (Mar-\ncus et al., 1993) with preprocessing from Mikolov\net al. (2010). Note that, when we train language\nmodels with the pretrained encoders, the parame-\nters of the encoder are not updated and only the\nEnglish word embeddings are learned from scratch\n(optimization details in Appendix B.2).\n4.2 Results\nWe provide two baseline models trained on the\nL2 training corpus from scratch and trained with\nfrozen random weights in the encoder to compare\nwith pretrained encoders. For each configuration,\nwe pretrain three encoders with different random\nseeds, and for each encoder fine-tuned three mod-\nels, which results in nine models in total. We sum-\nmarize the average scores and standard deviations\nin Figure 2.\nThe Transformer encoder is more flexible\nthan LSTM. We start by discussing overall trends.\nWe observe that the Transformer encoders give\nlower perplexity scores compared to LSTM regard-\nless of pretraining language. This tendency is in\nline with the observations on the surprisingly good\ntransferability or pretrained Transformer encoders\nto other languages (Conneau et al., 2020a), or even\nother modalities (Lu et al., 2021; Reid et al., 2022).\nWe think that this is because Transformer encoders\nare better at aggregating and preserving the context\ninformation at each time step, as we will see in §6,\npresumably because the Transformer architecture\nhas self-attention and residual connections.\n1https://github.com/moses-smt/\nmosesdecoder\n2http://taku910.github.io/mecab/\n7306\n(a) Comparison of token distributions.\n(b) Comparison of dependency structures.\n (c) Comparison of natural languages.\nFigure 2: The perplexity scores (the lower the better) on the sentence-level causal language modeling task with the\nEnglish Penn Treebank dataset. The two baselines (From scratch and Random weights) are not pretrained, and the\nothers are the results of pretrained encoders.\nNatural languages are better than the artifi-\ncial languages. As expected, pretraining with natu-\nral languages (English, Spanish and Japanese) pro-\nvides better encoders for language modeling than\nthe artificial languages both with LSTM and Trans-\nformer. However, the performance differences be-\ntween natural languages seem to be negligible, in-\ndicating that there is not much difference in the\nway the encoders process these different languages,\nconforming with the observation of cross-lingual\ntransferability of pretrained encoders (Artetxe et al.,\n2020).\nThe Uniform and Zipf languages degrade the\nencoders. Looking at the difference among un-\nstructured languages (Figure 2a), Uniform and Zipf\nlanguages give higher perplexities than the Ran-\ndom weights baseline particularly with LSTM. In\nhindsight, it is natural that encoders would be de-\ngraded even from random weights when trained\nwith sequences where tokens are drawn indepen-\ndently from each other because the encoders are\nnot incentivized to use contextual information and\nwill even learn to discard the input information.\nWe will demonstrate this with a follow-up probing\nexperiment in §6.\nThe Log-linear language provides a useful\ninductive bias to language modeling. On the\ncontrary, the Log-linear language gives reasonably\nlower perplexities compared to Random weights\n(Figure 2a). This indicates that knowing the exis-\ntence of statistical dependency within a sentence,\nor learning to predict tokens from the cooccurrence\ninformation, is a useful inductive bias even though\nthe cooccurrence statistics is not necessarily in line\nwith L2.\nWe do not observe the importance of the\nnested structure in the Parenthesis languages.\nPapadimitriou and Jurafsky (2020) showed that\nLSTM encoders trained on the Flat Parenthesis and\nNesting Parenthesis structures do not provide a sig-\nnificant difference in perplexity, and concluded that\nsimple non-hierarchical head-dependent-type rela-\ntions are important in LSTM language processing.\nA similar observation can be made in Figure 2b:\nalthough the Nesting Parenthesis exhibits the lower\naverage score, there is no significant difference\nbetween Flat Parenthesis and Nesting Parenthesis\n(232.9±30.0 vs. 203.8±7.7, p >0.01 in Welch’s\nt-test) with the unstable results of Flat Parenthesis.\nAlso, the trend of the average scores is reversed in\nTransformer: the Nesting Parenthesis exhibits the\nhigher average score (212.4 ± 8.8) than Flat Paren-\nthesis (191.9 ± 11.8), which makes it difficult to\ndraw a consistent conclusion from here.\n7307\nHowever, the Dependency languages suggest\nthat the nested structure is actually important\nin language modeling. While the Parenthesis lan-\nguage represents dependency relations with two\nidentical tokens (e.g., “4543” and “4543”), our\nDependency language represents relations with two\ndifferent tokens ( e.g., “ <4543” and “ 4543>”).\nWe expect that expressing dependency relations\nwith two different tokens is closer to natural lan-\nguage and thus provides more viable insights into\nnatural language. When we compare the scores of\nthe Dependency languages, Nesting Dependency\nprovides the lower and more stable perplexity than\nFlat Dependency with LSTM ( 175.7 ± 4.3 vs.\n187.2±10.7) and the significantly lower score with\nTransformer (160.6±1.6 vs. 175.7±4.3, p >0.01\nin Welch’s t-test). Overall, Nesting Dependency\nperforms best among other artificial languages, in-\ndicating our Dependency language is closer to nat-\nural language and the nested structure is useful for\nlanguage modeling.\n5 Masked Language Model Pretraining\nwith Artificial Language\nWe proceed to investigate transfer learning from\nartificial languages in one of the most successful\npretraining paradigms, masked language modeling\n(MLM) (Devlin et al., 2019) to see if we can ob-\nserve similar trends to what we see in the CLM\nexperiment (§4).\n5.1 Experimental Setups\nPretraining. To allow for fast experimentation, we\ntrain small Transformer encoders. The size of word\nembeddings is set to 300 and the encoders have\nthree layers (further details in Appendix C). The\npretraining datasets are the same as in §4.1.\nDownstream Task. We evaluate the pretrained en-\ncoders with dependency parsing to see if the struc-\ntural knowledge learned with artificial language is\nbeneficial to predict the structure of natural lan-\nguage. We use the English EWT dataset from\nUniversal Dependencies (UD) v2.8 (Nivre et al.,\n2020)3.\nModel. We adopt the biaffine graph-based parser\n(Dozat and Manning, 2017) with the Transformer\nencoder. The input word representations are the\nconcatenation of word embeddings and charac-\nter features computed by a character-level bi-\ndirectional LSTM encoder (Ling et al., 2015). For\n3https://universaldependencies.org/\nFigure 3: The downstream performance on two syntactic\ntasks with the English EWT dataset. The two baselines\n(From scratch and Random weights) are not pretrained,\nand the others are the results of encoders pretrained with\nmasked language modeling.\nthe details on fine-tuning these models, please refer\nto Appendix C.\n5.2 Results\nWe provide two baseline models trained from\nscratch and trained with random encoder weights.\nFor each pretraining language, we again train three\nencoders and fine-tune three models for each, and\ntake the mean and standard deviation of the nine\nmodels. Figure 3 shows the results.\nThe unstructured languages do not provide\nuseful transferable knowledge for dependency\nparsing. The Uniform, Zipf, and Log-linear en-\ncoders perform comparably to or worse than the\nRandom weights baseline. This is in contrast with\nthe causal language modeling task, where the Log-\nlinear language at least outperforms the Random\nweights baseline (§4.2).\nOn the other hand, learning from structured\nlanguages seems to be important in dependency\nparsing. The Dependency encoders outperform the\nRandom weights baseline, and also we can observe\nthat learning from the nesting structure is more\neffective than the flat structure, and Dependency\nlanguages outperform Parenthesis languages, as\nobserved in the CLM in §4.\n6 How much contextual information do\nthe pretrained encoders capture?\nIn the previous sections, we have seen that the en-\ncoders pretrained with different artificial languages\nexhibit various degrees of transferability to natural\n7308\nlanguage. In this section, we try to explain why\npretraining with some artificial languages is bet-\nter or worse for the transfer to natural language\nfrom the perspective of the amount of contextual\ninformation in the encoder outputs.\nThe intuition is, for example, if a pretrained en-\ncoder has learned to discard the input information,\nwe cannot expect the encoder to perform well when\ntransferred to any tasks. Also, existing studies show\nthat neural language models assign more impor-\ntance to local context when they make predictions\n(Khandelwal et al., 2018; Lai et al., 2020). Can\nwe observe that encoders pretrained with artificial\nlanguages exhibit similar patterns to natural lan-\nguages regarding how they encode the contextual\ninformation?\n6.1 Experimental Setups\nWe investigate how much contextual information\ncan be extracted from the outputs of the pretrained\nencoders by setting up a simple probing task. In\nthis task, the encoder is asked to recover the identity\nof the contextual words given the contextualized\nvector of a target word.\nSpecifically, we first randomly generate 100K\nsequences of integers with the length of 15 ∼ 25\n(close to most frequent sequence lengths in the\npretrained corpus) with the vocabulary size100 and\nsplit them into training (90K sequences), validation\n(5K) and test (5K) sets.\nThen we simultaneously train several linear clas-\nsifiers, each of which predicts the ID of the context\nword at a fixed relative position to the target word\nin the sequence, on top of a frozen pretrained en-\ncoder. For the encoders pretrained with CLM in §4,\nthe target word is the last word in sequences and\nthe classifiers predict the words at the positions of\n[-9, -4, -3, -2, -1, 0]; for the encoders pretrained\nwith MLM in §5, the target word is the middle\nword and the classifiers predict the words at [-6, -3,\n-2, -1, 0, 1, 2, 3, 6].\nAfter training, we measure the accuracy of pre-\ndicting the words at each position on the test set\nand interpret this as how much information on each\ncontextual word the encoder preserves.\n6.2 Results\nFigure 4 summarizes the results of the encoders\ntrained in §4 and §5.\nThe amount of the encoded contextual infor-\nmation can explain the transfer performance in\nsome obvious cases. In the experiment of CLM\n(Figure 2a), we observed that the Uniform and Zipf\nencoders tend to perform worse even than Ran-\ndom weights. Figure 4a and 4d demonstrate that\ntheir poor performance is because the encoders\nare trained to discard the input information. The\nUniform and Zipf encoders tend to preserve less\ncontextual information even than Random weights\nbecause capturing the contextual information does\nnot lead to solving the pretraining task in these\nlanguages.\nOn the other hand, if words are predictable from\nthe context, encoders are encouraged to learn to\npreserve the contextual information. The Log-\nlinear encoders trained with CLM encode a de-\ncent amount of the contextual information (Fig-\nure 4a and 4d) and also performed best among the\nunstructured artificial languages in CLM (Figure\n2a). Moreover, encoders trained with natural lan-\nguages (Figure 4c, 4f and 4i) capture not only the\nlocal context well (at distance 0 ∼ 2) but also a\nmodest amount of the farther context (at distance\n3 ∼), which is consistent with the existing obser-\nvation that LSTM encoders trained with natural\nlanguage are better at memorizing the inputs than\nones trained with randomly sampled data (Liu et al.,\n2018). In these cases, the downstream performance\nand the amount of the encoded contextual informa-\ntion seem to be correlated.\nHowever, this trend is not as clear when compar-\ning the structured artificial languages. For exam-\nple, the Nesting Dependency encoders perform the\nbest for the downstream tasks among the structured\nartificial languages but do not necessarily in the\nprobing task (Figure 4b and 4e).\nThe nesting structure seems to facilitate en-\ncoders to remember the local context with MLM.\nThe difference between the Nesting and Flat lan-\nguages is striking in Figure 4f. The Nesting en-\ncoders are consistently better at capturing the lo-\ncal contextual information (at positions −2 ∼ 2)\nthan their flat counterparts, which may explain the\nbetter performance of the Nesting encoders in de-\npendency parsing (Figure 3), given that the local\ncontextual information is particularly important to\npredict the syntactic characteristics of words (Levy\nand Goldberg, 2014; Ri and Tsuruoka, 2020).\n7 Discussion and Future Work\nIn this paper, we studied what kind of structural\nproperties in pretraining data is useful to train en-\ncoders for natural language tasks. We have found\n7309\n(a) LSTM-CLM.\n (b) LSTM-CLM.\n (c) LSTM-CLM.\n(d) Transformers-CLM.\n (e) Transformers-CLM.\n (f) Transformers-CLM.\n(g) Transformers-MLM.\n (h) Transformers-MLM.\n (i) Transformers-MLM.\nFigure 4: The accuracy of the task of recovering the contextual words from the encoder output of target words.\nthat to achieve decent results, L1 needs at least sta-\ntistical dependency in a sentence (§4), and having\nthe head-to-tail dependency with the nesting struc-\nture is further beneficial (§4 and §5). The probing\nexperiment in §6 suggests that the encoders trained\nwith languages with the above characteristics are\ngood at capturing the positions and identities of the\ncontext words.\nFrom these observations, we suggest a tentative\nanswer to the initial research question: what knowl-\nedge in pretrained encoders are transferred across\ndifferent languages? That is position-aware context\ndependence of language, in other words, “tokens\nin a sequence can be characterized by its neigh-\nbor tokens at specific positions”.\nWe think that it can explain the success of trans-\nferring the encoder across languages to some extent.\nTo solve natural language tasks, it is often useful\nto characterize words in a sentence by the words\naround them. For example, to understand the se-\nmantics of a sentence, it would be useful to look\nfor the subject by looking for a noun that precedes\nthe word is; to parse a sentence, a word can be iden-\ntified as a noun because it follows the article the.\nIf the encoder computes the output representation\nof a word in a sentence by aggregating the infor-\nmation from its surrounding words, that should be\na useful inductive bias to solve most NLP tasks\nin any language. Also, it is easy to imagine that\nthe knowledge of position-aware context depen-\ndence gives a reasonable prior for solving sequence\nmodeling problems in other domains, which may\nexplain the success of cross-modality transfer of\nlanguage models (Lu et al., 2021; Reid et al., 2022).\nOf course, we do not expect that the knowledge\nof position-aware context dependence explains ev-\nery aspect of the success of cross-lingual transfer.\nAs future work, we need further investigation for\na more fine-grained view of the transferred knowl-\nedge. Important questions include how much the\nmodel size affects the transferability of the encoder\nor if there is any difference in the knowledge trans-\nferred among different downstream tasks.\n7310\nAcknowledgement\nWe thank the anonymous reviewers for their in-\nsightful comments and constructive suggestions to\nimprove the paper.\nReferences\nSanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,\nand Andrej Risteski. 2016. A Latent Variable Model\nApproach to PMI-based Word Embeddings. Transac-\ntions of the Association for Computational Linguis-\ntics, 4:385–399.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the Cross-lingual Transferability of Mono-\nlingual Representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics.\nDavid M. Blei, A. Ng, and Michael I. Jordan. 2003. La-\ntent Dirichlet Allocation. Journal of Machine Learn-\ning Research, 3:993–1022.\nEthan A. Chi, John Hewitt, and Christopher D. Manning.\n2020. Finding Universal Grammatical Relations in\nMultilingual BERT. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics.\nCheng-Han Chiang and Hung yi Lee. 2022. On the\nTransferability of Pre-trained Language Models: A\nStudy from Artificial Datasets. In Proceedings of\nthe Thirty-Sixth AAAI Conference on Artificial Intel-\nligence.\nNoam Chomsky. 1957. Syntactic Structures . De\nGruyter Mouton.\nKenneth Ward Church and Patrick Hanks. 1989. Word\nAssociation Norms, Mutual Information, and Lexi-\ncography. In 27th Annual Meeting of the Association\nfor Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020a. Unsupervised\nCross-lingual Representation Learning at Scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Loïc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $&!#* vector: Probing\nsentence embeddings for linguistic properties. In\nProceedings of the 56th Annual Meeting of the Asso-\nciation for Computational Linguistics.\nAlexis Conneau, Shijie Wu, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020b. Emerging\nCross-lingual Structure in Pretrained Language Mod-\nels. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1.\nSumanth Doddapaneni, Gowtham Ramesh, Anoop\nKunchukuttan, Pratyush Kumar, and Mitesh M.\nKhapra. 2021. A Primer on Pretrained Multilingual\nLanguage Models. ArXiv, abs/2107.00676.\nTimothy Dozat and Christopher D. Manning. 2017.\nDeep Biaffine Attention for Neural Dependency Pars-\ning. In International Conference on Learning Repre-\nsentations.\nPhilipp Dufter and Hinrich Schütze. 2020. Identifying\nElements Essential for BERT’s Multilinguality. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nShort-Term Memory. Neural Computation, 9:1735–\n1780.\nDieuwke Hupkes and Willem Zuidema. 2018. Visuali-\nsation and ’Diagnostic Classifiers’ Reveal how Recur-\nrent and Recursive Neural Networks Process Hierar-\nchical Structure (Extended Abstract). In Proceedings\nof the Twenty-Seventh International Joint Conference\non Artificial Intelligence, IJCAI-18.\nStephen Mayhew Karthikeyan K, Zihan Wang and Dan\nRoth. 2020. Cross-Lingual Ability of Multilingual\nBERT: An Empirical Study. In International Confer-\nence on Learning Representations.\nUrvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky.\n2018. Sharp Nearby, Fuzzy Far Away: How Neu-\nral Language Models Use Context. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics.\nYi-An Lai, Garima Lalwani, and Yi Zhang. 2020. Con-\ntext Analysis for Pre-trained Masked Language Mod-\nels. In Findings of the Association for Computational\nLinguistics: EMNLP 2020.\nOmer Levy and Yoav Goldberg. 2014. Dependency-\nBased Word Embeddings. In Proceedings of the 52nd\nAnnual Meeting of the Association for Computational\nLinguistics.\nWang Ling, Chris Dyer, Alan W Black, Isabel Tran-\ncoso, Ramón Fermandez, Silvio Amir, Luís Marujo,\nand Tiago Luís. 2015. Finding Function in Form:\nCompositional Character Models for Open V ocab-\nulary Word Representation. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing.\nNelson F. Liu, Omer Levy, Roy Schwartz, Chenhao Tan,\nand Noah A. Smith. 2018. LSTMs Exploit Linguis-\ntic Attributes of Data. In Proceedings of The Third\nWorkshop on Representation Learning for NLP.\n7311\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual Denoising\nPre-training for Neural Machine Translation. Trans-\nactions of the Association for Computational Linguis-\ntics, 8:726–742.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nKevin Lu, Aditya Grover, P. Abbeel, and Igor Mordatch.\n2021. Pretrained Transformers as Universal Compu-\ntation Engines. ArXiv, abs/2103.05247.\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a Large Annotated\nCorpus of English: The Penn Treebank. Comput.\nLinguistics, 19:313–330.\nIgor Mel’ˇcuk. 1988. Dependency Syntax: Theory and\nPractice. State University Press of New York.\nTomas Mikolov, Martin Karafiát, Lukás Burget,\nJan Honza Cernocký, and Sanjeev Khudanpur. 2010.\nRecurrent neural network based language model. In\nProceedings of Annual Conference of the Interna-\ntional Speech Communication Association.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Jan Haji ˇc, Christopher D. Manning, Sampo\nPyysalo, Sebastian Schuster, Francis Tyers, and\nDaniel Zeman. 2020. Universal Dependencies v2:\nAn Evergrowing Multilingual Treebank Collection.\nIn Proceedings of the 12th Language Resources and\nEvaluation Conference.\nIsabel Papadimitriou, Ethan A. Chi, Richard Futrell, and\nKyle Mahowald. 2021. Deep Subjecthood: Higher-\nOrder Grammatical Features in Multilingual BERT.\nIn Proceedings of the 16th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics: Main Volume.\nIsabel Papadimitriou and Dan Jurafsky. 2020. Learning\nMusic Helps You Read: Using Transfer to Study Lin-\nguistic Structure in Language Models. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow Multilingual is Multilingual BERT? InProceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics.\nOfir Press and Lior Wolf. 2017. Using the output embed-\nding to improve language models. In Proceedings of\nthe 15th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Volume\n2, Short Papers.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the Lim-\nits of Transfer Learning with a Unified Text-to-Text\nTransformer. Journal of Machine Learning Research,\n21(140):1–67.\nShauli Ravfogel, Yoav Goldberg, and Tal Linzen. 2019.\nStudying the Inductive Biases of RNNs with Syn-\nthetic Variations of Natural Languages. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1.\nMachel Reid, Yutaro Yamada, and Shixiang Shane Gu.\n2022. Can Wikipedia Help Offline Reinforcement\nLearning? ArXiv, abs/2201.12122.\nRyokan Ri and Yoshimasa Tsuruoka. 2020. Revisiting\nthe Context Window for Cross-lingual Word Embed-\ndings. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics.\nKoustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle\nPineau, Adina Williams, and Douwe Kiela. 2021a.\nMasked language modeling and the distributional hy-\npothesis: Order word matters pre-training for little.\nIn Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing.\nKoustuv Sinha, Prasanna Parthasarathi, Joelle Pineau,\nand Adina Williams. 2021b. UnNatural Language In-\nference. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing.\nKe Tran, Arianna Bisazza, and Christof Monz. 2018.\nThe Importance of Being Recurrent for Modeling\nHierarchical Structure. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In Advances in Neural Information Pro-\ncessing Systems, volume 30.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Interna-\ntional Conference on Learning Representations.\nJennifer C. White and Ryan Cotterell. 2021. Examining\nthe Inductive Bias of Neural Language Models with\nArtificial Languages. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing.\nShijie Wu and Mark Dredze. 2019. Beto, Bentz, Becas:\nThe Surprising Cross-Lingual Effectiveness of BERT.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A Massively Multilingual\n7312\nPre-trained Text-to-Text Transformer. In Proceed-\nings of the 2021 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXLNet: Generalized Autoregressive Pretraining for\nLanguage Understanding. In Advances in Neural\nInformation Processing Systems, volume 32.\nDamián H. Zanette and Marcelo A. Montemurro. 2005.\nDynamics of Text Generation with Realistic Zipf’s\nDistribution. Journal of Quantitative Linguistics ,\n12:29 – 40.\nGeorge Kingsley Zipf. 1949. Human behavior and the\nprinciple of least effort. Addison-Wesley Press.\n7313\nAppendix for “Pretraining with Artificial\nLanguage: Studying Transferable\nKnowledge in Language Models”\nA Generating the Nesting Structure\nIn the Nesting languages introduced in §3.2.3, to-\nkens are ordered in a way that any dependency arcs\nin a sequence are not crossed. This is realized by\nthe stack-based algorithm in Algorithm 1. We set\nthe probability of closing a dependency pair to 0.4\nfollowing Papadimitriou and Jurafsky (2020).\nAlgorithm 1 Generating a sentence from the Nest-\ning Dependency language.\nInput: input_pairs: Stack[(w, w)]]\nOutput: sentence: List[w]\n1: closing_stack = []\n2: while not input_pairs.is_empty() do\n3: Uniform sampling p ∼ [0, 1]\n4: if closing_stack.is_empty() or p < 0.4 then\n5: head, tail = input_pairs.pop()\n6: sentence.append(head)\n7: closing_stack.push(tail)\n8: else\n9: tail = closing_stack.pop()\n10: sentence.append(tail)\n11: end if\n12: end while\n13: while not closing_stack.is_empty() do\n14: tail = closing_stack.pop()\n15: sentence.append(tail)\n16: end while\n17: return sentence\nB Details of Causal Language Modeling\nTask\nB.1 Model configuration\nFor the experiment with causal language modeling\n(§4), we set the number of layers of the LSTM and\nTransformer encoders to 3 and configure them so\nthat they have the same number of parameters (2.1\nM parameters without the embedding and output\nprojection layers). The details of configuration are\nshown in Table 1 and Table 2.\nThe weights of the output projection layer are\ntied with the word embedding layer (Press and\nWolf, 2017). Note that, to enable this, the LSTM\nencoder has an additional linear layer to project the\nhidden vector (294 dim) to the input size (300 dim),\nwhich the Transformer encoder does not have.\n# of layers 3\ninput size 300\nhidden size 294\nTable 1: Configuration of the LSTM encoder.\n# of layers 3\nsize 300\nfeedforward size 600\n# of attention heads 4\nTable 2: Configuration of the Transformer encoder.\nB.2 Optimization\nWe optimize the pretrained models for 10k steps\nwith 12.8 M sentences and the batch size of 128\nusing AdamW (Loshchilov and Hutter, 2019). We\nuse the the Noam Learning rate scheduler described\nin Vaswani et al. (2017) with the warmup steps\nof 4000, and the other hyper-parameter details\nare shown in Table 3. We use the same hyper-\nparameters for fine-tuning with the L2 language.\nName Value\nPretraining minimum sentence length 6\nPretraining maximum sentence length 60\nDropout 0.1\nWeight decay 0.01\nAdam β1 0.9\nAdam β2 0.98\nAdam ϵ 1e-9\nGradient clipping 0.25\nTable 3: Hyper-parameters for pretraining.\nC Details of Masked Language Modeling\nTask\nC.1 Model configuration\nFor the experiment with masked language model-\ning (§5), we set the number of layers of the Trans-\nformer encoders to 3. The details of configuration\nare shown in Table 4 (2.1 M parameters without\nthe embedding and output projection layers).\nThe hyper-parameters for the masked language\nmodeling task is shown in Table 5. For optimiza-\ntion, we used the same hyper-parameters as in Ap-\npendix B.2.\n7314\n# of layers 3\nsize 300\nfeedforward size 600\n# of attention heads 4\nTable 4: Model configuration of the Transformer en-\ncoder.\nMask probability for words 15%\nRandom-word probability for words 10%\nUnmasked probability for words 10%\nTable 5: The hyper-parameters for masked language\nmodeling.\nD Computing Infrastructure\nWe ran the experiments on a server with a Intel(R)\nXeon(R) CPU E5-2698 v4 @ 2.20GHz CPU and\n10 NVIDIA TITAN Xp GPUs. Each pretraining\nand finetuning were run with a single GPU.\n7315",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.859859824180603
    },
    {
      "name": "Natural language",
      "score": 0.6745790839195251
    },
    {
      "name": "Natural language processing",
      "score": 0.6307470798492432
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6082148551940918
    },
    {
      "name": "Artificial neural network",
      "score": 0.5356070399284363
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5280612111091614
    },
    {
      "name": "Encoder",
      "score": 0.519693911075592
    },
    {
      "name": "Universal Networking Language",
      "score": 0.5072793364524841
    },
    {
      "name": "Transferability",
      "score": 0.4607701003551483
    },
    {
      "name": "Language model",
      "score": 0.4300727844238281
    },
    {
      "name": "Comprehension approach",
      "score": 0.14951622486114502
    },
    {
      "name": "Machine learning",
      "score": 0.1136879026889801
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Logit",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}