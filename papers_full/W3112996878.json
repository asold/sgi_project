{
  "title": "Point Transformer.",
  "url": "https://openalex.org/W3112996878",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2516465340",
      "name": "Hengshuang Zhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096544739",
      "name": "Li Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096891736",
      "name": "Jiaya Jia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2107932047",
      "name": "Philip H. S. Torr",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1809196549",
      "name": "Vladlen Koltun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2796422723",
    "https://openalex.org/W1920022804",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2963517242",
    "https://openalex.org/W2963057320",
    "https://openalex.org/W2996324165",
    "https://openalex.org/W2960986959",
    "https://openalex.org/W2991084087",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2553307952",
    "https://openalex.org/W2963121255",
    "https://openalex.org/W2963083779",
    "https://openalex.org/W2982210492",
    "https://openalex.org/W2788158258",
    "https://openalex.org/W2981199548",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2769312834",
    "https://openalex.org/W2981983525",
    "https://openalex.org/W2968296999",
    "https://openalex.org/W3012494314",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W2963125977",
    "https://openalex.org/W2990045899",
    "https://openalex.org/W2902302021",
    "https://openalex.org/W2211722331",
    "https://openalex.org/W2963226018",
    "https://openalex.org/W2963830382",
    "https://openalex.org/W2990613095",
    "https://openalex.org/W2460657278",
    "https://openalex.org/W2963281829",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2963158438",
    "https://openalex.org/W2963053547",
    "https://openalex.org/W2556802233",
    "https://openalex.org/W2963123724",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W2964342398",
    "https://openalex.org/W2606202972",
    "https://openalex.org/W2963182550",
    "https://openalex.org/W1644641054",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W2555618208",
    "https://openalex.org/W2979750740",
    "https://openalex.org/W2963231572",
    "https://openalex.org/W2953273646",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963094037",
    "https://openalex.org/W2557465155",
    "https://openalex.org/W2798270772",
    "https://openalex.org/W3122159272",
    "https://openalex.org/W2797997528",
    "https://openalex.org/W2962731536"
  ],
  "abstract": "Self-attention networks have revolutionized natural language processing and\nare making impressive strides in image analysis tasks such as image\nclassification and object detection. Inspired by this success, we investigate\nthe application of self-attention networks to 3D point cloud processing. We\ndesign self-attention layers for point clouds and use these to construct\nself-attention networks for tasks such as semantic scene segmentation, object\npart segmentation, and object classification. Our Point Transformer design\nimproves upon prior work across domains and tasks. For example, on the\nchallenging S3DIS dataset for large-scale semantic scene segmentation, the\nPoint Transformer attains an mIoU of 70.4% on Area 5, outperforming the\nstrongest prior model by 3.3 absolute percentage points and crossing the 70%\nmIoU threshold for the first time.",
  "full_text": "Point Transformer\nHengshuang Zhao1,2 Li Jiang3 Jiaya Jia3 Philip Torr1 Vladlen Koltun4\n1University of Oxford 2The University of Hong Kong\n3The Chinese University of Hong Kong 4Intel Labs\nAbstract\nSelf-attention networks have revolutionized natural lan-\nguage processing and are making impressive strides in im-\nage analysis tasks such as image classification and object\ndetection. Inspired by this success, we investigate the ap-\nplication of self-attention networks to 3D point cloud pro-\ncessing. We design self-attention layers for point clouds and\nuse these to construct self-attention networks for tasks such\nas semantic scene segmentation, object part segmentation,\nand object classification. Our Point Transformer design im-\nproves upon prior work across domains and tasks. For ex-\nample, on the challenging S3DIS dataset for large-scale se-\nmantic scene segmentation, the Point Transformer attains\nan mIoU of 70.4% on Area 5, outperforming the strongest\nprior model by 3.3 absolute percentage points and crossing\nthe 70% mIoU threshold for the first time.\n1. Introduction\n3D data arises in many application areas such as au-\ntonomous driving, augmented reality, and robotics. Unlike\nimages, which are arranged on regular pixel grids, 3D point\nclouds are sets embedded in continuous space. This makes\n3D point clouds structurally different from images and pre-\ncludes immediate application of deep network designs that\nhave become standard in computer vision, such as networks\nbased on the discrete convolution operator.\nA variety of approaches to deep learning on 3D point\nclouds have arisen in response to this challenge. Some vox-\nelize the 3D space to enable the application of 3D discrete\nconvolutions [23, 32]. This induces massive computational\nand memory costs and underutilizes the sparsity of point\nsets in 3D. Sparse convolutional networks relieve these limi-\ntations by operating only on voxels that are not empty [9, 3].\nOther designs operate directly on points and propagate in-\nformation via pooling operators [25, 27] or continuous con-\nvolutions [42, 37]. Another family of approaches connect\nthe point set into a graph for message passing [44, 19].\nIn this work, we develop an approach to deep learning on\npoint clouds that is inspired by the success of transformers\nsemantic segmentation\npart segmentation\nclassification\nairplanelamp\nbed\nPointTransformer\nFigure 1. The Point Transformer can serve as the backbone for var-\nious 3D point cloud understanding tasks such as object classifica-\ntion, object part segmentation, and semantic scene segmentation.\nin natural language processing [39, 45, 5, 4, 51] and image\nanalysis [10, 28, 54]. The transformer family of models is\nparticularly appropriate for point cloud processing because\nthe self-attention operator, which is at the core of trans-\nformer networks, is in essence a set operator: it is invariant\nto permutation and cardinality of the input elements. The\napplication of self-attention to 3D point clouds is therefore\nquite natural, since point clouds are essentially sets embed-\nded in 3D space.\nWe flesh out this intuition and develop a self-attention\nlayer for 3D point cloud processing. Based on this layer,\nwe construct Point Transformer networks for a variety of\n3D understanding tasks. We investigate the form of the self-\nattention operator, the application of self-attention to local\nneighborhoods around each point, and the encoding of po-\nsitional information in the network. The resulting networks\nare based purely on self-attention and pointwise operations.\nWe show that Point Transformers are remarkably effec-\ntive in 3D deep learning tasks, both at the level of detailed\nobject analysis and large-scale parsing of massive scenes.\nIn particular, Point Transformers set the new state of the art\non large-scale semantic segmentation on the S3DIS dataset\n(70.4% mIoU on Area 5), shape classification on Model-\nNet40 (93.7% overall accuracy), and object part segmenta-\narXiv:2012.09164v2  [cs.CV]  26 Sep 2021\ntion on ShapeNetPart (86.6% instance mIoU). Our full im-\nplementation and trained models will be released upon ac-\nceptance. In summary, our main contributions include the\nfollowing.\n‚Ä¢ We design a highly expressive Point Transformer layer\nfor point cloud processing. The layer is invariant\nto permutation and cardinality and is thus inherently\nsuited to point cloud processing.\n‚Ä¢ Based on the Point Transformer layer, we construct\nhigh-performing Point Transformer networks for clas-\nsification and dense prediction on point clouds. These\nnetworks can serve as general backbones for 3D scene\nunderstanding.\n‚Ä¢ We report extensive experiments over multiple do-\nmains and datasets. We conduct controlled studies to\nexamine specific choices in the Point Transformer de-\nsign and set the new state of the art on multiple highly\ncompetitive benchmarks, outperforming long lines of\nprior work.\n2. Related Work\nFor 2D image understanding, pixels are placed in regu-\nlar grids and can be processed with classical convolution.\nIn contrast, 3D point clouds are unordered and scattered\nin 3D space: they are essentially sets. Learning-based ap-\nproaches to processing 3D point clouds can be classified\ninto the following types: projection-based, voxel-based, and\npoint-based networks.\nProjection-based networks. For processing irregular in-\nputs like point clouds, an intuitive way is to transform ir-\nregular representations to regular ones. Considering the\nsuccess of 2D CNNs, some approaches [34, 18, 2, 14, 16]\nadopt multi-view projection, where 3D point clouds are pro-\njected into various image planes. Then 2D CNNs are used\nto extract feature representations in these image planes, fol-\nlowed by multi-view feature fusion to form the final output\nrepresentations. In a related approach, TangentConv [35]\nprojects local surface geometry onto a tangent plane at ev-\nery point, forming tangent images that can be processed by\n2D convolution. However, this approach heavily relies on\ntangent estimation. In projection-based frameworks, the ge-\nometric information inside point clouds is collapsed during\nthe projection stage. These approaches may also underuti-\nlize the sparsity of point clouds when forming dense pixel\ngrids on projection planes. The choice of projection planes\nmay heavily influence recognition performance and occlu-\nsion in 3D may impede accuracy.\nVoxel-based networks. An alternative approach to trans-\nforming irregular point clouds to regular representations is\n3D voxelization [23, 32], followed by convolutions in 3D.\nWhen applied naively, this strategy can incur massive com-\nputation and memory costs due to the cubic growth in the\nnumber of voxels as a function of resolution. The solution\nis to take advantage of sparsity, as most voxels are usually\nunoccupied. For example, OctNet [29] uses unbalanced\noctrees with hierarchical partitions. Approaches based on\nsparse convolutions, where the convolution kernel is only\nevaluated at occupied voxels, can further reduce computa-\ntion and memory requirements [9, 3]. These methods have\ndemonstrated good accuracy but may still lose geometric\ndetail due to quantization onto the voxel grid.\nPoint-based networks. Rather than projecting or quantiz-\ning irregular point clouds onto regular grids in 2D or 3D, re-\nsearchers have designed deep network structures that ingest\npoint clouds directly, as sets embedded in continuous space.\nPointNet [25] utilizes permutation-invariant operators such\nas pointwise MLPs and pooling layers to aggregate features\nacross a set. PointNet++ [27] applies these ideas within a\nhierarchical spatial structure to increase sensitivity to local\ngeometric layout. Such models can benefit from efficient\nsampling of the point set, and a variety of sampling strate-\ngies have been developed [27, 7, 46, 50, 11].\nA number of approaches connect the point set into\na graph and conduct message passing on this graph.\nDGCNN [44] performs graph convolutions on kNN graphs.\nPointWeb [55] densely connects local neightborhoods.\nECC [31] uses dynamic edge-conditioned filters where con-\nvolution kernels are generated based on edges inside point\nclouds. SPG [15] operates on a superpoint graph that rep-\nresents contextual relationships. KCNet [30] utilizes kernel\ncorrelation and graph pooling. Wang et al. [40] investigate\nthe local spectral graph convolution. GACNet [41] employs\ngraph attention convolution and HPEIN [13] builds a hierar-\nchical point-edge interaction architecture. DeepGCNs [19]\nexplore the advantages of depth in graph convolutional net-\nworks for 3D scene understanding.\nA number of methods are based on continuous convolu-\ntions that apply directly to the 3D point set, with no quan-\ntization. PCCN [42] represents convolutional kernels as\nMLPs. SpiderCNN [49] defines kernel weights as a fam-\nily of polynomial functions. Spherical CNN [8] designs\nspherical convolution to address the problem of 3D rota-\ntion equivariance. PointConv [46] and KPConv [37] con-\nstruct convolution weights based on the input coordinates.\nInterpCNN [22] utilizes coordinates to interpolate point-\nwise kernel weights. PointCNN [20] proposes to reorder the\ninput unordered point clouds with special operators. Um-\nmenhofer et al. [38] apply continuous convolutions to learn\nparticle-based fluid dynamics.\nTransformer and self-attention. Transformer and self-\nattention models have revolutionized machine translation\nand natural language processing [39, 45, 5, 4, 51]. This\nhas inspired the development of self-attention networks for\n2D image recognition [10, 28, 54, 6]. Hu et al. [10] and\nRamachandran et al. [28] apply scalar dot-product self-\nattention within local image patches. Zhao et al. [54] de-\nvelop a family of vector self-attention operators. Dosovit-\nskiy et al. [6] treat images as sequences of patches.\nOur work is inspired by the findings that transform-\ners and self-attention networks can match or even outper-\nform convolutional networks on sequences and 2D images.\nSelf-attention is of particular interest in our setting because\nit is intrinsically a set operator: positional information is\nprovided as attributes of elements that are processed as a\nset [39, 54]. Since 3D point clouds are essentially sets of\npoints with positional attributes, the self-attention mecha-\nnism seems particularly suitable to this type of data. We\nthus develop a Point Transformer layer that applies self-\nattention to 3D point clouds.\nThere are a number of previous works [48, 21, 50, 17]\nthat utilize attention for point cloud analysis. They apply\nglobal attention on the whole point cloud, which introduces\nheavy computation and renders these approaches inapplica-\nble to large-scale 3D scene understanding. They also utilize\nscalar dot-product attention, where different channels share\nthe same aggregation weights. In contrast, we apply self-\nattention locally, which enables scalability to large scenes\nwith millions of points, and we utilize vector attention,\nwhich we show to be important for achieving high accuracy.\nWe also demonstrate the importance of appropriate position\nencoding in large-scale point cloud understanding, in con-\ntrast to prior approaches that omitted position information.\nOverall, we show that appropriately designed self-attention\nnetworks can scale to large and complex 3D scenes, and\nsubstantially advance the state of the art in large-scale point\ncloud understanding.\n3. Point Transformer\nWe begin by briefly revisiting the general formulation of\ntransformers and self-attention operators. Then we present\nthe point transformer layer for 3D point cloud processing.\nLastly, we present our network architecture for 3D scene\nunderstanding.\n3.1. Background\nTransformers and self-attention networks have revolu-\ntionized natural language processing [39, 45, 5, 4, 51] and\nhave demonstrated impressive results in 2D image analy-\nsis [10, 28, 54, 6]. Self-attention operators can be classi-\nfied into two types: scalar attention [39] and vector atten-\ntion [54].\nLet X = {xi}i be a set of feature vectors. The stan-\ndard scalar dot-product attention layer can be represented\nas follows:\nyi =\nX\nxj ‚ààX\nœÅ\n\u0000\nœÜ(xi)‚ä§œà(xj ) +Œ¥\n\u0001\nŒ±(xj ), (1)\nwhere yi is the output feature. œÜ, œà, and Œ± are pointwise\nfeature transformations, such as linear projections or MLPs.\nŒ¥ is a position encoding function and œÅ is a normalization\nfunction such as softmax. The scalar attention layer com-\nputes the scalar product between features transformed by œÜ\nand œà and uses the output as an attention weight for aggre-\ngating features transformed by Œ±.\nIn vector attention, the computation of attention weights\nis different. In particular, attention weights are vectors that\ncan modulate individual feature channels:\nyi =\nX\nxj ‚ààX\nœÅ\n\u0000\nŒ≥(Œ≤(œÜ(xi), œà(xj )) +Œ¥)\n\u0001\n‚äô Œ±(xj ), (2)\nwhere Œ≤ is a relation function (e.g., subtraction) and Œ≥ is\na mapping function (e.g., an MLP) that produces attention\nvectors for feature aggregation.\nBoth scalar and vector self-attention are set operators.\nThe set can be a collection of feature vectors that represent\nthe entire signal (e.g., sentence or image) [39, 6] or a collec-\ntion of feature vectors from a local patch within the signal\n(e.g., an image patch) [10, 28, 54].\n3.2. Point Transformer Layer\nSelf-attention is a natural fit for point clouds because\npoint clouds are essentially sets embedded irregularly in a\nmetric space. Our point transformer layer is based on vec-\ntor self-attention. We use the subtraction relation and add\na position encoding Œ¥ to both the attention vector Œ≥ and the\ntransformed features Œ±:\nyi =\nX\nxj ‚ààX(i)\nœÅ\n\u0000\nŒ≥(œÜ(xi) ‚àíœà(xj ) +Œ¥)\n\u0001\n‚äô\n\u0000\nŒ±(xj ) +Œ¥\n\u0001\n(3)\nHere the subset X(i) ‚äÜ Xis a set of points in a local neigh-\nborhood (specifically, k nearest neighbors) of xi. Thus we\nadopt the practice of recent self-attention networks for im-\nage analysis in applying self-attention locally, within a lo-\ncal neighborhood around each datapoint [10, 28, 54]. The\nmapping function Œ≥ is an MLP with two linear layers and\none ReLU nonlinearity. The point transformer layer is il-\nlustrated in Figure 2.\naggregation\nùúë, ùúì: linear ùõø: mlp ùõº: linear\noutput: (y, p)\ninput: (x, p)\nùõæ:mlp\nFigure 2. Point transformer layer.\n(N, Dout)(N, 32) (N/16, 128) (N/64, 256) (N/256, 512)(N/4, 64) (N/16, 128)(N/64, 256)(N/256, 512) (N/4, 64) (N, 32)\nLabel: chair\n(1, Dout)\nMLP\nPoint Transformer\nTransition Down\nTransition Up\nGlobal AvgPooling\n(N, 32) (N/16, 128) (N/64, 256) (N/256, 512)(N/4, 64) (1, 512)\nFigure 3. Point transformer networks for semantic segmentation (top) and classification (bottom).\nlinear\nlinear\npoint transformer\ninput: (x, p)\noutput: (y, p)\nfarthest point sampl.\nlocal max pooling\nkNN, mlp\ninput: (x, p1)\noutput: (y, p2)\nlinear\nsummation\ninterpolation\ninput1: (x1, p1)\noutput: (y, p2)\nlinear\ninput2: (x2, p2)\n(a) point transformer block (b) transition down (c) transition up\nFigure 4. Detailed structure design for each module.\n3.3. Position Encoding\nPosition encoding plays an important role in self-\nattention, allowing the operator to adapt to local structure\nin the data [39]. Standard position encoding schemes for\nsequences and image grids are crafted manually, for exam-\nple based on sine and cosine functions or normalized range\nvalues [39, 54]. In 3D point cloud processing, the 3D point\ncoordinates themselves are a natural candidate for position\nencoding. We go beyond this by introducing trainable, pa-\nrameterized position encoding. Our position encoding func-\ntion Œ¥ is defined as follows:\nŒ¥ = Œ∏(pi ‚àí pj ). (4)\nHere pi and pj are the 3D point coordinates for points i\nand j. The encoding function Œ∏ is an MLP with two linear\nlayers and one ReLU nonlinearity. Notably, we found that\nposition encoding is important for both the attention gener-\nation branch and the feature transformation branch. Thus\nEq. 3 adds the trainable position encoding in both branches.\nThe position encoding Œ∏ is trained end-to-end with the other\nsubnetworks.\n3.4. Point Transformer Block\nWe construct a residual point transformer block with the\npoint transformer layer at its core, as shown in Figure 4(a).\nThe transformer block integrates the self-attention layer,\nlinear projections that can reduce dimensionality and ac-\ncelerate processing, and a residual connection. The input\nis a set of feature vectors x with associated 3D coordinates\np. The point transformer block facilitates information ex-\nchange between these localized feature vectors, producing\nnew feature vectors for all data points as its output. The\ninformation aggregation adapts both to the content of the\nfeature vectors and their layout in 3D.\n3.5. Network Architecture\nWe construct complete 3D point cloud understanding\nnetworks based on the point transformer block. Note that\nthe point transformer is the primary feature aggregation op-\nerator throughout the network. We do not use convolu-\ntions for preprocessing or auxiliary branches: the network is\nbased entirely on point transformer layers, pointwise trans-\nformations, and pooling. The network architectures are vi-\nsualized in Figure 3.\nBackbone structure. The feature encoder in point trans-\nformer networks for semantic segmentation and classifica-\ntion has five stages that operate on progressively downsam-\npled point sets. The downsampling rates for the stages are\n[1, 4, 4, 4, 4], thus the cardinality of the point set produced\nby each stage is [N, N/4, N/16, N/64, N/256], where N is the\nnumber of input points. Note that the number of stages and\nthe downsampling rates can be varied depending on the ap-\nplication, for example to construct light-weight backbones\nfor fast processing. Consecutive stages are connected by\ntransition modules: transition down for feature encoding\nand transition up for feature decoding.\nTransition down. A key function of the transition down\nmodule is to reduce the cardinality of the point set as re-\nquired, for example from N to N/4 in the transition from\nthe first to the second stage. Denote the point set provided\nas input to the transition down module as P1 and denote\nthe output point set as P2. We perform farthest point sam-\npling [27] in P1 to identify a well-spread subset P2 ‚äÇ P1\nwith the requisite cardinality. To pool feature vectors from\nP1 onto P2, we use a kNN graph on P1. (This is the same\nk as in Section 3.2. We use k = 16 throughout and report\na controlled study of this hyperparameter in Section 4.4.)\nEach input feature goes through a linear transformation, fol-\nlowed by batch normalization and ReLU, followed by max\npooling onto each point in P2 from its k neighbors in P1.\nThe transition down module is schematically illustrated in\nFigure 4(b).\nTransition up. For dense prediction tasks such as seman-\ntic segmentation, we adopt a U-net design in which the\nencoder described above is coupled with a symmetric de-\ncoder [27, 3]. Consecutive stages in the decoder are con-\nnected by transition up modules. Their primary function is\nto map features from the downsampled input point set P2\nonto its superset P1 ‚äÉ P2. To this end, each input point\nfeature is processed by a linear layer, followed by batch\nnormalization and ReLU, and then the features are mapped\nonto the higher-resolution point set P1 via trilinear inter-\npolation. These interpolated features from the preceding\ndecoder stage are summarized with the features from the\ncorresponding encoder stage, provided via a skip connec-\ntion. The structure of the transition up module is illustrated\nin Figure 4(c).\nOutput head. For semantic segmentation, the final decoder\nstage produces a feature vector for each point in the input\npoint set. We apply an MLP to map this feature to the final\nlogits. For classification, we perform global average pool-\ning over the pointwise features to get a global feature vec-\ntor for the whole point set. This global feature is passed\nthrough an MLP to get the global classification logits.\n4. Experiments\nWe evaluate the effectiveness of the presented Point\nTransformer design on a number of domains and tasks. For\n3D semantic segmentation, we use the challenging Stanford\nLarge-Scale 3D Indoor Spaces (S3DIS) dataset [1]. For\n3D shape classification, we use the widely adopted Mod-\nelNet40 dataset [47]. And for object part segmentation, we\nuse ShapeNetPart [52].\nImplementation details. We implement the Point Trans-\nformer in PyTorch [24]. We use the SGD optimizer with\nmomentum and weight decay set to 0.9 and 0.0001, respec-\ntively. For semantic segmentation on S3DIS, we train for\n40K iterations with initial learning rate 0.5, dropped by 10x\nat steps 24K and 32K. For 3D shape classification on Mod-\nelNet40 and 3D object part segmentation on ShapeNetPart,\nwe train for 200 epochs. The initial learning rate is set to\n0.05 and is dropped by 10x at epochs 120 and 160.\n4.1. Semantic Segmentation\nData and metric. The S3DIS [1] dataset for semantic scene\nparsing consists of 271 rooms in six areas from three differ-\nent buildings. Each point in the scan is assigned a semantic\nlabel from 13 categories (ceiling, floor, table, etc.). Follow-\ning a common protocol [36, 27], we evaluate the presented\napproach in two modes: (a) Area 5 is withheld during train-\ning and is used for testing, and (b) 6-fold cross-validation.\nFor evaluation metrics, we use mean classwise intersection\nover union (mIoU), mean of classwise accuracy (mAcc),\nand overall pointwise accuracy (OA).\nPerformance comparison. The results are presented in Ta-\nbles 1 and 2. The Point Transformer outperforms all prior\nmodels according to all metrics in both evaluation modes.\nOn Area 5, the Point Transformer attains mIoU/mAcc/OA\nof 70.4%/76.5%/90.8%, outperforming all prior work by\nmultiple percentage points in each metric. The Point Trans-\nformer is the first model to pass the 70% mIoU bar, outper-\nforming the prior state of the art by 3.3 absolute percent-\nage points in mIoU. The Point Transformer outperforms\nMLPs-based frameworks such as PointNet [25], voxel-\nbased architectures such as SegCloud [36], graph-based\nmethods such as SPGraph [15], attention-based methods\nsuch as PAT [50], sparse convolutional networks such as\nMinkowskiNet [3], and continuous convolutional networks\nsuch as KPConv [37]. Point Transformer also substantially\noutperforms all prior models under 6-fold cross-validation.\nThe mIoU in this mode is 73.5%, outperforming the prior\nstate of the art (KPConv) by 2.9 absolute percentage points.\nThe number of parameters in Point Transformer (4.9M) is\nmuch smaller than in current high-performing architectures\nsuch as KPConv (14.9M) and SparseConv (30.1M).\nVisualization. Figure 5 shows the Point Transformer‚Äôs pre-\ndictions. We can see that the predictions are very close to\nMethod OA mAcc mIoU ceiling floor wall beam column window door table chair sofa bookcase board clutter\nPointNet [25] ‚Äì 49.0 41.1 88.8 97.3 69.8 0.1 3.9 46.3 10.8 59.0 52.6 5.9 40.3 26.4 33.2\nSegCloud [36] ‚Äì 57.4 48.9 90.1 96.1 69.9 0.0 18.4 38.4 23.1 70.4 75.9 40.9 58.4 13.0 41.6\nTangentConv [35]‚Äì 62.2 52.6 90.5 97.7 74.0 0.0 20.7 39.0 31.3 77.5 69.4 57.3 38.5 48.8 39.8\nPointCNN [20] 85.9 63.9 57.3 92.3 98.2 79.4 0.0 17.6 22.8 62.1 74.4 80.6 31.7 66.7 62.1 56.7\nSPGraph [15] 86.4 66.5 58.0 89.4 96.9 78.1 0.0 42.8 48.9 61.6 84.7 75.4 69.8 52.6 2.1 52.2\nPCCN [42] ‚Äì 67.0 58.3 92.3 96.2 75.9 0.3 6.0 69.5 63.5 66.9 65.6 47.3 68.9 59.1 46.2\nPAT [50] ‚Äì 70.8 60.1 93.0 98.5 72.3 1.0 41.5 85.1 38.2 57.7 83.6 48.1 67.0 61.3 33.6\nPointWeb [55] 87.0 66.6 60.3 92.0 98.5 79.4 0.0 21.1 59.7 34.8 76.3 88.3 46.9 69.3 64.9 52.5\nHPEIN [13] 87.2 68.3 61.9 91.5 98.2 81.4 0.0 23.3 65.3 40.0 75.5 87.7 58.5 67.8 65.6 49.4\nMinkowskiNet [37]‚Äì 71.7 65.4 91.8 98.7 86.2 0.0 34.1 48.9 62.4 81.6 89.8 47.2 74.9 74.4 58.6\nKPConv [37] ‚Äì 72.8 67.1 92.8 97.3 82.4 0.0 23.9 58.0 69.0 81.5 91.0 75.4 75.3 66.7 58.9\nPointTransformer90.8 76.5 70.4 94.0 98.5 86.3 0.0 38.0 63.4 74.3 89.1 82.4 74.3 80.2 76.0 59.3\nTable 1. Semantic segmentation results on the S3DIS dataset, evaluated on Area 5.\nMethod OA mAcc mIoU\nPointNet [25] 78.5 66.2 47.6\nRSNet [12] ‚Äì 66.5 56.5\nSPGraph [15] 85.5 73.0 62.1\nPAT [50] ‚Äì 76.5 64.3\nPointCNN [20] 88.1 75.6 65.4\nPointWeb [55] 87.3 76.2 66.7\nShellNet [53] 87.1 ‚Äì 66.8\nRandLA-Net [37] 88.0 82.0 70.0\nKPConv [37] ‚Äì 79.1 70.6\nPointTransformer 90.2 81.9 73.5\nTable 2. Semantic segmentation results on the S3DIS dataset, eval-\nuated with 6-fold cross-validation.\nthe ground truth. Point Transformer captures detailed se-\nmantic structure in complex 3D scenes, such as the legs of\nchairs, the outlines of poster boards, and the trim around\ndoorways.\nMethod input mAcc OA\n3DShapeNets [47] voxel 77.3 84.7\nV oxNet [23] voxel 83.0 85.9\nSubvolume [26] voxel 86.0 89.2\nMVCNN [34] image ‚Äì 90.1\nPointNet [25] point 86.2 89.2\nA-SCN [48] point 87.6 90.0\nSet Transformer [17] point ‚Äì 90.4\nPAT [50] point ‚Äì 91.7\nPointNet++ [27] point ‚Äì 91.9\nSpecGCN [40] point ‚Äì 92.1\nPointCNN [20] point 88.1 92.2\nDGCNN [44] point 90.2 92.2\nPointWeb [55] point 89.4 92.3\nSpiderCNN [49] point ‚Äì 92.4\nPointConv [46] point ‚Äì 92.5\nPoint2Sequence [21] point 90.4 92.6\nKPConv [37] point ‚Äì 92.9\nInterpCNN [22] point ‚Äì 93.0\nPointTransformer point 90.6 93.7\nTable 3. Shape classification results on the ModelNet40 dataset.\n4.2. Shape Classification\nData and metric. The ModelNet40 [47] dataset contains\n12,311 CAD models with 40 object categories. They are\nsplit into 9,843 models for training and 2,468 for testing.\nWe follow the data preparation procedure of Qi et al. [27]\nand uniformly sample the points from each CAD model\ntogether with the normal vectors from the object meshes.\nFor evaluation metrics, we use the mean accuracy within\neach category (mAcc) and the overall accuracy (OA) over\nall classes.\nPerformance comparison. The results are presented in Ta-\nble 3. The Point Transformer sets the new state of the art in\nboth metrics. The overall accuracy of Point Transformer on\nModelNet40 is 93.7%. It outperforms strong graph-based\nmodels such as DGCNN [44], attention-based models such\nas A-SCN [48] and Point2Sequence [21], and strong point-\nbased models such as KPConv [37].\nVisualization. To probe the representation learned by the\nPoint Transformer, we conduct shape retrieval by retrieving\nnearest neighbors in the space of the output features pro-\nduced by the Point Transformer. Some results are shown in\nFigure 6. The retrieved shapes are very similar to the query,\nand when they differ, they differ along aspects that we per-\nceive as less semantically salient, such as legs of desks.\n4.3. Object Part Segmentation\nData and metric. The ShapeNetPart dataset [52] is anno-\ntated for 3D object part segmentation. It consists of 16,880\nmodels from 16 shape categories, with 14,006 3D models\nfor training and 2,874 for testing. The number of parts for\neach category is between 2 and 6, with 50 different parts\nin total. We use the sampled point sets produced by Qi et\nal. [27] for a fair comparison with prior work. For evalua-\ntion metrics, we report category mIoU and instance mIoU.\nPerformance comparison. The results are presented in Ta-\nble 4. The Point Transformer outperforms all prior mod-\nels as measured by instance mIoU. (Note that we did not\nuse loss-balancing during training, which can boost cate-\ngory mIoU.)\nMethod cat. mIoU ins. mIoU\nPointNet [25] 80.4 83.7\nA-SCN [48] ‚Äì 84.6\nPCNN [42] 81.8 85.1\nPointNet++ [27] 81.9 85.1\nDGCNN [44] 82.3 85.1\nPoint2Sequence [21] ‚Äì 85.2\nSpiderCNN [49] 81.7 85.3\nSPLATNet [33] 83.7 85.4\nPointConv [46] 82.8 85.7\nSGPN [43] 82.8 85.8\nPointCNN [20] 84.6 86.1\nInterpCNN [22] 84.0 86.3\nKPConv [37] 85.1 86.4\nPointTransformer 83.7 86.6\nTable 4. Object part segmentation results on the ShapeNetPart\ndataset.\nk mIoU mAcc OA\n4 59.6 66.0 86.0\n8 67.7 73.8 89.9\n16 70.4 76.5 90.8\n32 68.3 75.0 89.8\n64 67.7 74.1 89.9\nTable 5. Ablation study: number of neighbors k in the definition\nof local neighborhoods.\nPos. encoding mIoU mAcc OA\nnone 64.6 71.9 88.2\nabsolute 66.5 73.2 88.9\nrelative 70.4 76.5 90.8\nrelative for attention 67.0 73.0 89.3\nrelative for feature 68.7 74.4 90.4\nTable 6. Ablation study: position encoding.\nOperator mIoU mAcc OA\nMLP 61.7 68.6 87.1\nMLP+pooling 63.7 71.0 87.8\nscalar attention 64.6 71.9 88.4\nvector attention 70.4 76.5 90.8\nTable 7. Ablation study: form of self-attention operator.\nVisualization. Object part segmentation results on a num-\nber of models are shown in Figure 7. The Point Trans-\nformer‚Äôs part segmentation predictions are clean and close\nto the ground truth.\n4.4. Ablation Study\nWe now conduct a number of controlled experiments that\nexamine specific decisions in the Point Transformer design.\nThese studies are performed on the semantic segmentation\ntask on the S3DIS dataset, tested on Area 5.\nNumber of neighbors. We first investigate the setting of\nthe number of neighbors k, which is used in determining\nthe local neighborhood around each point. The results are\nshown in Table 5. The best performance is achieved when\nk is set to 16. When the neighborhood is smaller ( k = 4\nor k = 8), the model may not have sufficient context for its\npredictions. When the neighborhood is larger ( k = 32 or\nk = 64), each self-attention layer is provided with a large\nnumber of datapoints, many of which may be farther and\nless relevant. This may introduce excessive noise into the\nprocessing, lowering the model‚Äôs accuracy.\nSoftmax regularization. We conduct an ablation study\non the normalization function œÅ in Eq. 3. The perfor-\nmance without softmax regularization on S3DIS Area5 is\n66.5%/72.8%/89.3%, in terms of mIoU/mAcc/OA. It is\nmuch lower than the performance with softmax regulariza-\ntion (70.4%/76.5%90.8%). This suggests that the normal-\nization is essential in this setting.\nPosition encoding. We now study the choice of the posi-\ntion encoding Œ¥. The results are shown in Table 6. We can\nsee that without position encoding, the performance drops\nsignificantly. With absolute position encoding, the perfor-\nmance is higher than without. Relative position encoding\nyields the highest performance. When relative position en-\ncoding is added only to the attention generation branch (first\nterm in Eq. 3) or only to the feature transformation branch\n(second term in Eq. 3), the performance drops again, in-\ndicating that adding the relative position encoding to both\nbranches is important.\nAttention type. Finally, we investigate the type of self-\nattention used in the point transformer layer. The results are\nshown in Table 7. We examine four conditions. ‚ÄòMLP‚Äô is\na no-attention baseline that replaces the point transformer\nlayer in the point transformer block with a pointwise MLP.\n‚ÄòMLP+pooling‚Äô is a more advanced no-attention baseline\nthat replaces the point transformer layer with a pointwise\nMLP followed by max pooling within each kNN neighbor-\nhood: this performs feature transformation at each point\nand enables each point to exchange information with its lo-\ncal neighborhood, but does not leverage attention mecha-\nnisms. ‚Äòscalar attention‚Äô replaces the vector attention used\nin Eq. 3 by scalar attention, as in Eq. 1 and in the original\ntransformer design [39]. ‚Äòvector attention‚Äô is the formula-\ntion we use, presented in Eq. 3. We can see that scalar at-\ntention is more expressive than the no-attention baselines,\nbut is in turn outperformed by vector attention. The per-\nformance gap between vector and scalar attention is signif-\nicant: 70.4% vs. 64.6%, an improvement of 5.8 absolute\npercentage points. Vector attention is more expressive since\nit supports adaptive modulation of individual feature chan-\nnels, not just whole feature vectors. This expressivity ap-\npears to be very beneficial in 3D data processing.\nInput Ground Truth Point Transformer Input Ground Truth Point Transformer\nceiling \n wall beam\n bookcase\nfloor\n column\n window\n door\n table\n chair\n sofa\n board clutter\nFigure 5. Visualization of semantic segmentation results on the S3DIS dataset.\nFigure 6. Visualization of shape retrieval results on the ModelNet40 dataset. The leftmost column shows the input query and the other\ncolumns show the retrieved models.\nFigure 7. Visualization of object part segmentation results on the ShapeNetPart dataset. The ground truth is in the top row, Point Trans-\nformer predictions on the bottom.\n5. Conclusion\nTransformers have revolutionized natural language pro-\ncessing and are making impressive gains in 2D image anal-\nysis. Inspired by this progress, we have developed a trans-\nformer architecture for 3D point clouds. Transformers are\nperhaps an even more natural fit for point cloud process-\ning than they are for language or image processing, be-\ncause point clouds are essentially sets embedded in a metric\nspace, and the self-attention operator at the core of trans-\nformer networks is fundamentally a set operator. We have\nshown that beyond this conceptual compatibility, transform-\ners are remarkably effective in point cloud processing, out-\nperforming state-of-the-art designs from a variety of fam-\nilies: graph-based models, sparse convolutional networks,\ncontinuous convolutional networks, and others. We hope\nthat our work will inspire further investigation of the proper-\nties of point transformers, the development of new operators\nand network designs, and the application of transformers to\nother tasks, such as 3D object detection.\nA. Appendix\nMore detailed results. In Table A.1, we present the de-\ntailed comparison of the semantic segmentation results on\nthe S3DIS dataset [1], under the 6-fold cross-validation\nsetting. We get the highest mIoU as 73.5%, outperform-\ning previous approaches (e.g., RandLA-Net [11] and KP-\nConv [37]) by a large margin. For most of the categories\n(like wall, column, table, etc.), our method gets the best ac-\ncuracy. We will release all the implementation details and\ntrained models to the community soon.\nInference time and memory. We test the infer-\nence time and memory consumption of Point Trans-\nformer on one Quadro RTX 6000, with different\nsize of input point clouds. The inference time and\nmemory consumption are 44ms/86ms/222ms/719ms and\n1702M/2064M/2800M/4266M for 10k/20k/40k/80k input\npoints respectively and they can be further reduced with op-\ntimized implementation.\nkNN efficiency. For kNN, when constructing local point\ncloud regions, previous methods like KPConv [37] and\nRandLA-Net [11] use precomputed kNN indices, which\nlimits the flexibility of the overall framework. In our archi-\ntecture, we implement a high-efficiency solution for kNN\nusing the heap sort algorithm. We test the running time of\nour efficient implementation on one Quadro RTX 6000; the\nresults are listed in Table A.2. We also test some naive im-\nplementations, the running time is 56ms/228ms when given\n10k/20k points, which is much slower than ours. More-\nover, naive implementations run out of memory when given\nlarger point clouds.\nReferences\n[1] Iro Armeni, Ozan Sener, Amir R. Zamir, Helen Jiang, Ioan-\nnis Brilakis, Martin Fischer, and Silvio Savarese. 3D seman-\ntic parsing of large-scale indoor spaces. In CVPR, 2016. 5,\n9\n[2] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia.\nMulti-view 3d object detection network for autonomous\ndriving. In CVPR, 2017. 2\n[3] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d\nspatio-temporal convnets: Minkowski convolutional neural\nnetworks. In CVPR, 2019. 1, 2, 5\n[4] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell,\nQuoc V . Le, and Ruslan Salakhutdinov. Transformer-XL:\nAttentive language models beyond a fixed-length context. In\nACL, 2019. 1, 2, 3\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL-HLT, 2019.\n1, 2, 3\n[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. ICLR, 2021. 2, 3\n[7] Oren Dovrat, Itai Lang, and Shai Avidan. Learning to sam-\nple. In CVPR, 2019. 2\n[8] Carlos Esteves, Christine Allen-Blanchette, Ameesh Maka-\ndia, and Kostas Daniilidis. Learning so (3) equivariant rep-\nresentations with spherical cnns. In ECCV, 2018. 2\n[9] Benjamin Graham, Martin Engelcke, and Laurens Van\nDer Maaten. 3d semantic segmentation with submanifold\nsparse convolutional networks. In CVPR, 2018. 1, 2\n[10] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local\nrelation networks for image recognition. In ICCV, 2019. 1,\n2, 3\n[11] Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan\nGuo, Zhihua Wang, Niki Trigoni, and Andrew Markham.\nRandla-net: Efficient semantic segmentation of large-scale\npoint clouds. In CVPR, 2020. 2, 9\n[12] Qiangui Huang, Weiyue Wang, and Ulrich Neumann. Recur-\nrent slice networks for 3d segmentation of point clouds. In\nCVPR, 2018. 6, 10\n[13] Li Jiang, Hengshuang Zhao, Shu Liu, Xiaoyong Shen, Chi-\nWing Fu, and Jiaya Jia. Hierarchical point-edge interaction\nnetwork for point cloud semantic segmentation. In ICCV,\n2019. 2, 6\n[14] Asako Kanezaki, Yasuyuki Matsushita, and Yoshifumi\nNishida. Rotationnet: Joint object categorization and pose\nestimation using multiviews from unsupervised viewpoints.\nIn CVPR, 2018. 2\n[15] Loic Landrieu and Martin Simonovsky. Large-scale point\ncloud semantic segmentation with superpoint graphs. In\nCVPR, 2018. 2, 5, 6, 10\n[16] Alex H Lang, Sourabh V ora, Holger Caesar, Lubing Zhou,\nJiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders\nfor object detection from point clouds. In CVPR, 2019. 2\n[17] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Se-\nungjin Choi, and Yee Whye Teh. Set transformer: A frame-\nwork for attention-based permutation-invariant neural net-\nworks. In ICML, 2019. 3, 6\n[18] Bo Li, Tianlei Zhang, and Tian Xia. Vehicle detection from\n3d lidar using fully convolutional network. In RSS, 2016. 2\n[19] Guohao Li, Matthias Muller, Ali Thabet, and Bernard\nGhanem. Deepgcns: Can gcns go as deep as cnns? In ICCV,\n2019. 1, 2\n[20] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan\nDi, and Baoquan Chen. Pointcnn: Convolution on X-\ntransformed points. In NIPS, 2018. 2, 6, 7, 10\n[21] Xinhai Liu, Zhizhong Han, Yu-Shen Liu, and Matthias\nZwicker. Point2sequence: Learning the shape representa-\ntion of 3d point clouds with an attention-based sequence to\nsequence network. In AAAI, 2019. 3, 6, 7\n[22] Jiageng Mao, Xiaogang Wang, and Hongsheng Li. Interpo-\nlated convolutional networks for 3d point cloud understand-\ning. In ICCV, 2019. 2, 6, 7\n[23] Daniel Maturana and Sebastian Scherer. V oxnet: A 3d con-\nvolutional neural network for real-time object recognition. In\nIROS, 2015. 1, 2, 6\nMethod OA mAcc mIoU ceiling floor wall beam column window door table chair sofa bookcase board clutter\nPointNet [25] 78.5 66.2 47.6 88.0 88.7 69.3 42.4 23.1 47.5 51.6 54.1 42.0 9.6 38.2 29.4 35.2\nRSNet [12] ‚Äì 66.5 56.5 92.5 92.8 78.6 32.8 34.4 51.6 68.1 60.1 59.7 50.2 16.4 44.9 52.0\nSPGraph [15] 85.5 73.0 62.1 89.9 95.1 76.4 62.8 47.1 55.3 68.4 73.5 69.2 63.2 45.9 8.7 52.9\nPAT [50] ‚Äì 76.5 64.3 93.0 98.4 73.5 58.5 38.9 77.4 67.7 62.7 67.3 30.6 59.6 66.6 41.4\nPointCNN [20] 88.1 75.6 65.4 94.8 97.3 75.8 63.3 51.7 58.4 57.2 71.6 69.1 39.1 61.2 52.2 58.6\nPointWeb [55] 87.3 76.2 66.7 93.5 94.2 80.8 52.4 41.3 64.9 68.1 71.4 67.1 50.3 62.7 62.2 58.5\nShellNet [53] 87.1 ‚Äì 66.8 90.2 93.6 79.9 60.4 44.1 64.9 52.9 71.6 84.7 53.8 64.6 48.6 59.4\nRandLA-Net [37]88.0 82.0 70.0 93.1 96.1 80.6 62.4 48.0 64.4 69.4 69.4 76.4 60.0 64.2 65.9 60.1\nKPConv [37] ‚Äì 79.1 70.6 93.6 92.4 83.1 63.9 54.3 66.1 76.6 64.0 57.8 74.9 69.3 61.3 60.3\nPointTransformer90.2 81.9 73.5 94.3 97.5 84.7 55.6 58.1 66.1 78.2 77.6 74.1 67.3 71.2 65.7 64.8\nTable A.1. Semantic segmentation results on S3DIS, evaluated with 6-fold cross-validation.\n#pts k=8 k=16 k=32 k=64 k=128 k=256\n10k 2 2 5 10 17 21\n20k 3 5 8 23 43 49\n40k 8 12 26 71 127 144\n80k 23 37 82 198 356 399\n100k 32 46 99 248 445 494\n200k 104 125 225 545 992 1091\n500k 639 695 867 1589 2865 3143\n1m 2496 2648 2949 4087 6362 6878\nTable A.2. High-efficiency kNN implementation with heap sort\nalgorithm. The leftmost column stands for the number of points\nand the topmost row specifies the number of nearest neighbors.\nThe reported running time is in milliseconds.\n[24] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An\nimperative style, high-performance deep learning library. In\nNIPS, 2019. 5\n[25] Charles Ruizhongtai Qi, Hao Su, Kaichun Mo, and\nLeonidas J. Guibas. Pointnet: Deep learning on point sets\nfor 3d classification and segmentation. In CVPR, 2017. 1, 2,\n5, 6, 7, 10\n[26] Charles Ruizhongtai Qi, Hao Su, Matthias Nie√üner, Angela\nDai, Mengyuan Yan, and Leonidas Guibas. V olumetric and\nmulti-view cnns for object classification on 3d data. In\nCVPR, 2016. 6\n[27] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J.\nGuibas. Pointnet++: Deep hierarchical feature learning on\npoint sets in a metric space. In NIPS, 2017. 1, 2, 5, 6, 7\n[28] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jonathon Shlens. Stand-alone\nself-attention in vision models. In NeurIPS, 2019. 1, 2, 3\n[29] Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger.\nOctnet: Learning deep 3d representations at high resolutions.\nIn CVPR, 2017. 2\n[30] Yiru Shen, Chen Feng, Yaoqing Yang, and Dong Tian. Min-\ning point cloud local structures by kernel correlation and\ngraph pooling. In CVPR, 2018. 2\n[31] Martin Simonovsky and Nikos Komodakis. Dynamic edge-\nconditioned filters in convolutional neural networks on\ngraphs. In CVPR, 2017. 2\n[32] Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Mano-\nlis Savva, and Thomas Funkhouser. Semantic scene comple-\ntion from a single depth image. In CVPR, 2017. 1, 2\n[33] Hang Su, Varun Jampani, Deqing Sun, Subhransu Maji,\nEvangelos Kalogerakis, Ming-Hsuan Yang, and Jan Kautz.\nSplatnet: Sparse lattice networks for point cloud processing.\nIn CVPR, 2018. 7\n[34] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and\nErik G. Learned-Miller. Multi-view convolutional neural\nnetworks for 3d shape recognition. In ICCV, 2015. 2, 6\n[35] Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, and Qian-\nYi Zhou. Tangent convolutions for dense prediction in 3d. In\nCVPR, 2018. 2, 6\n[36] Lyne P. Tchapmi, Christopher B. Choy, Iro Armeni, JunY-\noung Gwak, and Silvio Savarese. Segcloud: Semantic seg-\nmentation of 3d point clouds. In 3DV, 2017. 5, 6\n[37] Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud,\nBeatriz Marcotegui, Franc ¬∏ois Goulette, and Leonidas J\nGuibas. Kpconv: Flexible and deformable convolution for\npoint clouds. In ICCV, 2019. 1, 2, 5, 6, 7, 9, 10\n[38] Benjamin Ummenhofer, Lukas Prantl, Nils Thuerey, and\nVladlen Koltun. Lagrangian fluid simulation with continu-\nous convolutions. In ICLR, 2020. 2\n[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NIPS, 2017. 1, 2,\n3, 4, 7\n[40] Chu Wang, Babak Samari, and Kaleem Siddiqi. Local spec-\ntral graph convolution for point set feature learning. In\nECCV, 2018. 2, 6\n[41] Lei Wang, Yuchun Huang, Yaolin Hou, Shenman Zhang, and\nJie Shan. Graph attention convolution for point cloud seman-\ntic segmentation. In CVPR, 2019. 2\n[42] Shenlong Wang, Simon Suo, Wei-Chiu Ma, Andrei\nPokrovsky, and Raquel Urtasun. Deep parametric continu-\nous convolutional neural networks. In CVPR, 2018. 1, 2, 6,\n7\n[43] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neu-\nmann. Sgpn: Similarity group proposal network for 3d point\ncloud instance segmentation. In CVPR, 2018. 7\n[44] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma,\nMichael M. Bronstein, and Justin M. Solomon. Dynamic\ngraph cnn for learning on point clouds. TOG, 2019. 1, 2, 6,\n7\n[45] Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin,\nand Michael Auli. Pay less attention with lightweight and\ndynamic convolutions. In ICLR, 2019. 1, 2, 3\n[46] Wenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep\nconvolutional networks on 3d point clouds. In CVPR, 2019.\n2, 6, 7\n[47] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-\nguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d\nshapenets: A deep representation for volumetric shapes. In\nCVPR, 2015. 5, 6\n[48] Saining Xie, Sainan Liu, Zeyu Chen, and Zhuowen Tu. At-\ntentional shapecontextnet for point cloud recognition. In\nCVPR, 2018. 3, 6, 7\n[49] Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao.\nSpidercnn: Deep learning on point sets with parameterized\nconvolutional filters. In ECCV, 2018. 2, 6, 7\n[50] Jiancheng Yang, Qiang Zhang, Bingbing Ni, Linguo Li,\nJinxian Liu, Mengdie Zhou, and Qi Tian. Modeling point\nclouds with self-attention and gumbel subset sampling. In\nCVPR, 2019. 2, 3, 5, 6, 10\n[51] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell,\nRuslan Salakhutdinov, and Quoc V . Le. XLNet: General-\nized autoregressive pretraining for language understanding.\nIn NeurIPS, 2019. 1, 2, 3\n[52] Li Yi, Vladimir G Kim, Duygu Ceylan, I-Chao Shen,\nMengyan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Shef-\nfer, and Leonidas Guibas. A scalable active framework for\nregion annotation in 3d shape collections. TOG, 2016. 5, 6\n[53] Zhiyuan Zhang, Binh-Son Hua, and Sai-Kit Yeung. Shell-\nnet: Efficient point cloud convolutional neural networks us-\ning concentric shells statistics. In ICCV, 2019. 6, 10\n[54] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring\nself-attention for image recognition. In CVPR, 2020. 1, 2, 3,\n4\n[55] Hengshuang Zhao, Li Jiang, Chi-Wing Fu, and Jiaya Jia.\nPointWeb: Enhancing local neighborhood features for point\ncloud processing. In CVPR, 2019. 2, 6, 10",
  "topic": "Segmentation",
  "concepts": [
    {
      "name": "Segmentation",
      "score": 0.7924355268478394
    },
    {
      "name": "Computer science",
      "score": 0.7790511250495911
    },
    {
      "name": "Transformer",
      "score": 0.685756504535675
    },
    {
      "name": "Point cloud",
      "score": 0.6822967529296875
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6150375008583069
    },
    {
      "name": "Construct (python library)",
      "score": 0.45308223366737366
    },
    {
      "name": "Point (geometry)",
      "score": 0.4483923017978668
    },
    {
      "name": "Computer vision",
      "score": 0.4331444203853607
    },
    {
      "name": "Image segmentation",
      "score": 0.4154791533946991
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3455185890197754
    },
    {
      "name": "Engineering",
      "score": 0.09100115299224854
    },
    {
      "name": "Mathematics",
      "score": 0.07664352655410767
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": []
}