{
  "title": "A Global Past-Future Early Exit Method for Accelerating Inference of Pre-trained Language Models",
  "url": "https://openalex.org/W3170113752",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3171856958",
      "name": "Kaiyuan Liao",
      "affiliations": [
        "Peking University",
        "King University"
      ]
    },
    {
      "id": "https://openalex.org/A1965939310",
      "name": "Yi Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2671201830",
      "name": "Xuancheng Ren",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1960108382",
      "name": "Qi Su",
      "affiliations": [
        "Huawei Technologies (Sweden)",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2107643647",
      "name": "Xu Sun",
      "affiliations": [
        "Peking University",
        "King University"
      ]
    },
    {
      "id": "https://openalex.org/A1968357941",
      "name": "Bin He",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4295315912",
    "https://openalex.org/W3101163004",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3035038672",
    "https://openalex.org/W2944701285",
    "https://openalex.org/W3103884771",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3101248447",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1931877416",
    "https://openalex.org/W2963277051",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2981757109",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W2962677625",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2962957031",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W3115348206",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3034292689",
    "https://openalex.org/W3008374555"
  ],
  "abstract": "Kaiyuan Liao, Yi Zhang, Xuancheng Ren, Qi Su, Xu Sun, Bin He. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 2013‚Äì2023\nJune 6‚Äì11, 2021. ¬©2021 Association for Computational Linguistics\n2013\nA Global Past-Future Early Exit Method for Accelerating Inference\nof Pre-trained Language Models\nKaiyuan Liao‚Ä†‚àó, Yi Zhang‚Ä°‚àó, Xuancheng Ren‚Ä°, Qi Su‚Ä°¬ß, Xu Sun‚Ä†‚Ä°, Bin He‚ãÜ\n‚Ä†Center for Data Science, Peking University\n‚Ä°MOE Key Laboratory of Computational Linguistics, School of EECS, Peking University\n¬ßSchool of Foreign Languages, Peking University\n‚ãÜ Huawei Noah‚Äôs Ark Lab\n{kyliao,zhangyi16,renxc,sukia,xusun}@pku.edu.cn\nhebin.nlp@huawei.com\nAbstract\nEarly exit mechanism aims to accelerate the\ninference speed of large-scale pre-trained lan-\nguage models. The essential idea is to exit\nearly without passing through all the inference\nlayers at the inference stage. To make accu-\nrate predictions for downstream tasks, the hi-\nerarchical linguistic information embedded in\nall layers should be jointly considered. How-\never, much of the research up to now has been\nlimited to use local representations of the exit\nlayer. Such treatment inevitably loses infor-\nmation of the unused past layers as well as\nthe high-level features embedded in future lay-\ners, leading to sub-optimal performance. To\naddress this issue, we propose a novel Past-\nFuture method to make comprehensive pre-\ndictions from a global perspective. We Ô¨Årst\ntake into consideration all the linguistic infor-\nmation embedded in the past layers and fur-\nther engage the future information which is\noriginally inaccessible for predictions. Exten-\nsive experiments demonstrate that our method\noutperforms previous early exit methods by a\nlarge margin, yielding better and robust perfor-\nmance1.\n1 Introduction\nPre-trained language models (PLMs), e.g.,\nBERT (Devlin et al., 2019), RoBERTa (Liu\net al., 2019) and XLNet (Yang et al., 2019), have\nobtained remarkable success in a wide range of\nNLP tasks. Despite their impressive performance,\nPLMs are usually associated with large memory\nrequirement and high computational cost. Such\ndrawbacks slow down the inference and further\nencumber the application of PLMs in the scenarios\nwhere inference time and computation budget are\nrestricted.\nTo address this issue, a growing number of stud-\nies focusing on improving model efÔ¨Åciency have\n‚àóEqual contribution\n1The code is available at https://github.com/\nlancopku/Early-Exit\nemerged recently. Particularly, Kaya et al. (2019)\npoint out that the current over-parameterized mod-\nels conduct excessive computation for simple in-\nstances, which is actually undesirable and compu-\ntationally wasteful. In light of this observation, an\nincreasing amount of work seeks various early exit\nmethods, of which the basic idea is to exit early\nwithout passing through the entire model during\ninference. Concretely, for NLP tasks, they couple\nbranch classiÔ¨Åers with each layer of the pre-trained\nlanguage models and stop forward propagation at\nan intermediate layer. Then the current branch\nclassiÔ¨Åer makes a prediction based on the represen-\ntation of the token that is used as the aggregated\nsequence representation for classiÔ¨Åcation tasks and\nis referred to as the state of the layer in this work.\nHowever, existing work on early exit has two\nmajor drawbacks. First, existing work (Xin et al.,\n2020; Zhou et al., 2020) uses only local states in the\nearly exit framework. They inevitably lose valu-\nable features that are captured by passed layers\nbut are ignored for prediction, leading to less reli-\nable prediction results. Moreover, these methods\nabandon the potentially useful features captured by\nthe future layers that have not been passed, which\nmay hurt the performance of the instances requir-\ning high-level features embedded in the deep layers.\nConsequently, their performance dramatically de-\nclines when the inference exits earlier for a higher\nspeed-up ratio.\nThese two major drawbacks hinder the progress\nof early exit research and motivate us to develop\na new mechanism using the hierarchical linguis-\ntic information embedded in all layers (Jawahar\net al., 2019) from a global perspective. However,\nup to now, a global early exit mechanism remains\na under-explored challenging problem. We extend\nthe existing methods to their corresponding global\nversions and Ô¨Ånd that naive global strategies only\nresult in poor performance. Meanwhile, the future\nstates are originally inaccessible in the early exit\n2014\n‚Ä¶\nLayer 1\nLayer 6\n‚Ä¶\n‚Ä¶\nLayer 12\n‚Ä¶\nLayer 7\n0  1 2\n‚úò\nPrediction\nInput\n0.81\nùëÜùëÜ6\nùëÜùëÜ1\n(a) DeeBERT with local early exit\nmethod\n‚Ä¶\nLayer 1\nLayer 6\n‚Ä¶\n‚Ä¶\nLayer 12\n‚Ä¶\nLayer 7\nùëÜùëÜ6\nùë†ùë†ùëùùëù\n0 1  2\n‚úî\nPrediction\nùëÜùëÜ1\nInput\n0.63\n(b) The Global Past version of our early\nexit method\n‚Ä¶\n0 1  2\n‚úî\nLayer 1\nLayer 6\n‚Ä¶\n‚Ä¶\nùëÜùëÜ1\nLayer 12\n‚Ä¶\nLayer 7\nùëÜùëÜ6\n‚Ä¶\nùëÜùëÜ7\nùëÜùëÜ12\nùë†ùë†ùëìùëì\nùë†ùë†ùëùùëù\nPrediction\nInput\n0.85\ns\n(c) The Global Past-Future version of\nour early exit method\nFigure 1: Comparison of the local early exit method and our proposed method. The red rectangles highlight the\nstates that the models rely on to make predictions.\nframework, which also remains a bottleneck for a\nglobal prediction considering both past and future\nstates.\nIn this paper, we focus on the aforementioned\nproblems and Ô¨Årst put into practice a global Past-\nFuture early exit mechanism. The term global is\ntwo-fold: (1) instead of using one or several lo-\ncal state(s) for prediction in previous work, all the\navailable past states are effectively incorporated in\nour method; (2) furthermore, to grasp the features\nembedded in the deep layers, the originally inacces-\nsible future states are approximated by imitation\nlearning and are also engaged for prediction. The\ncomparison of the previous method and our method\nis illustrated in Figure 1. By combining both past\nand future states, our model is able to make more\naccurate predictions for downstream tasks.\nExtensive experiments reveal that the proposal\nsigniÔ¨Åcantly outperforms previous early exit meth-\nods. Particularly, it surpasses the previous methods\nby a large margin when the speed-up ratio is rel-\natively high. In addition, extensive experiments\nwith different pre-trained language models as back-\nbones demonstrate consistent improvement over\nthe baseline methods, which veriÔ¨Åes the generality\nof our method.\nTo summarize, our contributions are as follows:\n‚Ä¢We propose a set of global strategies which\neffectively incorporate all available states and\nthey achieve better performance compared to\nthe existing naive global strategies.\n‚Ä¢Our early exit method Ô¨Årst utilizes the future\nstates which are originally inaccessible at the\ninference stage, enabling more comprehensive\nglobal predictions.\n‚Ä¢Experiments show that our proposal achieves\nbetter performance compared to the previous\nstate-of-the-art early exit methods.\n2 Related Work\nLarge-scale pre-trained language models (Devlin\net al., 2019; Liu et al., 2019) based on the Trans-\nformer (Vaswani et al., 2017) architecture demon-\nstrate superior performance in various NLP tasks.\nHowever, the impressive performance is on the\nbasis of massive parameters, leading to large mem-\nory requirement and computational cost during in-\nference. To overcome this bottleneck, increasing\nstudies work on improving the efÔ¨Åciency of over-\nparameterized pre-trained language models.\nKnowledge distillation (Hinton et al., 2015; Turc\net al., 2019; Jiao et al., 2019; Li et al., 2020a) com-\npacts the model architecture to obtain a smaller\nmodel that remains static for all instances at the\ninference stage. Sanh et al. (2019) focus on reduc-\ning the number of layers since their investigation\nreveals variations on hidden size dimension have\na smaller impact on computation efÔ¨Åciency. Sun\net al. (2019) learn from multiple intermediate lay-\ners of the teacher model for incremental knowledge\nextraction instead of only learning from the last hid-\nden representations. Further, Wang et al. (2020) de-\nsign elaborate techniques to drive the student model\nto mimic the self-attention module of teacher mod-\nels. Xu et al. (2020) compress model by progres-\nsive module replacing, showing a new perspective\nof model compression. However, these static model\ncompression methods treat the instances requiring\ndifferent computational cost without distinction.\nMoreover, they have to distill a model from scratch\nto meet the varying speed-up ratio requirements.\n2015\nTo meet different constraints for acceleration, an-\nother line of work studies instance-adaptive meth-\nods to adjust the number of executed layers for\ndifferent instances. Li et al. (2020b) select mod-\nels in different sizes depending on the difÔ¨Åculty\nof input instance. Besides, early exit is a practi-\ncal method to adaptively accelerate inference and\nis Ô¨Årst proposed for computer vision tasks (Kaya\net al., 2019; Teerapittayanon et al., 2016). El-\nbayad et al. (2020); Xin et al. (2020); Schwartz\net al. (2020) follow the essential idea and leverage\nthe method in NLP tasks. To prevent the error from\none single classiÔ¨Åer, Zhou et al. (2020) make the\nmodel stop inference when a cross-layer consistent\nprediction is achieved. However, researches on the\nsubject has been mostly restricted to only use the\nlocal states around the exit layer.\n3 Method\nWe Ô¨Årst introduce the strategies to incorporate mul-\ntiple states and the imitation learning method for\ngenerating approximations of future states. Then\nwe introduce the merging gate to adaptively fuse\npast and future states. At last, we show the training\nprocess and the exit condition during inference.\n3.1 Incorporation of Past States\nExisting work (Xin et al., 2020) focuses on making\nexit decision based on a single branch classiÔ¨Åer.\nThe consequent unreliable result motivates the re-\ncent advance (Zhou et al., 2020) that uses consecu-\ntive states to improve the accuracy and robustness.\nHowever, the model prediction is still limited to\nuse several local states. In contrast, we investigate\nhow to incorporate all the past states from a global\nperspective. The existing strategy using consecu-\ntive consistent prediction labels can be easily ex-\ntended to a global version that counts the majority\nof the predicted labels which is regarded as a voting\nstrategy. Another alternative is the commonly-used\nensemble strategy that averages the output probabil-\nities for prediction. Besides these naive solutions,\nwe explore the following strategies to integrate mul-\ntiple states into a single one:\n‚Ä¢Max-Pooling: The max-pooling operation is\nperformed on all available states, resulting in\nthe integrated state.\n‚Ä¢Avg-Pooling: The average-pooling operation\nis performed on all available states, resulting\nin the integrated state.\n‚Ä¢Attn-Pooling: The attentive-pooling takes the\nweighted summation of all available states as\nthe integrated state. The attention weights are\ncomputed with the last state as the query.\n‚Ä¢Concatenation: All available states are con-\ncatenated and then fed into a linear transfor-\nmation layer to obtain the compressed state.\n‚Ä¢Sequential Neural Network: All available\nstates are sequentially fed into an LSTM and\nthe hidden output of the last time-step is re-\ngarded as the integrated state.\nFormally, the state of the i-th layer is denoted\nas si. When forward propagation proceeds to the\ni-th intermediate layer, all the past states s1:i are\nincorporated into a global past state sp:\nsp = G(s1:i) (1)\nwhere G(¬∑) refers to one of the state incorporation\nstrategies.\n3.2 Imitation of Future States\nExisting work for early exit stops inference at\nan intermediate layer and ignores the underlying\nvaluable features captured by the future layers.\nSuch treatment is partly rationalized by the recent\nclaim (Kaya et al., 2019) that shallow layers are\nadequate to make a correct prediction. However,\nJawahar et al. (2019) reveal that the pre-trained\nlanguage models capture a hierarchy of linguis-\ntic information from the lower to the upper layers,\ne.g., the lower layers learn the surface or syntactic\nfeatures while the upper layers capture high-level\ninformation like the semantic features. We hypoth-\nesize that some instances not only rely on syntactic\nfeatures but also require semantic features. It is\nactually undesirable to only consider features cap-\ntured by shallow layers. Therefore, we propose to\ntake advantage of both past and future states.\nNormally, we can directly fetch the past states,\nwhile using future information is intractable how\nsince the future states are inaccessible before pass-\ning through the future layers. To bridge this gap,\nwe propose a simple method to approximate the fu-\nture states in light of imitation learning (Ross et al.,\n2011; Nguyen, 2016; Ho and Ermon, 2016). We\ncouple each layer with an imitation learner. Dur-\ning training, the imitation learner is encouraged to\nmimic the representation of the real state of that\nlayer. Through this layer-wise imitation, we can\n2016\nInput\nLayer 1\nLayer m\nLayer N\nLayer m+1\n‚Ä¶\n‚Ä¶ Future\nInfo\nPast \nInfo\n‚Ä¶\nùë†ùë†1ùë†ùë†1\nùë†ùë†ùëöùëö ùë†ùë†ùëöùëö\nÃÇùë†ùë†ùëöùëö+1\nÃÇùë†ùë†ùëÅùëÅ\n‚Ä¶\nùêøùêøùëöùëö\n‚Ä¶\nùêøùêøùëöùëö+1\nùêøùêøùëÅùëÅ\n‚Ä¶\nComputed \nLayers\nUncomputed\nLayers\nFigure 2: The illustration of the future imitation learn-\ning. mis the exit layer and the dashed line denotes the\nunused modules at inference stage.\nobtain approximations of the future states with min-\nimum cost. The illustration of the future imitation\nlearning during inference is shown in Figure 2.\nTo be precise, we intend to obtain a state approx-\nimation of the j-th layer if the forward pass exits\nat the intermediate i-th layer for any j >i. During\ntraining, we pass through the entire n-layer model\nbut we simulate the situation that the forward pass\nends up at the i-th layer for any i< n. The j-th\nlearner corresponding to the j-th layer takes si as\ninput and outputs an approximation ÀÜsi\nj of the real\nstate sj. Then sj serves as a teacher to guide the j-\nth imitation learner. We adopt cosine similarity as\nthe distance measurement and penalize the discrep-\nancy between the real state sj and the learned state\nÀÜsi\nj. Let Li\ncos denotes the imitation loss of the situa-\ntion that the forward pass exits at thei-th layer, it is\ncomputed as the average of the similarity loss for\nany j> i. Since the exit layer ican be any number\nbetween 2 to nduring inference, we go through all\npossible number iand average the corresponding\nLi\ncos, resulting the overall loss Lcos:\nÀÜsi\nj = Learnerj(si) (2)\nli,j\ncos(sj,ÀÜsi\nj) = 1‚àí\nÀÜsi\nj ¬∑sj\n‚à•ÀÜsi\nj‚à•‚à•sj‚à• (3)\nLi\ncos = 1\nn‚àíi\n‚àën\nj=i+1\nli,j\ncos(sj,ÀÜsi\nj) (4)\nLcos = 1\nn‚àí1\n‚àën\ni=2\nLi\ncos (5)\nwhere ‚à•¬∑‚à• denotes the L2 norm. Learnerj(¬∑) is a\nsimple feed-forward layer with learnable parame-\nters W i and bi.\nDuring training, the forward propagation is com-\nputed on all layers and all imitation learners are\nencouraged to generate representations close to the\nreal states. During inference, the forward propaga-\ntion proceeds to the i-th intermediate layer and the\nsubsequent imitation learners take thei-th real state\nas input to generate the approximations of future\nstates. Then the approximations are incorporated\ninto a comprehensive future state sf with one of\nthe global strategies introduced before:\nsf = G(ÀÜsi\ni+1:n) (6)\nwhere ÀÜsi\ni+1:n denotes the approximations of the\nstates from the (i+1)-th layer to the n-th layer.\n3.3 Adaptive Merging Gate\nWe then explore how to adaptively merge the past\ninformation and future information. Intuitively, the\npast state sp and the future state sf are of different\nimportance since the authentic past states are more\nreliable than our imitated future states. In addition,\ndifferent instances depend differently on high-level\nfeatures learned by future layers. Therefore, it is\nindispensable to develop an adaptive method to au-\ntomatically combine the past statesp and the future\nstate sf . In our work, we design an adaptive merg-\ning gate to automatically fuse the past state sp and\nthe future state sf . As the forward propagation pro-\nceeds to the i-th layer, we compute the reliability\nof the past state sp, and the Ô¨Ånal merged represen-\ntation is a trade-off between these two states:\nŒ±= sigmoid(FFN(sp)) (7)\nzi = Œ±sp + (1‚àíŒ±)sf (8)\nwhere zi is the merged Ô¨Ånal state and FFN(¬∑) is a\nlinear feed forward layer of the merging gate.\nDuring training, each layer can generate the ap-\nproximated states of future and obtain a merged\nÔ¨Ånal state which is used for prediction. Then the\nmodel will be updated with the layer-wise cross-\nentropy loss against the ground-truth label y. The\nmerging gate adaptively learns to adjust the balance\nunder the supervision signal given by ground-truth\nlabels. However, with the layer-wise optimization\nobjectives, the shallow layers will be updated more\nfrequently since they receive more updating signals\nfrom higher layers. To address this issue, we heuris-\ntically re-weight the cross entropy loss of each layer\n2017\ndepending on its depth iand get its weight wi. The\nupdating procedure is formalized as:\nwi = i‚àën\nj=1 j (9)\npi = softmax(zi) (10)\nLi\nce = ‚àí\n‚àë\nl‚ààlabels\ny(l)log(pi(l)) (11)\nLce =\n‚àën\ni=1\nwiLi\nce (12)\nThe overall loss is computed as follows:\nL= Lce + Lcos (13)\n3.4 Fine-tuning and Inference\nHere we introduce the Ô¨Åne-tuning technique and\nthe exit condition at the inference stage.\nFine-tuning The representations learned by shal-\nlow layers have a big impact on performance in the\nearly exit framework since the prediction largely\ndepends on the states of shallow layers. Most\nexisting work updates all of the model layers at\neach step during Ô¨Åne-tuning to adapt to the data of\ndownstream tasks. However, we argue that such\nan aggressive updating strategy may undermine\nthe well-generalized features learned in the pre-\ntraining stage. In our work, we try to balance the\nrequirements of maintaining features learned in\npre-training and adapting to data at the Ô¨Åne-tuning\nstage. SpeciÔ¨Åcally, the parameters of a layer will\nbe frozen with a probability pand the probability\nplinearly decreases from the Ô¨Årst layer to the L-th\nlayer in a range of 1 to 0.\nInference Following Xin et al. (2020), we quan-\ntify the prediction conÔ¨Ådence ewith the entropy of\nthe output distribution pi of i-th layer:\ne(pi) =Entropy(pi) (14)\nThe inference stops once the conÔ¨Ådence e(pi) is\nlower than a predeÔ¨Åned threshold œÑ. The hyper-\nparameter œÑ is adjusted according to the required\nspeed-up ratios. If the exit condition is never\nreached, our model degrades into the common case\nof inference that the complete forward propagation\nis accomplished.\n4 Experiments\n4.1 Experimental Setup\nExperimental Settings Following previous\nwork (Xin et al., 2020), we evaluate our proposed\nmethod on six classiÔ¨Åcation datasets from the\nGLUE benchmark (Wang et al., 2019): SST-2,\nMRPC, QNLI, RTE, QQP, and MNLI. We perform\na grid search over the sets of learning rate as {1e-5,\n2e-5, 3e-5, 5e-5}, batch size as {16, 32, 128} and\nnumber of frozen layers during Ô¨Åne-tuning as\n{0,1,2,3}. The maximum sequence length is Ô¨Åxed\nto 128. We employ a linear decay learning rate\nscheduler and the AdamW optimizer. In addition,\nwe use the concatenation strategy to incorporate\nall available states for its best performance on the\nGLUE dev set.\nSpeed Measurement Since the measurement of\nruntime might not be stable, following Xin et al.\n(2020); Zhou et al. (2020), we manually adjust the\nexit threshold œÑ and calculate the speed-up ratio by\ncomparing the actually executed layers in forward\npropagation and the required complete layers. For\na n-layer model, the speed-up ratio is:\nspeed-up ratio =\n‚àën\ni=1 n‚àómi\n‚àën\ni=1 i‚àómi (15)\nwhere mi is the number of examples that exit at the\ni-th layer of the model.\n4.2 Baselines\nThe proposed method can be practical for a range of\nexisting pre-trained language models. Without los-\ning generality, we conduct experiments with several\nwell-known PLMs as backbones, namely, BERT,\nRoBERTa, and ALBERT (Lan et al., 2019). Both\nBERT and RoBERTa suffer from the problem of\nover-parameterization. ALBERT largely alleviates\nthis problem and is very efÔ¨Åcient in terms of model\nsize, the results on which verify the effectiveness\non such parameter-efÔ¨Åcient models. We mainly\ncompare our method with other methods targeting\non reducing the depth of models, including the re-\ncent early exit methods and the method directly\nreducing model depth to mlayers which is denoted\nas (AL)BERT-mL.\n4.3 Overall Comparison\nWe compare our model performance with the base-\nline methods when different backbone models are\nadopted and show the result in Table 1 and Ta-\nble 2. Both PABEE (Zhou et al., 2020) and Dee-\nBERT (Xin et al., 2020) accelerate inference with\na highest 2√óspeed-up ratio. To be consistent, we\nadjust the exit threshold to obtain a2√óspeed-up ra-\ntio and report the results in Table 1. As shown, our\n2018\nModel MNLI-mMNLI-mm QQP QNLI SST-2 MRPC RTE MacroAcc Spd-upAcc Spd-upF1/Acc Spd-upAcc Spd-upAcc Spd-upF1/Acc Spd-upAcc Spd-up\nBERT\nBERT-base (Devlin et al., 2019)84.6 1.00√ó83.4 1.00√ó71.2/ - 1.00√ó90.5 1.00√ó93.5 1.00√ó88.9/ - 1.00√ó66.4 1.00√ó -\nBERT-6L 80.8 2.00√ó79.9 2.00√ó69.7/88.3 2.00√ó86.7 2.00√ó91.0 2.00√ó85.1/78.6 2.00√ó63.9 2.00√ó 80.5DeeBERT (Xin et al., 2020)- - - 69.4/ - 1.96√ó87.9 1.79√ó91.5 1.89√ó85.2/ - 1.79√ó - - -DeeBERT 74.4 1.87√ó73.1 1.88√ó70.4/88.8 2.13√ó85.6 2.09√ó90.2 2.00√ó84.4/77.4 2.07√ó64.3 1.95√ó 74.7PABEE 79.8 2.07√ó78.7 2.08√ó70.4/88.6 2.09√ó88.0 1.87√ó89.3 1.95√ó84.4/77.4 2.01√ó64.0 1.81√ó 80.0Ours 83.31.96√ó82.71.96√ó71.2/89.42.18√ó89.81.97√ó92.82.02√ó87.0/81.81.98√ó64.52.04√ó 82.5\nRoBERTa\nRoBERTa-base (Xin et al., 2020)87.0 1.00√ó86.3 1.00√ó71.8/ - 1.00√ó92.4 1.00√ó94.3 1.00√ó90.4/ - 1.00√ó67.5 1.00√ó -\nRoBERTa-6L 84.4 2.00√ó83.4 2.00√ó71.6/89.2 2.00√ó90.4 2.00√ó93.5 2.00√ó89.3/85.52.00√ó58.0 2.00√ó 82.5DeeBERT 64.2 1.87√ó64.7 1.87√ó72.0/89.32.05√ó83.8 2.01√ó86.9 2.02√ó88.7/84.3 1.86√ó60.81.90√ó 75.4Ours 86.61.92√ó86.21.93√ó72.0/89.32.54√ó91.72.11√ó94.51.98√ó89.3/85.51.95√ó58.0 2.11√ó 83.6\nALBERT\nALBERT-base 85.2 1.00√ó84.7 1.00√ó70.5/88.7 1.00√ó92.0 1.00√ó93.3 1.00√ó89.0/84.8 1.00√ó72.0 1.00√ó 84.8\nALBERT-6L 82.4 2.00√ó81.7 2.00√ó69.8/88.3 2.00√ó90.0 2.00√ó91.8 2.00√ó87.0/82.4 2.00√ó65.8 2.00√ó 82.2PABEE 84.2 1.90√ó83.5 1.81√ó70.7/88.92.11√ó90.9 1.98√ó92.4 1.80√ó87.6/82.6 1.91√ó66.8 2.06√ó 83.2Ours 84.81.94√ó84.11.95√ó70.4/88.6 2.35√ó91.91.97√ó92.82.13√ó88.3/84.61.95√ó72.01.93√ó 84.5\nTable 1: Model performance on the GLUE test set with different PLMs as backbone. The speed-up ratio (Spd-up)\nis approximately 2.00 √óand our method signiÔ¨Åcantly outperforms previous early exit methods.\nModel MNLI-mMNLI-mm QQP QNLI SST-2 MRPC RTE MacroAcc Spd-upAcc Spd-upF1/Acc Spd-upAcc Spd-upAcc Spd-upF1/Acc Spd-upAcc Spd-up\nBERT\nBERT-base (Devlin et al., 2019)84.6 1.00√ó83.4 1.00√ó71.2/ - 1.00√ó90.5 1.00√ó93.5 1.00√ó88.9/ - 1.00√ó66.4 1.00√ó -\nBERT-4L 77.6 3.00√ó77.2 3.00√ó67.7/87.5 3.00√ó85.4 3.00√ó88.7 3.00√ó82.9/74.9 3.00√ó63.03.00√ó 78.4DeeBERT 61.0 2.80√ó59.8 2.84√ó66.1/86.9 3.19√ó80.8 2.88√ó84.7 2.71√ó83.5/75.5 2.61√ó60.5 2.90√ó 71.8PABEE 75.9 2.70√ó75.3 2.71√ó69.5/88.2 2.57√ó82.6 3.04√ó85.2 3.15√ó82.6/73.1 2.72√ó60.5 2.38√ó 76.6Ours 78.42.99√ó77.43.02√ó70.4/89.23.16√ó87.32.78√ó91.12.97√ó84.5/77.72.87√ó63.02.88√ó 79.7\nRoBERTa\nRoBERTa-base (Xin et al., 2020)87.0 1.00√ó86.3 1.00√ó71.8/ - 1.00√ó92.4 1.00√ó94.3 1.00√ó90.4/ - 1.00√ó67.5 1.00√ó -\nRoBERTa-4L 80.3 3.00√ó79.6 3.00√ó69.8/88.4 3.00√ó86.0 3.00√ó91.3 3.00√ó85.0/78.1 3.00√ó53.2 3.00√ó 80.2DeeBERT 55.1 2.31√ó56.6 2.27√ó67.1/88.1 3.24√ó76.0 2.82√ó72.3 2.67√ó85.9/79.4 2.87√ó - - -Ours 81.4 2.97√ó80.5 3.02√ó71.9/89.3 3.12√ó89.2 2.83√ó93.5 2.67√ó87.1/82.2 2.75√ó54.1 3.01√ó 80.6\nALBERT\nALBERT-base 85.2 1.00√ó84.7 1.00√ó70.5/88.7 1.00√ó92.0 1.00√ó93.3 1.00√ó89.0/84.8 1.00√ó72.0 1.00√ó 84.8\nALBERT-4L 80.1 3.00√ó79.2 3.00√ó68.9/88.1 3.00√ó87.6 3.00√ó89.5 3.00√ó84.4/78.9 3.00√ó61.2 3.00√ó 79.7PABEE 79.6 2.95√ó78.9 2.96√ó70.8/88.82.61√ó87.9 3.25√ó91.9 2.64√ó83.6/75.1 2.66√ó64.6 2.69√ó 80.3Ours 82.52.93√ó82.02.95√ó70.3/88.6 3.17√ó91.02.92√ó92.52.88√ó87.6/82.82.72√ó68.12.92√ó 83.0\nTable 2: Model performance on the GLUE test set with different PLMs as backbone. The speed-up ratio (Spd-up)\nis approximately 3.00√óand our method signiÔ¨Åcantly outperforms previous early exit methods.\nmethod maintains a comparable result with the orig-\ninal models on most datasets. We also notice that\ndirectly reducing layers performs well and serves\nas a strong baseline. Nevertheless, our proposal\nsigniÔ¨Åcantly outperforms such a method as well as\nthe other two early exit methods.\nWe then adopt a more aggressive 3.00√óspeed-\nup ratio to verify the effectiveness of our method.\nAccording to Table 2, the performance of PABEE\nand DeeBERT deteriorates badly. In contrast, our\nmodel exhibits more robust and stable performance,\nshowing its superiority over previous early exit\nmethods. Particularly, ALBERT is already very\nefÔ¨Åcient in model size owing to its layer-sharing\nmechanism. Results shown in the bottom of Table 2\nsuggest that our model can obtain a good result with\nminimum performance loss on such a parameter-\nefÔ¨Åcient model.\nThe success of our proposal might be attributed\nto the global perspective for prediction. DeeBERT\nmakes prediction with the help of the state of a sin-\ngle branch classiÔ¨Åer, leading to less reliable results.\nAlthough PABEE employs cross-layer prediction\nto prevent error from one single classiÔ¨Åer, they ig-\nnore much available information of past states as\nwell as the high-level semantic features captured\nby future layers. Different from those methods, our\nmethod jointly takes into consideration the hierar-\nchical linguistic information embedded in all layers\nand thus is able to produce more accurate results.\n4.4 Performance-EfÔ¨Åciency Trade-Off\nTo further verify the robustness and efÔ¨Åciency\nof our method, we visualize the performance-\nefÔ¨Åciency trade-off curves in Figure 3 on a represen-\ntative subset of the GLUE dev set. The backbone\n2019\n(a) MNLI\n (b) QQP\n (c) QNLI\n (d) SST-2\nFigure 3: Performance and efÔ¨Åciency trade-off for early exit methods with BERT as backbone. Our method\noutperforms previous early exit methods by a large margin especially under high speed-up ratios.\nMethod MNLI-mQNLISST-2 MRPC MacroAcc Acc Acc F1/Acc\nNaive global strategies\nV oting 71.33 87.19 89.5687.71/82.3583.28\nEnsemble 70.92 87.85 90.3787.44/81.1383.41\nOur global strategies\nAvg-Pooling81.11 90.43 92.4388.48/83.0987.44\nMax-Pooling82.86 90.18 92.3287.85/82.1187.59\nSequentialNN82.52 90.17 92.0989.35/85.0588.00\nAttn-Pooling83.02 90.37 93.0087.83/81.8687.81\nConcatenation83.30 90.46 92.8988.44/83.0888.10\nTable 3: The performance of different strategies to in-\ncorporate multiple states on the GLUE dev set. The\nspeed-up ratio is approximately 2.00√ó(¬±4%).\nmodel is BERT. Please refer to the Appendix A for\nresults of RoBERTa and ALBERT. As can be seen\nfrom Figure 3, the performance of previous state-\nof-the-art early exit methods drops dramatically\nwhen the speed-up ratio increases, which limits\ntheir practicality for higher acceleration require-\nments. By comparison, our method demonstrates\nmore tolerance of speed-up ratio. It signiÔ¨Åcantly\nimproves performance compared to previous best-\nperforming early exit models under the same speed-\nup ratio, especially in the case that the speed-up\nratio is high, indicating that it can be applied in a\nwider range of acceleration scenarios.\n4.5 Analysis\n4.5.1 Effect of Global Strategies\nThe results of different global strategies on a rep-\nresentative subset of GLUE dev are shown in Ta-\nble 3. The naive global strategies including voting\nand ensemble perform poorly, which demonstrates\nthat existing global strategies can only achieve sub-\noptimal performance. In contrast, we design simple\nyet effective global strategies to incorporate past\nstates which bring signiÔ¨Åcant improvement com-\npared to baselines. In addition, we empirically Ô¨Ånd\nthat the concatenation strategy works best from\nan overall point of view. We assume that such a\nstrategy allows interaction among different states,\nyielding better performance. In addition, the effect\nof the merging gate can be found in Appendix B.\n4.5.2 Analysis of Future Information\nTo assess whether and how future information con-\ntributes to the prediction, we Ô¨Årst evaluate the\nGlobal Future version of our early exit method\nwhere all the approximations of futures states are\nincorporated through the concatenation strategy.\nEffect of future information is backed with the re-\nsults shown in Table 4. We observe that the Global\nFuture mechanism brings improvement on most\ndatasets for both 2√óspeed-up ratio and 3√óspeed-\nup ratio, which conÔ¨Årms that the approximations\nof future states help enhance the model ability in\nprediction. Beyond that, the future states can be es-\npecially advantageous for the models with a higher\nspeed-up ratio. Recall that approximations of fu-\nture states complement the high-level semantic in-\nformation and the exit at shallow layers loses more\nsemantic information in comparison with the exit\nat deep layers. Therefore, the beneÔ¨Åt of future in-\nformation is more signiÔ¨Åcant compared to the exit\nat shallow layers, which is validated by the larger\nimprovement gap with a 3√óspeed-up ratio.\nWe also investigate the effect of future informa-\ntion on exit time. Figure 4 demonstrates the distri-\nbution of exit layers with and without future infor-\nmation. When future information is engaged, we\nobserve that the proportion of exit at shallow lay-\ners increases. The observation conforms with our\nintuition: with the approximations of future states\nsupplemented for prediction, the merged state at\na shallow layer is able to make a conÔ¨Ådent and\ncorrect prediction. Thus the exit time is earlier\ncompared to situations without future states, result-\n2020\nModel MNLI-m MNLI-mm QQP QNLI SST-2 MRPC RTE MacroAcc Spd-upAcc Spd-upF1/Acc Spd-upAcc Spd-upAcc Spd-upF1/Acc Spd-upAcc Spd-up\n‚âà2.00√óspeed-up\nBERT-local81.97 1.96√ó82.47 1.96√ó88.18/91.21 1.85√ó89.90 2.00√ó92.09 2.00√ó86.84/80.39 2.08√ó66.43 1.96√ó 83.73+Global Future82.14 2.04√ó82.93 1.96√ó88.01/91.12 1.89√ó90.04 2.04√ó92.32 2.08√ó87.19/80.64 2.08√ó66.78 1.92√ó 83.96\n‚âà3.00√óspeed-up\nBERT-local76.72 2.78√ó77.64 2.78√ó85.80/89.53 3.03√ó86.62 2.86√ó90.25 2.94√ó85.35/77.45 2.94√ó62.82 2.86√ó 80.45+Global Future79.06 2.70√ó78.86 2.70√ó85.65/89.61 3.03√ó86.93 2.86√ó91.40 2.94√ó86.12/78.43 2.86√ó62.45 2.86√ó 81.23\nTable 4: Effect of the approximated future states. BERT-local denotes the early exit method using only current\nstate and Global Future represents the incorporation of future states. Results are on the GLUE dev set.\n(a) MNLI\n (b) MRPC\nFigure 4: The distribution of exit layers with and with-\nout future states on the MNLI and MRPC tasks. The\nexit threshold for the same task is Ô¨Åxed. When fu-\nture states are engaged for prediction, we observe an\nincrease of exit at shallow (1-4) layers as well as a per-\nformance boost.\ning in a higher speed-up ratio. To be more speciÔ¨Åc,\nfor MRPC, the speed-up ratios with and without fu-\nture states are 1.69 and 1.99, and are 1.92 and 2.04\nfor MNLI, respectively. Meanwhile, we observe a\nperformance boost with future states involved. It\nconÔ¨Årms our assumption that the high-level seman-\ntic features embedded in future states help improve\nperformance in early exit framework.\n4.5.3 Comparison with Distillation Methods\nAs an alternative method to accelerate inference,\nknowledge distillation also exhibits promising per-\nformance for NLP tasks. We provide comparison\nwith typical knowledge distillation methods in Ta-\nble 5. Existing model TinyBERT (Jiao et al., 2019)\nexerts multiple elaborate strategies to achieve the\nstate-of-the-art results, including the expensive gen-\neral distillation process and a vast amount of aug-\nmented data for Ô¨Åne-tuning. We remove these two\ntechniques to exclude the effect of extra training\ndata. Under the same settings, we observe that our\nmethod outperforms the distillation methods with\nthe same speed-up ratio.\nIn general, early exit and distillation methods im-\nprove inference efÔ¨Åciency from different perspec-\nMethod MNLI-mQQPQNLISST-2MacroAcc F1/AccAcc Acc\nDistilBERT 81.9 70.0/88.488.2 92.1 85.4BERT-PKD (Sun et al., 2019)81.5 70.7/88.989.0 92.0 85.6PD-BERT (Turc et al., 2019)82.8 70.4/88.988.9 91.8 85.8BERT-of-Theseus (Xu et al., 2020)82.4 71.6/89.389.6 92.2 86.2TinyBERT‚Ä° 81.9 70.0/88.688.6 92.0 85.5\nOurs 83.3 71.2/89.489.8 92.8 86.6\nTable 5: Comparison with distillation methods on the\nGLUE test set. TinyBERT‚Ä°is our implementation that\nremoves general distillation and additional Ô¨Åne-tuning\nresources to match the settings of other methods. The\nspeed-up ratio is approximately 2.00√ó(¬±4%).\ntives. The distillation methods are more efÔ¨Åcient in\nsaving memory usage, but the downside is that such\nstatic methods suffer from high computation cost\nto adapt to different speed-up ratios. A new student\nmodel has to be trained from scratch if the speed-\nup requirement changes. By contrast, dynamic\nmethods are more Ô¨Çexible to meet different accel-\neration requirements. Concretely, simple instances\nwill be processed by passing through fewer layers\nand complex instances may require more layers.\nMoreover, the speed-up ratio can be easily adjusted\ndepending on the acceleration requests. Neverthe-\nless, early exit and distillation accelerate inference\nfrom different perspectives and these two kinds of\ntechniques can be integrated to further compress\nthe model size and accelerate the inference time.\n5 Conclusions\nWe propose a novel Past-Future early exit method\nfrom a global perspective. Unlike previous work\nusing only local states for prediction, our model em-\nploys all available past states for prediction and pro-\npose a novel approach to engage the future states\nwhich are originally inaccessible for prediction. Ex-\nperiments illustrate that our method achieves sig-\nniÔ¨Åcant improvement over baseline methods with\ndifferent models as backbones, suggesting the su-\nperiority of our early exit method.\n2021\nAcknowledgements\nWe thank all the anonymous reviewers for their\nconstructive comments. This work is partly sup-\nported by National Key R&D Program of China\nNo. 2019YFC1521200 and Beijing Academy of\nArtiÔ¨Åcial Intelligence (BAAI). Xu Sun is the corre-\nsponding author.\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171‚Äì4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMaha Elbayad, Jiatao Gu, Edouard Grave, and Michael\nAuli. 2020. Depth-adaptive transformer. In 8th\nInternational Conference on Learning Representa-\ntions, ICLR 2020, Addis Ababa, Ethiopia, April 26-\n30, 2020. OpenReview.net.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.\n2015. Distilling the knowledge in a neural network.\nCoRR, abs/1503.02531.\nJonathan Ho and Stefano Ermon. 2016. Generative ad-\nversarial imitation learning. In Advances in Neu-\nral Information Processing Systems 29: Annual\nConference on Neural Information Processing Sys-\ntems 2016, December 5-10, 2016, Barcelona, Spain,\npages 4565‚Äì4573.\nGanesh Jawahar, Beno√Æt Sagot, and Djam√© Seddah.\n2019. What does BERT learn about the structure of\nlanguage? In Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 3651‚Äì3657. Association\nfor Computational Linguistics.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2019. Tinybert: Distilling BERT for natural lan-\nguage understanding. CoRR, abs/1909.10351.\nYigitcan Kaya, Sanghyun Hong, and Tudor Dumitras.\n2019. Shallow-deep networks: Understanding and\nmitigating network overthinking. In Proceedings\nof the 36th International Conference on Machine\nLearning, ICML 2019, 9-15 June 2019, Long Beach,\nCalifornia, USA, volume 97 of Proceedings of Ma-\nchine Learning Research, pages 3301‚Äì3310. PMLR.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Sori-\ncut. 2019. ALBERT: A lite BERT for self-\nsupervised learning of language representations.\nCoRR, abs/1909.11942.\nJianquan Li, Xiaokang Liu, Honghong Zhao, Ruifeng\nXu, Min Yang, and Yaohong Jin. 2020a. BERT-\nEMD: many-to-many layer mapping for BERT com-\npression with earth mover‚Äôs distance. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2020, On-\nline, November 16-20, 2020 , pages 3009‚Äì3018. As-\nsociation for Computational Linguistics.\nLei Li, Yankai Lin, Shuhuai Ren, Deli Chen, Xu-\nancheng Ren, Peng Li, Jie Zhou, and Xu Sun. 2020b.\nAccelerating pre-trained language models via cali-\nbrated cascade. CoRR, abs/2012.14682.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nKhanh Nguyen. 2016. Imitation learning with recur-\nrent neural networks. CoRR, abs/1607.05241.\nSt√©phane Ross, Geoffrey J. Gordon, and Drew Bagnell.\n2011. A reduction of imitation learning and struc-\ntured prediction to no-regret online learning. In Pro-\nceedings of the Fourteenth International Conference\non ArtiÔ¨Åcial Intelligence and Statistics, AISTATS\n2011, Fort Lauderdale, USA, April 11-13, 2011 ,\nvolume 15 of JMLR Proceedings, pages 627‚Äì635.\nJMLR.org.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof BERT: smaller, faster, cheaper and lighter.CoRR,\nabs/1910.01108.\nRoy Schwartz, Gabriel Stanovsky, Swabha\nSwayamdipta, Jesse Dodge, and Noah A. Smith.\n2020. The right tool for the job: Matching model\nand instance complexities. In Proceedings of\nthe 58th Annual Meeting of the Association for\nComputational Linguistics, ACL 2020, Online, July\n5-10, 2020 , pages 6640‚Äì6651. Association for\nComputational Linguistics.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for BERT model com-\npression. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Nat-\nural Language Processing, EMNLP-IJCNLP 2019,\nHong Kong, China, November 3-7, 2019 , pages\n4322‚Äì4331. Association for Computational Linguis-\ntics.\nSurat Teerapittayanon, Bradley McDanel, and H. T.\nKung. 2016. Branchynet: Fast inference via early\nexiting from deep neural networks. In 23rd Inter-\nnational Conference on Pattern Recognition, ICPR\n2016, Canc√∫n, Mexico, December 4-8, 2016 , pages\n2464‚Äì2469. IEEE.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-Read Students Learn Better:\n2022\nOn the Importance of Pre-training Compact Models.\narXiv e-prints, page arXiv:1908.08962.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, 4-9 Decem-\nber 2017, Long Beach, CA, USA, pages 5998‚Äì6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th\nInternational Conference on Learning Representa-\ntions, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019. OpenReview.net.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers. CoRR, abs/2002.10957.\nJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and\nJimmy Lin. 2020. Deebert: Dynamic early exiting\nfor accelerating BERT inference. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics, ACL 2020, Online, July 5-10,\n2020, pages 2246‚Äì2251. Association for Computa-\ntional Linguistics.\nCanwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei,\nand Ming Zhou. 2020. Bert-of-theseus: Compress-\ning BERT by progressive module replacing. CoRR,\nabs/2002.02925.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural\nInformation Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n2019, NeurIPS 2019, 8-14 December 2019, Vancou-\nver, BC, Canada, pages 5754‚Äì5764.\nWangchunshu Zhou, Canwen Xu, Tao Ge, Julian\nMcAuley, Ke Xu, and Furu Wei. 2020. BERT loses\npatience: Fast and robust inference with early exit.\nCoRR, abs/2006.04152.\nA More Performance-EfÔ¨Åciency\nTrade-Off Curves\nPerformance-efÔ¨Åciency curves with RoBERTa and\nALBERT as backbones are shown in Figure 5 and\nFigure 6 respectively. Similar to the observation\nwith BERT as backbone, the performance of Dee-\nBERT and PABEE becomes progressively worse\nas the speed-up ratio increases. In contrast, our\npast-future early exit method shows more robust\nresults.\n(a) MNLI\n (b) QQP\n(c) QNLI\n (d) SST-2\nFigure 5: Performance-efÔ¨Åciency trade-off for early\nexit method DeeBERT with RoBERTa as backbone.\n(a) MNLI\n (b) QQP\n(c) QNLI\n (d) SST-2\nFigure 6: Performance-efÔ¨Åciency trade-off for early\nexit method PABEE with ALBERT as backbone.\nB Effect of Merging Gate\nMethod MNLI-mQNLISST-2 MRPC MacroAcc Acc Acc F1/Acc\nOurs 83.30 90.46 92.8988.44/83.0888.10\n-merging gate83.15 90.61 92.4386.86/80.6487.49\nTable 6: Ablation study of the merging gate. The speed-\nup ration is approximately 2.00√óand the model imple-\nmentation is based on BERT.\n2023\nWe conduct ablation study to show the effect of\nthe merging gate and report the result in Table 6.\nWe can see that the performance drops when we\nremove the merging gate from our model, suggest-\ning that the merging gate plays an important role in\nkeeping the balance between past information and\nfuture information.",
  "topic": "Inference",
  "concepts": [
    {
      "name": "Inference",
      "score": 0.704527735710144
    },
    {
      "name": "Computer science",
      "score": 0.5915080308914185
    },
    {
      "name": "Zh√†ng",
      "score": 0.5903832912445068
    },
    {
      "name": "Bin",
      "score": 0.5049462914466858
    },
    {
      "name": "Computational linguistics",
      "score": 0.49220195412635803
    },
    {
      "name": "Language model",
      "score": 0.4469001293182373
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44664862751960754
    },
    {
      "name": "Natural language processing",
      "score": 0.3834622800350189
    },
    {
      "name": "Linguistics",
      "score": 0.36465713381767273
    },
    {
      "name": "History",
      "score": 0.26603055000305176
    },
    {
      "name": "Philosophy",
      "score": 0.17892703413963318
    },
    {
      "name": "Programming language",
      "score": 0.17866814136505127
    },
    {
      "name": "China",
      "score": 0.09487226605415344
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ]
}