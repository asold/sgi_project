{
  "title": "Biomedical and Clinical Language Models for Spanish: On the Benefits of Domain-Specific Pretraining in a Mid-Resource Scenario",
  "url": "https://openalex.org/W3198365756",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287067269",
      "name": "Carrino, Casimiro Pio",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222692909",
      "name": "Armengol-Estapé, Jordi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4228057361",
      "name": "Gutiérrez-Fandiño, Asier",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287212331",
      "name": "Llop-Palao, Joan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4284194693",
      "name": "Pàmies, Marc",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286943466",
      "name": "Gonzalez-Agirre, Aitor",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2746277756",
      "name": "Villegas, Marta",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2805211535",
    "https://openalex.org/W3003266211",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3097750323",
    "https://openalex.org/W2098921539",
    "https://openalex.org/W3096590546",
    "https://openalex.org/W3101058639",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W2970636124",
    "https://openalex.org/W3080321165",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3096131366",
    "https://openalex.org/W2986178925",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3096841987",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W3157252883",
    "https://openalex.org/W2983842045",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3023545062",
    "https://openalex.org/W2985294119",
    "https://openalex.org/W2971258845"
  ],
  "abstract": "This work presents biomedical and clinical language models for Spanish by experimenting with different pretraining choices, such as masking at word and subword level, varying the vocabulary size and testing with domain data, looking for better language representations. Interestingly, in the absence of enough clinical data to train a model from scratch, we applied mixed-domain pretraining and cross-domain transfer approaches to generate a performant bio-clinical model suitable for real-world clinical data. We evaluated our models on Named Entity Recognition (NER) tasks for biomedical documents and challenging hospital discharge reports. When compared against the competitive mBERT and BETO models, we outperform them in all NER tasks by a significant margin. Finally, we studied the impact of the model's vocabulary on the NER performances by offering an interesting vocabulary-centric analysis. The results confirm that domain-specific pretraining is fundamental to achieving higher performances in downstream NER tasks, even within a mid-resource scenario. To the best of our knowledge, we provide the first biomedical and clinical transformer-based pretrained language models for Spanish, intending to boost native Spanish NLP applications in biomedicine. Our best models are freely available in the HuggingFace hub: https://huggingface.co/BSC-TeMU.",
  "full_text": "BIOMEDICAL AND CLINICAL LANGUAGE MODELS FOR\nSPANISH : O N THE BENEFITS OF DOMAIN -SPECIFIC\nPRETRAINING IN A MID-RESOURCE SCENARIO\nCasimiro Pio Carrino1, Jordi Armengol-Estapé1, Asier Gutiérrez-Fandiño1, Joan Llop-Palao1, Marc Pàmies1, Aitor\nGonzalez-Agirre1, and Marta Villegas1\n1Text Mining Unit, Barcelona Supercomputing Center\n{casimiro.carrino,jordi.armengol,asier.gutierrez,joan.lloppalao,marc.pamies,aitor.gonzalez,marta.villegas}@bsc.es\nSeptember 20, 2021\nABSTRACT\nThis work presents biomedical and clinical language models for Spanish by experimenting with\ndifferent pretraining choices, such as masking at word and subword level, varying the vocabulary size\nand testing with domain data, looking for better language representations. Interestingly, in the absence\nof enough clinical data to train a model from scratch, we applied mixed-domain pretraining and\ncross-domain transfer approaches to generate a performant bio-clinical model suitable for real-world\nclinical data. We evaluated our models on Named Entity Recognition (NER) tasks for biomedical\ndocuments and challenging hospital discharge reports. When compared against the competitive\nmBERT and BETO models, we outperform them in all NER tasks by a signiﬁcant margin. Finally,\nwe studied the impact of the model’s vocabulary on the NER performances by offering an interesting\nvocabulary-centric analysis. The results conﬁrm that domain-speciﬁc pretraining is fundamental to\nachieving higher performances in downstream NER tasks, even within a mid-resource scenario. To\nthe best of our knowledge, we provide the ﬁrst biomedical and clinical transformer-based pretrained\nlanguage models for Spanish, intending to boost native Spanish NLP applications in biomedicine. Our\nbest models are freely available in the HuggingFace hub: https://huggingface.co/BSC-TeMU.\n1 Introduction\nThe success of transformer-based models in the general domain [ 11] has encouraged the development of language\nmodels for domain-speciﬁc scenarios [9, 30, 5, 17]. Speciﬁcally, in the biomedical domain, there has been a proliferation\nof models [24, 6, 4, 13] since the ﬁrst BioBERT [18] model was published. Unfortunately, there is still a signiﬁcant lack\nof biomedical and clinical models in languages other than English, despite the increasing efforts of the NLP community\n[21, 27]. Consequently, general-domain pretrained language models supporting Spanish, such as mBERT [ 11] and\nBETO [8], are often used as a proxy in the absence of a genuine domain-specialized model. To ﬁll this gap, we trained\nseveral biomedical language models experimenting with a wide range of pretraining choices, namely, masking with\nword and subword units, varying the vocabulary size and experimenting with the data domain. Furthermore, we applied\ncross-domain transfer and mixed-domain pretraining using biomedical and clinical data to train bio-clinical models\nintended for clinical applications. To evaluate our models, we choose the fundamental Named Entity Recognition (NER)\ntask in information retrieval deﬁned for biomedical and clinical scenarios, the former based on biomedical documents\nand the latter on real hospital documents. As the main result, our models obtained a signiﬁcant gain over both mBERT\nand BETO models in all tasks. Additionally, we studied the impact of the model’s vocabulary on several downstream\ntasks by performing a vocabulary-centric analysis. The evaluation results highlights the importance of domain-speciﬁc\npretraining over continual pretraining from general domain data in a mid-resource scenario. Our main contributions are:\n1. We release the ﬁrst Spanish biomedical and clinical transformer-based pretrained language models, trained\nwith the largest biomedical corpus known to date.\narXiv:2109.03570v2  [cs.CL]  17 Sep 2021\nA PREPRINT - SEPTEMBER 20, 2021\n2. We assess the effectiveness of these models in different settings, two biomedical scenarios and a demanding\nclinical scenario with real hospital reports.\n3. We show that biomedical models exhibit a remarkable cross-domain transfer ability on the clinical domain.\n4. We perform an in-depth vocabulary and segmentation analysis, which offers insights on domain-speciﬁc\npretraining and raises interesting open questions.\n2 Related work\nIn the last years, several language models for both the biomedical and clinical domain have been trained through\nunsupervised pretraining of transformer-based architectures [14]. The ﬁrst trained model was BioBERT [18], where\nthe authors adapted the BERT model [11], trained with general-domain data, to the biomedical domain by continual\npretraining. Similarly, other works followed the continual pretraining approach to train the BlueBERT [ 24] and\nClinicalBERT [4] models. When enough in-domain data is available, training from scratch has been used as an\nalternative method to continual pretraining, leading to the SciBERT [6] and PubMedBERT [13] models. However, the\nSciBERT model uses mixed-domain data from the biomedical and computer science domain, while PubMedBERT\nleverages only data belonging to the biomedical domain. Interestingly, in [13], the authors call into question the beneﬁts\nof mixed-domain pretraining, estimating its negative impact on a set of downstream tasks belonging to an extensive\nbiomedical benchmark (named BLURB) that they provide.\nOur pretraining approach employed training from scratch with mixed-domain data by combining biomedical and clinical\nresources. However, in contrast to [6, 13] i) we used a corpus 3-5 times smaller, ii) we performed pretraining from\nscratch with mixed-domain data from biomedical documents and clinical notes and iii) we assess the suitability of\ncross-domain transfer from the biomedical to the clinical domain.\n3 Corpora\nThis work considers two corpora with very different sizes and domains, namely, a clinical corpus and a biomedical one.\nThe clinical corpus contains 91M tokens from more than 278K clinical documents (including discharge reports, clinical\ncourse notes and X-ray reports). For the biomedical corpus we gathered data from a variety of sources, namely:\n• Medical crawler3: Crawler of more than 3,000 URLs belonging to Spanish biomedical and health domains\nintroduced in [7].\n• Clinical cases misc.: A miscellany of medical content, essentially clinical case. Note that a clinical case report\nis different from a scientiﬁc publication where medical practitioners share patient cases and it is different from\na clinical note or document.\n• Scielo4: Scientiﬁc publications written in Spanish crawled from the Spanish SciELO server in 2017.\n• PubMed: Open-access articles from the PubMed repository crawled in 2017.\n• BARR2_background5: Biomedical Abbreviation Recognition and Resolution (BARR2) containing Spanish\nclinical case study sections from a variety of clinical disciplines.\n• Wikipedia_life_sciences: Wikipedia articles crawled on 04/01/2021 with the Wikipedia API python library1\nstarting from the \"Ciencias_de_la_vida\" category up to a maximum of 5 subcategories. Multiple links to the\nsame articles are then discarded to avoid repeating content.\n• Patents: Google Patent in Medical Domain for Spain (Spanish). The accepted codes (Medical Domain) for\nJson ﬁles of patents are: \"A61B\", \"A61C\",\"A61F\", \"A61H\", \"A61K\", \"A61L\",\"A61M\", \"A61B\", \"A61P\".\n• EMEA6: Spanish-side documents extracted from parallel corpora made out of PDF documents from the\nEuropean Medicines Agency.\n• mespen_Medline7: Spanish-side articles extracted from a collection of Spanish-English parallel corpus\nconsisting of biomedical scientiﬁc literature. The collection of parallel resources are aggregated from the\nMedlinePlus source.\nNotice that, although the biomedical documents undoubtedly share a signiﬁcant percentage of medical terms with\nclinical notes, the syntax and vocabulary may change radically due to the speciﬁc contexts and the idiosyncrasies of the\nuser-generated content in clinical texts.\n1https://pypi.org/project/Wikipedia-API/\n2\nA PREPRINT - SEPTEMBER 20, 2021\nCorpus name No. tokens Documents Sentences\nMedical crawler 745,705,946 1,580,577 34,347,553\nClinical cases misc. 102,855,267 41,592 5,565,854\nScielo 60,007,289 16,981 2,668,231\nBARR2_background 24,516,442 127,635 1,029,600\nWikipedia_life_sciences 13,890,501 15,791 832,027\nPatents 13,463,387 99,611 253,924\nEMEA 5,377,448 284,119 284,575\nmespen_Medline 4,166,077 5,293 322,619\nPubMed 1,858,966 892 103,674\nTable 1: List of individual sources constituting the biomedical corpus. The number of tokens refers to white-spaced\ntokens calculated on cleaned untokenized text.\nWe cleaned each biomedical corpus to get the ﬁnal corpus, and left the clinical corpus uncleaned. For each biomedical\nresource, we applied a cleaning pipeline with customized operations designed to read data in different formats, split it\ninto sentences, detect the language, remove noisy and ill-formed sentences, deduplicate and eventually output the data\nwith their original document boundaries. Finally, to remove repetitive content, we concatenated the entire corpus and\ndeduplicate again, obtaining a total of 968M words. Table 1 shows detailed statistics of each dataset.\n4 Models pretraining\nThe models trained in this work employed a RoBERTa [19] base model with 12 self-attention layers. Following the\noriginal training, we prescind the auxiliary Next Sentence Prediction task used in BERT, and used masked language\nmodelling as the pretraining objective. We used Subword Masking (SWM) as in [19], and the Whole Word Masking\n(WWM) 2.\nWe tokenized with the Byte-Level BPE algorithm introduced in [ 25] and employed in the original RoBERTa [ 19],\nunlike previous biomedical language models [6, 13, 18] that use WordPiece [11] or SentencePiece [16] segmentations.\nWe learned a cased vocabulary of 52k and 30k tokens to perform a comparative analysis.\nWe run the training for 48 hours with 16 NVIDIA V100 GPUs of 16GB DDRAM, using Adam optimizer [15] with\npeak learning rate of 0.0005 and an effective batch size of 2,048 sentences 8. We left the other hyperparameters in\ntheir default values as the original RoBERTa trainings. We then selected the model with the lowest perplexity in a\nholdout subset as the best model. Moreover, training was performed at the document level, preserving document\nboundaries. Document-level training may be crucial to enforce long-range dependencies and push the model towards\nthe comprehension of entire documents, fostering the modelling of long-range dependencies.\nWe applied the pretraining method described above to train a variety of models, which can be divided into two groups,\nthe Biomedical language models and the Bio-clinical language models.\nBiomedical language models: We used the biomedical corpora described in section 3 of about 968M tokens to train\nbiomedical language models. To study the impact of the masking mechanism and the vocabulary size, we experimented\nwith the SWM and WWM techniques and with vocabularies of 52k and 30k tokens. We refer to the four variants as\nbio-52k-SWM, bio-52k-WWM, bio-30k-SWM and bio-30k-WWM.\nBio-clinical language models: Due to the lack of a large-scale clinical corpus of comparable size to the biomedical\none, we combined the biomedical and clinical corpora described in section 3 to train bio-clinical language models\nsuitable for clinical settings. Furthermore, we also trained a bio-clinical model variant that leverages a vocabulary\nlearned only from the clinical corpus with a vocabulary size of 52k. We refer to these two models as bio-cli-52k and\nbio-cli-52k-vocab-cli.\n2https://github.com/google-research/bert technique that masks all sub-words belonging to the same word [10].\n3https://zenodo.org/record/4561970\n4https://github.com/PlanTL-SANIDAD/SciELO-Spain-Crawler\n5https://temu.bsc.es/BARR2/downloads/background_set.raw_text.tar.bz2\n6http://opus.nlpl.eu/download.php?f=EMEA/v3/moses/en-es.txt.zip\n7https://zenodo.org/record/3562536\n8Through gradient accumulation as implemented in Fairseq [22].\n3\nA PREPRINT - SEPTEMBER 20, 2021\n5 Downstream NER tasks\nWe performed Named Entity Recognition (NER) tasks as a testbed for the our models since they are essential building\nblocks for many biomedical Text Mining and NLP applications. We employed a standard linear layer as a token\nclassiﬁcation head and the BIO tagging schema [ 26] to ﬁne-tune the pretrained models for the NER tasks. During\nﬁne-tuning, both the pretrained model and the classiﬁcation layers parameters are learned with stochastic gradient\ndescent.\nFine-tuning pretrained transformer-based language models for NER tasks by adding a linear layer on top of them is a\nusual practice in the literature (both in general-purpose models [11, 19] and domain-speciﬁc ones [18]). Remarkably,\nthis method obtained impressive performances compared to more sophisticated classiﬁcation layers such as Conditional\nRandom Field layers on top of Bidirectional Long Short-Term Memory Recurrent Networks [23]. Furthermore, its\nsimplicity allows a head-to-head comparison of different pretraining strategies and baseline models, emphasising the\nability of the pretrained representations.\nWe applied the ﬁne-tuning method described above to three different NER datasets. The ﬁrst two are data from two\nshared tasks and use annotations on curated medical data (clinical cases extracted from medical literature). The last one\nuses medical records from the ICTUSnet project9.\nPharmaCoNER [12] is a track on chemical and drug mention recognition from Spanish medical texts. The authors\ncollected a manually classiﬁed collection of clinical case report sections derived from open access Spanish medical\npublications, named the Spanish Clinical Case Corpus (SPACCC). The corpus contained a total of 1,000 clinical cases\nand 396,988 words and was manually annotated, with a total of 7,624 entity mentions, corresponding to four different\nmention types10. The track received several system submissions from the NLP community [29, 2, 3, 33]\nCANTEMIST [20] is a shared task speciﬁcally focusing on named entity recognition of tumor morphology, in Spanish.\nThe CANTEMIST corpus11 is a collection of 1,301 oncological case reports written in Spanish, with a total of 63,016\nsentences and 1,093,501 tokens. Several systems employing different strategies have been proposed to tackle the task\n[1, 32, 31]\nThe ICTUSnet data set consists of 1,006 hospital discharge reports of patients admitted for stroke from 18 different\nSpanish hospitals. It contains more than 79,000 annotations for 51 different kinds of variables. The dataset is part of the\nICTUSnet project, whose main objective was the development of an information extraction system to support domain\nexperts when identifying relevant information in discharge reports.\n6 Evaluation and Main Results\nWe evaluated our models on two biomedical benchmarks, PharmaCoNER and CANTEMIST, as well as the clinical\nICTUSnet dataset, described in section 5. For each NER dataset and model, we ﬁne-tune for 10 epochs with a batch\nsize of 32 and a maximum sentence length of 512 tokens. After training, we select as the best model the one with the\nhighest F1 score on the development set. Finally, we computed the evaluation scores by feeding the test set to the best\nmodel. Tables 2 and 3 show the results for the biomedical models and bio-clinical models, respectively. We would like\nto remark that the ICTUSnet dataset is a challenging task since its consists of real hospital discharge reports. Moreover,\nfor the biomedical models, the ICTUSnet evaluation represents a cross-domain transfer experiment from the biomedical\nto clinical domain.\nOverall, our models achieved the best scores, beating both mBERT and BETO signiﬁcantly. The biomedical models\nshowed a remarkable cross-domain transfer ability compared to the bio-clinical models, achieving competitive perfor-\nmances on the ICTUSnet task. Nonetheless, the bio-clinical models obtained the best performances, indicating that\nthe mixed-domain pretraining is practical approach to mitigate the lack of large-scale real-world clinical data. These\nresults conﬁrm the suitability of pretraining from scratch in a scenario with medium-size resources. We ﬁnally point out\nthat, although our primary objective aside from achieving the best task performance, the results obtained are promising,\nevincing that a more sophisticated classiﬁcation layer could further improve the tasks performances.\n9https://ictusnet-sudoe.eu/es/\n10For a detailed description, see https://temu.bsc.es/pharmaconer/\n11CANTEMIST corpus: https://doi.org/10.5281/zenodo.3878178\n4\nA PREPRINT - SEPTEMBER 20, 2021\nBio mBERT BETO\nvocab size 52k 30k 120k 31k\nmasking SWM WWM SWM WWM SWM WWM\nPharmaCoNER\n- F1 89.48± 0.60 89.62 ±0.57 89.47±0.33 89.85±0.47 87.46±0.23 88.18±0.41\n- Precision 87.85± 1.15 88.18 ±0.99 88.33±0.42 88.41 ±0.41 86.50±0.95 87.12±0.52\n- Recall 91.18± 0.74 91.11 ±0.40 90.64±0.76 91.35 ±0.58 88.46±0.57 89.28±0.69\nCANTEMIST\n- F1 83.87± 0.41 83.00±0.17 82.85±0.36 83.23 ±0.34 82.61±0.67 82.42±0.06\n- Precision 81.70±0.52 80.81 ±0.27 80.93±0.48 81.27 ±0.72 81.12±0.65 80.91±0.41\n- Recall 86.17±0.46 85.32 ±0.40 84.87±0.27 85.29 ±0.52 84.15±0.78 84.00±0.45\nICTUSnet\n- F1 88.12±0.25 87.94±0.43 87.77±0.35 88.21±0.26 86.75±0.21 85.95±0.25\n- Precision 85.56±0.26 84.88±0.80 84.33±0.28 85.20±0.57 83.53±0.35 83.10±0.57\n- Recall 90.83±0.47 91.22±0.49 91.50±0.66 91.45±0.44 90.23±0.46 89.02±0.61\nTable 2: Evaluation scores (F1, Precision and Recall) for the PharmaCoNER, CANTEMIST and ICTUSnet NER tasks.\nWe compared our biomedical models with two general-domain baseline models, namely, multilingual BERT and BETO.\nRows in light gray stress that the Biomedical model is performing cross-domain transfer of the ICTUSnet task since it\nbelongs to the clinical domain. The scores are averaged across 5 random runs with standard deviation as error bar.\nBio-cli mBERT BETO\nvocab size-domain 52k 52k-vocab-cli 120k-general 31k-general\nPharmaCoNER\n- F1 90.04±0.13 89.29±0.52 87.46±0.23 88.18 ±0.41\n- Precision 88.92±0.36 87.67 ±0.96 86.50±0.95 87.12 ±0.52\n- Recall 91.18±0.29 90.98 ±0.47 88.46±0.57 89.28 ±0.69\nCANTEMIST\n- F1 83.34±0.39 84.41±0.33 82.61±0.67 82.42 ±0.06\n- Precision 81.48±0.66 82.75 ±0.25 81.12±0.65 80.91 ±0.41\n- Recall 85.30±0.45 86.15 ±0.53 84.15±0.78 84.00 ±0.45\nICTUSnet\n- F1 88.08±0.12 88.45±0.63 86.75±0.21 85.95 ±0.25\n- Precision 84.92±0.29 84.92 ±0.86 83.53±0.35 83.10 ±0.57\n- Recall 91.50±0.29 92.3 ±0.52 90.23±0.46 89.02 ±0.61\nTable 3: Evaluation scores (F1, Precision and Recall) for the PharmaCoNER, CANTEMIST and ICTUSnet NER tasks.\nWe compared our Bio-clinical models with two baselines obtained with non domain-speciﬁc models in Spanish, namely,\nmultilingual BERT and BETO. The scores are averaged across 5 random runs with standard deviation as error bar.\n7 Discussion and Analysis\nThis section attempts to shed light on the evaluation results by conducting a vocabulary analysis and segmentation\nexperiments.\n7.1 Vocabulary overlap\nWe look at the evaluation results through the lens of the model’s vocabulary. Undoubtedly, the vocabulary plays a\ncrucial role during the downstream transfer since it is responsible for encoding the task-speciﬁc data. Intuitively, we\nexpect that the more the overlap between vocabulary and task’s tokens, the better. A high overlap leverages more\npretrained representations that could be beneﬁcial for ﬁne-tuning. Speciﬁcally, we hypothesize that the performance on\neach downstream task may be related to the number of vocabulary tokens used to encode the task’s data. Therefore,\nwe ﬁrst tokenize the three NER tasks with each model’s vocabulary and then calculate the vocabulary overlap with\neach task, expressed as the number of tokens. The results shown in Table 4 seem to support our hypothesis by showing\nthat the best evaluation scores for each task are obtained by the model with the maximum number of tokens overlap.\nAccordingly, the lowest overlap exhibited by the baselines models may explain their lowest evaluation scores.\n5\nA PREPRINT - SEPTEMBER 20, 2021\nModel PharmaCoNER CANTEMIST ICTUSnet\nBio-cli-52k 20,620 (40%) 22,001 (42%) 23,360 (45%)\nBio-cli-vocab-cli-52k 20,335 (39%) 23,095 (44%) 30,467 (59%)\nBio-52k 19,978 (38%) 20,951 (40%) 21,449 (41%)\nBio-30k 15,792 (53%) 16,302 (54%) 16,266 (54%)\nBETO 12,829 (41%) 13,044 (42%) 13,388 (43%)\nmBERT 11,084 (9%) 11,434 (9%) 13,187 (11%)\nTable 4: The vocabulary overlap is expressed as the intersection of each model’s vocabulary and segmented tokens for\neach NER task. We also show the percentage of overlapped tokens to the total vocabulary size. Note that the SWM and\nWWM variants of the biomedical models are merged under the same names since they are equivalent for the vocabulary\nanalysis.\n7.2 Impact of segmentation\nIntuitively, it is reasonable to assume that a proper domain-speciﬁc segmentation should preserve the integrity of\nbiomedical terms, minimizing the number of subword units required to encode them. As pointed out in [13], models\nemploying out-of-domain vocabulary \"are forced to divert parametrization capacity and training bandwidth to model\nbiomedical terms using fragmented subwords\" . Therefore, from one side, over-segmenting may have a negative\ninﬂuence on the downstream performance. On the other side, under-segmentation that employs term-speciﬁc units\ncould dramatically increase the vocabulary size. Moreover, under-segmenting could prevent the model from detecting\nrelatedness between terms based on the shared subwords, especially in morphologically rich terminology. Therefore,\nwe conducted a meticulous analysis of domain-speciﬁc terms segmentation to study the trade-off between under and\nover-segmentation conditions and shed light on its relation with the NER downstream performances.\n7.2.1 Splitting terms\nWe analyzed, both qualitatively and quantitatively, how the different models under study segment biomedical terms.\nWe retrieved all the biomedical terms used as annotations in the CANTEMIST, PharmaCoNER and ICTUSnet tasks\nand segmented them applying each model’s tokenizer. Table 7 shows the quality of segmentation on a random set of\nNER annotations, making a comparison between mBERT, BETO and our best model bio-cli-52k-vocab-cli. Table 5\nshows the average number of subwords generated by each model computed over all the NER annotations. As expected,\nmBERT and BETO split terms into many more pieces than our models, on average. However, the variations across\nbiomedical and clinical models also point out that the vocabulary size and mixed-domain pretraining are relevant factors\ndetermining the ﬁnal segmentation. Finally, Table 6 illustrates, for each model, how many tokens are segmented with a\ngiven number of subwords (up to more than 4 subwords). Again, both mBERT and BETO tend to over-segment when\ncompared to our models. As an example, our best model (bio-cli-vocab-cli-52k) has roughly 50% of PharmaCoNER\nannotations segmented in either one or two tokens. In comparison, mBERT and BETO have less than 20% of the\nannotations segmented in that same amount of subwords.\n7.2.2 Dissecting the F1 score\nFinally, we seek a more precise relationship between the evaluation scores and the model’s over-segmentation, expressed\nas the average number of subwords per term. Speciﬁcally, for each model and NER task, we group the annotations\nin the test split by the number of subwords they are shattered into and then recalculate the performance scores for\neach group of annotations. From the results presented in Figure 1, it can be observed that the F1 score decreases as\nthe number of subwords increases. On average, the decreasing trend is exhibited across the three tasks, with a more\nconsiderable variation starting from 7 subwords. The analysis suggests that over-segmentation should be avoided in\norder to obtain higher F1 scores, conﬁrming the intuition that a helpful segmentation preserves terms integrity. Note\nthat, rather than criticising subword segmentation, we believe that further experiments unravelling the relationship\nbetween the segmentation and the downstream task performances could guide the design of an optimal biomedical\nsegmentation.\n8 Open Questions\nIn this section, we open up interesting questions motivated by the evaluation results and analysis presented in previous\nsections.\n6\nA PREPRINT - SEPTEMBER 20, 2021\nFigure 1: F1 score computed on the subset of annotations from the CANTEMIST, PharmaCoNer and ICTUSnet test\nsets that are shuttered in a given number of subword units.\nModel Mean Median\nbio-cli-vocab-cli-52k 6.01 4\nbio-cli-52k 6.34 4\nbio-52k 6.63 5\nbio-30k 7.12 5\nBETO 8.60 6\nmBERT 8.93 7\nTable 5: Comparison of mean and median number of tokens generated by each model on the PharmaCoNER, CAN-\nTEMIST and ICTUSnet annotations. Note that the SWM and WWM variants of the biomedical models are merged\nunder the same names since they are equivalent for the vocabulary analysis.\nIs WWM better than SWM? The comparison between SWM and WWM pretraining techniques shows, in the case\nof the biomedical evaluation (see Table 2), that the impact provided by the latter is affected by the vocabulary size. In\nparticular, the WWM technique shows consistent superiority only with a vocab size of 30k. This evidence is in contrast\nto the ﬁnding, pointed out in [ 13], that WWM is in general beneﬁcial. We believe that further ablation studies are\nnecessary to elucidate the interplay between the vocabulary size and the masking mechanism.\nIs mixed-domain pretraining beneﬁcial? The evaluation scores show that the bio-clinical models obtained the\nbest performances across all tasks. Surprisingly, the bio-clinical model with clinical vocabulary obtains the highest\nperformance on the CANTEMIST and ICTUSnet test sets. These results suggest that mixed-domain data might\nnot always degrade performance, questioning the ﬁnding presented in [13], where they show the negative impact of\nmixed-domain pretraining with biomedical and computer science text, as applied in SciBERT [6]. In our case, since we\ndeal with two distinct but close domains, the biomedical and the clinical ones, we somehow expect that mixed-domain\npretraining could proﬁt by adding more training data. On the other side, the results of the bio-cli-52k-vocab-cli model\nalso highlight that, under the same training size conditions, the vocabulary plays an important role. In general, we\nbelieve further experiments are needed to understand the limitations of mixed-domain pretraining. However, in our\nevaluation scenario, we hypothesize the reason behind the encouraging performances may be related to how much\noverlap the speciﬁc NER data has with each model’s vocabulary, as partially supported by the results obtained in Table\n4.\n9 Conclusions and Future Work\nIn this work, we trained the ﬁrst biomedical and clinical transformer-based pretrained language models for Spanish. We\nthen evaluated them on a set of NER tasks, including a demanding one based on real hospital discharge reports. Our\nmodels overcome two competitive baselines, namely mBERT and BETO, representing superior solutions for biomedical\nNLP applications in Spanish. Finally, we analyzed the results by performing an in-depth analysis involving the model\n7\nA PREPRINT - SEPTEMBER 20, 2021\nmBERT BETO bio-30k bio-52k bio-cli-52k bio-cli-vocab-cli-52k\nPharmaCoNER\n1 tok 2.79% 3.04% 7.47% 15.76% 18.22% 24.99%\n2 tok 15.35% 14.73% 22.16% 25.93% 26.59% 24.46%\n3 tok 23.18% 22.36% 26.55% 26.55% 25.44% 19.82%\n4 tok 26.51% 27.94% 21.95% 17.32% 15.51% 15.39%\n5+ tok 32.17% 31.93% 21.87% 14.44% 14.24% 15.34%\nCANTEMIST\n1 tok 0.41% 0.32% 1.02% 1.82% 1.89% 2.84%\n2 tok 2.04% 2.88% 8.20% 10.56% 11.88% 14.29%\n3 tok 4.18% 5.04% 10.70% 12.79% 12.40% 11.63%\n4 tok 6.0% 7.61% 13.08% 14.31% 14.99% 15.33%\n5+ tok 87.37% 84.15% 67.0% 60.52% 58.84% 55.91%\nICTUSnet\n1 tok 1.12% 1.37% 1.70% 2.29% 3.18% 6.11%\n2 tok 2.37% 2.82% 4.85% 5.94% 8.12% 11.43%\n3 tok 11.36% 10.21% 15.37% 16.38% 20.90% 22.43%\n4 tok 10.04% 9.93% 13.36% 12.81% 11.08% 9.65%\n5+ tok 75.11% 75.67% 64.72% 62.58% 56.72% 50.4%\nTable 6: Percentage of biomedical terms from the PharmaCoNER, CANTEMIST and ICTUSnet annotations that are by\neach model into one subword up to more than ﬁve subwords. Note that the SWM and WWM variants of the biomedical\nmodels are merged under the same names since they are equivalent for the vocabulary analysis.\nOriginal term mBERT BETO bio-cli-vocab-cli-52k\nADN ADN ADN ADN\nHBsAg HB-s-A-g H-B-s-A-g HBsAg\nantitransglutaminasa anti-tra-ns-gl-uta-minas-a anti-trans-gl-uta-minas-a anti-trans-glutaminasa\nclonidina c-loni-dina clon-idi-na clon-idina\ndiclofenaco di-clo-fen-aco dic-lo-fen-aco diclofenaco\nglucagonoma g-luca-gono-ma glu-ca-gono-ma gluca-gono-ma\nhemangioblastomas hem-ang-io-blast-omas hem-ang-io-blas-tomas hema-ngio-blas-tomas\nhidrocortisona hi-dro-cor-tis-ona hidro-cor-tis-ona hidrocortisona\ninmunoglobulina in-mun-og-lob-ulin-a inmun-o-glo-bul-ina inmun-oglobulina\ninsulina ins-ulin-a insulina insulina\nintratumorales intrat-umo-rales intra-tum-ora-les intra-tum-orales\nleucemia le-uce-mia le-ucemia leucemia\nlidocaína lid-oca-ína li-doc-a-ína lidocaína\nlinfoma li-nfo-ma linf-oma linfoma\nmetastasis meta-stas-is metas-tasis metastasis\nmetilfenidato met-il-fen-idat-o metil-fen-ida-to metil-fenidato\norofaringe oro-fari-nge oro-far-ing-e orofaringe\ntransaminasas trans-ami-nas-as trans-aminas-as transaminasas\nTable 7: Segmentation comparison between mBERT, BETO and our best model bio-cli-vocab-cli-52k. The terms are\nextracted randomly from the NER annotations and are presented in alphabetic order.\n8\nA PREPRINT - SEPTEMBER 20, 2021\nvocabularies and segmentations. Throughout the work, we outline some underexplored aspects of language model\npretraining, such as the feasibility of the mixed-domain approach, the effectiveness of cross-domain transfer for clinical\nsettings and the interplay between the vocabulary size and the token masking mechanism. Moreover, we showed the\nimpact of different terms segmentation on the evaluation score, suggesting that over-segmentation can be detrimental\nfor downstream tasks such as NER. Overall, we experimentally show that domain-speciﬁc pretraining has a positive\nimpact than general-domain pretraining in a mid-resource scenario.\nAs future work, we suggest extending the evaluation to other tasks apart from NER, arguably the most studied task in\nthe biomedical and clinical NLP literature but not the only relevant one. In addition, according to open questions raised\nin Section 8, we will perform more in-depth experiments to elucidate under which conditions mixed-domain pretraining\nis advantageous, and we will investigate the relationship between the vocabulary size and the masking mechanism.\n10 Accessibility\nWe released our best models under the Apache License 2.0 to encourage the development of Spanish NLP applications\nin the biomedical and clinical domains. We uploaded the model in the HuggingFace models hub under the following\nlinks:\n• bio-52k-SWM: https://huggingface.co/BSC-TeMU/roberta-base-biomedical-es\n• bio-cli-52k: https://huggingface.co/BSC-TeMU/roberta-base-biomedical-clinical-es\nAcknowledgements\nThis work was partially funded by the Spanish State Secretariat for Digitalization and Artiﬁcial Intelligence (SEDIA)\nwithin the framework of the Plan-TL and by Fundació La Marató de TV3 under the project 201712.31.\n9\nA PREPRINT - SEPTEMBER 20, 2021\nReferences\n[1] M. Cuadros A. García-Pablos, N. Perez. Vicomtech at cantemist 2020. In Proceedings of the Iberian Languages\nEvaluation Forum (IberLEF 2020), CEUR Workshop Proceedings, 2020.\n[2] L. Akhtyamova, Paloma Martínez, K. Verspoor, and J. Cardiff. Testing contextualized word embeddings to\nimprove ner in spanish clinical case narratives. IEEE Access, 8:164717–164726, 2020.\n[3] Liliya Akhtyamova. Named entity recognition in spanish biomedical literature: Short review and bert model. In\n2020 26th Conference of Open Innovations Association (FRUCT), pages 1–7, 2020.\n[4] Emily Alsentzer, John Murphy, William Boag, Wei-Hung Weng, Di Jindi, Tristan Naumann, and Matthew\nMcDermott. Publicly available clinical BERT embeddings. In Proceedings of the 2nd Clinical Natural Language\nProcessing Workshop, pages 72–78, Minneapolis, Minnesota, USA, June 2019. Association for Computational\nLinguistics.\n[5] Dogu Araci. Finbert: Financial sentiment analysis with pre-trained language models. CoRR, abs/1908.10063,\n2019.\n[6] Iz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language model for scientiﬁc text. In Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP), pages 3615–3620, Hong Kong, China, November\n2019. Association for Computational Linguistics.\n[7] Casimiro Pio Carrino, Jordi Armengol-Estapé, Ona de Gibert Bonet, Asier Gutiérrez-Fandiño, Aitor Gonzalez-\nAgirre, Martin Krallinger, and Marta Villegas. Spanish biomedical crawled corpus: A large, diverse dataset for\nspanish biomedical language models, 2021.\n[8] José Cañete, Gabriel Chaperon, Rodrigo Fuentes, Jou-Hui Ho, Hojin Kang, and Jorge Pérez. Spanish pre-trained\nbert model and evaluation data. In PML4DC at ICLR 2020, 2020.\n[9] Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos. LEGAL-\nBERT: The muppets straight out of law school. In Findings of the Association for Computational Linguistics:\nEMNLP 2020, pages 2898–2904, Online, November 2020. Association for Computational Linguistics.\n[10] Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, and Guoping Hu. Pre-training with\nwhole word masking for chinese bert. ArXiv, abs/1906.08101, 2019.\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.\n[12] Aitor Gonzalez-Agirre, Montserrat Marimon, Ander Intxaurrondo, Obdulia Rabal, Marta Villegas, and Martin\nKrallinger. PharmaCoNER: Pharmacological substances, compounds and proteins named entity recognition track.\nIn Proceedings of The 5th Workshop on BioNLP Open Shared Tasks, pages 1–10, Hong Kong, China, November\n2019. Association for Computational Linguistics.\n[13] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. Domain-speciﬁc language model pretraining for biomedical natural language processing,\n2020.\n[14] Katikapalli Subramanyam Kalyan, A. Rajasekharan, and S. Sangeetha. Ammu - a survey of transformer-based\nbiomedical pretrained language models. ArXiv, abs/2105.00827, 2021.\n[15] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference on\nLearning Representations, 12 2014.\n[16] Taku Kudo. Subword regularization: Improving neural network translation models with multiple subword\ncandidates. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pages 66–75, Melbourne, Australia, July 2018. Association for Computational Linguistics.\n[17] Jieh-Sheng Lee and Jieh Hsiang. Patentbert: Patent classiﬁcation with ﬁne-tuning a pre-trained BERT model.\nCoRR, abs/1906.02124, 2019.\n[18] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\nBioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics,\n36(4):1234–1240, 09 2019.\n10\nA PREPRINT - SEPTEMBER 20, 2021\n[19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\nmoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692,\n2019.\n[20] A Miranda-Escalada, E Farré, and M Krallinger. Named entity recognition, concept normalization and clinical\ncoding: Overview of the cantemist track for cancer text mining in spanish, corpus, guidelines, methods and results.\nIn Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2020), CEUR Workshop Proceedings, 2020.\n[21] Aurélie Névéol, H. Dalianis, G. Savova, and Pierre Zweigenbaum. Clinical natural language processing in\nlanguages other than english: opportunities and challenges. Journal of Biomedical Semantics, 9, 2014.\n[22] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli.\nfairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics (Demonstrations), pages 48–53, Minneapolis,\nMinnesota, June 2019. Association for Computational Linguistics.\n[23] Rrubaa Panchendrarajan and Aravindh Amaresan. Bidirectional LSTM-CRF for named entity recognition. In\nProceedings of the 32nd Paciﬁc Asia Conference on Language, Information and Computation, Hong Kong, 1–3\nDecember 2018. Association for Computational Linguistics.\n[24] Yifan Peng, Shankai Yan, and Zhiyong Lu. Transfer learning in biomedical natural language processing: An\nevaluation of BERT and ELMo on ten benchmarking datasets. In Proceedings of the 18th BioNLP Workshop and\nShared Task, pages 58–65, Florence, Italy, August 2019. Association for Computational Linguistics.\n[25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[26] Erik F Sang and Sabine Buchholz. Introduction to the conll-2000 shared task: Chunking. arXiv preprint\ncs/0009008, 2000.\n[27] Elisa Terumi Rubel Schneider, João Vitor Andrioli de Souza, Julien Knafou, Lucas Emanuel Silva e Oliveira,\nJenny Copara, Yohan Bonescki Gumiel, Lucas Ferro Antunes de Oliveira, Emerson Cabrera Paraiso, Douglas\nTeodoro, and Cláudia Maria Cabral Moro Barra. BioBERTpt - a Portuguese neural language model for clinical\nnamed entity recognition. In Proceedings of the 3rd Clinical Natural Language Processing Workshop, pages\n65–72, Online, November 2020. Association for Computational Linguistics.\n[28] Felipe Soares, Marta Villegas, Aitor Gonzalez-Agirre, Martin Krallinger, and Jordi Armengol-Estapé. Medical\nword embeddings for Spanish: Development and evaluation. In Proceedings of the 2nd Clinical Natural Language\nProcessing Workshop, pages 124–133, Minneapolis, Minnesota, USA, June 2019. Association for Computational\nLinguistics.\n[29] Manuel Stoeckel, Wahed Hemati, and Alexander Mehler. When specialization helps: Using pooled contextualized\nembeddings to detect chemical and biomedical entities in Spanish. In Proceedings of The 5th Workshop on\nBioNLP Open Shared Tasks, pages 11–15, Hong Kong, China, November 2019. Association for Computational\nLinguistics.\n[30] Wen Tai, H. T. Kung, Xin Dong, Marcus Comiter, and Chang-Fu Kuo. exBERT: Extending pre-trained models with\ndomain-speciﬁc vocabulary under constrained training resources. In Findings of the Association for Computational\nLinguistics: EMNLP 2020, pages 1433–1439, Online, November 2020. Association for Computational Linguistics.\n[31] Ramya Vunikili, HN Supriya, Vasile George Marica, and Oladimeji Farri. Clinical ner using spanish bert\nembeddings. In IberLEF@ SEPLN, pages 505–511, 2020.\n[32] Ying Xiong, Yuanhang Huang, Qingcai Chen, Xiaolong Wang, Yuan Nic, and Buzhou Tang. A joint model for\nmedical named entity recognition and normalization. In Proceedings of the Iberian Languages Evaluation Forum\n(IberLEF 2020), CEUR Workshop Proceedings, 2020.\n[33] Ying Xiong, Yedan Shen, Yuanhang Huang, Shuai Chen, Buzhou Tang, Xiaolong Wang, Qingcai Chen, Jun Yan,\nand Yi Zhou. A deep learning-based system for PharmaCoNER. In Proceedings of The 5th Workshop on BioNLP\nOpen Shared Tasks, pages 33–37, Hong Kong, China, November 2019. Association for Computational Linguistics.\n11",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7848514318466187
    },
    {
      "name": "Vocabulary",
      "score": 0.7330409288406372
    },
    {
      "name": "Named-entity recognition",
      "score": 0.6625282764434814
    },
    {
      "name": "Biomedicine",
      "score": 0.6515613794326782
    },
    {
      "name": "Language model",
      "score": 0.6513593792915344
    },
    {
      "name": "Natural language processing",
      "score": 0.6343513131141663
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5751860737800598
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.5663010478019714
    },
    {
      "name": "Transformer",
      "score": 0.547452986240387
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5051775574684143
    },
    {
      "name": "Machine learning",
      "score": 0.26240044832229614
    },
    {
      "name": "Linguistics",
      "score": 0.17569711804389954
    },
    {
      "name": "Bioinformatics",
      "score": 0.07037454843521118
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2799803557",
      "name": "Barcelona Supercomputing Center",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I9617848",
      "name": "Universitat Politècnica de Catalunya",
      "country": "ES"
    }
  ],
  "cited_by": 23
}