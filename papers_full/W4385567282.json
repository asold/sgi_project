{
  "title": "SpaBERT: A Pretrained Language Model from Geographic Data for Geo-Entity Representation",
  "url": "https://openalex.org/W4385567282",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2133641497",
      "name": "Zekun Li",
      "affiliations": [
        "Twin Cities Orthopedics",
        "University of Minnesota"
      ]
    },
    {
      "id": "https://openalex.org/A2128998484",
      "name": "Jina Kim",
      "affiliations": [
        "University of Minnesota",
        "Twin Cities Orthopedics"
      ]
    },
    {
      "id": "https://openalex.org/A4202309607",
      "name": "Yao-Yi Chiang",
      "affiliations": [
        "University of Minnesota",
        "Twin Cities Orthopedics"
      ]
    },
    {
      "id": "https://openalex.org/A2656641051",
      "name": "Muhao Chen",
      "affiliations": [
        "University of Southern California"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2995950742",
    "https://openalex.org/W3035140194",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W3168090480",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2901165057",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W4205451033",
    "https://openalex.org/W3034503357",
    "https://openalex.org/W3200514825",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3104415840",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W3040157551",
    "https://openalex.org/W4280586943",
    "https://openalex.org/W4283388932",
    "https://openalex.org/W3213454282",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W3035231859",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2982041904",
    "https://openalex.org/W4286905627",
    "https://openalex.org/W3035101152",
    "https://openalex.org/W3127711473",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W3197798882",
    "https://openalex.org/W2952437275",
    "https://openalex.org/W3207976211",
    "https://openalex.org/W3125675327",
    "https://openalex.org/W2776048679",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W3006775836",
    "https://openalex.org/W3158303960",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3036120435",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W3034837210",
    "https://openalex.org/W3206375861",
    "https://openalex.org/W2750303327",
    "https://openalex.org/W3105486918",
    "https://openalex.org/W4221009220",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W3002535593",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W3167118264"
  ],
  "abstract": "Named geographic entities (geo-entities for short) are the building blocks of many geographic datasets. Characterizing geo-entities is integral to various application domains, such as geo-intelligence and map comprehension, while a key challenge is to capture the spatial-varying context of an entity. We hypothesize that we shall know the characteristics of a geo-entity by its surrounding entities, similar to knowing word meanings by their linguistic context. Accordingly, we propose a novel spatial language model, SpaBERT, which provides a general-purpose geo-entity representation based on neighboring entities in geospatial data. SpaBERT extends BERT to capture linearized spatial context, while incorporating a spatial coordinate embedding mechanism to preserve spatial relations of entities in the 2-dimensional space. SpaBERT is pretrained with masked language modeling and masked entity prediction tasks to learn spatial dependencies. We apply SpaBERT to two downstream tasks: geo-entity typing and geo-entity linking. Compared with the existing language models that do not use spatial context, SpaBERT shows significant performance improvement on both tasks. We also analyze the entity representation from SpaBERT in various settings and the effect of spatial coordinate embedding.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2757–2769\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nSPABERT: A Pretrained Language Model from Geographic Data for\nGeo-Entity Representation\nZekun Li1, Jina Kim1, Yao-Yi Chiang1, Muhao Chen2\n1Department of Computer Science and Engineering, University of Minnesota, Twin Cities\n2Department of Computer Science, University of Southern California\n{li002666,kim01479,yaoyi}@umn.edu, muhaoche@usc.edu\nAbstract\nNamed geographic entities (geo-entities for\nshort) are the building blocks of many geo-\ngraphic datasets. Characterizing geo-entities\nis integral to various application domains, such\nas geo-intelligence and map comprehension,\nwhile a key challenge is to capture the spatial-\nvarying context of an entity. We hypothesize\nthat we shall know the characteristics of a geo-\nentity by its surrounding entities, similar to\nknowing word meanings by their linguistic con-\ntext. Accordingly, we propose a novel spatial\nlanguage model, SPABERT (\n ), which pro-\nvides a general-purpose geo-entity representa-\ntion based on neighboring entities in geospa-\ntial data. SPABERT extends BERT to cap-\nture linearized spatial context, while incorpo-\nrating a spatial coordinate embedding mech-\nanism to preserve spatial relations of entities\nin the 2-dimensional space. SPABERT is pre-\ntrained with masked language modeling and\nmasked entity prediction tasks to learn spatial\ndependencies. We apply SPABERT to two\ndownstream tasks: geo-entity typing and geo-\nentity linking. Compared with the existing lan-\nguage models that do not use spatial context,\nSPABERT shows significant performance im-\nprovement on both tasks. We also analyze the\nentity representation from SPABERT in vari-\nous settings and the effect of spatial coordinate\nembedding.\n1 Introduction\nInterpreting human behaviors requires consider-\ning human activities and their surrounding en-\nvironment. Looking at a stopping location,\n[ Speedway ,],1 from a person’s trajectory, we\nmight assume that this person needs to use the loca-\ntion’s amenities if Speedway implies a gas station,\nand is near a highway exit. We might predict a\nmeetup at [ Speedway ,] if the trajectory travels\nthrough many other locations, , , ..., of the same\n1A geographic entity name Speedway and its loca-\ntion (e.g., latitude and longitude). Best viewed in color.\nname, Speedway , to arrive at [ Speedway ,] in\nthe middle of farmlands. As humans, we are able\nto make such inferences using the name of a ge-\nographic entity (geo-entity) and other entities in\na spatial neighborhood. Specifically, we contex-\ntualize a geo-entity by a reasonable surrounding\nneighborhood learned from experience and, from\nthe neighborhood, relate other relevant geo-entities\nbased on their name and spatial relations (e.g., dis-\ntance) to the geo-entity. This way, even if two gas\nstations have the same name (e.g., Speedway ) and\nentity type (e.g., ‘gas station’), we can still reason\nabout their spatially varying semantics and use the\nsemantics for prediction.\nCapturing this spatially varying location seman-\ntics can help recognizing and resolving geospa-\ntial concepts (e.g., toponym detection, typing and\nlinking) and the grounding of geo-entities in doc-\numents, scanned historical maps, and a variety\nof knowledge bases, such as Wikidata, Open-\nStreetMap, and GeoNames. Also, the location se-\nmantics can support effective use of spatial textual\ninformation (geo-entities names) in many spatial\ncomputing task, including moving behavior detec-\ntion from visiting locations of trajectories (Yue\net al., 2021, 2019), point of interest recommen-\ndations (Yin et al., 2017; Zhao et al., 2022), air\nquality (Lin et al., 2017, 2018, 2020; Jiang et al.,\n2019) and traffic prediction (Yuan and Li, 2021;\nGao et al., 2019) using location context.\nRecently, the research community has seen a\nrapid advancement in pretrained language mod-\nels (PLMs) (Devlin et al., 2019; Liu et al., 2019;\nLewis et al., 2020; Sanh et al., 2019), which sup-\nports strong contextualized language representa-\ntion abilities (Lan et al., 2020) and serves as the\nbackbones of various NLP systems (Rothe et al.,\n2020; Yang et al., 2019). The extensions of these\nPLMs help NL tasks in different data domains (e.g.,\nbiomedicine (Lee et al., 2020; Phan et al., 2021),\nsoftware engineering (Tabassum et al., 2020), fi-\n2757\nFigure 1: Overview for generating the pivot entity representation, Ep ( : pivot entity; : neighboring geo-entities).\nSPABERT sorts and concatenate neighboring geo-entities by their distance to pivot in ascending order to form\na pseudo-sentence. [CLS] is prepended at the beginning. [SEP] separates entities. SPABERT generates token\nrepresentations and aggregates representations of pivot tokens to produce Ep.\nnance (Liu et al., 2021)) and modalities (e.g., tables\n(Herzig et al., 2020; Yin et al., 2020; Wang et al.,\n2022) and images (Li et al., 2020a; Su et al., 2019)).\nHowever, it is challenging to adopt existing PLMs\nor their extensions to capture geo-entities’ spatially\nvarying semantics. First, geo-entities exist in the\nphysical world. Their spatial relations (i.e., dis-\ntance and orientation) do not have a fixed structure\n(e.g., within a table or along a row of a table) that\ncan help contextualization. Second, existing lan-\nguage models (LMs) pretrained on general domain\ncorpora (Devlin et al., 2019; Liu et al., 2019) re-\nquire fine-tuning for domain adaptation to handle\nnames of geo-entities.\nTo tackle these challenges, we present\nSPABERT (\n ), a LM that captures the spatially\nvarying semantics of geo-entity names using large\ngeographic datasets for entity representation. Built\nupon BERT (Devlin et al., 2019), SPABERT\ngenerates the contextualized representation\nfor a geo-entity of interest (referred to as the\npivot), based on its geographically nearby geo-\nentities. Specifically, SPABERT linearizes the\n2-dimensional spatial context by forming pseudo\nsentences that consist of names of the pivot and\nneighboring geo-entities, ordered by their spatial\ndistance to the pivot. SPABERT also encodes the\nspatial relations between the pivot and neighboring\ngeo-entities with a continuous spatial coordinate\nembedding. The spatial coordinate embedding\nmodels horizontal and vertical distance relations\nseparately and, in turn, can capture the orientation\nrelations. These techniques make the inputs\ncompatible with the BERT-family structures.\nIn addition, the backbone LM, BERT, in\nSPABERT is pretrained with general-purpose cor-\npora and would not work well directly on geo-\nentity names because of the domain shift. We\nthus train SPABERT using pseudo sentences gen-\nerated from large geographic datasets derived from\nOpenStreetMap (OSM).2 This pretraining process\nconducts Masked Language Modeling (MLM) and\nMasked Entity Prediction (MEP) that randomly\nmasks subtokens and full geo-entity names in the\npseudo sentences, respectively. MLM and MEP\nenable SPABERT to learn from pseudo sentences\nfor generating spatially varying contextualized rep-\nresentations for geo-entities.\nSPABERT provides a general-purpose represen-\ntation for geo-entities based on their spatial context.\nSimilar to linguistic context, spatial context refers\nto the surrounding environment of a geo-entity.\nFor example, [ Speedway ,], [ Speedway ,], and\n[ Speedway ,] would have different representa-\ntions since the surrounding environment of , ,\nand could vary. We evaluateSPABERT on two\ntasks: 1) geo-entity typing and 2) geo-entity linking\nto external knowledge bases. Our analysis includes\nthe performance comparison of SPABERT in var-\nious settings due to characteristics of geographic\ndata sources (e.g., entity omission).\nTo summarize, this work has the following con-\ntributions. We propose an approach to linearize\nthe 2-dimensional spatial context, encode the geo-\nentity spatial relations, and use a LM to pro-\nduce spatial varying feature representations of geo-\nentities. We show that SPABERT is a general-\npurpose encoder by supporting geo-entity typing\nand geo-entity linking, which are keys to effec-\ntively integrating large varieties of geographic data\nsources, the grounding of geo-entities as well as\nsupports for a broad range of spatial computing\napplications. The experiments demonstrate that\nSPABERT is effective for both tasks and outper-\nforms the SOTA LMs.\n2OpenStreetMap: https://www.openstreetmap.org/\n2758\n2 S PABERT\nSPABERT is a LM built upon a pretrained BERT\nand further trained to produce contextualized\ngeo-entity representations given large geographic\ndatasets. Fig. 1 shows the outline of SPABERT,\nwith the details below. This section first presents\nthe preliminary (§2.1) and then describes the over-\nall approach for learning geo-entities’ contextual-\nized representations (§2.2), the pretraining strate-\ngies (§2.3), and inference procedures (§2.4).\n2.1 Preliminary\nWe assume that given a geographic dataset (e.g.,\nOSM) with many geo-entities, S = {g1, g2, ..., gl},\neach geo-entity gi (e.g., [ Speedway ,]) has two\nattributes: name gname( Speedway ) and location\ngloc(). The location attribute, gloc, is a tuple\ngloc = ( glocx, glocy, ...) that identifies the loca-\ntion in a coordinate system (e.g., x and y image\npixel coordinates or latitude and longitude geo-\ncoordinates with altitudes). WLOG, here we as-\nsume a 2-dimensional space. SPABERT aims to\ngenerate a contextualized representation for each\ngeo-entity gi in S given its spatial context. We\ndenote a geo-entity that we seek to contextual-\nize as the pivot entity, gp, p for short. The spa-\ntial context of p is SC(p) = {gn1 , ..., gnk }where\ndistance(p, gnk ) < T. T is a spatial distance pa-\nrameter defining a local area. We call gn1 , ..., gnk\nas p’s neighboring geo-entities and denote them as\nn1, ..., nk when there is no confusion.\n2.2 Contextualizing Geo-entities\nLinearizing Neighboring Geo-entity Names For\na pivot, p, SPABERT first linearizes its neighbor-\ning geo-entitie names to form a BERT-compatible\ninput sequence, called a pseudo sentence . The\ncorresponding pseudo sentence for the example in\nFig. 1 is constructed as:\n[CLS] University of Minnesota [SEP]\nMinneapolis [SEP] St. Anthony Park [SEP]\nBloom Island Park [SEP] Bell Museum [SEP]\nThe pseudo sentence starts with the pivot name\nfollowed by the names of the pivot’s neighboring\ngeo-entities, ordered by their spatial distance to the\npivot in ascending order. The idea is that nearby\ngeo-entities are more related (for contextualization)\nthan distant geo-entities. SPABERT also tokenizes\nthe pseudo sentences using the original BERT to-\nkenizer with the special token [SEP] to separate\nentity names. The subtokens of a neighbor nk is\ndenoted as Tnk\nj as in Fig. 2.\nEncoding Spatial Relations SPABERT adopts\ntwo types of position embeddings in addtion to the\ntoken embeddings in the pseudo sentences (Fig. 2).\nThe sequence position embedding represents the\ntoken order, same as the original position embed-\nding in BERT and other Transformer LMs. Also,\nSPABERT incorporates a spatial coordinate em-\nbedding mechanism, which seeks to represent the\nspatial relations between the pivot and its neigh-\nboring geo-entities. SPABERT’s spatial coordinate\nembeddings encode each location dimension sep-\narately to capture the relative distance and orien-\ntation between geo-entities. Specifically, for the\n2-dimensional space, SPABERT generates normal-\nized distance distnk\nx and distnk\ny for each neighbor-\ning geo-entity using the following equations:\ndistnk\nx = (glocx\nnk −glocx\np )/Z\ndistnk\ny = (glocy\nnk −glocy\np )/Z\nwhere Z is a normalization factor, and (glocx\np , glocy\np ),\n(glocx\nnk , glocy\nnk ) are the locations of pivot p and neigh-\nboring entity nk. Note that the tokens belong to\nthe same geo-entity name have the same distnk\nx\nand distnk\ny . Also, SPABERT uses DSEP, a constant\nnumerical value larger than max(distnk\nx , distnk\ny )\nfor all neighboring entities to differentiate special\ntokens from entity name tokens.\nSPABERT encodes distnk\nx and distnk\ny using a\ncontinuous spatial coordinate embedding layer with\nreal-valued distances as input to preserve the con-\ntinuity of the output embeddings. Let Snk be the\nspatial coordinate embedding of the neighbor en-\ntity nk, M be the embedding’s dimension, and\nSnk ∈RM . We define Snk as:\nS(m)\nnk =\n{\nsin(distnk /100002j/M ), m= 2j\ncos(distnk /100002j/M ), m= 2j + 1\nwhere S(m)\nnk is the m-th component of Snk . distnk\nrepresents distnk\nx and distnk\ny for the spatial coordi-\nnate embeddings along the horizontal and vertical\ndirections, respectively.\nThe token embedding, sequence position embed-\nding and spatial coordinate embedding are summed\nup then fed into the encoder. Similar to BERT,\nSPABERT encoder calculates an output embed-\nding for each token. Then SPABERT averages the\n2759\nFigure 2: Embedding modules. Inputs to the token-embedding are the tokenized geo-entity names in the pseudo-\nsentence. Inputs to the sequence position embedding are the sequence indices of the tokens. Inputs to the spatial\ncoordinate embedding are the normalized distances from the pivot in each spatial dimension (details in §2.2).\npivot’s token-level embeddings to produce a fixed-\nlength embedding for the pivot’s contextualized\nrepresentation.\n2.3 Pretraining\nWe train SPABERT with two tasks to adapt the pre-\ntrained BERT backbone to geo-entity pseudo sen-\ntences. One task is the masked language modeling\n(MLM) (Devlin et al., 2019), for which SPABERT\nneeds to learn how to complete the full names of\ngeo-entities from pseudo sentences with randomly\nmasked subtokens using the remaining subtokens\nand their spatial coordinates (i.e., partial names and\nspatial relations between subtokens). The block be-\nlow shows an example of the masked input for\nMLM, where ### are the masked subtokens.\n[CLS] ### of Minnesota [SEP] Minneapolis\n[SEP] St. ### Park [SEP] ### Island Park\n### Bell Museum [SEP]\nIn addition, we hypothesize that given common\nspatial co-occurrence patterns in the real world,\none can use neighboring geo-entities to predict the\nname of a geo-entity. Therefore, we propose and\nincorporate a masked entity prediction (MEP) task,\nwhich randomly masks all subtokens of an entity\nname in a pseudo sentence. For MEP, SPABERT\nrelies on the masked entity’s spatial relation to\nneighboring entities to recover the masked name.\nThe block below shows an example of the masked\ninput for MEP.\n[CLS] University of Minnesota [SEP]\nMinneapolis [SEP] ### ### ### [SEP] Bloom\nIsland Park [SEP] Bell Museum [SEP]\nBoth MLM and MEP have a masking rate of 15%\n(without masking [CLS] and [SEP]). The sequence\nposition and spatial coordinates are not masked.\nPretraining Data We construct a large training set\nfrom OSM covering the City of London and the\nState of California. Since OSM is crowd-sourced,\nwe clean the raw data by removing non-alpha-\nnumeric place names and geo-entities that do not\nFigure 3: Example of the Geohash representations with\na pivot entity named ‘Kenneth H Keller Hall’ and its\nneighboring entities from OpenStreetMap. ( : pivot\nentity; : neighboring entities; : entities not within the\nnearest nine grids). The upper hash grids have a string\nlength of six, and the lower ones have a length of seven.\nhave a place name or geocoordinates. We randomly\nselect geo-entities as pivots and construct pseudo\nsentences as described in §2.2.\nTo efficiently find the neighboring entities from a\npivot sorted by distances, we leverage the Geohash\nalgorithm introduced by Gustavo Niemeyer in 2008.\nWe first generate the Geohash string for geo-entities\non OpenStreetMap, which encodes a location into\na string of a maximum length of 20 with base 32.\nFor example, a city ‘Madelia’ with the geolocation\n(44.0508° N, 94.4183° W) has a Geohash string of\n‘9zufe7nwjefpjksty0m8’. The length of the hash\nstring is associated with the size of the geographic\narea that the string represents. The first character\ndivides the entire Earth’s surface into 32 large grids,\nand the second character further divides one large\ngrid into 32 grids. Thus, by comparing the leading\n2760\ncharacters of two hash strings, we can estimate the\nspatial proximity of two locations without calcu-\nlating the exact distance between them. We use\nthe hash strings to first filter out the geo-entities\nguaranteed to be outside the spatial context radius\nand keep the neighbors within the nearest nine hash\ngrids for distance calculation. The use of Geohash\nreduces the computation time when constructing\nthe pseudo sentences. Fig. 3 shows an example of\nGeohash representations. Given the pivot ‘Kenneth\nH Keller Hall’, we can calculate its hash string of\ndifferent lengths to quickly retrieve the neighboring\nentities within a desired range (i.e., the green box).\nThis way, we generate 69,168 and 148,811\npseudo sentences in London and California, respec-\ntively, leading to a pretraining corpus of around\n150M words. We use all of them for pretraining\nSPABERT with MLM and MEP.\n2.4 Inference\nThe pretrained SPABERT can already generate spa-\ntially varying contextualized representations for\ngeo-entities. The representations support various\ndownstream applications, including contextualized\ngeo-entity classification and similarity-based infer-\nence tasks, such as geo-entity typing and linking.\nContextualized Geo-entity ClassificationClassifi-\ncation of a geo-entity is crucial for recognizing and\nresolving geospatial concepts and the grounding\nof geo-entities in various data sources. Here, we\nuse geo-entity typing as an example task to demon-\nstrate the effectiveness of contextualized geo-entity\nrepresentations. In this task, we aim to predict the\ngeo-entity’s semantic type (e.g., transportation and\nhealthcare). We stack a softmax prediction head on\ntop of the final-layer hidden states (i.e., geo-entity\nembeddings) to perform the classification.\nSimilarity-based Inference Integrating multi-\nsource geographic datasets often involves finding\nthe top K nearest geo-entities in some representa-\ntion space. One example task, which we refer to as\ngeo-entity linking, is to link geo-entities from a ge-\nographic information system (GIS) oriented dataset\nto graph-based knowledge bases (KBs). In such a\ntask, we can directly use the contextualized geo-\nentity embeddings from SPABERT and calculate\nthe embedding similarity based on some metrics,\nsuch as the cosine distance, to match the corre-\nsponding geo-entities from separate data sources.\n3 Experiments\nWe evaluate SPABERT on supervised geo-entity\ntyping and unsupervised geo-entity linking tasks\n(§3.1-§3.3). We also investigate how SPABERT\nperforms under various common characteristics of\ngeographic data sources (e.g., entity omission from\ncartographic generalization) and how critical tech-\nnical components and their parameters affect the\nperformance (§3.4).\n3.1 Experimental Setup\nDatasets Open Street Map (OSM)2 is a large-scale\ngeographic database, and it is widely used in many\npopular map-related services. For supervised geo-\nentity typing, we randomly select 25,872 and 7,726\npivot geo-entities together with their neighboring\nentities from point features of nine semantic types\non OSM in the State of California and the City of\nLondon, respectively (Tab. 1). Each geo-entity is\nassociated with a name and its geocoordinates. We\nuse 80% of the data in both regions for training and\n20% for testing. The task is to predict the OSM\nsemantic type for each geo-entity. (§3.2).\nFor unsupervised geo-entity linking , we ran-\ndomly select 14 scanned historical maps in Cal-\nifornia from the United States Geological Survey\n(USGS) Historical Topographic Maps Collection.\nWe manually transcribe text labels in these maps to\ngenerate 154 geo-entities, each containing a name\n(from the text label) and the image coordinates (text\nlabel’s pixel location in the scanned map) The task\nis to find the corresponding Wikidata geo-entity3\nfor each USGS geo-entity with only their spatial\nrelations from pixel locations. Note that the geoco-\nordinates of geo-entities in the scanned maps are\nunknown. We select 15K Wikidata geo-entities\nhaving an identical name to one of the USGS geo-\nentities4 and then randomly add another 30K Wiki-\ndata geo-entities to construct the final Wikidata\ndataset of 45K geo-entities. For the ground truth,\nwe manually identify the matched Wikidata geo-\nentity for each USGS geo-entity.\nFor geo-entity linking task, the annotation de-\ntails are described below. We hire three undergrad-\nuate students (not the co-authors) to transcribe text\nlabels on the USGS maps. They draw bounding\nboxes/polygons around text labels and transcribe\n3Entities with the coordinate location attribute ( https:\n//www.wikidata.org/wiki/Property:P625)\n4Two geo-entities can have the same name (e.g., Los An-\ngeles, CA vs. Los Angeles, TX)\n2761\nClasses California London\nEducation 6,222 618\nEntertainment_Arts_Culture 1,380 601\nFacilities 574 179\nFinancial 2,590 769\nHealthcare 3,779 1,779\nPublic_Service 2,658 393\nSustenance 4,276 1,693\nTransportation 4,226 1,618\nWaste_Management 167 76\nTotal 25,872 7,726\nTable 1: OSM Geo-entity typing dataset statistics. The\nfirst column shows the nine OSM semantic types, fol-\nlowing by the numbers of samples for each class in\nCalifornia and London.\nthe text string to indicate geo-entity names and their\nlocations. The results are verified and corrected by\nanother undergraduate student and one Ph.D. stu-\ndent. Then, we run an automatic script to find\nthe potential corresponding geo-entity in Wikidata\nby matching their names. The collected Wikidata\nURIs are noisy, and we have one Ph.D. student\nverify the URIs by marking them as True or False\nlinking. The verification results are randomly fur-\nther reviewed by a senior digital humanities scholar\nspecialized in the representation and reception of\nhistorical places.\nModel Configuration We create two vari-\nants of SPABERT, namely SPABERTbase and\nSPABERTlarge with weights and tokenizers initial-\nized from the uncased versions of the BERTbase\nand BERTlarge, respectively. During pretraining,\nwe mix the masked instances for MLM and MEP\ntasks in the same ratio. We primarily use the same\noptimization parameters as in (Devlin et al., 2019)\nand train the model with the AdamW optimizer.\nWe save the model checkpoints every 2K iterations.\nThe learning rate is5×10−5 for SPABERTBase and\n1 ×10−6 for SPABERTLarge for stability. Distance\nnormalization factor Z is 0.0001, and distance sep-\narator DSEP is 20. The batch size is 12 to fit in one\nNVIDIA RTX A5000 GPU with 24GB memory.\n3.2 Supervised Geo-Entity Typing\nTask Description We tackle geo-entity typing as\na supervised classification task using the modified\nSPABERT architecture in §2.4 and finetune the\nmodel end-to-end by optimizing the cross-entropy\nloss. Specifically, the training set of this task con-\ntains {(gi, SC(gi), yi)}N\ni=1, where gi is the pivot\nentity; yi is its OSM semantic type label; SC(gi)\ncontains its neighboring entities in the nearest nine\nFigure 4: TSNE visualization for entity features pro-\nduced by SPABERTBase. The color indicates the\nground-truth labels. Financial and Public_Service\n(pointed by the arrows) are well separated from other\nclasses which conforms to the high F1 scores in Tab. 2\n(best viewed in color).\nhash grids. Note that during training, the model\ndoes not use the neighboring geo-entities’ OSM se-\nmantic types. We report the F1 score for each class\n(i.e., an OSM semantic type) and the micro-F1 for\nall samples in the test set.\nModel Comparison We compare SPABERT with\nseveral strong PLMs, including BERT (Devlin\net al., 2019), RoBERTa (Liu et al., 2019), Span-\nBERT (Joshi et al., 2020), LUKE (Yamada et al.,\n2020), and SimCSE (Gao et al., 2021). For all\nthese models, we use the pretrained weights with\nthe uncased version. For SimCSE, we use both\nthe BERT and RoBERTa versions with the weights\nobtained from unsupervised training. We further\nfinetune these baseline LMs end-to-end on the same\npseudo-sentence dataset as SPABERT (Tab. 1) with\na softmax prediction head appended after the base-\nline model. Note that these baseline models only\nuse the token and position embeddings but not the\nspatial coordinate embeddings.\nResult Analysis Tab. 2 summarizes the geo-entity\ntyping results on the OSM test set with finetuned\nbaselines and SPABERT. The last column shows\nthe micro-F1 scores (per-instance average). The\nremaining columns are the F1 scores for individual\nclasses. For all baseline models, BERT Base and\nSimCSEBERT-Large (BERTLarge as a close second)\nhave the highest F1 in the base and large groups,\nrespectively. In addition, SPABERT shows the best\nperformance over the existing context-aware LMs\nfor both base and large groups. We hypothesize\nthat SPABERT’s advantage comes from 1) the pre-\ntraining tasks, especially MEP, and 2) the spatial\n2762\nClasses → Edu. Ent. Fac. Fin. Hea. Pub. Sus. Tra. Was. Micro Avg\nBERTBase .674 .634 .763 .929 .856 .872 .856 .862 .678 .835\nRoBERTaBase .626 .627 .605 .951 .869 .818 .838 .850 .475 .820\nSpanBERTBase .633 .589 .608 .916 .859 .882 .824 .867 .735 .819\nLUKEBase .648 .608 .598 .945 .857 .867 .854 .851 .517 .825\nSimCSEBERT-Base .623 .590 .504 .925 .867 .852 .857 .810 .470 .810\nSimCSERoBERTa-Base .621 .629 .499 .951 .841 .853 .828 .856 .500 .814\nSPABERTBase .674 .653 .680 .959 .865 .900 .883 .888 .703 .852\nBERTLarge .707 .661 .647 .937 .874 .850 .873 .864 .526 .841\nRoBERTaLarge .657 .626 .682 .907 .855 .805 .831 .859 .587 .817\nSpanBERTLarge .683 .652 .661 .931 .868 .853 .851 .848 .624 .829\nLUKELarge .665 .607 .660 .899 .855 .809 .813 .844 .587 .808\nSimCSEBERT-Large .693 .661 .713 .940 .880 .871 .864 .867 .564 .844\nSimCSERoBERTa-Large .683 .630 .648 .916 .865 .802 .807 .848 .587 .811\nSPABERTLarge .731 .690 .710 .956 .901 .892 .893 .903 .677 .871\nTable 2: Geo-entity typing with the state-of-the-art LMs and SPABERT. Column names are the OSM classes. Bold\nnumbers are the highest scores in each column. Underlined numbers are the highest scores among baselines.\ncoordinate embeddings. Additionally, compared\nto its backbone, BERT, SPABERT shows perfor-\nmance improvement in almost all classes for both\nbase and large models. The results demonstrate\nthat SPABERT can effectively contextualize geo-\nentities with their spatial context.\nWe can also observe that the Financial class\nhas high F1 scores for all models. The reason\nis that when entity names are strongly associated\nwith the semantic type (e.g., U.S.Bank), pretrained\nLMs could produce meaningful entity represen-\ntation even without the spatial context. How-\never, the typing performance can still be further\nimproved after encoding the spatial context with\nSPABERT. Fig. 4 visualizes the geo-entity features\nfrom SPABERTBase. It shows that Financial and\nPublic_Service have the most separable features\nfrom other classes. The non-separable region in the\nmiddle is mainly due to the similar spatial context\nand semantics of geo-entities (e.g. “nursing home\"\ncan be close to both Facilities and Healthcare).\n3.3 Unsupervised Geo-Entity Linking\nTask Description We define the problem as fol-\nlows. For a query set Q = {(qi, SC(qi))}|Q|\ni=1, the\ngoal is to find the corresponding geo-entity for each\nqi in the candidate set C = {(ci, SC(ci))}|C|\ni=1 (typ-\nically |C|≫| Q|). Here, Q and C are the USGS\nand Wikidata geo-entities (§3.1). SC(qi) contains\nqi’s nearest 20 geo-entities, and SC(ci) includes\nci’s neighboring geo-entities within a 5km radius.5\nFor this task, we use the evaluation metrics of Re-\ncall@K and Mean-Reciprocal-Rank (MRR).\nModel Comparison We use the same baseline\n5Because Q is not georeferenced, we use the top K nearest\nneighbors instead of pixel distances.\nModel MRR R@1 R@5 R@10\nBERTBase .400 .289 .559 .635\nRoBERTaBase .326 .232 .446 .540\nSpanBERTBase .164 .138 .201 .213\nLUKEBase .306 .188 .440 .547\nSimCSEBERT-Base .453 .371 .547 .628\nSimCSERoBERTa-Base .227 .188 .264 .301\nSPABERTBase .515 .338 .744 .850\nBERTLarge .337 .245 .459 .509\nRoBERTaLarge .379 .220 .603 .704\nSpanBERTLarge .229 .176 .308 .339\nLUKELarge .402 .232 .635 .767\nSimCSEBERT-Large .475 .402 .559 .616\nSimCSERoBERTa-Large.214 .176 .239 .283\nSPABERTLarge .537 .383 .744 .864\nTable 3: Geo-entity linking result. Bold and underlined\nnumbers are the highest scores in each column and the\nhighest scores among the baselines, respectively.\nmodels as in the typing task without finetuning.\nResult Analysis Tab. 3 shows the geo-entity link-\ning results. Large models perform better than their\nbase counterparts for both baseline models and\nSPABERT. In particular, among the baseline mod-\nels, SimCSEBERT has the highest score for MRR\nand R@1. SPABERT has significantly higher R@5\nand R@10 than SimCSE. Also, SPABERT outper-\nforms all baselines on MRR, R@5, and R@10. For\nboth base and large versions, SPABERT outper-\nforms its backbone model BERT. The difference\nbetween BERT and SPABERT shows the effec-\ntiveness of the spatial coordinate embeddings and\ndomain adaptation using MLM and the proposed\nMEP. Also, SpanBERT shows the worst perfor-\nmance among all models. This could be that Span-\nBERT utilizes masked random spans in pretraining,\nwhich does not work well for pseudo sentences\nof geo-entity names. In contrast, SPABERT’s\nMEP specifically masks tokens from the same\n2763\nMap Set Scale ⇓ MRR R@1 R@5 R@10\n15-CA 1:62500 0.503 0.422 0.680 0.914\n30-CA 1:125000 0.639 0.599 0.862 0.932\n60-CA 1:250000 0.404 0.133 0.678 0.800\nTable 4: Impact of entity omission for linking (USGS\nand Wikidata). Results from SPABERTLarge. 1:625000\nmeans 1 centimeter on a map represents 625 meters in\nthe physical world.\ngeo-entity and could effectively contextualize geo-\nentity names distributed in the 2D space with the\nspatial coordinate embeddings.\n3.4 Spatial Context and Ablation Study\nImpact of Entity Omission for Linking We ana-\nlyze how SPABERT handles different USGS map\nscales for geo-entity linking with entity omission\nfrom cartographic generalization (e.g., some enti-\nties only exist in certain scales). Here SPABERT\nperforms the best for the 1:125K maps (30-CA)\n(Tab. 4). Our hypothesis is that the 1:125K maps\ncontain denser geo-entities than the test maps at\nother scales. When selecting the top K nearest\nneighbors for a pivot in other map scales, some\nneighbors could be far away and would not provide\nrelevant context.\nSince the entity omission criteria of the USGS\nmaps are unknown and Wikidata mostly contain\nimportant landmark geo-entities, we also simulate\nomission using the OSM dataset by gradually re-\nmoving random neighbors of a pivot entity within\na fixed neighborhood. We test this scenario for geo-\nentity linking to see if the same pivot entity would\nhave similar representations from decreasing neigh-\nbor densities (i.e., the original set of neighbors vs.\nafter random omission at varying rates). We use the\nsame evaluation metric as in §3.3, as they reflect\nthe similarity of the learned geo-entity represen-\ntation (Fig. 5). The linking scores decrease with\nadditional entities removed, indicating the similar-\nity between the same pivot entity’s representations\nlearned from the original data and the omitted data\ndeclines when omission percentage becomes larger.\nThe elbow point is at about 30%, showing that\nSPABERT is reasonably robust if the percentage\nof the removed neighbors is within 30%.\nImpact of Pseudo Sentence Length We analyze\nthe impact of pseudo sentence length on geo-entity\ntyping by varying the number of neighboring en-\ntities. Tab. 5 shows the results of SPABERTBase\nand SPABERTLarge with an increasing number of\nneighbors. We observe that as #Neighbor increases,\n#Neighbor 1 2 3 4 5 6 7 8 9\nBase .773 .808 .814 .827 .835 .831 .835 .836 .840\nLarge .795 .822 .834 .838 .843 .847 .852 .848 .854\n#Neighbor 10 20 30 40 50 60 70 80 90\nBase .843 .844 .846 .851 .852 .850 .849 .851 .852\nLarge .850 .857 .862 .858 .858 .868 .863 .867 .869\nTable 5: Impact of the pseudo sentence length. Num-\nbers in the table are micro-F1 scores on the OSM typing\ndataset. Base and Large are short for SPABERTBase\nand SPABERTLarge.\nFigure 5: Geo-entity feature similarity with a decreasing\nneighbor density. X-axis indicates the percentage of the\nneighbors removed for the pivot entities.\nthe micro-F1 score increases accordingly, as ex-\npected. The increment is more evident at the be-\nginning, illustrating that the spatial context helps\ncontextualize the pivot entity, especially in a lo-\ncal neighborhood. The benefit of adding additional\nneighboring entities decreases as distance increases\nbeyond a certain point. This conforms to the well-\nstudied spatial autocorrelation.\nEffect of Spatial Coordinate Embedding We\ntrain two additional SPABERT variants that do\nnot include the spatial coordinate embedding dur-\ning MLM and MEP pretraining and use them\nfor geo-entity linking. For MRR, SPABERTBase\nand SPABERTLarge drop from 0.515 to 0.458 and\nfrom 0.537 to 0.478 compared to their original\nSPABERT version. Also, the variants have lower\nrecall scores compared to their original SPABERT\nversion. The most significant drop in the recall is\non R@1 from 0.383 to 0.283 for SPABERTLarge.\nThe results demonstrate the effectiveness of the\nspatial coordinate embedding.\n4 Related Work\nPretrained Language Models PLMs have been\nthe dominant paradigm for language representa-\ntion. Following the success of MLMs (Devlin et al.,\n2019; Liu et al., 2019) and autoregressive PLMs\n2764\n(Peters et al., 2018; Radford et al., 2019), more re-\ncent work has extended the pretraining process with\nmore tasks or learning objectives for span predic-\ntion (Joshi et al., 2020), cross-encoders (Reimers\nand Gurevych, 2019), contrastive learning (Gao\net al., 2021), and massively multi-task learning\n(Raffel et al., 2020). To support entity representa-\ntion, several approaches propose to perform men-\ntion detection (Yamada et al., 2020), incorporate\nmention memory cells (de Jong et al., 2021), or in-\njecting structural knowledge representations (Wang\net al., 2021; Zhang et al., 2019; Peters et al., 2019;\nZhou et al., 2022). Due to the large body of work,\nwe refer readers to recent surveys summarizing this\nline of work (Qiu et al., 2020; Wei et al., 2021).\nTo extend the use of PLMs beyond language,\nmuch exploration has also been conducted to rep-\nresent other modalities. For example, a signifi-\ncant amount of vision-language models (Kim et al.,\n2021; Zhai et al., 2022; Li et al., 2020a; Lu et al.,\n2019) have been developed by jointly pretraining\non co-occurring vision and language corpora, and\nare in the support of grounding (Zhang et al., 2020;\nChen et al., 2020), generation (Cho et al., 2021; Yu\net al., 2022) and retrieval tasks (Zhang et al., 2021;\nLi et al., 2020b) on the vision modality. Other\nPLMs capture semi-structure tabular data by lin-\nearizing table cells (Herzig et al., 2020; Yin et al.,\n2020; Iida et al., 2021) or incorporating structural\nprior in the attention mechanism (Wang et al., 2022;\nTrabelsi et al., 2022; Eisenschlos et al., 2021). To\nthe best of our knowledge, none of the prior studies\nhave extended pretrained LMs to geo-entity repre-\nsentation, nor do they support a suitable mechanism\nto capture the geo-entities’ spatial relations, which\ndo not have structural prior. This is exactly the\nfocus of our work.\nDomain-specific Language Modeling Another\nline of studies has been conducted to adapt PLMs\nto specific domains typically by pretraining on\ndomain-specific corpora. For example, in the\nbiomedicine domain, a series of models (Lee et al.,\n2020; Peng et al., 2019; Alsentzer et al., 2019; Phan\net al., 2021) have been developed by training PLMs\non corpora derived from PubMed. Similarly, PLMs\nhave been trained on corpora specific to software\nengineering (Tabassum et al., 2020), finance (Liu\net al., 2021), and proteomics (Zhou et al., 2020)\ndomains. In this context, SPABERT represents a\npilot study in the geographical domain by allowing\nthe PLM to learn from spatially distributed text.\n5 Conclusion\nThis paper presented SPABERT (\n ), a language\nmodel trained on geographic datasets for contex-\ntualizing geo-entities. SPABERT utilizes a novel\nspatial coordinate embedding mechanism to cap-\nture spatial relations between 2D geo-entities and\nlinearizes the geo-entities into 1D sequences, com-\npatible with the BERT-family structures. The ex-\nperiments show that the general-purpose represen-\ntations learned from SPABERT achieve better or\ncompetitive results on the geo-entity typing and\ngeo-entity linking tasks compared to SOTA pre-\ntrained LMs. We plan to evaluate SPABERT on\nadditional related tasks, such as geo-entity to nat-\nural language grounding. Also, we plan to extend\nSPABERT to support other geo-entity geometry\ntypes, including lines and polygons.\nLimitations\nThe current model design only considers points but\nnot polygon and line geometries, which could also\nhelp provide meaningful spatial relations for con-\ntextualizing a geo-entity. Training of SPABERT\nalso requires considerable GPU resources which\nmight produce environmental impacts.\nEthical Consideration\nSPABERT was evaluated on the English-spoken\nregions, so the model and results could have a bias\ntowards these regions and their commonly used\nlanguages. Replacing the backbone of SPABERT\nwith a multi-lingual model and training SPABERT\nwith diverse regions could mitigate the bias.\nAcknowledgement\nWe thank the reviewers for their insightful com-\nments and suggestions. We thank Dr. Valeria Vi-\ntale for her valuable input. This material is based\nupon work supported in part by NVIDIA Corpo-\nration, the National Endowment for the Humani-\nties under Award No. HC-278125-21 and Council\nReference AH/V009400/1, the National Science\nFoundation of United States Grant IIS 2105329,\na Cisco Faculty Research Award (72953213), and\nthe University of Minnesota, Computer Science &\nEngineering Faculty startup funds.\n2765\nReferences\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clin-\nical BERT embeddings. In Proceedings of the 2nd\nClinical Natural Language Processing Workshop ,\npages 72–78, Minneapolis, Minnesota, USA. Associ-\nation for Computational Linguistics.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. UNITER: UNiversal Image-TExt\nRepresentation Learning. In European Conference\non Computer Vision, pages 104–120. Springer.\nJaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021.\nUnifying vision-and-language tasks via text genera-\ntion. In Proceedings of the 38th International Con-\nference on Machine Learning, volume 139 of Pro-\nceedings of Machine Learning Research, pages 1931–\n1942. PMLR.\nMichiel de Jong, Yury Zemlyanskiy, Nicholas FitzGer-\nald, Fei Sha, and William W Cohen. 2021. Men-\ntion Memory: incorporating textual knowledge into\nTransformers through entity mention attention. In In-\nternational Conference on Learning Representations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJulian Eisenschlos, Maharshi Gor, Thomas Müller, and\nWilliam Cohen. 2021. MATE: Multi-view attention\nfor table transformer efficiency. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 7606–7619, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6894–6910, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nY Gao, Z Duan, W Shi, J Feng, and Y-Y Chiang. 2019.\nPersonalized Recommendation Method of POI Based\non Deep Neural Network. In Proceedings of the\n2019 6th International Conference on Behavioral,\nEconomic and Socio-Cultural Computing (BESC) ,\npages 1–6.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4320–4333, Online. Association for Computa-\ntional Linguistics.\nHiroshi Iida, Dung Thai, Varun Manjunatha, and Mohit\nIyyer. 2021. TABBIE: Pretrained representations of\ntabular data. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 3446–3456, Online. Association\nfor Computational Linguistics.\nJyun-Yu Jiang, Xue Sun, Wei Wang, and Sean Young.\n2019. Enhancing air quality prediction with social\nmedia and natural language processing. In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 2627–2632, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Span-\nBERT: Improving pre-training by representing and\npredicting spans. Transactions of the Association for\nComputational Linguistics, 8:64–77.\nWonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt:\nVision-and-language transformer without convolu-\ntion or region supervision. In International Con-\nference on Machine Learning , pages 5583–5594.\nPMLR.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2020a. What does BERT\nwith vision look at? In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 5265–5275, Online. Association\nfor Computational Linguistics.\n2766\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang,\nXiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\nLi Dong, Furu Wei, et al. 2020b. OSCAR: Object-\nSemantics Aligned Pre-training for Vision-Language\nTasks. In European Conference on Computer Vision,\npages 121–137. Springer.\nYijun Lin, Yao-Yi Chiang, Meredith Franklin, San-\ndrah P. Eckel, and José Luis Ambite. 2020. Build-\ning Autocorrelation-Aware Representations for Fine-\nScale Spatiotemporal Prediction. In 2020 IEEE Inter-\nnational Conference on Data Mining (ICDM), pages\n352–361.\nYijun Lin, Yao-Yi Chiang, Fan Pan, Dimitrios Stripelis,\nJose Luis Ambite, Sandrah P Eckel, and Rima Habre.\n2017. Mining Public Datasets for Modeling Intra-\nCity PM2.5 Concentrations at a Fine Spatial Resolu-\ntion. In Proceedings of the 25th ACM SIGSPATIAL\nInternational Conference on Advances in Geographic\nInformation Systems, volume 8, pages 1–10, New\nYork, NY , USA. ACM.\nYijun Lin, Nikhit Mago, Yu Gao, Yaguang Li, Yao-\nYi Chiang, Cyrus Shahabi, and José Luis Ambite.\n2018. Exploiting spatiotemporal patterns for accu-\nrate air quality forecasting using deep learning. In\nProceedings of the 26th ACM SIGSPATIAL Interna-\ntional Conference on Advances in Geographic Infor-\nmation Systems, SIGSPATIAL ’18, pages 359–368,\nNew York, NY , USA. Association for Computing\nMachinery.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nZhuang Liu, Degen Huang, Kaiyu Huang, Zhuang Li,\nand Jun Zhao. 2021. FinBERT: A Pre-trained Finan-\ncial Language Representation Model for Financial\nText Mining. In Proceedings of the Twenty-Ninth\nInternational Conference on International Joint Con-\nferences on Artificial Intelligence, pages 4513–4519.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.\n2019. Vilbert: Pretraining task-agnostic visiolinguis-\ntic representations for vision-and-language tasks. In\nAdvances in Neural Information Processing Systems,\nvolume 32. Curran Associates, Inc.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019. Trans-\nfer learning in biomedical natural language process-\ning: An evaluation of BERT and ELMo on ten bench-\nmarking datasets. In Proceedings of the 18th BioNLP\nWorkshop and Shared Task, pages 58–65, Florence,\nItaly. Association for Computational Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nMatthew E. Peters, Mark Neumann, Robert Logan, Roy\nSchwartz, Vidur Joshi, Sameer Singh, and Noah A.\nSmith. 2019. Knowledge enhanced contextual word\nrepresentations. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 43–54, Hong Kong, China. Association for\nComputational Linguistics.\nLong N Phan, James T Anibal, Hieu Tran, Shaurya\nChanana, Erol Bahadroglu, Alec Peltekian, and Gré-\ngoire Altan-Bonnet. 2021. Scifive: a text-to-text\ntransformer model for biomedical literature. arXiv\npreprint arXiv:2106.03598.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\narXiv preprint arXiv:2003.08271.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nSascha Rothe, Shashi Narayan, and Aliaksei Severyn.\n2020. Leveraging pre-trained checkpoints for se-\nquence generation tasks. Transactions of the Associ-\nation for Computational Linguistics, 8:264–280.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2019. VL-BERT: Pre-\ntraining of Generic Visual-Linguistic Representa-\ntions. In International Conference on Learning Rep-\nresentations.\nJeniya Tabassum, Mounica Maddela, Wei Xu, and Alan\nRitter. 2020. Code and named entity recognition in\nStackOverflow. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 4913–4926, Online. Association for\nComputational Linguistics.\n2767\nMohamed Trabelsi, Zhiyu Chen, Shuo Zhang, Brian D\nDavison, and Jeff Heflin. 2022. StruBERT: Structure-\naware BERT for Table Search and Matching. In\nProceedings of the ACM Web Conference 2022, pages\n442–451.\nFei Wang, Zhewei Xu, Pedro Szekely, and Muhao Chen.\n2022. Robust (controlled) table-to-text generation\nwith structure-aware equivariance learning. In Pro-\nceedings of the 2022 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n5037–5048, Seattle, United States. Association for\nComputational Linguistics.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuanjing Huang, Jianshu Ji, Guihong Cao, Daxin\nJiang, and Ming Zhou. 2021. K-Adapter: Infusing\nKnowledge into Pre-Trained Models with Adapters.\nIn Findings of the Association for Computational\nLinguistics: ACL-IJCNLP 2021, pages 1405–1418,\nOnline. Association for Computational Linguistics.\nXiaokai Wei, Shen Wang, Dejiao Zhang, Parminder Bha-\ntia, and Andrew Arnold. 2021. Knowledge enhanced\npretrained language models: A compreshensive sur-\nvey. arXiv preprint arXiv:2110.08455.\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki\nTakeda, and Yuji Matsumoto. 2020. LUKE: Deep\ncontextualized entity representations with entity-\naware self-attention. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 6442–6454, On-\nline. Association for Computational Linguistics.\nSen Yang, Dawei Feng, Linbo Qiao, Zhigang Kan, and\nDongsheng Li. 2019. Exploring pre-trained language\nmodels for event extraction and generation. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5284–\n5294, Florence, Italy. Association for Computational\nLinguistics.\nHongzhi Yin, Weiqing Wang, Hao Wang, Ling Chen,\nand Xiaofang Zhou. 2017. Spatial-aware hierarchical\ncollaborative deep learning for poi recommendation.\nIEEE Transactions on Knowledge and Data Engi-\nneering, 29(11):2537–2551.\nPengcheng Yin, Graham Neubig, Wen-tau Yih, and Se-\nbastian Riedel. 2020. TaBERT: Pretraining for joint\nunderstanding of textual and tabular data. InProceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 8413–8426, On-\nline. Association for Computational Linguistics.\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Lu-\nong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan,\nBen Hutchinson, Wei Han, Zarana Parekh, Xin Li,\nHan Zhang, Jason Baldridge, and Yonghui Wu. 2022.\nScaling Autoregressive Models for Content-Rich\nText-to-Image Generation.\nHaitao Yuan and Guoliang Li. 2021. A survey of traf-\nfic prediction: from spatio-temporal data to intelli-\ngent transportation. Data Science and Engineering,\n6(1):63–85.\nM Yue, Y Li, H Yang, R Ahuja, Y Chiang, and C Sha-\nhabi. 2019. DETECT: Deep Trajectory Clustering\nfor Mobility-Behavior Analysis. In Proceedings of\nthe 2019 IEEE International Conference on Big Data\n(Big Data), pages 988–997.\nMingxuan Yue, Yao-Yi Chiang, and Cyrus Shahabi.\n2021. V AMBC: A Variational Approach for Mo-\nbility Behavior Clustering. In Machine Learning and\nKnowledge Discovery in Databases. Applied Data\nScience Track, pages 453–469. Springer International\nPublishing.\nXiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas\nSteiner, Daniel Keysers, Alexander Kolesnikov, and\nLucas Beyer. 2022. LiT: Zero-Shot Transfer With\nLocked-Image Text Tuning. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 18123–18133.\nBowen Zhang, Hexiang Hu, Vihan Jain, Eugene Ie, and\nFei Sha. 2020. Learning to represent image and text\nwith denotation graph. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 823–839, Online.\nAssociation for Computational Linguistics.\nBowen Zhang, Hexiang Hu, Linlu Qiu, Peter Shaw, and\nFei Sha. 2021. Visually grounded concept compo-\nsition. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2021, pages 201–215,\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n1441–1451, Florence, Italy. Association for Compu-\ntational Linguistics.\nPengpeng Zhao, Anjing Luo, Yanchi Liu, Jiajie Xu,\nZhixu Li, Fuzhen Zhuang, Victor S. Sheng, and Xi-\naofang Zhou. 2022. Where to Go Next: A Spatio-\nTemporal Gated Network for Next POI Recommen-\ndation. IEEE Transactions on Knowledge and Data\nEngineering, 34(5):2512–2524.\nGuangyu Zhou, Muhao Chen, Chelsea JT Ju, Zheng\nWang, Jyun-Yu Jiang, and Wei Wang. 2020. Muta-\ntion effect estimation on protein–protein interactions\nusing deep contextualized representation learning.\nNAR genomics and bioinformatics, 2(2):lqaa015.\nWenxuan Zhou, Fangyu Liu, Ivan Vuli´c, Nigel Collier,\nand Muhao Chen. 2022. Prix-LM: Pretraining for\nmultilingual knowledge base construction. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\n2768\nPapers), pages 5412–5424, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\n2769",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8549992442131042
    },
    {
      "name": "Geospatial analysis",
      "score": 0.7503843307495117
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6041717529296875
    },
    {
      "name": "Natural language processing",
      "score": 0.6008122563362122
    },
    {
      "name": "Representation (politics)",
      "score": 0.5862223505973816
    },
    {
      "name": "Spatial contextual awareness",
      "score": 0.5810407996177673
    },
    {
      "name": "Entity linking",
      "score": 0.573423445224762
    },
    {
      "name": "Embedding",
      "score": 0.5413292646408081
    },
    {
      "name": "Language model",
      "score": 0.49616608023643494
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49004065990448
    },
    {
      "name": "Information retrieval",
      "score": 0.4863598942756653
    },
    {
      "name": "Spatial database",
      "score": 0.45395252108573914
    },
    {
      "name": "Spatial analysis",
      "score": 0.4493577182292938
    },
    {
      "name": "Space (punctuation)",
      "score": 0.4233302175998688
    },
    {
      "name": "Spatial relation",
      "score": 0.41515424847602844
    },
    {
      "name": "Cartography",
      "score": 0.1456018090248108
    },
    {
      "name": "Geography",
      "score": 0.12495496869087219
    },
    {
      "name": "Remote sensing",
      "score": 0.07682633399963379
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Knowledge base",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ]
}