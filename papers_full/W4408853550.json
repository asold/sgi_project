{
  "title": "Accuracy of LLMs in medical education: evidence from a concordance test with medical teacher",
  "url": "https://openalex.org/W4408853550",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5015934551",
      "name": "Vinaytosh Mishra",
      "affiliations": [
        "Gulf Medical University",
        "Datta Meghe Institute of Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5013163033",
      "name": "Yotam Lurie",
      "affiliations": [
        "Ben-Gurion University of the Negev"
      ]
    },
    {
      "id": "https://openalex.org/A5041885354",
      "name": "Shlomo Mark",
      "affiliations": [
        "Sami Shamoon College of Engineering"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2995177752",
    "https://openalex.org/W3086481015",
    "https://openalex.org/W2301443924",
    "https://openalex.org/W2488551656",
    "https://openalex.org/W1559973877",
    "https://openalex.org/W4386693657",
    "https://openalex.org/W4389313528",
    "https://openalex.org/W4385654693",
    "https://openalex.org/W4393049683",
    "https://openalex.org/W4387296868",
    "https://openalex.org/W4389728663",
    "https://openalex.org/W4382751541",
    "https://openalex.org/W4393941607",
    "https://openalex.org/W4392505993",
    "https://openalex.org/W4396732742",
    "https://openalex.org/W4396609241",
    "https://openalex.org/W4394579620",
    "https://openalex.org/W2022077643",
    "https://openalex.org/W2085996979",
    "https://openalex.org/W2333035970",
    "https://openalex.org/W2076061140",
    "https://openalex.org/W4396778560",
    "https://openalex.org/W4399890721",
    "https://openalex.org/W4400371216",
    "https://openalex.org/W4400041715",
    "https://openalex.org/W4399830266",
    "https://openalex.org/W4403907717",
    "https://openalex.org/W4404067249",
    "https://openalex.org/W4391170193",
    "https://openalex.org/W4377197101"
  ],
  "abstract": "Abstract Background There is an unprecedented increase in the use of Generative AI in medical education. There is a need to assess these models’ accuracy to ensure patient safety. This study assesses the accuracy of ChatGPT, Gemini, and Copilot in answering multiple-choice questions (MCQs) compared to a qualified medical teacher. Methods This study randomly selected 40 Multiple Choice Questions (MCQs) from past United States Medical Licensing Examination (USMLE) and asked for answers to three LLMs: ChatGPT, Gemini, and Copilot. The results of an LLM are then compared with those of a qualified medical teacher and with responses from other LLMs. The Fleiss’ Kappa Test was used to determine the concordance between four responders (3 LLMs + 1 Medical Teacher). In case of poor agreement between responders, Cohen’s Kappa test was performed to assess the agreement between responders. Results ChatGPT demonstrated the highest accuracy (70%, Cohen’s Kappa = 0.84), followed by Copilot (60%, Cohen’s Kappa = 0.69), while Gemini showed the lowest accuracy (50%, Cohen’s Kappa = 0.53). The Fleiss’ Kappa value of -0.056 indicated significant disagreement among all four responders. Conclusion The study provides an approach for assessing the accuracy of different LLMs. The study concludes that ChatGPT is far superior (70%) to other LLMs when asked medical questions across different specialties, while contrary to expectations, Gemini (50%) performed poorly. When compared with medical teachers, the low accuracy of LLMs suggests that general-purpose LLMs should be used with caution in medical education.",
  "full_text": null,
  "topic": "Concordance",
  "concepts": [
    {
      "name": "Concordance",
      "score": 0.8772364854812622
    },
    {
      "name": "Cohen's kappa",
      "score": 0.6847286224365234
    },
    {
      "name": "Kappa",
      "score": 0.6446468830108643
    },
    {
      "name": "Test (biology)",
      "score": 0.5697570443153381
    },
    {
      "name": "Medicine",
      "score": 0.4521430432796478
    },
    {
      "name": "Medical education",
      "score": 0.4417673349380493
    },
    {
      "name": "Family medicine",
      "score": 0.42768824100494385
    },
    {
      "name": "Psychology",
      "score": 0.4097975492477417
    },
    {
      "name": "Internal medicine",
      "score": 0.19277891516685486
    },
    {
      "name": "Statistics",
      "score": 0.1419891119003296
    },
    {
      "name": "Mathematics",
      "score": 0.08456969261169434
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "cited_by": 6
}