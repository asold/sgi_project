{
  "title": "Multiscale Convolutional Transformer for EEG Classification of Mental Imagery in Different Modalities",
  "url": "https://openalex.org/W4312892255",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4314771996",
      "name": "Hyung ju Ahn",
      "affiliations": [
        "Korea University"
      ]
    },
    {
      "id": "https://openalex.org/A3046422849",
      "name": "Dae Hyeok Lee",
      "affiliations": [
        "Korea University"
      ]
    },
    {
      "id": "https://openalex.org/A2240877957",
      "name": "Ji Hoon Jeong",
      "affiliations": [
        "Chungbuk National University"
      ]
    },
    {
      "id": "https://openalex.org/A2588548732",
      "name": "Seong-Whan Lee",
      "affiliations": [
        "Korea University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2098644279",
    "https://openalex.org/W2949263306",
    "https://openalex.org/W2343715419",
    "https://openalex.org/W2804035092",
    "https://openalex.org/W3212535881",
    "https://openalex.org/W2940585064",
    "https://openalex.org/W2888927777",
    "https://openalex.org/W2922465551",
    "https://openalex.org/W2963037717",
    "https://openalex.org/W3046279878",
    "https://openalex.org/W4206927580",
    "https://openalex.org/W4281399130",
    "https://openalex.org/W3181073797",
    "https://openalex.org/W4225005389",
    "https://openalex.org/W2741907166",
    "https://openalex.org/W2132360759",
    "https://openalex.org/W2559463885",
    "https://openalex.org/W2998747857",
    "https://openalex.org/W2797340155",
    "https://openalex.org/W2980481100",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W4229068725",
    "https://openalex.org/W3177342940",
    "https://openalex.org/W3009259306",
    "https://openalex.org/W2971170463",
    "https://openalex.org/W4281642485",
    "https://openalex.org/W4220728439",
    "https://openalex.org/W6963588625",
    "https://openalex.org/W2739095506",
    "https://openalex.org/W1969715890",
    "https://openalex.org/W2128495200",
    "https://openalex.org/W2014502281",
    "https://openalex.org/W6600012500",
    "https://openalex.org/W3091553737",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W4283655414",
    "https://openalex.org/W6797185979",
    "https://openalex.org/W2129138901",
    "https://openalex.org/W2108629469",
    "https://openalex.org/W3162043918",
    "https://openalex.org/W2921285805",
    "https://openalex.org/W3035521319",
    "https://openalex.org/W2966745778",
    "https://openalex.org/W3119685619",
    "https://openalex.org/W2905183880",
    "https://openalex.org/W2237320994",
    "https://openalex.org/W2736142580",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W245658",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4226479556",
    "https://openalex.org/W3173912422",
    "https://openalex.org/W3102455230"
  ],
  "abstract": "A new kind of sequence-to-sequence model called a transformer has been applied to electroencephalogram (EEG) systems. However, the majority of EEG-based transformer models have applied attention mechanisms to the temporal domain, while the connectivity between brain regions and the relationship between different frequencies have been neglected. In addition, many related studies on imagery-based brain-computer interface (BCI) have been limited to classifying EEG signals within one type of imagery. Therefore, it is important to develop a general model to learn various types of neural representations. In this study, we designed an experimental paradigm based on motor imagery, visual imagery, and speech imagery tasks to interpret the neural representations during mental imagery in different modalities. We conducted EEG source localization to investigate the brain networks. In addition, we propose the multiscale convolutional transformer for decoding mental imagery, which applies multi-head attention over the spatial, spectral, and temporal domains. The proposed network shows promising performance with 0.62, 0.70, and 0.72 mental imagery accuracy with the private EEG dataset, BCI competition IV 2a dataset, and Arizona State University dataset, respectively, as compared to the conventional deep learning models. Hence, we believe that it will contribute significantly to overcoming the limited number of classes and low classification performances in the BCI system.",
  "full_text": "646 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023\nMultiscale Convolutional T ransformer for EEG\nClassiﬁcation of Mental Imagery in\nDifferent Modalities\nHyung-Ju Ahn , Dae-Hyeok Lee , Ji-Hoon Jeong , Associate Member, IEEE,\nand Seong-Whan Lee , Fellow, IEEE\nAbstract — A new kind of sequence–to–sequence model\ncalled a transformer has been applied to electroencephalo-\ngram (EEG) systems. However, the majority of EEG–based\ntransformer models have applied attention mechanisms\nto the temporal domain, while the connectivity between\nbrain regions and the relationship between different fre-\nquencies have been neglected. In addition, many related\nstudies on imagery–based brain–computer interface (BCI)\nhave been limited to classifying EEG signals within one type\nof imagery. Therefore, it is important to develop a general\nmodel to learn various types of neural representations.\nIn this study, we designed an experimental paradigm based\non motor imagery, visual imagery, and speech imagery\ntasks to interpret the neural representations during mental\nimagery in different modalities. We conducted EEG source\nlocalization to investigate the brain networks. In addition,\nwe propose the multiscale convolutional transformer for\ndecoding mental imagery, which applies multi–head atten-\ntion over the spatial, spectral, and temporal domains. The\nproposed network shows promising performance with 0.62,\n0.70, and 0.72 mental imagery accuracy with the private\nEEG dataset, BCI competition IV 2a dataset, and Arizona\nState University dataset, respectively, as compared to the\nconventional deep learning models. Hence, we believe that\nit will contribute signiﬁcantly to overcoming the limited\nnumber of classes and low classiﬁcation performances in\nthe BCI system.\nIndex Terms — Brain–computer interface, electroen-\ncephalogram, mental imagery, transformer, self–attention.\nManuscript received 20 June 2022; revised 19 October 2022;\naccepted 23 October 2022. Date of publication 15 December 2022; date\nof current version 2 February 2023. This work was supported in part\nby the Challengeable Future Defense Technology Research and Devel-\nopment Program of Agency for Defense Development in 2020 under\nGrant 912911601; and in part by the Institute of Information and Com-\nmunications Technology Planning and Evaluation (IITP) Grant through\nthe Korea Government (MSIT), Artiﬁcial Intelligence Graduate School\nProgram, Korea University, under Grant 2019-0-00079.\n(Corresponding\nauthor: Seong-Whan Lee.)\nThis work involved human subjectsor animals in its research. Approval\nof all ethical and experimental procedures and protocols was granted\nby the Korea University Institutional Review Board under Application\nNo. KUIRB-2020-0318-01.\nHyung-Ju Ahn and Dae-Hyeok Lee are with the Department of Brain\nand Cognitive Engineering, Korea University, Seongbuk, Seoul 02841,\nSouth Korea (e-mail: hj_ahn@korea.ac.kr; lee_dh@korea.ac.kr).\nJi-Hoon Jeong is with the School of Computer Science, Chungbuk\nNational University, Seowon, Cheongju, Chungbuk 28644, South Korea\n(e-mail: jh.jeong@chungbuk.ac.kr).\nSeong-Whan Lee is with the Department of Artiﬁcial Intelligence,\nKorea University, Seongbuk, Seoul 02841, South Korea (e-mail:\nsw.lee@korea.ac.kr).\nDigital Object Identiﬁer 10.1109/TNSRE.2022.3229330\nI. I NTRODUCTION\nE\nLECTROENCEPHALOGRAM (EEG)–based brain–\ncomputer interface (BCI) is one of the most actively\nused non–invasive BCI systems. Owing to the advantages of\nnon–surgical electrode placements , high temporal resolution,\nportability, and cost efﬁciency, various EEG–based systems\nhave been developed to rehabilitate or assist patients with\nneurological impairment [1]. Numerous experimental BCI\nparadigms have been proposed to control external devices\nsuch as a robotic arm [2], an exoskeleton [3], and a speller\nsystem [4] using particular brain signals in a speciﬁc condition.\nAmong them, mental imagery ha s been deeply researched as\na control strategy for BCI since it uses intrinsic brain activity\nmanifested directly from users’ voluntary imagination. Also,\nas mental imagery is independent of external stimuli, they\nfacilitate the realization of a user–friendly interface that causes\nless fatigue and enables more natural situation awareness\nfor users. The majority of mental imagery experimental\nparadigms were designed modality—speciﬁcally for the\npurpose of the system. For example, motor imagery (MI)\nwas used to develop a system to rehabilitate or restore lost\nmotor function in stroke patient s [5], speech imagery (SI) can\nbe used to develop communication tools for people who are\nunable to communicate [6], and visual imagery (VI) can be\nused as an alternative for users who have BCI illiteracy for\nMI [7].\nMeanwhile, many studies based on functional magnetic\nresonance imaging, magneticencephalogram (MEG), and EEG\nhave found prominent brain areas and spectral band groups\nwhich engage in speciﬁc mental imagery. Furthermore, studies\non EEG classiﬁcation using both MI and SI have shown the\npossibility of expanding the limited number of commands for\nEEG–based BCI [8], [9]. Also, a recent MEG study on the\nclassiﬁcation of MI, motor execution, VI, and visual percep-\ntion [10] has reported that imagery behaviors are correlated\nwith modulated activity in the respective modality–speciﬁc\nregions and with additional activity in supramodal imagery\nregions.\nTherefore, EEG classiﬁcation of mental imagery in different\nmodalities can be used to overcome the limited number of\nclasses and low classiﬁcation performance of EEG–based\nBCI. Since the EEG patterns have a spatial constraint for\nmodality–speciﬁc regions, it becomes more challenging to\nextract discriminative features as the number of classes\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAHN et al.: MULTISCALE CONVOLUTIONAL TRANSFORMER FOR EEG CLASSIFICA TION OF MENTAL IMAGERY 647\nincreases. Moreover, EEG has a low spatial resolution and\nlow signal–to–noise ratio (SNR) owing to the large distance\nbetween the source of the signal (brain) and the location of the\nelectrode (scalp). Consequently, the classiﬁcation performance\nof modality–speciﬁc imagery exhibits a trade–off between\nthe classiﬁcation accuracy and the number of classes, which\nresults in low stability or a limited number of commands for\nEEG–based rehabilitation applications. By taking advantage of\ndifferent functional brain networks of mental imagery in dif-\nferent modalities, this approach can circumvent the constraint\nof maintaining adequate spacing between the corresponding\nsources. Also, it can reinforce the system redundancy by\nproviding extra control options for BCI illiteracy in particular\nimagery.\nHowever, multiclass classiﬁcation of mental imagery in\ndifferent modalities still remains a challenge. Firstly, as the\nsimilarity among the classes within the same modality is\nhigher than that in the case of different modalities, it causes\nlow classiﬁcation accuracy due to biased distribution of the\nclasses in the cases of inter–and intra–modalities. Secondly,\nas each modality of the mental imagery has prominent fre-\nquency bands, it is difﬁcult to extract optimal spectral features\nfor each class.\nIn this study, we designed the experimental paradigms\nand explored the EEG multi–classiﬁcation of mental imagery\nin different modalities (MI, VI, and SI). The main contri-\nbutions of the research are summarized as follows. i)W e\nconﬁrmed that distinguishable spectral–spatial patterns can be\nfound from mental imagery in different m odalities t hrough\nEEG source localization analysis. ii) We propose a neural\nnetwork called a multiscale convolutional transformer for\ndecoding mental imagery in different modalities. To devise\nthe all–rounded network for classiﬁcation of different mental\nimageries, we adopted a multis cale temporal convolutional\nblock to extract a various range of spectral features. Also,\nto learn better spatio–temporal representation of EEG signals\nduring mental imagery, we designed factorized transformer\nencoder with spatial mapping. Statistical and neurophysio-\nlogical analysis was conducted to investigate the signiﬁcant\nfeatures for classifying MI, VI, and SI EEG signals. We clas-\nsiﬁed a total of six–class EEG signals with high and robust\naccuracy of 0.62 using our proposed multiscale convolutional\ntransformer. To the best of our knowledge, this is the ﬁrst\nattempt to classify high–dimensional imagery tasks using MI,\nVI, and SI, which have the potential for expanding the number\nof commands and enhancing the cl assiﬁcation performance of\nmulti–class EEG signals.\nII. R\nELA TEDWORK\nVarious kinds of deep learning models were proposed to\nclassify EEG signals [11], [12], [13], [14]. The application of\na convolutional neural network (CNN) to an EEG–based BCI\ndomain showed successful results for an end–to–end feature\nextraction and classiﬁcation. Sc hirrmeister et al. [15] designed\nCNN architectures called DeepConvNet and ShallowConvNet\nto classify raw MI EEG signals. Their performances were\ncompetitive as that of the ﬁlter bank CSP [16], which is\nthe one of the most well—known machine learning method\nto classify MI EEG signals. Lawhern et al. [17] proposed\na versatile and lightweight CNN architecture called EEGNet\nwhich can classify EEG signals from various BCI paradigms\n(P300, movement–related cortical potentials [18], error–related\nnegativity responses, and sensory–motor rhythms). Antelis\net al. [19] proposed a dendrite morphological neural network\nwhich incorporates the computational structure in the dendrites\nof the neurons. They designed the model architecture to\nproduce closed separation surfaces between the classes to\noffer a different solution to enhance multi–classs classiﬁcation\nperformance. Virgilio et al. [20] introduced spiking neural\nnetwork (SNN) to EEG classiﬁcation task and achieved the\naccuracy of 0.83 for binary classiﬁcation of MI task. SNN\nis considered as a third generation artiﬁcial neural network\n(ANN) which has a great potential to replace second genera-\ntion (ANN) such as CNN.\nRecently, an attention mechan ism–based deep learning\nmodel named transformer [21] demonstrated outstanding\nperformance in diverse domains such as natural language\nprocessing [21], [22] and computer vision [23]. Conventional\nrecurrent neural networks (RNNs) such as the gated recurrent\nunits and long short–term memory [24] suffer from the gra-\ndient vanishing / exploding problems as the layers become\ndeeper, thus hindering the learning of long data sequences.\nWith its developments, it has been introduced to EEG systems\n[25], [26], [27], [28], [29]. Bangchi et al. [25] proposed a\ndeep learning network that consists of MHA and convolutional\nlayers to classify the EEG–based visual stimulus classiﬁca-\ntion. Kostas et al. [26] adapted the transformer architecture\nof the language model to the EEG domain and found a\nsingle pre–trained model is capable of modeling new raw\nEEG sequences and different subjects performing different\ntasks. Li et al. [30] proposed a convolutional self–attention\nnetwork for emotion recognition by capturing the individual\ninformation in and within different frequency bands to learn\ncomplementary frequency information. While the majority of\nworks used attention mechanisms and convolutional layers\nwith limited spatial—temporal information, the application of\nself—attention with multi—kernel and multi—branch convo-\nlutional layers can extract diverse features.\nIII. M\nA TERIALS AND METHODS\nA. Participants\nForty healthy subjects (40 males, aged 25.5 ( ±3.1)\nyears) participated in the experiments. All experiments have\nbeen approved by the Korea University Institutional Review\nBoard (KUIRB-2020-0318-01). Prior to the experiments,\nwe informed the subjects to get adequate sleep (at least seven\nhours) and to refrain from drinking alcohol the previous day.\nA detailed explanation of the experimental protocol and pro-\ncedures was provided to the subjects. In accordance with the\nDeclaration of Helsinki, they provided their written consent.\nB. Data Description\nWe used various mental imagery-based EEG datasets to\nvalidate our proposed model, including private and public\ndatasets. The details of the datasets are shown in Table I.\n1) Mental Imagery Dataset (Private Dataset): We acquired\nthe EEG signals during the mental imagery tasks using our\nexperimental paradigms. Fig. 1 describes the mental imagery\n648 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023\nTABLE I\nDESCRIPTION OF THE PRIVATE AND PUBLIC EEG DATASETS\nFig. 1. The mental imagery in different modalities and the method for\npreserving spatial information of EEG. The functional modality of mental\nimagery differentiates MI, VI, and SI. After the transformer encoder\ncompressed temporal features, EEG features were rearranged into an\n8 × 8 shape to preserve spatial information based on EEG channel\nmontage.\nin different modalities and the method for preserving spatial\ninformation of EEG. The lower part of Fig. 1 corresponds to\nsubclasses in each mental imagery type. The upper part of\nFig. 1 shows the method for preserving spatial information\nof EEG. After the transformer encoder compressed temporal\nfeatures, EEG features were rearranged into an 8 × 8 shape.\nFig. 2 presents the experimental protocols for each mental\nimagery task. EEG signals were measured using BrainAmp\n(BrainProduct GmbH, Germany) and 64 Ag/AgCl electrodes\naccording to the 10/20 internati onal system. An electrode was\nplaced at FCz for the reference electrode and an electrode\nat FPz for the ground electrode. In this experiment, the\nsampling rate was set to 500 Hz, and the notch ﬁlter was\napplied at 60 Hz. The conductive gel was injected between\nthe electrodes and the scalp before the EEG signals were\nacquired to maintain electrode impedance below 10 k /Omega1.T h e\nMI tasks involve kinesthetic imagery of the “left hand” and\n“right hand.” In the VI tasks, participants were instructed\nto visualize two different swarm behaviors (“split” and “fall\nin”) after watching the video clips of corresponding tasks.\nFor the SI tasks, two words were presented to imagine the\npronunciations of the given words (“go” and “stop”).\nThe imagery period for the MI and VI tasks was set at\n4 s because imagery of kinesthe tic movement or visual repre-\nsentation can continue for a few seconds per trial. However,\nSI tasks only take one or two seconds to pronounce each word.\nFor the SI tasks, the imagery period was set to 1.5 s, which\nwas repeated four times consecutively in one run. The tasks\nwere repeated 50 times each. There fore, we acquired 50 trials\nper class for the MI and VI, and 200 trials per class for the SI.\nFig. 2. Experimental paradigms for acquiring EEG signals of mental\nimagery in different modalities. The ﬁxation cross was used for eliminat-\ning any possible afterimages.(a) Motor imagery,(b) Visual imagery, and\n(c) Speech imagery. (Fix.: ﬁxation cross, Rep.: repetition.)\n2) BCI Competition IV 2a Dataset : To validate the pro-\nposed method for the public EEG dataset, we used the BCI\ncompetition (BCIC) IV 2a dataset [31]. The BCIC IV 2a\ndataset consists of four class MI of the “left hand”, “right\nhand”, “feet”, and “tongue”. The data were acquired from\nnine subjects, using twenty two EEG channels and three\nelectrooculogram (EOG) channels. Each subject recorded two\nsessions on different days. There were six runs in each session,\nseparated by short breaks. There were 144 trials in each\nclass, and the participants were instructed to perform each\nMI task for 3 s. The data were downsampled to 256 Hz with\na frequency range of 0.5–100 Hz.\n3) Arizona State University Dataset: The Arizona state uni-\nversity (ASU) dataset [32] was used for the classiﬁcation of\nthe SI tasks. In this study, we used a dataset for short versus\nlong word classiﬁcation. The dataset consists of two class SI\nof the English words “in” and “cooperate.” Each class consists\nof 100 trials, and a single trial lasts for 5 s. The data were\nacquired from six subjects with sixty EEG channels and four\nEOG channels. The data was preprocessed using a frequency\nrange of 8–70 Hz.\nC. Data Preprocessing\nThe mental imagery dataset was pre–processed using the\nBBCI Toolbox [33] and EEGLAB Toolbox [34] (version\n2021.1). The δ band (0.5–4 Hz) is the most sensitive (easy to\nbe contaminated) frequency band for EOG artifacts. To exam-\nine the inﬂuence of with and without δ band, the high–pass\nﬁlter was set to 0.5 and 4 Hz, respectively. In addition, each\nmental imagery has different sp ectral features that contribute\nto the classiﬁcation accuracy. Therefore, low–pass ﬁlters were\nset to 30, 60, and 120 Hz to investigate coordinated fre-\nquency bands for a multi–class classiﬁcation task with dif-\nferent mental imageries. In summary, the data were bandpass\nﬁltered using a ﬁfth–order Butterworth ﬁlter into six types\nAHN et al.: MULTISCALE CONVOLUTIONAL TRANSFORMER FOR EEG CLASSIFICA TION OF MENTAL IMAGERY 649\n(0.5–30 Hz, 0.5–60 Hz, 0.5–120 Hz, 4–30 Hz, 4–60 Hz, and\n4–120 Hz). After that, the data were downsampled to 250 Hz.\nIn order to alleviate the ﬂuctuation and nonstationarity, z–score\nnormalization was employed as\nˆX = X − μ\n√\nσ2\n(1)\nwhere ˆX ∈ RC×T and X ∈ RC×T indicate the normalized\nand input signal, respectively. The mean value and variance\nof the data are represented by μ and σ\n2, respectively. 60 %\nof the total data were used as the training dataset, 20 % of\nthe total dataset were used as the validation dataset to prevent\noverﬁtting, and the rest of the 20 % of the total dataset were\nused as the test dataset. To match the training dataset’s length,\nthe sliding window technique was applied to the MI and VI\ndatasets. A single trial of 4 s was divided into 1.5 s of four\nepochs with 0.7 s of the overla pping period. Consequently,\n720 trials (120 trials × six–class) were used for the training\ndataset, 240 trials (40 trials × six–class) were used as the\nvalidation dataset, and 240 trials (40 trials × six–class) were\nused for the test dataset.\nD. Data Analysis\nData analysis was conducted utilizing EEGLAB Toolbox\nand Brainstorm [35]. EEG source localization (ESL) was\nperformed with a standardized low–resolution electromag-\nnetic tomography (sLORETA) [36] inverse solution, which is\nimplemented in Brainstorm [35]. An inverse problem entails\nestimating the current density or activity values of the source\nthat generated the measured electric potential or magnetic\nﬁeld vector. Under a smoothness constraint, the Tikhonov\nmethod is employed to regularize the EEG inverse problem.\nsLORETA implicitly assumes that the activity of the adjacent\nneuronal populations is highly correlated, which is physiolog-\nically plausible. The noise covariance was calculated using\nthe resting state (ﬁxation cross). The parameter λ for the\nregularization of the ill–posed problem was used as a default\n(λ = 0.1). As we used the Montreal Neurological Institute\ntemplate instead of the individual anatomy of the subject,\nunconstrained dipole orientations were used to prevent failure\nin representing common activity patterns.\nE. Multiscale Convolutional Transformer\nfor EEG Classiﬁcation\nAs a decoding method, we propose the multiscale\nconvolutional transformer for EEG classiﬁcation in different\nmodalities. We de vised our proposed method to learn the\nrepresentation of functional brain networks during mental\nimagery as a temporal–spatial–spectral pattern from EEG\nsignals. Fig. 3 indicates the overall framework of the pro-\nposed method. The pipeline consists of multiscale temporal\nconvolutional blocks, temporal transformer encoder blocks,\nparallel spatial convolutional blocks with spatial mapping, and\nspatial transformer encoder. The speciﬁcations of the proposed\nmethod are listed in Table II in detail.\nFig. 3. The overall framework of the proposed method. The model\nconsists of multi–kernel temporal convolutional blocks, temporal trans-\nformer encoder, parallel spatial convolutional blocks, spatial transformer\nencoder, and fusion convolutional block. (fs: sampling rate, C: the number\nof channels).\nTABLE II\nDETAILED ARCHITECTURE OF THE MULTISCALE\nCONVOLUTIONAL TRANSFORMER\n1) Multiscale T emporal Convolutional Block: To extract a var-\nious range of spectral features of mental imagery, we adopted\na multiscale temporal convolutional block inspired by TScep-\ntion [37]. TSception is a CNN model for EEG—based emotion\ndetection, which showed robust classiﬁcation performance\nby learning multiple temporal and frequency representations.\nSince mental imagery shows different optimal frequency\nranges depending on its modality, utilizing a multiscale kernel\nis more beneﬁcial. The multiscale temporal convolutional\nblock is comprised of three convolutional blocks with different\nsizes of kernel windows. The convolutional block consists of\n650 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023\nFig. 4. The architecture of the temporal and spatial transformer\nencoder block. Both transformer encoders have identical structures and\nit comprised of multi–head attention, skip connection, and feed forward\nlayers with a stack of four identical layers.\nthe convolutional layer, GELU activation function, and the\naverage pooling layer. The sizes of the temporal kernels were\nset as half, quarter, and half of a quarter of the sampling\nrate, to capture frequency information at greater than 2 Hz,\n4 Hz, and 8 Hz, respectively. Since the sampling rate of EEG\nsignals was 250 Hz, ﬁlter sizes were round to the nearest\ninteger. The number of output channels of convolutional layer\nis ﬁxed to 8 to compress the EEG features. After each\nfeature passed through the activation function and average\npooling layer with a window size of (1,16) and stride size of\n(1,8), they were concatenated along with the output channel\ndimension. As a result, the number of output channels of\nmultiscale convolutional layer were set to 24. After that, batch\nnormalization was applied.\n2) T emporal and Spatial T ransformer Encoder: To apply\ntemporal and spatial attentio n separately, we used the two\ntransformer encoder which have a same architecture. For\nthe spatial attention scoring, we presented a region—based\nself—attention. Most of the conventional methods applied\nself—attention to channel dimension, while the relationship\nof channel locations was ignored, and most of the spatial\ninformation was blurred. To apply self—attention on spatial\ndomain with multiple output kernels from convolutional layer,\nwe reshaped the input features by merging the batch size and\ntemporal size into a same dim ension. In this case, we ﬁxed\nthe batch size to 32.\nFig. 4 shows detailed architecture of the transformer\nencoder. The process for region—based attention can be\nexpressed as\nAttention (Q, K, V ) = Sof tmax ( QK\nT\n√\ndk\n)V (2)\nwhere Attention(Q,K,V) is the weighted representation. Q,\nK,a n d V are matrices packed by vectors for simultaneous\ncalculation and dk is a dimension of K vector. To apply\nself—attention on spatial domain with multiple output kernels\nfrom convolutional layer, we reshaped the input features.\nWe merged the dimension of batch size and temporal size\ninto a same dimension. The MHA is employed to emphasize\nthe spatial and temporal features from a different perspective.\nThe operation of MHA is expressed as\nMHA (X\nQ, XK , XV ) =[ head 0; ... ; head h−1]Wo (3)\nhead i = Attention (XQ W Q\ni ,XK W K\ni ,XV W V\ni )\n(4)\nA feed–forward network(FFN) contains two fully–connected\nlayers and the GeLU activation function is connected after the\nMHA (Eq.5-8), to improve non– linear learning capabilities of\nthe model. A FFN is expressed as\nf1(X) = W1 X + B1 (5)\nf2(X) = GeLU (X) = X/Phi1(X)\n= X · 1\n2 [1 + erf(X/\n√\n2)] (6)\nf3(X) = W2 f2(X) + B2 (7)\nFFN (X) = W2GeLU (W1 X + B1) + B2 (8)\n/Phi1is a cumulative distribution of Gaussian distribution and\noften computed with the error function (erf), hence GeLU is\ndeﬁned as Eq. 6. The layer normalization is performed before\nthe MHA and FF blocks, and res idual connections are also\nused to improve training. For the ensemble effect, the MHA\nand FF blocks are repeated four times.\n3) Spatial Mapping and Parallel Spatial Convolutional Blocks:\nTo maintain the spatial information of EEG channel locations,\nwe introduce a spatial mapping strategy. After the temporal\nfeatures were compressed into a single temporal class token,\nwe reshaped the feature. Using t he temporal axis as an extra\nchannel dimension, we reform ed one–dimensional channel\nvectors into two–dimensional vectors. As shown in the upper\npart of Fig. 1, EEG channels were reshaped into (8,8) shapes\nto compress the spatial information while maintaining its sym-\nmetry. Speciﬁcally, channels along the middle position (gray\ntone) were reshaped into a di agonal position. The positions of\nFz and AFz were shifted in order to bridge the gap of reference\nposition (FPz). The row components on the left hemisphere\n(odd numbers) and right hemisphere (even numbers) were\nrepositioned along with horizontal (left) and vertical (down)\ndirections from the diagonal lin e as a reference line. Channels\nin bold letters indicate necessary repositioning in order to\nmaintain an 8×8 shape. Table II show detailed output shape of\neach layer. The parallel spatial convolutional block consists of\ntwo convolutional blocks, which are applied before and after\nthe spatial mapping to produce more diverse spatial features.\nThe ﬁrst spatial convolutional layer has a larger ﬁlter size of\n(64,1), to extract global spatial features. The second spatial\nconvolutional layer has a smaller ﬁlter size of (2,2) with\na stride of (2,2) to extract local spatial features. The local\nspatial features were reshaped with a 1D-channel vector to\nfeed into the spatial transformer encoder. After the parallel\nspatial convolutional blocks, EEG features were concatenated.\nIn this stage, the spatial class token of the spatial transformer\nencoder and the spatial features from the convolutional layer\nwere fused together.\n4) Classiﬁer: After concatenating encoded features of the\nspatial transformer encoder based on the class token and\noutput features from the spatial convolutional layer, layer\nnormalization was applied. Finally, a fully connected layer is\nAHN et al.: MULTISCALE CONVOLUTIONAL TRANSFORMER FOR EEG CLASSIFICA TION OF MENTAL IMAGERY 651\nTABLE III\nPERFORMANCE COMPARISON OF THE CONVENTIONAL AND PROPOSED\nMETHODS USING THE MENTAL IMAGERY DATASET.T HE DATASET\nCONSISTS OF SIX MENTAL IMAGERY TASKS RELATED TO MI, VI,\nAND SI. * AND ** DENOTE p < 0.05 AND p < 0.01,\nRESPECTIVEL Y, AS ESTIMATED BY THE TWO – TAI L ED\nWILCOXON ’S SIGNED RANK TEST BETWEEN EACH\nMETHOD AND THE PROPOSED MULTISCALE\nCONVOLUTIONAL TRANSFORMER\nused to classify the fused features. To obtain the predicted\nprobability, we used the Softmax function. The loss function\nindicates the classiﬁcation loss obtained by cross–entropy as\nL =− 1\nN\nN∑\nn=1\nM∑\nm=1\nyn,mlog(pn,m) (9)\nwhere N is the number of trials and M is the number of classes.\nyn,m and pn,m denote the real label and predicted probability\nof the n–th trial for the class m, respectively.\n5) T raining and Optimization: In order to optimize the\ncross–entropy loss, we used AdamW [38]. We also utilized\nbatch normalization and dropout for each block, and used\ncosine annealing to speed up the training. The network was\ntrained for a maximum of 200 epochs, and the epoch with the\nlowest validation loss was selected.\nIV . R\nESULTS\nA. Decoding Performance Evaluation\nTo evaluate the decoding performance of the proposed\nmethod in a fair manner, we used the public and private dataset\ngathered from our experiment. We calculated the classiﬁcation\naccuracy using ﬁve–fold cross validation. The dataset was\nrandomly shufﬂed and divided into training, validation, and\ntest datasets in a 6:2:2 ratio. We performed the experiments on\nGeforce RTX 3090 GPUs with 24 GB memory. The machines\nhad Intel(R) Core(TM) i9-10980XE CPU @ 3.00 GHz with\n36 cores and 128 GB RAM. We implemented the deep learning\nmodels using the python 3.7 version.\nTable III presents the performance comparison of the con-\nventional and proposed methods using the mental imagery\ndataset. To investigate the optimal frequency range for the\nEEG classiﬁcation of mental imagery in different modalities,\nwe divided the frequency ranges into six bandwidths. The aver-\nage accuracy and standard deviation (std.) of each frequency\nband are listed in Table III.\nOur proposed method exhibits a superior performances in\nall frequency bands with a reasonable standard deviation. The\nfrequency ranges that include the δ band exhibited lower\naccuracy as compared to others. This indicates that the δ band\nis highly contaminated by the EOG artifacts. Moreover, the\nfrequency bands that include the γ band (30–120 Hz) exhibit\na better performance in the case of all the decoding algorithms.\nAlthough the difference between the 4–60 Hz and 4–120 Hz\nfrequency bands was not statistically signiﬁcant, the frequency\nrange with the high– γ band exhibited better performance\nin the case of the majority of the methods. However, only\nShallowConvNet [15] exhibited a superior performance in\nthe frequency band of 4–60 Hz as compared to the case\nof 4–120 Hz. EEGNet [17] exhibited robust performance\ncompared to DeepConvNet a nd ShallowConvNet within\n4–30 Hz, 4–50 Hz, and 4–120 Hz, while DeepConvNet [15]\nand ShallowConvNet showed better performance than EEGNet\nwithin 0.5–30 Hz, 0.5–60 Hz, and 0.5–120 Hz. TCACNet [39]\nshowed similar performance as DeepConvNet. However, the\nchannel attention module from the TCACNet showed less\neffective performance than our proposed models. Among the\nconventional models, TSception [37] exhibited a superior\nperformance as compared to the other CNN–based methods,\nwhich further introduces differ ent–sized kernels for handling\ntemporal dependencies. S3T [40] also showed decent perfor-\nmance as TSception, by utilizing CSP ﬁlters and multi–head\nattention.\nHowever, our proposed method exhibited the signiﬁcant\nimprovement over that of TSception. The highest average\nperformance of the proposed method was 0.62 ( ±0.07) in the\nfrequency band of 4–120 Hz. Although the std. of the classiﬁ-\ncation accuracy was slightly increased, the results indicate that\nthe average classiﬁcation accuracy of the proposed method was\nimproved by 0.05 as compared to that of TSception. To inves-\ntigate the statistically signiﬁcant difference among various fre-\nquency bands, a two-way analysis of variance (ANOV A, at the\nsigniﬁcance level of p<0.05) was used. One factor was the\nfrequency band, and the other factor was the decoding method.\nFurthermore, we performed the two–tailed Wilcoxon’s signed\nrank test to estimate the p–values of the competing baseline\nmodels and our proposed multiscale convolutional transformer,\nand compared the performance di fferences statistically. * and\n** denote p<0.05 and p<0.01, respectively.\nFig. 5 indicates a confusion matrix of six–class classiﬁ-\ncation using the proposed method. The classiﬁcation perfor-\nmance was averaged among 40 subjects over the frequency\nrange of 4–120 Hz. Both true positive and true negative are\nevenly distributed with a small proportion of false negative\nand false positive. The result shows that the proposed method\nis capable of decoding mental imagery in different modalities\nwithout bias to speciﬁc tasks.\nThe performance comparison of the private and public EEG\ndatasets is presented in Table IV. For a fair comparison, the\nBCIC IV 2a dataset and mental imagery dataset were both\npreprocessed using a frequency range of 4–60 Hz. Since\nthe ASU dataset was already band–pass ﬁltered a frequency\nrange of 8–70 Hz, we did not a pply additional ﬁltering to\navoid distortion. The proposed method exhibited outstanding\nperformance on the mental imagery dataset. In the case of\nBCIC IV 2a dataset, TCACNet showed the best performance\n652 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023\nTABLE IV\nPERFORMANCE COMPARISON OF THE CONVENTIONAL AND THE PROPOSED METHODS USING THE PRIVATE AND PUBLIC EEG DATASETS\nTABLE V\nABLATION STUDY OF THE TSFORMER ON DIFFERENT PARAMETERS OF TRANSFORMER ENCODER\nAND TEMPORAL CONVOLUTIONAL LAYER USING THE MENTAL IMAGERY DATASET\nFig. 5. Confusion matrix of six–class classiﬁcation using the proposed\nmethod with the frequency range of 4–120 Hz across 40 subjects. The\nx–axis indicates the predicted label, and they –axis displays the true\nlabel.\namong the comparison models. Nevertheless, the proposed\nmethod showed superior performance in BCIC IV 2a dataset\ncompared to rest of the conventional CNN models. In the case\nof ASU dataset, the proposed method reached the accuracy of\n0.7 for two–class SI classiﬁcation task. However, TSception\nshowed a better performance than the proposed model. The\nresult demonstrates that our proposed method is a suitable\nmodel for decoding EEG signals with class versatility.\nTable V shows the results of the ablation study for TSformer\non different parameters of transformer encoder and temporal\nconvolutional layer using the mental imagery dataset. The\ntemporal and spatial transformer (TSformer) is the basic\nversion of the proposed model without multiscale temporal\nconvolutional blocks and parallel spatial convolutional blocks.\nNote that the TSformer is a different version of the proposed\nmethod. We made some varia tions on each parameter of\nthe proposed method to explore the effect of the depth of\nthe transformer encoder ( D), the number of output channels\n(C\nout ), kernel size of the temporal convolutional layer (T), and\nthe kernel size and stride of the pooling layer (P,S). According\nto the results in table V, the optimal parameters are D = 4,\nO = 24, T = 62, ( P,S) = (16,8). As the depth (number of\nthe repetition) of the transformer encoder either increased or\ndecreased, it showed less performance than the case of D = 4.\nThe number of output channels for the temporal convolutional\nlayer is important to train the transformer encoder with an\nadequate amount of temporal features. With a smaller number\nof output channels, it results in decreased performance by\nan insufﬁcient amount of features to train the transformer\nencoder. With a larger number of output channels, it also\nshowed reduced accuracy due to larger parameters. The kernel\nsizes of the temporal convolutional layer were set as half,\nquarter, and half of a quarter of the sampling rate, to capture\nfrequency information at greater than 2 Hz, 4 Hz, and 8 Hz,\nrespectively. Although the average accuracy of T = 125 and\nT = 62 were similar, the standard deviation of the T =\n62 was smaller. The kernel size and stride size of the pooling\nlayer are also important to compress temporal features, which\nwill inﬂuence the pe rformance and computational cost of the\ntemporal transformer encoder. As the size of the pooling kernel\nand stride decreased, not only did the computational cost of\nthe model increase due to longer temporal sequences, but\nalso the performance has been d ecreased. With the larger size\nof the pooling kernel and stride, the computational cost was\ndecreased. However, the average accuracy has decreased due\nto the lack of a sufﬁcient amount of temporal sequences.\nFig. 6 shows the result of the ablation study on three impor-\ntant modules, 1) multi–scale ( MS) temporal convolutional\nblock, 2) temporal and spatial (TS) transformer encoder, and\n3) dual–stream (DS) spatial learner. MS–module is a modiﬁ-\ncation by replacing the temporal convolutional block with MS\ntemporal convolutional block. DS–module is a modiﬁcation\nto fuse different spatial features from spatial transformer\nencoder and spatial convolutional block. The Tformer–sConv\nand tConv–Sformer are models that the temporal or spatial\ntransformer encoder was replaced the convolutional block. The\nMS–TSformer–DS showed the highest performance through\nvariation models resulting average accuracy of 64 %. The\nAHN et al.: MULTISCALE CONVOLUTIONAL TRANSFORMER FOR EEG CLASSIFICA TION OF MENTAL IMAGERY 653\nFig. 6. Ablation study of the proposed method on the different modules\nusing the mental imagery dataset. The ‘×’ in the box plot represents the\naverage accuracy, and the bold horizontal line denotes median accuracy.\nThe ablation study was conducted focusing on three important parts,\n1) Multi–scale (MS) temporal convolutional block, 2) Temporal and spatial\n(TS) transformer encoder, and 3) Dual–stream (DS) spatial learner.\nTABLE VI\nC\nOMPARISON OF MODEL COMPLEXITY BASED ON THE\nNUMBER OF TRAINABLE PARAMETERS\nMS–TSformer and TSformer–DS showed better results than\nthe TSformer, which concludes that MS and DS modules\nare effective for extracting temporal and spatial features.\nAs the MS–TSformer–DS model showed the best performance,\nwe chose it as the proposed model. The replacement of the\ntemporal transformer encoder with additional temporal con-\nvolutional layers showed lower accuracy than the TSformer.\nHowever, in the case of the spatial transformer encoder,\nreplacing it with a spatial convolutional layer showed similar\naccuracy compared to the TSformer.\nTable VI shows the model complexity comparison in num-\nber of trainable parameters. The proposed method (TSformer)\nshows approximately 33K of trainable parameters while the\nMS-TSformer has 32K parameters owing to smaller size of\nkernels. TSformer-DS shows approximately 66K parameters\non account of duel–stream spatial convolutional layer. The\nDeepConvNet shows the largest number of trainable parame-\nters due to multiple convolutional layers and large number\nof output channels. The EEGNet shows the smallest number\nof trainable parameters although the performance was lower\nthan the proposed method. The TCACNet, which showed\nthe highest performance on BCI competition IV 2a dataset,\nshowed 233K parameters by placing second largest number of\ntrainable parameters. Our proposed methods shows balanced\nperformance while maintaining less amount of parameters\ncompared to conventional deep learning methods.\nFig. 7. t–SNE visualization of EEG features gathered from the last\nlayer of TSception and the proposed method. (Label 0: MI–left, Label 1:\nMI–right, Label 2: VI–split, Label 3: VI–fall in, Label 4: SI–go, and Label 5:\nSI–stop.)\nB. Feature Visualization\nWe visualized the learning procedure using t–stochastic\nneighbor embedding (t–SNE) to further interpret the proposed\nmodel further. We used the MS–TSformer–DS as it showed\nthe best performance among our proposed models. Fig. 7\npresents a comparison of the visualization using the TSception\nand the proposed method. The data of one epoch is visualized\nin the last layer. We used the data from subject 1 to display\nthe learned features. The six colors in the ﬁgure denote the\nsix–classes of the EEG signals (i.e., MI of “left hand” and\n“right hand”, VI of “split” and “fall in”, and SI of “go”\nand “stop”). In the case of the TSception, we could see that\nthree types of mental imagery have been distinguished, but the\nclasses within the same category of imagery were vague and\ndifﬁcult to differentiate. In the case of the proposed method,\nthe separation among the classes is clearly distinguished\ncompared to the result of TSception.\nAccording to the t–SNE result, the proposed model showed\nstrong capabilities to classify EEG–based mental imagery in\ndifferent modalities. Even though the separation level between\nthe classes in the same modality is closer to that of between\nthe classes in a different modality, it showed a decent level\nof separation. By combining the convolutional layers and\ntransformer encoder, our proposed model achieved enhanced\nfeature extraction capacity in the temporal, spatial, and spectral\ndomains.\nC. EEG Source Localization\nWe conducted ESL to examine activation patterns dur-\ning mental imagery in different modalities. We used the\n654 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023\nFig. 8. ESL results of all 40 subjects during imagery tasks. The row\nelements represent imagery tasks, and the column elements indicate\nthe different views of the source estimation model. The acronyms at the\ntop of the ﬁgures indicate the orientation of the brain. (L: left, R: right,\nF: front, and B: back.)\nsLORETA [36] implemented in Brainstorm [35], which is an\ninverse solution method.\nFig. 8 depicts the grand–average source localization results\nacross all 40 subjects obtained using sLORETA. The source\nlocalization was performed on each class of mental imagery.\nAll the trials and times were averaged into a single value. The\nfrequency ranges were set as 0.5–120 Hz in order to survey the\nmost prominent brain activity. Through all the imagery tasks,\nthe shared neural activities were revealed in the prefrontal\ncortex (PFC) and supplementary motor area (SMA), while the\nPFC exhibited stronger activity. EEG source estimations were\ncaptured from the top, left, and right views. White letters on\nthe top of the ﬁgure refer to L: left, R: right, F: front, and\nB: back, respectively.\nDuring both MI tasks, the left dorsolateral prefrontal cortex\n(dlPFC), the center of primary motor cortex (M1), and the pri-\nmary somatosensory cortex (S1) exhibited predominant neural\nactivity. From the top view, we can observe the symmetrical\nhighlighting patterns of S1 depending on the tasks. For the MI\nof “left hand”, the bilateral temporal lobe, left inferior parietal\nlobule, and right inferior occipital lobe were activated. While\nthe MI of the “right hand”, the left ventrolateral PFC(vlPFC)\nwas activated as well as the left dlPFC.\nAs both of the VI tasks were performed, we observed the\nactivity over the ventromedial PFC (vmPFC), primary visual\ncortex (V1), and inferior temporal gyrus (ITG). During the\nVI of the “split”, we can observe the connected patterns from\nthe PFC to V1. The superior parietal lobule, and right inferior\nparietal lobule were also activated. While the VI of the “fall in”\nwas performed, the vmPFC, right dlPFC, and superior occipital\ngyrus were activated.\nFor both the SI tasks, we iden tiﬁed common neural repre-\nsentation in the left vmPFC and Wernicke’s area. As the SI\nof the “go” was performed, the right inferior parietal lobule\nand right V1 were activated. During the SI of the “stop”, the\nleft ITG and left inferior frontal gyrus (IFG) exhibited strong\nactivity in addition to the left vmPFC. The overall results\nof ESL indicate that distinguishable spatial patterns could be\npresented depending on the type of mental imagery.\nV. D\nISCUSSION\nThis study demonstrates the possibility of classifying mul-\ntiple mental imagery tasks using only brain signals. First\nof all, we acquired EEG signals related to various mental\nimagery EEG signals from 40 subjects. In order to acquire\nEEG signals of MI, VI, and SI independently, we designed the\nexperimental paradigms delicately and conducted experiments\nin a strict environment. Also, our proposed network decoded\na total of six mental imagery tasks (MI–“left hand” and “right\nhand”, VI–“split” and “fall in”, and SI–“go” and “stop”)\nwith high performance. EEG classiﬁcation of mental imagery\nin different modalities can be used to overcome the limited\nnumber of classes and low classiﬁcation performances within\none mental imagery category. In addition, we conducted a\nsource localization analysis t o conﬁrm that mental imagery\nin different modalities has notable differences in spectral—\nspatial patterns. Hence, we believe that it can be the signiﬁcant\ncontribution to future research in decoding mental imagery.\nA. Source Localization Analysis\ni Through a source localization analysis across all the mental\nimagery tasks, the ESL results showed common neural activity\nin the PFC and SMA. The source of the mental imagery is\nunknown, but it is likely that the executive structure in the PFC\nis critical [41]. Furthermore, the SMA is known to contribute\nnot only to multiple aspects of motor behavior, but also\nfunctions as a core network of brain regions recruited during\nimagery, irrespective of the task [42]. The overall results\nof source estimation imply that unique brain networks are\nengaged during MI, VI, and SI tasks and the neural representa-\ntions of the cortical activity can be revealed in the connectivity\nof the spatial–spectral–temporal domain. The result shows\nthat valuable features could b e extracted in different brain\nregions and frequency bands from three different imagery\ntypes. In the case of MI tasks, as expected from the extant\nliterature, the M1 exhibited the predominant neural activity.\nMoreover, the dlPFC exhibited strong activity, which has been\nfound to be involved in superordinate control functions for\nvarious cognitive tasks such as decision making, novelty detec-\ntion, working memory, conﬂict management, mood regulation,\nAHN et al.: MULTISCALE CONVOLUTIONAL TRANSFORMER FOR EEG CLASSIFICA TION OF MENTAL IMAGERY 655\ntheory of mind processing, and timing [43]. In the case of VI\ntasks, the occipital lobe exhibited the predominant activity as\ncompared to the other mental im agery tasks. The mechanism\nof VI is known as it has a neural mechanism similar to that\nof visual perception [44], [45], especially over V1. It shows\noverlapping neural representations much like a weak version\nof afferent perception [41], [46]. Moreover, the activation of\nthe left ITG implies the relationship with the left fusiform\ngyrus. The exact functionality of the fusiform gyrus is still\ndisputed, but there is relative consensus on its involvement in\nface and body recognition, and within–category identiﬁcations.\nMoreover, a recent study has shown that VI engages the left\nfusiform gyrus before it inﬂu ences the early visual cortex\n[47]. In the case of SI tasks, although the motor related\ncortical activity of articulation imagery [48] was not visible\non the present ﬁgure, Broca’s and Wernicke’s areas have been\nhighlighted, which take part in language processing [49].\nB. Model Analysis\nBased on the vast source estimation results and the several\nresearches on brain network of mental imagery, we propose\na multiscale convolutional transformer for EEG classiﬁcation\nin different modalities. Contri butions of the proposed method\nare summarized as follow. First, we adopted a multiscale\ntemporal convolutional block to extract a various range of\nspectral features. Second, we extracted global and local spa-\ntial information by utilizing parallel spatial convolutional\nblocks and preserved EEG channel relations with spatial\nmapping. Thrid, we devised regional—spatial self—attention\nto emphasize the prominent brain region instead of the channel\nattention method, which is computationally expensive and\nless effective. One of the signiﬁcant differences between the\ntransformer–based models and RNN–based models is that the\ntransformer does not have the same memory bottleneck as\nRNN–based models, which means they have direct access to\nall previous sequences. The RNN–based models, in contrast,\nonly have one current state, which is adjusted with each new\ninput. Thus, this increased memory capacity is the capacity\nto “remember” exactly which sequences precede the current\nsequence. Also, our proposed method used broad frequency\nranges which is adequate to captu re valuable spectral features\nfrom different types of imagery.\nC. Limitations and Future Works\nEven though the overall performance of the proposed\nmethod showed relatively robust performance and source\nlocalization results of mental imagery in different modality\nwere correspond with neurophysiological evidences reported\nin related works, there are still some of limitations in our study.\nFirst of all, although the entire experiments were conducted\nconsecutively, sequences of the experimental protocol are\ndifferent from each mental imagery in different modalities.\nThe main reason for the ununiﬁed experimental design in this\nstudy is to follow the optimized protocols for each mental\nimagery. Thus, we will continue the additional experiment\nwith uniﬁed experimental protocols to validate if the signal\nquality is comparable with the original experimental para-\ndigm. Secondly, our experiments are conducted in a strictly\ncontrolled laboratory environment, this method may not be\nreliable in real–world environment. Also, as the proposed\nmethod is restricted to ofﬂine classiﬁcation, it is not suitable\nfor online classiﬁcation.\nAlso, one of the limitations of the grand–average source\nestimation result is that some of the brain networks are func-\ntioning as a subject–dependent matters. Since human imag-\ninations depend on an individual’s unique memory, it could\nresult in totally different neural representations even though\nthe experimental instructions were identical. Moreover, some\nstudies reported that mental imagery often shows cross–modal\naspects [50]. For example, motor imagery was not always\nshow prominent representations on the motor–related brain\nregions but also engages more prominent in visual–associated\nareas for some participants. Also, time–average source estima-\ntion results shows limited temporal–spatial patterns. Speciﬁ-\ncally, we could ﬁnd repetitive activation patterns in source\nestimation during a certain period of time with video format,\nbut some of these patterns were invisible in time–average\nimage.\nIn future works, investigations on cross–modal mental\nimagery tasks would be valuable for expanding the limited\nDoF of the BCI system with robust performances. Further-\nmore, the development of an online decoding model and bridg-\ning the gap between laboratory and real–world environment\nwould be desirable.\nVI. C\nONCLUSION\nVarious mental imageries such as MI, VI, and SI use the\nneural signals generated by performing designated mental\nimagery tasks without having any restrictions on external\ndevices. In this study, we proposed the multiscale convolu-\ntional transformer based on self–attention on spatial–spectral–\ntemporal domain. Our proposed method outperformed the\nconventional deep learning methods on classifying mental\nimagery in different modalities with an accuracy of 0.62. The\nstability of the method and the effectiveness of each module\nwere also demonstrated. The result of the ESL analysis shows\nthat the functioning brain network and EEG representation\nare different for each type of mental imagery. This study\ndemonstrates the possibility of multi–class inter–modality clas-\nsiﬁcation through robust feature extraction from each mental\nimagery task.\nR\nEFERENCES\n[1] J. J. Daly and J. R. Wolpaw, “Brain–computer interfaces in neurological\nrehabilitation,” Lancet Neurol., vol. 7, no. 11, pp. 1032–1043, 2008.\n[2] B. J. Edelman et al., “Noninvasiv e neuroimaging enhances continuous\nneural tracking for robotic device control,” Sci. Robot., vol. 4, no. 31,\nJun. 2019, Art. no. eaaw6844.\n[3] A. J. Young and D. P. Ferris, “State of the art and future directions\nfor lower limb robotic exoskeletons,” IEEE Trans. Neural Syst. Rehabil.\nEng., vol. 25, no. 2, pp. 171–182, Feb. 2017.\n[4] M.-H. Lee, J. Williamson, D.-O. Won, and S.-W. Lee, “A high per-\nformance spelling system based on EEG-EOG signals with visual\nfeedback,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 26, no. 7,\npp. 1443–1459, Jul. 2018.\n656 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023\n[5] J.-H. Cho, J.-H. Jeong, and S.-W. Lee, “NeuroGrasp: Real-time EEG\nclassiﬁcation of high-level motor imagery tasks using a dual-stage\ndeep learning framework,” IEEE Trans. Cybern. , vol. 52, no. 12,\npp. 13279–13292, Dec. 2022.\n[6] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, “Speech synthesis\nfrom neural decoding of spoken sentences,” Nature, vol. 568, no. 7753,\npp. 493–498, Apr. 2019.\n[7] N. Kosmyna, J. T. Lindgren, and A. Lécuyer, “Attending to visual stimuli\nversus performing visual imagery as a control strategy for EEG-based\nbrain-computer interfaces,” Sci. Rep., vol. 8, no. 1, pp. 1–14, Dec. 2018.\n[8] C. H. Nguyen, G. K. Karavas, and P. Artemiadis, “Adaptive multi-\ndegree of freedom brain computer interface using online feedback:\nTowards novel methods and metrics of mutual adaptation between\nhumans and machines for BCI,” PLoS ONE, vol. 14, no. 3, Mar. 2019,\nArt. no. e0212620.\n[9] L. Wang, X. Liu, Z. Liang, Z. Yang, and X. Hu, “Analysis and classi-\nﬁcation of hybrid BCI based on motor imagery and speech imagery,”\nMeasurement, vol. 147, Dec. 2019, Art. no. 106842.\n[10] N. Furutani et al., “Neural decoding of multi-modal imagery behavior\nfocusing on temporal complexity,” Frontiers Psychiatry, vol. 11, p. 746,\nJul. 2020.\n[11] A. M. Roy, “An efﬁcient multi-scale CNN model with intrinsic\nfeature integration for motor imagery EEG subject classiﬁcation in\nbrain-machine interfaces,” Biomed. Signal Process. Control, vol. 74,\nApr. 2022, Art. no. 103496.\n[12] Y . Du and J. Liu, “IENet: A robust convolutional neural network for\nEEG based brain-computer interfaces,” J. Neural Eng., vol. 19, no. 3,\nJun. 2022, Art. no. 036031.\n[13] M. Suchetha, R. Madhumitha, M. S. Meena, and R. Sruth i, “Sequential\nconvolutional neural networks for classiﬁcation of cognitive tasks from\nEEG signals,”Appl. Soft Comput., vol. 111, Nov. 2021, Art. no. 107664.\n[14] W. Ma et al., “A novel multi-bran ch hybrid neural network for motor\nimagery EEG signal classiﬁcation,” Biomed. Signal Process. Control,\nvol. 77, Aug. 2022, Art. no. 103718.\n[15] R. T. Schirrmeister et al., “Deep learning with convolutional neural\nnetworks for EEG decoding and visualization,” Hum. Brain Mapping,\nvol. 38, no. 11, pp. 5391–5420, 2017.\n[16] K. K. Ang, Z. Y . Chin, H. Zhang, and C. Guan, “Filter bank common\nspatial pattern (FBCSP) in brain-computer interface,” in Proc. IEEE\nInt. Joint Conf. Neural Netw. (IEEE World Congr. Comput. Intell.),\nJun. 2008, pp. 2390–2397.\n[17] V . J. Lawhern, A. J. Solon, N. R. Waytowich, S. M. Gordon, C. P. Hung,\nand B. J. Lance, “EEGNet: A compact convolutional neural network for\nEEG-based brain–computer interfaces,” J. Neural Eng., vol. 15, no. 5,\nOct. 2018, Art. no. 056013.\n[18] J.-H. Jeong, N.-S. Kwak, C. Guan, and S.-W. Lee, “Decoding movement-\nrelated cortical potentials based on s ubject-dependent and section-wise\nspectral ﬁltering,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 28, no. 3,\npp. 687–698, Mar. 2020.\n[19] J. M. Antelis, B. Gudiño-Mendoza, L. E. Falcón, G. Sanchez-Ante,\nand H. Sossa, “Dendrite morphological neural networks for motor\ntask recognition from electroencephalographic signals,” Biomed. Signal\nProcess. Control, vol. 44, pp. 12–24, Jul. 2018.\n[20] G. C. D. Virgilio, J. H. Sossa A., J. M. Antelis, and L. E. Falcón,\n“Spiking neural networks applied to t he classiﬁcation of motor tasks in\nEEG signals,” Neural Netw., vol. 122, pp. 130–143, Feb. 2020.\n[21] A. Vaswani et al., “Attention is all you need,” in Proc. Adv. Neural\nInform. Process. Syst. (NeurIPS), 2017, pp. 5998–6008.\n[22] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training\nof deep bidirectional transform ers for language understanding,” 2018,\narXiv:1810.04805.\n[23] A. Dosovitskiy et al., “An image is worth 16 ×16 words: Transformers\nfor image recognition at scale,” 2020,\narXiv:2010.11929.\n[24] S. Hochreiter and J. Schmi dhuber, “Long short-term memory,” Neural\nComput., vol. 9, no. 8, pp. 1735–1780, 1997.\n[25] S. Bagchi and D. R. Bathula, “EEG-C onvTransformer for single-trial\nEEG-based visual stimulus classiﬁcation,” Pattern Recognit., vol. 129,\nSep. 2022, Art. no. 108757.\n[26] D. Kostas, S. Aroca-Ouellette, and F. Rudzicz, “BENDR: Using trans-\nformers and a contrastive self-supervised learning task to learn from\nmassive amounts of EEG data,” Frontiers Hum. Neurosci., vol. 15,\np. 253, Jun. 2021.\n[27] W. Qu et al., “A residual based attention model for EEG based sleep stag-\ning,” IEEE J. Biomed. Health Informat., vol. 24, no. 10, pp. 2833–2843,\nOct. 2020.\n[28] R. Maldonado and S. M. Harabagiu, “ Active deep learning for the iden-\ntiﬁcation of concepts and relations in electroencephalography reports,”\nJ. Biomed. Informat., vol. 98, Oct. 2019, Art. no. 103265.\n[29] Y . Zheng, X. Zhao, and L. Yao, “Copula-based transformer in EEG to\nassess visual discomfort induced by stereoscopic 3D,” Biomed. Signal\nProcess. Control, vol. 77, Aug. 2022, Art. no. 103803.\n[30] D. Li, L. Xie, B. Chai, Z. Wang, and H. Yang, “Spatial-frequency\nconvolutional self-attention network for EEG emotion recognition,”\nAppl. Soft Comput., vol. 122, Jun. 2022, Art. no. 108740.\n[31] C. Brunner, R. Leeb, G. Müller-Putz , A. Schlögl, and G. Pfurtscheller,\n“BCI competition 2008–Graz data set A,” Inst. Knowl. Discovery\n(Lab. Brain-Comput. Interfaces), Graz Univ. Technol., vol. 16, pp. 1–6,\nJan. 2008.\n[32] C. H. Nguyen, G. K. Karavas, and P . Artemiadis, “Inferring imagined\nspeech using EEG signals: A new approach using Riemannian manifold\nfeatures,” J. Neural Eng., vol. 15, no. 1, Feb. 2018, Art. no. 016002.\n[33] B. Blankertz et al., “The Berlin brain–computer interface: Non-medical\nuses of BCI technology,” Frontiers Neurosci., vol. 4, p. 198, Dec. 2010.\n[34] A. Delorme and S. Makeig, “EEGLAB: An open source toolbox for\nanalysis of single-trial EEG dynamics including independent component\nanalysis,” J. Neurosci. Methods, vol. 134, no. 1, pp. 9–21, Mar. 2004.\n[35] F. Tadel, S. Baillet, J. C. Mosher, D. Pantazis, and R. M. Leahy, “Brain-\nstorm: A user-friendly application for MEG/EEG analysis,” Comput.\nIntell. Neurosci., vol. 2011, p. 13, Oct. 2011.\n[36] R. D. Pascual-Marqui , “Standardized low-resolution brain electromag-\nnetic tomography (sLORETA): Technical details,” Methods Findings\nExp. Clin. Pharmacol., vol. 24, pp. 5–12, Feb. 2002.\n[37] Y . Ding et al., “TSception: A deep learning framework for emotion\ndetection using EEG,” in Proc. Int. Joint Conf. Neural Netw. (IJCNN),\nJul. 2020, pp. 1–7.\n[38] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”\n2017, arXiv:1711.05101.\n[39] X. Liu et al., “TCACNet: Temporal and channel attention convolutional\nnetwork for motor imagery classiﬁcation of EEG-based BCI,” Inf.\nProcess. Manage., vol. 59, no. 5, Sep. 2022, Art. no. 103001.\n[40] Y . Song, X. Jia, L. Yang, and L. Xie, “Transformer-based spatial–\ntemporal feature learning for EEG decoding,” 2021, arXiv:2106.11170.\n[41] J. Pearson, T. Naselaris, E. A. Holmes, and S. M. Kosslyn, “Mental\nimagery: Functional mechanisms and clinical applications,” Trends Cog-\nnit. Sci., vol. 19, no. 10, pp. 590–602, Oct. 2015.\n[42] C. McNorgan, “A meta-analytic review of multisensory imagery\nidentiﬁes the neural correlate s of modality-speciﬁc and modality-\ngeneral imagery,” Frontiers Hum. Neurosci. , vol. 6, p. 285,\nOct. 2012.\n[43] I. Hertrich, S. Dietrich, C. Bl um, and H. Ackermann, “The role of\nthe dorsolateral prefrontal cortex for speech and language processing,”\nFrontiers Hum. Neurosci., vol. 15, p. 217, May 2021.\n[44] N. Dijkstra, S. E. Bosch, and M. A. J. van Gerven, “Shared neural\nmechanisms of visual perception and imagery,” Trends Cognit. Sci.,\nvol. 23, no. 5, pp. 423–434, May 2019.\n[45] S. Xie, D. Kaiser, and R. M. Cichy, “Visual imagery and perception\nshare neural representations in the alpha frequency band,” Curr. Biol.,\nvol. 30, no. 13, pp. 2621–2627, 2020.\n[46] J. Pearson, “The human imagin ation: The cognitive neuroscience\nof visual mental imagery,” Nature Rev. Neurosci., vol. 20, no. 10,\npp. 624–634, Oct. 2019.\n[47] A. Spagna, D. Hajhajate, J. Liu , and P. Bartolomeo, “Visual mental\nimagery engages the left fusiform gyrus, but not the early visual cortex:\nA meta-analysis of neuroimaging evidence,” Neurosci. Biobehav. Rev.,\nvol. 122, pp. 201–217, Mar. 2021.\n[48] A. Jahangiri and F. Sepulveda, “Th e relative contribution of high-gamma\nlinguistic processing stages of word production, and motor imagery of\narticulation in class separability of covert speech tasks in EEG data,”\nJ. Med. Syst., vol. 43, no. 2, pp. 1–9, Feb. 2019.\n[49] X. Tian, J. M. Zarate, and D. Poeppel, “Mental imagery of speech\nimplicates two mechanisms of perceptual reactivation,” Cortex, vol. 77,\npp. 1–12, Apr. 2016.\n[50] B. Nanay, “Multimodal mental imagery,” Cortex, vol. 105, pp. 125–134,\nAug. 2018.",
  "topic": "Brain–computer interface",
  "concepts": [
    {
      "name": "Brain–computer interface",
      "score": 0.7960660457611084
    },
    {
      "name": "Motor imagery",
      "score": 0.7900055646896362
    },
    {
      "name": "Electroencephalography",
      "score": 0.7756400108337402
    },
    {
      "name": "Computer science",
      "score": 0.6877004504203796
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5794478058815002
    },
    {
      "name": "Artificial intelligence",
      "score": 0.576653242111206
    },
    {
      "name": "Modalities",
      "score": 0.5561573505401611
    },
    {
      "name": "Mental image",
      "score": 0.5467596650123596
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4346136152744293
    },
    {
      "name": "Transformer",
      "score": 0.42539751529693604
    },
    {
      "name": "Speech recognition",
      "score": 0.3542962670326233
    },
    {
      "name": "Cognition",
      "score": 0.2621259093284607
    },
    {
      "name": "Psychology",
      "score": 0.13669678568840027
    },
    {
      "name": "Neuroscience",
      "score": 0.10171481966972351
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I197347611",
      "name": "Korea University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I163753206",
      "name": "Chungbuk National University",
      "country": "KR"
    }
  ]
}