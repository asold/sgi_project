{
  "title": "LLM-RM at SemEval-2023 Task 2: Multilingual Complex NER Using XLM-RoBERTa",
  "url": "https://openalex.org/W4385570013",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1975855560",
      "name": "Rahul Mehta",
      "affiliations": [
        "Indian Institute of Technology Hyderabad"
      ]
    },
    {
      "id": "https://openalex.org/A2146554912",
      "name": "Vasudeva Varma",
      "affiliations": [
        "Indian Institute of Technology Hyderabad"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2042188227",
    "https://openalex.org/W4385572425",
    "https://openalex.org/W3156534109",
    "https://openalex.org/W3162486530",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W4294294857",
    "https://openalex.org/W4389519502",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4211191796",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4287854446"
  ],
  "abstract": "Named Entity Recognition(NER) is a task ofrecognizing entities at a token level in a sen-tence. This paper focuses on solving NER tasksin a multilingual setting for complex named en-tities.Our team, LLM-RM participated in therecently organized SemEval 2023 task, Task 2:MultiCoNER II,Multilingual Complex NamedEntity Recognition. We approach the problemby leveraging cross-lingual representation pro-vided by fine-tuning XLM-Roberta base modelon datasets of all of the 12 languages provided - Bangla, Chinese, English, Farsi, French,German, Hindi, Italian, Portuguese, Spanish,Swedish and Ukrainian.",
  "full_text": "Proceedings of the The 17th International Workshop on Semantic Evaluation (SemEval-2023), pages 453–456\nJuly 13-14, 2023 ©2023 Association for Computational Linguistics\nLLM-RM at SemEval-2023 Task 2: Multilingual Complex NER using\nXLM-RoBERTa\nRahul Mehta\nIIIT Hyderabad,India\nrahul.mehta@research.iiit.ac.in\nVasudeva Varma\nIIIT Hyderabad,India\nvv@iiit.ac.in\nAbstract\nNamed Entity Recognition(NER) is a task of\nrecognizing entities at a token level in a sen-\ntence. This paper focuses on solving NER tasks\nin a multilingual setting for complex named en-\ntities.Our team, LLM-RM participated in the\nrecently organized SemEval 2023 task, Task 2:\nMultiCoNER II,Multilingual Complex Named\nEntity Recognition. We approach the problem\nby leveraging cross-lingual representation pro-\nvided by fine-tuning XLM-Roberta base model\non datasets of all of the 12 languages pro-\nvided - Bangla, Chinese, English, Farsi, French,\nGerman, Hindi, Italian, Portuguese, Spanish,\nSwedish and Ukrainian.\n1 Introduction\nNamed Entity Recognition(NER) is the task of rec-\nognizing entities (e.g., person, location, organiza-\ntion) in a piece of text.\nMost of the NER datasets like CoNLL2003 (Erik\nF. Tjong and Meulder, 2003) are focussed on high-\nresource languages like English and are specific to\na given domain like news. The SemEval-2023 Task\n2, Multilingual Complex Named Entity Recogni-\ntion (MultiCoNER II) (Fetahu et al., 2023b) con-\ntains NER datasets curated for 12 languages which\nare low resource and from other domains as well.\nThe task also provided a large number of 30 classes\nand simulated errors being added to the test set to\nmake it more challenging. It contains the following\nlanguages: English, Spanish, Dutch, Russian, Turk-\nish, Portugues, Korean, Farsi, German, Chinese,\nHindi, and Bangla.\nWith the advent of Deep Learning based mod-\nels, Transformer models like BERT, and bidirec-\ntional LSTM based ELMo became state of the art.\nBERT’s multilingual counterpart, mBERT became\nstate of the art in multilingual NER tasks. Another\nmodel XLM-RoBERTa(XLM-R) (Alexis Conneau\nand Stoyanov, 2020) has shown to outperform\nmBERT on low resource languages on the NER\ntasks. XLM-R is suitable for our task as it is pre-\ntrained for more than 100 languages including the\n4 languages which we worked upon. Therefore, we\nhave used XLM-RoBERTa-base transformer model\nand fine-tuned it on each of the given 12 languages.\n1.1 The Challenge\nWe participated in the shared task named Multi-\nCoNER II, which is part of Semeval 2023. The task\nconsists of building a named entity recognition sys-\ntem in 12 languages. There are 2 tracks to the tasks,\none is the monolingual track for each language and\nthe other is a multilingual track consisting of all\nlanguages. The condition of the competition was\nthat one can only use the prediction of the mono-\nlingual model in the monolingual track and cannot\nuse it for the multilingual track\nThe task is a successor to the 2022 challenge of\nMultiCoNER shared task, (Malmasi et al., 2022b)\nwhere the key challenges were the following 1)\nsyntactically complex entities like movies titles e.g\nShawshank Redemption 2) these complex entities\nhaving low context and 3) long-tail entity distribu-\ntions.\nThese challenges of NER for recognizing com-\nplex entities and in low-context situations were\nmentioned by Meng et al. (2021). The authors\nmention that for complex entities, some particular\ntypes (e.g creative works) can be linguistically com-\nplex. They can be complex noun phrases (Eternal\nSunshine of the Spotless Mind), gerunds (Saving\nPrivate Ryan), infinitives (To Kill a Mockingbird),\nor full clauses (Mr Smith Goes to Washington). For\nlong-tail entities, they mention that many domain\nentities have large long-tail distribution, with poten-\ntially millions of values (e.g., location names). It’s\nvery hard to build representative training data, as it\ncan only cover a portion of the potentially infinite\npossible values.\nThe current MultiCoNER II challenge, expands\nthe previous challenge of 2022 with new tasks and\n453\nit emphasize the shortcomings of the current top\nmodels that are transformers based and depends on\nknowledge bases. It focusses on challenges like\nout of knowledge-base entities and noisy scenarios\nlike the presence of spelling mistakes and typos.\n2 Related Work\nThe earliest use of the neural networks to solve\nNER was by (Hammerton, 2003) who attempted to\nuse an LSTM network to solve the problem. Recent\ntransformer networks like BERT (Jacob Devlin and\nToutanova, 2018) and ELMo (Matthew E. Peters\nand Zettlemoyer, 2018) have further led to the state\nof the art results in the NER task for English lan-\nguage.\nCRF layer (Sutton and McCallum, 2012) was\nalso proposed to be used as a final layer for token\nclassification.\nTo expand NER in a multilingual setting, (Fe-\ntahu et al., 2021) introduced a multilingual and\ncode-mixed dataset concerning the search domain.\nThey also used an XLM-RoBERTa model com-\nbined with gazetteers to build their multilingual\nNER classification system.\nIn a paper for the first edition of this task in\n2022, XLM-RoBERTa has been applied to Hindi\nand English dataset and have shown to perform\nbetter than mBERT with a similar set of languages\nas in this current task (Sumit Singh and Tiwary,\n2022).\n3 Data\nThe dataset was first released in the Semeval task in\n2022, called MultiCoNER dataset (Malmasi et al.,\n2022a). In the 2nd edition of the task, The organ-\nisers provided a new dataset called MultiCoNER\nv2 (Fetahu et al., 2023a) comprising of individual\nlanguage datasets in 12 languages.\nTable 1 contains the number of sentences in the\ntraining, development and test datasets per lan-\nguage in MulticoNER v2 dataset.\nThe test dataset is used for the final evaluation of\nthe leaderboard and is further split into corrupted\nand non-corrupted sets. It is to be noted that the\nuncorrupted test set size is quite large compared to\nthe training set for all the languages.\nTable 2 contains the list of 30 entities present\nacross all datasets and their grouping into the cor-\nresponding entity types.\nLanguage Train Dev Test-1 Test-2\nBN 9708 507 0 19,859\nZH 9,759 506 5696 14,569\nEN 16,778 871 74,960 21,0267\nDE 9,785 512 5,880 16,334\nFA 16,321 855 0 219,168\nFR 16,548 857 74918 174,868\nHI 9,632 514 0 18,406\nIT 16,579 858 74,334 173,547\nPT 16,469 854 68,822 160,668\nES 16,453 854 74,050 252,257\nSV 16,363 856 69,342 161,848\nUK 16,429 851 0 238,296\nTable 1: Sentences per split (Train,Test,Test-1 : Test-\nCorrupted, Test-2: Test-Uncorrupted) per language\nwhere BN is Bangla, ZH is Chinese, EN is English,\nFA is Farsi, FR is French, DE is German,HI is Hindi,\nIT is Italian, PT is Portuguese, ES is Spanish and UK is\nUkranian language\nFigure 1: NER token classification using transformer\nencoder models like RoBERTa\n4 Methodology\nWe utilised XLM-RoBERTa model for all 12 lan-\nguages we participated in. XLM-RoBERTa is a\nmassive Transformer trained on 100+ languages on\n2TBs of CommonCrawl data.\nFigure 1 describes the architecture of trans-\nformer encoder models like RoBERTa. It is trained\nwith a multilingual MLM objective using only\nmonolingual data. XLM-RoBERTa has already\nbeen shown to outperform mBERT on cross-lingual\nclassification by upto 23 percent accuracy on low-\nresource languages. It also gave competitive results\nwith respect to the state of the art monolingual mod-\nels.\n454\nEntity Name Entity Type\nFacility, OtherLOC,\nHumanSettlement,\nStation Location\nVisualWork,\nMusicalWork,\nWrittenWork, ArtWork,\nSoftware Creative Work\nMusicalGRP,\nPublicCORP,\nPrivateCORP, ORG,\nAerospaceManufacturer,\nSportsGRP,\nCarManufacturer Group\nScientist, Artist,\nAthlete, OtherPER\nPolitician, Cleric,\nSportsManager, Person\nClothing, Vehicle,\nFood, Drink, OtherPROD Product\nMedication/Vaccine,\nMedicalProcedure,\nAnatomicalStructure,\nSymptom,\nDisease Medical\nTable 2: List of Entities\n4.1 Experimental Set up\nWe have used the PyTorch framework and Hug-\ngingFace’s Tranformers library for our system.The\ntraining is done on a single GPU Nvidia Titan 2080\nmachine.\nWe used the XLM-RoBERTa-base model and\nfine-tuned it on each of the language datasets. We\ntrain the model in a batch size of 4 on the training\ndataset. A dropout of 0.2 is applied between the\nlast hidden layer and the output layer to prevent\nthe model from overfitting. We used a learning\nrate of 2e-5. For the optimizer, we used AdamW\n(Loshchilov and Hutter, 2019) optimizer with an\nepsilon of 1e-08. AdamW is a stochastic optimiza-\ntion method and it modifies the implementation of\nweight decay in Adam, by separating weight decay\nfrom the gradient update. The max length of the\nsequence used is 16. We trained the model on 15\nepochs and select the best one based on f1-scores\non the test set.\nFor token classification at the end, we used a\nlinear layer as the last layer to classify into the\ngiven 30 named entities.\nThe final model is based on the checkpoints of\nthe model which are selected based on best F1-\nscores on the development set.\n5 Results\nTable 3 shows the macro average scores of preci-\nsion, recall and F-1.\nLanguage Precision Recall F1-Score\nBN 62.18 61.60 60.46\nZH 26.51 27.34 25.50\nEN 55.97 52.85 53.11\nFA 53.41 54.87 53.13\nFR 53.17 54.42 53.33\nDE 66.56 59.92 61.62\nHI 70.64 69.17 69.04\nIT 60.48 62.48 60.97\nPT 64.08 63.59 63.41\nES 61.02 58.19 58.32\nSV 57.19 60.03 57.55\nUK 55.59 49.22 49.06\nTable 3: Scores of XLM-RoBERTa-base on 12 lan-\nguages on development set where BN is Bangla, ZH is\nChinese, EN is English, FA is Farsi, FR is French, DE\nis German,HI is Hindi, IT is Italian, PT is Portuguese,\nES is Spanish and UK is Ukranian language\nEntity-Type HI EN DE SP\nGroup 73.11 55.54 65.35 61.82\nMedical 75.69 56.27 55.18 62.68\nPerson 58.87 46.99 51.80 48.66\nCreative Work 74.37 59.80 66.57 64.12\nProduct 59.95 46.88 48.90 54.55\nLocation 76.15 55.08 64.17 55.86\nTable 4: F1-Scores of XLM-RoBERTa-base on 4 lan-\nguages by Entity types on the development set\nFrom Table 4, we observe that the entities be-\nlonging to the Creative Work category consistently\nhave the highest F1-score across all 4 languages.\nAlso, it can be noted that the entities belonging\nto Person and Product group have the lowest F1-\nscores for the model.\nTable 5 shows the macro average F1-scores of\nthe final test set used for leaderboard\nFrom table 3 and table 5, we observe that the\nperformance for Hindi drops most from develop-\nment to test set, while for English and Spanish, it\njust drops slightly.\n455\nLang Overall F1 Corrupted F1 Uncor. F1\nHI 63.29 0 63.29\nEN 52.08 46.3 54.73\nDE 55.54 54.1 54.73\nES 54.81 49.32 57.42\nTable 5: Scores of XLM-RoBERTa-base on 4 languages\non the final test set where HI is Hindi, EN is English,DE\nis German and ES is Spanish language\nAlso, we couldn’t submit results for languages\nother than Hindi,English,German and Spanish by\nthe time the competition ended, therefore we only\nhave F1,corrupted f1 and uncorrupted F1 scores for\nthese languages for the test set.\n6 Conclusion\nIn this paper, we presented using XLM-RoBERTa-\nbase to solve the shared task of MultiCoNER.\nFuture work can include exploring more recent\ntransformer-based models like XLM-V with very\nlarge vocabularies. Also, data augmentation tech-\nniques like entity replacement can be tried.\nReferences\nNaman Goyal Vishrav Chaudhary Guillaume Wen-\nzek Francisco Guzmán Edouard Grave Myle Ott\nLuke Zettlemoyer Alexis Conneau, Kartikay Khan-\ndelwal and Veselin Stoyanov. 2020. Unsupervised\nCross-lingual Representation Learning at Scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451.\nKim Sang Erik F. Tjong and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natural\nLanguage Learning at HLT-NAACL, page 142– 147.\nBesnik Fetahu, Zhiyu Chen, Sudipta Kar, Oleg\nRokhlenko, and Shervin Malmasi. 2023a. Multi-\nCoNER v2: a Large Multilingual dataset for Fine-\ngrained and Noisy Named Entity Recognition.\nBesnik Fetahu, Anjie Fang, Oleg Rokhlenko, and\nShervin Malmasi. 2021. Gazetteer Enhanced Named\nEntity Recognition for Code-Mixed Web Queries. In\nProceedings of the 44th International ACM SIGIR\nConference on Research and Development in Infor-\nmation Retrieval, pages 1677–1681.\nBesnik Fetahu, Sudipta Kar, Zhiyu Chen, Oleg\nRokhlenko, and Shervin Malmasi. 2023b. SemEval-\n2023 Task 2: Fine-grained Multilingual Named En-\ntity Recognition (MultiCoNER 2). In Proceedings of\nthe 17th International Workshop on Semantic Evalua-\ntion (SemEval-2023). Association for Computational\nLinguistics.\nJames Hammerton. 2003. Named entity recognition\nwith long short-term memory. In Proceedings of the\nseventh conference on Natural language learning at\nHLT-NAACL 2003-Volume 4,Association for Compu-\ntational Linguistics, page 172–175.\nKenton Lee Jacob Devlin, Ming-Wei Chang and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In arXiv preprint, page arXiv:1810.04805.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nWeight Decay Regularization.\nShervin Malmasi, Anjie Fang, Besnik Fetahu, Sudipta\nKar, and Oleg Rokhlenko. 2022a. MultiCoNER: a\nLarge-scale Multilingual dataset for Complex Named\nEntity Recognition.\nShervin Malmasi, Anjie Fang, Besnik Fetahu, Sudipta\nKar, and Oleg Rokhlenko. 2022b. SemEval-2022\nTask 11: Multilingual Complex Named Entity Recog-\nnition (MultiCoNER). In Proceedings of the 16th\nInternational Workshop on Semantic Evaluation\n(SemEval-2022). Association for Computational Lin-\nguistics.\nMohit Iyyer Matt Gardner Christopher Clark Ken-\nton Lee Matthew E. Peters, Mark Neumann and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In NAACL 2018.\nTao Meng, Anjie Fang, Oleg Rokhlenko, and Shervin\nMalmasi. 2021. GEMNET: Effective gated gazetteer\nrepresentations for recognizing complex entities in\nlow-context input. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 1499–1512.\nPawankumar Jawale Sumit Singh and Uma Shanker\nTiwary. 2022. Transformer based NER models for\nHindi and Bangla languages. In The 16th Interna-\ntional Workshop on Semantic Evaluation.\nCharles Sutton and Andrew McCallum. 2012. An in-\ntroduction to conditional random fields. In Foun-\ndations and Trends® in Machine Learning, page\n4(4):267–373.\n456",
  "topic": "Named-entity recognition",
  "concepts": [
    {
      "name": "Named-entity recognition",
      "score": 0.8207423686981201
    },
    {
      "name": "SemEval",
      "score": 0.7839605212211609
    },
    {
      "name": "Computer science",
      "score": 0.7796906232833862
    },
    {
      "name": "Natural language processing",
      "score": 0.6396327018737793
    },
    {
      "name": "Task (project management)",
      "score": 0.6267579793930054
    },
    {
      "name": "German",
      "score": 0.5639718174934387
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5365680456161499
    },
    {
      "name": "Security token",
      "score": 0.46141254901885986
    },
    {
      "name": "Portuguese",
      "score": 0.4603971838951111
    },
    {
      "name": "Linguistics",
      "score": 0.24763333797454834
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}