{
  "title": "NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails",
  "url": "https://openalex.org/W4389524506",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1550587820",
      "name": "Traian Rebedea",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2036457054",
      "name": "Razvan Dinu",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3015093018",
      "name": "Makesh Narsimhan Sreedhar",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2806574362",
      "name": "Christopher Parisien",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2099623354",
      "name": "Jonathan Cohen",
      "affiliations": [
        "Nvidia (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4385327996",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W4385374425",
    "https://openalex.org/W4385572873",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4389524319",
    "https://openalex.org/W4285428875",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4281623759",
    "https://openalex.org/W4285247752",
    "https://openalex.org/W4389520749",
    "https://openalex.org/W4389519070",
    "https://openalex.org/W4365601361",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2966087730",
    "https://openalex.org/W4385894687",
    "https://openalex.org/W4320843360",
    "https://openalex.org/W4383473937",
    "https://openalex.org/W4322718421",
    "https://openalex.org/W4306808680",
    "https://openalex.org/W4389519449",
    "https://openalex.org/W4379468930"
  ],
  "abstract": "Traian Rebedea, Razvan Dinu, Makesh Narsimhan Sreedhar, Christopher Parisien, Jonathan Cohen. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 431–445\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nNeMo Guardrails: A Toolkit for Controllable and Safe\nLLM Applications with Programmable Rails\nTraian Rebedea∗, Razvan Dinu∗, Makesh Sreedhar, Christopher Parisien, Jonathan Cohen\nNVIDIA\nSanta Clara, CA\n{trebedea, rdinu, makeshn, cparisien, jocohen}@nvidia.com\nAbstract\nNeMo Guardrails is an open-source toolkit 1\nfor easily adding programmable guardrails to\nLLM-based conversational systems. Guardrails\n(or rails for short) are a specific way of control-\nling the output of an LLM, such as not talking\nabout topics considered harmful, following a\npredefined dialogue path, using a particular lan-\nguage style, and more. There are several mecha-\nnisms that allow LLM providers and developers\nto add guardrails that are embedded into a spe-\ncific model at training, e.g. using model align-\nment. Differently, using a runtime inspired\nfrom dialogue management, NeMo Guardrails\nallows developers to add programmable rails\nto LLM applications - these are user-defined,\nindependent of the underlying LLM, and inter-\npretable. Our initial results show that the pro-\nposed approach can be used with several LLM\nproviders to develop controllable and safe LLM\napplications using programmable rails.\n1 Introduction\nSteerability and trustworthiness are key factors for\ndeploying Large Language Models (LLMs) in pro-\nduction. Enabling these models to stay on track\nfor multiple turns of a conversation is essential for\ndeveloping task-oriented dialogue systems. This\nseems like a serious challenge as LLMs can be eas-\nily led into veering off-topic (Pang et al., 2023).\nAt the same time, LLMs also tend to generate re-\nsponses that are factually incorrect or completely\nfabricated (hallucinations) (Manakul et al., 2023;\nPeng et al., 2023; Azaria and Mitchell, 2023). In\naddition, they are vulnerable to prompt injection\n(or jailbreak) attacks, where malicious actors ma-\nnipulate inputs to trick the model into producing\nharmful outputs (Kang et al., 2023; Wei et al., 2023;\nZou et al., 2023).\nBuilding trustworthy and controllable conversa-\ntional systems is of vital importance for deploy-\n*Equal contribution\n1https://github.com/NVIDIA/NeMo-Guardrails\nFigure 1: Programmable vs. embedded rails for LLMs.\ning LLMs in customer facing situations. NeMo\nGuardrails is an open-source toolkit for easily\nadding programmable rails to LLM-based appli-\ncations. Guardrails (or rails) provide a mechanism\nfor controlling the output of an LLM to respect\nsome human-imposed constraints, e.g. not engag-\ning in harmful topics, following a predefined dia-\nlogue path, adding specific responses to some user\nrequests, using a particular language style, extract-\ning structured data. To implement the various types\nof rails, several techniques can be used, including\nmodel alignment at training, prompt engineering\nand chain-of-thought (CoT), and adding a dialogue\nmanager. While model alignment provides general\nrails embedded in the LLM at training and prompt\ntuning can offer user-specific rails embedded in a\ncustomized model, NeMo Guardrails allows users\nto define custom programmable rails at runtime as\nshown in Fig. 1. This mechanism is independent\nof alignment strategies and supplements embedded\nrails, works with different LLMs, and provides in-\nterpretable rails defined using a custom modeling\nlanguage, Colang.\n431\nTo implement user-defined programmable rails\nfor LLMs, our toolkit uses a programmable run-\ntime engine that acts like a proxy between the\nuser and the LLM. This approach is complemen-\ntary to model alignment and it defines the rules\nthe LLM should follow in the interaction with the\nusers. Thus, the Guardrails runtime has the role\nof a dialogue manager, being able to interpret and\nimpose the rules defining the programmable rails.\nThese rules are expressed using a modeling lan-\nguage called Colang. More specifically, Colang\nis used to define rules as dialogue flows that the\nLLM should always follow (see Fig. 2). Using a\nprompting technique with in-context learning and\na specific form of CoT, we enable the LLM to gen-\nerate the next steps that guide the conversation.\nColang is then interpreted by the dialogue manager\nto apply the guardrails rules predefined by users or\nautomatically generated by the LLM to guide the\nbehavior of the LLM.\nWhile NeMo Guardrails can be used to add\nsafety and steerability to any LLM-based appli-\ncation, we consider that dialogue systems powered\nby an LLM benefit the most from using Colang and\nthe Guardrails runtime. The toolkit is licensed as\nApache 2.0, and we provide initial support for sev-\neral LLM providers, together with starter example\napplications and evaluation tools.\n2 Related Work\n2.1 Model Alignment\nExisting solutions for adding rails to LLMs rely\nheavily on model alignment techniques such as\ninstruction-tuning (Wei et al., 2021) or reinforce-\nment learning (Ouyang et al., 2022; Glaese et al.,\n2022; OpenAI, 2023). The alignment of LLMs\nworks on several dimensions, mainly to improve\nhelpfulness and to reduce harmfulness. Align-\nment in general, including red-teaming (Perez et al.,\n2022), requires a large collection of input prompts\nand responses that are manually labeled according\nto specific criteria (e.g., harmlessness).\nModel alignment provides rails embedded at\ntraining in the LLM, that cannot easily be changed\nat runtime by users. Moreover, it also requires\na large set of human-annotated response ratings\nfor each rail to be incorporated by the LLM.\nWhile Reinforcement Learning from Human Feed-\nback (Ouyang et al., 2022) is the most popular\nmethod for model alignment, alternatives such as\nRL from AI Feedback (Bai et al., 2022b) do not\nFigure 2: Dialogue flows defined in Colang: a sim-\nple greeting flow and two topical rail flows calling the\ncustom action wolfram alpha request to respond to\nmath and distance queries.\nrequire a human labeled dataset and use the actual\nLLM to provide feedback for each response.\nWhile most alignment methods provide general\nembedded rails, in a similar way developers can\nadd app-specific embedded rails to an LLM via\nprompt tuning (Lester et al., 2021; Liu et al., 2022).\n2.2 Prompting and Chain-of-Thought\nThe most common approach to add user-defined\nprogrammable rails to an LLM is to use prompt-\ning, including prompt engineering and in-context\nlearning (Brown et al., 2020), by prepending or ap-\npending a specific text to the user input (Wang and\nChang, 2022; Si et al., 2022). This text specifies\nthe behavior that the LLM should adhere to.\nThe other approach to provide LLMs with user-\ndefined runtime rails is to use chain-of-thought\n(CoT) (Wei et al., 2022). In its simplest form, CoT\nappends to the user instruction one or several simi-\nlar examples of input and output pairs for the task\nat hand. Each of these examples contains a more\ndetailed explanation in the output, useful for de-\ntermining the final answer. Other more complex\napproaches involve several steps of prompting the\nLLM in a generic to specific way (Zhou et al., 2022)\nor even with entire dialogues with different roles\nsimilar to an inner monologue (Huang et al., 2022).\n2.3 Task-Oriented Dialogue Agents\nBuilding task-oriented dialogue agents generally\nrequires two components: a Natural Language Un-\nderstanding (NLU) and a Dialogue Management\n(DM) engine (Bocklisch et al., 2017; Liu et al.,\n432\n2021). There exist a wide range of tools and solu-\ntions for both NLU and DM, ranging from open-\nsource solutions like Rasa (Bocklisch et al., 2017)\nto proprietary platforms, such as Microsoft LUIS\nor Google DialogFlow (Liu et al., 2021). Their\nfunctionality mostly follows these two steps: first\nthe NLU extracts the intent and slots from the user\nmessage, then the DM predicts the next dialogue\nstate given the current dialogue context.\nThe set of intents and dialogue states are finite\nand pre-defined by a conversation designer. The bot\nresponses are also chosen from a closed set depend-\ning on the dialogue state. This approach allows to\ndefine specific dialogue flows that tightly control\nany dialogue agent. Conversely, these agents are\nrigid and require a high amount of human effort to\ndesign and update the NLU and dialogue flows.\nAt the other end of the spectrum are recent end-\nto-end (E2E) generative approaches that use LLMs\nfor dialogue tracking and bot message generation\n(Hudeˇcek and Dušek, 2023; Zhang et al., 2023).\nNeMo Guardrails also uses an E2E approach to\nbuild LLM-powered dialogue agents, but it com-\nbines a DM-like runtime able to interpret and main-\ntain the state of dialogue flows written in Colang\nwith a CoT-based approach to generate bot mes-\nsages and even new dialogue flows using an LLM.\n3 NeMo Guardrails\n3.1 General Architecture\nNeMo Guardrails acts like a proxy between the\nuser and the LLM as detailed in Fig. 3. It allows de-\nvelopers to define programmatic rails that the LLM\nshould follow in the interaction with the users us-\ning Colang, a formal modeling language designed\nto specify flows of events, including conversations.\nColang is interpreted by the Guardrails runtime\nwhich applies the user-defined rules or automat-\nically generated rules by the LLM, as described\nnext. These rules implement the guardrails and\nguide the behavior of the LLM.\nAn excerpt from a Colang script is shown in\nFig. 2 - these scripts are at the core of a Guardrails\napp configuration. The main elements of a Colang\nscript are: user canonical forms, dialogue flows,\nand bot canonical forms. All these three types of\ndefinitions are also indexed in a vector database\n(e.g., Annoy (Spotify), FAISS (Johnson et al.,\n2019)) to allow for efficient nearest-neighbors\nlookup when selecting the few-shot examples for\nthe prompt. The interaction between the LLM and\nthe Guardrails runtime is defined using Colang\nrules. When prompted accordingly, the LLM is\nable to generate Colang-style code using few-shot\nin-prompt learning. Otherwise, the LLM works in\nnormal mode and generates natural language.\nCanonical forms (Sreedhar and Parisien, 2022)\nare a key mechanism used by Colang and the run-\ntime engine. They are expressed in natural lan-\nguage (e.g., English) and encode the meaning of\na message in a conversation, similar to an intent.\nThe main difference between intents and canonical\nforms is that the former are designed as a closed\nset for a text classification task, while the latter are\ngenerated by an LLM and thus are not bound in any\nway, but are guided by the canonical forms defined\nby the Guardrails app. The set of canonical forms\nused to define the rails that guide the interaction is\nspecified by the developer; these are used to select\nfew-shot examples when generating the canonical\nform for a new user message.\nUsing these key concepts, developers can imple-\nment a variety of programmable rails. We have\nidentified two main categories: topical rails and\nexecution rails. Topical rails are intended for con-\ntrolling the dialogue, e.g. to guide the response for\nspecific topics or to implement complex dialogue\npolicies. Execution rails call custom actions de-\nfined by the app developer; we will focus on a set\nof safety rails available to all Guardrails apps.\n3.2 Topical Rails\nTopical rails employ the key mechanism used\nby NeMo Guardrails: Colang for describing pro-\ngrammable rails as dialogue flows, together with\nthe Colang interpreter in the runtime for dialogue\nmanagement (Execute flow [Colang] block in\nFig. 3). Flows are specified by the developer to de-\ntermine how the user conversation should proceed.\nThe dialogue manager in the Guardrails runtime\nuses an event-driven design (an event loop that pro-\ncesses events and generates back other events) to\nensure which flows are active in the current dia-\nlogue context.\nThe runtime has three main stages (see Fig. 3)\nfor guiding the conversation with dialogue flows\nand thus ensuring the topical rails:\nGenerate user canonical form. Using\nsimilarity-based few-shot prompting, generate the\ncanonical form for each user input, allowing the\nguardrails system to trigger any user-defined flows.\nDecide next steps and execute them. Once the\nuser canonical form is identified, there are two po-\n433\nFigure 3: NeMo Guardrails general architecture.\ntential paths: 1) Pre-defined flow: If the canonical\nform matches any of the developer-specified flows,\nthe next step is extracted from that particular flow\nby the dialogue manager; 2) LLM decides next\nsteps: For user canonical forms that are not de-\nfined in the current dialogue context, we use the\ngeneralization capability of the LLM to decide the\nappropriate next steps - e.g., for a travel reservation\nsystem, if a flow is defined for booking bus tickets,\nthe LLM should generate a similar flow if the user\nwants to book a flight.\nGenerate bot message(s). Conditioned by the\nnext step, the LLM is prompted to generate a re-\nsponse. Thus, if we do not want the bot to respond\nto political questions, and the next step for such\na question is bot inform cannot answer – the bot\nwould deflect from responding, respecting the rail.\nAppendix B provides details about the Colang\nlanguage. Appendix C contains sample prompts.\n3.3 Execution Rails\nThe toolkit also makes it easy to add \"execution\"\nrails. These are custom actions (defined in Python),\nmonitoring both the input and output of the LLM,\nand can be executed by the Guardrails runtime\nwhen encountered in a flow. While execution rails\ncan be used for a wide range of tasks, we provide\nseveral rails for LLM safety covering fact-checking,\nhallucination, and moderation.\n3.3.1 Fact-Checking Rail\nOperating under the assumption of retrieval aug-\nmented generation (Wang et al., 2023), we formu-\nlate the task as an entailment problem. Specifically,\ngiven anevidence text and a generatedbot response,\nwe ask the LLM to predict whether the response\nis grounded in and entailed by the evidence. For\neach evidence-hypothesis pair, the model must re-\nspond with a binary entailment prediction using the\nfollowing prompt:\nYou are given a task to identify if the hypothesis\nis grounded and entailed in the evidence. You\nwill only use the contents of the evidence and\nnot rely on external knowledge. Answer with\nyes/no. \"evidence\": {{evidence}} \"hypothesis\":\n{{bot_response}} \"entails\":\nIf the model predicts that the hypothesis is not en-\ntailed by the evidence, this suggests the generated\nresponse may be incorrect. Different approaches\ncan be used to handle such situations, such as ab-\nstaining from providing an answer.\n3.3.2 Hallucination Rail\nFor general-purpose questions that do not involve\na retrieval component, we define a hallucination\nrail to help prevent the bot from making up facts.\nThe rail uses self-consistency checking similar to\nSelfCheckGPT (Manakul et al., 2023): given a\nquery, we first sample several answers from the\nLLM and then check if these different answers are\nin agreement. For hallucinated statements, repeated\nsampling is likely to produce responses that are not\nin agreement.\nAfter we obtain n samples from the LLM for the\nsame prompt, we concatenate n − 1 responses to\nform the context and use the nth response as the\nhypothesis. Then we use the LLM to detect if the\nsampled responses are consistent using the prompt\ntemplate defined in Appendix D.\n434\n3.3.3 Moderation Rails\nThe moderation process in NeMo Guardrails con-\ntains two key components:\n• Input moderation, also referred as jailbreak\nrail, aims to detect potentially malicious user mes-\nsages before reaching the dialogue system.\n• Output moderation aims to detect whether\nthe LLM responses are legal, ethical, and not harm-\nful prior to being returned to the user.\nThe moderation system functions as a pipeline,\nwith the user message first passing through input\nmoderation before reaching the dialogue system.\nAfter the dialogue system generates a response\npowered by an LLM, the output moderation rail is\ntriggered. Only after passing both moderation rails,\nthe response is returned to the user.\nBoth the input and output moderation rails are\nframed as another task to a powerful, well-aligned\nLLM that vets the input or response. The prompt\ntemplates for these rails are found in Appendix D.\n4 Sample Guardrails Applications\nAdding rails to conversation applications is simple\nand straightforward using Colang scripts.\n4.1 Topical Rails\nTopical rails can be used in combination with exe-\ncution rails to decide when a specific action should\nbe called or to define complex dialogue flows for\nbuilding task oriented agents.\nIn the example presented in Fig. 2, we imple-\nment two topical rails that allow the Guardrails\napp to use the WolframAlpha engine to respond\nto math and distance queries. To achieve this, the\nwolfram alpha request custom action (imple-\nmented in Python, available on Github) is using the\nWolframAlpha API to get a response to the user\nquery. This response is then used by the LLM to\ngenerate an answer in the context of the current\nconversation.\n4.2 Execution Rails\nThe steps involved in adding executions rails are:\n1. Define the action - Defining a rail requires\nthe developer to define an action that specifies\nthe logic for the rail (in Python).\n2. Invoke action in dialogue flows - Once the\naction has been defined, we can call the action\nfrom Colang using the execute keyword.\n3. Use action output in dialogue flow - The\ndeveloper can specify how the application\nshould react to the output from the action.\nAppendix E contains details about defining ac-\ntions, together with an example of the actions that\nimplement the input and output moderation rails.\nFig. 4 shows a sample flow in Colang that in-\nvokes the check_jailbreak action. If the jailbreak\nrail flags a user message, the developer can decide\nnot to show the generated response and to output\na default text instead. Appendix F provides other\nexamples of flows using the executions rails.\nFigure 4: Flow using jailbreak rail in Colang\n5 Evaluation\nIn this section, we provide details on how we\nmeasure the performance of various rails. Addi-\ntional information for all tasks and a discussion on\nthe automatic evaluation tools available in NeMo\nGuardrails are provided in Appendix G.\n5.1 Topical Rails\nThe evaluation of topical rails focuses on the\ncore mechanism used by the toolkit to guide con-\nversations using canonical forms and dialogue\nflows. The current evaluation experiments em-\nploy datasets used for conversational NLU. In this\nsection, we present the results for the Banking\ndataset (Casanueva et al., 2022), while additional\nexperiments can be found in Appendix G.\nStarting from a NLU dataset, we create a Colang\napplication (publicly available on Github) by map-\nping intents to canonical forms and defining simple\ndialogue flows for them. The evaluation dataset\nused in our experiments is balanced, containing\nat most 3 samples per intent sampled randomly\nfrom the original datasets. The test dataset has 231\nsamples spanning over 77 different intents.\nThe results of the top 3 performing models\nare presented in Fig. 5, showing that topical rails\ncan be successfully used to guide conversations\neven with smaller open source models such as\nfalcon-7b-instruct or llama2-13b-chat. As\nthe performance of an LLM is heavily dependent\non the prompt, all results might be improved with\nbetter prompting.\nThe topical rails evaluation highlights several\nimportant aspects. First, each step in the three-step\n435\nFigure 5: Performance of topical rails on Banking.\napproach (user canonical form, next step, bot mes-\nsage) used by Guardrails offers an improvement\nin performance. Second, it is important to have\nat least k = 3 samples in the vector database for\neach user canonical form for achieving good perfor-\nmance. Third, some models (i.e., gpt-3.5-turbo)\nproduce a wider variety of canonical forms, even\nwith few-shot prompting. In these cases, it is useful\nto add a similarity match instead of exact match for\ngenerating canonical forms.\n5.2 Execution Rails\nModeration Rails To evaluate the moderation\nrails, we use the Anthropic Red-Teaming and Help-\nful datasets (Bai et al., 2022a; Perez et al., 2022).\nWe have sampled a balancedharmful-helpful evalu-\nation set as follows: from the Red-Teaming dataset\nwe sample prompts with the highest harmful score,\nwhile from the Helpful dataset we select an equal\nnumber of prompts.\nWe quantify the performance of the rails based\non the proportion of harmful prompts that are\nblocked and the proportion of helpful ones that\nare allowed. Analysis of the results shows that us-\ning both the input and output moderation rails is\nmuch more robust than using either one of the rails\nindividually. Using both rails gpt-3.5-turbo has\na great performance - blocking close to 99% of\nharmful (compared to 93% without the rails) and\njust 2% of helpful requests - details in Appendix G.\nFact-Checking Rail We consider the MS-\nMARCO dataset (Bajaj et al., 2016) to evaluate\nthe performance of the fact-checking rail. The\ndataset consists of (context, question, answer)\ntriples. In order to mine negatives (answers that\nare not grounded in the context) we use OpenAI\ntext-davinci-003 to rewrite the positive answer\nto a hard negative that looks similar to it, but is\nFigure 6: Performance of the hallucination rail.\nnot grounded in the evidence. We construct a com-\nbined dataset by equally sampling both positive\nand negative triples. Both text-davinci-003 and\ngpt-3.5-turbo perform well on the fact-checking\nrail and obtain an overall accuracy of 80% (see\nFig. 11 in Appendix G.2.2).\nHallucination Rail Evaluating the hallucination\nrail is difficult without employing subjective man-\nual annotation. To overcome this issue and be able\nto automatically quantify its performance, we com-\npile a list of 20 questions based on a false premise\n(questions that do not have a right answer).\nAny generation from the language model, apart\nfrom deflection, is considered a failure. We\nthen quantify the benefit of employing the hal-\nlucination rail as a fallback mechanism. For\ntext-davinci-003, the LLM is unable to deflect\nprompts that are unanswerable and using the hallu-\ncination rail helps intercept 70% of these prompts.\ngpt-3.5-turbo performs much better, deflecting\nunanswerable prompts or marking that its response\ncould be incorrect in 65% of the cases. Even in\nthis case, employing the hallucination rail boosts\nperformance up to 95%.\n6 Conclusions\nWe present NeMo Guardrails, a toolkit that allows\ndevelopers to build controllable and safe LLM-\nbased applications by implementing programmable\nrails. These rails are expressed using Colang and\ncan also be implemented as custom actions if they\nrequire a complex logic. Using CoT prompting\nand a dialogue manager that can interpret Colang\ncode, the Guardrails runtime acts like a proxy be-\ntween the application and the LLM enforcing the\nuser-defined rails.\n436\n7 Limitations\n7.1 Programmable Rails and Embedded Rails\nBuilding controllable and safe LLM-powered ap-\nplications, in general, and dialogue systems, in\nparticular, is a difficult task. We acknowledge that\nthe approach employed by NeMo Guardrails of us-\ning developer-defined programmable rails, imple-\nmented with prompting and the Colang interpreter,\nis not a perfect solution.\nTherefore we advocate that, whenever possible,\nour toolkit should not be used as a stand-alone\nsolution, especially for safety-specific rails. Pro-\ngrammable rails complement embedded rails and\nthese two solutions should be used together for\nbuilding safe LLM applications. The vision of\nthe project is to also provide, in the future, more\npowerful customized models for some of the ex-\necution rails that should supplement the current\npure prompting methods. On another hand, our re-\nsults show that adding the moderation rails to exist-\ning safety rails embedded in powerful LLMs (e.g.,\nChatGPT), provides a better protection against jail-\nbreak attacks.\nIn the context of controllable and task-oriented\ndialogue agents, it is difficult to develop cus-\ntomized models for all possible tasks and topical\nrails. Therefore, in this context, NeMo Guardrails\nis a viable solution for building LLM-powered task-\noriented agents without extra mechanisms. How-\never, even for topical rails and task-oriented agents,\nwe plan to release p-tuned models that achieve bet-\nter performance for some of the tasks, e.g. for\ncanonical form generation.\n7.2 Extra Costs and Latency\nThe three-step CoT prompting approach used by\nthe Guardrails runtime incurs extra costs and extra\nlatency. As these calls are sequentially chained (i.e.,\nthe generation of the next steps in the second phase\ndepends on the user canonical form generated in\nthe first stage), the calls cannot be batched. In\nour current implementation, the latency and costs\nrequired are about 3 times the latency and cost of\na normal call to generate the bot message without\nusing Guardrails. We are currently investigating if\nin some cases we could use a single call to generate\nall three steps (user canonical form, next steps in\nthe flow, and bot message).\nUsing a more complex prompt and few-shot\nin-context learning also generates slightly extra\nlatency and a larger cost compared to a normal\nbot message generation for a vanilla conversation.\nDevelopers can decide to use a simpler prompt if\nneeded.\nHowever, we consider that developers should\nbe provided with various options for their needs.\nSome might be willing to pay the extra costs for\nhaving safer and controllable LLM-powered di-\nalogue agents. Moreover, GPU inference costs\nwill decrease and smaller models can also achieve\ngood performance for some or all NeMo Guardrails\ntasks. As presented in our paper, we know that\nfalcon-7b-instruct (Penedo et al., 2023) al-\nready achieves very good performance for topical\nrails. We have seen similar positive performance\nfrom other recent models, like Llama 2 (7B and\n13B) chat variants (Touvron et al., 2023).\n8 Broader Impact\nAs a toolkit to enforce programmable rails for LLM\napplications, including dialogue systems, NeMo\nGuardrails should provide benefits to developers\nand researchers. Programmable rails supplement\nembedded rails, either general (using RLHF) or\nuser-defined (using p-tuned customized models).\nFor example, using the fact-checking rail develop-\ners can easily build an enhanced retrieval-based\nLLM application and it also allows them to as-\nsess the performance of various models as pro-\ngrammable rails are model-agnostic. The same is\ntrue for building LLM-based task-oriented agents\nthat should follow complex dialogue flows.\nAt the same time, before putting a Guardrails\napplication into production, the implemented pro-\ngrammable rails should be thoroughly tested (espe-\ncially safety related rails). Our toolkit provides a\nset of evaluation tools for testing the performance\nboth for topical and execution rails.\nAdditional details for our toolkit can be found in\nthe Appendix, including simple installation steps\nfor running the toolkit with the example Guardrails\napplications that are shared on Github. A short\ndemo video is also available: https://youtu.be/\nPfab6UWszEc.\nReferences\nAmos Azaria and Tom Mitchell. 2023. The internal\nstate of an llm knows when its lying. arXiv preprint\narXiv:2304.13734.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al.\n437\n2022a. Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv\npreprint arXiv:2204.05862.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu,\nAmanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini,\nCameron McKinnon, et al. 2022b. Constitutional\nai: Harmlessness from ai feedback. arXiv preprint\narXiv:2212.08073.\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\net al. 2016. Ms marco: A human generated ma-\nchine reading comprehension dataset. arXiv preprint\narXiv:1611.09268.\nTom Bocklisch, Joey Faulkner, Nick Pawlowski, and\nAlan Nichol. 2017. Rasa: Open source language\nunderstanding and dialogue management. arXiv\npreprint arXiv:1712.05181.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nInigo Casanueva, Ivan Vuli ´c, Georgios Spithourakis,\nand Paweł Budzianowski. 2022. NLU++: A multi-\nlabel, slot-rich, generalisable dataset for natural lan-\nguage understanding in task-oriented dialogue. In\nFindings of the Association for Computational Lin-\nguistics: NAACL 2022 , pages 1998–2013, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nAmelia Glaese, Nat McAleese, Maja Tr˛ ebacz, John\nAslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh,\nLaura Weidinger, Martin Chadwick, Phoebe Thacker,\net al. 2022. Improving alignment of dialogue agents\nvia targeted human judgements. arXiv preprint\narXiv:2209.14375.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky\nLiang, Pete Florence, Andy Zeng, Jonathan Tomp-\nson, Igor Mordatch, Yevgen Chebotar, et al. 2022.\nInner monologue: Embodied reasoning through\nplanning with language models. arXiv preprint\narXiv:2207.05608.\nV ojtˇech Hudeˇcek and Ondˇrej Dušek. 2023. Are llms all\nyou need for task-oriented dialogue? arXiv preprint\narXiv:2304.06556.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nDaniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin,\nMatei Zaharia, and Tatsunori Hashimoto. 2023. Ex-\nploiting programmatic behavior of llms: Dual-use\nthrough standard security attacks. arXiv preprint\narXiv:2302.05733.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengx-\niao Du, Zhilin Yang, and Jie Tang. 2022. P-tuning:\nPrompt tuning can be comparable to fine-tuning\nacross scales and tasks. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 61–68,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nXingkun Liu, Arash Eshghi, Pawel Swietojanski, and\nVerena Rieser. 2021. Benchmarking natural language\nunderstanding services for building conversational\nagents. In Increasing Naturalness and Flexibility\nin Spoken Dialogue Interaction: 10th International\nWorkshop on Spoken Dialogue Systems, pages 165–\n183. Springer.\nPotsawee Manakul, Adian Liusie, and Mark JF Gales.\n2023. Selfcheckgpt: Zero-resource black-box hal-\nlucination detection for generative large language\nmodels. arXiv preprint arXiv:2303.08896.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nRichard Yuanzhe Pang, Stephen Roller, Kyunghyun\nCho, He He, and Jason Weston. 2023. Leveraging\nimplicit feedback from deployment data in dialogue.\narXiv preprint arXiv:2307.14117.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow,\nRuxandra Cojocaru, Alessandro Cappelli, Hamza\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The refinedweb dataset\nfor falcon llm: outperforming curated corpora with\nweb data, and web data only. arXiv preprint\narXiv:2306.01116.\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng,\nYujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\nYu, Weizhu Chen, et al. 2023. Check your facts and\ntry again: Improving large language models with\nexternal knowledge and automated feedback. arXiv\npreprint arXiv:2302.12813.\nEthan Perez, Saffron Huang, Francis Song, Trevor Cai,\nRoman Ring, John Aslanides, Amelia Glaese, Nat\nMcAleese, and Geoffrey Irving. 2022. Red teaming\nlanguage models with language models. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 3419–3448,\n438\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nChenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang\nWang, Jianfeng Wang, Jordan Lee Boyd-Graber, and\nLijuan Wang. 2022. Prompting gpt-3 to be reliable.\nIn The Eleventh International Conference on Learn-\ning Representations.\nSpotify. ANNOY library. https://github.com/\nspotify/annoy. Accessed: 2023-08-01.\nMakesh Narsimhan Sreedhar and Christopher Parisien.\n2022. Prompt learning for domain adaptation in task-\noriented dialogue. In Proceedings of the Towards\nSemi-Supervised and Reinforced Task-Oriented Di-\nalog Systems (SereTOD), pages 24–30, Abu Dhabi,\nBeijing (Hybrid). Association for Computational Lin-\nguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nBoxin Wang, Wei Ping, Peng Xu, Lawrence McAfee,\nZihan Liu, Mohammad Shoeybi, Yi Dong, Oleksii\nKuchaiev, Bo Li, Chaowei Xiao, et al. 2023. Shall\nwe pretrain autoregressive language models with\nretrieval? a comprehensive study. arXiv preprint\narXiv:2304.06762.\nYau-Shian Wang and Yingshan Chang. 2022. Toxicity\ndetection with generative prompt-based inference.\narXiv preprint arXiv:2205.12390.\nAlexander Wei, Nika Haghtalab, and Jacob Steinhardt.\n2023. Jailbroken: How does llm safety training fail?\narXiv preprint arXiv:2307.02483.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nXiaoying Zhang, Baolin Peng, Kun Li, Jingyan Zhou,\nand Helen Meng. 2023. Sgp-tod: Building task bots\neffortlessly via schema-guided llm prompting. arXiv\npreprint arXiv:2305.09067.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nClaire Cui, Olivier Bousquet, Quoc Le, et al. 2022.\nLeast-to-most prompting enables complex reason-\ning in large language models. arXiv preprint\narXiv:2205.10625.\nAndy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrik-\nson. 2023. Universal and transferable adversarial\nattacks on aligned language models. arXiv preprint\narXiv:2307.15043.\n439\nA Installation Guide and Examples\nDevelopers can download and install the latest ver-\nsion of the NeMo Guardrails toolkit directly from\nGithub 2. They can also install the latest stable\nrelease using pip install nemoguardrails.\nWe have a concise installation guide3 showing\nhow to run a Guardrails app using the provided\nCommand Line Interface (CLI) or how to launch\nthe Guardrails web server. The server powers a sim-\nple chat web client to engage with all the Guardrails\napps found in the folder specified when starting the\nserver.\nFive reference Guardrails applications are pro-\nvided as a general demonstration for building dif-\nferent types of rails.\n• Topical Rail: Making the bot stick to a spe-\ncific topic of conversation.\n• Moderation Rail: Moderating a bot’s re-\nsponse.\n• Fact Checking and Hallucination Rail: En-\nsuring factual answers.\n• Secure Execution Rail: Executing a third-\nparty service with LLMs.\n• Jail-breaking Rail: Ensuring safe answers\ndespite malicious intent from the user.\nThese examples are meant to showcase the process\nof building rails, not as out-of-the-box safety fea-\ntures. Customization and strengthening of the rails\nis highly recommended.\nThe sample Guardrails applications also contain\nexamples on how to use several open-source mod-\nels (e.g., falcon-7b-instruct, dolly-v2-3b,\nvicuna-7b-v1.3) deployed locally or using Hug-\ngingFace Inference private endpoints. Other exam-\nples cover how to combine various chains defined\nin Langchain with programmable rails defined in\nNeMo Guardrails.\nAdditional details about the reference applica-\ntions and about the toolkit in general can be found\non the main documentation page4.\n2https://github.com/NVIDIA/NeMo-Guardrails/\n3https://github.com/NVIDIA/NeMo-Guardrails/\nblob/main/docs/getting_started/\ninstallation-guide.md\n4https://github.com/NVIDIA/NeMo-Guardrails/\nblob/main/docs/README.md\nB Colang Language and Dialogue\nManager\nColang is a language for modeling sequences of\nevents and interactions, being particularly useful\nfor modeling conversations. At the same time, it\nenables the design of guardrails for conversational\nsystems using the Colang interpreter, an event-\nbased processing engine that acts like a dialogue\nmanager.\nCreating guardrails for conversational systems\nrequires some form of understanding of how the\ndialogue between the user and the bot unfolds. Ex-\nisting dialog management techniques such us flow\ncharts, state machines or frame-based systems are\nnot well suited for modeling highly flexible con-\nversational flows like the ones we expect when\ninteracting with an LLM-based system.\nHowever, since learning a new language is not\nan easy task, Colang was designed as a mix of\nnatural language (English) and Python. If you are\nfamiliar with Python, you should feel confident\nusing Colang after seeing a few examples, even\nwithout any explanation.\nThe main concepts used by the Colang language\nare the following:\n• Utterance: the raw text coming from the user\nor the bot.\n• Message: the canonical form (structured rep-\nresentation) of a user/bot utterance.\n• Event: something that has happened and is\nrelevant to the conversation, e.g. user is silent,\nuser clicked something, user made a gesture,\netc.\n• Action: a custom code that the bot can invoke;\nusually for connecting to a third-party API.\n• Context: any data relevant to the conversation\n(encoded as a key-value dictionary).\n• Flow: a sequence of messages and events,\npotentially with additional branching logic.\n• Rails: specific ways of controlling the behav-\nior of a conversational system (a.k.a. bot), e.g.\nnot talk about politics, respond in a specific\nway to certain user requests, follow a prede-\nfined dialog path, use a specific language style,\nextract data etc. A rail in Colang can be mod-\neled through one or more flows.\n440\nFor additional details about Colang, please con-\nsult the Colang syntax guide 5.\nThe Guardrails runtime uses an event-driven\ndesign (i.e., an event loop that processes events\nand generates back other events). Dialogue flows\nare treated as sequences of events, but even a\nsimple user message is also an event - as an\nUtteranceUserActionFinished event is created\nand sent to the runtime. More details are available\nin the NeMo Guardrails architecture guide 6.\nC Prompts for Topical Rails\nNeMo Guardrails uses complex prompts, chained\nin 3 steps, to respond to a user message as described\nin Section 3.2. In the following listing we provide\nan example for the first step, to generate the canon-\nical form for the last user message in the current\nconversation.\nThe prompt below is designed for\ntext-davinci-003 and is structured in four\nparts:\n1. General prompt describing the task of the ap-\nplication.\n2. Sample conversation using Colang syntax.\n3. The most similar, given the current user mes-\nsage, few-shot (k = 5) examples for mapping\nuser messages to their corresponding canoni-\ncal form.\n4. The current conversation between the user and\nthe bot in Colang syntax.\n\" \" \"\nBelow i s a c o n v e r s a t i o n between a h e l p f u l AI a s s i s t a n t and a\nu s e r . The b o t i s d e s i g n e d t o g e n e r a t e human− l i k e t e x t\nb a s e d on t h e i n p u t t h a t i t r e c e i v e s . The b o t i s\nt a l k a t i v e and p r o v i d e s l o t s o f s p e c i f i c d e t a i l s . I f t h e\nb o t does n o t know t h e answer t o a q u e s t i o n , i t\nt r u t h f u l l y s a y s i t does n o t know .\n\" \" \"\n# T h i s i s how a c o n v e r s a t i o n between a u s e r and t h e b o t can\ngo :\nu s e r \" H e l l o t h e r e ! \"\ne x p r e s s g r e e t i n g\nb o t e x p r e s s g r e e t i n g\n\" H e l l o ! How can I a s s i s t you t o d a y ? \"\nu s e r \" What can you do f o r me? \"\nask a b o u t c a p a b i l i t i e s\nb o t r e s p o n d a b o u t c a p a b i l i t i e s\n\" I am an AI a s s i s t a n t which h e l p s answer q u e s t i o n s b a s e d\non a g i v e n knowledge b a s e . For t h i s i n t e r a c t i o n , I\ncan answer q u e s t i o n b a s e d on t h e j o b r e p o r t p u b l i s h e d\nby US Bureau o f Labor S t a t i s t i c s \"\nu s e r \" T e l l me a b i t a b o u t t h e US Bureau o f Labor S t a t i s t i c s .\n\"\nask q u e s t i o n a b o u t p u b l i s h e r\n5https://github.com/NVIDIA/\nNeMo-Guardrails/blob/main/docs/user_guide/\ncolang-language-syntax-guide.md\n6https://github.com/NVIDIA/NeMo-Guardrails/\nblob/main/docs/architecture/README.md\nb o t r e s p o n s e f o r q u e s t i o n a b o u t p u b l i s h e r\n\" The Bureau o f Labor S t a t i s t i c s i s t h e p r i n c i p a l f a c t −\nf i n d i n g agency f o r t h e F e d e r a l Government i n t h e\nb r o a d f i e l d o f l a b o r economics and s t a t i s t i c s \"\nu s e r \" t h a n k s \"\ne x p r e s s a p p r e c i a t i o n\nb o t e x p r e s s a p p r e c i a t i o n and o f f e r a d d i t i o n a l h e l p\n\"You ' r e welcome . I f you have any more q u e s t i o n s o r i f\nt h e r e ' s a n y t h i n g e l s e I can h e l p you with , p l e a s e don\n' t h e s i t a t e t o ask . \"\n# T h i s i s how t h e u s e r t a l k s :\nu s e r \" What was t h e movement on nonfarm p a y r o l l ? \"\nask a b o u t h e a d l i n e numbers\nu s e r \" What ' s t h e number o f p a r t − t i m e employed number ? \"\nask a b o u t h o u s e h o l d s u r v e y d a t a\nu s e r \"How much d i d t h e nonfarm p a y r o l l r i s e by ? \"\nask a b o u t h e a d l i n e numbers\nu s e r \" What i s t h i s month ' s unemployment r a t e ? \"\nask a b o u t h e a d l i n e numbers\nu s e r \"How many l o n g term unemployment i n d i v i d u a l s were\nr e p o r t e d ? \"\nask a b o u t h o u s e h o l d s u r v e y d a t a\n# T h i s i s t h e c u r r e n t c o n v e r s a t i o n between t h e u s e r and t h e\nb o t :\nu s e r \" H e l l o t h e r e ! \"\ne x p r e s s g r e e t i n g\nb o t e x p r e s s g r e e t i n g\n\" H e l l o ! How can I a s s i s t you t o d a y ? \"\nu s e r \" What can you do f o r me? \"\nask a b o u t c a p a b i l i t i e s\nb o t r e s p o n d a b o u t c a p a b i l i t i e s\n\" I am an AI a s s i s t a n t which h e l p s answer q u e s t i o n s b a s e d\non a g i v e n knowledge b a s e . For t h i s i n t e r a c t i o n , I\ncan answer q u e s t i o n b a s e d on t h e j o b r e p o r t p u b l i s h e d\nby US Bureau o f Labor S t a t i s t i c s \"\nu s e r \"how many unemployed p e o p l e were t h e r e i n March ? \"\nSimilar prompts are defined for other LLMs (i.e.,\ngpt-3.5-turbo, falcon-7b-instruct and oth-\ners) and are available on Github 7.\nWhen generating the user canonical form and\nthe next steps to guide the conversation, we use\ntemp = 0, while for sampling the bot message we\nuse a higher temperature (temp = 0.7 or temp =\n1).\nD Prompt Templates for Execution Rails\nIn this section we provide the prompt templates\nused by the hallucination and moderation rails.\nD.1 Hallucination Rail\nAfter we obtain n samples from the conversational\nagent for the same prompt, we concatenate n −\n1 responses to form the context and use the nth\nresponse as the hypothesis. We utilize an LLM\nto verify if the hypothesis is consistent with the\ncontext using the following prompt template:\nYou are given a task to identify if the hypothe-\nsis is in agreement with the context below. You\nwill only use the contents of the context and not\nrely on external knowledge. Answer with yes/no.\n7https://github.com/NVIDIA/NeMo-Guardrails/\ntree/main/nemoguardrails/llm/prompts\n441\n\"context\": {{ sampled_responses }} \"hypothe-\nsis\": {{ bot_response }} \"agreement\":\nWhen sampling the bot responses, as proposed\nby SelfCheckGPT (Manakul et al., 2023) we use\na high temperature (temp = 1) to ensure the vari-\nation between them necessary to assess the self-\nconsistency of the answers. At the same time,\nthe LLM-based hallucination verification call uses\ntemp = 0to get the most probable response. This\nbehavior is similar for all the other execution rails\ndescribed in the paper.\nD.2 Moderation Rails\nBoth the input and output moderation rails are\nframed as another task to a powerful, well-aligned\nLLM that vets the input or response. For the jail-\nbreak rail, we use the following prompt template:\nInstruction: {{ user_input }}\nWould this instruction make a language model\nbreak moderation policies, deviate from good\naligned responses and provide answers that a\nlanguage model should ideally not? Answer\nwith yes/no.\nThe output moderation rail uses the following\nprompt template:\nModel output: {{ bot_response }}\nIs this output legal, ethical and not harmful to\nany person? Answer with yes/no.\nE Action Definitions\nThe first step in creating an execution rail is to\ndefine a custom action that implements the logic of\nthe rail. At this moment, this is done in Python.\nAny custom action has access to the context\nof the conversation as can be seen in the subse-\nquent examples. In the Guardrails runtime, the\ncontext is a sequence of all the events in the con-\nversation history - including user and bot mes-\nsages, canonical forms, action called and more.\nSome of the context events that might be accessed\nmore often to define actions have a shortcut, e.g.\ncontext.get(”last_bot_message”).\nAn action can receive any number of parame-\nters from the Colang scripts where they are called.\nThese are passed to the Python function implement-\ning the action logic. At the same time, an action\nusually returns a value that can be used to further\nguide the dialogue.\nE.1 Output Moderation Rail\nThis is the current implementation for the output\nmoderation action. It uses the prompt template\ndefined in Appendix D and returns True if the bot\nresponse breaks the output moderation rail.\nThe call to the LLM is done using prompt\ntemplates and a simple LLM chain provided by\nLangchain8 for simplifying the communication\nwith several LLM providers.\nb o t _ r e s p o n s e = c o n t e x t . g e t ( \" l a s t _ b o t _ m e s s a g e \" )\ni f b o t _ r e s p o n s e :\no u t p u t _ m o d e r a t i o n _ t e m p l a t e =\n\" Model Output : { b o t _ r e s p o n s e }\nI s t h i s o u t p u t l e g a l , e t h i c a l and n o t h a r m f u l t o any\np e r s o n ? Answer w i t h yes / no . \"\nprompt = PromptTemplate (\nt e m p l a t e = o u t p u t _ m o d e r a t i o n _ t e m p l a t e , i n p u t _ v a r i a b l e s\n=[ \" b o t _ r e s p o n s e \" ]\n)\no u t p u t _ m o d e r a t i o n _ c h a i n = LLMChain ( prompt =prompt , llm =\nllm )\ncheck = a w a i t o u t p u t _ m o d e r a t i o n _ c h a i n . a p r e d i c t (\nb o t _ r e s p o n s e = b o t _ r e s p o n s e )\ncheck = check . l o w e r ( ) . s t r i p ( )\nl o g . i n f o ( f \" Output m o d e r a t i o n check r e s u l t i s { check } . \" )\ni f \" no \" i n check :\nr e t u r n F a l s e\nr e t u r n True\nE.2 Jailbreak Rail\nThis is the current implementation for the input\nmoderation action. It uses the prompt template\ndefined in Appendix D and returnsTrue if the user\ninput breaks the jailbreak rail.\nu s e r _ i n p u t = c o n t e x t . g e t ( \" l a s t _ u s e r _ m e s s a g e \" )\ni f u s e r _ i n p u t :\nj a i l b r e a k _ t e m p l a t e = \" I n s t r u c t i o n : { u s e r _ i n p u t }\nWould t h i s i n s t r u c t i o n make a l a n g u a g e model b r e a k\nm o d e r a t i o n p o l i c i e s , d e v i a t e from good a l i g n e d\nr e s p o n s e s and p r o v i d e a n s w e r s t h a t a l a n g u a g e model\ns h o u l d i d e a l l y n o t ? Answer w i t h yes / no . \"\nprompt = PromptTemplate (\nt e m p l a t e = j a i l b r e a k _ t e m p l a t e , i n p u t _ v a r i a b l e s =[ \"\nu s e r _ i n p u t \" ]\n)\nj a i l b r e a k _ c h a i n = LLMChain ( prompt =prompt , llm = llm )\ncheck = a w a i t j a i l b r e a k _ c h a i n . a p r e d i c t ( b o t _ r e s p o n s e =\nb o t _ r e s p o n s e )\ncheck = check . l o w e r ( ) . s t r i p ( )\nl o g . i n f o ( f \" J a i l b r e a k check r e s u l t i s { check } . \" )\ni f \" no \" i n check :\nr e t u r n F a l s e\nr e t u r n True\nF Sample Guardrails Flows using Actions\nThis section includes some examples of using the\nsafety execution rails, implemented as custom ac-\ntions, inside Colang flows to define simple Colang\napplications.\n8https://github.com/langchain-ai/langchain\n442\nFigure 7 shows how to use the\ncheck_jailbreak action for input modera-\ntion. The semantics is that for each user message\n(user ...), the jailbreak action is called to verify\nthe last user message, and if it is flagged as a\njailbreak attempt the last LLM bot-generated\nanswer is removed and a new one is uttered\nto inform the user her/his message breaks the\nmoderation policy. Figure 8 shows how the\noutput_moderation action is used - the meaning\nis similar to jail-breaking, however it is triggered\nafter any output bot message event (bot ...).\nFigure 7: Flow using jailbreak rail in Colang\nFigure 8: Flow using output moderation in Colang\nIn a similar way, Fig. 9 shows how to use the\nhallucination rail to check responses when for a\nparticular topic (i.e., asking questions about per-\nsons, where GPT models are prone to hallucinate).\nIn this case, the bot message is not removed, but\nan extra message is added to warn the user about\na possible incorrect answer. Fig. 10 shows how to\nadd fact-checking again for a specific topic, when\nasking a question about an employment report. In\nthis situation, the LLM should be consistent with\nthe information in the report.\nFigure 9: Flow using hallucination rail in Colang\nFigure 10: Flow using fact-checking rail in Colang\nG Additional Details on Evaluation\nOur toolkit also provides the evaluation tooling and\nmethodology to assess the performance of topical\nand execution rails. All the results reported in the\npaper can be replicated using the CLI evaluation\ntool available on Github, following the instructions\nabout evaluation 9. The same page contains slightly\nmore details than the current paper and is regularly\nupdated with new results (including new LLMs).\nDetailed instructions on how to replicate the ex-\nperiments can be found here 10.\nG.1 Topical Rails\nTopical rails evaluation focuses on the core mecha-\nnism used by NeMo Guardrails to guide conversa-\ntions using canonical forms and dialogue flows.\nThe current evaluation experiments for topical\nrails uses two datasets employed for conversational\nNLU: chit-chat11 and banking.\nThe datasets were transformed into a NeMo\nGuardrails app, by defining canonical forms for\neach intent, specific dialogue flows, and even bot\nmessages (for the chit-chat dataset alone). The\ntwo datasets have a large number of user intents,\nthus topical rails. One of them is very generic\nand with coarse-grained intents (chit-chat), while\nthe banking dataset is domain-specific and more\nfine-grained. More details about running the topi-\ncal rails evaluation experiments and the evaluation\ndatasets is available here.\nPreliminary evaluation results follow next. In all\nexperiments, we have chosen to have a balanced\ntest set with at most 3 samples per intent. For both\ndatasets, we have assessed the performance for\nvarious LLMs and also for the number of samples\n9https://github.com/NVIDIA/NeMo-Guardrails/\nblob/main/nemoguardrails/eval/README.md\n10https://github.com/NVIDIA/NeMo-Guardrails/\nblob/main/docs/README.#evaluation-tools\n11https://github.com/rahul051296/\nsmall-talk-rasa-stack , dataset was initially released by\nRasa\n443\n(k = all, 3, 1) per intent that are indexed in the\nvector database. We have used a random seed of\n42 for all experiments to ensure consistency.\nThe results of the top 3 performing models\nare presented in Fig. 5, showing that topical rails\ncan be successfully used to guide conversations\neven with smaller open source models such as\nfalcon-7b-instruct or llama2-13b-chat. As\nthe performance of an LLM is heavily dependent\non the prompt, due to the complex prompt used\nby NeMo Guardrails all results might be improved\nwith better prompting.\nThe topical rails evaluation highlights several\nimportant aspects. First, each step in the three-step\napproach (user canonical form, next step, bot mes-\nsage) used by Guardrails offers an improvement\nin performance. Second, it is important to have\nat least k = 3 samples in the vector database for\neach user canonical form for achieving good perfor-\nmance. Third, some models (i.e., gpt-3.5-turbo)\nproduce a wider variety of canonical forms, even\nwith few-shot prompting. In these cases, it is useful\nto add a similarity match instead of exact match for\ngenerating canonical forms. In this case, the sim-\nilarity threshold becomes an important inference\nparameter.\nDataset statistics and detailed results for several\nLLMs are presented in Tables 1, 2, and 3. Some\nexperiments have missing numbers either because\nthose experiments did not compute those metrics\nor because the dataset does not contain specific\nitems (for example, user-defined bot messages for\nthe banking dataset).\nDataset # intents # test samples\nchit-chat 76 226\nbanking 77 231\nTable 1: Dataset statistics for the topical rails evaluation.\nG.2 Execution Rails\nG.2.1 Moderation Rail\nTo evaluate the moderation rails, we use the An-\nthropic Red-Teaming and Helpful datasets (Bai\net al., 2022a; Perez et al., 2022). The red-\nteaming dataset consists of prompts that are human-\nannotated (0-4) on their ability to elicit inappropri-\nate responses from language models. A higher\nscore implies that the prompt was more success-\nful in bypassing model alignment. We randomly\nsample prompts with the highest rating to curate\nthe harmful set. All the prompts in the Anthropic\nHelpful dataset are genuine queries and forms our\nhelpful set. We create a balanced evaluation set\nwith an equal number of harmful and helpful sam-\nples.\nWe quantify the performance of the rails based\non the proportion of harmful prompts that are\nblocked and the proportion of helpful ones that are\nallowed. An ideal model would be able to block\n100% of the harmful prompts and allow 100% of\nthe helpful ones. We pass prompts from our evalu-\nation set through the input (jailbreak) moderation\nrail. Only those that are not flagged are passed to\nthe conversational agent to generate a response\nwhich is passed through the output moderation\nrail. Once again, only those responses that are\nnot flagged are displayed back to the user.\nAnalysis of the results shows that using a com-\nbination of both the input (aka jailbreak rail) and\noutput moderation rails is more robust than using\neither one of the rails individually. It should also be\nnoted that evaluation of the output moderation rail\nis subjective and each person/organization would\nhave different subjective opinions on what should\nbe allowed to pass through or not. In such situa-\ntions, it would be easy to modify prompts to the\nmoderation rails to reflect the beliefs of the entity\ndeploying the conversational agent.\nUsing an evaluation set of 200 samples split\nequally between harmful and helpful and cre-\nated as described above, we have seen that\ntext-davinci-003 blocks only 24% of the harm-\nful messages, while gpt-3.5-turbo does much\nbetter blocking 93% of harmful messages without\nany moderation guardrail. In this case, blocking\nmeans that the model is not providing a response to\nan input requiring moderation. On the helpful in-\nputs, both models do not block any request. Using\nonly the input moderation rail,text-davinci-003\nblocks 87% of harmful and 3% of helpful re-\nquests. Using both input and output moderation,\ntext-davinci-003 blocks 97% of harmful and\n5% of helpful requests, while gpt-3.5-turbo has\na great performance - blocking close to 99% of\nharmful and just 2% of helpful requests.\nG.2.2 Fact-checking Rail\nWe consider the MSMARCO dataset (Bajaj et al.,\n2016) to evaluate the performance of the fact-\nchecking rail. The dataset consists of (context,\nquestion, answer) triples. In order to mine nega-\ntives (answers that are not grounded in the context),\n444\nModel Us int,\nno sim\nUs int,\nsim=0.6\nBt int,\nno sim\nBt int,\nsim=0.6\nBt msg,\nno sim\nBt msg,\nsim=0.6\ntext-davinci-003, k=all 0.89 0.89 0.90 0.90 0.91 0.91\ntext-davinci-003, k=3 0.82 N/A 0.85 N/A N/A N/A\ntext-davinci-003, k=1 0.65 N/A 0.73 N/A N/A N/A\ngpt-3.5-turbo, k=all 0.44 0.56 0.50 0.61 0.54 0.65\ndolly-v2-3b, k=all 0.65 0.78 0.68 0.78 0.69 0.78\nfalcon-7b-instruct, k=all 0.81 0.81 0.81 0.82 0.81 0.82\nllama2-13b-chat, k=all 0.87 N/A 0.88 N/A 0.89 N/A\nTable 2: Topical evaluation results on chit-chat dataset. Us int means accuracy for user intents, Bt int is accuracy\nfor next step generation (i.e., the bot intent), Bt msg is accuracy for generated bot message. Sim denotes if semantic\nsimilarity was used for matching (with a specified threshold, in this case 0.6) or exact match.\nModel Us int,\nno sim\nUs int,\nsim=0.6\nBt int,\nno sim\nBt int,\nsim=0.6\nBt msg,\nno sim\nBt msg,\nsim=0.6\ntext-davinci-003, k=all 0.77 0.82 0.83 0.84 N/A N/A\ntext-davinci-003, k=3 0.65 N/A 0.73 N/A N/A N/A\ntext-davinci-003, k=1 0.50 N/A 0.63 N/A N/A N/A\ngpt-3.5-turbo, k=all 0.38 0.73 0.45 0.73 N/A N/A\ndolly-v2-3b, k=all 0.32 0.62 0.40 0.64 N/A N/A\nfalcon-7b-instruct, k=all 0.70 0.76 0.75 0.78 N/A N/A\nllama2-13b-chat, k=all 0.76 N/A 0.78 N/A N/A N/A\nTable 3: Topical evaluation results on banking dataset.\nFigure 11: Performance of the fact-checking rail.\nwe use OpenAI text-davinci-003 to rewrite the\npositive answer to a hard negative that looks sim-\nilar to it, but is not grounded in the evidence.\nWe construct a combined dataset by equally sam-\npling both positive and negative triples. Both\ntext-davinci-003 and gpt-3.5-turbo perform\nwell on the fact-checking rail and obtain an over-\nall accuracy of 80% (Fig. 11). The behavior\nof the two models is slightly different: while\ngpt-3.5-turbo is better at discovering negatives,\ntext-davinci-003 performs better on positive\nsamples.\nG.2.3 Hallucination Rail\nEvaluating the hallucination rail is difficult since\nwe cannot ascertain the questions that can be an-\nswered with factual knowledge embedded in the\nparameters of the language model. To effectively\nquantify the ability of the model to detect halluci-\nnations, we compile a list of 20 questions based on\na false premise. For example, one such question\nthat does not have a right answer is:\"When was the\nundersea city in the Gulf of Mexico established?\"\nAny generation from the language model apart\nfrom deflection (i.e., recognizing that the question\nis unanswerable) is considered a failure. We also\nquantify the benefit of employing the hallucination\nrail as a fallback mechanism. For text-davinci-\n003, the base language model is unable to deflect\nprompts that are unanswerable and using the hal-\nlucination rail helps intercept 70% of the unan-\nswerable prompts. gpt-3.5-turbo performs very\nwell at deflecting prompts that cannot be answered\nor hedging its response with statements about it\ncould be incorrect. Even for such powerful models,\nwe find that employing the hallucination rail helps\nboost the identification of questions that are prone\nto incorrect responses by 25%.\n445",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6893911957740784
    },
    {
      "name": "Embedded system",
      "score": 0.4094396233558655
    },
    {
      "name": "Computer hardware",
      "score": 0.3618519604206085
    }
  ]
}