{
    "title": "ViT-FRD: A Vision Transformer Model for Cardiac MRI Image Segmentation Based on Feature Recombination Distillation",
    "url": "https://openalex.org/W4385626881",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2105832261",
            "name": "Chunyu Fan",
            "affiliations": [
                "Liaoning Provincial People's Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A1960108382",
            "name": "Qi Su",
            "affiliations": [
                "Liaoning Provincial People's Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A1991537926",
            "name": "Zhifeng Xiao",
            "affiliations": [
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A2101046302",
            "name": "Hao Su",
            "affiliations": [
                "Liaoning Provincial People's Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2411829346",
            "name": "Ai-jie Hou",
            "affiliations": [
                "Liaoning Provincial People's Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A1971165745",
            "name": "Luan Bo",
            "affiliations": [
                "Liaoning Provincial People's Hospital"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3111930466",
        "https://openalex.org/W3136021864",
        "https://openalex.org/W3104386278",
        "https://openalex.org/W1941358101",
        "https://openalex.org/W1526103685",
        "https://openalex.org/W3023537913",
        "https://openalex.org/W3112385165",
        "https://openalex.org/W3009563704",
        "https://openalex.org/W6637373629",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2963446712",
        "https://openalex.org/W6762718338",
        "https://openalex.org/W2531409750",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2213612645",
        "https://openalex.org/W2335999708",
        "https://openalex.org/W6640212811",
        "https://openalex.org/W6779163297",
        "https://openalex.org/W2291593693",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W6714138976",
        "https://openalex.org/W2963881378",
        "https://openalex.org/W2963797156",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3033529678"
    ],
    "abstract": "Cardiac magnetic resonance imaging analysis has been a useful tool in screening patients for heart disease. Early, timely and accurate diagnosis of diseases of the heart series is the key to effective treatment. MRI provides important material for the diagnosis of cardiac diseases. The rise of deep learning has transformed computer-aided diagnostic systems, especially in the field of medical imaging. Existing work on cardiac structure segmentation models based on MRI imaging mainly relies on convolutional neural networks (CNNs), which lack model diversity and limit the prediction performance. This paper introduces Visual Transformer with Feature Recombination and Feature Distillation(ViT-FRD), a novel learning pipeline that combines a visual transformer (ViT) and a CNN through knowledge refinement. The training procedure allows the student model, i.e., ViT, to learn from the teacher model, i.e., CNN, by optimizing distillation losses. Meanwhile, ViT-FRD provides two performance boosters to increase the efficacy and efficiency of training. The proposed method is validated on two cardiac MRI image datasets. The findings demonstrate that ViT-FRD achieves SOTA and outperforms the widely used baseline model.",
    "full_text": "VOLUME XX, 2017 1\nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1109/ACCESS.2017.Doi Number\nViT-FRD: A Vision Transformer Model for\nCardiac MRI Image Segmentation Based on\nFeature Recombination Distillation\nChunyu Fan1, Qi Su1, Zhifeng Xiao2, Hao Su1, Aijie Hou1, and Bo Luan1\n1DepartmentofCardiovascularMedicine,ThePeople's HospitalOfLiaoningProvince,Shengyang110067China\n2SchoolofEngineering,PennStateErie,TheBehrendCollege,Erie,16563,PA,USA\nCorrespondingauthor:AijieHou(1758624242@qq.com)andBoLuan(luanbo369@hotmail.com)\nABSTRACT Cardiac magnetic resonance imaging analysis has been auseful toolin screening patients for\nheart disease. Early, timely and accurate diagnosis of diseases of the heart series is the key to effective\ntreatment. MRI provides important material for the diagnosis of cardiac diseases. The rise of deep learning\nhas transformed computer-aided diagnostic systems, especially in the field of medical imaging. Existing\nwork on cardiac structure segmentation models based on MRI imaging mainly relies on convolutional\nneural networks (CNNs), which lack model diversity and limit the prediction performance. This paper\nintroduces Visual Transformer with Feature Recombination and Feature Distillation(ViT-FRD), a novel\nlearning pipeline that combines a visual transformer (ViT) and aCNN through knowledge refinement. The\ntraining procedure allows the student model, i.e., ViT, to learn from the teacher model, i.e., CNN, by\noptimizing distillation losses. Meanwhile, ViT-FRD provides two performance boosters to increase the\nefficacy and efficiency of training. The proposed method is validated on two cardiac MRI image datasets.\nThefindingsdemonstratethatViT-FRDachievesSOTAandoutperformsthewidelyusedbaselinemodel.\nINDEX TERMS ViT,FeatureRecombination,FeatureDistillation,Heartsegmentation,MRI,CNN\nI. INTRODUCTION\nCardiovascular disease (CVD) is the underlying factors for\n8.9 million female deaths and 9.6 million male deaths in\n2019, accounting for approximately one third of all deaths\nglobally [1]. Sometimes, even experienced radiologists may\nhave different opinions. To this end, a computer-aided\ndiagnosis (CAD) system can assist physicians and\nradiologists in the detection of heart disease via the\ncomputationalandpredictivepowerofdeeplearning.\nEarly,timely,and accuratediagnosisof heartdiseaseis\nof utmost importance for effective treatment planning,\ndisease progression monitoring, lifestyle modifications, and\nprevention of complications. Timely diagnosis enables\nhealthcare professionals to develop appropriate treatment\nplans and closely monitor the progression of the disease. It\nalso allows for the implementation of lifestyle modifications\nand preventivemeasures to reduce the risk of complications.\nIn thecontextof cardiacfoci,semanticsegmentation plays a\ncritical role by accurately localizing and delineating specific\nregions within the heart. This precise identification of\nabnormalities enables targeted interventions and quantitative\nassessment,enhancingtheoveralldiagnosisandmanagement\nofheartdisease[2,3].\nExisting cardiac MRI analysis methods face several\nlimitations and challenges. One major limitation is the\nlengthy acquisition time required for cardiac MRI scans,\nwhich not only causes patient discomfort but also limits the\nnumber of patients that can be examined. Furthermore,\ncardiac MRI images are often affected by motion artifacts\nand variability due to factors like cardiac and respiratory\nmotion, as well as magnetic field inhomogeneity. These\nchallenges make accurate image analysis and interpretation\ndifficult. Additionally, the complex anatomy and function of\nthe heart present another hurdle, requiring sophisticated\nalgorithms to segment structures, track motion, and quantify\nfunction. However, the development and adoption of such\nalgorithms are hampered by the need for expert knowledge\nand manual intervention. Lastly, there is a need for better\nintegration of cardiac MRI analysis methods with clinical\ndecision-making, ensuring standardized protocols and\nautomatedanalysistechniquesthatprovideclinicallyrelevant\nmeasurements for effective patient management. Addressing\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3302522\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\n1 VOLUME XX, 2017\nthese limitations is crucial for enhancing the efficiency,\naccuracy,andclinicalutilityofcardiacMRIanalysis[4,5].\nThe need for improved model diversity and prediction\nperformance in cardiac structure segmentation is critical.\nAccurate segmentation of cardiac structures is essential for\nvarious clinical applications, including diagnosis and\ntreatment planning for cardiovascular diseases. However,\nexisting models often struggle to accurately delineate\ncomplex cardiac structures due to limited training datasets\nthat fail to represent anatomical variations and pathologies\nencountered in real-world scenarios. Incorporating a more\ndiverse and representative dataset during model training can\nenhance the model's ability to handle anatomical\ncomplexities and improve its overall performance.\nFurthermore,improvingpredictionperformanceiscrucialfor\nreliable and timely clinical information. Advancements in\ndata acquisition and algorithm development, including\ncomprehensivedatasetsandnoveldeeplearningarchitectures,\ncan enhance the models' predictive capabilities, leading to\nmore accurate and efficient cardiac structure\nsegmentation[6,7].\nRecent advances have witnessed the prosperity of deep\nlearning in almost all industries [5]. A wide spectrum of\nmodels,developmentframeworks,applications,theories,and\nlearning paradigms have emerged and gained worldwide\nattention. Driven by data explosion and hardware\nacceleration techniques, deep neural network (DNN) models\ncanbe trainedand validatedwith a unprecedented speed and\nscale,leadingtoaprofoundrevolutionineveryindustry,and\nhealth careis one of the domains that is significantly benefit\nfrom this transform. Traditional CAD systems are gradually\nreplaced by DNN-based CAD systems, which are usually\ntrainedwithmassiveannotateddataanddemonstratesuperior\npredictiveperformance.DNN models havegreatly improved\nthedetection accuracy for medical imaging-based diagnoses,\nincludes computerized tomography (CT) scans, ultrasound,\nMRI,andmagneticresonanceimaging(MRI).\nHeart identification using MRI pictures has been\ninvestigated using a variety of DNN-based techniques [8, 9,\n10, 11, 12]. Our investigation shows that most of the prior\nefforts utilize existing or custom convolutional neural\nnetworks (CNNs) to build heartsegmentation models. These\nmodels, despite the design differences, share a core module,\nnamely, the convolutional layer that allows the model to\nextract features and share parameters to reduce model size.\nWell-knownmodelsthathavebeenexaminedincludeVGG,\nResNet,DenseNet,EfficientNet[13],GoogleNet[14],andso\non. Besides, several performance boosting techniques have\nbeen investigated, including data augmentation [15, 16],\nattention[17], and ensemble learning [9], which have been\ncommonly seen in numerous DNN-based computer vision\nproblems. Since features are extracted via convolutional\nlayers, these CNN-based models are limited by the\nhomogeneous internal design. It is thus necessary to explore\na model with a different design principle to examine how\nwell models other than CNNs can tackle the heart\nsegmentation task.Transformer[18] and itsvariants[19,20]\nare considered as promising candidates. Transformer is\nfeatured by the self-attention module, which is the core\nstructure integrated into an encoder-decoder neural\narchitecture. Transformer has demonstrated superior\nperformance in numerous learning tasks, and its variant\ndeveloped regarding computer vision tasks, namely, Vision\nTransformer (ViT) [21] has also been extensively utilized to\ncompete against CNN models. The validated success of ViT\ndrivesustoexploreitsusageinheartsegmentation.\nVisual Transformers (ViT) are a recent development in\ndeep learning that revolutionizes image processing tasks.\nUnlike Convolutional NeuralNetworks (CNNs),which have\nbeen the dominant architecture for computer vision, ViT\nappliesthetransformer modeloriginally designed for natural\nlanguage processing (NLP) to images. ViT breaks down an\nimageintosmallerpatches,treatingthemastokenssimilarto\nwords in NLP. These patches are then fed into the\ntransformer,whichleveragesitsself-attentionmechanismsto\ncapture global relationships and dependencies across the\nentire image. This allows ViT to understand the context and\ninteractions between different image regions, enabling it to\nlearn complex patterns and make accurate predictions. By\nutilizing the transformer's ability to capture long-range\ndependencies, ViT surpasses the local receptive fields of\nCNNs,resultinginimproved performanceonvariousimage-\nrelated tasks such as object detection, image classification,\nand image segmentation. The application of transformers to\nimages has opened up exciting possibilities in computer\nvisionandhasledtosignificantadvancementsinthefield.\nSinceViT and CNNrepresenttwo DNN design flavors,it\nis desired to keep the merits of both to increase the model\ndiversity. Knowledge distillation (KD)[22] is originally\ndesignedformodelcompression.KDinvolvestwomodels,a\nteacher and a student model. The vanilla KD first trains a\ncomplex teacher modeland then distills knowledge from the\nteacher model into a much simpler student model. We argue\nthat the knowledge transfer within KD can not only be used\nfor model compression, but also serves as a form of\nknowledge fusion between models. Knowledge, or learned\npatterns, can flow between the student and teacher models;\nmeanwhile, the student model can learn patterns by itself\nduring training. The final student can capture patterns from\nitself and theteacher.This finding motivates us toapplyKD\nto heart segmentation in order for the student (ViT) to\nappreciatetheinformationsharedbytheteacher(CNN).\nAnother observation is the performance gap between\ntraining and test scores, causing overfitting[23]. The\nphenomenon also presents in heart segmentation. The root\ncause of overfitting is the discrepancy of data distribution\nbetween training and test data. Traditional methods to\naddress this issue include 1) adding more data, 2)\nregularization,3)reducingtheDNN’scapacity,and4)useof\ndropout layers, etc. In this research article, we design a\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3302522\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\n1 VOLUME XX, 2017\nmethod called Cascading Cross-layer Reformer\n(CCReformer) to transform an image in the test set to an\nimage that looks more similar to the training data, reducing\nthe difference between test and training data. The following\nis what this paper contributes. We propose a ViT-based\nlearning pipeline, named Knowledge-distilled and data-\nreformed ViT (ViT-FRD) for heart segmentation. ViT-FRD\nconsists of three core modules, including 1) CCReformer\nused to reform test data during inference, 2) Linformer that\nallows training a ViT model with linear time and space\nefficiency, and 3) KD that distills knowledge from a CNN\nmodel into theViT model.Toour knowledge,this structural\ndesign has not been found in the literature. The suggested\napproach has shown greater performance in comparison to a\nnumber of traditional CNN-based models and the winning\nentries in the competition after being verified on the Multi-\nModality Whole Heart Segmentation Competition 2017\n(MM-WHS)dataset. The results suggest the effectiveness of\nthe proposed method and that all three modules of the\nframeworkcanboostthedetectionperformance.\nThe remainder of this research articleis organized as\nfollows. Section 2 provides a breakdown of each\nincorporatedcomponentaswellastheoverallstructureofthe\nsuggestedstrategy.Section3reportstheexperimentaldetails,\nbaselines,andresults.Section4isapaper'sconclusionwitha\ndiscussionofthelimitationsandfuturedirections.\nII. MATERIALS AND METHODS\nThedatasetutilizedinthestudyisdescribedinthispart,\nalongwithspecificsontheVisualTransformerwithFeature\nRecombinationandFeatureDistillation(ViT-FRD)model\nthatissuggested.\nA. DATASET\nThestudydescribedinthisexcerptfocusesonevaluatingthe\nperformance of a whole-heart segmentation algorithm in the\ncontextofcardiacimaging.Thevalidationofthealgorithmis\nperformed using two datasets: the EchoNet-Dynamic dataset\nand the 2017 Multimodal Whole Heart Segmentation\nCompetition(MM-WHS)dataset.\nTheMM-WHS2017dataset[21]contains120multimodal\ncardiac images obtained in a real clinical setting and serves\nas the primary dataset for validating our model. The dataset\ncomprises 20 labeled and 40 unlabeled CT volumes, as well\nas 20 labeled and 40 unlabeled MR volumes. Our focus in\nthis study is specifically on MR images. The dataset is\ndesigned to evaluate various whole-heart segmentation\nalgorithms and explore different topics related to cardiac\nimagesegmentation,registration,andmodeling.Asshownin\nFigure1.\nThe EchoNet-Dynamic database [22] is a large video\ndataset of echocardiograms created for computer vision\nresearch in the field of cardiovascular imaging.It consists of\n10,030 labeled echocardiogram videos with accompanying\nexpert annotations, including measurements, tracings, and\ncalculations. This dataset aims to establish a baseline for\nstudying cardiac motion and chamber sizes using machine\nlearning techniques. It encompasses a wide range of typical\nimaging conditions encountered in echocardiography\nlaboratories.AsshowninFigure2.\nOverall, the use of these two datasets allows for a\ncomprehensive evaluation of the whole-heart segmentation\nalgorithm and its performance in different imaging\nmodalitiesandclinicalscenarios.\nFIGURE 1. Sample heart MRI images from the MM-WHS 2017 dataset.2.2.\nViT-FRD System Overview\nFIGURE 2. Sample cardiac ultrasound images from the EchoNet-\nDynamic dataset\nTo provide sufficient justification for the subsequent\nexperimental section, this paper presents the partitioning\nrulesandresultsoftwodatasetsasshowninTable1.\nThe partitioning of the MM-WHS 2017 dataset is not\nbased on group-level division, but rather involves sampling\nwithin different groups. On the other hand, for ECHONET-\nDYNAMIC, which comprises numerous independent\ndynamic videos organized into groups, no sampling is\nrequired,andthedatasetcanbedirectlypartitionedbygroup.\nThe specific categories for partitioning ECHONET-\nDYNAMICaredetailedinTable2.\nTABLEI DATASETDIVISIONRULESANDNUMBERS.\nDatasets Training Validation Testset Division\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3302522\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\n1 VOLUME XX, 2017\nset set Methodology\nMM-WHS\n2017\n80\ngroups 10groups 10\ngroups\nStratifiedrandom\nsamplingbyimage\ntype\nECHONET-\nDYNAMIC\n8000\ngroups\n1000\ngroups\n1030\ngroups\nStratifiedrandom\nsamplingaccording\ntoEFvalues\nTABLEII ECHONET-DYNAMICDIVISIONDETAILS.\nDatasets Normal(EF＞=50%) Abnormal(EF<50%)\nTrainingset 6400groups 1600groups\nValidationset 800groups 200groups\nTestset 824groups 206groups\nFigure 3 shows the proposed ViT-FRD framework for heart\nsegmentation. The framework contains threemajor modules,\nincluding CCReformer, Linformer, and the KD. The\nCCReformer is educated through self-supervision using only\nthetrainingset.Duringinference,atestimagepassesthrough\nthe CCReformer, which transforms the input to an image\nmore comparable to the ones in the drill set. After that, the\ntransformed image is split into multiple patches that are fed\ninto a Linformer module, which is a ViT model with linear\nefficiency. Meanwhile, the ViT model acts as a teacher\nmodelthathasbeenpreparedtocomplementastudentmodel.\nTheKDprocessallowsknowledgetobetransferredfromthe\nteacher to the student model by minimizing a loss function.\nTherefore, the student model can not only learn from the\ntraining data but also from theteacher, which has a different\nneuralarchitectureandcouldcapturepatternsthatthestudent\nmaynotsee.\nFIGURE3. TheproposedViT-FRDlearning framework.TheCCReformeris\ntrainedusingthetrainingdataonlyandusedduring inferencetotransformtest\ndata;theLinformerisaprocesstoreducethetimecomplexityofViT;theKD\nstrategyallowstheViTstudentmodeltolearnfromaCNN-basedteachermodel\nforbettersegmentationeffect.\nB. TRANSFORMER AND VIT\nThe Transformer[18] is a neural architecture that uses\nattention mechanisms to process sequential data. It has\nseveral advantages over traditional recurrent models such as\nLSTM[31] and GRU[25], including the ability to be\nparallelized. The Transformer has been used for a variety of\nNLP applications since it was first developed for machine\ntranslation inNLP.Itssuccessin thesetasks hasmadeitone\nofthemostsignificantdevelopmentsinAIinrecentyears.\nTransformer uses a structure of encoders and decoders, in\nwhich the decoder module is made up of a stack of\nTransformerdecodersandtheencodermoduleismadeupof\na stack of Transformer decoders. For the purpose of\nrecordingthesemanticrelationshipsbetweentheinputtokens,\na self-attention layer with numerous attention heads is\npresent in each Transformer encoder. From the outputs of\nthese attention heads, a set of embeddings is produced using\nthe feed-forward layer, which are then fed into the next\nencoder.The decoder, on the other hand, has three layers: a\nfeed-forwardlayer,anencoder-decoderattentionlayer,anda\nmulti-headself-attentionlayer.Ateachtimestep,thedecoder\ntakes the embeddings from the previous step as input and\nproducesapredictionresultthroughlayersthatarelinearand\nsoftmax.\nResearchers are investigating Transformer's potential in\ncomputer vision as a result of its performance in NLP tasks.\nOne example is the Vision Transformer (ViT), which\nmodifies the original Transformer architecture to process\nimage data. The transformation of an image into image\npatches, where N is the number of patches and D is the size\nofthepatchembedding,resultsina2DtensorofsizeNxD.\nApositionencodingvectorisaddedtoeachpatchembedding\nto maintain the relative position relationships between\npatches,andToencodeinformationusefulforcategorization,\na unique [CLS] token is added to the token sequence's first\nposition. The training process for Transformer and ViT is\nsame.\nC. LINFORMER\nTime-consuming nature of Transformer is O(n^2), where n\nrefers to the input sequence length. The main efficiency\nbottleneck lies in the self-attention module, leading to slow\nspeed in both training and inference, especially for inputs\nwith a long sequence. To address this problem, Wang et al.\n[26] propose Linformer, a method to approximate the self-\nattention calculation with a low-rank matrix, reducing the\ncomplexitytoO(n).Linformerhasbeenvalidatedtobemore\nefficientinboth time andspace,whilethemodelaccuracyis\non par with the standard Transformer. Therefore, we choose\nto integrate Linformer into the proposed ViT-FRD\nframeworktopursueamoreefficientlearningpipeline.\nThe main idea of Linformer is to combine two matrices for\nlinearprojectiontothecomputationofkeysandvalues.Also,\nparameters are shared between projections through both the\nlayers and the head. Specifically, there are three levels of\nsharing:\n Headwise sharing: Two projection matrices, E and\nF,aresharedforeachlayersuchthat,onallheadsi,\nEi=EandFi=F.\n Sharing of keys and values is subject to the\naforementioned restriction. A single projection\nmatrixEisproducedforeverylayer.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3302522\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\n1 VOLUME XX, 2017\n Layerwise sharing: The single projection matrix E\nissharedforalllayers,allheads,keysandvaluesin\nalllayers.\nD. CASCADING CROSS-LAYER REFORMER\nBy approximating the attention matrix, Linformer reduces\nthe computational complexity of the self-attention operation\nto O(n), significantly improving the efficiency of the\nTransformer architecture, especially for inputs with long\nsequences. Integrating Linformer into the ViT-FRD\nframework allows us to effectively process image data using\nthe Transformer architecture while addressing the efficiency\nproblem.\nThe different data distribution between the training and\ntest sets is one of the causes of the performance discrepancy\nbetween the two sets. Since the model is trained on the\ntraining set, it captures patterns based on what are presented\nbythetrainingsamples,whichmay beslightlydifferentthan\nthe ones in the test set. This discrepancy could lead to\nperformancedegradationwhenevaluatingthetestset'smodel.\nCCReformerissuggestedtosolvethisissue.Thecoreideais\nto train a neural network, namely, theCCReformer,with the\ndata in the training set, so that it can transform an input\nimage from the test set to an image that is more comparable\nto the ones in the training set. This way, the discrepancy\nbetween the training and test images can be eliminated,\nleading to a performance gain. We custom an AutoEncoder\n(AE) to fulfill the design requirement. An AE is usually an\nencoder-decoder network that goals to replicate the input to\nthe output by reducing the loss during reconstruction, which\nisdefinedinEquation1:\nLAE =||x−AEΘ(x)||2 (1)\nwhere x refers to an input image, AEΘ is the AE network\nparameterized by Θ. By minimizing the L2 loss, AE is\ntrained to capture the patterns from the input data. For our\ncase,acustomAEistrainedusingalltrainingdata.Atrained\nAE can thus transform an image in the test test to an image\nthat is closer to the ones in the training set. We name such a\ntrainedAEasReformer.\nBy stacking a series of Reformers together, we form a\nCascading Cross-layer Reformer (CCReformer) module. Let\nK represent the number of Reformers used to construct\nCCReformer.TheCCReformerwithKlayerscanbedefined\nrecursivelyasfollows.\nCCR1(x)=RF(x) (2)\nCCR2(x)=RF(CCR1(x)⊕ x) (3)\nCCRK(x)=RF(CCRK−1(x)⊕ CCRK−2(x)⊕ x)) (4)\nWhere RF and CCR represent the Reformer and\nCCReformer modules, respectively, and ⊕ is the element-\nwiseaddition.Figure4showsanexampleofCCReformer(K\n=3).\nFIGURE 4. The proposed CCReformer.\nE. FEATURE DISTILLATION\nThe main goal of using KD for this learning task is to\ntransmit understanding from a pre-trained teacher model to\nour model (the student model). Since the student model is\nbased on ViT, we tend to choose a model with a different\nneuralarchitecturetobetheteachermodel,whichisexpected\nto learn different patterns that are otherwise may not be\nlearned by the student model when trained individually. The\nKDprocedureutilizedinthisstudyfollowstheclassicsetting\nproposed by Hinton et al. [22]. We break down the process\nintothefollowingsteps.First,thetrainingsetisusedtotrain\nthe teacher model. Second,the student model is trained with\nthelossfunctiondefinedinEquation5.\nLKD(x,y)=α ·||fT(x)−fS(x)||2+(1−α)·CE(y,fS(x)) (5)\nLKDconsistsofthedistillationlossandthestudentloss.\nTheformer,denotedby||fT(x)−fS(x)||2 isaL2loss,\nmeasuresthesoftpredictiondifferencebetweentheteacher\nandthestudent;thelatter,denotedbyCE(y,fS(x)),isa\ncross-entropylossthatrepresentsthedifferencebetween\nstudentpredictionsandthegroundtruth.Factorαisutilized\ntoweightthetwolosses.\nIn the process of knowledge refinement, where (x,y)\nrepresents a labeled sample from the training set, theteacher\nmodel fT and student model fS are involved. By optimizing\nthe neural network using LKD, we ensure that the student\nmodel learns from both the instructor and the training set.\nThis way, the trained student model, after KD, can benefit\nmerits from both models, leading to a potential performance\ngain.\nF. SEGMENTATION METHOD\nThe detection head of ViT-FRD is used to obtain the\nsegmentation mapping by computing the outputs at different\nlocationsduringthedecodingphase.\nIntheprocessofdecodingtheViT-FRDmodel,theoutput\nof each location is a fixed dimensional vector that encodes\ntheimagefeaturesatthatlocation.Togeneratesegmentation\nmappings, the ViT detection head converts these output\nvectorsintosegmentationmappings(alsoknownassemantic\nsegmentation maps) by feeding them into a fully connected\nlayer that computes the segmentation probability for each\nlocation.\nIn the final output, the color of each pixel can be mapped\ntotheobjectclasstowhichthepixelbelongs.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3302522\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\n1 VOLUME XX, 2017\nIn summary, the detection head of ViT-FRD implements\nimagesegmentationbysemanticallyanalyzingthefeaturesat\neach location during the decoding process and generating\nsegmentationmappings..\nFeature Recombination and Feature Distillation (FRD)\nare two key components in the proposed ViT-FRD learning\nframework for whole-heart segmentation. Feature\nrecombination involves combining different feature\nrepresentations to capture complementary information and\nenhance the model's overall representation power. This is\nachieved through the cascading cross-layer reformer\n(CCReformer) module, which combines features from\nmultiplelayersusingelement-wiseaddition.Byrecombining\nfeatures, the model can address discrepancies between the\ntraining andtestsets,leadingtoimproved generalizationand\nperformanceonunseendata.Featuredistillation,ontheother\nhand, involves transferring knowledge from a pre-trained\nteacher model, typically a CNN,to the student model (ViT).\nThe student model learns from the teacher model's\nrepresentations and behavior, benefiting from its ability to\ncapture patterns that the student may struggle to learn from\nthe training data alone. Together, these techniques enhance\nthe learning capabilities of the ViT-FRD model, improving\nits performance in whole-heart segmentation across different\nimagingmodalitiesandclinicalscenarios.\nIII. EXPERIMENTS AND RESULTS\nThis section offers details of the experiment design and\nimplementation and reports the performance comparison\nresults.\nA. PERFORMANCE METRICS\nDice Coefficient: A measure of the similarity of two sets,\nusually used in semantic partitioning problems. As shown in\nEquation 6, where A and B represent two sets respectively,\n|S| denotes the number of elements in set S, and ∩ denotes\ntheintersection.\nJaccard Coefficient (Jaccard Coefficient) or Mean\nIntersection-over-Union (mIoU): used in the semantic\nsegmentation task to indicate the similarity of the predicted\nresulttothetrueresult.AsshowninEquation7,whereAand\nB represent two sets respectively, |S| denotes the number of\nelements in set S, ∩ denotes the intersection and ∪ denotes\ntheunion.\nPixelAccuracy:indicatestheproportionofallpixelpoints\nthatarecorrectlyclassifiedinthesemanticsegmentationtask.\nAs shown in Equation 8, where True Positives denotes the\nnumberofcorrectlypredictedpixels,TrueNegativesdenotes\nthe number of correctly predicted background pixels, and\nTotalPixelsdenotesthenumberoftotalpixels.\nDice=2|A∩B|/(|A|+|B|) (6)\nmIoU=|A∩B|/|A∪B| (7)\nPixel Accuracy = (True Positives + True Negatives) / Total\nPixels (8)\nThe maximum Hausdorff distance (maxD) between the\npredicted segmentationand thegroundtruthsegmentation.It\nmeasures the largest distance between any point in one\nsegmentationtothenearestpointintheothersegmentation.\nMaxHausdorffDistanceiscalculatedasfollows:\nmaxD=max{maxmindist(p,q),maxmindist(q,p)} (9)\nwhere p is a point in the predicted segmentation and q is a\npointinthegroundtruthsegmentation.Thefunctiondist(p,q)\nrepresentsthedistancebetweentwopoints.\nMean Absolute Distance (MAD): The average distance\nbetween corresponding points in the predicted segmentation\nand the ground truth segmentation. It provides a measure of\ntheoveralldisplacementbetweenthetwosegmentations.\nMeanAbsoluteDistance iscalculatedasfollows:\nMAD=(1/N)*sum{dist(p,q)} (10)\nwhereNisthetotalnumberofcorrespondingpointsbetween\nthe predicted segmentation and the ground truth\nsegmentation, and dist(p, q) represents the distance between\ntwopoints.\nB. MODEL TRAINING\nThe tests are carried out using Python 3.9, while Pytorch\nV1.10servesasthedeeplearningframework.AWindows10\nworkstationwith32GBofRAMandani7-10875hCPUwas\nused for the experiments. A GTX2080TI graphics card was\nusedtospeeduptraining.Whenitcametotrainingthemodel,\nwe selected Adam as the optimizer and set the learning rate\nto0.0001.Topreventthedenominatorfrombecoming0,the\noptimizer's beta1 and beta2 parameters are set to 0.9 and\n0.999, respectively, with eps=1e-08. Moreover, a batch size\nof eight was employed, and the weight decay was set to 0.\n200trainingepochswereusedforallmodels.Forthetraining\nset, we used random crop, random horizontal flip with a\nprobabilityof0.5andresizedtheinputimagestoafixedsize\nof224×224.Thereisnodataaugmentationforthetestdata.\nThe teacher model of KD was ResNet152. The AE used to\nbuild a Reformer follows a straightforward design, which\nconsists of an encoder and a decoder. The encoder is a six-\nlayer multi-layer perceptron (MLP) to transform a flattened\nimage to a latent vector of size three, which then passes\nthroughasix-layerMLPdecodertorecoveranimageof224\n×224.\nC. BASELINES\nThefollowingmethodshavebeenusedasbaselinesforafair\ncomparisonwiththeproposedmethod.\nCNN-based models. Several representative CNN models\nare used as baselines, including UNet [23], FCN [24], and\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3302522\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\n1 VOLUME XX, 2017\nSegNet [25]. Cheng Chen developed a custom model based\non Transform architecture-SIFA (Synergistic Image and\nFeature Adaptation), which consists of two encoders with\ndifferent angles and a decoder[26]. The two encoders with\ndifferent angles propose a collaborative approach from the\nperspective of images and features. Judy Hoffman's solution\nis a CyCADA (Cycle-Consistent Adversarial Domain\nAdaptation) model based on GAN, where the authors find\ndomain-invariant representations by using an adversarial\napproach and capture pixel-level and low-level domain\nchangesandreachmultiplesegmenteddatasetsinSOTA[27].\nD. RESULTS AND ANALYSIS\nA performance comparison between the baseline and\nproposed methods is reported in Table 3-4. The top half of\nthetablepresentsthebaselineperformance,whilethebottom\nhalf displays the results of the ablation study. All results are\nbased on the two datasets mentioned in Section 2.1, and the\nevaluation metrics include Dice, mIoU, Pixel Accuracy,\nmaxD and MAD, as defined in Section 3.1. The observed\nresultsareasfollows:\n The proposed method outperforms the baselines,\nincluding mature CNN models like ResNet and\nXception and the top-ranking solutions of the\nKagglecontestinallfourmetrics.\n The base ViT model is worse than several CNN\ncompetitors. However, the addition of the three\nboosting modules have proven to be effective.\nSpecifically,Linformer,KD,andCCReformerhave\nbrought an incremental Acc gain of 0.48%, 1.44%,\nand0.48%,respectively,leadingtoacombinedAcc\ngain of 2.4% compared to the base ViT model.\nThesejointeffortsallowtheproposedmethodtobe\nsuperiorthanthebaselinesaswellastheSOTA.\n In addition to Acc, the other three metrics present\nstoriesabouttheperformancegain.Specifically,Pre\ncan be regarded as one minus the ratio of false\nalarms, a kind of mis-classification. A higher Pre\nindicatesalarms.WenoticethatthejointgainofPre\nwas 1.63%, meaning that a portion of false alarms\nhave been fixed. Also, Rec reflects the ratio of TP\nand all correct predictions. A higher Rec indicates\nthat less missing for the heart cases. It is observed\nthat a total gain of Rec was 2.3%, meaning that\nmore heart samples that were missed previously\nhavebeendetected.\nTABLEⅢ PERFORMANCECOMPARISON.ABBREVIATIONSINTHETABLEINCLUDELINFORMER(LIN.)\nANDCCREFORMER(CCR.).onMM-WHS2017\nFrame\nwork Method\n10%（labeleddataintrainset）\nDice mIoU PixAcc maxD MAD\nCNN\nU-Net 0.5647325 0.4236036 0.9735768 7.307813263 8.87914\nFCN 0.600243 0.447083 0.95195 10.94169849 12.581523\nSegNet 0.6584865 0.4985509 0.9551059 10.79153794 13.542914\nCyCADA 0.686343 0.5413044 0.95918 10.92167708 14.173181\nViT\nSIFA\n(SOTA) 0.7335639 0.601217 0.9538222 12.2831327 15.023388\nViT-FRD(WithoutFeature\nRecombination) 0.731 0.588 0.971 8.969589978 10.452304\nViT-FRD (WithoutFeature\nDistillation) 0.747 0.581 0.933 11.48227646 13.184765\nViT-FRD 0.761 0.611 0.985 5.405779674 5.97186\nFrame\nwork Method\n20%（labeleddataintrainset）\nDice mIoU PixAcc maxD MAD\nCNN\nU-Net 0.7227831 0.5818752 0.969053 15.8269216 17.729334\nFCN 0.7297774 0.60228 0.9722468 15.24630082 16.791075\nSegNet 0.7954996 0.657888 0.9626042 14.96600113 18.916235\nCyCADA 0.7964432 0.6870852 0.9619216 14.27526262 17.873484\nViT\nSIFA\n(SOTA) 0.8009614 0.7070944 0.971727 12.12296145 15.716358\nViT-FRD(WithoutFeature\nRecombination) 0.794 0.69 0.978 6.386828578 7.731284\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3302522\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\n1 VOLUME XX, 2017\nViT-FRD (WithoutFeature\nDistillation) 0.801 0.721 0.966 5.986400454 7.135934\nViT-FRD 0.814 0.718 0.98 4.364666552 5.407272\nFrame\nwork Method\n40%（labeleddataintrainset）\nDice mIoU PixAcc maxD MAD\nCNN\nU-Net 0.8099738 0.6951116 0.9742914 17.29849496 20.844864\nFCN 0.8209587 0.7197246 0.9812306 15.99710355 19.28786\nSegNet 0.8136853 0.7087684 0.978728 13.08398895 15.664395\nCyCADA 0.8201916 0.7158448 0.9733666 11.24881935 10.664208\nViT\nSIFA\n(SOTA) 0.8481696 0.7499394 0.975741 11.14764201 8.016078\nViT-FRD(WithoutFeature\nRecombination) 0.839 0.717 0.984 6.466914203 7.946446\nViT-FRD (WithoutFeature\nDistillation) 0.823 0.788 0.98 4.735062566 5.517072\nViT-FRD 0.856 0.763 0.988 4.274570224 4.821257\nTABLEⅣPERFORMANCECOMPARISON.ABBREVIATIONSINTHETABLEINCLUDELINFORMER(LIN.)AND\nCCREFORMER(CCR.).ON ECHONET-DYNAMIC\nFrame\nwork Method\n10%（labeleddataintrainset）\nDice mIoU PixAcc maxD MAD\nCNN\nU-Net 0.52362 0.4200877 0.9085419 7.942862236 9.986368758\nFCN 0.5959813 0.4349671 0.871415 11.49097175 13.26344155\nSegNet 0.649992 0.4788083 0.9132723 11.20809131 14.25256269\nCyCADA 0.6785873 0.5063903 0.8678661 12.13398324 15.94624594\nViT\nSIFA\n(SOTA) 0.7096497 0.5983312 0.9474316 13.90450622 16.1231\nViT-FRD(WithoutFeature\nRecombination) 0.6987629 0.566538 0.9345875 9.416275558 10.95819551\nViT-FRD (WithoutFeature\nDistillation) 0.7374384 0.537425 0.9294546 12.56161044 14.30810698\nViT-FRD 0.7483674 0.5825885 0.969043 5.972305384 6.707593152\nFrame\nwork Method\n20%（labeleddataintrainset）\nDice mIoU PixAcc maxD MAD\nCNN\nU-Net 0.6656832 0.5521414 0.9441483 17.67708874 19.4809922\nFCN 0.678766 0.5530135 0.9439544 16.77245553 17.72130056\nSegNet 0.7819761 0.5932834 0.8674026 16.24858743 20.90811455\nCyCADA 0.7548689 0.6557541 0.9435489 15.48723242 19.75019982\nViT\nSIFA\n(SOTA) 0.7351224 0.7070237 0.9483084 13.03703275 16.45031192\nViT-FRD(WithoutFeature\nRecombination) 0.7176172 0.645357 0.9078774 6.853067064 7.98255073\nViT-FRD (WithoutFeature\nDistillation) 0.7663167 0.6678121 0.9177966 6.357557282 7.67326983\nViT-FRD 0.7774514 0.670612 0.952168 4.648369877 6.061551912\nFrame\nwork Method\n40%（labeleddataintrainset）\nDice mIoU PixAcc maxD MAD\nCNN\nU-Net 0.7781418 0.6276858 0.9121316 18.6944835 21.79955877\nFCN 0.8123386 0.7036747 0.9619004 17.38565214 21.19928693\nSegNet 0.795296 0.6933172 0.8900552 14.55593771 16.84549038\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3302522\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\n1 VOLUME XX, 2017\nCyCADA 0.7993587 0.6858509 0.9178847 12.60767673 11.68157344\nViT\nSIFA\n(SOTA) 0.8125465 0.7087677 0.9049998 12.07958489 8.585219538\nViT-FRD(WithoutFeature\nRecombination) 0.7556034 0.6979278 0.8945544 7.271398329 8.334232565\nViT-FRD (WithoutFeature\nDistillation) 0.7746076 0.7156616 0.902286 5.132807822 5.967265075\nViT-FRD 0.7852944 0.7577353 0.9572732 4.41434867 5.149584602\nIV. DISCUSSION\nCurrent DNN-based models for cardiac MRI Image\nsegmentation are mostly CNN models, relying on the\nconvolutional layer for feature extraction. These models are\nlimited by the homogeneous building blocks, affecting the\npredictive per-formance. Increasing model diversity is a\npotential strategy for performance improvement. This paper\ninvestigates a KD-based training procedure to combine the\nmerits of two types of models, namely, ViT and CNN. KD\ninvolves two models, namely, a student (ViT) and a teacher\n(CNN). The training allows the student to learn knowledge\nfromtheteacherbyoptimizingadistillationloss.Meanwhile,\nthestudentcanstilllearnknowledgebyitself.DrivenbyKD,\nthestudentmodelappreciatesthepatternsdiscoveredbyboth\nViT and CNN, which can effectively lift the performance.\nOntheotherhand,likemanyotherimagesegmentationtasks,\noverfitting has been an issue for heart segmentation. It is\nobserved that the discrepancy of training and test data\ndistribution is the key to cause the performance gap. To this\nend,wedesignCCReformer,aneuralnetworkthatcanlearn\nthe distribution of training data and apply to test data. The\ndistribution of the reformed test data is closer to that of the\ntraining data, leading to a performance gain. Overall, the\nproposed method exhibits superior performance in four\nmetrics when evaluated on the MM-WHS 2017 dataset,\noutperforming the selected CNN baseline models as well as\nthe top-ranking solutions (i.e., SOTA) from the Kaggle\ncompetition.\nThe study has the following limitations that can be\naddressed in the future work. First, the KD procedure can\ninvolve more teacher models in addition to CNN, e.g., the\nCapsuleNeuralNetwork,tofurtherenhancemodeldiversity.\nIn other words, the student model can learn from more than\none teacher models, which may lead to further gains in\nperformance. From the data perspective, we adopt\nCCReformer, an effort to close the distribution gap between\ntraining and test data. Results show that the effect of\nCCReformer depends on the number of reformers, namely,\nparameter K. It is desired to explore a general usage of\nCCReformeronmoreimagesegmentationtasksanddevelop\nstrategies to enhance the robustness of CCReformer. Lastly,\nin addition to the detection result, it would be beneficial to\nenable the DNN models to provide evidence on the subtle\ndifference between normal and heart samples, which would\nlead to a more convincing decision making process in a real\nworldsettingwithbetterexplainability.\nACKNOWLEDGMENTS\nTheworkisnotfundedbyanyorganization.\nDECLARATION OF COMPETING INTEREST\nThe authors declare that they have no known competing\nfinancial interests or personal relationships that could have\nappearedtoinfluencetheworkreportedinthispaper.\nData charts which are typically black and white, but\nsometimesincludecolor.\nREFERENCES\n[1] Roth GA, Mensah GA, Fuster V. The Global Burden of\nCardiovascular Diseases and Risks: A Compass for Global Action.\nJ Am Coll Cardiol. 2020 Dec 22;76(25):2980-2981. doi:\n10.1016/j.jacc.2020.11.021. PMID: 33309174. S. Dong, P. Wang,\nK.Abbas,Asurveyondeeplearninganditsapplications,Computer\nScienceReview40(2021)100379.\n[2] Muhammad Y, Tahir M, Hayat M, et al. Early and accurate\ndetection and diagnosis of heart disease using intelligent\ncomputationalmodel[J].Scientificreports,2020,10(1):19747.\n[3] Celermajer D S, Chow C K, Marijon E, et al. Cardiovascular\ndisease in the developing world: prevalences, patterns, and the\npotential of early disease detection[J]. Journal of the American\nCollegeofCardiology,2012,60(14):1207-1216.\n[4] Clinical cardiac MRI[M]. Springer Science & Business Media,\n2012.\n[5] Pirruccello J P, Bick A, Wang M, et al. Analysis of cardiac\nmagnetic resonance imaging in 36,000 individuals yields genetic\ninsights into dilated cardiomyopathy[J]. Nature communications,\n2020,11(1):2254.\n[6] Sander J, de Vos B D, Išgum I. Automatic segmentation with\ndetection of local segmentation failures in cardiac MRI[J].\nScientificReports,2020,10(1):21769.\n[7] Chen C, Qin C, Qiu H, et al. Deep learning for cardiac image\nsegmentation: a review[J]. Frontiers in Cardiovascular Medicine,\n2020,7:25.\n[8] K. Simonyan, A. Zisserman, Very deep convolutional networks for\nlarge380scaleimagerecognition,arXivpreprintarXiv:1409.1556.\n[9] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image\nrecognition, in: Proceedings of the IEEE conference on computer\nvisionandpatternrecognition,2016,pp.770–778.\n[10] G. Huang, Z. Liu, L. Van Der Maaten, K. Q. Weinberger, Densely\ncon-nected convolutional networks, in: Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2017, pp.\n4700–4708.\n[11] M. Tan, Q. Le, Efficientnet: Rethinking model scaling for\nconvolutional neural networks, in: International conference on\nmachinelearning,PMLR,2019,pp.6105–6114.\n[12] F. Chollet, Xception: Deep learning with depthwise separable\nconvolutions, in: Proceedings of the IEEE conference on computer\nvisionandpatternrecognition,2017,pp.1251–1258.\n[13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, I. Polosukhin, Attention is all you need,\nAdvancesinneuralinformationprocessingsystems30.\n[14] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training\nof deep bidirectional transformers for language understanding,\narXivpreprintarXiv:1810.04805.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3302522\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\n1 VOLUME XX, 2017\n[15] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M.\nLewis,L. Zettlemoyer, V. Stoyanov, Roberta: A robustly optimized\nbertpretrain-ingapproach,arXivpreprintarXiv:1907.11692.\n[16] A. Dosovitskiy, L. Beyer, A.Kolesnikov, D. Weissenborn,X. Zhai,\nT. Unterthiner,M.Dehghani,M.Minderer, G. Heigold,S. Gelly, et\nal., An image is worth 16x16 words: Transformers for image\nrecognitionatscale,arXivpreprintarXiv:2010.11929.\n[17] G. Hinton, O. Vinyals, J. Dean, et al., Distillingthe knowledge ina\nneuralnetwork,arXivpreprintarXiv:1503.025312(7).\n[18] D. M. Hawkins, The problem of overfitting, Journal of chemical\ninformationandcomputersciences44(1)(2004)1–12.\n[19] J. Kim, J. Kim, H. L. T. Thu, H. Kim, Long short term memory\nrecur-rent neural network classifier for intrusion detection,in: 2016\ninternational conference on platform technology and service\n(PlatCon),IEEE,2016,pp.1–5.\n[20] J. Chung, C. Gulcehre, K. Cho, Y. Bengio, Empirical evaluation of\ngated recurrent neural networks on sequence modeling, arXiv\npreprintarXiv:1412.3555.\n[21] S. Wang, B. Z. Li, M. Khabsa, H. Fang, H. Ma, Linformer: Self-\nattentionwithlinearcomplexity,arXivpreprintarXiv:2006.04768.\n[22] X. Zhuang and J. Shen, “Multi-scale patch and multimodality\natlases for whole heart segmentation of mri,” Medical image\nanalysis,vol.31,pp.77–87,2016.\n[23] Ouyang, D., He, B., Ghorbani, A., Lungren, M.P., Ashley, E.A.,\nLiang, D.H., Zou, J.Y.: Echonet-dynamic: a large new cardiac\nmotion video data resource for medical machine learning. In:\nNeurIPSML4HWorkshop:Vancouver,BC,Canada(2019)\n[24] Ronneberger,Olaf,PhilippFischer,andThomasBrox.\"U-net:\nConvolutionalnetworksforbiomedicalimage\nsegmentation.\"MedicalImageComputingandComputer-Assisted\nIntervention–MICCAI2015:18thInternationalConference,\nMunich,Germany,October5-9,2015,Proceedings,PartIII18.\nSpringerInternationalPublishing,2015.\n[25] Dai,Jifeng,etal.\"R-fcn:Objectdetectionviaregion-basedfully\nconvolutionalnetworks.\"Advancesinneuralinformation\nprocessingsystems29(2016).\n[26] Badrinarayanan,Vijay,AlexKendall,andRobertoCipolla.\"Segnet:\nAdeepconvolutionalencoder-decoderarchitectureforimage\nsegmentation.\"IEEEtransactionsonpatternanalysisandmachine\nintelligence39.12(2017):2481-2495.\n[27] Chen,Cheng,etal.\"Synergisticimageandfeatureadaptation:\nTowardscross-modalitydomainadaptationformedicalimage\nsegmentation.\"ProceedingsoftheAAAIconferenceonartificial\nintelligence.Vol.33.No.01.2019.\nCHUNYU FAN, Hereceivedhisbachelor's\ndegreefromBethuneClinicalCollegeofJilin\nUniversityandthenreceivedamaster'sdegree\nfromChinaMedicalUniversity.Currently,heis\nalectureratLiaoningProvincialPeople's\nHospital.Hismainresearchinterestsare\ninterventionaltherapyofcoronaryheartdisease.\nHeisgoodatemergencyandplainclinic\ncoronaryangiographyandstentimplantation.\nHeisskilledinnewtechnologiessuchas\ncoronaryluminalvascularultrasound(IVUS)\nandcoronaryflowreservefractionmeasurement(FFR)andhasrich\nexperienceinthetreatmentofperioperativecomplications.\nQI SU, He is a young medical specialist born in\n1987 and graduated from Liaoning University of\nTraditional Chinese Medicine with a master's\ndegree. Currently, he is working as a lecturer in\nLiaoning People's Hospital.His main research\ninterests are in the interventional treatment of\ncoronary artery disease, and he is dedicated to the\nin-depth investigation of the pathological\nmechanism and treatment of coronary artery\ndisease.\nZHIFENG XIAO isanAssociateProfessor\nofDepartmentComputerScienceand\nSoftwareEngineeringatPennStateErie,\nTheBehrendCollege.HewasanAssistant\nProfessoratPennStateBehrendfrom2013\nto2019.Beforethat,heobtainedaPh.D.\ndegreeinComputerScienceatthe\nUniversityofAlabamain2013andaB.S.\ndegreeinComputerScienceatShandong\nUniversity,China,in2008.Heisbroadly\ninterestedininterdisciplinaryAIand\ncybersecurity,withaparticularfocusonthe\nareasofAI-powereddecisionscience,accountablesystems,and\nbioinformatics.\nAIJIE HOU,, ShewasChiefphysician,\nsecond-levelprofessor,vicepresidentof\nLiaoningProvincialPeople'sHospital,\ndoctoraldegree,memberofZhigong,\nStandingCommitteeofShenyangCPPCC,\nSpecialexpertofTheStateCouncil,\nDirectorofLiaoningClinicalMedical\nResearchCenterforCoronaryHeartDisease,\nDirectorofCardiovascularDisease\nDiagnosisandTreatmentCenter,Directorof\nPharmacologicalBase,DirectorofCatheter\nOffice.MemberoftheCardiovascular\nSocietyofChineseMedicalAssociation,\nViceChairmanoftheCardiovascularSpecialtyCommitteeofLiaoning\nMedicalAssociation,ViceChairmanoftheHumanitiesBranchof\nLiaoningMedicalAssociation,mastertutorofChinaMedicalUniversity,\nDalianMedicalUniversityandLiaoningUniversityofTraditionalChinese\nMedicine.Heisgoodininterventionaltherapyofcoronaryheartdisease\n(PCI)andchemicalablationtherapyofhypertrophicobstructive\ncardiomyopathy,especiallyinradialarterycoronaryheartdiseaseand\ninterventionaltherapyofcompletelyoccludedvessels.\nBO LUAN, He is an MD, MSc and Chief\nPhysician with extensive medical experience\nand expertise. His main research interests are\nthe application of intracavitary imaging for\ncoronary interventions, especially for\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3302522\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\n1 VOLUME XX, 2017\ncomplexlesions inthecoronaryarteries. As thedirectorofCardiovascular\nV and the chief expert of the provincial CTO club, he has rich clinical\nexperience and expertise, especially in the diagnosis and treatment of\ncommon diseases, multi-morbidities and difficult and serious diseases of\nthecardiovascularsystem.HehasbeentotheUniversityofMelbournefor\ntraining on interventional treatment of coronary artery disease and is very\nfamiliar with various advanced techniques of coronary interventional\ntreatment, such as reverse guide wire opening CTO lesion technique,\ncalcified lesion spin mill technique, intravascular ultrasound and OCT\nexamination.\nHAO SU, Heisamedicalspecialistandwas\nbornin1987.HegraduatedfromDalian\nMedicalUniversitywithamaster'sdegreeand\niscurrentlyworkingasalectureratLiaoning\nProvincialPeople'sHospitalandispursuinghis\nPhDatChinaMedicalUniversity.Hismain\nresearchinterestsareincoronaryinterventional\ntherapy,andheisdedicatedtoexplorethe\ntreatmentmethodsandeffectsofcoronaryheart\ndiseaseindepth.Currently,hehasmasteredthe\ntechniquesrelatedtocoronaryangiographyand\nstentimplantation,andhashighclinicalpracticeexperience.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3302522\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
}