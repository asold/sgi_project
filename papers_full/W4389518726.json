{
  "title": "PaRaDe: Passage Ranking using Demonstrations with LLMs",
  "url": "https://openalex.org/W4389518726",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2583984171",
      "name": "Andrew Drozdov",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A2128637305",
      "name": "Honglei Zhuang",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A2398480995",
      "name": "Zhuyun Dai",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A2096947334",
      "name": "Zhen Qin",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A2134428435",
      "name": "Razieh Rahimi",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A2102775025",
      "name": "Xuanhui Wang",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A2503842870",
      "name": "Dana Alon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2068391019",
      "name": "Mohit Iyyer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2077104176",
      "name": "Andrew McCallum",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A2151486164",
      "name": "Donald Metzler",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A2130499761",
      "name": "Kai-hui",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2891177506",
    "https://openalex.org/W4385571898",
    "https://openalex.org/W4287891464",
    "https://openalex.org/W3104657626",
    "https://openalex.org/W4385573504",
    "https://openalex.org/W2903158431",
    "https://openalex.org/W4310509152",
    "https://openalex.org/W4320839455",
    "https://openalex.org/W4385571264",
    "https://openalex.org/W4384890816",
    "https://openalex.org/W2963516811",
    "https://openalex.org/W1553262910",
    "https://openalex.org/W4385573057",
    "https://openalex.org/W4378508715",
    "https://openalex.org/W3210277894",
    "https://openalex.org/W4313680149",
    "https://openalex.org/W3156470785",
    "https://openalex.org/W4385567387",
    "https://openalex.org/W3180230246",
    "https://openalex.org/W4368755400",
    "https://openalex.org/W4383046915",
    "https://openalex.org/W4284669679",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2609701267",
    "https://openalex.org/W4206765718",
    "https://openalex.org/W4321855256",
    "https://openalex.org/W4389518841",
    "https://openalex.org/W4366559971",
    "https://openalex.org/W4297162632",
    "https://openalex.org/W3156836409",
    "https://openalex.org/W4385571445",
    "https://openalex.org/W4307079201"
  ],
  "abstract": "Andrew Drozdov, Honglei Zhuang, Zhuyun Dai, Zhen Qin, Razieh Rahimi, Xuanhui Wang, Dana Alon, Mohit Iyyer, Andrew McCallum, Donald Metzler, Kai Hui. Findings of the Association for Computational Linguistics: EMNLP 2023. 2023.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 14242–14252\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nPaRaDe: Passage Ranking using Demonstrations\nwith Large Language Models\nAndrew Drozdov\n♠♦∗\nHonglei Zhuang♠ Zhuyun Dai♠ Zhen Qin♠\nRazieh Rahimi♦ Xuanhui Wang♠ Dana Alon♠ Mohit Iyyer♦\nAndrew McCallum♦ Donald Metzler♠ Kai Hui\n♠†\n♠Google ♦UMass Amherst CICS\nAbstract\nRecent studies show that large language mod-\nels (LLMs) can be instructed to effectively per-\nform zero-shot passage re-ranking, in which\nthe results of a ﬁrst stage retrieval method,\nsuch as BM25, are rated and reordered to im-\nprove relevance. In this work, we improve\nLLM-based re-ranking by algorithmically se-\nlecting few-shot demonstrations to include in\nthe prompt. Our analysis investigates the con-\nditions where demonstrations are most help-\nful, and shows that adding even one demon-\nstration is signiﬁcantly beneﬁcial. We propose\na novel demonstration selection strategy based\non difﬁculty rather than the commonly used se-\nmantic similarity. Furthermore, we ﬁnd that\ndemonstrations helpful for ranking are also ef-\nfective at question generation. We hope our\nwork will spur more principled research into\nquestion generation and passage ranking.\n1 Introduction\nLarge language models (LLMs) exhibit strong per-\nformance on a variety of tasks without additional\ntask-speciﬁc ﬁne-tuning. Their success is often\nattributed to in-context learning, where the param-\neters of the language model are frozen and it learns\nhow to perform a new task by reading demonstra-\ntions in the prompt (Brown et al., 2020; Basu et al.,\n2023; Min et al., 2022; Akyürek et al., 2023).\nWhile LLMs are often used to generate answers,\nour focus is on scoring for the task of passage re-\nranking—passages are ﬁrst retrieved by an efﬁcient\nretriever, e.g. BM25, then rated and reordered by\nthe LLM. Existing works like UPR (Sachan et al.,\n2022) demonstrate promising results for zero-shot\nranking using LLM. We aim to improve over zero-\nshot ranking by including demonstrations in the\nprompt and explore multiple strategies for select-\ning demonstrations. Manual selection is often sub-\n∗Work completed while a Student Researcher at Google.\n†Final author. Also generated Table 1 results using DQL-\nscored demonstration candidates from AD.\noptimal and requires a human-in-the-loop when\nusing the LLM for a new task. Instead, we seek\na method that ﬁnds effective demonstrations auto-\nmatically, with minimal or no human involvement.\nIn this paper, we investigate approaches for au-\ntomatic demonstration selection to improve upon\nUPR’s zero-shot ranking approach. Our initial anal-\nysis highlights the complex nature of the problem,\nshowing that ranking performance varies drasti-\ncally depending on the demonstrations included\nin the prompt. Furthermore, simply including\nmore demonstrations does not always lead to better\nranking quality. Next, we investigate the use of\nestablished demonstration selection methods, i.e.\nsimilarity-based selection (Rubin et al., 2022; Luo\net al., 2023), on ranking tasks and show that similar-\nity of demonstrations does not correlate well with\nranking quality. Thereafter, we propose difﬁculty-\nbased selection (DBS) as a simple and effective\napproach to automatically ﬁnd challenging, i.e. low\nlikelihood, demonstrations to include in the prompt.\nAlthough we prompt frozen LLMs, we intend to\nemulate the training dynamics of ﬁne-tuning, and\nchoose hard samples because they potentially cor-\nrespond to large gradient updates and are often cho-\nsen to improve learning in gradient descent (Shri-\nvastava et al., 2016; Chang et al., 2017). Finally,\ngiven the increasing importance of question gener-\nation for ranking (Nogueira et al., 2019; Bonifacio\net al., 2022; Dai et al., 2023; Jeronymo et al., 2023),\nwe extend the uses of the proposed difﬁculty-based\nselection for better question generation.\nTo this end, we present Passage Ranking with\nDemonstrations (PaRaDe). Our main contributions\ninclude: (1) analysis highlighting the complexity\nof demonstration selection; (2) DBS, an automatic\nand effective way to choose demonstrations; and (3)\nextensive experiments on re-ranking and question\ngeneration, including results with an extension of\nDBS that jointly selects multiple demonstrations.\n14242\n2 LLM Re-ranking by Query Likelihood\nBackground. Given a query q and set of initially\nretrieved documents D, the goal is to rank each\ndocument d in D by relevance with respect to\nq. UPR (Sachan et al., 2022) introduces a zero-\nshot method to rank documents according to the\nlog-likelihood of q given d using a large language\nmodel,\nℓ(q | d) ∝\n∑\ni=1..N\nlog P(qi | d, q1:i−1), (1)\nwhich resembles the query likelihood (QL) retrieval\nmodel (Ponte and Croft, 1998). It is factorized\nusing the probabilities of each token in the query\nqi and preﬁx of that token q1:i−1. Extending QL to\ninclude demonstrations in the context yields,\nℓ(q | d) ∝\n∑\ni=1..N\nlog P(qi | z1, ..., zk, d, q1:i−1),\n(2)\nwhere each zi is a positive query-document pair.\n3 Experiments on TREC and BEIR\nTo empirically measure the effectiveness of demon-\nstration selection, we conduct analysis using the re-\nranking task on TREC 2019 and 2020, and further\nperform evaluation on seven datasets from BEIR\n(Thakur et al., 2021). We use language models\nknown to effectively incorporate demonstrations\nthrough in-context learning, including the Flan-T5-\nXL and XXL (Chung et al., 2022) and the more\nrecently PaLM 2-S (Google et al., 2023).\nSetup. For each dataset, we retrieve the top-100\ndocuments using BM25 from Pyserini (Lin et al.,\n2021). The LLMs re-rank these top documents\nusing query likelihood (§2) in a point-wise man-\nner, and the re-ranked results are evaluated using\nnDCG@10. Herein, the instruction (Table 4) and\nthe selected demonstrations composite the prompt\nstring when scoring each (query, passage) pair.\n4 Demonstrations for Ranking\n4.1 The Impact of Demonstrations\nIn this section, we investigate the helpfulness of\ndemonstrations for ranking and sensitivity to the\nchoice of demonstration. We explore multiple\nstrategies for selecting demonstrations: random\nsampling, similarity-based selection (SBS), and\nour new approach difﬁculty-based selection (DBS).\nOur ﬁndings indicate that the choice of demonstra-\ntions considerably impacts ranking performance.\nQuery ID\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0nDCG@10\n0-shot\nFigure 1: Statistics for nDCG@10 on TREC 2020,\naggregated using the same query with 100 different\none-shot demonstrations. Flan-T5-XXL is used for re-\nranking. Zero-shot results included for reference.\nDemonstrations inﬂuence ranking. Figure 1\nshows ranking performance of zero-shot and one-\nshot prompts, and we can see that LLMs are quite\nsensitive to the choice of demonstrations. We ran-\ndomly sampled 100 demonstrations for use in one-\nshot re-ranking with query likelihood and also com-\npare against zero-shot. On 25.9% of queries the\nminimum one-shot nDCG@10 outperforms zero-\nshot, while zero-shot outperforms the max one-shot\nfor 11.1% of queries. It is worth emphasizing that\nthere is a high variance across different one-shot\ndemonstrations on many queries.\nIncreasing demonstrations does not necessarily\nhelp. Surprisingly, we ﬁnd little beneﬁt when ran-\ndomly sampling more demonstrations beyond one-\nshot (see Figure 2). There is a minor, almost neg-\nligible improvement in median performance when\nusing four demonstrations, and even less change\nwith eight. We do notice slightly decreased varia-\ntion as we increase the number of demonstrations.\nThis highlights the difﬁculties in selecting demon-\nstrations for ranking tasks beyond one-shot.\nGiven the large variance in one-shot perfor-\nmance and the difﬁculty to improve performance\nby increasing demonstrations, we need an effective\nway to select high performing demonstrations.\nSimilarity-based selection is limited. A simple\nand widely used baseline for selecting demonstra-\ntions is semantic similarity (Rubin et al., 2022; Luo\net al., 2023). Intuitively, it makes sense that se-\nmantically similar demonstrations to the test query\nwould help teach the LLM how to re-rank, al-\nthough, if the LLM is already familiar with the\ndemonstrations then it is not clear whether they\n14243\nwill prove helpful. We perform post-hoc analysis\non our TREC 2020 experiments with 100 random\none-shot demonstrations. We measured semantic\nsimilarity using cosine similarity and Universal\nSentence Encoder (Cer et al., 2018) embeddings of\nthe demonstration and test queries. By comparing\nthe semantic similarity and the nDCG, we ascertain\nthere is little or no correlation between high seman-\ntic similarity and strong re-ranking. Our ﬁndings\nshow this correlation is signiﬁcant only 5% of the\ntime, thus conclude that similarity alone has clear\nlimitations for demonstration selection. In the next\nsubsection, we explore a new technique inspired by\nin-context learning dynamics rather than semantic\nsimilarity for selecting demonstrations.\n4.2 Difﬁculty-based Selection (DBS)\nWe propose difﬁculty-based selection to ﬁnd chal-\nlenging demonstrations to include in the prompt.\nWe estimate difﬁculty using demonstration query\nlikelihood (DQL):\nDQL(z) ∝ 1\n|q(z)| log P(q(z) | d(z)),\nthen select the demonstrations with the lowest DQL.\nIntuitively, this should ﬁnd hard samples that poten-\ntially correspond to large gradients had we directly\ntrained the model instead of prompting.1\n4.2.1 Difﬁcult Demonstrations are Beneﬁcial\nOn TREC 2020, we observe a statistically signiﬁ-\ncant (p=0.008) correlation (0.26) between negative\nDQL and ranking performance with the 100 ran-\ndom one-shot demonstrations from §4.1. The low-\nest DQL outperforms average nDCG@10 across\nthe 100 random demonstrations (64.1 vs. 62.8).\nTo further investigate how well DQL works for\nselecting demonstrations, we sample easy or hard\ndemonstrations from the full MS Marco training\ndata rather than only using our initially selected\n100. We form four bins by sampling 30 demon-\nstrations from each of the bottom-1% and 10% by\nDQL (these are the hardest, and should give the\nbest results), and the same with easy ones. We\nplot the mean and max nDCG@10 for each bin in\nFigure 3. For Flan-T5-XXL, the performance im-\nproves as we use more challenging demonstrations.\nThis trend is less prominent for Flan-T5-XL.\n1Recent theories on the effectiveness of in-context learn-\ning view few-shot prompting similarly to ﬁne-tuning on the\ndemonstrations in the prompt (Basu et al., 2023).\nXL-0 XL-1 XL-4 XL-8 XXL-0 XXL-1 XXL-4 XXL-8\n60\n61\n62\n63\n64nDCG@10\nFigure 2: Statistics for nDCG@10 on TREC 2020,\naggregated using 100 different k-shot demonstrations\nwith Flan-T5-XL and XXL models. Number of demon-\nstrations (k) shown after the dash.\n1%\n(Easy)\n10%\n(Easy)\n10%\n(Hard)\n1%\n(Hard)\n0.62\n0.63\n0.64\n0.65nDCG@10\nFLAN-T5-XXL\nFLAN-T5-XL\nFigure 3: The nDCG@10 of DQL-based bins measured\non TREC 2020. The x-axis increases in difﬁculty of\ndemonstration from left-to-right.\n5 Main Results and Discussion\n5.1 DBS for TREC and BEIR\nIn Table 1, we compare DBS with zero-shot and\nmanual demonstration selection. Manual curation\nis with demonstrations from Promptagator (Dai\net al., 2023), which uses up to eight demonstra-\ntions depending on the task. 2 The results show\nthat demonstrations often improve re-ranking on\nTREC and BEIR, and furthermore, that our DBS\nis effective for automatic demonstration selection.\nThe improvement over zero-shot can be substantial,\nsuch as the case for TREC 2020, FiQA, and NQ\nwhere using demonstrations leads to more than 3-\npoints improvement in all settings. When zero-shot\noutperforms few-shot, it is only by a small margin\nand often on datasets that require complex retrieval\nsuch as FEVER or HotpotQA. DBS outperforms\n2For example, there are eight, six, and two demonstrations\nfor TREC, FiQA, and Scifact.\n14244\nT19 T20 FiQA Scifact BioASQ FEVER HotpotQA NQ Quora\nBM25 50.60 48.00 23.60 66.50 46.45 75.32 60.27 32.84 78.83\nFlan-T5-XL\n0-shot 61.10 59.90 38.20 70.40 54.34 68.02 72.79 40.26 77.09\nPromptagator 61.00 61.40 43.40 71.90 54.57 69.40 72.36 44.70 84.53\nDBS 1-shot 59.80 62.50 44.10 72.00 54.81 69.42 72.69 44.07 83.66\nDBS 4-shot 61.00 63.00 44.70 72.70 54.32 70.46 72.53 44.58 84.32\nFlan-T5-XXL\n0-shot 61.80 60.30 42.90 73.00 55.11 78.17 72.56 44.93 83.70\nPromptagator 61.90 63.30 47.40 73.80 55.32 78.00 73.53 47.90 85.56\nDBS 1-shot 62.66 63.99 47.60 74.30 55.41 77.62 74.11 48.46 85.31\nDBS 4-shot 63.38 62.93 47.70 74.50 55.71 77.68 73.78 48.41 85.73\nPalm 2-S\n0-shot 55.84 55.55 38.26 74.69 52.31 76.94 71.98 43.33 83.51\nPromptagator 61.24 60.92 48.11 76.89 55.69 78.18 75.43 44.71 85.89\nDBS 1-shot 58.50 60.62 46.99 74.87 53.43 78.07 72.93 42.91 85.52\nDBS 4-shot 61.39 61.20 47.96 76.52 54.34 78.59 75.36 49.84 85.81\nTable 1: nDCG@10 for TREC 2019/2020 and seven BEIR datasets after re-ranking the top-100 documents re-\ntrieved by BM25. Flan-T5-XL/XXL and Palm 2-S perform prompt-based re-ranking, where zero-shot is identical\nto UPR (Sachan et al., 2022). We also use manually selected demonstrations from Promptagator (Dai et al., 2023).\nPromptagator demonstrations in many settings, in-\ncluding by a large margin for TREC 2019 with\nFlan-T5-XXL and NQ with Palm 2-S.\nDemonstration ﬁltering. When using DBS, we\nreturn the top-30 demonstrations and then perform\na lightweight manual ﬁltering to remove demon-\nstrations with incorrect labels. 3 We also remove\nduplicate queries. In the future, ﬁltering for incor-\nrect labels should be easy to automate, and it would\nbe interesting to explore how DBS can be used to\nmine for incorrect annotations.\n5.2 Comparison to Random Selection\nOur ﬁndings thus far show that zero-shot ranking\nis outperformed by demonstration-based ranking\nwith DBS in almost every setting. To understand\nhow much potential there is to further improve\ndemonstration selection, we compare DBS using\nFlan-T5-XXL against 10 randomly selected one-\nshot demonstrations for TREC and BEIR (see Ap-\npendix A.4 for full results). We see that the max\nnDCG from random selection outperforms DBS\nin ﬁve out of nine datasets. This suggests LLM-\nbased ranking may be further improved through\n3Statistics for ﬁltering are in Appendix A.2.\nadvanced selection. In the future it would be help-\nful to see a richer distribution of performance using\nsubstantially more than 10 random demonstrations.\n5.3 DBS with Conditional DQL (CDQL)\nDQL overlooks that variations in demonstration\ndifﬁculty depends on the other demonstrations in\nthe context. To jointly consider the difﬁculty of all\ndemonstrations in the prompt, we propose condi-\ntional demonstration query likelihood (CDQL):\nCDQL(z1, z2, ..., zK) ∝\n∑\ni=1..K\n1\n|q(zi)| logP(q(zi) | z1:i−1, d(zi))\nIn preliminary results, we ﬁnd that Flan-T5-XXL\nwith CDQL improves over DQL on TREC 2019\nand 2020, respectively giving 63.5 vs. 63.4 and\n64.4 vs. 64.0 nDCG.4 To use CDQL we chose 30\ndemonstrations ﬁrst by DQL, ﬁltered for any in-\ncorrect labels, then computed CDQL for each per-\nmutation including four demonstrations and took\nthe lowest CDQL. We leave further exploration of\nCDQL to future work, and believe it may be bene-\nﬁcial when selecting more than one demonstration.\n4Results for CDQL are in Table 3, in the Appendix.\n14245\n0 20 40 60 80 100\nDemonstration ID\n0.78\n0.79\n0.80\n0.81Average of Max Query Similarity\nDBS\nFigure 4: For each demonstration, we compute the\nsemantic similarity between ground-truth and the syn-\nthetic queries. We ﬁrst measure the max similarity by\ndemonstration and query. Then we average this across\nall queries, giving a single scalar per demonstration.\nThe dashed line shows the “average max similarity” for\nthe demonstration chosen using DBS.\n5.4 DBS for Question Generation\nAs an auxiliary evaluation of DBS we study ques-\ntion generation, which plays important roles in dif-\nferent NLP applications (Dai et al., 2023; Bonifacio\net al., 2022; Jeronymo et al., 2023; Nogueira et al.,\n2019; Ma et al., 2021). Using the top-100 passages\nretrieved from BM25 for each query in TREC 2020,\nwe greedily generate with Flan-T5-XXL 100 ques-\ntions per passage using a one-shot prompt and the\n100 random demonstrations from §2. We compare\nthe generated questions from random demonstra-\ntions and the ones from DBS (1-shot). For each\nquery, we compute the maximum cosine similar-\nity between the ground truth and generated ques-\ntions after embedding with Universal Sentence En-\ncoder (Cer et al., 2018). The average of the max\nsimilarity among 100 random demonstrations is\n0.8018 (min=0.7770 and max=0.8150), whereas,\nwe achieve 0.8081 when using DBS (one-shot).\nCompared with the random demonstrations, the\nDBS result ranks 8% highest similarity in the pop-\nulation and is signiﬁcantly greater than the mean\n(p=4e-18) according to two-tailed t-test (Figure 4).\nThese ﬁndings indicate that the demonstrations ef-\nfective for LLM-based scoring of passages are sim-\nilarly effective for generation of questions.\n6 Related Work\nConcurrent with UPR, PromptRank (Khalifa et al.,\n2023) is the most related prior work, using demon-\nstrations to re-rank “document paths” for multihop-\nQA. Details of how they select demonstrations is\nunclear, motivating us to conduct our own study.\nOur difﬁculty-based demonstration selection\n(§4.2) is closely related to active learning (Dagan\nand Argamon, 1995; Roy and McCallum, 2001;\nSettles, 2009). Similarly, Diao et al. (2023) mea-\nsure uncertainty with generation instead of scoring.\nZhang et al. (2022) formulate demonstration selec-\ntion as a reinforcement learning problem. Rubin\net al. (2022) use LLM-scoring to ﬁnd hard nega-\ntives for their trained demonstration retriever. Oth-\ners explore demonstration ordering (Lu et al., 2022)\nand joint selection (Drozdov et al., 2023; Levy\net al., 2023; Agrawal et al., 2023; Ye et al., 2023).\nConcurrent to our work, Li and Qiu (2023) perform\nmultiple rounds of hill climbing to ﬁnd groups of\ndemonstrations that perform well according to a\nvalidation set. In contrast, DBS selects demonstra-\ntions directly and does not rely on validation.\nDiscriminative methods are widely used in super-\nvised ranking (Zhuang et al., 2023; Nogueira dos\nSantos et al., 2020; Hui et al., 2022). Listwise\nprompting is an alternative to query likelihood, but\nrequires a sliding window strategy as not all docu-\nments ﬁt in the context (Ma et al., 2023; Sun et al.,\n2023). Rather than query likelihood, HyDE (Gao\net al., 2023) achieves zero-shot ranking through\ndocument generation, which we hypothesize would\nbe improved through demonstrations. PaRaDe is\nbounded by the ﬁrst stage BM25 retrieval, and it\nmay be fruitful to explore approaches that align\nﬁrst stage retrieval with our demonstration-based\napproach (Yadav et al., 2022).\n7 Conclusion\nIn this work we present Passage Ranking with\nDemonstrations (PaRaDe), an extensive study on\nthe topic of using demonstrations to improve re-\nranking performance of LLMs. We show the chal-\nlenges of applying demonstrations effectively, and\nthat performance heavily relies on selecting “good”\ndemonstrations. We propose a simple yet effective\nselection method, named difﬁculty-based selection\n(DBS), and conﬁrm its effectiveness in both re-\nranking using query likelihood scoring and query\ngeneration tasks. For future work, we plan to\ncombine difﬁculty-based selection with similarity-\nbased selection as an effort to further improve\nthe robustness and effectiveness of the selected\ndemonstrations, and extend DBS to other ranking\nparadigms (Qin et al., 2023; Sun et al., 2023).\n14246\nAcknowledgements\nWe thank Tal Schuster for their detailed comments\non earlier versions of this manuscript. We are grate-\nful to Vinh Tran for insightful discussions regard-\ning the Flan family of models and their in-context\nlearning capabilities.\nLimitations\nOne limitation of DBS is when naively used to se-\nlect multiple demonstrations, the demonstrations\nthat may appear challenging at ﬁrst may become\nrelatively easy once the other demonstrations have\nbeen processed in the prompt context. We par-\ntially address this by replacing DQL in DBS with\nCDQL (§5.3) so that demonstrations are selected\njointly rather than scored individually. Our CDQL\napproach selects a high scoring subset from the ini-\ntial list provided by DQL. More challenging com-\nbinations of demonstrations may be available by\nsearching the entire candidate set, but exact search\nis computationally prohibitive. Another limitation\nis that we only incorporate positive demonstrations,\nand for retrieval, training with hard negatives is\noften beneﬁcial to model performance. We hypoth-\nesize a DBS-like algorithm can be used to ﬁnd hard\nnegatives, but it may be important to add signals\ndistinguishing positive from negative demonstra-\ntion when using LLMs with query likelihood for\nranking.\nReferences\nSweta Agrawal, Chunting Zhou, Mike Lewis, Luke\nZettlemoyer, and Marjan Ghazvininejad. 2023. In-\ncontext examples selection for machine translation.\nIn Findings of the Association for Computational\nLinguistics: ACL 2023 , pages 8857–8873, Toronto,\nCanada. Association for Computational Linguistics.\nEkin Akyürek, Dale Schuurmans, Jacob Andreas,\nTengyu Ma, and Denny Zhou. 2023. What learn-\ning algorithm is in-context learning? investigations\nwith linear models. In The Eleventh International\nConference on Learning Representations.\nSoumya Basu, Ankit Singh Rawat, and Manzil Zaheer.\n2023. A Statistical Perspective on Retrieval-Based\nModels. In Proceedings of the 40th International\nConference on Machine Learning , volume 202 of\nProceedings of Machine Learning Research , pages\n1852–1886. PMLR.\nLuiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and\nRodrigo Nogueira. 2022. InPars: Unsupervised\ndataset generation for information retrieval. In Pro-\nceedings of the 45th International ACM SIGIR Con-\nference on Research and Development in Informa-\ntion Retrieval , SIGIR ’22, page 2387–2392, New\nYork, NY , USA. Association for Computing Machin-\nery.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. arXiv preprint arXiv: 2005.14165.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St. John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\nBrian Strope, and Ray Kurzweil. 2018. Universal\nSentence Encoder for English. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations,\npages 169–174, Brussels, Belgium. Association for\nComputational Linguistics.\nHaw-Shiuan Chang, Erik G. Learned-Miller, and An-\ndrew McCallum. 2017. Active Bias: Training More\nAccurate Neural Networks by Emphasizing High\nVariance Samples. In Neural Information Process-\ning Systems.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robin-\nson, Dasha Valter, Sharan Narang, Gaurav Mishra,\nAdams Yu, Vincent Zhao, Yanping Huang, Andrew\nDai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean,\nJacob Devlin, Adam Roberts, Denny Zhou, Quoc V .\nLe, and Jason Wei. 2022. Scaling instruction-\nﬁnetuned language models. arXiv preprint arXiv:\n2210.11416.\nIdo Dagan and Shlomo Engelson Argamon. 1995.\nCommittee-based sampling for training probabilistic\nclassiﬁers. In International Conference on Machine\nLearning.\nZhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo\nNi, Jing Lu, Anton Bakalov, Kelvin Guu, Keith\nHall, and Ming-Wei Chang. 2023. Promptagator:\nFew-shot dense retrieval from 8 examples. In The\nEleventh International Conference on Learning Rep-\nresentations.\nShizhe Diao, Pengcheng Wang, Yong Lin, and Tong\nZhang. 2023. Active prompting with chain-of-\nthought for large language models. arXiv preprint\narXiv: 2302.12246.\n14247\nAndrew Drozdov, Nathanael Schärli, Ekin Akyürek,\nNathan Scales, Xinying Song, Xinyun Chen, Olivier\nBousquet, and Denny Zhou. 2023. Compositional\nsemantic parsing with large language models. In\nThe Eleventh International Conference on Learning\nRepresentations.\nLuyu Gao, Xueguang Ma, Jimmy Lin, and Jamie\nCallan. 2023. Precise zero-shot dense retrieval with-\nout relevance labels. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1762–\n1777, Toronto, Canada. Association for Computa-\ntional Linguistics.\nRohan Anil Google, Andrew M. Dai, Orhan Fi-\nrat, Melvin Johnson, Dmitry Lepikhin, Alexandre\nPassos, Siamak Shakeri, Emanuel Taropa, Paige\nBailey, Zhifeng Chen, Eric Chu, Jonathan H.\nClark, Laurent El Shafey, Yanping Huang, Kathy\nMeier-Hellstern, Gaurav Mishra, Erica Moreira,\nMark Omernick, Kevin Robinson, Sebastian Ruder,\nYi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,\nGustavo Hernandez Abrego, Junwhan Ahn, Ja-\ncob Austin, Paul Barham, Jan Botha, James Brad-\nbury, Siddhartha Brahma, Kevin Brooks, Michele\nCatasta, Yong Cheng, Colin Cherry, Christopher A.\nChoquette-Choo, Aakanksha Chowdhery, Clément\nCrepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev,\nJacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad\nFeinberg, Fangxiaoyu Feng, Vlad Fienber, Markus\nFreitag, Xavier Garcia, Sebastian Gehrmann, Lu-\ncas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi\nHashemi, Le Hou, Joshua Howland, Andrea Hu, Jef-\nfrey Hui, Jeremy Hurwitz, Michael Isard, Abe Itty-\ncheriah, Matthew Jagielski, Wenhao Jia, Kathleen\nKenealy, Maxim Krikun, Sneha Kudugunta, Chang\nLan, Katherine Lee, Benjamin Lee, Eric Li, Music\nLi, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim,\nHanzhao Lin, Zhongtao Liu, Frederick Liu, Mar-\ncello Maggioni, Aroma Mahendru, Joshua Maynez,\nVedant Misra, Maysam Moussalem, Zachary Nado,\nJohn Nham, Eric Ni, Andrew Nystrom, Alicia\nParrish, Marie Pellat, Martin Polacek, Alex Polo-\nzov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan\nRichter, Parker Riley, Alex Castro Ros, Aurko Roy,\nBrennan Saeta, Rajkumar Samuel, Renee Shelby,\nAmbrose Slone, Daniel Smilkov, David R. So,\nDaniel Sohn, Simon Tokumine, Dasha Valter, Vi-\njay Vasudevan, Kiran V odrahalli, Xuezhi Wang, Pi-\ndong Wang, Zirui Wang, Tao Wang, John Wiet-\ning, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven\nZheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav\nPetrov, and Yonghui Wu. 2023. Palm 2 technical re-\nport.\nKai Hui, Honglei Zhuang, Tao Chen, Zhen Qin,\nJing Lu, Dara Bahri, Ji Ma, Jai Gupta, Cicero\nNogueira dos Santos, Yi Tay, and Donald Metzler.\n2022. ED2LM: Encoder-decoder to language model\nfor faster document re-ranking inference. In Find-\nings of the Association for Computational Linguis-\ntics: ACL 2022 , pages 3747–3758, Dublin, Ireland.\nAssociation for Computational Linguistics.\nVitor Jeronymo, Luiz Bonifacio, Hugo Abonizio,\nMarzieh Fadaee, Roberto Lotufo, Jakub Zavrel,\nand Rodrigo Nogueira. 2023. InPars-v2: Large\nLanguage Models as Efﬁcient Dataset Generators\nfor Information Retrieval. arXiv preprint arXiv:\n2301.01820.\nMuhammad Khalifa, Lajanugen Logeswaran, Moontae\nLee, Honglak Lee, and Lu Wang. 2023. Few-shot\nReranking for Multi-hop QA via Language Model\nPrompting. arXiv preprint arXiv: 2205.12650.\nItay Levy, Ben Bogin, and Jonathan Berant. 2023. Di-\nverse demonstrations improve in-context composi-\ntional generalization. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1401–\n1422, Toronto, Canada. Association for Computa-\ntional Linguistics.\nXiaonan Li and Xipeng Qiu. 2023. Finding support\nexamples for in-context learning. In Proceedings of\nthe 2023 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP).\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-\nHong Yang, Ronak Pradeep, and Rodrigo Nogueira.\n2021. Pyserini: A Python Toolkit for Reproducible\nInformation Retrieval Research with Sparse and\nDense Representations. In Proceedings of the 44th\nAnnual International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval\n(SIGIR 2021), pages 2356–2362.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian\nRiedel, and Pontus Stenetorp. 2022. Fantastically\nOrdered Prompts and Where to Find Them: Over-\ncoming Few-Shot Prompt Order Sensitivity. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 8086–8098, Dublin, Ireland. Associ-\nation for Computational Linguistics.\nMan Luo, Xin Xu, Zhuyun Dai, Panupong Pasupat,\nMehran Kazemi, Chitta Baral, Vaiva Imbrasaite,\nand Vincent Zhao. 2023. Dr.ICL: Demonstration-\nRetrieved In-context Learning. arXiv preprint arXiv:\n2305.14128.\nJi Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and\nRyan McDonald. 2021. Zero-shot neural passage re-\ntrieval via domain-targeted synthetic question gener-\nation. In Proceedings of the 16th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Main Volume, pages 1075–1088,\nOnline. Association for Computational Linguistics.\nXueguang Ma, Xinyu Crystina Zhang, Ronak Pradeep,\nand Jimmy Lin. 2023. RankVicuna: Zero-Shot List-\nwise Document Reranking with a Large Language\nModel. arXiv preprint arXiv: 2305.02156.\n14248\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2022. MetaICL: Learning to learn\nin context. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2791–2809, Seattle, United States.\nAssociation for Computational Linguistics.\nRodrigo Nogueira, Jimmy Lin, and AI Epistemic.\n2019. From doc2query to docTTTTTquery. Online\npreprint.\nCicero Nogueira dos Santos, Xiaofei Ma, Ramesh Nal-\nlapati, Zhiheng Huang, and Bing Xiang. 2020. Be-\nyond [CLS] through ranking by generation. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 1722–1727, Online. Association for Computa-\ntional Linguistics.\nJay M. Ponte and W. Bruce Croft. 1998. A Language\nModeling Approach to Information Retrieval. In\nProceedings of the 21st Annual International ACM\nSIGIR Conference on Research and Development in\nInformation Retrieval , SIGIR ’98, page 275–281,\nNew York, NY , USA. Association for Computing\nMachinery.\nZhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang,\nJunru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, Don-\nald Metzler, Xuanhui Wang, et al. 2023. Large lan-\nguage models are effective text rankers with pair-\nwise ranking prompting. arXiv preprint arXiv:\n2306.17563.\nNicholas Roy and Andrew McCallum. 2001. Toward\noptimal active learning through monte carlo estima-\ntion of error reduction. In International Conference\non Machine Learning.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2655–2671, Seattle, United States.\nAssociation for Computational Linguistics.\nDevendra Sachan, Mike Lewis, Mandar Joshi, Armen\nAghajanyan, Wen-tau Yih, Joelle Pineau, and Luke\nZettlemoyer. 2022. Improving passage retrieval\nwith zero-shot question generation. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 3781–3797,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nBurr Settles. 2009. Active learning literature survey.\nAbhinav Shrivastava, Abhinav Kumar Gupta, and\nRoss B. Girshick. 2016. Training region-based\nobject detectors with online hard example mining.\n2016 IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 761–769.\nWeiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren,\nDawei Yin, and Zhaochun Ren. 2023. Is Chat-\nGPT Good at Search? Investigating Large Language\nModels as Re-Ranking Agent. arXiv preprint arXiv:\n2304.09542.\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. BEIR:\nA heterogeneous benchmark for zero-shot evalua-\ntion of information retrieval models. In Thirty-ﬁfth\nConference on Neural Information Processing Sys-\ntems Datasets and Benchmarks Track (Round 2).\nNishant Yadav, Nicholas Monath, Rico Angell, Manzil\nZaheer, and Andrew McCallum. 2022. Efﬁcient\nnearest neighbor search for cross-encoder models\nusing matrix factorization. In Proceedings of the\n2022 Conference on Empirical Methods in Natu-\nral Language Processing , pages 2171–2194, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nJiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and\nLingpeng Kong. 2023. Compositional exemplars for\nin-context learning. In International Conference on\nMachine Learning.\nYiming Zhang, Shi Feng, and Chenhao Tan. 2022. Ac-\ntive example selection for in-context learning. In\nProceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing , pages\n9134–9148, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nHonglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui,\nJi Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, and Mike\nBendersky. 2023. RankT5: Fine-Tuning T5 for Text\nRanking with Ranking Losses. In Proc. of the 46th\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval (SIGIR).\n14249\nA Appendix\nA.1 Prompt Format\nThe instructions we use are in Table 4. We also\ninclude a short preﬁx indicating the start of docu-\nment or query, shown in Table 5. These are used\nonce for the test query and document, and dupli-\ncated for any demonstrations in the prompt. For\nfew-shot the prompt includes an instruction, one\nor more demonstrations, and the test data. Scores\nfor ranking are computed only on the query. For\nzero-shot, no demonstrations are included.\nA.2 Statistics for Demonstration Filtering\nTable 2 shows statistics for demonstration ﬁltering,\nincluding the ranks of selected demonstrations and\nthe number of demonstrations that are skipped per\ndataset. Demonstrations are skipped if they are\nincorrectly labeled or duplicates of already selected\ndemonstrations. The demonstrations are selected\nfrom the training data, and MSMarco is used for\nTREC 2019 and 2020. In the case of FEVER, we\nneeded to truncate long demonstrations so they\nwould ﬁt in the prompt. Sometimes this would\ninadvertently make labels incorrect as necessary\ninformation resided in the removed text. We always\nperform any preprocessing and truncation before\nrunning demonstration selection.\nA.3 Results with CDQL\nTable 3 includes results using conditional demon-\nstration query likelihood (CDQL). These are only\npreliminary results, and we believe that CDQL\nshould be an effective alternative to DQL when\nselecting more than one demonstration.\nA.4 Results with Random Selection\nIn Table 6 we compare DBS one-shot with random\none-shot demonstrations when using Flan-T5-XXL.\nRandom performance is aggregated across 10 ran-\ndomly selected demonstrations, and we show the\nmean, standard deviation, minimum, and maximum\nvalues. The results indicate there is further oppor-\ntunity to improve demonstration selection, and also\nthat demonstration selection is a challenging task.\nDataset Ranks of Selected No. Skipped\nMSMarco 1,2,3,4 0\nFiQA 4,5,6,7 3\nScifact 5,6,7,8 3\nBioASQ 1,3,5,18 5\nFEVER 5,8,16,22 17\nHotpotQA 2,3,6,8 4\nNQ 3,4,5,8 4\nQuora 1,2,4,5 1\nTable 2: Statistics for demonstration ﬁltering.\nT19 T20\nBM25 50.6 48.0\nFlan-T5-XXL\n0-shot (UPR) 61.8 60.3\nPromptagator 61.9 63.3\nDBS 1-shot (DQL) 62.7 64.0\nDBS 4-shot (DQL) 63.4 62.9\nDBS 4-shot (CDQL) 63.5 64.4\nTable 3: nDCG@10 on TREC 2019 and 2020 when\nusing Flan-T5-XXL. We compare zero demonstrations,\nmanual curation (Promptagator), and automatic selec-\ntion with DBS using DQL or CDQL.\n14250\nDataset Instruction\nTREC 2019 [web] I will check whether what you said could answer my question.\nTREC 2020 [web] I will check whether what you said could answer my question.\nBEIR FiQA [web] I will check if what you said could verify my question.\nBEIR Scifact [web] I will check if the argument you said could verify my scientiﬁc claim.\nTable 4: The instructions for zero-shot and few-shot prompts. Zero-shot prompts include only the instruction and\ntest document, with scoring on the test query. Few-shot prompts also include demonstrations. The same prompts\nare used for both query likelihood and question generation, although question generation excludes the test query.\nDataset Query-Document Template\nTREC 2019 You said: DOCUMENT <newline> I googled: QUERY\nTREC 2020 You said: DOCUMENT <newline> I googled: QUERY\nBEIR FiQA You said: DOCUMENT <newline> I googled: QUERY\nBEIR Scifact Argument: DOCUMENT <newline> My scientiﬁc claim: QUERY\nTable 5: The prompt template for zero-shot and few-shot prompts. Zero-shot prompts include only the instruction\nand test document, with scoring on the test query. Few-shot prompts also include demonstrations. The same\nprompts are used for both query likelihood and question generation, although question generation excludes the test\nquery.\n14251\nT19 T20 FiQA Scifact BioASQ Fever HotpotQA NQ Quora\nBM25 50.58 47.96 23.61 66.47 46.45 75.32 60.27 32.84 78.83\n0-shot 61.80 60.30 42.90 73.00 55.11 78.17 72.56 44.93 83.70\nPromptagator 61.90 63.30 47.40 73.80 55.32 78.00 73.53 47.90 85.56\nDBS 1-shot 62.66 63.99 47.60 74.30 55.41 77.62 74.11 48.46 85.31\nDBS 4-shot 63.38 62.93 47.70 74.50 55.71 77.68 73.78 48.41 85.73\nDBS 1-shot 62.66 63.99 47.60 74.30 55.41 77.62 74.11 48.46 85.31\nR 1-shot (avg) 62.02 62.84 48.58 75.58 55.54 80.41 71.98 49.28 84.71\nR 1-shot (std) 0.47 0.44 0.27 0.37 0.23 1.39 0.24 0.38 0.35\nR 1-shot (min) 60.92 62.19 48.05 74.97 55.34 77.64 71.56 48.61 83.78\nR 1-shot (max) 62.80 63.50 48.93 76.21 56.03 81.93 72.31 49.71 85.05\nTable 6: nDCG@10 on BEIR datasets, using all queries when using Flan-T5-XXL. We compare DBS 1-shot\nwith random (R) 1-shot. Random performance is aggregated across 10 randomly selected demonstrations, and\nwe show the mean, standard deviation, minimum, and maximum values. DBS 1-shot is underlined if it is greater\nthan the maximum random 1-shot and vice versa. When maximum random 1-shot outperforms DBS 1-shot, this\nsuggests there is further opportunity to improve demonstration selection, and also that demonstration selection is a\nchallenging task.\n14252",
  "topic": "Parade",
  "concepts": [
    {
      "name": "Parade",
      "score": 0.5985775589942932
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.5656799674034119
    },
    {
      "name": "Computer science",
      "score": 0.33347904682159424
    },
    {
      "name": "History",
      "score": 0.33098965883255005
    },
    {
      "name": "Art history",
      "score": 0.28886014223098755
    },
    {
      "name": "Artificial intelligence",
      "score": 0.27775758504867554
    }
  ]
}