{
  "title": "Empowering News Recommendation with Pre-trained Language Models",
  "url": "https://openalex.org/W3153647376",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5001967239",
      "name": "Chuhan Wu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5076423724",
      "name": "Fangzhao Wu",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5035067940",
      "name": "Tao Qi",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5100768896",
      "name": "Yongfeng Huang",
      "affiliations": [
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2951001079",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W3034236656",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2950421571",
    "https://openalex.org/W2955854688",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2971207485",
    "https://openalex.org/W2470673105",
    "https://openalex.org/W2964536660",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2970793364",
    "https://openalex.org/W2607303184",
    "https://openalex.org/W2963869731",
    "https://openalex.org/W3042711927",
    "https://openalex.org/W3035725536",
    "https://openalex.org/W2950416834",
    "https://openalex.org/W2742272831",
    "https://openalex.org/W3117150731",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963626623"
  ],
  "abstract": "Personalized news recommendation is an essential technique for online news services. News articles usually contain rich textual content, and accurate news modeling is important for personalized news recommendation. Existing news recommendation methods mainly model news texts based on traditional text modeling methods, which is not optimal for mining the deep semantic information in news texts. Pre-trained language models (PLMs) are powerful for natural language understanding, which has the potential for better news modeling. However, there is no public report that show PLMs have been applied to news recommendation. In this paper, we report our work on exploiting pre-trained language models to empower news recommendation. Offline experimental results on both monolingual and multilingual news recommendation datasets show that leveraging PLMs for news modeling can effectively improve the performance of news recommendation. Our PLM-empowered news recommendation models have been deployed to the Microsoft News platform, and achieved significant gains in terms of both click and pageview in both English-speaking and global markets.",
  "full_text": "Empowering News Recommendation with\nPre-trained Language Models\nChuhan Wu1, Fangzhao Wu2, Tao Qi1, Yongfeng Huang1\n1Department of Electronic Engineering & BNRist, Tsinghua University, Beijing 100084\n2Microsoft Research Asia, Beijing 100080, China\n{wuchuhan15,wufangzhao,taoqi.qt}@gmail.com,yfhuang@tsinghua.edu.cn\nABSTRACT\nPersonalized news recommendation is an essential technique for\nonline news services. News articles usually contain rich textual\ncontent, and accurate news modeling is important for personalized\nnews recommendation. Existing news recommendation methods\nmainly model news texts based on traditional text modeling meth-\nods, which is not optimal for mining the deep semantic information\nin news texts. Pre-trained language models (PLMs) are powerful\nfor natural language understanding, which has the potential for\nbetter news modeling. However, there is no public report that show\nPLMs have been applied to news recommendation. In this paper, we\nreport our work on exploiting pre-trained language models to em-\npower news recommendation. Offline experimental results on both\nmonolingual and multilingual news recommendation datasets show\nthat leveraging PLMs for news modeling can effectively improve\nthe performance of news recommendation. Our PLM-empowered\nnews recommendation models have been deployed to the Microsoft\nNews platform, and achieved significant gains in terms of both click\nand pageview in both English-speaking and global markets.\nCCS CONCEPTS\nâ€¢ Information systems â†’Recommender systems; â€¢ Comput-\ning methodologies â†’Natural language processing ;\nKEYWORDS\nNews recommendation, pre-trained language model\nACM Reference Format:\nChuhan Wu1, Fangzhao Wu2, Tao Qi1, Yongfeng Huang1. 2021. Empowering\nNews Recommendation with Pre-trained Language Models. InProceedings of\nThe 44th International ACM SIGIR Conference on Research and Development\nin Information Retrieval (SIGIR 2021),Jennifer B. Sartor, Theo Dâ€™Hondt,\nand Wolfgang De Meuter (Eds.). ACM, New York, NY, USA, Article 4, 5 pages.\nhttps://doi.org/10.475/123_4\n1 INTRODUCTION\nNews recommendation techniques have played critical roles in\nmany online news platforms to alleviate the information overload\nof users [15]. News modeling is an important step in news recom-\nmendation, because it is a core technique to understand the content\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nSIGIR 2021, July 2021, Online\nÂ© 2021 Copyright held by the owner/author(s).\nACM ISBN 123-4567-24-567/08/06.\nhttps://doi.org/10.475/123_4\nof candidate news and a prerequisite for inferring user interests\nfrom clicked news. Since news articles usually have rich textual in-\nformation, news texts modeling is the key for understanding news\ncontent for news recommendation. Existing news recommenda-\ntion methods usually model news texts based on traditional NLP\nmodels [15, 19, 20, 22, 23, 25, 26]. For example, Wang et al . [20]\nproposed to use a knowledge-aware CNN network to learn news\nrepresentations from embeddings of words and entities in news\ntitle. Wu et al. [23] proposed to use multi-head self-attention net-\nwork to learn news representations from news titles. However, it is\ndifficult for these shallow models to understand the deep semantic\ninformation in news texts [18]. In addition, their models are only\nlearned from the supervisions in the news recommendation task,\nwhich may not be optimal for capturing the semantic information.\nPre-trained language models (PLMs) have achieved great success\nin NLP due to their strong ability in text modeling [2, 5, 6, 11, 12,\n14, 28]. Different from traditional models that are usually directly\ntrained with labeled data in specific tasks, PLMs are usually first\npre-trained on a large unlabeled corpus via self-supervision to\nencode universal text information [ 5]. Thus, PLMs can usually\nprovide better initial point for finetuning in downstream tasks [16].\nIn addition, different from many traditional NLP methods with\nshallow models [9, 10, 24, 29], PLMs are usually much deeper with\na huge number of parameters. For example, the BERT-Base model\ncontains 12 Transformer layers and up to 109M parameters [ 5].\nThus, PLMs may have greater ability in modeling the complicated\ncontextual information in news text, which have the potentials to\nimprove news text modeling for news recommendation.\nIn this paper, we present our work on empowering large-scale\nnews recommendation with pre-trained language models.1 Differ-\nent from existing news recommendation methods that use shallow\nNLP models for news modeling, we explore to model news with\npre-trained language models and finetune them with the news rec-\nommendation task. Offline experiments on real-world English and\nmultilingual news recommendation datasets validate that incorpo-\nrating PLMs into news modeling can consistently improve the news\nrecommendation performance. In addition, our PLM-empowered\nnews recommendation models have been deployed to the Microsoft\nNews platform.2 To our best knowledge, this is the first reported ef-\nfort to empower large-scale news recommender systems with PLMs.\nThe online flight experiments show that our PLM-empowered news\nrecommendation models achieved 8.53% click and 2.63% pageview\ngains in English-speaking markets, and 10.68% click and 6.04%\npageview gains in other 43 global markets.\n1Source codes will be available at https://github.com/wuch15/PLM4NewsRec.\n2https://microsoftnews.msn.com\narXiv:2104.07413v1  [cs.IR]  15 Apr 2021\nCandidate News Clicked News\nâ€¦\nğ’‰ğ’‰ğ‘ğ‘\nğ’‰ğ’‰1 ğ’‰ğ’‰2 ğ’‰ğ’‰ğ‘ğ‘\nğ·ğ·1 ğ·ğ·ğ‘‡ğ‘‡ğ·ğ·ğ‘ğ‘\nCandidate News\nEmbedding\nNews \nEncoder\nUser Encoder\nğ’–ğ’– User \nEmbedding\nğ·ğ·2\nâ€¦\nâ€¦\nï¿½ğ‘¦ğ‘¦\nClick Prediction\nClick Score\nNews \nEncoder\nNews \nEncoder\nNews \nEncoder\nFigure 1: A common framework of news recommendation.\n2 METHODOLOGY\nIn this section, we introduce the details of PLM-empowered news\nrecommendation. We first introduce the general news recommen-\ndation model framework, and then introduce how to incorporate\nPLMs into this framework to empower news modeling.\n2.1 General News Recommendation\nFramework\nThe general framework of news recommendation used in many\nexisting methods [1, 15, 21, 23] is shown in Fig. 1. The core compo-\nnents in this framework include a news encoder that aims to learn\nthe embeddings of news from their texts, a user encoder that learns\nuser embedding from the embeddings of clicked news, and a click\nprediction module that computes personalized click score for news\nranking based on the relevance between user embedding and can-\ndidate news embedding. We assume a user hasğ‘‡ historical clicked\nnews, which are denoted as [ğ·1,ğ·2,...,ğ· ğ‘‡ ]. The news encoder pro-\ncesses these clicked news of a user and each candidate news ğ·ğ‘\nto obtain their embeddings, which are denoted as [h1,h2,..., hğ‘‡ ]\nand hğ‘ , respectively. It can be implemented by various NLP models,\nsuch as CNN [10] and self-attention [18]. The user encoder receives\nthe sequence of clicked news embeddings as input, and outputs a\nuser embedding u that summarizes user interest information. It can\nalso be implemented by various models, such as the GRU network\nused in [15], the attention network used in [21] and the combina-\ntion of multi-head self-attention and additive attention networks\nused in [23]. The click prediction module takes the user embedding\nu and hğ‘ as inputs, and compute the click score Ë†ğ‘¦ by evaluating\ntheir relevance. It can also be implemented by various methods\nsuch as inner product [15], neural network [20] and factorization\nmachine [7].\n2.2 PLM Empowered News Recommendation\nNext, we introduce the framework of PLM empowered news rec-\nommendation, as shown in Fig. 2. We instantiate the news encoder\nwith a pre-trained language model to capture the deep contexts in\nnews texts and an attention network to pool the output of PLM. We\ndenote an input news text with ğ‘€ tokens as [ğ‘¤1,ğ‘¤2,...,ğ‘¤ ğ‘€ ]. The\nPLM converts each token into its embedding, and then learns the\nhidden representations of words through several Transformer [18]\nCandidate News Clicked News\nâ€¦\nğ’‰ğ’‰ğ‘ğ‘\nğ’‰ğ’‰1 ğ’‰ğ’‰2 ğ’‰ğ’‰ğ‘ğ‘\nğ·ğ·1 ğ·ğ·ğ‘‡ğ‘‡ğ·ğ·ğ‘ğ‘\nCandidate News\nEmbedding\nâ€¦\nâ€¦PLM\nAttention\nâ€¦\nâ€¦PLM\nAttention\nâ€¦\nâ€¦PLM\nAttention\nâ€¦\nUser Encoder\nğ’–ğ’– User Embedding\nğ·ğ·2\nâ€¦\nâ€¦PLM\nAttention â€¦\nâ€¦\nï¿½ğ‘¦ğ‘¦\nClick Prediction\nClick Score\nFigure 2: The framework of PLM empowered news recom-\nmendation.\nlayers. We denote the hidden token representation sequence as\n[r1,r2,..., rğ‘€ ]. We use an attention [29] network to summarize the\nhidden token representations into a unified news embedding. The\nnews embeddings learned by the PLM and attention network are\nfurther used for user modeling and candidate matching.\n2.3 Model Training\nFollowing [22, 23], we also use negative sampling techniques to\nbuild labeled samples from raw news impression logs, and we use\nthe cross-entropy loss function for model training by classifying\nwhich candidate news is clicked. By optimizing the loss function\nvia backward-propagation, the parameters in the recommendation\nmodel and PLMs can be tuned for the news recommendation task.\n3 EXPERIMENTS\n3.1 Datasets and Experimental Settings\nOur offline experiments are conducted on two real-world datasets.\nThe first one is MIND [27], which is an English dataset for mono-\nlingual news recommendation. It contains the news click logs of 1\nmillion users on Microsoft News in six weeks.3 The second one is\na multilingual news recommendation dataset (denoted as Multilin-\ngual) collected by ourselves on MSN News platform from Dec. 1,\n2020 to Jan. 14, 2021. It contains users from 7 countries with differ-\nent language usage, and their market language codes are EN-US,\nDE-DE, FR-FR, IT-IT, JA-JP, ES-ES and KO-KR, respectively. We\nrandomly sample 200,000 impression logs in each market. The logs\nin the last week are used for test and the rest are used for training\nand validation (9:1 split). The detailed statistics of the two datasets\nare shown in Table 1.\nTable 1: Detailed statistics of the two datasets.\nMIND Multilingual\n# Users 1,000,000 1,392,531\n# News 161,013 4,378,487\n# Impressions 15,777,377 1,400,000\n# Click Behaviors 24,155,470 1,814,927\n3More details are on https://msnews.github.io/.\nIn our experiments, we used the â€œBaseâ€ version of different pre-\ntrained language models if not specially mentioned. We finetuned\nthe last two Transformer layers because we find there is only a very\nsmall performance difference between finetuning all layers and\nthe last two layers. Following [27], we used the titles of news for\nnews modeling. We used Adam [3] as the optimization algorithm\nand the learning rate was 1e-5. The batch size was 128. 4 These\nhyperparameters are developed on the validation sets. We used\naverage AUC, MRR, nDCG@5 and nDCG@10 over all impressions\nas the performance metrics. We repeated each experiment 5 times\nindependently and reported the average performance.\n3.2 Offline Performance Evaluation\nWe first compare the performance of several methods on the MIND\ndataset to validate the effectiveness of PLM-based models in mono-\nlingual news recommendation. We compared several recent news\nrecommendation methods including EBNR [15], NAML [21], NPA [22],\nLSTUR [1], NRMS [23] and their variants empowered by different\npre-trained language models, including BERT [5], RoBERTa [14]\nand UniLM [2]. The results are shown in Table 2. Referring to this\ntable, we find that incorporating pre-trained language models can\nconsistently improve the performance of basic models. 5 This is\nbecause pre-trained language models have stronger text model-\ning ability than the shallow models learned from scratch in the\nnews recommendation. In addition, we find that the models based\non RoBERTa are better than those based on BERT. This may be\nbecause RoBERTa has better hyperparameter settings than BERT\nand is pre-trained on larger corpus for a longer time. Besides, the\nmodels based on UniLM achieve the best performance. This may be\ndue to UniLM can exploit the self-supervision information in both\ntext understanding and generation tasks, which can help learn a\nhigher-quality PLM.\nIn addition, we conduct experiments on the Multilingual dataset\nto validate the effectiveness of PLMs in multilingual news rec-\nommendation. We compare the performance of EBNR, NAML,\nNPA, LSTUR and NRMS with different multilingual text modeling\nmethods, including: (1) MUSE [13], using modularizing unsuper-\nvised sense embeddings; (2) Unicoder [ 8], a universal language\nencoder pre-trained by cross-lingual self-supervision tasks; and\n(3) InfoXLM [4], a contrastively pre-trained cross-lingual language\nmodel based on information-theoretic framework. In these methods,\nfollowing [8] we mix up the training data in different languages.\nIn addition, we also compare the performance of independently\nlearned monolingual models based on MUSE for each market (de-\nnoted as Single). The results of different methods in terms of AUC\nare shown in Table 3. We find that multilingual models usually\noutperform the independently learned monolingual models. This\nmay be because different languages usually have some inherent\nrelatedness and users in different countries may also have some sim-\nilar interests. Thus, jointly training models with multilingual data\ncan help learn a more accurate recommendation model. It also pro-\nvides the potential to use a unified recommendation model to serve\nusers in different countries with diverse language usage (e.g., Indo-\nEuropean and Altaic), which can greatly reduce the computation\n4We used 4 GPUs in parallel and the batch size on each of them was 32.\n5The results of t-test show the improvements are significant (ğ‘ < 0.001).\nTable 2: Performance of different methods on MIND.\nMethods AUC MRR nDCG@5 nDCG@10\nEBNR 66.54 32.43 35.38 40.09\nEBNR-BERT 69.56 34.77 38.04 43.72\nEBNR-RoBERTa 69.70 34.84 38.21 43.88\nEBNR-UniLM 70.56 35.31 38.65 44.32\nNAML 67.78 33.24 36.19 41.95\nNAML-BERT 69.42 34.66 37.91 43.65\nNAML-RoBERTa 69.60 34.78 38.13 43.79\nNAML-UniLM 70.50 35.26 38.60 44.27\nNPA 67.87 33.20 36.26 42.03\nNPA-BERT 69.50 34.72 37.96 43.72\nNPA-RoBERTa 69.64 34.81 38.14 43.82\nNPA-UniLM 70.52 35.29 38.63 44.29\nLSTUR 68.04 33.31 36.28 42.10\nLSTUR-BERT 69.49 34.72 37.97 43.70\nLSTUR-RoBERTa 69.62 34.80 38.15 43.79\nLSTUR-UniLM 70.56 35.29 38.67 44.31\nNRMS 68.18 33.29 36.31 42.20\nNRMS-BERT 69.50 34.75 37.99 43.72\nNRMS-RoBERTa 69.56 34.81 38.05 43.79\nNRMS-UniLM 70.64 35.39 38.71 44.38\nTable 3: Performance of different methods on Multilingual.\nMethods EN-US DE-DE FR-FR IT-IT JA-JP ES-ES KO-KR\nEBNR-Single 62.08 59.94 61.66 60.27 61.57 58.30 63.53\nEBNR-MUSE 62.26 60.19 61.75 60.44 61.74 57.53 63.78\nEBNR-Unicoder 63.35 61.44 62.34 61.18 62.76 58.70 64.80\nEBNR-InfoXLM 64.29 62.03 62.97 61.98 63.34 59.33 65.58\nNAML-Single 62.05 59.89 61.56 60.21 61.54 58.21 63.5\nNAML-MUSE 62.17 60.17 61.71 60.4 61.69 57.46 63.73\nNAML-Unicoder 63.3 61.37 62.32 61.16 62.74 58.61 64.77\nNAML-InfoXLM 64.27 61.98 62.94 61.91 63.29 59.33 65.49\nNPA-Single 62.09 59.90 61.56 60.24 61.57 58.24 63.56\nNPA-MUSE 62.23 60.21 61.78 60.44 61.75 57.47 63.71\nNPA-Unicoder 63.32 61.41 62.35 61.20 62.77 58.64 64.80\nNPA-InfoXLM 64.29 62.00 62.93 61.94 63.31 59.37 65.50\nLSTUR-Single 62.09 59.95 61.58 60.22 61.58 58.22 63.57\nLSTUR-MUSE 62.21 60.21 61.79 60.44 61.73 57.49 63.75\nLSTUR-Unicoder 63.34 61.40 62.36 61.20 62.77 58.65 64.80\nLSTUR-InfoXLM 64.31 62.03 62.96 61.95 63.32 59.38 65.54\nNRMS-Single 62.11 59.94 61.62 60.28 61.57 58.30 63.64\nNRMS-MUSE 62.33 60.29 61.86 60.54 61.90 57.62 63.93\nNRMS-Unicoder 63.41 61.50 62.46 61.22 62.81 58.84 64.79\nNRMS-InfoXLM64.34 62.05 63.04 61.98 63.40 59.44 65.58\nand memory cost of online serving. In addition, the performance\nmethods based on multilingual PLMs are better than those based\non MUSE embeddings. This may be because PLMs are also stronger\nthan word embeddings in capturing the complicated multilingual\nsemantic information. In addition, InfoXLM can better empower\nmultilingual news recommendation than Unicoder. This may be\nbecause InfoXLM uses better contrastive pre-training strategies\nthan Unicoder to help learn more accurate models.\n3.3 Influence of Model Size\nNext, we explore the influence of PLM size on the recommendation\nperformance. We compare the performance of two representative\nNAML NRMS\n67.0\n67.5\n68.0\n68.5\n69.0\n69.5\n70.0AUC\nBERT-Tiny\nBERT-Small\nBERT-Medium\nBERT-Base\nFigure 3: Influence of the size of PLMs.\nNAML-BERT NRMS-BERT\n67.0\n67.5\n68.0\n68.5\n69.0\n69.5\n70.0AUC\nCLS\nAverage\nAttention\nFigure 4: Influence of the pooling methods.\nmethods (i.e., NAML and NRMS) with different versions of BERT,\nincluding BERT-Base (12 layers), BERT-Medium (8 layers), BERT-\nSmall (4 layers) and BERT-Tiny (2 layers). The results on MIND are\nshown in Fig. 3. We find that using larger PLMs with more parame-\nters usually yields better recommendation performance. This may\nbe because larger PLMs usually have stronger abilities in captur-\ning the deep semantic information of news, and the performance\nmay be further improved if more giant PLMs (e.g., BERT-Large) are\nincorporated. However, since huge PLMs are too cumbersome for\nonline applications, we prefer the base version of PLMs.\n3.4 Influence of Different Pooling Methods\nWe also explore using different pooling methods for learning news\nembeddings from the hidden states of PLMs. We compare three\nmethods, including: (1) CLS, using the representation of the â€œ[CLS]â€\ntoken as news embedding, which is a widely used method for obtain-\ning sentence embedding; (2) Average, using the average of hidden\nstates of PLM; (3) Attention, using an attention network to learn\nnews embeddings from hidden states. The results of NAML-BERT\nand NRMS-BERT on MIND are shown in Fig. 4.6 We find it is very\ninteresting that the CLS method yields the worst performance. This\nmay be because it cannot exploit all output hidden states of the PLM.\nIn addition, Attention outperforms Average. This may be because\nattention networks can distinguish the informativeness of hidden\nstates, which can help learn more accurate news representations.\nThus, we choose attention mechanism as the pooling method.\n3.5 Visualization of News Embedding\nWe also study the differences between the news embeddings learned\nby shallow models and PLM-empowered models. We use t-SNE [17]\nto visualize the news embeddings learned by NRMS and NRMS-\nUniLM, and the results are shown in Fig. 5. We find an interesting\nphenomenon that the news embeddings learned by NRMS-UniLM\nare much more discriminative than NRMS. This may be because the\n6We observe similar phenomenons in other PLM-based methods.\n40\n 20\n 0 20 40\n60\n40\n20\n0\n20\n40\n(a) NRMS.\n60\n 40\n 20\n 0 20 40 60\n60\n40\n20\n0\n20\n40\n60 (b) NRMS-UniLM.\nFigure 5: Visualization of news embeddings learned by\nNRMS and NRMS-UniLM.\nshallow self-attention network in NRMS cannot effectively model\nthe semantic information in news texts. Since user interests are also\ninferred from embeddings of clicked news, it is difficult for NRMS\nto accurately model user interests from non-discriminative news\nrepresentations. In addition, we observe that the news embeddings\nlearned by NRMS-UniLM form several clear clusters. This may\nbe because the PLM-empowered model can disentangle different\nkinds of news for better user interest modeling and news matching.\nThese results demonstrate that deep PLMs have greater ability than\nshallow NLP models in learning discriminative text representations,\nwhich is usually beneficial for accurate news recommendation.\n3.6 Online Flight Experiments\nWe have deployed our PLM-empowered news recommendation\nmodels into the Microsoft News platform. Our NAML-UniLM model\nwas used to serve users in English-speaking markets, including\nEN-US, EN-GB, EN-AU, EN-CA and EN-IN. The online flight exper-\nimental results have shown a gain of 8.53% in click and 2.63% in\npageview against the previous news recommendation model with-\nout pre-trained language model. In addition, our NAML-InfoXLM\nmodel was used to serve users in other 43 markets with different\nlanguages. The online flight results show an improvement of 10.68%\nin click and 6.04% in pageview. These results validate that incor-\nporating pre-trained language models into news recommendation\ncan effectively improve the recommendation performance and user\nexperience of online news services.\n4 CONCLUSION\nIn this paper, we present our work on empowering personalized\nnews recommendation with pre-trained language models. We con-\nduct extensive offline experiments on both English and multilingual\nnews recommendation datasets, and the results show incorporating\npre-trained language models can effectively improve news model-\ning for news recommendation. In addition, our PLM-empowered\nnews recommendation models have been deployed to a commercial\nnews platform, which is the first public reported effort to empower\nreal-world large-scale news recommender systems with PLMs. The\nonline flight results show significant improvement in both click and\npageview in a large number of markets with different languages.\nACKNOWLEDGMENTS\nThis work was supported by the National Natural Science Founda-\ntion of China under Grant numbers U1936216 and U1936208.\nREFERENCES\n[1] Mingxiao An, Fangzhao Wu, Chuhan Wu, Kun Zhang, Zheng Liu, and Xing Xie.\n2019. Neural News Recommendation with Long-and Short-term User Represen-\ntations. In ACL. 336â€“345.\n[2] Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu\nWang, Jianfeng Gao, Songhao Piao, Ming Zhou, et al. 2020. Unilmv2: Pseudo-\nmasked language models for unified language model pre-training. InICML. PMLR,\n642â€“652.\n[3] Yoshua Bengio and Yann LeCun. 2015. Adam: A Method for Stochastic Optimiza-\ntion. In ICLR.\n[4] Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang,\nXia Song, Xian-Ling Mao, Heyan Huang, and Ming Zhou. 2020. Infoxlm: An\ninformation-theoretic framework for cross-lingual language model pre-training.\narXiv preprint arXiv:2007.07834(2020).\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nNAACL-HLT. 4171â€“4186.\n[6] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng\nGao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified Language Model Pre-\ntraining for Natural Language Understanding and Generation. InNeurIPS. 13042â€“\n13054.\n[7] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\nDeepFM: a factorization-machine based neural network for CTR prediction. In\nAAAI. 1725â€“1731.\n[8] Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang,\nand Ming Zhou. 2019. Unicoder: A Universal Language Encoder by Pre-training\nwith Multiple Cross-lingual Tasks. In EMNLP-IJCNLP. 2485â€“2494.\n[9] Armand Joulin, Ã‰douard Grave, Piotr Bojanowski, and TomÃ¡Å¡ Mikolov. 2017. Bag\nof Tricks for Efficient Text Classification. In EACL. 427â€“431.\n[10] Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification. In\nEMNLP. 1746â€“1751.\n[11] Guillaume Lample and Alexis Conneau. 2019. Cross-lingual language model\npretraining. arXiv preprint arXiv:1901.07291(2019).\n[12] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush\nSharma, and Radu Soricut. 2019. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. In ICLR.\n[13] Guang-He Lee and Yun-Nung Chen. 2017. MUSE: Modularizing Unsupervised\nSense Embeddings. In EMNLP. 327â€“337.\n[14] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A\nrobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692\n(2019).\n[15] Shumpei Okura, Yukihiro Tagami, Shingo Ono, and Akira Tajima. 2017.\nEmbedding-based news recommendation for millions of users. In KDD. ACM,\n1933â€“1942.\n[16] Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang.\n2020. Pre-trained models for natural language processing: A survey. Science\nChina Technological Sciences(2020), 1â€“26.\n[17] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.\nJMLR 9, 11 (2008).\n[18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS. 5998â€“6008.\n[19] Heyuan Wang, Fangzhao Wu, Zheng Liu, and Xing Xie. 2020. Fine-grained\nInterest Matching for Neural News Recommendation. In ACL. 836â€“845.\n[20] Hongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi Guo. 2018. DKN: Deep\nKnowledge-Aware Network for News Recommendation. In WWW. 1835â€“1844.\n[21] Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang,\nand Xing Xie. 2019. Neural News Recommendation with Attentive Multi-View\nLearning. In IJCAI. 3863â€“3869.\n[22] Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, and\nXing Xie. 2019. Npa: Neural news recommendation with personalized attention.\nIn KDD. 2576â€“2584.\n[23] Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang, and Xing Xie.\n2019. Neural News Recommendation with Multi-Head Self-Attention. In EMNLP.\n6390â€“6395.\n[24] Chuhan Wu, Fangzhao Wu, Junxin Liu, Sixing Wu, Yongfeng Huang, and Xing\nXie. 2018. Detecting tweets mentioning drug name and adverse drug reaction\nwith hierarchical tweet representation and multi-head self-attention. In SMM4H.\n34â€“37.\n[25] Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2020. SentiRec: Senti-\nment Diversity-aware Neural News Recommendation. In AACL. 44â€“53.\n[26] Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2020. User Modeling\nwith Click Preference and Reading Satisfaction for News Recommendation. In\nIJCAI. 3023â€“3029.\n[27] Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian,\nDanyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, et al. 2020. MIND: A Large-\nscale Dataset for News Recommendation. In ACL. 3597â€“3606.\n[28] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and\nQuoc V Le. 2019. XLNet: Generalized Autoregressive Pretraining for Language\nUnderstanding. NeurIPS 32 (2019), 5753â€“5763.\n[29] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard\nHovy. 2016. Hierarchical attention networks for document classification. In\nNAACL-HLT. 1480â€“1489.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8158962726593018
    },
    {
      "name": "Language model",
      "score": 0.7081927061080933
    },
    {
      "name": "Recommender system",
      "score": 0.5926670432090759
    },
    {
      "name": "Topic model",
      "score": 0.515612781047821
    },
    {
      "name": "Information retrieval",
      "score": 0.5066088438034058
    },
    {
      "name": "World Wide Web",
      "score": 0.4595000743865967
    },
    {
      "name": "News analytics",
      "score": 0.4554937779903412
    },
    {
      "name": "News media",
      "score": 0.4521978497505188
    },
    {
      "name": "Natural language",
      "score": 0.4421883523464203
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34761086106300354
    },
    {
      "name": "Natural language processing",
      "score": 0.34153616428375244
    },
    {
      "name": "Advertising",
      "score": 0.16296353936195374
    },
    {
      "name": "Machine learning",
      "score": 0.14447447657585144
    },
    {
      "name": "Business",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210113369",
      "name": "Microsoft Research Asia (China)",
      "country": "CN"
    }
  ],
  "cited_by": 5
}