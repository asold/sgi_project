{
  "title": "Meta-Transformer: A Meta-Learning Framework for Scalable Automatic Modulation Classification",
  "url": "https://openalex.org/W4390738763",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5004310075",
      "name": "Jungik Jang",
      "affiliations": [
        "Gachon University"
      ]
    },
    {
      "id": "https://openalex.org/A5061054986",
      "name": "Jisung Pyo",
      "affiliations": [
        "Gachon University"
      ]
    },
    {
      "id": "https://openalex.org/A5104219523",
      "name": "Young-Il Yoon",
      "affiliations": [
        "GS Caltex (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A5028161582",
      "name": "Jaehyuk Choi",
      "affiliations": [
        "Gachon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3168231499",
    "https://openalex.org/W2944891925",
    "https://openalex.org/W2005956500",
    "https://openalex.org/W2114267371",
    "https://openalex.org/W2773170971",
    "https://openalex.org/W3000943722",
    "https://openalex.org/W3113059984",
    "https://openalex.org/W3085631338",
    "https://openalex.org/W3128367445",
    "https://openalex.org/W3174334351",
    "https://openalex.org/W4220783431",
    "https://openalex.org/W3155899199",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4205590867",
    "https://openalex.org/W3115053547",
    "https://openalex.org/W4321483861",
    "https://openalex.org/W3046698617",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W3162199993",
    "https://openalex.org/W1522301498"
  ],
  "abstract": "Recent advances in deep learning (DL) have led many contemporary automatic modulation classification (AMC) techniques to use deep networks in classifying the modulation type of incoming signals at the receiver. However, current DL-based methods face scalability challenges, particularly when encountering unseen modulations or input signals from environments not present during model training, making them less suitable for real-world applications like software-defined radio devices. In this paper, we introduce a scalable AMC scheme that provides flexibility for new modulations and adaptability to input signals with diverse configurations. We propose the Meta-Transformer, a meta-learning framework based on few-shot learning (FSL) to acquire general knowledge and a learning method for AMC tasks. This approach empowers the model to identify new unseen modulations using only a very small number of samples, eliminating the need for complete model retraining. Furthermore, we enhance the scalability of the classifier by leveraging main-sub transformer-based encoders, enabling efficient processing of input signals with diverse setups. Extensive evaluations demonstrate that the proposed AMC method outperforms existing techniques across all signal-to-noise ratios (SNRs) on RadioML2018.01A. The source code and pre-trained models are released at <uri>https://github.com/cheeseBG/meta-transformer-amc</uri>.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.0322000\nMeta-Transformer: A meta-learning framework\nfor scalable automatic modulation classification\nJUNGIK JANG1, (Student Member, IEEE), JISUNG PYO1, YOUNG-IL YOON2, and JAEHYUK\nCHOI1, (Member, IEEE)\n1School of Computing, Gachon University, Seongnam-si 13120, Republic of Korea\n2Research and Development Center, LIG Nex1, Seongnam 13488, Republic of Korea\nCorresponding author: Jaehyuk Choi (e-mail:jchoi@gachon.ac.kr).\nThis work was supported by the Korea Research Institute for Defense Technology Planning and Advancement (KRIT) Grant funded by the\nDefence Acquisition Program Administration (DAPA) under Grant KRIT-CT-22-002.\nABSTRACT Recent advances in deep learning (DL) have led many contemporary automatic modulation\nclassification (AMC) techniques to use deep networks in classifying the modulation type of incoming\nsignals at the receiver. However, current DL-based methods face scalability challenges, particularly when\nencountering unseen modulations or input signals from environments not present during model training,\nmaking them less suitable for real-world applications like software-defined radio devices. In this paper,\nwe introduce a scalable AMC scheme that provides flexibility for new modulations and adaptability to\ninput signals with diverse configurations. We propose the Meta-Transformer, a meta-learning framework\nbased on few-shot learning (FSL) to acquire general knowledge and a learning method for AMC tasks.\nThis approach empowers the model to identify new unseen modulations using just a very small number\nof samples, eliminating the need for a complete model retraining. Furthermore, we enhance the scalability\nof the classifier by leveraging main-sub transformer-based encoders, enabling efficient processing of input\nsignals with diverse setups. Extensive evaluations demonstrate that the proposed AMC method outperforms\nexisting techniques across all signal-to-noise ratios (SNRs) on RadioML2018.01A.\nINDEX TERMS Automatic modulation classification, few-shot learning, meta-learning, Transformer,\nunseen dataset.\nI. INTRODUCTION\nA\nCCURATE classification of modulation types in incom-\ning signals is a key element of the wireless communi-\ncation system. Automatic modulation classification (AMC)\nand radio signal recognition methods play a crucial role\nin recognizing modulation types for various military and\ncivilian services such as dynamic spectrum access, jamming\ndetection, surveillance, and spectrum coexistence. Typically,\nthe preamble of an received signal carries details about its\nmodulation scheme, enabling the receiver to determine the\nmodulation type and pass it through to the appropriate de-\nmodulation process [1].\nHowever, the design of a highly precise AMC scheme is\nchallenging in the modern wireless communication environ-\nment since numerous heterogeneous communication systems\ncoexist in a complex and non-cooperative manner. Perform-\ning the AMC task is particularly challenging in Cognitive\nRadio (CR) networks and Software Defined Radio (SDR) sys-\ntems, as they provide the flexibility to employ various wire-\nless communication services over a wide frequency range.\nIn CR and SDR environments, dynamic spectrum sensing\nand access is performed over a wide frequency band in a\nnon-cooperative manner. This frequently results in inconsis-\ntent and partial signal reception. It is important to note that\nthe AMC in these environments should be able to identify\nmodulation types even when the received samples do not\ncontain the entire packet information and may only have\npartial information in the middle or tail [2].\nAMC methods can be categorized into two primary types:\n(i) likelihood-based (LB) and (ii) feature-based (FB) ap-\nproaches [3]. LB approaches achieve high classification ac-\ncuracy by harnessing prior knowledge about the target mod-\nulations [2]. Nonetheless, with an increase in the number\nof target modulations, LB approaches encounter difficulties,\nincluding elevated computational complexity and even math-\nematical intractability [4].\nIn recent years, FB approaches have extensively incorpo-\nrated deep learning (DL) into AMC, attracting attention due\nto their outstanding classification performance, even when\ndealing with numerous target modulations [1], [2], [5]–[11].\nVOLUME 11, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3352634\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nDL-based AMC methods learn valid classification rules from\na substantial amount of complex modulation data [12] and\nachieve high accuracy in modulation classification thanks to\nrecent breakthroughs in deep learning techniques.\nHowever, current DL-based AMC methods still face chal-\nlenges in terms of real-world deployment, primarily due to\ntheir limited scalability, particularly when dealing with un-\nseen modulations or input signals with different configura-\ntions not seen during training process. In non-cooperative and\ncomplex real-world communication environments, received\ninputs frequently deviate from the features employed during\nthe model training phase, resulting in substantial classifi-\ncation errors. Note that the performance of most DL-based\nAMC methods heavily depends on the availability of a sub-\nstantial volume of training data. For instance, most DL-based\nAMC methods utilize fixed frame lengths as inputs to their\nmodels and do not account for scenarios with variable input\nsizes [10]. Accordingly, they may not work properly for short\ninput frames.\nFig. 1 illustrates the classification accuracy of ResNet-\nbased and CNN-based methods [10], [13] across various\ninput frame lengths. These methods utilize an input frame\nlength of 1024, which corresponds to the frame length of the\ntraining dataset. Testing for variable input lengths were per-\nformed by undersampling to lengths of {512, 256, 128, 64},\nwhere shorter inputs were duplicated and concatenated in\nthe preprocessing step to reach the default length of 1024.\nWe can observe that the input frame length decreases, the\nclassification performance deteriorates where models were\ntrained only with fixed-length frames of 1024 samples. Un-\nfortunately, it is nearly impossible to collect sufficient labeled\ntraining datasets in advance for numerous combinations of the\ntarget classes, such as varying frame lengths and signal-to-\nnoise ratios (SNRs), to maintain classification accuracy. Fur-\nthermore, when introducing previously unseen modulations,\nexisting solutions necessitate the collection of a substantial\nnumber of samples and a subsequent retraining of the model.\nIn this paper, the term \"unseen\" refers to the data belonging\nto classes that the model did not encounter during the training\nphase. Therefore, it is essential to devise a more intelligent\nand scalable AMC technique capable of adapting to new\nunseen modulations and recognize input signals with complex\ncombinations of temporal and spatial features.\nIn this paper, we introduce Meta-Transformer, a scalable\nAMC scheme that provides flexibility for new unseen mod-\nulations and adaptability to input signals with diverse con-\nfigurations. Our proposed framework comprises three key\ncomponents: (i) a meta-learning framework that employs\nfew-shot learning (FSL), (ii) a feature extractor built upon\na Transformer architecture [14], and (iii) a main-sub model\narchitecture to ensure scalability for input frame sizes. Within\nour proposed meta-learning framework, we initially train the\nmain-sub model on a source dataset, targeting a specific set of\nmodulations. Subsequently, we adapt the trained encoders to\nnew target modulations using only a small number of newly\ngathered samples.\nTABLE 1. Abbreviations and Meanings\nAbbreviation Meaning\nDL Deep Learning\nAMC Automatic Modulation Classification\nFSL Few-Shot Learning\nSNR Signal-to-Noise Ratio\nCR Cognitive Radio\nSDR Software Defined Radio\nLB Likelihood-Based\nFB Feature-Based\nIQ In-phase and Quadrature\nOOK On-Off Keying\nASK Amplitude Shift Keying\nPSK Phase Shift Keying\nAPSK Amplitude Phase Shift Keying\nBPSK Binary Phase Shift Keying\nQPSK Quadrature Phase Shift Keying\nQAM Quadrature Amplitude Modulation\nAM Amplitude Modulation\nAM-SSB-WC AM Single-Sideband With Carrier\nAM-SSB-SC AM Single-Sideband Suppressed Carrier\nAM-DSB-WC AM Double Side Band With Carrier\nAM-DSB-SC AM Double Side Band Suppressed Carrier\nFM Frequency Modulation\nGMSK Gaussian low-pass-filtered Minimum Shift Keying\nOQPSK Offset Quadrature Phase Shift Keying\nFIGURE 1. Impact of input frames’ lengths on the classification accuracy\nof ResNet-based and CNN-based method [10], [13] under different SNR\nvalues based on the RadioML2018.01A [5].\nThis approach effectively mitigates the issues related to\ndata collection and the overhead of retraining when ad-\ndressing new unseen modulations. Moreover, to enhance the\nscalability and performance of the model, we leverage a\nTransformer-based encoder [14] in the design of the feature\nextractor for our proposed AMC method.\nThe noteworthy point is the operation of the Vision Trans-\nformer [14]. ViT divides an image into patches and tokenizes\nthem for processing. The size of these patches plays a critical\nrole in determining the receptive field. When configuring\nlarger patch sizes, the model can capture a broader context\ninitially, but it might miss finer details. Conversely, opting for\nsmaller patch sizes allows the model to capture more intricate\ndetails but concurrently increases the risk of overfitting, as\nit might focus excessively on localized information. Hence,\nthere exists a trade-off relationship to consider when deter-\nmining the optimal patch size. Through extensive evaluations\non the RadioML2018.01A dataset [5], we find the appropri-\nate patch size for AMC task, demonstrate that the proposed\n2 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3352634\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nmethod consistently outperforms existing techniques across\nall signal-to-noise ratios (SNRs).\nIn real-world scenarios where AMC technology is applied,\nthe input frame length is likely to vary, unlike the fixed\nlength of 1024 frames used in this paper’s dataset. This de-\nmands a solution for variable input signal lengths, and when\nusing a single encoder, performance significantly degrades\nif the input length during testing is shorter than that used\nduring training(Fig. 1). In fact, using multiple encoders en-\nsures higher performance for diverse input lengths. However,\nthere is a trade-off as the model becomes heavier with an\nincreasing number of encoders, requiring consideration of\ncomputational constraints. Therefore, this paper proposes a\nmethod of employing two encoders.\nThe remainder of the paper is structured as follows: Sec-\ntion II summarizes related research work. Section III de-\nscribes the overview of the proposed meta-learning based\nAMC scheme and its details. Section IV presents the eval-\nuation results, and Section V concludes this paper.\nII. RELATED WORK\nRelated work can be categorized into two main groups: (i)\ndeep learning-based approaches focused on enhancing mod-\nulation recognition and classification performance, and (ii)\nstudies utilizing few-shot learning techniques for AMC.\nO’Shea et al. [5] utilized a 1D CNN based on ResNet [13]\nto extract features from the in-phase and quadrature phase\n(IQ) components of the signal. They conducted experiments\nusing the RadioML2018.01A dataset and achieved high accu-\nracy across 24 modulations, demonstrating the effectiveness\nof CNN-based models for AMC. Subsequent studies using\nCNNs have made efforts to enhance performance in AMC us-\ning the RadioML2018.01A dataset. Kim et al. [10] proposed a\nCNN model that employed frame replication to expand it into\na size of 4 × 1024 facilitating meaningful feature extraction,\nand utilized average pooling to reduce computational com-\nplexity. Huynh-The et al. [6], [11] proposed MCNet, which\ndemonstrates efficient computational complexity based on\n1D CNN, and RanNet, which shows high performance using\nthe residual-attention structure. Additionally, various other\nCNN-based studies [7]–[9] have been conducted.\nRecently, methodologies employing few-shot learning\ntechniques have emerged to tackle the constraints of DL in\nAMC, as discussed in Section I. Zhou et al. [15] introduced\nAMCRN, an architecture based on CNN that assesses feature\nsimilarity between test data and annotated few-shot data.\nZhang et al. [16] proposed the Attention Relation Network,\nwhich incorporates channel and spatial attention to enable\nmodulation pattern recognition even with few-shot samples.\nHao et al. [17] proposed M-MFOR, a meta-learning system\nleveraging few-shot learning. Their work demonstrates the\nability to achieve high accuracy, even on modulation datasets\nwith distribution bias, by effectively generalizing the meta-\nknowledge learned through meta-learning.\nIn contrast to prior research employing few-shot learning\napproaches with CNN-based models, to the best of our knowl-\nTABLE 2. Notations and meanings\nNotation Meaning\nfθ Main encoder\nfθ′ Sub encoder\nθ Main encoder’s parameters\nθ′ Sub encoder’s parameters\nzm Main encoder input frame length\nzs Sub encoder input frame length\nzmin Minimum input length\np Number of patch\ns Patch length\nϵ Episode (composed of support set and query set)\nNϵ Total number of episodes\nptrain Training data ratio\nNS Number of support set\nNQ Number of query set\nNepoch training epoch\nS Annotated data\nyi Class label set\ncl Prototype for class l\nd Distance function\nL Loss function\nα Learning rate\nγ Learning scheduler parameter\nedge, we have pioneered the integration of a Transformer-\nbased encoder into the meta-learning-based AMC. Our ap-\nproach efficiently learns inter-sample relationships through\nthe self-attention mechanism during the training phase. This\nenables rapid adaptation and achieves excellent performance\neven with limited data for unseen modulations during the\ntesting phase.\nIII. PROPOSED METHOD\nThis section begins with an introduction to the Meta-\nTransformer, our proposed meta-learning framework de-\nsigned for the AMC task. In this section, we first introduce\nthe Meta-Transformer, which is the proposed meta-learning\nframework for the AMC task and then explain its specifics,\ncovering both the meta-training and meta-testing processes.\nThe objective of our work is to overcome the aforementioned\nlimitations of supervised learning-based DL approaches and\nensure scalability for unseen modulations or input signals\nwith varying configurations not seen during training phase.\nA. META-TRANSFORMER\nFig. 2 illustrates the architecture of Meta-Transformer. Our\nsystem consists of two main modules: (i) a meta-training\nmodule and (ii) a meta-testing module.\n• Meta-Training Module: The module utilizes source\ndataset for specific modulation classes, referred to as\nseen modulations. It trains the modulation classifier,\nnamely main-sub Transformer-based encoders fθ and fθ′ ,\nwhere θ and θ′ represents the trainable parameters. Un-\nlike traditional supervised learning methods, our meta-\nlearning approach acquires meta-knowledge, enabling\nquicker adaptation to new tasks even with limited sam-\nples (Section III-B). During the meta-training phase,\nthe main encoder is trained with an input frame size\nVOLUME 11, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3352634\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 2. Overview of Meta-Transformer: meta-training with source datasets for given target modulations and meta-testing process with unseen\nmodulations limited size datasets\n2 × zm, while the sub-encoder is trained with 2 × zs.\nThe variables zm and zs denote the length of the frame,\nplaying a crucial role in determining the performance of\nboth the main and sub encoders. A detailed explanation\nof the variables is covered in Section III-C.\n• Meta-Testing Module: Once trained, meta-testing mod-\nule uses the encoders fθ, fθ′ for new unseen modulations\nwith fewer collected samples (Section III-C).\nThe main encoder fθ and the sub encoder fθ′ learn general\nmeta-knowledge to extract appropriate feature vectors for\nAMC tasks, where meta-knowledge represents the underly-\ning essence or commonality among multiple tasks [17]. To\nachieve this, we utilize the methodology of learning the metric\nspace by using prototypes of each class, as introduced in\nProtoNet [18].\nFig. 3 depicts the architecture of the encoder and its op-\nerational sequence. To handle various input signal configu-\nrations, we employ a feature extractor based on the Trans-\nformer architecture proposed by Dosovitskiy et al. [14]. The\nTransformer-based encoder has a modular architecture, al-\nlowing each layer to work independently. Communication be-\ntween layers is facilitated through attention mechanisms [14],\nproviding flexibility in adjusting the model’s size and com-\nplexity. In this setup, each module has an input layer of\n2 × N, which takes IQ components of signal data as input.\nThese components are split into p patches of size 2 × s, with\neach patch undergoing linear embedding after the addition of\nposition information embeddings.\nTable 3 summarizes the hyperparameters used for the main\nTABLE 3. Details of Proposed Model Hyperparameters\nmodel Layers Hidden D MLP Heads Input Patch\nmain 8 36 32 9 2 × zm 2 × s\nsub 8 108 32 9 2 × zs 2 × s\nFIGURE 3. Transformer-based encoderfθ to extract feature vectors of I/Q\nsignals. We employed ViT [14]’s encoder structure.\nand sub encoders within Meta-Transformer. Since the sub\nencoder is trained with a smaller input frame size compared to\nthe main encoder, we adjusted the Hidden size dimension D to\na larger value to optimize performance. The input length z and\npatch length s are pivotal hyperparameters affecting signal\ndata manipulation and overall model performance. We con-\nducted experiments focused on determining the most suitable\nvalues for these parameters, elaborated upon in Section IV.\nAdditionally, we determined the remaining hyperparameters\nthrough empirical experiments to achieve optimal model per-\nformance.\n4 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3352634\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nB. META-TRAINING\nThe meta-training module trains two encoders, denoted as\nfθ and fθ′ , incorporating meta-knowledge for the ACM task.\nBoth encoders follows an identical training approach, and\nhereafter explained with respect to fθ. Training occurs episod-\nically in this phase. Each episode, labeled as ϵ, comprises two\nparts: (i) a support set (training set) for prototype generation\nand (ii) a query set (validation set) for modulation prediction\nand parameter updating. To generate the support set and query\nset for each episode, we first randomly choose k categories\nfrom the source dataset. Within each selected category, we\nthen randomly pick n instances. Here, k represents the total\nnumber of classes within the support set, often referred to as\nk-way, and n represents the number of data samples for each\nclass (way), known as n-shot. The total number of episodes,\ndenoted as Nϵ, can be determined using the following equa-\ntion:\nNϵ = ptrain ∗ N\nNS + NQ\n∗ Nepoch, (1)\nwhere N represents the total number of data, ptrain is the\nratio of the training dataset, NS is the number of support sets,\nNQ is the number of query sets, and Nepoch is the number of\ntraining epochs. Here, the modulation classes used in training\nare regarded as seen modulations. The N annotated data used\nas input, denoted as S = {(x1, y1), . . . ,(xN , yN )}, have a\nframe size of 1 × 2 × z (C × H × W ), which is provided\nin RadioML2018.01A [5] dataset. The corresponding class\nlabels are represented as yi = {1, . . . ,K}.\nAlgorithm 1Process of meta training. k≤K is the number of\nclasses per episode, E is the selected k classes for episode, NS\nis the number of support sample per class, NQ is the number\nof query sample per class, ˆm is the bias-corrected moving av-\nerage of the gradients, ˆv is the bias-corrected moving average\nof the squared gradients, α is the learning rate and ϵ is a small\nvalue used for numerical stability. Random uniform( S, N)\ndenotes uniform and random selection of N values from the S\nset. Signal Length(S, z) denotes the adjustment of all x lengths\nin set S to z.\nInput: Training set Strain = {(x1, y1), ...,(xN , yN )}\nOutput: Trained base encoder fθ\nfor l in {1,. . . ,k} do\nSsupport ← Random uniform(SEl , NS )\nSquery ← Random uniform(SEl \\Ssupport , NQ)\nSsupport ← Signal Length(Ssupport , zm)\nSquery ← Signal Length(Squery, zm)\ncl ← 1\nNS\nP\n(xi,yi)∈Sl fθ(xi)\nend for\nL ← 0 {Initialize loss L}\nfor l in {1,. . . ,k} do\nfor (x, y) in Squery do\nθ ← θ − α√\nˆv+ϵ ˆm\nend for\nend for\nIn each episode, the signal undergoes preprocessing based\non experimentally derived values for zm and zs. Following\nthis, the support set and query set data are segmented into\npatches as previously described and then fed into the encoder.\nFor the support set, the prototype cl is created by averaging\nthe extracted feature vectors (referred to as embedded support\npoints) from the annotated dataset Sl belonging to class l.\ncl = 1\n|Sl |\nX\n(xi,yi)∈Sl\nfθ(xi) (2)\nThe feature vectors extracted from the query set are classified\nusing the generated prototypes, based on a distance function\nd, which could be methods such as Euclidean distance or\ncosine similarity. In ProtoNet [18], the distance between the\nquery embedding and the prototype is measured by Euclidean\ndistance, which shows excellent performance. Consequently,\nwe also employed Euclidean distance as our distance metric.\nBased on softmax over the distances between the query point\nx and the prototypes in the embedding space, we generate a\ndistribution over classes. The equation for this distribution is\nas follows:\npθ(y = l|x) = exp(−d(fθ(x), cl ))P\nl′ exp(−d(fθ(x), cl′ )) (3)\nAs each episode progresses, the parameters θ of fθ are itera-\ntively updated using the Adam optimizer [19] to minimize the\nnegative log probability of the actual class k, as described in\nEquation 4.\nL(θ) =−logpθ(y = k|x) (4)\nThe algorithm 1 illustrates the main encoder meta-training\nprocess for an episode. The sub encoder is also trained using\nthe same approach.\nC. META-TESTING\nMeta-Testing module utilizes the encoders fθ and fθ′ trained\nthrough the meta-training phase. The parameters θ, θ′ remain\nfixed and are not updated during the meta-testing process. In\nthe meta-testing phase, both the support set and the query set\nconsist of unseen modulations, enabling us to evaluate the\nmodel’s adaptation to a new domain and assess its general-\nization capability. The signal length L for both the support\nset and the query set follows these conditions: zs < L ≤ zm\nfor input to the main encoder, and zmin ≤ L ≤ zs for input to\nthe sub-encoder. Here, zm is set to 1024, the signal length of\nRadioML2018.01A, and zs is experimentally chosen as 128 to\nenhance the model’s scalability for shorter signal lengths. Ad-\nditionally, the minimum length zmin is set to 64, achieving over\n50% performance, and anything below cannot be used for\nclassification. As we will discuss in Section IV, we consider\nthe application of our method to SDR platform scenarios.\nFor example, in the case of an operational SDR equipment,\nupgrades are necessary to enable recognition of new modula-\ntions not encompassed in the current model’s training. For this\npurpose, the meta-testing module can include the datasets for\nboth the seen modulations used in the training phase and new\nVOLUME 11, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3352634\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nunseen modulations. The results of these tests are presented in\nSection IV-D. A commonly adopted configuration for support\nsets in most FSL-based approaches is the 5-shot setting,\nwhere the support set comprises five data samples. Similar\nto the meta-trainig phase, the meta-testing module generates\nk′ prototypes using the trained fθ, fθ′ , where k′ denotes the\nnumber of target classes for meta-testing. The query set used\nfor inference is classified based on the Euclidean distance\nbetween the embedding vectors and the prototypes. In our\nexperiments, we investigated the impact of the k′ value, the\nresults of which can also be found in section IV-D.\nIV. PERFORMANCE EVALUATION\nIn this section, we evaluate the performance of our proposed\nsystem through a series of extensive experiments. These ex-\nperiments include experiments to determine the appropriate\npatch size for the proposed Meta-Transformer (Section IV-B),\ncomparing meta-learning and supervised learning approaches\n(Section IV-C), evaluating the few-shot learning capability of\nour method on new Unseen modulations (Section IV-D), and\nexamining the scalability of our method for different input\nframe sizes (Section IV-E).\nThe training dataset ratio ptrain is set to 0.8, and Nepoch is\nset to 50 for the main encoder and 100 for the sub encoder.\nThe optimizer used is Adam [19], with an initial learning\nrate α of 0.001. A scheduler with a step size of 10 and γ\nof 0.9 is employed. The experiments were conducted on an\nUbuntu 20.04 system with an Intel(R) i9-9900KF processor\nand GeForce RTX 2080 Ti 11GB GPU.\nA. DATASET\nWe conducted our experiments using the widely utilized\nRadioML2018.01A dataset [5] in the field of AMC research.\nThis dataset comprises a total of 24 modulations, includ-\ning analog modulations such as AM-DSB-WC, AM-DSB-\nSC, AM-SSB-WC, AM-SSB-SC, FM, and digital modula-\ntions such as OOK, 4ASK, 8ASK, BPSK, QPSK, 8PSK,\n16PSK, 32PSK, 16APSK, 32APSK, 64APSK, 128APSK,\n16QAM, 32QAM, 64QAM, 128QAM, 256QAM, GMSK,\nand OQPSK. This diverse set of modulations includes high\norder schemes like QAM256 and APSK256. Each frame\nconsists 1024 samples for the IQ components. The dataset\nconsist of 4096 frames for each modulation-SNR combina-\ntion, resulting in a total of 2.5 million frames. The SNR range\nspans from -20 dB to 30 dB with a step size of 2 dB.\nB. PATCH SIZE\nThe proposed Meta-Transformer utilizes an encoder based\non ViT [14], and the input signal is divided and tokenized at\nthe patch level for processing. In this case, the patch size s\nused has an impact on the receptive field, ultimately affecting\nthe classification performance. In the original ViT, images are\ndivided into s × s patches assuming a square-shaped image.\nHowever, for the AMC task, signals are provided in the form\nof IQ components, resulting in a 2D shape of 2 × L. When\nsetting s to 2 and dividing the signal into 2 ×2 patch size, the\nFIGURE 4. Performance test based on the patch size of the main encoder.\nFIGURE 5. Performance test based on the patch size of the sub encoder.\namount of information becomes extremely low, resulting in\nsignificantly reduced classification performance. Therefore,\nwe conducted experiments to find an appropriate value for\ns that is suitable for the AMC task. The proposed model\ncomprises both the main and sub encoders, each trained for\nsignal lengths of 1024 and 128. Hence, we conducted sepa-\nrate experiments for finding the appropriate s value for both\nencoders with these two input sizes.\nFig. 4 and Fig. 5 depict the experimental results for s =\n{8, 16, 32, 64}. In both figures, the best performance is ob-\nserved when s = 16 in good SNR environments (0 dB or\nhigher). It can be noted that performance decreases as the\npatch size increases or decreases from this optimal value. On\nthe other hand, in poor SNR environments, there is a slight\ndifference, but a better performance is observed as s increases.\nThis suggests that in the presence of high noise, expanding\nthe receptive field is necessary to capture a broader range of\ninformation. We selected s = 16 as the default patch size,\nensuring robust performance in good SNR environments, and\nproceeded with the remaining experiments.\nC. COMPARING META-LEARNING AND SUPERVISED\nLEARNING\n6 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3352634\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 6. Performance comparison between meta-learning (our\nproposed method, DAELSTM [20] and ProtoNet [18]) and supervised\nlearning (ResNet [5] and CNN [10]) models for all 24 modulations.\nWe first conducted an experiment to compare how well\nmeta-learning models, including our proposed model, per-\nform in accurately classifying the 24 different modula-\ntions, compared to supervised learning models and other\ntransformer-based models. The supervised learning mod-\nels used in the experiments include ResNet [5] based and\nCNN [10] based models, where we will denote them as\nResNet and CNN, respectively. For the meta-learning models,\nwe employed ProtoNet [18] and DAELSTM [20] along with\nour proposed model. Note that the original DAELSTM [20]\nis a supervised learning-based model, but we modified it into\na meta-learning structure by leveraging our encoder-based\nframework. We then included both the modified DAELSTM\nand the original one in our comparison experiment. These\nfive models were trained using SNR range [-10, 20] dB,\ndemonstrating their optimal performance within this SNR\nrange. Their performance was evaluated in terms of accuracy,\nwith a step size of 2 dB, across the entire range of [-20\nto 20] dB contained in the RadioML2018.01A dataset. This\nexperiment was designed to compare the performance of the\nproposed meta-learning approach with models proposed us-\ning traditional supervised learning methods. Additionally, it\naims to verify the effectiveness of the transformer architecture\nfor the AMC task by comparing it with commonly used CNN\nand LSTM-based models.\nFig. 6 shows the results of the evaluation. We observed\nthat our proposed Meta-Transformer achieved the highest\nperformance at 95.76% accuracy in the good SNR range,\nparticularly at SNR 20 dB (Fig. 7). This demonstrates the ef-\nfectiveness of our transformer architecture and meta-learning\napproach for the AMC task, outperforming CNN and ResNet\nmodels based on conventional supervised learning methods.\nFurthermore, we demonstrated that even existing models\nlike DAELSTM, originally proposed using supervised learn-\ning, can be adapted to our proposed framework by utilizing\nthe framework’s encoder. ProtoNet, despite being a meta-\nlearning approach, exhibited lower performance. This sug-\ngests that the model architecture was not specifically designed\nto address the AMC task, highlighting the importance of\nFIGURE 7. Confusion matrix for proposed model inference results at SNR\n20dB.\ntailoring the model structure to suit the requirements of the\ntask.\nWe also compared the complexity of the models in Table 4.\nDespite having a higher computational complexity compared\nto the other four models, the proposed model demonstrated\nthe best performance across all 24 modulations.\nTABLE 4. Complexity comparison of different models\nModel FLOPs Memory Speed Params\nResNet [5] 0.026G 5.32MB 0.004s 0.17M\nCNN [10] 0.038G 16.69MB 0.005s 0.04M\nProtoNet [18] 0.014G 7.06MB 0.002s .02M\nDAELSTM [20] 0.028G 0.06MB 0.012s 0.01M\nProposed 0.046G 15.98MB 0.005s 0.72M\nD. UNSEEN MODULATION\nNext, we evaluate the adaptation performance of our pro-\nposed method to new modulation types. As mentioned pre-\nviously, one of the advantages of meta-learning is its ability\nto quickly adapt the model to new unseen classes. For in-\nstance, consider a scenario where an operational SDR equip-\nment requires an upgrade to recognize new modulation types.\nWe conducted experiments where the proposed model was\ntrained on 12 randomly selected modulations (denoted as\nSeen modulations) out of the total 24 modulations. We then\nrandomly selected 5 modulations out of the remaining 12\nmodulations as Unseen modulations for testing. We divided\nthe test cases into three categories as indicated in Table 5.\nFor each test case, we carried out 100 test iterations, with\neach iteration involving the random selection of five Unseen\nmodulations. We then calculated the average accuracy. The\ndefault value for \"shot\" was set to 5-shots. The reason is\nthat many few-shot learning studies use 1-shot and 5-shot\nevaluations as benchmarks. The number of sample frames\nVOLUME 11, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3352634\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 8. Performance comparison for three test cases in Table 5.\nused for training was approximately 1.3 million, while for\ntesting, around 0.5 million frames were used for 5 randomly\nselected modulations.\nTABLE 5. 12 Modulations Used For Training in Three Repeated Test Cases\nTest Case Modulations\nA ’8ASK’, ’BPSK’, ’32PSK’, ’16APSK’, ’64APSK’,\n’128APSK’, ’128QAM’, ’AM-SSB-WC’, ’AM-SSB-\nSC’, ’AM-DSB-SC’, ’GMSK’, ’OQPSK’\nB ‘BPSK’, ‘8PSK’, ’32PSK’, ’32APSK’, ’64APSK’,\n’128APSK’, ’64QAM’, ’AM-SSB-WC’, ’AM-DSB-\nWC’, ’FM’, ’GMSK’\nC ’8ASK’, ’BPSK’, ’QPSK’, ’16PSK’, ’32PSK’,\n’32APSK’, ’32QAM’, ’128QAM’, ’AM-SSB-WC’,\n’AM-DSB-WC’, ’FM’, ’GMSK’\nFig. 8 depicts the accuracy results for the three test cases, il-\nlustrating an average accuracy of around 80% in the high SNR\nregion for the five randomly selected Unseen modulations.\nThe variation in accuracy among the test cases is influenced\nby the complexity of the modulations used during the training\nphase. More complex modulations tend to demonstrate better\nperformance during the inference phase. For the subsequent\nexperiments, we used the Test B category in Table 5.\nFig. 9 presents the results of an experiment that investigated\nthe influence of shots on each class (way) of the support set\nduring the meta-testing phase. For the 5-way classification,\nwe used the {1, 5, 10, 15} shots. The results demonstrate that\naccuracy increases with a higher number of shots. With 15\nshots, our method achieved 90% accuracy on the Unseen\nmodulations, demonstrating its ability to quickly acquire gen-\neral knowledge about a new domain even with a few dataset.\nFig. 10displays the classification performance results for\nthree different numbers of Unseen modulations while keeping\nthe shots fixed at 5. We conducted tests with Unseen modu-\nlations consisting of 3, 5, and 7 classes. The results indicate\nthat as the number of Unseen modulations decreases, there\nis an improvement in differentiating prototypes within the\nembedding space, leading to higher performance. Notably, a\nsubstantial increase in classification accuracy is observed for\nlower SNRs as the number of Unseen modulations decreases.\nFIGURE 9. Impact of number of shots on classification performance for\n5-way (fiveUnseen modulations/classes) with 1, 5, 10, and 15 different\nshots.\nFIGURE 10. Performance evaluation for different number of ways, i.e., 3,\n5, and 7Unseen modulations, with a fixed 5-shot learning.\nFIGURE 11. Performance evaluation using both the 12Seen modulations\nused during training and additionalUnseen modulations in the test phase.\nFig. 11 represents an experiment tailored for the SDR plat-\nform scenarios. The evaluation encompasses both the Seen\nmodulations used in the training phase and the Unseen mod-\nulations. Despite the inherent challenges posed by the 12-way\nand 13-way configurations in the meta-learning context, our\nmethod achieved a high performance level, surpassing 80%\naccuracy with 5-shot learning. We expect that performance\ncan be further improved by investigation of hyperparameters\nand by leveraging more powerful computing environments.\nWe plan to explore these possibilities in greater detail in our\nfuture work.\n8 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3352634\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 12. The impact of varying input frame lengths on the\nclassification accuracy of the proposed model(only main encoder) in the\nRadioML2018.01A dataset [5].\nFIGURE 13. The impact of varying input frame lengths on the\nclassification accuracy of the proposed model in the RadioML2018.01A\ndataset [5].\nE. INPUT SIZE SCALABILITY\nIn real-world scenarios, modulation classification may be\nrequired for signals with incomplete reception or varying\nlengths. In many existing AMC methods, however, the in-\nput frame size was often overlooked in both model design\nand evaluation. Fig. 12 clearly indicates that using the main\nmodel trained with a 2 × 1024 frame size results in perfor-\nmance degradation for smaller input frame sizes when actu-\nally utilized. Therefore, we conducted additional experiment\nevaluate the scalability of our Meta-Transformer using two\nencoders for frame sizes smaller than the given 2 × 1024\nframes. Specifically, we experimented with frame sizes of\n2×64, 128, 256, 512 to evaluate the model’s performance and\ngeneralizability.\nFig. 13 presents the results of evaluating the proposed\nmodel using smaller input frames while it was trained with\n2 × 1024 frames. Thanks to the main-sub encoder structure,\neven for samples with smaller input frame sizes, each en-\ncoder effectively captures the interactions between sample\npatches within the same frame. Consequently, using smaller\ninput frame sizes results in a relatively minor performance\ndegradation compared to using the main encoder architecture\nalone, specifically for sizes below 2 × 128.\nV. CONCLUSION\nIn this work, we introduced a Meta-Transformer, a scal-\nable AMC scheme that provides flexibility for handling new\nmodulations and adaptability to diverse input signal config-\nurations. By utilizing a meta-learning framework based on\nFSL, we empowered the model to acquire general knowledge\nand effectively recognize new unseen modulations using a\nsmall number of samples, without the need for complete\nretraining. Furthermore, we improved the scalability of the\nclassifier by employing two Transformer-based encoders, al-\nlowing effective processing of signals with varying config-\nurations. Through extensive evaluations on the widely used\nRadioML2018.01A dataset, we demonstrated the effective-\nness of our proposed AMC method over existing techniques\nin all SNR ranges.\nDespite achieving rapid adaptation to a new set of modula-\ntions and providing a high classification performance of over\n90%, incorporating the modulations from the initial training\nstage poses a challenging task. This challenge can particularly\nbe critical in SDR platform scenarios demanding precise\nclassification across a diverse range of modulation types. In\nour future endeavors, we plan to address these challenges.\nREFERENCES\n[1] Shengliang Peng, Shujun Sun, and Yu-Dong Yao. A survey of modulation\nclassification using deep learning: Signal representation and data prepro-\ncessing. IEEE Transactions on Neural Networks and Learning Systems,\n33(12):7020–7038, 2021.\n[2] Shilian Zheng, Peihan Qi, Shichuan Chen, and Xiaoniu Yang. Fusion\nmethods for cnn-based automatic modulation classification. IEEE Access,\n7:66496–66504, 2019.\n[3] Octavia A Dobre, Ali Abdi, Yeheskel Bar-Ness, and Wei Su. Survey of\nautomatic modulation classification techniques: classical approaches and\nnew trends. IET communications, 1(2):137–156, 2007.\n[4] Fahed Hameed, Octavia A Dobre, and Dimitrie C Popescu. On the\nlikelihood-based approach to modulation classification. IEEE transactions\non wireless communications, 8(12):5884–5892, 2009.\n[5] Timothy James O’Shea, Tamoghna Roy, and T Charles Clancy. Over-the-\nair deep learning based radio signal classification. IEEE Journal of Selected\nTopics in Signal Processing, 12(1):168–179, 2018.\n[6] Thien Huynh-The, Cam-Hao Hua, Quoc-Viet Pham, and Dong-Seong\nKim. Mcnet: An efficient cnn architecture for robust automatic modulation\nclassification. IEEE Communications Letters, 24(4):811–815, 2020.\n[7] Godwin Brown Tunze, Thien Huynh-The, Jae-Min Lee, and Dong-Seong\nKim. Sparsely connected cnn for efficient automatic modulation recogni-\ntion. IEEE Transactions on Vehicular Technology, 69(12):15557–15568,\n2020.\n[8] Peihan Qi, Xiaoyu Zhou, Shilian Zheng, and Zan Li. Automatic modulation\nclassification based on deep residual networks with multimodal informa-\ntion. IEEE Transactions on Cognitive Communications and Networking,\n7(1):21–33, 2020.\n[9] Rui Zhang, Zhendong Yin, Zhilu Wu, and Siyang Zhou. A novel automatic\nmodulation classification method using attention mechanism and hybrid\nparallel neural network. Applied Sciences, 11(3):1327, 2021.\n[10] Seung-Hwan Kim, Jae-Woo Kim, Williams-Paul Nwadiugwu, and Dong-\nSeong Kim. Deep learning-based robust automatic modulation classifica-\ntion for cognitive radio networks. IEEE access, 9:92386–92393, 2021.\n[11] Thien Huynh-The, Quoc-Viet Pham, Toan-Van Nguyen, Thanh Thi\nNguyen, Daniel Benevides da Costa, and Dong-Seong Kim. Rannet:\nLearning residual-attention structure in cnns for automatic modulation\nclassification. IEEE Wireless Communications Letters, 11(6):1243–1247,\n2022.\n[12] Bachir Jdid, Kais Hassan, Iyad Dayoub, Wei Hong Lim, and Mastaneh\nMokayef. Machine learning based automatic modulation recognition\nfor wireless communications: A comprehensive survey. IEEE Access,\n9:57851–57873, 2021.\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual\nlearning for image recognition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–778, 2016.\nVOLUME 11, 2023 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3352634\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weis-\nsenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[15] Quan Zhou, Ronghui Zhang, Junsheng Mu, Hongming Zhang, Fangpei\nZhang, and Xiaojun Jing. Amcrn: Few-shot learning for automatic modu-\nlation classification. IEEE Communications Letters, 26(3):542–546, 2021.\n[16] Zilin Zhang, Yan Li, and Meiguo Gao. Few-shot learning of signal\nmodulation recognition based on attention relation network. In 2020 28th\nEuropean Signal Processing Conference (EUSIPCO), pages 1372–1376.\nIEEE, 2021.\n[17] Xiaoyang Hao, Zhixi Feng, Shuyuan Yang, Min Wang, and Licheng Jiao.\nAutomatic modulation classification via meta-learning. IEEE Internet of\nThings Journal, 2023.\n[18] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for\nfew-shot learning. Advances in neural information processing systems, 30,\n2017.\n[19] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\n[20] Ziqi Ke and Haris Vikalo. Real-time radio modulation classification\nwith an lstm auto-encoder. In ICASSP 2021 - 2021 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), pages\n4935–4939, 2021.\nJUNGIK JANG received the M.S. degree from\nthe Department of Software, Gachon University,\nSouth Korea, in 2023. Since 2021, he has been\na Researcher with the Intelligent Networking and\nComputing Laboratory (INC Lab.), Gachon Uni-\nversity. His current research interests include deep\nlearning based wireless sensing, domain adapta-\ntion and generative AI.\nJISUNG PYOis currently pursuing a B.S. degree\nat the School of Computing, Gachon University\nin South Korea. Since 2022, he has been working\nas a researcher at the Intelligent Networking and\nComputing Lab (INC Lab), Gachon University.\nHis research interests include Wi-Fi sensing and\nmeta-learning.\nYOUNG-IL YOONreceived the M.S. degree from\nChungnam National University, Daejeon, South\nKorea, in 2013. From 2012 to 2013, he was a\nSoftware Engineer with the Cloud Team, Naver\nCloud, contributing to the virtual machine. Since\n2013, he has been the Project Manager with the C4I\nResearch Center, LIG Nex1, contributing to the\ntactical radio systems. His current research inter-\nests include software frameworks and embedded\nLinux kernel.\nJAEHYUK CHOI (IEEE Member) received the\nPh.D. degree in Electrical Engineering and Com-\nputer Science from Seoul National University,\nSeoul, South Korea, in 2008. He is currently a\nProfessor with the Department of Software, Ga-\nchon University, Seongnam, South Korea. He was\nwith the Real-Time Computing Laboratory, Uni-\nversity of Michigan, Ann Arbor, MI, USA, as a\nPost-Doctoral Researcher from 2008 to 2011. His\ncurrent research interests include wireless/mobile\nsystems, Internet of Things connectivity and intelligent sensing systems.\n10 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3352634\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8276672959327698
    },
    {
      "name": "Scalability",
      "score": 0.7393027544021606
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5847941040992737
    },
    {
      "name": "Machine learning",
      "score": 0.5412641763687134
    },
    {
      "name": "Deep learning",
      "score": 0.5042744874954224
    },
    {
      "name": "Software-defined radio",
      "score": 0.4911138713359833
    },
    {
      "name": "Classifier (UML)",
      "score": 0.4583325982093811
    },
    {
      "name": "Transformer",
      "score": 0.4369141161441803
    },
    {
      "name": "Encoder",
      "score": 0.4340222477912903
    },
    {
      "name": "Telecommunications",
      "score": 0.10058918595314026
    },
    {
      "name": "Engineering",
      "score": 0.0946270227432251
    },
    {
      "name": "Voltage",
      "score": 0.0888095498085022
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I12832649",
      "name": "Gachon University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I4210089444",
      "name": "GS Caltex (South Korea)",
      "country": "KR"
    }
  ]
}