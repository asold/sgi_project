{
  "title": "Accurate prediction of synthesizability and precursors of 3D crystal structures via large language models",
  "url": "https://openalex.org/W4412441324",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5085689796",
      "name": "Zhilong Song",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A5073415244",
      "name": "Shuaihua Lu",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A5050615248",
      "name": "Ming‐Gang Ju",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A5044259453",
      "name": "Qionghua Zhou",
      "affiliations": [
        "Southeast University",
        "Suzhou Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5020585562",
      "name": "Jinlan Wang",
      "affiliations": [
        "Southeast University",
        "Suzhou Research Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3096478584",
    "https://openalex.org/W2478294658",
    "https://openalex.org/W2884430236",
    "https://openalex.org/W4401151749",
    "https://openalex.org/W4388847513",
    "https://openalex.org/W3034773351",
    "https://openalex.org/W4200559673",
    "https://openalex.org/W3042344738",
    "https://openalex.org/W4323907504",
    "https://openalex.org/W2888395196",
    "https://openalex.org/W4390510489",
    "https://openalex.org/W4399781535",
    "https://openalex.org/W4406472463",
    "https://openalex.org/W2979285519",
    "https://openalex.org/W2997100726",
    "https://openalex.org/W3118507387",
    "https://openalex.org/W4388421323",
    "https://openalex.org/W3111047061",
    "https://openalex.org/W2801403847",
    "https://openalex.org/W3128520882",
    "https://openalex.org/W2801006326",
    "https://openalex.org/W4311746758",
    "https://openalex.org/W4360601153",
    "https://openalex.org/W1801166380",
    "https://openalex.org/W4386167257",
    "https://openalex.org/W4321494448",
    "https://openalex.org/W3211705646",
    "https://openalex.org/W2920535269",
    "https://openalex.org/W3094175649",
    "https://openalex.org/W4318218504",
    "https://openalex.org/W4405114800",
    "https://openalex.org/W4391561379",
    "https://openalex.org/W4389991792",
    "https://openalex.org/W4405114187",
    "https://openalex.org/W4396723768",
    "https://openalex.org/W4385027818",
    "https://openalex.org/W4400524753",
    "https://openalex.org/W2899070097",
    "https://openalex.org/W2176516200",
    "https://openalex.org/W4404792833",
    "https://openalex.org/W2023972500",
    "https://openalex.org/W4206633671",
    "https://openalex.org/W1992985800",
    "https://openalex.org/W2170214641",
    "https://openalex.org/W2278970271",
    "https://openalex.org/W3039596418",
    "https://openalex.org/W2070942956",
    "https://openalex.org/W2083222334",
    "https://openalex.org/W4402423358",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W2980932864",
    "https://openalex.org/W4281476575",
    "https://openalex.org/W4404408679",
    "https://openalex.org/W6606404423",
    "https://openalex.org/W1974668858",
    "https://openalex.org/W1975521355",
    "https://openalex.org/W2164591957",
    "https://openalex.org/W2778048962",
    "https://openalex.org/W2290992327",
    "https://openalex.org/W4387561082",
    "https://openalex.org/W4401042360",
    "https://openalex.org/W3081168214",
    "https://openalex.org/W3105152113",
    "https://openalex.org/W3206992093",
    "https://openalex.org/W2766856748",
    "https://openalex.org/W2778051509",
    "https://openalex.org/W2015197254",
    "https://openalex.org/W2989624650",
    "https://openalex.org/W6921178206",
    "https://openalex.org/W6893217695",
    "https://openalex.org/W3105321997",
    "https://openalex.org/W3104644561",
    "https://openalex.org/W4399996512",
    "https://openalex.org/W2913999598"
  ],
  "abstract": "Accessing the synthesizability of crystal structures is crucial for transforming theoretical materials into real-world applications. Nevertheless, there is a significant gap between actual synthesizability and thermodynamic or kinetic stability commonly used to screen synthesizable structures. Herein, we develop the Crystal Synthesis Large Language Models (CSLLM) framework, which utilizes three specialized LLMs to predict the synthesizability of arbitrary 3D crystal structures, possible synthetic methods, and suitable precursors, respectively. We construct a comprehensive dataset including synthesizable/non-synthesizable crystal structures and develop an efficient text representation for crystal structures to fine-tune LLMs. Our Synthesizability LLM achieves state-of-the-art accuracy (98.6%), significantly outperforming traditional synthesizability screening based on thermodynamic and kinetic stability. Its outstanding generalization ability is further demonstrated in experimental structures with complexity considerably exceeding that of the training data. Furthermore, both the Method and Precursor LLMs exceed 90% accuracy in classifying possible synthetic methods and identifying solid-state synthetic precursors for common binary and ternary compounds, respectively. Leveraging CSLLM, tens of thousands of synthesizable theoretical structures are successfully identified, with their 23 key properties predicted using accurate graph neural network models.",
  "full_text": "Article https://doi.org/10.1038/s41467-025-61778-y\nAccurate prediction of synthesizability and\nprecursors of 3D crystal structures via large\nlanguage models\nZhilong Song 1, Shuaihua Lu1,M i n g g a n gJ u1, Qionghua Zhou 1,2 &\nJinlan Wang 1,2\nAccessing the synthesizability of crystal structures is crucial for transforming\ntheoretical materials into real-world applications. Nevertheless, there is a sig-\nniﬁcant gap between actual synthesizability and thermodynamic or kinetic\nstability commonly used to screen synthesizable structures. Herein, we\ndevelop the Crystal Synthesis Large Language Models (CSLLM) framework,\nwhich utilizes three specialized LLMs to predict the synthesizability of arbi-\ntrary 3D crystal structures, possible synthetic methods, and suitable pre-\ncursors, respectively. We construct a comprehensive dataset including\nsynthesizable/non-synthesizable crystal structures and develop an ef ﬁcient\ntext representation for crystal structures to ﬁne-tune LLMs. Our Synthesiz-\nability LLM achieves state-of-the-art accuracy (98.6%), signiﬁcantly out-\nperforming traditional synthesizability screening based on thermodynamic\nand kinetic stability. Its outstanding generalization ability is further demon-\nstrated in experimental structures with complexity considerably exceeding\nthat of the training data. Furthermor e, both the Method and Precursor LLMs\nexceed 90% accuracy in classifying possible synthetic methods and identifying\nsolid-state synthetic precursors for common binary and ternary compounds,\nrespectively. Leveraging CSLLM, tens of thousands of synthesizable theore-\ntical structures are successfully identiﬁed, with their 23 key properties pre-\ndicted using accurate graph neural network models.\nThe journey of materials design has undergone a remarkable evolu-\ntion, characterized by four paradigms 1.W h i l et h e ﬁrst and second\nparadigms built the foundation through trial-and-error experiments\nand scienti ﬁc theories, respectively, the third paradigm leveraged\ncomputational methods, such as density functional theory (DFT) for\nefﬁcient atomic-level simulations to identify numerous functional\nmaterials\n2,3. The fourth paradigm represents a transformative leap by\nharnessing the power of accumulated data and machine learning (ML),\nsigniﬁcantly accelerating materials discovery by predicting properties\nrapidly and accurately\n4–13. Notably, generative models within this\nparadigm facilitate the inverse design of novel material structures14–17,\nsuch as semiconductors, zeolites and metal-organic frameworks 18–20,\novercoming the limitation of traditional ML restricted to known\nstructures. While the computational and data-driven paradigms have\ncollectively identiﬁed millions of candidate materials with excellent\nproperties, a great challenge remains —the synthesizability of pre-\ndicted material structures, which is imperative for transforming the-\noretical innovations into real-world applications.\nConventional approaches for identifying promising synthesizable\nmaterial structures typically involve assessing thermodynamic forma-\ntion energies or energy above convex hull via DFT calculations\n16,17,21,22.\nHowever, numerous structures with favorable formation energies have\nReceived: 12 November 2024\nAccepted: 25 June 2025\nCheck for updates\n1Key Laboratory of Quantum Materials and Devices of Ministry of Education, School of Physics, Southeast University, Nanjing, China. 2Suzhou Laboratory,\nSuzhou, China. e-mail: qh.zhou@seu.edu.cn; jlwang@seu.edu.cn\nNature Communications|         (2025) 16:6530 1\n1234567890():,;\n1234567890():,;\nyet to be synthesized 23, while various metastable structures are syn-\nthesized with less favorable formation energies at the convex hull\nminimum\n24. An alternative approach involves assessing kinetic stability\nby computationally expensive phonon spectra analyses25.N o n e t h e l e s s ,\nmaterial structures with imaginary phonon frequencies can still be\nsynthesized26. Jones et al. combined basin hypervolume with thermo-\ndynamic stability to explain the synthesis of metastable silicon\npolymorphs\n27. Phase diagrams offer a more direct correlation with\nsynthesizability, as they delineate stable phases under varying tem-\nperatures, pressures, and compositions\n28,29. However, constructing the\nfree energy surface for all possible phases as a function of these vari-\nables is computationally impractical. Moreover, the synthesis of\nmaterials is a complex process in ﬂuenced by the choice of precursors\nand reaction conditions\n30. Thus, there is an urgent need for a method\nthat can accurately and rapidly predict synthesizability and suitable\nprecursors, thereby providing direct guidance for experimental\nsynthesis.\nML has shown promising attempts in predicting material synthe-\nsizability, such as the SynthNN model for assessing the synthesizability\nof inorganic crystals based on their compositions\n31. When predicting\nthe synthesizability of crystal structures, constructing negative sam-\nples (non-synthesizable materials) is a major challenge. The simplest\napproach treats structures with unknown synthesizability as negative\nsamples, which inevitably introduces numerous synthesizable\nstructures\n32. Another method collects unobserved structures from\nwell-studied compositions, yet such datasets are limited in scale (only\n3000 samples) 33. More commonly, semi-supervised methods like\npositive-unlabeled (PU) learning are used —achieving over 75% and\n87.9% accuracy in synthesizability prediction for 2D MXenes 34 and 3D\ncrystals35, respectively. Additionally, the teacher-student dual neural\nnetwork further improved the prediction accuracy for 3D crystals to\n92.9%\n36. Notably, our previous work utilized failed experimental data\nas negative samples, though constrained to 2D silver/bismuth organic-\ninorganic hybrid perovskites\n12.\nOverall, these ML methods for synthesizability prediction exhibit\nmoderate accuracy or are conﬁned to speciﬁc material systems, while\nalso lacking the capability to simultaneously predict synthesis routes\nand precursors. Recent advances in large language models (LLMs),\nsuch as ChatGPT 37 and open-source LLMs like LLaMA 38,h a v er e v o l u -\ntionized various research domains. In materials science, LLMs facilitate\npredicting material properties\n39, optimizing experimental workﬂows40,\ngenerating crystal structures 41, proposing synthesis strategies for\norganic molecules42, metal-organic frameworks43, and evaluating syn-\nthesizability based on inorganic material formulas 44. These demon-\nstrate the exceptional capabilities of LLMs to learn from text due to\ntheir extensive architectures and vast training datasets. Thus, LLMs\nhold great potential to predict the synthesizability of theoretical\ncrystal structures. However, challenges persist in curating data toﬁne-\ntune LLMs for ensuring accuracy and reliability. Crystal structures\nexhibit vast diversity, yet the available data (10\n5–106) pales in com-\nparison to that of organic molecules (10 8–109)45,46.M e a n w h i l e ,t h e r ei s\nan absence of an effective text representation method for crystal\nstructures, akin to the SMILES\n47 notation for organic molecules.\nAdditionally, the issue of“hallucination” in LLMs, where the generated\ninformation is not grounded in real-world facts 48,p o s e sar i s kt ot h e\nreliability of synthesizability predictions.\nIn this work, we address the prediction of crystal structure syn-\nthesizability into three tasks: i) establishing whether a structure is\nsynthesizable, ii) pinpointing the appropriate synthetic method (solid-\nstate or solution), and iii) identifying the appropriate precursors.\nThese challenges are addressed using three ﬁne-tuned LLMs, which\ncollectively constitute a framework denoted as Crystal Synthesis Large\nLanguage Models (CSLLM), as illustrated in Fig. 1a. We curated a\nbalanced dataset containing 70,120 synthesizable crystal structures\nfrom the Inorganic Crystal Structure Database (ICSD)\n49 and 80,000\nnon-synthesizable structures screened from 1,401,562 theoretical\nstructures via a PU learning model 35. To facilitate ef ﬁcient LLM ﬁne-\ntuning, we introduced a te xt representation termed “material string”\nthat integrates essential crystal information. The Synthesizability LLM\nachieves a remarkable 98.6% accuracy on testing data, outperforming\nthermodynamic method—energy above hull ≥0.1 eV/atom (74.1%) and\nkinetic method—lowest frequency of the phonon spectrum ≥− 0.1 THz\n(82.2%). Moreover, it demonstrates exceptional generalization by\npredicting the synthesizability of additional testing structures,\nachieving 97.9% accuracy even for complex structures with large unit\ncells. The Method and Precursor LLMs achieve 91.0% classi ﬁcation\naccuracy and 80.2% precursor prediction success. We also calculated\nreaction energies and performed a combinatorial analysis to suggest\nmore potential precursors. Furthermore, CSLLM was used to assess\nthe synthesizability of 105,321 theoretical structures, and 45,632 syn-\nthesizable materials were screened out, whose 23 key properties were\nfurther predicted in batches by GNNs. Finally, A user-friendly CSLLM\ninterface was developed for automatic synthesizability and precursor\npredictions from uploaded crystal structure ﬁles (Fig S1). These\nachievements arise from domain-focusedﬁne-tuning, which aligns the\nbroad linguistic features of LLMs with material features critical to\nsynthesizability, thereby reﬁning its attention mechanisms and redu-\ncing hallucinations. Such domain adaptation has been widely reported\nto yield substantial performance boosts for LLMs even when only\nrelatively small datasets are available\n39,50. This groundbreaking frame-\nwork bridges the gap between theoretical predictions and practical\nsynthesis, thereby paving the way for the ef ﬁcient discovery of novel\nmaterials.\nResults\nBalanced and comprehensive dataset for predicting\nsynthesizability\nTo develop a robust LLM for predicting the synthesizability of 3D\ncrystal structures, it is essential to construct a comprehensive dataset\ncomprising synthesizable (positive examples) and non-synthesizable\n(negative examples) materials. The ICSD\n49 serves as a reliable source of\nexperimentally validated material structures, all of which are con-\nﬁrmed to be synthesizable. For our study, we meticulously selected\n70,120 crystal structures from the ICSD, each containing no more than\n40 atoms and seven different elements, as the positive examples.\nNotably, disordered structures within the ICSD were excluded from\nour selection, as our objective focuses on the prediction of the syn-\nthesizability of ordered crystal structures.\nTo construct a set of negative examples, we employed a pre-trained\nPU learning model developed by Jang et al.\n35.T h i sm o d e lg e n e r a t e sa\nCLscore for each structure, with scores below 0.5 indicative of non-\nsynthesizability. This enables us to discern non-synthesizable structures\nfrom a vast pool of 1,401,562 crystal structures sourced from the\nmaterials project (MP)\n51, Computational Material Database 52,O p e n\nQuantum Materials Database53,a n dJ o i n tA u t o m a t e dR e p o s i t o r yf o r\nVarious Integrated Simulations (JARVIS)54. In order to build a balanced\ndataset, we calculated the CLscores for all 1,401,562 structures and\nselected the 80,000 structures with the lowest CLscores ( i.e., CLscore\n<0.1) as our non-synthesizable examples (Fig. 1d ) .N o t et h a tw ec o m -\nputed the CLscores for our 70,120 positive examples and 98.3% of these\nstructures had CLscores greater than 0.1, which afﬁrmed the validity of\nour CLscore threshold.\nAll 150,120 crystal structures are visualized using t-SNE\n55,c o v e r i n g\nseven crystal structures: cubic, hexagonal, tetragonal, orthorhombic,\nmonoclinic, triclinic, and trigonal with the cubic system being the most\nprevalent (Fig.1b). Additionally, our dataset includes crystal structures\nwith 1–7 elements, predominantly featuring 2–4 elements, and covers\natomic numbers 1 –94 from the periodic table, excluding atomic\nnumbers 85 and 87 (Fig S2). This demonstrates that our dataset is both\nbalanced and comprehensive, laying a solid foundation for the\nArticle https://doi.org/10.1038/s41467-025-61778-y\nNature Communications|         (2025) 16:6530 2\nsubsequent training of a high- ﬁdelity LLM for predicting the synthe-\nsizability of inorganic crystal structures.\nText representation of material structures andﬁne-tuning of\nsynthesizability LLM\nGiven that LLMs process text inputs, it is essential to represent material\nstructures in the simplest reversible text format that includes compre-\nhensive information on the lattice, c omposition, atomic coordinates,\nand symmetry. The most common text representations of crystal\nstructures are the CIF format\n56 and the POSCAR format used by the\nVienna Ab initio Simulation Package57. These formats provide detailed\ni n f o r m a t i o no nt h el a t t i c e ,c o m p o s ition, and atomic coordinates, but\ncontain redundant information. For example, multiple atomic coordi-\nnates at the same Wyckoff position can be inferred from one atomic\ncoordinate along with the space group and Wyckoff position symbols.\nTherefore, it is unnecessary to list all the atomic coordinates within the\ncell. Additionally, POSCAR format lacks symmetry-related information,\nalthough it is more concise than CIF. To address these issues, we pro-\npose a text representation for crystal structures namedmaterial string:\nS P|a ,b ,c , α, β, γ |( A S\n1-WS1[WP1])- > (AS2-WS2[WP2])- > (ASN-WSN[WPN]),\nwhere the SP, (a, b, c, α, β, γ), AS, WS and WP are the space group\nnumber, lattice constants, atom symbol, Wyckoff position symbol and\n1\n2\n3\n4\n5\n6\n7\n0 20000 40000 60000 80000\nNumber of structures\nNumber of elements\na\nb\nc\nde\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5Density\nCLScore\n0.104\nFormula N Precursors:\nFormula 2 Precursors:\nConvert\nFine tunning\nCrystal structures Synthesizability LLM\nICSD\nPU learning\nMP OQMD\nJARVIS \nLoRA\nCMD\nSynthesizable\nUnsynthesizable\n80000\n70120\nScreening\nMaterial string 1 False:\nMaterial string 2 True:\nMaterial string N False:\nFormula 1 Precursors:\nFine tunning\nPrecursor\nLLM\nLoRA\nSolid-state synthesis \nprecursors\nRemove \nduplicates\nRemove \nduplicates\nMethod\nLLM\n221 | 3.897,3.897,3.897,90.00,90.00,90.00 | (Ca-1a[0. 0. 0.])->(Ti-1b[0.5 0.5 0.5])->(O-3c[0.  0.5 0.5])\nMaterial string\nSpace group Lattice parameters Atom types and Wyckoff positionsConvert\nFormulas\nThis material structure is synthesizable / unsynthesizable\nPrecursors: [precursor 1, precursor 2, … precursor N]: ΔH\nTrue/False\n23 property\nGNN models\nSynthesizable structures \nwith superior properties\nSynthesizability \nLLM\nMethod \nLLM\nPrecursor LLM\nUse material \nstring as prompt\nMethod\nMethod\nPrecursors\nCubic\nMonoclinic\nTrigonal\nTriclinic\nTetragonal\nOrthorhombic\nHexagonal\nFig. 1 | Crystal Synthesis Large Language Models (CSLLM) framework and data\nfor ﬁne-tuning large language model (LLM). aOverall workﬂow for predicting the\nsynthesizability and recommending precursors using the LLM. Crystal structures\nwere ﬁrst converted into material strings. Their synthesizability was then predicted\nby the Synthesizability LLM. Based on the chemical formulas, the Precursor and\nMethod LLMs provide a batch of potential precursors along with their reaction\nenergies. 23 material properties of synthesizable structures were further predicted\nby graph neural networks. b t-distributed Stochastic Neighbor Embedding (t-SNE)\nvisualization of 150,120 material structures along with statistical data on crystal\nsystems and the number of elements, which cubic, hexagonal, tetragonal, orthor-\nhombic, monoclinic, triclinic, and trigonal structures represented by gray, purple,\ncyan, red, orange, blue, and green dots, respectively. c Training process of the\nSynthesizability LLM. A balanced dataset was constructed from Inorganic Crystal\nStructure Database (ICSD)\n49, Materials Project (MP)51, Open Quantum Materials\nDatabase (OQMD)53, Computational Material Database (CMD)52,a n dJ A R V I S( J o i n t\nAutomated Repository for Various Integrated Simulations)54 using a positive-\nunlabeled learning model, converted to material strings in batches, and then ﬁne-\ntuned LLMs with low-rank adaptation (LoRA). d Crystal-likeness score (CLScore)\ndistribution for 1,401,562 structures and dash line refers to the CLScore threshold\nfor ﬁltering out non-synthesizable structures.e Training process for the Precursor\nand Method LLMs. Data pairs of chemical formulas with their precursors and syn-\nthetic methods were collected from the literature and ﬁne-tuned with LoRA.\nArticle https://doi.org/10.1038/s41467-025-61778-y\nNature Communications|         (2025) 16:6530 3\nWyckoff position of a crystal structur e, respectively. For instance, the\nmaterial string of the CaTiO3 structure (mp-4019) from MP database is\n“221 | 3.897.3.897.3.897.90.00.90.00.90.00 | (Ca-1a[0.0.0.])->(Ti-1b[0.5\n0.5 0.5])- > (O-3c[0.5 0.5 0.5]) ”(Fig. 1a). Notably, crystal structures are\nﬁrst converted into primitive cells before converting to material strings.\nThe material string not only contains all the crystallographic\ninformation but also is as concise as possible. For example, the con-\nverted CaTiO3 structure has a string length of only 102 characters,\nsigniﬁcantly shorter compared to 1817 characters in CIF format and\n1644 characters in POSCAR format. This high-information-density text\nis beneﬁcial for improving the ﬁne-tune performance of LLMs\n58.T h u s ,\nwe converted 150,120 unique crysta l structures and their synthesiz-\nability labels into material strings —True/False data pairs, then split\nthem into training and testing sets in a 9:1 ratio. Notably, due to the\ndeduplication of crystal structures, the material strings in the testing\nset did not appear in the training set. Some examples of these data\npairs are shown in Fig S3. Considering the substantial expenses asso-\nciated with the comprehensive ﬁne-tuning of LLMs, we utilized the\nlow-rank adaptation (LoRA) method\n59 to efﬁciently reﬁne the LLaMA3-\n8B model. This approach enables cost-effective tuning by targeting the\noptimization of a limited subset of parameters.\nThe ﬁne-tuned synthesizability LLM achieves remarkably high\naccuracy on the test set, with only 88 incorrect predictions out of\n7529 unseen synthesizable crystal structures (Fig. 2a). To demon-\nstrate that the outstanding ability of LLM to distinguish synthesiz-\nable crystal structures is a result of our ﬁne-tuning rather than\ninherent to the model, we trained the synthesizability LLM with 0,\n1000, 10,000, 50,000, and 135,108 data pairs for comparison. As\nshown in Fig. 2b, the recall (the proportion of actual positives cor-\nrectly identi ﬁed) and fallout (the proportion of actual negatives\nincorrectly identiﬁed as positive) of the untuned LLM are only 53.1%\nand 49.0%, respectively, almost equivalent to random guessing.\nAfter ﬁne-tuning with 1000 data points, the recall and fallout of\nsynthesizability LLM improve to 79.8 and 9.36%, demonstrating the\npowerful ability of LLMs to learn patterns from textual data. Ulti-\nmately, the precision, accuracy, recall, and F1 score reach 98.8, 98.6,\n98.4, and 98.6%, respectively. The detailed testing performance of\nthe ﬁve LLMs in Fig. 2b are shown in Table S1 and Fig S4. Further-\nmore, we interpret the synthesizability LLM using perturbation-\nbased feature ablation method, revealing the most contributive\nfeatures (i.e., space group number or lattice constants or atom\nsymbol or Wyckoff position) in the prompts (material string) for\nsynthesizability prediction (details are discussed in Supplementary\nNote 1 and Fig S5 –9). “This function is incorporated into the CSLLM\nframework, the importance score of each feature is automatically\ncalculated after synthesizability prediction. ”\nSynthesizable Unsynthesizable\nSynthesizable Unsynthesizable\nTruth\nPredicted\n50\n1640\n3230\n4820\n6410\n8000\nNumber of structures\n0.06%\n0.04%\n0.06%\n0.03%\n0.04%\n0.04%\n0.04%\n0.06%\n0.05%\n0.04%\n98.8\n98.9\n98.8\n98.8\n98.8\n98.7\n98.8\n98.9\n98.8\n98.8\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n05 1 0\nNumber of differences\nRun\n98.6 98.8 99.0\nRecall (%)\n濕濖\n濗濘\nPrecision Accuracy\nRecall F1 score\n98.8%\n98.4%\n98.6%\n98.6%\n98.6\n90.0\n82.2\n74.1\n101\n104 104\n102\n70\n80\n90\n100Success rate (%)Success rate (%)\nMethod\nCSLLM Phonon+E hull Phonon Ehull\n100\n101\n102\n103\n104\nTime (s)\nPrecision\n Accuracy\nRecall\n F\n1 \nscore\n9\n8\n.\n8\n%\n9\n8\n.\n4\n%\n9\n8\n.\n6\n%\n9\n8\n.\n6\n%\n0 20k 40k 60k 80k 100k 120k 140k 160k\n50\n60\n70\n80\n90\n100\nTraining data size\nRecall (%)\n0\n10\n20\n30\n40\n50\nFallout (%)\nFig. 2 | Performance of Synthesizability large language model (LLM).\na Confusion matrix of the Synthesizability LLM on the test set. b Recall and fallout\nof the Synthesizability LLM under different amounts of training data. The pie chart\nsubﬁgure shows the ﬁnal classiﬁcation metrics of the model trained with the full\ndataset.c Variations in results and recall of Synthesizability LLM for ten additional\ntest set inferences (Runs). The left subplot shows the number and percentages of\ndifferences between each run and initial inference results, while the right subplot\ndisplays the recall value of each run. d Comparison of the accuracy and runtime of\nsynthesizability predictions for crystal structures between the Synthesizability LLM\nand density functional theory computational methods (E\nhull refers to the energy\nabove hull).\nArticle https://doi.org/10.1038/s41467-025-61778-y\nNature Communications|         (2025) 16:6530 4\nHowever, identical prompt input into LLMs often yields different\nor even erroneous results due to the inherent hallucination problem\npresent in LLMs. Hallucination, deﬁned as the generation of plausible\nyet incorrect or nonsensical information, is a well-documented issue in\nLLMs, affecting the reliability across various applications\n36.T o\ndemonstrate that our synthesizability LLM can reliably predict the\nsynthesizability of crystal structures without hallucination, we con-\nducted an additional ten inference runs on the test set. As shown in\nFig. 2c, the largest discrepancy in synthesizability predictions com-\npared to the ﬁrst run is only 10 instances, representing 0.06% of the\ntotal. The recall rate varies by no more than 0.2%, afﬁrming the stability\nof the synthesizability LLM.\nBuilding on this stability, we fur ther compared the synthesiz-\nability LLM with three DFT-based methods commonly used for\nscreening experimentally synthesizable material structures: (i) energy\nabove hull (E\nhull) ≥ 0.1 eV/atom, (ii) the lowest frequency of the phonon\nspectrum (Phonon) ≥ -0.1 THz, and (iii) both Ehull ≥ 0.1 eV/atom and\nPhonon≥− 0.1 THz. This comparison utilized data from 1512 structures\nin the MP database for which phonon spectrum calculations had been\nconducted. TheE\nhull, the lowest phonon frequency, and synthesizable/\nnon-synthesizable labels of these 1512 structures are collected. The\ndistribution of the lowest phonon frequency is shown in Fig S10, where\n34.2% of these structures have imaginary frequencies. We then per-\nformed inferences on these structu res using the synthesizability LLM\nand assessed the accuracy of the aforementioned methods based on\nthe proportion of experimentally synthesized structures that met the\nabove criteria. As shown in Fig. 2d, the synthesizability LLM not only\nachieves a signi ﬁcantly higher accuracy of 98.6% compared to DFT\nmethods, but also reduces runtime by 1 –3 orders of magnitude,\ndemonstrating its capability to accurately and rapidly predict the\nsynthesizability of 3D crystal materials.\nFine-tuning of method and precursor LLMs\nAfter identifying synthesizable material structures, suitable synthesis\nrecipes (methods and precursors) need to be selected before con-\nducting experimental synthesis. T he prevalent synthetic methods\nencompass the solid-state and solution method, each requiring dif-\nferent precursors. To build a comprehensive dataset for synthesis\nrecipes prediction, we integrated data from 19,488 solid-state synth-\nesis recipes collected by Kononova et al.\n60 and 35,675 solution synth-\nesis recipes collected by Wang et al. 61. Additionally, we extracted\n6,136 solid-state/solution synthesis recipes from the literature span-\nning 2021–2024, utilizing scripts provided by Kononova et al.\n60.C o l -\nlectively, these data link each chemical formula to a speci ﬁcs y n t h e t i c\nmethod and a set of precursors, with a subset also containing struc-\ntural information. After deduplication, we identi ﬁed 31,780 unique\nchemical formula-method/precursors data pairs, which cover a wide\nrange of elements with atomic numbers from 1 to 98 (Fig S12). All these\nunique synthesis data were utilized for the subsequent LLM ﬁne-tun-\ning, rather than the smaller subset with structural information, as the\nlatter only has 3650 unique structure-method/precursor data pairs,\nwhich are inadequate for effective ﬁne-tuning of LLM (Fig S13).\nTo anticipate potential precursors, it is imperative to ﬁrst ascer-\ntain the applicable synthetic methodology for a given chemical for-\nmula. Thus, the LLaMA3-8B model was initially ﬁne-tuned using the\n31,780 formula-method pairs from the abovementioned dataset. The\ndataset distribution includes solid-state, solution, and combined\nmethods, which account for 80.64, 12.22, and 7.14% of the entries,\nrespectively, as illustrated in Fig. 3a. 90% of the dataset was employed\nfor ﬁne-tuning, with the residual 10% reserved for testing purposes.\nThe Method LLM achieves an overall classi ﬁcation accuracy of 91.0%\non the test dataset, signi ﬁcantly outperforming the pre- ﬁne-tuned\nLLM, which achieves only 38.8% (Fig S14a). However, the prediction\naccuracy varies across different synthesis methods due to the imbal-\nanced distribution of the dataset. Speciﬁcally, as shown in Fig. 3b, the\nmodel performs well in predicting solid-state synthesis (97.98% accu-\nracy) but exhibits lower accuracy for solution-based synthesis (51.42%)\nand combined methods (77.65%). This discrepancy is attributed to the\nlimited training data of the solution and combined methods, high-\nlighting the inﬂuence of data availability on model performance. Fur-\nthermore, this Method LLM exhibits consistency across 10 inference\ntrials, with the largest discrepancy observed in 18 instances (0.58%)\nand a maximum ﬂuctuation in the accuracy of 0.4%, as depicted\nin Fig. 3c.\nNext, we split the 31,780 formula-precursors data pairs into\ntraining and test sets in a 9:1 ratio and ﬁne-tuned another LLaMA3-8B\nmodel to predict possible precursors for arbitrary chemical formulas\nderived from crystal structures. The prediction accuracy for the solid-\nstate method reaches 80.2% (Fig.4a), signiﬁcantly higher than the pre-\nﬁne-tuned accuracy of 5.7% on the same test set (Fig S14b). However,\nthe accuracy for the solution method is only 39.7%. Thus, we focused\non predicting precursors of solid-state synthesis and conducted an in-\ndepth analysis of the LLM prediction of solid-state precursors. In the\ntest set, discrepancies between predicted and actual precursors fell\ninto two cases (Fig. 4a): (i) cases of missing or additional precursors\n(6.7%), and (ii) cases where the number of precursors is correct but\nincludes incorrect ones (13.1%). In case (i), most (63.0%) predictions\nmiss one precursor (Fig. 4b). In case (ii), most (73.5%) errors involve a\nsingle incorrect precursor (Fig. 4c). Furthermore, we analyzed the\nabove two cases and overall success rates by the number of elements,\nas shown in Table 1 and Fig S15 –20. Notably, the success rate of\ncommon binary and ternary materials exceeds 91%, with cases of\nprecursor prediction errors below 5%. For compounds with 4 –7 ele-\nments, the success rate is relatively low (76.5, 68.4, 56.8, and 50.0%,\nrespectively) due to limited training data; however, most precursors\nare mis-predicted by just 1–2 incorrect precursors. It is noteworthy that\n7.1%\n12.2%\n80.6%\nSolid Solution Soli.|Solu.\nSolid Solution Soli.|Solu.\nTruth\nPredicted\n10\n554\n1098\n1642\n2186\n2730\nNumber of materialsSolutionSolid\nSoli. | Solu.\n濕 濖 濗\n0.52%\n0.40%\n0.55%\n0.38%\n0.40%\n0.55%\n0.58%\n0.52%\n0.43%\n0.49%\n90.7\n91.1\n90.9\n90.8\n90.9\n90.7\n90.7\n90.9\n91.1\n90.9\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0 5 10 15 20 25\nNumber of differences\nRun\n90.6 90.9 91.2\nTotal accuracy (%)\nFig. 3 | Performance of Method large language model (LLM). aProportion of\nsolid-state, solution, and dual-method synthesis among the 31,780 synthesis data\npoints. “Soli.|Solu.” refers to “Solid|Solution” (i.e., the combined methods).\nb Confusion matrix of the Methods LLM on the test set. c Variations in results and\naccuracy of the Methods LLM for ten additional test set inferences compared to the\ninitial results.\nArticle https://doi.org/10.1038/s41467-025-61778-y\nNature Communications|         (2025) 16:6530 5\nthe success rate of compounds composed of three, ﬁv e ,s i x ,o rs e v e n\nelements exhibits a linear correlation with the size of the training\ndataset. In contrast, binary and quaternary compounds do not con-\nform to this linear relationship (Fig S21). Due to the limited dataset of\nseptenary compounds (90 samples), which lacks representativeness,\nwe exclude them from our analysis. This discrepancy may be attrib-\nuted to the relatively simple precursor composition of binary com-\npounds (typically involving only two types), whereas quaternary\ncompounds often derive from a more diverse array of precursors\n(generally ranging from three to ﬁve types). Moreover, this Precursor\nLLM passed ten stability tests, with a maximum discrepancy of only\n0.57% in inference results and a maximum success rate variation of\n0.3% (Fig. 4d). The above analysis demonstrates that our Precursor\nLLM can provide accurate and robust precursor suggestions.\nBy combining the ﬁne-tuned method and precursor LLMs, the\nprecursors of solid-state synthesis can be recommended based on the\nchemical formulas of materials that are identiﬁed as synthesizable. To\nfurther improve the reliability and practical applicability of our\napproach, we aimed to calculate the reaction energies using the MP\ndatabase and explored alternative precursor sets. However, current\nreaction equations only include the target materials, missing bypro-\nducts such as CO ₂ and H₂O. To address this, we collected 30 unique\nbyproduct sets from 31,780 material synthesis recipes and iteratively\ntested these sets to balance the stoichiometry of reaction equations.\nNext, to calculate the reaction energies, we extracted the total energies\nof the most stable structures corresponding to the chemical formulas\nin the reaction equations from the MP database (Fig.4e). Furthermore,\na l lp o s s i b l ea l t e r n a t i v ep r e c u r s o rs e t sw e r ei d e n t iﬁed by permuting\nand combining elements from the initial precursor suggestions (e.g.,\nY\n2O3 c o u l db er e p l a c e dw i t hY O2 or Y and O 2). These combinations\nwere rebalanced and their reaction energies were recalculated\n(Fig S22). This comprehensive approach broadens the range of\npotential synthetic routes, making the model more reliable and\napplicable for real-world synthesis design. Finally, the whole workﬂow\nof CSLLM is fully automated and integrated within a graphical user\ninterface (Fig S1), users can simply upload a structure ﬁle (CIF or\nPOSCAR format) or type in material string to predict the synthesiz-\nability, possible synthetic method, and precursors. The GUI usage\nexample is shown in https://github.com/szl666/CSLLM.\nExperimental structure validation and identiﬁcation of synthe-\nsizable material structures with outstanding properties\nAbove, we have demonstrated that CSLLM is capable of accurately and\nrapidly assessing the synthesizability of material structures in the\ntraining set and in-domain test set, as well as identifying the precursors\nrequired for synthesis. However, these models lacked experimental\nvalidation to conﬁrm their generalization capability. Additionally, our\nsynthesizability LLM was trained on crystal structures with cells con-\ntaining no more than 40 atoms and seven elements, whereas large\nmaterials databases like the MP contain structures exceeding this limit.\nTherefore, we sought to address these limitations by expanding the\napplication of our synthesizability model to evaluate 20,237 experi-\nmentally synthesized structures in the MP database outside the train-\ning data. The predictive accuracy for structures with numbers of atoms\nranging from 1 to 275 atoms remained consistently high, exceeding\n9 5 %a c r o s sa l ls i z e s( F i g .5a). Notably, the accuracy did not decline for\nlarger structures (more than 100 atoms). With an average accuracy of\n97.8%, the nearly uniform accuracy distribution underscores the\nremarkable generalization capability of the synthesizability LLM even\nwhen applied to structures signi ﬁcantly exceeding the complexity of\nthe training set. Moreover, A previous study warned of spurious high\naccuracy of synthesizability prediction due to large lattice volume bias\nwhen using different data sources\n62. However, our crystal structure\nscreening prevented such signiﬁcant bias, and the synthesizability LLM\nconsistently maintained above 93% accuracy across all lattice volumes\n(details in Supplementary Note 3 and Fig S23a-b).\nBa3YB3O9BaCO3 H3BO3Y2O3\nPrecursors Target by-products\nUnknown+\nCO2\nCO2 H2O\nCO2 H2O NH3\nCO2 NH3 O2\nMissing atoms\nAll by-products \nin database \nBalance \nstoichiometry\nDeduce\nTry\nFailSuccess\n6BaCO3+6H3BO3+Y2O3 2Ba3YB3O9+12CO2+18H2O\nDistilled \nenergies of \nall MP \nstructures\nEnergies of MP structures   (Ehull = 0 eV/atom)\nCalculate reaction energy (ΔH)\n濙\nNumber of wrong precursors\n3%\n7.1%\n16.4%\n73.5%\n6.7%\n13.1%\n80.2%\nNumber of missing/excess precursors\n63% 32.9%\n4.1%\n0.38%\n0.48%\n0.57%\n0.33%\n0.38%\n0.43%\n0.29%\n0.43%\n0.38%\n0.43%\n80.3\n80.2\n80.4\n80.2\n80.1\n80.3\n80.2\n80.1\n80.2\n80.1\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0 5 10 15\nNumber of differencesRun\n80.0 80.2 80.4 80.6\nTotal accuracy (%)\n1\n2\n-1 \nError Missing/excessSuccess\n1\n3\n2\n4\n濕\n濗\n濖\n濘\nFig. 4 | Performance of Precursor large language model (LLM). aProportions of\nprecursor predictions by the Precursors LLM that are entirely correct, have miss-\ning/excess precursors, or contain errors. b Statistics on the number of cases with\nmissing/excess precursors.c Statistics on the number of cases with incorrect\nprecursors.d Variations in results and success rate of the Methods LLM for ten\nadditional test set inferences compared to the initial results. e Workﬂow for\ninferring byproducts and calculating reaction energies based on precursor pre-\ndictions from Precursors LLM.\nTable 1 | Success rate, error rate, and missing/excess rate of\nt h es o l i d - s t a t ep r e c u r s o r sl a r g el a n g u a g em o d e lb yn u m b e r\nof elements\nNumber of elements Success rate Error rate Missing/excess rate\nAll 80.2% 13.1% 6.7%\n2 92.2% 4.4% 3.3%\n3 91.4% 4.6% 4.0%\n4 76.5% 14.1% 9.4%\n5 68.4% 24.9% 6.8%\n6 56.8% 37.8% 5.4%\n7 50.0% 33.3% 16.7%\nArticle https://doi.org/10.1038/s41467-025-61778-y\nNature Communications|         (2025) 16:6530 6\nBuilding on the remarkable generalization capability of synthe-\nsizability LLM across diverse theoretical structures, we deﬁnitely want\nto quickly predict key properties of synthesizable theoretical struc-\ntures in existing material databases. Thus, we collected 105,321 theo-\nretical structures from the MP database without experimental\nsynthesis labels. Of these, 5,529 structures were removed because they\nwere part of the training set used to ﬁne-tune the Synthesizability LLM,\nleaving 99,792 structures for synthesizability prediction. Among these,\n45,632 structures were identiﬁed as synthesizable. We then collected\ndata on 23 material properties, ensuring that each dataset is exclu-\nsively derived from either theoretical calculations or experimental\nmeasurements, with no intermixing between the two sources. These\ndata include optical, thermodynamic stability, hydration stability,\nmechanical, optoelectronic, magnetic, topological, thermoelectric,\npiezoelectric, dielectric, and superconducting properties. For each of\nthese properties, a high-precision graph neural network (GNN) model\nbased on transformer proposed in our previous work, CGTNet\n63,w a s\ntrained.\nThe testing errors (10% data) and data quantities for 23 property\nprediction models are shown in Table 2 and Fig. 5b. For simple prop-\nerties that can be directly derived from energy and electronic structure\ncalculations (red points in Fig.5b), models with larger training datasets\nexhibit high accuracy. In contrast, complex and experimental prop-\nerties do not display this trend, likely due to variations in data quality,\nsuch as differing experimental protocols used for testing experimental\nband gap, which limit data comparability.\nNext, these GNN models were employed to batch predict the 23\nproperties of the 45,632 synthesizable theoretical structures. Four of\nthese predicted material properties —HSE band gap, bulk modulus,\np-type conductivity and maximum piezoelectric coef ﬁcient—are\nvisualized in Fig. 5c–f. 14,097 synthesizable theoretical structures with\nHSE band gaps within the semiconductor range (1 –3e V ) w e r e i d e n t i -\nﬁed from 45,632 structures (Fig. 5c). For the remaining three proper-\nties, while the majority of synthesizable structures do not exhibit\nremarkable performance, several structures with exceptional proper-\nties are uncovered. This subset includes numerous structures pre-\ndicted to surpass the performance of existing materials with\nexceptional properties, such as ultra-hard materials diamond\n64 and\nReB2\n65, conductive materials TiN66 and Cu54, and piezoelectric materi-\nals BaTiO3\n67 and PbTi0.5Zr0.5O3\n68. The top three structures for each of\nthese properties are featured in Fig. 5b–g. A comprehensive list of all\npredicted properties for the synthes izable theoretical structures is\nprovided in Supplementary Data 1.\nDiscussion\nTo sum up, we have developed an innovative framework —CSLLM,\nintegration of Synthesizability, Method, and Precursor LLM, to predict\nthe synthesizability, synthetic method and precursors of crystal\nstructures, respectively. The Synthesizability LLM, ﬁne-tuned on a\ncomprehensive and balanced material structure dataset, demonstrates\nstate-of-the-art 98.6% accuracy, outperforming conventional thermo-\ndynamic and kinetic stability approaches. Notably, its exceptional\n0 20000 40000 60000 80000\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nSimple property\nComplex property\nTesting R2\nData amount\n0.94\n0.96\n0.98\n1.00Accuracy\n0 45 90 135 180 225 270\n0.94\n0.96\n0.98\n1.00Accuracy\nNumber of atoms in structure\n01 0 M 2 0 M 3 0 M 4 0 M 5 0 M\nP-Conductivity (S/m)\n0 250 500 750 1k\nBulk modulus (GPa)\n012345\nHSE band gap (eV)\n1 eV 3 eV\n0 200 400 600\nMaximun piezo dij(pC/N)\nȼ KFeBr4ȺPr9Al5S21 Ȼ TmMnB4\nȺȻȼ ȺȻȼ\nȺErLuRu2 Ȼ CsVZnF6 ȼ CaTi8S16\nDensityBox plotJitter plot\n0.0 0.25 0.5\na b c\nd f\nAverage accuracy: 97.8%\nAccuracy \ndistribution\ne\nȺȻȼ\nȺErTa3O9 Ȼ Ca3WO6 ȼ Sr4MnRuO8\nTiN Cu BaTiO3 PbTi0.5Zr0.5O3\nDensityBox plotJitter plot\n0.0 0.25 0.5\nDensityBox plotJitter plot\n0.0 0.25 0.5\nDensityBox plotJitter plot\n0.0 0.25 0.5\nSynthesizable\nsemiconductors\nReB2Diamond\nFig. 5 | Experimental structure validation and property predictions of syn-\nthesizable material structures. aSynthesizability prediction accuracy for struc-\ntures with varying numbers of atoms (1–270). The upper plot shows the distribution\nof prediction accuracy with an interval of 10, while the lower plot depicts the\nprediction accuracy corresponding to each atom count.b Training data quantity vs.\ntest set coefﬁcient of determination (R²). Red and blue represent simple and\ncomplex properties, respectively.c–f Kernel Density Estimation (KDE) distribution,\nbox plot, and jitter plot of predicted properties are shown with pink (top), blue\n(middle), and green (bottom), respectively. In all box plots ( c–f), bounds of box\nrepresent 25–75th percentiles, and whiskers extend to minima and maxima values.\nFor Heyd-Scuseria-Ernzerhof (HSE) band gap (c), minima = 0 eV, maxima = 44.12 eV,\nmedian = 2.50 eV, 25–75th percentile = 1.03–4.47 eV; For bulk modulus ( d),\nminima = 0.04 GPa, maxima = 3273.62 GPa, median = 52.28 GPa, 25 –75th\npercentile = 29.18–80.73 GPa; For p-type conductivity ( e), minima = 216.25 S/m,\nmaxima = 5.37 × 10\n7 S/m, median = 3.68 × 105 S/m, 25–75th\npercentile = 1.99 × 105−1.36 × 106 S/m; For maximum piezoelectric coefﬁcient (f),\nminima = 0.001 pC/N, maxima = 686.50 pC/N, median = 6.78 pC/N, 25 –75th\npercentile = 3.82–12.77 pC/N. Data are presented as individual data points in the\njitter plots (n = 45,632 synthesizable materials per property, representing inde-\npendent predictions). No technical replicates were used in the generation of these\nstatistics. Top three material structures are marked by gray short lines.\nArticle https://doi.org/10.1038/s41467-025-61778-y\nNature Communications|         (2025) 16:6530 7\ngeneralization ability was proved on an additional experimental data-\nset, with structures signi ﬁcantly exceeding the complexity of the\ntraining set. The Method LLM and Precursor LLM, ﬁne-tuned on\nsynthesis recipes from literature, achieve over 90% accuracy in clas-\nsifying synthetic methods and identifying solid-state synthesis pre-\ncursors for binary and ternary compounds, respectively. Furthermore,\nCSLLM identiﬁed 45,632 synthesizable theoretical structures from the\nMP database, with 23 key properties available for these structures.\nHowever, accurately predicting synthesis conditions, such as tem-\nperature and pressure for any material, remains challenging due to\ninsufﬁcient data. This highlights the critical need for constructing high-\nquality datasets to ﬁne-tune LLMs effectively. To address this need, a\npossible solution is leveraging the reinforcement learning with human\nfeedback (RLHF) method\n69, enabling iterative ﬁne-tuning of the LLM\nthrough continuous feedback of experimental synthesis data. Despite\nthese limitations, our CSLLM framework showcases the transformative\npotential of LLMs in bridging the gap between theoretical material design\nand practical synthesis. Its robust and powerful predictive capability,\ncoupled with a user-friendly interface, renders it a valuable tool for\nresearchers, enabling them to swiftly upload crystal structures and obtain\nreliable synthesizability and precursor prediction in a few seconds.\nMethods\nFine-tuning and inference of LLMs70\nThe Synthesizability LLM and Method, Precursors LLMs wereﬁne-tuned\nusing the pre-trained LLaMA3-8B71 models. For the Synthesizability LLM,\nwe also tested LLaMA-7B 38, achieving a similar accuracy of 98.6%. All\nLLMs were ﬁne-tuned utilizing the LoRA technique59 and LMFlow70 fra-\nmework withr =8 , α = 32, and a dropout rate of 0.1. LoRA was designed\nto ef ﬁciently adapt large pre-trained models to downstream tasks by\nintroducing low-rank updates to the model ’sw e i g h t s .T h i sa p p r o a c h\nsigniﬁcantly reduces the number of trainable parameters, making the\nﬁne-tuning process more computationally efﬁcient while maintaining\nhigh performance. All theﬁne-tunings and inferences were performed on\na server with one NVIDIA A800 GPU with 80 GB VRAM and 1 TB memory.\nNotably, the DeepSpeed library 72 and FlashAttention73 were used to\nr e d u c et h eG P Um e m o r yu s a g ea n di m p r o v et r a i n i n ge fﬁciency. The\ntemperature parameter for all the inferences was set to 0, as higher\nvalues (e.g., 0.1 and 0.2) would reduce accuracy and increase the output\nof redundant informati on. The details about the loss function and\nparameters in ﬁne-tunning and inference are shown in Supplementary\nNote 2-3, Table S2–4 and Fig S24–26.\nTraining of the CGTNet models\nAmong the 23 MPs, the HSE band gap data is sourced from the study by\nYoon et al. 74, while the experimental band gap data is collected from\nthe work of Li et al. 75. The PBE band gap and electrochemical stability\n(ΔGpbx) are obtained from the MP database 51, and the remaining\nproperties are sourced from the JARVIS database 54. Simple properties\ninclude the mBJ band gap, PBE band gap, energy above hull, bulk\nmodulus, shear modulus, Poisson ’s ratio, refractive index, electronic\nconductivity, ionic conductivity, maximum electronic dielectric con-\nstant, and maximum ionic dielectric constant. The remaining proper-\nties are classi ﬁed as complex properties. For each of the 23 material\nproperty datasets, we trained our previously proposed transformer-\nbased GNN model, CGTNet. This model has demonstrated better\naccuracy and small data friendliness compared to CGCNN\n76, SchNet77,\nPaiNN78 DimeNet + +79,a n dG e m N e t80. The main hyperparameters used\nfor training CGTNets are shown in Supplementary Note 3.\nGUI of CSLLM and material structure textualization\nThe GUI of CSLLM was built using the Gradio Python library. Uploaded\nCIF or POSCAR ﬁles are automatically converted to material strings.\nSpeciﬁcally, the space group of the structure is ﬁrst analyzed by the\nSpacegroupAnalyzer class in pymatgen\n81 with ‘‘symprec = 0.01’’ and\n‘‘angle_tolerance = 5.’’ Then, the structure is transformed into its pri-\nmitive cell, and the Wyckoff positions are analyzed using the PyXtal\nlibrary\n82. Finally, the material string undergoes inferences through the\npreloaded Synthesizability, Method, and Precursor LLM, followed by\nreaction energy calculations and other precursor recommendations.\nThis entire process runs in the background, and the ﬁnal output is\npresented as shown in Fig. 1a. For a video overview, see https://github.\ncom/szl666/CSLLM.\nReporting summary\nFurther information on research design is available in the Nature\nPortfolio Reporting Summary linked to this article.\nData availability\nThe comprehensive list of all predicted properties of the synthesizable\ntheoretical structures is provided in Supplementary Data 1 and Source\nData ﬁles. Source data are provided with this paper. The initial non-\nsynthesizable structures data used for ﬁne-tuning the large language\nmodel has been deposited in Figshare under https://ﬁgshare.com/\narticles/dataset/Non-synthesizable_structures/29086931\n83.T h es y n -\nthesizable structures data are obtained from the ICSD, which is subject\nto a commercial license and therefore cannot be publicly shared.\nInterested researchers may request access by contacting FIZ Karlsruhe\ndirectly through their of ﬁcial website ( https://www.ﬁz-karlsruhe.de/\nen/icsd). Access is typically granted to individuals or institutions with a\nvalid license for research purposes only.\nCode availability\nThe codes to perform the CSLLM framework along with the result\nanalysis and visualization are available at https://github.com/szl666/\nCSLLM under MIT license\n84.\nTable 2 | Properties, testing mean absolute error (MAR) and\ndata amount of 23 GNN models\nProperty Testing MAE R 2 Data\namount\nmBJ band gap 0.270 eV 0.952 18,172\nHSE band gap 0.380 eV 0.927 7376\nExperiment band gap 0.422 eV 0.848 2808\nPBE band gap 0.270 eV 0.913 74,992\nElectrochemical stabilityΔG\npbx 0.276 eV 0.918 3820\nEnergy above hull 0.102 eV 0.981 59,635\nBulk modulus 9.908 GPa 0.895 23,824\nShear modulus 9.933 GPa 0.793 23,824\nMaximum phonon frequency 68.14 cm\n-1 0.853 1265\nExfoliation energy 37.49 meV/atom 0.776 813\nPoisson ratio 0.087 0.796 10,987\nSLME 4.902 % 0.631 9770\nTotal magnetic moment 0.358 A m\n-2 0.785 74,261\nRefractive index 1.27 0.800 4764\nElectronic conductivity 4.292×10\n5 S/m 0.882 23,218\nIonic conductivity 4.114×10 5 S/m 0.893 23,218\np-Seebeck 45.295 μV/K 0.758 23,218\nn-Seebeck 40.042 μV/K 0.609 23,218\nMaximum piezoelectric eij 1.285 C/m² 0.624 4799\nMaximum piezoelectric dij 1.409 pC/N 0.660 3347\nMaximum dielectric constant 1.297 0.683 4809\nMaximum electronic dielectric\nconstant\n1.285 0.816 4809\nMaximum ionic dielectric\nconstant\n1.581 0.797 4809\nArticle https://doi.org/10.1038/s41467-025-61778-y\nNature Communications|         (2025) 16:6530 8\nReferences\n1 . H e y ,T . ,T a n s l e y ,S . ,T o l l e ,K .&G r a y ,J .The Fourth Paradigm: Data-\nIntensive ScientiﬁcD i s c o v e r y. (Microsoft Research, 2009).\n2. Xu, Y. et al. High-throughput calculations of magnetic topological\nmaterials.Nature 586,7 0 2–707 (2020).\n3. Gómez-Bombarelli, R. et al. Design of ef ﬁcient molecular organic\nlight-emitting diodes by a high-throughput virtual screening and\nexperimental approach.Nat. Mater.15, 1120–1127 (2016).\n4 . B u t l e r ,K .T . ,D a v i e s ,D .W . ,C a r t w r i g h t ,H . ,I s a y e v ,O .&W a l s h ,A .\nMachine learning for molecular and materials science.Nature 559,\n547–555 (2018).\n5. Zhang, Q. et al. Machine learning-aided design of highly conductive\nanion exchange membranes for fuel cells and water electrolyzers.\nAdv. Mater.36, 2404981 (2024).\n6. Dai, Y. et al. Machine-learning-driven G-quartet-based circularly\npolarized luminescence materials.Adv. Mater.36, 2310455 (2024).\n7. Lu, S. et al. Coupling a crystal graph multilayer descriptor to active\nlearning for rapid discovery of 2D ferromagnetic semiconductors/\nhalf-metals/metals.Adv. Mater.32, 2002658 (2020).\n8. Lu, S., Zhou, Q., Guo, Y. & Wang, J. On-the- ﬂy interpretable machine\nlearning for rapid discovery of two-dimensional ferromagnets with\nhigh Curie temperature. Chem. 8,7 6 9–783 (2022).\n9 . W e n g ,B .e ta l .S i m p l ed e s c r i p t o rderived from symbolic regression\naccelerating the discovery of new perovskite catalysts. Nat. Com-\nmun. 11, 3513 (2020).\n10. Song, Z. et al. Distilling universal activity descriptors for perovskite\ncatalysts from multiple data sources via multi-task symbolic\nregression.Mater. Horiz.10,1 6 5 1–1660 (2023).\n11. Lu, S. et al. Accelerated discovery of stable lead-free hybrid\norganic-inorganic perovskites via machine learning. Nat. Commun.\n9,3 4 0 5( 2 0 1 8 ) .\n12. Wu, Y. et al. Universal machine learning aided synthesis approach of\ntwo-dimensional perovskites in a typical laboratory.Nat. Commun.\n15,1 3 8( 2 0 2 4 ) .\n13. Chen, X., Lu, S., Chen, Q., Zhou, Q. & Wang, J. From bulk effective\nmass to 2D carrier mobility accurate prediction via adversarial\ntransfer learning.Nat. Commun.15,5 3 9 1( 2 0 2 4 ) .\n14. Du, Y. et al. Machine learning-a ided generative molecular design.\nNat. Mach. Intell.6,5 8 9–604 (2024).\n15. Lu, S., Zhou, Q., Chen, X., Song, Z. & Wang, J. Inverse design with\ndeep generative models: next step in materials discovery. Natl Sci.\nRev. 9,9 –11 (2022).\n16. Zeni, C. et al. A generative model for inorganic materials design.\nNature 639,6 2 4–632 (2025).\n17. Xie, T., Fu, X., Ganea, O.-E., Barzilay, R. & Jaakkola, T. Crystal Diffu-\nsion Variational Autoencoder for Periodic Material Generation. In\nProc. 10th International Conference on Learning Representations\n(ICLR, 2022).\n18. Noh, J. et al. Inverse design of solid-state materials via a continuous\nrepresentation.Matter 1,1 3 7 0–1384 (2019).\n19. Kim, B., Lee, S. & Kim, J. Inverse design of porous materials using\nartiﬁcial neural networks. Sci. Adv.6, eaax9324 (2020).\n20. Yao, Z. et al. Inverse design of n anoporous crystalline reticular\nmaterials with deep generative models. Nat. Mach. Intell.3,\n76–86 (2021).\n21. Singh, A. K., Montoya, J. H., Gregoire, J. M. & Persson, K. A. Robust\nand synthesizable photocatalysts for CO\n2 reduction: a data-driven\nmaterials discovery.Nat. Commun.10, 443 (2019).\n22. Wu, Q., Kang, L. & Lin, Z. A machine learning study on high thermal\nconductivity assisted to discover chalcogenides with balanced\ninfrared nonlinear optical performance.Adv. Mater.36,\n2309675 (2024).\n23. Szczypi ńs k i ,F .T . ,B e n n e t t ,S .&J e l f s ,K .E .C a nw ep r e d i c tm a t e r i a l s\nthat can be synthesised? Chem. Sci.12,8 3 0–840 (2021).\n24. Aykol, M., Dwaraknath, S. S., Sun, W. & Persson, K. A. Thermo-\ndynamic limit for synthesis of metastable inorganic materials. Sci.\nAdv. 4, eaaq0148 (2018).\n25. Zhao, Y. et al. High-throughput discovery of novel cubic crystal\nmaterials using deep generative neural networks. Adv. Sci.8,\n14–16 (2021).\n26. Petretto, G. et al. High-throughpu t density-functional perturbation\ntheory phonons for inorganic materials.Sci. Data5, 180065 (2018).\n2 7 . J o n e s ,E .B .&S t e v a n o v ić, V. Polymorphism in elemental silicon:\nprobabilistic interpretation of the realizability of metastable struc-\ntures. P h y s .R e v .B .96,1 –8( 2 0 1 7 ) .\n28. Chew, P. Y. & Reinhardt, A. Phase diagrams — why they matter and\nhow to predict them. J. Chem. Phys.158,0 3 0 9 0 2( 2 0 2 3 ) .\n29. Schmidt, J. et al. Machine-learning-assisted determination of the\nglobal zero-temperature phase diagram of materials. Adv. Mater.\n35, e2210788 (2023).\n30. De Yoreo, J. J. et al. Crystallization by particle attachment in syn-\nthetic, biogenic, and geologic environments.Science 349,\naaa6760 (2015).\n31. Antoniuk, E. R. et al. Predicting the synthesizability of crystalline\ninorganic materials from the data of known material compositions.\nnpj Comput. Mater.9, 155 (2023).\n32. Zhu, R. et al. Predicting synthesizability using machine learning on\ndatabases of existing inorganic materials. ACS Omega8,\n8210–8218 (2023).\n33. Davariashtiyani, A., Kadkhodaie, Z. & Kadkhodaei, S. Predicting\nsynthesizability of crystalline materials via deep learning.Commun.\nMater. 2, 115 (2021).\n34. Frey, N. C. et al. Prediction of synthesis of 2D metal carbides and\nnitrides (MXenes) and their precursors with positive and unlabeled\nmachine learning.ACS Nano13,3 0 3 1–3041 (2019).\n35. Jang, J., Gu, G. H., Noh, J., Kim, J. & Jung, Y. Structure-based syn-\nthesizability prediction of crystals using partially supervised learn-\ning. J. Am. Chem. Soc.142, 18836–\n18843 (2020).\n36. Gleaves, D., Fu, N., Dilanga Siriwardane, E. M., Zhao, Y. & Hu, J.\nMaterials synthesizability and stability prediction using a semi-\nsupervised teacher-student dual neural network.Digit. Discov.2,\n377–391 (2023).\n37. Cao, Y. et al. A Survey of AI-Generated Content (AIGC). ACM\nComput. Surv.57,1 –38 (2025).\n38. Touvron, H. et al. LLaMA: open and ef ﬁcient foundation language\nmodels. arXiv:2302.13971(2023).\n39. Jablonka, K. M., Schwaller, P., Ortega-Guerrero, A. & Smit, B.\nLeveraging large language models for predictive chemistry. Nat.\nMach. Intell.6,1 6 1–169 (2024).\n40. Boiko, D. A., MacKnight, R., Kline, B. & Gomes, G. Autonomous\nchemical research with large language models. Nature 624,\n570–578 (2023).\n41. Antunes, L. M., Butler, K. T. & Grau-Crespo, R. Crystal structure\ngeneration with autoregressive large language modeling.Nat.\nCommun. 15, 10570 (2024).\n42. M. Bran, A. et al. Augmenting large language models with chemistry\ntools. Nat. Mach. Intell.6,5 2 5–535 (2024).\n43. Zheng, Z., Zhang, O., Borgs, C., Chayes, J. T. & Yaghi, O. M. ChatGPT\nchemistry assistant for text mining and the prediction of MOF\nsynthesis.J. Am. Chem. Soc.145,1 8 0 4 8–18062 (2023).\n44. Kim, S., Jung, Y. & Schrier, J. Large language models for inorganic\nsynthesis predictions.J. Am. Chem. Soc.146, 19654–19659 (2024).\n45. Kim, S. et al. PubChem 2019 update: improved access to chemical\ndata. Nucleic Acids Res.47, D1102–D1109 (2019).\n4 6 . I r w i n ,J .J .&S h o i c h e t ,B .K .Z I N C− a free database of commercially\navailable compounds for virtual screening.J. Chem. Inf. Model.45,\n177–182 (2005).\nArticle https://doi.org/10.1038/s41467-025-61778-y\nNature Communications|         (2025) 16:6530 9\n47. Weininger, D. S. M. I. L. E. S. a chemical language and information\nsystem. 1. Introduction to methodology and encoding rules. J.\nChem. Inf. Model.28,3 1–36 (1988).\n48. Sahoo, P. et al. A Comprehensive Survey of Hallucination in Large\nLanguage, Image, Video and Audio Foundation Models. inFindings\nof the Association for Computational Linguistics: EMNLP 2024,\n11709–11724 (Association for Computational Linguistics, 2024).\n49. Hellenbrandt, M. The inorganic crystal structure database (ICSD) -\nPresent and future. Crystallogr. Rev.10,1 7–22 (2004).\n50. Gupta, T., Zaki, M. & Krishnan, N. M. A. & Mausam. MatSciBERT: a\nmaterials domain language model for text mining and information\nextraction.npj Comput. Mater.8,1 –11 (2022).\n51. Jain, A. et al. Commentary: the materials project: a materials gen-\nome approach to accelerating materials innovation. APL Mater.1,\n011002 (2013).\n52. Graulis, S. et al. Crystallography open database - an open-access\ncollection of crystal structures. J. Appl. Crystallogr.42,\n726–729 (2009).\n53. Kirklin, S. et al. The open qua ntum materials database (OQMD):\nassessing the accuracy of DFT formation energies. npj Comput.\nMater. 1, 15010 (2015).\n54. Choudhary, K. et al. The joint automated repository for various\nintegrated simulations (JARVIS) for data-driven materials design.\nnpj Comput. Mater.6,1 7 3( 2 0 2 0 ) .\n55. Van Der Maaten, L. & Hinton, G. Visualizing data using t-SNE. J.\nMach. Learn. Res.9,2 5 7 9–2625 (2008).\n5 6 . H a l l ,S .R . ,A l l e n ,F .H .&B r o w n ,I .D .T h ec r y s t a l l o g r a p h i ci n f o r m a -\ntion ﬁle (CIF): a new standard archive ﬁle for crystallography.\nFound. Crystallogr.47,6 5 5–685 (1991).\n57. Kresse, G. & Furthmüller, J. Ef ﬁcient iterative schemes for ab initio\ntotal-energy calculations using a plane-wave basis set.P h y s .R e v .B\n54, 11169–11186 (1996).\n58. Zhang, S. et al. in Document Analysis and Recognition - ICDAR\n2024: 18th International Conference, Athens, Part VI 281–298\n(Springer-Verlag, 2024).\n59. Hu, E. J. et al. LoRA: low-rank adaptation of large language models.\nIn Proc. 10th International Conference on Learning Representations\n(ICLR, 2022).\n60. Kononova, O. et al. Text-mined dataset of inorganic materials\nsynthesis recipes.Sci. Data6,2 0 3( 2 0 1 9 ) .\n61. Wang, Z. et al. Dataset of solution-based inorganic materials\nsynthesis procedures extracted from the scientiﬁ\ncl i t e r a t u r e .Sci.\nData 9, 231 (2022).\n62. Davariashtiyani, A., Wang, B., Ha jinazar, S., Zurek, E. & Kadkhodaei,\nS. Impact of data bias on machine learning for crystal compound\nsynthesizability predictions.Mach. Learn. Sci. Technol.5,\n040501 (2024).\n63. Song, Z., Lu, S., Zhou, Q. & Wang, J. T2MAT (text-to-materials): a\nuniversal framework for generating material structures with goal\nproperties from a single sentence. arXiv: 2407.06489(2024).\n64. Clerc, D. G. Ab initio elastic properties of diamond-like materials:\nelectronic factors that determine a high bulk modulus1Contribution\nof NIST— an agency of the US government; Not Subject to Copy-\nright in the United States1.J. Phys. Chem. Solids60,1 0 3–110 (1999).\n65. Chung, H.-Y. et al. Synthesis of ultra-incompressible superhard\nrhenium diboride at ambient pressure. Science 316,\n436–439 (2007).\n6 6 . X i n ,X . ,H e ,M . ,H a n ,W . ,J u n g ,J .&L i n ,Z .L o w - c o s tc o p p e rz i n ct i n\nsulﬁde counter electrodes for high-efﬁciency dye-sensitized solar\ncells. Angew. Chem. Int. Ed.50,1 1 7 3 9–11742 (2011).\n67. Acosta, M. et al. BaTiO3-based piezoelectrics: fundamentals, cur-\nrent status, and perspectives. Appl. Phys. Rev.4,4 1 3 0 5( 2 0 1 7 ) .\n68. Horchidan, N. et al. A comparative study of hard/soft PZT-based\nceramic composites.Ceram. Int.42,9 1 2 5–9132 (2016).\n69. Kirk, R. et al. Understanding the Effects of RLHF on LLM General-\nisation and Diversity. In Proc. 12th International Conference on\nLearning Representations(ICLR, 2024).\n70. Diao, S. et al. LMFlow: An Extensible Toolkit for Finetuning and\nInference of Large Foundation Models. in Proceedings of the 2024\nConference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies(Asso-\nciation for Computational Linguistics, 2024).\n71. Llama Team, A. The Llama 3 herd of models. arXiv:2407.21783(2024).\n72. Rasley, J., Rajbhandari, S., Ruwase, O. & He, Y. DeepSpeed: System\nOptimizations Enable Training Deep Learning Models with Over 100\nBillion Parameters. In Proc. 26th ACM SIGKDD International Con-\nference on Knowledge Discovery & Data Mining3505–3506\n(KDD, 2020).\n7 3 . D a o ,T .F l a s h A t t e n t i o n - 2 :f a s t e rattention with better parallelism and\nwork partitioning. In Proc.12th International Conference on Learning\nRepresentations(ICLR, 2024).\n74. Kim, S. et al. A band-gap database for semiconducting inorganic\nmaterials calculated with hybrid functional.Sci. Data7,3 8 7( 2 0 2 0 ) .\n75. Li, X.-G. et al. Graph network based deep learning of bandgaps. J.\nChem. Phys.155,1 5 4 7 0 2( 2 0 2 1 ) .\n7 6 . X i e ,T .&G r o s s m a n ,J .C .C r y s t a lg r a p hc o n v o l u t i o n a ln e u r a ln e t -\nworks for an accurate and interpretable prediction of material\nproperties.Phys. Rev. Lett.120,1 4 5 3 0 1( 2 0 1 8 ) .\n77. Schütt, K. T., Sauceda, H. E., Kindermans, P.-J., Tkatchenko, A. &\nMüller, K.-R. SchNet – a deep learning architecture for molecules\nand materials. J. Chem. Phys.148,2 4 1 7 2 2( 2 0 1 8 ) .\n78. Schütt, K. T., Unke, O. T. & Gastegger, M. Equivariant message\npassing for the prediction of tensorial properties and molecular\nspectra. InProc. 38th International Conference on Machine Learning\n(ICML, 2021).\n79. Gasteiger, J., Groß, J. & Günne mann, S. Directional message pas-\nsing for molecular graphs. In Proc. 8th International Conference on\nLearning Representations(ICLR, 2020).\n80. Gasteiger, J., Becker, F. & Günnemann, S. GemNet: universal\ndirectional graph neural networks for molecules. In Proc. 35th\nConference on Neural Information Processing Systems9,\n6790–6802 (NeurIPS, 2021).\n81. Ong, S. P. et al. Python materials genomics (pymatgen): a robust,\nopen-source Python library for materials analysis. Comput. Mater.\nSci. 68,3 1 4–319 (2013).\n82. Fredericks, S., Parrish, K., Sayre, D. & Zhu, Q. PyXtal: a Python library\nfor crystal structure generation and symmetry analysis. Comput.\nPhys. Commun.261,1 0 7 8 1 0( 2 0 2 1 ) .\n83. Song Z. Non-synthesizable structures, Figshare, https://doi.org/10.\n6084/m9.ﬁgshare.29086931.v1(2025).\n84. Song Z. Accurate Prediction of Sy nthesizability and Precursors of\n3D Crystal Structures via Large Language Models, CSLLM. 1,\nhttps://doi.org/10.5281/zenodo.15438489(2025).\nAcknowledgements\nThis work was supported by the National Key Research and Develop-\nment Program of China (grant 2022YFA1503103, 2021YFA1500700), the\nNatural Science Foundation of China (grant 22033002, 92261112,\n22373013, T2321002), and the Basic Research Program of Jiangsu Pro-\nvince (BK20232012, BK20222007),Jiangsu Provincial Scientiﬁc\nResearch Center of Applied Mathematics (BK20233002) and the Fun-\ndamental Research Funds for the Central Universities. We thank the Big\nData Computing Center of Southeast University for providing the facility\nsupport on the calculations.\nAuthor contributions\nZ.S., Q.Z. and J.W. conceived this work. Z.S. proposed the CSLLM fra-\nmework and wrote the code with guidance from Q.Z., S.L. and J.W. Z.S.,\nArticle https://doi.org/10.1038/s41467-025-61778-y\nNature Communications|         (2025) 16:6530 10\nQ.Z., M.J. and J.W. analyzed the data and co-wrote the manuscript, with\ninput from the other authors.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41467-025-61778-y.\nCorrespondenceand requests for materials should be addressed to\nQionghua Zhou or Jinlan Wang.\nPeer review information: Nature Communicationsthanks Jun Jiang,\nSara Kadkhodaei who co-reviewed with Devi Dutta Biswajeet, and the\nother anonymous reviewer(s) for their contribution to the peer review of\nthis work. A peer review ﬁle is available.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the\nCreative Commons licence, and indicate if you modi ﬁed the licensed\nmaterial. You do not have permission under this licence to share adapted\nmaterial derived from this article or parts of it. The images or other third\nparty material in this article are included in the article ’s Creative\nCommons licence, unless indicated otherwise in a credit line to the\nmaterial. If material is not included in the article ’s Creative Commons\nlicence and your intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission directly\nfrom the copyright holder. To view a copy of this licence, visit http://\ncreativecommons.org/licenses/by-nc-nd/4.0/.\n© The Author(s) 2025\nArticle https://doi.org/10.1038/s41467-025-61778-y\nNature Communications|         (2025) 16:6530 11",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.43699365854263306
    },
    {
      "name": "Computational biology",
      "score": 0.39089834690093994
    },
    {
      "name": "Biology",
      "score": 0.21649160981178284
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I76569877",
      "name": "Southeast University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210125878",
      "name": "Suzhou Research Institute",
      "country": "CN"
    }
  ],
  "cited_by": 2
}