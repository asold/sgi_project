{
    "title": "Performance of Publicly Available Large Language Models on Internal Medicine Board-style Questions",
    "url": "https://openalex.org/W4402576207",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2505529454",
            "name": "Constantine Tarabanis",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1987853345",
            "name": "Sohail Zahid",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2626480520",
            "name": "Marios Mamalis",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2112951965",
            "name": "Kevin Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A158436118",
            "name": "Evangelos Kalampokis",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2901888768",
            "name": "Lior Jankelson",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4381092249",
        "https://openalex.org/W4361289889",
        "https://openalex.org/W4319460874",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W4384561376",
        "https://openalex.org/W4385498032",
        "https://openalex.org/W1605353467",
        "https://openalex.org/W4384261711",
        "https://openalex.org/W3012397908",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W4284670538",
        "https://openalex.org/W3147524341",
        "https://openalex.org/W4226399820",
        "https://openalex.org/W4384918448"
    ],
    "abstract": "Ongoing research attempts to benchmark large language models (LLM) against physicians’ fund of knowledge by assessing LLM performance on medical examinations. No prior study has assessed LLM performance on internal medicine (IM) board examination questions. Limited data exists on how knowledge supplied to the models, derived from medical texts improves LLM performance. The performance of GPT-3.5, GPT-4.0, LaMDA and Llama 2, with and without additional model input augmentation, was assessed on 240 randomly selected IM board-style questions. Questions were sourced from the Medical Knowledge Self-Assessment Program released by the American College of Physicians with each question serving as part of the LLM prompt. When available, LLMs were accessed both through their application programming interface (API) and their corresponding chatbot. Mode inputs were augmented with Harrison’s Principles of Internal Medicine using the method of Retrieval Augmented Generation. LLM-generated explanations to 25 correctly answered questions were presented in a blinded fashion alongside the MKSAP explanation to an IM board-certified physician tasked with selecting the human generated response. GPT-4.0, accessed either through Bing Chat or its API, scored 77.5–80.7% outperforming GPT-3.5, human respondents, LaMDA and Llama 2 in that order. GPT-4.0 outperformed human MKSAP users on every tested IM subject with its highest and lowest percentile scores in Infectious Disease (80th) and Rheumatology (99.7th), respectively. There is a 3.2–5.3% decrease in performance of both GPT-3.5 and GPT-4.0 when accessing the LLM through its API instead of its online chatbot. There is 4.5–7.5% increase in performance of both GPT-3.5 and GPT-4.0 accessed through their APIs after additional input augmentation. The blinded reviewer correctly identified the human generated MKSAP response in 72% of the 25-question sample set. GPT-4.0 performed best on IM board-style questions outperforming human respondents. Augmenting with domain-specific information improved performance rendering Retrieval Augmented Generation a possible technique for improving accuracy in medical examination LLM responses.",
    "full_text": null
}