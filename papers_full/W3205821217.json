{
    "title": "Transformers for EEG Emotion Recognition.",
    "url": "https://openalex.org/W3205821217",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2491000341",
            "name": "Jiyao Liu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1973164540",
            "name": "Li Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A845246572",
            "name": "Hao Wu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2102854265",
            "name": "Huan Zhao",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1970727126",
        "https://openalex.org/W3205969702",
        "https://openalex.org/W2790404832",
        "https://openalex.org/W2808223797",
        "https://openalex.org/W3110327404",
        "https://openalex.org/W2772766867",
        "https://openalex.org/W1596717185",
        "https://openalex.org/W2750382711",
        "https://openalex.org/W2786768213",
        "https://openalex.org/W1947251450",
        "https://openalex.org/W3033046106",
        "https://openalex.org/W2992762269",
        "https://openalex.org/W2765856398",
        "https://openalex.org/W2004104731",
        "https://openalex.org/W3102455230",
        "https://openalex.org/W2765355882",
        "https://openalex.org/W2962699674",
        "https://openalex.org/W3088256290",
        "https://openalex.org/W2760473359"
    ],
    "abstract": "Electroencephalogram (EEG) can objectively reflect emotional state and changes. However, the transmission mechanism of EEG in the brain and its internal relationship with emotion are still ambiguous to human beings. This paper presents a novel approach to EEG emotion recognition built exclusively on self-attention over the spectrum, space, and time dimensions to explore the contribution of different EEG electrodes and temporal slices to specific emotional states. Our method, named EEG emotion Transformer (EeT), adapts the conventional Transformer architecture to EEG signals by enabling spatiospectral feature learning directly from the sequences of EEG signals. Our experimental results demonstrate that joint where temporal and spatial attention are applied simultaneously within each block, leads to the best emotion recognition accuracy among the design choices. In addition, compared with other competitive methods, the proposed method achieves state-of-art results on SEED and SEED-IV datasets.",
    "full_text": "SPATIAL-TEMPORAL TRANSFORMERS FOR EEG EMOTION RECOGNITION\nJiyao Liu, Hao Wu, Li Zhang, Yanxi Zhao\nSchool of Computer Science,\nNorthwestern Polytechnical University, Xi’an, China\nABSTRACT\nElectroencephalography (EEG) is a popular and effective tool\nfor emotion recognition. However, the propagation mecha-\nnisms of EEG in the human brain and its intrinsic correlation\nwith emotions are still obscure to researchers. This work pro-\nposes four variant transformer frameworks (spatial attention,\ntemporal attention, sequential spatial-temporal attention and\nsimultaneous spatial-temporal attention) for EEG emotion\nrecognition to explore the relationship between emotion and\nspatial-temporal EEG features. Speciﬁcally, spatial attention\nand temporal attention are to learn the topological structure\ninformation and time-varying EEG characteristics for emo-\ntion recognition respectively. Sequential spatial-temporal\nattention does the spatial attention within a one-second seg-\nment and temporal attention within one sample sequentially\nto explore the inﬂuence degree of emotional stimulation on\nEEG signals of diverse EEG electrodes in the same tempo-\nral segment. The simultaneous spatial-temporal attention,\nwhose spatial and temporal attention are performed simulta-\nneously, is used to model the relationship between different\nspatial features in different time segments. The experimental\nresults demonstrate that simultaneous spatial-temporal atten-\ntion leads to the best emotion recognition accuracy among\nthe design choices, indicating modeling the correlation of\nspatial and temporal features of EEG signals is signiﬁcant to\nemotion recognition.\nIndex Terms— EEG, emotion recognition, transformer\n1. INTRODUCTION\nEEG emotion recognition is to detect the current emotional\nstates of the subjects [1], [2], [3]. In recent years, with the\ndevelopment of deep learning and the availability of EEG\ndata, many emotion recognition methods based on neural net-\nworks have dominated the state-of-art position [4, 5]. In gen-\neral, the EEG signals collected by a spherical EEG cap have\nthree-dimensional characteristics which are spatial, spectral\nand temporal respectively. Many researchers have drawn at-\ntention to how to effectively utilize time-varying spatial and\ntemporal features from multi-channel brain signals.\nIn order to model the space relationships among multi-\nchannel EEG signals, a hierarchical convolutional neural net-\nwork (CNN) is proposed by Li et al. [5] to capture spatial\ninformation among different channels. A deep CNN model\nis present by Zhang et al. [6] to capture the spatio-temporal\nrobust feature representation of the raw EEG data stream for\nmotion intention classiﬁcation. A utilization of multi-layer\nCNN with no full connection layers is proposed by Lawhern\net al. [7] for P300-based oddball recognition task, ﬁnger mo-\ntor task and motor imagination task.\nConsidering the change of EEG signals over time, an\nEcho State Network (ESN) is present by Fourati et al. [8],\nESN used recursive layer to map the EEG signal into the\nhigh-dimension state space. A two-layer long short term\nmemory (LSTM), which uses EEG signal as the input, is\nadopted by Alhagry et al. [9] and obtain promising EEG\nemotion classiﬁcation results. A deep recursive convolutional\nneural network (R-CNN) is present by Bashivan et al. [10],\nthe proposed R-CNN gets a satisfactory result on the task\nmental load classiﬁcation based on EEG signal.\nMost of the above works are on the basis of convolution\nor recursive operation. CNN is good at modeling local re-\nceptive ﬁeld message, while pays less attention to the global\ninformation. Recurrent Neural Networks (RNN) network is\nrelatively weak to capture the spatial information and its par-\nallel computational efﬁciency is slower. To solve the above\nweaknesses, some works lead attention mechanism into CNN\nand RNN.\nSince different spatial-temporal features have different\ncontributions to emotion recognition, they should be assigned\nto different weight in the classiﬁer recognizing emotions.\nA LSTM with attention mechanism is proposed by Kim\net al. [11], the network assigns weights to the emotional\nstates appearing at speciﬁc moments to conduct two-level and\nthree-level classiﬁcation on the valence and arousal emotion\nmodels. A fresh multi-channel model on the basis of sparse\ngraphic attention long short term memory (SGA-LSTM) is\npresent by Liu et al. [12] to classify EEG emotion.\nAs is mentioned above, existing works have attained grat-\nifying results. However,The transmission characteristic as\nwell as spatial-temporal relevance of different EEG electrodes\nare more or less neglected in most of them. The change-\nless size kernels in convolution operation [13] may damage\nthe spatial correlation of EEG signals. Though the RNN op-\neration [14] takes the temporal features of EEG signals into\narXiv:2110.06553v2  [cs.RO]  23 Sep 2022\nconsideration, it ignores the spatial relation among EEG elec-\ntrodes. Furthermore, on account of the diverse impedance of\nvarious brain areas, there may be a slight error in time be-\ntween the EEG signal displayed by the EEG collection device\nand the real EEG signal, that is, EEG signals may delay varies\nwith different EEG electrodes.\nTo deal with the mentioned issues, we present a fresh\nEEG emotion transformer (EeT) framework built exclusively\non self-attention blocks. The variants of self-attention block\ninclude spatial (S) attention, temporal (T) attention, sequen-\ntial spatial-temporal (S-T) attention and simultaneous spatial-\ntemporal (S+T) attention. The spatial attention is to learn\nthe spatial structure information. The temporal attention is\nto learn the correlation between EEG signals and emotional\nstimuli as well as temporal changes. The sequential spatial-\ntemporal attention is to do spatial attention within the same\ntime segment and temporal attention among different time\nsegments in one sample. The simultaneous spatial-temporal\nattention is to do the two attention simultaneously. Experi-\nmental setups are elaborately picked to study the effects of\nspatial and temporal EEG signals on emotion recognition, and\nwhether there is some correlation between the features of dif-\nferent channels at different time segments.\n2. V ARIANTS OF EEG EMOTION TRANSFORMERS\n2.1. Framework of EeT\nEEG-based emotion recognition is to classify the emotion\nstates according to the EEG signal. As illustrated in Fig. 1,\nthe overview of EeT includes four modules, namely the\nfeature preparation module, the spatial-temporal attention\nmodule, deep neural network (DNN) module and classiﬁ-\ncation module. We focus on the design of self-attention\nmodule which includes spatial attention, temporal atten-\ntion, sequential spatial-temporal attention and simultaneous\nspatial-temporal attention.\nFeature\nPreparation \nSpatial-\nAttention\n EEG\nSignal DNN Classification \nLayer \nTemporal-\nAttention\n...\n4 ✖  Blocks\nFig. 1: Overview of EEG Emotion Recognition Transformer (EeT)\nEmotion recognition based on EEG is to learn a function\nf which maps the raw signals to emotion tags:\nY = f(X\n′\n) (1)\nwhere X\n′\nrepresents the EEG features. f represents the map-\nping function i.e., convolutional neural network transforma-\ntion. Y ∈{y1,y2,...,y n}represents the emotional tags. In\nour work, the cross entropy is adopted as the loss function,\nwhich can be deﬁned as:\nL= −\nC∑\nc=1\nyclog(y\n′\nc) (2)\nwhere Ldenotes the loss function of the EEG emotion clas-\nsiﬁcation, C denotes the number of emotion states, yc is the\nground-truth emotion tag andy\n′\nc is the predictor of neural net-\nworks.\n2.2. Preprocessing\nThe EEG features can be denoted as X = (F1,F2,...,F T ) ∈\nRC×S×T , where C is 5, equals to the number of frequency\nbands ( δ[1-4Hz], θ [4-8Hz], α [8-14Hz], β [14-31Hz],\nand γ [31-50 Hz]) used to compute the EEG features. S\nequals to the number of electrodes in the EEG cap and\nT equals to the number of the time slots in a EEG sam-\nple. Ft = ( B1,B2,...,B S) ∈ RC×S(t ∈ {1,2,...,T })\nis the one-second EEG feature. Bs = ( b1,b2,...,b C) ∈\nRC(s ∈{1,2,...,S }) presents the feature of one EEG chan-\nnel. Speciﬁcally, we map the S electrodes into a V ×H\nmatrix according to the layout of the EEG cap in order to\npreserve the spatial topology structure information of the\nEEG cap, then we take advantage of the linear interpola-\ntion [15] to replenish the spatial information which are not\ncollected by EEG acquisition equipment. The re-organized\nfeature can be represent as F\n′\nt = ( B1,B2,...,B V ×H) ∈\nRC×V ×H (t ∈1,2,...,T ) for a one-second EEG slice and\nX\n′\n= (F1,F2,...,F T ) ∈RC×V ×H×T for the whole EEG\nsample.\n2.3. Positional Encoding for Spatial EEG Electrodes\nWe divided the EEG feature of each secondFt(i= 1,2,3..S)\ninto G non-overlapping regions, just like the different brain\nregions in neuroscience. Here we regroup the V ×H ma-\ntrices into region sequences, the size of each divided region\nis P ×P, so we get G = VH/P 2 regions. Each region is\nﬂatten into a vector I(x)(p,t) ∈R5P2\nwith p= 1,2...,G rep-\nresenting spatial layout of EEG electrodes and t = 1,2,...T\ndenoting the index over seconds. Then we linearly map each\nregion I(x)(p,t) into a latent vector z(0)\n(p,t) ∈RD by means of\nlearnable matrix M ∈RD×5P2\n:\nz(0)\n(p,t) = M ⊗I(x)(p,t) + eposition\n(p,t) (3)\nwhere ⊗is matrix multiplication and epos\n(p,t) ∈RD stands for a\nlearnable position embedding to encode the spatial-temporal\nposition of each brain region. The resulting sequence of em-\nbedding vectors z(0)\n(p,t) stands for the input to the next layer of\nthe self-attention block. Note that zi is output of the ithlayer\nin self-attention block. p = 1,...G and t = 1,...,T are the\nspatial locations and indexes over time segments respectively.\n2.4. Query-Key-Value Mechanism\nOur Transformer consists of L encoding blocks. Instead of\nperforming a single attention function, we use different pro-\njected versions of queries, keys and values to perform the at-\ntention function in parallel, which is called multi-head atten-\ntion. At each block l, the query/key/value vectors are com-\nputed for each region from the representation z(l−1)\n(p,t) encoded\nby the preceding block:\nq(l,a)\n(p,t) = W(l,a)\nQ z(l−1)\n(p,t) ∈RDh (4)\nk(l,a)\n(p,t) = W(l,a)\nK z(l−1)\n(p,t) ∈RDh (5)\nv(l,a)\n(p,t) = W(l,a)\nV z(l−1)\n(p,t) ∈RDh (6)\nz(l−1)\n(p,t) , which is the output of the previous block, need to be\nlayer normalized before the above operations. a = 1,2...,A\ndenotes an index over multiple attention heads and A is the\nnumber of attention heads.\n2.5. Variants of Attention Mask Learning\nThe variants of self-attention block include spatial attention\n(S), temporal attention (T), sequential spatial-temporal atten-\ntion (S-T) and simultaneous spatial-temporal attention (S+T).\nThe spatial attention is to learn the spatial structure informa-\ntion while the temporal attention is to the relationship be-\ntween EEG and time. The sequential spatial-temporal atten-\ntion is the concatenation of two operations. The simultaneous\nspatial-temporal attention is to do the two operations simulta-\nneously.\n2.5.1. Spatial Attention\nIn the case of spatial attention, the self-attention weights\nα(a,l)\n(p,t) ∈RN+1 for query brain region (p,t) are given by:\nα(l,a) spatial\n(p,t) = σ\n\nq(l,a)⊤\n(p,t)√Dh\n·\n[\nk(l,a)\n(0,0)\n{\nk(l,a)\n(p′,t)\n}\np′\n]\n (7)\nwhere p′denotes the index of the brain regions.σdenotes\nthe softmax activation function. The formula is to consider\nthat different brain regions react differently under the same\nemotional stimulation thus different weights are given to the\nfeatures of different brain regions.\n2.5.2. Temporal Attention\nFor the temporal attention, the self-attention weights α(l,a)\n(p,t) ∈\nRT+1 for query brain region (p,t) are given by:\nα(l,a) temporal\n(p,t) = σ\n\nq(l,a)⊤\n(p,t)√Dh\n·\n[\nk(l,a)\n(0,0)\n{\nk(l,a)\n(p,t′)\n}\nt′\n]\n\n (8)\nwhere t′denotes the index of the time slots. In this formula,\ndifferent weights are given to the of different time slots in\nconsideration of the change of EEG signals with emotional\nstimulation and time.\n2.5.3. Sequential Spatial-Temporal (S-T) Attention\nThe sequential spatial-temporal attention is to do spatial at-\ntention within the same time segment and temporal atten-\ntion among different time segments. Firstly, the spatial self-\nattention weights are calculated as Eq. 7. Then the the tem-\nporal attention weights are learned by Eq. 8 from the output\nof spatial attention layer. (S-T) Attention comprehensively\nconsiders the attention of space and time, but the default is\nthat the spatial features in the same time period are closely\nrelated, while the features of different brain in different time\nare weakly related.\n2.5.4. Simultaneous Spatial-Temporal (S+T) Attention\nThe simultaneous spatial-temporal (S+T) attention is to do\nspatial and temporal attention simultaneously. The self-\nattention weights α(a,l)\n(p,t) ∈ RNT +1 for query brain region\n(p,t) are given by:\nα(l,a)\n(p,t) = σ\n\nq(l,a)\n(p,t)\nT\n√\nDh\n\n·\n\nk(l,a)\n(0,0)\n{\nk(l,a)\n(p′,t′)\n}\np′= 1,...,N\nt′= 1,...,T\n\n\n(9)\nDifferent from S-T Attention, which regards space and time\nseparately, the S+T attention considers that the spatial infor-\nmation in the same time point and different time points are\nboth strongly correlated.\n2.6. Multi-head Attention Recalibration\nThe encoding z(l)\n(p,t) at block l is obtained by the ﬁrst com-\nputing the weighted sum of value vectors using self-attention\ncoefﬁcients from each attention head:\ns(l,a)\n(p,t) = α(l,a)\n(p,t),(0,0)v(l,a)\n(0,0)+\nN∑\np′=1\nF∑\nt′=1\nα(l,a)\n(p,t),(p′,t′)v(l,a)\n(p′,t′), (10)\nAs is mentioned above, adenotes an index over multiple at-\ntention heads and lis the index of the blocks. Then, the con-\ncatenation of these vectors from all heads is projected and\npassed through an MLP, using residual connections after each\noperation:\nz′l\n(p,t) = W(l−1)\nO\n\n\ns(l,1)\n(p,t)\n.\n.\n.\ns(l,A)\n(p,t)\n\n\n+ z(l−1)\n(p,t) , (11)\nwhere WO is the Value of z(l−1)\n(p,t) by concatenating v(l,a)\n(p,t).\nzl\n(p,t) = MLP(z′l\n(p,t)) +z′\n(p,t)\nl (12)\nThe z′l\n(p,t) goes through the MLP layer to get the output of\nlthlayer.\n3. EXPERIMENTS\n3.1. Datasets\nWe validate our model on SEED [16, 17], SEED-IV [18] and\nDeap [19] databases.\nDeap dataset is a open source dataset including diverse\nphysiological signals with emotion evaluations provided by\nthe research team of Queen Mary University in London. It\nrecords the EEG, ECG, EMG and other bioelectrical signals\nof 32 subjects induced by watching 40 one-minute music\nvideos of different emotional tendencies. The subjects evalu-\nated the videos’ emotion categories on scale of one to nine in\ndimension of arousal, valence, liking, dominance and famil-\niarity. Valence reports the degree of subjects’ joy, the greater\nthe valence value, the higher the joy degree. Arousal reports\nthe subjects’ emotional intensity, the higher the arousal value,\nthe more intense and perceptible the emotion. The rating\nvalue from small to large indicates the emotion metric is from\nnegative to positive or from weak to strong. The 40 stimulus\nvideos include 20 high valence/arousal stimuli and 20 low\nvalence/arousal stimuli.\nSEED contains three different categories of emotion,\nnamely positive, negative, and neutral. Fifteen participants’\nEEG data of the dataset were collected while they were\nwatching the stimulus videos. The videos are carefully se-\nlected and can elicit a single desired target emotion. With an\ninterval of about one week, each subject participated in three\nexperiments, and each session contained 15 ﬁlm clips. The\nparticipants are asked to give feedback immediately after each\nexperiment. The EEG signals of 62 channels are recorded at\na sampling frequency of 1000 Hz and down-sampled with\n200 Hz. The Differential entropy [17] DE features are pre-\ncomputed over different frequency bands for each sample in\neach channel.\nSEED-IV contains four different categories of emotions,\nincluding happy, sad, fear, and neutral emotion. The exper-\niment consists of 15 participants. Three experiments are de-\nsigned for each participant on different days, and each session\ncontains 24 video clips and six clips for each type of emo-\ntion in each session. After each experiment, the subjects are\nasked to give feedback, while 62 EEG signals of the subjects\nare recorded. The EEG signals are sliced into 4-second non-\noverlapping segments and down-sampled with 128 Hz. The\nDE feature is also pre-computed over ﬁve frequency bands in\neach channel.\n3.2. Experimental Setup\nWe train our model on NVIDIA RTX 2080 GPU. Cross en-\ntropy loss is used as the loss function. The optimizer is Adam.\nThe initial learning rate is set to 1e-3 with multi-step decay to\n1e-7. The number of the attention blocks is set to 4 and the\nlength of each sample is set to 10s. We conduct experiments\non each subject. For each experiment, we randomly shufﬂe\nthe samples and use 5-fold cross validation. The ratio of the\ntraining set to test set is 9:6.\n3.3. Compared Models\nWe compare the proposed EeT with the following competitive\nmodels.\nSVM [20] is a least squares support vector machine classi-\nﬁer. DBN [21] is deep Belief Networks investigate the critical\nfrequency bands and channels. DGCNN [22] is Dynamical\nGraph Convolutional Neural Networks model the multichan-\nnel EEG features. BiDANN [23] is bi-hemispheres domain\nadversarial neural network maps the EEG feature of both\nhemispheres into discriminative feature spaces separately.\nBiHDM [24] is bi-hemispheric discrepancy model learns the\nasymmetric differences between two hemispheres for EEG\nemotion recognition. 3D-CNN with PST-Attention [25] is a\nself-attention module combined with 3D-CNN to learn criti-\ncal information among different dimensions of EEG feature.\nLSTM [9] is a time series model for identifying continuous\ndimension emotion. 3D-CNN [26] is 3D-CNN model to\nrecognize arousal and valence. BT [27] is deep convolution\nneural network for continuous dimension emotion recogni-\ntion.\n3.4. Experimental Results and Analysis\nTable 1: Experimental Results on DEAP Dataset\nModels\nArousal Valence\nacc(%) F1 acc(%) F1\nLSTM[9] 85.65 - 85.45 -\n3D-CNN [26] 88.49 - 87.44 -\nBT[27] 86.18 - 86.31 -\nEeT ∼(S+T Attention) 93.34 0.9326 92.86 0.9196\nTable 1 presents the average accuracy (acc) and F1 value\n(F1) of the compared models for EEG based emotion recog-\nnition on the DEAP datasets. Compared with 3D-CNN [26],\nthe acc of the proposed simultaneous spatial-temporal (S+T)\nattention EeT framework have 4.85%/5.42% improvement.\nEeT with S+T attention also gets superior performance com-\npared with other competitive models.\nTable 2: Experimental Results on SEED and SEED-IV Dataset\nModels SEED SEED-IVMean (%) Std (%) Mean (%) Std (%)SVM [20] 83.99 9.72 56.61 20.05DBN [21] 86.08 8.34 66.777.38DGCNN [22] 90.40 8.49 69.88 16.29BiDANN [23] 92.38 7.04 70.29 12.63BiHDM [24] 93.12 6.06 74.35 14.093D-CNN with PST-Attention[25] 95.76 4.98 82.73 8.96EeT (S+T Attention) 96.28 4.39 83.278.37\nTable 2 presents the average accuracy (Mean) and stan-\ndard deviation (Std) of the compared models for EEG based\nemotion recognition on SEED and SEED-IV datasets. Com-\npared with 3D-CNN with PST-Attention, the means of the\nproposed joint spatial+temporal (S+T) attention EeT frame-\nwork have 0.52%/0.54% improvements on SEED and SEED-\nIV . The Stds of Eet with S+T attention achieve 0.59%/0.59%\nreductions on SEED and SEED-IV respectively compared\nwith those of 3D-CNN with PST-Attention. Moreover, EeT\nwith S+T attention gets superior performance compared with\nother competitive models.\nThe Visualization Features Extracted From Eet without AttentionThe Visualization Features Extracted From Eet with S+T Attention\nFig. 2: The Visualization of High Level Features Extracted From Eet\nWe use t-SNE [28] to visualize the high-level bottleneck\nfeatures from the well trained Eet. As shown in Fig. 2, differ-\nent colors represent different emotional labels, the distances\nof different classes in the high-level feature space of Eet with\nS+T Attention (the right part) are more dispersed than that\nof EeT without attention (the left part), which demonstrates\nthat the high-level features learned with simultaneous spatial-\ntemporal attention is more discriminative.\n3.5. Ablation Experiments\nTable 3: Experimental Results of Variant Transformers\nModels SEED SEED-IV\nMean (%) Std (%) Mean (%) Std (%)\nS Attention 93.14 9.31 73.31 13.67\nT Attention 92.74 10.21 72.37 12.83\nS-T Attention 95.65 6.73 80.31 8.51\nS+T Attention 96.28 4.39 83.27 8.37\nTable 3 presents the results of different variants of the\nproposed transformer framework, from which we can see that\nJoint Spatial-Temporal Attention gets the best results, achiev-\ning 0.63%/2.96% improvements compared with the second\nbest variant, (S-T) Attention on SEED and SEED-IV respec-\ntively, indicating comprehensively considering the temporal\nand spatial characteristics of EEG may boost the emotion\nrecognition results most notably. As for single dimensional\nattention, the results are a bit of lower than those of com-\nbined variants’. Spatial Attention is 0.4% /0.94% higher than\nthat of Temporal Attention, implying the spatial dimension\nmay have more emotion-related message than the temporal\ndimension.\n4. CONCLUSION\nIn this paper, we propose a new EEG emotion recognition\nframework based on self-attention, which is built exclusively\non self-attention. Our approach considers the relationship be-\ntween emotion and brain regions, time series change as well\nas the intrinsic spatiotemporal characteristics of EEG signals.\nThe results of our methods show that the attention mechanism\ncan boost the performance of emotion recognition evidently.\nFurthermore, the simultaneous spatio-temporal attention gets\nthe best results among the four designed structures, the result\nis also better than most state of the art methods, indicating that\nconsidering the spatio-temporal feature jointly and simultane-\nously is more in line with the transmission law of EEG signals\nin the human brain.\n5. REFERENCES\n[1] Danny Oude Bos et al., “Eeg-based emotion recogni-\ntion,” The inﬂuence of visual and auditory stimuli , vol.\n56, no. 3, pp. 1–17, 2006.\n[2] Roddy Cowie, Ellen Douglas-Cowie, Nicolas Tsapat-\nsoulis, George V otsis, Stefanos Kollias, Winfried Fel-\nlenz, and John G Taylor, “Emotion recognition in\nhuman-computer interaction,” IEEE Signal processing\nmagazine, vol. 18, no. 1, pp. 32–80, 2001.\n[3] Hatice Gunes, Bj ¨orn Schuller, Maja Pantic, and Roddy\nCowie, “Emotion representation, analysis and synthesis\nin continuous space: A survey,” in 2011 IEEE Interna-\ntional Conference on Automatic Face & Gesture Recog-\nnition (FG). IEEE, 2011, pp. 827–834.\n[4] Abeer Al-Nafjan, Manar Hosny, Areej Al-Wabil, and\nYousef Al-Ohali, “Classiﬁcation of human emotions\nfrom electroencephalogram (eeg) signal using deep neu-\nral network,” Int. J. Adv. Comput. Sci. Appl, vol. 8, no.\n9, pp. 419–425, 2017.\n[5] Jinpeng Li, Zhaoxiang Zhang, and Huiguang He, “Hi-\nerarchical convolutional neural networks for eeg-based\nemotion recognition,” Cognitive Computation, vol. 10,\nno. 2, pp. 368–380, 2018.\n[6] Dalin Zhang, Lina Yao, Xiang Zhang, Sen Wang,\nWeitong Chen, and Robert Boots, “Eeg-based inten-\ntion recognition from spatio-temporal representations\nvia cascade and parallel convolutional recurrent neural\nnetworks,” arXiv preprint arXiv:1708.06578, 2017.\n[7] Vernon J Lawhern, Amelia J Solon, Nicholas R Way-\ntowich, Stephen M Gordon, Chou P Hung, and Brent J\nLance, “Eegnet: a compact convolutional neural net-\nwork for eeg-based brain–computer interfaces,”Journal\nof neural engineering, vol. 15, no. 5, pp. 056013, 2018.\n[8] Rahma Fourati, Boudour Ammar, Chaouki Aouiti,\nJavier Sanchez-Medina, and Adel M Alimi, “Opti-\nmized echo state network with intrinsic plasticity for\neeg-based emotion recognition,” in International Con-\nference on Neural Information Processing . Springer,\n2017, pp. 718–727.\n[9] Salma Alhagry, Aly Aly Fahmy, and Reda A El-Khoribi,\n“Emotion recognition based on eeg using lstm recurrent\nneural network,” Emotion, vol. 8, no. 10, pp. 355–358,\n2017.\n[10] Pouya Bashivan, Irina Rish, Mohammed Yeasin, and\nNoel Codella, “Learning representations from eeg with\ndeep recurrent-convolutional neural networks,” arXiv\npreprint arXiv:1511.06448, 2015.\n[11] Youmin Kim and Ahyoung Choi, “Eeg-based emo-\ntion classiﬁcation using long short-term memory net-\nwork with attention mechanism,” Sensors, vol. 20, no.\n23, pp. 6727, 2020.\n[12] Suyuan Liu, Wenming Zheng, Tengfei Song, and Yuan\nZong, “Sparse graphic attention lstm for eeg emotion\nrecognition,” in International Conference on Neural In-\nformation Processing. Springer, 2019, pp. 690–697.\n[13] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-\nton, “Imagenet classiﬁcation with deep convolutional\nneural networks,” Advances in neural information pro-\ncessing systems, vol. 25, pp. 1097–1105, 2012.\n[14] David E Rumelhart, Geoffrey E Hinton, and Ronald J\nWilliams, “Learning representations by back-\npropagating errors,”nature, vol. 323, no. 6088, pp. 533–\n536, 1986.\n[15] Heidi A Schlitt, L Heller, R Aaron, E Best, and\nDM Ranken, “Evaluation of boundary element methods\nfor the eeg forward problem: effect of linear interpo-\nlation,” IEEE transactions on biomedical engineering ,\nvol. 42, no. 1, pp. 52–58, 1995.\n[16] Wei-Long Zheng and Bao-Liang Lu, “Investigating crit-\nical frequency bands and channels for eeg-based emo-\ntion recognition with deep neural networks,” IEEE\nTransactions on Autonomous Mental Development, vol.\n7, no. 3, pp. 162–175, 2015.\n[17] Ruo-Nan Duan, Jia-Yi Zhu, and Bao-Liang Lu, “Differ-\nential entropy feature for eeg-based emotion classiﬁca-\ntion,” in2013 6th International IEEE/EMBS Conference\non Neural Engineering (NER). IEEE, 2013, pp. 81–84.\n[18] Wei-Long Zheng, Wei Liu, Yifei Lu, Bao-Liang Lu,\nand Andrzej Cichocki, “Emotionmeter: A multimodal\nframework for recognizing human emotions,” IEEE\ntransactions on cybernetics , vol. 49, no. 3, pp. 1110–\n1122, 2018.\n[19] Sander Koelstra, Christian Muhl, Mohammad So-\nleymani, Jong-Seok Lee, Ashkan Yazdani, Touradj\nEbrahimi, Thierry Pun, Anton Nijholt, and Ioannis Pa-\ntras, “Deap: A database for emotion analysis; using\nphysiological signals,” IEEE transactions on affective\ncomputing, vol. 3, no. 1, pp. 18–31, 2011.\n[20] Johan AK Suykens and Joos Vandewalle, “Least squares\nsupport vector machine classiﬁers,” Neural processing\nletters, vol. 9, no. 3, pp. 293–300, 1999.\n[21] Wei-Long Zheng, Jia-Yi Zhu, Yong Peng, and Bao-\nLiang Lu, “Eeg-based emotion classiﬁcation using deep\nbelief networks,” in 2014 IEEE International Confer-\nence on Multimedia and Expo (ICME). IEEE, 2014, pp.\n1–6.\n[22] Tengfei Song, Wenming Zheng, Peng Song, and Zhen\nCui, “Eeg emotion recognition using dynamical graph\nconvolutional neural networks,” IEEE Transactions on\nAffective Computing, vol. 11, no. 3, pp. 532–541, 2018.\n[23] Yang Li, Wenming Zheng, Zhen Cui, Tong Zhang, and\nYuan Zong, “A novel neural network model based on\ncerebral hemispheric asymmetry for eeg emotion recog-\nnition.,” in IJCAI, 2018, pp. 1561–1567.\n[24] Yang Li, Lei Wang, Wenming Zheng, Yuan Zong, Lei\nQi, Zhen Cui, Tong Zhang, and Tengfei Song, “A\nnovel bi-hemispheric discrepancy model for eeg emo-\ntion recognition,” IEEE Transactions on Cognitive and\nDevelopmental Systems , vol. 13, no. 2, pp. 354–367,\n2020.\n[25] Jiyao Liu, Yanxi Zhao, Hao Wu, and Dongmei Jiang,\n“Positional-spectral-temporal attention in 3d convolu-\ntional neural networks for eeg emotion recognition,”\nProc. APSIPA, 2021.\n[26] Elham S Salama, Reda A El-Khoribi, Mahmoud E\nShoman, and Mohamed A Wahby Shalaby, “Eeg-based\nemotion recognition using 3d convolutional neural net-\nworks,” Int. J. Adv. Comput. Sci. Appl, vol. 9, no. 8, pp.\n329–337, 2018.\n[27] JX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang,\nand YN Zhang, “Accurate eeg-based emotion recogni-\ntion on combined features using deep convolutional neu-\nral networks,” IEEE Access, vol. 7, pp. 44317–44328,\n2019.\n[28] Laurens Van der Maaten and Geoffrey Hinton, “Visu-\nalizing data using t-sne.,” Journal of machine learning\nresearch, vol. 9, no. 11, 2008."
}