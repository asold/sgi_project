{
  "title": "Large Dataset and Language Model Fun-Tuning for Humor Recognition",
  "url": "https://openalex.org/W2951804840",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2748566288",
      "name": "Vladislav Blinov",
      "affiliations": [
        "Ural Federal University"
      ]
    },
    {
      "id": "https://openalex.org/A5097047972",
      "name": "Valeria Bolotova-Baranova",
      "affiliations": [
        "Ural Federal University"
      ]
    },
    {
      "id": "https://openalex.org/A2025620057",
      "name": "Pavel Braslavski",
      "affiliations": [
        "National Research University Higher School of Economics",
        "Ural Federal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2877603826",
    "https://openalex.org/W2804900514",
    "https://openalex.org/W3204092490",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W1749601410",
    "https://openalex.org/W2888951210",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2013580886",
    "https://openalex.org/W4301481313",
    "https://openalex.org/W2775897458",
    "https://openalex.org/W19282688",
    "https://openalex.org/W172035031",
    "https://openalex.org/W2129983478",
    "https://openalex.org/W1968333171",
    "https://openalex.org/W2894118680",
    "https://openalex.org/W2251785914",
    "https://openalex.org/W2832501608",
    "https://openalex.org/W2952005787",
    "https://openalex.org/W2753059774",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2751923720",
    "https://openalex.org/W2937328183",
    "https://openalex.org/W2033175753",
    "https://openalex.org/W2090915937"
  ],
  "abstract": "The task of humor recognition has attracted a lot of attention recently due to the urge to process large amounts of user-generated texts and rise of conversational agents. We collected a dataset of jokes and funny dialogues in Russian from various online resources and complemented them carefully with unfunny texts with similar lexical properties. The dataset comprises of more than 300,000 short texts, which is significantly larger than any previous humor-related corpus. Manual annotation of 2,000 items proved the reliability of the corpus construction approach. Further, we applied language model fine-tuning for text classification and obtained an F1 score of 0.91 on a test set, which constitutes a considerable gain over baseline methods. The dataset is freely available for research community.",
  "full_text": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4027–4032\nFlorence, Italy, July 28 - August 2, 2019.c⃝2019 Association for Computational Linguistics\n4027\nLarge Dataset and Language Model Fun-Tuning for Humor Recognition\nVladislav Blinov1,3, Valeriia Bolotova-Baranova1,3, and Pavel Braslavski1,2\n1Ural Federal University, Yekaterinburg, Russia\n2Higher School of Economics, Saint Petersburg, Russia\n3Tinkoff.ru\n{vladislav.blinov,pavel.braslavsky}@urfu.ru, lurunchik@gmail.com\nAbstract\nThe task of humor recognition has attracted a\nlot of attention recently due to the urge to pro-\ncess large amounts of user-generated texts and\nrise of conversational agents. We collected a\ndataset of jokes and funny dialogues in Rus-\nsian from various online resources and com-\nplemented them carefully with unfunny texts\nwith similar lexical properties. The dataset\ncomprises of more than 300,000 short texts,\nwhich is signiﬁcantly larger than any previous\nhumor-related corpus. Manual annotation of\nabout 2,000 items proved the reliability of the\ncorpus construction approach. Further, we ap-\nplied language model ﬁne-tuning for text clas-\nsiﬁcation and obtained an F1 score of 0.91 on\ntest set, which constitutes a considerable gain\nover baseline methods. The dataset is freely\navailable for research community.\n1 Introduction\nHumor is an important element of everyday hu-\nman communication (Martin, 2007). With a rapid\ndevelopment of conversational systems and NLP\napplications for social media content, the task of\nautomatic recognition of humor and other types\nof ﬁgurative language has gained a lot of atten-\ntion (Nijholt et al., 2017). Standard publicly avail-\nable datasets signiﬁcantly contribute to steady and\nmeasurable progress in solving NLP tasks. To the\ndate, there are several humor-related datasets, but\nthe majority of them contain English texts only,\nare relatively small, and focus predominantly on\npuns, thus don’t reﬂect a wide variety of humor-\nous content.\nIn this work we describe the creation of a large\ndataset of funny short texts in Russian. We started\nwith an existing dataset and more than tripled it\nin size. The texts were automatically collected\nfrom various online sources to ensure their diver-\nsity and representativeness. A separate task was\nthe compilation of a contrasting corpus of unfunny\ntexts in such a way that their distinguishing char-\nacteristic was absence of humor, and not their lex-\nical properties and style. The dataset comprises of\nmore than 300,000 short texts in total, about half\nof them being funny. Manual annotation of 1,877\nexamples conﬁrmed the validity of the automatic\napproach and formed a golden test set.\nWe implemented a humor detection method\nbased on the universal language model ﬁne-\ntuning. Unlike most approaches to humor recog-\nnition described in the literature, this method nei-\nther draws upon an existing theory of humor, nor\nmakes explicit assumptions about the structure and\n‘mechanics’ of jokes; it needs no feature engineer-\ning and is purely data-driven. This approach is jus-\ntiﬁed in the case of a large heterogeneous collec-\ntion. Evaluation of the trained model on several\ntest collections of Russian jokes shows that it has\nnot been overﬁtted and generalizes well.\nThe compiled dataset publicly available 1. We\nhope that the resource will intensify research on\nmultilingual computational humor.\n2 Related Work\nHumor recognition is usually formulated as a clas-\nsiﬁcation problem with a wide variety of fea-\ntures – syntactic parsing, alliteration and rhyme,\nantonymy and other WordNet relations, dictionar-\nies of slang and sexually explicit words, polar-\nity and subjectivity lexicons, distances between\nwords in terms of word2vec representations, word\nassociation measures, etc. (Taylor and Mazlack,\n2004; Mihalcea and Strapparava, 2005; Kiddon\nand Brun, 2011; Yang et al., 2015; Zhang and Liu,\n2014; Liu et al., 2018; Cattle and Ma, 2018; Er-\nmilov et al., 2018). A cognate task is humor rank-\ning (Shahaf et al., 2015; Potash et al., 2017). Fea-\ntures engineered for classiﬁcation/ranking are of-\n1https://github.com/\ncomputational-humor/humor-recognition/\ntree/master/data\n4028\nten inspired by linguistic theories of humor, see\na survey in (Attardo, 1994). Most recent stud-\nies (Yang et al., 2015; Liu et al., 2018; Cattle and\nMa, 2018) employ Random Forest classiﬁers for\nhumor recognition and word embeddings as fea-\nture vectors. At the moment, there are a few stud-\nies that use neural architectures to directly address\nhumor recognition: Ortega-Bueno et al. (2018)\nand Hasan et al. (2019) exploit LSTM, while Chen\nand Soo (2018) use CNN architecture.\nThe dataset collected by Mihalcea and Strap-\nparava (2005) became a de facto standard for hu-\nmor recognition. It contains 16,000 one-liners and\n16,000 non-humorous sentences from news titles,\nproverbs, British National Corpus, and Open Mind\nCommon Sense collection. Another dataset used\nin several studies (Yang et al., 2015; Cattle and\nMa, 2018) comprises of 2,400 puns and an equal\nnumber of negative samples from the news, Ya-\nhoo!Answers, and proverbs. In both cases au-\nthors tried to ensure lexical and structural simi-\nlarity between the humorous and ‘serious’ classes.\nTwo datasets were prepared within SemEval 2017\nshared tasks: #HashtagWars (Potash et al., 2017)\nand English Puns (Miller et al., 2017). The former\ndataset comprises of 12,000 tweets corresponding\nto about 100 episodes of a TV show, each anno-\ntated with a 3-point funniness score. The latter\none contains about 4,000 contexts, 71% of which\nare puns, annotated with WordNet senses. Most\nof humor recognition research deals with English;\nexceptions are studies working with Italian (Reyes\net al., 2009), Russian (Ermilov et al., 2018), and\nSpanish (Castro et al., 2018).\n3 Data\nSTIERLITZ and P UNS . We started with a\ndataset of Russian one-liners and non-humorous\ntexts used previously in (Ermilov et al., 2018). The\nbalanced dataset was assembled by complement-\ning a collection of jokes from social media (Bolo-\ntova et al., 2017) with non-humorous proverbs,\nnews headlines, and sentences from ﬁction books.\nFollowing the authors, we refer to the dataset as\nSTIERLITZ .2 We also use a small collection of\nRussian puns from (Ermilov et al., 2018) for eval-\nuation. Puns as a special type of humor seem to be\nless articulated in the Russian culture compared to\n2Stierlitz is a protagonist of a popular TV series, a So-\nviet spy working undercover in Nazi Germany. He is also a\npopular character of jokes in post-Soviet countries.\nDataset Jokes Non-jokes Total\nSTIERLITZ 46,608 46,608 93,216\ntrain 37,447 37,447 65,530\nvalidation 4,682 4,682 9,364\ntest 9,361 9,361 18,722\nPUNS 213 0 213\nFUN 156,605 156,605 313,210\ntrain 125,708 125,708 251,416\ntest 30,897 30,897 61,794\nGOLD 899 978 1,877\nTable 1: Datasets for humor recognition.\nBritish/US tradition. The authors were able to spot\nonly few online collections of puns.\nFUN: dataset expansion. Our goal was to sig-\nniﬁcantly expand S TIERLITZ and to ensure that\nfunny/serious counterparts are more similar in\nterms of vocabulary, style, and structure than in\nthe original collection.\nFirst, we collected more than 1M jokes from\nmultiple humorous public pages from the largest\nRussian social network VK.com through its API\n(556K) and from the website anekdot.ru (477K),\nthe oldest online resource of humorous content on\nthe Russian Web.\nThen, we ﬁltered out less popular jokes based\non user ratings, duplicates, and jokes already pre-\nsented in S TIERLITZ and P UNS collections. The\nnewly obtained collection is quite diverse: it\ncontains one-liners, multi-turn jokes, and short\nsketches.\nSecond, we downloaded 10M posts from a large\nonline forum of the city portal of Yekaterinburg\nE1.ru3. We opted for online forums as a source of\nnegative examples, since social media and human\nconversations are immediate application domains\nof humor recognition. We indexed the forum data\nwith Elastic4 and returned a BM25-ranked list of\nmatching forum posts for each joke. To ﬁlter out\npotential occurrences of jokes in the forum data,\nwe removed all forum snippets with Jaccard sim-\nilarity higher than 0.4 to the query joke. This\nthreshold was inferred empirically from the data.\nAfter that, we added the highest-ranked post for\neach joke to the collection. Here is an example of\nsuch a joke/non-joke pair (hereafter, we cite En-\nglish translations of original texts in Russian):\n3https://www.e1.ru/talk/forum/\n4https://www.elastic.co/\n4029\nFigure 1: Item length distributions for jokes/non-jokes\nin the FUN dataset.\nFUN: Russian Mars rover will hand out Russian\npassports to the Martians.\nFORUM : They say in the Crimea, too, they are\nhanding out Russian passports or have already\nhanded them out.\nTo assess lexical diversity of the resulted\ndataset, we calculated KL-divergence of add-one\nsmoothed word frequency distributions of non-\njokes with respect to jokes for both S TIERLITZ\nand F UN. The resulted in 0.18 for F UN and 0.50\nfor STIERLITZ , which demonstrates that joke/non-\njoke classes in the new dataset are lexically more\nsimilar than in the S TIERLITZ dataset. We also\nexamined words with most frequency dispropor-\ntions in funny/non-funny parts of the dataset. Lo-\ncal toponyms, abbreviations, and non-standard\nspellings typical for online communications ap-\npeared to be the most salient words in the ‘un-\nfunny’ part of the dataset that stems from an online\nforum, while names of some jokes characters and\nmat (Russian profane language) are most typical\nfor the funny part.\nWhen compiling the dataset we introduced\nhigh/low cut-off thresholds for text lengths, but\ndidn’t try to balance out lengths distributions of\njokes/non-jokes subsets. Figure 1 shows that the\njokes’ length distribution is skewed towards longer\ntexts compared to non-jokes.\nWe also removed URLs and user names, retain-\ning only unique entries. Finally, we partitioned\nthe dataset into train/test sets (80:20) ensuring the\noriginal S TIERLITZ train and validation subsets\nbelong to FUN train and test subsets, respectively.\nGOLD : dataset validation. To verify that our\nautomatically created collection contains valid\njokes and non-jokes, we conducted an evalua-\ntion using an online interface, where 1,000 ran-\ndom jokes and 1,000 random non-jokes were as-\nsessed on a 3-point scale: ‘not a joke’, ‘an un-\nfunny joke’ and ‘a joke’. We were able to recruit\nmore than 100 volunteers through online social\nnetworks; evaluation resulted in 1,877 examples\nbeing labeled by at least three assessors. In case of\n238 items (12.7%) we could observe opposite as-\nsessments, i.e. ‘not a joke’ and ‘a joke’, which is\nan acceptable agreement for a crowdsourcing set-\nting. Majority voting resulted in 94% of non-jokes\nmarked as ‘not a joke’ and 95% of jokes marked as\neither ‘an unfunny joke’ or ‘a joke’, which demon-\nstrates a good performance of the automatic pro-\ncedure.5 The errors in the non-jokes are mostly\nhumorous responses from the forum users, for ex-\nample:\nInvite a girl, cook a dinner for two... but do not\nask “how to get rid of a girlfriend?” a week later.\nTexts from humorous sources marked as ‘not\na joke’ are examples of dry humor or context-\ndependent jokes, e.g.:\nTen to the power of thirty of electrons is almost a\nkilogram.\nTable 1 summarizes statistics of the datasets\nused in the study.\n4 Classiﬁcation Methods\nRecently, various neural network architectures\nhave achieved state-of-the-art results in many ar-\neas of natural language processing. Given that we\nhave a large enough corpus, we opted for universal\nlanguage model ﬁne-tuning method (ULMFiT) for\ntext classiﬁcation (Howard and Ruder, 2018) that\nhas demonstrated good performance and general-\nization capabilities.\nIn case of humor recognition, it is desirable to\nmodel deeper word and context dependencies, as\nhumorous effect is usually enabled by combina-\ntions of words rather than individual words them-\nselves. Language models (LMs) have been used\nas baselines in several humor recognition stud-\nies (Shahaf et al., 2015; Yang et al., 2015; Cattle\nand Ma, 2018). In contrast to most previous hu-\nmor recognition studies, we didn’t engineer any\n5For example, manual veriﬁcation of the dataset in (Mi-\nhalcea and Strapparava, 2005) revealed 9% of noise.\n4030\nlinguistic features. However, LM-based approach\ncan be seen as indirect reﬂection of some common\nhumor features such as incongruity, unexpected-\nness, or nonsense.\nOur humor corpora are relatively small com-\npared to the corpora that are used to train lan-\nguage models. To overcome this limitation, we\nﬁrst trained a language model on 10M online fo-\nrum texts for 15 epochs. Texts were tokenized\nusing unigram subword tokenization method im-\nplemented in SentencePiece library (Kudo and\nRichardson, 2018) with the vocabulary size of\n100,000. Architecture and parameters were di-\nrectly transferred from (Howard and Ruder, 2018).\nFurther, we used either STIERLITZ or FUN dataset\nto ﬁne-tune the model for ﬁve epochs. Finally,\nwe replaced the target task with humor classi-\nﬁer by augmenting the model with linear blocks\nand trained the model with gradual unfreezing fol-\nlowed by 14 consecutive epochs. We further refer\nto this model as ULMFun.\nAs a baseline classiﬁcation method, we chose\nan SVM classiﬁer on top of tf.idf features, which\nis usually a good starting point in text classiﬁca-\ntion tasks. In addition, the authors of (Ermilov\net al., 2018) kindly agreed to run their best learned\nmodel on our new dataset.\n5 Results and Discussion\nThe goals of the experiment were to estimate the\nimpact of the increased dataset size and its con-\nstruction methods, to introduce a strong baseline\nbased on deep neural network approach, to com-\npare it with a baseline and published work, as well\nas to evaluate generalization abilities of the model.\nIn the ﬁrst series of experiments we trained a\nlinear SVM baseline on tf.idf features and ULM-\nFun model on S TIERLITZ train set. We tested the\nobtained models on held-out test sets of S TIER -\nLITZ and FUN, as well as on smaller manually an-\nnotated GOLD and PUNS collections. In addition,\nwe were able to apply the best model from (Er-\nmilov et al., 2018) to the test data. Table 2 summa-\nrizes performance of the models. What stands out\nfrom the results is that baseline SVM outperforms\na previous feature-rich approach (Ermilov et al.,\n2018). Due to high lexical diversity between pos-\nitive and negative classes in S TIERLITZ , it seems\nto be trivial to distinguish between jokes and non-\njokes with lexical features only. Even a simple lin-\near model achieves F1 score of 0.91. Unsurpris-\nFigure 2: F1-scores for humor class depending on the\ntext length in FUN test set.\ningly, ULMFun outperforms both Stierlitz SVM\nand the baseline. Since F UN was constructed in\nquite a different way compared to STIERLITZ and\nrepresents a much harder task, classiﬁcation qual-\nity of all three methods decreases on FUN test set.\nFor instance, Stierlitz SVM achieves only 23% re-\ncall on FUN non-jokes. It is interesting to note that\nmore versatile Stierlitz SVM features demonstrate\nbetter transferability and help the method to beat\nthe baseline on ‘unfamiliar’ FUN. Performance of\nthe baseline is stable on G OLD , while the other\ntwo classiﬁers’ scores decrease more signiﬁcantly\nthan one can expect based on manual veriﬁcation\nresults, i.e. by 5-6%. Variance of recall scores\nof the three methods on P UNS is much higher,\nthough the results must be treated with caution due\nto small size of the collection.\nTable 3 represents results of baseline SVM\nmodel and ULMFun trained on F UN training set.\nAs expected, more data signiﬁcantly improve clas-\nsiﬁcation quality on F UN test set in case of both\nmethods. However, performance on presumably\n‘simpler’ STIERLITZ test set drops since F UN\ndataset is a lot more diverse in terms of joke\ntypes and topics. Performance of both methods on\nGOLD decreases less than by 5% of noise expected\nin the data.\nFigure 2 shows that the lowest humor detec-\ntion quality is observed for texts in the range\nfrom 50 to 100 characters, which can be ex-\nplained by the imbalance of the dataset in regard\nof length. Moreover, longer jokes are easier to\ndetect due to a richer context. Manual inspec-\ntion suggests that misclassiﬁed jokes can be di-\nvided into three categories. The most common\n4031\nModel STIERLITZ Test F1 FUN Test F1 GOLD F1 PUNS Recall\nBaseline SVM 0.910 0.677 0.643 0.725\nStierlitz SVM (Ermilov et al., 2018) 0.884 0.735 0.638 0.695\nULMFun 0.965 0.768 0.662 0.920\nTable 2: Humor detection quality – models trained on STIERLITZ train.\nModel STIERLITZ Test F1 FUN Test F1 GOLD F1 PUNS Recall\nBaseline SVM 0.787 0.798 0.803 0.436\nULMFun 0.921 0.907 0.890 0.892\nTable 3: Humor detection quality – models trained on FUN train.\none is jokes whose comprehension requires exter-\nnal world knowledge, for example:\nThe absolute record in worldwide compact disk\nsales was set by a little-known band called CD-\nR with its new single 700MB.\nThe following examples demonstrate two other er-\nror types – hard to get jokes, e.g.\nNo GMO, no artiﬁcial dyes, no plans for the fu-\nture, no meaning in life, and no preservatives.\nand noisy non-jokes from the positive class:\nWould you like to celebrate your birthday in Las\nVegas?\nSimilarly, misclassiﬁed examples from negative\nclass are occasionally present noisy jokes:\nNo doctor is as worried about the patient’s high\nheart beat rate as a pathologist.\nULMFun also triggers on context changes that are\ntypical for many jokes, for example:\nModel of an ideal person – and an out-of-class\nﬁre-breathing dragon!\n6 Conclusion and Future Work\nIn this paper, we introduced a publicly available\ndataset for humor recognition in Russian that ex-\nceeds in size all previous public datasets. We com-\npared the performance of a baseline SVM method\nand a more sophisticated ULMFiT method on this\ndataset, with the latter yielding favorable results.\nIn the future, we aim to analyze how changes\nin the training procedure and hyperparameters of\nULMFiT affect resulting model performance. On\ntop of that, we hope to improve model generaliza-\ntion by augmenting negative examples with a split\nof jokes into setups and punchlines, as they should\nnot be funny by themselves. We also plan to re-\nproduce the experiment on English data.\nAcknowledgments\nWe thank volunteers who took part in data annota-\ntion. We are also thanksful to authors of (Ermilov\net al., 2018), who ran their code on our data.\nThe authors acknowledge support from the\nMinistry of Education and Science of the Rus-\nsian Federation (project no. 1.3253.2017) and the\nCompetitiveness Program of the Ural Federal Uni-\nversity.\nReferences\nSalvatore Attardo. 1994. Linguistic Theories of Hu-\nmor. Walter de Gruyter.\nValeria Bolotova, Vladislav Blinov, Kirill Mishchenko,\nand Pavel Braslavski. 2017. Which IR model has a\nbetter sense of humor? Search over a large collec-\ntion of jokes. In Proceedings of the Dialogue Con-\nference, pages 29–42.\nSantiago Castro, Luis Chiruzzo, Aiala Ros ´a, Diego\nGarat, and Guillermo Moncecchi. 2018. A crowd-\nannotated Spanish corpus for humor analysis. In\nProceedings of SocialNLP Workshop.\nAndrew Cattle and Xiaojuan Ma. 2018. Recognizing\nhumour using word associations and humour anchor\nextraction. In COLING, pages 1849–1858.\nPeng-Yu Chen and V on-Wun Soo. 2018. Humor recog-\nnition using deep learning. In NAACL-HLT, pages\n113–117.\nAnton Ermilov, Natasha Murashkina, Valeria Gory-\nacheva, and Pavel Braslavski. 2018. Stierlitz Meets\nSVM: Humor Detection in Russian. In Artiﬁcial In-\ntelligence and Natural Language, pages 178–184.\nMd. Kamrul Hasan, Wasifur Rahman, Amir Zadeh,\nJianyuan Zhong, Md. Iftekhar Tanveer, Louis-\nPhilippe Morency, and Mohammed E. Hoque. 2019.\nUR-FUNNY: A multimodal language dataset for un-\nderstanding humor. CoRR, abs/1904.06618.\n4032\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nACL, pages 328–339.\nChloe Kiddon and Yuriy Brun. 2011. That’s what she\nsaid: double entendre identiﬁcation. In ACL-HLT,\npages 89–94.\nTaku Kudo and John Richardson. 2018. Sentence-\nPiece: A simple and language independent subword\ntokenizer and detokenizer for Neural Text Process-\ning. In EMNLP: System Demonstrations, pages 66–\n71.\nLizhen Liu, Donghai Zhang, and Wei Song. 2018. Ex-\nploiting syntactic structures for humor recognition.\nIn COLING, pages 1875–1883.\nRod A. Martin. 2007. The Psychology of Humor: An\nIntegrative Approach. Elsevier.\nRada Mihalcea and Carlo Strapparava. 2005. Making\ncomputers laugh: Investigations in automatic humor\nrecognition. In HLT-EMNLP, pages 531–538.\nTristan Miller, Christian Hempelmann, and Iryna\nGurevych. 2017. SemEval-2017 Task 7: Detection\nand Interpretation of English Puns. In Proceedings\nof the SemEval Workshop, pages 58–68.\nAnton Nijholt, Andreea Niculescu, Alessandro Vali-\ntutti, and Rafael E. Banchs. 2017. Humor in human-\ncomputer interaction: A short survey. In Adjunct\nConference Proceedings INTERACT , pages 192–\n214.\nReynier Ortega-Bueno, Carlos E Muniz-Cuza, Jos ´e\nE Medina Pagola, and Paolo Rosso. 2018. UO UPV:\nDeep Linguistic Humor Detection in Spanish Social\nMedia. In Proceedings of the IberEval Workshop).\nPeter Potash, Alexey Romanov, and Anna Rumshisky.\n2017. SemEval-2017 Task 6: #HashtagWars:\nLearning a Sense of Humor. In Proceedings of the\nSemEval Workshop, pages 49–57.\nAntonio Reyes, Davide Buscaldi, and Paolo Rosso.\n2009. An analysis of the impact of ambiguity on\nautomatic humour recognition. In Text, Speech and\nDialogue, pages 162–169.\nDafna Shahaf, Eric Horvitz, and Robert Mankoff.\n2015. Inside jokes: Identifying humorous cartoon\ncaptions. In PKDD, pages 1065–1074.\nJulia M Taylor and Lawrence J Mazlack. 2004. Com-\nputationally recognizing wordplay in jokes. In Pro-\nceedings of the Annual Meeting of the Cognitive Sci-\nence Society, pages 1315–1320.\nDiyi Yang, Alon Lavie, Chris Dyer, and Eduard Hovy.\n2015. Humor recognition and humor anchor extrac-\ntion. In EMNLP, pages 2367–2376.\nRenxian Zhang and Naishi Liu. 2014. Recognizing hu-\nmor on Twitter. In CIKM, pages 889–898.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.795852780342102
    },
    {
      "name": "Natural language processing",
      "score": 0.7121136784553528
    },
    {
      "name": "Task (project management)",
      "score": 0.6751580834388733
    },
    {
      "name": "Baseline (sea)",
      "score": 0.6673921942710876
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6439881324768066
    },
    {
      "name": "Annotation",
      "score": 0.6397859454154968
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.6197261810302734
    },
    {
      "name": "Process (computing)",
      "score": 0.5039836764335632
    },
    {
      "name": "Reliability (semiconductor)",
      "score": 0.472024142742157
    },
    {
      "name": "Test set",
      "score": 0.46987396478652954
    },
    {
      "name": "Language model",
      "score": 0.4434194266796112
    },
    {
      "name": "Test (biology)",
      "score": 0.44224363565444946
    },
    {
      "name": "Speech recognition",
      "score": 0.33984068036079407
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130823665",
      "name": "Ural Federal University",
      "country": "RU"
    },
    {
      "id": "https://openalex.org/I118501908",
      "name": "National Research University Higher School of Economics",
      "country": "RU"
    }
  ]
}