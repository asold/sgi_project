{
    "title": "Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora",
    "url": "https://openalex.org/W4287890137",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2798730168",
            "name": "Xisen Jin",
            "affiliations": [
                "Southern California University for Professional Studies",
                "University of Southern California"
            ]
        },
        {
            "id": "https://openalex.org/A2120360973",
            "name": "Dejiao Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2513492337",
            "name": "Henghui Zhu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1966630993",
            "name": "Wei Xiao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287859061",
            "name": "Shang-Wen Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2405326445",
            "name": "Xiaokai Wei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2107934253",
            "name": "Andrew Arnold",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2108009659",
            "name": "Xiang Ren",
            "affiliations": [
                "Southern California University for Professional Studies",
                "University of Southern California"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2554863749",
        "https://openalex.org/W3153675281",
        "https://openalex.org/W2005874308",
        "https://openalex.org/W2995409942",
        "https://openalex.org/W3190540921",
        "https://openalex.org/W3104186312",
        "https://openalex.org/W3173870405",
        "https://openalex.org/W4286856923",
        "https://openalex.org/W4287322461",
        "https://openalex.org/W2279376656",
        "https://openalex.org/W3121052760",
        "https://openalex.org/W2963733234",
        "https://openalex.org/W4294554825",
        "https://openalex.org/W3015453090",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W2798837230",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2806198715",
        "https://openalex.org/W2962783425",
        "https://openalex.org/W2963718112",
        "https://openalex.org/W3171057731",
        "https://openalex.org/W3105966348",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2970454332",
        "https://openalex.org/W4287111051",
        "https://openalex.org/W2577986441",
        "https://openalex.org/W3115677442",
        "https://openalex.org/W4287112127",
        "https://openalex.org/W4324392584",
        "https://openalex.org/W3099543642",
        "https://openalex.org/W2895723011",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W3171460770",
        "https://openalex.org/W3202099651",
        "https://openalex.org/W3213460052",
        "https://openalex.org/W3098170909",
        "https://openalex.org/W3153269634",
        "https://openalex.org/W3156636935",
        "https://openalex.org/W3103978933",
        "https://openalex.org/W3034623328",
        "https://openalex.org/W3035067238",
        "https://openalex.org/W2473930607",
        "https://openalex.org/W2804175194",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3177466351",
        "https://openalex.org/W4288336773",
        "https://openalex.org/W2964189064",
        "https://openalex.org/W2116522068",
        "https://openalex.org/W2947461406",
        "https://openalex.org/W2966049804",
        "https://openalex.org/W2962903510",
        "https://openalex.org/W3156785025",
        "https://openalex.org/W2808556605"
    ],
    "abstract": "Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-Wen Li, Xiaokai Wei, Andrew Arnold, Xiang Ren. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
    "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 4764 - 4780\nJuly 10-15, 2022 ¬©2022 Association for Computational Linguistics\nLifelong Pretraining: Continually Adapting Language Models\nto Emerging Corpora\nXisen Jin‚Ä†1 Dejiao Zhang2 Henghui Zhu2 Wei Xiao2\nShang-Wen Li‚Ä°2 Xiaokai Wei2 Andrew Arnold2 Xiang Ren1\n1University of Southern California 2AWS AI Labs\n{xisenjin,xiangren}@usc.edu\n{dejiaoz, henghui, weixiaow, shangwenl, xiaokaiw, anarnld}\n@amazon.com\nAbstract\nPretrained language models (PTLMs) are typi-\ncally learned over a large, static corpus and fur-\nther Ô¨Åne-tuned for various downstream tasks.\nHowever, when deployed in the real world, a\nPTLM-based model must deal with data dis-\ntributions that deviate from what the PTLM\nwas initially trained on. In this paper, we\nstudy a lifelong language model pretraining\nchallenge where a PTLM is continually up-\ndated so as to adapt to emerging data. Over\na domain-incremental research paper stream\nand a chronologically-ordered tweet stream,\nwe incrementally pretrain a PTLM with dif-\nferent continual learning algorithms, and keep\ntrack of the downstream task performance (af-\nter Ô¨Åne-tuning). We evaluate PTLM‚Äôs ability\nto adapt to new corpora while retaining learned\nknowledge in earlier corpora. Our experiments\nshow distillation-based approaches to be most\neffective in retaining downstream performance\nin earlier domains. The algorithms also im-\nprove knowledge transfer, allowing models to\nachieve better downstream performance over\nthe latest data, and improve temporal gen-\neralization when distribution gaps exist be-\ntween training and evaluation because of time.\nWe believe our problem formulation, methods,\nand analysis will inspire future studies towards\ncontinual pretraining of language models.\n1 Introduction\nPretrained language models (PTLMs) have\nachieved remarkable performance on benchmark\ndatasets for a range of NLP tasks (Liu et al., 2019b;\nBrown et al., 2020). However, when deployed in\nthe wild, NLP systems must deal with emerging\ndata that have constantly shifting data distribution,\ndifferent from the text corpora they were initially\npretrained on ‚Äî for example, when new data do-\nmains are introduced (upper part of Fig. 1) (Gu-\nrurangan et al., 2020), or when the language uses\n‚Ä† Work done during an internship at AWS AI Labs.\n‚Ä° Work done while at Amazon.\nDomain-Incremental Research Paper Stream\nKnowledgeretention\nAll domains\nChronologically-Ordered Tweet Stream\n2014201620182020Adaptation & Temporal generalization\nLatest data\nBio-MedicalComputer ScienceMaterials SciencePhysics\nFigure 1: Two data streams created for studying life-\nlong language model pre-training. We focus on evalu-\nating knowledge retention on the domain-incremental\nresearch papers stream; we focus on adaptation to the\nlatest data and temporal generalization on the chrono-\nlogically ordered tweet stream.\nand vocabulary change over time (lower part of\nFig. 1) (Lazaridou et al., 2021). Fine-tuning from a\nstatic and possibly ‚Äúoutdated\" PTLM may limit the\nmodel performance on downstream tasks, as the\nPTLM may no longer provide an effective model\ninitialization (Beltagy et al., 2019; M√ºller et al.,\n2020). Here we look to understand whether con-\ntinuously adapting a PTLM to emerging data can\nyield gains on various downstream tasks, and how\nto achieve better downstream performance for such\nlifelong PTLM adaptation.\nA number of recent works make attempts on\nadapting PTLMs to a new data domain. Gururan-\ngan et al. (2020); Yao et al. (2021) adapt language\nmodels to corpora of different genres and topics\nand observe performance improvement in domain-\nspeciÔ¨Åc downstream tasks. Arumae et al. (2020)\nfurther show that by regularizing the parameters\nof PTLMs, the downstream tasks performance on\nthe general domain can be preserved. Another line\nof works focuses on temporal domain shift (Hom-\nbaiah et al., 2021), which analyzes the effect of\npretraining over up-to-date data to the downstream\n4764\ntasks. R√∂ttger and Pierrehumbert (2021) further\nstudy vocabulary composition approaches for im-\nproving adaptation to up-to-date corpora. However,\nthese work focus their study on adapting PTLM\nto a single new domain; while in practice, cor-\npora from distinct domains and time stamps may\nemerge sequentially. Whether one can maintain\na single, up-to-date PTLM remains an open prob-\nlem. Related to this, Lazaridou et al. (2021) study\nadaptation of PTLMs over temporal data streams,\nbut solely focus on language modeling instead of\nÔ¨Åne-tuning performance. It is also important to un-\nderstand multiple aspects of the utility of lifelong\nPTLM pretraining, such as knowledge retention\nover all the seen data, and study what methods can\nimprove the utility of PTLMs in such a continual\npretraining process.\nIn this paper, we formulate a Lifelong Language\nModel Pretraining task to simulate practical sce-\nnarios of maintaining and adapting a PTLM over\nemerging corpora, create a testbed (along with\npretraining data streams and downstream tasks)\nfor studying continual pretraining algorithms, and\npresent a systematic evaluation protocol for measur-\ning the progress made on this challenging problem\n(see Figure 2 for an illustration). We consider two\ntypes of text corpus sequences when constructing\npretraining data streams, each of which simulates a\nrepresentative use case and that has slightly differ-\nent focuses on the evaluation: continuously learn-\ning a single model that is applicable to both old and\nnew domains; and improving the model‚Äôs ability to\nhandle latest data. SpeciÔ¨Åcally, we construct 1) a\ndomain-incremental text stream that consists of aca-\ndemic papers published in four research Ô¨Åelds, and\n2) a temporal tweet stream that consists of tweets\ncollected from four different years. By conducting\nsystematic experiments on these two data streams,\nwe look to answer a series of analysis questions:\n1) whether continual pretraining retains Ô¨Åne-tuning\nperformance over earlier corpora compared to tra-\nditional ofÔ¨Çine pretraining, 2) whether pretraining\nimproves downstream performance on the latest\ndata, and 3) whether pretraining improves temporal\ngeneralization where training and evaluation have\ndistribution gaps because of time.\nTo address the research questions above, we con-\nduct a systematic evaluation of existing continual\nlearning (CL) algorithms, spanning over model-\nexpansion based, memory-based, and distillation-\nbased approaches. Our results show distillation-\nbased approaches are most effective in knowledge\nretention in the research paper stream, while si-\nmultaneously improve adaptation to latest data and\ntemporal generalization in the tweet stream. We\nbelieve our problem formulation, evaluation setup,\nmethods and analysis can inspire more future work\non continual pretraining of language models.\n2 Problem Formulation\nHere we present the problem formulation for life-\nlong pretraining of PTLM, provide details about the\ndata stream construction process and downstream\ntasks, and introduce the evaluation protocol.\n2.1 Lifelong Pretraining of PTLMs\nWe consider the scenario where one needs to de-\nploy and/or maintain NLP models over a sequence\nof T data domains. At each time step tthe model\nvisits an unlabeled text corpus Dt from a domain\nwith a data distribution P(Dt). The data distribu-\ntion P(Dt) evolves as the time step t, forming a\ndata stream D1..T = {D1,D2,...DT }. In practice,\nthe data domain shift can refer to the topic change\nof the text content (from computer science research\npapers to biomedical papers), or temporal evolution\nof the text (from past to recent tweets). The task of\nlifelong pretraining of PTLM looks to continuously\nadapt a language model f as the model visits (unla-\nbeled) text corpus Dt from the data stream D1..T ,\nin order to provide a good model initialization for\nÔ¨Åne-tuning on downstream tasks from the same do-\nmain. With slight abuse in notations, we also use\nDt to directly refer to a data domain.\nHere, we assume a language model f is updated\nsequentially over each pretraining corporaDt, with-\nout accessing the full earlier corpora{Di}i<t in the\ndata stream D1..T . This aims to capture practical\nconstraints such as privacy restriction for storing\nearlier data, or computation budget for training\nover all the text corpora in D1..T . We use ft to\ndenote the language model right after updating on\nthe domain Dt. In our study, f is a RoBERTa-base\ntransformer (Liu et al., 2019b) and the model (f0)\nis initialized with pretrained RoBERTa weights.\nThe utility of the PTLMs{ft}is evaluated based\non their Ô¨Åne-tuned model performance on various\ndownstream tasks. After updating on a domain\nDi, the model fi can be Ô¨Åne-tuned over down-\nstream tasks from visited domains Dt where t‚â§i.\nWe note the set of downstream tasks related to do-\nmain Dt as St = {Sj\nt }Nt\nj=1, assuming the number\n4765\nContinual Pretraining‚Ä¶ùê∑! ùê∑\" ùê∑# ùëì#\nùëì! Knowledge Retention\nTrain\nùëÜ\"\nOldOld\nTest\n‚Ä¶ùê∑$Fine-Tuning & Evaluation\nTrain\nLatestLatest\nTest\nAdaptation toLatestData Temporal GeneralizationùëÜ\" Train\nùëÜ\"\n ùëÜ!\nOldLatest\nTestùëÜ! ùëÜ!\nFigure 2: Training, evaluation setups, and metrics of life-\nlong language model pretraining.The model sequentially\nvisits each corpus, and is Ô¨Åne-tuned on downstream datasets\nrelated to the domains of pretraining. We evaluate knowl-\nedge retention and adaptation to new data with downstream\nÔ¨Åne-tuning performance on old and latest domains respec-\ntively. Besides, we evaluate temporal generalization where\ntraining/test examples are drawn from different time steps.\nof downstream tasks is Nt. Note that in the Ô¨Åne-\ntuning stage, model ft has no access to any of the\npretraining corpus D1..T .\n2.2 Data Streams & Downstream Datasets\nWe construct data streams to simulate two repre-\nsentative scenarios of data domain shifts in practice\n(also see Fig. 1): one domain-incremental stream to\nsimulate the sequential changes of research paper\nareas; and one chronologically-ordered stream to\nsimulate tweets emerging over time.\nDomain-incremental Paper Stream. This pa-\nper stream consists of the full text of research pa-\npers published in four research areas: biomedical,\ncomputer science, material science, and physics,\nÔ¨Åltered from the S2ORC dataset 1, which are pre-\nsented sequentially to the model. For each domain,\nwe evaluate downstream performance over two\ndatasets. The downstream tasks span over vari-\nous tasks such as relation extraction and named\nentity recognition, and are summarized in Table 1.\nWe detail these datasets in Appendix D.\nChronologically-ordered Tweet Stream. This\ntweet data stream consists of tweets from the\nyear 2014, 2016, 2018 and 2020, collected\nby the Archive Team 2 and preprocessed follow-\ning Nguyen et al. (2020). These four tweet corpora\nare presented sequentially to the language model\nfollowing the chronological order of the tweet year.\n1We use the 20200705v1 version of the S2ORC dataset at https://\ngithub.com/allenai/s2orc\n2https://archive.org/details/twitterstream\nDomains Downstream Datasets Metrics\nBio-Medicine Chemprot (Vindahl, 2016) Micro-F1\nRCT-Sample (Dernoncourt and Lee, 2017) Micro-F1\nComp. Science ACL-ARC (Jurgens et al., 2018) Macro-F1\nSciERC (Luan et al., 2018) Macro-F1\nMat. Science Synthesis (Mysore et al., 2019) Macro-F1\nMNER (Olivetti et al., 2020) Micro-F1\nPhysics Keyphrase (Augenstein et al., 2017) Macro-F1\nHyponym (Augenstein et al., 2017) Macro-F1\nTable 1: Summary of downstream datasets relevant to\neach domain in the research paper stream.\nFor downstream tasks, we hold out 1M tweets from\neach year‚Äôs corpus to construct multi-label hash-\ntag prediction datasets (Gong and Zhang, 2016)\nand single-label emoji prediction datasets (Barbieri\net al., 2018). On two datasets, we report label rank-\ning average precision scores (a multi-label version\nof MRR) of models (Azeemi and Waheed, 2021)\nand Macro-F1 respectively. The detailed dataset\nconstruction process is included in Appendix D.\n2.3 Evaluation Protocol\nWe consider three key aspects for evaluating the\nutility of the language models {ft}that are con-\ntinuously updated over the data stream D1..T , also\nillustrated in Figure 2: 1) knowledge retention and\ntransfer over the pretraining corpora seen earlier;\n2) adaptation to the latest data domain, and 3) tem-\nporal generalization when training and evaluation\ndata are from different time steps.\nKnowledge Retention. A key utility of contin-\nual language model pretraining is to obtain a sin-\ngle model applicable to all domains. We focus\non the evaluation of the ability with the domain-\nincremental paper stream, because for the tweet\nstream, the practical need of performance over out-\ndated data is limited. Knowledge retention is mea-\nsured with the downstream task performance from\nearlier or the current domains that the pretrained\nmodel has visited. More formally, for each pre-\ntrained model checkpoint in {fi}, we Ô¨Åne-tune fi\nover downstream tasks {St}where t‚â§iand eval-\nuate the corresponding test set performance. It is\nimportant that the models do not suffer from catas-\ntrophic forgetting (Robins, 1995), i.e., signiÔ¨Åcantly\nreduced helpfulness when fi is Ô¨Åne-tuned for down-\nstream tasks St from earlier domains with t<i .\nAdaption to Latest Data Domain. In certain\nscenarios, performance of downstream models over\nthe latest data domain should be emphasized. For\nexample, classiÔ¨Åers in the tweet domain are usually\n4766\ntrained and evaluated with up-to-date data for prac-\ntical deployment. Formally, we focus on the down-\nstream task performance of models Ô¨Åne-tuned from\nthe Ô¨Ånal pretrained model checkpoint fT , where\nthe downstream tasks ST are also from the latest\ndomain. To succeed in these metrics, it is crucial\nfor the model to transfer knowledge from earlier\ndomains to the latest domain.\nTemporal Generalization Ability. We consider\nanother practical Ô¨Åne-tuning scenario in the tweet\nstream where the model is trained on outdated\ndata and evaluated on the latest data (Rijhwani\nand Preotiuc-Pietro, 2020; Huang and Paul, 2018),\nreferred to as the temporal generalization ability.\nFormally, we Ô¨Åne-tune the Ô¨Ånal pretrained model\ncheckpoint fT over the training set of downstream\ntasks St from an earlier time step ( t < T), and\nevaluate on the test set of the downstream tasks ST\nfrom the latest time step T.\n3 Methods\nLifelong language model pretraining introduces\nnovel challenges because of the large training sets\nand more comprehensive evaluation protocols com-\npared to classiÔ¨Åcation tasks. We establish several\nstrong baselines, and evaluate the performance of\ncontinual learning algorithms from different cate-\ngories spanning over model-expansion, memory-\nbased, and distillation-based approaches, We illus-\ntrate the approaches in Figure 3.\n3.1 Simple Baselines\nWe consider several simple baselines which contin-\nual learning algorithms will be compared against.\nRoBERTa-base (f0) corresponds to not pre-\ntraining on any of the domain-speciÔ¨Åc corpora.\nBy separately pretraining f0 on each corpus\nD1,D2,...DT , we obtain T Task-Specific\npretrained models. We also pretrain f0 sequentially\nover D1..T , which we refer to as sequential\npretraining. While it allows knowledge trans-\nfer between domains compared to domain-speciÔ¨Åc\nmodels, without any continual learning algorithms,\nsequential pretraining is prone to catastrophic for-\ngetting (Robins, 1995). Finally, we randomly\nshufÔ¨Çe corpora from all domains D1..T before\npretraining, noted as Multi-Task Learning\n(MTL). MTL corresponds to an ofÔ¨Çine training\nparadigm that models new corpora by re-training\nover all corpora seen before. The drawback is that\nit requires storing full data from earlier domains,\nMemory Replay\nDistillation + CLAdapters \nùëì!ùëî\"#$! ùëî\"#%! ùëî\"!ùëì!&%\nLayer ùëò\nLayer ùëò+1\nReplay memoryùë•'ùë•( ùëì\"MemoryStream\nReplay memory\nùëì\"#%{ùë•(,ùë•'}ùëì\"ùë¶\"#% ùë¶\"‚Ñé\"#% ‚Ñé\"\nLogit DistillationRep. Distillationùêµ\"#% ùêµ\"Contrast&SEEDDistillation\n~~~Similarity matrix\nFigure 3: Comparison of adapter, memory re-\nplay, and distillation-based continual learning algo-\nrithms. Details of the methods are introduced in Sec. 3.\nand that it can be extremely costly to repetitively\nretrain over earlier data if new data keeps emerging.\n3.2 Model-expansion and\nRegularization-based Methods\nWe Ô¨Årst introduce model-expansion based ap-\nproaches, which add small trainable modules (e.g.,\nmulti-layer perceptron) to the model per new do-\nmain while keeping other parts of the model frozen.\nThe Adapter approach is a representative ap-\nproach that learns a set of ‚Äúadapter‚Äù layers gt =\n{gk\nt }K\nk=1 for each domain Dt and each of the K\ntransformer layers (Houlsby et al., 2019). We also\nexperiment with a simple Layer Expansion\napproach, which learns separate top two layers of\nthe transformer and the prediction head for each\ndomain. We also involve a regularization-based\ncontinual learning baseline, online EWC (Schwarz\net al., 2018), which directly penalize change of\nmodel parameters.\n3.3 Memory Replay Methods\nWe also experiment with Experience Replay\n(ER) (Chaudhry et al., 2019), which alleviates for-\ngetting by storing a subset of earlier examples and\nperiodically re-training (replaying) over them. We\nmaintain a Ô¨Åxed-size memory M (100kexamples\nby default) and populate the memory M each time\npretraining on a domain Dt Ô¨Ånishes with examples\nin the current domain. We ensure M always con-\ntains a balanced sample of examples from all seen\ndomains D1..t. We replay a mini-batch of examples\nfrom the memory every 10 training steps.\n3.4 Distillation-based CL Methods\nWhile knowledge distillation (KD) (Hinton et al.,\n2015) techniques have been studied intensively for\npretrained language models (Sun et al., 2019), ap-\nplying them to continual learning has been under-\n4767\nexplored outside image classiÔ¨Åcation tasks (Li and\nHoiem, 2018; RebufÔ¨Å et al., 2017; Hou et al., 2018).\nDistillation based CL approaches store one previ-\nous model checkpoint of the model (noted as ft‚àí1)\nand regularize the differences between ft‚àí1 and\nthe current model ft. We adapt several existing\nknowledge distillation techniques to PTLMs and\nutilize them for continual learning. We note, while\nindividual distillation techniques are not original,\ntheir adaptation to CL algorithms can be novel.\nWe perform distillation with examples from the\ncurrent domain Dt and a replay memory M (sim-\nilar to ER). Despite the potential gap between Dt\nand the training data of ft‚àí1, the approach allows\nutilizing more data for distillation. Formally, each\ntime the model receives a mini-batch of stream\nexamples xs or a draws mini-batch of memory ex-\namples xm from M (both noted as x), we collect\ncertain outputs of the model (e.g., output logits or\nintermediate representations) with ft‚àí1 and ft. We\ncompute a distillation loss ‚ÑìKD(x,ft‚àí1,ft) that pe-\nnalizes the differences between the model outputs,\nand jointly optimize it with the masked language\nmodeling loss ‚ÑìMLM. The Ô¨Ånal objective is written\nas ‚Ñì= ‚ÑìMLM + Œ±‚ÑìKD, where Œ±is a hyperparameter\nto weight the distillation loss.\nLogit Distillation. In logit distillation (Hinton\net al., 2015), we collect the output logits of ft and\nft‚àí1, noted as yt and yt‚àí1 respectively. The dis-\ntillation loss is computed as DKL(yt,yt‚àí1), where\nDKL is the Kullback‚ÄìLeibler divergence function.\nRepresentation Distillation. We also consider\nminimizing the representational deviation of sen-\ntences between previous and current models (Sun\net al., 2019; Jiao et al., 2020). We extract the rep-\nresentation of each word of two models, noted\nas h1:N\nt‚àí1 and h1:N\nt , before the masked language\nmodeling prediction head, where N is the length\nof the sentence. Then, we compute MSE loss\n||h1:N\nt‚àí1 ‚àíh1:N\nt ||2\n2 as the distillation loss.\nContrastive Distillation. In addition to output\nlogits and hidden representations, we further look\ninto representational similarity within a batch of\nexamples as additional knowledge to distill. The\napproach is adapted from (Cha et al., 2021), which\nis originally studied for supervised image classiÔ¨Å-\ncation tasks. We brieÔ¨Çy introduce the adapted algo-\nrithm and leave the details in Appendix E. During\ncontinual pretraining, in addition to the language\nmodel pretraining objective, we add an unsuper-\nvised contrastive learning objective, namely the\nSimCSE (Gao et al., 2021) objective to encourage\nsentence representations to reÔ¨Çect semantic simi-\nlarities between sentences. Then, we compute the\nintra-batch representational similarity matrices of\nsentence representations (i.e. between each pair of\nexamples in the mini-batch) withft‚àí1 and ft, noted\nas Bt‚àí1 and Bt, and minimize the cross entropy\nloss ‚Ñìdistill = ‚àí1\nN\n‚àëN\ni=1\n‚àëN\nj=1 Bt‚àí1\nij log Bt\nij\nSelf-Supervised Distillation (SEED). SEED\ndistillation proposed by (Fang et al., 2021) has a\nsimilar spirit as the contrastive distillation. The\nonly difference is that it distills representational\nsimilarity between the batch and a large set of\nother examples. We leave the details of the algo-\nrithm in Appendix E. We further combine SEED\nDistillationwith logit distillation and refer to the\napproach as SEED-Logit Distillation.\n4 Results\nWe summarize our Ô¨Åndings over the created data\nstreams. We ask whether lifelong pretraining and\ncontinual learning algorthms are effective base on\nour evaluation protocol proposed in Sec. 2.3.\n4.1 Experiment Settings\nWe use the RoBERTa-base model (Liu et al.,\n2019b), initialized with RoBERTa-base weights\nthroughout the experiments. We set the maximal\nsequence length to 128 and an effective training\nbatch size of 2,048. On the research paper stream,\nmodels are trained for 8ksteps in the Ô¨Årst domain\nand 4k steps in the subsequent domains. On the\nTweet stream, we train the models for 4ksteps in\neach domain. These correspond to less than a single\npass of data in each domain. See Appendix A for\ndetailed setups.\n4.2 Domain Incremental Data Stream\nAs we introduced in Sec. 2.2, in the domain incre-\nmental research paper stream, we expect a model\nft to perform well on all downstream tasks S1..t\nfrom domains D1..t. In Table 2, we report the per-\nformance of models on all downstream tasks S1..T\nÔ¨Åne-tuned from the Ô¨Ånal pretraining checkpoint,\nfT . We visualize more complete change of down-\nstream task performance over different time steps\nof pretraining ( i.e.,, f1,f2,f3,f4) in Fig. 4. We\nalso report the log perplexity of masked language\nmodeling (MLM) in Table 2 as additional informa-\n4768\nTask D1- Biomedical D2- Computer ScienceD3- Materials Science D4- Physics\nDataset Chemprot RCT-Sample MLM ACL-ARC SciERC MLM MNER Synthesis MLM Keyphrase Hyponym MLM\nRoberta-base 82.03¬±0.7 78.07¬±0.7 1.993 64.32¬±2.8 79.07¬±1.6 2.153 83.15¬±0.3 91.25¬±0.6 2.117 66.21¬±1.0 67.59¬±4.5 2.278Sequential Pretraining 82.09¬±0.5 79.60¬±0.5 1.654 72.73¬±2.9 81.43¬±0.8 1.80783.99¬±0.3 92.10¬±1.0 1.59067.57¬±1.0 74.68¬±4.4 1.381\nER 82.73 ¬±0.3 79.98¬±0.3 1.737 72.50¬±1.0 81.64¬±1.1 1.85783.99¬±0.4 92.65¬±0.4 1.621 66.11¬±1.1 72.82¬±4.3 1.391Online EWC 81.83¬±0.2 78.84¬±0.5 1.655 71.81¬±2.6 80.79¬±0.5 1.803 83.43¬±0.4 91.89¬±0.5 1.571 66.70¬±0.6 72.98¬±6.0 1.388Adapter 83.30 ¬±0.4 80.41¬±0.4 1.417 69.32¬±3.5 80.22¬±1.5 1.63383.91¬±0.3 91.69¬±0.6 1.522 66.23¬±1.4 69.65¬±4.5 1.554Layer Expansion 83.74¬±0.3 81.10¬±0.5 1.21065.17¬±2.9 79.35¬±0.8 1.756 82.48¬±0.4 92.33¬±1.0 1.389 65.70¬±1.1 73.34¬±3.7 1.534Logit-KD 83.39 ¬±0.4 81.21¬±0.1 1.39273.70¬±3.4 81.92¬±0.8 1.699 83.96¬±0.3 92.20¬±1.0 1.42564.75¬±1.1 71.29¬±3.6 1.460Rep-KD 82.34 ¬±0.3 79.59¬±0.5 1.684 71.17¬±2.5 78.78¬±1.1 1.810 84.13¬±0.3 92.02¬±0.8 1.585 65.96¬±1.6 73.93¬±5.5 1.389Contrast-KD 82.29¬±0.5 79.92¬±0.4 1.722 71.15¬±1.1 80.49¬±1.6 1.856 83.26¬±0.4 92.62¬±0.7 1.612 65.95¬±1.7 72.26¬±3.1 1.428SEED-KD 82.78 ¬±0.3 80.38¬±0.4 1.720 69.98¬±2.4 81.61¬±0.7 1.829 82.99¬±0.4 92.35¬±0.7 1.609 65.35¬±1.0 74.79¬±4.1 1.401SEED-Logit-KD83.72¬±0.4 81.05¬±0.2 1.391 69.90¬±4.5 83.03¬±0.6 1.703 83.28¬±0.5 92.87¬±1.0 1.428 65.96¬±1.5 71.92¬±5.5 1.460\nTask-SpeciÔ¨Åc LM 83.74¬±0.3 81.10¬±0.5 1.210 72.20¬±2.6 81.24¬±1.7 1.629 84.02¬±0.2 91.56¬±0.4 1.418 65.95¬±1.1 69.43¬±4.5 1.426MTL 82.91 ¬±1.6 80.67¬±0.4 1.289 69.46¬±1.8 81.12¬±0.8 1.616 83.92¬±0.3 92.66¬±0.6 1.355 65.37¬±1.6 73.31¬±5.2 1.418\nTable 2: Results on the Research Paper stream.We report log perplexity of MLM and the performance of downstream\nmodels Ô¨Åne-tuned from the Ô¨Ånal checkpoint of the pretrained model (t = 4). Performance of the best performing CL algorithm\nis marked bold.\n|M|, k Chemprot RCT ACL-ARC SciERC MLM-D1,2\n100k,10 82.73 79.98 72.50 81.64 1.737/1.857\n100k,100 82.06 78.64 71.97 81.62 1.599/1.789\n10M,10 82.87 79.98 71.80 81.63 1.438/1.732\nTable 3: Downstream task and MLM performance of fT\nunder different memory sizes |M| and the frequency of replay\nk (replaying every k steps of training) in ER.\ntion. With these results, we address the research\nquestions below.\nDoes lifelong pretraining help retain knowledge\nacross different domain corpora? We Ô¨Årst ex-\namine whether task-speciÔ¨Åc or lifelong pretraining\nimproves performance over domain-speciÔ¨Åc down-\nstream tasks. Comparing Task-SpeciÔ¨Åc LMs with\nRoBERTa-base in Table 2, we notice consistent per-\nformance improvements, especially on Biomedical\nand Computer Science domains (D1,D2). We also\nsee Sequential Pretraining could consistently out-\nperform RoBERTa-base. However, the comparison\nbetween Sequential Pretraining and Task SpeciÔ¨Åc\nLMs are mixed: on D1,D2,D3, Sequential Pre-\ntraining could outperform Task-SpeciÔ¨Åc LMs only\nexcept MNER; while on the earliest biomedical\ndomain (D1), Sequential Pretraining achieves sub-\nstantially lower performance. From Figure 4, we\nsee the performance of Sequential Pretraining on\nChemprot and RCT (from D1) drops signiÔ¨Åcantly\nfrom t= 1to 4. The results imply lifelong pretrain-\ning allows later domains to beneÔ¨Åt from knowledge\ntransfer from earlier domains, but the performance\non earlier domains is limited because of forgetting.\nDoes continual learning algorithms help retain\nknowledge in sequential pretraining? Next, we\ncompare different kinds of CL algorithms and in-\nvestigate the effect of CL algorithms in alleviating\nforgetting and improving knowledge transfer. Ta-\nble 2 shows that Online-EWC slightly improves\nMLM perplexity compared to Sequential PT, but\nbrings no improvement to the Ô¨Åne-tuning perfor-\nmance. We hypothesize that regularization directly\nin the parameter space as in Online-EWC is not\neffective when the parameter space is very high\ndimensional. Adapter improves downstream task\nF1 scores on the bio-medical domain (D1) by 1.2%\nand 0.8%, but does not outperform Sequential Pre-\ntraining in other domains (similarly for Simple\nLayer Expansion approach), likely because a great\nportion of the model is kept frozen.\nIn contrast, the memory-replay based approach\n(ER) allows training the full parameters of the\nmodel and has been shown to be highly effective\nin continual learning of classiÔ¨Åcation tasks (Wang\net al., 2019; Chaudhry et al., 2019). However, we\nsurprisingly Ô¨Ånd that ER could hardly improve over\nSequential Pretraining exceptD1. A similar pattern\ncan be found in the MLM perplexity. We hypothe-\nsize that the positive effect of example replay has\ndiminished because of the overÔ¨Åtting to the mem-\nory examples. Table 3 summarizes the effect of\ntuning hyperpameters in ER. When we reduce the\nfrequency of replay (from every 10 steps to 100\nsteps), the MLM performance improves, which im-\nplies reduced overÔ¨Åtting; however, the performance\nof downstream task performance does not improve.\nWhen we increase the size of the memory|M|from\n100kto 10M, the MLM perplexity also improves;\nstill, there are still no improvements in downstream\ntasks. It may imply ER itself is not an effective\napproach for continual pretraining.\nUnlike ER, distillation approaches utilize richer\ninformation such as output logits or representation\nsimilarity to preserve past knowledge. We Ô¨Ånd\neither Logit KD or SEED-Logit KD to be most\n4769\nSequential ER Adapter Logit\nKD\nRep\nKD\nContrast\nKD\nSEED-Logit\nKD\n81\n82\n83\n84\nRoberta-base\nT ask-Specific\nMTL\n(a) Chemprot\nSequential ER Adapter Logit\nKD\nRep\nKD\nContrast\nKD\nSEED-Logit\nKD\n78\n79\n80\n81\n82\nRoberta-base\nT ask-Specific\nMTL (b) RCT-Sample\nSequential ER Adapter Logit\nKD\nRep\nKD\nContrast\nKD\nSEED-Logit\nKD\n60\n65\n70\n75\nRoberta-base\nT ask-Specific\nMTL\n(c) ACL-ARC\nSequential ER Adapter Logit\nKD\nRep\nKD\nContrast\nKD\nSEED-Logit\nKD\n78\n80\n82\n84\nRoberta-base\nT ask-Specific\nMTL (d) SciERC\nFigure 4: Performance evolution of downstream models.Models are Ô¨Åne-tuned from checkpoints of lifelong pretrained LMs\nat different time steps t. For Chemprot and RCT-Sample from D1, we use t ‚àà {1, 2, 3, 4}; while for ACL-ARC and SciERC\nfrom D2, t ‚àà {2, 3, 4}. Methods achieving the best performance at the end of training (t = 4) is highlighted.\n100 200 500\n#. Downstream Training Instances\n0\n20\n40\n60\n80\n100Macro F1 29.5\n40.1\n63.9\n36.3\n49.6\n65.4\n30.2\n48.0\n66.7\n29.7\n46.4\n64.9\n42.0\n52.9\n70.1\nRoBERT a-base\nSequential-PT\nMTL\nER\nLogit-KD\nFigure 5: Performance of downstream models with vari-\nous number of training examples, exempliÔ¨Åed with SciERC.\nThe models are Ô¨Åne-tuned from the Ô¨Ånal pretrained model (f4).\neffective depending on the task, while Rep-KD\nand Contrastive-KD are less effective. The best\nperforming distillation approach improves F1 over\nSequential Pretraining on downstream tasks from\nD1, D2 at least by 1.0%. However, performance on\nD3,D4, which come later in the data stream, does\nnot improve over Sequential Pretraining, possibly\nbecause the distillation loss term makes the model\nrigid in obtaining new knowledge.\nWhat is the gap between lifelong pretraining\nand multi-task learning across all the domains?\nMulti-Task Learning refers to the ofÔ¨Çine training\nparadigm, which retrain PTLMs over all corpora\n(D1..t) each time a new corpus Dt becomes avail-\nable. We examine whether lifelong pretraining is\ncomparable to multi-task pretraining in terms of\nperformance. From Table 2 and Figure 4, we see\nSequential Pretraining in general underperforms\nMTL except for the Ô¨Ånal domain. However, certain\nCL approaches, such as Logit-Distillation, could\nimprove over MTL on all downstream tasks from\nthe Ô¨Årst and the second domain. We speculate the\nreason is that continual learning naturally provides\na curriculum (Xu et al., 2020; Shi et al., 2015) to\nmodels where each individual task is easier to learn.\nThe results have a positive implication that lifelong\npretraining is not only more computationally efÔ¨Å-\ncient and requires less storage of past data, but may\nalso improve the performance of pretraining.\nDoes lifelong pretraining make models more data\nefÔ¨Åcient? In Table 5, we further examine the per-\nformance of Ô¨Ånal pretrained models under different\namounts of training examples. We include full\nresults in Appendix B. We Ô¨Ånd in general, perfor-\nmance improvements are more signiÔ¨Åcant in the\nlow-resource setup.\nComputational Costs. We quantify computa-\ntional costs of different CL algorithms with the\nnumber of forward and backward passes in Table 4\nand present additional experiments with controlled\ncomputational costs in Appendix F. We Ô¨Ånd ad-\nditional computational cost is necessary for per-\nformance improvement of distillation-based CL.\nHowever, it is not possible to trade performance\nsimply by investing more computation budget with\narbitrary CL algorithms. We leave detailed discus-\nsions in Appendix F.\n4.3 Temporal Data Stream\nWe conduct analysis on pretraining PTLM on\nchronologically-ordered tweet corpora, to under-\nstand whether lifelong pretraining helps adaptation\nto the latest data and improves temporal generaliza-\ntion ability. The results are summarized in Table 5.\n4770\nMethod #. of Forward #. of Backward #. Total #. Total ( k=10) Wall Time4k\nMain results\nSequential PT b b 2b 2b 4.0√ó104sec.\nER (1 + 1/k)b (1 + 1/k)b (2 + 2/k)b 2.2b 4.2√ó104sec.\nLogit-Distill (2 + 2/k)b (1 + 1/k)b (3 + 3/k)b 3.3b 6.9√ó104sec.\nSEED-Logit-Distill (3 + 3/k)b (2 + 2/k)b (5 + 5/k)b 5.5b 9.7√ó104sec.\nAdditional Controlled Experiments\nSequential PTb‚Ä≤=1.2b 1.2b 1.2b 2.4b 2.4b 4.4√ó104sec.\nERk=5 1.2b 1.2b 2.4b 2.4b 4.4√ó104sec.\nSparse Logit-KD 1.3b 1.1b 2.4b 2.4b 4.4√ó104sec.\nSparse SEED-Logit-KD\\contrast 1.3b 1.1b 2.4b 2.4b 4.8√ó104sec.\nTable 4: Number of forward and backward passes over PTLMs and wall clock time of different approaches. The\nnumber of forward and backwards passes are computed over visits of b batches from the training data stream,\nwhere k is the frequency of replay. The wall clock time is calculated over 4 k steps of training (which is the\nnumber of training steps of a single domain in the Research Paper stream) excluding the Ô¨Årst domain, as no\nreplay or distillation happens while learning the Ô¨Årst domain. In the additional controlled experiments (described\nin Appendix. F), we control the total number of forward and backward passes of different approaches.\nYears 2018 (D3) 2020(D4) 2014(D1)‚Üí2020(D4) 2016(D2)‚Üí2020(D4)\nHashtag Prediction\nRoBERTa-base 48.08¬±1.0 56.42¬±0.2 39.31¬±2.7 42.23¬±2.7\nSequential PT 56.79¬±0.5 59.85¬±0.4 44.00¬±1.1 49.87¬±1.8\nER 56.93 ¬±0.1 59.56¬±1.7 43.31¬±0.2 50.72¬±0.6\nLogit-KD 58.21¬±0.5 60.52¬±0.2 44.26¬±0.9 50.92¬±0.8\nContrast-KD 57.94¬±0.4 59.54¬±0.3 45.22¬±0.1 52.14¬±1.1\nSEED-KD 56.87 ¬±0.2 59.71¬±0.2 43.39¬±0.4 49.62¬±1.0\nSEED-Logit-KD 57.75¬±0.4 60.74¬±0.6 45.35¬±0.6 51.56¬±0.7\nTask-SpeciÔ¨Åc (2014) 56.16¬±0.6 59.59¬±0.3 44.34¬±0.6 49.26¬±0.7\nTask-SpeciÔ¨Åc (Latest) 56.61¬±0.4 59.87¬±0.6 43.44¬±0.5 49.41¬±1.1\nMTL 57.89 ¬±0.4 59.95¬±0.3 44.04¬±0.3 50.37¬±0.3\nEmoji Prediction\nRoBERTa-base 25.71¬±0.1 24.42¬±0.2 12.02¬±0.4 13.24¬±0.2\nSequential PT 29.30¬±0.1 27.69¬±0.1 14.20¬±0.2 16.08¬±1.4\nER 29.50 ¬±0.1 27.75¬±0.1 14.36¬±0.4 16.82¬±0.3\nLogit-KD 29.77 ¬±0.1 27.80¬±0.1 14.20¬±0.3 16.28¬±1.1\nContrast-KD 29.48¬±0.2 27.72¬±0.3 14.42¬±0.3 17.52¬±0.1\nSEED-KD 30.12¬±0.1 27.66¬±0.1 14.36¬±0.1 16.97¬±0.4\nSEED-Logit-KD 29.98¬±0.1 27.84¬±0.2 14.36¬±0.1 16.97¬±0.3\nTask-SpeciÔ¨Åc (2014) 28.94¬±0.0 26.98¬±0.2 13.39¬±0.2 15.14¬±0.2\nTask-SpeciÔ¨Åc (Latest) 29.06¬±0.2 27.19¬±0.1 13.00¬±0.2 14.48¬±0.3\nMTL 29.52 ¬±0.2 27.47¬±0.0 14.07¬±0.2 16.64¬±0.2\nTable 5: Results on temporal data stream.We show Ô¨Åne-\ntuning performance over years 2018 and 2020 (D3, D4) and\nthe Temporal generalization from 2014 or 2016 to 2020 data\n(D1 ‚Üí D4, D2 ‚Üí D4) on Twitter Hashtag and Emoji predic-\ntion datasets. Models are Ô¨Åne-tuned from the Ô¨Ånal pre-trained\nmodel fT . We include full results on other years ( D1, D2,\nD3 ‚Üí D4) in Appendix C.\nWill LMs be outdated? We compare the perfor-\nmance of Task-SpeciÔ¨Åc (2014) to the Task-SpeciÔ¨Åc\nmodels pretrained on the year of downstream\ndatasets (noted as Task-SpeciÔ¨Åc (Latest)) and no-\ntice consistent improvements in downstream tasks\nin 2018 and 2020 (Ô¨Årst two columns in Table 5).\nSequential Pretraining could also outperform the\nTask-SpeciÔ¨Åc (2014) model. It veriÔ¨Åes that lan-\nguage models may get outdated over time, but the\nissue can be addressed by task-speciÔ¨Åc or lifelong\npretraining over the latest corpora.\nDoes lifelong pretraining help improve the down-\nstream model‚Äôs performance on latest data? We\nshow that downstream model‚Äôs performance over\nlater data ( D3,D4) can be improved over Task-\nSpeciÔ¨Åc models when continual learning algo-\nrithms are applied. From the Ô¨Årst two columns\nof Table 5, we see Logit-KD and SEED-KD im-\nprove Hashtag prediction score over data of years\n2018 and 2020. SEED-Logit KD further improves\nprediction F1 on Emoji prediction. Note that these\nÔ¨Åndings are in contrast to the research paper stream,\nwhere CL algorithms do not improve performance\nin the latest domain D4. The reason can be the\nhigher similarity between domains in the tweet cor-\npora making the knowledge transfer easier, which\nis further discussed in Appendix I.\nDoes lifelong pretraining improve temporal gen-\neralization? Temporal generalization evaluates\ndownstream performance over latest test data when\nÔ¨Åne-tuned over outdated training data. We show\nlifelong pretraining brings clear improvement to\ntemporal generalization. From Table 5, we see\neven Sequential Pretraining could improve over\nthe model pretrained merely on the year 2020 data\n(Task-SpeciÔ¨Åc (2020)) consistently. We Ô¨Ånd per-\nformance further improves with CL algorithms ap-\nplied. SEED-Logit-KD performs best in general\non crossyear hashtag prediction tasks. In crossyear\nemoji prediction, we Ô¨Ånd Contrast-KD and SEED-\nKD perform best. We also Ô¨Ånd that SEED-Logit-\nKD could slightly outperform Logit-KD.\n5 Related Works\nDomain and Temporal Adaptation of Language\nModels. Gururangan et al. (2020) study adapta-\n4771\ntion of PTLMs to domain-speciÔ¨Åc corpora. Aru-\nmae et al. (2020) study algorithms to mitigate for-\ngetting in original PTLMs, but does not investigate\nforgetting that happens over a sequence of domains.\nMaronikolakis and Sch√ºtze (2021); R√∂ttger and\nPierrehumbert (2021); Luu et al. (2021) proposes\nsequential pretraining over domains or emerging\ndata, but did not investigate CL algorithms. Sev-\neral recent studies have demonstrated the neces-\nsity of adapting LMs over time (Lazaridou et al.,\n2021) while speciÔ¨Åcally focusing on factual knowl-\nedge (Dhingra et al., 2021; Jang et al., 2021).\nContinual Learning Algorithms in NLP.Con-\ntinual learning in NLP has mainly been studied for\nclassiÔ¨Åcation tasks. An effective approach is to\nutilize a number of stored past examples (de Mas-\nson d‚ÄôAutume et al., 2019; Wang et al., 2020), or\npseudo examples (e.g., the ones generated with a\nPTLM (Sun et al., 2020; Kanwatchara et al., 2021)).\nRecent extensions of the algorithm (Chuang et al.,\n2020) perform knowledge distillation with gener-\nated pseudo examples. Other lines of works fo-\ncus on regularization over the sentence representa-\ntions (Wang et al., 2019; Huang et al., 2021; Liu\net al., 2019a) or directly merging models in the\nparameter space (Matena and Raffel, 2021). Model\nexpansion-based approaches (Liu et al., 2019a;\nPfeiffer et al., 2021), including learning domain\nspeciÔ¨Åc expert models (Gururangan et al., 2021),\nare also actively studied. Wu et al. (2022) present a\ncomparative study of algorithms in the context of\ncontinual Ô¨Åne-tuning over NLP tasks.\n6 Conclusion\nIn this paper, we formulated the lifelong language\nmodel pretraining problem and constructed two\ndata streams associated with downstream datasets.\nWe evaluated knowledge retention, adaptation to\nthe latest data, and temporal generalization ability\nof continually pretrained language models. Our\nexperiments show distillation-based approaches\nbeing most effective in these evaluation setups.\nA limitation of the work is that it has not been\nfully addressed whether there exists a variant of\ndistillation-based CL approach that consistently\noutperforms Logit-KD. Based on the current obser-\nvation, we conclude the performance of different\nKD approaches for CL is highly task-dependent. It\nasks for more future works into continual learning\nalgorithms within the proposed problem setup.\nReferences\nKristjan Arumae, Qing Sun, and Parminder Bhatia.\n2020. An empirical investigation towards efÔ¨Å-\ncient multi-domain language model pre-training. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4854‚Äì4864, Online. Association for Computa-\ntional Linguistics.\nIsabelle Augenstein, Mrinal Das, Sebastian Riedel,\nLakshmi Vikraman, and Andrew McCallum. 2017.\nSemEval 2017 task 10: ScienceIE - extracting\nkeyphrases and relations from scientiÔ¨Åc publica-\ntions. In Proceedings of the 11th International\nWorkshop on Semantic Evaluation (SemEval-2017) ,\npages 546‚Äì555, Vancouver, Canada. Association for\nComputational Linguistics.\nAbdul Hameed Azeemi and Adeel Waheed. 2021.\nCovid-19 tweets analysis through transformer lan-\nguage models. ArXiv, abs/2103.00199.\nFrancesco Barbieri, Jose Camacho-Collados,\nFrancesco Ronzano, Luis Espinosa-Anke, Miguel\nBallesteros, Valerio Basile, Viviana Patti, and\nHoracio Saggion. 2018. SemEval 2018 task 2:\nMultilingual emoji prediction. In Proceedings\nof The 12th International Workshop on Semantic\nEvaluation, pages 24‚Äì33, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientiÔ¨Åc text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3615‚Äì\n3620, Hong Kong, China. Association for Computa-\ntional Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Informa-\ntion Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual.\nHyuntak Cha, Jaeho Lee, and Jinwoo Shin. 2021.\nCo2l: Contrastive continual learning. ArXiv,\nabs/2106.14413.\nArslan Chaudhry, Marcus Rohrbach, Mohamed Elho-\nseiny, Thalaiyasingam Ajanthan, Puneet K Dokania,\nPhilip HS Torr, and Marc‚ÄôAurelio Ranzato. 2019.\nOn tiny episodic memories in continual learning.\narXiv preprint arXiv:1902.10486.\n4772\nYung-Sung Chuang, Shang-Yu Su, and Yun-Nung\nChen. 2020. Lifelong language knowledge distil-\nlation. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Process-\ning (EMNLP) , pages 2914‚Äì2924, Online. Associa-\ntion for Computational Linguistics.\nCyprien de Masson d‚ÄôAutume, Sebastian Ruder, Ling-\npeng Kong, and Dani Yogatama. 2019. Episodic\nmemory in lifelong language learning. In Advances\nin Neural Information Processing Systems 32: An-\nnual Conference on Neural Information Processing\nSystems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada, pages 13122‚Äì13131.\nFranck Dernoncourt and Ji Young Lee. 2017. PubMed\n200k RCT: a dataset for sequential sentence clas-\nsiÔ¨Åcation in medical abstracts. In Proceedings of\nthe Eighth International Joint Conference on Natu-\nral Language Processing (Volume 2: Short Papers),\npages 308‚Äì313, Taipei, Taiwan. Asian Federation of\nNatural Language Processing.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171‚Äì4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nBhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisen-\nschlos, D. Gillick, Jacob Eisenstein, and William W.\nCohen. 2021. Time-aware language models as tem-\nporal knowledge bases. ArXiv, abs/2106.15110.\nZhiyuan Fang, Jianfeng Wang, Lijuan Wang, L. Zhang,\nYezhou Yang, and Zicheng Liu. 2021. Seed:\nSelf-supervised distillation for visual representation.\nArXiv, abs/2101.04731.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimcse: Simple contrastive learning of sentence em-\nbeddings. ArXiv, abs/2104.08821.\nYuyun Gong and Qi Zhang. 2016. Hashtag recom-\nmendation using attention-based convolutional neu-\nral network. In Proceedings of the Twenty-Fifth\nInternational Joint Conference on ArtiÔ¨Åcial Intelli-\ngence, IJCAI 2016, New York, NY, USA, 9-15 July\n2016, pages 2782‚Äì2788. IJCAI/AAAI Press.\nSuchin Gururangan, Michael Lewis, Ari Holtzman,\nNoah A. Smith, and Luke Zettlemoyer. 2021. Demix\nlayers: Disentangling domains for modular lan-\nguage modeling. ArXiv, abs/2108.05036.\nSuchin Gururangan, Ana Marasovi ¬¥c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don‚Äôt stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342‚Äì8360, Online. Association for Computational\nLinguistics.\nGeoffrey E. Hinton, Oriol Vinyals, and J. Dean. 2015.\nDistilling the knowledge in a neural network. ArXiv,\nabs/1503.02531.\nSpurthi Amba Hombaiah, Tao Chen, Mingyang Zhang,\nMichael Bendersky, and Marc-Alexander Najork.\n2021. Dynamic language models for continuously\nevolving content. Proceedings of the 27th ACM\nSIGKDD Conference on Knowledge Discovery &\nData Mining.\nSaihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang,\nand Dahua Lin. 2018. Lifelong learning via progres-\nsive distillation and retrospection. In ECCV.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efÔ¨Åcient transfer learning for NLP.\nIn Proceedings of the 36th International Confer-\nence on Machine Learning, ICML 2019, 9-15 June\n2019, Long Beach, California, USA , volume 97 of\nProceedings of Machine Learning Research , pages\n2790‚Äì2799. PMLR.\nXiaolei Huang and Michael J. Paul. 2018. Examining\ntemporality in document classiÔ¨Åcation. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 694‚Äì699, Melbourne, Australia. Asso-\nciation for Computational Linguistics.\nYufan Huang, Yanzhe Zhang, Jiaao Chen, Xuezhi\nWang, and Diyi Yang. 2021. Continual learning for\ntext classiÔ¨Åcation with information disentanglement\nbased regularization. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2736‚Äì2746, Online.\nAssociation for Computational Linguistics.\nJoel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin,\nJanghoon Han, Gyeonghun Kim, Stanley Jungkyu\nChoi, and Minjoon Seo. 2021. Towards contin-\nual knowledge learning of language models. arXiv\npreprint arXiv:2110.03215.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2020. TinyBERT: Distilling BERT for natural lan-\nguage understanding. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n4163‚Äì4174, Online. Association for Computational\nLinguistics.\nDavid Jurgens, Srijan Kumar, Raine Hoover, Dan Mc-\nFarland, and Dan Jurafsky. 2018. Measuring the evo-\nlution of a scientiÔ¨Åc Ô¨Åeld through citation frames.\nTransactions of the Association for Computational\nLinguistics, 6:391‚Äì406.\n4773\nKasidis Kanwatchara, Thanapapas Horsuwan, Piyawat\nLertvittayakumjorn, Boonserm Kijsirikul, and Peer-\napon Vateekul. 2021. Rational LAMOL: A\nrationale-based lifelong learning framework. In Pro-\nceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics and the 11th In-\nternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers) , pages 2942‚Äì\n2953, Online. Association for Computational Lin-\nguistics.\nAngeliki Lazaridou, A. Kuncoro, E. Gribovskaya, De-\nvang Agrawal, Adam Liska, Tayfun Terzi, Mai\nGimenez, Cyprien de Masson d‚ÄôAutume, Sebastian\nRuder, Dani Yogatama, Kris Cao, Tom√°s Kocisk√Ω,\nSusannah Young, and P. Blunsom. 2021. Assessing\ntemporal generalization in neural language models.\nNeurIPS.\nZhizhong Li and Derek Hoiem. 2018. Learning with-\nout forgetting. IEEE Transactions on Pattern Analy-\nsis and Machine Intelligence, 40:2935‚Äì2947.\nTianlin Liu, Lyle Ungar, and Jo√£o Sedoc. 2019a. Con-\ntinual learning for sentence representations using\nconceptors. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 3274‚Äì3279, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv, abs/1907.11692.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-\nney, and Daniel Weld. 2020. S2ORC: The semantic\nscholar open research corpus. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4969‚Äì4983, Online. As-\nsociation for Computational Linguistics.\nYi Luan, Luheng He, Mari Ostendorf, and Hannaneh\nHajishirzi. 2018. Multi-task identiÔ¨Åcation of enti-\nties, relations, and coreference for scientiÔ¨Åc knowl-\nedge graph construction. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 3219‚Äì3232, Brussels, Bel-\ngium. Association for Computational Linguistics.\nKelvin Luu, Daniel Khashabi, Suchin Gururangan, Kar-\nishma Mandyam, and Noah A. Smith. 2021. Time\nwaits for no one! analysis and challenges of tempo-\nral misalignment.\nAntonis Maronikolakis and Hinrich Sch√ºtze. 2021.\nMultidomain pretrained language models for green\nNLP. In Proceedings of the Second Workshop\non Domain Adaptation for NLP , pages 1‚Äì8, Kyiv,\nUkraine. Association for Computational Linguistics.\nMichael Matena and Colin Raffel. 2021. Merging\nmodels with Ô¨Åsher-weighted averaging. ArXiv,\nabs/2111.09832.\nMartin M√ºller, Marcel Salath√©, and Per Egil Kummer-\nvold. 2020. Covid-twitter-bert: A natural language\nprocessing model to analyse covid-19 content on\ntwitter. ArXiv, abs/2005.07503.\nSheshera Mysore, Zachary Jensen, Edward Kim, Kevin\nHuang, Haw-Shiuan Chang, Emma Strubell, Jef-\nfrey Flanigan, Andrew McCallum, and Elsa Olivetti.\n2019. The materials science procedural text corpus:\nAnnotating materials synthesis procedures with shal-\nlow semantic structures. In Proceedings of the 13th\nLinguistic Annotation Workshop, pages 56‚Äì64, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nDat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen.\n2020. BERTweet: A pre-trained language model\nfor English tweets. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing: System Demonstrations, pages 9‚Äì\n14, Online. Association for Computational Linguis-\ntics.\nElsa A Olivetti, Jacqueline M Cole, Edward Kim, Olga\nKononova, Gerbrand Ceder, Thomas Yong-Jin Han,\nand Anna M Hiszpanski. 2020. Data-driven mate-\nrials research enabled by natural language process-\ning and information extraction. Applied Physics Re-\nviews, 7(4):041317.\nJonas Pfeiffer, Aishwarya Kamath, Andreas R√ºckl√©,\nKyunghyun Cho, and Iryna Gurevych. 2021.\nAdapterFusion: Non-destructive task composition\nfor transfer learning. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume ,\npages 487‚Äì503, Online. Association for Computa-\ntional Linguistics.\nSylvestre-Alvise RebufÔ¨Å, Alexander Kolesnikov,\nGeorg Sperl, and Christoph H. Lampert. 2017. icarl:\nIncremental classiÔ¨Åer and representation learning.\nIn 2017 IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2017, Honolulu, HI,\nUSA, July 21-26, 2017 , pages 5533‚Äì5542. IEEE\nComputer Society.\nShruti Rijhwani and Daniel Preotiuc-Pietro. 2020.\nTemporally-informed analysis of named entity\nrecognition. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 7605‚Äì7617, Online. Association for\nComputational Linguistics.\nAnthony V . Robins. 1995. Catastrophic forgetting, re-\nhearsal and pseudorehearsal. Connect. Sci., 7:123‚Äì\n146.\nPaul R√∂ttger and J. Pierrehumbert. 2021. Temporal\nadaptation of bert and performance on downstream\ndocument classiÔ¨Åcation: Insights from social media.\nFindings of EMNLP.\n4774\nJonathan Schwarz, Wojciech Czarnecki, Je-\nlena Luketina, Agnieszka Grabska-Barwinska,\nYee Whye Teh, Razvan Pascanu, and Raia Hadsell.\n2018. Progress & compress: A scalable framework\nfor continual learning. In Proceedings of the 35th\nInternational Conference on Machine Learning,\nICML 2018, Stockholmsm√§ssan, Stockholm, Swe-\nden, July 10-15, 2018 , volume 80 of Proceedings\nof Machine Learning Research , pages 4535‚Äì4544.\nPMLR.\nYangyang Shi, Martha Larson, and Catholijn M. Jonker.\n2015. Recurrent neural network language model\nadaptation with curriculum learning. Comput.\nSpeech Lang., 33:136‚Äì154.\nFan-Keng Sun, Cheng-Hao Ho, and Hung-Yi Lee.\n2020. LAMOL: language modeling for lifelong lan-\nguage learning. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for BERT model com-\npression. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4323‚Äì4332, Hong Kong, China. Association for\nComputational Linguistics.\nJens Vindahl. 2016. Chemprot-3.0: a global chemical\nbiology diseases mapping.\nHong Wang, Wenhan Xiong, Mo Yu, Xiaoxiao Guo,\nShiyu Chang, and William Yang Wang. 2019. Sen-\ntence embedding alignment for lifelong relation ex-\ntraction. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n796‚Äì806, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nZirui Wang, Sanket Vaibhav Mehta, Barnabas Poczos,\nand Jaime Carbonell. 2020. EfÔ¨Åcient meta lifelong-\nlearning with limited memory. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pages 535‚Äì548,\nOnline. Association for Computational Linguistics.\nTongtong Wu, Massimo Caccia, Zhuang Li, Yuan-Fang\nLi, Guilin Qi, and Gholamreza Haffari. 2022. Pre-\ntrained language model in continual learning: A\ncomparative study. In International Conference on\nLearning Representations.\nBenfeng Xu, Licheng Zhang, Zhendong Mao, Quan\nWang, Hongtao Xie, and Yongdong Zhang. 2020.\nCurriculum learning for natural language under-\nstanding. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6095‚Äì6104, Online. Association for Computa-\ntional Linguistics.\nYunzhi Yao, Shaohan Huang, Wenhui Wang, Li Dong,\nand Furu Wei. 2021. Adapt-and-distill: Developing\nsmall, fast and effective pretrained language mod-\nels for domains. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021 ,\npages 460‚Äì470, Online. Association for Computa-\ntional Linguistics.\n4775\n100 200 500\n#. Downstream Training Instances\n0\n20\n40\n60\n80\n100Micro F1\n47.1\n55.0\n70.7\n49.8\n58.5\n70.4\n51.1\n58.8\n71.5\n53.5 59.4\n69.3\n53.4\n63.6\n74.3\nRoBERT a-base\nSequential-PT\nMTL\nER\nLogit-KD\n(a) Chemprot\n100 200 500\n#. Downstream Training Instances\n0\n20\n40\n60\n80\n100Macro F1 29.5\n40.1\n63.9\n36.3\n49.6\n65.4\n30.2\n48.0\n66.7\n29.7\n46.4\n64.9\n42.0\n52.9\n70.1\nRoBERT a-base\nSequential-PT\nMTL\nER\nLogit-KD\n(b) SciERC\nFigure 6: Performance of downstream models with various\nnumber of training examples. The models are Ô¨Åne-tuned from\nthe Ô¨Ånal pretrained model (f4).\nA Detailed Experiment Settings\nWe use a linearly decreasing learning rate initial-\nized with 5e-4 on the research paper stream and\n3e-4 on the tweet stream. On the research paper\nstream, we train the model for 8,000 steps in the\nÔ¨Årst task, and 4,000 steps in the subsequent tasks.\nOn the tweet stream, we train the model for 8,000\nsteps in all tasks. We hold out 128,000 sentences\nfrom each corpus to evaluate MLM performance.\nAs the size of pretraining corpora is large, during\ntraining, each training example is visited only once.\nWe use the masked language modeling perplex-\nity over held-out validation sets of the pretraining\ncorpora as the metrics for hyperparameter tuning.\nCommon hyperparameters such as learning rate\nand batch sizes are tuned with Task-speciÔ¨Åc models\nwith the Ô¨Årst task. Hyperparameters that are spe-\nciÔ¨Åc to continual learning algorithms, such as the\nscale of the distillation loss, is tuned using the Ô¨Årst\ntwo domains in the stream according to the MLM\nperformance over validation sets. The weight of\nthe distillation term Œ± is set as 1.0 for logit dis-\ntillation and 0.1 for other distillation algorithms.\nBy default, we replay or perform distillation with\na mini-batch of examples from the replay mem-\nory every 10 training steps in ER and Distillation-\nbased CL approaches. We use the huggingface\ntransformers library https://github.com/\nhuggingface/transformers for implemen-\ntation.\nB Low-Resource Fine-Tuning\nFigure 6 summarizes the performance of Ô¨Åne-tuned\nmodels from the Ô¨Ånal model checkpoint ( t = 4)\nTask 2014 2016 2018 2020\nHashtag Prediction\nRoBERTa-base 56.65¬±0.6 45.50¬±2.1 48.08¬±1.0 56.42¬±0.2\nSequential PT 59.00¬±0.1 54.28¬±0.3 56.79¬±0.5 59.85¬±0.4\nER 59.00 ¬±0.1 54.90¬±0.2 56.93¬±0.1 59.56¬±1.7\nAdapter 58.76 ¬±0.7 52.55¬±1.5 54.34¬±1.7 59.01¬±1.0\nLogit-KD 60.93 ¬±0.5 55.96¬±0.2 58.21¬±0.5 60.52¬±0.2\nRep-KD 60.47 ¬±0.1 51.77¬±2.6 55.79¬±1.4 59.80¬±0.2\nContrast-KD 60.72 ¬±0.6 55.85¬±0.0 57.94¬±0.4 59.54¬±0.3\nSEED-KD 58.82 ¬±0.4 54.55¬±0.5 56.87¬±0.2 59.71¬±0.2\nSEED-Logit-KD 61.28¬±0.2 55.59¬±0.5 57.75¬±0.4 60.74¬±0.6\nTask-SpeciÔ¨Åc (2014) 61.62¬±0.3 55.38¬±0.6 56.16¬±0.6 59.59¬±0.3\nTask-SpeciÔ¨Åc (Latest) 59.91¬±0.3 55.47¬±1.0 56.61¬±0.4 59.87¬±0.6\nMTL 60.51 ¬±0.3 55.16¬±1.6 57.89¬±0.4 59.95¬±0.3\nEmoji Prediction\nRoBERTa-base 28.73¬±0.2 26.86¬±0.2 25.71¬±0.1 24.42¬±0.2\nSequential PT 32.69¬±0.2 30.55¬±0.3 29.30¬±0.1 27.69¬±0.1\nER 32.88 ¬±0.2 30.52¬±0.2 29.50¬±0.1 27.75¬±0.1\nAdapter 32.15 ¬±0.2 29.85¬±0.0 28.72¬±0.0 26.80¬±0.3\nLogit-KD 33.08 ¬±0.3 30.88¬±0.1 29.77¬±0.1 27.80¬±0.1\nRep-KD 32.71 ¬±0.2 30.51¬±0.2 29.45¬±0.1 27.27¬±0.2\nContrast-KD 32.90 ¬±0.1 31.01¬±0.1 29.48¬±0.2 27.72¬±0.3\nSEED-KD 32.91 ¬±0.1 30.84¬±0.3 30.12¬±0.1 27.66¬±0.1\nSEED-Logit-KD 33.28¬±0.1 31.17¬±0.1 29.98¬±0.1 27.84¬±0.2\nTask-SpeciÔ¨Åc (2014) 33.37¬±0.2 30.54¬±0.3 28.94¬±0.0 26.98¬±0.2\nTask-SpeciÔ¨Åc (Latest) 32.31¬±0.0 29.83¬±0.5 29.06¬±0.2 27.19¬±0.1\nMTL 32.78 ¬±0.1 30.54¬±0.0 29.52¬±0.2 27.47¬±0.0\nTable 6: Full performance on Twitter Hashtag prediction and\nEmoji prediction, Ô¨Åne-tuned from the pre-trained model in the\nÔ¨Ånal time step.\nusing different amount of downstream training ex-\namples. We see on Chemprot and SciERC, the ben-\neÔ¨Åt of Sequential Pretraining over RoBERTa-base\nis more signiÔ¨Åcant in low-resource Ô¨Åne-tuning se-\ntups. Whenever Seqential Pretraining outperforms\nRoBERTa-base, we notice Logit-KD could further\nimprove over Sequential Pretraining.\nC Full Results over the Tweet Stream\nTables 6 and 7 summarize full results over the\nTweet stream. Compared to the table 5 in the main\ntext, we add downstream performance over data\nfrom years 2014 and 2016 (D1, D2), and temporal\ngeneralization from year 2014 to 2020 (D1 ‚ÜíD4).\nD Dataset Details\nThe research paper stream consists of full text\nof 6.6M, 12.1M, 7.8M, and 7.5M research pa-\npers from the S2ORC (Lo et al., 2020) dataset.\nWe evaluate downstream Ô¨Åne-tuning performance\non two in-domain datasets for each research area:\nChemprot relation exaction dataset (Vindahl, 2016)\nand RCT abstract sentence role labeling dataset\n(Dernoncourt and Lee, 2017) for the bio-medical\ndomain; ACL-ARC citation intent classiÔ¨Åcation\ndataset (Jurgens et al., 2018) and SciERC rela-\ntion extraction dataset (Luan et al., 2018) for the\n4776\nTask 2014 ‚Üí2020 2016‚Üí2020 2018‚Üí2020\nCrossyear Hashtag Prediction\nRoBERTa-base 39.31¬±2.7 42.23¬±2.7 37.19¬±2.1\nSequential PT 44.00¬±1.1 49.87¬±1.8 46.63¬±0.9\nER 43.31 ¬±0.2 50.72¬±0.6 46.27¬±0.4\nAdapter 42.61 ¬±0.5 48.00¬±1.6 42.63¬±0.9\nLogit-KD 44.26 ¬±0.9 50.92¬±0.8 46.84¬±1.0\nRep-KD 42.48 ¬±0.2 50.38¬±1.5 42.23¬±0.2\nContrast-KD 45.22 ¬±0.1 52.14¬±1.1 47.47¬±0.8\nSEED-KD 43.39 ¬±0.4 49.62¬±1.0 46.37¬±0.8\nSEED-Logit-KD 45.35¬±0.6 51.56¬±0.7 47.74¬±0.3\nTask-SpeciÔ¨Åc (2014) 44.34¬±0.6 49.26¬±0.7 45.09¬±0.7\nTask-SpeciÔ¨Åc (2020) 43.44¬±0.5 49.41¬±1.1 44.34¬±0.4\n-4x steps 44.34¬±0.6 51.78¬±0.7 44.69¬±0.7\nMTL 44.04 ¬±0.3 50.37¬±0.3 44.31¬±0.0\nCrossyear Emoji Prediction\nRoBERTa-base 12.02¬±0.4 13.24¬±0.2 18.67¬±0.1\nSequential PT 14.20¬±0.2 16.08¬±1.4 21.06¬±0.9\nER 14.36 ¬±0.4 16.82¬±0.3 21.57¬±0.1\nAdapter 13.53 ¬±0.2 15.68¬±0.3 20.64¬±0.1\nLogit-KD 14.20 ¬±0.3 16.28¬±1.1 21.29¬±1.0\nRep-KD 13.89 ¬±0.1 16.03¬±0.3 20.86¬±0.2\nContrast-KD 14.42 ¬±0.3 17.52¬±0.1 21.43¬±0.1\nSEED-KD 14.36 ¬±0.1 16.97¬±0.4 21.88¬±0.3\nSEED-Logit-KD 14.36¬±0.1 16.97¬±0.3 21.62¬±0.1\nTask-SpeciÔ¨Åc (2014) 13.39¬±0.2 15.14¬±0.2 20.79¬±0.3\nTask-SpeciÔ¨Åc (2020) 13.00¬±0.2 14.48¬±0.3 19.30¬±0.2\n-4x steps 12.90¬±0.4 14.85¬±0.3 19.83¬±0.2\nMTL 14.07 ¬±0.2 16.64¬±0.2 20.94¬±0.7\nTable 7: Temporal generalization performance on Twitter\nHashtag prediction datasets Ô¨Åne-tuned from the Ô¨Ånal pre-\ntrained model. Year 1‚ÜíYear 2 indicates the hashtag pre-\ndiction model is Ô¨Åne-tuned on data in year Year 1, and\nevaluated on test data in Year 2.\ncomputer science domain; relation extraction over\nSynthesis procedures (Mysore et al., 2019) and\nnamed entity recognition over material science\npapers (MNER) (Olivetti et al., 2020) for mate-\nrial science domain; keyphrase classiÔ¨Åcation and\nhyponym classiÔ¨Åcation after Ô¨Åltering out physics\npapers for the physics domain (Augenstein et al.,\n2017). We report micro-averaged F1 on Chemprot,\nRCT, MNER datasets following the evaluation\nmetrics in the original work, and report macro-\naveraged F1 on all other datasets. We use the of-\nÔ¨Åcial data splits for all datasets except for RCT,\nwhere we employ a low-resource training setup\nfollowing Gururangan et al. (2020).\nThe pretraining corpora for the tweet stream con-\nsist of 25M tweets in each year. For downstream\ntasks, we use a separate set of 1M tweets from\neach year to construct multi-label hashtag predic-\ntion (Gong and Zhang, 2016) datasets and single-\nlabel emoji prediction datasets (Barbieri et al.,\n2018). We replace user names to special tokens.\nFor Hashtag prediction, the label space consists of\ntweets containing 200 most frequent hashtags in\neach year. We independently sample 500 tweets\nper label (hashtag) as training, validation and test\nsets, which results 10 k examples in each of the\ndata splits. For emoji prediction, we construct 20-\nway single-label emoji prediction datasets for each\nyear following Barbieri et al. (2018) with the 1M\nheld out tweets. We sample 5,000 tweets per emoji\nin each split, resulting in balanced datasets of the\nsame size as the hashtag prediction datasets.\nE Details of Continual Learning\nAlgorithms\nE.1 Contrastive Distillation\nDuring continual pretraining, in addition to the\nlanguage model pretraining objective, we add a un-\nsupervised contrastive learning objective, namely\nthe SimCSE (Gao et al., 2021) objective, so that\nthe similarity in the sentence representation better\nreÔ¨Çects the semantic similarity in the sentence. We\nuse the l2-normalized representation of the start-\nof-sequence token at the Ô¨Ånal layer as the sentence\nrepresentation, noted as h. Then, we distill the\nintra-batch representational similarity from the pre-\nvious model ft‚àí1 to the current model ft. Given a\nmini-batch of N examples x, we compute the rep-\nresentational dot-product similarity matrix between\nnormalized sentence representations h between\neach pair of examples with ft‚àí1 and ft, noted as\nBt‚àí1 and Bt, where each element Bij is,\nBij = exp(hi ¬∑hj/œÑ)‚àë\nk=1..N exp(hi ¬∑hk/œÑ) (1)\nwhere œÑ is a temperature hyperparameter. We spec-\nify a temperature œÑt = 0.05 for the teacher model\nft‚àí1 and a temperature œÑs for the student model\nft = 0.01. We compute the cross-entropy between\nBt‚àí1 and Bt as the distillation loss,\n‚Ñìdistill = ‚àí1\nN\nN‚àë\ni=1\nN‚àë\nj=1\nBt‚àí1\nij log Bt\nij (2)\nE.2 SEED Distillation\nSEED distillation proposed by (Fang et al., 2021)\nhas a similar spirit as the contrastive distillation\nwith differences in the examples used for com-\nputing similarity matrices computes. The algo-\nrithm distills representational similarity between\nthe batch and a large set of other examples, main-\ntained in an example queue Q. As the number\nof target examples K can be much larger than\nthe batch size, it allows distillation of richer in-\nformation by regularizing similarities. During pre-\ntraining, the method maintains a Ô¨Åxed-size queue\n4777\nQ to cache examples from the current domain\nDt. Given a mini-batch of training examples x,\nit computes cosine similarity between each pair\nof examples within the batch x and Qwith ft‚àí1\nand ft, resulting in two similarity matrices Bt‚àí1,\nPy ‚ààR|B|√ó|Q|. Similar to the contrastive distil-\nlation, the distillation loss is the cross-entropy be-\ntween two similarity matrices Bt‚àí1 and Bt com-\nputed in the same way as Eq. 2.\nF Analysis and Controlled Experiments\nof Computational Costs\nComputational cost is a crucial matter for online\ncontinual learning systems. In this section, we ana-\nlyze the computational costs of continual learning\nalgorithms and perform controlled experiments of\ncomputational costs.\nWe quantify computational costs with the total\nnumber of forward (Cf ) and backward (Cb) com-\nputations (C = Cf +Cb) over the PTLMs, which is\neasy to control; in practice, we Ô¨Ånd the wall clock\ntime of training was approximately linear to C. We\nsummarize the number of forward and backward\npasses and the wall clock time of training in Table 4.\nIn the visit of bbatches from the training stream,\nSequential PT performs bforward and backward\npasses respectively over the PTLM, resulting in\nC = 2b. Experience replay further replays 1 batch\nof examples every ksteps over the training stream,\nwhich results in C = (2 + 2/k)b. In our main\nexperiments, ris set to 10 (Sec. 3.3). Logit-Distill\nand Rep-Distill require one additional forward pass\nover a frozen PTLM to compute the target of dis-\ntillation, resulting in C = (3 + 3/k)b. Distilla-\ntion algorithms that perform contrastive learning\nwith SimCSE (i.e. SEED-Distill and SEED-Logit-\nDistill) additionally require one forward and back-\nward pass using the same batch of examples with\ndifferent dropout masks. Therefore, for SEED-\nLogit-Distill, C = (5 + 5/k)b.\nTo control the number of forward and backward\npasses, we present approaches to compensate the\nlower computation costs compared to Distillation\nalgorithms and one approach to shrink the com-\nputational cost of distillation algorithms: (1) for\nSequential PT, we train the models for 1.2 times\nmore steps so that C = 2.4b, noted as Sequential\nPTb‚Ä≤=1.2b; (2) for ER, we increase the replay fre-\nquency k to 5 from the default setup 10, so that\nC = 2.4b. We also decrease the cost of Logit-KD\nand SEED-Logit-KD by reducing the frequency\nof distillation from every 1 batch to every r‚Ä≤=10\nsteps, while still replaying and distilling knowledge\nover 1 batch of memory examples every 10 train-\ning steps. This results in Cf = (1 + 2/k+ 1/k‚Ä≤)b\nand Cb = (1 + 1/k)b, where C = 2.4b when\nboth rand r‚Ä≤are 10. The approach is referred to\nas Sparse Logit-KD. Finally, for SEED-Logit-KD,\nwe remove the SimCSE loss from training and per-\nform sparse distillation similar to Sparse-Logit-KD,\nwhich also results in C = 2.4b.\nThe performance of the models is presented in\nTable 9. We notice that at the end of pretraining, in-\ncreasing the number of training steps in Sequential\nPT by 1.2 times does not lead to performance boost\non the latest domain (D4), while the performance\nover tasks from earlier domains (Chemprot, ACL-\nARC, SciERC) slightly dropped, possibly due to\nincreased forgetting. For ER, we notice replay-\ning only slightly more frequently (ER k=5) than\nthe default setup (k=10) greatly increased the per-\nplexity of MLM, implying signiÔ¨Åcantly increased\noverÔ¨Åtting to the memory; while the performance\ndifferences of downstream tasks compared to the\ndefault ER is mixed. When we decrease the replay\nfrequency of distillation, the performance on Logit-\nKD and SEED-KD also decreased and does not\noutperform ER.\nThe results show additional computation costs\ncan be necessary for continual learning algorithms\nsuch as Logit-KD and SEED-Logit-KD. However,\nthe results also show that there is no simple trade-\noff between computational cost and performance.\nWe have seen that it is not always beneÔ¨Åcial to in-\ncrease the number of training steps over the emerg-\ning data, as it increases forgetting in earlier do-\nmains. Similarly, increasing the frequency of re-\nplay may lead to signiÔ¨Åcant overÔ¨Åtting to the re-\nplay memory. Investigating into more effective\ncontinual learning algorithms, despite increased\ncomputation costs, allows us to obtain performance\nimprovement that cannot be simply traded with\nmore computation with arbitrary continual learning\nalgorithms. We leave more thorough studies into\nthis topic as future work.\nG Experiments with RoBERTa-large\nWe present additional experiments on RoBERTa-\nlarge. Figure 7 and Table 8 summarizes the results\nof selected continual learning algorithms and base-\nlines. On Chemprot, RCT-Sample and SciERC,\nLogit-KD achieves best performance with the last\n4778\nSequential ER Logit\nKD\n83\n84\n85\n86\n87\nRoBERT a-large\nDomain-Specific\n(a) Chemprot\nSequential ER Logit\nKD\n78\n79\n80\n81\n82\nRoBERT a-large\nDomain-Specific (b) RCT-Sample\nSequential ER Logit\nKD\n60\n65\n70\n75\nRoBERT a-large\nDomain-Specific\n(c) ACL-ARC\nSequential ER Logit\nKD\n80\n82\n84\n86\nRoBERT a-large\nDomain-Specific (d) SciERC\nFigure 7: Performance evolution of downstream models with RoBERTa-large as the base model. Models are Ô¨Åne-tuned from\ncheckpoints of lifelong pretrained LMs at different time stepst. For Chemprot and RCT-Sample fromD1, we use t ‚àà {1, 2, 3, 4};\nwhile for ACL-ARC and SciERC from D2, t ‚àà {2, 3, 4}. Methods achieving the best performance at the end of training (t = 4)\nis highlighted.\nDomain Biomedical Computer Science\nDataset Chemprot RCT-Sample ACL-ARC SciERC\nRoBERTa-large 84.39¬±0.7 80.76¬±0.7 72.20¬±3.2 83.02¬±0.8\nNaive 85.43 ¬±0.6 81.10¬±0.5 73.44¬±2.0 82.88¬±0.7\nER 85.42 ¬±0.2 81.30¬±0.4 71.51¬±2.5 83.22¬±0.5\nLogit-KD 86.18¬±0.7 81.93¬±0.7 72.10¬±2.0 83.23¬±0.6\nTask-SpeciÔ¨Åc 85.99¬±0.3 82.02¬±0.6 76.07¬±1.0 82.91¬±0.9\nTable 8: Results on the Research Paper stream with\nRoBERTa-large as the base model.\npretraining checkpoint. On ACL-ARC, however,\nwe notice that Sequential PT achieves the best per-\nformance, while all continual learning algorithms\n(ER, Logit-KD) achieves lower F1 at the initial\ntime step (t=2, Figure 7(c)). This implies continual\nlearning algorithms may hurt model‚Äôs performance\nin capturing new knowledge, despite its potential\nto reduce forgetting.\nH Experiments with BERT on Tweet\nStream After 2019\nIn this section, we present an additional set of exper-\niments on BERT-base (Devlin et al., 2019) model,\nwhich is originally pretrained with Wikipedia arti-\ncles before 2019, with Tweets only after 2019. The\ntraining corpora D1..4 consist of tweets from the\nÔ¨Årst half of 2019, the second half of 2019, the Ô¨Årst\nhalf of 2020, and the second half of 2020 respec-\ntively. We accordingly construct hashtag prediction\nand cross-year hashtag prediction datasets. The\nperformance of downstream tasks Ô¨Åne-tuned from\nthe Ô¨Ånal pretrained model is presented in Table 10.\nWe see Sequential PT clearly outperforms BERT-\nbase which is not continually pretrained, and that\nLogit-KD generally improves hashtag prediction\nperformance compared to Sequential PT except on\nthe Ô¨Årst half of 2019. We hypothesize the small\ntemporal gap between D1..4 makes improvements\nless signiÔ¨Åcant than our main experiment setup.\nWe present temporal generalization performance\nin cross-year hashtag prediction tasks in Table 11.\nSimilarly, Logit-KD improves over Sequential PT\nin two out of three cross-year hashtag prediction\nsetups.\nI Analysis of Data Streams\nIn this section, we provide further analysis about\nthe created research paper stream and the tweet\nstream. We measure cosine distances dv of vocab-\nulary distributions between each pair of different\ndomains (D1..4) and summarize the results in Fig-\nure 8. The results indicate that the Tweet stream has\na magnitude smaller vocabulary distribution gap\nbetween domains, which is in the scale of 1e‚àí5,\ncompared to the research paper stream, which is\nin the scale of 1e‚àí2. On the Tweet stream, we see\nthe differences of vocabulary distributions align\nwith the temporal gap between domains. On the\nresearch paper stream, we Ô¨Ånd some domains to\nbe more similar than others. For example, Bio-\nmedical (D1) and Material Science domains (D3)\nhave larger similarity in their vocabulary distribu-\ntions, which explains general downstream perfor-\nmance increase on D1 after the model is pretrained\non D3 (Fig. 4 (a,b)).\nThe differences in vocabulary distribution ex-\nplain inconsistency in results between two data\n4779\nTask D1- Biomedical D2- Computer ScienceD3- Materials Science D4- Physics\nDataset Chemprot RCT-Sample MLM ACL-ARC SciERC MLM MNER Synthesis MLM Keyphrase Hyponym MLM\nSequential Pretraining 82.09¬±0.5 79.60¬±0.5 1.654 72.73¬±2.9 81.43¬±0.8 1.807 83.99¬±0.3 92.10¬±1.0 1.590 67.57¬±1.0 74.68¬±4.4 1.381Sequential Pretrainingb‚Ä≤=1.2b 81.68¬±0.5 79.80¬±0.4 1.656 70.57¬±3.0 80.89¬±1.2 1.793 83.65¬±0.3 92.16¬±0.7 1.57867.61¬±1.4 75.03¬±4.1 1.379ER 82.73 ¬±0.3 79.98¬±0.3 1.737 72.50¬±1.0 81.64¬±1.1 1.857 83.99¬±0.4 92.65¬±0.4 1.621 66.11¬±1.1 72.82¬±4.3 1.391ERk=5 83.00¬±0.1 79.79¬±0.4 1.913 69.85¬±2.6 82.30¬±1.2 2.04984.03¬±0.2 91.60¬±0.6 1.721 65.55¬±0.4 75.64¬±3.2 1.418Logit-KD-Sparse 82.80¬±0.4 79.80¬±0.5 1.476 73.31¬±2.0 81.19¬±0.8 1.744 83.84¬±0.4 92.29¬±0.7 1.47266.65¬±0.7 77.27¬±7.1 1.385SEED-KD-Sparse 82.51¬±0.4 79.52¬±0.5 1.474 73.70¬±3.4 81.92¬±0.8 1.74183.96¬±0.3 92.20¬±1.0 1.480 64.75¬±1.1 71.29¬±3.6 1.381\nTable 9: Performance of distillation algorithms in the setup of controlled computational costs.\nTask 2019-1 2019-2 2020-1 2020-2\nHashtag Prediction\nBERT-base 46.38¬±0.4 48.05¬±0.8 41.67¬±1.0 69.00¬±0.5\nSequential PT 50.46¬±0.1 52.70¬±0.7 46.49¬±1.0 71.63¬±0.7\nER 49.90 ¬±0.4 52.33¬±0.6 46.84¬±0.3 71.67¬±0.4\nLogit-KD 50.19 ¬±0.9 53.70¬±0.4 47.64¬±0.4 72.44¬±0.5\nSEED-Logit-KD50.79¬±0.8 52.84¬±0.5 46.04¬±0.4 72.24¬±0.6\nTable 10: Hashtag prediction performance of continually\npretrained BERT models over tweets after 2019.\nTask 2019-1 ‚Üí2019-2 2019-1‚Üí2020-1 2019-1‚Üí2020-2\nHashtag Prediction\nBERT-base 40.19 ¬±0.3 41.00¬±0.6 40.85¬±0.8\nSequential PT 43.30¬±0.7 48.60¬±2.1 44.07¬±0.8\nER 42.96 ¬±0.9 46.07¬±1.6 44.26¬±0.7\nLogit-KD 43.35 ¬±1.6 46.91¬±0.5 45.03¬±0.2\nSEED-Logit-KD43.56¬±0.4 45.77¬±0.7 43.76¬±0.5\nTable 11: Temporal generalization performance of Hash-\ntag prediction models Ô¨Åne-tuned from continually pretrained\nBERT models over tweets after 2019.\nstreams, speciÔ¨Åcally, whether lifelong pretraining\nimproves downstream model performance on the\nlatest domain, as we mentioned in Sec. 4.3. Other\nthan this, our main Ô¨Åndings, such as the effect of\ndistillation-based CL algorithms on reducing for-\ngetting, are consistent over two datasets with such\nsigniÔ¨Åcant differences in their changes of vocab-\nulary distribution. We believe it implies the con-\nclusions in this paper should be reliable in diverse\ndata streams.\nJ Ethic Risks\nWe would like to note that, in practice, continu-\nally pretrained models over real-world data streams\nwould require identiÔ¨Åcation and removal of biased\ncontents from pretraining corpora, which may af-\nfect the prediction of downstream models. As\nPTLMs are continuously updated, the bias in earlier\npretraining may have a profound negative impact.\nIn future works, it is preferable to develop algo-\nrithms to ‚Äúforget‚Äù certain biased knowledge from\nlanguage models. We further note that any data\nreleased in this paper, especially the tweet stream,\nshould only be used for research purposes.\nBioMed\nCS\nMaterials Physics\nBioMed\nCS\nMaterials\nPhysics\n0.00e+00 5.35e-02 1.28e-02 6.58e-02\n0.00e+00 2.65e-02 1.17e-02\n0.00e+00 2.97e-02\n0.00e+00\nResearch Paper Stream\n(a) Research Paper Stream\n2014 2016 2018 2020\n2014\n2016\n2018\n2020\n0.00e+00 9.92e-06 1.61e-05 2.03e-05\n0.00e+00 9.40e-06 2.54e-05\n0.00e+00 6.83e-06\n0.00e+00\nT weet Stream\n(b) Tweet Stream\nFigure 8: Cosine distance of vocabulary distributions\nbetween each pair of datasets in two data streams.\n4780"
}