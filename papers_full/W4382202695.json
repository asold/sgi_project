{
  "title": "Graphix-T5: Mixing Pre-trained Transformers with Graph-Aware Layers for Text-to-SQL Parsing",
  "url": "https://openalex.org/W4382202695",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2098844517",
      "name": "Jin-yang Li",
      "affiliations": [
        "University of Hong Kong",
        "Alibaba Group (China)",
        "Alibaba Group (United States)",
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2965193840",
      "name": "Binyuan Hui",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2138267588",
      "name": "Reynold Cheng",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2104991633",
      "name": "Bowen Qin",
      "affiliations": [
        "Shenzhen Institutes of Advanced Technology",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2398493207",
      "name": "Chenhao Ma",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A1827387864",
      "name": "Nan Huo",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A1936961387",
      "name": "Fei Huang",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2105203284",
      "name": "Wenyu Du",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2127260490",
      "name": "Luo Si",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2100492699",
      "name": "Yong-Bin Li",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2098844517",
      "name": "Jin-yang Li",
      "affiliations": [
        "Alibaba Group (Cayman Islands)",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2138267588",
      "name": "Reynold Cheng",
      "affiliations": [
        "University of Hong Kong",
        "Guangdong-Hongkong-Macau Joint Laboratory of Collaborative Innovation for Environmental Quality"
      ]
    },
    {
      "id": "https://openalex.org/A2104991633",
      "name": "Bowen Qin",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Shenzhen Institutes of Advanced Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2770846590",
    "https://openalex.org/W4307205863",
    "https://openalex.org/W3169459536",
    "https://openalex.org/W2971821075",
    "https://openalex.org/W3104123491",
    "https://openalex.org/W3152888762",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2989536007",
    "https://openalex.org/W2060277733",
    "https://openalex.org/W3199077625",
    "https://openalex.org/W3200079259",
    "https://openalex.org/W2945102109",
    "https://openalex.org/W3120409403",
    "https://openalex.org/W4221161254",
    "https://openalex.org/W6791680975",
    "https://openalex.org/W4293819888",
    "https://openalex.org/W4290638352",
    "https://openalex.org/W4295990229",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3094344961",
    "https://openalex.org/W3200285169",
    "https://openalex.org/W2797328513",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6745537798",
    "https://openalex.org/W2985377636",
    "https://openalex.org/W3017464126",
    "https://openalex.org/W4283733706",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4221166833",
    "https://openalex.org/W2768409085",
    "https://openalex.org/W6744893907",
    "https://openalex.org/W2163274265",
    "https://openalex.org/W3085281588",
    "https://openalex.org/W3175488485",
    "https://openalex.org/W3134835171",
    "https://openalex.org/W3170698273",
    "https://openalex.org/W4289494028",
    "https://openalex.org/W3119822474",
    "https://openalex.org/W3035529900",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W4385573280",
    "https://openalex.org/W4288025992",
    "https://openalex.org/W2998496395",
    "https://openalex.org/W2952032096",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2762513422",
    "https://openalex.org/W3214600982",
    "https://openalex.org/W2963993485",
    "https://openalex.org/W3173274550",
    "https://openalex.org/W4385572953",
    "https://openalex.org/W4301674784",
    "https://openalex.org/W4293569541",
    "https://openalex.org/W3105055324",
    "https://openalex.org/W4285244831",
    "https://openalex.org/W3103962261",
    "https://openalex.org/W4288089799"
  ],
  "abstract": "The task of text-to-SQL parsing, which aims at converting natural language questions into executable SQL queries, has garnered increasing attention in recent years. One of the major challenges in text-to-SQL parsing is domain generalization, i.e., how to generalize well to unseen databases. Recently, the pre-trained text-to-text transformer model, namely T5, though not specialized for text-to-SQL parsing, has achieved state-of-the-art performance on standard benchmarks targeting domain generalization. In this work, we explore ways to further augment the pre-trained T5 model with specialized components for text-to-SQL parsing. Such components are expected to introduce structural inductive bias into text-to-SQL parsers thus improving the model‚Äôs capacity on (potentially multi-hop) reasoning, which is critical for generating structure-rich SQLs. To this end, we propose a new architecture GRAPHIX-T5, a mixed model with the standard pre-trained transformer model augmented by specially-designed graph-aware layers. Extensive experiments and analysis demonstrate the effectiveness of GRAPHIX-T5 across four text-to-SQL benchmarks: SPIDER, SYN, REALISTIC and DK. GRAPHIX-T5 surpasses all other T5-based parsers with a significant margin, achieving new state-of-the-art performance. Notably, GRAPHIX-T5-large reaches performance superior to the original T5-large by 5.7% on exact match (EM) accuracy and 6.6% on execution accuracy (EX). This even outperforms the T5-3B by 1.2% on EM and 1.5% on EX",
  "full_text": "Graphix-T5: Mixing Pre-trained Transformers with\nGraph-Aware Layers for Text-to-SQL Parsing\nJinyang Li1,2*, Binyuan Hui2, Reynold Cheng1,5‚Ä† , Bowen Qin3, Chenhao Ma4, Nan Huo1,\nFei Huang2, Wenyu Du1, Luo Si2, Yongbin Li2 ‚Ä†\n1The University of Hong Kong\n2DAMO Academy, Alibaba Group\n3Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences\n4The Chinese University of Hong Kong (Shenzhen)\n5Guangdong‚ÄìHong Kong-Macau Joint Laboratory\n{jl0725,huonan,wenyudu}@connect.hku.hk, ckcheng@cs.hku.hk,\nbw.qin@siat.ac.cn, machenhao@cuhk.edu.cn,\n{binyuan.hby,f.huang,luo.si,shuide.lyb}@alibaba-inc.com\nAbstract\nThe task of text-to-SQL parsing, which aims at converting\nnatural language questions into executable SQL queries, has\ngarnered increasing attention in recent years. One of the\nmajor challenges in text-to-SQL parsing is domain gener-\nalization, i.e. , how to generalize well to unseen databases.\nRecently, the pre-trained text-to-text transformer model,\nnamely T5, though not specialized for text-to-SQL pars-\ning, has achieved state-of-the-art performance on standard\nbenchmarks targeting domain generalization. In this work,\nwe explore ways to further augment the pre-trained T5\nmodel with specialized components for text-to-SQL pars-\ning. Such components are expected to introduce struc-\ntural inductive bias into text-to-SQL parsers thus improv-\ning model‚Äôs capacity on (potentially multi-hop) reasoning,\nwhich is critical for generating structure-rich SQLs. To this\nend, we propose a new architecture GRAPHIX -T5, a mixed\nmodel with the standard pre-trained transformer model aug-\nmented by specially-designed graph-aware layers. Exten-\nsive experiments and analysis demonstrate the effectiveness\nof G RAPHIX -T5 across four text-to-SQL benchmarks: SPI-\nDER , SYN, REALISTIC and DK. G RAPHIX -T5 surpass all\nother T5-based parsers with a significant margin, achiev-\ning new state-of-the-art performance. Notably, GRAPHIX -T5-\nlarge reaches performance superior to the original T5-large by\n5.7% on exact match (EM) accuracy and 6.6% on execution\naccuracy (EX). This even outperforms the T5-3B by 1.2% on\nEM and 1.5% on EX.\n1 Introduction\nRelational database, serving as an important resource for\nusers to make decision in many fields, such as health care,\nsports, and entertainment, has emerged frequently because\nof the big data era. It is efficient for data users to access the\ninformation from databases via structured query language,\n* Work done during an intern at Alibaba DAMO Academy.\n‚Ä† Corresponding authors are Reynold Cheng and Yongbin Li.\nCopyright ¬© 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nNature Language Question:\nFind the number of dog pets that are raised by female  students\nüßëüíª\nfemalestudent\nStudent Sex\nMOD\nEM\nHAS\nQuestion\nColumn\nTable\nDesired Linking\nDatabase:\nPets\nPetID PetType Pet_age PetID StuID\nHas_Pet Student\nStuID Sex Age\nSQL:\nSELECT COUNT(*) FROM student AS T1 JOIN has_pet AS \nT2 ON T1.stuid = T2.stuid JOIN pets AS T3 ON T2.petid = \nT3.petid WHERE T1.sex = 'F' AND T3.pettype = 'dog' \nFigure 1: This is an illustration of cross-domain text-to-SQL\nchallenge. The link between the target column sex and the\ntoken female is highly desired but extremely challeng-\ning for the model to capture. However, this dilemma can\nbe mitigated by a multi-hop reasoning path (female\nMOD\n‚àí‚Üí\nstudent\nEM\n‚àí‚ÜíStudent\nHAS\n‚àí‚ÜíSex).\ne.g., SQL. Despite its effectiveness and efficiency, the com-\nplex nature of SQLs leads to extremely expensive learning\nefforts for non-technical users. Therefore, text-to-SQL (Cai\net al. 2018; Zelle and Mooney 1996; Xu, Liu, and Song\n2017; Yu et al. 2018a; Yaghmazadeh et al. 2017), aiming to\nconvert natural language instructions or questions into SQL\nqueries, has attracted remarkable attention.\nIn this work, we explore the challenging cross-domain\nsetting where a text-to-SQL parser needs to achieve domain\ngeneralization, i.e. , the ability to generalize to domains that\nare unseen during training. Achieving this goal would, in\nprinciple, contribute to a universal natural language interface\nthat allows users to interact with data in arbitrary domains.\nThe major challenge towards domain generalization (Wang\net al. 2020a; Cao et al. 2021; Wang et al. 2022; Cai et al.\n2021; Hui et al. 2022) is that generating structure-rich SQLs\nrequires (potentially multi-hop) reasoning, i.e. the ability\nto properly contextualize a user question against a given\ndatabase by considering many explicit relations (e.g., table-\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n13076\nGNN\nEncoder\nDecoder\nPre-trained (BERT)\nQuestion | Schema\nTransformer\nRandom Init.\nSQL\n(a) RATSQL\nTransformer\nTransformer\nTransformer\nTransformer\nTransformer\nPre-trained (T5)\nQuestion | Schema\nSQL\n(b) T5\nTransformer\nTransformer\nTransformer\nPre-trained (T5)\nTransformer\nTransformer\nTransformer\nGNN\nPre-trained (T5)\nQuestion | Schema\nSQL\n(c) GNN-T5\nTransformer\nTransformer\nTransformer\nPre-trained (T5)\nTransformer\nTransformer\nTransformer\nPre-trained (T5)\nQuestion | Schema\nSQL\n(d) Graphix-T5\nTransformer\nTransformer\nTransformer\nPre-trained (T5)\nTransformer\nTransformer\nTransformer\nGraphix Layer\nGraphix Layer\nGraphix Layer\nFigure 2: Graphical illustration of existing methods (a) RATSQL [pre-trained BERT-encoder ‚Üí graph-based module ‚Üí ran-\ndomly initialized decoder]. (b) T5 [pre-trained T5-encoder ‚Üí pre-trained T5-decoder] and the proposed variant (c) GNN-\nT5 [pre-trained T5-encoder ‚Üí graph-based module ‚Üí pre-trained T5-decoder] (d) G RAPHIX -T5 [semi-pre-trained graphix-\nmodule ‚Üí pre-trained T5-decoder].\ncolumn relations specified by database schema) and implicit\nrelations (e.g., whether a phrase refers to a column or table).\nFigure 1 shows an introductory example of multi-hop rea-\nsoning in the text-to-SQL parsing and Figure 5 presents two\nmore detailed cases.\nFrom the modeling perspective, there are two critical di-\nmensions along which we can differentiate current text-to-\nSQL parsers. The first is how to effectively imbue rela-\ntional structures (both explicit and implicit) in the form of\ngraphs into neural networks, and the second is how to take\nthe most advantage of pre-trained models (e.g.T5 (Raffel\net al. 2020)). These two dimensions are inter-connected and\nform a spectrum of methods. On one end of the spectrum,\nPICARD (Scholak, Schucher, and Bahdanau 2021) uses\nthe original pre-trained T5 model by linearizing database\nschemas into sequences, hoping that T5 can successfully\ncapture the underlying relational structures. On the other\nend of the spectrum, RAT-SQL (Wang et al. 2020a) only uti-\nlizes pre-trained encoders (e.g., BERT (Devlin et al. 2019))\nand explicitly captures desired relations via specialized\nrelation-aware models. However, more powerful encoder-\ndecoder based pre-trained models are not exploited in this\nframework, but relational structures are accommodated at\nmost. In this work, we explore the cross zone where the\nencoder-decoder based pre-trained models (specifically T5)\nand relation-aware encodings are deeply coupled in favor of\nbetter domain generalization. We first observe that naively\nadding a relational graph-based module in the middle of T5,\nresulting in a ‚ÄòT5-encoder ‚Üí graph-based module ‚Üí T5-\ndecoder architecture‚Äô (see also Figure 2(c), namely GNN-\nT5), does not work very well on standard benchmarks. Pre-\nsumably, the deficiency comes from the middle graph-based\nmodules breaking the original information flow inside T5.\nIn order to address this problem, we present a novel ar-\nchitecture called GRAPHIX -T5 that is capable of effectively\nmodelling relational structure information while maintain-\ning the powerful contextual encoding capability of the pre-\ntrained T5. First, we design a G RAPHIX layer that simul-\ntaneously encodes a mixture of semantic and structural in-\nformation. Concretely, hidden states of inputs composed by\nquestions and databases are modeled by contextualized se-\nmantic encoding, and the structural representation is injected\nin each transformer layer using a relational GNN block\nthat enhances multi-hop reasoning through message passing\n(Fang et al. 2020; Velickovic et al. 2018) to capture explicit\nand implicit relations. Second, we construct a new encoder\nby stacking the G RAPHIX layers and replacing the origi-\nnal T5 encoder. In each G RAPHIX layer, the parameters of\nthe semantic block are still initialized by T5, in an attempt\nto maintain the contextualized encoding power of the pre-\ntraining. In contrast to the severed GNN-T5 (Figure 2.(c)),\nthe GRAPHIX -T5 (Figure 2.(d)) will allow intensive interac-\ntion between semantic and structure from the starting layers.\nWe empirically show the effectiveness of G RAPHIX -T5\non several cross-domain text-to-SQL benchmarks,i.e. , SPI-\nDER , S YN, D K and R EALISTIC . On these datasets, the\nproposed model achieves new state-of-the-art performance,\nsubstantially outperforming all existing models by large\nmargins. Specifically, GRAPHIX -T5-large surprisingly beats\nthe vanilla T5-3B. Furthermore, we verified that GRAPHIX -\nT5 can also achieve the significant improvement in the low-\nresource and compositional generalization obviously thanks\nto the introduction of structural bias.\n2 Task Formulation and Notations\n2.1 Task Definition\nGiven a natural language question Q =\n\b\nq1, ..., q|Q|\n\t\nwith\nits corresponding database schemas D = ‚ü®C, T ‚ü©, whereC =\n13077\n\b\nc1, ..., c|C|\n\t\nand T =\n\b\nt1, ..., t|T |\n\t\nrepresent columns and\ntables, |C| and |T |refer to the number of columns and tables\nin each database respectively. The goal of text-to-SQL is to\ngenerate the corresponding SQL query y.\n2.2 Vanilla T5 Architecture\nModel Inputs The most canonical and effective format of\ninputs to T5 performing text-to-SQL task isPeteShaw(Shaw\net al. 2021), which unifies natural language questions Q and\ndatabase schema D as a joint sequence as shown:\nx = [q1, ..., q|Q| |Dname |t1 : ct1\n1 , ..., ct1\n|C||...|t|T |: c\nt|T |\n1 , ..., c\nt|T |\n|C| |‚àó],\n(1)\nwhere qi is ith token in the question, tj represents jth table\nin the D, and ctj\nk refers to the kth column in the jth table.\n‚àó is the special column token in the database. Dname is the\nname of each database.\nEncoder-Decoder Training Mechanism Following\n(Shaw et al. 2021), T5 (Raffel et al. 2020) adopt an\nencoder-decoder mechanism to generate SQLs. First, the\nbi-directional encoder learns the hidden state h of input x,\nthen the decoder generates SQLs based on h as:\nh = EncŒò (x) ;y = DecŒ•(h), (2)\nwhere Œò and Œ• refers to parameters of the encoder and de-\ncoder, and h connects the encoder and decoder. The model\nis initialized with pretrained T5 parameters and optimized as\nthe following objective.\nmax\nŒò,Œ•\nlog pŒò,Œ•(y | x) =\n|y|X\ni=1\nlog pŒò,Œ• (yi | y1:i‚àí1, x) , (3)\nwhere x, y indicates the input and output tokens respectively\nand |y| is the max length of generation SQL.\n3 Proposed Approach: G RAPHIX -T5\n3.1 Model Inputs\nContextual Encoding We continue to take both questions\nand database schemas as depicted in Eq. (1) to encode the\ncontextual information through the original T5.\nGraph Construction The joint input questions and\nschemas can be displayed as a heterogeneous graph G =\n‚ü®V, R‚ü© consisting of three types of nodesV = Q‚à™C‚à™T and\nmultiple types of relations R = r1, ..., r|R|, where each ri\nrefers to a one-hop relation between nodes and a multi-hop\nrelation rk is defined as a composition of one-hop relations:\nrk = r1 ‚ó¶r2 ¬∑¬∑¬∑‚ó¶ rI as shown in the Figure 1, whereI refers\nto the length of eachrk. Inspired by (Wang et al. 2020a; Cao\net al. 2021; Qin et al. 2022b; Hui et al. 2022), we enumerated\na list of pre-defined relations to connect nodes. The relation\nsets can be divided into three main categories:\n‚Ä¢ Schema relations: F OREIGN -KEY, P RIMARY -KEY, and\nSAME -TABLE pertain to the particular explicit schema\nrelations that the original T5 cannot obtain from linear\ninputs.\nDocument_id\nParagraph_text\nÔ¨Åles\nids\ncontent\nARG\nMOD\ntext\nDocuments\nMOD\nPARTIAL-MATCH\nPARTIAL-MATCH\ncontent\nÔ¨Åles\ntext\nParagraph_text\nDocument_id\nDocument\nN\nO-MATCH\ncontent\nÔ¨Åles\ntext\nParagraph_text\nDocument_id\nDocument\nBridge Node\n*\n(a) No-Match Mode (b) Bridge Node Mode\nFigure 3: Figure shows the circumstances when entities in\nthe question are hard to string-match the schema items. (a) is\nthe strategy to solve this case by N O-MATCH Mode, which\nfully connects schema nodes with all token nodes. (b) is our\nsolution to add a bridge node to link the question and schema\nnodes via less number of edges.\n‚Ä¢ Schema linking relations: E XACT-MATCH, P ARTIAL -\nMATCH, and V ALUE -MATCH are implicit linking rela-\ntions between question and schema nodes. A new type of\nrelation BRIDGE is introduced.\n‚Ä¢ Question relations: M ODIFIER and ARGUMENT are im-\nplicit dependency relations between tokens in a question.\nNO-MATCH Mode vs. B RIDGE Mode Previous works\n(Cao et al. 2021; Hui et al. 2022) through adding the dummy\nedges called NO-MATCH indicate that the there are question\ntokens and the schema tokens, which should be correlated\nbut cannot be linked due to existing string-matched rules.\nHowever, as shown in Figure 3, N O-MATCH may lead to\nan over-smoothing problem (Chen et al. 2020a) since they\nbring out too many noisy neighbors to compute the attention\nscore. Suppose there exists A tokens for the question and B\nschema items that are semantically relevant but not linked\nby the rule, the number of edges need to be linked as N O-\nMATCH is A √óB. In contrast, we leverage the special token\n* as a bridge node, allowing all schema nodes to be reached\nfrom the question token nodes by decreasing the number of\nedges drastically from A √ó B to A + B.\n3.2 Graphix-Layer\nThe GRAPHIX layer is designed to integrate semantic infor-\nmation obtained from each transformer block with structural\ninformation of a relational graph neural network (GNN)\nblock.\nSemantic Representation The semantic representations\nof hidden states are firstly encoded by a Transformer\n(Vaswani et al. 2017) block, which contains two important\ncomponents, including Multi-head Self-attention Network\n(MHA) and Fully-connected Forward Network (FFN). In\nthe lth GRAPHIX Layer, the hidden states represent Hl\nS =n\nh(l)\n1 , . . . , h(l)\nN\no\n, N is the max length of the inputs. MHA\nat first maps query matrix Q ‚àà Rm√ódk , key and value ma-\ntrix K ‚àà Rn√ódk , V ‚àà Rn√ódv into an attention vector via\n13078\nself-attention mechanism as Eq. (4)\nAttn(Q,K, V) =softmax\n\u0012QKT\n‚àödk\n\u0013\nV, (4)\nin which m is the number of query vectors and n is the\nnumber of key or value vectors. MHA executes the self-\nattention over h heads with each head i being indepen-\ndently parameterized by WQ\ni ‚àà Rdm√ódk , WK\ni ‚àà Rdm√ódk ,\nWV\ni ‚àà Rdm√ódv and mapping inputs into queries, key-value\npairs. Usually dk = dv = dm/h in the transformer blocks\nof T5 and dm denotes the dimension of T5. Then MHA cal-\nculates the attention outputs for each head and concatenate\nthem as following:\nheadi = Attn(QWQ\ni , KWK\ni , VWV\ni ), (5)\nMHA(H(l)\nS ) = Concat (head1, ¬∑¬∑¬∑ , headh) WO, (6)\nbH(l)\nS = MHA(H(l)\nS ), (7)\nwhere WO ‚àà Rdmh√ódm is a trainable parameter matrix.\nSemantic hidden states need to be acquired through another\ncomponent, i.e. , FFN, which is applied as Eq. (8).\nFFN( bH(l)\nS ) =max\n\u0010\n0, bH(l)\nS W1 + b1\n\u0011\nW2 + b2, (8)\nwhere linear weight matrices represent W1 ‚àà Rdm√ódff ,\nW2 ‚àà Rdff √ódm respectively. Experimentally, larger dff is\npreferred, which is usually set as dff = 4d m. Eventually,\nthe semantic hidden states are acquired after layer normal-\nization and residual connection as\nÀúH(l)\nS = LayerNorm( bH(l)\nS + FFN( bH(l)\nS )), (9)\nStructural Representation In each G RAPHIX Layer,\nstructural representations are produced through the rela-\ntional graph attention network (RGAT) (Wang et al. 2020b)\nover the pre-defined question-schema heterogeneous graph.\nFormally, given initial node embedding 1 einit\ni for ith node\nand its jth neighbor einit\nj linked by specific types of rela-\ntions, it can be computed through:\n‚Éó Œ±ij =\neinit\ni fWQ\n\u0010\neinit\nj fWK + œï (rij)\n\u0011‚ä§\n‚àödz\n, (10)\nŒ±ij = softmaxj (‚Éó Œ±ij) , (11)\nÀÜeinit\ni =\nX\nj‚àà eNi\nŒ±ij\n\u0010\neinit\nj fWV + œï(rij)\n\u0011\n, (12)\nÀÜe(l)\ni = LayerNorm(einit\ni + ÀÜeinit\ni fWO), (13)\nÀúe(l)\ni = LayerNorm(ÀÜe(l)\ni + FFN(ÀÜe(l)\ni )), (14)\nThen the output node embeddings are collected as ÀúE(l)\nG =n\nÀúe(l)\n1 , . . . ,Àúe(l)\nN\no\n, where fWQ, fWK, fWV , fWO ‚àà Rd√ód are\n1Various initialization strategies could be implemented. In this\nwork, we initialized the node embeddings with their semantic rep-\nresentations.\ntrainable parameters in the RGAT.œï(rij) is a mapping func-\ntion that can produce a d-dim embedding representing for\neach relation between ith node and jth node. More impor-\ntantly, eNi denotes the relational reception field, which is\nequal to the number of how many neighbors ofith node that\nRGAT will consider when updating representation of each\nnode via message passing.\nJointly Representation After computing representations\nfrom both semantic and structural space, the lth GRAPHIX\nLayer employs a mixture of semantic and structural infor-\nmation to enable information integration as following:\nÀúH(l)\nM = ÀúH(l)\nS + ÀúE(l)\nG , (15)\n3.3 Graphix-T5\nHere we present our entire G RAPHIX -T5 model formally.\nThe hidden states of the last layer of GRAPHIX -encoder can\nbe represented as:\nh = EncŒò,Œ® (x, G) , (16)\nwhere G is the question-schema heterogeneous graph, the Œ®\nare the additional parameters of the RGAT, which are initial-\nized randomly. In order to preserve the pre-trained semantic\nknowledge, we migrate parameters Œò from original T5 en-\ncoder as the initial parameters of semantic transformer block\nof the GRAPHIX layer.\n3.4 Training\nSimilar to original T5, we also follow a fine-tuning strategy.\nThe whole training framework is to optimize the following\nlog-likelihood.\nmax\nŒò,Œ•,Œ®\nlog pŒò,Œ•,Œ®(y | x) =\n|y|X\ni=1\nlog pŒò,Œ•,Œ® (yi | y1:i‚àí1, x,G) .\n(17)\n4 Experiment\n4.1 Set Up\nDatasets and Settings We conduct extensive experiments\non four challenging benchmarks for cross-domain text-to-\nSQLs and two different training settings. (1) S PIDER (Yu\net al. 2018b) is a large-scale cross-domain text-to-SQL\nbenchmark. It contains 8659 training examples and 1034 de-\nvelopment examples, which covers 200 complex databases\nacross 138 domains. The testing set is not available for indi-\nvidual review. (2) SYN (Gan et al. 2021a) replaces the sim-\nple string-matched question tokens or schema names with\ntheir synonyms. (3) D K (Gan, Chen, and Purver 2021) re-\nquires the text-to-SQL parsers to equip with the capability of\ndomain knowledge reasoning. (4) REALISTIC removes and\nswitches the obvious mentions of schema items in questions,\nmaking it closer to the real scenarios. Furthermore, we also\ntest the compositional generalization ability of our model\non the SPIDER -SSP (Shaw et al. 2021) with three splits\nfrom S PIDER : Spider-Length (split dataset based on vari-\nant lengths); Spider-TMCD (Target Maximum Compound\nDivergence) and Spider-Template (split based on different\n13079\nMODEL EM EX\nRAT-SQL +\nBERT ‚ô° 69.7 -\nRAT-SQL + Grappa‚ô° 73.9 -\nGAZP + BERT 59.1 59.2\nBRIDGE v2 + BERT 70.0 68.3\nNatSQL + GAP 73.7 75.0\nSMBOP + GRAPPA 74.7 75.0\nLGESQL + ELECTRA ‚ô° 75.1 -\nS2SQL + ELECTRA ‚ô° 76.4 -\nT5-large 67.0 69.3\nGRAPH\nIX-T5-large 72.7(‚Üë 5.7) 75.9(‚Üë 6.6)\nT5-large + PI\nCARD ‚ô£ 69.1 72.9\nGRAPH\nIX-T5-large + PICARD ‚ô£ 76.6(‚Üë 7.5) 80.5(‚Üë 7.6)\nT5-3B 71.5 74.4\nGRAPH\nIX-T5-3B 75.6 (‚Üë 4.1) 78.2 (‚Üë 3.8)\nT5-3B + PIC\nARD ‚ô£ 75.5 79.3\nGRAPH\nIX-T5-3B + PICARD ‚ô£ 77.1(‚Üë 1.6) 81.0(‚Üë 1.7)\nTable 1: Exact match (EM) and execution (EX) accuracy (%)\non S PIDER development set. ‚ô° means the model does not\npredict SQL values. ‚ô£ means the model uses the constrained\ndecoding PICARD . ‚Üë is an absolute improvement.\nparsing templates). Finally, the performances of G RAPHIX -\nT5 on LOW-RESOURCE setting are evaluated on usage of\n10%, 20%, 50% data separately.\nEvaluation Metrics Following (Yu et al. 2018b), Exact\nMatch (EM) and Execution Accuracy (EX) are the two stan-\ndard metrics we use to measure performance of our model.\nEM can evaluate how much a generated SQL is comparable\nto the gold SQL.\nImplementation Details We implement our codes 2\nmainly based on hugging-face transformers library (Wolf\net al. 2020) 3. We set the max input length as 1024, gen-\neration max length as 128, and batch size as 32. We also\nadopt Adafactor (Shazeer and Stern 2018) as our primary\noptimizer with a linear decayed learning rate of 5e-5. Dur-\ning the experiment, G RAPHIX layers are mainly injected\ninto the encoder to learn better representations for structural\ngeneralization. We evaluate our effectiveness of GRAPHIX -\nT5 across two main versions: T5-Large with approximately\n800M parameters and T5-3B, with more than 3 Billion pa-\nrameters literally. All experiments are conducted on one\nNVIDIA Tesla A100, which is available for most research\ncenters.\nCompared Methods Our model are compared mainly to\nmainstream strong baseline models such as GNNSQL (Bo-\ngin, Berant, and Gardner 2019), RATSQL (Wang et al.\n2020a), GAZP (Zhong et al. 2020), BRIDEGE (Chen et al.\n2020b), SMBOP (Rubin and Berant 2021), NatSQL (Gan\net al. 2021b), LGESQL (Cao et al. 2021), S2SQL (Hui et al.\n2022) and T5+PICARD (Scholak, Schucher, and Bahdanau\n2021) across the disparate datasets and settings.\n2https://github.com/AlibabaResearch/DAMO-ConvAI/tree/\nmain/graphix\n3https://huggingface.co/\nMODEL SY\nN DK REALISTIC\nGNN 23.6 26.0 -\nIRNet 28.4\n33.1 -\nRAT-SQL 33.6 35.8 -\nRAT-SQL + BERT 48.2 40.9 58.1\nRAT-SQL + Grappa 49.1 38.5 59.3\nLGESQL + ELECTRA 64.6 48.4 69.2\nT5-large 53.6 40.0 58.5\nGRAPH\nIX-T5-large 61.1 (‚Üë 7.5) 48.6 (‚Üë 8.6) 67.3 (‚Üë 8.8)\nT5-3B 58.0 46.9 62.0\nGRAPH\nIX-T5-3B 66.9 (‚Üë 8.9) 51.2 (‚Üë 4.3) 72.4 (‚Üë 10.4)\nTable 2: Exact match (EM) accuracy (%) on S YN, D K and\nREALISTIC benchmark.\nMODEL TE\nMPLATE LENGTH TMCD\nT5-base 59.3 49.0 60.9\nT5-3B 64.8\n56.7 69.6\nNQG-T5-3B 64.7 56.7 69.5\nGRAPH\nIX-T5-3B 70.1 (‚Üë 5.4) 60.6 (‚Üë 3.9) 73.8 (‚Üë 4.3)\nTable 3: Exact match (EM) accuracy (%) on compositional\ndataset SPIDER -SSP.\n4.2 Overall Performance\nResults on S PIDER Table 1 displays the performance\nof G RAPHIX -T5 and other competitive baseline models\non official S PIDER benchmark. First, we demonstrate that\nGRAPHIX -T5-3B with a constrained decoding module PI-\nCARD (Scholak, Schucher, and Bahdanau 2021) achieves\nthe state-of-the-art on this challenging text-to-SQL bench-\nmark. Also, it is evident that GRAPHIX -T5 is vastly superior\nto the vanilla T5 on large and 3B scales with a significant\nmargin. This indicates that the structural generalization ca-\npability of the GRAPHIX layer is crucial for T5, such a text-\nto-text PLM to perform the text-to-SQL task.\nZero-shot Results on More Challenging Settings As\nshown in the Table 2, we further demonstrate the robust-\nness of GRAPHIX -T5 when it confronts with more challeng-\ning and closer to realistic evaluations in S YN, D K, R EAL -\nISTIC without any additional training. First of all, the re-\nsults show that GRAPHIX -T5-3B outperforms other baseline\nmodels across all three datasets. Furthermore, we observe\nthat G RAPHIX -T5-large and G RAPHIX -T5-3B surpass the\nperformance of vanilla T5-large and T5-3B with a clear mar-\ngin, respectively. This demonstrates that vanilla T5 is hun-\ngry for structural reasoning when dealing with more flexible\nand complicated questions for text-to-SQLs from real-world\nscenarios. And GRAPHIX can mitigate this problem.\nResults on Compositional Generalization As shown in\nTable 3, on S PIDER -SSP, the grammar-based inductive T5\nmodel provided by (Shaw et al. 2021), named NQG-T5, has\nno obvious advantages over vanilla T5, which indicates that\nthe grammar of natural language is not helpful to enhance T5\nfor compositional generation. However, GRAPHIX -T5 helps\nthe T5 gain the SQL knowledge and makes it less vulner-\nable to these modifications through the effective fusion of\n13080\nMODEL SPIDE\nR SYN REALI\nSTIC\neasy medium hard e\nxtra all easy medium hard e\nxtra all easy medium hard e\nxtra all\nT5-large 85.5 70.9 55.2 41.6\n67.0 69.0 56.8 46.3 30.2\n53.6 79.8 68.0 44.4 28.9\n58.5\nGRAPH\nIX-T5-large 89.9 78.7 59.8 44.0 72.6 75.8 67.5 50.6 33.1 61.1 88.1 77.3 50.5 40.2 67.3\nT5-3B 89.5 78.3 58.6 40.4\n71.6 74.2 64.5 48.0 27.8\n58.0 85.3 73.4 46.5 27.8\n62.0\nGRAPH\nIX-T5-3B 91.9 81.6 61.5 50.0 75.6 80.6 73.1 52.9 44.6 66.9 93.6 85.7 52.5 41.2 72.4\nTable 4: Exact matching (EM) accuracy by varying the levels of difficulty of the inference data on three main benchmarks.\nMODEL EM EX\n(a) RAT-SQL + BERT 69.7 -\n(b) T5-large 67.0 69.3\n(c) GNN-T5-large 51.6 54.5\n(d) GRAPHIX -T5-large\nw/ BRIDGE Mode 72.7 75.9\nw/ NO-MATCH Mode 71.1 74.2\nw/ DOUBLE -GRAPH 72.0 74.7\nTable 5: Ablation study for the variant GNN + PLM tactics\non cross-domain text-to-SQLs, echoing Figure 2, (a) is RAT-\nSQL, (b) is vanilla T5, (c) is GNN-T5 and (d) is GRAPHIX .\n0 5 10 15 20 25\nTraining Step (k)\n0\n10\n20\n30\n40\n50\n60\n70\n80Accuracy (%)\nGNN-T5\nGraphix-T5\nFigure 4: The performance of the validation sets during the\nconvergence of G RAPHIX -T5 and GNN-T5 on S PIDER . It\ncan be clearly demonstrated that GNN-T5 has extremely un-\nsatisfactory performance, due to catastrophic forgetting.\nstructural information.\nResults on Complex Queries As presented in Table 4,\nwe also compare the more precise performance results of\nGRAPHIX -T5 to the vanilla T5 in four separate SQL diffi-\nculty levels splitted by S PIDER officially, in order to bet-\nter comprehend the performance improvements. We observe\nthat G RAPHIX -T5 is more capable of handling harder text-\nto-SQL cases, as illustrated in the Hard and Extra-hard\nexamples, indicating that structural bias training is benefi-\ncial to hard text-to-SQL cases.\n4.3 Ablation Study\nAs shown in Table 5, to better validate the function of each\ncomponent of GRAPHIX -T5, ablation studies are performed\nin large version and expected to answer the following ques-\ntions.\n[1] How effective is B RIDGE MODE ? GRAPHIX -T5-\nlarge with B RIDGE MODE can achieve the better perfor-\nmance than with N O-MATCH Mode via reducing the num-\nber of noisy neighbors. It indicates that N O-MATCH mode\nwill greatly increase the number of noisy neighbors, re-\nsulting in higher risk of over-smoothing issues (Chen et al.\n2020a).\n[2] Could G RAPHIX be incorporated into decoder ?\nWith D OUBLE -GRAPH means that G RAPHIX -T5 incorpo-\nrate GRAPHIX layer into the both encoder and decoder. The\nresult reveals that adding G RAPHIX layers to the decoder\ndoes not lead to any improvements. Since decoder is an auto-\nregressive model, which only considers the history tokens\nwhen generating the current token. However, G RAPHIX -\nT5, which can forecast the information of future tokens by\nglobal linking, may disrupt this characteristic leading to the\nnegative impact on the decoder. Therefore, we propose that\nthe best tactic is to only incorporate G RAPHIX layers into\nthe encoder.\n[3] Is GRAPHIX superior than other architecture vari-\nants ? Echoing Figure 2, we access the performance of 4\ncategories of models using PLMs on S PIDER . According to\nTable 5 (c), the performance of GNN-T5 has decreased by\nroughly 20% when compared to GRAPHIX -T5, proving that\nGNN-T5 meets the catastrophic forgetting problem (French\n1999). Since the accuracy of the GNN-T5 continues to be\n0 in the first thousands of steps and the performance de-\ncreases significantly from vanilla T5, as shown in Figure 4,\nit is evident that all pre-trained knowledge from T5 would\nbe forgotten. In contrast, the result verifies the advantages\nof G RAPHIX -T5 that can avoid catastrophic forgetting and\naugment generalization capability.\n4.4 Case Study\nTo illustrate the effectiveness of G RAPHIX qualitatively,\ntwo examples are displayed in Figure 5, which are sam-\npled randomly from S YN. Figure 5 shows the compari-\nson of predicted SQLs by vanilla T5-3B and G RAPHIX -\nT5-3B. We can observe that GRAPHIX can generate correct\nSQLs even in the hard scenarios. That is because that, even\nwith a small number of keywords overlapped, G RAPHIX -\nT5 can accurately identify counterpart column or table ob-\njects and generate a high-quality SQL through multi-hop\nreasoning and structural grounding. For example, in the first\ncase, vanilla T5-3B picks the incorrect columnspaper\nid,\npaper name, and paper description, which even\n13081\nValue-Match Belongs-To\nids\ndescription\nDocuments\npaper\nname\ndocument_name\ndocument_id\ndocument_description\nQuestionÔºöList paper IDs, paper names, and paper descriptions for all papers.\nT5-3BÔºöSELECT paper_id, paper_name, paper_description FROM documents\nGraphix-T5-3BÔºöSELECT document_id, document_name, document_description FROM documents\nMuti-hop Path\npaper ids document_id\npaper description document_description\npaper name document_name\nGoldÔºöSELECT document_id, document_name, document_description FROM documents\nQuestionÔºöHow many French car manufacturers are there?\nT5-3BÔºöSELECT COUNT(*) FROM car_makers WHERE country = \"France\"\nGraphix-T5-3BÔºö\nGoldÔºöSELECT COUNT(*) FROM car_makers AS T1 JOIN countries AS T2 ON T1.country  \n= T2.countryid WHERE T2.countryname = 'France';\nFrench\ncar\nmanufacture\ncar_makers\ncountries\ncountry\ncountryid\ncountryname\nFrench countryname countries\nFrench countryname country\nValue-Match Foreign-Key\nMuti-hop Path\nSELECT COUNT(*) FROM car_makers AS T1 JOIN countries AS T2 ON T1.country  \n= T2.countryid WHERE T2.countryname = \"France\"\ncountryid\nSame-Table\nModiÔ¨Åer\nModiÔ¨Åer\nModiÔ¨Åer\nPartial-Match\nPartial-Match\nPartial-Match\nFigure 5: Case study: two illustrative cases sampled randomly from SYN. It shows that multi-hop reasoning can help GRAPHIX -\nT5 generate more correct SQLs in terms of semantic meanings and database schema structures.\ndon‚Äôt appear in the table documents. This implies that\nvanilla T5-3B is unable to reach the target schema ele-\nments without the capability of structural grounding when\nconfronting challenging text-to-SQLs. Instead, G RAPHIX -\nT5-3B can correspond the question entities to the correct\ncolumn names through multi-hop paths presented in the\nFigure 5. In the second case, vanilla T5-3B misidentifies\nthe country as their target column, however, \"France\"\nonly appears in the column countryname of the table\ncountries. This suggests T5-3B is only able to generate\nsemantically valid SQLs, which fails to take into account\nthe real database structure. On contrary, G RAPHIX -T5 can\nproduce truly valid SQLs in terms of both questions and\ndatabases via a successful mixture of semantic and structural\ninformation during training.\n5 Related Works\nThe basic principle of a cross-domain text-to-SQL parser\nis to build an encoder to learn the representations of the\nquestions and schemas, while employing a decoder to gen-\nerate SQLs with the information learnt in the encoder (Qin\net al. 2022a). In particular, IRNET (Guo et al. 2019) pro-\nposes to design an encoder to learn the representations of\nquestions and schemas respectively via an attention-based\nBi-LSTM and a decoder to predict SQLs via encoded in-\ntermediate representations. Later, the graph-based encoders\nhave been successfully proved its effectiveness in text-to-\nSQL tasks, for example, some works (Bogin, Berant, and\nGardner 2019; Chen et al. 2021) construct the schema graph\nand enhance the representations of inputs. RATSQL (Wang\net al. 2020a), SDSQL (Hui et al. 2021b), LGESQL (Cao\net al. 2021), S2SQL (Hui et al. 2022) further improve struc-\ntural reasoning through modelling relations between schema\nand questions. R2SQL (Hui et al. 2021a), S CORE (Yu et al.\n2021) and STAR (Cai et al. 2022) enhance structural reason-\ning for context-dependent text-to-SQL parsing. These works\nare performed by the PLM independently building the se-\nmantic features, followed by the graph-based module inject-\ning the structural information. However, such training strat-\negy is just effective to encoder-based PLMs (i.e. , BERT\n(Devlin et al. 2019).\nRecently, the text-to-text PLM T5 has been proven effec-\ntiveness in text-to-SQL (Shaw et al. 2021; Qin et al. 2022c).\nBesides, (Scholak, Schucher, and Bahdanau 2021) designs\na constrained decoding process, namely PICARD, to refuse\nerroneous tokens during the beam-search phase. Xie et al.\n(2022) further injects the knowledge from other structural\nknowledge grounding tasks into T5 with multi-task to boost\nperformance on text-to-SQL. Despite effectiveness, these\nmethods still struggle to generate SQLs in the more chal-\nlenging and complex scenarios without explicit and implicit\nstructural information.\n6 Conclusion\nIn this paper, we proposed an effective architecture to boost\nthe capability of structural encoding of T5 cohesively while\nkeeping the pretrained T5‚Äôs potent contextual encoding abil-\nity. In order to achieve this goal, we designed a Graph-Aware\nsemi-pretrained text-to-text PLM, namely G RAPHIX -T5, to\naugment the multi-hop reasoning for the challenging text-\nto-SQL task. The results under the extensive experiments\ndemonstrate the effectiveness of GRAPHIX -T5, proving that\nstructural information is crucial for the current text-to-text\nPLMs for complicated text-to-SQL cases.\n13082\nAcknowledgements\nWe thank Dr. Tao Yu and Tianbao Xie for evaluation of our\nwork on SPIDER leaderboard. We thank Dr. Bailin Wang\nand Dr. Bowen Li for constructive suggestions. Reynold\nCheng, Jinyang Li, Nan Huo, and Wenyu Du were supported\nby the University of Hong Kong (Project 104006830), the\nGuangdong‚ÄìHong Kong-Macau Joint Laboratory Program\n2020 (Project No: 2020B1212030009), and the Innovation\nWing Two Research fund. Jinyang Li was also supported\nby HKU Presidential PhD Scholar Programme and Alibaba\nGroup through Alibaba Research Intern Program. Chenhao\nMa was supported in part by Shenzhen Science and Technol-\nogy Program under grant No.ZDSYS20211021111415025.\nReferences\nBogin, B.; Berant, J.; and Gardner, M. 2019. Representing\nSchema Structure with Graph Neural Networks for Text-to-\nSQL Parsing. In Proc. of ACL.\nCai, R.; Xu, B.; Zhang, Z.; Yang, X.; Li, Z.; and Liang, Z.\n2018. An Encoder-Decoder Framework Translating Natural\nLanguage to Database Queries. In Proc. of IJCAI.\nCai, R.; Yuan, J.; Xu, B.; and Hao, Z. 2021. SADGA:\nStructure-Aware Dual Graph Aggregation Network for Text-\nto-SQL. In Proc. of NeurIPS.\nCai, Z.; Li, X.; Hui, B.; Yang, M.; Li, B.; Li, B.; Cao, Z.; Li,\nW.; Huang, F.; Si, L.; and Li, Y . 2022. STAR: SQL Guided\nPre-Training for Context-dependent Text-to-SQL Parsing.\nIn Proc. of EMNLP Findings.\nCao, R.; Chen, L.; Chen, Z.; Zhao, Y .; Zhu, S.; and Yu, K.\n2021. LGESQL: Line Graph Enhanced Text-to-SQL Model\nwith Mixed Local and Non-Local Relations. In Proc. of\nACL.\nChen, D.; Lin, Y .; Li, W.; Li, P.; Zhou, J.; and Sun, X. 2020a.\nMeasuring and Relieving the Over-Smoothing Problem for\nGraph Neural Networks from the Topological View. InProc.\nof AAAI.\nChen, X.; Meng, F.; Li, P.; Chen, F.; Xu, S.; Xu, B.; and\nZhou, J. 2020b. Bridging the Gap between Prior and Pos-\nterior Knowledge Selection for Knowledge-Grounded Dia-\nlogue Generation. In Proc. of EMNLP.\nChen, Z.; Chen, L.; Zhao, Y .; Cao, R.; Xu, Z.; Zhu, S.; and\nYu, K. 2021. ShadowGNN: Graph Projection Neural Net-\nwork for Text-to-SQL Parser. In Proc. of NAACL.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proc. of NAACL.\nFang, Y .; Sun, S.; Gan, Z.; Pillai, R.; Wang, S.; and Liu, J.\n2020. Hierarchical Graph Network for Multi-hop Question\nAnswering. In Proc. of EMNLP.\nFrench, R. M. 1999. Catastrophic forgetting in connectionist\nnetworks. Trends in cognitive sciences.\nGan, Y .; Chen, X.; Huang, Q.; Purver, M.; Woodward, J. R.;\nXie, J.; and Huang, P. 2021a. Towards Robustness of Text-\nto-SQL Models against Synonym Substitution. In Proc. of\nACL.\nGan, Y .; Chen, X.; and Purver, M. 2021. Exploring Under-\nexplored Limitations of Cross-Domain Text-to-SQL Gener-\nalization. In Proc. of EMNLP.\nGan, Y .; Chen, X.; Xie, J.; Purver, M.; Woodward, J. R.;\nDrake, J.; and Zhang, Q. 2021b. Natural SQL: Making SQL\nEasier to Infer from Natural Language Specifications. In\nProc. of EMNLP Findings.\nGuo, J.; Zhan, Z.; Gao, Y .; Xiao, Y .; Lou, J.-G.; Liu, T.;\nand Zhang, D. 2019. Towards Complex Text-to-SQL in\nCross-Domain Database with Intermediate Representation.\nIn Proc. of ACL.\nHui, B.; Geng, R.; Ren, Q.; Li, B.; Li, Y .; Sun, J.; Huang, F.;\nSi, L.; Zhu, P.; and Zhu, X. 2021a. Dynamic Hybrid Rela-\ntion Network for Cross-Domain Context-Dependent Seman-\ntic Parsing. In Proc. of AAAI.\nHui, B.; Geng, R.; Wang, L.; Qin, B.; Li, Y .; Li, B.; Sun,\nJ.; and Li, Y . 2022. S 2SQL: Injecting Syntax to Question-\nSchema Interaction Graph Encoder for Text-to-SQL Parsers.\nIn Proc. of ACL Findings.\nHui, B.; Shi, X.; Geng, R.; Li, B.; Li, Y .; Sun, J.; and Zhu, X.\n2021b. Improving Text-to-SQL with Schema Dependency\nLearning. In arXiv:2103.04399.\nQin, B.; Hui, B.; Wang, L.; Yang, M.; Li, J.; Li, B.; Geng,\nR.; Cao, R.; Sun, J.; Si, L.; Huang, F.; and Li, Y . 2022a.\nA Survey on Text-to-SQL Parsing: Concepts, Methods, and\nFuture Directions. In arXiv:2208.13629.\nQin, B.; Wang, L.; Hui, B.; Geng, R.; Cao, Z.; Yang, M.;\nSun, J.; and Li, Y . 2022b. Linking-Enhanced Pre-Training\nfor Table Semantic Parsing. In arXiv:2111.09486.\nQin, B.; Wang, L.; Hui, B.; Li, B.; Wei, X.; Li, B.; Huang, F.;\nSi, L.; Yang, M.; and Li, Y . 2022c. SUN: Exploring Intrinsic\nUncertainties in Text-to-SQL Parsers. In Proc. of COLING.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Exploring\nthe Limits of Transfer Learning with a Unified Text-to-Text\nTransformer. Journal of Machine Learning Research.\nRubin, O.; and Berant, J. 2021. SmBoP: Semi-\nautoregressive Bottom-up Semantic Parsing. In Proc. of\nNAACL.\nScholak, T.; Schucher, N.; and Bahdanau, D. 2021.\nPICARD: Parsing Incrementally for Constrained Auto-\nRegressive Decoding from Language Models. In Proc. of\nEMNLP.\nShaw, P.; Chang, M.-W.; Pasupat, P.; and Toutanova, K.\n2021. Compositional Generalization and Natural Language\nVariation: Can a Semantic Parsing Approach Handle Both?\nIn Proc. of ACL.\nShazeer, N.; and Stern, M. 2018. Adafactor: Adaptive Learn-\ning Rates with Sublinear Memory Cost. In Proc. of ICML.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is All you Need. In Proc. of NeurIPS.\nVelickovic, P.; Cucurull, G.; Casanova, A.; Romero, A.; Li`o,\nP.; and Bengio, Y . 2018. Graph Attention Networks. InProc.\nof ICLR.\n13083\nWang, B.; Shin, R.; Liu, X.; Polozov, O.; and Richardson, M.\n2020a. RAT-SQL: Relation-Aware Schema Encoding and\nLinking for Text-to-SQL Parsers. In Proc. of ACL.\nWang, K.; Shen, W.; Yang, Y .; Quan, X.; and Wang, R.\n2020b. Relational Graph Attention Network for Aspect-\nbased Sentiment Analysis. In Proc. of ACL.\nWang, L.; Qin, B.; Hui, B.; Li, B.; Yang, M.; Wang, B.;\nLi, B.; Huang, F.; Si, L.; and Li, Y . 2022. Proton: Prob-\ning Schema Linking Information from Pre-trained Language\nModels for Text-to-SQL Parsing. In Proc. of KDD.\nWolf, T.; Debut, L.; Sanh, V .; Chaumond, J.; Delangue, C.;\nMoi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davi-\nson, J.; Shleifer, S.; von Platen, P.; Ma, C.; Jernite, Y .; Plu,\nJ.; Xu, C.; Le Scao, T.; Gugger, S.; Drame, M.; Lhoest, Q.;\nand Rush, A. 2020. Transformers: State-of-the-Art Natural\nLanguage Processing. In Proc. of EMNLP.\nXie, T.; Wu, C. H.; Shi, P.; Zhong, R.; Scholak, T.; Yasunaga,\nM.; Wu, C.-S.; Zhong, M.; Yin, P.; Wang, S. I.; Zhong, V .;\nWang, B.; Li, C.; Boyle, C.; Ni, A.; Yao, Z.; Radev, D.;\nXiong, C.; Kong, L.; Zhang, R.; Smith, N. A.; Zettlemoyer,\nL.; and Yu, T. 2022. UnifiedSKG: Unifying and Multi-\nTasking Structured Knowledge Grounding with Text-to-Text\nLanguage Models. ArXiv preprint.\nXu, X.; Liu, C.; and Song, D. 2017. Sqlnet: Generating\nstructured queries from natural language without reinforce-\nment learning. ArXiv preprint.\nYaghmazadeh, N.; Wang, Y .; Dillig, I.; and Dillig, T. 2017.\nSQLizer: query synthesis from natural language. Proceed-\nings of the ACM on Programming Languages.\nYu, T.; Li, Z.; Zhang, Z.; Zhang, R.; and Radev, D. 2018a.\nTypeSQL: Knowledge-Based Type-Aware Neural Text-to-\nSQL Generation. In Proc. of NAACL.\nYu, T.; Zhang, R.; Polozov, A.; Meek, C.; and Awadallah,\nA., Hassan. 2021. SCoRe: Pre-Training for Context Repre-\nsentation in Conversational Semantic Parsing. In Proc. of\nICLR.\nYu, T.; Zhang, R.; Yang, K.; Yasunaga, M.; Wang, D.; Li, Z.;\nMa, J.; Li, I.; Yao, Q.; Roman, S.; Zhang, Z.; and Radev, D.\n2018b. Spider: A Large-Scale Human-Labeled Dataset for\nComplex and Cross-Domain Semantic Parsing and Text-to-\nSQL Task. In Proc. of EMNLP.\nZelle, J. M.; and Mooney, R. J. 1996. Learning to Parse\nDatabase Queries Using Inductive Logic Programming. In\nProc. of AAAI.\nZhong, V .; Lewis, M.; Wang, S. I.; and Zettlemoyer, L. 2020.\nGrounded Adaptation for Zero-shot Executable Semantic\nParsing. In Proc. of EMNLP.\n13084",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.855664849281311
    },
    {
      "name": "Parsing",
      "score": 0.7213515043258667
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6265484690666199
    },
    {
      "name": "SQL",
      "score": 0.5931534171104431
    },
    {
      "name": "Natural language processing",
      "score": 0.5653924942016602
    },
    {
      "name": "Programming language",
      "score": 0.47627848386764526
    },
    {
      "name": "Bottom-up parsing",
      "score": 0.4236557185649872
    },
    {
      "name": "Top-down parsing",
      "score": 0.295529842376709
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I45928872",
      "name": "Alibaba Group (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I4210086143",
      "name": "Alibaba Group (Cayman Islands)",
      "country": "KY"
    },
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210145761",
      "name": "Shenzhen Institutes of Advanced Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210116924",
      "name": "Chinese University of Hong Kong, Shenzhen",
      "country": "CN"
    }
  ],
  "cited_by": 60
}