{
  "title": "Knowledge-Driven Feature Selection and Engineering for Genotype Data with Large Language Models.",
  "url": "https://openalex.org/W4403852132",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2099906469",
      "name": "Joseph Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103132892",
      "name": "Shu Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3135427163",
      "name": "Jae Young Baik",
      "affiliations": [
        "United States Naval Academy",
        "University of North Carolina at Chapel Hill",
        "Arizona State University",
        "RIKEN Center for Integrative Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2116253576",
      "name": "Xiaoxi Liu",
      "affiliations": [
        "RIKEN Center for Integrative Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2102304230",
      "name": "Zhen Tan",
      "affiliations": [
        "Arizona State University"
      ]
    },
    {
      "id": "https://openalex.org/A2149064293",
      "name": "Dawei Li",
      "affiliations": [
        "Arizona State University"
      ]
    },
    {
      "id": "https://openalex.org/A2899872995",
      "name": "Zixuan Wen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2626217422",
      "name": "Bojian Hou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2552287035",
      "name": "Duy Duong‐Tran",
      "affiliations": [
        "United States Naval Academy"
      ]
    },
    {
      "id": "https://openalex.org/A2109104503",
      "name": "Tianlong Chen",
      "affiliations": [
        "University of North Carolina at Chapel Hill"
      ]
    },
    {
      "id": "https://openalex.org/A1898662817",
      "name": "Li Shen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4311256635",
    "https://openalex.org/W4414452742",
    "https://openalex.org/W4387665659",
    "https://openalex.org/W2805627121",
    "https://openalex.org/W4404764563",
    "https://openalex.org/W4293785792",
    "https://openalex.org/W4283593236",
    "https://openalex.org/W3181344512",
    "https://openalex.org/W2978740330",
    "https://openalex.org/W4393153069",
    "https://openalex.org/W3174446981",
    "https://openalex.org/W4366563389",
    "https://openalex.org/W2803332630",
    "https://openalex.org/W4412692453",
    "https://openalex.org/W2238337252",
    "https://openalex.org/W2067328319",
    "https://openalex.org/W4384071683"
  ],
  "abstract": "Predicting phenotypes with complex genetic bases based on a small, interpretable set of variant features remains a challenging task. Conventionally, data-driven approaches are used for this task, yet the high dimensional nature of genotype data makes the analysis and prediction difficult. Motivated by the biomedical knowledge encoded in pre-trained LLMs and the emerging applications for genetics, we set to examine the ability of LLMs in feature selection and engineering for tabular genotype data, with a novel knowledge-driven framework. We develop FREEFORM, Free-flow Reasoning and Ensembling for Enhanced Feature Output and Robust Modeling, designed with chain-of-thought and ensembling principles, to select and engineer features with the intrinsic knowledge of LLMs. Evaluated on two distinct genotype-phenotype datasets, genetic ancestry and hereditary hearing loss, we find this framework outperforms several data-driven methods, particularly on low-data regimes. FREEFORM is available as open-source framework at GitHub: https://github.com/PennShenLab/FREEFORM.",
  "full_text": "Knowledge-Driven Feature Selection and Engineering for Genotype Data with\nLarge Language Models\nJoseph Lee, BS1,∗, Shu Yang, PhD1,∗,†, Jae Young Baik, Xiaoxi Liu, PhD2, Zhen Tan, MS3,\nDawei Li, MS3, Zixuan Wen, MA1, Bojian Hou, PhD1, Duy Duong-Tran, PhD4,\nTianlong Chen, PhD5, Li Shen, PhD1,†\n1 Unversity of Pennsylvania, Philadelphia, USA. 2 RIKEN, Yokohama, Japan. 3 Arizona\nState University, Tempe, USA.4 United States Naval Academy, Annapolis, USA. 5The\nUniversity of North Carolina at Chapel Hill, Chapel Hill, USA\nAbstract\nPredicting phenotypes with complex genetic bases based on a small, interpretable set of variant features remains a\nchallenging task. Conventionally, data-driven approaches are used for this task, yet the high dimensional nature of\ngenotype data makes the analysis and prediction difficult. Motivated by the biomedical knowledge encoded in pre-\ntrained LLMs and the emerging applications for genetics, we set to examine the ability of LLMs in feature selection\nand engineering for tabular genotype data, with a novel knowledge-driven framework. We develop FREE FORM ,\nFree-flow Reasoning and E nsembling for E nhanced Feature Output and R obust Modeling, designed with chain-of-\nthought and ensembling principles, to select and engineer features with the intrinsic knowledge of LLMs. Evaluated\non two distinct genotype-phenotype datasets, genetic ancestry and hereditary hearing loss, we find this framework\noutperforms several data-driven methods, particularly on low-data regimes. FREE FORM is available as open-source\nframework at GitHub: https://github.com/PennShenLab/FREEFORM.\nIntroduction\nPredicting observable phenotypes from genotype data has proven to be a monumental task in the field of genetics, with\ndiverse applications ranging from personalized medicine 1 to genomic selection of crops 2. Vast amounts of genetic\nvariants, such as single nucleotide polymorphisms (SNPs) from high-throughput sequencing data, are often harnessed\nto make predictions on the phenotype. To deal with the sparsity of this data, linear models with regularization have\nbeen developed to much success in the case of polygenic risk scores 1,3. Furthermore, machine learning models have\nbeen developed to capture the entangled epistatic relationships between genes and to predict complex traits4.\nThe difficulty, however, of modeling genotype data is substantial: (1) First, the topic of interest is not merely predictive\n(e.g. discovering causal variants5) and machine learning methods are known to identify spurious features as significant\ndue to multicollinearity6. Analyzing interaction terms—like epistatic relationships between variants—also introduces\nchallenges such as a lack of interpretability when using complex models or multiple testing when examining higher-\norder interactions 7. (2) Furthermore, a table of genotype data could contain thousands or even millions of columns.\nThis ‘curse of dimensionality’ can cause severe overfitting and amplify many issues, including those we’ve named\nabove8. (3) Lastly, data can be limited in real settings, exacerbating existing concerns with overfitting.\nFeature selection and feature engineering (i.e. feature construction) can be crucial steps to mitigate these concerns.\nData-driven methods, such as Lasso regression, have demonstrated great success in selecting features 9. Feature en-\ngineering improves predictive performance without resorting to complex models and can help uncover interactions\nbetween features 10. However, these methods have their own issues: data-driven methods can struggle with small\nsample sizes and feature engineering is a laborious process that requires expertise to avoid multiple testing.\nRecent advances in large language models (LLMs) have shown promise in addressing these challenges. With their\nremarkable performance across various tasks, LLMs have established themselves as powerful tools across many do-\nmains11,12. A key strength of LLMs lies in the knowledge they acquire through pre-training, enabling them to act\nas domain experts; recent LLMs have showed extensive understanding of biomedical concepts 13,14,15. Furthermore,\nLLMs can be greatly enhanced with well-designed prompting strategies16,17,18. Chain-of-thought prompting (CoT) 16\nimproves the reasoning of LLMs by encouraging step-by-step problem solving. Self-consistency17 addresses the naive\ngreedy decoding used in CoT prompting by selecting the most consistent outcome across multiple reasoning paths.\n∗Equal contribution by JL and SY .†Correspondence: shu.yang@pennmedicine.upenn.edu (SY) and li.shen@pennmedicine.upenn.edu (LS).\narXiv:2410.01795v2  [cs.LG]  16 Apr 2025\nSeveral studies have proposed to employ such capabilities of LLMs to perform feature preprocessing. Among them,\nChoi et al. 19 adopts the notion of prior knowledge in LLMs to conduct feature selection and casual discovery. Jeong\net al. 20 examines three types of selection strategies with LLMs e.g. ranking vs. scoring features. Hollmann et al. 21\nhas an agentic approach, using the LLM to generate Python code that creates features in an iterative fashion based on\ncross-validation feedback. Han et al.22 employs LLMs to do feature engineering, generating conditional rules for each\nclass label (e.g. age > 21 increases the logits for label 0) and repeating this several times to form an ensemble.\nDue to the promising performance of LLMs, there has also been a growing exploration of LLMs’ usage in biomedical\napplications, especially genetics23,15,24,25. Despite these innovations, most genetic studies have focused on gene-level\ndata, likely due to the limitations of earlier LLMs. To our knowledge, no prior work has explored LLMs on variant-\nlevel data, except one that used API calls on NCBI databases to retrieve SNP information 26. While this approach\ncompensates for LLMs’ inability to pass the GeneTuring benchmark27, we argue the benchmark is an inadequate test\nof their utility, as it evaluates gene-SNP association by randomly sampling 100 SNPs from hundreds of millions. This\nsetup overlooks how well LLMs leverage their knowledge of known variants (Fig. 5), a gap we address in this study.\nInspired by these recent explorations 20,22,23,28, we propose to leverage LLMs’ knowledge, intrinsic and augmented,\nand reasoning capabilities16,29 to select the most informative genetic variants and generate novel features. We develop\na knowledge-driven framework FREE FORM , Free-flow Reasoning and Ensembling for Enhanced Feature Output and\nRobust Modeling, designed from ground principles in ensembling (self-consistency) 17 and “free-flow reasoning” 30\nto best leverage the expertise of LLMs. In the LLM-enabled framework, we implement scalable feature selection\nstrategies that can process a large number of variants and feature engineering approaches that focus on interaction\nterms which are more interpretable. We evaluate F REE FORM on two real genotype datasets, genetic ancestry and\nhereditary hearing loss, and compare with data-driven and LLM-enabled methods. In particular, we focus on few-shot\n(i.e. few data samples) settings where data-driven methods struggle due to limited sample size but LLMs have shown\nsurprising generalizability 31,32. In our study, we expect challenges when applying LLMs to tabular genotype data\ndue to their limited semantics—column names are variant IDs and values indicate the number of minor allele copies\n(0,1,2). Thus, we analyze the effect of retrieval augmentation and domain-specific serialization when prompting.\nOur results highlight F REE FORM ’s potential to address three challenges present in modelling genotype data: Our\nmethod (1) enhances prediction while upholding interpretability, (2) reduces the dimensionality of the dataset, and\n(3) excels in low-shot regimes compared to data-driven methods by grounding ours in knowledge. Furthermore, we\nchallenge the notion that LLMs lack knowledge of genetic variants by novelly applying them in this domain.\nMethods\nIn this section, we introduce our FREE FORM framework (Fig. 1), designed to address the challenges of training models\non genotype data. The framework is built on two components: (1) leveraging the knowledge of LLMs to select a set\nof features and (2) leveraging the knowledge of LLMs to engineer new features from the selected features.\nFeature Selection\nThe genotype dataset can be formalized asD = {(xi, yi)}N\ni=1, i.e. a table comprising of N labeled samples (i.e. rows)\neach with d variants (i.e. columns). Each entry xi\nj ∈ {0, 1, 2} represents the number of minor allele copies, while the\ncorresponding label yi denotes the phenotype, e.g. yi ∈ {“African”, “American”, “East Asian”, “European”, “South\nAsian”}. The column names, denoted by S = {sj}d\nj=1, are text strings representing variants such as rsIDs.\nThe goal of feature selection is to identify a subset S′ ⊆ S with d′ ≪ d variants such that a downstream model\nf, trained on DS′ = {(xi\nS′ , yi)}N\ni=1, can make efficient, interpretable predictions. We use an LLM, modelled as a\nstochastic operator T parameterized by θ and given prompt p, to output a selected subset of features S′:\nS′ = T (S, p; θ), (1)\nNotably, the proposed selection method is model agnostic unlike model-based rankings. Furthermore, previous LLM-\nbased feature selection methods have not been tested in the setting of variants-level data where hallucination can be\nmore likely33 and high dimensionality is an issue. While context windows have become longer, new challenges have\nemerged34,35 in which naive usage of the context would be ineffective; to address these challenges, we design our\nTabular Genotype Data\nVariant1Variant2Variant3...Phenotype\n0 0 0 A\n2 1 1 B\n1 0 1 A\n0 1 1 C\n1 0 1 ?\nClassifier 1\nB\nBaggingLLM-driven feature engineering\nClassifier N\nClassifier 2\nEnsembled \nPrediction\n... \nTransformed Datasets \nSelected features\n<Variant 1>, ...,\n<Variant d'>\nLLM-driven feature selection\nA. Self-Consistent Hierarchical Selection \nPartition Randomly\nRepeat StartSelect d'StartStartSelect d'StartStartSelect d'\nd' d' d'\nB. Self-Consistent Sequential\nForward Selection \n... \n Variant 1 * Variant 2\n Variant 2 > 0\n Variant 4 + Variant 5... ... \n... \nEngineered Features\n Variant 1 * Variant 2\n Variant 2 > 0\n Variant 4 + Variant 5 + ...\nSelected FeaturesEngineered\nFeatures\nTask Description: ...\nFeature Names: ...\nDetailed Instructions: ...\nAssetsAssetsAssets\nIs the genetic\nvariant <Variant> \nrelevant for <Task>?\nIs the genetic\nvariant <Variant> \nrelevant for <Task>?\nIs the genetic\nvariant <Variant> \nrelevant for <Task>?\nSelect one & remove\nWhat's the most signifcant\nvariant?\nRepeat\nFigure 1: Overview of the F REE FORM framework. The pipeline consists of two parts: (1) LLM-driven feature selec-\ntion takes d variants and selects d′ of them (2) Given the selected features, we use LLMs to generate sets of engineered\nfeatures to create an ensemble of classifiers.\nfeature selection strategies to scale for high dimensions while remaining token-efficient. In the downstream analysis,\nwe train two models, Random Forest and Logistic Regression, on the selected features to evaluate their quality.\nRelevance Filtering We first ask the LLM to determine whether each of thed variants is relevant to the task, request-\ning a simple “Yes” or “No”, yielding a subset of variants. This set may still be large, so we adjust the language of the\nprompt appropriately (e.g. “potentially relevant”) based on how many are filtered.\nSelf-Consistent Hierarchical Selection is the first strategy we employ for selecting d′ features from the filtered\nset of variants. We begin by randomly partitioning the variants into buckets of approximately 50 to 100 variants (a\nhyperparameter) to prevent the loss of information when contexts become too large 34. Each bucket is independently\npassed to the LLM, tasked with selecting the d′ most relevant variants; we select d′ at every step in the case that the\nrelevant features are concentrated in a single bucket. The d′ selected variants from each bucket are merged together,\nand the process repeats as delineated in Fig. 1. We observe that the selection process is sensitive to the order in which\nfeatures are presented to the LLM. Thus, for each bucket, we conduct multiple iterations, randomizing the order of\nvariants and using a temperature of 0.3. This approach naturally integrates self-consistency 17 by retaining the top d′\nvariants that appear the most across iterations. During the final selection, we enhance the LLM with chain-of-thought\n(CoT) prompting, increase the temperature to 0.7, and increase the number of iterations to ten.\nSelf-Consistent Sequential Forward Selection is the second strategy we employ. Starting with the filtered set of\nvariants, we task the LLM with identifying the single most relevant variant with CoT. After each selection, the chosen\nvariant is removed and the process repeats. Initially, we perform this extraction without any ensembling; the top few\nfeatures are easy for the LLM to identify, but the task becomes more challenging as it becomes ambiguous which\nvariants are more significant. After selecting a few, we start to apply self-consistency: repeating the extraction several\ntimes, in increasing amounts as we near the end of the selection. We find that this also mitigates the LLM’s tendency\nto return the features that were already selected. In cases where the LLM still fails to identify a valid variant, which\noccurs frequently towards the end, we implement an exception handling mechanism that retries the selection process.\nFeature Engineering\nGiven the selected features S′, our goal is to engineer new features that capture meaningful relationships the model\nmight not identify on its own (e.g., household density = family size ÷ number of rooms). We automate this tradition-\nally manual, expert-driven process by leveraging the knowledge and reasoning capabilities of LLMs. As outlined in\nFig. 1, we repeat this several times and train a model on each feature set, forming an ensemble.\nFormally, we transform our dataset DS′ into K transformed datasets Dk = {(xi\nS′\nk\n, yi)}N\ni=1, where S′\nk ⊇ S′. Each\ntransformed dataset is created by prompting the LLMT with p′, which includes a serialized representation of selected\nexamples R = {(xi\nS′ , yi)}i∈R⊆DS′ , to enhance the LLM with context 31. We define this function Serialize to\nconvert each row to a textual description (e.g. “The s1 variant of the person has xi\n1 minor alleles... The sd′ variant\nof the person has xi\nd′ minor alleles.”) as LLMs usually prefer natural language. Since LLMs can be sensitive to the\ninput, we examine various serialization templates and prompts. To address gaps in the LLM’s prior knowledge, we\nalso explore retrieval-augmentation to supplement the variant IDs with its associated gene36.\nThe LLM T outputs the new features that are added back onto S′ to create S′\nk:\nS′\nk = S′ ∪ T(S′, p′; θ) , where p′ includes {Serialize(xi\nS′ , yi, S′)}i∈R (2)\nWe then train K models, F = {fk}K\nk=1, each on a dataset Dk, aiming to capture different hypotheses about how the\nvariants relate to the phenotype. During inference, for input x, we average the class probabilities p(fk(x)) from each\nmodel fk. The final prediction ˆy is made by selecting the class i with the highest averaged probability:\np∗(x) = 1\nK\nKX\nk=1\np(fk(x)),\nˆy = arg max\ni\np∗\ni (x).\n(3)\nOur aim is to harness the diverse feature representations generated by the LLM, reducing the risk of overfitting in low-\nshot settings by anchoring each feature set in the knowledge embedded within LLMs rather than the limited data. To\nuphold interpretability, we limit our feature construction to interaction terms. Notably, our method is model-agnostic,\nde-coupled from the classifier. We train two models, Random Forest and Logistic Regression, on the transformed\ndatasets to evaluate the quality of the constructed features. We discuss key steps of our method in depth below.\nAutomating Feature Engineering When asking the LLM to engineer features, we provide a comprehensive prompt\np′ that includes the following components:\n• Instructions: Directions to use the provided features to engineer new features relevant to the task.\n• Task Description: A concise description of the specific task for which the features are being engineered.\n• Features: A list of features including the name of each genetic variant.\n• Examples: |R| examples that illustrate the data in a serialized format.\n• Detailed Instruction: List of specific choices for feature engineering, such as multiplying or adding features, accom-\npanied by a task-specific demonstration as seen in Fig 2.\n• Step-by-Step Solution: Directions for the LLM to solve the problem step by step.\n# Detailed Instructions\nBased on the above examples and your extensive knowledge, engineer\na few features with some of the features listed above. Consider the \nfollowing actions for feature engineering for each [Feature_name]:\n- [Feature_name] is in [list of Categorical_values]\n- [Feature_name] (> or >= or < or <=) [Numerical_value]\n- [Feature1_name] + [Feature2_name] if you believe there is an additive effect\n- [Feature1_name] * [Feature2_name] if you believe there is a multiplicative effect \n- [Condition with Feature 1] AND [Condition with Feature 2] if you believe there's a \ntree-like interaction \n- Feel free to engineer other complex, creative interactions between features\nHere's a demonstration of how one might do feature engineering.\n- GJB2 Mutation (c.35delG): The GJB2 gene encodes the Connexin 26 protein, \nwhich is crucial for cell-to-cell communication in the cochlea of the inner ear. The \nc.35delG mutation leads to a nonfunctional protein, disrupting this communication \nand causing hearing loss.\n- SLC26A4 Mutation (c.919-2A>G): The SLC26A4 gene encodes the Pendrin \nprotein, which is involved in ion transport in the inner ear. The c.919-2A>G mutation \nleads to improper splicing and a dysfunctional protein, contributing to hearing loss.\n- Analysis of Synergistic Effect: ... The combined effect of these mutations \ndisrupts multiple pathways critical for auditory function. The GJB2 mutation affects \ngap junction communication, while the SLC26A4 mutation impairs ion transport. \nTogether, these disruptions can lead to more profound hearing loss compared to \nhaving a mutation in only one of these genes.\n- Output: combined_count_c.35delG_c.919-2A>G = c.35delG + c.919-2A>G\nFigure 2: Example of Detailed Instructions\nFree-Flow Reasoning Traditionally, we enforce the output of the LLM to be structured e.g. JSON. However, we\ndeliberately allow the LLM to freely generate its response, recognizing that enforcing a rigid structure can diminish\nthe depth and quality of the LLM’s CoT reasoning 30. To guide the LLM, we provide an example of a “correctly”\nengineered feature37, which we self-generate with an LLM and then manually verify.\nSelf-Parsing and Function Writing via LLMs The unstructured output generated by the LLM can present chal-\nlenges for parsing. To address this, we employ the LLM itself to extract the engineered features from its output, listing\nthem line by line for easy parsing. Subsequently, we task the LLM with writing an executable Python function that\nwill generate the new columns of dataset Dk based on the extracted features. When errors are detected, we implement\nerror-handling mechanisms to catch any issues and they are fed back into the LLM which rewrites the function.\nEnsembling To mitigate overfitting, we repeat the feature engineering processK times to generate allDk which we’ll\nuse to train K classifiers. Similar to F EATLLM 22, we incorporate additional ensembling and order bias mitigation\nstrategies such as bagging and order shuffling, where we pass a random subset of |R| ≤N samples to the LLM;\nwe limit the number of samples |R| to 16. We do this to further diversify the LLM output for each iteration, and\nsimultaneously avoid exorbitant usage of the context window. Our free-flow reasoning approach further contributes to\nthe diversity of the output, as we naturally allow the LLM to determine the number of features that will be constructed\nduring its generation. By setting a non-zero temperature of 1, we ensure that each iteration further produces a varied\nset of features, especially in type. The resulting k transformed datasets form the basis for the ensembled model.\nDue to the page limit, we direct readers to our GitHub repo, which contains the detailed prompts and relevant hyper-\nparameters used in the entire pipeline. The source code of F REE FORM is provided to promote reproducibility.\nResults\nExperiment Setup\nDatasets Our experiments involve two datasets: the Genomic Ancestry Dataset 38 and the Hereditary Hearing\nLoss Dataset 39. The Genomic Ancestry Dataset is derived from the 1000 Genomes Project (1KGP). We focus on\ndetermining the superpopulation ancestry phenotype (African, American, East Asian, European, and South Asian).\nIn particular, we used a curated set of 10,000 SNPs predefined by GRAF to pinpoint ancestry markers 40, and we’ll\ndiscuss how we addressed the issues that arise from their quality control (QC). After QC and preprocessing, the dataset\nincludes 2,403 subjects and 8,688 variants as columns in the rsID format, a standard identifier used by dbSNP.\nThe Hereditary Hearing Loss Dataset is considerably smaller, comprising of 1,209 subjects and 144 variants as\ncolumns, employing the HGVS nomenclature system. This dataset is notably imbalanced, with approximately 75.9%\nof the samples classified as “Yes” (indicating the presence of hereditary hearing loss) and 24.1% as “No.” To our best\nknowledge, these two are the only open-access genotype datasets available online.\nBaselines For feature selection, we conduct comparisons with four baseline methods. The first three baselines are\nconventional machine learning approaches: (1) LASSO, (2) PCA and (3) RF-based Gini Importance where we fit\na Random Forest on the training data and rank the features by their Gini importance. In our case, the specification of\na fixed number of features is required for our study. LASSO, however, reduces an arbitrary number of coefficients to\nzero during model training. Similarly, PCA provides loadings along principal axes and has been widely used in the\ngenetic field to select variants. To adapt these methods to our requirements, we select the d′ features with the largest\ncoefficients in the highest-performing LASSO model. For PCA, we take the top d′ loadings of the first principle\ncomponent. Lastly, we include (4) LLM-S ELECT 20 (we use their LLM-RANK prompts).\nFor feature engineering, we compared our approach against five baselines. The first three are traditional machine\nlearning methods: (1) Logistic Regression, (2) Random Forest and (3) XGBoost. We also include recent baselines,\n(4) TabPFN41, a foundation model for tabular data, and (5) FeatLLM22 which also leverages LLMs to do feature\nengineering but limits themselves to conditional rules (e.g. variant 1 is > 0) for a linear model.\nImplementation Details Our FREE FORM framework utilizes GPT-4o (2024-05-13) as the primary LLM back-\nbone, particularly for tasks requiring advanced reasoning capabilities, such as automating feature engineering and\nselecting relevant features. For more routine tasks, including parsing output and writing Python functions, we employ\nGPT-3.5-turbo which offers a cost-effective solution that meets the performance requirements for these functions.\nThese models are called upon using the OpenAI API which only requires internet access.\nFor feature engineering, we employ an ensemble of K = 20 models, striking a balance between cost-effectiveness\nand model performance, noting that performance gains diminish beyond this point. In replicating baselines, machine\nlearning models were implemented using Python’s scikit-learn library. Hyperparameters were optimized using\ngrid search and k-fold cross-validation, with k set to either 2 or 4, ensuring that the training set includes at least one\nexample of each class. For other methods, such as F EATLLM and T ABPFN, we used default parameters with slight\nadjustments for fair comparison (e.g., using 15 conditions instead of 10 for FEATLLM). Also, for the evaluation of all\nfeature engineering methods, we choose one of the feature sets generated by hierarchical selection.\nMain Results\nIn Fig 3, we compare the performance of our feature selection methods against baselines using Logistic Regression\nand Random Forest as downstream classifiers. For feature selection, we repeat our experiments five times with cross-\nvalidation, limiting the evaluation to few-shot settings where N ≤ 320 for ancestry and N ≤ 128 for hearing loss.\nFor data-driven methods, we use the training data (varying the size of N) to perform feature selection and train the\nclassifier on the same training data. We also emphasize that, unlike the data-driven methods, the LLM-driven methods\nperform feature selection without relying on any data samples, leveraging only the model’s prior knowledge. Their\nusage of the training data is limited to training the downstream classifier.\n10 20 40 80 160 320\nNumber of Shots\n0.75\n0.80\n0.85\n0.90\n0.95AUROC\nAncestry, Logistic Regression\n10 20 40 80 160 320\nNumber of Shots\n0.75\n0.80\n0.85\n0.90\n0.95\nAncestry, Random Forest\n4 8 16 32 64 128\nNumber of Shots\n0.48\n0.50\n0.52\n0.54\n0.56\n0.58AUROC\nHearing Loss, Logistic Regression\n4 8 16 32 64 128\nNumber of Shots\n0.50\n0.55\n0.60\n0.65\n0.70\nHearing Loss, Random Forest\nLasso\nPCA\nRF Gini Importance\nLLM-Select\nFREEFORM: Forward\nFREEFORM: Hierarchical\nFigure 3: Evaluation of Feature Selection on Ancestry and Hearing Loss\nOur findings indicate that LLM-driven methods significantly outperform data-driven approaches for feature selection\nin low-shot regimes, achieving gains of up to approximately 20%. Notably, in the genomic ancestry task, LASSO\nrequires 80 shots to achieve similar results to what our framework achieves with just 10 shots. In hearing loss, we\nobserve that the performance gap is smaller and our advantage remains until 16 shots, when using Random Forests.\nThis discrepancy is likely due to the limited presence of variants within the dataset that the LLM has knowledge of.\nFor feature engineering, we conduct our experiments five times with cross-validation, limiting the evaluation to few-\nshot settings where N < 120. In Fig 4, F REE FORM consistently consistently ranks at or above baseline models. In\nthe genomic ancestry task, our framework improves the performance of both Logistic Regression and Random Forest,\nespecially in the lower-shot scenarios, and outperforms recent models such as FEATLLM and T ABPFN. However, as\nthe number of shots approaches 80, the gap between our methods and the baselines decreases. In the hearing loss task,\nour framework notably enhances the performance of Logistic Regression on higher-shot scenarios. While Random\nForest does not benefit from the engineered features, our approach remains competitive. This outcome suggests\nlimited effectiveness of the interaction terms we engineer for complex models like Random Forests.\n10 20 40 80\nNumber of Shots\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975AUROC\nAncestry\n4 8 16 32 64 128\nNumber of Shots\n0.475\n0.500\n0.525\n0.550\n0.575\n0.600\n0.625\n0.650\nHearing Loss\nLogistic Regression\nRandom Forest\nXGBoost\nT abPFN\nFeatLLM\nFREEFORM w/ Logistic Regression\nFREEFORM w/ Random Forest\nFigure 4: Evaluation of Feature Engineering on Ancestry and Hearing Loss\nAblations and Analysis\nOpen Source Models In Table 1, we examine the performance of various LLMs in our framework, repeating the\nexperiment five times with cross-validation. For feature selection, gpt-4o performs better as expected, except for\nhearing loss, further suggesting limited knowledge of the relevant variants across the LLMs. It remains unclear whether\nthis advantage arises from the LLMs’ ability to identify relevant features or their depth of knowledge (e.g. while an\nLLM may recognize a variant, its expertise regarding that variant may vary). For feature engineering, the weaker\nmodels are highly competitive. This is surprising; this may indicate that the interaction terms we enable (e.g., con-\nditional rules, multiplicative expressions) are within the reasoning capabilities of all studied models. Alternatively, it\ncould reflect all the models’ inability to capture epistasis. Either way, these results align with the shared awareness of\nvariants among models (see Figure 5), suggesting that even basic knowledge can enable LLMs to act as weak learners.\nTable 1: FREE FORM using different models. For feature selection, hierarchical selection is used across all models. We\nlimit the analysis to 16-shots (Hearing Loss) and 20-shots (Ancestry). LR: Logistic Regression. RF: Random Forest.\nValues shown are AUROC (AUC). Standard deviation is in parenthesis.\nModel Feature Selection Feature Engineering\nHearing Ancestry Hearing Ancestry\nLR RF LR RF LR RF LR RF\nGPT-3.5-turbo 0.524 (0.03) 0.539 (0.04) 0.785 (0.03) 0.765 (0.04) 0.506 (0.10) 0.540 (0.11) 0.955 (0.01) 0.951 (0.01)\nLlama-3.1-405B — — — — 0.511 (0.11) 0.544 (0.11) 0.956 (0.02) 0.952 (0.01)\nGPT-4o 0.506 (0.09) 0.544 (0.10) 0.943 (0.01) 0.943 (0.02) 0.514 (0.11) 0.543 (0.11) 0.957 (0.01) 0.953 (0.01)\nAugmentation and Serialization In our main results of F REE FORM , we use a simple serialization strategy like the\nfollowing: “s1 is xi\n1. s2 is xi\n2... Answer: yi”. In this ablation study, we find that using a more elaborate schema like\n“The s1 variant of the person has xi\n1 minor alleles ...” does not make a difference. While this may be surprising given\nthe existing efforts on exploring serialization strategies 42, large foundation models may be robust to such formatting.\nFurthermore, our findings show that the augmentation of gene information is not significant. Our experimentation is\nlimited to providing the genes associated with the variant. This may be redundant information for the LLM but other\nstrategies are not straightforward; augmenting literature for a variant is challenging due to the lack of relevance it may\nhave to the task. This will be an important avenue to explore for future work.\nFeature Nomination The genomic ancestry dataset we used is a curated version of the full dataset, after applying\nrigorous quality control. While this ensures data reliability, it can lead to the omission of significant genetic variants.\nTo address this concern, we ask GPT-4o to suggest fifteen SNPs; SNPs such as rs671 or rs2814778 with clear causal\nrelationships or statuses as standard AIMs were usually suggested. These SNPs originally existed in the database\nTable 2: FREE FORM using genotype-specific strategies for featuring engineering on Hearing Loss (16-shot) and An-\ncestry (20-shot). LR: Logistic Regression. RF: Random Forest.\nConfiguration LR AUC (Std) RF AUC (Std)\nHearing Loss\nFreeForm: Feature Engineering 0.5145 (0.11) 0.5438 (0.11)\n+ Genotype Serialization 0.5127 (0.11) 0.5384 (0.11)\n+ Gene Augmentation 0.5110 (0.09) 0.5387 (0.11)\nAncestry\nFreeForm: Feature Engineering 0.9572 (0.01) 0.9527 (0.01)\n+ Genotype Serialization 0.9568 (0.01) 0.9532 (0.01)\n+ Gene Augmentation 0.9571 (0.01) 0.9530 (0.01)\nbut were omitted in the curation of the 10K version so we inserted them back. Our LLM-driven selection methods\nwere able to recover most of these variants while the data-driven methods could not, and we found that these variants\ncontributed largely to the performance gap we observed for feature selection in the genomic ancestry dataset. We\nfind it concerning that data-driven methods failed to identify many of these variants despite their predictive power. In\nadditional analysis, training a Logistic Regression model with theGPT-4o-suggested SNPs alone achieved an average\nof 0.94 AUC on 20 shots whereas a set of fifteen SNPs selected by PCA on the original dataset achieved 0.78 AUC.\nLLMs could be a promising method to mitigate the issues of quality control, a frequent issue in this domain, providing\na potentially more robust, automatable method than manually imputing well-known variants from the literature.\nQ: Is rs671 relevant to genomic\nancestry?\nA: Yes, this SNP is located in the ALDH2\ngene, which encodes the enzymealdehyde dehydrogenase 2.  This variantis particularly common in East Asian\npopulations, including Chinese, Japanese,and Korean individuals, but is rare in otherpopulations. \nGPT-4o\nA: The SNP rs671 is located in a region of\nthe genome that is otherwise functionallyand evolutionary unremarkable. However,\nit has been found to be associated withvariation in the timing and intensity of lightskin pigmentation in human populations,\nparticularly in populations from Africa andAsia. The T allele of rs671 is associatedwith the expression of lighter skin\npigmentation, while the C allele isassociated with darker skin pigmentation.\nLlama 2 7B Llama 3.1 8B\nA: Yes, this SNP is located in the ALDH2gene, which codes for the enzyme\naldehyde dehydrogenase 2. The presenceof the A allele at rs671 is associated with\ndecreased enzyme activity, leading to abuildup of acetaldehyde in the body afteralcohol consumption. The rs671 SNP is\nparticularly relevant to genomic ancestrybecause it is more common in individualsof East Asian descent, with up to 40-50%\nof East Asians carrying the A allelecompared to only 5-10% of individuals of\nEuropean descent. \nGPT-3.5-Turbo\nA: The rs671 variant is closely linked to\nalcohol misuse and alcoholic liver disease,particularly in populations of East Asiandescent. It is associated with a genetic\nmarker known as ALDH2, which codes foraldehyde dehydrogenase 2. People who\nare homozygous for the mutated variant ofALDH2 (often referred to as the\"ALDH2*2\" allele) that corresponds to the\nrs671 marker have a significant reductionin their ability to metabolize acetaldehyde,a toxic byproduct of alcohol.\nFigure 5: Comparison between different LLM models on their knowledge of the SNP rs671 relating to genomic\nancestry. Red text indicates a hallucination, which was only observed in the case of the Llama 2 7B model.\nDiscussion\nWe present FREE FORM which advances the state-of-the-art in LLM-based, few-shot tabular learning and we novelly\napply our LLM-driven framework to genotype data. FREE FORM goes beyond the typical usage of LLMs for inference\nand aids the process of feature selection and engineering, tackling the issues of high dimensionality, limited data sam-\nples, and interpretability. Furthermore, we find that LLMs have a robust knowledge of genetic variants, demonstrating\nstate-of-the-art performance across different variant ID schemas, showcasing the promise of LLMs in genetics.\nOur framework notably has several key advantages over existing approaches besides performance: (1) it is model-\nagnostic, (2) it scales well to higher dimensions, and (2) it incurs no inference costs, as features are engineered once\nduring training, unlike LLM-only methods with high computational costs at inference time. Moreover, the entire\npipeline can be executed using API access, costing approximately one dollar to run, with the majority of the pipeline\ncompleting within minutes, aside from the initial filtering step in feature selection.\nHowever, FREE FORM has room for growth. The current results suggest we either allow a naive range of variants\ninteraction types in feature engineering, or none of the models are capable of capturing epistasis. Our retrieval aug-\nmentation is also limited to gene information. Additionally, the LLM’s input could be enhanced by including high-level\nfeature statistics typically considered in this domain. Future work remains in expanding its capabilities to generate\nnovel features, improve the extraction of knowledge intrinsic to LLMs, better augment task-specific knowledge from\nAPIs such as PubMed, and integrate further interpretability or explainability. Recent studies have demonstrated the\npotential for LLMs in the discovery of new gene sets or causal genes 24,25, where LLMs effectively interpolate across\nthe vast corpus of scientific literature they are trained on. We find this approach promising for addressing the issue\nof multicollinearity, which we acknowledged but did not resolve in this work. Furthermore, we see promising future\nwork in developing feature nomination, where we used the LLM to suggest predictive features. The controllability of\nLLMs through prompting opens up interesting possibilities, such as the nomination of features that better represent\ndiverse populations, thereby mitigating biases that data-driven methods can exacerbate. One limitation of our study\nis that a few assumptions were made in the evaluation, including the fixed selection of fifteen features, to showcase\nour pipeline. Thus, we plan to expand our evaluation to more scenarios and more phenotypes, such as Alzheimer’s\ndisease, to demonstrate its robust utility.\nAs LLMs advance in domain expertise, potentially surpassing humans43, their potential to revolutionize bioinformatics\nbecomes increasingly imminent. While our study demonstrates LLMs’ excellence in low-shot regimes, we acknowl-\nedge such scenarios are rare in practice. We anticipate, however, these capabilities will scale as foundational models\nadvance and domain-specific LLMs develop. Recent efforts, such as fine-tuning foundational models on literature 43\nor augmenting LLMs with knowledge graphs 44, are making progress towards this. While our framework focuses on\nfeature selection and engineering, our work serves as a prototype, showcasing the potential of LLMs in genetics.\nAcknowledgments This work was supported in part by the NIH grants U01 AG066833, U01 AG068057, R01\nAG071470, U19 AG074879, and S10 OD023495.\nReferences\n1. Torkmani A, Wineinger NE, Topol EJ. The personal and clinical utility of polygenic risk scores. Nature Reviews Genetics.\n2018;19:581-90.\n2. Guo T, Li X. Machine learning for predicting phenotype from genotype and environment. Current Opinion in Biotechnology.\n2023;79:102853.\n3. Ma Y , Zhou X. Genetic prediction of complex traits with polygenic scores: a statistical review. Trends in Genetics.\n2021;37(11):995-1011.\n4. Medvedev A, Mishra Sharma S, Tsatsorin E, Nabieva E, Yarotsky D. Human genotype-to-phenotype predictions: Boosting\naccuracy with nonlinear models. PloS one. 2022;17(8):e0273293.\n5. Uffelmann E, Huang QQ, Munung NS, De Vries J, Okada Y , Martin AR, et al. Genome-wide association studies. Nature\nReviews Methods Primers. 2021;1(1):59.\n6. Krzywinski M, Altman N. Multiple linear regression: when multiple variables are associated with a response, the interpretation\nof a prediction equation is seldom simple. Nature methods. 2015;12(12):1103-5.\n7. Lippert C, Listgarten J, Davidson RI, Baxter J, Poon H, Kadie CM, et al. An exhaustive epistatic SNP association analysis on\nexpanded Wellcome Trust data. Scientific reports. 2013;3(1):1099.\n8. Altman N, Krzywinski M. The curse (s) of dimensionality. Nat Methods. 2018;15(6):399-400.\n9. Pudjihartono N, Fadason T, Kempa-Liehr AW, O’Sullivan JM. A review of feature selection methods for machine learning-\nbased disease risk prediction. Frontiers in Bioinformatics. 2022;2:927312.\n10. Lou Y , Caruana R, Gehrke J, Hooker G. Accurate intelligible models with pairwise interactions. In: Proceedings of the 19th\nACM SIGKDD international conference on Knowledge discovery and data mining; 2013. p. 623-31.\n11. Achiam J, Adler S, Agarwal S, Ahmad L, Akkaya I, Aleman FL, et al. Gpt-4 technical report. arXiv preprint arXiv:230308774.\n2023.\n12. Chang Y , Wang X, Wang J, Wu Y , Yang L, Zhu K, et al. A survey on evaluation of large language models. ACM Transactions\non Intelligent Systems and Technology. 2023.\n13. Singhal K, Azizi S, Tu T, Mahdavi SS, Wei J, Chung HW, et al. Large language models encode clinical knowledge. Nature.\n2023;620(7972):172-80.\n14. Nori H, Lee YT, Zhang S, Carignan D, Edgar R, Fusi N, et al. Can generalist foundation models outcompete special-purpose\ntuning? case study in medicine. arXiv preprint arXiv:231116452. 2023.\n15. Hou W, Ji Z. Assessing GPT-4 for cell type annotation in single-cell RNA-seq analysis. Nature Methods. 2024:1-4.\n16. Wei J, Wang X, Schuurmans D, Bosma M, Xia F, Chi E, et al. Chain-of-thought prompting elicits reasoning in large language\nmodels. Advances in neural information processing systems. 2022;35:24824-37.\n17. Wang X, Wei J, Schuurmans D, Le Q, Chi E, Narang S, et al. Self-consistency improves chain of thought reasoning in language\nmodels. arXiv preprint arXiv:220311171. 2022.\n18. Yao S, Yu D, Zhao J, Shafran I, Griffiths T, Cao Y , et al. Tree of thoughts: Deliberate problem solving with large language\nmodels. Advances in Neural Information Processing Systems. 2024;36.\n19. Choi K, Cundy C, Srivastava S, Ermon S. LMPriors: Pre-Trained Language Models as Task-Specific Priors. In: NeurIPS 2022\nFoundation Models for Decision Making Workshop;. .\n20. Jeong DP, Lipton ZC, Ravikumar P. LLM-Select: Feature Selection with Large Language Models. arXiv preprint\narXiv:240702694. 2024.\n21. Hollmann N, M ¨uller S, Hutter F. Large language models for automated data science: Introducing caafe for context-aware\nautomated feature engineering. Advances in Neural Information Processing Systems. 2024;36.\n22. Han S, Yoon J, Arik SO, Pfister T. Large Language Models Can Automatically Engineer Features for Few-Shot Tabular\nLearning. In: Forty-first International Conference on Machine Learning; 2024. .\n23. Toufiq M, Rinchai D, Bettacchioli E, Kabeer BSA, Khan T, Subba B, et al. Harnessing large language models (LLMs) for\ncandidate gene prioritization and selection. Journal of Translational Medicine. 2023;21(1):728.\n24. Wang Z, Jin Q, Wei CH, Tian S, Lai PT, Zhu Q, et al. GeneAgent: Self-verification Language Agent for Gene Set Knowledge\nDiscovery using Domain Databases. arXiv preprint arXiv:240516205. 2024.\n25. Shringarpure SS, Wang W, Karagounis S, Wang X, Reisetter AC, Auton A, et al. Large language models identify causal genes\nin complex trait GW AS. medRxiv. 2024:2024-05.\n26. Jin Q, Yang Y , Chen Q, Lu Z. Genegpt: Augmenting large language models with domain tools for improved access to\nbiomedical information. Bioinformatics. 2024;40(2):btae075.\n27. Hou W, Shang X, Ji Z. Benchmarking large language models for genomic knowledge with GeneTuring. bioRxiv. 2025.\nAvailable from: https://www.biorxiv.org/content/early/2025/01/05/2023.03.11.532238.\n28. Li D, Tan Z, Liu H. Exploring Large Language Models for Feature Selection: A Data-centric Perspective. arXiv preprint\narXiv:240812025. 2024.\n29. Huang J, Chang KCC. Towards reasoning in large language models: A survey. arXiv preprint arXiv:221210403. 2022.\n30. Tam ZR, Wu CK, Tsai YL, Lin CY , Lee Hy, Chen YN. Let Me Speak Freely? A Study on the Impact of Format Restrictions\non Performance of Large Language Models. arXiv preprint arXiv:240802442. 2024.\n31. Brown TB. Language models are few-shot learners. arXiv preprint ArXiv:200514165. 2020.\n32. Agrawal M, Hegselmann S, Lang H, Kim Y , Sontag D. Large language models are few-shot clinical information extractors.\narXiv preprint arXiv:220512689. 2022.\n33. Kandpal N, Deng H, Roberts A, Wallace E, Raffel C. Large language models struggle to learn long-tail knowledge. In:\nInternational Conference on Machine Learning. PMLR; 2023. p. 15696-707.\n34. Liu NF, Lin K, Hewitt J, Paranjape A, Bevilacqua M, Petroni F, et al. Lost in the middle: How language models use long\ncontexts. Transactions of the Association for Computational Linguistics. 2024;12:157-73.\n35. Kortukov E, Rubinstein A, Nguyen E, Oh SJ. Studying Large Language Model Behaviors Under Realistic Knowledge Con-\nflicts. arXiv preprint arXiv:240416032. 2024.\n36. Lewis P, Perez E, Piktus A, Petroni F, Karpukhin V , Goyal N, et al. Retrieval-augmented generation for knowledge-intensive\nnlp tasks. Advances in Neural Information Processing Systems. 2020;33:9459-74.\n37. Tong Y , Li D, Wang S, Wang Y , Teng F, Shang J. Can LLMs Learn from Previous Mistakes? Investigating LLMs’ Errors to\nBoost for Reasoning. arXiv preprint arXiv:240320046. 2024.\n38. Fairley S, Lowy-Gallego E, Perry E, Flicek P. The International Genome Sample Resource (IGSR) collection of open human\ngenomic variation resources. Nucleic acids research. 2020;48(D1):D941-7.\n39. Luo X, Li F, Xu W, Hong K, Yang T, Chen J, et al. Machine learning-based genetic diagnosis models for hereditary hearing\nloss by the GJB2, SLC26A4 and MT-RNR1 variants. EBioMedicine. 2021;69.\n40. Moustafa A. Genetic Ancestry; 2023. https://github.com/ahmedmoustafa/genetic-ancestry [Accessed: May 6 2024].\n41. Hollmann N, M ¨uller S, Eggensperger K, Hutter F. Tabpfn: A transformer that solves small tabular classification problems in a\nsecond. arXiv preprint arXiv:220701848. 2022.\n42. Hegselmann S, Buendia A, Lang H, Agrawal M, Jiang X, Sontag D. Tabllm: Few-shot classification of tabular data with large\nlanguage models. In: International Conference on Artificial Intelligence and Statistics. PMLR; 2023. p. 5549-81.\n43. Luo X, Rechardt A, Sun G, Nejad KK, Y ´a˜nez F, Yilmaz B, et al. Large language models surpass human experts in predicting\nneuroscience results. Nature human behaviour. 2024:1-11.\n44. Li D, Yang S, Tan Z, Baik JY , Yun S, Lee J, et al. DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer’s\nDisease Questions with Scientific Literature. arXiv preprint arXiv:240504819. 2024.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6905727386474609
    },
    {
      "name": "Task (project management)",
      "score": 0.6616161465644836
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.613734245300293
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.5924695134162903
    },
    {
      "name": "Feature selection",
      "score": 0.5878190398216248
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.5353790521621704
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5099841356277466
    },
    {
      "name": "Machine learning",
      "score": 0.4322667717933655
    },
    {
      "name": "Natural language processing",
      "score": 0.4231453537940979
    },
    {
      "name": "Feature engineering",
      "score": 0.42132091522216797
    },
    {
      "name": "Data mining",
      "score": 0.3541293740272522
    },
    {
      "name": "Data science",
      "score": 0.32058924436569214
    },
    {
      "name": "Engineering",
      "score": 0.11606928706169128
    },
    {
      "name": "Deep learning",
      "score": 0.10310205817222595
    },
    {
      "name": "Linguistics",
      "score": 0.06989970803260803
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}