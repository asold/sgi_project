{
  "title": "CoTr: Efficiently Bridging CNN and Transformer for 3D Medical Image Segmentation",
  "url": "https://openalex.org/W3134689216",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1757568559",
      "name": "Xie, Yutong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2204687677",
      "name": "Zhang, Jianpeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2360656043",
      "name": "Shen, Chunhua",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1965130480",
      "name": "Xia, Yong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2598666589",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2996290406",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W3132890542",
    "https://openalex.org/W2402213890",
    "https://openalex.org/W3109167713",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3108995912",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963840672",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2965669393",
    "https://openalex.org/W3035367255",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3112701542",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W3166513219",
    "https://openalex.org/W3008652188",
    "https://openalex.org/W2502312327"
  ],
  "abstract": "Convolutional neural networks (CNNs) have been the de facto standard for nowadays 3D medical image segmentation. The convolutional operations used in these networks, however, inevitably have limitations in modeling the long-range dependency due to their inductive bias of locality and weight sharing. Although Transformer was born to address this issue, it suffers from extreme computational and spatial complexities in processing high-resolution 3D feature maps. In this paper, we propose a novel framework that efficiently bridges a {\\bf Co}nvolutional neural network and a {\\bf Tr}ansformer {\\bf (CoTr)} for accurate 3D medical image segmentation. Under this framework, the CNN is constructed to extract feature representations and an efficient deformable Transformer (DeTrans) is built to model the long-range dependency on the extracted feature maps. Different from the vanilla Transformer which treats all image positions equally, our DeTrans pays attention only to a small set of key positions by introducing the deformable self-attention mechanism. Thus, the computational and spatial complexities of DeTrans have been greatly reduced, making it possible to process the multi-scale and high-resolution feature maps, which are usually of paramount importance for image segmentation. We conduct an extensive evaluation on the Multi-Atlas Labeling Beyond the Cranial Vault (BCV) dataset that covers 11 major human organs. The results indicate that our CoTr leads to a substantial performance improvement over other CNN-based, transformer-based, and hybrid methods on the 3D multi-organ segmentation task. Code is available at \\def\\UrlFont{\\rm\\small\\ttfamily} \\url{https://github.com/YtongXie/CoTr}",
  "full_text": "CoTr: Eﬃciently Bridging CNN and\nTransformer for 3D Medical Image Segmentation\nYutong Xie1,2⋆, Jianpeng Zhang1,2⋆, Chunhua Shen2,\nYong Xia\f1\n1 Northwestern Polytechnical University, China\n2 The University of Adelaide, Australia\nAbstract. Convolutional neural networks (CNNs) have been the de\nfacto standard for nowadays 3D medical image segmentation. The con-\nvolutional operations used in these networks, however, inevitably have\nlimitations in modeling the long-range dependency due to their induc-\ntive bias of locality and weight sharing. Although Transformer was born\nto address this issue, it suﬀers from extreme computational and spatial\ncomplexities in processing high-resolution 3D feature maps. In this paper,\nwe propose a novel framework that eﬃciently bridges a Convolutional\nneural network and a Transformer (CoTr) for accurate 3D medical im-\nage segmentation. Under this framework, the CNN is constructed to\nextract feature representations and an eﬃcient deformable Transformer\n(DeTrans) is built to model the long-range dependency on the extracted\nfeature maps. Diﬀerent from the vanilla Transformer which treats all im-\nage positions equally, our DeTrans pays attention only to a small set of\nkey positions by introducing the deformable self-attention mechanism.\nThus, the computational and spatial complexities of DeTrans have been\ngreatly reduced, making it possible to process the multi-scale and high-\nresolution feature maps, which are usually of paramount importance for\nimage segmentation. We conduct an extensive evaluation on the Multi-\nAtlas Labeling Beyond the Cranial Vault (BCV) dataset that covers 11\nmajor human organs. The results indicate that our CoTr leads to a sub-\nstantial performance improvement over other CNN-based, transformer-\nbased, and hybrid methods on the 3D multi-organ segmentation task.\nCode is available at https://github.com/YtongXie/CoTr\nKeywords: 3D Medical image segmentation · Deformable self-attention\n· CNN · Transformer.\n1 Introduction\nImage segmentation is a longstanding challenge in medical image analysis. Since\nthe introduction of U-Net [16], fully convolutional neural networks (CNNs) have\nbecome the predominant approach to addressing this task [10,12,23–25,28]. De-\nspite their prevalence, CNNs still suﬀer from the limited receptive ﬁeld and fail\n⋆ Y. Xie and J. Zhang contributed equally to this work.\narXiv:2103.03024v1  [cs.CV]  4 Mar 2021\n2 Yutong Xie 1,2, Jianpeng Zhang1,2⋆, Chunhua Shen2, Yong Xia\f1\nCNNencoderCNNdecoderTransformer(a)CNN(b)SETR(c)TransUNet (d)CoTr\nEncoder\nDecoder\nFig. 1.Comparison of diﬀerent segmentation architectures. All of them have an\nencoder-decoder structure, but with diﬀerent encoders. The encoder in CNN (a) is\ncomposed of multiple stacked convolutional layers. The encoder in SETR (b) is purely\nformed from self-attention layers, i.e., Transformer. The encoder in both TransUNet\n(c) and our proposed CoTr (d) are the hybrid of CNN and Transformer. Diﬀerently,\nTransUNet only processes the low-resolution feature maps from the last stage due\nto the high computation and spatial complexities. Thanks to the eﬃcient design of\nTransformer, CoTr is able to process the multi-scale and high-resolution feature maps.\nto capture the long-range dependency, due to the inductive bias of locality and\nweight sharing [6]. Many eﬀorts have been devoted to enlarge a CNN’s receptive\nﬁeld thus improve its ability to context modeling. Yu et al. [22] proposed the\natrous convolution with an adjustable dilated rate, which shows superior per-\nformance in semantic segmentation [5]. More straightforwardly, Peng et al. [15]\ndesigned large kernels to capture rich global context information. Zhaoet al.[26]\nemployed the pyramid pooling at multiple feature scales to aggregate multi-scale\nglobal information. Wang et al.[20] presented the non-local operations which is\nusually embedded at the end of encoder to capture the long-range dependency.\nAlthough improving the context modeling to some extent, these models still have\nan inevitably limited receptive ﬁeld, stranded by the CNN architecture.\nTransformer, a sequence-to-sequence prediction framework, has a proven\ntrack record in machine translation and nature language processing [8, 19], due\nto its strong ability to long-range modeling. The self-attention mechanism in\nTransformer can dynamically adjust the receptive ﬁeld according to the input\ncontent, and hence is superior to convolutional operations in modeling the long-\nrange dependency.\nRecently, Transformer has been considered as an alternative architecture,\nand has achieved competitive performance on many computer vision tasks, like\nimage recognition [9, 17], semantic/instance segmentation [21, 27], object detec-\ntion [2,29], low-level vision [3,14], and image generation [13]. A typical example\nis the vision Transformer (ViT) [9], which outperforms a ResNet-based CNN on\nrecognition tasks but at a cost of using 300M data for training. Since a huge\ntraining dataset is not always available, recent studies attempt to combine a\nCNN and a Transformer into a hybrid model. Carion et al.[2] employed a CNN\nto extract image features and a Transformer to further process the extracted fea-\ntures. Chen et al. [4] designed TransUNet, in which a CNN and a Transformer\nare combined in a cascade manner to make a strong encoder for 2D medical\nimage segmentation. Although the design of TransUNet is interesting and the\nCoTr: Bridging CNN and Transformer for 3D Medical Image Segmentation 3\nDeTransLayer\nDeTransLayer...F\nF\nF R\nR\nR\nDeTransLayer\nPositional encoding Upsampling\nFlatten ReshapeCNN-encoderDeTrans-encoderDecoder\nMS-DMSA\nFeed Forward\nReferencepoint\nLayerNorm\nLayerNorm\nDeformableTransformerLayer\nFig. 2.Diagram of CoTr: A CNN-encoder, a DeTrans-encoder, and a decoder. Gray\nrectangles: CNN blocks. Yellow rectangles: 3D deformable Transformer layers. The\nCNN-encoder extracts multi-scale feature maps from an input image. The DeTrans-\nencoder processes the ﬂattened multi-scale feature maps that embedded with the posi-\ntional encoding in a sequence-to-sequence manner. The features with long-range depen-\ndency are generated by the DeTrans-encoder and fed to the decoder for segmentation.\nperformance is good, it is challenging to optimize this model due to the existence\nof self-attention [19]. First, it requires extremely long training time to focus the\nattention, which was initially cast to each pixel uniformly, on salient locations,\nespecially in a 3D scenario. Second, due to its high computational complexity,\na vanilla Transformer [19] can hardly process multi-scale and high-resolution\nfeature maps, which play a critical role in image segmentation.\nIn this paper, we propose a hybrid framework that eﬃciently bridges Co-\nnvolutional neural network and Transformer (CoTr) for 3D medical image seg-\nmentation. CoTr has an encoder-decoder structure. In the encoder, a concise\nCNN structure is adopted to extract feature maps and a Transformer is used\nto capture the long-range dependency (see Fig. 1). Inspired by [7, 29], we intro-\nduce the deformable self-attention mechanism to the Transformer. This attention\nmechanism casts attentions only to a small set of key sampling points, and thus\ndramatically reduces the computational and spatial complexity of Transformer.\nAs a result, it is possible for the Transformer to process the multi-scale feature\nmaps produced by the CNN and keep abundant high resolution information for\nsegmentation. The main contributions of this paper are three-fold: (1) we are the\nﬁrst to explore Transformer for 3D medical image segmentation, particularly in\na computationally and spatially eﬃcient way; (2) we introduce the deformable\nself-attention mechanism to reduce the complexity of vanilla Transformer, and\nthus enable our CoTr to model the long-range dependency using multi-scale fea-\ntures; (3) our CoTr outperforms the competing CNN-based, Transformer-based,\nand hybrid methods on the 3D multi-organ segmentation task.\n4 Yutong Xie 1,2, Jianpeng Zhang1,2⋆, Chunhua Shen2, Yong Xia\f1\n2 Materials\nThe Multi-Atlas Labeling Beyond the Cranial Vault (BCV) dataset 1 was used\nfor this study. It contains 30 labeled CT scans for automated segmentation of\n11 abdominal organs, including the spleen (Sp), kidney (Ki), gallbladder (Gb),\nesophagus (Es), liver (Li), stomach (St), aorta (Ao), inferior vena cava (IVC),\nportal vein and splenic vein (PSV), pancreas (Pa), and adrenal gland (AG).\n3 Methods\nCoTr aims to learn more eﬀective representations for medical image segmentation\nvia bridging CNN and Transformer. As shown in Fig. 2, it consists of a CNN-\nencoder for feature extraction, a deformable Transformer-encoder (DeTrans-\nencoder) for long-range dependency modeling, and a decoder for segmentation.\nWe now delve into the details of each module.\n3.1 CNN-encoder\nThe CNN-encoder FCNN (·) contains a Conv-IN-ReLU block and three stages\nof 3D residual blocks. The Conv-IN-ReLU block contains a 3D convolutional\nlayer followed by an instance normalization (IN) [18] and Rectiﬁed Linear Unit\n(ReLU) activation. The numbers of 3D residual blocks in three stages are three,\nthree, and two, respectively.\nGiven an input image xwith a height of H, a width of W, and a depth (i.e.,\nnumber of slices) of D, the feature maps produced by FCNN (·) can be formally\nexpressed as\n{fl}L\nl=1 = FCNN\nl (x; Θ) ∈RC×D\n2l × H\n2l+1 × W\n2l+1 , (1)\nwhere Lindicates the number of feature levels, Θdenotes the parameters of the\nCNN-encoder, and C denotes the number of channels.\n3.2 DeTrans-encoder\nDue to the intrinsic locality of convolution operations, the CNN-encoder can-\nnot capture the long-range dependency of pixels eﬀectively. To this end, we\npropose the DeTrans-encoder that introduces the multi-scale deformable self-\nattention (MS-DMSA) mechanism for eﬃcient long-range contextual modeling.\nThe DeTrans-encoder is a composition of an input-to-sequence layer and LD\nstacked deformable Transformer (DeTrans) layers.\nInput-to-sequence Transformation.Considering that Transformer processes\nthe information in a sequence-to-sequence manner, we ﬁrst ﬂatten the feature\nmaps produced by the CNN-encoder {fl}L\nl=1 into a 1D sequence. Unfortunately,\nthe operation of ﬂattening the features leads to losing the spatial information\nthat is critical for image segmentation. To address this issue, we supplement the\n1 https://www.synapse.org/#!Synapse:syn3193805/wiki/217789\nCoTr: Bridging CNN and Transformer for 3D Medical Image Segmentation 5\n3D positional encoding sequence {pl}L\nl=1 to the ﬂattened {fl}L\nl=1. For this study,\nwe use sine and cosine functions with diﬀerent frequencies [19] to compute the\npositional coordinates of each dimension pos, shown as follows\n{ PE#(pos,2k) = sin(pos·υ)\nPE#(pos,2k+ 1) = cos(pos·υ) (2)\nwhere # ∈{D,H,W }indicates each of three dimensions, υ= 1/100002k/ C\n3 . For\neach feature level l, we concatenate PED, PEH, and PEW as the 3D positional\nencoding pl and combine it with the ﬂattened fl via element-wise summation\nto form the input sequence of DeTrans-encoder.\nMS-DMSA Layer.In the architecture of Transformer, the self-attention layer\nwould look over all possible locations in the feature map. It has the drawback\nof slow convergence and high computational complexity, and hence can hardly\nprocess multi-scale features. To remedy this, we design the MS-DMSA layer that\nfocuses only on a small set of key sampling locations around a reference location,\ninstead of all locations.\nLet zq ∈RC be the feature representation of query q and ˆpq ∈[0,1]3 be the\nnormalized 3D coordinate of the reference point. Given the multi-scale feature\nmaps {fl}L\nl=1 that are extracted in the last Lstages of CNN-encoder, the feature\nrepresentation of the i-th attention head can be calculated as\nheadi =\nL∑\nl\nK∑\nk\nΛ(zq)ilqk ·Ψ(fl)(σl( ˆpq) + ∆pilqk ) (3)\nwhere K is the number of sampled key points, Λ(zq)ilqk ∈[0,1] is the attention\nweight, ∆pilqk ∈R3 is the sampling oﬀset of the k-th sampling point in the l-th\nfeature level, and σl(·) re-scales ˆpq to the l-th level feature. Following [29], both\nΛ(zq)ilqk and ∆pilqk are obtained via linear projection over the query feature zq.\nThen, the MS-DMSA layer can be formulated as\nMS −DMSA(zq,{fl}L\nl=1) = Φ(Concat(head1,head2,..., headH)) (4)\nwhere H is the number of attention heads, and Φ(·) is a linear projection layer\nthat weights and aggregates the feature representation of all attention heads.\nDeTrans Layer.The DeTrans layer is composed of a MS-DMSA layer and a\nfeed forward network, each being followed by the layer normalization [1] (see\nFig. 2). The skip connection strategy [11] is employed in each sub-layer to avoid\ngradient vanishing. The DeTrans-encoder is constructed by repeatedly stacking\nDeTrans layers.\n3.3 Decoder\nThe output sequence of DeTrans-encoder is reshaped into feature maps according\nto the size at each scale. The decoder, a pure CNN architecture, progressively\n6 Yutong Xie 1,2, Jianpeng Zhang1,2⋆, Chunhua Shen2, Yong Xia\f1\nupsamples the feature maps to the input resolution ( i.e., D×H×W) using the\ntranspose convolution, and then reﬁnes the upsampled feature maps using a 3D\nresidual block. Besides, the skip connections between encoder and decoder are\nalso added to keep more low-level details for better segmentation. We also use\nthe deep supervision strategy by adding auxiliary losses to the decoder outputs\nwith diﬀerent scales. The loss function of our model is the sum of the Dice loss\nand cross-entropy loss [12,24,28]. More details on the network architecture gare\nin Appendix.\n3.4 Implementation details\nFollowing [12], we ﬁrst truncated the HU values of each scan using the range\nof [−958,327] to ﬁlter irrelevant regions, and then normalized truncated voxel\nvalues by subtracting 82.92 and dividing by 136.97. We randomly split the BCV\ndataset into two parts: 21 scans for training and 9 scans for test, and randomly\nselected 6 training scans to form a validation set, which just was used to select\nthe hyper-parameters of CoTr. The ﬁnal results on the test set are obtained by\nthe model trained on all training scans.\nIn the training stage, we randomly cropped sub-volumes of size 48×192×192\nfrom CT scans as the input. To alleviate the over-ﬁtting of limited training data,\nwe employed the online data argumentation [12], including the random rota-\ntion, scaling, ﬂipping, adding white Gaussian noise, Gaussian blurring, adjusting\nrightness and contrast, simulation of low resolution, and Gamma transformation,\nto diversify the training set. Due to the beneﬁts of instance normalization [18],\nwe adopted the micro-batch training strategy with a small batch size of 2. To\nweigh the balance between training time cost and performance reward, CoTr\nwas trained for 1000 epochs and each epoch contains 250 iterations. We adopted\nthe stochastic gradient descent algorithm with a momentum of 0.99 and an ini-\ntial learning rate of 0.01 as the optimizer. We set the hidden size in MS-DMSA\nand feed forward network to 384 and 1536, respectively, and empirically set the\nhyper-parameters LD = 6, H = 6, and K = 4. Besides, we formed two vari-\nants of CoTr with small CNN-encoders, denoted as CoTr∗and CoTr†. In CoTr∗,\nthere is only one 3D residual block in each stage of CNN-encoder. In CoTr †, the\nnumber of 3D residual blocks in each stage of CNN-encoder is two.\nIn the test stage, we employed the sliding window strategy, where the win-\ndow size equals to the training patch size. Besides, Gaussian importance weight-\ning [12] and test time augmentation by ﬂipping along all axes were also utilized\nto improve the robustness of segmentation. To quantitatively evaluate the seg-\nmentation results, we calculated the Dice coeﬃcient scores (Dice) metric that\nmeasures the overlapping between a prediction and its ground truth.\n4 Results\nComparing to models with only Transformer encoder.We ﬁrst evaluated\nour CoTr against two variants of the state-of-the-art SEgmentation Transformer\nCoTr: Bridging CNN and Transformer for 3D Medical Image Segmentation 7\nTable 1.Dice scores of our CoTr and several competing methods on the BCV test set.\nCoTr∗and CoTr†are two variants of CoTr with small CNN-encoders\nMethods Param\n(M)\nOrgans AveSp Ki Gb Es Li St Ao IVC PSV Pa AG\nSETR (ViT-B/16-rand) [27] 100.5 95.2 92.3 55.6 71.3 96.2 80.2 89.7 83.9 68.9 68.7 60.5 78.4\nSETR (ViT-B/16-pre) [27] 100.5 94.8 91.7 55.2 70.9 96.2 76.9 89.3 82.4 69.6 70.7 58.7 77.8\nCoTr w/o CNN-encoder 21.9 95.2 92.8 59.2 72.2 96.3 81.2 89.9 85.1 71.9 73.3 61.0 79.8\nCoTr w/o DeTrans 32.6 96.0 92.6 63.8 77.9 97.0 83.6 90.8 87.8 76.7 81.2 72.6 83.6\nAPSS [5] 45.5 96.5 93.8 65.6 78.1 97.1 84.0 91.1 87.9 77.0 82.6 73.9 84.3\nPP [26] 33.9 96.1 93.1 64.3 77.4 97.0 85.3 90.8 87.4 77.2 81.9 72.8 83.9\nNon-local [20] 32.8 96.3 93.7 64.6 77.9 97.1 84.1 90.8 87.7 77.2 82.1 73.3 84.1\nTransUnet [4] 43.5 95.9 93.7 63.1 77.8 97.0 86.2 91.0 87.8 77.8 81.6 73.9 84.2\nCoTr∗ 27.9 96.4 94.0 66.2 76.4 97.0 84.2 90.3 87.6 76.3 80.8 72.9 83.8\nCoTr† 36.9 96.2 93.8 66.5 78.6 97.1 86.9 90.8 87.8 77.7 82.8 73.2 84.7\nCoTr 41.9 96.3 93.9 66.6 78.0 97.1 88.2 91.2 88.0 78.1 83.1 74.185.0\n(SETR) [27], which were formed by using randomly initialized and pre-trained\nViT-B/16 [9] as the encoder. We also compared to a variant of CoTr that re-\nmoves the CNN-encoder (CoTr w/o CNN-encoder). To ensure an unprejudiced\ncomparison, all models use the same decoder. The segmentation performance of\nthese models is shown in Table 1, from which three conclusions can be drawn.\nFirst, although the Transformer architecture is not limited by the type of input\nimages, the ViT-B/16 pre-trained on 2D natural images does not work well on\n3D medical images. The suboptimal performance may be attributed to the do-\nmain shift between 2D natural images and 3D medical images. Second, ‘CoTr\nw/o CNN-encoder’ has about 22M parameters and outperforms the SETR with\nabout 100M parameters. We believe that a lightweight Transformer may be more\nfriendly for medical image segmentation tasks, where there is usually a small\ntraining dataset. Third, our CoTr ∗ with comparable parameters signiﬁcantly\noutperforms ‘CoTr w/o CNN-encoder’, improving the average Dice over 11 or-\ngans by 4%. It suggests that the hybrid CNN-Transformer encoder has distinct\nadvantages over the pure Transformer encoder in medical image segmentation.\nComparing to models with only CNN encoder.Then, we compared CoTr\nagainst a variant of CoTr that removes the DeTrans-encoder (CoTr w/o De-\nTrans) and three CNN-based context modeling methods, i.e., the Atrous Spatial\nPyramid Pooling (ASPP) [5] module, pyramid parsing (PP) [26] module, and\nNon-local [20] module. For a fair comparison, we used the same CNN-encoder\nand decoder but replaced our DeTrans-encoder with ASPP, PP, and Non-local\nmodules, respectively. The results in Table 1 shows that our CoTr elevates con-\nsistently the segmentation performance over ‘CoTr w/o DeTrans’ on all organs\nand improves the average Dice by 1.4%. It corroborates that our CoTr using\na hybrid CNN-Transformer encoder has a stronger ability than using a pure\nCNN encoder to learn eﬀective representations for medical image segmentation.\nMoreover, comparing to these context modeling methods, our Transformer ar-\nchitecture contributes to more accurate segmentation.\n8 Yutong Xie 1,2, Jianpeng Zhang1,2⋆, Chunhua Shen2, Yong Xia\f1\n81.381.882.2\n8181.481.882.282.6\n1 2 4\nDice(%)\nK\n81.581.982.2\n80.881.281.68282.4\n2 4 6\nDice(%)\nHeads\n81.281.782.2\n8181.58282.5\n2 4 6\nDice(%)\nLayers\n81.0\n82.2\n80.581.081.582.082.5\nSSMS\nDice(%)\nSS:Single-scaleMS:Multi-scale (a) (b) (c) (d)\nFig. 3.Average Dice over all organs obtained on the validation set versus (a) the\nnumber of sampled key points K, (b) number of heads H, and (c) number of DeTrans\nlayers LD, and (d) Average Dice obtained by our CoTr using, respectively, single-scale\nand multi-scale feature maps on the validation set.\nComparing to models with hybrid CNN-Transformer encoder. We\nalso compared CoTr to other hybrid CNN-Transformer architectures like Tran-\nsUNet [4]. To process 3D images directly, we extended the original 2D TransUNet\nto a 3D version by using 3D CNN-encoder and decoder as done in CoTr. We\nalso set the number of heads and layers of Transformer in 3D TransUNet to\nbe the same as our CoTr. It shows in Table 1 that CoTr steadily beats Tran-\nsUNet in the segmentation of all organs, particularly for the gallbladder and\npancreas segmentation. Even with a smaller CNN-encoder, CoTr †still achieves\nbetter performance than TransUnet in the segmentation of seven organs. The\nsuperior performance owes to the deformable mechanism in CoTr that makes\nit possible to process high-resolution and multi-scale feature maps due to the\nreduced computational and spatial complexities.\nComputational Complexity.The proposed CoTr was trained using a work-\nstation with a NVIDIA GTX 2080Ti GPU and the Pytorch software packages.\nIt took about 2 days for training, and less than 30ms to segment a volume of\nsize 48 ×192 ×192.\n5 Discussion on Hyper-parameter Settings\nIn the DeTrans-encoder, there are three hyper-parameters, i.e., K, H, and LD,\nwhich represent the number of sampled key points, heads, and stacked DeTrans\nlayers, respectively. To investigate the impact of their settings on the segmenta-\ntion, we set K to 1, 2, and 4, set H to 2, 4, and 6, and set LD to 2, 4, and 6. In\nFig. 3 (a-c), we plotted the average Dice over all organs obtained on the valida-\ntion set versus the values of K, H, and LD. It shows that increasing the number\nof K, H, or LD can improve the segmentation performance. To demonstrate\nthe performance gain resulted from the multi-scale strategy, we also attempted\nto train CoTr with single-scale feature maps from the last stage. The results in\nFig. 3 (d) show that using multi-scale feature maps instead of single-scale feature\nmaps can eﬀectively improve the average Dice by 1.2%.\nCoTr: Bridging CNN and Transformer for 3D Medical Image Segmentation 9\n6 Conclusion\nIn this paper, we propose a hybrid model of CNN Transformer, namely CoTr,\nfor 3D medical image segmentation. In this model, we design the deformable\nTransformer (DeTrans) that employs the deformable self-attention mechanism\nto reduce the computational and spatial complexities of modelling the long-\nrange dependency on multi-scale and high-resolution feature maps. Comparative\nexperiments were conducted on the BCV dataset. The superior performance of\nour CoTr over both CNN-based and vanilla Transformer-based models suggests\nthat, via combining the advantages of CNN and Transformer, the proposed CoTr\nachieves the balance in keeping the details of low-level features and modeling the\nlong-range dependency. As a stronger baseline, our CoTr can be extended to deal\nwith other structures (e.g., brain structure or tumor segmentation) in the future.\nReferences\n1. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. arXiv preprint\narXiv:1607.06450 (2016)\n2. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-\nto-end object detection with transformers. In: European Conference on Computer\nVision. pp. 213–229. Springer (2020)\n3. Chen, H., Wang, Y., Guo, T., Xu, C., Deng, Y., Liu, Z., Ma, S., Xu, C., Xu, C., Gao,\nW.: Pre-trained image processing transformer. arXiv preprint arXiv:2012.00364\n(2020)\n4. Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A.L., Zhou,\nY.: Transunet: Transformers make strong encoders for medical image segmentation.\narXiv preprint arXiv:2102.04306 (2021)\n5. Chen, L.C., Zhu, Y., Papandreou, G., Schroﬀ, F., Adam, H.: Encoder-decoder with\natrous separable convolution for semantic image segmentation. In: Proceedings of\nthe European conference on computer vision (ECCV). pp. 801–818 (2018)\n6. Cohen, N., Shashua, A.: Inductive bias of deep convolutional networks through\npooling geometry. arXiv preprint arXiv:1605.06743 (2016)\n7. Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y.: Deformable convolu-\ntional networks. In: Proceedings of the IEEE international conference on computer\nvision. pp. 764–773 (2017)\n8. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805\n(2018)\n9. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is\nworth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929 (2020)\n10. Fang, X., Yan, P.: Multi-organ segmentation over partially labeled datasets with\nmulti-scale feature abstraction. IEEE Transactions on Medical Imaging 39(11),\n3619–3629 (2020)\n11. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\nProceedings of the IEEE conference on computer vision and pattern recognition.\npp. 770–778 (2016)\n10 Yutong Xie 1,2, Jianpeng Zhang1,2⋆, Chunhua Shen2, Yong Xia\f1\n12. Isensee, F., J¨ ager, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H.: Automated\ndesign of deep learning methods for biomedical image segmentation. arXiv preprint\narXiv:1904.08128 (2019)\n13. Jiang, Y., Chang, S., Wang, Z.: Transgan: Two transformers can make one strong\ngan. arXiv preprint arXiv:2102.07074 (2021)\n14. Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., Tran, D.:\nImage transformer. In: International Conference on Machine Learning. pp. 4055–\n4064. PMLR (2018)\n15. Peng, C., Zhang, X., Yu, G., Luo, G., Sun, J.: Large kernel matters–improve se-\nmantic segmentation by global convolutional network. In: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR). pp. 4353–4361\n(2017)\n16. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi-\ncal image segmentation. In: International Conference on Medical image computing\nand computer-assisted intervention. pp. 234–241. Springer (2015)\n17. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J´ egou, H.: Training\ndata-eﬃcient image transformers & distillation through attention. arXiv preprint\narXiv:2012.12877 (2020)\n18. Ulyanov, D., Vedaldi, A., Lempitsky, V.: Instance normalization: The missing in-\ngredient for fast stylization. arXiv preprint arXiv:1607.08022 (2016)\n19. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. In: Proceedings of the 31st Interna-\ntional Conference on Neural Information Processing Systems. pp. 6000–6010 (2017)\n20. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: Pro-\nceedings of the IEEE conference on computer vision and pattern recognition. pp.\n7794–7803 (2018)\n21. Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., Xia, H.: End-to-end\nvideo instance segmentation with transformers. arXiv preprint arXiv:2011.14503\n(2020)\n22. Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. In:\nInternational Conference on Learning Representations (ICLR) (2016)\n23. Zhang, J., Xie, Y., Wang, Y., Xia, Y.: Inter-slice context residual learning for 3d\nmedical image segmentation. IEEE Transactions on Medical Imaging (2020)\n24. Zhang, J., Xie, Y., Zhang, P., Chen, H., Xia, Y., Shen, C.: Light-weight hybrid con-\nvolutional network for liver tumor segmentation. In: IJCAI. pp. 4271–4277 (2019)\n25. Zhang, L., Zhang, J., Shen, P., Zhu, G., Li, P., Lu, X., Zhang, H., Shah, S.A.,\nBennamoun, M.: Block level skip connections across cascaded v-net for multi-organ\nsegmentation. IEEE Transactions on Medical Imaging 39(9), 2782–2793 (2020)\n26. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In:\nProceedings of the IEEE conference on computer vision and pattern recognition.\npp. 2881–2890 (2017)\n27. Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T.,\nTorr, P.H., et al.: Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. arXiv preprint arXiv:2012.15840 (2020)\n28. Zhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., Liang, J.: Unet++: Redesigning skip\nconnections to exploit multiscale features in image segmentation. IEEE Transac-\ntions on Medical Imaging 39(6), 1856–1867 (2019)\n29. Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable\ntransformers for end-to-end object detection. arXiv preprint arXiv:2010.04159\n(2020)\nCoTr: Bridging CNN and Transformer for 3D Medical Image Segmentation 11\n7 Appendix\n7.1 Detailed network architecture\nFig. 4 shows the architecture of CNN-encoder, decoder and feed forward network\nin Detrans-encoder. It consists of a Conv-In-Relu and three stages of 3D residual\nblocks. The numbers of 3D residual blocks are three, three, and two in three\nstages, respectively. The decoder contains four upsampling modules. Each of\nﬁrst three modules has a TransConv layer followed by a residual block, and a\npixel-wise summation with the corresponding feature maps from the encoder\nand the TransConv layer. The last module comprises of an Upsampling layer\nfollowed by a 1 × 1 Conv layer that maps the 64-channel feature maps to the\ndesired number of classes. The feed forward network in Detrans-encoder has two\nlinear projection layers. The ﬁrst layer is followed by a layer normalization layer\nand a Dropout layer. The second layer is followed by a Dropout layer.\n3DDeTransLayer\n3DDeTransLayer…\nFlatten\nReshape\nFlatten\n3DConv64,7,(1,2,2)1×48×192×192Inputs\n64×48×96×96\n192×24×48×48\n384×12×24×24\n384×6×12×12 384×6×12×12TransConv384,2,23DResBlock#384,1,2\nUpsampling14×48×192×192CNN-encoder\nDecoder\n3DConv14,1,1\nDeTrans-encoder\n3DDeTrans\n3DDeTrans…\n 384×12×24×24TransConv192,2,23DResBlock#192,1,2192×24×48×48TransConv64,2,23DResBlock#64,1,264×48×96×96\n3D ConvF,3,S\n3DResBlock#F,S,N3DResBlock#192,1,1Stage1:(b1) 3DResBlock#192,2,1\n…\n3DResBlock#384,1,1Stage2:(b2) 3DResBlock#384,2,1\n…\n3DResBlock#384,1,1Stage3:(b3) 3DResBlock#384,2,1\n… 3D ConvF,3,S…N\nPosition Embedding F\nF\nF\nFlattenoperationRReshapeoperation\nRR\n3DConv64,7,(1,2,2)1×48×192×192Inputs\n64×48×96×96\n192×24×48×48\n384×12×24×24\n384×6×12×12 384×6×12×12TransConv384,2,23DResBlock#384,1,2\nUpsampling14×48×192×192CNN-encoder\nDecoder\n3DConv14,1,1\n384×12×24×24TransConv192,2,23DResBlock#192,1,2192×24×48×48TransConv64,2,23DResBlock#64,1,264×48×96×963DResBlock#192,1,1Stage1: 3DResBlock#192,2,1\n3DResBlock#384,1,1Stage2:3DResBlock#384,2,1\n3DResBlock#384,1,1Stage3:3DResBlock#384,2,1 Feedforward network\nLinear ProjectionLayer Norm\nLinear ProjectionDropout0.1\nDropout0.1\n3DResBlock#384,1,1\n3DResBlock#192,1,1 3D ConvF,3,S3D ConvF,3,S…N\n3DResBlock#F,S,NSkip connection\n384×7776\n1536×7776\n384×7776\nFig. 4.Detailed architecture of CNN-encoder, decoder and feed forward network. Blue\n‘Conv’: Conv-In-Relu block that contains a 3D convolutional layer followed by an in-\nstance normalization (IN) layer [18] and ReLU activation; Gray ‘Conv’: a 3D convo-\nlutional layer; Orange ‘TransConv’: 3D transposed convolutional layer. Note that the\nnumbers in each Conv block / layer indicate the number of ﬁlters, kernel size, and\nstride, respectively. The numbers in each residual block indicate the number of ﬁlters,\nstride, and Conv blocks, respectively.\n7.2 Loss function\nWe jointly use the Dice loss and cross-entropy loss for optimization, which is\npopular in many medical image segmentation applications and has achieved\n12 Yutong Xie 1,2, Jianpeng Zhang1,2⋆, Chunhua Shen2, Yong Xia\f1\nprominent success [16,24,28]. The loss function is formulated as\nL= 1\nc\nc∑\nn=1\n{\n− 2 ∑˜ynyn + ε∑(˜yn + yn) + ε −E[ynlog˜yn]\n}\n(5)\nwhere the ﬁrst item is the soft Dice loss, the second item is the cross-entropy\nloss, the prediction and ground truth are denoted by ˜ y and y, respectively, E\nis the expectation operation, ε is a smoothing factor, and c is the number of\ncategories. To speed up convergence and alleviate the vanishing gradient prob-\nlem, we also use the deep supervision strategy that adds auxiliary losses to the\ndecoder outputs with diﬀerent resolutions. The total loss function is the sum of\nthe losses at all resolutions.\n7.3 Visualization\nThe segmentation results produced by (1) SETR with pre-trained ViT-B/16,\n(2) replacing DeTrans-encoder with ASPP module, (3) 3D TransUNet, and (4)\nour CoTr, were visually compared in Fig. 5. We can see that: 1) comparing to\nthe pure Transformer encoder method (SETR) and pure CNN encoder method\n(ASPP), our CoTr with the hybrid CNN-Transformer encoder is able to produce\nthe segmentation results that are more similar to the ground truth, and 2) our\nCoTr are more likely to produce less false positives compared to TransUNet,\nwhich conﬁrms the superiority of our 3D deformable Transformer over vanilla\nTransformer.\nCoTr: Bridging CNN and Transformer for 3D Medical Image Segmentation 13\n81.381.882.2\n8181.481.882.282.6\n1 2 4\nDice(%)\nK\n81.581.982.2\n80.881.281.68282.4\n2 4 6\nDice(%)\nHeads\n81.281.782.2\n8181.58282.5\n2 4 6\nDice(%)\nLayers\n81.0\n82.2\n80.581.081.582.082.5\nSSMS\nDice(%)\nSS:Single-scaleMS:Multi-scale (a) (b) (c) (d)\nOriginalimages\nGT\nSETR(pre)\nTransUNet\nASPP\nCoTr\nFig. 5.Visualization of segmentation results of four cases. The regions in yellow rect-\nangles indicate our superiority. Each type of organs are denoted by a unique color.",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7768746614456177
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6689972877502441
    },
    {
      "name": "Segmentation",
      "score": 0.641399085521698
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6063045263290405
    },
    {
      "name": "Locality",
      "score": 0.4982025623321533
    },
    {
      "name": "Transformer",
      "score": 0.49647337198257446
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4765251874923706
    },
    {
      "name": "Image segmentation",
      "score": 0.4330994486808777
    },
    {
      "name": "Engineering",
      "score": 0.07261413335800171
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "topic": "Computer science"
}