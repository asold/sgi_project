{
  "title": "Ask2Transformers: Zero-Shot Domain labelling with Pretrained Language Models",
  "url": "https://openalex.org/W3118364895",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4223316880",
      "name": "Sainz, Oscar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742554330",
      "name": "Rigau, German",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2970200208",
    "https://openalex.org/W12472515",
    "https://openalex.org/W2517456239",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2079478118",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2740540529",
    "https://openalex.org/W2038721957",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2251746691",
    "https://openalex.org/W2161103800",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963846996"
  ],
  "abstract": "In this paper we present a system that exploits different pre-trained Language Models for assigning domain labels to WordNet synsets without any kind of supervision. Furthermore, the system is not restricted to use a particular set of domain labels. We exploit the knowledge encoded within different off-the-shelf pre-trained Language Models and task formulations to infer the domain label of a particular WordNet definition. The proposed zero-shot system achieves a new state-of-the-art on the English dataset used in the evaluation.",
  "full_text": "Ask2Transformers: Zero-Shot Domain labelling with Pre-trained\nLanguage Models\nOscar Sainzand German Rigau\nHiTZ Center - Ixa Group,\nUniversity of the Basque Country (UPV/EHU)\n{oscar.sainz, german.rigau}@ehu.eus\nAbstract\nIn this paper we present a system that ex-\nploits different pre-trained Language Mod-\nels for assigning domain labels to WordNet\nsynsets without any kind of supervision. Fur-\nthermore, the system is not restricted to use a\nparticular set of domain labels. We exploit the\nknowledge encoded within different off-the-\nshelf pre-trained Language Models and task\nformulations to infer the domain label of a\nparticular WordNet deﬁnition. The proposed\nzero-shot system achieves a new state-of-the-\nart on the English dataset used in the evalua-\ntion.\n1 Introduction\nThe whole Natural Language Processing (NLP)\nresearch area have been accelerated with the ad-\nvent of the unsupervised pre-trained Language\nModels. First with ELMo (Peters et al., 2018) and\nthen with BERT (Devlin et al., 2019) the paradigm\nof using pre-trained Language Models for ﬁne-\ntuning on a particular NLP task has became the\nnew standard approach, replacing the more tradi-\ntional knowledge-based and fully supervised ap-\nproaches. Currently, as the size of the corpus and\nmodels increase, the research community has ob-\nserved that the Transfer Learning approach has the\ncapacity to work without any or with a very small\nﬁne-tuning. Some examples of the strength of this\napproach are GPT-2 (Radford et al., 2019) or more\nrecently GPT-3 (Brown et al., 2020) that shows the\nability of these huge pre-trained Language Models\nto solve tasks for which have not even trained.\nRecently, with the arrival of the GPT-3 new\nways to perform zero and few shot approaches\nhave been discovered. These approaches propose\nthe inclusion of a small number of supervised ex-\namples in the input as a hint for the model. The\nmodel then, just by looking a small set of exam-\nples, is able to complete successfully the task at\nhand. Brown et al. (2020) report that they solve a\nwide range of NLP tasks just following the previ-\nous approach. However, this approach only looks\nappropriate when the model is large enough.\nIn this paper we exploit the domain knowl-\nedge already encoded within the existing pre-\ntrained Language Models to enrich the WordNet\n(Miller, 1998) synsets and glosses with domain\nlabels. We explore and evaluate different pre-\ntrained Language Models and pattern objectives.\nFor instance, consider the example shown in Ta-\nble 1. Given a WordNet deﬁnition such as the one\nof <hospital, inﬁrmary > and the knowledge en-\ncoded in a pre-trained Language Model, the task\nis to assess which is its most suitable domain la-\nbel. Thus, we create an appropriate pattern in nat-\nural language adapted to the objective of the Lan-\nguage Model. In the example, we use a Language\nModel ﬁne-tuned on a general task such as Nat-\nural Language Inference (NLI) (Bowman et al.,\n2015). The NLI objective is to train a model able\nto classify the relation between two sentences as\nentailment, contradiction or neutral. Having four\ndomains such as medicine, biology, business and\nculture, our system performs four queries to the\nmodel, each one with one of the four domains.\nEach query takes as a ﬁrst sentence the WordNet\ndeﬁnition and as a second sentence The domain\nof the sentence is about [domain-label]. As ex-\npected, the most suitable domain label in this ex-\nample is medicine with a conﬁdence of 0.77. As\nshown, an off-the-shelf Language Model which\nhave been ﬁne-tuned on a general NLI task is able\nto infer the most appropriate domain label for the\nWordNet deﬁnition without any further training.\nAlso note that the approach can use any given set\nof domain labels.\nInterestingly, without any training on the task at\nhand, the proposed zero-shot system obtains an F1\nscore of 92.4% on the English dataset used in the\narXiv:2101.02661v2  [cs.CL]  29 Jan 2021\nevaluation.\nAll the implementation code along with the ex-\nperiments is freely available on a GitHub reposi-\ntory 1.\nAfter this short introduction, the next section\npresents previous work on domain labelling of\nWordNet. Section 3 presents our approach, Sec-\ntion 4 the experimental setup and Section 5 the\nresults from our experiments. Finally, Section 6\nrevises the main conclusions and the future work.\n2 Related Work\nBuilding large and rich lexical knowledge bases is\na very costly effort which involves large research\ngroups for long periods of development. Starting\nfrom version 3.0, Princeton WordNet has associ-\nated topic information with a subset of its synsets.\nThis topic labeling is achieved through pointers\nfrom a source synset to a target synset representing\nthe topic. WordNet uses 440 topics and the most\nfrequent one is <law, jurisprudence>.\nIn order to reduce the manual effort required,\na few semi-automatic and fully automatic meth-\nods have been applied for associating domain la-\nbels to synsets. For instance, WordNet Domains 2\n(WND) is a lexical resource where synsets have\nbeen semi-automatically annotated with one or\nmore domain labels from a set of 165 hierarchi-\ncally organized domains (Magnini, 2000; Ben-\ntivogli et al., 2004). The uses of WND include\nthe possibility to reduce the polysemy degree of\nthe words, grouping those senses that belong to the\nsame domain (Magnini et al., 2002). But the semi-\nautomatic method used to develop this resource\nwas far from being perfect. For instance, the noun\nsynset <diver, frogman, underwater diver > de-\nﬁned as some-one who works underwater has do-\nmain history because it inherits from its hyper-\nnym <explorer, adventurer > also labelled with\nhistory. Moreover, many synsets have been la-\nbelled as factotum meaning that the synset cannot\nbe labelled with a particular domain. WND also\nprovides mappings to WordNet Topics and also to\nWikipedia categories.\neXtended WordNet Domains 3 (XWND)\n(Gonzalez-Agirre et al., 2012; Gonz ´alez et al.,\n2012) applied a graph-based method to propagate\nthe WND labels through the WordNet structure.\n1https://github.com/osainz59/\nAsk2Transformers\n2http://wndomains.fbk.eu/\n3https://adimen.si.ehu.es/web/XWND\nDomain information is also available in other\nlexical resources. For instance, IATE4, a European\nUnion inter-institutional terminology database.\nThe domain labels of IATE are based on the Eu-\nrovoc thesaurus5 and were introduced manually.\nMore recently, BabelDomains 6 (Camacho-\nCollados and Navigli, 2017) propose an automatic\nmethod that propagates the knowledge categories\nfrom the Wikipedia to WordNet by exploiting\nboth distributional and graph-based clues. As do-\nmains of knowledge, BabelDomains opted for do-\nmains from the Wikipedia featured articles page7.\nThis page contains a set of thirty-two domains\nof knowledge. When labelling WordNet synsets\nwith these domains, BabelDomains reports a pre-\ncision of 81.7, a recall of 68.7 and an F1 score\nof 74.6. Unfortunately, as these numbers sug-\ngest not all WordNet synsets have been labelled\nwith a domain. For instance, the synset <hospital,\ninﬁrmary> with a gloss deﬁnition a health facility\nwhere patients receive treatment has no Babeldo-\nmain assigned.\nIt is worth to note that all these methods de-\npart from a particular set of domain labels (or cat-\negories) manually assigned to a set of WordNet\nsynsets (or Wikipedia pages). Then, these labels\nare propagated through the WordNet structure fol-\nlowing automatic or semi-automatic methods. In\ncontrast, our zero-shot method does not require\nan initial manual annotation. Furthermore, it is\nnot designed for a particular set of domain labels.\nThat is, it can be applied to label from scratch any\ndictionary or lexical knowledge base (or wordnet)\nwith distinct sets of domain labels.\n3 Using pre-trained LMs for domain\nlabelling\nRecent studies such as the one of GPT-3 (Brown\net al., 2020) shows that when increasing the size\nof the model, the capacity to solve different tasks\nwith just a few positive examples also increases\n(few-shot learning). However, very large Lan-\nguage Models also have important hardware re-\nquirements (i.e. large RAM GPUs). Thus, we de-\ncided to keep the size of the models used manage-\n4http://iate.europa.eu/\n5https://op.europa.eu/en/web/\neu-vocabularies/th-dataset/-/resource/\ndataset/eurovoc\n6http://lcl.uniroma1.it/babeldomains/\n7https://en.wikipedia.org/wiki/\nWikipedia:Featured_articles\nDeﬁnition: hospital: a health facility where patients receive treatment.\nPattern: The domain of the sentence is about medicine 0.77\nbiology 0.08\nbusiness 0.04\nculture 0.02\nTable 1: An example of domain labelling.\nable with small hardware requirements.\nThe task where we focused on is the domain\nlabelling of WordNet glosses. This task con-\nsist in the following. Given a WordNet gloss\ng to predict the corresponding domain d of the\nWordNet concept deﬁned. In this paper, the do-\nmains are taken from BabelDomains (Camacho-\nCollados and Navigli, 2017). Supervised domain\nlabelling can be solved as any other multiclass\nproblem, where the output of the model is a class\nprobability distribution. In our zero-shot experi-\nments we did not modify any of the pre-trained\nmodels. We just reformulate the domain labelling\ntask to match with the LMs training objective.\n3.1 Masked Language Modeling\nThe Masked Language Modeling (MLM) is a pre-\ntraining objective followed by models such as\nBERT (Devlin et al., 2019) and RoBERTa (Liu\net al., 2019). This objective works as follows.\nGiven a sequence of tokens s = [t1, t2, ..., tn], the\nsequence is ﬁrst perturbed by replacing some of\nthe tokens t with an special token [MASK]. Then,\nthe model is trained to recover the original se-\nquence s given the modiﬁed sequence ˆs. This de-\nnoising objective can be seen as an evolution for\nthe contextual embeddings of the previous CBOW\nfrom word2vec (Mikolov et al., 2013).\nFor domain labelling, we have replaced the in-\nput for the model following the next pattern:\ns: Context: [context] Topic: [MASK]\nwhere we introduce the input sentence replacing\nthe [context] tag. Then, we let the model predict\nthe most probable token for the [MASK] tag. For\ninstance, given the biological deﬁnition ofcell, the\nmodel returns the following topics: Biology, evo-\nlution, life, etc.\nThis approach has been used to explore the\nknowledge of the model without any predeﬁned\nset of domain labels in Section 5.7.\n3.2 Next Sentence Prediction\nAlong with the MLM the Next Sentence Predic-\ntion (NSP) is the training objective used by the\nBERT models. Given a pair of sentences s1 and\ns2, this objective predicts whether s1 is followed\nby s2 or not.\nTo adapt the BERT objective to the domain la-\nbelling task, we propose the next strategy inspired\nin the work from Yin et al. (2019). We use the\nfollowing input pattern:\ns1: [context]\ns2: Domain or topic about [domain-\nlabel]\nwhere s1 encodes a WordNet gloss as a context\nand s2 is formed by atemplate and a domain-label.\nIn order to make the classiﬁcation, we run as many\ntimes as domain labels and then apply a softmax\nover the positive class outputs. We hypothesize\nthat, no matter if any of the s2 can really follow\nthe given s1, the most probable one should be the\ns2 formed by the correct label. For instance, recall\nthe hospital example shown in Table 1.\n3.3 Natural Language Inference\nIn this case, we use a pre-trained LM that has been\nﬁne-tuned for a general inference task which is\nthe Natural Language Inference (Williams et al.,\n2018a). Given two sentences in the form of a\npremise s1 and an hypothesiss2, the NLI task con-\nsists on redicting whether the s1 entails or contra-\ndicts s2 or if the relation between both is neutral.\nWe also used the input pattern shown in the pre-\nvious NSP approach to adapt the NLI models to\nthe domain labelling task. In this case, we just use\nthe predictions of theentailment class. The predic-\ntions of the contradiction and neutral are not used.\nAs in the previous case, no matter if any of the s2\nhypothesis entails the premise s1 or not, the most\nprobable entailment should be the correct domain\nlabel. For example, consider again the example\npresented in Table 1.\n4 Experimental setting\nThis section describes our experimental setup. We\nintroduce the pre-trained Language Models and\nthe dataset used. For the case of the Language\nModels, we have tested BERT (Devlin et al.,\n2019), RoBERTa (Liu et al., 2019) and BART\n(Wang et al., 2019). For the dataset, we have\nused the one released by Camacho-Collados et al.\n(2016) based on WordNet.\n4.1 Pretrained models\nAll the Language Models have been obtained from\nthe Huggingface Transformers library (Wolf et al.,\n2019).\nMLM For the objective we have used roberta-\nlarge and roberta-base checkpoints. These mod-\nels have obtained state-of-the-art results on many\nNLP tasks and benchmarks.\nNSP For this objective we use the BERT mod-\nels as they are the only ones trained on that ob-\njective. For the sake of comparing the perfor-\nmance of more than one model of each objective\nwe have selected the bert-large-uncased and bert-\nbase-uncased checkpoints. They only differ on the\nsize of the Language Model.\nNLI For this objective we used a checkpoint\nbased on RoBERTa roberta-large-mnli which\nhave been ﬁne-tuned with MultiNLI (Williams\net al., 2018b). We also include bart-large-mnli for\ntesting a generative model.\n4.2 Dataset\nWe evaluate our approaches on a dataset derived\nfrom WordNet which have been annotated with\nBabeldomain labels (Camacho-Collados et al.,\n2016). This dataset consist of 1540 synsets man-\nually annotated with their corresponding Babeldo-\nmain label. The distribution of domain labels in\nthe dataset is shown in Figure 1. Note that the\ndataset is quite unbalanced. In fact, some impor-\ntant domains such as Transport and travelor Food\nand drink have no single labelled example. As our\nsystem is unsupervised, we use the whole dataset\nfor testing.\n5 Evaluation and Results\nThis section presents a quantitative and qualita-\ntive evaluation. One the one hand, the quantita-\nFigure 1: Distribution of domains in the WordNet\ndataset.\nMethod Top-1 Top-3 Top-5\nMNLI (roberta-large-mnli)78.44 87.46 89.74\nMNLI (bart-large-mnli) 61.81 79.85 87.59\nNSP (bert-large-uncased) 2.07 8.57 16.49\nNSP (bert-base-uncased) 2.85 10.32 16.88\nTable 2: Top-K accuracy of different approaches.\ntive evaluation has been done incrementally in or-\nder to obtain the best-performing system. First,\nwe have evaluated the different alternative models\nusing the same objective pattern. Then, once the\nbest approach was selected we have explored al-\nternative patterns using the best model. When the\nbest performing pattern was discovered we have\nfocus on ﬁnding a better label representation. Fi-\nnally, we have compared our best system against\nthe previous state-of-the-art methods.\nOn the other hand, as one of our system is based\non a generative approach (MLM) the applied re-\nstrictions may not show the real performance of\nthe method. So, we decided to at least do an small\nqualitative review of the approach.\n5.1 Approach comparison\nTable 2 shows the Top-1, Top-3 and Top-5 accu-\nracy of each system when using the same objective\npattern. To understand better the behaviour of the\nsystems we also present in the Figure 2 the Top-K\nFigure 2: Top-K accuracy curve of the different ap-\nproaches and a random classiﬁer baseline.\naccuracy curve comparing all the approaches and\na random baseline. As expected the systems that\nfollow the same approaches perform similarly and\nshare a similar curve. The best performing system\nis the MNLI based roberta-large-mnli, followed\nby the bart-large-mnli checkpoint. We observe a\nlarge difference between the different models. For\ninstance, the models pre-trained on the NLI task\nperform much better than those pre-trained on the\ngeneral NSP task. The NSP approaches perform\nslightly better than the random classiﬁer which can\nbe a signal of a non appropriated objective model\nto use.\n5.2 Input representation\nOnce selected the pre-trained Language Model,\nwe evaluate different input patterns for the\nroberta-large-mnli checkpoint. As mentioned be-\nfore, the MNLI approaches follow the same struc-\nture as NSP, wheres1 is the gloss of the synset and\ns2 the sequence formed by a textual template plus\nthe label.\nTable 3 shows the results obtained by testing\ndifferent textual patterns. Very short patterns ob-\ntain low results. The best performing textual tem-\nplate is obtained with The domain of the sentence\nis about [label].\n5.3 Label descriptors / Mapping\nAs important as the input patterns is the set of do-\nmain labels used. Actually, BabelDomains uses\nlabels that refers to one or several speciﬁc do-\nmains. For instance, Art, architecture and archae-\nology. Although these coarse-grained labels can\nbe useful when clustering close-related domains,\nwe also implemented a two-step labelling proce-\ndure taking into account those speciﬁc domains.\nFirst, we run the system over a set of speciﬁc do-\nmains or descriptors. Second, we apply a function\nthat maps the descriptors to the original BabelDo-\nmains.\nDescriptors The descriptors deﬁned in this\nwork are quite simple. Given a composed domain\nlabel such us Art, architecture and archaeology ,\nwe deﬁne the set of descriptors as each of the com-\nponents of the label. For instance, in this case Art,\nArchitecture and Archaeology. In the case of la-\nbels that consist on a single domain, the descrip-\ntors are just the labels. For example, in the case of\nMusic the descriptor is also Music.\nMapping function The mapping function that\nwe use in this work consists on taking the\nmaximum result of the descriptors as the re-\nsult of the original domain label, i.e. li =\nmax(di1, di2, ..., din).\n5.4 Training a specialized student\nThe inference time increases linearly with the\nnumber of labels. That is, for each example we\nneed to test all the different domain labels. To\nspeed-up the labelling process we annotate au-\ntomatically the rest of WordNet glosses (around\n79.000 glosses) using our best zero-shot approach.\nThen, we use that automatically annotated dataset\nto train a much smaller Language Model for the\ntask. For instance, to label new deﬁnitions or\nnew lexicons. We have ﬁne-tuned two different\nmodels, the ﬁrst one based with DistilBert (Sanh\net al., 2019) which is 5 times smaller than the\nroberta-large-mnli and a XLM-RoBERTa (Con-\nneau et al., 2020) base which is 2 times smaller\nand is trained in a multilingual fashion. We called\nthem A2T FT-small and A2T FT-xlingual respectively.\nThe ﬁrst one achieve a x425 faster inference (5\ntimes smaller and 85 times less inferences) while\nthe second one a speed boost of x170.\n5.5 Results\nIn order to know how good is our ﬁnal approach\nwe compare our new systems with the previous\nones. The results are reported on the Table 4 in\nterms of Precision, Recall and F1 for comparison\npurposes. We also include the results from two\nprevious state-of-the-art systems. As we can see,\nthe new systems based on pre-trained Language\nModels obtain much better performance (from a\nInput pattern Top-1 Top-3 Top-5\nTopic: [label] 59.61 69.48 74.02\nDomain: [label] 58.50 67.40 72.27\nTheme: [label] 59.67 73.96 81.36\nSubject: [label] 60.58 69.74 74.35\nIs about [label] 73.37 87.72 91.94\nTopic or domain about [label] 78.44 87.46 89.74\nThe topic of the sentence is about [label] 80.71 92.92 95.77\nThe domain of the sentence is about [label] 81.62 93.96 96.42\nThe topic or domain of the sentence is about [label] 76.62 88.63 91.23\nTable 3: Some of the explored input patterns for the MNLI approach and their Top-1, Top-3 and Top-5 accuracy.\nprevious best result with an F1 of 74.6 to the\nnew one of 82.10). We also obtain an small im-\nprovement when establishing a threshold to decide\nwhether a prediction is taken into consideration\nor not. Our system performs slightly better with\na conﬁdence score greater than 5% (A2T (> 0.05)).\nFigure 3 reports the Precision/Recall trade-off of\nthe A2T system. As mentioned before labels com-\nposed of multiple domains can make the predic-\ntion harder for the zero-shot system. As a result, a\nsimple system using the label descriptors boosts\nthe performance of the system reaching a ﬁnal\n92.14 F1 score (A2T + descriptors). Finally, we also\ninclude the results of both the ﬁne-tuned student\nversions which still obtain very competitive results\nwhile drastically reducing the inference time of the\noriginal models.\nMethod Precision Recall F1\nDistributional 84.0 59.8 69.9\nBabelDomains 81.7 68.7 74.6\nA2T 81.62 81.62 81.62\nA2T(> 0.05) 83.20 81.03 82.10\nA2T+ descriptors 92.14 92.14 92.14\nA2TFT-small 91.42 91.42 91.42\nA2TFT-xlingual 90.58 90.58 90.58\nTable 4: Micro-averaged precision, recall and F1 for\neach of the systems. Distributional (Camacho-Collados\net al., 2016) and BabelDomains (Camacho-Collados\nand Navigli, 2017) measures are the ones reported by\nthem.\n5.6 Error analysis\nFigure 4 presents the confusion matrix of our best\nsystem. The matrix is row wise normalized due\nFigure 3: Precision/Recall trade-off of A2T system.\nAnnotations indicates the probability thresholds.\nto the imbalance of the dataset label distribution.\nLooking at the ﬁgure there are 4 classes that are\nmisleading. The ”Animals” domain is confused\nwith the related domains ”Biology” and ”Food\nand drink”. For instance, this is the case of the\nsynset <diet> with the deﬁnition the usual food\nand drink consumed by an organism (person or\nanimal) which is labelled by our system as ”Food\nand drink”. The ”Games and video games” do-\nmain is confused with the related domain ”Sport\nand recreation”. For example the sense referring\nto game: a single play of a sport or other contest;\n”the game lasted two hours” which is labelled by\nour system as ”Sport and recreation”. The third\none, ”Heraldry, honors and vexillology” is also\nconfused with a very close domain ”Royalty and\nnobility”. Obviously, close-related domains can\nbe very difﬁcult to distinguish even for humans.\nFor example, the sense <audio cd, audio compact\ndisc> annotated in the gold standard as ”Music”\nis labelled by our system as ”Media”. Finally,\nSynset cell phase space rounding error wipeout\nLabel Biology Physics and astronomy Mathematics Sports and Recreation\nTop Biology EOS rounding sports\npredictions EOS physics EOS EOS\nbiology Physics math sport\nevolution geometry taxes accident\nlife relativity Math Sports\nTable 5: Top predictions of the MLM approach using the roberta-large checkpoint.\nFigure 4: Rowise normalized confusion matrix of the\nA2T+ descriptors system.\nsometimes the ”History” domain is confused with\n”Food and drink”. A curious example of this case\nis the sense referring to the history event <Boston\ntea party> that is labelled as ”Food and drink”.\n5.7 Qualitative analysis\nTable 5 shows some of the top predictions ob-\ntained by a Masked Language Model (MLM) and\nthe real label for 4 different synsets. In this case,\nthe system is guessing its best predicted domain.\nThat is, the system is not restricted to a select the\nbest label from a pre-deﬁned set of domain labels.\nNow, the system is free to return the word that best\nﬁt the masked term.\nWe can see in the table that the predictions of\nthe model are close to the correct label although\nnot always equal. Sometimes because of a differ-\nent case. They can also be seen as ﬁne-grained\ndomains or domain keywords of the real domain.\n6 Conclusions and Future Work\nIn this paper we have explored some approaches\nfor domain labelling of WordNet glosses by ex-\nploiting pre-trained LM in a zero-shot manner. We\nhave presented a simple approach that achieves a\nnew state-of the art on the Babeldomain dataset.\nEven if we have focused on domain labelling of\nWordNet glosses, our method seems to be robust\nenough to be adapted to work on tasks such as Sen-\ntiment Analysis or other type of text classiﬁcation.\nIn particular, we think that the approach can be\nvery useful when no annotated data is available.\nFor the future, we have considered three main\nobjectives. First, we plan to apply this approach\nto other sources of domain information such as\nWordNet topics and WordNet Domains. We will\nalso explore how to deal with deﬁnitions with\ngeneric domains (with no BabelDomains labels or\nwith WordNet Domains factotum label). Second,\nwe also aim to explore the cross-lingual capabil-\nities of pre-trained Language Models for domain\nlabelling of non-English wordnets and other lexi-\ncal resources. Finally, we also plan to explore the\nutility of these ﬁndings in the Word Sense Disam-\nbiguation task.\nAcknowledgments\nThis work has been funded by the Spanish Min-\nistry of Science, Innovation and Universities under\nthe project DeepReading (RTI2018-096846-B-\nC21) (MCIU/AEI/FEDER,UE) and by the BBV A\nBig Data 2018 “BigKnowledge for Text Mining\n(BigKnowledge)” project. We also acknowledge\nthe support of the Nvidia Corporation with the do-\nnation of a GTX Titan X GPU used for this re-\nsearch.\nReferences\nLuisa Bentivogli, Pamela Forner, Bernardo Magnini,\nand Emanuele Pianta. 2004. Revising the wordnet\ndomains hierarchy: semantics, coverage and balanc-\ning. In Proceedings of the workshop on multilingual\nlinguistic resources, pages 94–101.\nSamuel Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nJose Camacho-Collados and Roberto Navigli. 2017.\nBabelDomains: Large-scale domain labeling of lex-\nical resources. In Proceedings of the 15th Confer-\nence of the European Chapter of the Association\nfor Computational Linguistics: Volume 2, Short Pa-\npers, pages 223–228, Valencia, Spain. Association\nfor Computational Linguistics.\nJos´e Camacho-Collados, Mohammad Taher Pilehvar,\nand Roberto Navigli. 2016. Nasari: Integrating ex-\nplicit knowledge and corpus statistics for a multilin-\ngual representation of concepts and entities. Artiﬁ-\ncial Intelligence, 240:36 – 64.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAitor Gonz ´alez, German Rigau, and Mauro Castillo.\n2012. A graph-based method to improve wordnet\ndomains. In International Conference on Intelli-\ngent Text Processing and Computational Linguistics,\npages 17–28. Springer.\nAitor Gonzalez-Agirre, Mauro Castillo, and German\nRigau. 2012. A proposal for improving wordnet do-\nmains. In LREC, pages 3457–3462.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nB Magnini. 2000. G. cavagli a. integrating subject\nﬁeld codes into wordnet. In Proceedings of LREC-\n2000, 2nd International Conference on Language\nResources and Evaluation, pages 1413–1418.\nBernardo Magnini, Carlo Strapparava, Giovanni Pez-\nzulo, and Alﬁo Gliozzo. 2002. The role of domain\ninformation in word sense disambiguation. Natural\nLanguage Engineering, 8(4):359–373.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efﬁcient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nGeorge A Miller. 1998. WordNet: An electronic lexical\ndatabase. MIT press.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nLiang Wang, Wei Zhao, Ruoyu Jia, Sujian Li, and\nJingming Liu. 2019. Denoising based sequence-to-\nsequence pre-training for text generation. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP) , pages 4003–4015,\nHong Kong, China. Association for Computational\nLinguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018a. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1112–1122. Association for\nComputational Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018b. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1112–1122.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nWenpeng Yin, Jamaal Hay, and Dan Roth. 2019.\nBenchmarking zero-shot text classiﬁcation:\nDatasets, evaluation and entailment approach.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP) , pages\n3914–3923, Hong Kong, China. Association for\nComputational Linguistics.",
  "topic": "WordNet",
  "concepts": [
    {
      "name": "WordNet",
      "score": 0.8930937051773071
    },
    {
      "name": "Exploit",
      "score": 0.8686672449111938
    },
    {
      "name": "Computer science",
      "score": 0.8404333591461182
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.704541802406311
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6511685848236084
    },
    {
      "name": "Natural language processing",
      "score": 0.6208701133728027
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5989081859588623
    },
    {
      "name": "Task (project management)",
      "score": 0.598410964012146
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.5480554103851318
    },
    {
      "name": "Shot (pellet)",
      "score": 0.5253614187240601
    },
    {
      "name": "Labelling",
      "score": 0.4862912595272064
    },
    {
      "name": "Language model",
      "score": 0.474281370639801
    },
    {
      "name": "Training set",
      "score": 0.4201992154121399
    },
    {
      "name": "Programming language",
      "score": 0.10266757011413574
    },
    {
      "name": "Mathematics",
      "score": 0.09141099452972412
    },
    {
      "name": "Linguistics",
      "score": 0.07938113808631897
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Criminology",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}