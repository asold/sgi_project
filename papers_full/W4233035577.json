{
  "title": "Evaluating the performance of tools used to call minority variants from whole genome short-read data",
  "url": "https://openalex.org/W4233035577",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A5090004393",
      "name": "Khadija Said Mohammed",
      "affiliations": [
        "James Hutton Institute",
        "Kenya Medical Research Institute",
        "Pwani University"
      ]
    },
    {
      "id": "https://openalex.org/A5068783967",
      "name": "Nelson Kibinge",
      "affiliations": [
        "Kenya Medical Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5028085028",
      "name": "Pjotr Prins",
      "affiliations": [
        "Kenya Medical Research Institute",
        "University Medical Center Utrecht"
      ]
    },
    {
      "id": "https://openalex.org/A5020679457",
      "name": "Charles N. Agoti",
      "affiliations": [
        "James Hutton Institute",
        "Kenya Medical Research Institute",
        "Pwani University"
      ]
    },
    {
      "id": "https://openalex.org/A5050782615",
      "name": "Matthew Cotten",
      "affiliations": [
        "Erasmus University Rotterdam"
      ]
    },
    {
      "id": "https://openalex.org/A5087452041",
      "name": "D. James Nokes",
      "affiliations": [
        "University of Warwick",
        "Kenya Medical Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5042065599",
      "name": "Samuel P. C. Brand",
      "affiliations": [
        "University of Warwick"
      ]
    },
    {
      "id": "https://openalex.org/A5030465292",
      "name": "George Githinji",
      "affiliations": [
        "Kenya Medical Research Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2073918065",
    "https://openalex.org/W2045979783",
    "https://openalex.org/W1565378970",
    "https://openalex.org/W2148686896",
    "https://openalex.org/W2567652848",
    "https://openalex.org/W136988059",
    "https://openalex.org/W1925040232",
    "https://openalex.org/W2157539385",
    "https://openalex.org/W2190569576",
    "https://openalex.org/W2152241467",
    "https://openalex.org/W2161815151",
    "https://openalex.org/W1590169480",
    "https://openalex.org/W2155943701",
    "https://openalex.org/W2304422002",
    "https://openalex.org/W2323851578",
    "https://openalex.org/W1973012451",
    "https://openalex.org/W2163314170",
    "https://openalex.org/W2397977624",
    "https://openalex.org/W4398860893",
    "https://openalex.org/W1990653767",
    "https://openalex.org/W2168440186",
    "https://openalex.org/W2236879788",
    "https://openalex.org/W2590415818",
    "https://openalex.org/W1970438376",
    "https://openalex.org/W2170191171",
    "https://openalex.org/W2003574450",
    "https://openalex.org/W2125375614",
    "https://openalex.org/W1964807436",
    "https://openalex.org/W2951580378",
    "https://openalex.org/W2064930197",
    "https://openalex.org/W2338906383",
    "https://openalex.org/W1809466148"
  ],
  "abstract": "<ns4:p><ns4:bold>Background: </ns4:bold>High-throughput whole genome sequencing facilitates investigation of minority sub-populations from virus positive samples. Minority variants are useful in understanding within and between host diversity, population dynamics and can potentially help to elucidate person-person transmission chains. Several minority variant callers have been developed to describe the minority variants sub-populations from whole genome sequence data. However, they differ on bioinformatics and statistical approaches used to discriminate sequencing errors from low-frequency variants.</ns4:p><ns4:p> <ns4:bold>Methods: </ns4:bold>We evaluated the diagnostic performance and concordance between published minority variant callers used in identifying minority variants from whole-genome sequence data. The ART-Illumina read simulation tool was used to generate three artificial short-read datasets of varying coverage and error profiles from an RSV reference genome. The datasets were spiked with nucleotide variants at predetermined positions and frequencies. Variants were called using FreeBayes, LoFreq, Vardict, and VarScan2. The variant callers’ agreement in identifying known variants was quantified using two measures; concordance accuracy and the inter-caller concordance.</ns4:p><ns4:p> <ns4:bold>Results: </ns4:bold>The variant callers reported differences in identifying minority variants from the datasets. Concordance accuracy and inter-caller concordance were positively correlated with sample coverage. FreeBayes identified majority of the variants although it was characterised by variable sensitivity and precision in addition to a high false positive rate relative to the other minority variant callers and which varied with sample coverage. LoFreq was the most conservative caller.</ns4:p><ns4:p> <ns4:bold>Conclusions: </ns4:bold>We conducted a performance and concordance evaluation of four minority variant calling tools used to identify and quantify low frequency variants. Inconsistency in the quality of sequenced samples impact on sensitivity and accuracy of minority variant callers. Our study suggests that combining at least three tools when identifying minority variants is useful in filtering errors when calling low frequency variants.</ns4:p>",
  "full_text": "RESEARCH ARTICLE\nEvaluating the performance of tools used to call minority \nvariants from whole genome short-read data\n[version 1; peer review: 2 approved with reservations]\nKhadija Said Mohammed\n 1,2, Nelson Kibinge\n 2, Pjotr Prins2,3, \nCharles N. Agoti\n 1,2, Matthew Cotten\n 4, D.J. Nokes\n 2,5, Samuel Brand\n 5, \nGeorge Githinji\n 2\n1Pwani University, Kilifi, Kenya \n2KEMRI-Wellcome Trust Research Programme, KEMRI Centre for Geographic Medicine Research – Coast, Kilifi, Kenya \n3University Medical Center Utrecht, Utrecht, The Netherlands \n4Virosciences Department, Erasmus Medical Centre, Rotterdam, The Netherlands \n5School of Life Sciences and Zeeman Institute (SBIDER), University of Warwick, Coventry, UK \nFirst published: 05 Mar 2018, 3:21  \nhttps://doi.org/10.12688/wellcomeopenres.13538.1\nLatest published: 13 Sep 2018, 3:21  \nhttps://doi.org/10.12688/wellcomeopenres.13538.2\nv1\n \nAbstract \nBackground: High-throughput whole genome sequencing facilitates \ninvestigation of minority sub-populations from virus positive samples. \nMinority variants are useful in understanding within and between host \ndiversity, population dynamics and can potentially help to elucidate \nperson-person transmission chains. Several minority variant callers \nhave been developed to describe the minority variants sub-\npopulations from whole genome sequence data. However, they differ \non bioinformatics and statistical approaches used to discriminate \nsequencing errors from low-frequency variants. \nMethods: We evaluated the diagnostic performance and concordance \nbetween published minority variant callers used in identifying \nminority variants from whole-genome sequence data. The ART-\nIllumina read simulation tool was used to generate three artificial \nshort-read datasets of varying coverage and error profiles from an \nRSV reference genome. The datasets were spiked with nucleotide \nvariants at predetermined positions and frequencies. Variants were \ncalled using FreeBayes, LoFreq, Vardict, and VarScan2. The variant \ncallers’ agreement in identifying known variants was quantified using \ntwo measures; concordance accuracy and the inter-caller \nconcordance. \nResults: The variant callers reported differences in identifying \nminority variants from the datasets. Concordance accuracy and inter-\ncaller concordance were positively correlated with sample coverage. \nFreeBayes identified majority of the variants although it was \ncharacterised by variable sensitivity and precision in addition to a high \nfalse positive rate relative to the other minority variant callers and \nwhich varied with sample coverage. LoFreq was the most conservative \nOpen Peer Review\nApproval Status  \n  \n1 2\nversion 2\n(revision)\n13 Sep 2018\nview\n view\nversion 1\n05 Mar 2018\n view\n view\nBrad A. Chapman\n , Harvard Public School \nof Health, Boston, USA\n1. \nMicha M. Bayer\n , James Hutton Institute, \nDundee, UK\n2. \nAny reports and responses or comments on the \narticle can be found at the end of the article.\n \n Page 1 of 17\nWellcome Open Research 2018, 3:21 Last updated: 17 SEP 2025\nCorresponding author: George Githinji (GGithinji@kemri-wellcome.org)\nAuthor roles: Said Mohammed K: Conceptualization, Data Curation, Methodology, Software, Visualization, Writing – Original Draft \nPreparation, Writing – Review & Editing; Kibinge N: Conceptualization, Data Curation, Methodology, Writing – Review & Editing; Prins P: \nInvestigation, Writing – Review & Editing; Agoti CN: Investigation, Writing – Review & Editing; Cotten M: Investigation, Writing – Review \n& Editing; Nokes DJ: Conceptualization, Funding Acquisition, Investigation, Methodology, Project Administration, Writing – Review & \nEditing; Brand S: Formal Analysis, Methodology, Writing – Review & Editing; Githinji G: Conceptualization, Data Curation, Investigation, \nMethodology, Project Administration, Software, Supervision, Visualization, Writing – Review & Editing\nCompeting interests: No competing interests were disclosed.\nGrant information: The work was funded by the Wellcome Trust Senior Investigator Award to Prof D. James Nokes [102975] in addition, \nthis work was supported through the DELTAS Africa Initiative [DEL-15-003]. The DELTAS Africa Initiative is an independent funding \nscheme of the African Academy of Sciences (AAS)'s Alliance for Accelerating Excellence in Science in Africa (AESA) and supported by the \nNew Partnership for Africa's Development Planning and Coordinating Agency (NEPAD Agency) with funding from the Wellcome Trust \n[107769] and the UK government. The views expressed in this publication are those of the author(s) and not necessarily those of AAS, \nNEPAD Agency, Wellcome Trust or the UK government \nThe funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.\nCopyright: © 2018 Said Mohammed K et al. This is an open access article distributed under the terms of the Creative Commons \nAttribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly \ncited.\nHow to cite this article: Said Mohammed K, Kibinge N, Prins P et al. Evaluating the performance of tools used to call minority \nvariants from whole genome short-read data [version 1; peer review: 2 approved with reservations] Wellcome Open Research \n2018, 3:21 https://doi.org/10.12688/wellcomeopenres.13538.1\nFirst published: 05 Mar 2018, 3:21 https://doi.org/10.12688/wellcomeopenres.13538.1  \ncaller. \nConclusions: We conducted a performance and concordance \nevaluation of four minority variant calling tools used to identify and \nquantify low frequency variants. Inconsistency in the quality of \nsequenced samples impact on sensitivity and accuracy of minority \nvariant callers. Our study suggests that combining at least three tools \nwhen identifying minority variants is useful in filtering errors when \ncalling low frequency variants.\nKeywords \nvariant calling, minority variants, concordance, performance, RSV\n \nThis article is included in the KEMRI | Wellcome \nTrust gateway.\n  Page 2 of 17\nWellcome Open Research 2018, 3:21 Last updated: 17 SEP 2025\nIntroduction\nRNA viruses have been described as a population of closely  \nrelated sequences that arise from rapid genomic evolution  \ncoupled with a high replication and mutation rates \n(Domingo et al., 2012; Eigen et al., 1988; Holland et al., \n1992). Genetic changes in RNA viruses result from genetic \ndrift, erroneous replication processes and mutagenic  \nagents and upon which natural selection acts ( Moya et al., 2004). \nRapid replication and mutations generate an ensemble of mutant \ngenomes comprised of dominant and low frequency variants.  \nThis diversity has been shown to affect virus fitness landscape, \ntransmission, colonization, and replication (Henn et al., 2012; \nStack et al., 2013; Vignuzzi et al., 2006).\nA number of recent studies have demonstrated the potential  \napplication of virus diversity to inform person-to-person trans -\nmission during virus outbreaks (Henn et al., 2012; Poon  \net al., 2016; Stack et al., 2013). A number of methods that  \nincorporate both genomic and epidemiologic data to infer patho -\ngen transmission have recently been developed ( Worby et al.,  \n2017). These approaches rely partly on the accurate detection \nand quantification of minority variant populations from genomic  \nsamples.\nSeveral tools have been developed to identify and quantify  \nminority variants from short-read data ( Koboldt et al., 2009; \nKoboldt et al., 2012; Lai et al., 2016; Macalalad et al., 2012;  \nWilm et al., 2012; Yang et al., 2013). Nonetheless, these tools \ndo not fully account for errors or discrepancies that arise from  \nsample collection, pre-processing, sequencing in addition to \nerrors introduced during bioinformatic analysis. Rigorous quality  \ncontrol in sample processing and analysis is often suggested \nto distinguish true biological variants from artefactual variants  \n(Zhang & Flaherty, 2017). In some cases, sequencing errors can \nbe reduced by developing high-fidelity protocols and labora -\ntory quality control measures ( Kinde et al., 2011; McCrone &  \nLauring, 2016; Watson et al., 2013). Additionally, the uncer -\ntainty resulting from random sequencing errors can be countered \nby sequencing larger populations at higher coverage ( Zukurov  \net al\n., 2016).\nSeveral studies have  extensively explored  variants  from somatic \nor tumour samples ( Hofmann et al ., 2017 ; Koboldt et al .,  \n2012; Kroigard et al., 2016; Lai et al., 2016; Pabinger et al., 2014) \nand their application in clinical genomics, but  only a limited  \nnumber of studies have  explored the nature of variants  from  \npatient-derived samples that target  viral populations ( Henn  \net al., 2012; Macalalad et al., 2012; Wilm  et al., 2012; Y ang  \net al., 2013; Zukurov et al., 2016) and especially when calling  \nvariants from respiratory viruses such as the respiratory syncytial \nvirus (RSV).\nIn this study,  we evaluated  five published minority variant   \ndetection tools using both artifi cial and real RSV short-read data \nfrom a household community study.  We explored  their ability \nto detect and quantify minority variants  and assessed their  \noverall agreement which we defined using two  metrics, concord-\nance accuracy,  which measures the combined accuracy  of the   \nvariant callers and inter-caller  concordance, which is the size of  \nthe largest  set of variant  callers that agree at each position. We  \nshow that concordance metrics are dependent on sample coverage \nand are influenced by the quality of input data.\nMethods\nOverall, we considered eleven  published, open-source tools \nwith presumed ability to call minority variants  from virus deep  \nsequence data. Some of the callers were excluded from the analy-\nsis for various reasons including deprecation, ease of setup, \nand accepted input, among other reasons (\nTable 1 ). The \nfollowing  four tools were evaluated, FreeBayes version \n1.1.0-3-g961e5f3, LoFreq version 2.1.2, VarDict version \n30.3.17 and VarScan  version 2.4.2. A schematic diagram \nshowing the overall approach is shown in Figure 1.\nArtifi\ncial datasets\nThree artificial d atasets were generated based on an RSV  \nreference sequence (GenBank accession number KX510245.1) \nusing AR T-Illumina v ersion 2.5.8 (Huang et al., 2012). Each  \ndataset comprised of eight samples with v arying depth of  \ncoverage (20, 50, 100, 500, 1000, 2000, 5000, 10000) generated \nusing methods described in Supplementary File 1 , section S1.1. \nTable 1. A table of the exclusion criteria used to discard commonly used variant callers from the \nanalysis.\nCaller Exclusion Criteria\nGATK \nUnifiedGenotyper\nDeprecated\nGATK \nHaplotypeCaller\nAdapted for calling SNPs and genotypes from human genetic datasets. It has several \nparameters. Not easy to know which set of parameters are best suited for calling \nminor variants from haploid virus genomes.\nPlatypus Difficulties in installation and setup. It provided calls for some samples and not others.\nV-Phaser It has known bugs and was superseded by V-Phaser2 \nV-Phaser2 Could not handle reads aligned with BWA-MEM (version 0.7.13-r1126). It has some \nknown bugs and it is not actively maintained. \nSAMtools mpileup Does not provide direct allele frequencies.\nPage 3 of 17\nWellcome Open Research 2018, 3:21 Last updated: 17 SEP 2025\nFigure 1. A schematic diagram showing the variant calling workflow. The artificial datasets (BAM files) were generated using ART Illumina \nfor an RSV reference genome. BAMSurgeon was used to spike the resulting BAM files by inserting know variants at known locations across \nthe BAM file.\nThe first dataset did not contain an error profile while the second \nand third datasets comprised of artificial reads with an error pro -\nfile that was derived from sequenced RSV whole genome samples  \nand subjectively comprised of a “good” and a “bad” sam -\nple judged from FastQC metrics ( Supplementary File 2  and  \nSupplementary File 3 ) from the respective sequenced short  \nread data. For each sample, ART-Illumina artificial SAM files  \nwere converted to BAM format, sorted and indexed using  \nSAMtools version 1.3.1.\n166 randomly and uniformly generated nucleotide mutations \nat varying frequencies were inserted into each of the artificial  \ndatasets using BamSurgeon ( Ewing et al ., 2015 ), such that a  \nbase change was made amongst the reads at each alignment  \nposition as described in Supplementary File 1, section S1.2.\nVariant calling\nThe BAM files from each of the three datasets were used as input \nto each of the four variant callers (FreeBayes, Lofreq, VarDict \nand VarScan2). All output files were provided in the variant call  \nformat (VCF) or as a tabular file for the case of VarDict. Where \nmultithreading was possible, the tool was run in multithreaded  \nmode. For example, LoFreq calls were run in parallel using 10 \nthreads. The output from the VCF and tabular file was parsed  \nand written as a comma separated (CSV) files.\nPerformance measures\nTo evaluate the performance of the variant calling algorithms,  \nwe compared the sequence generated by each variant caller vc, \ndenoted 1,...,( { , , , }) ,vc vc\ni i NS S A C T G == ∈ , to the gold standard  \n“spiked” sequence, denoted Strue, at each of N=15205 nucle -\notide positions. The accuracy of each variant caller is the nor -\nmalized Hamming distance from the gold standard sequence,  \n1\n1 ( , ),\nN true v\ni ii d S SN =∑  where d(x,y) is the standard discrete metric  \ngiving 1 when x=y, and 0 otherwise. By distinguishing between  \nthe sets of positions where variants did and did not occur (shown \nin Table 2), in the gold standard sequence we calculated sensitivity, \nspecificity, precision and accuracy (Table 3).\nConcordance analysis\nWe defined two concordance metrics to present the level  \nof agreement between different callers in detecting the same \nvariant positions in the sequence. The first concordance metric is  \nconcordance accuracy, which measures the combined \nPage 4 of 17\nWellcome Open Research 2018, 3:21 Last updated: 17 SEP 2025\nTable 3. A breakdown of performance metrics of variant callers evaluated from first dataset that did not \nincorporate an error profile. The samples represent simulated datasets of varying depth of coverage. True positive \n(TP), true negative (TN), false positive (FP) and false negatives (FN) were used to calculate performance metrics of \neach caller. FPR – False positive rate.\nSample Caller TP TN FP FN Sensitivity Specificity Precision FPR Accuracy\n1 (20X) freebayes 87 15018 21 79 0.5241 0.9986 0.80556 0.0014 0.99342\nlofreq 29 15039 0 137 0.1747 1 1 0 0.99099\nvardict 72 15039 0 94 0.43373 1 1 0 0.99382\nvarscan 11 15039 0 155 0.06627 1 1 0 0.98981\n2 (50X) freebayes 118 14901 138 47 0.71515 0.99082 0.46094 0.00918 0.98783\nlofreq 67 15039 0 99 0.40361 1 1 0 0.99349\nvardict 108 15039 0 58 0.6506 1 1 0 0.99619\nvarscan 22 15039 0 144 0.13253 1 1 0 0.99053\n3 (100X) freebayes 40 15039 0 126 0.24096 1 1 0 0.99171\nlofreq 57 15039 0 109 0.34337 1 1 0 0.99283\nvardict 104 15038 1 62 0.62651 0.99993 0.99048 0.000067 0.99586\nvarscan 40 15039 0 126 0.24096 1 1 0 0.99171\n4 (500X) freebayes 31 12559 2480 30 0.5082 0.8351 0.01235 0.1649 0.83378\nlofreq 60 15039 0 106 0.36145 1 1 0 0.99303\nvardict 110 15029 10 56 0.66265 0.99934 0.91667 0.00066 0.99566\nvarscan 73 15039 0 93 0.43976 1 1 0 0.99388\n5 (1000X) freebayes 37 14414 625 20 0.64912 0.95844 0.05589 0.04156 0.95727\nlofreq 57 15039 0 109 0.34337 1 1 0 0.99283\nvardict 109 15036 3 57 0.65663 0.9998 0.97321 0.0002 0.99605\nvarscan 79 15039 0 87 0.4759 1 1 0 0.99428\n6 (2000X) freebayes 35 14923 116 20 0.63636 0.99229 0.23179 0.00771 0.99099\nlofreq 70 15039 0 96 0.42169 1 1 0 0.99369\nvardict 120 15039 0 46 0.72289 1 1 0 0.99698\nvarscan 83 15039 0 83 0.5 1 1 0 0.99454\n7 (5000X) freebayes 36 15020 19 17 0.67925 0.99874 0.65455 0.00126 0.99762\nlofreq 67 15039 0 99 0.40361 1 1 0 0.99349\nvardict 117 15036 3 49 0.70482 0.9998 0.975 0.0002 0.99658\nvarscan 78 15039 0 88 0.46988 1 1 0 0.99421\n8 (10000X) freebayes 35 15022 17 21 0.625 0.99887 0.67308 0.00113 0.99748\nlofreq 72 15039 0 94 0.43373 1 1 0 0.99382\nvardict 118 15038 1 48 0.71084 0.99993 0.9916 0.0000665 0.99678\nvarscan 97 15039 0 69 0.58434 1 1 0 0.99546\nTable 2. A table defining true positives, false positives, true negatives and false \nnegative. The gold standard was the set of variants that were added to the artificial \ndata at known positions and frequencies.\nTrue Positives \n(True variants from the gold standard \nset detected by the variant caller)\nFalse Positives \n(Variants called by the caller but not \npresent in the gold standard set)\nFalse Negatives \n(Gold standard variants that were not \ndetected by the caller)\nTrue Negatives \n(Non-variants not called by the caller)\nPage 5 of 17\nWellcome Open Research 2018, 3:21 Last updated: 17 SEP 2025\nFigure 2. Proportion of fully concordant positions with respect to sample coverage. Each plot A–C represents the proportion (y-axis) of \nfully concordant variants with respect to read coverage (x-axis) for the first, second and third dataset. Concordant positions were defined as \npositions that were identified by all the four variant callers.\naccuracy of the variant callers. At the true variant posi -\ntion i we then \n4\n1\nC (i) ( , ), true vc\nacc i ivc\nd S S\n=\n=∑  which can be either \n0, 1, 2, 3, 4 for each true variant position. The second \nconcordance metric is inter-caller concordance, which  \nis the size of the largest set of variant callers that agree at each  \nposition i, without reference to any gold standard sequence. \nWe used both bar plots and heat maps to visualize the effect of  \ncoverage on C acc. Visualization of inter-caller concordance for \nvariant sets was achieved using a bar plot and expounded by  \nUpSet plots (Lex et al., 2014) in R version 3.4.2.\nResults\nWe used three artificial datasets of varying coverage and error \nprofile to assess the concordance accuracy and inter-caller  \nconcordance for four minority variant callers. The first dataset  \ncomprised of artificial reads based on an RSV genome, the  \nsecond dataset comprised of the similar simulated set of reads \nwhilst incorporating an error profile from the set of reads \nused to assemble the reference genome, the third dataset was \ngenerated using an error profile from a poorly sequenced  \nsample.\nOverall, concordance accuracy improved with increase in sample \ncoverage ( Figure 2 ), and the proportion of positions that could  \nnot be identified by any variant caller decreased with increase in \ncoverage ( Supplementary Figure 1 ). For all the three datasets, \nand at each coverage level, fully concordant variants were below  \n50% of the total variants suggesting that considering only fully  \nconcordant positions eliminated a substantial number of variant \npositions. There were marginal improvements in the number of \nconcordant variants in the second dataset compared to the first and \nthe third error profile. Across all datasets, there was little improve-\nment at detecting fully concordant positions after a coverage of \n2000 (Figure 2).\nFreeBayes identified majority of the variants ( Figure 3 ) across  \nall the datasets although it was characterised by variable sen -\nsitivity and precision ( Figure 4 ) in addition to a high false \npositive rate ( Table 3) relative to the other minority variant call -\ners depending on the sample coverage. This was observed  \nacross all the three datasets. Lofreq was the most conservative \nof the callers and it missed majority of the variants across all the \nthree datasets. In addition, Lofreq’s sensitivity did not increase \nwith increased coverage ( Figure 4). We utilized UpSetR plots to \nprovide a visual summary of the combination of variant callers  \nthat contributed to the observed concordance accuracy  \n(Supplementary Figure 2).\nPage 6 of 17\nWellcome Open Research 2018, 3:21 Last updated: 17 SEP 2025\nFigure 3. Heat maps illustrating tool specific concordance for the first artificial dataset. The red tiles represent variants detected by \neach caller from the list of 166 variant positions. The panels are arranged left to right A–H in the order of increasing sample coverage  \n(20,50,100,500,1000,2000,5000 and 10,000). The “not called” column in each panel represents the variants that were not identified by any \nof the variant callers.\nPage 7 of 17\nWellcome Open Research 2018, 3:21 Last updated: 17 SEP 2025\nFigure 4. A summary of the relationship between sample coverage, sensitivity ( A–C) and sample coverage and precision (D–F). The x-axis shows the sample coverage and the y-\naxis represents the sensitivity and precision respectively. Sensitivity of the callers rose gradually from low to high coverage samples with the exception of FreeBayes which showed \nvariable sensitivity for samples with different coverage. Again, precision was variable for FreeBayes calls while relatively high for the rest of the three callers.\nPage 8 of 17\nWellcome Open Research 2018, 3:21 Last updated: 17 SEP 2025\nDiscussion\nDetecting and reporting minority variant calls is challenging,  \ngiven that low frequency calls occur at similar frequency as \nsequencing and PCR errors. Recent studies have linked the  \nsharing of minority variants with transmission patterns, and hence \nit is important to ascertain actual minor variants from spurious  \nvariant calls. Several minority variants callers use different  \ndetection algorithms and statistics, each of which attempt to  \noptimize an aspect of the variant calling process. Therefore,  \nthere could be disparities between what is reported by a given \nminority variant caller, given datasets of varying sequencing  \ndepths and error profiles.\nThis study aimed to identify the proportion of positions that \nwere recognized as minority variants by a set of tools using \nthree artificial datasets of varying coverage and error profiles.  \nConcordance accuracy and inter-caller concordance measures  \nwere dependent on the sample coverage and error profile.\nSensitivity for majority of the tools was positively correlated  \nwith depth of coverage. This was similarly observed by Spencer  \net al ., ( Spencer et al ., 2014 ) in a study that investigated  \nperformance in methods used to detect low-frequency vari -\nants. In the first artificial dataset, VarDict detected true posi -\ntive variants with comparably good performance (sensitivity \n43.4% – 72.3%), though it was marginally invariant to changes \nin average coverage above 20. VarDict has in-built features \nthat could contribute to its efficient performance. It is able to  \nactivate an “amplicon calling mode” that filters out amplicon  \nbiased variants and mispaired primers as PCR artefacts. A  \nsimilar pattern was observed with LoFreq, where sensitivity was \nnot significantly affected by depth of coverage. VarScan2 was  \nmore affected by coverage and maintained average sensitivity \n(6.6% – 58.4%). Applying filters in VarScan has been reported to  \nimprove sensitivity by reducing number of false positives  \n(Hofmann et al ., 2017 ; Koboldt et al ., 2013 ). FreeBayes’ trade-\noff between sensitivity and precision was also reported by other  \nstudies (Hwang et al., 2015; Sandmann et al., 2017).\nBased on artificial reads from the second dataset, FreeBayes \nperformed comparatively better than the other tools ( Figure 3 ).  \nThis suggests that FreeBayes is potentially useful in identifying  \nminority variants when sample data comes from reads with a low \nerror profile. This implies the error rate results are outcome of  \ntool performance.\nSpecificity of a caller is its ability to correctly predict the absence \nof a minority variant. The variant callers make use of a high  \nspecificity to minimize the number of false positive calls thereby  \nreducing post-call filtering and consequently filter out true low-  \nfrequency variants. Moreover, high accuracy measures demon -\nstrate the reliability of the variant caller in correctly identifying  \ntrue variants.\nIn absence of an explicit error model from samples of heteroge -\nneous sequencing quality, combining at least three tools when  \nidentifying minority variants could potentially assist in filtering  \nout errors from low frequency variants. Given that there are no \ndefinitive data and next generation sequencing pipeline standards  \nfor minority variant calling approaches that are specific for viruses, \nthere are opportunities to develop robust methods and tools \nthat strike a balance between detecting errors and true minority  \nvariants from field virus samples that present with  different \nsequencing quality.\nData availability\nThe data analysis scripts and datasets used in analysis are  \navailable from our institutional Dataverse repository: http://dx.doi.\norg/10.7910/DVN/ZIO43M (Mohammed & Githinji, 2018)\nData are available under the terms of the Creative Commons  \nAttribution 4.0 International license (CC-BY 4.0).\nCompeting interests\nNo competing interests were disclosed.\nGrant information\nThe work was funded by the Wellcome Trust Senior Investiga -\ntor Award to Prof D. James Nokes [102975] in addition, this  \nwork was supported through the DELTAS Africa Initiative  \n[DEL-15-003]. The DELTAS Africa Initiative is an independ -\nent funding scheme of the African Academy of Sciences (AAS)’s \nAlliance for Accelerating Excellence in Science in Africa  \n(AESA) and supported by the New Partnership for Africa’s  \nDevelopment Planning and Coordinating Agency (NEPAD  \nAgency) with funding from the Wellcome Trust [107769] and \nthe UK government. The views expressed in this publication are \nthose of the author(s) and not necessarily those of AAS, NEPAD  \nAgency, Wellcome Trust or the UK government. \nThe funders had no role in study design, data generation and  \nanalysis, decision to publish, or preparation of the manuscript.\nPage 9 of 17\nWellcome Open Research 2018, 3:21 Last updated: 17 SEP 2025\nSupplementary material\nSupplementary File 1: A description of data generation methods and command line tools used to create the artificial datasets.\nClick here to access the data.\nSupplementary File 2: FastQC metrics for sample used to generate simulated reads for the second dataset.\nClick here to access the data.\nSupplementary File 3: FastQC quality profile for sample used to generate simulated reads for the third dataset.\nClick here to access the data.\nSupplementary Figure 1: Proportion of fully concordant positions with respect to sample coverage in dataset1.  (A) Represents the \nproportion (y-axis) of fully concordant variants with respect to read coverage (x-axis) and B) shows the proportion of variants positions that \ncould not be identified by any minority caller at each level of sample coverage.\nClick here to access the data.\nSupplementary Figure 2: UpSetR plots showing the concordance between called variants and the respective variant callers using \nthe first artificial dataset. The intersection size illustrates the number of variants in each intersection set. The horizontal axis shows the \ncombination matrix identifying the intersections. A single filled circle represents a unique set of variants. Connected lines depict shared \nvariants (intersections) among the variant callers. Intersection size was positively correlated with average coverage at (a) 20X, (b) 50X,  \n(c) 100X, (d) 500X, (e) 1000X, (f) 2000X, (g) 5000X and (h) 10,000X.\nClick here to access the data.\nReferences\n Domingo E, Sheldon J, Perales C: Viral quasispecies evolution. Microbiol Mol Biol \nRev. 2012; 76(2): 159–216.  \nPubMed Abstract | Publisher Full Text | Free Full Text \n Eigen M, McCaskill J, Schuster P: Molecular Quasi-Species. J Phys Chem. 1988; \n92(24): 6881–6891.  \nPublisher Full Text\n Ewing AD, Houlahan KE, Hu Y, et al.: Combining tumor genome simulation with \ncrowdsourcing to benchmark somatic single-nucleotide-variant detection. Nat \nMethods. 2015; 12(7): 623–630.  \nPubMed Abstract | Publisher Full Text | Free Full Text \n Henn MR, Boutwell CL, Charlebois P, et al.: Whole genome deep sequencing \nof HIV-1 reveals the impact of early minor variants upon immune recognition \nduring acute infection. PLoS Pathog. 2012; 8(3): e1002529.  \nPubMed Abstract | Publisher Full Text | Free Full Text \n Hofmann AL, Behr J, Singer J, et al.: Detailed simulation of cancer exome \nsequencing data reveals differences and common limitations of variant \ncallers. BMC Bioinformatics. 2017; 18(1): 8.  \nPubMed Abstract | Publisher Full Text | Free Full Text \n Holland JJ, De La Torre JC, Steinhauer DA: RNA virus populations as \nquasispecies. Curr Top Microbiol Immunol. 1992; 176: 1–20.  \nPubMed Abstract | Publisher Full Text \n Huang HW, NISC Comparative Sequencing Program, Mullikin JC, et al.: Evaluation \nof variant detection software for pooled next-generation sequence data. BMC \nBioinformatics. 2015; 16: 235.  \nPubMed Abstract | Publisher Full Text | Free Full Text \n Huang W, Li L, Myers JR, et al.: ART: a next-generation sequencing read \nsimulator. Bioinformatics. 2012; 28(4): 593–594.  \nPubMed Abstract | Publisher Full Text | Free Full Text\n Hwang S, Kim E, Lee I, et al.: Systematic comparison of variant calling \npipelines using gold standard personal exome variants. Sci Rep. 2015; 5: \n17875.  \nPubMed Abstract | Publisher Full Text | Free Full Text \n Kinde I, Wu J, Papadopoulos N, et al.: Detection and quantification of rare \nmutations with massively parallel sequencing. Proc Natl Acad Sci U S A. 2011; \n108(23): 9530–9535.  \nPubMed Abstract | Publisher Full Text | Free Full Text \n Koboldt DC, Chen K, Wylie T, et al.: VarScan: variant detection in massively \nparallel sequencing of individual and pooled samples. Bioinformatics. 2009; \n25(17): 2283–2285.  \nPubMed Abstract | Publisher Full Text | Free Full Text \n Koboldt DC, Larson DE, Wilson RK: Using VarScan 2 for Germline Variant Calling \nand Somatic Mutation Detection. Curr Protoc Bioinformatics. 2013; 44: 15.4.1–17.  \nPubMed Abstract | Publisher Full Text | Free Full Text \n Koboldt DC, Zhang Q, Larson DE, et al.: VarScan 2: somatic mutation and copy \nnumber alteration discovery in cancer by exome sequencing. Genome Res. \n2012; 22(3): 568–576.  \nPubMed Abstract | Publisher Full Text | Free Full Text \n Kroigard AB, Thomassen M, Laenkholm AV, et al.: Evaluation of Nine Somatic \nVariant Callers for Detection of Somatic Mutations in Exome and Targeted \nDeep Sequencing Data. PLoS One. 2016; 11(3): e0151664.  \nPubMed Abstract | Publisher Full Text | Free Full Text \n Lai Z, Markovets A, Ahdesmaki M, et al.: VarDict: a novel and versatile variant \ncaller for next-generation sequencing in cancer research. Nucleic Acids Res. \n2016; 44(11): e108.  \nPubMed Abstract | Publisher Full Text | Free Full Text \n Lex A, Gehlenborg N, Strobelt H, et al.: UpSet: Visualization of Intersecting Sets. \nIEEE Trans Vis Comput Graph. 2014; 20(12): 1983–1992.  \nPubMed Abstract | Publisher Full Text | Free Full Text \n Macalalad AR, Zody MC, Charlebois P, et al.: Highly sensitive and specific \ndetection of rare variants in mixed viral populations from massively parallel \nsequence data. PLoS Comput Biol. 2012; 8(3): e1002417.  \nPubMed Abstract | Publisher Full Text | Free Full Text \n McCrone JT, Lauring AS: Measurements of Intrahost Viral Diversity Are \nExtremely Sensitive to Systematic Errors in Variant Calling. J Virol. 2016; \n90(15): 6884–6895.  \nPubMed Abstract | Publisher Full Text | Free Full Text \n Mohammed KS, Githinji G: Replication Data for: Evaluating the Performance of \nTools Used to Call Minority Variants from Whole Genome Short-Read Data. \nHarvard Dataverse, V3. 2018.  \nData Source \n Moya A, Holmes EC, Gonzalez-Candelas F: The population genetics and \nevolutionary epidemiology of RNA viruses. Nat Rev Microbiol. 2004; 2(4):  \n279–288.  \nPubMed Abstract | Publisher Full Text \n Pabinger S, Dander A, Fischer M, et al.: A survey of tools for variant analysis of \nnext-generation genome sequencing data. Brief Bioinform. 2014; 15(2):  \nPage 10 of 17\nWellcome Open Research 2018, 3:21 Last updated: 17 SEP 2025\n256–278. \nPubMed Abstract | Publisher Full Text | Free Full Text \n Poon LL, Song T, Rosenfeld R, et al.: Quantifying influenza virus diversity and \ntransmission in humans. Nat Genet. 2016; 48(2): 195–200.  \nPubMed Abstract | Publisher Full Text | Free Full Text \n Sandmann S, de Graaf AO, Karimi M, et al.: Evaluating Variant Calling Tools for \nNon-Matched Next-Generation Sequencing Data. Sci Rep. 2017; 7: 43169.  \nPubMed Abstract | Publisher Full Text | Free Full Text \n Spencer DH, Tyagi M, Vallania F, et al.: Performance of common analysis \nmethods for detecting low-frequency single nucleotide variants in targeted \nnext-generation sequence data. J Mol Diagn. 2014; 16(1): 75–88.  \nPubMed Abstract | Publisher Full Text | Free Full Text \n Stack JC, Murcia PR, Grenfell BT, et al.: Inferring the inter-host transmission of \ninfluenza A virus using patterns of intra-host genetic variation. Proc Biol Sci. \n2013; 280(1750): 20122173.  \nPubMed Abstract | Publisher Full Text | Free Full Text \n Vignuzzi M, Stone JK, Arnold JJ, et al.: Quasispecies diversity determines \npathogenesis through cooperative interactions in a viral population. Nature. \n2006; 439(7074): 344–348.  \nPubMed Abstract | Publisher Full Text | Free Full Text\n Watson SJ, Welkers MR, Depledge DP, et al.: Viral population analysis and \nminority-variant detection using short read next-generation sequencing. Philos \nTrans R Soc Lond B Biol Sci. 2013; 368(1614): 20120205.  \nPubMed Abstract | Publisher Full Text | Free Full Text \n Wilm A, Aw PP, Bertrand D, et al.: LoFreq: a sequence-quality aware, ultra-\nsensitive variant caller for uncovering cell-population heterogeneity from \nhigh-throughput sequencing datasets. Nucleic Acids Res. 2012; 40(22):  \n11189–11201.  \nPubMed Abstract | Publisher Full Text | Free Full Text \n Worby CJ, Lipsitch M, Hanage WP: Shared Genomic Variants: Identification of \nTransmission Routes Using Pathogen Deep-Sequence Data. Am J Epidemiol. \n2017; 186(10): 1209–1216.  \nPubMed Abstract | Publisher Full Text \n Yang X, Charlebois P, Macalalad A, et al.: V-Phaser 2: variant inference for viral \npopulations. BMC Genomics. 2013; 14: 674.  \nPubMed Abstract | Publisher Full Text | Free Full Text \n Zhang F, Flaherty P: Variational inference for rare variant detection in deep, \nheterogeneous next-generation sequencing data. BMC Bioinformatics. 2017; \n18(1): 45.  \nPubMed Abstract | Publisher Full Text | Free Full Text \n Zukurov JP, do Nascimento-Brito S, Volpini AC, et al.: Estimation of genetic \ndiversity in viral populations from next generation sequencing data with \nextremely deep coverage. Algorithms Mol Biol. 2016; 11: 2.  \nPubMed Abstract | Publisher Full Text | Free Full Text \nPage 11 of 17\nWellcome Open Research 2018, 3:21 Last updated: 17 SEP 2025\nOpen Peer Review\nCurrent Peer Review Status:  \n  \nVersion 1\nReviewer Report 13 April 2018\nhttps://doi.org/10.21956/wellcomeopenres.14703.r32418\n© 2018 Bayer M. This is an open access peer review report distributed under the terms of the Creative Commons \nAttribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the \noriginal work is properly cited.\nMicha M. Bayer \n   \nInformation & Computational Sciences (ICS) group, James Hutton Institute, Dundee, UK \nThis is a comparative review of variant calling tools with focus on the detection of minority variant \ncalls, specifically with reference to virus data. The authors have provided data on four freely \navailable tools that can be tuned to call minority variants, or are specifically designed for this \npurpose. The data used here was artificial reads generated with the ART-Illumina tool, which were \nspiked in with variants at known locations. This was replicated eight times at levels of read \ncoverage spanning four orders of magnitude. The outcome of the experiment was a low level of \nconcordance between tools. None of the tools were able to identify all of the spiked-in variants. \n  \nThe approach taken here is technically sound, as it is based on a strictly defined truth set, \nalthough I am slightly uncomfortable with the lack of real data (but equally I am aware of the \nmethodological problems real data presents in this kind of scenario). The study is useful for the \nwider community as a) it reiterates the point that using a single tool for a given bioinformatics \ntask comes with the risk of missing information and b) it provides concrete pointers as to which \ntools perform well in this scenario. \n  \nMy main reservation is that there is no mention of the importance of parameter settings. Different \nparameter values can affect variant calling outcomes dramatically, even when just comparing \ndifferent runs of the same tool. FreeBayes alone has over 70 command line parameters, many of \nthese continuous variables. This makes for an incredibly large parameter space, and using \ndifferent sets of parameter values can potentially have much more of an impact on the results \nthan the choice of tool. This should at least be mentioned in the Discussion section. \n \nThere also doesn’t seem to be any data on how the individual tools were parameterised in this \nstudy. Were the defaults used in each case? Presumably not, given that we are looking for minority \nvariants (and tools like FreeBayes appear to be developed and parameterised with a human \ngermline use case in mind). This information is critical for reproducibility and must be included \n(even if it is just a statement saying the defaults were used). \n  \nMy other comments below reference the online PDF version of the article. \n  Page 12 of 17\nWellcome Open Research 2018, 3:21 Last updated: 17 SEP 2025\n  \nAbstract\nResults section: change “FreeBayes identified majority” to “FreeBayes identified the \nmajority”\n○\nConclusions section: change “impact” to “impacts”○\n  \nIntroduction\npara 1 line 7: is the “and” redundant?○\npara 5 line 7: comma after “callers”○\n  \nMethods\npara 1 line 1: “we considered eleven published,open-source tools” I counted six excluded \ntools in Table 1, plus four that have been taken through to evaluation = 10\n○\npara 2 (“Artificial datasets”): The concept of the error profile has not been well explained. \nThis section needs to be expanded. I would like to see some description of what the error \nprofile consists of, how it has been derived from the FASTQC data, and how it has been \napplied to the reads in practice.\n○\npara 3 line 2:“at varying frequencies” – what were they? I presume we are talking about the \nproportion of reads at a spiked-in variant site that carried the alternate allele at this site. \nThis information strikes me as absolutely critical for this paper – this is about minority \nvariants after all. Please provide this data.\n○\npara 4 line 8: “written as a comma separated (CSV) files” should be “written as a comma \nseparated (CSV) file”\n○\n  \nResults\npara 3 line 1: “FreeBayes identified majority” should be “FreeBayes identified the majority”○\npara 3 line 7: “it missed majority” should be “it missed the majority”○\nGeneral comment: Vardict should get a mention in this section, as it actually had better \nrecall rates than FreeBayes in 6 out of 8 samples.\n○\n  \nDiscussion\npara 1 line 5: change “ascertain” to “distinguish”○\npara 3 line 1: “Sensitivity for majority” should be “Sensitivity for the majority”○\n  \nTable 3\nAs far as I can tell, the counts for FN + TP should always add up to 166. This is not the case \nfor the majority of the Freebayes runs (except for Freebayes with samples 1 and 3, which \nare ok).\n○\n  \nSupplementary File S1\nTo ensure reproducibility, please include the BAMSurgeon command line statement for \nspiking in the variants.\n○\n  \nSupplementary Figure 2\nIt is not apparent what the individual figures/pages in the PDF file represent. Please label \nthese appropriately.\n○\n  Page 13 of 17\nWellcome Open Research 2018, 3:21 Last updated: 17 SEP 2025\n \nIs the work clearly and accurately presented and does it cite the current literature?\nYes\nIs the study design appropriate and is the work technically sound?\nYes\nAre sufficient details of methods and analysis provided to allow replication by others?\nPartly\nIf applicable, is the statistical analysis and its interpretation appropriate?\nYes\nAre all the source data underlying the results available to ensure full reproducibility?\nYes\nAre the conclusions drawn adequately supported by the results?\nYes\nCompeting Interests: No competing interests were disclosed.\nI confirm that I have read this submission and believe that I have an appropriate level of \nexpertise to confirm that it is of an acceptable scientific standard, however I have \nsignificant reservations, as outlined above.\nAuthor Response 13 Aug 2018\nGeorge Githinji \nWe are grateful for taking time to review this work and for providing useful comments that \nwe have considered in this new draft of the manuscript.\nWe agree on the challenges of obtaining and using the appropriate actual data when \nconducting these types of analysis. We have done our best to generate reliable \nartificial datasets based on an actual RSV reference genome and error profiles from \ntwo sequenced samples using the methods described in this manuscript.\n○\nIn regard to the importance of parameter settings and lack of data on how individual \ntools were parameterized, we have explicitly stated the parameter settings in the \nvariant calling section and in the Supplementary File 4. In the discussion section, we \nhighlight the challenge of exploring all possible parameter settings.\n○\nThe concept of the error profile has is now well explained in paragraph 1 of the \nartificial datasets section in the current manuscript.\n○\nThe actual frequencies used to spike the variants are now described and provided in \nSupplementary file 5.\n○\nThere was a bug in the script used to calculate the FN and TP counts in FreeBayes \nruns. This has been corrected and updated to reflect the true FN and TP counts. A \nrevised table and subsequent figures have been provided.\n○\n \n  Page 14 of 17\nWellcome Open Research 2018, 3:21 Last updated: 17 SEP 2025\nCompeting Interests: No competing interests were disclosed.\nReviewer Report 22 March 2018\nhttps://doi.org/10.21956/wellcomeopenres.14703.r31459\n© 2018 Chapman B. This is an open access peer review report distributed under the terms of the Creative \nCommons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, \nprovided the original work is properly cited.\nBrad A. Chapman \n   \nDepartment of Biostatistics, Harvard Public School of Health, Boston, MA, USA \nThe authors validate methods to call low frequency minority variants from viral samples. Detection \nof minority species in viral population sequencing helps improve treatments by matching drugs to \nthe full spectrum of sub-species, and allows tracking of sub-species within outbreaks. \n \nThe paper uses synthetic datasets generated by the ART read simulator \n(https://www.niehs.nih.gov/research/resources/software/biostatistics/art/) with randomly \ngenerated variants. While this won't fully describe the complexity of a real viral mixture, it does \nprovide a solid baseline for assessing callers. The authors used mutation error profiles from good \nand bad sequencing samples to model different error rates, and evaluated the impact of differing \ncoverage on sensitivity and specificity. \n \nMy main suggestion is to stratify comparisons by variant frequency to assess how callers do with \ndifferent low frequency events, and I provide more specific comments below. \n \nMethods suggestions\nThe stratifications by coverage, error rates and callers are great for identifying the effect of \ndifferent callers on detection. The one missing component is the allele frequency of the \ngenerated variants. The paper does not define the range of frequencies generated, and this \nwill have a large impact on the ability of detection across callers. Looking through the list of \ngenerated variants (Githinji_2018_variant.list.tab) the range varies from high frequency \n(90%+) to very low frequency (<1%). Since the focus is on low frequency detection, I'd \nsuggest stratifying the results into bins: standard (>25%), moderately low frequency (5-25%) \nand low frequency (<5%) and examine the caller and depth detection within these bins. \nPending the results of this, the authors may want to generate additional low frequency \nvariants to help differentiate caller methods. \n \n○\nA confusing aspect of evaluating the depth metrics is that the number of possible true \npositives differs between sample depths. In Table 3, the FreeBayes TP + FNs are 166, 165 \nand 166 for 20x, 50x and 100x. However they change to 61, 55 and 55 for 500x, 1000x and \n200x. Other callers seem to be consistent. Why does FreeBayes have different total variant \nnumbers across depth?\n○\n  Page 15 of 17\nWellcome Open Research 2018, 3:21 Last updated: 17 SEP 2025\n \nPaper suggestions\nTable 1 should get compressed into a single paragraph saying that HaploptypeCaller, \nPlatypus and mpileup target germline calling and not low frequency detection. It doesn't \nneed to be a separate table. \n \n○\nTable 2 should be replaced into a link to descriptions of true/false positives/negatives and \ndoes not need to be a separate figure in the paper. \n \n○\nThe title should reflect that this paper describes viral calling (versus, say low frequency \nsomatic variant detection).\n○\n \nIs the work clearly and accurately presented and does it cite the current literature?\nYes\nIs the study design appropriate and is the work technically sound?\nYes\nAre sufficient details of methods and analysis provided to allow replication by others?\nYes\nIf applicable, is the statistical analysis and its interpretation appropriate?\nNot applicable\nAre all the source data underlying the results available to ensure full reproducibility?\nYes\nAre the conclusions drawn adequately supported by the results?\nYes\nCompeting Interests: No competing interests were disclosed.\nI confirm that I have read this submission and believe that I have an appropriate level of \nexpertise to confirm that it is of an acceptable scientific standard, however I have \nsignificant reservations, as outlined above.\nAuthor Response 13 Aug 2018\nGeorge Githinji \nWe are very grateful for taking time to review this work and for providing useful comments \nwhich we have considered in the new version of the manuscript:\nAll the paper suggestions have been considered table 1 and table 2 were removed \nand replaced with text descriptions.\n○\nWe have added a new figure comparing the allele frequency of the spiked and called \nvariants. We highlight the observations in the result and discussion sections. In \naddition, we provided a stratified result of the variants frequency and discuss how the \n○\n  Page 16 of 17\nWellcome Open Research 2018, 3:21 Last updated: 17 SEP 2025\ncallers perform for different low frequency events. We also provided an addition set \nof spiked variants at frequency below 50%.\nThe results section in table 3 have been revised to reflect the correct number of TP \nand FN for FreeBayes. Figure was also revised to reflect the new changes.\n○\n \nCompeting Interests: No competing interests were disclosed.\n  Page 17 of 17\nWellcome Open Research 2018, 3:21 Last updated: 17 SEP 2025",
  "topic": "Concordance",
  "concepts": [
    {
      "name": "Concordance",
      "score": 0.8720170855522156
    },
    {
      "name": "Biology",
      "score": 0.5836838483810425
    },
    {
      "name": "Genome",
      "score": 0.5237603187561035
    },
    {
      "name": "Population",
      "score": 0.4814795255661011
    },
    {
      "name": "Whole genome sequencing",
      "score": 0.4760446548461914
    },
    {
      "name": "Computational biology",
      "score": 0.4238932430744171
    },
    {
      "name": "Genetics",
      "score": 0.40274396538734436
    },
    {
      "name": "Gene",
      "score": 0.12338757514953613
    },
    {
      "name": "Medicine",
      "score": 0.12227863073348999
    },
    {
      "name": "Environmental health",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I15477984",
      "name": "James Hutton Institute",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I2841861",
      "name": "Kenya Medical Research Institute",
      "country": "KE"
    },
    {
      "id": "https://openalex.org/I279922783",
      "name": "Pwani University",
      "country": "KE"
    },
    {
      "id": "https://openalex.org/I3018483916",
      "name": "University Medical Center Utrecht",
      "country": "NL"
    },
    {
      "id": "https://openalex.org/I913958620",
      "name": "Erasmus University Rotterdam",
      "country": "NL"
    },
    {
      "id": "https://openalex.org/I39555362",
      "name": "University of Warwick",
      "country": "GB"
    }
  ],
  "cited_by": 17
}