{
  "title": "Morphological Analysis for Unsegmented Languages using Recurrent Neural Network Language Model",
  "url": "https://openalex.org/W2251627854",
  "year": 2015,
  "authors": [
    {
      "id": "https://openalex.org/A2098197681",
      "name": "Hajime Morita",
      "affiliations": [
        "Kyoto University",
        "Japan Science and Technology Agency"
      ]
    },
    {
      "id": "https://openalex.org/A2047746676",
      "name": "Daisuke Kawahara",
      "affiliations": [
        "Kyoto University"
      ]
    },
    {
      "id": "https://openalex.org/A112209514",
      "name": "Sadao Kurohashi",
      "affiliations": [
        "Japan Science and Technology Agency",
        "Kyoto University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2405762604",
    "https://openalex.org/W2252225757",
    "https://openalex.org/W2251736053",
    "https://openalex.org/W2103275845",
    "https://openalex.org/W165283731",
    "https://openalex.org/W1965154800",
    "https://openalex.org/W2251230323",
    "https://openalex.org/W2760780548",
    "https://openalex.org/W2155280192",
    "https://openalex.org/W2251811146",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W2760298467",
    "https://openalex.org/W2160097208",
    "https://openalex.org/W179875071",
    "https://openalex.org/W827795486",
    "https://openalex.org/W2184325618",
    "https://openalex.org/W1572771420",
    "https://openalex.org/W1504913471",
    "https://openalex.org/W2120661206"
  ],
  "abstract": "We present a new morphological analysis model that considers semantic plausibility of word sequences by using a recurrent neural network language model (RNNLM).In unsegmented languages, since language models are learned from automatically segmented texts and inevitably contain errors, it is not apparent that conventional language models contribute to morphological analysis.To solve this problem, we do not use language models based on raw word sequences but use a semantically generalized language model, RNNLM, in morphological analysis.In our experiments on two Japanese corpora, our proposed model significantly outperformed baseline models.This result indicates the effectiveness of RNNLM in morphological analysis.",
  "full_text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2292–2297,\nLisbon, Portugal, 17-21 September 2015.c⃝2015 Association for Computational Linguistics.\nMorphological Analysis for Unsegmented Languages\nusing Recurrent Neural Network Language Model\nHajime Morita1,2 Daisuke Kawahara1 Sadao Kurohashi1,2\n1 Kyoto University 2 CREST, Japan Science and Technology Agency\n{hmorita, dk, kuro }@i.kyoto-u.ac.jp\nAbstract\nWe present a new morphological analy-\nsis model that considers semantic plausi-\nbility of word sequences by using a re-\ncurrent neural network language model\n(RNNLM). In unsegmented languages,\nsince language models are learned from\nautomatically segmented texts and in-\nevitably contain errors, it is not apparent\nthat conventional language models con-\ntribute to morphological analysis. To solve\nthis problem, we do not use language mod-\nels based on raw word sequences but use a\nsemantically generalized language model,\nRNNLM, in morphological analysis. In\nour experiments on two Japanese corpora,\nour proposed model signiﬁcantly outper-\nformed baseline models. This result indi-\ncates the effectiveness of RNNLM in mor-\nphological analysis.\n1 Introduction\nIn contrast to space-delimited languages like En-\nglish, word segmentation is the ﬁrst and most cru-\ncial step for natural language processing (NLP)\nin unsegmented languages like Japanese, Chinese,\nand Thai (Kudo et al., 2004; Kaji and Kitsure-\ngawa, 2014; Shen et al., 2014; Kruengkrai et al.,\n2006). Word segmentation is usually performed\njointly with related analysis: POS tagging for Chi-\nnese, and POS tagging and lemmatization (anal-\nysis of inﬂected words) for Japanese. Morpho-\nlogical analysis including word segmentation has\nbeen widely and actively studied, and for exam-\nple, Japanese word segmentation accuracy is in the\nhigh 90s. However, we often observe that strange\noutputs of downstream NLP applications such as\nmachine translation and question answering come\nfrom incorrect word segmentations.\nFor example, the state-of-the-art and popu-\nlar Japanese morphological analyzers, JUMAN\n(Kurohashi and Kawahara, 2009) and MeCab\n(Kudo et al., 2004) both analyze “ݖ\nforeigner’s right to vote)” not into the correct seg-\nmentation of (1a), but into the incorrect and awk-\nward segmentation of (1b).\n(1) a.ࠃ/ ਓ\nforeigner\n/੓ /ݖ\nright to vote\nb.ࠃ\nforeign\n/ࢀ\ncarrot\n/ݖ\nregime\nJUMAN is a rule-based morphological analyzer,\ndeﬁning word-to-word (including inﬂection) con-\nnectivities and their scores. MeCab is a supervised\nmorphological analyzer, learning the probabilities\nof word/POS/inﬂection sequence from an anno-\ntated corpus of tens of thousands of sentences.\nBoth systems, however, cannot realize semanti-\ncally appropriate analysis, and often produce to-\ntally strange outputs like the above.\nThis paper proposes a semantically appropriate\nmorphological analysis method for unsegmented\nlanguages using a language model. For unseg-\nmented languages, morphological analysis and\nlanguage modeling form a chicken-and-egg prob-\nlem. That is, if high-quality morphological analy-\nsis is available, we can learn a high-quality lan-\nguage model from a morphologically analyzed\nlarge corpus. On the other hand, if a high-quality\nlanguage model is available, we can achieve high-\nquality morphological analysis by looking for a\nsegmented word sequence with a large language\nmodel score. However, even if we learn a language\nmodel from a corpus analyzed by a certain level\nof morphological analyzer, the language model is\naffected by the analysis errors of the morphologi-\ncal analyzer and it is no practical use for the im-\nprovement of the morphological analyzer. A lan-\nguage model trained by incorrectly segmented “ ֎\nࠃforeign)/ࢀcarrot)/ݖregime)” just sup-\nports that incorrect segmentation.\nThe point of the paper is that we have tackled\nthe chicken-and-egg problem, not by using a lan-\n2292\n(foreign)\t\r (carrot)\t\r (regime)\t\r\nEOS\t\r\nਓ\u0001\n (a person name)\t\r\n੓\u0001(voting)\t\r\nΛ\u0001\n(particle)\t\r\n࿦\n(discuss)\t\r\nnoun\t\rnoun\t\r\nverb past form\t\r\nnoun\t\r noun\t\r noun\t\r\u0001ࠃ \u0001ࢀ \u0001ݖ\n࿦͡Δ\u0001\n͡ ͨΛ֎\u0001 \u0001ࠃ ਓ\u0001 \u0001ࢀ ੓\u0001 \u0001ݖ\n\u0001ࠃ֎\u0001 ੓\u0001\u0001ࢀਓ\u0001 \u0001ݖ\nright)\t\r(three)\t\r(person)\t\r(country)\t\r(out)\t\r (politics)\t\r\nnoun\t\r noun\t\r noun\t\r noun\t\r noun\t\r noun\t\rBOS\t\r\nInput:\t\r\nFigure 1: An example of a word lattice.\nguage model of raw word sequences, but by using\na semantically generalized language model based\non word embeddings, RNNLM (Recurrent Neural\nNetwork Language Model) (Mikolov et al., 2010;\nMikolov et al., 2011). The RNNLM is trained on\nan automatically analyzed corpus of ten million\nsentences, which possibly includes incorrect seg-\nmentations such as “ࠃforeign)/ࢀcarrot)/\nݖregime).” However, on semantically gener-\nalized level, it is an unnatural semantic sequence\nlike nation vegetable politics . Since the state-of-\nthe-art morphological analyzer achieves the high\naccuracy, it does not often produce incorrect anal-\nyses which support such a semantically strange se-\nquence. This would prefer analysis toward seman-\ntically appropriate word sequences. When a mor-\nphological analyzer utilizes such a generalized and\nreasonable language model, it can penalize strange\nsegmentations like “ࠃforeign)/ࢀcarrot)/\nݖregime),” leading to better accuracy.\nWe furthermore retrain RNNLM using an an-\nnotated corpus of manually segmented 45k sen-\ntences, which further improves morphological\nanalysis.\n2 Related Work\nThere have been several studies that have inte-\ngrated language models into morphological anal-\nysis. Wang et al. (2011) improved Chinese word\nsegmentation and POS tagging by using N-gram\nfeatures learned from an automatically segmented\ncorpus. However, since the auto-segmented cor-\npus inevitably contains segmentation errors, fre-\nquent N-grams are not always correct and thus\nthis problem might affect the performance of\nmorphological analysis. They also divided N-\ngram frequencies into three binned features: high-\nfrequency, middle-frequency and low-frequency.\nSuch coarse features cannot express slight differ-\nences in the likelihood of language models.\nKaji and Kitsuregawa (2014) used a bigram lan-\nguage model feature for Japanese word segmenta-\ntion and POS tagging. Their objective of using a\nlanguage model is to normalize informally spelled\nwords in microblogs. Therefore, their objective is\ndifferent from ours.\nSome studies have used character-based lan-\nguage models for Chinese word segmentation and\nPOS tagging (Zheng et al., 2013; Liu et al., 2014).\nAlthough their approaches have no drawbacks of\nlearning incorrect segmentations, they only cap-\nture more local information than word-based lan-\nguage models.\nWord embeddings have been also used for mor-\nphological analysis. Neural network based models\nhave been proposed for Chinese word segmenta-\ntion and POS tagging (Pei et al., 2014) or word\nsegmentation (Mansur et al., 2013). These meth-\nods acquire word embeddings from a corpus, and\nthen use them as the input of the neural networks.\nOur proposed model learns word embeddings via\nRNNLM, and these embeddings are used for scor-\ning word transitions in morphological analysis.\nOur usage of word embeddings is different from\nthe previous studies.\n3 Proposed Method\nWe propose a new morphological analysis model\nthat considers semantic plausibility of word se-\nquences by using RNNLM. We integrate RNNLM\ninto morphological analysis (Figure 2). We train\nthe RNNLM using both an automatically analyzed\ncorpus and a manually labeled corpus.\n3.1 Recurrent Neural Network Language\nModel\nRNNLM is a recurrent neural network language\nmodel (Mikolov et al., 2010), which outputs a\nprobability distribution of the next word, given the\nembedding of the last word and its context. We\n2293\nRNNLM\t\r RNNLM retrained\t\rRe-training\t\r\nTraining\t\r Training\t\r Base model\t\r\nLabeled corpusAuto segmented  \n corpus\t\r Proposed model\t\r\nFigure 2: Workﬂow for training RNNLM and base\nmodel.\nemploy the RNNME language model 1 proposed\nby (Mikolov et al., 2011; Mikolov, 2012) as the\nimplementation of RNNLM. The RNNME lan-\nguage model has direct connections from the input\nlayer of the recurrent neural network to the output\nlayer, which act as a maximum entropy model and\navoid to waste a lot of parameters to describe sim-\nple patterns. Hereafter, we refer to the RNNME\nlanguage model simply as RNNLM.\nTo train RNNLM, we use a raw corpus of 10\nmillion sentences from the web corpus (Kawa-\nhara and Kurohashi, 2006). These sentences are\nautomatically segmented by JUMAN (Kurohashi\nand Kawahara, 2009). The training of RNNLM\nis based on lemmatized word sequences without\nPOS tags.\nThe trained model contains errors caused by\nan automatically analyzed corpus. We retrain\nRNNLM using a manually labeled corpus after\ntraining RNNLM using the automatically ana-\nlyzed corpus as shown in Figure 2. The retraining\naims to cope with errors related to function word\nsequences.\n3.2 Base Model\nFor our base model, we adopt a model for su-\npervised morphological analysis, which performs\nsegmentation, lemmatization and POS tagging\njointly. We train this model using a tagged cor-\npus of tens of thousands of sentences that contain\ngold segmentations, lemmas, inﬂection forms and\nPOS tags. To predict the most probable sequence\nof words with lemmas and POS tags given an input\nsentence, we execute the following procedure:\n1. Look up the string of the input sentence using\na dictionary.\n2. Make a word lattice.\n3. Search for the path with the highest score\nfrom the lattice.\n1RNNME is the abbreviation of Recurrent Neural Net-\nwork trained jointly with Maximum Entropy model.\nFigure 1 illustrates the constructed lattice during\nthe procedure. At the dictionary lookup step, we\nuse the basic dictionary of JUMAN and an ad-\nditional dictionary comprising 0.8 million words,\nboth of which have lemma, POS and inﬂection in-\nformation. The additional dictionary mainly con-\nsists of itemizations in articles and article titles in\nJapanese Wikipedia.\nWe deﬁne the scoring function as follows:\nscoreB(y) = Φ( y) ·⃗ w, (1)\nwhere y is a tagged word sequence, Φ( y) is a\nfeature vector for y, and ⃗ wis a weight vector.\nEach element in ⃗ wgives a weight to its corre-\nsponding feature in Φ( y). We use the unigram\nand the bigram features composed from word base\nform, POS and inﬂection described in Kudo et al.\n(2004). We also use additional lexical features\nsuch as character type, and trigram features used\nin Zhang and Clark (2008). To learn the weight\nvector, we adopt exact soft conﬁdence-weighted\nlearning (Wang et al., 2012).\nTo consider out-of-vocabulary (OOV) words\nthat are not found in the dictionary, we automat-\nically generate words at the lookup step by seg-\nmenting the input string by character types 2. For\ntraining, we regard words that are not found in the\ndictionary but found in the training corpus as OOV\nwords to learn their weights.\n3.3 RNNLM Integrated Model\nBased on retrained RNNLM, we calculate an\nRNNLM score (score R(y)) to be integrated into\nthe base model. The RNNLM score is deﬁned as\nthe log probability of the next word given its con-\ntext (path). Here, the score for an OOV word is\ngiven by the following formula:\n□Cp □ Lp ·length(n), (2)\nwhere Cp is a constant penalty for OOV words,\nLp is a factor for the character length penalty, and\nlength(n) returns the character length of the next\nword n. This formula is deﬁned to penalize longer\nwords, which are likely to produce segmentation\nerrors.\nWe then integrate the RNNLM score into the\nbase model using the following equation:\nscoreI(y) = (1 □ α)scoreB(y) + α scoreR(y),\n(3)\n2Japanese has three types of characters: Kanji, Hiragana\nand Katakana.\n2294\nwhere α is an interpolation parameter that is tuned\non development data.\nFor decoding, we employ beam search as used\nin Zhang and Clark (2008). Since the possi-\nble context (paths in the word lattice) consid-\nered in RNNLM falls into combinatorial explosion\nin morphological analysis, we keep only prob-\nable context candidates inside the beam. That\nis, each node keeps candidates inside the beam\nwidth. Each candidate has a vector represent-\ning context, and two words of history. The re-\ncurrent model makes decoding harder than non-\nrecurrent neural network language models. How-\never, we use RNNLM because the model outper-\nforms other NNLMs (Mikolov, 2012) and the re-\nsult suggests that the model is more likely to cap-\nture semantic plausibility. Since a sentence rarely\ncontains ambiguous and semantically appropriate\nword sequences, we think that beam search with\nenough beam size is able to keep the ambiguous\ncandidates of word sequences. In the case of non-\nrecurrent NNLMs and the base model, which uses\ntrigram features, we can conduct exact decoding\nusing the second-order Viterbi algorithm (Thede\nand Harper, 1999).\n4 Experiments\n4.1 Experimental Settings\nIn our experiments, we used the Kyoto University\nText Corpus (Kawahara et al., 2002) and Kyoto\nUniversity Web Document Leads Corpus (Hangyo\net al., 2012) as manually tagged corpora. We ran-\ndomly chose 2,000 sentences from each corpus\nfor test data, and 500 sentences for development\ndata. We used the remaining part of the corpora\nas training data to train our base model and retrain\nRNNLM. In total, we used 45,000 sentences for\ntraining.\nFor comparative purposes, we used the follow-\ning four baselines: the Japanese morphological an-\nalyzer JUMAN, the supervised morphological an-\nalyzer MeCab, the base model, and a model using\na conventional language model. For this language\nmodel, we built a trigram language model with\nKneser-Ney smoothing using SRILM (Stolcke,\n2002) from the same automatically segmented cor-\npus. The language model is modiﬁed to have an\ninterpolation parameter α and length penalty for\nOOV ,Lp.\nWe set the beam width to 5 by preliminary ex-\nperiments. We also set a constant penalty for OOV\nwords ( Cp) as 5, which is the default value in\nthe implementation of Mikolov et al. (2011). We\ntuned the parameters of our proposed model and\nthe baseline model ( α and Lp) and the parameters\nof language models using grid search on the de-\nvelopment data. We set α = 0.3, Lp =1.5 for the\nproposed model (“ Base + RNNLM retrain”).3\nWe measured the performance of the baseline\nmodels and the proposed model by F-value of\nword segmentation and F-value of joint evaluation\nof word segmentation and POS tagging. We calcu-\nlated F-value for the two corpora (news and web)\nand the merged corpus (all).\nWe used the bootstrapping method (Zhang et\nal., 2004) to test statistical signiﬁcance between\nproposed models and other models. Suppose we\nhave a test set T that includes N sentences. The\nmethod repeatedly creates M new test sets by re-\nsampling N sentences with replacement from T.\nWe calculate the F-value of each model on M + 1\ntest sets including T, and then we have M + 1\nscore differences. From the scores, we calculate\nthe 95% conﬁdence interval. If the interval does\nnot overlap with zero, the two models are consid-\nered as statistically signiﬁcantly different. In our\nevaluation, M is set to 2,000.\n4.2 Results and Discussions\nTable 1 lists the results of our proposed model and\nthe baseline models. Our proposed model (“Base\n+ RNNLM retrain”) signiﬁcantly outperforms all the\nbaseline models and “Base + RNNLM,” which\ndoes not use retraining. In particular, we achieved\na large improvement for segmentation. This can be\nattributed to the use of RNNLM that was learned\nbased on lemmatized word sequence without POS\ntags.\n“Base + SRILM” segmented the example de-\nscribed in Section 1 (“ݖinto the\nincorrect segmentation “ࠃ/ࢀ/ݖin the\nsame way as JUMAN. This segmentation error\nwas caused by errors in the automatically seg-\nmented corpus that was used to train the language\nmodel. Our proposed model can correctly seg-\nment this example if a proper context is available\nby semantically capturing word transitions using\nRNNLM.\nThe base model, JUMAN and “Base + SRILM”\nincorrectly segmented “߁݈healthy)/ͳͲ(etc.)/\n3We set α = 0.1, Lp = 2.0 for “Base + RNNLM”, and α =\n0.3, Lp = 0.5 for “Base + SRILM.”\n2295\nSegmentation Seg + POS Segmentation Seg + POS Segmentation Seg + POS\n(news) (news) (web) (web) (all) (all)\nJUMAN 98.92 98.47 98.20 97.64 98.64 98.14\nMeCab 99.07 98.58 98.22 97.51 98.74 98.16\nBase model 98.94 98.46 97.71 96.90 98.46 97.85\nBase + SRILM 98.94 98.40 98.13 97.33 98.62 97.98\nBase + RNNLM 99.06 98.59 98.17 97.45 98.71 98.14\nBase + RNNLMretrain 99. 15∗ 98. 70∗ 98. 37∗ 97. 68∗ 98. 84∗ 98. 30∗\nTable 1: Results for test datasets. ∗ means the score of “Base + RNNLM retrain” is signiﬁcantly improved\nfrom that of all other models.\nͷ(of)/఺(point)/Ͱ(in)/ʜʜ” (in terms of health\nand so on) into “ͳ(healthy)/Ͳͷ(any)/఺\n(point)/Ͱ(in)/ʜʜ.” Although this segmentation\ncan be grammatically accepted, it is difﬁcult to\nsemantically interpret this word sequence. Our\nproposed model can correctly segment this exam-\nple because RNNLM learns semantically plausible\nword sequences.\n5 Conclusion\nIn this paper, we proposed a new model for\nmorphological analysis that is integrated with\nRNNLM. We trained RNNLM on an automati-\ncally segmented corpus and tuned on a manually\ntagged corpus. The proposed model was able to\nsigniﬁcantly reduce errors in the base model by\ncapturing semantic plausibility of word sequences\nusing RNNLM. In the future, we will design fea-\ntures derived from RNNLM models, and integrate\nthem into a uniﬁed learning framework. We also\nintend to apply our method to unsegmented lan-\nguages other than Japanese, such as Chinese and\nThai.\nReferences\nMasatsugu Hangyo, Daisuke Kawahara, and Sadao\nKurohashi. 2012. Building a diverse document\nleads corpus annotated with semantic relations. In\nProceedings of the 26th Paciﬁc Asia Conference\non Language, Information, and Computation , pages\n535–544.\nNobuhiro Kaji and Masaru Kitsuregawa. 2014. Ac-\ncurate word segmentation and POS tagging for\njapanese microblogs: Corpus annotation and joint\nmodeling with lexical normalization. In Proceed-\nings of the 2014 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n99–109, Doha, Qatar. Association for Computa-\ntional Linguistics.\nDaisuke Kawahara and Sadao Kurohashi. 2006.\nCase frame compilation from the web using high-\nperformance computing. In Proceedings of the 5th\nInternational Conference on Language Resources\nand Evaluation , pages 1344–1347.\nDaisuke Kawahara, Sadao Kurohashi, and K ˆoiti\nHasida. 2002. Construction of a Japanese\nrelevance-tagged corpus. In Proceedings of the\nThird International Conference on Language Re-\nsources and Evaluation (LREC-2002) , Las Palmas,\nCanary Islands - Spain, May. European Language\nResources Association (ELRA). ACL Anthology\nIdentiﬁer: L02-1302.\nCanasai Kruengkrai, Virach Sornlertlamvanich, and\nHitoshi Isahara. 2006. A conditional random ﬁeld\nframework for Thai morphological analysis. In Pro-\nceedings of LREC , pages 2419–2424.\nTaku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.\n2004. Applying conditional random ﬁelds to\nJapanese morphological analysis. In Proceedings\nof the Conference on Emprical Methods in Natural\nLanguage Processing (EMNLP 2004) , volume 2004.\nSadao Kurohashi and Daisuke Kawahara, 2009.\nJapanese Morphological Analysis System JUMAN\n6.0 Users Manual . http://nlp.ist.i.\nkyoto-u.ac.jp/EN/index.php?JUMAN.\nXiaodong Liu, Kevin Duh, Yuji Matsumoto, and To-\nmoya Iwakura. 2014. Learning character repre-\nsentations for Chinese word segmentation. In NIPS\n2014 Workshop on Modern Machine Learning and\nNatural Language Processing .\nMairgup Mansur, Wenzhe Pei, and Baobao Chang.\n2013. Feature-based neural language model and\nChinese word segmentation. In Proceedings of\nthe Sixth International Joint Conference on Natural\nLanguage Processing , pages 1271–1277, Nagoya,\nJapan, October. Asian Federation of Natural Lan-\nguage Processing.\nTomas Mikolov, Martin Karaﬁ ´at, Lukas Burget, Jan\nCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In IN-\nTERSPEECH 2010, 11th Annual Conference of the\nInternational Speech Communication Association,\nMakuhari, Chiba, Japan, September 26-30, 2010 ,\npages 1045–1048.\nTomas Mikolov, Anoop Deoras, Dan Povey, Lukar\nBurget, and Jan Honza Cernocky. 2011. Strate-\ngies for training large scale neural network language\n2296\nmodels. In Proceedings of ASRU 2011 , pages 196–\n201. IEEE Automatic Speech Recognition and Un-\nderstanding Workshop.\nTomas Mikolov. 2012. Statistical language models\nbased on neural networks . Ph.D. thesis, Brno uni-\nversity of technology.\nWenzhe Pei, Tao Ge, and Baobao Chang. 2014. Max-\nmargin tensor neural network for Chinese word seg-\nmentation. In Proceedings of the 52nd Annual Meet-\ning of the Association for Computational Linguis-\ntics, ACL 2014, June 22-27, 2014, Baltimore, MD,\nUSA, Volume 1: Long Papers , pages 293–303.\nMo Shen, Hongxiao Liu, Daisuke Kawahara, and\nSadao Kurohashi. 2014. Chinese Morphological\nAnalysis with Character-level POS Tagging. In Pro-\nceedings of the 52nd Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers), pages 253–258, Baltimore, Maryland. As-\nsociation for Computational Linguistics.\nAndreas Stolcke. 2002. SRILM - an extensible lan-\nguage modeling toolkit. In John H L Hansen and\nBryan L Pellom, editors, 7th International Confer-\nence on Spoken Language Processing, ICSLP2002\n- INTERSPEECH 2002, Denver, Colorado, USA,\nSeptember 16-20, 2002 , pages 901–904. ISCA.\nScott M. Thede and Mary P. Harper. 1999. A second-\norder Hidden Markov Model for part-of-speech tag-\nging. In Proceedings of the 37th annual meeting\nof the Association for Computational Linguistics on\nComputational Linguistics , pages 175–182, Morris-\ntown, NJ, USA, June. Association for Computa-\ntional Linguistics.\nYiou Wang, Jun’ichi Kazama, Wenliang Chen, Yu-\njie Zhang, Kentaro Torisawa, and Yoshimasa Tsu-\nruoka. 2011. Improving chinese word segmenta-\ntion and POS tagging with semi-supervised meth-\nods using large auto-analyzed data. In Proceedings\nof the Fifth International Joint Conference on Nat-\nural Language Processing (IJCNLP-2011) , pages\n309–317, Chiang Mai, Thailand. Asian Federation\nof Natural Language Processing.\nJialei Wang, Peilin Zhao, and Steven C.H. Hoi.\n2012. Exact soft conﬁdence-weighted learning. In\n29th International Conference on Machine Learning\n(ICML 2012) , pages 121–128.\nYue Zhang and Stephen Clark. 2008. Joint word seg-\nmentation and pos tagging using a single perceptron.\nIn Proceedings of ACL-08: HLT , pages 888–896,\nColumbus, Ohio, June. Association for Computa-\ntional Linguistics.\nYing Zhang, Stephan V ogel, and Alex Waibel. 2004.\nInterpreting bleu/nist scores: How much improve-\nment do we need to have a better system? In Pro-\nceedings of the Fourth International Conference on\nLanguage Resources and Evaluation (LREC-2004) ,\nLisbon, Portugal, May. European Language Re-\nsources Association (ELRA). ACL Anthology Iden-\ntiﬁer: L04-1489.\nXiaoqing Zheng, Hanyang Chen, and Tianyu Xu.\n2013. Deep learning for Chinese word segmen-\ntation and POS tagging. In Proceedings of the\n2013 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2013, 18-21 Octo-\nber 2013, Grand Hyatt Seattle, Seattle, Washington,\nUSA, A meeting of SIGDAT, a Special Interest Group\nof the ACL , pages 647–657.\n2297",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8715091943740845
    },
    {
      "name": "Language model",
      "score": 0.7172387838363647
    },
    {
      "name": "Natural language processing",
      "score": 0.6962215900421143
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6747323274612427
    },
    {
      "name": "Recurrent neural network",
      "score": 0.5521496534347534
    },
    {
      "name": "Word (group theory)",
      "score": 0.5435488224029541
    },
    {
      "name": "Artificial neural network",
      "score": 0.3489735424518585
    },
    {
      "name": "Linguistics",
      "score": 0.22435733675956726
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210086780",
      "name": "Japan Science and Technology Agency",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I22299242",
      "name": "Kyoto University",
      "country": "JP"
    }
  ],
  "cited_by": 99
}