{
  "title": "Sabiá: Portuguese Large Language Models",
  "url": "https://openalex.org/W4366571901",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2207288414",
      "name": "Ramon Pires",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222101911",
      "name": "Abonizio, Hugo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4307479264",
      "name": "Almeida, Thales Sales",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221795652",
      "name": "Nogueira, Rodrigo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4229506649",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W4385565879",
    "https://openalex.org/W4318719086",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2946659172",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W2991878188",
    "https://openalex.org/W4284691825",
    "https://openalex.org/W3213418658",
    "https://openalex.org/W4310742926",
    "https://openalex.org/W3098637735",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4283768109",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W3159795318",
    "https://openalex.org/W4385570226",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W4224442590",
    "https://openalex.org/W2780851414",
    "https://openalex.org/W4212932674",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4361866125",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3008931407",
    "https://openalex.org/W4388979610",
    "https://openalex.org/W3177252310",
    "https://openalex.org/W3114950584",
    "https://openalex.org/W4300466035",
    "https://openalex.org/W3103187652",
    "https://openalex.org/W4387546783",
    "https://openalex.org/W3045958725",
    "https://openalex.org/W4361865428",
    "https://openalex.org/W4289828103",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4287760320",
    "https://openalex.org/W4224247062",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W4320005767",
    "https://openalex.org/W4361021241",
    "https://openalex.org/W3008110149",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W2905246312",
    "https://openalex.org/W4287888679",
    "https://openalex.org/W3153675281",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4226107112",
    "https://openalex.org/W4287687023",
    "https://openalex.org/W4293569541",
    "https://openalex.org/W4385681388",
    "https://openalex.org/W4385572438",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W4287181720",
    "https://openalex.org/W3096266342",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W4287025617",
    "https://openalex.org/W2963250244"
  ],
  "abstract": "As the capabilities of language models continue to advance, it is conceivable that \"one-size-fits-all\" model will remain as the main paradigm. For instance, given the vast number of languages worldwide, many of which are low-resource, the prevalent practice is to pretrain a single model on multiple languages. In this paper, we add to the growing body of evidence that challenges this practice, demonstrating that monolingual pretraining on the target language significantly improves models already extensively trained on diverse corpora. More specifically, we further pretrain GPT-J and LLaMA models on Portuguese texts using 3% or less of their original pretraining budget. Few-shot evaluations on Poeta, a suite of 14 Portuguese datasets, reveal that our models outperform English-centric and multilingual counterparts by a significant margin. Our best model, Sabiá-65B, performs on par with GPT-3.5-turbo. By evaluating on datasets originally conceived in the target language as well as translated ones, we study the contributions of language-specific pretraining in terms of 1) capturing linguistic nuances and structures inherent to the target language, and 2) enriching the model's knowledge about a domain or culture. Our results indicate that the majority of the benefits stem from the domain-specific knowledge acquired through monolingual pretraining.",
  "full_text": "Sabi´ a: Portuguese Large Language Models\nRamon Pires, Hugo Abonizio, Thales Sales Almeida, and Rodrigo Nogueira\nMaritaca AI\n{ramon,hugo,thales,rodrigo}@maritaca.ai\nAbstract. As the capabilities of language models continue to advance,\nit is conceivable that “one-size-fits-all” model will remain as the main\nparadigm. For instance, given the vast number of languages worldwide,\nmany of which are low-resource, the prevalent practice is to pretrain a\nsingle model on multiple languages. In this paper, we add to the grow-\ning body of evidence that challenges this practice, demonstrating that\nmonolingual pretraining on the target language significantly improves\nmodels already extensively trained on diverse corpora. More specifically,\nwe further pretrain GPT-J and LLaMA models on Portuguese texts using\n3% or less of their original pretraining budget. Few-shot evaluations on\nPoeta, a suite of 14 Portuguese datasets, reveal that our models outper-\nform English-centric and multilingual counterparts by a significant mar-\ngin. Our best model, Sabi´ a-65B, performs on par with GPT-3.5-turbo.\nBy evaluating on datasets originally conceived in the target language as\nwell as translated ones, we study the impact of language-specific pretrain-\ning in terms of 1) capturing linguistic nuances and structures inherent\nto the target language, and 2) enriching the model’s knowledge about\na domain or culture. Our results indicate that most benefits stem from\nthe domain-specific knowledge acquired through monolingual pretrain-\ning. Finally, we show that our Portuguese model has a lower performance\nin English tasks, thereby substantiating the inherent compromise in re-\nfining models for specific domains. Sabi´ a-7B is avaliable at HuggingFace:\nhttps://huggingface.co/maritaca-ai/sabia-7b\n1 Introduction\nLanguage Models have revolutionized the field of natural language processing\nwith their exceptional ability to perform tasks with minimal supervision. Al-\nthough primarily pretrained on English-centric corpora, the models have shown\nimpressive multilingual capabilities [10]. Given the abundance of languages world-\nwide, the majority of which are low-resource, it has become a common practice to\npretrain single models on multiple languages simultaneously. Models like XLM-\nR [12], mBART [28], mT5 [70], and BLOOM [54] exemplify this approach.\nDespite the success of these multilingual models, we argue that they may not\nbe the optimal approach for capturing the cultural and knowledge richness inher-\nent in individual languages. When a moderately-sized language-specific corpus is\navailable, continued pretraining could integrate the missing knowledge into the\narXiv:2304.07880v4  [cs.CL]  9 Nov 2023\n2 R. Pires et al.\nmodel, enhancing its performance on targeted tasks. To test this hypothesis, we\nextend the pretraining of English-centric models using Portuguese corpora and\nevaluate their performance on an extensive range of Portuguese datasets em-\nploying a few-shot learning approach. Our results indicate that, even for models\ntrained beyond the recommendations by Hoffmann et al [18], this additional pre-\ntraining considerably improves performance compared to multilingual models.\nWe evaluate our models on datasets comprising texts originally created by na-\ntive Brazilian Portuguese speakers, as well as datasets translated from English\nto Portuguese. We observe improvements across all datasets due to the Por-\ntuguese pretraining, with the gains being particularly pronounced for datasets\ncreated by Brazilian speakers. One of the largest improvements was observed on\nthe ENEM dataset [57], which is derived from entrance exams used by Brazilian\nuniversities and requires extensive knowledge of the country’s history, geogra-\nphy, and literature. This result provides evidence that the major contribution of\nour language-specific pretraining is to inject domain-specific knowledge about a\nparticular culture as opposed to solely enhancing language proficiency.\n2 Related Work\nThe success of multilingual pretraining has been well-documented in the liter-\nature, with models such as ByT5 [69], mT5 [70], XLM-R [12], XGLM [27] and\nmGPT [56] paving the way for more inclusive language understanding and gener-\nation by leveraging shared knowledge across multiple languages. However, there\nare limitations to this approach.\nBLOOM, a 175B-parameter model pretrained on 46 languages, performs\nworse on English tasks compared to OPT [74], a similarly sized model pretrained\non English-centric corpora using comparable computational resources and data\nsize. We conjecture that BLOOM’s underperformance may be attributed to its\nrelatively limited exposure to English tokens during the pretraining phase. Con-\nsequently, this observation suggests that monolingual pretraining could offer sup-\nplementary advantages.\nIn support of this hypothesis, models with hundreds of millions of parameters\npretrained on monolingual texts have demonstrated gains over multilingual coun-\nterparts [7,59,6,52,32,24,8,2,25,36,21]. Additionally, research has indicated that\nlanguage adaptation is beneficial even for low-resource languages [38,13,4,72].\nHowever, there is a limited number of published research articles with compre-\nhensive evaluations of the benefits of continued pretraining at the multi-billion-\nparameter scale [50,73,22]. Through this study, we contribute to the literature\nby demonstrating the effectiveness of continued language-specific pretraining for\nPortuguese language models up to the 65B-parameter scale.\nThe question concerning whether it is advantageous to train models for spe-\ncific languages is closely associated with the question of whether it is beneficial\nto train models for particular domains of knowledge. Recent studies, such as\nMinerva [26] and Galactica [62], have shown that domain-specific pretraining\ncan lead to significant improvements, even with a smaller pretraining corpus\nSabi´ a: Portuguese Large Language Models 3\ncompared to large-scale, general-purpose pretraining corpora. Analogously, Fu\net al. [15] demonstrated the feasibility of specializing smaller models to per-\nform multi-step reasoning, a capability typically exclusive to models with at\nleast 50B parameters, at the expense of diminished performance in other, more\ngeneral tasks.\nPretraining with a combination of general and domain-specific corpora can\npotentially enhance performance in specialized tasks without compromising ef-\nfectiveness in general-purpose tasks, albeit at the cost of increased computational\ndemands. For example, BloombergGPT [68], a 50B-parameter model pretrained\non heterogeneous corpus in which more than half of texts are from the finan-\ncial domain, exhibits comparable performance to OPT-66B in general tasks.\nHowever, BloombergGPT’s pretraining dataset is three times larger, and conse-\nquently used more computational resources.\nRather than pursuing a single model that performs well across multiple do-\nmains, Gururangan et al. [17] propose an alternative approach: using multiple\nexpert models, each trained on a domain-specific subset within a broader, diverse\ndataset, to function as a single general-purpose model. Their models outperform\ndense ones across various domain-specific tasks, at the expense of an increased\nparameter count, consequently leading to larger memory requirements for effi-\ncient inference.1\n3 Methodology\nIn this section, we outline the pretraining data and training details used to\nbuild our models, including data sources, preprocessing techniques, architectures,\nhyperparameters, and optimization methods.\n3.1 Pretraining Data\nThe pretraining data is derived from the Portuguese subset of the ClueWeb\n2022 dataset [40,41]. To increase the datasets’s quality, we apply the quality\nfilters from MassiveText [45], modifying them to accommodate the specific re-\nquirements of the Portuguese language. We normalize the text withftfy2, convert\nwikitexts into human-readable texts, and exclude documents containing less than\n200 unique tokens.\nThese quality filters are primarily designed for web pages and may not seam-\nlessly transfer to other domains. There is potential for improvement by employing\nmore automated methods; however, this study did not explore such approaches\ndue to the resource-intensive nature of pretraining experiments.\nFollowing the cleaning process, all documents are concatenated using an end-\nof-sequence token as a separator, and then tokenized. The GPT-J tokenizer,\nwhich is identical to the GPT-2 tokenizer [44], produces 7.8 billion tokens, while\n1 To serve their ensemble with a low latency, the weights for each expert must be kept\nin GPU memory.\n2 ftfy normalization fixes mojibakes and remove remnant HTML tags.\n4 R. Pires et al.\nthe LLaMA tokenizer produces 7.3 billion tokens. The discrepancy in the total\nnumber of tokens is primarily due to the different tokenization strategies each\nmodel employs, byte-level BPE and BPE based on sentencepiece [23], respec-\ntively along with the variation of the vocabularies used by each tokenizer.\nWe extended the training of three models — LLaMA 7B and 65B [63] as well\nas GPT-J [66] — originally trained on English-centric corpora, on Portuguese\ntexts; these further pretrained models from LLaMA are denoted as Sabi´ a, while\nthe one derived from GPT-J is referred to as Sabi´ a-J.3\n3.2 Sabi´ a models\nThe LLaMA 7B and 65B models are decoder-only Transformer models [64] with\na similar architecture to PALM’s [10]. The models were trained using a causal\nlanguage modeling objective on a massive dataset sourced from webpages, code,\nbooks, and scientific papers. The 7B model was trained on 1 trillion tokens and\nthe 65B model was trained on 1.4 trillion tokens. While the majority of the\ncorpus is in English, it also includes an unspecified amount of Portuguese text.\nStarting from the LLaMA weights, we train the Sabi´ a models on our Por-\ntuguese dataset (see Section 3.1) using the t5x and seqio frameworks [48].\nAdhering closely to the hyperparameters used by PALM, we use the AdaFactor\noptimizer [55] without factorization, a first-order momentum β1 = 0 .9, and a\nsecond-order momentum β2 = 1 − k−0.8, where k represents the step number.\nWe apply global norm clipping at 1.0 and dynamic weight decay of lr2, with lr\ndenoting the current learning rate.\nBesides the standard causal language modeling loss, we use an auxiliary loss\nof 10 −4 log2(P\ni ezi ), where z are the logits, to decrease the likelihood of loss\nspikes at the 65B-parameter scale. The learning rate is linearly increased from 0\nto 1e-3 over the initial 1,000 steps, followed by a constant learning rate of 1e-3\nfor an additional 9,000 steps.\nThe models were trained on a TPU v2-512, using batches of 512 sequences,\neach containing 2048 tokens. We utilized gradient checkpointing, also known as\nrematerialization, to enable the use of larger batches, thereby increasing TPU\nutilization. For the 7B model, this configuration results in a throughput of\n124,000 tokens/sec, corresponding to a Model FLOPs Utilization (MFU) [10]\nof 45.2%, excluding the self-attention operations. For the 65B model, we achieve\na throughput of 14,000 tokens/sec, resulting in an MFU of 47.4%.\nThe resulting models were trained on a total of 10.4 billion tokens, or 1.52\nepochs of the Portuguese dataset. This equals to 10,000 training steps. We no-\nticed improvements in few-shot tasks beyond one epoch, which corroborates\nresults from Taylor et al. [62]. However, due to the high costs of pretraining, we\ndid not continue training.4\n3 Sabi´ a is a tribute to the eponymous bird, renowned for its diverse and intricate\nvocalizations.\n4 Considering the on-demand pricing of 384 USD per hour for a TPU v2-512, pretrain-\ning Sabi´ a-7B and Sabi´ a-65B costs approximately 9,000 and 80,000 USD, respectively.\nSabi´ a: Portuguese Large Language Models 5\n3.3 Sabi´ a-J\nThe GPT-J model is a 6B-parameter decoder-only Transformer model whose\narchitecture and training hyperparameters closely follow GPT-3 6.7B. The main\ndifferences reside on computing the MLP and self-attention in parallel, applying\nattention head with dimension 256 (twice larger than GPT-3 6.7B), and using\nRotary Positional Embedding (RoPE) [61]. GPT-J was trained on 400B tokens\nfrom The Pile dataset [16], whose 97.4% tokens are in English.\nWe begin training Sabi´ a-J from the released GPT-J checkpoint,5 using the\nmesh-transformer-jax framework [65] and AdamW optimizer [30] with a weight\ndecay of 0.1. We start the pretraining by warming up the learning rate until 1.2e-\n5 over 13,500 steps, followed by a cosine annealing decay during 135,518 steps\nuntil the end learning rate of 2.4e-6, and kept it constant from there on. We train\non a TPU v3-8 using an effective batch size of 32 sequences of 2048 tokens. This\nresults in a throughput of 5,200 tokens/sec, corresponding to a MFU of 44.5%\nwithout self-attention. The model was trained for 18 days on 7.8B tokens, or one\nepoch of the Portuguese dataset. 6\n4 Evaluation on Poeta\nWe evaluate the Sabi´ a models on the Portuguese Evaluation Tasks (Poeta)\nbenchmark, which comprises 14 downstream NLP datasets in Portuguese: ASSIN\n2 RTE and STS [47], ENEM Challenge [57], ENEM 2022 [37], FaQuAD [53],\nTweetSentBr [5], AG News [75], IMDB [31], MASSIVE [14], MKQA [29], BoolQ [11],\nSST2 [58], WSC [33], and BLUEX [1]. Half of them (ASSIN 2 RTE and STS,\nBLUEX, ENEM Challenge, ENEM 2022, FaQuAD, and TweetSentBr) were orig-\ninally written in Portuguese, and the remaining ones were either manually or\nautomatically translated into Portuguese from their originals in English. We re-\nfer to the first group as “Native” datasets and the second group as “Translated”\ndatasets.7\nThe models were evaluated in a few-shot manner using the maximum number\nof examples that fits into a 2048-token context for each task. We used the GPT-\n2 tokenizer as a reference because it results in more tokens. This allowed us to\ncomfortably fit prompts tokenized with other tokenizers.\nTo evaluate the models, we manually select a set of few-shot examples for\neach dataset on Poeta. Depending on the dataset, these examples are balanced by\nclass (except for FaQuAD, BLUEX, ENEM Challenge, ENEM 2022, MKQA, and\nWSC). For each test example, the prompts are built with the selected few-shot\n5 https://huggingface.co/EleutherAI/gpt-j-6b\n6 Due to constraints in our hardware budget, this model was trained with fewer tokens\ncompared to Sabi´ a.\n7 The MASSIVE dataset underwent manual translation and localization; however,\ngiven that the original text was composed in English, it has been categorized as a\ntranslated dataset.\n6 R. Pires et al.\nPreferred Rand. Avg Len Num Num Num\nDataset Type Metric Score Transl. (chars) Train Test Few-shot\nAG News Multiclass classification (4) Accuracy 25 Yes 282.34 120,000 (110,953) 7,600 12\nASSIN 2 RTE Binary classification F1 50 No 139.99 6,500 2448 18\nASSIN 2 STS Regression Pearson 0 No 139.99 6,500 2448 15\nBLUEX Multiple choice (4) Accuracy 25 No 1,228.08 - 178 1\nBoolQ Binary classification Accuracy 50 Yes 562.30 9,427 (7,015) 3,270 4\nENEM ChallengeMultiple choice (5) Accuracy 20 No 1,286.68 - 916 1\nENEM 2022 Multiple choice (5) Accuracy 20 No 1,170.24 - 118 1\nFaQuAD Extractive QA F1 0 No 1,056.47 - 63 4\nIMDB Binary classification Accuracy 50 Yes 1,114.56 25,000 (18,613) 25,000 2\nMASSIVE Multiclass classification (18) F1-macro 0.58 Yes 68.35 11,514 2,974 36\nMKQA Extractive QA F1 0 Yes 80.32 - 10,000 (6,758) 40\nSST2 Binary classification Accuracy 50 Yes 84.19 67,349 872 34\nTweetSentBRMulticlass classification (3) F1-macro 32.4 No 93.32 12,990 2010 30\nWSC Binary classification Accuracy 50 Yes 102.15 - 285 18\nTable 1.A summary of the datasets constituting the Poeta benchmark.\nexamples in alternating order. Each task on Poeta has a particular instruction\nthat is placed at the beginning of the prompt.\nFollowing Srivastava et al [60], we adopt the Normalized Preferred Metric\n(NPM) as our primary evaluation measure:\nNPM = 1\nN\nNX\ni=1\n100 × [raw preferred metric]i − [random score]i\n[high score]i − [random score]i\n(1)\nwhere N is the number of evaluation datasets, [raw preferred metric]i is the\nscore obtained by the model on the i-th dataset, [random score]i is the score of\na random model (e.g., 50% for a binary classification task) and [high score]i\nis the highest possible score on that dataset, which is either 1 or 100. The\npreferred metric and random score for each dataset are presented in Table 1. The\nrationale behind employing NPM rather than a straightforward average across\nall datasets is to mitigate the undue influence of datasets with inherently high\nscores, such as binary classification datasets, which could otherwise outweigh\ndatasets characterized by lower scores.\n5 Results\nThe main results can be found in Table 2. Models such as BLOOMZ, XGLM\nand Bertin-GPT struggled to generate answers in Portuguese. To address this\nissue, we adopted an approach akin to that used by the XGLM authors: by\ncalculating the likelihood of each candidate answer string based on the input text\nand subsequently selecting the class with the highest probability. For FaQuAD,\nthe only dataset in the benchmark without predetermined candidate answers,\nwe allowed the models to generate answers in their original format.\nWe observe that the LLaMA baselines significantly outperform models of\nequivalent size trained with fewer tokens, such as Galactica and OPT. Further-\nmore, despite being trained on English-centric corpora, LLaMA-7B surpasses\nmultilingual BLOOM and XGLM of similar sizes. The Sabi´ a models demonstrate\nSabi´ a: Portuguese Large Language Models 7\nconsiderable improvement in NPM compared to their respective baseline mod-\nels. These NPM gains are more substantial for the smaller Sabi´ a-J and Sabi´ a-7B\nmodels. Notably, Sabi´ a-65B marginally outperforms OpenAI’s GPT-3.5-turbo,\nwhich serves as the base model for ChatGPT.\nTable 2.Few-shot NPM results on the Poeta benchmark.\nNative Translated All\nGALACTICA-6.7B 2.2 13.6 7.9\nOPT-6.7B 5.3 39.7 22.5\nOPT-66B 16.4 47.1 31.7\nBERTIN-GPT 5.8 42.5 24.2\nBLOOM-7.1B 10.6 44.2 27.4\nBLOOMZ-7.1B 18.3 44.7 31.5\nXGLM-7.5B 14.0 46.9 30.4\nGPT-3.5-turbo 67.9 66.0 67.0\nGPT-4 78.8 82.5 80.6\nGPT-J 10.2 33.9 22.0\nSabi´ a-J 25.0 43.1 34.0\nLLaMA-7B 20.2 45.8 33.0\nSabi´ a-7B 43.4 53.6 48.5\nLLaMA-65B 59.1 68.4 63.7\nSabi´ a-65B 69.2 69.6 69.4\nThrough our Portuguese pretraining, we observed that the improvement in\nNPM was higher in native datasets than that in translated datasets. For Sabi´ a-\n65B, improvements over LLaMA-65B were mostly from the native subset. We\nhypothesize that this is due to the “mechanistic” nature of translated datasets:\nsince they were translated from English, the baseline model already possesses\nthe knowledge needed to solve them and gains little from learning the linguistic,\nsyntactic, and grammatical knowledge of the target language. For instance, to\nanswer the question “does p o box come before street address” (BoolQ dataset),\nthe model gains little from additional pretraining on a Portuguese corpus as it is\nunlikely that the corpus would provide new information regarding the formatting\nof US mailing addresses that the model has not already encountered during\nits initial English-centric pretraining. Conversely, language-specific pretraining\nintroduces the specific knowledge required to solve tasks in the native subset.\nAlthough GPT-J exhibited lower few-shot performance in English tasks rel-\native to LLaMA, we use it in this study to illustrate that not only highly opti-\nmized models like LLaMA can benefit from extended pretraining. We chose not\nto use BLOOM-7.1B as our initial checkpoint for pretraining due to its inferior\nperformance compared to GPT-J in preliminary few-shot experiments on three\nPortuguese datasets. However, we later discovered that its performance on Po-\neta surpassed GPT-J’s. Nonetheless, BLOOM still exhibits lower performance\ncompared to LLaMA.\nAnalogous to Sabi´ a-J, BERTIN-GPT is a model pretrained on Spanish text\nstarting from the GPT-J weights. Since Spanish and Portuguese are similar\nlanguages, it is reasonable to expect that BERTIN-GPT would perform better\n8 R. Pires et al.\nthan its baseline model. Nevertheless, the observed NPM for BERTIN-GPT is\nonly slightly higher than GPT-J’s.\nA noteworthy comparison involves Galactica, a model pretrained on scientific\ntext, predominantly in English, and a similarly-sized OPT model, which utilized\ncomparable pretraining compute but was pretrained on a larger and more diverse\nEnglish-centric corpus. In their study, the authors demonstrate that Galactica\nperforms on par with OPT on English tasks and largely outperforms OPT on\nscientific-related tasks. Conversely, OPT significantly outperforms Galactica in\nPortuguese tasks. This result underscores the trade-offs associated with domain-\nspecific specialization, which often entails diminished performance in other tasks.\nBLOOMZ [35], a multilingual instruction-tuned model, demonstrated supe-\nrior performance compared to its baseline BLOOM model, rivaling LLaMA of\nequivalent size.8 Nevertheless, our approach of pretraining in Portuguese appears\nto yield superior results, as Sabi´ a-J surpasses BLOOMZ despite originating from\na lower-performing baseline model. We envision continued pretraining and in-\nstruction tuning as complementary techniques to be combined in future research.\n5.1 Results per Dataset\nTable 3 presents the results per Poeta dataset for Sabi´ a models, their baselines,\nand for the supervised state-of-the-art. The SOTA results reported for the trans-\nlated datasets were obtained using their original English versions [71,76,46,51].\nSince the Poeta benchmark excludes unanswerable examples of the MKQA\ndataset, we decided not to include the SOTA result for this dataset.\nIn more challenging datasets, such as ENEM Challenge, ENEM 2022, and\nBLUEX, which are derived from admission exams to Brazilian universities, we\nsee the most significant gains due to language-specific pretraining. Substantial\nimprovements are also observed in TweetSentBr, a dataset containing tweets\nwith an abundance of slang and references to Brazilian popular culture. We\nhypothesize that this pretraining imparts specific knowledge about the country’s\nculture, literature, and geography that is less frequently encountered and learned\nduring the original pretraining with more diverse texts.\nCertain capabilities only emerge at scale, as evidenced by [67]. For example,\n6-7B models perform close to the random baseline in datasets such as ASSIN\n2 RTE and STS, and WSC. However, at the 65B scale, we observe substantial\nimprovements, approaching or surpassing state-of-the-art supervised models on\nthe ASSIN 2 RTE and FaQuAD datasets.\nGPT-4 [39] results indicate that there is still room for improvement for Sabi´ a-\n65B in the majority of the datasets evaluated in this work. Nevertheless, Sabi´ a-\n65B performs on par with GPT-4 in datasets such as ASSIN 2 RTE, ENEM\nChallenge, and FaQuAD.\n8 This model was used in the experiments: https://huggingface.co/bigscience/\nbloomz-7b1-mt\nSabi´ a: Portuguese Large Language Models 9\nTable 3.Results per dataset. 1[49]; 2[9]; 3[34]; 4[3]; 5[71]; 6[76]; 7[46]; 8[51].\nNative Translated\nAvgASSIN 2 RTE(F1) ASSIN 2 STS(Pearson)BLUEX(Acc)ENEM(Acc)ENEM 2022(Acc)FaQuAD(F1)TweetSentBr(F1-macro)AG News(Acc)BoolQ(Acc)IMDB(Acc)MASSIVE(F1-macro)MKQA(F1)SST2(Acc)WSC(Acc)\nSOTA supervised - 92.071 86.002 - - - 82.40 3 77.274 95.555 92.40696.215 - - 97.50 790.108\nGPT-4 84.99 90.96 77.58 76.40 92.00 79.66 84.74 82.40 93.50 86.50 97.00 83.30 55.67 97.50 92.63GPT-3.5-turbo 76.08 88.28 66.41 60.11 80.57 75.42 78.28 74.39 87.71 71.43 84.86 84.19 44.92 91.71 76.84Galactica-6.7B 34.11 34.92 11.63 28.65 20.74 22.88 40.16 21.98 38.33 57.13 51.08 35.62 3.01 62.27 49.12Bertin-GPT-6B 45.18 33.24 6.23 22.47 20.52 22.03 64.00 35.52 82.44 44.25 87.66 55.46 15.56 81.08 62.11OPT-6.7B 43.35 43.33 21.35 24.16 19.87 20.34 56.45 14.37 55.67 61.31 90.42 51.84 13.64 86.47 47.72OPT-66B 49.68 65.66 7.88 29.78 20.41 17.80 71.12 32.54 81.87 58.75 92.66 61.64 21.17 87.50 46.67BLOOM-7.1B 47.01 50.32 12.16 25.84 20.85 17.08 72.67 25.12 79.48 60.43 89.60 56.23 15.72 83.83 48.77BLOOMZ-7.1B 50.94 33.57 24.50 34.27 28.38 27.12 79.90 50.36 83.82 38.23 93.80 55.31 12.36 86.93 64.56XGLM-7.5B 48.79 53.75 15.07 24.16 19.10 19.49 44.84 63.23 77.47 49.76 91.46 59.74 13.72 89.11 62.11\nGPT-J 43.51 54.88 17.86 24.72 20.85 20.34 59.52 20.98 64.15 48.75 72.68 55.67 10.69 83.94 54.04Sabi´ a-J 52.84 35.49 22.97 39.89 39.41 36.44 69.28 64.16 84.30 51.53 90.86 58.82 13.84 87.16 45.61\nLLAMA-7B 51.30 56.82 7.39 32.02 29.04 23.73 77.38 44.19 76.94 57.37 86.92 59.90 30.08 88.76 47.72Sabi´ a-7B 62.43 64.87 13.63 47.75 60.59 60.17 77.43 67.17 83.28 64.07 92.70 68.95 31.98 90.60 50.88\nLLAMA-65B 73.84 74.98 62.85 53.93 75.00 62.71 87.25 68.05 88.01 73.12 94.98 78.71 48.34 94.27 71.58Sabi´ a-65B 77.65 88.07 63.29 57.87 90.39 72.03 88.47 72.91 88.34 75.96 92.76 79.41 49.47 93.43 74.74\n5.2 Data Contamination\nThe pretraining data for Sabi´ a models were collected up until February 2022.\nSince ENEM 2022 was publicly released in November 2022, the model could not\nhave access to the answers for the questions present within its pretraining data.\nConsequently, the improvements observed at least for ENEM 2022, which were\nhigher than the average of the datasets, cannot be attributed to data contam-\nination. However, for the other datasets, the possibility of data contamination\ncannot be ruled out.\n5.3 Ablation: English datasets\nIn this ablation study, we investigate the potential impact of Portuguese pre-\ntraining on the performance of the model in English datasets. We evaluated\nthe LLaMA-7B and the Sabi´ a-7B models in English multiple-choice tasks. For\nsimplicity, we employed a few-shot evaluation setup with 10 randomly selected\nexamples (dynamic-sampled prompt). Importantly, we did not incorporate any\ndescriptions or include Portuguese keywords to delimit the few-shot examples.\nWe also restricted all the datasets to 350 test examples.\nFollowing LLaMA’s [63] approach, given the provided context, we select the\nanswer with the highest likelihood normalized by the number of characters. The\nresults in Table 4 indicate that the Sabi´ a-7B model exhibits a slightly reduced\nperformance in English tasks compared to the baseline. This result corroborates\nour premise that model specialization invariably entails a balancing act, where\nimprovements in one domain frequently coincide with degradation in another.\nTable 4.Results in English datasets.\nPIQA HellaSwag WinoGrande ARC-e ARC-c OBQANPM\nLLaMA-7B 83.43 77.43 74.29 69.43 48.86 43.14 50.10\nSabi´ a-7B 80.86 75.71 72.29 72.86 50.00 42.29 49.02\n10 R. Pires et al.\n6 Limitations\nOwing to the financial constraints associated with pretraining and, more signifi-\ncantly, the manual labor involved in collecting and curating evaluation datasets,\nexperiments were conducted exclusively in Portuguese. Given that our models\nstarted pretraining from English-pretrained models and that Portuguese and\nEnglish exhibit relatively close linguistic proximity, we anticipate that other re-\nsearchers conducting further pretraining on languages closely related to English\nwill observe comparable improvements in their target tasks. However, determin-\ning whether the benefits of this method persist for languages more distant from\nEnglish remains an open research question.\nPortuguese is a language with an abundance of high-quality web-based texts.\nThus, the gains observed with the proposed method may not necessarily extend\nto low-resource languages with limited availability of quality texts. In such cases,\nparameter-efficient methods [19,43,42] could be advantageous, as evidenced by\nYong et al. [72]. We did not use these techniques in this study due to the training\ncosts, which are approximately equivalent to training the entire model. 9\n7 Conclusion\nIn this study, we contributed to the expanding body of scientific evidence that\nspecializing models for individual languages leads to improvements, even when\nthe baseline model is large and extensively trained. We achieved this for the Por-\ntuguese language utilizing a near state-of-the-art model with 65 billion parame-\nters. Given the relatively low pretraining cost and significant performance gains\nobserved, we foresee a future landscape consisting of a diverse array of models,\neach tailored to a specific domain, rather than a single, all-encompassing model.\n8 Acknowledgments\nWe thank Google Cloud for the generous TPU grant.\nReferences\n1. Almeida, T.S., Laitz, T., Bon´ as, G.K., Nogueira, R.: Bluex: A benchmark based\non brazilian leading universities entrance exams. To appear (2023)\n2. Antoun, W., Baly, F., Hajj, H.: AraBERT: Transformer-based model for Arabic\nlanguage understanding. In: Proceedings of the 4th Workshop on Open-Source\nArabic Corpora and Processing Tools, with a Shared Task on Offensive Language\nDetection. pp. 9–15. European Language Resource Association, Marseille, France\n(May 2020)\n9 Although parameter-efficient methods adjust only a fraction of the weights, they use\nonly marginally fewer training FLOPs, as activations and gradients are computed for\nthe entire model. For instance, LoRA [20], a parameter-efficient method, improves\ntraining throughput of a GPT-3 175B model by only nearly 32%.\nSabi´ a: Portuguese Large Language Models 11\n3. Barros, T.M.d., et al.: Employing transformers and emoji to perform sentiment\nclassification of social media texts: Utilizando transformers e emoji na classifica¸ c˜ ao\nde sentimento de textos oriundos de redes sociais (2021)\n4. Bhattacharjee, A., Hasan, T., Ahmad, W., Mubasshir, K.S., Islam, M.S., Iqbal,\nA., Rahman, M.S., Shahriyar, R.: BanglaBERT: Language model pretraining and\nbenchmarks for low-resource language understanding evaluation in Bangla. In:\nFindings of the Association for Computational Linguistics: NAACL 2022. pp. 1318–\n1327. Association for Computational Linguistics, Seattle, United States (Jul 2022).\nhttps://doi.org/10.18653/v1/2022.findings-naacl.98\n5. Brum, H., Volpe Nunes, M.d.G.: Building a sentiment corpus of tweets in Brazilian\nPortuguese. In: Proceedings of the Eleventh International Conference on Language\nResources and Evaluation (LREC 2018). European Language Resources Associa-\ntion (ELRA), Miyazaki, Japan (May 2018)\n6. Carmo, D., Piau, M., Campiotti, I., Nogueira, R., Lotufo, R.: Ptt5: Pretrain-\ning and validating the t5 model on brazilian portuguese data. arXiv preprint\narXiv:2008.09144 (2020)\n7. Ca˜ nete, J., Chaperon, G., Fuentes, R., Ho, J.H., Kang, H., P´ erez, J.: Spanish pre-\ntrained BERT model and evaluation data. In: PML4DC at ICLR 2020 (2020)\n8. Chan, B., Schweter, S., M¨ oller, T.: German’s next language model. In: Proceedings\nof the 28th International Conference on Computational Linguistics. pp. 6788–6796.\nInternational Committee on Computational Linguistics, Barcelona, Spain (Online)\n(Dec 2020). https://doi.org/10.18653/v1/2020.coling-main.598\n9. Chaves Rodrigues, R., Tanti, M., Agerri, R.: Evaluation of Portuguese Language\nModels (3 2023). https://doi.org/10.5281/zenodo.7781848, https://github.com/\nruanchaves/eplm\n10. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,\nBarham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al.: Palm: Scaling lan-\nguage modeling with pathways. arXiv preprint arXiv:2204.02311 (2022)\n11. Clark, C., Lee, K., Chang, M.W., Kwiatkowski, T., Collins, M., Toutanova, K.:\nBoolQ: Exploring the surprising difficulty of natural yes/no questions. In: Proceed-\nings of the 2019 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers). pp. 2924–2936. Association for Computational Linguistics, Min-\nneapolis, Minnesota (Jun 2019). https://doi.org/10.18653/v1/N19-1300\n12. Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzm´ an,\nF., Grave, ´E., Ott, M., Zettlemoyer, L., Stoyanov, V.: Unsupervised cross-lingual\nrepresentation learning at scale. In: Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics. pp. 8440–8451 (2020)\n13. Ebrahimi, A., Kann, K.: How to adapt your pretrained multilingual model to 1600\nlanguages. In: Proceedings of the 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers). pp. 4555–4567. Association for Com-\nputational Linguistics, Online (Aug 2021). https://doi.org/10.18653/v1/2021.acl-\nlong.351\n14. FitzGerald, J., Hench, C., Peris, C., Mackie, S., Rottmann, K., Sanchez, A., Nash,\nA., Urbach, L., Kakarala, V., Singh, R., Ranganath, S., Crist, L., Britan, M.,\nLeeuwis, W., Tur, G., Natarajan, P.: MASSIVE: A 1m-example multilingual natu-\nral language understanding dataset with 51 typologically-diverse languages (2022)\n15. Fu, Y., Peng, H., Ou, L., Sabharwal, A., Khot, T.: Specializing smaller language\nmodels towards multi-step reasoning. arXiv preprint arXiv:2301.12726 (2023)\n12 R. Pires et al.\n16. Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J.,\nHe, H., Thite, A., Nabeshima, N., et al.: The pile: An 800gb dataset of diverse text\nfor language modeling. arXiv preprint arXiv:2101.00027 (2020)\n17. Gururangan, S., Li, M., Lewis, M., Shi, W., Althoff, T., Smith, N.A., Zettlemoyer,\nL.: Scaling expert language models with unsupervised domain discovery. arXiv\npreprint arXiv:2303.14177 (2023)\n18. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford,\nE., Casas, D.d.L., Hendricks, L.A., Welbl, J., Clark, A., et al.: Training compute-\noptimal large language models. arXiv preprint arXiv:2203.15556 (2022)\n19. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Ges-\nmundo, A., Attariyan, M., Gelly, S.: Parameter-efficient transfer learning for nlp.\nIn: International Conference on Machine Learning. pp. 2790–2799. PMLR (2019)\n20. Hu, E.J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen,\nW.: LoRA: Low-rank adaptation of large language models. In: International Con-\nference on Learning Representations (2022), https://openreview.net/forum?id=\nnZeVKeeFYf9\n21. Kalyan, K.S., Rajasekharan, A., Sangeetha, S.: Ammus: A survey of transformer-\nbased pretrained models in natural language processing. arXiv preprint\narXiv:2108.05542 (2021)\n22. Kim, B., Kim, H., Lee, S.W., Lee, G., Kwak, D., Hyeon, J.D., Park, S., Kim,\nS., Kim, S., Seo, D., et al.: What changes can large-scale language models bring?\nintensive study on hyperclova: Billions-scale korean generative pretrained trans-\nformers. In: Proceedings of the 2021 Conference on Empirical Methods in Natural\nLanguage Processing. pp. 3405–3424 (2021)\n23. Kudo, T., Richardson, J.: SentencePiece: A simple and language independent sub-\nword tokenizer and detokenizer for neural text processing. In: Proceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing: System\nDemonstrations. pp. 66–71. Association for Computational Linguistics, Brussels,\nBelgium (Nov 2018). https://doi.org/10.18653/v1/D18-2012\n24. Le, H., Vial, L., Frej, J., Segonne, V., Coavoux, M., Lecouteux, B., Allauzen, A.,\nCrabb´ e, B., Besacier, L., Schwab, D.: FlauBERT: Unsupervised language model\npre-training for French. In: Proceedings of the Twelfth Language Resources and\nEvaluation Conference. pp. 2479–2490. European Language Resources Association,\nMarseille, France (May 2020)\n25. Lee, H., Yoon, J., Hwang, B., Joe, S., Min, S., Gwon, Y.: Korealbert: Pretraining\na lite bert model for korean language understanding. In: 2020 25th International\nConference on Pattern Recognition (ICPR). pp. 5551–5557. IEEE (2021)\n26. Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh,\nV., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., et al.: Solving quantitative\nreasoning problems with language models. arXiv preprint arXiv:2206.14858 (2022)\n27. Lin, X.V., Mihaylov, T., Artetxe, M., Wang, T., Chen, S., Simig, D., Ott, M.,\nGoyal, N., Bhosale, S., Du, J., et al.: Few-shot learning with multilingual generative\nlanguage models. In: Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing. pp. 9019–9052 (2022)\n28. Liu, Y., Gu, J., Goyal, N., Li, X., Edunov, S., Ghazvininejad, M., Lewis, M.,\nZettlemoyer, L.: Multilingual denoising pre-training for neural machine translation.\nTransactions of the Association for Computational Linguistics 8, 726–742 (2020)\n29. Longpre, S., Lu, Y., Daiber, J.: MKQA: A linguistically diverse benchmark for\nmultilingual open domain question answering. Transactions of the Association for\nComputational Linguistics 9, 1389–1406 (2021)\nSabi´ a: Portuguese Large Language Models 13\n30. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: International\nConference on Learning Representations (2019)\n31. Maas, A.L., Daly, R.E., Pham, P.T., Huang, D., Ng, A.Y., Potts, C.: Learning\nword vectors for sentiment analysis. In: Proceedings of the 49th Annual Meeting\nof the Association for Computational Linguistics: Human Language Technologies.\npp. 142–150. Association for Computational Linguistics, Portland, Oregon, USA\n(Jun 2011)\n32. Martin, L., Muller, B., Ortiz Su´ arez, P.J., Dupont, Y., Romary, L., de la Clerg-\nerie, ´E., Seddah, D., Sagot, B.: CamemBERT: a tasty French language model.\nIn: Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics. pp. 7203–7219. Association for Computational Linguistics, Online (Jul\n2020). https://doi.org/10.18653/v1/2020.acl-main.645\n33. de Melo, G., Imaizumi, V., Cozman, F.: Winograd schemas in portuguese. In: Anais\ndo XVI Encontro Nacional de Inteligˆ encia Artificial e Computacional. pp. 787–798.\nSBC (2019)\n34. Moraes, G., Bonif´ acio, L.H., Rodrigues de Souza, L., Nogueira, R., Lotufo,\nR.: A cost-benefit analysis of cross-lingual transfer methods. arXiv preprint\narXiv:2105.06813 (2021), https://arxiv.org/abs/2105.06813\n35. Muennighoff, N., Wang, T., Sutawika, L., Roberts, A., Biderman, S., Scao, T.L.,\nBari, M.S., Shen, S., Yong, Z.X., Schoelkopf, H., Tang, X., Radev, D., Aji, A.F.,\nAlmubarak, K., Albanie, S., Alyafeai, Z., Webson, A., Raff, E., Raffel, C.: Crosslin-\ngual generalization through multitask finetuning (2022)\n36. Nguyen, D.Q., Tuan Nguyen, A.: PhoBERT: Pre-trained language models for Viet-\nnamese. In: Findings of the Association for Computational Linguistics: EMNLP\n2020. pp. 1037–1042. Association for Computational Linguistics, Online (Nov\n2020). https://doi.org/10.18653/v1/2020.findings-emnlp.92\n37. Nunes, D., Primi, R., Pires, R., Lotufo, R., Nogueira, R.: Evaluating gpt-3.5 and\ngpt-4 models on brazilian university admission exams (2023)\n38. Ogueji, K., Zhu, Y., Lin, J.: Small data? no problem! exploring the viability of\npretrained multilingual language models for low-resourced languages. In: Proceed-\nings of the 1st Workshop on Multilingual Representation Learning. pp. 116–126.\nAssociation for Computational Linguistics, Punta Cana, Dominican Republic (Nov\n2021)\n39. OpenAI: Gpt-4 technical report (2023)\n40. Overwijk, A., Xiong, C., Callan, J.: Clueweb22: 10 billion web documents with rich\ninformation. In: Proceedings of the 45th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval. pp. 3360–3362 (2022)\n41. Overwijk, A., Xiong, C., Liu, X., VandenBerg, C., Callan, J.: Clueweb22: 10 billion\nweb documents with visual and semantic information (2022)\n42. Pfeiffer, J., Kamath, A., R¨ uckl´ e, A., Cho, K., Gurevych, I.: AdapterFusion: Non-\ndestructive task composition for transfer learning. In: Proceedings of the 16th\nConference of the European Chapter of the Association for Computational Lin-\nguistics: Main Volume. pp. 487–503. Association for Computational Linguistics,\nOnline (Apr 2021). https://doi.org/10.18653/v1/2021.eacl-main.39\n43. Pfeiffer, J., Vuli´ c, I., Gurevych, I., Ruder, S.: Mad-x: An adapter-based framework\nfor multi-task cross-lingual transfer. arXiv preprint arXiv:2005.00052 (2020)\n44. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language\nmodels are unsupervised multitask learners. OpenAI blog 1(8), 9 (2019)\n45. Rae, J.W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides,\nJ., Henderson, S., Ring, R., Young, S., et al.: Scaling language models: Methods,\nanalysis & insights from training gopher. arXiv preprint arXiv:2112.11446 (2021)\n14 R. Pires et al.\n46. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li,\nW., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research 21(1), 5485–5551 (2020)\n47. Real, L., Fonseca, E., Gon¸ calo Oliveira, H.: The ASSIN 2 shared task: a quick\noverview. In: Computational Processing of the Portuguese Language: 14th Interna-\ntional Conference, PROPOR 2020, Evora, Portugal, March 2–4, 2020, Proceedings\n14. pp. 406–412. Springer (2020)\n48. Roberts, A., Chung, H.W., Levskaya, A., Mishra, G., Bradbury, J., Andor, D.,\nNarang, S., Lester, B., Gaffney, C., Mohiuddin, A., et al.: Scaling up models and\ndata with t5x and seqio. arXiv preprint arXiv:2203.17189 13 (2022)\n49. Rosa, G.M., Bonifacio, L.H., de Souza, L.R., Lotufo, R., Nogueira, R.: A cost-\nbenefit analysis of cross-lingual transfer methods. arXiv preprint arXiv:2105.06813\n(2021)\n50. la Rosa, J.D., Fern´ andez, A.: Zero-shot reading comprehension and reasoning for\nspanish with BERTIN GPT-J-6B. In: y G´ omez, M.M., Gonzalo, J., Rangel, F.,\nCasavantes, M., ´Angel ´Alvarez Carmona, M., Bel-Enguix, G., Escalante, H.J.,\nFreitas, L., Miranda-Escalada, A., Rodr´ ıguez-S´ anchez, F., Ros´ a, A., Sobrevilla-\nCabezudo, M.A., Taul´ e, M., Valencia-Garc´ ıa, R. (eds.) Proceedings of the Iberian\nLanguages Evaluation Forum (IberLEF 2022). CEUR Workshop Proceedings\n(2022)\n51. Sakaguchi, K., Bras, R.L., Bhagavatula, C., Choi, Y.: Winogrande: An adversarial\nwinograd schema challenge at scale. Communications of the ACM 64(9), 99–106\n(2021)\n52. Sarti, G., Nissim, M.: It5: Large-scale text-to-text pretraining for italian language\nunderstanding and generation. arXiv preprint arXiv:2203.03759 (2022)\n53. Sayama, H.F., Araujo, A.V., Fernandes, E.R.: FaQuAD: Reading compre-\nhension dataset in the domain of brazilian higher education. In: 2019 8th\nBrazilian Conference on Intelligent Systems (BRACIS). pp. 443–448 (2019).\nhttps://doi.org/10.1109/BRACIS.2019.00084\n54. Scao, T.L., Fan, A., Akiki, C., Pavlick, E., Ili´ c, S., Hesslow, D., Castagn´ e, R.,\nLuccioni, A.S., Yvon, F., Gall´ e, M., et al.: Bloom: A 176b-parameter open-access\nmultilingual language model. arXiv preprint arXiv:2211.05100 (2022)\n55. Shazeer, N., Stern, M.: Adafactor: Adaptive learning rates with sublinear memory\ncost. In: International Conference on Machine Learning. pp. 4596–4604. PMLR\n(2018)\n56. Shliazhko, O., Fenogenova, A., Tikhonova, M., Mikhailov, V., Kozlova, A., Shav-\nrina, T.: mgpt: Few-shot learners go multilingual. arXiv preprint arXiv:2204.07580\n(2022)\n57. Silveira, I.C., Maua, D.D.: Advances in automatically solving the enem.\nIn: 2018 7th Brazilian Conference on Intelligent Systems (BRACIS). pp.\n43–48. IEEE Computer Society, Los Alamitos, CA, USA (oct 2018).\nhttps://doi.org/10.1109/BRACIS.2018.00016\n58. Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.D., Ng, A., Potts,\nC.: Recursive deep models for semantic compositionality over a sentiment tree-\nbank. In: Proceedings of the 2013 Conference on Empirical Methods in Natural\nLanguage Processing. pp. 1631–1642. Association for Computational Linguistics,\nSeattle, Washington, USA (Oct 2013)\n59. Souza, F., Nogueira, R., Lotufo, R.: BERTimbau: pretrained BERT models for\nbrazilian portuguese. In: Intelligent Systems: 9th Brazilian Conference, BRACIS\n2020, Rio Grande, Brazil, October 20–23, 2020, Proceedings, Part I 9. pp. 403–417.\nSpringer (2020)\nSabi´ a: Portuguese Large Language Models 15\n60. Srivastava, A., Rastogi, A., Rao, A., Shoeb, A.A.M., Abid, A., Fisch, A., Brown,\nA.R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al.: Beyond the imitation\ngame: Quantifying and extrapolating the capabilities of language models. arXiv\npreprint arXiv:2206.04615 (2022)\n61. Su, J., Lu, Y., Pan, S., Wen, B., Liu, Y.: Roformer: Enhanced transformer with\nrotary position embedding. arXiv preprint arXiv:2104.09864 (2021)\n62. Taylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn, A., Saravia, E.,\nPoulton, A., Kerkez, V., Stojnic, R.: Galactica: A large language model for science.\narXiv preprint arXiv:2211.09085 (2022)\n63. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,\nRozi` ere, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient\nfoundation language models. arXiv preprint arXiv:2302.13971 (2023)\n64. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. Advances in neural information pro-\ncessing systems 30 (2017)\n65. Wang, B.: Mesh-Transformer-JAX: Model-Parallel Implementation of Trans-\nformer Language Model with JAX. https://github.com/kingoflolz/\nmesh-transformer-jax (May 2021)\n66. Wang, B., Komatsuzaki, A.: GPT-J-6B: A 6 Billion Parameter Autoregressive\nLanguage Model (May 2021)\n67. Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D.,\nBosma, M., Zhou, D., Metzler, D., Chi, E.H., Hashimoto, T., Vinyals, O., Liang,\nP., Dean, J., Fedus, W.: Emergent abilities of large language models. Transactions\non Machine Learning Research (2022), survey Certification\n68. Wu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann, S., Kambadur,\nP., Rosenberg, D., Mann, G.: BloombergGPT: A large language model for finance\n(2023)\n69. Xue, L., Barua, A., Constant, N., Al-Rfou, R., Narang, S., Kale, M., Roberts, A.,\nRaffel, C.: Byt5: Towards a token-free future with pre-trained byte-to-byte models.\nTransactions of the Association for Computational Linguistics 10, 291–306 (2022)\n70. Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua,\nA., Raffel, C.: mt5: A massively multilingual pre-trained text-to-text transformer.\narXiv preprint arXiv:2010.11934 (2020)\n71. Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., Le, Q.V.: XLNet:\nGeneralized Autoregressive Pretraining for Language Understanding. Curran As-\nsociates Inc., Red Hook, NY, USA (2019)\n72. Yong, Z.X., Schoelkopf, H., Muennighoff, N., Aji, A.F., Adelani, D.I., Almubarak,\nK., Bari, M.S., Sutawika, L., Kasai, J., Baruwa, A., et al.: Bloom+ 1: Adding lan-\nguage support to bloom for zero-shot prompting. arXiv preprint arXiv:2212.09535\n(2022)\n73. Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., Yang, Z., Xu, Y., Zheng,\nW., Xia, X., et al.: Glm-130b: An open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414 (2022)\n74. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab,\nM., Li, X., Lin, X.V., et al.: Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068 (2022)\n75. Zhang, X., Zhao, J.J., LeCun, Y.: Character-level convolutional networks for text\nclassification. In: NIPS (2015)\n76. Zoph, B.: Designing effective sparse expert models. In: 2022 IEEE International\nParallel and Distributed Processing Symposium Workshops (IPDPSW). pp. 1044–\n1044. IEEE (2022)",
  "topic": "Portuguese",
  "concepts": [
    {
      "name": "Portuguese",
      "score": 0.7651921510696411
    },
    {
      "name": "Computer science",
      "score": 0.7568215727806091
    },
    {
      "name": "Natural language processing",
      "score": 0.6174002289772034
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.6027941107749939
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.6017853021621704
    },
    {
      "name": "Suite",
      "score": 0.5752226710319519
    },
    {
      "name": "Language model",
      "score": 0.558066189289093
    },
    {
      "name": "Artificial intelligence",
      "score": 0.534392774105072
    },
    {
      "name": "Spanish language",
      "score": 0.4355652928352356
    },
    {
      "name": "European Portuguese",
      "score": 0.4319758713245392
    },
    {
      "name": "Linguistics",
      "score": 0.4258202314376831
    },
    {
      "name": "Machine learning",
      "score": 0.17427998781204224
    },
    {
      "name": "History",
      "score": 0.09366527199745178
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}