{
  "title": "Assessing risk of bias in human environmental epidemiology studies using three tools: different conclusions from different tools",
  "url": "https://openalex.org/W3097377405",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5042474641",
      "name": "Stephanie M. Eick",
      "affiliations": [
        "University of California, San Francisco"
      ]
    },
    {
      "id": "https://openalex.org/A5040665913",
      "name": "Dana E. Goin",
      "affiliations": [
        "University of California, San Francisco"
      ]
    },
    {
      "id": "https://openalex.org/A5048516912",
      "name": "Nicholas Chartres",
      "affiliations": [
        "University of California, San Francisco"
      ]
    },
    {
      "id": "https://openalex.org/A5016607357",
      "name": "Juleen Lam",
      "affiliations": [
        "University of California, San Francisco"
      ]
    },
    {
      "id": "https://openalex.org/A5048164907",
      "name": "Tracey J. Woodruff",
      "affiliations": [
        "University of California, San Francisco"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2066218383",
    "https://openalex.org/W4236277434",
    "https://openalex.org/W2180426014",
    "https://openalex.org/W1997898721",
    "https://openalex.org/W2098923148",
    "https://openalex.org/W4254480653",
    "https://openalex.org/W2255858454",
    "https://openalex.org/W2461177968",
    "https://openalex.org/W2906243336",
    "https://openalex.org/W2015957719",
    "https://openalex.org/W2805584423",
    "https://openalex.org/W1840938196",
    "https://openalex.org/W2071955543",
    "https://openalex.org/W2030551462",
    "https://openalex.org/W2033890227",
    "https://openalex.org/W2984065930",
    "https://openalex.org/W2743038351",
    "https://openalex.org/W2345569386",
    "https://openalex.org/W4243002209",
    "https://openalex.org/W2522738868",
    "https://openalex.org/W2134700529",
    "https://openalex.org/W2736010387",
    "https://openalex.org/W65520449",
    "https://openalex.org/W2531269403",
    "https://openalex.org/W1731051886",
    "https://openalex.org/W2051378230",
    "https://openalex.org/W2066749251",
    "https://openalex.org/W2055985809",
    "https://openalex.org/W2132733741",
    "https://openalex.org/W2069787673",
    "https://openalex.org/W2045496595",
    "https://openalex.org/W1208593343",
    "https://openalex.org/W2069523442",
    "https://openalex.org/W1598602811",
    "https://openalex.org/W2051556928",
    "https://openalex.org/W4296024375",
    "https://openalex.org/W2588681363",
    "https://openalex.org/W3041430913",
    "https://openalex.org/W2116810060",
    "https://openalex.org/W1969835924"
  ],
  "abstract": "Abstract Background Systematic reviews are increasingly prevalent in environmental health due to their ability to synthesize evidence while reducing bias. Different systematic review methods have been developed by the US National Toxicology Program’s Office of Health Assessment and Translation (OHAT), the US Environmental Protection Agency’s (EPA) Integrated Risk Information System (IRIS), and by the US EPA under the Toxic Substances Control Act (TSCA), including the approach to assess risk of bias (ROB), one of the most vital steps which is used to evaluate internal validity of the studies. Our objective was to compare the performance of three tools (OHAT, IRIS, TSCA) in assessing ROB. Methods We selected a systematic review on polybrominated diphenyl ethers and intelligence quotient and/or attention deficit hyperactivity disorder because it had been endorsed by the National Academy of Sciences. Two reviewers followed verbatim instructions from the tools and independently applied each tool to assess ROB in 15 studies previously identified. We documented the time to apply each tool and the impact the ROB ratings for each tool had on the final rating of the quality of the overall body of evidence. Results The time to complete the ROB assessments varied widely (mean = 20, 32, and 40 min per study for the OHAT, IRIS, and TSCA tools, respectively). All studies were rated overall “low” or “uninformative” using IRIS, due to “deficient” or “critically deficient” ratings in one or two domains. Similarly, all studies were rated “unacceptable” using the TSCA tool because of one “unacceptable” rating in a metric related to statistical power. Approximately half of the studies had “low” or “probably low ROB” ratings across all domains with the OHAT and Navigation Guide tools. Conclusions Tools that use overall ROB or study quality ratings, such as IRIS and TSCA, may reduce the available evidence to assess the harms of environmental exposures by erroneously excluding studies, which leads to inaccurate conclusions about the quality of the body of evidence. We recommend using ROB tools that circumvents these issues, such as OHAT and Navigation Guide. Systematic review registration This review has not been registered as it is not a systematic review.",
  "full_text": "RESEARCH Open Access\nAssessing risk of bias in human\nenvironmental epidemiology studies using\nthree tools: different conclusions from\ndifferent tools\nStephanie M. Eick 1, Dana E. Goin 1, Nicholas Chartres 1, Juleen Lam 1,2 and Tracey J. Woodruff 1*\nAbstract\nBackground: Systematic reviews are increasingly prevalent in environmental health due to their ability to\nsynthesize evidence while reducing bias. Different systematic review methods have been developed by the US\nNational Toxicology Program ’s Office of Health Assessment and Translation (OHAT), the US Environmental\nProtection Agency ’s (EPA) Integrated Risk Information System (IRIS), and by the US EPA under the Toxic Substances\nControl Act (TSCA), including the approach to assess risk of bias (ROB), one of the most vital steps which is used to\nevaluate internal validity of the studies. Our objective was to compare the performance of three tools (OHAT, IRIS,\nTSCA) in assessing ROB.\nMethods: We selected a systematic review on polybrominated diphenyl ethers and intelligence quotient and/or\nattention deficit hyperactivity disorder because it had been endorsed by the National Academy of Sciences. Two\nreviewers followed verbatim instructions from the tools and independently applied each tool to assess ROB in 15\nstudies previously identified. We documented the time to apply each tool and the impact the ROB ratings for each\ntool had on the final rating of the quality of the overall body of evidence.\nResults: The time to complete the ROB assessments varied widely (mean = 20, 32, and 40 min per study for the OHAT,\nIRIS, and TSCA tools, respectively). All studies were rated overall “low” or “uninformative” using IRIS, due to “deficient” or\n“critically deficient” ratings in one or two domains. Similarly, all studies were rated “unacceptable” using the TSCA tool\nbecause of one “unacceptable” rating in a metric related to statistical power. Approximately half of the studies had\n“low” or “probably low ROB ” ratings across all domains with the OHAT and Navigation Guide tools.\nConclusions: Tools that use overall ROB or study quality ratings, such as IRIS and TSCA, may reduce the available\nevidence to assess the harms of environmental exposures by erroneously excluding studies, which leads to inaccurate\nconclusions about the quality of the body of evidence. We recommend using ROB tools that circumvents these issues,\nsuch as OHAT and Navigation Guide.\nSystematic review registration: This review has not been registered as it is not a systematic review.\nKeywords: Risk of bias, Systematic review, Quality assessment, Critical appraisal, Evidence evaluation, Risk assessment\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if\nchanges were made. The images or other third party material in this article are included in the article's Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons\nlicence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain\npermission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nThe Creative Commons Public Domain Dedication waiver ( http://creativecommons.org/publicdomain/zero/1.0/) applies to the\ndata made available in this article, unless otherwise stated in a credit line to the data.\n* Correspondence: tracey.woodruff@ucsf.edu\n1Program on Reproductive Health and the Environment, Department of\nObstetrics, Gynecology and Reproductive Sciences, University of California,\nSan Francisco, USA\nFull list of author information is available at the end of the article\nEick et al. Systematic Reviews           (2020) 9:249 \nhttps://doi.org/10.1186/s13643-020-01490-8\nBackground\nSystematic review methods are becoming increasingly rec-\nommended and used to inform environmental health de-\ncisions [ 1– 4]. Application of these methods supports\ngreater transparency in the scientific basis and judgments\nbehind public health decisions [ 5]. Well-conducted sys-\ntematic reviews therefore result in improved consistency\nof evidence with less bias [ 5]. Systematic reviews have\nbeen adapted from clinical medicine, where data is largely\nderived from randomized, controlled trials [ 6]. However,\nenvironmental health systematic reviews used to inform\nthe relationship between exposures and adverse health\noutcomes must typically rely on data from human obser-\nvational studies due to ethical considerations [ 5]. The Uni-\nversity of California, San Francisco (UCSF), developed the\nNavigation Guide, which was the first systematic review\nmethodology aimed to evaluate the evidence linking envir-\nonmental exposures to adverse health outcomes [ 5]. Since\nthen, different systematic review methods have been de-\nveloped and implemented by various US authoritative\nbodies and agencies including the National Toxicology\nProgram (NTP)’s Office of Health Assessment and Trans-\nlation (OHAT) [ 7], Environmental Protection Agency\n(EPA)’s Integrated Risk Information System (IRIS) [ 4], and\nby EPA under the Toxic Substances Control Act (TSCA)\n[8] to inform environmental health decisions that have\nmajor implications for public health. An important benefit\nof these tools is that they are specifically designed for\nevaluating studies of environmental exposures, thus elim-\ninating the translation required from more generic tools\napplied in clinical medicine [ 4].\nThe evaluation of the risk of bias (ROB) of the in-\ncluded studies is a key component in a systematic review\nto assess the validity of the studies which are the basis of\nthe conclusions of the review. ROB is a measure of in-\nternal study validity that reflects features of a study ’s\nmethodological design, conduct, or analysis that can lead\nto a systematic under- or over-estimation of the true ef-\nfect the study aims to estimate [ 9].\nIn clinical systematic reviews, the Cochrane Collabora-\ntion’s ROB evaluation tool is widely accepted and used\n[6]. However, as environmental health assessment evi-\ndence often consists of observational human studies of\nexposure to evaluate harms, this tool cannot be applied\ndirectly. While there are multiple tools currently in use\nto assess the ROB in human observational studies, there\nis a lack of consensus over advantages of any one single\ninstrument over another for use in systematic review or\nguideline development [ 10]. There is also limited empir-\nical evidence about how well individual questions on\nexisting and new ROB instruments assess the exposure-\noutcome relationship [ 11]. Selection of the items for a\nROB tool should, at a minimum, be informed from do-\nmains with empirical evidence in randomized trials and\nconceptual considerations [ 12, 13]. The lack of agree-\nment on the most appropriate method to use to evaluate\nthe ROB of observational design studies can make both\nsystematic reviews and the assessments and guidelines\nthey inform challenging to evaluate and interpret [ 14].\nFurthermore, the application of different tools to assess\nthe ROB of a single observational study has been shown\nto lead to different conclusions [ 15, 16].\nAlthough there is no consensus on which is the best tool\nto assess ROB in observational studies, several tools have\nbeen developed from the tools used to evaluate clinical in-\nterventions [ 5, 7, 9]. Methods such as the Navigation\nGuide rely on the evaluation of individual domains, which\nprovides a structured framework within which to make\nqualitative decisions on the overall quality of studies and\nto identify potential sources of bias [ 17]. In contrast, sev-\neral methods include an “overall quality score ” to assess\nROB of individual studies. Recent studies have suggested\nthat it is inappropriate to use these “overall quality scores ”\n[14], as they have been shown to inadequately distinguish\nbetween studies with a high and low ROB in meta-\nanalyses [ 18, 19]. Importantly, there is a lack of empirical\nevidence to demonstrate how individual items should be\nweighted when creating “overall quality scores ” [17, 18,\n20]. Thus, overall ROB scores can lead to biased evalua-\ntions of the literature, as several systematic review\nmethods propose that studies that receive poor ratings for\noverall study quality are excluded from the overall body of\nevidence [ 20]. Overall quantitative scores have also been\ncriticized and recommended against by the National\nAcademy of Sciences (NAS), as they can be a poor reflec-\ntion of overall study quality [ 21, 22]. While there is inevit-\nably variation in the internal validity and ROB across\nindividual studies, the most appropriate method to ex-\nclude studies is based on predefined exclusion criteria, as\nopposed to an arbitrary rating of the evidence, which is\noften based on a limited number of domains with minimal\nempirical basis [ 7].\nThere is a need, therefore, to understand how different\ntools designed to assess the ROB of human observational\nstudies compare to one another in terms of their reliabil-\nity and usability, in particular, those that are used by\ngovernment agencies due to their importance in policy\nand regulatory decision-making. Further, it is important\nto understand if differences between these ROB methods\ninfluence the overall conclusions of a systematic review,\nwhich could affect conclusions about the evidence and\nsubsequent public health recommendations and action\nby decision-makers. Therefore, the objective of this\nstudy was to compare the performance of the different\nROB tools used by NTP ’s OHAT [ 7], US EPA ’s IRIS\nprogram [ 4], and by US EPA under TSCA [ 8] in a case\nstudy relevant to human environmental epidemiology.\nSpecifically, we aimed to assess the usability of each tool\nEick et al. Systematic Reviews           (2020) 9:249 Page 2 of 13\nby calculating the mean time to conduct the ROB assess-\nment, evaluate inter-rater reliability for each tool, and\ndetermine the impact of the ROB ratings for each tool\non the conclusions with respect to the overall quality of\nthe body of evidence.\nMethods\nCase study\nWe compiled a diverse team of researchers with expertise\nin reproductive epidemiology , environmental health, sys-\ntematic review, and public health, all with doctoral degrees.\nTwo reviewers (SME, DEG), applied the OHAT, IRIS, and\nTSCA systematic review tools to 15 human epidemiologic\nstudies on polybrominated diphenyl ethers (PBDEs) and\nintelligence quotient (IQ) an d/or attention deficit hyper-\nactivity disorder (ADHD) from a previously completed sys-\ntematic review [ 23]. We selected this review because it\napplied the Navigation Guide framework to assess ROB, a\nframework that has been empirically demonstrated on six\ncase studies to date [ 23– 28]a sw e l la se n d o r s e db yt h e\nNAS [ 29]. In particular, the PBDE and IQ/ADHD case\nstudy was critically evaluated by the NAS, concluding that\nthere was no evidence of bias in the assessment [ 3]. How-\never, we acknowledge that there is likely no tool that can\nprovide a fully objective measure of ROB. Although numer-\nous tools exist to assess ROB, we focused on the OHAT\n[7], IRIS [ 4], and TSCA [ 8] tools in our review because they\na r ec u r r e n t l yi nu s eb yt h eU Sg o v e r n m e n t( N T P ,E P A ) ,\nand routinely applied for asse ssing risks from environmen-\ntal chemicals. These tools all evaluate different constructs\nrelated to validity, including elements of ROB as well as\nstudy quality (Table 1). Therefore, for consistency, we sub-\nsequently use the term ROB when describing these tools\nand how they were applied.\nRisk of bias tools\nOHAT\nThe OHAT handbook describes a tool for evaluating the\nROB for both human and animal studies [ 7]. The OHAT\ntool consists of an overall set of questions, of which cer-\ntain questions are relevant for human versus animal\nstudies. The OHAT tool is domain-based and asks seven\nquestions relevant for human studies which cover six\npossible sources of bias: participant selection, confound-\ning, attrition/exclusion, detection, selective reporting,\nand other sources [ 7]. The detection bias domain in-\ncludes two questions, one on exposure characterization\nand the other on outcome characterization. Answer op-\ntions for each question are the ratings “definitely low, ”\n“probably low, ”“ probably high, ” and “definitely high ”\nROB. The OHAT tool does not apply an overall rating\nfor each study. Additionally, OHAT instructs that stud-\nies should not be removed from consideration of the\noverall body of evidence as a result of “probably high ” or\n“definitely high ” ratings.\nIRIS\nThe IRIS tool [ 4] is an adaption of the risk of bias in\nnon-randomized studies of interventions (ROBINS-I),\nwhich evaluates the ROB predominantly in non-\nrandomized studies investigating the comparative effect-\niveness (harm or benefit) of clinical interventions [ 30].\nThe IRIS method states that it has modified this tool for\napplications to assess population health risks resulting\nfrom exposures to environmental chemicals. IRIS states\nthat the tool is designed to evaluate study quality or\nstudy utility, which encompasses multiple issues includ-\ning ROB, study execution, study sensitivity, and report-\ning of results. The IRIS tool is a domain-based tool and\nincludes seven questions spanning possible sources of\nbias: participant selection, confounding, selective report-\ning, exposure measurement, outcome ascertainment\nanalysis, and sensitivity. For each question, answer op-\ntions are the ratings “good” (indicative of a low ROB),\n“adequate” (indicative of a probably low ROB), “defi-\ncient” (indicative of a probably high ROB), “critically de-\nficient” (indicative of a high ROB), and “not reported. ”\nIn addition to the seven sources of bias, the IRIS tool\nalso includes a question on “overall study confidence, ”\nwhich relies on interpreting the ratings from all domains\nand on reviewer interpretation, as opposed to utilizing a\nweighted average or other numeric equation. The IRIS\nguidelines say that overall study confidence “will be\nbased on the reviewer judgments across the evaluation\ndomains for each health outcome under consideration,\nand will include the likely impact the noted deficiencies\nin bias and sensitivity, or inadequate reporting, have on\nthe results. ” This can be interpreted as “high” if all do-\nmains are rated “good,”“ medium” if all domains are\nrated “adequate” or “good,”“ low” if one or more do-\nmains are rated “deficient,” and “uninformative” if any\ndomain is rated “critically deficient. ” However, the IRIS\ntool permits subjective reviewer judgment when asses-\nsing overall study confidence, and alternate approaches\nmay be possible. Therefore, while we used the instruc-\ntions explicitly to rate the ROB, in an effort to examine\nthe robustness of our original findings, we also con-\nducted a sensitivity analysis to determine if the overall\nstudy confidence rating would vary with alternative guid-\nance. Specifically, we rated the overall study confidence\nto be “high” if none or one of the domains was rated as\n“deficient,”“ medium” if two of the domains were “defi-\ncient,” and “low” if three or more domains were “defi-\ncient” or “critically deficient. ” As with the original rating\nprocess, studies deemed “low” or “uninformative” overall\nstudy quality are removed from the overall body of\nevidence.\nEick et al. Systematic Reviews           (2020) 9:249 Page 3 of 13\nTable 1 Description of tools used to assess risk of bias\nProgram or\nagency\nTool Assessment process Domains assessed Number of\nquestions\nAvailable answer options\nNTP OHAT Risk of bias, internal validity Selection bias 1 Definitely low ROB; probably low ROB; probably\nhigh ROB; definitely high ROB\nConfounding bias 1 Definitely low ROB; probably low ROB; probably\nhigh ROB; definitely high ROB\nAttrition/exclusion bias 1 Definitely low ROB; probably low ROB; probably\nhigh ROB; definitely high ROB\nDetection bias 2 Definitely low ROB; probably low ROB; probably\nhigh ROB; definitely high ROB\nSelective bias reporting\nbias\n1 Definitely low ROB; probably low ROB; probably\nhigh ROB; definitely high ROB\nOther bias 1 Definitely low ROB; probably low ROB; probably\nhigh ROB; definitely high ROB\nEPA IRIS Study evaluation Exposure measurement 1 Good; adequate; deficient; critically deficient\nOutcome\nascertainment\n1 Good; adequate; deficient; critically deficient\nParticipant selection 1 Good; adequate; deficient; critically deficient\nConfounding 1 Good; adequate; deficient; critically deficient\nAnalysis 1 Good; adequate; deficient; critically deficient\nSelective reporting 1 Good; adequate; deficient\nSensitivity 1 Adequate; deficient\nOverall study\nconfidence\n1 High; medium; low; uninformative\nEPA TSCA Data quality criteria for\nepidemiological studies\nStudy population 3 High; medium; low; unacceptable\nExposure\ncharacterization\n3 High; medium; low; unacceptable\nOutcome assessment 2 High; medium; low; unacceptable*\n*Unacceptable not available for one question\nPotential confounding/\nvariable control\n3 High; medium; low; unacceptable*\n*High and unacceptable not available for one\nquestion\nAnalysis 4 High; medium; low; unacceptable*\n*High and low not available for two questions\n*High and unacceptable not available for two\nquestions\nBiomarker selection and\nmeasurement\n7 High; medium; low; unacceptable\n*High and unacceptable not available for one\nquestion\n*Unacceptable not available for three questions\nOverall study\nconfidence\n1 Sum of weighted scores/sum of metric\nweighting factor\nUCSF Navigation\nGuide\nRisk of bias, internal validity Source population\nrepresentation\n1 Low; probably low; probably high; high\nBlinding 1 Low; probably low; probably high; high\nExposure assessment 1 Low; probably low; probably high; high\nOutcome assessment 1 Low; probably low; probably high; high\nIncomplete outcome\ndata\n1 Low; probably low; probably high; high\nSelective outcome\nreporting\n1 Low; probably low; probably high; high\nEick et al. Systematic Reviews           (2020) 9:249 Page 4 of 13\nTSCA\nThe TSCA ROB tool [ 8] is presented as a ROB evalu-\nation tool that includes quantitative scoring. The TSCA\ntool is domain-based and employs a quality-based scor-\ning system which includes 22 metrics (questions). These\nmetrics are grouped into 6 possible sources of bias:\nstudy population, exposure characterization, outcome\nassessment, potential confounding/variable control, ana-\nlysis, and other. These metrics conflate ROB concepts\nwith study quality domains (e.g., how well the study is\nreported). Answer options include “high” (a quantitative\nscore of 1; indicative of a low ROB), “medium” (score of\n2; indicative of a probably low ROB), “low” (score of 3;\nindicative of a probably high ROB), and “unacceptable”\n(score of 4; indicative of a probably high ROB). Not all\nanswer options are available for each question. For ex-\nample, the question assessing co-exposure confounding\nrequires reviewers to choose “medium” or “low” only\nand it does not allow choosing “high” and “unaccept-\nable” (Table 1).\nThe TSCA method identifies key or “critical” metrics\nand gives them a higher weight than other metrics within\nthe same domain. Examples of critical metrics include\nparticipant selection, temporality, and covariate adjust-\nment. Critical metrics are assigned a weighting factor that\nis twice the value of the other metrics within the same do-\nmain and the non-critical metrics are assigned a weighting\nfactor of half the weighting factor assigned to the critical\nmetrics within each domain. The TSCA method states\nthat “critical metrics are identified based on professional\njudgment in conjunction with consideration of the factors\nthat are most frequently included in other study quality/\nrisk of bias tools for epidemiologic literature ” [8]. How-\never, there is no documented reference for this decision\nprocess and the TSCA method has not gone through peer\nreview. If the response to any individual question is “un-\nacceptable,” the overall study quality is automatically rated\nas “unacceptable.” Studies deemed “unacceptable” are re-\nmoved from the overall body of evidence. Each study is\nsubsequently given an overall score, calculated by multi-\nplying the score for each metric by a weighting factor and\nthen dividing by the sum of the weighting factors (see\nSupplemental Material).\nDetailed instructions for making ROB determinations\nprovided by each tool are provided in the Supplemental\nMaterial (see “Instructions for making risk of bias deter-\nminations using OHAT framework, ”“ Instructions for\nmaking risk of bias determinations using IRIS frame-\nwork,” and “Instructions for making risk of bias determi-\nnations using TSCA framework ”). Instructions applied\nin this case study were taken verbatim from the methods\ndocumentation of each tool. The TSCA tool applied in\nthe risk evaluation for 1,4-Dioxane was applied here.\nComparison of domains assessed across tools\nThe Navigation Guide, OHAT, IRIS, and TSCA tools all\nassess bias due to exposure and outcome measurement,\nstudy population, and confounding. The Navigation\nGuide is the only tool to include conflicts of interest as a\nseparate ROB domain. In contrast, TSCA is the only tool\nto consider biomarker selection and measurement. IRIS\nand TSCA are the only tools that include indicators of\noverall study quality.\nApplication of risk of bias tool and statistical analysis\nPrior to applying the different tools, the two reviewers\n(SME, DEG) completed training (approximately 4 h) on\nassessing ROB in epidemiology studies with a systematic\nreview expert (JL). Trainings included a broad overview\non assessing ROB and specific clarification on the appli-\ncation of each tool.\nEach tool asks at least one question regarding con-\nfounders or covariates retained in adjusted models (see\n“Instructions for making risk of bias determinations ” in\nthe Supplemental Material). Therefore, we made pre-\nspecified list of important confounders (Tier 1) and other\npotentially relevant confounders (Tier 2) from the previ-\nously completed systematic review on PBDEs and IQ and/\nor ADHD [ 23]. Confounders were identified by individuals\nwith subject matter expertise on PBDEs and IQ and/or\nADHD. This list included Tier 1 (HOME inventory, ma-\nternal age, maternal education, marital status, maternal\nuse of alcohol during pregnancy, maternal depression,\nhousehold income/poverty, gestational exposure to envir-\nonmental tobacco smoke, child sex, exposure to other\nneurotoxic agents) and Tier 2 (birth weight or gestational\nage, number of children in the home, fathers ’ presence in\nthe home, preschool, and out-of-home child care attend-\nance, psychometrician, location, and language of assess-\nment) confounders. Tier 1 and Tier 2 confounders were\nTable 1 Description of tools used to assess risk of bias (Continued)\nProgram or\nagency\nTool Assessment process Domains assessed Number of\nquestions\nAvailable answer options\nConfounding 1 Low; probably low; probably high; high\nConflicts of interest 1 Low; probably low; probably high; high\nOther 1 Low; probably low; probably high; high\nAbbreviations: NTP National Toxicology Program, OHAT, Office of Health Assessment and Translation, EPA, Environmental Protection Agency, IRIS Integrated Risk\nInformation System, TSCA Toxic Substances Control Act; ROB risk of bias, UCSF, University of California, San Francisco\nEick et al. Systematic Reviews           (2020) 9:249 Page 5 of 13\nconsistent with the previously completed systematic re-\nview [ 23]. For all tools, we decided that studies including\nall Tier 1 and Tier 2 confounders were rated as low ROB.\nStudies including all Tier 1 confounders were rated as\nprobably low ROB. Studies including only some Tier 1\nconfounders or reported only crude analyses were rated as\nprobably high and high ROB, respectively.\nAfter completing the training, the two reviewers began\nby applying the OHAT tool to independently rate ROB\nfor one article [ 31]. Studies included in the previous sys-\ntematic review [ 23] were reviewed in alphabetical order\nby last name of the first author. The two reviewers\nreviewed the first study and then met, compared ratings,\ndiscussed discrepancies, came to consensus on ratings,\nand standardized their approach to improve the clarity\nof the OHAT instructions and maximize consistency in\nsubsequent ratings. Researchers entered their ROB rat-\nings and justification in Microsoft Excel. After assessing\nROB using the OHAT tool in the remaining studies, the\ntwo reviewers met to discuss their ratings. In the event\nthat the reviewers had different ratings, a consensus was\nreached via discussion. A third reviewer (JL) was brought\nin to resolve any discrepancies in case consensus could\nnot be reached. This same approach was also used for\napplying first the IRIS then the TSCA tool.\nEach reviewer tracked the time it took to complete the\nROB assessment for each tool. The time to apply each tool\nhas been used in previous study as an indicator of ease of\nuse for ROB tools [ 14]. The total time and average time to\nreview individual studies was calculated for the individual\ntools. We standardized this estimate by dividing by the\nnumber of questions on each tool. Kappa statistics were\ncalculated for each tool as measures of inter-rater reliabil-\nity using the package “IRR” in R version 3.6.0. Kappa\nvalues range from 0 to 100%, where 0% indicates no agree-\nment and 100% indicates perfect agreement.\nResults\nA description of the different tools, domains assessed,\nand number of questions and available answer options\nfor each tool is provided in Table 1 and a detailed de-\nscription of what each domain measures is provided in\nthe Supplemental Information (Table S1).\nIt took approximately 5 h for each reviewer to review\nall 15 studies using the OHAT tool (average 20 min per\nstudy and 3 min per question); for the IRIS tool, it was\napproximately 8 h (average of 32 min per study and 4\nmin per question); and the TSCA tool was the longest,\napproximately 10 h (average 40 min per study and 2 min\nper question). Kappa values ranged from 54 to 58%.\nSimilarities and differences across tools\nWe found consistent ratings across the Navigation Guide,\nOHAT, IRIS, and TSCA tools for the domains of exposure\ncharacterization, outcome measurement, and confounding\n(Fig. 1). We found ratings of low or probably low bias\nacross the tools for exposure characterization (Fig. 1). For\noutcome measurement, ratings were also consistent with\nthe same four studies rated down across the tools in this\ndomain [ 32– 35]. Nearly all studies were considered to be\nat a high ROB due to confounding when using the IRIS,\nOHAT, and TSCA tools (Fig. 1).\nWe found differences in ratings across the tools for\nthe domains of selection of study participants and select-\nive reporting (Fig. 1). The Navigation Guide, OHAT,\nand TSCA tools generally rated studies as having low\nROB due to selection of study participants. In contrast,\nthe majority of studies were rated as being of medium or\nhigh ROB due to selection of study participants using\nIRIS. For selective reporting, using the IRIS tool, 12 out\nof 15 studies received “good” or “adequate” ratings for\nthis domain. A different set of 2 studies received a rating\nof “probably high ” ROB using OHAT [ 32, 36], these\nstudies were generally not rated as having a high ROB in\nthe other tools. We rated an additional 2 out of the 15\nstudies as “definitely high ” ROB due to selective report-\ning using the OHAT tool [ 33, 34], these studies received\nsimilar ROB ratings for this question using the other\ntools. One of these studies was classified as “high” ROB\nusing the Navigation Guide, along with one other study\n[37]. A detailed description of the final consensus deci-\nsions for each individual study across the IRIS, TSCA,\nand OHAT tools is provided in the Supplemental Infor-\nmation (Tables S3-S17).\nDifferences were also observed between the tools with\nrespect to overall study confidence, as the IRIS and\nTSCA tools were the only ones to calculate overall study\nconfidence metrics. All studies were retained in the body\nof evidence using the OHAT and Navigation Guide\nmethods. In contrast, the IRIS and TSCA tools substan-\ntially reduced the number of studies available.\nOverall study confidence determinations using OHAT\nWhen evaluating the confidence of the body of evidence\nusing the OHAT tool, approximately half of the studies\nhad “low” or “probably low ” ROB ratings across all do-\nmains, with the exception of confounding bias and attri-\ntion/exclusion bias. Seven out of 15 studies received\n“probably high ” or “high” ratings across three or more\nquestions (Fig. 2).\nOverall study confidence determinations using IRIS\nUsing the IRIS tool, every study was rated either “low”\nor “uninformative” overall study confidence, with 13\nstudies rated “low” and two studies rated “uninforma-\ntive” (Fig. 3). Most studies received “low” overall ratings\nas a result of bias due to confounding, participant selec-\ntion, or sensitivity.\nEick et al. Systematic Reviews           (2020) 9:249 Page 6 of 13\nFig. 1 Comparison of risk of bias determinations for Navigation Guide, OHAT, IRIS, and TSCA tools. Abbreviations: OHAT, Office of Health\nAssessment and Translation; IRIS, Integrated Risk Information System; TSCA, Toxic Substances Control Act; ROB, risk of bias. Note: Double plus sign\nindicates low ROB, single plus sign indicates probably low ROB, single minus sign indicates probably high ROB, double minus sign indicates high\nROB for IRIS, OHAT, and Navigation Guide. 1 indicates high, 2 indicates medium, 3 indicates low, 4 indicates unacceptable for TSCA. Study\npopulation for TSCA pertains to question 1. Attrition/exclusion for TSCA pertains to question 2. Sensitivity for TSCA pertains to question 5. Scores\nfor TSCA were calculated by using the weighted sum of the individual questions within each domain\nEick et al. Systematic Reviews           (2020) 9:249 Page 7 of 13\nThe sensitivity analysis examined a different definition\nfor overall study confidence using the IRIS tool, which\nled to different overall study ratings. One study was\nrated as “high,” nine studies were rated as “medium,”\nand five studies were determined to have “low” overall\nstudy confidence, respectively (see Supplemental Infor-\nmation Figure S2). Consistent with our main findings,\nstudy confidence was generally downgraded from “high”\nor “medium” to “low” confidence as a result of bias due\nto analysis, confounding, and/or participant selection.\nOverall study confidence determinations using TSCA\nAll studies were rated “unacceptable” for overall study\nquality using the TSCA tool (Fig. 4). This occurred as a re-\nsult of all studies being rated “unacceptable” for the\nquestion pertaining to statistical power. For the statistical\npower question, answer options included “medium” and\n“unacceptable” only (Table 1). To be rated “medium,” the\nnumber of participants needed to be adequate to detect an\neffect in the exposed population or study authors had to\nreport that they had ≥ 80% power. No studies reported a\npower calculation and most studies had relatively small\nsample sizes.\nTwo studies [34, 35] were additionally rated“unacceptable”\nfor bias due to exposure levels and three others [ 32, 34, 38]\nfor bias due to study design and methods using TSCA. For\nthe exposure levels question, answer options were “medium,\n”“ low,” and “unacceptable” only. To be rated “unacceptable,”\nreviewers determined that the distribution of exposure was\nnot adequate to determine an exposure-outcome\nFig. 2 Summary of risk of bias judgments (low, probably low, probably high, high) using the OHAT framework for the human studies included in\nour case series. The justification for risk of bias designations for individual studies are provided in Tables S3-S17. Kappa value was 56% (95%\nconfidence interval 44-66%). Note: Double plus sign indicates low, single plus sign indicates probably low, single minus sign indicates probably\nhigh, double minus sign indicates high study quality\nFig. 3 Summary of risk of bias judgments (good, adequate, deficient, critically deficient) using the IRIS framework for the human studies included\nin our case series. The justification for risk of bias designations for individual studies are provided in Tables S3-S17. Kappa value was 58% (95%\nconfidence interval 48-69%). Note: For individual domains double plus sign indicates good, single plus sign indicates adequate, single minus sign\nindicates deficient, and double minus sign indicates critically deficient. For overall study confidence double plus sign indicates high, single pl us\nsign indicates medium, single minus sign indicates low, and double minus sign indicates uninformative study quality\nEick et al. Systematic Reviews           (2020) 9:249 Page 8 of 13\nrelationship or study authors did not provide information on\nthe range of exposure levels. Answer options for the study\ndesign and methods question were “medium” and “un-\nacceptable” only. Studies were rated “unacceptable” for this\nquestion if inappropriate statistical analyses were applied or\nif the study design was not appropriate for the research\nquestion.\nROB determinations using the Navigation Guide\nFinal ROB determinations made using the Navigation\nGuide in the original review are shown in the Supple-\nmental Information (Figure S1) and final consensus deci-\nsions for the Navigation Guide is provided elsewhere\n[23]. Briefly, most studies received ratings of “low” or\n“probably low ” ROB across most domains. All studies\nwere free of conflicts of interest and all studies were\nrated as “low” or “probably low ” ROB due to exposure\nassessment. With the exception of two studies, all stud-\nies were rated “low” ROB due to selective reporting [ 34,\n37]. Approximately half of studies were rated as “high”\nor “probably high ” ROB due to confounding [ 31– 37]\nand a third of studies were rated as “probably high ” ROB\ndue to blinding [ 33– 37].\nDiscussion\nThe goal of our study was to compare three tools,\nOHAT, IRIS, and TSCA, for assessing ROB in a case\nstudy of human environmental observational evidence of\nPBDEs and IQ and/or ADHD. We also compared these\ntools to the Navigation Guide. Our results find that the\nuse of these different tools can lead to different conclu-\nsions about the overall body of evidence, which has im-\nportant implications for regulation of hazardous\nchemicals and public health. When the IRIS tool was ap-\nplied, all studies were determined to have “low” or\n“uninformative” confidence in the overall study quality.\nSimilarly, using the TSCA tool, all studies were deter-\nmined to be of “uninformative” study quality, with no\nevidence of sufficient quality to be used to formulate an\noverall conclusion. In contrast, the OHAT tool did not\ninclude an overall quality indicator for individual studies,\nwhich allowed all studies to be retained in the body of\nevidence.\nOur findings are consistent with other studies that\nhave assessed systematic review methods, namely that\nusing different tools can result in different conclusions\n[15] and that single ROB summary scores can be mis-\nleading [ 39]. All ROB tools assessed here showed overlap\nin some of the domains assessed, which supports previ-\nous findings [ 39]. For example, there is some consistency\nin the consideration of the domains across the Naviga-\ntion Guide, IRIS, and OHAT tools for how they evaluate\nconfounding, exposure assessment, outcome assessment,\nand selective reporting as they have been derived from\nthe domains with empirical testing in randomized clin-\nical trials. Further, as these domains are derived from\nthe empirical tests of randomized clinical trials, they\nminimize over or double counting bias domains.\nHowever, an important distinction between the IRIS,\nOHAT, and TSCA tools is that the IRIS tool includes a\nsubjective indicator, as opposed to a weighted average or\nsimilar, for overall study quality. In our main analysis, all\nstudies were downgraded when assessing ROB and the\noverall study quality due to “low” or “uninformative”\noverall study confidence determinations. These ratings\nwere consistent with the guidance provided in the IRIS\nhandbook. However, there is a great deal of flexibility in\nhow overall study confidence is determined with the\nIRIS tool. For example, the IRIS tool allows studies to be\nclassified as “medium” study confidence if there is a\nFig. 4 Summary of risk of bias judgments (high, medium, low, unacceptable) using the TSCA framework for the human studies included in our case\nseries. The justification for risk of bias designations for individual studies are provided in Tables S3-S17. Kappa value was 54% (95% confidence int erval\n47-61%). Note: 1 indicates high, 2 indicates medium, 3 indicates low, 4 indicates unacceptable study quality. Abbreviations: NA, not applicable\nEick et al. Systematic Reviews           (2020) 9:249 Page 9 of 13\ndeficient rating in a domain that is considered to have\nless influence on the direction of the effect estimate.\nHowever, the handbook does not define which domains\nhave less influence and we were unable to find scientific\nevidence to support judgments of certain domains as be-\ning more influential than others. We felt that rating a\nstudy as “medium” or “low” overall study confidence\nbased on measures that have not been validated or em-\npirically demonstrated was concerning, as it may result\nin the exclusion of studies that are ultimately inform-\native when assessing the harms of hazardous exposures.\nThus, in our sensitivity analyses, we used a more lenient\ncriterion for assessing overall study confidence. For the\nsensitivity analysis, we found that only five studies (33%)\nwere removed from the overall quality of evidence due\nto “low” overall study confidence determinations. It is\npossible that other reviewers would have vastly different\ninterpretations of the overall study confidence question,\nwhich could lead to substantial differences in the num-\nber of studies retained in the overall body of evidence, as\nwe have demonstrated.\nAll studies were considered to be “uninformative”\nusing the TSCA tool, as all studies were rated “unaccept-\nable ROB ” for the question regarding statistical power.\nThis resulted in the exclusion of all studies from the\nbody of evidence. Notably, most studies included in our\ncase study were prospective cohort studies, which are\nconsidered a strong evidence base for environmental\nhealth, and should lead to increased confidence in the\nrating as a result of better control for confounders as op-\nposed to cross-sectional or case-control studies. For the\nstatistical power question, the available answer options\nwere limited to only “medium” (indicative of a prob-\nably low ROB) and “unacceptable” (indicative of a high\nROB). To be considered “medium,” the study had to re-\nport ≥ 80% power to detect an effect or that the number\nof exposed participants was adequate to detect an effect.\nHowever, it is suggested that it is inappropriate to con-\nduct post hoc power calculations once statistical analysis\nis complete and so even if a study had ≥ 80% power, if it\ndid not report the results from a power calculation it\nwas to be rated “unacceptable” [40]. The wording of the\nTSCA tool ’s statistical analysis question essentially en-\nsures that all epidemiologic studies are rated “unaccept-\nable” and effectively removes epidemiologic studies from\nthe body of evidence unless there are large, statistically\nsignificant findings for all exposures of interest. Import-\nantly, sample size is one contributing factor to statistical\npower which may influence precision [ 6]. Small studies\noften produce imprecise effect estimates, which does not\nnecessarily mean that they are biased. As demonstrated\nin the previously completed systematic review we used\nto evaluate these tools [ 23], small studies can demon-\nstrate a statistically significant and precise association\nbetween exposure and outcomes when combined in a\nmeta-analysis that increases the statistical power of the\nbody of evidence.\nAn additional limitation of the TSCA tool is the broad\ndomains comprised of 2-7 individual questions. When\napplying the tool, the individual questions for each do-\nmain appear to be very similar and it was often hard for\nstudy reviewers to differentiate between them. For ex-\nample, domain 4 covers potential confounding/variable\ncontrol and includes one question for covariate adjust-\nment, one question for covariate characterization, and\none question for co-exposure confounding. When read-\ning these questions, the reviewers applying the TSCA\ntool found it difficult to separate out the ratings for each\nof these questions so that underlying limitations would\nnot be represented more than once. We found this\ntroubling, as other reviewers may have rated the evi-\ndence differently and experienced more difficulties.\nTherefore, studies could be downgraded or excluded\nmultiple times from the overall body of evidence due to\none aspect of the study design.\nAlthough both the IRIS and TSCA tools include an in-\ndicator for overall study confidence, there exist key dif-\nferences in how this indicator rating is determined. The\nIRIS tool weights each domain equally and reviewers de-\ntermine the overall rating based on considering the rat-\nings across several domains. In contrast, the TSCA tool\nuses a numerical scoring system which weights individ-\nual questions. Despite using different indicators for over-\nall study quality, we found that both the IRIS and TSCA\ntools excluded all studies from further evaluation based\non the rating from one or two domains or metrics. Im-\nportantly, by using these overall scoring systems, it is\nimpossible to determine the differences between studies\nthat were rated down for a single question or multiple\nquestions [ 14]. Furthermore, there is no empirical evi-\ndence demonstrating how each ROB domain should be\nweighted [ 41] and the exclusion of studies based on an\narbitrary rating of the evidence is not supported. It has\nalso been empirically demonstrated overall “quality\nscores” are unable to distinguish between studies with a\nhigh or low ROB in meta-analyses [ 20, 42]. Thus, includ-\ning only “high” quality studies may lead to a biased\nevaluation of the evidence, as there is no scientific justi-\nfication for the use of overall quality scoring measures. If\nstudies are to be excluded from a body of evidence, it is\nmore appropriate to evaluate their influence on the over-\nall effect estimates quantitatively using meta-analysis.\nStrategies including conducting sensitivity analyses\nwhich calculate overall effect estimates among high qual-\nity studies only or stratifying results based on overall\nstudy quality. Researchers may also choose to present all\nstudies and qualitatively discuss the ROB using struc-\ntured approaches, similar to OHAT and GRADE [ 43].\nEick et al. Systematic Reviews           (2020) 9:249 Page 10 of 13\nThe TSCA tool also includes study reporting measures\nin its scoring of studies. Study reporting addresses how\nwell a study ’s findings are detailed, which contrasts with\nROB which is designed to assess internal validity of a\nstudy and research quality. Within the TSCA tool, these\nstudy reporting guidelines are incorporated into the justi-\nfication for rating studies as “low” (metrics 1 and 15) or\n“unacceptable” (metrics 2-7). Validated guidelines and\nchecklists to enhance study reporting already exist (see\nStrengthening of Reporting of Observational Studies in\nEpidemiology [STROBE]) [ 44]. These guidelines have\nbeen developed to help ensure authors present all infor-\nmation needed to assess the quality and meaning of the\nresearch in the study. Importantly, STROBE guidelines\nspecifically state that indicators of study reporting are not\na measure of the quality of the underlying research [ 44].\nAn important goal of our study was to assess the\nease and feasibility of using the different tools. The\nkappa statistics indicated moderate inter-rater reliabil-\nity for all the different tools and providing more de-\ntail for each individua l question could help to\nimprove the agreement across all of the tools assessed\nhere. Nonetheless, the inter-rater reliability findings\nare consistent with previ ous research assessing the\nuse of different ROB tools that also found moderate\nkappa values when using the ROBINS-I (of which the\nIRIS tool is adapted) tool [ 45].\nOur study has a number of important strengths. To\nthe best of our knowledge, this is the first time that the\nreliability and validity of these different tools has been\ntested and compared. Additionally, we had two repro-\nductive epidemiologists and a team member with expert-\nise in systematic review and evaluating ROB complete\nthe ROB assessments.\nWe also acknowledge our limitations. Our team did\nnot include a neurodevelopment or biomarker assess-\nment subject matter expert, and inclusion of these indi-\nviduals may have led to different ratings for some of the\ndomains. For example, in our case study, we assessed\nconfounding using a yes/no approach where the study\nhad to either adjust for all confounders on our pre-\ndetermined list or report that the confounders were\nevaluated and omitted because they did not influence\nthe results. When the Navigation Guide tool was applied\nin the previously completed systematic review, expert\njudgment was applied in determining if including add-\nitional confounders would change the study results,\nwhich was outlined in their protocol [ 23]. It is possible\nthat inclusion of additional subject matter experts on\nour research team would have led to different conclu-\nsions for the confounding domains within these different\ntools. However, we note that the transparency of systematic\nreview ensures that the justification for conclusions is readily\navailable for an independen t reviewer to identify where\nexpert judgment was applied and where one might disagree\nwith this judgment. Additionally, we did not randomize the\norder of the studies or the tools, which may have influenced\nthe average time it took to review the studies using each of\nthe tools, resulting from increasing familiarity with the stud-\nies as the reviewers applied each tool sequentially. However,\nthe average time to increase studies increased across the\ntools as they were applied, thus indicating that the time to\napply the IRIS and TSCA tools would likely be higher if ap-\nplied to a study being evaluated for the first time. Lastly, our\ncase study was based on a relatively small number of studies\n(n = 15) and we may have more confidence in our results if\nour results are consistent upon replication in a larger num-\nber of studies. Additionally, it is possible that the kappa\nvalues observed in our study would have been higher if a lar-\nger number of studies were included.\nConclusions\nSystematic reviews are becoming increasingly important\nin environmental health and our case study finds that\nusing these different tools can lead to opposite conclu-\nsions regarding the body of evidence. Tools that use an\noverall ROB or study quality rating based on weighting\nof domains or scoring metrics that have not been vali-\ndated or empirically demonstrated may lead to errone-\nous conclusions about the quality of the body of\nevidence, as these tools may only consider a subset of\nstudies when drawing conclusions. Further, the exclu-\nsion of studies based off only one “unacceptable” or\n“critically deficient ” criterion can significantly reduce the\navailable evidence to assess the harms of hazardous en-\nvironmental exposures, which could lead to underesti-\nmating the health effects of hazardous chemicals and\nthus inadequate support for regulation of hazardous che-\nmicals. When assessing ROB in systematic reviews, we\nrecommend using tools that use validated, domain based\napproaches which do not exclude studies based off one\nsingle criterion. Rather, tools should consider the\nstrengths and limitations of the entire body of evidence\nwhen formulating conclusions. Examples of these\nmethods include the OHAT or Navigation Guide.\nSupplementary information\nSupplementary information accompanies this paper at https://doi.org/10.\n1186/s13643-020-01490-8.\nAdditional file 1: Table S1. Description of domains measured across\ntools. Figure S1. Summary of risk of bias judgments (low, probably low,\nprobably high, high) using the Navigation Guide framework for the\nhuman studies included in our case series. Risk of bias designations for\nindividual studies and the justification for each study is provided in Lam\net al. Note: ++ indicates low, + indicates probably low, - indicates\nprobably high, -- indicates high. Figure S2. Results from sensitivity\nanalysis of risk of bias judgments (good, adequate, deficient, critically\ndeficient) using the IRIS framework for the human studies included in our\ncase series. The justification for risk of bias designations for individual\nEick et al. Systematic Reviews           (2020) 9:249 Page 11 of 13\nstudies are provided in Tables S2-S16. Note: ++ indicates good, + indi-\ncates adequate, - indicates deficient, -- indicates critically deficient. In-\nstructions for making risk of bias determinations using OHAT framework.\nInstructions for making risk of bias determinations using TSCA framework.\nTable S2. Metric Weighting Factors and Range of Weighted Metric\nScores for Scoring the Quality of Epidemiology Studies. Table S3. Risk of\nbias ratings using the Adgent et al. (2014) study Table S4. Risk of bias\nratings using the Chao et al. (2011) study Table S5. Risk of bias ratings\nusing the Chen et al. (2014) study Table S6. Risk of bias ratings using\nthe Cowell et al. (2015) study Table S7. Risk of bias ratings using the\nEskenazi et al. (2013) study Table S8. Risk of bias ratings using the Gas-\ncon et al. (2012) study Table S9. Risk of bias ratings using the Gascon\net al. (2011) study Table S10. Risk of bias ratings using the Gump et al.\n(2014) study Table S11. Risk of bias ratings using the Herbstman et al.\n(2010) study Table S12. Risk of bias ratings using the Hoffman et al.\n(2012) study Table S13. Risk of bias ratings using the Lin et al. (2010)\nstudy Table S14. Risk of bias ratings using the Roze et al. (2009) study\nTable S15. Risk of bias ratings using the Sagiv et al. (2015) study Table\nS16. Risk of bias ratings using the Shy et al. (2011) study Table S17. Risk\nof bias ratings using the Zhang et al. (2017) study\nAbbreviations\nOHAT: Office of Health Assessment and Translation; IRIS: Integrated Risk\nInformation System; TSCA: Toxic Substances Control Act; NTP: National\nToxicology Program; NAS: National Academy of Sciences; UCSF: University of\nCalifornia, San Francisco; EPA: Environmental Protection Agency; ROB: Risk of\nbias; PBDE: Polybrominated diphenyl ethers; IQ: Intelligence quotient;\nADHD: Attention deficit hyperactivity disorder\nAcknowledgements\nThe authors would like to acknowledge Veena Singla, PhD for study\nconceptualization.\nAuthors’ contributions\nSME was responsible for completing the risk of bias assessments,\ninterpretation of results, writing-original draft, writing-reviewing, and editing.\nDEG was responsible for completing the risk of bias assessments, interpret-\nation of results, writing-reviewing, and editing. NC was responsible for inter-\npretation of results, writing-original draft, writing-reviewing, and editing. JL\nwas responsible for study conceptualization, interpretation of results, writing-\nreviewing, and editing. TJW was responsible for funding acquisition, study\nconceptualization, interpretation of results, writing-reviewing, and editing.\nThe author(s) read and approved the final manuscript.\nFunding\nFunding for this work was provided by the JPB Foundation (grant 681), the\nPassport Foundation, and the Clarence E. Heller Charitable Foundation. This\nwork was also suported by grants P01ES022841 and R01ES027051 from the\nNational Institute of Environmental Health Sciences, and UG3OD023272 and\nUH3OD023272 from the National Institutes of Health Environmental\ninfluences on Child Health Outcomes (ECHO) program.\nAvailability of data and materials\nAll data generated or analyzed during this study are included in this\npublished article and its supplementary information files.\nEthics approval and consent to participate\nNot applicable\nConsent for publication\nNot applicable\nCompeting interests\nThe authors declare that they have no competing interests.\nAuthor details\n1Program on Reproductive Health and the Environment, Department of\nObstetrics, Gynecology and Reproductive Sciences, University of California,\nSan Francisco, USA. 2Department of Health Sciences, California State\nUniversity, East Bay, Hayward, CA, USA.\nReceived: 27 May 2020 Accepted: 21 September 2020\nReferences\n1. National Research Council. Review of EPA ’s integrated risk information\nsystem (IRIS) process. Washington, DC: The National Academies Press; 2014.\n2. Rooney AA, Boyles AL, Wolfe MS, Bucher JR, Thayer KA. Systematic review\nand evidence integration for literature-based environmental health science\nassessments. Environmental health perspectives. 2014;122(7):711 – 8.\n3. National Academies of Sciences E, Medicine, Division on E, Life S, Board on\nEnvironmental S, Toxicology, et al. Application of systematic review\nmethods in an overall strategy for evaluating low-dose toxicity from\nendocrine active chemicals. Washington (DC): National Academies Press\n(US) Copyright 2017 by the National Academy of Sciences. All rights\nreserved.; 2017.\n4. The National Academies of Sciences Engineering Medicine. Progress toward\ntransforming the integrated risk information system (IRIS) program: a 2018\nevaluation. Washington, DC; 2018.\n5. Woodruff TJ, Sutton P. The Navigation Guide systematic review\nmethodology: a rigorous and transparent method for translating\nenvironmental health science into better health outcomes. Environ Health\nPerspect. 2014;122(10):1007 – 14.\n6. Higgins JPT, S G. Cochrane Handbook for Systematic Reviews of\nInterventions. Version 5.1.0 2011. Available from: http://www.cochrane-\nhandbook.org.\n7. Office of Health Assessment and Translation (OHAT). Handbook for\nconducting a literature-based health assessment using OHAT approach for\nsystematic review and evidence integration: National Institute of\nEnvironmental Health Sciences; 2019 [Available from: https://ntp.niehs.nih.\ngov/ntp/ohat/pubs/handbookmarch2019_508.pdf.\n8. U.S. Environmental Protection Agency. Application of systematic review in\nTSCA risk evaluations 2018 [Available from: https://www.epa.gov/assessing-\nand-managing-chemicals-under-tsca/application-systematic-review-tsca-risk-\nevaluations.\n9. Higgins JPT, Altman DG, Gøtzsche PC, Jüni P, Moher D, Oxman AD, et al.\nThe Cochrane Collaboration ’s tool for assessing risk of bias in randomised\ntrials. BMJ. 2011;343:d5928-d.\n10. Wang Z, Taylor K, Allman-Farinelli M, Armstrong B, Askie L, Ghersi D, et al. A\nsystematic review: tools for assessing methodological quality of human\nobservational studies 2019.\n11. Rooney AA, Cooper GS, Jahnke GD, Lam J, Morgan RL, Boyles AL, et al. How\ncredible are the study results? Evaluating and applying internal validity tools\nto literature-based assessments of environmental health hazards.\nEnvironment international. 2016;92-93:617 – 29.\n12. Page MJ, Higgins JP, Clayton G, Sterne JA, Hróbjartsson A, Savovi ć J.\nEmpirical evidence of study design biases in randomized trials:\nsystematic review of meta-epidemiological studies. PLoS One. 2016;\n11(7):e0159267.\n13. Rooney AA, Cooper GS, Jahnke GD, Lam J, Morgan RL, Boyles AL, et al. How\ncredible are the study results? Evaluating and applying internal validity tools\nto literature-based assessments of environmental health hazards. Environ\nInt. 2016;92-93:617 – 29.\n14. Bero L, Chartres N, Diong J, Fabbri A, Ghersi D, Lam J, et al. The risk of bias\nin observational studies of exposures (ROBINS-E) tool: concerns arising from\napplication to observational studies of exposures. Syst Rev. 2018;7(1):242.\n15. Hootman JM, Driban JB, Sitler MR, Harris KP, Cattano NM. Reliability and\nvalidity of three quality rating instruments for systematic reviews of\nobservational studies. Research Synthesis Methods. 2011;2(2):110 – 8.\n16. Losilla JM, Oliveras I, Marin-Garcia JA, Vives J. Three risk of bias tools lead to\nopposite conclusions in observational research synthesis. Journal of clinical\nepidemiology. 2018;101:61 – 72.\n1 7 . O ' C o n n o rS R ,T u l l yM A ,R y a nB ,B r a d l e yJ M ,B a x t e rG D ,M c D o n o u g hS M .\nFailure of a numerical quality assessment scale to identify potential risk\nof bias in a systematic review: a comparison study. BMC research notes.\n2015;8:224.\n18. Herbison P, Hay-Smith J, Gillespie WJ. Adjustment of meta-analyses on the\nbasis of quality scores should be abandoned. J Clin Epidemiol. 2006;59(12):\n1249– 56.\n19. Whiting P, Rutjes AW, Dinnes J, Reitsma JB, Bossuyt PM, Kleijnen J. A\nsystematic review finds that diagnostic reviews fail to incorporate quality\ndespite available tools. J Clin Epidemiol. 2005;58(1):1 – 12.\nEick et al. Systematic Reviews           (2020) 9:249 Page 12 of 13\n20. Jüni P, Altman DG, Egger M. Assessing the quality of controlled clinical\ntrials. BMJ. 2001;323(7303):42 – 6.\n21. National Academies of Science, Engineering, and Medicine. Review of DOD ’s\napproach to deriving an occupational exposure level for trichloroethylene.\nWashington, DC: The National Academies Press; 2019. p. 76.\n22. National Research Council. Review of EPA ’s integrated risk information system\n(IRIS) process. Washington, DC: The National Academies Press; 2014. p. 170.\n23. Lam J, Lanphear BP, Bellinger D, Axelrad DA, McPartland J, Sutton P, et al.\nDevelopmental PBDE exposure and IQ/ADHD in childhood: a systematic\nreview and meta-analysis. Environ Health Perspect. 2017;125(8):086001.\n24. Johnson PI, Koustas E, Vesterinen HM, Sutton P, Atchley DS, Kim AN, et al.\nApplication of the Navigation Guide systematic review methodology to the\nevidence for developmental and reproductive toxicity of triclosan. Environ\nInt. 2016;92-93:716 – 28.\n25. Johnson PI, Sutton P, Atchley DS, Koustas E, Lam J, Sen S, et al. The\nNavigation Guide - evidence-based medicine meets environmental health:\nsystematic review of human evidence for PFOA effects on fetal growth.\nEnviron Health Perspect. 2014;122(10):1028 – 39.\n26. Lam J, Sutton P, Kalkbrenner A, Windham G, Halladay A, Koustas E, et al. A\nsystematic review and meta-analysis of multiple airborne pollutants and\nautism spectrum disorder. PLoS One. 2016;11(9):e0161851.\n27. Vesterinen HM, Johnson PI, Atchley DS, Sutton P, Lam J, Zlatnik MG, et al.\nFetal growth and maternal glomerular filtration rate: a systematic review. J\nMatern Fetal Neonatal Med. 2015;28(18):2176 – 81.\n28. Vesterinen HM, Morello-Frosch R, Sen S, Zeise L, Woodruff TJ. Cumulative\neffects of prenatal-exposure to exogenous chemicals and psychosocial\nstress on fetal growth: systematic-review of the human and animal\nevidence. PLOS ONE. 2017;12(7):e0176331.\n29. Committee to Review the IRIS Process; Board on Environmental Studies and\nToxicology; Division on Earth and Life Studies; National Research Council.\nReview of EPA ’s integrated risk information system (IRIS) process. 2014.\nAvailable from: https://www.ncbi.nlm.nih.gov/books/NBK230060/.\n30. Sterne JA, Hernán MA, Reeves BC, Savovi ć J, Berkman ND, Viswanathan M,\net al. ROBINS-I: a tool for assessing risk of bias in non-randomised studies of\ninterventions. BMJ. 2016;355:i4919.\n31. Adgent MA, Hoffman K, Goldman BD, Sjödin A, Daniels JL. Brominated\nflame retardants in breast milk and behavioural and cognitive development\nat 36 months. Paediatric Perinatal Epidemiolgy. 2014;28(1):48 – 57.\n32. Gump BB, Yun S, Kannan K. Polybrominated diphenyl ether (PBDE) exposure\nin children: possible associations with cardiovascular and psychological\nfunctions. Environmental Research. 2014;132:244 – 50.\n33. Chao H-R, Tsou T-C, Huang H-L, Chang-Chien G-P. Levels of breast milk\nPBDEs from southern Taiwan and their potential impact on\nneurodevelopment. Pediatric Research. 2011;70(6):596 – 600.\n34. Lin D-Y, Chao H-R, Gou Y-Y, Huang C-Y. Infants ingesting high breast milk\nlevels of polybrominated diphenyl ethers may have negative impact on\ntheir neurodevelopment. 2010 International Conference on Chemistry and\nChemical. Engineering. 2010:325 – 7.\n35. Shy C-G, Huang H-L, Chang-Chien G-P, Chao H-R, Tsou T-C. Neurodevelopment of\ninfants with prenatal exposure to polybrominated diphenyl ethers. Bulletin of\nEnvironmental Contaminationand Toxicology. 2011;87(6):643– 8.\n36. Roze E, Meijer L, Bakker A, Van Braeckel KNJA, Sauer PJJ, Bos AF. Prenatal\nexposure to organohalogens, including brominated flame retardants,\ninfluences motor, cognitive, and behavioral performance at school age.\nEnviron Health Perspect. 2009;117(12):1953 – 8.\n37. Hoffman K, Adgent M, Goldman BD, Sjödin A, Daniels JL. Lactational exposure\nto polybrominated diphenyl ethers and its relation to social and emotional\ndevelopment among toddlers. Environ Health Perspect. 2012;120(10):1438 – 42.\n38. Cowell WJ, Lederman SA, Sjödin A, Jones R, Wang S, Perera FP, et al.\nPrenatal exposure to polybrominated diphenyl ethers and child attention\nproblems at 3 – 7 years. Neurotoxicology and Teratology. 2015;52:143 – 50.\n39. Wang Z, Taylor K, Allman-Farinelli M, Armstrong B, Askie L, Ghersi D, et al. A\nsystematic review: tools for assessing methodological quality of human\nobservational studies. MetaArXiv. 2019.\n40. Hoenig JM, Heisey DM. The abuse of power. The American Statistician.\n2001;55(1):19– 24.\n41. Higgins JPT, Green S, Cochrane C. Cochrane handbook for systematic\nreviews of interventions. Chichester, England; Hoboken, NJ: Wiley-\nBlackwell; 2008.\n42. Juni P, Witschi A, Bloch R, Egger M. The hazards of scoring the quality of\nclinical trials for meta-analysis. Jama. 1999;282(11):1054 – 60.\n43. Ntp. Handbook for conducting a literature-based health assessment using\nOHAT approach for systematic review and evidence integration. U.S. Dept.\nof Health and Human Services, National Toxicology Program; 2015.\n44. Vandenbroucke JP, von Elm E, Altman DG, Gøtzsche PC, Mulrow CD, Pocock SJ,\net al. Strengthening the Reporting of Observational Studies in Epidemiology\n(STROBE): explanation and elaboration. Int J Surg. 2014;12(12):1500– 24.\n45. Losilla J-M, Oliveras I, Marin-Garcia JA, Vives J. Three risk of bias tools lead to\nopposite conclusions in observational research synthesis. J Clin Epidemiol.\n2018;101:61– 72.\nPublisher’sN o t e\nSpringer Nature remains neutral with regard to jurisdictional claims in\npublished maps and institutional affiliations.\nEick et al. Systematic Reviews           (2020) 9:249 Page 13 of 13",
  "topic": "Medicine",
  "concepts": [
    {
      "name": "Medicine",
      "score": 0.9384000301361084
    },
    {
      "name": "Epidemiology",
      "score": 0.674439013004303
    },
    {
      "name": "Environmental health",
      "score": 0.5409733653068542
    },
    {
      "name": "Environmental epidemiology",
      "score": 0.4262973368167877
    },
    {
      "name": "Risk analysis (engineering)",
      "score": 0.333631694316864
    },
    {
      "name": "Pathology",
      "score": 0.16065874695777893
    }
  ]
}