{
    "title": "Hi-ToM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models",
    "url": "https://openalex.org/W4389520279",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2112421103",
            "name": "Yufan Wu",
            "affiliations": [
                "Westlake Health Center",
                "University of Michigan–Ann Arbor"
            ]
        },
        {
            "id": "https://openalex.org/A2120861514",
            "name": "Yinghui He",
            "affiliations": [
                "University of Michigan–Ann Arbor",
                "Westlake Health Center"
            ]
        },
        {
            "id": "https://openalex.org/A2144146505",
            "name": "Yilin Jia",
            "affiliations": [
                "University of Michigan–Ann Arbor",
                "Westlake Health Center"
            ]
        },
        {
            "id": "https://openalex.org/A2068190112",
            "name": "Rada Mihalcea",
            "affiliations": [
                "Westlake Health Center",
                "University of Michigan–Ann Arbor"
            ]
        },
        {
            "id": "https://openalex.org/A2121903534",
            "name": "Yulong Chen",
            "affiliations": [
                "Westlake Health Center",
                "University of Michigan–Ann Arbor"
            ]
        },
        {
            "id": "https://openalex.org/A3204750842",
            "name": "Naihao Deng",
            "affiliations": [
                "University of Michigan–Ann Arbor",
                "Westlake Health Center"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2889107415",
        "https://openalex.org/W2573084000",
        "https://openalex.org/W4380356267",
        "https://openalex.org/W4327526719",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4361806395",
        "https://openalex.org/W2970536767",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W2093410327",
        "https://openalex.org/W2063294759",
        "https://openalex.org/W1990680837",
        "https://openalex.org/W2036621537",
        "https://openalex.org/W1849645106",
        "https://openalex.org/W2141538250",
        "https://openalex.org/W4378509449",
        "https://openalex.org/W4232488397",
        "https://openalex.org/W4389519585",
        "https://openalex.org/W4387835442",
        "https://openalex.org/W2123947560",
        "https://openalex.org/W2042801896",
        "https://openalex.org/W4321277158",
        "https://openalex.org/W4206078757",
        "https://openalex.org/W4389519998",
        "https://openalex.org/W3182396019",
        "https://openalex.org/W4300402905",
        "https://openalex.org/W4360836968",
        "https://openalex.org/W4385572854",
        "https://openalex.org/W4285107714",
        "https://openalex.org/W3171668871",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2078451758",
        "https://openalex.org/W4241489251",
        "https://openalex.org/W4229038594",
        "https://openalex.org/W3127593076",
        "https://openalex.org/W4378770815"
    ],
    "abstract": "Theory of Mind (ToM) is the ability to reason about one’s own and others’ mental states. ToM plays a critical role in the development of intelligence, language understanding, and cognitive processes. While previous work has primarily focused on first and second-order ToM, we explore higher-order ToM, which involves recursive reasoning on others’ beliefs. %We also incorporate a new deception mechanism in ToM reasoning. We introduce Hi-ToM, a Higher Order Theory of Mind benchmark. Our experimental evaluation using various Large Language Models (LLMs) indicates a decline in performance on higher-order ToM tasks, demonstrating the limitations of current LLMs. We conduct a thorough analysis of different failure cases of LLMs, and share our thoughts on the implications of our findings on the future of NLP.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10691–10706\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nHI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind\nReasoning in Large Language Models\nYinghui He∗\n , Yufan Wu∗\n , Yilin Jia\n , Rada Mihalcea\n , Yulong Chen\n , Naihao Deng †\nUniversity of Michigan\n Westlake University\n{huihui, umwyf, kripf, mihalcea, dnaihao}@umich.edu\nAbstract\nTheory of Mind (ToM) is the ability to rea-\nson about one’s own and others’ mental states.\nToM plays a critical role in the development of\nintelligence, language understanding, and cog-\nnitive processes. While previous work has pri-\nmarily focused on ﬁrst and second-order ToM,\nwe explore higher-order ToM, which involves\nrecursive reasoning on others’ beliefs. We in-\ntroduce HI-TOM, a Higher Order Theory of\nMind benchmark. Our experimental evaluation\nusing various Large Language Models (LLMs)\nindicates a decline in performance on higher-\norder ToM tasks, demonstrating the limitations\nof current LLMs. We conduct a thorough analy-\nsis of different failure cases of LLMs, and share\nour thoughts on the implications of our ﬁndings\non the future of NLP.\n1 Introduction\nTheory of Mind (ToM) refers to the ability to un-\nderstand and reason about the mental states of\nothers such as intentions and beliefs, and also to\ndistinguish them from one’s own ( Premack and\nWoodruff, 1978). Such an ability has been con-\nsidered a crucial point in the development of intel-\nligence functions ( Premack and Woodruff , 1978;\nBretherton and Beeghly , 1982; Frith and Frith ,\n2003), and previous research has demonstrated that\nToM reasoning is highly related to linguistic and\ncognitive processes (Perner, 1991; Sperber and Wil-\nson, 2002). ToM has thus been widely used as a pro-\ntocol to evaluate the language understanding and\nreasoning ability of intelligence agents ( Premack\nand Woodruff, 1978; Takano et al. , 2006), such as\nyoung children ( Osterhaus and Koerber , 2021).\nWith the recent advance in large language mod-\nels (LLMs), research has been undertaken to eval-\nuate the language skills of LLMs using ToM ( Sap\net al., 2022; Ullman, 2023). Most of the previous\n*Contributed equally to this work.\n†Corresponding author of this work.\nFigure 1: A scene shot from the TV series Friends that\nexhibits fourth-order Theory of Mind (ToM).\nwork has been conﬁned to ﬁrst-order and second-\norder ToM, where LLMs are asked to perform in-\nference on others’ belief of reality in one or two\npasses, e.g., the ﬁrst and second-order questions in\nFigure 2 (see Section 2 for a more comprehensive\ndiscussion of ToM background and the evaluation\nof ToM in LLMs).\nHigher-order ToM, referring to third-order rea-\nsoning and beyond, requires recursive reasoning on\nothers’ beliefs in multiple passes. Figure 1 shows\na higher-order ToM example from the TV series\nFriends. In Figure 1, a character says “ They don’t\nknow that we know they know we know ” when she\nand the other character try to recursively identify\nthe situation. Such an example underscores that\nhuman beings are capable of higher-order ToM in\ndaily interactions. In addition, evidence shows that\nhigher-order ToM is not only essential to commu-\nnicate effectively in complicated scenarios, such as\nmulti-party conversations ( Liddle and Nettle , 2006;\nDe Weerd et al. , 2015; Ridinger and McBride ,\n2017; De Weerd et al. , 2022), but it also enables\nbetter emotional support and empathetic commu-\nnication ( Mitchell and Phillips , 2015). However,\nbecause of a lack of higher-order ToM datasets in\nthe NLP community, there is signiﬁcantly less re-\nsearch on higher-order ToM compared to the lower\norders.\nPrevious work has mainly constructed ToM\n10691\nSallyAnneThe milk is on the tableSally exited the room\nAnne transferred the milk onto the box\nAlex\nAlex exited the room, then Anne exited the room\n(Alex lied to all) Themilk is in the fridge! (Sally secretly told Anne)Themilk is on the table!\nMilk\nTable\nBox1 2\n3 4\n5Outside the room, the three interacted with each other\nAnneAlex\nAnneAlex\nSallyAnneAlex\nWhere is the milk?Where does Anne think the milk is?Where does Sally think Anne thinks the milk is?Where does Alex think Sally thinks Anne thinks the milk is?\n0th1st2nd3rd\nFigure 2: A sample from H I-TOM dataset, which con-\ntains communications among agents, and questions that\naddress 0-th (reality) to 3-rd ToM reasoning..\nbenchmarks using automatic story generation\nscripts. Although simple and inexpensive, this\nmethod cannot be directly extended to generating\nstories of higher-order ToMs because the generated\nstories contain insufﬁcient information for raising a\nhigher-order question. In this paper, we build upon\nprevious work and introduce HI-TOM, a multiple-\nchoice question benchmark consisting of Sally-\nAnne-like stories (Figure 2), speciﬁcally designed\nfor higher-order ToM evaluation. Unlike previous\ndatasets, H I-TOM contains questions from zeroth-\norder to fourth-order ToM, and incorporates agent\ncommunications in the stories. We manually check\nthe quality of the constructed data, and empirically\nﬁnd that H I-TOM presents greater diversity and\nchallenges compared to previous datasets.\nWe experiment with various LLMs, including\nGPT-4 (OpenAI, 2023), GPT-3.5-turbo ( OpenAI,\n2022), Claude, and Guanaco ( Dettmers et al. ,\n2023), on H I-TOM under a zero-shot setting. Fur-\nthermore, we test the chain-of-thought prompt-\ning ( Wei et al. , 2022) and conduct a thorough\nanalysis of LLMs’ performances on different story\ntypes in H I-TOM and their failure cases. Our work\ndemonstrates that the claim of LLMs having gen-\nuine ToM abilities ( Kosinski, 2023; Bubeck et al. ,\n2023) is questionable, especially in the cases of\nhigher-order ToM, where several rounds of recur-\nsive reasoning are required. To our knowledge, we\nare the ﬁrst to introduce a benchmark for evaluat-\ning higher-order ToM reasoning and analyzing the\nabilities of current LLMs on high-order ToM. Fur-\nthermore, we share our thoughts on the future of\nNLP and the way forward with LLMs of enhancing\nLLMs from the perspective of human intelligence,\nunderstanding humans through the lens of LLMs,\nand enhancing LLMs’ ToM abilities for better NLP\napplications. We release our dataset and code\nat https://github.com/ying-hui-he/Hi-ToM_\ndataset.\n2 Background and Related Work\nTheory of Mind. Most of prior work focuses on\nﬁrst or second-order ToM ( Nematzadeh et al. , 2018;\nLe et al. , 2019; Sap et al. , 2022), while higher-\norder ToM (third-order and beyond) remains under-\nexplored. The concept of “ orders” refers to the\nnumber of mental state attributions that are required\nto answer a particular question or reason about a\nparticular scenario. For instance, a third-order ToM\nquestion can be “ Where does Anne think that Sally\nthinks that Isabella searches for the milk? ”, where\nSally’s reasoning about Isabella is of second-order,\nand Anna’s reasoning on Sally’s reasoning is of\nthird-order.\nHigher-order ToM is useful in social interac-\ntion such as maintaining social networks ( Lid-\ndle and Nettle , 2006), winning limited bidding\n(de Weerd and Verheij , 2011), efﬁciently coopera-\ntion (De Weerd et al. , 2015; Ridinger and McBride ,\n2017), and unpredictable negotiations ( De Weerd\net al. , 2022). Researchers from cognitive science\ninvestigate second-order and higher-order ToM\namong young children via complex forms of false-\nbelief tests, such as the Sally-Anne false-belief\nexperiment (Baron-Cohen et al. , 1985).\nEvaluating ToM in LLMs. Sap et al. (2022) ﬁnd\nthat GPT-3’s ToM ability is well below humans on\nthe T OMI dataset (Le et al. , 2019), which is a ToM\nevaluation dataset consisting of questions up to the\nsecond order. Kosinski (2023); Bubeck et al. (2023)\nshow the promising performance of recent LLMs\nsuch as GPT-3.5 and GPT-4 on ToM tasks. How-\never, it is questionable whether LLMs have genuine\nToM ability, especially for higher-order ToM. Ull-\nman (2023) ﬁnd that for GPT-3.5, small variations\nthat maintain the principles of ToM can cause a\nﬂip of the answer. Different from previous work\nthat only evaluates LLMs’ ToM ability up to the\nsecond order, we take a step forward and evaluate\nLLMs’ ability in higher-order ToM settings. Also,\n10692\nComponent Num. Example\nRoom 30 kitchen\n , bedroom\nObject 37 lemon\n , peach\nContainer 39 red_envelope\n , blue_bottle\nAgent 40 Jack\n , Ella\n , Noah\nTable 1: Basic components, numbers of choices for each\ncomponent (Num.), and their examples in H I-TOM sto-\nries.\nwe are the ﬁrst one pioneering in adding the decep-\ntive communication protocol in ToM setups, which\ntakes an initial step toward evaluating LLMs’ abil-\nity in real-world scenarios. Concurrent to our work,\nMa et al. (2023) surveyed the existing ToM bench-\nmarks and conducted preliminary experiments on\nsituated evaluation of ToM for LLMs.\n3 The H I-TOM Dataset\nTo systematically examine how effectively LLMs\nreason Theory of Mind (ToM) at different orders,\neach story is coupled with ﬁve questions that re-\nquire the zeroth to fourth level of ToM reasoning,\nrespectively. Following Nematzadeh et al. (2018)\nand Le et al. (2019), we automatically generate H I-\nTOM stories. Additionally, we manually review\nthe generated stories, questions, and answers to\nensure that they are consistent with each other, and\nthey are logically correct.\n3.1 Dataset Design\nStory Design. HI-TOM stories consist of four\nfundamental elements: rooms, objects, containers,\nand agents, as shown in Table 1. A story narrates\nevents occurring in one or more rooms, where mul-\ntiple objects are placed inside their respective con-\ntainers. Each story features ﬁve rational agents.\nEach story comprises one to three chapters. Each\nchapter corresponds to a single round in the object-\nﬁnding game. In each chapter, we design multi-\nple actions and optional communication protocols\namong agents:\n• Entry: At least one agent enters one room, where\nthey observe all the objects, other agents, and\ntheir actions in that room (e.g., Figure 2 Scene\n1).\n• Object Movement: When in a room, each agent\ncan choose whether to move an object before\ntheir exit. Such actions are done in a sequential\nmanner. In other words, the later agent can only\nperform such an action after the former agent\nHI-TOM One-Chapter Story\n1 Emma, Charlotte, Benjamin, Aiden and Isabella en-\ntered the workshop.\n2 The pear is in the red_treasure_chest.\n3 Emma moved the pear to the blue_suitcase.\n4 Emma exited the workshop.\n5 Charlotte exited the workshop.\n6 Benjamin lost his watch.\n7 Benjamin exited the workshop.\n8 Aiden moved the pear to the blue_crate.\n9 Aiden exited the workshop.\n10 Isabella moved the pear to the red_treasure_chest.\n11 Isabella likes the red_box.\n12 Isabella exited the workshop.\n13 Aiden publicly claimed that the pear is in the\nblue_drawer now.\n14 Emma privately told Isabella that the radish is in the\nred_suitcase now.\nTable 2: An example H I-TOM one-chapter story with\nagent communications. Random distractors are inserted\nin lines 6 and 11, where the latter introduces “red_box”\nas a distractive answer choice.\nmoves the object (or not) and leaves the room\n(Scenes 2, 3, and 4).\n• Agent communication: Outside the room,\nagents may be involved in two types of com-\nmunications: public, where an agent shares infor-\nmation with every agent, and private, where an\nagent only speaks to another agent privately, or\nthey can remain silent (Scene 5).\nFor agent communications, we set the shared\ninformation to be deceptive in order to emulate the\ndynamics of the complicated social life. It also adds\nanother layer of complexity to the ToM reasoning\nprocess, which requires the answerers to not only\nreason about an agent’s perceptions of other agents\nknowledge of the objects location, but also reason\nabout whether an agent would trust another agent.\nIn this way, we evolved the simplistic toy stories in\nthe previous dataset ( Le et al. , 2019), and solved a\ncore problem in the previous evaluations that the\nanswers may be simply found from the objects\noriginal or ﬁnal location.\nMoreover, we pose a constraint that the listener\nwould update their world knowledge based on the\ninformation given by the speaker if the speaker ex-\nits the room later than the listener. This is based on\nthe assumption that the listener would be unaware\nof any changes after their exit, but the speaker\nmight possess more up-to-date knowledge as they\nleave later. Additionally, we assume that Alex and\nSally, who give out information publicly or pri-\nvately, will believe that all the listeners trust their\n10693\nrespective information. A full assumption list is\nattached to each story, as shown in Table 7 in Ap-\npendix B.1.\nEach chapter involves entry and object move-\nment, while agent communication is optional. In\nHI-TOM, half of the stories have at least one chap-\nter with agent communications, while the other half\nonly contains chapters without communications.\nQuestion-Answer Design. Following Le et al.\n(2019), for each story, we provide ﬁve questions\nthat are progressively built from lower-order ques-\ntions to higher ones as shown in Table 3.\nOrder Question\n0th Where is O really?\n1st Where does A1 think O is?\n2nd Where does A2 think A1 thinks O is?\n3rd Where does A3 think A2 thinks A1 thinks O is?\n4th Where does A4 think A3 thinks A2 thinks A1\nthinks O is?\nTable 3: Questions asked in a story involving object O\nand ﬁve agents. A1 to A4 are randomly chosen from\nthe ﬁve agents. 0th, 1st, 2nd, 3rd, and 4th represent the\nToM orders of the questions.\nFollowing Sap et al. (2022), we adopt the\nmultiple-choice setting and provide the correct an-\nswer along with several distractor choices.\n3.2 Data Generation\nTo generate the aforementioned ToM stories with\nhigher-order questions among agents, we adapt the\ngeneration scripts from Nematzadeh et al. (2018),\nwhich are originally limited to ﬁrst or second-order\nToM stories.\nOur script takes a list of story components\nRooms, Objects, Containers, Agents, as well\nas the number of chapters ℓ as inputs, and outputs\na story with ℓ chapters along with ﬁve questions\nfrom zeroth to fourth order ToM.\nFor the generation of each chapter, we randomly\nchoose the story components and ﬁt them into the\nchapter template. Speciﬁcally, we randomly deter-\nmine whether or not each agent moves the object\nto another container. Then, we incorporate agent\ncommunications in certain chapters, where we use\nthe phrases “publicly claim” and “privately tell” to\nencode public and private communications.\nTo generate the questions and answers for each\nstory, we integrate the relevant story components\ninto a predeﬁned question template. Subsequently,\nwe utilize an answer generator to track the actions\nDatasets ToM/ToM-easy TOMI HI-TOM\n1st\n2nd\n3rd\n4th\nComm.\n#Line 15.05 8.86 26.47\n#Agent 3.22 2.75 5\n#Container 5 2 7.39\nTable 4: Comparison between H I-TOM and other\ndatasets. 1st, 2nd, 3rd, and 4th refer to whether a dataset\ncontains story-question pairs of a speciﬁc ToM order.\nComm. stands for the existence of agent communica-\ntions. #Line, #Agent, and #Container represent average\nnumber per story.\nof all agents and derive the correct answer to each\nquestion. Further details and pseudocode related\nto our story generation process can be found in\nAppendix B.2.\nAdditionally, based on Le et al. (2019), we fur-\nther incorporate distractor sentences that relate an\nagent with a random container, such as “Jack likes\nthe red_container”. This reduces the regularity and\npredictability of the stories. Table 2 shows an exam-\nple one-chapter story with agent communications\nand random distractors.\n3.3 Dataset Characteristics\nTable 4 shows a comparison between H I-TOM and\nthe other ToM datasets. First, unlike previous\ndatasets, H I-TOM is the only benchmark that con-\ntains third and fourth-order stories, which suggests\nthat H I-TOM is more challenging and requires\nhigher-order ToM reasoning. Also, we ﬁrst intro-\nduce communications among agents, which poses\ngreater challenges to LLMs to reason about human\ninteractions. In addition, stories in H I-TOM are\nsigniﬁcantly longer, with a larger number of agents\nand containers per story. This requires the LLMs’\ncapability to comprehend the complete storyline\nand reason about each agent’s beliefs.\nNotably, HI-TOM features a larger pool of poten-\ntial answers and a balanced distribution of correct\nanswers throughout the story. In ToM/ToM-easy\nand T OMI, all the correct answers appear within\nthe last two containers or the only two containers\nin the corresponding story. In contrast, the pro-\nportions of correct answers appearing in the ﬁrst,\nsecond, third, and ﬁnal quarters of the H I-TOM sto-\nries are 28.7%, 27.2%, 18.8%, and 25.3%, respec-\ntively. The even distribution of correct answers\n10694\neliminates the position bias of correct answers be-\ning concentrated in speciﬁc segments of the stories\nin H I-TOM.\n4 Experimental Setup\n4.1 Models\nWe evaluate the following four LLMs on H I-TOM:\n1. GPT-3.5-Turbo (OpenAI, 2022) and GPT-\n4 (OpenAI, 2023) are closed-sourced models\nfrom OpenAI. We use gpt-4-32k and gpt-\n3.5-turbo for experiments, which are con-\nducted on June 14th ∼15th 2023.\n2. Claude-instant is a close-sourced model pub-\nlished by Anthropic †.\n3. Guanaco (65B) is an open-sourced model ﬁne-\ntuned from LLaMA ( Touvron et al. , 2023).\nWe adhere to the default parameter conﬁgura-\ntions across all the examined language models.\n4.2 Methods\nFor each H I-TOM story, we conduct trials using\ntwo prompting styles: Vanilla Prompting (VP)\nand Chain-of-Thought Prompting (CoTP). In VP\nprompting, the model needs to pick the best answer\nfrom a given set of options without explanation.\nCoTP prompting requires the model to offer a step-\nby-step explanation of its thought process along\nwith the answer. Appendix C.1 provides an exam-\nple (Table 8) and more details of our prompting\nmethods.\n4.3 Evaluation\nWe evaluate the model performance using both\nstandard accuracy (hereafter referred to as accu-\nracy) and joint accuracy. Adapted from ( Le et al. ,\n2019), joint accuracy represents a more stringent\nmetric than standard accuracy . It considers an\nanswer as correct only when the related question,\nalong with all preceding, lower-order questions\nwithin the same story are answered correctly. For\ninstance, the third-order question in Table 3 is con-\nsidered correct only if the model correctly answers\nthe zeroth, ﬁrst, second, and third-order questions\nabove it. Joint accuracy effectively reveals the\nmodel’s genuine ability in higher-order ToM rea-\nsoning, as the model may only reason the higher-\norder ToM correctly if it is able to reason the lower-\norder ToM because the higher-order question is a\n†www.anthropic.com/index/introducing-claude\nModel Accuracy (%)\n& Methods w/o w/ Overalldec. dec.\nGuanaco VP 33.33 33.33 33.33 32.17\n65B CoTP 35.00 26.99 30.99\n+1.67 -6.34 -2.34\nClaude VP 49.33 42.00 45.67 46.00\n-instant CoTP 52.33 40.33 46.33\n+3.00 -1.67 +0.66\nGPT-3.5 VP 28.67 26.33 27.50 31.50\n-turbo CoTP 35.67 35.33 35.50\n+7.00 +9.00 +8.00\nGPT-4 VP 60.42 55.81 58.11 58.99\n-32k CoTP 64.04 55.72 59.88\n+3.60 -0.09 +1.77\nTable 5: Standard accuracy results of the four tested\nmodels on H I-TOM stories. “w/o dec.” and “w/ dec.”\nindicate accuracy in stories with and without deception,\nrespectively. The performance increase and decrease\nfrom VP to CoTP prompting style are highlighted.\nrecursive successor of the lower-order ones for the\nsame story.\n5 Experimental Results\nTable 5 presents the accuracy scores of the four\nLLMs. All the LLMs we evaluate exhibit less\nthan 60% accuracy scores, demonstrating that H I-\nTOM is challenging even for the most sophisticated\nLLMs. Figure 3 depicts the joint accuracy scores of\nGPT-4 and GPT-3.5 under various settings. As the\nstory length decreases or the ToM order increases,\nLLMs’ performance decreases across various set-\ntings. In addition, LLMs perform worse when there\nare deceptive agent communications involved in the\nstory. The trend observed in Guanaco and Claude\naligns with that of GPT-4 and GPT-3.5, as shown\nin Appendix C.2.\nThe experimental results also reveal the follow-\ning noteworthy patterns:\nCoTP prompting yields insigniﬁcant perfor-\nmance gains. We observe no substantial improve-\nment in accuracy when transitioning from VP to\nCoTP in 5. Furthermore, in the assessments involv-\ning stories with deception, the switch in prompting\nmethods even leads to a decrease in accuracy. We\nhypothesize that as there are more steps involved,\nthere are higher chances of deceptive information\nmisleading steps in between. The chain may then\namplify the error in that step, leading to a cascade\nof errors throughout the reasoning process.\n10695\n0 1 2 3 4\n1\n2\n3\nGPT-4\n1 0.84 0.7 0.45 0.35\n0.95 0.79 0.5 0.2 0.1\n0.95 0.55 0.2 0.16 0.15\nCoTP\n0 1 2 3 4\n1\n2\n3\n1 0.7 0.5 0.45 0.4\n0.95 0.75 0.35 0.1 0.05\n0.93 0.58 0.2 0.15 0.15\nVP\n0 1 2 3 4\n1\n2\n3\n1 0.65 0.3 0.2 0.15\n1 0.55 0.2 0.1 0.1\n1 0.45 0.3 0.1 0.05\n, CoTP\n0 1 2 3 4\n1\n2\n3\n1 0.6 0.2 0.1 0.1\n1 0.6 0.35 0.15 0.15\n0.95 0.4 0.21 0.05 0.05\n, VP\n0 1 2 3 4\n1\n2\n3\nGPT-3.5\n0.95 0.4 0.15 0.05 0\n0.9 0.55 0.2 0.05 0\n0.75 0.1 0 0 0\n0 1 2 3 4\n1\n2\n3\n0.7 0.5 0.2 0.1 0.1\n0.3 0.05 0 0 0\n0.25 0.1 0 0 0\n0 1 2 3 4\n1\n2\n3\n0.5 0.15 0 0 0\n0.7 0.35 0.1 0.05 0\n0.6 0.3 0 0 0\n0 1 2 3 4\n1\n2\n3\n0.3 0.1 0.05 0 0\n0.45 0.15 0.15 0.05 0.05\n0.45 0.15 0.05 0.05 0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nJoint accuracy\nFigure 3: Joint accuracy of GPT-4 and GPT-3.5 on H I-TOM stories w/ or w/o deceptive agent communications. The\nx-axis stands for ToM orders, and the y-axis is for story lengths (number of chapters). CoTP and VP respectively\nrepresent chain-of-thought and multiple-choice-w/o-explanation prompting styles. The devil sign (\n0 1 2 3 4\n1\n2\n3\n1 0.84 0.7 0.45 0.35\n0.95 0.79 0.5 0.2 0.1\n0.95 0.55 0.2 0.16 0.15\nGPT-4, CoT\n0 1 2 3 4\n1\n2\n3\n1 0.7 0.5 0.45 0.4\n0.95 0.75 0.35 0.1 0.05\n0.93 0.58 0.2 0.15 0.15\nGPT-4, MC\n0 1 2 3 4\n1\n2\n3\n1 0.65 0.3 0.2 0.15\n1 0.55 0.2 0.1 0.1\n1 0.45 0.3 0.1 0.05\n, GPT-4, CoT\n0 1 2 3 4\n1\n2\n3\n1 0.6 0.2 0.1 0.1\n1 0.6 0.35 0.15 0.15\n0.95 0.4 0.21 0.05 0.05\n, GPT-4, MC\n0 1 2 3 4\n1\n2\n3\n0.95 0.4 0.15 0.05 0\n0.9 0.55 0.2 0.05 0\n0.75 0.1 0 0 0\nGPT-3.5, CoT\n0 1 2 3 4\n1\n2\n3\n0.7 0.5 0.2 0.1 0.1\n0.3 0.05 0 0 0\n0.25 0.1 0 0 0\nGPT-3.5, MC\n0 1 2 3 4\n1\n2\n3\n0.5 0.15 0 0 0\n0.7 0.35 0.1 0.05 0\n0.6 0.3 0 0 0\n, GPT-3.5, CoT\n0 1 2 3 4\n1\n2\n3\n0.3 0.1 0.05 0 0\n0.45 0.15 0.15 0.05 0.05\n0.45 0.15 0.05 0.05 0\n, GPT-3.5, MC\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nJoint accuracy ) signiﬁes\naccuracy results on stories with deception, while other results pertain to non-deceptive stories.\n0 1 2 3 4\nDeception times per story\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7Joint accuracy\nOrder: 1st 2nd 3rd 4th\nFigure 4: Joint accuracy of GPT-4 on H I-TOM stories\nwith 0 to 4 sentences of deceptive agent communication.\n0th-order (reality) accuracy is not included, since the\nanswer to the real room of the objects is not affected by\ndeceptive communications.\nIncreased ToM order leads to decreased perfor-\nmances. As the ToM order increases from the\nzeroth to the fourth, the joint accuracy goes from\nnear perfect to near zero. We also observe the dras-\ntic decline in the conventional accuracy scores in\nAppendix C.2.\nLLMs’ performance decreases as there are more\ndeception communications involved. The per-\nformance drops when deception communications\nare involved. Table 5 and Figure 3 reveals a worse\nperformance on stories with agent communications.\nTo further investigate the models’ reasoning abili-\nties in handling deceptive agent communications,\nwe plot the resulting accuracy versus the number\nof deceptive communication sentences (“deception\ntimes”) per story for GPT-4 in Figure 4. As shown\nin Figure 4, as deception times increase from 0\nto 4, the joint accuracy experiences drops of 32%,\n18.1%, 16%, and 11% respectively for the four\nToM orders. This suggests that the deceptive agent\ncommunications challenge the LLMs in their ToM\nreasoning process.\n6 Discussion and Analyses\n6.1 Underlying Patterns in Correct LLM\nPredictions\nAlthough the overall accuracy of the models leaves\nroom for improvement, we observe a higher fre-\nquency of correct model choices under speciﬁc\nconditions. We thus examine the scenarios where\nmodels have higher answer accuracy.\nLLMs handle answers that appear at the begin-\nning and end better. When dealing with long\nthree-chapter stories, LLMs frequently overlook\nkey information, such as the movement of a spe-\nciﬁc container or agent conversations. Yet, they\ntend to pay special attention to the beginning and\nthe end of the story.\nIn Figure 5, we highlight GPT-4’s higher perfor-\nmance when the correct answer aligns with the ﬁrst\nor last container mentioned in the story, as com-\npared to other cases, as demonstrated by the higher\nvalues on the diagonal. This suggests that LLMs\nare better at handling answers that appear at the\nbeginning or at the end. In contrast, The accuracy\nwhen the correct answers are the middle contain-\ners (i.e. neither the ﬁrst nor the last) is similar to\nthose that are not in those containers, as shown in\nFigure 14 in Appendix C.2. We observe similar\npatterns of the Claude model focusing on the be-\nginning and end of stories, as shown in Figure 15\nin Appendix C.2. Our ﬁndings about position bias\nin LLMs align with other works on LLMs ( Wang\net al., 2023).\n10696\nCorrect Incorrect\nLast\n¬Last\n0.7 0.3\n0.46 0.54\nCorrect Incorrect\nFirst\n¬First\n0.62 0.38\n0.38 0.62\n 0.4\n0.6\nFrequency\nFigure 5: Frequency of GPT-4 correctly or incorrectly\nanswering a question of a three-chapter story, based\non whether or not the correct answer is the last or ﬁrst\ncontainer mentioned in the story. “Last”/“First” and\n“¬Last”/“¬First” indicate whether or not the correct\nanswer lies at the last/ﬁrst container.\nLLMs perform better if the answers across or-\nders are the same. We observe that LLMs per-\nform better on question sets where the higher-order\nanswer coincides with a lower-order answer. In\nFigure 6, we see a clear performance disparity be-\ntween LLMs answering correctly if the answers are\nthe same across orders versus the answers being dif-\nferent across orders. However, this may result from\nLLMs’ tendency to predict the same answers across\norders. We ﬁnd that 72.4%, 64.6%, and 59.8% of\nGPT-4’s second, third, and fourth-order answers\nmatch their ﬁrst-order responses. In contrast, only\n30.9%, 20.9%, and 22.2% of the corresponding\ncorrect answers in H I-TOM are the same as their\nﬁrst-order answers. This suggests that GPT-4’s en-\nhanced performance on certain questions may be\ndue to the coincidence of correct answers across\ndifferent ToM orders.\n6.2 Classifying Reasoning Errors\nTo provide a comprehensive overview of the fail-\nure cases of LLMs in ToM reasoning, we manu-\nally evaluate a total of 300 step-by-step responses\nacross all ToM orders by ourselves, comprising\n150 from each of GPT-4 and GPT-3.5. Table 6\ndescribes the ﬁve most prevalent error types with\ncorresponding examples. Figure 7 provides the\nfrequencies of these errors in GPT-4’s responses\nacross different orders. We also show the results\nfor GPT-3.5 and do a comparison between the two\nLLMs in Appendix C.2. As the ToM order in-\ncreases, LLMs tend to demonstrate a higher fre-\nquency of errors. Here we provide hypotheses and\ndiscussions for each of the error types:\nInsufﬁcient reasoning depth. We notice that\nLLMs tend to skip steps in their reasoning process\nand end up with an answer to a lower-order ques-\ntion, as we observed earlier in Section 6.1. One\nreason can be that the pre-training corpus often\n2nd 3rd 4th0\n10\n20\n30\n40\n50\n60\n70Accuracy\nCorrect answer same as the 1st-order answer\nCorrect answer different  from the 1st-order answer\nFigure 6: Standard accuracy of GPT-4 on 2nd, 3rd, and\n4th-order questions, categorized by whether the correct\nanswer matches the corresponding 1st-order answer.\nconsists of simple patterns rather than complex and\nnuanced reasoning scenarios, leading to its frequent\nsimpliﬁcation of the questions. In addition, LLMs\nmay possess a limited contextual understanding of\nthe story. They may struggle to retain and integrate\ninformation from multiple steps or make connec-\ntions across different parts of the text, leading to\noversimpliﬁcation of the question.\nCommonsense errors. LLMs have demonstrated\nremarkable performance on standard benchmarks\nof commonsense reasoning ( Bian et al. , 2023).\nHowever, when it comes to ToM reasoning, even\nadvanced models like GPT-4 are prone to mak-\ning mistakes in handling commonsense knowledge.\nOne key aspect that contributes to these errors is the\ndisparity between the models’ knowledge of com-\nmonsense facts and their ability to effectively apply\nthat knowledge in the complex reasoning process.\nWhile LLMs may possess a vast amount of ex-\nplicit commonsense knowledge, they can struggle\nto appropriately utilize this knowledge while avoid-\ning overgeneralization. In addition, the frequent\ncommonsense errors in H I-TOM might be due to\nthat H I-TOM is newly constructed, and therefore\nLLMs have never seen such data before. In con-\ntrast, LLMs’ pre-training corpus might contain the\ndata in the publicly available commonsense bench-\nmarks, leading to the high performances on LLMs\non those benchmarks ( Magar and Schwartz , 2022).\nHallucinations. Hallucination is a well-\nknown phenomenon in LLMs’ generation process\n(McKenna et al. , 2023). In our experiments, LLMs\nmay have relied on superﬁcial cues and statistical\nassociations to answer the questions, rather than\ngaining a solid understanding of the underlying\ncontext and meaning. Hence, they might resort\nto fabricating baseless details to bridge the logic\ngap between the true story and their erroneous re-\nsponses.\n10697\nError Types Description Example\n▶ Insufﬁcient\nReasoning-Depth\nOversimplify the question\nand skip the required multi-\nstep reasoning.\n: Where does Jack think Hannah thinks William thinks the\ncarrot is?\n: green_box, as that’s where Jack last saw it.\n▶ Commonsense\nErrors\nGenerate outputs that violate\ncommon sense.\n: . . .Aiden exited the pantry after step 8, but he can still\nwitness the move in the pantry after the exit, so . . .\n▶ Hallucinations Fabricate ungrounded de-\ntails or facts.\n(In the story, Benjamin saw a cat, but did not talk about it)\n: . . .But there’s another twist. Suppose Ella also learns that\nBenjamin lied about seeing a cat to distract everyone from his\nreal plan . . .\n▶ Temporal Ignorance Confuse or ignore the tempo-\nral order of events.\n: . . .Lily exited the hallway (step 8) after Amelia moved\nthe corn to the red_basket (step 11), . . .\n▶ Spurious Causal\nInference\nAttribute a cause-and-effect\nrelationship between unre-\nlated events.\n: . . .Carter privately told Emma that the tomato is in the\ngreen_drawer. Private communications are not heard by others,\nso Emma has no reason to doubt Carter’s information.\nTable 6: Types of reasoning errors commonly made by LLMs, with their description and example erroneous\nresponses (\n ) to questions (\n ) from our experiment results on GPT-4.\n0th 1st 2nd 3rd 4th0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nFrequency\nInsufficient reasoning-depth\nCommonsense errors\nHallucinations\nT emporal ignorance\nSpurious causal inference\nFigure 7: Ratio of GPT-4 answers containing the ﬁve\nreasoning errors. The x-axis corresponds to ToM orders.\nLack of temporal information. We observe that\nLLMs’ understanding of the sequence of agent ac-\ntions is often skewed, as the actions are closely\nlisted in H I-TOM stories. This confusion in tempo-\nral order is also found in Yuan et al. (2023). This\nerror may be attributed to biases inherent in the\npre-training corpus which leads to LLMs’ lack of\ngenuine understanding of temporal relations.\nSpurious Causal Inference. The current learning\nparadigm for LLMs is designed to capture the sta-\ntistical correlations among the data ( Devlin et al. ,\n2019). Through such a paradigm, it is difﬁcult\nfor LLMs to capture the underlying logic behind\nthese correlations ( Jin et al. , 2023). As a result,\nLLMs may make incorrect or misleading causal in-\nferences based solely on these superﬁcial patterns.\n7 Implications on the Future of NLP\nWe believe our work has important implications\non the future directions of NLP with respect to the\ntwo-way relation between artiﬁcial intelligence and\nhuman intelligence.\nEnhance LLMs’ ToM ability from the perspec-\ntive of human intelligence. According to Kah-\nneman (2011), human decisions are supported and\nguided by the cooperation of two capabilities or\ntwo systems: System 1 for intuitive, imprecise,\nfast, and often unconscious decisions (“thinking\nfast”), and System 2 for more complex situations\nwith logical and rational thinking (“thinking slow”).\nThis theory has inspired works in computer vision\nand natural language processing communities to\nexplicitly equip models with the two systems ( Hill\net al., 2021; Miech et al. , 2021).\nThrough our examination of LLMs’ ToM ability,\nwe ﬁnd failure cases that resemble the character-\nistics of System 1 thinking; for instance, LLMs\nmay invent causes and intentions (“hallucination”),\nor substitute an easier question for a difﬁcult one\n(“insufﬁcient reasoning-depth”). Furthermore, the\nsigniﬁcant performance drop from zeroth to fourth\norder ToM in H I-TOM suggests that LLMs may\nbe more inclined to System 1 thinking rather than\nSystem 2 thinking, as higher-order ToM requires\ncareful in-depth logical inference.\nHowever, there exists a line of research com-\nbining the symbolic reasoning process ( Simon and\nNewell, 1971; Winograd, 1971), which aligns with\nSystem 2 thinking, with connectionist paradigm\nor neural learning ( Rumelhart et al. , 1986; Le-\nCun et al. , 2015), which captures the intuitive and\npattern recognition aspects of System 1 thinking\n10698\n(Shavlik, 1994; Hitzler, 2022). This integration\nholds the promise of enabling AI systems to per-\nform complex tasks that require both logical de-\nduction and statistical generalization. We believe\nour ﬁndings of ToM limitations of LLMs alongside\nthis previous line of research clearly points to a\ndirection where neural and symbolic approaches\nare combined in order to achieve abilities that are\nmore closely aligned to human intelligence.\nUnderstanding humans through the lens of\nLLMs. ToM plays a crucial role in understand-\ning human intelligence, as it is an important as-\npect of human cognition that enables us to make\ninferences about others’ thoughts, emotions, and\nbehaviors. Enabling progress in ToM reasoning in\nLLMs entails progress in emulating the function-\ning of human mind, which in turn offers intriguing\npossibilities for gaining insights into human inter-\nactions and the emergence of intelligence. While it\nis important to recognize that the analogy between\nhuman and artiﬁcial intelligence has its limitations\nand is a subject of debate within the NLP com-\nmunity ( Bender et al. , 2021), recent research has\nexplored the extrinsic understanding of human in-\nteractions through multi-agent systems ( Park et al. ,\n2023). This approach allows us to observe how\nLLMs can mimic and simulate aspects of human\nbehavior and communication. By studying LLMs\nand their intrinsic properties, such as the emergence\nof intelligence, we can gain valuable insights into\nthe fundamental processes underlying human cog-\nnition ( Wijmans et al. , 2023). Researchers have\nalso developed methods to elicit human-like behav-\nior from LLMs, providing further opportunities to\nexplore and understand the capabilities and limita-\ntions of these models ( Belrose et al. , 2023). While\nLLMs offer a close-up view of human-like lan-\nguage processing, it is crucial to approach the topic\nwith caution and recognize the complexities and\nnuances of human intelligence and behavior.\nEnhance LLMs’ ToM abilities for better NLP\napplications. In daily life, our ToM ability plays\na vital role in understanding others’ intentions,\ntherefore helping us in our communication. In\nHI-TOM, we enable higher-order ToM reasoning,\nwhich in turn can lead to improvements in LLMs\nperformance on tasks such as deception detection,\nemotional support, empathetic communication, and\nothers. Additionally, since LLMs represent foun-\ndational models that are used across various NLP\ntasks and applications, enhancing the abilities of\nLLMs opens up exciting possibilities for improving\nspeciﬁc NLP tasks that beneﬁt from these models.\n8 Conclusion\nIn this paper, we introduce H I-TOM, the ﬁrst ToM\nbenchmark that contains higher-order ToM tasks.\nWe demonstrated that LLMs’ performance suffers a\nsigniﬁcant drop in ToM tasks from lower to higher\norder. By proposing H I-TOM, we hope to address\nthe challenges of ToM in complicated scenarios and\nspark further research on enhancing the reasoning\nability of LLMs.\nFurthermore, we present our insights on the fu-\nture of NLP and discuss potential directions for\nenhancing LLMs. Our aim is to stimulate research\nthat draws inspiration from human intelligence,\nstrives to understand humans better, and ultimately\nleads to the development of NLP applications that\nbetter cater to the needs of humans.\nAcknowledgement\nWe thank the anonymous reviewers for their valu-\nable feedback and discussion. This paper’s draft\nversion was accepted to the non-archival track of\nthe ToM workshop at ICML 2023. We would also\nlike to extend our appreciation to the reviewers\nfrom the ToM workshop for their feedback.\nEthical Considerations\nThe data used in our study were collected from the\nAPI of language models. No sensitive or personal\ninformation was included.\nOur research aims at examining the high-order\nToM reasoning ability of LLMs, and we demon-\nstrate the insufﬁcient higher-order ToM abilities of\nthe current LLMs. There is no direct misuse of our\nﬁndings. However, we recognize that future LLMs\nwith stronger ToM abilities may become more pow-\nerful at generating misinformation and even manip-\nulating people if used by some ill-intended parties.\nTherefore, we advocate for the responsible use of\nLLMs and the associated technologies.\nWe adhere to the principles of transparency and\nopenness. All methods and ﬁndings are reported\ncompletely and honestly. Furthermore, we will\nmake our code public upon acceptance. We invite\nreaders to utilize this resource for a more compre-\nhensive understanding of our methods and results.\n10699\nLimitations\nThe limitations of our work can be stated from the\nfollowing perspectives.\n1. Due to the constraint of computing resources\nand budget, we only test four LLMs. However,\nwe try our best to select the representative\nLLMs from close-sourced to open-sourced\nLLMs including GPT-3.5 and GPT-4 from\nOpenAI, Claude from Anthropic, and Gua-\nnaco from the community.\n2. Due to the scope of this paper, we only demon-\nstrate the insufﬁcient ToM abilities of LLMs.\nFuture works may further investigate how dif-\nferent training paradigms such as training with\nor without reinforcement learning with human\nfeedback (RLHF) affect the ToM ability of\nthese LLMs.\n3. We acknowledge that our dataset was con-\nstructed based on speciﬁc rules, which means\nits dialog syntax may differ from genuine con-\nversations. In the real world, higher-order\ninteractions might occur in a more implicit\nmanner, embedded within more intricate dia-\nlogues and questions. We plan to address this\nin future research.\n4. We share our thoughts on the future of NLP\nand research with LLMs, hoping to stimulate\nresearch that draws inspiration from human\nintelligence, understands humans better, and\nserves humans better. We admit that there\nexist alternative ways of moving forward on\nNLP research. We welcome feedback and\nopen discussion on how we can collectively\nadvance NLP research.\nReferences\nSimon Baron-Cohen, Alan M Leslie, and Uta Frith.\n1985. Does the autistic child have a theory of mind?\nCognition, 21(1):37–46.\nNora Belrose, Zach Furman, Logan Smith, Danny Ha-\nlawi, Igor Ostrovsky, Lev McKinney, Stella Bider-\nman, and Jacob Steinhardt. 2023. Eliciting latent\npredictions from transformers with the tuned lens.\narXiv preprint arXiv:2303.08112 .\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM confer-\nence on fairness, accountability, and transparency ,\npages 610–623.\nNing Bian, Xianpei Han, Le Sun, Hongyu Lin, Yaojie\nLu, and Ben He. 2023. Chatgpt is a knowledgeable\nbut inexperienced solver: An investigation of com-\nmonsense problem in large language models. arXiv\npreprint arXiv:2303.16421.\nInge Bretherton and Marjorie Beeghly. 1982. Talk-\ning about internal states: The acquisition of an ex-\nplicit theory of mind. Developmental psychology ,\n18(6):906.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artiﬁcial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nHarmen De Weerd, Rineke Verbrugge, and Bart Verheij.\n2015. Higher-order theory of mind in the tacit com-\nmunication game. Biologically Inspired Cognitive\nArchitectures, 11:10–21.\nHarmen De Weerd, Rineke Verbrugge, and Bart Ver-\nheij. 2022. Higher-order theory of mind is especially\nuseful in unpredictable negotiations. Autonomous\nAgents and Multi-Agent Systems , 36(2):30.\nHarmen de Weerd and Bart Verheij. 2011. The advan-\ntage of higher-order theory of mind in the game of\nlimited bidding. In Proceedings Workshop Reason-\ning about other Minds, CEUR Workshop Proceed-\nings, volume 751, pages 149–164.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: Efﬁcient ﬁnetuning\nof quantized llms. arXiv preprint arXiv:2305.14314 .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers) , pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nUta Frith and Christopher D Frith. 2003. Development\nand neurophysiology of mentalizing. Philosophical\nTransactions of the Royal Society of London. Series\nB: Biological Sciences , 358(1431):459–473.\nFelix Hill, Olivier Tieleman, Tamara von Glehn,\nNathaniel Wong, Hamza Merzic, and Stephen Clark.\n2021. Grounded language learning fast and slow . In\nInternational Conference on Learning Representa-\ntions.\nPascal Hitzler. 2022. Neuro-symbolic artiﬁcial intelli-\ngence: The state of the art.\n10700\nZhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff, Mrin-\nmaya Sachan, Rada Mihalcea, Mona Diab, and Bern-\nhard Schölkopf. 2023. Can large language models\ninfer causation from correlation?\nDaniel Kahneman. 2011. Thinking, fast and slow .\nmacmillan.\nMichal Kosinski. 2023. Theory of mind may have spon-\ntaneously emerged in large language models .\nMatthew Le, Y-Lan Boureau, and Maximilian Nickel.\n2019. Revisiting the evaluation of theory of mind\nthrough question answering. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5872–5877.\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton.\n2015. Deep learning. nature, 521(7553):436–444.\nBethany Liddle and Daniel Nettle. 2006. Higher-order\ntheory of mind and social competence in school-age\nchildren. Journal of Cultural and Evolutionary Psy-\nchology, 4(3-4):231–244.\nZiqiao Ma, Jacob Sansom, Run Peng, and Joyce Chai.\n2023. Towards a holistic landscape of situated theory\nof mind in large language models. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2023. Association for Computational Linguistics.\nInbal Magar and Roy Schwartz. 2022. Data contamina-\ntion: From memorization to exploitation . In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 157–165, Dublin, Ireland. Association\nfor Computational Linguistics.\nNick McKenna, Tianyi Li, Liang Cheng, Moham-\nmad Javad Hosseini, Mark Johnson, and Mark Steed-\nman. 2023. Sources of hallucination by large lan-\nguage models on inference tasks. arXiv preprint\narXiv:2305.14552.\nAntoine Miech, Jean-Baptiste Alayrac, Ivan Laptev,\nJosef Sivic, and Andrew Zisserman. 2021. Think-\ning fast and slow: Efﬁcient text-to-visual retrieval\nwith transformers. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 9826–9836.\nRachel LC Mitchell and Louise H Phillips. 2015. The\noverlapping relationship between emotion perception\nand theory of mind. Neuropsychologia, 70:1–10.\nAida Nematzadeh, Kaylee Burns, Erin Grant, Alison\nGopnik, and Thomas L Grifﬁths. 2018. Evaluating\ntheory of mind in question answering. arXiv preprint\narXiv:1808.09352.\nOpenAI. 2023. Gpt-4 technical report .\nTB OpenAI. 2022. Chatgpt: Optimizing language mod-\nels for dialogue. OpenAI.\nChristopher Osterhaus and Susanne Koerber. 2021. The\ndevelopment of advanced theory of mind in middle\nchildhood: A longitudinal study from age 5 to 10\nyears. Child Development, 92(5):1872–1888.\nJoon Sung Park, Joseph C O’Brien, Carrie J Cai, Mered-\nith Ringel Morris, Percy Liang, and Michael S\nBernstein. 2023. Generative agents: Interactive\nsimulacra of human behavior. arXiv preprint\narXiv:2304.03442.\nJosef Perner. 1991. Understanding the representational\nmind. The MIT Press.\nDavid Premack and Guy Woodruff. 1978. Does the\nchimpanzee have a theory of mind? Behavioral and\nbrain sciences, 1(4):515–526.\nGarret Ridinger and Michael McBride. 2017. Theory\nof mind ability and cooperation. Manuscript, Univ.\nCalifornia, Irvine.\nDavid E. Rumelhart, James L. McClelland, and PDP Re-\nsearch Group. 1986. Parallel Distributed Process-\ning: Explorations in the Microstructure of Cognition:\nFoundations. The MIT Press.\nMaarten Sap, Ronan LeBras, Daniel Fried, and Yejin\nChoi. 2022. Neural theory-of-mind? on the limits\nof social intelligence in large lms. arXiv preprint\narXiv:2210.13312.\nJude W Shavlik. 1994. Combining symbolic and neural\nlearning. Machine Learning, 14:321–331.\nHerbert A Simon and Allen Newell. 1971. Human\nproblem solving: The state of the theory in 1970.\nAmerican psychologist, 26(2):145.\nDan Sperber and Deirdre Wilson. 2002. Pragmatics,\nmodularity and mind-reading. Mind & language ,\n17(1-2):3–23.\nMasanori Takano, Takaya Arita, et al. 2006. Asymmetry\nbetween even and odd levels of recursion in a theory\nof mind. Proceedings of ALife X , pages 405–411.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efﬁcient foundation language models .\nTomer Ullman. 2023. Large language models fail on\ntrivial alterations to theory-of-mind tasks. arXiv\npreprint arXiv:2302.08399.\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai\nLin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\n2023. Large language models are not fair evaluators .\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903 .\n10701\nErik Wijmans, Manolis Savva, Irfan Essa, Stefan Lee,\nAri S Morcos, and Dhruv Batra. 2023. Emergence\nof maps in the memories of blind navigation agents.\narXiv preprint arXiv:2301.13261 .\nTerry Winograd. 1971. Procedures as a represen-\ntation for data in a computer program for under-\nstanding natural language. Technical report, MAS-\nSACHUSETTS INST OF TECH CAMBRIDGE\nPROJECT MAC.\nChenhan Yuan, Qianqian Xie, and Sophia Ananiadou.\n2023. Zero-shot temporal relation extraction with\nchatgpt. arXiv preprint arXiv:2304.05454 .\nA Author Contributions\nYufan Wu and Yinghui He conceived of the idea\nand planned the experiments. Yufan Wu and\nYinghui He took the lead in writing the script\nfor dataset generation. Yilin Jia, Yufan Wu, and\nYinghui He carried out the experiments of testing\nlanguage models. Yulong Chen and Naihao Deng\nsupervised the project. Naihao Deng came up with\nthe high-level idea for the paper, which was later\nreﬁned in team discussions. All authors contributed\nto the writing and editing of this paper. Speciﬁcally,\nYufan Wu and Yinghui He drafted the paper. Yilin\nJia contributed to the writing for experiment setups.\nRada Mihalcea, Yulong Chen, and Naihao Deng\nhelped edit the paper.\nB H I-TOM Details\nB.1 Assumptions\nOur simpliﬁed deception and belief mechanisms\nare based on four assumptions. Table 7 shows the\noriginal assumption list we attach to each story in\nthe dataset and prompt into LLMs.\nNote: You should assume the following.\n(1) An agent witnesses everything and every\nmovement before exiting a room.\n(2) An agent A can infer another agent B’s\nmental state only if A and B have been in\nthe same room, or have private or public\ninteractions.\n(3) Note that every agent tend to lie. What\na character tells others doesn’t affect his\nactual belief. An agent tend to trust a agent\nthat exited the room later than himself. The\nexit order is known to all agents.\n(4) Agents in private communications know that\nothers won’t hear them, but they know that\nanyone can hear any public claims.\nTable 7: Assumption list attached to each H I-TOM story\nand prompt into LLMs.\nB.2 Story Generation Details\nAlgorithm 1 and Algorithm 2 provide the pseu-\ndocode for the generation process of each chapter\nand the whole story in H I-TOM.\nIn Algorithm 1, the function M OVE is employed\nto populate the story components into the template,\nthereby producing a sentence that describes the\nmovement. The function C OMMUNICATE gener-\nates content related to the \"tell\" action. Meanwhile,\nthe R ANDOM _DISTRACTOR function introduces\nrandom distractors into the story.\n10702\nIn Algorithm 2, the question generator Q_G EN\nrandomly picks the agents and the object appearing\nin the story and populates them into a predeﬁned\nquestion template. Then, the answer generator\nA_G EN generates the answer to the correspond-\ning question based on a dictionary that traces the\nbeliefs of different orders of each agent.\nAlgorithm 1 HI-TOM Chapter Generation Algorithm\nInput: agents, room, conts, obj\nOutput: chap\n1: function CHAP (agents, room, conts, obj)\n2: for agent in agents do\n3: set no_move to random boolean value\n4: move ←MOVE(agent, conts, obj, no_move)\n5: add move into chap\n6: end for\n7: com ←COMMUNICATE (agents)\n8: rd ←RANDOM _DISTRACTOR (agents, conts)\n9: add com and rd into chap\n10: return chap\n11: end function\nC Experiment Details\nC.1 Prompting inputs\nIn our experiments, the average number of tokens\nin a single prompt is 453.3, and the total token\nnumber of our prompts on each model is 543968,\nincluding VP and CoTP prompts on stories with or\nwithout deception.\nTable 8 is a sample CoTP prompt in our experi-\nments. We specify the range of the story, question,\nchoices, and assumptions to enhance the models’\nunderstanding. We also order each line of the story\nto indicate the chronological order. The provided\nanswer choices are all the containers appearing in\nthe story.\nC.2 Supplementary Results\nFigure 9 and Figure 8 shows the detailed joint accu-\nracy results of Guanaco and Claude. The joint ac-\ncuracy generally decreases as the story length and\nthe ToM orders increase, aligning with the results\nof GPT-4 and GPT-3.5. The overall joint accuracy\nperformance of Claude is better than Guanaco.\nFigure 10 to Figure 13 show the standard ac-\ncuracy results of GPT-4, GPT-3.5-turbo, Claude-\ninstant and Guanaco 65B, as the break-down details\nof Table 5. Among the four models, GPT-4 has\nthe highest and most stable performance, reaching\nnearly perfect accuracy on the zeroth order and\nhigher than 20% on the fourth.\nUnder CoTP prompting, each model reaches a\nhigh performance on zeroth-order questions, espe-\n0 1 2 3 4\n1\n2\n3\n0.9 0.6 0.4 0.3 0.15\n0.85 0.5 0.25 0.05 0\n0.6 0.4 0.15 0.1 0.05\nCoTP\n0 1 2 3 4\n1\n2\n3\n0.7 0.15 0.05 0 0\n0.65 0.3 0.1 0.1 0.05\n0.7 0.3 0.15 0.05 0\n, CoTP\n0 1 2 3 4\n1\n2\n3\n0.6 0.3 0.15 0.15 0.15\n0.45 0.35 0.1 0 0\n0.6 0.45 0.15 0.05 0.05\nVP\n0 1 2 3 4\n1\n2\n3\n0.5 0.2 0.05 0.05 0.05\n0.6 0.25 0.2 0.15 0.1\n0.5 0.3 0.1 0 0\n, VP\nClaude, Joint\nFigure 8: Joint accuracy results of Claude-instant.\n0 1 2 3 4\n1\n2\n3\n0.8 0.25 0.1 0 0\n0.45 0.1 0.1 0.1 0\n0.7 0.25 0.05 0 0\nCoTP\n0 1 2 3 4\n1\n2\n3\n0.85 0.3 0 0 0\n0.6 0.2 0.05 0 0\n0.2 0.15 0 0 0\n, CoTP\n0 1 2 3 4\n1\n2\n3\n0.55 0.3 0.25 0.25 0.2\n0.55 0.35 0.15 0.1 0.1\n0.45 0.3 0.2 0.2 0.15\nVP\n0 1 2 3 4\n1\n2\n3\n0.55 0.35 0.2 0.2 0.15\n0.45 0.3 0.15 0.15 0.15\n0.4 0.2 0.1 0.05 0.05\n, VP\nGuanaco, Joint\nFigure 9: Joint accuracy results of Guanaco 65B.\n10703\nRead the following story and answer the multiple-choice question. Think step-by-step.\nProvide the answer first, and then explain it.\nStory:\n1 William, Jack, Charlotte, Noah and Hannah entered the hall.\n2 Noah saw a monkey.\n3 The carrot is in the red_basket.\n4 William made no movements and stayed in the hall for 1 minute.\n5 William exited the hall.\n6 Hannah saw a mouse.\n7 Jack made no movements and stayed in the hall for 1 minute.\n8 Jack exited the hall.\n9 Charlotte made no movements and stayed in the hall for 1 minute.\n10 Charlotte exited the hall.\n11 Charlotte dislikes the tomato.\n12 Noah moved the carrot to the green_envelope.\n13 Noah exited the hall.\n14 Hannah moved the carrot to the red_basket.\n15 Hannah exited the hall.\n16 William, Jack, Charlotte, Noah and Hannah entered the waiting_room.\n17 Charlotte publicly claimed that carrot is in the green_envelope.\n18 Hannah privately told Charlotte that the carrot is in the blue_container.\nQuestion: Where does Charlotte think Jack thinks Hannah thinks William thinks the carrot\nis?\nChoices: A. green_envelope, B. red_basket, C. blue_container, D. red_crate, E. green_drawer,\nF. blue_bucket, G. green_cupboard, H. red_bottle, I. green_treasure_chest, J. blue_cupboard,\nK. red_pantry, L. red_container, M. blue_bathtub, N. red_envelope, O. blue_pantry\nNote: You should assume the following. (1) An agent witnesses everything and every\nmovements before exiting a room. (2) An agent A can infer another agent B’s mental state\nonly if A and B have been in the same room, or have private or public interactions. (3)\nNote that every agent tend to lie. What a character tells others doesn’t affect his actual\nbelief. An agent tend to trust a agent that exited the room later than himself. The exit\norder is known to all agents. (4) Agents in private communications know that others won’t\nhear them, but they know that anyone can hear any public claims.\nTable 8: An example CoTP prompt of a one-chapter H I-TOM story with a fourth-order question.\nAlgorithm 2 HI-TOM Story Generation Algorithm\nInput: Number of chapters: ℓ ∈{1, 2, 3}\nStory components: Rooms, Objects,\nContainers, Agents\nOutput: story, question, answer\n1: function STORY (ℓ, Rooms, Objects, Containers,\nAgents)\n2: for i ←1 to l do\n3: randomly choose room,obj, conts, agents\n4: chap ←CHAP (room, obj, conts, agents)\n5: add chap into story\n6: end for\n7: question ←Q_G EN(Agents, Objects)\n8: answer ←A_G EN(Agents, Objects)\n9: return story, question, answer\n10: end function\ncially for stories without agent communications,\nand their performance deteriorates with increased\nToM order and story length. Yet, under VP prompt-\ning, Claude and Guanaco exhibit a uniform perfor-\nmance of around 50% across all the orders.\n0 1 2 3 4\n1\n2\n3\n1.0 0.85 0.8 0.65 0.6\n0.95 0.8 0.6 0.35 0.3\n0.95 0.55 0.3 0.4 0.45\nCoTP\n0 1 2 3 4\n1\n2\n3\n0.9 0.65 0.55 0.5 0.3\n1.0 0.55 0.45 0.5 0.5\n1.0 0.45 0.45 0.3 0.2\n, CoTP\n0 1 2 3 4\n1\n2\n3\n1.0 0.7 0.6 0.5 0.4\n0.95 0.8 0.45 0.4 0.4\n0.9 0.55 0.35 0.5 0.5\nVP\n0 1 2 3 4\n1\n2\n3\n1.0 0.6 0.35 0.35 0.4\n1.0 0.6 0.6 0.6 0.45\n0.95 0.45 0.55 0.55 0.5\n, VP\nGPT-4, Standard\nFigure 10: Standard accuracy results of GPT-4.\n10704\n0 1 2 3 4\n1\n2\n3\n0.95 0.45 0.45 0.05 0.1\n0.9 0.6 0.2 0.15 0.2\n0.8 0.2 0.2 0.1 0.2\nCoTP\n0 1 2 3 4\n1\n2\n3\n0.55 0.4 0.3 0.3 0.2\n0.7 0.5 0.35 0.4 0.25\n0.65 0.55 0.15 0.15 0.05\n, CoTP\n0 1 2 3 4\n1\n2\n3\n0.7 0.7 0.3 0.25 0.1\n0.3 0.5 0.2 0.05 0.2\n0.25 0.5 0.15 0.05 0.05\nVP\n0 1 2 3 4\n1\n2\n3\n0.3 0.3 0.3 0 0.05\n0.45 0.3 0.55 0.2 0.2\n0.45 0.35 0.3 0.15 0.05\n, VP\nGPT-3.5, Standard\nFigure 11: Standard accuracy results of GPT-3.5-turbo.\n0 1 2 3 4\n1\n2\n3\n0.9 0.7 0.6 0.55 0.5\n0.85 0.6 0.4 0.3 0.05\n0.6 0.5 0.55 0.45 0.3\nCoTP\n0 1 2 3 4\n1\n2\n3\n0.7 0.25 0.3 0.3 0.3\n0.65 0.4 0.35 0.4 0.3\n0.7 0.45 0.4 0.25 0.3\n, CoTP\n0 1 2 3 4\n1\n2\n3\n0.6 0.65 0.5 0.7 0.45\n0.45 0.5 0.25 0.4 0.45\n0.6 0.6 0.3 0.5 0.45\nVP\n0 1 2 3 4\n1\n2\n3\n0.5 0.35 0.35 0.45 0.4\n0.6 0.5 0.6 0.4 0.25\n0.5 0.45 0.4 0.25 0.3\n, VP\nClaude, Standard\nFigure 12: Standard accuracy results of Claude-instant.\n0 1 2 3 4\n1\n2\n3\n0.8 0.4 0.35 0.45 0.45\n0.45 0.35 0.35 0.3 0.15\n0.7 0.35 0.4 0.3 0.2\nCoTP\n0 1 2 3 4\n1\n2\n3\n0.85 0.3 0.5 0.3 0.3\n0.6 0.35 0.5 0.2 0.25\n0.2 0.15 0.25 0.15 0.3\n, CoTP\n0 1 2 3 4\n1\n2\n3\n0.55 0.4 0.4 0.5 0.45\n0.55 0.45 0.5 0.4 0.55\n0.45 0.45 0.35 0.4 0.4\nVP\n0 1 2 3 4\n1\n2\n3\n0.55 0.45 0.4 0.7 0.7\n0.45 0.5 0.25 0.45 0.45\n0.4 0.45 0.6 0.4 0.5\n, VP\nGuanaco, Standard\nFigure 13: Standard accuracy results of Guanaco 65B.\nFigure 14 illustrates the performance compari-\nson of GPT-4 between the case when the correct\nanswer is in a certain position in the middle of the\nstory, and the case when it is not. We observe that\nGPT-4 does not signiﬁcantly perform better when\nthe correct answer lies in the middle of the story.\nThis serves as a contrast to Figure 5, highlighting\nthe better ability of GPT-4 to capture answers at\nthe beginning or the end of a story.\nCorrect Incorrect\nSecond\n¬Second\n0.42 0.58\n0.52 0.48\nCorrect Incorrect\nThird\n¬Third\n0.58 0.42\n0.47 0.53\nCorrect Incorrect\nSecond\nlast\n¬Second\nlast\n0.49 0.51\n0.5 0.5\nCorrect Incorrect\nThird\nlast\n¬Third\nlast\n0.52 0.48\n0.49 0.51\nFigure 14: Performance of GPT-4 in the cases when the\ncorrect answer does or does not lie in a certain position.\nFigure 15 shows similar observations for Claude.\nThe plots for the last and ﬁrst positions of con-\ntainers show a higher frequency in the top-left and\nbottom-right cells, while the plots for other posi-\ntions do not imply such a pattern.\nCorrect Incorrect\nLast\n¬Last\n0.52 0.48\n0.42 0.58\nCorrect Incorrect\nFirst\n¬First\n0.53 0.47\n0.36 0.64\nCorrect Incorrect\nSecond\n¬Second\n0.43 0.57\n0.44 0.56\nCorrect Incorrect\nThird\n¬Third\n0.51 0.49\n0.42 0.58\nCorrect Incorrect\nSecond\nlast\n¬Second\nlast\n0.42 0.58\n0.44 0.56\nCorrect Incorrect\nThird\nlast\n¬Third\nlast\n0.38 0.62\n0.45 0.55\nFigure 15: Performance of Claude in the cases when the\ncorrect answer does or does not lie in a certain position.\nFigure 16 details the appearance frequency of the\nﬁve reasoning errors in the step-by-step responses\nof GPT-3.5. Compared to the error ratios of GPT-4\n(Figure 7), the frequencies of commonsense errors,\nhallucinations, and spurious causal inference are\nsigniﬁcantly higher, implying GPT-3.5’s immature\nperceptions of the world and its deﬁcient logical\nreasoning abilities. The occurrence of insufﬁcient\nreasoning depth and temporal ignorance escalates\n10705\n0th 1st 2nd 3rd 4th0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nFrequency\nGPT-3.5\nInsufficient reasoning-depth\nCommonsense errors\nHallucinations\nT emporal ignorance\nSpurious causal inference\nFigure 16: Ratio of the ﬁve reasoning errors in GPT-\n3.5’s responses.\nin higher-order responses.\nThe comparison between Figure 7 and Figure 16\nyields that GPT-4 has not resolved the errors of\ncommonsense, insufﬁcient reasoning depth, and\ntemporal ignorance, while hallucinations and spuri-\nous causal inference have been largely addressed.\n10706"
}