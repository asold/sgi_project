{
  "title": "MaxMViT-MLP: Multiaxis and Multiscale Vision Transformers Fusion Network for Speech Emotion Recognition",
  "url": "https://openalex.org/W4391407069",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4364765659",
      "name": "Kah Liang Ong",
      "affiliations": [
        "Multimedia University"
      ]
    },
    {
      "id": "https://openalex.org/A2097415797",
      "name": "Chin-Poo Lee",
      "affiliations": [
        "Multimedia University"
      ]
    },
    {
      "id": "https://openalex.org/A2133491723",
      "name": "Heng Siong Lim",
      "affiliations": [
        "Multimedia University"
      ]
    },
    {
      "id": "https://openalex.org/A2053525470",
      "name": "Kian Ming Lim",
      "affiliations": [
        "Multimedia University"
      ]
    },
    {
      "id": "https://openalex.org/A2163176472",
      "name": "Ali Alqahtani",
      "affiliations": [
        "King Khalid University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3157935640",
    "https://openalex.org/W3199176154",
    "https://openalex.org/W3217072367",
    "https://openalex.org/W3145643603",
    "https://openalex.org/W4285319295",
    "https://openalex.org/W4205567678",
    "https://openalex.org/W3095435288",
    "https://openalex.org/W3210714942",
    "https://openalex.org/W4226153164",
    "https://openalex.org/W4285112472",
    "https://openalex.org/W4293833005",
    "https://openalex.org/W4312776009",
    "https://openalex.org/W4306938603",
    "https://openalex.org/W4378191676",
    "https://openalex.org/W4379203891",
    "https://openalex.org/W4360856918",
    "https://openalex.org/W4379033883",
    "https://openalex.org/W2085662862",
    "https://openalex.org/W3041561163",
    "https://openalex.org/W3148040514",
    "https://openalex.org/W3198858531",
    "https://openalex.org/W2973049979",
    "https://openalex.org/W6769196770",
    "https://openalex.org/W4362579608",
    "https://openalex.org/W4309564859",
    "https://openalex.org/W4312847199",
    "https://openalex.org/W6796931752",
    "https://openalex.org/W4214614183",
    "https://openalex.org/W4312769131",
    "https://openalex.org/W175750906",
    "https://openalex.org/W2803193013",
    "https://openalex.org/W2979476256"
  ],
  "abstract": "Vision Transformers, known for their innovative architectural design and modeling capabilities, have gained significant attention in computer vision. This paper presents a dual-path approach that leverages the strengths of the Multi-Axis Vision Transformer (MaxViT) and the Improved Multiscale Vision Transformer (MViTv2). It starts by encoding speech signals into Constant-Q Transform (CQT) spectrograms and Mel Spectrograms with Short-Time Fourier Transform (Mel-STFT). The CQT spectrogram is then fed into the MaxViT model, while the Mel-STFT is input to the MViTv2 model to extract informative features from the spectrograms. These features are integrated and passed into a Multilayer Perceptron (MLP) model for final classification. This hybrid model is named the &#x201C;MaxViT and MViTv2 Fusion Network with Multilayer Perceptron (MaxMViT-MLP).&#x201D; The MaxMViT-MLP model achieves remarkable results with an accuracy of 95.28&#x0025; on the Emo-DB, 89.12&#x0025; on the RAVDESS dataset, and 68.39&#x0025; on the IEMOCAP dataset, substantiating the advantages of integrating multiple audio feature representations and Vision Transformers in speech emotion recognition.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1109/ACCESS.2017.DOI\nMaxMViT-MLP: Multiaxis and Multiscale\nVision Transformers Fusion Network for\nSpeech Emotion Recognition\nKAH LIANG ONG1, CHIN POO LEE1, HENG SIONG LIM2, KIAN MING LIM1, and ALI\nALQAHTANI3,4\n1Faculty of Information Science and Technology, Multimedia University, Melaka, 75450 Malaysia\n2Faculty of Engineering and Technology, Multimedia University, Melaka, 75450 Malaysia\n3Department of Computer Science, King Khalid University, Abha 61421, Saudi Arabia\n4Center for Artificial Intelligence (CAI), King Khalid University, Abha 61421, Saudi Arabia\nCorresponding author: Chin Poo Lee (e-mail: cplee@mmu.edu.my).\nThe research reported in this paper is funded by the Telekom Malaysia R&D grants (RDTC/221064 and RDTC/231075) and the Deanship\nof Scientific Research at King Khalid University, Saudi Arabia, under Grant number RGP2/332/44.\nABSTRACT Vision Transformers, known for their innovative architectural design and modeling capa-\nbilities, have gained significant attention in computer vision. This paper presents a dual-path approach that\nleverages the strengths of the Multi-Axis Vision Transformer (MaxViT) and the Improved Multiscale Vision\nTransformer (MViTv2). It starts by encoding speech signals into Constant-Q Transform (CQT) spectro-\ngrams and Mel Spectrograms with Short-Time Fourier Transform (Mel-STFT). The CQT spectrogram is\nthen fed into the MaxViT model, while the Mel-STFT is input to the MViTv2 model to extract informative\nfeatures from the spectrograms. These features are integrated and passed into a Multilayer Perceptron (MLP)\nmodel for final classification. This hybrid model is named the “MaxViT and MViTv2 Fusion Network with\nMultilayer Perceptron (MaxMViT-MLP).” The MaxMViT-MLP model achieves remarkable results with an\naccuracy of 95.28% on the Emo-DB, 89.12% on the RA VDESS dataset, and 68.39% on the IEMOCAP\ndataset, substantiating the advantages of integrating multiple audio feature representations and Vision\nTransformers in speech emotion recognition.\nINDEX TERMSSpeech emotion recognition, Ensemble learning, Spectrogram, Vision Transformer, Emo-\nDB, RA VDESS, IEMOCAP\nI. INTRODUCTION\nS\nPEECH emotion recognition stands at the confluence\nof signal processing and machine learning, addressing\nthe automatic identification and classification of emotional\nexpressions within spoken language. Signal processing forms\nthe bedrock of speech emotion recognition, involving the\nextraction of significant information from speech signals, in-\ncluding spectral features, prosodic cues, and acoustic charac-\nteristics. Machine learning techniques play a pivotal role, en-\nabling the development of models that recognize emotions by\nlearning patterns and features associated with different emo-\ntional states. With applications spanning human-computer in-\nteraction, sentiment analysis, mental health assessment, and\ncustomer service enhancement, speech emotion recognition\nhas captured the attention of researchers and practitioners\nalike.\nIn light of this, our paper introduces a dual-path ap-\nproach referred to as the “MaxViT and MViTv2 Fusion\nNetwork with Multilayer Perceptron (MaxMViT-MLP)”. The\nmethod encodes the speech signals into two representations:\nthe Constant-Q Transform (CQT) spectrogram and the Mel\nSpectrogram via Short-Time Fourier Transform (Mel-STFT).\nThis dual spectrogram strategy leverages the complementary\nattributes of CQT and Mel-STFT, providing a holistic and\ninformative portrayal of the input data. The CQT spectro-\ngram is routed to the MaxViT model, while the Mel-STFT is\nchanneled to the MViTv2 model. These Vision Transformers\nexcel in extracting meaningful features from their respective\nspectrogram inputs. The resulting features are integrated,\nculminating in a comprehensive representation of the input\ndata, which is then directed into a Multilayer Perceptron\n(MLP) for the final classification. The main contributions of\nVOLUME 4, 2016 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3360483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nthis work can be summarized as follows:\n• Introduces a dual-path architecture for speech emotion\nrecognition, denoted as the “MaxViT and MViTv2 Fu-\nsion Network with Multilayer Perceptron (MaxMViT-\nMLP)”. This approach involves encoding speech signals\ninto two distinct representations. Subsequently, each\nrepresentation is channeled into its respective Vision\nTransformer model. The features from the Vision Trans-\nformer models are amalgamated and subjected to a MLP\nfor further representation learning and classification.\n• Represents speech signals in dual spectrograms: CQT\nand Mel-STFT. CQT uses logarithmic frequency bin-\nning, and one of its key strengths is its constant-Q\nresolution, meaning it offers a higher resolution at lower\nfrequencies and a coarser resolution at higher frequen-\ncies. This makes it particularly effective at capturing\nfine details in the low-frequency range, which is well-\nsuited for harmonic analysis. Mel-STFT uses a non-\nlinear frequency scale to capture broader spectral char-\nacteristics and is more intuitive for human interpre-\ntation of sound. Mel-STFT provides a detailed time-\nfrequency representation of the audio signal, making it\nsuitable for tasks that involve transient events. This dual\nspectrogram strategy capitalizes on the complementary\ncharacteristics of CQT and Mel-STFT, resulting in a\ncomprehensive representation of the speech signals.\n• Incorporates MaxViT and MViTv2 for representation\nlearning on the CQT and Mel-STFT spectrograms,\nrespectively. The hierarchical architecture of MaxViT\nallows it to capture multiscale information effectively.\nThe block attention module partitions the input feature\nmap into distinct windows and applies self-attention\nmechanisms to foster contextual understanding, allow-\ning MaxViT to recognize intricate patterns and rela-\ntionships within audio data. The grid attention module\nattends globally to pixels through a sparse grid, which\nis beneficial for capturing global contextual informa-\ntion. On the other hand, MViTv2 facilitates the capture\nof multiscale features with different channel-resolution\nscales. The utilization of relative positional embeddings\nin MViTv2 injects shift-invariance properties, enhanc-\ning its ability to understand spatial relationships within\nthe spectrograms.\nII. RELATED WORKS\nResearchers have ventured into a multitude of learning ap-\nproaches for speech emotion recognition. Nevertheless, the\ncomplexity of speech emotion recognition endures, primarily\nstemming from the myriad variations in speaking styles,\ngender, cultural influences, emotional expression, and other\nfactors. This section presents an overview of the existing\nresearch in the field of speech emotion recognition.\nWen et al. (2020) [1] presented a fusion model that com-\nbined the strengths of a capsule network and a Convolutional\nNeural Network, denoted as CapCNN. The method involved\na two-step pre-processing procedure comprising voice ac-\ntivity detection and a windowed framework. These steps\nspeech emotion recognitionved to locate the speech segments\nand enhance the overall quality of the audio signals. The\nCapCNN architecture was trained using a diverse set of input\nfeatures, encompassing MFCC, spectrogram, and spectral\nfeatures. The proposed CapCNN model, when applied to\nspectral features, achieved remarkable performance on the\nEmo-DB with an accuracy of 82.90%.\nA two-dimensional Convolutional Neural Network (2D-\nCNN) was proposed by Mujaddidurrahman et al. (2021) [2]\nfor speech emotion recognition. The method involved data\naugmentation with noise injection and variations in loudness\nto enhance the diversity of the training data. Following aug-\nmentation, the speech signals were transformed into log-mel\nspectrogram features, which were used as input for the 2D-\nCNN model. The proposed model achieved an accuracy of\n88% on the Emo-DB.\nHe & Ren (2021) [3] utilized various techniques for\nspeech emotion recognition, including XGBoost, Convo-\nlutional Neural Network (CNN), and Bi-directional Long\nShort-term Memory (BiLSTM) with an attention model.\nThe speech signals first went through the framing with a\n25ms frame length. Subsequently, 34 low-level descriptors\nwere extracted from each frame. These descriptors were\nthen passed into the XGBoost classifier for feature selection.\nSubsequently, the chosen features were fed into a hybrid\narchitecture combining both CNN and BiLSTM with atten-\ntion model. The proposed CNN and BiLSTM with attention\nmodel achieved 86.87% accuracy on the Emo-DB.\nIn this study, Ancilin & Milton (2021) [4] employed\nmel-frequency magnitude coefficient (MFMC) features for\nspeech emotion recognition. The MFMC features encapsu-\nlated a logarithmic representation of the magnitude spectrum\naligned with the non-linear mel frequency scale. The MFMC\nfeatures were then classified using a Support Vector Machine\n(SVM). The experimental results showed that the MFMC-\nSVM method yielded an accuracy of 81.50% on the Emo-DB\nand 64.31% on the RA VDESS dataset.\nPham et al. (2021) [5] explored various spectral features,\nnamely mel frequency cepstral coefficients (MFCCs), mel\nscaled spectrogram, chromagram, spectral contrast feature,\nand tonnetz representation for speech emotion recognition.\nThe method calculated the mean values of the features and\nstacked them together to form spectral mean vector features.\nThereafter, CNN was engaged to learn and classify the\nspectral mean vector features. The proposed method yielded\n76.40% accuracy on the Emo-DB and 70.80% accuracy on\nthe RA VDESS dataset.\nSingh et al. (2021) [6] presented an approach for speech\nemotion recognition using scattering transforms applied to\nspeech signals. The method incorporated diverse features,\nincluding frequency-domain scattering representation (F-\nSCATNET), time-domain scattering representation (SCAT-\nNET), and MFCCs. The classifier utilized was a radial basis\nfunction kernel-based SVM classifier. The F-SCATNET with\nSVM achieved a recognition rate of 74.59% on the Emo-\n2 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3360483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nDB, 51.81% on the RA VDESS dataset, and 61.55% on the\nIEMOCAP dataset.\nTuncer et al. (2021) [7] presented a novel approach\nfor speech emotion recognition. The method started with\nthe transformation of speech signals through a Tunable Q\nwavelet transform (TQWT) together with a twine shuffle\npattern feature generator (twine-shuf-pat) to extract pertinent\nfeatures. Subsequently, the features underwent a feature se-\nlection process via iterative neighborhood component anal-\nysis (INCA), concluding in a refined set of features. These\nfinal features were then passed to an SVM classifier for\nmodel training. The proposed model attained an impressive\naccuracy of 90.09% on the Emo-DB and 87.42% on the\nRA VDESS dataset.\nThirumuru et al. (2022) [8] introduced a novel repre-\nsentation, known as the single frequency filtered-nonlinear\nenergy cepstral coefficients (SFF-NEC) for speech emotion\nrecognition. This representation is constructed by employing\nthe nonlinear energy operator in conjunction with single fre-\nquency filtering on distinct frequency sub-bands. The SFF-\nNEC was then transformed into an identity vector (i-vector),\nmaking it a compact and low-dimensional representation. As\nfor the classification, three models were evaluated: the Gaus-\nsian probabilistic linear discriminant analysis (G-PLDA),\nSVM, and Random Forest. The i-vector integrated with the\nSVM classifier achieved accuracies of 85.75% and 65.78%\non the Emo-DB and IEMOCAP dataset, respectively.\nA hybrid Long Short-Term Memory (LSTM) combined\nwith a Transformer Encoder for speech emotion recognition\nwas proposed by Andayani et al. (2022) [9]. The proposed\nmodel used the MFCCs extracted from speech signals as the\nfeatures. As for the enhanced LSTM, the researchers replaced\nthe single attention layer within the LSTM architecture with\nthe multi-head attention mechanism inherent to the Trans-\nformer encoder. This adaptation improved the capability of\nthe model to learn intricate patterns and features from the\ninput data. The method achieved an accuracy of 85.55% on\nthe Emo-DB and 75.62% on the RA VDESS dataset.\nIn another work, Hason Rudd et al. (2022) [10] first\nconverted the speech signals into the mel spectrogram rep-\nresentation. Then, they applied a VGG16 model to extract\nfeature maps with various dimensions and signal sampling\nratios. The feature maps were subsequently fed into a multi-\nlayer perceptron (MLP) architecture for classification. The\nproposed method achieved an accuracy of 92.79% with a\nbandwidth of 128 Hz, a frame rate of 128 fps, and a sampling\nrate of 88200 KHz on the Emo-DB.\nKakuba & Han (2022) [11] presented a multi-head atten-\ntion machine with residual Bi-LSTM called (ResBLSTMA).\nThe work leveraged spectral and voice representations.\nSpectral representations encompassed crucial features like\nMFCCs and chromagrams, while voice representations in-\nvolved mel spectrograms. These features were then passed\nas feature vectors into the ResBLSTMA model, which ef-\nfectively leveraged its multi-head attention mechanism and\nresidual bidirectional long short-term memory for classifi-\ncation tasks. The proposed ResBLSTMA method achieved\nan accuracy of 90.57% on the Emo-DB and 84.50% on the\nRA VDESS dataset.\nIn the study by Singh et al. (2022) [12], the researchers\ninvestigated time-frequency-based features for speech emo-\ntion recognition. Three specific features were explored: mel-\nfrequency spectral coefficients (MFSC), constant-Q trans-\nform (CQT), and continuous wavelet transform (CWT).\nClassification was performed by a 2D-CNN with LSTM\narchitecture, referred to as Conv2D-LSTM. The proposed\nConv2D-LSTM with CQT features demonstrated accura-\ncies of 65.69%, 46.49%, and 54.83% on the Emo-DB,\nRA VDESS, and IEMOCAP datasets, respectively.\nVu et al. (2022) [13] applied principal component analysis\n(PCA) to pitch-related features for speech emotion recogni-\ntion. PCA was used to reduce the dimensionality of feature\nvectors, mitigating the learning complexity of the model.\nThe CQT spectrogram served as the input in this work.\nFor classification, an MLP with two dense neural network\nlayers activated by the ReLU function, followed by softmax\nlayers, was employed. The proposed MLP with CQT, exclud-\ning PCA, achieved the highest accuracy of 66.67% on the\nRA VDESS dataset and 57.09% on IEMOCAP.\nSekkate et al. (2023) [14] introduced a statistical-based\ntechnique for speech emotion recognition. The researchers\nfirst converted the speech signals into MFCC where the mean\nvalues of MFCC were calculated as features. Subsequently,\nthe MFCC features were subjected to a statistical distribution\nprocess to select pertinent features. The selected features\nwere then employed as input for a combination of three\nCNNs for model training. Each CNN contains three 1D-CNN\nlayers, followed by the max pooling and batch normaliza-\ntion layers. The decision scores from each classifier will be\nmerged to determine the final class label. The method was\nevaluated on the RA VDESS dataset and achieved an accuracy\nof 87.08%.\nMishra et al. (2023) [15] performed a comparative anal-\nysis of different features and classifiers in speech emotion\nrecognition. Their study extracted two features: MFCC and\nMFMC. These features were then employed for training and\ntesting of deep neural networks (DNN) and CNN classifiers.\nThe experimental results demonstrated that the MFMC fea-\ntures with DNN obtained the highest accuracy of 84.72% on\nthe Emo-DB and 76.72% on the RA VDESS dataset.\nRehman et al. (2023) [16] employed the syllable-level fea-\nture extraction for speech emotion recognition. The speech\nsignals were first transformed into mel-spectrograms, fol-\nlowed by segmentation into distinct syllable-level compo-\nnents. The syllable-level components were then encoded as\nthe statistical features. A SVM was utilized in classifica-\ntion. The proposed method yielded promising performance\nachieving an accuracy of 62.90% when evaluated on the\nIEMOCAP dataset.\nTu et al. (2023) [17] performed speech emotion recogni-\ntion using feature fusion and data augmentation techniques.\nInitially, the speech signals were augmented by randomly\nVOLUME 4, 2016 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3360483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\ncombining segments from various speech signals. The re-\nsulting augmented speech signals were then transformed into\nlog-mel spectrograms and high-level statistical features. Sub-\nsequently, a LightGBM classifier was employed for global\nfeature selection of the statistical features. A deep learning\nmodel incorporating the multi-head attention mechanism into\nthe CNN and LSTM, named MHA-CRNN, was proposed.\nThis MHA-CRNN was used to perform feature fusion and\nclassification of the log-mel spectrogram features and the\nselected statistical features. The proposed method achieved\nan accuracy of 66.44% on the IEMOCAP dataset.\nFeature fusion was also employed in Liu et al. (2023) [18],\nintegrating acoustic and pre-trained features. The speech sig-\nnals were segmented and encoded as acoustis features using\nthe OpenSmile [19] library. The study also incorporated\nvarious pre-trained features, such as self-supervised learning\nof Transformer encoder representation (Tera) [20], A Lite\nBERT for self-supervised learning of audio representation\n(Audio ALBERT) [21], non-autoregressive predictive coding\nfor learning speech representations (NPC) [22], unsuper-\nvised pre-training representations (Wav2Vec) [23], and self-\nsupervised learning of discrete speech representations (Vq-\nwav2vec) [24]. The authors proposed a Transformer-inspired\nmodel with attention mechanism, along with two 1D convo-\nlutional layers, two Transformer modules, and two BiLSTM\nmodules, forming the innovative Dual-TBNet architecture.\nThe Dual-TBNet achieved accuracies of 84.10% on the Emo-\nDB and 64.80% on the IEMOCAP dataset.\nOng et al. (2023) [25] engaged a lightweight gradient\nboosting machine (LightGBM) approach for speech emo-\ntion recognition, called Emo-LGBM. The researchers first\napplied pitch shifting and time stretching data augmentation\ntechniques to the input speech signals. Subsequently, both\ntime and frequency domain features were extracted from\nthe augmented signals as the input to the LightGBM. The\nproposed Emo-LGBM approach achieved an accuracy of\n84.91% on the Emo-DB, 67.72% on the RA VDESS dataset,\nand 62.94% on the IEMOCAP dataset.\nSingh et al. (2023) [26] introduced constant-Q based mod-\nulation spectral features (CQT-MSF) by combining CQT\nwith modulation spectral features (MSF). The author utilized\nCNN to extract feature embeddings from CQT-MSF and em-\nployed SVM to classify the embeddings into emotions. The\nresults highlight that the proposed DNN-SVM with CQT-\nMSF outperforms a single mel-scale-based spectrogram,\nachieving 79.86% accuracy on the Emo-DB and 52.24% on\nthe RA VDESS dataset. Table 1 presents the summary of\nrelated works.\nIII. SPEECH EMOTION RECOGNITION WITH MULTIAXIS\nAND MULTISCALE VISION TRANSFORMERS FUSION\nNETWORK\nThis paper presents a novel dual-path architecture for speech\nemotion recognition. In the first path, speech signals are\ntransformed into CQT spectrograms, which are subsequently\nprocessed by the MaxViT model for feature extraction and\nrepresentation learning. Simultaneously, the second path en-\ncodes speech signals into Mel-STFT spectrograms, which\nare then subjected to feature extraction using the MViTv2\nmodel. The feature maps generated by MaxViT and MViTv2\nare concatenated to create a comprehensive representation\nof the input data. This combined feature representation is\nthen subjected to classification using the MLP algorithm.\nThe dual-path architecture, which incorporates both CQT and\nMel-STFT spectrograms, leverages the strengths of Multiaxis\nand Multiscale Vision Transformers, resulting in improved\nperformance for speech emotion recognition.Figure 1 illus-\ntrates the framework of the proposed emo-MMViT.\nA. CONSTANT-Q TRANSFORM SPECTROGRAM (CQT)\nThe CQT spectrogram is a time-frequency representation\nused in audio signal processing. It is designed to closely\nmimic the frequency resolution of the human auditory sys-\ntem. This makes CQT particularly well-suited for speech\nemotion recognition, as it aligns with how the ears perceive\ndifferent frequencies. The process of creating a CQT spec-\ntrogram involves convolving the audio signal with a set of\ncomplex exponential functions. These functions are evenly\ndistributed in logarithmic frequency steps while maintaining\na consistent Q-factor (center frequency to bandwidth ratio)\nacross all bins. This results in a time-frequency representa-\ntion where each frequency bin corresponds to a specific Q\nvalue and center frequency. The CQT spectrogram captures\nthe variations in the frequency content of the audio signal\nover time, providing valuable insights into its tonal and\nharmonic components.\nMathematically, the CQT at a specific frequency, f and\ntime, t can be expressed as:\nCQT (f, t) =\nN−1X\nn=0\nx(n) · w∗(n − t\ns )e2πifn (1)\nwhere x(n) is the input signal, w represents the wavelet,\ns represents the scale factor that controls the width of the\nwavelet, and i is the imaginary unit. By varying the fre-\nquency, f, and time, t, the CQT generates a time-frequency\nrepresentation, where each entry corresponds to the energy\nof the signal at a particular frequency and time. This repre-\nsentation encapsulates both fine and broad frequency details,\nmaking it highly suitable for speech emotion recognition. An\nexample of the CQT spectrogram is depicted in Figure 2.\nB. MULTI-AXIS VISION TRANSFORMER (MAXVIT)\nThe CQT spectrograms serve as the input data for the\nMaxViT model [27] for further representation learning. The\nMaxViT model is designed in a hierarchical architecture,\nas depicted in Figure 3. The CQT spectrograms are down-\nsampled in the stem stage (S0), which is composed of two\nconvolutional layers with a 3×3 kernel size (Conv3×3). The\nbody of MaxViT comprises four stages (S1-S4), wherein\neach stage comprises a MaxViT block. Each MaxViT block\nconsists of a Mobile Inverted Residual Bottleneck Convolu-\n4 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3360483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 1. Summary of related works\nReference Feature Used Classification Methods\nWen et al. (2020) [1] MFCC, spectrogram, spectral features CapCNN\nMujaddidurrahman et al. (2021) [2] log-mel spectrogram 2D-CNN\nHe & Ren (2021) [3] 34 low-level descriptors CNN with BiLSTM\nAncilin & Milton (2021) [4] MFMC SVM\nPham et al. (2021) [5] MFCC, mel scaled spectrogram, chromagram, spectral\ncontrast feature, tonnetz representation\nCNN\nSingh et al. (2021) [6] F-SCATNET SVM\nTuncer et al. (2021) [7] TQWT SVM\nThirumuru et al. (2022) [8] SFF-NEC SVM\nAndayani et al. (2022) [9] MFCC LSTM with Transformer Encoder\nHason Rudd et al. (2022) [10] mel spectrogram MLP\nKakuba & Han (2022) [11] MFCC, chromagrams ResBLSTMA\nSingh et al. (2022) [12] CQT Conv2D-LSTM\nVu et al. (2022) [13] CQT MLP\nSekkate et al. (2023) [14] MFCC CNN\nMishra et al. (2023) [15] MFCC, MFMC DNN\nRehman et al. (2023) [16] mel-spectrograms SVM\nTu et al. (2023) [17] log-mel spectrograms, high-level statistical feature MHA-CRNN\nLiu et al. (2023) [18] acoustic features Dual-TBNet\nOng et al. (2023) [25] time and frequency domain features LightGBM\nSingh et al. (2023) [26] CQT-MSF DNN-SVM\nFIGURE 1. System flow of the proposed MaxMViT -MLP model.\ntion (MBConv) module, a block attention module, and a grid\nattention module.\nThe MBConv module begins with a 1×1 convolution. This\ninitial layer is responsible for dimensionality expansion. It\ntakes the input feature maps and projects them into a higher-\ndimensional space, typically with an expansion factor of 4.\nFollowing the expansion, the module applies Depthwise Con-\nvolution with a 3×3 kernel. Depthwise convolution operates\non each input channel separately and captures spatial features\nacross the input. Next, the module includes a Squeeze and\nExcitation (SE) mechanism. The SE mechanism is designed\nto adaptively recalibrate the importance of each channel in\nthe feature maps. It consists of two steps: a global average\npooling operation, which computes channel-wise statistics\nto capture the most informative features, and a set of fully\nconnected layers. These fully connected layers learn channel-\nwise scaling factors, allowing the network to emphasize\nessential features while suppressing less relevant ones. Fi-\nVOLUME 4, 2016 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3360483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 2. CQT spectrogram for the utterance ‘Kids are talking by the door’\nfrom the RAVDESS dataset.\nnally, the module concludes with another 1 ×1 convolution.\nThis layer reduces the dimensionality back to the original\nchannel dimension, achieving a shrink factor of 0.25. This\nreduction helps in managing computational complexity and\nmemory usage while preserving the essential features learned\nthroughout the module’s operations.\nThe block attention module operates by first partitioning\nthe input feature map into distinct windows. Specifically,\ngiven an input feature map X with dimensions H × W × C,\nwhere H represents height, W represents width, and C\nrepresents the number of channels, the block attention re-\nshapes this input into a tensor with dimensions of (H\nP ×\nW\nP , P× P, C), where P × P denotes the dimension of\neach block. This transformation results in the creation of\nnon-overlapping windows within the feature map, each of\nwhich is characterized by dimensions of P × P. Within each\nof these windows, self-attention mechanisms are applied to\ncapture interactions among the elements, fostering contextual\nunderstanding. Subsequently, a feedforward network (FFN)\nis employed to further process the information obtained\nfrom the block-based self-attention step. This FFN introduces\nnon-linear transformations to the representations within each\nwindow, enabling the model to capture intricate patterns and\nrelationships.\nOn the other hand, the grid attention module attends\nglobally to pixels through a sparse, evenly distributed grid\nspanning the entire 2D space. The grid attention mecha-\nnism is applied by reshaping the tensor into dimensions of\n(G × G, H\nG × W\nG , C), effectively gridding the feature maps\ninto G × G partitions. In this work, fixed window and grid\nsizes ( P = G = 7) are utilized. The output of the grid-\nbased self-attention step is also passed into an FFN for further\nrepresentation learning.\nIt is worth noting that the pre-normalized relative self-\nattention mechanism [28] is applied in the block attention\nand grid attention modules. The pre-normalized relative self-\nattention is a variant of self-attention that combines absolute\npositional encodings (standard positional encodings used in\nthe Transformer) and learned relative positional biases before\nsoftmax normalization. The learned biases allow the model\nto attend differently to tokens that are at different relative\ndistances from each other within the sequence. The pre-\nnormalized relative self-attention is defined as:\nypre\ni =\nX\nj∈G\nexp\n\u0000\nx⊤\ni xj + wi−j\n\u0001\nP\nk∈G exp\n\u0000\nx⊤\ni xk + wi−k\n\u0001xj (2)\nwhere for each token at position i, it computes a weighted\nsum of the embeddings of all other tokens in the global spatial\nspace G based on their pairwise relationships and learned\npositional biases wi−j. This mechanism captures how differ-\nent tokens interact and contribute to the representation of the\ntoken at position i in the context of the entire sequence.\nResidual connections are also integrated into the MaxViT\nblocks. These residual connections allow the module to learn\nresidual representations, which are essentially the differences\nbetween the original input and the output of the self-attention\nand feedforward operations. By adding these residuals back\nto the output, the model can effectively fine-tune the learned\nrepresentations and mitigate the challenges associated with\nvanishing gradients during training.\nC. MEL SPECTROGRAM USING SHORT TIME FOURIER\nTRANSFORM (MEL-STFT)\nThe Mel-STFT is another technique used in audio signal\nprocessing. It provides a visual representation of how the\nfrequency content of a signal changes over time, enabling\nthe analysis of the spectral characteristics of an audio signal.\nTo generate a Mel-STFT, a sequence of steps is undertaken.\nFirstly, the audio signal is divided into overlapping frames of\nfixed duration. For each frame, the Fast Fourier Transform\n(FFT) is applied, which transforms the signal from the time\ndomain to the frequency domain, providing a snapshot of\nits spectral components at that moment. The magnitude of\nthe FFT output represents the energy present in different\nfrequency bands.\nTo align the frequency representation, the linear frequency\nvalues obtained from the FFT are converted into the Mel scale\nusing the formula:\nM(f) = 2595× log10(1 + f\n700) (3)\nwhere M(f) is the frequency in Mel, and f is the linear\nfrequency in Hertz. This transformation accounts for the\nnonlinear way in perceiving frequencies. Following the Mel\nscale conversion, a set of Mel filterbanks is applied to the\ntransformed frequency values. Each filterbank is shaped as\na triangle on the Mel scale and is used to capture the en-\nergy in a specific frequency range. The filterbank outputs\nare computed by multiplying the Mel spectrum with each\ntriangular filterbank function. The logarithm of the energy\nvalues from the filterbanks is then calculated to approximate\nthe logarithmic perception of loudness. This yields the Mel\nSpectrogram, a time-frequency representation where each\n6 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3360483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 3. Model architecture of MaxViT.\npixel corresponds to the energy content of a specific fre-\nquency band within a given time frame. A frame length of\n4096 samples and a hop size of 256 samples are applied\nin this work. An example of the Mel-STFT spectrogram is\nillustrated in Figure 4.\nFIGURE 4. Mel-STFT spectrogram for the utterance ‘Kids are talking by the\ndoor’ from the RAVDESS dataset.\nD. IMPROVED MULTISCALE VISION TRANSFORMERS\n(MVITV2)\nThe Mel-STFT spectrograms are channeled into the MViTv2\nfor representation learning. In contrast to Vision Transform-\ners [29] characterized by a fixed channel capacity and spatial\nresolution across the network, Multiscale Vision Transform-\ners (MViT) [30] introduce multiple stages with different\nchannel-resolution scales. During the transition from input\nto output stages, MViT gradually expands the channel width\nwhile reducing resolution. Consequently, this construct forms\na multiscale pyramid of feature maps within the transformer\nnetwork. The initial layers can operate at high spatial resolu-\ntion, modeling low-level visual information due to the lighter\nchannel capacity. Conversely, deeper layers can focus on\nspatially coarse yet complex high-level features, effectively\nmodeling visual semantics. Figure 5 depicts the architecture\nof MViTv2.\nFIGURE 5. Model architecture of MViTv2.\nTo facilitate downsampling within a transformer block,\nMViT introduces Pooling Attention, a mechanism incorpo-\nrating linear projections and pooling operators for query (Q),\nkey (K), and value (V ) tensors. The operations is defined as:\nQ = PQ(XWQ)\nK = PK(XWK)\nV = PV (XWV )\nwhere X ∈ RL×D represents the input sequence with a\nsequence length of L and channel width of D, and WQ,\nWK, and WV are the linear projections for Q, K, and V\nVOLUME 4, 2016 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3360483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\ntensors, respectively. Additionally, PQ, PK, and PV denote\nthe pooling operators for Q, K, and V tensors. The attention\nin MViT is computed by:\nZ1 := Attn1(Q, K, V) = Softmax\n\u0010\nQK⊤/\n√\nD\n\u0011\nV (4)\nThe Improved Multiscale Vision Transformers (MViTv2)\n[31] proposes some enhancements over MViT. Firstly, MViT\nrelies solely on “absolute” positional embeddings to convey\nlocation information, which is less robust to shift-invariance.\nTo rectify this, MViTv2 introduces relative positional em-\nbeddings that capture the relative location between input\ntokens. The computation of relative positional embeddings\nRp(i),p(j) ∈ Rd of element i and j is decomposed as follows:\nRp(i),p(j) = Rh\nh(i),h(j) + Rw\nw(i),w(j) (5)\nwhere p(i) and p(j) are the spatial position of element\ni and j, Rh and Rw are the positional embeddings along\nthe height and width axes, and h(i), h(j), w(i), and w(j)\ndenote the vertical and horizontal positions of tokensi and j,\nrespectively. These positional embeddings are then integrated\ninto the self-attention module as follows:\nAttn2(Q, K, V) =Softmax\n\u0012QK⊤ + E(rel)\n√\nd\n\u0013\nV\nwhere E(rel)\nij = Qi · Rp(i),p(j)\nSecondly, MViTv2 employs a residual pooling connection\nwith the pooled Q tensor, enhancing information flow and\nreducing potential information loss during pooling attention.\nThis technique maintains low-complexity attention compu-\ntation with large strides in key ( K) and value ( V ) pooling,\nthus improving overall model efficiency. The residual pooling\nconnection can be formulated as:\nZ2 := Attn2(Q, K, V) +Q (6)\nThe improved pooling attention is illustrated in Figure 6.\nThe MViTv2 model integrates features from multiple scales,\nenabling the model to capture information at varying granu-\nlarity levels. Additionally, MViTv2 utilizes the decomposed\nrelative position embedding and residual pooling connection\nto preserve essential information at a lower computational\ncost.\nIn addition, the channel dimension expansion, previously\nlocated within the last MLP block of MViT’s preceding stage,\nhas been replaced with the attention computation embedded\nin the initial transformer block of each stage. This adjustment\nmaintains a comparable level of accuracy while significantly\nreducing the model’s parameter count and its floating point\noperations per second (FLOPs). Subsequently, the output\ntokens from the last transformer block are averaged, and the\nfinal classification head is employed, replacing the default\nclass token in MViT. These modifications have led to a\nshortened training time and a reduction in computational\nresource demands.\n1) Multilayer Perceptron (MLP)\nThe feature maps of MaxViT and MViTv2 are concatenated\nand directed into the MLP for representation learning and\nclassification. The MLP comprises a series of key compo-\nnents, including a dense layer, a batch normalization layer, a\ndropout layer, and a classification layer.\nThe dense layer plays a central role by applying a linear\ntransformation to the concatenated feature maps, enabling the\ncapture of intricate patterns within the data. Simultaneously,\nthe batch normalization layer normalizes the activations\nwithin mini-batches, enhancing training stability, expediting\nconvergence, and mitigating overfitting. To prevent overfit-\nting and encourage robust feature learning, the dropout layer\nselectively deactivates a fraction of neurons during training.\nIn the final stage, the classification layer computes the\nprobabilities associated with speech emotion classes through\nthe softmax function. This transformation converts raw\nscores into a probability distribution, resulting in the ultimate\nprediction. The dense layer in this study comprises 512 hid-\nden units, and a dropout rate of 0.2 is employed to achieve the\ndesired balance between feature learning and regularization.\nE. DATASETS\nThe proposed MaxMViT-MLP was evaluated on three pub-\nlicly available speech emotion datasets: the Berlin Database\nof Emotional Speech (Emo-DB), the Ryerson Audio-Visual\nDatabase of Emotional Speech and Song (RA VDESS), and\nthe Interactive Emotional Dyadic Motion Capture (IEMO-\nCAP).\nThe Emo-DB [27] comprises 535 audio samples collected\nfrom ten proficient German speakers, including five male\nand five female actors. These recordings encompass seven\ndistinct emotions: anger, boredom, neutrality, happiness, anx-\niety, sadness, and disgust.\nThe RA VDESS [32] dataset consists of a diverse col-\nlection of 1440 audio samples recorded in English. The\ndataset showcases the performances of 24 professional ac-\ntors, with 12 male and 12 female speakers. The samples\nin the RA VDESS dataset are categorized into eight classes:\nneutrality, calmness, happiness, sadness, anger, fear, disgust,\nand surprise.\nThe IEMOCAP [33] comprises 5507 English-language\naudio samples recorded by ten professional actors, with\nfive male and five female speakers. There are four emotion\nclasses commonly used by existing works, namely neutrality,\nhappiness, anger, and sadness.\nIV. EXPERIMENTS AND ANALYSIS\nIn the experiments, the datasets were divided into an 80%\ntraining set and a 20% testing set to facilitate a systematic\ncomparison with existing works. To ensure uniformity, the\ndata samples were resampled to a frequency of 44.1kHz.\nSubsequently, the samples underwent transformation into\nboth CQT and Mel-STFT spectrograms, which were then\nresized to 244 × 244 pixels in compliance with the input size\nrequirements of MaxViT and MViTv2.\n8 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3360483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 6. Pooling attention of MViT (left) and residual pooling attention of MViTv2 (right).\nA. HYPERPARAMETER TUNING\nHyperparameter tuning was conducted to determine the opti-\nmal settings for the proposed MaxMViT-MLP. The process\ninvolved tuning five hyperparameters: the optimizer ( O1)\nand learning rate ( R1) for MaxViT, the optimizer ( O2) and\nlearning rate ( R2) for MViTv2, and the number of hidden\nnodes in the MLP ( N). The grid search mechanism was\nemployed, where the experiments encompassed a range of\nvalues for each hyperparameter, allowing for a comprehen-\nsive examination of the settings. Table 2 provides a summary\nof the hyperparameter settings for MaxMViT-MLP.\nTABLE 2. Summary of hyperparameter settings for MaxMViT -MLP .\nHyperparameter Measured Values Optimal\nValue\nOptimizer (O1) Adam, RAdam, QHAdam Adam\nLearning Rate (R1) 0.01, 0.02, 0.03 0.02\nOptimizer (O2) Adam, RAdam, QHAdam RAdam\nLearning Rate (R2) 0.01, 0.02, 0.03 0.02\nHidden Nodes (N) 128, 256, 512, 1024 512\nThe results in Table 3 showcase the performance of\ndifferent optimizers on MaxViT, O1. Three optimizers,\nnamely Adam, Rectified Adam optimizer (RAdam), and\nQuasi-Hyperbolic Adam (QHAdam), were evaluated on\nthe datasets. The Adam optimizer demonstrated superior\nperformance with accuracy rates of 95.28%, 89.12%, and\n68.39% for Emo-DB, RA VDESS, and IEMOCAP, respec-\ntively. Adam is known for its adaptive learning rate mech-\nanism, combining the benefits of both momentum and root\nmean square propagation. This adaptability allows Adam to\nefficiently navigate complex optimization landscapes, con-\ntributing to its success in enhancing the MaxViT model’s\naccuracy. RAdam incorporates a rectified term in its adap-\ntive learning rate, enhancing robustness during training. Al-\nthough slightly outperformed by Adam, RAdam’s results\nunderscore its effectiveness in optimizing MaxViT. QHAdam\nintroduces quasi-hyperbolic terms to Adam’s optimization\nstrategy, striking a balance between stability and adaptability.\nDespite a lower accuracy than Adam, QHAdam’s perfor-\nmance highlights its relevance as a competitive optimizer for\nthe MaxViT model.\nTABLE 3. Experimental results of different optimizers of MaxViT,O1 [R1 =\n0.02, O2 = RAdam, R2 = 0.02, N = 512].\nOptimizer, O1\nAccuracy (%)\nEmo-DB RA VDESS IEMOCAP\nAdam 95.28 89.12 68.39\nRAdam 92.45 85.96 65.76\nQHAdam 90.57 88.77 67.48\nThe results displayed in Table 4 provide insights into the\nimpact of different learning rates on the performance of\nMaxViT. A learning rate of 0.02 emerges as the optimal\nchoice, yielding the highest accuracy rates for the datasets.\nThis finding suggests that the selected learning rate strikes\na balance between the model’s convergence speed and sta-\nbility during training. A learning rate that is too low may\nhinder convergence, while one that is too high can lead to\novershooting and instability. The superior performance at\nVOLUME 4, 2016 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3360483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nthe 0.02 learning rate indicates its effectiveness in guiding\nthe optimization process for MaxViT under the specified\nhyperparameter configuration. Comparatively, a learning rate\nof 0.01 produces slightly lower accuracy across the three\ndatasets, emphasizing the sensitivity of the model to varia-\ntions in the learning rate. Meanwhile, a learning rate of 0.03\nexhibits a similar trend, suggesting that higher learning rates\nmay lead to suboptimal convergence.\nTABLE 4. Experimental results of different learning rates of MaxViT,R1 [O1 =\nAdam, O2 = RAdam, R2 = 0.02, N = 512].\nLearning Rate, R1\nAccuracy (%)\nEmo-DB RA VDESS IEMOCAP\n0.01 92.45 85.96 63.85\n0.02 95.28 89.12 68.39\n0.03 93.40 87.72 63.49\nTable 5 presents experimental results showcasing the\nimpact of different optimizers on the performance of\nMaxMViT-MLP. The incorporation of RAdam in MViTv2\nachieves the highest accuracy rates on the datasets, demon-\nstrating superior performance compared to both Adam and\nQHAdam. RAdam enhances the standard Adam optimizer by\nintroducing a rectification term in its adaptive learning rate\nmechanism. This modification contributes to the optimizer’s\nstability during training, preventing potential convergence\nissues and improving overall optimization efficiency.\nTABLE 5. Experimental results of different optimizers of MViTv2, O2 [O1 =\nAdam, R1 = 0.02, R2 = 0.02, N = 512].\nOptimizer, O2\nAccuracy (%)\nEmo-DB RA VDESS IEMOCAP\nAdam 93.40 88.42 67.30\nRAdam 95.28 89.12 68.39\nQHAdam 94.34 86.32 60.67\nTable 6 displays experimental results elucidating the in-\nfluence of different learning rates on the performance of\nMViTv2. A learning rate of 0.02 emerges as the most ef-\nfective choice, yielding the highest accuracy rates across the\ndatasets. This finding suggests that the specified learning\nrate strikes a delicate balance, allowing for a harmonious\nconvergence of the optimization process.\nTABLE 6. Experimental results of different learning rates of MViTv2, R2 [O1\n= Adam, R1 = 0.02, O2 = RAdam, N = 512].\nLearning Rate, R2\nAccuracy (%)\nEmo-DB RA VDESS IEMOCAP\n0.01 92.45 87.72 67.39\n0.02 95.28 89.12 68.39\n0.03 91.51 84.91 62.13\nThe experimental findings in Table 7 highlight the critical\nrole of selecting an optimal number of hidden nodes of\nMLP. Among the tested configurations, the number of hidden\nnodes set at 512 emerges as the optimal choice. This finding\nunderscores the impact of the hidden layer’s capacity on the\nmodel’s ability to capture complex patterns within the data.\nA hidden layer with 512 nodes strikes an effective balance,\nproviding sufficient representational capacity without intro-\nducing excessive complexity that might lead to overfitting.\nTABLE 7. Experimental results of different hidden nodes of MLP ,N [O1 =\nAdam, R1 = 0.02, O2 = RAdam, R2 = 0.02].\nHidden Nodes, N Accuracy (%)\nEmo-DB RA VDESS IEMOCAP\n128 84.91 88.07 63.67\n256 93.40 88.42 64.03\n512 95.28 89.12 68.39\n1024 94.34 87.37 64.21\nB. ABLATION STUDY\nThe ablation study presented in Table 8 systematically ex-\nplores the impact of different configurations on the perfor-\nmance of the proposed MaxMViT-MLP model across the\nEmo-DB, RA VDESS, and IEMOCAP datasets. Two spectro-\ngram representations, CQT and Mel-STFT, are individually\ncombined with two transformer architectures, MaxViT and\nMViTv2, to assess their standalone effectiveness.\nFirstly, employing CQT with MaxViT yields an accu-\nracy of 85.85% on Emo-DB, 77.54% on RA VDESS, and\n62.49% on IEMOCAP. Similarly, Mel-STFT with MViTv2\nproduces competitive results with accuracy rates of 88.68%,\n77.89%, and 62.85% across the three datasets. These specific\ncombinations are chosen based on their initial promise and\nindividual strengths.\nFurthermore, combining both CQT + MaxViT and Mel-\nSTFT + MViTv2 leads to a notable improvement, achieving\naccuracy rates of 91.51%, 84.91%, and 67.85%. This amal-\ngamation leverages the complementary features of CQT and\nMel-STFT, enhancing the model’s ability to capture diverse\nspectral characteristics present in speech signals.\nFinally, the addition of the MLP layer to the amalgamated\nconfiguration (CQT + MaxViT + Mel-STFT + MViTv2 +\nMLP) results in the best performance, reaching accuracy\nrates of 95.28%, 89.12%, and 68.39% across the three\ndatasets. The MLP introduces non-linearity and further re-\nfines the model’s representation capabilities, demonstrating\nits crucial role in achieving optimal performance in the\ncontext of speech emotion recognition. The stepwise pro-\ngression in accuracy highlights the cumulative improvement\nobtained by incorporating diverse components, underscoring\nthe effectiveness of the proposed MaxMViT-MLP model.\nC. COMPARISON RESULTS WITH EXISTING METHODS\nTable 9 presents the performance of various methods for\nspeech emotion recognition on the Emo-DB. Traditional\nmethods often rely on handcrafted feature extraction and\n10 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3360483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 8. Ablation study across the Emo-DB, RAVDESS, and IEMOCAP\ndatasets.\nConfiguration Accuracy (%)\nEmo-DB RA VDESS IEMOCAP\nCQT + MaxViT 85.85 77.54 62.49\nCQT + MViTv2 78.30 73.33 61.76\nMel-STFT + MaxViT 81.13 75.79 62.13\nMel-STFT + MViTv2 88.68 77.89 62.85\nCQT + MaxViT + Mel-\nSTFT + MViTv2\n91.51 84.91 67.85\nCQT + MaxViT + Mel-\nSTFT + MViTv2 + MLP\n95.28 89.12 68.39\nmachine learning models. For instance, MFCC with SVM\nattains a modest accuracy of 58.39%, indicating the lim-\nitations of traditional feature-based methods in this task.\ni-vector techniques, such as i-vector with SVM (85.75%)\nand i-vector with PLDA (82.84%), display more competitive\nresults, highlighting the importance of feature engineering\nin achieving accurate emotion recognition. However, these\ntraditional methods are outperformed by the deep learning\nmodels in this evaluation.\nDeep learning methods, including models like CNN-\nVGG16 and Hybrid LSTM, demonstrate their effectiveness\nin capturing emotional cues from audio data. CNN-VGG16\nachieves an impressive accuracy of 92.79%, emphasizing\nthe power of deep neural networks in extracting relevant\nemotional features. Moreover, the proposed MaxMViT-MLP\nmodel emerges as the top performer, setting a new benchmark\nwith an accuracy of 95.28%.\nMoving to the RA VDESS dataset, a comparison of results\nwith existing methods is presented in Table 10. Traditional\nmethods, as evidenced by the results, yield a mixed bag\nof outcomes. MFCC with SVM delivers an accuracy of\n36.74%, underscoring the complexities encountered when\napplying traditional feature-based approaches to this dataset.\nSimilarly, F-ScatNet with SVM (51.81%) and ScatNet with\nSVM (50.00%) produce suboptimal results, underscoring the\nlimitations of classical signal processing techniques when\ndealing with RA VDESS data.\nIn contrast, deep learning models exhibit a more robust\nperformance in addressing the intricacies of the RA VDESS\ndataset. Models like ResBLSTMA (84.50%) and ResBLSTM\n(85.41%) showcase strong performance, highlighting the\neffectiveness of recurrent neural networks in this specific\ncontext. Furthermore, the CNN model (87.08%) and MFMC\nwith DNN (76.72%) yield competitive results, emphasizing\nthe adaptability of deep learning architectures in the domain\nof emotion recognition when dealing with the RA VDESS\ndataset.\nThe proposed MaxMViT-MLP method maintains its ex-\nceptional performance, achieving an accuracy of 89.12%\non the RA VDESS dataset. This reinforces the model’s ca-\npacity to generalize effectively across different datasets and\nTABLE 9. Comparison results on the Emo-DB.\nMethods Accuracy (%)\nCapCNN [1] 82.90\n2D-CNN [2] 88.00\nCNN-BiLSTM [3] 86.87\nMFMC with SVM [4] 81.50\nCNN [5] 76.40\nF-ScatNet with SVM [6] 74.59\nScatNet with SVM [6] 74.40\nMFCC with SVM [6] 58.39\nINCA with SVM [7] 90.09\ni-vector with SVM [8] 85.75\ni-vector with PLDA [8] 82.84\ni-vector with RF [8] 82.10\nHybrid LSTM [9] 85.55\nCNN-VGG16 [10] 92.79\nResBLSTMA [11] 90.57\nResBLSTM [11] 79.25\nConv2D-LSTM [12] 65.69\nMFMC with DNN [15] 84.72\nMFMC with CNN [15] 82.41\nMFCC with DNN [15] 82.24\nMFCC with CNN [15] 81.31\nDual-TBNet [18] 84.10\nEmo-LGBM [25] 84.91\nDNN-SVM [26] 79.86\nMaxMViT-MLP (Proposed) 95.28\nunderscores its efficacy as a versatile solution for emotion\nrecognition, regardless of the distinctive characteristics of the\ndataset at hand.\nThe performance of various emotion recognition methods\non the IEMOCAP dataset is presented in Table 11. In the con-\ntext of the IEMOCAP dataset, traditional approaches, such as\nMFCC with SVM (55.54%), ScatNet with SVM (60.41%),\nF-ScatNet with SVM (61.55%), Emo-LGBM (62.94%), i-\nvector with RF (63.57%), i-vector with PLDA (64.52%),\nand i-vector with SVM (65.78%), exhibit moderate accuracy.\nThese results highlight the challenges of utilizing traditional\nfeature-based approaches for emotion recognition on the\nIEMOCAP dataset, which contains complex and dynamic\nemotional expressions. Likewise, the deep learning models,\nsuch as Fusion features with MHA-CRNN (66.44%) and\nDual-TBNet (64.80%), demonstrate moderate accuracy. Fu-\nsion features with MHA-CRNN method achieves the high-\nest accuracy among the existing methods, emphasizing the\nstrength of combining multiple modalities and context-aware\nmodeling. The proposed MaxMViT-MLP model stands out\nas the top-performing method, setting a new benchmark with\nan accuracy of 68.39%.\nThe proposed MaxMViT-MLP model harnesses the\nstrengths of two audio feature representation techniques:\nVOLUME 4, 2016 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3360483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 10. Comparison results on the RAVDESS dataset.\nMethods Accuracy (%)\nMFMC with SVM [4] 64.31\nCNN [5] 70.80\nF-ScatNet with SVM [6] 51.81\nScatNet with SVM [6] 50.00\nMFCC with SVM [6] 36.74\nINCA with SVM [7] 87.42\nHybrid LSTM [9] 75.62\nResBLSTMA [11] 84.50\nResBLSTM [11] 85.41\nConv2D-LSTM [12] 46.69\nMLP [13] 66.67\nCNN [14] 87.08\nMFMC with DNN [15] 76.72\nMFMC with CNN [15] 72.90\nMFCC with DNN [15] 73.26\nMFCC with CNN [15] 68.54\nEmo-LGBM [25] 67.72\nDNN-SVM [26] 52.24\nMaxMViT-MLP (Proposed) 89.12\nTABLE 11. Comparison results on the IEMOCAP dataset.\nMethods Accuracy (%)\nF-ScatNet with SVM [6] 61.55\nScatNet with SVM [6] 60.41\nMFCC with SVM [6] 55.54\ni-vector with SVM [8] 65.78\ni-vector with PLDA [8] 64.52\ni-vector with RF [8] 63.57\nConv2D-LSTM [12] 54.83\nMLP [13] 57.09\nsyl-SVM [16] 62.90\nFusion features with MHA-CRNN [17] 66.44\nDual-TBNet [18] 64.80\nEmo-LGBM [25] 62.94\nMaxMViT-MLP (Proposed) 68.39\nCQT and Mel-STFT. CQT excels in capturing tonal charac-\nteristics through its logarithmic frequency binning, closely\nmimicking human perception of pitch and timbre. The\nconstant-Q resolution of CQT provides detailed informa-\ntion in the low-frequency range, making it well-suited for\ntasks that demand harmonic analysis. In contrast, Mel-STFT\nemploys a linear frequency scale, enabling the capture of\nbroader spectral features that align with human auditory\ninterpretation. This approach yields rich time-frequency rep-\nresentations, particularly valuable in tasks involving transient\nevents, such as speech recognition and environmental sound\nclassification. By combining CQT and Mel-STFT, the pro-\nposed model assembles a comprehensive set of features for\naudio analysis, capitalizing on their complementary abilities\nto capture diverse aspects of audio signals.\nAs for the feature extraction, the MaxMViT-MLP model\nemploys two Vision Transformers: MaxViT and MViTv2.\nThe hierarchical architecture of MaxViT effectively captures\nmultiscale information, utilizing advanced features like the\nblock attention module to foster contextual understanding,\nmaking it proficient in recognizing intricate audio patterns\nand relationships within CQT spectrograms. In contrast,\nMViTv2 provides a range of multiscale features with dif-\nferent channel-resolution scales. Its incorporation of relative\npositional embeddings enhances its understanding of spatial\nrelationships, rendering it a suitable choice for the analysis\nof Mel-STFT spectrograms. The subsequent Multilayer Per-\nceptron (MLP) further enhances the model’s representation\nlearning capabilities.\nFigure 7 presents the confusion matrix for the MaxMViT-\nMLP method applied to the Emo-DB dataset. Notably, the\nanger, disgust, and neutral classes achieve impeccable clas-\nsification accuracy, while occasional misclassifications occur\nbetween the boredom and happiness classes. Figure 8 dis-\nplays the confusion matrix for the MaxMViT-MLP method\non the RA VDESS dataset. In RA VDESS, high misclassi-\nfication rates are evident in the disgust, sadness, and fear\nclasses. These challenges arise from the intra-class acoustic\nsimilarities and variability in speech emotion expressions\namong different speakers.\nFIGURE 7. Confusion matrix of the Emo-DB.\nFor the IEMOCAP dataset, the confusion matrix in Fig-\nure 9 introduces additional complexities due to the pres-\nence of dual speakers in conversations. The analysis of the\nconfusion matrix reveals that the happiness and sadness\nclasses exhibit the highest misclassification rates. This may\nbe attributed to instances where individuals express multiple\nemotions simultaneously or swiftly transition between emo-\n12 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3360483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 8. Confusion matrix of the RAVDESS dataset.\ntions, making it challenging for the model to assign a single,\naccurate label.\nFIGURE 9. Confusion matrix of the IEMOCAP dataset.\nV. CONCLUSION\nThis paper introduces a dual-path speech emotion recogni-\ntion model, referred to as the “MaxMViT-MLP”, capitalizing\non two potent audio feature representations, CQT and Mel-\nSTFT. CQT’s proficiency in capturing tonal characteristics\nand its suitability for harmonic analysis complements Mel-\nSTFT’s ability to record broader spectral features, ideal for\ntransient event analysis. The fusion of these representations\nresults in a versatile feature set, contributing significantly to\nthe model’s performance.\nThe adoption of MaxViT and MViTv2, further enhances\nthe model’s capacity to process CQT and Mel-STFT spectro-\ngrams. MaxViT effectively captures multiscale information\nand intricate patterns within audio data, while MViTv2 ex-\ncels in handling multiscale features with different channel-\nresolution scales. This collaborative approach between the\nmodels plays a pivotal role in the model’s robustness and\nadaptability. The experimental results across three datasets:\nEmo-DB, RA VDESS, and IEMOCAP, affirm the outstanding\nperformance of the MaxMViT-MLP model, achieving accu-\nracy rates of 95.28% on Emo-DB, 89.12% on RA VDESS,\nand 68.39% on IEMOCAP. The harmonious blend of CQT\nand Mel-STFT, coupled with the strengths of MaxViT and\nMViTv2, underpins the model’s remarkable results.\nLooking ahead, future work could explore alternative\ntransformer-based models that might offer valuable insights\nto enhance performance in speech emotion recognition.\nContinuous efforts in refining and extending the proposed\nMaxMViT-MLP model will contribute to advancing the field\nof speech emotion recognition and its broader applications.\nAdditionally, investigating the effectiveness of the proposed\nmodel in real-world scenarios and diverse cultural contexts\nwould contribute to its practical applicability.\nREFERENCES\n[1] Xin-Cheng Wen, Kun-Hong Liu, Wei-Ming Zhang, and Kai Jiang. The\napplication of capsule neural network based cnn for speech emotion\nrecognition. In 2020 25th International Conference on Pattern Recognition\n(ICPR), pages 9356–9362. IEEE, 2021.\n[2] Auliya Mujaddidurrahman, Ferda Ernawan, Adi Wibowo, Eko Adi Sar-\nwoko, Aris Sugiharto, and Muhammad Didik Rohmad Wahyudi. Speech\nemotion recognition using 2d-cnn with data augmentation. In 2021 Inter-\nnational Conference on Software Engineering & Computer Systems and\n4th International Conference on Computational Science and Information\nManagement (ICSECS-ICOCSIM), pages 685–689. IEEE, 2021.\n[3] Jingru He and Liyong Ren. Speech emotion recognition using xgboost\nand cnn blstm with attention. In 2021 IEEE SmartWorld, Ubiquitous\nIntelligence & Computing, Advanced & Trusted Computing, Scalable\nComputing & Communications, Internet of People and Smart City Innova-\ntion (SmartWorld/SCALCOM/UIC/ATC/IOP/SCI), pages 154–159. IEEE,\n2021.\n[4] J Ancilin and A Milton. Improved speech emotion recognition with mel\nfrequency magnitude coefficient. Applied Acoustics, 179:108046, 2021.\n[5] Minh H Pham, Farzan M Noori, and Jim Torresen. Emotion recognition\nusing speech data with convolutional neural network. In 2021 IEEE 2nd\nInternational Conference on Signal, Control and Communication (SCC),\npages 182–187. IEEE, 2021.\n[6] P Singh, G Saha, and M Sahidullah. Deep scattering network for speech\nemotion recognition. arxiv 2021. arXiv preprint arXiv:2105.04806, 2022.\n[7] Turker Tuncer, Sengul Dogan, and U Rajendra Acharya. Automated\naccurate speech emotion recognition system using twine shuffle pattern\nand iterative neighborhood component analysis techniques. Knowledge-\nBased Systems, 211:106547, 2021.\n[8] Ramakrishna Thirumuru, Krishna Gurugubelli, and Anil Kumar Vuppala.\nNovel feature representation using single frequency filtering and nonlinear\nenergy operator for speech emotion recognition. Digital Signal Processing,\n120:103293, 2022.\n[9] Felicia Andayani, Lau Bee Theng, Mark Teekit Tsun, and Caslon Chua.\nHybrid lstm-transformer model for emotion recognition from speech audio\nfiles. IEEE Access, 10:36018–36027, 2022.\n[10] David Hason Rudd, Huan Huo, and Guandong Xu. Leveraged mel spec-\ntrograms using harmonic and percussive components in speech emotion\nrecognition. In Pacific-Asia Conference on Knowledge Discovery and\nData Mining, pages 392–404. Springer, 2022.\nVOLUME 4, 2016 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3360483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n[11] Samuel Kakuba and Dong Seog Han. Residual bidirectional lstm with\nmulti-head attention for speech emotion recognition. , pages 1419–1421,\n2022.\n[12] Premjeet Singh, Shefali Waldekar, Md Sahidullah, and Goutam Saha.\nAnalysis of constant-q filterbank based representations for speech emotion\nrecognition. Digital Signal Processing, 130:103712, 2022.\n[13] Linh Vu, Raphaël C-W Phan, Lim Wern Han, and Dinh Phung. Improved\nspeech emotion recognition based on music-related audio features. In 2022\n30th European Signal Processing Conference (EUSIPCO), pages 120–124.\nIEEE, 2022.\n[14] Sara Sekkate, Mohammed Khalil, and Abdellah Adib. A statistical feature\nextraction for deep speech emotion recognition in a bilingual scenario.\nMultimedia Tools and Applications, 82(8):11443–11460, 2023.\n[15] Siba Prasad Mishra, Pankaj Warule, and Suman Deb. Deep learning based\nemotion classification using mel frequency magnitude coefficient. In 2023\n1st International Conference on Innovations in High Speed Communica-\ntion and Signal Processing (IHCSP), pages 93–98. IEEE, 2023.\n[16] Abdul Rehman, Zhen-Tao Liu, Min Wu, Wei-Hua Cao, and Cheng-\nShan Jiang. Speech emotion recognition based on syllable-level feature\nextraction. Applied Acoustics, 211:109444, 2023.\n[17] Zhongwen Tu, Bin Liu, Wei Zhao, Raoxin Yan, and Yang Zou. A feature\nfusion model with data augmentation for speech emotion recognition.\nApplied Sciences, 13(7):4124, 2023.\n[18] Zheng Liu, Xin Kang, and Fuji Ren. Dual-tbnet: Improving the robust-\nness of speech features via dual-transformer-bilstm for speech emotion\nrecognition. IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, 2023.\n[19] Florian Eyben, Martin Wöllmer, and Björn Schuller. Opensmile: the mu-\nnich versatile and fast open-source audio feature extractor. In Proceedings\nof the 18th ACM international conference on Multimedia, pages 1459–\n1462, 2010.\n[20] Andy T Liu, Shang-Wen Li, and Hung-yi Lee. Tera: Self-supervised\nlearning of transformer encoder representation for speech. IEEE/ACM\nTransactions on Audio, Speech, and Language Processing, 29:2351–2366,\n2021.\n[21] Po-Han Chi, Pei-Hung Chung, Tsung-Han Wu, Chun-Cheng Hsieh, Yen-\nHao Chen, Shang-Wen Li, and Hung-yi Lee. Audio albert: A lite bert for\nself-supervised learning of audio representation. In 2021 IEEE Spoken\nLanguage Technology Workshop (SLT), pages 344–350. IEEE, 2021.\n[22] Alexander H Liu, Yu-An Chung, and James Glass. Non-autoregressive\npredictive coding for learning speech representations from local depen-\ndencies. arXiv preprint arXiv:2011.00406, 2020.\n[23] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli.\nwav2vec: Unsupervised pre-training for speech recognition. arXiv preprint\narXiv:1904.05862, 2019.\n[24] Alexei Baevski, Steffen Schneider, and Michael Auli. vq-wav2vec: Self-\nsupervised learning of discrete speech representations. arXiv preprint\narXiv:1910.05453, 2019.\n[25] Kah Liang Ong, Chin Poo Lee, Heng Siong Lim, and Kian Ming Lim.\nSpeech emotion recognition with light gradient boosting decision trees\nmachine. International Journal of Electrical and Computer Engineering\n(IJECE), 13(4):4020, 2023.\n[26] Premjeet Singh, Md Sahidullah, and Goutam Saha. Modulation spectral\nfeatures for speech emotion recognition using deep neural networks.\nSpeech Communication, 146:53–69, 2023.\n[27] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milan-\nfar, Alan Bovik, and Yinxiao Li. Maxvit: Multi-axis vision transformer. In\nEuropean conference on computer vision, pages 459–479. Springer, 2022.\n[28] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet:\nMarrying convolution and attention for all data sizes. Advances in neural\ninformation processing systems, 34:3965–3977, 2021.\n[29] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weis-\nsenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[30] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan,\nJitendra Malik, and Christoph Feichtenhofer. Multiscale vision trans-\nformers. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 6824–6835, 2021.\n[31] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong,\nJitendra Malik, and Christoph Feichtenhofer. Mvitv2: Improved multiscale\nvision transformers for classification and detection. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 4804–4814, 2022.\n[32] Felix Burkhardt, Astrid Paeschke, Miriam Rolfes, Walter F Sendlmeier,\nBenjamin Weiss, et al. A database of german emotional speech. In\nInterspeech, volume 5, pages 1517–1520, 2005.\n[33] Steven R Livingstone and Frank A Russo. The ryerson audio-visual\ndatabase of emotional speech and song (ravdess): A dynamic, multimodal\nset of facial and vocal expressions in north american english. PloS one,\n13(5):e0196391, 2018.\nKAH LIANG ONG received his Bachelor’s De-\ngree in Information Technology (Hons.) Arti-\nficial Intelligence from Multimedia University,\nMalaysia, in 2021. Currently, he is a full-time\nMaster’s student and his current research inter-\nest is speech emotion recognition which mainly\ninvolves audio pre-processing, feature extraction,\nand emotion classification.\nCHIN POO LEE is an Associate Professor in the\nFaculty of Information Science and Technology\nat Multimedia University, Malaysia. She earned\nher Master of Science and Ph.D. in Information\nTechnology, specializing in abnormal behavior de-\ntection and gait recognition. She holds the status\nof a certified Professional Technologist and IEEE\nSenior Membership. She has been a member of the\nInternational Association of Engineers and serves\nas an Outcome-Based Education Consultant and\nTrainer. Her research interests encompass action recognition, computer\nvision, gait recognition, natural language processing, and deep learning.\nHENG SIONG LIM received his BEng (Hons)\nDegree in Electrical Engineering from Univer-\nsiti Teknologi Malaysia in 1999. He obtained his\nMEngSc and PhD in Engineering focusing on sig-\nnal processing for wireless communications from\nMultimedia University in 2002 and 2008 respec-\ntively. He is currently a Professor of Faculty of\nEngineering and Technology, Multimedia Univer-\nsity. His current research interests are in the areas\nof signal processing for advanced communication\nsystems, with emphasis on detection and estimation theory as well as their\napplications.\nKIAN MING LIM received B.IT (Hons) in Infor-\nmation Systems Engineering, Master of Engineer-\ning Science (MEngSc) and Ph.D. (I.T.) degrees\nfrom Multimedia University. He is currently an\nAssociate Professor with the Faculty of Informa-\ntion Science and Technology, Multimedia Univer-\nsity. His research and teaching interests includes\nmachine learning, deep learning, and computer\nvision and pattern recognition.\n14 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3360483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nALI ALQAHTANI received the Ph.D. degree\nin computer science from Swansea University,\nSwansea, U.K., in 2021. He is currently an Assis-\ntant Professor with the Department of Computer\nScience, King Khalid University, Abha, Saudi\nArabia. He has published several refereed confer-\nence and journal publications. His research inter-\nests include aspects of pattern recognition, deep\nlearning and machine intelligence and their appli-\ncations to real-world problems.\nVOLUME 4, 2016 15\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3360483\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Spectrogram",
  "concepts": [
    {
      "name": "Spectrogram",
      "score": 0.8743698000907898
    },
    {
      "name": "Computer science",
      "score": 0.7785896062850952
    },
    {
      "name": "Artificial intelligence",
      "score": 0.677611768245697
    },
    {
      "name": "Short-time Fourier transform",
      "score": 0.6587575674057007
    },
    {
      "name": "Multilayer perceptron",
      "score": 0.629338264465332
    },
    {
      "name": "Speech recognition",
      "score": 0.5961748957633972
    },
    {
      "name": "Transformer",
      "score": 0.5490541458129883
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5372934341430664
    },
    {
      "name": "Feature extraction",
      "score": 0.506255567073822
    },
    {
      "name": "Artificial neural network",
      "score": 0.4126422703266144
    },
    {
      "name": "Computer vision",
      "score": 0.4075203537940979
    },
    {
      "name": "Fourier transform",
      "score": 0.35075730085372925
    },
    {
      "name": "Engineering",
      "score": 0.12765058875083923
    },
    {
      "name": "Fourier analysis",
      "score": 0.11603868007659912
    },
    {
      "name": "Mathematics",
      "score": 0.0732429027557373
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I173029219",
      "name": "Multimedia University",
      "country": "MY"
    },
    {
      "id": "https://openalex.org/I82952536",
      "name": "King Khalid University",
      "country": "SA"
    }
  ],
  "cited_by": 12
}