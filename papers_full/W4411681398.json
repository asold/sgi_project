{
  "title": "A guide to evade hallucinations and maintain reliability when using large language models for medical research: a narrative review",
  "url": "https://openalex.org/W4411681398",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2149346067",
      "name": "Sangzin Ahn",
      "affiliations": [
        "Inje University"
      ]
    },
    {
      "id": "https://openalex.org/A2149346067",
      "name": "Sangzin Ahn",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4399598212",
    "https://openalex.org/W4392757369",
    "https://openalex.org/W4385294579",
    "https://openalex.org/W4403618835",
    "https://openalex.org/W4399465031",
    "https://openalex.org/W4399597451",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4403839497",
    "https://openalex.org/W4403795293",
    "https://openalex.org/W4399738410",
    "https://openalex.org/W4402025438",
    "https://openalex.org/W4402952666"
  ],
  "abstract": "Large language models (LLMs) are increasingly prevalent in medical research; however, fundamental limitations in their architecture create inherent reliability challenges, particularly in specialized medical contexts. These limitations stem from autoregressive prediction mechanisms and computational constraints related to undecidability, hindering perfect accuracy. Current mitigation strategies include advanced prompting techniques such as Chain-of-Thought reasoning and Retrieval-Augmented Generation (RAG) frameworks, although these approaches are insufficient to eliminate the core reliability issues. Meta-analyses of human-artificial intelligence collaboration experiments revealed that, although LLMs can augment individual human capabilities, they are most effective in specific contexts allowing human verification. Successful integration of LLMs in medical research requires careful tool selection aligned with task requirements and appropriate verification mechanisms. Evolution of the field indicates a balanced approach combining technological innovation with established expertise, emphasizing human oversight particularly in complex biological systems. This review highlights the importance of understanding the technical limitations of LLMs while maximizing their potential through thoughtful application and rigorous verification processes, ensuring high standards of scientific integrity in medical research.",
  "full_text": "Review article\nLarge language models (LLMs) are increasingly prevalent in medical research; \nhowever, fundamental limitations in their architecture create inherent reliability \nchallenges, particularly in specialized medical contexts. These limitations stem \nfrom autoregressive prediction mechanisms and computational constraints \nrelated to undecidability, hindering perfect accuracy. Current mitigation strategies \ninclude advanced prompting techniques such as Chain-of-Thought reasoning and \nRetrieval-Augmented Generation (RAG) frameworks, although these approaches are \ninsufficient to eliminate the core reliability issues. Meta-analyses of human-artificial \nintelligence collaboration experiments revealed that, although LLMs can augment \nindividual human capabilities, they are most effective in specific contexts allowing \nhuman verification. Successful integration of LLMs in medical research requires \ncareful tool selection aligned with task requirements and appropriate verification \nmechanisms. Evolution of the field indicates a balanced approach combining \ntechnological innovation with established expertise, emphasizing human oversight \nparticularly in complex biological systems. This review highlights the importance of \nunderstanding the technical limitations of LLMs while maximizing their potential \nthrough thoughtful application and rigorous verification processes, ensuring high \nstandards of scientific integrity in medical research.\nKeywords: Medical writing, Scholarly communication, Reliability, Natural language \nprocessing, Artificial intelligence\n©2025 Annals of Pediatric Endocrinology & Metabolism\nThis is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://\ncreativecommons.org/licenses/by-nc/4.0) which permits unrestricted non-commercial use, distribution, and reproduction in any \nmedium, provided the original work is properly cited.\nISSN: 2287-1012(Print) \nISSN: 2287-1292(Online) \nHighlights\n·  Large language models (LLMs) inherently generate inaccuracies due to autoregressive \narchitecture and computational limits, posing challenges in medical research.\n·  Advanced prompting and Retrieval-Augmented Generation (RAG) can partially mitigate \nhallucinations but require human oversight.\n·  Effective LLM use in medical research demands task-specific tool selection paired with \nrigorous human validation for complex tasks.\nIntroduction\nLarge language models (LLMs) are increasingly being adopted in academic writing, \npotentially changing the patterns of scholarly communication. In a recent report, at least 10% \nof PubMed abstracts published during the first half of 2024 showed signs of LLM usage when \nanalyzing vocabulary patterns [1]. Some examples include \"meticulously delving\" and \"intricate \ninterplay,\" which are highly flowery words that were not historically popular in scientific \nabstracts. This high prevalence raises important questions regarding reliability and appropriate \nA guide to evade hallucinations and maintain \nreliability when using large language models for \nmedical research: a narrative review\nSangzin Ahn\n1,2\n1\nDepartment of Pharmacology and \nPharmacogenomics Research Center, \nInje University College of Medicine, \nBusan, Korea\n2\nCenter for Personalized Precision \nMedicine of Tuberculosis, Inje University \nCollege of Medicine, Busan, Korea\nReceived: 11 November, 2024\nRevised: 3 February, 2025\nAccepted: 7 February, 2025\nAddress for correspondence: \nSangzin Ahn\nDepartment of Pharmacology and \nPharmacoGenomics Research Center, \nInje University College of Medicine, \n75 Bokjiro, Busanjin-gu, Busan 47693, \nKorea\nEmail: sangzinahn@inje.ac.kr\nhttps://orcid.org/0000-0003-2749-\n0014\nhttps://doi.org/10.6065/apem.2448278.139\nAnn Pediatr Endocrinol Metab 2025;30:115-118\nAhn S • Hallucinations and reliability in LLMs\n116\nwww.e-apem.org\nuse of LLMs in medical research contexts.\nThe response of the medical community to LLMs is cautious. \nAlthough clinicians generally accept LLMs for supportive \nresearch roles such as data processing and content drafting, \nthey express significant reservations regarding their use in \nhigh-stakes tasks such as original manuscript writing [2]. These \nconcerns are not unfounded, and some studies have identified \nlimitations in the ability of LLMs to handle specialized \nmedical content. For example, a comprehensive evaluation \nin ophthalmology research revealed that LLMs struggle with \naccurate reference generation and cannot reliably handle \nspecific medical information, such as failing to distinguish \nbetween oral versus intravenous corticosteroid doses in optic \nneuritis treatment [3].\nThis review addresses the gap between LLM adoption and \nreliability concerns based on three key objectives: analyzing \ntechnical limitations of LLMs in medical research contexts, \nevaluating emerging reliability enhancement strategies,  \nand providing evidence-based recommendations for LLM \nintegration in research workflows. The results can provide \nmedical researchers with practical knowledge for leveraging \nLLMs while maintaining scientific integrity.\nTechnical understanding of LLM limitations\nThe reliability limitations of LLMs are rooted in their \narchitectural design. These models generate text through \nautoregressive prediction, where each token is iteratively \npredicted and appended based on preceding tokens, creating \nan inherent constraint on their ability to maintain long-range \ncoherence and accuracy. This sequential generation process is \nparticularly challenging in specialized domains where high-\nquality training data may be scarce, leading to decreased \nprediction accuracy in technical fields [4].\nRecent theoretical work in computational logic indicates \nthat \"hallucinations\" or generation of false or unsupported \ninformation is an intrinsic characteristic of LLM architecture \nrather than a temporary limitation. Banerjee et al. [5] demons-\ntrated that fundamental computational constraints, particularly \nthose associated with undecidability as established in concepts \nlike Gödel's First Incompleteness Theorem, hindering \nthe guarantee of perfect accuracy in LLM outputs. These \nstructural hallucinations manifest in four distinct forms: factual \nincorrectness, misinterpretation, partial incorrectness, and \nfabrications, each carrying a non-zero probability of occurrence \nregardless of architectural sophistication or training data quality.\nIn addition to these technical limitations, the fundamental \nnature of LLM-generated inaccuracies can be understood \nthrough a philosophical concept. Although these models excel at \nproducing linguistically coherent responses, they fundamentally \nlack mechanisms for truth verification. Hicks et al. [6] charac-\nterizes this behavior as \"bullshit\" in the philosophical sense \ndefined by Harry Frankfurt, distinguishing it from mere \ninaccuracies or lies. This characterization emphasizes that \nLLMs are not attempting to represent or misrepresent truth \nbut are instead inherently indifferent to it, producing what the \nauthors term \"soft bullshit,\" which is content generated without \nany intentional stance toward truth or deception. Furthermore, \nwhen these pattern-matching systems are presented to users \nas if they are reliable sources of factual information, they may \ninadvertently produce \"hard bullshit,\" actively misleading users \nregarding their capability for truth-telling.\nCurrent approaches to enhancing reliability\nV arious strategies have been proposed to mitigate the \nreliability issues of LLMs. One key approach involves advanced \nprompting techniques that guide the model to produce more \naccurate and reliable outputs [7]. For example, prompting the \nmodel to articulate the reasoning steps through techniques like \nChain-of-Thought can reduce hallucinations by encouraging \nsequential processing of information. Generating multiple \nresponses and selecting the most consistent one, known as \nSelf-Consistency, can also improve reliability. Breaking down \ncomplex questions into simpler subquestions (Decomposition) \nallows the model to focus on each component individually, \nreducing the chance of a logic leap or fabricated information. \nAdding verification steps throughout the response generation \nprocess (Chain-of-V erification) forces the model to assess its \nassertions against internal information, increasing the reliability \nof its outputs (T able 1). For details and examples of these \nprompting techniques, readers are encouraged to refer to the \ncomprehensive survey of Schulhoff et al. [7], which provides \nextensive documentation of effective prompting strategies \napplicable to various contexts.\nAnother approach is Retrieval-Augmented Generation \n(RAG), which combines a language model with an information \nretrieval component [8]. This allows the model to pull relevant \ninformation from trusted sources or databases, grounding its \nresponses in factual data (Fig. 1). However, even RAG systems \nare not immune to errors. For instance, a Google artificial \nintelligence (AI) search feature powered by RAG erroneously \nsuggested using \"nontoxic glue\" to prevent pizza toppings from \nfalling off, highlighting the persistent challenge of hallucinations \neven in retrieval-augmented systems [9].\nTable 1. Prompting techniques to enhance large language model \nreliability\nTechnique Description\nChain-of-thought Prompts the model to reason step-by-step, \nprocessing information sequentially.\nSelf-consistency Generates multiple responses and selects the \nmost consistent one.\nDecomposition Breaks down complex questions into simpler \nsubquestions, allowing focused analysis of \neach component.\nChain-of-verification Incorporates verification at each step of \nreasoning, ensuring each step aligns with \nknown information.\n117\nAhn S • Hallucinations and reliability in LLMs\nwww.e-apem.org\nOptimizing LLM use in medical research\nT o effectively integrate LLMs into medical research, a nuanced \nunderstanding of task-specific performance is essential. A meta-\nanalysis of 370 human-AI collaboration experiments revealed a \ncomplex interaction pattern in which human-AI combinations \ngenerally augmented individual human capabilities but often \nfailed to achieve synergistic benefits over AI systems alone, \nparticularly in decision-making tasks [10]. However, the analysis \nidentified specific contexts where human-AI collaboration \nshowed superior performance, especially in content creation \nand jobs that humans can perform adequately without the \nassistance of AI. These findings suggest that medical researchers \nshould strategically employ LLMs in roles that are well-suited to \nhuman verification and validation, such as literature summary, \nhypothesis generation, and data analysis, but maintain careful \nconsideration of task allocation and oversight mechanisms.\nThe practical implementation of these principles can be \nobserved in several key research applications. In literature \nsynthesis, systems like PaperQA2 have demonstrated impressive \ncapabilities through an \"agentic\" system equipped with \nsophisticated ranking and contextual summarization tools, \nsometimes exceeding PhD-level expertise in specific tasks [11]. \nThese results show the potential of well-designed augmentation \nsystems while emphasizing the continued importance of human \nexpertise because human feedback helps refine benchmarks \nand guide the development of the system to better handle \nnuanced and complex scientific questions. Similarly, Unlu et al. \n[12] demonstrated that non-experts using a RAG-enabled LLM \nsystem achieved performance comparable to expert screeners in \nclinical trial screening tasks. However, the authors emphasized \ncritical considerations, including the need for human expert \nverification and monitoring of potential biases, reinforcing the \nimportance of human oversight.\nThe selection of appropriate tools and implementation \nstrategies is crucial for successful LLM integration. Although \ngeneral-purpose LLMs such as ChatGPT or Claude are useful \nfor many low-stake tasks, they may produce high rates of \nerrors in rigorous research applications. For research-related \ninformation searches, specialized tools with robust search \nand verification capabilities are essential [13]. The choice of \ntools should align with task requirements and incorporate \nappropriate verification mechanisms, particularly for tasks \ninvolving medical knowledge where accuracy is paramount. \nThis careful tool selection process ensures that LLMs enhance \nrather than compromise research quality.\nAs the field continues to evolve, emerging technologies \nindicate potential future directions for LLM integration in \nmedical research. Lu et al. [14] demonstrated an 'AI Scientist' \nframework capable of autonomously generating research \nideas, conducting in silico  experiments, and writing papers \nin machine learning domains. However, the success of this \nsystem in controlled machine learning environments highlights \nan important contrast with medical research, where human \noversight remains crucial due to ethical implications and \nthe complexity of biological systems. This reinforces the \nimportance of viewing LLMs as augmentation tools rather than \nreplacements for human expertise in medical contexts.\nConclusions\nThis review highlights the importance of understanding \nthe reliability limitations of LLMs in medical research.  \nUnderstanding the technical backgrounds of the inherent \ntendency of LLMs to generate inaccurate information is \nUser Query\nSystem instruction\nUser query\nLLM\nLLM answer\nUser Query\nSystem instruction\nUser query\nLLM\nAugmented\nLLM answer\nRetriever\nRetrieved information\nRetrieved information\nPrompt\nGeneration\nDatabase\nSearch query\nRetrieval\nPrompt\nGeneration\n(A) (B)\nFig. 1. Comparison of standard and Retrieval-Augmented Generation (RAG) frameworks in large language models (LLMs). \n(A) In standard LLM generation, the model generates an answer from the user query and system instruction alone. (B) \nIn RAG, a retriever supplements the prompt with relevant information from a database, aiming to ground responses in \nfactual data and reduce hallucinations.\nAhn S • Hallucinations and reliability in LLMs\n118\nwww.e-apem.org\nessential.\nThe successful integration of LLMs into research may require \na balanced approach that combines technological innovation \nwith established expertise. According to Kellogg et al. [15], \nit may be a misconception to assume that issues related to \nAI utilization will resolve over time, or that they can be fully \naddressed by junior professionals who are more technologically \nadept. Experienced senior staff excel in assessing risks,  \nforeseeing potential disruptions, and evaluating how new \ntechnologies can align with or challenge organizational goals. \nCollaborative efforts that balance both the adaptability of \nnewer researchers and the experience of established experts are \nnecessary to effectively address the challenges associated with \nLLM reliability.\nThe findings of this review suggest that optimal LLM integra-\ntion in medical research depends on matching tool capabilities \nwith task requirements: utilizing general-purpose LLMs \nfor preliminary drafting and data organization, employing \nspecialized retrieval-augmented systems for literature synthesis \nand hypothesis generation, and maintaining direct human \noversight for complex biological interpretations and high-stakes \ndecisions. This task-specific approach combined with systematic \nverification offers a practical framework for leveraging LLM \ncapabilities while preserving research quality.\nNotes\nConflicts of interest: No potential conflict of interest relevant \nto this article was reported.\nFunding: This research was supported by a National Research \nFoundation of Korea (NRF) grant funded by the Korean \ngovernment (MSIT; Grant No. 2018R1A5A2021242).\nAuthor contribution: This article is a single-author article.\nORCID\nSangzin Ahn: 0000-0003-2749-0014\nReferences\n1.   Kobak D, González-Márquez R, Horvát EÁ, Lause J.  \nDelving into ChatGPT usage in academic writing through \nexcess vocabulary. arXiv:2406.07016 [Preprint]. 2016 [cited \n2024 Nov 8]. Available from: https://doi.org/10.48550/\narXiv.2406.07016.\n2.  Spotnitz M, Idnay B, Gordon ER, Shyu R, Zhang G, Liu \nC, et al. A survey of clinicians' views of the utility of large \nlanguage models. Appl Clin Inform 2024;15:306-12.\n3.  Hua HU , Kaakour AH, Rachitskaya A, Srivastava S, Sharma \nS, Mammo DA. Evaluation and comparison of ophthalmic \nscientific abstracts and references by current artificial \nintelligence chatbots. JAMA Ophthalmol 2023;141:819-24.\n4.  Lin CC, Jaech A, Li X, Gormley MR, Eisner J. Limitations \nof autoregressive models and their alternatives. In:  \nT outanova K, Rumshisky A, Zettlemoyer L, Hakkani-\nT ur D, Beltagy I, Bethard S, et al. editors. Proceedings of \nthe 2021 Conference of the North American Chapter of \nthe Association for Computational Linguistics: Human \nLanguage T echnologies. Association for Computational \nLinguistics, 2021:5147-73.\n5.   Banerjee S, Agarwal A, Singla S. LLMs will always \nhallucinate, and we need to live with this. arXiv:2409.05746 \n[Preprint]. 2024 [cited 2024 Sep 12]. Available from: https://\ndoi.org/10.48550/arXiv.2409.05746.\n6.  Hicks MT , Humphries J, Slater J. ChatGPT is bullshit. Ethics \nInf T echnol 2024;26:38.\n7.  Schulhoff S, Ilie M, Balepur N, Kahadze K, Liu A, Si C, et \nal. The prompt report: a systematic survey of prompting \ntechniques. arXiv:2406.06608 [Preprint]. 2024 [cited \n2024 Sep 3]. Available from: https://doi.org/10.48550/\narXiv.2406.06608.\n8.  Lewis P , Perez E, Piktus A, Petroni F , Karpukhin V , Goyal \nN, et al. Retrieval-augmented generation for knowledge-\nintensive NLP tasks. arXiv:2005.11401 [Preprint].  \n2021 [cited 2024 Nov 7]. Available from: https://doi.\norg/10.48550/arXiv.2005.11401.\n9.  Google AI search tells users to glue pizza and eat rocks \n[Internet]. BBC News; [cited 2024 Nov 7]. Available from: \nhttps://www.bbc.com/news/articles/cd11gzejgz4o.\n10. V accaro M, Almaatouq A, Malone T . When combinations \nof humans and AI are useful: a systematic review and meta-\nanalysis. Nat Hum Behav 2024;8:2293-303.\n11. Skarlinski MD, Cox S, Laurent JM, Laurent JM, Braza \nJD, Hinks M, et al. Language agents achieve superhuman \nsynthesis of scientific knowledge. arXiv:2409.13740 \n[Preprint]. 2024 [cited  2024 Sep 12]. Available from: \nhttps://doi.org/10.48550/arXiv.2409.13740.\n12. Unlu O, Shin J, Mailly CJ, Oates MF , T ucci MR, V arugheese \nM, et al. Retrieval-augmented generation–enabled GPT-4 \nfor clinical trial screening. NEJM AI 2024;1:AIoa2400181.\n13. Ahn S. The transformative impact of large language models \non medical writing and publishing: current applications, \nchallenges and future directions. Korean J Physiol \nPharmacol 2024;28:393-401.\n14. Lu C, Lu C, Lange RT , Foerster J, Clune J, Ha D. The AI \nscientist: towards fully automated open-ended scientific \ndiscovery. arXiv:2408.06292 [Preprint]. 2024 [cited \n2024 Aug 13]. Available from: https://doi.org/10.48550/\narXiv.2408.06292.\n15. Kellogg K, Lifshitz-Assaf H, Randazzo S, Mollick ER,  \nDell'Acqua F , McFowland E III, et al. Don ’t expect \njuniors to teach senior professionals to use generative AI: \nemerging technology risks and novice AI Risk mitigation \ntactics. 2024. Harvard Business School T echnology & \nOperations Mgt. Unit W orking Paper 24-074, The Wharton \nSchool Research Paper, Available at SSRN: https://ssrn.\ncom/abstract=4857373  or http://dx.doi.org/10.2139/\nssrn.4857373.",
  "topic": "Reliability (semiconductor)",
  "concepts": [
    {
      "name": "Reliability (semiconductor)",
      "score": 0.635513424873352
    },
    {
      "name": "Risk analysis (engineering)",
      "score": 0.5752741694450378
    },
    {
      "name": "Computer science",
      "score": 0.4859253466129303
    },
    {
      "name": "Task (project management)",
      "score": 0.46796900033950806
    },
    {
      "name": "Management science",
      "score": 0.4482908844947815
    },
    {
      "name": "Data science",
      "score": 0.39060136675834656
    },
    {
      "name": "Medicine",
      "score": 0.364997923374176
    },
    {
      "name": "Engineering ethics",
      "score": 0.3614409267902374
    },
    {
      "name": "Systems engineering",
      "score": 0.2003224492073059
    },
    {
      "name": "Engineering",
      "score": 0.13436129689216614
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I104338594",
      "name": "Inje University",
      "country": "KR"
    }
  ],
  "cited_by": 3
}