{
  "title": "Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model",
  "url": "https://openalex.org/W3176589722",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2222723425",
      "name": "Dai Hong-liang",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2651021975",
      "name": "Song, Yangqiu",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2743859328",
      "name": "Wang Haixun",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2971109883",
    "https://openalex.org/W2970544797",
    "https://openalex.org/W2945878859",
    "https://openalex.org/W3168355451",
    "https://openalex.org/W2798442412",
    "https://openalex.org/W1713614699",
    "https://openalex.org/W3035586841",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962891712",
    "https://openalex.org/W2132096166",
    "https://openalex.org/W3017271548",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W638568100",
    "https://openalex.org/W3032280238",
    "https://openalex.org/W3105057865",
    "https://openalex.org/W2250718161",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3035458998",
    "https://openalex.org/W3041008240",
    "https://openalex.org/W2963158304",
    "https://openalex.org/W2068737686",
    "https://openalex.org/W3114027256",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2970076461",
    "https://openalex.org/W2406945108",
    "https://openalex.org/W2789018230",
    "https://openalex.org/W2283140239",
    "https://openalex.org/W2572998653"
  ],
  "abstract": "Recently, there is an effort to extend fine-grained entity typing by using a richer and ultra-fine set of types, and labeling noun phrases including pronouns and nominal nouns instead of just named entity mentions. A key challenge for this ultra-fine entity typing task is that human annotated data are extremely scarce, and the annotation ability of existing distant or weak supervision approaches is very limited. To remedy this problem, in this paper, we propose to obtain training data for ultra-fine entity typing by using a BERT Masked Language Model (MLM). Given a mention in a sentence, our approach constructs an input for the BERT MLM so that it predicts context dependent hypernyms of the mention, which can be used as type labels. Experimental results demonstrate that, with the help of these automatically generated labels, the performance of an ultra-fine entity typing model can be improved substantially. We also show that our approach can be applied to improve traditional fine-grained entity typing after performing simple type mapping.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 1790–1799\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1790\nUltra-Fine Entity Typing with Weak Supervision from a Masked\nLanguage Model\nHongliang Dai, Yangqiu Song\nDepartment of CSE, HKUST\n{hdai,yqsong}@cse.ust.hk\nHaixun Wang\nInstacart\nhaixun.wang@instacart.com\nAbstract\nRecently, there is an effort to extend ﬁne-\ngrained entity typing by using a richer\nand ultra-ﬁne set of types, and labeling\nnoun phrases including pronouns and nomi-\nnal nouns instead of just named entity men-\ntions. A key challenge for this ultra-ﬁne en-\ntity typing task is that human annotated data\nare extremely scarce, and the annotation abil-\nity of existing distant or weak supervision\napproaches is very limited. To remedy this\nproblem, in this paper, we propose to obtain\ntraining data for ultra-ﬁne entity typing by us-\ning a BERT Masked Language Model (MLM).\nGiven a mention in a sentence, our approach\nconstructs an input for the BERT MLM so that\nit predicts context dependent hypernyms of the\nmention, which can be used as type labels. Ex-\nperimental results demonstrate that, with the\nhelp of these automatically generated labels,\nthe performance of an ultra-ﬁne entity typing\nmodel can be improved substantially. We also\nshow that our approach can be applied to im-\nprove traditional ﬁne-grained entity typing af-\nter performing simple type mapping.\n1 Introduction\nFine-grained entity typing (Ling and Weld, 2012)\nhas been long studied in the natural language pro-\ncessing community as the extracted type informa-\ntion is useful for downstream tasks such as entity\nlinking (Ling et al., 2015; Onoe and Durrett, 2020),\nrelation extraction (Koch et al., 2014), coreference\nresolution (Onoe and Durrett, 2020), etc. Recently,\nultra-ﬁne entity typing (Choi et al., 2018) extends\nthe effort to using a richer set of types (e.g., per-\nson, actor, company, victim) to label noun phrases\nincluding not only named entity mentions, but also\npronouns and nominal nouns. This task directly\nuses type words or phrases as tags. Its tag set can\ncontain more than 10,000 types. A challenge is that\nwith the large type set, it is extremely difﬁcult and\ntime-consuming for humans to annotate samples.\nAs a result, most existing works use weak labels\nthat are automatically generated (Ling and Weld,\n2012; Choi et al., 2018; Lee et al., 2020).\nThere are two main approaches to obtaining\nweakly labeled training examples. One approach\nis to ﬁnd the Wikipedia pages that correspond to\nentity mentions, which can be done by using hyper-\nlinks to Wikipedia or applying entity linking. Then\nthe entity types can be obtained from knowledge\nbases. The other approach is to directly use the\nhead words of nominal mentions as ultra-ﬁne type\nlabels. For example, if a nominal mention is “a\nfamous actor,” then the head word “actor” can be\nused as its type label.\nSeveral problems exist when using these weak\nlabels for the ultra-ﬁne typing task. First, in the\ndataset created by Choi et al. (2018), on average\nthere are fewer than two labels (types) for each\nsample annotated through either entity linking or\nhead word supervision. On the other hand, a hu-\nman annotated sample has on average 5.4 labels.\nAs a result, models trained from the automatically\nobtained labels have a low recall. Second, neither\nof the above approaches can create a large number\nof training samples for pronoun mentions. Third, it\nis difﬁcult to obtain types that are highly dependent\non the context. For example, in “I met the movie\nstar Leonardo DiCaprio on the plane to L.A.,” the\ntype passenger is correct for “Leonardo DiCaprio.”\nHowever, this type cannot be obtained by linking\nto knowledge bases.\nIn this paper, to alleviate the problems above, we\npropose an approach that combines hypernym ex-\ntraction patterns (Hearst, 1992; Seitner et al., 2016)\nwith a masked language model (MLM), such as\nBERT (Devlin et al., 2019), to generate weak la-\nbels for ultra-ﬁne entity typing. Given a sentence\nthat contains a mention, our approach adds a short\npiece of text that contains a “[MASK]” token into it\n1791\nInput Top Words for [MASK]\nIn late 2015, [MASK] such as Leonardo DiCaprio starred in The\nRevenant.\nactors, stars, actor, directors,\nﬁlmmakers\nAt some clinics,they and some other [MASK] are told the doctors\ndon’t know how to deal with AIDS, and to go someplace else.\npatients, people, doctors, kids,\nchildren\nFinkelstein says he expects the company to “beneﬁt from some of\nthe disruption faced byour competitors and any other [MASK] .”\ncompany, business, companies,\ngroup, investors\nTable 1: Examples of constructed BERT MLM inputs for obtaining weak entity typing labels. Entity mentions\nare in bold and underlined. The texts highlighted with blue background are not in the original sentences. They are\ninserted to create inputs for BERT. The right column lists the ﬁve most probable words predicted by a pretrained\nBERT-Base-Cased MLM.\nto construct an input to BERT. Then, the pretrained\nMLM will predict the hypernyms of the mention\nas the most probable words for “[MASK].” These\nwords can then be used as type labels. For example,\nconsider the ﬁrst example in Table 1. The origi-\nnal sentence is “In late 2015, Leonardo DiCaprio\nstarred in The Revenant.” We construct an input\nfor the BERT MLM by inserting “[MASK] such as”\nbefore the mention “Leonardo DiCaprio.” With this\ninput, the pretrained BERT MLM predicts “actors,”\n“stars,” “actor,” “directors,” and “ﬁlmmakers” as\nthe ﬁve most probable words for “[MASK].” Most\nof them are correct types for the mention after sin-\ngularization. This approach can generate labels\nfor different kinds of mentions, including named\nentity mentions, pronoun mentions, and nominal\nmentions. Another advantage is that it can produce\nlabels that needs to be inferred from the context.\nThis allows us to generate more context-dependent\nlabels for each mention, such as passenger, patient,\netc.\nThen, we propose a method to select from the\nresults obtained through different hypernym extrac-\ntion patterns to improve the quality of the weak la-\nbels. We also use a weighted loss function to make\nbetter use of the generated labels for model train-\ning. Finally, we adopt a self-training step to further\nimprove the performance of the model. We evalu-\nate our approach with the dataset created by Choi\net al. (2018), which to the best of our knowledge, is\nthe only English ultra-ﬁne entity typing dataset cur-\nrently available. On this dataset, we achieve more\nthan 4% absolute F1 improvement over the previ-\nously reported best result. Additionally, we also\napply our approach to a traditional ﬁne-grained en-\ntity typing dataset: Ontonotes (Gillick et al., 2014),\nwhere it also yields better performance than the\nstate of the art.\nOur contributions are summarized as follows.\n• We propose a new way to generate weak labels\nfor ultra-ﬁne entity typing.\n• We propose an approach to make use of the\nnewly obtained weak labels to improve entity\ntyping results.\n• We conduct experiments on both an ultra-ﬁne\nentity typing dataset and a traditional ﬁne-\ngrained entity typing dataset to verify the ef-\nfectiveness of our method.\nOur code is available at https://github.com/\nHKUST-KnowComp/MLMET.\n2 Related Work\nThe ultra-ﬁne entity typing task proposed by Choi\net al. (2018) uses a large, open type vocabulary to\nachieve better type coverage than the traditional\nﬁne-grained entity typing task (Ling and Weld,\n2012) that uses manually designed entity type on-\ntologies. There are only limited studies on this\nnewly proposed task: A neural model introduced\nby (Onoe and Durrett, 2019) ﬁlters samples that\nare too noisy to be used and relabels the remaining\nsamples to get cleaner labels. A graph propaga-\ntion layer is introduced by (Xiong et al., 2019) to\nimpose a label-relational bias on entity typing mod-\nels, so as to implicitly capture type dependencies.\nOnoe et al. (2021) use box embeddings to capture\nlatent type hierarchies. There is also some work on\nthe applications of ultra-ﬁne entity typing: Onoe\nand Durrett (2020) apply ultra-ﬁne entity typing\nto learn entity representations for two downstream\ntasks: coreference arc prediction and named entity\ndisambiguation.\nThe traditional ﬁne-grained entity typing task\n(Ling and Weld, 2012; Yosef et al., 2012) is closely\n1792\nrelated to ultra-ﬁne entity typing. Automatic anno-\ntation (Ling and Weld, 2012; Gillick et al., 2014;\nDai et al., 2020) is also commonly used in the\nstudies of this task to produce large size training\ndata. Many different approaches have been pro-\nposed to improve ﬁne-grained entity typing per-\nformance. For example, denoising the automati-\ncally generated labels (Ren et al., 2016), taking\nadvantage of the entity type hierarchies or type\ninter-dependencies (Chen et al., 2020; Murty et al.,\n2018; Lin and Ji, 2019), exploiting external re-\nsources such as the information of entities provided\nin knowledge bases (Jin et al., 2019; Dai et al.,\n2019; Xin et al., 2018), etc.\nOur work is also related to recent studies (Petroni\net al., 2019; Jiang et al., 2020; Zhang et al., 2020)\nthat probe pretrained language models to obtain\nknowledge or results for target tasks. Different\nfrom them, we use the predictions produced by\nBERT as intermediate results that are regarded as\nweak supervision to train better models. (Zhang\net al., 2020) also uses Hearst patterns to probe\nmasked language models. However, they target\nat the entity set expansion task.\n3 Methodology\nOur methodology consists of two main steps. First,\nwe obtain weak ultra-ﬁne entity typing labels from\na BERT masked language model. Second, we use\nthe generated labels in model training to learn bet-\nter ultra-ﬁne entity typing models.\n3.1 Labels from BERT MLM\nGiven a sentence and a mention of interest in the\nsentence, our goal is to derive the hypernym or the\ntype of the mention using a BERT MLM. To do this,\nwe insert into the sentence a few tokens to create\nan artiﬁcial Hearst pattern (Hearst, 1992). One of\nthe inserted tokens is a special “[MASK]” token,\nwhich serves as the placeholder of the hypernym\nof the mention. As the BERT MLM predicts the\n“[MASK]” token, we derive the hypernyms of the\nmention.\nConsider the ﬁrst sentence in Table 1 as an ex-\nample: “In late 2015, Leonardo DiCaprio starred\nin The Revenant.” To ﬁnd the hypernym or the\ntype of “Leonardo DiCaprio”, we insert three to-\nkens to create a “such as” pattern: “In late 2015,\n[MASK] such as Leonardo DiCaprio starred in\nThe Revenant.” Applying the BERT MLM on the\nsentence, we derive hypernyms such as “actors,”\nPattern F1\nM and any other H 25.3\nM and some other H 24.8\nH such as M 20.7\nsuch H as M 18.1\nH including M 17.4\nH especially M 11.5\nTable 2: Hypernym extraction patterns. M denotes\nthe hyponym; H denotes the hypernym. The F1 score\nis evaluated with the development set of the ultra-ﬁne\ndataset (Choi et al., 2018) for the labels generated with\nthe corresponding pattern.\n“stars,” “directors,” “ﬁlmmakers.” Table 1 shows a\nfew more examples.\nWe consider the 63 Hearst-like patterns (Hearst,\n1992) presented in (Seitner et al., 2016) that express\na hypernym-hypnonym relationship between two\nterms. Table 2 lists some of the patterns, wherein\nH and M denote a hypernym and a hyponym, re-\nspectively. For example, “M and some other H”\ncan be used to match “Microsoft and some other\ncompanies.”\nThe general procedure to use these patterns to\ncreate input samples for BERT MLM and obtain\nlabels from its predictions is as follows. We ﬁrst\nregard the mention as M. Then, we insert the rest\nof the pattern either before or after the mention,\nand we replace H with the special “[MASK]” to-\nken. After applying the BERT MLM on sentences\nwith artiﬁcial Hearst patterns, we derive top ktype\nlabels from the prediction for “[MASK].” To drive\nthese k labels, we ﬁrst sigularize the most prob-\nable words that are plural. Then, remove those\nthat are not in the type vocabulary of the dataset.\nFinally, use the most probable k different words\nas k labels. For example, if we want to obtain 3\nlabels, and the most probable words are “people,”\n“actors,” “celebrities,” “famous,” “actor,” etc. Then\nthe 3 labels should be person, actor, celebrity. Be-\ncause “actor” is the singluar form of “actors,” and\n“famous” is not in the type vocabulary.\nWe show the performance of our method for\nobtaining 10 type labels for each mention with\ndifferent patterns in Table 2. A pre-trained BERT-\nBase-Cased MLM is used to obtain the results1.\nFor nominal mentions, directly applying the pat-\nterns that starts with “M” with the above procedure\n1We use the pretrained model provided in the Transform-\ners library. We also tried using BERT-Large and RoBERTa\nmodels. However, they do not yield better performance.\n1793\nmay sometimes be problematic. For example, con-\nsider the noun phrase “the factory in Thailand” as\na mention. If we use the “ M and some other H”\npattern and insert “and other [MASK]” after the\nmention, the BERT MLM will predict the type\ncountry for Thailand instead of for the entire men-\ntion. To avoid such errors, while applying patterns\nthat starts with “M” for nominal mentions, we re-\ngard the head word of the mention as M instead.\nA more subtle and challenging problem is that\nthe quality of the type labels derived from different\npatterns for different mentions can be very different.\nFor example, for the mention “He” in sentence “He\nhas won some very tough elections and he’s gover-\nnor of the largest state,” the pattern “H such as M”\nleads to person, election, time, thing, leader as the\ntop ﬁve types. But using the pattern “ M and any\nother H,” we get candidate, politician, man, per-\nson, governor. On the other hand, for mention “the\nAl Merreikh Stadium” in “It was only Chaouchi’s\nthird cap during that unforgettable night in the Al\nMerreikh Stadium,” the results of using “Hsuch as\nM” (the top ﬁve types are venue, place, facility, lo-\ncation, area) is better than using “M and any other\nH” (the top ﬁve types are venue, stadium, game,\nmatch, time).\nTo address the above problem, we do not use a\nsame pattern for all the mentions. Instead, for each\nmention, we try to select the best pattern to apply\nfrom a list of patterns. This is achieved by using\na baseline ultra-ﬁne entity typing model, BERT-\nUltra-Pre, which is trained beforehand without us-\ning labels generated with our BERT MLM based\napproach. Details of BERT-Ultra-Precan be found\nin Section 5.2. Denote the pattern list as L. With\neach pattern in L, we can apply it on the given\nmention to derive a set of labels from the BERT\nMLM. Then, we ﬁnd the set of labels that have the\nmost overlap with the labels predicted by BERT-\nUltra-Pre. Finally, the given mention is annotated\nwith this set of labels.\nIt is not necessary to use all the patterns in (Seit-\nner et al., 2016). To construct L, the list of patterns\nused for annotation, we perform the following pro-\ncedure.\nStep 1: Initialize Lto contain the best performing\npattern (i.e., “M and any other H”) only.\nStep 2: From all the patterns not in L, ﬁnd the one\nthat may bring the greatest improvement in\nF1 score if it is added to L.\nStep 3: Add the pattern found in Step 2 to the L\nif the improvement brought by it is larger\nthan a threshold.\nStep 4: Repeat steps 2-3 until no patterns can be\nadded.\nDiscussion on Type Coverage Since we only\nuse one [MASK] token to generate labels, the\nmodel cannot produce multi-word types (e.g., foot-\nball player) or single word types that are not\npresent in the BERT MLM vocabulary. The BERT\nMLM vocabulary covers about 92% of the labels in\nthe human annotated dataset constructed by Choi\net al. (2018). Type coverage is a known issue with\nweak supervision, and is tolerable if the generated\nlabels can be used to achieve our ﬁnal goal: improv-\ning the performance of the ultra-ﬁne entity typing\nmodel.\n3.2 Training Data\nOur approach generates type labels for all three\ntypes of mentions: named entity mentions, pro-\nnoun mentions, and nominal mentions. For named\nentity mentions and nominal mentions, existing au-\ntomatic annotation approaches can already provide\nsome labels for them by using the entity types in\nknowledge bases or using the head words as types\n(Ling and Weld, 2012; Choi et al., 2018). Thus, we\ncombine these labels with the labels generated by\nus. For pronoun mentions, no other labels are used.\nBesides the automatically annotated samples, we\ncan also use a small amount of human annotated\nsamples provided by the dataset for model training.\n3.3 Model Training\nOur ultra-ﬁne entity typing model follows the\nBERT-based model in (Onoe and Durrett, 2019).\nGiven a sentence that contains an entity mention,\nwe form the sequence “[CLS] sentence [SEP] men-\ntion string [SEP]” as the input to BERT. Then, de-\nnoting the ﬁnal hidden vector of the “[CLS]” token\nas u, we add a linear classiﬁcation layer on top of\nu to model the probability of each type:\np = σ(W u), (1)\nwhere σis the sigmoid function, W is a trainable\nweight matrix. p ∈Rd, where dis the number of\ntypes used by the dataset. We assign a type tto\nthe mention if pt, its corresponding element in p,\nis larger than 0.5. If no such types are found, we\nassign the one with the largest predicted probability\nto the mention.\n1794\nTo make use of the automatically labeled sam-\nples, some existing approaches mix them with high\nquality human annotated samples while training\nmodels (Choi et al., 2018; Onoe and Durrett, 2019).\nHowever, we ﬁnd that better performance can be\nobtained by pretraining the model on automatically\nlabeled samples, then ﬁne-tuning it on human an-\nnotated samples.\nFollowing (Choi et al., 2018), we partition the\nwhole type vocabulary used by the dataset into\nthree non-overlapping sets: general, ﬁne, and ultra-\nﬁne types, denoted withTg,Tf and Tu, respectively.\nThen, we use the following objective for training:\nJ(x) =L(x,Tg)1 (L,Tg) +L(x,Tf )1 (L,Tf )\n+ L(x,Tu)1 (L,Tu),\n(2)\nwhere xis a training sample; L denotes the set of\ntype labels assigned to xthrough either human or\nautomatic annotation. The function 1 (L,T) equals\n1 when a type in L is in set T and 0 otherwise.\nThis loss can avoid penalizing some false negative\nlabels.\nUnlike existing studies, we deﬁne the function L\ndifferently for human annotated samples and auto-\nmatically labeled samples. While pretraining with\nautomatically labeled samples, the labels obtained\nthrough entity linking and head word supervision\nare usually of higher precision than those obtained\nthrough BERT MLM. Thus, we propose to assign\ndifferent weights in the training objective to the\nlabels generated with different methods:\nL(x,T) =−\n∑\nt∈T\nα(t)[yt ·log(pt)\n+ (1−yt) ·log(1 −pt)],\n(3)\nwhere yt equals to 1 if tis annotated as a type for\nxand 0 otherwise; pt is the probability of whether\ntshould be assigned to xpredicted by the model.\nThe value of α(t) indicates how conﬁdent we are\nabout the label tfor x. Speciﬁcally, it equals to a\npredeﬁned constant value larger than 1 when tis a\npositive type for xobtained through entity linking\nor head word supervision, otherwise, it equals to 1.\nWhile ﬁne-tuning with human annotated sam-\nples, we directly use the binary cross entropy loss:\nL(x,T) =−\n∑\nt∈T\n[yt·log(pt)+(1−yt)·log(1−pt)].\n(4)\n3.4 Self-Training\nDenote the ultra-ﬁne entity typing model obtained\nafter pretraining on the automatically labeled data\nas h, and the model obtained after ﬁne-tuning h\nwith human annotated data as m. A weakness of\nmis that at the ﬁne-tuning stage, it is trained with\nonly a small number of samples. Thus, we employ\nself-training to remedy this problem.\nBy using mas a teacher model, our self-training\nstep ﬁne-tunes the model hagain with a mixture of\nthe samples from the automatically labeled data and\nthe human annotated data. This time, for the auto-\nmatically annotated samples, we use pseudo labels\ngenerated based on the predictions of minstead of\ntheir original weak labels. The newly ﬁne-tuned\nmodel should perform better than m, and is used\nfor evaluation.\nDenote the set of human annotated samples as\nH, the set of automatically labeled samples as A.\nThe training objective at this step is\nJST = 1\n|H|\n∑\nx∈H\nJ(x) +λ 1\n|A|\n∑\nx∈A\nLST (x), (5)\nwhere λ is a hyperparameter that controls the\nstrength of the supervision from the automatically\nlabeled data.\nWhile computing loss for the samples in A, we\nonly use the types that are very likely to be positive\nor negative. For a samplex, let pt be the probability\nof it belonging to type tpredicted by the model m.\nWe consider a type tvery likely to be positive if\npt is larger than a threshold P, or if t is a weak\nlabel of xand pt is larger than a smaller threshold\nPw. Denote the set of such types as ˆY+(x). We\nconsider a type tvery likely to be negative if pt is\nsmaller than 1 −P. Denote the set of such types\nas ˆY−(x). Then we have:\nLST (x) =−\n∑\nt∈ˆY +(x)\nlog(pt)\n−\n∑\nt∈ˆY −(x)\nlog(1 −pt).\n(6)\nThus, we compute the binary cross entropy loss\nwith only the types in ˆY+(x) and ˆY−(x).\n4 Application to Traditional\nFine-grained Entity Typing\nOur approach to generating weak entity type la-\nbels with BERT MLM can also be applied to the\n1795\ntraditional ﬁne-grained entity typing task. Differ-\nent from ultra-ﬁne entity typing, traditional ﬁne-\ngrained entity typing uses a manually designed\nentity type ontology to annotate mentions. The\ntypes in the ontology are organized in an hierar-\nchical structure. For example, the ontology used\nby the Ontonotes dataset contains 89 types includ-\ning /organization, /organization/company, /person,\n/person/politician, etc. On this dataset, our auto-\nmatic annotation approach can mainly be helpful\nto generate better labels for nominal mentions.\nWe still use the same method described in Sec-\ntion 3.1 to create input for BERT MLM based\non the given mention. But with traditional ﬁne-\ngrained entity typing, most mentions are assigned\nonly one type path (e.g., a company mention will\nonly be assigned labels {/organization, /organiza-\ntion/company}, which includes all the types along\nthe path of /organization/company). Thus, while\ngenerating labels, we only use the most proba-\nble word predicted by the BERT MLM, which is\nmapped to the types used by the dataset if possible.\nFor example, the word “company” and its plural\nform are both mapped to /organization/company.\nSuch a mapping from free-form entity type words\nto the types used by the dataset can be created\nmanually, which does not require much effort. We\nmainly construct the mapping with two ways: 1)\nCheck each type used by the dataset, and think of a\nfew words that should belong to it, if possible. For\nexample, for the type /person/artist/author, corre-\nsponding words can be “author,” “writer,” etc. 2)\nRun the BERT MLM on a large number of inputs\nconstructed with unannotated mentions, then try to\nmap the words that are most frequently predicted as\nthe most probable word to the entity type ontology.\nSince only the most probable word predicted by\nthe BERT MLM is used to produce labels, we also\nonly use one hypernym relation pattern: “M and\nany other H.”\nFor traditional ﬁne-grained entity typing, we use\nour approach to generate labels for mentions that\nare not previously annotated with other automatic\nannotation approaches. While training, all the auto-\nmatically labeled mentions are used together. The\ntyping model is the same as the model described\nin 3.3. The binary cross entropy loss is directly\nemployed as the training objective.\n5 Experiments\nWe conduct experiments on our primary task: ultra-\nﬁne entity typing. In addition, we evaluate the\nperformance of our approach when applied to tra-\nditional ﬁne-grained entity typing.\n5.1 Evaluation on Ultraﬁne\nFor ultra-ﬁne entity typing, we use the dataset cre-\nated by Choi et al. (2018). It uses a type set that\ncontains 10,331 types. These types are partitioned\ninto three categories: 9 general types, 121 ﬁne-\ngrained types, and 10,201 ultra-ﬁne types. There\nare 5,994 human annotated samples. They are split\ninto train/dev/test with ratio 1:1:1. It also provides\n5.2M samples weakly labeled through entity link-\ning and 20M samples weakly labeled through head\nword supervision.\nWe compare with the following approaches:\n• UFET (Choi et al., 2018). This approach ob-\ntains the feature vector for classiﬁcation by\nusing a bi-LSTM, a character level CNN, and\npretrained word embeddings.\n• LabelGCN (Xiong et al., 2019). LabelGCN\nuses a graph propagation layer to capture label\ncorrelations.\n• LDET (Onoe and Durrett, 2019). LDET\nlearns a model that performs relabeling and\nsample ﬁltering to the automatically labeled\nsamples. Their typing model, which employs\nELMo embeddings and a bi-LSTM, is train\nwith the denoised labels.\n• Box (Onoe et al., 2021). Box represents entity\ntypes with box embeddings to capture latent\ntype hierarchies. Their model is BERT-based.\nWe use the BERT-Base-Cased version of BERT\nfor both weak label generation and the typing\nmodel in Section 3.3. The hyperparameters are\ntuned through grid search using F1 on the dev set\nas criterion. The value of α(t) in Equation (3) is\nset to 5.0 for positive types obtained through entity\nlinking or head word supervision. λin Equation\n(5) is set to 0.01. P and Pw in Section 3.4 are set to\n0.9 and 0.7, respectively. Our approach to generate\nlabels through BERT MLM is applied to each weak\nsample provided in the original dataset. In addition,\nwe also use our approach to annotate about 3.7M\npronoun mentions, which are extracted through\nstring matching from the English Gigaword corpus\n1796\nMethod P R F1\nUFET 47.1 24.2 32.0\nLabelGCN 50.3 29.2 36.9\nLDET 51.5 33.0 40.2\nBox 52.8 38.8 44.8\nOurs 53.6 45.3 49.1\nTable 3: Macro-averaged Precision, Recall, and F1 of\ndifferent approaches on the test set.\nMethod P R F1\nBERT-Ultra-Direct 51.0 33.8 40.7\nBERT-Ultra-Pre 50.8 39.7 44.6\nOurs (Single Pattern) 52.4 44.9 48.3\nOurs (Unweighted Loss) 51.5 45.8 48.5\nOurs (No Self-train) 53.5 42.8 47.5\nOurs 53.6 45.3 49.1\nTable 4: Performance of different variants of our ap-\nproach on the test set. BERT-Ultra-Direct and BERT-\nUltra-Pre are two baseline approaches that do not use\nlabels generated with our BERT MLM based method\nin training.\n(Parker et al., 2011). We generate 10 types for each\nsample2. With the procedure described in Sectiton\n3.1, three hypernym extraction patterns are used\nwhile generating labels with BERT MLM: “M and\nany other H,” “Hsuch as M,” “M and some other\nH.” Speciﬁcally, adding “H such as M” and “M\nand some other H” improves the F1 score from\n0.253 to 0.274, and from 0.274 to 0.279, respec-\ntively. Adding any more patterns cannot improve\nthe F1 score for more than 0.007.\nFollowing existing work (Onoe et al., 2021;\nOnoe and Durrett, 2019), we evaluate the macro-\naveraged precision, recall, and F1 of different ap-\nproaches on the manually annotated test set. The\nresults are in Table 3. Our approach achieves the\nbest F1 score. It obtains more than 4% F1 score\nimprovement over the existing best reported perfor-\nmance by Box in (Onoe et al., 2021). This demon-\nstrates the effectiveness of our approach.\n5.2 Ablation Study\nFor ablation study, we verify the effectiveness of\nthe different techniques used in our full entity typ-\ning approach by evaluating the performance of the\nfollowing variants: Ours (Single Pattern) only\n2The performance of the trained model is relatively insensi-\ntive with respect to the number of labels generated with MLM.\nThe difference between the F1 scores of the models trained\nusing 10 and 15 generated types is less than 0.005.\nuses one pattern: M and any other H; Ours (Un-\nweighted Loss) removes the α(t) term in Equation\n(3); Ours (No Self-train) does not perform the\nself-training step. We also evaluate two baseline\napproaches: BERT-Ultra-Direct uses the same\nBERT based model described in Section 3.3, but is\ntrained with only the human annotated training sam-\nples; BERT-Ultra-Pre also uses the same BERT\nbased model, but is ﬁrst pretrained with the ex-\nisting automatically generated training samples in\nthe dataset provided by Choi et al. (2018), then\nﬁne-tuned on the human annotated training data.\nFirst, the beneﬁt of using the labels generated\nthrough BERT MLM can be veriﬁed by comparing\nOurs (No Self-train)and BERT-Ultra-Pre. Because\nthe techniques employed in Ours (No Self-train),\nincluding the use of multiple hypernym extraction\npatterns and the weighted loss, are both for bet-\nter utilization of our automatic entity type label\ngeneration method.\nThe effectiveness of the use of multiple hyper-\nnym extraction patterns, the weighted loss, and\nthe self-training step can be veriﬁed by compar-\ning Ours with Ours (Single Pattern), Ours (Un-\nweighted Loss)and Ours (No Self-train), respec-\ntively. Among them, self-training is most beneﬁ-\ncial.\n5.3 Evaluation on Different Kinds of\nMentions\nIt is also interesting to see how our approach per-\nforms on different kinds of mentions. Table 5 lists\nthe performance of our full approach and two base-\nline systems on the three kinds of mentions in the\ndataset: named entity mention, pronoun mentions,\nand nominal mentions.\nOur approach performs much better than BERT-\nUltra-Pre on all three kinds of mentions. The im-\nprovements in F1 on pronoun and nominal men-\ntions are relatively more substantial.\n5.4 Case Study\nTable 6 presents several ultra-ﬁne entity typing ex-\namples, along with the human annotated labels,\nand the labels predicted by BERT-Ultra-Pre, BERT\nMLM, and our full approach.\nIn the ﬁrst example, the label prisoner is a type\nthat depends on the context, and is usually not\nassigned to humans in knowledge bases. We think\nthat since we can assign such labels to the training\nsamples with our BERT MLM based approach, our\n1797\nNamed Entity Pronoun Nominal\nMethod P R F1 P R F1 P R F1\nBERT-Ultra 58.1 45.1 50.8 52.9 42.9 47.4 47.4 26.9 34.3\nBERT-Ultra-Pre 54.7 50.5 52.5 51.3 46.1 48.6 45.2 33.7 38.6\nOurs 58.3 54.4 56.3 57.2 50.0 53.4 49.5 38.9 43.5\nTable 5: Performance on named entity mentions, pronoun mentions, and nominal mentions, respectively.\nSentence\nCaptured in 1795, he was con-\nﬁned at Dunkirk, escaped, set sail\nfor India, was wrecked on the\nFrench coast, and condemned to\ndeath by the decree of the French\nDirectory.\nHuman prisoner, person\nBERT-Ultra-Pre person, soldier, man, criminal\nBERT MLM man, prisoner, person, soldier, ofﬁ-\ncer\nOurs person, soldier, man, prisoner\nSentence\nAlso in the morning,\na roadside bomb struck a\npolice patrol on a main road in\nBaghdad’s northern neighbor-\nhood of Waziriya, damaging a\npolice vehicle ...\nHuman bomb, weapon, object, explosive\nBERT-Ultra-Pre object, event, attack, bomb\nBERT MLM weapon, threat, evidence, device, de-\nbris\nOurs object, weapon, bomb\nSentence\nIn October 1917, Sutton was pro-\nmoted (temporarily) to the rank\nof major and appointed Ofﬁcer\nCommanding No.7 Squadron, a\nposition he held for the remained\nof the War.\nHuman soldier, ofﬁcer, male, person\nBERT-Ultra-Pre person, politician, male\nBERT MLM ofﬁcer, pilot, man, unit, aircraft\nOurs person, soldier, male, ofﬁcer\nTable 6: Ultra-ﬁne entity typing examples with the cor-\nresponding human annotated labels and predictions of\nthree different systems. Entity mentions are in bold and\nunderlined. For BERT MLM, we list the top ﬁve labels.\nmodel is better at predicting them than the baseline\nmodel.\nThe second and third examples demonstrate that\nour model may not only improve the recall by pre-\ndicting more correct types, but also reduce incor-\nrect predictions that do not ﬁt the mention or the\ncontext well.\n5.5 Evaluation on Ontonotes\nThe Ontonotes dataset uses an ontology that con-\ntains 89 types to label entity mentions. We use the\nversion provided by Choi et al. (2018). It includes\n11,165 manually annotated mentions, which are\nsplit into a test set that contains 8,963 mentions,\nand a dev set that contain 2,202 mentions. It also\nprovides about 3.4M automatically labeled men-\ntions.\nSince existing annotations for named entity men-\ntions may be more accurate than the annotations\nobtained through our approach, we only apply our\nmethod to label nominal mentions. Applying the\napproach in Section 4, we create 1M new auto-\nmatically labeled mentions with the head word su-\npervision samples (such samples contain mostly\nnominal mentions) in the ultra-ﬁne dataset. They\nare used together with the originally provided 3.4M\nmentions to train the typing model.\nOn this dataset, we compare with the follow-\ning approaches: UFET (Choi et al., 2018), LDET\n(Onoe and Durrett, 2019), DSAM (Hu et al.,\n2020), LTRFET (Lin and Ji, 2019), BERT-Direct.\nWhere BERT-Direct uses the same BERT based\nmodel as our approach, but trains with only the\nweak samples provided in the dataset. LTRFET\nadopts a hybrid classiﬁcation method to exploit\ntype inter-dependency. DSAM is a diversiﬁed se-\nmantic attention model with both mention-level\nattention and context-level attention.\nFor our approach and BERT-Direct, we still use\nthe pretrained BERT-Base-Cased model for initial-\nization. Although a very large number of weakly\nlabeled mentions are provided, not all of them are\nneeded for training the models. In our experiments,\nfor both our approach and BERT-Direct, the per-\nformance does not increase after training on about\n0.3M mentions.\nWe report strict accuracy, macro-averaged F1,\nand micro-averaged F1 (Ling and Weld, 2012). The\nresults are in Table 7. As we can see, our approach\nalso achieves the best performance on this dataset.\nComparing it with BERT-Direct demonstrates the\nbeneﬁt of the samples automatically labeled with\nBERT MLM.\nHowever, less improvement is achieved on\nOntoNotes than on the ultra-ﬁne entity typing\n1798\nMethod Acc Macro F1 Micro F1\nUFET 59.5 76.8 71.8\nLTRFET 63.8 82.9 77.3\nLDET 64.9 84.5 79.2\nDSAM 66.06 83.07 78.19\nBERT-Direct 63.25 80.84 75.90\nOurs 67.44 85.44 80.35\nTable 7: Performance of different approaches on\nOntonotes. We report strict accuracy, macro-averaged\nF1, and micro-averaged F1.\ndataset. We think there are two main reasons. First,\nOntoNotes uses a much smaller entity type set\n(89 types) than the ultra-ﬁne entity typing dataset\n(10,331 types). As a result, some ﬁner grained\ntypes that can be produced by our approach be-\ncome less beneﬁcial. Second, generating type la-\nbels that are highly dependent on the context (e.g.,\ntypes like criminal, speaker) is an advantage of our\napproach, and the ultra-ﬁne entity typing dataset\ncontains more such type labels.\n6 Conclusion\nIn this work, we propose a new approach to auto-\nmatically generate ultra-ﬁne entity typing labels.\nGiven a sentence that contains a mention, we insert\na hypernym extraction pattern with a “[MASK]” to-\nken in it, so that a pretrained BERT MLM may pre-\ndict hypernyms of the mention for “[MASK].” Mul-\ntiple patterns are used to produce better labels for\neach mention. We also propose to use a weighted\nloss and perform a self-training step to learn better\nentity typing models. Experimental results show\nthat our approach greatly outperforms state-of-the-\nart systems. Additionally, we also apply our ap-\nproach to traditional ﬁne-grained entity typing, and\nverify its effectiveness with experiments.\nAcknowledgments\nThis paper was supported by the NSFC Grant (No.\nU20B2053) from China, the Early Career Scheme\n(ECS, No. 26206717), the General Research Fund\n(GRF, No. 16211520), and the Research Impact\nFund (RIF, No. R6020-19 and No. R6021-20) from\nthe Research Grants Council (RGC) of Hong Kong,\nwith special thanks to the WeChat-HKUST WHAT\nLab on Artiﬁcial Intelligence Technology.\nReferences\nTongfei Chen, Yunmo Chen, and Benjamin Van Durme.\n2020. Hierarchical entity typing via multi-level\nlearning to rank. In Proceedings of ACL, pages\n8465–8475.\nEunsol Choi, Omer Levy, Yejin Choi, and Luke Zettle-\nmoyer. 2018. Ultra-ﬁne entity typing. In Proceed-\nings of ACL, pages 87–96.\nHongliang Dai, Donghong Du, Xin Li, and Yangqiu\nSong. 2019. Improving ﬁne-grained entity typing\nwith entity linking. In Proceedings of EMNLP-\nIJCNLP, pages 6211–6216.\nHongliang Dai, Yangqiu Song, and Xin Li. 2020. Ex-\nploiting semantic relations for ﬁne-grained entity\ntyping. In Automated Knowledge Base Construc-\ntion.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of NAACL-HLT, pages 4171–\n4186.\nDan Gillick, Nevena Lazic, Kuzman Ganchev, Jesse\nKirchner, and David Huynh. 2014. Context-\ndependent ﬁne-grained entity type tagging. arXiv\npreprint arXiv:1412.1820.\nMarti A Hearst. 1992. Automatic acquisition of hy-\nponyms from large text corpora. In Proceedings of\nCOLING.\nYanfeng Hu, Xue Qiao, Xing Luo, and Chen Peng.\n2020. Diversiﬁed semantic attention model for ﬁne-\ngrained entity typing. IEEE Access.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nHailong Jin, Lei Hou, Juanzi Li, and Tiansi Dong.\n2019. Fine-grained entity typing via hierarchical\nmulti graph convolutional networks. In Proceedings\nof EMNLP-IJCNLP, pages 4970–4979.\nMitchell Koch, John Gilmer, Stephen Soderland, and\nDaniel S. Weld. 2014. Type-aware distantly super-\nvised relation extraction with linked arguments. In\nProceedings of EMNLP, pages 1891–1901.\nChin Lee, Hongliang Dai, Yangqiu Song, and Xin Li.\n2020. A chinese corpus for ﬁne-grained entity typ-\ning. In Proceedings of LREC, pages 4451–4457.\nYing Lin and Heng Ji. 2019. An attentive ﬁne-grained\nentity typing model with latent type representation.\nIn Proceedings of EMNLP-IJCNLP, pages 6198–\n6203.\nXiao Ling, Sameer Singh, and Daniel S Weld. 2015.\nDesign challenges for entity linking. Transactions\nof the Association for Computational Linguistics,\n3:315–328.\n1799\nXiao Ling and Daniel S Weld. 2012. Fine-grained en-\ntity recognition. In Proceedings of AAAI, volume 12,\npages 94–100.\nShikhar Murty, Patrick Verga, Luke Vilnis, Irena\nRadovanovic, and Andrew McCallum. 2018. Hier-\narchical losses and new resources for ﬁne-grained\nentity typing and linking. In Proceedings of ACL,\npages 97–109.\nYasumasa Onoe, Michael Boratko, and Greg Durrett.\n2021. Modeling ﬁne-grained entity types with box\nembeddings. arXiv preprint arXiv:2101.00345.\nYasumasa Onoe and Greg Durrett. 2019. Learning to\ndenoise distantly-labeled data for entity typing. In\nProceedings of NAACL-HLT, pages 2407–2417.\nYasumasa Onoe and Greg Durrett. 2020. Interpretable\nentity representations through large-scale typing. In\nProceedings of EMNLP, pages 612–624.\nRobert Parker, David Graff, Junbo Kong, Ke Chen, and\nKazuaki Maeda. 2011. English gigaword ﬁfth edi-\ntion. Linguistic Data Consortium.\nFabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of EMNLP-IJCNLP,\npages 2463–2473.\nXiang Ren, Wenqi He, Meng Qu, Clare R V oss, Heng\nJi, and Jiawei Han. 2016. Label noise reduction in\nentity typing by heterogeneous partial-label embed-\nding. In Proceedings of ACM SIGKDD, pages 1825–\n1834.\nJulian Seitner, Christian Bizer, Kai Eckert, Stefano\nFaralli, Robert Meusel, Heiko Paulheim, and Si-\nmone Paolo Ponzetto. 2016. A large database of\nhypernymy relations extracted from the web. InPro-\nceedings of LREC, pages 360–367.\nJi Xin, Yankai Lin, Zhiyuan Liu, and Maosong Sun.\n2018. Improving neural ﬁne-grained entity typing\nwith knowledge attention. In Proceedings of AAAI,\nvolume 32.\nWenhan Xiong, Jiawei Wu, Deren Lei, Mo Yu, Shiyu\nChang, Xiaoxiao Guo, and William Yang Wang.\n2019. Imposing label-relational inductive bias for\nextremely ﬁne-grained entity typing. In Proceed-\nings of NAACL-HLT, pages 773–784.\nMohamed Amir Yosef, Sandro Bauer, Johannes Hof-\nfart, Marc Spaniol, and Gerhard Weikum. 2012.\nHyena: Hierarchical type classiﬁcation for entity\nnames. In Proceedings of COLING, pages 1361–\n1370.\nYunyi Zhang, Jiaming Shen, Jingbo Shang, and Jiawei\nHan. 2020. Empower entity set expansion via lan-\nguage model probing. In Proceedings of ACL, pages\n8151–8160.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6946758031845093
    },
    {
      "name": "Natural language processing",
      "score": 0.5998546481132507
    },
    {
      "name": "Computational linguistics",
      "score": 0.5527431964874268
    },
    {
      "name": "Linguistics",
      "score": 0.5372338891029358
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.5335206389427185
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4443328380584717
    },
    {
      "name": "Joint (building)",
      "score": 0.4298285245895386
    },
    {
      "name": "Programming language",
      "score": 0.334454208612442
    },
    {
      "name": "Engineering",
      "score": 0.14889463782310486
    },
    {
      "name": "Philosophy",
      "score": 0.09304893016815186
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    }
  ]
}