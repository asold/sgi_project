{
  "title": "Robot tool use: A survey",
  "url": "https://openalex.org/W4316363533",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2888217968",
      "name": "Meiying Qin",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2795867696",
      "name": "Jake Brawer",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A1615599874",
      "name": "Brian Scassellati",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2888217968",
      "name": "Meiying Qin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2795867696",
      "name": "Jake Brawer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1615599874",
      "name": "Brian Scassellati",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2763283821",
    "https://openalex.org/W2412396384",
    "https://openalex.org/W2210658236",
    "https://openalex.org/W4249956778",
    "https://openalex.org/W2136080516",
    "https://openalex.org/W796238542",
    "https://openalex.org/W2737549438",
    "https://openalex.org/W2076235166",
    "https://openalex.org/W3130073106",
    "https://openalex.org/W2008925121",
    "https://openalex.org/W2963149945",
    "https://openalex.org/W3087773504",
    "https://openalex.org/W2159830351",
    "https://openalex.org/W1565703369",
    "https://openalex.org/W3010694140",
    "https://openalex.org/W2100607994",
    "https://openalex.org/W2121588500",
    "https://openalex.org/W2144415816",
    "https://openalex.org/W2409171770",
    "https://openalex.org/W2772168088",
    "https://openalex.org/W2037968013",
    "https://openalex.org/W3004314363",
    "https://openalex.org/W2033546134",
    "https://openalex.org/W6635997582",
    "https://openalex.org/W2889759086",
    "https://openalex.org/W2409068304",
    "https://openalex.org/W2775067858",
    "https://openalex.org/W2970710335",
    "https://openalex.org/W6762821728",
    "https://openalex.org/W2565678376",
    "https://openalex.org/W2558961194",
    "https://openalex.org/W2964277872",
    "https://openalex.org/W3134664367",
    "https://openalex.org/W2657144928",
    "https://openalex.org/W3128894241",
    "https://openalex.org/W2075673359",
    "https://openalex.org/W2321135324",
    "https://openalex.org/W2004727380",
    "https://openalex.org/W2077290818",
    "https://openalex.org/W1969694787",
    "https://openalex.org/W3084214343",
    "https://openalex.org/W2039967891",
    "https://openalex.org/W2563442528",
    "https://openalex.org/W2010873124",
    "https://openalex.org/W3003318685",
    "https://openalex.org/W6693481431",
    "https://openalex.org/W995852687",
    "https://openalex.org/W2136719407",
    "https://openalex.org/W2110304639",
    "https://openalex.org/W6759245520",
    "https://openalex.org/W2084932815",
    "https://openalex.org/W2093851526",
    "https://openalex.org/W2500440871",
    "https://openalex.org/W1988558268",
    "https://openalex.org/W6697727265",
    "https://openalex.org/W2041108426",
    "https://openalex.org/W3206464427",
    "https://openalex.org/W1978825553",
    "https://openalex.org/W2155259778",
    "https://openalex.org/W2108579172",
    "https://openalex.org/W4226053429",
    "https://openalex.org/W2783693550",
    "https://openalex.org/W2125079768",
    "https://openalex.org/W6764969207",
    "https://openalex.org/W2165308133",
    "https://openalex.org/W2054949188",
    "https://openalex.org/W3040183635",
    "https://openalex.org/W2606803570",
    "https://openalex.org/W6678621847",
    "https://openalex.org/W3206812026",
    "https://openalex.org/W2020649037",
    "https://openalex.org/W2067928539",
    "https://openalex.org/W6752614172",
    "https://openalex.org/W2998994734",
    "https://openalex.org/W4210922806",
    "https://openalex.org/W2021570254",
    "https://openalex.org/W2737944367",
    "https://openalex.org/W2963802910",
    "https://openalex.org/W1972276901",
    "https://openalex.org/W2952182464",
    "https://openalex.org/W1969114834",
    "https://openalex.org/W6609127383",
    "https://openalex.org/W6760546089",
    "https://openalex.org/W1574909006",
    "https://openalex.org/W2736480544",
    "https://openalex.org/W2789839425",
    "https://openalex.org/W2984756091",
    "https://openalex.org/W1990352827",
    "https://openalex.org/W2028798328",
    "https://openalex.org/W2155217025",
    "https://openalex.org/W2039541017",
    "https://openalex.org/W2144576818",
    "https://openalex.org/W6786287230",
    "https://openalex.org/W1524405667",
    "https://openalex.org/W2143445032",
    "https://openalex.org/W2156393371",
    "https://openalex.org/W2098686244",
    "https://openalex.org/W6766573671",
    "https://openalex.org/W6766144634",
    "https://openalex.org/W2047360783",
    "https://openalex.org/W2036640283",
    "https://openalex.org/W4248647548",
    "https://openalex.org/W2540400161",
    "https://openalex.org/W6680971464",
    "https://openalex.org/W2010762902",
    "https://openalex.org/W2161395589",
    "https://openalex.org/W2172158418",
    "https://openalex.org/W1517823811",
    "https://openalex.org/W2127107099",
    "https://openalex.org/W2772033348",
    "https://openalex.org/W4200288823",
    "https://openalex.org/W4298128152",
    "https://openalex.org/W3091470805",
    "https://openalex.org/W2910445581",
    "https://openalex.org/W6653240293",
    "https://openalex.org/W2120044059",
    "https://openalex.org/W6646961928",
    "https://openalex.org/W6840307717",
    "https://openalex.org/W2145395202",
    "https://openalex.org/W2963534555",
    "https://openalex.org/W2081857580",
    "https://openalex.org/W2793904209",
    "https://openalex.org/W2912216687",
    "https://openalex.org/W6732025202",
    "https://openalex.org/W2587007321",
    "https://openalex.org/W130216483",
    "https://openalex.org/W2342768878",
    "https://openalex.org/W3207832698",
    "https://openalex.org/W4206785111",
    "https://openalex.org/W6677682937",
    "https://openalex.org/W2120216773",
    "https://openalex.org/W6675786215",
    "https://openalex.org/W6642536418",
    "https://openalex.org/W2063533050",
    "https://openalex.org/W2047346158",
    "https://openalex.org/W2067383678",
    "https://openalex.org/W2102959966",
    "https://openalex.org/W6680267968",
    "https://openalex.org/W1483356636",
    "https://openalex.org/W2085978998",
    "https://openalex.org/W1966783348",
    "https://openalex.org/W132725453",
    "https://openalex.org/W305287582",
    "https://openalex.org/W2533802761",
    "https://openalex.org/W6750895817",
    "https://openalex.org/W2126224645",
    "https://openalex.org/W2577148968",
    "https://openalex.org/W6674718939",
    "https://openalex.org/W4283260218",
    "https://openalex.org/W2890051203",
    "https://openalex.org/W2082711337",
    "https://openalex.org/W2805883505",
    "https://openalex.org/W2292964075",
    "https://openalex.org/W1567352255",
    "https://openalex.org/W6637884514",
    "https://openalex.org/W2737037882",
    "https://openalex.org/W2097556395",
    "https://openalex.org/W2899555241",
    "https://openalex.org/W2937206389",
    "https://openalex.org/W3130795169",
    "https://openalex.org/W2022080072",
    "https://openalex.org/W2754217830",
    "https://openalex.org/W6640120858",
    "https://openalex.org/W2796864868",
    "https://openalex.org/W4210700398",
    "https://openalex.org/W2576004467",
    "https://openalex.org/W4285546305",
    "https://openalex.org/W3098499184",
    "https://openalex.org/W2501319790",
    "https://openalex.org/W2285395050",
    "https://openalex.org/W3177247973",
    "https://openalex.org/W2766373678",
    "https://openalex.org/W4230853744",
    "https://openalex.org/W1578056686",
    "https://openalex.org/W1601360341",
    "https://openalex.org/W3150997492",
    "https://openalex.org/W4237665350",
    "https://openalex.org/W4241095316",
    "https://openalex.org/W3127352841",
    "https://openalex.org/W4232035701",
    "https://openalex.org/W3098609708",
    "https://openalex.org/W4255088250",
    "https://openalex.org/W4376496912",
    "https://openalex.org/W4242073862",
    "https://openalex.org/W2140801763",
    "https://openalex.org/W4234552385",
    "https://openalex.org/W2807591401",
    "https://openalex.org/W4283704716",
    "https://openalex.org/W1933657216",
    "https://openalex.org/W2582998992",
    "https://openalex.org/W4376523934",
    "https://openalex.org/W2324959332",
    "https://openalex.org/W2946078727",
    "https://openalex.org/W3149322120",
    "https://openalex.org/W3146490120",
    "https://openalex.org/W3098152091"
  ],
  "abstract": "Using human tools can significantly benefit robots in many application domains. Such ability would allow robots to solve problems that they were unable to without tools. However, robot tool use is a challenging task. Tool use was initially considered to be the ability that distinguishes human beings from other animals. We identify three skills required for robot tool use: perception, manipulation, and high-level cognition skills. While both general manipulation tasks and tool use tasks require the same level of perception accuracy, there are unique manipulation and cognition challenges in robot tool use. In this survey, we first define robot tool use. The definition highlighted the skills required for robot tool use. The skills coincide with an affordance model which defined a three-way relation between actions, objects, and effects. We also compile a taxonomy of robot tool use with insights from animal tool use literature. Our definition and taxonomy lay a theoretical foundation for future robot tool use studies and also serve as practical guidelines for robot tool use applications. We first categorize tool use based on the context of the task. The contexts are highly similar for the same task (e.g., cutting) in non-causal tool use , while the contexts for causal tool use are diverse. We further categorize causal tool use based on the task complexity suggested in animal tool use studies into single-manipulation tool use and multiple-manipulation tool use . Single-manipulation tool use are sub-categorized based on tool features and prior experiences of tool use. This type of tool may be considered as building blocks of causal tool use. Multiple-manipulation tool use combines these building blocks in different ways. The different combinations categorize multiple-manipulation tool use. Moreover, we identify different skills required in each sub-type in the taxonomy. We then review previous studies on robot tool use based on the taxonomy and describe how the relations are learned in these studies. We conclude with a discussion of the current applications of robot tool use and open questions to address future robot tool use.",
  "full_text": "TYPE Review\nPUBLISHED 16 January 2023\nDOI 10.3389/frobt.2022.1009488\nOPEN ACCESS\nEDITED BY\nChenguang Yang,\nUniversity of the West of England, United\nKingdom\nREVIEWED BY\nKai-Chieh Lin,\nTatung University, Taiwan\nFrank Guerin,\nUniversity of Surrey, United Kingdom\n*CORRESPONDENCE\nMeiying Qin,\nmeiying.qin@yale.edu,\nmeiyingqin@gmail.com\nSPECIALTY SECTION\nThis article was submitted to Robotic\nControl Systems, a section of the journal\nFrontiers in Robotics and AI\nRECEIVED 05 September 2022\nACCEPTED 28 December 2022\nPUBLISHED 16 January 2023\nCITATION\nQin M, Brawer J and Scassellati B (2023),\nRobot tool use: A survey.\nFront. Robot. AI 9:1009488.\ndoi: 10.3389/frobt.2022.1009488\nCOPYRIGHT\n© 2023 Qin, Brawer and Scassellati. This is\nan open-access article distributed under\nthe terms of the Creative Commons\nAttribution License (CC BY) . The use,\ndistribution or reproduction in other\nforums is permitted, provided the original\nauthor(s) and the copyright owner(s) are\ncredited and that the original publication in\nthis journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted\nwhich does not comply with these terms.\nRobot tool use: A survey\nMeiying Qin *, Jake Brawer and Brian Scassellati\nYale Social Robotics Lab, Department of Computer Science, Yale University, New Haven, CT, United States\nUsing human tools can significantly benefit robots in many application domains.\nSuch ability would allow robots to solve problems that they were unable to without\ntools. However, robot tool use is a challenging task. Tool use was initially considered\nto be the ability that distinguishes human beings from other animals. We identify\nthree skills required for robot tool use: perception, manipulation, and high-level\ncognition skills. While both general manipulation tasks and tool use tasks require\nthe same level of perception accuracy, there are unique manipulation and cognition\nchallenges in robot tool use. In this survey, we first define robot tool use. The\ndefinition highlighted the skills required for robot tool use. The skills coincide with\nan affordance model which defined a three-way relation between actions, objects,\nand effects. We also compile a taxonomy of robot tool use with insights from animal\ntool use literature. Our definition and taxonomy lay a theoretical foundation for\nfuture robot tool use studies and also serve as practical guidelines for robot tool\nuse applications. We first categorize tool use based on the context of the task. The\ncontexts are highly similar for the same task (e.g., cutting) in non-causal tool use ,\nwhile the contexts for causal tool use are diverse. We further categorize causal tool\nuse based on the task complexity suggested in animal tool use studies into single-\nmanipulation tool use and multiple-manipulation tool use . Single-manipulation\ntool use are sub-categorized based on tool features and prior experiences of tool\nuse. This type of tool may be considered as building blocks of causal tool use.\nMultiple-manipulation tool use combines these building blocks in different ways.\nThe different combinations categorize multiple-manipulation tool use. Moreover,\nwe identify different skills required in each sub-type in the taxonomy. We then\nreview previous studies on robot tool use based on the taxonomy and describe\nhow the relations are learned in these studies. We conclude with a discussion of\nthe current applications of robot tool use and open questions to address future\nrobot tool use.\nKEYWORDS\nsurvey, robot tool use, human tools, manipulation, affordance\n1 Introduction\nMany robots are designed to interact with objects in the environment. Recent advances\ngrant robots the ability to perform various tasks ranging from everyday tasks, such as swiping a\ncard(Sukhoy et al.,2012),toprofessionaltasksthatrequirehighprecision,suchasrobotsurgery\n(Sarikaya et al., 2017).\nAmong these tasks, robot tool use is gaining increasing attention. Being able to use\nhuman tools such as screwdrivers and scissors can greatly expand the applicability of a\nrobot. Household robots will be able to assist humans better by performing a wider range\nof tasks with everyday tools; robots in chemistry labs will be able to run more experiments\nby leveraging the lab tools; manufacturing robots will be able to complete more tasks by\nutilizing construction tools without the need for specialized grippers. In this survey, a\ntool refers to the object attached to a robot. A manipulandum refers to the object being\nmanipulated by the tool. Anobject is an umbrella term to include both tools and manipulanda.\nFrontiers in Robotics and AI 01 frontiersin.org\nQin et al. 10.3389/frobt.2022.1009488\nRobot tool use requires three skills. The first skill is perception.\nA robot should identify and localize tools and manipulanda from\nthe environment. For example, to drive a slotted screw, the robot\nneeds to align the slotted screwdriver with the screw. Inaccurate pose\nperception of the screw will lead to misalignment, resulting in the\nfailure of the tool use action. To successfully drive a screw, position\nknowledge alone is insufficient. In the above example, the tip of the\nscrewdriver should both be at the position of the top of the screw, and\noriented in a way that the flat tip of the screwdriver is aligned with\nthe slot of the screw. Though challenging, the perception requirement\nis not unique to tool use. General robot manipulation also requires\nsimilar perceptual capabilities (Li et al., 2022).\nThe second skill of robot tool use is manipulation. Manipulation\nskills focus on how to realize the required kinematics and dynamics\nof tool use actions. The actions include two components as defined\nin Qin et al. (2021) and demonstrated inFigure 1: the contact poses\nand the course of the action. The contact poses include tool-\nmanipulandum contact poses and gripper-tool contact poses (i.e.,\ngrasping). These poses consider both the translational (e.g., the tip\nof the pen should contact a point on the paper) and the rotational\n(e.g., the pen should contact the paper close to perpendicular to the\nplane of the paper, rather than parallel to it) relations of the tool\nand the manipulanda, or the tools and the gripper. Manipulation\nalso encompasses both the tool trajectory (i.e., a time series of poses\nof a tool) and dynamics (i.e., the forces required for successful tool\nuse). Though the manipulation skills required in tool use tasks may\nshare similarity with general manipulation tasks, tool use additionally\nrequiresthatarobotshouldupdateitsbodyschemawhenatoolisheld\n(Stoytchev, 2003). For example, general manipulation tasks consider\nexerting certain forces at the end-effector, while tool use tasks concern\nhow the forces be generated at the tool rather than at the end-effector.\nThe third skill of robot tool use is high-level cognition. This\nincludes reasoning and planning tool use actions given the tasks\nand available tools. For example, a robot may need to reason about\nhow to grasp the tool to facilitate tool use, and determine the tool-\nmanipulanda contact pose, the trajectory, and the force needed to use\ntoolssuccessfully.Therobotalsoreasonsaboutusinganoveltoolwhen\nlearned tools are unavailable. Moreover, the robot should plan to use\nmultiple tools to achieve a goal. Neurological evidence also supports\nthat different cognitive processes were involved when a human uses a\ntool compared with separate hand and tool actions (Cabrera-Álvarez\nand Clayton, 2020).\nThe unique skills required by tool use distinguish tool use from\ngeneral manipulation tasks (for a review on robot manipulation, see\nMason, 2018; Kroemer et al., 2021). In the following sections, we start\nFIGURE 1\nComponents of an action. The diagram is adapted from Qin et al. (2021),\nCC BY.\nwith the discussion of what is tool use in robotics. Defining robot tool\nuse is not a trivial task and we gain insights in animal studies which\nhas a long history of studying tool use, especially how they distinguish\ntoolusefromgeneralmanipulationsinanimalbehaviors.In Section 2,\nwe present key results in animal studies and describe our definition\nof robot tool use. The definition not only sets a boundary for tool\nuse, but also illuminates important skills of robot tool use. Moreover,\nresearchers in animal studies also categorized different types of tool\nuse and summarized the different skills required in each type of tool\nuse.Withthisknowledge,wecompileatoolusetaxonomyin Section 3\nand describe the skills required by different sub-types of tool use tasks\ninSection 4.Thedefinitionandtaxonomysetatheoreticalfoundation\nfor robot tool use, which is currently lacking in robotics research and\nacts. They also serve as a practical guideline for future applications. In\nSection 5, we organize robot tool use studies based on our taxonomy\nand focus on 1) the unique challenges of tool use tasks compared\nwith general manipulation tasks in learning the required skills and\n2) current advancements in how the skills are learned. We conclude\nthis survey by discussing current applications of robot tool use and\nidentifying the open challenges remaining in robot tool use.\n2 Definition of robot tool use\nDefining robot tool use is necessary to understand the uniqueness\nof tool use as described inSection 1. Although the concept of robot\ntool use is intuitive, it is challenging to provide a precise definition to\nshow clear distinctions from general manipulations. For example, one\nmayconsiderrobottooluseasrobotsusingobjectsintheenvironment\nto achieve a goal. Under this definition, robots using refrigerators to\nstore food should count as robot tool use. Along this line of logic,\nrobots using the floor to support themselves should also be considered\nas robot tool use, which is counter-intuitive.\nResearchers in animal tool use also encountered similar\nchallenges. Tool use was initially considered a unique behavior only\nshown in humans (Oakley, 1944). With observations of tool use in\nanimals, researchers debated which instances are genuinely tool use.\nFor example, some may argue that a chimpanzee using a rock to crack\na nut is tool use but cracking a nut against an anvil is not, while others\nmay consider both cases are tool use. The debate urged a precise\ndefinition of animal tool use. In this section, we present key results\nin animal studies, gain insights from their argument, describe our\ndefinition of robot tool use, and explore the necessary aspects of tool\nuse based on our definition. We focus on the implications of these\nanimal studies for robot tool use, rather than on the implications for\nanimal cognition. Therefore, the review of animal tool use studies is\nnot meant to be comprehensive.\nVan Lawick-Goodall (1970, p. 195) defined tool use as “the use of\nan external object as a functional extension of mouth or beak, hand or\nclaw, in the attainment of an immediate goal,” emphasizing the goal-\noriented and functional character of tool-use.Alcock (1972, p. 464)\nrevisedthedefinitionbyspecifyingthekindofobjectsthatcanbeused\nas tools and identifying the scope of the goals: “Tool-using involves\nthe manipulation of an inanimate object, not internally manufactured,\nwith the effect of improving the animal’s efficiency in altering the form\nor position of some separate object.”\nBeck (1980) identified a number of shortcomings with these\ndefinitions. First, only objects that are portable and manipulable\nshould be considered as tools. Under this definition, dropping a\nFrontiers in Robotics and AI 02 frontiersin.org\nQin et al. 10.3389/frobt.2022.1009488\nstone on an egg would be considered an example of tool use, but\npounding a fruit on a tree would not be. The latter case is considered\nproto-tool-use (Parker and Gibson, 1977). Second, an agent should\nunderstand the connection between the goal and the tool. Otherwise,\nthe conditioned behavior of a rat pressing a lever in a Skinner box\nwould be considered tool use, and Beck considered this inappropriate.\nThird, the tool need not be externally manufactured to the agent using\nit nor inanimate. Researchers observed that captive apes threw feces\ntoward human intruders, and a chimpanzee utilized the dead body\nof a colobus monkey to hit a conspecific, suggesting that a live ape\ncould be utilized in a similar way. Beck argued that these behaviors\nshould be considered tool use. Fourth, the goal of tool use can be\nextended beyond feeding or drinking to other goals such as self-\nmaintenance. As a result, Beck (1980, p. 10) re-defined tool use as\n“the external employment of an unattached environmental object to\naltermoreefficientlytheform,position,orconditionofanotherobject,\nanother organism, or the user itself, when the user holds or carries the\ntool during or just prior to use and is responsible for the proper and\neffective orientation of the tool.”\nFor decades, Beck’s definition has been accepted widely in the\nfield of animal cognition and was even adopted in early robot\ntool use studies (e.g.,Stoytchev, 2007). Two observations motivated\nSt. Amant and Horton (2008)to propose a new definition of tool use.\nKrützen et al. (2005) reported that dolphins hold marine sponges in\ntheir rostrum in order to prevent potential injuries when probing for\nfood. Breuer et al. (2005)observed that a wild gorilla tested the depth\nofwaterwithastickwhileitwalkedacrossapond.Thesetwobehaviors\nfall outside of Beck’s definition of tool use since they do not involve\naltering the state of another object. St. Amant and Horton were also\nconcerned about Beck’s definition that it over-emphasized peripheral\naspects of tool use, such as the unattached property; an animal can\nuse a stick that is still attached to a tree as a tool. Moreover, they\nargued that Beck’s definition is vague to determine whether a goal\nwas achieved accidentally. They observed that purposeful behaviors\nrequire a continuum of control. Therefore, St. Amant and Horton\n(2008, p. 1203) re-defined tool use as “the exertion of control over\na freely manipulable external object (the tool) with the goal of 1)\nalteringthephysicalpropertiesofanotherobject,substance,surfaceor\nmedium (the target, which may be the tool user or another organism)\nvia a dynamic mechanical interaction, or 2) mediating the flow of\ninformation between the tool user and the environment or other\norganisms in the environment.” They elaborated that the interactions\nbetween tools and manipulanda should be dynamic. Under this\ndefinition, stacking boxes to reach bananas is not tool use since the\ninteractionsbetweenboxesremainsfixedoncetheyhavebeenstacked,\nwhile cracking a nut with rock is tool use because the interactions\nbetween the nut and the rock is constantly changing.\nShumaker and Walkup joined Beck to revise the Beck’s widely\naccepted definition and incorporated St. Amant and Horton’s\nargument: “the external employment of an unattached or manipulable\nattached environmental object to alter more efficiently the form,\nposition, or condition of another object, another organism, or the user\nitself, when the user holds and directly manipulates the tool during or\nprior to use and is responsible for the proper and effective orientation\nof the tool.” (Shumaker et al., 2011, p. 36) We based our definition of\nrobot tool use on this revised definition.\nOther definitions of tool use in animal studies exist. Some\nmay simply be shorter versions of these definitions (e.g.,Chevalier-\nSkolnikoff, 1989; Matsuzawa, 1999). Others may disagree with the\nscope of tool use. For example,Asano (1994)did not restrict the tools\nto be something being held. This might result in the scope of tool use\nbeing overly broad since any behavior may eventually count as tool\nuse, such as walking, which utilizes the ground as the “tool”.Lestel\nand Grundmann (1999)expands the scope of tool use even more by\nincluding abstract concepts such as culture as potential tools. These\ndiscussions may be too philosophical and lack operational details for\nrobotics research.\nWe identify three essential points in these definitions. First,\ntool use must have a goal, despite a lack of consensus regarding a\ngoal’s scope. Second, instead of achieving a goal through random\nexploration, an agent utilizing a tool should understand the\nconnection between the goal and the behavior. Third, the tool should\nsatisfy specific physical criteria, such as being freely manipulable.\nBased on these points, we definerobot tool useas:\nA robot attaches or secures to its end-effector an external,\nunanimated, freely available object or an object attached to\nanother object, in order to achieve a goal of altering the\nstate of another object, updating its own state, or other goals,\nthrough purposeful manipulations.\nOur definition adopts Shumaker et al.‘s definition with minor\nmodifications.First,werestrictthetoolstobeexternallymanufactured\nand unanimated. Unlike living creatures, a robot typically does not\nproduce materials (e.g., feces, spider webs) from its body. We require\nthe tools to be unanimated because an animated object that a robot\nwould most likely manipulate is another robot. We consider this a\nbetter fit to the area of multi-agent systems rather than tool use\nsince it involves synchronization and communication between robots.\nSecond, we relax the interactions between the tool and object to be\nmanipulated to be dynamic or static. Therefore, using a container\nto relocate other objects would count as tool use. Third, we relax\nthe goal of tool use. The scope of the goals in animal tool use was\nsummarized based on animal behavioral observations. Given that tool\nuse in animals is structurally simple even in non-human primates\n(Fragaszy and Eshchar, 2017), the goals in the above definitions are\nrestricted to altering the state of another object and updating one’s\nknowledge about the environment. In contrast, as robots often utilize\nhuman tools, robot tool use is motivated by the same goals for which\nthese tools were designed, goals that can far exceed in scope and\ncomplexity those observed in animal studies. On the contrary, robots\nare required to utilize human tools. The design of human tools is more\ncomplex than those used by animals so that human tools may serve\npurposes beyond the goals identified in animal studies. Therefore, we\nprefer not to restrict the scope of the goal of robot tool use.\n3 A taxonomy of robot tool use\nResearchers of animal tool use recognizes that there are different\ntypes of tool use. For example, antlions mechanically throwing sand\nto capture preys in the same manner across all contexts is notably\ndifferent from chimpanzees carefully adjusting sticks to fish termites\neven in the same context. Similarly, robot tool use also has many\ndifferent types. A robot being pre-programmed to cut pizza with a\nparticular knife is very different from it learning to adjust the gestures\nwhen presented with different knifes. In this section, we overview\nFrontiers in Robotics and AI 03 frontiersin.org\nQin et al. 10.3389/frobt.2022.1009488\ntaxonomies proposed in animal and robotics studies, and present a\ntaxonomy on robot tool use.\n3.1 Taxonomy of animal tool use\nAlcock (1972) proposed a dichotomy of tool use in animals:\nstereotyped tool useis seen mostly in invertebrates and fish andflexible\ntool use is typically seen in birds and mammals. Hunt et al. (2013)\nconsideredthisdichotomyanaccuratedescriptionoftwofundamental\ntypes of tool use with different underlying processes, despite being\noversimplified. Stereotyped tool use is inherited and animals only\nutilize tools in default ways in particular contexts. Examples of\nstereotyped tool use include antlions throwing sand to capture preys\n(Alcock, 1972). The species of antlions developed this behavior from\nthe pre-existing non-tool use behavior of random sand flicking in\nordertomaintaintheirpits.Thetoolusebehaviorofantlionsthrowing\nsand evolves as a phenotypic change in this species. As a result, these\nbehaviors are widespread across the species of antlions and rarely vary\nwithin and across individuals.\nIn contrast, flexible tool use, which is also referred to as creative\ntool use, is learning-based that animals explicitly reasoning about the\nusages based on the context. It is this type of tool use that some believe\nsignals intelligence (Call, 2013) and interests researchers in animal\ncognition.Chimpanzees’crackingnutswithrocksandfishingtermites\nwithsticksareexamplesofflexibletooluse( Biro et al.,2003;Lonsdorf,\n2006), as a juvenile chimpanzee acquires such skills by observing its\nparent(s).Therefore,thelearninghappensattheleveloftheindividual,\nrather than at the level of genus. Indeed, each instance of nut cracking\nor fishing termites can be differentiated even within the same context\nby the same chimpanzee. Unlike stereotyped tool use, flexible tool use\ndoesnotsharecontext-dependencyandthuscanoccuracrossdifferent\ncontexts.Wesummarizedthedifferencesbetweenstereotypedtooluse\nand flexible tool use suggested by Hunt et al. inTable 1.\nCall (2013) further identified different types of flexible tool use\nfrom the perspective of problem-solving in terms of creativity and\nadaptivity.\n• Solving novel problems with old solutions;\n• Solving old problems with novel solutions;\n• Solving novel problems with novel solutions.\nSolutions may include utilizing one tool, selecting a tool from\navailable options, manufacturing a novel tool, or using multiple tools\nsequentially.\nIn contrast to the above taxonomies, Wimpenny et al. (2009)\ncategorized tool use based on the number of tools involved in a\nproblem but overlooked the complexity of the decision process.\nBoesch (2013)categorized tool use based on four levels of increasing\ncomplexity though in some sense reminiscent of Wimpenny et al.‘s\napproach.\n• Simple tool use: Using one tool, e.g., a chimpanzee uses a twig\nfor fishing termites (Goodall, 1964). The animal only needs to\nunderstand the connection between itself and the rewardvia the\ntool, which is a first-order problem (Visalberghi and Fragaszy,\n2006);\n• Combined tool use : Using two tools simultaneously, e.g., a\ncapuchin monkey uses a rock to pound a nut on a hard\nsurface (Spagnoletti et al., 2011). The animal needs to consider\nboth spatial relationships concurrently to connect itself with\nthe reward, which is a second-order problem (Visalberghi and\nFragaszy, 2006);\n• Sequential tool use : Using multiple tools one after another,\nincluding using a tool for manufacturing another tool, e.g., a\nchimpanzee using multiple tools in sequence to break a bee\nhive, open honey chambers, and extract the honey. This behavior\nnot only requires the animal to keep in mind multiple causal\nrelationships sequentially and choose the correct sequence, but\nalso imposes temporal delay for the reward;\n• Composite tool use: Combining multiple tools to use as one tool,\na tool use behavior yet to be discovered in animals and currently\nunique to humans.\n3.2 Taxonomy of robot tool use\nWhiletheabovetaxonomiesarebasedonanimal studies, Tee et al.\n(2018,2022)proposedacategorizationbasedondefaultusagesoftools\nin robotics, and identified three types of tools: category-I tools that\n“help to amplify/augment certain kinematic or dynamic aspects of\nfunctions that are already in an agents repertoire.” (p. 6439), category-\nIItoolsthataresimilartocategory-Itoolsbut“requireactionsdifferent\nfrom what the agent would have performed, without the tool, to\nachieve these functions.” (p. 6439), and category-III “provide new\nfunctions that a human cannot perform without a tool.” (p. 6440)\nAs an example, they categorized a vacuum cleaner as a Category-III\ntool because a robot cannot perform a cleaning task without this tool.\nHowever, a vacuum cleaner can be used as a rake to reach objects\nor as a hammer to hit objects in other contexts. In these contexts,\nthe vacuum cleaner should be classified as category-I tools. Given\nthat this categorization does not consider contexts of tool use, it will\nbe challenging for a system following this categorization to perform\nflexible tool use, which is context-based.\nBased on the taxonomies of animal tool use and the characteristics\nof robot tool use, we devise a taxonomy as shown inFigure 2. We\ncategorize robot tool use intonon-causal tool useand causal tool use,\nwhich are similar to stereotyped tool use and flexible tool use in\nanimals, respectively. We changed the terminology for two reasons.\nFrist,wewouldliketoemphasizethefundamentaldifferencesbetween\nthe two types of tool use behaviors in robots regarding whether robots\nshould understand required causal relations, which are elaborated in\nSection 4. Second, though we consider it necessary for a robot to\nunderstand required causal relations in order to achieve behaviors\nsimilar to flexible tool use, there is a lack of evidence showing the\nmechanism of flexible tool use in animals. Therefore, we would like\nto avoid claiming that flexible tool use in animals is causal-based, and\nsuch discussion is beyond the scope of this survey.\nWe further categorize causal tool use intosingle-manipulation tool\nuse and multiple-manipulation tool usebased on Boesch’s taxonomy.\nA single manipulation refers to being presented with a single tool and\nusing the tool to perform one action (e.g., pushing, scooping) in order\nto achieve one goal, though a robot may observe the usage of multiple\ntools to learn a task. Multiple manipulations may involve one or any\ncombinations of multiple tools, multiple actions, and goals consisting\nof multiple sub-goals.\nInspired by Call’s taxonomy, we categorize single-manipulation\ntool use intobasic tool use, transferable tool use, improvisatory tool use,\nFrontiers in Robotics and AI 04 frontiersin.org\nQin et al. 10.3389/frobt.2022.1009488\nTABLE 1 Comparison of stereotyped tool use and flexible tool use based on Hunt et al. (2013)’s descriptions.\nStereotyped tool use Flexible tool use\nDistribution genus level individual level\nDevelopment phenotypic changes stemed from pre-existing non-tool\nuse behaviors\nobservational learning\nVariability almost no variations within and between individuals very different within and between individuals\nFIGURE 2\nT ool use taxonomy.\nand deductive tool use. Basic tool use is the most basic form of tool\nuse. In basic tool use, a robot uses a learned tool to solve a learned\ntask, such as pushing a block, striking a xylophone, and cutting a cake.\nUnlike non-causal tool use that exclusively focuses on the actions,\nbasic tool use focuses on the causal relations between actions and\neffects. Transferable tool use is a more complicated form of tool use,\nwhich aims to transfer learned tool use skills to other intra-category\nobjects that share common form factors (e.g., using mugs of different\nshapes to pour liquid into different containers). Improvisatory tool\nuse adds further complexity by generalizing learned tool use skills to\nnovel inter-category objects. These objects are generally not designed\nto perform these tasks, such as using the handle of a screwdriver in\nplace of a hammer to drive a nail. Deductive tool use concerns the\nproblem of using a novel tool to solve a novel task. A robot will not\nbe provided any prior knowledge about the tool or the task, nor given\nopportunities to learn about them from demonstrations. Instead, the\nrobot should deduct the usage of a tool from its physical knowledge\nabout the world.\nWe categorize multiple-manipulation tool use into combined\ntool use, sequential tool use, tool selection, and tool manufacturing.\nCombined tool use and sequential tool use are similar to the\ndefinitions as in Boesch’s taxonomy, though sequential tool use does\nnot include constructing a new tool in our definition as it requires\nmore sophisticated manipulation skills. Tool selection refers to the\nprocess of choosing the most appropriate tool among many options in\norder to complete a tool use task.Shumaker et al. (2011)defined tool\nmanufacturing as “simply any structural modification of an object or\nan existing tool so that the object serves, or serves more effectively, as\na tool.” This definition only includes modifying an existing object. We\ncombine this definition with composite tool use in Boesch’s taxonomy,\nand re-define tool manufacturing as the process of modifying or\ncombining objects or existing tools, with or without the usage of\nother tools, to complete a tool use task, or to complete the task more\nefficiently. DifferentfromShumakeret al.andBoesch’sdefinition,our\ndefinition also explicitly includes the possibility of utilizing other tools\nin the process of manufacturing.\nWe do not enforce a subdivision of multiple-manipulation\ntool use by difficulty, unlike a comparable category in Boesch’s\ntaxonomy. Boesch was able to rank the categories in animal tool\nuse because the types of tools leveraged by non-human animals\nare comparatively limited, and the manipulation skills in these\nanimals are usually relatively simple. In robot tool use, the difficulty\nis dependent on the actual problem to solve, rather than the\ncategory that the problem belongs. For example, utilizing two\ntools in sequence may be simpler than creating a new tool that\nrequires sophisticated manipulation skills. However, a problem that\nrequires planning to use ten tools may be more challenging than\na problem that requires a robot to combine two parts as a new\ntool.\nAs a summary, the criterion of the top-level classification is\nwhether the context exhibit much variations across instances of\ntool use in the same task. As a result, robot tool use is classified\ninto non-causal tool use and causal tool use. Causal tool use is\nfurther categorized into single-manipulation tool use and multiple-\nmanipulation tool use based on the complexity of the task, which\nis quantified by the number of actions, goals, and tools. We\ncategorize single-manipulation tool use into four sub-types. Three\nof the sub-types (i.e., basic tool use, transferable tool use, and\nimprovisatory tool use) rely on prior experiences of tool use and\nthe other (i.e., deductive tool use) only requires knowledge of\nphysical rules or experiences of general manipulation. The three\nsub-types differ from each other in terms of object features.\nSingle-manipulation tool use can be considered as building blocks\nof causal tool use, and multiple-manipulation tool use combines\nthe building blocks in different ways. The different combinations\nform the sub-types of multiple-manipulation tool use, which are\ncombined tool use, sequential tool use, tool selection, and tool\nmanufacturing.\nFrontiers in Robotics and AI 05 frontiersin.org\nQin et al. 10.3389/frobt.2022.1009488\nFIGURE 3\nThe modified affordance model in Montesano et al. (2008) © 2008, IEEE\n(reuse licence #5360371164789). Affordances as relations between\n(A)ctions, (O)bjects, and (E)ffects that can be used to address different\npurposes: predict the outcome of an action, plan actions to achieve a\ngoal, or recognize objects or actions. We update the colors of the model\nand represent manipulation skills with brown, cognition skills with pink,\nand perception skills with grey.\n4 Required skills of robot tool use\nOur definition of tool use has three important components:\nobjects, goals or desired effects, and manipulations or actions to\nachieve the goals. These three components agree with the three\ningredients of the affordance model defined by Montesano et al.\n(2008). The affordance model attempts to provide an operational\ndefinition of the concept of affordances, whose precise definition is\nstill debatable (for a review, seeJamone et al., 2016; Zech et al., 2017).\nThe concept was first introduced by Gibson (1979, p. 127) as what the\nenvironment “offers the animal, what it provides or furnishes, either\nfor good or ill”. Despite the lack of consensus around its definition,\nthe concept of affordances has facilitated much robotic research\n(Stoytchev, 2005b; Cakmak et al., 2007; Moldovan et al., 2012; 2013;\nKatz et al., 2014; Ruiz and Mayol-Cuevas, 2018 ; Lueddecke et al.,\n2019).\nThe affordance model by Montesano et al. formulated affordance\nas three-way relations between objects, actions, and effects.Figure 3\nshows our modified version of this affordance model with coloring.\nThe coloring captures the three skills required for robot tool\nuse as described in Section 1. While generating actions requires\nmanipulation skills, perceiving the effects and the objects demands\nperception skills. Understanding the connections between the nodes\nin the model needs cognition skills. The key to robot tool use is to\nunderstand tool affordances.\nEach subtype of tool use in our taxonomy addresses different\naspects of affordances and requires different skills, as shown in\nFigure 4. Non-causal tool use focuses on generating desired motions,\nwhich correspond to the action node in the affordance model. While\nthe manipulation skills are similar to the ones in general manipulation\ntasks, tool use tasks require additional manipulation skills, such as\nupdating a robot’s body schema when a tool is attached to its gripper.\nThis type of tool use requires a robot to focus on the actions, though\nwithout the need to consider the objects, the effects, or the relations.\nCausal tool use involves learning and applying tool affordances,\nwhich focuses on cognition skills. Single-manipulation tool use learns\nand reasons about affordances. Among the different types of single-\nmanipulation tool use, basic tool use learns how to achieve desired\neffects with actions. With learned relations between actions and\neffects, transferable tool use learns the relations between tools and\nactions in order to adjust actions based on novel tools that share\nsimilar form factors with the learned tools. In addition to these two\nrelations, improvisatory tool use requires a robot to understand what\nspecific tool features cause the effects so that it can generalize learned\nskills to inter-category objects. As a result, improvisatory tool use\nrequires a robot to learn the entire affordance model. These tool use\ntasks generalize tool use to novel objects by learning and inducing\naffordance from observations. In contrast, deductive tool use requires\na robot to complete a tool use task without prior knowledge using\nunlearned tools. As a result, the robot has no information to induce\naffordances and should perform deductive reasoning from general\nphysical rules. In short, basic tool use, transferable tool use, and\nimprovisatory tool use requires the incremental learning of more\nrelations from tool use demonstrations, while deductive tool use\nrequires a robot to be able to infer rather than learn the relations\nfrom demonstrations of general manipulation tasks or physical rules\nprovided directly.\nThe challenge for multiple-manipulation tool use is to apply\nthe affordances effectively rather than learning the affordances.\nTool manufacturing requires more sophisticated manipulation\nskills; sequential tool use and tool selection require higher\ncognition skills; tool manufacturing requires a high level of\nmanipulation and cognition skills. We summarize the additional\nskills required in each sub-type inTable 2, and we elaborate more in\nSection 5.3.\nAsasummary,whilenon-causaltoolusefocusesoncharacterizing\nand replicating actions,causal tooluse considerstherelationsbetween\nactions and other skills in addition. Under causal tool use category,\nsingle-manipulation tool use focuses on learning the skills and\nthe relations while multiple-manipulation applies this knowledge to\nsolve more complicated tasks and may require more sophisticated\nskills.\n5 Robot tool use literature\nIntheprevioussections,wepresentourdefinitionofrobottooluse\ndrawing from the animal studies literature. Our definition highlights\nthe three important skills in tool use, echoing the three-way relations\nbetween actions, objects, and effects in standard affordance models.\nWe also describe a taxonomy of tool use with each type of tool use\nrequiring different skills or relations.\nIn this section, we organize robot tool use literature based on\nour taxonomy. As different levels of assumptions can be made, we\nwould like to point out that techniques that focus on the unique\nchallenge in a higher-level tool use may not necessarily allow a\nrobot to handle the challenges that belong to lower-level tool use.\nIn each section, we focus on the techniques that can solve the\nunique challenge at each level of tool use. We would also like to\nemphasize that the purpose of robot tool use is not to mimic or\nmodel how animals use tools, but rather to allow robots to use\ntools.\nFrontiers in Robotics and AI 06 frontiersin.org\nQin et al. 10.3389/frobt.2022.1009488\nTABLE 2 Additional skills required in multiple-manipulation tool use beyond single-manipulation tool use.\nAdditional manipulation skills Additional cognition skills\nReasoning Planning\nCombined Tool Use Coordinate the use of multiple tools Choose appropriate parameters for each\ntool use\nN/A\nSequential Tool Use N/A N/A Plan the order of using multiple tools\nTool Selection N/A Choose appropriate tools among many\navailable tools\nN/A\nTool Manufacturing Perform the actions of assembling\ndifferent parts together as a tool, or\nmodify the current tool\nChoose appropriate parts to be assembled\nand decide where to attach each part, or\ndecide the desired state of the unmodified\ntool\nPlanning the order of the manufacturing,\nwhich may include sequential tool use\nFIGURE 4\nThe different aspects of tool affordances need to be addressed in the subtype of tool use.\n5.1 Non-causal tool use\nNon-causal tool use is the ability to use learned tools to solve\nlearnedtasks,withoutunderstandingthecause-and-effectrelationship\nbetween the actions and the goals. The purpose of non-causal tool\nuse is to duplicate or reproduce actions with limited variations.\nIt could be achieved by programming a wide variety of tool use\nactions such as nut fastening in aircraft production that requires high\nprecision (Pfeiffer et al., 2017), stub grinding and deburring with force\ncontrol (Robertsson et al., 2006), handwriting that involves multi-\ncontact manipulation (Kim et al., 2014), furniture polishing that uses\nan impedance model (Nagata et al., 2001), generating a collision-free\npolishing path (Takeuchi et al., 1993), accurately drawing a circle with\na compass that involves complex contacts (Kutsuzawa et al., 2017),\nunfastening screws in collaborative tasks (Li et al., 2020), and pouring\nbasedonthevolumeofliquid( Rozo et al.,2013),oractionsrelevantto\ntoolusesuchasgraspingakniferestingonacuttingboardthatrequires\na high level of dexterity (Xue and Jia, 2020), or segmenting a surgical\ntool from the background while using it (Garcia-Peraza-Herrera et al.,\n2017; Su et al., 2018). The purpose of these approaches is to automate\none process to facilitate human work. Therefore, the implementations\nare designed to be highly specific to the task.\nHowever, given the wide range of tasks, it is impractical to\nprogram all tool use tasks. Being able to learn these tasks is desired.\nOne approach is to treat tool use tasks the same way as general\nmanipulation tasks and learn the actions accordingly. One of the\nclassicalgorithmsoflearningactionsisdynamicmovementprimitives\n(DMP) (Schaal, 2006;Ijspeert et al., 2013). DMP leverages theconcept\nof attractors from dynamical systems, and actions are represented as\na set of linear differential equations. A more intuitive approach to\nunderstandingDMPistovisualizetheequationsasvectorfields,where\na trajectory is formed by following the vectors from a starting point\nto an end point. Each dimension may need to be learned separately\nand then coupled together. One advantage of DMP is that the shape\nof the trajectory can be distorted based on the starting point and\nthe end point. DMP and its variations have been demonstrated with\ntool use tasks such as swinging a tennis racket (Ijspeert et al., 2002;\nSchaal, 2006), playing table tennis ( Muelling et al., 2010), playing\nball-in-a-cup (Kober et al., 2008), pouring liquid (Pastor et al., 2009),\nand whiteboard cleaning (Kormushev et al., 2011). Algorithms other\nthan DMP have also been employed to represent action primitives,\nsuch as probabilistic movement primitives ( Paraschos et al., 2013)\nand Fourier movement primitives (Kulak et al., 2020). Actions were\nalso parameterized as minimal plans to facilitate action interpretation\n(Guha et al., 2013). To handle tasks that require high manipulation\nprecision such as using chopsticks, model-free imitation learning\nwas chosen ( Ke et al., 2021). For tasks that do not require high\nprecision of force or position control, indirect force controllers\n(Lutscher and Cheng, 2013) or a unified algorithm for dynamic object\nmanipulation (Tsuji et al., 2015) can be considered. While these are\nmethods designed to learn actions, general learning methods such\nas deep learning (Droniou et al., 2014; Byravan and Fox, 2017) and\nreinforcement learning (Peters and Schaal, 2006) were also used.\nWhile these studies focus on learning actions, others focus on\nsegmenting continuous actions into action primitives for tool use\n(Ramirez-Amaro et al., 2014a; Lioutikov et al., 2017). A similar line of\nresearch on general manipulation tasks is to recognize the tasks based\non the classification of actions (Ramirez-Amaro et al., 2014b; 2015;\nFrontiers in Robotics and AI 07 frontiersin.org\nQin et al. 10.3389/frobt.2022.1009488\nHu et al.,2014;WölfelandHenrich,2018 ;Shao et al.,2021;Koch et al.,\n2022). This approach attempts to ground action profiles to labels,\neither primitive labels or task labels, and do not relate actions to the\neffects on the objects being manipulated. For example, the whiteboard\nswiping action will be characterized as the translational movement of\nthe eraser in this approach, rather than the words being erased, which\nis the effect. While grounding action profiles to labels is useful in some\napplications, it does not permit causal tool use.\nThe above studies treated tool use tasks in the same manner as\ngeneral manipulation tasks. As a result, they cannot adjust actions\nbased on how the tools are grasped since they do not have tool-\nrelated knowledge. In order to accommodate the tools attached to\nthe end-effector, a robot needs to update its body schema to include\nthe tools, or in other words, to calibrate the tool in the gripper.\nPrior studies focus on updating robot kinematics by considering the\ntip of a tool [e.g.,Kemp and Edsinger (2006)], which is considered\nthe primary contact point between the tool and the environment.\nAmong these studies, some manipulated the tool with kinematic\ncontrol (Stoytchev, 2003; Nabeshima et al., 2005), and others found\nit necessary to perform dynamic control (Kemp and Edsinger, 2006;\nNabeshima et al., 2007; Jamone et al., 2013; Hoffmann et al., 2014;\nKarayiannidis et al., 2014). Despite these studies’ success, considering\nthe tool’s tip only is insufficient for all tool use tasks. For example, it\nis insufficient to know where the tip of a mug is when it is used to\npour liquid into another container. The mug needs to be tracked with\nmultiple markers attached to it (Lee et al., 2008). Another example\nis joint tools such as a pair of scissors. In this scenario, a grounded\nrelational representation of the entire tool is needed (Katz et al., 2008).\nBeyond tool calibration, other studies explored collision detection\n(Colgate et al., 1995) and obstacle avoidance (Lee and Song, 2021)\nwith tools attached to the gripper, as well as robot motion planning to\ncomplete tool use tasks (Kobayashi and Hosoe, 2009; Holladay et al.,\n2019) or planning for the grasping of the tool (Lin and Sun, 2015;\nChen et al., 2019; Raessa et al., 2019) in robot motion generation.\n5.2 Causal tool use—Single-manipulation\ntool use\n5.2.1 Basic tool use\nThough both basic tool use and non-causal tool use leverage\nlearned tools to solve learned tasks, basic tool use can adjust actions\nbased on the desired effects while non-causal tool use cannot. In other\nwords,basictooluserequiresrobotstounderstandthecausalrelations\nbetween actions and effects. For example, a robot performing basic\ntool use is able to push an object further away given a target region\nthat is further away, while a robot performing non-causal tool use will\nsimply attempt to duplicate learned actions and does not adjust the\nactions based on the target region that is further away.\nSinapov and Stoytchev (2008)conducted an early study to explore\nthe relation between actions and effects with motion babbling. They\nutilized six different tools (T-stick, L-stick, straight stick, L-hook, Y-\nhook, and an arrow-shaped tool) to relocate a puck with six pre-\ndefined exploratory behaviors (i.e., push, pull, slide-left, slide-right,\nrotate-left, and rotate-right). For each tool, the robot learned the\ndistribution of movement trajectories of the puck relative to its\nstarting location. Forestier and Oudeyer (2016)employed an active\nversion of Model Babbling to explore the distribution of manipulanda\nafter tool use with two different sticks. These studies focus on the\npotential distribution of the location of manipulanda, rather than the\none-to-onerelationshipbetweenanactionanditseffect.Therefore,itis\nchallenging to utilize tools to achieve desired effects with this method.\nOther studies learned the one-to-one relation of an action and its\neffect, though in a quantitative manner.Okada et al. (2006) focused\non verifying the effects as success or failure of tool use tasks such as\npouring. Pastor et al. (2011)focused on predicting whether an object\nhas been successfully struck by a pool cue or flipped using chopsticks.\nStudying the relation of an action and its effect in this manner is\nsuitable if the state of the effects is discrete, but may not fit tool use\ntasks whose effects are continuous such as pushing an object 10 cm to\nits right.\nStudies that focus on learning the one-to-one relation of an action\nand its effect in a qualitative way generally employed tasks that result\nin the relocation of manipulanda.Stoytchev (2005a,2008)pre-defined\neight pulling actions and recorded the effects of these actions with\nfive different tools into an affordance table. As the actions were\ndiscretized, the effects can also be categorized in discretized space. In\nthe evaluation, a robot needed to choose appropriate actions based\non the affordance table in order to pull the manipulanda into a goal\nregion, given one of the learned tools. ThoughTikhanoff et al. (2013)\nalso leveraged pre-defined actions, they allowed the actions to be\nparameterized with continuous variables, e.g., a randomly sampled\npushing direction. Rather than keeping an affordance table, they used\nLeast Square Support Vector Machines to regress the actions to the\neffects. Elliott et al. (2016) considered more types of push and pull.\nThey also leveraged two regression techniques: linear regression and\nGaussian process regression. Other than pulling and pushing tasks,\nElliott and Cakmak (2018) explored cleaning tasks to relocate dirt.\nAs the manipulanda are clusters of rigid bodies rather than a single\nrigid-body manipulandum, they represented the surface as a grid, and\ntrained a pixel-level classifier to predict whether each pixel contains\ndirt after an action. The robots in the above studies explored tool use\nby themselves, pre-defined actions are necessary. In contrast,Liu et al.\n(2018) did not pre-define actions and took the method of imitation\nlearning and learned with deep reinforcement learning.\nThese studies focus on pushing and pulling tasks. A common\nfeature of these tasks is that the desired effect determines how a\ntool should contact a manipulandum. Other tool use tasks may\npermit multiple equally viable ways for a tool to make contact with a\nmanipulandumtoachievethesameeffect.Forexample,pouringliquid\nfrom different orientations all result in the same effect of a container\nbeing filled. Claassens and Demiris (2011) conducted preliminary\nstudies and termed such properties with affordance symmetries.\nAffordance symmetries are important because a robot will be able to\ngenerate different trajectories to complete a tool use task when the\nlearnedcontactresultsincollision.However,fewstudieshaveexplored\nthis direction to our knowledge.\n5.2.2 Transferable tool use\nTransferable tool use describes the ability to take tool use skills\ntrained on an object to other intra-category objects defined by a\ncommon form factor. Therefore, the key to transferable tool use is to\nmatch the unlearned objects with learned objects.\nWe first present studies that concern specific types of tool use\ntasks. Most of these focused on relocation tasks such as pulling and\npushing. Mar et al. (2017) and Nishide et al. (2011) leveraged self-\norganized maps to extract tool features to avoid the need to pre-\ndefining the features.Takahashi et al. (2017)learned a model with a\nFrontiers in Robotics and AI 08 frontiersin.org\nQin et al. 10.3389/frobt.2022.1009488\ndeepneural networkthatincorporated both graspinginformationand\ntool functions. Vogel et al. (2017)searched for the “sweet spot” of a\nnovel baseball bat-like object when used to hit a baseball by sensing\nthe force at the end-effector. Other studies considered pouring tasks.\nKroemer et al. (2012) used a kernel-based approach to generalize\nlearned action skills to novel objects.Brandi et al. (2014) performed\nwarping to the point cloud of a learned container to match a novel\ncontainer.Dong et al. (2019)adjusted pouring behavior by estimating\nthe volume of liquid in the unknown containers. While these studies\nattempted to transfer tool use skills to novel tools, other studies\nexplored how to act upon novel manipulanda. Gemici and Saxena\n(2014) sought to transfer cutting skills to food of varying physical\nproperties such as hardness. Elliott et al. (2017) transferred learned\nsurface cleaning actions to different surfaces, including surfaces of\ndifferent sizes. Li et al. (2018) developed the Push-Net so that the\nsystem can push novel objects for re-positioning and re-orientation.\nThough these studies demonstrated promising results on specific\ntool use tasks, it is unknown whether these algorithms could\ngeneralize to other types of tool use tasks. Therefore, other researchers\ninvestigated algorithms that transfer learned skills more broadly and\ndemonstrated with multiple tool use tasks. Tee et al. (2018, 2022)\nmatched the point cloud of unseen tools to the point cloud of the end-\neffector and arms of the robot to obtain the usage of the tools. The\nkPAM/kPAM 2.0 (Manuelli et al., 2019; Gao and Tedrake, 2021) used\nkeypoints on the tools to represent shared global shape of the category\noftools,andtooluseskillswereinferredfromthesekeypoints. Stückler\nand Behnke (2014b); Stückler et al. (2016, 2013); Stückler and Behnke\n(2014a, 2015) considered the point cloud presentation of tools and\nperformed deformable registration with different levels of resolution\nin order to match the overall shape of the tools.\nThe approach of these studies requires two steps: one to learn\ntool use skills as basic tool use, and one to learn the transfer process.\nOther studies merged the two steps and learned them in one step.\nSinapov and Stoytchev (2007)incorporated the shape of the tool when\nlearning tool use models for the pulling task. As a preliminary model,\ntransfer was only demonstrated with tools of the same shape but\ndifferent sizes.Gonçalves et al. (2014a,b) utilized a Bayesian network\nto learn how the actions and tool shapes influence the effects. The\nshape parameters include area, convexity, eccentricity, compactness,\ncircleness, and squareness. Due to the large size of the network, it\nneeded to be reduced to be able to train effectively. They validated\ntheir technique with pulling and pushing tasks.Dehban et al. (2016)\ntook a similar approach but overcame the drawback of the need for a\ndiscretization of data. To be able to handle grasping,Mar et al. (2015)\nleveragedsupportvectormachinestomapgeometricfeaturesbetween\nlearned and novel tools for pulling.\nThere are pros and cons of these two approaches. Training\neverything in one step may be more convenient, but the feature space\ncan be quite large and requires more data. Training in a modular way\nwill make it easier to diagnose when the algorithm does not function\nas intended. It will also make it easier to modify or incorporate new\nfeatures as the former requires the entire model to be retrained.\n5.2.3 Improvisatory tool use\nImprovisatory tool use describes the ability to use tools in a\ncreative way, which involves generalizing learned tool use skills\nfrom objects designed for the tasks to inter-category objects. These\nobjects may not share common form factors with the canonical tools.\nTherefore, local features of the tools that lead to the desired effects\nshould be identified.\nWhile transferring tool use requires a robot to infer how actions\nare affected by novel tools given the relation between actions and\neffects,improvisatorytooluserequiresarobottoalsounderstandwhat\nfeatures of the tools caused the effects, which is the relation between\ntools and effects. In other words, improvisatory tool use calls for the\nlearning of the full affordance model (Montesano et al., 2007; 2008).\nIn order to identify local features in unlearned tools, the function\nofatoolneedstobedetectedonaper-partbasis.Insightscanbegained\nfrom a related line of research that explores task-oriented grasping\nof novel objects. These studies made efforts to detect the functional\npart of a tool in different tasks so that the system can generate\ndifferent grasping of the same object based on the task (Myers et al.,\n2015; Song et al., 2010; 2011b; a; Ek et al., 2010; Madry et al., 2012;\nSong et al., 2015; Murali et al., 2020; Kokic et al., 2017; Detry et al.,\n2017). Similar to these studies, studies that focus on part detection\nfor tool use also leveraged geometric features.Schoeler and Wörgötter\n(2015) segmented the tools and used graphs to represent the relations\nbetween different tools parts.Nakamura and Nagai (2010)learned the\nfull affordance model. They provided human static demonstrations\nwithoutshowingthecourseofactions,anddetectedlocalfeatureswith\nthe Scale Invariant Feature Transform.\nGiven the functions of each tool part alone, a robot cannot\nrealize improvisatory tool use since a robot have no knowledge about\nhow to orient a tool. The robot needs to combine the tool parts\ninformation with tool use knowledge. Due to challenges in modeling\ngrasping,Fitzgerald et al.(2019) achievedthegoalwithhuman-guided\nadaptation that gained information on how to improvise each tool\nfrom human demonstrators. To improvise tool use without the need\nof human demonstrations for each tool,Agostini et al. (2015)learned\nactions with a modified DMP and used a Repository of Objects and\nAttributes with Roles to detect potential usages of a tool. This method\nis based on matching the global shapes of tools. Though this method\ncan perform some improvisatory tool use (e.g., utilizing a knife\nvertically for stirring in a way similar to using a spatula), the transfer\nis limited. Other studies considered both global and local features.\nFang et al. (2020) and Xie et al. (2019) took 2D images as input and\ntrained neural networks for improvisatory tool use. While these\nstudieslearnedtooluseskillsandtoolfeaturedetectiontogether,other\nattempts learned them in a modular manner;Jain and Inamura (2013)\nmanually pre-defined local features, discretized actions for the pulling\nand pushing tasks, and trained a robot with a T-shaped tool. They\nclaimed that the skills could be generalized to novel tools, though no\ndemonstration was provided. The Keto framework (Qin et al., 2020)\nand the GIFT framework (Turpin et al., 2021) generated keypoints\non the tools, such as grasping points and function points, based on\nlocal features. The robot then planned motion based on the keypoints.\nHowever, the keypoint approach may have difficulty on tasks where\nthe tool contact point cannot be readily represented using only one\npoint on the surface, such as a pencil sharpener whose contact is\ninside the object and the contact is more than a single point. Without\nusing keypoints, Abelha and Guerin (2017), Gajewski et al. (2019),\nAbelha et al. (2016), and Guerin and Ferreira (2019) characterized\nthe point cloud of a tool by approximating each of its segments with\nsuperquadrics and superparaboloids. They parametrized tool use with\nso-called p-tools, and demonstrated their technique with a wide range\nof tasks such as hammering and scooping in simulation or on a\nFrontiers in Robotics and AI 09 frontiersin.org\nQin et al. 10.3389/frobt.2022.1009488\nphysical robot.Qin et al. (2021) developed an integrated system and\nlearns basic tool use, rather than pre-define the tool usages as in\nother studies. The system can achieve both transferable tool use and\nimprovisatory tool use by considering both global and local geometric\nfeatures.Whilethesestudiesutilizedvisualfeaturestotransfertooluse,\nZhu et al. (2015)included both geometric and physical features such\nas mass.\n5.2.4 Deductive tool use\nIn deductive tool use, a robot should be able to utilize a novel tool\nto solve a task for which it has no prior knowledge. This is a very\nchallenging task, and no current studies can perform deductive tool\nuse to our knowledge. This type of tool use requires a robot to infer the\nentireaffordancemodel,whichistherelationsbetweenactions,effects,\nand tools, without tool use training samples as in improvisatory tool\nuse.\n5.3 Causal tool use—Multiple-manipulation\ntool use\nMultiple-manipulation tool use involves many different types of\ntooluse.Unlikesingle-manipulationtooluse,thesubtypesofmultiple-\nmanipulation tool use may not be interrelated. Compared with\nsingle-manipulation tool use, they may require more sophisticated\nmanipulation skills and cognition skills such as planning, which are\ngenerally not needed in single tool use where only one tool use task is\nconsidered. In terms of tool knowledge, they usually require the full\nmodel of tool affordance knowledge.\n5.3.1 Combined tool use\nCombined tool use refers to using multiple tools simultaneously,\nsuch as using a fork and a knife to cut a steak. No prior studies have\ndemonstrated combined tool use to our knowledge. We identify two\nmain challenges of combined tool use. The first challenge is at the\ncognition level. A robot should choose the appropriate parameters for\neach tool use, such as where to cut with the knife and where to stab\nthe steak with the fork. The second challenge is at the manipulation\nlevel, which is how to coordinate the actions of each tool. It involves\ncollision-free motion planning and adjusting the actions of one tool\nbased on the other tool. For example, the force exerted on the fork\nto stabilize the steak is dependent on the course of the cutting action\nwith the knife. Though generating collision-free motion planning may\nshare similar techniques in multi-agent systems [for a review, see\nRossi et al. (2018); Ismail et al. (2018); Rasheed et al. (2022)], how to\nchoose appropriate parameters and how to coordinate tools are issues\nspecific to tool use and may need to be handled differently from\ngeneral manipulation tasks.\n5.3.2 Sequential tool use\nSequential tool use involves completing multiple tool use tasks in\norder. Yamazaki et al. (2010)designed an integrated system of daily\nassistiverobotsandappliedittothetaskoftidyingandcleaningrooms.\nThis system focused on failure detection and recovery, and manually\ndefined the sequence of tasks to be completed. For a robot to be fully\nautonomous, the robot should be able to arrange appropriate orders\nand decide appropriate task parameters for each tool use task since\nthe end state of a task is the start state of the next task.\nThisrequirementfallsunderthetopicoftaskandmotionplanning\n(TAMP) [for a review, seeGarrett et al. (2021)]. As its name suggests,\nit integrates low-level motion planning which includes classic robotic\nmanipulation techniques and high-level task planning which belongs\nto classic AI planning. Task planning aims to find an action skeleton\nto achieve a goal (e.g., pick up a pencil, use it to write, and put the\npen down). Motion planning aims to find motion plans to execute in a\nrobot (e.g., the joint states for each action). TAMP aims to find action\nparameters to connect task planning and motion planning (e.g., where\nto grasp the pencil to pick it up so the pencil can be used to write).\nTAMP currently has two main approaches to find action parameters:\nthe sampling-based approach and the optimization-based approach.\nThe sampling-based approach, which is used in the majority of TAMP\nstudies, samples action parameters and tests the feasibility of the\nsampled combinations. Therefore, this approach may have difficulty\nwhen the solution space is relatively small since the probability of\nbeing able to sample the correct solution is small. In contrast, the\noptimization-based approach used optimization techniques such as\nlogic-geometric programming (Toussaint et al., 2018) or sequential\nquadratic programming (Hadfield-Menell et al., 2016). It is able to\nhandle problems with a small solution space more efficiently if\nthe local optima can be handled properly. However, this approach\ngenerally requires a longer running time for tasks with many objects\ndue to the increased dimension.\nSequential tool use has been demonstrated with optimization-\nbased TAMP. Toussaint et al. (2018) enabled a robot in simulation\nto reach a tool that was initially out of reach with another tool in\norder to grab the target object. While they can handle tasks in a static\nenvironment,Migimatsu and Bohg (2020)improved the method with\nan object-centric approach to adapt to situations where objects were\nmoved by other agents. Though this study was not demonstrated with\nsequential tool use, it has the potential to be applied to sequential\ntool use. Due to the current preliminary stage of tool use research,\nsequential tool use has not been demonstrated with a sampling-based\napproach to our knowledge.\nIn the above optimization-based TAMP approach, sequential\ntool use is only included as a demonstration to validate TAMP\nmethods. Tool use, especially sequential tool use, usually includes\nmultiple objects, which makes it challenging for the optimization-\nbasedapproach.Itisalsochallengingforthesampling-basedapproach\nsince tool use tasks generally have a smaller solution space due\nto the additional constraints of tools. Therefore, alternative TAMP\nalgorithms designed for sequential tool use may be needed due to\nthe special requirements of tool use tasks compared with general\nmanipulation tasks.\n5.3.3 Tool selection\nTool selection is the ability to choose the most appropriate\ntool among many options. In order to select the most appropriate\nobject to be used as a ram to keep a door open, Levihn and\nStilman (2014) identified four properties of a ram. In order to\nlearn the properties, Wicaksono and Sammut (2016) demonstrated\na robot with an instance of the pulling task, and the robot then\nperformed experiments to generate hypotheses about what features\nare important. As an example of the hypotheses, “the hook (of the\npulling tool, which is a tube) must be located on the same end of\ntube as the cube (manipulandum).” The hypotheses are expressed\nin Horn clauses so that the features are qualitative. To learn the\nFrontiers in Robotics and AI 10 frontiersin.org\nQin et al. 10.3389/frobt.2022.1009488\nfeatures in a quantitative manner,Saito et al. (2018)and Brawer et al.\n(2020) learned the full model of tool affordances and performed tool\nselection.\n5.3.4 Tool manufacturing\nTool manufacturing is the ability to complete a tool use task by\nconstructingatoolbycombiningavailablematerials,modifyingatool,\nor both. As this process may involve combining different pieces, the\nmanipulation skills required may be similar to the skills in robotic\nassembly. The peg-in-hole task, which is to insert a peg in a hole, is a\nstandard task in robotic assembly. Researchers have explored methods\nto improvea robot’sperformance, suchas workingwith morecomplex\npartswithforce-guidedassembly( Dietrich et al.,2010)andincreasing\nthespeedofcompliantmanipulators( Bös et al.,2017)(Forareviewon\nrobotic assembly with learning from demonstration, seeZhu and Hu,\n2018). Beyond the peg-in-hole task, previous studies also considered\nthe slide-in-the-groove assembly task (Peternel et al., 2015), and robot\nassembly that leveraged tool use such as hammering and wrenching\n(Gu et al., 2014).\nNair et al. (2019a,b) studied tool manufacturing by combining\navailable parts. Their system was provided with examples of tool use,\nand selected appropriate parts as the grasping parts and function\nparts. The selection was made by comparing the similarity between\ntheavailablepartsandsegmentedpartsofthedemonstratedexamples.\nThe next step is to combine the parts selected with appropriate\norientations. The system then performed tool use tasks to validate\nthe assembled tool. Unlike robotic assembly, the manipulation skills\nrequiredinthesestudiesarerelativelysimple.Itpre-definedthreeways\nof attaching the different parts: pierce attachment, grasp attachment,\nand magnetic attachment. Sammut et al. (2015) designed a robot\nengineer to perform tool manufacturing. The robot engineer first\nidentified important features of a tool use task, and then constructed\nthe tool using 3D printing.\nTool manufacturing is a complicated task. The task settings\nof current studies reduce the difficulty of both manipulation and\ncognition skills. At the manipulation level, a robot may need to\ncombine different parts with simple manipulation skills or leverage\nan external machine. While in animal or human tool use, the\nmanipulation skills required in tool manufacturing are sophisticated\nand may even require using other tools. At the cognition level, the\nchoice of available parts discretizes the solution space compared with\nthe task whose solution space is continuous, such as a chimpanzee\nneeding to make a hook to retrieve food. Tool manufacturing also\nrequires tool affordance knowledge to identify important features\nof a tool to be assembled and requires planning skills to arrange\nthe manipulation actions, especially when sequential tool use is\nneeded.\n5.4 Summary\nIn this section, we reviewed previous studies in robot tool use\nand summarized them in Supplementary Material. Many studies\ntreated tool use tasks as general manipulation tasks and focused\non programming or characterizing and duplicating the actions. The\nlearning in these non-causal tool use tasks did not consider the objects\nnortheeffects.Asaresult,robotslearnedtooluseinthismannershare\nsimilar characteristics as animals performing stereotyped tool use that\nthe tool use demonstrate limited variations and challenge to adapt to\ndifferent contexts.\nAs an emerging topic, relatively few studies have explored causal\ntool use. Early studies investigated the action-effect relations. Some\nstudiesfocusedondescribingtheeffectspaceofanactionorpredicting\nthe success or failure of an action, while other studies focused on\nlearning how to adjust actions in order to achieve the desired effects.\nThe latter mostly leveraged the pulling and/or the pushing tasks. To\nachieve transferable tool use, some studies focused on particular tasks\nand very few studies explored generic frameworks that may work with\nmultiple tasks. Improvisatory tool use is even more challenging. To\nrealize this type of tool use, some studies attempted to identify the\nfunctions of parts of a tool while others treated a tool in a holistic\nmanner and detected key characters to represent the tool including\nthe relations between different parts of tools. The former can generally\nachieve amazing results in tool use tasks whose local features are\ncrucial to solve a task (e.g., the blade of a knife), and the latter enjoys\nadvantages for tasks whose tools share common factors (e.g., mugs in\ndifferent shapes). Though very few studies have attempted to handle\nboth tasks. Different from these sub-types of single-manipulation tool\nuse tasks, no previous studies have attempted deductive tool use.\nCompared with single-manipulation tool use, fewer studies\nhave addressed multiple-manipulation tool use. No previous studies\nreported that their systems can handle combined tool use; sequential\ntool use was usually treated as a regular manipulation task in TAMP;\ntool selection is most similar to single-manipulation tool use and\ngained most attention among multiple-manipulation tool use; only\nlimited attempts have been made to address even simplified version\nof tool manufacturing.\n6 Discussions\nWe defined robot tool use, provided a taxonomy of robot use,\nand identified the required skills in each category of tool use. As\na summary, non-causal tool use focuses on the manipulation skills\nof using tools. Causal tool use focuses on learning or applying\naffordances. The sub-categories of single-manipulation tool use learn\ndifferent parts of affordances. Basic tool use learns the actions-effects\nrelation. Transferable tool use focuses on the tools-actions relation\nin addition to the actions-effects relation. Improvisatory tool use\nrequires the knowledge of the full model. Deductive tool use generates\naffordances with general knowledge, rather than inducting the model\nfrom experiences or demonstrations. While single tool use relies on\nlearningaffordances,multiple-manipulationtooluseleverageslearned\naffordances and requires more sophisticated manipulation and/or\nhigher-level cognition skills. In addition, we review literature on robot\ntool use. In this section, we discuss current or near future applications\nas well as future directions for robot tool use.\n6.1 Current applications of robot tool use\nThe taxonomy can be used as a practical guideline for robot\nengineers when developing tool use applications. Robot engineers\nmay start with the categorization criterion in the taxonomy since\nthese are important features of robot tool use. This process also\nfacilitates engineers to identify the sub-type of tool use involved in\nFrontiers in Robotics and AI 11 frontiersin.org\nQin et al. 10.3389/frobt.2022.1009488\nFIGURE 5\nA cheat sheet for developing robot tool use applications.\nthe application. We include a convenient cheat sheet for engineers as\nFigure 5. Upon the identification of the sub-type, our taxonomy also\nprovides information regarding which skills should be focused on as\nsummarized inFigure 4.\nWewouldliketoemphasizethatthetaxonomywasnotdesignedto\ndecide which type of tool use is superior, but to show the differences\nbetween the sub-types of tool use. For example, causal tool use may\nlook more appealing than non-causal tool use as the actions can be\ngenerated more flexibly in different contexts. However, the former is\nmore challenging to learn and the performance is comprised due to\nthe limitations of current learning techniques. For industrial settings,\nrobots on the assembly line may be required to complete the same\ntask repeatedly. In this scenario, being more reliable may be more\nimportant than being more flexible. Moreover, the environment is\ndesigned for the robot and it is highly-controlled and being able to\nadapt to diverse context is less of a concern. As a result, non-causal\ntool use is sufficient. Conversely, household robots may face dynamic,\ncomplex, and stochastic environments. Such environments necessitate\nmore flexible tool use skills. Therefore, causal tool use is required. In\na nutshell, what kind of tool use to implement in a certain application\nis dependent on the need of the task and the limitations of current\ntechniques.\nWe would also like to point out that the characteristics to classify\nthe taxonomy may not be an exhaustive list for tool use in every\nscenario. For example, our taxonomy is designed to consider a\nrobot solving a tool use task by itself. However, in practice, a robot\nmay be required to solve a task jointly with a human collaborator.\nConsiderations around safely handling tools, and doing so in a\nway that fosters effective collaboration are important, but beyond\nthe scope of the taxonomy and this survey. Moreover, different\napplications may have significantly different requirements making it\nimpractical to design a guideline that may apply to every conceivable\napplication. Our taxonomy provides a basic guideline to design a\nrobottocompletetoolusetasks,andengineersshouldidentifyspecific\nrequirements of particular applications. Not to mention that the real-\nworld environment is noisier which adds another layer of complexity.\n6.2 Open challenges of future robot tool use\nThe study of tool use is still in the preliminary stages, and most\nstudies aim to solve non-causal tool use and basic tool use. We identify\nthe following open challenges in tool use.\n1. How can a robot learn the relations between tool-manipulanda\ncontact poses and effects in transferable tool use?There is a lack of\nstudiesontherelationshipbetweentool-manipulandacontactposes\nand tool use effects. Most studies focus on the relation between\ntrajectories and effects.\n2. How can an integrative system for improvisatory tool use handle a\nwide range of tasks? While it is challenging to improvise tool use\nbased on either local or global features, it is even more challenging\nto develop a system that can solve a wide range of tool use tasks.\nSuch a system should decide whether local or global features should\nbe considered, or choose features beyond geometric ones.\n3. How can a robot perform deductive tool use? The challenge for\ndeductive tool use is the lack of prior experiences. Current\ntechniques for other sub-types of single-manipulation tool\nuse performs inductive reasoning that learns affordances from\nexperiences, and cannot be applied to deductive tool use.\n4. How can a robot perform multiple-manipulation tool use?Multiple-\nmanipulation tool use requires a robot to perform single-\nmanipulation tool use. In addition, each sub-type in multiple-\nmanipulation tool use requires more sophisticated manipulation\nskills or higher level cognition skills. Moreover, the additional skills\ndiffer among the sub-types of multiple-manipulation tool use.\n5. How can a robot learn the dynamics in causal tool use?It is already\nchallengingforcurrentstudiestoconsidertasksthatcanbeachieved\nwith only kinematic control. It will be even more challenging to\nincorporate dynamics as it adds additional dimensions to consider.\n6. Can we design a benchmark database for standard tool use\ntasks? It is not trivial to design standard tool use tasks with a\nbenchmark database of object models to facilitate comparisons\nbetween different algorithms. The requirements of the tasks should\nFrontiers in Robotics and AI 12 frontiersin.org\nQin et al. 10.3389/frobt.2022.1009488\nbe detailed enough for precise replication. However, detailed\nrequirements may lead to algorithms tailored for these tasks, and\nloss of generality. Moreover, it is challenging to select representative\ntools for improvisatory tool use as tools are expected to be used in\ncreative manners. It is also impossible to include all possible tools\nfor a given task due to the almost endless choices of physical objects\nthat can be used as tools. It is also time-consuming to obtain the 3D\nmodel of an object.\n7. When and how can tool use knowledge be applied other areas in\nrobotics? Most studies that are relevant to tool use ignore the\naffordance model. For example, when learning robot grasping or\nrobot handovers, a system typically observes how a human grasps\na tool or hands over a tool, rather than inferring how a tool should\nbe grasped or handed over based on the subsequent tasks. It is\nimportant for a system to be equipped with affordance knowledge\nsince affordance causally determines how a tool should be grasped\nor handed over for to perform subsequent tool use tasks (Qin et al.,\n2022). However, not every study involving tool use requires a\nrobot to learn the full affordance model, and it is important to\nidentify which part of the model should be learned. Moreover, it\nalso requires effort to connect tool use learning module with other\nmodules, such as robot grasping and human-robot-collaboration\ntasks.\nAuthor contributions\nMQ, JB, and BS participated in writing the paper.\nFunding\nThis work is funded by the Office of Naval Research (ONR) award\nNo.N00014-18-1-2776andNationalScienceFoundation(NSF)under\ngrants No. 1955653, 1928448, 2106690, and 1813651.\nConflict of interest\nTheauthorsdeclarethattheresearchwasconductedintheabsence\nof any commercial or financial relationships that could be construed\nas a potential conflict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their affiliated organizations,\nor those of the publisher, the editors and the reviewers. Any product\nthat may be evaluated in this article, or claim that may be made by its\nmanufacturer, is not guaranteed or endorsed by the publisher.\nSupplementary material\nThe Supplementary Material for this article can\nbe found online at: https://www.frontiersin.org/articles/\n10.3389/frobt.2022.1009488/full#supplementary-material\nReferences\nAbelha, P., and Guerin, F. (2017). “Learning how a tool affords by simulating 3d models\nfrom the web,” in2017 IEEE/RSJ international conference on intelligent robots and systems\n(IROS) (IEEE), 4923–4929.\nAbelha, P., Guerin, F., and Schoeler, M. (2016). “A model-based approach to finding\nsubstitute tools in 3d vision data,” in2016 IEEE international conference on robotics and\nautomation (ICRA)(IEEE), 2471–2478.\nAgostini, A., Aein, M. J., Szedmak, S., Aksoy, E. E., Piater, J., and Würgütter, F. (2015).\n“Using structural bootstrapping for object substitution in robotic executions of human-\nlike manipulation tasks,” in2015 IEEE/RSJ international conference on intelligent robots\nand systems (IROS)(IEEE), 6479–6486.\nAlcock, J. (1972). The evolution of the use of tools by feeding animals.Evolution 26,\n464–473. doi:10.2307/2407020\nAsano, T. (1994). inTool using behavior and language in primates. Behavior analysis of\nlanguage and cognition. Editors S. C. Hayes, L. J. Hayes, M. Sato, and K. Ono, 145–148.\nBeck, B. B. (1980).Animal tool behavior: The use and manufacture of tools by animals.\nNew York: Garland STPM Press.\nBiro, D., Inoue-Nakamura, N., Tonooka, R., Yamakoshi, G., Sousa, C., and Matsuzawa,\nT. (2003). Cultural innovation and transmission of tool use in wild chimpanzees: Evidence\nfrom field experiments.Anim. Cogn.6, 213–223. doi:10.1007/s10071-003-0183-x\nBoesch, C. (2013). “Ecology and cognition of tool use in chimpanzees,” inTool use in\nanimals: Cognition and ecology, 21–47.\nBös, J., Wahrburg, A., and Listmann, K. D. (2017). “Iteratively learned and temporally\nscaled force control with application to robotic assembly in unstructured environments,”\nin 2017 IEEE international conference on robotics and automation (ICRA) (IEEE),\n3000–3007.\nBrandi, S., Kroemer, O., and Peters, J. (2014). “Generalizing pouring actions between\nobjects using warped parameters,” in 2014 IEEE-RAS international conference on\nhumanoid robots(IEEE), 616–621.\nBrawer, J., Qin, M., and Scassellati, B. (2020). “A causal approach to tool affordance\nlearning,” in 2020 IEEE/RSJ international conference on intelligent robots and systems\n(IROS) (IEEE), 8394–8399.\nBreuer,T.,Ndoundou-Hockemba, M., andFishlock,V.(2005). Firstobservationoftool\nuse in wild gorillas.PLoS Biol.3, e380. doi:10.1371/journal.pbio.0030380\nByravan, A., and Fox, D. (2017). “Se3-nets: Learning rigid body motion using deep\nneuralnetworks,”in 2017IEEEinternationalconferenceonroboticsandautomation(ICRA)\n(IEEE), 173–180.\nCabrera-Álvarez, M. J., and Clayton, N. S. (2020). Neural processes underlying\ntool use in humans, macaques, and corvids. Front. Psychol. 11, 560669.\ndoi:10.3389/fpsyg.2020.560669\nCakmak, M., Dogar, M., Ugur, E., and Sahin, E. (2007). “Affordances as a framework\nfor robot control,” inProceedings of the 7th international conference on epigenetic robotics\nepirob’07.\nCall, J. (2013). “Three ingredients for becoming a creative tool user,” inTool use in\nanimals: Cognition and ecology, 3–20.\nChen, H., Wan, W., and Harada, K. (2019). “Combined task and motion planning for\na dual-arm robot to use a suction cup tool,” inIEEE-RAS 19th international conference on\nhumanoid robots (humanoids)(IEEE), 446–452.\nChevalier-Skolnikoff, S. (1989). Spontaneous tool use and sensorimotor intelligence\nin cebus compared with other monkeys and apes. Behav. brain Sci. 12, 561–588.\ndoi:10.1017/s0140525x00057678\nClaassens, J., and Demiris, Y. (2011). “Generalising human demonstration data by\nidentifying affordance symmetries in object interaction trajectories,” in2011 IEEE/RSJ\ninternational conference on intelligent robots and systems(IEEE), 1980–1985.\nColgate, J. E., Stanley, M. C., and Brown, J. M. (1995). “Issues in the haptic display of\ntool use,” inProceedings 1995 IEEE/RSJ international conference on intelligent robots and\nsystems. Human robot interaction and cooperative robots(IEEE), 3, 140–145.\nFrontiers in Robotics and AI 13 frontiersin.org\nQin et al. 10.3389/frobt.2022.1009488\nDehban, A., Jamone, L., Kampff, A. R., and Santos-Victor, J. (2016). “Denoising auto-\nencoders for learning of objects and tools affordances in continuous space,” in2016 IEEE\ninternational conference on robotics and automation (ICRA)(IEEE), 4866–4871.\nDetry,R.,Papon,J.,andMatthies,L.(2017).“Task-orientedgraspingwithsemanticand\ngeometric scene understanding,” in2017 IEEE/RSJ international conference on intelligent\nrobots and systems (IROS)(IEEE), 3266–3273.\nDietrich, F., Buchholz, D., Wobbe, F., Sowinski, F., Raatz, A., Schumacher, W., et al.\n(2010). “On contact models for assembly tasks: Experimental investigation beyond the\npeg-in-holeproblemontheexampleofforce-torquemaps,”in 2010IEEE/RSJinternational\nconference on intelligent robots and systems(IEEE), 2313–2318.\nDong, C., Takizawa, M., Kudoh, S., and Suehiro, T. (2019). “Precision pouring into\nunknown containers by service robots,” in 2019 IEEE/RSJ international conference on\nintelligent robots and systems (IROS)(IEEE), 5875–5882.\nDroniou, A., Ivaldi, S., and Sigaud, O. (2014). “Learning a repertoire of actions with\ndeep neural networks,” in4th international conference on development and learning and on\nepigenetic robotics(IEEE), 229–234.\nEk, C. H., Song, D., Huebner, K., and Kragic, D. (2010). Exploring affordances in robot\ngrasping through latent structure representation.Vis. Cognitive TasksECCV.\nElliott, S., and Cakmak, M. (2018). “Robotic cleaning through dirt rearrangement\nplanningwithlearnedtransitionmodels,”in 2018IEEEinternationalconferenceonrobotics\nand automation (ICRA)(IEEE), 1623–1630.\nElliott, S., Valente, M., and Cakmak, M. (2016). “Making objects graspable in confined\nenvironmentsthroughpushandpullmanipulationwithatool,”in 2016 IEEE international\nconference on robotics and automation (ICRA)(IEEE), 4851–4858.\nElliott, S., Xu, Z., and Cakmak, M. (2017). “Learning generalizable surface cleaning\nactions from demonstration,” in2017 26th IEEE international symposium on robot and\nhuman interactive communication (RO-MAN)(IEEE), 993–999.\nFang, K., Zhu, Y., Garg, A., Kurenkov, A., Mehta, V., Fei-Fei, L., et al. (2020). Learning\ntask-oriented grasping for tool manipulation from simulated self-supervision. Int. J.\nRobotics Res.39, 202–216. doi:10.1177/0278364919872545\nFitzgerald, T., Short, E., Goel, A., and Thomaz, A. (2019). Human-guided trajectory\nadaptation for tool transfer.Proc. 18th Int. Conf. Aut. Agents MultiAgent Syst., 1350–1358.\nForestier, S., and Oudeyer, P.-Y. (2016). “Modular active curiosity-driven discovery of\ntooluse,”in 2016IEEE/RSJinternationalconferenceonintelligentrobotsandsystems(IROS)\n(IEEE), 3965–3972.\nFragaszy, D., and Eshchar, Y. (2017). Tool use in nonhuman primates: Natural history,\nontogenetic development and social supports for learning. Evol. Nerv. Syst., 317–328.\ndoi:10.1016/b978-0-12-804042-3.00087-7\nGajewski, P., Ferreira, P., Bartels, G., Wang, C., Guerin, F., Indurkhya, B., et al.\n(2019). “Adapting everyday manipulation skills to varied scenarios,” in2019 international\nconference on robotics and automation (ICRA)(IEEE), 1345–1351.\nGao, W., and Tedrake, R. (2021). Kpam 2.0: Feedback control for category-\nlevel robotic manipulation. IEEE Robotics Automation Lett. 6, 2962–2969.\ndoi:10.1109/lra.2021.3062315\nGarcia-Peraza-Herrera, L. C., Li, W., Fidon, L., Gruijthuijsen, C., Devreker, A.,\nAttilakos, G., et al. (2017). “Toolnet: Holistically-nested real-time segmentation of robotic\nsurgical tools,” in2017 IEEE/RSJ international conference on intelligent robots and systems\n(IROS) (IEEE), 5717–5722.\nGarrett, C. R., Chitnis, R., Holladay, R., Kim, B., Silver, T., Kaelbling, L. P., et al. (2021).\nIntegrated task and motion planning.Annu. Rev. control, robotics, Aut. Syst.4, 265–293.\ndoi:10.1146/annurev-control-091420-084139\nGemici,M.C.,andSaxena,A.(2014).“Learninghapticrepresentationformanipulating\ndeformable food objects,” in2014 IEEE/RSJ international conference on intelligent robots\nand systems(IEEE), 638–645.\nGibson, J. J. (1979).The Ecological Approach to Visual Perception. Boston: Houghton\nMiffiin.\nGonçalves, A., Abrantes, J., Saponaro, G., Jamone, L., and Bernardino, A. (2014a).\n“Learning intermediate object affordances: Towards the development of a tool concept,” in\n4th international conference on development and learning and on epigenetic robotics(IEEE),\n482–488.\nGonçalves, A., Saponaro, G., Jamone, L., and Bernardino, A. (2014b). “Learning visual\naffordances of objects and tools through autonomous robot exploration,” in2014 IEEE\ninternational conference on autonomous robot systems and competitions (ICARSC)(IEEE),\n128–133.\nGoodall, J. (1964). Tool-using and aimed throwing in a community of free-living\nchimpanzees. Nature201, 1264–1266. doi:10.1038/2011264a0\nGu, Y., Sheng, W., and Ou, Y. (2014). “Automated assembly skill acquisition through\nhuman demonstration,” in2014 IEEE international conference on robotics and automation\n(ICRA) (IEEE), 6313–6318.\nGuerin, F., and Ferreira, P. (2019). Robot manipulation in open environments: New\nperspectives. IEEE Trans. cognitive Dev. Syst.12, 669–675. doi:10.1109/tcds.2019.2921098\nGuha, A., Yang, Y., Fermu, C., and Aloimonos, Y. (2013). “Minimalist plans for\ninterpretingmanipulationactions,”in 2013IEEE/RSJinternationalconferenceonintelligent\nrobots and systems(IEEE), 5908–5914.\nHadfield-Menell, D., Lin, C., Chitnis, R., Russell, S., and Abbeel, P. (2016). “Sequential\nquadratic programming for task plan optimization,” in 2016 IEEE/RSJ international\nconference on intelligent robots and systems (IROS)(IEEE), 5040–5047.\nHoffmann, H., Chen, Z., Earl, D., Mitchell, D., Salemi, B., and Sinapov, J. (2014).\nAdaptive robotic tool use under variable grasps. Robotics Aut. Syst. 62, 833–846.\ndoi:10.1016/j.robot.2014.02.001\nHolladay, R., Lozano-Pérez, T., and Rodriguez, A. (2019). “Force-and-motion\nconstrained planning for tool use,” in2019 IEEE/RSJ international conference on intelligent\nrobots and systems (IROS)(IEEE), 7409–7416.\nHu,N.,Lou,Z.,Englebienne,G.,andKröse,B.J.(2014).“Learningtorecognizehuman\nactivities from soft labeled data,” inRobotics: Science and systems.\nHunt, G. R., Gray, R. D., and Taylor, A. H. (2013). “Why is tool use rare in animals,” in\nTool use in animals: Cognition and ecology, 89–118.\nIjspeert, A. J., Nakanishi, J., Hoffmann, H., Pastor, P., and Schaal, S. (2013). Dynamical\nmovement primitives: Learning attractor models for motor behaviors.Neural Comput.25,\n328–373. doi:10.1162/neco_a_00393\nIjspeert, A. J., Nakanishi, J., and Schaal, S. (2002). “Movement imitation with nonlinear\ndynamicalsystemsinhumanoidrobots,”in Proceedings2002IEEEinternationalconference\non robotics and automation (cat. No. 02CH37292)(IEEE), 2, 1398–1403.\nIsmail, Z. H., Sariff, N., and Hurtado, E. (2018). “A survey and analysis of cooperative\nmulti-agent robot systems: Challenges and directions,” inApplications of mobile robots\n(IntechOpen), 8–14.\nJain, R., and Inamura, T. (2013). Bayesian learning of tool affordances based on\ngeneralization of functional feature to estimate effects of unseen tools.Artif. Life Robotics\n18, 95–103. doi:10.1007/s10015-013-0105-1\nJamone, L., Damas, B., Santos-Victor, J., and Takanishi, A. (2013). “Online learning of\nhumanoid robot kinematics under switching tools contexts,” in2013 IEEE international\nconference on robotics and automation(IEEE), 4811–4817.\nJamone, L., Ugur, E., Cangelosi, A., Fadiga, L., Bernardino, A., Piater, J., et al. (2016).\nAffordances in psychology, neuroscience, and robotics: A survey.IEEE Trans. Cognitive\nDev. Syst.10, 4–25. doi:10.1109/tcds.2016.2594134\nKarayiannidis, Y., Smith, C., Vina, F. E., and Kragic, D. (2014). “Online contact point\nestimationforuncalibratedtooluse,”in 2014 IEEE international conference on robotics and\nautomation (ICRA)(IEEE), 2488–2494.\nKatz, D., Pyuro, Y., and Brock, O. (2008). Learning to manipulate articulated objects\nin unstructured environments using a grounded relational representation. In InRobotics:\nScience and systems (citeseer).\nKatz,D.,Venkatraman,A., Kazemi,M.,Bagnell,J.A., andStentz,A. (2014). Perceiving,\nlearning,andexploitingobjectaffordancesforautonomouspilemanipulation. Aut. Robots\n37, 369–382. doi:10.1007/s10514-014-9407-y\nKe, L., Wang, J., Bhattacharjee, T., Boots, B., and Srinivasa, S. (2021). “Grasping\nwith chopsticks: Combating covariate shift in model-free imitation learning for fine\nmanipulation,” in2021 IEEE international conference on robotics and automation(ICRA)\n(IEEE), 6185–6191.\nKemp, C. C., and Edsinger, A. (2006). “Robot manipulation of human tools:\nAutonomous detection and control of task relevant features,” inProc. Of the fifth intl.\nConference on development and learning, 42.\nKim, S.-K., Jo, J., Oh, Y., Oh, S.-R., Srinivasa, S., and Likhachev, M. (2014).\n“Robotic handwriting: Multi-contact manipulation based on reactional internal contact\nhypothesis,” in2014 IEEE/RSJ international conference on intelligent robots and systems\n(IEEE), 877–884.\nKobayashi, Y., and Hosoe, S. (2009). “Planning-space shift learning: Variable-\nspace motion planning toward flexible extension of body schema,” in 2009 IEEE/RSJ\ninternational conference on intelligent robots and systems(IEEE), 3107–3114.\nKober, J., Mohler, B., and Peters, J. (2008). “Learning perceptual coupling for motor\nprimitives,” in2008 IEEE/RSJ international conference on intelligent robots and systems\n(IEEE), 834–839.\nKoch, J., Büsch, L., Gomse, M., and Schüppstuhl, T. (2022). A methods-\ntime-measurement based approach to enable action recognition for multi-\nvariant assembly in human-robot collaboration. Procedia CIRP 106, 233–238.\ndoi:10.1016/j.procir.2022.02.184\nKokic, M., Stork, J. A., Haustein, J. A., and Kragic, D. (2017). “Affordance detection\nfor task-specific grasping using deep learning,” in 2017 IEEE-RAS 17th international\nconference on humanoid robotics (humanoids)(IEEE), 91–98.\nKormushev, P., Nenchev, D. N., Calinon, S., and Caldwell, D. G. (2011). “Upper-\nbody kinesthetic teaching of a free-standing humanoid robot,” in2011 IEEE international\nconference on robotics and automation(IEEE), 3970–3975.\nKroemer, O., Niekum, S., and Konidaris, G. (2021). A review of robot learning\nfor manipulation: Challenges, representations, and algorithms.J. Mach. Learn. Res. 22,\n1395–1476.\nKroemer, O., Ugur, E., Oztop, E., and Peters, J. (2012). “A kernel-based approach to\ndirectactionperception,”in 2012IEEEinternationalconferenceonroboticsandautomation\n(IEEE), 2605–2610.\nKrützen, M., Mann, J., Heithaus, M. R., Connor, R. C., Bejder, L., and Sherwin, W. B.\n(2005). Cultural transmission of tool use in bottlenose dolphins.Proc. Natl. Acad. Sci.102,\n8939–8943. doi:10.1073/pnas.0500232102\nFrontiers in Robotics and AI 14 frontiersin.org\nQin et al. 10.3389/frobt.2022.1009488\nKulak, T., Silvério, J., and Calinon, S. (2020). “Fourier movement primitives: An\napproachforlearningrhythmicrobotskillsfromdemonstrations,”in Robotics: Science and\nsystems.\nKutsuzawa, K., Sakaino, S., and Tsuji, T. (2017). A control system for a tool use robot:\nDrawing a circle by educing functions of a compass.J. Robotics Mechatronics29, 395–405.\ndoi:10.20965/jrm.2017.p0395\nLee, D., Kunori, H., and Nakamura, Y. (2008). “Association of whole body motion\nfrom tool knowledge for humanoid robots,” in2008 IEEE/RSJ international conference on\nintelligent robots and systems (IEEE), 2867–2874.\nLee, Y.-H., and Song, K.-T. (2021). “Real-time obstacle avoidance with a virtual torque\napproach for a robotic tool in the end effector,” in2021 IEEE international conference on\nrobotics and automation (ICRA)(IEEE), 8436–8442.\nLestel, D., and Grundmann, E. (1999). Tools, techniques and animals: The role of\nmediations of actions in the dynamics of social behaviours.Soc. Sci. Inf. 38, 367–407.\ndoi:10.1177/053901899038003002\nLevihn, M., and Stilman, M. (2014). “Using environment objects as tools:\nUnconventional door opening,” in2014 IEEE/RSJ international conference on intelligent\nrobots and systems (IEEE), 2502–2508.\nLi, J. K., Lee, W. S., and Hsu, D. (2018). Push-net: Deep planar pushing for objects with\nunknown physical properties.Robotics Sci. Syst.14, 1–9.\nLi, R., Pham, D. T., Huang, J., Tan, Y., Qu, M., Wang, Y., et al. (2020). Unfastening of\nhexagonal headed screws by a collaborative robot.IEEE Trans. Automation Sci. Eng.17,\n1–14. doi:10.1109/tase.2019.2958712\nLi, X., Cao, R., Feng, Y., Chen, K., Yang, B., Fu, C.-W., et al. (2022). A sim-to-real object\nrecognition and localization framework for industrial robotic bin picking.IEEE Robotics\nAutomation Lett.7, 3961–3968. doi:10.1109/lra.2022.3149026\nLin, Y., and Sun, Y. (2015). Robot grasp planning based on demonstrated grasp\nstrategies. Int. J. Robotics Res.34, 26–42. doi:10.1177/0278364914555544\nLioutikov, R., Neumann, G., Maeda, G., and Peters, J. (2017). Learning movement\nprimitive libraries through probabilistic segmentation.Int. J. Robotics Res.36, 879–894.\ndoi:10.1177/0278364917713116\nLiu, Y., Gupta, A., Abbeel, P., and Levine, S. (2018). “Imitation from observation:\nLearning to imitate behaviors from raw video via context translation,” in 2018 IEEE\ninternational conference on robotics and automation (ICRA)(IEEE), 1118–1125.\nLonsdorf, E. V. (2006). What is the role of mothers in the acquisition of termite-fishing\nbehaviors in wild chimpanzees (pan troglodytes schweinfurthii)?Anim. Cogn.9, 36–46.\ndoi:10.1007/s10071-005-0002-7\nLueddecke, T., Kulvicius, T., and Woergoetter, F. (2019). Context-based affordance\nsegmentation from 2d images for robot actions. Robotics Aut. Syst. 119, 92–107.\ndoi:10.1016/j.robot.2019.05.005\nLutscher, E., and Cheng, G. (2013). “A practical approach to generalized hierarchical\ntask specification for indirect force controlled robots,” in2013 IEEE/RSJ international\nconference on intelligent robots and systems(IEEE), 1854–1859.\nMadry, M., Song, D., Ek, C. H., and Kragic, D. (2012). “Robot bring me something to\ndrinkfrom”:objectrepresentationfortransferringtaskspecificgrasps,”in ICRA Workshop\non semantic perception, mapping and exploration, 1–6.\nManuelli,L.,Gao,W.,Florence,P.,andTedrake,R.(2019).“kpam:Keypointaffordances\nfor category-level robotic manipulation,” in The international symposium of robotics\nresearch(Springer), 132–157.\nMar, T., Tikhanoff, V., Metta, G., and Natale, L. (2015). “Self-supervised learning of\ngraspdependenttoolaffordancesontheicubhumanoidrobot,”in 2015 IEEE international\nconference on robotics and automation (ICRA)(IEEE), 3200–3206.\nMar, T., Tikhanoff, V., Metta, G., and Natale, L. (2017). “Self-supervised learning\nof tool affordances from 3d tool representation through parallel som mapping,” in\n2017 IEEE international conference on robotics and automation (ICRA)(IEEE), 894–901.\ndoi:10.1109/ICRA.2017.7989110\nMason, M. T. (2018). Toward robotic manipulation.Annu. Rev. Control, Robotics, Aut.\nSyst. 1, 1–28. doi:10.1146/annurev-control-060117-104848\nMatsuzawa, T. (1999).Communication and tool use in chimpanzees: Cultural and social\ncontexts. Cambridge, Massachusetts: The Designs of Animal Communication, 645–671.\nMigimatsu, T., and Bohg, J. (2020). Object-centric task and motion\nplanning in dynamic environments. IEEE Robotics Automation Lett. 5, 844–851.\ndoi:10.1109/lra.2020.2965875\nMoldovan, B., Moreno, P., and van Otterlo, M. (2013). “On the use of probabilistic\nrelational affordance models for sequential manipulation tasks in robotics,” in2013 IEEE\ninternational conference on robotics and automation(IEEE), 1290–1295.\nMoldovan, B., Moreno, P., Van Otterlo, M., Santos-Victor, J., and De Raedt, L. (2012).\n“Learning relational affordance models for robots in multi-object manipulation tasks,” in\n2012 ieee international conference on robotics and automation(IEEE), 4373–4378.\nMontesano, L., Lopes, M., Bernardino, A., and Santos-Victor, J. (2008). Learning object\naffordances: From sensory–motor coordination to imitation. IEEE Trans. Robotics 24,\n15–26. doi:10.1109/tro.2007.914848\nMontesano, L., Lopes, M., Bernardino, A., and Santos-Victor, J. (2007). “Modeling\naffordances using bayesian networks,” in 2007 IEEE/RSJ international conference on\nintelligent robots and systems(IEEE), 4102–4107.\nMuelling, K., Kober, J., and Peters, J. (2010). “Learning table tennis with a mixture of\nmotor primitives,” in2010 10th IEEE-RAS international conference on humanoid robots\n(IEEE), 411–416.\nMurali, A., Liu, W., Marino, K., Chernova, S., and Gupta, A. (2020). “Same object,\ndifferent grasps: Data and semantic knowledge for task-oriented grasping,” inConference\non robot learning.\nMyers, A., Teo, C. L., Fermüller, C., and Aloimonos, Y. (2015). “Affordance detection of\ntool parts from geometric features,” in2015 IEEE international conference on robotics and\nautomation (ICRA)(IEEE), 1374–1381.\nNabeshima, C., Kuniyoshi, Y., and Lungarella, M. (2007). “Towards a model for tool-\nbody assimilation and adaptive tool-use,” in2007 IEEE 6th international conference on\ndevelopment and learning(IEEE), 288–293.\nNabeshima, C., Lungarella, M., and Kuniyoshi, Y. (2005). “Timing-based model of\nbody schema adaptation and its role in perception and tool use: A robot case study,” in\nProceedings. The 4th international conference on development and learning, 2005(IEEE),\n7–12.\nNagata, F., Watanabe, K., and Izumi, K. (2001). “Furniture polishing robot using\na trajectory generator based on cutter location data,” inProceedings 2001 ICRA. IEEE\ninternational conference on robotics and automation (cat. No. 01CH37164) (IEEE), 1,\n319–324.\nNair, L., Balloch, J., and Chernova, S. (2019a). “Tool macgyvering: Tool construction\nusing geometric reasoning,” in2019 international conference on robotics and automation\n(ICRA) (IEEE), 5837–5843.\nNair, L., Srikanth, N. S., Erickson, Z. M., and Chernova, S. (2019b). “Autonomous tool\nconstructionusingpartshapeandattachmentprediction,”in Robotics:Scienceandsystems .\nNakamura, T., and Nagai, T. (2010). “Object concept modeling based on the\nrelationship among appearance, usage and functions,” in 2010 IEEE/RSJ international\nconference on intelligent robots and systems(IEEE), 5410–5415.\nNishide, S., Tani, J., Takahashi, T., Okuno, H. G., and Ogata, T. (2011). Tool–body\nassimilation of humanoid robot using a neurodynamical system.IEEE Trans. Aut. Ment.\nDev.4, 139–149. doi:10.1109/tamd.2011.2177660\nOakley, K. P. (1944). Man the tool-maker. Proc. Geologists’ Assoc. 55, 115–118.\ndoi:10.1016/s0016-7878(44)80012-8\nOkada, K., Kojima, M., Sagawa, Y., Ichino, T., Sato, K., and Inaba, M. (2006). “Vision\nbased behavior verification system of humanoid robot for daily environment tasks,” in\n2006 6th IEEE-RAS international conference on humanoid robots(IEEE), 7–12.\nParaschos, A., Daniel, C., Peters, J. R., and Neumann, G. (2013). Probabilistic\nmovement primitives.Adv. neural Inf. Process. Syst.26.\nParker, S. T., and Gibson, K. R. (1977). Object manipulation, tooluse and sensorimotor\nintelligence as feeding adaptations in cebus monkeys and great apes. J. Hum. Evol. 6,\n623–641. doi:10.1016/s0047-2484(77)80135-8\nPastor,P.,Hoffmann,H.,Asfour,T.,andSchaal,S.(2009).“Learningandgeneralization\nof motor skills by learning from demonstration,” in2009 IEEE international conference on\nrobotics and automation(IEEE), 763–768.\nPastor, P., Kalakrishnan, M., Chitta, S., Theodorou, E., and Schaal, S. (2011). “Skill\nlearning and task outcome prediction for manipulation,” in 2011 IEEE international\nconference on robotics and automation(IEEE), 3828–3834.\nPeternel, L., Petrič, T., and Babič, J. (2015). “Human-in-the-loop approach for teaching\nrobot assembly tasks using impedance control interface,” in 2015 IEEE international\nconference on robotics and automation (ICRA)(IEEE), 1497–1502.\nPeters, J., and Schaal, S. (2006). “Policy gradient methods for robotics,” in 2006\nIEEE/RSJ international conference on intelligent robots and systems(IEEE), 2219–2225.\nPfeiffer, K., Escande, A., and Kheddar, A. (2017). “Nut fastening with a humanoid\nrobot,” in2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)\n(IEEE), 6142–6148.\nQin, M., Brawer, J., and Scassellati, B. (2021). Rapidly learning generalizable and\nrobot-agnostic tool-use skills for a wide range of tasks. Front. Robotics AI 8, 726463.\ndoi:10.3389/frobt.2021.726463\nQin, M., Brawer, J., and Scassellati, B. (2022). “Task-oriented robot-to-human\nhandovers in collaborative tool-use tasks,” in2022 31th IEEE international conference on\nrobot and human interactive communication (RO-MAN)(IEEE).\nQin, Z., Fang, K., Zhu, Y., Fei-Fei, L., and Savarese, S. (2020). “Keto: Learning keypoint\nrepresentations for tool manipulation,” in2020 IEEE international conference on robotics\nand automation (ICRA)(IEEE), 7278–7285.\nRaessa, M., Sánchez, D., Wan, W., Petit, D., and Harada, K. (2019). Teaching a\nrobot to use electric tools with regrasp planning.CAAI Trans. Intell. Technol.4, 54–63.\ndoi:10.1049/trit.2018.1062\nRamirez-Amaro, K., Beetz, M., and Cheng, G. (2014a). “Automatic segmentation and\nrecognition of human activities from observation based on semantic reasoning,” in2014\nIEEE/RSJ international conference on intelligent robots and systems(IEEE), 5043–5048.\nRamirez-Amaro, K., Beetz, M., and Cheng, G. (2015). Understanding the intention of\nhumanactivitiesthroughsemanticperception:Observation,understandingandexecution\non a humanoid robot.Adv. Robot.29, 345–362. doi:10.1080/01691864.2014.1003096\nFrontiers in Robotics and AI 15 frontiersin.org\nQin et al. 10.3389/frobt.2022.1009488\nRamirez-Amaro, K., Inamura, T., Dean-León, E., Beetz, M., and Cheng, G. (2014b).\n“Bootstrapping humanoid robot skills by extracting semantic representations of human-\nlikeactivitiesfromvirtualreality,”in 2014 IEEE-RAS international conference on humanoid\nrobots (IEEE), 438–443.\nRasheed, A. A. A., Abdullah, M. N., and Al-Araji, A. S. (2022). A review of multi-agent\nmobile robot systems applications.Int. J. Electr. Comput. Eng., 12.\nRobertsson, A., Olsson, T., Johansson, R., Blomdell, A., Nilsson, K., Haage, M., et al.\n(2006). “Implementation of industrial robot force control case study: High power stub\ngrinding and deburring,” in2006 IEEE/RSJ international conference on intelligent robots\nand systems(IEEE), 2743–2748.\nRossi, F., Bandyopadhyay, S., Wolf, M., and Pavone, M. (2018). Review of multi-\nagent algorithms for collective behavior: A structural taxonomy.IFAC-PapersOnLine51,\n112–117. doi:10.1016/j.ifacol.2018.07.097\nRozo, L., Jiménez, P., and Torras, C. (2013). “Force-based robot learning of pouring\nskills using parametric hidden markov models,” in9th international workshop on robot\nmotion and control(IEEE), 227–232.\nRuiz, E., and Mayol-Cuevas, W. (2018). “Where can i do this? Geometric affordances\nfrom a single example with the interaction tensor,” in2018 IEEE international conference\non robotics and automation (ICRA)(IEEE), 2192–2199.\nSaito, N., Kim, K., Murata, S., Ogata, T., and Sugano, S. (2018). “Tool-use model\nconsidering tool selection by a robot using deep learning,” in 2018 IEEE-RAS 18th\ninternational conference on humanoid robots (humanoids)(IEEE), 270–276.\nSammut, C., Sheh, R., Haber, A., and Wicaksono, H. (2015). “The robot engineer,” in\nILP (late breaking papers), 101–106.\nSarikaya, D., Corso, J. J., and Guru, K. A. (2017). Detection and localization of robotic\ntools in robot-assisted surgery videos using deep neural networks for region proposal and\ndetection. IEEE Trans. Med. imaging36, 1542–1549. doi:10.1109/tmi.2017.2665671\nSchaal, S. (2006). “Dynamic movement primitives-a framework for motor control in\nhumans and humanoid robotics,” inAdaptive motion of animals and machines(Springer),\n261–280.\nSchoeler, M., and Wörgötter, F. (2015). Bootstrapping the semantics of tools:\nAffordance analysis of real world objects on a per-part basis.IEEE Trans. Cognitive Dev.\nSyst. 8, 84–98. doi:10.1109/tamd.2015.2488284\nShao, L., Migimatsu, T., Zhang, Q., Yang, K., and Bohg, J. (2021). Concept2robot:\nLearning manipulation concepts from instructions and human demonstrations.Int. J.\nRobotics Res.40, 1419–1434. doi:10.1177/02783649211046285\nShumaker, R. W., Walkup, K. R., and Beck, B. B. (2011).Animal tool behavior: The use\nand manufacture of tools by animals. Charles Village, Baltimore: JHU Press.\nSinapov, J., and Stoytchev, A. (2008). “Detecting the functional similarities between\ntoolsusingahierarchicalrepresentationofoutcomes,”in 7th IEEE international conference\non development and learning(IEEE), 91–96. doi:10.1109/DEVLRN.2008.4640811\nSinapov, J., and Stoytchev, A. (2007). “Learning and generalization of behavior-\ngrounded tool affordances,” in2007 IEEE 6th international conference on development and\nlearning (IEEE), 19–24.\nSong, D., Ek, C. H., Huebner, K., and Kragic, D. (2011a). “Embodiment-specific\nrepresentation of robot grasping using graphical models and latent-space discretization,”\nin 2011 IEEE/RSJ international conference on intelligent robots and systems (IEEE),\n980–986.\nSong, D., Ek, C. H., Huebner, K., and Kragic, D. (2011b). “Multivariate discretization\nfor bayesian network structure learning in robot grasping,” in2011 IEEE international\nconference on robotics and automation(IEEE), 1944–1950.\nSong, D., Ek, C. H., Huebner, K., and Kragic, D. (2015). Task-based robot\ngrasp planning using probabilistic inference. IEEE Trans. robotics 31, 546–561.\ndoi:10.1109/tro.2015.2409912\nSong, D., Huebner, K., Kyrki, V., and Kragic, D. (2010). “Learning task constraints\nfor robot grasping using graphical models,” in2010 IEEE/RSJ international conference on\nintelligent robots and systems(IEEE), 1579–1585.\nSpagnoletti, N., Visalberghi, E., Ottoni, E., Izar, P., and Fragaszy, D. (2011). Stone tool\nusebyadultwildbeardedcapuchinmonkeys(cebuslibidinosus).frequency,efficiencyand\ntool selectivity.J. Hum. Evol.61, 97–107. doi:10.1016/j.jhevol.2011.02.010\nSt. Amant, R., and Horton, T. E. (2008). Revisiting the definition of animal tool use.\nAnim. Behav.75, 1199–1208. doi:10.1016/j.anbehav.2007.09.028\nStoytchev, A. (2005a). “Behavior-grounded representation of tool affordances,” in\nProceedings of IEEE international conference on robotics and automation (ICRA)(IEEE),\n3071–3076. doi:10.1109/ROBOT.2005.1570580\nStoytchev, A. (2003).Computational model for an extendable robot body schema. Tech.\nrep. Atlanta, Georgia: Georgia Institute of Technology.\nStoytchev, A. (2008). “Learning the affordances of tools using a behavior-grounded\napproach,” inTowards affordance-based robot control(Springer), 140–158.\nStoytchev,A.(2007). Robot tool behavior: A developmental approach to autonomous tool\nuse. Atlanta, Georgia: Georgia Institute of Technology.\nStoytchev,A.(2005b).“Towardlearningthebindingaffordancesofobjects:Abehavior-\ngrounded approach,” in Proceedings of AAAI symposium on developmental robotics\n(Stanford University Menlo Park), 17–22.\nStückler, J., and Behnke, S. (2014a). “Adaptive tool-use strategies for anthropomorphic\nservice robots,” in2014 IEEE-RAS international conference on humanoid robots(IEEE),\n755–760.\nStückler, J., and Behnke, S. (2014b). “Efficient deformable registration of\nmulti-resolution surfel maps for object manipulation skill transfer,” in 2014\nIEEE international conference on robotics and automation (ICRA) (IEEE), 994–\n1001.\nStückler, J., and Behnke, S. (2015). “Perception of deformable objects and compliant\nmanipulation for service robots,” inSoft robotics(Springer), 69–80.\nStückler, J., Droeschel, D., Gräve, K., Holz, D., Schreiber, M., Topalidou-\nKyniazopoulou, A., et al. (2013). “Increasing flexibility of mobile manipulation\nand intuitive human-robot interaction in robocup@ home,” inRobot soccer world cup\n(Springer), 135–146.\nStückler, J., Schwarz, M., and Behnke, S. (2016). Mobile manipulation, tool use,\nand intuitive interaction for cognitive service robot cosero. Front. Robotics AI 3, 58.\ndoi:10.3389/frobt.2016.00058\nSu, Y.-H., Huang, K., and Hannaford, B. (2018). “Real-time vision-based surgical tool\nsegmentationwithrobotkinematicsprior,”in International symposium on medical robotics\n(ISMR) (IEEE), 1–6.\nSukhoy, V., Georgiev, V., Wegter, T., Sweidan, R., and Stoytchev, A. (2012). Learning to\nslide a magnetic card through a card reader.2012 IEEE international conference on robotics\nand automation(IEEE), 2398–2404.\nTakahashi, K., Kim, K., Ogata, T., and Sugano, S. (2017). Tool-body assimilation model\nconsidering grasping motion through deep learning. Robotics Aut. Syst. 91, 115–127.\ndoi:10.1016/j.robot.2017.01.002\nTakeuchi, Y., Ge, D., and Asakawa, N. (1993). “Automated polishing process with a\nhuman-likedexterousrobot,”in 1993 proceedings IEEE international conference on robotics\nand automation(IEEE), 950–956.\nTee, K. P., Cheong, S., Li, J., and Ganesh, G. (2022). A framework for tool cognition\nin robots without prior tool learning or observation. Nat. Mach. Intell. 4, 533–543.\ndoi:10.1038/s42256-022-00500-9\nTee, K. P., Li, J., Chen, L. T. P., Wan, K. W., and Ganesh, G. (2018). “Towards emergence\nof tool use in robots: Automatic tool recognition and use without prior tool learning,”\nin 2018 IEEE international conference on robotics and automation (ICRA)(IEEE), 6439–\n6446.\nTikhanoff,V.,Pattacini,U.,Natale,L.,andMetta,G.(2013).“Exploringaffordancesand\ntool use on the icub,” in2013 13th IEEE-RAS international conference on humanoid robots\n(humanoids)(IEEE), 130–137. doi:10.1109/HUMANOIDS.2013.7029967\nToussaint,M.A.,Allen,K.R.,Smith,K.A.,andTenenbaum,J.B.(2018).“Differentiable\nphysicsandstablemodesfortool-useandmanipulationplanning,”in Robotics:Scienceand\nsystems foundation.\nTsuji,T.,Ohkuma,J.,andSakaino,S.(2015).Dynamicobjectmanipulationconsidering\ncontact condition of robot with tool. IEEE Trans. Industrial Electron. 63, 1972–1980.\ndoi:10.1109/tie.2015.2508929\nTurpin, D., Wang, L., Tsogkas, S., Dickinson, S., and Garg, A. (2021). Gift:\nGeneralizable interaction-aware functional tool affordances without labels. arXiv preprint\narXiv:2106.14973.\nVan Lawick-Goodall, J. (1970). “Tool-using in primates and other vertebrates,” in\nAdvances in the study of behavior(Elsevier), 3, 195–249.\nVisalberghi, E., and Fragaszy, D. (2006). What is challenging about tool use? The\ncapuchin’s perspective.Comp. cognition Exp. Explor. animal Intell., 529–552.\nVogel, J., Takemura, N., Höppner, H., van der Smagt, P., and Ganesh, G. (2017).\n“Hitting the sweet spot: Automatic optimization of energy transfer during tool-held hits,”\nin 2017 IEEE international conference on robotics and automation (ICRA)(IEEE), 1549–\n1556.\nWicaksono, H., and Sammut, C. (2016). Relational tool use learning by a robot in a real\nand simulated world.Proc. ACRA.\nWimpenny, J. H., Weir, A. A., Clayton, L., Rutz, C., and Kacelnik, A. (2009). Cognitive\nprocesses associated with sequential tool use in new caledonian crows.PLoS One4, e6471.\ndoi:10.1371/journal.pone.0006471\nFrontiers in Robotics and AI 16 frontiersin.org\nQin et al. 10.3389/frobt.2022.1009488\nWölfel,K.,andHenrich,D.(2018).“Groundingverbsfortool-dependent,sensor-based\nrobot tasks,” in2018 27th IEEE international symposium on robot and human interactive\ncommunication (RO-MAN)(IEEE), 378–383.\nXie, A., Ebert, F., Levine, S., and Finn, C. (2019). “Improvisation through\nphysical understanding: Using novel objects as tools with visual foresight,”\nin Proceedings of robotics: Science and systems (Germany: FreiburgimBreisgau).\ndoi:10.15607/RSS.2019.XV.001\nXue, Y., and Jia, Y.-B. (2020). “Gripping a kitchen knife on the cutting board,” in\n2020 IEEE/RSJ international conference on intelligent robots and systems (IROS)(IEEE).\n9226–9231.\nYamazaki, K., Ueda, R., Nozawa, S., Mori, Y., Maki, T., Hatao, N., et al. (2010).\n“System integration of a daily assistive robot and its application to tidying and cleaning\nrooms,” in2010 IEEE/RSJ international conference on intelligent robots and systems(IEEE),\n1365–1371.\nZech, P., Haller, S., Lakani, S. R., Ridge, B., Ugur, E., and Piater, J.\n(2017). Computational models of affordance in robotics: A taxonomy and\nsystematic classification. Adapt. Behav. 25, 235–271. doi: 10.1177/105971231\n7726357\nZhu, Y., Zhao, Y., and Chun Zhu, S. (2015). “Understanding tools:\nTask-oriented object modeling, learning and recognition,” in Proceedings\nof the IEEE conference on computer vision and pattern recognition , 2855–\n2864.\nZhu, Z., and Hu, H. (2018). Robot learning from demonstration in robotic assembly:\nA survey.Robotics 7, 17. doi:10.3390/robotics7020017\nFrontiers in Robotics and AI 17 frontiersin.org",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7079095840454102
    },
    {
      "name": "Human–computer interaction",
      "score": 0.6823060512542725
    },
    {
      "name": "Robot",
      "score": 0.6554387211799622
    },
    {
      "name": "Affordance",
      "score": 0.5816909074783325
    },
    {
      "name": "Task (project management)",
      "score": 0.5639106035232544
    },
    {
      "name": "Categorization",
      "score": 0.5551846623420715
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47999307513237
    },
    {
      "name": "Perception",
      "score": 0.43817609548568726
    },
    {
      "name": "Context (archaeology)",
      "score": 0.43674567341804504
    },
    {
      "name": "Engineering",
      "score": 0.11588865518569946
    },
    {
      "name": "Systems engineering",
      "score": 0.09710279107093811
    },
    {
      "name": "Psychology",
      "score": 0.07544472813606262
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I32971472",
      "name": "Yale University",
      "country": "US"
    }
  ]
}