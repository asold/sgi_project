{
  "title": "Bidirectional Encoder Representations from Transformers-like large language models in patient safety and pharmacovigilance: A comprehensive assessment of causal inference implications",
  "url": "https://openalex.org/W4389617061",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2108084834",
      "name": "Xingqiao Wang",
      "affiliations": [
        "University of Arkansas at Little Rock"
      ]
    },
    {
      "id": "https://openalex.org/A2097932941",
      "name": "Xu Xiaowei",
      "affiliations": [
        "University of Arkansas at Little Rock"
      ]
    },
    {
      "id": "https://openalex.org/A2120112472",
      "name": "Zhichao Liu",
      "affiliations": [
        "Boehringer Ingelheim (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2267638323",
      "name": "Weida Tong",
      "affiliations": [
        "National Center for Toxicological Research"
      ]
    },
    {
      "id": "https://openalex.org/A2108084834",
      "name": "Xingqiao Wang",
      "affiliations": [
        "University of Arkansas at Little Rock"
      ]
    },
    {
      "id": "https://openalex.org/A2097932941",
      "name": "Xu Xiaowei",
      "affiliations": [
        "University of Arkansas at Little Rock"
      ]
    },
    {
      "id": "https://openalex.org/A2120112472",
      "name": "Zhichao Liu",
      "affiliations": [
        "Boehringer Ingelheim (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2267638323",
      "name": "Weida Tong",
      "affiliations": [
        "National Center for Toxicological Research"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2612600212",
    "https://openalex.org/W2027710934",
    "https://openalex.org/W2925078665",
    "https://openalex.org/W2137053633",
    "https://openalex.org/W3121914643",
    "https://openalex.org/W4280590646",
    "https://openalex.org/W3138676254",
    "https://openalex.org/W4280488717",
    "https://openalex.org/W4280496566",
    "https://openalex.org/W4221141771",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4383346782",
    "https://openalex.org/W3174167596",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4365143687",
    "https://openalex.org/W3165015862",
    "https://openalex.org/W4311631201",
    "https://openalex.org/W2000660841",
    "https://openalex.org/W3198127956",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W6600424091",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W6636364444",
    "https://openalex.org/W2017851316",
    "https://openalex.org/W2041816447"
  ],
  "abstract": "Causality assessment is vital in patient safety and pharmacovigilance (PSPV) for safety signal detection, adverse reaction management, and regulatory submission. Large language models (LLMs), especially those designed with transformer architecture, are revolutionizing various fields, including PSPV. While attempts to utilize Bidirectional Encoder Representations from Transformers (BERT)-like LLMs for causal inference in PSPV are underway, a detailed evaluation of “fit-for-purpose” BERT-like model selection to enhance causal inference performance within PSPV applications remains absent. This study conducts an in-depth exploration of BERT-like LLMs, including generic pre-trained BERT LLMs, domain-specific pre-trained LLMs, and domain-specific pre-trained LLMs with safety knowledge-specific fine-tuning, for causal inference in PSPV. Our investigation centers around (1) the influence of data complexity and model architecture, (2) the correlation between the BERT size and its impact, and (3) the role of domain-specific training and fine-tuning on three publicly accessible PSPV data sets. The findings suggest that (1) BERT-like LLMs deliver consistent predictive power across varied data complexity levels, (2) the predictive performance and causal inference results do not directly correspond to the BERT-like model size, and (3) domain-specific pre-trained LLMs, with or without safety knowledge-specific fine-tuning, surpass generic pre-trained BERT models in causal inference. The findings are valuable to guide the future application of LLMs in a broad range of application.",
  "full_text": null,
  "topic": "Pharmacovigilance",
  "concepts": [
    {
      "name": "Pharmacovigilance",
      "score": 0.8899839520454407
    },
    {
      "name": "Inference",
      "score": 0.7016983032226562
    },
    {
      "name": "Transformer",
      "score": 0.5941038131713867
    },
    {
      "name": "Causal inference",
      "score": 0.5697672963142395
    },
    {
      "name": "Computer science",
      "score": 0.5206674337387085
    },
    {
      "name": "Encoder",
      "score": 0.4442005157470703
    },
    {
      "name": "Natural language processing",
      "score": 0.38149788975715637
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3564341962337494
    },
    {
      "name": "Medicine",
      "score": 0.34667158126831055
    },
    {
      "name": "Econometrics",
      "score": 0.2291310429573059
    },
    {
      "name": "Pharmacology",
      "score": 0.22511819005012512
    },
    {
      "name": "Adverse effect",
      "score": 0.19898280501365662
    },
    {
      "name": "Engineering",
      "score": 0.15938329696655273
    },
    {
      "name": "Mathematics",
      "score": 0.12954729795455933
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I102401767",
      "name": "University of Arkansas at Little Rock",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210160255",
      "name": "Boehringer Ingelheim (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1304557061",
      "name": "National Center for Toxicological Research",
      "country": "US"
    }
  ],
  "cited_by": 2
}