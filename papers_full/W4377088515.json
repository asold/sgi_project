{
  "title": "SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification",
  "url": "https://openalex.org/W4377088515",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2747825853",
      "name": "Miao, Xupeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227517171",
      "name": "Oliaro, Gabriele",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2353748884",
      "name": "Zhang Zhihao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2353613879",
      "name": "Cheng Xin-hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2352651512",
      "name": "Wang, Zeyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2352700116",
      "name": "Zhang Zhengxin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4377702969",
      "name": "Wong, Rae Ying Yee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2353791890",
      "name": "Zhu A-lan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1929462639",
      "name": "Yang Lijie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2349685105",
      "name": "Shi Xiaoxiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4310284392",
      "name": "Shi, Chunan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A28950106",
      "name": "Chen Zhuo-ming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4301616898",
      "name": "Arfeen, Daiyaan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4377702972",
      "name": "Abhyankar, Reyna",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108421916",
      "name": "Jia, Zhihao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2158992088",
    "https://openalex.org/W4390306161",
    "https://openalex.org/W4310561894",
    "https://openalex.org/W4289302788",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W3173154111",
    "https://openalex.org/W4313484599",
    "https://openalex.org/W2145073242",
    "https://openalex.org/W4394998727",
    "https://openalex.org/W4362707064",
    "https://openalex.org/W4288562606",
    "https://openalex.org/W3037847693",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W4200634402",
    "https://openalex.org/W2725179571",
    "https://openalex.org/W2122563027",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3149839747",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4309591680",
    "https://openalex.org/W4283313765",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W4287758790",
    "https://openalex.org/W4377088515",
    "https://openalex.org/W3159727696",
    "https://openalex.org/W4281651027",
    "https://openalex.org/W4364385701",
    "https://openalex.org/W4319166707",
    "https://openalex.org/W2804032941",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W4324297016",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W4287327026",
    "https://openalex.org/W3008282111"
  ],
  "abstract": "This paper introduces SpecInfer, a system that accelerates generative large language model (LLM) serving with tree-based speculative inference and verification. The key idea behind SpecInfer is leveraging small speculative models to predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified against the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality. Our evaluation shows that SpecInfer outperforms existing LLM serving systems by 1.5-2.8x for distributed LLM inference and by 2.6-3.5x for offloading-based LLM inference, while preserving the same generative performance. SpecInfer is publicly available at https://github.com/flexflow/FlexFlow/",
  "full_text": "SpecInfer: Accelerating Large Language Model Serving\nwith Tree-based Speculative Inference and Verification\nXupeng Miaoâˆ—\nCarnegie Mellon University\nPittsburgh, PA, USA\nxupeng@cmu.edu\nGabriele Oliaroâˆ—\nCarnegie Mellon University\nPittsburgh, PA, USA\ngoliaro@cs.cmu.edu\nZhihao Zhangâˆ—\nCarnegie Mellon University\nPittsburgh, PA, USA\nzhihaoz3@andrew.cmu.edu\nXinhao Chengâˆ—\nCarnegie Mellon University\nPittsburgh, PA, USA\nxinhaoc@andrew.cmu.edu\nZeyu Wang\nCarnegie Mellon University\nPittsburgh, PA, USA\nzeyuwang@alumni.cmu.edu\nZhengxin Zhang\nTsinghua University\nBeijing, China\nzhang-zx21@mails.tsinghua.edu.cn\nRae Ying Yee Wong\nStanford University\nStanford, CA, USA\nraewong@stanford.edu\nAlan Zhu\nCarnegie Mellon University\nPittsburgh, PA, USA\naczhu@andrew.cmu.edu\nLijie Yang\nCarnegie Mellon University\nPittsburgh, PA, USA\nlijiey@andrew.cmu.edu\nXiaoxiang Shi\nShanghai Jiao Tong University\nShanghai, China\nlambda7shi@sjtu.edu.cn\nChunan Shi\nPeking University\nBeijing, China\nspirited_away@pku.edu.cn\nZhuoming Chen\nCarnegie Mellon University\nPittsburgh, PA, USA\nzhuominc@andrew.cmu.edu\nDaiyaan Arfeen\nCarnegie Mellon University\nPittsburgh, PA, USA\nmarfeen@andrew.cmu.edu\nReyna Abhyankar\nUniversity of California, San Diego\nSan Diego, CA, USA\nvabhyank@ucsd.edu\nZhihao Jia\nCarnegie Mellon University\nPittsburgh, PA, USA\nzhihao@cmu.edu\nAbstract\nThis paper introduces SpecInfer, a system that accelerates\ngenerative large language model (LLM) serving with tree-\nbased speculative inference and verification. The key idea\nbehind SpecInfer is leveraging small speculative models to\npredict the LLMâ€™s outputs; the predictions are organized as\na token tree, whose nodes each represent a candidate token\nsequence. The correctness of all candidate token sequences\nrepresented by a token tree is verified against the LLM in par-\nallel using a novel tree-based parallel decoding mechanism.\nSpecInfer uses an LLM as a token tree verifier instead of an\nincremental decoder, which significantly reduces the end-to-\nend latency and computational requirement for serving gen-\nerative LLMs while provably preserving model quality. Our\nâˆ—Equal contribution.\nPermission to make digital or hard copies of part or all of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies\nbear this notice and the full citation on the first page. Copyrights for third-\nparty components of this work must be honored. For all other uses, contact\nthe owner/author(s).\nASPLOS â€™24, April 27-May 1, 2024, La Jolla, CA, USA\nÂ© 2024 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0386-7/24/04.\nhttps://doi.org/10.1145/3620666.3651335\nevaluation shows that SpecInfer outperforms existing LLM\nserving systems by 1.5-2.8Ã—for distributed LLM inference\nand by 2.6-3.5Ã—for offloading-based LLM inference, while\npreserving the same generative performance. SpecInfer is\npublicly available at https://github.com/flexflow/FlexFlow/\nKeywords: large language model serving, speculative decod-\ning, token tree verification\nACM Reference Format:\nXupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu\nWang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang,\nXiaoxiang Shi, Chunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna\nAbhyankar, and Zhihao Jia. 2024. SpecInfer: Accelerating Large\nLanguage Model Serving with Tree-based Speculative Inference and\nVerification. In 29th ACM International Conference on Architectural\nSupport for Programming Languages and Operating Systems, Volume\n3 (ASPLOS â€™24), April 27-May 1, 2024, La Jolla, CA, USA. ACM, New\nYork, NY, USA, 18 pages. https://doi.org/10.1145/3620666.3651335\n1 Introduction\nGenerative large language models (LLMs), such as Chat-\nGPT [3] and GPT-4 [ 33], have proven to be powerful in\nvarious application domains, including question answering,\nprogram synthesis, and task automation [26, 56]. However,\nit is challenging to quickly and cheaply serve these LLMs\narXiv:2305.09781v4  [cs.CL]  1 Apr 2024\ndue to their large volume of parameters, complex architec-\ntures, and high computational requirements. For example,\nthe largest GPT-3 architecture has 175 billion parameters,\nwhich requires more than eight NVIDIA 40GB A100 GPUs\nto store in half-precision floating points, and takes several\nseconds to serve a single inference request [3].\nAn LLM generally takes as input a sequence of tokens,\ncalled prompt, and generates subsequent tokens one at a time,\nas shown in Figure 1a. The generation of each token in the\nsequence is conditioned on the input prompt and previously\ngenerated tokens and does not consider future tokens. This\napproach is also called autoregressive decoding because each\ngenerated token is also used as input for generating future\ntokens. This dependency between tokens is crucial for many\nNLP tasks that require preserving the order and context of\nthe generated tokens, such as text completion [55].\nExisting LLM systems generally use an incremental decod-\ning approach to serving a request where the system computes\nthe activations for all prompt tokens in a single step and then\niteratively decodes one new token using the input prompt\nand all previously generated tokens [27]. This approach re-\nspects data dependencies between tokens, but achieves sub-\noptimal runtime performance and limited GPU utilization,\nsince the degree of parallelism within each request is greatly\nlimited in the incremental phase. In addition, the attention\nmechanism of Transformer [48] requires accessing the keys\nand values of all previous tokens to compute the attention\noutput of a new token. To avoid recomputing the keys and\nvalues for all preceding tokens, todayâ€™s LLM systems use a\ncaching mechanism to store their keys and values for reuse\nin future iterations. For long-sequence generative tasks (e.g.,\nGPT-4 supports up to 32K tokens in a request), caching keys\nand values introduces significant memory overhead, which\nprevents existing systems from serving a large number of re-\nquests in parallel due to the memory requirement of caching\ntheir keys and values.\nMotivated by the idea of speculative execution in proces-\nsor optimizations [13, 42], recent work introduces sequence-\nbased speculative inference , which leverages a small specula-\ntive model (SSM) to generate a sequence of tokens and uses an\nLLM to examine their correctness in parallel [5, 22, 25, 44, 51].\nThese attempts only consider a token sequence generated by\na single SSM for speculation, which cannot align well with\nan LLM due to the model capacity gap between them, since\nSSMs are generally orders of magnitude smaller than the\nLLM to maintain low memory and runtime overheads.\nThis paper introduces SpecInfer, a system that improves\nthe end-to-end latency and computational efficiency of LLM\nserving with tree-based speculative inference and verifica-\ntion. Figure 1b illustrates a comparison between existing\nincremental decoding, sequence-based speculative inference,\nand our tree-based speculative inference. A key insight be-\nhind SpecInfer is to simultaneously consider a diversity of\nspeculation candidates (instead of just one as in existing\napproaches) to maximize speculative performance. These\ncandidates are organized as a token tree , whose nodes each\nrepresents a sequence of speculated tokens. The correctness\nof all candidate token sequences is verified against the LLM\nin parallel, which allows SpecInfer to significantly increase\nthe number of generated tokens in an LLM decoding step.\nCompared with sequence-based speculative inference, lever-\naging tree structures can significantly improve the success\nrate of verifying a token (e.g., from 52-57% to 96-97% for\nstochastic decoding as shown in Table 1). However, realizing\nthis improvement requires addressing two unique challenges.\nNext, we elaborate on these challenges and the main ideas\nSpecInfer uses to address them.\nFirst, SpecInfer must explore an extremely large search\nspace of candidate token sequences to maximize speculative\nperformance. While the idea of speculative execution has\nbeen widely deployed in a variety of optimization tasks in\ncomputer architecture and systems, including branch pre-\ndiction in modern pipelined processors and value prediction\nfor pre-fetching memory and files [13, 42], the search space\nconsidered by SpecInfer is significantly larger due to two\nreasons: (1) modern LLMs generally involve very large vocab-\nularies, and (2) maximizing speculative performance requires\npredicting multiple future tokens (instead of just the next\ntoken). For example, all LLMs in the OPT model family con-\nsider 50,272 different possible tokens in their vocabulary,\nwhile SpecInfer can correctly predict the next 4 tokens on\naverage. Achieving this goal requires considering a search\nspace of 502724 â‰ˆ6 Ã—1018 different combinations of tokens.\nSpecInfer leverages existing distilled, quantized, and/or\npruned variants of an LLM, which we call small specula-\ntive models (SSMs), to guide speculation. A key challenging\nof using SSMs for speculative inference is that the align-\nment between an SSM and an LLM is inherently bounded\nby the model capacity gap, since an SSM is generally 100-\n1000Ã—smaller than an LLM. Instead of using a single SSM\nfor sequence-based speculation, SpecInfer maximizes spec-\nulative performance by simultaneously considering a vari-\nety of token sequences organized in a tree structure for a\ngiven input prompt. SpecInfer introduces an expansion- and\na merge-based mechanism for constructing token trees by\nexploiting diversity within a single SSM and across multiple\nSSMs, respectively.\nA second challenge SpecInfer must address is verifying the\nspeculated tokens. Many LLM applications perform stochas-\ntic decoding, which samples the next token from a probability\ndistribution instead of deterministically generating a token.\nTo preserve an LLMâ€™s generative performance, SpecInfer\nmust guarantee that its tree-based speculative inference and\nverification mechanism generates the next token by follow-\ning the exact same probability distribution as incremental\ndecoding. To achieve this goal, we propose multi-step spec-\nulative sampling , a new sampling approach for SSMs that\nguarantees equivalence while maximizing the number of\n2\nLLM\nmachinelearningsystemoptimization\nlearningsystemoptimizationis\nIterations:0 1 2 3\nOutputs:\n!! !\" !# IncDecodeIncDecodeIncDecodeIncDecode...!$\n!! !\" !# !%\nTreeVerifyTreeVerify!$\n!!â†’!\"â†’!#â†’!%\n!$\n!%\n!&â†’!'â†’!(â†’!)\n...\nIncremental Decoding\n Incremental Decoding Timeline\nTree-based Speculative Inference TimelineSpecSpec SpecSpec ......SpecSpec ...\nSequence-based Speculative Inference Timeline\nSeqVerifySeqVerify!$\n!!â†’!\"\n!\"\n!#â†’!%\n...Spec Spec ...\nSequenceVerifySequenceVerify!$\n!!â†’!\"\n!\"\n!#â†’!% ...Spec Spec ...\n(a) Incremental decoding.\nLLM\nmachinelearningsystemoptimization\nlearningsystemoptimizationis\nIterations:0 1 2 3\nOutputs:\n!! !\" !# IncDecodeIncDecodeIncDecodeIncDecode...!$\n!! !\" !# !%\nTreeVerifyTreeVerify!$\n!!â†’!\"â†’!#â†’!%\n!$\n!%\n!&â†’!'â†’!(â†’!)\n...\nIncremental Decoding\n Incremental Decoding Timeline\nTree-based Speculative Inference TimelineSpecSpec SpecSpec ......SpecSpec ...\nSequence-based Speculative Inference Timeline\nSeqVerifySeqVerify!$\n!!â†’!\"\n!\"\n!#â†’!%\n...Spec Spec ...\nSequenceVerifySequenceVerify!$\n!!â†’!\"\n!\"\n!#â†’!% ...Spec Spec ... (b) Timeline Comparison.\nFigure 1. Comparing the incremental decoding approach used by existing LLM serving systems, the sequence-based speculative\ninference approach, and the tree-based speculative inference approach used by SpecInfer.\nspeculated tokens that can be verified. To minimize the to-\nken tree verification cost, SpecInfer introduces a tree-based\nparallel decoding mechanism, simultaneously verifying all\ntokens of a token tree against the LLMâ€™s output in a single\nLLM decoding step.\nBy leveraging tree-based speculative inference and verifi-\ncation, SpecInfer accelerates both distributed LLM inference\nacross multiple GPUs and offloading-based LLM inference on\none GPU. Our evaluation shows that SpecInfer outperforms\nexisting LLM serving systems by 1.5-2.8 Ã—for distributed\nLLM inference and by 2.6-3.5 Ã—for offloading-based LLM\ninference, while preserving the same generative accuracy.\nTo summarize, we make the following contributions:\nâ€¢We present SpecInfer, a tree-based speculative infer-\nence and verification system for LLM serving.\nâ€¢To maximize speculative performance, we propose a\nmerge- and an expansion-based method to construct\ntoken trees by exploiting diversity within and across\nSSMs, respectively.\nâ€¢To minimize verification cost, we introduce a tree-\nbased parallel decoding mechanism to simultaneously\nverify all tokens of a token tree.\nâ€¢We evaluate SpecInfer and show that it outperforms\nexisting systems by up to 2.8Ã—for distributed inference\nand by up to 3.5Ã—for offloading-based inference.\n2 SpecInferâ€™s Overview\nFigure 2 shows an overview of SpecInfer, which includes a\nlearning-based speculator that takes as input a sequence of\ntokens, and produces a speculated token tree . The goal of the\nspeculator is to predict the LLMâ€™s output by maximizing the\noverlap between the speculated token tree and the tokens\ngenerated by the LLM using incremental decoding (Alg. 1).\nAlgorithm 1 The incremental decoding algorithm used in\nexisting LLM serving systems.\n1: Input: A sequence of input tokens â„\n2: Output: A sequence of generated tokens\n3: ğ’® = â„\n4: while true do\n5: ğ‘¡ = Decode(LLM,ğ’®)\n6: ğ’®.append(ğ‘¡)\n7: if ğ‘¡ = âŸ¨EOSâŸ©then\n8: Return ğ’®\nThere are several ways to prepare SSMs for speculative\ninference. First, modern LLMs generally have many smaller\narchitectures pre-trained together with the LLM using the\nsame datasets. For example, in addition to the OPT-175B\nmodel with 175 billion parameters, the OPT model family\nalso includes OPT-125M and OPT-350M, two variants with\n125 million and 350 million parameters, which were pre-\ntrained using the same datasets as OPT-175B [57]. These pre-\ntrained small models can be directly used as SSMs. Second,\nto improve the coverage of speculated tokens from SSMs,\nSpecInfer takes an expansion-based and a merge-based spec-\nulation method as shown at the top of Figure 2. The specu-\nlated tokens are organized in a token tree structure.\nSpecInferâ€™s usage of an LLM is also different from that of\nexisting LLM serving systems. Instead of using the LLM as\nan incremental decoder that predicts the next single token,\nSpecInfer uses the LLM as a token tree verifier that verifies\na speculated token tree against the LLMâ€™s output. For each\ntoken, SpecInfer computes its activations by considering all\nof its ancestors in the token tree as its preceding tokens. For\nexample, in Figure 2, the attention output of the token ğ‘¡3,0\nis calculated based on sequence (ğ‘¡0,ğ‘¡1,0,ğ‘¡2,1,ğ‘¡3,0), where ğ‘¡0,\n3\nLearning-based Speculator(Â§\")\n!!machine!\",!learning!\",\"translation\n!#,!algorithm!#,\"system!$,!design!#,#modelSpeculatedToken Tree\n[inputs] Tree Merging\nmachine learning system optimization model\nToken Tree Verifier (Â§4)\nVerifiedSubtree\nSynchronization\nVerified output: machine learning system optimization\n!!machine\n!\",!learning\n!\",\"translation\n!#,!algorithm\n!#,\"system\n!$,!design\n!#,#model\n[machine]  learning  algorithm  system  design  translation  model\nLLMTree-based Parallel Decoding\nlearning   system    design optimization [EOS]    model    [EOS]\nLinearization\nSSM 0SSM1 SSM 2\n!$,#optimization\n!$,\"design!%,![EOS]\n!$,$[EOS]\nMerge-basedToken Tree Construction\nSequence Representation of Speculated Tokens\nSSM\nExpansion-basedToken Tree ConstructionOutput 0Output 1Output 2ExpansionExpansion\nOutput 0 Output 1Output 2\nOutput 0: [machine] learning algorithmOutput 1: [machine] learning system designOutput 2: [machine] translation model\nOR\nFigure 2. An overview of SpecInferâ€™s tree-based speculative inference and verification mechanism.\nğ‘¡1,0, and ğ‘¡2,1 are ğ‘¡3,0â€™s ancestors in the token tree. SpecInfer\nincludes a novel tree-based parallel decoding mechanism to\nsimultaneously verify all tokens of a token tree in a single\nLLM decoding step.\nSpecInferâ€™s speculative inference and token tree verifi-\ncation provide two key advantages over the incremental\ndecoding approach of existing LLM inference systems.\nReduced memory accesses to LLM parameters. The per-\nformance of LLM inference is largely limited by accesses to\nGPU memory. In the existing incremental decoding approach,\ngenerating a single token requires accessing all parameters\nof an LLM. The problem is exacerbated for offloading-based\nLLM inference systems, which use limited computational\nresources such as a single commodity GPU to serve LLMs by\nutilizing CPU DRAM and persistent storage to save model\nparameters and loading these parameters to GPUâ€™s high\nbandwidth memory (HBM) for computation. Compared to\nthe incremental decoding approach, SpecInfer significantly\nreduces accesses to LLM parameters whenever the overlap\nbetween a speculated token tree and the LLMâ€™s actual out-\nput is not empty. Reduced accesses to GPU device memory\nand reduced data transfers between GPU and CPU memory\ncan also directly translate to decreased energy consumption,\nsince accessing GPU HBM consumes two or three orders\nof magnitude more energy than floating point arithmetic\noperations.\nReduced end-to-end inference latency. Serving LLMs\nsuffers from long end-to-end inference latency. For exam-\nple, the GPT-3 architecture includes 175 billion parameters\nand requires many seconds to serve a request. In the ex-\nisting incremental decoding approach, the computation for\ngenerating each token depends on the keys and values of\nall previously generated tokens, which introduces sequen-\ntial dependencies between tokens and requires modern LLM\nserving systems to serialize the generation of different tokens\nfor each request. In SpecInfer, LLMs are used as a verifier\nthat takes a speculated token tree as an input and can simul-\ntaneously examine all tokens in the token tree by making a\nsingle verification pass over the LLM. This approach enables\nparallelization across different tokens in a single request and\nreduces the LLMâ€™s end-to-end inference latency.\n3 Learning-based Speculator\nExisting speculative decoding methods perform sequence-\nbased speculation, where an SSM predicts a single sequence\nof tokens to be verified by an LLM. However, a key limitation\nof a single speculated sequence is that the probability of a suc-\ncessful alignment between the LLM and the speculated token\nsequence decays exponentially with the expected alignment\nlength. This can be further exacerbated by the fact that the\nspeculation only includes a single candidate token to verify\nper step, resulting in suboptimal speculative performance.\nOn the other hand, by encouraging more diverse speculated\ncandidates per step, the probability of a successful match per\n4\nAlgorithm 2 The speculation and verification algorithm\nused by SpecInfer. Speculate takes the current token se-\nquence ğ’® as an input and generates a speculated token tree\nğ’©. TreeParallelDecode generates a token ğ’ª(ğ‘¢)for each\nnode ğ‘¢ âˆˆğ’©. VerifyGreedy and VerifyStochastic exam-\nine ğ’© against ğ’ª and produce a sequence of verified tokens\nğ’± using greedy or stochastic sampling, respectively.\n1: Input: A sequence of input tokens â„\n2: Output: A sequence of generated tokens\n3: ğ’® = â„\n4: while true do\n5: ğ’© = Speculate(ğ’®)\n6: ğ’ª = TreeParallelDecode(LLM,ğ’©)\n7: if use greedy decoding then\n8: ğ’± = VerifyGreedy(ğ’ª,ğ’©)\n9: else\n10: ğ’± = VerifyStochastic(ğ’ª,ğ’©)\n11: for ğ‘¡ âˆˆğ’± do\n12: ğ’®.append(ğ‘¡)\n13: if ğ‘¡ = âŸ¨EOSâŸ©then\n14: return ğ’®\n15:\n16: function VerifyGreedy(ğ’ª,ğ’©)\n17: ğ’± = âˆ…, ğ‘¢ â†the root of token tree ğ’©\n18: while âˆƒğ‘£ âˆˆğ’©.ğ‘ğ‘£ = ğ‘¢and ğ‘¡ğ‘£ = ğ’ª(ğ‘¢)do\n19: ğ’±.append(ğ‘¡ğ‘£)\n20: ğ‘¢ = ğ‘£\n21: ğ’±.append(ğ’ª(ğ‘¢))\n22: return ğ’±\n23:\n24: function VerifyStochastic(ğ’ª,ğ’©)\n25: ğ’± = âˆ…, ğ‘¢ â†the root of token tree ğ’©\n26: while ğ‘¢is a non-leaf node do\n27: â„‹ = child(ğ‘¢) âŠ² The set of child nodes for ğ‘¢\n28: while â„‹ is not empty do\n29: ğ‘  âˆ¼rand(â„‹),ğ‘Ÿ âˆ¼ğ‘ˆ(0,1),ğ‘¥ğ‘  = â„‹[ğ‘ ]\n30: if ğ‘Ÿ â‰¤ ğ‘ƒ(ğ‘¥ğ‘  |ğ‘¢,Î˜ğ¿ğ¿ğ‘€)/ğ‘ƒ(ğ‘¥ğ‘  |ğ‘¢,Î˜ğ‘†ğ‘†ğ‘€ğ‘  )\nthen\n31: âŠ² Token ğ‘¥ğ‘  passes verification.\n32: ğ’±.append(ğ‘¥ğ‘ )\n33: ğ‘¢ = ğ‘ \n34: break\n35: else\n36: âŠ² Normalize the residual ğ‘ƒ(ğ‘¥ |ğ‘¢,Î˜ğ¿ğ¿ğ‘€)\n37: ğ‘ƒ(ğ‘¥ | ğ‘¢,Î˜LLM) B norm(max(0,ğ‘ƒ(ğ‘¥ |\nğ‘¢,Î˜LLM)âˆ’ğ‘ƒ(ğ‘¥ |ğ‘¢,Î˜SSMğ‘  )))\n38: â„‹.pop(ğ‘ )\n39: if â„‹ is empty then\n40: break\n41: âŠ² All SSMs fail verification; sample the next token\n42: ğ‘¥next âˆ¼ğ‘ƒ(ğ‘¥ |ğ‘¢,Î˜ğ¿ğ¿ğ‘€)\n43: ğ’±.append(ğ‘¥next)\n44: return ğ’±\nTable 1. The success rate of verifying a token for LLaMA-7B\nusing the top-ğ‘˜ tokens derived from LLaMA-68M. The five\nprompt datasets are described in Section 6.1.\nDataset ğ‘˜ = 1 ğ‘˜ = 2 ğ‘˜ = 3 ğ‘˜ = 4 ğ‘˜ = 5\nGreedy\ndecoding\nAlpaca 68% 77% 81% 84% 85%\nCP 69% 79% 83% 86% 87%\nWebQA 62% 72% 77% 80% 82%\nCIP 70% 81% 85% 88% 89%\nPIQA 63% 75% 79% 83% 85%\nStochastic\ndecoding\nAlpaca 54% 81% 91% 95% 97%\nCP 56% 82% 92% 95% 97%\nWebQA 52% 80% 90% 94% 96%\nCIP 57% 84% 92% 95% 97%\nPIQA 55% 82% 91% 95% 97%\nstep (i.e., the token decoded by the LLM is in this candidate\npool) can be greatly improved. To this end, SpecInfer aims\nto construct a tree of speculated candidates by exploiting\ndiversity within a single SSM and across multiple SSMs. In\nparticular, SpecInferâ€™s learning-based speculator aggregates\nthe predictions of one or multiple SSMs to maximize specula-\ntive performance while maintaining low memory overhead\nand inference latency. SpecInfer uses a token tree to orga-\nnize the tokens produced by the speculator and introduces\ntwo methods for constructing token trees: expansion- and\nmerge-based tree constructions.\nDefinition 3.1 (Token Tree). A token tree ğ’© is a tree struc-\nture, where each node ğ‘¢ âˆˆğ’© is labeled with a token ğ‘¡ğ‘¢, and\nğ‘ğ‘¢ represents ğ‘¢â€™s parent node in the token tree. For each\nnode ğ‘¢, ğ‘†ğ‘¢ represents a sequence of tokens identified by\nconcatenating ğ‘†ğ‘ğ‘¢ and {ğ‘¡ğ‘¢}1.\nExpansion-based token tree construction. One approach\nto creating a token tree involves deriving multiple tokens\nfrom an SSM within a single decoding step. This approach is\nmotivated by an important observation that when an SSM\nmisaligns with an LLM (i.e., the two models select different\ntop-1 tokens), the token selected by the LLM is generally\namong the top-ğ‘˜ tokens from the SSM for very small values\nof ğ‘˜. Table 1 shows the success rate of verifying a token using\nthe top-ğ‘˜ tokens derived from an SSM, where a verification\nis successful if the token selected by the LLM is among the\ntop-ğ‘˜tokens from the SSM. Compared to only using the top-\n1 token from an SSM, using the top-5 tokens can increase\nthe success rate from 70% to 89% for greedy decoding and\nfrom 57% to 97% for stochastic decoding.\nDirectly selecting the top-ğ‘˜ tokens at each step leads to\nan exponential increase in the number of potential token se-\nquences, which substantially elevates inference latency and\n1For the root node ğ‘Ÿ, ğ‘†ğ‘Ÿ represents the token sequence {ğ‘¡ğ‘Ÿ }.\n5\nmemory overhead. Consequently, we adopt a static strategy\nthat expands the token tree following a presetexpansion con-\nfiguration represented as a vector of integers âŸ¨ğ‘˜1,ğ‘˜2,...,ğ‘˜ ğ‘šâŸ©,\nwhere ğ‘šdenotes the maximum number of speculative decod-\ning steps, andğ‘˜ğ‘– indicates the number of tokens to expand for\neach token in the ğ‘–-th step. For example, Figure 3 illustrates\nthe expansion configuration âŸ¨2,2,1âŸ©, leading to four token\nsequences. Our evaluation (see Section 6.4) shows that even\na simple strategy can generate highly accurate speculative\nresults. We acknowledge thatdynamically expanding a token\ntree from an SSM is an opening research problem beyond\nthe scope of this paper, which we leave as future work.\nMerge-based token tree construction. In addition to us-\ning a single SSM, SpecInfer can also combine multiple SSMs\nto jointly predict an LLMâ€™s output. SpecInfer uses an unsu-\npervised method to collectively boost-tune a pool of SSMs\nto align their outputs with that of the LLM by leveraging\nadaptive boosting [12]. SpecInfer uses SSMs to predict the\nnext few tokens that an LLM will generate, and uses gen-\neral text datasets (e.g., the OpenWebText corpus [15] in our\nevaluation) to adaptively align the aggregated output of mul-\ntiple SSMs with the LLM in a fully unsupervised fashion. In\nparticular, SpecInfer converts a text corpus into a collection\nof prompt samples and use the LLM to generate a token se-\nquence for each prompt. SpecInfer first fine-tunes one SSM at\na time to the fullest and marks all prompt samples where the\nSSM and LLM generate identical subsequent tokens. Next,\nSpecInfer filters all marked prompt samples and uses all re-\nmaining samples in the corpus to fine-tune the next SSM to\nthe fullest.\nBy repeating this process for every SSM in the pool, SpecIn-\nfer obtains a diverse set of SSMs whose aggregated output\nlargely overlaps with the LLMâ€™s output on the training cor-\npus. All SSMs have identical inference latency, and therefore\nrunning all SSMs on different GPUs in parallel does not in-\ncrease the latency of speculative inference compared to using\na single SSM. In addition, SpecInfer uses data parallelism to\nserve SSMs across multiple GPUs, and therefore using multi-\nple SSMs does not increase the memory overhead on each\nGPU. In the case where multiple SSMs are employed, the out-\nput of each SSM is considered as a token tree, and SpecInfer\nperforms token tree merge to aggregate all speculated tokens\nin a single tree structure.\nDefinition 3.2 (Token Tree Merge). â„³ is the tree merge\nof ğ‘štoken trees {ğ’©ğ‘–}(1 â‰¤ğ‘– â‰¤ğ‘š) if and only if âˆ€1 â‰¤ğ‘– â‰¤\nğ‘š,âˆ€ğ‘¢ âˆˆğ’©ğ‘–,âˆƒğ‘£ âˆˆâ„³ such that ğ‘†ğ‘£ = ğ‘†ğ‘¢ and vice versa.\nIntuitively, each token tree represents a set of token se-\nquences. Merging multiple token trees produces a new tree\nthat includes all token sequences of the original trees. For\nexample, Figure 3 shows the token tree derived by merging\nfour sequences of tokens. Each token sequence is identified\nby a node in the merged token tree.\n!#\n!$\n!%\n!&!' !(!)machinelearningalgorithmsystemmodelstranslationdesign\nSpeculated Token SequencesExpanded Token Tree (Depth=3)!*system!$#design\nSequence 1:machine learning algorithm isSequence 2:machine learning system designSequence 3:machine translation models areSequence 4:machine translation system design\nStep 0Width=2Step 1Width=2Step 2Width=1!+is!,are\nFigure 3. Illustration of token tree expansion.\nNote that, in addition to boosting, there are several other\nensemble learning methods (e.g., voting, bagging, and stack-\ning) [14] that can be used to combine the outputs from mul-\ntiple SSMs, and we leave the exploration as future work.\n4 Token Tree Verifier\nThis section introduces SpecInferâ€™s token tree verifier , which\ntakes as input a token tree generated by the speculator and\nverifies the correctness of its tokens against an LLMâ€™s output.\nA key idea behind the design of SpecInfer is simultaneously\nverifying all sequences of a token tree against the original\nLLMâ€™s output by making asingle pass over the LLMâ€™s parame-\nters. This functionality allows SpecInfer to opportunistically\ndecode multiple tokens (instead of a single token in incre-\nmental decoding), resulting in reduced memory accesses to\nthe LLMâ€™s parameters. A challenge SpecInfer must address\nin token tree verification is efficiently computing the atten-\ntion scores for all sequences of a token tree. To this end,\nwe introduce tree attention , which generalizes the attention\nmechanism [48] from sequence to tree structure. In addition,\nwe develop a tree-based parallel decoding mechanism that\ncan decode all tokens in a token tree in parallel.\nÂ§4.1 and Â§4.2 describe tree attention and tree-based parallel\ndecoding. Â§4.3 introduces the mechanism to verify a token\ntree against the LLMâ€™s output.\n4.1 Tree Attention\nTransformer-based language models use the attention mech-\nanism to reason about sequential information [ 48]. LLMs\ngenerally use decoder-only, multi-head self-attention, which\ntakes a single input tensor ğ‘‹ and computes an output tensor\nğ‘‚ via scaled multiplicative formulations as follows.\nğ‘„ğ‘– = ğ‘‹ Ã—ğ‘Šğ‘„\nğ‘– , ğ¾ ğ‘– = ğ‘‹ Ã—ğ‘Šğ¾\nğ‘– , (1)\nğ‘‰ğ‘– = ğ‘‹ Ã—ğ‘Šğ‘‰\nğ‘– , ğ´ ğ‘– = (ğ‘„ğ‘– Ã—ğ¾ğ‘‡\nğ‘– )âˆš\nğ‘‘ , (2)\nğ»ğ‘– = softmax\u0000mask(ğ´ğ‘–)\u0001ğ‘‰ğ‘–, ğ‘‚ = (ğ»1,...,ğ» â„)ğ‘Šğ‘‚ (3)\nwhere ğ‘„ğ‘–, ğ¾ğ‘–, and ğ‘‰ğ‘– denote the query, key, and value tensors\nof the ğ‘–-th attention head (1 â‰¤ğ‘– â‰¤â„), ğ‘Šğ‘„\nğ‘– , ğ‘Šğ¾\nğ‘– , and ğ‘Šğ‘‰\nğ‘– are\nthe corresponding weight matrices. ğ´ğ‘– is an ğ‘™Ã—ğ‘™ matrix that\nrepresents the attention scores between different tokens in\nthe input sequence, where ğ‘™ is the sequence length. To pre-\nserve causality when generating tokens (i.e., a token in the\n6\nsequence should not affect the hidden states of any preceding\ntokens), the following causal mask function is applied:\nmask(ğ´)ğ‘—ğ‘˜ =\n(\nğ´ğ‘—ğ‘˜ ğ‘— â‰¥ğ‘˜\nâˆ’âˆ ğ‘— < ğ‘˜ . (4)\nIntuitively, when computing the attention output of the ğ‘—-th\ntoken in the sequence, all subsequent tokens should have\nan attention score of âˆ’âˆto indicate that the subsequent\ntokens will not affect the attention output of the ğ‘—-th token2.\nIn Equation 3, ğ»ğ‘– represents the output of the ğ‘–-th attention\nhead, andğ‘Šğ‘‚ is a weight matrix used for computing the final\noutput of the attention layer.\nNote that the attention mechanism described above ap-\nplies only to a sequence of tokens. We generalize the atten-\ntion mechanism to arbitrary tree structures.\nDefinition 4.1 (Tree Attention). For a token tree ğ’© and an\narbitrary node ğ‘¢ âˆˆğ’©, its tree attention is defined as the out-\nput of computing the original Transformer-based sequence\nattention on ğ‘†ğ‘¢ (i.e., the token sequence represented by ğ‘¢):\nTreeAttention(ğ‘¢)= Attention(ğ‘†ğ‘¢)âˆ€ğ‘¢ âˆˆğ’© (5)\nFor a given set of token sequences, since each sequence ğ‘†\nis covered by a node of the merged token tree, performing\ntree attention on the token tree allows SpecInfer to obtain\nthe attention output for all token sequences.\n4.2 Tree-based Parallel Decoding\nThis section describes SpecInferâ€™stree-based parallel decoding\nmechanism for computing tree attention forall tokens in a to-\nken tree in parallel . A key challenge SpecInfer must address\nin computing tree attention is managing key-value cache .\nIn particular, the attention mechanism of Transformer [48]\nrequires accessing the keys and values of all preceding to-\nkens to compute the attention output of each new token,\nas shown in Equation 3. To avoid recomputing these keys\nand values, todayâ€™s LLM inference systems generally cache\nthe keys and values of all tokens for reuse in future iter-\nations, since the causal relation guarantees that a tokenâ€™s\nkey and value remain unchanged in subsequent iterations\n(i.e., mask(A)jk = âˆ’âˆfor any ğ‘— < ğ‘˜). However, when com-\nputing tree attention, different sequences in a token tree\nmay include conflicting key-value caches. For example, for\nthe speculated token tree in Figure 4, two token sequences\n(ğ‘¡2,ğ‘¡3,ğ‘¡4,ğ‘¡5)and (ğ‘¡2,ğ‘¡3,ğ‘¡8,ğ‘¡9)have different keys and values\nfor the third and fourth positions.\nA straightforward approach to supporting key-value cache\nis employing the sequence-based decoding of existing LLM\ninference systems and using a different key-value cache for\neach sequence of a token tree, as shown on the left of Figure 4.\nHowever, this approach is computationally very expensive\n2Note that we use âˆ’âˆ(instead of 0) to guarantee that the softmaxâ€™s output\nis 0 for these positions.\nand involves redundant computation, since two token se-\nquences sharing a common prefix have the same attention\noutputs for the common prefix due to the causal mask in\nEquation 3. In addition, launching one kernel for each token\nsequence introduces additional kernel launch overhead.\nSpecInfer introduces two key techniques to realize tree-\nbased parallel decoding.\nDepth-first search to update key-value cache. Instead\nof caching the keys and values for individual token sequences\nof a token tree, SpecInfer reuses the same key-value cache\nacross all token sequences by leveraging a depth-first search\nmechanism to traverse the token tree, as shown in Figure 4,\nwhere SpecInfer visits ğ‘¡2,ğ‘¡3,...,ğ‘¡ 9 by following a depth-first\norder to traverse the token tree and update the shared key-\nvalue cache. This approach allows SpecInfer to maintain\nthe correct keys and values for all preceding tokens when\ncomputing the attention output of a new token.\nTopology-aware causal mask. A straightforward ap-\nproach to computing tree attention is calculating the tree\nattention output for individual tokens by following the depth-\nfirst order described earlier. However, this approach would\nresult in high GPU kernel launch overhead since each kernel\nonly computes tree attention for one token sequence. In ad-\ndition, executing these kernels in parallel requires additional\nGPU memory to store their key-value caches separately due\nto cache conflict. A key challenge that prevents SpecInfer\nfrom batching multiple tokens is that the attention computa-\ntion for different tokens requires different key-value caches\nand therefore cannot be processed in parallel.\nWe introduce topology-aware casual mask to fuse tree at-\ntention computation of all tokens in a single kernel. To batch\nattention computation, SpecInfer uses a tree topology in-\nstead of the original sequence topology to store the keys and\nvalues of all tokens in a token tree in the key-value cache. For\nexample, to compute tree attention for the speculated token\ntree shown in Figure 4, SpecInfer takes both verified tokens\n(i.e., ğ‘¡2) and all speculated tokens (i.e., ğ‘¡3,ğ‘¡4,...,ğ‘¡ 9) as inputs.\nThis approach allows SpecInfer to fuse the attention compu-\ntation into a single kernel but also results in attention scores\nthat violate the causal dependency (e.g., ğ‘¡7â€™s attention com-\nputation uses all previous tokens, including ğ‘¡5 which is not\nin ğ‘¡7â€™s token sequence). To fix the attention scores for these\npairs, SpecInfer updates the causal mask based on the token\ntreeâ€™s topology. This approach computes the exact same at-\ntention output as incremental decoding, while resulting in\nmuch fewer kernel launches compared to sequence-based\ndecoding.\n4.3 Token Verification\nFor a given speculated token tree ğ’©, SpecInfer uses tree-\nbased parallel decoding (see Section 4.2) to compute its tree\nattention and generate an output tensor ğ’ª that includes a\n7\nKernel 1:  ğ‘¡!â†’ğ‘¡\"â†’ğ‘¡#â†’ğ‘¡$\nğ‘¡!ğ‘¡\"ğ‘¡#ğ‘¡$ğ‘¡%ğ‘¡&ğ‘¡'ğ‘¡(SpeculatedToken Tree Verified tokensğ‘¡) Speculated tokensâ€™ KV-cacheAvailable KV-cache slotsSequence-based Parallel Decoding\nt5t4t3t2 âœ“âœ“âœ“âœ“t2 âœ“âœ“âœ“t3 âœ“âœ“t4 âœ“t5\nt9t8t7t6t5t4t3t2 âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“t2 âœ“âœ“âœ“âœ“âœ“âœ“âœ“t3 âœ“âœ“âœ“âœ“t4 âœ“t5 âœ“âœ“t6 âœ“t7 âœ“âœ“t8 âœ“t9\nâ€¦t5t4t3t2KV-cache\nCausal Mask\nKernel 3:  ğ‘¡!â†’ğ‘¡\"â†’ğ‘¡#â†’ğ‘¡%â†’ğ‘¡&\nt7t6t4t3t2 âœ“âœ“âœ“âœ“âœ“t2 âœ“âœ“âœ“âœ“t3 âœ“âœ“âœ“t4 âœ“âœ“t6 âœ“t7\nâ€¦t7t6t4t3t2Kernel 2:  ğ‘¡!â†’ğ‘¡\"â†’ğ‘¡'â†’ğ‘¡(\nt9t8t3t2 âœ“âœ“âœ“âœ“t2 âœ“âœ“âœ“t3 âœ“âœ“t8 âœ“t8\nâ€¦t9t8t3t2\nTree-based Parallel Decodingâ€¦t9t8t7t6t5t4t3t2Kernel 1: ğ‘¡!,ğ‘¡\",ğ‘¡#,ğ‘¡$,ğ‘¡%,ğ‘¡&,ğ‘¡',ğ‘¡(KV-cache\nTopology-Aware Causal Mask\nVerified tokensâ€™ KV-cache\nFigure 4. Comparing SpecInferâ€™s tree-based parallel decoding with existing sequence-based decoding.\nStochastic Decoding\nWith probability :min(1, P(ui âˆ£ U,Î˜LLM)\nP(ui âˆ£ U,Î˜SSM3))\nWith probability :min(1, P(ui âˆ£ U,Î˜LLM)\nP(ui âˆ£ U,Î˜SSM2))\nSSM 1 LLM\nP(ui âˆ£ U, Î˜SSM1)\nFailed\nNormalized residual \ndistribution\nSSM 2\nVerification\nWith probability :min(1, P(ui âˆ£ U,Î˜LLM)\nP(ui âˆ£ U,Î˜SSM1)) Verified\nVerified\nVerified\nelse\nSSM 3\nVerification\nVerification\nelse\nelse\nP(ui âˆ£ U, Î˜SSM2)\nP(ui âˆ£ U, Î˜SSM3)\nP(ui âˆ£ U, Î˜LLM)\nNormalized residual \ndistribution\nFigure 5. Illustrating the multi-step speculative sampling\nmechanism for verifying LLMs with stochastic sampling.\ntoken for each node ğ‘¢ âˆˆğ’©. Next, SpecInferâ€™s token tree veri-\nfier examines the correctness of speculated tokens against\nthe LLM. SpecInfer supports both greedy and stochastic sam-\npling as shown in Algorithm 2.\nGreedy decoding. Many LLM applications generate to-\nkens using greedy decoding , which greedily selects the to-\nken with the highest likelihood in each decoding step. The\nVerifyGreedy function in Algorithm 2 shows how SpecIn-\nfer verifies a speculated token tree ğ’© with greedy decoding.\nSpecInfer starts from the root of ğ’© and iteratively exam-\nines a nodeâ€™s speculated results against the LLMâ€™s original\noutput. For a node ğ‘¢ âˆˆ ğ’©, SpecInfer successfully specu-\nlates its next token if ğ‘¢includes a child node ğ‘£ (i.e., ğ‘ğ‘£ = ğ‘¢)\nwhose token matches the LLMâ€™s output (i.e., ğ‘¡ğ‘£ = ğ’ª(ğ‘¢)). In\nthis case, SpecInfer finishes its verification for node ğ‘¢ and\nmoves on to examine its child ğ‘£. When the node ğ‘¢does not\ninclude a child that contains the LLMâ€™s output, SpecInfer\nadds ğ’ª(ğ‘¢)as a verified node in ğ’© and terminates the verifi-\ncation process. Finally, all verified nodes are appended to the\ncurrent generated token sequence ğ’±. Token tree verification\nallows SpecInfer to opportunistically decode multiple tokens\n(instead of a single token in the incremental decoding ap-\nproach), while preserving the same generative performance\nas incremental decoding.\nStochastic decoding. To improve the diversity of gener-\nated tokens, many LLM applications perform stochastic de-\ncoding, which samples a token from a probability distribution\nğ‘ƒ(ğ‘¢ğ‘–|ğ‘¢0,...,ğ‘¢ ğ‘–âˆ’1; Î˜ğ¿ğ¿ğ‘€), where ğ‘ˆ = ğ‘¢0,...,ğ‘¢ ğ‘–âˆ’1 are previously\ngenerated tokens, ğ‘¢ğ‘– is the next token to generate, and Î˜ğ¿ğ¿ğ‘€\nrepresents a parameterized LLM.\nTo verify a speculated token tree with stochastic decod-\ning, we introduce a multi-step speculative sampling (MSS)\nalgorithm to conduct verification, whose pseudocode code\nis shown in the VerifyStochastic function in Algorithm 2\nand illustrated in Figure 5. Our method provably preserves\nan LLMâ€™s generative performance as incremental decoding\nwhile optimizing the number of speculated tokens that can\nbe verified. Theorem 4.2 proves its correctness.\nTheorem 4.2. For a given ğ¿ğ¿ğ‘€andğ‘šSSMs (i.e., ğ‘†ğ‘†ğ‘€1,...,ğ‘†ğ‘†ğ‘€ğ‘š,\nlet ğ‘ƒ(ğ‘¢ğ‘–|ğ‘ˆ; Î˜ğ¿ğ¿ğ‘€)be the probability distribution of sampling\n8\na token using stochastic decoding, where ğ‘ˆ = ğ‘¢0,...,ğ‘¢ ğ‘–âˆ’1 are\npreviously generated tokens, ğ‘¢ğ‘– is the next token to generate,\nÎ˜ğ¿ğ¿ğ‘€ represents the parameterized LLM.\nLet ğ‘ƒSpecInfer (ğ‘¢ğ‘–|ğ‘ˆ; Î˜ğ¿ğ¿ğ‘€,{Î˜ğ‘†ğ‘†ğ‘€ğ‘— })be the probability dis-\ntribution of sampling token ğ‘¢ğ‘– using SpecInferâ€™s multi-step\nspeculative sampling (see the VerifyStochastic function in\nAlgorithm 2), where Î˜ğ‘†ğ‘†ğ‘€ğ‘— is the ğ‘—-th parameterized SSM.\nThen âˆ€ğ‘ˆ,ğ‘¢ğ‘–,Î˜ğ¿ğ¿ğ‘€,Î˜ğ‘†ğ‘†ğ‘€ğ‘— we have\nğ‘ƒ(ğ‘¢ğ‘– |ğ‘ˆ; Î˜ğ¿ğ¿ğ‘€)= ğ‘ƒSpecInfer (ğ‘¢ğ‘– |ğ‘ˆ; Î˜ğ¿ğ¿ğ‘€,{Î˜ğ‘†ğ‘†ğ‘€ğ‘— }) (6)\nA proof of this theorem is presented in [28].\nWe acknowledge that a more straightforward approach to\npreserving the probability distribution of stochastic decoding\nis directly sampling the next token ğ‘¥ âˆ¼ğ‘ƒ(ğ‘¢ğ‘– |ğ‘ˆ; Î˜ğ¿ğ¿ğ‘€)and\nexamining whetherğ‘¥is a child node ofğ‘¢ğ‘–âˆ’1 in the speculated\ntoken tree. We call this approach naive sampling (NS) and\nshow that SpecInferâ€™s multi-step speculative sampling has a\nuniformly lower rejection probability than naive sampling.\nTheorem 4.3. Let ğ‘ƒ\n\u0010\nreject |MSS,ğ‘ˆ, Î˜LLM,{Î˜SSMğ‘— }\n\u0011\ndenote\nthe probability of rejecting speculation following multi-step\nspeculative sampling with abbreviation ğ‘ƒ(reject |MSS), and\nğ‘ƒ\n\u0010\nreject |NS,ğ‘ˆ, Î˜LLM,{Î˜SSMğ‘— }\n\u0011\nthe probability of rejecting\nspeculation following Naive Sampling (NS) with abbreviation\nğ‘ƒ(reject |NS). Then âˆ€ğ‘ˆ,Î˜LLM,{Î˜SSMğ‘— }, we have\nğ‘ƒ(reject |MSS)â‰¤ ğ‘ƒ(reject |NS)\nWe present a proof of Theorem 4.3 in [28].\nNote that prior work has introduced single-step specula-\ntive sampling for sequence-based speculative inference [5,\n25]. Different from these approaches, SpecInfer leverages\ntoken trees for improving speculative performance, which\nrequires a different verification algorithm. As a result, SpecIn-\nfer performs multi-step verification (see VerifyStochastic\nin Algorithm 2) across all branches of a token to maximize\nthe success rate while preserving equivalence as incremental\ndecoding. The proposed MSS algorithm not only works for\nmerge-based method with multiple SSMs, but also supports\nexpansion-based method with one SSM and top-ğ‘˜sampling.\n5 System Design and Implementation\nThis section describes the design and implementation of\nSpecInferâ€™s distributed runtime system (Â§5.1 and Â§5.2), ana-\nlyzes the computation and memory overheads of speculation\nand verification (Â§5.3), and introduces potential LLM appli-\ncations that can benefit from SpecInferâ€™s techniques (Â§5.4).\n5.1 SpecInferâ€™s Runtime Design\nFigure 6 shows the workflow for one iteration of speculative\ninference and verification. SpecInferâ€™s request manager re-\nceives LLM serving requests and schedules these requests for\nserving by adapting theiteration-level scheduling policy from\nOrca [55]. Specifically, SpecInfer iteratively selects requests\nfrom a pool of pending requests and performs one iteration\nLLMTree-based Parallel Decoding\nSSM1\nSSM1\nSSM2\nSSM2\nRequest ManagerToken Tree MergeSSM-generated Tokens\nToken Tree Verification\nSpeculative Token Trees\nLLM-generated Tokens\nRequest Scheduling CPU\nGPU1\nGPU2\nGPU3\nGPU4\nr1, r2\nr3, r4\nr1, r2\nr3, r4\nDistributing Requests\nFigure 6. SpecInferâ€™s workflow for one iteration of specu-\nlative inference and verification. SpecInfer uses data paral-\nlelism to serve SSMs, and combine tensor model parallelism\nand pipeline model parallelism for serving an LLM.\nof speculative inference and token tree verification for the se-\nlected requests. Since SSMs are small and can fit in one GPU,\nSpecInfer equally distributes GPUs across SSMs and serves\nthese SSMs using data parallelism. For example, Figure 6\nshows how SpecInfer serves two SSMs and four requests\n(i.e., ğ‘Ÿ1, ğ‘Ÿ2, ğ‘Ÿ3, and ğ‘Ÿ4) on four GPUs. The SSM-generated to-\nkens are sent back to the request manager, which produces a\nspeculated token tree for each request using the tree merge\nalgorithm introduced in Â§4.\nSpecInfer serves an LLM using the hybrid parallelization\nstrategy introduced in Megatron-LM [41], which uses tensor\nmodel parallelism for parallelizing each Transformer layer\nacross GPUs within a node, and uses pipeline model paral-\nlelism for partitioning Transformer layers across nodes. All\nGPUs perform the tree-based parallel decoding (see Â§4.2) to\ncompute tree attention scores and send the LLM-generated\ntokens back to the request manager, which finally verifies\nthe speculated tokens against the LLMâ€™s output (see Â§4.3).\nNote that the overhead introduced by the request manager\n(i.e., request scheduling, token tree merge, and verification) is\nnegligible compared to the execution time of LLM inference.\nIn addition, SpecInferâ€™s request manager and GPU workers\nonly communicate tokens and do not transfer the vector\nrepresentations of these tokens, which again introduces neg-\nligible communication overheads.\nContinuous batching. SpecInfer uses continuous batch-\ning introduced in Orca [55] to serve multiple LLM inference\nrequests in parallel. Specifically, SpecInfer schedules LLM\nexecution at the granularity of iterations instead of requests.\nAfter each LLM decoding iteration, SpecInfer checks each re-\nquestâ€™s status and sends the generated results of all finished\nrequests to the client. This design also allows SpecInfer to\nstart processing newly arrived requests without waiting for\nall current requests to complete.\n9\n5.2 SpecInferâ€™s Implementation\nSpecInfer was implemented on top of FlexFlow [21, 47], a dis-\ntributed multi-GPU runtime for DNN computation. FlexFlow\nexposes an API that allows users to define a DNN model in\nterms of its layers. It is compatible with PyTorchâ€™s model\ndefinition due to the alignment of underlying operators. For\nexample, the open-source LLMs from HuggingFace [19] can\nbe directly imported into SpecInfer for serving without mod-\nification. Users can also provide a parallelization strategy,\nspecifying the degree of data, model, and pipeline parallelism\nfor each layer. A DNN is represented as a computational\ngraph where each node is a region of memory, and each\nedge is an operation on one or more regions. Operations\ncan be represented using three levels of abstraction: lay-\ners, operators, and tasks. The FlexFlow compiler transforms\nthe computational graph from the highest abstractions (i.e.,\nlayers) to the lowest (i.e., tasks). Tasks are also the unit of\nparallelization; they are non-preemptible, and are executed\nasynchronously.\nCUDA kernel optimizations. Directly launching cuBLAS\nand cuDNN kernels for calculating attention results in high\nkernel launch overhead and does not leverage the shared\nmemory available on modern GPUs. To address this ineffi-\nciency, SpecInfer uses a customized kernel built on top of\nFasterTransformer [32] for computing attention. Within this\nkernel, each thread block computes a single head for a single\nrequest. The process begins with loading the query tensor\ninto GPU shared memory accessible by all threads within a\nthread block. Each thread then performs a segment of the\nquery/key product and broadcasts the result to other threads\nfor computing the max query/key product and exponential\nsum. To support tree-based parallel decoding, SpecInfer com-\nputes all tokens within a tree in parallel and leverages the\ntopology-aware causal mask to preserve casuality.\n5.3 Overhead of Speculation and Verification\nSpecInfer accelerates generative LLM inference at the cost of\nadditional memory and computation overheads. This section\nanalyzes these overheads and shows that they are generally\none or two orders of magnitude smaller than the memory\nand computation cost of executing LLM inference.\nMemory overhead. The memory overhead of SpecInferâ€™s\nspeculation-verification approach comes from two aspects.\nFirst, in addition to serving an LLM, SpecInfer also needs to\nallocate memory for saving the parameters of one or mul-\ntiple SSMs, which collectively speculate the LLMâ€™s output.\nOur evaluation shows that SpecInfer can achieve signifi-\ncant performance improvement by using SSMs 100-1000Ã—\nsmaller than the LLM. As a result, hosting each SSM increases\nthe overall memory requirement by less than 1%. A second\nsource of memory overhead comes from the token tree veri-\nfication engine, which verifies an entire token tree instead\nof decoding a single token. Therefore, additional memory\nis needed for caching the keys and values, and storing the\nattention scores for all tokens. Due to the necessity for sup-\nporting very long sequence length in todayâ€™s LLM serving,\nwe observe that the memory overhead associated with token\ntree is negligible compared to key-value cache.\nComputation overhead. Similarly, the computation over-\nhead introduced by speculation and verification also comes\nfrom two aspects. First, SpecInfer needs to run SSMs in the\nincremental-decoding mode to generate candidate tokens.\nWhen multiple SSMs are employed, SpecInfer processes these\nSSMs in parallel across GPUs to minimize speculation la-\ntency. Second, SpecInfer verifies a token tree by comput-\ning the attention outputs for an entire token tree, most of\nwhich do not match the LLMâ€™s output and therefore are un-\nnecessary in the incremental-decoding inference. However,\nthe key-value cache mechanism of existing LLM inference\nsystems prevents them from serving a large number of re-\nquests in parallel, resulting in under-utilized computation\nresources on GPUs when serving LLMs in incremental de-\ncoding. SpecInferâ€™s token tree verification leverages these\nunder-utilized resources and therefore introduces negligible\nruntime overhead compared to incremental decoding.\n5.4 Applications\nOur speculative inference and token tree verification tech-\nniques can be directly applied to a variety of LLM applica-\ntions. We identify two practical scenarios where LLM infer-\nence can significantly benefit from our techniques.\nDistributed LLM inference. The memory requirements\nof modern LLMs exceed the capacity of a single compute\nnode with one or multiple GPUs, and the current approach\nto addressing the high memory requirement is distributing\nthe LLMâ€™s parameters across multiple GPUs [ 29]. For ex-\nample, serving a single inference pipeline for GPT-3 with\n175 billion parameters requires more than 16 NVIDIA A100-\n40GB GPUs to store the model parameters in single-precision\nfloating points. Distributed LLM inference is largely limited\nby the latency to transfer intermediate activations between\nGPUs for each LLM decoding step. While SpecInferâ€™s ap-\nproach does not directly reduce the amount of inter-GPU\ncommunications, its verification mechanism can increase\nthe communication granularity and reduce the number of\ndecoding steps.\nOffloading-based LLM inference. Another practical sce-\nnario that can benefit from SpecInferâ€™s techniques is offloading-\nbased LLM inference, which leverages CPU DRAM to store\nan LLMâ€™s parameters and loads a subset of these parame-\nters to GPUs for computation in a pipeline fashion [40]. By\nopportunistically verifying multiple tokens, SpecInfer can\nreduce the number of LLM decoding steps and the overall\ncommunication between CPU DRAM and GPU HBM.\n10\n6 Evaluation\n6.1 Experimental Setup\nLLMs. To compare the runtime performance of SpecInfer\nwith existing LLM serving systems, we evaluate these sys-\ntems using two publicly available LLM families: OPT [57] and\nLLaMA [46]. More specifically, we select LLaMA-7B, OPT-\n13B, OPT-30B, and LLaMA-65B as the LLMs, and LLaMA-\n68M and OPT-125M as the SSMs. The pre-trained model\nparameters for the LLMs and SSMs were obtained from their\nHuggingFace repositories [19], and we describe how SpecIn-\nfer collectively boost-tunes multiple SSMs in [28].\nDatasets. We evaluate SpecInfer on five datasets: Chatbot\nInstruction Prompts (CIP) [34], ChatGPT Prompts (CP) [30],\nWebQA [1], Alpaca [36, 45], and PIQA [2]. We only use the\nprompts/questions from these datasets to form our input\nprompts to simulate real-world conversation traces.\nPlatform. The experiments were conducted on two AWS\ng5.12xlargeinstances, each of which is equipped with four\nNVIDIA A10 24GB GPUs, 48 CPU cores, and 192 GB DRAM.\nNodes are connected by 100 Gbps Ethernet.\nOur experiments use the expansion-based method (see\nSection 3) for constructing token trees and use the expan-\nsion configuration âŸ¨1,1,3,1,1,1,1,1âŸ©, which provides good\nresults for our benchmarks. We analyze the impact of ex-\npansion configurations in Â§6.4, evaluate tree-based parallel\ndecoding and multi-step speculative sampling in Â§6.5 and\nÂ§6.6, and finally compares the expansion- and merge-based\ntree construction methods in [28].\n6.2 Distributed LLM Inference\nWe compare the end-to-end distributed LLM inference perfor-\nmance among SpecInfer, vLLM [24], HuggingFace Text Gen-\neration Inference (TGI) [18], and FasterTransformer [32] on\nLLaMA-7B, OPT-30B, and LLaMA-65B. For LLaMA-7B and\nOPT-30B, all systems serve the two LLMs in half-precision\nfloating points across one and four A10 GPUs using tensor\nmodel parallelism. LLaMA-65B do not fit on four GPUs on a\nsingle node, therefore both FasterTransformer and SpecInfer\nserve it on eight A10 GPUs on two nodes by combining ten-\nsor model parallelism within each node and pipeline model\nparallelism across nodes. vLLM and HuggingFace TGI do\nnot support pipeline model parallelism and cannot serve an\nLLM on multiple nodes.\nTo rule out potential effects of our system implementation,\nwe also evaluate SpecInfer with two additional configura-\ntions. First, SpecInfer withincremental decoding evaluates the\nruntime performance of our implementation when the spec-\nulator generates empty token trees, and the verifier verifies\nexactly one token in each decoding step. Second, SpecInfer\nwith sequence-based speculative inference serves as a refer-\nence for existing speculative inference system and is enabled\nby using a single pre-trained SSM and sequence-based de-\ncoding.\nWe use prompts from the five datasets described in Â§6.1.\nFor each prompt, we let all systems generate up to 128 new\ntokens and report the average per-token latency in Figure 7.\nNote that SpecInfer may generate more than 128 new tokens\nsince the verifier can verify multiple tokens in each iteration.\nIn this case, we truncate SpecInferâ€™s output to 128 tokens.\nSpecInfer with incremental decoding achieves on-par perfor-\nmance as existing systems. This is because all systems use\nthe same strategies to parallelize LLM inference across GPUs\nand use the same kernel libraries (i.e., cuDNN, cuBLAS, and\ncuTLASS) to execute inference computation on GPUs. With\ntree-based speculative inference and verification, SpecInfer\noutperforms incremental decoding systems by 1.5-2.5Ã—for\nsingle-node, multi-GPU inference and by 2.4-2.8Ã—for multi-\nnode, multi-GPU inference, while generating the exact same\nsequence of tokens as incremental decoding for all prompts.\nThe speedup comes from leveraging spare GPU resources to\nperform tree-based parallel decoding while maintaining the\nsame per-iteration latency as incremental decoding.\nCompared to sequence-based speculative inference, SpecIn-\nferâ€™s tree-based approach further reduces LLM serving la-\ntency by 1.2-1.5Ã—. The improvement is achieved by (1) lever-\naging token trees to optimize speculative performance, (2)\nusing tree-based parallel decoding to verify an entire token\ntree in parallel, and (3) performing multi-step speculative\nsampling to improve verification performance. We further\nevaluates these aspects in Â§6.4, Â§6.5, and Â§6.6.\nNote that SpecInferâ€™s performance improvement over ex-\nisting systems reduces as the batch size (i.e., number of\nconcurrent requests) increases. This is because SpecInfer\nleverages spare GPU resources to perform tree-based par-\nallel decoding while maintaining the same per-iteration la-\ntency as incremental decoding. A larger batch size introduces\nmore parallelizable computation for incremental decoding,\nand thus less spare GPU resources that can be leveraged by\nSpecInfer. On the flip side, larger batch sizes also increase\nthe end-to-end latency of each request, as shown in Figure 7.\nOverall, SpecInfer is most beneficial for low-latency LLM\ninference.\n6.3 Offloading-based LLM Inference\nAnother important application of SpecInfer is offloading-\nbased LLM inference, where the system offloads an LLMâ€™s\nparameters to CPU DRAM and loads a subset of these param-\neters to GPUs for inference computation in a pipeline fashion.\nWe compare the end-to-end offloading-based LLM inference\nperformance between SpecInfer and FlexGen [ 39] using a\nsingle 24GB A10 GPU and two LLMs (i.e., OPT-13B and OPT-\n30B), both of which exceed the memory capacity of an A10\nGPU and requires offloading for serving. Both SpecInfer and\nFlexGen retain all model parameters on CPU DRAM. During\ncomputation, the demand weights are loaded from the CPU\n11\nBS=1 BS=2 BS=4 BS=8 BS=16\nLLaMA-7B\n(1 GPU/node, 1 node)\n0\n5\n10\n15\n20\n25\n30\n35\n40\nBS=1 BS=2 BS=4 BS=8 BS=16\nOPT-30B\n(4 GPUs/node, 1 node)\n0\n10\n20\n30\n40\n50\n60\nBS=1 BS=2 BS=4 BS=8 BS=16\nLLaMA-65B\n(4 GPUs/node, 2 nodes)\n0\n20\n40\n60\n80\n100\n120\nPer-token latency (ms)\nvLLM\nHuggingFace TGI\nFasterTransformer\nSpecInfer w/ Incremental Decoding\nSpecInfer w/ Sequence-based Speculative Inference\nSpecInfer w/ Tree-based Speculative Inference\nFigure 7. Comparing the end-to-end inference latency of SpecInfer with existing systems. Numbers in parenthesis show the\nnumber of GPUs and compute node used to serve each LLM. All systems parallelize LLM inference by combining tensor model\nparallelism (within a node) and pipeline parallelism (across nodes).\nBS=1 BS=2 BS=4 BS=8 BS=16\nOPT-13B\n0.0\n0.5\n1.0\n1.5\n2.0\n3.3x 3.3x 3.1x\n2.7x 2.6x\nBS=1 BS=2 BS=4 BS=8 BS=16\nOPT-30B\n0\n1\n2\n3\n4\n3.5x 3.3x 3.0x 3.0x 2.7x\nPer-token latency (seconds)\nFlexGen SpecInfer\nFigure 8. Comparing the end-to-end offloading-based infer-\nence latency of FlexGen and SpecInfer. Both FlexGen and\nSpecInfer perform model offloading to serve OPT-13B and\nOPT-30B models on a single 24GB A10 GPU.\nto the GPU. Figure 8 shows the results. Compared to FlexGen,\nSpecInfer reduces the per-token latency by 2.6-3.5Ã—. Since\noffloading-based LLM inference is mostly bottlenecked by\nthe communication between CPU DRAM and GPU HBM for\nloading an LLMâ€™s parameters, SpecInferâ€™s improvement over\nexisting systems is achieved by opportunistically verifying\nmultiple tokens, which in turn reduces the number of LLM\ndecoding steps and data transfers between CPU and GPU.\n6.4 Token Tree Construction\nThis section evaluates the expansion-based token tree con-\nstruction mechanism. We first study how token tree width\naffects SpecInferâ€™s speculative performance. In this experi-\nment, we use LLaMA-7B and LLaMA-68M as the LLM and\nSSM, and use the expansion configuration âŸ¨1,1,ğ‘˜, 1,1,1,1,1âŸ©\n(i.e., expanding at the third token), where ğ‘˜ is the token tree\nwidth. Figure 9 shows the cumulative distribution function\n(CDF) of the average number of verified tokens per decoding\n0.0 0.2 0.4 0.6 0.8 1.0\nCDF \n Greedy decoding\n2\n4\n6\n8\nAverage # verified tokens \n per decoding step\nTree width = 1\nTree width = 2\nTree width = 3\nTree width = 4\nTree width = 5\n0.0 0.2 0.4 0.6 0.8 1.0\nCDF \n Stochastic decoding\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nFigure 9. Comparing speculative performance of SpecInfer\nwith different token tree structures.\nBS=1 BS=2 BS=4 BS=8 BS=16\n15\n20\n25\n30\n35\n40Per-token latency (ms)\nTree width = 1\nTree width = 2\nTree width = 3\nTree width = 4\nTree width = 5\nFigure 10. SpecInferâ€™s end-to-end inference latency with\ndifferent tree widths. We use LLaMA-7B and LLaMA-68M\nas the LLM and SSM.\nstep for all prompts in the Alpaca dataset [45]. Compared to\nsequence-based speculation (i.e., tree width = 1), leveraging\ntoken trees can reduce LLM decoding steps by 1.2-1.5Ã—for\ngreedy decoding and by 1.3-1.4Ã—for stochastic decoding.\nA larger token tree width reduces the LLM decoding steps\nto process a request at the cost of increased verification\n12\nTable 2. Average number of tokens verified by SpecInfer in\na decoding step. We use LLaMA-7B and LLaMA-68M as the\nLLM and SSM, and use different tree widths for constructing\na token tree. The speculation length is 8.\nToken tree width\nDataset 1 2 3 4 5\nGreedy\ndecoding\nAlpaca 2.95 3.07 3.21 3.33 3.43\nCP 2.58 3.24 3.46 3.59 3.69\nWebQA 2.27 2.69 2.86 2.98 3.07\nCIP 2.73 3.40 3.62 3.79 3.91\nPIQA 2.18 2.80 2.97 3.10 3.21\nStochastic\ndecoding\nAlpaca 1.79 2.11 2.26 2.32 2.38\nCP 1.69 1.99 2.15 2.23 2.28\nWebQA 1.64 1.93 2.08 2.15 2.21\nCIP 1.72 2.05 2.19 2.28 2.29\nPIQA 1.67 1.93 2.08 2.15 2.21\nBS=1 BS=2 BS=4 BS=8 BS=16\nLLM: LLaMA-7B, SSM: LLaMA-68M\n0\n5\n10\n15\n20\n25\n30Per-token latency (ms)\nSequence-based Decoding\nTree-based Decoding\nFigure 11. Comparing SpecInferâ€™s tree-based parallel decod-\ning with the sequence-based decoding mechanism employed\nby existing LLM inference systems.\noverhead, since SpecInfer must verify more tokens. Figure 10\ncompares the end-to-end inference latency of SpecInfer using\ndifferent tree widths. For small batch sizes (i.e., BS = 1 and 2),\nusing a large tree width can consistently reduce per-token\nlatency, since SpecInfer can leverage sparse GPU resources\nto verify more tokens in parallel while maintaining the same\nper-iteration latency. For large batch sizes (i.e., BS â‰¥ 4),\nusing a large tree width increases the latency to verify a\ntoken tree due to less sparse GPU resources that can be\nleveraged by SpecInfer, and a tree width of 2 or 3 achieves\nthe best performance by striking a perfect balance between\nspeculative performance and verification latency.\n6.5 Tree-based Parallel Decoding\nWe now evaluate the effectiveness of SpecInferâ€™s tree-based\nparallel decoding mechanism, which decodes all tokens of a\nTable 3. Average number of tokens verified by SpecInfer in a\nstochastic decoding step with different sampling algorithms.\nWe use LLaMA-7B and LLaMA-68M as the LLM and SSM.\nEach token tree has a width of 5 and a depth of 8.\nNaive Multi-Step Improvement\nSampling Spec. Sampling\nAlpaca 1.87 2.38 1.27 Ã—\nCP 1.80 2.28 1.26 Ã—\nWebQA 1.73 2.21 1.28 Ã—\nCIP 1.79 2.29 1.28 Ã—\nPIQA 1.73 2.21 1.28 Ã—\ntoken tree in parallel. As a comparison, all existing LLM infer-\nence systems use sequence-based decoding, which requires\ndecomposing a token tree into multiple sequences of tokens\nand processing these sequences using separate resources due\nto potential key-value cache conflicts (see Â§4.2). As shown in\nFigure 11, SpecInferâ€™s tree-based parallel decoding achieves\non-par performance as existing sequence-based decoding\nmechanism for small batch sizes and outperforms it by up\nto 1.8Ã—for large batch sizes. The improvement is realized\nby (1) eliminating redundant attention computation for se-\nquences with a shared prefix, and (2) fusing tree attention\nof all tokens in a single kernel through the topology-aware\ncasual mask (see Â§4.2).\n6.6 Multi-Step Speculative Sampling\nThis section evaluates how our multi-step speculative sam-\npling (MSS) and the VerifyStochastic algorithm improves\nthe speculative performance of SpecInfer when performing\nstochastic decoding. We use naive sampling as a baseline\nwhere SpecInfer directly samples the next token from the\nLLM and examines whether the sampled token is included\nin the speculated token tree (see Â§4.3). Since different sam-\npling algorithms involve identical speculation and verifica-\ntion overheads, we focus on the average number of tokens\nthat can be verified in each stochastic decoding step in this\nexperiment. Table 3 shows the results. Compared to naive\nsampling, MSS can consistently improve the number of veri-\nfied tokens by 1.2-1.3Ã—on average across a variety of prompt\ndatasets, while guaranteeing the same output distribution\nwith the LLM.\n7 Related Work\nTransformer-based LLMs have demonstrated significant po-\ntential in numerous human-level language modeling tasks by\ncontinuously increasing their sizes [7, 9, 37, 43, 48]. As GPT-\n3 becomes the first model to surpass 100B parameters [ 3],\nmultiple LLMs (>100B) have been released, including OPT-\n175B [57], Bloom-176B [38], and PaLM [7]. Recent work has\n13\nproposed a variety of approaches to accelerating generative\nLLM inference, which can be categorized into two classes.\nLossless acceleration. Prior work has explored the idea of\nusing an LLM as a verifier instead of a decoder to boost infer-\nence. For example, Yang et al. [53] introduced inference with\nreference, which leverages the overlap between an LLMâ€™s out-\nput and the references obtained by retrieving documents, and\nchecks each referenceâ€™s appropriateness by examining the\ndecoding results of the LLM. Motivated by the idea of specula-\ntive execution in processor optimizations [4, 16], recent work\nproposed speculative decoding , which uses a small language\nmodel to produce a sequence of tokens and examines the\ncorrectness of these tokens using an LLM [5, 22, 25, 44, 51].\nThere are two key differences between SpecInfer and these\nprior works. First, instead of only considering a single se-\nquence of tokens, SpecInfer generates and verifies a token\ntree, whose nodes each represent a unique token sequence.\nSpecInfer performs tree attention to compute the attention\noutput of these token sequences in parallel and uses a novel\ntree-based decoding algorithm to reuse intermediate results\nshared across these sequences. Second, prior attempts gener-\nally consider a single small language model for speculation,\nwhich cannot align well with an LLM due to the model ca-\npacity gap between them. SpecInfer introduces two novel\nspeculation methods, including 1) expanding from a single\nSSM and 2) merging from multiple fine-tuned SSMs, and the\ngenerated token tree largely increases the coverage of the\nLLMâ€™s output.\nPrior work has also introduced a variety of techniques to\noptimize ML computations on modern hardware platforms.\nFor example, TVM [ 6] and Ansor [ 58] automatically gen-\nerate kernels for a given tensor program. TASO [ 20] and\nPET [50] automatically discover graph-level transformations\nto optimize the computation graph of a DNN. SpecInferâ€™s\ntechniques are orthogonal and can be combined with these\nsystems to accelerate generative LLM computation, which\nwe believe is a promising avenue for future work.\nLossy acceleration. BiLD [23] is a speculative decoding\nframework that uses a single SSM to accelerate LLM decod-\ning. Unlike the systems mentioned above, the acceleration\nis lossy: speed comes at the cost of a possible degradation\nin the generated tokens. Another line of research leverages\nmodel compression to reduce LLM inference latency while\ncompromising the predictive performance of the LLM. For\nexample, prior work proposed to leverage weight/activation\nquantization of LLMs to reduce the memory and computa-\ntion requirements of serving these LLMs [8, 11, 35, 52, 54].\nRecent work further explores a variety of structured prun-\ning techniques for accelerating Transformer-based architec-\ntures [10, 17, 49]. A key difference between SpecInfer and\nthese prior works is that SpecInfer does not directly reduce\nthe computation requirement for performing LLM inference,\nbut instead reorganizing LLM inference computation in a\nmore parallelizable way, which reduces memory accesses\nand inference latency at the cost of manageable memory and\ncomputation overheads.\nTree-structured attention. Nguyen et al. [31] introduced\ntree-structured attention , a technique that lets a Transformer\nmodel capture the hierarchical composition of input text by\nrunning the model on the textâ€™s parse tree. To process with\nattention, it uses a one-on-one mapping to encode and de-\ncode the tree. There are two key differences from SpecInferâ€™s\ntree-based decoding. First, SpecInfer uses a tree to combine\ncandidate sequences to condense prefixes, whereas Nguyen\net al. represent a single sequence with its parse tree. SpecInfer\ndoes not incorporate parse tree into the LLM, but accelerates\ninference by verifying decoded sequences in parallel. Second,\nSpecInferâ€™s attention outputs a token sequence, not a tree.\nMulti-sample decoding techniques. Like tree-based spec-\nulative inference,beam search, top-k sampling, and top-p sam-\npling consider multiple candidate token sequences at each\nstep and can prune low-probability options. However, tree-\nbased decoding in SpecInfer speculatively predicts and veri-\nfies multiple candidates in parallel against an LLM to reduce\ndecoding iterations and latency, leveraging small specula-\ntive models (SSMs). In contrast, beam search and top-k/top-\np sampling are decoding strategies applied directly to the\nLLMâ€™s output probabilities to generate high-probability se-\nquences without reducing decoding steps. SpecInfer supports\nbeam search, top-k sampling, and top-p sampling. These tech-\nniques are orthogonal decoding optimizations and can be\ncombined with tree-based speculative decoding.\n8 Conclusion\nThis paper introduces SpecInfer, a system that accelerates\ngenerative LLM inference with tree-based speculative in-\nference and verification. A key insight behind SpecInfer is\nto simultaneously consider a diversity of speculation can-\ndidates to efficiently predict the LLMâ€™s outputs, which are\norganized as a token tree and verified against the LLM in\nparallel using a tree-based parallel decoding mechanism.\nSpecInfer significantly reduces the memory accesses to the\nLLMâ€™s parameters and the end-to-end LLM inference latency\nfor both distributed and offloading-based LLM inference.\nAcknowledgement\nWe thank Tianqi Chen, Bohan Hou, Hongyi Jin, the anony-\nmous ASPLOS reviewers, and our shepherd Shan Lu for their\nfeedback on this work. This research is partially supported by\nNSF awards CNS-2147909, CNS-2211882, and CNS-2239351,\nand research awards from Amazon, Cisco, Google, Meta,\nOracle, Qualcomm, and Samsung.\n14\nA Artifact Appendix\nA.1 Abstract\nThe artifact contains the code to run SpecInfer, as well as\nthe datasets and scripts that can be used to reproduce the\nexperiments in the paper.\nA.2 Artifact check-list (meta-information)\nâ€¢Algorithm: Tree-based Speculative Inference\nâ€¢Program: spec_infer.cc, incr_decoding.cc\nâ€¢Compilation: CMake\nâ€¢Run-time environment: CUDA, NCCL, MPI, UCX, Python3.\nâ€¢Hardware: Two AWS g5.12xlarge instances, each with 4\nNVIDIA A10 24GB GPUs, 48 CPU cores, and 192 GB DRAM.\nâ€¢Metrics: End to-end average latency\nâ€¢Output: End-to-end latency\nâ€¢Experiments: Server-grade GPU inference, Offloading-\nbased inference\nâ€¢How much disk space required (approximately)?: 350GB\nper node\nâ€¢How much time is needed to prepare workflow (ap-\nproximately)?: 2h\nâ€¢How much time is needed to complete experiments\n(approximately)?: 6h\nâ€¢Publicly available?: Yes\nâ€¢Code licenses (if publicly available)?: Apache License\nv2.0\nâ€¢Data licenses (if publicly available)?: LLAMA is under\nthe GNU license and OPT is under a Non-commercial license\nâ€¢Archived (provide DOI)?: https://doi.org/10.5281/zenodo.\n10854410.\nA.3 Description\nA.3.1 How to access. The artifact is released on Github:\nhttps://github.com/goliaro/specinfer-ae. The repository con-\ntains SpecInferâ€™s source code, and the instructions to build\nthe framework. We also provide scripts to reproduce the\nexperiments from the paper. To clone the repository, use\nthe following command (make sure to pass the â€“recursive\nflag):\ngit clone --recursive \\\nhttps://github.com/goliaro/specinfer-ae.git\nA.3.2 Hardware dependencies. We run out experiments\non two AWS g5.12xlarge instances, each with 4 NVIDIA A10\n24GB GPUs, 48 CPU cores, and 192 GB DRAM. We provide\ninstructions to create and setup the instances.\nA.3.3 Software dependencies. The following software\nis required: CUDA 12.1, NCCL, Rust, CMake and Python3.\nFurther, UCX and MPI are required for the multinode experi-\nments. Additional Python dependencies are listed here:https:\n//github.com/flexflow/FlexFlow/blob/inference/requirements.\ntxt. We recommend using the Deep Learning OSS Nvidia\nDriver AMI GPU PyTorch 2.1.0 (Ubuntu 20.04) AMI,\nand provide scripts and a conda environment to install all\nthe remaining dependencies.\nA.3.4 Models. We use the following LLM/SSM models for\nour experiments (for each model, we specify in parentheses\nthe corresponding HuggingFace repository): LLaMA-68M\n(JackFram/llama-68m), LLaMA-7B (huggyllama/llama-7b),\nLLaMA-65B (huggyllama/llama-65b), OPT-125M (facebook/opt-\n125m), OPT-13B (facebook/opt-13b), OPT-125M (facebook/opt-\n30b). You can download all these models with the script:\n./download_models.sh\nA.4 Installation\nTo reproduce the experiments, you will need access to two\nAWS g5.12xlarge instances (or other machines with the same\nGPU/CPU/network specs). If you are using the preconfigured\ninstances we provided, you can skip this step.\nLaunching the instances. Launch two AWS g5.12xlarge\ninstances using the Deep Learning OSS Nvidia Driver\nAMI GPU PyTorch 2.1.0 (Ubuntu 20.04)AMI. Make sure\nto place the instances in a placement group that utilizes the\ncluster strategy to achieve low-latency network performance.\nAttach the same security group to all instances and add an\ninbound rule in the security group to allow all incoming\ntraffic from the same security group. For example, you can\nadd the following rule: Type: All TCP, Source: Anywhere-\nIPv4.\nInstalling the prerequisites. After gaining access to the\nAWS instances, install the prerequisites by following the\nsteps below. First, activate the conda shell support by running\nconda init bash , and then restarting the shell session.\nNext, create the conda environment with all the required\ndependencies by running:\nconda env create -f FlexFlow/conda/flexflow.yml\nconda activate flexflow\nMultinode setup. Download and build UCX by running\nthe install_ucx.shscript. Next, if you are running SpecIn-\nfer on two AWS instances, you will need to configure MPI so\nthat the two instances are mutually accessible. Pick a main\nnode, and create a SSH key pair with:\nssh-keygen -t ed25519\nAppend the contents of the public key (~/.ssh/id_ed25519.pub)\nto the ~/.ssh/authorized_keys file on BOTH the main\nand secondary machine. Note that if the .sshfolder or the\nauthorized_keysfile do not exist, you will need to create\nthem manually. Finally, create a file at the path ~/hostfile\nwith the following contents:\n<main_node_private_ip> slots=4\n<secondary_node_private_ip> slots=4\n15\nreplacing <main_node_private_ip> and <secondary_node_private_ip>\nwith the private IP addresses of the two machines, and the\nnumber of slots with the number of GPUs available (if you\nare using the recommended AWS instances, you will use a\nvalue of 4). You can find each machineâ€™s private IP address\nby running the command (and use the first IP value that is\nprinted):\nhostname -I\nInstall SpecInfer. To install SpecInfer, run the script:\n./install_specinfer.sh\nA.5 Basic Test\nTo ensure that SpecInfer is installed correctly and is func-\ntional, run the basic_test.sh script. This script will test\nthe basic incremental decoding and speculative inference\nfunctionalities, on both single and multi nodes. It will also\ntest the support for offloading. The test passes if it prints the\n\"Test passed!\" message.\nA.6 Experiment workflow\nThe artifact comes with scripts to gather the data that can be\nused to reproduce the results from the paper. It also comes\nwith scripts that can be used to convert the output data into\nCSV format for plotting.\nRunning Experiments. We run the following two exper-\niments to evaluate SpecInfer under different hardware setups.\nThe output data will be saved to theFlexFlow/inference/output\npath.\nâ€¢Server-grade GPU evaluation. This experiment tests\nthe performance of SpecInfer on server-grade GPUs.\nThe LLMs and SSMs are loaded in GPU memory, and\nwe measure the end-to-end inference latency using 1\nnode, and 2 nodes. In the single node case, we measure\nthe performance using 1 GPU, or 4 GPUs. In the multin-\node case, we use 4GPUs per node. The experiments use\nLLAMA-7B, OPT-30B and LLAMA-65B as the LLMs,\nand LLAMA-68M and OPT-125M as SSMs. The exper-\niment runs SpecInfer in three different modes: incre-\nmental decoding, sequence-based speculative decod-\ning, and tree-based speculative decoding. The former\ntwo are used to obtain data for the ablation study, and\nthe latter is the novel inference mode proposed by\nSpecInfer, and will be deployed by the user. To run the\nserver-grade GPU evaluation, run:\n./server_gpu_experiments.sh\nâ€¢Offloading evaluation. This experiment tests the\nperformance of SpecInfer when loading only a subset\nof parameters in GPU memory, while offloading the\nremaining ones on CPU DRAM. This technique is used\nto perform inference when the target model is larger\nthan the available GPU memory. In the experiment,\nSpecInfer uses a single GPU and swaps the modelâ€™s\nweights to and from the CPU. To run the offloading\nevaluation, run:\n./offloading_experiments.sh\nThird-party frameworks. Please follow the vLLM, Faster-\nTransformer, and HuggingFace TGI, and FlexGen official doc-\numentation to reproduce the performance of the third-party\nframeworks under the experiment scenarios.\nOutput data. The scripts above will generate data at\nthe FlexFlow/inference/output path. For each scenario,\na .txt file contains the generated output for each prompt,\nand a .out file contains the stdout logs. The quality of the\ngenerated output can be evaluated visually and compared\nwith the output from third-party inference frameworks. We\nprovide scripts to parse the raw output data and generate\nCSV files that can be used to generate the paperâ€™s figures. The\nREADME provides all details on the scripts and the mapping\nbetween CSV files and figures.\nA.7 Evaluation and expected results\nThe data from the CSV files should show similar performance\nto the figures from the paper. Some variability is to be ex-\npected, but overall, SpecInfer should behave according to\nFigures 7-11 from the paper.\nA.8 Experiment customization\nUsers can edit the configuration parameters from the evalu-\nation scripts to change various parameters, such as the num-\nber of GPUs/CPUs, GPU/CPU memory, batch size, LLM/SSM\nmodels used, prompt dataset, full vs. half-precision, and the\nmaximum number of tokens to generate.\nReferences\n[1] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Seman-\ntic parsing on Freebase from question-answer pairs. In Proceedings\nof the 2013 Conference on Empirical Methods in Natural Language Pro-\ncessing, pages 1533â€“1544, Seattle, Washington, USA, October 2013.\nAssociation for Computational Linguistics.\n[2] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin\nChoi. Piqa: Reasoning about physical commonsense in natural lan-\nguage. In Thirty-Fourth AAAI Conference on Artificial Intelligence ,\n2020.\n[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared\nKaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam,\nGirish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,\nMark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. Language models are few-shot learners,\n2020.\n[4] F Warren Burton. Speculative computation, parallelism, and functional\nprogramming. IEEE Transactions on Computers , 100(12):1190â€“1193,\n1985.\n16\n[5] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste\nLespiau, Laurent Sifre, and John Jumper. Accelerating large lan-\nguage model decoding with speculative sampling. arXiv preprint\narXiv:2302.01318, 2023.\n[6] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan,\nHaichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze,\nCarlos Guestrin, and Arvind Krishnamurthy. TVM: An automated\nend-to-end optimizing compiler for deep learning. In 13th USENIX\nSymposium on Operating Systems Design and Implementation (OSDI\n18), pages 578â€“594, 2018.\n[7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma,\nGaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung,\nCharles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\nmodeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\n[8] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.\nGpt3. int8 (): 8-bit matrix multiplication for transformers at scale.\nAdvances in Neural Information Processing Systems , 35:30318â€“30332,\n2022.\n[9] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lep-\nikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu,\nOrhan Firat, et al. Glam: Efficient scaling of language models with\nmixture-of-experts. In International Conference on Machine Learning ,\npages 5547â€“5569. PMLR, 2022.\n[10] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models\ncan be accurately pruned in one-shot, 2023.\n[11] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq:\nAccurate quantization for generative pre-trained transformers. In\nInternational Conference on Learning Representations , 2023.\n[12] Yoav Freund, Robert Schapire, and Naoki Abe. A short introduction\nto boosting. Journal-Japanese Society For Artificial Intelligence , 14(771-\n780):1612, 1999.\n[13] Freddy Gabbay and Avi Mendelson. Speculative execution based on\nvalue prediction . Citeseer, 1996.\n[14] Mudasir A Ganaie, Minghui Hu, AK Malik, M Tanveer, and PN Sugan-\nthan. Ensemble deep learning: A review. Engineering Applications of\nArtificial Intelligence, 115:105151, 2022.\n[15] Aaron Gokaslan*, Vanya Cohen*, Ellie Pavlick, and Stefanie\nTellex. Openwebtext corpus. http://Skylion007.github.io/\nOpenWebTextCorpus, 2019.\n[16] John L Hennessy and David A Patterson. Computer architecture: a\nquantitative approach. Elsevier, 2011.\n[17] Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Joseph Naor,\nand Daniel Soudry. Accelerated sparse neural training: A provable\nand efficient method to find n: m transposable masks. Advances in\nNeural Information Processing Systems , 34:21099â€“21111, 2021.\n[18] HuggingFace. Large language model text generation inference. https:\n//github.com/huggingface/text-generation-inference. (Accessed on\n08/09/2023).\n[19] Hugging Face Inc. Hugging face. https://huggingface.co, 2023.\n[20] Zhihao Jia, Oded Padon, James Thomas, Todd Warszawski, Matei\nZaharia, and Alex Aiken. Taso: optimizing deep learning computation\nwith automatic generation of graph substitutions. In Proceedings of the\n27th ACM Symposium on Operating Systems Principles , pages 47â€“62,\n2019.\n[21] Zhihao Jia, Matei Zaharia, and Alex Aiken. Beyond data and model par-\nallelism for deep neural networks. In Proceedings of the 2nd Conference\non Systems and Machine Learning , SysMLâ€™19, 2019.\n[22] Joao Gante. Assisted generation: a new direction toward low-latency\ntext generation, 2023.\n[23] Sehoon Kim, Karttikeya Mangalam, Suhong Moon, John Canny, Jiten-\ndra Malik, Michael W. Mahoney, Amir Gholami, and Kurt Keutzer. Big\nlittle transformer decoder, 2023.\n[24] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin\nZheng, Cody Yu, Joseph E Gonzalez, Hao Zhang, and Ion Stoica.\nvllm: Easy, fast, and cheap llm serving with pagedattention. See\nhttps://vllm.ai/ (accessed 9 August 2023) , 2023.\n[25] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from\ntransformers via speculative decoding. arXiv preprint arXiv:2211.17192 ,\n2022.\n[26] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin,\nand Weizhu Chen. What makes good in-context examples for gpt-3?\narXiv preprint arXiv:2101.06804 , 2021.\n[27] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Hongyi\nJin, Tianqi Chen, and Zhihao Jia. Towards efficient generative large\nlanguage model serving: A survey from algorithms to systems. arXiv\npreprint arXiv:2312.15234, 2023.\n[28] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu\nWang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna\nAbhyankar, and Zhihao Jia. Specinfer: Accelerating generative llm\nserving with speculative inference and token tree verification. arXiv\npreprint arXiv:2305.09781, 2023.\n[29] Xupeng Miao, Chunan Shi, Jiangfei Duan, Xiaoli Xi, Dahua Lin, Bin\nCui, and Zhihao Jia. Spotserve: Serving generative large language\nmodels on preemptible instances. arXiv preprint arXiv:2311.15566 ,\n2023.\n[30] MohamedRashad. Chatgpt-prompts. https://huggingface.co/datasets/\nMohamedRashad/ChatGPT-prompts, 2023.\n[31] Xuan-Phi Nguyen, Shafiq Joty, Steven Hoi, and Richard Socher. Tree-\nstructured attention with hierarchical accumulation. In International\nConference on Learning Representations , 2020.\n[32] NVIDIA. Fastertransformer. https://github.com/NVIDIA/\nFasterTransformer. (Accessed on 08/09/2023).\n[33] OpenAI. Gpt-4 technical report, 2023.\n[34] Alessandro Palla. chatbot instruction prompts. https://huggingface.\nco/datasets/alespalla/chatbot_instruction_prompts, 2023.\n[35] Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim,\nYoungjoo Lee, and Dongsoo Lee. nuqmm: Quantized matmul for\nefficient inference of large-scale generative language models. arXiv\npreprint arXiv:2206.09557, 2022.\n[36] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng\nGao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277 ,\n2023.\n[37] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan\nHoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman\nRing, Susannah Young, et al. Scaling language models: Methods, anal-\nysis & insights from training gopher. arXiv preprint arXiv:2112.11446 ,\n2021.\n[38] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana IliÄ‡,\nDaniel Hesslow, Roman CastagnÃ©, Alexandra Sasha Luccioni, FranÃ§ois\nYvon, Matthias GallÃ©, et al. Bloom: A 176b-parameter open-access\nmultilingual language model. arXiv preprint arXiv:2211.05100 , 2022.\n[39] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin,\nDaniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gon-\nzalez, Percy Liang, Christopher RÃ©, Ion Stoica, and Ce Zhang. Flexgen:\nHigh-throughput generative inference of large language models with\na single gpu, 2023.\n[40] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin,\nDaniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gon-\nzalez, Percy Liang, Christopher RÃ©, Ion Stoica, and Ce Zhang. High-\nthroughput generative inference of large language models with a single\ngpu, 2023.\n[41] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley,\nJared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-\nbillion parameter language models using model parallelism, 2020.\n[42] James E Smith. A study of branch prediction strategies. In 25 years of\nthe international symposia on Computer architecture (selected papers) ,\npages 202â€“215, 1998.\n[43] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley,\nSamyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye,\n17\nGeorge Zerveas, Vijay Korthikanti, et al. Using deepspeed and mega-\ntron to train megatron-turing nlg 530b, a large-scale generative lan-\nguage model. arXiv preprint arXiv:2201.11990 , 2022.\n[44] Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise par-\nallel decoding for deep autoregressive models. Advances in Neural\nInformation Processing Systems , 31, 2018.\n[45] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li,\nCarlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford\nalpaca: An instruction-following llama model. https://github.com/\ntatsu-lab/stanford_alpaca, 2023.\n[46] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-\nAnne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric\nHambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971 , 2023.\n[47] Colin Unger, Zhihao Jia, Wei Wu, Sina Lin, Mandeep Baines, Carlos\nEfrain Quintero Narvaez, Vinay Ramakrishnaiah, Nirmal Prajapati,\nPat McCormick, Jamaludin Mohd-Yusof, Xi Luo, Dheevatsa Mudigere,\nJongsoo Park, Misha Smelyanskiy, and Alex Aiken. Unity: Accelerating\nDNN training through joint optimization of algebraic transformations\nand parallelization. In 16th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 22) , pages 267â€“284, Carlsbad, CA,\nJuly 2022. USENIX Association.\n[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention\nis all you need, 2017.\n[49] Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse\nattention architecture with cascade token and head pruning. In 2021\nIEEE International Symposium on High-Performance Computer Archi-\ntecture (HPCA), pages 97â€“110. IEEE, 2021.\n[50] Haojie Wang, Jidong Zhai, Mingyu Gao, Zixuan Ma, Shizhi Tang, Liyan\nZheng, Yuanzhi Li, Kaiyuan Rong, Yuanyong Chen, and Zhihao Jia.\nPET: Optimizing tensor programs with partially equivalent transfor-\nmations and automated corrections. In 15th USENIX Symposium on\nOperating Systems Design and Implementation (OSDI 21) , pages 37â€“54.\nUSENIX Association, July 2021.\n[51] Heming Xia, Tao Ge, Si-Qing Chen, Furu Wei, and Zhifang Sui. Specu-\nlative decoding: Lossless speedup of autoregressive translation.\n[52] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song\nHan. Smoothquant: Accurate and efficient post-training quantization\nfor large language models. arXiv preprint arXiv:2211.10438 , 2022.\n[53] Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang,\nRangan Majumder, and Furu Wei. Inference with reference: Lossless\nacceleration of large language models. arXiv preprint arXiv:2304.04487 ,\n2023.\n[54] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu,\nConglong Li, and Yuxiong He. Zeroquant: Efficient and affordable\npost-training quantization for large-scale transformers. Advances in\nNeural Information Processing Systems , 35:27168â€“27183, 2022.\n[55] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and\nByung-Gon Chun. Orca: A distributed serving system for Transformer-\nBased generative models. In 16th USENIX Symposium on Operating\nSystems Design and Implementation (OSDI 22) , pages 521â€“538, Carlsbad,\nCA, July 2022. USENIX Association.\n[56] Haoyu Zhang, Jianjun Xu, and Ji Wang. Pretraining-based nat-\nural language generation for text summarization. arXiv preprint\narXiv:1902.09243, 2019.\n[57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen,\nShuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria\nLin, et al. Opt: Open pre-trained transformer language models. arXiv\npreprint arXiv:2205.01068, 2022.\n[58] Lianmin Zheng, Chengfan Jia, Minmin Sun, Zhao Wu, Cody Hao Yu,\nAmeer Haj-Ali, Yida Wang, Jun Yang, Danyang Zhuo, Koushik Sen,\net al. Ansor: Generating high-performance tensor programs for deep\nlearning. In 14th {USENIX}Symposium on Operating Systems Design\nand Implementation ( {OSDI}20), pages 863â€“879, 2020.\n18",
  "topic": "Security token",
  "concepts": [
    {
      "name": "Security token",
      "score": 0.899824321269989
    },
    {
      "name": "Computer science",
      "score": 0.8521169424057007
    },
    {
      "name": "Correctness",
      "score": 0.7378621101379395
    },
    {
      "name": "Inference",
      "score": 0.6846386790275574
    },
    {
      "name": "Tree (set theory)",
      "score": 0.5779197216033936
    },
    {
      "name": "Generative model",
      "score": 0.5589202642440796
    },
    {
      "name": "Generative grammar",
      "score": 0.5509476065635681
    },
    {
      "name": "Decoding methods",
      "score": 0.4985935688018799
    },
    {
      "name": "Language model",
      "score": 0.4879559874534607
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3992273211479187
    },
    {
      "name": "Distributed computing",
      "score": 0.32806530594825745
    },
    {
      "name": "Artificial intelligence",
      "score": 0.304030179977417
    },
    {
      "name": "Programming language",
      "score": 0.2800641655921936
    },
    {
      "name": "Computer network",
      "score": 0.20546507835388184
    },
    {
      "name": "Algorithm",
      "score": 0.19341251254081726
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I36258959",
      "name": "University of California, San Diego",
      "country": "US"
    }
  ],
  "cited_by": 4
}