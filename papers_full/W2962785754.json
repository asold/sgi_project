{
  "title": "HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization",
  "url": "https://openalex.org/W2962785754",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2101736369",
      "name": "Xingxing Zhang",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2171151462",
      "name": "Furu Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096867225",
      "name": "Ming Zhou",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2888556271",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W1620608722",
    "https://openalex.org/W2962972512",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2889518897",
    "https://openalex.org/W2054211469",
    "https://openalex.org/W2962985882",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2896807716",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2101390659",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2890419630",
    "https://openalex.org/W2156387975",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2307381258",
    "https://openalex.org/W2574535369",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W3151369355",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2963545005",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2140440594",
    "https://openalex.org/W2963125472",
    "https://openalex.org/W1602831581",
    "https://openalex.org/W2964165364",
    "https://openalex.org/W2293771131",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963385935",
    "https://openalex.org/W2612675303",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2970830889",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2112077341",
    "https://openalex.org/W2962788148",
    "https://openalex.org/W2890861846",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W2952138241"
  ],
  "abstract": "Neural extractive summarization models usually employ a hierarchical encoder for document encoding and they are trained using sentence-level labels, which are created heuristically using rule-based methods. Training the hierarchical encoder with these inaccurate labels is challenging. Inspired by the recent work on pre-training transformer sentence encoders (Devlin et al., 2018), we propose Hibert (as shorthand for HIerachical Bidirectional Encoder Representations from Transformers) for document encoding and a method to pre-train it using unlabeled data. We apply the pre-trained Hibert to our summarization model and it outperforms its randomly initialized counterpart by 1.25 ROUGE on the CNN/Dailymail dataset and by 2.0 ROUGE on a version of New York Times dataset. We also achieve the state-of-the-art performance on these two datasets.",
  "full_text": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5059–5069\nFlorence, Italy, July 28 - August 2, 2019.c⃝2019 Association for Computational Linguistics\n5059\nHIBERT: Document Level Pre-training of Hierarchical Bidirectional\nTransformers for Document Summarization\nXingxing Zhang, Furu Weiand Ming Zhou\nMicrosoft Research Asia, Beijing, China\n{xizhang,fuwei,mingzhou}@microsoft.com\nAbstract\nNeural extractive summarization models usu-\nally employ a hierarchical encoder for doc-\nument encoding and they are trained us-\ning sentence-level labels, which are created\nheuristically using rule-based methods. Train-\ning the hierarchical encoder with these inac-\ncurate labels is challenging. Inspired by the\nrecent work on pre-training transformer sen-\ntence encoders (Devlin et al., 2018), we pro-\npose H IBERT (as shorthand for HIerachical\nBidirectional Encoder Representations from\nTransformers) for document encoding and a\nmethod to pre-train it using unlabeled data. We\napply the pre-trained H IBERT to our summa-\nrization model and it outperforms its randomly\ninitialized counterpart by 1.25 ROUGE on the\nCNN/Dailymail dataset and by 2.0 ROUGE\non a version of New York Times dataset. We\nalso achieve the state-of-the-art performance\non these two datasets.\n1 Introduction\nAutomatic document summarization is the task of\nrewriting a document into its shorter form while\nstill retaining its important content. Over the\nyears, many paradigms for document summariza-\ntion have been explored (see Nenkova and McK-\neown (2011) for an overview). The most popular\ntwo among them areextractive approaches and ab-\nstractive approaches. As the name implies, extrac-\ntive approaches generate summaries by extract-\ning parts of the original document (usually sen-\ntences), while abstractive methods may generate\nnew words or phrases which are not in the original\ndocument.\nExtractive summarization is usually modeled\nas a sentence ranking problem with length con-\nstraints (e.g., max number of words or sentences).\nTop ranked sentences (under constraints) are se-\nlected as summaries. Early attempts mostly lever-\nage manually engineered features (Filatova and\nHatzivassiloglou, 2004a). Based on these sparse\nfeatures, sentence are selected using a classiﬁer or\na regression model. Later, the feature engineering\npart in this paradigm is replaced with neural net-\nworks. Cheng and Lapata (2016) propose a hierar-\nchical long short-term memory network (LSTM;\nHochreiter and Schmidhuber 1997) to encode a\ndocument and then use another LSTM to predict\nbinary labels for each sentence in the document.\nThis architecture is widely adopted recently (Nal-\nlapati et al., 2017; Narayan et al., 2018; Zhang\net al., 2018). Our model also employs a hierarchi-\ncal document encoder, but we adopt a hierarchical\ntransformer (Vaswani et al., 2017) rather a hier-\narchical LSTM. Because recent studies (Vaswani\net al., 2017; Devlin et al., 2018) show the trans-\nformer model performs better than LSTM in many\ntasks.\nAbstractive models do not attract much atten-\ntion until recently. They are mostly based on se-\nquence to sequence (seq2seq) models (Bahdanau\net al., 2015), where a document is viewed a se-\nquence and its summary is viewed as another se-\nquence. Although seq2seq based summarizers\ncan be equipped with copy mechanism (Gu et al.,\n2016; See et al., 2017), coverage model (See et al.,\n2017) and reinforcement learning (Paulus et al.,\n2017), there is still no guarantee that the generated\nsummaries are grammatical and convey the same\nmeaning as the original document does. It seems\nthat extractive models are more reliable than their\nabstractive counterparts.\nHowever, extractive models require sentence\nlevel labels, which are usually not included in\nmost summarization datasets (most datasets only\ncontain document-summary pairs). Sentence la-\nbels are usually obtained by rule-based methods\n(e.g., maximizing the ROUGE score between a set\nof sentences and reference summaries) and may\nnot be accurate. Extractive models proposed re-\n5060\ncently (Cheng and Lapata, 2016; Nallapati et al.,\n2017) employ hierarchical document encoders and\neven have neural decoders, which are complex.\nTraining such complex neural models with inac-\ncurate binary labels is challenging. We observed\nin our initial experiments on one of our dataset\nthat our extractive model (see Section 3.3 for de-\ntails) overﬁts to the training set quickly after the\nsecond epoch, which indicates the training set\nmay not be fully utilized. Inspired by the recent\npre-training work in natural language processing\n(Peters et al., 2018; Radford et al., 2018; Devlin\net al., 2018), our solution to this problem is to\nﬁrst pre-train the “complex”’ part (i.e., the hier-\narchical encoder) of the extractive model on unla-\nbeled data and then we learn to classify sentences\nwith our model initialized from the pre-trained en-\ncoder. In this paper, we propose H IBERT , which\nstands for HIerachical Bidirectional Encoder\nRepresentations from Transformers. We design\nan unsupervised method to pre-train H IBERT for\ndocument modeling. We apply the pre-trained\nHIBERT to the task of document summarization\nand achieve state-of-the-art performance on both\nthe CNN/Dailymail and New York Times dataset.\n2 Related Work\nIn this section, we introduce work on extractive\nsummarization, abstractive summarization and\npre-trained natural language processing models.\nFor a more comprehensive review of summariza-\ntion, we refer the interested readers to Nenkova\nand McKeown (2011) and Mani (2001).\nExtractive Summarization Extractive summa-\nrization aims to select important sentences (some-\ntimes other textual units such as elementary dis-\ncourse units (EDUs)) from a document as its sum-\nmary. It is usually modeled as a sentence rank-\ning problem by using the scores from classiﬁers\n(Kupiec et al., 1995), sequential labeling models\n(Conroy and O’leary, 2001) as well as integer lin-\near programmers (Woodsend and Lapata, 2010).\nEarly work with these models above mostly lever-\nage human engineered features such as sentence\nposition and length (Radev et al., 2004), word fre-\nquency (Nenkova et al., 2006) and event features\n(Filatova and Hatzivassiloglou, 2004b).\nAs the very successful applications of neural\nnetworks to a wide range of NLP tasks, the man-\nually engineered features (for document encod-\ning) are replaced with hierarchical LSTMs/CNNs\nand the sequence labeling (or classiﬁcation) model\nis replaced with an LSTM decoder (Cheng and\nLapata, 2016; Nallapati et al., 2017). The ar-\nchitecture is widely adopted in recent neural ex-\ntractive models and is extended with reinforce-\nment learning (Narayan et al., 2018; Dong et al.,\n2018), latent variable models (Zhang et al., 2018),\njoint scoring (Zhou et al., 2018) and iterative doc-\nument representation (Chen et al., 2018). Re-\ncently, transformer networks (Vaswani et al.,\n2017) achieves good performance in machine\ntranslation (Vaswani et al., 2017) and a range of\nNLP tasks (Devlin et al., 2018; Radford et al.,\n2018). Different from the extractive models\nabove, we adopt a hierarchical Transformer for\ndocument encoding and also propose a method to\npre-train the document encoder.\nAbstractive Summarization Abstractive sum-\nmarization aims to generate the summary of a\ndocument with rewriting. Most recent abstractive\nmodels (Nallapati et al., 2016) are based on neural\nsequence to sequence learning (Bahdanau et al.,\n2015; Sutskever et al., 2014). However, the gen-\nerated summaries of these models can not be con-\ntrolled (i.e., their meanings can be quite different\nfrom the original and contents can be repeated).\nTherefore, copy mechanism (Gu et al., 2016), cov-\nerage model (See et al., 2017) and reinforcement\nlearning model optimizing ROUGE (Paulus et al.,\n2017) are introduced. These problems are allevi-\nated but not solved. There is also an interesting\nline of work combining extractive and abstractive\nsummarization with reinforcement learning (Chen\nand Bansal, 2018), fused attention (Hsu et al.,\n2018) and bottom-up attention (Gehrmann et al.,\n2018). Our model, which is a very good extractive\nmodel, can be used as the sentence extraction com-\nponent in these models and potentially improves\ntheir performance.\nPre-trained NLP Models Most model pre-\ntraining methods in NLP leverage the natural or-\ndering of text. For example, word2vec uses the\nsurrounding words within a ﬁxed size window to\npredict the word in the middle with a log bilin-\near model. The resulting word embedding table\ncan be used in other downstream tasks. There are\nother word embedding pre-training methods using\nsimilar techniques (Pennington et al., 2014; Bo-\njanowski et al., 2017). Peters et al. (2018) and\nRadford et al. (2018) ﬁnd even a sentence encoder\n5061\nFigure 1: The architecture of H IBERT during training.\nsenti is a sentence in the document above, which has\nfour sentences in total. sent 3 is masked during encod-\ning and the decoder predicts the original sent3.\n(not just word embeddings) can also be pre-trained\nwith language model objectives (i.e., predicting\nthe next or previous word). Language model ob-\njective is unidirectional, while many tasks can\nleverage the context in both directions. Therefore,\nDevlin et al. (2018) propose the naturally bidi-\nrectional masked language model objective (i.e.,\nmasking several words with a special token in\na sentence and then predicting them). All the\nmethods above aim to pre-train word embeddings\nor sentence encoders, while our method aims to\npre-train the hierarchical document encoders (i.e.,\nhierarchical transformers), which is important in\nsummarization.\n3 Model\nIn this section, we present our model HIBERT . We\nﬁrst introduce how documents are represented in\nHIBERT . We then describe our method to pre-train\nHIBERT and ﬁnally move on to the application of\nHIBERT to summarization.\n3.1 Document Representation\nLet D = (S1,S2,...,S |D|) denote a document,\nwhere Si = (wi\n1,wi\n2,...,w i\n|Si|) is a sentence in D\nand wi\nj a word in Si. Note that following common\npractice in natural language processing literatures,\nwi\n|Si|is an artiﬁcial EOS (End Of Sentence) token.\nTo obtain the representation of D, we use two en-\ncoders: a sentence encoder to transform each sen-\ntence in Dto a vector and a document encoder\nto learn sentence representations given their sur-\nrounding sentences as context. Both the sentence\nencoder and document encoder are based on the\nTransformer encoder described in Vaswani et al.\n(2017). As shown in Figure 1, they are nested\nin a hierarchical fashion. A transformer encoder\nusually has multiple layers and each layer is com-\nposed of a multi-head self attentive sub-layer fol-\nlowed by a feed-forward sub-layer with residual\nconnections (He et al., 2016) and layer normal-\nizations (Ba et al., 2016). For more details of the\nTransformer encoder, we refer the interested read-\ners to Vaswani et al. (2017). To learn the repre-\nsentation of Si, Si = (wi\n1,wi\n2,...,w i\n|Si|) is ﬁrst\nmapped into continuous space\nEi = (ei\n1,ei\n2,..., ei\n|Si|)\nwhere ei\nj = e(wi\nj) +pj\n(1)\nwhere e(wi\nj) and pj are the word and positional\nembeddings of wi\nj, respectively. The word embed-\nding matrix is randomly initialized and we adopt\nthe sine-cosine positional embedding (Vaswani\net al., 2017)1. Then the sentence encoder (a Trans-\nformer) transforms Ei into a list of hidden rep-\nresentations (hi\n1,hi\n2,..., hi\n|Si|). We take the last\nhidden representation hi\n|Si|(i.e., the representation\nat the EOS token) as the representation of sentence\nSi. Similar to the representation of each word in\nSi, we also take the sentence position into account.\nThe ﬁnal representation of Si is\nˆhi = hi\n|Si|+ pi (2)\nNote that words and sentences share the same po-\nsitional embedding matrix.\nIn analogy to the sentence encoder , as shown\nin Figure 1, the document encoder is yet another\nTransformer but applies on the sentence level. Af-\nter running the Transformer on a sequence of sen-\ntence representations (ˆh1,ˆh2,..., ˆh|D|), we ob-\ntain the context sensitive sentence representations\n(d1,d2,..., d|D|). Now we have ﬁnished the en-\ncoding of a document with a hierarchical bidirec-\ntional transformer encoder H IBERT . Note that in\nprevious work, document representation are also\n1We use the sine-cosine embedding because it works well\nand do not introduce additional trainable parameters.\n5062\nlearned with hierarchical models, but each hier-\narchy is a Recurrent Neural Network (Nallapati\net al., 2017; Zhou et al., 2018) or Convolutional\nNeural Network (Cheng and Lapata, 2016). We\nchoose the Transformer because it outperforms\nCNN and RNN in machine translation (Vaswani\net al., 2017), semantic role labeling (Strubell et al.,\n2018) and other NLP tasks (Devlin et al., 2018).\nIn the next section we will introduce how we train\nHIBERT with an unsupervised training objective.\n3.2 Pre-training\nMost recent encoding neural models used in NLP\n(e.g., RNNs, CNNs or Transformers) can be pre-\ntrained by predicting a word in a sentence (or a\ntext span) using other words within the same sen-\ntence (or span). For example, ELMo (Peters et al.,\n2018) and OpenAI-GPT (Radford et al., 2018)\npredict a word using all words on its left (or right);\nwhile word2vec (Mikolov et al., 2013) predicts\none word with its surrounding words in a ﬁxed\nwindow and BERT (Devlin et al., 2018) predicts\n(masked) missing words in a sentence given all the\nother words.\nAll the models above learn the representation\nof a sentence, where its basic units are words.\nHIBERT aims to learn the representation of a doc-\nument, where its basic units are sentences. There-\nfore, a natural way of pre-training a document\nlevel model (e.g., HIBERT ) is to predict a sentence\n(or sentences) instead of a word (or words). We\ncould predict a sentence in a document with all the\nsentences on its left (or right) as in a (document\nlevel) language model. However, in summariza-\ntion, context on both directions are available. We\ntherefore opt to predict a sentence using all sen-\ntences on both its left and right.\nDocument Masking Speciﬁcally, suppose D=\n(S1,S2,...,S |D|) is a document, where Si =\n(wi\n1,wi\n2,...,w i\n|Si|) is a sentence in it. We ran-\ndomly select 15% of the sentences in Dand mask\nthem. Then, we predict these masked sentences.\nThe prediction task here is similar with the Cloze\ntask (Taylor, 1953; Devlin et al., 2018), but the\nmissing part is a sentence. However, during test\ntime the input document is not masked, to make\nour model can adapt to documents without masks,\nwe do not always mask the selected sentences.\nOnce a sentence is selected (as one of the 15%\nselected masked sentences), we transform it with\none of three methods below. We will use an ex-\nample to demonstrate the transformation. For in-\nstance, we have the following document and the\nsecond sentence is selected2:\nWilliam Shakespeare is a poet .\nHe died in 1616 . He is regarded\nas the greatest writer .\nIn 80% of the cases, we mask the selected\nsentence (i.e., we replace each word in the sen-\ntence with a mask token [MASK]). The document\nabove becomes William Shakespeare is\na poet . [MASK] [MASK] [MASK]\n[MASK] [MASK] He is regarded as\nthe greatest writer . (where “ He\ndied in 1616 . ” is masked).\nIn 10% of the cases, we keep the selected sen-\ntence as it is. This strategy is to simulate the input\ndocument during test time (with no masked sen-\ntences).\nIn the rest 10% cases, we replace the selected\nsentence with a random sentence. In this case,\nthe document after transformation is William\nShakespeare is a poet . Birds\ncan fly . He is regarded as the\ngreatest writer . The second sentence\nis replaced with “ Birds can fly .” This\nstrategy intends to add some noise during training\nand make the model more robust.\nSentence Prediction After the application of\nthe above procedures to a document D =\n(S1,S2,...,S |D|), we obtain the masked docu-\nment ˜D= ( ˜S1, ˜S2,..., ˜S|D|). Let Kdenote the\nset of indicies of selected sentences in D. Now\nwe are ready to predict the masked sentences\nM = {Sk|k ∈ K}using ˜D. We ﬁrst apply\nthe hierarchical encoder HIBERT in Section 3.1 to\n˜Dand obtain its context sensitive sentence rep-\nresentations ( ˜d1, ˜d2,..., ˜d|D|). We will demon-\nstrate how we predict the masked sentence Sk =\n(wk\n0 ,wk\n1 ,wk\n2 ,...,w k\n|Sk|) one word per step ( wk\n0 is\nan artiﬁcially added BOS token). At the jth step,\nwe predict wk\nj given wk\n0 ,...,w k\nj−1 and ˜D. ˜dk al-\nready encodes the information of ˜Dwith a focus\naround its kth sentence ˜Sk. As shown in Figure 1,\nwe employ a Transformer decoder (Vaswani et al.,\n2017) to predict wk\nj with ˜dk as its additional input.\nThe transformer decoder we used here is slightly\ndifferent from the original one. The original de-\ncoder employs two multi-head attention layers to\n2There might be multiple sentences selected in a docu-\nment, but in this example there is only one.\n5063\ninclude both the context in encoder and decoder,\nwhile we only need one to learn the decoder con-\ntext, since the context in encoder is a vector (i.e.,\n˜dk). Speciﬁcally, after applying the word and po-\nsitional embeddings to (wk\n0 ,...,w k\nj−1), we obtain\n˜Ek\n1:j−1 = ( ˜ek\n0,..., ˜ek\nj−1) (also see Equation 1).\nThen we apply multi-head attention sub-layer to\n˜Ek\n1:j−1:\n˜hj−1 = MultiHead(qj−1,Kj−1,Vj−1)\nqj−1 = WQ ˜ek\nj−1\nKj−1 = WK ˜Ek\n1:j−1\nKj−1 = WV ˜Ek\n1:j−1\n(3)\nwhere qj−1, Kj−1, Vj−1 are the input query,\nkey and value matrices of the multi-head attention\nfunction (Vaswani et al., 2017) MultiHead (·,·,·),\nrespectively. WQ ∈ Rd×d, WK ∈ Rd×d and\nWV ∈Rd×d are weight matrices.\nThen we include the information of ˜Dby addi-\ntion:\n˜xj−1 = ˜hj−1 + ˜dk (4)\nWe also follow a feedforward sub-layer (one hid-\nden layer with ReLU (Glorot et al., 2011) acti-\nvation function) after ˜xj−1 as in Vaswani et al.\n(2017):\n˜gj−1 = Wff\n2 max(0,Wff\n1 ˜xj−1 + b1) +b2 (5)\nNote that the transformer decoder can have multi-\nple layers by applying Equation (3) to (5) multiple\ntimes and we only show the computation of one\nlayer for simplicity.\nThe probability of wk\nj given wk\n0 ,...,w k\nj−1 and\n˜Dis:\np(wk\nj |wk\n0:j−1, ˜D) =softmax(WO ˜gj−1) (6)\nFinally the probability of all masked sentences M\ngiven ˜Dis\np(M|˜D) =\n∏\nk∈K\n|Sk|∏\nj=1\np(wk\nj |wk\n0:j−1, ˜D) (7)\nThe model above can be trained by minimizing the\nnegative log-likelihood of all masked sentences\ngiven their paired documents. We can in the-\nory have unlimited amount of training data for\nHIBERT , since they can be generated automati-\ncally from (unlabeled) documents. Therefore, we\ncan ﬁrst train HIBERT on large amount of data and\nthen apply it to downstream tasks. In the next sec-\ntion, we will introduce its application to document\nsummarization.\nFigure 2: The architecture of our extractive summa-\nrization model. The sentence and document level trans-\nformers can be pretrained.\n3.3 Extractive Summarization\nExtractive summarization selects the most impor-\ntant sentences in a document as its summary. In\nthis section, summarization is modeled as a se-\nquence labeling problem. Speciﬁcally, a docu-\nment is viewed as a sequence of sentences and\na summarization model is expected to assign a\nTrue or False label for each sentence, where\nTrue means this sentence should be included in\nthe summary. In the following, we will intro-\nduce the details of our summarization model based\nHIBERT .\nLet D = ( S1,S2,...,S |D|) denote a docu-\nment and Y = ( y1,y2,...,y |D|) its sentence\nlabels (methods for obtaining these labels are\nin Section 4.1). As shown in Figure 2, we\nﬁrst apply the hierarchical bidirectional trans-\nformer encoder H IBERT to Dand yields the con-\ntext dependent representations for all sentences\n(d1,d2,..., d|D|). The probability of the label of\nSi can be estimated using an additional linear pro-\njection and a softmax:\np(yi|D) =softmax(WS di) (8)\nwhere WS ∈R2×d. The summarization model\ncan be trained by minimizing the negative log-\nlikelihood of all sentence labels given their paired\ndocuments.\n4 Experiments\nIn this section we assess the performance of our\nmodel on the document summarization task. We\n5064\nﬁrst introduce the dataset we used for pre-training\nand the summarization task and give implementa-\ntion details of our model. We also compare our\nmodel against multiple previous models.\n4.1 Datasets\nWe conducted our summarization experiments\non the non-anonymous version CNN/Dailymail\n(CNNDM) dataset (Hermann et al., 2015; See\net al., 2017), and the New York Times dataset\n(Durrett et al., 2016; Xu and Durrett, 2019). For\nthe CNNDM dataset, we preprocessed the dataset\nusing the scripts from the authors of See et al.\n(2017)3. The resulting dataset contains 287,226\ndocuments with summaries for training, 13,368\nfor validation and 11,490 for test. Following (Xu\nand Durrett, 2019; Durrett et al., 2016), we cre-\nated the NYT50 dataset by removing the docu-\nments whose summaries are shorter than 50 words\nfrom New York Times dataset. We used the same\ntraining/validation/test splits as in Xu and Dur-\nrett (2019), which contain 137,778 documents for\ntraining, 17,222 for validation and 17,223 for test.\nTo create sentence level labels for extractive sum-\nmarization, we used a strategy similar to Nallapati\net al. (2017). We label the subset of sentences in\na document that maximizes R OUGE (Lin, 2004)\n(against the human summary) as True and all\nother sentences as False.\nTo unsupervisedly pre-train our document\nmodel H IBERT (see Section 3.2 for details), we\ncreated the GIGA-CM dataset (totally 6,626,842\ndocuments and 2,854 million words), which in-\ncludes 6,339,616 documents sampled from the En-\nglish Gigaword4 dataset and the training split of\nthe CNNDM dataset. We used the validation set\nof CNNDM as the validation set of GIGA-CM\nas well. As in See et al. (2017), documents and\nsummaries in CNNDM, NYT50 and GIGA-CM\nare all segmented and tokenized using Stanford\nCoreNLP toolkit (Manning et al., 2014). To re-\nduce the vocabulary size, we applied byte pair en-\ncoding (BPE; Sennrich et al. 2016) to all of our\ndatasets. To limit the memory consumption dur-\ning training, we limit the length of each sentence\nto be 50 words (51th word and onwards are re-\nmoved) and split documents with more than 30\nsentences into smaller documents with each con-\ntaining at most 30 sentences.\n3Scripts publicly available athttps://github.com/\nabisee/cnn-dailymail\n4https://catalog.ldc.upenn.edu/LDC2012T21\n4.2 Implementation Details\nOur model is trained in three stages, which in-\ncludes two pre-training stages and one ﬁnetuning\nstage. The ﬁrst stage is the open-domain pre-\ntraining and in this stage we train HIBERT with the\npre-training objective (Section 3.2) on GIGA-CM\ndataset. In the second stage, we perform the in-\ndomain pre-training on the CNNDM (or NYT50)\ndataset still with the same pre-training objective.\nIn the ﬁnal stage, we ﬁnetune H IBERT in the sum-\nmarization model (Section 3.3) to predict extrac-\ntive sentence labels on CNNDM (or NYT50).\nThe sizes of the sentence and document level\nTransformers as well as the Transformer decoder\nin H IBERT are the same. Let L denote the num-\nber of layers in Transformer, H the hidden size\nand A the number of attention heads. As in\n(Vaswani et al., 2017; Devlin et al., 2018), the hid-\nden size of the feedforward sublayer is 4H. We\nmainly trained two model sizes: HIBERT S (L= 6,\nH = 512 and A = 8) and H IBERT M (L = 6,\nH = 768and A= 12). We trained both HIBERT S\nand HIBERT M on a single machine with 8 Nvidia\nTesla V100 GPUs with a batch size of 256 doc-\numents. We optimized our models using Adam\nwith learning rate of 1e-4, β1 = 0.9, β2 = 0.999,\nL2 norm of 0.01, learning rate warmup 10,000\nsteps and learning rate decay afterwards using the\nstrategies in Vaswani et al. (2017). The dropout\nrate in all layers are 0.1. In pre-training stages,\nwe trained our models until validation perplexities\ndo not decrease signiﬁcantly (around 45 epochs on\nGIGA-CM dataset and 100 to 200 epochs on CN-\nNDM and NYT50). Training H IBERT M for one\nepoch on GIGA-CM dataset takes approximately\n20 hours.\nOur models during ﬁne-tuning stage can be\ntrained on a single GPU. The hyper-parameters are\nalmost identical to these in the pre-training stages\nexcept that the learning rate is 5e-5, the batch size\nis 32, the warmup steps are 4,000 and we train our\nmodels for 5 epochs. During inference, we rank\nsentences using p(yi|D) (Equation (8)) and choose\nthe top Ksentences as summary, whereKis tuned\non the validation set.\n4.3 Evaluations\nWe evaluated the quality of summaries from dif-\nferent systems automatically using ROUGE (Lin,\n2004). We reported the full length F1 based\nROUGE-1, ROUGE-2 and ROUGE-L on the\n5065\nModel R-1 R-2 R-L\nPointer+Coverage 39.53 17.28 36.38\nAbstract-ML+RL 39.87 15.82 36.90\nDCA 41.69 19.47 37.92\nSentRewrite 40.88 17.80 38.54\nInconsisLoss 40.68 17.97 37.13\nBottom-Up 41.22 18.68 38.34\nLead3 40.34 17.70 36.57\nSummaRuNNer 39.60 16.20 35.30\nNeuSum 40.11 17.52 36.39\nRefresh 40.00 18.20 36.60\nNeuSum-MMR 41.59 19.01 37.98\nBanditSum 41.50 18.70 37.60\nJECS 41.70 18.50 37.90\nLatentSum 41.05 18.77 37.54\nHierTransformer 41.11 18.69 37.53\nBERT 41.82 19.48 38.30\nHIBERT S (in-domain) 42.10 19.70 38.53\nHIBERT S 42.31 19.87 38.78\nHIBERT M 42.37 19.95 38.83\nTable 1: Results of various models on the CNNDM test\nset using full-length F1 ROUGE -1 (R-1), ROUGE -2 (R-\n2), and ROUGE -L (R-L).\nCNNDM and NYT50 datasets. We compute\nROUGE scores using the ROUGE-1.5.5.pl\nscript.\nAdditionally, we also evaluated the generated\nsummaries by eliciting human judgments. Fol-\nlowing (Cheng and Lapata, 2016; Narayan et al.,\n2018), we randomly sampled 20 documents from\nthe CNNDM test set. Participants were presented\nwith a document and a list of summaries produced\nby different systems. We asked subjects to rank\nthese summaries (ties allowed) by taking informa-\ntiveness (is the summary capture the important in-\nformation from the document?) and ﬂuency (is the\nsummary grammatical?) into account. Each docu-\nment is annotated by three different subjects.\n4.4 Results\nOur main results on the CNNDM dataset are\nshown in Table 1, with abstractive models in\nthe top block and extractive models in the bot-\ntom block. Pointer+Coverage (See et al., 2017),\nAbstract-ML+RL (Paulus et al., 2017) and DCA\n(Celikyilmaz et al., 2018) are all sequence to se-\nquence learning based models with copy and cov-\nerage modeling, reinforcement learning and deep\ncommunicating agents extensions. SentRewrite\n(Hsu et al., 2018) and InconsisLoss (Chen and\nBansal, 2018) all try to decompose the word by\nword summary generation into sentence selection\nfrom document and “sentence” level summariza-\ntion (or compression). Bottom-Up (Gehrmann\net al., 2018) generates summaries by combines a\nword prediction model with the decoder attention\nmodel. The extractive models are usually based\non hierarchical encoders (SummaRuNNer; Nalla-\npati et al. 2017 and NeuSum; Cheng and Lapata\n2016). They have been extended with reinforce-\nment learning (Refresh; Narayan et al. 2018 and\nBanditSum; Dong et al. 2018), Maximal Marginal\nRelevance (NeuSum-MMR; Zhou et al. 2018), la-\ntent variable modeling (LatentSum; Zhang et al.\n2018) and syntactic compression (JECS; Xu and\nDurrett 2019). Lead3 is a baseline which sim-\nply selects the ﬁrst three sentences. Our model\nHIBERT S (in-domain), which only use one pre-\ntraining stage on the in-domain CNNDM training\nset, outperforms all of them and differences be-\ntween them are all signiﬁcant with a 0.95 conﬁ-\ndence interval (estimated with the ROUGE script).\nNote that pre-training H IBERT S (in-domain) is\nvery fast and it only takes around 30 minutes\nfor one epoch on the CNNDM training set. Our\nmodels with two pre-training stages (HIBERT S) or\nlarger size (H IBERT M ) perform even better and\nHIBERT M outperforms BERT by 0.5 ROUGE 5.\nWe also implemented two baselines. One is\nthe hierarchical transformer summarization model\n(HeriTransfomer; described in 3.3) without pre-\ntraining. Note the setting for HeriTransfomer is\n(L = 4,H = 300 and A = 4) 6. We can see\nthat the pre-training (details in Section 3.2) leads\nto a +1.25 ROUGE improvement. Another base-\nline is based on a pre-trained BERT (Devlin et al.,\n2018)7 and ﬁnetuned on the CNNDM dataset. We\nused the BERTbase model because our 16G RAM\nV100 GPU cannot ﬁt BERT large for the summa-\nrization task even with batch size of 1. The posi-\ntional embedding of BERT supports input length\nup to 512 words, we therefore split documents\nwith more than 10 sentences into multiple blocks\n5The difference is signiﬁcant according to the ROUGE\nscript.\n6We tried deeper and larger models, but obtained inferior\nresults, which may indicates training large or deep models on\nthis dataset without a good initialization is challenging.\n7Our BERT baseline is adapted from this imple-\nmentation https://github.com/huggingface/\npytorch-pretrained-BERT\n5066\nModels R-1 R-2 R-L\nLead 41.80 22.60 35.00\nEXTRACTION 44.30 25.50 37.10\nJECS 45.50 25.30 38.20\nHeriTransformer 47.44 28.08 39.56\nBERT 48.38 29.04 40.53\nHIBERT S (in-domain) 48.92 29.58 41.10\nHIBERT M (in-domain) 49.06 29.70 41.23\nHIBERT S 49.25 29.92 41.43\nHIBERT M 49.47 30.11 41.63\nTable 2: Results of various models on the NYT50\ntest set using full-length F1 ROUGE. H IBERT S (in-\ndomain) and HIBERT M (in-domain) only uses one pre-\ntraining stage on the NYT50 training set.\nPretraining Strategies R-1 R-2 R-L\nOpen-Domain 42.97 20.31 39.51\nIn-Domain 42.93 20.28 39.46\nOpen+In-Domain 43.19 20.46 39.72\nTable 3: Results of summarization model (H IBERT S\nsetting) with different pre-training strategies on the\nCNNDM validation set using full-length F1 ROUGE.\n(each block with 10 sentences 8). We feed each\nblock (the BOS and EOS tokens of each sentence\nare replaced with [CLS] and [SEP] tokens) into\nBERT and use the representation at [CLS] token\nto classify each sentence. Our model H IBERT S\noutperforms BERT by 0.4 to 0.5 ROUGE despite\nwith only half the number of model parameters\n(HIBERT S 54.6M v.s. BERT 110M).\nResults on the NYT50 dataset show the similar\ntrends (see Table 2). EXTRACTION is a extrac-\ntive model based hierarchical LSTM and we use\nthe numbers reported by Xu and Durrett (2019).\nThe improvement of H IBERT M over the baseline\nwithout pre-training (HeriTransformer) becomes\n2.0 ROUGE. H IBERT S (in-domain), H IBERT M\n(in-domain), H IBERT S and H IBERT M all outper-\nform BERT signiﬁcantly according to the ROUGE\nscript.\nWe also conducted human experiment with 20\nrandomly sampled documents from the CNNDM\ntest set. We compared our model H IBERT M\nagainst Lead3, DCA, Latent, BERT and the human\nreference (Human)9. We asked the subjects to rank\n8We use 10 sentences per block, because maximum sen-\ntence length 50 × 10 < 512 (maximum BERT supported\nlength). The last block of a document may have less than 10\nsentences.\n9We obtained the outputs of DCA via emails.\nModels 1st 2nd 3rd 4th 5th 6th MeanR\nLead3 0.03 0.18 0.15 0.30 0.30 0.03 3.75\nDCA 0.08 0.15 0.18 0.20 0.15 0.23 3.88\nLatent 0.05 0.33 0.28 0.20 0.13 0.00 3.03\nBERT 0.13 0.37 0.32 0.15 0.03 0.00 2.58\nHIBERT M 0.30 0.35 0.25 0.10 0.00 0.00 2.15\nHuman 0.58 0.15 0.20 0.00 0.03 0.03 1.85\nTable 4: Human evaluation: proportions of rankings\nand mean ranks (MeanR; lower is better) of various\nmodels.\nthe outputs of these systems from best to worst.\nAs shown in Table 4, the output of H IBERT M is\nselected as the best in 30% of cases and we ob-\ntained lower mean rank than all systems except for\nHuman. We also converted the rank numbers into\nratings (rank ito 7 −i) and applied student t-test\non the ratings. H IBERT M is signiﬁcantly different\nfrom all systems in comparison (p< 0.05), which\nindicates our model still lags behind Human, but\nis better than all other systems.\nPre-training Strategies As mentioned earlier,\nour pre-training includes two stages. The ﬁrst\nstage is the open-domain pre-training stage on\nthe GIGA-CM dataset and the following stage\nis the in-domain pre-training on the CNNDM\n(or NYT50) dataset. As shown in Table 3,\nwe pretrained H IBERT S using only open-domain\nstage (Open-Domain), only in-domain stage (In-\nDomain) or both stages (Open+In-Domain) and\napplied it to the CNNDM summarization task. Re-\nsults on the validation set of CNNDM indicate the\ntwo-stage pre-training process is necessary.\n5 Conclusions\nThe core part of a neural extractive summariza-\ntion model is the hierarchical document encoder.\nWe proposed a method to pre-train document level\nhierarchical bidirectional transformer encoders on\nunlabeled data. When we only pre-train hierar-\nchical transformers on the training sets of summa-\nrization datasets with our proposed objective, ap-\nplication of the pre-trained hierarchical transform-\ners to extractive summarization models already\nleads to wide improvement of summarization per-\nformance. Adding the large open-domain dataset\nto pre-training leads to even better performance.\nIn the future, we plan to apply models to other\ntasks that also require hierarchical document en-\ncodings (e.g., document question answering). We\nare also interested in improving the architectures\n5067\nof hierarchical document encoders and designing\nother objectives to train hierarchical transformers.\nAcknowledgments\nWe would like to thank Nan Yang, Houwen Peng,\nLi Dong and the ACL reviewers for their valu-\nable feedback. We are grateful to Jiacheng Xu and\nGreg Durrett for sharing their splits of the New\nYork Times dataset with us.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In In Proceedings of\nthe 3rd International Conference on Learning Rep-\nresentations, San Diego, California.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nAsli Celikyilmaz, Antoine Bosselut, Xiaodong He, and\nYejin Choi. 2018. Deep communicating agents for\nabstractive summarization. In Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Pa-\npers), pages 1662–1675, New Orleans, Louisiana.\nXiuying Chen, Shen Gao, Chongyang Tao, Yan Song,\nDongyan Zhao, and Rui Yan. 2018. Iterative docu-\nment representation learning towards summarization\nwith polishing. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 4088–4097. Association for Com-\nputational Linguistics.\nYen-Chun Chen and Mohit Bansal. 2018. Fast abstrac-\ntive summarization with reinforce-selected sentence\nrewriting. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 675–686. Associa-\ntion for Computational Linguistics.\nJianpeng Cheng and Mirella Lapata. 2016. Neural\nsummarization by extracting sentences and words.\nIn Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 484–494, Berlin, Germany.\nJohn M Conroy and Dianne P O’leary. 2001. Text sum-\nmarization via hidden markov models. In Proceed-\nings of the 24th annual international ACM SIGIR\nconference on Research and development in infor-\nmation retrieval, pages 406–407. ACM.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. arXiv preprint arXiv:1810.04805.\nYue Dong, Yikang Shen, Eric Crawford, Herke van\nHoof, and Jackie Chi Kit Cheung. 2018. Bandit-\nsum: Extractive summarization as a contextual ban-\ndit. In Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 3739–3748. Association for Computational\nLinguistics.\nGreg Durrett, Taylor Berg-Kirkpatrick, and Dan Klein.\n2016. Learning-based single-document summariza-\ntion with compression and anaphoricity constraints.\nIn Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1998–2008. Association for\nComputational Linguistics.\nElena Filatova and Vasileios Hatzivassiloglou. 2004a.\nEvent-based extractive summarization. In Text Sum-\nmarization Branches Out: Proceedings of the ACL-\n04 Workshop, pages 104–111, Barcelona, Spain.\nElena Filatova and Vasileios Hatzivassiloglou. 2004b.\nEvent-based extractive summarization.\nSebastian Gehrmann, Yuntian Deng, and Alexander\nRush. 2018. Bottom-up abstractive summarization.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n4098–4109. Association for Computational Linguis-\ntics.\nXavier Glorot, Antoine Bordes, and Yoshua Bengio.\n2011. Deep sparse rectiﬁer neural networks. In Pro-\nceedings of the fourteenth international conference\non artiﬁcial intelligence and statistics , pages 315–\n323.\nJiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.\nLi. 2016. Incorporating copying mechanism in\nsequence-to-sequence learning. In Proceedings of\nthe 54th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) ,\npages 1631–1640. Association for Computational\nLinguistics.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nKarl Moritz Hermann, Tomas Kocisky, Edward\nGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-\nleyman, and Phil Blunsom. 2015. Teaching ma-\nchines to read and comprehend. InAdvances in Neu-\nral Information Processing Systems , pages 1693–\n1701. Curran Associates, Inc.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\n5068\nWan-Ting Hsu, Chieh-Kai Lin, Ming-Ying Lee, Kerui\nMin, Jing Tang, and Min Sun. 2018. A uniﬁed\nmodel for extractive and abstractive summarization\nusing inconsistency loss. In Proceedings of the 56th\nAnnual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers) , pages\n132–141. Association for Computational Linguis-\ntics.\nJulian Kupiec, Jan Pedersen, and Francine Chen. 1995.\nA trainable document summarizer. In Proceedings\nof the 18th annual international ACM SIGIR confer-\nence on Research and development in information\nretrieval, pages 68–73. ACM.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text Summarization\nBranches Out: Proceedings of the ACL-04 Work-\nshop, pages 74–81, Barcelona, Spain.\nInderjeet Mani. 2001. Automatic Summarization. John\nBenjamins Pub Co.\nChristopher Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven Bethard, and David McClosky.\n2014. The stanford corenlp natural language pro-\ncessing toolkit. In Proceedings of 52nd annual\nmeeting of the association for computational lin-\nguistics: system demonstrations, pages 55–60.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems, pages 3111–3119.\nRamesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017.\nSummarunner: A recurrent neural network based se-\nquence model for extractive summarization of doc-\numents. In In Proceedings of the 31st AAAI Con-\nference on Artiﬁcial Intelligence, pages 3075–3091,\nSan Francisco, California.\nRamesh Nallapati, Bowen Zhou, Caglar Gulcehre,\nBing Xiang, et al. 2016. Abstractive text summa-\nrization using sequence-to-sequence rnns and be-\nyond. arXiv preprint arXiv:1602.06023.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Ranking sentences for extractive summariza-\ntion with reinforcement learning. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 1747–1759, New Orleans, Louisiana.\nAni Nenkova and Kathleen McKeown. 2011. Auto-\nmatic summarization. Foundations and Trends in\nInformation Retrieval, 5(2–3):103–233.\nAni Nenkova, Lucy Vanderwende, and Kathleen McK-\neown. 2006. A compositional context sensitive\nmulti-document summarizer: exploring the factors\nthat inﬂuence summarization. In Proceedings of\nthe 29th annual international ACM SIGIR confer-\nence on Research and development in information\nretrieval, pages 573–580. ACM.\nRomain Paulus, Caiming Xiong, and Richard Socher.\n2017. A deep reinforced model for abstractive sum-\nmarization. arXiv preprint arXiv:1705.04304.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 confer-\nence on empirical methods in natural language pro-\ncessing (EMNLP), pages 1532–1543.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages 2227–\n2237. Association for Computational Linguistics.\nDragomir Radev, Timothy Allison, Sasha Blair-\nGoldensohn, John Blitzer, Arda C ¸ elebi, Stanko\nDimitrov, Elliott Drabek, Ali Hakim, Wai Lam,\nDanyu Liu, Jahna Otterbacher, Hong Qi, Horacio\nSaggion, Simone Teufel, Michael Topper, Adam\nWinkel, and Zhu Zhang. 2004. Mead - a plat-\nform for multidocument multilingual text summa-\nrization. In Proceedings of the Fourth International\nConference on Language Resources and Evaluation\n(LREC’04). European Language Resources Associ-\nation (ELRA).\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. URL https://s3-\nus-west-2. amazonaws. com/openai-assets/research-\ncovers/languageunsupervised/language under-\nstanding paper. pdf.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1073–\n1083, Vancouver, Canada.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725. Association for Computational Linguistics.\nEmma Strubell, Patrick Verga, Daniel Andor,\nDavid Weiss, and Andrew McCallum. 2018.\nLinguistically-informed self-attention for semantic\nrole labeling. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 5027–5038. Association for\nComputational Linguistics.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in neural information process-\ning systems, pages 3104–3112.\n5069\nWilson L Taylor. 1953. cloze procedure: A new\ntool for measuring readability. Journalism Bulletin,\n30(4):415–433.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nKristian Woodsend and Mirella Lapata. 2010. Auto-\nmatic generation of story highlights. In Proceed-\nings of the 48th Annual Meeting of the Association\nfor Computational Linguistics, pages 565–574, Up-\npsala, Sweden.\nJiacheng Xu and Greg Durrett. 2019. Neural extrac-\ntive text summarization with syntactic compression.\narXiv preprint arXiv:1902.00863.\nXingxing Zhang, Mirella Lapata, Furu Wei, and Ming\nZhou. 2018. Neural latent extractive document sum-\nmarization. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Pro-\ncessing, pages 779–784. Association for Computa-\ntional Linguistics.\nQingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang,\nMing Zhou, and Tiejun Zhao. 2018. Neural docu-\nment summarization by jointly learning to score and\nselect sentences. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) , pages 654–663.\nAssociation for Computational Linguistics.",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.8756284117698669
    },
    {
      "name": "Computer science",
      "score": 0.813724160194397
    },
    {
      "name": "Transformer",
      "score": 0.7948484420776367
    },
    {
      "name": "Encoder",
      "score": 0.7860299348831177
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6246135830879211
    },
    {
      "name": "Sentence",
      "score": 0.6045653820037842
    },
    {
      "name": "Encoding (memory)",
      "score": 0.46373143792152405
    },
    {
      "name": "Natural language processing",
      "score": 0.4577125906944275
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4063635766506195
    },
    {
      "name": "Speech recognition",
      "score": 0.36406564712524414
    },
    {
      "name": "Machine learning",
      "score": 0.3226844072341919
    },
    {
      "name": "Engineering",
      "score": 0.06988251209259033
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210113369",
      "name": "Microsoft Research Asia (China)",
      "country": "CN"
    }
  ]
}