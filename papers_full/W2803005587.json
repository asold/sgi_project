{
  "title": "A Study of AI Population Dynamics with Million-agent Reinforcement Learning",
  "url": "https://openalex.org/W2803005587",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2367124232",
      "name": "Yang, Yaodong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222402111",
      "name": "Yu, Lantao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4293395242",
      "name": "Bai, Yiwei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1940314070",
      "name": "Wang Jun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2255876763",
      "name": "Zhang, Weinan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2143621997",
      "name": "Wen Ying",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2045189425",
      "name": "Yu Yong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2145339207",
    "https://openalex.org/W2020712278",
    "https://openalex.org/W2754233544",
    "https://openalex.org/W2154718404",
    "https://openalex.org/W2062029823",
    "https://openalex.org/W1532836182",
    "https://openalex.org/W2257979135",
    "https://openalex.org/W2037567262",
    "https://openalex.org/W1993691435",
    "https://openalex.org/W1575214265",
    "https://openalex.org/W2331625639",
    "https://openalex.org/W2024189170",
    "https://openalex.org/W2106684603"
  ],
  "abstract": "We conduct an empirical study on discovering the ordered collective dynamics obtained by a population of intelligence agents, driven by million-agent reinforcement learning. Our intention is to put intelligent agents into a simulated natural context and verify if the principles developed in the real world could also be used in understanding an artificially-created intelligent population. To achieve this, we simulate a large-scale predator-prey world, where the laws of the world are designed by only the findings or logical equivalence that have been discovered in nature. We endow the agents with the intelligence based on deep reinforcement learning (DRL). In order to scale the population size up to millions agents, a large-scale DRL training platform with redesigned experience buffer is proposed. Our results show that the population dynamics of AI agents, driven only by each agent's individual self-interest, reveals an ordered pattern that is similar to the Lotka-Volterra model studied in population biology. We further discover the emergent behaviors of collective adaptations in studying how the agents' grouping behaviors will change with the environmental resources. Both of the two findings could be explained by the self-organization theory in nature.",
  "full_text": "A Study of AI Population Dynamics with\nMillion-agent Reinforcement Learning\nYaodong Yang†, Lantao Yu‡, Yiwei Bai‡, Jun Wang†, Weinan Zhang‡, Ying Wen†, Yong Yu‡\nUniversity College London†, Shanghai Jiao Tong University‡\nABSTRACT\nWe conduct an empirical study on discovering the ordered col-\nlective dynamics obtained by a population of intelligence agents,\ndriven by million-agent reinforcement learning. Our intention is to\nput intelligent agents into a simulated natural context and verify\nif the principles developed in the real world could also be used\nin understanding an articially-created intelligent population. To\nachieve this, we simulate a large-scale predator-prey world, where\nthe laws of the world are designed by only the ndings or logical\nequivalence that have been discovered in nature. We endow the\nagents with the intelligence based on deep reinforcement learning\n(DRL). In order to scale the population size up to millions agents,\na large-scale DRL training platform with redesigned experience\nbuer is proposed. Our results show that the population dynamics\nof AI agents, driven only by each agent’s individual self-interest,\nreveals an ordered pattern that is similar to theLotka-Volterramodel\nstudied in population biology. We further discover the emergent\nbehaviors of collective adaptations in studying how the agents’\ngrouping behaviors will change with the environmental resources.\nBoth of the two ndings could be explained by theself-organization\ntheory in nature.\nKEYWORDS\nMulti-agent reinforcement learning, deep Q-learning, population\ndynamics, self-organization theory, collective behaviors, grouping\n1 INTRODUCTION\nBy employing the modeling power of deep learning, single-agent\nreinforcement learning (RL) has started to display, even surpass,\nhuman-level intelligence on a wide variety of tasks, ranging from\nplaying the games of Labyrinth [ 31], Atari [ 33], and Go [ 43] to\nother tasks such as continuous control on locomotions [27], text\ngeneration [53], and neural architecture design [54]. Very recently,\nmulti-agent RL algorithms have further broadened the use of RL\nand demonstrated their potentials in the setting where both of\nthe agents’ incentives and economical constraints exist. For ex-\nample, the studies [ 8, 25, 34, 49] have shown that with dierent\nmulti-agent cooperative learning environments, the compositional\nlanguage naturally emerges. Researchers in [13, 40, 48] have also\nProc. of the 17th International Conference on Autonomous Agents and Multiagent Systems\n(AAMAS 2018), M. Dastani, G. Sukthankar, E. Andre, S. Koenig (eds.), July 2018, Stockholm,\nSweden\n© 2018 International Foundation for Autonomous Agents and Multiagent Systems\n(www.ifaamas.org). All rights reserved.\nFirst three authors contribute equally.\nCorrespondence to <yaodong.yang@cs.ucl.ac.uk>\ndemonstrated that multiple agents can be trained to play the com-\nbat game in StarCraft, and the agents have mastered collaborative\nstrategies that are similar to those of experienced human players.\nNonetheless, all of the aforementioned RL systems so far have been\nlimited to less than tens of agents, and the focuses of their studies\nare rather in the optimization of a micro and individual level policy.\nMacro-level studies about the resulting collective behaviors and\ndynamics emerging from a large population of AI agents remain\nuntouched.\nYet, on the other hand, real-world populations exhibit certain\norders and regularity on collective behaviors: honey bees use spe-\ncic waggle dance to transmit signals, a trail of ants transfer food\nby leaving chemical marks on the routes, V-shaped formations of\nbird ocks during migration, or particular sizes of sh schools in\nthe deep ocean. Even human beings can easily show ordered macro\ndynamics, for example, the rhythmical audience applause after the\nconcerts, the periodical human waves in the fanatic football game,\netc. A stream of research on the theory of self-organization [2] ex-\nplores a new approach to explaining the emergence of orders in\nnature. In fact, the self-organizing dynamics appears in many other\ndisciplines of natural sciences [3]. The theory of self-organization\nsuggests that the ordered global dynamics, no matter how complex,\nare induced from repeated interactions between local individual\nparts of a system that are initially disordered, without external\nsupervisions or interventions. The concept has proven important\nin multiple elds in nature sciences [7, 23, 45].\nAs once the ancient philosopher Lucretius said: “A designing\nintelligence is necessary to create orders in nature. ” [38], an interest-\ning question for us is to understand what kinds of ordered macro\ndynamics, if any, that a community of articially-created agents\nwould possess when they are together put into the natural context.\nIn this paper, we ll the research gap by conducting an empirical\nstudy on the above questions. We aim to understand whether the\nprinciples, e.g., self-organization theory [2], that are developed in\nthe real world could also be applied on understanding an AI pop-\nulation. In order to achieve these, we argue that the key to this\nstudy is to have a clear methodology of introducing the micro-level\nintelligence; therefore, we simulate a predator-prey world where\neach individual AI agent is endowed with intelligence through a\nlarge-scale deep reinforcement learning framework. The population\nsize is scaled up to million level. To maximize the generality, the\nlaws of the predator-prey world are designed by only incorporating\nthe natural ndings or logic equivalence; miscellaneous potential\ndynamics can thus be studied. We rst study the macro dynamics\nof the population size for both the predators and preys, and then\nwe investigate the emergence of one most fundamental collective\nbehavior – grouping. In particular, we compare the statistics and\narXiv:1709.04511v4  [cs.AI]  14 May 2018\nAAMAS’18, July 2018, Stockholm, Sweden\nYaodong Yang†, Lantao Yu‡, Yiwei Bai‡, Jun Wang†, Weinan Zhang‡, Ying Wen†, Yong Yu‡\nUniversity College London†, Shanghai Jiao Tong University‡\ndynamics of the intelligent population with the theories and models\nfrom the real-world biological studies. Interestingly, we nd that\nthe articial predator-prey ecosystem, with individual intelligence\nincorporated, reaches an ordered pattern on dynamics that is simi-\nlar to what theLotka-Volterramodel indicates in population biology.\nAlso, we discover the emergence of the collective adaptations on\ngrouping behaviors when the environment changes. Both of the\ntwo ndings can be well explained based on the self-organization\ntheory. Moreover, the proposed million-agent RL platform could\nserve as the initial steps to understand the behaviors of large-scale\nAI population, driven by deep reinforcement learning. It could po-\ntentially open up an interesting research direction of understanding\nAI population by linking the ndings from the AI world with the\nnatural science principles from the real world, thus contributing to\nthe research frontiers such as smart city [35] and swarm intelligence\n[24].\n2 RELATED WORK\n2.1 Reinforcement learning\nReinforcement learning (RL) [46] employs a goal-oriented learning\nscheme that reinforces an agent to maximize its cumulative re-\nwards through sequentially interacting with the environment. The\nintelligence evolves by agent’s learning from the past experiences\nand trying to perform better in the future. Recently, deep neural\nnetworks succeed in marrying the RL algorithms; in particular, they\nshow remarkable performance in approximating the value function\n[32], the policy function [ 27], or both the value and policy func-\ntion (a.k.a. actor-critic) [31], all of which increase the “intelligence\"\nof traditional RL methods. Single-agent RL methods have been\nextended to the multi-agent settings where multiple agents exist\nand interact with each other. Q-learning methods such as minimax\nQ-learning [19], Nash Q-learning [18] have been proposed.\nIn addition to the work where minimal or even no communica-\ntion between learning agents are considered [9, 10, 30], a fundamen-\ntal question to answer in multi-agent RL is how dierent agents\nshould communicate so as to reach a coherent goal. Several dieren-\ntiable communication protocols have been proposed [12, 44], which\ncan be easily embedded into the error back-propagation training\nscheme. The work in [40] employed bidirectional recurrent neural\nnetworks to coordinate groups of agents to play StarCraft com-\nbat games, and achieved human-level micro-management skills.\nBeyond pursuing high performance on playing video games, re-\nsearchers recently start to shift the focus onto studying the commu-\nnity of AI agents, and its corresponding attributes. A few concur-\nrent studies [8, 25, 34, 49] were conducted in dierent cooperative\nlearning environments, through which the emergence of compo-\nsitional language has been found. Leibo et al . introduced agents’\nself-interested policy learning into solving sequential social dilem-\nmas, and discovered how agents’ behaviors would be inuenced by\nthe environmental factors, and when conicts would emerge from\ncompeting over shared resources. Nonetheless, the multi-agent sys-\ntems in those studies consider no more than tens of agents; it is\nthus unfair to generalize the ndings to the population level. Macro\ndynamics of large AI population remain to be disclosed.\n2.2 Population Biology\nWhile our subject is computerized artifacts, our work is also re-\nlated to the research conducted in natural sciences. The theory of\nself-organization proposed in [2] serves as a fundamental way of\nthinking to understand the emergence of orders in nature (even\nthough Physicists tend to challenge this theory because The Second\nLaw of Thermodynamics [5] states that the total level of disorders in\nan isolated system can never decrease over time). Self-organization\ntheory believers think that the global ordered dynamics of a system\ncan originate from numerous interactions between local individuals\nthat are initially disordered, with no needs for external interven-\ntions. The theory predicts the existence of the ordered dynamics in\nthe population. In fact, the self-organizing phenomena have been\nobserved in multiple elds in natural sciences [7, 23, 45]. For exam-\nple, in population biology, one important discovery is the ordered\nharmonic dynamics of the population sizes between predators and\npreys (e.g., the lynx and snowshoe hare [15]), which is summarized\ninto the Lotka-Volterra model [28]. It basically describes the fact\nthat there is 90° lag in the phase space between the population sizes\nof predators and preys (more details are discussed later in Section\n5.1). Even though explainable via the self-organization theory, the\nLotka-Volterra models are summarized based on the statistics from\nthe ecological eld studies. There is essentially no learning process\nor individual intelligence involved. In this work, we chose a dier-\nent approach by incorporating the individual intelligence into the\npopulation dynamics studies where each agent is endowed with the\nintelligence to make its own decision rather than is considered as\nhomogeneous and rule-based. Our intention is to nd out whether\nan AI population still creates ordered dynamics such as the Lotka-\nVolterra equations, if so, whether the dynamics is explainable from\nthe perspective of the self-organization theory.\nIn multiple disciplines of natural sciences spanning from zoology,\npsycholog, to economy [11, 17, 45], one of the most fundamental\nthus important collective behaviors to study is: grouping – a popu-\nlation of units aggregate together for collective decision-making.\nGrouping is believed to imply the emergence of sociality and to\ninduce other collective behaviors [21]. In studying the grouping be-\nhaviors, traditional approaches include setting up a game with rigid\nand reductive predened interactive rules for each agent and then\nconduct simulations based on the ad-hoc game [14, 20, 37]. Rule-\nbased games might work well on biological organisms that inherit\nthe same characteristics from their ancestors; however, they show\nlimits on studying the large-scale heterogeneous agents [4]. In con-\ntrast to rule-based games with no learning process involved, here we\ninvestigate the formation of grouping behaviors on a million-level\nAI population driven by RL algorithms. Our intention is to nd out\nhow the grouping behaviors in AI population emerge and change\nw.r.t. the environmental factors such as the food resources, and if\nthere is any other collective behaviors emerging from grouping.\n3 DESIGN OF THE LARGE-SCALE\nPREDATOR-PREY WORLD\nIn this paper, we try to understand: 1) whether AI population create\nany ordered patterns on population dynamics, and 2) the dynamics\nof the collective grouping behaviors. Predator-prey interaction is\none fundamental relationship observed in nature. Here we intend\nA Study of AI Population Dynamics with\nMillion-agent Reinforcement Learning AAMAS’18, July 2018, Stockholm, Sweden\nTable 1: Natural Evidence of the Axioms in the Simulated\nPredator-prey Environment\nAxiom Examples in nature\nPositive\nFeedback\nThe observations on ants [ 6, 52] suggest that\nwhen an ant discovers a food source through a\nparticular search trail, the path will soon serve as\nthe trigger for a positive feedback, by its leaving\nchemical landmarks, through which other ants\nstart to follow.\nNegative\nFeedback\nIn the ant’s case, as the population size of ants\nis limited, with increasing number of ants forage\nthe food from outside, the distribution of ants\nbetween food sources will be stable [16].\nIndividual\nVariation\nSocial insects as honey bees are evolved to be\nhighly variable in the directional sense, response\nto sucrose, level of focus in food collection in\norder to ensure the diversication in ways of food\ncollection, otherwise one single food resource\nwill be depleted quickly [22, 39].\nResponse\nThreshold\nBumble bees will start to fan so as to cool down\nthe hive when the temperature inside goes above\na threshold level [51].\nRedundancy In the kingdom of bees, if the community suers\na drastic reduction in the number of worker bees,\nyounger bees will soon replace their positions to\nguarantee that the whole community function\nwell [42].\nSynchroni-\nsation\nIn a concert, individuals with unique frequency\nof applause could aect the frequency of the\ncrowd through implicit synchronisation [36]. Em-\npirically, audience applause are often achieved\nthrough adjustments by individuals having an\nunique frequency among the local average.\nSelshness Easily observable in nature. A typical example\nwould be the Praying Mantis female who eats the\nmale head after mating as a reproductive strategy\nto enhance fertilization [47].\nto simulate a predator-prey world with million-level agents (shown\nin Fig. 1). The world is deigned to be easily adaptable to incorporate\nother environmental complexity to investigate the miscellaneous\ndynamics as well as collective behaviors of AI population where\neach individual agent is driven by purely self-interest.\n3.1 The Axioms of Natural Environments\nTo avoid introducing any specic rules that could harm the gen-\nerality of the observed results, we design the laws of the world\nby only considering those real ndings or logical equivalence that\nhave been observed in the natural system. We regard those laws\nas the axioms of studying population dynamics and collective be-\nhaviors. Here we briey review the axioms accepted, and refer\nthe corresponding natural evidence to the Table.1. Note that these\naxioms should not be treated separately, we consider instead how\nthe combination of these dierent axioms could produce and aect\ncollective dynamics.\n(1) Positive Feedback. Positive feedback enhances particular behav-\niors through reinforcement. It helps spread the information of\na meaningful action quickly between individuals.\n(2) Negative Feedback. Negative feedback leads to homeostasis. It\nhelps stabilize the collective behaviors produced in favor of the\npositive feedback from going to extremes.\n(3) Individual Variation. Individual variation is of the essence to\nguarantee the continual explorations of new solutions to the\nsame problem within a population.\n(4) Response Threshold. Response threshold is the threshold beyond\nwhich individuals will change their behaviors as a response to\nthe stimulus.\n(5) Redundancy. Redundancy ensures functional continuity of the\nwhole population even when a catastrophic event happens.\n(6) Synchronization. Synchronization is a special kind of positive\nfeedback in time rather than space. An example would be how\nindividual with unique frequency of applause aect the fre-\nquency of the crowd in a concert.\n(7) Selshness. Individuals always tend to maximize their own util-\nity. One will not behave altruistically for others until he can\nbenet more from behaving collectively than acting alone.\n3.2 Realization of the Axioms\nWe realize the predator-prey world via designing aStochastic Game.\nWe list the detailed rules of the game and its corresponding axiom.\n3.2.1 Population Dynamics. In the predator-prey world (see\nFig. 1), the goal for the predator species is to survive in the ecosys-\ntem and procreate their next generations (Axiom.7). Positions of\npredator/prey/obstacles in the world are all initialized randomly\nat the beginning. The environment is considered under an innite\nhorizon. While the population of both preys and predators can\nbe boomed by breeding osprings (Axiom.5), they however face\nthe hazards of either being hunted as preys, or dying of starva-\ntion as predators (Axiom.2). To realize the idea of starvation, we\nmake the health status of predator decrease with time by a con-\nstant factor, which can also be restored by capturing and eating\npreys (Axiom.1). Predators are assumed to have innite appetite;\nthe logical equivalence in nature is that a predator normally has\nthe ability of storing food resource for future survival. Each preda-\ntor can have unique characteristics, e.g., identity vector, eyesight\nand health status (Axiom.3). The unique characteristics of each\nagent represents the diversity of the population. Each individual\nagent make independent decision, and can behave dierently even\ngiven the same scenario. Predators can form a group to increase\nthe chance of capturing a prey (Axiom.1,4). Group members are\nvisible to predators within its view. If a single agent chooses the\naction of “join a group”, the environment will select a group within\nits view randomly, and the agent will become a member of that\ngroup until it decides to “leave the current group” afterwards. Note\nthat a single predator may hunt for the prey alone as well as hunt\nas a group member. As illustrated in Fig. 1, each prey is assigned\na square capture area with a capture radius ρ, which reects the\ndiculty of being hunted (Axiom.4). Groups of predators, or singles,\nwill only be able to hunt the prey if they manage to stay within the\ncapture radius. Apart from the capture radius, another parameter,\nAAMAS’18, July 2018, Stockholm, Sweden\nYaodong Yang†, Lantao Yu‡, Yiwei Bai‡, Jun Wang†, Weinan Zhang‡, Ying Wen†, Yong Yu‡\nUniversity College London†, Shanghai Jiao Tong University‡\nPredator       Prey       Obstacle        Health       ID       Group1       Group2 \n2\n1\n3 4\n6\n1\n2\n3 4\n6\n3\nTimestep t Timestep t+1\n5\n5\nFigure 1: Illustration of the predator-prey world. In the 2-D world, there exist preys, predators, and obstacles. Predators hunt\nthe prey so as to survive from starvation. Each predator has its own health bar and limited eyesight view. Predators can form a\ngroup to hunt the prey so that the chance of capturing can increase, but this also means that the captured prey will be shared\namong all group members. When there are multiple group targeting the same prey, the largest group within capture radius\nwill win. In this example, predators {2, 3, 4}form a group and win the prey over the group {5, 6}. Predator 5 soon dies because\nof starvation.\nthe capture threshold k (k=0,1,2,...), also reects the capturing di-\nculty of each prey (Axiom.4). Within the capture area, only meeting\nthe threshold will a group of predators become a valid candidate.\nWhen there are multiple valid candidate groups targeting at the\nsame prey, the group with the largest group size will be the winner,\nwhich mimics the law of jungle. When a group wins over other can-\ndidates, all the members in that group will share the prey equally\n(Axiom.2). The trade-o here is, in the pursuit of preys, grouping\nis encouraged as large group can help increase the probability of\ncapturing a prey; however, huge group size will also be inhibited\ndue to the less proportion of prey each group member obtains from\nthe sharing (Axiom.7).\n3.2.2 Grouping Behaviors. Considering synchronization of Ax-\niom.6 and selshness of Axiom.7, we incorporate a second type of\nprey that can be captured by an individual predator alone, which\nmeans we set the capture threshold k to 1 for that species. An anal-\nogy here is to think of tigers as the predators, sheep as the preys\nwhose captures require collaborative grouping between predators,\nand rabbits as the preys that can be captured by a single predator.\nThese two kinds of preys can be considered as an abstraction of in-\ndividual reward and grouping reward respectively. Predators have\nto make a decision to either join a group for hunting the sheep or\nconduct hunting the rabbit by itself in order to maximize its long-\nterm reward and the probability of survival (Axiom.1,2,7), which\nintroduces a trade-o between acting alone and collaborating with\nothers. We keep alternating the environments by feeding these two\nkinds of preys one after another (Axiom.6) and examine the dynam-\nics of grouping behaviors. To emphasize the dynamics of grouping\nQ-network\nExperience\nBuffer\n(Obs, ID)\nQ-value\n(Obs, ID)\nQ-value\n(Obs, ID)\nQ-value\n(Obs, ID)\nQ-value\n(st, at, rt, st+1)\nupdates action\nID embedding\naction\nreward\naction\nreward\nreward\n. . .\n. . .\n(st, at, rt, st+1)\n(st, at, rt, st+1)\n1\n2\n3 4\n6 5\nFigure 2: Million-agent Q-learning System in the Predator-\nprey World.\nbehaviors and also to avoid the inuences from the systematic pref-\nerences for grouping as a result of the changing population size, we\nkeep the population size of predators xed by endowing them with\neternal longevity, which can also be considered as a short-term\nobservation during which there is little change of the predator pop-\nulation size. Under the environment, the optimal strategy for each\nagent continuously varies over time and the predator population\nhas to learn to adapt their collective strategy correspondingly.\n4 AI POPULATION BUILT BY MULTI-AGENT\nDEEP REINFORCEMENT LEARNING\n4.1 Multi-agent Markov Decision Process\nIn the designed predator-prey world, we build AI population un-\nder the multi-agent deep reinforcement learning setting. Formally,\nA Study of AI Population Dynamics with\nMillion-agent Reinforcement Learning AAMAS’18, July 2018, Stockholm, Sweden\nAlgorithm 1 Million-agent Q-learning (in the case of population\ndynamics in Section 3.2)\n1: Initialize agent’s Q-networkπi , agent’s identity/v.alti .\n2: Randomly initialize the environment s ∼ρ0(S).\n3: for time step=1,2,..., do\n4: Procreate predators/preys osprings with random posi-\ntions.\n5: for agent i=1,2,...,n do\n6: Compute the local observation features O(i).\n7: Compute the identity embedding I(i).\n8: Compute the current state for agent: si\nt = (O(i), I(i)).\n9: Take action ai\nt ∼πi\nθ (ai |si )= ϵ-greedy(Qπ i\n(si\nt , ai\nt )).\n10: Apply action ai\nt , and get reward ri\nt , next state si\nt+1.\nWithin the capture radius of each prey, the group of predators\nmeeting the threshold will become valid candidate, the\ncandidate with largest group size will be the nal winer, and\nthe reward are shared among the group members equally.\n11: Store tuple < si\nt ,ai\nt ,si\nt+1,ri\nt > in the experience buer.\n12: end for\n13: while |B|≥batch size do\n14: Sample a mini-batch from B.\n15: Update the parameters of Q-function w.r.t. the loss:\n16: (ri\nt + γ maxa′∈AQπ (si\nt+1,a′)−Qπ (si\nt ,ai\nt ))2\n17: end while\n18: Clear experience buer B.\n19: Decay the health of predators who starve.\n20: Reward (the group of) predator(s) who win the preys.\n21: Remove the dead predators and preys from the map.\n22: end for\nthe multi-agent Markov decision process (or, stochastic game) is\ndenoted by {S, A, T, R, O, γ, ρ0, N }. Sdenotes the set of true en-\nvironmental states, and ρ0(S)denotes the initial state distribution.\nAt each time step, each agent i ∈{1, ..., N }in the predators popu-\nlation (they have to hunt preys to survive) takes an action ai ∈A\nwhere Ais the valid action space. The joint actions a ∈ AN\ninduce a transition of the environment based on the transition\nfunction between states, T : S×A N →S . The reward func-\ntion is dened by R : S×A N → RN , and γ ∈[0, 1)denotes\nthe discount factor. The environment is partially-observed; each\nagent can observe oi ∈O(s, ai ). An agent gains “intelligence” by\nlearning a stochastic policy πi\nθ (ai |si = oi )that could maximize\nits expected cumulative reward in the predator-prey environment,\ni.e., θ∗:= argmaxθ E(s,a)[/summationtext.1∞\nt=0 γt Ri\nt ]. The action-value function is\ndened by Qπ i\n(st , at )= Est+1:∞,at+1:∞[/summationtext.1∞\nl=0 γl Ri\nt+l |st , at ]. Consid-\nering the exploration in the action space, ϵ-greedy methods can be\napplied on selecting the action, πi\nθ (ai |si )= ϵ-greedy(Qπ i\n(si , ai )).\nThe action space Aincludes {forward, backward, left, right,\nrotate left, rotate right, stand still, join a group, and leave a group}.\nIt is considered as invalid if a predator takes the “join a group”\naction as a group member already, takes the “leave a group” action\nas a single individual, or tries to cross the map boarders. Invalid\nactions will not be settled by the environment. Within the horizon\nof each individual agent, there are ve channels for the observation\noi . The observation Oi\nt ∈Rm×n×5 is dependent on the agent’s\ncurrent position and orientation. The agent’s eyesight ranges up\nto a distance limit towards the grids ahead and the grids to the\nleft and right. The type of object (predators/preys/obstacles/blank\nareas) on the map occupy the rst three channels, which are the\nraw RGB pixels. The fourth channel is an indicator of whether\nor not that object is a group member. The fth channel is the\nhealth status h ∈R if the object is an agent, otherwise padded with\nzero. Each agent is assigned with an unique identity embedding\n/v.alti ∈R5, together with the local observation, it makes up the state\nfor each agent si = (oi , /v.alti ). Individual agent is supposed to make\nindependent decisions, and behave dierently based on its local\nobservation as well as ID embeddings as the inputs of policy πi\nθ .\n4.2 The Implementation\nWe designed a multi-agent reinforcement learning platform with\nenvironmental optimizations in TensorFlow [1] to make the training\nof million-agent learning feasible. To the best of our knowledge,\nwe are the rst1 to introduce the training environment that enables\nsimulating millions of agents driven by deep reinforcement learning\nalgorithms. The demonstration of the platform will be presented\nduring NIPS 2017, for double blind review, we omit the author\ndetails here.\nIn particular, our setting is implemented through \"centralised\ntraining with independent execution\". This is a natural paradigm\nfor a large set of computationally tractable multi-agent problems. In\nthe training stage, agents update the centralised Q-value function\napproximated by a deep neural network:Qπ (si , ai )= Q((oi , /v.alti ), ai ).\nEach individual agent, however, must rely on its local observation\nas well as unique identity to make independent decisions during the\nexecution time. Apart from the standard setting of Q-learning [50]\nand deep Q-learning [32], here we introduce a special experience\nbuer consdiering the GPU eciency as well as mitigating the\nnon-stationary issue in the o-policy learning. At each time step,\nall agents contribute its experienced transitions (si\nt , ai\nt , ri\nt , si\nt+1)to\nthe buer, as shown in Fig. 2. We collect all the agents’ experience\nof one time step in parallel and then update the Q-network using\nthe experience at the same time. This signicantly increases the\nutilization of the GPU memory, and is essential to the million-agent\ntraining. Based on the experience from the buer, the Q-network is\nupdated as:\nQπ (si\nt , ai\nt )←Qπ (si\nt , ai\nt )+α[ri\nt +γ max\na′∈A\nQπ (si\nt+1, a′)−Qπ (si\nt , ai\nt )].\n(1)\nIt is worth mentioning that the experience buer in Fig. 2 stores\nthe experience from the agents only for the current time step; this is\nmarkedly dierent from the replay buer that is commonly used in\nthe traditional DQN where the buer maintains a rst-in-rst-out\nqueue across dierent time steps. Using the o-policy replay buer\nwill typically lead to the non-stationarity issue for the multi-agent\nlearning tasks [29]. On the other hand, Mnih et al. introduced the\nreplay buer aiming at disrupting the auto-correlations between\nthe consecutive examples. In our million-agent RL setting, the ex-\nperiences are sampled concurrently from millions of agents, each\nindividual agent with dierent states and policies; therefore, there\nis naturally no strong auto-correlations between the training ex-\namples. Moreover, it is unlikely that the unwanted feedback loops\n1The Github address of the platform will be presented in the nal version of this paper.\nAAMAS’18, July 2018, Stockholm, Sweden\nYaodong Yang†, Lantao Yu‡, Yiwei Bai‡, Jun Wang†, Weinan Zhang‡, Ying Wen†, Yong Yu‡\nUniversity College London†, Shanghai Jiao Tong University‡\n(a)\n(b)\nFigure 3: Population dynamics in both the time space ( 1st row) and the phase space ( 2nd row). The orange circles denote the\ntheoretical solutions to the Lotka-Volterra equation, with the red spot as the equilibrium. The green-blue circles denote the\nsimulation results. a): The simulated birth rate of preys is 0.006. Fitted LV model: α = 0.0067, β = 3.75 ×10−7, δ = 6.11 ×10−7, γ =\n0.001. b): The simulated birth rate of preys is 0.01. Fitted LV model: α = 0.0086, β = 3.57 ×10−7, δ = 9.47 ×10−7, γ = 0.0012, where\nα in the LV model represents the birth rate.\narise since the sampled experiences will hardly be dominated by\none single agent’s decision. The results further testify the robust-\nness of our design of the experience buer. See Algorithm.1 for\nthe pesudocode of the population dynamics example described in\nSection 3.2.\n5 EXPERIMENTS AND FINDINGS\nTwo sets of experiments – understanding population dynamics &\ncollective behaviors – have been conducted. The environmental\nparameter settings (e.g., the eyesight limit of predators), and the\ncode to reproduce the results with no needs for further adjustments\nwill be released in Supplementary Material in the nal version.\n5.1 Understanding the Population Dynamics\nWe rst study the population dynamics with a community of preda-\ntors and preys by tracking the population size of each species over\ntime. Specically, we initialize 10,000 predators and 5,000 preys\nrandomly scattered over a map of size 1, 000 ×1, 000. All preda-\ntors’ health status is set to 1.0 initially and decays by 0.01 at each\ntime step. In two comparing settings, the birth rates of preys are\nset to 0.006 and 0.01 respectively. The Q-network has two hidden\nlayers, each with 32 hidden units, interleaved with sigmoid non-\nlinear layers, which then project to 9-dimensional outputs, one for\neach potential action. During training, the predators learn in an o-\npolicy reinforcement learning scheme, with exploratory parameter\nϵ = 0.1.\nSurprisingly, we nd that the AI population reveals an ordered\npattern when measuring the population dynamics. As shown in\nFig. 3, the population sizes of both predators and preys reach a\ndynamic equilibrium where both curves present a wax-and-wane\nshape, but with a90◦lag in the phase,i.e., the crest of one is aligned\nwith the trough of the other. The underlying logic of such ordered\ndynamics could be that when the predators’ population grows be-\ncause they learn to know how to hunt eciently, as a consequence\nof more preys being captured, the preys’ population shrinks, which\nwill later cause the predators’ population also shrinks due to the lack\nof food supply, and with the help of less predators, the population\nof preys will recover from the shrinkage and start to regrow. Such\nlogic drives the 2-D contour of population sizes (see the green-blue\ntraits in the 2nd row in Fig. 3) into harmonic cycles, and the circle\npatterns become stable with the increasing level of intelligence\nagents acquire from the reinforcement learning. As it will be shown\nlater in the ablation study, enabling the individual intelligence is the\nkey to observe these ordered patterns in the population dynamics.\nIn fact, the population dynamics possessed by AI agents are\nconsistent with the Lotka-Volterra (LV) model studied in biology\n(shown by the orange traits in Fig. 3). In population biology, the LV\nA Study of AI Population Dynamics with\nMillion-agent Reinforcement Learning AAMAS’18, July 2018, Stockholm, Sweden\n6000 8000 10000 12000 14000 16000 18000\nTimestep\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n40000Number\nPredator number\nPrey-Sheep number\nPrey-Rabbit number\nPredator\n12500150001750020000 22500 25000 27500 30000 32500\nPrey-Sheep500\n1000\n1500\n2000\n2500\n3000\n3500\nPrey-Rabbit\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000\n0\n2000\n4000\n6000\n8000\n10000\n12000\nTimestep\nFigure 4: Population dynamics in the time space and the phase space. A new type of prey (green line) is introduced, which can\nbe captured by a single agent. The AI population shows ordered dynamics in the 3-D phase space.\n0 100 200 300 400 500 600\nTime step\n0\n200000\n400000\n600000\n800000\n1000000Prey Number\nPrey Number\nPredator Number\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000\nPredator Number\nFigure 5: Population dynamics with the learning function of\nAI population disabled. The simulation follows the same set-\nting as Fig. 3(b). No ordered dynamics are found any more.\nmodel [28] describes a Hamiltonian system with two-species inter-\nactions, e.g., predators and preys. In the LV model, the population\nsize of predators q and of preys p change over time based on the\nfollowing pair of nonlinear dierential equations:\n1\np\ndp\ndt = α −βq, 1\nq\ndq\ndt = δp −γ. (2)\nThe preys are assumed to have an auent food resource and thus\ncan reproduce exponentially with rate α, until meeting predation,\nwhich is proportional to the rate at which the predators and the prey\nmeet, represented by βq. The predators have an exponential decay\nin the population due to natural death denoted by γ. Meanwhile,\nthey can also boost the population by hunting the prey, represented\nby δp. The solution to the equations is a harmonic function (wax-\nand-wane shaped) with the population size of predators lagging\nthat of preys by 90◦in the phase. On the phase space plot, it shows\nas a series of periodical circle V = −δp + γ ln(p)−βq + α ln(q),\nwith V dependent on initial conditions. In other words, which\nequilibrium cycle to reach depends on where the ecosystem starts.\nSimilar patterns on the population dynamics might indicate that\nthe orders from an AI population is induced from the same logic as\nthe ecosystem that LV model describes. However, the key dierence\nhere is that, unlike the LV equations that model the observed macro\ndynamics directly, we start from a microcosmic point of view – the\nAI population is only driven by the self-interest (powered by RL)\nof individual agent, and then reaching the macroscopic principles.\nTo further test the robustness of our ndings, we perform an\nablation study on three of the most important factors that we think\nof are critical to the generation of the ordered dynamics. First, we\nanalyze whether the observed pattern is restricted by the specic\nsettings of the predator-prey world. We expose the predator models,\nwhich are trained in the environment where the birth rate of preys\nis 0.006 in Fig. 3(a), into a new environment where the birth rate\nof preys is 0.01. Fig. 3(b) shows that after a period of time for\nadjustment, the predators adapt to the new environment, and the\nAI agents as a whole manage to maintain the patterns. Second,\nwe break the binary predator-prey relationships by introducing\na second type of prey that does not require group hunting. As\nshown in Fig. 4, in the case of three species which the LV model\nmay nd challenging to analyze, we can still observe the ordered\nharmonic circles in 3-D space. Third, we investigate the role of\nindividual intelligence by disabling the learning function in the\nsetting of Fig. 3(b). Fig. 5 shows that the AI population does not\npossess any ordered dynamics anymore if the intelligence of each\nindividual agent is disabled. As such, the whole ecosystem explodes\nwith exponentially-increasing amount of preys and the extinction\nof predators. The reason why predator goes extinct is that the\nincreased birth rate of preys leads to new distributions on the states,\nthus the observations; consequently, the original optimal policy\nof predators becomes suboptimal in the new environment. Given\nthat the number of preys increases exponentially, and the map size\nis limited, the sheep will soon cover all the blank spaces and the\npredators can barely aggregate any valid groups for hunting and\nnally die of starvation.\n5.2 Understanding the Grouping Behaviors\nNext, we investigate the dynamics of the collective grouping behav-\niors. In particular, we intend to nd out the relationship between\nenvironmental food resources and the proportion of the predators\nthat participate in the group hunting, which we refer to as the\n“group proportion”. In the face of two kinds of preys (one requires\ngroup hunting and the other does not), the predators have to make\na decision to either join a group for hunting a sheep or hunt a\nrabbit itself alone. We conduct two experiments with the predator\npopulation size equaling 10 thousands and 2 millions, the map size\nequaling 103 ×103 and 104 ×104 respectively. Acting like a “zoo-\nkeeper”, we supplement the number of preys to a xed amount if\nthe number drops below a certain threshold. For each supplement,\nwe alternate the types of preys to feed in. Suppose the number of\nspecies A is below the threshold, we supply species B. The setting\nof Q-network is the same as in the study on population dynamics.\nAAMAS’18, July 2018, Stockholm, Sweden\nYaodong Yang†, Lantao Yu‡, Yiwei Bai‡, Jun Wang†, Weinan Zhang‡, Ying Wen†, Yong Yu‡\nUniversity College London†, Shanghai Jiao Tong University‡\n15000 15500 16000 16500 17000 17500 18000\nTimestep\n0\n10000\n20000Number\nPrey-Sheep number Prey-Rabbit number \n0.0\n0.2\n0.4\n0.6\nGroup proportion\n ↑\n \n \n \n \n↑ ↑↑↑↑↑↑↑↑↑↑↑\nGroup proportion\n(a)\n0 100 200 300 400 500\nTimestep\n0\n1000000\n2000000Number\nPrey-Sheep number Prey-Rabbit number Group proportion\n0.0\n0.2\n0.4\n0.6\nGroup proportion\n↑ ↑ ↑↑↑ �\n(b)\nFigure 6: a) Grouping proportion in the predator-prey world where two kinds of preys are fed alternatively. ↑points out the\ntime step that preys are fed. It tells that when the number of the prey sheep (that requires group hunting) increases, the\nproportion of groups in AI population increases, and adapting to grouping becomes collective behaviors. Vice verse to the\ncase when the prey rabbit are fed. b) The same experiment on two-million AI population.\nAs the preys are alternatively fed, the predator’s policy needs\nto react correspondingly to the new environment so as to survive.\nAs shown in Fig. 6(a) and 6(b), the moment right after the rabbits\nare fed into the environment, the proportion of groups drastically\ndrop down to nearly 0. Predators collectively behave to be selsh\nrather than to be altruistic to the group. With the number of rabbits\nbeing captured, the proportion of grouping behaviors increases\nmildly again, and meets a spike soon after the sheep are fed into\nthe environment, and reaches another dynamic equilibrium. In a\nhighly-variable environment, the population of predators show\nthe intelligence of adapting their hunting strategies collectively\nwithout any external supervisions or controls.\n5.3 Discussions\nJudging from the ordered patterns of the AI population in the\npredator-prey world, we have reasons to agree with Lucretius that\na designing intelligence is necessary to create orders in nature. In\nfact, in understanding the emergence of orders in a system, the\ntheory of self-organization proposed in [2] considers that the global\nordered dynamics of a system can spontaneously originate from\nnumerous interactions between local individuals that are initially\ndisordered, with no needs of external interventions. The theory\npredicts the existence of the ordered dynamics from numerous local\ninteractions between the individuals and the system. This could\npotentially explain the ordered patterns observed on our AI popu-\nlation that has been tested. Meanwhile, according to the theory, the\ncreated order is independent of the complexity of the individual\ninvolved. For example, the Lotka-Volterra dynamics also hold for\nother natural systems such as the herbivore and the plants, or the\nparasite and the host. Even though the LV models are based on a\nset of equations with xed interaction terms, while our ndings\ndepend on intelligent agents driven by consistent learning process,\nthe generalization of the resulting dynamics onto an AI population\nstill leads us to imagine a general law that could unify the articially\ncreated agents with the population we have studied in the natural\nsciences for long time.\nArguably, in contrast to theself-organization theory, reductionist\nscientists hold a dierent view that order can only be created by\ntransferring it from external systems. A typical example is The\nSecond Law of Thermodynamics [5] stating that the total entropy\n(the level of disorder) will always increase over time in a closed\nsystem. Such an idea has widely been accepted, particularly in\nphysics where quantitative analysis is feasible. However, we argue\nthat our ndings from the AI population do not go against this law.\nRL-based agents are not exceptions simply because the environment\nthey “live\" in are notclosed. Whenever a system can exchange matter\nwith its environment, an entropy decrease of that system (orders\nemerge) is still compatible with the second law. A further discussion\non entropy and life [41] certainly goes beyond this topic, and we\nleave it for future work.\n6 CONCLUSIONS\nWe conducted an empirical study on an AI population by simulating\na predator-prey world where each individual agent was empowered\nby deep reinforcement learning, and the number of agents is up to\nmillions. We found that the AI population possessed the ordered\nA Study of AI Population Dynamics with\nMillion-agent Reinforcement Learning AAMAS’18, July 2018, Stockholm, Sweden\npopulation dynamics consistent with the Lotka-Volterra model in\necology. We also discovered the emergent collective adaptations\nwhen the environmental resources changed over time. Importantly,\nboth of the ndings could be well explained by theself-organization\ntheory from natural sciences.\nIn the future, we will conduct further experiments on our million-\nagent RL platform by involving the ideas of leadership, cannibalism,\nand irrationality for discovering other profound natural principles\nin the full deep-RL-driven population. In return, we expect our\nndings could also enlighten an interesting research direction of\ninterpreting the RL-based AI population using the natural science\nprinciples developed in the real world, and apply the AI population\ndriven by RL for applications like smart cities or swarm intelligence.\nREFERENCES\n[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,\nCraig Citro, Greg S Corrado, Andy Davis, Jerey Dean, Matthieu Devin, et al .\n2016. Tensorow: Large-scale machine learning on heterogeneous distributed\nsystems. arXiv preprint arXiv:1603.04467 (2016).\n[2] W Ross Ashby. 1991. Principles of the self-organizing system. InFacets of Systems\nScience. Springer, 521–536.\n[3] Per Bak. 2013. How nature works: the science of self-organized criticality . Springer\nScience & Business Media.\n[4] Stefano Boccaletti, Vito Latora, Yamir Moreno, Martin Chavez, and D-U Hwang.\n2006. Complex networks: Structure and dynamics. Physics reports 424, 4 (2006),\n175–308.\n[5] Ludwig Boltzmann. 1974. The second law of thermodynamics. In Theoretical\nphysics and philosophical problems . Springer, 13–32.\n[6] Eric Bonabeau, Guy Theraulaz, Jean-Louls Deneubourg, Serge Aron, and Scott\nCamazine. 1997. Self-organization in social insects. Trends in Ecology & Evolution\n12, 5 (1997), 188–193.\n[7] Scott Camazine. 2003.Self-organization in biological systems . Princeton University\nPress.\n[8] Harm de Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo Larochelle,\nand Aaron Courville. 2016. GuessWhat?! Visual object discovery through multi-\nmodal dialogue. (nov 2016). arXiv:1611.08481 http://arxiv.org/abs/1611.08481\n[9] Sam Devlin and Daniel Kudenko. 2011. Theoretical considerations of potential-\nbased reward shaping for multi-agent systems. In The 10th International Con-\nference on Autonomous Agents and Multiagent Systems-Volume 1 . International\nFoundation for Autonomous Agents and Multiagent Systems, 225–232.\n[10] Sam Devlin, Logan Yliniemi, Daniel Kudenko, and Kagan Tumer. 2014. Potential-\nbased dierence rewards for multiagent reinforcement learning. InProceedings of\nthe 2014 international conference on Autonomous agents and multi-agent systems .\nInternational Foundation for Autonomous Agents and Multiagent Systems, 165–\n172.\n[11] RIM Dunbar. 2016. Group size, vocal grooming and the origins of language.\nPsychonomic Bulletin & Review (2016), 1–4.\n[12] Jakob Foerster, Yannis M Assael, Nando de Freitas, and Shimon Whiteson. 2016.\nLearning to communicate with deep multi-agent reinforcement learning. In\nAdvances in Neural Information Processing Systems . 2137–2145.\n[13] Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Philip Torr, Pushmeet Kohli,\nShimon Whiteson, et al. 2017. Stabilising Experience Replay for Deep Multi-Agent\nReinforcement Learning. arXiv preprint arXiv:1702.08887 (2017).\n[14] John M Fryxell, Anna Mosser, Anthony RE Sinclair, and Craig Packer. 2007.\nGroup formation stabilizes predator–prey dynamics. Nature 449, 7165 (2007),\n1041–1043.\n[15] Michael E Gilpin. 1973. Do hares eat lynx? The American Naturalist 107, 957\n(1973), 727–730.\n[16] Simon Goss, Serge Aron, Jean-Louis Deneubourg, and Jacques Marie Pasteels.\n1989. Self-organized shortcuts in the Argentine ant. Naturwissenschaften 76, 12\n(1989), 579–581.\n[17] Mauro F Guillen. 2000. Business groups in emerging economies: A resource-based\nview. academy of Management Journal 43, 3 (2000), 362–380.\n[18] Junling Hu and Michael P Wellman. 2003. Nash Q-learning for general-sum\nstochastic games. Journal of Machine learning research 4, Nov (2003), 1039–1069.\n[19] Junling Hu, Michael P Wellman, et al. 1998. Multiagent reinforcement learning:\ntheoretical framework and an algorithm.. In ICML, Vol. 98. Citeseer, 242–250.\n[20] Yoshinobu Inada and Keiji Kawachi. 2002. Order and exibility in the motion of\nsh schools. Journal of theoretical Biology 214, 3 (2002), 371–387.\n[21] Marco Alberto Javarone and Daniele Marinazzo. 2016. Evolutionary Dynamics\nof Group Formation. arXiv preprint arXiv:1612.03834 (2016).\n[22] Robert L Jeanne. 1988. Interindividual behavioral variability in social insects .\nWestview Press.\n[23] Stuart A Kauman. 1993. The origins of order: Self-organization and selection in\nevolution. Oxford University Press, USA.\n[24] James Kennedy. 2006. Swarm intelligence. In Handbook of nature-inspired and\ninnovative computing. Springer, 187–219.\n[25] Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. 2016. Multi-\nAgent Cooperation and the Emergence of (Natural) Language. CoRR\nabs/1612.07182 (2016). http://arxiv.org/abs/1612.07182\n[26] Joel Z Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and Thore Graepel.\n2017. Multi-agent Reinforcement Learning in Sequential Social Dilemmas. arXiv\npreprint arXiv:1702.03037 (2017).\n[27] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez,\nYuval Tassa, David Silver, and Daan Wierstra. 2015. Continuous control with\ndeep reinforcement learning. arXiv preprint arXiv:1509.02971 (2015).\n[28] Alfred J Lotka. 1925. Elements of physical biology. (1925).\n[29] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. 2017.\nMulti-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.\narXiv preprint arXiv:1706.02275 (2017).\n[30] Kleanthis Malialis, Sam Devlin, and Daniel Kudenko. 2016. Resource Abstraction\nfor Reinforcement Learning in Multiagent Congestion Problems. In Proceedings\nof the 2016 International Conference on Autonomous Agents &#38; Multiagent\nSystems (AAMAS ’16) . International Foundation for Autonomous Agents and\nMultiagent Systems, Richland, SC, 503–511. http://dl.acm.org/citation.cfm?id=\n2936924.2937000\n[31] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timo-\nthy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchro-\nnous methods for deep reinforcement learning. In International Conference on\nMachine Learning . 1928–1937.\n[32] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis\nAntonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing atari with deep\nreinforcement learning. arXiv preprint arXiv:1312.5602 (2013).\n[33] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,\nMarc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg\nOstrovski, et al. 2015. Human-level control through deep reinforcement learning.\nNature 518, 7540 (2015), 529–533.\n[34] Igor Mordatch and Pieter Abbeel. 2017. Emergence of Grounded Compositional\nLanguage in Multi-Agent Populations. (mar 2017). arXiv:1703.04908 http://arxiv.\norg/abs/1703.04908\n[35] Taewoo Nam and Theresa A Pardo. 2011. Conceptualizing smart city with\ndimensions of technology, people, and institutions. In Proceedings of the 12th\nannual international digital government research conference: digital government\ninnovation in challenging times . ACM, 282–291.\n[36] Zoltán Néda, Erzsébet Ravasz, Tamás Vicsek, Yves Brechet, and Albert-Lázló\nBarabási. 2000. Physics of the rhythmic applause. Physical Review E 61, 6 (2000),\n6987.\n[37] Hiro-Sato Niwa. 1994. Self-organizing dynamic model of sh schooling. Journal\nof theoretical Biology 171, 2 (1994), 123–136.\n[38] Ada Palmer. 2014.Reading Lucretius in the Renaissance . Vol. 16. Harvard University\nPress.\n[39] Tanya Pankiw and Robert E Page Jr. 2000. Response thresholds to sucrose predict\nforaging division of labor in honeybees. Behavioral Ecology and Sociobiology 47,\n4 (2000), 265–267.\n[40] Peng Peng, Quan Yuan, Ying Wen, Yaodong Yang, Zhenkun Tang, Haitao Long,\nand Jun Wang. 2017. Multiagent Bidirectionally-Coordinated Nets for Learning\nto Play StarCraft Combat Games. arXiv preprint arXiv:1703.10069 (2017).\n[41] Erwin Schrodinger. 1943. What is life? University Press: Cambridge.\n[42] Thomas D Seeley. 2009. The wisdom of the hive: the social physiology of honey bee\ncolonies. Harvard University Press.\n[43] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George\nVan Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershel-\nvam, Marc Lanctot, et al . 2016. Mastering the game of Go with deep neural\nnetworks and tree search. Nature 529, 7587 (2016), 484–489.\n[44] Sainbayar Sukhbaatar, Rob Fergus, et al. 2016. Learning multiagent communica-\ntion with backpropagation. In Advances in Neural Information Processing Systems .\n2244–2252.\n[45] David JT Sumpter. 2006. The principles of collective animal behaviour.Philosoph-\nical Transactions of the Royal Society of London B: Biological Sciences 361, 1465\n(2006), 5–22.\n[46] Richard S Sutton and Andrew G Barto. 1998. Reinforcement learning: An intro-\nduction. Vol. 1. MIT press Cambridge.\n[47] Randy Thornhill. 1976. Sexual selection and paternal investment in insects. The\nAmerican Naturalist 110, 971 (1976), 153–163.\n[48] Nicolas Usunier, Gabriel Synnaeve, Zeming Lin, and Soumith Chintala. 2016.\nEpisodic Exploration for Deep Deterministic Policies: An Application to StarCraft\nMicromanagement Tasks. arXiv preprint arXiv:1609.02993 (2016).\n[49] Sida I Wang, Percy Liang, and Christopher D Manning. 2016. Learning Language\nGames through Interaction. arXiv preprint arXiv:1606.02447 (2016).\nAAMAS’18, July 2018, Stockholm, Sweden\nYaodong Yang†, Lantao Yu‡, Yiwei Bai‡, Jun Wang†, Weinan Zhang‡, Ying Wen†, Yong Yu‡\nUniversity College London†, Shanghai Jiao Tong University‡\n[50] Christopher JCH Watkins and Peter Dayan. 1992. Q-learning. Machine learning\n8, 3-4 (1992), 279–292.\n[51] Anja Weidenmüller. 2004. The control of nest climate in bumblebee (Bombus\nterrestris) colonies: interindividual variability and self reinforcement in fanning\nresponse. Behavioral Ecology 15, 1 (2004), 120–128.\n[52] Edward O Wilson et al. 1971. The insect societies. The insect societies. (1971).\n[53] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. SeqGAN: Sequence\nGenerative Adversarial Nets with Policy Gradient.. In AAAI. 2852–2858.\n[54] Barret Zoph and Quoc V Le. 2016. Neural architecture search with reinforcement\nlearning. arXiv preprint arXiv:1611.01578 (2016).",
  "topic": "Reinforcement learning",
  "concepts": [
    {
      "name": "Reinforcement learning",
      "score": 0.7656484246253967
    },
    {
      "name": "Population",
      "score": 0.6919217705726624
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5798572301864624
    },
    {
      "name": "Computer science",
      "score": 0.5606379508972168
    },
    {
      "name": "Scale (ratio)",
      "score": 0.5137323141098022
    },
    {
      "name": "Context (archaeology)",
      "score": 0.48711276054382324
    },
    {
      "name": "Agent-based model",
      "score": 0.43688151240348816
    },
    {
      "name": "Collective behavior",
      "score": 0.42949387431144714
    },
    {
      "name": "Order (exchange)",
      "score": 0.41237208247184753
    },
    {
      "name": "Geography",
      "score": 0.18029358983039856
    },
    {
      "name": "Sociology",
      "score": 0.08245012164115906
    },
    {
      "name": "Cartography",
      "score": 0.07962417602539062
    },
    {
      "name": "Business",
      "score": 0.06744757294654846
    },
    {
      "name": "Anthropology",
      "score": 0.0
    },
    {
      "name": "Demography",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Finance",
      "score": 0.0
    }
  ]
}