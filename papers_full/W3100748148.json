{
    "title": "Probabilistic Predictions of People Perusing: Evaluating Metrics of Language Model Performance for Psycholinguistic Modeling",
    "url": "https://openalex.org/W3100748148",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2892220716",
            "name": "Yiding Hao",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A2174787775",
            "name": "Simon Mendelsohn",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A3084236399",
            "name": "Rachel Sterneck",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A3044184390",
            "name": "Randi Martinez",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A2103130191",
            "name": "Robert Frank",
            "affiliations": [
                "Yale University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3027353876",
        "https://openalex.org/W2952729433",
        "https://openalex.org/W2270070752",
        "https://openalex.org/W1575784839",
        "https://openalex.org/W2106850936",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2578785875",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W2131241448",
        "https://openalex.org/W2977268464",
        "https://openalex.org/W3033254023",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2899771611",
        "https://openalex.org/W2009499611",
        "https://openalex.org/W1486038082",
        "https://openalex.org/W2100719658",
        "https://openalex.org/W2139450036",
        "https://openalex.org/W2141440284",
        "https://openalex.org/W3168987555",
        "https://openalex.org/W2596272155",
        "https://openalex.org/W3118485687",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W1579838312",
        "https://openalex.org/W2067575282",
        "https://openalex.org/W2795342569",
        "https://openalex.org/W2108010971",
        "https://openalex.org/W2963494889",
        "https://openalex.org/W2611669587",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2082283091",
        "https://openalex.org/W2135982437",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2164418233",
        "https://openalex.org/W2185726469",
        "https://openalex.org/W138474712",
        "https://openalex.org/W2982442115",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2054125330",
        "https://openalex.org/W2076196178",
        "https://openalex.org/W3034510440",
        "https://openalex.org/W1996359725",
        "https://openalex.org/W2970713124",
        "https://openalex.org/W1539309091",
        "https://openalex.org/W2134800885",
        "https://openalex.org/W2057458773"
    ],
    "abstract": "By positing a relationship between naturalistic reading times and information-theoretic surprisal, surprisal theory (Hale, 2001; Levy, 2008) provides a natural interface between language models and psycholinguistic models. This paper re-evaluates a claim due to Goodkind and Bicknell (2018) that a language model’s ability to model reading times is a linear function of its perplexity. By extending Goodkind and Bicknell’s analysis to modern neural architectures, we show that the proposed relation does not always hold for Long Short-Term Memory networks, Transformers, and pre-trained models. We introduce an alternate measure of language modeling performance called predictability norm correlation based on Cloze probabilities measured from human subjects. Our new metric yields a more robust relationship between language model quality and psycholinguistic modeling performance that allows for comparison between models with different training configurations.",
    "full_text": "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 75–86\nOnline Event, November 19, 2020.c⃝2020 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17\n75\nProbabilistic Predictions of People Perusing: Evaluating Metrics of\nLanguage Model Performance for Psycholinguistic Modeling\nYiding Hao, Simon Mendelsohn, Rachel Sterneck,\nRandi Martinez, and Robert Frank\nYale University, New Haven, CT, USA\nfirstname.lastname@yale.edu\nAbstract\nBy positing a relationship between natural-\nistic reading times and information-theoretic\nsurprisal, surprisal theory (Hale, 2001; Levy,\n2008) provides a natural interface between\nlanguage models and psycholinguistic mod-\nels. This paper re-evaluates a claim due to\nGoodkind and Bicknell (2018) that a language\nmodel’s ability to model reading times is a\nlinear function of its perplexity. By extend-\ning Goodkind and Bicknell’s analysis to mod-\nern neural architectures, we show that the pro-\nposed relation does not always hold for Long\nShort-Term Memory networks, Transformers,\nand pre-trained models. We introduce an al-\nternate measure of language modeling perfor-\nmance called predictability norm correlation\nbased on Cloze probabilities measured from\nhuman subjects. Our new metric yields a more\nrobust relationship between language model\nquality and psycholinguistic modeling perfor-\nmance that allows for comparison between\nmodels with different training conﬁgurations.\n1 Introduction\nNaturalistic reading times are known to be affected\nby incongruities between the current word and the\ncontext created by preceding words (Zola, 1981,\n1984; Ehrlich and Rayner, 1981, inter alia). Words\nthat are unexpected within their contexts are ﬁx-\nated for longer periods, while predictable words\nare ﬁxated for a shorter amount of time or skipped\naltogether. This observation has been described\nformally by the suprisal theory of sentence process-\ning (Hale, 2001; Levy, 2008), which posits that the\ndifﬁculty of processing a particular word is directly\nproportional to its surprisal, deﬁned as the nega-\ntive logarithm of its probability given its preceding\ncontext.\nBy relating psychometric observations with\ninformation-theoretic concepts, surprisal theory\nprovides a natural bridge between psycholinguistic\nmodeling on the one hand and language modeling\non the other. While modeling studies for read-\ning times traditionally use surprisals obtained from\nprobabilistic parsers (Hale, 2001; Demberg and\nKeller, 2008; Roark et al., 2009) or directly from\nprobabilistic context-free grammars (Levy, 2008;\nBoston et al., 2008), surprisals fromn-gram models\n(Mitchell et al., 2010; Smith and Levy, 2013) and\nsimple recurrent network language models (Frank,\n2009; Monsalve et al., 2012) have also been in-\ncorporated into reading-time studies. More recent\nwork has sought to leverage the advances in lan-\nguage modeling made possible by neural NLP in\norder to determine whether modern techniques can\nyield more reliable estimates of surprisal for cog-\nnitive modeling. These studies have investigated\nthe psycholinguistic capabilities of several neural\narchitectures, including Long Short-Term Memory\nnetworks and Gated Recurrent Unit networks (Au-\nrnhammer and Frank, 2019), Transformers (Merkx\nand Frank, 2020), and GPT-2 (Wilcox et al., 2020).\nIn general, language models achieving a better per-\nplexity have been found to yield better psycholin-\nguistic models via the surprisal estimates they fur-\nnish, with Goodkind and Bicknell (2018) proposing\na linear relationship between the two factors.\nThis paper revisits Goodkind and Bicknell’s pro-\nposed linear relationship between perplexity and\npsycholinguistic modeling performance. We ad-\ndress two drawbacks of their analysis, which hinder\nits applicability to the state of the art in language\nmodeling. Firstly, Goodkind and Bicknell only con-\nsider n-gram models and a small LSTM language\nmodel with 50 hidden units. These constitute a\nsmall and relatively weak collection of language\nmodels compared to the diverse array of techniques\nand vast computing resources that are available\ntoday. Secondly, the reliability of perplexity as\na measure of language modeling performance is\nhighly dependent on training conﬁgurations, such\n76\nthat only perplexities for models with the same vo-\ncabulary are comparable (see Jurafsky and Martin,\n2008, pp. 95–97). This is especially problematic\ngiven the rise of large-scale, general-purpose pre-\ntrained language models, since vocabulary cannot\nbe controlled for when comparing different pre-\ntrained models.\nTo address these issues, we ﬁrst expand Good-\nkind and Bicknell’s empirical coverage by extend-\ning their analysis to Long Short-Term Memory\nnetworks (LSTMs, Hochreiter and Schmidhuber,\n1997) and Transformers (Vaswani et al., 2017) of\nvarying sizes and training corpora, along with four\nlarge pre-trained models. We then propose an al-\nternate metric of language modeling performance\nbased on word probabilities obtained from humans\nusing a Cloze procedure (Taylor, 1953), which al-\nlows different pre-trained models to be compared\nwith one another despite differences in vocabulary.\nIn line with previous work, we ﬁnd that the rela-\ntionship between perplexity and psycholinguistic\nmodeling performance is weaker for LSTMs and\nTransformers than for n-gram models, and not con-\nsistent across vocabularies. However, our Cloze-\nbased metric yields a relationship which is robust\nto differences in model vocabulary and incorpo-\nrates both our trained and our pre-trained models.\nBased on this, we argue that our Cloze-based met-\nric is more suitable than perplexity for evaluating\npre-trained models, and better reveals the relation-\nship between language modeling performance and\npsycholinguistic modeling performance.\nThe structure of this paper is as follows. Sec-\ntion 2 describes our procedure for modeling read-\ning times, and Section 3 describes our language\nmodels as well as the Cloze-based performance\nmetric. Our results are presented in Section 4 and\ndiscussed in Section 5. Section 6 concludes.\n2 Psycholinguistic Modeling\nWe follow the methodology of Goodkind and Bick-\nnell (2018) for psycholinguistic modeling. We\nmodel reading times using generalized additive\nmixed-effect models (GAMMs), as implemented in\nthe mgcv R package (Wood, 2004). Our GAMMs\ntake several variables as input, optionally including\nlanguage model surprisals. We ﬁt a GAMM for\neach language model, along with a baseline model\nthat does not include surprisal values. We mea-\nsure the psycholinguistic modeling performance of\neach model by a score called delta log likelihood\n(∆LogLik), deﬁned as the difference in log likeli-\nhood between the model’s corresponding GAMM\nand the baseline GAMM.\nIn the following subsections, we describe the\nreading time data used in our study and our proce-\ndure for ﬁtting GAMMs.\n2.1 Eyetracking Data\nThe data for our psycholinguistic models come\nfrom the English portion of the Dundee Corpus\n(Kennedy, 2003; Kennedy et al., 2003), a dataset\ncontaining naturalistic reading times from an eye-\ntracking study. The data were elicited from ten\nEnglish-speaking participants reading editorials\nfrom British newspaper The Independent. The edi-\ntorials were divided into several texts, which were\nshown to subjects on a screen one at a time. Read-\ning times were obtained for a total of 56,212 tokens,\ndrawn from a vocabulary of 9,776 unique word\ntypes, excluding punctuation. Punctuation marks\nwere not treated as separate tokens, but rather as\nbelonging to the words they are attached to.\n2.2 GAMMs\nFollowing Goodkind and Bicknell (2018) and\nSmith and Levy (2013), our GAMMs consist of\nthe following input terms:\n•linear terms for the surprisal of the current and\nprevious word;\n•tensor product interactions 1 between word\nlength and log (unigram) frequency for the\ncurrent and previous word;\n•a spline2 term for the current word’s position\nwithin a text; and\n•a binary term representing whether or not the\nprevious word was ﬁxated.\nThe linear terms for surprisal are excluded from\nthe baseline GAMM. Unigram frequencies were\nestimated using the One Billion Word Benchmark\n(LM1B, Chelba et al., 2014). We use the same\npreprocessing procedure as Goodkind and Bick-\nnell, which removes from the data the ﬁrst and last\nwords of each text, words preceding punctuation\n1This is a 2-dimentional tensor spline interaction.\n2Penalized spline regression uses a high-dimensional\nspline basis to estimate unknown non-linear relations. In\norder to avoid the over-ﬁtting that otherwise plagues such\nhigh-dimensional models, it combines the standard maximum\nlikelihood criterion with a curvature penalty term that biases\nthe regression towards smoother curves.\n77\nmarks, words containing non-alphabetic characters,\nwords for which no unigram frequency estimate\nis available, and words following these words. In\ncases where a word from the Dundee Corpus is to-\nkenized into multiple tokens by a language model,\nwe take the word’s surprisal to be the sum of the\nsurprisals for each of the constituent tokens.\nGoodkind and Bicknell’s GAMM outputs rep-\nresent predictions for gaze duration (GD), deﬁned\nas the time elapsed between the ﬁrst ﬁxation on\na word token and the ﬁrst time the subject exits\nthat token. In addition to gaze duration, we train\nGAMMs to predict two other eyetracking measures:\nﬁrst ﬁxation duration (FFD), deﬁned as the time\nelapsed during the ﬁrst ﬁxation on a token, and\ntotal duration (TD), deﬁned as the total amount of\ntime spent looking at a token.\n3 Language Modeling\nTraditionally, language models form independent\ncomponents of NLP systems, their outputs serving\nas inputs to downstream applications. However,\nfollowing recent advances in transfer learning for\nNLP, modern neural language models are often de-\nsigned not to compute word probabilities per se,\nbut rather to provide representations of linguistic\nknowledge that can facilitate training on other tasks\n(Dai and Le, 2015; Howard and Ruder, 2018). Un-\nder this paradigm, a single large neural network is\npre-trained on a language modeling or other word\nprediction objective. This pre-trained language\nmodel can then be trained, or ﬁne-tuned, on an-\nother task such as text classiﬁcation (Yang et al.,\n2019) or machine translation (Conneau and Lam-\nple, 2019). Both kinds of language models are\nconsidered in this paper.\n3.1 Traditional Language Models\nWe consider three types of traditional language\nmodels: n-gram models, LSTM language models,\nand Transformer language models. We train four\nmodels of each type, with varying numbers of pa-\nrameters, on the Penn Treebank corpus (Mikolov\net al., 2011) and the WikiText-2 corpus (Merity\net al., 2016). We additionally train n-gram mod-\nels on LM1B in order to reproduce and facilitate\ncomparison with Goodkind and Bicknell’s (2018)\nresults.\nFollowing Goodkind and Bicknell (2018), our n-\ngram models were trained using modiﬁed Kneser–\nNey smoothing with KenLM (Heaﬁeld, 2011;\nModel Penn Treebank WikiText-2\n2-Gram 185.7 248.5\n3-Gram 148.3 211.1\n4-Gram 142.7 206.0\n5-Gram 141.2 204.8\nLSTM-200 89.3 106.0\nLSTM-425 85.1 98.5\nLSTM-650 89.8 98.8\nLSTM-1100 97.9 110.5\nTransformer-200 117.8 150.2\nTransformer-424 119.1 151.5\nTransformer-650 118.7 154.7\nTransformer-1100 119.7 156.6\nTable 1: Perplexities attained by our trained models on\nthe Penn Treebank and WikiText-2 testing sets.\nHeaﬁeld et al., 2013). We trained n-gram mod-\nels with n = 2, 3, 4, and 5 on each dataset. We\nadditionally trained a unigram model in order to\npopulate the word frequency term in the input to\nthe GAMMs.\nOur LSTM and Transformer models are based\non an implementation publicly available on the\nofﬁcial PyTorch website (Paszke et al., 2017). 3\nBoth models include an embedding layer and a\nsoftmax decoding layer. The LSTM model con-\nsists of two LSTM layers, while the Transformer\nmodel consists of two encoder blocks with masked\nself-attention, each containing two attention heads.\nWe trained LSTM models with 200, 425, 650, and\n1100 hidden units (LSTM-200, -425, -650, and\n-1100, respectively), as well as Transformer mod-\nels with 200, 424, 4 650, and 1100 hidden units\n(Transformer-200, -424, -650, and -1100, respec-\ntively). The embedding size used by each model\nwas equal to its hidden size. For each architec-\nture, training corpus, and hidden size, we trained a\nmodel using SGD with the learning rate annealed\nby .5 when perplexity does not improve on a val-\nidation set. Training occurred for a maximum\nof 50 epochs, stopping early with a patience of\n5. The batch size, dropout rate, and initial learn-\ning rate were tuned using a Bayesian optimization\nroutine consisting of 7 random trials followed by\n13 GPEI trials (Snoek et al., 2012). The LSTM-\n1100 and Transformer-1100 models were trained\nusing the best hyperparameters for LSTM-650 and\nTransformer-650, respectively. The testing perplex-\nities attained by our trained models are shown in\n3https://github.com/pytorch/examples/\ntree/master/word_language_model\n4Because our Transformer model has two attention heads,\nit needs to have an even number of hidden units, so we used\n424 instead of 425 (like we used in the LSTM).\n78\nTable 1.\n3.2 Pre-Trained Models\nIn addition to our traditional language models, we\ninclude four pre-trained models in our analysis:\nGPT-2 (Radford et al., 2019), XLM (Conneau and\nLample, 2019), Transformer-XL (Dai et al., 2019),\nand XLNet (Yang et al., 2019). All four models are\nvariants of Transformer language models but differ\nfrom one another in terms of training setup and\narchitectural details. We use the implementations\nfrom Hugging Face’sTransformers library (Wolf\net al., 2020) off the shelf, with no ﬁne-tuning. We\nbrieﬂy describe the distinguishing features of each\nmodel below.\n3.2.1 GPT-2\nGPT-2 is a Transformer language model trained on\na large corpus called WebText, which was designed\nto include a diverse array of documents in order to\ncapture domain-general linguistic knowledge. The\ndataset consists of roughly 8 million webpages ob-\ntained from Reddit links. It demonstrates state of\nthe art perplexities on various language modeling\ntesting sets, as well as a notable ability to gener-\nate human-like text, especially in the context of\nsummarization. Radford et al. (2019) present GPT-\n2 models in four sizes; we use the smallest size,\nwhich consists of 12 layers, 12 attention heads,\nand 768 hidden units. GPT-2’s vocabulary uses a\nbyte-pair encoding, with 50,257 unique word and\nsub-word types.\n3.2.2 XLM\nXLM is similar in approach to GPT-2, but it is\nspeciﬁcally designed for cross-lingual language\ntasks, including multilingual classiﬁcation and ma-\nchine translation. The XLM model we use was\ntrained on a combination of English and German\nWikipedia entries. It has 6 layers, 1024 hidden\nunits, and 8 attention heads. XLM also uses a byte-\npair encoding in its vocabulary, with 64,699 unique\nword and subword types. Unlike the other mod-\nels, however, XLM’s vocabulary includes German\ntokens in addition to English tokens.\n3.2.3 Transformer-XL\nThe Transformer-XL model introduces architec-\ntural enhancements to the Transformer that facil-\nitate learning of long-distance dependencies. It\ndoes this using relative encodings and a segment\nrecurrence mechanism, which augments the Trans-\nformer with elements of RNN language models.\nOur Transformer-XL model was trained on the\nWikiText-103 corpus (Merity et al., 2016), and con-\ntains 18 layers, 1024 hidden units, and 16 attention\nheads.\n3.2.4 XLNet\nFinally, XLNet is a language model that extends\nTransformer-XL to account for bidirectional con-\ntext within a language modeling setting. This is\nachieved using a generalized autoregressive tech-\nnique, in which words are predicted in a random-\nized order for each training batch. We use an XL-\nNet model with 12 layers, 768 hidden units, and 12\nattention heads, trained on a custom corpus drawn\nfrom various sources.\n3.3 Language Model Evaluation\nWe employ two methods for evaluating the outputs\nof our language models: perplexity, the standard\nevaluation metric for language modeling, and pre-\ndictability norm correlation, our proposed metric\nfor comparing models with different vocabularies.\n3.3.1 Perplexity\nFor a given language model, the perplexity of a text\nconsisting of N tokens is deﬁned by the formula\nperplexity =\n(N∏\ni=1\nP(tokeni|tokensj<i)\n)−1\nN\nwhere P(tokeni|tokensj<i) is the probability as-\nsigned to the ith token after the model has pro-\ncessed the ﬁrst i−1 tokens. Perplexity can also be\ndeﬁned as the exponential of the average surprisal\nof the text.\nperplexity = e−1\nN\n∑N\ni=1 ln(P(tokeni|tokensj<i))\nIntuitively, perplexity may be interpreted as the\nweighted average number of possibilities the lan-\nguage model chooses between when predicting the\nwords in the text. Lower perplexities indicate better\nlanguage modeling performance, since the model is\nless uncertain about its predictions. Note that using\na larger vocabulary artiﬁcially increases perplexity,\nsince the model automatically has more words to\nchoose from, thus decreasing P(tokeni|tokensj<i)\non average.\nIn this study, we calculate the perplexity of each\nlanguage model on the entire Dundee Corpus, with-\nout the preprocessing of Subsection 2.2. All per-\nplexity calculations are based on the tokenization\n79\nused in Goodkind and Bicknell (2018), which di-\nvides the Dundee Corpus into N = 60,916 tokens.\nThis produces slightly more tokens than the orig-\ninal tokenization used by Kennedy (2003), since\npunctuation marks are treated as separate tokens,\nfollowing common practice in language modeling.\n3.3.2 Predictability Norm Correlation\nAs an alternative to perplexity, we evaluate mod-\nels according to a predictability norm correlation\nscore (PNC), deﬁned as the Pearson correlation\nbetween surprisal values computed by a language\nmodel and surprisal values measured from human\nsubjects using a Cloze task. The data used to calcu-\nlate PNC come from predictability norms collected\nby Kennedy et al. (2013) for a 16-sentence subset\nof the Dundee Corpus. Each subject was shown a\nrandom and possibly empty initial segment of each\nsentence and asked to predict the next word, typing\ntheir response into a computer. Predictions were\ncollected from 272 subjects in total, with roughly\n25 predictions for each token. The “human” prob-\nability values are deﬁned by the proportion of re-\nsponses for each token that represent correct pre-\ndictions. Kennedy et al. (2013) report two sets of\nhuman probability values: one that counts minor\nmisspellings of the target word as correct predic-\ntions, and one that counts them as incorrect. We\nuse the former set of scores.\n4 Results\nOur results are presented in Table 2 and Figure 1,\nwhich show the same data in tabular and graph-\nical form. Overall, GPT-2 outperforms all other\nmodels on all metrics, achieving the best perplex-\nity, PNC, and ∆LogLik. Transformer-XL, XL-\nNet, and the n-gram models trained on LM1B per-\nform signiﬁcantly better in terms of ∆LogLik than\nXLM and the models trained on Penn Treebank and\nWikiText-2. This may be because Penn Treebank\nand WikiText-2 are signiﬁcantly smaller than the\nother training datasets, and because XLM, being\na multilingual model, is not as suited to model-\ning data from native English speakers as the other\nmodels.\nThe models trained on Penn Treebank and\nWikiText-2 achieve similar levels of ∆LogLik,\nthough the Penn Treebank models generally have\na better perplexity while the WikiText-2 models\ngenerally have a better PNC. Between LSTMs\nand Transformers, neither architecture is consis-\ntently better than the other in terms of ∆LogLik:\nModel PPL PNC ∆LogLik\nFFD GD TD\nPRE-TRAINED\nGPT-2 87.6 .633 180.9 332.7 841.2\nXLM 410.3 .155 35.7 95.6 131.3\nTrans.-XL 152.6 .566 103.9 183.7 493.7\nXLNet 489.2 .580 141.5 259.7 646.0\nPENN TREEBANK\n2-Gram 216.9 .300 22.4 27.6 69.0\n3-Gram 207.0 .326 24.1 33.2 95.1\n4-Gram 206.1 .330 22.5 32.4 95.6\n5-Gram 205.7 .331 22.4 32.3 96.3\nLSTM-200 121.4 .270 49.7 93.9 226.8\nLSTM-425 118.4 .271 51.3 94.4 238.1\nLSTM-650 125.5 .273 60.8 107.7 250.3\nLSTM-1100 123.3 .270 47.1 87.9 222.3\nTrans.-200 137.5 .277 49.2 88.0 215.6\nTrans.-424 144.1 .274 47.8 84.5 209.0\nTrans.-650 141.4 .275 51.4 89.0 221.8\nTrans.-1100 150.7 .278 44.1 81.8 211.4\nWIKI TEXT-2\n2-Gram 381.1 .425 19.3 32.6 97.0\n3-Gram 364.4 .453 28.2 45.0 125.4\n4-Gram 359.8 .455 28.2 45.5 126.6\n5-Gram 358.6 .455 28.2 45.9 127.3\nLSTM-200 236.5 .443 52.1 93.0 255.7\nLSTM-425 224.4 .437 49.2 84.8 243.4\nLSTM-650 230.5 .436 50.6 84.9 240.6\nLSTM-1100 262.3 .450 47.2 84.0 250.9\nTrans.-200 319.1 .443 54.4 92.9 251.3\nTrans.-424 329.4 .462 61.6 104.5 279.5\nTrans.-650 340.7 .450 53.6 95.5 271.9\nTrans.-1100 337.1 .454 53.9 92.6 261.2\nLM1B\n2-Gram 291.1 .506 86.9 149.1 413.6\n3-Gram 191.2 .560 122.1 212.2 546.8\n4-Gram 172.2 .582 130.5 220.4 552.1\n5-Gram 169.0 .583 131.3 223.4 553.9\nTable 2: Perplexity (PPL), PNC, and ∆LogLik.\nPenn Treebank LSTMs outperform Penn Treebank\nTransformers on average, while WikiText-2 Trans-\nformers outperform WikiText-2 LSTMs on aver-\nage. However, controlling for training corpus, our\nLSTMs are consistently better than our Transform-\ners in terms of perplexity, while our Transformers\nconsistently outperform our LSTMs in terms of\nPNC.\nIn the remainder of this section, we describe spe-\nciﬁc observations about the relationship between\nlanguage model performance and ∆LogLik.\n4.1 Perplexity vs. ∆LogLik\nThe relationship between perplexity and ∆LogLik\nis visualized in the top row of Figure 1. Recall\nthat only models trained on the same corpus can be\ncompared with one another in these plots, since the\ndifferent training corpora have different vocabular-\nies. Indeed, we see a large effect overall of training\ndata on the relationship between perplexity and\n80\nFigure 1: The relationship between language modeling performance and ∆LogLik.\n∆LogLik. Apart from GPT-2 and Transformer-XL,\nobserve that the models trained on the Penn Tree-\nbank achieve the lowest perplexities. This likely\nreﬂects the fact that the Penn Treebank has the\nsmallest vocabulary among the different models,\nas well as the fact that the Penn Treebank and the\nDundee Corpus are both drawn from newspapers.\nFor all three eyetracking measures, there appears\nto be a linear relationship between perplexity and\n∆LogLik for different sized n-gram models with\nthe same training corpus, as well as the models\ntrained on the Penn Treebank. However, this rela-\ntionship does not generalize well to the LSTMs and\nTransformers trained on WikiText-2. Neither the\nLSTMs nor the Transformers extrapolate the line\nthat connects the n-gram models. In agreement\nwith Goodkind and Bicknell (2018), we observe\nthat the LSTMs lie below the n-gram line, while\nthe Transformers lie above it. Among models of\nthe same training corpus and architecture, we do\nnot see any relationship between perplexity and\n∆LogLik for LSTMs or Transformers trained on\nWikiText-2.\nBecause they all use different vocabularies, the\npre-trained models cannot be compared with other\nmodels in these plots. Nonetheless, XLNet ap-\npears to be an outlier among pre-trained models,\nhaving the highest perplexity despite achieving the\nsecond-highest ∆LogLik for all three eyetracking\nmeasures.\nLooking across the columns of Figure 1, we\nobserve that the results described in this subsection\ngeneralize across all three reading time metrics for\nthe Dundee Corpus.\n4.2 PNC vs. ∆LogLik\nNext, let us turn to the relationship between PNC\nand ∆LogLik, visualized on the bottom row of\nFigure 1. We see a robust relationship in the data,\nespecially among the better models. In particu-\nlar, the models that achieve a PNC of at least 0.4\nshow a strong linear relationship between PNC and\n∆LogLik. Alternatively, the results may be seen\nas an exponential or logistic relation that subsumes\nthe models with PNC <0.4, as ∆LogLik cannot\nbe negative and PNC is capped at 1. Crucially,\nall of the models with PNC ≥0.4 are subsumed\nunder the same trend, despite the fact that these\nmodels collectively use ﬁve different vocabularies\n81\nFigure 2: The relationship between training dataset size\n(MB) and ∆LogLik for monolingual English models.\nValues are averaged among models trained on the each\ndataset.\nof differing sizes. Furthermore, unlike in the case\nof perplexity, LSTMs do not overperform in terms\nof PNC relative to their ∆LogLik performance.\nAmong the pre-trained models, the outlying data\npoint is the XLM model, which achieves the lowest\nPNC out of all the models. This is unsurprising,\nconsidering that XLM is a multilingual model. The\nother potential outliers are the Penn Treebank data\npoints, which achieve PNCs below 0.4 despite hav-\ning a similar ∆LogLik to the WikiText-2 models.\nAs with perplexity, these results generalize\nacross the reading metrics for the Dundee Corpus,\nthough visually speaking, ∆LogLiks based on ﬁrst\nﬁxation duration and total duration appear to pro-\nvide a bitter ﬁt for an exponential model than gaze\nduration.\n5 Discussion\nWe focus our discussion on three topics. First, we\nconsider the ways in which each of our experimen-\ntal variables affects our models’ psycholinguistic\nmodeling performance. Next, we brieﬂy identify\nsome properties of PNC as a metric, particularly in\nterms of its relationship with training corpus size.\nFinally, we reﬂect on the implications of our re-\nsults for language modeling and psycholinguistic\nmodeling evaluation more generally.\n5.1 Factors Affecting ∆LogLik\nIn our experiment, we have analyzed several model\narchitectures, model sizes, and training datasets.\nSimilarly to Merkx and Frank (2020), we determine\nFigure 3: The relationship between training dataset size\n(MB) and PNC for monolingual English models. Val-\nues are averaged among models trained on the each\ndataset.\nthat the number of model parameters generally does\nnot signiﬁcantly impact psycholinguistic modeling\nperformance, whereas model architecture, along\nwith the composition and size of the training corpus,\ndo have a signiﬁcant impact.\nOur ﬁndings regarding the effect of model archi-\ntecture are consistent with the model class effect\nidentiﬁed by both Goodkind and Bicknell (2018)\nand Wilcox et al. (2020), in that LSTMs appear\nto underperform in terms of ∆LogLik given their\nperplexities. However, our architecture effect is not\nas dramatic as the one reported by Wilcox et al.:\nwhereas their n-grams generally show superior psy-\nchometric predictive power over their LSTMs, our\nn-gram models are not on par with that of our\nLSTMs.\nAlthough our experiment did not control for both\ntraining data size and composition separately, we\nargue that both properties are important factors that\naffect ∆LogLik. Firstly, notice that the ∆LogLiks\nfor the Penn Treebank LSTMs and Transformers\nare exceptionally high given that their PNC is less\nthan 0.4, especially the values computed for total\nduration. Indeed, whereas all other models with\nPNC <0.4 achieve ∆LogLiks close to 100 for to-\ntal duration, the Penn Treebank LSTMs and Trans-\nformers exhibit ∆LogLiks in excess of 200. We\nposit that this phenomenon is due to domain sim-\nilarity between the newspaper data found in the\nPenn Treebank and Dundee Corpus datasets, sug-\ngesting that the content of the training dataset can\nimprove ∆LogLik even when the amount of data\n82\navailable is small. This is consistent with previous\nwork such as Hale et al. (2019), showing that genre\nmatters when it comes to cognitive modeling. On\nthe other hand, observe that the 3-, 4-, and 5-gram\nmodels trained on LM1B outperform Transformer-\nXL and rival XLNet on ∆LogLik for all three read-\ning time metrics, despite having a much simpler\nmodel architecture. Given that LM1B (4 GB) is\nmuch bigger than WikiText-103 (515 MB), the\ntraining corpus for Transformer-XL, this observa-\ntion shows that a large dataset can dramatically\nenhance the psycholinguistic modeling capabilities\nof an otherwise simple architecture.\nIt is worth noting that dataset size and quality\nare both important factors for ∆LogLik. Figure 2\nshows the average ∆LogLik for ﬁrst ﬁxation du-\nration attained by models trained on each dataset,\nexcluding XLM. There, we ﬁnd that GPT-2 outper-\nforms the other models in terms of psycholinguis-\ntic predictive power, including those that are more\narchitecturally sophisticated, namely Transformer-\nXL and XLNet. Although WebText (40 GB) is not\nthe largest dataset, as it is smaller than XLNet’s\ntraining corpus (158 GB), it was constructed in a\nmore curated approach than the other datasets. The\nwide variety of document types included in Web-\nText likely makes it a higher-quality training corpus\nthan XLNet’s corpus or LM1B, allowing GPT-2 to\nsurpass other models.\n5.2 Factors Affecting PNC\nLikewise, Figure 3 depicts the relationship be-\ntween dataset size and PNC. Here, we ﬁnd that\nthe PNC consistently lies between 0.55 and 0.65\nfor the larger datasets, but is signiﬁcantly lower for\nthe smaller datasets. While the lower values for\nWikiText-2 and Penn Treebank show that smaller\ndatasets generally produce models with lower PNC,\nthe small variance in PNC for the larger datasets\nis suggestive of diminishing returns in PNC when\ntraining corpora are sufﬁciently large.\nFigures 4 and 5 compare PNC to perplexity-\nbased metrics. Figure 4 shows that PNC cannot\nbe predicted from perplexity, highlighting the dis-\ntinctness of the two metrics. Figure 5 plots the\n∆LogLik of gaze duration against normalized per-\nplexity (Marti and Bunke, 2001), deﬁned as perplex-\nity divided by vocabulary size. The relationship\nbetween normalized perplexity and ∆LogLik ap-\npears to be slightly stronger than perplexity alone,\nbut not as strong as the relationship between PNC\nFigure 4: There is no correlation between perplexity\nand PNC (ρ= .222).\nFigure 5: Unlike PNC, normalized perplexity does not\nexhibit a strong relation with ∆LogLik.\nand ∆LogLik. This suggests that the beneﬁts of\nusing PNC over perplexity cannot be replicated\nsimply by adjusting perplexity for differences in\nvocabulary size.\n5.3 Evaluating Surprisal Estimates\nWhile perplexity is the standard metric for eval-\nuating surprisal estimates produced by language\nmodels, we have argued throughout this paper that\nperplexity is not a reliable metric of language mod-\neling performance. In addition to the difﬁculty\npresented by perplexity for comparing models with\ndifferent training conditions, especially pre-trained\nmodels, perplexity has been recently shown to be a\npoor predictor of a language model’s ability to cap-\nture generalizations about natural language gram-\nmar (Ek et al., 2019; Hu et al., 2020). Along those\nlines, in Figure 4 we have seen that perplexity is\na poor predictor of PNC. Taken together, these\nobservations indicate that perplexity does not cap-\n83\nture the extent to which language models exhibit\nhuman-like behavior. Instead, alternative metrics\nlike PNC or Hu et al.’s (2020) syntactic general-\nization score explicitly assess the degree to which\nlanguage models behave like humans, without sen-\nsitivity to training conditions or other artifacts of\nthe model.\nOur arguments about the reliability of PNC as a\nmeasure of language modeling performance raise\nan interesting question about the role of language\nmodels in psycholinguistic modeling. Within\nthe psycholinguistic modeling literature, language\nmodels are often viewed as statistical estimators of\ncorpus-based frequencies (e.g., Smith and Levy,\n2011). While n-gram models can certainly be\nunderstood this way, the complex, increasingly\nopaque models used in neural NLP do not read-\nily lend themselves to this interpretation. Instead,\nthe literature on pre-training has recast language\nmodeling as a generalization of NLP training objec-\ntives (Radford et al., 2019), and pre-trained models\nhave been shown to encode a wide range of lin-\nguistic information beyond word and n-gram fre-\nquencies (see Rogers et al., 2020 for an overview).\nTherefore, while Frisson et al. (2005) and Smith\nand Levy (2011) have argued that corpus-based\nfrequencies are not as suitable for psycholinguistic\nmodeling as Cloze probabilities, our results indi-\ncate that pre-trained models with a high PNC such\nas GPT-2 may capture a notion of predictability\nthat more closely estimates subjective predictabil-\nity than empirical probabilities, allowing them to\nserve as better psycholinguistic models.\n6 Conclusion\nThe results reported here suggest that for the pur-\npose of modeling reading times, perplexity does\nnot adequately reveal the relationship between lan-\nguage modeling performance and psycholinguistic\nmodeling performance, especially when vocabu-\nlary cannot be controlled for. In contrast, PNC\nhas proven to be a much better predictor of the\npsychometric capabilities of our language models.\nThis ﬁnding is consistent with observations by Fris-\nson et al. (2005) and Smith and Levy (2011) that\nCloze probabilities predict self-paced reading times\nbetter than corpus probabilities do. In addition to\nallowing models with different vocabularies to be\ncompared with one another, PNC is much more\nstrongly correlated with psycholinguistic modeling\nperformance than perplexity, as demonstrated by\nour results on ∆LogLik. More generally, as there is\nlittle to no correlation between perplexity and PNC,\nPNC can serve as a good supplement to perplexity\nfor language model evaluation, providing informa-\ntion about model behavior that is not captured by\nthe latter.\nWe have also shown that model architecture,\ntraining dataset size, and training dataset compo-\nsition all contribute substantially to the psycholin-\nguistic modeling capabilities of language models.\nIn particular, the importance of corpus size and\ncomposition is reﬂective of trends in transfer learn-\ning, in which advances in downstream NLP tasks\nare made by using language models to extract gen-\neral linguistic information from large corpora. Our\nanalysis has shown that large corpora have the po-\ntential to provide considerable amounts of linguis-\ntic knowledge even through simple model architec-\ntures, as in the case of the LM1B n-grams.\nAs new pre-trained language models are devel-\noped, especially with custom vocabularies that\nmake a direct comparison of perplexities impos-\nsible, metrics such as PNC can serve as a valuable\ntool for assessing the quality of language models.\nIn establishing a strong relationship between PNC\nand ∆LogLik, we have demonstrated that PNC\nscores convey a psychometrically relevant notion of\nlanguage model quality that directly measures the\ndegree to which language models exhibit human-\nlike behavior. Reliable metrics like PNC, which\nare robust to variations in model setup, have the\npotential to greatly improve our ability to under-\nstand the relationship between language models\nand language.\nAcknowledgments\nWe would like to thank Adam Goodkind, Klin-\nton Bicknell, and Vera Demberg for helping us\nreproduce Goodkind and Bicknell’s (2018) anal-\nysis; Alan Kennedy for providing us the Dundee\nCorpus data; Christopher Geissler for his helpful\ndiscussion and assistance in statistical modeling;\nand the reviewers for their feedback.\nReferences\nChristoph Aurnhammer and Stefan Frank. 2019. Com-\nparing Gated and Simple Recurrent Neural Network\nArchitectures as Models of Human Sentence Pro-\ncessing. In Proceedings of the 41st Annual Confer-\nence of the Cognitive Science Society , pages 112–\n118, Montreal, Canada. Cognitive Science Society.\n84\nMarisa Ferrara Boston, John Hale, Reinhold Kliegl,\nUmesh Patil, and Shravan Vasishth. 2008. Parsing\ncosts as predictors of reading difﬁculty: An evalua-\ntion using the Potsdam Sentence Corpus. Journal of\nEye Movement Research, 2(1).\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2014. One Billion Word Benchmark for Mea-\nsuring Progress in Statistical Language Modeling.\nComputing Research Repository, arXiv:1312.3005.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual Language Model Pretraining. In Advances\nin Neural Information Processing Systems 32, pages\n7059–7069, Vancouver, Canada. Curran Associates,\nInc.\nAndrew M. Dai and Quoc V . Le. 2015. Semi-\nsupervised Sequence Learning. In Advances in Neu-\nral Information Processing Systems 28, pages 3079–\n3087, Montreal, Canada. Curran Associates, Inc.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive Language Models be-\nyond a Fixed-Length Context. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 2978–2988, Florence,\nItaly. Association for Computational Linguistics.\nVera Demberg and Frank Keller. 2008. Data from eye-\ntracking corpora as evidence for theories of syntactic\nprocessing complexity. Cognition, 109(2):193–210.\nSusan F. Ehrlich and Keith Rayner. 1981. Contextual\nEffects on Word Perception and Eye Movements dur-\ning Reading. Journal of Verbal Learning and Verbal\nBehavior, 20(6):641–655.\nAdam Ek, Jean-Philippe Bernardy, and Shalom Lap-\npin. 2019. Language Modeling with Syntactic and\nSemantic Representation for Sentence Acceptabil-\nity Predictions. In Proceedings of the 22nd Nordic\nConference on Computational Linguistics, pages 76–\n85, Turku, Finland. Link¨oping University Electronic\nPress.\nStefan Frank. 2009. Surprisal-based comparison be-\ntween a symbolic and a connectionist model of sen-\ntence processing. In Proceedings of the 31st Annual\nConference of the Cognitive Science Society , pages\n1139–1144, Amsterdam, Netherlands. Cognitive Sci-\nence Society.\nSteven Frisson, Keith Rayner, and Martin J. Picker-\ning. 2005. Effects of Contextual Predictability and\nTransitional Probability on Eye Movements Dur-\ning Reading. Journal of Experimental Psychology:\nLearning, Memory, and Cognition , 31(5):862–877.\nPlace: US Publisher: American Psychological Asso-\nciation.\nAdam Goodkind and Klinton Bicknell. 2018. Predic-\ntive power of word surprisal for reading times is a\nlinear function of language model quality. In Pro-\nceedings of the 8th Workshop on Cognitive Modeling\nand Computational Linguistics (CMCL 2018), pages\n10–18, Salt Lake City, UT. Association for Compu-\ntational Linguistics.\nJohn Hale. 2001. A Probabilistic Earley Parser as a\nPsycholinguistic Model. In Second Meeting of the\nNorth American Chapter of the Association for Com-\nputational Linguistics , Pittsburgh, PA. Association\nfor Computational Linguistics.\nJohn Hale, Adhiguna Kuncoro, Keith Hall, Chris Dyer,\nand Jonathan Brennan. 2019. Text genre and train-\ning data size in human-like parsing. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 5846–5852, Hong\nKong, China. Association for Computational Lin-\nguistics.\nKenneth Heaﬁeld. 2011. KenLM: Faster and Smaller\nLanguage Model Queries. In Proceedings of the\nSixth Workshop on Statistical Machine Translation ,\npages 187–197, Edinburgh, United Kingdom. Asso-\nciation for Computational Linguistics.\nKenneth Heaﬁeld, Ivan Pouzyrevsky, Jonathan H.\nClark, and Philipp Koehn. 2013. Scalable Modiﬁed\nKneser–Ney Language Model Estimation. In Pro-\nceedings of the 51st Annual Meeting of the Associa-\ntion for Computational Linguistics, volume 2: Short\nPapers, pages 690–696, Soﬁa, Bulgaria. Association\nfor Computational Linguistics.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong Short-Term Memory. Neural Computation ,\n9(8):1735–1780.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nLanguage Model Fine-tuning for Text Classiﬁcation.\nIn Proceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics , volume 1:\nLong Papers, pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nJennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox,\nand Roger Levy. 2020. A Systematic Assessment of\nSyntactic Generalization in Neural Language Mod-\nels. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 1725–1744, Seattle, W A. Association for\nComputational Linguistics.\nDan Jurafsky and James H. Martin. 2008. Speech\nand Language Processing: An Introduction to Nat-\nural Language Processing, Computational Linguis-\ntics, and Speech Recognition , 2 edition. Prentice\nHall Series in Artiﬁcial Intelligence. Prentice Hall,\nUpper Saddle River, NJ.\nAlan Kennedy. 2003. The Dundee Corpus. CD-ROM,\nUniversity of Dundee Department of Psychology,\nDundee, United Kingdom.\n85\nAlan Kennedy, Robin Hill, and Jo ¨el Pynte. 2003. The\nDundee Corpus. Poster presented at the 12th Eu-\nropean Conference on Eye Movements, Dundee,\nUnited Kingdom.\nAlan Kennedy, Jo ¨el Pynte, Wayne S. Murray, and\nShirley-Anne Paul. 2013. Frequency and pre-\ndictability effects in the Dundee Corpus: An eye\nmovement analysis. Quarterly Journal of Experi-\nmental Psychology, 66(3):601–618.\nRoger Levy. 2008. Expectation-based syntactic com-\nprehension. Cognition, 106(3):1126–1177.\nU.-V . Marti and H. Bunke. 2001. On the inﬂuence\nof vocabulary size and language models in uncon-\nstrained handwritten text recognition. In Proceed-\nings of Sixth International Conference on Document\nAnalysis and Recognition , pages 260–265, Seattle,\nW A. IEEE.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer Sentinel Mixture\nModels. In ICLR 2017 Conference Track , Toulon,\nFrance. OpenReview.\nDanny Merkx and Stefan L. Frank. 2020. Comparing\nTransformers and RNNs on predicting human sen-\ntence processing data. Computing Research Reposi-\ntory, arXiv:2005.09471.\nTom´aˇs Mikolov, Anoop Deoras, Stefan Kombrink,\nLuk´aˇs Burget, and Jan ˇCernock´y. 2011. Empiri-\ncal Evaluation and Combination of Advanced Lan-\nguage Modeling Techniques. In INTERSPEECH-\n2011, pages 605–608, Florence, Italy. International\nSpeech Communication Association.\nJeff Mitchell, Mirella Lapata, Vera Demberg, and\nFrank Keller. 2010. Syntactic and Semantic Factors\nin Processing Difﬁculty: An Integrated Measure. In\nProceedings of the 48th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 196–\n206, Uppsala, Sweden. Association for Computa-\ntional Linguistics.\nIrene Fernandez Monsalve, Stefan L. Frank, and\nGabriella Vigliocco. 2012. Lexical surprisal as a\ngeneral predictor of reading time. In Proceedings\nof the 13th Conference of the European Chapter\nof the Association for Computational Linguistics ,\npages 398–408, Avignon, France. Association for\nComputational Linguistics.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming\nLin, Alban Desmaison, Luca Antiga, and Adam\nLerer. 2017. Automatic differentiation in PyTorch.\nIn NIPS 2017 Autodiff Workshop, Long Beach, CA.\nOpenReview.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nModels are Unsupervised Multitask Learners. Tech-\nnical Report, OpenAI, San Francisco, CA.\nBrian Roark, Asaf Bachrach, Carlos Cardenas, and\nChristophe Pallier. 2009. Deriving lexical and syn-\ntactic expectation-based measures for psycholinguis-\ntic modeling via incremental top-down parsing. In\nProceedings of the 2009 Conference on Empirical\nMethods in Natural Language Processing , pages\n324–333, Singapore. Association for Computational\nLinguistics.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A Primer in BERTology: What we know\nabout how BERT works. Computing Research\nRepository, arXiv:2002.12327.\nNathaniel Smith and Roger Levy. 2011. Cloze but no\ncigar: The complex relationship between cloze, cor-\npus, and subjective probabilities in language process-\ning. In Expanding the Space of Cognitive Science:\nProceedings of the 33rd Annual Meeting of the Cog-\nnitive Science Society , pages 1637–1642, Boston,\nMA. Cognitive Science Society.\nNathaniel J. Smith and Roger Levy. 2013. The effect\nof word predictability on reading time is logarithmic.\nCognition, 128(3):302–319.\nJasper Snoek, Hugo Larochelle, and Ryan P Adams.\n2012. Practical Bayesian Optimization of Machine\nLearning Algorithms. In Advances in Neural In-\nformation Processing Systems 25, pages 2951–2959,\nLake Tahoe, NV . Curran Associates, Inc.\nWilson L. Taylor. 1953. “Cloze Procedure”: A New\nTool for Measuring Readability. Journalism Quar-\nterly, 30(4):415–433.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008, Long Beach,\nCA. Curran Associates, Inc.\nEthan Gotlieb Wilcox, Jon Gauthier, Jennifer Hu, Peng\nQian, and Roger Levy. 2020. On the Predictive\nPower of Neural Language Models for Human Real-\nTime Comprehension Behavior. Computing Re-\nsearch Repository, arXiv:2006.01912.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Fun-\ntowicz, Joe Davison, Sam Shleifer, Patrick von\nPlaten, Clara Ma, Yacine Jernite, Julien Plu, Can-\nwen Xu, Teven Le Scao, Sylvain Gugger, Mariama\nDrame, Quentin Lhoest, and Alexander M. Rush.\n2020. HuggingFace’s Transformers: State-of-the-\nart Natural Language Processing. Computing Re-\nsearch Repository, arXiv:1910.03771.\nSimon N. Wood. 2004. Stable and Efﬁcient Multi-\nple Smoothing Parameter Estimation for General-\nized Additive Models. Journal of the American Sta-\ntistical Association, 99(467):673–686.\n86\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R. Salakhutdinov, and Quoc V . Le.\n2019. XLNet: Generalized Autoregressive Pretrain-\ning for Language Understanding. In Advances in\nNeural Information Processing Systems 32 , pages\n5753–5763, Vancouver, Canada. Curran Associates,\nInc.\nDavid Zola. 1981. The Effect of Redundancy on the\nPerception of Words in Reading. Technical Report\n216, University of Illinois at Urbana-Champaign\nCenter for the Study of Reading, Champaign, IL.\nDavid Zola. 1984. Redundancy and word percep-\ntion during reading. Perception & Psychophysics ,\n36(3):277–284."
}