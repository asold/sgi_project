{
    "title": "TableFormer: Table Structure Understanding with Transformers",
    "url": "https://openalex.org/W4214970283",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5088090169",
            "name": "Ahmed Nassar",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5084933934",
            "name": "Nikolaos Livathinos",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5024178356",
            "name": "Maksym Lysak",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5024778597",
            "name": "Peter Staar",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3003711898",
        "https://openalex.org/W3107064625",
        "https://openalex.org/W1895577753",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W4289751790",
        "https://openalex.org/W3004127423",
        "https://openalex.org/W3118722740",
        "https://openalex.org/W2302086703",
        "https://openalex.org/W3003953860",
        "https://openalex.org/W1970549718",
        "https://openalex.org/W2111297753",
        "https://openalex.org/W6797650329",
        "https://openalex.org/W2046941907",
        "https://openalex.org/W2222512263",
        "https://openalex.org/W1969616664",
        "https://openalex.org/W3134658493",
        "https://openalex.org/W2786162033",
        "https://openalex.org/W2022351003",
        "https://openalex.org/W6648075034",
        "https://openalex.org/W2117462434",
        "https://openalex.org/W3205155483",
        "https://openalex.org/W2963150697",
        "https://openalex.org/W6778485988",
        "https://openalex.org/W3175487048",
        "https://openalex.org/W3003496674",
        "https://openalex.org/W4214693170",
        "https://openalex.org/W3034997246",
        "https://openalex.org/W6766978945",
        "https://openalex.org/W2962766617",
        "https://openalex.org/W3003206728"
    ],
    "abstract": "Tables organize valuable content in a concise and compact representation. This content is extremely valuable for systems such as search engines, Knowledge Graph's, etc, since they enhance their predictive capabilities. Unfortunately, tables come in a large variety of shapes and sizes. Furthermore, they can have complex column/row-header configurations, multiline rows, different variety of separation lines, missing entries, etc. As such, the correct identification of the table-structure from an image is a non-trivial task. In this paper, we present a new table-structure identification model. The latter improves the latest end-to-end deep learning model (i.e. encoder-dual-decoder from PubTabNet) in two significant ways. First, we introduce a new object detection decoder for table-cells. In this way, we can obtain the content of the table-cells from programmatic PDF's directly from the PDF source and avoid the training of the custom OCR decoders. This architectural change leads to more accurate table-content extraction and allows us to tackle non-english tables. Second, we replace the LSTM decoders with transformer based decoders. This upgrade improves significantly the previous state-of-the-art tree-editing-distance-score (TEDS) from 91% to 98.5% on simple tables and from 88.7% to 95% on complex tables.",
    "full_text": "TableFormer: Table Structure Understanding with Transformers.\nAhmed Nassar, Nikolaos Livathinos, Maksym Lysak, Peter Staar\nIBM Research\n{ahn,nli,mly,taa}@zurich.ibm.com\nAbstract\nTables organize valuable content in a concise and com-\npact representation. This content is extremely valuable for\nsystems such as search engines, Knowledge Graph’s, etc,\nsince they enhance their predictive capabilities. Unfortu-\nnately, tables come in a large variety of shapes and sizes.\nFurthermore, they can have complex column/row-header\nconﬁgurations, multiline rows, different variety of separa-\ntion lines, missing entries, etc. As such, the correct iden-\ntiﬁcation of the table-structure from an image is a non-\ntrivial task. In this paper, we present a new table-structure\nidentiﬁcation model. The latter improves the latest end-to-\nend deep learning model (i.e. encoder-dual-decoder from\nPubTabNet) in two signiﬁcant ways. First, we introduce a\nnew object detection decoder for table-cells. In this way,\nwe can obtain the content of the table-cells from program-\nmatic PDF’s directly from the PDF source and avoid the\ntraining of the custom OCR decoders. This architectural\nchange leads to more accurate table-content extraction and\nallows us to tackle non-english tables. Second, we replace\nthe LSTM decoders with transformer based decoders. This\nupgrade improves signiﬁcantly the previous state-of-the-art\ntree-editing-distance-score (TEDS) from 91% to 98.5% on\nsimple tables and from 88.7% to 95% on complex tables.\n1. Introduction\nThe occurrence of tables in documents is ubiquitous.\nThey often summarise quantitative or factual data, which is\ncumbersome to describe in verbose text but nevertheless ex-\ntremely valuable. Unfortunately, this compact representa-\ntion is often not easy to parse by machines. There are many\nimplicit conventions used to obtain a compact table repre-\nsentation. For example, tables often have complex column-\nand row-headers in order to reduce duplicated cell content.\nLines of different shapes and sizes are leveraged to separate\ncontent or indicate a tree structure. Additionally, tables can\nalso have empty/missing table-entries or multi-row textual\ntable-entries. Fig. 1 shows a table which presents all these\nissues.\na. Picture of a table:\nb. Red-annotation of bounding boxes,\nBlue-predictions by T ableFormer\nc. Structure predicted by T ableFormer:\n10 2\n3 4 5 6 7\n8\n9 10 11 12\n13 14 15 16\n17 18 19 20\n10 2\n3 4 5 6 7\n8\n9 10 11 12\n13 14 15 16\n17 18 19 20\n1\n3\n3\n2\n2\n2\n1\n1\n3\nFigure 1: Picture of a table with subtle, complex features\nsuch as (1) multi-column headers, (2) cell with multi-row\ntext and (3) cells with no content. Image from PubTabNet\nevaluation set, ﬁlename: ‘PMC2944238 004 02’.\nRecently, signiﬁcant progress has been made with vi-\nsion based approaches to extract tables in documents. For\nthe sake of completeness, the issue of table extraction from\ndocuments is typically decomposed into two separate chal-\nlenges, i.e. (1) ﬁnding the location of the table(s) on a\ndocument-page and (2) ﬁnding the structure of a given table\nin the document.\nThe ﬁrst problem is called table-location and has been\npreviously addressed [30, 38, 19, 21, 23, 26, 8] with state-\nof-the-art object-detection networks (e.g. YOLO and later\non Mask-RCNN [9]). For all practical purposes, it can be\n1\narXiv:2203.01017v2  [cs.CV]  11 Mar 2022\nconsidered as a solved problem, given enough ground-truth\ndata to train on.\nThe second problem is called table-structure decompo-\nsition. The latter is a long standing problem in the com-\nmunity of document understanding [6, 4, 14]. Contrary to\nthe table-location problem, there are no commonly used ap-\nproaches that can easily be re-purposed to solve this prob-\nlem. Lately, a set of new model-architectures has been pro-\nposed by the community to address table-structure decom-\nposition [37, 36, 18, 20]. All these models have some weak-\nnesses (see Sec. 2). The common denominator here is the\nreliance on textual features and/or the inability to provide\nthe bounding box of each table-cell in the original image.\nIn this paper, we want to address these weaknesses and\npresent a robust table-structure decomposition algorithm.\nThe design criteria for our model are the following. First,\nwe want our algorithm to be language agnostic. In this way,\nwe can obtain the structure of any table, irregardless of the\nlanguage. Second, we want our algorithm to leverage as\nmuch data as possible from the original PDF document. For\nprogrammatic PDF documents, the text-cells can often be\nextracted much faster and with higher accuracy compared\nto OCR methods. Last but not least, we want to have a di-\nrect link between the table-cell and its bounding box in the\nimage.\nTo meet the design criteria listed above, we developed a\nnew model called TableFormer and a synthetically gener-\nated table structure dataset called SynthTabNet1. In partic-\nular, our contributions in this work can be summarised as\nfollows:\n• We propose TableFormer, a transformer based model\nthat predicts tables structure and bounding boxes for\nthe table content simultaneously in an end-to-end ap-\nproach.\n• Across all benchmark datasets TableFormer signif-\nicantly outperforms existing state-of-the-art metrics,\nwhile being much more efﬁcient in training and infer-\nence to existing works.\n• We present SynthTabNet a synthetically generated\ndataset, with various appearance styles and complex-\nity.\n• An augmented dataset based on PubTabNet [37],\nFinTabNet [36], and TableBank [17] with generated\nground-truth for reproducibility.\nThe paper is structured as follows. In Sec. 2, we give\na brief overview of the current state-of-the-art. In Sec. 3,\nwe describe the datasets on which we train. In Sec. 4, we\nintroduce the TableFormer model-architecture and describe\n1https://github.com/IBM/SynthTabNet\nits results & performance in Sec. 5. As a conclusion, we de-\nscribe how this new model-architecture can be re-purposed\nfor other tasks in the computer-vision community.\n2. Previous work and State of the Art\nIdentifying the structure of a table has been an outstand-\ning problem in the document-parsing community, that mo-\ntivates many organised public challenges [6, 4, 14]. The\ndifﬁculty of the problem can be attributed to a number of\nfactors. First, there is a large variety in the shapes and sizes\nof tables. Such large variety requires a ﬂexible method.\nThis is especially true for complex column- and row head-\ners, which can be extremely intricate and demanding. A\nsecond factor of complexity is the lack of data with regard\nto table-structure. Until the publication of PubTabNet [37],\nthere were no large datasets (i.e. > 100K tables) that pro-\nvided structure information. This happens primarily due to\nthe fact that tables are notoriously time-consuming to an-\nnotate by hand. However, this has deﬁnitely changed in re-\ncent years with the deliverance of PubTabNet [37], FinTab-\nNet [36], TableBank [17] etc.\nBefore the rising popularity of deep neural networks,\nthe community relied heavily on heuristic and/or statistical\nmethods to do table structure identiﬁcation [3, 7, 11, 5, 13,\n28]. Although such methods work well on constrained ta-\nbles [12], a more data-driven approach can be applied due\nto the advent of convolutional neural networks (CNNs) and\nthe availability of large datasets. To the best-of-our knowl-\nedge, there are currently two different types of network ar-\nchitecture that are being pursued for state-of-the-art table-\nstructure identiﬁcation.\nImage-to-Text networks: In this type of network, one\npredicts a sequence of tokens starting from an encoded\nimage. Such sequences of tokens can be HTML table\ntags [37, 17] or LaTeX symbols[10]. The choice of sym-\nbols is ultimately not very important, since one can be trans-\nformed into the other. There are however subtle variations\nin the Image-to-Text networks. The easiest network archi-\ntectures are “image-encoder →text-decoder” (IETD), sim-\nilar to network architectures that try to provide captions to\nimages [32]. In these IETD networks, one expects as output\nthe LaTeX/HTML string of the entire table, i.e. the sym-\nbols necessary for creating the table with the content of the\ntable. Another approach is the “image-encoder →dual de-\ncoder” (IEDD) networks. In these type of networks, one has\ntwo consecutive decoders with different purposes. The ﬁrst\ndecoder is the tag-decoder, i.e. it only produces the HTM-\nL/LaTeX tags which construct an empty table. The second\ncontent-decoder uses the encoding of the image in combi-\nnation with the output encoding of each cell-tag (from the\ntag-decoder) to generate the textual content of each table\ncell. The network architecture of IEDD is certainly more\nelaborate, but it has the advantage that one can pre-train the\n2\ntag-decoder which is constrained to the table-tags.\nIn practice, both network architectures (IETD and\nIEDD) require an implicit, custom trained object-character-\nrecognition (OCR) to obtain the content of the table-cells.\nIn the case of IETD, this OCR engine is implicit in the de-\ncoder similar to [24]. For the IEDD, the OCR is solely em-\nbedded in the content-decoder. This reliance on a custom,\nimplicit OCR decoder is of course problematic. OCR is a\nwell known and extremely tough problem, that often needs\ncustom training for each individual language. However, the\nlimited availability for non-english content in the current\ndatasets, makes it impractical to apply the IETD and IEDD\nmethods on tables with other languages. Additionally, OCR\ncan be completely omitted if the tables originate from pro-\ngrammatic PDF documents with known positions of each\ncell. The latter was the inspiration for the work of this pa-\nper.\nGraph Neural networks : Graph Neural networks\n(GNN’s) take a radically different approach to table-\nstructure extraction. Note that one table cell can consti-\ntute out of multiple text-cells. To obtain the table-structure,\none creates an initial graph, where each of the text-cells\nbecomes a node in the graph similar to [33, 34, 2]. Each\nnode is then associated with en embedding vector coming\nfrom the encoded image, its coordinates and the encoded\ntext. Furthermore, nodes that represent adjacent text-cells\nare linked. Graph Convolutional Networks (GCN’s) based\nmethods take the image as an input, but also the position of\nthe text-cells and their content [18]. The purpose of a GCN\nis to transform the input graph into a new graph, which re-\nplaces the old links with new ones. The new links then\nrepresent the table-structure. With this approach, one can\navoid the need to build custom OCR decoders. However,\nthe quality of the reconstructed structure is not comparable\nto the current state-of-the-art [18].\nHybrid Deep Learning-Rule-Based approach: A pop-\nular current model for table-structure identiﬁcation is the\nuse of a hybrid Deep Learning-Rule-Based approach similar\nto [27, 29]. In this approach, one ﬁrst detects the position of\nthe table-cells with object detection (e.g. YoloVx or Mask-\nRCNN), then classiﬁes the table into different types (from\nits images) and ﬁnally uses different rule-sets to obtain\nits table-structure. Currently, this approach achieves state-\nof-the-art results, but is not an end-to-end deep-learning\nmethod. As such, new rules need to be written if different\ntypes of tables are encountered.\n3. Datasets\nWe rely on large-scale datasets such as PubTabNet [37],\nFinTabNet [36], and TableBank [17] datasets to train and\nevaluate our models. These datasets span over various ap-\npearance styles and content. We also introduce our own\nsynthetically generated SynthTabNet dataset to ﬁx an im-\nPubT abNet + FinT abNet\nRows / Columns\n0 20 4010 30 500\n20\n40\n60\n80\n10\n30\n50\n70\n90\n0\n10K\n8K\n6K\n4K\n2K\nFigure 2: Distribution of the tables across different table\ndimensions in PubTabNet + FinTabNet datasets\nbalance in the previous datasets.\nThe PubTabNet dataset contains 509k tables delivered as\nannotated PNG images. The annotations consist of the table\nstructure represented in HTML format, the tokenized text\nand its bounding boxes per table cell. Fig. 1 shows the ap-\npearance style of PubTabNet. Depending on its complexity,\na table is characterized as “simple” when it does not contain\nrow spans or column spans, otherwise it is “complex”. The\ndataset is divided into Train and Val splits (roughly 98% and\n2%). The Train split consists of 54% simple and 46% com-\nplex tables and the Val split of 51% and 49% respectively.\nThe FinTabNet dataset contains 112k tables delivered as\nsingle-page PDF documents with mixed table structures and\ntext content. Similarly to the PubTabNet, the annotations\nof FinTabNet include the table structure in HTML, the to-\nkenized text and the bounding boxes on a table cell basis.\nThe dataset is divided into Train, Test and Val splits (81%,\n9.5%, 9.5%), and each one is almost equally divided into\nsimple and complex tables (Train: 48% simple, 52% com-\nplex, Test: 48% simple, 52% complex, Test: 53% simple,\n47% complex). Finally the TableBank dataset consists of\n145k tables provided as JPEG images. The latter has anno-\ntations for the table structure, but only few with bounding\nboxes of the table cells. The entire dataset consists of sim-\nple tables and it is divided into 90% Train, 3% Test and 7%\nVal splits.\nDue to the heterogeneity across the dataset formats, it\nwas necessary to combine all available data into one homog-\nenized dataset before we could train our models for practi-\ncal purposes. Given the size of PubTabNet, we adopted its\nannotation format and we extracted and converted all tables\nas PNG images with a resolution of 72 dpi. Additionally,\nwe have ﬁltered out tables with extreme sizes due to small\n3\namount of such tables, and kept only those ones ranging\nbetween 1*1 and 20*10 (rows/columns).\nThe availability of the bounding boxes for all table cells\nis essential to train our models. In order to distinguish be-\ntween empty and non-empty bounding boxes, we have in-\ntroduced a binary class in the annotation. Unfortunately, the\noriginal datasets either omit the bounding boxes for whole\ntables (e.g. TableBank) or they narrow their scope only to\nnon-empty cells. Therefore, it was imperative to introduce\na data pre-processing procedure that generates the missing\nbounding boxes out of the annotation information. This pro-\ncedure ﬁrst parses the provided table structure and calcu-\nlates the dimensions of the most ﬁne-grained grid that cov-\ners the table structure. Notice that each table cell may oc-\ncupy multiple grid squares due to row or column spans. In\ncase of PubTabNet we had to compute missing bounding\nboxes for 48% of the simple and 69% of the complex ta-\nbles. Regarding FinTabNet, 68% of the simple and 98%\nof the complex tables require the generation of bounding\nboxes.\nAs it is illustrated in Fig. 2, the table distributions from\nall datasets are skewed towards simpler structures with\nfewer number of rows/columns. Additionally, there is very\nlimited variance in the table styles, which in case of Pub-\nTabNet and FinTabNet means one styling format for the\nmajority of the tables. Similar limitations appear also in\nthe type of table content, which in some cases (e.g. FinTab-\nNet) is restricted to a certain domain. Ultimately, the lack\nof diversity in the training dataset damages the ability of the\nmodels to generalize well on unseen data.\nMotivated by those observations we aimed at generating\na synthetic table dataset namedSynthTabNet. This approach\noffers control over: 1) the size of the dataset, 2) the table\nstructure, 3) the table style and 4) the type of content. The\ncomplexity of the table structure is described by the size of\nthe table header and the table body, as well as the percentage\nof the table cells covered by row spans and column spans.\nA set of carefully designed styling templates provides the\nbasis to build a wide range of table appearances. Lastly, the\ntable content is generated out of a curated collection of text\ncorpora. By controlling the size and scope of the synthetic\ndatasets we are able to train and evaluate our models in a\nvariety of different conditions. For example, we can ﬁrst\ngenerate a highly diverse dataset to train our models and\nthen evaluate their performance on other synthetic datasets\nwhich are focused on a speciﬁc domain.\nIn this regard, we have prepared four synthetic datasets,\neach one containing 150k examples. The corpora to gener-\nate the table text consists of the most frequent terms appear-\ning in PubTabNet and FinTabNet together with randomly\ngenerated text. The ﬁrst two synthetic datasets have been\nﬁne-tuned to mimic the appearance of the original datasets\nbut encompass more complicated table structures. The third\nTags Bbox Size Format\nPubTabNet \u0013 \u0013 509k PNG\nFinTabNet \u0013 \u0013 112k PDF\nTableBank \u0013 \u0017 145k JPEG\nCombined-Tabnet(*) \u0013 \u0013 400k PNG\nCombined(**) \u0013 \u0013 500k PNG\nSynthTabNet \u0013 \u0013 600k PNG\nTable 1: Both “Combined-Tabnet” and ”Combined-\nTabnet” are variations of the following: (*) The Combined-\nTabnet dataset is the processed combination of PubTabNet\nand Fintabnet. (**) The combined dataset is the processed\ncombination of PubTabNet, Fintabnet and TableBank.\none adopts a colorful appearance with high contrast and the\nlast one contains tables with sparse content. Lastly, we have\ncombined all synthetic datasets into one big uniﬁed syn-\nthetic dataset of 600k examples.\nTab. 1 summarizes the various attributes of the datasets.\n4. The TableFormer model\nGiven the image of a table, TableFormer is able to pre-\ndict: 1) a sequence of tokens that represent the structure of\na table, and 2) a bounding box coupled to a subset of those\ntokens. The conversion of an image into a sequence of to-\nkens is a well-known task [35, 16]. While attention is often\nused as an implicit method to associate each token of the\nsequence with a position in the original image, an explicit\nassociation between the individual table-cells and the image\nbounding boxes is also required.\n4.1. Model architecture.\nWe now describe in detail the proposed method, which\nis composed of three main components, see Fig. 4. Our\nCNN Backbone Network encodes the input as a feature vec-\ntor of predeﬁned length. The input feature vector of the\nencoded image is passed to the Structure Decoder to pro-\nduce a sequence of HTML tags that represent the structure\nof the table. With each prediction of an HTML standard\ndata cell (‘<td>’) the hidden state of that cell is passed to\nthe Cell BBox Decoder. As for spanning cells, such as row\nor column span, the tag is broken down to ‘<’, ‘rowspan=’\nor ‘colspan=’, with the number of spanning cells (attribute),\nand ‘>’. The hidden state attached to ‘ <’ is passed to the\nCell BBox Decoder. A shared feed forward network (FFN)\nreceives the hidden states from the Structure Decoder, to\nprovide the ﬁnal detection predictions of the bounding box\ncoordinates and their classiﬁcation.\nCNN Backbone Network. A ResNet-18 CNN is the\nbackbone that receives the table image and encodes it as a\nvector of predeﬁned length. The network has been modiﬁed\nby removing the linear and pooling layer, as we are not per-\n4\n1. Item\nAmountNames\n1000\n500\n3500\n150\nunit\nunit\nunit\nunit\n2. Item\n3. Item\n4. Item\nExtracted\nTable Images Standardized\nImages\nBBox\nDecoder\nBBoxes\nBBoxes can be\ntraced back to the\noriginal image to\nextract content\nStructureTags sequence\nprovide full description of\nthe table structure\nStructureTags\nBBoxes in sync\nwith tag sequence\nEncoder\nStructure\nDecoder\n[x1, y2, x2, y2]\n[x1', y2', x2', y2']\n[x1'', y2'', x2'', y2'']\n...\n<TR>\n<TD> 1 </TD><TD colspan=\"2\"> </TD>\n</TR><TR>\n<TD> </TD><TD>...\n...\n1\n2\n3\n2\n3\n3\n21\n1\nFigure 3: TableFormer takes in an image of the PDF and creates bounding box and HTML structure predictions that are\nsynchronized. The bounding boxes grabs the content from the PDF and inserts it in the structure.\nInput Image Tokenised Tags\nMulti-Head Attention\nAdd & Normalisation\nFeed Forward Network\nAdd & Normalisation\nLinear\nSoftmax\nCNN BACKBONE ENCODER\n[30,  1,  2,  3,  4, … 3,  \n4,  5,  8, 31]\nPositional \nEncoding\nPositional \nEncoding\nAdd & Normalisation\nAdd & Normalisation\nMulti-Head Attention\nAdd & Normalisation\nFeed Forward Network\nLinear\nLinear\nAttention Network\nMLP Linear \nSigmoid\nTransformer Encoder Network\nx2\nEncoded Output\nEncoded Output\nPredicted Tags\nBounding Boxes & \nClassiﬁcation\nTransformer \nDecoder Network\nx4\nCELL BBOX DECODER\nMasked Multi-Head \nAttention\nFigure 4: Given an input image of a table, theEncoder pro-\nduces ﬁxed-length features that represent the input image.\nThe features are then passed to both theStructure Decoder\nand Cell BBox Decoder. During training, the Structure\nDecoder receives ‘tokenized tags’ of the HTML code that\nrepresent the table structure. Afterwards, a transformer en-\ncoder and decoder architecture is employed to produce fea-\ntures that are received by a linear layer, and the Cell BBox\nDecoder. The linear layer is applied to the features to\npredict the tags. Simultaneously, the Cell BBox Decoder\nselects features referring to the data cells (‘<td>’, ‘<’) and\npasses them through an attention network, an MLP, and a\nlinear layer to predict the bounding boxes.\nforming classiﬁcation, and adding an adaptive pooling layer\nof size 28*28. ResNet by default downsamples the image\nresolution by 32 and then the encoded image is provided to\nboth the Structure Decoder, and Cell BBox Decoder.\nStructure Decoder.The transformer architecture of this\ncomponent is based on the work proposed in [31]. After\nextensive experimentation, the Structure Decoder is mod-\neled as a transformer encoder with two encoder layers and\na transformer decoder made from a stack of 4 decoder lay-\ners that comprise mainly of multi-head attention and feed\nforward layers. This conﬁguration uses fewer layers and\nheads in comparison to networks applied to other problems\n(e.g. “Scene Understanding”, “Image Captioning”), some-\nthing which we relate to the simplicity of table images.\nThe transformer encoder receives an encoded image\nfrom the CNN Backbone Network and reﬁnes it through a\nmulti-head dot-product attention layer, followed by a Feed\nForward Network. During training, the transformer de-\ncoder receives as input the output feature produced by the\ntransformer encoder, and the tokenized input of the HTML\nground-truth tags. Using a stack of multi-head attention lay-\ners, different aspects of the tag sequence could be inferred.\nThis is achieved by each attention head on a layer operating\nin a different subspace, and then combining altogether their\nattention score.\nCell BBox Decoder.Our architecture allows to simul-\ntaneously predict HTML tags and bounding boxes for each\ntable cell without the need of a separate object detector end\nto end. This approach is inspired by DETR [1] which em-\nploys a Transformer Encoder, and Decoder that looks for\na speciﬁc number of object queries (potential object detec-\ntions). As our model utilizes a transformer architecture, the\nhidden state of the <td>’ and ‘<’ HTML structure tags be-\ncome the object query.\nThe encoding generated by the CNN Backbone Network\nalong with the features acquired for every data cell from the\nTransformer Decoder are then passed to the attention net-\nwork. The attention network takes both inputs and learns to\nprovide an attention weighted encoding. This weighted at-\n5\ntention encoding is then multiplied to the encoded image to\nproduce a feature for each table cell. Notice that this is dif-\nferent than the typical object detection problem where im-\nbalances between the number of detections and the amount\nof objects may exist. In our case, we know up front that\nthe produced detections always match with the table cells\nin number and correspondence.\nThe output features for each table cell are then fed\ninto the feed-forward network (FFN). The FFN consists\nof a Multi-Layer Perceptron (3 layers with ReLU activa-\ntion function) that predicts the normalized coordinates for\nthe bounding box of each table cell. Finally, the predicted\nbounding boxes are classiﬁed based on whether they are\nempty or not using a linear layer.\nLoss Functions. We formulate a multi-task loss Eq. 2\nto train our network. The Cross-Entropy loss (denoted as\nls) is used to train the Structure Decoder which predicts the\nstructure tokens. As for the Cell BBox Decoder it is trained\nwith a combination of losses denoted as lbox. lbox consists\nof the generally used l1 loss for object detection and the\nIoU loss (liou) to be scale invariant as explained in [25]. In\ncomparison to DETR, we do not use the Hungarian algo-\nrithm [15] to match the predicted bounding boxes with the\nground-truth boxes, as we have already achieved a one-to-\none match through two steps: 1) Our token input sequence\nis naturally ordered, therefore the hidden states of the table\ndata cells are also in order when they are provided as in-\nput to the Cell BBox Decoder, and 2) Our bounding boxes\ngeneration mechanism (see Sec. 3) ensures a one-to-one\nmapping between the cell content and its bounding box for\nall post-processed datasets.\nThe loss used to train the TableFormer can be deﬁned as\nfollowing:\nlbox = λiouliou + λl1\nl= λls + (1−λ)lbox\n(1)\nwhere λ∈[0, 1], and λiou,λl1 ∈R are hyper-parameters.\n5. Experimental Results\n5.1. Implementation Details\nTableFormer uses ResNet-18 as theCNN Backbone Net-\nwork. The input images are resized to 448*448 pixels and\nthe feature map has a dimension of 28*28. Additionally, we\nenforce the following input constraints:\nImage width and height ≤1024 pixels\nStructural tags length ≤512 tokens. (2)\nAlthough input constraints are used also by other methods,\nsuch as EDD, ours are less restrictive due to the improved\nruntime performance and lower memory footprint of Table-\nFormer. This allows to utilize input samples with longer\nsequences and images with larger dimensions.\nThe Transformer Encoder consists of two “Transformer\nEncoder Layers”, with an input feature size of 512, feed\nforward network of 1024, and 4 attention heads. As for the\nTransformer Decoder it is composed of four “Transformer\nDecoder Layers” with similar input and output dimensions\nas the “Transformer Encoder Layers”. Even though our\nmodel uses fewer layers and heads than the default imple-\nmentation parameters, our extensive experimentation has\nproved this setup to be more suitable for table images. We\nattribute this ﬁnding to the inherent design of table im-\nages, which contain mostly lines and text, unlike the more\nelaborate content present in other scopes (e.g. the COCO\ndataset). Moreover, we have added ResNet blocks to the\ninputs of the Structure Decoder and Cell BBox Decoder.\nThis prevents a decoder having a stronger inﬂuence over the\nlearned weights which would damage the other prediction\ntask (structure vs bounding boxes), but learn task speciﬁc\nweights instead. Lastly our dropout layers are set to 0.5.\nFor training, TableFormer is trained with 3 Adam opti-\nmizers, each one for the CNN Backbone Network, Structure\nDecoder, and Cell BBox Decoder. Taking the PubTabNet as\nan example for our parameter set up, the initializing learn-\ning rate is 0.001 for 12 epochs with a batch size of 24, and\nλ set to 0.5. Afterwards, we reduce the learning rate to\n0.0001, the batch size to 18 and train for 12 more epochs or\nconvergence.\nTableFormer is implemented with PyTorch and Torchvi-\nsion libraries [22]. To speed up the inference, the image\nundergoes a single forward pass through the CNN Back-\nbone Network and transformer encoder. This eliminates the\noverhead of generating the same features for each decoding\nstep. Similarly, we employ a ’caching’ technique to preform\nfaster autoregressive decoding. This is achieved by storing\nthe features of decoded tokens so we can reuse them for\neach time step. Therefore, we only compute the attention\nfor each new tag.\n5.2. Generalization\nTableFormer is evaluated on three major publicly avail-\nable datasets of different nature to prove the generalization\nand effectiveness of our model. The datasets used for eval-\nuation are the PubTabNet, FinTabNet and TableBank which\nstem from the scientiﬁc, ﬁnancial and general domains re-\nspectively.\nWe also share our baseline results on the challenging\nSynthTabNet dataset. Throughout our experiments, the\nsame parameters stated in Sec. 5.1 are utilized.\n6\n5.3. Datasets and Metrics\nThe Tree-Edit-Distance-Based Similarity (TEDS) met-\nric was introduced in [37]. It represents the prediction, and\nground-truth as a tree structure of HTML tags. This simi-\nlarity is calculated as:\nTEDS (Ta,Tb) = 1−EditDist (Ta,Tb)\nmax (|Ta|,|Tb|) (3)\nwhere Ta and Tb represent tables in tree structure HTML\nformat. EditDist denotes the tree-edit distance, and |T|rep-\nresents the number of nodes in T.\n5.4. Quantitative Analysis\nStructure. As shown in Tab. 2, TableFormer outper-\nforms all SOTA methods across different datasets by a large\nmargin for predicting the table structure from an image.\nAll the more, our model outperforms pre-trained methods.\nDuring the evaluation we do not apply any table ﬁltering.\nWe also provide our baseline results on the SynthTabNet\ndataset. It has been observed that large tables (e.g. tables\nthat occupy half of the page or more) yield poor predictions.\nWe attribute this issue to the image resizing during the pre-\nprocessing step, that produces downsampled images with\nindistinguishable features. This problem can be addressed\nby treating such big tables with a separate model which ac-\ncepts a large input image size.\nModel TEDS\nDataset Simple Complex All\nEDD PTN 91.1 88.7 89.9\nGTE PTN - - 93.01\nTableFormer PTN 98.5 95.0 96.75\nEDD FTN 88.4 92.08 90.6\nGTE FTN - - 87.14\nGTE (FT) FTN - - 91.02\nTableFormer FTN 97.5 96.0 96.8\nEDD TB 86.0 - 86.0\nTableFormer TB 89.6 - 89.6\nTableFormer STN 96.9 95.7 96.7\nTable 2: Structure results on PubTabNet (PTN), FinTabNet\n(FTN), TableBank (TB) and SynthTabNet (STN).\nFT: Model was trained on PubTabNet then ﬁnetuned.\nCell Detection.Like any object detector, our Cell BBox\nDetector provides bounding boxes that can be improved\nwith post-processing during inference. We make use of the\ngrid-like structure of tables to reﬁne the predictions. A de-\ntailed explanation on the post-processing is available in the\nsupplementary material. As shown in Tab. 3, we evaluate\nour Cell BBox Decoder accuracy for cells with a class la-\nbel of ‘content’ only using the PASCAL VOC mAP metric\nfor pre-processing and post-processing. Note that we do\nnot have post-processing results for SynthTabNet as images\nare only provided. To compare the performance of our pro-\nposed approach, we’ve integrated TableFormer’sCell BBox\nDecoder into EDD architecture. As mentioned previously,\nthe Structure Decoder provides theCell BBox Decoder with\nthe features needed to predict the bounding box predictions.\nTherefore, the accuracy of the Structure Decoder directly\ninﬂuences the accuracy of the Cell BBox Decoder . If the\nStructure Decoder predicts an extra column, this will result\nin an extra column of predicted bounding boxes.\nModel Dataset mAP mAP (PP)\nEDD+BBox PubTabNet 79.2 82.7\nTableFormer PubTabNet 82.1 86.8\nTableFormer SynthTabNet 87.7 -\nTable 3: Cell Bounding Box detection results on PubTab-\nNet, and FinTabNet. PP: Post-processing.\nCell Content. In this section, we evaluate the entire\npipeline of recovering a table with content. Here we put\nour approach to test by capitalizing on extracting content\nfrom the PDF cells rather than decoding from images. Tab.\n4 shows the TEDs score of HTML code representing the\nstructure of the table along with the content inserted in the\ndata cell and compared with the ground-truth. Our method\nachieved a5.3% increase over the state-of-the-art, and com-\nmercial solutions. We believe our scores would be higher\nif the HTML ground-truth matched the extracted PDF cell\ncontent. Unfortunately, there are small discrepancies such\nas spacings around words or special characters with various\nunicode representations.\nModel TEDS\nSimple Complex All\nTabula 78.0 57.8 67.9\nTraprange 60.8 49.9 55.4\nCamelot 80.0 66.0 73.0\nAcrobat Pro 68.9 61.8 65.3\nEDD 91.2 85.4 88.3\nTableFormer 95.4 90.1 93.6\nTable 4: Results of structure with content retrieved using\ncell detection on PubTabNet. In all cases the input is PDF\ndocuments with cropped tables.\n7\nb. Structure predicted by TableFormer, with superimposed matched PDF cell text:\nJapanese language (previously unseen by TableFormer): Example table from FinTabNet:\na. Red - PDF cells, Green - predicted bounding boxes, Blue - post-processed predictions matched to PDF cells\n論文ファイル 参考文献\n出典 ファイル数 英語 日本語 英語 日本語\nAssociation for Computational Linguistics(ACL2003) 65 65 0 150 0\nComputational Linguistics(COLING2002) 140 140 0 150 0\n電気情報通信学会 2003年総合大会 150 8 142 223 147\n情報処理学会第 65回全国大会 (2003) 177 1 176 150 236\n第 17回人工知能学会全国大会 (2003) 208 5 203 152 244\n自然言語処理研究会第 146〜 155回 98 2 96 150 232\nWWWから収集した論文 107 73 34 147 96\n計 945 294 651 1122 955\nText is aligned to match original for ease of viewing\nWeightedAverage Grant Date Fair\nValue\nRSUs\nShares (in millions)\nPSUs RSUs PSUs\nNonvested on January 1 1.1 0.3 90.10 $ $ 91.19\nGranted 0.5 0.1 117.44 122.41\nVested (0.5) (0.1) 87.08 81.14\nCanceled or forfeited (0.1) — 102.01 92.18\nNonvested on December 31 1.0 0.3 104.85 $ $ 104.51\nFigure 5: One of the beneﬁts of TableFormer is that it is language agnostic, as an example, the left part of the illustration\ndemonstrates TableFormer predictions on previously unseen language (Japanese). Additionally, we see that TableFormer is\nrobust to variability in style and content, right side of the illustration shows the example of the TableFormer prediction from\nthe FinTabNet dataset.\nRed - PDF cells, Green - predicted bounding boxesGround T ruth\n16 17 18 19 20 21 22\n23 24 25 26 27 28\n30 31 32 33 34 35 36 37 38 39 40 41\n42 43 44 45 46 47 48 49 50 51 52 53\n0 1 2 3 4 5 6 7 8 9 10 11\n12 13 14\n15 29\nPredicted Structure\nFigure 6: An example of TableFormer predictions (bounding boxes and structure) from generated SynthTabNet table.\n5.5. Qualitative Analysis\nWe showcase several visualizations for the different\ncomponents of our network on various “complex” tables\nwithin datasets presented in this work in Fig. 5 and Fig. 6\nAs it is shown, our model is able to predict bounding boxes\nfor all table cells, even for the empty ones. Additionally,\nour post-processing techniques can extract the cell content\nby matching the predicted bounding boxes to the PDF cells\nbased on their overlap and spatial proximity. The left part\nof Fig. 5 demonstrates also the adaptability of our method\nto any language, as it can successfully extract Japanese\ntext, although the training set contains only English content.\nWe provide more visualizations including the intermediate\nsteps in the supplementary material. Overall these illustra-\ntions justify the versatility of our method across a diverse\nrange of table appearances and content type.\n6. Future Work & Conclusion\nIn this paper, we presented TableFormer an end-to-end\ntransformer based approach to predict table structures and\nbounding boxes of cells from an image. This approach en-\nables us to recreate the table structure, and extract the cell\ncontent from PDF or OCR by using bounding boxes. Ad-\nditionally, it provides the versatility required in real-world\nscenarios when dealing with various types of PDF docu-\nments, and languages. Furthermore, our method outper-\nforms all state-of-the-arts with a wide margin. Finally, we\nintroduce “SynthTabNet” a challenging synthetically gen-\nerated dataset that reinforces missing characteristics from\nother datasets.\nReferences\n[1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\n8\nend object detection with transformers. In Andrea Vedaldi,\nHorst Bischof, Thomas Brox, and Jan-Michael Frahm, edi-\ntors, Computer Vision – ECCV 2020, pages 213–229, Cham,\n2020. Springer International Publishing. 5\n[2] Zewen Chi, Heyan Huang, Heng-Da Xu, Houjin Yu, Wanx-\nuan Yin, and Xian-Ling Mao. Complicated table structure\nrecognition. arXiv preprint arXiv:1908.04729, 2019. 3\n[3] Bertrand Couasnon and Aurelie Lemaitre. Recognition of Ta-\nbles and Forms, pages 647–677. Springer London, London,\n2014. 2\n[4] Herv ´e D ´ejean, Jean-Luc Meunier, Liangcai Gao, Yilun\nHuang, Yu Fang, Florian Kleber, and Eva-Maria Lang. IC-\nDAR 2019 Competition on Table Detection and Recognition\n(cTDaR), Apr. 2019. http://sac.founderit.com/. 2\n[5] Basilios Gatos, Dimitrios Danatsas, Ioannis Pratikakis, and\nStavros J Perantonis. Automatic table detection in document\nimages. In International Conference on Pattern Recognition\nand Image Analysis, pages 609–618. Springer, 2005. 2\n[6] Max G ¨obel, Tamir Hassan, Ermelinda Oro, and Giorgio Orsi.\nIcdar 2013 table competition. In 2013 12th International\nConference on Document Analysis and Recognition , pages\n1449–1453, 2013. 2\n[7] EA Green and M Krishnamoorthy. Recognition of tables\nusing table grammars. procs. In Symposium on Document\nAnalysis and Recognition (SDAIR’95), pages 261–277. 2\n[8] Khurram Azeem Hashmi, Alain Pagani, Marcus Liwicki, Di-\ndier Stricker, and Muhammad Zeshan Afzal. Castabdetec-\ntors: Cascade network for table detection in document im-\nages with recursive feature pyramid and switchable atrous\nconvolution. Journal of Imaging, 7(10), 2021. 1\n[9] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE International\nConference on Computer Vision (ICCV), Oct 2017. 1\n[10] Yelin He, X. Qi, Jiaquan Ye, Peng Gao, Yihao Chen, Bing-\ncong Li, Xin Tang, and Rong Xiao. Pingan-vcgroup’s so-\nlution for icdar 2021 competition on scientiﬁc table image\nrecognition to latex. ArXiv, abs/2105.01846, 2021. 2\n[11] Jianying Hu, Ramanujan S Kashi, Daniel P Lopresti, and\nGordon Wilfong. Medium-independent table detection. In\nDocument Recognition and Retrieval VII , volume 3967,\npages 291–302. International Society for Optics and Photon-\nics, 1999. 2\n[12] Matthew Hurst. A constraint-based approach to table struc-\nture derivation. In Proceedings of the Seventh International\nConference on Document Analysis and Recognition - Volume\n2, ICDAR ’03, page 911, USA, 2003. IEEE Computer Soci-\nety. 2\n[13] Thotreingam Kasar, Philippine Barlas, Sebastien Adam,\nCl´ement Chatelain, and Thierry Paquet. Learning to detect\ntables in scanned document images using line information.\nIn 2013 12th International Conference on Document Analy-\nsis and Recognition, pages 1185–1189. IEEE, 2013. 2\n[14] Pratik Kayal, Mrinal Anand, Harsh Desai, and Mayank\nSingh. Icdar 2021 competition on scientiﬁc table image\nrecognition to latex, 2021. 2\n[15] Harold W Kuhn. The hungarian method for the assignment\nproblem. Naval research logistics quarterly , 2(1-2):83–97,\n1955. 6\n[16] Girish Kulkarni, Visruth Premraj, Vicente Ordonez, Sag-\nnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and\nTamara L. Berg. Babytalk: Understanding and generat-\ning simple image descriptions. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence , 35(12):2891–2903,\n2013. 4\n[17] Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming\nZhou, and Zhoujun Li. Tablebank: A benchmark dataset\nfor table detection and recognition, 2019. 2, 3\n[18] Yiren Li, Zheng Huang, Junchi Yan, Yi Zhou, Fan Ye, and\nXianhui Liu. Gfte: Graph-based ﬁnancial table extraction.\nIn Alberto Del Bimbo, Rita Cucchiara, Stan Sclaroff, Gio-\nvanni Maria Farinella, Tao Mei, Marco Bertini, Hugo Jair\nEscalante, and Roberto Vezzani, editors, Pattern Recogni-\ntion. ICPR International Workshops and Challenges , pages\n644–658, Cham, 2021. Springer International Publishing. 2,\n3\n[19] Nikolaos Livathinos, Cesar Berrospi, Maksym Lysak, Vik-\ntor Kuropiatnyk, Ahmed Nassar, Andre Carvalho, Michele\nDolﬁ, Christoph Auer, Kasper Dinkla, and Peter Staar. Ro-\nbust pdf document conversion using recurrent neural net-\nworks. Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, 35(17):15137–15145, May 2021. 1\n[20] Rujiao Long, Wen Wang, Nan Xue, Feiyu Gao, Zhibo Yang,\nYongpan Wang, and Gui-Song Xia. Parsing table structures\nin the wild. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 944–952, 2021. 2\n[21] Shubham Singh Paliwal, D Vishwanath, Rohit Rahul,\nMonika Sharma, and Lovekesh Vig. Tablenet: Deep learn-\ning model for end-to-end table detection and tabular data ex-\ntraction from scanned document images. In 2019 Interna-\ntional Conference on Document Analysis and Recognition\n(ICDAR), pages 128–133. IEEE, 2019. 1\n[22] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\nAndreas Kopf, Edward Yang, Zachary DeVito, Martin Rai-\nson, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An im-\nperative style, high-performance deep learning library. In H.\nWallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E.\nFox, and R. Garnett, editors, Advances in Neural Informa-\ntion Processing Systems 32, pages 8024–8035. Curran Asso-\nciates, Inc., 2019. 6\n[23] Devashish Prasad, Ayan Gadpal, Kshitij Kapadni, Manish\nVisave, and Kavita Sultanpure. Cascadetabnet: An approach\nfor end to end table detection and structure recognition from\nimage-based documents. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\nWorkshops, pages 572–573, 2020. 1\n[24] Shah Rukh Qasim, Hassan Mahmood, and Faisal Shafait.\nRethinking table recognition using graph neural networks.\nIn 2019 International Conference on Document Analysis and\nRecognition (ICDAR), pages 142–147. IEEE, 2019. 3\n[25] Hamid Rezatoﬁghi, Nathan Tsoi, JunYoung Gwak, Amir\nSadeghian, Ian Reid, and Silvio Savarese. Generalized in-\ntersection over union: A metric and a loss for bounding box\nregression. In Proceedings of the IEEE/CVF Conference on\n9\nComputer Vision and Pattern Recognition , pages 658–666,\n2019. 6\n[26] Sebastian Schreiber, Stefan Agne, Ivo Wolf, Andreas Den-\ngel, and Sheraz Ahmed. Deepdesrt: Deep learning for detec-\ntion and structure recognition of tables in document images.\nIn 2017 14th IAPR International Conference on Document\nAnalysis and Recognition (ICDAR), volume 01, pages 1162–\n1167, 2017. 1\n[27] Sebastian Schreiber, Stefan Agne, Ivo Wolf, Andreas Den-\ngel, and Sheraz Ahmed. Deepdesrt: Deep learning for de-\ntection and structure recognition of tables in document im-\nages. In 2017 14th IAPR international conference on doc-\nument analysis and recognition (ICDAR) , volume 1, pages\n1162–1167. IEEE, 2017. 3\n[28] Faisal Shafait and Ray Smith. Table detection in heteroge-\nneous documents. In Proceedings of the 9th IAPR Interna-\ntional Workshop on Document Analysis Systems , pages 65–\n72, 2010. 2\n[29] Shoaib Ahmed Siddiqui, Imran Ali Fateh, Syed Tah-\nseen Raza Rizvi, Andreas Dengel, and Sheraz Ahmed.\nDeeptabstr: Deep learning based table structure recognition.\nIn 2019 International Conference on Document Analysis and\nRecognition (ICDAR), pages 1403–1409. IEEE, 2019. 3\n[30] Peter W J Staar, Michele Dolﬁ, Christoph Auer, and Costas\nBekas. Corpus conversion service: A machine learning plat-\nform to ingest documents at scale. In Proceedings of the\n24th ACM SIGKDD, KDD ’18, pages 774–782, New York,\nNY , USA, 2018. ACM. 1\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Il-\nlia Polosukhin. Attention is all you need. In I. Guyon,\nU. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-\nwanathan, and R. Garnett, editors, Advances in Neural In-\nformation Processing Systems 30, pages 5998–6008. Curran\nAssociates, Inc., 2017. 5\n[32] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Du-\nmitru Erhan. Show and tell: A neural image caption gen-\nerator. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), June 2015. 2\n[33] Wenyuan Xue, Qingyong Li, and Dacheng Tao. Res2tim:\nreconstruct syntactic structures from table images. In 2019\nInternational Conference on Document Analysis and Recog-\nnition (ICDAR), pages 749–755. IEEE, 2019. 3\n[34] Wenyuan Xue, Baosheng Yu, Wen Wang, Dacheng Tao,\nand Qingyong Li. Tgrnet: A table graph reconstruction\nnetwork for table structure recognition. arXiv preprint\narXiv:2106.10598, 2021. 3\n[35] Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and\nJiebo Luo. Image captioning with semantic attention. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 4651–4659, 2016. 4\n[36] Xinyi Zheng, Doug Burdick, Lucian Popa, Peter Zhong, and\nNancy Xin Ru Wang. Global table extractor (gte): A frame-\nwork for joint table identiﬁcation and cell structure recogni-\ntion using visual context.Winter Conference for Applications\nin Computer Vision (WACV), 2021. 2, 3\n[37] Xu Zhong, Elaheh ShaﬁeiBavani, and Antonio Ji-\nmeno Yepes. Image-based table recognition: Data, model,\nand evaluation. In Andrea Vedaldi, Horst Bischof, Thomas\nBrox, and Jan-Michael Frahm, editors, Computer Vision –\nECCV 2020, pages 564–580, Cham, 2020. Springer Interna-\ntional Publishing. 2, 3, 7\n[38] Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes. Pub-\nlaynet: Largest dataset ever for document layout analysis. In\n2019 International Conference on Document Analysis and\nRecognition (ICDAR), pages 1015–1022, 2019. 1\n10\nTableFormer: Table Structure Understanding with Transformers\nSupplementary Material\n1. Details on the datasets\n1.1. Data preparation\nAs a ﬁrst step of our data preparation process, we have\ncalculated statistics over the datasets across the following\ndimensions: (1) table size measured in the number of rows\nand columns, (2) complexity of the table, (3) strictness of\nthe provided HTML structure and (4) completeness (i.e. no\nomitted bounding boxes). A table is considered to be simple\nif it does not contain row spans or column spans. Addition-\nally, a table has a strict HTML structure if every row has the\nsame number of columns after taking into account any row\nor column spans. Therefore a strict HTML structure looks\nalways rectangular. However, HTML is a lenient encoding\nformat, i.e. tables with rows of different sizes might still\nbe regarded as correct due to implicit display rules. These\nimplicit rules leave room for ambiguity, which we want to\navoid. As such, we prefer to have ”strict” tables, i.e. tables\nwhere every row has exactly the same length.\nWe have developed a technique that tries to derive a\nmissing bounding box out of its neighbors. As a ﬁrst step,\nwe use the annotation data to generate the most ﬁne-grained\ngrid that covers the table structure. In case of strict HTML\ntables, all grid squares are associated with some table cell\nand in the presence of table spans a cell extends across mul-\ntiple grid squares. When enough bounding boxes are known\nfor a rectangular table, it is possible to compute the geo-\nmetrical border lines between the grid rows and columns.\nEventually this information is used to generate the missing\nbounding boxes. Additionally, the existence of unused grid\nsquares indicates that the table rows have unequal number\nof columns and the overall structure is non-strict. The gen-\neration of missing bounding boxes for non-strict HTML ta-\nbles is ambiguous and therefore quite challenging. Thus,\nwe have decided to simply discard those tables. In case of\nPubTabNet we have computed missing bounding boxes for\n48% of the simple and 69% of the complex tables. Regard-\ning FinTabNet, 68% of the simple and 98% of the complex\ntables require the generation of bounding boxes.\nFigure 7 illustrates the distribution of the tables across\ndifferent dimensions per dataset.\n1.2. Synthetic datasets\nAiming to train and evaluate our models in a broader\nspectrum of table data we have synthesized four types of\ndatasets. Each one contains tables with different appear-\nances in regard to their size, structure, style and content.\nEvery synthetic dataset contains 150k examples, summing\nup to 600k synthetic examples. All datasets are divided into\nTrain, Test and Val splits (80%, 10%, 10%).\nThe process of generating a synthetic dataset can be de-\ncomposed into the following steps:\n1. Prepare styling and content templates: The styling\ntemplates have been manually designed and organized into\ngroups of scope speciﬁc appearances (e.g. ﬁnancial data,\nmarketing data, etc.) Additionally, we have prepared cu-\nrated collections of content templates by extracting the most\nfrequently used terms out of non-synthetic datasets (e.g.\nPubTabNet, FinTabNet, etc.).\n2. Generate table structures: The structure of each syn-\nthetic dataset assumes a horizontal table header which po-\ntentially spans over multiple rows and a table body that\nmay contain a combination of row spans and column spans.\nHowever, spans are not allowed to cross the header - body\nboundary. The table structure is described by the parame-\nters: Total number of table rows and columns, number of\nheader rows, type of spans (header only spans, row only\nspans, column only spans, both row and column spans),\nmaximum span size and the ratio of the table area covered\nby spans.\n3. Generate content: Based on the dataset theme, a set of\nsuitable content templates is chosen ﬁrst. Then, this content\ncan be combined with purely random text to produce the\nsynthetic content.\n4. Apply styling templates: Depending on the domain\nof the synthetic dataset, a set of styling templates is ﬁrst\nmanually selected. Then, a style is randomly selected to\nformat the appearance of the synthesized table.\n5. Render the complete tables: The synthetic table is\nﬁnally rendered by a web browser engine to generate the\nbounding boxes for each table cell. A batching technique is\nutilized to optimize the runtime overhead of the rendering\nprocess.\n2. Prediction post-processing for PDF docu-\nments\nAlthough TableFormer can predict the table structure and\nthe bounding boxes for tables recognized inside PDF docu-\nments, this is not enough when a full reconstruction of the\noriginal table is required. This happens mainly due the fol-\nlowing reasons:\n11\nPubTabNetb. FinTabNet Table Bank\nTrain\nComplex\nSimple\nComplex\nSimple\nSimple\nVal\n100% 500K 10K\nTrain Test Val\n100% 91K 10K10K\nTrainTest Val\n100% 130K 5K 10K\nComplex\nNon\nStrict \nHTML\nStrict \nHTML\nSimple\n230K 280K 65K\nComplex\nNon \nStrict \nHTML\nStrict \nHTML\nSimple\n47K\nSimple\nNon \nStrict \nHTML\n145K\nComplex\nContain\nMissing \nbboxes Contain\nMissing \nbboxes\nDataset \ndoesn't \nprovide \nbboxes\nSimple\n230K 280K 65K\nComplex Simple\n47K\nSimple\n145K\nFigure 7: Distribution of the tables across different dimensions per dataset. Simple vs complex tables per dataset and split,\nstrict vs non strict html structures per dataset and table complexity, missing bboxes per dataset and table complexity.\n• TableFormer output does not include the table cell con-\ntent.\n• There are occasional inaccuracies in the predictions of\nthe bounding boxes.\nHowever, it is possible to mitigate those limitations by\ncombining the TableFormer predictions with the informa-\ntion already present inside a programmatic PDF document.\nMore speciﬁcally, PDF documents can be seen as a se-\nquence of PDF cells where each cell is described by its con-\ntent and bounding box. If we are able to associate the PDF\ncells with the predicted table cells, we can directly link the\nPDF cell content to the table cell structure and use the PDF\nbounding boxes to correct misalignments in the predicted\ntable cell bounding boxes.\nHere is a step-by-step description of the prediction post-\nprocessing:\n1. Get the minimal grid dimensions - number of rows and\ncolumns for the predicted table structure. This represents\nthe most granular grid for the underlying table structure.\n2. Generate pair-wise matches between the bounding\nboxes of the PDF cells and the predicted cells. The Intersec-\ntion Over Union (IOU) metric is used to evaluate the quality\nof the matches.\n3. Use a carefully selected IOU threshold to designate\nthe matches as “good” ones and “bad” ones.\n3.a. If all IOU scores in a column are below the thresh-\nold, discard all predictions (structure and bounding boxes)\nfor that column.\n4. Find the best-ﬁtting content alignment for the pre-\ndicted cells with good IOU per each column. The alignment\nof the column can be identiﬁed by the following formula:\nalignment = arg min\nc\n{Dc}\nDc = max{xc}−min{xc}\n(4)\nwhere c is one of {left, centroid, right }and xc is the x-\ncoordinate for the corresponding point.\n5. Use the alignment computed in step 4, to compute\nthe median x-coordinate for all table columns and the me-\ndian cell size for all table cells. The usage of median dur-\ning the computations, helps to eliminate outliers caused by\noccasional column spans which are usually wider than the\nnormal.\n6. Snap all cells with bad IOU to their corresponding\nmedian x-coordinates and cell sizes.\n7. Generate a new set of pair-wise matches between the\ncorrected bounding boxes and PDF cells. This time use a\nmodiﬁed version of the IOU metric, where the area of the\nintersection between the predicted and PDF cells is divided\nby the PDF cell area. In case there are multiple matches\nfor the same PDF cell, the prediction with the higher score\nis preferred. This covers the cases where the PDF cells are\nsmaller than the area of predicted or corrected prediction\ncells.\n8. In some rare occasions, we have noticed that Table-\nFormer can confuse a single column as two. When the post-\nprocessing steps are applied, this results with two predicted\ncolumns pointing to the same PDF column. In such case\nwe must de-duplicate the columns according to highest to-\ntal column intersection score.\n9. Pick up the remaining orphan cells. There could be\ncases, when after applying all the previous post-processing\nsteps, some PDF cells could still remain without any match\nto predicted cells. However, it is still possible to deduce\nthe correct matching for an orphan PDF cell by mapping its\nbounding box on the geometry of the grid. This mapping\ndecides if the content of the orphan cell will be appended to\nan already matched table cell, or a new table cell should be\ncreated to match with the orphan.\n9a. Compute the top and bottom boundary of the hori-\nzontal band for each grid row (min/max y coordinates per\nrow).\n9b. Intersect the orphan’s bounding box with the row\nbands, and map the cell to the closest grid row.\n9c. Compute the left and right boundary of the vertical\nband for each grid column (min/max xcoordinates per col-\numn).\n9d. Intersect the orphan’s bounding box with the column\nbands, and map the cell to the closest grid column.\n9e. If the table cell under the identiﬁed row and column\nis not empty, extend its content with the content of the or-\n12\nphan cell.\n9f. Otherwise create a new structural cell and match it\nwit the orphan cell.\nAditional images with examples of TableFormer predic-\ntions and post-processing can be found below.\nFigure 8: Example of a table with multi-line header.\nFigure 9: Example of a table with big empty distance be-\ntween cells.\nFigure 10: Example of a complex table with empty cells.\n13\nFigure 11: Simple table with different style and empty\ncells.\nFigure 12: Simple table predictions and post processing.\nFigure 13: Table predictions example on colorful table.\nFigure 14: Example with multi-line text.\n14\nFigure 15: Example with triangular table.\nFigure 16: Example of how post-processing helps to restore\nmis-aligned bounding boxes prediction artifact.\n15\nFigure 17: Example of long table. End-to-end example from initial PDF cells to prediction of bounding boxes, post process-\ning and prediction of structure.\n16"
}