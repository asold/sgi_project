{
    "title": "An Extended Clustering Algorithm for Statistical Language Models",
    "url": "https://openalex.org/W2950667851",
    "year": 2022,
    "authors": [
        {
            "id": null,
            "name": "Ueberla, Joerg P.",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2134237567",
        "https://openalex.org/W2158905201",
        "https://openalex.org/W2065449542",
        "https://openalex.org/W2950492505",
        "https://openalex.org/W2441154163",
        "https://openalex.org/W161643443",
        "https://openalex.org/W47415966",
        "https://openalex.org/W1874966221",
        "https://openalex.org/W2127314673",
        "https://openalex.org/W2084531783"
    ],
    "abstract": "Statistical language models frequently suffer from a lack of training data. This problem can be alleviated by clustering, because it reduces the number of free parameters that need to be trained. However, clustered models have the following drawback: if there is ``enough'' data to train an unclustered model, then the clustered variant may perform worse. On currently used language modeling corpora, e.g. the Wall Street Journal corpus, how do the performances of a clustered and an unclustered model compare? While trying to address this question, we develop the following two ideas. First, to get a clustering algorithm with potentially high performance, an existing algorithm is extended to deal with higher order N-grams. Second, to make it possible to cluster large amounts of training data more efficiently, a heuristic to speed up the algorithm is presented. The resulting clustering algorithm can be used to cluster trigrams on the Wall Street Journal corpus and the language models it produces can compete with existing back-off models. Especially when there is only little training data available, the clustered models clearly outperform the back-off models.",
    "full_text": "arXiv:cmp-lg/9412003v1  6 Dec 1994\nAn Extended Clustering Algorithm for\nStatistical Language Models\nJoerg P. Ueberla\nForum Technology - DRA Malvern\nSt. Andrews Road, Malvern\nWorcestershire, WR14 3PS, UK\nemail: ueberla@signal.dra.hmg.gb\nDRA/CIS(CSE1)/RN94/13\nOctober 25, 2018\nAbstract\nStatistical language models frequently suﬀer from a lack of t raining\ndata. This problem can be alleviated by clustering, because it reduces\nthe number of free parameters that need to be trained. Howeve r,\nclustered models have the following drawback: if there is “e nough”\ndata to train an unclustered model, then the clustered varia nt may\nperform worse. On currently used language modeling corpora , e.g. the\nWall Street Journal corpus, how do the performances of a clus tered\nand an unclustered model compare? While trying to address th is\nquestion, we develop the following two ideas. First, to get a clustering\nalgorithm with potentially high performance, an existing a lgorithm\nis extended to deal with higher order N-grams. Second, to mak e it\npossible to cluster large amounts of training data more eﬃci ently,\na heuristic to speed up the algorithm is presented. The resul ting\nclustering algorithm can be used to cluster trigrams on the W all Street\nJournal corpus and the language models it produces can compe te with\nexisting back-oﬀ models. Especially when there is only litt le training\ndata available, the clustered models clearly outperform th e back-oﬀ\nmodels.\n1\n1 Introduction\nIt is well known that statistical language models often suﬀer from a lack of\ntraining data. This is true for standard tasks and even more so whe n one tries\nto build a language model for a new domain, because a large corpus of texts\nfrom that domain is usually not available. One frequently used approa ch to\nalleviate this problem is to construct a clustered language model. Bec ause it\nhas fewer parameters, it needs less training data. The main advant age of a\nclustered model are its robustness, even in the face of little or spa rse training\ndata, and its compactness. Particularly when a language model is us ed during\nthe acoustic search in a speech recogniser, having a more compact , e.g. less\ncomplex model, can be of considerable importance. The main drawbac k of\nclustered models is that they may perform worse than an uncluster ed model,\nif there is “enough” data to train the latter. Do corpora currently used for\nlanguage modeling, e.g. the Wall Street Journal corpus, contain en ough data\nin that sense? Or, in other words, how does the performance of a c lustered\nmodel compare with that of an unclustered model? In this paper, we will\nattempt to partly answer this question and, along the way, an exte nded, more\neﬃcient clustering algorithm will be developed.\nIn the next section (section 2), a brief review of existing clustering al-\ngorithms will be given. For the work presented here, we use the clus tering\nalgorithm proposed in [8], because, in the spirit of decision directed lea rning,\nit uses an optimisation function that is very closely related to the ﬁna l per-\nformance measure we wish to maximise. Since the algorithm forms the basis\nof our work, its optimisation criterion is derived in detail.\nIn order to achieve a clustered model with potentially high performa nce,\nthe algorithm is then extended (section 3) so that it can cluster high er order\nN-grams. We present three possible approaches for this extensio n and then\ndevelop the one chosen for this work in more detail.\nWhen such a clustering algorithm is applied to a large training corpus,\ne.g. the Wall Street Journal corpus, with tens of millions of words, t he\ncomputational eﬀort required can easily become prohibitive. There fore, a\nsimple heuristic to speed up the algorithm is developed in section 4. Its\nmain idea is as follows. Rather than trying to move each word w to all\npossible clusters, as the algorithm requires initially, one only tries mov ing\nw to a ﬁxed number of clusters that have been selected from all poss ible\nclusters by a simple heuristic. This reduces the order of the complex ity of\n2\nthe algorithm. Of course, it may lead to a decrease in performance. However,\nin practice, the decrease in performance is minor (less than 5%), wh ereas the\nobtained speedup is large (up to a factor of 32).\nBecause of the increase in the speed of the algorithm, it can be applie d\nmore easily to the Wall Street Journal corpus and the obtained res ults will\nbe presented in section 5.\n2 Background and Related Work\nIn speech recognition, one is given a sequence of acoustic observa tions A and\none tries to ﬁnd the word sequence W ∗ that is most likely to correspond to A.\nIn order to minimise the average probability of error, one should, ac cording\nto Bayes’ decision rule ([3, p.17]), choose\nW ∗ = argmaxW p(W |A). (1)\nBased on Bayes’ formula (see for example [4, p.150]), one can rewrit e the\nprobability from the right hand side of equation 1 according to the fo llowing\nequation:\np(W |A) = p(W ) ∗ p(A|W )\np(A) . (2)\np(W ) is the probability that the word sequence W is spoken, p(A|W ) is the\nconditional probability that the acoustic signal A is observed when W is\nspoken and p(A) is the probability of observing the acoustic signal A. Based\non this formula, one can rewrite the maximization of equation 1 as\nW ∗ = argmaxW\np(W ) ∗ p(A|W )\np(A) . (3)\nSince p(A) is the same for all W , the factor p(A) does not inﬂuence the choice\nof W and maximising equation 3 is equivalent to maximising\nW ∗ = argmaxW p(W ) ∗ p(A|W ). (4)\nThe component of the speech recogniser that calculates p(A|W ) is called the\nacoustic model, the component calculating p(W ) the language model. With\n3\nW = w1, ..., wn, one can further decompose p(W ) using the deﬁnition of\nconditional probabilities as\np(W ) =\ni=n∏\ni=1\np(wi|w1, ..., wi−1). (5)\nIn practice, because of the large number of parameters in equatio n 5, the\nprobability of wi usually only depends on the immediately preceding M\nwords:\np(wi|w1, ..., wi−1) ≈ p(wi|wi−M , ..., wi−1). (6)\nThese models are called ( M +1)-gram models and in practice, mostly bigram\n(M = 1) and trigram ( M = 2) models are used. Even in these cases, the\nnumber of parameters that need to be estimated from training dat a can be\nquite large. For a speech recogniser with a vocabulary of 20 ,000 words, the\nbigram needs to estimate roughly 20 ,0002 = 4 ∗108 parameters and a trigram\n20,0003 = 8 ∗ 1012.\nOne way to alleviate this problem is to use class based models. Let\nG : w → G(w) = gw be a function that maps each word w to its class\nG(w) = gw and let |G|denote the number of classes. We can then model the\nprobability of wi as\np(wi|w1, ..., wi−1) ≈ pG(wi|wi−M , ..., wi−1) (7)\n= pG(G(wi)|G(wi−M ), ..., G(wi−1)) ∗ pG(wi|G(wi)).(8)\nThus, if |G|=1000 classes are being used, the class-based bigram model 1 has\n1,0002 + 20,000 = 1 .02 ∗ 106 parameters and the class-based trigram model\n1,0003 + 20,000 = 1 .00002 ∗ 109. This constitutes a signiﬁcant reduction in\nboth cases.\nMany researchers have developed algorithms for determining the c lus-\ntering function G automatically (see for example [2], [5], [6], [8] and [12]).\nStarting from an initial clustering function, the basic principle often is to\nmove words from their current cluster to another cluster, but on ly if this\nimproves the value of an optimisation criterion. The algorithms often diﬀer\nin the optimisation criterion and in general, there are many possible ch oices\nfor it. However, in the spirit of decision-directed learning, it makes s ense to\n1This model is also sometimes referred to as bi-pos model, where pos s tands for Parts\nOf Speech.\n4\nuse as optimisation criterion a function that is very closely related or iden-\ntical to the ﬁnal performance measure we wish to maximise. This way , one\ncan be very conﬁdent that an improvement in the optimisation criter ion will\nactually translate to an improvement of performance. We therefo re chose\nthe algorithm proposed in [8] as the basis for our work. In the followin g,\nthe optimisation criterion for a bigram based model (e.g. M = 1) will be\nderived, roughly as presented in [8].\nIn order to automatically ﬁnd classiﬁcation functions G, the classiﬁcation\nproblem is ﬁrst converted into an optimisation problem. Suppose the function\nF (G) indicates how good the classiﬁcation G is. One can then reformulate\nthe classiﬁcation problem as ﬁnding the classiﬁcation G∗ that maximises F:\nG∗ = argmaxG∈GF (G), (9)\nwhere G contains the set of possible classiﬁcations which are at our disposal.\nWhat is a suitable function F , also called optimisation criterion? Given\na classiﬁcation function G, the probabilities pG(w|v) of equation 8 can be\nestimated using the maximum likelihood (ML) estimator, e.g. relative fr e-\nquencies:\npG(w|v) = p(G(w)|G(v)) ∗ p(w|G(w)) (10)\n= N(G(v), G(w))\nN(G(v)) ∗ N(G(w), y)\nN(G(w)) , (11)\nwhere N(x) denotes the number of times x occurs in the training data. Given\nthese probability estimates pG(w|v), the likelihood FML of the training data,\ne.g. the probability of the training data being generated by our prob ability\nestimates pG(w|v), measures how well the training data is represented by the\nestimates and can be used as optimisation criterion ([6]). The likelihood of\nthe training data FML is simply\nFML =\nN∏\ni=1\npG(wi|wi−1) (12)\n=\nN∏\ni=1\nN(gwi−1 , gwi )\nN(gwi−1 ) ∗ N(gwi , wi)\nN(gwi ) . (13)\nAssuming that the classiﬁcation is unique, e.g. that G is a function, N(gwi , wi) =\nN(wi) always holds (because wi always occurs with the same class gwi ). Since\n5\none tries to optimise FML with respect to G, any term that does not depend\non G can be removed, because it will not inﬂuence the optimisation. It is\nthus equivalent to optimise\nF\n′\nML =\nN∏\ni=1\nN(gwi−1 , gwi )\nN(gwi−1 ) ∗ 1\nN(gwi ) (14)\n=:\nN∏\ni=1\nf(wi−1, wi). (15)\nIf, for two pairs ( wi−1, wi) and ( wj−1, wj), G(wi−1) = G(wj−1) and G(wi) =\nG(wj) holds, then f(wi−1, wi) = f(wj−1, wj) is also true. Identical terms can\nthus be regrouped to obtain\nF\n′\nML =\n∏\ng1,g2\n[N(g1, g2)\nN(g1) ∗ 1\nN(g2)]N(g1,g2), (16)\nwhere the product is over all possible pairs ( g1, g2). Because N(g1) does not\ndepend on g2 and N(g2) does not depend on g1, this can again be simpliﬁed\nto\nF\n′\nML =\n∏\ng1,g2\nN(g1, g2)N(g1,g2) ∏\ng1\n1\nN(g1)\nN(g1) ∏\ng2\n1\nN(g2)\nN(g2)\n. (17)\nAfter taking the logarithm, one obtains the equivalent optimisation c riterion\nF\n′′\nML\nF\n′′\nML =\n∑\ng1,g2\nN(g1, g2) ∗ log(N(g1, g2)) −\n∑\ng1\nN(g1) ∗ log(N(g1)) (18)\n−\n∑\ng2\nN(g2) ∗ log(N(g2)).\nF\n′′\nML is the maximum likelihood optimisation criterion that can be used\nto ﬁnd a good classiﬁcation G. However, the problem with this maximum\nlikelihood criterion is that one ﬁrst estimates the probabilities pG(w|v) on the\ntraining data T and then, given pG(w|v), one evaluates the classiﬁcation G\non T . In other words, both the classiﬁcation G and the estimator pG(w|v) are\ntrained on the same data. Thus, there will not be any unseen event , a fact\nthat overestimates the power for generalisation of the class base d model. In\norder to avoid this, a cross-validation technique will be incorporate d directly\ninto the optimisation criterion in section 2.1.\n6\n2.1 Leaving-One-Out Criterion\nThe basic principle of cross-validation is to split the training data T into\na “retained” part TR and a “held-out” part TH . One can then use TR to\nestimate the probabilities pG(w|v) for a given classiﬁcation G, and TH to\nevaluate how well the classiﬁcation G performs. The so-called leaving-one-\nout technique is a special case of cross-validation ([3, pp.75]). It div ides the\ndata into N − 1 samples as “retained” part and only one sample as “held-out”\npart. This is repeated N − 1 times, so that each sample is once in the “held-\nout” part. The advantage of this approach is that all samples are u sed in the\n“retained” and in the “held-out” part, thus making very eﬃcient us e of the\nexisting data. In other words, the “held-out” part TH to evaluate a classiﬁca-\ntion G is the entire set of data points; but when we calculate the probability\nof the ith data point, one assumes that the probability distributions pG(w|v)\nwere estimated on all the data expect point i.\nLet Ti denote the data without the pair ( wi−1, wi) and pG,Ti (w|v) the\nprobability estimates based on a given classiﬁcation G and training corpus\nTi. Given a particular Ti, the probability of the “held-out” part ( wi−1, wi) is\npG,Ti (wi|wi−1). The probability of the complete corpus, where each pair is in\nturn considered the “held-out” part is the leaving-one-out likelihoo d LLO\nLLO =\nN∏\ni=1\npG,Ti (wi|wi−1). (19)\nIn the following, an optimisation function FLO will be derived by specifying\nhow pG,Ti (wi|wi−1) is estimated from frequency counts. First pG,Ti (wi|wi−1)\nis rewritten as usual (see equation 8):\npG,Ti (w|v) = pG,Ti (G(w)|G(v)) ∗ pG,Ti (w|G(w)) (20)\n= pG,Ti (g1, g2)\npG,Ti (g1) ∗ pG,Ti (g2, w)\npG,Ti (g2) , (21)\nwhere g1 = G(v) and g2 = G(w). Now we will specify how each term in\nequation 21 is estimated.\nAs shown before, pG,Ti (g2, w) = pG,Ti (w) (if the classiﬁcation G is a func-\ntion) and since pTi (w) is actually independent of G, one can drop it out of\nthe maximization and thus need not specify an estimate for it.\n7\nAs will be shown later, one can guarantee that every class g1 and g2 has\nbeen seen at least once in the “retained” part and one can thus use relative\ncounts as estimates for class uni-grams:\npG,Ti (g1) = NTi (g1)\nNTi\n(22)\npG,Ti (g2) = NTi (g2)\nNTi\n. (23)\nHowever, in the case of the class bi-gram, one might have to predict\nunseen events 2. We therefore use the absolute discounting method ([9]),\nwhere the counts are reduced by a constant value b < 1 and where the\ngained probability mass is redistributed over unseen events. Let n0,Ti be the\nnumber of unseen pairs ( g1, g2) and n+,Ti the number of seen pairs ( g1, g2).\nThis leads to the following smoothed estimate\npG,Ti (g1, g2)\n=\n\n\n\n\n\nNTi (g1,g2)−b\nNTi\nif NTi (g1, g2) > 0\nn+,Ti ∗b\nn0,Ti ∗NTi\nif NTi (g1, g2) = 0\n(24)\nIdeally, one would make b depend on the classiﬁcation, e.g. use b = n1\nn1+2∗n2\n,\nwhere n1 and n2 depend on G. However, due to computational reasons, we\nuse, as suggested in [8], the empirically determined constant value b = 0 .75\nduring clustering. The probability distribution pG,Ti (g1, g2) will always be\nevaluated on the “held-out” part ( wi−1, wi) and with g1,i = gwi−1 and g2,i =\ngwi , one obtains\npG,Ti (g1,i, g2,i)\n=\n\n\n\n\n\nNTi (g1,i,g2,i)−b\nNTi\nif NTi (g1,i, g2,i) > 0\nn+,Ti ∗b\nn0,Ti ∗NTi\nif NTi (g1,i, g2,i) = 0\n(25)\nIn order to facilitate future regrouping of terms, one can now exp ress the\ncounts NTi , NTi (g1) etc. in terms of the counts of the complete corpus T as\n2If ( wi− 1, wi) occurs only once in the complete corpus, then pG,Ti (wi|wi− 1) will have to\nbe calculated based on the corpus Ti, which does not contain any occurrences of ( wi− 1, wi).\n8\nfollows:\nNTi = NT − 1 (26)\nNTi (g1) = NT (g1) − 1 (27)\nNTi (g2) = NT (g2) − 1 (28)\nNTi (g1,i, g2,i) = NT (g1,i, g2,i) − 1 (29)\nNTi = NT − 1 (30)\nn+,Ti =\n{\nn+,T if NT (g1,i, g2,i) > 1\nn+,T − 1 if NT (g1,i, g2,i) = 1 (31)\nn0,Ti =\n{\nn0,T if NT (g1,i, g2,i) > 1\nn0,T − 1 if NT (g1,i, g2,i) = 1 (32)\nAll the expressions can now be substituted back into equation 19. A fter\ndropping pG,Ti (w) because it is independent of G, one arrives at\nF ′\nLO =\nN∏\ni=1\npG,Ti (g1,i, g2,i)\npG,Ti (g1,i) ∗ 1\npG,Ti (g2,i) (33)\n=\n∏\ng1,g2\n(pG,Ti (g1, g2))N(g1,g2) ∗\n∏\ng1\n( 1\npG,Ti (g1))N(g1) (34)\n∗\n∏\ng2\n( 1\npG,Ti (g2))N(g2).\nOne can now substitute equations 22, 23 and 25, using the counts o f the whole\ncorpus of equations 26 to 32 . After having dropped terms indepen dent of\nG, one obtains\nF ′′\nLO =\n∏\ng1,g2:N(g1,g2)>1\n(NT (g1, g2) − 1 − b)NT (g1,g2) ∗\n(\n(n+,T − 1) ∗ b\n(n0,T + 1)\n) n1,T\n(35)\n∗\n∏\ng1\n(\n1\n(NT (g1 − 1))\n) NT (g1)\n∗\n∏\ng2\n(\n1\n(NT (g2 − 1))\n) NT (g2)\n,\nwhere n1,T is the number of pairs ( g1, g2) seen exactly once in T (e.g. the\nnumber of pairs that will be unseen when used as “held-out” part). Taking\nthe logarithm, we obtain the ﬁnal optimisation criterion F ′′′\nLO\nF ′′′\nLO =\n∑\ng1,g2:NT (g1,g2)>1\nNT (g1, g2) ∗ log(NT (g1, g2) − 1 − b) (36)\n9\n+ n1,T ∗ log(b ∗ (n+,T − 1)\n(n0,T + 1) )\n−\n∑\ng1\nNT (g1) ∗ log(NT (g1) − 1) −\n∑\ng2\nNT (g2) ∗ log(NT (g2) − 1).\n2.2 Clustering Algorithm\nGiven the maximization criterion F ′′′\nLO, we use the algorithm in Figure 1 to\nﬁnd a good clustering function G. The algorithm tries to make local changes\nby moving words between classes, but only if it improves the value of t he\noptimisation function. The algorithm will converge because the optim isation\ncriterion is made up of logarithms of probabilities and thus has an uppe r limit\nand because the value of the optimisation criterion increases in each iteration.\nHowever, the solution found by this greedy algorithm is only locally opt imal\nand it depends on the starting conditions. Furthermore, since the clustering\nof one word aﬀects the future clustering of other words, the ord er in which\nwords are moved is important. As suggested in [8], the words are sor ted by\nthe number of times they occur such that the most frequent word s, about\nwhich one knows the most, are clustered ﬁrst. Moreover, infrequ ent words\n(e.g. words with occurrence counts smaller than 5) are not conside red for\nclustering, because the information they provide is not very reliable . Thus, if\none starts out with an initial clustering in which no cluster occurs only once,\nand if one never moves words that occur only once, then one will nev er have\na cluster which occurs only once. Thus, the assumption we made ear lier,\nwhen it was decided to estimate cluster uni-grams by frequency cou nts, can\nbe guaranteed.\nWe will now determine the complexity of the algorithm. Let C be the\nmaximal number of clusters for G, let E be the number of elements one tries\nto cluster (e.g. E = |V |), and let I be the number of iterations. When one\nmoves w from gw to g′\nw in the inner loop, one needs to change the counts\nN(gw, g2) and N(g′\nw, g2) for all g2. The amount by which the counts need\nto be changed is equal to the number of times w occurred with cluster g2.\nSince this amount is independent of g′\nw, one only needs to calculate it once\nfor each w. The amount can then be looked up in constant time within the\nloop, thus making the inner loop of order C. The inner loop is executed once\nfor every cluster w can be moved to, thus giving a complexity of the order\nof C2. For each w, one needed to calculate the number of times w occurred\n10\nAlgorithm 1: Clustering()\nstart with initial clustering function G\niterate until some convergence criterion is met\n{\nfor all w ∈ V\n{\nfor all g′\nw ∈ G\n{\ncalculate the diﬀerence in F (G) when w is moved from gw to\ng′\nw\n}\nmove w to g′\nw that results in biggest improvement in F (G)\n}\n}\nEnd Clustering\nFigure 1: The clustering algorithm\nwith all clusters g2. For that one has to sum up all the bigram counts\nN(w, v) : G(v) = g2, which is on the order of E, thus giving a complexity of\nthe order of E + C2. The two outer loops are executed I and E times, thus\ngiving a total complexity of the order of I ∗ E ∗ (E + C2).\n3 Extending the Clustering Algorithm to N-\ngrams\nIt is well known that a trigram model outperforms a bigram model if t here\nis suﬃcient training data. If we want our clustering algorithm to comp ete\nwith unclustered models on a corpus like the Wall Street Journal, whe re the\ntrigram indeed outperforms the bi-gram, it therefore seems logica l that the\nclustering algorithm should be extended to deal with trigrams (and h igher\norder N-grams) as well. The original clustered bigram model, as derived\nfrom equation 8, is\np(wi|wi−1) = pG(G(wi)|G(wi−1)) ∗ pG(wi|G(wi)). (37)\n11\nThere are at least three ways of extending the clustering to ( M + 1)-grams,\ndepending on how one models the probability p(wi|wi−M , ..., wi−1):\na) pG(G(wi)|G(wi−M ), ..., G(wi−1)) ∗ pG(wi|G(wi)) (38)\nb) pG(GM+1(wi)|GM (wi−M ), ..., G1(wi−1)) ∗ pG(wi|GM+1(wi)) (39)\nc) pG(G2(wi)|G1(wi−M , ..., wi−1)) ∗ pG(wi|G2(wi)) (40)\nThe tradeoﬀ between these models is one of accuracy versus comp lexity.\nApproach a), which only uses one clustering function G, could produce\n|G||V |\ndiﬀerent clusterings (for each word in V , it can choose one of the |G|clusters).\nApproach b), which uses M + 1 diﬀerent clustering functions, can represent\ni=M+1∑\ni=0\n|Gi|V\ndiﬀerent clusterings, including all the clusterings of approach a). Approach\nc), which uses one clustering function for the tuples wi−M , ..., wi−1 and one\nfor wi, can produce\n|G1|(|V |M ) + |G2||V |\npossible clusterings, including all the ones represented by approac h a) and\nb). Approach c) therefore has the highest potential for accuracy, as long\nas there is suﬃcient training data. Since the Wall Street Journal co rpus is\nvery large, we decided to use approach c). Please note that for M = 1,\napproach a) gives the traditional clustered bigram approach, but approache s\nb) and c) ( c) collapses to b) for M = 1) are more general than the traditional\nmodel. Moreover, approach c) is referred to in a recent publication ([10]) as\na two-sided (non symmetric) approach.\nSimilar to the derivation presented in section 2, one can now derive th e\noptimisation criterion for approach c). However, since it is very similar to the\nderivation shown in section 2, only the ﬁnal formulae will be given here . The\ncomplete derivation is given in appendix A. Let g1 and g2 denote clusters of G1\nand G2 respectively. The optimisation criterion for the extended algorithm\nis\nFLO =\n∑\ng1,g2:NT (g1,g2)>1\nNT (g1, g2) ∗ log(NT (g1, g2) − 1 − b) (41)\n12\nAlgorithm 2: Clustering()\nstart with initial clustering function G\niterate until some convergence criterion is met\n{\nfor all w ∈ V and t ∈ V M\n{\nfor all g′\nw ∈ G2 and g′\nt ∈ G1\n{\ncalculate the diﬀerence in F (G) when w/t is moved from\ngw/gt to g′\nw/g′\nt\n}\nmove the w/t to the g′\nw/g′\nt that results in the biggest improvement\nin F (G)\n}\n}\nEnd Clustering\nFigure 2: The extended clustering algorithm\n+ n1,T ∗ log(b ∗ (n+,T − 1)\n(n0,T + 1) )\n−\n∑\ng1\nNT (g1) ∗ log(NT (g1) − 1) −\n∑\ng2\nNT (g2) ∗ log(NT (g2) − 1).\nThe corresponding clustering algorithm, which is shown in ﬁgure 2, is a\nstraight forward extension of the one given in section 2. It’s comple xity can\nbe derived as follows. Let CG1 and CG2 be the maximal number of clusters\nfor G1 and G2, let E1 and E2 be the number of elements G1 and G2 try to\ncluster, let C = max(CG1 , CG2 ), E = max(E1, E2) and let I be the number of\niterations. When one moves w from gw to g′\nw in the inner loop (the situation is\nsymmetrical for t), one needs to change the counts N(gw, g2) and N(g′\nw, g2)\nfor all g2 ∈ G2. The amount by which the counts need to be changed is\nequal to the number of times w occurs with cluster g2. Since this amount\nis independent of g′\nw, one only needs to calculate it once for each w. The\namount can then be looked up in constant time within the loop, thus ma king\nthe inner loop of order C. The inner loop is executed once for every cluster\n13\nw can be moved to, thus giving a complexity of the order of C2. For each w,\none needed to calculate the number of times w occurred with all clusters g2.\nFor that, one has to sum up all the bigram counts N(w, t) : G2(t) = g2, which\nis on the order of E, thus giving a complexity of the order of E + C2. The\ntwo outer loops are executed I and E times thus giving a total complexity\nof the order of I ∗ E ∗ (E + C2). This is almost identical to the complexity\nof the bigram clustering algorithm given in section 2, except that E is now\nthe number of ( M + 1)-grams one wishes to cluster, rather than the number\nof unigrams (e.g. words of the vocabulary).\n4 Speeding up the Algorithm\nIf one wants to use the clustering algorithm on large corpora, the c omplexity\nof the algorithm becomes a crucial issue. As shown in the last two sec tions,\nthe complexity of the algorithm is O(I ∗ E ∗ (E + C2)), where C is the\nmaximally allowed number of clusters, I is the number of iterations and\nE is the number of elements to cluster ( |V | in case of bigrams, |V |M+1 in\ncase of the extended algorithm). C crucially determines the quality of the\nresulting language model and one would therefore like to chose it as b ig as\npossible. Unfortunately, because the algorithm is quadratic in C, this may\nbe very costly. We therefore developed the following heuristic to sp eed up\nthe algorithm.\nThe factor C2 comes from the fact that one tries to move a word w to\neach of the C possible clusters ( O(C)), and for each of these one has to\ncalculate the diﬀerence in the optimisation function ( O(C) again). If, based\non some heuristic, one could select a ﬁxed number t of target clusters, then\none could only try moving w to these t clusters, rather than to all possible\nclusters C. This may of course lead to the situation where one does not\nmove a word to the best possible cluster (because it was not selected by\nthe heuristic), and thus potentially to a decrease in performance. But this\ndecrease in performance depends of course on the heuristic func tion used and\nwe will come back to this issue when we look at the practical results.\nThe heuristic used in this work is as follows. For each cluster g1, one keeps\ntrack of the h clusters that most frequently co-occur with g1 in the tables\nN(g1, g2). For example, if g1 is a cluster of G1 (the situation is symmetric\nfor G2), then the h biggest entries in N(g1, g) are the h clusters being stored.\n14\nWhen one tries to move a word w, one also constructs a list of the h most\nfrequent clusters that follow w (one can get this for free as part of the factor\nE in ( E +C2)). One then simply calculates the number of clusters that are in\nboth lists and takes this as the heuristic score H(g1). The bigger H(g1), the\nmore similar are the distributions of w and g1, and the more likely it is that\ng1 is a good target cluster to which w should be moved. Because the length\nof the lists is a constant h calculating the heuristic score is also independent\nof C. One can thus calculate the heuristic score of all C clusters in O(C).\nHowever, once one has decided to move w to a given cluster, one would have\nto update the lists containing the h most frequent clusters following each\ncluster g1 (the lists might have changed due to the last moving of a word).\nSince the update is O(C) for a given g1, the update would again be O(C2) for\nall clusters. In order to avoid this, one can make another approxim ation at\nthis point. One can only update the list for the original and the new clu ster\nof w. The full update of all the lists is only performed after a certain num ber\nu of words have been moved.\nTo sum up, we can say that one can select t target clusters using the\nheuristic in O(C). Following that, one tries moving w to each of these t clus-\nters, which is again O(C). Moreover, several times per iteration (depending\non u), one updates the list of most frequent clusters which is O(C2). Thus, the\ncomplexity of the heuristic version of the algorithm is O(I∗(E∗(E+C)+C2)).\nThe complexity still contains the factor C2, but this time not within the in-\nner parenthesis. The factor C2 will thus be smaller than E ∗ (E + C), and is\nonly given for completeness.\nWe will now present a practical evaluation of the heuristic algorithm. The\nheuristic itself is parameterised by h, the number of most frequent clusters\none uses to calculate the heuristic score, t, the number of best ranked target\nclusters one tries to move word w to and u, the number indicating after how\nmany words a full update of the list of most frequent clusters is per formed.\nIn order to evaluate the heuristic for a given set of parameters, o ne can sim-\nply compare the ﬁnal value of the approximation function and the re sulting\nperplexity of the heuristic algorithm with that of the full algorithm.\nTable 1 contains a comparison of the results using approximately one\nmillion words of training data (from the Wall Street Journal corpus) and\nvalues t = 10, h = 10 and u = 1000. The CPU Time given was measured\non a DEC alpha workstation (DEC 3000, model 600), which was used in\nall the experiments reported in this paper. One can see that the ex ecution\n15\nClusters Standard Algorithm Heuristic Version\nPP CPU Time PP (∆ %) CPU Time (∆%)\n10 746 1:02 746 (0.0) 1:05 (4.8)\n20 630 2:28 653 (3.6) 1:35 (-36)\n40 548 7:46 558 (1.8) 2:36 (-67)\n80 477 26:41 490 (2.7) 4:30 (-83)\n160 421 1:33:29 437 (3.8) 7:59 (-91)\n320 394 5:20:18 402 (2.0) 14:17 (-96)\nTable 1: Comparison of the algorithm with its heuristic version\ntime of the standard algorithm seems indeed quadratic in the number of\nclusters, whereas that of the heuristic version seems to be linear. Moreover,\nthe perplexity of the heuristic version is always within 4% of that of th e\nstandard algorithm, a number which does not seem to increase syst ematically\nwith the number of clusters. Furthermore, the speed up of the alg orithm\nseems to be closely related to the number of clusters divided by t. For\nexample, in the case of 320 clusters, this ration is 320 /10 = 32 and the\nheuristic version is indeed almost 32 times as fast (the speed up is almo st 1 −\n1\n32 = 0 .97). Judging from the time behaviour of the standard algorithm, on e\nwould expect it to take around 32 hours to run with 1000 clusters, w hereas\nthe heuristic algorithm, as will be shown later, only takes about half a n hour\n(for t = 10).\nTables 2 to 4 contain a more detailed analysis of the inﬂuence of the\nparameters t, u, and h on the heuristic version of the algorithm, this time\nwith a maximal number of allowed clusters of 1000. The ﬁrst point to n ote is\nthat in all tables, a change in the value of the optimisation function is v ery\nclosely related to a change in perplexity. This is a very reassuring ﬁnd ing,\nbecause it indicates that the clustering algorithm actually tries to op timise\nthe correct criterion.\nFrom table 2, one can see that an increase in t leads to an increase in\nexecution time, but also to an increase in performance. This is becau se as t\nincreases, the chances of the heuristic missing the overall best ta rget cluster\nfor a given word w decreases.\nIn table 3, one can see that the eﬀect of u on the algorithm is very minor.\nThis could be explained by the fact that even though the full lists of m ost\nfrequent clusters are not updated at every move, the update in c lusters gw\n16\nt opt PP time\n5 -1.246e+07 359 20:24\n10 -1.243e+07 354 31:35\n20 -1.241e+07 350 55:32\n40 -1.240e+07 349 1:34:16\n80 -1.239e+07 348 2:51:19\nTable 2: Results for u = 1000 and h = 10\nu opt PP time\n4 -1.243e+07 352 36:27\n20 -1.243e+07 353 34:00\n100 -1.243e+07 353 34:00\n500 -1.243e+07 354 34:00\n2500 -1.243e+07 354 32:38\nTable 3: Results for t = 10 and h = 10\nand g′\nw, which is performed at every move, contains the most important\nchanges.\nFinally, in table 4, one can see that the performance of the algorithm\ndecreases with an increase in h. This in a way counter intuitive result could\nbe explained by the following hypothesis. If the suitability of a target cluster\nis determined by a small number of very frequently co-occurring clu sters, then\nincreasing h could make the heuristic perform worse, because the eﬀect of\nthe most important clusters is perturbed by a large number of less im portant\nclusters (the heuristic only counts the number of clusters in both lis ts and\ndoes not weigh them).\nBased on the results of these experiments, we chose h = 5, t = 10 and\nu = 1000 for future experiments with the heuristic version of the algo rithm.\nh opt PP time\n5 -1.243e+07 353 31:09\n10 -1.243e+07 353 33:09\n20 -1.245e+07 357 36:26\n40 -1.254e+07 370 41:50\nTable 4: Results for t = 10 and u = 10\n17\n5 Results\nIn the following, we will present results of clustered language models on the\nWall Street Journal (WSJ) corpus. The work reported here was p erformed on\nthe WSJ0 corpus, using the verbalised pronunciation (VP) and non v erbalised\npronunciation (NVP) versions of the corpus with the 20K open voca bulary.\nAs mentioned on the CDROM (and as discussed in [14]), the results for open\nvocabularies are usually not meaningful, if the unknown words are ta ken\ninto account when calculating the perplexity. One way to solve this pr oblem\n([13]) is to simply skip the unknown words, when calculating the perplex ity.\nSince all our experiments were performed with the open vocabulary , this\nis the approach taken here, except when indicated otherwise. In o rder to\ninvestigate the inﬂuence of the amount of training data on the resu lts, we\nused seven diﬀerent sets of training data, T 1 to T 7, with about 2K, 12K,\n60K, 350K, 1.7M, 8.5M and 40M words respectively. All perplexity res ults\nwere calculated on approximately 2.3 million words of text that were no t\npart of the training material. The clustered models were produced w ith the\nextended heuristic version of the algorithm. To run to completion, it took\nless than 12 hours real time for the bigram case, and several days for the\ntrigram case.\nAs a yardstick for the performance of the clustered models, we imp le-\nmented the commonly used compact back-oﬀ model ([7], [11]). Becau se the\nbigram counts were not smoothed, the probability mass, that could be re-\ndistributed to unseen events, was only gained through events tha t fell below\nthe cut-oﬀ threshold. If a given cut-oﬀ threshold did did not lead to any\ngained probability mass for a particular distribution (because no eve nt was\nbelow the threshold and thus no probability mass could be redistribut ed), the\ncut-oﬀ threshold of this distribution was set to the lowest value, th at would\nlead to some gain in probability mass. Table 5 and 6 give the perplexity\nof back-oﬀ models with various cut-oﬀ thresholds C for verbalised and non-\nverbalised pronunciation respectively. First, one can see that a big ger value\nof C leads to a higher perplexity. This is because as C increases, more and\nmore bigram counts are discarded and replaced by unigram, rather than bi-\ngram, probability estimates. However, a good reason why higher va lues of C\nmight still be of interest is that they lead to substantially smaller mode ls and\nthis can be of crucial importance for the time performance of a rec ogniser.\nSecond, and more importantly for our purposes, the results seem comparable\n18\nTraining Set C = 250 C = 50 C = 10 C = 2\nT1 2180 2180 2150 2240\nT2 1350 1320 1210 1130\nT3 1030 936 750 621\nT4 812 645 480 373\nT5 620 462 330 254\nT6 456 323 236 190\nT7 324 233 182 159\nTable 5: Back-oﬀ perplexity results on VP data\nTraining Set C = 250 C = 50 C = 10 C = 2\nT1 3170 3170 3150 3220\nT2 1980 1950 1790 1610\nT3 1440 1310 1060 878\nT4 1170 936 696 537\nT5 928 693 488 369\nT6 666 466 332 265\nT7 464 327 250 216\nTable 6: Back-oﬀ perplexity results on NVP data\n19\nTraining back-oﬀ clustered improvement (%)\nT1 2240 1750 22\nT2 1130 831 27\nT3 621 515 17\nT4 373 324 13\nT5 254 231 9.1\nT6 190 188 1.1\nT7 159 172 -8.2\nTable 7: Perplexity results for (2000,2000) bigram clusters (VP)\nTraining back-oﬀ clustered improvement(%)\nT1 3220 2420 25\nT2 1610 1230 24\nT3 878 762 13\nT4 537 491 8.6\nT5 369 345 6.5\nT6 265 267 -0.76\nT7 216 240 -11\nTable 8: Perplexity results for (2000,2000) bigram clusters (NVP)\nto other results reported in the literature. In [1] for example, the perplexity\nresults for the non-verbalised data with open vocabulary is 205, qu ite close\nto our 216 (for C = 2). However, it is quite likely that the probabilities of\nunknown words were taken into account for the calculation of the 2 05 value\nand our model also gives a perplexity of 205 in that case. The back-o ﬀ results\nof tables 5 and 6 therefore constitute a reasonable yardstick to e valuate the\nperformance of the clustered language models.\nTables 7 and 8 give the results of a clustered bigram with 2000 cluster s\nfor both G1 and G2, for verbalised and non-verbalised pronunciation respec-\ntively. For better comparison, the matching results of the back-o ﬀ models are\nrepeated and the diﬀerence is given in percent. Even though the clu stered\nmodel performs worse than the back-oﬀ model on the largest set of data, it\noutperforms the back-oﬀ model in almost all other cases. This clea rly shows\nthe superior robustness of the clustered models.\n20\nTraining clustered bigram clustered trigram improvement(%)\n(3000,3000) (7000,1000)\n5.2M 191 208 -8.9\n41M 167 151 9.6\nTable 9: Results for bigram and trigram clusters (VP)\nTable 9 shows the results for a clustered tri-gram 3 with 7000 and 1000\nclusters for G1 and G2 on VP data. Because these results were obtained on\nslightly diﬀerent training and testing texts, the table also contains t he results\nof the clustered bi-gram on the same data. One can see that the clu stered\ntrigram outperforms the clustered bigram, at least with suﬃcient t raining\ndata. But even with only ﬁve million words of training data, the cluster ed\ntrigram is only slightly worse than the clustered bigram, showing again the\nrobustness of the clustered language models.\nFrom all the results given here, one can see that the clustered lang uage\nmodels can still compete with unclustered models, even when a large c orpus,\nsuch as the Wall Street Journal corpus, is being used.\n6 Conclusions\nIn this paper, an existing clustering algorithm is extended to deal wit h higher\norder N-grams. Moreover, a heuristic version of the algorithm is introduce d,\nwhich leads to a very signiﬁcant speed up (up to a factor of 32), with only a\nslight loss in performance (5%). This makes it possible to apply the res ult-\ning algorithm to the clustering of bigrams and trigrams on the Wall Str eet\nJournal corpus. The results are shown to be comparable to stand ard back-oﬀ\nbigram models. Moreover, in the absence of many million words of train ing\ndata, the clustered model is more robust and clearly outperforms the non-\nclustered models. This is an important point, because for many real world\nspeech recognition applications, the amount of training data availab le for a\ncertain task or domain is in general unlikely to exceed several million wo rds.\nIn those cases, the clustered models seem like a good alternative to back-oﬀ\nmodels and certainly one that deserves close investigation.\n3Only the 500,000 most frequent bigrams were clustered using G1.\n21\nThe main advantage of the clustering models, its robustness in the f ace of\nlittle training data, can also be seen from the results and in these situ ations,\nthe clustered algorithm is preferable to the standard back-oﬀ mod els.\nAppendix A: Deriving the Optimisation Func-\ntion\nIn this appendix, we will present the derivation of the optimisation fu nction\nfor the extended clustering algorithm in detail. It is a generalisation o f [15],\nwhere the derivation was given for M = 1.\nLet G be a short hand to denote both classiﬁcation functions G1 and\nG2. Following the same approach as in section 2, one can estimate the\nprobabilities in equation 40 using the maximum likelihood estimator\npG(w|vM , ..., v1) = p(G2(w)|G1(vM , ..., v1)) ∗ p(w|G2(w)) (42)\n= N(g1, g2)\nN(g1) ∗ N(g2, y)\nN(g2) , (43)\nwhere g1 = G1(vM , ..., v1), g2 = G2(w) and N(x) denotes the number of times\nx occurs in the data. Given these probability estimates pG(w|vM , ..., v1), the\nlikelihood FML of the training data, e.g. the probability of the training\ndata being generated by our probability estimates pG(w|vM , ..., v1), measures\nhow well the training data is represented by the estimates and can b e used\nas optimisation criterion ([6]). The likelihood of the training data FML is\nsimply\nFML =\nN∏\ni=1\npG(wi|wi−M , ..., wi−1) (44)\n=\nN∏\ni=1\nN(G1(wi−M , ..., wi−1), G2(wi))\nN(G1(wi−M , ..., wi−1)) ∗ N(G2(wi), wi)\nN(G2(wi)) . (45)\nAssuming that the classiﬁcation is unique, e.g. that G1 and G2 are functions,\nN(G2(wi), wi) = N(wi) always holds (because wi always occurs with the same\nclass G2(wi)). Since one is trying to optimise FML with respect to G, one can\nremove any term that does not depend on G, because it will not inﬂuence\n22\nthe optimisation. It is thus equivalent to optimise\nF\n′\nML =\nN∏\ni=1\nN(G1(wi−M , ..., wi−1), G2(wi))\nN(G1(wi−M , ..., wi−1)) ∗ 1\nN(G2(wi)) (46)\n=:\nN∏\ni=1\nf(wi−M , ..., wi−1, wi). (47)\nIf, for two tuples ( wi−M , ..., wi−1, wi) and ( wj−M , ..., wj−1, wj), G1(wi−M , ..., wi−1) =\nG1(wj−M , ..., wj−1) and ( G2(wi) = G2(wj) is true, then f(wi−M , ..., wi−1, wi) =\nf(wj−M , ..., wj−1, wj) also holds. One can thus regroup identical terms to ob-\ntain\nF\n′\nML =\n∏\ng1,g2\n[N(g1, g2)\nN(g1) ∗ 1\nN(g2)]N(g1,g2), (48)\nwhere the product is over all possible pairs ( g1, g2). Because N(g1) does not\ndepend on g2 and N(g2) does not depend on g1, one can simplify this again\nto\nF\n′\nML =\n∏\ng1,g2\nN(g1, g2)N(g1,g2) ∏\ng1\n1\nN(g1)\nN(g1) ∏\ng2\n1\nN(g2)\nN(g2)\n. (49)\nTaking the logarithm, one obtains the equivalent optimisation criterio n\nF\n′′\nML =\n∑\ng1,g2\nN(g1, g2) ∗ log(N(g1, g2)) −\n∑\ng1\nN(g1) ∗ log(N(g1)) (50)\n−\n∑\ng2\nN(g2) ∗ log(N(g2)).\nF\n′′\nML is the maximum likelihood optimisation criterion which could be\nused to ﬁnd a good classiﬁcations G. However, the problem with this max-\nimum likelihood criterion is the same as in section 2. In the following, a\nleaving-one-out criterion is therefore developed.\nLet Ti denote the data without the pair ( wi−M , ..., wi−1, wi) and pG,Ti (w|vM , ..., v1)\nthe probability estimates based on a given classiﬁcation G and training\ncorpus Ti. Given a particular Ti, the probability of the “held-out” part\n(wi−M , ..., wi−1, wi) is pG,Ti (wi|wi−M , ..., wi−1). The probability of the com-\nplete corpus, where each pair is in turn considered the “held-out” p art is the\nleaving-one-out likelihood LLO\nLLO =\nN∏\ni=1\npG,Ti (wi|wi−M , ..., wi−1). (51)\n23\nIn the following, we will derive an optimisation function FLO by specifying\nhow pG,Ti (wi|wi−M , ..., wi−1) is estimated from frequency counts. One ﬁrst\nrewrites pG,Ti (wi|wi−M , ..., wi−1) as usual (see equation 40):\npG,Ti (w|vM , ..., v1) = PG,Ti (G2(w)|G1(vM , ..., v1)) ∗ PG,Ti (w|G2(w))(52)\n= pG,Ti (g1, g2)\npG,Ti (g1) ∗ pG,Ti (g2, w)\npG,Ti (g2) , (53)\nwhere g1 = G1(vM , ..., v1) and g2 = G2(w). Now we will specify how we\nestimate each term in equation 53.\nAs before, pG,Ti can be dropped from the optimisation criterion and rel-\native frequencies can be used as estimators for the class unigrams :\npG,Ti (g1) = NTi (g1)\nNTi\n(54)\npG,Ti (g2) = NTi (g2)\nNTi\n. (55)\nIn the case of the class bi-gram, one can again use the absolute disc ounting\nmethod for smoothing. Let n0,Ti be the number of unseen pairs ( g1, g2) and\nn+,Ti the number of seen pairs ( g1, g2), leading to the following smoothed\nestimate\npG,Ti (g1, g2)\n=\n\n\n\n\n\nNTi (g1,g2)−b\nNTi\nif NTi (g1, g2) > 0\nn+,Ti ∗b\nn0,Ti ∗NTi\nif NTi (g1, g2) = 0\n(56)\nAgain, the empirically determined constant value b = 0 .75 is used during\nclustering. The probability distribution pG,Ti (g1, g2) will always be evaluated\non the “held-out” part ( wi−M , ..., wi−1, wi) and with g1,i = G1(wi−M , ..., wi−1)\nand g2,i = G2(wi) one obtains\npG,Ti (g1,i, g2,i)\n=\n\n\n\n\n\nNTi (g1,i,g2,i)−b\nNTi\nif NTi (g1,i, g2,i) > 0\nn+,Ti ∗b\nn0,Ti ∗NTi\nif NTi (g1,i, g2,i) = 0\n(57)\n24\nIn order to facilitate future regrouping of terms, one again expre sses the\ncounts NTi , NTi (g1) etc. in terms of the counts of the complete corpus T as\nfollows:\nNTi = NT − 1 (58)\nNTi (g1) = NT (g1) − 1 (59)\nNTi (g2) = NT (g2) − 1 (60)\nNTi (g1,i, g2,i) = NT (g1,i, g2,i) − 1 (61)\nNTi = NT − 1 (62)\nn+,Ti =\n{\nn+,T if NT (g1,i, g2,i) > 1\nn+,T − 1 if NT (g1,i, g2,i) = 1 (63)\nn0,Ti =\n{\nn0,T if NT (g1,i, g2,i) > 1\nn0,T − 1 if NT (g1,i, g2,i) = 1 (64)\nAfter dropping pG,Ti (w) and substituting the expressions back into equation\n51, one obtains:\nF ′\nLO =\nN∏\ni=1\npG,Ti (g1,i, g2,i)\npG,Ti (g1,i) ∗ 1\npG,Ti (g2,i) (65)\n=\n∏\ng1,g2\n(pG,Ti (g1, g2))N(g1,g2) ∗\n∏\ng1\n( 1\npG,Ti (g1))N(g1) ∗\n∏\ng2\n( 1\npG,Ti (g2))N(g2).(66)\nOne can now substitute equations 54, 55 and 57, using the counts o f the whole\ncorpus of equations 58 to 64 . After having dropped terms indepen dent of\nG, one obtains\nF ′′\nLO =\n∏\ng1,g2:N(g1,g2)>1\n(NT (g1, g2) − 1 − b)NT (g1,g2) ∗\n(\n(n+,T − 1) ∗ b\n(n0,T + 1)\n) n1,T\n(67)\n∗\n∏\ng1\n(\n1\n(NT (g1 − 1))\n) NT (g1)\n∗\n∏\ng2\n(\n1\n(NT (g2 − 1))\n) NT (g2)\n,\nwhere n1,T is the number of pairs ( g1, g2) seen exactly once in T (e.g. the\nnumber of pairs that will be unseen when used as “held-out” part). Taking\nthe logarithm, one obtains the ﬁnal optimisation criterion F ′′′\nLO\nF ′′′\nLO =\n∑\ng1,g2:NT (g1,g2)>1\nNT (g1, g2) ∗ log(NT (g1, g2) − 1 − b) (68)\n25\n+ n1,T ∗ log(b ∗ (n+,T − 1)\n(n0,T + 1) )\n−\n∑\ng1\nNT (g1) ∗ log(NT (g1) − 1) −\n∑\ng2\nNT (g2) ∗ log(NT (g2) − 1).\nReferences\n[1] X. Aubert, C. Dugast, H. Ney, and V. Steinbiss. Large vocabula ry\ncontinuous speech recognition of wall street journal data. In Proceedings\nof International Conference on Acoustics, Speech and Signa l Processing,\n1994.\n[2] David Carter. Improving language models by clustering training se n-\ntences. In to appear in ANLP 94, Stuttgart Germany , 1994.\n[3] R. O. Duda and P.E. Hart. Pattern Classiﬁcation and Scene Analysis .\nWiley, New York, 1973.\n[4] John E. Freund. Modern Elementary Statistics . Prentice-Hall, Engle-\nwood Cliﬀs, New Jersey, 7th Edition, 1988.\n[5] Michele Jardino and Gilles Adda. Automatic word classiﬁcation us-\ning simulated annealing. In Proceedings of International Conference on\nAcoustics, Speech and Signal Processing , pages 41–43. Minneapolis, MN,\n1993.\n[6] Fred Jelinek. Self-organized language modeling for speech recog nition.\nIn Alex Waibel and Kai-Fu Lee, editors, Readings in Speech Recognition,\npages 450–506. Morgan Kaufmann, San Mateo, CA, 1990.\n[7] S. Katz. Estimation of probabilities from sparse data for the lang uage\nmodel component of a speech recognizer. IEEE Transactions on Acous-\ntics, Speech and Signal Processing , 35:400–401, 1987.\n[8] Reinhard Kneser and Hermann Ney. Improved clustering techniq ues\nfor class-based statistical language modelling. In European Conference\non Speech Communication and Technology , pages 973–976. Berlin, Ger-\nmany, September 1993.\n26\n[9] Hermann Ney and Ute Essen. Estimating ‘small’ probabilities by\nleaving-one-out. In European Conference on Speech Communication and\nTechnology, pages 2239–2242. Berlin, Germany, 1993.\n[10] Hermann Ney, Ute Essen, and Reinhard Kneser. On structurin g prob-\nabilistic dependencies in stochastic language modelling. Computer,\nSpeech and Language , 8:1:1–38, 1994.\n[11] Douglas B. Paul. Experience with a stack decoder-based HMM CS R\nand back-oﬀ n-gram language models. In Proceedings of DARPA Speech\nand Natural Language Workshop , pages 284–288, 1991.\n[12] Fernando Pereira, Naftali Tishby, and Lillian Lee. Distributional clus-\ntering of English words. In Proceedings of the Annual Meeting of the\nAssociation for Computational Linguistics , pages 183–190. Columbus,\nOH, 1993.\n[13] Ronald Rosenfeld. personal communication. 1994.\n[14] Joerg P. Ueberla. Analysing a simple language model - some gener al con-\nclusions for language models for speech recognition. Computer, Speech\nand Language , 8:153–176, 1994.\n[15] Joerg P. Ueberla. On using selectional restriction in language mo dels\nfor speech recognition. Technical report, School of Computing S cience,\nSimon Fraser University, Canada, CMPT TR 94-03, 1994.\n27"
}