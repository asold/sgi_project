{
    "title": "Transformer with sparse self‐attention mechanism for image captioning",
    "url": "https://openalex.org/W3019057642",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A3081230547",
            "name": "Duofeng Wang",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2100924466",
            "name": "Hai-Feng Hu",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2107136260",
            "name": "Dihu Chen",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A3081230547",
            "name": "Duofeng Wang",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2100924466",
            "name": "Hai-Feng Hu",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2107136260",
            "name": "Dihu Chen",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1861492603",
        "https://openalex.org/W639708223",
        "https://openalex.org/W2575842049",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2481240925",
        "https://openalex.org/W2963084599",
        "https://openalex.org/W2885013662",
        "https://openalex.org/W2745461083"
    ],
    "abstract": "Recently, transformer has been applied to the image caption model, in which the convolutional neural network and the transformer encoder act as the image encoder of the model, and the transformer decoder acts as the decoder of the model. However, transformer may suffer from the interference of non‐critical objects of a scene and meet with difficulty to fully capture image information due to its self‐attention mechanism's dense characteristics. In this Letter, in order to address this issue, the authors propose a novel transformer model with decreasing attention gates and attention fusion module. Specifically, they firstly use attention gate to force transformer to overcome the interference of non‐critical objects and capture objects information more efficiently via truncating all the attention weights that smaller than gate threshold. Secondly, through inheriting attentional matrix from the previous layer of each network layer, the attention fusion module enables each network layer to consider other objects without losing the most critical ones. Their method is evaluated using the benchmark Microsoft COCO dataset and achieves better performance compared to the state‐of‐the‐art methods.",
    "full_text": null
}