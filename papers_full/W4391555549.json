{
  "title": "Institutional Platform for Secure Self-Service Large Language Model Exploration.",
  "url": "https://openalex.org/W4391555549",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A4297260380",
      "name": "Bumgardner, V. K. Cody",
      "affiliations": [
        "University of Kentucky"
      ]
    },
    {
      "id": null,
      "name": "Klusty, Mitchell A.",
      "affiliations": [
        "University of Kentucky"
      ]
    },
    {
      "id": null,
      "name": "Logan, W. Vaiden",
      "affiliations": [
        "University of Kentucky"
      ]
    },
    {
      "id": "https://openalex.org/A4376740296",
      "name": "Armstrong, Samuel E.",
      "affiliations": [
        "University of Kentucky"
      ]
    },
    {
      "id": null,
      "name": "Leach, Caroline N.",
      "affiliations": [
        "University of Kentucky"
      ]
    },
    {
      "id": "https://openalex.org/A4275843944",
      "name": "Calvert Kenneth L.",
      "affiliations": [
        "University of Kentucky"
      ]
    },
    {
      "id": "https://openalex.org/A4302383144",
      "name": "Hickey, Caylin",
      "affiliations": [
        "University of Kentucky"
      ]
    },
    {
      "id": null,
      "name": "Talbert, Jeff",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1563486531",
    "https://openalex.org/W4319662928"
  ],
  "abstract": "This paper introduces a user-friendly platform developed by the University of Kentucky Center for Applied AI, designed to make customized large language models (LLMs) more accessible. By capitalizing on recent advancements in multi-LoRA inference, the system efficiently accommodates custom adapters for a diverse range of users and projects. The paper outlines the system's architecture and key features, encompassing dataset curation, model training, secure inference, and text-based feature extraction. We illustrate the establishment of a tenant-aware computational network using agent-based methods, securely utilizing islands of isolated resources as a unified system. The platform strives to deliver secure, affordable LLM services, emphasizing process and data isolation, end-to-end encryption, and role-based resource authentication. This contribution aligns with the overarching goal of enabling simplified access to cutting-edge AI models and technology in support of scientific discovery and the development of biomedical informatics.",
  "full_text": "Institutional Platform for Secure Self-Service Large Language Model \nExploration \nV . K. Cody Bumgardner, PhD1, Mitchell A. Klusty, BS1, W. Vaiden Logan, BS1, Samuel E. \nArmstrong, MS1, Caroline N. Leach, BS1, Kenneth L. Calvert, PhD1, Caylin Hickey, BS1, \nJeff Talbert, PhD1 \n1University of Kentucky, Lexington, KY, USA \n \nAbstract \nThis paper introduces a user -friendly platform developed by the University of Kentucky Center for Applied AI, \ndesigned to make customized large language models (LLMs) more accessible. By capitalizing on recent advancements \nin multi -LoRA inference, the system efficiently accommodates custom adapters for a diverse range of users and \nprojects. The paper outlines the system’ s architecture and key features, encompassing dataset curation, model training, \nsecure inference, and text-based feature extraction.  We illustrate the establishment of a tenant-aware computational \nnetwork using agent-based methods, securely utilizing islands of isolated resources as a unified system. The platform \nstrives to deliver secure, affordable LLM services, emphasizing process and data isolation, end-to-end encryption, and \nrole-based resource authentication. This contribution aligns with the overarching goal of enabling simplified access \nto cutting -edge AI models and technology in support of sc ientific discovery an d the development of biomedical \ninformatics. \n \nIntroduction \nGenerative pre-trained transformers (GPT) ha ve garnered great research and public interest. Driven by the public \navailability and ground -breaking performance of ChatGPT,1 an incredibly broad set of use cases and research areas \nare being developed. Generative models, such as Stable Diffusion2 and multi-modal models like GPT-43 further expand \nthe generative reach of AI through audio, images, and videos. While the hosted infrastructure of ChatGPT/GPT -4 \nprovides broad accessibility, users’ ability to adapt the service to their own domain was initially limited to how they \ninstructed (input content and structure) the model to respond, a process commonly referred to as “prompt \nengineering”4. Shortly after the release of ChatGPT, Meta publicly released Llama 5 and later Llama 2 and Llama 3,6 \nwhich, unlike ChatGPT and other commercial services offerings, could not only be used locally but also trained for \nspecific domains and fine -tuned for specific problems. While the practice of publicly releasing model architectures \nand pre -trained weights w as already commo n with BERT 7 and other derivative efforts,  the size of the Llama \nfoundational model  and the associated computational cost of training  such large language models (LLMs) were \nunprecedented. At the time of writing, there are dozens of popular foundational LLMs and tens of thousands of \nderivative models 8. Unfortunately, generative tasks are difficult to evaluate, and benchmarks can be easily \nmanipulated9. Therefore, model training processes and associated data set generation require a careful understanding \nof the problem domain and often human evaluation. \n       In large part, the success of the LLM ecosystem is due to the coordination of open-source libraries, models, and \ntools by the Hugging Face 10 community. The Hugging Face Transformers11 library has become the de facto standard \nfor LLM implementation s, allowing for broad interoperability and rapid adoption of new models and associated \ntechnologies. Numerous transformers -based platforms and tools, such as Large Model Systems Organization’s \n(LMSYS) FastChat12 and OpenAccess AI Collective’s Axolotl 13 have been de veloped for the in -house training and \nevaluation of LLMs. Likewise, cloud -based services such as AWS SageMaker 14 and Microsoft’s Machine Learning \nServices15 implement standard Hugging Face libraries and interoperable model standards. In addition to standard \nlibraries, projects such as vLLM 16 provide multi -user inference services with OpenAI -compatible API interfaces. \nSupport for the OpenAI API allows frameworks that are powered by language models, like LangChain,17 to seamlessly \nmake use of commercial and local LLMs. Most recently, Microsoft announced the Azure OpenAI Service,18 allowing \nusers to better adapt OpenAI models to their data and tasks. Through the combination of commercial and open-source \nefforts, a wide range of options are available for LLM training and inference. However, in either case, users are \nresponsible for the curation, integration, and protection of their data and associated models, which can influence what \nresources are available to specific use cases. \n       At the time of writing, commercial offerings like OpenAI’s GPT-4 dominate evaluation leaderboards, such as the \nLMsys Chatbot Arena.19 Despite the generally superior performance of proprietary models, a large and active open -\nsource LLM community has emerged. Along with standard supervised fine-tuning,20 numerous LLM technologies and \ntechniques have developed in attempts to meet or exceed the capabilities and performance of commercial services. \nFor example, additional pre -training21 with unstructured data and instruction fine-tuning with structured data  can be \nused to add domain -specific information to existing foundational models. In addition, Reinforcement Learning with \nHuman Feedback (RLHF) techniques such as Reward Modeling,22 Proximal Policy Optimization (PPO),23 and Direct \nPreference Optimization (DPO)24 are used to refine model behavior based on human preference. In addition to training \ntechniques, dozens of innovative technologies have been implemented to both improve efficiency and expand the \ncapabilities of available foundational models. For example, RoPE scaling25 extends the maximum context length of \npre-trained models, Flash Attention26 reduces training resource requirements, and Low Rank Adaptation ( LoRA) \nadapters27 efficiently decouple user-trained parameters from the underlying larger base models. In addition, numerous \nquantization techniques such as GPTQ,28 GGML,29 qLoRA,30 and AWQ31 have been developed to reduce model weight \nresolution, and associated resource costs, while attempting to maintain model accuracy. The rate of local LLM  \ncommunity development, combined with a diverse set of techniques, tools, and development options, has created a \nlearning curve that continues to grow steeper by the day, impeding the application of local LL Ms to potentially \nvaluable use cases. \n       Perhaps the greatest impediment to the broad operational deployment of local LLMs is the associated resource \ncosts. A modest-sized 7 billion parameter model requires \n14GB of GPU vRAM for inference when loaded at half \nresolution (fp16). As previously mentioned, model \nweights can be further quantized, reducing memory \nrequirements. However, the operational cost of running \nhundreds or thousands of unique models is financially \nprohibitive for many use cases. As with previous non -\nLLM natural language processing  models, tools such as \nNVIDIA ’s TensorRT-LLM32 have been developed to \noptimize model inference performance, reduce \ncomputational cost,  and manage parallelism for \nproduction models. Despite inference optimizations, \nresource costs associated with hosting large collections are \nsignificant. While LoRA techniques are used to decouple \nuser-trained parameters from base models, the common practice is to merge LoRA and base weights , effectively \ncreating a new, unique model. While only a fraction of the weights might have changed through LoRA training, once \nmerged, the costs of the changed and unchanged weights are encumbered. This process and associated cost are repeated \nfor each merged model, regardless of the global ratio of trained to duplicated parameters. Through a process called \nSegmented Gather Matrix-V ector multiplication (SGMV), a team of researchers was able to demonstrate the efficient \nuse of LoRA adapters without the need to first merge training and existing weights, as demonstrated in the Punica 33 \nproject. Derivative works such as S-LoRA34 have demonstrated the ability to host thousands of independent adapters \nconcurrently from the same base model. This is accomplished using a unified memory pool to dynamically manage \nadapter weights with differing ranks and varying sequence lengths. The efficiencies gained through multi-LoRA efforts \nhave reduced the resource costs of hosting large collections of models by orders of magnitude. Sharing the cost of \nhosting a base model between adapters allows larger base models to be used  while reducing the overall costs  and \nincreasing accessibility. \n       Locally trained LLMs are well-suited for biomedical informatic applications that require stringent  security and \ndata access controls. Utilizing Protected Health Information (PHI) requires a HIPAA-compliant system that ensures \npatient privacy. This cannot be guaranteed by general publicly available commercial LLMs. Local training and hosting \nof base models and  associated LoRA adapters allows organizations to securely use PHI data in training datasets and \nmodel queries, as those organizations can maintain the data management policies of the local models and ensure HIPAA \nstandards are followed within their system. Training models on custom ized datasets also provides an opportunity to \ncreate a wide range of informatics tools with specialized purposes. Potential applications of such a model could be a \ntrainer for drug counselors to simulate conversations with a patient, a tool for generating potential differential diagnoses \nusing patient information, or even a tool that generates treatment recommendations based on the latest information.  \nLocal LLMs allow us to create models that work from private data and operate within sensitive areas that commercial \nmodels have been explicitly trained to avoid.  \n       In a rapidly evolving area like generative AI, it can be difficult to identify the appropriate technologies for specific \nuse cases and even harder to put those technologies into use. In this paper, we present a self -service platform for the \nsecure end -to-end selection, training, evaluation, and private hosting of user -trained LLMs. Our system provides \nFigure 1. High-level system overview of models \n\ninteractive dataset curation interfaces, model configuration, composition tools, and agent-based methods for the secure \nhosting of custom LLMs and associated adapters.  This system operates within a secured environment that can be \nvalidated for HIPAA and more stringent compliance where necessary. It is designed to provide sustainably affordable \naccess to private LLMs, embedding extraction/search services, and transcription models, ensuring users can train and \ndeploy custom LLMs without the typical management complexity and overhead cost of hosting these models.  The \nsystem was also designed with full OpenAI API compatibility, meaning it meets the industry standards of accessibility \nto AI tools and can easily be integrated with a wide range of tools  and applications that were designed to work with \nOpenAI’s models. This opens our system to the broader ecosystem of AI technologies. \n \nMethods \nIn this section, we will describe the components of LLM Factory, explain architectural considerations, and discuss key \nfeatures. These features include  exposing LLM  inference through a n OpenAI compatible API,  training of LoRA \nadapters, multi-adapter inference, text-based feature extraction, and system security. \n       Figure 1 illustrates a high-level overview of our system’s components and data flow. Figure labels correspond to \nthe subsections presented in this section.  Figure 2 shows a more detailed diagram of the various components of the \nsystem that permit the creation of LoRA adapters and querying of available models. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nModel Inference \nAn LLM inference interface that is capable of switching between private models at time of execution  is essential to \nthe utility of this system. We began by creating an internal web interface that stores histories of chat sessions and gives \nthe user the ability to adjust the system prompt, maximum output tokens, temperature, and the weights of selected  \nadapters. It also can be put in “Comparison Mode,” which shows side -by-side comparisons of the model output with \nand without the active adapters. This permits the user to test various configurations of their generated adapters and \nparameters of the model in an internal playground environment and store the sessions for later review. \nFigure 2 System Overview Diagram \n         More important than an internal interface is an externally accessible API. We make use of the LoRAX35 \nserver for model inference. LoRAX supports multi-LoRA inference through a custom REST API, Python client library, \nand OpenAI-compatible API.  The multi-LoRA inference capabilities of LoRAX allow us to serve hundreds of private \nmodels in isolation from the same GPU, providing considerable efficiencies over standalone model deployments. Our \nOpenAI-compatible API allows our system to be used with other open and commercial libraries and applications that \nare OpenAI-compatible. We ensure security on this API by performing input format verification and content standard \nadherence before passing the request to LoRAX for inference. Security is further enhanced through an API key \nmanagement system, which is used to verify user access to specific models and adapters. It also logs requests for our \naudit system to prevent abuse and track token usage. This system permits users to easily generate and delete API keys, \nallowing independence between each of the user’s keys and swift rotation of compromised keys. \n        This API also permits  users to create frontend  interfaces that can be customized to their individual needs and \nutilize LLM Factory as a back end. For instance, we developed a WordPress plugin that allows seamless integration \nbetween LLM Factory and an interface deployable on WordPress sites. This capability provides users the flexibility to \ndesign unique user experiences tailored for specific tasks with the concerns of managing their own AI infrastructure \nalleviated by LLM Factory integration. \n \nAdapter Training \nLLM Factory enables efficient training of custom LoRA adapters, supporting both pretraining on unstructured text \nand generation of full adapters trained on structured JSON datasets. This system utilizes ClearML35, a job management \nservice to orchestrate training across standalone server training, local clusters, or cloud -based resources. In addition, \nClearML provides hyperparameter tuning optimization and enforces a consistent training and evaluation environment \nby tracking source code, libraries, and data used in the training process.  \n       Resulting LoRA adapters , related artifacts , and training datasets  are stored in a secure, locally hosted S3-\ncompatable server. This S3 server integrates  with ClearML and LoRAX to ensure all data is only accessible where \nnecessary and only transmitted when necessary, during training or inference. Data is never stored on or passed through \nClearML, rather, there is a reference to the relevant S3 object locations that are passed to and then accessed by the \ntraining machine. LoRAX provides the capability to load adapters from an S3 source file extremely quickly, permitting \nthe adapter files to only exist in the S3 server. \n       Our approach facilitates flexible and scalable development of LoRA adapters while ensuring compliance with \nsecurity protocols, simplifying the process of creating fine -tuned LLMs, especially in research environments that \nbenefit from tailored machine learning solutions. \n        We also expose a number of adapters to all users. These include  several code generation adapters, which have \nbeen trained on producing reliable code  from natural language inputs , a N eo4j Cypher query adapter, and an \nuncensored adapter, which allows the user to bypass restrictions on Llama 3 that prevent it from discussing sensitive \ntopics. These adapters are widely applicable to many areas of research , so they have been shared with all users to \nexpedite usage of LLM Factory. \n \nMulti-Adapter Inference \nAt the time of writing, the dominant paradigm for LLM distribution and inference is to use monolithic models. While \nLoRA techniques are widely used to train model adapters efficiently, the resulting adapters are typically merged with \nbase models prior to use. The result is that while custom training might only impact a small percentage of parameters, \nthe full cost of model storage and computation is incurred for each independent instance. \n       As previously mentioned, in late 2023, researchers representing Punica and S -LoRA projects demonstrated the \nability to host thousands of independent adapters concurrently from the same base model. This breakthrough in \nefficiency has enabled the cost-effective hosting of custom models in a way not previously possible, opening the door \nto much broader involvement in LLM efforts. The ability to support multi-adapter inference serves as the backbone of \nLLM Factory, allowing end -users to train and interac t with their models directly with limited costs compared to \nprevious techniques where the cost of each adapter matches the cost of hosting another base model. \n       We make use of the LoRAX 36 server for model inference. LoRAX supports multi -LoRA inference through a \ncustom REST API, Python client library, and OpenAI -compatible API. Each LoRAX server instance is bound to a \nsupported base model, such as Bloom, GPT2, Llama, Mistral, Mixtral, Phi, Q wen, and Zephyr.37 In addition to being \ntrained from a supported base model, adapters must be trained with LoRAX -supported targets,38 the components of \nthe model that will be affected by the adapter.  A known list of inference servers with associated base models and \ncandidate adapters is actively maintained by the system through a process that will be described under Secure \nComputational Network. Incoming requests are routed to appropriate inference servers based on the requested adapter \nand associated base model. The prompt and additional configuration options are included at the time of execution, as \nshown in Figure 3. \n \nFigure 3 Multi-LoRA Request \nAPI interactions between the user and the inference server are proxied through the system, where we isolate the users’ \ntraffic and inject the appropriate LoRA adapter based on the user’s session and associated permissions. The requests \nare made through our secure tunneling system detailed in Secure Computational Network. This process holds true for \nboth interactive chat and API-based access. The inference server request is regenerated for each request, allowing for \nthe translation of API protocols and future migration between inference servers. Outside of Punica and S -LoRA \nconceptual demonstration, LoRAX was one of the first projects to support multi -LoRA. However, many popular \ninference servers, such as vLLM, are implementing multi-LoRA support. Likewise, as new adapter technologies, such \nas LoHa,39 LoKr,40 and LoftQ41 are added to the supporting PEFT libraries, they are implemented in inference servers. \nThis allows our system to offer cutting edge adapter support and rapidly switch between adapters and merge multiple \ntogether in ways that would not be possible with other inference servers. \n \nVector Embeddings \nIn addition to various chat completion models, LLM Factory provides access vector embeddings models.  At the time \nof writing, LLM and embedding services are independent. V ector embeddings are numerical representations of objects, \nwords, sentences, or concepts in a vector space which captures semantic relationships between the objects the vectors \nrepresent. Large sets of data/documents can be encoded into vectors and stored in a vector database42. A method called \nRetrieval Augmented Generation (RAG) 43 is used to search a vector database  by encoding a search query and \nperforming a Euclidean distance calculation to find the closest related documents in the vector database  based on \nsemantic similarities between the query and the documents in the database. The extracted entities can then be used as \ncontext for an LLM, meaning vector databases can serve as vast knowledge stores for language models, allowing them \nto generate informed, contextually relevant responses.  \n       Embeddings provide a mechanism for enriching responses by injecting domain specific knowledge on a per-query \nbasis. Unlike LoRA adapters which adjust the LLM itself to specialize it in a particular domain, embeddings using \nRAG techniques allow dynamic retrieval of relevant information, enhancing the model’s knowledge without affecting \nits root language capabilities. This also provides security benefits for the encoded data. A custom permission schema \ncan be designed to limit the scope of the search to only include specific vertexes from the database based on the access \nrights verified in the request. This allows for strict control over which users and applications can extract which records \nfrom the database.  \n     In our system, we  exposed an embeddings model through an API that give the means to vectorize datasets and \nqueries, and, by extension, the means to perform similarity searches. We also wrote a Python library to integrate our \nsystem with the popular language model toolkit, LangChain. This library meshes the embeddings model of our system \nwith LangChain to ensure consistent vectorization. \n  \nAudio Transcriptions \nLLM Factory also hosts the OpenAI Whisper 44 transcription model, accessible through an OpenAI-compatible API \nendpoint. Whisper is an automatic speech recognition model that transcribe s audio files into text. The model offers \n\nmultilingual support and is noise-robust, meaning it can handle noisy environments. These qualities make the model \nextremely versatile for practical and research purposes. LLM Factory allows for fast transcription of short audio files \n(~30-60s) and provides the means to open a data stream to asynchronously transcribe longer audio files, or even \ntranscribe live-recorded audio. Applications of transcriptions in this domain might include facilitating data collection, \ndictation of medical documentation , and transcription of clinical  consultations. There are also applications for \ntelemedicine conferences, such as automatic translation  between patients and practitioners  who speak different \nlanguages.  \n \nProjects \nWhen working on a research project with LLM Factory, we envision that multiple team members may individually \ncreate adapters that should be accessible to the entire team. This necessitates a way to share adapters within the system. \nOur solution is a project management system. This allows users to create projects, assign roles within the project, \nassociate adapters with the project that can be queried by project members, and share configurations of those adapters \nthat are found to have desirable results. \n       Our implementation allows for API key management within the project, precipitating security best practices in \ncontrolling agents with access to specific adapters. This is accomplished by creating individual API keys for each \nagent that would access the project’s API , so in the case that a key is compromised, it can be easily regenerated , \npreventing outside actors from accessing the project. \n \nSecure Computational Network \nAs illustrated in Figure 1, our system manages the end -to-end data management, training, and inference for isolated \nmodels. This process requires the coordination of data movement, model training, and inference. If our resulting \nmodels were intended for public consumption, we could host them on a server exposed to the Internet or utilize a \nmodel hosting service such as Hugging Face. If the highest level  of security was our goal, we could host individual \nmodels on isolated infrastructure, exposing model inference to back-end servers and services. However, if we want to \nprovide large-scale model services that are both flexible and secure, our options are very limited. \n       The generative nature of LLMs creates the possibility for training data exposure based on input prompts and \ninference settings. Models and adapters trained with sensitive data must be limited to authorized users. Unfortunately, \nLLM inference servers are typically used with single models and provide limited, if any, security restrictions, relying \non network security to prevent unauthorized access. The lack of granular security is especially problematic in the case \nof multi-LoRA hosting, where a shared interface is used, and specific adapters are identified per request. \nSecurity implications aside, we still must address the challenge of data and process distribution. We would like to be \nable to utilize computational resources wherever they are available and without the need for direct network access to \ntraining and inference systems. Likewise, we would like to dynamically track the availability, load, and state of known \nresources and actively discover new resources as they become available. \n       To satisfy our security, accessibility, and active resource monitoring requirements, we make use of Cresco, 43 an \nagent-based framework, to establish a computational network overlay between islands of resources. Cresco was \ndeveloped to support edge -based applications 45 across heterogeneous networks and resources. Key features of the \nCresco platform are end-to-end encryption of data, multi-tenant isolation of resources, and the concept of a secure data \nplane.46 The Cresco ecosystem is composed of an ap plication description language, software agents, agent plugins, \nand client libraries. While the details of these implementations are beyond the scope of this targeted paper, we will \ndiscuss how key Cresco features are used in this implementation. The overl ay network that is established between \nagent message brokers provides end-to-end secure communication methods through agent-to-agent messages and data \nstreams over the data plane (text and binary data). These resources could be deployed within an instituti on, a cloud \nprovider, or any location where one or more agents can contact another, forming a mesh of connected resources. \n \nResults \nA key aspect of this system is the ability to provide multi-lora inferencing at time of request, with negligible overhead.  \nFigure 4 shows a sustained load test on a single GPU with request randomly switching between nine adapters across \nhundreds of prompts  with the Llama 2 8B model .  As shown in the graph we see an average of 24 responses per \nsecond, with an average response time of 1.6 seconds.  Scaling inference services across eight independent GPUs on \na single node, we can service nearly 200 request per second.   \n       While it is becoming more common for the community to publish adapter s, most models are still released with \nfull weights. Thankfully, the MergeKit47 project provides tools to extract adapters from full fine-tuned weights if the \noriginal model is available.  For example, Hermes 348 is a popular model  for function calling, that is based on the \nLlama 3 model.  With MergeKit, the original Llama 3 model, and the Hermes 3 fine-tuned model, we can extract a \nHermes 3 adapter.  We can then use the Hermes 3 adapter like we would our own custom trained adapters.  Given our \nbased model is Llama 3, we can made immediate use of this adapter for function calling on request.  In the case of a \nfunctional pipeline, the Hermes 3 adapter might be applied during a functional calling stage, while a domain-specific \nmedical adapter might be used to interpret results.   As inference server and model technology advances, we hope to \ntransition embedding extraction and multi -model inferencing in the same way, allowing a dapters to be used for \ndifferent functions, even within the same request.   \n \n \nFigure 4 NVIDIA A100 GPU Performance with Llama 3 8B (response time in ms) \nMulti-lora inferencing is not limited to a single adapter.  So -called mixture-of-adapters49 can be used that combine \nseveral adapters at time of inferencing with varying weights to influence output.  Treating adapter weights like \nhyperparameters, we used ClearML to  optimally determine the mix-of-adapters and associated weights for a set of \nmultiple-choice medical questions.  We produced a series of Medical Education Language Transformer (MELT)50 \nmodels demonstrating the benefit of this approach.  Using a mix of medical unstructured text, instructional training, \nand question and answer , and multiple-choice datasets obtained from public sources, we trained  four independent \nadapters.  The optimal mix of adapters and weights were evaluated using medqa51, medmcqa52, and usmle53 multiple-\nchoice medical datasets, as shown in Table 1. \n \n \nTable 1 Scores of various base model vs. scores when the MELT adapter was applied \n \nOn average, our mixture-of-adapter optimization approach resulted in a 19% improvement  across model types.   \nAdditional details related to results for specific models and datasets are provided in the associated model cards.   In \nfuture efforts we intend on utilizing embeddings to find thematic adapters that are associated with an input question  \nto dynamically optimal inclusion of adapters with associated weights.       \n \n\nConclusion \nIn this paper, we describe a self-service system developed at the University of Kentucky Center for Applied AI to \ndemocratize access to large, customized language model resources. Leveraging recent advancements in multi -LoRA \ninference, allowing hundreds or thousands of adapters to be hosted from shared computational resources, we can \nefficiently host custom adapters for users across our institution. Through Cresco, a previously developed agent-based \nsystem, we can securely bridge islands of resources, including client-facing interfaces, model training, inference, and \nstorage resources. In the described system, we provide secure LLM services through process and data isolation, end -\nto-end message encryption, and role-based resource authentication. These features facilitate more secure, customizable \ndevelopment and usage of local LLMs, aligning with the stringent privacy regulations that researchers in biomedical \ninformatics must adhere to. \n       In future efforts, we aim to target the development of tools to assist in dataset and adapter composition. While not \nyet implemented in available inference servers, the PEFT library allows for the weighted application of multiple \nadapters within the same i nference instance. Conceivably, at execution time, methods could be developed to predict \nwhat adapters and specific weights should be used for an incoming request. To accomplish this, we need to rethink \nhow we assemble datasets. For dataset composition, we will continue the development of clustering and classification \nof individual instruction records across dataset collections. Using existing feature extraction services, semantic themes \ncan be trained into individual adapters of similar thematic clu sters. Incoming prompts can be evaluated for adapter \nsimilarity based on known adapter datasets, informing execution-time compositions of adapters. \n       Our intent through this and future related efforts is to provide the research community with self-service access to \nthe latest models and technology as simply and cost -effectively as possible. Building from this and other efforts, we \nforesee the ability to create networks of participating AI agents, securely providing their own specializations an d \ncontributing to broader scientific discovery efforts and provide the biomedical informatics community with new tools \nwith which to promote higher quality healthcare. \n \nCode, examples, and setup instructions can be found in the following repository:  \nhttps://github.com/innovationcore/llm_factory_demos \n \nAcknowledgements \nThe project described was supported by the University of Kentucky Institute for Biomedical Informatics and the Center \nfor Clinical and Translational Sciences through NIH National Center for Advancing Translational Sciences through \ngrant number UL1TR001998. The content is solely the responsibility of the authors and does not necessarily represent \nthe official views of the NIH. \n \nThis material is based upon work supported by the National Science Foundation under Grant No. 2216140. \n \nReferences \n1 OpenAI. ChatGPT; 2023. Accessed: 2023-07-30. https://chat.openai.com. \n2 Rombach R, Blattmann A, Lorenz D, Esser P, Ommer B. High-resolution image synthesis with latent diffusion models. \nIn: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition; 2022. p. 1068495. \n3 Achiam J, Adler S, Agarwal S, Ahmad L, Akkaya I, Aleman FL, et al. GPT -4 Technical Report. arXiv preprint \narXiv:230308774. 2023. \n4 White J, Fu Q, Hays S, Sandborn M, Olea C, Gilbert H, et al. A prompt pattern catalog to enhance prompt engineering \nwith chatgpt. arXiv preprint arXiv:230211382. 2023. \n5 Touvron H, Lavril T, Izacard G, Martinet X, Lachaux MA, Lacroix T, et al. Llama: Open and efficient foundation \nlanguage models. arXiv preprint arXiv:230213971. 2023. \n6 Touvron H, Martin L, Stone K, Albert P, Almahairi A, Babaei Y , et al. Llama 2: Open Foundation and Fine-Tuned Chat \nModels. arXiv preprint arXiv:230709288. 2023. \n7 Devlin J, Chang MW, Lee K, Toutanova K. Bert: Pre -training of deep bidirectional transformers for language \nunderstanding. arXiv preprint arXiv:181004805. 2018. \n8 LLM Explorer. Extractum.io; 2024. Accessed: 2024-01-11. https://llm.extractum.io/. \n9 Zhou K, Zhu Y , Chen Z, Chen W, Zhao WX, Chen X, et al. Don’t Make Your LLM an Evaluation Benchmark \nCheater. arXiv preprint arXiv:231101964. 2023. \n10 Hugging Face. Hugging Face; 2024. Accessed: 2024-01-11. https://huggingface.co/. \n11 Wolf T, Debut L, Sanh V , Chaumond J, Delangue C, Moi A, et al. Huggingface’s transformers: State-of-the-art natural \nlanguage processing. arXiv preprint arXiv:191003771. 2019. \n12 LMSYS: Large Model Systems Organization. LMSYS Org; 2024. https://github.com/lm-sys/FastChat. 13 OpenAccess\n AI Collective: Axolotl. OpenAccess AI Collective; 2024. https://github.com/ \nOpenAccess-AI-Collective/axolotl. \n14 Das P, Ivkin N, Bansal T, Rouesnel L, Gautier P, Karnin Z, et al. Amazon SageMaker Autopilot: a white box AutoML \nsolution at scale. In: Proceedings of the fourth international workshop on data management for end -to-end machine \nlearning; 2020. p. 1-7. \n15 Azure Machine Learning. Microsoft; 2024. https://azure.microsoft.com/en-us/products/machine-learning. \n16 vLLM. vLLM Team; 2024. https://github.com/vllm-project/vllm. \n17 LangChain. LangChain; 2024. Accessed: 2024-01-13. https://github.com/langchain-ai/langchain. \n18 Azure OpenAI Service. Microsoft; 2024. https://azure.microsoft.com/en-us/products/ai-services/openai-service. \n19 LMSYS Chatbot Arena Leaderboard. LMSYS Org; 2024. https://huggingface.co/spaces/lmsys/ \nchatbot-arena-leaderboard. \n20 Gunel B, Du J, Conneau A, Stoyanov V . Supervised contrastive learning for pre-trained language model fine-tuning. \narXiv preprint arXiv:201101403. 2020. \n21 Xia M, Gao T, Zeng Z, Chen D. Sheared llama: Accelerating language model pre-training via structured pruning. arXiv \npreprint arXiv:231006694. 2023. \n22 Kwon M, Xie SM, Bullard K, Sadigh D. Reward design with language models. arXiv preprint arXiv:230300001. 2023. \n23 Schulman J, Wolski F, Dhariwal P, Radford A, Klimov O. Proximal policy optimization algorithms. arXiv preprint \narXiv:170706347. 2017. \n24 Rafailov R, Sharma A, Mitchell E, Ermon S, Manning CD, Finn C. Direct preference optimization: Your language \nmodel is secretly a reward model. arXiv preprint arXiv:230518290. 2023. \n25 Liu X, Yan H, Zhang S, An C, Qiu X, Lin D. Scaling laws of rope -based extrapolation. arXiv preprint \narXiv:231005209. 2023. \n26 Dao T. Flashattention -2: Faster attention with better parallelism and work partitioning. arXiv preprint \narXiv:230708691. 2023. \n27 Hu EJ, Shen Y , Wallis P, Allen-Zhu Z, Li Y , Wang S, et al. Lora: Low-rank adaptation of large language models. arXiv \npreprint arXiv:210609685. 2021. \n28 Frantar E, Ashkboos S, Hoefler T, Alistarh D. Gptq: Accurate post -training quantization for generative pre -trained \ntransformers. arXiv preprint arXiv:221017323. 2022. \n29 GGML. GGML Team; 2023. https://github.com/ggerganov/ggml. \n30 Dettmers T, Pagnoni A, Holtzman A, Zettlemoyer L. Qlora: Efficient finetuning of quantized llms. arXiv preprint \narXiv:230514314. 2023. \n31 Lin J, Tang J, Tang H, Yang S, Dang X, Han S. AWQ: Activation-aware Weight Quantization for LLM Compression \nand Acceleration. arXiv preprint arXiv:230600978. 2023. \n32 NVIDIA TensorRT-LLM. NVIDIA; 2024. Accessed: 2024-01-13. https://github.com/NVIDIA/TensorRT-LLM. \n33 Chen L, Y e Z, Wu Y , Zhuo D, Ceze L, Krishnamurthy A. Punica: Multi-Tenant LoRA Serving. arXiv preprint \narXiv:231018547. 2023. \n34 Sheng Y , Cao S, Li D, Hooper C, Lee N, Yang S, et al. S-lora: Serving thousands of concurrent lora adapters. arXiv \npreprint arXiv:231103285. 2023. \n35 ClearML documentation [Internet]. 2023 [cited 2024 Sep 16]; Available from: https://clear.ml/docs/latest/docs/ \n36 LoRAX. LoRAX Team; 2024. https://github.com/predibase/lorax. \n37 LoRAX Supported Models. LoRAX Team; 2024. https://predibase.github.io/lorax/models/base models/. \n38 LoRAX Supported Adapters. LoRAX Team; 2024. https://predibase.github.io/lorax/models/adapters/. \n39 Hyeon-Woo N, Ye -Bin M, Oh TH. Fedpara: Low -rank hadamard product for communication -efficient federated \nlearning. arXiv preprint arXiv:210806098. 2021. \n40 Edalati A, Tahaei M, Kobyzev I, Nia VP, Clark JJ, Rezagholizadeh M. Krona: Parameter efficient tuning with \nkronecker adapter. arXiv preprint arXiv:221210650. 2022. \n41 Li Y , Y u Y , Liang C, He P, Karampatziakis N, Chen W, et al. Loftq: Lora-fine-tuning-aware quantization for large \nlanguage models. arXiv preprint arXiv:231008659. 2023. \n42 Markowitz D. GPTMeet AI’s multitool: V ector embeddings. Google Cloud Blog. 2022. \n43 Lewis P, Perez E, Piktus A, Petroni F, Karpukhin V , Goyal N, et al. Retrieval-augmented generation for knowledge -\nintensive NLP tasks. [Internet]. arXiv; 2020 [cited 2024 Sep 11]; Available from: https://arxiv.org/abs/2005.11401 \n44 Radford A, Kim J W, Xu T, Brockman G, McLeavey C, Sutskever, I. Robust speech recognition via large-scale weak \nsupervision [Internet]. arXiv; 2022 [cited 2024 Sep 16]; Available from: https://arxiv.org/abs/2212.04356 \n45 Bumgardner VC, Marek VW, Hickey CD. Cresco: A distributed agent -based edge computing framework. In: 2016 \n12th International Conference on Network and Service Management (CNSM). IEEE; 2016. p. 400-5. \n46 Bumgardner VC, Hickey C, Marek VW. An edge-focused model for distributed streaming data applications. In: 2018 \nIEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops). \nIEEE; 2018. p. 657-62. \n47 Bumgardner C, Hickey C, Seyedtalebi N. Agent Communications in Edge Computing. In: 2019 IEEE International \nConference on Industrial Internet (ICII). IEEE; 2019. p. 387-92. \n48 Goddard, Charles, et al. \"Arcee's MergeKit: A Toolkit for Merging Large Language Models.\"  arXiv preprint \narXiv:2403.13257 (2024). \n49 Teknium, Ryan, Jeffrey Quesnelle, and Chen Guang. \"Hermes 3 Technical Report.\"  arXiv preprint \narXiv:2408.11857 (2024). \n50 Wang, Yaqing, et al. \"AdaMix: Mixture -of-adaptations for parameter -efficient model tuning.\"  arXiv preprint \narXiv:2205.12410 (2022). \n51 Center for Applied AI. (2024). MELT -Mixtral-8x7B-Instruct-v0.1 (Revision 9728518).  https://huggingface.co/IBI-\nCAAI/MELT-Mixtral-8x7B-Instruct-v0.1 \n52 Lee, Minsuk, et al. \"Beyond information retrieval —medical question answering.\"  AMIA annual symposium \nproceedings. V ol. 2006. American Medical Informatics Association, 2006. \n53 Pal, Ankit, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. \"Medmcqa: A large-scale multi-subject multi-\nchoice dataset for medical domain question answering.\" Conference on health, inference, and learning. PMLR, 2022. \n54 Kung, Tiffany H., et al. \"Performance of ChatGPT on USMLE: potential for AI-assisted medical education using large \nlanguage models.\" PLoS digital health 2.2 (2023): e0000198. ",
  "topic": "Service (business)",
  "concepts": [
    {
      "name": "Service (business)",
      "score": 0.5190784931182861
    },
    {
      "name": "Computer science",
      "score": 0.49090296030044556
    },
    {
      "name": "Computer security",
      "score": 0.4146726727485657
    },
    {
      "name": "Self-service",
      "score": 0.4117584228515625
    },
    {
      "name": "Process management",
      "score": 0.322085440158844
    },
    {
      "name": "Business",
      "score": 0.2711699306964874
    },
    {
      "name": "Marketing",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I143302722",
      "name": "University of Kentucky",
      "country": "US"
    }
  ],
  "cited_by": 1
}