{
  "title": "AlephBERT: Language Model Pre-training and Evaluation from Sub-Word to Sentence Level",
  "url": "https://openalex.org/W4285232264",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2898877596",
      "name": "Amit Seker",
      "affiliations": [
        "Bar-Ilan University"
      ]
    },
    {
      "id": "https://openalex.org/A3155428765",
      "name": "Elron Bandel",
      "affiliations": [
        "Bar-Ilan University"
      ]
    },
    {
      "id": "https://openalex.org/A3022387344",
      "name": "Dan Bareket",
      "affiliations": [
        "Bar-Ilan University"
      ]
    },
    {
      "id": "https://openalex.org/A3155151002",
      "name": "Idan Brusilovsky",
      "affiliations": [
        "Bar-Ilan University"
      ]
    },
    {
      "id": "https://openalex.org/A4320561272",
      "name": "Refael Greenfeld",
      "affiliations": [
        "Bar-Ilan University"
      ]
    },
    {
      "id": "https://openalex.org/A186962661",
      "name": "Reut Tsarfaty",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2143391265",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3101860695",
    "https://openalex.org/W3008110149",
    "https://openalex.org/W2912509754",
    "https://openalex.org/W2911227954",
    "https://openalex.org/W2880029892",
    "https://openalex.org/W4287993739",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3098265177",
    "https://openalex.org/W3037575273",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2929068185",
    "https://openalex.org/W3204526376",
    "https://openalex.org/W3156159991",
    "https://openalex.org/W3127069001",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2990188683",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3013563411",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2917956622",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3035614045",
    "https://openalex.org/W3035110948"
  ],
  "abstract": "Amit Seker, Elron Bandel, Dan Bareket, Idan Brusilovsky, Refael Greenfeld, Reut Tsarfaty. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 46 - 56\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nAlephBERT: Language Model Pre-training and Evaluation\nfrom Sub-Word to Sentence Level\nAmit Seker, Elron Bandel, Dan Bareket, Idan Brusilovsky,\nRefael Shaked Greenfeld, Reut Tsarfaty\nDepartment of Computer Science, Bar Ilan University, Ramat-Gan, Israel\n{aseker00,elronbandel,dbareket,brusli1,\nshakedgreenfeld,reut.tsarfaty}@gmail.com\nAbstract\nLarge Pre-trained Language Models (PLMs)\nhave become ubiquitous in the development\nof language understanding technology and lie\nat the heart of many artificial intelligence ad-\nvances. While advances reported for English\nusing PLMs are unprecedented, reported ad-\nvances using PLMs for Hebrew are few and\nfar between. The problem is twofold. First,\nso far, Hebrew resources for training large lan-\nguage models are not of the same magnitude\nas their English counterparts. Second, most\nbenchmarks available to evaluate progress in\nHebrew NLP require morphological bound-\naries which are not available in the output of\nPLMs. In this work we remedy both aspects.\nWe present AlephBERT, a large PLM for Mod-\nern Hebrew, trained on larger vocabulary and\na larger dataset than any Hebrew PLM before.\nMoreover, we introduce a novel neural architec-\nture that recovers the morphological segments\nencoded in contextualized embedding vectors.\nBased on this new morphological component\nwe offer an evaluation suite consisting of mul-\ntiple tasks and benchmarks that cover sentence-\nlevel, word-level and sub-word level analyses.\nOn all tasks, AlephBERT obtains state-of-the-\nart results beyond contemporary Hebrew state-\nof-the-art models. We make our AlephBERT\nmodel, the morphological extraction compo-\nnent, and the Hebrew evaluation suite publicly\navailable, for future investigations and evalua-\ntions of Hebrew PLMs.\n1 Introduction\nContextualized word representations provided by\nmodels such as BERT (Devlin et al., 2019),\nRoBERTa (Liu et al., 2019), GPT3 (Brown et al.,\n2020), T5 (Raffel et al., 2020) and more, were\nshown in recent years to be a critical component for\nobtaining state-of-the-art performance on a wide\nrange of Natural Language Processing (NLP) tasks,\nfrom surface syntactic tasks as tagging and parsing,\nto downstream semantic tasks as question answer-\ning, information extraction and text summarization.\nWhile advances reported for English using such\nmodels are unprecedented, previously reported re-\nsults using PLMs in Modern Hebrew are far from\nsatisfactory. Specifically, the BERT-based Hebrew\nsection of multilingual-BERT (Devlin et al., 2019)\n(henceforth, mBERT), did not provide a similar\nboost in performance as observed by the English\nsection of mBERT. In fact, for several reported\ntasks, the results of the mBERT model are on a par\nwith pre-neural models or neural models based on\nnon-contextual embeddings (Tsarfaty et al., 2020;\nKlein and Tsarfaty, 2020). An additional Hebrew\nBERT-based model, HeBERT (Chriqui and Yahav,\n2021), has been recently released, yet without em-\npirical evidence of performance improvements on\nkey components of the Hebrew NLP pipeline.\nThe challenge of developing PLMs for\nmorphologically-rich and medium-resourced lan-\nguages such as Modern Hebrew is twofold. First,\ncontextualized word representations are obtained\nby pre-training a large language model on massive\nquantities of unlabeled texts. In Hebrew, the size of\npublished texts available for training is relatively\nsmall. To wit, Hebrew Wikipedia (300K articles)\nused for training mBERT is orders of magnitude\nsmaller compared to English Wikipedia (6M arti-\ncles). Second, commonly accepted benchmarks for\nevaluating Hebrew models, via Morpho-Syntactic\nTagging and Parsing (Sadde et al., 2018), or Named\nEntity Recognition (Bareket and Tsarfaty, 2020)\nrequire decomposition of words into morphemes,1\nwhich are distinct of the sub-words (a.k.a. word-\npieces) provided by standard PLMs. Such mor-\nphemes are as of yet not readily available in the\nPLMs’ output embeddings.\nEvaluating BERT-based models on morpheme-\nlevel tasks is thus non-trivial due to the mismatch\nbetween the sub-word tokens used as sub-word\n1These morphemes are affixes and clitics bearing their own\nPOS. They are termed syntactic words in UD (Zeman et al.,\n2018), or segments in previous literature on Hebrew NLP.\n46\nFigure 1: PLM Morphological Extraction Pipeline. The\ntwo-word phrase “לביתהלבלַנִ transliterated as “lbit\nhlbn”, mapped to word-pieces which are consumed by\na PLM to generate contextualized vectors and extract\nthe sub-word morphological units. In this example the\nWordPiece Tokenizer splits the first word, “lbit”, into\ntwo pieces while leaving the second word, “hlbn”, intact.\nConsequently, AlephBERT generates 3 embedded vec-\ntors - the vectors associated with the split word pieces\nare averaged to form a single contextualized vector. Fi-\nnally, the resulting two word vectors are used by the\nMorphological Extraction Model that generates the dis-\nambiguated morphological segments.\ninput units used by the PLMs and the sub-word\nmorphological units needed for evaluation. PLMs\nemploy sub-word tokenization mechanisms such\nas WordPiece or Byte-Pair Encoding (BPE) for the\npurposes of minimizing Out-Of-V ocabulary words\n(Sennrich et al., 2016). These sub-word tokens are\ngenerated in a pre-processing step, without utiliza-\ntion of any linguistic information, and passed as\ninput to the PLM. Crucially, such word-pieces do\nnot reflect morphological units. Extracting morpho-\nlogical units from contextualized vectors provided\nby PLMs is challenging yet necessary in order to\nenable morphological-level evaluation of Hebrew\nPLMs on standard benchmarks.\nIn this paper we introduceAlephBERT, a Hebrew\nPLM trained on more data and a larger vocabulary\nthan any Hebrew PLM before.2 Moreover, we pro-\npose a novel architecture that extracts the morpho-\nlogical sub-word units implicitly encoded in the\ncontextualized vectors outputted by PLMs. Using\nAlephBERT and the proposed morphological ex-\ntraction model we enable evaluation on all existing\nHebrew benchmarks. We thus present a process-\ning and evaluation pipeline tailored to fit Morpho-\nlogically Rich Languages (MRLs), i.e., covering\n2We make our PLM https://huggingface.co/\nonlplab/alephbert-base and demo https://nlp.\nbiu.ac.il/~amitse/alephbert/ publicly available,\nto qualitatively assess present and future Hebrew PLMs.\nsentence-level, word-level and most importantly\nsub-word morphological-level tasks (Segmentation,\nPart-of-Speech Tagging, full Morphological Tag-\nging, Dependency Parsing, Named Entity Recog-\nnition (NER) and Sentiment Analysis), and present\nnew and improved SOTA for Modern Hebrew on\nall of these tasks.\n2 Previous Work\nContextualized word embedding vectors are a ma-\njor driver for improved performance of deep learn-\ning models on many Natural Language Understand-\ning (NLU) tasks. Initially, ELMo (Peters et al.,\n2018) and ULMFit (Howard and Ruder, 2018) in-\ntroduced contextualized word embedding frame-\nworks by training LSTM-based models on massive\namounts of texts. The linguistic quality encoded\nin these models was demonstrated over 6 tasks:\nQuestion Answering, Textual Entailment, Seman-\ntic Role labeling, Coreference Resolution, Name\nEntity Extraction, and Sentiment Analysis. The\nnext big leap was obtained with the introduction\nof the GPT-1 framework by Radford and Sutskever\n(2018). Instead of using LSTM layers, GPT is\nbased on 12 layers of Transformer decoders with\neach decoder layer composed of a 768-dimensional\nfeed-forward layer and 12 self-attention heads. De-\nvlin et al. (2019) followed along the same lines and\nimplemented Bidirectional Encoder Representa-\ntions from Transformers, or BERT in short. BERT\nattends to the input tokens in both forward and\nbackward directions while optimizing a Masked\nLanguage Model and a Next Sentence Prediction\nobjective objectives.\nBERT Benchmarks An integral part involved in\ndeveloping various PLMs is providing NLU multi-\ntask benchmarks used to demonstrate the linguistic\nabilities of new models and approaches. English\nBERT models are evaluated on 3 standard major\nbenchmarks. The Stanford Question Answering\nDataset (SQuAD) (Rajpurkar et al., 2016) is used\nfor testing paragraph-level reading comprehension\nabilities. Wang et al. (2018) selected a diverse and\nrelatively hard set of sentence and sentence-pair\ntasks which comprise the General Language Un-\nderstanding Evaluation (GLUE) benchmark. The\nSW AG (Situations With Adversarial Generations)\ndataset (Zellers et al., 2018) presents models with\npartial description of grounded situations to see if\nthey can consistently predict subsequent scenarios,\nthus indicating abilities of commonsense reasoning.\n47\nWhen evaluating Hebrew PLMs, one of the key pit-\nfalls is that there are no Hebrew versions for these\nbenchmarks. Furthermore, none of the suggested\nbenchmarks account for examining the capacity of\nPLMs for encoding the word-internal morphologi-\ncal structures which are inherent in MRLs. In this\nwork we enable a generic morphological-level eval-\nuation pipeline that is suited for PLMs of MRLs.\nMultilingual vs. Monolingual BERT Devlin\net al. (2019) produced 2 BERT models, for En-\nglish and Chinese. To support other languages,\nthey trained a multilingual BERT (mBERT) model\ncombining texts covering over 100 languages,\nin the hoped to benefit low-resource languages\nwith the linguistic information obtained from lan-\nguages with larger datasets. In reality, however,\nmBERT performance on specific languages has not\nbeen as successful as English. Consequently, sev-\neral research efforts focused on building monolin-\ngual BERT models as well as providing language-\nspecific evaluation benchmarks. Liu et al. (2019)\ntrained CamemBERT, a French BERT model eval-\nuated on syntactic and semantic tasks in addition\nto natural language inference tasks. Rybak et al.\n(2020) trained HerBERT, a BERT PLM for Polish.\nThey evaluated it on a diverse set of existing NLU\nbenchmarks as well as a new dataset for sentiment\nanalysis for the e-commerce domain. Polignano\net al. (2019) created Alberto, a BERT model for\nItalian, using a massive tweet collection. They\ntested it on several NLU tasks — subjectivity, po-\nlarity (sentiment) and irony detection in tweets. In\norder to obtain a large enough training corpus in\nlow-resources languages, such as Finnish (Virtanen\net al., 2019) and Persian (Farahani et al., 2020), a\ngreat deal of effort went into filtering and cleaning\ntext samples obtained from web crawls.\nBERT for MRLs Languages with rich morphol-\nogy introduce another challenge involving the iden-\ntification and extraction of sub-word morphological\ninformation. In many MRLs words are composed\nof sub-word morphological units, with each unit\nacting as a single syntactic unit bearing as single\nPOS tag (mimicking ‘words’ in English). Antoun\net al. (2020) addressed this for Arabic, a Semitic\nMRLs, by pre-processing the training data using a\nmorphological segmenter, producing morpholog-\nical segments to be used for training AraBERT\ninstead of the actual words. By doing so, they\nwere able to produce output vectors that corre-\nLanguage Oscar (duped) Size Wikipedia Articles\nEnglish 2.3T 6,282,774\nRussian 1.2T 1,713,164\nChinese 508G 1,188,715\nFrench 282G 2,316,002\nArabic 82G 1,109,879\nHebrew 20G 292,201\nTable 1: Corpora Size Comparison: Resource-savvy\nlanguages vs. Hebrew.\nspond to morphological segments rather than the\noriginal space-delimited word-tokens. However,\nthis approach requires the application of the same\nsegmenter at inference time as well, and like any\npipeline approach, this setup is susceptible to er-\nror propagation. This risk is magnified as words in\nMRLs may be morphologically ambiguous, and the\npredicted segments might not represent the correct\ninterpretation of the words. As a result, the quality\nof the PLM depends on the accuracy achieved by\nthe segmenting component. A particular novelty of\nthis work is not making any changes to the input,\nletting the PLM encode morphological information\nassociated with complete Hebrew tokens. Instead,\ntransforming the resulting contextualized word vec-\ntors into morphological-level segments via a novel\nneural architecture which we discuss shortly.\nEvaluating PLMs for MRLsAcross all of the\nabove-mentioned language-specific PLMs, eval-\nuation was performed on the word-,sentence- or\nparagraph-level. Non examined the capacity of\nPLMs to encode sub-word morphological-level in-\nformation which we focus on in this work. ¸ Sahin\net al. (2019) probed various information types en-\ncoded in embedded word vectors. Similarly to us,\nthey focused on languages with rich morphology\nwhere linguistic signals are encoded at the morpho-\nlogical, subword level. Their work is more about\nexplainability — showing high positive correlation\nof probing tasks to the downstream tasks, especially\nfor morphologically rich languages. Unlike us, they\nassume a single POS tag and set of features per\nword in their probing tasks. In Hebrew, Arabic and\nother MRLs, tokens may carry multiple POS per\nword, and are required to be segmented for further\nprocessing. We provide a framework that extracts\nsubword morphological units given contextualized\nword vectors, that enables to evaluate PLMs on\nmorphologically-aware datasets where words can\nhave multiple POS tags and feature-bundles.\n48\nCorpus File Size Sentences Words\nOscar (deduped) 9.8GB 20.9M 1,043M\nTwitter 6.9GB 71.5M 774M\nWikipedia 1.1GB 6.3M 127M\nTotal 17.9GB 98.7M 1.9B\nTable 2: AlephBERT’s Training Data.\n3 AlephBERT Pre-Training\nData The PLM termed AlephBERT that we pro-\nvide herein is trained on a larger dataset and a larger\nvocabulary than any Hebrew BERT instantiation\nbefore. The data we train on is listed in Table 2.\nConcretely, we employ the following datasets for\npre-training: (i) Oscar:Deduplicated Hebrew por-\ntion extracted from Common Crawl via language\nclassification, filtering and cleaning (Ortiz Suárez\net al., 2020). (ii) Wikipedia: Texts from all of\nHebrew Wikipedia, extracted using Attardi (2015).\n(iii) Twitter: Hebrew tweets collected between\n2014-09-28 and 2018-03-07. We removed markers\n(“RT:”, “@” user mentions and URLs), and elimi-\nnated duplicates. For data statistics, see Table 2.\nThe Hebrew portions of Oscar and Wikipedia\nprovide us with a training-set size orders-of-\nmagnitude smaller compared with resource-savvy\nlanguages, as shown in Table 1. In order to build\na strong PLM we need a considerable boost in\nthe amount of sentences the PLM can learn from,\nwhich in our case comes form massive amounts of\ntweets added to the training set. We acknowledge\nthe potential inherent concerns associated with this\ndata source (population bias, behavior patterns, bot\nmasquerading as humans etc.) and note that we\nhave not made any explicit attempt to identify these\ncases. Honoring ethical and legal constraints we\nhave not manually analyzed nor published this data\nsource. While the free form language expressed\nin tweets might differ significantly from the text\nfound in Oscar and Wikipedia, the sheer volume of\ntweets helps us close the resource gap substantially\nwith minimal effort.3\nModel We used the Transformers training frame-\nwork of Huggingface (Wolf et al., 2020) and trained\ntwo different models — a small model with 6\nhidden layers learned from the Oscar portion of\nour dataset, and a base model with 12 hidden lay-\ners which was trained on the entire dataset. The\nprocessing units used are wordpieces generated\nby training BERT tokenizers over the respective\n3For more details and an ethical discussion, see Section 8.\ndatasets with a vocabulary size of 52K in both cases.\nFollowing the work on RoBERTa (Liu et al., 2019)\nwe optimize AlephBERT with a masked-token pre-\ndiction loss. We deploy the default masking con-\nfiguration where 15% of word piece tokens are\nmasked. In 80% of the cases, they are replaced by\n[MASK], in 10% of the cases, they are replaced\nby a random token and in the remaining cases, the\nmasked tokens are left as is.\nOperation To optimize GPU utilization and de-\ncrease training time we split the dataset into 4\nchunks based on the number of tokens in a sen-\ntence and consequently we are able to increase\nbatch sizes and dramatically shorten training time.\nchunk1 chunk2 chunk3 chunk4\nmax tokens 0>32 32>64 64>128 128>512\nnum sentences 70M 20M 5M 2M\nWe trained for 5 epochs with learning rate 1e-\n4 followed by an additional 5 epochs with learn-\ning rate at 5e-5 for a total of 10 epochs. We\ntrained AlephBERTbase over the entire dataset on\nan NVidia DGX server with 8 V100 GPUs which\ntook 8 days. AlephBERTsmall was trained over the\nOscar portion only, using 4 GTX 2080ti GPUs tak-\ning 5 days in total.\n4 The Morphological Extraction Model\nModern Hebrew is a Semitic language with rich\nmorphology and complex orthography. As a re-\nsult, the basic processing units in the language\nare typically smaller than raw space-delimited to-\nkens. Subsequently, most standard evaluation tasks\nrequire knowledge of the internal morphological\nboundaries within the raw tokens. To accommodate\nthis granularity requirement we developed a neu-\nral model designed to produce the disambiguated\nmorphological segments for each token in context.\nThese linguistic segmentations are distinct of the\nword-pieces employed by the PLM.\nIn the morphological extraction neural model,\neach input token is represented by (one or more)\ncontextualized word-vectors produced by the PLM.\nEach word-piece token is associated with a vector,\nand for each space-delimited token, we average the\nword-piece vectors. We feed the resulting vector\ninto a seq2seq model and encode the surface to-\nken as a sequence of characters using a BiLSTM,\nfollowed by a decoder that generates an output\nsequence of characters, using space as a special\nsymbol signaling morphological boundaries.\n49\nRaw inputלביתהלבלַנִ lbit hlbn)\nSpace-delimited wordsהלבלַנִhlbn)לביתדרֹוlbit)\nIndex 5 4 3 2 1\nSegmentationלבלַנִlbn) whiteהדרֹוh) theביתדרֹוbit) houseהדרֹוh) theלדרֹוl) to\nPOS ADJ DET NOUN DET ADP\nMorphology Gender=Masc|Number=Sing PronType=Art Gender=Masc|Number=Sing PronType=Art -\nDependencies 3/amod 5/det 1/obj 3/def 0/ROOT\nWord-level NER E-ORG B-ORG\nMorpheme-level NER E-ORG I-ORG I-ORG B-ORG O\nTable 3: Illustration of Evaluated Word-Based and Morpheme-Based Downstream Tasks. The two-word input\nphrase “לביתהלבלַנִ transliterated as “lbit hlbn” (to the White House), decompose into five morphological segments\n(‘to-the-house the-white’). The Hebrew text goes from right to left.\nFigure 2: Illustration of the Morphological Extraction\nModel. The embedded vectors associated with the word-\npieces (v1 and v2 representing word-piece vectors gen-\nerated in Figure 1) are combined (averaged) to produce\na single word context vector. This context vector ini-\ntializes the hidden (forward and backward) state of a\nBiLSTM that encodes the characters of the origin word.\nThe decoder LSTM outputs a sequence of characters,\nwhere a special empty symbol indicates a morphologi-\ncal segment boundary. In multi-task setup, a fully con-\nnected linear layer is used to predict a label whenever a\nsegment boundary is detected.\nFor tasks involving both segments and labels\n(Part-of-Speech Tagging, Morphological-Features\nTagging, Named-Entity Recognition) we expand\nthis network in a multi-task learning setup; when\ngenerating an end-of-segment (space) symbol, the\nmodel also predicts task label, and we combine the\nsegment-label losses. The complete morphological\nextraction architecture is illustrated in Figure 2.\n5 Experimental Setup\nGoal In order to empirically gauge the effect of\nmodel size and data quantity on the quality of the\nlanguage model, we compare the performance of\nAlephBERT (both small and base) with all existing\nHebrew BERT instantiations. In this Section, we\ndetail the tasks and evaluation metrics. In the next\nSection, we present and analyze the results.\n5.1 Sentence-Based Modeling\nSentiment Analysis We first report on a sentence\nclassification task, assigning a sentence with one of\nthree sentiment values: negative, positive, neutral.\nSentence-level predictions are achieved by directly\nfine-tuning the PLM using an additional sentence-\nclassification head The sentence-level embedding\nvector representation is the one associated with the\nspecial [CLS] BERT token.\nWe used a version of the Hebrew Facebook Sen-\ntiment dataset (henceforth FB) of Amram et al.\n(2018) which we corrected by removing leaked\nsamples.4 We fine-tuned all models for 15 epochs\nwith 5 different seeds, and report mean accuracy.\n5.2 Word-Based Modeling\nNamed Entity Recognition In this setup we as-\nsume a sequence labeling task based on space-\ndelimited word-tokens. The input comprises of\nthe sequence of words in the sentence, and the out-\nput contains BIOES tags indicating entity spans.\nWord-level NER predictions are achieved by di-\nrectly fine-tuning the PLMs using an additional\ntoken-classification head In cases where a word is\nsplit into multiple word pieces by the PLM tok-\nenizer, we employ common practice and use the\nfirst word-piece vector.\nWe evaluate this model on two corpora. (i) The\nBen-Mordecai (BMC) corpus (Ben Mordecai and\nElhadad, 2005), which contains 3294 sentences\nwith 4600 entities and seven different entity cate-\ngories (Date, Location, Money, Organization, Per-\nson, Percent, Time). To remain compatible with\nthe original work we train and test the models on 3\n4This version has a total of 8,465 samples and is pub-\nlicly available here: https://github.com/OnlpLab/\nHebrew-Sentiment-Data\n50\ndifferent splits as in Bareket and Tsarfaty (2020).\n(ii) The Named Entities and MOrphology (NEMO)\ncorpus5 (Bareket and Tsarfaty, 2020) which is an\nextension of the SPMRL dataset with Named Enti-\nties. The NEMO corpus contains 6220 sentences\nwith 7713 entities of nine entity types (Language,\nProduct, Event, Facility, Geo-Political Entity, Lo-\ncation, Organization, Person, Work-Of-Art). We\ntrained both models for 15 epochs with 5 different\nseeds and report mean F1 scores on entity spans.\n5.3 Morpheme-Based Modeling\nFinally, to probe the PLM capacity to accurately\npredict word-internal structure, we test all models\non five tasks that require knowledge of the internal\nmorphology of raw words. The input to all these\ntasks is a Hebrew sentence represented as a raw\nsequence of space-delimited words:\n(i) Segmentation: Generating a sequence of\nmorphological segments representing the ba-\nsic processing units. These units comply with\nthe 2-level representation of tokens defined by\nUD, each unit with a single POS tag.6\n(ii) Part-of-Speech (POS) Tagging: Tagging\neach segment with a single POS.\n(iii) Morphological Tagging: Tagging each\nsegment with a single POS and a set of fea-\ntures. Equivalent to the AllTags evaluation\ndefined in the CoNLL18 shared task.7\n(iv) Morpheme-Based NER: Tagging each\nsegment with a BIOES and its entity-type.\n(v) Dependency Parsing: Use each segment\nas a node in the predicted dependency tree.\nWe train and test all morphologically-aware mod-\nels using two available morphologically-aware He-\nbrew resources:\n• The Hebrew Section of the SPMRL Task (Sed-\ndah et al., 2013).\n• The Hebrew Section of the UD treebanks col-\nlection (Sadde et al., 2018)\nAll models were trained for 15 epochs with 5 dif-\nferent seeds and we report two variants of mean F1\nscores as described next.\n5Available here: https://github.com/OnlpLab/\nNEMO-Corpus\n6https://universaldependencies.org/u/\noverview/tokenization.html\n7https://universaldependencies.org/\nconll18/results-alltags.html\nFor tasks (i)–(iv) we use the morphological ex-\ntraction model (Section 4) to extract the morpho-\nlogical segments of each word in context and also\npredict the labels via Multitask training.\nFor task (iv) the NER task, we use the\nmorphologically-annotated data files of the afore-\nmentioned SPMRL-based NEMO corpus (Bareket\nand Tsarfaty, 2020). In addition to the multi-task\nsetup described earlier, we design another setup\nin which we first only segment the text, and then\nperform fine-tuning with a token classification at-\ntention head directly applied to the PLM output\nfor the segmented tokens (similar to the way we\nfine-tune the PLM for the word-based NER task de-\nscribed in the previous section). We acknowledge\nthat we are fine-tuning the PLM on morphological\nsegments the model was not originally pre-trained\non, however, as we shall see shortly, this seemingly\nunintuitive strategy performs surprisingly well.\nFor task (v) we set up a dependency parsing\nevaluation pipeline using the standalone Hebrew\nparser offered by More et al. (2019) (a.k.a YAP)\nwhich was trained to produce SPMRL dependency\nlabels. The morphological information for each\nword (namely the segments and POS tags) is recov-\nered by our morphological extraction model, and\nis used as input features for the YAP standalone\ndependency parser.\n5.4 Morpheme-Based Evaluation Metrics\nAligned Segment The CoNLL18 Shared Task\nevaluation campaign8 reports scores for segmen-\ntation and POS tagging 9 for all participating lan-\nguages. For multi-segment words, the gold and pre-\ndicted segments are aligned by their Longest Com-\nmon Sub-sequence, and only matching segments\nare counted as true positives. We use the script\nto compare aligned segment and tagging scores\nbetween oracle (gold) segmentation and realistic\n(predicted) segmentation.\nAligned Multi-Set In addition to the CoNLL18\nmetrics, we compute F1 scores, with a slight but\nimportant difference from the shared task, as de-\nfined by More et al. (2019) and Seker and Tsarfaty\n(2020). For each word, counts are based on multi-\nset intersections of the gold and predicted labels\nignoring the order of the segments while account-\n8https://universaldependencies.org/\nconll18/results.html\n9respectively referred to as ’Segmented Words’ and\n’UPOS’ in the CoNLL18 evaluation script\n51\nTask NER (Word) Sentiment\nCorpus NEMO BMC FB\nPrev. SOTA 77.75 85.22 NA\nmBERT 79.07 87.77 79.07\nHeBERT 81.48 89.41 81.48\nAlephBERTsmall 78.69 89.07 78.69\nAlephBERTbase 84.91 91.12 84.91\nTable 4: Word-based NER F1. Previous SOTA on both\ncorpora reported by the NEMO models of Bareket and\nTsarfaty (2020). Sentiment Analysis accuracy on the\ncorrected version of the corpus of Amram et al. (2018).\ning for the number of each segment. Aligned mset\nis based on set difference which acknowledges the\npossible undercover of covert morphemes which is\nan appropriate measure of morphological accuracy.\nDiscussion To illustrate the difference between\naligned segment and aligned mset, let us take for\nexample the gold segmented tag sequence: b/IN,\nh/DET, bit/NOUN and the predicted segmented tag\nsequence b/IN, bit/NOUN. According to aligned\nsegment, the first segment ( b/IN) is aligned and\ncounted as a true positive, the second segment how-\never is considered as a false positive (bit/NOUN)\nand false negative (h/DET) while the third gold seg-\nment is also counted as a false negative (bit/NOUN).\nOn the other hand with aligned multi-set both b/IN\nand bit/NOUN exist in the gold and predicted sets\nand counted as true positives, while h/DET is mis-\nmatched and counted as a false negative. In both\ncased the total counts across words in the entire\ndatasets are incremented accordingly and finally\nused for computing Precision, Recall and F1.\n6 Results\nSentence-Level Task Sentiment analysis accu-\nracy results are provided in Table 4. All BERT-\nbased models substantially outperform the original\nCNN Baseline reported by Amram et al. (2018).\nAlephBERTbase is setting a new SOTA.\nWord-Based Task On our two NER benchmarks,\nwe report F1 scores on the word-based fine-tuned\nmodel in Table 4. While we see noticeable improve-\nments for the mBERT and HeBert variants over\nthe current SOTA, the most significant increase\nis achieved by AlephBERTbase, setting a new and\nimproved SOTA on this task.\nMorpheme-Level Tasks As a particular novelty\nof this work, we report BERT-based results on sub-\nTask Segment POS Features UAS LAS\nPrev. SOTA NA 90.49 85.98 75.73 69.41\nmBERT 97.36 93.37 89.36 80.17 74.9\nHeBERT 97.97 94.61 90.93 81.86 76.54\nAlephBERTsmall 97.71 94.11 90.56 81.5 76.07\nAlephBERTbase 98.10 94.90 91.41 82.07 76.9\nTable 5: Morpheme-Based results on the SPMRL cor-\npus. Aligned MultiSet (mset) F1 for Segmentation, POS\ntags and Morphological Features - previous SOTA re-\nported by Seker and Tsarfaty (2020) (POS) and More\net al. (2019) (features). Labeled and Unlabeled Accu-\nracy Scores for morphological-level Dependency Pars-\ning - previous SOTA reported by More et al. (2019)\n(uninfused/realistic scenario)\nTask Segment POS Features\nPrev. SOTA NA 94.02 NA\nmBERT 97.70 94.76 90.98\nHeBERT 98.05 96.07 92.53\nAlephBERTsmall 97.86 95.58 92.06\nAlephBERTbase 98.20 96.20 93.05\nTable 6: Morpheme-Based Aligned MultiSet (mset) F1\nresults on the UD corpus. Previous SOTA reported by\nSeker and Tsarfaty (2020) (POS)\nword (segment-level) information. Specifically, we\nevaluate word segmentation, POS, Morphological\nFeatures, NER and dependencies compared against\nmorphologically-labeled test sets.\nIn all cases, we use raw space-delimited tokens\nas input and produce morphological segments with\nour morphological extraction model.\nTable 5 presents evaluation results for the\nSPRML dataset, compared against the previous\nSOTA of More et al. (2019). For segmentation,\nPOS tagging, and morphological tagging we report\naligned multiset F1 scores. BERT-based segmen-\ntations are similar, all scoring in the high range of\n97-98 F1, which are hard to improve further.10\nFor POS tagging and morphological features, all\nBERT-based models considerably outperform the\nprevious SOTA. For syntactic dependencies we re-\nport labeled and unlabeled accuracy scores of the\ntrees generated by Y AP (More et al., 2019) on our\npredicted segmentation. Here we see impressive\nimprovement compared to the previous SOTA of\na joint morpho-syntactic framework. It confirms\nthat morphological errors early in the pipeline neg-\natively impact downstream tasks, and highlight the\nimportance of morphologically-driven benchmarks\n10According to error analysis, most of these errors are an-\nnotation errors or truly ambiguous cases.\n52\nTask Segment POS Features\nPrev. SOTA 96.03 93.75 91.24\nmBERT 97.17 94.27 90.51\nHeBERT 97.54 95.60 92.15\nAlephBERTsmall 97.31 95.13 91.65\nAlephBERTbase 97.70 95.84 92.71\nTable 7: Morpheme-Based Aligned (CoNLL shared\ntask) F1 on the UD corpus. Previous SOTA reported by\nMinh Van Nguyen and Nguyen (2021)\nArchitecture Pipeline Pipeline MultiTask\nSegmentation (Oracle) (Predicted)\nTask Seg NER Seg NER Seg NER\nPrev. SOTA100.0079.1095.1569.5297.0577.11\nmBERT100.0077.9297.6872.7297.2472.97\nHeBERT100.00 82 98.1576.7497.9274.86\nAlephBERTsmall 100.0079.4497.7873.0897.7472.46\nAlephBERTbase 100.0083.9498.2980.1598.1979.15\nTable 8: Morpheme-Based NER F1 on the NEMO cor-\npus. Previous SOTA reported by Bareket and Tsarfaty\n(2020) for the Pipeline (Oracle), Pipeline (Predicted)\nand a Hybrid (almost-joint) scenarios, respectively.\nas an integral part of PLM evaluation for MRLs.\nAll in all we see a repeating trend placing\nAlephBERTbase first on all morphological tasks,\nindicating the depth of the model and a larger pre-\ntraining dataset improve the ability of the PLM to\ncapture word-internal structure. These trends are\nreplicated on the UD Hebrew corpus, for two differ-\nent evaluation metrics — the Aligned MultiSet F1\nScores as in previous work on Hebrew (More et al.,\n2019), (Seker and Tsarfaty, 2020), and the Aligned\nSegment F1 scores metrics as described in the UD\nshared task (Zeman et al., 2018) — reported in\nTables 6 and 7 respectively.\nMorpheme-Level NER results Earlier in this\nsection we considered NER a word-level task that\nsimply requires fine-tuning on the word level. How-\never, this setup is not accurate enough and less\nuseful for downstream tasks, since the exact entity\nboundaries are often word internal (Bareket and\nTsarfaty, 2020). We hence report morpheme-based\nNER evaluation, respecting the exact boundaries\nof entity mentions.\nTo obtain morpheme-based labeled-span of\nNamed Entities, we could either employ a pipeline,\nfirst predicting segmentation and then applying a\nfine-tuned labeling model directly on the segments,\nor employ a multi-task model and predict NER\nlabels while performing segmentation.\nTable 8 presents segmentation and NER re-\nsults for 3 different scenarios: (i) a pipeline as-\nsuming gold segmentation (ii) a pipeline assum-\ning predicted segmentation (iii) segmentation and\nNER labels obtained jointly in a multi-task setup.\nAlephBERTbase consistently scores highest in all 3.\nLooking at the Pipeline-Predicted scores, there\nis a clear correlation between a higher segmenta-\ntion quality of a PLM and its ability to produce\nbetter NER results. Moreover, the differences\nin NER scores are considerable (unlike the sub-\ntle differences in segmentation, POS and morpho-\nlogical features scores) and draw our attention to\nthe relationship between the size of the PLM, the\nsize of the pre-training data and the quality of\nthe final NER models. Specifically, HeBERT and\nAlephBERTsmall were both pre-trained on similar\ndatasets and comparable vocabulary sizes (heBERT\nwith 30K and AlephBERT-small with 52K) but\nHeBERT, with its 12 hidden layers, performs better\ncompared to AlephBERTsmall which is composed\nof only 6 hidden layers. It thus appears that seman-\ntic information is learned in those deeper layers,\nhelping in both discriminating entities and improv-\ning the morphological segmentation capacity.\nIn addition, comparing AlephBERT base and\nHeBERT we note that they are both modeled with\nthe same 12 hidden layer architecture — the only\ndifferences between them are in the size of their vo-\ncabularies (30K vs 52K respectively) and the size\nof the training data (Oscar-Wikipedia vs Oscar-\nWikipedia-Tweets). The improvements exhibited\nby AlephBERTbase, compared to HeBERT, suggest\nlarge amounts of training data and larger vocabu-\nlary are invaluable. By exposing AlephBERTbase to\na substantially larger amount of text we increased\nthe ability of the PLM to encode syntactic and se-\nmantic signals associated with Named Entities.\nOur NER experiments further suggest that a\npipeline composed of our accurate morphological\nsegmentation model followed by AlephBERTbase\nwith a token classification head is the best strategy\nfor generating morphologically-aware NER labels.\nFinally, we observe that while AlephBERT excels\nat morphosyntactic tasks, on tasks with a more se-\nmantic flavor there is room for improvement.\n7 Conclusion\nModern Hebrew, a morphologically-rich and\nmedium-resourced language, has for long suffered\nfrom a gap in the resources available for NLP ap-\nplications, and lower level of empirical results than\nobserved in other, resource-rich languages. This\n53\nwork provides the first step in remedying the situ-\nation, by making available a large Hebrew PLM,\nnamed AlephBERT, with larger vocabulary and\nlarger training set than any Hebrew PLM before,\nand with clear evidence as to its empirical ad-\nvantages. Crucially, we augment the PLM with\na morphological disambiguation component that\nmatches the input granularity of the downstream\ntasks. Our system does not presuppose Hebrew-\nspecific linguistic-rules, and can be transparently\napplied to any language for which 2-level segmenta-\ntion data (i.e., the standard UD benchmarks) exists.\nAlephBERTbase obtains state-of-the-art results on\nmorphological segmentation, POS tagging, mor-\nphological feature extraction, dependency parsing,\nnamed-entity recognition, and sentiment analysis,\noutperforming all existing Hebrew PLMs. Our pro-\nposed morphologically-driven pipeline11 serves as\na solid foundation for future evaluation of Hebrew\nPLMs and of MRLs in general.\n8 Ethical Statement\nWe follow Bender and Friedman (2018) regarding\nprofessional practice for NLP technology and ad-\ndress ethical issues that result from the use of data\nin the development of the models in our work.\nPre-Training Data. The two initial data sources\nwe used to pre-train the language models are Os-\ncar and Wikipedia. In using the Wikipedia and\nOscar we followed standard language model train-\ning efforts, such as BERT and RoBERTa (Devlin\net al., 2019; Liu et al., 2019). We use the language-\nspecific Oscar data according to the terms specified\nin Ortiz Suárez et al. (2020) and we extract texts\nfrom language-specific Wikipedia dumps. On top\nof that, a big portion of the data used to train Aleph-\nBERT originates from the Twitter sample stream.12\nAs shown in Table 2, this data set includes 70M\nHebrew tweets which were collected over a pe-\nriod of 4 years (2014 to 2018). We acknowledge\nthe potential concerns inherently associated with\nTwitter data (population bias, behavior patterns,\nbot masquerading as humans etc.) and note that we\nhave not made any explicit attempt to identify these\ncases. We only used the text field of the tweets and\ncompletely discard any other information included\n11Available at https://github.com/OnlpLab/\nAlephBERT\n12https://developer.twitter.com/en/\ndocs/twitter-api/tweets/volume-streams/\napi-reference/get-tweets-sample-stream\nin the stream (such as identities, followers, struc-\nture of threads, date of publication, etc). We have\nnot made any effort to identify or filter out any\nsamples based on user properties such as age, gen-\nder and location nor have we made any effort to\nidentify content characteristics such as genre or\ntopic. To reduce exposure of private information\nwe cleaned up all user mentions and URLs from\nthe text. Honoring ethical and legal constraints we\nhave not manually analyzed nor published this data\nsource. While the free-form language expressed\nin tweets might differ significantly from the text\nfound in Oscar/Wikipedia, the sheer volume of\ntweets helps us close the substantial resource gap.\nTraining and Evaluation Benchmarks. The\nSPMRL (Seddah et al., 2013) and UD (Sadde et al.,\n2018) datasets we used for evaluating segmentation,\ntagging and parsing, were used to both train our\nmorphological extraction model as well as provide\nus with the test data to evaluate on morphological\nlevel tasks. Both datasets are publicly available and\nwidely used in research and industry.\nThe NEMO corpus (Bareket and Tsarfaty, 2020)\nused to train and evaluate word and morpheme\nlevel NER is an extension of the SPMRL dataset\naugmented with entities and follows the same li-\ncense terms. The BMC dataset used for training\nand evaluating word-level NER was created and\npublished by Ben Mordecai and Elhadad (2005)\nand it is publicly available for NER evaluation.\nWe used the sentiment analysis dataset of Am-\nram et al. (2018) for training and evaluating Ale-\nphBERT on a sentence level task, and we follow\ntheir terms of use. As mentioned, this dataset had\nsome flows, and we describe carefully the steps\nwe’ve taken to fix them before using this corpus in\nour experiments for internal evaluation purposes.\nWe make our in-house cleaning scripts and split\ninformation publicly available.\nAcknowledgements\nThis research was funded by the European Re-\nsearch Council (ERC grant agreement no. 677352)\nand by a research grant from the Ministry of Sci-\nence and Technology (MOST) of the Israeli Gov-\nernment, for which we are grateful.\nReferences\nAdam Amram, Anat Ben-David, and Reut Tsarfaty.\n2018. Representations and architectures in neu-\n54\nral sentiment analysis for morphologically rich lan-\nguages: A case study from modern hebrew. In Pro-\nceedings of the 27th International Conference on\nComputational Linguistics, COLING 2018, Santa Fe,\nNew Mexico, USA, August 20-26, 2018, pages 2242–\n2252.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020.\nAraBERT: Transformer-based model for Arabic lan-\nguage understanding. In Proceedings of the 4th Work-\nshop on Open-Source Arabic Corpora and Process-\ning Tools, with a Shared Task on Offensive Language\nDetection, pages 9–15, Marseille, France. European\nLanguage Resource Association.\nGiusepppe Attardi. 2015. Wikiextractor. https://\ngithub.com/attardi/wikiextractor.\nDan Bareket and Reut Tsarfaty. 2020. Neural modeling\nfor named entities and morphology (nemoˆ2). CoRR,\nabs/2007.15620.\nNaama Ben Mordecai and Michael Elhadad. 2005. He-\nbrew named entity recognition.\nEmily M. Bender and Batya Friedman. 2018. Data\nstatements for natural language processing: Toward\nmitigating system bias and enabling better science.\nTransactions of the Association for Computational\nLinguistics, 6:587–604.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nAvihay Chriqui and Inbal Yahav. 2021. Hebert |&\nhebemo: a hebrew bert model and a tool for polarity\nanalysis and emotion recognition.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nMehrdad Farahani, Mohammad Gharachorloo, Marzieh\nFarahani, and Mohammad Manthouri. 2020. Pars-\nbert: Transformer-based model for persian language\nunderstanding.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model fine-tuning for text classification.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nStav Klein and Reut Tsarfaty. 2020. Getting the ##life\nout of living: How adequate are word-pieces for mod-\nelling complex morphology? In Proceedings of the\n17th SIGMORPHON Workshop on Computational\nResearch in Phonetics, Phonology, and Morphology,\nSIGMORPHON 2020, Online, July 10, 2020, pages\n204–209.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach.\nAmir Pouran Ben Veyseh Minh Van Nguyen, Viet Lai\nand Thien Huu Nguyen. 2021. Trankit: A light-\nweight transformer-based toolkit for multilingual nat-\nural language processing. In Proceedings of the 16th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics: System Demon-\nstrations.\nAmir More, Amit Seker, Victoria Basmova, and Reut\nTsarfaty. 2019. Joint transition-based models for\nmorpho-syntactic parsing: Parsing strategies for mrls\nand a case study from modern hebrew. Trans. Assoc.\nComput. Linguistics, 7:33–48.\nPedro Javier Ortiz Suárez, Laurent Romary, and Benoît\nSagot. 2020. A monolingual approach to contextual-\nized word embeddings for mid-resource languages.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 1703–\n1714, Online. Association for Computational Linguis-\ntics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nMarco Polignano, Pierpaolo Basile, Marco de Gemmis,\nGiovanni Semeraro, and Valerio Basile. 2019. Al-\nberto: Italian bert language understanding model for\nnlp challenging tasks based on tweets.\nAlec Radford and Ilya Sutskever. 2018. Improving\nlanguage understanding by generative pre-training.\nIn arxiv.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\n55\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nPiotr Rybak, Robert Mroczkowski, Janusz Tracz, and\nIreneusz Gawlik. 2020. KLEJ: Comprehensive\nbenchmark for Polish language understanding. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1191–\n1201, Online. Association for Computational Linguis-\ntics.\nShoval Sadde, Amit Seker, and Reut Tsarfaty.\n2018. The hebrew universal dependency tree-\nbank: Past present and future. In Proceedings of\nthe Second Workshop on Universal Dependencies,\nUDW@EMNLP 2018, Brussels, Belgium, November\n1, 2018, pages 133–143.\nGözde Gül ¸ Sahin, Clara Vania, Ilia Kuznetsov, and\nIryna Gurevych. 2019. LINSPECTOR: multilin-\ngual probing tasks for word representations. CoRR,\nabs/1903.09442.\nDjamé Seddah, Reut Tsarfaty, Sandra Kübler, Marie\nCandito, Jinho D. Choi, Richárd Farkas, Jennifer\nFoster, Iakes Goenaga, Koldo Gojenola Gallete-\nbeitia, Yoav Goldberg, Spence Green, Nizar Habash,\nMarco Kuhlmann, Wolfgang Maier, Joakim Nivre,\nAdam Przepiórkowski, Ryan Roth, Wolfgang Seeker,\nYannick Versley, Veronika Vincze, Marcin Wolin-\nski, Alina Wróblewska, and Éric Villemonte de la\nClergerie. 2013. Overview of the SPMRL 2013\nshared task: A cross-framework evaluation of pars-\ning morphologically rich languages. In Proceed-\nings of the Fourth Workshop on Statistical Parsing of\nMorphologically-Rich Languages, SPMRL@EMNLP\n2013, Seattle, Washington, USA, October 18, 2013,\npages 146–182.\nAmit Seker and Reut Tsarfaty. 2020. A pointer net-\nwork architecture for joint morphological segmen-\ntation and tagging. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n4368–4378, Online. Association for Computational\nLinguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nReut Tsarfaty, Dan Bareket, Stav Klein, and Amit Seker.\n2020. From SPMRL to NMRL: what did we learn\n(and unlearn) in a decade of parsing morphologically-\nrich languages (mrls)? In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, ACL 2020, Online, July 5-10, 2020,\npages 7396–7408.\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma,\nJuhani Luotolahti, Tapio Salakoski, Filip Ginter, and\nSampo Pyysalo. 2019. Multilingual is not enough:\nBert for finnish.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\nChoi. 2018. SW AG: A large-scale adversarial dataset\nfor grounded commonsense inference. In Proceed-\nings of the 2018 Conference on Empirical Methods in\nNatural Language Processing, pages 93–104, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nDaniel Zeman, Jan Hajiˇc, Martin Popel, Martin Potthast,\nMilan Straka, Filip Ginter, Joakim Nivre, and Slav\nPetrov. 2018. CoNLL 2018 shared task: Multilingual\nparsing from raw text to Universal Dependencies. In\nProceedings of the CoNLL 2018 Shared Task: Multi-\nlingual Parsing from Raw Text to Universal Depen-\ndencies, pages 1–21, Brussels, Belgium. Association\nfor Computational Linguistics.\n56",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7110393643379211
    },
    {
      "name": "Sentence",
      "score": 0.6240803003311157
    },
    {
      "name": "Computational linguistics",
      "score": 0.5622333288192749
    },
    {
      "name": "Natural language processing",
      "score": 0.5571766495704651
    },
    {
      "name": "Word (group theory)",
      "score": 0.5116363167762756
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4955528974533081
    },
    {
      "name": "Linguistics",
      "score": 0.43864360451698303
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.4131827652454376
    },
    {
      "name": "Philosophy",
      "score": 0.06622463464736938
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I13955877",
      "name": "Bar-Ilan University",
      "country": "IL"
    }
  ],
  "cited_by": 26
}