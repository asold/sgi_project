{
  "title": "H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences",
  "url": "https://openalex.org/W3175710487",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2115760862",
      "name": "Zhen‐Hai Zhu",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2404746938",
      "name": "Radu Soricut",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2963951265",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W1574901103",
    "https://openalex.org/W4241469193",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W2083206954",
    "https://openalex.org/W3103398325",
    "https://openalex.org/W2093686670",
    "https://openalex.org/W4298395628",
    "https://openalex.org/W3106298483",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2079279323",
    "https://openalex.org/W4253873847",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W3021293129",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W3103682594",
    "https://openalex.org/W1530872699",
    "https://openalex.org/W2891534142",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2998108143",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W3177318507",
    "https://openalex.org/W3121592593",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W4295224299",
    "https://openalex.org/W2073517977",
    "https://openalex.org/W4320800818",
    "https://openalex.org/W2012371030",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3006983028",
    "https://openalex.org/W4287667694",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W2950670227",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2949718784",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4287585714",
    "https://openalex.org/W2153856645",
    "https://openalex.org/W2963351145",
    "https://openalex.org/W2111572005",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2171879949",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3034609440",
    "https://openalex.org/W3108981297",
    "https://openalex.org/W2080803508",
    "https://openalex.org/W2798909945",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W1576347883",
    "https://openalex.org/W4287762561"
  ],
  "abstract": "Zhenhai Zhu, Radu Soricut. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 3801–3815\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3801\nH-Transformer-1D: Fast One-Dimensional Hierarchical Attention for\nSequences\nZhenhai Zhu\nGoogle Research\nzhenhai@google.com\nRadu Soricut\nGoogle Research\nrsoricut@google.com\nAbstract\nWe describe an efﬁcient hierarchical method to\ncompute attention in the Transformer architec-\nture. The proposed attention mechanism ex-\nploits a matrix structure similar to the Hier-\narchical Matrix (H-Matrix) developed by the\nnumerical analysis community, and has linear\nrun time and memory complexity. We per-\nform extensive experiments to show that the\ninductive bias embodied by our hierarchical at-\ntention is effective in capturing the hierarchi-\ncal structure in the sequences typical for nat-\nural language and vision tasks. Our method\nis superior to alternative sub-quadratic propos-\nals by over +6 points on average on the Long\nRange Arena benchmark. It also sets a new\nSOTA test perplexity on One-Billion Word\ndataset with 5x fewer model parameters than\nthat of the previous-best Transformer-based\nmodels.\n1 Introduction\nLinearly combining information using content-\nbased weights, a method generically known as at-\ntention, is a key building block in many deep neu-\nral networks such as recurrent neural networks\n(RNN) (Luong et al., 2015), convolutional neu-\nral networks (CNN) (Bello et al., 2019) and graph\nconvolutional networks (GCN) (Velickovic et al.,\n2018). One particular type of such attention,\ncalled multi-head scaled dot-product attention, is\none of the main components of the Transformer\narchitecture proposed by Vaswani et al. (2017),\nwhich has been shown to push the state-of-the-\nart (SOTA) performance for various understanding\nand generation tasks. These include standard nat-\nural language processing (NLP) tasks such as ma-\nchine translation, document classiﬁcation, entail-\nment, summarization and question answering (Za-\nheer et al., 2020; Dai et al., 2019; Baevski and\nAuli, 2019), as well as music generation (Huang\net al., 2018), image generation (Parmar et al.,\n2018; Chen et al., 2020) and genomics (Zaheer\net al., 2020; Choromanski et al., 2020). The\nTransformer is also the backbone architecture for\nmodels such as BERT (Devlin et al., 2019) (and\nits numerous relatives) and GPT3 (Brown et al.,\n2020), which have delivered impressive perfor-\nmance across many NLP tasks. However, the\nstandard attention mechanism of the Transformer\nhas a run time and memory usage that scales\nquadratically with sequence length. Therefore,\nthis quadratic complexity has become a critical\nbottleneck in processing long sequences (over\n1,000 tokens), and has since motivated many new\nattention algorithms, see (Tay et al., 2020d) for a\nsurvey of such work.\nIn this paper, we draw inspiration from two\nbranches in numerical analysis: Hierarchical Ma-\ntrix (H-Matrix) (Hackbusch, 1999, 2000) and\nMultigrid method (Briggs et al., 2000). We pro-\npose a hierarchical attention that has linear com-\nplexity in run time and memory, and only uti-\nlizes dense linear algebra operations optimized for\nGPUs or TPUs.\nWe hypothesize that the inductive bias embod-\nied by the proposed hierarchical structure for the\nattention matrix is effective in capturing the hier-\narchical structure in the sequences typically seen\nin many natural language processing and com-\nputer vision tasks. The main benchmark we use in\nthis paper is the Long Range Arena (LRA) bench-\nmark (Tay et al., 2020c), which has been specif-\nically designed to evaluate and compare various\nsub-quadratic attention algorithms. Our new hier-\narchical attention mechanism achieves best aver-\nage performance to-date on the LRA benchmark\nby more than 6 points over the previous-best Big-\nBird algorithm (Zaheer et al., 2020), while push-\ning SOTA performance higher in 4 of the 5 suc-\ncessful tasks. Furthermore, using this new atten-\n3802\ntion, a Transformer-based language model trained\non the One-Billion Word dataset (Chelba et al.,\n2014) sets a new SOTA performance record by\nreducing the test perplexity by 1.55 points com-\nparing to the previous-best Transformer-XL (Dai\net al., 2019) with 5x more parameters. Overall,\nthese empirical results both validate the soundness\nof our approximation method for computing atten-\ntion weights, as well as the the appropriateness of\nthe inductive bias present in the proposed hierar-\nchical attention.\n2 Related Works\nIt is well established in the NLP literature that the\nembeddings of nearby tokens tend to be more sim-\nilar than the distant ones (Manning and Sch ¨utze,\n1999). This leads to the intuition that token simi-\nlarity and hence the attention should decrease with\nthe sequence distance between a query token and a\nkey token1. This motivates the sliding-window lo-\ncal attention (Parmar et al., 2018; Ramachandran\net al., 2019; Qiu et al., 2019) which amounts to\ntruncating off-diagonal entries in the attention ma-\ntrix beyond a user-speciﬁed sequence distance. A\nsecond approach is to keepO(1) number of nonze-\nros per row in the attention matrix. The nonzero\nentry selection is either content-based (Kitaev\net al., 2020; Roy et al., 2020; Tay et al., 2020b;\nZhou et al., 2020), hand-crafted (Beltagy et al.,\n2020; Brown et al., 2020; Child et al., 2019; Ho\net al., 2019) or simply random (Zaheer et al.,\n2020). It is also well known in the NLP litera-\nture that long-range contextual information is nec-\nessary for many NLP tasks (Khandelwal et al.,\n2018; Liu and Lapata, 2019). So a set of global\ntokens are also considered. This adds O(1) num-\nber of dense rows and columns to the attention ma-\ntrix (Zaheer et al., 2020; Ainslie et al., 2020; Belt-\nagy et al., 2020). A third approach is to approxi-\nmate the attention matrix with a low-rank factored\nform (Choromanski et al., 2020; Wang et al., 2020;\nTay et al., 2020a).\nThe ﬁrst two approaches are based on the\npremise that one needs to explicitly zero out\nentries in the attention matrix in order to re-\nduce the quadratic complexity. Decades of re-\nsearch by the scientiﬁc computing and numeri-\ncal analysis community has resulted in more so-\nphisticated algorithms to sparsify matrices. A\n1Eq. (11) and (12) offer a simple illustration of this intu-\nition.\nsmall set of samples of these algorithms and their\nengineering applications include Fast Multipole\nMethod (Greengard and Rokhlin, 1987; Green-\ngard, 1994; Nabors et al., 1994; Shi et al., 1998),\nPre-corrected FFT (Phillips and White, 1997; Zhu\net al., 2005), Hierarchical Singular Value Decom-\nposition (SVD) (Kapur and Long, 1997) and Hi-\nerarchical Matrix (H-Matrix) (Hackbusch, 1999,\n2000; Zhu and White, 2005). These are generally\ncalled Multilevel Methods (Brandt and Lubrecht,\n1990). The hierarchical attention proposed in this\npaper is inspired by these Multilevel Methods in\ngeneral and the H-Matrix in particular. The hier-\narchical matrix structure allows a linear complex-\nity in both constructing and applying the attention\nmatrix.\n3 Deﬁnition and Notation\nGiven matrices Q, K and V, with rows represent-\ning sequences of token embedding or feature vec-\ntors for query, key and value respectively, the out-\nput weighted by the scaled dot-product attention in\nthe Transformer (Vaswani et al., 2017) is deﬁned\nas\nZ = softmax(QKT\n√\nd\n)V (1)\nwhere Z,Q,K,V ∈RL×d, Lis the length of the\nsequences, and dis the embedding or feature size.\nIn a more compact matrix form, Eq. (1) can be\nwritten as\nZ = D−1AV (2)\nwhere\nA = eS (3)\nSi,j =\nQiKT\nj√\nd\n(4)\nD = diag{A·1L} (5)\n1L = [1 ,1,..., 1]T . (6)\nHere, A,S ∈RL×L, 1L ∈RL is a vector with all\nones, and Si,j represents the unnormalized cosine\nsimilarity between query embedding Qi (the i-th\nrow in Q) and key embedding Kj (the j-th row in\nK).\nFor the sake of clarity, we focus on the single-\nhead attention in the exposition of the proposed\nalgorithm. Extension to the multi-head case is\nstraightforward since each attention head is com-\nputed independently (Vaswani et al., 2017).\n3803\nComputing the similarity matrix S in Eq. (4)\nand the attention matrixAin Eq. (3) takesO(L2d)\ntime and O(L2) memory. Similarly, computing\nAV in Eq. (2) takes O(L2d) time, and computing\nA·1L in Eq. (5) takes O(L2) time. The O(L2d)\nand O(L2) complexities are the bottlenecks for ap-\nplying the attention mechanism over very long se-\nquences.\n4 Introduction on H-Matrix and\nMultigrid Method\n4.1 H-Matrix\nThe singular-value decomposition of the attention\nmatrix Ain Eq. (3) is\nA= UΣVT (7)\nwhere Σ = diag{σ1,σ2,...,σ L}and σi is the i-th\nsingular value. The numerical rank of matrix Ais\nr if ∑L\ni=r+1 σi < ϵfor a given tolerance ϵ (Tre-\nfethen and Bau, 1997). The standard rank- r ap-\nproximation to matrix Ais\nA≈ˆUˆΣ ˆVT = ˆU˜VT (8)\nwhere ˆΣ = diag{σ1,σ2,...,σ r}, ˆU, ˆV ∈RL×r\nhave the ﬁrst r columns of U and V, and ˜V =\nˆV ˆΣ. This is the low-rank approximation used\nin (Choromanski et al., 2020; Wang et al., 2020;\nTay et al., 2020a). This approximation compresses\nL2 entries in Ato 2rLentries in ˆUand ˜VT . So the\ncompression rate is L\n2r .\nThe H-Matrix generalizes this low-rank approx-\nimation by using matrix block hierarchy. Consider\na two-level H-Matrix with 4 ×4 and 2 ×2 block\npartition at level-0 and level-1, respectively. Ma-\ntrix Ais partitioned as\nA=\n\n\nA(0)\n11 A(0)\n12\nA(0)\n21 A(0)\n22\nA(1)\n12\nA(1)\n21\nA(0)\n33 A(0)\n34\nA(0)\n43 A(0)\n44\n\n\n. (9)\nThe low-rank approximation in Eq. (8) is applied\nto the off-diagonal blocks at each level. For exam-\nple,\nA(l)\n12 ≈ˆU(l)\n12 ˜V(l)\n12\nT (10)\nwhere l = 0,1. To give a concrete example, sup-\npose each entry in matrixAhas the analytical form\nAi,j = eSi,j (11)\nSi,j = 2e−(i−j)2\n−1 (12)\nwhere i,j = 0 ,1,2,..., 15 2. With the block hi-\nerarchy deﬁned in Eq. (9), the size of the matrix\nblock at level-1 and level-0 is 8 ×8 and 4 ×4, re-\nspectively. For tolerance ϵ= 10−3, one can verify\nthat the numerical rank map of matrix Ais\n\n\n4 2\n2 4 2\n2 4 2\n2 4\n\n (13)\nwhere the number in each block is the numerical\nrank of the corresponding block in Eq. (9). Note\nthat matrix A still has full numerical rank of 16\nat a looser tolerance 10−1. So the standard low-\nrank approximation is ineffective in this case. But\neven this simple two-level H-matrix already offers\na compression rate of 4\n3 since storing an H-matrix\nwith the rank map in Eq. (13) takes 192 entries 3.\nIn addition, one can verify that no entry Ai,j in\nEq. (11) is very small, since Si,j ∈ [−1,1] in\nEq. (12). Therefore, truncating off-diagonal en-\ntries of matrix A, as proposed in (Parmar et al.,\n2018), would produce a poor approximation. In\npractice, the number of levels is adapted to the un-\nderlining governing equations that result in matrix\nAand it can easily be over 10 (Kapur and Long,\n1997; Hackbusch, 2000; Zhu and White, 2005). In\nturn, this can substantially increase the compres-\nsion rate. In general, the computation complexity\nof the H-Matrix is either O(L) or O(Llog L), de-\npending on the underlining physics (Hackbusch,\n1999, 2000).\n4.2 Elements of the Multigrid Method\nMultigrid Method is a multi-level nested itera-\ntive method for solving large-scale sparse matri-\nces resulting from discretized partial-differential\nequations (PDEs) (Briggs et al., 2000; Trottenberg\net al., 2000). At its core are two simple but power-\nfully complementary ideas: relaxation and correc-\ntion. Our proposed hierarchical attention only uses\nthe correction scheme as a building block since\nthere is no sparse matrix to relax on.\nThe correction scheme has two components: re-\nstriction or coarsening, and interpolation or pro-\n2Matrix A in Eq.(11) is a symmetric Toeplitz ma-\ntrix (Golub and Loan, 1996) and hence only has 16 unique\nentries. But we ignore this fact and treat A as a general ma-\ntrix here.\n3Each one of four diagonal blocks at level-0 takes 16 en-\ntries. Each one of four off-diagonal blocks at level-0 takes 16\nentries. Each one of two off-diagonal blocks at level-1 takes\n32 entries.\n3804\nlongation. Consider a vector ¯vh of scalar values\ndeﬁned on a set of N grids with uniform interval\nh. The simplest coarsening is to take the average\nof the scalar values on each pair of grids, i.e.,\n¯v2h\nj = 1\n2(¯vh\n2j + ¯vh\n2j+1) (14)\nwhere j = 0,1,2,...N/2 −1. The superscript in\nEq. (14) indicates that the grid interval at these two\nlevels is hand 2h, respectively. The simplest in-\nterpolation is to duplicate the value on each coarse\ngrid to values on a pair of ﬁne grids, i.e.,\n¯vh\n2j = ¯v2h\nj , ¯vh\n2j+1 = ¯v2h\nj (15)\nwhere j = 0,1,2,...N/2 −1.\n5 Intuition for Hierarchical Attention\nThe hierarchical low-rank structure like Eq. (13)\nturns out to be pervasive in many if not all physics\nphenomena. Much of the theoretical analysis\nby (Greengard and Rokhlin, 1987; Hackbusch,\n1999) is concerned with quantifying such aspects.\nThe key insight into these Multilevel Methods can\nbe summarized as follows: perform no approxi-\nmation for near interactions, and apply progres-\nsively lower-precision approximation for progres-\nsively longer distance interactions. The simple\ncase shown in Eq. (9)-(13) is a good example. To\nsatisfy the tolerance of10−3, we need full rank (no\napproximation) for the diagonal blocks (near inter-\nactions), higher precision approximation (rank-2\nvs full-rank of 4) for the 4 ×4 off-diagonal blocks\nat level-0 (mid-distance) and lower precision ap-\nproximation (rank-2 vs full-rank of 8) for the8×8\noff-diagonal blocks at level-1 (long-distance).\nIn this section, we present some intuition to an-\nswer two important questions: 1) Does the hier-\narchical low-rank structure hold for the attention\nmatrix Ain Eq. (3)? 2) What is the algorithm to ef-\nﬁciently compute the hierarchical low-rank struc-\nture? We only give an informal exposition of the\nhierarchical attention. The formal mathematical\nderivation is deferred to the Appendix.\n5.1 Hierarchical Structure As Inductive Bias\nThe error analysis in (Greengard and Rokhlin,\n1987; Hackbusch, 1999) offers little direct insight\nsince the attention matrix Ain Eq. (3) is data de-\npendent by deﬁnition and hence its analytical form\nlike Eq. (11) and (12) is generally unknown. So\ngathering empirical evidences seems the only vi-\nable path to answer the ﬁrst question listed above.\nThe ablation studies by (Khandelwal et al.,\n2018) examine the effect of context words on a\nlanguage model. Within the context range of about\n200 tokens, word order is only relevant within the\n20 most recent tokens or about a sentence. In the\nlong-range context, order has almost no effect on\nperformance, suggesting that the model maintains\na high-level, rough semantic representation of far-\naway words. The observation is succinctly sum-\nmarized by the title of the paper ”sharp nearby,\nfuzzy far away”. Remarkably, this is in spirit very\nclose to the key insight into the Multilevel Meth-\nods.\nA few recent attention-related studies have ex-\nplored this direction with some success, such\nas word-level and sentence-level attentions in\n(Miculicich et al., 2018; Abreu et al., 2019),\nand sentence-level and paragraph-level attentions\nin (Liu and Lapata, 2019). Even though the pro-\nposed hierarchical attention in these studies only\nhas two levels, as opposed to ten or more levels\ntypically used by the Multilevel Methods, the re-\nported positive results are quite suggestive.\nWe therefore hypothesize that the same hier-\narchical low-rank structure as shown in Eq (13)\nmight also hold for the attention matrix in many\nNLP tasks. And we treat it as the inductive bias\nin the hierarchical attention mechanism proposed\nin this paper. As pointed out in (Goyal and Ben-\ngio, 2020), inductive biases encourage the learning\nalgorithm to prioritise solutions with certain prop-\nerties. Hence good benchmark performance deliv-\nered by a Transformer-based model with proposed\nhierarchical attention can be regarded as a posi-\ntive evidence to support the hierarchical low-rank\nstructure hypothesis.\n5.2 Informal Exposition of Hierarchical\nAttention\nIn the standard deﬁnition of attention in Eq. (3)\nand (4), there is no preference given to any keys\nbased on the sequence distance between a query\nand keys. The observation in (Khandelwal et al.,\n2018) clearly suggests that a distance-dependent\nattention mechanism should be a better alternative.\nWe will take three steps to informally explain\nthe hierarchical attention mechanism. First, the\nattention matrix blocks for nearby, mid-distance\nand long-distance attention are separated in sec-\n3805\ntion 5.2.1. This is the ﬁrst step toward the\ndistance-dependent attention mentioned above.\nSecond, a token hierarchy is established in sec-\ntion 5.2.2. Third, the hierarchical attention is con-\nstructed in section 5.2.3\n5.2.1 Attention Partition\nConsider a 16-word sentence in Fig. 1. The sen-\ntence is partitioned at three segment granularity.\nThis induces a three-level partition of the attention\nmatrix Afor the original sequence:\nA= A(2) + A(1) + A(0) (16)\nwhere\nA(2) =\n[\n0 A(2)\n12\nA(2)\n21 0\n]\n(17)\nA(1) =\n\n\nA(1)\n12\nA(1)\n21 A(1)\n23\nA(1)\n32 A(1)\n34\nA(1)\n43\n\n\n(18)\nA(0) =\n\n\nA(0)\n11 A(0)\n12\nA(0)\n21 A(0)\n22 A(0)\n23\n... ... ...\nA(0)\n87 A(0)\n88\n\n\n.\n(19)\nNote that the nonzero entries in A(0), A(1) and\nA(2) are the same as the corresponding entries of\nmatrix A in Eq. (3). Matrix block size of A(0)\nij ,\nA(1)\nij and A(2)\nij is 2×2, 4×4 and 8×8, respectively.\nFollowing the key insight into Multilevel Meth-\nods, we perform no approximation to any level-0\nmatrix block A(0)\nij and apply a low-rank approxi-\nmation to off-diagonal matrix blocks in A(1) and\nA(2). If we set the numerical rank of all these\nblocks to 2, then we can assemble the three rank\nmaps into a single rank map as 4\n\n\n2 2\n2 2 2\n2 2 2\n2 2\n2\n2\n2 2\n2 2 2\n2 2 2\n2 2\n\n\n. (20)\n4We omit some of implementation details to handle the\noverlapping entries between adjacent levels.\nthis sentence is to illustrate how to set up token hierarchy level by level with aggregation\na)  Level-0:  16 tokens partitioned into 8 segments\nb)  Level-1:  16 tokens partitioned in 4 segments\nc)  Level-2:  16 tokens partitioned in 2 segments\nthis sentence is to to set up tokenillustrate how hierarchy level by level with aggregation\nthis sentence is to illustrate how to set up token hierarchy level by level with aggregation\nFigure 1: Token sequence partitions in three segment\ngranularity.\nThe hierarchical structure embodied by the prede-\ntermined rank map in Eq. (20) represents the in-\nductive bias for the attention matrix Ain Eq. (16).\nBut this construction step is inefﬁcient because we\nneed to form the original attention matrix and then\nperform SVD to discover the low-rank approxima-\ntion.\n5.2.2 Token Hierarchy\nTo illustrate the notion of token hierarchy, con-\nsider the same 16-word sentence in Fig. 2. A\nsimple 3-level binary-tree hierarchy can be set\nup by following the simple coarsening deﬁned in\nEq. (14): 1) At level-0, each one of the 16 words\nis mapped to its word embedding; 2) At level-1,\neach token (parent node) corresponds to a pair of\nadjacent words at level-0 (child nodes), which are\nshown inside each box. The embedding of each\nparent token is simply the average of its child to-\nken embeddings; 3) At level-2, each token (parent\nnode) corresponds to one pair of adjacent tokens at\nlevel-1 (child nodes) or 4 adjacent words at level-0\n(grand child nodes), which are shown inside each\nbox. The embedding of each parent token is sim-\nply the average of its child token embeddings.\nIn general, the height of the binary tree is\nO(log2(L) and the total number of tree nodes is\nO(2L), where Lis the sequence length. We only\nneed word embeddings for the leaf nodes since the\nembeddings of all other tree nodes can be recur-\nsively computed. The formal deﬁnition and no-\ntations of the recursion for query and key are de-\ntailed in section 6.1.\n5.2.3 Informal Construction of Hierarchical\nAttention\nIt is clear from Fig. 2 that the embeddings of\nhigher level tokens represent a coarser level repre-\nsentation of a larger chunk of the text. The tokens\nat different levels can be understood as multi-scale\nsnapshots of the original token sequence at level-0.\n3806\nthis sentence is to illustrate how to set up token hierarchy level by level with aggregation\nthis\nsentence\nis\nto\nillustrate\nhow\nto\nset\nup\ntoken\nhierarchy\nlevel\nby \nlevel\nwith \naggregation\nthis\nsentence \nis \nto\nillustrate\nhow\nho\nset\nup\ntoken\nhierarchy\nlevel\nby\nlevel\nwith \naggregation\na)  Level-0:  16 tokens partitioned into 8 segments\nb)  Level-1:  8 tokens partitioned into 4 segments\nc)  Level-2:  4 tokens partitioned into 2 segments\nFigure 2: A three-level token hierarchy. Dashed boxes\nrepresent segmentation and solid boxes represents to-\nkens.\nHence this token hierarchy naturally induces a set\nof multi-scale attention matrices. Let ˜A(i) be the\nattention matrix induced by the tokens at level-i. It\nis clear from Fig. 2 that the size of ˜A(0), ˜A(1) and\n˜A(2) is 16 ×16, 8 ×8 and 4 ×4, respectively. This\nmulti-scale viewpoint does not directly lead to a\nuseful algorithm since matrix ˜A(0) contains all the\ninformation and there is little additional informa-\ntion from ˜A(1) and ˜A(2).\nA key step to arrive at the hierarchical attention\nis to apply the contextual sliding window at each\nhierarchy level. The tokens at each level are parti-\ntioned into segments of size 2 in Fig. 2. One way\nto implement the local attention is to allow each\nquery token segment to attend only two adjacent\nkey token segments, one to its left and another to\nits right. At level-0, each query token segment also\nattends to the collocated key token segment. The\ntoken segment partition and local attention lead\nto a tri-diagonal block sparse matrix structure for\n˜A(0) and bi-diagonal block sparse matrix structure\nfor ˜A(1) and ˜A(2). Their sparsity patterns are\n˜A(0) ∝\n\n\n2 2\n2 2 2\n2 2 2\n2 2 2\n2 2 2\n2 2 2\n2 2 2\n2 2\n\n\n(21)\n˜A(1) ∝\n\n\n2\n2 2\n2 2\n2\n\n (22)\n˜A(2) ∝\n[ 2\n2\n]\n(23)\nwhere the 2 in the nonzero blocks indicates that\nthese are dense blocks of size 2 ×2.\nIt is clear that ˜A(0) is identical to A(0) in\nEq. (19). The efﬁciency gain comes from ˜A(2) and\n˜A(1). Each nonzero entry in ˜A(2) and ˜A(1) cap-\ntures the aggregated or coarse attention between\ntwo disjoint chunk of four and two tokens, re-\nspectively. Progressively larger token chunks lead\nto progressively lower-precision approximation to\nthe original attention blocks. This is precisely the\nintention of the rank map in Eq. (20). We can now\nsee that ˜A(2) and ˜A(1) provide an efﬁcient way to\napproximate A(2) in Eq. (17) and A(1) in Eq. (18),\nrespectively.\n6 Key Components in Hierarchical\nAttention\n6.1 Constructing Hierarchical Attention\nThe simple example in Fig. 2 can be easily gener-\nalized. Eq. (14) is used to coarsen or merge rows\nin matrices Q, K and V in Eq. (1). For sequence\nlength L = 2 M+1, the coarsening establishes a\nbinary tree of depth M for Q, K and V, respec-\ntively. Each tree node represents a matrix row and\nthere are 2M+1−l nodes or rows at level- l. To fa-\ncilitate the discussion, we deﬁne a few hierarchy\nrelated notations here. Let ˜Q(l), ˜K(l) and ˜V(l) be\ncoarsened versions of Q, Kand V at level-lin the\nbinary tree. We note that l = 0 is a special case,\nwhich is deﬁned as\n˜Q(0) = Q, ˜K(0) = K, ˜V(0) = V. (24)\nFollowing Eq. (14), the recursion to coarsen Q, K\nand V is:\n˜Q(l+1)\nj = 1\n2( ˜Q(l)\n2j + ˜Q(l)\n2j+1) (25)\n˜K(l+1)\nj = 1\n2( ˜K(l)\n2j + ˜K(l)\n2j+1) (26)\n˜V(l+1)\nj = ( ˜V(l)\n2j + ˜V(l)\n2j+1) (27)\nwhere l = 0 ,1,...,M − 2 and j =\n0,1,2,..., 2M−l. It should be noted that the coars-\nening of V in Eq. (27) does not have the averaging\nfactor 1\n2 . We defer more details on coarsening to\nAppendix Section A.1.\nNow we are ready to compute the nonzero en-\ntries in Eq. (21), (22) and (23) and construct\nhierarchical attention matrix ˜A(l). Substituting\nEq. (25) and (26) into (4) and then into (3), we\nobtain\n˜A(l)\nij = e\n˜S(l)\nij = e\n˜Q(l)\ni ( ˜K(l)\nj )T\n√\nd (28)\n3807\nAgain, we note that l= 0 is a special case because\n˜A(0)\nij = Aij.\n6.2 Applying Hierarchical Attention\nThe hierarchical matrix structure in Eq. (17), (18)\nand (19) naturally leads to a hierarchical approach\nto the matrix-matrix multiplication in Eq. (2) and\nthe matrix-vector multiplication in Eq. (5). We\nuse the matrix-matrix multiplication as an exam-\nple since matrix-vector multiplication is just a spe-\ncial case of the matrix-matrix multiplication.\nIn view of Eq. (17), (18) and (19), we write the\nmatrix-matrix multiplication in Eq. (2) as\nY = AV ≈A(0)V(0) + ˜A(1) ˜V(1) + ˜A(2) ˜V(2)\n= Y(0) + P(0)\n(\n˜Y(1) + P(1) ˜Y(2)\n)\n(29)\nwhere\n˜Y(l) = ˜A(l) ˜V(l), l = 1,2 (30)\nWe defer the detailed derivation of Eq. (29) to Ap-\npendix Section A.5 and A.6.\n7 Algorithm And Computational\nComplexity\nTo facilitate the description and the complexity\nanalysis of the algorithm, we deﬁne a few more\nhierarchy-related notations. In addition to se-\nquence length L, number of hierarchy levels M\nand embedding or feature sizedin Eq. (1), the new\nnotations include: 1) Nr : numerical rank of the\noff-diagonal blocks (for instance, 2 in Eq. (20)).\nThis is also the diagonal block size at level-0; 2)\nN(l)\nb : number of blocks at level-l. Note that Land\nd are usually data-dependent hyper-parameters,\nwhile Nr is the only model hyper-parameter re-\nsponsible for our method’s inductive bias. In turn,\nN(l)\nb and M are derived parameters, computed as:\nN(0)\nb = L\nNr\n, N(l+1)\nb = N(l)\nb\n2 (31)\nM = log 2(N(0)\nb ). (32)\nIt is easy to verify that\nM−1∑\nl=0\nN(l)\nb =\nM−1∑\nl=0\nN(0)\nb\n2l ≈2N(0)\nb . (33)\nIt is important to note that only the diagonal\nblocks at level-0 and the super-diagonal and sub-\ndiagonal blocks at level- l are needed in applying\nthe hierarchical attention matrix. This is clearly\nshown in Eq. (21)- (23). This means that only\nN(l)\nb −1 super-diagonal and sub-diagonal blocks\nare computed at level-l. This is crucial to the over-\nall linear complexity in run time and memory.\nWe should also note that all matrix blocks in\ncoarse attention matrix ˜A(l) have the same size\nNr ×Nr. This is due to the rank map in Eq. (20).\nThis is crucial for efﬁciency reason since the\nsingle-instruction-multiple-data (SIMD) program-\nming style supported by the dense linear algebra\nlibraries for GPU and TPU encourages uniform\ntensor shapes.\nWe summarize the main steps to construct and\napply the hierarchical attention in Algorithm 1.\nAlgorithm 1 H-Transformer-1D\nInput: Q(query), K(key), V(value)\nOutput: Z\nCoarsen Q using Eq. (25) and coarsen K using\nEq. (26)\nCompute diagonal blocks in ˜A(0) and super-\ndiagonal and sub-diagonal blocks in ˜A(l) using\nEq. (28)\nCoarsen V using Eq. (27)\nCompute Y = AV in Eq. (2) using Eq. (29)\nCompute Din Eq. (5) using Eq. (29)\nCompute Z = D−1Y\nThe computational cost for Algorithm 1 has two\nparts:\n1. Computing the hierarchical attention matrix:\n(a) diagonal blocks at level- 0: dN2\nr N(0)\nb\n(b) Super- and sub-diagonal blocks at level-\nl: 4dN2\nr (N(l)\nb −1)\n(c) total: 5dLNr = O(dL)\n2. Computing matrix-matrix (MM) multiplica-\ntion in Eq. (2) and matrix-vector (MV) mul-\ntiplication in Eq. (5):\n(a) MM: 5dLNr\n(b) MV: 5LNr\n(c) total: 5(d+ 1)LNr = O(dL)\nSo the overall run time complexity of the hierar-\nchical attention algorithm is O(dL). Likewise, the\nmemory complexity can be shown to be O(dL) as\nwell. We defer the detailed analysis to appendix\nSection A.5 and A.6.\n3808\nModel ListOps Text Retrieval Image Pathﬁnder Path-X Avg\nChance 10.00 50.00 50.00 10.00 50.00 50.00 44.00\nTransformer 36.37 64.27 57.46 42.44 71.40 FAIL 54.39\nLocal Attention 15.82 52.98 53.39 41.46 66.63 FAIL 46.06\nSparse Trans. 17.07 63.58 59.59 44.24 71.71 FAIL 51.24\nLongformer 35.63 62.85 56.89 42.22 69.71 FAIL 53.46\nLinformer 35.70 53.94 52.27 38.56 76.34 FAIL 51.36\nReformer 37.27 56.10 53.40 38.07 68.50 FAIL 50.67\nSinkhorn Trans. 33.67 61.20 53.83 41.23 67.45 FAIL 51.39\nSynthesizer 36.99 61.68 54.67 41.61 69.45 FAIL 52.88\nBigBird 36.05 64.02 59.29 40.83 74.87 FAIL 55.01\nLinear Trans. 16.13 65.90 53.09 42.34 75.30 FAIL 50.55\nPerformer 18.01 65.40 53.82 42.77 77.05 FAIL 51.41\nH-Transformer-1D 49.53 78.69 63.99 46.05 68.78 FAIL 61.41\nTable 1: Experimental results on long-range arena benchmark. Best model is in boldface and second best is\nunderlined. All models do not learn anything on Path-X task, contrary to the Pathﬁnder task and this is denoted by\nFAIL. Path-X is not counted toward the Average score as it has no impact on relative performance.\n8 Experiments And Results\nWe have implemented the proposed hierarchical\nattention using Jax, an open source library 5 for\nautomatic gradient computation and linear alge-\nbra operations on GPUs and TPUs. All numer-\nical operations in our algorithm use the Numpy\nnative linear algebra functions supported by Jax.\nIn all our experiments in this section, we use\nthe standard Transformer architecture described in\n(Vaswani et al., 2017) as the backbone for our H-\nTransformer-1D model. Unless speciﬁed other-\nwise, the model parameters are: number of lay-\ners is 6, number of heads is 8, word embedding\nsize is 512 and the feed-forward module (FFN)\nsize is 2048. We follow the API for the standard\nmultihead scaled dot-product attention implemen-\ntation 6 so that we can perform a simple drop-in re-\nplacement of the standard multihead attention with\nour hierarchical attention implementation. This al-\nlows for an easy and fair comparison.\n8.1 Long-Range Arena\nThe open-source Long-Range Arena (LRA)\nbenchmark 7 has been proposed as a standard\nway to probe and quantify the capabilities of var-\nious xformer (long-range Transformer) architec-\ntures (Tay et al., 2020c). In our case, it also serves\nto highlight the effectiveness of the inductive bias\n5https://github.com/google/jax\n6https://github.com/google/ﬂax/blob/master/ﬂax/nn\n7https://github.com/google-research/long-range-arena\ninspired by the H-Matrix method, as well as the\ncapability of our hierarchical attention to handle\nlong sequences.\nThe LRA has several desirable qualities that\nmade us focus on it as a primary evaluation bench-\nmark: generality (restricted to encoder-only tasks\nto accommodate most proposals); simplicity (no\npretraining, no data augmentation allowed); difﬁ-\nculty (large headroom with existing approaches);\nlong-input focus (so that modeling improvements\nin this area are visible); diverse (6 tasks, cover-\ning math, language, image, and spatial modeling);\nand lightweight (so that modeling improvements\nare measurable independently of the ability to train\nand run high-capacity models).\nThe tasks that comprise LRA are: ListOps\n(sequences of arithmetical expressions of lengths\nof up to 2K that tests the ability to reason hi-\nerarchically while handling long context); Text\n(byte/character-level text classiﬁcation at docu-\nment level, which both simulates longer input se-\nquences – max length 4K – and increases the difﬁ-\nculty level); Retrieval (byte/character-level doc-\nument retrieval, which simulates the ability to\nmodel document similarity as a score between\ntwo independently-encoded long input sequences\n– max length 4K + 4K = 8K); Image (image clas-\nsiﬁcation based on the CIFAR-10 dataset, where\nan NxN image is ﬂattened to a sequence of length\nN2 pixels); Pathﬁnder (long-range spatial depen-\ndency task, with images consisting of two small\n3809\nModel perplexity parameters\n(Dai et al., 2019) 21.8 800M\n(Baevski and Auli, 2019) 23.02 1000M\n(Dai et al., 2019) 23.5 465M\n(Baevski and Auli, 2019) 23.91 465M\n(Shazeer et al., 2018) 24.0 4900M\nTransformer baseline 30.04 53M\nTransformer baseline 24.8 144M\nH-Transformer-1D Nr = 16 23.95 53M\nH-Transformer-1D Nr = 16 20.25 144M\nTable 2: Experimental results on one-billion word benchmark. We compare previous SOTA results obtained with\nmodels of size 465M-4900M parameters against the performance of the quadratic attention baseline and the H-\nTransformer-1D models.\ncircles and dash-line paths that either connect the\ntwo circles or not – image dimensions of 32x32\nfor a pixel sequence of length 1,024); Path-X\n(same as Pathﬁnder, but for image dimensions\nof 128x128 for a total pixel sequence of length\n16,384). The default Transformer model parame-\nters such as number of layers and number of heads\netc are pre-determined by the benchmark conﬁgu-\nration for each task.\nThe results obtained by our H-Transformer-1D\nmodel on the LRA benchmark are given in Table 1.\nOverall, the H-Transformer-1D model achieves\n61.41 average accuracy, a +6.4 points improve-\nment over the previous-best average performance\nfrom BigBird (Zaheer et al., 2020). We want to\nhighlight ListOps, Text and Retrieval because they\nall involve long sequences and H-Transformer-1D\nmodel improves SOTA performance by relatively\nlarge margins. These should be strong evidences\nto support our hypothesis in section 5.1 and vali-\ndate the inductive bias due to the hierarchical at-\ntention.\n8.2 Language Models Trained on One-Billion\nWords\nWe have used Flax, an open-source library 8 to\ntrain neural networks, as the code base for the\nmodel training. Our H-Transformer-1D model\nuses the standard Transformer decoder implemen-\ntation in Flax as the backbone. Only the atten-\ntion is replaced with our hierarchical attention.\nWe trained both the Transformer baseline and H-\nTransformer-1D on the One-Billion Word bench-\nmark (Chelba et al., 2014). We tried different Nr\n8https://github.com/google/ﬂax\n(numerical rank) in our H-Transformer-1D model.\nThese represent different inductive bias. We found\nthat H-Transformer-1D with Nr = 16 generated\ntext with quality comparable to that of the base-\nline Transformer. For both Transformer baseline\nand H-Transformer-1D, we also tried two sets of\nmodel parameters: 1) embedding size is 512 and\nfeed-forward module size is 2048 and hence the\nparameter count is 53M; 2) embedding size is\n1024 and feed-forward module size is 4096 and\nhence the parameter count is 144M. The test per-\nplexity results of these four models and various\nSOTA models are shown in table 2.\nH-Transformer-1D delivers the lowest perplex-\nity to-date while using 5× smaller model ca-\npacity than that of the previous SOTA model\nTransformer-XL (Dai et al., 2019). This is another\nstrong evidence to support our hypothesis in sec-\ntion 5.1 and validate the inductive bias due to the\nhierarchical attention.\n9 Conclusions and Future Work\nWe have proposed a new Transformer atten-\ntion using the inductive bias inspired by the H-\nMatrix. The new algorithm has linear complex-\nity in run time and memory usage and is fully\ncompatible with dense linear algebra libraries on\nGPU and TPU. The effectiveness of this new\nattention is demonstrated by the empirical ev-\nidences from long-range arena benchmark and\nOne-Billion word language modeling. Future\nwork include applying the new attention to mu-\nsic and genomics, developing proper inductive\nbias for cross-attention and extending the one-\ndimensional hierarchical attention to 2D images.\n3810\nReferences\nJader Abreu, Luis Fred, David Mac ˆedo, and\nC. Zanchettin. 2019. Hierarchical attentional hybrid\nneural networks for document classiﬁcation. ArXiv,\nabs/1901.06610.\nJoshua Ainslie, S. Onta ˜n´on, C. Alberti, V . Cvicek,\nZachary Kenneth Fisher, Philip Pham, Anirudh\nRavula, S. Sanghai, Qifan Wang, and L. Yang. 2020.\nEtc: Encoding long and structured inputs in trans-\nformers. In EMNLP.\nAlexei Baevski and M. Auli. 2019. Adaptive input rep-\nresentations for neural language modeling. ArXiv,\nabs/1809.10853.\nI. Bello, Barret Zoph, Ashish Vaswani, Jonathon\nShlens, and Quoc V . Le. 2019. Attention augmented\nconvolutional networks. 2019 IEEE/CVF Inter-\nnational Conference on Computer Vision (ICCV),\npages 3285–3294.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document trans-\nformer. ArXiv, abs/2004.05150.\nA. Brandt and A. A. Lubrecht. 1990. Multilevel matrix\nmultiplication and fast solution of integral equations.\n90:348–370.\nW.L. Briggs, V .E. Henson, and S.F. McCormick. 2000.\nA Multigrid Tutorial. SIAM.\nTom B. Brown, Benjamin Pickman Mann, Nick Ryder,\nMelanie Subbiah, Jean Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, G. Kr ¨uger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric J Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot\nlearners. ArXiv, abs/2005.14165.\nCiprian Chelba, Tomas Mikolov, M. Schuster, Qi Ge,\nT. Brants, Phillipp Koehn, and T. Robinson.\n2014. One billion word benchmark for measuring\nprogress in statistical language modeling. ArXiv,\nabs/1312.3005.\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu,\nHeewoo Jun, David Luan, and Ilya Sutskever. 2020.\nGenerative pretraining from pixels. Proceedings\nof the 37th International Conference on Machine\nLearning, PMLR 119.\nR. Child, Scott Gray, A. Radford, and Ilya Sutskever.\n2019. Generating long sequences with sparse trans-\nformers. ArXiv, abs/1904.10509.\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Jared Davis, Tam ´as Sarl ´os,\nDavid Belanger, Lucy J. Colwell, and Adrian Weller.\n2020. Masked language modeling for proteins via\nlinearly scalable long-context transformers. ArXiv,\nabs/2006.03555.\nZihang Dai, Z. Yang, Yiming Yang, J. Car-\nbonell, Quoc V . Le, and R. Salakhutdinov. 2019.\nTransformer-xl: Attentive language models beyond\na ﬁxed-length context. In ACL.\nJ. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nNAACL-HLT.\nG.H. Golub and C.F. Van Loan. 1996. Matrix Compu-\ntation. The John Hopkins University Press, Balti-\nmore.\nAnirudh Goyal and Yoshua Bengio. 2020. Inductive\nbiases for deep learning of higher-level cognition.\nArXiv, abs/2011.15091.\nL Greengard. 1994. Fast algorithms for classical\nphysics. Science, 265:909–914.\nL Greengard and V Rokhlin. 1987. A fast algorithm\nfor particle simulations. 73:325–348.\nW. Hackbusch. 1999. A sparse matrix arithmetic based\non h-matrices. part I: Introduction to H-matrices.\nComputing, 62:89–108.\nW. Hackbusch. 2000. A sparse matrix arithmetic\nbased on H-matrices. part II: Application to multi-\ndimensional problems. Computing, 64:21–47.\nJonathan Ho, Nal Kalchbrenner, Dirk Weissenborn,\nand Tim Salimans. 2019. Axial attention in multi-\ndimensional transformers. ArXiv, abs/1912.12180.\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob\nUszkoreit, Noam Shazeer, Ian Simon, Curtis\nHawthorne, Andrew M. Dai, Matthew D. Hoffman,\nMonica Dinculescu, and Douglas Eck. 2018. Music\ntransformer. arXiv: Learning.\nS. Kapur and D.E. Long. 1997. IES3: A fast integral\nequation solver for efﬁcient 3-dimensional extrac-\ntion. International Conference on Computer Aided-\nDesign, pages 448–455.\nUrvashi Khandelwal, He He, Peng Qi, and Dan Ju-\nrafsky. 2018. Sharp nearby, fuzzy far away:\nHow neural language models use context. ArXiv,\nabs/1805.04623.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. ArXiv,\nabs/2001.04451.\nYang Liu and Mirella Lapata. 2019. Hierarchical\ntransformers for multi-document summarization. In\nACL.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. ArXiv, abs/1508.04025.\n3811\nChris Manning and Hinrich Sch ¨utze. 1999. Founda-\ntions of Statistical Natural Language Processing.\nMIT Press, Cambridge, MA.\nLesly Miculicich, Dhananjay Ram, Nikolaos Pappas,\nand James Henderson. 2018. Document-level neural\nmachine translation with hierarchical attention net-\nworks. In EMNLP.\nK. Nabors, T. Korsmeyer, and J. White. 1994. Mul-\ntipole accelerated preconditioned iterative methods\nfor three-dimensional potential integral equations of\nthe ﬁrst kind. SIAM J. Sci. and Stat. Comp.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit,\nLukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran. 2018. Image transformer. ArXiv,\nabs/1802.05751.\nJoel R. Phillips and J. K. White. 1997. A precorrected-\nFFT method for electrostatic analysis of complicated\n3D structures. IEEE Transactions on Computer-\nAided Design of Integrated Circuits and Systems,\npages 1059–1072.\nJiezhong Qiu, Hao Ma, Omer Levy, Scott Yih, Sinong\nWang, and Jie Tang. 2019. Blockwise self-\nattention for long document understanding. ArXiv,\nabs/1911.02972.\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Ir-\nwan Bello, Anselm Levskaya, and Jonathon Shlens.\n2019. Stand-alone self-attention in vision models.\nArXiv, abs/1906.05909.\nAurko Roy, M. Saffar, Ashish Vaswani, and David\nGrangier. 2020. Efﬁcient content-based sparse\nattention with routing transformers. ArXiv,\nabs/2003.05997.\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin\nTran, Ashish Vaswani, Penporn Koanantakool,\nP. Hawkins, H. Lee, Mingsheng Hong, C. Young,\nRyan Sepassi, and Blake A. Hechtman. 2018. Mesh-\ntensorﬂow: Deep learning for supercomputers. In\nNeurIPS.\nW. Shi, J. Liu, N. Kakani, and T. Yu. 1998. A fast hi-\nerarchical algorithm for 3-d capacitance extraction.\nACM/IEEE Design Automation Conference.\nYi Tay, Dara Bahri, Donald Metzler, D. Juan, Zhe\nZhao, and Che Zheng. 2020a. Synthesizer: Rethink-\ning self-attention in transformer models. ArXiv,\nabs/2005.00743.\nYi Tay, Dara Bahri, L. Yang, Donald Metzler, and\nD. Juan. 2020b. Sparse sinkhorn attention. In\nICML.\nYi Tay, M. Dehghani, Samira Abnar, Y . Shen, Dara\nBahri, Philip Pham, J. Rao, Liu Yang, Sebastian\nRuder, and Donald Metzler. 2020c. Long range\narena: A benchmark for efﬁcient transformers.\nArXiv, abs/2011.04006.\nYi Tay, M. Dehghani, Dara Bahri, and Donald Metzler.\n2020d. Efﬁcient transformers: A survey. ArXiv,\nabs/2009.06732.\nL.N. Trefethen and D. Bau. 1997. Numerical linear\nalgebra. SIAM, Philadelphia.\nUlrich Trottenberg, Cornelius W. Oosterlee, and Anton\nSchuller. 2000. Multigrid. Academic Press.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. ArXiv, abs/1706.03762.\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Li `o, and Yoshua Ben-\ngio. 2018. Graph attention networks. ArXiv,\nabs/1710.10903.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han\nFang, and Hao Ma. 2020. Linformer: Self-attention\nwith linear complexity. ArXiv, abs/2006.04768.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago\nOnta˜n´on, Philip Pham, Anirudh Ravula, Qifan\nWang, Li Yang, and Amr Ahmed. 2020. Big bird:\nTransformers for longer sequences.\nHao-Yi Zhou, Shanghang Zhang, Jieqi Peng, Shuai\nZhang, Jianxin Li, Hui Xiong, and Wancai Zhang.\n2020. Informer: Beyond efﬁcient transformer\nfor long sequence time-series forecasting. ArXiv,\nabs/2012.07436.\nZhenhai Zhu, Ben Song, and J. K. White. 2005. Algo-\nrithms in FastImp: A fast and wideband impedance\nextraction program for complicated 3D geometries.\nIEEE Transactions on Computer-Aided Design of\nIntegrated Circuits and Systems.\nZhenhai Zhu and J. K. White. 2005. Fastsies: a fast\nstochastic integral equation solver for modeling the\nrough surface effect. International Conference on\nComputer Aided-Design, pages 675–682.\n3812\nA Appendix\nA.1 Restriction or Coarsening Matrices\nFor sequence length L = 2M , the coarsening es-\ntablishes a binary tree of depthMfor Q, Kand V,\nrespectively. The root of the binary tree at level-\n(M −1) has two nodes which correspond to the\ntwo matrix rows coarsened from four matrix rows\nat level-(M −2). The piecewise constant restric-\ntion matrix at level-(M −2) is\nR(M−2) =\n[ 1 1 0 0\n0 0 1 1\n]\n2×4\n. (34)\nLikewise, the piecewise constant restriction matrix\nat level-(M −3) is\nR(M−3) =\n\n\n1 1 0 0 0 0 0 0\n0 0 1 1 0 0 0 0\n0 0 0 0 1 1 0 0\n0 0 0 0 0 0 1 1\n\n\n4×8\n=\n[ R(M−2) 0\n0 R(M−2)\n]\n. (35)\nIn general, the restriction matrices follow the re-\ncursion\nR(l−1) =\n[ R(l) 0\n0 R(l)\n]\n(36)\nwhich starts from R(M−2) of size 2 ×4 and goes\nbackward to R(0) of size L\n2 ×L.\nA.2 Interpolation Matrices\nGiven Y(l) at level- l, the interpolated Y(l−1) at\nlevel-(l−1) can be written as\nY(l−1) = P(l)Y(l) (37)\nwhere l = 1,2,...,M −1, sparse matrix P(l) has\nsize L(l−1) ×L(l), and L(l) = 2 M−l is the node\ncount at level-lof the binary tree.\nThis recursion also follows the binary tree hi-\nerarchy. The four matrix rows at level- (M −2)\nare interpolated from the two matrix rows at level-\n(M −1). Speciﬁcally, the piecewise constant in-\nterpolation matrix at level-(M −1) is\nP(M−1) =\n\n\n1 0\n1 0\n0 1\n0 1\n\n\n4×2\n. (38)\nLikewise, the piecewise constant interpolation ma-\ntrix at level-(M −2) is\nP(M−2) =\n\n\n1 0 0 0\n1 0 0 0\n0 1 0 0\n0 1 0 0\n0 0 1 0\n0 0 1 0\n0 0 0 1\n0 0 0 1\n\n\n8×4\n=\n[ P(M−1) 0\n0 P(M−1)\n]\n. (39)\nIn general, the interpolation matrices follow the re-\ncursion\nP(l−1) =\n[ P(l) 0\n0 P(l)\n]\n(40)\nwhich starts from P(M−1) of size 4 ×2 and goes\nbackward to P(0) of size L×L\n2 . In view of Eq. (34)\nand (38), it is obvious that\nP(M−1) = (R(M−2))T . (41)\nIn view of the recursions in Eq. (36) and (40), it is\neasy to prove by induction that\nP(l) = (R(l−1))T . (42)\nA.3 Expansion Matrices\nFor the purpose of factored low-rank approxima-\ntion for the off-diagonal attention matrix blocks,\nwe design a series of so-called expansion matri-\nces. The ﬁrst two expansion matrices in this series\nare\nT(M−1) = P(M−1) =\n\n\n1 0\n1 0\n0 1\n0 1\n\n\n4×2\n=\n[ 12 0\n0 12\n]\n(43)\nand\nT(M−2) = P(M−2)P(M−1) =\n\n\n1 0\n1 0\n1 0\n1 0\n0 1\n0 1\n0 1\n0 1\n\n\n8×2\n=\n[ 14 0\n0 14\n]\n(44)\n3813\nwhere 1N is a length-N vector of ones. The gen-\neral form of matrix T(l) is deﬁned as\nT(l) = ΠM−1\ni=l P(i) (45)\nwhere l = 1 ,2,...,M −1. In view of Eq. (43),\n(45) and (40), it is easy to prove by induction that\nT(l) =\n[ 12M−l 0\n0 12M−l\n]\n(46)\nand it has size 2M−l+1 ×2. Further more, in view\nof Eq. (45) and (42), we have\n(T(l))T = Πl\ni=M−1R(i−1). (47)\nA.4 Low-Rank Factored Form\nMatrix T(l) plays a pivotal role in constructing the\nlow-rank approximation to the off-diagonal atten-\ntion matrix blocks. Let theij-th block in the coars-\nened attention matrix at level-1 be\n˜A(1)\nij =\n[ a11 a12\na21 a22\n]\n(48)\nwhere aij is the entry resulted from the inner prod-\nuct between a row in ˜Q(1) and ˜K(1). The rank-2\napproximation to the corresponding ij-th block in\nthe original attention matrix A at level-1 can be\nwritten as\nA(1)\nij ≈T(M−1) ˜A(1)\nij (T(M−1))T (49)\n=\n\n\n1 0\n1 0\n0 1\n0 1\n\n\n[ a11 a12\na21 a22\n][ 1 1 0 0\n0 0 1 1\n]\n=\n\n\na11 a11 a12 a12\na11 a11 a12 a12\na21 a21 a22 a22\na21 a21 a22 a22\n\n. (50)\nIt is clear that the resulting 4 ×4 matrix A(1)\nij is\nessentially the piecewise constant interpolation of\nthe 2 ×2 matrix ˜A(1)\nij along row and column di-\nrection. And since both T(M−1) and ˜A(1)\nij have\nfull rank 2, A(1)\nij necessarily has rank 2. One can\nalso view aij as being similar to the average value\nat the ij-th cluster center in the K-mean method.\nThe role of matrixT(M−1) is to expand from these\n2 ×2 clusters to the 4 ×4 grid and hence the name\nexpansion matrix.\nSince we maintain the same numerical rank\n2 for all super- and sub-diagonal attention ma-\ntrix blocks, the rank-2 approximation to the ij-th\nblock in the original attention matrix Aat level-l\nis\nA(l)\nij ≈ T(M−l) ˜A(l)\nij (T(M−l))T\n= Π M−1\ni=M−lP(i) ˜A(l)\nij ΠM−l\ni=M−1R(i−1)(51)\nwhere the last equality is due to Eq. (45) and (47).\nWe note that matrix T(l) has full column rank\n2 by design and this can be easily shown from\nEq. (46). We have used this fact to construct the\nrank-2 approximation in Eq. (51).\nA.5 Construct Hierarchical Attention Matrix\nTo see how Eq. (51) can be used, consider a simple\nthree-level partition of the attention matrix A for\nsequence length L= 16\nA=\n[\nA(2)\n11 A(2)\n12\nA(2)\n21 A(2)\n22\n]\n(52)\nA(2)\n11 =\n\n\nA(0)\n11 A(0)\n12\nA(0)\n21 A(0)\n22\nA(1)\n12\nA(1)\n21\nA(0)\n33 A(0)\n34\nA(0)\n43 A(0)\n44\n\n\n(53)\nA(2)\n22 =\n\n\nA(0)\n55 A(0)\n56\nA(0)\n65 A(0)\n66\nA(1)\n34\nA(1)\n43\nA(0)\n77 A(0)\n78\nA(0)\n87 A(0)\n88\n\n\n(54)\nwhere the size of level-0, level-1 and level-2 ma-\ntrix blocks is 2 ×2, 4 ×4 and 8 ×8, respec-\ntively. Note that the number of levels is M =\nlog2(L/2) = 3. We use this simple three-level ex-\nample to illustrate the key steps in both construct-\ning and applying the hierarchical attention matrix.\nIn view of Eq. (51), we have\nA≈\n[\n˜A(2)\n11 T(1) ˜A(2)\n12 (T(1))T\nT(1) ˜A(2)\n21 (T(1))T ˜A(2)\n22\n]\n(55)\n˜A(2)\n11 =\n\n\nA(0)\n11 A(0)\n12\nA(0)\n21 A(0)\n22\nT(2) ˜A(1)\n12 (T(2))T\nT(2) ˜A(1)\n21 (T(2))T A(0)\n33 A(0)\n34\nA(0)\n43 A(0)\n44\n\n\n(56)\n3814\n˜A(2)\n22 =\n\n\nA(0)\n55 A(0)\n56\nA(0)\n65 A(0)\n66\nT(2) ˜A(1)\n34 (T(2))T\nT(2) ˜A(1)\n43 (T(2))T A(0)\n77 A(0)\n78\nA(0)\n87 A(0)\n88\n\n\n.\n(57)\nWe note that matrices T(l),l = 1,2 are never ex-\nplicitly formed and are only implicitly used, as\nshown in next section. So only the diagonal blocks\nat level-0 and super- and sub-diagonal blocks of\nthe coarsened matrix ˜A at level-l need to be ex-\nplicitly computed. By design, all these blocks\nhave the same size 2 ×2 if we set the numeri-\ncal rank to Nr = 2 . The total number of super-\nand sub-diagonal blocks in the binary tree hier-\narchy is upper bounded by twice the number of\nsuper- and sub-diagonal blocks at level-0, which\nis 2N(0)\nb . Hence the total number of entries is\n5N(0)\nb N2\nr = 5 LNr = O(LNr). Each entry is\nequal to the inner product between ˜Q(l)\ni and ˜K(l)\nj\nand hence the run time cost per entry is O(d),\nwhere dis the embedding size. So the ﬁnal total\nrun time cost is O(Ld) and memory foot print is\nO(L). Here we leave out Nr since it is a constant\nmodel hyper parameter.\nA.6 Apply Hierarchical Attention Matrix\nComputing matrix-matrix product AV follows the\nhierarchical structure of matrix Ain Eq. (55), (56)\nand (57). We ﬁrst partition matrix V according to\nthe three-level binary tree established by the coars-\nening process, i.e.,\nV =\n\n\nV(0)\n1\nV(0)\n2\n...\nV(0)\n7\nV(0)\n8\n\n\n=\n\n\nV(1)\n1\nV(1)\n2\nV(1)\n3\nV(1)\n4\n\n=\n[\nV(2)\n1\nV(2)\n2\n]\n.\n(58)\nNote that these are partitions of the same matrix\nV at 3 different levels. For sequence length L =\n16, matrix V has size 16 ×d, and the size of the\npartitioned blocks V(0)\ni , V(1)\nj and V(2)\nk are 2 ×d,\n4 ×d and 8 ×d, respectively. In the derivation\nto come, we may exchange partitions at different\nlevels. For instance, in view of Eq. (58), we have\nV(2)\n1 =\n[\nV(1)\n1\nV(1)\n2\n]\n. (59)\nSo we may replace V(2)\n1 with the right-hand side\nin Eq. (59).\nIn view of Eq. (52) and (58), matrix-matrix\nproduct AV can be written as\nY = AV =\n[\nA(2)\n11 V(2)\n1\nA(2)\n22 V(2)\n2\n]\n+\n[\nA(2)\n12 V(2)\n2\nA(2)\n21 V(2)\n1\n]\n=\n[\nA(2)\n11 V(2)\n1\nA(2)\n22 V(2)\n2\n]\n+ Y(2). (60)\nIn view of Eq. (55), we have\nY(2) =\n[\nA(2)\n12 V(2)\n2\nA(2)\n21 V(2)\n1\n]\n≈\n[\nT(1) ˜A(2)\n12 (T(1))T V(2)\n2\nT(1) ˜A(2)\n21 (T(1))T V(2)\n1\n]\n=\n[\nP(1)P(2) ˜A(2)\n12 R(1)R(0)V(2)\n2\nP(1)P(2) ˜A(2)\n21 R(1)R(0)V(2)\n1\n]\n= P(0)P(1)\n[\n˜A(2)\n12 ˜V(2)\n2\n˜A(2)\n21 ˜V(2)\n1\n]\n= P(0)P(1)\n[\n˜Y(2)\n1\n˜Y(2)\n2\n]\n(61)\nwhere\n[\n˜V(2)\n1\n˜V(2)\n2\n]\n=\n[\nR(1)R(0)V(2)\n1\nR(1)R(0)V(2)\n2\n]\n. (62)\nThe third equality in Eq. (61) is due to Eq. (45) and\n(47) where l = 1. The fourth equality in Eq. (61)\nis due to Eq. (40).\nIn view of Eq. (56), we have\nA(2)\n11 V(2)\n1 ≈ ˜A(2)\n11 V(2)\n1\n=\n\n\nA(0)\n11 A(0)\n12\nA(0)\n21 A(0)\n22\nT(2) ˜A(1)\n12 (T(2))T\nT(2) ˜A(1)\n21 (T(2))T A(0)\n33 A(0)\n34\nA(0)\n43 A(0)\n44\n\n\nV(2)\n1\n=\n\n\nY(0)\n1\nY(0)\n2\nY(0)\n3\nY(0)\n4\n\n+ Y(1)\n1 (63)\n3815\nwhere\nY(1)\n1 =\n[\nT(2) ˜A(1)\n12 (T(2))T V(1)\n2\nT(2) ˜A(1)\n21 (T(2))T V(1)\n1\n]\n=\n[\nP(2) ˜A(1)\n12 R(1)V(1)\n2\nP(2) ˜A(1)\n21 R(1)V(1)\n1\n]\n= P(1)\n[\n˜A(1)\n12 ˜V(1)\n2\n˜A(1)\n21 ˜V(1)\n1\n]\n= P(1)\n[\n˜Y(1)\n1\n˜Y(1)\n2\n]\n(64)\nand [\n˜V(1)\n1\n˜V(1)\n2\n]\n=\n[\nR(1)V(1)\n1\nR(1)V(1)\n2\n]\n. (65)\nThe second equality in Eq. (64) is due to Eq. (45)\nand (47) where l = 2 . The third equality in\nEq. (64) is due to Eq. (40).\nIn view of Eq.(57), we have\nA(2)\n22 V(2)\n2 ≈ ˜A(2)\n22 V(2)\n2\n=\n\n\nA(0)\n55 A(0)\n56\nA(0)\n65 A(0)\n66\nT(1) ˜A(1)\n34 (T(1))T\nT(1) ˜A(1)\n43 (T(1))T A(0)\n77 A(0)\n78\nA(0)\n87 A(0)\n88\n\n\nV(2)\n2\n=\n\n\nY(0)\n5\nY(0)\n6\nY(0)\n7\nY(0)\n8\n\n+ Y(1)\n2 (66)\nwhere\nY(1)\n2 =\n[\nP(2) ˜A(1)\n34 R(1)V(1)\n4\nP(2) ˜A(1)\n43 R(1)V(1)\n3\n]\n= P(1)\n[\n˜A(1)\n34 ˜V(1)\n4\n˜A(1)\n43 ˜V(1)\n3\n]\n= P(1)\n[\n˜Y(1)\n3\n˜Y(1)\n4\n]\n(67)\nand [\n˜V(1)\n3\n˜V(1)\n4\n]\n=\n[\nR(1)V(1)\n3\nR(1)V(1)\n4\n]\n. (68)\nSubstituting Eq. (61), (63) and (66) into (60),\nwe obtain the ﬁnal result for the matrix-matrix\nproduct\nY = AV ≈Y(0) + P(0)\n(\n˜Y(1) + P(1) ˜Y(2)\n)\n(69)\nwhere\nY(0) =\n\n\nA(0)\n11 V(0)\n1 + A(0)\n12 V(0)\n2\nA(0)\n21 V(0)\n1 + A(0)\n22 V(0)\n2\n...\nA(0)\n87 V(0)\n7 + A(0)\n88 V(0)\n8\n\n\n(70)\n˜Y(1) =\n\n\n˜Y(1)\n1\n˜Y(1)\n2\n˜Y(1)\n3\n˜Y(1)\n4\n\n=\n\n\n˜A(1)\n12 ˜V(1)\n2\n˜A(1)\n21 ˜V(1)\n1\n˜A(1)\n34 ˜V(1)\n4\n˜A(1)\n43 ˜V(1)\n3\n\n (71)\n˜Y(2) =\n[\n˜Y(2)\n1\n˜Y(2)\n2\n]\n=\n[\n˜A(2)\n12 ˜V(2)\n2\n˜A(2)\n21 ˜V(2)\n1\n]\n(72)\nTo summarize, matrix-matrix product computa-\ntion includes the following steps:\n1. Compute ˜V(1) in Eq. (65) and (68), and com-\npute ˜V(2) in Eq. (62);\n2. Compute Y(0) in Eq. (70), ˜Y(1) in Eq. (71)\nand ˜Y(2) in Eq. (72);\n3. Interpolate and cumulative sum in Eq. (69);\nNote that all operations in step-2 are dense matrix-\nmatrix product, well suited for dense linear alge-\nbra libraries optimized for GPU and TPU. The to-\ntal number of super- and sub-diagonal blocks is\nupper bounded by twice the number of super- and\nsub-diagonal blocks at level-0, which is 2N(0)\nb .\nThe run time of each dense matrix-matrix product\nis O(N2\nr d). So the total run time is 5N(0)\nb N2\nr d =\n5LNrd = O(Ld). Here we leave out Nr since it\nis a constant model hyper-parameter.\nThe coarsening in step-1 and interpolation in\nstep-3 all use sparse matrices with ﬁxed sparsity\npatterns. Hence matrices P(l) and R(l) are never\nexplicitly formed and applying them can be eas-\nily done with standard library functions. Take Jax\nNumpy library as an example, coarsening can be\ndone with sum() along row axis and interpolation\ncan be done with repeat() along row axis. For this\nreason, step-1 and step-3 only have dense matrix\noperations as well.\nThe formulation of the matrix-matrix product\nfor the general level-M case is\nY = AV = Y(0) + P(0)( ˜Y(1) + P(1)( ˜Y(2)\n+ P(2)(··· + P(M−2) ˜Y(M−1)) ···)).(73)\nThis formulation is a direct consequence of the\nnested attention matrix structure and can be de-\nrived similarly as Eq. (69).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5569429993629456
    },
    {
      "name": "Transformer",
      "score": 0.5100708603858948
    },
    {
      "name": "Joint (building)",
      "score": 0.46115991473197937
    },
    {
      "name": "Computational linguistics",
      "score": 0.44154250621795654
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.4301861524581909
    },
    {
      "name": "Natural language processing",
      "score": 0.4115731716156006
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39411669969558716
    },
    {
      "name": "Cognitive science",
      "score": 0.3396744430065155
    },
    {
      "name": "Linguistics",
      "score": 0.33533090353012085
    },
    {
      "name": "Engineering",
      "score": 0.23938718438148499
    },
    {
      "name": "Psychology",
      "score": 0.16203969717025757
    },
    {
      "name": "Electrical engineering",
      "score": 0.1246795654296875
    },
    {
      "name": "Philosophy",
      "score": 0.11268550157546997
    },
    {
      "name": "Physics",
      "score": 0.0874795913696289
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}