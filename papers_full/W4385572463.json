{
    "title": "Text-to-SQL Error Correction with Language Models of Code",
    "url": "https://openalex.org/W4385572463",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5060070647",
            "name": "Ziru Chen",
            "affiliations": [
                "The Ohio State University"
            ]
        },
        {
            "id": "https://openalex.org/A5100706869",
            "name": "Shijie Chen",
            "affiliations": [
                "The Ohio State University"
            ]
        },
        {
            "id": "https://openalex.org/A5017049205",
            "name": "Michael White",
            "affiliations": [
                "The Ohio State University"
            ]
        },
        {
            "id": "https://openalex.org/A5008715111",
            "name": "Raymond J. Mooney",
            "affiliations": [
                "The University of Texas at Austin"
            ]
        },
        {
            "id": "https://openalex.org/A5076250758",
            "name": "Ali Payani",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5036007832",
            "name": "Jayanth Srinivasa",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5075435632",
            "name": "Yu Su",
            "affiliations": [
                "The Ohio State University"
            ]
        },
        {
            "id": "https://openalex.org/A5101488340",
            "name": "Huan Sun",
            "affiliations": [
                "The Ohio State University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3037140688",
        "https://openalex.org/W4383108457",
        "https://openalex.org/W2970393840",
        "https://openalex.org/W3035715371",
        "https://openalex.org/W4293569541",
        "https://openalex.org/W2139410856",
        "https://openalex.org/W3034835156",
        "https://openalex.org/W2963617989",
        "https://openalex.org/W2954823997",
        "https://openalex.org/W3170721718",
        "https://openalex.org/W3198685994",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3102841213",
        "https://openalex.org/W4385573719",
        "https://openalex.org/W4313547604",
        "https://openalex.org/W3170092793",
        "https://openalex.org/W3177813494",
        "https://openalex.org/W4385572953",
        "https://openalex.org/W3170656151",
        "https://openalex.org/W2890431379",
        "https://openalex.org/W3169611685",
        "https://openalex.org/W3098605233",
        "https://openalex.org/W2153710242",
        "https://openalex.org/W4389518685",
        "https://openalex.org/W3214600982",
        "https://openalex.org/W4295312788"
    ],
    "abstract": "Ziru Chen, Shijie Chen, Michael White, Raymond Mooney, Ali Payani, Jayanth Srinivasa, Yu Su, Huan Sun. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 2023.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 1359–1372\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nText-to-SQL Error Correction with Language Models of Code\nZiru Chen1, Shijie Chen1, Michael White1, Raymond Mooney2\nAli Payani3, Jayanth Srinivasa3, Yu Su1, Huan Sun1\n1The Ohio State University\n2The University of Texas at Austin 3Cisco Research\n{chen.8336, chen.10216, white.1240, su.809, sun.397}@osu.edu\nmooney@cs.utexas.edu {apayani, jasriniv}@cisco\nAbstract\nDespite recent progress in text-to-SQL parsing,\ncurrent semantic parsers are still not accurate\nenough for practical use. In this paper, we in-\nvestigate how to build automatic text-to-SQL\nerror correction models. Noticing that token-\nlevel edits are out of context and sometimes am-\nbiguous, we propose building clause-level edit\nmodels instead. Besides, while most language\nmodels of code are not specifically pre-trained\nfor SQL, they know common data structures\nand their operations in programming languages\nsuch as Python. Thus, we propose a novel rep-\nresentation for SQL queries and their edits that\nadheres more closely to the pre-training cor-\npora of language models of code. Our error\ncorrection model improves the exact set match\naccuracy of different parsers by 2.4–6.5 and\nobtains up to 4.3 point absolute improvement\nover two strong baselines.1\n1 Introduction\nText-to-SQL parsing is a classic semantic pars-\ning task that finds wide applications (Zelle and\nMooney, 1996; Tang and Mooney, 2000). Since the\nrelease of Spider (Yu et al., 2018), a cross-database\ntext-to-SQL benchmark, many semantic parsers\nwith decent performance have been developed (Lin\net al., 2020; Wang et al., 2020; Deng et al., 2021;\nRubin and Berant, 2021; Scholak et al., 2021).\nNonetheless, state-of-the-art semantic parsers are\nstill not accurate enough. As a result, their users\nneed to constantly correct wrongly predicted SQL\nqueries, which can be as time-consuming and error-\nprone as writing a SQL query from scratch (Jor-\ngensen and Shepperd, 2007; Weiss et al., 2007).\nTherefore, in this paper, we study the problem of\nautomatic text-to-SQL error correction to better\nassist users in querying complex databases.\nWe first highlight that it is essential to factor\nin the compositional substructures within SQL\n1Our code and data are available at https://github.\ncom/OSU-NLP-Group/Auto-SQL-Correction .\nqueries, such as abstract syntax trees (Yin and Neu-\nbig, 2017; Guo et al., 2022) and data-flow graphs\n(Guo et al., 2021), instead of treating code snip-\npets as string sequences. Compared to individual\ntokens, substructures (e.g. SQL clauses) include\nmore context of the entire program and are more se-\nmantically meaningful. Consequently, edit patterns\nof such substructures are more intuitive for humans\nto understand and easier for language models to\nlearn. Moreover, while the pre-training corpora for\nlanguage models of code, such as CodeT5 (Wang\net al., 2021), do not include many SQL queries\nbased on their documentation, they naturally con-\ntain abundant examples of common data structures\nlike dictionaries. Therefore, we hypothesize that\ntransforming unfamiliar SQL queries into familiar\ndata structures can help language models of code\nbetter perform structural editing of SQL queries.\nBased on these observations, we develop our er-\nror correction model and make two contributions.\nFirst, we propose considering SQL clauses instead\nof tokens as basic semantic units for editing. Us-\ning a context-free grammar, we can decompose a\nSQL query and identify its clauses by traversing\nits abstract syntax tree. Second, we propose a new\nrepresentation of SQL queries and their edits that\nadheres more closely to common code pre-training\ncorpora, including CodeSearchNet (Husain et al.,\n2020), and makes the structures of a SQL query\nmore explicit. With a decomposed SQL query, we\npair each clause with its SQL keyword and repre-\nsent the entire query as a Python dictionary. Then,\nwe format edits on a wrong SQL query as a pro-\ngram that modifies data of the query’s correspond-\ning dictionary. Unlike token-level edits in existing\nwork (Zhang et al., 2023), such dictionary oper-\nations define all edits unambiguously and can be\ndirectly executed with a Python interpreter.\nThrough comprehensive experiments with differ-\nent representations, we show that: (1) our proposed\nrepresentation has the lowest zero-shot perplexity\n1359\nQuery Representation Edit Representation\nSQL select tweets.text\nfrom tweets\norder by tweets.text\nToken-Level <ReplaceOld> tweets.text <ReplaceNew>\ntweets.createdate <ReplaceEnd>\nClause-Level <ReplaceOld> order by tweets.text <Re-\nplaceNew> order by tweets.createdate <Re-\nplaceEnd>\nPyDict sql = {\n\"select\": \"select tweets.text\",\n\"from\": \"from tweets\",\n\"orderBy\": \"order by tweets.text\"\n}\nClause-Level <ReplaceOld> \"orderBy\": \"order by\ntweets.text\" <ReplaceNew> \"orderBy\":\n\"order by tweets.createdate\" <ReplaceEnd>\nProgram sql[\"orderBy\"] = \"order by tweets.createdate\"\nTable 1: Example representations for a wrong SQL query and the Replace edit action. The corresponding natural\nlanguage utterance is “List the text of all tweets in the order of date.” For token-level and clause-level representations,\nwe format them as “<ReplaceOld> Span of wrong tokens/clauses <ReplaceNew> Span of correct tokens/clauses\n<ReplaceEnd>”, where <ReplaceOld>, <ReplaceNew>, and <ReplaceEnd> are special tokens.\nwith CodeT5; (2) simply changing token-level ed-\nits to clause-level edits can effectively improve the\nperformance of our models; and (3) our method\nimproves the exact set match accuracy of differ-\nent parsers by 2.4–6.5 and obtains up to 4.3 point\nabsolute improvement over two strong baselines.\n2 Text-to-SQL Error Correction\nGiven a natural language utterance u, a database\nschema s, and a wrong SQL query q− produced\nby an existing parser, our goal is to develop an\nerror correction model that predicts a sequence of\nedit actions e and the correct query q+. Following\nprevious work (Zhang et al., 2023), we formulate\nour task as sequence-to-sequence generation:\nP(y|x) = ΠT\nt=1P(yt|x, y1:t−1) (1)\nwhere x = [u; s; q−] is the concatenation of the\ngiven inputs and y = [e; q+] is the concatenation\nof all edit actions and the resulting correct query. In\nthis section, we study different representations of\nSQL queries (Section 2.1) and edits (Section 2.2)\nto better leverage language models of code.\n2.1 Query Representation\nWe consider two representations for a predicted\nquery: (1) the original SQL format and (2) our pro-\nposed PyDict (Python Dictionary) representation.\nTo prepare for editing, we disambiguate each SQL\nquery following Rubin and Berant (2021), includ-\ning lower-casing non-value tokens, resolving table\nreferences, and formatting punctuation. This pre-\nprocessing normalizes SQL queries predicted by\ndifferent base parsers and the gold annotations into\nthe same format. To build our PyDict representa-\ntion, we parse a SQL query into its abstract syntax\ntree (AST) with Spider’s context-free grammar. We\nuse depth-first search to traverse through the AST,\nfind any nested substructures, and construct the dic-\ntionary representation bottom-up. Table 1 shows\nthe “SQL” and “PyDict” representations of a SQL\nquery (more details in Appendix A).\n2.2 Edit Representation\nWe first follow Zhang et al. (2023) to use token-\nlevel edit representation with special tokens (Ta-\nble 1), which have unique entries in the tokenizer\nand the model’s embedding layer to describe Re-\nplace, Insert , and Delete edit actions (more ex-\namples in Appendix F). However, we realize this\nrepresentation can sometimes be ambiguous. As\nshown in Table 1, the span “tweets.text” appears\ntwice in the SQL query. This repetition would con-\nfuse the error correction model with which span to\nreplace when generating the corrected query. Also,\nthe ambiguity makes it difficult to implement rules\nand directly carry out the edit actions on the wrong\nquery. Hence, we change the token-level edit rep-\nresentation to clause-level, which includes more\ncontext of the query to make different edits more\ndistinguishable. In our experiments (Section 4.1),\nwe demonstrate that this simple modification is al-\nready effective. Our program representation further\nimproves the performance because it is more simi-\nlar to the code pre-training corpora and eliminates\nthe need to learn special tokens’ representations.\n3 Experimental Setup\n3.1 Data Synthesis for SQL Error Correction\nTo train a text-to-SQL error correction model, we\nneed to collect a set of wrong SQL parses that re-\nflects a realistic distribution of errors (Section 4.2)\nas our training data. We synthesize this dataset by\n1360\nCodeT5 BRIDGEv2 SmBoP\n# of Train 47,020 24,776 20,083\n# of Dev 448 448 448\n# of Test 430 392 310\nAvg. Train Edits 2.34 3.11 2.72\nAvg. Dev Edits 2.70 3.29 3.31\nAvg. Test Edits 1.84 1.51 1.47\nTable 2: Summary of data statistics.\nperforming 5-fold cross-validation on each parser,\nwhich approximates the actual evaluation setting.\nFollowing the evaluation setup in Yu et al.\n(2018), we split Spider’s training set into five\nroughly equal subsets by different databases. For\neach cross-validation fold, we train a text-to-SQL\nparser (Section 3.2) on four subsets and evaluate it\non the remaining one. At inference time, we per-\nform beam search with size 20 for each example\nand collect grammatical and executable parses in\nthe beam.2 If a SQL parse is not an exact set match\nor execution match to the gold annotation, we la-\nbel it wrong and include it in our training set for\nerror correction. Having synthesized our training\ndataset, we randomly sample 8 databases and their\nassociated questions to construct a held-out devel-\nopment set. For development set examples, we only\nkeep incorrect SQL parses with the highest beam\nconfidence. For our error correction test set, we\ntrain each parser on the full Spider training set and\nevaluate it on the original Spider’s development\nset without modifications. We similarly keep SQL\nparses with exact match or execution match errors.\nTable 2 summarizes the statistics of our data.\n3.2 Models\nText-to-SQL base parsers. We choose three text-\nto-SQL parsers with different decoding strategies\nand levels of performance (Table 3). We elaborate\non our selection criteria in Appendix B.\n• CodeT5 (Wang et al., 2021): We fine-tune\nCodeT5-base following Xie et al. (2022).\nThis parser represents those using beam\nsearch decoding and having a lower accuracy.\n• BRIDGEv2 (Lin et al., 2020): A represen-\ntative parser with constrained decoding and\nachieving a medium-level accuracy.\n• SmBoP (Rubin and Berant, 2021): A repre-\nsentative parser with bottom-up decoding and\nachieving higher accuracy.\n2Due to SmBoP’s bottom-up decoding, we keep its original\nbeam size and collect the top-20 unique beam predictions.\nError correction models. We use two language\nmodels of code in all our experiments:\n• CoditT5 (Zhang et al., 2023): A language\nmodel pre-trained for code editing tasks by in-\njecting noises to code snippets in CodeSearch-\nNet (Husain et al., 2020) and then denoising\nwith token-level edit representations.\n• CodeT5 (Wang et al., 2021): A language\nmodel pre-trained for general code under-\nstanding and generation with four different\npre-training objectives.\nWe compare the existing SQL+Token-Level rep-\nresentation with our proposed ones: SQL+Clause-\nLevel, PyDict+Clause-Level, and PyDict+Program\non CodeT5 and the first three on CoditT5.3 Imple-\nmentation details are in Appendix C.\n3.3 Evaluation\nWe use the increase in Exact Set Match (EM) and\nExecution Match (EX) accuracy on our error cor-\nrection test set to measure each model’s perfor-\nmance. Because CoditT5’s experiments assume\nthe input program has at least one error, we keep\nthis assumption for fair comparisons. To construct\na test set satisfying this assumption, we have to\ncompare parser-generated SQL queries with gold\nannotations (Section 3.1). Thus, we use the Spi-\nder development set as our test set and split the\nSpider training set to build a held-out development\nset (Table 2) to select model checkpoints during\ntraining. We also include results on our held-out\ndevelopment set in the appendix (Table E.1).\n4 Results and Analysis\n4.1 Main Results\nWe summarize our main results in this section. To\nensure robustness, we repeat all experiments with\n3 different random seeds and report the average\nperformances with standard deviations. Our model\ncan also be used in an interactive framework that\nallows users to select edit actions from the top- k\nbeam candidates. We include more experiments\nwith simulated user interactions in Appendix E.\nOur representation’s perplexity is the smallest.\nWe validate that our PyDict+Program representa-\ntion adheres more closely to the code pre-training\ncorpora by measuring its zero-shot perplexity on\nCodeT5 using our development set (Section 3.1).\n3We did not use CoditT5 for PyDict+Program because it\nwas pre-trained on token-level edit representations. Its decoder\nmay be specialized in generating edits instead of programs.\n1361\nModels Query Edit CodeT5 BRIDGEv2 SmBoP\nEM EX EM EX EM EX\nNo Edit N/A N/A 62.7 (-) 63.6 (-) 70.1 (-) 68.2 (-) 74.6 (-) 75.3 (-)\nCoditT5\nSQL Token-Level 64.3 (0.1) 64.4 (0.2) 65.4 (0.5) 66.6 (0.3) 74.2 (0.4) 75.3 (0.1)\nSQL Clause-Level 67.0 (0.4) 65.4 (0.5) 71.3 (0.5) 70.9 (0.2) 76.3 (0.0) 77.2 (0.3)\nPyDict Clause-Level 67.1 (0.2) 66.5 (0.4) 70.6 (0.8) 70.8 (0.6) 76.3 (0.3) 77.0 (0.3)\nCodeT5\nSQL Token-Level 66.7 (0.9) 65.9 (0.5) 68.2 (0.4) 69.4 (0.8) 75.6 (0.4) 76.5 (0.6)\nSQL Clause-Level 68.3 (0.3) 68.2 +(0.6) 71.8 +(0.4) 72.5 +(0.2) 76.7 (0.6) 77.4 (0.3)\nPyDict Clause-Level 66.6 (0.8) 67.1 (0.8) 72.0 +(0.3) 72.4 +(0.2) 77.3 (0.6) 77.8 (0.2)\nCodeT5∗\nPyDict Program 69.2+(0.4) 68.4+(0.2) 72.5+(0.4) 73.1+(0.2) 77.3 (0.4) 77.6 (0.6)\nCodeT5 69.0 +(0.2) 68.2 +(0.1) 72.5+(0.3) 73.0 +(0.6) 78.0+(0.3) 78.5+(0.3)\nTable 3: Exact Set Match (EM) and Execution Match (EX) accuracy on Spider development set. The best perfor-\nmances are in bold and the second bests are underlined. Results with + are statistically significant (McNemar’s;\np < 0.05) compared to CodeT5-SQL+Token-Level (Appendix D). Otherwise, the results are not statistically\nsignificant. ∗We fine-tune the model to generate edit programs only (without resulting queries) and use Python\ninterpreter to execute the edit actions.\nFigure 1: CodeT5’s zero-shot perplexity (in log scale)\nof all four representations on our synthesized SQL error\ndevelopment set.\nAs shown in Figure 1, by representing data in Py-\nDict, we can reduce the perplexity of CodeT5 by\n2 orders of magnitude. After augmenting it with\nour program representation, we further reduce the\nzero-shot perplexity of CodeT5 to only 5.96 ×102,\n3 orders of magnitude less than the SQL+Token-\nLevel representation (1.26 ×105).\nClause-level editing is more effective, especially\nwhen represented in PyDict+Program. Since\nCodeT5 consistently outperforms CoditT5 with the\nsame representations, we focus on comparisons\namong CodeT5 variations. As shown in Table\n3, compared to CodeT5-SQL+Token-Level, only\nCodeT5-PyDict+Program achieves statistically sig-\nnificant improvement on all three parsers, while\nclause-level models fail McNemar’s significance\ntest for some parsers. More concretely, it achieves\nup to 4.3 point more absolute improvement on\nEM accuracy (68.2 →72.5; BRIDGEv2) and 3.7\npoint more absolute improvement on EX accuracy\n(69.4 → 73.1; BRIDGEv2). Overall, CodeT5-\nPyDict+Program can boost the parsers’ EM accu-\nracy by 2.4–6.5. Thus, both clause-level editing\nand PyDict+Program representation can better take\nadvantage of language models of code.\n4.2 Error Analysis\nAdditionally, we conduct an error analysis (Table\n4) by sampling 100 wrong parses from all three\nparsers and classifying them into five categories:\n• Database Grounding : A generated SQL\nquery has the correct structure, but some ta-\nble/column names or entity values are wrong.\n• Incorrect Structure: A generated SQL query\nhas missing, wrong, or redundant structures.\n• Syntax & Grammar: A generated SQL query\nviolates the programming language’s syntax.\n• False Negative: A generated SQL query is se-\nmantically correct but not captured by evalua-\ntion metrics, or the gold annotation is wrong.\n• Other: All other errors, such as wrong aggre-\ngation functions, besides the above categories.\nSince the error distributions for each parser are\nsimilar, as an example, we discuss our findings\nbased on the strongest parser, SmBoP:\nDatabase grounding is the major type of error.\nAmong the 100 samples from SmBoP, we find\nthat 54 of them have database grounding errors.\nParticularly, SmBoP predicts wrong table/column\nnames in 34 parses, inaccurate entity values in 9\nparses, and incorrect JOIN relations in 11 parses.\nOur CodeT5-PyDict+Program model can success-\nfully fix 16 of the 54 erroneous parses, includ-\ning 10 parses with wrong table/column names, 4\nparses with inaccurate entity values, and 2 parses\nwith incorrect JOIN relations. We hypothesize that\n1362\nError Category CodeT5 BRIDGEv2 SmBoP\nResolved Unresolved All Resolved Unresolved All Resolved Unresolved All\nDatabase Grounding 15 51 66 14 48 62 16 38 54\nIncorrect Structure 2 15 17 2 12 14 3 23 26\nSyntax & Grammar 0 0 0 0 0 0 1 4 5\nFalse Negative 0 9 9 0 6 6 0 8 8\nOther 1 7 8 2 16 18 1 6 7\nTable 4: Analysis of 100 sample errors made by each text-to-SQL parser. We group the errors into 5 categories and\nexamine if our CodeT5-PyDict+Program model resolves them.\ndatabase grounding is also a major category of er-\nrors in our synthesized training set, so our model\nhas learned to resolve similar errors. Neverthe-\nless, it still cannot correct the remaining 38 SQL\nparses. We notice that our current representation\nfor database schema is missing critical information,\nsuch as column data types and foreign key rela-\ntions, for our error correction model to fix database\ngrounding errors. Following our PyDict represen-\ntation for SQL, we suggest designing a code rep-\nresentation for database schema that includes such\ninformation to tackle this issue in future work.\nStructural errors are hard to edit automatically.\nBesides database grounding, 26 of SmBoP’s er-\nrors belong to another category, incorrect struc-\nture. These 26 samples contain 7 parses with incor-\nrect SQL clauses and 19 parses with incorrect sub-\nqueries, but our CodeT5-PyDict+Program model\nonly resolves 1 and 2 of them, respectively. We\nfind that correcting such errors usually involves\nmultiple edit steps, which motivates us to incor-\nporate our model into an interactive framework in\nfuture work. As our experiments with simulated\nuser interaction (Appendix E.2) show, when our\nmodel interacts with the simulated user to correct\none clause at a time, it is able to fully correct more\nSQL parses. Thus, we deem interactive correction\nwould maximize our model’s utility in practice.\n5 Related Work\nSince the release of CodeBERT (Feng et al., 2020),\nmany language models of code have emerged for\nprogram understanding and generation (Ahmad\net al., 2021; Chen et al., 2021; Guo et al., 2021;\nWang et al., 2021; Guo et al., 2022; Fried et al.,\n2023; Nijkamp et al., 2023). In addition to program-\nrelated tasks, recent work shows they also excel at\nprocessing natural language structures. Using code\nas meaning representations (MRs), we can lever-\nage language models of code in various tasks, such\nas commonsense reasoning (Madaan et al., 2022),\naction planning (Singh et al., 2022), and event ex-\ntraction (Wang et al., 2022). In fact, how to design\nMRs to reduce model learning difficulty is a salient\nresearch question in semantic parsing (Guo et al.,\n2019; Gan et al., 2021b; Nie et al., 2022).\nOur work demonstrates that program-related\ntasks themselves can also benefit from code-based\nMRs. Specifically, we apply such MRs to SQL\nerror correction, a variant of automatic program\nrepair tasks (Tufano et al., 2019; Panthaplackel\net al., 2022; Zhang et al., 2023). Although SQL is\na code-based MR, it is much harder for models to\nlearn compared to other MRs, such as FunQL and\nlambda calculus (Li et al., 2022). Consequently,\nwithout many SQL queries in their pre-training cor-\npora, language models of code can underperform\nstate-of-the-art text-to-SQL parsers. By converting\nSQL queries into Python dictionaries, we can ex-\nplicitly represent their compositional substructures\nand define edit actions as programs, which reduces\nthe learning difficulty for language models of code\nand yields better performance.\n6 Conclusion and Future Work\nThis paper presents a study on developing a text-to-\nSQL error correction model with clause-level edits\nand different representations. Our comprehensive\nexperiments demonstrate that clauses are better se-\nmantic units than tokens for editing SQL queries\nand mimicking patterns in code pre-training cor-\npora helps better leverage language models of\ncode. As a future direction, we plan to incorporate\nour model into interactive semantic parsing frame-\nworks (Li et al., 2020; Yao et al., 2019, 2020; Zeng\net al., 2020) by suggesting possible edits to users\nonce a wrong parse is identified. In this way, users\nwould more efficiently correct parse errors and get\nbetter assistance. We also plan to experiment with\nother language models of code (Fried et al., 2023;\nNijkamp et al., 2023) and text-to-SQL datasets\n(Zelle and Mooney, 1996; Gan et al., 2021a) to\nverify the generalizability of our method.\n1363\nLimitations\nActual applications of our model. Our work\nassumes that input SQL queries to our model are\nalways wrong. This assumption is more feasible in\nan interactive semantic parsing framework, where\nthe users are expected to decide whether a SQL\nparse, accompanied by its natural language expla-\nnations (Elgohary et al., 2020, 2021; Narechania\net al., 2021; Mo et al., 2022), has errors or not. Al-\nternatively, to remove this assumption, it would be\ninteresting for future work to study the performance\nof our error correction model in combination with\nan automatic error detection model (Chen et al.,\n2023).\nExperiments with more language models of code.\nWe have only experimented with two language\nmodels of code, CoditT5 and CodeT5, both us-\ning T5-base (Raffel et al., 2020) as their underlying\nmodel architecture. It would be interesting to test\nhow our conclusions generalize to other language\nmodels of code in the future. Based on the strong\ncapabilities of large language models of code, such\nas Codex (Chen et al., 2021), InCoder (Fried et al.,\n2023), and CodeGen (Nijkamp et al., 2023), we\nbelieve that these models can better exploit their\nknowledge about data structures and their opera-\ntions in Python. These models may perform even\nbetter on Text-to-SQL error correction with our\nproposed representations.\nAcknowledgements\nWe would like to thank the anonymous reviewers\nand colleagues from the OSU NLP group for their\nthoughtful comments. This research was supported\nin part by a sponsored award from Cisco Research,\nNSF IIS-1815674, NSF CAREER #1942980, NSF\nOAC-2112606, and Ohio Supercomputer Center\n(Center, 1987). The views and conclusions con-\ntained herein are those of the authors and should not\nbe interpreted as representing the official policies,\neither expressed or implied, of the U.S. government.\nThe U.S. Government is authorized to reproduce\nand distribute reprints for Government purposes\nnotwithstanding any copyright notice herein. Ziru\nis also supported by The Ohio State University\nGraduate School through University Fellowship.\nReferences\nWasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and\nKai-Wei Chang. 2021. Unified pre-training for pro-\ngram understanding and generation. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2655–2668,\nOnline. Association for Computational Linguistics.\nOhio Supercomputer Center. 1987. Ohio supercomputer\ncenter.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluating\nlarge language models trained on code.\nShijie Chen, Ziru Chen, Huan Sun, and Yu Su. 2023.\nError detection for text-to-sql semantic parsing.\nXiang Deng, Ahmed Hassan Awadallah, Christopher\nMeek, Oleksandr Polozov, Huan Sun, and Matthew\nRichardson. 2021. Structure-grounded pretraining\nfor text-to-SQL. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 1337–1350, Online. As-\nsociation for Computational Linguistics.\nAhmed Elgohary, Saghar Hosseini, and Ahmed Has-\nsan Awadallah. 2020. Speak to your parser: Interac-\ntive text-to-SQL with natural language feedback. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 2065–\n2077, Online. Association for Computational Lin-\nguistics.\nAhmed Elgohary, Christopher Meek, Matthew\nRichardson, Adam Fourney, Gonzalo Ramos,\nand Ahmed Hassan Awadallah. 2021. NL-EDIT:\nCorrecting semantic parse errors through natural\nlanguage interaction. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 5599–5610, Online.\nAssociation for Computational Linguistics.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, and Ming Zhou. 2020. Code-\nBERT: A pre-trained model for programming and\nnatural languages. In Findings of the Association\n1364\nfor Computational Linguistics: EMNLP 2020, pages\n1536–1547, Online. Association for Computational\nLinguistics.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang,\nEric Wallace, Freda Shi, Ruiqi Zhong, Scott Yih,\nLuke Zettlemoyer, and Mike Lewis. 2023. Incoder:\nA generative model for code infilling and synthesis.\nIn The Eleventh International Conference on Learn-\ning Representations.\nYujian Gan, Xinyun Chen, Qiuping Huang, Matthew\nPurver, John R. Woodward, Jinxia Xie, and Peng-\nsheng Huang. 2021a. Towards robustness of text-\nto-SQL models against synonym substitution. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 2505–\n2515, Online. Association for Computational Lin-\nguistics.\nYujian Gan, Xinyun Chen, Jinxia Xie, Matthew Purver,\nJohn R. Woodward, John Drake, and Qiaofu Zhang.\n2021b. Natural SQL: Making SQL easier to infer\nfrom natural language specifications. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, pages 2030–2042, Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nDaya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming\nZhou, and Jian Yin. 2022. UniXcoder: Unified cross-\nmodal pre-training for code representation. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 7212–7225, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nDaya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu\nTang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svy-\natkovskiy, Shengyu Fu, Michele Tufano, Shao Kun\nDeng, Colin Clement, Dawn Drain, Neel Sundaresan,\nJian Yin, Daxin Jiang, and Ming Zhou. 2021. Graph-\nCodeBERT: Pre-training code representations with\ndata flow. In International Conference on Learning\nRepresentations.\nJiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian-\nGuang Lou, Ting Liu, and Dongmei Zhang. 2019. To-\nwards complex text-to-SQL in cross-domain database\nwith intermediate representation. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4524–4535, Florence,\nItaly. Association for Computational Linguistics.\nHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2020. Code-\nsearchnet challenge: Evaluating the state of semantic\ncode search.\nMagne Jorgensen and Martin Shepperd. 2007. A sys-\ntematic review of software development cost estima-\ntion studies. IEEE Transactions on Software Engi-\nneering, 33(1):33–53.\nYuntao Li, Bei Chen, Qian Liu, Yan Gao, Jian-Guang\nLou, Yan Zhang, and Dongmei Zhang. 2020. “what\ndo you mean by that?” a parser-independent interac-\ntive approach for enhancing text-to-SQL. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n6913–6922, Online. Association for Computational\nLinguistics.\nZhenwen Li, Jiaqi Guo, Qian Liu, Jian-Guang Lou, and\nTao Xie. 2022. Exploring the secrets behind the learn-\ning difficulty of meaning representations for semantic\nparsing. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3616–3625, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nXi Victoria Lin, Richard Socher, and Caiming Xiong.\n2020. Bridging textual and tabular data for cross-\ndomain text-to-SQL semantic parsing. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2020, pages 4870–4888, Online. Association\nfor Computational Linguistics.\nAman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang,\nand Graham Neubig. 2022. Language models of code\nare few-shot commonsense learners. In Proceedings\nof the 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1384–1403, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nQuinn McNemar. 1947. Note on the sampling error\nof the difference between correlated proportions or\npercentages. In Psychometrika, volume 12, page\n153–157.\nLingbo Mo, Ashley Lewis, Huan Sun, and Michael\nWhite. 2022. Towards transparent interactive seman-\ntic parsing via step-by-step correction. In Findings of\nthe Association for Computational Linguistics: ACL\n2022, pages 322–342, Dublin, Ireland. Association\nfor Computational Linguistics.\nMarius Mosbach, Maksym Andriushchenko, and Diet-\nrich Klakow. 2021. On the stability of fine-tuning\nBERT: Misconceptions, explanations, and strong\nbaselines. In International Conference on Learning\nRepresentations.\nArpit Narechania, Adam Fourney, Bongshin Lee, and\nGonzalo Ramos. 2021. Diy: Assessing the correct-\nness of natural language to sql systems. In 26th\nInternational Conference on Intelligent User Inter-\nfaces, IUI ’21, page 597–607, New York, NY , USA.\nAssociation for Computing Machinery.\nLunyiu Nie, Shulin Cao, Jiaxin Shi, Jiuding Sun,\nQi Tian, Lei Hou, Juanzi Li, and Jidong Zhai. 2022.\nGraphQ IR: Unifying the semantic parsing of graph\nquery languages with one intermediate representation.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n5848–5865, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\n1365\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2023. Codegen: An open large language\nmodel for code with multi-turn program synthesis. In\nThe Eleventh International Conference on Learning\nRepresentations.\nSheena Panthaplackel, Milos Gligoric, Junyi Jessy Li,\nand Raymond Mooney. 2022. Using developer dis-\ncussions to guide fixing bugs in software. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2022, pages 2292–2301, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch: An\nimperative style, high-performance deep learning li-\nbrary. In Advances in Neural Information Processing\nSystems, volume 32. Curran Associates, Inc.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nOhad Rubin and Jonathan Berant. 2021. SmBoP: Semi-\nautoregressive bottom-up semantic parsing. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n311–324, Online. Association for Computational Lin-\nguistics.\nTorsten Scholak, Nathan Schucher, and Dzmitry Bah-\ndanau. 2021. PICARD: Parsing incrementally for\nconstrained auto-regressive decoding from language\nmodels. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 9895–9901, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn Proceedings of the 35th International Conference\non Machine Learning , volume 80 of Proceedings\nof Machine Learning Research , pages 4596–4604.\nPMLR.\nIshika Singh, Valts Blukis, Arsalan Mousavian, Ankit\nGoyal, Danfei Xu, Jonathan Tremblay, Dieter Fox,\nJesse Thomason, and Animesh Garg. 2022. Prog-\nprompt: Generating situated robot task plans using\nlarge language models. In Workshop on Language\nand Robotics at CoRL 2022.\nLappoon R. Tang and Raymond J. Mooney. 2000. Au-\ntomated construction of database interfaces: Integrat-\ning statistical and relational learning for semantic\nparsing. In Proceedings of the 2000 Joint SIGDAT\nConference on Empirical Methods in Natural Lan-\nguage Processing and Very Large Corpora: Held\nin Conjunction with the 38th Annual Meeting of the\nAssociation for Computational Linguistics - Volume\n13, EMNLP ’00, page 133–141, USA. Association\nfor Computational Linguistics.\nMichele Tufano, Jevgenija Pantiuchina, Cody Watson,\nGabriele Bavota, and Denys Poshyvanyk. 2019. On\nlearning meaningful code changes via neural machine\ntranslation. In Proceedings of the 41st International\nConference on Software Engineering, ICSE ’19, page\n25–36. IEEE Press.\nBailin Wang, Richard Shin, Xiaodong Liu, Oleksandr\nPolozov, and Matthew Richardson. 2020. RAT-SQL:\nRelation-aware schema encoding and linking for text-\nto-SQL parsers. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 7567–7578, Online. Association for\nComputational Linguistics.\nXingyao Wang, Sha Li, and Heng Ji. 2022. Code4struct:\nCode generation for few-shot structured prediction\nfrom natural language.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven C.H.\nHoi. 2021. CodeT5: Identifier-aware unified pre-\ntrained encoder-decoder models for code understand-\ning and generation. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 8696–8708, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nCathrin Weiss, Rahul Premraj, Thomas Zimmermann,\nand Andreas Zeller. 2007. How long will it take to fix\nthis bug? In Fourth International Workshop on Min-\ning Software Repositories (MSR’07:ICSE Workshops\n2007), pages 1–1.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong,\nTorsten Scholak, Michihiro Yasunaga, Chien-Sheng\nWu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Vic-\ntor Zhong, Bailin Wang, Chengzu Li, Connor Boyle,\nAnsong Ni, Ziyu Yao, Dragomir Radev, Caiming\nXiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,\nLuke Zettlemoyer, and Tao Yu. 2022. UnifiedSKG:\n1366\nUnifying and multi-tasking structured knowledge\ngrounding with text-to-text language models. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 602–631,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nZiyu Yao, Yu Su, Huan Sun, and Wen-tau Yih. 2019.\nModel-based interactive semantic parsing: A unified\nframework and a text-to-SQL case study. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 5447–5458, Hong\nKong, China. Association for Computational Linguis-\ntics.\nZiyu Yao, Yiqi Tang, Wen-tau Yih, Huan Sun, and\nYu Su. 2020. An imitation game for learning se-\nmantic parsers from user interaction. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n6883–6902, Online. Association for Computational\nLinguistics.\nPengcheng Yin and Graham Neubig. 2017. A syntactic\nneural model for general-purpose code generation.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 440–450, Vancouver, Canada.\nAssociation for Computational Linguistics.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-\ning Yao, Shanelle Roman, Zilin Zhang, and Dragomir\nRadev. 2018. Spider: A large-scale human-labeled\ndataset for complex and cross-domain semantic pars-\ning and text-to-SQL task. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 3911–3921, Brussels, Bel-\ngium. Association for Computational Linguistics.\nJohn M. Zelle and Raymond J. Mooney. 1996. Learn-\ning to parse database queries using inductive logic\nprogramming. In Proceedings of the Thirteenth Na-\ntional Conference on Artificial Intelligence - Volume\n2, AAAI’96, page 1050–1055. AAAI Press.\nJichuan Zeng, Xi Victoria Lin, Steven C.H. Hoi, Richard\nSocher, Caiming Xiong, Michael Lyu, and Irwin\nKing. 2020. Photon: A robust cross-domain text-\nto-SQL system. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics: System Demonstrations , pages 204–214,\nOnline. Association for Computational Linguistics.\nJiyang Zhang, Sheena Panthaplackel, Pengyu Nie,\nJunyi Jessy Li, and Milos Gligoric. 2023. Coditt5:\nPretraining for source code and natural language edit-\ning. In Proceedings of the 37th IEEE/ACM Interna-\ntional Conference on Automated Software Engineer-\ning, ASE ’22, New York, NY , USA. Association for\nComputing Machinery.\nAppendices\nWe provide more details omitted in the main text\nas follows:\n• Appendix A: SQL PyDict Representation\n• Appendix B: Text-to-SQL Parser Selection\n• Appendix C: Implementation Details\n• Appendix D: Statistical Significance Test\n• Appendix E: Additional Results\n• Appendix F: More Representation Examples\nA SQL PyDict Representation\nWe implement the transformation from any SQL\nquery to our PyDict representation in three steps\n(Section 2.1). First, we use context-free grammar\nto parse a SQL query and obtain its abstract syn-\ntax tree (AST). The AST naturally contains a SQL\ndecomposition where each clause has its unique\nsubtree. In addition, if a clause contains a nested\nquery, it would be represented as another indepen-\ndent subtree, which is a child of the root node in\nthe clause’s AST subtree. With these substructures\nexplicitly represented, we use depth-first search to\ntraverse through the AST to build our PyDict rep-\nresentation bottom-up. In other words, if a clause\ncontains a subquery, we process the subquery tree\nas an independent SQL AST and build a dictionary\nfor it. Then, we combine it with other substructures\nof the clause with different dictionary keys. For ex-\nample, in Table F.1, we first build the dictionary for\n“subquery0” and assign this identifier as the key. In\nthe main “clause,” we replace the subquery’s corre-\nsponding span with this identifier. Finally, we use\nanother dictionary to wrap the main “clause” and\n“subquery0” together as the final representation of\nthe “where” clause. We repeat this procedure for\neach clause to incrementally add (key, value) pairs\nto the dictionary and “store” it to the variable sql,\nwhich we refer to in program edit representations.\nB Text-to-SQL Parser Selection\nWe choose existing text-to-SQL parsers in our ex-\nperiments according to two principles: the parsers\npredict database entity values, and they cover differ-\nent decoding strategies, including grammar-based\n(BRIDGEv2), bottom-up (SmBop), and token-\nbased (CodeT5). We did not include parsers using\ntop-down decoders because they usually cannot pre-\ndict entity values in conditional statements, such as\nRAT-SQL (Wang et al., 2020). Instead, we include\nBRIDGEv2 because its decoding method mimics\n1367\nthe left-to-right CFG derivation of a program, and\nit uses SQL syntax-based constraints to prevent\ngrammatical errors. In recent work, such decoders,\nalso used in PICARD (Scholak et al., 2021), are\nmore popular than top-down decoders.\nC Implementation Details\nOur models (Section 3.2) are implemented in Py-\nTorch (Paszke et al., 2019) using Huggingface\n(Wolf et al., 2020) and trained on a single NVIDIA\nRTX A6000 GPU (48GB). We use Adafactor\n(Shazeer and Stern, 2018) to train all our mod-\nels with the same hyperparameters adapted from\nMosbach et al. (2021):\n• Learning rate: 3e −5\n• Batch size: 16\n• Epochs: 10\n• Scheduler: Linear decay with 10% warmup\nD Statistical Significance Test\nTo demonstrate the effectiveness of our three\nclause-level edit representations (Section 4.1), we\nperform McNemar’s Test (McNemar, 1947) to mea-\nsure the statistical significance of their results in\ncomparison to CodeT5-SQL+Token-Level. For\neach significance test between two models, we use\nthe median results among our three runs to calcu-\nlate the comparison matrix. Then, we compute the\np-values using statsmodels.4 When p < 0.05,\nwe reject the null hypothesis. In other words, we\nconsider the accuracy improvement statistically sig-\nnificant when p <0.05.\nE Additional Results\nResults on our development set. We report\nmodel performances on our held-out development\nset (Section 3.1) in Table E.1. During training,\nwe select the best model by evaluating its EX and\nEM accuracy on the development set (Section 3.3)\nevery 500 steps. Surprisingly, we find that CodeT5-\nSQL+Clause-Level sometimes achieves the best\nperformance. For BRIDGEv2, it obtains 35.9 EM\naccuracy and 39.3 EX accuracy, while CodeT5-\nPyDict+Program only obtains 34.5 EM accuracy\nand 37.1 EX accuracy. A possible explanation is\nthat in comparison to the test set, our development\nset has SQL structures and databases that are more\n4https://www.statsmodels.org/dev/generated/\nstatsmodels.stats.contingency_tables.mcnemar.\nhtml\nsimilar to the training set, while the test set has un-\nseen SQL structures and less similar databases. It\nmay also indicate that CodeT5-SQL+Clause-Level\noverfits the synthetic training data and fails to gen-\neralize to realistic test data.\nResults for simulated interaction experiments.\nTo show the potential of using our model in an inter-\nactive framework, we extend our main experiments\n(Section 4.1) by adding simulated user interactions.\nSince our model uses beam search to decode the\nedit actions e = {e1, e2, ..., en}and the resulting\ncorrect SQL query q+ (Equation 1), we simulate\nuser interactions to select one edit action ei at a\ntime from the beam results.\nAt each time step t, we prompt the decoder with\npreviously selected edit actions e1, ..., et−1 to com-\nplete the sequence et, ..., en, q+ using beam search\nwith size 3. Then, we use gold SQL annotations\nto simulate the user interaction, which selects an\nedit action et from the three candidates at step t\nor chooses to skip the current step when all three\ncandidates are wrong. If skipping, the user con-\ntinues to check the consequent edit actions et+j\n(j = 1, 2, ..., n−t) until it selects the next edit ac-\ntion. When the interaction finishes, we append the\nselected edit action to the prompt and let the model\nregenerate a completion with the new prompt for\nthe next step’s interaction. Having simulated in-\nteractions for all edit actions, we do not use the\ngenerated q+ directly because some edit actions\nare skipped. Instead, we execute the selected ones\non the initial SQL query to derive the final query.\nAs shown in Table E.2, when collaborating with\na simulated user, our error correction model can\nfurther improve the base parsers’ accuracy. Com-\npared to its performance without using any interac-\ntions, our model achieves up to 4.1 point more abso-\nlute improvement on EM accuracy (72.5 →76.6;\nBRIDGEv2) and 5.0 point more absolute improve-\nment on EX accuracy (73.1 →78.1; BRIDGEv2).\nWith these results for simulated interaction exper-\niments, we deem that incorporating our error cor-\nrection model into an interactive framework is a\npromising future direction.\n1368\nModels Query Edit CodeT5 BRIDGEv2 SmBoP\nEM EX EM EX EM EX\nCoditT5\nSQL Token-Level 26.1 (0.4) 28.6 (1.0) 25.8 (0.3) 27.2 (0.6) 28.1 (0.9) 30.7 (0.7)\nSQL Clause-Level 28.6 (0.4) 31.3 (0.5) 28.4 (0.5) 30.0 (0.2) 30.2 (0.8) 33.4 (0.8)\nPyDict Clause-Level 28.9 (0.6) 32.3 (0.8) 28.0 (0.1) 30.1 (0.2) 27.6 (0.1) 30.9 (0.4)\nCodeT5\nSQL Token-Level 32.1 (1.1) 34.1 (1.2) 31.8 (0.4) 34.5 (0.8) 34.2 (0.1) 37.6 (0.1)\nSQL Clause-Level 36.5 (0.6) 38.6 (0.5) 35.9 (0.4) 39.3 (1.3) 36.1 (0.6) 38.8 (0.5)\nPyDict Clause-Level 35.6 (0.9) 37.9 (0.3) 32.9 (1.0) 34.8 (0.8) 33.0 (0.2) 36.3 (0.3)\nCodeT5∗\nPyDict Program 35.7 (0.8) 37.9 (0.3) 34.8 (0.8) 38.3 (0.7) 36.0 (0.3) 40.2 (0.5)\nCodeT5 36.7 (0.2) 38.5 (0.6) 34.5 (0.1) 37.1 (0.2) 35.6 (0.8) 39.0 (0.1)\nTable E.1: Exact Set Match (EM) and Execution Match (EX) accuracy on our held-out development set (Section\n3.1). The best performances are in bold and the second bests are underlined. ∗We fine-tune the model to generate\nedit programs only (without resulting queries) and use Python interpreter to execute the edit actions.\nModels Query Edit CodeT5 BRIDGEv2 SmBoP\nEM EX EM EX EM EX\nNo Edit N/A N/A 62.7 (-) 63.6 (-) 70.1 (-) 68.2 (-) 74.6 (-) 75.3 (-)\nCodeT5∗\nPyDict Program 69.2 (0.4) 68.4 (0.2) 72.5 (0.4) 73.1 (0.2) 77.3 (0.4) 77.6 (0.6)\nCodeT5 69.0 (0.2) 68.2 (0.1) 72.5 (0.3) 73.0 (0.6) 78.0 (0.3) 78.5 (0.3)\nCodeT5† PyDict Program 73.0 (0.7) 72.9 (0.8) 76.6 (0.4) 78.1 (0.2) 80.0 (0.3) 81.2 (0.6)\nTable E.2: Exact Set Match (EM) and Execution Match (EX) accuracy on Spider development set. The best\nperformances are in bold. ∗We fine-tune the model to generate edit programs only (without resulting queries) and\nuse Python interpreter to execute the edit actions. †We simulate user interactions using gold SQL queries to choose\nedit actions during beam search (size 3) and then execute the chosen actions to get the resulting SQL parse.\nF More Representation Examples\nWe provide two more examples in Table F.1 and\nF.2 to demonstrate how we represent SQL with\nsubqueries and their edits (Section 2.2). We also\nshow different representations forInsert and Delete\nedit actions.\n1369\nQuery Representation Edit Representation\nSQL select count(*)\nfrom cars_data\nwhere cars_data.accelerate > (\nselect max(cars_data.horsepower)\nfrom cars_data\n)\nToken-level <ReplaceOld> max(cars_data.horsepower) <Re-\nplaceNew> cars_data.accelerate <ReplaceEnd>\n<Insert> order by cars_data.horsepower desc\nlimit 1 <InsertEnd>\nClause-level <ReplaceOld> select\nmax(cars_data.horsepower) <ReplaceNew>\nselect cars_data.accelerate <ReplaceEnd>\n<Insert> order by cars_data.horsepower desc\nlimit 1 <InsertEnd>\nPyDict sql = {\n\"select\": \"select count(*)\",\n\"from\": \"from cars_data\",\n\"where\": {\n\"clause\": \"where cars_data.accelerate >\n(subquery0)\",\n\"subquery0\": {\n\"select\": \" select\nmax(cars_data.horsepower)\",\n\"from\": \"from cars_data\"\n}\n}\n}\nClause-level <ReplaceOld> \"select\": \"select max(\ncars_data.horsepower)\" <ReplaceNew>\n\"select\": \"select cars_data.accelerate\" <Re-\nplaceEnd> <Insert> \"orderBy\": \"order by\ncars_data.horsepower desc\", \"limit\": \"limit 1\"\n<InsertEnd>\nProgram sql[\"where\"][\"subquery0\"][\"select\"] = \"select\ncars_data.accelerate\"\nsql[\"where\"][\"subquery0\"][\"orderBy\"] = \"order\nby cars_data.horsepower desc\"\nsql[\"where\"][\"subquery0\"][\"limit\"] = \"limit 1\"\nTable F.1: Example representations for a wrong SQL query that contains a nested subquery and its edit actions\n(including Insert edits). The corresponding natural language utterance is “What is the number of cars with a greater\naccelerate than the one with the most horsepower?”\nQuery Representation Edit Representation\nSQL select employee.name\nfrom employee join evaluation on\nemployee.employee_id =\nevaluation.employee_id\ngroup by evaluation.employee_id\"\norder by sum(evaluation.bonus) desc\nlimit 1\nToken-level <Delete> group by evaluation.employee_id\n<DeleteEnd> <Delete> sum( <DeleteEnd>\n<Delete> ) <DeleteEnd>\nClause-level <Delete> group by evaluation.employee_id\n<DeleteEnd> <ReplaceOld> order by\nsum(evaluation.bonus) desc <ReplaceNew>\norder by evaluation.bonus desc <ReplaceEnd>\nPyDict sql = {\n\"select\": \"select employee.name\",\n\"from\": \"from employee join evaluation on\nemployee.employee_id =\nevaluation.employee_id\",\n\"groupBy\": \"group by\nevaluation.employee_id\",\n\"orderBy\": \"order by sum(evaluation.bonus)\ndesc\",\n\"limit\": \"limit 1\"\n}\nClause-level <Delete> \"groupBy\": \"group by evalua-\ntion.employee_id\" <DeleteEnd> <ReplaceOld>\n\"orderBy\": \"order by sum(evaluation.bonus)\ndesc\" <ReplaceNew> \"orderBy\": \"order by eval-\nuation.bonus desc\" <ReplaceEnd>\nProgram sql.pop(\"groupBy\")\nsql[\"orderBy\"] = \"order by evaluation.bonus\ndesc\"\nTable F.2: Example representations for a wrong SQL query and its edit actions (including Delete edits). The\ncorresponding natural language utterance is “Find the name of the employee who got the highest one time bonus.”\n1370\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n6\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n3\n□\u0013 B1. Did you cite the creators of artifacts you used?\n3\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\n3\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\n3\nC □\u0013 Did you run computational experiments?\n4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAppendix B\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1371\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nAppendix B\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nAppendix B\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n1372"
}