{
  "title": "NLP-CUET@DravidianLangTech-EACL2021: Offensive Language Detection from Multilingual Code-Mixed Text using Transformers",
  "url": "https://openalex.org/W3133538548",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3157958732",
      "name": "Sharif, Omar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287520837",
      "name": "Hossain, Eftekhar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2184396509",
      "name": "Hoque, Mohammed Moshiul",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2747187574",
    "https://openalex.org/W3121099462",
    "https://openalex.org/W3099919888",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3154077674",
    "https://openalex.org/W2922580172",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2962977603",
    "https://openalex.org/W2912123473",
    "https://openalex.org/W3030198970",
    "https://openalex.org/W2169477395",
    "https://openalex.org/W132962688",
    "https://openalex.org/W3119228675",
    "https://openalex.org/W3000571327",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W1492737170",
    "https://openalex.org/W3017852832",
    "https://openalex.org/W3100941475",
    "https://openalex.org/W2963667932"
  ],
  "abstract": "The increasing accessibility of the internet facilitated social media usage and encouraged individuals to express their opinions liberally. Nevertheless, it also creates a place for content polluters to disseminate offensive posts or contents. Most of such offensive posts are written in a cross-lingual manner and can easily evade the online surveillance systems. This paper presents an automated system that can identify offensive text from multilingual code-mixed data. In the task, datasets provided in three languages including Tamil, Malayalam and Kannada code-mixed with English where participants are asked to implement separate models for each language. To accomplish the tasks, we employed two machine learning techniques (LR, SVM), three deep learning (LSTM, LSTM+Attention) techniques and three transformers (m-BERT, Indic-BERT, XLM-R) based methods. Results show that XLM-R outperforms other techniques in Tamil and Malayalam languages while m-BERT achieves the highest score in the Kannada language. The proposed models gained weighted $f_1$ score of $0.76$ (for Tamil), $0.93$ (for Malayalam), and $0.71$ (for Kannada) with a rank of $3^{rd}$, $5^{th}$ and $4^{th}$ respectively.",
  "full_text": "NLP-CUET@DravidianLangTech-EACL2021: Offensive Language\nDetection from Multilingual Code-Mixed Text using Transformers\nOmar Sharif†, Eftekhar Hossain* and Mohammed Moshiul Hoque†\n†Department of Computer Science and Engineering\n*Department of Electronics and Telecommunication Engineering\nChittagong University of Engineering and Technology, Bangladesh\n{omar.sharif, eftekhar.hossain, moshiul 240}@cuet.ac.bd\nAbstract\nThe increasing accessibility of the internet\nfacilitated social media usage and encouraged\nindividuals to express their opinions liberally.\nNevertheless, it also creates a place for\ncontent polluters to disseminate offensive\nposts or contents. Most of such offensive\nposts are written in a cross-lingual manner\nand can easily evade the online surveillance\nsystems. This paper presents an automated\nsystem that can identify offensive text from\nmultilingual code-mixed data. In the\ntask, datasets provided in three languages\nincluding Tamil, Malayalam and Kannada\ncode-mixed with English where participants\nare asked to implement separate models\nfor each language. To accomplish the\ntasks, we employed two machine learning\ntechniques (LR, SVM), three deep learning\n(LSTM, LSTM+Attention) techniques and\nthree transformers (m-BERT, Indic-BERT,\nXLM-R) based methods. Results show that\nXLM-R outperforms other techniques in Tamil\nand Malayalam languages while m-BERT\nachieves the highest score in the Kannada\nlanguage. The proposed models gained\nweighted f1 score of 0.76 (for Tamil), 0.93\n(for Malayalam), and 0.71 (for Kannada) with\na rank of 3rd, 5th and 4th respectively.\n1 Introduction\nThe exponential increase of offensive contents\nin social media has become major concern to\ngovernment organizations and tech companies. It\nis impossible to identify offensive texts manually\nfrom enormous amounts of online contents\ngenerated every moment in social media and\nother online platforms. Therefore, academicians,\npolicymakers and stakeholders are trying to\ndevelop robust computational systems to limit the\nspread of offensive contents using state of the art\nNLP techniques (Kumar et al., 2020; Akiwowo\net al., 2020). In the last few years, several\nstudies have been conducted regarding offensive\nlanguages such as hate speech, aggression, toxicity\nand abusive language. Although most social\nmedia users used their regional languages to carry\nout communication; however, plenty of resources\ndeveloped in English, Arabic (Mubarak et al.,\n2017) and other high-resources languages. Lately,\npeople started using code-mixed texts on social\nmedia. Linguistic level understanding of the text,\ncomplex morphological structure, and lack of\ntraining corpora are crucial barriers to classifying\ncode-mixed data. Moreover, the system trained\non monolingual data alone may fail to classify\ncode-mixed data because of the texts linguistic\nlevel code-switching complexity. This work aims\nto address the above mentioned problems by\ncontributing the following:\n• Prepared transformer-based methods to\nidentify the offensive texts from multilingual\n(Tamil, Malayalam, Kannada) code-mixed\ndata.\n• Perform experiments on the dataset with detail\nperformance and error analysis, thus setting\nan important baseline to compare in future.\n2 Related Work\nOffensive social media content might trigger\nobjectionable consequences to its user like mental\nhealth problem, and suicide attempts (Bonanno and\nHymel, 2013). To keep the social media ecosystem,\ncoherent researchers and stakeholders should try\nto develop computational models to identify and\nclassify offensive contents within a short period.\nZampieri et al. (2019a) develop an offensive\nlanguage identiﬁcation dataset using hierarchical\nannotation schema. Three layers of annotation are\nused in the dataset: offensive language detection,\narXiv:2103.00455v1  [cs.CL]  28 Feb 2021\ncategorization of offensive language, and offensive\nlanguage target identiﬁcation. SemEval task 6\nis organized based on this dataset which had\nopened interesting research directions (Zampieri\net al., 2019b). In early stages, computational\nmodels created by using support vector machine,\nnaive Bayes and other traditional machine learning\napproach (Dadvar et al., 2013). These models\nperformance is not up to the mark as they could not\ncapture the semantic and contextual information\nin texts. This problem was mitigated with\nthe arrival of word embeddings and recurrent\nneural networks (Aroyehun and Gelbukh, 2018).\nNetworks like bidirectional LSTMs and GRUs\ncan hold contextual information from both past\nand future, creating more robust classiﬁcation\nsystems (Mandl et al., 2019). In recent years,\ntransformer-based model such as BERT (Sharif\net al., 2021), XLM-R (Ranasinghe and Zampieri,\n2020) gained more attention to identify and classify\noffensive texts. These large pre-trained model can\nclassify code-mixed texts of different languages\nwith astonishing accuracy (Hande et al., 2020;\nChakravarthi et al., 2020).\n3 Dataset\nIn order to detect offensive text from social media,\ntask organizers developed a gold standard corpus.\nAs many social media texts are code-mixed, so\nsystem trained on monolingual data fails to classify\ncode-mixed data due to the complexity of code-\nswitching at different linguistic levels in the texts.\nTo address this phenomenon, Chakravarthi et al.\n(2020) developed a code-mixed text corpus in\nDravidian languages. Corpus has three types\nof code-mixing texts (Tamil-English, Kannada-\nEnglish and Malayalam-English). The task aims\nto implement a system that can successfully detect\ncode-mixed offensive texts. To implement such\na system, we utilize the corpus provided by the\nworkshop organizers1(Chakravarthi et al., 2021).\nThe corpus consists of the text of three different\nlanguages, i.e. Tamil, Kannada and Malayalam.\nSystem developed with the different number\nof train, validation and test examples for each\nlanguage. The assigned task is a multi-class\nclassiﬁcation problem where the model has\nto classify a potential text into one of six\npredeﬁned classes. These classes are Not offensive\n(NF), Offensive-Targeted-Insult-Other (OTIO),\nOffensive-Targeted-Insult-Individual (OTII),\nOffensive-Targeted-Insult-Group (OTIG), not-\nTamil (NT)/not-Malayalam (NM)/not-Kannada\n(NK), and Offensive-Untargetede (OU). Table 1\nshows the number of instances for each class in\ntrain, validation and test sets. Datasets are highly\nimbalanced where a number of instances in NF\nclass is much higher compare to other classes.\nBefore system development, a cleaning process is\napplied to every dataset. In this phase, unwanted\ncharacters, symbols, punctuation, emojis, and\nnumbers are discarded from the texts and thus, a\ncleaned dataset prepared for each language.\n4 Methodology\nIn this part, we brieﬂy describe the methods\nand techniques employed to address the previous\nsection’s problem. First, features are extracted\nwith different feature extraction techniques and\nvarious machine learning (ML) and deep learning\nalgorithms are used for the baseline evaluation.\n1https://dravidianlangtech.github.io/2021/index.html\nTamil Malayalam Kannada\nTrain Valid Test Train Valid Test Train Valid Test\nNF 25425 3193 3190 14153 1779 1765 3544 426 427\nOTIO 454 65 71 - - - 123 16 14\nOTII 2343 307 315 239 24 27 487 66 75\nOTIG 2557 295 288 140 13 23 329 45 44\nOU 2906 356 368 191 20 29 212 33 33\nNT 1454 172 160 - - - - - -\nNM - - - 1287 163 157 - - -\nNK - - - - - - 1522 191 185\nTotal 23962 4850 2500 58500 5842 1923 36009 5724 2682\nTable 1: Class wise distribution of train, validation and test set for each language. The acronym used for classes\nabbreviated in Section 3.\nFurthermore, different transformer models, i.e.\nXLM-R, m-BERT and Indic-BERT, are exploited\nas well. Figure 1 shows the schematic process of\nthe developed system.\nFigure 1: Abstract process of of offensive language\ndetection\nFeature Extraction: ML and DL algorithms are\nunable to learn from raw texts. Therefore, feature\nextraction is required to train the classiﬁer models.\nFor ML methods, tf-idf technique (Tokunaga\nand Makoto, 1994) is applied to extract the\nunigram features. On the other hand, Word2Vec\n(Mikolov et al., 2013) and FastText (Grave et al.,\n2018) embeddings are used as feature extraction\ntechniques for DL models. Word2Vec implemented\nusing Keras embedding layer with embedding\ndimension 100 for all languages while pre-trained\nembedding matrix of each language is utilized for\nFastText embedding.\nMachine Learning Models To develop the\noffensive text detection system, we begin our\ninvestigation with traditional ML approaches,\nincluding LR and SVM. The ensemble of multiple\nML classiﬁers is also employed to achieve better\nperformance. We use ‘lbfgs’ optimizer with C\nvalue of 0.4, 0.7, and 5 for Tamil, Malayalam and\nKannada languages to implement LR. We selected\n‘linear’ SVM and settled C value to 3, 10, and\n7. Besides, Decision Tree (DT) and Random\nForest (RF) classiﬁers are incorporated with LR\nand SVM to construct the ensemble approach. For\nLR and SVM, the same parameters were retained\nwhile ‘n estimators = 100’ is used for DT and RF.\nMajority voting technique is applied to get the\nprediction from the ensemble method.\nDeep Learning Models DL algorithms are\nproven superior over many traditional ML\napproaches in various classiﬁcation tasks (Sharif\net al., 2020). We employ LSTM and combination\nof LSTM with Attention-based approach to classify\nthe offensive text to continue the investigations.\nLSTM is well known for its ability to capture\nthe semantic information as well as long term\ndependencies. Bidirectional LSTM (BiLSTM)\nwith 100 cells are used to exploit the information\nfrom both past and future states. To mitigate the\nchance of overﬁtting, a dropout technique utilized\nwith a rate of 0.1. Finally, the output of the\nBiLSTM layer transferred to a softmax layer for the\nprediction. However, sometimes all the words in a\ntext do not contribute equally for the classiﬁcation.\nTo emphasize the words that have a noticeable\nimpact on the input text, we use attention (Vaswani\net al., 2017) mechanism. An attention layer of 20\nneurons is constructed on the top of a BiLSTM\nlayer. The attention operation applied upon the\noutput of BiLSTM layer and the attention vector\npassed to a softmax layer for the prediction. We\nemploy the same architectures for all the languages\nand use ‘sparse categorical crossentropy’ as the\nloss function. ‘Adam’ optimizer with learning rate\n1e−3 and batch size 32 is used to train the models\nfor 20 epochs. The best intermediate model is\nstored by using Keras callbacks.\nTransformer Models In recent years,\ntransformers have gained popularity for its\ntremendous performance in almost every aspect of\nNLP. As our given datasets consist of cross-lingual\ntexts of different languages, we choose three\ntransformers such as XLM-R (Conneau et al.,\n2020), m-BERT (Devlin et al., 2019), Indic-BERT\n(Kakwani et al., 2020) to develop our models.\nXLM-R is a self-supervised training technique\nfor cross-lingual understanding particularly well\nfor low resource languages. On the other hand,\nm-BERT is a transformer model pre-trained over\n104 languages, and Indic-BERT is speciﬁcally\npre-trained on Indian languages such as Kannada,\nTamil, Telugu and Malayalam. These models are\nculled from Pytorch Huggingface 2 transformers\nlibrary and ﬁne-tuned on our dataset using ktrain\n(Maiya, 2020) package. To ﬁne-tune these models,\nwe use ktrain ‘ﬁt onecycle’ method with learning\nrate 2e−5 for all the languages. We observed that\neach input text’s average length is less than 50\nwords for Kannada and 70 words for Tamil and\nMalayalam languages. Therefore, to reduce the\n2https://huggingface.co/transformers/\ncomputational cost, the input texts’ maximum size\nsettle to 50 for Kannada and 70 for Tamil and\nMalayalam. All the models have trained up to 20\nepochs with batch size 4 for XLM-R and 12 for\nm-BERT and Indic-BERT.\n5 Results and Analysis\nThis section presents a performance comparison of\nvarious ML, DL, and transformer-based offensive\ntext detection models for all the mentioned\nlanguages. The superiority of the models is\ndetermined based on the weighted f1 score.\nHowever, the precision and recall metrics also\nconsidered. Table 2 reports the evaluation results\nof models. The results showed that the ML models\nensemble achieved the highest f1 score of 0.73\nfor the Tamil language. Both SVM and ensemble\nmethods obtained a similar f1 score for Malayalam\n(0.88) and Kannada (0.48) languages. In contrast,\nfor all the languages, the LR method was poorly\nperformed where a small difference (< 0.04) in f1\nscore value was observed with other ML models.\nIn case of DL techniques, LSTM (with Word2Vec)\nmodel obtained comparatively higher f1 score of\n0.72, 0.87 and 0.47 than that of provided by LSTM\n(FastText) modes which are 0.68, 0.86, and 0.45\nfor Tamil, Malayalam and Kannada languages.\nHowever, a combination of LSTM and Attention\nmodel shows 0.01 rise amounting to 0.87 and\n0.46 respectively for Malayalam and Kannada\nlanguages.\nMeanwhile, transformer-based models showed\noutstanding performance for all the languages.\nFor Tamil, Indic-BERT obtainedf1 score of 0.74\nwhile both m-BERT and XLM-R exceeds all\nthe previously mentioned models by achieving\nmaximum f1 score of 0.76. Though two models\ngive an identical f1 score, XLM-R is opted as\nthe best model by considering both the precision,\nrecall scores. In Malayalam language, m-BERT\nand Indic-BERT obtained f1 score of 0.90 and\n0.92 respectively. While XLM-R shows a rise\nof 0.01 and thus beats all the other models. For\nthe Kannada language, both XLM-R and m-BERT\noutperformed all the previous models obtaining f1\nscore of 0.71. However, m-BERT is selected as\nthe best model as it outdoes XLM-R concerning\nprecision and recall values.\nThe results revealed that the transformer-\nbased models performed exceptionally well to\naccomplish the assigned task for all the languages.\nCompared to other techniques, the ML models\nperformed moderately while the deep learning\nmodels lag than the other models. The possible\nreason for this weaker performance is the extensive\nappearance of cross-lingual words in the text. As\na result, Word2Vec and FastText embeddings have\nfailed to create the appropriate feature mapping\namong the words. Thus, LSTM and attention-\nbased models may not ﬁnd sufﬁcient relational\ndependencies among the features and performed\nbelow the expectation.\n5.1 Error Analysis\nTable 2 showed that XLM-R is the best performing\nfor Tamil and Malayalam languages model,\nwhereas, for Kannada, m-BERT is the best model.\nTo get more insights, we present a detail error\nanalysis of these models. The error analysis is\ncarried out by using the confusion matrix (Figure 2).\nOnly NF and NT classes have a high true-positive\nrate (TPR) while in other classes, texts have\nMethod Classiﬁers Tamil Malaylam Kannada\nP R F P R F P R F\nML models\nLR 0.76 0.65 0.69 0.88 0.84 0.86 0.52 0.44 0.47\nSVM 0.74 0.68 0.70 0.88 0.87 0.88 0.48 0.48 0.48\nEnsemble 0.72 0.76 0.73 0.88 0.89 0.88 0.46 0.51 0.48\nDL models\nLSTM (Word2vec) 0.73 0.72 0.72 0.86 0.87 0.86 0.48 0.44 0.46\nLSTM (Fasttext) 0.70 0.67 0.68 0.87 0.85 0.86 0.50 0.45 0.45\nLSTM + Attention 0.71 0.73 0.72 0.86 0.87 0.87 0.49 0.46 0.47\nTransformers\nm-BERT 0.74 0.78 0.76 0.93 0.88 0.90 0.70 0.74 0.71\nIndic-BERT 0.74 0.78 0.74 0.95 0.91 0.92 0.69 0.74 0.70\nXLM-R 0.75 0.78 0.76 0.92 0.94 0.93 0.71 0.70 0.71\nTable 2: Evaluation results of ML, DL and transformer-based models on the test set. Here P, R, F denotes the\nprecision, recall and weighted f1 score.\n(a) Tamil\n (b) Malayalam\n (c) Kannada\nFigure 2: Confusion matrix of the best models for each language task\nmostly misclassiﬁed as non-offensive (Figure 2\n(a)). However, among all six classes, OTIO got\n0% TPR where among 77 texts more than 50%\nwrongly classiﬁed as NF. The high-class imbalance\nsituation might be the reason for this vulnerable\noutcome. Figure 2 (b) reveals that the model is\nbiased towards NF class. Due to the insufﬁcient\nnumber of instances, the model failed to identify\nnone of the 23 OTIG texts. Among 27 OTII texts,\nonly 2 have correctly classiﬁed. Meanwhile, we\nobserve that only NF, OTII and NK classes have\na high TPR where most of the cases, the model\nmisclassiﬁed as NF class (Figure 2 (c)). In contrast,\nOTIO and OU classes have shown misclassiﬁcation\nrate 100% and 97%.\n6 Conclusion\nIn this work, we have described and analyzed\nthe system’s performance implemented as\na participation in the offensive language\nidentiﬁcation shared task at EACL-2021. Initially,\nSVM, LR, LSTM, LSTM+Attention models\nhave employed with tf-idf and word embedding\nfeatures. Results indicate that ML ensemble\nachieved higher accuracy than DL methods.\nHowever, the outcomes are not promising for the\navailable datasets. Code-mixing of multilingual\ntexts might be a reason behind this. We applied\ntransformer-based models to overcome this\nsituation, which provides an astonishing rise\nin accuracy than ML and DL-based methods.\nWeighted f1 score increased from 0.73 to 0.76,\n0.88 to 0.93 and 0.48 to 0.71 for Tamil, Malayalam\nand Kannada language respectively. In future,\nthe idea of ensemble technique could be adopted\non transformer-based models to investigate the\nsystem’s overall performance.\nReferences\nSeyi Akiwowo, Bertie Vidgen, Vinodkumar\nPrabhakaran, and Zeerak Waseem, editors. 2020.\nProceedings of the Fourth Workshop on Online\nAbuse and Harms. Association for Computational\nLinguistics, Online.\nSegun Taofeek Aroyehun and Alexander Gelbukh.\n2018. Aggression detection in social media:\nUsing deep neural networks, data augmentation,\nand pseudo labeling. In Proceedings of the\nFirst Workshop on Trolling, Aggression and\nCyberbullying (TRAC-2018), pages 90–97, Santa Fe,\nNew Mexico, USA. Association for Computational\nLinguistics.\nRina A Bonanno and Shelley Hymel. 2013. Cyber\nbullying and internalizing difﬁculties: Above and\nbeyond the impact of traditional forms of bullying.\nJournal of youth and adolescence, 42(5):685–697.\nBharathi Raja Chakravarthi, Vigneshwaran\nMuralidaran, Ruba Priyadharshini, and John Philip\nMcCrae. 2020. Corpus creation for sentiment\nanalysis in code-mixed Tamil-English text. In\nProceedings of the 1st Joint Workshop on Spoken\nLanguage Technologies for Under-resourced\nlanguages (SLTU) and Collaboration and\nComputing for Under-Resourced Languages\n(CCURL), pages 202–210, Marseille, France.\nEuropean Language Resources association.\nBharathi Raja Chakravarthi, Ruba Priyadharshini,\nNavya Jose, Anand Kumar M, Thomas Mandl,\nPrasanna Kumar Kumaresan, Rahul Ponnusamy,\nHariharan V , Elizabeth Sherly, and John Philip\nMcCrae. 2021. Findings of the shared task\non Offensive Language Identiﬁcation in Tamil,\nMalayalam, and Kannada. In Proceedings of\nthe First Workshop on Speech and Language\nTechnologies for Dravidian Languages. Association\nfor Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman\nGoyal, Vishrav Chaudhary, Guillaume Wenzek,\nFrancisco Guzm ´an, Edouard Grave, Myle Ott,\nLuke Zettlemoyer, and Veselin Stoyanov. 2020.\nUnsupervised cross-lingual representation learning\nat scale. In Proceedings of the 58th Annual\nMeeting of the Association for Computational\nLinguistics, pages 8440–8451, Online. Association\nfor Computational Linguistics.\nMaral Dadvar, Dolf Trieschnigg, Roeland Ordelman,\nand Franciska de Jong. 2013. Improving\ncyberbullying detection with user context. ECIR’13,\npage 693–696, Berlin, Heidelberg. Springer-Verlag.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training\nof deep bidirectional transformers for language\nunderstanding.\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta,\nArmand Joulin, and Tomas Mikolov. 2018.\nLearning word vectors for 157 languages.\nIn Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018), Miyazaki, Japan. European Language\nResources Association (ELRA).\nAdeep Hande, Ruba Priyadharshini, and Bharathi Raja\nChakravarthi. 2020. KanCMD: Kannada\nCodeMixed dataset for sentiment analysis and\noffensive language detection. In Proceedings of\nthe Third Workshop on Computational Modeling of\nPeople’s Opinions, Personality, and Emotion’s\nin Social Media , pages 54–63, Barcelona,\nSpain (Online). Association for Computational\nLinguistics.\nDivyanshu Kakwani, Anoop Kunchukuttan, Satish\nGolla, Gokul N.C., Avik Bhattacharyya, Mitesh M.\nKhapra, and Pratyush Kumar. 2020. IndicNLPSuite:\nMonolingual corpora, evaluation benchmarks and\npre-trained multilingual language models for Indian\nlanguages. In Findings of the Association for\nComputational Linguistics: EMNLP 2020 , pages\n4948–4961, Online. Association for Computational\nLinguistics.\nRitesh Kumar, Atul Kr. Ojha, Bornini Lahiri, Marcos\nZampieri, Shervin Malmasi, Vanessa Murdock,\nand Daniel Kadar, editors. 2020. Proceedings\nof the Second Workshop on Trolling, Aggression\nand Cyberbullying. European Language Resources\nAssociation (ELRA), Marseille, France.\nArun S. Maiya. 2020. ktrain: A low-code library for\naugmented machine learning.\nThomas Mandl, Sandip Modha, Prasenjit Majumder,\nDaksh Patel, Mohana Dave, Chintak Mandlia, and\nAditya Patel. 2019. Overview of the hasoc track\nat ﬁre 2019: Hate speech and offensive content\nidentiﬁcation in indo-european languages. In\nProceedings of the 11th Forum for Information\nRetrieval Evaluation , FIRE ’19, page 14–17,\nNew York, NY , USA. Association for Computing\nMachinery.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg\nCorrado, and Jeffrey Dean. 2013. Distributed\nrepresentations of words and phrases and their\ncompositionality.\nHamdy Mubarak, Kareem Darwish, and Walid Magdy.\n2017. Abusive language detection on Arabic\nsocial media. In Proceedings of the First\nWorkshop on Abusive Language Online , pages\n52–56, Vancouver, BC, Canada. Association for\nComputational Linguistics.\nTharindu Ranasinghe and Marcos Zampieri. 2020.\nMultilingual offensive language identiﬁcation with\ncross-lingual embeddings. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5838–5844,\nOnline. Association for Computational Linguistics.\nOmar Sharif, Eftekhar Hossain, and\nMohammed Moshiul Hoque. 2020. Techtexc:\nClassiﬁcation of technical texts using convolution\nand bidirectional long short term memory network.\nOmar Sharif, Eftekhar Hossain, and\nMohammed Moshiul Hoque. 2021. Combating\nhostility: Covid-19 fake news and hostile post\ndetection in social media.\nTakenobu Tokunaga and Iwayama Makoto. 1994.\nText categorization based on weighted inverse\ndocument frequency. In Special Interest Groups\nand Information Process Society of Japan (SIG-IPSJ.\nCiteseer.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need.\nMarcos Zampieri, Shervin Malmasi, Preslav Nakov,\nSara Rosenthal, Noura Farra, and Ritesh Kumar.\n2019a. Predicting the type and target of offensive\nposts in social media. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 1415–1420, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nMarcos Zampieri, Shervin Malmasi, Preslav Nakov,\nSara Rosenthal, Noura Farra, and Ritesh Kumar.\n2019b. SemEval-2019 task 6: Identifying and\ncategorizing offensive language in social media\n(OffensEval). In Proceedings of the 13th\nInternational Workshop on Semantic Evaluation ,\npages 75–86, Minneapolis, Minnesota, USA.\nAssociation for Computational Linguistics.",
  "topic": "Malayalam",
  "concepts": [
    {
      "name": "Malayalam",
      "score": 0.9117505550384521
    },
    {
      "name": "Offensive",
      "score": 0.8223857879638672
    },
    {
      "name": "Tamil",
      "score": 0.813014030456543
    },
    {
      "name": "Computer science",
      "score": 0.7374690175056458
    },
    {
      "name": "Natural language processing",
      "score": 0.6358798146247864
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5914490222930908
    },
    {
      "name": "Telugu",
      "score": 0.5452594757080078
    },
    {
      "name": "Transformer",
      "score": 0.5002789497375488
    },
    {
      "name": "Language model",
      "score": 0.4572524428367615
    },
    {
      "name": "Kannada",
      "score": 0.44286662340164185
    },
    {
      "name": "Code (set theory)",
      "score": 0.4325272738933563
    },
    {
      "name": "World Wide Web",
      "score": 0.3468620777130127
    },
    {
      "name": "Linguistics",
      "score": 0.18710190057754517
    },
    {
      "name": "Mathematics",
      "score": 0.0950160026550293
    },
    {
      "name": "Engineering",
      "score": 0.08680778741836548
    },
    {
      "name": "Operations research",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}