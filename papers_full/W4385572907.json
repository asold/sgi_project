{
  "title": "DRLK: Dynamic Hierarchical Reasoning with Language Model and Knowledge Graph for Question Answering",
  "url": "https://openalex.org/W4385572907",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2075577364",
      "name": "Miao Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2151595138",
      "name": "Rufeng Dai",
      "affiliations": [
        "Central China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A1998630634",
      "name": "Ming Dong",
      "affiliations": [
        "Central China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2127008437",
      "name": "Tingting He",
      "affiliations": [
        "Central China Normal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4280637601",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2963895422",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W3164540570",
    "https://openalex.org/W2986836624",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W3171530662",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W2998374885",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W3172335055",
    "https://openalex.org/W3118741274",
    "https://openalex.org/W4200629408",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2767891136",
    "https://openalex.org/W2116341502",
    "https://openalex.org/W2604314403",
    "https://openalex.org/W3100452049",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3174660442",
    "https://openalex.org/W3173566921",
    "https://openalex.org/W3198080531",
    "https://openalex.org/W2963907629",
    "https://openalex.org/W3162922479",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W3097986428",
    "https://openalex.org/W2983995706",
    "https://openalex.org/W2547185913",
    "https://openalex.org/W3116099796",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W4221154592",
    "https://openalex.org/W4226281578"
  ],
  "abstract": "In recent years, Graph Neural Network (GNN) approaches with enhanced knowledge graphs (KG) perform well in question answering (QA) tasks. One critical challenge is how to effectively utilize interactions between the QA context and KG. However, existing work only adopts the identical QA context representation to interact with multiple layers of KG, which results in a restricted interaction. In this paper, we propose DRLK (Dynamic Hierarchical Reasoning with Language Model and Knowledge Graphs), a novel model that utilizes dynamic hierarchical interactions between the QA context and KG for reasoning. DRLK extracts dynamic hierarchical features in the QA context, and performs inter-layer and intra-layer interactions on each iteration, allowing the KG representation to be grounded with the hierarchical features of the QA context. We conduct extensive experiments on four benchmark datasets in medical QA and commonsense reasoning. The experimental results demonstrate that DRLK achieves state-of-the-art performances on two benchmark datasets and performs competitively on the others.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5123–5133\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nDRLK: Dynamic Hierarchical Reasoning with Language Model and\nKnowledge Graph for Question Answering\nMiao Zhang1,2,3, Rufeng Dai2,3,4, Ming Dong2,3,4∗, Tingting He2,3,4∗\n1National Engineering Research Center for E-Learning,\n2Hubei Provincial Key Laboratory of Artiﬁcial Intelligence and Smart Learning,\n3National Language Resources Monitoring and Research Center for Network Media,\n4School of Computer, Central China Normal University, Wuhan, China\n{zmzhangmiao,dairufeng}@mails.ccnu.edu.cn\n{dongming,tthe}@ccnu.edu.cn\nAbstract\nIn recent years, Graph Neural Network (GNN)\napproaches with enhanced knowledge graphs\n(KG) perform well in question answering (QA)\ntasks. One critical challenge is how to ef-\nfectively utilize interactions between the QA\ncontext and KG. However, existing work only\nadopts the identical QA context representa-\ntion to interact with multiple layers of KG,\nwhich results in a restricted interaction. In\nthis paper, we propose DRLK ( Dynamic Hi-\nerarchical Reasoning with Language Model\nand Knowledge Graphs), a novel model that\nutilizes dynamic hierarchical interactions be-\ntween the QA context and KG for reasoning.\nDRLK extracts dynamic hierarchical features\nin the QA context, and performs inter-layer\nand intra-layer interactions on each iteration,\nallowing the KG representation to be grounded\nwith the hierarchical features of the QA con-\ntext. We conduct extensive experiments on four\nbenchmark datasets in medical QA and com-\nmonsense reasoning. The experimental results\ndemonstrate that DRLK achieves state-of-the-\nart performances on two benchmark datasets\nand performs competitively on the others1.\n1 Introduction\nQuestion answering (QA) system is a hot research\narea in natural language processing, requiring the\nrobot to clearly understand the scenario described\nin the question and then reason with relevant do-\nmain knowledge (Jin et al., 2022). Recently, large-\nscale pre-trained language models (LMs) (Gu et al.,\n2022; Liu et al., 2021) have become a popular\nsolution in several QA datasets (Mutabazi et al.,\n2021), achieving excellent performance. By train-\ning on an ultra large-scale corpus, LMs learn the la-\ntent domain knowledge and perform well in down-\nstream tasks through ﬁne-tuning (Lewis et al., 2020;\n∗Corresponding author.\n1Our code is available at https://github.com/MZ-\nMiaoZhang/DRLK\nKG\nQ A man presents it n rashes on face and also \ncomplains of decreased mental function. He is also \nhaving few macular lesions on his skin. On CT scan, \nintracranial calcification was seen. His 6-year old son \nis also having similar skin lesions. What would be \nthe most likely diagnosis?\nA a) Neurofibrornatosis-1     \nb) Neurofibromatosis-2\nc) Xeroderma pigmentosum   \nd) Autosomal dominant inheritanceMacular  \nlesions\nCerebral \ncalcification\nAutosomal \ndominant \ninheritance\nFamilial ganglia\ncalcification\nAutosomal \nrecessive\nIntracranial\n calcification\nXeroderma \npigmentosum\nHaw river\nsyndrome\nFigure 1: An example from the MedMCQA dataset with\nKG, where the correct answer is indicated in bold.\nChakraborty et al., 2020). However, ﬁne-tuned\nLMs perform poorly when downstream tasks in-\nvolve complex reasoning or require explicit knowl-\nedge. The ﬁne-tuned approach relies on similar\ntask patterns and sample forms, while black-box\nmodels result in uninterpretable behavior (McCoy\net al., 2019).\nOne research topic is to introduce external\nknowledge graphs (KGs) for effective and inter-\npretable joint reasoning. Large-scale KGs, such\nas UMLS (Bodenreider, 2004) and DrugBank\n(Wishart et al., 2018), explicitly deﬁne structured\nknowledge through triples, entities, and relations.\nExisting work (Yasunaga et al., 2021; Zhang et al.,\n2022) demonstrates that KGs perform well in rea-\nsoning tasks involving knowledge. However, the\nquestion in QA task is always in natural language\nrather than a structured and logical query. The in-\nevitable challenge is to constrain and integrate the\nstructured KG according to the question, so as to\n5123\nextend the reasoning advantage to QA.\nFurthermore, well-designed graph neural net-\nworks (GNNs) (Scarselli et al., 2009; Schlichtkrull\net al., 2018; Yasunaga et al., 2021) are employed\nfor structured knowledge processing. Related ap-\nproaches follow a two-stage paradigm (Lin et al.,\n2019; Feng et al., 2020): retrieval and modeling.\nFirst, researchers construct the knowledge sub-\ngraph by retrieving triples relevant to question via\ncharacter matching or entity recognition. Fig. 1\nshows an example of knowledge subgraph, where\n\"cerebral calciﬁcation\" and \"macular lesions\" are\ncore entities in the question. Then, designed GNN\nmodules are utilized to constrain the knowledge\nsubgraph and perform inference. Structured infer-\nence paths are contained in multi-hop relations in\nthe subgraph. However, these methods only focus\non isolated modeling of multi-hop relationships in\nKG. They only interact in a shallow manner, fusing\nthe QA context and KG representations on the out-\nput layer or enhancing KG representations by the\nQA context statically (Zhang et al., 2022; Sun et al.,\n2022). Consequently, these methods demonstrate\nlimited ability to exchange useful information. Ef-\nfective interaction, especially in a non-shallow way\nbetween KG and QA context, is critical to break-\ning the bottleneck of correctly understanding the\ncomplex knowledge relationships in the question.\nAccording to the above consideration, we pro-\npose DRLK, a novel model that utilizes hierarchical\ninteractions between QA context and KG for rea-\nsoning (See in Fig. 2). DRLK extracts dynamic hi-\nerarchical features in the QA context, and performs\ninter-layer and intra-layer interactions on each iter-\nation, allowing KG representations to be grounded\nwith the hierarchical features of the QA context.\nSpeciﬁcally, we design the hierarchical awareness\nmodule and the heterogeneous relationship module\nfor dynamic hierarchical interactions. The former\nextracts hierarchical features of the QA context and\nKG, while the latter performs the message passing\nmechanism on the heterogeneous relational net-\nwork to update the KG. DRLK employs dynamic\nhierarchical interactions between the QA context\nand KG via inter-layer and intra-layer interactions,\naccomplishing correct reasoning via an iterative\nexecution of above interactions.\nIn summary, our contributions are three-fold:\n• We propose DRLK, a novel approach that fo-\ncuses on the hierarchical features of KG and\nthe QA context, employing joint reason be-\ntween LM and KG through inter-layer and\nintra-layer hierarchical interactions.\n• We design a heterogeneous relationship graph\nto perform effective hierarchical interactions\nover the heterogeneous relationships, and en-\nsure reasoning with correct knowledge rela-\ntionships.\n• We conduct experiments on four benchmark\ndatasets in medical QA and commonsense rea-\nsoning. The results show that DRLK outper-\nforms existing KG enhancement methods.\n2 Related Work\nIntegrating KG has become a hot research topic\nfor enhancing QA systems. Due to the formal het-\nerogeneity between structured knowledge and nat-\nural language, some work (Lv et al., 2020; Bian\net al., 2021) uniﬁes two description forms during\ninput, such as transforming structure knowledge\ninto text via templates or grammar. These meth-\nods use PLMs as an encoder to perform end-to-end\ninference on KG and QA context. Such formal\ntransformations inevitably lose the original formal\ncharacteristics. Other work (Bosselut et al., 2019,\n2021) models structure knowledge with GNN and\nintegrates them at the embedding representation\nlevel. Wang et al. (2019a) directly integrate the\ngraphical representation and context of knowledge\nvia the twin-tower model. Lin et al. (2019) enhance\nQA context through the KG representation. In con-\ntrast, Feng et al. (2020) augment the reasoning\nof KG by the QA context, which is usually static.\nAmong these methods, enhancing KG with the QA\ncontext has the highest ceiling and is now the most\npopular method. However, in these methods, there\nis no interaction between two representations or\nonly limited interaction with a static representation.\nAlthough such methods can model two represen-\ntations separately, the shallow interaction limits\nthe extraction of effective features. Our proposed\napproach DRLK improves mainly on this point.\nAdditionally, PLMs show excellent performance\non QA tasks, such as ﬁne-tuning (Su et al., 2019;\nChakraborty et al., 2020) and prompt learning\n(Paranjape et al., 2021; Zhong et al., 2022). These\nmethods do not require extra knowledge, but they\nare limited by the reasoning capability of PLM.\nOther researchers propose integrating the advan-\ntages of LM and KG for joint reasoning. QA-GNN\n(Yasunaga et al., 2021) proposes to consider the\n5124\nThe l-th layer of Hierarchical Awareness Module The l-th layer of  Heterogeneous Relational Module\nAnswer PredictionLanguage Model Encoder \n[CLS] question [SEP] \ncandidate answer [SEP]\nFeature Extract\nHierarchical Interaction\n… \nPlausibility\nScore\nMask Pooling\n… \nFC\nFC Softmax\nMLP\nHierarchical Awareness Module Heterogeneous Relational Module\nInter-layer Intra-layer\nEmbedding Embedding\nLM Encoder LM Encoder\nMLP\nnode\nNorm\nNorm\n… relation\n… \nnode\nrelation\nAggr\nMLP\nQA Context\nKG\nc\nv\nc\ng\n݅ܞ\nെ1ൌ1\nܰ\n ݅\nൌ1\nܰ\n \n݅\nൌ1\nܰ\n \n݅ܞ\nൌ1\nܰ\n \nെ1 ݈܋ \n݈܋ \n݅ܞ\nൌ1\nܰ\n \nܮ܋  ݈܋ \n݅\nൌ1\nܰ\n \n· \n· \nFigure 2: Overview of DRLK architecture.\nQA context as a node directly connected to KG,\nand update the representation of both the context\nand the node through the message passing mech-\nanism of the graph. However, the QA context fo-\ncuses on one node, limiting the deep interaction\nbetween LM and GNN. GreaseLM (Zhang et al.,\n2022) and JointJK (Sun et al., 2022) are further\nextensions of QA-GNN to enhance the interaction\nwhile retaining the individual structure of both mod-\nels. GreaseLM mixes LM and GNN node repre-\nsentations in the transformer module to achieve\ncommunication between the two modes. JointJK\nconsiders ﬁne-grained interactions between tokens\nin the question and entities in KG via intensive\nbi-directional attention. In contrast to previous\nwork, we focus on the cascading features of the\nQA context and KG, and update the QA context\nrepresentation by a designed hierarchical feature\nextraction. For the interaction between them, we\nconsider both before and inside the GNN layer for\nhierarchical features. In addition, we preserve the\nheterogeneous relational network and assess the\ninteraction of heterogeneous relations with the QA\ncontext, making the structured inference paths in-\nterpretable.\n3 Method\n3.1 Task Deﬁnition\nThe task of multiple choice question answering\n(MCQA) in this paper can be formulated as Y =\n{Q, A}, where Q denotes the question and A de-\nnotes the set of candidate answers. As the example\nin Fig. 1, each question has multiple candidate an-\nswers {a1, a2, . . . , ak}. The set of ground truth\nlabels is y = {yi}, where yi ∈{0, 1}k is a one-hot\nvector. k is the number of candidate answers. The\ntarget of MCQA is to select one answer with high-\nest plausibility from the candidate answer set, by\nlearning a prediction function f : Y →y.\nAdditionally, for each QA sample, a domain KG\nis assumed to be accessible, providing the neces-\nsary background knowledge. We extract a subgraph\nfrom the external KG, guided by the question and\ncandidate answers. We deﬁne the knowledge sub-\ngraph G = (V, R), where V is the set of nodes\nfrom the external KG entities, R is the set of rela-\ntionships. E = V ×R ×V deﬁnes a set of edges\nthat connect the nodes.\n3.2 Language Model Encoder\nIn the encoding component, we use a pre-trained\nLM encoder as shown in Fig. 2, such as SapBERT-\nBase (Liu et al., 2021) and RoBERTa-Large (Liu\net al., 2019), to encode the QA context and entities\nin KG separately.\nGiven a QA context {wm}M\nm=1 (question and\ncandidate answer), we ﬁrst obtain its representation\nvia the pre-trained LM encoder.\nc = LMencoder({w1, w2, . . . , wM }) (1)\nwhere c is the last hidden layer embedding of the\n5125\n[CLS] token, which represents the embedding of\nthe QA context.\nFor the set of entities V = {vi}N\ni=1 in KG, each\nentity vi ∈V is regarded as a sequence of tokens\n{vi,t}N\n′\nt=1. We concatenate vi and the QA context,\nand then encode them into a sequence of embed-\ndings by sequence-to-sequence structure in LM\nencoder.\n{ˆw1, . . . ,ˆwM , ˆvi,1, . . . ,ˆvi,N′}\n=LMencoder({w1, . . . , wM , vi,1, . . . , vi,N′})\n(2)\nNext, an average pooling operation is performed to\nget the initial representation for vi, where we only\noperate on the entity tokens and drop the question\ntokens.\nvi = Poolave({ˆvi,1, . . . ,ˆvi,N′}) (3)\nWe apply Eq. 2 and Eq. 3 on each node in {vi}N\ni=1\nto get the embedding set {vi}N\ni=1. Then the repre-\nsentation of QA context c and entities {vi}N\ni=1 will\nbe provided to the hierarchical awareness module\nfor further interactions.\n3.3 Hierarchical Awareness Module\nTo achieve an effective inter-layer interaction, we\ncapture the corresponding hierarchical features of\nthe QA context and entities in the hierarchical\nawareness module, then integrate them via a multi-\nhead attention mechanism.\nThe hierarchical awareness module accepts the\nembedding of the QA context and entities as input.\nAs shown in Fig. 2, the input of thel-th layer are the\nQA context embedding cl−1 and entity embedding\nset {vl−1\ni }N\ni=1.\nWe ﬁrst employ a special MLP, with two layers\nof neural networks, to extract hierarchical features\nof the QA context.\ncl = MLP(cl−1) (4)\nThen we update the representation of entities by\na multi-headed attention interaction (Devlin et al.,\n2019), aiming to let it focus on the features in the\ncurrent layer.\nˆvl\ni = Softmax(cl ⊙vl−1\ni√\nd\n)vl−1\ni (5)\nwhere d is the dimension of ˆvl−1\ni . The updated em-\nbedding of entity set {ˆvl\ni}N\ni=1 and the QA context\ncl will be provided as input to the l layer of the\nheterogeneous relational module. The QA context\nrepresentation will also be used as input to the l+1\nlayer of the hierarchical awareness module.\n3.4 Heterogeneous Relational Module\nThe heterogeneous relations module is a graph neu-\nral network with heterogeneous relation and node\ntypes, achieving the intra-layer interaction and mes-\nsage passing. As shown in Fig. 2, the input of the l\nlayer are the QA context embedding cl and entity\nembedding set {ˆvl\ni}N\ni=1.\nWe apply linear transformations (Feng et al.,\n2020) on relation and node types to make the model\nsensitive to heterogeneous networks.\nt(i) =Wt(i)cl\ni + bt(i)\nr(i) =Wr(i)cl\ni + br(i)\n(6)\nwhere Wt(i), Wr(i), bt(i) and br(i) are learnable\nparameters for node vi on relation and node type.\nWe apply message passing over heterogeneous\ngraphs, which is built on the RGCN (Schlichtkrull\net al., 2018). For brevity, we formulate the update\nand aggregation process of each entity in KG as:\nvl\ni = GeLU(\nN∑\ni,j=1\ntjrjˆvl−1\nj Wl) (7)\nwhere Wl is the learnable parameter. Node vj is\nthe neighbor of vi and GeLU(·) is the activation\nfunction. We deﬁne {vl\ni}N\ni=1 as the output of the\nheterogeneous relationship module, distinguishing\nfrom {ˆvl\ni}N\ni=1 in Section 3.3.\nAfter multiple layers of iterative message pass-\ning, we obtain the last layer output {vL\ni }N\ni=1 =\n{vL\n1 , vL\n2 , . . . ,vL\nN }as the ﬁnal output of KG, which\nfuses incorporating contextual information. An av-\nerage attention-based pooling and mask operation\nare applied to obtain the core KG representation g:\ng = AttPoolave({vL\n1 , vL\n2 , . . . ,vL\nN }⊙Vmask) (8)\nwhere Vmask is the masked matrix of the entity\nnodes, such as masking ﬁlled KG nodes.\n3.5 Answer Prediction\nBy means of the iterations of the hierarchical aware-\nness module and the heterogeneous relational mod-\nule, we obtain the QA context representation c and\n5126\nthe KG representation g. We calculate the scores\nof candidate answers via:\np = (a|q, g) =MLP(c; g) (9)\nFinally, we utilize the softmax function to normal-\nize all candidate answers, and obtain one choice\nwith argmaxa∈A p(a|q, g).\n4 Experiments Setup\nAll experiments are conducted on one GPU (RTX-\n8000-48GB). The framework of our code relies on\nthe PyTorch2 and Transformer3 packages.\n4.1 Datasets and Metrics\nDatasets. We evaluate DRLK on four benchmark\ndatasets across two domains: MedMCQA (Pal\net al., 2022) and MedQA-USMLE (Jin et al., 2021)\nare in medical QA; OpenBookQA (Mihaylov et al.,\n2018) and CommonsenseQA (Talmor et al., 2019)\nare in commonsense reasoning.\nMedMCQA is a 4-choice question answering\ndataset of medical entrance exams, including more\nthan 194k questions from AIIMS and NEET PG\nentrance exams. We conduct experiments on the\noriginal data splits from Pal et al. (2022).\nMedQA-USMLE is a 4-choice question answer-\ning dataset about biomedical and clinical question\nbased on the United States Medical License Exams.\nWe conduct experiments on the ofﬁcial data splits\nin Jin et al. (2021).\nOpenBookQA is a 5-choice question answering\ndataset about scientiﬁc knowledge. We conduct ex-\nperiments on the ofﬁcial data splits from Mihaylov\net al. (2018).\nCommonsenseQA is a 4-choice question answer-\ning dataset about commonsense knowledge beyond\nreal world. Since the test data is inaccessible, we\nconduct experiments on the in-house data split in\nLin et al. (2019).\nDataset Train Dev Test Choices\nMedQA-USMLE 10178 1272 1273 4\nMedMCQA 182822 4183 6150 4\nOpenBookQA 4957 500 500 4\nCommonsenseQA 9741 1221 1140 5\nTable 1: Overall statistics of Datasets.\nMetrics. We follow baselines (Feng et al., 2020;\nZhang et al., 2022) to utilize the accuracy score\n2https://pytorch.org/\n3https://huggingface.co/docs\n(Acc) as the metric protocol. We report the over-\nall accuracy score on all benchmark datasets and\nsubject accuracy score on MedMCQA.\n4.2 Implementation Details\nIn pre-processing, we extract and construct knowl-\nedge subgraphs for each sample from DDB (Ya-\nsunaga et al., 2021) on Medical QA followed Zhang\net al. (2022), while ConceptNet (Speer et al., 2017)\non commonsense reasoning followed Feng et al.\n(2020). The maximum of nodes in the subgraph\nis set to 200 by truncating or completing. The\nQA context is concatenated as \" [CLS] question\n[SEP] candidate answer [SEP]\" and encoded via\nLM.\nIn training, we set the early stop mechanism\nwith the guidance of the dev set (Lin et al., 2019).\nFor hyperparameters, we set them empirically and\nmake manual tuning. The batch size is set to 128\nor a mini-size to be applied to computations on\none single GPU. We use cross-entropy loss and\nRAdam optimizer. Separate learning rates is set\nin DRLK, {1e-5, 2e-5, 5e-5} for LM encoder and\n{1e-3, 2e-3, 3e-4} for other modules. We set the\nnumber of layers (L = 4) of GNN module, with\ndropout rate 0.1 applied to each layer. For MedQA-\nUSMLE, we set the batch size to 128, maximum\nrounds to 50, and early stopping rounds to 10 as\nthe best conﬁg. The overall training takes 8 hours\non average, while the training and testing in one\nepoch take 18 minutes and 1.6 minutes on average.\n4.3 Compared Methods\nDue to the excellent performance in NLP, we use\nLM as a language encoder to obtain an initial rep-\nresentation of the input. For reasoning, DRLK\nfocuses on enhancing the hierarchical interaction\nbetween the QA context and KG, so we use the\nstrong model associated with LM and KG as the\ncomparison model.\nSince LM is KG-agnostic, a comparison with\nthe ﬁne-tuned model shows the improvement of\nKG on reasoning intuitively. We choose the cor-\nresponding pre-trained LM on different datasets,\nsuch as SapBERT-Base (Liu et al., 2021), Pub-\nmedBERT (Gu et al., 2022), BioBERT-Base and\nBioBERT-Large (Lee et al., 2020), BioRoBERTa-\nBase (Gururangan et al., 2020), SciBERT (Beltagy\net al., 2019), ClinicalBERT (Alsentzer et al., 2019),\nBERT-Base (Devlin et al., 2019), and RoBERTa-\nLarge (Liu et al., 2019).\n5127\nMethods Dev Test\nPMI (Clark et al., 2016) 29.8 31.1\nMAX OUT (Mihaylov et al., 2018) 28.9 28.6\nIR-ES (Chen et al., 2017) 34.0 35.5\nIR-CUSTOM (Chen et al., 2017) 38.3 36.1\nBERT-Base (Devlin et al., 2019) 33.9 34.3\nClinicalBERT-Base (Alsentzer et al., 2019) 33.7 32.4\nBioBERT-Base (Lee et al., 2020) 34.3 34.1\nBioRoBERTa-Base (Gururangan et al., 2020) 35.1 36.1\nRoBERT-Large (Liu et al., 2019) 35.2 35.0\nBioBERTa-Large (Lee et al., 2020) 36.1 36.7\nSapBERT-Base (Liu et al., 2021) - 37.2\n+ QA-GNN (Yasunaga et al., 2021) - 38.0\n+ GreaseLM (Zhang et al., 2022) 38.3 38.5 ∗\n+ DRLK(Ours) 39.1 40.4\nTable 2: Performance of baseline models on MedQA-\nUSMLE. Here ∗ indicates the improvement of DRLK is\nstatistically signiﬁcant (p < 0.05).\nMethods Dev Test\nBERT-Base (Devlin et al., 2019) 35.0 33.0\nClinicalBERT-Base (Alsentzer et al., 2019) 34.7 -\nBioBERT-Base (Lee et al., 2020) 38.0 37.0\nBioRoBERTa-Base (Gururangan et al., 2020) 34.6 -\nSciBERT (Beltagy et al., 2019) 39.0 39.0\nPubMedBERT (Gu et al., 2022) 40.0 41.0\nSapBERT-Base (Liu et al., 2021) 40.3 40.0\n+ QA-GNN (Yasunaga et al., 2021) 48.7 50.8\n+ GreaseLM (Zhang et al., 2022) 49.3 51.0 ∗\n+ DRLK(Ours) 51.3 52.5\nTable 3: Performance of baseline models on MedMCQA\n(without context).\nFor the evaluation of KG, we compare with sim-\nilar approaches, which also use LM as a language\nencoder, with differences in the use of KG. RGCN\n(Schlichtkrull et al., 2018), RN (Santoro et al.,\n2017), GconAttn (Wang et al., 2019b), KageNet\n(Lin et al., 2019), MHGRN (Feng et al., 2020), QA-\nGNN, GreaseLM (Zhang et al., 2022), and Join-\ntJK (Sun et al., 2022) are QA paradigms with KG\naugmentation. GreaseLM and JointJK are the best-\nperforming models as we know, which enhance\nreasoning by fusing LM and KG representations\nin GNNs. The main difference between DRLK\nand these approaches is that we not only perform\na hierarchical feature extraction of the QA context\nbut also perform a two-step interaction to enhance\nreasoning. We use LM to initialize these baselines\nfor a fair comparison, consistent with DRLK.\n5 Results and Analysis\n5.1 Main Results\nThe results of MedQA-USMLE and MedMCQA\nare shown in Table 2-4, where Table 4 shows the\nSubject Name GreaseLM DRLK (Ours)\nDev Test Dev Test\nAnaesthesia 44.1 45.8 47.1 45.8\nAnatomy 54.3 49.4 61.1 50.2\nBiochemistry 67.8 58.8 69.6 58.8\nDental 39.2 42.3 40.2 42.0\nENT 52.8 52.3 62.3 46.5\nFM 37.3 56.8 38.8 62.9\nO&G 58.5 50.2 64.3 46.1\nMedicine 53.2 50.0 59.0 57.5\nMicrobiology 54.1 52.1 55.7 57.5\nOphthalmology 65.5 54.2 67.2 62.1\nOrthopaedics 60.0 - 50.0 -\nPathology 54.9 55.7 55.5 59.3\nPediatrics 58.5 45.8 56.0 45.8\nPharmacology 61.7 56.1 60.9 58.7\nPhysiology 49.7 45.9 53.8 47.9\nPsychiatry 56.2 50.0 68.8 50.0\nRadiology 47.8 57.1 49.3 53.8\nSkin 76.5 51.7 76.5 46.7\nPSM 49.6 50.6 51.2 51.9\nSurgery 42.0 51.7 44.2 52.1\nUnknown 100.0 60.9 100.0 66.0\nAverage 49.3 51.0 51.3 52.5\nTable 4: Subject performance on MedMCQA.\nsubject wise accuracies in MedMCQA. We observe\nthat DRLK outperforms all LM models and KG\nenhanced models. In addition to DRLK, SapBERT-\nBase and GreaseLM on MedQA-USMLE are the\nbest LM ﬁne-tuning and KG augmentation models.\nDRLK has an absolute improvement of 3.2% rel-\native to the SapBERT-Base ﬁne-tuning model and\n1.9% absolute improvement over the best model\nGreaseLM. On MedMCQA, DRLK also show the\nbest performance, with absolute improvement of\n12.5% relative to the SapBERT-Base ﬁne-tuned\nmodel and 1.5% absolute improvement over the\nbest model GreaseLM 4. As shown in Table 4,\nDRLK outperforms more than half of the subjects\nin MedMCQA, equal on 5 subjects, and under-\nperform only on 5 subjects. The state-of-the-art\nperformance on MedQA-USMLE and MedMCQA\ndemonstrates the effectiveness of hierarchical in-\nteractions and heterogeneous relational reasoning\nnetworks.\nIn addition to the medical domain, we also per-\nform further robustness validation in the common-\nsense reasoning domain. Table 5 shows the com-\nparison results on CommonsenseQA and Open-\nBookQA. As a result, DRLK outperforms the best\n4We run it on MedMCQA following the open-source code\nof GreaseLM.\n5128\nDataset OBQA CSQA\nRoBERTa-large (w/o KG) 64.8 68.7\n+ RGCN (Schlichtkrull et al., 2018) 62.5 68.4\n+ GconAttn (Wang et al., 2019b) 64.8 68.6\n+ KagNet (Lin et al., 2019) - 69.0\n+ RN (Santoro et al., 2017) 65.2 69.1\n+ MHGRN (Feng et al., 2020) 66.9 71.1\n+ QA-GNN (Yasunaga et al., 2021) 67.8 73.4\n+ GreaseLM (Zhang et al., 2022) - 74.2\n+ JointLK (Sun et al., 2022) 70.3 74.4∗\n+ DRLK(Ours) 70.2 74.5\nTable 5: Performance of baseline models on Open-\nBookQA (OBQA) and CommonsenceQA (CSQA).\nHere ∗ indicates the improvement of DRLK is statis-\ntically signiﬁcant (p < 0.05).\nDataset Dev-Acc. (%)(Overall)\nDev-Acc. (%)(Questionw/ negation)\nDev-Acc. (%)(Question w/≤5 entities)\nDev-Acc. (%)(Question w/>5 entities)GreaseLM 49.3 48.0 49.4 50.0DRLK (Ours) 51.3 50.1 51.4 55.0\nTable 6: Performance of DRLK on MedMCQA dev set\non questions with negative words and different number\nof entities.\nJoinkJK on CommonsenseQA and slightly under-\nperforms on OpenBookQA. Overall, DRLK shows\nsigniﬁcant competitiveness with the best models\nin the commonsense reasoning domain. The ex-\nperimental results demonstrate the robustness of\nDRLK in different domains.\n5.2 Ablation Studies\nTable 7 shows the further analysis of the different\ncomponents of the model. We show the accuracy\nof the ablation experiments on MedMCQA.\nNumbers of Layers. We investigate the effect of\nthe layer number on KG reasoning. As shown in\nFig. 3, the growth of layers is beneﬁcial until N\n= 4. The performance starts to decrease when N\n> 4. Our analysis is that high layers make neigh-\nbors nodes be averaged too much, which leads to\noverﬁtting.\nHierarchical Features. We perform a separate\nevaluation of the layered feature extraction oper-\nation, which was the original motivation for our\nidea. As shown in Table 7, disabling the hierarchi-\ncal feature extraction operation leads to 2.1% drop\nin performance, which indicates that the operation\nextracts features from the corresponding layers and\nimpacts the subsequent reasoning.\nHierarchical Interactions. The purpose of the\nhierarchical interaction operation is to extract the\nhierarchical features of the nodes in the KG at each\nlayer. We disable this operation to evaluate its\nDataset Test\nw/o hierarchical feature (§3.3) 38.3\nw/o hierarchical interaction (§3.3) 39.4\nw/o feature and interaction (§3.3) 38.1\nw/o heterogeneous relation type (§3.4 ) 40.0\nw/o heterogeneous node type (§3.4 ) 39.8\nw/o relation and node type (§3.4 ) 38.9\nDRLK (layer = 4) 40.4\nTable 7: Ablation study on different components.\n2 3 4 5 6\n# Layers\n36\n37\n38\n39\n40\n41Acc. (%)\nDRLK GreaseLM\nFigure 3: Impact on stacked of layers.\nimpact on the overall effect. From the results in\nTable 7, disabling the operation results in 1% and\n2.3% drop in performance. The ablation experi-\nments indicate that the hierarchical interaction is\ncrucial for subsequent reasoning.\nHeterogeneous Relational Module. We evaluate\nthe relation and node type in the heterogeneous\nawareness module. Disabling them results in 0.4%,\n0.6%, 1.5% drop in performance. The ablation\nexperiments show that interactions in the heteroge-\nneous relational module are indispensable for the\nﬁnal reasoning.\n5.3 Quantitative Analysis\nTo investigate whether the holistic performance\nimprovement of DRLK is reﬂected in questions\nrequiring complex inference, we analyze the infer-\nence complexity of the question prediction, such\nas the inclusion of negatives and multiple entities\n(Sun et al., 2022). Table 6 shows the compari-\nson results of DRLK and GreaseLM (Zhang et al.,\n2022), the previous best-performing KG-enhanced\nmodels. First, the model faces enhanced noise inter-\nference and increased relational complexity when\nthe question contains more entities. For example,\nthe question in Fig. 1 involves multiple symptoms,\ne.g., \"rashes\", \"decreased mental function\", \"macu-\nlar lesions\", and \"intracranial calciﬁcation\". An-\nswering the question requires a comprehensive con-\n5129\nAutosomal \ndominant inheritance\nFamilial ganglia \ncalcification\nAutosomal \nrecessive conditions\nIntracranial \ncalcification\nXeroderma \npigmentosum\nHaw river \nsyndrome\nSubgraph First Layer Subgraph Middle Layer Subgraph Final Layer\nMacUlar\nlesions\nCerebral\ncalcification\nAutosomal \ndominant inheritance\nFamilial ganglia \ncalcification\nAutosomal \nrecessive conditions\nIntracranial \ncalcification\nXeroderma \npigmentosum\nHaw river \nsyndrome\nMacUlar\nlesions\nCerebral\ncalcification\nAutosomal \ndominant inheritance\nFamilial ganglia \ncalcification\nAutosomal \nrecessive conditions\nIntracranial \ncalcification\nXeroderma \npigmentosum\nHaw river \nsyndrome\nMacUlar\nlesions\nCerebral\ncalcification\nFigure 4: Case study of the variation in attention during DRLK reasoning, with red lines and blue nodes indicating\nhigh-weighted attention. The question corresponding to this case is that a man has a few macular lesions and\nintracranial calciﬁcation, while his 6-year-old son also has similar skin lesions.\nsideration of the diseases corresponding to these\nsymptoms. We classify the questions into two cate-\ngories: entities less than 5 and entities more than\n5. DRLK improves 2% in absolute on more en-\ntity questions and 5% in absolute on fewer entity\nquestions, indicating that our model is more ad-\nvantageous in dealing with more entity questions.\nFurthermore, the negation is a speciﬁc relationship,\nand the LM model is easily distracted by it. For\nexample, the question \"What is not a major crite-\nria for rheumatic heart disease __?\" contains the\nnegative item \"not\". The model needs to focus\non the negative relationship of \"criteria\", not only\non \"rheumatic heart disease \". We retrieve ques-\ntions with negatives and measure reasoning ability\nby the accuracy of negated questions. Compared\nto GreaseLM, DRLK improves 2.1% in absolute,\nindicating that it beneﬁts in negative QA.\n5.4 Case Study\nWe perform an interpretability analysis of DRLK’s\nreasoning process, with the shift of attention in the\nknowledge subgraph. Fig. 4 shows a case where\nthe bolded red lines and blue nodes indicate the\nhigh-weighted attention. Correspondingly, we use\nthe dotted lines and gray nodes to represent the\nlow-weighted parts. In this case, DRLK correctly\nanswers the question and infers a reasonable in-\nference path. The ﬂow in Fig. 4 from left to right\nrepresents updating model attention in KG. The\ncritical entities in the question are Cerebral calci-\nﬁcation and Macular lesions. In the left subgraph,\n\"Cerebral calciﬁcation →Intracranial calciﬁca-\ntion\", \"Cerebral calciﬁcation →Haw River syn-\ndrome\", and \"Macular lesions →Autosomal reces-\nsive conditions\", are all reasonable speculations\nabout critical entities. From left to middle, DRLK\nfocuses on the critical evidence of KG’s second\nlayer over the previous layer. In terms of KG as-\nsociation, \"Autosomal dominant inheritance\" and\n\"Xeroderma pigmentosum\" are possible scenarios,\nbut the former is related to the semantics of the\nquestion. Therefore, DRLK keeps only \" Autoso-\nmal dominant inheritance\" as the correct answer in\nthe subgraphs from middle to right.\n5.5 Error Analysis\nTo understand why DRLK fails in reason, we ana-\nlyze the error cases of MedMCQA, which with a\nlow correct and suitable sentence length between\nseveral datasets. The following is a categorization\nof 100 randomly selected error cases:\nIncomprehensible Questions. Complex questions\nin medical scenarios usually involve diagnosing\ncauses and proper treatments in speciﬁc medical\nsituations. Such questions usually need to be devel-\noped over multiple symptoms, diseases, and treat-\nments to choose the most appropriate option. This\nparticular medical situation is difﬁcult for humans\ntoo. The failure of the model to understand speciﬁc\ncomplex medical situations (individual patients)\nmay cause erroneous predictions.\nIndistinguishable Answers. The candidate an-\nswers may be similar. Distinguishing similar enti-\nties, such as {\"Anti C\", \"Anti D\", \"Anti E\", \"Anti\nLewis\"}, may be self-evident to humans, but it is\na huge challenge for the model. In addition, there\nis another situation in the dataset: the candidate\nanswers may all be correct and the choice may\ndepend on the person. Such a situation is also in-\ndistinguishable from the model.\nMissing Evidence Entity. One critical aspect of\nreasoning with KG is obtaining as much back-\nground knowledge as possible. Although we use\ndomain-related KGs, the retrieval of issue-related\nentities may miss some evidence entities due to\nthe limitation of the KG coverage. In addition, the\nlength of medical entities is usually long, making\nit difﬁcult to achieve complete identiﬁcation by\ncharacter matching. The missing evidence entity\n5130\nleads to an incomplete reasoning process, which\nmay cause erroneous predictions.\nNumerical Reasoning. Numerical questions are\nalways a challenge in reasoning. Numerical reason-\ning shows some wrong predictions, where different\ndoses of treatments need to be selected according\nto different symptoms in medical scenarios. For\nquestion \"Concentration of triple antibiotic paste\n(TAP) in treatment of revascularisation is? \", the\ncandidate answers are \"1 mg\", \"0.1 mg\", \"100 mg\",\n\"10 mg\". We predict a wrong answer as \"0.1 mg\".\n6 Conclusion and Future Work\nIn this paper, we propose a novel model that\nenables accurate reasoning through a hierarchi-\ncal interaction between the QA context and KG.\nCompared to the ﬁne-tuning based LM and KG-\nenhanced methods, DRLK achieves SOTA perfor-\nmances on two medical QA benchmark datasets.\nOn commonsense reasoning benchmark datasets,\nDRLK performs competitively. Experimental re-\nsults show that dynamic hierarchical interactions\nachieve superiority in dealing with complex knowl-\nedge relationships. In addition, the results on dif-\nferent domains show that DRLK possesses gener-\nalizations for the question answering task.\n7 Limitations\nTo validate the effectiveness of DRLK, we conduct\nextensive experiments on four benchmark datasets,\nwith different domains and scales. The results on\nfour datasets show that DRLK achieves SOTA per-\nformance on two, just slightly inferior on the oth-\ners. Nonetheless, DRLK relies on the domain KG,\nwhere the absence or low quality of the KG will\ndirectly affect the performance of DRLK.\nMoreover, we follow Feng et al. (2020) to extract\na knowledge subgraph with 200 nodes for each\nquestion. Due to the limitation of GPU resources,\nwe do not test the effect of different knowledge\nsubgraph scales, as it is not a major concern of\nDRLK. In response to the above two limitations,\nwe will conduct further research in the future.\nEthical Considerations\nWe honor and support the ACL code of Ethics.\nThis paper focuses on the question answering task,\nwhich aims to answer questions with an extra\nknowledge graph. Moreover, the datasets we use\nin this paper are from published open-source work\nand have no privacy or ethical implications. We\nneither introduce any social or ethical bias to the\nmodel nor amplify any bias in the data, so we do\nnot foresee any direct social consequences or ethi-\ncal issues.\nAcknowledgements\nThis work is partially supported by the Key\nResearch and Development Program of Hubei\nProvince (2020BAB017), and Scientiﬁc Research\nCenter Program of National Language Commis-\nsion (ZDI135-135), and the Fundamental Research\nFunds for the Central Universities (KJ02502022-\n0155, CCNU22XJ037).\nReferences\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clin-\nical BERT embeddings. In Proceedings of the 2nd\nClinical Natural Language Processing Workshop ,\npages 72–78, Minneapolis, Minnesota, USA. Associ-\nation for Computational Linguistics.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientiﬁc text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3615–\n3620, Hong Kong, China. Association for Computa-\ntional Linguistics.\nNing Bian, Xianpei Han, Bo Chen, and Le Sun. 2021.\nBenchmarking knowledge-enhanced commonsense\nquestion answering via knowledge-to-text transfor-\nmation. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, pages 12574–12582. AAAI\nPress.\nOlivier Bodenreider. 2004. The uniﬁed medical lan-\nguage system (umls): integrating biomedical termi-\nnology. Nucleic acids research, 32:267–270.\nAntoine Bosselut, Ronan Le Bras, and Yejin Choi. 2021.\nDynamic neuro-symbolic knowledge graph construc-\ntion for zero-shot commonsense question answering.\npages 4923–4931, Proceedings of the AAAI Confer-\nence on Artiﬁcial Intelligence. AAAI Press.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. COMET: Commonsense transformers for auto-\nmatic knowledge graph construction. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4762–4779, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nSouradip Chakraborty, Ekaba Bisong, Shweta Bhatt,\nThomas Wagner, Riley Elliott, and Francesco\n5131\nMosconi. 2020. BioMedBERT: A pre-trained\nbiomedical language model for QA and IR. In\nProceedings of the 28th International Conference\non Computational Linguistics , pages 669–679,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading wikipedia to answer open-\ndomain questions. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 1870–1879, Vancouver, Canada.\nAssociation for Computational Linguistics.\nPeter Clark, Oren Etzioni, Tushar Khot, Ashish Sab-\nharwal, Oyvind Tafjord, Peter D. Turney, and Daniel\nKhashabi. 2016. Combining retrieval, statistics, and\ninference to answer elementary science questions.\npages 2580–2586, Proceedings of the AAAI Confer-\nence on Artiﬁcial Intelligence. AAAI Press.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nYanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng\nWang, Jun Yan, and Xiang Ren. 2020. Scalable multi-\nhop relational reasoning for knowledge-aware ques-\ntion answering. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1295–1309, Online. As-\nsociation for Computational Linguistics.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2022. Domain-speciﬁc lan-\nguage model pretraining for biomedical natural lan-\nguage processing. ACM Transactions on Computing\nfor Healthcare, 3:2:1–2:23.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2021. What disease\ndoes this patient have? a large-scale open domain\nquestion answering dataset from medical exams. Ap-\nplied Sciences, 11:6421.\nQiao Jin, Zheng Yuan, Guangzhi Xiong, Qianlan Yu,\nHuaiyuan Ying, Chuanqi Tan, Mosha Chen, Song-\nfang Huang, Xiaozhong Liu, and Sheng Yu. 2022.\nBiomedical question answering: A survey of ap-\nproaches and challenges. ACM Computing Surveys,\n55:35:1–35:36.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36:1234–1240.\nPatrick Lewis, Myle Ott, Jingfei Du, and Veselin Stoy-\nanov. 2020. Pretrained language models for biomedi-\ncal and clinical tasks: Understanding and extending\nthe state-of-the-art. In Proceedings of the 3rd Clini-\ncal Natural Language Processing Workshop, pages\n146–157, Online. Association for Computational Lin-\nguistics.\nBill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang\nRen. 2019. KagNet: Knowledge-aware graph net-\nworks for commonsense reasoning. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2829–2839, Hong Kong,\nChina. Association for Computational Linguistics.\nFangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco\nBasaldella, and Nigel Collier. 2021. Self-alignment\npretraining for biomedical entity representations. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 4228–4238, Online. Association for Computa-\ntional Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. arXiv:1907.11692.\nShangwen Lv, Daya Guo, Jingjing Xu, Duyu Tang, Nan\nDuan, Ming Gong, Linjun Shou, Daxin Jiang, Gui-\nhong Cao, and Songlin Hu. 2020. Graph-based rea-\nsoning over heterogeneous external knowledge for\ncommonsense question answering. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence, pages\n8449–8456. AAAI Press.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right\nfor the wrong reasons: Diagnosing syntactic heuris-\ntics in natural language inference. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 3428–3448, Florence,\nItaly. Association for Computational Linguistics.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question an-\nswering. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2381–2391, Brussels, Belgium. Association\nfor Computational Linguistics.\n5132\nEmmanuel Mutabazi, Jianjun Ni, Guangyi Tang, and\nWeidong Cao. 2021. A review on medical textual\nquestion answering systems based on deep learning\napproaches. Applied Sciences, 11:5456.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikan-\nnan Sankarasubbu. 2022. Medmcqa: A large-scale\nmulti-subject multi-choice dataset for medical do-\nmain question answering. In Conference on Health,\nInference, and Learning, pages 248–260. PMLR.\nBhargavi Paranjape, Julian Michael, Marjan\nGhazvininejad, Hannaneh Hajishirzi, and Luke\nZettlemoyer. 2021. Prompting contrastive explana-\ntions for commonsense reasoning tasks. In Findings\nof the Association for Computational Linguistics:\nACL-IJCNLP 2021 , pages 4179–4192, Online.\nAssociation for Computational Linguistics.\nAdam Santoro, David Raposo, David G. T. Barrett,\nMateusz Malinowski, Razvan Pascanu, Peter W.\nBattaglia, and Tim Lillicrap. 2017. A simple neu-\nral network module for relational reasoning. In An-\nnual Conference on Neural Information Processing\nSystems, pages 4967–4976.\nFranco Scarselli, Marco Gori, Ah Chung Tsoi, Markus\nHagenbuchner, and Gabriele Monfardini. 2009. The\ngraph neural network model. IEEE transactions on\nneural networks, 20:61–80.\nMichael Sejr Schlichtkrull, Thomas N. Kipf, Peter\nBloem, Rianne van den Berg, Ivan Titov, and Max\nWelling. 2018. Modeling relational data with graph\nconvolutional networks. In European semantic web\nconference, pages 593–607, Cham. Springer.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the Thirty-First\nAAAI Conference on Artiﬁcial Intelligence , pages\n4444–4451.\nDan Su, Yan Xu, Genta Indra Winata, Peng Xu,\nHyeondey Kim, Zihan Liu, and Pascale Fung. 2019.\nGeneralizing question answering system with pre-\ntrained language model ﬁne-tuning. In Proceedings\nof the 2nd Workshop on Machine Reading for Ques-\ntion Answering, pages 203–211, Hong Kong, China.\nAssociation for Computational Linguistics.\nYueqing Sun, Qi Shi, Le Qi, and Yu Zhang. 2022.\nJointLK: Joint reasoning with language models and\nknowledge graphs for commonsense question answer-\ning. In Proceedings of the 2022 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, pages 5049–5060, Seattle, United States. Asso-\nciation for Computational Linguistics.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149–4158, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nXiaoyan Wang, Pavan Kapanipathi, Ryan Musa, Mo Yu,\nKartik Talamadupula, Ibrahim Abdelaziz, Maria\nChang, Achille Fokoue, Bassem Makni, Nicholas\nMattei, and Michael Witbrock. 2019a. Improving\nnatural language inference using external knowledge\nin the science questions domain. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence, pages\n7208–7215. AAAI Press.\nXiaoyan Wang, Pavan Kapanipathi, Ryan Musa, Mo Yu,\nKartik Talamadupula, Ibrahim Abdelaziz, Maria\nChang, Achille Fokoue, Bassem Makni, Nicholas\nMattei, and Michael Witbrock. 2019b. Improving\nnatural language inference using external knowledge\nin the science questions domain. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence, pages\n7208–7215.\nDavid S. Wishart, Yannick D. Feunang, An Chi Guo,\nElvis J. Lo, Ana Marcu, Jason R. Grant, Tanvir Sajed,\nDaniel Johnson, Carin Li, Zinat Sayeeda, Nazanin\nAssempour, Ithayavani Iynkkaran, Yifeng Liu, Adam\nMaciejewski, Nicola Gale, Alex Wilson, Lucy Chin,\nRyan Cummings, Diana Le, Allison Pon, Craig Knox,\nand Michael Wilson. 2018. Drugbank 5.0: a major\nupdate to the drugbank database for 2018. Nucleic\nacids research, 46:D1074–D1082.\nMichihiro Yasunaga, Hongyu Ren, Antoine Bosselut,\nPercy Liang, and Jure Leskovec. 2021. Qa-gnn: Rea-\nsoning with language models and knowledge graphs\nfor question answering. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 535–546, Online. As-\nsociation for Computational Linguistics.\nXikun Zhang, Antoine Bosselut, Michihiro Yasunaga,\nHongyu Ren, Percy Liang, Christopher D. Manning,\nand Jure Leskovec. 2022. Greaselm: Graph reason-\ning enhanced language models for question answer-\ning. In International Conference on Learning Repre-\nsentations.\nWanjun Zhong, Yifan Gao, Ning Ding, Yujia Qin,\nZhiyuan Liu, Ming Zhou, Jiahai Wang, Jian Yin,\nand Nan Duan. 2022. ProQA: Structural prompt-\nbased pre-training for uniﬁed question answering. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 4230–4243, Seattle, United States. Association\nfor Computational Linguistics.\n5133",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8230863213539124
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.766863226890564
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5657882690429688
    },
    {
      "name": "Question answering",
      "score": 0.5588332414627075
    },
    {
      "name": "Graph",
      "score": 0.5523350238800049
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5482833385467529
    },
    {
      "name": "Language model",
      "score": 0.5055384635925293
    },
    {
      "name": "Representation (politics)",
      "score": 0.5040968656539917
    },
    {
      "name": "Knowledge representation and reasoning",
      "score": 0.4757363796234131
    },
    {
      "name": "Knowledge graph",
      "score": 0.46818816661834717
    },
    {
      "name": "Natural language processing",
      "score": 0.4371321201324463
    },
    {
      "name": "Context model",
      "score": 0.4211363196372986
    },
    {
      "name": "Machine learning",
      "score": 0.39822885394096375
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3147256374359131
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I40963666",
      "name": "Central China Normal University",
      "country": "CN"
    }
  ]
}