{
  "title": "A clinical benchmark of public self-supervised pathology foundation models",
  "url": "https://openalex.org/W4409488923",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2759907069",
      "name": "Gabriele Campanella",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2495701830",
      "name": "Shengjia Chen",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2565325584",
      "name": "Manbir Singh",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2186159954",
      "name": "Ruchika Verma",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2646273416",
      "name": "Silke Muehlstedt",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2170048748",
      "name": "Jennifer Zeng",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2801810120",
      "name": "Aryeh Stock",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A5114191000",
      "name": "Matt Croken",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2949246167",
      "name": "Brandon Veremis",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2088267611",
      "name": "Abdulkadir Elmas",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A5116822279",
      "name": "Ivan Shujski",
      "affiliations": [
        "University of Gothenburg",
        "Sahlgrenska University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2805972043",
      "name": "Noora Neittaanmäki",
      "affiliations": [
        "University of Gothenburg",
        "Sahlgrenska University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A4222098766",
      "name": "Kuan-lin Huang",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2111729684",
      "name": "Ricky Kwan",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2151003159",
      "name": "Jane Houldsworth",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2809360702",
      "name": "Adam J. Schoenfeld",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A3173432579",
      "name": "Chad Vanderbilt",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A2759907069",
      "name": "Gabriele Campanella",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2495701830",
      "name": "Shengjia Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2565325584",
      "name": "Manbir Singh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2186159954",
      "name": "Ruchika Verma",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2646273416",
      "name": "Silke Muehlstedt",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2170048748",
      "name": "Jennifer Zeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2801810120",
      "name": "Aryeh Stock",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5114191000",
      "name": "Matt Croken",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2949246167",
      "name": "Brandon Veremis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2088267611",
      "name": "Abdulkadir Elmas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5116822279",
      "name": "Ivan Shujski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2805972043",
      "name": "Noora Neittaanmäki",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222098766",
      "name": "Kuan-lin Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111729684",
      "name": "Ricky Kwan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2151003159",
      "name": "Jane Houldsworth",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2809360702",
      "name": "Adam J. Schoenfeld",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3173432579",
      "name": "Chad Vanderbilt",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2919115771",
    "https://openalex.org/W3175201698",
    "https://openalex.org/W3203328008",
    "https://openalex.org/W4286587621",
    "https://openalex.org/W2594770219",
    "https://openalex.org/W2760946358",
    "https://openalex.org/W4401809436",
    "https://openalex.org/W4283816523",
    "https://openalex.org/W2761668583",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W4403713395",
    "https://openalex.org/W3018295606",
    "https://openalex.org/W3135547872",
    "https://openalex.org/W3211647829",
    "https://openalex.org/W2804905867",
    "https://openalex.org/W2964324957",
    "https://openalex.org/W4283210261",
    "https://openalex.org/W4295937529",
    "https://openalex.org/W4288809219",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2158485828",
    "https://openalex.org/W4385285835",
    "https://openalex.org/W4392947521",
    "https://openalex.org/W4366208220",
    "https://openalex.org/W4400889241",
    "https://openalex.org/W4387595925",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W6803870738",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W4390723185",
    "https://openalex.org/W4398201291",
    "https://openalex.org/W4318424231",
    "https://openalex.org/W6854054124",
    "https://openalex.org/W2180481128",
    "https://openalex.org/W7008814651",
    "https://openalex.org/W4401307720",
    "https://openalex.org/W4386076317",
    "https://openalex.org/W4400611172",
    "https://openalex.org/W4409488923",
    "https://openalex.org/W4221167446",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W4402706047",
    "https://openalex.org/W4205925879",
    "https://openalex.org/W4400601311",
    "https://openalex.org/W2614654783",
    "https://openalex.org/W2613362156",
    "https://openalex.org/W6893164572"
  ],
  "abstract": "The use of self-supervised learning to train pathology foundation models has increased substantially in the past few years. Notably, several models trained on large quantities of clinical data have been made publicly available in recent months. This will significantly enhance scientific research in computational pathology and help bridge the gap between research and clinical deployment. With the increase in availability of public foundation models of different sizes, trained using different algorithms on different datasets, it becomes important to establish a benchmark to compare the performance of such models on a variety of clinically relevant tasks spanning multiple organs and diseases. In this work, we present a collection of pathology datasets comprising clinical slides associated with clinically relevant endpoints including cancer diagnoses and a variety of biomarkers generated during standard hospital operation from three medical centers. We leverage these datasets to systematically assess the performance of public pathology foundation models and provide insights into best practices for training foundation models and selecting appropriate pretrained models. To enable the community to evaluate their models on our clinical datasets, we make available an automated benchmarking pipeline for external use.",
  "full_text": "Article https://doi.org/10.1038/s41467-025-58796-1\nA clinical benchmark of public self-\nsupervised pathology foundation models\nGabriele Campanella 1,2 , Shengjia Chen 1,2, Manbir Singh1,2,\nRuchika Verma1,2,S i l k eM u e h l s t e d t1,2, Jennifer Zeng3,A r y e hS t o c k3,\nMatt Croken3, Brandon Veremis3, Abdulkadir Elmas 4,I v a nS h u j s k i5,6,\nNoora Neittaanmäki5,6, Kuan-lin Huang 4,R i c k yK w a n3, Jane Houldsworth 3,\nAdam J. Schoenfeld 7 &C h a dV a n d e r b i l t8\nThe use of self-supervised learning to train pathology foundation models has\nincreased substantially in the past few years. Notably, several models trained\non large quantities of clinical data have been made publicly available in recent\nmonths. This will signiﬁcantly enhance scientiﬁc research in computational\npathology and help bridge the gap between research and clinical deployment.\nWith the increase in availability of public foundation models of different sizes,\ntrained using different algorithms on different datasets, it becomes important\nto establish a benchmark to comparethe performance of such models on a\nvariety of clinically relevant tasks spanning multiple organs and diseases. In\nthis work, we present a collection of pathology datasets comprising clinical\nslides associated with clinically relevant endpoints including cancer diagnoses\nand a variety of biomarkers generatedduring standard hospital operation\nfrom three medical centers. We leverage these datasets to systematically\nassess the performance of public pathology foundation models and provide\ninsights into best practices for training foundation models and selecting\nappropriate pretrained models. To enable the community to evaluate their\nmodels on our clinical datasets, we make available an automated bench-\nmarking pipeline for external use.\nArtiﬁcial Intelligence (AI) is revolutionizing the medicalﬁeld. The\nintroduction of deep learning1 has greatly accelerated the develop-\nment of predictive models for high-dimensional data modalities such\nas images and text that are not readily amenable to classical machine\nlearning algorithms. Convolutional neural networks (CNNs) and\nvision transformers\n2 (ViTs) have been used to solve numerous pro-\nblems using supervised learning and have enabled the training of\npredictive models for a variety of tasks with high performance\n3– 9.\nRecently, the development of self-supervised learning (SSL) algo-\nrithms has marked a paradigm shift by enabling the training of deep\nneural networks on very large unlabeled datasets, yielding results on\npar with supervised learning strategies\n10,11. Large neural networks\ntrained this way can be described as foundation models that can be\nused for a wide variety of downstream tasks with little to noﬁne-\ntuning. Despite the great successes in the computer vision and nat-\nural languageﬁelds, SSL algorithms and foundation models are still\nReceived: 8 July 2024\nAccepted: 2 April 2025\nCheck for updates\n1Windreich Department of AI and Human Health, Icahn School of Medicine at Mount Sinai, New York 10029 NY, USA.2Hasso Plattner Institute at Mount Sinai,\nIcahn School of Medicine at Mount Sinai, New York 10029 NY, USA.3Department of Pathology, Icahn School of Medicine at Mount Sinai, New York 10029 NY,\nUSA. 4Department of Genetics and Genomics, Icahn School of Medicine at Mount Sinai, New York 10029 NY, USA.5Department of Clinical Pathology,\nSahlgrenska University Hospital, Gothenburg, Sweden.6Department of Laboratory Medicine, University of Gothenburg, Gothenburg, Sweden.7Department\nof Medicine, Memorial Sloan Kettering Cancer Center, New York 10065 NY, USA.8Department of Pathology, Memorial Sloan Kettering Cancer Center, New\nYork 10065 NY, USA. e-mail: gabriele.campanella@mssm.edu; vanderbc@mskcc.org\nNature Communications|         (2025) 16:3640 1\n1234567890():,;\n1234567890():,;\nin their infancy in the medical domain. One of the main reasons is the\nlack of medical datasets and the necessary computing infrastructure,\nwhich makes large-scale SSL experiments only possible at large well-\nfunded institutions.\nIn pathology, the lack of data is even more acute due to the still\nlow adoption of digital pathology. Additionally, digital whole slide\nimages (WSI) are orders of magnitude larger than other image mod-\nalities, with resolutions of tens to hundreds of thousands of pixels in\neach dimension. This poses challenges in terms of the methods used to\nanalyze the images and the hardware requirements to effectively\nperform experiments. A common strategy to analyze these images is to\ndivide the slide into small tiles or patches and encode them using a\ndeep neural network, expressing the slide as a list of feature vectors\nand thus reducing the dimensionality of the slide by multiple orders of\nmagnitude\n2,12. In a second step, the feature vectors are aggregated\nusing a neural network to obtain a slide-level representation12,13.T h e\nﬁrst step is by far the most computationally expensive, while the sec-\nond step requires much fewer resources. This is why most studies in\ncomputational pathology rely on already existing pretrained encoders,\nusually trained on natural images and not WSIs\n12,14– 17.T h e r ei san e e df o r\nstrategies that enable training of encoders directly on pathology\nimages, and SSL lends itself well for this task as it does not require any\nsort of labels and thus enables the training of pathology foundation\nmodels on large unannotated datasets. SSL for pathology has recently\nreceived lots of attention, and there are many academic and non-\nacademic efforts to build a general-purpose pathology foundation\nmodel (Table1).\nWang et al.\n18 proposed SRCL, an SSL method based on MoCo\nv310, along with CTransPath, a model architecture that combines\nconvolutional layers with the Swin Transformer 19 model. They\ntrained their model on 15.6 million tiles from 32,220 slides from the\nTCGA\n20 and PAIP datasets spanning 25 anatomic sites and over 32\ncancer subtypes. The downstream performance was assessed on\npatch retrieval, supervised patch classiﬁcation, weakly supervised\nWSI classiﬁcation, mitosis detection, and colorectal adenocarcinoma\ngland segmentation. Methodological advances include the intro-\nduction of a strategy to sample positive examples for the contrastive\nlearning approach, and the hybrid convolutional-transformer model\narchitecture.\nFiliot et al.\n21 analyzed the performance of iBOT22,a nS S Lf r a m e -\nwork that combines masked image modeling and contrastive learning,\non histology data. They trained several ViT models on a dataset con-\nsisting of up to 43.3 million tiles from 6093 TCGA slides of 13 anatomic\nsites. They assessed the performance of learned features on 17\ndownstream tasks across seven cancer indications, including tile-level\nand slide-level tasks for subtype, genomic alteration, and overall sur-\nvival prediction. Ultimately, they publicly released Phikon\n21,aV i T -\nbase model.\nChen et al.23 introduced UNI, a ViT-large model trained on\n100,000 proprietary slides using the DINOv224 SSL algorithm. The\npretraining dataset they used included 100 million tiles from 20 major\ntissue types. They evaluated the downstream performance across 33\ntasks, which included tile-level tasks such as classiﬁcation, segmenta-\ntion, retrieval, as well as slide-level classiﬁcation tasks.\nVorontsov et al.\n25 introduced Virchow, a ViT-huge model trained\non 2 billion tiles from almost 1.5 million proprietary slides with\nDINOv2\n24. Slides were included from 17 tissue types and the perfor-\nmance on downstream tasks was evaluated using tile-level and slide-\nlevel benchmarks, encompassing tissue classiﬁcation and biomarker\nprediction.\nCampanella et al.\n26 compared the performance of masked\nautoencoders27 (MAE) and DINO28 using over 3 billion tiles sourced\nfrom more than 423,000 pathology slides. The models were evaluated\non six clinical tasks spanning three anatomical sites and two institu-\ntions. Their results showed the superiority of the DINO algorithm for\npathology foundation model pretraining.\nDippel et al.\n29 introduced RudolfV, a model that integrates\npathologist expertise, semi-automated data curation, and a diverse\ndataset from over 15 laboratories. Their dataset comprised\n134,000 slides from 34,000 cases, representing a broad spectrum of\nhistological samples with various ﬁxation, staining, and scanning\nprotocols from laboratories across the EU and US. Additionally,\nsemantically similar slides and tissue patches were grouped to opti-\nmize data sampling for training, and stain-speciﬁc data augmentation\nwas applied.\nXu et al.\n30 introduced Prov-GigaPath, that was created by tile-level\npretraining using DINOv224, followed by slide-level pretraining using a\nmasked autoencoder27 and LongNet31. This model was pretrained on\n1.3 billion tiles derived from 171,189 WSIs comprising H&E-stained and\nimmunohistochemistry (IHC) slides from Providence Health and Ser-\nvices. These WSIs originated from over 30,000 patients encompassing\n31 tissue types. Prov-GigaPath was evaluated on 17 genomic prediction\ntasks and 9 cancer subtyping tasks using both Providence and\nTCGA\n32 data.\nZimmermann et al.33 introduced two models, Virchow2 (ViT-\nhuge) and Virchow2G (ViT-giant), trained on 1.7 billion and 1.9 billion\ntiles, respectively, from 3.1 million histopathology whole slide images\nwith DINOv2. Virchow2 examines the impact of increased data scale\nand diversity across multiple magniﬁcations, while Virchow2G focu-\nses on scaling model size. Slides from 225,401 patients, with\nTable 1 | A summary of recently published pathology foundation models\nParam. Algorithm Training Data Tiles Slides Training\nModel (M) Source (M) (K) Resolution\nCTransPath18 28 SRCL TCGA, PAIP 16 32 20x\nPhikon21 86 iBOT TCGA 43 6 20x\nUNI23 303 DINOv2 MGB 100 100 20x\nVirchow25 631 DINOv2 MSKCC 2000 1488 20x\nRef. 26 22 DINO MSHS 1600 423 20x\nRef. 26 303 MAE MSHS 3200 423 20x\nRudolf-V29 304 DINOv2 Multicenter 1200 134 20x\nProv-GigaPath30 1135 DINOv2 PHS 1300 171 20x\nVirchow233 631 DINOv2 MSKCC 1700 3100 5,10,20,40 ×\nH-optimus-034 1135 DINOv2 Proprietary >100 >500 20x\nPhikon-v235 307 DINOv2 Multicenter 456 58 20x\nMGB Mass General Brigham,MSKCC Memorial Sloan Kettering Cancer Center,MSHS Mount Sinai Health System,PHS Providence Health and Services.\nArticle https://doi.org/10.1038/s41467-025-58796-1\nNature Communications|         (2025) 16:3640 2\nmagniﬁcations of 5x, 10x, 20x, and 40x, were included from both\nH&E and IHC stains across nearly 200 tissue types. Their approach\nachieved state-of-the-art performance on 12 tile-level tasks, surpass-\ning the top-performing competing models.\nSaillard et al.\n34 introduced H-optimus-0, a ViT-giant model trained\non hundreds of millions of tiles derived from over 500,000 proprie-\ntary H&E stained whole-slide histology images. The dataset included\nslides from a wide range of tissue types, and the model’s performance\nwas assessed on tile-level and slide-level benchmarks, covering tasks\nsuch as tissue classiﬁcation, mutation prediction, and survival analysis.\nPhikon-v2\n35, a self-supervised vision transformer trained using\nDINOv224, demonstrates superior performance compared to its pre-\ndecessor, Phikon-v121, and achieves results comparable to other lead-\ning histopathology foundation models. The model was pretrained on a\ndiverse dataset of 460 million pathology tiles derived from over 100\npublicly available cohorts, encompassing more than 50,000 histo-\npathology slides across 30 cancer types. Benchmark evaluations,\ncovering eight slide-level tasks with external validation cohorts, high-\nlight its robust performance and generalizability.\nIt is becoming abundantly clear that using SSL to train image\nencoders on unlabeled pathology data is superior to relying on\nmodels pretrained on other domains such as natural images\n26,36,37.\nWhile SSL-trained pathology models hold immense potential, there\nare still challenges that need to be overcome before pathology\nfoundation models can be used reliably in clinical workﬂows. One\nconsideration is that datasets used to train pathology models are still\nrelatively small compared to other domains, in particular natural\nimages, especially when considering the number of slides or cases.\nSince each pathology slide can contain tens of thousands of tiles, it is\npossible to generate large number of tiles from a small number of\nslides. Thus, it is essential to consider not only the number of tiles or\nslides used, but also other metrics of tissue heterogeneity such as\nanatomic sites and organ inclusion. Given the evidence from the\nnatural language and vision domains that larger datasets and higher\ncapacity models will produce better performance especially in the\nSSL setting\n38– 40, training on larger pathology datasets should be a\npriority. Recent works show progress in this respect as the digitiza-\ntion of pathology data becomes more prevalent25,30,33. Most impor-\ntantly, the downstream performance of SSL models for pathology\nshould be assessed on clinically derived data, preferably from mul-\ntiple institutions, for clinically relevant tasks such as diagnostic\nassessment, biomarker prediction, and outcome prediction. This\neffect is compounded by the use of curated public datasets which\nmay not be suited for assessing generalization to real world data. It\nshould be noted that progress in this regard is being made and a\ntrend towards the use of more clinical data in recent publications can\nbe observed. Yet, there is still a lack of a systematic comparison of\ncurrent models on a wide variety of clinical tasks.\nIn the present work we overcome this limitation by introducing a\nclinical benchmark dataset which is used to systematically compare\npublic pathology foundation models. In contrast to previous\nefforts\n36,41, the dataset consists of clinical data generated during\nstandard hospital operations from three health systems. It includes\ntwo broad task types (disease detection and biomarker prediction),\nand a wide range of disease indications and anatomic sites. Con-\nsidering the rate of progress in computational pathology, as new\nfoundation models are published and additional datasets are added\nto our benchmarks, we will regularly update ourﬁndings to provide\nthe community with a comprehensive view of the state of foundation\nmodels in computational pathology. The live benchmark can be\nfound in the ofﬁcial GitHub repository. In addition, we provide an\nautomated benchmarking mechanism for external users who wish to\ntake advantage of our clinical cohorts. Instruction are provided on\nGitHub.\nResults\nDisease Detection Tasks\nFor disease detection, all models show consistent performance across\nall tasks with AUCs above 0.9 in all cases (Fig.1). The ImageNet pre-\ntrained encoder is consistently under-performing the pathology\ntrained encoders. This behavior is statistically signiﬁcant across tasks\nwith the exception of the Thyroid cohort (see Supplementary Fig. 7).\nAmong the pathology trained encoders, CTransPath consistently\nshows inferior performance. This behavior is statistically signiﬁcant\nacross tasks with the exception of the Colorectal, Oral, and Thyroid\ncohorts (see Supplementary Fig. 7). CTransPath was trained on a\nrelatively small dataset and used a contrastive learning algorithm,\nwhich may explain the difference in performance. The other founda-\ntion models tested were trained with either iBOT, DINO, or DINOv2. In\ngeneral, they all achieve similar performances with largely non statis-\ntically signiﬁcant differences despite the diversity in pretraining\ndatasets and model architectures (see Supplementary Fig. 7). Ranking\nthe models based on the average AUC across tasks (Supplementary\nFig. 9), the top 3 ranked models are H-optimus-0, Prov-GigaPath, and\nSP85M. Overall, for detection tasks, all the DINO and DINOv2 trained\nmodels achieve comparable performance and the choice of model\nmay depend on other considerations, such as inference cost.\nComputational Biomarker Prediction Tasks\nBiomarker prediction tasks are more challenging than disease detec-\ntion tasks since it is generally unknown whether measurable mor-\nphological changes in H&E stained slides even exist for the biomarker\nof interest. For some biomarkers, prediction from H&E may not be\nfeasible. As expected, the biomarker prediction tasks show a higher\ndegree of variability in performance than the detection tasks (Fig.2).\nThe gap in performance of the ImageNet pretrained model is more\nevident here than in the detection tasks. Pairwise comparisons with\nTRes50 are statistically signiﬁcant except for the NSCLC IO task (see\nSupplementary Fig. 8). As before, CTransPath tends to perform worse\nthan the DINO and DINOv2 models. This difference is for the most part\nstatistically signiﬁcant with the exception of the NSCL IO, breast HRD,\nand melanoma BRAF tasks (see Supplementary Fig. 8). On the other\nend of the spectrum, H-optimus-0 and Prov-GigaPath tend to be sig-\nniﬁcantly better than other models in the majority of tasks (see Sup-\nplementary Fig. 8). The main exceptions include the Breast HRD,\nMelanoma, and NSCLC IO tasks. For the other models, it is more dif-\nﬁcult to make general statements. Ranking the models based on the\naverage AUC across tasks (Supplementary Fig. 10), H-optimus-0, Prov-\nGigaPath, and UNI are the top 3 ranked.\nStratifying the analysis by biomarker panels, we can make some\nfurther observations. For the breast cancer IHC/FISH biomarkers, the\nobservations made before are largely accurate with H-optimus-0, Prov-\nGigaPath, and UNI performing-generally better. Here Virchow and\nVirchow2 also compare positively to some of the other models. For the\nsomatic mutation panel in melanoma, differences in performance\nbetween the various models are less obvious. For the somatic NGS\npanel in LUAD, again H-optimus-0, Prov-GigaPath, and UNI tend to be\nsigniﬁcantly better than the other models. Interestingly, we noticed\nthat the prevalence of lung tissue in the pretraining cohort explain in\npart the lung biomarker results. For UNI, lung is the second most\ncommon tissue in their dataset, with around 10% of the slides or about\nten thousand WSIs. For Prov-GigaPath, lung is the most common tis-\nsue, comprising over 45% of the slides, or about 77 thousand WSIs. This\npoints to the hypothesis that while for detection tasks, dataset com-\nposition seems not an important factor, it may play a signiﬁcant role\nfor biomarker prediction.\nFinally, for the task of predicting ICI response in NSCLC, all\nmodels obtained equally poor results with AUCs barely above chance.\nUNI, with and average AUC of 0.6 performed performed signiﬁcantly\nArticle https://doi.org/10.1038/s41467-025-58796-1\nNature Communications|         (2025) 16:3640 3\nbetter than the other models. ICI response prediction from H&E slides\nis a challenging task, yet there is evidence that descriptors of local\ncellular networks\n42, that better model the tumor microenvironment\n(TME) can achieve AUCs of around 0.7, on-par with PD-L1 IHC, the\ncurrent clinical gold standard. It is reasonable to hypothesize that SSL-\ntrained foundation models should be able to capture local cellular\ninformation and reach similar performance. One potential explanation\nis that the pretraining data may be skewed in terms of cancer presence,\ncancer subtype, and cancer stage. Given that foundation models are\ntrained on large data collections with minimal to no data curation, the\nmagnitude of these biases with this level of detail is generally not\nmeasurable. Yet, this result suggests that the composition of the pre-\ntraining dataset may be crucial, especially for challenging response\nprediction tasks.\nFoundation Model Size\nOne important aspect of foundation models is their representational\ncapacity which can be roughly estimated by the model’s parameter\ncount. Here we investigate how model size correlates with down-\nstream performance to assess whether scaling laws observed in other\ndomains, such as natural language processing are occurring for\npathology data. For this analysis we excluded tRes50 and CTransPath\nto restrict the analysis to vision transformers trained with iBOT,\nDINO, or DINOv2 (UNI, Virchow, Prov-GigaPath, SP22M, SP85M,\nVirchow2, h-optimus-0, Phikon-v2, Phikon). Model sizes range from\n22 million (SP22M) to 1.1 billion (Prov-GigaPath) parameters (see\nTable 1).\nFigure 3 shows how the downstream performance of detection\nand biomarker prediction tasks correlate with encoder model sizes.\nFor detection tasks, our results suggest a tendency of downstream\nperformance scaling with model size, but this effect is rather minor\n(Pearson statistic: 0.055,p value: 2.59e-2). As we showed previously, on\naverage a 22 million parameter model is comparable to a 1.1 billion\nparameter model for these tasks. For biomarker prediction, an overall\ntendency of higher performance with larger models is observed to a\nmore signiﬁcant extent compared to the detection tasks (Pearson\nstatistic: 0.091,p value: 9.52e-6). While overall biomarkers for bio-\nmarkers the model size correlates with performance, we observed that\nthis effect may be task-dependent. Focusing on several breast bio-\nmarkers, there is no beneﬁt from larger models, whereas for the NGS\nlung tasks there seems to be a larger beneﬁt. As noted earlier, this may\nbe due to the pretraining dataset composition and not to the larger\nmodel capacity.\nPretraining Dataset Size\nNext, we investigated the effect of pretraining dataset size on the\ndownstream performance in terms of number of slides and number\nof tiles. The models included in the analysis were trained on datasets\nwith a wide range of number of slides, from six thousand (Phikon) to\nover three million (Virchow2). Focusing on slides used for pretrain-\ning, we observed no evidence that larger pretraining datasets are\ncorrelated with better performance. This was true for both detection\nand biomarker tasks (see Supplementary Fig. 2; detection tasks:\nr = 0.008,p val = 7.58e-01; biomarker tasks: r = -0.033,p val = 1.07e-\n01). Similarly, the number of tiles used to pretrain the foundation\nmodels varied widely, from 43 million (Phikon) to 1.7 billion\n(Virchow2). For detection tasks we observed a slight trend of\nincreased performance, while for biomarker tasks we observed a\nslight trend of decreased performance (see Supplementary Fig. 3;\ndetection tasks: r = 0.050,p value = 7.79e-02; biomarker tasks: r =\n−0.048, p value = 4.03e-02). Overall, the dataset size does not cor-\nrelate strongly with downstream performance.\nFig. 1 | Benchmarking Results: Detection Tasks.Each box plots summarizes the distribution of validation performance across 20 MCCV splits (N = 20). Boxes show the\nquartiles of each distribution, while the whiskers extend 1.5 times the interquartile range. Source data are provided as a Source Dataﬁle.\nArticle https://doi.org/10.1038/s41467-025-58796-1\nNature Communications|         (2025) 16:3640 4\nComputational Resources\nPreviously, we analyzed model size and dataset size independently. To\nassess their joint effect, we studied the extent to which the overall\npretraining computational resources explain downstream performance.\nComputational resources needed to train a model depend on the model\nsize, dataset size, and also the pretraining algorithm. For example,\nSP22M with 22 million parameters was trained with DINO using full\nprecision on 40GB GPUs with a batch size per GPU of 90 tiles. In\nFig. 2 | Benchmarking Results: Biomarker Prediction Tasks.Each box plots summarizes the distribution of validation performance across 20 MCCV splits (N = 20). Boxes\nshow the quartiles of each distribution, while the whiskers extend 1.5 times the interquartile range. Source data are provided as a Source Dataﬁle.\nArticle https://doi.org/10.1038/s41467-025-58796-1\nNature Communications|         (2025) 16:3640 5\ncomparison, Prov-GigaPath with 1.1 billion parameters was trained with\nDINOv2 using half-precision on 80GB GPUs with a batch size of 12 tiles\nper GPU. To harmonize the various training runs, we measured overall\ncomputational resources using GPU-hours normalized to a hypothetical\n80GB GPU card. We assume that, for models trained on a 40GB card, the\ncomputation time would be halved by using an 80GB card. GPU usage\nand training times were obtained from each respective paper or model\ncards in public repositories and are summarized in Supplementary\nTable 6. Foundation models included in this analysis are: SP22M, SP85M,\nUNI, Phikon, Phikon-v2. Others had to be excluded due to lack of data.\nFigure 4 shows how the downstream performance of detection\nand biomarker prediction tasks correlate with computational resour-\nces used for training. For detection tasks, our results show no evidence\nof improved performance associated with higher computational costs\n(Pearson statistic: 0.055,p value: 1.01e-1). The same conclusion can be\nmade for biomarker prediction tasks where the linear tendency even\nhad a slight negative slope (Pearson statistic: −0.074,\np value: 7.51e-3). It is important to note the lack of data for Prov-\nGigaPath and H-optimus-0 in this analysis. Based on these results, we\nhighlight how UNI, while trained with a comparatively modest\nresource budget, achieves competitive performance, especially in our\nbiomarker tasks. While this could be explained again by the prevalence\nof lung tasks, it may also point to the importance of pretraining dataset\ncomposition.\nPretraining Dataset Composition\nWe have previously hinted that the composition of the pretraining\ndataset may be a crucial aspect for explaining downstream perfor-\nmance. We explore this hypothesis more in detail by correlating the\nperformance of tissue-speciﬁc tasks with the percentage of the pre-\ntraining dataset devoted to that tissue in terms of slides. To perform\nthis analysis, we collected tissue percentages for each model from\ntheir respective papers when available. The data collected is presented\nin Supplementary Table 7. The foundation models included in this\nanalysis are: SP22M, SP85M, UNI, Prov-GigaPath, and Virchow. To\ninvestigate the effect of the tissue prevalence, we combined tasks by\ntissue and analyzed the AUC performance as a function of tissue per-\ncentage. We focused on four tissue/organs that had the most complete\ndata: lung, breast, colon/rectum, and prostate.\nFor lung, our benchmark contains seven biomarker tasks. Tissue\nprevalence ranged from 0.36% (SP22M, SP85M) to 45.29% (Prov-Giga-\nPath). Correlating the performance on lung tasks with tissue prevalence\nFig. 3 | Scaling Laws: downstream performance vs foundation model size.\nScatter plots summarize the validation performance across 20 MCCV runs for each\ntask (N = 20). The error bars show the 95% conﬁdence interval calculated via\nbootstrapping with 1000 iterations. Linear tendency line and 95% bootstrapped\nconﬁdence interval is shown in red. The line summarizes the performance across all\ntasks at once (detection tasksN = 1620; biomarker tasksN = 2340). Left: detection\ntasks, Pearson correlation coefﬁcient: 0.055, two-sidedp-value: 2.59e-2. Right:\nbiomarker tasks, Pearson correlation coefﬁcient: 0.091, two-sidedp value: 9.52e-6.\nP values not adjusted for multiple hypothesis testing. Source data are provided as a\nSource Dataﬁle.\nFig. 4 | Scaling Laws: downstream performance vs computational resources\nused for pretraining the foundation models.Scatter plots summarize the vali-\ndation performance across 20 MCCV runs for each task (N = 20). The error bars\nshow the 95% conﬁdence interval calculated via bootstrapping with 1000 itera-\ntions. Linear tendency line and 95% bootstrapped conﬁdence interval is shown in\nred. The line summarizes the performance across all tasks at once (detection tasks\nN= 900; biomarker tasksN = 1300). Left: detection tasks, Pearson correlation\ncoefﬁcient: 0.055, two-sidedp value: 1.01e-1. Right: biomarker tasks, Pearson cor-\nrelation coefﬁcient: -0.074, two-sidedp value: 7.51e-3.P values not adjusted for\nmultiple hypothesis testing. Source data are provided as a Source Dataﬁle.\nArticle https://doi.org/10.1038/s41467-025-58796-1\nNature Communications|         (2025) 16:3640 6\nyielded the strongest effect with a Pearson statistic of 0.229 (p-value:\n9.50e-10, Supplementary Fig. 4a). For breast, we have two detection\ntasks and four biomarker tasks. Tissue prevalence ranged from 2.76%\n(Prov-GigaPath) to 24.90% (Virchow). We observed no signiﬁcant cor-\nr e l a t i o nb e t w e e np e r f o r m a n c ea n dt i s s u ep r e v a l e n c ef o rt h eb r e a s tt a s k s\nconsidered with a Pearson statistic of -0.044 (p value: 2.83e-01, Sup-\nplementary Fig. 4b). For colon and rectum, we have two detection tasks.\nTissue prevalence ranged from 3.20%( V i r c h o w )t o3 0 . 43% (Prov-Giga-\nPath). We didn’to b s e r v eas i g n iﬁcant correlation between performance\na n dt i s s u ep r e v a l e n c ef o rt h eb r e a s tt a s k sc o n s i d e r e dw i t haP e a r s o n\nstatistic of 0.122 (p value: 8.59e-2, Supplementary Fig. 4c). Finally, for\nprostate, only one task was considered. Tissue prevalence ranged from\n0% (UNI) to 10.61% (SP22M, SP85M). No signiﬁcant correlation was\no b s e r v e dw i t haP e a r s o ns t a t i s t i co f- 0 . 0 6 8(p-value: 4.99e-1, Supple-\nmentary Fig. 4d). Overall, the importance of the tissue prevalence seems\nto be tissue-speciﬁc. While there was an indication of positive correla-\ntion for the lung tasks, the same was not the case for the other organs\ntested.\nFoundation Model Inference\nCompared to pretraining, model inference covers a fraction of the\ncomputational expense, yet it is an important consideration for model\ndeployment. On comparable hardware, inference largely depends on\nmodel architecture. To assess the inference performance of founda-\ntion models, we measured the minimal GPU memory required and the\nmaximum throughput in tiles per second (TPS). These analyses were\nconducted using synthetic data. Each condition was repeated 20 times\nto assess variability.\nFirst, we considered the minimal GPU memory required in giga-\nbytes (GB) as the memory necessary to run a forward pass through a\nmodel with a single image (batch size of 1). Supplementary Fig. 5a\nshows the memory requirements for each foundation model we con-\nsidered. Requirements range from 0.127GB for SP22M, to 4.643GB for\nH-optimus-0 (see Supplementary Table 8). We related the memory\nrequirements with the average performance for detection and bio-\nmarker tasks for each model. For detection tasks (Supplementary\nFig. 5b), we identiﬁed SP85M as the best trade-off between memory\nand performance with an average AUC of 0.978 across all detection\ntasks and a minimal memory requirement of 0.387GB. It compares\nfavorably against the best-performing H-optimus-0 which requires\n4643GB for a 0.2% increase in performance on average over SP85M.\nFor biomarker tasks (Supplementary Fig. 5c), we identiﬁed UNI as\noffering the best trade-off with its average AUC of 0.773 across all\nbiomarker tasks with a 1.259GB memory requirement. The best-\nperforming H-optimus-0 required 4.643GB to achieve an average AUC\nof 0.785, a 1.3% increase in performance over UNI.\nWe also analyzed the inference performance in terms of maximal\nthroughput. For this, we considered the average number of tiles that\ncan be processed by a foundation model every second assuming no\ndata loading bottleneck. We run this analysis on a single H100 80GB\nGPU. To maximize throughput, it is important to utilize the GPU close\nto its full memory capacity. To this end, weﬁrst identiﬁed for each\nfoundation model the largest batch size (that is a multiple of 8) that\nallows to run a forward pass and used that for the analysis. Supple-\nmentary Fig. 6a shows the TPS throughput for all models considered.\nThese ranged from 4417.4 TPS by the truncated ResNet50 (followed by\n2569.4 TPS by SP22M) to 75.3 TPS by H-optimus-0 (see also Supple-\nmentary Table 9). We analyzed the relation between TPS and average\nAUC performance for detection and biomarker tasks. The results\nmostly coincided with our previous analysis of minimum memory\nrequirements. For detection tasks (see Supplementary Fig. 6b), SP85M\nseems to strike the best trade-off between TPS and AUC. Compared to\nthe best performer, it shows a 0.2% decrease in average performance\n(0.980 for H-optimus-0 vs 0.978 for SP85M) with a 14-fold increase in\nthroughput (75.3 TPS for H-optimus-0 vs 1063.6 TPS for SP85M). For\nbiomarker tasks (see Supplementary Fig. 6c), these results highlight\nUNI and Phikon. Compared to the best performer, UNI shows a 1.5%\ndrop in average AUC (0.773 for UNI vs 0.785 for H-optimus-0) with a\n4.8-fold increase in throughput. Instead, Phikon shows a 3.5% decrease\nin AUC compared to H-optimus-0 and a 2.1% decrease compared to\nUNI, but achieves a throughput 14.2 and 3 times higher, respectively.\nDiscussion\nSelf-supervised learning and foundation models have the potential to\nrevolutionize medical research. Training foundation models for com-\nputational pathology is showing a clear beneﬁto v e rt r a d i t i o n a l\nsupervised approaches in terms of performance and generalizability.\nNotably, recent models trained by both academic and private institu-\ntions are being released in public repositories, empowering research-\ners with the tools to develop the next generation of predictive models.\nWhile there is still much work to be done towards democratizing\ncomputational pathology-based decision support systems and making\nthem available to the research community, the emergence of foun-\ndation models likely will play a signiﬁcant role.\nAs more and more foundation models are trained, an independent\nbenchmark of clinically relevant tasks becomes essential for both\nresearchers training foundation models and looking to apply these\npretrained foundation models on downstream tasks. Training new\nfoundation models is expensive and it is important to learn from\nprevious efforts. A benchmark can provide insights for improving\npretraining and yield better models in the future. For downstream\nclinical applications, a benchmark can guide the decision to use one\nmodel over another, considering a variety of factors, from perfor-\nmance on various tasks to computational resource constraints. In this\nwork, we presented a benchmark of publicly available pathology\nfoundation models focusing on 22 clinically relevant slide-level tasks\nacross a variety of tissues and disease indications. Importantly, all the\ndata was generated during clinical operations without further cura-\ntion, representing the variability, both biological and technical, that\ncan be observed under real-world conditions. Further, most founda-\ntion models were trained on a combination of public and private\ndatasets without any overlap with the cohorts in this study. As an\nexception, we must note that since the Virchow and Virchow2 models\nwere trained on a large sample of slides from MSKCC, we can’te n s u r e\nthat there is no overlap between their pretraining cohort and the\nclinical tasks based on MSKCC data. In addition, the SP22M and SP85M\nmodels were trained on MSH data but we ensured no overlap with the\nclinical tasks.\nWe made a deliberate decision to not release the test data used for\nthese benchmarks. Efforts to scrape all publicly available data for\npretraining foundation models may lead to data contamination and\nnegatively impact the relevance of such benchmark results. Instead, we\nwill regularly update the benchmark results with the latest models as\nthey become publicly available. In addition, we are exposing an auto-\nmated pipeline allowing external users to benchmark their foundation\nmodels on our clinical cohorts. Instructions are provided in theGitHub\nrepository.\nIn summary, the ImageNet pretrained encoder, and CTransPath to\na lesser degree, consistently underperformed compared to newer\nmodels. For the disease detection tasks, in general all DINO and\nDINOv2-trained models performed comparably. H-optimus-0 and\nProv-GigaPath performed signiﬁcantly better in a few tasks, yet the\nimprovement is minimal. For biomarker tasks, there was a larger\nspread of performances, here H-optimus-0, Prov-GigaPath, and UNI\ncompared favorably to the other models in most tasks. We also found\nthat model size and pretraining dataset composition inﬂuenced per-\nformance, particularly for biomarker prediction, but not necessarily\nfor disease detection. Additionally, inference cost and computational\nefﬁciency varied, with models like SP85M and UNI offering a good\nbalance between performance and resource usage. The ﬁndings\nArticle https://doi.org/10.1038/s41467-025-58796-1\nNature Communications|         (2025) 16:3640 7\nunderscore the importance of pretraining dataset composition and\nmodel architecture in optimizing performance for speciﬁc tasks.\nFrom our analyses, we can make the following observations:\nStrong evidence does not yet exist supporting that scaling laws\nobserved in pretraining SSL models for natural language and images\nare applicable for tile encoders in pathology. Performance does not\nscale with model size and dataset size as clearly as in other domains\ngiven current training algorithms. Smaller models perform on par with\nmuch larger models on most tasks and are only marginally worse in\nothers, particularly for detection tasks. Similarly, the dataset size and\noverall computational expense does not appear to lead to signiﬁcantly\nbetter models. It is likely that dataset composition may be a crucial\naspect in the downstream performance, and more efforts in the\ncuration of the pretraining data is likely to be beneﬁcial. While general-\npurpose foundation models may be desirable, tissue-speciﬁcf o u n d a -\ntion models may be a viable alternative. Recent efforts to benchmark\npathology foundation models by Neidlinger et al.\n41 have come to\nsimilar conclusions. Moving forward, we expect small incremental\nperformance gains with current SSL algorithms. It is possible that we\nare saturating the capabilities of current SSL strategies in pathology\nand great leaps forward may not occur without innovations from the\nalgorithmic side or integration of SSL with other forms of supervision.\nFinally, we hypothesize that for challenging tasks like ICI response, tile-\nlevel encoders alone, especially with current tile sizes in the order of\n224 pixels, are not enough to fully describe all the relevant features and\nslide-level aggregators are likely to play an important role. More\nresearch in this direction will be needed as there is currently a lack of\nstrategies that are capable of fully leveraging the global topology of\nthe tissue in a slide\n43.\nIn this work we have focused on gathering a large set of clinically\nrelevant downstream tasks. We were able to include a variety of disease\nindications and task types. Yet, in the current version of the bench-\nmark, the technical variability (tissueﬁxation, staining, scanning, etc.)\nis limited to three institutions. Additionally, here we focused on\ncomparing the expressivity of tile-level features generated by pathol-\nogy foundation models. To that end we leveraged GMA aggregator\nwith a linear classiﬁer. We considered the exploration of more complex\naggregators as out of scope and it has been addressed in other\nstudies\n43.\nThere are several aspects of pretraining pathology foundation\nmodels that we could not address at this time due to lack of evidence.\nThe majority of foundation models have been trained at 20x magniﬁ-\ncation as it allows to use the largest possible cohort of data. One\nquestion is whether a higher resolution may be beneﬁcial especially for\ntasks where cellular features may be important. Some works have\nstarted to appear where several magniﬁcation levels are used jointly.\nWhether mixing magniﬁcations or training magniﬁcation speciﬁc\nmodels is of an advantage is largely unanswered. Similarly, a majority\nof efforts have focused on using H&E stained slides and ignoring IHC\nones. H&E slides are the basis of diagnostic work and are the fastest\nand cheapest to produce. Meanwhile, IHC slides provide supporting\ninformation but are slower and more costly to generate and are not\nroutine except for in very small subset of pathologists’ workﬂow.\nFurther, the technical complexity of IHC (e.g. differences in tissue\nprocessing, antigen extraction, unique antibodies for same protein\ntarget, unique automation platforms) make the inter-institutional\nvariability much greater than H&E. As a result, it has yet to be proven\nthat foundation models can be useful for IHC-based computa-\ntional pathology models. Furthermore, it might be possible that\nthe inclusion of IHC slides could be bene ﬁcial for H&E based\ntasks. Future work will be needed to address these questions.\nFinally, gathering large collections of pathology slides for pre-\ntraining is a daunting task within the constraints of single insti-\ntutions. While, collecting multi-institutional pretraining data\nmight improve the robustness and generalizability of foundation\nmodels, there are several important obstacles in the way, and it\nhas yet to be proven beneﬁcial or necessary.\nAs the development of foundation models in pathology pro-\ngresses, we will continue providing the community with a leader board\nof publicly available foundation models as well as external models\nautomatically benchmarked by external users. At the same time we will\nexpand the scope of the tasks included in terms of technical variability\nand prediction endpoints. We will include data from partner institu-\ntions, national and international. We will increase our focus on tasks\nrelated to biomarker prediction and treatment response as well as\nsurvival analysis tasks. Based on the accumulated evidence, we will\nupdate our recommendations on how to train foundation models in\ncomputational pathology. As theﬁeld develops, future work will also\nfocus on assessing the performance of slide-level foundation models.\nMethods\nThis research study was approved by the respective Institutional\nReview Boards at the Icahn School of Medicine at Mount Sinai (Pro-\ntocol 19-00951) and Memorial Sloan Kettering Cancer Center (Protocol\n18-013). Informed consent was waived as per the IRB protocols. Parti-\ncipants were not compensated. Sex and/or gender was not considered\nin the study design as cohorts were generated as random samples of\nthe patient population.\nDownstream Tasks\nTo assess the representation power of pathology foundation models,\nwe collected a series of clinical datasets spanning clinically relevant\ntasks from three institutions and scanned with a variety of scanners.\nFor analysis, data was extracted always at 20x magniﬁcation (0.5\nmicrons per pixel). The tasks are described below and summarized in\nTables 2 and 3 for the detection and the biomarker tasks respectively.\nAdditional demographic information for each cohort are provided in\nSupplementary Table 1, Supplementary Table 2, and Supplementary\nTable 3.\nDisease Detection\nMSHS Breast Cancer Detection Cohort. Breast cancer blocks and\nnormal breast blocks were obtained from the pathology LIS. A total of\n1998 slides were sampled, with 999 positive and 999 negative. The\npositive slides were selected from blocks that received the routine\nbiomarker panel for cancer cases (estrogen receptor ER, progesterone\nreceptor PR, HER2, and Ki67), while negative slides were selected from\nbreast cases that did not have an order for the routine panel. Addi-\ntionally, negative cases were selected if they were not a mastectomy\ncase, did not have a synoptic report associated with the case, and had\nno mention of cancer or carcinoma in the report.\nMSHS Oral Cancers Detection Cohort. Tumor (positive) and normal\n(negative) block information were extracted from structured synoptic\nTable 2 | Summary of detectiondownstream tasks currently\nincluded\nOrigin Disease Slides (Positive) Scanner\nMSHS Breast Cancer 1998 (999) Philips Ultrafast\nMSHS Oral Cancer 279 (145) Philips Ultrafast\nMSHS Bladder Cancer 448 (272) Philips Ultrafast\nMSHS Kidney Cancer 1000 (562) Philips Ultrafast\nMSHS Thyroid Cancer 710 (390) Philips Ultrafast\nMSHS DCIS 233 (135) Philips Ultrafast\nMSHS Prostate Cancer 1000 (547) Philips Ultrafast\nMSHS Colo-rectal Cancer 413 (257) Philips Ultrafast\nMSHS IBD 1448 (717) Philips Ultrafast\nMSHS Mount Sinai Health System.\nArticle https://doi.org/10.1038/s41467-025-58796-1\nNature Communications|         (2025) 16:3640 8\nreports obtained from the LIS. Synoptic reports for“Lip and Oral\nCavity” were included. The positive samples included a variety of\ncancer diagnoses: squamous cell carcinoma, adenoid cystic carci-\nnoma, mucoepidermoid carcinoma, and others.\nMSHS Bladder Cancers Detection Cohort. Tumor (positive) and\nnormal (negative) block information were extracted from structured\nsynoptic reports obtained from the LIS. Synoptic reports for“Cystect-\nomy, Anterior Exenteration” and “Transurethral Resection of Bladder\nTumor” were included. The positive samples included a variety of\ncancer diagnoses: urothelial carcinoma, small cell neuroendocrine car-\ncinoma, adenocarcinoma, squamous cell carcinoma, and others.\nMSHS Kidney Cancers Detection Cohort. Tumor (positive) and\nnormal (negative) block information were extracted from structured\nsynoptic reports obtained from the LIS. Synoptic reports for\n“Nephrectomy” were included. The positive samples included a variety\nof cancer diagnoses: clear cell renal cell carcinoma, chromophobe\nrenal cell carcinoma, papillary renal cell carcinoma, Xp11 translocation\nrenal cell carcinoma, clear cell sarcoma, and others.\nMSHS Thyroid Cancers Detection Cohort. Tumor (positive) and\nnormal (negative) block information were extracted from structured\nsynoptic reports obtained from the LIS. Synoptic reports for“Thyroid\nGland” were included. The positive samples included a variety of\ncancer diagnoses: papillary carcinoma, follicular carcinoma, Hurthle\ncell carcinoma, and others.\nMSHS Prostate Cancer Detection Cohort. Tumor (positive) and\nnormal (negative) block information was extracted from structured\nsynoptic reports obtained from the LIS. Synoptic reports for“Radical\nProstatectomy” and “Transurethral Prostatic Resection” were inclu-\nded. The positive samples included acinar and ductal prostate\nadenocarcinomas.\nMSHS Colo-rectal Cancers Detection Cohort. Tumor (positive) and\nnormal (negative) block information was extracted from structured\nsynoptic reports obtained from the LIS. Synoptic reports for“Resec-\ntion”, “Transanal Disk Excision of Rectal Neoplasms”, “Excisional\nBiopsy (Polypectomy)”,a n d“Neuroendocrine Tumor” were included.\nThe positive samples included a variety of cancer diagnoses: adeno-\ncarcinoma, signet-ring cell carcinoma, micropapillary carcinoma, and\nothers.\nMSHS DCIS Detection Cohort. Tumor (positive) and normal (nega-\ntive) block information was extracted from structured synoptic\nreports obtained from the LIS. The synoptic report“DCIS of the Breast”\nwas used for this cohort.\nMSHS IBD Detection Cohort. Normal mucosa samples were obtained\nfrom patients undergoing screening and routine surveillance lower\nendoscopy from 2018 to 2022. Inﬂammatory bowel disease (IBD)\ncases, includingﬁrst diagnoses and follow ups, were included. Active\nIBD samples were scored using the Mount Sinai histologic disease\ncriteria and found to have Histologic Activity Score (HAI) > = 1. A total\no f1 4 4 1s l i d e sw e r es a m p l e d ,7 1 7w i t ha c t i v ei nﬂammation and 724 with\nnormal mucosa.\nMSHS Breast Cancer IHC/FISH Panel. Breast cancer cases with\norders for Estrogen Receptor (ER), Progesterone (PR), and HER2 were\nqueried from the LIS. The test results were automatically extracted\nf r o mt h er e s p e c t i v ep a t h o l o g yr e p o r t .\nMSHS Breast Cancer ER Prediction Cohort\n. ER IHC orders were\nincluded and a total of 2000 slides were sampled, 1000 positive, 1000\nnegative.\nMSHS Breast Cancer PR Prediction Cohort. PR IHC orders were\nincluded and a total of 1986 slides were sampled, 953 positive, 1033\nnegative.\nMSHS Breast Cancer HER2 Prediction Cohort. Orders for HER2 IHC\nand FISH were included and a total of 2018 slides were sampled, 760\npositive, 1258 negative.\nMSHS Breast HRD Prediction Cohort. Mount Sinai BioMe is a whole-\nexome sequencing cohort of 30k individuals, where carriers of\npathogenic and protein-truncating variants affecting Homologous\nRepair Deﬁciency (HRD) genes (i.e., BRCA1, BRCA2, BRIP1, PALB2,\nRAD51, RAD51C, RAD51D, ATM, ATR, CHEK1,a n dCHEK2), where inclu-\nded as positives. A subset of the BioMe dataset of patients with avail-\nable breast pathology slides were included. Slides containing solely\nnormal breast tissue and slides with breast cancer were both included.\nSUH Melanoma Somatic Mutation Panel. A total of 283 melanoma\ncases were retrospectively collected from the archives of the Depart-\nments of Pathology at Sahlgrenska University Hospital (SUH), Södra\nÄlvsborg hospital and Norra Älvsborgs hospital in the Region Västra\nGötaland, Sweden.BRAFand NRASmutation status was veriﬁed by NGS\nor IHC. The dataset included both primary and metastatic samples.\nSUH BRAF Mutation Prediction in Melanoma Cohort.O ft h e\n283 samples, 113 had veriﬁed V600E/K BRAF mutations and were\nconsidered positive. The rest had no clinically relevantBRAFmutations\nand were considered negative.\nSUH NRAS Mutation Prediction in Melanoma Cohort.O ft h e\n283 samples, 94 were detected to have anNRAS mutation and were\nconsidered positive.\nMSHS EGFR mutation detection in Lung Adenocarcinoma.L u n g\nadenocarcinoma (LUAD) patients that underwent next generation\nsequencing (NGS) proﬁl i n gf o rt h e i rc a n c e rw e r ei d e n t iﬁed. A total of\n294 slides were obtained from MSHS’sc l i n i c a ls l i d ed a t a b a s e ,1 0 3\nTable 3 | Summary of downstream tasks currently included\nfor computational biomarker prediction\nOrigin Biomarker Specimen Slides\n(Positive)\nScanner\nMSHS IHC ER Breast\nCancer\n2000 (1000) Philips Ultrafast\nMSHS IHC PR Breast\nCancer\n1986 (953) Philips Ultrafast\nMSHS IHC/\nFISH HER2\nBreast\nCancer\n2018 (760) Philips Ultrafast\nMSHS BioMe HRD Breast 563 (188) Philips Ultrafast\nSUH NGS BRAF Skin 283 (113) Nanozoomer\nS210\nSUH NGS NRAS Skin 283 (94) Nanozoomer\nS210\nMSHS NGS EGFR LUAD 294 (103) Philips Ultrafast\nMSKCC NGS EGFR LUAD 1000 (307) Aperio AT2\nMSKCC NGS ALK LUAD 999 (144) Aperio AT2\nMSKCC NGS STK11 LUAD 998 (122) Aperio AT2\nMSKCC NGS KRAS LUAD 998 (325) Aperio AT2\nMSKCC NGS TP53 LUAD 998 (430) Aperio AT2\nMSKCC ICI Response NSCLC 454 (86) Aperio AT2\nMSHSMount Sinai Health System,SUH Sahlgrenska University Hospital,MSKCCMemorial Sloan\nKettering Cancer Center.\nArticle https://doi.org/10.1038/s41467-025-58796-1\nNature Communications|         (2025) 16:3640 9\npositive and 191 negative. Mutations outside of theEGFR kinase\ndomain (exons 18-24) are not considered oncogenic and are con-\nsidered negative in this analysis.\nMSKCC Lung Adenocarcinoma Somatic Mutation Panel.L U A D\npatients at Memorial Sloan Kettering Cancer Center with respective\nmolecular analysis from the MSK-IMPACT assay\n44,45 and corresponding\ndigitized slides where identiﬁed. MSK-IMPACT is an NGS assay that can\ndetect variants in up to 505 unique cancer genes, includingEGFR, TP53,\nKRAS, STK11,a n dALK.\nMSKCC EGFR Mutation Prediction in LUAD. LUAD samples with an\noncogenic EGFR mutation detected by MSK-IMPACT were included.\nMutations outside of theEGFR kinase domain (exons 18-24) are not\nconsidered oncogenic and are considered negative in this analysis.\nThis is a sample of the dataset described in Campanella et al.46 from\nwhich 1000 slides were sampled at random, 307 positive and 693\nnegative.\nMSKCC TP53 Mutation Prediction in LUAD.M S K - I M P A C Td e r i v e d\nTP53 mutational status. A total of 998 slides were sampled, 430 posi-\ntive and 568 negative.\nMSKCC KRAS Mutation Prediction in LUAD. MSK-IMPACT derived\nKRAS mutational status. A total of 998 slides were sampled, 325 posi-\ntive and 673 negative.\nMSKCC STK11 Mutation Prediction in LUAD.M S K - I M P A C Td e r i v e d\nSTK11 mutational status. A total of 998 slides were sampled, 122 posi-\ntive and 876 negative.\nMSKCCALK Mutation Prediction in LUAD. MSK-IMPACT derivedALK\nmutational status. A total of 999 slides were sampled, 144 positive and\n855 negative.\nMSKCC ICI Therapy Response Prediction in NSCLC. Non-small cell\nlung cancer (NSCLC) patients who received PD-L1 blockade-based\nimmune checkpoint inhibitor (ICI) therapy between 2013 and 2019 at\nMSKCC were considered. Cytology specimens were excluded. The\nobjective overall response was determined by RECIST and performed\nby a blinded thoracic radiologist. A total of 454 slides were obtained,\n86 positive and 368 negative.\nDownstream Task Training\nIn the SSL literature, the performance of downstream tasks is frequently\nassessed by training a linear classiﬁer (linear probing) on top of features\nextracted by a frozen encoder, or via zero-shot approaches such as k-NN.\nFor pathology slides, there is no direct way to translate these approaches\nwithout having tile-level annotations. Instead, it is common practice to\ntrain a slide-level aggregator. For this purpose we chose the popular\nGated MIL Attention (GMA) model\n47 with a linear classiﬁer on top. Since\nGMA does not consider the spatial distribution of tiles over the slide in\nits prediction, it is a simple method to test the expressiveness of the\nfeature space generated by the SSL pretraining. In fact GMA is widely\nused to assess the performance of pathology foundation models\n23,25,26.\nFurther, despite being a simple strategy, it is highly performant even\ncompared to more recent aggregators in computational pathology\n43.\nFor each slide, tissue tiles were extracted at 20x magniﬁcation (0.5\nmicrons per pixel, MPP) and embedded into a feature representation\nusing a speciﬁc foundation model. This magniﬁcation is appropriate\nfor all foundation models considered. Each slide is then converted to a\n2D matrix where every row corresponds to a tile in the slide and the\ncolumns contain the features. The vectorized slide is the input to the\nGMA model, which combines the tile representations into a slide-level\nrepresentation, which is then linearly projected to class scores.\nTo estimate generalization performance, we employed a Monte\nCarlo Cross-Validation (MCCV) strategy. For each MCCV split, 80% of\nthe samples were assigned to the training set and the remaining 20%\nwere assigned to the validation set. For each benchmark task, the 20\nMCCV folds were randomly sampled and keptﬁxed for all experi-\nments. Each MCCV split was run twice to assess stochasticﬂuctuations\nduring training and the results were averaged across the two replicas.\nAll models were trained using a single GPU for 50 epochs using the\nAdamW\n48 optimizer. A cosine decay with warm-up schedule was used\nfor the learning rate with a peak learning rate of 0.0001. The exact\nparameters used for training can be found in theGitHub repository\nand in Supplementary Tables 4 and 5. For each task and foundation\nmodel, the distribution of validation AUCs across the 20 MCCVs are\nused to assess the trained model performance.\nFoundation Models\nIn this work we focus on benchmarking publicly available vision\nfoundation models trained on large pathology corpora. More speciﬁ-\ncally, we consider only tile-level vision encoder models. Pre-trained\naggregation models are not considered and vision-language models\nare also not included. Given that the number of publicly available\nfoundation models has been increasing steadily, it is beyond the scope\nof this manuscript to exhaustively benchmark all models available. We\nstrived to chose a wide selection of models of different sizes from\nacademia and private companies using public and private pretraining\ndatasets, including CTransPath\n18, UNI23, Virchow25, Prov-GigaPath30,\nVirchow233, h-optimus-034,a n dPhikon-v235. We also included a trun-\ncated ResNet50 (tRes50) pretrained on ImageNet as a baseline due to\nits popularity in the computational pathology community. We must\nnote that for Virchow and Virchow2, since they were trained on slides\nfrom MSKCC, we can’t ensure that there is no overlap between their\npretraining cohort and the clinical tasks based on MSKCC data. In\naddition, only the tile-level encoder of Prov-GigaPath was considered\nin this work. For each foundation model, we followed the embedding\ninstructions provided by the authors in each respective repository.\nFor comparison, we further include two in-house trained foun-\ndation models: a ViT-small (21.7M parameters) and a ViT-base (85.8M\nparameters) trained with DINO\n28. These models were pretrained on a\nclinical dataset compiled at MSHS during normal hospital operation.\nThe pretraining dataset consisted of 423,563 H&E stained slides from\n88,035 cases and 76,794 patients. These include slides from 42 organs\nacross all pathology specialities. We ensured that no overlap exists\nbetween this pretraining dataset and the clinical benchmarking data-\nset. All slides were scanned on a Philips Ultrafast scanner at 40x\nmagniﬁcation (0.25 MPP), de-identiﬁed and converted to tiff format.\nThe total storage required for the raw tiffﬁles was around 600TB. As a\npreprocessing step, tissue tiles were extracted from each slide at 0.5\nMPP resolution, yielding approximately 3.2 billion tiles. The ViT-small\n( S P 2 2 M )w a st r a i n e do n1 2N v i d i aA 1 0 04 0 G BG P U sw i t hab a t c hs i z eo f\n90 per GPU for 17 days and 16 hours. The ViT-base (SP85M) was trained\non 8 Nvidia H100 80GB GPUs with a batch size of 100 per GPU for\n26 days and 11 hours. Both models were trained on approximately 1.6\nbillion tiles. The models are publicly available on HuggingFace:\nSP22M, SP85M.\nWe can observe that older foundation models are trained with\nvariants of contrastive learning. After the introduction of DINO, and\nlater DINOv2, recent foundation models have used the latter as go-to\npretraining algorithm. While evidence emerged that DINO tends to\noutperform contrastive learning and masked image modeling\napproaches for pathology pretraining\n26,36, there is no direct compar-\nison of DINO and DINOv2. To summarize, current pathology founda-\ntion models are trained in a very homogeneous manner, leveraging for\nthe most part DINOv2 and a small number of ViT architectures. The\nmain differences between the various efforts mainly lay in the com-\nposition of the pretraining dataset.\nArticle https://doi.org/10.1038/s41467-025-58796-1\nNature Communications|         (2025) 16:3640 10\nAutomated External Benchmarking\nTo facilitate the benchmarking of external models, we have developed\nan automated pipeline that leverages the Azure cloud infrastructure for\ninterfacing with users and on-premise computing to minimize costs.\nInterested users will need toﬁll out aMicrosoft Formto express their\ninterest in benchmarking their model. This form collects essential details\nsuch as the user’s email address and model information. The form\nsubmission triggers a Power Automate process in the back-end which\ngenerates a OneDrive folder accessible to the external user. The process\nalso generates an email containing the OneDrive link and instructions,\nwhich is sent to the external user. The instructions prompt the user to\nupload twoﬁles to OneDrive: i) a docker (or singularity) container that\nincludes the model weights, and ii) an inference script that returns the\nmodel’s output. We provide a template of the inference script that users\ncan customize and detailed instructions onGitHub. Currently,ﬁles up to\n250GB can be uploaded. Once uploaded, theseﬁles will trigger the\nbenchmarking pipeline. Files are copied to the local cluster and are used\nto generate tile embeddings which are stored as binaryﬁles. These are\nthen used to train a GMA aggregator asdescribed previously. Results of\nthe benchmark are then returned to the user via email. If the user opted\nto release to the leaderboard, the results will be posted on ourGitHub\npage. An overview of the workﬂow is presented in Supplementary Fig. 1.\nReporting summary\nFurther information on research design is available in the Nature\nPortfolio Reporting Summary linked to this article.\nData availability\nDigital pathology benchmark data will not be made available due to\nlegal, privacy, and data contamination considerations. If benchmark\ndata would be made available, it would likely be scraped for pretrain-\ning foundation models, negating the beneﬁts of an independent\nbenchmark. We provide a mechanism to evaluate user provided\nfoundation models on our benchmarking tasks. Instructions can be\nfound in theGitHub repository. Source data for eachﬁgure are pro-\nvided as a Source Dataﬁle. Source data are provided with this paper.\nCode availability\nCode used for pretraining SP22M and SP85M was taken from the\nofﬁcial DINO repository. The code associated with this work is avail-\nable in thisGitHub repository with a MIT License49.\nReferences\n1. LeCun, Y., Bengio, Y. & Hinton, G. Deep learning.Nature 521,\n436– 444 (2015).\n2. Dosovitskiy, A. et al. An image isworth 16 × 16 words: Transformers\nfor image recognition at scale. Preprint atarXiv:2010.11929(2020).\n3. Gao, Z. et al. Instance-based vision transformer for subtyping of\npapillary renal cell carcinoma in histopathological image. InProc.\nPart VIII 24, Medical Image Computing and Computer Assisted\nIntervention–MICCAI 2021: 24th International Conference299– 308\n(Strasbourg, 2021).\n4. Akinyelu, A. A., Zaccagna, F., Grist, J. T., Castelli, M. & Rundo, L.\nBrain tumor diagnosis using machinelearning, convolutional neural\nnetworks, capsule neural networks and vision transformers, applied\nto mri: a survey.J. Imaging8, 205 (2022).\n5. Kumar, N. et al. Convolutional neural networks for prostate cancer\nrecurrence prediction.Med. Imaging 2017: Digi. Pathol.10140,\n106– 117 (2017).\n6. Coudray, N. et al. Classiﬁcation and mutation prediction from\nnon– small cell lung cancer histopathology images using deep\nlearning.Nat. Med.24, 1559– 1567 (2018).\n7. Verma, R. et al. Sexually dimorphic computational histopathologi-\ncal signatures prognostic of overall survival in high-grade gliomas\nvia deep learning.Sci. Adv.10,e a d i 0 3 0 2( 2 0 2 4 ) .\n8. Shen, Y. et al. Explainable survival analysis with convolution-\ninvolved vision transformer.Proc. AAAI Conf. Artif. Intell.36,\n2207– 2215 (2022).\n9. Mobadersany, P. et al. Predicting cancer outcomes from histology\nand genomics using convolutional networks.P r o c .N a t lA c a d .S c i .\n115,E 2 9 7 0– E2979 (2018).\n1 0 . C h e n ,X . ,X i e ,S .&H e ,K .A ne m p i r i c a ls t u d yo ft r a i n i n gs e l f -\nsupervised vision transformers. InProc. IEEE/CVF International\nConference on Computer Vision9640– 9649 (IEEE, 2021).\n11. Khan, A. et al. A survey of the self supervised learning mechanisms\nfor vision transformers. Preprint atarXiv:2408.17059(2024).\n12. Lu, M. Y. et al. Data-efﬁcient and weakly supervised computational\npathology on whole-slide images.Nat. Biomed. Eng.5,5 5 5– 570\n(2021).\n13. Ilse, M., Tomczak, J. & Welling,M. Attention-based deep multiple\ninstance learning.Int. Confer. Mach. Learn.80,2 1 2 7–\n2136 (2018).\n14. Shao, Z. et al. Transmil: Transformer based correlated multiple\ninstance learning for whole slide image classiﬁcation. Adv. neural\nInf. Process. Syst.34,2 1 3 6– 2147 (2021).\n15. Mormont, R., Geurts, P. & Marée, R. Comparison of deep transfer\nlearning strategies for digital pathology. InProc. IEEE Conference on\nComputer Vision and Pattern Recognition Workshops2262– 2271\n(IEEE, 2018).\n16. Tabibu, S., Vinod, P. & Jawahar, C. Pan-renal cell carcinoma clas-\nsiﬁcation and survival prediction from histopathology images using\ndeep learning.Sci. Rep.9, 10509 (2019).\n17. Carmichael, I. et al. Incorporating intratumoral heterogeneity into\nweakly-supervised deep learning models via variance pooling. In\nProc. International Conference on Medical Image Computing and\nComputer-Assisted Intervention387– 397 (2022).\n18. Wang, X. et al. Transformer-based unsupervised contrastive\nlearning for histopathological image classiﬁcation. Med. Image\nAnal. 81,1 0 2 5 5 9 .https://linkinghub.elsevier.com/retrieve/pii/\nS1361841522002043 (2022).\n19. Liu, Z. et al. Swin transformer: Hierarchical vision transformer using\nshifted windows. InProc. IEEE/CVF International Conference on\nComputer Vision10012– 10022 (IEEE, 2021).\n20. Weinstein, J. N. et al. The cancer genome atlas pan-cancer analysis\nproject.Nat. Genet.45, 1113– 1120 (2013).\n21. Filiot, A. et al. Scaling self-supervised learning for histopathology\nwith masked image modeling.medRxiv 2023– 07 (2023).\n22. Zhou, J. et al. ibot: Image bert pre-training with online tokenizer.\nPreprint atarXiv:2111.07832(2021).\n23. Chen, R. J. et al. Towards a general-purpose foundation model for\ncomputational pathology.Nat. Med.30,8 5 0– 862 (2024).\n24. Oquab, M. et al. Dinov2: Learning robust visual features without\nsupervision. Preprint atarXiv:2304.07193(2023).\n25. Vorontsov, E. et al. A foundation model for clinical-grade compu-\ntational pathology and rare cancers detection.Nat. Med.30,\n2924– 2935 (2024).\n26. Campanella, G. et al. Computational pathology at health system\nscale– self-supervised foundation models from three billion images.\nPreprint atXiv:2310.07033(2023).\n27. He, K. et al. Masked autoencoders are scalable vision learners. In\nProc. IEEE/CVF Conference on Computer Vision and Pattern\nRecognition16000– 16009 (IEEE, 2022).\n28. Caron, M. et al. Emerging properties in self-supervised vision\ntransformers. InProc. IEEE/CVF International Conference on Com-\nputer Vision9650– 9660 (IEEE, 2021).\n29. Dippel, J. et al. Rudolfv: a foundation model by pathologists for\npathologists. Preprint atarXiv:2401.04079(2024).\n30. Xu, H. et al. A whole-slide foundation model for digital pathology\nfrom real-world data.Nature 630,1 8 1– 188 (2024).\n31. Ding, J. et al. Longnet: Scaling transformers to 1,000,000,000\ntokens. Preprint atarXiv:2307.02486(2023).\nArticle https://doi.org/10.1038/s41467-025-58796-1\nNature Communications|         (2025) 16:3640 11\n32. Network, C. G. A. R. et al. Comprehensive molecular proﬁling of\nlung adenocarcinoma.Nature 511, 543 (2014).\n33. Zimmermann, E. et al. Virchow 2: Scaling self-supervised mixed\nmagniﬁcation models in pathology. Preprint atarXiv:2408.00738\nhttps://arxiv.org/abs/2408.00738(2024).\n34. Saillard, C. et al. H-optimus-0.https://github.com/bioptimus/\nreleases/tree/main/models/h-optimus/v0(2024).\n35. Filiot, A., Jacob, P., Mac Kain, A. & Saillard, C. Phikon-v2, a large and\npublic feature extractor for biomarker prediction. Preprint at\narXiv:2409.09173(2024).\n36. Kang, M., Song, H., Park, S., Yoo, D. & Pereira, S. Benchmarking self-\nsupervised learning on diverse pathology datasets. InProc. IEEE/\nCVF Conference on Computer Vision and Pattern Recognition\n3344– 3354 (IEEE, 2023).\n37. Campanella, G. et al. A clinical benchmark of public self-supervised\npathology foundation models. Preprint atarXiv:2407.06508(2024).\n38. Goyal, P. et al. Vision models are more robust and fair when pre-\ntrained on uncurated images without supervision. Preprint at\narXiv:2202.08360.https://arxiv.org/abs/2202.08360(2022).\n39. Bhattacharyya, P., Huang, C. & Czarnecki, K. Ssl-lanes: Self-\nsupervised learning for motion forecasting in autonomous driving.\nConfer. Robot Learn.1793– 1805 (2023).\n40. Radford, A. et al. Learning transferable visual models from natural\nlanguage supervision.Int. Confer. Mach. Learning8748– 8763 (2021).\n41. Neidlinger, P. et al. Benchmarking foundation models as feature\nextractors for weakly-supervised computational pathology. Pre-\nprint atarXiv:2408.15823(2024).\n42. Xie, C. et al. Computational biomarker predicts lung ici response via\ndeep learning-driven hierarchical spatial modelling from h&e.\nhttps://www.researchsquare.com/article/rs-1251762/v1(2022).\n43. Chen, S. et al. Benchmarking embedding aggregation methods in\ncomputational pathology: A clinical data perspective. InProceed-\nings of the MICCAI Workshop on Computational Pathology,( e d s\nC i o m p i ,F .e ta l . )V o l .2 5 4 ,3 8– 50 (PMLR, 2024).https://\nproceedings.mlr.press/v254/chen24a.html.\n44. Cheng, D. T. et al. Comprehensive detection of germline variants by\nmsk-impact, a clinical diagnostic platform for solid tumor mole-\ncular oncology and concurrent cancer predisposition testing.BMC\nMed. Genom.10,1 – 9( 2 0 1 7 ) .\n45. Zehir, A. et al. Mutational landscape of metastatic cancer revealed\nfrom prospective clinical sequencing of 10,000 patients.Nat. Med.\n23,7 0 3– 713 (2017).\n46. Campanella, G. et al. H&e-based computational biomarker enables\nuniversal egfr screening for lung adenocarcinoma. Preprint at\narXiv:2206.10573(2022).\n47. Ilse, M., Tomczak, J. & Welling, M. Attention-based deep multiple\ninstance learning.Int Confer. Mach. Learning.2127– 2136 https://\narxiv.org/abs/1802.04712(2018).\n48. Loshchilov, I. & Hutter, F. Decoupled weight decay regularization.\nPreprint at\narXiv:1711.05101https://arxiv.org/abs/1711.05101\n(2017).\n49. Campanella, G. sinai-computational-pathology/SSL_tile_bench-\nmarks: First release.https://doi.org/10.5281/zenodo.15110130.\nAcknowledgements\nWe acknowledge Sahar Alawieh for curating the SUH cohorts. This work\nis supported in part through the use of the AI-Ready Mount Sinai (AIR.MS)\nresearch platform and the expertise provided by the team at the Hasso\nPlattner Institute for Digital Health at Mount Sinai (HPI.MS). We also uti-\nlized computational resources and expertise from Scientiﬁc Computing\nand Data at the Icahn School of Medicine, supported by the Clinical and\nTranslational Science Awards (CTSA) grant UL1TR004419. Additionally,\nresearch funding was provided in part by a grant from from the National\nLibrary of Medicine (NLM) (R01 LM013766, G.C.), by a Cancer Center\nSupport Grant from the NIH/NCI (P30CA008748), and by a grant from\nthe Warren Alpert Foundation (C.V.) through the Warren Alpert Center\nfor Digital and Computational Pathology at Memorial Sloan Kettering\nCancer Center.\nAuthor contributions\nG.C., C.V. conceived the study. G.C., S.C., R.V. performed the experi-\nments and analyzed the results. R.K., J.Z., A.S., B.V. curated the MSHS\ndetection cohorts. R.K., J.Z. curated the MSHS breast biomarker cohorts.\nM.C., J.H. curated the MSHS NGS cohort. A.E., K.H. curated the MSHS\nHRD cohort. I.S., N.N. curated the SUH NGS cohorts. C.V. provided the\nMSKCC NGS cohorts. C.V., A.J.S. provided the MSKCC IO cohort. S.M.\nfacilitated the collaborative research agreements between institutions.\nG.C., M.S. developed the automatic benchmarking pipeline.\nCompeting interests\nC.V. reports intellectual property rights and equity interest in Paige.AI,\nInc. A.J.S. reports consulting/advising role to J&J, Bayer, KSQ ther-\napeutics, BMS, Merck, Astrazeneca, Synthekine, cTRL therapeutics,\nRegeneron, Enara Bio, Perceptive Advisors, Oppenheimer and Co,\nUmoja Biopharma, Legend Biotech, Iovance Biotherapeutics, Obsidian\nTherapeutics, Prelude Therapeutics, Immunocore, Lyell Immuno-\npharma, Amgen and Heat Biologics. A.J.S. receives research funding\nfrom GSK (Inst), Obsidian (Inst), Lilly (Inst), PACT pharma (Inst), Iovance\nBiotherapeutics (Inst), Achilles therapeutics (Inst), Merck (Inst), Synthe-\nkine (Inst), BMS (Inst), Harpoon Therapeutics (Inst), AfﬁniT therapeutics\n(Inst), Legend Therapeutics (Inst),Synthekine (Inst) and Amgen (Inst).\nG.C., S.C., M.S., R.V., S.M., J.Z., A.S., M.C., B.V., A.E., I.S., N.N., K.H., R.K.,\nJ.H. declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41467-025-58796-1.\nCorrespondenceand requests for materials should be addressed to\nGabriele Campanella or Chad Vanderbilt.\nPeer review informationNature Communicationsthanks Frauke Wilm,\nand the other, anonymous, reviewer(s)for their contribution to the peer\nreview of this work. A peer reviewﬁle is available.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jur-\nisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the\nCreative Commons licence, and indicate if you modiﬁed the licensed\nmaterial. You do not have permission under this licence to share adapted\nmaterial derived from this article or parts of it. The images or other third\nparty material in this article are included in the article’s Creative\nCommons licence, unless indicatedotherwise in a credit line to the\nmaterial. If material is not included in the article’s Creative Commons\nlicence and your intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission directly\nfrom the copyright holder. To view a copy of this licence, visithttp://\ncreativecommons.org/licenses/by-nc-nd/4.0/.\n© The Author(s) 2025\nArticle https://doi.org/10.1038/s41467-025-58796-1\nNature Communications|         (2025) 16:3640 12",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.6764193177223206
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6760207414627075
    },
    {
      "name": "Computer science",
      "score": 0.4790821671485901
    },
    {
      "name": "Medicine",
      "score": 0.3678455948829651
    },
    {
      "name": "Data science",
      "score": 0.3588610887527466
    },
    {
      "name": "Pathology",
      "score": 0.3270952105522156
    },
    {
      "name": "Geography",
      "score": 0.09165987372398376
    },
    {
      "name": "Cartography",
      "score": 0.07244616746902466
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98704320",
      "name": "Icahn School of Medicine at Mount Sinai",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2801112325",
      "name": "Sahlgrenska University Hospital",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I881427289",
      "name": "University of Gothenburg",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I1334819555",
      "name": "Memorial Sloan Kettering Cancer Center",
      "country": "US"
    }
  ],
  "cited_by": 18
}