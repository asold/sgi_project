{
  "title": "A Multi-Level Circulant Cross-Modal Transformer for Multimodal Speech Emotion Recognition",
  "url": "https://openalex.org/W4307875332",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3095306701",
      "name": "Peizhu Gong",
      "affiliations": [
        "Shanghai Maritime University"
      ]
    },
    {
      "id": "https://openalex.org/A2108491511",
      "name": "Jin Liu",
      "affiliations": [
        "Shanghai Maritime University"
      ]
    },
    {
      "id": "https://openalex.org/A2269990679",
      "name": "Zhongdai Wu",
      "affiliations": [
        "Shanghai Ship and Shipping Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2095736400",
      "name": "Bing Han",
      "affiliations": [
        "Shanghai Ship and Shipping Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2281887601",
      "name": "Y. Ken Wang",
      "affiliations": [
        "University of Pittsburgh at Bradford"
      ]
    },
    {
      "id": "https://openalex.org/A2184694232",
      "name": "Huihua He",
      "affiliations": [
        "Shanghai Normal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3156576211",
    "https://openalex.org/W2074788634",
    "https://openalex.org/W6713634562",
    "https://openalex.org/W6755596326",
    "https://openalex.org/W6761186630",
    "https://openalex.org/W3112021549",
    "https://openalex.org/W6792597526",
    "https://openalex.org/W3088631780",
    "https://openalex.org/W6665983271",
    "https://openalex.org/W3175811101",
    "https://openalex.org/W6763015118",
    "https://openalex.org/W3118616296",
    "https://openalex.org/W3214373884",
    "https://openalex.org/W3080842357",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W3041561163",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2146334809",
    "https://openalex.org/W2963873807",
    "https://openalex.org/W6753277404",
    "https://openalex.org/W2098287351",
    "https://openalex.org/W1576180489",
    "https://openalex.org/W2772173423",
    "https://openalex.org/W3023056542",
    "https://openalex.org/W6691723933",
    "https://openalex.org/W3203668668",
    "https://openalex.org/W6803286138",
    "https://openalex.org/W2769309311",
    "https://openalex.org/W3025229828",
    "https://openalex.org/W6763255934",
    "https://openalex.org/W2889717020",
    "https://openalex.org/W6763650382",
    "https://openalex.org/W3045969489",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W2423557781",
    "https://openalex.org/W2752796333",
    "https://openalex.org/W2887997457",
    "https://openalex.org/W6770717842",
    "https://openalex.org/W6774314701",
    "https://openalex.org/W3115295967",
    "https://openalex.org/W6789512599",
    "https://openalex.org/W2583743457",
    "https://openalex.org/W6697498398",
    "https://openalex.org/W2153720647",
    "https://openalex.org/W6714031499",
    "https://openalex.org/W2962736520",
    "https://openalex.org/W6739380106",
    "https://openalex.org/W2951442257",
    "https://openalex.org/W6748096335",
    "https://openalex.org/W6725688724",
    "https://openalex.org/W6785738406",
    "https://openalex.org/W6758733797",
    "https://openalex.org/W6632024315",
    "https://openalex.org/W2797947982",
    "https://openalex.org/W6755893022",
    "https://openalex.org/W6757361602",
    "https://openalex.org/W6770174892",
    "https://openalex.org/W2970431814",
    "https://openalex.org/W6771188984",
    "https://openalex.org/W6766310171",
    "https://openalex.org/W2963175441",
    "https://openalex.org/W2946218857",
    "https://openalex.org/W6771401377",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W3209499093",
    "https://openalex.org/W3143477889",
    "https://openalex.org/W2950438065",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W2559655401",
    "https://openalex.org/W2061900096",
    "https://openalex.org/W4236377898"
  ],
  "abstract": "Speech emotion recognition, as an important component of human-computer interaction technology, has received increasing attention. Recent studies have treated emotion recognition of speech signals as a multimodal task, due to its inclusion of the semantic features of two different modalities, i.e., audio and text. However, existing methods often fail in effectively represent features and capture correlations. This paper presents a multi-level circulant cross-modal Transformer (MLCCT) for multimodal speech emotion recognition. The proposed model can be divided into three steps, feature extraction, interaction and fusion. Self-supervised embedding models are introduced for feature extraction, which give a more powerful representation of the original data than those using spectrograms or audio features such as Mel-frequency cepstral coefficients (MFCCs) and low-level descriptors (LLDs). In particular, MLCCT contains two types of feature interaction processes, where a bidirectional Long Short-term Memory (Bi-LSTM) with circulant interaction mechanism is proposed for low-level features, while a two-stream residual cross-modal Transformer block is applied when high-level features are involved. Finally, we choose self-attention blocks for fusion and a fully connected layer to make predictions. To evaluate the performance of our proposed model, comprehensive experiments are conducted on three widely used benchmark datasets including IEMOCAP, MELD and CMU-MOSEI. The competitive results verify the effectiveness of our approach.",
  "full_text": "This work is licensed under a Creative Commons Attribution 4.0 International License,\nwhich permits unrestricted use, distribution, and reproduction in any medium, provided\nthe original work is properly cited.\nechT PressScienceComputers, Materials & Continua\nDOI: 10.32604/cmc.2023.028291\nArticle\nA Multi-Level Circulant Cross-Modal Transformer for Multimodal Speech\nEmotion Recognition\nPeizhu Gong1,J i nL i u1, Zhongdai Wu2,B i n gH a n2,Y .K e nW a n g3 and Huihua He4,*\n1College of Information Engineering, Shanghai Maritime University, Shanghai, 201306, China\n2Shanghai Ship and Shipping Research Institute, Shanghai, 200135, China\n3Division of Management and Education, University of Pittsburgh, Bradford, USA\n4College of Early Childhood Education, Shanghai Normal University, Shanghai, 200234, China\n*Corresponding Author: Huihua He. Email: hehuihua@shnu.edu.cn\nReceived: 07 February 2022; Accepted: 14 March 2022\nAbstract: Speech emotion recognition, as an important component of human-\ncomputer interaction technology, has received increasing attention. Recent\nstudies have treated emotion recognition of speech signals as a multimodal\ntask, due to its inclusion of the semantic features of two different modali-\nties, i.e., audio and text. However, existing methods often fail in effectively\nrepresent features and capture correlations. This paper presents a multi-level\ncirculant cross-modal Transformer (MLCCT) for multimodal speech emotion\nrecognition. The proposed model can be divided into three steps, feature\nextraction, interaction and fusion. Self-supervised embedding models are\nintroduced for feature extraction, which give a more powerful representation\nof the original data than those using spectrograms or audio features such\nas Mel-frequency cepstral coefficients (MFCCs) and low-level descriptors\n(LLDs). In particular, MLCCT contains two types of feature interaction\nprocesses, where a bidirectional Long Short-term Memory (Bi-LSTM) with\ncirculant interaction mechanism is proposed for low-level features, while a\ntwo-stream residual cross-modal Transformer block is applied when high-level\nfeatures are involved. Finally, we choose self-attention blocks for fusion and a\nfully connected layer to make predictions. To evaluate the performance of our\nproposed model, comprehensive experiments are conducted on three widely\nused benchmark datasets including IEMOCAP, MELD and CMU-MOSEI.\nThe competitive results verify the effectiveness of our approach.\nKeywords: Speech emotion recognition; self-supervised embedding model;\ncross-modal transformer; self-attention\n1 Introduction\nSpeech emotion recognition (SER) [1–3] is of significant importance in subjective cognitive\nresearch, which aims to determine a human’s emotional states towards a certain topic by understanding\nthe characteristics of speech in media. Traditional emotion recognition methods tend to be based on\nunimodality like image or text. However, the limited amount and single distribution of information\n4204 CMC, 2023, vol.74, no.2\nmakes the results unsatisfactory. Unlike the traditional paradigm, recent studies consider emotion\nrecognition of speech signals as a multimodal task because it contains semantic features of two\ndifferent modalities, i.e., audio and text. Y oon et al. [4] combined the information from audio and\ntext sequences using dual recurrent neural networks (RNNs) and then predicted the emotion class.\nY oon et al. [5] put forward multi-hop attention mechanism to compute the relevant segments of the\ntextual data and audio signal. In recent years, Transformer [6,7] has been widely used in various\nresearch areas including Computer Vision (CV) and Natural Language Processing (NLP) and achieved\nstate-of-the-art results. Naturally, the cross-modal Transformer [8] has been proposed for interaction\nproblems in multimodal task. However, studies [9] have shown that using the Transformer purely at an\nearly stage is not good for results or requires an extremely large dataset. Therefore, we propose a multi-\nlevel framework to extract features and interactions in a stepwise manner. In addition, when it comes\nto representing raw data, traditional approaches usually employ Mel-frequency cepstral coefficients\n(MFCCs) [10] for audio signals and glove embeddings [11] for textual sequences. Rather than using\nthose low-level feature extractor, self-supervised embedding models (SSE) [12] would be a better choice\ndue to its powerful representation capabilities. SSE models are usually pre-trained on pretext tasks with\na large number of unlabeled data in advance, and then generate more valuable feature representations\nfor downstream tasks [13–15]. Bidirectional Encoder Representations from Transformer (BERT) [16]\nis known as one of the most outstanding SSE for text representation, while Masked Autoencoder\n(MAE) [17] gives unlimited possibilities for vision learning.\nIn this paper, we propose a multi-level circulant cross-modal Transformer (MLCCT) for mul-\ntimodal speech emotion recognition. To the best of our knowledge, it is the first time that a\nTransformer-based progressive framework is used in multimodal speech emotion recognition, which\nmay better capture correlations between different modalities. MLCCT is composed of three parts,\nfeature extraction, interaction and fusion. SSE models are introduced for feature extraction, which\ngive a powerful representation of the original data. Specially, Transformer Encoder Representations\nfrom Alteration (TERA) [18] and a robustly optimized BERT pretraining approach (RoBERTa) [19]\nare used to represent audio signal and textual sequence, respectively. In particular, MLCCT contains\ntwo types of feature interaction processes. The first one employs a bidirectional Long Short-term\nMemory (Bi-LSTM) with circulant interaction mechanism for low-level features, while in the second\nstage a two-stream residual cross-modal Transformer block (RCTB) is applied. Finally, we choose\nself-attention blocks for fusion and a fully connected layer to make predictions. Comprehensive\nexperimental results on three benchmark datasets including IEMOCAP, MELD and CMU-MOSEI\n[20–22] show that the proposed MLCCT has achieved competitive results. In summary, the major\ncontributions of our research can be summarized as follows:\n• A multi-level framework is proposed for a progressive cross-modal interaction, which combines\nlocal and global features for accurate predictions, which will serve as an inspiration for future\nresearch.\n• A circulant interaction mechanism is presented to take full advantage of capturing correlations\nbetween different modalities in an early stage.\n• Most of the work done focuses on handcrafted feature extracting by using machine learning\nmethods, while SSE models are introduced in this paper to generate more valuable feature\nrepresentations for downstream classification.\n• A two-stream residual cross-modal Transformer block is put forward to establish deep interac-\ntions between modalities.\nThe remaining of this paper is structured as follows: In Section 2, we introduce related work\non multimodal speech emotion recognition and self-supervised learning. In Section 3, the details\nCMC, 2023, vol.74, no.2 4205\nof proposed MLCCT are presented. Finally, we evaluate the experimental results three benchmark\ndatasets and draw a conclusion in Sections 4 and 5.\n2 Related Work\n2.1 Multimodal Speech Emotion Recognition\nEmotion recognition models can be divided into two categories, discrete models and dimensional\nmodels. Discrete models use a few adjectives such as anger, disgust, fear, happiness, sadness, and\nsurprise to describe emotional states. Thus, it is widely accepted for its simplicity despite of the\nlimitations in presenting dynamic processes. In contrast, several basic attributes are usually selected as\ncoordinate measures of the emotion space in the dimensional emotion model. Then all emotional states\ncan be found on this space with its own coordinate points. The Pleasure-Arousal-Dominance model\n(PAD) [23] is one of the most well-known dimensional models, which can theoretically represent an\ninfinite number of emotions. However, the dimensional model is difficult to understand and complex\nto manipulate, resulting in it not being mainstream.\nTraditional methods recognize speech emotion by acoustic feature extraction or semantic infor-\nmation analysis. Acoustic features are classified into three categories: rhythmic features, spectral-\nbased correlation features, and sound quality features, which describe information about the pitch,\namplitude and timbre of speech, respectively. Bhargava et al. [24] improved automatic emotion\nrecognition from speech by incorporating rhythm and temporal features. Different metrics of speech\nrhythm are investigated with the aim to determine general emotional tendencies at the overall level\nby studying the regularity of the appearance of certain linguistic elements in speech. Palo et al. [25]\nput forward a Wavelet-based MFCCs model to perform affective computing with respect to spectral\nfeatures, and the improved model is more resistant to interference from noise. Recently, deep learning\ntechniques have demonstrated breakthrough performance and have been considered as an alternative\nto tradition approaches in SER. The two most popular neural network architectures are convolutional\nneural networks (CNNs) [26–29] and recurrent neural networks (RNNs) [30–32]. Among them,\nCNN is beneficial for spatial information and RNN helps to capture temporal information in\nSER. Y enigalla et al. [33] presented a CNN-based model to classify emotion using phoneme and\nspectrogram. Zhao et al. [34] proposed a hybrid model, in which a 1D CNN-LSTM network and a 2D\nCNN-LSTM network were constructed to learn local and global emotion-related features from speech\nand log-mel spectrogram, respectively. In addition, the attention mechanism is particularly favored\nin multimodal interaction problems, especially Transformer. Tsai et al. [35] proposed Multimodal\nTransformer (MulT), which leverages inter-modal connections to individually reinforce target modal\ncharacteristics. A Transformer-based joint-encoding for emotion recognition [36] is put forward, which\nrelies on a modular co-attention and a glimpse layer to jointly encode one or more modalities. However,\nusing transformers purely at an early stage is detrimental to the results, so we propose a progressive\nframework to address this issue.\n2.2 Self-supervised Embedding\nIn recent years, supervised learning has hit a bottleneck, which relies heavily on expensive manual\nlabeling and suffers from poor generalization. As an alternative, self-supervised embedding models\n(SSE) has showed its soaring performance and gained increasing attention. SSE aims to learn a generic\nrepresentation for downstream tasks, where training data is labeled by using a semi-automatic process.\nSpecifically, the process may predict a portion of the data from the others. The general procedure is\nthat the SSE models are firstly pre-trained on a set of pretext tasks, and then the pre-trained SSE\nmodels extract features for downstream tasks.\n4206 CMC, 2023, vol.74, no.2\nSSE models can be categorized into generative, contrastive and adversarial. Generative models\nencode the input x into a high-dimensional vector and then reconstructsx from the vector as\nthe output. GPT and GPT-2 [37,38] reconstruct the sentence by predicting the next word. BERT\nincorporates masked language model on the basis of next word prediction and achieves better results.\nPixelCNN [39] fixes the picture by next pixel predicting, while VQ-V AE [40] acts on the whole picture.\nGenerative models recover the original data distribution without making assumptions for downstream\ntasks, leading to its wide applications. However, generative models are extremely sensitive to rare\nsamples and there is a semantic gap between pretext tasks and downstream tasks. Contrastive models\nmap pairs of data to a common space and measure similarity. Deep InfoMax [41] focus on modeling the\nmutual information between local feature and global context, while MoCo [42] and SimCLR [43]t e n d\ntowards instance-level representations. In addition, works including CLEAR [44] and BERT-CT [45]\nhave presented overwhelming performances on various benchmark datasets. In comparison, without\ndecoder, contrastive models are usually lightweight and take classification tasks as downstream tasks.\nAdversarial models train an encoder to generate fake samples and a decoder to distinguish them from\nreal samples. Adversarial models, especially Generative Adversarial Networks (GAN) [46], have shown\nsignificant results in image generation and style transformation. However, there are still challenges for\nits future development due to its easy collapse and limited application in NLP .\n3 Method\nAs shown in Fig. 1, the structure of proposed MLCCT is composed of three parts, feature\nextraction, interaction and fusion. SSE models are introduced for feature extraction, which give a\nmore powerful representation of the original data than those using spectrograms or MFCCs. In\nparticular, MLCCT contains two types of feature interaction processes, where a Bi-LSTM with\ncirculant interaction mechanism is proposed for low-level features, while a two-stream RCTB is applied\nwhen high-level features are involved. Finally, we choose self-attention blocks for fusion and a fully\nconnected layer to make predictions. It is believed that the progressive framework may better capture\ncorrelations between different modalities.\nFigure 1:Overall architecture of proposed MLCCT\nCMC, 2023, vol.74, no.2 4207\n3.1 Self-supervised Embedding Layer\nRather than using low-level feature extractor, SSE models would be a better choice due to its\npowerful representation capabilities. SSE models are usually pre-trained on pretext tasks with a large\nnumber of unlabeled data in advance, and then generate more valuable feature representations for\ndownstream tasks. Specially, TERA and RoBERTa are used to represent audio signal and textual\nsequence, respectively.\nTERA introduces a total of three types of alteration to pre-train Transformer encoders on a large\namount of unlabeled speech, namely time alteration, frequency alteration and magnitude alteration.\nAmong them, the time alteration enables to learn richer phonetic content through contextual under-\nstanding of previous and future content., the frequency alteration effectively encodes speaker identity\nand the magnitude alteration improves performance by increasing data diversity for pre-training. We\ndownload the checkpoint of TERA from the open-sourced fairseq toolkit, which is pre-trained on\nLibriSpeech dataset for 960 h. The models’ input is an 80-dimensional log ME.\nRoBERTa is an extension study on BERT pre-training, carefully measuring the effects of many key\nhyperparameters and training data sizes. The improvements specifically include the following points.\nFirstly, RoBERTa has a larger training dataset with the addition of CC-NEWS, OPEN WEB TEXT\nand STORIES, while expanding batch size from 256 to 8 K. Secondly, dynamic masks are proposed\nto replace the original static masks to perform data augmentation for larger datasets. Finally, the next\nsentence prediction mechanism is eliminated, which is proved to be of little use. In this work, we train\nRoBERTa following the BERT\nLARGE architecture, which contains a 24-layer Transformer with 16 self-\nattention heads and 1024 hidden dimensions. The model is pre-trained on the BOOKCORPUS and\nWIKIPEDIA datasets with 100 K steps.\n3.2 Multi-level Interaction Module\nThe proposed multi-level interaction module contains low-level interaction sub-module and high-\nlevel interaction sub-module, which capture the correlations between modalities in a progressive way.\nWith the low-level feature interaction sub-module, MLCCT can extract fine-grained local features\nfrom speech signals and text sequences. Considering the superiority of RNN in handling sequence\ntasks, we employ a Bi-LSTM with circulant interaction mechanism, as shown inFig. 2.L S T M\nis an extension of RNN, which can effectively solve the problem of gradient disappearance or\nexplosion. LSTM makes some improvements and optimization based on the structure of RNN, adding\nmemory cells and updating memory by input gates and forget gates. When LSTM is dealing with the\ninformation at current timestampt, it receives a total of three input vectorsx\nt, ht−1 and Ct−1,w h e r e\nxt is the input of current timestamp,ht−1 and Ct−1 are the output and memory cell state of previous\ntimestamp, respectively. The specific process can be described by the following equation.\nft = σ\n(\nδf · xt + ϕf · ht−1 + εf\n)\n(1)\nit = σ (δi · xt + ϕi · ht−1 + εi) (2)\not = σ (δo · xt + ϕo · ht−1 + εo) (3)\n˜Ct = tanh (δo · [xt, ht−1] + εC) (4)\nwhere ft, it and ot stand for forget gate, input gate and output gate, respectively.δ is weight matrix from\nthe input layer to the hidden layer, whileϕ denotes weight matrix from the hidden layer to the hidden\nlayer andε is the offset matrix. In addition,σ represents sigmoid activation function. The LSTM\nfirst determines current stateCt by controlling the memory cell how much prior informationCt−1 is\n4208 CMC, 2023, vol.74, no.2\nforgotten and how much temporary state˜Ct is retained through forgetting gatesft and input gatesit.\nAnd then, the output gateot is used to decide how to output the hidden state layerht by computingCt\nin memory cell.\nCt = it ∗ ˜Ct + ft ∗ Ct−1 (5)\nht = ot ∗ tanh (Ct) (6)\nFigure 2:Detailed procedures of Bi-LSTM with circulant interaction mechanism\nAfter Bi-LSTM, the circulant interaction mechanism is put forward to capture inter-modal\ncorrelation in an early stage. Specially, the intermediate results of audioa ∈ Ra and texts ∈ Rs will\nconstructed as circulant matrices and multiplied by each other, where elements in each row perform\na shift operation without changing the values as a whole. To further reduce computational cost, two\nweight matricesW\na ∈ Rd×a and Ws ∈ Rd×s are involved to project both feature vectors to a lower\ndimensional space. The detailed procedures are illustrated inEqs. (7)–(9).\nA = ˜A ⊙ (Ws · s) = 1\nd\nd∑\ni=1\nai ⊙ (Ws · s) (7)\nS = ˜S ⊙ (Wa · a) = 1\nd\nd∑\ni=1\nsi ⊙ (Wa · a) (8)\nwhere\n˜A = circ_matrix (Wa · a) , ˜S = circ_matrix (Ws · s) (9)\nwhere ai ∈ Rd and si ∈ Rd are row vectors of circulant matrices,⊙ is denoted as Hadmard product.\nThrough the element-wise product, we complete the preliminary interaction and get the integrated\nvectors A ∈ Rd and S ∈ Rd .\nAs shown inFig. 3, a two-stream RCTB is proposed for high-level interaction between audio and\ntext. The RCTB takes the low-level integrated vectors as input. SinceA and S share same dimension,\nthere are no additional adjustment operations. RCTB takes advantage of the cross-modal attention\nCMC, 2023, vol.74, no.2 4209\nmechanism to explore inter-modal relationships and strengthen the target modal representation. In\naddition, shortcuts are employed to learn the residuals, which makes it easier for the model to converge.\nTake audio as the query vectorQ\nA = AWQA ∈ Rd×1,t e x ta st h ek e yv e c t o rKS = SWKS ∈ Rd×1 and value\nvector VS = SWVS ∈ Rd×1,w h e r eWQA , WKS and WVS are weights. Then a scaled dot-product attention\nis computed, and the specific process can be seen in the following formula.\nYA = A + softmax\n(∑ d\ni=1QAi ⊙ KS\n√\nd\n)\nVS (10)\nwhere YA ∈ Rd×1 is the final integrated vector of audio. In the same way, we can also obtain the final\ninteraction vector with text as the target modalityYs.\nYS = S + softmax\n(∑ d\ni=1QSi ⊙ KA\n√\nd\n)\nVA (11)\nFigure 3:Structure of two-stream residual cross-modal Transformer block\nFor fusion, we first concatenate two final integrated vectorsYA and YS, and then feed them\ninto multi-head self-attention blocks. The output from self-attention layer will pass through a fully\nconnected layer to make prediction. Multi-head attention allows the model to jointly attend to\ninformation from different representation subspaces.\nY = FC\n(\nW\nO\n(∑ h\ni=1\n/Theta1\n(\nQτ WQ\ni , Kτ WK\ni , Vτ WV\ni\n)))\n(12)\n4210 CMC, 2023, vol.74, no.2\nwhere\nQτ = Kτ = Vτ = concat (YA, YS) (13)\nwhere FC denotes fully connected layer,/Theta1is self-attention computing and h is the number of\nhead. Compared with somewhat straightforward fusion, self-attention blocks may lead to a further\nperformance gain.\n4 Experiment and Discussion\nWe carried out our experiments on three widely used benchmark datasets including IEMOCAP,\nMELD and CMU-MOSEI to verify the effectiveness of our proposed MLCCT. The testing environ-\nment was conducted on one single Nvidia RTX 3090 graphic card with 16GB memory, and an Intel(R)\nCore(TM) i7–7700 3.60 GHz.\n4.1 Dataset\nIEMOCAP collects data from five sessions of ten male and female participants, each session\nconsisting of two unique participants. IEMOCAP is segmented by dialogue and each dialogue is\nannotated with categorical labels, such as angry, happy, sad, neutral, surprised, etc. We divided the\nwhole dataset into five subsets, using the first four dialogues as training and validation and the last one\nas testing, which excludes speaker-related interference and is useful for real-life scenario applications.\nThe MELD dataset has over 12,000 discourses from the Old Friends TV series. Unlike other\ndatasets, MELD is a conversational dataset, with several speakers participating in a single dialogue.\nEach dialogue is annotated with any of seven emotions: anger, disgust, sadness, joy, surprise, fear, and\nneutrality.\nThe CMU-MOSEI dataset is the largest sentence-level sentiment analysis and emotion recognition\ndataset available in online video. CMU-MOSEI contains over 65 h of annotated video from over 1,000\nspeakers and 250 topics. Similar to the MELD dataset, CMU-MOSEI is divided into three groups:\ntraining set, validation set, and testing set. We performed data statistics for the three benchmark\ndatasets mentioned above, which are recorded inTabs. 1and 2.\nTable 1: Amount of data for each type of emotion in IEMOCAP\nEmotion IEMOCAP\nTraining Validation Testing\nHappy 198 28 58\nSad 426 60 122\nAngry 202 29 58\nNeutral 769 110 220\nCMC, 2023, vol.74, no.2 4211\nTable 2: Amount of data for each type of emotion in MELD and CMU-MOSEI\nEmotion MELD CMU-MOSEI\nTraining Validation Testing Training Validation Testing\nNeutral 4513 450 1204 - - -\nHappy 1661 157 380 7136 636 1438\nFear 264 36 50 257 28 55\nSad 674 107 204 2669 279 502\nDisgust 266 21 68 767 61 150\nAngry 1065 148 328 1607 136 384\nSurprise 1150 142 207 348 29 78\n4.2 Comparative Study\n4.2.1 Experiments on IEMOCAP\nDue to the uneven distribution of samples of each category in the IEMOCAP dataset, we\nselected the four most used emotion categories (neutral, angry, sad and happy) for our classification\nexperiments. The comparison of the proposed MLCCT with prior studies on the IEMOCAP dataset\ncan be clearly seen inTab. 3, and mean accuracy is used as the evaluation metric. It confirms that\nmultimodal speech emotion recognition tends to outperform those based on unimodal, audio or text.\nCompared with Lex-eVector, which is also based on multimodal data, MLCCT improves nearly 7%\nin accuracy, further verifying that our proposed progressive framework has a greater effect on the\ninteraction and fusion in multimodal speech emotion recognition.\nTable 3: Comparison of MLCCT with prior models on IEMOCAP\nModel Modality Acc\nLSTM [47] Audio 54\nExtreme Learning Machine (ELM) [48] Audio 54.3\nHierarchical Decision Tree [49] Audio 56.83\nRNN-ELM [50] Audio 62.85\nACNN [51] Audio 62.11\nRNN + Attention [52] Audio 63.5\nCNN + LSTM [53] Audio 64.5\nBi-LSTM-ELM + LLDs [54] Audio 64.2\nGreedy-Attention [55] Audio 59.4\nFCN + Attention [56] Audio 70.2\nBi-LSTM + Context-aware Attention [57] Audio 68.8\nLex-eVector [58] Audio 57.4\nText 53.5\nMultimodal 69.2\n(Continued)\n4212 CMC, 2023, vol.74, no.2\nTable 3:Continued\nModel Modality Acc\nEnsemble model of CNN and LSTM [59] Audio 62.72\nText 64.78\nMLCCT Audio 66.19\nText 56.5\nMultimodal 75.92\nTo further analyze the recognition results,Tab. 4shows the accuracy and F1 scores for each\ncategory, andFig. 4plots the confusion matrix. From the experimental results, we can find that our\nmodel has better performance in terms of accuracy compared to RA VEN [60]a n dM C T N[61] models.\nIn terms of the MulT model, MLCCT performs better overall, except for a 1.1% lower classification\naccuracy in Anger. We believe that the improvement stems from placing the Transformer further back\nin the network and thus more adapted to high-level features. In brief, our model achieved significant\nrecognition results on the IEMOCAP dataset.\nTable 4: Performance for each category on IEMOCAP\nHappy Sad Angry Neutral\nMethod Acc F1 Acc F1 Acc F1 Acc F1\nRA VEN [60] 77 76.8 67.6 65.6 65 64.1 62 59.5\nMCTN [61] 80.5 77.5 72 71.4 64.9 65.6 49.4 49.3\nMulT [35] 84.8 81.9 77.7 74.1 73.9 70.2 62.5 59.7\nMLCCT 84.7 82.8 78.1 75.2 72.8 72.1 65.1 62.5\nFig. 4shows in more detail the specific effects of the model on each category. It can be seen that\nMLCCT achieves the best recognition in “Happy”, followed by “Sad” and “Angry”, and the last one is\n“Neutral”. We speculate that it may be “Neutral” that is the most vaguely defined and is more difficult\nto represent in a multimodal form.\n4.2.2 Experiments on MELD\nThe evaluation of our proposed model compared to the conventional studies on MELD is\npresented inTab. 5. It can be seen that MLCCT outperforms any other model in terms of average\naccuracy and F1 score. Although the improvement is limited, for example, its accuracy is only 1.5%\nhigher than that of [102]. It is believed that the reason leading to this result may be the more complex\nspeakers and scenes in MELD.\nA more detailed analysis is conducted based on each category on the MELD. As can be seen in\nFig. 5, the recognition accuracy of our proposed model inevitably decreases as the number of emotion\ncategories increases. Specifically, MLCCT performs well on Neutral, Joy and Surprise, reaching\nan accuracy of 78.68 on Neural. However, its recognition results on Sadness and Disgust are not\nsatisfactory, especially sadness is often misclassified as Neutral. It is speculated that the reason leading\nto this result may lie in the uneven distribution of the various types of data in the MELD dataset, with\nrelatively little disgust and sadness data.\nCMC, 2023, vol.74, no.2 4213\nFigure 4:The recognition rate confusion matrix on IEMOCAP\nTable 5: Comparison of MLCCT with prior models on MELD\nModel Modality Acc F1 score\nAGHMN [62] Text 60.3 58.1\nKET [63] Text 60.6 58.2\nConfidence-estimator Ensemble Model [64] Multimodal 61.2 59.5\nCon-GCN [65] Audio 49.32 47.4\nText 45.61 42.2\nMultimodal 61.7 59.4\nMLCCT Audio 48.8 45.34\nText 61.7 58.9\nMultimodal 63.2 62.4\n4.2.3 Experiments on CMU-MOSEI\nTab. 6presents the comparison of the proposed model and prior works on CMU-MOSEI dataset\nin terms of mean accuracy and F1 score. The experimental results again demonstrate the fact that\nspeech emotion recognition based on multimodal data tend to have better accuracy than those relying\non unimodal data alone. Moreover, our proposed multi-level model also proves to be a better choice,\nachieving an accuracy of 51.2% and an F1 score of 82.0.\n4214 CMC, 2023, vol.74, no.2\nFigure 5:The recognition rate confusion matrix on MELD\nTable 6: Comparison of MLCCT with prior models on CMU-MOSEI\nModel Modality Acc F1 score\nRTN [66] Multimodal 48.12 62.12\nCIM [67] Audio 30.25 38.72\nText 42.6 59.22\nMultimodal 49.12 66.6\nGMU + Attention [68] Audio 31.75 43.65\nText 43.58 60.82\nMultimodal 50.31 72.21\nMLCCT Audio 35.6 46.56\nText 48.2 62.78\nMultimodal 51.2 82.0\nAs in the previous two subsections, we also provide a more specific analysis of the results for\neach category on CMU-MOSEI shown inFig. 6. It can be further seen that our model has certain\ngeneralization ability and can still cope well with the overall accuracy when facing the problem of\nunbalanced data across emotion labels. In particular, MLCCT achieved the best accuracy of 79.82%\nin Happy, and the worst performance in Fear.\nCMC, 2023, vol.74, no.2 4215\nFigure 6:The recognition rate confusion matrix on CMU-MOSEI\n4.3 Ablation Study\n4.3.1 Ablation Study on Multi-Level Interaction\nTo demonstrate the effectiveness of the progressive framework, we conduct an ablation study on\nmulti-level interaction. Specifically, we design the following strategies for comparison, as shown in\nTab. 7.\nTable 7: Ablation study on multi-level interaction on CMU-MOSEI\nSSE Low-level interaction High-level interaction Acc F1 score\nA √√ 28.12 30.18\nB √√ 42.71 72.51\nC √√ 45.25 75.62\nD √√ √ 51.20 82.03\nThe experimental results inTab. 7show that the model containing only single-level interactions is\nlimited in terms of recognition results. In particular, the accuracy of Strategy B, which only employs a\nBi-LSTM with circulant interaction mechanism for low-level features, dropped significantly to 28.12%.\nWithout the low-level interactions, the model in Strategy C also encountered a bottleneck, achieving\nan accuracy of 42.71% and an F1 score of 72.51. In addition, SSE also proved to be effective, which\nhelped to improve the model’s accuracy rate by about 6%.\n4216 CMC, 2023, vol.74, no.2\n4.3.2 Ablation Study on Attention Blocks\nA total of two types of attention mechanisms are included in the structure of this network, which\nare cross-modal attention mechanism and self-attention mechanism. To further explore the influence\nof these two attention mechanisms on the final recognition accuracy, we select the CMU-MOSEI\ndataset with the largest amount of data for the ablation experiments, as shown inTab. 8.\nTable 8: Ablation study on attention blocks on CMU-MOSEI\nCross-modal\nattention block\nSelf-attention\nblock\nCross-model\nhead\nSelf-attention head ACC-6 ACC-2 F1 score\n1 1 1 1 49.7% 79.6% 80.2\n1 1 2 4 50.1% 80.2% 80.6\n2 4 1 1 50.9% 82.4% 81.5\n2 4 2 4 51.2% 82.9% 82.0\n2 4 4 8 51.3% 83.6% 82.7\n4 8 2 4 51.4% 83.5% 82.5\n4 8 4 8 51.2% 82.1% 81.7\nIt is known that as the number of attention blocks increases, the accuracy of the model improves\nmore slowly and the training difficulty and recognition time also increase. Therefore, combining\nvarious factors, we finally chose two cross-modal attention blocks with four heads and two self-\nattention blocks with four heads as hyperparametric settings.\n5 Conclusion\nIn this paper, we propose a multi-level circulant cross-modal Transformer (MLCCT) for multi-\nmodal speech emotion recognition. Different from prior works, MLCCT adopts a progressive frame-\nwork, which combines local and global features for accurate predictions. To the best of our knowledge,\nthis is the first time that a Transformer-based progressive framework is used in multimodal speech\nemotion recognition, which will serve as an inspiration for future research. Specifically, MLCCT better\ncaptures inter-modal relationships through two modal interaction processes. The first one employs a\nbidirectional Long Short-term Memory (Bi-LSTM) with circulant interaction mechanism for low-\nlevel features, while a two-stream residual cross-modal Transformer block is applied when high-level\nfeatures are involved. Finally, self-attention blocks are used for fusion. Comprehensive experimental\nresults on three benchmark datasets including IEMOCAP, MELD and CMU-MOSEI show that the\nproposed MLCCT has achieved competitive results.\nAcknowledgement: Peizhu Gong: Conceptualization, Methodology, Writing-original draft, Software.\nJin Liu: Supervision, Project administration, Formal analysis, Data curation. Huihua He: Supervision,\nProject administration, Data curation. Bing Han: Visualization, Software, Resources. Zhongdai Wu:\nResources, Validation. Y . Ken Wang: Data curation.\nFunding Statement: This work was supported by the National Natural Science Foundation of\nChina (No. 61872231), the National Key Research and Development Program of China (No.\n2021YFC2801000), and the Major Research plan of the National Social Science Foundation of China\n(No. 2000&ZD130).\nCMC, 2023, vol.74, no.2 4217\nConflicts of Interest:The authors declare that they have no conflicts of interest to report regarding the\npresent study.\nReferences\n[1] S. M. S. A. Abdullah, S. Y . A. Ameen and S. Zeebaree, “Multimodal emotion recognition using deep\nlearning,” Journal of Applied Science and Technology Trends, vol. 2, no. 2, pp. 52–58, 2021.\n[2] M. Ayadi, M. S. Kamel and F . Karray, “Survey on speech emotion recognition: Features, classification\nschemes, and databases,”Pattern Recognition, vol. 44, no. 3, pp. 572–587, 2011.\n[3] H. Ranganathan, S. Chakraborty and S. Panchanathan, “Multimodal emotion recognition using deep\nlearning architectures,” inIEEE Winter Conf. on Applications of Computer Vision (WACV), Lake Placid,\nNY , USA, pp. 1–9, 2016.\n[4] S. Y oon, S. Byun and K. Jung, “Multimodal speech emotion recognition using audio and text,” inIEEE\nSpoken Language Technology Workshop (SLT), Greece, pp. 112–118, 2018.\n[5] S. Y oon, S. Byun, S. Dey and K. Jung, “Speech emotion recognition using multi-hop attention mechanism,”\nin IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), Brighton, UK, pp. 2822–2826,\n2019.\n[6] P . Gong, J. Liu, Y . Yang and H. He, “Towards knowledge enhanced language model for machine reading\ncomprehension,” IEEE Access, vol. 8, pp. 224837–224851, 2020.\n[7] X. Jiang, F . Yu, T. Song and V . C. Leung, “Resource allocation of video streaming over vehicular networks:\nA survey, some research issues and challenges,”IEEE Transactions on Intelligent Transportation Systems,\nvol. 12, no. 37, pp. 1–30, 2021.\n[8] S. Siriwardhana, T. Kaluarachchi, M. Billinghurst and S. Nanayakkara, “Multimodal emotion recognition\nwith transformer-based self-supervised feature fusion,”IEEE Access, vol. 8, pp. 176274–176285, 2020.\n[9] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhaiet al.,“An image is worth 16×16 words:\nTransformers for image recognition at scale,” arXiv preprint arXiv:2010.11929, 2020.\n[10] N. Sato and Y . Obuchi, “Emotion recognition using Mel-frequency cepstral coefficients,”Information and\nMedia Technologies, vol. 2, no. 3, pp. 835–848, 2007.\n[11] N. Alsaaran and M. Alrabiah, “Classical arabic named entity recognition using variant deep neural network\narchitectures and BERT,”IEEE Access, vol. 9, pp. 91537–91547, 2021.\n[12] X. Zhai, A. Oliver, A. Kolesnikov and L. Beyer, “S4 l: Self-supervised semi-supervised learning,” inProc.\nof the IEEE/CVF Int. Conf. on Computer Vision (ICCV), Seoul, Korea, pp. 1476–1485, 2019.\n[13] J. Xia, Y . Lu, L. Tan and P . Jiang, “Intelligent fusion of infrared and visible image data based on\nconvolutional sparse representation and improved pulse-coupled neural network,”Computers, Materials\n& Continua, vol. 67, no. 1, pp. 613–624, 2021.\n[14] M. H. Changrampadi, A. Shahina, M. B. Narayanan and A. N. Khan, “End-to-end speech recognition of\ntamil language,”Intelligent Automation & Soft Computing, vol. 32, no. 2, pp. 1309–1323, 2022.\n[15] R. Chen, L. Pan, C. Li, Y . Zhou, A. Chenet al.,“An improved deep fusion CNN for image recognition,”\nComputers, Materials & Continua, vol. 65, no. 2, pp. 1691–1706, 2020.\n[16] J. Devlin, M. Chang, K. Lee and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for\nlanguage understanding,” arXiv preprint arXiv: 1810.04805, 2018.\n[17] K. He, X. Chen, S. Xie, Y . Li, P . Dollaret al.,“Masked autoencoders are scalable vision learners,” arXiv\npreprint arXiv: 2111.06377, 2021.\n[18] A. Liu, S. Li, and H. Lee, “Tera: Self-supervised learning of transformer encoder representation for speech,”\nIEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 2351–2366, 2021.\n[19] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshiet al.,“Roberta: A robustly optimized bert pretraining approach,”\narXiv preprint arXiv: 1907.11692, 2019.\n[20] C. Busso, M. Bulut, C. Lee, A. Kazemzadeh, E. Moweret al.,“IEMOCAP: Interactive emotional dyadic\nmotion capture database,”Language Resources and Evaluation, vol. 42, no. 4, pp. 335–359, 2008.\n4218 CMC, 2023, vol.74, no.2\n[21] S. Poria, N. Majumder, R. Mihalcea and E. Hovy, “Emotion recognition in conversation: Research\nchallenges, datasets, and recent advances,”IEEE Access, vol. 7, pp. 100943–100953, 2019.\n[22] A. B. Zadeh, P . P . Liang, S. Poria, E. Cambria, L. Morencyet al.,“Multimodal language analysis in the\nwild: CMU-MOSEI dataset and interpretable dynamic fusion graph,” inProc. of the 56th Annual Meeting\nof the Association for Computational Linguistics, Melbourne, Australia, vol. 1, pp. 2236–2246, 2018.\n[23] S. Arifin and P . Cheung, “Affective level video segmentation by utilizing the pleasure-arousal-dominance\ninformation,” IEEE Transactions on Multimedia, vol. 10, no. 7, pp. 1325–1341, 2008.\n[24] M. Bhargava and T. Polzehl, “Improving automatic emotion recognition from speech using rhythm and\ntemporal feature,” arXiv preprint arXiv: 1303.1761, 2013.\n[25] H. K. Palo, M. Chandra and M. N. Mohanty, “Recognition of human speech emotion using variants of\nMel-frequency cepstral coefficients,”Advances in Systems, Control and Automation, vol. 442, pp. 491–498,\n2018.\n[26] S. Chang and J. Liu, “Multi-lane capsule network for classifying images with complex background,”IEEE\nAccess, vol. 8, pp. 79876–79886, 2020.\n[27] D. Zeng, K. Liu, S. Lai, G. Zhou and J. Zhao, “Relation classification via convolutional deep neural\nnetwork,” inProc. of COLING 2014, the 25th Int. Conf. on Computational Linguistics: Technical Papers,\nDublin, Ireland, pp. 2335–2344, 2014.\n[28] W . Sun, G. Z. Dai, X. R. Zhang, X. Z. He, X. Chen, “TBE-Net: A three-branch embedding network with\npart-aware ability and feature complementary learning for vehicle re-identification,”IEEE Transactions on\nIntelligent Transportation Systems, vol. 1, pp. 1–13, 2021.\n[29] W . Sun, L. Dai, X. R. Zhang, P . S. Chang and X. Z. He, “RSOD: Real-time small object detection algorithm\nin UA V-based traffic monitoring,”Applied Intelligence, vol. 8, pp. 1–16, 2021.\n[30] W . Liu, J. Wang, L. Chen and B. Chen, “Prediction of protein essentiality by the improved particle swarm\noptimization,” Soft Computing, vol. 22, no. 20, pp. 6657–6669, 2018.\n[31] J. Liu, Y . Yang and H. He, “Multi-level semantic representation enhancement network for relationship\nextraction,” Neurocomputing, vol. 403, no. 5, pp. 282–293, 2020.\n[32] J. Liu, Y . Yang, S. Lv, J. Wang and H. Chen, “Attention-based BiGRU-CNN for Chinese question\nclassification,” Journal of Ambient Intelligence and Humanized Computing, vol. 1, pp. 1–12, 2019.\n[33] P . Y enigalla, A. Kumar, S. Tripathi, C. Singh, S. Karet al.,“Speech emotion recognition using spectrogram\nand phoneme embedding,” inProc. of Interspeech 2018, Hyderabad, India, pp. 3688–3692, 2018.\n[34] J. Zhao, M. Xia and L. Chen, “Speech emotion recognition using deep 1D & 2D CNN LSTM networks,”\nBiomedical Signal Processing and Control, vol. 47, pp. 312–323, 2019.\n[35] Y . H. Tsai, S. Bai, P . Liang, J. Z. Kolter, L. Morencyet al., “Multimodal transformer for unaligned\nmultimodal language sequences,” inProc. of the 57th Annual Meeting of the Association for Computational\nLinguistics, Firenze, pp. 6558–6569, 2019.\n[36] J. Delbrouck, N. Tits, M. Brousmiche and S. Dupont, “A Transformer-based joint-encoding for emotion\nrecognition and sentiment analysis,” arXiv preprint arXiv: 2006.15955, 2020.\n[37] A. Radford, J. Wu, R. Child, D. Luan, D. Amodeiet al.,“Language models are unsupervised multitask\nlearners,” OpenAI Blog, vol. 1, no. 8, pp. 9, 2019.\n[38] K. Ethayarajh, “How contextual are contextualized word representations? comparing the geometry of\nBERT, ELMo, and GPT-2 embeddings,” arXiv preprint arXiv: 1909.00512, 2019.\n[39] A. Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals and A. Graves, “Conditional image generation with\npixel-CNN decoders,” inAdvances in Neural Information Processing Systems, Spain, vol. 29, pp. 4790–\n4798, 2016.\n[40] A. Oord and O. Vinyals, “Neural discrete representation learning,” inAdvances in Neural Information\nProcessing Systems, USA, vol. 16, pp. 6306–6315, 2017.\n[41] R. Hjelm, A. Fedorov, S. Marchildon, K. Grewal, P . Bachmanet al.,“Learning deep representations by\nmutual information estimation and maximization,” arXiv preprint arXiv:1808.06670, 2018.\nCMC, 2023, vol.74, no.2 4219\n[42] K. He, H. Fan, Y . Wu, S. Xie and R. Girshick, “Momentum contrast for unsupervised visual representation\nlearning,” inProc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition, Seattle, WA, USA,\npp. 9729–9738, 2020.\n[43] T. Chen, S. Kornblith, M. Norouzi and G. Hinton, “A simple framework for contrastive learning of visual\nrepresentations,” inInt. Conf. on Machine Learning (PMLR), London, UK, pp. 1597–1607, 2020.\n[44] Z. Wu, S. Wang, J. Gu, M. Khabsa, F . Sunet al.,“Clear: Contrastive learning for sentence representation,”\narXiv preprint arXiv: 2012.15466, 2020.\n[45] F . Carlsson, A. C. Gyllensten, E. Gogoulou, E. Y . Hellqvist and M. Sahlgren, “Semantic re-tuning with\ncontrastive tension,” inInt. Conf. on Learning Representations (ICLR), Addis Ababa, Ethiopia, pp. 1–21,\n2020.\n[46] I. Goodfellow, J. Abadie, M. Mirza, B. Xu, D. Farleyet al.,“Generative adversarial nets,”Advances in\nNeural Information Processing Systems (NIPS), vol. 27, pp. 1–10, 2014.\n[47] V . Chernykh and P . Prikhodko, “Emotion recognition from speech with recurrent neural networks,” arXiv\npreprint arXiv: 1701.08071, 2017.\n[48] K. Han, D. Yu and I. Tashev, “Speech emotion recognition using deep neural network and extreme learning\nmachine,” inInterspeech, Singapore, pp. 223–227, 2014.\n[49] C. Lee, E. Mower, C. Busso, S. Lee, and S. Narayanan, “Emotion recognition using a hierarchical binary\ndecision tree approach, ”Speech Commun, vol. 53, no. 9, pp. 1162–1171, 2011.\n[50] J. Lee and I. Tashev, “High-level feature representation using recurrent neural network for speech emotion\nrecognition,” inInterspeech, pp. 1–4, 2015.\n[51] M. Neumann and N. T. Vu, “Attentive convolutional neural network-based speech emotion recognition: A\nstudy on the impact of input features, signal length, and acted speech,” arXiv preprint arXiv: 1706.00612,\n2017.\n[52] S. Mirsamadi, E. Barsoum and C. Zhang, “Automatic speech emotion recognition using recurrent neural\nnetworks with local attention,” inIEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP),\nNew OLEANS, USA, pp. 2227–2231, 2017.\n[53] C. Etienne, G. Fidanza, A. Petrovskii, L. Devillers and B. Schmauch, “CNN+ lSTM architecture for\nspeech emotion recognition with data augmentation,” arXiv preprint arXiv: 1802.05630, 2018.\n[54] E. Tzinis and A. Potamianos, “Segment-based speech emotion recognition using recurrent neural net-\nworks,” inSeventh Int. Conf. on Affective Computing and Intelligent Interaction (ACII), San Antonio, Texas,\npp. 190–195, 2017.\n[55] C. Huang and S. Narayanan. “Attention assisted discovery of sub-utterance structure in speech emotion\nrecognition,” inInterspeech, Singapore, pp. 1387–1391, 2016.\n[56] Y . Zhang, J. Du, Z. Wang, J. Zhang and Y . Tu, “Attention based fully convolutional network for speech\nemotion recognition,” inAsia-Pacific Signal and Information Processing Association Annual Summit and\nConf. (APSIPA ASC), Honolulu, Hawaii, USA, pp. 1771–1775, 2018.\n[57] G. Ramet, P . N. Garner, M. Baeriswyl and A. Lazaridis, “Context-aware attention mechanism for speech\nemotion recognition,” inIEEE Spoken Language Technology Workshop (SLT), Greece, pp. 126–131, 2018.\n[58] Q. Jin, C. Li, S. Chen and H. Wu, “Speech emotion recognition with acoustic and lexical features,” inIEEE\nInt. Conf. on Acoustics, Speech and Signal Processing (ICASSP), Queensland, Australia, pp. 4749–4753,\n2015.\n[59] S. Tripathi, S. Tripathi and H. Beigi, “Multi-modal emotion recognition on IEMOCAP dataset using deep\nlearning,” arXiv preprint arXiv: 1804.05788, 2018.\n[60] Y . Wang, Y . Shen, Z. Liu, P . P . Liang, A. Zadehet al., “Words can shift: Dynamically adjusting word\nrepresentations using nonverbal behaviors,” inProc. of the AAAI Conf. on Artificial Intelligence, Honolulu,\nHawaii, USA, vol. 33, no. 1, pp. 7216–7223, 2019.\n[61] H. Pham, P . P . Liang, T. Manzini, L. P . Morency and B. Poczos, “Found in translation: Learning robust\njoint representations by cyclic translations between modalities,” inProc. of the AAAI Conf. on Artificial\nIntelligence, Honolulu, Hawaii, USA, vol. 33, no. 1, pp. 6892–6899, 2019.\n4220 CMC, 2023, vol.74, no.2\n[62] W . Jiao, M. Lyu and I. King, “Real-time emotion recognition via attention gated hierarchical memory\nnetwork,” inProc. of the AAAI Conf. on Artificial Intelligence, New Y ork, USA, vol. 34, no. 5, pp. 8002–\n8009, 2020.\n[63] P . Zhong, D. Wang and C. Miao, “Knowledge-enriched transformer for emotion detection in textual\nconversations,” arXiv preprint arXiv: 1909.10681, 2019.\n[64] U. Nadeem, M. Bennamoun, F . Sohel and R. Togneri, “Learning-based confidence estimation for multi-\nmodal classifier fusion,” inInt. Conf. on Neural Information Processing, Canada, pp. 299–312, 2019.\n[65] D. Zhang, L. Wu, C. Sun, S. Li, Q. Zhuet al.,“Modeling both context-and speaker-sensitive dependence\nfor emotion detection in multi-speaker conversations,” inProc. of the Twenty-Eighth Int. Joint Conf. on\nArtificial Intelligence (IJCAI), Macao, China, pp. 5415–5421, 2019.\n[66] S. Sahay, S. H. Kumar, R. Xia, J. Huang and L. Nachman, “Multimodal relational tensor network for\nsentiment and emotion classification,” arXiv preprint arXiv: 1806.02923, 2018.\n[67] M. S. Akhtar, D. S. Chauhan, D. Ghosal, S. Poria, A. Ekbalet al.,“Multi-task learning for multi-modal\nemotion recognition and sentiment analysis,” arXiv preprint arXiv:1905.05812, 2019.\n[68] S. Sangwan, D. S. Chauhan, M. Akhtar, A. Ekbal, P . Bhattacharyyaet al.,“Multi-task gated contextual\ncross-modal attention framework for sentiment and emotion analysis,” inInt. Conf. on Neural Information\nProcessing, Canada, pp. 662–669, 2019.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7843982577323914
    },
    {
      "name": "Speech recognition",
      "score": 0.6195269823074341
    },
    {
      "name": "Spectrogram",
      "score": 0.6148342490196228
    },
    {
      "name": "Feature extraction",
      "score": 0.5770468711853027
    },
    {
      "name": "Mel-frequency cepstrum",
      "score": 0.529923141002655
    },
    {
      "name": "Transformer",
      "score": 0.47097137570381165
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4567706286907196
    },
    {
      "name": "Feature learning",
      "score": 0.43510088324546814
    },
    {
      "name": "Embedding",
      "score": 0.4270683825016022
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4048311710357666
    },
    {
      "name": "Engineering",
      "score": 0.08855664730072021
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I96733725",
      "name": "Shanghai Maritime University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210097049",
      "name": "Shanghai Ship and Shipping Research Institute",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210086901",
      "name": "University of Pittsburgh at Bradford",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I21945476",
      "name": "Shanghai Normal University",
      "country": "CN"
    }
  ],
  "cited_by": 13
}