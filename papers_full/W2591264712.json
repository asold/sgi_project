{
  "title": "Character-Word LSTM Language Models",
  "url": "https://openalex.org/W2591264712",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A2402126497",
      "name": "Lyan Verwimp",
      "affiliations": [
        "KU Leuven"
      ]
    },
    {
      "id": "https://openalex.org/A39710514",
      "name": "Joris Pelemans",
      "affiliations": [
        "KU Leuven"
      ]
    },
    {
      "id": "https://openalex.org/A75623772",
      "name": "Hugo Van hamme",
      "affiliations": [
        "KU Leuven"
      ]
    },
    {
      "id": "https://openalex.org/A2068045828",
      "name": "Patrick Wambacq",
      "affiliations": [
        "KU Leuven"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W29952999",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2394700483",
    "https://openalex.org/W2101609803",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2176796957",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2321916036",
    "https://openalex.org/W189630165",
    "https://openalex.org/W2417736714",
    "https://openalex.org/W2399344342",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963251942",
    "https://openalex.org/W2168664213",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W32493531",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W1860935423",
    "https://openalex.org/W2955584699",
    "https://openalex.org/W2311921240",
    "https://openalex.org/W2962732637",
    "https://openalex.org/W2019096529",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W2214043111",
    "https://openalex.org/W2963582782",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W2950621961",
    "https://openalex.org/W581956982"
  ],
  "abstract": "© 2017 Association for Computational Linguistics. We present a Character-Word Long Short- Term Memory Language Model which both reduces the perplexity with respect to a baseline word-level language model and reduces the number of parameters of the model. Character information can reveal structural (dis)similarities between words and can even be used when a word is out-of-vocabulary, thus improving the modeling of infrequent and unknownwords. By concatenating word and character embeddings, we achieve up to 2.77% relative improvement on English compared to a baseline model with a similar amount of parameters and 4.57% on Dutch. Moreover, we also outperform baseline word-level models with a larger number of parameters.",
  "full_text": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 417–427,\nValencia, Spain, April 3-7, 2017.c⃝2017 Association for Computational Linguistics\nCharacter-Word LSTM Language Models\nLyan Verwimp Joris Pelemans Hugo Van hamme Patrick Wambacq\nESAT – PSI, KU Leuven\nKasteelpark Arenberg 10, 3001 Heverlee, Belgium\nfirstname.lastname@esat.kuleuven.be\nAbstract\nWe present a Character-Word Long Short-\nTerm Memory Language Model which\nboth reduces the perplexity with respect\nto a baseline word-level language model\nand reduces the number of parameters\nof the model. Character information can\nreveal structural (dis)similarities between\nwords and can even be used when a word\nis out-of-vocabulary, thus improving the\nmodeling of infrequent and unknown words.\nBy concatenating word and character\nembeddings, we achieve up to 2.77%\nrelative improvement on English compared\nto a baseline model with a similar amount of\nparameters and 4.57% on Dutch. Moreover,\nwe also outperform baseline word-level\nmodels with a larger number of parameters.\n1 Introduction\nLanguage models (LMs) play a crucial role in\nmany speech and language processing tasks, among\nothers speech recognition, machine translation and\noptical character recognition. The current state of\nthe art are recurrent neural network (RNN) based\nLMs (Mikolov et al., 2010), and more speciﬁcally\nlong short-term memory models (LSTM) (Hochre-\niter and Schmidhuber, 1997) LMs (Sundermeyer\net al., 2012) and their variants (e.g. gated recurrent\nunits (GRU) (Cho et al., 2014)). LSTMs and GRUs\nare usually very similar in performance, with GRU\nmodels often even outperforming LSTM models\ndespite the fact that they have less parameters to\ntrain. However, Jozefowicz et al. (2015) recently\nshowed that for the task of language modeling\nLSTMs work better than GRUs, therefore we focus\non LSTM-based LMs.\nIn this work, we address some of the drawbacks\nof NN based LMs (and many other types of LMs).\nA ﬁrst drawback is the fact that the parameters for\ninfrequent words are typically less accurate because\nthe network requires a lot of training examples to\noptimize the parameters. The second and most\nimportant drawback addressed is the fact that the\nmodel does not make use of the internal structure\nof the words, given that they are encoded as one-hot\nvectors. For example, ‘felicity’ (great happiness) is\na relatively infrequent word (its frequency is much\nlower compared to the frequency of ‘happiness’\naccording to Google Ngram Viewer (Michel et al.,\n2011)) and will probably be an out-of-vocabulary\n(OOV) word in many applications, but since there\nare many nouns also ending on ‘ity’ (ability, com-\nplexity, creativity . . . ), knowledge of the surface\nform of the word will help in determining that ‘felic-\nity’ is a noun. Hence, subword information can play\nan important role in improving the representations\nfor infrequent words and even OOV words.\nIn our character-word (CW) LSTM LM, we\nconcatenate character and word embeddings and\nfeed the resulting character-word embedding to the\nLSTM. Hence, we provide the LSTM with infor-\nmation about the structure of the word. By concate-\nnating the embeddings, the individual characters\n(as opposed to e.g. a bag-of-characters approach)\nare preserved and the order of the characters is im-\nplicitly modeled. Moreover, since we keep the total\nembedding size constant, the ‘word’ embedding\nshrinks in size and is partly replaced by character\nembeddings (with a much smaller vocabulary and\nhence a much smaller embedding matrix), which\ndecreases the number of parameters of the model.\nWe investigate the inﬂuence of the number of\ncharacters added, the size of the character embed-\ndings, weight sharing for the characters and the size\nof the (hidden layer of the) model. Given that com-\nmon or similar character sequences do not always\noccur at the beginning of words (e.g. ‘overﬁtting’\n– ‘underﬁtting’), we also examine adding the charac-\n417\nters in forward order, backward order or both orders.\nWe test our CW LMs on both English and\nDutch. Since Dutch has a richer morphology\nthan English due to among others its productive\ncompounding (see e.g. (R´eveil, 2012)), we expect\nthat it should beneﬁt more from a LM augmented\nwith formal/morphological information.\nThe contributions of this paper are the following:\n1. We present a method to combine word\nand subword information in an LSTM LM:\nconcatenating word and character embeddings.\nAs far as we know, this method has not been\ninvestigated before.\n2. By decreasing the size of the word-level em-\nbedding (and hence the huge word embedding\nmatrix), we effectively reduce the number of\nparameters in the model (see section 3.3).\n3. We ﬁnd that the CW model both outperforms\nword-level LMs with the same number of\nhidden units (and hence a larger number of\nparameters) and word-level LMs with the\nsame number of parameters. These ﬁndings\nare conﬁrmed for English and Dutch, for a\nsmall model size and a large model size. The\nsize of the character embeddings should be\nproportional to the total size of the embedding\n(the concatenation of characters should not\nexceed the size of the word-level embedding),\nand using characters in the backward order\nimproves the perplexity even more (see\nsections 3.1, 4.3 and 4.4).\n4. The LM improves the modeling of OOV\nwords by exploiting their surface form (see\nsection 4.7).\nThe remainder of this paper is structured as\nfollows: ﬁrst, we discuss related work (section 2);\nthen the CW LSTM LM is described (section 3) and\ntested (section 4). Finally, we give an overview of\nthe results and an outlook to future work (section 5).\n2 Related work\nOther work that investigates the use of character in-\nformation in RNN LMs either completely replaces\nthe word-level representation by a character-level\none or combines word and character information.\nMuch research has also been done on modeling\nother types of subword information (e.g. mor-\nphemes, syllables), but in this discussion, we limit\nourselves to characters as subword information.\nResearch on replacing the word embeddings\nentirely has been done for neural machine transla-\ntion (NMT) by Ling et al. (2015) and Costa-juss`a\nand Fonollosa (2016), who replace word-level\nembeddings with character-level embeddings.\nChung et al. (2016) use a subword-level encoder and\na character-level decoder for NMT. In dependency\nparsing, Ballesteros et al. (2015) achieve improve-\nments by generating character-level embeddings\nwith a bidirectional LSTM. Xie et al. (2016) work\non natural language correction and also use an\nencoder-decoder, but operate for both the encoder\nand the decoder on the character level.\nCharacter-level word representations can also\nbe generated with convolutional neural networks\n(CNNs), as Zhang et al. (2015) and Kim et al. (2016)\nhave proven for text classiﬁcation and language\nmodeling respectively. Kim et al. (2016) achieve\nstate-of-the-art results in language modeling for\nseveral languages by combining a character-level\nCNN with highway (Srivastava et al., 2015) and\nLSTM layers. However, the major improvement\nis achieved by adding the highway layers: for a\nsmall model size, the purely character-level model\nwithout highway layers does not perform better\nthan the word-level model (perplexity of 100.3\ncompared to 97.6), even though the character model\nhas two hidden layers of 300 LSTM units each and\nis compared to a word model of two hidden layers\nof only 200 units (in order to keep the number\nof parameters similar). For a model of larger\nsize, the character-level LM improves the word\nbaseline (84.6 compared to 85.4), but the largest\nimprovement is achieved by adding two highway\nlayers (78.9). Finally, Jozefowicz et al. (2016) also\ndescribe character embeddings generated by a CNN,\nbut they test on the 1B Word Benchmark, a data set\nof an entirely different scale than the one we use.\nOther authors combine the word and character\ninformation (as we do in this paper) rather than\ndoing away completely with word inputs. Chen et\nal. (2015) and Kang et al. (2011) work on models\ncombining words and Chinese characters to learn\nembeddings. Note however that Chinese characters\nmore closely match subwords or words than\nphonemes. Bojanowski et al. (2015) operate on the\ncharacter level but use knowledge about the context\nwords in two variants of character-level RNN LMs.\nDos Santos and Zadrozny (2014) join word and\ncharacter representations in a deep neural network\nfor part-of-speech tagging. Finally, Miyamoto and\n418\nCho (2016) describe a LM that is related to our\nmodel, although their character-level embedding\nis generated by a bidirectional LSTM and we do\nnot use a gate to determine how much of the word\nand how much of the character embedding is used.\nHowever, they only compare to a simple baseline\nmodel of 2 LSTM layers of each 200 hidden units\nwithout dropout, resulting in a higher baseline\nperplexity (as mentioned in section 4.3, our CW\nmodel also achieves larger improvements than\nreported in this paper with respect to that baseline).\nWe can conclude that in various NLP tasks, char-\nacters have recently been introduced in several dif-\nferent manners. However, the models investigated\nin related work are either not tested on a competitive\nbaseline (Miyamoto and Cho, 2016) or do not per-\nform better than our models (Kim et al., 2016). In\nthis paper, we introduce a new and straightforward\nmanner to incorporate characters in a LM that (as\nfar as we know) has not been investigated before.\n3 Character-Word LSTM LMs\nA word-level LSTM LM works as follows: a word\nencoded as a one-hot column vector wt (at time\nstep t) is fed to the input layer and multiplied with\nthe embedding matrix Ww, resulting in a word\nembedding et:\net =Ww ×wt (1)\nThe word embedding of the current wordet will\nbe the input for a series of non-linear operations in\nthe LSTM layer (we refer to (Zaremba et al., 2015)\nfor more details about the equations of the LSTM\ncell). In the output layer, probabilities for the next\nword are calculated based on a softmax function.\nIn our character-word LSTM LM, the only differ-\nence with the baseline LM is the computation of the\n‘word’ embedding, which is now the result of word\nand character input rather than word input only. We\nconcatenate the word embedding with embeddings\nof the characters occurring in that word:\ne⊤\nt =[(Ww ×wt)⊤(W1\nc ×c1\nt )⊤\n(W2\nc ×c2\nt )⊤...(Wn\nc ×cn\nt )⊤]\n(2)\nwhere c1\nt is the one-hot encoding of the ﬁrst charac-\nter added, W1\nc its embedding matrix andnthe total\nnumber of characters added to the model. The word\nwt and its characters c1\nt ,c2\nt ...cn\nt are each projected\nonto their own embeddings, and the concatenation\nwte.g. cat\nc1\ntc\nc2\nta\nc3\ntt\nembedding\nht□1\nLSTM\nht\n1\nFigure 1: Concatenating word and character\nembeddings in an LSTM LM.\nof the embeddings is the input for the LSTM layer.\nBy concatenating the embeddings, we implicitly\npreserve the order of the characters: the embedding\nfor e.g. the ﬁrst character of a word will always cor-\nrespond to the same portion of the input vector for\nthe LSTM (see ﬁgure 1). We also experimented with\nadding word and character embeddings (a method\nwhich does not preserve the order of the characters),\nbut that did not improve the perplexity of the LM.\nThe number of characters added (n) is ﬁxed. If\na word is longer than ncharacters, only the ﬁrst\n(or last, depending on the order in which they are\nadded) ncharacters are added. If the word is shorter\nthan n, it is padded with a special symbol. Because\nwe can still model the surface form of OOV words\nwith the help of their characters, this model reduces\nthe number of errors made immediately after OOV\nwords (see section 4.7).\n3.1 Order of the characters\nThe characters can be added in the order in which\nthey appear in the word (in the experiments this is\ncalled ‘forward order’), in the reversed order (‘back-\nward order’) or both (‘both orders’). In English and\nDutch (and many other languages), sufﬁxes can bear\nmeaningful relations (such as plurality and verb con-\njugation) and compounds typically have word-ﬁnal\nheads. Hence, putting more emphasis on the end of\na word might help to better model those properties.\n419\n3.2 Weight sharing\nNote that in equation 2 each position in the word\nis associated with different weights: the weights\nfor the ﬁrst characterc1\nt , W1\nc , are different from the\nweights for the character in the second position,W2\nc .\nGiven that the input ‘vocabulary’ for characters is\nalways the same, one could argue that the same set\nof weights Wc could be used for all positions in the\nword:\ne⊤\nt =[(Ww ×wt)⊤(Wc ×c1\nt )⊤\n(Wc ×c2\nt )⊤...(Wc ×cn\nt )⊤]\n(3)\nHowever, one could also argue in favor of the\nopposite case (no shared weights between the\ncharacters): for example, an ‘s’ at the end of a word\noften has a speciﬁc meaning, such as indicating\na third person singular verb form of the present\ntense (in English), which it does not have at other\npositions in the word. Both models with and\nwithout weight sharing are tested (see section 4.6).\n3.3 Number of parameters\nGiven that a portion of the total embedding is\nused for modeling the characters, the actual ‘word’\nembedding is smaller which reduces the number\nof parameters signiﬁcantly. In a normal word-level\nLSTM LM, the number of parameters in the\nembedding matrix is\nV×E (4)\nwith V the vocabulary size and E= Ew the total\nembedding size/word embedding size. In our CW\nmodel however, the number of parameters is\nV×(E−n×Ec)+n×(C×Ec) (5)\nwith nthe number of characters,Ec the size of the\ncharacter embedding andCthe size of the character\nvocabulary. SinceV is by far the dominant factor, re-\nducing the size of the purely word-level embedding\nvastly reduces the total number of parameters to\ntrain. If we share the character weights, that number\nbecomes even smaller:\nV×(E−n×Ec)+C×Ec (6)\n4 Experiments\n4.1 Setup\nAll LMs were trained and tested with TensorFlow\n(Abadi et al., 2015). We test the performance of\nthe CW architectures for a small model and a large\nmodel, with hyperparameters based on Zaremba et\nal. (2015) and Kim et al. (2016)). The small LSTM\nconsists of 2 layers of 200 hidden units and the large\nLSTM has 2 layers of 650 hidden units. The total\nsize of the embedding layer always equals the size\nof the hidden layer. During the ﬁrst 4/6 (small/large\nmodel) epochs, the learning rate is 1, after which\nwe apply an exponential decay:\nηi =αηi−1 (7)\nwhere ηi is the learning rate at epoch iand αthe\nlearning rate decay, which is set to 0.5 for the small\nLSTM and to 0.8 for the large LSTM. The smaller\nα, the faster the learning rate decreases. The to-\ntal number of epochs is ﬁxed to 13/39 (small/large\nmodel). During training, 25% of the neurons are\ndropped (Srivastava et al., 2014) for the small model\nand 50% for the large model. The weights are ran-\ndomly initialized to small values (between -0.1 and\n0.1 for the small model and between -0.05 and 0.05\nfor the large model) based on a uniform distribution.\nWe train on mini-batches of 20 with backpropaga-\ntion through time, where the network is unrolled for\n20 steps for the small LSTM and 35 for the large\nLSTM. The norm of the gradients is clipped at 5 for\nboth models.\nFor English, we test on the publicly available\nPenn Treebank (PTB) data set, which contains 900k\nword tokens for training, 70k word tokens as vali-\ndation set and 80k words as test set. This data set\nis small but widely used in related work (among\nothers Zaremba et al. (2015) and Kim et al. (2016)),\nenabling the comparison between different models.\nWe adopt the same pre-processing as used by previ-\nous work (Mikolov et al., 2010) to facilitate compar-\nison, which implies that the dataset contains only\nlowercase characters (the size of the character vocab-\nulary is 48). Unknown words are mapped to⟨unk⟩,\nbut since we do not have the original text, we cannot\nuse the characters of the unknown words for PTB.\nThe Dutch data set consists of components g, h, n\nand o of the Corpus of Spoken Dutch (CGN) (Oost-\ndijk, 2000), containing recordings of meetings,\ndebates, courses, lectures and read text. Approx-\nimately 80% was chosen as training set (1.4M word\ntokens), 10% as validation set (180k word tokens)\nand 10% as test set (190k word tokens). The size of\nthe Dutch data set is chosen to be similar to the size\nof the English data set. We also use the same vocab-\nulary size as used for Penn Treebank (10k), since\n420\nwe want to compare the performance on different\nlanguages and exclude any effect of the vocabulary\nsize. However, we do not convert all uppercase char-\nacters to lowercase (although the data is normalized\nsuch that sentence-initial words with a capital are\nconverted to lowercase if necessary) because the\nfact that a character is uppercase is meaningful in it-\nself. The character vocabulary size is 88 (Dutch also\nincludes more accented characters due to French\nloan words, e.g. ‘caf´e’). Hence, we do not only com-\npare two different languages but also models with\nonly lowercase characters and models with both\nupper- and lowercase characters. Moreover, since\nwe have the original text at our disposal (as opposed\nto PTB), we can use the characters of the unknown\nwords and still have a character-level representation.\n4.2 Baseline models\nIn our experiments, we compare the CW model\nwith two word-level baselines: one with the\nsame number of hidden units in the LSTM layers\n(thus containing more parameters) and one with\napproximately the same number of parameters as\nthe CW model (like Kim et al. (2016) do), because\nwe are interested in both reducing the number of\nparameters and improving the performance. For\nthe latter baseline, this implies that we change the\nnumber of hidden units from 200 to 175 for the\nsmall model and from 650 to 475 for the large,\nkeeping the other hyperparameters the same.\nThe number of parameters for those models is\nlarger than for all CW models except when only 1 or\n2 characters are added. The size difference between\nthe CW models and the smaller word-level models\nbecomes larger if more characters are added, if the\nsize of the characters embeddings is larger and if the\ncharacter weights are shared. The size of the embed-\nding matrix for a word-level LSTM of size 475 is\n10,000 ×475 = 475,000 (V is 10k in all our exper-\niments), whereas for a CW model with 10 character\nembeddings of size 25 it is of size 10,000×(650 -\n10 ×25) + 10×(48 ×25) = 412,000 (the size of the\ncharacter vocabulary for PTB is 48), following equa-\ntion 5. If the character weights are shared, the size of\nthe embedding matrix is only 401,200 (equation 6).\nThe baseline perplexities for the smaller word-\nlevel models are shown in table 1. In the remainder\nof this paper, ‘w x’ = means word embeddings\nof size xfor a word-level model and ‘cx’ means\ncharacter embeddings of sizexfor CW models.\nPerplexity\nCorpus Size Validation Test\nPTB\nsmall w200 100.7 96.86\nw175 102.62 98.82\nlarge w650 87.38 83.6\nw465 88.39 84.38\nCGN\nsmall w200 69.13 76\nw175 69.6 76.78\nlarge w650 63.36 70.69\nw475 63.88 70.88\nTable 1: Perplexities for the baseline models.\nBaselines w200 and w650 have the same number of\nhidden units as the CW models and baselines w175\nand w475 approximately have the same number of\nparameters as the CW models.\n4.3 English\nIn ﬁgure 2, the results for a small model trained on\nPenn Treebank are shown. Almost all CW models\noutperform the word-based baseline with the same\nnumber of parameters (2 LSTM layers of 175 units).\nOnly the CW models in which the concatenated\ncharacter embeddings take up the majority of\nthe total embedding (more than 7 characters of\nembedding size 15) perform worse. With respect\nto the word-level LM with more parameters, only\nsmall improvements are obtained. The smaller the\ncharacter embeddings, the better the performance of\nthe CW model. For example, for a total embedding\nsize of only 200, adding 8 character embeddings\nof size 15 results in an embedding consisting of\n120 units ‘character embedding’ and only 80 units\n‘word embedding’, which is not sufﬁcient. The\ntwo best performing models add 3 and 7 character\nembeddings of size 5, giving a perplexity of 100.12\nand 100.25 respectively, achieving a relative\nimprovement of 2.44%/2.31% w.r.t. the w175\nbaseline and 0.58%/0.45% w.r.t. the w200 baseline.\nFor those models, the ‘word embedding’ consists\nof 185 and 165 units respectively.\nWe test the performance of the CW architecture\non a large model too. In ﬁgure 3, the results for\ndifferent embedding sizes are shown. Just like we\nsaw for the small model, the size of the character\nembeddings should not be too large: for embed-\ndings of size 50 (‘c50’), the performance drops\nwhen a larger number of characters is added. The\nbest result is obtained by adding 8 characters with\nembeddings of size 25 (‘c25’): a perplexity of 85.97\n(2.74%/1.61% relative improvement with respect to\n421\n2 4 6 8 10\n100\n102\n104\n106\nnumber of characters\nperplexity\nSmall baseline w175\nSmall baseline w200\nSmall CW c5\nSmall CW c10\nSmall CW c15\nFigure 2: Validation perplexity results on PTB,\nsmall model. Different sizes for the character\nembeddings are tested (‘c5’, ‘c10’, ‘c15’).\nthe w475/w650 baseline). For embeddings of size\n10, adding more than 10 characters gives additional\nimprovements (see ﬁgure 4).\nWe also verify whether the order in which the\ncharacters are added is important (ﬁgure 4). The\nbest result is achieved by adding the ﬁrst 3 and\nthe last 3 characters to the model (‘both orders’),\ngiving a perplexity of 85.69, 3.05%/1.87% relative\nimprovement with respect to the w475/w650\nbaseline. However, adding more characters in both\norders causes a decrease in performance. When only\nadding the characters in the forward order or the\nbackward order, adding the characters in backward\norder seems to perform slightly better overall (best\nresult: adding 9 characters in the backward order\ngives a perplexity of 85.7 or 3.04%/1.92% improve-\nment with respect to the w475/w650 baseline).\nWe can conclude that the size of the character\nembeddings should be proportional to the total em-\nbedding size: the word-level embedding should be\nlarger than the concatenation of the character-level\nembeddings. Adding characters in the backward\norder is slightly better than adding them in the\nforward order, and the largest improvement is\nmade for the large LSTM LM. The test perplexities\nfor some of the best performing models (table 2)\nconﬁrm these ﬁndings.\n2 4 6 8 1085\n86\n87\n88\n89\nnumber of characters\nperplexity\nLarge baseline w475\nLarge baseline w650\nLarge CW c10\nLarge CW c25\nLarge CW c50\nFigure 3: Validation perplexity results on PTB,\nlarge model. Different sizes for the character\nembeddings are tested (‘c10’, ‘c25’, ‘c50’).\nIf we compare the test perplexities with two\nrelated models that incorporate characters, we see\nthat our models perform better. Kim et al. (2016)\ngenerate character-level embeddings with a\nconvolutional neural network and also report results\nfor both a small and a large model. Their small\ncharacter-level model has more hidden units than\nours (300 compared to 200), but it does not improve\nwith respect to the word-level baseline (since we do\nnot use highway layers, we only compare with the\nresults for models without highway layers). Their\nlarge model slightly improves their own baseline\nperplexity (85.4) by 0.94%. Compare with our\nresults: 2.64% perplexity reduction for the best\nsmall LSTM (c5 withn=3) and 2.77% for the best\nlarge LSTM (c10 with n= 3 + 3(b)). Miyamoto\nand Cho (2016) only report results for a small\nmodel that is trained without dropout, resulting\nin a baseline perplexity of 115.65. If we train our\nsmall model without dropout we get a comparable\nbaseline perplexity (116.33) and a character-word\nperplexity of 110.54 (compare to 109.05 reported\nby Miyamoto and Cho (2016)). It remains to be seen\nwhether their model performs equally well com-\npared to better baselines. Moreover, their hybrid\ncharacter-word model is more complex than ours\nbecause it uses a bidirectional LSTM to generate\n422\n2 4 6 8 10 12 1485\n86\n87\n88\n89\nnumber of characters\nperplexity\nLarge baseline w475\nLarge baseline w650\nLarge CW c10 forward order\nLarge CW c10 backward order\nLarge CW c10 both orders\nFigure 4: Validation perplexity results on PTB,\nlarge model. Several options for the order in which\nthe characters are added are investigated.\nthe character-level embedding (instead of a lookup\ntable) and a gate to determine the mixing weights\nbetween the character- and word-level embeddings.\n4.4 Dutch\nAs we explained in the introduction, we expect that\nusing information about the internal structure of\nthe word will help more for languages with a richer\nmorphology. Although Dutch is still an analytic\nlanguage (most grammatical relations are marked\nwith separate words or word order rather than mor-\nphemes), it has a richer morphology than English be-\ncause compounding is a productive and widely used\nprocess and because it has more lexical variation\ndue to inﬂection (e.g. verb conjugation, adjective in-\nﬂection). The results for the LSTM LMs trained on\nDutch seem to conﬁrm this hypothesis (see ﬁgure 5).\nThe CW model outperforms the baseline\nword-level LM both for the small model and the\nlarge model. The best result for the small model\nis obtained by adding 2 or 3 characters, giving a\nperplexity of 67.59 or 67.65 which equals a relative\nimprovement of 2.89%/2.23% (w175/w200) and\n2.80%/2.14% (w175/w200) respectively.\nFor the large model, we test several embedding\nsizes and orders for the characters. The best model\nis the one to which 6 characters in backward\norder are added, with a perplexity of 60.88 or\nSmall model Perplexity\nBaseline w175/w200 98.82/96.86\n(Kim et al., 2016) 100.3\n(Miyamoto and Cho, 2016) 109.05\nc5 with n=3 96.21\nc5 with n=7 96.35\nLarge model Perplexity\nBaseline w475/w650 84.38/83.6\n(Kim et al., 2016) 84.6\nc25 with n=8 82.69\nc10 with n=9(b) 82.68\nc10 with n=3+3( b) 82.04\nTable 2: Test perplexity results for the best\nmodels on PTB. Baseline perplexities are for sizes\nw175/w200 for a small model and w475/w650 for\na large model. n= number of characters added,\n(b) means backward order. Comparison with other\ncharacter-level LMs (Kim et al., 2016) (we only\ncompare to models without highway layers) and\ncharacter-word models (Miyamoto and Cho, 2016)\n(they do not use dropout and only report results for\na small model).\n4.70%/3.91% (w475/w650) relative improvement.\nJust like for PTB, an embedding size of 25 proves to\nbe the best compromise: if the characters are added\nin the normal order, 4 characters with embeddings\nof size 25 is the best model (perplexity 61.47 or\n3.77%/2.98% (w475/w650) relative improvement).\nThese results are conﬁrmed for the test set\n(table 3). The best small model has a perplexity\nof 75.04 which is 2.27% compared to the baseline\nand the best large model has a perplexity of 67.64,\na relative improvement of 4.57%. The larger\nimprovement for Dutch might be due to the fact that\nit has a richer morphology and/or the fact that we\ncan use the surface form of the OOV words for the\nDutch data set (see sections 4.1 and 4.7).\n4.5 Random CW models\nIn order to investigate whether the improvements\nof the CW models are not caused by the fact that\nthe characters add some sort of noise to the input,\nwe experiment with adding real noise – random\n‘character’ information – rather than the real char-\nacters. Both the number of characters (the length of\nthe random ‘word’) and the ‘characters’ themselves\nare generated based on a uniform distribution. In\ntable 4, the relative change in perplexity, averaged\nover models to which 1 to 10 characters are added,\n423\n2 4 6 8 1060\n62\n64\n66\n68\n70\nnumber of characters\nperplexity\nSmall baseline w175\nSmall baseline w200\nSmall CW c10\nLarge baseline w475\nLarge baseline w650\nLarge CW c10\nLarge CW c25\nLarge CW c50\nLarge CW c25 backward order\nFigure 5: Validation perplexity results on CGN.\nSeveral options for the size and order of the\ncharacter embeddings are investigated.\nSmall model Perplexity\nBaseline w175/w200 76.78/76\nc10 with n=2 75.23\nc10 with n=3 75.04\nLarge model Perplexity\nBaseline w475/w650 70.88/70.69\nc25 with n=4 68.79\nc25 with n=6(b) 67.64\nTable 3: Test perplexity results for the best models\non CGN. Baseline perplexities are for sizes\nw175/w200 for a small model and w475/w650 for\na large model. n= number of characters added,(b)\nmeans backward order.\nwith respect to the baseline word-level LM and the\nCW model with real characters is shown.\nFor English, adding random information had a\nnegative impact on the performance with respect\nto both the baseline and the CW model. For Dutch\non the other hand, adding some random noise to the\nword-level model gave small improvements. How-\never, the random models perform much worse than\nthe CW models. We can conclude that the characters\nprovide meaningful information to the LM.\nRelative change in\nvalid perplexity w.r.t.\nBaseline Char-Word\nPTB small c5 0.34 (0.30) 0.54 (0.46)\nlarge c15 0.00 (0.29) 0.53 (0.49)\nCGN small c10 - 0.18 (0.53) 1.79 (0.47)\nlarge c10 - 0.15 (0.26) 1.52 (1.24)\nTable 4: Relative change in validation perplexity\nfor models to which random information is\nadded, w.r.t. word-level and CW models. The\nimprovements are averaged over the results for\nadding 1 to 10 characters/random information, the\nnumbers between brackets are standard deviations.\nNegative numbers indicate a decrease in perplexity.\n4.6 Sharing weights\nWe repeat certain experiments with the CW models,\nbut with embedding matrices that are shared\nacross all character positions (see section 3.2).\nNote that sharing the weights does not imply\nthat the position information is lost, because for\nexample the ﬁrst portion of the character-level\nembedding always corresponds to the character\non the ﬁrst position. Sharing the weights ensures\nthat a character is always mapped onto the same\nembedding, regardless of the position of that\ncharacter in the word, e.g. both occurrences of ‘i’\nin ‘felicity’ are represented by the same embedding.\nThis effectively reduces the number of parameters.\nWe compare the performance of the CW models\nwith weight sharing with the baseline word-level\nLM and the CW model without weight sharing. In\ntable 5, the relative change with respect to those\nLMs is listed.\nCW models with weight sharing generally\nimprove with respect to a word-level baseline,\nexcept for the small English LM. For Dutch, the\nimprovements are more pronounced. The difference\nwith the CW model without weight sharing is small\n(right column), although not sharing the weights\nworks slightly better, which suggests that characters\ncan convey different meanings depending on the\nposition in which they occur. Again, the results are\nmore clear-cut for Dutch than for English.\n4.7 Dealing with out-of-vocabulary words\nAs we mentioned in the introduction, we expect that\nby providing information about the surface form of\nOOV words (namely, their characters), the number\nof errors induced by those words should decrease.\n424\nRelative change in\nvalid perplexity w.r.t.\nBaseline Char-Word\nPTB small c10 0.53 (0.88) 0.19 (0.67)\nlarge c10 - 0.54 (0.37) - 0.02 (0.22)\nCGN small c10 - 1.70 (0.34) 0.24 (0.30)\nlarge c10 - 2.10 (0.32) 0.15 (0.50)\nTable 5: Relative change in validation perplexity for\nCW models withweight sharingfor the characters,\nw.r.t. baseline and CW models without weight\nsharing. The improvements are averaged over the\nresults for adding 1 until 10 characters, the numbers\nbetween brackets are standard deviations. Negative\nnumbers indicate a decrease in perplexity.\nWe conduct the following experiment to check\nwhether this is indeed the case: for the CGN test\nset, we keep track of the probabilities of each word\nduring testing. If an OOV word is encountered, we\ncheck the probability of the target word given by\na word-level LM and a CW LM. The word-level\nmodel is a large model of size 475 and the CW\nmodel is a large model in which 6 characters embed-\ndings of size 25 in the backward order are used (the\nbest performing CW model in our experiments).\nWe observe that in 17,483 of the cases, the CW\nmodel assigns a higher probability to the target\nword following an OOV word, whereas the opposite\nhappens only in 10,724 cases. This is an indication\nthat using the character information indeed helps\nin better modeling the OOV words.\n5 Conclusion and future work\nWe investigated a character-word LSTM language\nmodel, which combines character and word\ninformation by concatenating the respective\nembeddings. This both reduces the size of the\nLSTM and improves the perplexity with respect\nto a baseline word-level LM. The model was\ntested on English and Dutch, for different model\nsizes, several embedding sizes for the characters,\ndifferent orders in which the characters are added\nand for weight sharing of the characters. We can\nconclude that for almost all setups, the CW LM\noutperforms the word-level model, whereas it has\nfewer parameters than the word-level model with\nthe same number of LSTM units. If we compare\nwith a word-level LM with approximately the same\nnumber of parameters, the improvement is larger.\nOne might argue that using a CNN or an RNN\nto generate character-level embeddings is a more\ngeneral approach to incorporate characters in a LM,\nbut this model is simple, easier to train and smaller.\nMoreover, related models using a CNN-based\ncharacter embedding (Kim et al., 2016) do not\nperform better.\nFor both English and Dutch, we see that the size\nof the character embedding is important and should\nbe proportional to the total embedding size: the\ntotal size of the concatenated character embeddings\nshould not be larger than the word-level embedding.\nNot using the characters in the order in which they\nappear in the word, but in the reversed order (and\nhence putting more emphasis on the end of the\nword), performs slightly better, although adding\nonly a few characters both from the beginning and\nthe end of the word achieves good performance too.\nUsing random inputs instead of the charac-\nters performed worse than using the characters\nthemselves, thus refuting the hypothesis that the\ncharacters simply introduce noise. Sharing the\nweights/embedding matrices for the characters\nreduces the size of the model even more, but causes\na small increase in perplexity with respect to a\nmodel without weight sharing. Finally, we observe\nthat the CW models are better able to deal with\nOOV words than word-level LMs.\nIn future work, we will test other architectures\nto incorporate character information in a word-level\nLSTM LM, such as combining a character-level\nLSTM with a word-level LSTM. Another rep-\nresentation that might be useful uses character\nco-occurrence vectors (by analogy with the acoustic\nco-occurrences used by Van hamme (2008; 2012))\nrather than one-hot character vectors, because\nco-occurrences intrinsically give information about\nthe order of the characters. Other models could\nbe more inspired by human language processing:\naccording to the theory of blocking, we humans\nhave both a mental lexicon of frequent words and\na morphological module that is used to process\ninfrequent/ unknown words or to create new words\n(see e.g. (Aronoff and Anshen, 2001)). This could\ncorrespond to a word-level LM for frequent words\nand a subword-level LM for infrequent words.\nAcknowledgments\nThis work was funded by IWT-INNOV ATIEF\nAANBESTEDEN and VRT in the STON project.\n425\nReferences\nMart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene\nBrevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-\nrado, Andy Davis, Jeffrey Dean, Matthieu Devin,\nSanjay Ghemawat, Ian Goodfellow, Geoffrey Irving\nAndrew Harp, Michael Isard, Rafal Jozefowicz,\nYangqing Jia, Lukasz Kaiser, Manjunath Kudlur,\nJosh Levenberg, Dan Man, Mike Schuster, Rajat\nMonga, Sherry Moore, Derek Murray, Chris Olah,\nJonathon Shlens, Benoit Steiner, Ilya Sutskever,\nKunal Talwar, Paul Tucker, Vincent Vanhoucke,\nVijay Vasudevan, Fernanda Vigas, Oriol Vinyals,\nPete Warden, Martin Wattenberg, Martin Wicke,\nYuan Yu, and Xiaoqiang Zheng. 2015. Tensorﬂow:\nLarge-scale machine learning on heterogeneous\nsystems. Software available from tensorﬂow.org.\nMarc Aronoff and Frank Anshen. 2001. Morphology\nand the lexicon: Lexicalization and productivity. In\nAndrew Spencer and Arnold M. Zwicky, editors,\nThe Handbook of Morphology , pages 237–247.\nBlackwell Publishing.\nMiguel Ballesteros, Chris Dyer, and Noah A. Smith.\n2015. Improved Transition-Based Parsing by\nModeling characters instead of words with LSTMs.\nConference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 349–359.\nPiotr Bojanowski, Armand Joulin, and Tom´a˘s Mikolov.\n2015. Alternative structures for character-level\nRNNs. arXiv:1511.06303.\nXinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun,\nand Huanbo Luan. 2015. Joint Learning of Charac-\nter and Word Embeddings. Conference on Artiﬁcial\nIntelligence (AAAI), pages 1236–1242.\nKyunghyun Cho, Bart van Merri ¨enboer, Caglar\nGulcehre, Dzmitry Bahdanau, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. 2014.\nLearning Phrase Representations using RNN\nEncoder–Decoder for Statistical Machine Transla-\ntion. Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1724–1734.\nJunyoung Chung, Kyunghyun Cho, and Yoshua Bengio.\n2016. A Character-Level Decoder without Explicit\nSegmentation for Neural Machine Translation.\narXiv:1603.06147.\nMarta R. Costa-juss `a and Jos ´e A.R. Fonollosa. 2016.\nCharacter-based Neural Machine Translation. Pro-\nceedings of the Association for Computational\nLinguistics (ACL), 2:357–361.\nC´ıcero N. dos Santos and Bianca Zadrozny. 2014.\nLearning Character-level Representations for\nPart-of-Speech Tagging. Proceedings of The 31st\nInternational Conference on Machine Learning\n(ICML), pages 1818–1826.\nHugo Van hamme. 2008. HAC-models: a novel\napproach to continuous speech recognition. Pro-\nceedings Interspeech, pages 2554–2557.\nHugo Van hamme. 2012. An on-line NMF model for\ntemporal pattern learning: Theory with application\nto automatic speech recognition. International\nConference on Latent Variable Analysis and Signal\nSeparation (LVA/ICA), pages 306–313.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation ,\n9(8):1735–1780.\nRafal Jozefowicz, Wojciech Zaremba, and Ilya\nSutskever. 2015. An Empirical Exploration of\nRecurrent Network Architectures. Proceedings\nof the 32nd International Conference on Machine\nLearning (ICML), pages 2342–2350.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring the\nLimits of Language Modeling. arXiv:1602.02410.\nMoonyoung Kang, Tim Ng, and Long Nguyen. 2011.\nMandarin word-character hybrid-input Neural Net-\nwork Language Model. Proceedings Interspeech,\npages 1261–1264.\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M. Rush. 2016. Character-Aware Neural\nLanguage Models. Proc. Conference on Artiﬁcial\nIntelligence (AAAI), pages 2741–2749.\nWang Ling, Isabel Trancoso, Chris Dyer, and Alan\nBlack. 2015. Character-based Neural Machine\nTranslation. arXiv:1511.04586.\nJean-Baptiste Michel, Yuan Kui Shen, Aviva Presser\nAiden, Adrian Veres, Matthew K. Gray, William\nBrockman, The Google Books Team, Joseph P.\nPickett, Dale Hoiberg, Dan Clancy, Peter Norvig,\nJon Orwant, Steven Pinker, Martin A. Nowak, and\nErez Lieberman Aiden. 2011. Quantitative Analysis\nof Culture Using Millions of Digitized Books.\nScience, 331(6014):176–182.\nTom´a˘s Mikolov, Martin Karaﬁ ´at, Luk ´a˘s Burget, Jan\n˘Cernock´y, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. Proceedings\nInterspeech, pages 1045–1048.\nYasumasa Miyamoto and Kyunghyun Cho. 2016.\nGated Word-Character Recurrent Language Model.\narXiv:1606.01700.\nNelleke Oostdijk. 2000. The Spoken Dutch Cor-\npus. Overview and ﬁrst Evaluation. Proceedings\nLanguage Resources and Evaluation Conference\n(LREC), pages 887–894.\nBert R ´eveil. 2012. Optimizing the Recognition\nLexicon for Automatic Speech Recognition. PhD\nthesis, University of Ghent, Belgium.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A Simple Way to Prevent Neural Networks\nfrom Overﬁtting. Journal of Machine Learning\nResearch, 15:1929–1958.\n426\nRupesh K. Srivastava, Klaus Greff, and J¨urgen Schmid-\nhuber. 2015. Training Very Deep Networks. Neural\nInformation Processing Systems Conference (NIPS),\npages 2377–2385.\nMartin Sundermeyer, Ralf Schl¨uter, and Hermann Ney.\n2012. LSTM Neural Networks for Language Model-\ning. Proceedings Interspeech, pages 1724–1734.\nZiang Xie, Anand Avati, Naveen Arivzhagan, Dan\nJurafsky, and Andrew Y . Ng. 2016. Neural Lan-\nguage Correction with Character-Based Attention.\narXiv:1603.09727.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2015. Recurrent Neural Network Regularization.\narXiv:1409.2329.\nXiang Zhang, Junbo Zhao, and Yann LeCunn. 2015.\nCharacter-level Convolutional Networks for Text\nClassiﬁcation. Neural Information Processing\nSystems Conference (NIPS), pages 649–657.\n427",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9614725112915039
    },
    {
      "name": "Word (group theory)",
      "score": 0.8022481203079224
    },
    {
      "name": "Character (mathematics)",
      "score": 0.7995430827140808
    },
    {
      "name": "Computer science",
      "score": 0.7794815301895142
    },
    {
      "name": "Language model",
      "score": 0.7697113752365112
    },
    {
      "name": "Vocabulary",
      "score": 0.6437097787857056
    },
    {
      "name": "Natural language processing",
      "score": 0.6173067092895508
    },
    {
      "name": "Baseline (sea)",
      "score": 0.6145410537719727
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6092008948326111
    },
    {
      "name": "Speech recognition",
      "score": 0.40899163484573364
    },
    {
      "name": "Linguistics",
      "score": 0.2252722978591919
    },
    {
      "name": "Mathematics",
      "score": 0.10676804184913635
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99464096",
      "name": "KU Leuven",
      "country": "BE"
    }
  ]
}