{
  "title": "Multi-image Feature Map-Based Watermarking Techniques Using Transformer",
  "url": "https://openalex.org/W4379229435",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4379235370",
      "name": "Aberna Palani",
      "affiliations": [
        "Vellore Institute of Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A1989861085",
      "name": "Agilandeeswari Loganathan",
      "affiliations": [
        "Vellore Institute of Technology University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4283833396",
    "https://openalex.org/W4283825664",
    "https://openalex.org/W1972533335",
    "https://openalex.org/W3186526369",
    "https://openalex.org/W2156351904",
    "https://openalex.org/W2793905823",
    "https://openalex.org/W3008936423",
    "https://openalex.org/W228639283",
    "https://openalex.org/W2921634466",
    "https://openalex.org/W2062383108",
    "https://openalex.org/W4365448040",
    "https://openalex.org/W1934028671",
    "https://openalex.org/W4225542251",
    "https://openalex.org/W4224060672",
    "https://openalex.org/W3016137241",
    "https://openalex.org/W4281394244",
    "https://openalex.org/W3011861564",
    "https://openalex.org/W3204738537",
    "https://openalex.org/W2391676209",
    "https://openalex.org/W792294310",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2906361067",
    "https://openalex.org/W3212034002"
  ],
  "abstract": "Nowadays, protecting multimedia data is a significant challenge because of the advancement of technology and software. The embedding process heavily relies on watermarking to accomplish multimedia security in terms of content authentication, proof of ownership, and tamper detection. Our objective is to develop an invariant watermark that can survive different signal-processing attacks. We presented a unique hybrid technique (DWT-QR-SWT) and multi-image invariant features generated as a watermark using a Transformer encoder-decoder model. The encoded image features are subsampled using PCA in order to decrease the dimensionality of the watermark image. The first two images are used as watermark1 and the next two images as watermark2 to produce multi-watermark feature maps. To embed the watermark, a hybrid DWT-QR decomposition has been applied to the original image1. On the primary watermarked image, two Level Stationary Wavelet Transform (SWT) were applied to embed the secondary watermark2. At the extraction phase, the tampered image is recovered by passing the extracted watermark image as input to the transformer decoder. A multi-image watermark increases data embedding capabilities and also achieves two-level content authentication, tamper detection, localization, and recovery. With a PSNR of 59.05 dB, the testing result demonstrates great resilience and improved imperceptibility.",
  "full_text": "                                                     International Journal of \n                     Electrical and Electronics Research (IJEER) \nOpen Access | Rapid and quality publishing                                           Research Article | Volume 11, Issue 2 | Pages 339-344 | e-ISSN: 2347-470X \n \n339 \nWebsite: www.ijeer.forexjournal.co.in             Multi-image Feature Map-Based Watermarking \nTechniques Using Transformer \n \nâ–‘ ABSTRACT- Nowadays, protecting multimedia data is a significant challenge because of the advancement of technology \nand software. The embedding process heavily relies on watermarking to accomplish multimedia security in terms of content \nauthentication, proof of ownership, and tamper detection. Our objective is to develop  an invariant watermark that can survive \ndifferent signal-processing attacks. We presented a unique hybrid technique (DWT -QR-SWT) and multi-image invariant features \ngenerated as a watermark using a Transformer encoder-decoder model. The encoded image features are subsampled using PCA in \norder to decrease the dimensionality of the watermark image. The first two images are used as watermark1 and the next two \nimages as watermark2 to produce multi-watermark feature maps. To embed the watermark, a hybrid DWT-QR decomposition has \nbeen applied to the original image1. On the primary  watermarked image, two Level Stationary Wavelet Transform (SWT) were \napplied to embed the  secondary watermark2. At the extraction phase, the tampered image is recovered by passing the extracted \nwatermark image as input to the  transformer decoder. A multi-image watermark increases data embedding capabilities and also \nachieves two-level content authentication, tamper detecti on, localization,  and recovery. With a PSNR of 59.05 dB, the testing \nresult demonstrates great resilience and improved imperceptibility. \n \nKeywords: Transformer; Tamper detection; QR decomposition; Discrete Wavelet Transform; Stationary Wavelet Transform; \nPrincipal component analysis. \n \n \n \nâ–‘ 1. INTRODUCTION   \nNetwork and internet technology has grown in advance for \nvarious applications such as social media platforms, e -Health \ncare, T elemedicine [1], e -banking, etc . On one end, the \ntransmission of multimedia data such as audio, text, video, and \nthe image growth rate increased, on the other hand, \nmultimedia data tampering has also been increased. Resulting \nto this, protecting multimedia  data in digital networks has \nbecome a major problem. Security is a top concern in \nmultimedia platforms across various applications, including \nContent Authentication, Proof of Ownership, and Copyright \nProtection. To secure digital multimedia, cryptography [2], \nsteganography [ 3], and watermarking [4] techniques are \nintroduced. Among that watermarking showed a state-of-the-\nart method in data embedding, and also it has significant to \nprove ownership, copyright protection, and multimedia data \nauthentication. Digital watermarking techniques are generally \nprocessed on two domains  [5]: Pixel -Based domain and \nFrequency-Based domain. To transform the image in the \nfrequency domain, techniques such as Discrete Fourier \nTransform (DFT) [6], Quaternion Curvelet Transform (QCT) \n[7],  Discrete Cosine Transform (DCT) [8], Hilbert Transform \n[9], Wavelet Transform [Discrete wavelet (DWT)  [10], \nStationary Wavelet (SWT)  [11], Integer Wavelet Transform \n(IWT) [ 12], Complex Wavelet Transform (CWT)[ 13]], etc.  \nThe hybrid embedding tec hniques are also  been attracted by \nthe researchers to achieve authentication [ 14]. A robust blind \nSWT watermarking technique is proposed by Nagarjuna, P.V. \net al., [11]. The Watermark image is encrypted by XOR \noperation, embedded in the original imageâ€™s low -frequency \nstationary wavelet transform (SWT) coefficient values. The \nextraction phase shows higher imperceptibility for tamper \ndetection applications. In addition to the frequency domain, \nmatrix decomposition techniques such as Singular value \ndecomposition (SVD) [10], and QRD decomposition [15]. In \n[15] a QR decomposition (QRD) -based blind watermarking \napproach for colour images is designed. The watermark is \nembedded by altering the 2 -bit per  pixel of each 4Ã—4 QR \nblock, and the watermarked image is downsam pled to the \nCNN to train the image for tamper detection and extracting the \nwatermark. To compute QR matrices, a unique technique \noffered by [ 16] was employed instead of the Gram -Schmidt \napproach. The image is partitioned into blocks before \nwatermark embedding to decrease the extraction time and the \nwatermark embedded in R(1,1) matrix.  The experimental \nfindings reveal that the watermarked images have good \nquality. In [10] DWT-SVD transform is suggested to provide \ncontent authentication. They generate the sin gular coefficient \nMulti-image Feature Map-Based Watermarking Techniques \nUsing Transformer \n  \nAberna Palani1, Agilandeeswari Loganathan2  \n \n1Research Scholar, School of Information Technology and Engineering (SITE), Vellore Institute Of Technology, Vellore, India, \naberna.p@vitstudent.ac.in  \n2Professor, School of Information Technology and Engineering (SITE), Vellore Institute Of Technology, Vellore, India, \nagila.l@vit.ac.in \n \n#Correspondence:  Aberna Palani; agila.l@vit.ac.in \nARTICLE INFORMATION \nAuthor(s):  Aberna Palani and Agilandeeswari Loganathan; \n \nReceived: 18-02-2023; Accepted: 20-05-2023; Published: 30-05-2023;     \nE- ISSN: 2347-470X; \nPaper Id:  IJEER230207b; \nCitation: 10.37391/IJEER.110214 \nWebpage-link: \nhttps://ijeer.forexjournal.co.in/archive/volume-11/ijeer-110214.html \n \nPublisherâ€™s Note: FOREX Publication stays neutral with regard to \njurisdictional claims in Published maps and institutional afï¬liations. \n \n\n                                                     International Journal of \n                     Electrical and Electronics Research (IJEER) \nOpen Access | Rapid and quality publishing                                           Research Article | Volume 11, Issue 2 | Pages 339-344 | e-ISSN: 2347-470X \n \n340 \nWebsite: www.ijeer.forexjournal.co.in             Multi-image Feature Map-Based Watermarking \nTechniques Using Transformer \nmatrix as a watermark which is embedded in the 2-level DWT \nHH sub-band using a secret key. Whereas in [17], the original \nimage was preprocessed by compressing it using the PCA \ntechnique. The watermark is generated by generating singular \nvalues from both the original and watermark image which are \ncombined to form a new singular matrix . The new singular \nmatrix is a dot product with the original image U and V matrix \nto form a modified LL sub -band which show s acceptable \nresults for copyright application. In recent trends, researchers \nhave strongly recommended ensemble techniques, where deep \nlearning-based watermarking approaches such as DCT -CNN \n[18], DWT-CNN [19], and others demonstrated state -of-the-\nart ways rather than conventional watermarki ng methods. In \nthe deep Learning approach, CNN plays a prominent role in \nclassification, object detection, invariant feature map \nextraction [20], and so on. On the other hand, some researchers \nare concentrated on Neural networks and fuzzy -based \nwatermarking systems [21, 22]. In recent work, the attention-\nbased transformer is slowly getting more performance \nrecognition than CNN in various applications.  CNN's \ndrawback is that hard inductive bias is required to perform \nbetter for small datasets. So , to overcom e this, the hard \ninductive bias usage is avoided by the transformer using a key \ncomponent, which is the attention mechanism [ 23]. \nTransformer designed primarily for natural language \nprocessing (NLP) applications to help with language \ntranslation, image translation, and speech understanding [24]. \nTransformer is a straightforward and parallel processing \nmethod so, the researcher extended this technique to various \ncomputer vision applications like image classification, video, \nand audio processing. T he t ransformer concept is briefly \nexplained in section 2.1. \nThe main contributions of this research are: \n(i) Attempted Transformer model for the first time in the \nfield of watermarking and succeeded in image tampering \napplications. \n(ii) Proposed a novel hybrid technique namel y DWT -QR-\nSWT which has the potential to hold a high -capacity \nwatermark.  \n(iii) Achieved high robustness using data augmentations and \nit is fed to train transformers using rotation, scaling, and \nflipping which produces highly invariant watermark \nfeature map images. \nThe rest of the paper is explained as follows: section 2 explains \nthe preliminary concept, section 3  describes the proposed \nsystem followed by results and discussion in section 4.  \n \nâ–‘ 2. PRELIMINARY CONCEPTS \n2.1 Transformer \nThe t ransformer encoder -decoder model was proposed by \nGoogle brain team members in 2017 [24] for various image \ntranslation and Language translation applications. T he \ntransformer encoder-decoder architecture model is co mposed \nof six identical stack layers (i.e., six residual encoder and six \nresidual decoder blocks). T he t ransformer encoder has two \nsub-layers: one is a multi-head self-attention layer; the second \nsub-layer is a feed -forward layer. Each sub -layer is followe d \nby a residual connection and normalization layer. The \npositional embedding information is concatenated with the \nflattened 1D sequence input vector ğ‘‹ğ‘–, which is fed as input to \nthe transformer encoder. In the encoder, self -attention \ngenerates a key vector ğ¾ğ‘–, value vector ğ‘‰ğ‘–, and query vector ğ‘„ğ‘–  \nby multiplying each input vector  ğ‘‹ğ‘– with three weight \nmatrices ğ‘Šğ‘ğ‘˜ğ‘‰. The weight matrix will be the same for all the \ninput vector ğ‘‹i:n sequence. Self -attention computes the dot \nproduct of each query vector ğ‘„ğ‘– with all key transpose vector \nKi:n and divided by key dimension dk followed by the softmax \nfunction. The self -attention is then multiplied by the \ncorresponding value vectors which give the output vector Self-\nAttention (Q, K, V) as represented in equation (1). The self-\nattention mechanism performs parallelly, where each self -\nattention head is concatenated together as multi-head attention \n(MHA) represented as shown in equation (2). \n \n     Self-Attention (â„ğ‘’ğ‘ğ‘‘1)=softmax(\nğ¾ğ‘–:ğ‘›ğ‘‡ğ‘„ğ‘–\nâˆšğ‘‘ğ¾\n) ğ‘‰ğ‘–               (1) \n \n    MultiHead (MHA)=Concat(â„ğ‘’ğ‘ğ‘‘1+...,â„ğ‘’ğ‘ğ‘‘ğ‘›)            (2) \n \nWhere â„ğ‘’ğ‘ğ‘‘ğ‘–  = Self-Attention (Q, K, V). Normalized multi-\nhead attention passes to the feed-forward layer. ğ‘‹ğ‘–\nâ€² is fed has \nan input for the next stack encoder and the process continues \nuntil the last encoder block outputs the final encoded data. \n \n     ğ‘1 = MHA(Layernorm(ğ‘‹ğ‘–ğ¸))+. . . ğ‘‹ğ‘–ğ¸ ğ‘€âˆ’1,                  (3) \n \nWhere M=1,2,â€¦n. \n \nThe normalized MHA output is concatenated together and \npassed as input to the MLP classifier of the encoder block to \nobtain learned feature maps eq.6.  \n     ğ‘ğ‘›\nâ€² = MLP(Layernorm(ğ‘1)) +ğ‘ğ‘™,    l=1,2â€¦N     (4) \n                ğ‘Œğ‘–=(Layernorm (ğ‘ğ‘›\nâ€² ))                         (5) \nThe transformer-based decoder has three sub -layers: the first \ntwo layers are masked multi -head attention and norm layer \nfollowed by the residual connection for each sub-layer. Instead \nof self-attention in the decoder masking multi -head attention \nis used to prevent positions from attending to subsequent \npositions. Masked MHA value as well as key -value of the \nencoder is fed to the next multi-head attention layer. The MSA \noutput is given to the feed-forward layer foll owed by linear \nand softmax layers to train and classify the input.  \n \n2.2 QR decomposition \nQR decomposition is a matrix decomposition method that is \nsimilar to the Singular Value Decomposition method [3, 2], \napplied to original image matrix A of size 4Ã—4, which  \ndecomposes into two matrices as eq. (6). \n \n          A= Q R = [q1 q2 q3 q4] [\nğ‘Ÿ11 ğ‘Ÿ12 ğ‘Ÿ13\n0 ğ‘Ÿ22 ğ‘Ÿ23\n0\n0\n0\n0\nğ‘Ÿ33\n0\n  \nğ‘Ÿ14\nğ‘Ÿ24\nğ‘Ÿ34\nğ‘Ÿ44\n]            (6) \n \nwhere Q is an orthogonal matrix and R is an upper triangu lar \nmatrix, q1, q2, q3, q4 are column unitary vector s of the Q \nmatrix. Elements of the Q matrix derived from the columns of \nA by the Gram-Schmidt orthogonalization process.   \n \n                                                     International Journal of \n                     Electrical and Electronics Research (IJEER) \nOpen Access | Rapid and quality publishing                                           Research Article | Volume 11, Issue 2 | Pages 339-344 | e-ISSN: 2347-470X \n \n341 \nWebsite: www.ijeer.forexjournal.co.in             Multi-image Feature Map-Based Watermarking \nTechniques Using Transformer \nStationary Wavelet Transform \nA new wavelet transform called stationary wavelet t ransform \n(SWT) is applied to the primary watermarked image in the \nsecondary level of embedding. Due to its translation -\ninvariance traits against signal processing attacks, SWT is also \nfrequently referred to as an un-decimated wavelet transform \n[11]. SWT is similar to DWT, additionally SWT level contains \nthe same amount of redundancy. In SWT, instead of down -\nsampling steps, it used a null placing procedure instead of the \ndownsampling [25]. In the proposed method, the LH2 sub -\nband coefficients in the SWT domain are selected as a suitable \nregion to embed for the watermark which doesnâ€™t influence \nthe image quality and maintains better robustness.  \n \nâ–‘ 3. PROPOSED WORK    \nIn the proposed method, an enhanced benchmark dataset is fed \ninto the transformer encoder together with them being used to \nfine-tune the transformer model. Image authentication, tamper \ndetection, and recovery, as well as watermark emb edding \ncapabilities, are assessed using multi -image encoder feature \nmaps that are produced as watermarks that are embedded in \nthe cover image. The taxonomy of the suggested model is \nshown in figure 1 , where the dimensionality of the \ntransformer-encoded feature maps is reduced using the PCA \napproach to obtain the final watermarks. The DWT approach \nis applied to the original image at the primary level of \nembedding, as illustrated in fig. 3f. After that, the 8Ã—8  blocks \nof the low -frequency LL sub -band are divi ded up, and each \nblock is then broken down into a QR matrix.  The primary \nwatermarked image is provided by DWT after the first two \nimages are inserted in the Q matrix, followed by inverse QR. \nTwo-level SWT is applied on the primary watermarked image \nat the secondary level of embedding, as illustrated in Figures \n3(d) and 3(e), and the other two watermarks are embedded in \nthe mid-frequency coefficient together with a secret key that \nwas produced randomly. During the extraction phase, two \nlevels of data authentication are verified. F irst, the secret key \nmatches, then the second level of the authentic fingerprint \nimage is extracted from stationary wavelet coefficients and \nmatched with the original watermark. If the image is not \nauthentic, then extract the other embedded images from the Q \nmatrix of low -frequency sub -bands to detect tampered \nregions. The self -recovered image is fed as input to the \ntransformer decoder which extracts the original image.  \nIdentify tampered regions by extracting the other embedded \nimages from the Q matrix of low -frequency sub-bands if the \nimage is not authentic otherwise no extraction process. The \ntransformer decoder receives the extracted self -recovery \nwatermark as input, which produces the exact original image. \n \n3.1 Watermark generation and embedding   \n  To process a 2D image, the image is split into patches â€˜Pâ€™ of \nfixed size, producing a n N number of patches, where N=  \nHW/ğ‘ƒ2ğ¶, where C stands for the number of channels and P \nstands for patch  size and H & W are the original image \ndimension [23]. T he image patches are linearly flattened \nthrough a linear patch projection layer, the image patches are \nconverted into a vector  ğ‘‹ğ‘– along with embedding matrix E as \nshown in eq. (7). Followed by adding positional embedding \ninformation to the linear projection layer results in the final \ninput context vector ğ‘1, as stated in eq. (8),  \n \nTo process the 2D image, the image is d ivided into patches P \nof fixed size resulting in N number of patches N= HW/ğ‘ƒ2ğ¶, \nwhere P represents the height and width of fixed patch size \nand H & W are the size of the original image, C represents the \nnumber of channels [23]. Image patches are linearly flattened \nthrough linear patch projection which converts the image \npatches into matrix E with dimension vector size  ğ‘‹ğ‘– as \nrepresented in eq (7).  Adding positional embedding \ninformation to the input sequence vector which produce s the \nfinal input context vector ğ‘1 as shown in eq (8).  \n \n        ğ¹ğ‘–=[ ğ‘‹1ğ¸, ğ‘‹2ğ¸. . . . . . ğ‘‹ğ‘›ğ¸],                                         (7) \n \n            ğ‘1 =  ğ¹ğ‘–   + ğ¸ğ‘ğ‘œğ‘                                                       (8) \n \nwhere X represents the number of vectors, E represents the \nembedding matrix, ğ‘1 represents vectorized context vector \ninput, ğ¸ğ‘ğ‘œğ‘  represents positional embedding. \n \nContext vector ğ‘1 is fed as an input to the transformer encoder. \nTransformer encoder feature maps generation is mentioned in \nequation (1-5) in section 2.1. To reduce the dimensionality of \nthe multi-image watermark feature maps ğ‘Œğ‘–, PCA technique is \napplied to each encoded transformer image that produces the \nfinal watermark image. \n \nAlgorithm  \nWatermark generation  \n1. The transformer model is fine -tuned on the standard \nbenchmark dataset. \n2. A benchmark image is given as input by splitting the \nimage into several patches and flattening the image \npatches. \n3. Create linear dimensional embeddings from t hese \nflattened image patches and concatenate positional \nembeddings to the image patch sequence. \n4. Feed the sequence as an input to the transformer \nencoder \n5. Self-attention is computed and normalized followed by \nthe feed-forward network that generates an encode d \nfeature image. \n6. Encoded image dimensionality is reduced using the \nPCA technique.   \n \nWatermark Embedding  \n1. Embed the first two encoded image features as \nwatermark1 represented as ğ‘Šğ‘€1 and the other two \nencoded image features as watermark2 represented as  \nğ‘Šğ‘€2.  \n2. Apply DWT on the original image O \n3. Divide the LL band into 8 Ã— 8 blocks and perform QR \ndecomposition  \n4. Embed watermark1 in matrix Qâ€™ using the secret key,  \nQâ€™= ğ‘„ğ‘– + ğ›¼ğ‘Šğ‘€1  \n5. Perform inverse IQR and IDWT \nOutput: Primary Watermark Image \n6. Apply 2-Level SWT on the primary watermarked image \n7. Embed watermark2 on LH2 mid-frequency band. \n8. Perform 2-Level inverse SWT \n                                                     International Journal of \n                     Electrical and Electronics Research (IJEER) \nOpen Access | Rapid and quality publishing                                           Research Article | Volume 11, Issue 2 | Pages 339-344 | e-ISSN: 2347-470X \n \n342 \nWebsite: www.ijeer.forexjournal.co.in             Multi-image Feature Map-Based Watermarking \nTechniques Using Transformer \n3.2 Extraction Algorithm \nTo authenticate the watermarked image and to detect the \ntampered region extraction process is carried out as \nfollows. \nAlgorithm \n1. Perform 2 -Level SWT on watermarked image and \nextract watermark2 from the LH2 sub-band. \n2. Compare extracted watermark WMâ€™ with the original \nwatermark WM, if it is true, then the image is authentic \nallowing to extract watermark and locate tampered \nregion, otherwise, No extraction process. \n3. To recover the tampered region perform Inverse SWT \nand apply DWT on the image. \n4. Divide the LL low-frequency band into 8 Ã— 8 blocks and \nperform QR decomposition. \n5. Extract the watermark1 from the Q matrix and apply \ninverse PCA on the self-recovery image. \n6. Fed the extracted image into the transformer decoder to \nrecover the tampered image. \n \nâ–‘ 4. RESULT AND DISCUSSION \nThe proposed model is evaluated for various types of attacks \nto verify the robustness and imperceptibility. A benchmark \ndatabase is used to evaluate the proposed model for the \ndetection of image manipulation traces of size 512 Ã— 512, 256 \nÃ— 256. Figure 4 shows a few standard dataset s and sample \nuser-generated images of various sizes. The model is validated \nfor various noise attacks, median filter attacks, and geometric \nattacks. The proposed model is evaluated for various attacks \nlike Salt and pepper noise, Gaussian noise, Rotation, copy -\npaste, and flipping results are shown in Table 1. Figure 3 \nshows the original image, encoded watermark image, \nwatermarked image, stationary wavelet transform image, and \nDiscrete wavelet image. Our pr oposed model presented two \ndifferent wavelets transform along with QR decomposition \nand invariant multi -image watermark features are generated \nusing the Transformer encoder -decoder model. Extracted \nwatermarks are highly invariant against rotation, flipping, and \nscaling attack s. Transformer -based encoded feature map \nshows improved result than deep learning techniques as it \nextracts invariant feature maps. For future work, the \ntransformer model can extend to various applications like \nproof of ownership, and copyright protection. \n \n4.1 Quality Metrics \nVarious metrics are provided here to demonstrate the proposed \nscheme's performance against tamper detection and \nlocalization. The performance evaluated for various attacks \nwas measured by Peak -Signal-to-Noise-Ratio (PS NR) and \nNormalized correlation coefficient (NCC) as represented \nbelow. PSNR (Peak Signal -to-Noise Ratio) [26] evaluates the \nvisual perception  by comparing the difference between the \noriginal image and watermarked image and is deemed to be of \nacceptable quality if the PSNR score is more than 25 to 30 dB. \nThe PSNR is determined by using the following formula: \n \n PSNR =   10log 10 ( \n(ğ‘€ğ‘ğ‘¥âˆ’1)2\n[âˆ‘  ğ‘€,ğ‘ (ğ¼1(ğ‘š,ğ‘›)âˆ’ğ¼2(m,n)2] 1\nğ‘€âˆ—ğ‘\n)         (9)    \n     \nwhere M and N are the numbers of rows a nd columns in the \ninput images. The robustness of the embedded watermark is \nmeasured using the metric Normalized Correlation \nCoefficient. NCC measures the robustness between the \noriginal watermark and extracted watermark. NCC can be \ncalculated using below shown equation: \n \n             NCC = \nâˆ‘ âˆ‘ (|ğ‘Š(ğ‘–,ğ‘—)+ğ‘Šâ€² (ğ‘–,ğ‘—)|/2)ğ‘›ğ¾\nğ‘—=1\nğ‘›ğ¿\nğ‘–=1\nğ‘›ğ¿Ã—ğ‘›ğ¾\n                          (10) \n4.2 Comparative Analysis \nThe two -watermark extracted from the watermarked image \nshowed the highest imperceptibility for the recovered image \nwith the PSNR  value of 59.05 dB and robustness attained \nhigh with an NCC value of 0.997. Figure 5 , shows the \nperformance of the existing paper compared with the \nproposed work which show s that watermark feature maps \ngenerated using the transformer model attained high \nrobustness and imperceptibility. \n \n \n \nFigure 1:  Taxonomy of Proposed Architecture \n \n \n \nFigure 2:  Transformer Encoder-Decoder Architecture \n\n                                                     International Journal of \n                     Electrical and Electronics Research (IJEER) \nOpen Access | Rapid and quality publishing                                           Research Article | Volume 11, Issue 2 | Pages 339-344 | e-ISSN: 2347-470X \n \n343 \nWebsite: www.ijeer.forexjournal.co.in             Multi-image Feature Map-Based Watermarking \nTechniques Using Transformer \n                 \n                     (a)                          (b)                             (c)                         \n    \n                    (d)                                   (e)                               (f)                                     \nFigure 3: a) Original Image, b) Watermark Image, c) Watermarked \nImage d) 1-Level Stationary wavelet transform, e) 2-Level \nStationary wavelet transform, f) Discrete Wavelet transform \n \n                  \n         (a)                  (b)                (c)              (d)                    (e)                                               \n     \n              (f)                     (g)                     (h)                          (i) \nFigure 4: Sample images of Standard dataset (a-e), (f-i) User \nGenerated Images \n \n \n \nFigure 5: Comparative Analysis of existing works Vs Proposed \n \nâ–‘Table 1: Result on various Tampered images and its result \nWatermarked \nImage \nAttacks &Tampered \nImage \nTamper \ndetection \nExtracted WM1 \nWith NCC value \nExtracted WM2 \nWith NCC value \nRecovered \nImage \nPSNR for \nrecovered \nimage \n \nCopy-Paste Image \n \n NCC = 0.97 NCC = 0.989 \n \n56.45 dB \n \nImage Splicing/Cropping \n   \nNCC = 0.97 \n \nNCC = 0.989  \n56.45 dB \n Flipped Image  \n    \nNCC = 0.987 \n \nNCC = 0.98  \n49.02 dB \n Rotated 45áµ’     \nNCC = 0.974 \n \nNCC = 0.9762 \n \n \n55.30 dB \n \n Rotated 65áµ’   NCC = 0.982 \n \nNCC = 0.982  \n55.2 dB \n \n \nSalt & Pepper Noise \n   \nNCC = 0.97 NCC = 0.97 \n \n \n58.45 dB \n \n \nGaussian Noise \n \n \n   \nNCC = 0.964 \n \nNCC = 0.98  \n59.05 dB \n \n \nMedian Filter  \n  \nNCC = 0.95  \nNCC = 0.952  \n52.4 dB \n \n\n                                                     International Journal of \n                     Electrical and Electronics Research (IJEER) \nOpen Access | Rapid and quality publishing                                           Research Article | Volume 11, Issue 2 | Pages 339-344 | e-ISSN: 2347-470X \n \n344 \nWebsite: www.ijeer.forexjournal.co.in             Multi-image Feature Map-Based Watermarking \nTechniques Using Transformer \nâ–‘ 5. CONCLUSION \nIn digital media platforms securing images from manipulation \nis a challenging task in the real world. Watermarking \ntechnique laid a path for digital data to be secured against \nvarious appl ications like Authentica tion and Tamper \ndetection. The a ttention-based model shows more \nperformance than state-of-the-art deep learning techniques. \nWe have attempted the transformer model for the first time for \nwatermarking applications and succeeded with higher results. \nOur proposed model presented two different wavelets \ntransform along with QR decomposition and invariant multi -\nimage watermark features are generated using the Transformer \nencoder-decoder model. Extracted watermark s are highly \ninvariant agai nst rotation, flipping, and scaling attack s. \nTransformer-based encoded feature map shows improved \nresult than deep learning techniques as it extracts invariant \nfeature maps. For future work, the transformer model can \nextended to various applications like proof of ownership, and \ncopyright protection.  Also, the complexity of transformer \nmodel can be reduced by modifying attentions into linear \nattentions. \n \nâ–‘ REFERENCES \n[1] N. Shyamala and D. S. Geetha, \"Compression of Medical Images Using \nWavelet Transform and Met aheuristic Algorithm for Telemedicine \nApplications,\" IJEER, vol. 10(2), pp. 161-166, 2022. \n[2] L.Agilandeeswari, Sunny. S (2013) Secure data sharing of patient record \nin cloud environment using attribute based encryption, International \nJournal of Applied Engineering Research, (IJAER), Vol 8, Spec. Issue 19, \npp 2423-2426.  \n[3] Singh, J., & Singla, M. (2022). Image Steganography Technique based on \nSingular Value Decomposition and Discrete Wavelet \nTransform. International Journal of Electrical and Electronics \nResearch, 10(2), 122-125. \n[4] G. Zhou and D. Lv, \"An overview of digital watermarking in image \nforensics,\" 2011 Fourth International Joint Conference on Computational \nSciences and Optimization, pp. 332-335, April 2011.  \n[5] S. Kaur and J. Patel, \"A Robust Image Mosaicing Technique Using \nFrequency Domain.,\" IJEER, vol. 6(1), pp. 1-8, 2018. \n[6] T. K. Tsui, X. P. Zhang, and D. Androutsos, \"Color image watermarking \nusing multidimensional Fourier transforms, \" IEEE Transactions on \nInformation Forensics and Security, vol. 3(1), pp. 16-28, 2008. \n[7] Agilandeeswari, L., & Ganesan, K. (2018). RST invariant robust video \nwatermarking algorithm using quaternion curvelet transform. Multimedia \nTools and Applications, 77(19), 25431-25474. \n[8] D. Ariatmanto and F. Ernawan, \"Adaptive scaling factors bas ed on the \nimpact of selected DCT coefficients for image watermarking\", Journal of \nKing Saud University-Computer and Information Sciences, 2020. \n[9] L.Agilandeeswari, K.Ganesan, K.Muralibabu, (2013). A Side View Based \nVideo in Video Watermarking Using DWT and H ilbert Transform, \nSecurity in Computing and Communications, Communications in \nComputer and Information Science (CCIS) Series â€“ Springer, page 366-\n367 \n[10] L.Agilandeeswari, K.Muralibabu (2013). A Robust Video Watermarking \nAlgorithm for Content Authentication using Discrete Wavelet Transform \n(DWT) and Singular Value Decomposition (SVD), International Journal \nof Security and its Applications, Vol 7(4), pp. 145-158. \n[11] P. V. Nagarjuna and K. T. Ranjeet, \"Robust blind digital image \nwatermarking scheme based on stationa ry wavelet transform,\" In 2013 \nSixth International Conference on Contemporary Computing (IC3). \nIEEE., pp. 451-454, 2013, August. \n[12] L.Agilandeeswari and K. Ganesan. (2016) An Efficient Hilbert and \nInteger Wavelet Transform based Video Watermarking. Journal of  \nEngineering Science and Technology, Vol.11, Issue.3, Page. 327 - 345  \n[13] Agilandeeswari, L., Prabukumar, M., & Alenizi, F. A. (2023). A robust \nsemi-fragile watermarking system using Pseudo -Zernike moments and \ndual tree complex wavelet transform for social me dia content \nauthentication. Multimedia Tools and Applications, 1-53. \n[14] L.Agilandeeswari, K.Ganesan, (2016) â€œA Robust Color Video \nWatermarking scheme based on Hybrid Embedding Techniquesâ€ \nMULTIMEDIA TOOLS AND APPLICATIONS (Springer), Vol. 75, \nIssue 14, Page 8745 â€“ 8780  \n[15] P. T. Nha, T. M. Thanh, and N. T. Phong, \"Consideration of a robust \nwatermarking algorithm for color image using improved QR \ndecomposition,\" Soft Computing, vol. 26(11), pp. 5069-5093, 2022. \n[16] L. Y. Hsu, H. T. Hu, and H. H. Chou, \"A high-capacity QRD-based blind \ncolor image watermarking algorithm incorporated with AI technologies,\" \nExpert Systems with Applications, vol. 117134, pp. 199, 2022. \n[17] Rana, M. M., & Abdelhadi, A. (2020, February). Optimal Image \nWatermark Technique Using Singular Value Deco mposition with PCA. \nIn 2020 22nd International Conference on Advanced Communication \nTechnology (ICACT) (pp. 342-347). IEEE  \n[18] M. Rezaei and H. Taheri, \"Digital image self -recovery using CNN \nnetworks,\" Optik, p. 169345, 2022.  \n[19] F. Khan and M. S. Raeen, \"Robust  and Blind Multiple Image \nWatermarking Using CNN and DWT in Video\". \n[20] T. Song, X. Yu, S. Yu, Z. Ren, and Y. Qu, \"Feature Extraction Processing \nMethod of Medical Image Fusion Based on Neural Network Algorithm,\" \nComplexity, 2021  \n[21] Loganathan, A., & Kaliyaperuma l, G. (2016). An adaptive HVS based \nvideo watermarking scheme for multiple watermarks using BAM neural \nnetworks and fuzzy inference system. Expert Systems with Applications, \n63, 412-434.  \n[22] Agilandeeswari, L., Ganesan, K. A bi -directional associative memory \nbased multiple image watermarking on cover video. Multimed Tools Appl \n75, 7211â€“7256 (2016). \n[23] A. Dosovitskiy, L. K. A. Beyer, D. Weissenborn, X. Zhai, T. Unterthiner, \n... and N. Houlsby, \"An image is worth 16x16 words: Transformers for \nimage recognition at scale,\" arXiv preprint arXiv:2010.11929, 2020  \n[24] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. .. \nGomez, and I. Polosukhin (2017). \"Attention is all you need,\" Advances \nin neural information processing systems, pp. 30, 2017.  \n[25] N. N. Hurrah, S. A. Parah, N. A. Loan, J. A. Sheikh, M. Elhoseny and K. \nMuhammad, \"Dual watermarking framework for privacy protection and \ncontent authentication of multimedia,\" Future generation computer \nSystems, vol. 94, pp. 654-673, 2019. \n[26] A. Soualmi, A. Alti and L. La ouamer, \"An Imperceptible Watermarking \nScheme for Medical Image Tamper Detection,\" International Journal of \nInformation Security and Privacy (IJISP), vol. 16(1), pp. 1-18, 2022. \n \nÂ© 2023 by Aberna Palani and Agilandeeswari \nLoganathan. Submitted for possibl e open \naccess publication under the terms and \nconditions of the Creative Commons Attribution (CC BY) license \n(http://creativecommons.org/licenses/by/4.0/). \n \n \n",
  "topic": "Watermark",
  "concepts": [
    {
      "name": "Watermark",
      "score": 0.9470528364181519
    },
    {
      "name": "Digital watermarking",
      "score": 0.7689499855041504
    },
    {
      "name": "Computer science",
      "score": 0.6947147846221924
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6887263059616089
    },
    {
      "name": "Computer vision",
      "score": 0.5440818667411804
    },
    {
      "name": "Discrete wavelet transform",
      "score": 0.47695600986480713
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4719739854335785
    },
    {
      "name": "Embedding",
      "score": 0.4701535701751709
    },
    {
      "name": "Wavelet",
      "score": 0.4064878225326538
    },
    {
      "name": "Wavelet transform",
      "score": 0.3730695843696594
    },
    {
      "name": "Image (mathematics)",
      "score": 0.27960216999053955
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I876193797",
      "name": "Vellore Institute of Technology University",
      "country": "IN"
    }
  ],
  "cited_by": 17
}