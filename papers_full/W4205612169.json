{
  "title": "Progressive Multi-Scale Vision Transformer for Facial Action Unit Detection",
  "url": "https://openalex.org/W4205612169",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2098214385",
      "name": "Chongwen Wang",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2114728494",
      "name": "Zicheng Wang",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098214385",
      "name": "Chongwen Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114728494",
      "name": "Zicheng Wang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2115941714",
    "https://openalex.org/W6718219422",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W6794166682",
    "https://openalex.org/W6749370440",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W3002638829",
    "https://openalex.org/W6757010476",
    "https://openalex.org/W6741568391",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W6797311819",
    "https://openalex.org/W3157525179",
    "https://openalex.org/W2903991757",
    "https://openalex.org/W2612215259",
    "https://openalex.org/W2589142773",
    "https://openalex.org/W6795793143",
    "https://openalex.org/W6800016538",
    "https://openalex.org/W3044955494",
    "https://openalex.org/W2904483377",
    "https://openalex.org/W6756547384",
    "https://openalex.org/W6763438500",
    "https://openalex.org/W6787248546",
    "https://openalex.org/W6790291653",
    "https://openalex.org/W2045472600",
    "https://openalex.org/W6750597549",
    "https://openalex.org/W6785775482",
    "https://openalex.org/W2888200171",
    "https://openalex.org/W6754988648",
    "https://openalex.org/W6798235175",
    "https://openalex.org/W6791783338",
    "https://openalex.org/W2049640633",
    "https://openalex.org/W2018727909",
    "https://openalex.org/W6750245561",
    "https://openalex.org/W6717584496",
    "https://openalex.org/W2436394355",
    "https://openalex.org/W4289710724",
    "https://openalex.org/W3101614002",
    "https://openalex.org/W4205148344",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2887997593",
    "https://openalex.org/W3175851380",
    "https://openalex.org/W3136416617",
    "https://openalex.org/W4226105459",
    "https://openalex.org/W2560674852",
    "https://openalex.org/W3191359587",
    "https://openalex.org/W3102025332",
    "https://openalex.org/W2784208377",
    "https://openalex.org/W2560835477",
    "https://openalex.org/W2798545058",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4234552385"
  ],
  "abstract": "Facial action unit (AU) detection is an important task in affective computing and has attracted extensive attention in the field of computer vision and artificial intelligence. Previous studies for AU detection usually encode complex regional feature representations with manually defined facial landmarks and learn to model the relationships among AUs via graph neural network. Albeit some progress has been achieved, it is still tedious for existing methods to capture the exclusive and concurrent relationships among different combinations of the facial AUs. To circumvent this issue, we proposed a new progressive multi-scale vision transformer (PMVT) to capture the complex relationships among different AUs for the wide range of expressions in a data-driven fashion. PMVT is based on the multi-scale self-attention mechanism that can flexibly attend to a sequence of image patches to encode the critical cues for AUs. Compared with previous AU detection methods, the benefits of PMVT are 2-fold: (i) PMVT does not rely on manually defined facial landmarks to extract the regional representations, and (ii) PMVT is capable of encoding facial regions with adaptive receptive fields, thus facilitating representation of different AU flexibly. Experimental results show that PMVT improves the AU detection accuracy on the popular BP4D and DISFA datasets. Compared with other state-of-the-art AU detection methods, PMVT obtains consistent improvements. Visualization results show PMVT automatically perceives the discriminative facial regions for robust AU detection.",
  "full_text": "METHODS\npublished: 12 January 2022\ndoi: 10.3389/fnbot.2021.824592\nFrontiers in Neurorobotics | www.frontiersin.org 1 January 2022 | Volume 15 | Article 824592\nEdited by:\nYong Li,\nNanjing University of Science and\nTechnology, China\nReviewed by:\nXiaoya Zhang,\nNanjing University of Science and\nTechnology, China\nYaohui Zhu,\nBeijing Normal University, China\nHao Su,\nBeihang University, China\n*Correspondence:\nChongwen Wang\nwcwzzw@bit.edu.cn\nReceived: 29 November 2021\nAccepted: 10 December 2021\nPublished: 12 January 2022\nCitation:\nWang C and Wang Z (2022)\nProgressive Multi-Scale Vision\nTransformer for Facial Action Unit\nDetection.\nFront. Neurorobot. 15:824592.\ndoi: 10.3389/fnbot.2021.824592\nProgressive Multi-Scale Vision\nTransformer for Facial Action Unit\nDetection\nChongwen Wang* and Zicheng Wang\nSchool of Computer Science, Beijing Institute of Technology, Beijing, China\nFacial action unit (AU) detection is an important task in aff ective computing and\nhas attracted extensive attention in the ﬁeld of computer vi sion and artiﬁcial\nintelligence. Previous studies for AU detection usually en code complex regional\nfeature representations with manually deﬁned facial landm arks and learn to model the\nrelationships among AUs via graph neural network. Albeit some progress has been\nachieved, it is still tedious for existing methods to captur e the exclusive and concurrent\nrelationships among different combinations of the facial A Us. To circumvent this issue,\nwe proposed a new progressive multi-scale vision transform er (PMVT) to capture the\ncomplex relationships among different AUs for the wide rang e of expressions in a\ndata-driven fashion. PMVT is based on the multi-scale self- attention mechanism that\ncan ﬂexibly attend to a sequence of image patches to encode th e critical cues for\nAUs. Compared with previous AU detection methods, the beneﬁ ts of PMVT are 2-fold:\n(i) PMVT does not rely on manually deﬁned facial landmarks to extract the regional\nrepresentations, and (ii) PMVT is capable of encoding facia l regions with adaptive\nreceptive ﬁelds, thus facilitating representation of diff erent AU ﬂexibly. Experimental results\nshow that PMVT improves the AU detection accuracy on the popu lar BP4D and DISFA\ndatasets. Compared with other state-of-the-art AU detecti on methods, PMVT obtains\nconsistent improvements. Visualization results show PMVT automatically perceives the\ndiscriminative facial regions for robust AU detection.\nKeywords: affective computing, facial action unit recognition, multi-scale transformer, self-attention, cross-\nattention\n1. INTRODUCTION\nFacial expression is a natural way for non-verbal communicat ion in our daily life and can be\nconsidered as an intuitive illustration of human emotions an d mental states. There are some\npopular facial expression topics categorized as discrete facia l expression categories, facial micro-\nexpression, and the Facial Action Coding System (FACS) (\nEkman and Friesen, 1978 ). Among\nthem, FACS is the most comprehensive, anatomical system for e ncoding expression. FACS deﬁnes\na detailed set of about 30 atomic non-overlapping facial muscle actions, i.e., action units (AUs).\nAlmost any anatomical facial muscle activity can be introdu ced via a combination of facial AUs.\nAutomatic AU detection has drawn signiﬁcant interest from c omputer scientists and psychologists\nover recent decades, as it holds promise to several practical a pplications (Bartlett et al., 2003; Zafar\nand Khan, 2014 ), such as human aﬀect analysis, human-computer interaction, a nd pain estimation.\nWang and Wang PMVT for AU Detection\nThus, a reliable AU detection system is of great importance fo r\nthe analysis of ﬁne-grained facial expressions.\nIn FACS, diﬀerent AUs are tightly associated with diﬀerent\nfacial muscles. It actually means we can observe the active A Us\nfrom speciﬁc facial regions. For example, the raising of the\ninner corners of the eyebrows means activated AU1 (inner brow\nraiser). Lowering the inner corners of the brows corresponds to\nAU4 (brow lowerer). AU annotators are ofter unable to describ e\nthe precise location and the facial scope of the AUs due to\nthe ambiguities of the AUs and individual diﬀerences. Actual ly,\nthe manually deﬁned local AU regions are ambiguous. Existin g\nmethods (\nLi et al., 2017a,b, 2018a,b; Corneanu et al., 2018; Shao\net al., 2018; Jacob and Stenger, 2021 ) usually use artiﬁcially\ndeﬁne rectangle local regions, or use adaptive attention mas ks\nto focus on the expected local facial representations. Howeve r,\nthe rectangle local regions violate the actual appearance of t he\nAUs. Moreover, several AUs are simultaneously correlated wi th\nmultiple and ﬁne-grained facial regions. The learned adaptiv e\nattention masks fail to perceive the correlations among diﬀer ent\nAUs. Therefore, it is critical to automatically learn the AU-\nadaptive local representations and perceive the dependencies o f\nthe facial AUs.\nTo mitigate this issue, we introduce a new progressive\nmulti-scale vision transformer (PMVT) to capture the complex\nrelationships among diﬀerent AUs for the wide range of facial\nexpressions in a data-driven fashion. PMVT is based on the\nmulti-scale self-attention mechanism that can ﬂexibly att end\nto a sequence of image patches to encode the critical cues\nfor AU detection. Currently, vision transformers (\nDosovitskiy\net al., 2020; Li et al., 2021 ) have shown promising performance\nacross several vision tasks. The vision transformer models\ncontain MSA mechanisms that can ﬂexibly attend to a sequence\nof image patches to encode the dependencies of the image\npatches. The self-attention in the transformers has been sho wn\nto eﬀectively learn global interactions and relations betwe en\ndistant object parts. A series of works on various tasks such a s\nimage segmentation (\nJin et al., 2021 ), object detection ( Carion\net al., 2020 ), video representation learning ( Girdhar et al., 2019;\nFang et al., 2020 ) have veriﬁed the superiority of the vision\ntransformer models. Inspired by CrossViT ( Chen et al., 2021 ) that\nprocesses the input image tokens with two separate transformer\nbranches, our proposed PMVT ﬁrstly uses the convolutional\nneural network (CNN) to encode the convolutional AU feature\nmaps. Then PMVT obtains the multi-scale AU tokens with\nthe small-patch and large-patch branches. The two branches\nreceive diﬀerent scale AU tokens and exchange semantic AU\ninformation via a cross attention mechanism. The self-/cross-\nattention mechanisms facilitate PMVT the content-dependent\nlong-range interaction perceiving capabilities. Thus, PMVT c an\nﬂexibly focus on the region-speciﬁc AU representations and\nencode the correlations among diﬀerent AUs to enhance the\ndiscriminability of the AU representations. Figure 1 shows the\nattention maps of several faces. It is clear that PMVT is capable of\nfocusing on the critical and AU-related facial regions for a w ide\nrange of identities and races. More facial examples and detail ed\nexplanations can be seen in section 4.2.1.\nIn summary, the contributions of this study are as follows:\n1. We introduce a PMVT for facial AU detection. PMVT does\nnot rely on manually deﬁned facial landmarks to extract the\nregional AU representations.\n2. To further enhance the discriminability of the facial\nexpression representation, PMVT consists of separate\ntransformer branches that receive the multi-scale AU token s\nas input. PMVT is capable of encoding multi-scale facial\nAU representations and perceiving the correlations among\ndiﬀerent AUs to facilitate representing diﬀerent AU ﬂexibly.\n3. Experimental results demonstrate the advantages of the\nproposed PMVT over other state-of-the-art AU detection\nmethods on two popular AU datasets. Visualization results\nshow that PMVT is superior in perceiving and capturing the\nAU-speciﬁc facial regions.\n2. RELATED WORK\nWe focus on the previous studies considering two aspects that ar e\ntightly related to the proposed PMVT, i.e., the facial AU detect ion\nand vision transformer.\n2.1. Methods for Facial AU Detection\nAction units detection is a multi-label classiﬁcation proble m and\nhas been studied for decades. Several AU detection methods\nhave been proposed (\nZhao et al., 2016; Li et al., 2017a,b; Shao\net al., 2018; Li and Shan, 2021 ). To achieve higher AU detection\naccuracy, diﬀerent hand-crafted features have been used to\nencode the characteristics of AUs, such as Histogram of Orie nted\nGradient (HOG), local binary pattern (LBP), Gabor (\nBenitez-\nQuiroz et al., 2016 ) etc. Recently, AU detection has achieved\nconsiderable improvements due to deep learning. Since AU\ncorresponds to the movement of facial muscles, many methods\ndetect the occurrence of AU based on location (\nZhao et al.,\n2016; Li et al., 2017a,b; Shao et al., 2018 ). For example, Zhao\net al. (2016) used a regionally connected convolutional layer\nand learned the region-speciﬁc convolutional ﬁlters from th e\nsub-areas of the face. EAC-Net ( Li et al., 2017b ) and ROI ( Li\net al., 2017a ) extracted AU features around the manually deﬁned\nfacial landmarks that are robust with respect to non-rigid sha pe\nchanges. SEV-Net ( Yang et al., 2021 ) utilized the AU semantic\ndescription as auxiliary information for AU detection. Jacob and\nStenger (2021) used a transformer-based encoder to capture the\nrelationships between AUs. However, these supervised method s\nrely on precisely annotated images and often overﬁt on a speciﬁc\ndataset as a result of insuﬃcient training images.\nRecently, weakly-supervised (\nPeng and Wang, 2018; Zhao\net al., 2018 ) and self-supervised ( Wiles et al., 2018; Li et al., 2019b,\n2020; Lu et al., 2020 ) methods have attracted a lot of attention to\nmitigate the AU data scarcity issue. Weakly supervised metho ds\ntypically use the incomplete AU annotations and learn AU\nclassiﬁers from the prior knowledge between facial expressio n\nand facial AU (\nPeng and Wang, 2018 ). The self-supervised\nlearning approaches usually adopt pseudo supervisory signals to\nlearn facial AU representation without manual AU annotation s\n(\nLi et al., 2019b, 2020; Lu et al., 2020 ). Among them, Lu et al.\n(2020) proposed a triplet ranking loss to learn AU representations\nvia capturing the temporal AU consistency. Fab-Net ( Wiles et al.,\nFrontiers in Neurorobotics | www.frontiersin.org 2 January 2022 | Volume 15 | Article 824592\nWang and Wang PMVT for AU Detection\nFIGURE 1 |Attention maps of some faces. Our proposed PMVT is capable of capturing the AU-speciﬁc facial regions for different ident ities with diverse\nfacial expressions.\n2018) was optimized to map a source facial frame to a target facial\nframe via estimating an optical ﬂow ﬁeld between the source\nand target frames. TCAE ( Li et al., 2019b ) was introduced to\nencode the pose-invariant facial AU representation via predicting\nseparate displacements for pose and AU and using the cycle\nconsistency in the feature and image domains simultaneousl y.\nOur proposed PMVT diﬀers from previous CNN-based\nor transformer-based (\nJacob and Stenger, 2021 ) AU detection\nmethods in two ways. One, PMVT does not rely on facial\nlandmarks to crop the regional AU features. It is because the facial\nlandmarks may suﬀer from considerable misalignments under\nsevere facial poses. Under this condition, the encoded facial\nparts are not part-aligned and will lead to incorrect results. Two,\nPMVT is the multi-scale transformer-based and the self-atte ntion\nand cross-attention mechanisms in PMVT can ﬂexibly focus on a\nsequence of image fragments to encode the correlations amon g\nAUs. PMVT is potentially to obtain better facial AU detection\nperformance than previous approaches. We will verify this in\nsection 4.\n2.2. Vision Transformer\nSelf-attention is capable of improving computer vision models\ndue to its content-dependent interactions and parameter-\nindependent scaling of the receptive ﬁelds, in contrast to\nprevious parameter-dependent scaling and content-independen t\ninteractions of convolutions. Recently, self-attention-b ased\ntransformer models have greatly facilitated research in ma chine\ntranslation and natural language processing tasks (\nWaswani\net al., 2017 ). Transformer architecture has become the de-\nfacto standard for a wide range of applications. The core\nintuition of the original transformer is to obtain self-att ention by\ncomparing a feature to all other features in the input sequence .\nIn detail, features are ﬁrst encoded to obtain a query ( Query)\nand memory [(including key ( Key) and value ( Value)] embedding\nvia linear projections. The product of Query with Key is used\nas the attention weight for Value. A position embedding is also\nintroduced for each input token to remember the positional\ninformation which will be lost in the transformer, which is\nespecially good at capturing long-range dependencies between\ntokens within an input sequence.\nInspired by this, many recent studies use transformers in\nvarious computer vision tasks (\nDosovitskiy et al., 2020; Li\net al., 2021 ). Among them, ViT ( Dosovitskiy et al., 2020 )\nintroduces to view an image as a sequence of tokens and\nconduct image classiﬁcation with a transformer encoder. To\nobtain the input patch features, ViT partition the input image\ninto non-overlapping tokens with 16 × 16 spatial dimension\nand linearly project the tokens to match the encoder’s input\ndimension. DeiT (\nTouvron et al., 2021 ) further proposes\nthe data-eﬃcient training and distillation for transformer-\nbased image classiﬁcation models. DETR (\nCarion et al., 2020 )\nintroduces an excellent object detection model based on the\ntransformer, which considerably simpliﬁes the traditional object\ndetection pipeline and obtains comparable performances with\nprior CNN-based detectors. CrossViT (\nChen et al., 2021 ) encodes\nsmall-patch and large-patch image tokens with two exclusive\nbranches and these image tokens are then fused purely by a\ncross-attention mechanism. Subsequently, transformer mo dels\nare further extended to other popular computer vision tasks\nsuch as segmentation (\nJin et al., 2021 ), face recognition ( Li\net al., 2021 ), and 3D reconstruction ( Lin et al., 2021 ). In\nthis study, we extend CrossViT to facial AU detection and\nshow its feasibility and superiority on two publicly availabl e\nAU datasets.\nFrontiers in Neurorobotics | www.frontiersin.org 3 January 2022 | Volume 15 | Article 824592\nWang and Wang PMVT for AU Detection\nFIGURE 2 |The main idea of the proposed progressive multi-scale visio n transformer (PMVT). With the encoded convolutional featu re map Xcon, PMVT uses L and S\nbranch transformer encoders that each receives tokens with different resolutions as input. The two branches will be fus ed adaptively via cross-attention mechanism.\n3. METHOD\nFigure 2 illustrates the main idea of the proposed PMVT. Given\nan input face, PMVT ﬁrst extracts its convolutional feature\nmaps via a commonly-used backbone network. Second, PMVT\nencodes the discriminative facial AU feature by the multi-s cale\ntransformer blocks. We will ﬁrst review the traditional vis ion\ntransformer and present our proposed PMVT afterward.\n3.1. Revisiting Vision Transformer\nWe ﬁrst revisit the critical components in ViT (\nDosovitskiy et al.,\n2020) that mainly consist of image tokenization and several laye rs\nof the token encoder. Each encoder consists of two layers, i. e.,\nmulti-head self-attention (MSA) layer and feed-forward net work\n(FFN) layer.\nTraditional vision transformer typically receives a sequen ce\nof image patch embeddings as input. To obtain the token\nembeddings, ViT encodes the input image X ∈ RH×W×C\ninto a set of ﬂattened two-dimensional image patches: Xp ∈\nRN×P2×C. Among the mathematic symbols, H W , C denote the\nheight, width, channel of the input image X. P means the spatial\nresolution of each image patch Xp. After the image tokenization,\nwe can obtain N = H×W\nP2 patches that will be treated as the\nsequential input for the transformer. These image patches are\nthen ﬂattened and projected to embeddings with a size of S.\nTypically, ViT adds an extra class token that will be concatena ted\nFrontiers in Neurorobotics | www.frontiersin.org 4 January 2022 | Volume 15 | Article 824592\nWang and Wang PMVT for AU Detection\nwith the image embeddings, resulting in the input sequence\nwith a size of Xt ∈ R(N+1)×S. Finally, the class token will\nserve as the image representation that will be used for image\nclassiﬁcation. ViT uses a residual connection for each enco der.\nThe computation in each encoder can be formulated as:\nXt′ = LN(Xt + MSA(Xt)), (1)\nY = LN(Xt′ + FFN(Xt′)), (2)\nwhere Xt and Y denote the input and output of the encoder. Xt′ is\nthe output of the MSA layer. LN means layer normalization. MSA\nmeans multi-head self-attention which will be described ne xt.\nFor the self-attention module in ViT, the sequential input\ntokens Xt ∈ R(N+1)×S are linearly transformed into Query, Key,\nValue spaces. Typically, Query ∈ R(N+1)×S, Key ∈ R(N+1)×S,\nValue ∈ R(N+1)×S. Afterward, a weighted sum over all values in\nthe sequential tokens is computed as,\nAttention(Quey, Key, Value) = softmax( Query × KeyT\n√\nS\n)Value.\n(3)\nThen a linear projection is conducted to the weighted values\nAttention(Quey, Key, Value). MSA is a natural extension of the\nsingle-head self-attention described above. MSA splits Query,\nKey, Valuefor h times and performs the self-attention mechanism\nin parallel, then maps their concatenated outputs via linear\ntransformation. In addition to the MSA module, ViT exploits\nthe FFN module to conduct dimension adjustment and non-\nlinear transformation on each image token to enhance the\nrepresentation ability of the transformed tokens.\n3.2. Progressive Multi-Scale Transformer\nThe direct tokenization of input images into large patches in\nViT has been found to show its limitations (\nYuan et al., 2021 ).\nOn the one hand, it is diﬃcult to perceive the important low-\nlevel characteristics (e.g., edges, colors, corners) in im ages; On the\nother hand, large CNN kernels for the image tokenization con tain\ntoo many trainable parameters and are often diﬃcult to optimiz e,\nand thus, ViT requires much more training samples. This is\nparticularly impartial for facial AU detection. As AU annotat ion\nis time-consuming, cumbersome, and error-prone. Currently ,\nthe publicly available AU datasets merely contain limited fa cial\nimages. To cope with this issue, we exploit the popular ResNet-\nbased backbone to encode the input facial image X to obtain\nthe convolutional feature map Xcon = F(X), where F means the\nneural operation in the backbone network.\nTo obtain multi-scale tokens from Xcon, we use two separate\nbranch transformer encoder that each receives tokens with\ndiﬀerent resolution as input. We illustrate the main idea of ou r\nproposed PMVT in Figure 2. Mathematically speaking, let us\ndenote the two branches as L and S, respectively. In PMVT,\nthe L branch uses coarse-grained token as input while the S\nbranch directly operates at a much more ﬁne-grained token.\nBoth branches are adaptively fused K times via a cross-attention\nmechanism. Finally, PMVT exploits the CLS token of the L\nand S branches for facial AU detection. For each token within\neach branch, PMVT introduces a trainable position embedding.\nNote that we can use multiple multi-scale transformer encoder s\n(MST) or perform cross-attention times within each MST. We\nwill analyze the performance variations in section 4.2.1.\nFigure 3 illustrates the cross-attention mechanism in PMVT.\nTo eﬀectively fuse the multi-scale AU features, PMVT utilizes\nthe CLS token at each branch (e.g., L branch) as an agent to\nexchange semantic AU information among the patch tokens from\nthe other branch (e.g., S branch) and then project the CLS\ntoken back to its own branch (e.g., L branch). Such operation\nis reasonable because the CLS token in L or S branch already\nlearns semantic features among all patch tokens in its own\nbranch. Thus, interacting with the patch tokens at the other\nbranch can absorb more semantic AU information at a diﬀerent\nscale. We hypothesize such cross-attention mechanism will h elp\nlearn discriminative AU features as diﬀerent AU usually have\ndiﬀerent appearance scopes and there exist correlations among\nthe facial AUs. The multi-scale features will help encode AU s\nmore precisely and PMVT will encode the AU correlations with\nthe self-/cross-attention mechanism.\nTake L for example to show the cross-attention mechanism\nin PMVT. Specially, PMVT uses the CLS token Xl\ncls from the L\nbranch and patch tokens the Xs\ni from S branch for feature fusing.\nPMVT uses Xl\ncls to obtain a query and use Xs\ni to obtain the key and\nvalue. The query, key, value will be transformed into a weighted\nsum overall values in the sequential tokens as that in Equati on\n(3). Notably, such a cross-attention mechanism is similar t o self-\nattention except that the query is obtained from the CLS token in\nanother transformer branch. In Figure 3, f (.) and g(.) mean linear\nprojections that aim the alignment of the feature dimension.\nWe will evaluate the eﬀectiveness of the proposed PMVT in the\nnext section.\n3.3. Training Objective\nWe utilize the multi-label sigmoid cross-entropy loss for\ntraining the facial AU detection model in PMVT, which can be\nformulated as:\nLAU = −\nJ∑\nj\nzj log ˆzj + (1 − zj) log(1 − ˆzj), (4)\nwhere J denotes the number of facial AUs. zj denotes the j-th\nground truth AU annotation of the input AU sample. ˆzj means\nthe predicted AU score. zi ∈ { 0, 1} denotes the annotation with\nrespect to the ith AU. 1 means the AU is active, 0 means inactive.\n4. EXPERIMENT\n4.1. Implementation Details\nWe adopted ResNet-34 (\nHe et al., 2016 ) as the backbone network\nfor PMVT due to its elegant network structure and excellent\nperformance in image classiﬁcation. We chose the output of\nthe third stage as the convolutional feature maps: Xcon ∈\nR14×14×512. For the L branch, the token size is set as N =\n5 × 5 via adaptative pooling operation. For the S branch, the\ntoken size is set as N = 14 × 14. The pre-trained model\nbased on the ImageNet dataset was used for initializing the\nFrontiers in Neurorobotics | www.frontiersin.org 5 January 2022 | Volume 15 | Article 824592\nWang and Wang PMVT for AU Detection\nFIGURE 3 |The main idea of the cross-attention in PMVT. In this study, we show that PMVT utilizes the classiﬁcation ( CLS) token at the L branch as an agent to\nexchange semantic AU information among the patch tokens fro m the S branch. PMVT can also use the CLS token at S to absorb information among the tokens from\nthe L branch.\nFrontiers in Neurorobotics | www.frontiersin.org 6 January 2022 | Volume 15 | Article 824592\nWang and Wang PMVT for AU Detection\nbackbone network. For the transformer part, we use one layer\nof transformer encoder that consists of two-layer cross-at tention.\nWe exploited a batch-based stochastic gradient descent meth od\nto optimize the proposed PMVT. During the training process,\nwe set the batch size as 64 and the initial learning rate as 0.0 02.\nThe momentum was set as 0.9 and the weight decay was set\nas 0.0005.\n4.1.1. Datasets\nFor AU detection, we adopted BP4D (\nZhang et al., 2013 ) and\nDISFA ( Mavadati et al., 2013 ) datasets. Among them, BP4D\nis a spontaneous FACS dataset that consists of 328 videos for\n41 subjects (18 men and 23 women). Eight diﬀerent tasks are\nevaluated on a total of 41 participants, and their spontaneous\nfacial expression variations were recorded in several video s.\nEach participant subject is involved in eight sessions, and th eir\nspontaneous facial expressions were captured in both 2D and\n3D videos. A total of 12 AUs were annotated for the 328\nvideos, and there are approximately 1,40,000 frames with AU\nannotations. DISFA contains 27 participants that consists of 12\nwomen and 15 men. Each subject is asked to watch a 4-min\nvideo to elicit their facial AUs. The facial AUs are annotate d\nwith intensities from 0 to 5. In our experiments, we obtained\nnearly 1,30,000 AU-annotated images in the DISFA dataset by\nconsidering the images with intensities greater than 1 as ac tive.\nFor BP4D and DISFA datasets, the images are split into 3-\nfold in a subject-independent manner. Based on the datasets,\nwe conducted 3-fold cross-validation. We adopted 12 AUs in\nBP4D and 8 AUs in DISFA dataset for evaluation. For the DISFA\ndataset, we leveraged the model trained on BP4D to initializ e the\nbackbone network, following the same experimental setting o f\nLi et al. (2017b) .\n4.1.2. Evaluation Metric\nWe adopted F1-score ( F1 = 2RP\nR+P ) to evaluate the performance of\nthe proposed AU detection method, where R and P, respectively,\ndenote recall and precision. We additionally calculated the\naverage F1-score over all AUs (AVE) to quantitatively evalua te\nthe overall facial AU detection performance. We show the AU\ndetection results as F1 × 100.\n4.2. Experimental Results\nWe compare the proposed with the state-of-the-art facial AU\ndetection approaches, including DRML (\nZhao et al., 2016 ), EAC-\nNet ( Li et al., 2017b ), ROI ( Li et al., 2017a ), JAA-Net ( Shao\net al., 2018 ), OFS-CNN ( Han et al., 2018 ), DSIN ( Corneanu et al.,\n2018), TCAE ( Li et al., 2019b ), TAE ( Li et al., 2020 ), SRERL\n(Li et al., 2019a ), ARL ( Shao et al., 2019 ), SEV-Net ( Yang et al.,\n2021), and FAUT ( Jacob and Stenger, 2021 ). Among them, most\nof the AU methods ( Li et al., 2017a, 2019a; Corneanu et al., 2018;\nShao et al., 2018 ) manually crop the local facial regions to learn\nthe AU-speciﬁc representations with exclusive CNN branches.\nTAE (\nLi et al., 2020 ) utilize unlabeled videos that consist of\napproximately 7,000 subjects to encode the AU-discriminative\nrepresentation without AU annotations. SEV-Net (\nYang et al.,\n2021) introduce the auxiliary semantic word embedding and\nvisual feature for AU detection. FAUT ( Jacob and Stenger, 2021 )\nintroduce an AU correlation network based on a transformer\narchitecture to perceive the relationships between diﬀerent A U\nin an end-to-end manner.\nTable 1 shows the AU detection results of our method\nand studies works on the BP4D dataset. Our PMVT achieves\ncomparable AU detection accuracy with the best state-of-the -\nart AU detection methods in the average F1 score. Compared\nwith other methods, PMVT obtains consistent improvements\nin the average accuracy ( +14.6% over DRML, +7.0% over\nEAC-Net, +6.5% over ROI, +2.9% over JAA-Net, +4.0% over\nDSIN, +6.8% over TCAE, +2.6% over TAE). The beneﬁts of\nour proposed PMVT over other methods can be explained in\n2-fold. First, PMVT explicitly introduces transformer modul es\nin the network structure. The self-attention mechanism in\nthe transformer modules is capable of perceiving the local to\nglobal interactions between diﬀerent facial AUs. Second, we\nuse multi-scale features to better encode the regional feat ures\nof the facial AUs, as diﬀerent AUs have diﬀerent appearance\nscopes. The cross-attention mechanism between the multi-\nscale features is beneﬁcial for learning discriminative fa cial\nAU representations. Table 2 shows the quantitative facial AU\ndetection results of our PMVT and other methods on the\nDISFA dataset. PMVT achieves the second-best AU detection\naccuracy compared with all the state-of-the-art AU detectio n\nmethods in the average F1 score. In detail, PMVT outperforms\nEAC-Net, JAA-Net, OFS-CNN, TCAE, TAE, SRERL, ARL,\nand SEV-Net with +12.4%, +4.9%, +9.5%, +7.3%, +15.9%,\n+9.4%, +5.0%, +2.2%, and +2.1% improvements in the\naverage F1 scores. The consistent improvements over other\nmethods on the two popular datasets verify the feasibility and\nsuperiority of our proposed PVMT. We will carry out an\nablation study to investigate the contribution of the self- /cross-\nattention in PVMT and illustrate visualization results in th e\nnext section.\n4.2.1. Ablation Study\nWe illustrate the ablation study experimental results in Table 3.\nIn Table 3, we show the AU detection performance variations\nwith diﬀerent cross-attention layers ( CL = 1, 2, 3) in the multi-\nscale transformer encoder and with diﬀerent layers of multi- scale\ntransformer encoders ( MS = 1, 2, 3).\nAs shown in Table 3, PMVT shows its best AU detection\nperformance with CL = 2 and MS = 1. It means PMVT\nmerely contains one layer of the multi-scale transformer en coder,\nand the encoder contains two layers of cross-attention. Wit h\nmore MST encoders, PMVT will contain too many trainable\nparameters and will suﬀer from insuﬃcient training images.\nWith CL = 1 or CL = 3, PMVT shows degraded\nAU detection performance, and it suggests that information\nfusion should be performed twice to achieve the discriminati ve\nAU representations.\nWe additionally show the attention maps of PMVT on some\nrandomly sampled faces in Figure 4. The visualization results\nshow the beneﬁts of the proposed PMVT for robust facial AU\ndetection. It is obvious that PVMT shows consistent activation\nmaps for each face under diﬀerent races, expressions, lighting s,\nand identities. For example, the third face in the second row\nFrontiers in Neurorobotics | www.frontiersin.org 7 January 2022 | Volume 15 | Article 824592\nWang and Wang PMVT for AU Detection\nTABLE 1 |Action unit (AU) detection performance of our proposed prog ressive multi-scale vision transformer (PMVT) and state-o f-the-art methods on the BP4D dataset.\nMethods AU1 AU2 AU4 AU6 AU7 AU10 AU12 AU14 AU15 AU17 AU23 AU24 A VE\nLSVM ( Fan et al., 2008 ) 23.2 22.8 23.1 27.2 47.1 77.2 63.7 64.3 18.4 33.0 19.4 20.7 35 .3\nDRML ( Zhao et al., 2016 ) 36.4 41.8 43.0 55.0 67.0 66.3 65.8 54.1 33.2 48.0 31.7 30.0 48 .3\nEAC-Net ( Li et al., 2017b ) 39.0 35.2 48.6 76.1 72.9 81.9 86.2 58.8 37.5 59.1 35.9 35.8 55 .9\nROI ( Li et al., 2017a ) 36.2 31.6 43.4 77.1 73.7 85.0 87.0 62.6 45.7 58.0 38.3 37.4 56 .4\nJAA-Net ( Shao et al., 2018 ) 47.2 44.0 54.9 77.5 74.6 84.0 86.9 61.9 43.6 60.3 42.7 41.9 60 .0\nDSIN ( Corneanu et al., 2018 ) 51.7 40.4 56.0 76.1 73.5 79.9 85.4 62.7 37.3 62.9 38.8 41.6 58 .9\nTCAE ( Li et al., 2019b ) 43.1 32.2 44.4 75.1 70.5 80.8 85.5 61.8 34.7 58.5 37.2 48.7 56 .1\nTAE (Li et al., 2020 ) 47.0 45.9 50.9 74.7 72.0 82.4 85.6 62.3 48.1 62.3 45.9 46.3 60 .3\nSRERL ( Li et al., 2019a ) 46.9 45.3 55.6 77.1 78.4 83.5 87.6 63.9 52.2 63.9 47.1 53.3 62.9\nARL ( Shao et al., 2019 ) 45.8 39.8 55.1 75.7 77.2 82.3 86.6 58.8 47.6 62.1 47.4 55.4 61 .1\nFAUT (Jacob and Stenger, 2021 ) 51.7 49.3 61.0 77.8 79.5 82.9 86.3 67.6 51.9 63.0 43.7 56.3 64.2\nSEV-Net (Yang et al., 2021 ) 58.2 50.4 58.3 81.9 73.9 87.8 87.5 61.6 52.6 62.2 44.6 47.6 63.9\nPMVT (Ours) 59.3 43.0 59.3 82.3 73.6 82.6 86.1 57.6 53.0 60.2 47.9 50.6 62.9\nThe highest values are illustrated in Bold format.\nTABLE 2 |Action unit detection performance of our proposed PMVT and s tate-of-the-art methods on the DISFA dataset.\nMethods AU1 AU2 AU4 AU6 AU9 AU12 AU25 AU26 ave\nDRML ( Zhao et al., 2016 ) 17.3 17.7 37.4 29.0 10.7 37.7 38.5 20.1 26.7\nEAC-Net ( Li et al., 2017b ) 41.5 26.4 66.4 50.7 80.5 89.3 88.9 15.6 48.5\nJAA-Net ( Shao et al., 2018 ) 43.7 46.2 56.0 41.4 44.7 69.6 88.3 58.4 56.0\nOFS-CNN ( Han et al., 2018 ) 43.7 40.0 67.2 59.0 49.7 75.8 72.4 54.8 51.4\nDSIN ( Corneanu et al., 2018 ) 42.4 39.0 68.4 28.6 46.8 70.8 90.4 42.2 53.6\nTCAE ( Li et al., 2019b ) 15.1 15.2 50.5 48.7 23.3 72.1 82.1 52.9 45.0\nTAE (Li et al., 2020 ) 21.4 19.6 64.5 46.8 44.0 73.2 85.1 55.3 51.5\nSRERL ( Li et al., 2019a ) 45.7 47.8 59.6 47.1 45.6 73.5 84.3 43.6 55.9\nFAUT (Jacob and Stenger, 2021 ) 46.1 48.6 72.8 56.7 50.0 72.1 90.8 55.4 61.5\nARL ( Shao et al., 2019 ) 43.9 42.1 63.6 41.8 40.0 76.2 95.2 66.8 58.7\nSEV-Net (Yang et al., 2021 ) 55.3 53.1 61.5 53.6 38.2 71.6 95.7 41.5 58.8\nPMVT (Ours) 50.0 54.3 63.2 55.6 40.0 72.2 95.9 56.3 60.9\nThe highest values are illustrated in Bold format.\nTABLE 3 |Ablation studies on the BP4D and DISFA datasets.\nMethods BP4D DISFA\nCL=1 60.7 56.3\nCL=2 62.9 60.9\nCL=3 59.5 55.8\nMS=1 62.9 60.9\nMS=2 59.8 58.1\nMS=3 55.0 51.1\nis annotated with active AU1 (inner brow raiser), AU2 (outer\nbrow raiser), AU6 (cheek raiser), AU7 (inner brow raiser), AU10\n(inner brow raiser), and AU12 (inner brow raiser). The second\nface in the third row is annotated with active AU1 (inner brow\nraiser), AU10 (inner brow raiser), AU12 (inner brow raiser),\nand AU15 (lip corner depressor). The ﬁrst face in the fourth\nrow is annotated with active AU7 (inner brow raiser) and AU14\n(dimpler). The attention maps of these faces are in line and\nconsistent with the annotated AUs. The visualization maps in\nFigure 4 show the generalization ability and feasibility of our\nproposed PVMT.\n5. CONCLUSIONS\nIn this study, we propose a PMVT to perceive the complex\nrelationships among diﬀerent AUs in an end-to-end data-drive n\nmanner. PMVT is based on the multi-scale self-/cross-attent ion\nmechanism that can ﬂexibly focus on sequential image patches\nto eﬀectively encode the discriminative AU representation and\nperceive the correlations among diﬀerent facial AUs. Compared\nwith previous facial AU detection methods, PMVT obtains\nFrontiers in Neurorobotics | www.frontiersin.org 8 January 2022 | Volume 15 | Article 824592\nWang and Wang PMVT for AU Detection\nFIGURE 4 |Attention maps of some representative faces. We illustrate a subject with different facial expressions in each row. It is obvious that the proposed PMVT is\ncapable of focusing on the most silent parts for facial AU det ection. Deep red denotes high activation, better viewed in c olor and zoom in.\ncomparable AU detection performance. Visualization results\nshow the superiority and feasibility of our proposed PMVT.\nFor future study, we will explore utilizing PMVT for more\naﬀective computing tasks, such as facial expression recogniti on,\nAU density estimation.\nDATA AVAILABILITY STATEMENT\nThe original contributions presented in the study\nare included in the article/supplementary material,\nfurther inquiries can be directed to the corresponding\nauthor/s.\nAUTHOR CONTRIBUTIONS\nCW and ZW cooperatively completed the method design and\nexperiment parts. CW wrote all the sections of the manuscript.\nZW carried out the experiments and gave the detailed analysis .\nBoth the two authors have carefully read, polished, and approve d\nthe ﬁnal manuscript.\nREFERENCES\nBartlett, M. S., Littlewort, G., Fasel, I., and Movellan, J. R. (2003 ). “Real time face\ndetection and facial expression recognition: development and applic ations to\nhuman computer interaction, ” in 2003 Conference on Computer Vision and\nPattern Recognition Workshop , Vol. 5 (Madison, WI: IEEE), 53–53.\nBenitez-Quiroz, C. F., Srinivasan, R., and Martinez, A. M. (201 6). “Emotionet: An\naccurate, real-time algorithm for the automatic annotation of a millio n facial\nexpressions in the wild, ” in Proceedings of the IEEE conference on computer\nvision and pattern recognition (CVPR) , 5562–5570.\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., a nd Zagoruyko,\nS. (2020). “End-to-end object detection with transformers, ” i n European\nConference on Computer Vision (Glasgow: Springer), 213–229.\nChen, C.-F., Fan, Q., and Panda, R. (2021). Crossvit: cross-at tention multi-scale\nvision transformer for image classiﬁcation. arXiv preprint arXiv:2103.14899.\nCorneanu, C., Madadi, M., and Escalera, S. (2018). “Deep structu re inference\nnetwork for facial action unit recognition, ” in ECCV (Munich), 298–313.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., et al. (2020). An image is worth 16x16 words: transformers for imag e\nrecognition at scale. arXiv preprint arXiv:2010.11929.\nFrontiers in Neurorobotics | www.frontiersin.org 9 January 2022 | Volume 15 | Article 824592\nWang and Wang PMVT for AU Detection\nEkman, P., and Friesen, W. V. (1978). Manual for the Facial Action Coding System .\nConsulting Psychologists Press.\nFan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., and Lin, C.-J. (2008). Liblinear:\na library for large linear classiﬁcation. J. Mach. Learn. Res. 9, 1871–1874.\nAvailable online at: https://dl.acm.org/citation.cfm?id=14427 94\nFang, Y., Gao, S., Li, J., Luo, W., He, L., and Hu, B. (2020). Multi -level\nfeature fusion based locality-constrained spatial transformer net work for video\ncrowd counting. Neurocomputing 392, 98–107. doi: 10.1016/j.neucom.2020.\n01.087\nGirdhar, R., Carreira, J., Doersch, C., and Zisserman, A. (2019). “ Video\naction transformer network, ” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , (Long Beach, CA: IEEE),\n244–253.\nHan, S., Meng, Z., Li, Z., O’Reilly, J., Cai, J., Wang, X., et al. (2 018). “Optimizing\nﬁlter size in convolutional neural networks for facial action un it recognition, ” in\nProceedings of the IEEE Conference on Computer Vision and Pat tern Recognition\n(Salt Lake City, UT), 5070–5078.\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). “Deep residual learn ing for image\nrecognition, ” in Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 770–778.\nJacob, G. M., and Stenger, B. (2021). “Facial action unit det ection with\ntransformers, ” inProceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , 7680–7689.\nJin, Y., Han, D., and Ko, H. (2021). Trseg: transformer for semanti c segmentation.\nPattern Recognit. Lett. 148, 29–35. doi: 10.1016/j.patrec.2021.04.024\nLi, G., Zhu, X., Zeng, Y., Wang, Q., and Lin, L. (2019a). “Semant ic relationships\nguided representation learning for facial action unit recognitio n, ” in AAAI,\nvol. 33, 8594–8601.\nLi, W., Abtahi, F., and Zhu, Z. (2017a). “Action unit detecti on with region\nadaptation, multi-labeling learning and optimal temporal fusing, ” in CVPR\n(Honolulu, HI: IEEE).\nLi, W., Abtahi, F., Zhu, Z., and Yin, L. (2017b). “Eac-net: a re gion-based deep\nenhancing and cropping approach for facial action unit detection, ” in FG\n(Washington, DC).\nLi, Y., and Shan, S. (2021). Meta auxiliary learning for facial ac tion unit detection.\narXiv preprint arXiv:2105.06620.\nLi, Y., Sun, Y., Cui, Z., Shan, S., and Yang, J. (2021). Learnin g fair\nface representation with progressive cross transformer. arXiv preprint\narXiv:2108.04983.\nLi, Y., Zeng, J., and Shan, S. (2020). Learning representations f or facial actions\nfrom unlabeled videos. IEEE Trans. Pattern Anal. Mach. Intell. 44, 302–317.\ndoi: 10.1109/TPAMI.2020.3011063\nLi, Y., Zeng, J., Shan, S., and Chen, X. (2018a). Occlusion aware\nfacial expression recognition using cnn with attention mechani sm.\nIEEE Trans. Image Process. 28, 2439–2450. doi: 10.1109/TIP.2018.28\n86767\nLi, Y., Zeng, J., Shan, S., and Chen, X. (2018b). “Patch-gated cnn for occlusion-\naware facial expression recognition, ” in 2018 24th International Conference on\nPattern Recognition (ICPR) (Beijing: IEEE), 2209–2214.\nLi, Y., Zeng, J., Shan, S., and Chen, X. (2019b). “Self-supervis ed representation\nlearning from videos for facial action unit detection, ” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognit ion (Long\nBeach, CA), 10924–10933.\nLin, K., Wang, L., and Liu, Z. (2021). “End-to-end human pose an d\nmesh reconstruction with transformers, ” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (Long Beach, CA), 19\n54–1963.\nLu, L., Tavabi, L., and Soleymani, M. (2020). “Self-supervised le arning for facial\naction unit recognition through temporal consistency, ” in Proceedings of the\nBritish Machine Vision Conference (BMVC). BMVA Press .\nMavadati, S. M., Mahoor, M. H., Bartlett, K., Trinh, P., and Cohn, J. F. (2013). Disfa:\na spontaneous facial action intensity database. IEEE Trans. Aﬀect. Comput. 4,\n151–160. doi: 10.1109/T-AFFC.2013.4\nPeng, G., and Wang, S. (2018). “Weakly supervised facial actio n unit recognition\nthrough adversarial training, ” in CVPR (Salt Lake City, UT), 2188–2196.\nShao, Z., Liu, Z., Cai, J., and Ma, L. (2018). “Deep adaptive at tention for joint facial\naction unit detection and face alignment, ” in ECCV Munich.\nShao, Z., Liu, Z., Cai, J., Wu, Y., and Ma, L. (2019). Facial ac tion unit\ndetection using attention and relation learning. IEEE Trans. Aﬀect. Comput.\ndoi: 10.1109/TAFFC.2019.2948635\nTouvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Je gou, H.\n(2021).“Training data-eﬃcient image transformers and distillati on through\nattention, ” in Proceedings of the 38th International Conference on Machine\nLearning (ICML), 10347–10357.\nWaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. et al.\n(2017).“Attention is all you need, ” in Proceedings of the Conference on Neural\nInformation Processing Systems (Long Beach, CA), 1–11.\nWiles, O., Koepke, A., and Zisserman, A. (2018). Self-supervised le arning of a facial\nattribute embedding from video. arXiv preprint arXiv:1808.06882.\nYang, H., Yin, L., Zhou, Y., and Gu, J. (2021). “Exploiting semant ic embedding\nand visual feature for facial action unit detection, ” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognit ion (Nashville,\nTN), 10482–10491.\nYuan, K., Guo, S., Liu, Z., Zhou, A., Yu, F., and Wu, W. (2021). Incorporating\nconvolution designs into visual transformers. arXiv preprint arXiv:2103.11816.\nZafar, Z., and Khan, N. A. (2014). “Pain intensity evaluation through facial\naction units, ” in 2014 22nd International Conference on Pattern Recognition\n(Stockholm:IEEE), 4696–4701.\nZhang, X., Yin, L., Cohn, J. F., Canavan, S., Reale, M., Horowitz, A., and Liu, P.\n(2013). “A high-resolution spontaneous 3d dynamic facial express ion database, ”\nin FG (Shanghai: IEEE).\nZhao, K., Chu, W.-S., and Martinez, A. M. (2018). “Learning fac ial action units\nfrom web images with scalable weakly supervised clustering, ” in CVPR (Salt\nLake City, UT), 2090–2099.\nZhao, K., Chu, W.-S., and Zhang, H. (2016). “Deep region and multi -label learning\nfor facial action unit detection, ” in CVPR (Las Vegas, NV), 3391–3399.\nConﬂict of Interest: The authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be c onstrued as a\npotential conﬂict of interest.\nPublisher’s Note:All claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their aﬃliated organizat ions, or those of\nthe publisher, the editors and the reviewers. Any product that may b e evaluated in\nthis article, or claim that may be made by its manufacturer, is not gua ranteed or\nendorsed by the publisher.\nCopyright © 2022 Wang and Wang. This is an open-access article dis tributed\nunder the terms of the Creative Commons Attribution License (CC BY). The use,\ndistribution or reproduction in other forums is permitted, p rovided the original\nauthor(s) and the copyright owner(s) are credited and that th e original publication\nin this journal is cited, in accordance with accepted academ ic practice. No use,\ndistribution or reproduction is permitted which does not co mply with these terms.\nFrontiers in Neurorobotics | www.frontiersin.org 10 January 2022 | Volume 15 | Article 824592",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8378041982650757
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7779716849327087
    },
    {
      "name": "Discriminative model",
      "score": 0.552094042301178
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5189154148101807
    },
    {
      "name": "Visualization",
      "score": 0.505434513092041
    },
    {
      "name": "ENCODE",
      "score": 0.5008232593536377
    },
    {
      "name": "Face detection",
      "score": 0.4731144607067108
    },
    {
      "name": "Computer vision",
      "score": 0.4639071822166443
    },
    {
      "name": "Facial recognition system",
      "score": 0.3599545359611511
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I125839683",
      "name": "Beijing Institute of Technology",
      "country": "CN"
    }
  ]
}