{
  "title": "GATE: Graph Attention Transformer Encoder for Cross-lingual Relation and Event Extraction",
  "url": "https://openalex.org/W3091998909",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2786891118",
      "name": "Wasi Uddin Ahmad",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2147504447",
      "name": "Nanyun Peng",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2208999240",
      "name": "Kai-Wei Chang",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2915444840",
    "https://openalex.org/W2973533024",
    "https://openalex.org/W6668750346",
    "https://openalex.org/W2888597155",
    "https://openalex.org/W3014518624",
    "https://openalex.org/W2250999640",
    "https://openalex.org/W2927229913",
    "https://openalex.org/W2983040767",
    "https://openalex.org/W2986267869",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W6622855691",
    "https://openalex.org/W2249610072",
    "https://openalex.org/W2949212908",
    "https://openalex.org/W2970170773",
    "https://openalex.org/W3085395857",
    "https://openalex.org/W6666761814",
    "https://openalex.org/W2130714105",
    "https://openalex.org/W2573923070",
    "https://openalex.org/W3092186712",
    "https://openalex.org/W6740650839",
    "https://openalex.org/W2165516035",
    "https://openalex.org/W2891896107",
    "https://openalex.org/W2141891248",
    "https://openalex.org/W2153199128",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W2574872930",
    "https://openalex.org/W2134033474",
    "https://openalex.org/W2164343063",
    "https://openalex.org/W2165962657",
    "https://openalex.org/W2108743083",
    "https://openalex.org/W2251251652",
    "https://openalex.org/W2739722817",
    "https://openalex.org/W2184957013",
    "https://openalex.org/W2788322837",
    "https://openalex.org/W2971036674",
    "https://openalex.org/W2891553865",
    "https://openalex.org/W2890776849",
    "https://openalex.org/W2971194156",
    "https://openalex.org/W2229639163",
    "https://openalex.org/W2475245295",
    "https://openalex.org/W2250575108",
    "https://openalex.org/W2788474500",
    "https://openalex.org/W3102086967",
    "https://openalex.org/W6657648587",
    "https://openalex.org/W6723544798",
    "https://openalex.org/W2116582594",
    "https://openalex.org/W2922197727",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W2971145411",
    "https://openalex.org/W2951231735",
    "https://openalex.org/W3035740499",
    "https://openalex.org/W2849265501",
    "https://openalex.org/W2970550739",
    "https://openalex.org/W2474272410",
    "https://openalex.org/W2250521169",
    "https://openalex.org/W2971220558",
    "https://openalex.org/W2885177932",
    "https://openalex.org/W2942904230",
    "https://openalex.org/W2892094955",
    "https://openalex.org/W2163141428",
    "https://openalex.org/W2823100154",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W4285798792",
    "https://openalex.org/W2741029840",
    "https://openalex.org/W2493109812",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2964167098",
    "https://openalex.org/W2028509346",
    "https://openalex.org/W2964206023",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W2612364175",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2963020213",
    "https://openalex.org/W2988692181",
    "https://openalex.org/W2963360413",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2952768586",
    "https://openalex.org/W2953049305",
    "https://openalex.org/W2963997908",
    "https://openalex.org/W3101620491",
    "https://openalex.org/W2139813029",
    "https://openalex.org/W3035589854",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W3104725668",
    "https://openalex.org/W2072628044",
    "https://openalex.org/W803770162"
  ],
  "abstract": "Recent progress in cross-lingual relation and event extraction use graph convolutional networks (GCNs) with universal dependency parses to learn language-agnostic sentence representations such that models trained on one language can be applied to other languages. However, GCNs struggle to model words with long-range dependencies or are not directly connected in the dependency tree. To address these challenges, we propose to utilize the self-attention mechanism where we explicitly fuse structural information to learn the dependencies between words with different syntactic distances. We introduce GATE, a Graph Attention Transformer Encoder, and test its cross-lingual transferability on relation and event extraction tasks. We perform experiments on the ACE05 dataset that includes three typologically different languages: English, Chinese, and Arabic. The evaluation results show that GATE outperforms three recently proposed methods by a large margin. Our detailed analysis reveals that due to the reliance on syntactic dependencies, GATE produces robust representations that facilitate transfer across languages.",
  "full_text": "GATE: Graph Attention Transformer Encoder for Cross-lingual\nRelation and Event Extraction\nWasi Uddin Ahmad, Nanyun Peng, Kai-Wei Chang\nUniversity of California, Los Angeles\nfwasiahmad, violetpeng, kwchangg@cs.ucla.edu\nAbstract\nRecent progress in cross-lingual relation and event extraction\nuse graph convolutional networks (GCNs) with universal de-\npendency parses to learn language-agnostic sentence repre-\nsentations such that models trained on one language can be\napplied to other languages. However, GCNs struggle to model\nwords with long-range dependencies or are not directly con-\nnected in the dependency tree. To address these challenges,\nwe propose to utilize the self-attention mechanism where we\nexplicitly fuse structural information to learn the dependen-\ncies between words with different syntactic distances. We in-\ntroduce GATE, aGraph Attention Transformer Encoder, and\ntest its cross-lingual transferability on relation and event ex-\ntraction tasks. We perform experiments on the ACE05 dataset\nthat includes three typologically different languages: English,\nChinese, and Arabic. The evaluation results show that GATE\noutperforms three recently proposed methods by a large mar-\ngin. Our detailed analysis reveals that due to the reliance on\nsyntactic dependencies, GATE produces robust representa-\ntions that facilitate transfer across languages.\n1 Introduction\nRelation and event extraction are two challenging informa-\ntion extraction (IE) tasks; wherein a model learns to identify\nsemantic relationships between entities and events in nar-\nratives. They provide useful information for many natural\nlanguage processing (NLP) applications such as knowledge\ngraph completion (Lin et al. 2015) and question answering\n(Chen et al. 2019). Figure 1 gives an example of relation\nand event extraction tasks. Recent advances in cross-lingual\ntransfer learning approaches for relation and event extraction\nlearns a universal encoder that produces language-agnostic\ncontextualized representations so the model learned on one\nlanguage can easily transfer to others. Recent works (Huang\net al. 2018; Subburathinam et al. 2019) suggested embed-\nding universal dependency structure into contextual repre-\nsentations improves cross-lingual transfer for IE.\nThere are a couple of advantages of leveraging depen-\ndency structures. First, the syntactic distance between two\nwords1 in a sentence is typically smaller than the sequen-\ntial distance. For example, in the sentence A ﬁre in a\nCopyright © 2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n1The shortest path in the dependency graph structure.\nFigure 1: A relation (red dashed) between two entities and\nan event of type Attack (triggered by “ﬁring”) including two\narguments and their role labels (blue) are highlighted.\nBangladeshi garment factory has left at least 37 people dead\nand 100 hospitalized, the sequential and syntactic distance\nbetween “ﬁre” and “hospitalized” is 15 and 4, respectively.\nTherefore, encoding syntax structure helps capture long-\nrange dependencies (Liu, Luo, and Huang 2018). Second,\nlanguages have different word order, e.g., adjectives precede\nor follow nouns as (“red apple”) in English or (“pomme\nrouge”) in French. Thus, processing sentences sequentially\nsuffers from the word order difference issue (Ahmad et al.\n2019a), while modeling dependency structures can mitigate\nthe problem in cross-lingual transfer (Liu et al. 2019).\nA common way to leverage dependency structures for\ncross-lingual NLP tasks is using universal dependency\nparses.2 A large pool of recent works in IE (Liu, Luo, and\nHuang 2018; Zhang, Qi, and Manning 2018; Subburathi-\nnam et al. 2019; Fu, Li, and Ma 2019; Sun et al. 2019;\nLiu et al. 2019) employed Graph Convolutional Networks\n(GCNs) (Kipf and Welling 2017) to learn sentence represen-\ntations based on their universal dependency parses, where a\nk-layers GCN aggregates information of words that are k\nhop away. Such a way of embedding structure may hinder\ncross-lingual transfer when the source and target languages\nhave different path length distributions among words (see\nTable 1). Presumably, a two-layer GCN would work well on\nEnglish but may not transfer well to Arabic.\nMoreover, GCNs have shown to perform poorly in mod-\neling long-distance dependencies or disconnected words in\nthe dependency tree (Zhang, Li, and Song 2019; Tang et al.\n2020). In contrast, the self-attention mechanism (Vaswani\net al. 2017) is capable of capturing long-range dependencies.\nConsequently, a few recent studies proposed dependency-\n2https://universaldependencies.org/\nTheThi rty-Fi fth AAA ICon ferenceon A rti fi ci al Intellig ence(AAAI-21)\n12462\naware self-attention and found effective for machine trans-\nlation (Deguchi, Tamura, and Ninomiya 2019; Bugliarello\nand Okazaki 2020). The key idea is to allow attention be-\ntween connected words in the dependency tree and gradu-\nally aggregate information across layers. However, IE tasks\nare relatively low-resource (the number of annotated docu-\nments available for training is small), and thus stacking more\nlayers is not feasible. Besides, our preliminary analysis indi-\ncates that syntactic distance between entities could charac-\nterize certain relation and event types. 3 Hence, we propose\nto allow attention between all words but use the pairwise\nsyntactic distances to weigh the attention.\nWe introduce a Graph Attention Transformer Encoder\n(GATE) that utilizes self-attention (Vaswani et al. 2017) to\nlearn structured contextual representations. On one hand,\nGATE enjoys the capability of capturing long-range depen-\ndencies, which is crucial for languages with longer sen-\ntences, e.g., Arabic. 4 On the other hand, GATE is agnos-\ntic to language word order as it uses syntactic distance to\nmodel pairwise relationship between words. This charac-\nteristic makes GATE suitable to transfer across typologi-\ncally diverse languages, e.g., English to Arabic. One crucial\nproperty of GATE is that it allows information propagation\namong different heads in the multi-head attention structure\nbased on syntactic distances, which allows to learn the cor-\nrelation between different mention types and target labels.\nWe conduct experiments on cross-lingual transfer among\nEnglish, Chinese, and Arabic languages using the ACE 2005\nbenchmark (Walker et al. 2006). The experimental results\ndemonstrate that GATE outperforms three recently proposed\nrelation and event extraction methods by a signiﬁcant mar-\ngin.5 We perform a thorough ablation study and analysis,\nwhich shows that GATE is less sensitive to source lan-\nguage’s characteristics (e.g., word order, sentence structure)\nand thus excels in the cross-lingual transfer.\n2 Task Description\nIn this paper, we focus on sentence-level relation extraction\n(Subburathinam et al. 2019; Ni and Florian 2019) and event\nextraction (Subburathinam et al. 2019; Liu et al. 2019) tasks.\nBelow, we ﬁrst introduce the basic concepts, the notations,\nas well as deﬁne the problem and the scope of the work.\nRelation Extraction is the task of identifying the relation\ntype of an ordered pair of entity mentions. Formally, given a\npair of entity mentions from a sentence s- (es;eo; s) where\nes and eo denoted as the subject and object entities respec-\ntively, the relation extraction (RE) task is deﬁned as predict-\ning the relationr2R[fNonegbetween the entity mentions,\nwhere Ris a pre-deﬁned set of relation types. In the example\n3In ACE 2005 dataset, the relation type PHYS:Located ex-\nists among fPER, ORG, LOC, FAC, GPEg entities. The aver-\nage syntactic distance in English and Arabic sentences amongPER\nand any of the fLOC, FAC, GPEg entities are approx. 2.8 and\n4.2, while the distance between PER and ORG is 3.3 and 1.5.\n4After tokenization, on average, ACE 2005 English and Arabic\nsentences have approximately 30 and 210 words, respectively.\n5Code available at https://github.com/wasiahmad/GATE\nprovided in Figure 1, there is a PHYS:Located relation\nbetween the entity mentions “Terrorists” and “hotel”.\nEvent Extraction can be decomposed into two sub-tasks,\nEvent Detection and Event Argument Role Labeling. Event\ndetection refers to the task of identifying event triggers (the\nwords or phrases that express event occurrences) and their\ntypes. In the example shown in Figure 1, the word “ﬁring”\ntriggers the Attack event.\nEvent argument role labeling (EARL) is deﬁned as pre-\ndicting whether words or phrases (arguments) participate in\nevents and their roles. Formally, given an event trigger et\nand a mention ea (an entity, time expression, or value) from\na sentence s, the argument role labeling refers to predicting\nthe mention’s roler2R[fNoneg, where Ris a pre-deﬁned\nset of role labels. In Figure 1, the “Terrorists” and “hotel” en-\ntities are the arguments of the Attack event and they have\nthe Attacker and Place role labels, respectively.\nIn this work, we focus on the EARL task; we assume\nevent mentions (triggers) of the input sentence are provided.\nZero-Short Cross-Lingual Transferrefers to the setting,\nwhere there is no labeled examples available for the target\nlanguage. We train neural relation extraction and event ar-\ngument role labeling models on one (single-source) or mul-\ntiple (multi-source) source languages and then deploy the\nmodels in target languages. The overall cross-lingual trans-\nfer approach consists of four steps:\n1. Convert the input sentence into a language-universal\ntree structure using an off-the-shelf universal dependency\nparser, e.g., UDPipe6 (Straka and Strakov´a 2017).\n2. Embed the words in the sentence into a shared seman-\ntic space across languages. We use off-the-shelf multi-\nlingual contextual encoders (Devlin et al. 2019; Conneau\net al. 2020) to form the word representations. To enrich\nthe word representations, we concatenate them with uni-\nversal part-of-speech (POS) tag, dependency relation, and\nentity type embeddings (Subburathinam et al. 2019). We\ncollectively refer them as language-universal features.\n3. Based on the word representations, we encode the input\nsentence using the proposed GATE architecture that lever-\nages the syntactic depth and distance information. Note\nthat this step is the main focus of this work.\n4. A pair of classiﬁer predicts the target relation and argu-\nment role labels based on the encoded representations.\n3 Approach\nOur proposed approach GATE revises the multi-head at-\ntention architecture in Transformer Encoder (Vaswani et al.\n2017) to model syntactic information while encoding a se-\nquence of input vectors (represent the words in a sentence)\ninto contextualized representations. We ﬁrst review the stan-\ndard multi-head attention mechanism (§3.1). Then, we in-\ntroduce our proposed method GATE (§3.2). Finally, we de-\nscribe how we perform relation extraction (§3.3) and event\nargument role labeling (§3.4) tasks.\n6http://ufal.mff.cuni.cz/udpipe\n12463\n3.1 Transformer Encoder\nUnlike recent works (Zhang, Qi, and Manning 2018; Sub-\nburathinam et al. 2019) that use GCNs (Kipf and Welling\n2017) to encode the input sequences into contextualized rep-\nresentations, we propose to employ Transformer encoder as\nit excels in capturing long-range dependencies. First, the se-\nquence of input word vectors,x= [x1;:::;x jxj] where xi 2\nRd are packed into a matrixH0 = [x1;:::;x jxj]. Then anL-\nlayer Transformer Encoder Hl = Transformerl(Hl\u00001),\nl2[1;L] takes H0 as input and generates different levels of\nlatent representations Hl = [hl\n1;:::;h l\njxj], recursively. Typ-\nically the latent representations generated by the last layer\n(L-th layer) are used as the contextual representations of the\ninput words. To aggregate the output vectors of the previ-\nous layer, multiple ( nh) self-attention heads are employed\nin each Transformer layer. For the l-th Transformer layer,\nthe output of the previous layer Hl\u00001 2 Rjxj\u0002dmodel is\nﬁrst linearly projected to queries Q, keys K, and values\nV using parameter matrices WQ\nl ;WK\nl 2Rdmodel\u0002dk and\nWV\nl 2Rdmodel\u0002dv , respectively.\nQl = Hl\u00001WQ\nl ;Kl = Hl\u00001WK\nl ;Vl = Hl\u00001WV\nl :\nThe output of a self-attention head Al is computed as:\nAl = softmax\n\u0012QKT\npdk\n+ M\n\u0013\nVl; (1)\nwhere the matrix M 2Rjxj\u0002jxjdetermines whether a pair\nof tokens can attend each other.\nMij =\n\u001a0; allow to attend\n\u00001; prevent from attending (2)\nThe matrix M is deduced as a mask. By default, the ma-\ntrix M is a zero-matrix. In the next section, we discuss how\nwe manipulate the mask matrix M to incorporate syntactic\ndepth and distance information in sentence representations.\n3.2 Graph Attention Transformer Encoder\nThe self-attention as described in §3.1 learns how much at-\ntention to put on words in a text sequence when encoding\na word at a given position. In this work, we revise the self-\nattention mechanism such that it takes into account the syn-\ntactic structure and distances when a token attends to all\nthe other tokens. The key idea is to manipulate the mask\nmatrix to impose the graph structure and retroﬁt the atten-\ntion weights based on pairwise syntactic distances. We use\nthe universal dependency parse of a sentence and compute\nthe syntactic (shortest path) distances between every pair of\nwords. We illustrate an example in Figure 2.\nWe denote distance matrixD2Rjxj\u0002jxjwhere Dij repre-\nsents the syntactic distance between words at position iand\njin the input sequence. If we want to allow tokens to attend\ntheir adjacent tokens (that are1 hop away) at each layer, then\nwe can set the mask matrix as follows.\nMij =\n\u001a0; D ij = 1\n\u00001; otherwise\nWe generalize this notion to model a distance based atten-\ntion; allowing tokens to attend tokens that are within dis-\nFigure 2: Distance matrix showing the shortest path dis-\ntances between all pairs of words. The dependency arc di-\nrection is ignored while computing pairwise distances. The\ndiagonal value is set to 1, indicating a self-loop. If we set\nthe values in white cells (with value > 1) to 0, the distance\nmatrix becomes an adjacency matrix.\ntance \u000e(hyper-parameter).\nMij =\n\u001a0; D ij \u0014\u000e\n\u00001; otherwise (3)\nDuring our preliminary analysis, we observed that syn-\ntactic distances between entity mentions or event mentions\noften correlate with the target label. For example, if an ORG\nentity mention appears closer to aPER entity than a LOC en-\ntity, then thefPER, ORGgentity pair is more likely to have\nthe PHYS:Located relation. We hypothesize that model-\ning syntactic distance between words can help to identify\ncomplex semantic structure such as events and entity rela-\ntions. Hence we revise the attention head Al (deﬁned in Eq.\n(1)) computation as follows.\nAl = F\n\u0012\nsoftmax\n\u0012QKT\npdk\n+ M\n\u0013\u0013\nVl: (4)\nHere, softmax produces an attention matrix P 2Rjxj\u0002jxj\nwhere Pi denotes the attentions thati-th token pays to the all\nthe tokens in the sentence, and F is a function that modiﬁes\nthose attention weights. We can treat F as a parameterized\nfunction that can be learned based on distances. However,\nwe adopt a simple formulation of F such that GATE pays\nmore attention to tokens that are closer and less attention\nto tokens that are faraway in the parse tree. We deﬁne the\n(i;j)-th element of the attention matrix produced by F as:\nF(P)ij = Pij\nZiDij\n; (5)\nwhere Zi = P\nj\nPij\nDij\nis the normalization factor and Dij\nis the distance between i-th and j-th token. We found this\nformulation of F effective for the IE tasks.\n12464\n3.3 Relation Extractor\nRelation Extractor predicts the relationship label (or None)\nfor each mention pair in a sentence. For an input sen-\ntence s, GATE produces contextualized word representa-\ntions hl\n1;:::;h l\njxj where hl\ni 2 Rdmodel . As different sen-\ntences and entity mentions may have different lengths, we\nperform max-pooling over their contextual representations\nto obtain ﬁxed-length vectors. Suppose for a pair of entity\nmentions es = [hl\nbs;:::;h l\nes] and eo = [hl\nbo;:::;h l\neo], we\nobtain single vector representations ^es and ^eo by performing\nmax-pooling. Following Zhang, Qi, and Manning (2018);\nSubburathinam et al. (2019), we also obtain a vector repre-\nsentation for the sentence, ^sby applying max-pooling over\n[hl\n1;:::;h l\njxj] and concatenate the three vectors. Then the\nconcatenation of the three vectors [^es; ^eo; ^s] are fed to a lin-\near classiﬁer followed by a Softmax layer to predict the re-\nlation type between entity mentions es and eo as follows.\nOr = softmax(WT\nr [ ^es; ^eo; ^s] +br);\nwhere Wr 2R3dmodel\u0002r and br 2Rr are parameters, and\nr is the total number of relation types. The probability of\nt-th relation type is denoted as P(rtjs;es;eo), which corre-\nsponds to the t-th element of Or. To train the relation ex-\ntractor, we adopt the cross-entropy loss.\nLr = \u0000\nNX\ns=1\nNX\no=1\nlog(P(yr\nsojs;es;eo));\nwhere N is the number of entity mentions in the input sen-\ntence s and yr\nso denotes the ground truth relation type be-\ntween entity mentions es and eo.\n3.4 Event Argument Role Labeler\nEvent argument role labeler predicts the argument men-\ntions (or None for non-argument mentions) of an event\nmention and assigns a role label to each argument from a\npre-deﬁned set of labels. To label an argument candidate\nea = [hl\nba;:::;h l\nea] for an event trigger et = [hl\nbt;:::;h l\net]\nin sentence s = [hl\n1;:::;h l\njxj], we apply max-pooling to\nform vectors ^ea, ^et, and ^s respectively, which is same as\nthat for relation extraction. Then we concatenate the vectors\n([ ^et; ^ea; ^s]) and pass it through a linear classiﬁer and Soft-\nmax layer to predict the role label as follows.\nOa = softmax(WT\na [ ^et; ^ea; ^s] +ba);\nwhere Wa 2R3dmodel\u0002r and ba 2Rr are parameters, andr\nis the total number of argument role label types. We optimize\nthe role labeler by minimizing the cross-entropy loss.\n4 Experiment Setup\nDataset We conduct experiments based on the Automatic\nContent Extraction (ACE) 2005 corpus (Walker et al. 2006)\nthat includes manual annotation of relation and event men-\ntions (with their arguments) in three languages: English\n(En), Chinese (Zh), and Arabic (Ar). We present the data\nstatistics in Appendix. ACE deﬁnes an ontology that in-\ncludes 7 entity types, 18 relation subtypes, and 33 event\nsubtypes. We add a class label None to denote that two en-\nSequential Syntactic\nEn Zh\nAr En Zh\nAr\nRelation Mention 4.8 3.9\n25.8 2.2 2.6\n5.1\nEvent Mention & Arg. 9.8 21.7\n58.1 3.1 4.6\n12.3\nTable 1: Average sequential and syntactic (shortest path)\ndistance between relation mentions and event mentions and\ntheir candidate arguments in ACE05 dataset. Distances are\ncomputed by ignoring the order of mentions.\ntity mentions or a pair of an event mention and an argument\ncandidate under consideration do not have a relationship be-\nlong to the target ontology. We use the same dataset split as\nSubburathinam et al. (2019) and follow their preprocessing\nsteps. We refer the readers to Subburathinam et al. (2019)\nfor the dataset preprocessing details.\nEvaluation Criteria Following the previous works (Ji and\nGrishman 2008; Li, Ji, and Huang 2013; Li and Ji 2014;\nSubburathinam et al. 2019), we set the evaluation criteria\nas, (1) a relation mention is correct if its predicted type and\nthe head offsets of the two associated entity mentions are\ncorrect, and (2) an event argument role label is correct if the\nevent type, offsets, and argument role label match any of the\nreference argument mentions.\nBaseline Models To compare GATE on relation and event\nargument role labeling tasks, we chose three recently pro-\nposed approaches as baselines. The source code of the base-\nlines are not publicly available at the time this research is\nconducted. Therefore, we reimplemented them.\n\u000fCL\nTrans GCN (Liu et al. 2019) is a context-dependent\nlexical mapping approach where each word in a source lan-\nguage sentence is mapped to its best-suited translation in\nthe target language. We use multilingual word embeddings\n(Joulin et al. 2018) as the continuous representations of to-\nkens along with the language-universal features embeddings\nincluding part-of-speech (POS) tag embedding, dependency\nrelation label embedding, and entity type embedding.7 Since\nthis model focuses on the target language, we train this base-\nline for each combination of source and target languages.\n\u000fCL\nGCN (Subburathinam et al. 2019) uses GCN (Kipf\nand Welling 2017) to learn structured common space repre-\nsentation. To embed the tokens in an input sentence, we use\nmultilingual contextual representations (Devlin et al. 2019;\nConneau et al. 2020) and the language-universal feature em-\nbeddings. We train this baseline on the source languages and\ndirectly evaluate on the target languages.\n\u000fCL\nRNN (Ni and Florian 2019) uses a bidirectional Long\nShort-Term Memory (LSTM) type recurrnet neural net-\nworks (Hochreiter and Schmidhuber 1997) to learn contex-\ntual representation. We feed language-universal features for\nwords in a sentence, constructed in the same way as Subbu-\nrathinam et al. (2019). We train and evaluate this baseline in\nthe same way as CL\nGCN.\nIn addition to the above three baseline methods, we com-\npare GATE with the following two encoding methods.\n7Due to the design principle of Liu et al. (2019), we cannot use\nmultilingual contextual encoders in CL Trans GCN.\n12465\nModel\nEvent Argument Role Labeling Relation Extraction\nEn En Zh Zh Ar Ar En En Zh Zh Ar Ar\n+ + + + + + + + + + + +\nZh Ar En Ar En Zh Zh Ar En Ar En Zh\nCL Trans GCN 41.8 55.6 41.2 52.9 39.6 40.8 56.7 65.3 65.9 59.7 59.6 46.3\nCL GCN 51.9 50.4 53.7 51.5 50.3 51.9 49.4 58.3 65.0 55.0 56.7 42.4\nCL RNN 60.4 53.9 55.7 52.5 50.7 50.9 53.7 63.9 70.9 57.6 67.1 55.7\nTransformer 61.5 55.0 58.0 57.7 54.3 57.0 57.1 63.4 69.6 60.6 67.0 52.6\nTransformer RPR 62.3 60.8 57.3 66.3 57.5 59.8 58.0 59.9 70.0 55.6 66.5 56.5\nGATE (this work) 63.2 68.5 59.3 69.2 53.9 57.8 55.1 66.8 71.5 61.2 69.0 54.3\nTable 2: Single-source transfer results (F-score % on the test set) using perfect event triggers and entity mentions. The language\non top and bottom of +denotes the source and target languages, respectively.\nModel\nfEn, Zhg fEn, Arg fZh, Arg\n+ + +\nAr Zh En\nEvent Argument Role Labeling\nCL Trans GCN 57.0 44.5 44.8\nCL GCN 58.9 56.2 57.9\nCL RNN 53.5 62.5 60.8\nTransformer 59.5 62.0 60.7\nTransformer RPR 71.1 68.4 62.2\nGATE (this work) 73.9 65.3 61.3\nRelation Extraction\nCL Trans GCN 66.8 54.4 69.5\nCL GCN 64.0 46.6 65.8\nCL RNN 66.5 60.5 73.0\nTransformer 68.3 59.3 73.7\nTransformer RPR 65.0 62.3 73.8\nGATE (this work) 67.0 57.9 74.1\nTable 3: Multi-source transfer results (F-score % on the test\nset) using perfect event triggers and entity mentions. The\nlanguage on top and bottom of +denotes the source and\ntarget languages, respectively.\n\u000fTransformer (Vaswani et al. 2017) uses multi-head self-\nattention mechanism and is the base structure of our pro-\nposed model, GATE. Note that GATE has the same number\nof parameters as Transformer since GATE does not intro-\nduce any new parameter while modeling the pairwise syn-\ntactic distance into the self-attention mechanism. Therefore,\nwe credit the GATE’s improvements over the Transformer\nto its distance-based attention modeling strategy.\n\u000fTransformer\nRPR (Shaw, Uszkoreit, and Vaswani 2018)\nuses relative position representations to encode the structure\nof the input sequences. This method uses the pairwise se-\nquential distances while GATE uses pairwise syntactic dis-\ntances to model attentions between tokens.\nImplementation Details To embed words into vector rep-\nresentations, we use multilingual BERT (M-BERT) (De-\nvlin et al. 2019). Note that we do not ﬁne-tune M-BERT,\nbut only use it as a feature extractor. We use the univer-\nsal part-of-speech (POS) tags, dependency relation labels,\nand seven entity types deﬁned by ACE: person, organi-\nzation, geo-political entity, location, facility, weapon, and\nvehicle. We embed these language-universal features into\nﬁxed-length vectors and concatenate them with M-BERT\nvectors to form the input word representations. We set the\nmodel size (dmodel), number of encoder layers (L), and at-\ntention heads (nh) in multi-head to 512, 1, and 8 respec-\ntively. We tune the distance threshold \u000e (as shown in Eq.\n(3)) in [1;2;4;8;1] for each attention head on each source\nlanguage (more details are provided in the supplementary).\nWe implement all the baselines and our approach based on\nthe implementation of Zhang, Qi, and Manning (2018) and\nOpenNMT (Klein et al. 2017). We used transformers8\nto extract M-BERT and XLM-R features. We provide a de-\ntailed description of the dataset, hyper-parameters, and train-\ning of the baselines and our approach in the supplementary.\n5 Results and Analysis\nWe compare GATE with ﬁve baseline approaches on event\nargument role labeling (EARL) and relation extraction (RE)\ntasks, and the results are presented in Table 2 and 3.\nSingle-source transfer In the single-source transfer set-\nting, all the models are individually trained on one source\nlanguage, e.g., English and directly evaluated on the other\ntwo languages (target), e.g., Chinese and Arabic. Table 2\nshows that GATE outperforms all the baselines in four out of\nsix transfer directions on both tasks. CL\nRNN surprisingly\noutperforms CL GCN in most settings, although CL RNN\nuses a BiLSTM that is not suitable to transfer across syn-\ntactically different languages (Ahmad et al. 2019a). We hy-\npothesize the reason being GCNs cannot capture long-range\ndependencies, which is crucial for the two tasks. In com-\nparison, by modeling distance-based pairwise relationships\namong words, GATE excels in cross-lingual transfer.\nA comparison between Transformer and GATE demon-\nstrates the effectiveness of syntactic distance-based self-\nattention over the standard mechanism. From Table 2, we\nsee GATE outperforms Transformer with an average im-\nprovement of 4.7% and 1.3% in EARL and RE tasks,\nrespectively. Due to implicitly modeling graph structure,\nTransformer\nRPR performs effectively. However, GATE\nachieves an average improvement of 1.3% and 1.9% in\nEARL and RE tasks over Transformer RPR. Overall, the\nsigniﬁcant performance improvements achieved by GATE\ncorroborate our hypothesis that syntactic distance-based at-\ntention helps in the cross-lingual transfer.\n8https://github.com/huggingface/transformers\n12466\nModel EARL RE\nChinese Arabic Chinese Arabic\nWang et al. (2019)\nAbsolute 61.2 53.5 57.8 65.2\nRelative 55.3 47.1 58.1 66.4\nGATE 63.2 68.5 55.1 66.8\nTable 4: GATE vs. Wang et al. (2019) results (F-score %) on\nevent argument role labeling (EARL) and relation extraction\n(RE); using English as source and Chinese, Arabic as the tar-\nget languages, respectively. To limit the maximum relative\nposition, the clipping distance is set to 10 and 5 for EARL\nand RE tasks, respectively.\nMulti-source transfer In the multi-source cross-lingual\ntransfer, the models are trained on a pair of languages:\nfEnglish, Chineseg, fEnglish, Arabicg, and fChinese, Ara-\nbicg. Hence, the models observe more examples during\ntraining, and as a result, the cross-lingual transfer perfor-\nmance improves compared to the single-source transfer set-\nting. In Table 3, we see GATE outperforms the previous\nthree IE approaches in multi-source transfer settings, except\non RE for the source:fEnglish, Arabicgand target: Chinese\nlanguage setting. On the other hand, GATE performs com-\npetitively toTransformerand Transformer\nRPR baselines.\nDue to observing more training examples,Transformerand\nTransformer RPR perform more effectively in this set-\nting. The overall result indicates that GATE more efﬁciently\nlearns transferable representations for the IE tasks.\nEncoding dependency structure GATE encodes the de-\npendency structure of sentences by guiding the attention\nmechanism in self-attention networks (SANs). However, an\nalternative way to encode the sentence structure is through\npositional encoding for SANs. Conceptually, the key differ-\nence is the modeling of syntactic distances to capture ﬁne-\ngrained relations among tokens. Hence, we compare these\ntwo notions of encoding the dependency structure to empha-\nsize the promise of modeling syntactic distances.\nTo this end, we compare the GATE with Wang et al.\n(2019) that proposed structural position encoding using the\ndependency structure of sentences. Results are presented in\nTable 4. We see that Wang et al. (2019) performs well on\nRE but poorly on EARL, especially on the Arabic language.\nWhile GATE directly uses syntactic distances between to-\nkens to guide the self-attention mechanism, Wang et al.\n(2019) learns parameters to encode structural positions that\ncan become sensitive to the source language. For example,\nthe average shortest path distance between event mentions\nand their candidate arguments in English and Arabic is 3.1\nand 12.3, respectively (see Table 1). As a result, a model\ntrained in English may learn only to attend closer tokens,\nthus fails to generalize on Arabic.\nMoreover, we anticipate that different order of subject\nand verb in English and Arabic 9 causes Wang et al. (2019)\nto transfer poorly on the EARL task (as event triggers are\n9According to W ALS (Dryer and Haspelmath 2013), the order\nof subject (S), object (O), and verb (V) for English, Chinese and\nArabic is SVO, SVO, and VSO.\nModel EARL RE\nEnglish Chinese \u0003 English Chinese \u0003\nCL GCN 51.5 56.3 46.9 50.7\nCL RNN 55.6 59.3 56.8 62.0\nGATE 63.8 64.2 58.8 57.0\nTable 5: Event argument role labeling (EARL) and relation\nextraction (RE) results (F-score %); using Chinese as the\nsource and English as the target language. \u0003 indicates the\nEnglish examples are translated into Chinese usingGoogle\nCloud Translate.\nFigure 3: Models trained on the Chinese language perform\non event argument role labeling in English and their paral-\nlel Chinese sentences. The parallel sentences have the same\nmeaning but a different structure. To quantify the structural\ndifference between two parallel sentences, we compute the\ntree edit distances.\nmostly verbs). To verify our anticipation, we modify the\nrelative structural position encoding (Wang et al. 2019) by\ndropping the directional information (Ahmad et al. 2019a),\nand observed a performance increase from 47.1 to 52.2 for\nEnglish to Arabic language transfer. In comparison, GATE is\norder-agnostic as it models syntactic distance; hence, it has a\nbetter transferability across typologically diverse languages.\nSensitivity towards source language Intuitively, an RE\nor EARL model would transfer well on target languages if\nthe model is less sensitive towards the source language char-\nacteristics (e.g., word order, grammar structure). To mea-\nsure sensitivity towards the source language, we evaluate the\nmodel performance on the target language and their paral-\nlel (translated) source language sentences. We hypothesize\nthat if a model performs signiﬁcantly well on the translated\nsource language sentences, then the model is more sensi-\ntive towards the source language and may not be ideal for\ncross-lingual transfer. To test the models on this hypothesis,\nwe translate all the ACE05 English test set examples into\nChinese using Google Cloud Translate.10 We train\nGATE and two baselines on the Chinese and evaluate them\non both English (test set) examples and their Chinese trans-\nlations. To quantify the difference between the dependency\n10Details are provided in the supplementary.\n12467\nWord features EARL RE\nChinese Arabic Chinese Arabic\nMulti-WE 35.9 43.7 41.0 54.9\nM-BERT 57.1 54.8 55.1 66.8\nXLM-R 51.8 61.7 51.4 68.1\nTable 6: Contribution of multilingual word embeddings\n(Multi-WE) (Joulin et al. 2018), M-BERT (Devlin et al.\n2019), and XLM-R (Conneau et al. 2020) as a source of\nword features; using English as source and Chinese, Arabic\nas the target languages, respectively.\nInput features EARL RE\nChinese Arabic Chinese Arabic\nM-BERT 52.5 47.4 44.0 49.7\n+ POS tag 49.3 47.5 44.1 47.0\n+ Dep. label 49.7 51.0 48.6 47.0\n+ Entity type 57.8 60.2 56.3 63.0\nTable 7: Ablation on the use of language-universal features\n(part-of-speech (POS) tag, dependency relation label, and\nentity type) in GATE (F-score (%); using English as source\nand Chinese, Arabic as the target languages, respectively.\nstructure of an English and its Chinese translation sentences,\nwe compute edit distance between two tree structures using\nthe APTED11 algorithm (Pawlik and Augsten 2015, 2016).\nThe results are presented in Table 5. We see thatCL\nGCN\nand CL RNN have much higher accuracy on the translated\n(Chinese) sentences than the target language (English) sen-\ntences. On the other hand, GATE makes a roughly similar\nnumber of correct predictions when the target and translated\nsentences are given as input. Figure 3 illustrates how the\nmodels perform when the structural distance between target\nsentences and their translation increases. The results suggest\nthat GATE performs substantially better than the baselines\nwhen the target language sentences are structurally different\nfrom the source language. The overall ﬁndings signal that\nGATE is less sensitive to source language features, and we\ncredit this to the modeling of distance-based syntactic rela-\ntionships between words. We acknowledge that there might\nbe other factors associated with a model’s language sensitiv-\nity. However, we leave the detailed analysis for measuring a\nmodel’s sensitivity towards languages as future work.\nAblation study We perform a detailed ablation study on\nlanguage-universal features and sources of word features to\nexamine their individual impact on cross-lingual transfer.\nThe results are presented in Table 6 and 7. We observed that\nM-BERT and XLM-R produced word features performed\nbetter in Chinese and Arabic, respectively, while they are\ncomparable in English. On average M-BERT performs bet-\nter, and thus we chose it as the word feature extractor in\nall our experiments. Table 7 shows that part-of-speech and\ndependency relation embedding has a limited contribution.\nThis is perhaps due to the tokenization errors, as pointed\nout by Subburathinam et al. (2019). However, the use of\nlanguage-universal features is useful, particularly when we\n11https://pypi.org/project/apted/\nhave minimal training data. We provide more analysis and\nresults in the supplementary.\n6 Related Work\nRelation and event extraction has drawn signiﬁcant atten-\ntion from the natural language processing (NLP) commu-\nnity. Most of the approaches developed in past several years\nare based on supervised machine learning, using either sym-\nbolic features (Ahn 2006; Ji and Grishman 2008; Liao and\nGrishman 2010; Hong et al. 2011; Li, Ji, and Huang 2013;\nLi and Ji 2014) or distributional features (Liao and Grishman\n2011; Nguyen, Cho, and Grishman 2016; Miwa and Bansal\n2016; Liu et al. 2018; Zhang et al. 2018; Lu and Nguyen\n2018; Chen et al. 2015; Nguyen and Grishman 2015; Zeng\net al. 2014; Peng et al. 2017; Nguyen and Grishman 2018;\nZhang, Qi, and Manning 2018; Subburathinam et al. 2019;\nLiu et al. 2019; Huang, Yang, and Peng 2020) from a large\nnumber of annotations. Joint learning or inference (Bekoulis\net al. 2018; Li et al. 2014; Zhang, Ji, and Sil 2019; Liu, Luo,\nand Huang 2018; Nguyen, Cho, and Grishman 2016; Yang\nand Mitchell 2016; Han, Ning, and Peng 2019; Han, Zhou,\nand Peng 2020) are also among the noteworthy techniques.\nMost previous works on cross-lingual transfer for rela-\ntion and event extraction are based on annotation projection\n(Kim et al. 2010a; Kim and Lee 2012), bilingual dictionaries\n(Hsi et al. 2016; Ni and Florian 2019), parallel data (Chen\nand Ji 2009; Kim et al. 2010b; Qian et al. 2014) or machine\ntranslation (Zhu et al. 2014; Faruqui and Kumar 2015; Zou\net al. 2018). Learning common patterns across languages is\nalso explored (Lin, Liu, and Sun 2017; Wang et al. 2018;\nLiu et al. 2018). In contrast to these approaches, Subburathi-\nnam et al. (2019); Liu et al. (2019) proposed to use graph\nconvolutional networks (GCNs) (Kipf and Welling 2017)\nto learn multi-lingual structured representations. However,\nGCNs struggle to model long-range dependencies or dis-\nconnected words in the dependency tree. To overcome the\nlimitation, we use the syntactic distances to weigh the atten-\ntions while learning contextualized representations via the\nmulti-head attention mechanism (Vaswani et al. 2017).\nMoreover, our proposed syntax driven distance-based at-\ntention modeling helps to mitigate the word order difference\nissue (Ahmad et al. 2019a) that hinders cross-lingual trans-\nfer. Prior works studied dependency structure modeling (Liu\net al. 2019), source reordering (Rasooli and Collins 2019),\nadversarial training (Ahmad et al. 2019b), constrained infer-\nence (Meng, Peng, and Chang 2019) to tackle word order\ndifferences across typologically different languages.\n7 Conclusion\nIn this paper, we proposed to model ﬁne-grained syntac-\ntic structural information based on the dependency parse of\na sentence. We developed a Graph Attention Transformer\nEncoder (GATE) to generate structured contextual represen-\ntations. Extensive experiments on three languages demon-\nstrates the effectiveness of GATE in cross-lingual relation\nand event extraction. In the future, we want to explore other\nsources of language-universal information to improve struc-\ntured representation learning.\n12468\nAcknowledgments\nThis work was supported in part by National Science Foun-\ndation (NSF) Grant OAC 1920462 and the Intelligence Ad-\nvanced Research Projects Activity (IARPA) via Contract\nNo. 2019-19051600007.\nReferences\nAhmad, W.; Zhang, Z.; Ma, X.; Hovy, E.; Chang, K.-W.; and\nPeng, N. 2019a. On Difﬁculties of Cross-Lingual Transfer\nwith Order Differences: A Case Study on Dependency Pars-\ning. In Proceedings of NAACL, 2440–2452.\nAhmad, W. U.; Zhang, Z.; Ma, X.; Chang, K.-W.; and Peng,\nN. 2019b. Cross-Lingual Dependency Parsing with Unla-\nbeled Auxiliary Languages. In Proceedings of CoNLL, 372–\n382.\nAhn, D. 2006. The stages of event extraction. In Proceed-\nings of the Workshop on Annotating and Reasoning about\nTime and Events, 1–8.\nBekoulis, G.; Deleu, J.; Demeester, T.; and Develder, C.\n2018. Adversarial training for multi-context joint entity and\nrelation extraction. In Proceedings of EMNLP, 2830–2836.\nBugliarello, E.; and Okazaki, N. 2020. Enhancing Machine\nTranslation with Dependency-Aware Self-Attention. InPro-\nceedings of ACL, 1618–1627.\nChen, Y .; Xu, L.; Liu, K.; Zeng, D.; and Zhao, J. 2015. Event\nExtraction via Dynamic Multi-Pooling Convolutional Neu-\nral Networks. In Proceedings of ACL-IJCNLP, 167–176.\nChen, Z.; and Ji, H. 2009. Can One Language Bootstrap\nthe Other: A Case Study on Event Extraction. In Proceed-\nings of the NAACL HLT 2009 Workshop on Semi-supervised\nLearning for Natural Language Processing, 66–74.\nChen, Z.-Y .; Chang, C.-H.; Chen, Y .-P.; Nayak, J.; and Ku,\nL.-W. 2019. UHop: An Unrestricted-Hop Relation Extrac-\ntion Framework for Knowledge-Based Question Answering.\nIn Proceedings of NAACL, 345–356.\nConneau, A.; Khandelwal, K.; Goyal, N.; Chaudhary, V .;\nWenzek, G.; Guzm ´an, F.; Grave, E.; Ott, M.; Zettlemoyer,\nL.; and Stoyanov, V . 2020. Unsupervised Cross-lingual Rep-\nresentation Learning at Scale. In Proceedings of ACL.\nDeguchi, H.; Tamura, A.; and Ninomiya, T. 2019.\nDependency-Based Self-Attention for Transformer NMT. In\nProceedings of RANLP, 239–246.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of NAACL.\nDryer, M. S.; and Haspelmath, M. 2013. The world atlas of\nlanguage structures online .\nFaruqui, M.; and Kumar, S. 2015. Multilingual Open Rela-\ntion Extraction Using Cross-lingual Projection. In Proceed-\nings of NAACL, 1351–1356.\nFu, T.-J.; Li, P.-H.; and Ma, W.-Y . 2019. GraphRel: Model-\ning Text as Relational Graphs for Joint Entity and Relation\nExtraction. In Proceedings of ACL, 1409–1418.\nHan, R.; Ning, Q.; and Peng, N. 2019. Joint Event and Tem-\nporal Relation Extraction with Shared Representations and\nStructured Prediction. In Proceedings of EMNLP, 434–444.\nHan, R.; Zhou, Y .; and Peng, N. 2020. Domain Knowledge\nEmpowered Structured Neural Net for End-to-End Event\nTemporal Relation Extraction. In Proceedings of EMNLP,\n5717–5729.\nHochreiter, S.; and Schmidhuber, J. 1997. Long short-term\nmemory. Neural computation 9(8): 1735–1780.\nHong, Y .; Zhang, J.; Ma, B.; Yao, J.; Zhou, G.; and Zhu,\nQ. 2011. Using Cross-Entity Inference to Improve Event\nExtraction. In Proceedings of NAACL, 1127–1136.\nHsi, A.; Yang, Y .; Carbonell, J.; and Xu, R. 2016. Leverag-\ning Multilingual Training for Limited Resource Event Ex-\ntraction. In Proceedings of COLING, 1201–1210.\nHuang, K.-H.; Yang, M.; and Peng, N. 2020. Biomedical\nEvent Extraction with Hierarchical Knowledge Graphs. In\nFindings of ACL: EMNLP 2020.\nHuang, L.; Ji, H.; Cho, K.; Dagan, I.; Riedel, S.; and V oss,\nC. 2018. Zero-Shot Transfer Learning for Event Extraction.\nIn Proceedings of ACL, 2160–2170.\nJi, H.; and Grishman, R. 2008. Reﬁning Event Extraction\nthrough Cross-Document Inference. In Proceedings of ACL,\n254–262.\nJoulin, A.; Bojanowski, P.; Mikolov, T.; J ´egou, H.; and\nGrave, E. 2018. Loss in Translation: Learning Bilingual\nWord Mapping with a Retrieval Criterion. In Proceedings\nof EMNLP, 2979–2984.\nKim, S.; Jeong, M.; Lee, J.; and Lee, G. G. 2010a. A cross-\nlingual annotation projection approach for relation detec-\ntion. In Proceedings of COLING, 564–571.\nKim, S.; Jeong, M.; Lee, J.; and Lee, G. G. 2010b. A Cross-\nlingual Annotation Projection Approach for Relation Detec-\ntion. In Proceedings of COLING, 564–571.\nKim, S.; and Lee, G. G. 2012. A Graph-based Cross-lingual\nProjection Approach for Weakly Supervised Relation Ex-\ntraction. In Proceedings of ACL, 48–53.\nKipf, T. N.; and Welling, M. 2017. Semi-supervised classi-\nﬁcation with graph convolutional networks. In ICLR.\nKlein, G.; Kim, Y .; Deng, Y .; Senellart, J.; and Rush, A.\n2017. OpenNMT: Open-Source Toolkit for Neural Machine\nTranslation. In Proceedings of ACL 2017, System Demo.\nLi, Q.; and Ji, H. 2014. Incremental Joint Extraction of En-\ntity Mentions and Relations. In Proceedings of ACL, 402–\n412.\nLi, Q.; Ji, H.; Hong, Y .; and Li, S. 2014. Constructing Infor-\nmation Networks Using One Single Model. In Proceedings\nof EMNLP, 1846–1851.\nLi, Q.; Ji, H.; and Huang, L. 2013. Joint Event Extraction via\nStructured Prediction with Global Features. In Proceedings\nof ACL, 73–82.\nLiao, S.; and Grishman, R. 2010. Using Document Level\nCross-Event Inference to Improve Event Extraction. In Pro-\nceedings of ACL, 789–797.\n12469\nLiao, S.; and Grishman, R. 2011. Acquiring Topic Features\nto improve Event Extraction: in Pre-selected and Balanced\nCollections. In Proceedings of RANLP, 9–16.\nLin, Y .; Liu, Z.; and Sun, M. 2017. Neural Relation Extrac-\ntion with Multi-lingual Attention. In Proceedings of ACL.\nLin, Y .; Liu, Z.; Sun, M.; Liu, Y .; and Zhu, X. 2015. Learn-\ning Entity and Relation Embeddings for Knowledge Graph\nCompletion. In Proceedings of AAAI, 2181–2187.\nLiu, J.; Chen, Y .; Liu, K.; and Zhao, J. 2018. Event detection\nvia gated multilingual attention mechanism. In Proceedings\nof AAAI, 4865–4872.\nLiu, J.; Chen, Y .; Liu, K.; and Zhao, J. 2019. Neural Cross-\nLingual Event Detection with Minimal Parallel Resources.\nIn Proceedings of EMNLP-IJCNLP, 738–748.\nLiu, X.; Luo, Z.; and Huang, H. 2018. Jointly Multiple\nEvents Extraction via Attention-based Graph Information\nAggregation. In Proceedings of EMNLP, 1247–1256.\nLu, W.; and Nguyen, T. H. 2018. Similar but not the\nSame: Word Sense Disambiguation Improves Event Detec-\ntion via Neural Representation Matching. In Proceedings of\nEMNLP, 4822–4828.\nMeng, T.; Peng, N.; and Chang, K.-W. 2019. Target\nLanguage-Aware Constrained Inference for Cross-lingual\nDependency Parsing. In Proceedings of EMNLP-IJCNLP,\n1117–1128.\nMiwa, M.; and Bansal, M. 2016. End-to-End Relation Ex-\ntraction using LSTMs on Sequences and Tree Structures. In\nProceedings of ACL, 1105–1116.\nNguyen, T. H.; Cho, K.; and Grishman, R. 2016. Joint Event\nExtraction via Recurrent Neural Networks. In Proceedings\nof NAACL, 300–309.\nNguyen, T. H.; and Grishman, R. 2015. Event Detection and\nDomain Adaptation with Convolutional Neural Networks.\nIn Proceedings of EMNLP-IJCNLP, 365–371.\nNguyen, T. H.; and Grishman, R. 2018. Graph convolutional\nnetworks with argument-aware pooling for event detection.\nIn Proceedings of AAAI.\nNi, J.; and Florian, R. 2019. Neural Cross-Lingual Relation\nExtraction Based on Bilingual Word Embedding Mapping.\nIn Proceedings of EMNLP-IJCNLP, 399–409.\nPawlik, M.; and Augsten, N. 2015. Efﬁcient computation\nof the tree edit distance. ACM Transactions on Database\nSystems (TODS) 40(1): 1–40.\nPawlik, M.; and Augsten, N. 2016. Tree edit distance: Ro-\nbust and memory-efﬁcient. Information Systems 157–173.\nPeng, N.; Poon, H.; Quirk, C.; Toutanova, K.; and Yih, W.-t.\n2017. Cross-sentence N-ary Relation Extraction with Graph\nLSTMs. Transactions of ACL .\nQian, L.; Hui, H.; Hu, Y .; Zhou, G.; and Zhu, Q. 2014. Bilin-\ngual Active Learning for Relation Classiﬁcation via Pseudo\nParallel Corpora. In Proceedings of ACL, 582–592.\nRasooli, M. S.; and Collins, M. 2019. Low-Resource Syn-\ntactic Transfer with Unsupervised Source Reordering. In\nProceedings of NAACL, 3845–3856.\nShaw, P.; Uszkoreit, J.; and Vaswani, A. 2018. Self-\nAttention with Relative Position Representations. In Pro-\nceedings of NAACL, 464–468.\nStraka, M.; and Strakov ´a, J. 2017. Tokenizing, POS Tag-\nging, Lemmatizing and Parsing UD 2.0 with UDPipe. In\nProceedings of the CoNLL 2017 Shared Task: Multilingual\nParsing from Raw Text to Universal Dependencies, 88–99.\nSubburathinam, A.; Lu, D.; Ji, H.; May, J.; Chang, S.-F.; Sil,\nA.; and V oss, C. 2019. Cross-lingual Structure Transfer for\nRelation and Event Extraction. In Proceedings of EMNLP-\nIJCNLP, 313–325.\nSun, C.; Gong, Y .; Wu, Y .; Gong, M.; Jiang, D.; Lan, M.;\nSun, S.; and Duan, N. 2019. Joint Type Inference on En-\ntities and Relations via Graph Convolutional Networks. In\nProceedings of ACL, 1361–1370.\nTang, H.; Ji, D.; Li, C.; and Zhou, Q. 2020. Dependency\nGraph Enhanced Dual-transformer Structure for Aspect-\nbased Sentiment Classiﬁcation. In Proceedings of ACL.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In NeurIPS, 5998–6008.\nWalker, C.; Strassel, S.; Medero, J.; and Maeda, K. 2006.\nACE 2005 multilingual training corpus. Linguistic Data\nConsortium, Philadelphia 57.\nWang, X.; Han, X.; Lin, Y .; Liu, Z.; and Sun, M. 2018. Ad-\nversarial Multi-lingual Neural Relation Extraction. In Pro-\nceedings of COLING, 1156–1166.\nWang, X.; Tu, Z.; Wang, L.; and Shi, S. 2019. Self-Attention\nwith Structural Position Representations. In Proceedings of\nEMNLP-IJCNLP, 1403–1409.\nYang, B.; and Mitchell, T. M. 2016. Joint Extraction of\nEvents and Entities within a Document Context. In Pro-\nceedings of NAACL, 289–299.\nZeng, D.; Liu, K.; Lai, S.; Zhou, G.; and Zhao, J. 2014. Rela-\ntion Classiﬁcation via Convolutional Deep Neural Network.\nIn Proceedings of COLING, 2335–2344.\nZhang, C.; Li, Q.; and Song, D. 2019. Aspect-based Sen-\ntiment Classiﬁcation with Aspect-speciﬁc Graph Convolu-\ntional Networks. In Proceedings of EMNLP-IJCNLP.\nZhang, J.; Zhou, W.; Hong, Y .; Yao, J.; and Zhang, M. 2018.\nUsing entity relation to improve event detection via attention\nmechanism. In CCF International Conference on NLPCC.\nZhang, T.; Ji, H.; and Sil, A. 2019. Joint entity and event ex-\ntraction with generative adversarial imitation learning. Data\nIntelligence 99–120.\nZhang, Y .; Qi, P.; and Manning, C. D. 2018. Graph Con-\nvolution over Pruned Dependency Trees Improves Relation\nExtraction. In Proceedings of EMNLP, 2205–2215.\nZhu, Z.; Li, S.; Zhou, G.; and Xia, R. 2014. Bilingual Event\nExtraction: a Case Study on Trigger Type Determination. In\nProceedings of ACL, 842–847.\nZou, B.; Xu, Z.; Hong, Y .; and Zhou, G. 2018. Adversar-\nial Feature Adaptation for Cross-lingual Relation Classiﬁca-\ntion. In Proceedings of COLING, 437–448.\n12470",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7701959609985352
    },
    {
      "name": "Encoder",
      "score": 0.6022388935089111
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5955033302307129
    },
    {
      "name": "Transformer",
      "score": 0.5911964178085327
    },
    {
      "name": "Natural language processing",
      "score": 0.5702598094940186
    },
    {
      "name": "Relationship extraction",
      "score": 0.5516121983528137
    },
    {
      "name": "Transferability",
      "score": 0.5260267853736877
    },
    {
      "name": "Sentence",
      "score": 0.517586350440979
    },
    {
      "name": "Graph",
      "score": 0.48047035932540894
    },
    {
      "name": "Dependency grammar",
      "score": 0.4462585747241974
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.44034916162490845
    },
    {
      "name": "Dependency (UML)",
      "score": 0.4187772274017334
    },
    {
      "name": "Theoretical computer science",
      "score": 0.22568413615226746
    },
    {
      "name": "Information extraction",
      "score": 0.20692786574363708
    },
    {
      "name": "Machine learning",
      "score": 0.17295700311660767
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Logit",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I161318765",
      "name": "University of California, Los Angeles",
      "country": "US"
    }
  ]
}