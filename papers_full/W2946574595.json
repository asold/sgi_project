{
  "title": "Multimodal Transformer with Multi-View Visual Representation for Image Captioning",
  "url": "https://openalex.org/W2946574595",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1986192139",
      "name": "Yu Jun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1219896553",
      "name": "Li Jing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105593998",
      "name": "Yu Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2017848976",
      "name": "Huang, Qingming",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2889418103",
    "https://openalex.org/W2112912048",
    "https://openalex.org/W2963521239",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2962706528",
    "https://openalex.org/W2575842049",
    "https://openalex.org/W2963656855",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2952122856",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2952782394",
    "https://openalex.org/W1947481528",
    "https://openalex.org/W2951690276",
    "https://openalex.org/W1858383477",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2735680049",
    "https://openalex.org/W2964022527",
    "https://openalex.org/W2552161745",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W2963668159",
    "https://openalex.org/W2890531016",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2953276893",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2747623286",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W2963150162",
    "https://openalex.org/W2963176022",
    "https://openalex.org/W1969616664",
    "https://openalex.org/W2550553598",
    "https://openalex.org/W2412400526",
    "https://openalex.org/W1996219872",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W8316075",
    "https://openalex.org/W2049538695",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W1897761818",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2943070176",
    "https://openalex.org/W2885013662"
  ],
  "abstract": "Image captioning aims to automatically generate a natural language description of a given image, and most state-of-the-art models have adopted an encoder-decoder framework. The framework consists of a convolution neural network (CNN)-based image encoder that extracts region-based visual features from the input image, and an recurrent neural network (RNN)-based caption decoder that generates the output caption words based on the visual features with the attention mechanism. Despite the success of existing studies, current methods only model the co-attention that characterizes the inter-modal interactions while neglecting the self-attention that characterizes the intra-modal interactions. Inspired by the success of the Transformer model in machine translation, here we extend it to a Multimodal Transformer (MT) model for image captioning. Compared to existing image captioning approaches, the MT model simultaneously captures intra- and inter-modal interactions in a unified attention block. Due to the in-depth modular composition of such attention blocks, the MT model can perform complex multimodal reasoning and output accurate captions. Moreover, to further improve the image captioning performance, multi-view visual features are seamlessly introduced into the MT model. We quantitatively and qualitatively evaluate our approach using the benchmark MSCOCO image captioning dataset and conduct extensive ablation studies to investigate the reasons behind its effectiveness. The experimental results show that our method significantly outperforms the previous state-of-the-art methods. With an ensemble of seven models, our solution ranks the 1st place on the real-time leaderboard of the MSCOCO image captioning challenge at the time of the writing of this paper.",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\nMultimodal Transformer with Multi-View\nVisual Representation for Image Captioning\nJun Yu, Member, IEEE, Jing Li, Zhou Yu, Member, IEEE, Qingming Huang, Fellow, IEEE\nAbstract—Image captioning aims to automatically generate a\nnatural language description of a given image, and most state-of-\nthe-art models have adopted an encoder-decoder framework. The\nframework consists of a convolution neural network (CNN)-based\nimage encoder that extracts region-based visual features from\nthe input image, and an recurrent neural network (RNN)-based\ncaption decoder that generates the output caption words based\non the visual features with the attention mechanism. Despite\nthe success of existing studies, current methods only model\nthe co-attention that characterizes the inter-modal interactions\nwhile neglecting the self-attention that characterizes the intra-\nmodal interactions. Inspired by the success of the Transformer\nmodel in machine translation, here we extend it to a Multimodal\nTransformer (MT) model for image captioning. Compared to\nexisting image captioning approaches, the MT model simultane-\nously captures intra- and inter-modal interactions in a uniﬁed\nattention block. Due to the in-depth modular composition of\nsuch attention blocks, the MT model can perform complex\nmultimodal reasoning and output accurate captions. Moreover, to\nfurther improve the image captioning performance, multi-view\nvisual features are seamlessly introduced into the MT model.\nWe quantitatively and qualitatively evaluate our approach using\nthe benchmark MSCOCO image captioning dataset and conduct\nextensive ablation studies to investigate the reasons behind its\neffectiveness. The experimental results show that our method\nsigniﬁcantly outperforms the previous state-of-the-art methods.\nWith an ensemble of seven models, our solution ranks the 1st\nplace on the real-time leaderboard of the MSCOCO image\ncaptioning challenge at the time of the writing of this paper.\nIndex Terms—Image captioning, multi-view learning, deep\nlearning.\nI. I NTRODUCTION\nR\nEcent advances in deep learning have resulted in great\nprogress in both the computer vision and natural lan-\nguage processing communities. These achievements make it\npossible to connect vision and language, and facilitate multi-\nmodal learning tasks such as image-text matching [1], visual\nquestion answering [2][3][4], visual grounding [5] and image\ncaptioning [6][7][8][9][10].\nImage captioning aims to automatically describe an im-\nage’s content using a natural language sentence. The task\nis challenging since it requires one to recognize key objects\nin an image, and to understand their relationships with each\nJ. Yu, J. Li and Z. Yu are with Key Laboratory of Complex Systems Model-\ning and Simulation, School of Computer Science and Technology, Hangzhou\nDianzi University, Hangzhou, 310018, China (e-mail: yujun@hdu.edu.cn;\njingli@hdu.edu.cn; yuz@hdu.edu.cn).\nQ. Huang is with the School of Computer and Control Engineering,\nUniversity of Chinese Academy of Sciences, Beijing 101408, China (email:\nqmhuang@ucas.ac.cn).\nCorresponding author: Zhou Yu.\nother. Most successful image captioning approaches adopt\nthe encoder-decoder framework, which is inspired by the\nsequence-to-sequence model for machine translation [11]. The\nframework consists of a convolutional neural network (CNN)-\nbased image encoder that extracts region-based visual features\nfrom an input image, and an recurrent neural network (RNN)-\nbased caption decoder that iteratively generates the output\ncaption words based on the visual features. The encoder-\ndecoder model is usually trained in an end-to-end manner\nto minimize the cross-entropy loss. Based on the framework,\nplenty of improvements have been made by recent works to\nfurther improve image captioning performance further. For\ninstance, to establish the ﬁne-grained connections of caption\nwords and their related image regions, an attention mechanism\ncan be seamlessly inserted into the framework [7]. To provide\na better understanding of the objects in the image, region-\nbased bottom-up-attention features can be extracted from a\npre-trained object detector to replace the traditional CNN\nconvolutional features [6]. To address the exposure bias of\ngenerated captions by using the cross-entropy loss, reinforce-\nment learning (RL)-based algorithms are designed to directly\noptimize the non-differentiable evaluation metrics (e.g., BLEU\n[12] and CIDEr [13]) [10].\nDespite the success that existing approaches have achieved,\nthey have the following limitations: 1) the current attention\nmechanism in image captioning only models the co-attention\nthat characterizes inter-modal interactions ( i.e., object-to-\nword) while neglecting the self-attention that characterizes\nintra-modal interactions ( i.e., word-to-word and object-to-\nobject); 2) current image captioning models are usually shal-\nlow and may fail to fully understand the complex relationships\namong visual objects; and 3) the region-based visual features\nmay fail to cover all objects in the image, leading to insufﬁ-\ncient visual representations for generating accurate captions.\nTo address the ﬁrst and second limitations, we extend\nthe Transformer model for machine translation [14] to a\nMultimodal Transformer (MT) model for image captioning.\nDifferent from the CNN-RNN captioning models, the MT\nmodel does not use RNN and instead relies entirely on\nan attention mechanism to assess the global dependencies\nbetween the input and output. By properly stacking such\nattention blocks in depth, MT forms a deep encoder-decoder\nmodel that simultaneously captures the self-attention within\neach modality and the co-attention across different modalities.\nTo address the last limitation, we introduce multi-view feature\nlearning into the MT model to adapt both the aligned and\nunaligned multi-view visual features.\nTo summarize, the main contributions of this study are\narXiv:1905.07841v1  [cs.CV]  20 May 2019\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\nthree-fold:\n• The joint modeling of the self-attention and the co-\nattention interactions for image captioning is ﬁrst pro-\nposed in the MT model. The MT model is capable of\nmodeling three types of relationships using a modular\nattention block, i.e., word-to-word, object-to-object, and\nword-to-object. By stacking such attention blocks in\ndepth, the deep MT model signiﬁcantly outperforms the\nstate-of-the-art models, thereby highlighting the impor-\ntance of deep reasoning for image captioning.\n• Multi-view learning on the image is introduced in con-\njunction with the MT model to provide more diverse\nand discriminative visual representations. We introduce\ntwo alternative strategies to handle aligned and unaligned\nmulti-view features, respectively.\n• Extensive experiments on the benchmark MSCOCO im-\nage captioning dataset are conducted to quantitatively and\nqualitatively prove the effectiveness of the proposed mod-\nels. The experimental results show that the MT signif-\nicantly outperforms previous state-of-the-art approaches\nwith a single model. Furthermore, our solution ranks the\n1st place on the real-time leaderboard of the MSCOCO\nimage captioning challenge with an ensemble of MT\nmodels.\nThe rest of the paper is organized as follows: In section II,\nwe review the related work of image captioning approaches,\nespecially the ones introducing attention mechanisms. In sec-\ntion III, we revisit the basic Transformer model and then pro-\npose the Multimodal Transformer model for image captioning.\nIn section IV, we introduce multi-view image representation\ninto the MT model to increase the visual representation ca-\npacity, and the quality of the generated captions. In section V,\nwe introduce our extensive experimental results for algorithm\nevaluation and use the benchmark MSCOCO image captioning\ndataset to evaluate our proposed approaches. Finally, we\nconclude this work in section VI.\nII. R ELATED WORK\nIn this section, we brieﬂy review the most relevant research\non image captioning, especially those studies that introduce\nattention models.\nA. Image Captioning\nThe research on image captioning can be categorized\ninto the following three classes: template-based approaches\n[15][16][17], retrieval-based approaches [18][19][20], and\ngeneration-based approaches [10][9][21][6][22].\nThe template-based approaches address the task using a\ntwo-stage strategy: 1) align the sentence fragments ( e.g.,\nsubject, object, and verb) with the predicted labels from\nthe image; and 2) generate the sentence from the segments\nusing pre-deﬁned language templates. Kulkarni et al. use the\nconditional random ﬁeld (CRF) model to predict labels based\non the detected objects, attributes, and prepositions, and then\ngenerate caption sentences with a template by ﬁlling in the\nblanks with the most likely labels [15]. Yang et al. employ the\nHMM model to select the best objects, verbs, and prepositions\nwith respect to the log-likelihood for segments generation [17].\nIntuitively, the captions that are generated by the template-\nbased approaches highly depend on the quality of the templates\nand usually follow the syntactical structures. However, the\ndiversity of the generated captions is severely restricted.\nTo ease the diversity problem, retrieval-based approaches\nare proposed to search the most relevant captions from a\nlarge-scale caption database with respect to their cross-modal\nsimilarities to the given image. Karpathy et al. propose a deep\nfragment embedding approach to match the image-caption\npairs based on the alignment of visual segments (the detected\nobjects) and caption segments (subjects, objects, and verbs)\n[18]. In the testing stage, the cross-modal matching over\nthe whole caption database (usually the captions from the\ntraining set) is performed to generate the caption for one\nimage. Other methods such as [19][20] use different metrics\nor loss functions to learn the cross-modal matching model.\nHowever, the retrieval efﬁciency becomes a bottleneck for\nthese approaches when the caption database is large and\nrestricting the size of the database may reduce the caption di-\nversity. Moreover, retrieval-based approaches cannot generate\nnovel captions beyond the database, which means the diversity\nproblem has not been completely resolved.\nDifferent from template-based and retrieval-based models,\ngeneration-based models aim to learn a language model that\ncan generate novel captions with more ﬂexible syntactical\nstructures. With this purpose, recent works explore this direc-\ntion by introducing the neural networks for image captioning.\nVinyals et al. propose an encoder-decoder architecture by\nutilizing the GoogLeNet [23] and LSTM networks [24] as\nits backbones. Similar architectures are also proposed by\nDonahue et al. [25] and Karpathy et al. [26]. Due to the\nﬂexibility and excellent performance, generation-based models\nhave become the mainstream for image captioning.\nB. Attention Mechanism\nWithin the encoder-decoder framework, one of the most\nimportant improvements for generation-based models is the\nattention mechanism. Xu et al. introduce the soft and hard\nattention models to mimic the human eye focusing on different\nregions in an image when generating different caption words.\nThe attention model is a pluggable module that can be seam-\nlessly inserted into previous approaches to remarkably improve\nthe caption quality. The attention model is further improved in\n[6][27][9][10]. Anderson et al. introduce a bottom-up module,\nthat uses a pre-trained object detector to extract region-based\nimage features, and a top-down module that utilizes soft\nattention to dynamically attend to these object [6]. Chen et\nal. propose a spatial- and channel-wise attention model to\nattend to visual features [27]. Lu et al. present an adaptive\nattention encoder-decoder model for automatically deciding\nwhen to rely on visual or language signals [9]. Rennie et al.\ndesign a FC model and an Att2in model that achieve good\nperformance [10].\nBeyond the image captioning tasks, attention mechanisms\nare widely used in other multi-modal learning tasks such as\nvisual question answering (VQA). Lu et al. propose a co-\nattention learning framework to alternately learn the image\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\nattention and question attention [28]. Yu et al. reduce the co-\nattention method into two steps, self-attention for a question\nembedding and the question-conditioned attention for a visual\nembedding [29]. Nam et al. propose a multi-stage co-attention\nlearning model to reﬁne the attentions based on the memory of\nprevious attentions [30]. However, these co-attention models\nlearn separate attention distributions for each modality (image\nor question) and neglect the dense interaction between each\nquestion word and each image region, which becomes a\nbottleneck for understanding the ﬁne-grained relationships of\nmultimodal features. To address this issue, dense co-attention\nmodels have been proposed, which establish the complete\ninteraction between each question word and each image region\n[31][3]. Compared to the previous co-attention models with\ncoarse interactions, the dense co-attention models deliver\nsigniﬁcantly better VQA performance.\nIII. M ULTIMODAL TRANSFORMER\nIn this section, we ﬁrst brieﬂy describe the preliminary\nknowledge of the Transformer model [14]. Then, we introduce\nthe proposed Multimodal Transformer (MT) framework for\nimage captioning, which consists of an image encoder and\na caption decoder. The image encoder learns the deep image\nrepresentation in a self-attention manner, and then, the caption\ndecoder uses the attended image representations to generate\ntextual captions.\nA. The Transformer Model\nThe Transformer model [14] was ﬁrst proposed for machine\ntranslation, and has been successfully applied to many natural\nlanguage processing tasks. We ﬁrst introduce the scaled dot-\nproduct attention, which is the core component of the Trans-\nformer.\nThe input of the scaled dot-product attention consists of a\nquery q ∈Rd, a set of keys kt ∈Rd and values vt ∈Rd, where\nt ∈{1, 2, ..., n}is the number of key-value pairs and d is the\ncommon dimensionality of all the inputs features. We calculate\nthe dot products of query with all keys, divide each by\n√\nd\nand apply a softmax function to obtain the attention weights\non the values. In practice, we pack all the keys and values\ninto matrices K = [k1, ..., kn] ∈Rn×d and V = [v1, ..., vn] ∈\nRn×d respectively. The attention function on a set of queries\nQ = [q1, ..., qm] ∈ Rm×d can be computed in parallel as\nfollows:\nF = A(Q, K, V) = softmax(QKT\n√\nd\n)V (1)\nwhere F ∈Rm×d correspond to the attended features of the\nqueries Q.\nInstead of performing a single attention function for the\nqueries, multi-head attention is introduced in [14] to allow the\nmodel to attend to diverse information from different represen-\ntation subspaces. The multi-head attention contains h parallel\n‘heads’ with each head corresponding to an independent scaled\ndot-product attention function. The attended features F of the\nmulti-head attention functions is given as follows:\nF = MA(Q, K, V) = Concat(h1, ..., hh)Wo (2)\nFFN\nAdd & Norm\nAdd & Norm\nMHA\nL ×MHA \nAdd & Norm\nL ×\nEncoder\nDecoder\nMHA (mask)\nAdd & Norm\nFFN\nAdd & Norm\nQKV\nQKVQKV\nFig. 1: Transformer architecture in an encoder-decoder man-\nner. MHA and FFN denote the multi-head attention module\nand the feed-forward networks module, respectively. L is\nthe number of stacked attention blocks for the encoder and\ndecoder, and is set to the same number for simplicity.\nhi = A(QWQ\ni , KWK\ni , V WV\ni ) (3)\nwhere WQ\ni , WK\ni , WV\ni ∈Rd×dh are the projection matrices of\nthe i-th head. WO ∈Rh∗dh×d is the output projection matrix\nthat aggregates the information from different heads. dh is the\ndimensionality of the output features of each head. To prevent\nthe model from becoming too large, we set dh = d/h.\nIn addition to the multi-head attention (MHA), another basic\ncomponent in the Transformer is the feed-forward networks\n(FFN). FFN takes the input from MHA and further transform\nit using two linear layers with the ReLU activation and dropout\n[32] in between as follows:\nFFN(x) = FC(Dropout(ReLU(FC(x)))) (4)\nThe Transformer is a deep end-to-end architecture that\nstacks attention blocks to form an encoder-decoder strategy\n(see Fig. 1). Both the encoder and the decoder consist of N\nattention block, and each attention block contains the MHA\nand FFN modules. The MHA module learns the attended fea-\ntures that consider the pairwise interactions between two input\nfeatures, and the FFN module further nonlinearly transforms\nthe attended features. In the encoder, each attention block is\nself-attentional such that the queries, keys and values in Eq.(1)\nrefer to the same input features. In contrast, the attention block\nin the decoder contains a self-attention layer and a guided-\nattention layer. It ﬁrst models the self-attention of given input\nfeatures and then takes the output features of the last encoder\nattention block to guide the attention learning. To simplify the\noptimization, shortcut connection [33] and layer normalization\n[34] are applied after all the MHA and FFN modules.\nB. Multimodal Transformer for Image Captioning\nBased on the preliminary information about the Transformer\nabove, we describe the Multimodal Transformer (MT) ar-\nchitecture for image captioning, which consists of an image\nencoder and a textual decoder. The image encoder takes\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\nLinear\nA child on a surf board\nLinear\nSoftmax\nFaster R-CNN\n…\nGloVe+LSTM\n…\n[s] A child board\nEncoder\n… Decoder\nA child [s]on\nFig. 2: Multimodal Transformer (MT) model for image cap-\ntioning. It consists of an image encoder to learn self-attended\nvisual features, and a caption decoder to generate the caption\nfrom the attended visual features. [s] is a delimiter that\nindicates the start or the end of the caption.\nan image as its input and uses a pre-trained Faster-RCNN\nmodel [35] to extract region-based visual features. The visual\nfeatures are then fed into the encoder to obtain the attended\nvisual representation with self-attention learning. The decoder\ntakes the attended visual features and the previous word to\npredict the next word recursively. The ﬂowchart of the MT\narchitecture is shown in Fig. 2.\nImage Encoder. The input image is represented as a group\nof visual features that are extracted from a pre-trained object\ndetector [6]. Speciﬁcally, the detector is a Faster-RCNN model\n[35] that is pre-trained on the Visual Genome dataset [36].\nWe sort the detected objects w.r.t. their conﬁdence scores in\ndescending order and keep the top- m objects. Each object is\nrepresented as a feature vector xi ∈ Rdx by mean-pooling\nthe convolutional feature from its detected region. Finally, the\nimage is represented as a feature matrix X ∈Rm×dx.\nThe visual features X is ﬁrst fed into a fully-connected\nlayer to adapt the feature dimensionality to the encoder. The\nprojected features (denote as X(0)) are then fed into the\nencoder with L attention blocks [A1\nenc, A2\nenc, ..., AL\nenc]. The ith\nattention block Al\nenc takes the output features Xl−1 from the\ni −1th attention block, and output their attended features Xl\nin a recursive manner.\nXl = Al\nenc(Xl−1) (5)\nwhere each Al\nenc consists of a MHA module and a FFN\nmodule with independent model weights (see Fig. 1).\nCaption Decoder. Based on the visual representations from\nthe encoder, the textual decoder generates captions for the\nimage. The input caption is ﬁrst tokenized into words and\ntrimmed to a maximum length of n words. Each word in the\ncaption is ﬁrst represented as a word vector yi ∈R300 by using\nthe 300-D GloVe word embedding [37] pre-trained on a large-\nscale corpus. We use a feature matrix Y ∈Rn×300 to represent\na caption sentence. For the captions that are shorter than 16\nwords, we use zero-padding to ﬁll them to the maximum size.\nTo model the temporal information of the captions, the word\nembeddings are then pass through a one-layer LSTM network\n[24] with dy hidden units, resulting in caption representations\nY = [y1, ..., yn] ∈Rn×dy .\nIn the training stage, the caption decoder takes the inputs\nfrom both the image encoder and caption representations.\nGiven the attended image features XL and the caption input\nfeatures Y , the caption decoder with L attention blocks\n([A1\ndec, A2\ndec, ..., AL\ndec]) learns to predict the attended word\nfeatures in an analogous manner to the strategy in the encoder.\nY l = Al\ndec(XL, Yl−1) (6)\nwhere each Al\ndec consists of two MHA modules and one FFN\nmodule (see Fig. 1). The ﬁrst MHA module models the self-\nattentions on the caption words and the second MHA module\nlearns the image-guided attention on the caption words. Note\nthat the self-attention ( i.e., the ﬁrst MHA module) is only\nallowed to attend to earlier positions in the output sequence\nand is implemented by masking subsequent positions (setting\nthem to - ∞) before the softmax step in the self-attention\ncalculation, thereby resulting in a triangular mask matrix\nM ∈Rn×n. The output features Y L = [yL\n1 , yL\n1 , ..., yL\nn ] are fed\ninto a linear word embedding layer to transform the features\nto a dv-dimensional space, where dv is the vocabulary size.\nSubquently, softmax cross-entropy loss is performed on each\nword to predict the probability of its next word.\nIn the testing stage, the caption is generated word-\nby-word in a sequential manner. When generating the\ntth word, the input features are represented as Y≤t =\n[y1, y2, ...yt−1, 0, ...,0] ∈Rn×dy , where 0 ∈Rdy corresponds\na zero-padded feature. The input caption features along with\nthe image features are fed forward the model to obtain the\nword with the largest probability among the whole word vo-\ncabulary. The predicted word is then integrated into the inputs\nto recursively generate the new inputs Y≤t+1. To improve the\ndiversity of generated captions, we also introduce the beam\nsearch strategy during the testing stage.\nIV. I MAGE ENCODER WITH MULTI -VIEW VISUAL\nREPRESENTATION\nIn this section, we introduce multi-view image represen-\ntations and modify the the image encoder in section III-B\nto multi-view image encoder to facilitate the representation\ncapacity of the MT model. Though it has been intensively\ninvestigated by previous works [38][39],[40], existing multi-\nview learning approaches focus on integrating the global\nmulti-view features (e.g., color histogram or GIST descriptor)\nfrom the whole image. The global multi-view features may\nfail to preserve the ﬁne-grained semantics of the image, thus\nleading to incorrect caption. In contrast, we extract region-\nbased local multi-view features to represent the image. Each\nobject detector ( i.e., pre-trained Faster R-CNN models with\ndifferent backbones) is regarded as one single view.\nNote that the objects extracted from different detectors are\nnaturally unaligned, thereby making it challenging to learn the\ncorrespondence across different views. To address this prob-\nlem, we extend the proposed image encoder model in section\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\nDetector 1\n...\nDetector M\nDetector 1\nDetector 2\nAMV Features\nM viewsUnified bboxes\nCaption \nDecoder\nAMV Image\nEncoder\nFig. 3: The ﬂowchart of the aligned multi-view (AMV) image encoder model. Given an image, different object detectors are\nregarded as the multiple views. To obtain the aligned multi-view features, we choose one of the M detectors to predict the\nuniﬁed bounding boxes for objects, and then use these bboxes to extract aligned multi-view features. The aligned multi-view\nfeatures are fed into the AMV image encoder (which is exactly the same as the one for single-view features introduced in\nsection III-B).\nIII-B, and introduce two multi-view image encoder models,\nnamely, the Aligned Multi-View (AMV) image encoder and the\nUnaligned Multi-View (UMV) image encoder, respectively.\nA. Aligned Multi-View Image Encoder\nThe AMV model uses a simple strategy to obtain the aligned\nmulti-view features from different object detectors. Rather\nthan extracting the object bounding boxes and correspond-\ning features for each view, we propose a two-stage feature\nextraction framework. Given M pre-trained Faster R-CNN\nmodels, we ﬁrst select one detector as the primary model\nto generate the uniﬁed bounding boxes for all views. The\nchoices of different primary models has little inﬂuence on\nthe quality of the generated features, and we simply choose\nthe model with the highest detection performance. Subquently,\nthe uniﬁed bounding boxes are used to extract features from\ndifferent Faster R-CNN models. Speciﬁcally, the Faster R-\nCNN models degenerate to their Fast R-CNN versions [41]\nthat take the pre-computed bounding boxes as inputs. The\nresulting multi-view features are aligned such that each paired\nmulti-view features correspond to one object in the image.\nAssuming that we generate m uniﬁed bounding boxes, the\nextracted features from the i-th view ( i ∈ {1, 2, ..., M})\ncan be represented as X(i) ∈ Rm×di, where di is the\ndimensionality of the features. By simply concatenating the\nfeatures in columns, we obtain the multi-view features X =\n[X(1), X(2), ..., X(M)] ∈ Rm×(d1+d2+...,dM ). These aligned\nmulti-view features can replace the aforementioned single-\nview feature, and be seamlessly fed into the image encoder.\nThe overall ﬂowchart of the AMV model is shown in Fig. 3.\nTo align the multi-view features, the AMV model uses the\nuniﬁed bounding boxes. However, we argue that this strategy\nmay harm the diversity of multi-view features, leading to a\nlimited representation capacity of the encoded image features.\nMoreover, the AMV model implicitly constrains the object\ndetector for each view to be a Faster R-CNN model, which can\neither take the pre-computed proposals as inputs or generate\nthe proposals using the built-in Region Proposal Networks\n(RPN) [35]. This constraint limits the usage of one-stage\nobject detectors, e.g., RetinaNet [42] and YOLO [43].\nB. Unaligned Multi-View Image Encoder\nTo address the limitations of the AMV encoder model,\nwe propose a more generalized unaligned multi-view (UMV)\nimage encoder model that can directly integrate the unaligned\nmulti-view features from different object detectors (see the\nﬂowchart in Fig. 4).\nThe extracted visual features for the i-th view can be\nrepresented as X(i) ∈Rmi×di, where the number of features\nmi and the feature dimensionality di can be different across\nmultiple views. The unaligned multi-view features are fed\ninto an encoder to be aligned and fused simultaneously.\nSpeciﬁcally, we choose one view as the primary view and\nuse its features to guide the attention learning for other views.\nThe attended features from other views are then integrated into\nthe features in the primary view to output the output features.\nGiven the multi-view features X(1), X(2), ..., X(M), they are\nﬁrst linearly projected into a common d-dimensional space to\nobtain their transformed representations F(1), F(2), ..., F(M).\nAssuming that F(1) corresponds to the features of the primary\nview, we have M −1 MHA modules in total to model the\ninteractions between F(1) and F(i) with i ∈{2, 3, ..., M}.\n˜F(i) = MHA(i)(F(1), F(i), F(i)) (7)\nwhere ˜F(i) ∈Rm1×d is the attended output features for the\ni-th view. The obtained features ˜F(2), ˜F(3), ...,˜F(M) have the\nsame shape as F(1), and so they can be integrated with F(1)\nvia an element-wise summation. The MHA modules here can\nbe understood as learning the image-guided attention over the\nimage features from other views.\n˜F(1) = F(1) + ˜F(2) + ˜F(3), ...,+ ˜F(M) (8)\nFollowing the image encoder model in section III-B, the inte-\ngrated features ˜F(1) that are followed by layer normalization\n[34] are then fed forward through the FFN module to obtain\nthe transformed representations. It is worth noting that the\nUMV model can also be stacked in depth to learn more\naccurate interactions across different views, thus resulting\nin more discriminative output visual features for generating\ncaptions.\nV. E XPERIMENTS\nIn this section, we conduct experiments and evaluate the\nproposed MT models on MSCOCO 2015 image captioning\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6\nDetector 1\nM views\n...\nDetector M\nDetector 2\n...\nMHA\nAdd & Norm\nQ\nK\nV\nMHA\nQ\nK\nV\nUMV Image Encoder\nFFN\nAdd & Norm\nL x\nCaption \nDecoder\nLinear\nLinear\nLinear\nUMV Features\nFig. 4: The ﬂowchart of the unaligned multi-view (UMV) image encoder model. Given an image, unaligned multi-view features\nare extracted from different object detectors in parallel. The unaligned multi-view features are fed into the UMV model to\noutput the attended features with adaptive alignment learning.\ndataset [44]. Additionally, we use the Visual Genome dataset\n[36] to pre-train the object detectors that are further used to\nextract the bottom-up-attention visual features [6].\nA. Datasets\nMSCOCO is a benchmark dataset for various computer vision\ntasks, including object detection, instance segmentation, and\nimage captioning [44]. It contains 83k training images, 40k\nvalidation images, and 81k test images. Each image is asso-\nciated with ﬁve captions. Similar to [6], we use the Karpathy\nsplits [26] that have been extensively used for reporting results\nin prior works. These splits merge the images from the original\ntrain and val splits, resulting in 121k images in total. After\nthat, the 123k images are split into 113k/5k/5k images for\ntraining/validation/testing, respectively. The trained models are\nensembled to obtain the predictions that are submitted to the\nofﬁcial MSCOCO test server. To evaluate the caption quality,\nwe use four automatic evaluation metrics, namely, BLEU [12],\nROUGE-L [45], METEOR [46] and CIDEr [13].\nVisual Genomeis a large-scale dataset to evaluate the interac-\ntions between objects in the images. It contains 108k images\nwith densely annotated objects, attributes, and relationships.\nFollowing the strategies in [6], we use the object and attribute\nannotations to pre-train the bottom-up-attention models. All\nthe images are split into training (98k images), validation\n(5k images) and testing (5k images). Since part of images\nin Visual Genome are also found in the MSCOCO captioning\ndataset, we perform careful checking to avoid affecting the\nMSCOCO validation and testing splits. Similar to [6], we\nperform extensive cleaning and ﬁltering of the training data\nto select 1,600 object classes and 400 attributes. This cleaned\ndataset is used for training our object detection models.\nB. Implementation Details\nFor the captions, we perform the pre-processing as follows.\nAll the caption sentences are converted to lower case and\ntokenized into words with white space. The rare words that\noccur less than 5 times or do not exist in the pre-trained\nGloVe vocabulary [37] are discarded, resulting in a vocabulary\nof 9,343 words. Each word in the caption is represented\nas word embedding vector by looking-up the GloVe word\nvocabulary. The out-of-vocabulary words are represented as\nall-zero vectors.\nFor the images, we use the pre-trained bottom-up-attention\nmodels to detect the objects and extract visual features for\nthe detected objects. For multi-view image representation, we\ntrained up to three Faster R-CNN [35] models ( i.e., number\nof views M=3) with different backbones, namely ResNet-101\n[33], ResNet-152 [33] and ResNeXt-101 [47], respectively. For\neach model, we select the top-100 objects with the highest\nconﬁdence scores to represent the image, where each object is\nrepresented as a vector by mean-pooling the last convolutional\nfeature from its detected region.\nThe hyper-parameters of the MT models that are used in\nthe experiments are listed as follows. The dimensionality of\ninput image features dx, and the input caption features dy are\n2048 and 512, respectively. According to the recommendation\nin [14], the latent dimensionality d in the MHA module is 512,\nthe number of heads h is 8, and the latent dimensionality for\neach head dh = d/h = 64. The number of attention blocks L\nin the encoder and decoder ranges in ∈{1, 2, 4, 6, 8}.\nTo train the MT models, we use the Adam solver [48]\nwith a batch size of 10. The base learning rate is set to\nmin(1te−4, 3e−4), where t is the current epoch number that\nstarts at 1. After 6 epochs, the learning rate is decayed by 1/2\nafter every 3 epochs. All models are ﬁrst trained for 15 epochs\nusing the cross-entropy loss and then are further trained for\nadditional 10 epochs using the self-critical loss to alleviate the\nexposure bias during cross-entropy optimization [10].\nC. Ablation Studies\nWe run a number of ablation experiments on MSCOCO\nimage captioning dataset to explore the effectiveness of\nthe single-view MT models (MT sv) with different hyper-\nparameters, as well as its multi-view variants with aligned\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7\n(a) Caption Representations: Scores of the MT sv models (ResNet-101\nbackbone) with different caption representations. The reference model uses\nrandomly initialized word embeddings and then ﬁne-tuned. PE denotes the\npositional encoding to model the temporal information of the caption [14].\nGloVept and GloVept+ft mean the word embeddings are pre-trained with\nGloVe, while GloVept+ft is additionally ﬁne-tuned along with the model.\nModel\nCross-Entropy Loss Self-Critical Loss\nB@1 M C B@1 M C\nRandft + PE 76.0 28.2 115.9 80.4 28.9 129.2\nGloVept + PE 76.2 28.0 116.6 80.5 29.0 129.3\nGloVept + LSTM 76.2 28.3 117.1 80.8 29.1 130.8\nGloVept+ft + LSTM 76.2 28.3 117.1 81.2 29.1 130.9\n(b) Number of Attention Blocks: Scores of the MTsv models\nwith different number of attention blocks L ∈ {1,2,4,6,8}.\nFor each model, we also report its corresponding number of\nmodel parameters.\nL #Params\n(×106)\nCross-Entropy Loss Self-Critical Loss\nB@1 M C B@1 M C\n1 17.1 76.3 27.9 113.7 79.4 28.3 124.3\n2 25.1 76.4 28.3 116.6 80.1 28.6 127.2\n4 39.8 76.5 28.4 117.1 80.4 29.0 129.6\n6 54.5 76.2 28.3 117.0 80.8 29.1 130.9\n8 69.2 76.4 28.1 116.5 80.7 29.0 130.4\n(c) Single-view vs. Multi-view: Scores of the MT models with single-\nview feature (MT sv), aligned multi-view features (MT amv) or unaligned\nmulti-view features (MT umv).\nModel Views\nCross-Entropy Loss Self-Critical Loss\nB@1 M C B@1 M C\nMTsv\nR-101 76.2 28.3 117.1 80.8 29.1 130.9\nR-152 76.4 28.4 117.5 81.0 29.3 131.2\nMTamv R-101 and R-152 77.0 28.6 119.4 81.2 29.4 132.7\nMTumv R-101 and R-152 77.1 28.6 119.5 81.6 29.5 133.4\n(d) Number of Views: Scores of the MT umv models with\ndifferent number of views M ∈ {2,3}.\nM Views\nCross-Entropy Loss Self-Critical Loss\nB@1 M C B@1 M C\n2\nR-101 and R-152 77.1 28.6 119.5 81.6 29.5 133.4\nR-101 and X-101 76.7 28.4 118.4 81.4 29.4 133.0\nR-152 and X-101 76.7 28.5 118.8 81.5 29.4 133.2\n3 R-101, R-152\nand X-101 77.3 28.7 119.6 81.9 29.5 134.1\nTABLE I: Ablations of the proposed MT models evaluated on the MSCOCO Karpathy test split. B@1, M, and C correspond\nto the BLEU@1, METEOR and CIDEr scores, respectively. For each model, we report the results optimized with either the\ncross-entropy loss or the self-critical loss [10]. R-101, R-152, X-101 denote the object detector with ResNet-101, ResNet-152\nand ResNeXt-101 backbones, respectively. All results are obtained with beam search in the testing stage. The best result for\neach evaluation metric is bolded.\nmulti-view image encoder MT amv and unaligned multi-view\nimage encoder MT umv. The results shown in Table I are\ndiscussed in detail below.\nCaption Representations: Table I(a) summarizes the ab-\nlation experiments on different caption representations for\nMTsv with the number of attention blocks L=6. Compared\nwith the reference model that uses randomly initialized word\nembeddings and positional encoding [14], we can see that\nusing the word embeddings that are pre-trained by GloVe\n[37] brings signiﬁcant improvements. In addition, introducing\nother tricks such as replacing PE with an LSTM network to\nmodel the temporal information, or ﬁne-tuning the GloVe word\nembeddings along with the MT model can slightly improve\nthe performance further. Note that the GloVe pt+LSTM model\nand the GloVe pt+ft+LSTM model report the same perfor-\nmance in the cross-entropy loss stage, as the ﬁne-tuning is\nperformed only in the self-critical loss stage. Directly ﬁne-\ntuning the GloVe embedding from scratch ( i.e., from the\ncross-entropy loss) leads to inferior performance. This result\ncan be explained as the word embeddings being sensitive to\nthe captioning performance, and training from scratch may\ndegenerate their representation capacity.\nNumber of Attention Blocks: Table I(b) shows the perfor-\nmance of the MT sv models with different number of attention\nblocks L ∈ {1, 2, 4, 6, 8}. We can see that the model size\ngrows linearly as L increases. Regarding the performance,\nwe have two observations as follows: 1) as increasing L, the\nmodel’s performance gradually improves and is saturated at\na certain number. This can be explained as a deeper model\ncapturing more complex relationships among objects, provid-\ning a more accurate understanding of the image contents. In\naddition, a deeper model has a larger representation capacity\nand has a larger risk to overﬁt the training set, and 2) the\noptimal model is achieved at different L that are trained with\ndifferent losses, i.e., L=4 for the cross-entropy loss and L=6\nfor the self-critical loss. The reinforcement learning-based\nself-critical loss provides a more diverse exploration of the\nhypothesis space to avoid overﬁtting, and thus it can better\nutilize the potential of large models.\nSingle-view vs. Multi-view: Next, we compare the MT model\nwith single-view or multi-view features in Table I(c). We use\ntwo Faster R-CNN models with different backbones (ResNet-\n101 or ResNet-152) to extract the multi-view features. For\nMTamv, the uniﬁed object boxes are extracted from the\ndetector with the ResNet-152 backbone. From the results,\nwe can see following that: 1) the representation capacity\nof the object detectors may slightly inﬂuence the image\ncaptioning performance. The MT sv model with the ResNet-\n152 backbone steadily outperforms the counterpart with the\nResNet-101 backbone; and 2) introducing multi-view features\nsigniﬁcantly improves the captioning performance over the\nsingle-view models. MTumv slightly outperforms MTamv, thus\nhighlighting the effect of using diverse multi-view features\nwith unaligned objects.\nNumber of Views: In Table I(d), we show the performance\nof the MT umv models with different number of views M. We\ncan see that when M = 2, different backbone combinations\nhave little inﬂuence on the captioning performance. Moreover,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8\nTABLE II: Single-model image captioning performance on the MSCOCO Karpathy test split. The methods marked with *\ndenote using the bottom-up-attention visual features from a pre-trained Faster R-CNN model. R, D, I-v3, I-v4 and IR-v2\ndenotes the ResNet, DenseNet, Inception-v3, Inception-v4 and Inception-ResNet-v2 model, respectively.\nModel Backbone\nCross-Entropy Loss Self-Critical Loss\nB@1 B@4 M R C B@1 B@4 M R C\nSCST [10] R-101 - 30.0 25.9 53.4 99.4 - 34.2 26.7 55.7 114.0\nADP-ATT [9] R-101 74.2 33.2 26.6 - 108.5 - - - - -\nLSTM-A [21] R-101 75.4 35.2 26.9 55.8 108.8 78.6 35.5 27.3 56.8 118.3\nUp-Down [6] R-101* 77.2 36.2 27.0 56.4 113.5 79.8 36.3 27.7 56.9 120.1\nRFNet [49] R, D, I-v3, I-v4 and IR-v2 76.4 35.8 27.4 56.5 112.5 79.1 36.5 27.7 57.3 121.9\nGCN-LSTM [22] R-101* 77.4 37.1 28.1 57.2 117.1 80.9 38.3 28.6 58.5 128.7\nMTsv (ours) R-101* 76.2 36.6 28.3 56.8 117.1 80.8 39.8 29.1 59.1 130.9\nMTumv (ours) R-101, R-152 and X-101* 77.3 37.4 28.7 57.4 119.6 81.9 40.7 29.5 59.7 134.1\nTABLE III: Real-time leaderboard of the state-of-the-art solutions on the online MSCOCO test server (April 21st, 2019).\nThe ﬁrst split shows the published solutions while the second split shows the unpublished ones.\nModel\nB@1 B@2 B@3 B@4 M R C\nc5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40\nGoogle NIC [50] 71.3 89.5 54.2 80.2 40.7 69.4 30.9 58.7 25.4 34.6 53.0 68.2 94.3 94.6\nM-RNN [51] 71.6 89.0 54.5 79.8 40.4 68.7 29.9 57.5 24.2 32.5 52.1 66.6 91.7 93.5\nLRCN [25] 71.8 89.5 54.8 80.4 40.9 69.5 30.6 58.5 24.7 33.5 52.8 67.8 92.1 93.4\nADP-ATT [9] 74.8 92.0 58.4 84.5 44.4 74.4 33.6 63.7 26.4 35.9 55.0 70.5 104.2 105.9\nLSTM-A [21] 78.7 93.7 62.7 86.7 47.6 76.5 35.6 65.2 27.0 35.4 56.4 70.5 116.0 118.0\nSCST [10] 78.1 93.7 61.9 86.0 47.0 75.9 35.2 65.5 27.0 35.5 56.3 70.7 114.7 116.7\nUp-Down [6] 80.2 95.2 64.1 88.8 49.1 79.4 36.9 68.5 27.6 36.7 57.1 72.4 117.9 120.5\nRFNet [49] 80.4 95.0 64.9 89.3 50.1 80.1 38.0 69.2 28.2 37.2 58.2 73.1 122.9 125.1\nGCN-LSTM [22] - - 65.5 89.3 50.8 80.3 38.7 69.7 28.5 37.6 58.5 73.4 125.3 126.5\nSRCB-ML-Lab 81.1 95.4 66.0 89.8 51.5 81.3 39.7 71.3 28.4 37.3 58.5 73.1 125.3 126.7\nh-p-hl 80.5 95.0 65.3 89.6 50.9 81.1 39.0 70.9 28.7 38.2 58.6 74.1 125.0 127.2\nTecentAI.v2 81.1 95.5 65.7 90.0 50.8 80.9 38.6 70.1 28.6 37.7 58.7 73.7 125.4 127.8\nlun 81.0 95.0 65.8 89.6 51.4 81.3 39.4 71.2 29.1 38.5 58.9 74.5 126.9 129.6\nMT (ours) 81.7 95.6 66.8 90.5 52.4 82.4 40.4 72.2 29.4 38.9 59.6 75.0 130.0 130.9\nincreasing the number of views M from 2 to 3 results in a\nslight performance improvement for MT umv, thus indicating\nthat the model is nearly saturated. Therefore, we do not further\nintroduce more views to the image encoder.\nD. Comparison with the State-of-the-Art\nBy taking the ablation results into account, we compare our\nbest single-view and multi-view MT models to the current\nstate-of-the-art approaches.\nResults on the Karpathy test split: In Table II, we report\nthe comparative results of our approaches along with the\nSCST [10], ADP-ATT [9], LSTM-A [21], Up-Down [6] and\nGCN-LSTM [22] on the Karpathy test split. Note that all the\ncompared methods use the same ResNet-101 backbone. With\nsingle-view features, the MT sv model outperforms most state-\nof-the-art methods, especially when it is optimized using the\nself-critical loss. When equipped with multi-view features, the\nMTumv model (trained with the self-critical loss) achieves the\nnew state-of-the-art single-model performance for this split\nin terms of all evaluation metrics. Note that the RFNet [49]\nalso incorporates multi-view features, and they introduce more\nviews than our approach (4 vs. 2). However, its performance\nis inferior to MT umv, which suggests that the strategy to fuse\nmulti-view features, rather than the number of views, is the\nkey to the captioning performance.\nResults on the ofﬁcial test server: We also submitted the\nresults of seven MT model ensemble (the MT sv, MTamv and\nMTumv models with different random seeds) to the ofﬁcial\nMSCOCO test server. 1 Table III demonstrates the results\nof the comparison to the state-of-the-art solutions on the\nleaderboard including the published ones (in the ﬁrst split) and\nthe unpublished ones (in the second split). C5 (or c40) denotes\nthe ofﬁcial test settings with 5 (or 40) ground-truth captions,\nrespectively. Compared to all the top performing solutions on\nthe leaderboard, our solution signiﬁcantly outperforms all the\nother solutions in terms of all reported evaluation metrics at\nthe time of submission (April 21st, 2019).\nE. Qualitative Analysis\nTo better understand the effectiveness of the proposed\napproach, we visualize the learned attentions of MT sv and\nMTumv in Fig. 5 and Fig. 6, respectively. Due to space limita-\ntions, we only show one example for each model and visualize\nthe attention maps from typical attention blocks. From the\ndemonstrated results, we have the following observations.\n1https://competitions.codalab.org/competitions/3221#results\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9\nDec(SA)-6\nDec(GA)-1\nDec(GA)-6\nSV: a woman riding a skateboard \ndown a street.\nGT: a woman who is skateboarding \ndown the street.\n5\n9\n3\n18\n16\nEnc(SA)-6\nDec(SA)-1Enc(SA)-1\n13\n15\nFig. 5: Visualizations of the 1st and 6th attention maps ( softmax(QK/\n√\nd)) of the MTsv model with R-101 backbone. Enc(SA)\ndenotes the self-attention in the image encoder; Dec(SA) and Dec(GA) denote the self-attention and guided-attention in the\ncaption decoder, respectively. GT denotes the one of the ﬁve ground-truth captions provided by MSCOCO. The index within\n[0-19] shown on the axes of the attention maps corresponds to each object in the image (20 objects in total) . For better\nvisualization effect, we highlight some objects in the image that receive have attention values.\nR-101\nUMV: a woman riding a skateboard down the street.\nGT: a woman who is skateboarding down the street. Enc(GA)-1 Enc(GA)-3 Enc(GA)-6\nR-152\n4\n10 5\n9\n11 12\n16\n18\n3\n13\n10 19\n17\n2\n58 6\n15\n1\nR-152 R-152 R-152\nR-101\nR-101\nR-101\nFig. 6: Visualizations of the 1st, 3rd and 6th attention maps of the MT umv model with R-101 and R-152 backbones. Enc(GA)\ndenotes the guided-attention in the multi-view image encoder.\nAttentions of the MT sv Encoder: The self-attentions (SA)\nof the 1st and 6th blocks in the image encoder that are in Fig.\n5 reﬂect the pairwise similarity of the visual objects. From\nthe results, we can see that the following: 1) in Enc(SA)-1,\nthe largest attention values almost appear on the diagonal line,\nindicating that the pairwise interactions are not learned in the\nﬁrst block; and 2) the largest values in Enc(SA)-6 form vertical\nlines (e.g., the 4th, 9th and 13th columns), which correspond to\nthe key objects of the image ( e.g., the girl and the skateboard).\nThis result reveals that all the attended features tend to use the\nfeatures of these key objects for the representation.\nAttentions of the MT sv Decoder: The self-attention the 1st\nand 6th blocks of the caption decoder that are shown in Fig.\n5 reﬂects the similarity of paired words. The largest attention\nvalues in Dec(SA)-1 almost appear on the diagonal line, which\nis similar to those in the Enc(SA)-1. In Dec(SA)-6, the word\nimportance and pairwise word similarities are simultaneously\nrepresented. For example, the columns of ‘ woman’ and ‘rid-\ning’ obtain focused attention weights, and the relationship\nbetween ‘woman’ and ‘skateboard’ is highlighted.\nThe guided-attention (GA) reﬂects the multimodal relation-\nships between word-object pairs. In Dec(GA)-1, the learned\nattentions are not concentrated, and some word-object simi-\nlarities are incorrect ( e.g., the 15th object is not related to the\nword ‘skateboard’). In contrast, the attention in Dec(GA)-6\nhas much clearer meanings. The co-attention of key objects\nalong with their word-object relationships are highlighted\naccordingly.\nAttentions of the MT umv Encoder: In Fig 6, we visualize\nthe 1st, 3rd and 6th guided-attention (GA) blocks in the multi-\nview image encoder. In Enc(GA)-1, the unaligned objects from\ndifferent views are adaptively aligned ( e.g., the 5th object in\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10\nSV: a toy cow standing in a parking lot .\nUMV: a white fire hydrant in a parking lot .\nGT: a fire hydrant in the middle of the \nparking lot.\nSV: two dogs and a dog laying on a couch .\nUMV: two dogs and a cat laying on a \ncouch.\nGT: two dogs and a cat laying down on a \ncouch.\nSV: a group of people playing with a \nfrisbee on the beach.\nUMV: a group of people playing volleyball \non the beach.\nGT: a few guys playing beach volleyball in \nthe sand.\nSV: three people walking down a street \nwith a pink umbrella.\nUMV: two people walking down a street \nwith a pink umbrella.\nGT: a couple sharing an umbrella on a \nrainy day.\nSV: a large clock in the middle of a \nbuilding.\nUMV: a clock on the side of a building .\nGT: a large golden clock sitting in the \nmiddle of a building.\nSV: a living room with chairs and a \ntelevision.\nUMV: a living room with a chair and a \ntelevision.\nGT: a television and some chairs in a \nroom.\nSV: a man sitting on a bench looking at a \ncell phone.\nUMV: a man sitting on a bench reading a \nbook.\nGT: a man sitting on top of a bench with a \nnewspaper.\nSV: a person walking down a path with a \ndog.\nUMV: two people walking a dog in a park .\nGT: a man is taking a walk with two dogs .\nFig. 7: Examples generated by the MT sv and MTumv models on MSCOCO validation set. GT denotes one of the ﬁve ground-\ntruth captions. The ﬁrst two rows show four examples that MT umv outperforms MTsv, and the third row shows two examples\nthat MTsv outperforms MTumv. The last row shows two examples that both models generate incorrect captions.\nR-101 and the 5-th object in R-152, and the 3rd object in R-\n101 and the 6th object in R-152). In Enc(GA)-3, the contextual\nrelationships are also involved ( e.g., the 5th object in R-152\nhas large attention values to the 1st and the 4th objects in\nR-101, which correspond to different parts of the woman’s\nbody). In Enc(GA)-6, the modeled contextual relationships\ncover speciﬁc objects and contain background scenes ( e.g., the\n13th object in R-152 and the 10-th object in R-101). These ob-\nservations reveal that the UMV image encoder learns to align\nthe objects and explores more complex interactions across\nmulti-view features to provide a ﬁne-grained understanding\nof the image content.\nMoreover, we show some predicted captioning examples in\nFig 7. The ﬁrst two rows show four examples where MT umv\noutperforms MT sv, and the third row shows two examples\nwhere MT sv outperforms MT umv. The last row shows two\nexamples where both models generate incorrect captions. From\nthe demonstrated results, we can see the following that: 1)\nalthough MTumv quantitatively outperforms MTsv, the perfor-\nmance gap is not qualitatively different and they have their\nown advantages in different cases. This results in a diverse\nensemble when they are integrated together; 2) the incorrect\ncaptions are caused by small objects ( e.g., the newspaper or\nthe second person).\nVI. C ONCLUSIONS\nIn this paper, we present a novel Multimodal Transformer\n(MT) framework for image captioning. The MT consists of an\nimage encoder that generates visual representations via deep\nself-attention learning, and a caption decoder to transform\nthe encoder’s visual features to textual captions. To further\nfacilitate the capacity of visual features, we introduce multi-\nview learning into the image encoder and propose two MT\nvariants, MT amv and MT umv, to model the aligned multi-\nview features and unaligned multi-view features, respectively.\nWe quantitatively and qualitatively evaluate the proposed MT\nmodels on the benchmark MSCOCO image captioning dataset\nand conduct extensive ablation studies to explore the reasons\nbehind the MT’ s effectiveness. Experimental results show that\nour method signiﬁcantly outperforms existing approaches, and\nan ensemble of seven models achieves the best performance on\nthe real-time leaderboard of the MSCOCO image captioning\nchallenge.\nREFERENCES\n[1] Z. Yu, F. Wu, Y . Yang, Q. Tian, J. Luo, and Y . Zhuang, “Discriminative\ncoupled dictionary hashing for fast cross-media retrieval,” in Associa-\ntion for Computing Machinerys Special Interest Group on Information\nRetrieval (ACM SIGIR)) , 2014, pp. 395–404.\n[2] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and\nM. Rohrbach, “Multimodal compact bilinear pooling for visual question\nanswering and visual grounding,” arXiv preprint arXiv:1606.01847 ,\n2016.\n[3] J.-H. Kim, J. Jun, and B.-T. Zhang, “Bilinear attention networks,”\nAdvances in Neural Information Processing Systems (NIPS) , 2018.\n[4] Z. Yu, J. Yu, J. Fan, and D. Tao, “Multi-modal factorized bilinear\npooling with co-attention learning for visual question answering,” IEEE\nInternational Conference on Computer Vision (ICCV) , pp. 1839–1848,\n2017.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11\n[5] Z. Yu, J. Yu, C. Xiang, Z. Zhao, Q. Tian, and D. Tao, “Rethinking\ndiversiﬁed and discriminative proposal generation for visual grounding,”\nInternational Joint Conference on Artiﬁcial Intelligence (IJCAI) , pp.\n1114–1120, 2018.\n[6] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and\nL. Zhang, “Bottom-up and top-down attention for image captioning and\nvisual question answering,” in IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , June 2018.\n[7] H. Xu and K. Saenko, “Ask, attend and answer: Exploring question-\nguided spatial attention for visual question answering,” in European\nConference on Computer Vision (ECCV) , 2016, pp. 451–466.\n[8] N. Xu, A.-A. Liu, Y . Wong, Y . Zhang, W. Nie, Y . Su, and M. Kankan-\nhalli, “Dual-stream recurrent neural network for video captioning,”IEEE\nTransactions on Circuits and Systems for Video Technology (TCSVT) ,\n2018.\n[9] J. Lu, C. Xiong, D. Parikh, and R. Socher, “Knowing when to look:\nAdaptive attention via a visual sentinel for image captioning,” in IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), 2017,\npp. 375–383.\n[10] S. J. Rennie, E. Marcheret, Y . Mroueh, J. Ross, and V . Goel, “Self-\ncritical sequence training for image captioning,” in IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) , 2017, pp. 7008–\n7024.\n[11] I. Sutskever, O. Vinyals, and Q. V . Le, “Sequence to sequence learning\nwith neural networks,” in Advances in Neural Information Processing\nSystems (NIPS), 2014, pp. 3104–3112.\n[12] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method\nfor automatic evaluation of machine translation,” in Association for\nComputational Linguistics (ACL) . Association for Computational\nLinguistics, 2002, pp. 311–318.\n[13] R. Vedantam, C. Lawrence Zitnick, and D. Parikh, “Cider: Consensus-\nbased image description evaluation,” in IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) , 2015, pp. 4566–4575.\n[14] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances in\nNeural Information Processing Systems (NIPS) , 2017, pp. 6000–6010.\n[15] G. Kulkarni, V . Premraj, V . Ordonez, S. Dhar, S. Li, Y . Choi, A. C. Berg,\nand T. L. Berg, “Babytalk: Understanding and generating simple image\ndescriptions,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence (TPAMI), vol. 35, no. 12, pp. 2891–2903, 2013.\n[16] M. Mitchell, X. Han, J. Dodge, A. Mensch, A. Goyal, A. Berg, K. Ya-\nmaguchi, T. Berg, K. Stratos, and H. Daum ´e III, “Midge: Generating\nimage descriptions from computer vision detections,” in Association\nfor Computational Linguistics (ACL) . Association for Computational\nLinguistics, 2012, pp. 747–756.\n[17] Y . Yang, C. L. Teo, H. Daum ´e III, and Y . Aloimonos, “Corpus-guided\nsentence generation of natural images,” in Conference on Empirical\nMethods in Natural Language Processing (EMNLP) , 2011, pp. 444–\n454.\n[18] A. Karpathy, A. Joulin, and L. F. Fei-Fei, “Deep fragment embeddings\nfor bidirectional image sentence mapping,” in Advances in Neural\nInformation Processing Systems (NIPS) , 2014, pp. 1889–1897.\n[19] A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young, C. Rashtchian,\nJ. Hockenmaier, and D. Forsyth, “Every picture tells a story: Generating\nsentences from images,” in European Conference on Computer Vision\n(ECCV). Springer, 2010, pp. 15–29.\n[20] J. Devlin, H. Cheng, H. Fang, S. Gupta, L. Deng, X. He, G. Zweig, and\nM. Mitchell, “Language models for image captioning: The quirks and\nwhat works,” arXiv preprint arXiv:1505.01809 , 2015.\n[21] T. Yao, Y . Pan, Y . Li, Z. Qiu, and T. Mei, “Boosting image captioning\nwith attributes,” in IEEE International Conference on Computer Vision\n(ICCV), 2017, pp. 4894–4902.\n[22] T. Yao, Y . Pan, Y . Li, and T. Mei, “Exploring visual relationship\nfor image captioning,” in European Conference on Computer Vision\n(ECCV), 2018, pp. 684–699.\n[23] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,\nV . Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,\n2015, pp. 1–9.\n[24] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\nComputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[25] J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venu-\ngopalan, K. Saenko, and T. Darrell, “Long-term recurrent convolutional\nnetworks for visual recognition and description,” in IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) , 2015, pp. 2625–\n2634.\n[26] A. Karpathy and L. Fei-Fei, “Deep visual-semantic alignments for\ngenerating image descriptions,” in IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , 2015, pp. 3128–3137.\n[27] L. Chen, H. Zhang, J. Xiao, L. Nie, J. Shao, W. Liu, and T.-S. Chua,\n“Sca-cnn: Spatial and channel-wise attention in convolutional networks\nfor image captioning,” in IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2017, pp. 5659–5667.\n[28] J. Lu, J. Yang, D. Batra, and D. Parikh, “Hierarchical question-image\nco-attention for visual question answering,” in Advances in neural\ninformation processing systems (NIPS) , 2016, pp. 289–297.\n[29] Z. Yu, J. Yu, C. Xiang, J. Fan, and D. Tao, “Beyond bilinear: Generalized\nmultimodal factorized high-order pooling for visual question answering,”\nIEEE Transactions on Neural Networks and Learning Systems , vol. 29,\nno. 12, pp. 5947–5959, 2018.\n[30] H. Nam, J.-W. Ha, and J. Kim, “Dual attention networks for multimodal\nreasoning and matching,” arXiv preprint arXiv:1611.00471 , 2016.\n[31] D.-K. Nguyen and T. Okatani, “Improved fusion of visual and language\nrepresentations by dense symmetric co-attention for visual question an-\nswering,”IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pp. 6087–6096, 2018.\n[32] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-\ndinov, “Dropout: a simple way to prevent neural networks from over-\nﬁtting,” The Journal of Machine Learning Research (JMLR) , vol. 15,\nno. 1, pp. 1929–1958, 2014.\n[33] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” inIEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2016, pp. 770–778.\n[34] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv\npreprint arXiv:1607.06450, 2016.\n[35] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time\nobject detection with region proposal networks,” in Advances in Neural\nInformation Processing Systems (NIPS) , 2015, pp. 91–99.\n[36] R. Krishna, Y . Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,\nY . Kalantidis, L.-J. Li, D. A. Shammaet al., “Visual genome: Connecting\nlanguage and vision using crowdsourced dense image annotations,”\nInternational Journal of Computer Vision (IJCV) , vol. 123, no. 1, pp.\n32–73, 2017.\n[37] J. Pennington, R. Socher, and C. Manning, “Glove: Global vectors for\nword representation,” in Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP) , 2014, pp. 1532–1543.\n[38] J. Yu, Y . Rui, Y . Y . Tang, and D. Tao, “High-order distance-based multi-\nview stochastic learning in image classiﬁcation,” IEEE Transactions On\nCybernetics (CYB), vol. 44, no. 12, pp. 2431–2442, 2014.\n[39] Z. Tu, W. Xie, J. Dauwels, B. Li, and J. Yuan, “Semantic cues\nenhanced multi-modality multi-stream cnn for action recognition,” IEEE\nTransactions on Circuits and Systems for Video Technology (TCSVT) ,\n2018.\n[40] D. Tao, Y . Guo, B. Yu, J. Pang, and Z. Yu, “Deep multi-view feature\nlearning for person re-identiﬁcation,” IEEE Transactions on Circuits and\nSystems for Video Technology (TCSVT), vol. 28, no. 10, pp. 2657–2666,\n2018.\n[41] R. Girshick, “Fast r-cnn,” in IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2015, pp. 1440–1448.\n[42] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. Doll ´ar, “Focal loss for\ndense object detection,” in IEEE international conference on computer\nvision (ICCV), 2017, pp. 2980–2988.\n[43] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look\nonce: Uniﬁed, real-time object detection,” in IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2016, pp. 779–788.\n[44] T.-Y . Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays,\nP. Perona, D. Ramanan, C. L. Zitnick, and P. Doll ´ar, “Microsoft coco:\nCommon objects in context,” arXiv preprint arXiv:1405.0312 , 2014.\n[45] C.-Y . Lin, “Rouge: A package for automatic evaluation of summaries,”\nin The 42nd Annual Meeting of the Association for Computational\nLinguistics (ACL) Workshop, 2004, p. 10.\n[46] S. Banerjee and A. Lavie, “Meteor: An automatic metric for mt\nevaluation with improved correlation with human judgments,” in The\n43rd Annual Meeting of the Association for Computational Linguistics\n(ACL) Workshop, 2005, pp. 65–72.\n[47] S. Xie, R. Girshick, P. Doll ´ar, Z. Tu, and K. He, “Aggregated residual\ntransformations for deep neural networks,” in IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2017, pp. 1492–\n1500.\n[48] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.6980 , 2014.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12\n[49] W. Jiang, L. Ma, Y .-G. Jiang, W. Liu, and T. Zhang, “Recurrent fusion\nnetwork for image captioning,” in European Conference on Computer\nVision (ECCV), 2018, pp. 499–515.\n[50] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell: A neural\nimage caption generator,” in IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2015, pp. 3156–3164.\n[51] J. Mao, W. Xu, Y . Yang, J. Wang, Z. Huang, and A. Yuille, “Deep\ncaptioning with multimodal recurrent neural networks (m-rnn),” in\nInternational Conference on Learning Representations (ICLR) , 2015.\nJun Yu (M’13) received the B.Eng. and Ph.D.\ndegrees from Zhejiang University, Zhejiang, China.\nHe was an Associate Professor with the School of\nInformation Science and Technology, Xiamen Uni-\nversity, Xiamen, China. From 2009 to 2011, he was\nwith Nanyang Technological University, Singapore.\nFrom 2012 to 2013, he was a Visiting Researcher\nat Microsoft Research Asia (MSRA). He is cur-\nrently a Professor with the School of Computer Sci-\nence and Technology, Hangzhou Dianzi University,\nHangzhou, China. He has authored or coauthored\nmore than 100 scientiﬁc articles. Over the past years, his research interests\nhave included multimedia analysis, machine learning, and image processing.\nHe is the associate editor of IEEE Trans. on CSVT and Pattern Recognition,\nand the reviewer of various international journals including IEEE Trans. on\nPAMI, IEEE Trans. on Image Processing, IEEE Trans. on Multimedia, etc. In\n2017, he received the IEEE SPS Best Paper Award. Dr. Yu has (co-)chaired\nseveral special sessions, invited sessions, and workshops. He served as a\nprogram committee member or reviewer of top conferences and prestigious\njournals. He is a Professional Member of the Association for Computing\nMachinery and the China Computer Federation.\nJing Li received the B.Eng. degree from the\nSchool of Management, Hangzhou Dianzi Univer-\nsity, Hangzhou, China, in 2017. He is currently\npursuing the M.Eng. degree with the School of Com-\nputer Science and Technology, Hangzhou Dianzi\nUniversity, Hangzhou, China. His current research\ninterests include multimodal analysis, computer vi-\nsion and machine learning.\nZhou Yu received the B.Eng. and Ph.D. degrees\nfrom Zhejiang University, Zhejiang, China, in 2010\nand 2015, respectively. He is currently an Asso-\nciate Professor with the School of Computer Sci-\nence and Technology, Hangzhou Dianzi Univer-\nsity, Hangzhou, China. His research interests in-\ncludes multimodal analysis, computer vision, ma-\nchine learning and deep learning. He has served\nas reviewers or program committee members of\nprestigious journals and top conferences including\nIEEE Trans. on CSVT, IEEE Trans. on Multimedia,\nIEEE Trans. on Image Processing, IJCAI and AAAI, etc.\nQingming Huang (F’18) is a professor in the\nUniversity of Chinese Academy of Sciences and an\nadjunct research professor in the Institute of Com-\nputing Technology, Chinese Academy of Sciences.\nHe graduated with a Bachelor degree in Computer\nScience in 1988 and Ph.D. degree in Computer\nEngineering in 1994, both from Harbin Institute\nof Technology, China. His research areas include\nmultimedia computing, image processing, computer\nvision and pattern recognition. He has authored\nor coauthored more than 400 academic papers in\nprestigious international journals and top-level international conferences. He\nis the associate editor of IEEE Trans. on CSVT and Acta Automatica Sinica,\nand the reviewer of various international journals including IEEE Trans. on\nPAMI, IEEE Trans. on Image Processing, IEEE Trans. on Multimedia, etc.\nHe is a Fellow of IEEE and has served as general chair, program chair, track\nchair and TPC member for various conferences, including ACM Multimedia,\nCVPR, ICCV , ICME, ICMR, PCM, BigMM, PSIVT, etc.",
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.9714961051940918
    },
    {
      "name": "Computer science",
      "score": 0.8211235404014587
    },
    {
      "name": "Transformer",
      "score": 0.5750287771224976
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5735081434249878
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.46182021498680115
    },
    {
      "name": "Encoder",
      "score": 0.4527774155139923
    },
    {
      "name": "Image (mathematics)",
      "score": 0.4494844377040863
    },
    {
      "name": "Vocabulary",
      "score": 0.41342997550964355
    },
    {
      "name": "Computer vision",
      "score": 0.32868683338165283
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.32404446601867676
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I50760025",
      "name": "Hangzhou Dianzi University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    }
  ]
}