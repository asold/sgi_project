{
  "title": "An Analysis of the Utility of Explicit Negative Examples to Improve the Syntactic Abilities of Neural Language Models",
  "url": "https://openalex.org/W3035597164",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5076043621",
      "name": "Hiroshi Noji",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5042127858",
      "name": "Hiroya Takamura",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2888922637",
    "https://openalex.org/W2963614302",
    "https://openalex.org/W2117823388",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2054125330",
    "https://openalex.org/W2787642437",
    "https://openalex.org/W4255690937",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2962964385",
    "https://openalex.org/W2995404354",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2549416390",
    "https://openalex.org/W2945185449",
    "https://openalex.org/W2798727047",
    "https://openalex.org/W2963951265",
    "https://openalex.org/W2971016963",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2949629417",
    "https://openalex.org/W2152496361",
    "https://openalex.org/W2625014264",
    "https://openalex.org/W2918996109",
    "https://openalex.org/W4299838440",
    "https://openalex.org/W2164418233",
    "https://openalex.org/W2968297680",
    "https://openalex.org/W2886663262",
    "https://openalex.org/W2963084773",
    "https://openalex.org/W2963351454",
    "https://openalex.org/W2962832505",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2139450036",
    "https://openalex.org/W2963073938"
  ],
  "abstract": "We explore the utilities of explicit negative examples in training neural language models. Negative examples here are incorrect words in a sentence, such as barks in *The dogs barks. Neural language models are commonly trained only on positive examples, a set of sentences in the training data, but recent studies suggest that the models trained in this way are not capable of robustly handling complex syntactic constructions, such as long-distance agreement. In this paper, we first demonstrate that appropriately using negative examples about particular constructions (e.g., subject-verb agreement) will boost the model’s robustness on them in English, with a negligible loss of perplexity. The key to our success is an additional margin loss between the log-likelihoods of a correct word and an incorrect word. We then provide a detailed analysis of the trained models. One of our findings is the difficulty of object-relative clauses for RNNs. We find that even with our direct learning signals the models still suffer from resolving agreement across an object-relative clause. Augmentation of training sentences involving the constructions somewhat helps, but the accuracy still does not reach the level of subject-relative clauses. Although not directly cognitively appealing, our method can be a tool to analyze the true architectural limitation of neural models on challenging linguistic constructions.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3375–3385\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n3375\nAn Analysis of the Utility of Explicit Negative Examples\nto Improve the Syntactic Abilities of Neural Language Models\nHiroshi Noji\nArtiﬁcial Intelligence Research Center\nAIST, Tokyo, Japan\nhiroshi.noji@aist.go.jp\nHiroya Takamura\nArtiﬁcial Intelligence Research Center\nAIST, Tokyo, Japan\ntakamura.hiroya@aist.go.jp\nAbstract\nWe explore the utilities of explicit negative\nexamples in training neural language mod-\nels. Negative examples here are incorrect\nwords in a sentence, such as barks in * The\ndogs barks. Neural language models are com-\nmonly trained only on positive examples, a\nset of sentences in the training data, but re-\ncent studies suggest that the models trained\nin this way are not capable of robustly han-\ndling complex syntactic constructions, such as\nlong-distance agreement. In this paper, we ﬁrst\ndemonstrate that appropriately using negative\nexamples about particular constructions (e.g.,\nsubject-verb agreement) will boost the model’s\nrobustness on them in English, with a negli-\ngible loss of perplexity. The key to our suc-\ncess is an additional margin loss between the\nlog-likelihoods of a correct word and an incor-\nrect word. We then provide a detailed analy-\nsis of the trained models. One of our ﬁndings\nis the difﬁculty of object-relative clauses for\nRNNs. We ﬁnd that even with our direct learn-\ning signals the models still suffer from resolv-\ning agreement across an object-relative clause.\nAugmentation of training sentences involving\nthe constructions somewhat helps, but the ac-\ncuracy still does not reach the level of subject-\nrelative clauses. Although not directly cogni-\ntively appealing, our method can be a tool to\nanalyze the true architectural limitation of neu-\nral models on challenging linguistic construc-\ntions.\n1 Introduction\nDespite not being exposed to explicit syntactic su-\npervision, neural language models (LMs), such\nas recurrent neural networks, are able to generate\nﬂuent and natural sentences, suggesting that they\ninduce syntactic knowledge about the language\nto some extent. However, it is still under debate\nwhether such induced knowledge about grammar is\nrobust enough to deal with syntactically challeng-\ning constructions such as long-distance subject-\nverb agreement. So far, the results for RNN lan-\nguage models (RNN-LMs) trained only with raw\ntext are overall negative; prior work has reported\nlow performance on the challenging test cases (Mar-\nvin and Linzen, 2018) even with the massive size\nof the data and model (van Schijndel et al., 2019),\nor argue the necessity of an architectural change\nto track the syntactic structure explicitly (Wilcox\net al., 2019b; Kuncoro et al., 2018). Here the task\nis to evaluate whether a model assigns a higher\nlikelihood on a grammatically correct sentence (1a)\nover an incorrect sentence (1b) that is minimally\ndifferent from the original one (Linzen et al., 2016).\n(1) a. The author that the guards like laughs.\nb. * The author that the guards like laugh .\nIn this paper, to obtain a new insight into the syn-\ntactic abilities of neural LMs, in particular RNN-\nLMs, we perform a series of experiments under a\ndifferent condition from the prior work. Speciﬁ-\ncally, we extensively analyze the performance of\nthe models that are exposed to explicit negative\nexamples. In this work, negative examples are the\nsentences or tokens that are grammatically incor-\nrect, such as (1b) above.\nSince these negative examples provide a direct\nlearning signal on the task at test time it may not\nbe very surprising if the task performance goes up.\nWe acknowledge this, and argue that our motiva-\ntion for this setup is to deepen understanding, in\nparticular the limitation or the capacity of the cur-\nrent architectures, which we expect can be reached\nwith such strong supervision. Another motivation\nis engineering: we could exploit negative examples\nin different ways, and establishing a better way\nwill be of practical importance toward building an\nLM or generator that can be robust on particular\nlinguistic constructions.\n3376\nThe ﬁrst research question we pursue is about\nthis latter point: what is a better method to uti-\nlize negative examples that help LMs to acquire\nrobustness on the target syntactic constructions?\nRegarding this point, we ﬁnd that adding additional\ntoken-level loss trying to guarantee a margin be-\ntween log-probabilities for the correct and incorrect\nwords (e.g., log p(laughs|h) and log p(laugh|h) for\n(1a)) is superior to the alternatives. On the test set\nof Marvin and Linzen (2018), we show that LSTM\nlanguage models (LSTM-LMs) trained by this loss\nreach near perfect level on most syntactic construc-\ntions for which we create negative examples, with\nonly a slight increase of perplexity about 1.0 point.\nPast work conceptually similar to us is Engue-\nhard et al. (2017), which, while not directly ex-\nploiting negative examples, trains an LM with ad-\nditional explicit supervision signals to the evalua-\ntion task. They hypothesize that LSTMs do have\nenough capacity to acquire robust syntactic abili-\nties but the learning signals given by the raw text\nare weak, and show that multi-task learning with a\nbinary classiﬁcation task to predict the upcoming\nverb form (singular or plural) helps models aware\nof the target syntax (subject-verb agreement). Our\nexperiments basically conﬁrm and strengthen this\nargument, with even stronger learning signals from\nnegative examples, and we argue this allows us to\nevaluate the true capacity of the current architec-\ntures. In our experiments (Section 4), we show that\nour margin loss achieves higher syntactic perfor-\nmance than their multi-task learning.\nAnother relevant work on the capacity of LSTM-\nLMs is Kuncoro et al. (2019), which shows that by\ndistilling from syntactic LMs (Dyer et al., 2016),\nLSTM-LMs can improve their robustness on var-\nious agreement phenomena. We show that our\nLMs with the margin loss outperform theirs in most\nof the aspects, further strengthening the argument\nabout a stronger capacity of LSTM-LMs.\nThe latter part of this paper is a detailed anal-\nysis of the trained models and introduced losses.\nOur second question is about the true limitation of\nLSTM-LMs: are there still any syntactic construc-\ntions that the models cannot handle robustly even\nwith our direct learning signals? This question can\nbe seen as a ﬁne-grained one raised by Enguehard\net al. (2017) with a stronger tool and improved eval-\nuation metric. Among tested constructions, we ﬁnd\nthat syntactic agreement across an object relative\nclause (RC) is challenging. To inspect whether this\nis due to the architectural limitation, we train an-\nother LM on a dataset, on which we unnaturally\naugment sentences involving object RCs. Since it\nis known that object RCs are relatively rare com-\npared to subject RCs (Hale, 2001), frequency may\nbe the main reason for the lower performance. In-\nterestingly, even when increasing the number of\nsentences with an object RC by eight times (more\nthan twice of sentences with a subject RC), the ac-\ncuracy does not reach the same level as agreement\nacross a subject RC. This result suggests an inher-\nent difﬁculty in tracking a syntactic state across an\nobject RC for sequential neural architectures.\nWe ﬁnally provide an ablation study to under-\nstand the encoded linguistic knowledge in the mod-\nels learned with the help of our method. We exper-\niment under reduced supervision at two different\nlevels: (1) at a lexical level, by not giving negative\nexamples on verbs that appear in the test set; (2)\nat a construction level, by not giving negative ex-\namples about a particular construction, e.g., verbs\nafter a subject RC. We observe no huge score drops\nby both. This suggests that our learning signals\nat a lexical level (negative words) strengthen the\nabstract syntactic knowledge about the target con-\nstructions, and also that the models can generalize\nthe knowledge acquired by negative examples to\nsimilar constructions for which negative examples\nare not explicitly given. The result also implies\nthat negative examples do not have to be complete\nand can be noisy, which will be appealing from an\nengineering perspective.\n2 Target Task and Setup\nThe most common evaluation metric of an LM\nis perplexity. Although neural LMs achieve im-\npressive perplexity (Merity et al., 2018), it is an\naverage score across all tokens and does not inform\nthe models’ behaviors on linguistically challenging\nstructures, which are rare in the corpus. This is\nthe primary motivation to separately evaluate the\nmodels’ syntactic robustness by a different task.\n2.1 Syntactic evaluation task\nAs introduced in Section 1, the task for a model\nis to assign a higher probability to the grammati-\ncal sentence over the ungrammatical one, given a\npair of minimally different sentences at a critical\nposition affecting the grammaticality. For example,\n(1a) and (1b) only differ at a ﬁnal verb form, and\nto assign a higher probability to (1a), models need\n3377\nto be aware of the agreement dependency between\nauthor and laughs over an RC.\nMarvin and Linzen (2018) test set While ini-\ntial work (Linzen et al., 2016; Gulordava et al.,\n2018) has collected test examples from naturally\noccurring sentences, this approach suffers from the\ncoverage issue, as syntactically challenging exam-\nples are relatively rare. We use the test set compiled\nby Marvin and Linzen (2018), which consists of\nsynthetic examples (in English) created by a ﬁxed\nvocabulary and a grammar. This approach allows\nus to collect varieties of sentences with complex\nstructures.\nThe test set is divided by the syntactic construc-\ntions appearing in each example. Many construc-\ntions are different types of subject-verb agreement,\nincluding local agreement on different sentential\npositions (2), and non-local agreement across dif-\nferent types of phrases. Intervening phrases include\nprepositional phrases, subject RCs, object RCs, and\ncoordinated verb phrases (3). (1) is an example of\nagreement across an object RC.\n(2) The senators smile/*smiles .\n(3) The senators like to watch television shows\nand are/*is twenty three years old.\nPrevious work has shown that non-local agreement\nis particularly challenging for sequential neural\nmodels (Marvin and Linzen, 2018).\nThe other patterns are reﬂexive anaphora depen-\ndencies between a noun and a reﬂexive pronoun\n(4), and on negative polarity items (NPIs), such\nas ever, which requires a preceding negation word\n(e.g., no and none) at an appropriate scope (5):\n(4) The authors hurt themselves/*himself .\n(5) No/*Most authors have ever been popular.\nNote that NPI examples differ from the others\nin that the context determining the grammaticality\nof the target word (No/*Most) does not precede\nit. Rather, the grammaticality is determined by\nthe following context. As we discuss in Section 3,\nthis property makes it difﬁcult to apply training\nwith negative examples for NPIs for most of the\nmethods studied in this work.\nAll examples above (1–5) are actual test sen-\ntences, and we can see that since they are synthetic\nsome may sound somewhat unnatural. The main\nargument behind using this dataset is that even\nnot very natural, they are still strictly grammatical,\nand an LM equipped with robust syntactic abilities\nshould be able to handle them as a human would\ndo.\nWe use the original test set used in Marvin and\nLinzen (2018).1 See the supplementary materials\nof this for the lexical items and example sentences\nin each construction.\n2.2 Language models\nTraining data Following the practice, we train\nLMs on the dataset not directly relevant to the\ntest set. Throughout the paper, we use an English\nWikipedia corpus assembled by Gulordava et al.\n(2018), which has been used as training data for\nthe present task (Marvin and Linzen, 2018; Kun-\ncoro et al., 2019), consisting of 80M/10M/10M\ntokens for training/dev/test sets. It is tokenized and\nrare words are replaced by a single unknown token,\namounting to the vocabulary size of 50,000.\nBaseline LSTM-LM Since our focus in this pa-\nper is an additional loss exploiting negative exam-\nples (Section 3), we ﬁx the baseline LM through-\nout the experiments. Our baseline is a three-layer\nLSTM-LM with 1,150 hidden units at internal lay-\ners trained with the standard cross-entropy loss.\nWord embeddings are 400-dimensional, and in-\nput and output embeddings are tied (Inan et al.,\n2016). Deviating from some prior work (Mar-\nvin and Linzen, 2018; van Schijndel et al., 2019),\nwe train LMs at sentence level as in sequence-to-\nsequence models (Sutskever et al., 2014). This\nsetting has been employed in some previous work\n(Kuncoro et al., 2018, 2019).2\nParameters are optimized by SGD. For regular-\nization, we apply dropout on word embeddings\nand outputs of every layer of LSTMs, with weight\ndecay of 1.2e-6, and anneal the learning rate by\n0.5 if the validation perplexity does not improve\nsuccessively, checking every 5,000 mini-batches.\nMini-batch size, dropout weight, and initial learn-\ning rate are tuned by perplexity on the dev set of\nWikipedia dataset.3 Note that we tune these values\nfor the baseline LSTM-LM and ﬁx them across the\nexperiments.\n1We use the “EMNLP2018” templates in\nhttps://github.com/BeckyMarvin/LM syneval.\n2On the other hand, the LSTM-LM of Marvin and Linzen\n(2018), which is prepared by Gulordava et al. (2018), is trained\nat document level through truncated backpropagation through\ntime (BPTT) (Mikolov et al., 2011). Since our training regime\nis more akin to the task setting of syntactic evaluation, it may\nprovide some advantage at test time.\n3Following values are found: mini-batch size: 128; initial\nlearnin rate: 20.0; dropout weight on the word embedding\nlayer and each output layer of LSTM: 0.1.\n3378\nThe size of our three-layer LM is the same as\nthe state-of-the-art LSTM-LM at document-level\n(Merity et al., 2018). Marvin and Linzen (2018)’s\nLSTM-LM is two-layer with 650 hidden units and\nword embeddings. Comparing two, since the word\nembeddings of our models are smaller (400 vs. 650)\nthe total model sizes are comparable (40M for ours\nvs. 39M for theirs). Nonetheless, we will see in the\nﬁrst experiment that our carefully tuned three-layer\nmodel achieves much higher syntactic performance\nthan their model (Section 4), being a stronger base-\nline to our extensions, which we introduce next.\n3 Learning with Negative Examples\nNow we describe four additional losses for exploit-\ning negative examples. The ﬁrst two are existing\nones, proposed for a similar purpose or under a\ndifferent motivation. As far as we know, the latter\ntwo have not appeared in past work.4\nWe note that we create negative examples by\nmodifying the original Wikipedia training sen-\ntences, not sentences in the test set. As a running\nexample, let us consider the case where sentence\n(6a) exists in a mini-batch, from which we create a\nnegative example (6b).\n(6) a. An industrial park with several compa-\nnies is located in the close vicinity.\nb. * An industrial park with several compa-\nnies are located in the close vicinity.\nNotations By a target word, we mean a word\nfor which we create a negative example (e.g., is).\nWe distinguish two types of negative examples: a\nnegative tokenand a negative sentence; the former\nmeans a single incorrect word (e.g., are), while the\nlatter means an entire ungrammatical sentence.\n3.1 Negative Example Losses\nBinary-classiﬁcation loss This is proposed by\nEnguehard et al. (2017) to complement a weak\ninductive bias in LSTM-LMs for learning syntax.\nIt is multi-task learning across the cross-entropy\nloss (Llm) and an additional loss (Ladd):\nL= Llm + βLadd, (1)\nwhere β is a relative weight for Ladd. Given out-\nputs of LSTMs, a linear and binary softmax layers\n4The loss for large-margin language models (Huang et al.,\n2018) is similar to our sentence-level margin loss. Whereas\ntheir formulation is more akin to the standard large-margin\nsetting, aiming to learn a reranking model, our margin loss\nis simpler, just comparing two log-likelihoods of predeﬁned\npositive and negative sentences.\npredict whether the next token is singular or plural.\nLadd is a loss for this classiﬁcation, only deﬁned\nfor the contexts preceding a target token xi:\nLadd =\n∑\nx1:i∈h∗\n−log p(num(xi)|x1:i−1),\nwhere x1:i = x1 ···xi is a preﬁx sequence and h∗\nis a set of all preﬁxes ending with a target word\n(e.g., An industrial park with several companies is)\nin the training data. num(x) ∈{singular, plural}\nis a function returning the number of x. In practice,\nfor each mini-batch for Llm, we calculate Ladd\nfor the same set of sentences and add these two to\nobtain a total loss for updating parameters.\nAs we mentioned in Section 1, this loss does not\nexploit negative examples explicitly; essentially a\nmodel is only informed of a key position (target\nword) that determines the grammaticality. This is\nrather an indirect learning signal, and we expect\nthat it does not outperform the other approaches.\nUnlikelihood loss This is recently proposed\n(Welleck et al., 2020) for resolving the repetition\nissue, a known problem for neural text genera-\ntors (Holtzman et al., 2019). Aiming at learning a\nmodel that can suppress repetition, they introduce\nan unlikelihood loss, which is an additional loss\nat a token level and explicitly penalizes choosing\nwords previously appeared in the current context.\nWe customize their loss for negative tokens x∗\ni\n(e.g., are in (6b)). Since this loss is added at token-\nlevel, instead of Eq. 1 the total loss is Llm, which\nwe modify as:\n∑\nx∈D\n∑\nxi∈x\n−log p(xi|x1:i−1) +\n∑\nx∗\ni ∈negt(xi)\ng(x∗\ni ),\ng(x∗\ni ) =−αlog(1 −p(x∗\ni |x1:i−1)),\nwhere negt(·) returns negative tokens for a target\nxi.5 αcontrols the weight. x is a sentence in the\ntraining data D. The unlikelihood loss strength-\nens the signal to penalize undesirable words in a\ncontext by explicitly reducing the likelihood of\nnegative tokens x∗\ni . This is a more direct learning\nsignal than the binary classiﬁcation loss.\nSentence-level margin loss We propose a differ-\nent loss, in which the likelihoods for correct and\nincorrect sentences are more tightly coupled. As in\n5Empty for non-target tokens. It may return multiple to-\nkens sometimes, e.g., themselves→{himself, herself}.\n3379\nthe binary classiﬁcation loss, the total loss is given\nby Eq. 1. We consider the following loss for Ladd:\n∑\nx∈D\n∑\nx∗\nj ∈negs(x)\nmax(0,δ−(log p(x)−log p(x∗\nj ))),\nwhere δ is a margin value between the log-\nlikelihood of original sentence x and negative sen-\ntences {x∗\nj }. negs(·) returns a set of negative sen-\ntences by modifying the original one. Note that\nwe change only one token for each x∗\nj , and thus\nmay obtain multiple negative sentences from one\nx when it contains multiple target tokens (e.g., she\nleaves there but comesback ...).6\nComparing to the unlikelihood loss, not only de-\ncreasing the likelihood of a negative example, this\nloss tries to guarantee a certain difference between\nthe two likelihoods. The learning signal of this loss\nseems stronger in this sense; however, the token-\nlevel supervision is missing, which may provide a\nmore direct signal to learn a clear contrast between\ncorrect and incorrect words. This is an empirical\nproblem we pursue in the experiments.\nToken-level margin loss Our ﬁnal loss is a com-\nbination of the previous two, by replacing g(xi) in\nthe unlikelihood loss by a margin loss:\ng(x∗\ni ) = max(0,δ−(log p(xi|x1:i−1)\n−log p(x∗\ni |x1:i−1)).\nWe will see that this loss is the most advantageous\nin the experiments (Section 4).\n3.2 Parameters\nEach method employs a few additional hyperparam-\neters (βfor the binary classiﬁcation loss, αfor the\nunlikelihood loss, and δfor the margin losses). We\npreliminary select βand αfrom {1,10,100,1000}\nthat achieve the best average syntactic performance\nand ﬁnd β = 1and α= 1000. For the two margin\nlosses, we ﬁx β = 1.0 and α = 1.0 and only see\nthe effects of margin value δ.\n6In principle, one can cumulate this loss within a single\nmini-batch for Llm as we do for the binary-classiﬁcation loss.\nHowever, obtaining Ladd needs to run an LM entirely on\nnegative sentences as well, which demands a lot of GPU\nmemories. We avoid this by separating mini-batches for Llm\nand Ladd. We precompute all possible pairs of ( x, x∗\nj ) and\ncreate a mini-batch by sampling from them. We make the\nbatch size for Ladd (the number of pairs) as the half of that\nfor Llm, to make the number of sentences contained in both\nkinds of batches equal. Finally, in each epoch, we only sample\nat most the half mini-batches of those for Llm to reduce the\ntotal amount of training time.\n3.3 Scope of Negative Examples\nSince our goal is to understand to what extent LMs\ncan be sensitive to the target syntactic constructions\nby giving explicit supervision via negative exam-\nples, we only prepare negative examples on the\nconstructions that are directly tested at evaluation.\nSpeciﬁcally, we mark the following words in the\ntraining data, and create negative examples:\nPresent verb To create negative examples on\nsubject-verb agreement, we mark all present\nverbs and change their numbers.7\nReﬂexive pronoun We also create negative exam-\nples on reﬂexive anaphora, by ﬂipping be-\ntween {themselves}↔{himself, herself}.\nThese two are both related to the syntactic number\nof a target word. For binary classiﬁcation we re-\ngard both as a target word, apart from the original\nwork that only deals with subject-verb agreement\n(Enguehard et al., 2017). We use a single common\nlinear layer for both constructions.\nIn this work, we do not create negative exam-\nples for NPIs. This is mainly for technical reasons.\nAmong four losses, only the sentence-level mar-\ngin loss can correctly handle negative examples for\nNPIs, essentially because other losses are token-\nlevel. For NPIs, left contexts do not have infor-\nmation to decide the grammaticality of the target\ntoken (a quantiﬁer; no, most, etc.) (Section 2.1).\nInstead, in this work, we use NPI test cases as a\nproxy to see possible negative (or positive) impacts\nas compensation for specially targeting some con-\nstructions. We will see that in particular for our\nmargin losses, such negative effects are very small.\n4 Experiments on Additional Losses\nWe ﬁrst see the overall performance of base-\nline LSTM-LMs as well as the effects of addi-\ntional losses. Throughout the experiments, for\neach setting, we train ﬁve models from differ-\nent random seeds and report the average score\nand standard deviation. The code is available at\nhttps://github.com/aistairc/lm syntax negative.\nNaive LSTM-LM performs well The main ac-\ncuracy comparison across target constructions for\ndifferent settings is presented in Table 1. We ﬁrst\n7We use Stanford tagger (Toutanova et al., 2003)\nto ﬁnd the present verbs. We change the number\nof verbs tagged by VBZ or VBP using inflect.py\n(https://pypi.org/project/inﬂect/).\n3380\nLSTM-LM Additional margin loss (δ= 10) Additional loss(α= 1000,β= 1) Distilled\nM&L18 Ours Sentence-level Token-level Binary-pred. Unlike. K19\nAGREEMENT:\nSimple 94.0 98.1 ( ±1.3) 100.0(±0.0) 100.0(±0.0) 99.1 (±1.2) 99.7 ( ±0.6) 100.0(±0.0)\nIn a sent. complement 99.0 96.1 (±2.0) 95.8 ( ±0.7) 99.3(±0.4) 96.9 (±2.4) 92.7 ( ±3.1) 98.0 (±2.0)\nShort VP coordination 90.0 93.6 (±3.0) 100.0(±0.0) 99.4 ( ±1.1) 93.8 (±3.3) 95.6 ( ±3.0) 99.0 (±2.0)\nLong VP coordination 61.0 82.2 (±3.4) 94.5 ( ±1.0) 99.0(±0.8) 83.9 (±3.2) 90.0 ( ±2.4) 80.0 (±2.0)\nAcross a PP 57.0 92.6 ( ±1.4) 98.8(±0.4) 98.6 ( ±0.3) 92.7 (±1.3) 95.2 ( ±1.2) 91.0 (±3.0)\nAcross a SRC 56.0 91.5 ( ±3.4) 99.6 ( ±0.4) 99.8(±0.2) 91.9 (±2.5) 97.1 ( ±0.7) 90.0 (±2.0)\nAcross an ORC 50.0 84.5 ( ±3.1) 93.5 ( ±4.0) 93.7(±2.0) 86.3 (±3.2) 88.7 ( ±4.1) 84.0 (±3.0)\nAcross an ORC (no that) 52.0 75.7 (±3.3) 86.7 ( ±4.2) 89.4(±2.7) 78.6 (±4.0) 86.4 ( ±3.5) 77.0 (±2.0)\nIn an ORC 84.0 84.3 ( ±5.5) 99.8 ( ±0.2) 99.9(±0.1) 89.3 (±6.2) 92.4 ( ±3.5) 92.0 (±4.0)\nIn an ORC (no that) 71.0 81.8 ( ±2.3) 97.0 ( ±1.0) 98.6(±0.9) 83.0 (±5.1) 88.9 ( ±2.4) 92.0 (±2.0)\nREFLEXIVE:\nSimple 83.0 94.1 ( ±1.9) 99.4 ( ±1.1) 99.9(±0.2) 91.8 (±2.9) 98.0 ( ±1.1) 91.0 (±4.0)\nIn a sent. complement 86.0 80.8 (±1.7) 99.2(±0.6) 97.9 ( ±0.8) 79.0 (±3.1) 92.6 ( ±2.9) 82.0 (±3.0)\nAcross an ORC 55.0 74.9 ( ±5.0) 72.8 ( ±2.4) 73.9 ( ±1.3) 72.3 (±3.0) 78.9(±8.6) 67.0 (±3.0)\nNPI:\nSimple 40.0 99.2(±0.7) 98.7 ( ±1.6) 97.7 ( ±2.0) 98.0 (±3.1) 98.2 ( ±1.2) 94.0 (±4.0)\nAcross an ORC 41.0 63.5 ( ±15.0) 56.8 ( ±6.0) 64.1 (±13.8) 64.5 (±14.0) 48.5 ( ±6.4) 91.0(±7.0)\nPerplexity 78.6 49.5(±0.2) 56.4 ( ±0.5) 50.4 ( ±0.6) 49.6 (±0.3) 50.3 ( ±0.2) 56.7 (±0.2)\nTable 1: Comparison of syntactic dependency evaluation accuracies across different types of dependencies and\nperplexities. Numbers in parentheses are standard deviations. M&L18 is the result of two-layer LSTM-LM in\nMarvin and Linzen (2018). K19 is the result of distilled two-layer LSTM-LM from RNNGs (Kuncoro et al., 2019).\nVP: verb phrase; PP: prepositional phrase; SRC: subject relative clause; and ORC: object-relative clause. Margin\nvalues are set to 10, which works better according to Figure 1. Perplexity values are calculated on the test set of\nthe Wikipedia dataset. The values of M&L18 and K19 are copied from Kuncoro et al. (2019).\n0 1 5 10 15\nmargin \n80\n85\n90\n95\n100Accuracy / Perplexity\nAgreement\nsentence-level\ntoken-level\n0 1 5 10 15\nmargin \n80\n85\n90\n95\n100\nReflexive\n0 1 5 10 15\nmargin \n60\n70\n80\n90\n100\nNPI\n0 1 5 10 15\nmargin \n50\n52\n54\n56\n58\nPerplexity\nFigure 1: Margin value vs. macro average accuracy over the same type of constructions, or perplexity, with standard\ndeviation for the sentence and token-level margin losses. δ= 0is the baseline LSTM-LM without additional loss.\nnotice that our baseline LSTM-LM (Section 2.2)\nperforms much better than Marvin and Linzen\n(2018)’s LM. A similar observation is recently\nmade by Kuncoro et al. (2019). 8 This suggests\nthat the original work underestimates the true syn-\ntactic ability induced by LSTM-LMs. The table\nalso shows the results by their distilled LSTM-LM\nfrom RNNGs (Section 1).\nHigher margin value is effective For the two\ntypes of margin loss, which margin value should\nwe use? Figure 1 reports average accuracies within\nthe same types of constructions. For both token\nand sentence-levels, the task performance increases\nalong δ, but a too large value (15) causes a nega-\n8We omit the comparison but the scores are overall similar.\ntive effect, in particular on reﬂexive anaphora. In-\ncreases (degradations) of perplexity are observed in\nboth methods but this effect is much smaller for the\ntoken-level loss. In the following experiments, we\nﬁx the margin value to 10 for both, which achieves\nthe best syntactic performance.\nWhich additional loss works better? We see\na clear tendency that our token-level margin loss\nachieves overall better performance. Unlikeli-\nhood loss does not work unless we choose a huge\nweight parameter ( α = 1000), but it does not\noutperform ours, with a similar value of perplex-\nity. The improvements by binary-classiﬁcation loss\nare smaller, indicating that the signals are weaker\nthan other methods with explicit negative exam-\n3381\n0.1M 0.37M 0.5M 0.8M\n# ORCs\n75\n80\n85\n90\n95\n100Accuracy on 'Across an ORC'\nwith that (all cases)\nLSTM-LM\nmargin (sent.)\nmargin (token)\n0.1M 0.37M 0.5M 0.8M\n# ORCs\nwith that (animate only)\n0.1M 0.37M 0.5M 0.8M\n# ORCs\nno that (all cases)\n0.1M 0.37M 0.5M 0.8M\n# ORCs\nno that (animate only)\nFigure 2: Accuracies on “Across an ORC” (with and without complementizer “that”) by models trained on aug-\nmented data with additional sentences containing an object RC. Margin is set to 10. X-axis denotes the total\nnumber of object RCs in the training data. 0.37M roughly equals the number of subject RCs in the original data.\n“animate only” is a subset of examples (see body). Error bars are standard deviations across 5 different runs.\nples. Sentence-level margin loss is conceptually\nadvantageous in that it can deal with any type of\nsentence-level grammaticality including NPIs. We\nsee that it is overall competitive with token-level\nmargin loss but suffers from a larger increase of\nperplexity (4.9 points), which is observed even with\nsmaller margin values (Figure 1). Understanding\nthe cause of this degradation as well as alleviating\nit is an important future direction.\n5 Limitations of LSTM-LMs\nIn Table 1, the accuracies on dependencies across\nan object RC are relatively low. The central ques-\ntion in this experiment is whether this low perfor-\nmance is due to the limitation of current architec-\ntures, or other factors such as frequency. We base\nour discussion on the contrast between object (7)\nand subject (8) RCs:\n(7) The authors (that) the chef likes laugh .\n(8) The authors that like the chef laugh .\nImportantly, the accuracies for a subject RC are\nmore stable, reaching 99.8% with the token-level\nmargin loss, although the content words used in the\nexamples are common.9\nIt is known that object RCs are less frequent\nthan subject RCs (Hale, 2001; Levy, 2008), and it\ncould be the case that the use of negative examples\nstill does not fully alleviate this factor. Here, to\nunderstand the true limitation of the current LSTM\narchitecture, we try to eliminate such other factors\nas much as possible under a controlled experiment.\n9 Precisely, they are not the same. Examples of object\nRCs are divided into two categories by the animacy of the\nmain subject (animate or not), while subject RCs only contain\nanimate cases. If we select only animate examples from object\nRCs the vocabularies for both RCs are the same, remaining\nonly differences in word order and inﬂection, as in (7, 8).\nSetup We ﬁrst inspect the frequencies of ob-\nject and subject RCs in the training data, by pars-\ning them with the state-of-the-art Berkeley neural\nparser (Kitaev and Klein, 2018). In total, while\nsubject RCs occur 373,186 times, object RCs only\noccur 106,558 times. We create three additional\ntraining datasets by adding sentences involving ob-\nject RCs to the original Wikipedia corpus (Sec-\ntion 2.2). To this end, we randomly pick up 30\nmillion sentences from Wikipedia (not overlapped\nto any sentences in the original corpus), parse by\nthe same parser, and ﬁlter sentences containing an\nobject RC, amounting to 680,000 sentences. We\ncreate augmented training sets by adding a subset,\nor all of these sentences to the original training\nsentences. Among the test cases about object RCs\nwe only report accuracies on subject-verb agree-\nment, on which the portion for subject RCs also\nexists. This allows us to compare the difﬁculties\nof two types of RCs for the present models. We\nalso evaluate on “animate only” subset, which has\na correspondence to the test cases for subject RCs\nwith only differences in word order and inﬂection\n(like (7) and (8); see footnote 9). Of particular in-\nterest to us is the accuracy on these animate cases.\nWe expect that the main reason for lower perfor-\nmance for object RCs is due to frequency, and with\nour augmentation the accuracy will reach the same\nlevel as that for subject RCs.\nResults However, for both all and animate cases,\naccuracies are below those for subject RCs (Fig-\nure 2). Although we see improvements from the\noriginal score (93.7), the highest average accuracy\nby the token-level margin loss on the “animate”\nsubset is 97.1 (“with that”), not beyond 99%. This\nresult indicates some architectural limitations of\nLSTM-LMs in handling object RCs robustly at a\nnear perfect level. Answering why the accuracy\n3382\nAcross a PP\n80\n85\n90\n95\n100Accuracy\nAcross a SRC Across an ORC\nLong VP coord.\n80\n85\n90\n95\n100Accuracy\nLSTM-LM\nmargin (token)\nmargin (token) w/o negative\nexamples on target verbs\nmargin (token) w/o negative\nexamples on a construction\nFigure 3: An ablation study to see the performance of\nmodels trained with reduced explicit negative examples\n(token-level and construction-level). One color repre-\nsents the same models across plots, except the last bar\n(construction-level), which is different for each plot.\ndoes not reach (almost) 100%, perhaps with other\nempirical properties or inductive biases (Khandel-\nwal et al., 2018; Ravfogel et al., 2019) is future\nwork.\n6 Do models generalize explicit\nsupervision, or just memorize it?\nOne distinguishing property of our margin loss,\nin particular token-level loss, is that it is highly\nlexical, making a contrast explicitly between cor-\nrect and incorrect words. This direct signal may\nmake models acquire very specialized knowledge\nabout each target word, not very generalizable one\nacross similar words and occurring contexts. In this\nsection, to get insights into the transferability of\nsyntactic knowledge induced by our margin losses,\nwe provide an ablation study by removing certain\nnegative examples during training.\nSetup We perform two kinds of ablation. For\ntoken-level ablation (-TOKEN ), we avoid creating\nnegative examples for all verbs that appear as a tar-\nget verb10 in the test set. Another is construction-\nlevel (-PATTERN ), by removing all negative ex-\namples occurring in a particular syntactic pattern.\nWe ablate a single construction at a time for -\nPATTERN , from four non-local subject-verb depen-\ndencies (across a prepositional phrase (PP), sub-\n10swim, smile, laugh, enjoy, hate, bring, interest, like, write,\nadmire, love, know, and is.\nSecond verb (V1 andV2)\nModels All verbs like other verbs\nLSTM-LM 82.2 ( ±3.4) 13.0 (±12.2) 89.9 (±3.6)\nMargin (token) 99.0 (±0.8) 94.0 (±6.5) 99.6 (±0.5)\n-TOKEN 90.8 (±3.3) 51.0 (±29.9) 95.2 (±2.6)\n-PATTERN 90.1 (±4.6) 50.0 (±30.6) 94.6 (±2.2)\nTable 2: Accuracies on long VP coordinations by the\nmodels with/without ablations. “All verbs” scores are\noverall accuracies. “like” scores are accuracies on ex-\namples on which the second verb (target verb) is like.\nFirst verb (V1and V2)\nModels likes other verbs\nLSTM-LM 61.5 ( ±20.0) 93.5 (±3.4)\nMargin (token) 97.0 (±4.5) 99.9 (±0.1)\n-TOKEN 63.5 (±18.5) 99.2 (±1.1)\n-PATTERN 67.0 (±21.2) 98.0 (±1.4)\nTable 3: Further analysis of accuracies on the “other\nverbs” cases of Table 2. Among these cases, the second\ncolumn (“likes”) shows accuracies on examples where\nthe ﬁrst verb (not target) is likes.\nject RC, object RC, and long verb phrase (VP)).11\nWe hypothesize that models are less affected by\ntoken-level ablation, as knowledge transfer across\nwords appearing in similar contexts is promoted\nby language modeling objective. We expect that\nconstruction-level supervision would be necessary\nto induce robust syntactic knowledge, as perhaps\ndifferent phrases, e.g., a PP and a VP, are processed\ndifferently.\nResults Figure 3 is the main results. Across\nmodels, we restrict the evaluation on four non-\nlocal dependency constructions, which we select\nas ablation candidates as well. For a model with\n-PATTERN , we evaluate only on examples of con-\nstruction ablated in training (see caption). To our\nsurprise, both -TOKEN and -PATTERN have sim-\nilar effects, except “Across an ORC”, on which\nthe degradation by -PATTERN is larger. This may\nbe related to the inherent difﬁculty of object RCs\nfor LSTM-LMs that we veriﬁed in Section 5. For\nsuch particularly challenging constructions, models\nmay need explicit supervision signals. We observe\nlesser score degradation by ablating prepositional\nphrases and subject RCs. This suggests that, for\nexample, the syntactic knowledge strengthened for\nprepositional phrases with negative examples could\nbe exploited to learn the syntactic patterns about\n11We identify all these cases from the parsed training data,\nwhich we prepared for the analysis in Section 5.\n3383\nsubject RCs, even when direct learning signals on\nsubject RCs are missing.\nWe see approximately 10.0 points score degra-\ndation on long VP coordination by both ablations.\nDoes this mean that long VPs are particularly hard\nin terms of transferability? We ﬁnd that the main\nreasons for this drop, relative to other cases, are\nrather technical, essentially due to the target verbs\nused in the test cases. See Table 2, 3, which show\nthat failed cases for the ablated models are often\ncharacterized by the existence of either like or likes.\nExcluding these cases (“other verbs” in Table 3),\nthe accuracies reach 99.2 and 98.0 by -TOKEN and\n-PATTERN , respectively. These verbs do not appear\nas a target verb in the test cases of other tested\nconstructions. This result suggests that the transfer-\nability of syntactic knowledge to a particular word\nmay depend on some characteristics of that word.\nWe conjecture that the reason for weak transferabil-\nity to likes and like is that they are polysemous;\ne.g., in the corpus, like is much more often used\nas a preposition and being used as a present tense\nverb is rare. This type of issue due to frequency\nmay be one reason for lessening the transferability.\nIn other words, like can be seen as a challenging\nverb to learn its usage only from the corpus, and\nour margin loss helps for such cases.\n7 Discussion and Conclusion\nOur results with explicit negative examples are\noverall positive. We have demonstrated that mod-\nels exposed to these examples at training time in\nan appropriate way will be capable of handling the\ntargeted constructions at near perfect level except\na few cases. We found that our new token-level\nmargin loss is superior to the other approaches and\nthe remaining challenging cases are dependencies\nacross an object relative clause.\nObject relative clauses are known to be harder\nfor a human as well, and our results may indicate\nsome similarities in the sentence processing be-\nhaviors by a human and RNN, though other stud-\nies also ﬁnd some dissimilarities between them\n(Linzen and Leonard, 2018; Wilcox et al., 2019a).\nThe difﬁculty of object relative clauses for RNN-\nLMs has also been observed in the prior work\n(Marvin and Linzen, 2018; van Schijndel et al.,\n2019). A new insight provided by our study is\nthat this difﬁculty holds even after alleviating the\nfrequency effects by augmenting the target struc-\ntures along with direct supervision signals. This\nindicates that RNNs might inherently suffer from\nsome memory limitation like a human subject, for\nwhich the difﬁculty of particular constructions, in-\ncluding center-embedded object relative clauses,\nare known to be incurred due to memory limitation\n(Gibson, 1998; Demberg and Keller, 2008) rather\nthan purely frequencies of the phenomena. In terms\nof language acquisition, the supervision provided\nin our approach can be seen as direct negative ev-\nidence (Marcus, 1993). Since human learners are\nknown to acquire syntax without such direct feed-\nback we do not claim that our proposed learning\nmethod itself is cognitively plausible.\nOne limitation of our approach is that the scope\nof negative examples has to be predetermined and\nﬁxed. Alleviating this restriction is an important fu-\nture direction. Though it is challenging, we believe\nthat our ﬁnal analysis for transferability, which in-\ndicates that the negative examples do not have to be\ncomplete and can be noisy, suggests a possibility\nof a mechanism to induce negative examples them-\nselves during training, perhaps relying on other\nlinguistic cues or external knowledge.\nAcknowledgements\nWe would like to thank Naho Orita and the mem-\nbers of Computational Psycholinguistics Tokyo for\ntheir valuable suggestions and comments. This\npaper is based on results obtained from projects\ncommissioned by the New Energy and Industrial\nTechnology Development Organization (NEDO).\nReferences\nVera Demberg and Frank Keller. 2008. Data from eye-\ntracking corpora as evidence for theories of syntactic\nprocessing complexity. Cognition, 109:193–210.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A. Smith. 2016. Recurrent neural network\ngrammars. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 199–209, San Diego, California.\nAssociation for Computational Linguistics.\n´Emile Enguehard, Yoav Goldberg, and Tal Linzen.\n2017. Exploring the syntactic abilities of RNNs\nwith multi-task learning. In Proceedings of the 21st\nConference on Computational Natural Language\nLearning (CoNLL 2017), pages 3–14, Vancouver,\nCanada. Association for Computational Linguistics.\nEdward Gibson. 1998. Linguistic complexity: Locality\nof syntactic dependencies. Cognition, 68(1):1–76.\n3384\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 1195–1205. Associ-\nation for Computational Linguistics.\nJohn Hale. 2001. A probabilistic earley parser as a psy-\ncholinguistic model. In Second Meeting of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics.\nAri Holtzman, Jan Buys, Maxwell Forbes, and Yejin\nChoi. 2019. The curious case of neural text degener-\nation.\nJiaji Huang, Yi Li, Wei Ping, and Liang Huang. 2018.\nLarge margin neural language model. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1183–1191,\nBrussels, Belgium. Association for Computational\nLinguistics.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2016. Tying word vectors and word classiﬁers: A\nloss framework for language modeling. In Interna-\ntional Conference on Learning Representations.\nUrvashi Khandelwal, He He, Peng Qi, and Dan Juraf-\nsky. 2018. Sharp nearby, fuzzy far away: How neu-\nral language models use context. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 284–294, Melbourne, Australia. Association\nfor Computational Linguistics.\nNikita Kitaev and Dan Klein. 2018. Constituency pars-\ning with a self-attentive encoder. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 2676–2686, Melbourne, Australia. Associa-\ntion for Computational Linguistics.\nAdhiguna Kuncoro, Chris Dyer, John Hale, Dani Yo-\ngatama, Stephen Clark, and Phil Blunsom. 2018.\nLstms can learn syntax-sensitive dependencies well,\nbut modeling structure makes them better. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 1426–1436, Melbourne, Australia.\nAssociation for Computational Linguistics.\nAdhiguna Kuncoro, Chris Dyer, Laura Rimell, Stephen\nClark, and Phil Blunsom. 2019. Scalable syntax-\naware language models using knowledge distillation.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n3472–3484, Florence, Italy. Association for Compu-\ntational Linguistics.\nRoger Levy. 2008. Expectation-based syntactic com-\nprehension. Cognition, 106(3):1126–1177.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of lstms to learn syntax-\nsensitive dependencies. Transactions of the Associa-\ntion for Computational Linguistics, 4:521–535.\nTal Linzen and Brian Leonard. 2018. Distinct patterns\nof syntactic agreement errors in recurrent networks\nand humans. In Proceedings of the 40th Annual Con-\nference of the Cognitive Science Society, pages 692–\n697, Austin, TX. Cognitive Science Society.\nGary F. Marcus. 1993. Negative evidence in language\nacquisition. Cognition, 46(1):53 – 85.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202,\nBrussels, Belgium. Association for Computational\nLinguistics.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. Regularizing and optimizing LSTM\nlanguage models. In International Conference on\nLearning Representations.\nTomas Mikolov, Stefan Kombrink, Luk ´as Burget, Jan\nCernock´y, and Sanjeev Khudanpur. 2011. Exten-\nsions of recurrent neural network language model.\nIn ICASSP, pages 5528–5531. IEEE.\nShauli Ravfogel, Yoav Goldberg, and Tal Linzen. 2019.\nStudying the inductive biases of RNNs with syn-\nthetic variations of natural languages. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 3532–3542, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nMarten van Schijndel, Aaron Mueller, and Tal Linzen.\n2019. Quantity doesn’t buy quality syntax with neu-\nral language models. In Proceedings of the 2010\nConference on Empirical Methods in Natural Lan-\nguage Processing, Hong Kong, China. Association\nfor Computational Linguistics.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in neural information processing sys-\ntems, pages 3104–3112.\nKristina Toutanova, Dan Klein, Christopher D Man-\nning, and Yoram Singer. 2003. Feature-rich part-of-\nspeech tagging with a cyclic dependency network.\nIn Proceedings of the 2003 conference of the North\nAmerican chapter of the association for computa-\ntional linguistics on human language technology-\nvolume 1, pages 173–180. Association for Compu-\ntational Linguistics.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-\nnan, Kyunghyun Cho, and Jason Weston. 2020. Neu-\nral text generation with unlikelihood training. In\nInternational Conference on Learning Representa-\ntions.\n3385\nEthan Wilcox, Roger P. Levy, and Richard Futrell.\n2019a. What syntactic structures block dependen-\ncies in rnn language models? In Proceedings of the\n41st Annual Meeting of the Cognitive Science Soci-\nety. Cognitive Science Society.\nEthan Wilcox, Peng Qian, Richard Futrell, Miguel\nBallesteros, and Roger Levy. 2019b. Structural su-\npervision improves learning of non-local grammati-\ncal dependencies. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3302–3312, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7692234516143799
    },
    {
      "name": "Perplexity",
      "score": 0.6910345554351807
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.619958758354187
    },
    {
      "name": "Sentence",
      "score": 0.6034435629844666
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6014723777770996
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.5590373277664185
    },
    {
      "name": "Natural language processing",
      "score": 0.5484344959259033
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5097987055778503
    },
    {
      "name": "Language model",
      "score": 0.4922734797000885
    },
    {
      "name": "Object (grammar)",
      "score": 0.48195740580558777
    },
    {
      "name": "Verb",
      "score": 0.4161294102668762
    },
    {
      "name": "Speech recognition",
      "score": 0.3465622663497925
    },
    {
      "name": "Machine learning",
      "score": 0.23459497094154358
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": []
}