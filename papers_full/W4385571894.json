{
    "title": "ThinkSum: Probabilistic reasoning over sets using large language models",
    "url": "https://openalex.org/W4385571894",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3111652561",
            "name": "Batu Ozturkler",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3012422146",
            "name": "Nikolay Malkin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2096525380",
            "name": "Zhen Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2092023269",
            "name": "Nebojsa Jojic",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4287585714",
        "https://openalex.org/W4385572965",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4285233740",
        "https://openalex.org/W3173343821",
        "https://openalex.org/W4285239949",
        "https://openalex.org/W4310625358",
        "https://openalex.org/W4205857304",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4229445617",
        "https://openalex.org/W4296300649",
        "https://openalex.org/W4286769130",
        "https://openalex.org/W4303441863",
        "https://openalex.org/W3156470785",
        "https://openalex.org/W4320858367",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W3172943453",
        "https://openalex.org/W3101204082",
        "https://openalex.org/W4308244910",
        "https://openalex.org/W3153046263",
        "https://openalex.org/W4281250694",
        "https://openalex.org/W4226369848",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3207553988",
        "https://openalex.org/W4394656990",
        "https://openalex.org/W4281483047",
        "https://openalex.org/W4287084089",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W3192405822"
    ],
    "abstract": "Large language models (LLMs) have a substantial capacity for high-level analogical reasoning: reproducing patterns in linear text that occur in their training data (zero-shot evaluation) or in the provided context (few-shot in-context learning). However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions. We propose a two-stage probabilistic inference paradigm, ThinkSum, which reasons over sets of objects or facts in a structured manner. In the first stage (Think ‚Äì retrieval of associations), a LLM is queried in parallel over a set of phrases extracted from the prompt or an auxiliary model call. In the second stage (Sum ‚Äì probabilistic inference or reasoning), the results of these queries are aggregated to make the final prediction. We demonstrate the possibilities and advantages of ThinkSum on the BIG-bench suite of LLM evaluation tasks, achieving improvements over the state of the art using GPT-family models on thirteen difficult tasks, often with far smaller model variants. We also compare and contrast ThinkSum with other proposed modifications to direct prompting of LLMs, such as variants of chain-of-thought prompting. Our results suggest that because the probabilistic inference in ThinkSum is performed outside of calls to the LLM, ThinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs. Overall, our proposed paradigm represents a promising approach for enhancing the reasoning capabilities of LLMs.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 1216‚Äì1239\nJuly 9-14, 2023 ¬©2023 Association for Computational Linguistics\nThinkSum: Probabilistic reasoning over sets using large language models\nBatu Ozturkler\nStanford University\nStanford, California, USA\nozt@stanford.edu\nNikolay Malkin\nMila, Universit√© de Montr√©al\nMontr√©al, Qu√©bec, Canada\nnikolay.malkin@mila.quebec\nZhen Wang\nOhio State University\nColumbus, Ohio, USA\nwang.9215@osu.edu\nNebojsa Jojic\nMicrosoft Research\nRedmond, Washington, USA\njojic@microsoft.com\nAbstract\nLarge language models (LLMs) have a substan-\ntial capacity for high-level analogical reason-\ning: reproducing patterns in linear text that oc-\ncur in their training data (zero-shot evaluation)\nor in the provided context (few-shot in-context\nlearning). However, recent studies show that\neven the more advanced LLMs fail in scenar-\nios that require reasoning over multiple objects\nor facts and making sequences of logical de-\nductions. We propose a two-stage probabilis-\ntic inference paradigm, ThinkSum, which rea-\nsons over sets of objects or facts in a struc-\ntured manner. In the first stage ( Think ‚Äì re-\ntrieval of associations), a LLM is queried in\nparallel over a set of phrases extracted from\nthe prompt or an auxiliary model call. In the\nsecond stage ( Sum ‚Äì probabilistic inference\nor reasoning), the results of these queries are\naggregated to make the final prediction. We\ndemonstrate the possibilities and advantages\nof ThinkSum on the BIG-bench suite of LLM\nevaluation tasks, achieving improvements over\nthe state of the art using GPT-family models on\nthirteen difficult tasks, often with far smaller\nmodel variants. We also compare and contrast\nThinkSum with other proposed modifications\nto direct prompting of LLMs, such as variants\nof chain-of-thought prompting. Our results sug-\ngest that because the probabilistic inference in\nThinkSum is performed outside of calls to the\nLLM, ThinkSum is less sensitive to prompt\ndesign, yields more interpretable predictions,\nand can be flexibly combined with latent vari-\nable models to extract structured knowledge\nfrom LLMs. Overall, our proposed paradigm\nrepresents a promising approach for enhancing\nthe reasoning capabilities of LLMs.\n1 Introduction\nLarge language models (LLMs; Brown et al., 2020;\nRae et al., 2021; Chowdhery et al., 2022) can recall\na broad range of basic facts, recognize and mimic\nvarious forms in language, and efficiently extrapo-\nlate analogies in structure and meaning. These abil-\nities allow LLMs to excel in zero-shot and few-shot\ntasks formulated as the generation or selection of a\nlikely completion to a prompt. This formulation re-\nquires LLMs to perform fast associative thinking,\nin which each token of text in the sequence making\nup the answer is generated or scored in one pass\nthrough the model and, other than that, no interme-\ndiate information is created or retained. This fast\nthinking is made possible by the compression of\ninformation that is repeated in a variety of ways in\nlarge training datasets, within the LLM‚Äôs weights.\nHowever, it is increasingly evident that when\nreasoning, or slow thinking, is required, fail-\nure modes of LLMs are revealed. In our usage,\nreasoning refers to the sequential manipulation\nof concepts that can be expressed in language.\nTasks that require iterative retrieval of rarely stated\nknowledge, uncertainties over multiple objects or\nfacts, or multiple steps of deduction are difficult\neven for the most advanced LLMs (Suzgun et al.,\n2022). In a recently designed suite of evalua-\ntions, BIG-bench (Srivastava et al., 2022), some\nof the tasks where the gap between machine and\nhuman performance is large involve inference se-\nquences with nested counterfactuals ( LOGICAL\nDEDUCTION ), concepts introduced through defi-\nnitions (CONCEPTUAL COMBINATIONS ), etc. (see\nFig. B.1). These are tasks where a human solver‚Äôs\nintuitive feeling of ‚Äò(in)coherence‚Äô is insufficient\nto produce the right answer, and a sequence of\nthoughts, along with the use of intermediate re-\nsults, may be necessary to arrive at the solution,\nparticularly when working memory is insufficient.\nWe show several tasks in BIG-bench that can be\naddressed by a two-component mechanism, which\nwe name ThinkSum1:\n1ThinkSum is named by analogy with other algorithms\n1216\nA binne is any furry four-legged creature, and a bam is a simpl e dwelling.\nDIRECT PROMPTING\nCHAIN OF THOUGHT / AUXILIARY KNOWLEDGE\nTHINK SUM\nA binne bam is a place for people (55%) animals (44%) birds (0.87%) researchers (0.022%)\nA binne is any furry four-legged creature, and a bam is a simpl e dwelling.\nExamples of binnes: cat, mink, ferret, guinea pig , rabbit.\nExamples of bams: hut, cabin, cottage, shelter, shack.\nA binne bam is a place for people (51%) animals (48%) birds (0.76%) researchers (0.011%)\nA binne is any furry four-legged creature, and a bam is a simpl e dwelling.\nbinne = {cat, mink, ferret, guinea pig , rabbit}\nbam = {hut, cabin, cottage, shelter, shack}\nA binne bam is a place for animals (65%) people (34%) birds (1.5%) researchers (0.056%)\n‚åâ ‚åãTHINK (auxiliary LM calls to deÔ¨Åne sets)\nA cat cottage is a place for\nA rabbit cabin is a place for\nA mink shelter is a place for\n¬∑ ¬∑ ¬∑\n‚àë ‚åâ\n‚åã\nSUM (aggregate LM likelihoods)\nFigure 1: An example adapted from the CONCEPTUAL COMBINATIONS (INVENTED WORDS ) task, in which models\nmust select the most likely completion of a phrase that includes nonce words whose definitions are given. Top:\nDirect prompting evaluates completion likelihoods normalized over the four answer choices (‚Äòpeople‚Äô, ‚Äòanimals‚Äô,\n‚Äòbirds‚Äô, ‚Äòresearchers‚Äô). Middle: Chain-of-thought-like or auxiliary knowledge approaches would query a LLM or\nknowledge base for additional context. This example shows the brittleness entrusting all ‚Äòreasoning‚Äô to self-attention\nin linear text, especially in smaller models, which have stronger recency bias (Malkin et al., 2022): if we simply list\ngenerated examples as the additional context in the prompt, the recency bias causes the LLM to still give a higher\nprobability to ‚Äòpeople‚Äô than to ‚Äòanimals‚Äô, simply because ‚Äòbam‚Äô (simple dwelling) examples are given after the\n‚Äòbinne‚Äô examples. Bottom: Our ThinkSum approach to this task queries a LLM (GPT-2 XL) to produce sets of\nexamples defining the nonce words, then marginalizes over substitutions of these examples into the target phrase.\n‚Ä¢Think (fast thinking / association / knowledge re-\ntrieval step): creating an association of text spans\nwith sets of strings. This process may involve\ngeneration from a language model, as is the case\nin Fig. 1, where the novel word ‚Äòbinne‚Äô is asso-\nciated with the set of strings {‚Äòcat‚Äô,‚Äòmink‚Äô,... }\nby prompting GPT-3 with the definition and ask-\ning for examples. Alternatively, it may consist\nsolely of a scoring mechanism, resulting in the\nformation of a matrix of probabilities on which\nprobabilistic inference is performed.\n‚Ä¢Sum (slow thinking / Summarization / reasoning\nstep): probabilistic inference that aggregates gen-\nerated strings or probabilities to produce the final\nanswer. Summarization typically involves, and\noften entirely consists of, summing of probabili-\nties of strings (computed in the Think step), as\nin Fig. 1, where the final word is assumed to be\nsampled from a mixture of possible substitutions\nof ‚Äòbinne‚Äô and ‚Äòbam‚Äô words into the input.\nWe discuss different ways to Think and to Sum\nin section ¬ß2, but we start with one example, illus-\nwith ‚Äòexpand‚Äô and ‚Äòaggregate‚Äô steps, such as MapReduce in\ndistributed computing and sum-product in graphical models.\ntrated in Fig. 1 (bottom), motivated by the CON-\nCEPTUAL COMBINATIONS (INVENTED WORDS )\ntask in BIG-bench. In this task, the LLM is pro-\nvided with the definitions of two invented words\nand asked to infer the most plausible sentence that\nuses a combination of the invented words. As the\nwords are not common or consistently used in the\ntraining set, the LLM needs to understand and com-\nbine the definitions of the invented words to reason\nabout the meaning of the combination. The LLM\nis queried to produce example instances of the in-\nvented words with the help of the definitions. These\nexample instances can be substituted into the query\nin place of the invented words. By mapping indi-\nvidual spans of the text of interest to sets, we arrive\nat a mixture model (in this example, a mixture with\n25 components for 5 possible replacements of each\nword), which can be used in the same manner the\noriginal LLM is used, either to score text or to\ngenerate it token by token. When we score all can-\ndidate completions using this mixture model and\nnormalize over the four choices, the correct answer\n‚Äì that ‚Äòbinne bams‚Äô are for animals and not people ‚Äì\nbecomes the most likely.\n1217\nAn important difference between ourThinkSum\nand existing chain-of-thought-like prompt engineer-\ning methods (Wei et al., 2022; Kojima et al., 2022),\nis that our reasoning step is not reduced to a gener-\nation problem for the LLM, but is performed as a\nprobabilistic inference external to the LLM. This re-\nduces vulnerability to features of the prompt, such\nas accidental distraction of the LLM by spurious\npatterns (see Fig. 1, middle). Instead, we engineer\nthe slow thinking process to make parallel calls\nto the LLM to query for intermediate information,\nthen possibly perform programmatic recombina-\ntion of strings ( Think). The final reasoning step\n‚Äì in which likelihoods obtained from the LLM for\nthe recombinations derived from earlier steps of\nthe reasoning process are combined to make the\nfinal prediction ‚Äì is left to classical probabilistic\nreasoning (Sum). In a sense, Sum replaces the\nself-attention mechanism over linear text, which is\nused as the sole ‚Äòreasoning‚Äô mechanism in chain-of-\nthought-like approaches that expect the intermedi-\nate ‚Äòthoughts‚Äô to take the form of generated tokens\nintervening between the input and output.\nImposing an alternative reasoning system over\nan associative ‚Äúknee-jerk reaction\" system has an\nanalogy with models of human cognitive processes\n(Tversky and Kahneman, 1974; Kahneman, 2011)\nthat separate System 1 (fast thinking) and System\n2 (slow thinking). System 2 acts as a ‚Äòcontroller‚Äô\nthat can prime System 1 to appropriately bias its\nfast thinking. In the context of reasoning with deep\nlearning models, System 2 has been interpreted\nas operating with sparse concepts that can be de-\nscribed in language (Bengio, 2017; Goyal and Ben-\ngio, 2020). Through repeated usage, the functions\nof System 2 become compressed into System 1\nintuitions, in the same manner that iterative ‚Äòrea-\nsoning‚Äô functions of which smaller LLMs are not\ncapable become zero-shot generation capacities for\nlarge LLMs. As is the case with humans, there\nis always the next frontier of problems where a\ntrained model with remarkable ‚Äòintuition‚Äô needs to\nbe slowed down. The main claim of this paper is\nthat more is possible with LLMs of existing scale\nwhen they are used in concert with a wise controller\nthat allows for probabilistic inference.\n2 ThinkSum\n2.1 How to Think\nHere we list examples of the ‚Äúfast thinking\" that\nprecedes the summarization stage.\nElementary string manipulations. Standard\nways to turn a question into a prompt that can be\ngiven to a LLM for generation or scoring involve\nchoices (e.g., of the prompt format) that can be\nseen as being made by a controlling agent. The\ndefault approach to multiple-choice questions is\nto write them as Cloze tasks. However, there are\nnontrivial operations used in inference procedures\nthat sometimes work better, such as:\n‚Ä¢Order inversion: Exchanging the order of the\nquestion and answers, as in Min et al. (2022).\n‚Ä¢Premise erasure: Deleting a part of the question.\nRemoving a premise with which the answer is\nexpected to have high mutual information is a\nstep in inference procedures that aim to correct\nfor bias towards answers with high unconditional\nlikelihood (Zhao et al., 2021; Holtzman et al.,\n2021; Malkin et al., 2022).\nSubstitution and normalization. An example\nis shown in Fig. 1. Elements from a set may be\nsubstituted in place of ‚Äòslot‚Äô words in a prompt,\nsuch as ‚Äòcat‚Äô substituted for ‚Äòbinne‚Äô in the prompt\n‚ÄúA binne bam is a place for‚Äù. This operation\ncan be combined with syntax-normalization steps\nthat are reliably achieved by standard NLP tools,\nsuch as ensuring subject-verb agreement.\nExample and list generation. A LLM can be\nprompted to generate or score lists of words or\nphrases. We suggest and experiment with three\ninstances of this:\n‚Ä¢Example generation : In Fig. 1, the LLM is\nprompted to turn a definition or characterizing\nproperty, such as ‚Äòsimple dwelling‚Äô, into a list of\nexamples. This can be achieved with a prompt\nsuch as ‚Äú A bam is a simple dwelling.\nExamples: 1.‚Äù. The generated completion can\nbe parsed into a set to be used later in the infer-\nence procedure.\n‚Ä¢List extension: A similar approach can also be\nused to hallucinate additional possible answers\nto questions, as we will show in some of the\nexperiments.\n‚Ä¢List of words: Similar prompts provide an even\nsimpler Think method that we use for scoring ‚Äì\nbut not generation ‚Äì in several tasks. Just prompt-\ning a LLM with ‚Äú List of words: ùê¥, ùêµ‚Äù,\nwhere ùê¥and ùêµare words or phrases, and com-\nputing the likelihood of ùêµconditioned on ‚ÄúList\nof words: ùê¥,‚Äù is a good measure of semantic\nrelatedness of ùê¥and ùêµ.\n1218\nFact generation. This way of Thinking asso-\nciates an input word with a set of phrases in a\nsimilar manner to generating examples from a def-\ninition. It can be achieved with prompts such as\n‚ÄúList facts about cats. 1. ‚Äù The generated\nfacts are good targets for substitutions of other con-\ncepts (‚Äòdogs‚Äô, ‚Äògalaxies‚Äô) in place of the concept\n(‚Äòcats‚Äô) about which facts are generated. A varia-\ntion on this asks the LLM to generate differences\nbetween two concepts, as shown in Fig. 2 (right).\nTranslation. The LLM can be prompted to con-\nvert between different forms of representing the\nsame concept as a sequence of tokens. We use two\nbasic examples of this in experiments:\n‚Ä¢Translation between languages by prompting the\nLLM in formats such as ‚Äú French: J‚Äôadore\nles chats noirs. English:‚Äù. A very similar\napproach can be used to convert non-alphabetic\nsymbols, such as emoji, into words with similar\nmeanings.\n‚Ä¢Converting text to formal (symbolic) structures,\nlike turning a word problem into a collection of\nmathematical equations.\n2.2 How to Sum\nElementary inference. As above, we begin by\nlisting existing standard ways of turning LLM out-\nputs into answers, which we see as trivial cases of\naggregation (Sum).\n‚Ä¢Majority/minority vote (argmin/argmax): a\ncomponent of most answer selection procedures.\n‚Ä¢Ratio of likelihoods: Likelihoods from different\nvariants of the same prompt can be combined\nby considering their ratio or more general log-\nlinear or other mixture. For example, this can\nbe done to correct the likelihood of an answer\nconditioned on a question by its unconditional\nlikelihood, in combination with the Premise era-\nsure operation described above.\nMixture (average) aggregation. A collection of\nprompts can be treated as the components of a\nmixture model over completions. An example is\nshown in Fig. 1, where substitutions of a set of\nwords yield 25 different prompts. Likelihoods of\nthe completion over these 25 prompts are averaged.\nProduct aggregation. We use products of likeli-\nhoods in two different ways:\n‚Ä¢In a similar way as mixtures, but when the more\nnatural probabilistic model has all elements of a\nset (of prompts) generating the answer, such as\nwhen a description or definition must be satisfied\nby all concepts in a set.\n‚Ä¢In a task where we are to determine whether a\nstatement ùëÜ or its negation ùëÜ‚Ä≤ is true, we can\ncompute the likelihood of both ùëÜand ùëÜ‚Ä≤being\ntrue (as posterior over the tokens ‚ÄòTrue‚Äô and\n‚ÄòFalse‚Äô in an appropriate prompt), then compare\nùëù(True|ùëÜ)ùëù(False|ùëÜ‚Ä≤)(ùëÜis true and ùëÜ‚Ä≤is false)\nwith ùëù(False|ùëÜ)ùëù(True|ùëÜ‚Ä≤)(ùëÜis false and ùëÜ‚Ä≤is\ntrue).\n3 Experiments\nIn this section, we perform case studies on three\ntasks from the BIG-bench suite to demonstrate the\npossibilities of the inference approaches discussed\nin ¬ß2. We also experiment with ten other tasks\nfrom BIG-bench; the best results are summarized\nin Table 1 and the methods, grouped by the style\nof Thinking and Summing, are described in Ap-\npendix (¬ßA).\nAll details of the tasks can be found in the Ap-\npendix (¬ßC). Comparisons to direct prompting and\nalgorithms that append retrieved or generated to-\nkens to the prompt are given in ¬ß3.4.\n3.1 Conceptual combinations: Invented words\nIn INVENTED WORDS , two nonce words ùë•1,ùë•2 are\ndefined and the correct statement must be chosen\nout of a set of statements ùëÜ= {ùë†ùëó }that begin with\n(possibly inflected forms of) ‚Äúùë•1 ùë•2‚Äù (Fig. 1).\nWe use an Example generation prompt to ob-\ntain a set of example words fitting the definitions of\nùë•1 and ùë•2. We thus obtain sets ùëÜ1 and ùëÜ2 of words\nthat can be substituted for ùë•1 and ùë•2, respectively.\nWe treat each statement ùë†ùëó as a template into\nwhich words ùë§1 ‚ààùëÜ1 and ùë§2 ‚ààùëÜ2 can be substi-\ntuted by replacing ùë•ùëñ with ùë§ùëñ and normalizing the\nsyntax to ensure subject-verb agreement. Denoting\nby ùë†ùëó ‚ü®ùë§1,ùë§2‚ü©such a substitution, we form a vector\nof probabilities ùëùùëó by scoring the Substitution of\neach possible pair of words into each statement and\nperforming Mixture aggregation and considering\nthe Ratio of likelihoods with the template without\nsubstitution:\nùëùùëó =\n1\n|ùëÜ1 ||ùëÜ2 |\n/‚àöÔ∏Ñummationtext.Ô£∂\nùë§1 ‚ààùëÜ1,ùë§2 ‚ààùëÜ2 ùëùLLM (ùë†ùëó ‚ü®ùë§1,ùë§2‚ü©)\nùëùLLM (ùë†ùëó) .\nThe statement ùë†ùëó with highest likelihood under this\nnormalized mixture, arg maxùëó ùëùùëó, is selected.\n3.2 Odd one out\nWe examine possible Think and Sum approaches\nin depth on the ODD ONE OUT task, in which the\n1219\nGPT-3 (davinci)ùëõ-shot ThinkSum\nTask Avg. H ùëõ=0 1 2 3 GPT-3 InstructGPT GPT-2 XL\nINVENTED WORDS(¬ß3.1) N/A 0.29 0.14 0.14 0.21 0.64 0.71 0.29\nODD ONE OUT(¬ß3.2) 0.80 0.27 0.20 0.23 0.23 0.80 0.84 0.71\nFIVE OBJECTS(¬ß3.3) N/A 0.23 0.29 0.28 0.32 ‚Äì 0.77 ‚Äì\nSPORTS UNDERSTANDING(¬ßA.1) 0.71 0.50 0.50 0.50 0.50 0.71 0.74 0.54\nKNOWN UNKNOWNS(¬ßA.1) 0.80 0.61 0.52 0.48 0.50 0.54 0.76 ‚Äì\nMISCONCEPTIONSRUSSIAN(¬ßA.2) 0.65 0.33 0.33 0.41 0.35 0.70 0.61 ‚Äì\nEMOJI MOVIE(¬ßA.2) 0.93 0.12 0.18 0.12 0.19 0.80 0.75 ‚Äì\nPARSINLUREADING COMPREHENSION(¬ßA.2) 0.02 0.00 0.00 0.00 0.00 ‚Äì 0.02 ‚Äì\nPHRASE RELATEDNESS(¬ßA.3) 0.74 0.37 0.42 0.52 0.59 0.85 0.87 0.79\nCODENAMES(¬ßA.3) 0.18 0.01 0.11 0.16 0.19 0.37 0.41 0.36\nNOVEL CONCEPTS(¬ßA.4) 0.67 0.47 0.47 0.56 0.56 0.72 0.75 0.50\nCODE LINE DESCRIPTION(¬ßA.4) 0.60 0.32 0.32 0.28 0.32 0.83 0.90 0.77\nLANGUAGE IDENTIFICATION(¬ßA.5) 0.16 0.16 0.12 0.13 0.11 0.57 ‚Äì 0.30\nTable 1: Standard metric (BLEU for CODENAMES , accuracy for other tasks) for GPT-3 175B (davinci) and\nThinkSum with 175B (davinci), InstructGPT and GPT-2 XL on BIG-bench tasks. A ‚Äò‚Äì‚Äô indicates that the model\nand task combination was not evaluated because the model does not reliably execute the appropriate Think prompt.\nWe did not evaluate InstructGPT on LANGUAGE IDENTIFICATION due to the large dataset size and API quota.\nS M L XL Ada Babbage Curie da Vinci\n0\n20\n40\n60\n80\n100accuracy (%)\ntext-davinci-002\nGPT-2 GPT-3\n0-shot\n1-shot\n2-shot\n3-shot\nAux. knowledge\nThinkSum\nAverage human\nFigure 2: ODD ONE OUT . Left: Performance of GPT-3 (ùëõ-shot, ùëõ= 0,1,2,3), auxiliary knowledge, and ThinkSum\nwith various model sizes. Middle: Auxiliary knowledge vs. ThinkSum with varying number of differences. Right:\nPrompt used to generate knowledge statements.\nword in a set ùëä = {ùë§ùëñ}that is least semantically\nrelated to the others must be chosen (e.g., Pick the\nodd word out: glass, head, arm, leg, hand, foot).\nList of words. We form a semantic relatedness\nmatrix ùëÉùëñ ùëó by querying the LLM with a List of\nwords Think prompt for each pair of indices ùëñ,ùëó :\nùëÉùëñ ùëó= ùëùLLM (ùë§ùëó |‚ÄúList of words: ùë§ùëñ, ‚Äù).\nThis matrix is aggregated by averaging over ùëó (in\nlog domain) and selecting theùëñwith lowest average,\ni.e., least likelihood of being generated by a product\nmixture of all words in the set:ùëñ= arg minùëñ\n/‚àöÔ∏Å‚àöÔ∏Çoducttext.Ô£∂\nùëó ùëÉùëñ ùëó.\nThis is a case of Product aggregation.\nBecause this approach is the most successful\nwith all model sizes we experimented with, its\nperformance is reported in Table 1. Remarkably,\nnear-average-human accuracy is maintained for all\nmodel sizes from GPT-2 Small to the largest GPT-3\nmodel (Fig. 2 (left)).\nFact generation. As an alternative approach, we\nuse a Fact generation prompt. An effective way\nto mine facts for semantic relatedness tasks is to\nconsider two items in the same context in order to\nget relevant facts regarding how items are related\nto each other (prompt in Fig. 2 (right)). The demon-\nstration used in the prompt ensures that the LLM\ngenerates statements in an expected format, which\ncan be parsed and used for probability computa-\ntion later. Using this prompt, we obtain a collec-\ntion of statements ùëÜ = {ùë†ùëñ}about items ùë§ùëó. We\ntreat each generated ùë†ùëñ as a template into which\ndifferent words ùë§ can be substituted and denote\nby ùë†ùëñ ‚ü®ùë§‚ü©the Substitution of word ùë§into template\nùë†ùëñ. We then form a |ùëÜ|√ó| ùëä|matrix ùëÉùëñ ùëó, defined\n1220\nby ùëÉùëñ ùëó = ùëùLLM (ùë†ùëñ ‚ü®ùë§ùëó‚ü©). Then, we can perform\nMinority voting: we take argmin over ùëó and pick\nas the answer the most frequently occurring value,\ni.e., the item that is most often the least likely to fit\na generated statement.\nComparison with auxiliary knowledge ap-\nproaches. We compare our method with a\nknowledge-based prompting method, herein re-\nferred to as auxiliary knowledge. In auxiliary\nknowledge, we prepend generated facts in the\nprompt before the question. Details of the prompt\nfor auxiliary knowledge are provided in ¬ßD.3. In\nFigure 2 (middle), we show that the accuracy of\nFact generation -based ThinkSum rises as the\nnumber of generated facts is increased, while the\nauxiliary knowledge technique peaks and then de-\ngrades as the prompt lengthens.\nFig. 2 (left) shows how performance varies with\nthe size of the LLM used for GPT-3, auxiliary\nknowledge and ThinkSum on ODD ONE OUT .\nEven with GPT-2 Small, ThinkSum dramatically\nimproves over much larger largest zero- or few-shot\nmodels with or without auxiliary knowledge. A\nfinetuned iteration of the largest GPT-3 model, text-\ndavinci-002, is the only model variant that, with the\nhelp of auxiliary knowledge, achieves competitive\nperformance with ThinkSum. This result provides\nexperimental evidence for our claim that while new\nmodels may create qualitative jumps, ThinkSum\ncan push the performance limits of smaller models.\nLatent variable models. As we have shown, the\ndetection of the odd item can be performed with\nsimple inference operations on items, facts, and\ntheir joint likelihoods. However, it is also possible\nto assume a latent structure in the items and facts,\nconsisting of two or more clusters such that the\nfacts and items belonging to a cluster can be freely\ninterchanged. We describe a problem-specific la-\ntent variable model that enables selecting the facts\nthat characterize the majority class, thus explaining\nwhy the minority item is ruled as the odd one out\nand helping interpret the decisions of the system.\nWe model items ùëñ ‚ààùêº and facts ùëì ‚ààùêπ as be-\ning generated from a latent class ùëê ‚àà{0,1}. The\ndistribution is modeled as:\nùëÉ(ùëñ, ùëì)=\n‚àëÔ∏Å\nùëê\nùëÉ(ùëê)ùëÉ(ùëñ|ùëê)ùëÉ(ùëì|ùëê)\nwhere ùëÉ(ùëñ, ùëì)is a matrix of likelihoods from the\nLLM and the semantic components, groupings\nùëÉ(ùëñ|ùëê)and ùëÉ(ùëì|ùëê), are derived from the matrix us-\ning a standard iterative expectation-maximization\nModel LoW LVM MV\ntext-davinci-002 0.84 0.67 0.70\ntext-davinci-001 0.74 0.77 0.70\nTable 2: Different alternatives of probabilistic reasoning\nwith ThinkSum for solving ODD ONE OUT : list of\nwords, latent variable model, minority voting.\n(EM; Dempster et al., 1977) inference procedure\n(see ¬ßE). Then, the score for an item ùëñbelonging\nto a cluster and all other items ùëö ‚ààùëÜ,{ùëö ‚â† ùëñ}\nbelonging to another cluster can be found as ùëÜùëñ =/‚àöÔ∏Ñummationtext.Ô£∂\nùëê,ùëê‚Ä≤‚â†ùëê ùëÉ(ùëñ|ùëê)ùëÉ(ùëê)/‚àöÔ∏Å‚àöÔ∏Çoducttext.Ô£∂\nùëö‚â†ùëñ ùëÉ(ùëö|ùëê‚Ä≤)ùëÉ(ùëê‚Ä≤).\nWe show the effectiveness of the latent vari-\nable models in Table 2, where we analyze dif-\nferent methods for solving ODD ONE OUT using\nthe InstructGPT variants text-davinci-001 and text-\ndavinci-002. For the ‚Äòlatent variable model‚Äô and\n‚Äòminority voting‚Äô methods, we use number of differ-\nences ùëÅùëë = 5. The latent variable model is trained\nfor 200 EM iterations. All probabilistic reason-\ning methods perform well, outperforming previous\nbaselines reported in Table 1. Inference using EM,\nas well as the other approaches, can be seen as a\nSum (inference) operation and can be applicable\nin other tasks of similar structure.\n3.3 Logical deduction\nIn the LOGICAL DEDUCTION task, different types\nof items and clues regarding their order are pro-\nvided (Fig. 3(a)). The goal is to select the correct\nstatement from a set of statements about their place-\nments. The ordering problems involve different\ntypes of objects (cars, birds, etc.) and orderings\n(by size, price, contest ranking, etc.). The task\ncreators emphasize that this task requires parsing\ninformation about multiple objects and their rela-\ntionships, understanding rules regarding ordered\nobjects in various scenarios, and iteratively apply-\ning these rules. The LLM calls in the Think stage\nof ThinkSum can perform mappings required to\nparse information and understand rules, and the\nSum stage can integrate mappings of objects to\nthe placements under these rules. Here, we use a\nTranslation prompt to map the given problem into\na set of mathematical (in)equalities (Fig. 3(c)).\nThe Translation prompt in Fig. 3(b), containing\ngeneric ordering statements and object names that\nare not used in the task as an in-context demonstra-\ntion, is sufficient to perform the translation from\nnatural language to equations. By prepending this\n1221\nFigure 3: Details for LOGICAL DEDUCTION . (a) Example question from the task, (b) demonstration for the Think\nprompt, (c) example LLM output. The demonstration induces the LLM to generalize from generic objects ordered\nby size to books ordered by position.\ndemonstration prompt to a problem statement, we\ninduce the LLM to map the objects in the problem\nto the set of strings corresponding to numbers from\n1 to ùëÅ, where ùëÅ is the number of objects, and to\nproduce a set of inequalities (Fig. 3(c)).\nOnce a translation of the problem into a set of\ninequalities is obtained, theSum stage considers all\npossible mappings of items to indices to determine\nthe mapping compatible with the discovered set\nof (in)equalities. This can be done by an external\nalgorithm or by the LLM itself, as an LLM may be\ncapable of understanding that, for example, ‚Äú2>3‚Äù\nis a less likely string than ‚Äú2>1‚Äù (see ¬ßD.2).\nFinally, the probability of each of the candidate\nstatements, like ‚Äú yellow book=2 ‚Äù, can thus be\nobtained by:\nùëù(‚Äúyellow book=2‚Ä≤‚Ä≤|ùëá)\n‚àù\n‚àëÔ∏Å\nb‚àà{1,...,ùëÅ }ùëÅ\nùëùLLM ({ùëáùë° ‚ü®b‚ü©: ùëáùë° ‚ààùëá} (1)\n‚à™{‚Äúyellow book=2‚Ä≤‚Ä≤‚ü®b‚ü©})\nwhere b denotes the vector of positions for the ùëÅ\nitems (e.g., (5,2,3,4,1)), ùëá = {ùëáùë° }ùëÅ\nùë°=1 is the set of\ninequalities obtained from the Translation prompt\nas a set of strings (e.g., ‚Äú black book<purple\nbook‚Äù), and ùë†‚ü®b‚ü©denotes the substitution of the cor-\nresponding entry in b in place of the object name\nin the string ùë† (e.g., ‚Äú4<5‚Äù). The term inside the\nsum is a case of Product aggregation: the LLM\nlikelihoods of all strings in the set are multiplied.\nIn summary, our solution to this task involves\ncomposition of two Think operations ‚Äì a Transla-\ntion into a set of equations and then Substitution\nof numbers in place of item names ‚Äì and two Sum\noperations ‚Äì a Product aggregation followed by\na Mixture aggregation. (Other options are dis-\ncussed below.)\nResults and discussion. For the 500 LOGI -\nCAL DEDUCTION problems with ùëÅ = 5 objects,\nThinkSum yields an accuracy of77% (see Table 1),\nbesting the average human performance. When the\nnecessary summations become large, it becomes\nvery unlikely that pure prompt engineering can be\ncompetitive, as even humans need paper and pencil\nto create and attend to many alternative solutions,\nand would likely translate the premises into a sim-\npler notation using a single letter (representing a\nvariable to which a numeric value can be assigned)\nto represent each object, rather than directly attend-\ning to the words in the problem statement.\nWe also test an auxiliary knowledge method akin\nto chain-of-thought reasoning, where the informa-\ntion obtained with the prompt in Fig. 3 is appended\nto the LLM input. In particular, the problem, to-\ngether with its translation into inequalities, is used\nas a prompt to each of the answer options, and then\nthe option with the highest likelihood is chosen\nfor the answer. This approach does improve over\nstraightforward zero-shot GPT-3 scoring, but only\nraises the accuracy to 50% (see ¬ß3.4 and Table 3).\nOptimizations, failure modes, and extensions.\nWe have seen that InstructGPT is able both to trans-\nlate logical deduction problems into (in)equalities\n1222\n(Fig. 3) and to evaluate each of them after replace-\nment of items with position numbers (¬ßD.2). We\nconclude that the Sum stage is there simply to\nsearch over all possible mappings, the way a human\nmight. But, just as a human might use shortcuts\nin the search, the Sum stage of ThinkSum could\nbe implemented in more or less efficient ways. For\nexample, instead of summing over all possible as-\nsignments of the five items, we can avoid the ones\nthat are not permutations of {1,2,3,4,5}. Further-\nmore, instead of using ùëùLLM from Fig. D.1 in (1),\nwe can simply evaluate each inequality externally,\ngiving a high constant probability for each inequal-\nity ùëáùë° ‚ü®b‚ü©that is true and a low probability when it\nis false, or the summing can be aborted whenever\nan incorrect statement is detected in a particular\nassignment b of positions to items.\nThe prompt in Fig. 3(b) instructs the LLM to\nassign positive integers depending on the language\nused (e.g., the smallest object gets 1), but a com-\nmon behaviour of the LLM is to generalize to as-\nsigning negative numbers, such as using ‚àí2 to rep-\nresent ‚Äòsecond from the end‚Äô (or second-largest,\netc.). To remain robust to such a behavior of the\nThink stage, we can convert negative position num-\nbers ùëüinto ùëÅ+ùëü+1 before evaluating statements.\nHowever, a persistent failure mode of this kind of\nThinkSum is that the LLM may translate inequal-\nity statements inconsistently with equality state-\nments (e.g., by coding the leftmost item as 1 and\nbeing consistent with this choice for other equality\nconstraints, but translating inequality constraints\nconsistently with the reverse order, with ‚Äòleft of‚Äô\nmeaning >). Such failures can be addressed by\ncareful engineering in the Sum stage, such as by\nsumming out a binary latent variable indicating\nwhether inequalities should be reversed. This in-\ncreases the number of model evaluations, but also\nallows for robust auto-correction by the Sum stage\nof inconsistencies in the Think stage.\n3.4 Comparisons with chain-of-thought and\nauxiliary knowledge approaches\nThinkSum vs. auxiliary knowledge. Table 3\nshows the comparison of ThinkSum with algo-\nrithms that append auxiliary knowledge as an or-\nacle ‚Äòreasoning chain‚Äô. For PHRASE RELATED -\nNESS , auxiliary knowledge was generated using\nthe ‚Äúlist differences‚Äù prompt shown in Fig. 2 (right).\nFor both auxiliary knowledge and ThinkSum, 6\ngenerated differences were used, as that was the\nODD ONE OUTPHRASE RELATEDNESSLOGICAL DEDUCTION(ùëÅ=5)\nThinkSum 0.84 0.87 0.77Aux. knowledge 0.71 0.75 0.50\nTable 3: ThinkSum vs. auxiliary knowledge with text-\ndavinci-002.\nbest for auxiliary knowledge (see Fig. 2 (middle)).\nThinkSum ODD ONE OUT and PHRASE RELAT -\nEDNESS are solved with the ‚Äúlist of words‚Äù prompt.\nFor LOGICAL DEDUCTION , the Think prompt\nshown in Fig. 3 was included before the question in\nthe prompt. In all cases, ThinkSum outperforms\nauxiliary knowledge.\nThinkSum vs. chain of thought. Following Wei\net al. (2022), we use ‚Äúchain-of-thought (CoT) meth-\nods\" to mean LLM scoring approaches that use in-\nsertion of generated tokens between the prompt and\nthe target answer. The model is taught, using few-\nshot demonstrations, how to generate these interme-\ndiate tokens. Above we have compared ThinkSum\nwith approaches that add extracted (from an auxil-\niary LM call), not generated (within the LM‚Äôs lin-\near workspace) token sequences after the prompt,\nfor the ODD ONE OUT , PHRASE RELATEDNESS ,\nand LOGICAL DEDUCTION tasks (see Table 3).\nWith suitable examples, it may be possible for\na CoT approach to replace the Think phase, by\nlearning from demonstrations to generate the ap-\npropriate knowledge, and parts of the Sum phase,\nalthough inference over parallel evaluations of the\nLLM is no longer possible. Our auxiliary knowl-\nedge baselines make precisely that generous as-\nsumption and focus the comparisons on the need\nfor parallel calls and reasoning over possibilities\nusing probabilistic inference (instead of leaving it\nto the LLM to make the right conclusions from the\nlist of extracted alternatives).\nAlthough we expect that appending facts in\na standard format to the prompt would help the\nmodel more than teaching the model to generate\nthese facts, we experimented with CoT approaches\non several tasks. Table A.1 shows example demon-\nstrations and prompt formats used for each task,\nand Table 4 shows the results using two variants of\nthe largest GPT-3 model.\nAs expected, ThinkSum outperforms CoT\nprompting on all tasks with all variants except\nKNOWN UNKNOWNS with the davinci variant,\nwhere direct prompting already performs well. (We\ndid not evaluate ThinkSum with davinci on LOG-\nICAL DEDUCTION because prompts like the one\n1223\nGPT-3 (davinci) GPT-3 (davinci-002)\nTask Direct CoT ThinkSumCoT ThinkSum\nODD ONE OUT 0.27 0.33 0.80 0.64 0.84PHRASE RELATEDNESS0.59 0.55 0.85 0.79 0.87LOGICAL DEDUCTION0.32 0.25 ‚Äì 0.39 0.77KNOWN UNKNOWNS0.61 0.70 0.54 0.74 0.76INVENTED WORDS0.29 0.50 0.64 0.64 0.71\nTable 4: Comparison of ThinkSum with chain-of-\nthought prompting approaches.\nin Figure 3 did not reliably produce outputs in the\ncorrect format; notice that CoT is barely better than\nrandom guessing (20%).)\nWhen interpreting these results, it is important\nto note that only one prompt format was evalu-\nated for both CoT and ThinkSum, and the format\nof prompts and demonstrations can have a strong\nand often unpredictable effect on the LLM. We ob-\nserved that CoT approaches are highly sensitive\nto minor changes in the prompt format or the con-\nstruction of in-context examples, consistent with\nthe known biases of in-context learning (Lu et al.,\n2022; Zhao et al., 2021). On the other hand, using\nstructured, shorter components is more reliable, as\ndemonstrated by the efficacy of the Think prompts\nused in ThinkSum.\n4 Related work\nImprovements to LLM inference. After the dis-\ncovery of the in-context learning abilities of LLMs,\nthere has been an explosion of interest in improving\ninference with LLMs in the zero-shot and few-shot\nsetting (Brown et al., 2020; Chowdhery et al., 2022;\nRae et al., 2021). One approach to improving the\nreasoning abilities of LLMs involves appending, or\nlearning to generate, auxiliary knowledge within\nthe prompt (Shwartz et al., 2020; Zelikman et al.,\n2022; Nye et al., 2021a). Recently, more general\nauxiliary knowledge or chain-of-thought prompt-\ning methods have been proposed (Wei et al., 2022;\nWang et al., 2022b; Zhou et al., 2022a; Creswell\net al., 2022; Wang et al., 2022a; Liu et al., 2022b),\nincluding those that allow a control flow external\nto the main LLM (Khot et al., 2022). Later, Kojima\net al. (2022) showed zero-shot chain-of-thought\nprompting can improve performance on a variety of\nreasoning tasks. This method does not require any\nhand-crafted few-shot examples, which is a shared\nproperty with ThinkSum. (Nye et al., 2021b) ob-\nserved that a dual-system approach where an asso-\nciative ‚ÄúSystem 1‚Äù and a logical ‚ÄúSystem 2‚Äù can\nincrease coherence of LLMs in tasks such as robust\nstory generation and grounded instruction follow-\ning. The two-step paradigm in ThinkSum is simi-\nlar, where ‚ÄúSystem 1‚Äù is the (querying of the LLM\nfor) fast thinking, and ‚ÄúSystem 2‚Äù is the probabilis-\ntic inference step.\nBrittleness of chain-of-thought prompting. De-\nspite the recent success of chain-of-thought ap-\nproaches, recent studies have raised concerns re-\ngarding the limitations of chain-of-thought ap-\nproaches. Webson and Pavlick (2022) observed\nthat instructive prompts perform similarly with mis-\nleading or intentionally irrelevant prompts. Addi-\ntionally, Ye and Durrett (2022) showed improve-\nments due to few-shot chain-of-thought are not ob-\nserved in question answering, or natural language\ninference. More critically, few-shot prompts are\nhighly sensitive to the order in which the samples\nare provided, the prompt format, and the selection\nof in-context examples, (Lu et al., 2022; Zhao et al.,\n2021). Thus, it is crucial to design techniques that\nare robust to such changes in the prompt.\nInference as reasoning. Iterative inference over\nLLM outputs has been proposed for tackling\ntrue/false question answering and commonsense\nquestion answering (Jung et al., 2022; Liu et al.,\n2022a). Xie et al. (2021) presents a Bayesian infer-\nence perspective on in-context learning, and Dohan\net al. (2022) formalizes and unifies existing prompt-\ning techniques in a probabilistic framework. Our\nwork generalizes such approaches to perform arbi-\ntrary probabilistic inference outside of the LLM.\n5 Conclusion\nIn this paper we presented ThinkSum, a two-step\nprobabilistic inference paradigm that reasons over\nsets in a structured manner. The fast thinking stage\nof ThinkSum allows elementary string manipula-\ntions as well as natural language prompting, which\nmay enable numerous approaches to solve a natural\nlanguage task. Even with far smaller model vari-\nants, ThinkSum achieves state-of-the-art results on\nten difficult tasks in BIG-bench using GPT-family\nmodels. The two-step paradigm allows operating\nover sets instead of manipulating the prompt it-\nself, preventing sensitivity to prompt format during\nthe probabilistic inference in ThinkSum, which\nis performed outside of calls to the LLM. As a re-\nsult, ThinkSum is more robust to prompt design,\nyields more interpretable predictions, and can be\ncombined with many probabilistic inference ap-\nproaches to tackle a diverse set of tasks.\n1224\nAcknowledgments\nThe authors thank Alexandros Graikos, Sudha Rao,\nand Alessandro Sordoni for valuable discussions.\nLimitations\nOur proposed ThinkSum has demonstrated strong\nperformance on thirteen challenging BIG-bench\ntasks. However, it is important to acknowledge\ncertain limitations of the system.\nFirstly, as the number of objects or facts that\nare reasoned over increases, the computation cost\nwill also rise. However, increasing the number of\nobjects will also make the task harder, and direct\nprompting may cease to work at all (as we indeed\nobserve in BIG-bench results, such as LOGICAL\nDEDUCTION with more than five objects), while\nThinkSum offers a generalizable methodology, as\nthe atomic Think operations do not increase in\ncomplexity as the number of objects grows.\nSecondly, when solving a new task, it is nec-\nessary to expend human effort to select specific\noperations in each step, as outlined in ¬ß2. This\nlimitation is shared with prompt engineering of all\nkinds, including direct or chain-of-thought prompt-\ning: finding a prompt for a new task requires an\noften-cumbersome prompt engineering procedure.\nWe have described ThinkSum as a general two-\nstage paradigm, with an external inference step.\nThis generality aims to facilitate the adaptation of\nThinkSum to new tasks, with minimal modifica-\ntions to the Think and Sum steps. Work on au-\ntomating the prompt engineering procedure (Zhou\net al., 2022b) is a promising path towards over-\ncoming this limitation. An alternative to prompt\nengineering that does not require such human effort\nis tuning (i.e., differentiable end-to-end learning)\nof prompts or model parameters; however, this re-\nmains impractical for GPT-3-scale models, and at-\ntempts to tune models directly on symbolic reason-\ning chains have met with limited success (Kassner\net al., 2020).\nLast but not least, ThinkSum has mainly been\nevaluated with GPT-3 (davinci) and InstructGPT\n(text-davinci-002) models. To further improve per-\nformance, it may be beneficial to apply ThinkSum\nto more recent instruction-tuned models such as\nFlan-PaLM (Chowdhery et al., 2022; Chung et al.,\n2022), text-davinci-003, ChatGPT, and GPT-4,\nwhich seem more capable of robustly performing\nThink steps.\nEthics and impact statement\nWe foresee no direct or immediate societal impacts\narising from this work. However, we would like\nto emphasize that relying solely on LLMs‚Äô asso-\nciative reactions to prompts can lead to undesired\nbias in the behaviour of systems. Control of LLMs‚Äô\nreasoning in the way we have proposed can poten-\ntially mitigate such bias, due both to the decomposi-\ntion of the argumentation process into interpretable\nfact-retrieval steps and to the averaging effect of\nsmoothing out spurious triggers when aggregating\nmany hypotheses and reasoning chains.\nReferences\nYoshua Bengio. 2017. The consciousness prior. arXiv\npreprint arXiv:1709.08568.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. Neural In-\nformation Processing Systems (NeurIPS).\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. PaLM: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nAntonia Creswell, Murray Shanahan, and Irina Higgins.\n2022. Selection-inference: Exploiting large language\nmodels for interpretable logical reasoning. arXiv\npreprint arXiv:2205.09712.\nA. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.\nMaximum likelihood from incomplete data via the\nEM algorithm. Journal of the Royal Statistical Soci-\nety B, 39(1):1‚Äì38.\nDavid Dohan, Winnie Xu, Aitor Lewkowycz, Ja-\ncob Austin, David Bieber, Raphael Gontijo Lopes,\nYuhuai Wu, Henryk Michalewski, Rif A Saurous,\nJascha Sohl-Dickstein, et al. 2022. Language model\ncascades. arXiv preprint arXiv:2207.10342.\n1225\nNouha Dziri, Andrea Madotto, Osmar Za√Øane, and\nAvishek Joey Bose. 2021. Neural path hunter: Re-\nducing hallucination in dialogue systems via path\ngrounding. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 2197‚Äì2214, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nAnirudh Goyal and Yoshua Bengio. 2020. Inductive\nbiases for deep learning of human cognition. arXiv\npreprint arXiv:2011.15091.\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi,\nand Luke Zettlemoyer. 2021. Surface form com-\npetition: Why the highest probability answer isn‚Äôt\nalways right. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 7038‚Äì7051, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nJaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brah-\nman, Chandra Bhagavatula, Ronan Le Bras, and\nYejin Choi. 2022. Maieutic prompting: Logically\nconsistent reasoning with recursive explanations.\narXiv preprint arXiv:2205.11822.\nDaniel Kahneman. 2011. Thinking, fast and slow .\nMacmillan.\nNora Kassner, Benno Krojer, and Hinrich Sch√ºtze. 2020.\nAre pretrained language models symbolic reasoners\nover knowledge? In Proceedings of the 24th Confer-\nence on Computational Natural Language Learning,\npages 552‚Äì564, Online. Association for Computa-\ntional Linguistics.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao\nFu, Kyle Richardson, Peter Clark, and Ashish Sab-\nharwal. 2022. Decomposed prompting: A modular\napproach for solving complex tasks. arXiv preprint\narXiv:2210.02406.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nJiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Pe-\nter West, Ronan Le Bras, Yejin Choi, and Hannaneh\nHajishirzi. 2022a. Generated knowledge prompting\nfor commonsense reasoning. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n3154‚Äì3169, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nTianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao,\nZhifang Sui, Weizhu Chen, and Bill Dolan. 2021.\nA token-level reference-free hallucination detection\nbenchmark for free-form text generation. arXiv\npreprint arXiv:2104.08704.\nZihan Liu, Mostofa Patwary, Ryan Prenger, Shrimai\nPrabhumoye, Wei Ping, Mohammad Shoeybi, and\nBryan Catanzaro. 2022b. Multi-stage prompting for\nknowledgeable dialogue generation. In Findings of\nthe Association for Computational Linguistics: ACL\n2022, pages 1317‚Äì1337, Dublin, Ireland. Association\nfor Computational Linguistics.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086‚Äì8098, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nNikolay Malkin, Zhen Wang, and Nebojsa Jojic. 2022.\nCoherence boosting: When your pretrained language\nmodel is not paying enough attention. InProceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 8214‚Äì8236, Dublin, Ireland. Association for\nComputational Linguistics.\nSewon Min, Mike Lewis, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2022. Noisy channel language\nmodel prompting for few-shot text classification. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 5316‚Äì5330, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, et al. 2021a. Show your work: Scratch-\npads for intermediate computation with language\nmodels. arXiv preprint arXiv:2112.00114.\nMaxwell Nye, Michael Tessler, Josh Tenenbaum, and\nBrenden M Lake. 2021b. Improving coherence and\nconsistency in neural sequence models with dual-\nsystem, neuro-symbolic reasoning. Neural Informa-\ntion Processing Systems (NeurIPS).\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. Neural In-\nformation Processing Systems (NeurIPS).\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\n1226\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training Gopher.\narXiv preprint arXiv:2112.11446.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, pages 3784‚Äì3803, Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nVered Shwartz, Peter West, Ronan Le Bras, Chandra\nBhagavatula, and Yejin Choi. 2020. Unsupervised\ncommonsense question answering with self-talk. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4615‚Äì4629, Online. Association for Computa-\ntional Linguistics.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta,\nAdri√† Garriga-Alonso, et al. 2022. Beyond the\nimitation game: Quantifying and extrapolating the\ncapabilities of language models. arXiv preprint\narXiv:2206.04615.\nMirac Suzgun, Nathan Scales, Nathanael Sch√§rli, Se-\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny\nZhou, et al. 2022. Challenging big-bench tasks and\nwhether chain-of-thought can solve them. arXiv\npreprint arXiv:2210.09261.\nAmos Tversky and Daniel Kahneman. 1974. Judgment\nunder uncertainty: Heuristics and biases: Biases in\njudgments reveal some heuristics of thinking under\nuncertainty. Science, 185(4157):1124‚Äì1131.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Chi, and Denny Zhou. 2022a. Rationale-\naugmented ensembles in language models. arXiv\npreprint arXiv:2207.00747.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022b. Self-consistency\nimproves chain of thought reasoning in language\nmodels. arXiv preprint arXiv:2203.11171.\nAlbert Webson and Ellie Pavlick. 2022. Do prompt-\nbased models really understand the meaning of their\nprompts? In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2300‚Äì2344, Seattle, United States.\nAssociation for Computational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and\nTengyu Ma. 2021. An explanation of in-context learn-\ning as implicit bayesian inference. arXiv preprint\narXiv:2111.02080.\nXi Ye and Greg Durrett. 2022. The unreliability of\nexplanations in few-shot in-context learning. arXiv\npreprint arXiv:2205.03401.\nEric Zelikman, Yuhuai Wu, and Noah D Goodman.\n2022. STaR: Bootstrapping reasoning with reasoning.\narXiv preprint arXiv:2203.14465.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. Inter-\nnational Conference on Machine Learning (ICML).\nChunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab,\nFrancisco Guzm√°n, Luke Zettlemoyer, and Marjan\nGhazvininejad. 2021. Detecting hallucinated content\nin conditional neural sequence generation. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021, pages 1393‚Äì1404, Online.\nAssociation for Computational Linguistics.\nDenny Zhou, Nathanael Sch√§rli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022a.\nLeast-to-most prompting enables complex reason-\ning in large language models. arXiv preprint\narXiv:2205.10625.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nKeiran Paster, Silviu Pitis, Harris Chan, and Jimmy\nBa. 2022b. Large language models are human-level\nprompt engineers. arXiv preprint arXiv:2211.01910.\n1227\nA Additional tasks\nDescriptions of all the tasks studied here can be found in ¬ßC.\nA.1 Uncertainty and hallucination detection\nLLMs are prone to generating hallucinations that contain incorrect statements. The likelihoods of these\nstatements are often dominated by short plausible patterns, which also makes it difficult for LLMs to\nevaluate their own uncertainty about a fact. Thus, detection (Liu et al., 2021; Zhou et al., 2021) and\nreduction of such hallucinations is crucial for widespread use of LLMs in real applications (Dziri et al.,\n2021; Shuster et al., 2021).\nA.1.1 Sports understanding\nFigure A.1: Example posterior probabilities generated from text-davinci-002 for SPORTS UNDERSTANDING with\nthe description ‚Äúthrew a touchdown‚Äù. The basketball player given in the question Draymond Green has a much\nlower posterior probability than the generated football players, from which we conclude the sentence ‚ÄúDraymond\nGreen threw a touchdown. ‚Äùis implausible.\nQuestions in SPORTS UNDERSTANDING ask to determine whether it is ‚Äòplausible‚Äô or ‚Äòimplausible‚Äô\nthat a professional sports player ùë•(e.g., ‚ÄòDraymond Green‚Äô, a basketball player) performed an action ùëé\nassociated with a sport (e.g., ‚Äòthrew a touchdown‚Äô, an action in American football). It is implied that\nthe combination of ùë•and ùëéis plausible if the sport with which player ùë•is associated coincides with the\nsport in which action ùëéis performed. We consider an approach that does not rely on identifying the latent\nvariable (sport) as an intermediate step and is thus more generalizable to other domains.\nWe use an Example generation Think prompt to produce a set ùëÜof players who perform action ùëé,\nthen do Posterior computation by normalizing the likelihood assigned by the LLM to each player in ùëÜ,\nas well as ùë•, performing action ùëé:\n‚àÄùë¶ ‚ààùëÜ‚à™{ùë•} ùëù(ùë¶|ùëé)= ùëùLLM (‚Äúùë¶ùëé ‚Äù)/‚àöÔ∏Ñummationtext.Ô£∂\nùë¶‚Ä≤‚ààùëÜ‚à™{ùë•}ùëùLLM (‚Äúùë¶‚Ä≤ùëé‚Äù)\nThe statement is considered to be implausible if the posterior on ùë•is sufficiently low (Thresholding) ‚Äì\nsee Fig. A.1.\nA.1.2 Known unknowns\nQuestions in the KNOWN UNKNOWNS task ask to determine whether the answer to a question is a certain\nprecise concept or ‚Äòunknown‚Äô.\nGiven a question ùëû(e.g., ‚ÄúWhat was the temperature in Cuzco on the day of the Emperor Vespasian‚Äôs\nbirth‚Äù) and the candidate precise answer ùëé(e.g., 25‚ó¶C), we use a List extension prompt to generate a set\nùëÜof other possible answers to ùëû. We then do a Posterior computation over ùëÜand the original answer ùëé,\nsimilar to that used for SPORTS UNDERSTANDING :\n‚àÄùë¶ ‚ààùëÜ‚à™{ùëé} ùëù(ùë¶|ùëû)= ùëùLLM (‚Äúùëû? ùë¶‚Äù)/‚àöÔ∏Ñummationtext.Ô£∂\nùë¶‚Ä≤‚ààùëÜ‚à™{ùëé}ùëùLLM (‚Äúùëû? ùë¶‚Ä≤‚Äù).\n1228\nThe answer ùëéis chosen if the posterior on ùëéis sufficiently high (Thresholding), and otherwise ‚Äòunknown‚Äô\nis chosen.\nA.2 Translation between languages and writing systems\nThis extends the results on LOGICAL DEDUCTION in ¬ß3.3.\nA.2.1 Russian misconceptions.\nIn the MISCONCEPTIONS RUSSIAN task, the true statement must be chosen out of a pair of Russian\nsentences: a statement ùë†and its negation ùë°.\nWe first describe an approach that does not use translation and already performs better than random\nguessing ‚Äì and better than baseline methods that simply select the more likely of the two statements ‚Äì\nusing the largest GPT-3 model, which has sufficient knowledge of Russian. We compute the posterior\nover the two hypotheses ‚Äúùë†is true, ùë°is false‚Äù and ‚Äúùë†is false, ùë°is true‚Äù:\nùëùLLM (‚ÄúT‚Äù |‚ÄúT or F? ùë†. Answer: ‚Äù)ùëùLLM (‚ÄúF‚Äù |‚ÄúT or F? ùë°. Answer: ‚Äù),\nùëùLLM (‚ÄúF‚Äù |‚ÄúT or F? ùë†. Answer: ‚Äù)ùëùLLM (‚ÄúT‚Äù |‚ÄúT or F? ùë°. Answer: ‚Äù).\nwhere T denotes True and F Falsein the actual prompt. This is a kind of Product aggregation. If the\nposterior on the first option is higher, ùë†is chosen as the true statement; otherwise, ùë°is chosen.\nThis approach can be combined with a Translation prompt that produces translations of ùë†and ùë°into\nEnglish, then uses these translations in place of ùë†and ùë° in the above computations. The approach can\nbe further extended by sampling a set of translations and performing Mixture aggregation over the\ntranslations. Our reported result uses 10 generated translation for each statement, but it is only 2% higher\nthan the result using one generated translation.\nA.2.2 Emoji movie\nThe multiple-choice EMOJI MOVIE task requires selecting the name of a movie from a list {ùëöùëñ}that is\nbest described by a sequence of emoji symbols ùë†= (ùë†1 ...ùë† ùëõ). An Order inversion prompt performs best\non this task using the Davinci variant of GPT-3: choosing the answer\narg max\nùëñ\nùëùLLM (ùë† |‚ÄúEmoji describing the movie ùëöùëñ‚Äù).\nWe also attempt to use aTranslation prompt to obtain a single-word English description ùë§ùëó of each emoji\nùë†ùëó in ùë†, then score using\narg max\nùëñ\nùëùLLM (ùë§1 ...ùë§ ùëõ |‚ÄúWords describing the movie ùëöùëñ‚Äù).\nThis approach performs slightly better than Order inversion alone using InstructGPT. However, it does\nnot work with the base GPT-3 models, which do not as reliably translate emoji to English.\nA.2.3 Persian QA\nWe solve this standard extractive question answering task by simply translating the passage and question\nfrom Persian to English using a Translation prompt, generating English text, up to the first period or line\nbreak, following the concatenation of the translated prompt and question, and translating the result back\nto Persian using another Translation prompt.\nNo few-shot algorithms have above zero accuracy on this task, indicating models‚Äô knowledge is\nsufficient to translate between languages (probably due to the presence of paired data in the training\ncorpus), but insufficient to reason in the source language without passing through an intermediate latent\nvariable, the translation.\nFinally, note that the accuracy is evaluated by exact string match, which contributes to the very low\nscores. We observed that the answers generated by ThinkSum are often paraphrases or terms related to\nthe correct answers, which suggests that the result could be improved by using the knowledge that the\ntarget string always appears verbatim as a substring of the prompt.\n1229\nA.3 Semantic relatedness\nThis extends the results on ODD ONE OUT in ¬ß3.2.\nA.3.1 Phrase relatedness\nEach question in the multiple-choice PHRASE RELATEDNESS task requires to determine which of a given\nset of words or phrases {ùë§ùëñ}is related to a query phrase ùëû. We query the LLM for the likelihood of ùëû\nfollowing a List of words prompt to form a vector of likelihoods:\nùëùùëñ = ùëùLLM (ùëû |‚ÄúList of words: ùë§ùëñ, ‚Äù).\nThe answer selected is the one with highest likelihood, arg maxùëñ ùëùùëñ (a trivial Sum operation). We note\nthat this is also an instance of Order inversion: the query is scored following a prompt in which each of\nthe candidate answers is substituted.\nA.3.2 Codenames\nEach question in CODENAMES requires selecting the ùëòwords from a set {ùë§ùëñ}that are most closely related\nto a query word ùëû. We form a vector ùëùùëñ in the same way as for PHRASE RELATEDNESS , then select the\ntop ùëòentries in ùëùùëñ to produce the output.2\nA.4 Substitution and aggregation\nWe give two other example of substitution and aggregation operations complementing the experiments on\nINVENTED WORDS (¬ß3.1) and ODD ONE OUT (¬ß3.2).\nA.4.1 Novel concepts\nIn the multiple-choice NOVEL CONCEPTS task, a set of words or phrasesùëä = {ùë§ùëñ}and a set of statements\nùëÜ= {ùë†ùëó }with third-person plural pronoun subjects (‚ÄòThey all...‚Äô) are given, and the statement which is\ntrue for all items in ùëä must be determined.\nWe treat each statement ùë†ùëó as a template, into which words ùë§can be substituted by replacing ‚ÄòThey\nall‚Äô with ùë§. Denoting by ùë†ùëó ‚ü®ùë§‚ü©the substitution of ùë§into ùë†ùëó, we form a |ùëä|√ó| ùëÜ|matrix ùëÉùëñ ùëóby scoring\nthe Substitution of each word into each statement and considering the Ratio of likelihoods with the\ntemplate without substitution: ùëÉùëñ ùëó = ùëùLLM (ùë†ùëó ‚ü®ùë§ùëñ ‚ü©)\nùëùLLM (ùë†ùëó) .We then perform Product aggregation to select the\nstatement which is most likely to be generated by all words in the set. To be precise, the selected statement\nis arg maxùëó\n/‚àöÔ∏Å‚àöÔ∏Çoducttext.Ô£∂\nùëñ ùëÉùëñ ùëó.\nA.4.2 Code line description\nWe solve the CODE LINE DESCRIPTION task, in which a correct comment for a code snippet is to be\nchosen, using Order inversion and Substitution techniques.\nThe greatest gain ‚Äì amounting for all but 1% of the improvement relative to direct prompting ‚Äì arises\nfrom Order inversion. Instead of ranking the candidate comments ùëêby their likelihood following the\ngiven code ùë†(i.e., ùëù(ùëê|ùë†)), we score each candidate comment ùëêby the likelihood of the code to follow ùëê\nformatted as a Python comment (ùëù(ùë†|‚Äú# ùëê‚Äù)).\nWe also experimented with Substitution and Product aggregation, which yielded an additional small\naccuracy gain. The code snippets are written in Python, which requires code to be formatted using an\narbitrary but consistent number of spaces for line indentation. Using the knowledge that the correct\ncomment should be most likely to generate the program in any of its equivalent representations, we scored\ncomments in the manner described in the preceding paragraph, but with ùë† reformatted with different\nnumber of indentation spaces ùëõ. The resulting scores were then multiplied over ùëõ= 1,2,..., 6 and the\nhighest-scoring comment selected.\n2Because the task is evaluated by BLEU score against the reference answers listed in alphabetical order, we perform the\nadditional step of converting the top indices to the answer in the right format. Alphabetization of short lists is trivial in code, but\ncan also very reliably be done by prompting GPT-3.\n1230\nFigure B.1: Margin between 0-shot GPT-3 and average human performance for BIG-bench Lite tasks. Using\nThinkSum, we address many of the tasks that have greater than 10% performance margin with average human, and\nsignificantly reduce and often overturn the margin.\nA.5 Other tasks\nA.5.1 Language identification\nThe multiple choice LANGUAGE IDENTIFICATION task is similar in form and solution to CODE LINE\nDESCRIPTION and we include it for completeness to show the large difference that can be made by Order\ninversion.\nRather than scoring all candidate language names ‚Ñìfollowing the given sentence ùë†(i.e., ùëù(ùë†|‚Ñì)), we\ninstead score each language name ‚Ñìby ùëù(ùë†|‚ÄúThe following is a sentence in ‚Ñì:‚Äù)and select the\nhighest-scoring ‚Ñìas the answer.\nB BIG-bench Lite\nFigure B.1 shows the performance margin between an average human and zero-shot GPT-3 on tasks\nin BIG-bench Lite, a select subset of tasks chosen by the authors of the benchmark to showcase the\nmost important aspects of LLMs that need improvement. The vertical black bar separates the dataset\ninto tasks where GPT-3 is already within the margin of just 10% compared to the average human\naccuracy, and the harder tasks (on the left). We show in the main text that some of these harder tasks,\nin particular EMOJI MOVIE , CONCEPTUAL COMBINATIONS ,KNOWN UNKNOWNS , NOVEL CONCEPTS ,\nMISCONCEPTIONS RUSSIAN and LOGICAL DEDUCTION , the margins are shrunk considerably, often\nexceeding average human performance. Other tasks in BIG-bench lite such as LOGIC GRID PUZZLE and\nSYMBOL INTERPRETATION share a similar structure to the addressed by ThinkSum, and thus could be\ninvestigated as part of future work. Another example where ThinkSum can be applied is the CODE LINE\nDESCRIPTION task, where we observe in our preliminary experiments that a simple order inversion can\nsignificantly outperform average human accuracy.\nC Task descriptions\nC.1 Hallucination detection\nKnown unknowns. In this task, the aim is to measure the ability of LLMs to identify whether the answer\nto a question is known, or unknown. If the answer to the question cannot be known, an imaginary answer\n1231\nis generated, and the model is asked to choose between the imaginary answer and the word ‚ÄúUnknown‚Äù.\nIf the question has a correct answer, the correct answer and ‚ÄúUnknown\" are provided as options. An\nexample question from this task is ‚ÄúHow often did Abraham Lincoln cut his toenails?‚Äù, and the options\nare ‚ÄúEvery Saturday night‚Äù,‚ÄúUnknown‚Äù. Similar to the sports understanding task, we can use an LLM\nto generate other plausible answers with the following prompt: How often did Abraham Lincoln cut his\ntoenails? A possible answer is: \\n 1. Every Saturday night. List 4 other possible answers in the same\nformat as the first: \\n 2. Then, the answer given in the question is predicted to be ‚Äúknown‚Äù if its posterior\nis higher by the second most likely option by some margin. In our experiments, we chose this value to be\n1/ùëÅùëí where ùëÅùëí is the number of examples, including the original option.\nSports understanding. In this task, artificial sentences are constructed using the name of professional\nsports players and actions from particular sports. The model is then asked to identify whether the sentence\nis plausible, where a sentence is considered plausible if the sport of the player matches the sport of the\naction described in the sentence. An example from the task is ‚ÄúStatement: Draymond Green threw a\ntouchdown. Plausible/implausible?‚Äù\nFor ThinkSum to be able to parse outputs, GPT-3 generations need to be in a pre-determined format.\nWhile larger models can obey a format without demonstrations, for smaller models it is helpful to\ndemonstrate the format with an example. Thus, we use the following prompt: ‚Äú List 4 examples of players\nwho scored a rabona goal.\\n 1. Cristiano Ronaldo\\n 2. Erik Lamela\\n 3. Mario Balotelli\\n 4. Angel Di\nMaria\\n List 4 examples of players who threw a touchdown.\\n1. ‚Äù.\nC.2 Translation\nEmoji movie. In this task, the LLM is presented with emojis that describe the plot of a movie, and\nthe model is asked to guess the correct movie from the given options. The task evaluates the LLMs\nunderstanding of emojis, and its knowledge of popular movies. An example from the task is ‚ÄúWhat movie\ndoes this emoji describe? {lying-face-emoji} \\n, pinocchio \\n heat \\n the big sleep, \\n inception \\n lilo &\nstitch\nMisconceptions Russian. In this task, the LLM is presented with sentences in Russian that contain mis-\nconceptions, and their true counterparts. The task evaluates if the LLM believes common misconceptions.\nOne example of a target pair from the task, translated to English is as follows: ‚ÄúVaccines do not cause\nautism. \\n Vaccines cause autism. ‚Äù\nParsiNLU reading comprehension. This is a standard extractive QA task in which a short passage and\nquestion are provided in Persian and the model must generate the answer, also in Persian.\nC.3 Semantic relatedness\nPhrase relatedness. In this task, an input phrase, and four target phrases are presented to the language\nmodel. The language model is asked to identify the most related choice from the listed target options.\nAn example from the task is ‚ÄúFor each word or phrase, identify the most related choice from the listed\noptions. \\n Input: home town \\n Option: town center \\n Option: location \\n Option: native city \\n Option:\nhome run‚Äù\nCodenames. In this task, the language model is asked to identify words associated with a given word.\nAn example from the task is ‚ÄúTry to identify the 2 words best associated with the word WHITE from the\nfollowing list: \\n book, anchor, rainbow, shoulder, tunnel, sack, drum, pacific, page, mark, gear, glacier.\nGive your answer in alphabetical order. ‚Äù\nOdd one out. This task is aimed at evaluating the capability of LLMs in semantic relatedness. This\ntask presents the model with four to six words, where all words except one word are semantically or\ngrammatically related to each other. The goal for the language model is to identify the odd word. An\nexample question from the task is ‚ÄúPick the odd word out: glass, head, arm, leg, hand, foot‚Äù.\nC.4 Concept understanding\nIn the following tasks, the shared goal is to test the ability of LLMs on concepts over entities that have\nlikely not been observed during training.\n1232\nConceptual combinations: Invented words. In this task, the LLM is provided with two invented\nwords, and their definitions in the input. The LLM is then asked to infer the most plausible meaning\nresulting from the combination of the invented words. As the words are invented, they are not present\nin the training set, and the LLM needs to understand and combine the definitions of the invented words\nto reason about the meaning of the combination. An example is: ‚ÄúThe word ‚Äôbinne‚Äô means any animal\nthat is furry and has four legs, and the word ‚Äôbam‚Äô means a simple sort of dwelling. Question: Which of\nthe following sentences best characterizes binne bams?‚Äù. Similar to SPORTS UNDERSTANDING , we can\nuse the following prompt to force the LLM to obey a fixed format: ‚ÄúList synonyms of binne, separate\nsynonyms by comma:‚Äù\nNovel concepts. In this task, the LLM is presented with two to four disparate entities that typically\nwould not co-occur frequently, but share an underlying conceptual or linguistic concept. The aim is to test\nthe ability of the LLM to reason about entities that are unlikely to have been observed in the same context\nduring training. In a multiple-choice setting, the LLM is given concepts relating to the entities, and is\nasked to generate the intended concepts against carefully chosen tempting distractors. The choices are not\npresented in the prompt. An example question from the task is as follows: ‚ÄúWhat do the following have in\ncommon? 1) bumble bees 2) 01010101 3) race cars‚Äù, and the answer options are They all make noise,\n‚ÄúThey all are yellow, They all are binary, They all go fast, They all have stripes‚Äù.\nC.5 Other tasks\nTwo multiple-choice tasks test the LLM‚Äôs knowledge of specific domains, such as uncommon languages\nand programs.\nCode line description. This task requires the LLM to select the appropriate text description, out of four\nchoices, for a short snippet of Python code, that could act as a comment describing the behaviour of a\nfunction.\nC.5.1 Language identification.\nThis task requires the LLM to select, out of eleven choices, the language in which a text is written. The\nlanguages represent a diversity of language families and writing systems and most are very infrequent in\ntext found on the Internet.\nD Additional experimental details\nOur experiments are performed using four different sizes of GPT-2 (Small, Medium, Large, and XL)\n(Radford et al., 2019), GPT-3 with four different model sizes (ada,babbage,curie,davinci) (Brown et al.,\n2020), and InstructGPT (Ouyang et al., 2022). All GPT-3 experiments are run between August 2022 and\nSeptember 2022 by using the OpenAI API. Our GPT-2 experiments were run in PyTorch (Paszke et al.,\n2019) and the Hugging Face Transformers library with a Tesla K80 GPU.\nD.1 Hyperparameters\nMaximum generation length. For tasks that require example and list generation, such as CONCEP -\nTUAL COMBINATIONS , KNOWN UNKNOWNS , and SPORTS UNDERSTANDING , we use max_tokens = 100.\nFor fact generation in ODD ONE OUT with auxiliary knowledge and ThinkSum, we use max_tokens =\n1000.\nTemperature. All GPT-2 experiments used temperature = 0.5. For SPORTS UNDERSTANDING and\ntranslation tasks, we used temperature = 0.5 to promote diversity of generated plausible options. All other\nexperiments used temperature = 0 (greedy decoding).\nNumber of examples ( ùëÅùëí). For CONCEPTUAL COMBINATIONS we used ùëÅùëí = 2, and for KNOWN\nUNKNOWNS and SPORTS UNDERSTANDING we used ùëÅùëí = 4.\nThreshold. A threshold of 0.01 was used for SPORTS UNDERSTANDING .\n1233\n1 2 3 4 5 6 7 8 9\n1\n2\n3\n4\n5\n6\n7\n8\n9\ni < j\n1 2 3 4 5 6 7 8 9\n1\n2\n3\n4\n5\n6\n7\n8\n9\ni = j\n1 2 3 4 5 6 7 8 9\n1\n2\n3\n4\n5\n6\n7\n8\n9\ni > j\nFigure D.1: Probabilities of different (in)equalities according to GPT-3 text-davinci-002 (logit).\nFigure D.2: Auxiliary knowledge prompting applied to ODD ONE OUT . Facts are generated using the ‚Äúlist\ndifferences‚Äù prompt described in Figure 2 (right) and post-processed according to ¬ßD.3.\nD.2 Using an LLM to evaluate inequalities.\nUsing GPT-3 or external algorithms to evaluate inequalities. We show how a LLM can be used\nto find the truth values of inequalities involving small numbers, rather than resorting to calls to an\nexternal system that is aware of arithmetic. Fig. D.1 shows the matrix of posterior probabilities evaluated\nusing InstructGPT (text-davinci-002) for strings of form ‚Äúùë•=ùë¶‚Äù, ‚Äúùë•<ùë¶‚Äù, ‚Äúùë•>ùë¶‚Äù for ùë•,ùë¶ ‚àà{1,.., 9}. The\nprobabilities are computed using prompts of the form ‚ÄúTrue or false: ùë•<ùë¶? The answer is:‚Äù and\nnormalizing the probability of the first token over the two options ‚Äú true‚Äù and ‚Äúfalse‚Äù. These are the\nprobabilities evaluated in (1).\nD.3 Knowledge generation details\nPost-processing. In our knowledge generation experiments for both ThinkSum and the auxiliary\nknowledge approach, we post-process the generated knowledge statements, to ensure formatting does not\nharm the predictions of each method. We first remove the extra spaces and the numbers and punctuation\ngenerated by the LLM before each fact while enumerating the items of the list. Later, we only keep\nsentences that contain only one of the objects of interest from the task, to make sure each sentence contains\na knowledge statement into which any of the objects can be substituted. Finally, sentences with less than\n3 words are removed as these are not likely to contain informative statements.\nAuxiliary knowledge. For auxiliary knowledge experiments, we prepend the generated and post-\nprocessed knowledge statements before the question in the task. An example is illustrated in Figure\nD.2.\nD.4 Inference Cost for ThinkSum\nThe inference cost for ThinkSum scales with the number of parallel calls to the LLM, which is determined\nfor each task by the number of Think prompts used and the number of objects for which likelihood\ncomputations are required at the Sum stage. For the tasks that we considered, as the number of Think\n1234\nprompts is not typically high and the prompts are short, the inference cost increase is marginal. In some\ncases, ThinkSum is faster than chain-of-thought prompting due to its ability to perform parallel calls to the\nLLM. For instance, ThinkSum is 23% faster for PHRASE RELATEDNESS compared to chain-of-thought\napproaches with 5 facts generated using InstructGPT.\nE Expectation Maximization\nWe model items ùëñ ‚ààùêºand facts ùëì ‚ààùêπas being generated from a latent class ùëê‚àà{0,1}. The distribution\nis modeled as:\nùëÉ(ùëñ, ùëì|ùëê)= ùëÉ(ùëñ |ùëê)ùëÉ(ùëì |ùëê) ùëÉ(ùëñ, ùëì)=\n‚àëÔ∏Å\nùëê\nùëÉ(ùëê)ùëÉ(ùëñ, ùëì|ùëê)\nwhere ùëÉ(ùëñ, ùëì)is a matrix of likelihoods from the LLM and the semantic components, groupings ùëÉ(ùëñ |ùëê)\nand ùëÉ(ùëì |ùëê). The iterative expectation-maximization (EM; Dempster et al., 1977) algorithm to derive\nùëÉ(ùëñ |ùëê)and ùëÉ(ùëì |ùëê)has the following updates:\nùëÑ(ùëê |ùëñ, ùëì)‚àù ùëÉ(ùëñ |ùëê)ùëÉ(ùëì |ùëê)ùëÉ(ùëê)\nùëÉ(ùëñ |ùëê)‚àù\n‚àëÔ∏Å\nùëì\nùëÉ(ùëñ, ùëì)ùëÑ(ùëê |ùëñ, ùëì)\nùëÉ(ùëì |ùëê)‚àù\n‚àëÔ∏Å\nùëñ\nùëÉ(ùëñ, ùëì)ùëÑ(ùëê |ùëñ, ùëì)\nùëÉ(ùëê)‚àù\n‚àëÔ∏Å\nùëñ, ùëì\nùëÉ(ùëñ, ùëì)ùëÑ(ùëê |ùëñ, ùëì)\nwhere ùëÑ(ùëê |ùëñ, ùëì)is the posterior distribution over the latent class ùëêthat we maintain for each pair (ùëñ, ùëì).\nEM is run for 200 iterations, which is more than sufficient for convergence.\n1235\nODD ONE OUT\nWords: blue, pink, magenta, banana\nAll words are colors except banana. The odd one out is banana.\nWords: pencil, eraser, baby, rule, notebook\nAll words are office supplies except baby. The odd one out is baby.\nPHRASE RELATEDNESS\nFor each word or phrase, identify the most related choice from the listed options.\nInput: Ice Cream\nOption: Antarctica\nOption: Titanic\nOption: Dessert\nOption: Sour Cream\nIce cream is a type of dessert. Therefore, ice cream and dessert are the most related.\nAnswer: Dessert\nKNOWN UNKNOWNS\nWhat was the population of San Francisco in 2018?\nOption: 879,676\nOption: Unknown\nThe question asks the population of San Francisco in 2018, for which data can be collected.\nPopulation data for cities on a yearly basis is available, and thus the answer is known, and\nit is 879,676.\nAnswer: 879,676\nWhat was the population of San Francisco yesterday?\nOption: 891,402\nOption: Unknown\nThe question asks the population of San Francisco yesterday. As it is not possible to know\nthe exact population of a city on a daily basis, the answer for this question is unknown.\nAnswer: Unknown\nLOGICAL DEDUCTION\nOn a table, there are five plates: a black plate, a white plate, a green plate, a blue plate,\nand a red plate. The white plate is bigger than the green plate. The red plate is the biggest.\nThe black plate is bigger than the blue plate. The black plate is smaller than the green\nplate. Which plate is the smallest?\nOption: The red plate is the smallest.\nOption: The black plate is the smallest.\nOption: The white plate is the smallest.\nOption: The green plate is the smallest.\nOption: The blue plate is the smallest.\nThe black plate is bigger than the blue plate. The black plate is smaller than the green\nplate, as a result the green plate is bigger than the blue plate as well. The white plate is\nbigger than the green plate, which is bigger than the blue plate. As a result, the green plate\nis bigger than the blue plate. The red plate is the biggest, so it is bigger than the blue plate.\nSince all other plates are bigger than the blue plate, the blue plate is smallest.\nAnswer: The blue plate is the smallest.\nINVENTED WORDS\nThe word ‚Äôborger‚Äô are animals who bite specific things for fun, and the word ‚Äôfolpt‚Äô is a\ntype of a chewy toy. Question: Which of the following sentences best characterizes borger\nfolpts?\nOption: Borger folpts are leashes for animals.\nOption: Borger folpts are toys for infants.\nOption: Borger folpts are hard to swallow.\nOption: Borger folpts are pet toys.\nBorgers are animals, and folpts are chewy toys. Therefore, borger folpts are chewy toys\nthat animals, or pets, can play with. Therefore, the answer is borger folpts are pet toys.\nAnswer: Borger folpts are pet toys.\nTable A.1: Few-shot demonstrations used for chain of thought (Table 4).\n1236\nTask: PHRASE RELATEDNESS\nInput: For each word or phrase, identify the most related choice from the listed options.\nInput: home town\nOption: town center\nOption: location\nOption: native city\nOption: home run\nTask: ODD ONE OUT\nInput: Pick the odd word out: glass, head, arm, leg, hand, foot\nTask: CONCEPTUAL COMBINATIONS : I NVENTED WORDS\nInput: The word ‚Äòbinne‚Äô means any animal that is furry and has four legs, and the word ‚Äòbam‚Äô means a simple sort of dwelling.\nQuestion: Which of the following sentences best characterizes binne bams?\nOption: Binne bams are for pets.\nOption: Binne bams are typically furnished with sofas and televisions.\nOption: Binne bams are luxurious apartments.\nOption: Binne bams are places where people live\nTask: NOVEL CONCEPTS\nInput: What do the following have in common? 1) bumble bees 2) 01010101 3) race cars\nOption: They all make noise.\nOption: They all are yellow.\nOption: They all are binary.\nOption: They all go fast.\nOption: They all have stripes.\nTask: LOGICAL DEDUCTION : FIVE OBJECTS\nInput: On a shelf, there are five books: a gray book, a red book, a purple book, a blue book, and a black book. The red book is\nto the right of the gray book. The black book is to the left of the blue book. The blue book is to the left of the gray book. The\npurple book is the second from the right.\nOption: The {gray, red, purple, blue, black}book is second from the right.\nTask: CODENAMES\nInput: Try to identify the 2 words best associated with the word WHITE from the following list: book, anchor, rainbow, shoulder,\ntunnel, sack, drum, pacific, page, mark, gear, glacier. Give your answer in alphabetical order.\nTask: KNOWN UNKNOWNS\nInput: How often did Abraham Lincoln cut his toenails?\nOption: Every Saturday night\nOption: Unknown\nTask: SPORTS UNDERSTANDING\nInput: Statement: Draymond Green threw a touchdown. Plausible/implausible?\nTask: EMOJI MOVIE\nInput: What movie does this emoji describe?{lying-face-emoji}\nOption: pinocchio\nOption: heat\nOption: the big sleep\nOption: inception\nOption: lilo & stitch\nTask: MISCONCEPTIONS RUSSIAN\nInput: Vaccines cause autism. / Vaccines do not cause autism.[in Russian]\nTask: CODE LINE DESCRIPTION\nInput:\nfor i in range(23):\nprint(i)\nOption: prints values from 0 to 22,\nOption: computes first 10 prime numbers,\nOption: prints values from 1 to 10,\nOption: prints ‚Äôhello world‚Äô to the terminal\nTask: PARSI NLU READING COMPREHENSION\nInput: To reduce fever, use over-the-counter medications such as acetaminophen and ibuprofen. Note the appropriate dosage\nand do not use them alongside other fever-reducing medications. You should not give aspirin to your baby without consulting a\ndoctor. Babies under 6 months of age should not be given ibuprofen.\nWhat brings down fever?\n[in Persian]\nTask: LANGUAGE IDENTIFICATION\nInput: Given a sentence, select the correct language among the choices.\nMi texaas o a mu vipin simi ri xavil ina vipin si Krais xa. E mi lamon o ne taa siak a xavil ina vipin si Krais e faxuvule xuvul\npana vipin sina tefin aava lisan xolane, piau paaliu!\nOptions: Assamese, Nandi, Patamona, Chavacano, Kapingamarangi, Turkish, Kara, Bribri, Gofa, Pali, Shatt\nTable D.1: List of examples for the studied BIG-bench tasks.\n1237\nACL 2023 Responsible NLP Checklist\nA For every submission:\n‚ñ°\u0013 A1. Did you describe the limitations of your work?\nSee \"limitations\" section on p.9.\n‚ñ°\u0017 A2. Did you discuss any potential risks of your work?\nWe see no risks beyond those already inherent in large language models, but we include Limitations\nand Ethics sections before the references (p.9).\n‚ñ°\u0013 A3. Do the abstract and introduction summarize the paper‚Äôs main claims?\nSee the abstract and introduction.\n‚ñ°\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB ‚ñ°\u0013 Did you use or create scientiÔ¨Åc artifacts?\nWe use existing models and datasets. See following answers.\n‚ñ°\u0013 B1. Did you cite the creators of artifacts you used?\nSee the introduction, where we cite the BIG-bench suite.\n‚ñ° B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Note that the BIG-bench benchmark, which we use, is licensed for use in academic\nwork such as ours.\n‚ñ°\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciÔ¨Åed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nWe use the BIG-bench suite. In the introduction, we describe it and summarize its motivations.\n‚ñ°\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiÔ¨Åes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nWe used an existing large-scale benchmark to evaluate pretrained language models. We believe the\ndata for the speciÔ¨Åc tasks we studied is very unlikely to contain such content, which should be clear\nfrom the task examples (last page of the paper), although this may not be true of all tasks in the\nBIG-bench suite.\n‚ñ°\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSee the task descriptions in Appendix D.\n‚ñ°\u0017 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiÔ¨Åcant, while on small test sets they may not be.\nWe used existing benchmarks (BIG-bench) for which extensive documentation exists.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1238\nC ‚ñ°\u0013 Did you run computational experiments?\nSee section 3 and the Appendix.\n‚ñ°\u0017 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nWe use the OpenAI API to run experiments with GPT-3-family models, which accounts for the bulk\nof the computational cost. However, the exact cost is unknown. On the order of 250k queries were\nmade to the API to obtain the results in the paper.\n‚ñ°\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSee Appendix E.\n‚ñ°\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nMost of the experiments are deterministic. A few experiments use sampled decoding of large language\nmodels (at low temperature), and we describe the settings in Appendix E.\n‚ñ°\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSee Appendix E.\nD ‚ñ°\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n‚ñ° D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n‚ñ° D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants‚Äô demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n‚ñ° D3. Did you discuss whether and how consent was obtained from people whose data you‚Äôre\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n‚ñ° D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n‚ñ° D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n1239"
}