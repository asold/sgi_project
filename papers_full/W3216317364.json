{
  "title": "Knowledge Based Multilingual Language Model",
  "url": "https://openalex.org/W3216317364",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2111528926",
      "name": "Lin-Lin Liu",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2100379612",
      "name": "Xin Li",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2740518062",
      "name": "Ruidan He",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2160800796",
      "name": "Lidong Bing",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1208744602",
      "name": "Shafiq Joty",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2127260490",
      "name": "Luo Si",
      "affiliations": [
        "Nanyang Technological University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3102859667",
    "https://openalex.org/W3100283070",
    "https://openalex.org/W2915429162",
    "https://openalex.org/W1838058638",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3032816972",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W2950342398",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W3104178968",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W3182414949",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2144578941",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W3034724424",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W3035579820",
    "https://openalex.org/W2989143494",
    "https://openalex.org/W2080133951"
  ],
  "abstract": "Knowledge enriched language representation learning has shown promising performance across various knowledge-intensive NLP tasks. However, existing knowledge based language models are all trained with monolingual knowledge graph data, which limits their application to more languages. In this work, we present a novel framework to pretrain knowledge based multilingual language models (KMLMs). We first generate a large amount of code-switched synthetic sentences and reasoning-based multilingual training data using the Wikidata knowledge graphs. Then based on the intra- and inter-sentence structures of the generated data, we design pretraining tasks to facilitate knowledge learning, which allows the language models to not only memorize the factual knowledge but also learn useful logical patterns. Our pretrained KMLMs demonstrate significant performance improvements on a wide range of knowledge-intensive cross-lingual NLP tasks, including named entity recognition, factual knowledge retrieval, relation classification, and a new task designed by us, namely, logic reasoning. Our code and pretrained language models will be made publicly available.",
  "full_text": "Enhancing Multilingual Language Model with\nMassive Multilingual Knowledge Triples\nLinlin Liu1,2∗ Xin Li1 Ruidan He1 Lidong Bing1† Shaﬁq Joty2,3 Luo Si1\n1DAMO Academy, Alibaba Group\n2Nanyang Technological University, Singapore\n3Salesforce Research\n1{linlin.liu, xinting.lx, ruidan.he, l.bing, luo.si}@alibaba-inc.com 2srjoty@ntu.edu.sg\nAbstract\nKnowledge-enhanced language representation\nlearning has shown promising results across\nvarious knowledge-intensive NLP tasks. How-\never, prior methods are limited in efﬁcient\nutilization of multilingual knowledge graph\n(KG) data for language model (LM) pretrain-\ning. They often train LMs with KGs in indi-\nrect ways, relying on extra entity/relation em-\nbeddings to facilitate knowledge injection. In\nthis work, we explore methods to make bet-\nter use of the multilingual annotation and lan-\nguage agnostic property of KG triples, and\npresent novel knowledge based multilingual\nlanguage models (KMLMs) trained directly on\nthe knowledge triples. We ﬁrst generate a large\namount of multilingual synthetic sentences us-\ning the Wikidata KG triples. Then based on\nthe intra- and inter-sentence structures of the\ngenerated data, we design pretraining tasks\nto enable the LMs to not only memorize the\nfactual knowledge but also learn useful logi-\ncal patterns. Our pretrained KMLMs demon-\nstrate signiﬁcant performance improvements\non a wide range of knowledge-intensive cross-\nlingual tasks, including named entity recogni-\ntion (NER), factual knowledge retrieval, rela-\ntion classiﬁcation, and a newly designed logi-\ncal reasoning task.1\n1 Introduction\nPretrained Language Models (PLMs) such as\nBERT (Devlin et al., 2019) and RoBERTa (Liu\net al., 2019) have achieved superior performances\non a wide range of NLP tasks. Existing PLMs usu-\nally learn universal language representations from\ngeneral-purpose large-scale corpora but do not con-\ncentrate on capturing world’s factual knowledge. It\nhas been shown that knowledge graphs (KGs), such\n∗Linlin Liu is under the Joint PhD Program between\nAlibaba and Nanyang Technological University.\n†Corresponding author.\n1Our code, data and pretrained models are available at\nhttps://github.com/ntunlp/kmlm.git.\nas Wikidata (Vrandeˇci´c and Krötzsch, 2014) and\nFreebase (Bollacker et al., 2008), can provide rich\nfactual information for better language understand-\ning. Many studies have demonstrated the effec-\ntiveness of incorporating such factual knowledge\ninto monolingual PLMs (Peters et al., 2019; Zhang\net al., 2019; Liu et al., 2020a; Poerner et al., 2020;\nWang et al., 2021a). Following this, a few recent\nattempts have been made to enhance multilingual\nPLMs with Wikipedia or KG triples (Calixto et al.,\n2021; Ri et al., 2022; Jiang et al., 2022). However,\ndue to the structural difference between KG and\ntexts, existing KG based pretraining often relies on\nextra relation/entity embeddings or additional KG\nencoders for knowledge enhancement. These ex-\ntra embeddings/components may add signiﬁcantly\nmore parameters which in turn increase inference\ncomplexity, or cause inconsistency between pre-\ntrain and downstream tasks. For example, mLUKE\n(Ri et al., 2022) has to enumerate all possible en-\ntity spans for NER to minimize the inconsistency\ncaused by entity and entity position embeddings.\nOther methods (Liu et al., 2020a; Jiang et al., 2022)\nalso require KG triples to be combined with rele-\nvant natural sentences as model input during train-\ning or inference.\nIn this work, we propose KMLM, a novel\nKnowledge-based Multilingual Language Model\npretrained on massive multilingual KG triples. Un-\nlike prior knowledge enhanced models (Zhang\net al., 2019; Peters et al., 2019; Liu et al., 2020a;\nWang et al., 2021a), our model requires neither a\nseparate encoder to encode entities/relations, nor\nheterogeneous information fusion to fuse multiple\ntypes of embeddings (e.g., entities from KGs and\nwords from sentences). The key idea of our method\nis to convert the structured knowledge from KGs\nto sequential data which can be directly fed as in-\nput to the LM during pretraining. Speciﬁcally, we\ngenerate three types of training data – the parallel\nknowledge data, the code-switched knowledge data\narXiv:2111.10962v4  [cs.CL]  19 Oct 2022\nand the reasoning-based data. The ﬁrst two are\nobtained by generating parallel or code-switched\nsentences from triples of Wikidata (Vrande ˇci´c and\nKrötzsch, 2014), a collaboratively edited multilin-\ngual KG. The reasoning-based data, containing rich\nlogical patterns, is constructed by converting cy-\ncles from Wikidata into word sequences in different\nlanguages. We then design pretraining tasks that\nare operated on the parallel/code-switched data to\nmemorize the factual knowledge across languages,\nand on the reasoning-based data to learn the logical\npatterns.\nCompared to existing knowledge-enhanced pre-\ntraining methods (Zhang et al., 2019; Liu et al.,\n2020a; Peters et al., 2019; Jiang et al., 2022),\nKMLM has the following key advantages. (1)\nKMLM is explicitly trained to derive new knowl-\nedge through logical reasoning. Therefore, in addi-\ntion to memorizing knowledge facts, it also learns\nthe logical patterns from the data. (2) KMLM does\nnot require a separate encoder for KG encoding,\nand eliminates relation/entity embeddings, which\nenables KMLM to be trained on a larger set of\nentities and relations without adding extra parame-\nters. The token embeddings are enhanced directly\nwith knowledge related training data. (3) KMLM\ndoes not rely on any entity linker to link the text to\nthe corresponding KG entities, as done in existing\nmethods (Zhang et al., 2019; Peters et al., 2019;\nPoerner et al., 2020). This ensures KMLM to uti-\nlize more KG triples even if they are not linked\nto any text data, and avoids noise caused by incor-\nrect links. (4) KMLM keeps the model structure\nof the multilingual PLM without introducing any\nadditional component during both training and in-\nference stages. This makes the training much eas-\nier, and the trained model is directly applicable to\ndownstream NLP tasks.\nWe evaluate KMLM on a wide range of\nknowledge-intensive cross-lingual tasks, including\nNER, factual knowledge retrieval, relation classi-\nﬁcation, and logical reasoning which is a novel\ntask designed by us to test the reasoning capability\nof the models. Our KMLM achieves consistent\nand signiﬁcant improvements on all knowledge-\nintensive tasks, meanwhile it does not sacriﬁce the\nperformance on general NLP tasks.\n2 Related Work\nKnowledge-enhanced language modeling aims to\nincorporate knowledge, concepts and relations into\nthe PLMs (Devlin et al., 2019; Liu et al., 2019;\nBrown et al., 2020), which proved to be beneﬁcial\nto language understanding (Talmor et al., 2020a).\nThe existing approaches mainly focus on mono-\nlingual PLMs, which can be roughly divided into\ntwo lines: implicit knowledge modeling and ex-\nplicit knowledge injection. Previous attempts on\nimplicit knowledge modeling usually consist of\nentity-level masked language modeling (Sun et al.,\n2019; Liu et al., 2020a), entity-based replacement\nprediction (Xiong et al., 2020), knowledge embed-\nding loss as regularization (Wang et al., 2021b)\nand universal knowledge-text prediction (Sun et al.,\n2021). In contrast to implicit knowledge modeling,\nthe methods of explicit knowledge injection sepa-\nrately maintain a group of parameters for represent-\ning structural knowledge. Such methods (Zhang\net al., 2019) usually require a heterogeneous infor-\nmation fusion component to fuse multiple types\nof embeddings obtained from the text and KGs.\nZhang et al. (2019) and Poerner et al. (2020) em-\nploy external entity linker to discover the entities in\nthe text and perform feature interaction between the\ntoken embeddings and entity embeddings during\nthe encoding phase of a transformer model. Peters\net al. (2019) borrow the pre-computed knowledge\nembeddings as the supporting features of training\nan internal entity linker. Wang et al. (2021a) insert\nan adapter component in each transformer layer to\nstore the learned factual knowledge.\nExtending knowledge based pretraining methods\nto the multilingual setting has received increasing\ninterest recently. Zhou et al. (2022) propose an\nauto-regressive model trained on knowledge triples\nfor multilingual KG completion. Calixto et al.\n(2021); Ri et al. (2022) attempt improving mul-\ntilingual entity representation via Wikipedia hyper-\nlink prediction, however, their methods add a large\namount of parameters due to the reliance on extra\nentity embeddings. For example, the mLUKEBASE\n(Ri et al., 2022) model initialized with XLM-RBASE\ndoubles the number of parameters (586M vs 270M).\nSimilar to us, Jiang et al. (2022) also utilize KG for\nPLM pretraining. They employ KG and Wikipedia\nentity descriptions to inject knowledge into multi-\nlingual LM, but relation embeddings are also re-\nquired to assist learning.\nMoreover, the above methods only focus on\nmemorizing the existing facts but ignore the rea-\nsoning over the unseen/implicit knowledge that\nis derivable from the existing facts. Such rea-\nID Language Label Aliases\nQ1420\nEnglish motor car auto, autocar, . . .\nSpanish automóvil coche, carro, . . .\nHungarian autó gépkocsi, személyautó, . . .\n. . . . . . . . .\nTable 1: An example ( https://www.wikidata.org/\nwiki/Q1420) of the Wikidata entity labels and aliases\nin multiple languages. Q1420 is the unique entity ID.\nsoning capability is regarded as a crucial part of\nbuilding consistent and controllable knowledge-\nbased models (Talmor et al., 2020b). In this paper,\nour explored methods for multilingual knowledge-\nenhanced pretraining boost the capability of im-\nplicit knowledge reasoning, together with the pur-\npose of consolidating knowledge modeling and\nmultilingual pretraining (Mulcaire et al., 2019;\nConneau et al., 2020).\n3 Framework\nIn this section, we describe the proposed frame-\nwork for knowledge based multilingual language\nmodel (KMLM) pretraining. We ﬁrst describe the\nprocess to generate knowledge-intensive multilin-\ngual training data, followed by the pretraining tasks\nto train the language models to memorize factual\nknowledge and learn logical patterns from the gen-\nerated data.\n3.1 Knowledge Intensive Training Data\nIn addition to the large-scale plain text corpus that\nis commonly used for language model pretraining,\nwe also generate a large amount of knowledge in-\ntensive training data from Wikidata (Vrandeˇci´c and\nKrötzsch, 2014), a publicly accessible knowledge\nbase edited collaboratively. Wikidata is composed\nof massive amounts of KG triples (h,r,t), where\nhand tare the head and tail entities respectively, r\nis the relation type. As shown in Table 1, most of\nthe entities, as well as the relations in Wikidata, are\nannotated in multiple languages. In each language,\nmany aliases are also given though some of them\nare used infrequently.\nCode-Switched Synthetic Sentences Training\nlanguage models on high-quality code-switched\nsentences is one of the most intuitive ways to learn\nlanguage agnostic representation (Winata et al.,\n2019), where the translations of words/phrases can\nbe treated in a similar way as their aliases. The code\nmixing techniques have also proved to be helpful\nOriginal (en):\n(motor car, designed to carry, passenger)\nCode-Switched (en-fr):\nmotor car [mask]conçu pour transporter[mask] passenger.\nmotor car [mask] designed to carry [mask]passager.\nCode-Switched (en-fr) & Alias-Replaced:\nautomobile[mask]destiné au transport[mask] passenger.\nmotor car [mask] intended to carry[mask]passager.\nParallel (en-fr) & Alias-Replaced:\nautocar[mask] designed to carry [mask] passenger.\nvoiture[mask]conçu pour transporter[mask]passager.\nFigure 1: Examples of the en-fr code-switched and\nparallel synthetic sentences. The words replaced with\ntranslations or aliases are marked with bold font and\nunderline, respectively.\nfor improving cross-lingual transfer performance\nin many NLP tasks (Qin et al., 2020; Santy et al.,\n2021). Therefore, we propose a novel method to\ngenerate code-switched synthetic sentences using\nthe multilingual KG triples. See Fig. 1 for some\ngenerated examples.\nFor each triple (h,r,t) in Wikidata, we use hl,0\nto denote the default label of hin language l. For\nthe entity Q1420 in Table 1, hen,0 is “motor car”\nand hes,0 is “automóvi”. hl,i denotes the aliases\nwhen the integer i > 0. We deﬁne rl,i and tl,i\nin the same way for the relation and the tail en-\ntity, respectively. Since English is resource-rich\nand often treated as the source language for cross-\nlingual transfer, we only consider language pairs\nof {(en,l′)} for code switching, where l′is an ar-\nbitrary non-English language. With such a design,\nEnglish can also work as a bridge for cross-lingual\ntransfer between a pair of none English languages.\nSpeciﬁcally, the code-switched sentences for\n(h,r,t) can be generated in 4 steps: 1) select\na language pair (en,l′); 2) ﬁnd the English de-\nfault labels (hen,0,ren,0,ten,0); 3) For each item\nin the triple, uniformly sample a value v ∈\n{true,false}, if vis trueand the item has a trans-\nlation (i.e. default label) in l′, then replace the item\nwith the translation in l′; 4) generate the sequence\nof “h [mask] r [mask] t. ” by inserting two mask\ntokens. The alias-replaced sentences can be gen-\nerated in a similar way, except that we randomly\nsample aliases in the desired language to replace\nthe default label in steps 2 and 3.\nParallel Synthetic Sentences Parallel data has\nalso been widely exploited to improve cross-lingual\ntransfer (Aharoni et al., 2019; Conneau and Lam-\nPresident of the\nUnited States\nWhite House Barack Obamaresidence\nofficial residence position held\n(a) Cycle of length 3.\nRitu Nanda\nRajiv Kapoor Krishna Kapoormother\nfather\nsibling\nRaj Kapoor\nspousemother (b) Cycle of length 4.\nFigure 2: Examples of extracted cycles of length 3 and\n4.\nple, 2019; Chi et al., 2021). However, it is expen-\nsive to obtain a large amount of parallel data for\nLM pretraining. We propose a method to gener-\nate a large amount of knowledge intensive parallel\nsynthetic sentences, with a minor modiﬁcation of\nthe method for generating code-switched sentences\ndescribed above. For each triple (h,r,t) extracted\nfrom Wikidata, the corresponding synthetic sen-\ntences in different languages can be generated by\nﬁrst ﬁnding the default labels (hl,0,rl,0,tl,0) for\neach language l, and then inserting mask tokens to\ngenerate sequences in the form “h [mask] r [mask]\nt. ”. Fig. 1 shows an example. More sentences can\nbe generated by replacing the default labels with\ntheir aliases.\nReasoning-Based Training Data The capabil-\nity of logical reasoning allows humans to solve\ncomplex problems with limited information. How-\never, this ability did not receive much attention in\nthe previous LM pretraining methods. In KGs, we\ncan use nodes to represent entities, and edges be-\ntween any two nodes to represent their relations. In\norder to train the model to learn logical patterns, we\ngenerate a large amount of reasoning-based train-\ning data by ﬁnding cycles from the Wikidata KG.\nAs shown with an example in Fig. 2(a), the cycles\nof length 3 can be viewed as the basic component\nfor more complex logical reasoning process. We\ntrain language models to learn the entity-relation\nco-occurrence patterns so as to infer the best candi-\ndate relations for incomplete cycles, i.e. deriving\nthe implicit information from the given context.\nSimilar to the structure of the parallel/code-\nswitched synthetic sentences described above, the\ncycles in Fig. 2(a) is composed of 3 triples, and\nhence can be converted to 3 synthetic sentences\n(the ﬁrst example in Fig. 4). To increase the difﬁ-\nculty, we also extract cycles of length 4 to generate\nthe reasoning oriented training data. However, we\nﬁnd that simply increasing the length of cycles\nTransformers x N\n[mask]    car     [mask]     [mask]     to    carry     [mask]   passager    .\nmotor designed\nCross Entropy Loss\nFigure 3: MLM on the code-switched synthetic sen-\ntence “motor car [mask] designed to carry [mask] pas-\nsager. ”. The cross entropy loss LK for Knowledge\nOriented Pretraining is only computed over the ran-\ndomly masked entity and relation tokens highlighted in\nlime green. For simplicity, the sub-word tokens are not\nshown in this example.\nmakes the samples less logically coherent. Thus,\nwe add an extra constraint that each length-4 cycle\nis required to have at least one additional diagonal\nedge. Fig. 2(b) shows such an example. It can\nbe converted to a training sample of 5 sentences\nin the same way as above. For the multilingual\nreasoning-based data, we only generate monolin-\ngual sentences, i.e. without applying code mixing.\nWe treat Wikidata as an undirected graph when\nextracting cycles. Given an entity, the length-3 cy-\ncles containing this entity can be easily extracted\nby ﬁrst ﬁnding all the neighbouring entities, and\nthen iterating through the pairs of neighbouring en-\ntities to check whether they are also connected. The\nlength-4 cycles with an additional diagonal edge\nconnecting any two neighbours can be extracted\nwith a few extra steps. Assuming we have iden-\ntiﬁed a length-3 cycle containing entity Aand its\ntwo neighbouring entities Band C, we can iterate\nthrough the neighbours of B(excluding Aand C)\nto check whether it is also connected to C. We\nremove the duplicate cycles in data generation.\n3.2 Pretraining Tasks\nMultilingual Knowledge Oriented Pretraining\nIn the generated code-switched and parallel syn-\nthetic sentences, the “[mask]” tokens are added\nbetween entities and relations to denote the linking\nwords. For example, the ﬁrst mask token in “mo-\ntor car [mask] designed to carry [mask] passager. ”\nmay denote “is”, while the second one may denote\n“certains” (French word “certains” means “some” or\n“certain”). Since the ground truth of such masked\nlinking words are not known, we do not compute\nthe loss for those corresponding predictions. In-\nstead, we randomly mask the remaining tokens in\nthe parallel/code-switched synthetic sentence, and\ncompute the cross entropy loss over these masked\nentity and relation tokens (Fig. 3). We use LK\nto denote this cross entropy loss for Knowledge\nOriented Pretraining. Note that our models are\nnot trained on the sentence pairs like the Transla-\ntion LM loss or TLM (Conneau and Lample, 2019)\nwhen utilizing the parallel or code-switched pairs.\nAlternatively, we shufﬂe the data, and feed one sen-\ntence into the model each time (as shown in Fig. 3),\nwhich makes our model inputs more consistent\nwith those of the downstream tasks.\nLogical Reasoning Oriented Pretraining We\ndesign tasks to train the model to learn logical rea-\nsoning patterns from the synthetic sentences gener-\nated from the length-3 and length-4 cycles. As can\nbe seen in Fig. 4, both of the relation prediction and\nentity prediction problems are cast as masked lan-\nguage modeling. For the length-3 cycles, each en-\ntity appears exactly twice in every training sample.\nFormulating the task as a masked entity prediction\nproblem may lead to shortcut learning (Geirhos\net al., 2020) by simply counting the appearance\nnumbers of the entities. Therefore, we only mask\none random relation in each sample for model train-\ning, and let the model learn to predict the masked\nrelation tokens based on the context.\nTwo types of tasks are designed to train the\nmodel to learn reasoning with the length-4 cycles:\n1) For 80% of the time, we train the model to pre-\ndict randomly masked relation and entities. We\nﬁrst mask one random relation. To increase the\ndifﬁculty, we also mask one or two randomly se-\nlected entities at equal chance. The lower half of\nFig. 4 shows an example where one relation and\none entity are masked. 2) For the remaining 20%\nof the time, we randomly mask a whole sentence to\nlet the model learn to derive new knowledge from\nthe remaining context. To provide some hints on\nthe expected new knowledge, we keep the relation\nof the selected sentence unmasked, i.e., only mask\nits two entities. The loss LL for Logical Reasoning\nOriented Pretraining can also be computed with the\ncross entropy loss over the masked tokens. Note\nthat masked entity prediction is not always non-\ntrivial in this task. For example, when we mask\nexactly one entity and the entity E only appears\nonce in the masked sample, then it is easy to guess\nEis the masked one. In Fig. 4, a concrete example\nis masking the ﬁrst appearance of “Raj Kapoor” in\nthe original sentence of the length-4 cycle. We do\nnot deliberately avoid such cases, since they may\nhelp introduce more diversity to the training data.\nLoss Function In addition to the pretraining\ntasks designed above, we also train the model on\nthe plain text data with the original masked lan-\nguage modeling loss LMLM used in previous work\n(Devlin et al., 2019; Conneau et al., 2020). There-\nfore, the ﬁnal loss can be computed as:\nL = LMLM + α(LK + LL) (1)\nwhere αis a hyper-parameter to adjust the weights\nof the original MLM and the losses for modeling\nthe multilingual knowledge and logical reasoning.\n4 Experiments\nWe ﬁrst describe the pretraining details of our\nKMLMs. Then we verify its effectiveness on the\nknowledge-intensive tasks. Finally, we examine its\nperformance on general cross-lingual tasks. In all\nof the tasks except X-FACTR (Jiang et al., 2020),\nthe PLMs are ﬁne-tuned on the English training\nset and then evaluated on the target language test\nsets. The evaluation results are averaged over 3\nruns with different random seeds. X-FACTR does\nnot require ﬁne-tuning, so the PLMs are directly\nevaluated using the ofﬁcial code. The results of\nthe baseline models are reproduced in the same\nenvironment.\n4.1 Pretraining Details\nOur proposed framework can be conveniently im-\nplemented on top of the existing transformer en-\ncoder based models like mBERT (Devlin et al.,\n2019) and XLM-R (Conneau et al., 2020) without\nany modiﬁcation to the model structure. There-\nfore, instead of pretraining the model from scratch,\nit is more time- and cost-efﬁcient to initialize the\nmodel with the checkpoints of existing pretrained\nmodels. We build our knowledge intensive training\ndata in 10 languages: English, Vietnamese, Dutch,\nGerman, French, Italian, Spanish, Japanese, Ko-\nrean and Chinese. We only use the 5M entities\nand 822 relations ﬁltered by Wang et al. (2021b),\nand generate 250M code-switched synthetic sen-\ntences, 190M parallel synthetic sentences 2 and\n100M reasoning-based samples following the steps\n2Half of the parallel and code-switched sentences are also\nalias-replaced. The size of the generated parallel data is\nsmaller than the code-switched one because some of the enti-\nties/relations do not have annotation in the target language.\nLength-3 Cycle:Original: President of the United States [mask] ofﬁcial residence [mask] White House. Barack Obama [mask] residence [mask] White House.Barack Obama [mask] position held [mask] President of the United States.\nMasked: President of the United States [mask] ofﬁcial residence [mask] White House. Barack Obama [mask] [mask] [mask] White House. BarackObama [mask] position held [mask] President of the United States.\nLength-4 Cycle:Original: Ritu Nanda [mask] father [mask] Raj Kapoor. Ritu Nanda [mask] mother [mask] Krishna Kapoor. Rajiv Kapoor [mask] mother [mask]Krishna Kapoor. Rajiv Kapoor [mask] sibling [mask] Ritu Nanda. Raj Kapoor [mask] spouse [mask] Krishna Kapoor.\nMasked: Ritu Nanda [mask] father [mask] Raj Kapoor. Ritu Nanda [mask] mother [mask] [mask] [mask]. Rajiv Kapoor [mask] mother [mask]Krishna Kapoor. Rajiv Kapoor [mask] [mask] [mask] Ritu Nanda. Raj Kapoor [mask] spouse [mask] Krishna Kapoor.\nFigure 4: Examples of the masked training samples for logical reasoning. The relations are highlighted in orange.\nThe masked entity and relation tokens are highlighted in lime green.\nin §3.1. In addition, 260M sentences are sampled\nfrom the CC100 corpus 3 (Wenzek et al., 2020)\nfor the 10 languages. Our models KMLM-XLM-\nRBASE and KMLM-XLM-RLARGE are initialized\nwith XLM-RBASE and XLM-RLARGE, respectively.\nThen we continue to pretrain these models with the\nproposed tasks (§3.2). KMLM CS, KMLMParallel\nand KMLMMix are used to differentiate the models\ntrained on the code-switched data, parallel data and\nthe concatenated data of these two, respectively.\nThe reasoning-based data is used in all these three\nmodels, and ablation studies are presented in §4.5\nto verify the effectiveness of logical reasoning task.\nPrevious studies showed that the original\nmBERT model outperforms XLM-R on the X-\nFACTR (Jiang et al., 2020) and RELX (Köksal and\nÖzgür, 2020) tasks, so we also initialize KMLM-\nmBERTBASE with mBERTBASE, and train it on\nWikipedia corpus for a more faithful comparison4.\nWe ﬁnd the KMLMCS and KMLMMix models ini-\ntialized with the XLM-R BASE checkpoint outper-\nform the corresponding KMLM Parallel model in\nmost of the tasks, so we only train KMLMCS and\nKMLMMix when comparing with XLM-R LARGE\nand mBERTBASE. See Appendix §A.1 for more\npretraining details.\n4.2 Cross-lingual Named Entity Recognition\nNamed entity recognition (NER) involves identi-\nfying and classifying named entities from unstruc-\ntured text data. The elimination of entity/relation\nembeddings allows our models to be trained di-\nrectly on a larger amount of entities without adding\nextra parameters or increasing computation cost.\nDirect training on entity-intensive synthetic sen-\ntences may also help improving entity representa-\ntion more efﬁciently. We conduct experiments on\n3http://data.statmt.org/cc-100/\n4The original mBERTBASE is trained using the Wikipedia.\nen de nl es avg tgt ∆avg\nmBERTBASE† 90.6 69.2 77.9 75.4 74.2 -XLM-K‡ 90.7 72.9 80.3 75.2 76.1 -XLM-RBASE 91.16 68.87 79.0076.7074.86 0KMLMCS-XLM-RBASE(ours) 91.4773.52 80.95 76.59 77.02 +2.16KMLMParallel-XLM-RBASE(ours) 91.4473.9681.06 75.94 76.99 +2.13KMLMMix-XLM-RBASE(ours) 91.38 73.9581.2976.1777.14+2.28XLM-RLARGE 92.98 73.79 82.00 79.33 78.37 0KMLMCS-XLM-RLARGE(ours) 92.81 76.2284.1278.63 79.66 +1.29KMLMMix-XLM-RLARGE(ours)93.12 76.8882.8480.08 79.93+1.56\nTable 2: Zero-shot cross-lingual NER F1 on the\nCoNLL02/03 datasets. The average results of non-\nEnglish languages are reported in column avgtgt. † The\nresults are from (Liang et al., 2020). ‡ The results are\nfrom (Jiang et al., 2022).\nthe CoNLL02/03 (Tjong Kim Sang, 2002; Tjong\nKim Sang and De Meulder, 2003) and WikiAnn\n(Pan et al., 2017) NER data to verify the effec-\ntiveness of our framework. The same transformer-\nbased NER model and hyper-parameters as Hu et al.\n(2020) are used in our experiments.\nThe results on CoNLL02/03 data are presented\nin Table 2. Compared with XLM-RBASE, all of our\ncorresponding models improve the average F1 on\ntarget languages by more than 2.13 points. Espe-\ncially on German, all of our models demonstrate at\nleast 4.65 absolute gains. Moreover, all of our mod-\nels also outperform XLM-K (Jiang et al., 2022), a\nknowledge-enhanced multilingual LM proposed in\na recent work. Even when compared with XLM-\nRLARGE, our large model still improves the average\nperformance by 1.56. The WikiAnn dataset al-\nlows us to evaluate our models on all of the 10 lan-\nguages involved in pretraining. Jiang et al. (2022)\ndid not report XLM-K results on WikiAnn, so we\nevaluate their pretrained model on WikiAnn and\nthe following knowledge intensive tasks for better\ncomparison. As the results shown in Table 3, our\nbest base and large models outperform the corre-\nsponding XLM-R models by 2.64 and 1.60 respec-\ntively. From both datasets we observe KMLMCS-\nXLM-RBASE performs better than KMLM Parallel-\nen vi nl de fr it es ja ko zh avg tgt ∆avg\nXLM-K 83.32 72.80 81.39 76.96 78.75 78.81 71.60 17.14 57.75 19.68 61.65 -\nXLM-RBASE 82.59 68.09 80.08 74.71 76.50 77.06 71.05 20.34 48.46 26.32 60.29 0\nKMLMCS-XLM-RBASE(ours) 83.43 70.55 82.18 77.87 79.19 80.06 75.96 19.32 57.54 20.95 62.62 +2.33\nKMLMParallel-XLM-RBASE(ours) 83.54 70.93 82.30 77.79 78.40 79.83 76.06 18.16 57.40 20.44 62.37 +2.08\nKMLMMix-XLM-RBASE(ours) 83.42 70.24 82.22 77.30 79.93 80.03 76.72 20.78 56.70 22.49 62.93 +2.64\nXLM-RLARGE 84.34 77.61 83.72 78.92 79.93 81.24 73.59 18.94 59.27 28.35 64.62 0\nKMLMCS-XLM-RLARGE(ours) 85.07 77.89 84.55 81.32 83.65 82.57 78.93 14.95 60.68 19.43 64.89 +0.27\nKMLMMix-XLM-RLARGE(ours) 84.87 77.99 84.62 81.13 82.85 82.28 77.30 21.22 61.88 26.69 66.22 +1.60\nTable 3: Zero-shot cross-lingual NER F1 on the WikiAnn dataset.\nXLM-RBASE, which shows the efﬁcacy of the code-\nswitching technique for large-scale cross-lingual\npretraining. Moreover, both KMLM Mix-XLM-\nRBASE and KMLMMix-XLM-RLARGE (i.e. the mod-\nels pretrained on the mixed code-switched and par-\nallel data) surpass all of the compared models in\nterms of F1, suggesting that the mixed data can\nhelp further generalize the representations across\nlanguages.\n4.3 Factual Knowledge Retrieval\nX-FACTR (Jiang et al., 2020) is a benchmark for\nassessing the capability of multilingual pretrained\nlanguage model on capturing factual knowledge.\nIt provides multilingual cloze-style question tem-\nplates and the underlying idea is to query knowl-\nedge from the models for ﬁlling in the blank of\nthese question templates. From (Jiang et al., 2020),\nwe notice the performance of XLM-RBASE is much\nworse than mBERTBASE (see Table 4). It is prob-\nably because mBERTBASE has a much smaller vo-\ncabulary than XLM-R (120k vs 250k) and em-\nploys Wikipedia corpus instead of the general data\ncrawled from the Internet. So we also pretrain\nKMLMCS-mBERTBASE for more comprehensive\ncomparison. As we can see from Table 4, all of the\nmodels trained with our framework demonstrate\nsigniﬁcant improvements on factual knowledge re-\ntrieval accuracy, which again indicates the bene-\nﬁts of our method on factual knowledge acquisi-\ntion. Our model still demonstrates better perfor-\nmance than XLM-K, though it is also trained using\nWikipedia.\n4.4 Cross-lingual Relation Classiﬁcation\nRELX (Köksal and Özgür, 2020) is developed by\nselecting a subset of KBP-37 (Zhang and Wang,\n2015), a commonly-used English relation classiﬁ-\ncation dataset, and by generating human transla-\ntions and annotations in French, German, Spanish,\nand Turkish. We evaluate the same set of mod-\nContext:(Poland, located in time zone, UTC+01:00)(Poland, located in time zone, Central European Time)\nQuestion:What is the relation between UTC+01:00 and Central European Time?\nChoices:part of, said to be the same as, located in time zone, instance of, has part, followed by\nAnswer:said to be the same as\nFigure 5: An example (English) extracted from our\ncross-lingual logical reasoning (XLR) dataset.\nels as §4.3, since mBERT BASE also outperforms\nXLM-RBASE on this task. The evaluation script\nprovided by Köksal and Özgür (2020) is used to\nﬁnetune the pretrained models on English training\nset and evaluate on the target language test sets.\nAs the results shown in Table 5, all of our models\nachieves consistently higher accuracy than XLM-K\nand XLM-R.\n4.5 Cross-lingual Logical Reasoning\nDataset To verify the effectiveness of our logical\nreasoning oriented pretraining tasks (§3.2) in an in-\ntrinsic way, we propose a cross-lingual logical rea-\nsoning (XLR) task in the form of multiple-choice\nquestions. An example of such reasoning question\nis given in Fig. 5. The dataset is constructed using\nthe cycles extracted from Wikidata. We manually\nannotate 1,050 samples in English and then trans-\nlated them to the other 9 non-English languages\n(see Sec. 4.1) to build the multilingual test sets.\nThe 3k train samples and 1k dev samples in En-\nglish are also generated and cleaned automatically.\nThe cycles used to build the test set are removed\nfrom the pretraining data, so our PLMs have never\nseen them beforehand. The detailed dataset con-\nstruction steps can be found in Appendix §A.2.\nen es fr nl ja ko vi zh avg\nXLM-K 7.7 7.3 3.6 5.0 0.3 4.0 5.0 0.9 4.2\nXLM-RBASE 4.5 3.1 2.0 1.6 1.8 2.1 3.6 1.0 2.5\nKMLMCS-XLM-RBASE(ours) 8.6 4.8 4.2 5.6 1.6 4.2 5.8 3.0 4.7\nKMLMParallel-XLM-RBASE(ours) 8.1 5.4 3.7 6.1 1.6 4.7 6.3 2.4 4.9\nKMLMMix-XLM-RBASE(ours) 7.9 5.1 4.8 6.1 1.7 4.8 6.2 3.1 5.0\nXLM-RLARGE 7.9 4.4 3.8 5.0 2.9 5.2 5.7 1.0 4.5\nKMLMCS-XLM-RLARGE(ours) 10.5 5.5 6.9 7.1 1.1 6.7 5.7 1.6 5.6\nKMLMMix-XLM-RLARGE(ours) 11.1 5.8 7.3 7.7 1.4 7.1 6 3.8 6.3\nmBERTBASE 8.4 8.7 5.5 8.6 1.0 2.0 4.7 4.5 5.4\nKMLMCS-mBERTBASE(ours) 13.0 10.9 8.5 11.8 2.0 3.2 10.1 10.7 8.8\nKMLMMix-mBERTBASE(ours) 12.5 11.3 8.7 11.7 2.2 3 9.9 11.6 8.9\nTable 4: Factual knowledge retrieval results (acc., %) on X-FACTR.\nen es de fr avg tgt ∆avg\nXLM-K 59.1 58.355.9 56.4 57.4 -XLM-RBASE 62.755.1 54.8 54.3 54.7 0KMLMCS-XLM-RBASE(ours) 61.2 57.9 56.6 55.9 57.9 +3.2KMLMParallel-XLM-RBASE(ours) 62.6 56.656.955.0 57.8 +3.1KMLMMix-XLM-RBASE(ours) 61.6 56.8 57 58.4 58.5 +3.8XLMRLARGE 62.8 62.6 60.4 59.5 60.8 0KMLMCS-XLM-RLARGE(ours) 63.563.760.1 60.461.4 +0.6KMLMMix-XLM-RLARGE(ours)63.661.761.8 60.7 61.4+0.6mBERTBASE 65.858.9 58.5 58.2 58.5 0KMLMCS-mBERTBASE(ours) 64.2 59.559.1 61.7 60.1+1.6KMLMMix-mBERTBASE(ours) 60.961.357.8 60.3 59.8 +1.3\nTable 5: Zero-shot cross-lingual relation classiﬁcation\nperformance (acc., %) on RELX.\nResults We modify the multiple choice evalu-\nation script implemented by Hugging Face 5 for\nthis experiment. The models are ﬁnetuned on\nthe English training set, and evaluated on the\ntest sets in different target languages. Results\nare presented in Table 6. All of our models out-\nperform the baselines signiﬁcantly. Unlike on\nthe previous tasks, where KMLM Mix often per-\nforms the best, KMLM CS shows slightly higher\naccuracy than KMLM Mix. We also conduct ab-\nlation study to verify the effectiveness of our\nproposed logical reasoning oriented pretraining\ntask. We pretrain the None-Reasoning mod-\nels, KMLMCS-NR-XLM-RBASE and KMLMMix-NR-\nXLM-RBASE on the same data as KMLMCS-XLM-\nRBASE and KMLMMix-XLM-RBASE, but without\nthe logical reasoning tasks, i.e., with the MLM\ntask only on the reasoning-based data. As pre-\nsented in Fig. 6, the none-reasoning models also\nperforms better than XLM-RBASE, which shows the\nusefulness of our reasoning-based data. We also\nobserve KMLMCS-XLM-RBASE and KMLMMix-\nXLM-RBASE, i.e., the models pretrained with logi-\ncal reasoning tasks, consistently perform the best,\nwhich proves our proposed task can help models\nlearn logical patterns more efﬁciently.\n5https://github.com/huggingface/transformers/\ntree/master/examples/pytorch/multiple-choice\nen de es fr it ja ko nl vi zh\nLanguage\n0\n10\n20\n30\n40\n50\n60\n70\n80Accuracy\nKMLMCS-XLM-RBASE\nKMLMCS NR-XLM-RBASE\nXLM-RBASE\nen de es fr it ja ko nl vi zh\nLanguage\n0\n10\n20\n30\n40\n50\n60\n70\n80Accuracy\nKMLMMix-XLM-RBASE\nKMLMMix NR-XLM-RBASE\nXLM-RBASE\nFigure 6: Comparison of the models trained with and\nwithout logical reasoning task.\n4.6 General Cross-lingual Tasks\nRecall that our models are directly trained on the\nstructured KG data. Though we attempt to mini-\nmize its difference from the natural sentences when\ndesigning the pretraining tasks, it is unknown how\nthe difference affects cross-lingual transfer per-\nformance on the general NLP tasks. Therefore,\nwe also evaluate our models on the part-of-speech\n(POS) tagging, question answering and classiﬁca-\ntion tasks prepared by XTREME (Hu et al., 2020).\nExperimental results are shown in Table 7. Note\nthat many of the languages covered by these tasks\nare not in our pretraining data, but we include\nall their results when computing the average per-\nformance. Overall, the performance of our mod-\nels is comparable with the baselines on all of the\ntasks, except POS. Possibly because the POS task\nis more sensitive to the change of the training sen-\ntence structures. Though some of our models per-\nform slightly better than the baselines on XQuAD\n(Artetxe et al., 2020) and MLQA (Lewis et al.,\n2020), we ﬁnd the performance gain of our mod-\nels on TyDiQA6 (Clark et al., 2020) is more ob-\nvious, which is a more challenging QA task that\n6Same as XTREME, we use the gold passage version of\nTyDiQA.\nen de es fr it ja ko nl vi zh avg tgt ∆avg\nXLM-K 58.29 44.19 44.41 45.14 41.02 25.94 34.32 46.92 38.19 29.81 38.88 -\nXLM-RBASE 64.38 46.38 49.36 45.46 46.38 21.58 32.53 54.06 39.11 27.80 40.30 0\nKMLMCS-XLM-RBASE(ours) 71.52 63.52 64.7366.1963.6837.04 43.3963.42 55.1745.77 55.88+15.58\nKMLMParallel-XLM-RBASE(ours) 70.03 60.48 61.78 62.38 62.95 35.21 44.76 62.0357.7542.60 54.44 +14.14\nKMLMMix-XLM-RBASE(ours) 72.70 63.71 66.4165.0564.9836.16 38.7964.8356.51 44.73 55.69 +15.39\nXLM-RLARGE 79.39 68.38 73.46 71.20 70.82 56.25 47.61 70.98 66.00 55.11 64.42 0\nKMLMCS-XLM-RLARGE(ours) 87.07 85.8783.5886.03 83.8075.04 75.39 85.0183.58 83.74 82.45+18.03\nKMLMMix-XLM-RLARGE(ours) 86.67 84.2583.7585.14 82.8976.03 77.30 85.3082.86 82.57 82.23 +17.81\nTable 6: Zero-shot cross-lingual logical reasoning performance (acc., %).\nPOS XQuAD MLQA TyDiQA XNLI PAWSXMetrics F1 F1/EM F1/EM F1/EM Acc. Acc.XLM-RBASE 72.8 69.6/53.964.9/47.2 45.2/28.573.7 84.8KMLMCS-XLM-RBASE(ours) 71.2 69.2/53.3 64.7/47.0 48.7/29.2 73.685.1KMLMParallel-XLM-RBASE(ours) 71.2 69.3/53.4 64.7/46.851.4/32.873.3 84.1KMLMMix-XLM-RBASE(ours) 71.4 69.5/53.365.4/47.349.6/32.6 72.9 84.7MMTE† 73.5 64.4/46.2 60.3/41.4 58.1/43.8 67.4 81.3mBERT†LARGE 70.3 64.5/49.4 61.4/44.2 59.7/43.0 65.4 81.9XLM-RLARGE 74.676.8/60.972.5/54.266.6/46.6 79.0 87.8KMLMCS-XLM-RLARGE(ours) 72.4 76.5/60.6 72.0/53.7 66.4/47.9 78.6 87.7KMLMMix-XLM-RLARGE(ours) 72.877.3/61.772.1/53.767.9/50.4 79.2 88.0\nTable 7: Zero-shot cross-lingual POS, QA and classi-\nﬁcation results. Note that the performance of the lan-\nguages not appearing in our prepared pretraining data\nare also counted. †The results are from (Hu et al.,\n2020).\nhas less lexical overlap between question-answer\npairs. From these results we can see that, when our\nKMLMs achieve consistent improvements on the\nknowledge-intensive tasks, as shown by the experi-\nmental results in the previous subsections, it does\nnot sacriﬁce the performance on the general NLP\ntasks.\n5 Conclusions\nIn this paper, we have presented a novel frame-\nwork for knowledge-based multilingual language\npretraining. Our approach ﬁrstly creates a syn-\nthetic multilingual corpus from the existing KG\nand then tailor-makes two pretraining tasks, mul-\ntilingual knowledge oriented pretraining and log-\nical reasoning oriented pretraining. These multi-\nlingual pretraining tasks not only facilitate factual\nknowledge memorization but also boost the capa-\nbility of implicit knowledge modeling. We evaluate\nthe proposed framework on a series of knowledge-\nintensive cross-lingual tasks and the comparison\nresults consistently demonstrate its effectiveness.\nLimitations\nThe KMLM models proposed in this work are pre-\ntrained on 10 languages in our experiments, so it is\nunclear whether scaling up to more languages will\nhelp further improve its performance on the down-\nstream tasks. Due to the high computation cost, we\nleave it for future work. Despite the promising per-\nformance improvement on the knowledge intensive\ntasks, we also observe that KMLM do not perform\nwell on the part-of-speech tagging tasks (§4.6). It\nis possibly caused by the large amount of synthetic\nsentences used in pretraining, where mask tokens\nare used to replace the linking words. In future, we\nwill explore efﬁcient ways to leverage pretrained\ndenoising models (Liu et al., 2020b) or graph-to-\nsequence models (Ammanabrolu and Riedl, 2021)\nto convert the synthetic sentences or knowledge\ntriples to the form more close to natural sentences.\nEthical Impact\nNeural models have achieved signiﬁcant success\nin many NLP tasks, especially for the popular lan-\nguages like English, Spanish, etc. However, neural\nmodels are data hungry, which poses challenges\nfor applying them to the low-resource languages\ndue to the limited NLP resources. In this work,\nwe propose methods to inject knowledge into the\nmultilingual pretrained language models, and en-\nhance their logical reasoning ability. Through ex-\ntensive experiments, our methods have been proven\neffective in a wide range of knowledge intensive\nmultilingual NLP tasks. Therefore, our proposed\nmethod could help overcome the resource barrier,\nand enable the advances in NLP to beneﬁt a wider\nrange of population.\nAcknowledgements\nThis research is partly supported by the Alibaba-\nNTU Singapore Joint Research Institute, Nanyang\nTechnological University. Linlin Liu would like to\nthank the support from Interdisciplinary Graduate\nSchool, Nanyang Technological University.\nReferences\nRoee Aharoni, Melvin Johnson, and Orhan Firat. 2019.\nMassively multilingual neural machine translation.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers) , pages\n3874–3884, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nPrithviraj Ammanabrolu and Mark Riedl. 2021. Learn-\ning knowledge graph-based world models of textual\nenvironments. In Advances in Neural Information\nProcessing Systems, volume 34, pages 3720–3731.\nCurran Associates, Inc.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 4623–4637, Online. Asso-\nciation for Computational Linguistics.\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim\nSturge, and Jamie Taylor. 2008. Freebase: a collab-\noratively created graph database for structuring hu-\nman knowledge. In Proceedings of the 2008 ACM\nSIGMOD international conference on Management\nof data, pages 1247–1250.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems,\npages 1877–1901.\nIacer Calixto, Alessandro Raganato, and Tommaso\nPasini. 2021. Wikipedia entities as rendezvous\nacross languages: Grounding multilingual language\nmodels by predicting Wikipedia hyperlinks. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n3651–3661, Online. Association for Computational\nLinguistics.\nZewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham\nSinghal, Wenhui Wang, Xia Song, Xian-Ling Mao,\nHeyan Huang, and Ming Zhou. 2021. InfoXLM: An\ninformation-theoretic framework for cross-lingual\nlanguage model pre-training. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3576–3588, On-\nline. Association for Computational Linguistics.\nJonathan H Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. Tydi qa: A benchmark\nfor information-seeking question answering in typo-\nlogically diverse languages. Transactions of the As-\nsociation for Computational Linguistics, 8:454–470.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. Advances in\nneural information processing systems, 32.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL-HLT , pages\n4171–4186.\nRobert Geirhos, Jörn-Henrik Jacobsen, Claudio\nMichaelis, Richard Zemel, Wieland Brendel,\nMatthias Bethge, and Felix A Wichmann. 2020.\nShortcut learning in deep neural networks. Nature\nMachine Intelligence, 2(11):665–673.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Graham\nNeubig, Orhan Firat, and Melvin Johnson. 2020.\nXtreme: A massively multilingual multi-task bench-\nmark for evaluating cross-lingual generalisation. In\nProceedings of ICML, pages 4411–4421.\nXiaoze Jiang, Yaobo Liang, Weizhu Chen, and Nan\nDuan. 2022. Xlm-k: Improving cross-lingual lan-\nguage model pre-training with multilingual knowl-\nedge. In AAAI 2022.\nZhengbao Jiang, Antonios Anastasopoulos, Jun Araki,\nHaibo Ding, and Graham Neubig. 2020. X-\nFACTR: Multilingual factual knowledge retrieval\nfrom pretrained language models. In Proceedings\nof EMNLP, pages 5943–5959.\nAbdullatif Köksal and Arzucan Özgür. 2020. The\nRELX dataset and matching the multilingual blanks\nfor cross-lingual relation classiﬁcation. In Findings\nof EMNLP, pages 340–350.\nPatrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2020. MLQA: Evalu-\nating cross-lingual extractive question answering. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 7315–\n7330, Online. Association for Computational Lin-\nguistics.\nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fen-\nfei Guo, Weizhen Qi, Ming Gong, Linjun Shou,\nDaxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei\nZhang, Rahul Agrawal, Edward Cui, Sining Wei,\nTaroon Bharti, Ying Qiao, Jiun-Hung Chen, Win-\nnie Wu, Shuguang Liu, Fan Yang, Daniel Campos,\nRangan Majumder, and Ming Zhou. 2020. XGLUE:\nA new benchmark dataset for cross-lingual pre-\ntraining, understanding and generation. In Proceed-\nings of EMNLP, pages 6008–6018.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,\nHaotang Deng, and Ping Wang. 2020a. K-BERT:\nEnabling language representation with knowledge\ngraph. In Proceedings of AAAI.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020b. Multilingual denoising\npre-training for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nPhoebe Mulcaire, Jungo Kasai, and Noah A. Smith.\n2019. Polyglot contextual representations improve\ncrosslingual transfer. In Proceedings of NAACL-\nNLT, pages 3912–3918.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel\nNothman, Kevin Knight, and Heng Ji. 2017. Cross-\nlingual name tagging and linking for 282 languages.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1946–1958.\nMatthew E. Peters, Mark Neumann, Robert Logan, Roy\nSchwartz, Vidur Joshi, Sameer Singh, and Noah A.\nSmith. 2019. Knowledge enhanced contextual word\nrepresentations. In Proceedings of EMNLP , pages\n43–54.\nNina Poerner, Ulli Waltinger, and Hinrich Schütze.\n2020. E-BERT: Efﬁcient-yet-effective entity embed-\ndings for BERT. In Findings of EMNLP, pages 803–\n818.\nLibo Qin, Minheng Ni, Yue Zhang, and Wanxiang Che.\n2020. Cosda-ml: Multi-lingual code-switching data\naugmentation for zero-shot cross-lingual NLP. In\nProceedings of the Twenty-Ninth International Joint\nConference on Artiﬁcial Intelligence, IJCAI 2020 ,\npages 3853–3860.\nRyokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka.\n2022. mLUKE: The power of entity representations\nin multilingual pretrained language models. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 7316–7330, Dublin, Ireland. Associ-\nation for Computational Linguistics.\nSebastin Santy, Anirudh Srinivasan, and Monojit\nChoudhury. 2021. Bertologicomix: How does code-\nmixing interact with multilingual bert? In Proceed-\nings of the Second Workshop on Domain Adaptation\nfor NLP, pages 111–121.\nYu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding,\nChao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi\nChen, Yanbin Zhao, Yuxiang Lu, Weixin Liu, Zhi-\nhua Wu, Weibao Gong, Jianzhong Liang, Zhizhou\nShang, Peng Sun, Wei Liu, Xuan Ouyang, Dianhai\nYu, Hao Tian, Hua Wu, and Haifeng Wang. 2021.\nERNIE 3.0: Large-scale knowledge enhanced pre-\ntraining for language understanding and generation.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. Ernie: Enhanced rep-\nresentation through knowledge integration. arXiv\npreprint arXiv:1904.09223.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020a. oLMpics-on what language\nmodel pre-training captures. Transactions of the As-\nsociation for Computational Linguistics, 8:743–758.\nAlon Talmor, Oyvind Tafjord, Peter Clark, Yoav Gold-\nberg, and Jonathan Berant. 2020b. Leap-of-thought:\nTeaching pre-trained models to systematically rea-\nson over implicit knowledge. In Proceedings of\nNeurIPS, pages 20227–20237.\nErik F. Tjong Kim Sang. 2002. Introduction to the\nCoNLL-2002 shared task: Language-independent\nnamed entity recognition. In COLING-02: The\n6th Conference on Natural Language Learning 2002\n(CoNLL-2002).\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natu-\nral Language Learning at HLT-NAACL 2003, pages\n142–147.\nDenny Vrande ˇci´c and Markus Krötzsch. 2014. Wiki-\ndata: A free collaborative knowledgebase. Commun.\nACM, 57(10):78–85.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuanjing Huang, Jianshu Ji, Guihong Cao, Daxin\nJiang, and Ming Zhou. 2021a. K-Adapter: Infusing\nKnowledge into Pre-Trained Models with Adapters.\nIn Findings of ACL, pages 1405–1418.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang.\n2021b. KEPLER: A Uniﬁed Model for Knowledge\nEmbedding and Pre-trained Language Representa-\ntion. Transactions of the Association for Computa-\ntional Linguistics, 9:176–194.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet: Ex-\ntracting high quality monolingual datasets from web\ncrawl data. In Proceedings of the 12th Language\nResources and Evaluation Conference, pages 4003–\n4012.\nGenta Indra Winata, Andrea Madotto, Chien-Sheng\nWu, and Pascale Fung. 2019. Code-switched lan-\nguage models using neural based synthetic data from\nparallel sentences. In Proceedings of the 23rd Con-\nference on Computational Natural Language Learn-\ning (CoNLL), pages 271–280. Association for Com-\nputational Linguistics.\nWenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2020. Pretrained encyclopedia:\nWeakly supervised knowledge-pretrained language\nmodel. In Proceedings of ICLR.\nDongxu Zhang and Dong Wang. 2015. Relation classi-\nﬁcation via recurrent neural network. arXiv preprint\narXiv:1508.01006.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of ACL, pages 1441–1451.\nWenxuan Zhou, Fangyu Liu, Ivan Vuli´c, Nigel Collier,\nand Muhao Chen. 2022. Prix-LM: Pretraining for\nmultilingual knowledge base construction. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 5412–5424, Dublin, Ireland. Associ-\nation for Computational Linguistics.\nA Appendix\nA.1 Language Model Pretraining Details\nTraining Data The statistics of the data used for\npretraining are shown in Table 8.\nDescription Number\nlanguages 10\ncode switched synthetic sentences 246,783,693\nparallel synthetic sentences 190,576,098\nunique relation combinations in length-3 cycles 29,819\nunique relation combinations in length-4 cycles 239,966\nreasoning based training samples from length-3 cycles 24,142,272\nreasoning based training samples from length-4 cycles 73,881,422\nsampled CC100 sentences (KMLM-XLM-R only) 260,000,000\nsampled Wikipedia sentences (KMLM-mBERT only) 153,011,930\nTable 8: Statistics of the data used for pretraining.\nHyper-Parameters The hyper-parameters used\nfor language model pretraining are presented in Ta-\nble 9. After pretraining, we ﬁnetune the models on\nthe plain text data with max sequence length of 512\nfor another 600 steps. Due to the high computation\ncost of LM pretraining, we do not run many exper-\niments for hyper-parameter searching. Instead, the\nlearning rate, batch size, mlm probability are deter-\nmined according to those used in the previous LM\npretraining studies. To determine the knowledge\ntask loss weight αfor large scale pretraining, we\ncompare α∈ {0.5,0.3,0.1} using the base models\npretrained on a smaller dataset. Each base model\ntakes about 30 days to train with 8 V100 GPUs.\nA.2 Cross-lingual Logical Reasoning Task\nWe propose a cross-lingual logical reasoning (XLR)\ntask in the form of multiple-choice questions to\nHyper-parameter Value\nlearning rate 5e-5\nweight decay 0\noptimizer AdamW\nnumber of train epochs 1\nbatch size for the natural sentences 9,600\nbatch size for code switched knowledge data 9,600\nbatch size for reasoning data 9,600\nmlm probability 0.15\nmax sequence length 128\nnumber of warmup steps 100\nknowledge task loss weight (αin the loss function) 0.3\nTable 9: Hyper-parameters used for language model\npretraining.\nverify the effectiveness of our logical reasoning\noriented pretraining tasks in an intrinsic way. An\nexample of such reasoning question is given in\nFig. 5. The dataset is constructed using the length-\n3 and length-4 cycles extracted from Wikidata. For\neach cycle, we pick a triplet to create the question\nand answer. The question is created by asking the\nrelation between a pair of entities in that triplet.\n6 choices are provided for each question (includ-\ning the correct answer), which contains all of the\nrelations appear in the cycle and some sampled\nrelations associated with the two entities. The re-\nmaining triplets from the cycle are used as the con-\ntext, which is in the form of knowledge graph (see\nFig. 5). The model is required to select the most\nprobable choice according to the given context and\nquestion. We provide correct and incorrect exam-\nples to the annotators, and manually annotate 1,050\nsamples in English to build the test set. The train\nand dev sets are automatically generated, and then\ncleaned by balancing the appearances of entities,\nrelations and answers. After cleaning, we randomly\nselect 3k train samples and 1k dev samples for the\nexperiment. Then the multilingual test data in the\nother 9 non-English languages are generated by\nselecting the entity/relation labels in the desired\nlanguages from Wikidata. The cycles used to build\nthe test set are removed from the pretraining data,\nso our PLMs have never seen them beforehand.\nStatistics of the self-constructed cross-lingual\nlogic reasoning (XLR) dataset are presented in Ta-\nble 10. The multilingual test data in the 9 non-\nEnglish languages are generated by selecting the\nentity/relation labels in the desired languages from\nWikidata. So the statistics for their test sets are the\nsame as English.\nDescription Value\nnumber of samples in the train set 3,000\nnumber of samples in the dev set 1,000\nnumber of samples in the test set 1,050\ntrain set unique relation combinations 1,419\ndev set unique relation combinations 746\ntest set unique relation combinations 444\nTable 10: Statistics of the self-constructed cross-lingual\nlogic reasoning data (English).\nen de nl es avg tgt\nKMLMCS-XLM-RBASE 91.47 73.52 80.95 76.59 77.02\nKMLMCS-NR-XLM-RBASE 91.38 73.76 81.55 76.03 77.11\nTable 11: Zero-shot cross-lingual NER F1 on the\nCoNLL02/03 datasets.\nen vi nl de fr itKMLMCS-XLM-RBASE 83.43 70.55 82.18 77.87 79.19 80.06KMLMCS-NR-XLM-RBASE83.75 70.73 82.44 78.03 78.88 80.20\nes ja ko zh avg tgt\nKMLMCS-XLM-RBASE 75.96 19.32 57.54 20.95 62.62KMLMCS-NR-XLM-RBASE74.97 18.39 58.16 20.58 62.49\nTable 12: Zero-shot cross-lingual NER F1 on the\nWikiAnn dataset.\nA.3 Impact of the Logical Reasoning Tasks\nAs discussed in §4.5, we pretrain the None-\nReasoning models, KMLMCS-NR-XLM-RBASE and\nKMLMMix-NR-XLM-RBASE on the same data\nas KMLMCS-XLM-RBASE and KMLMMix-XLM-\nRBASE, but without the logical reasoning tasks. The\nnone-reasoning models generally perform worse\nthan the corresponding models trained with the log-\nical reasoning tasks, which proves the usefulness of\nthe tailored logical reasoning oriented pretraining\ntask for logical reasoning.\nIn order to explore the impact of the logi-\ncal reasoning oriented pretraining tasks on the\nnone-logical reasoning tasks, we also conduct\nablation studies to compare the performance of\nKMLMCS-NR-XLM-RBASE and KMLMCS-XLM-\nRBASE on the CoNLL02/03 (Tjong Kim Sang,\n2002; Tjong Kim Sang and De Meulder, 2003)\nand WikiAnn (Pan et al., 2017) NER data. From\nthe results presented in Table 11 and 12 we can\nsee that the average performance on the target lan-\nguages are very close, which shows the logical\nreasoning oriented pretraining tasks do not have\nobvious impact on zero-shot cross-lingual NER.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8641941547393799
    },
    {
      "name": "Natural language processing",
      "score": 0.7341439127922058
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6058648228645325
    },
    {
      "name": "Sentence",
      "score": 0.5383315086364746
    },
    {
      "name": "Task (project management)",
      "score": 0.5059954524040222
    },
    {
      "name": "Language model",
      "score": 0.4964502453804016
    },
    {
      "name": "Memorization",
      "score": 0.4753120541572571
    },
    {
      "name": "Knowledge representation and reasoning",
      "score": 0.47085040807724
    },
    {
      "name": "Relation (database)",
      "score": 0.4400979280471802
    },
    {
      "name": "Machine translation",
      "score": 0.4153478145599365
    },
    {
      "name": "Linguistics",
      "score": 0.13055384159088135
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210095624",
      "name": "Alibaba Group (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I172675005",
      "name": "Nanyang Technological University",
      "country": "SG"
    }
  ]
}