{
  "title": "Instruction Tuning on Large Language Models to Improve Reasoning Performance",
  "url": "https://openalex.org/W4399423677",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5099045468",
      "name": "Emily Vaillancourt",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2000349217",
      "name": "Christopher Thompson",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4366341216",
    "https://openalex.org/W4393259344",
    "https://openalex.org/W4396584479",
    "https://openalex.org/W4390298466",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4380136538",
    "https://openalex.org/W4392908117",
    "https://openalex.org/W4392366668",
    "https://openalex.org/W4387929855",
    "https://openalex.org/W4372283945",
    "https://openalex.org/W4393318216",
    "https://openalex.org/W4385360966",
    "https://openalex.org/W4391407054",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4388964727"
  ],
  "abstract": "The growing demand for natural language processing models capable of understanding and executing complex instructions has driven significant advancements in model fine-tuning techniques. The novel concept of instruction tuning, which involves fine-tuning pre-trained language models on meticulously curated instruction datasets, has shown remarkable promise in enhancing model performance. The research presented here focuses on applying instruction tuning to GPT2 (124M parameters) to improve its reasoning capabilities on the Multi-task Language Understanding (MMLU) dataset. By systematically curating a diverse set of tasks and corresponding instructions, and rigorously fine-tuning the model, significant improvements were achieved in key performance metrics, including accuracy, precision, recall, and F1-score. Experimental results demonstrated that the instruction-tuned GPT-2 model significantly outperformed the baseline GPT-2 and other stateof-the-art models, showcasing the effectiveness of the instruction tuning approach. The enhanced capacity of the model to follow detailed instructions led to more accurate and contextually relevant responses, showing the potential of this methodology to refine and augment the capabilities of pre-trained models. The comprehensive preparation of the instruction dataset and the iterative tuning process were critical factors in achieving these substantial performance gains. The study’s findings suggest that instruction tuning can be a powerful tool for optimizing the performance of language models across a variety of tasks and domains, provided that the instruction datasets are carefully curated and validated. The instruction tuning of GPT-2 (124M parameters) resulted in significant improvements in the model’s reasoning capabilities, as evidenced by the enhanced performance metrics on the MMLU dataset. The research highlights the potential of instruction tuning as an effective approach for refining pre-trained models and enhancing their applicability in complex and diverse scenarios. By demonstrating the substantial benefits of fine-tuning models on carefully prepared instruction datasets, the study provides valuable insights into the potential of this technique for further advancements in natural language processing.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8295326232910156
    },
    {
      "name": "Language model",
      "score": 0.6390992403030396
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5857924818992615
    },
    {
      "name": "Process (computing)",
      "score": 0.5414232611656189
    },
    {
      "name": "Task (project management)",
      "score": 0.5082142353057861
    },
    {
      "name": "Machine learning",
      "score": 0.46610578894615173
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44867607951164246
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.4347711205482483
    },
    {
      "name": "Recall",
      "score": 0.4276919960975647
    },
    {
      "name": "Performance improvement",
      "score": 0.4174352288246155
    },
    {
      "name": "Key (lock)",
      "score": 0.41476017236709595
    },
    {
      "name": "Programming language",
      "score": 0.1295449435710907
    },
    {
      "name": "Systems engineering",
      "score": 0.10193315148353577
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 8
}