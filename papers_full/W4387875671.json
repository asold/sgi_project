{
    "title": "Design of a Modified Transformer Architecture Based on Relative Position Coding",
    "url": "https://openalex.org/W4387875671",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2490888232",
            "name": "Wenfeng Zheng",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2163621113",
            "name": "Gu Gong",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2097365987",
            "name": "Jiawei Tian",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2133269411",
            "name": "Siyu Lu",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2161412591",
            "name": "RuiYang Wang",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2153253619",
            "name": "Zhengtong Yin",
            "affiliations": [
                "Guizhou University"
            ]
        },
        {
            "id": "https://openalex.org/A2118685195",
            "name": "Xiaolu Li",
            "affiliations": [
                "Southwest University"
            ]
        },
        {
            "id": "https://openalex.org/A2002337148",
            "name": "Lirong Yin",
            "affiliations": [
                "Louisiana State University"
            ]
        },
        {
            "id": "https://openalex.org/A2490888232",
            "name": "Wenfeng Zheng",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2163621113",
            "name": "Gu Gong",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2097365987",
            "name": "Jiawei Tian",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2133269411",
            "name": "Siyu Lu",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2161412591",
            "name": "RuiYang Wang",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2153253619",
            "name": "Zhengtong Yin",
            "affiliations": [
                "Guizhou University"
            ]
        },
        {
            "id": "https://openalex.org/A2118685195",
            "name": "Xiaolu Li",
            "affiliations": [
                "Southwest University"
            ]
        },
        {
            "id": "https://openalex.org/A2002337148",
            "name": "Lirong Yin",
            "affiliations": [
                "Louisiana State University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3125449488",
        "https://openalex.org/W3161207330",
        "https://openalex.org/W3190730109",
        "https://openalex.org/W4214493767",
        "https://openalex.org/W4288056514",
        "https://openalex.org/W4280633620",
        "https://openalex.org/W4367056871",
        "https://openalex.org/W2885195348",
        "https://openalex.org/W4367186550",
        "https://openalex.org/W4365143768",
        "https://openalex.org/W4221071357",
        "https://openalex.org/W3025451744",
        "https://openalex.org/W2594990650",
        "https://openalex.org/W4226138338",
        "https://openalex.org/W4284881463",
        "https://openalex.org/W4281630638",
        "https://openalex.org/W3199934250",
        "https://openalex.org/W3126396269",
        "https://openalex.org/W4213012943",
        "https://openalex.org/W3000499162",
        "https://openalex.org/W3080648230",
        "https://openalex.org/W2131774270",
        "https://openalex.org/W1985258458",
        "https://openalex.org/W2972551450",
        "https://openalex.org/W2963541420",
        "https://openalex.org/W3146366485",
        "https://openalex.org/W3091618127",
        "https://openalex.org/W4224440221",
        "https://openalex.org/W3132607382",
        "https://openalex.org/W3093956460",
        "https://openalex.org/W4385380955",
        "https://openalex.org/W3126805253",
        "https://openalex.org/W3199716762",
        "https://openalex.org/W4224211827",
        "https://openalex.org/W4309452766",
        "https://openalex.org/W4223558833",
        "https://openalex.org/W4220924176",
        "https://openalex.org/W4361213318",
        "https://openalex.org/W3134284227",
        "https://openalex.org/W4220842010",
        "https://openalex.org/W1583837637",
        "https://openalex.org/W4303183350",
        "https://openalex.org/W4384699001",
        "https://openalex.org/W2159640018",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W1990524510",
        "https://openalex.org/W4205165244",
        "https://openalex.org/W3036789610",
        "https://openalex.org/W3155737070",
        "https://openalex.org/W3045738072",
        "https://openalex.org/W2988937804",
        "https://openalex.org/W4301184435"
    ],
    "abstract": "Abstract Natural language processing (NLP) based on deep learning provides a positive performance for generative dialogue system, and the transformer model is a new boost in NLP after the advent of word vectors. In this paper, a Chinese generative dialogue system based on transformer is designed, which only uses a multi-layer transformer decoder to build the system and uses the design of an incomplete mask to realize one-way language generation. That is, questions can perceive context information in both directions, while reply sentences can only output one-way autoregressive. The above system improvements make the one-way generation of dialogue tasks more logical and reasonable, and the performance is better than the traditional dialogue system scheme. In consideration of the long-distance information weakness of absolute position coding, we put forward the improvement of relative position coding in theory, and verify it in subsequent experiments. In the transformer module, the calculation formula of self-attention is modified, and the relative position information is added to replace the absolute position coding of the position embedding layer. The performance of the modified model in BLEU, embedding average, grammatical and semantic coherence is ideal, to enhance long-distance attention.",
    "full_text": "Vol.:(0123456789)1 3\nInternational Journal of Computational Intelligence Systems          (2023) 16:168  \nhttps://doi.org/10.1007/s44196-023-00345-z\nRESEARCH ARTICLE\nDesign of¬†a¬†Modified Transformer Architecture Based on¬†Relative \nPosition Coding\nWenfeng¬†Zheng1 ¬†¬∑ Gu¬†Gong1¬†¬∑ Jiawei¬†Tian1¬†¬∑ Siyu¬†Lu1¬†¬∑ Ruiyang¬†Wang1¬†¬∑ Zhengtong¬†Yin2¬†¬∑ Xiaolu¬†Li3¬†¬∑ Lirong¬†Yin4\nReceived: 23 May 2023 / Accepted: 2 October 2023 \n¬© The Author(s) 2023\nAbstract\nNatural language processing (NLP) based on deep learning provides a positive performance for generative dialogue system, \nand the transformer model is a new boost in NLP after the advent of word vectors. In this paper, a Chinese generative dialogue \nsystem based on transformer is designed, which only uses a multi-layer transformer decoder to build the system and uses the \ndesign of an incomplete mask to realize one-way language generation. That is, questions can perceive context information \nin both directions, while reply sentences can only output one-way autoregressive. The above system improvements make \nthe one-way generation of dialogue tasks more logical and reasonable, and the performance is better than the traditional \ndialogue system scheme. In consideration of the long-distance information weakness of absolute position coding, we put \nforward the improvement of relative position coding in theory, and verify it in subsequent experiments. In the transformer \nmodule, the calculation formula of self-attention is modified, and the relative position information is added to replace the \nabsolute position coding of the position embedding layer. The performance of the modified model in BLEU, embedding \naverage, grammatical and semantic coherence is ideal, to enhance long-distance attention.\nKeywords Relative position embedding¬†¬∑ Natural language processing¬†¬∑ Attention mechanism\n1 Introduction\nNatural Language Processing (NLP) is a critical area of \nresearch that aims to enable machines to emulate human lan-\nguage and engage in seamless conversations with humans. \nThis encompasses the capacity to read, comprehend, and \nfluently use language, master, and apply knowledge, and \nengage in logical thinking and inference [1 ‚Äì3]. Improved \nlanguage intelligence through deep learning methods not \nonly enhances a computer's ability to comprehend language \nbut also facilitates emotional expression and logical reason-\ning. Consequently, there are numerous potential applications \nfor natural language processing solutions based on deep \nlearning.\nIn the field of NLP, the adoption of recurrent neural net-\nworks (RNN) [4], attention mechanisms [5], and transform-\ners [6] within end-to-end dialogue systems has significantly \nelevated the language comprehension and expression capa-\nbilities of these systems. In the earlier stages, RNN-based \nlanguage models (RNNLM) gained prominence, achieving \nbreakthroughs in NLP tasks. However, researchers soon \nencountered challenges related to long-range dependen-\ncies during model training. This arose from the tendency \nof weight parameters in RNN-based models to approach \nextremes, resulting in slow convergence and imprecise train-\ning outcomes. The introduction of long‚Äìshort-term memory \n(LSTM) [7 ‚Äì10] addressed this issue. LSTM, a variant of \nRNN, is better suited for processing lengthy sequences due \nto its architectural design, which incorporates three gate \nstructures (input gate, output gate, and forgetting gate) for \ncontrolling information flow.\nThe concept of sequence-to-sequence (Seq2Seq) [11, \n12] models emerged in 2014 as a method to generate \n * Wenfeng Zheng \n winfirms@uestc.edu.cn\n * Lirong Yin \n lyin5@lsu.edu\n1 School of¬†Automation, University of¬†Electronic Science \nand¬†Technology of¬†China, Chengdu¬†610054, China\n2 College of¬†Resource and¬†Environment Engineering, Guizhou \nUniversity, Guiyang¬†550025, China\n3 School of¬†Geographic Science, Southwest University, \nChongqing¬†400715, China\n4 Department of¬†Geography and¬†Anthropology, Louisiana \nState University, Baton¬†Rouge, LA¬†70803, USA\n International Journal of Computational Intelligence Systems          (2023) 16:168 \n1 3  168  Page 2 of 17\nsequences based on given input sequences. Initially applied \nto machine translation, Seq2Seq models addressed the \nchallenge of handling variable-length input and output \nsequences. Over time, these models have shown promise in \nother NLP tasks, such as text summarization and dialogue \ngeneration. However, during the decoding phase, limita-\ntions were identified. The initial approach relied heavily \non the last hidden layer state of the encoder, resulting in \nsuboptimal information utilization [13]. In addition, when \nprocessing long input sequences, the fixed-length seman-\ntic vector struggled to retain critical feature information, \nleading to reduced accuracy. To overcome these issues, \nattention mechanisms were introduced [14].\nThe concept of attention mechanisms was initially pro-\nposed by Bahdanau et¬†al. for machine translation and later \nimproved by Luong et¬†al. [15, 16]. Drawing inspiration \nfrom human selective attention, attention mechanisms \nmimic the human process of rapidly scanning and focus-\ning on relevant information while disregarding irrelevant \ndetails. In the context of deep learning, attention mecha-\nnisms act as a resource allocation mechanism [17‚Äì 19]. \nThey dynamically redistribute the weight of information \nbased on its importance, ensuring that critical informa-\ntion is given higher weight, while less important informa-\ntion is assigned lower weight. This feature extraction and \nsequential data analysis capability has found applications \nin various fields, including language modeling and image \nprocessing [20, 21].\nAttention mechanism in the decoding process [22], each \noutput not only depends on the fixed-size semantic vector \nencoded by the encoder, but also depends on the hidden \nlayer state of the previous output unit and the corresponding \nhidden layer state of the current output unit in the decoding \nprocess. Attention is introduced into the Seq2Seq model to \nsolve the problem that the original RNN often loses part \nof the input sequence information, and the accuracy of the \nmodel is improved. In the specific translation task [23, 24], \nthe decoding phase is to translate one word by one word in \nthe time series. When decoding one word, it will not have the \nsame association with all the words in the source sequence. \nIn the decoder phase, the selected reference contributes the \nmost to the semantic vector of the current sequence word, \nrather than uniformly referring to all the semantic vectors.\nThe introduction of attention mechanisms into Seq2Seq \nmodels aimed to address limitations in retaining input \nsequence information and improve model accuracy. Dur -\ning the decoding phase, rather than uniformly considering \nall input semantic vectors, attention mechanisms enable \nthe model to selectively focus on the most relevant refer -\nence for the current sequence word. Prior to this develop-\nment, the most effective language models were based on \nSeq2Seq architecture with LSTM for modeling. However, \nthis approach lacked parallel computing capabilities during \ntraining, limiting the model's ability to meet the computa-\ntional demands of increasingly larger corpora.\nTo fill the vacancy mentioned above. In this research, we \nintroduced the implementation of a transformer-based gen-\nerative dialogue system tailored for Chinese text. Theoretical \nfoundations of the basic methods and process design were \nproposed. We designed a multi-turn generative dialogue \nsystem with an end-to-end structure that encodes natural \nlanguage sentences into the model's vector space and gen-\nerates sequences as output through the generative dialogue \nsystem's decoding process. When modeling and training the \nsystem, multi-turn statements were input in segments, and \na self-regressive method was used to create a unidirectional \ngenerative language model. Generated words were continu-\nously appended to the input until an end token was reached. \nWe introduced a novel method to enhance long-distance \nattention within the dialogue system, replacing absolute \nposition encoding in the position embedding module with \nrelative position encoding. To test the effectiveness of rela-\ntive position encoding in mitigating long-range information \ndecay, we conducted experiments using multi-turn dialogue \ndata sets, including the STC label data set and test data \nset. We compared the results with classical dialogue base -\nline models. The experimental results indicated that as the \nsequence length increased, accuracy improved, and the loss \nvalue decreased. This aligns with the expected outcomes of \nintroducing relative position encoding, demonstrating that \nrelative position encoding is better suited to handling long-\ntext sequences compared to absolute position encoding. This \nunderscores the effectiveness of our research optimization. \nIn conclusion, the use of relative position encoding miti-\ngates the issue of weak long-distance information, thereby \nenhancing the dialogue system's understanding of long-\nrange information.\n2  Related Work\n2.1  End‚Äëto‚Äëend Dialogue Systems\nTomas et¬†al. proposed a language model RNNLM based on \nRNN in 2010[25, 26]. The model uses the vector of hidden \nstates to record the historical information of word sequences. \nHidden states can obtain long-range dependencies in the lan-\nguage. In the past, language models can only use the sliding \nwindow information of the front and back n words to predict \nthe target words, while the advantage of the cyclic neural \nnetwork is to fully use the context information to predict \nthe target words. Sundermeyer et¬†al. Introduced LSTM into \nthe language model in 2012 and proposed LSTM‚ÄìRNNLM \n[27]. The article mentions that LSTM has advantages over \nfeed-forward neural networks, because it can utilize long-\nterm contextual information. However, standard gradient \nInternational Journal of Computational Intelligence Systems          (2023) 16:168  \n1 3 Page 3 of 17   168 \ndescent algorithms do not perform well in learning long-\nterm dependencies due to the instability of gradient com-\nputation. To solve this problem, the article introduces an \nimproved RNN architecture, namely, LSTM. LSTM controls \nthe flow of information by introducing input gates, forget \ngates, and output gates, thereby avoiding the problems of \ngradient disappearance and gradient explosion.\n2.2  Seq2Seq Eencoder‚ÄìDecoder Model\nSeq2Seq is an encoder‚Äìdecoder model [28]. The encoder \nand decoder are two cyclic neural networks, using the above-\nmentioned LSTM or its variant GRU. The recurrent neural \nnetwork is an autoregressive network structure. The out-\nput of the last time in the sequence is the input of the next \ntime. The function of the first recurrent neural network is \nto embed the input sequence into the fixed-length semantic \nvector space. The vector represents the characteristics of the \ninput sequence. The network is named encoder. The other \ntask of RNN is to generate the output sequence from the \nfixed-length vector. The network is named decoder. Seq2Seq \nmodel based on recurrent neural network (LSTM or GRU) \nhas achieved good results.\nIn their research, Tianyu Liu et¬†al. proposed a novel \nstructure-aware seq2seq model for generating table-to-text \ndescriptions [29]. This model improves generation perfor -\nmance by introducing an attention mechanism and a dual \nattention mechanism. The model can better utilize the struc-\ntural information of the table and generate descriptions \nrelated to the content of the table. The results show that the \nmodel outperforms traditional statistical language models \nand basic seq2seq models in generative performance.\n2.3  Transformer‚ÄëBased Language Model\nIn 2017, the language model based on the transformer began \nto try not to rely on RNN and LSTM modeling [30]. Trans-\nformer was proposed by Google in its paper on machine \ntranslation tasks, and achieved very good translation results, \nwhich consists of a positionwise feed-forward network \n(FFN) layer and a multi-head attention layer. FFN is used in \neach position separately, which can guarantee the position \ninformation of each symbol in the input sequence during \noperation. The latter makes the model focus on informa-\ntion from different representation subspaces from different \npositions [31].\nTransformer uses the self-attention mechanism to model \nthe language model. Compared with RNN, self-attention \nmechanism not only increases the training parameters, but \nalso realizes the parallelization through the complexity of \nspace and parameters [32], which greatly accelerates the \ntraining efficiency of the model. In addition to being more \nparallelizable, the transformer establishes long-distance \ndependence through the self-attention mechanism. Trans-\nformer model is unable to process long sequences due to its \nself-attention operation, which scales quadratically with the \nsequence length [33]. Relative position coding originated \nfrom Google's paper [34]. Shan et¬†al. restricted the scope \nof self-attention to reduce the hybrid network model‚Äôs con-\nsumption of memory and calculations and use the relative \nposition encoding to improve robustness of the model. It is \ngenerally believed that relative position coding is inspired by \nabsolute position coding [35]. Relative position information \ncoding does not completely model the position information \nof each input but considers the relative distance between the \ncurrent position and the position to be noticed when calcu-\nlating attention, because natural language generally depends \nmore on relative position. Therefore, relative position coding \nusually has excellent performance, and it is more flexible.\n3  Methods\nIn this section, we will provide a detailed overview of the \nfundamental methods and process design employed in our \nstudy. The entire multi-turn generative dialogue system has \nbeen devised as an end-to-end structure. It takes natural lan-\nguage sentences, encodes them into a model vector space, \nand generates sequences as output through the generative \ndialogue system's decoding process. Furthermore, we pro-\npose the use of relative position encoding for self-attention \ncomputations, replacing the absolute position encoding in \nthe dialogue system. This modification enhances long-range \nattention capabilities.\n3.1  Dialogue System Implementation\nFirst, a dialogue model network based on encoder‚Äìdecoder \nis proposed, and the autoregressive model is adopted in the \nimplementation process. In the autoregressive model, the \nstatements in the dialogue system are defined as the follow-\ning equation:\nwhere X is a natural language sentence. xi represents \nthe word vector of the ith word, so the problem turns \ninto encoding these sequences. Suppose the question is \nX =( a, b, c, d, e, f) , target output is Y = (P, Q, R, S, T) , the \nencoder‚Äìdecoder structure of a basic dialogue system is \nshown in Fig.¬†1.\nOn the left of Fig.¬†1 is the encoder of the dialogue system, \nwhich is responsible for encoding the variable length input \nsequence as long as possible into a fixed-length semantic \nvector. Theoretically, this fixed-length vector should contain \nall the useful information of the input sentence. The decoder \n(1)X =( x1 ,x2 ,‚Ä¶ ,xt)\n International Journal of Computational Intelligence Systems          (2023) 16:168 \n1 3  168  Page 4 of 17\nof the dialogue system is responsible for decoding the vector \njust encoded into our desired output sequence. Unlike the \nencoder, the decoder must be \"unidirectional and recursive\", \nbecause the decoding process is recursive. The specific pro-\ncess is as follows:\n1) All output terminals start with the general identi-\nfier < CLS > tag and end with the < SEP > tag. These two \nidentifiers are also regarded as one word;\n2) Input < CLS > to the decoder, then obtain the hidden \nlayer vector, mix this vector with the output of the decoder, \nand then input it to the interpreter. The result of the inter -\npreter should be output as p;\n3) Then input P into the decoder to obtain a new hidden \nlayer vector, mix it with the output of the encoder again, and \ninput it into the interpreter, which should output Q;\n4) Recurse successively until the output of the interpreter \nis < SEP > .\nIn the decoding process of the decoder, the output lan-\nguage model is shown in the following equation:\nThe so-called one-way language model, in a narrower \nsense, should be called positive language model. The cru-\ncial factor is that we cannot get \"future\" data. For example, \nin Eq.¬†(2), there is no additional input during predicting P; \nWhen forecasting Q, you can only enter P; when forecasting \nR, you can enter P, Q; and so on.\nAs shown in Fig.¬† 2, assume that the desired output \nresult is Y =  , when \n(2)\np(P, Q, R, S, T) = p(P)p(Q/uni007C.varP)p(R/uni007C.varP,Q)p(S/uni007C.varP,Q, R)p(T/uni007C.varP,Q, R, S)\nthe decoder outputs, first the prediction result starts with \nthe < CLS > identifier, input < CLS > into the decoder to \nget \"Ê∞î\", continue to input into the encoder to get y in turn \nY = „ÄÇ\nIn the basic architecture, the transformer model is used \nto implement Seq2Seq. At this time, some key prior knowl-\nedge is introduced: considering that the input language and \noutput language are Chinese, the hidden layer of encoder \nand decoder can share parameters and share the same set \nof word vectors, which will greatly reduce the number of \nparameters. The dialogue system is realized through multi-\nlayer transformer decoder.\nConsidering that the dialogue system is suitable for multi-\nple rounds of dialogue tasks, there is a context sentence seg-\nment with multiple rounds in the dialogue in the input text, \nwhich is expressed in English segment. The dialogue system \nintroduces segment-level recurrence mechanism (SLRM), \nwhich stores the information of the previous segment every \ntime and splices it with the information of the current seg-\nment. Suppose that there are two segments with length L in \na sample data text, expressed as s/u1D70F= /parenleft.s1x/u1D70F,1,x/u1D70F,2,‚Ä¶,x/u1D70F,L\n/parenright.s1 and \ns/u1D70F+1 =( x/u1D70F+1,1,x/u1D70F+1,2,‚Ä¶ ,x/u1D70F+1,L ) . Suppose the hidden layer \ninformation stored in s/u1D70F is represented as h/u1D70F‚àà RL√óD , D rep-\nresents the dimension of the hidden layer vector, then the \ncalculation method of s/u1D70F+1 is as Eqs.¬†(3)‚Äì(5):\nSG represents a stop gradient, which means that the \nparameters of the previous segment remain unchanged. The \nlength of the hidden layer is increased through sequence \nsplicing, and then the whole enters the transformer model \nfor training. The specific process of transformer is shown \nin Fig.¬†3.\nTo sum up, the network structure of the dialogue sys-\ntem is mainly composed of the input layer, feature splicing \nlayer, transformer decoder layer and output layer. The spe-\ncific steps are:\n1) Each data sample is spliced by multiple rounds of \ndialogue text. The LCCC (large-scale cleaned Chinese \nconversation) corpus data [36] preprocessing process is \nused to splice multiple rounds of dialogue into a language \nsequence for natural language processing, that is, first, take \nthe [CLS] tag as the starting character, extract continuous \ndialogue sentences and fill in the input sample, insert the \n[SEP] tag between the sentences of different speakers, and \nset the maximum length. Note that the sequence length of \nthe input sample is N.\n(3)/uni0303.s1h/u1D70F+1 = /bracketleft.s1SG/parenleft.s1h/u1D70F\n/parenright.s1‚ó¶h/u1D70F+1\n/bracketright.s1\n(4)q/u1D70F+1 ,k/u1D70F+1 ,v/u1D70F+1 = /uni0303.s1h/u1D70F+1 W q,/uni0303.s1h/u1D70F+1 W k,/uni0303.s1h/u1D70F+1 W v\n(5)h/u1D70F+1 = Transformer(q/u1D70F+1 ,k/u1D70F+1 ,v/u1D70F+1 )\nFig. 1  Seq2Seq structure of dialogue system\nFig. 2  Unidirectional language model\nInternational Journal of Computational Intelligence Systems          (2023) 16:168  \n1 3 Page 5 of 17   168 \n2) To encode the data samples, first organize all words \ninto a word table. By default, the word is the minimum gran-\nularity unit of the word vector. After the word table is estab-\nlished, record the number of word tables as V, and convert \neach word into a single hot coding vector to obtain N √ó V \nsize matrix as a training sample. The specific operation is \nto set the value at the index i dimension to 1 and the others \nto 0. Taking Fig.¬† 2 as an example, it is assumed that the \nprocessed sample is Y =  , \nthen N = 6, V = 6. The unique heat code in this scenario is \ndescribed as the following equation:\n3) Learn the word embedding matrix W , and transform \nthe unique hot coding into a word vector suitable for the \nsubsequent transformer model by initializing it into a ran-\ndom word embedding matrix. The word embedding matrix \nXWE = XW of N √ó D is obtained by embedding the input \nwords of the unique encoding into the network, where D \nrepresents the embedding dimension of the word embed-\nding vector, and W is the word embedding matrix, with \nthe size of V √ó D.\n4) Add segment embedding code. Segment embedding \ncode indicates different roles of dialogue, which is rep-\nresented by SegmentID. The specific vector content is a \nD-dimensional line vector filled with all 0 or all 1, where \n0 or 1, respectively, represents the questioner or respond-\nent, and N D-dimensional row vectors are spliced into a \n(6)\nsegment embedding matrix XSE of D √ó N according to the \nstatement sequence.\n5) To enhance the word vector representations with \nposition coding information, a different approach is \nrequired when compared to cyclic neural networks. The \ntransformer model, unlike cyclic neural networks, relies on \nthe self-attention module and does not employ recursive \noperations. Consequently, it is unable to naturally cap-\nture timing information within the input text sequence and \nlacks inherent positional information for different word \nvectors in sentences. To address this limitation, the posi-\ntion information for each word needs to be incorporated \ninto the word vectors. This enables the transformer model \nto distinguish the temporal sequence relationships among \nwords within the current word order. To achieve this, posi -\ntion embedding is introduced, with the dimension of the \nposition embedding set to N √ó D. The method employed \nis the trigonometric function-based absolute position cod-\ning, often referred to as sinusoidal position coding [ 37, \n38]. This technique integrates positional information into \nthe input by performing a linear transformation using \nboth sine and cosine functions, as shown in the following \nequation:\nEquation¬†(7 ) is the absolute position information cod-\ning formula; k represents the position of the word in the \nsentence. The value range is (0, N). d represents the dimen-\nsion of the position vector, pk,2i , pk,2i+1 represents the \n(2i)th , (2i + 1)th components of the position coding vector, \n(7)\n‚éß\n‚é™\n‚é®\n‚é™‚é©\npk,2i = /u1D460/u1D456/u1D45B(k\n10000\n2i\nd\n)\npk,2i+1 = /u1D450/u1D45C/u1D460(k\n10000\n2i\nd\n)\nFig. 3  Flow chart of trans-\nformer-based generative \ndialogue system\n\n International Journal of Computational Intelligence Systems          (2023) 16:168 \n1 3  168  Page 6 of 17\nrespectively, that is, the coding is calculated by sine func-\ntion and cosine function in even dimension and odd dimen-\nsion, respectively, to get XPE . Thus, timing information with \nperiodic changes is generated. The position is embedded \nin the dimension of the length n of the sample sequence. \nWith the increase of the dimension number, the cycle will \nbecome slower and slower. Therefore, the model containing \nposition texture information will be generated in even and \nodd dimensions. From this, the dependence between posi -\ntions and the timing characteristics of natural language can \nbe learned.\n6) Feature fusion, calculate the matrix after adding loca-\ntion information and segmentation information, as shown in \nthe following equation:\nTo avoid data loss, three information matrices can be \nspliced together \n/parenleft.s1XWE XSE XPE\n/parenright.s1\n , due to formula (7 ), the \nthree information matrices are added directly, as shown in \nthe following equation:\n7) Input XE into language expression layer is stacked by \nseveral layers of transformer decoder units, and the specific \ncalculation of each layer module is as Eqs.¬†(10)‚Äì(12):\nIntroduce the self-attention mechanism to calculate the \nattention moment matrix Z. First, XE is multiplied by three \nD √ó D size weight matrices W Q, W K, W V to obtain the query \nmatrix Q, key matrix K and value matrix V:\nThe specific formula of attention mechanism is as the \nfollowing equation:\nSuppose a sentence is X =( x1 ,x2 ,x3 ,x4 ) , that is, there \nare four-word vectors in the statement. After the operation \nfrom Eqs.¬†(10) to (12), the respective query vector Q , key \nvector K and value vector V are obtained, respectively. As \nshown in Fig.¬†4:\nWhen calculating the self-attention vector of the first word \nx1 , it is necessary to calculate the dot product between the \nkey vector of all words and the query vector of the current \n(8)\nÔøΩ XWE XSE XPE\nÔøΩ‚éõ\n‚éú\n‚éú‚éù\nW E\nW S\nW P\n‚éû\n‚éü\n‚éü‚é†\n= XWE W E + XSEW S + XPEW P\n(9)XE = XWE + XSE + XPE\n(10)Q = XEW Q =[ q1 ,q2 ,‚Ä¶ ,qN]\n(11)K = XEW K =[ k1 ,k2 ,‚Ä¶ ,kN ]\n(12)V = XEW V =[ v1 ,v2 ,‚Ä¶ ,vN ]\n(13)Attention(Q, K , V ) = softmax\nÔøΩ\nQK T\n‚àö\ndk\nÔøΩ\nV\nword x1 to get the score. Each score is divided by the square \nroot of \n‚àö\ndk to get Si, i =1 , 2, 3, 4 . Then, calculate Softmax \nto normalize the scores of all words to ai, i =1 , 2, 3, 4.\nThis study adds a multi-head attention matrix to the Q , \nK , and V matrices to improve the attention unit's ability to \nextract multiple semantics of a word [ 39‚Äì43]. The sche -\nmatic diagram of multi-head attention is shown in Fig.¬† 5. \nIn this example, the implementation process of multi-head \nattention mechanism is to define the super parameter h = 3 \nto represent the number of heads, divide D into h parts, \nand divide Q, K and V into parts \n/parenleft.s1Q i,Ki,Vi\n/parenright.s1,i=1 ,2,3  \nthrough linear mapping, calculate attention for each part. \nThe process is shown in Eqs.¬†(14)‚Äì(16):\nFig. 4  Specific process of attention calculation\nFig. 5  Multi-head attention\nInternational Journal of Computational Intelligence Systems          (2023) 16:168  \n1 3 Page 7 of 17   168 \nW0 is the weight of the linear layer, the h attention features \nare spliced and linearly projected to obtain the attention fea-\nture matrix Z.\nThen connect the residuals, and the specific implemen -\ntation is to add Z with XE get the attention matrix and get \nXA = Z + XE . At the same time, layer normalization is per -\nformed. The function of standard normalization is to treat \nthe hidden layer in the network as standard normal distribu-\ntion and speed up the convergence of loss function in the \ntraining process. Obtain X/uni2032.var\nA , as the following equation:\nParameters u i and /u1D70Ei , respectively, represent the mean \nand standard deviation of each element x ij , œµ, is a minimum \nconstant to prevent numerical calculation problems caused \nby division by 0, and Œ± and Œ≤ are trainable parameters to \ncompensate for information loss caused by normalized.\nTransfer the residual and normalized matrix X/uni2032.var\nA to the \nfeed-forward layer. The feed-forward module is a multi-layer \nperceptron (MLP), which has a hidden layer. The hidden \nlayer matrix is obtained by two-layer linear mapping and \n(14)Qi= QW Q\ni ,Ki= KW K\ni ,Vi= VW V\ni ,i= 1, ‚Ä¶,h\n(15)headi = Attention/parenleft.s1Q i,K i,Vi\n/parenright.s1,i= 1, ‚Ä¶,h\n(16)\nZ = MultiHead(Q, K ,V ) = Concat(head1 ,head2 ,‚Ä¶ ,headh)W 0\n(17)LayerNorm (x)= /u1D6FC√ó\nxij‚àí ui\n/uni221A.s1\n/u1D70E2\ni + /u1D716\n+ /u1D6FD\nactivation with the activation function ReLU, as shown in \nthe following equation:\nFor matrix XH is then connected with the residuals and \nadded with X/uni2032.var\nA to obtain XÔøΩ\nH = XÔøΩ\nA + XH . The X/uni2032.var\nH matrix is \nnormalized and a new embedded matrix XE is output.\nAfter the multi-layer transformer module, the matrix XTE \nof D √ó N is output. The processing steps of the multi-layer \ntransformer module are summarized as follows: first, it is \nprocessed through the self-attention layer, and then trans-\nferred to the neural network layer. After the current trans-\nformer module is processed, it then transfers the vector to \nthe next transformer module.\n8) In the output layer, when the last transformer mod-\nule generates the output, the model multiplies the output \nvector by the word embedding matrix W. Each row of the \nword embedding matrix corresponds to the word embed -\nding vector in the model word table, and the attention score \ncorresponding to each word in the word table is obtained by \nmultiplication [44‚Äì46].\nFinally, Softmax is used to predict word in the output \ndictionary, and the model uses cross entropy to update the \nparameters. In this way, the model completes a round and \noutputs a word. The model then continues to recurse until \na complete sequence is generated, the upper limit n of the \nsequence is reached, or the terminator < SEP > is generated. \nFinally, the complete basic structure of the system is shown \nin Fig.¬†6. Table¬†1 shows the specific network parameters of \nthe transformer-based generative dialogue system.\n(18)XH = ReLU (Linear(Linear(XÔøΩ\nA )))\nFig. 6  Structure diagram of \ngenerative dialogue system \nbased on transformer\n\n International Journal of Computational Intelligence Systems          (2023) 16:168 \n1 3  168  Page 8 of 17\nAs mentioned above, the transformer architecture can \nlearn global information. The key lies in the self-attention \nmechanism. The self-attention mechanism calculates the \nencoded input sequence with each other to obtain the cosine \nsimilarity and form a similarity matrix of size n2 . which \nrepresents the length of the input sentence sequence [X, X]. \nCompared with the spatial complexity O(n) of RNN, the \nspatial complexity of the self-attention matrix is O/parenleft.s1n2 /parenright.s1. The \ncomplexity of space and parameters realizes parallelism, and \nthe increase of parameters of the self-attention matrix can \ncontain more statement information, rather than the limita-\ntion of the result caused by the fixed-length semantic vector.\nEach column of the attention matrix represents \ninput and each row represents output. The matrix \nrepresents the correlation between input and out -\nput. Suppose you input \"‰Ω†ÊÉ≥ÂêÉÂï•\" and the reply is \"\nÁôΩÂàáÈ∏°\", then the above statements are spliced into \n \nby language sequence preprocessing. When train-\ning unidirectional generative language model, input \n , and predict in turn \n until < SEP > appears. Considering that current \ninput cannot take advantage of \"future\" information. To \ngenerate a unidirectional language sequence, the input and \noutput are staggered by one bit, as shown in Fig.¬†7a:\nThe white square represents 0. The first line indicates \nthat \"‰Ω†\" can only be related to the starting mark < CLS > , \nthe second line indicates that \"ÊÉ≥t\" is related to the starting \nmark < CLS > and \"‰Ω†\", and so on.\nBut the above model will also add the input \n to the prediction range, \nwhich belongs to additional constraints. The only thing that \nreally needs prediction is .Therefore, this study \nrefers to the idea of Mask for UNILM [23]. Design incom-\nplete mask, only mask  part, reserve information of \n part. As shown in Fig.¬†7b:\nThe attention of the input part obtains two-way context \ninformation, while the output part is one-way attention. There-\nfore, in the process of complementing the length of blank \nremaining sentences and mask in the maximum sentence \nlength, 0 is generally filled in. This process is called padding. \nHowever, there will be problems during Softmax. The reason \nis e0 = 1 is an effective number. In this way, the padded part of \nSoftmax participates in the operation, which is equivalent to \nallowing the invalid part to participate in the operation, which \nhas an impact on Softmax calculation and results in deviation. \nCurrently, it is necessary to cover up these invalid parts and \ndo not participate in the calculation. The specific method is \nto add a large negative offset to the invalid part, as shown in \nEqs.¬†(19)‚Äì(21):\n(19)zillegal = zillegal+ biasillegal\n(20)biasillegal ‚Üí ‚àí‚àû\nTable 1  Network structure of dialog model based on transformer\nOrder Layer Dimension\n(1) Embeddings V = 13,088 D = 384\n(2) Positional Encoding 256 384\n(3) Segement Encoding 256 384\n(4) Attn/c_attn 256 384 √ó 3\n(5) Attn/c_proj 256 384\n(6) Add&Norm 256 384\n(7) Mlp/c_fc 768 384 √ó 4\n(8) Mlp/c_proj 384 √ó 4 384\n(9) Add&Norm 256 384\nFig. 7  Mask design. a Mask \nof one-way language model; b \nincomplete mask of this dialog \nsystem\n\nInternational Journal of Computational Intelligence Systems          (2023) 16:168  \n1 3 Page 9 of 17   168 \nAfter the calculation of Eqs.¬†(19)‚Äì(21), the masking part \nwill not be affected after participating in Softmax calculation, \nand the calculation increment is still 0, to avoid the influence \nof invalid area on training accuracy.\n3.2  Absolute Position Information Coding\nThe transformer-based attention model is the dot product \nof the similarity between vectors in the matrix, there is no \ntiming information added by the recursive process of RNN. \nIn most cases, the previous solution is to integrate the loca-\ntion information into the input, which constitutes the general \npractice of absolute location coding. However, when dealing \nwith multiple rounds of QA dialogue, the dialogue length \nshould be unlimited in theory, but the design of absolute \nlocation coding limits the length of the dialogue text, and the \nabove memory effect is not ideal in the long text.\nThe following analyzes several commonly used absolute \nposition codes.\n1) One of the most concise schemes of absolute position \ncoding is to directly train the position coding as a trainable \nparameter without designing the position coding formula. \nIf the maximum length of the vector is N and the coding \ndimension is D, then initialize an N √ó D matrix as posi-\ntion vector to update with the training process. The current \nBERT, GPT and other models use this kind of coding. The \nearliest Facebook paper in 2017 used this method [47].\nHowever, for this training absolute position coding, its \ndisadvantage is that it has no scalability. If the maximum \nlength formula of pre training is set to 512, it can only pro-\ncess sentences with a length of 512 at most. There is no \nmatching location information for locations longer than 512.\nThe solution is to randomly initialize the position code of \nmore than 512, and then conduct training fine-tuning.\n2) Trigonometric function is another scheme of position \ncoding, also known as sinusoidal position coding [30]. The \ncoding is calculated by sine function and cosine function in \neven and odd dimensions, respectively, to generate timing \ninformation with periodic changes. The position is embed-\nded in the dimension of length N of the sample sequence. \nWith the increase of the dimension number, the cycle will \nbecome slower and slower. Therefore, the model contain-\ning position texture information is generated in even and \nodd dimensions, as shown in Fig.¬† 8. Thus, the dependency \nbetween positions and the timing characteristics of natural \nlanguage can be learned.\nIt can be seen from Fig.¬† 8 that the position coding of \ntrigonometric function is characterized by the periodic gen-\neration law according to the time sequence. However, with \nthe increase of dimension serial number, the periodic change \n(21)ezillegal ‚Üí 0\nwill be slower and slower, which leads to the decline of the \ndiscrimination of position information for the input of long \ntext in multiple rounds of dialogue.\n3) Theoretically, the reason why the RNN model does \nnot need location coding is that it naturally has the possi-\nbility of learning location information. Therefore, assum-\ning that a layer of RNN is added before the input word \nvector enters the model, and then input into the trans-\nformer module, the location information can be obtained \ntheoretically, and the location coding is no longer needed. \nSimilarly, RNN model training can be used to learn abso-\nlute position coding. ICML2020's paper [48] continues to \ndevelop this idea and proposes to model the position cod-\ning by means of differential equation (ODE). This method \nis called FLOATER. FLOATER belongs to recursive call \nmodel, so this differential equation is also called neural \ndifferential equation. In terms of basic theory, recursive \nlocation coding also has better scalability, and it also has \nbetter flexibility than trigonometric location coding. Obvi-\nously, recursive location coding sacrifices a certain degree \nof parallelism and will bring a speed bottleneck in theory.\nIn addition, language model performance improve-\nment techniques other than positional encoding continue \nto develop. Hassan I. Abdalla et¬†al. [49, 50] proposed a \nscheme to improve text classification neural network \nusing BoW, and improve the performance of text recog-\nnition and matching by integrating similarity measures \nwith machine learning models. Internal evaluation of the \nensemble model against the baseline model demonstrates \nFig. 8  Schematic diagram of timing information\n International Journal of Computational Intelligence Systems          (2023) 16:168 \n1 3  168  Page 10 of 17\nthat the above method has good optimization performance \nfor neural networks for NLP.\n3.3  Design of¬†Relative Position Information Coding\nIn this section, we will solve the problem of location infor -\nmation by introducing the design of relative location coding \nrepresentation and detailing this process. A new attention \ncalculation method of relative position coding is introduced \nto replace the absolute position coding of position embed-\nding layer.\nThe disadvantage of absolute position coding is that it \nwill produce remote attenuation. The larger the relative dis-\ntance, the weaker the correlation between the inputs. The \nreason why periodic trigonometric functions appear and \nshow attenuation trend is that the integral from high-fre-\nquency oscillation asymptotically approaches 0.\nThe assumed model is f(‚Ä¶ , xm , ‚Ä¶ , xn, ‚Ä¶) , where, xm,xn \nrepresent the mth and nth input words, respectively. Now, \nwe discuss the generality, and set f  as a scalar function for \ncalculation. This study uses transformer-based attention \nmechanism, so the function f  has the characteristics of total \nsymmetry, that is, for any m and n , there are shown in the \nfollowing equation:\nFull symmetry is the main reason why transformer cannot \nrecognize the position. Specifically, the function naturally \nsatisfies the identity f(x, y) = f(y, x) , so that it is impossible \nto distinguish whether the input is (x, y) or (y, x) from the \nresult.\nTherefore, to break this symmetry, it is necessary to add a \nposition coding information. One feasible scheme is to add a \nposition determined coding vector to each position, as shown \nin the following equation:\nIn general, assuming that all position encoding vectors \nare not the same, full symmetry does not hold. This means \nthat we can use /uni0303.s1f  instead of f  to address the input from \npositional timing information. To simplify the problem, we \nonly consider the position encoding at two positions, m and \nn , and introduce it as a perturbation term, expanding it to \nthe second-order Taylor term, as shown in the following \nequation:\nFrom Eq.¬†(24), item 1 is independent of location, \nitems 2 to 5 only depend on a single location, so they \n(22)f/parenleft.s1‚Ä¶ , xm , ‚Ä¶ , xn, ‚Ä¶ /parenright.s1= f(‚Ä¶ , xn, ‚Ä¶ , xm , ‚Ä¶)\n(23)\n/uni0303.s1f/parenleft.s1‚Ä¶ , xm , ‚Ä¶ , xn, ‚Ä¶ /parenright.s1= f(‚Ä¶ , xm + pm , ‚Ä¶ , xn + pm , ‚Ä¶)\n(24)\nÔøΩf ‚âà f + p‚ä§\nm\nùúïf\nùúïxm\n+ p‚ä§\nn\nùúïf\nùúïxn\n+ 1\n2p\n‚ä§\nm\nùúï2f\nùúïx2\nm\n+ 1\n2p\n‚ä§\nn\nùúï2f\nùúïx2\nn\n+ p‚ä§\nm\nùúï2f\nùúïxm xn\npn\nonly depend on absolute location information, and item \n6 owned the theinteractionitemof pm , pn at the same time, \nrecord it as p‚ä§\nm Ipn , it will be analyzed later, and it is \nexpected to express certain relative position information \non this basis.Suppose I is the identity matrix, at this time \np‚ä§\nm Ip‚ä§\nn = p‚ä§\nm pn =< pm , pn > is the inner product of two posi-\ntion codes. It is hoped that in this simple example, this \nitem represents the relative position information, that is, \nthere is a function g as the following equation:\nHere, pm , pn is a d-dimensional vector, assuming d = 2, \nthen for a two-dimensional vector, it is derived with the \nhelp of the complex number, that is, the vector [x, y] is \nregarded as the complex number x + yi . According to the \nalgorithm of complex number multiplication, get the fol-\nlowing equation:\np‚àó\nn is the conjugate complex of pn . Re represents the real \npart of the complex number. To satisfy Eq.¬†(26), it can be \nassumed that there is a complex number qm‚àín , as shown in \nthe following equation:\nIn this way, taking the real part on both sides and we can \nobtain the following equation:\nTo solve this equation, the exponential form of the com-\nplex number can be used. Suppose pm = rm ei/u1D719m  , p‚àó\nn = rn e‚àíi/u1D719n , \nqm‚àín = Rm‚àín eiŒ¶m‚àí n , then we can get the following equation:\nFor Eq.¬†(28), substitute n = m to get rm\n2 = R0 , rm  is a con-\nstant, set to 1 for simplicity; for the second equation, let \nn = 0, then /u1D719m ‚àí /u1D7190 =Œ¶ m , if /u1D7190 = 0, then  /u1D719m =Œ¶ m, which \nis the same as /u1D719m ‚àí /u1D719n = /u1D719m‚àín . Substitute n = m-1, then \n/u1D719m ‚àí /u1D719m‚àí1 = /u1D7191 . So that \n/braceleft.s1/u1D719m\n/braceright.s1\n is an arithmetic sequence. \nTherefore, the solution of position coding in two-dimen-\nsional case is shown in the following equation:\nBecause the inner product has the characteristic of lin-\near superposition, the higher dimensional even dimensional \nposition coding can be expressed as a combination of mul-\ntiple two-dimensional position codes to obtain formula (39).\nEquation¬†(31) choose /u1D703i = 10000‚àí2i‚àïd , this form has a \ngood property: it makes < pm , pn > tends to 0 when |m‚Äìn| \n(25)‚ü®pm , pn‚ü© = g(m ‚àí n)\n(26)‚ü®pm , pn‚ü© = Re[pm p‚àó\nn]\n(27)pm p‚àó\nn = qm‚àín\n(28)rm rnei(/u1D719m ‚àí/u1D719n) = Rm‚àín eiŒ¶m‚àí n\n(29)\n/braceleft.s3rm rn = Rm‚àín\n/u1D719m ‚àí /u1D719n =Œ¶ m‚àín\n(30)pm = eim/u1D703‚áî pm =\n/parenleft.s2/u1D450/u1D45C/u1D460m/u1D703\n/u1D460/u1D456/u1D45Bm/u1D703\n/parenright.s2\nInternational Journal of Computational Intelligence Systems          (2023) 16:168  \n1 3 Page 11 of 17   168 \ngets larger. The larger the relative distance, the weaker the \ncorrelation. The reason is that the high-frequency oscillation \nintegral gradually tends to 0, i.e., Eq.¬†(32):\nThe general attention with absolute position coding is \nas Eqs.¬†(33)‚Äì(35):\nSoftMax is used to normalize the row dimension j, xi , \np i are line vectors, p i indicates the added location informa-\ntion. Preliminary expand qikT\nj  , the expansion is shown in \nthe following equation:\nTo introduce relative position information, the structure \nof Eq.¬†(36) is modified to the following equation:\nThat is, remove the position of the first item and the \nposition information of second item piW K  is changed to \nbinary position vector R K\ni,j . Then, expand the following \nequation:\n(31)pm =\n‚éõ\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú‚éù\neim/u1D7030\neim/u1D7031\n‚Ä¶\ne\nim/u1D703\nd‚àï2\n‚àí1\n‚éû\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü‚é†\n‚áî pm =\n‚éõ\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú‚éù\ncosm/u1D7030\nsinm/u1D7030\ncosm/u1D7031\nsinm/u1D7031\n‚Ä¶\ncosm/u1D703d‚àï2‚àí1\nsinm/u1D703d‚àï2‚àí1\n‚éû\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü‚é†\n‚ü®pm ,pn‚ü© = ReÔøΩei(m‚àín )/u1D7030 + ei(m‚àín )/u1D7031 + ‚ãØ+ ei(m‚àín )/u1D703d‚àï2‚àí1\nÔøΩ\n= d\n2 Re\n/bracketleft.s4d‚àï2‚àí1/uni2211.s1\ni=0\nei(m‚àín )10000‚àí2i‚àïd 1\nd‚àï2\n/bracketright.s4\n(32)‚àº d\n2 Re\n/bracketleft.s3\n/uni222B.dsp\n1\n0\nei(m‚àín)10000‚àít\ndt\n/bracketright.s3\n(33)\n‚éß\n‚é™\n‚é®\n‚é™‚é©\nqi = ÔøΩxi+ pi\nÔøΩW Q\nki = ÔøΩxi+ pi\nÔøΩW K\nvi = ÔøΩxi+ pi\nÔøΩW V\n(34)ai,j = softmax\n/parenleft.s2\nqikT\nj\n/parenright.s2\n(35)O i =\n/uni2211.s1\nj\nai,jvi\n(36)\nqikT\nj = /parenleft.s1xi + pi\n/parenright.s1W Q W T\nK\n/parenleft.s1xi + pi\n/parenright.s1T\n= /parenleft.s1xiW Q + piW Q\n/parenright.s1/parenleft.s2\nW T\nK xT\nj + W T\nK pT\nj\n/parenright.s2\n(37)ai,j = softmax(xiW Q (xjW K + RK\ni,j)\nT\n)\nChange piW V  to R V\ni,j , get the following equation:\nAs can be seen from the formula above, the so-called rela-\ntive position is to change the vector that originally depends \non binary coordinates (i, j) to only depend on the relative \nposition distance i ‚àí j , and usually needs to be truncated to \nadapt to different arbitrary distances. Therefore, the expres-\nsion of R K\ni,j. is shown in Eqs.¬†(40) and (41):\nThrough the above modification, although only a limited \nnumber of position coding information are obtained, the \nrelative position of any length can be expressed, pK , pV is \nthe relative position code of trigonometric function formula. \nThe specific definition of relative position code is shown in \nEqs.¬†(42) and (43):\n4  Experiments and¬†Results\nThe purpose of this experiment is to test the role of relative \nposition coding in long-distance multi-round conversation \nscenarios, and to verify whether relative coding is better \nthan absolute position coding in slowing down the decline \nof long-distance information.\n4.1  Data Set and¬†Environment\nThe pre-training of all models in this experiment used the \nLCCC corpus [36] as the training data set. In this study, the \nChinese dialogue data set STC (short text conversation) [51] \nis selected for evaluation experiments, and the Chinese gen-\nerative dialogue system and some classical dialogue baseline \nmodels are compared.\nThe short text dialogue corpus STC published by 2015 \nHuawei Noah's Ark laboratory is required to predict the \nreply under a given number of rounds of contextual dialogue \n(38)O i =\n/uni2211.s1\nj\nai,jvi =\n/uni2211.s1\nj\nai,j(xjW V + piW V )\n(39)O i =\n/uni2211.s1\nj\nai,j(xjW V + RV\ni,j)\n(40)R K\ni,j = pK [clip(i‚àí j, pmin , pmax )]\n(41)R V\ni,j = pV [clip(i‚àí j, pmin , pmax )]\n(42)p ij[2k]=/u1D460/u1D456/u1D45B(j ‚àí i\n10000\n2k\ndz\n)\n(43)p ij[2k + 1]= /u1D450/u1D45C/u1D460(j ‚àí i\n10000\n2k\ndz\n)\n International Journal of Computational Intelligence Systems          (2023) 16:168 \n1 3  168  Page 12 of 17\ncorpus. The data set contains 4.43 million Chinese dia-\nlogues. Each post has an average of 20 different replies, and \nthere is a semantic gap between each reply, which is the \nmain difference from the traditional parallel data set.\nThis section mainly uses the division method in STC \nand selects its test data as the test data set for this study. \nCompared with the labeled data set, the test data are char -\nacterized by sparse roles. As there is no labeled data set, \nthe whole data set is manually labeled, and the tagger will \nactively guide the topic to talk about the characterization \ninformation. Therefore, most of the responses in the data set \nare characterization-related. However, in a real conversation \nscenario, it often does not involve a lot of characterization \ninformation in every chat. From this point of view, the test \ndata set is closer to the characterization conversation in a \nreal man‚Äìmachine conversation.\nThe experimental environment used for the experiment \nare some open-source frameworks, including TensorFlow \n2.1.0, keras2.3.1, bert4Keras0.9.8. Specifically, the experi-\nmental operation environment is shown in Table¬†2.\n4.2  Experimental Evaluation Criteria\nIn this study, we first use the accuracy of generated tags \n(ACC) to evaluate the effectiveness of relative position cod-\ning, and then use objective and subjective evaluation meth-\nods for the final model.\nIn terms of objective evaluation indicators, we apply sev-\neral commonly used dialogue evaluation methods, including \nPPL (perplexity score) [52], BLEU [53], Greedy Matching \n[54‚Äì56] and Embedded Average [57]. Some experiments \nshow that the evaluation methods based on word embedding \nhave higher correlations with human [58].\nIn the manual evaluation index [59], Grammatical and \nSemantic Continuity, Context Relevance and amount of \ninformation are used. 200 replies were sampled for each \nmodel, and 2 marked students were invited to manually \nevaluate these replies.\n4.3  Performance of¬†Chinese Dialogue System Based \non¬†Transformer\nIn this experiment, the model features a 12-layer trans-\nformer decoder architecture, with individual characters as \nthe smallest unit for word embeddings. It employs 12 multi-\nhead attention heads, a vocabulary size of 13,088, character \nvector dimensions set at 384, and a maximum context length \nof 256 characters. The batch size used is 16. For training \noptimization, the dialogue system utilizes the Adam opti-\nmizer with a primary learning rate of 2 √ó 10‚àí5 . The training \nis conducted over 100 epochs on the LCCC-base data set.\nTo validate the effectiveness of the proposed generative \ndialogue system in this study, three methods are introduced \nfor comparison with the baseline model: Attn-Seq2Seq, \ntransformer, and GPT. The baseline model is described as \nfollows:\nAttn-Seq2Seq [36]: This model is based on the tradi-\ntional Seq2Seq architecture with the addition of an attention \nmechanism. It also employs a multi-turn dialogue approach \nto concatenate multiple segments, similar to the dialogue \nsystem in this chapter. It uses the < CLS > and < SEP > iden-\ntifiers to segment the segments, and these specific identifi-\ners have corresponding word embeddings. The concatenated \nhistory dialogue is encoded and decoded using a bidirec -\ntional LSTM as the basic unit for Seq2Seq.\nTransformer [60]: The transformer model serves as the \nfoundational architecture. This model has found broad appli-\ncations in machine translation and dialogue generation. To \nensure fairness in training the transformer model, a 12-layer \ntransformer is used and trained for 100 epochs on the LCCC-\nbase data set.\nGPT-chatbot [61]: This model is based on the GPT2 \narchitecture for generative dialogue. Each training data \nare \"sequentially concatenated\" following the approach of \nMicrosoft's DialoGPT. The concatenated data are then input \ninto the network for training. The model consists of 12 lay-\ners of GPT and is trained for 100 epochs on the LCCC-base \ndata set.\nAll transformer-based models share the same parameter \nsettings for the encoder and decoder structures. They are \nessentially like the GPT model parameters. The vocabulary \nsize is 13,088, word vector dimensions are 384, the maxi-\nmum context length is 256, and the batch size is 16.\nBy conducting experiments, the objective evaluation \nindex experimental results as shown in Table¬†3 are obtained:\nTable¬†3 displays the results of different language mod-\nels in terms of perplexity (PPL), BLEU-2, BLEU-4, Dist-1, \nand Dist-2. A lower perplexity indicates smoother sentence \nTable 2  Experimental \nenvironment Processer Intel(R) Core(TM) i7-9800X CPU @ 3.80¬†GHz\nMemory 64¬†GB\nGraphics card NVIDIA GeForce GTX2080Ti\nOperating system Ubuntu 18.04.3 LTS\nDevelopment environment Pycharm + Anaconda\nOpen source framework TensorFlow2.1.0„ÄÅKeras2.3.1„ÄÅBERT4Keras0.9.8\nInternational Journal of Computational Intelligence Systems          (2023) 16:168  \n1 3 Page 13 of 17   168 \ngeneration and better relevance to the topic. The best PPL \nis achieved by the dialogue model in this chapter, which, \nalthough only slightly outperforms the GPT2-chatbot, still \nholds an advantage. This can be attributed to the effec-\ntiveness of adding partial masking. In terms of perplexity \nperformance, it surpasses other language models to some \nextent.\nIn BLEU-2 and BLEU-4 evaluations, this language model \nperforms best in BLEU-4 but falls short of transformer in \nBLEU-2. This difference can be explained by the fact that \nBLEU was originally developed as an evaluation metric for \ntranslation, and it tends to favor shorter translation results. It \nmay not handle morphologically rich sentences well, mak -\ning it less friendly for generative dialogue models. Finally, \nin terms of diversity as measured by Dist-1 and Dist-2, this \nexperimental model outperforms all baseline models.\nIn the human evaluation metrics, 200 responses were \nsampled for each model, and two annotators were invited to \nconduct manual evaluations on these responses. The results \nof the subjective evaluation metrics are presented in Table¬†4 \nas follows:\nThe experimental results from the manual subjective \nevaluations demonstrate that the dialogue system designed \nin this study outperforms the baseline models across all \nthree metrics. The dialogue system in this chapter is capable \nof generating high-quality dialogue responses. It not only \nproduces responses with sufficient information but also \nmaintains good fluency in the sentences and relevance to the \ncontext. It surpasses other baseline models, confirming the \neffectiveness of the generative dialogue system developed \nin this research.\n4.4  Relative Position Encoding Performance \nVerification\nTo test whether the relative position coding can slow down \nthe weakness of long-distance information, this segment \nselects the label data set and test data set of multi-round \ndialogue test data set STC for experiments.\nIn this section, we choose the dialogue system imple-\nmented in segment 3.1 as baseline. Then, multiple models \nare used for comparative experiments. Finally, the two opti-\nmization methods of relative position coding in this paper \nand word-character fusion [62] embedding are applied to the \ngenerative dialogue system at the same time, and compared \nwith the baseline model, as shown in Table¬†5.\nFirst, in the model of this section using relative position \ncoding, the results under different conditions are verified \nby increasing the sequence length and changing the size of \nbatch size. As shown in Table¬†6\nAt the same time, the attentional decline trend images at \ndifferent relative distances are expressed. The attentional \ndecline results using different /u1D703t are shown in Figs.¬† 9 and \n10 below.\nexcept /u1D703t = t is abnormal and intersects with the x -axis, \nother trends are basically the same. The power function \ndecreases faster in a short distance, while the exponential \nfunction decreases faster in a long distance. Therefore, \nchoosing /u1D703t = 10000‚àít is a compromise.\nThrough the comparative experiments of several models, \nthe objective evaluation index experimental results as shown \nTable 3  Objective evaluation index experimental results\nModel PPL BLEU-2 BLEU-4 Dist-1 Dist-2\nAttn-Seq2Seq 37.23 4.51 0.94 8.5 11.94\nTransformer 22.30 6.72 3.14 8.8 12.11\nGPT-chatbot 20.52 5.69 2.78 8.1 11.73\nOurs 19.83 6.63 3.20 9.2 12.68\nTable 4  Subjective evaluation results\nModel Syntax-semantic \ncoherence\nContextual \nrelevance\nInfor-\nmation \namount\nAttn-Seq2Seq 1.13 0.92 1.17\nTransformer 1.34 1.15 1.39\nGPT-chatbot 1.59 1.20 1.40\nOurs 1.64 1.33 1.42\nTable 5  Experimental model \nsetup Baseline Char-word Model of this study\nToken unit Chinese character Word-character fusion Chinese character\nLocation coding Absolute position Absolute position Relative position\nTable 6  Model generates labels under different sequence lengths and \nbatch sizes\nMaxlen Batch size Loss ACC \n1 128 16 1.86 63.4%\n2 256 16 1.78 64.6%\n3 256 32 1.75 64.8%\n4 512 16 1.66 66.2%\n International Journal of Computational Intelligence Systems          (2023) 16:168 \n1 3  168  Page 14 of 17\nin Table¬†7 are obtained. The experimental results of subjec-\ntive evaluation indicators are shown in Table¬†8.\nIt can be seen from the experimental results that \nthe final dialogue system using word-character fusion \nEmbedding and relative position coding achieves the best \nresults in all indicators.\n5  Discussion\nThrough an analysis and comparison of the labels generated \nby the models under varying sequence lengths and batch \nsizes, some valuable insights can be gleaned. Specifically, it \nbecomes evident that increasing the length of the sequence \nFig. 9  Attentional decline \nresults of different /u1D703t(short \ndistance trend)\nFig. 10  Attentional decline \nresults of different /u1D703t(long \ndistance trend)\nTable 7  Objective evaluation \nindex experimental results Models PPL BLEU-2 BLEU-4 Greedy Matching Embed-\nding \nAverage\nBaseline 19.83 6.63 3.20 65.89 78.94\nChar-word 19.79 6.72 3.81 66.34 84.12\nModel of this paper 18.56 6.90 4.12 66.28 86.13\nTable 8  Labor evaluation index \nexperiment results Models PPL BLEU-2 BLEU-4 Greedy Matching Embed-\nding \nAverage\nBaseline 19.83 6.63 3.20 65.89 78.94\nChar-word 19.79 6.72 3.81 66.34 84.12\nModel of this paper 18.56 6.90 4.12 66.28 86.13\nInternational Journal of Computational Intelligence Systems          (2023) 16:168  \n1 3 Page 15 of 17   168 \nleads to improved accuracy and smaller loss values. It indi-\ncates that relative position encoding has better processing \npower for long text sequences compared to absolute position \nencoding, which aligns with the anticipated outcomes of the \ndesign involving relative position coding.\nIn terms of objective performance indicators, the \nmodel presented in this paper outperforms the baseline \nand character models in metrics, such as bleu-2, bleu-4, \nand embedding average. In addition, it outperforms the \nbaseline model in greedy matching and perplexity (ppl), \nbut lags somewhat behind in char word.\nAlthough the model performs admirably in terms of gram-\nmatical and semantic coherence during manual evaluation, \nthere is still a definite discrepancy in terms of contextual \nrelevance and the amount of information when compared \nto the reference response. This emphasizes the significance \nof future research projects focused on context analysis and \nmaintaining logical coherence in text generation.\nThese findings also demonstrate relative position cod-\ning's improved ability to handle lengthy text sequences as \ncompared to absolute position coding. This observation \nunderscores the effectiveness of the optimization approach \nemployed in this study. In summary, this work showcases \nthe proposal to incorporate relative position information into \nthe self-attention formula within the transformer module, \nthereby enhancing long-distance attention mechanisms.\n6  Conclusions\nThe main focus of this study is to theoretically present a \ndesign framework for a generative dialogue system based \non the transformer architecture tailored for Chinese text. \nThe use of transformer technology serves as the founda-\ntional framework. To address the limitation of unidirec-\ntional generation in language sequences and enable bidi-\nrectional access to contextual information within input \nsentences, the application of partial masking is introduced.\nThe study also introduces training and optimization tech-\nniques for the dialogue system, including teacher forcing and \nbeam search, along with model pretraining on the LCCC \ndata set. Comparative analyses are conducted against various \nbaseline models, such as Attn-Seq2Seq, transformer, and \nGPT-chatbot to validate the effectiveness of this dialogue \nsystem in generating Chinese generative dialogues.\nSubsequently, the paper addresses the issue of text \nlength limitations associated with common absolute posi-\ntion encoding. Building upon relative position encoding, \nthe paper proposes a novel technique for relative position \nencoding tailored for Chinese text. In experiments, the \ntransformer-based Chinese text generation dialogue model \ndeveloped in this paper is used as the baseline model. \nThe test data set in the short text dialogue corpus STC \nreleased by Huawei's Noah's Ark Laboratory was selected \nas the test data for the research task. Model performance is \nevaluated using both absolute and relative position encod-\ning. Experimental results demonstrate the feasibility of \nenhancing the system's ability to mitigate the phenomenon \nof long-distance information decay by introducing rela-\ntive position encoding. Modification of the self-attention \ncalculation formula within the transformer module, by \nincorporating relative position information to replace the \nabsolute position encoding in the embedding layer, results \nin enhanced long-range attention capabilities.\nIn summary, this study's primary contributions lie in \noffering a theoretical framework for designing a genera-\ntive dialogue system for Chinese text, and it introduces a \nnovel approach for relative position encoding to address \ntext length limitations. Experimental findings support the \neffectiveness of this approach in mitigating long-distance \ninformation decay, achieved through adjustments to the \nself-attention mechanism within the transformer module.\nAcknowledgements The authors wish to thank the anonymous review-\ners for their helpful comments.\nAuthor contributions Conceptualization: WZ and LY; methodology: \nGG, RW and JT; formal analysis and investigation: XL, SL and ZY; \nwriting‚Äîoriginal draft preparation: LY, GG, JT and ZY; writing‚Äî\nreview and editing: WZ, LY and JT; software: XL, GG and ZY; data \ncuration: GG and RW; visualization: XL; resources: SL; supervision: \nWZ; project administration: LY; funding acquisition: WZ. All authors \nhave read and approved the final manuscript.\nFunding Supported by Sichuan Science and Technology Program \n(2021YFQ0003, 2023YFSY0026, 2023YFH0004).\nData availability The data sets presented during the current study are \navailable from the corresponding author on reasonable request.\nDeclarations \nConflict of interest The authors declare that they have no relevant fi-\nnancial or non-financial interests to disclose.\nEmployment Not applicable.\nEthical approval The Ethic statement is not applicable. This study does \nnot include any animal or human studies.\nInformed consent statement Not applicable.\nInstitutional review board statement Not applicable.\nConsent for publication Not applicable.\nOpen Access  This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \n International Journal of Computational Intelligence Systems          (2023) 16:168 \n1 3  168  Page 16 of 17\nthe article's Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\n 1. Mateju, L., Griol, D., Callejas, Z., Molina, J.M., Sanchis, A.: An \nempirical assessment of deep learning approaches to task-oriented \ndialog management. Neurocomputing 439(June), 327‚Äì339 (2021). \nhttps:// doi. org/ 10. 1016/j. neucom. 2020. 01. 126\n 2. Ni, J., Young, T., Pandelea, V., Xue, F., Cambria, E.: Recent \nadvances in deep learning based dialogue systems: a systematic \nsurvey. Artif. Intell. Rev. 56(4), 3055‚Äì3155 (2023). https:// doi.  \norg/ 10. 1007/ s10462- 022- 10248-8\n 3. Lauriola, I., Lavelli, A., Aiolli, F.: An introduction to deep learn-\ning in natural language processing: models, techniques, and tools. \nNeurocomputing 470(January), 443‚Äì456 (2022). https:// doi. org/ \n10. 1016/j. neucom. 2021. 05. 103\n 4. Zhu X (2022) ‚ÄúRNN Language Processing Model-Driven Spoken \nDialogue System Modeling Method.‚Äù Edited by Xin Ning. Com-\nputational Intelligence and Neuroscience 2022 (February): 1‚Äì9. \nhttps:// doi. org/ 10. 1155/ 2022/ 69935 15.\n 5. Park, Y., Ko, Y., Seo, J.: BERT-based response selection in dia-\nlogue systems using utterance attention mechanisms. Expert Syst. \nAppl. 209(December), 118277 (2022). https:// doi. org/ 10. 1016/j. \neswa. 2022. 118277\n 6. Junaid T, Sumathi D, Sasikumar AN, Suthir S, Manikandan \nJ, Rashmita K, Kuppusamy PG, Janardhana Raju M (2022) A \ncomparative analysis of transformer based models for figurative \nlanguage classification. Comput Electr Eng 101 (July): 108051. \nhttps:// doi. org/ 10. 1016/j. compe leceng. 2022. 108051\n 7. Li, J., Joe Qin, S.: Applying and dissecting LSTM neural net-\nworks and regularized learning for dynamic inferential modeling. \nComput. Chem. Eng. 175(July), 108264 (2023). https:// doi. org/  \n10. 1016/j. compc hemeng. 2023. 108264\n 8. Sherstinsky, A.: Fundamentals of recurrent neural network \n(RNN) and long short-term memory (LSTM) network. Physica \nD 404(March), 132306 (2020). https:// doi. org/ 10. 1016/j. physd. \n2019. 132306\n 9. Weerakody, P.B., Wong, K.W., Wang, G.: Policy gradient empow-\nered LSTM with dynamic skips for irregular time series data. \nAppl. Soft Comput. 142(July), 110314 (2023). https:// doi. org/ 10. \n1016/j. asoc. 2023. 110314\n 10. Zhang, X., Shi, J., Yang, M., Huang, X., Usmani, A.S., Chen, G., \nJianmin, Fu., Huang, J., Li, J.: Real-time pipeline leak detection \nand localization using an attention-based LSTM approach. Pro-\ncess. Saf. Environ. Prot. 174(June), 460‚Äì472 (2023). https:// doi.  \norg/ 10. 1016/j. psep. 2023. 04. 020\n 11. Sutskever I, Vinyals O (2014) Sequence to sequence learning with \nneural networks. Adv Neural Inform Process Syst\n 12. Bahdanau D, Cho K, Bengio Y (2014) Neural machine translation \nby jointly learning to align and translate. Computer Science\n 13. Li, J., Chen, R., Huang, X.: A sequence-to-sequence remaining \nuseful life prediction method combining unsupervised LSTM \nencoding-decoding and temporal convolutional network. Meas. \nSci. Technol. 33(8), 085013 (2022). https:// doi. org/ 10. 1088/  \n1361- 6501/ ac632d\n 14. Liang, Z., Junping, Du., Li, C.: Abstractive social media text \nsummarization using selective reinforced Seq2Seq attention \nmodel. Neurocomputing 410(October), 432‚Äì440 (2020). https://  \ndoi. org/ 10. 1016/j. neucom. 2020. 04. 137\n 15. Britz D, Goldie A, Luong M-T, Quoc L (2017) Massive Explo-\nration of Neural Machine Translation Architectures. arXiv.\n 16. Chorowski J, Bahdanau D, Serdyuk D, Cho K, Bengio Y (2015) \nAttention-Based Models for Speech Recognition. ArXiv.Org. \nJune 24, 2015\n 17. Shen, Y.: Bionic communication network and binary pigeon-\ninspired optimization for multiagent cooperative task allocation. \nIEEE Trans. Aerosp. Electron. Syst. 58(5), 3946‚Äì3961 (2022). \nhttps:// doi. org/ 10. 1109/ TAES. 2022. 31576 60\n 18. Lv, H., Chen, J., Pan, T., Zhang, T., Feng, Y., Liu, S.: Atten -\ntion mechanism in intelligent fault diagnosis of machinery: a \nreview of technique and application. Measurement 199(August), \n111594 (2022). https:// doi. org/ 10. 1016/j. measu rement. 2022. \n111594\n 19. Shi, Q., Fan, J., Wang, Z., Zhang, Z.: Multimodal channel-wise \nattention transformer inspired by multisensory integration \nmechanisms of the brain. Pattern Recogn. 130(October), 108837 \n(2022). https:// doi. org/ 10. 1016/j. patcog. 2022. 108837\n 20. Zhang, X., Yawen, Wu., Zhou, P., Tang, X., Jingtong, Hu.: \nAlgorithm-hardware co-design of attention mechanism on \nFPGA devices. Acm Trans Embedded Comput Syst 20(5), 71 \n(2021). https:// doi. org/ 10. 1145/ 34770 02\n 21. Ni, J., Huang, Z., Chang, Yu., Lv, D., Wang, C.: Comparative \nconvolutional dynamic multi-attention recommendation model. \nIeee Trans Neural Netw Learn Syst 33(8), 3510‚Äì3521 (2022). \nhttps:// doi. org/ 10. 1109/ TNNLS. 2021. 30532 45\n 22. Chen, J., He, Ye.: A novel u-shaped encoder‚Äìdecoder network \nwith attention mechanism for detection and evaluation of road \ncracks at pixel level. Comput-Aid Civ Infrastruct Eng 37(13), \n1721‚Äì1736 (2022). https:// doi. org/ 10. 1111/ mice. 12826\n 23. Du, S., Li, T., Yang, Y., Horng, S.-J.: Multivariate time series \nforecasting via attention-based encoder‚Äìdecoder framework. \nNeurocomputing 388(May), 269‚Äì279 (2020). https:// doi. org/ \n10. 1016/j. neucom. 2019. 12. 118\n 24. Feng, L., Zhao, C., Sun, Y.: Dual attention-based encoder‚Äì\ndecoder: a customized sequence-to-sequence learning for soft \nsensor development. IEEE Trans Neural Netw Learn Syst 32(8), \n3306‚Äì3317 (2021). https:// doi. org/ 10. 1109/ TNNLS. 2020. 30159 \n29\n 25. Mikolov T (2012) Statistical language models based on neural \nnetworks. PhD thesis, Brno University of Technology\n 26. Schuster, M., Paliwal, K.: Bidirectional recurrent neural net-\nworks. IEEE Trans. Signal Process. 45(11), 2673‚Äì2681 (1997)\n 27. Sundermeyer, M., Schluter, R.: From feedforward to recurrent \nLSTM neural networks for language modeling. IEEE/ACM \nTrans Audio Speech Lang Process 23(3), 517‚Äì529 (2015)\n 28. Zhu, S., Cheng, X., Sen, Su.: Knowledge-based question \nanswering by tree-to-sequence learning. Neurocomputing \n372(January), 64‚Äì72 (2020). https:// doi. org/ 10. 1016/j. neucom. \n2019. 09. 003\n 29. Liu T, Wang K, Sha L, Chang B, Sui Z Table-to-text generation \nby structure-aware Seq2seq learning. proceedings of the AAAI \nconference on artificial intelligence 32 https:// doi. org/ 10. 1609/ \naaai. v32i1. 11925 (2018).\n 30. Vaswani A, Shazeer N, Parmar N et¬†al. (2017) Attention is all \nyou need. In: Advances in neural information processing systems, \npages 5998‚Äì6008\n 31. Niu, Z., Zhong, G., Hui, Yu.: A review on the attention mechanism \nof deep learning. Neurocomputing 452(September), 48‚Äì62 (2021). \nhttps:// doi. org/ 10. 1016/j. neucom. 2021. 03. 091\n 32. Qun, He., Wenjing, L., Zhangli, C.: B&Anet: combining bidirec-\ntional LSTM and self-attention for end-to-end learning of task-\noriented dialogue system. Speech Commun. 125(December), \n15‚Äì23 (2020). https:// doi. org/ 10. 1016/j. specom. 2020. 09. 005\n 33. Beltagy Iz, Matthew EP, Cohan A (2020) Longformer: The Long-\nDocument Transformer. arXiv.\n 34. Shan, W., Huang, D., Wang, J., Zou, F., Li, S.: Self-attention \nbased fine-grained cross-media hybrid network. Pattern Recogn. \nInternational Journal of Computational Intelligence Systems          (2023) 16:168  \n1 3 Page 17 of 17   168 \n130(October), 108748 (2022). https:// doi. org/ 10. 1016/j. patcog. \n2022. 108748\n 35. Dufter, P., Schmitt, M., Sch√ºtze, H.: Position information in trans-\nformers: an overview. Comput. Linguist. 48(3), 733‚Äì763 (2022). \nhttps:// doi. org/ 10. 1162/ coli_a_ 00445\n 36. Yida W, Ke P, Zheng Y, Huang K, Jiang Y, Zhu X, Huang M \n(2020) A large-scale chinese short-text conversation dataset. In: \nPaper presented at the Natural Language Processing and Chinese \nComputing, Cham. https:// doi. org/ 10. 1007/ 978-3- 030- 60450-9_8\n 37. Abdalla, H.I., Amer, A.A., Amer, Y.A., et¬†al.: Boosting the item-\nbased collaborative filtering model with novel similarity meas-\nures. Int J Comput Intell Syst 16, 123 (2023). https:// doi. org/ 10. \n1007/ s44196- 023- 00299-2\n 38. Amer AA, Abdalla HI, Nguyen L (2021) Enhancing recommenda-\ntion systems performance using highly-effective similarity meas-\nures. Knowl-Based Syst 217: 106842. https:// doi. org/ 10. 1016/j. \nknosys. 2021. 106842\n 39. Liu, Z., Liu, H., Jia, W., Zhang, D., Tan, J.: A multi-head neural \nnetwork with unsymmetrical constraints for remaining useful life \nprediction. Adv. Eng. Inform. 50(October), 101396 (2021). https:// \ndoi. org/ 10. 1016/j. aei. 2021. 101396\n 40. Reza, S., Ferreira, M.C., Machado, J.J.M., Jo√£o, M.R., Tavares, \nS.: A multi-head attention-based transformer model for traffic \nflow forecasting with a comparative analysis to recurrent neural \nnetworks. Expert Syst. Appl. 202 (September), 117275 (2022). \nhttps:// doi. org/ 10. 1016/j. eswa. 2022. 117275\n 41. Zhang L, Wang C-C, Chen X (2022) Predicting Drug-target bind-\ning affinity through molecule representation block based on multi-\nhead attention and skip connection. Briefings Bioinform 23(6): \nbbac468. https:// doi. org/ 10. 1093/ bib/ bbac4 68.\n 42. Zheng, W., Yin, L.: Characterization inference based on joint-\noptimization of multi-layer semantics and deep fusion matching \nnetwork. PeerJ Comput Sci 8(April), e908 (2022). https:// doi. org/ \n10. 7717/ peerj- cs. 908\n 43. Zheng, W., Zhou, Yu., Liu, S., Tian, J., Yang, Bo., Yin, L.: A deep \nfusion matching network semantic reasoning model. Appl. Sci. \n12(7), 3416 (2022). https:// doi. org/ 10. 3390/ app12 073416\n 44. Atta, E.A., Ali, A.F., Elshamy, A.A.: A modified weighted chimp \noptimization algorithm for training feed-forward neural network \nEdited by Kathiravan Srinivasan. PLoS ONE 18(3), e0282514 \n(2023). https:// doi. org/ 10. 1371/ journ al. pone. 02825 14\n 45. Ma, Z., Zheng, W., Chen, X., Yin, L.: Joint embedding VQA \nmodel based on dynamic word vector. PeerJ Computer Science \n7(March), e353 (2021). https:// doi. org/ 10. 7717/ peerj- cs. 353\n 46. Zong, Yi., Pan, E.: A SOM-based customer stratification model. \nWirel. Commun. Mob. Comput. 2022(March), e7479110 (2022). \nhttps:// doi. org/ 10. 1155/ 2022/ 74791 10\n 47. Gehring J, Auli M, Grangier D, Yarats D, Dauphin YN. (2017) \nConvolutional Sequence to Sequence Learning. In: Proceedings of \nthe 34th international conference on machine learning, 1243‚Äì52. \nPMLR.\n 48. Liu X, Yu H-F, Dhillon I, Hsieh C-J (2020) Learning to encode \nposition for transformer with continuous dynamical model. In: \nProceedings of the 37th international conference on machine \nlearning, 6327‚Äì35. PMLR.\n 49. Abdalla HI, Amer AA (2022) On the integration of similar -\nity measures with machine learning models to enhance text \nclassification performance. Inform Sci 614: 263‚Äì288. https:// doi. \norg/ 10. 1016/j. ins. 2022. 10. 004\n 50. Abdalla HI, Amer AA, Ravana SD (2023) BoW-based neural net-\nworks vs. cutting-edge models for single-label text classification. \nNeural Comput Appl 35(27): 20103‚Äì20116. https:// doi. org/ 10.  \n1007/ s00521- 023- 08754-z\n 51. Shang L, Lu Z, Li H (2015) Neural responding machine for short-\ntext conversation. arXiv. https:// doi. org/ 10. 48550/ arXiv. 1503. \n02364.\n 52. Vinyals O, Le Q (2015) A neural conversational model. arXiv.\n 53. Papineni K, Roukos S, Ward T, Zhu W-J (2002) Bleu: a method \nfor automatic evaluation of machine translation. In: Proceedings \nof the 40th annual meeting of the association for computational \nlinguistics, 311‚Äì18. Philadelphia, Pennsylvania, USA: Association \nfor Computational Linguistics. https:// doi. org/ 10. 3115/ 10730 83. \n10731 35.\n 54. Corley C, Mihalcea R (2005) Measuring the Semantic Similar -\nity of Texts. In: Proceedings of the ACL workshop on empirical \nmodeling of semantic equivalence and entailment, 13‚Äì18. Ann \nArbor, Michigan: Association for Computational Linguistics.\n 55. Lintean M, Rus V (2012) Measuring semantic similarity in short \ntexts through greedy pairing and word semantics. In: Proceed-\nings of the twenty-fifth international FLAIRS conference, Marco \nIsland, FL, USA, 23‚Äì25 May\n 56. Yadav, S., Kaushik, A.: Do you ever get off track in a conversa-\ntion? the conversational system‚Äôs anatomy and evaluation metrics. \nKnowledge 2(1), 55‚Äì87 (2022). https:// doi. org/ 10. 3390/ knowl \nedge2 010004\n 57. Wieting J, Bansal M, Gimpel K, Livescu K (2016) Towards Uni-\nversal Paraphrastic Sentence Embeddings. arXiv.\n 58. Zhong, S.-H., Liu, P., Ming, Z., Liu, Y.: How to evaluate single-\nround dialogues like humans: an information-oriented metric. \nIEEE/ACM Trans Audio Speech Lang Process 28, 2211‚Äì2223 \n(2020). https:// doi. org/ 10. 1109/ TASLP. 2020. 30038 64\n 59. Zhang, C., Lee, G., D‚ÄôHaro, L.F., Li, H.: D-score: holistic dia-\nlogue evaluation without reference. IEEE/ACM Trans Audio \nSpeech Lang Process 29, 2502‚Äì2516 (2021). https:// doi. org/ 10. \n1109/ TASLP. 2021. 30740 12\n 60. Oluwatobi O, Mueller E (2020). DLGNet: A transformer-based \nmodel for dialogue response generation. In: Proceedings of the \n2nd workshop on natural language processing for conversational \nAI\n 61. Zhang Y, Sun S, Galley M, Chen Y-C, Brockett C, Gao X, Gao \nJ, Liu J, Dolan B (2019) Dialogpt: Large-scale generative pre-\ntraining for conversational response generation. arXiv preprint \narXiv: 1911. 00536.\n 62. Luo J, Zou X, Hou M (2022) A novel character-word fusion chi-\nnese named entity recognition model based on attention mecha-\nnism. In: 2022 IEEE 5th international conference on computer and \ncommunication engineering technology (CCET)\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations."
}