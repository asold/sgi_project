{
  "title": "Analyzing and Improving Statistical Language Models for Speech Recognition",
  "url": "https://openalex.org/W1843513176",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Ueberla, Joerg P.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1920769845",
    "https://openalex.org/W1524975733",
    "https://openalex.org/W1991133427",
    "https://openalex.org/W2993383518",
    "https://openalex.org/W1959142382",
    "https://openalex.org/W2083398114",
    "https://openalex.org/W1504879464",
    "https://openalex.org/W80259412",
    "https://openalex.org/W2099321136",
    "https://openalex.org/W2116708935",
    "https://openalex.org/W2084531783",
    "https://openalex.org/W3045215625",
    "https://openalex.org/W2168325069"
  ],
  "abstract": "In many current speech recognizers, a statistical language model is used to indicate how likely it is that a certain word will be spoken next, given the words recognized so far. How can statistical language models be improved so that more complex speech recognition tasks can be tackled? Since the knowledge of the weaknesses of any theory often makes improving the theory easier, the central idea of this thesis is to analyze the weaknesses of existing statistical language models in order to subsequently improve them. To that end, we formally define a weakness of a statistical language model in terms of the logarithm of the total probability, LTP, a term closely related to the standard perplexity measure used to evaluate statistical language models. We apply our definition of a weakness to a frequently used statistical language model, called a bi-pos model. This results, for example, in a new modeling of unknown words which improves the performance of the model by 14% to 21%. Moreover, one of the identified weaknesses has prompted the development of our generalized N-pos language model, which is also outlined in this thesis. It can incorporate linguistic knowledge even if it extends over many words and this is not feasible in a traditional N-pos model. This leads to a discussion of whatknowledge should be added to statistical language models in general and we give criteria for selecting potentially useful knowledge. These results show the usefulness of both our definition of a weakness and of performing an analysis of weaknesses of statistical language models in general.",
  "full_text": "cmp-lg/9406027   17 Jun 1994\nANAL YZING AND IMPR O VING ST A TISTICAL LANGUA GE\nMODELS F OR SPEECH RECOGNITION\nb y\nJo erg Ueb erla\nB/.Sc/. T ec hnisc he Univ ersitaet Muenc hen/, /1/9/8/8\nM/.Sc/. Univ ersite J/. F ourier/, Grenoble/, F rance/, /1/9/9/0\na thesis submitted in p ar tial fulfillment\nof the requirements f or the degree of\nDoctor of Philosophy\nin the Sc ho ol\nof\nComputing Science\nc\n/\r Jo erg Ueb erla /1/9/9/4\nSIMON FRASER UNIVERSITY\nMa y /1/9/9/4\nAll righ ts reserv ed/. This w ork ma y not b e\nrepro duced in whole or in part/, b y photo cop y\nor other means/, without the p ermission of the author/.\nAPPR O V AL\nName/: Jo erg Ueb erla\nDegree/: Do ctor of Philosoph y\nTitle of thesis/: Analyzing and Impro ving Statistical Language Mo dels for\nSp eec h Recognition\nExamining Comm ittee/: Dr/. V eronica Dahl\nChair\nSenior Sup ervisor/: Dr/. F red P op o wic h\nDr/. Bob Hadley\nDr/. T om P erry\nDr/. Bina y Bhattac hary a\nExternal Examiner/: Dr/. Renato De Mori\nDate Appro v ed/:\nii\nAbstract\nA sp eec h recognizer is a device that translates sp eec h in to text/. Man y curren t sp eec h\nrecognizers con tain t w o comp onen ts/, an acoustic mo del and a statistical language mo del/.\nThe acoustic mo del indicates ho w lik ely it is that a certain w ord corresp onds to a part of\nthe acoustic signal /(e/.g/. the sp eec h/)/. The statistical language mo del indicates ho w lik ely\nit is that a certain w ord will b e sp ok en next/, giv en the w ords recognized so far/. Ev en\nthough the acoustic mo del migh t for example not b e able to decide b et w een the acoustically\nsimilar w ords /\\p eac h/\" and /\\teac h/\"/, the statistical language mo del can indicate that the\nw ord /\\p eac h/\" is more lik ely if the previously recognized w ords are /\\He ate the/\"/.\nCurren t sp eec h recognizers p erform w ell on constrained tasks/, but the goal of con tin uous/,\nsp eak er indep enden t sp eec h recognition in p oten tially noisy en vironmen ts with a v ery large\nv o cabulary has not b een reac hed so far/. Ho w can statistical language mo dels b e impro v ed\nso that more complex tasks can b e tac kled/? This is the question addressed in this thesis/.\nSince the kno wledge of the w eaknesses of an y theory often mak es impro ving the theory\neasier/, the cen tral idea of this thesis is to analyze the w eaknesses of existing statistical\nlanguage mo dels in order to subsequen tly impro v e them/. T o that end/, w e formally de/\fne a\nw eakness of a statistical language mo del in terms of the logarithm of the total probabilit y /,\nLT P /, a term closely related to the standard p erplexit y measure used to ev aluate statistical\nlanguage mo dels/. This de/\fnition is applicable to man y probabilistic mo dels/, including\nalmost all of the curren tly used statistical language mo dels/.\nW e apply our de/\fnition of a w eakness to a frequen tly used statistical language mo del/,\ncalled a bi/-p os mo del/. This results/, for example/, in a new mo deling of unkno wn w ords whic h\nimpro v es the p erformance of the mo del b y /1/4/% to /2/1/%/. Moreo v er/, one of the iden ti/\fed\nw eaknesses has prompted the dev elopmen t of our generalized N /-p os language mo del/, whic h\nis also outlined in this thesis/. It can incorp orate linguistic kno wledge ev en if it extends o v er\nman y w ords and this is not feasible in a traditional N /-p os mo del/. This leads to a discussion\nof what kno wledge should b e added to statistical language mo dels in general and w e giv e\ncriteria for selecting p oten tially useful kno wledge/. These results sho w the usefulness of\nb oth our de/\fnition of a w eakness and of p erforming an analysis of w eaknesses of statistical\nlanguage mo dels in general/.\niii\nT o m y paren ts\niv\nAc kno wledgemen ts\nI w ould lik e to express m y deep est gratitude to m y senior sup ervisor/, Dr/. F/. P op o wic h/. He\nw as alw a ys there to help me deal with the m ultitude of problems I encoun tered/. Without his\ngreat en th usiasm and con tin uous encouragemen t/, this w ork w ould not ha v e b een p ossible/. I\nw ould also lik e to thank the other mem b ers of m y examining committee/, Drs/. B/. Hadley /, B/.\nBhattac hary a and T/. P erry for their v aluable commen ts/. Sp ecial thanks go to m y external\nexaminer/, Dr/. R/. De Mori/, for pro viding man y v aluable suggestions/. F urthermore/, I am\nindebted to E/. A t w ell/, for enabling a v ery stim ulating sta y at the Univ ersit y of Leeds/, and\nto Dr/. R/. Kuhn at CRIM/, for his help and encouragemen t/.\nDuring m y sta y at SFU/, I exp erienced man y p ersonally stim ulating and enric hing mo/-\nmen ts/. I w ould lik e to thank all the p eople who con tributed to them for sharing the mostly\nenjo y able life of a graduate studen t with me/. In particular/, I w ould lik e to sincerely thank\nEli/, Pp/, Allan bb/, Carl/, Glenn/, Katerina/, Alicja/, Pierre/, Diana/, Martin/, Graham and Brigitte\nfor making m y sta y at SFU suc h a memorable exp erience/.\nF or the /\fnancial supp ort that made m y sta y at SFU p ossible/, I am indebted to Dr/. F/.\nP op o wic h/, Dr/. V/. Dahl/, the Sc ho ol of Computing Science and Simon F raser Univ ersit y /.\nSp ecial thanks also go to the alw a ys friendly sta/\u000b in Computing Science/, in particular to\nElma and Kersti/.\nv\nCon ten ts\nAbstract iii\niv\nAc kno wledgem en ts v\n/1 In tro duction /1\n/1/./1 The Di/\u000eculties of Sp eec h Recognition /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /2\n/1/./2 Di/\u000beren t Approac hes to Sp eec h Recognition /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /4\n/1/./3 Incorp orating Natural Language Constrain ts in to Sp eec h Recognition /: /: /: /: /: /5\n/1/./4 Ov erview /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /9\n/1/./4/./1 Chapter /2/: Language Mo deling for Sp eec h Recognition /: /: /: /: /: /: /: /: /: /9\n/1/./4/./2 Chapter /3/: Analysing and Impro ving Language Mo dels /: /: /: /: /: /: /: /: /1/0\n/1/./4/./3 Chapter /4/: Analyzing and Impro ving a Bi/-p os Language Mo del /: /: /: /: /1/1\n/1/./4/./4 Chapter /5/: Adding Linguistic Kno wledge to Language Mo dels /: /: /: /: /: /1/3\n/2 Language Mo deling for Sp eec h Recognition /1/5\n/2/./1 The Comp onen ts of a Sp eec h Recognizer /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /1/5\n/2/./2 F requen tly Used Notations /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /1/7\n/2/./3 The T ask of a Language Mo del /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /1/9\n/2/./4 Estimation and Smo othing of Probabilit y Distributions from F requency Data /2/1\n/2/./4/./1 Estimation of Probabilit y Distributions from F requency Data /: /: /: /: /: /2/1\n/2/./4/./2 Smo othing of Probabilit y Distributions /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /2/3\n/2/./4/./3 Assumptions ab out Probabilit y Distributions in our W ork /: /: /: /: /: /: /: /2/4\n/2/./5 Review of Existing Language Mo dels /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /2/5\nvi\n/2/./5/./1 Con text Indep enden t Mo dels /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /2/5\n/2/./5/./2 N /-gram Mo dels /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /2/6\n/2/./5/./3 N /-p os Mo dels /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /2/7\n/2/./5/./4 Decision T ree Based Mo dels /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /3/0\n/2/./5/./5 Dynamic/, Adaptiv e and Cac he/-Based Mo dels /: /: /: /: /: /: /: /: /: /: /: /: /: /: /3/1\n/2/./6 Summary /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /3/3\n/3 Analyzing and Impro ving Language Mo dels /3/4\n/3/./1 In tuitions on Analysing and Impro ving Language Mo dels /: /: /: /: /: /: /: /: /: /: /: /3/5\n/3/./2 Ev aluating a Language Mo del /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /3/6\n/3/./2/./1 A Simple Mathematical Measure for the Qualit y of a Language Mo del /3/6\n/3/./2/./2 Information/, En trop y and P erplexit y from an Information Theoretic\nP oin t of View /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /3/7\n/3/./2/./3 Discussion of the Standard P erplexit y Measure /: /: /: /: /: /: /: /: /: /: /: /: /: /4/1\n/3/./3 De/\fning and Iden tifying W eaknesses of Language Mo dels /: /: /: /: /: /: /: /: /: /: /: /4/2\n/3/./4 Probabilit y Decomp osition /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /4/7\n/3/./5 Applicabilit y /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /5/2\n/3/./6 Summary /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /5/4\n/4 Analyzing and Impro ving a Bi/-p os Language Mo del /5/6\n/4/./1 Cho osing a Corpus and a Language Mo del /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /5/6\n/4/./1/./1 Cho osing a Corpus /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /5/7\n/4/./1/./2 Cho osing a Language Mo del /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /5/9\n/4/./2 Di/\u000berences in Sample Spaces /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /6/2\n/4/./2/./1 Di/\u000berences Due to the Mo deling of Unkno wn W ords /: /: /: /: /: /: /: /: /: /: /6/2\n/4/./2/./2 Di/\u000berences Due to Di/\u000beren t Amoun ts of T raining T ext /: /: /: /: /: /: /: /: /: /6/5\n/4/./2/./3 In/\ruence of the Amoun t of T raining Data on the P erformance of our\nLanguage Mo del /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /7/0\n/4/./3 W eaknesses of the Bi/-p os Mo del /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /7/1\n/4/./3/./1 Di/\u000beren t Con texts /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /7/2\n/4/./3/./2 Unkno wn W ords /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /7/6\n/4/./3/./3 Di/\u000beren t Comp onen ts /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /7/8\n/4/./4 The Generalized N /-p os Mo del /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /8/0\n/4/./4/./1 In tro ducing the Generalized N /-p os Mo del /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /8/1\nvii\n/4/./4/./2 Using the Generalized N /-p os Mo del /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /8/3\n/4/./5 Summary /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /8/6\n/5 Adding Linguistic Kno wledge to Language Mo dels /8/9\n/5/./1 Reasons for Adding Linguistic Kno wledge to Language Mo dels /: /: /: /: /: /: /: /: /8/9\n/5/./2 What Kno wledge Should W e Add/? /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /9/0\n/5/./2/./1 Criteria for Selecting Useful Kno wledge /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /9/1\n/5/./2/./2 Classi/\fcation of P ossibly Useful Kno wledge /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /9/5\n/5/./2/./3 The Usefulness of Collo cational Constrain ts /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /9/6\n/5/./3 Ho w Can W e Com bine Useful Kno wledge in a Language Mo del/? /: /: /: /: /: /: /: /9/9\n/5/./3/./1 T raditional Approac hes for Com bining Kno wledge in a Language Mo del /9/9\n/5/./3/./2 The Maxim um En trop y Approac h to Com bining Kno wledge in a Lan/-\nguage Mo del /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /1/0/4\n/5/./4 Summary /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /1/0/6\n/6 Summ ary of Results and F uture W ork /1/0/8\n/6/./1 Summary of Results /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /1/0/8\n/6/./2 F uture W ork /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /: /1/1/0\nA Sample T ext /1/1/2\nB The F our T agsets Used in Our Exp erimen ts /1/1/5\nC The New Mo del of Unkno wn W ords /1/1/9\nD The Generalized N /-p os Mo del /{ P art I /1/2/1\nE The Generalized N /-p os Mo del /{ P art I I /1/2/3\nBibliograph y /1/4/0\nviii\nChapter /1\nIn tro duction\nThe study of sp eec h recognition is of great imp ortance b ecause of the so cial and economic\nimpact sp eec h recognition will ha v e on our so ciet y /. W e h umans sp end a large fraction of\nour lifetime sp eaking/, listening/, reading and writing/. Already to da y /, computers are in v olv ed\nin a large part of h uman comm unication/, b e it telephone switc hing/, electronic mail/, w ord\npro cessing/, information retriev al or computer bulletin b oards/. A t little extra cost/, computers\npro vide additional features impro ving the qualit y of these pro cesses or making h uman lab our\nmore e/\u000bectiv e/. The impact of computer tec hnology on so ciet y has its go o d and bad sides /(see\n/[/1/2/5 /]/, /[/1/2/9 /] and /[/3/8 /] for a discussion/)/. But b ecause h umans will still w an t to comm unicate\nin the y ears to come and b ecause computers are v ery lik ely to con tin ue to b ecome c heap er/,\nit is v ery lik ely that an ev en bigger share of our sp ok en and written comm unication will b e\nmediated b y computers in the future/. As computers con tin ue to p enetrate our so ciet y /, b eing\nable to comm unicate with computers via sp eec h is therefore of great so cial and economical\nimp ortance/.\nMoreo v er/, the study of sp eec h recognition is in teresting b ecause of the in tellectual c hal/-\nlenge p osed b y a problem whose solution in v olv es man y di/\u000beren t scien ti/\fc disciplines/. In\nthe past/, the /\feld of sp eec h recognition has b ene/\fted from sciences as div erse as biology /,\ncomputer science/, electrical engineering/, linguistics/, mathematics/, philosoph y /, ph ysics/, psy/-\nc hology and statistics/. Th us/, the questions raised b y sp eec h recognition range from philo/-\nsophical questions ab out the nature of mind to practical design and implemen tation issues/.\nMotiv ated b y the study of arti/\fcial in telligence/, sp eec h recognition can therefore serv e as\na testing ground/, bringing man y disciplines together in a concrete task/, th us a v oiding the\ndangers of a p oten tially in trosp ectiv e and sub jectiv e undertaking/.\n/1\nCHAPTER /1/. INTR ODUCTION /2\nThe fascinating in terpla y b et w een di/\u000beren t scien ti/\fc disciplines and the great so cial and\neconomic imp ortance of sp eec h recognition mak e it a v ery c hallenging/, stim ulating and\nexciting researc h /\feld/.\nIn the follo wing/, w e\n/1\nwill brie/\ry presen t the main di/\u000eculties of sp eec h recognition and\nthe approac hes p eople ha v e used to tac kle them/. W e then giv e an o v erview of di/\u000beren t\nmetho ds of joining natural language pro cessing and sp eec h recognition and iden tify the\ntopic of this thesis/, statistical language mo dels for sp eec h recognition/, as one of them/. In\nthe remainder of this thesis/, w e will use the term language mo del as a short hand for\nstatistical language mo dels for sp eec h recognition/. W e conclude this c hapter b y giving an\no v erview of our w ork/. Most of the material of section /1/./1/, section /1/./2 and of the previous\nthree paragraphs is dra wn from /[/1/0/0 /, p/./1/0/]/, /[/1/5/1 /, p/./1/-/5/] and /[/1/0/4 /]/.\n/1/./1 The Di/\u000ecultie s of Sp eec h Recognition\nA sp eec h recognizer is a device that translates sp eec h in tro written text and the problem\nof sp eec h recognition has b een studied activ ely since the /1/9/5/0/'s/. Enormous progress has\nb een made/, but man y problems of sp eec h recognition remain unsolv ed to da y /. What mak es\nsp eec h recognition suc h a di/\u000ecult task/? Here are the main di/\u000eculties/:\n/1/) Eac h elemen tary sound/, also called a phoneme/, is mo di/\fed according to its con text/,\nfor example the immediately preceding and follo wing phoneme/. This is partly due to a\nprop ert y of our v o cal apparatus called coarticulation/: as one phoneme is pronounced/,\nthe pron unciation of the next phoneme is prepared b y a mo v emen t of the v o cal appa/-\nratus/. Mo di/\fcation of a phoneme is also caused b y the larger con text suc h as its place\nin a sen tence/.\n/2/) There is no separator/, e/.g/. no silence/, b et w een w ords/. This creates additional con/-\nfusable w ords and phrases /(e/.g/. /\\y outh in Asia/\" and /\\euthanasia/\"/)/. It also leads to\nmore coarticulation/, e/.g/. b et w een w ords/, and to p o orer articulation /(e/.g/. /\\did y ou/\"\nb ecomes /\\didja/\"/)/.\n/3/) The v ariabilit y of the sp eec h signal for the same utterance is enormous/. F or example/,\nthere is in tra/-sp eak er v ariabilit y due to the sp eaking mo de /(singing/, shouting/, with a\n/1\nI w ould lik e to clarify that ev en though I use the w ord /\\w e/\" throughout this thesis/, the w ork presen ted\nhere is m y o wn and th us quali/\fes for submission as a thesis/.\nCHAPTER /1/. INTR ODUCTION /3\ncold/, under stress/, sp eaking rate/, etc/./)/, in ter/-sp eak er v ariabilit y /(sex/, age etc/./) and\nv ariabilit y due to the en vironmen t /(noise/, lipsmac ks etc/./)/.\n/4/) Because of /1/) and /2/)/, it is necessary to pro cess large sets of data in order to de/\fne\nwhat constitutes an elemen tary sound/, despite the di/\u000beren t con texts/, sp eaking mo des\netc/. F or example/, it is hard to decide that an /\\a/\" pronounced b y a male adult is\nmore similar to an /\\a/\" pronounced b y a c hild in a di/\u000beren t w ord and in a di/\u000beren t\nen vironmen t than an /\\o/\" pronounced b y the same male adult in the same en vironmen t\n/2\n/.\n/5/) Because the signal carries di/\u000beren t t yp es of information /(sounds/, syn tactic structure/,\nseman tics/, iden tit y and mo o d of sp eak er etc/./)/, a sp eec h recognition system will ha v e to\ndi/\u000beren tiate b et w een the information useful for its task and the remaining/, irrelev an t\ninformation/.\n/6/) There is no precise formalism that allo ws us to formalize the kno wledge at all the\ndi/\u000beren t lev els /(e/.g/. acoustics/, syn tax/, seman tics etc/./)/. Ho w ev er/, recen t trends suggest\nthat a probabilistic framew ork migh t b e used at man y lev els/.\nThese six p oin ts are the main problems a sp eec h recognizer has to face in general/. Ho w ev er/,\nconcrete sp eec h recognition tasks ma y v ary greatly in the degree of di/\u000ecult y they presen t/.\nThe follo wing six dimensions can b e used to classify a sp eec h recognition task according to\nits di/\u000ecult y/:\n/1/) Isolated /(with pauses/) or con tin uous sp eec h/. Con tin uous sp eec h recognition is far more\ndi/\u000ecult b ecause there are no w ord b oundaries and b ecause the v ariabilit y of the signal\nis m uc h greater/.\n/2/) V o cabulary size/. As the v o cabulary size increases /(from small v o cabularies of less than\n/5/0/0 w ords\n/3\nto v ery large v o cabularies of ab out /2/0/,/0/0/0 w ords/)/, the task b ecomes more\ndi/\u000ecult b ecause the n um b er of acoustically confusable w ords increases and b ecause\nmore time is needed to ev aluate all p ossible w ords/.\n/2\nNev ertheless/, ev en if it is the case that the /\\a/\" of the male adult is more similar to his /\\o/\" than to a\nc hild/'s /\\a/\"/, w e do need to recognize his /\\o/\" as an /\\o/\"/, but the more di/\u000beren t /\\a/\" of the c hild as an /\\a/\"/.\n/3\nIt is imp ortan t to kno w that /\\car/\" and /\\cars/\" are coun ted as t w o di/\u000beren t w ords in a sp eec h recognizer/.\nTh us/, a sp eec h recognizer with /5/0/0 w ords has far few er w ords in the usual sense than the n um b er /5/0/0 migh t\nsuggest/.\nCHAPTER /1/. INTR ODUCTION /4\n/3/) T ask and language constrain ts/. The size of the v o cabulary is not su/\u000ecien t for deter/-\nmining the di/\u000ecult y of a task b ecause some w ords ma y not b e allo w ed at a giv en time/.\nF or example/, a task with /5/0/0 w ords/, eac h of whic h can app ear at an y time/, ma y b e\nmore di/\u000ecult than a task with /7/0/0 w ords with strong restrictions on whic h w ords ma y\nfollo w other w ords/.\n/4/) Sp eak er dep endence /(for one sp eak er only/) or sp eak er indep endence /(for man y sp eak/-\ners/)/. A sp eak er indep enden t task is m uc h more di/\u000ecult b ecause of the additional\nin ter/-sp eak er v ariabilit y /.\n/5/) Acoustic am biguit y /. The acoustic confusabilit y of w ords in the v o cabulary also in/\ru/-\nences the di/\u000ecult y of the task/. F or example/, a task with /1/0/0 w ords that are highly\nconfusable ma y b e harder than a task with /2/0/0 w ords that are v ery dissimilar/.\n/6/) En vironmen tal noise/. A task in a v ery noisy en vironmen t is more di/\u000ecult b ecause the\nnoise can lead to arbitrary distortions and mo di/\fcations of the sp eec h signal/.\n/1/./2 Di/\u000beren t Approac hes to Sp eec h Recognition\nHa ving seen the di/\u000eculties of sp eec h recognition/, ho w ha v e researc hers tried to tac kle these\nproblems/? W e can di/\u000beren tiate four di/\u000beren t approac hes /{ template/-based/, kno wledge/-\nbased/, sto c hastic and connectionist /{ and w e will brie/\ry presen t eac h one of them/.\nIn the template/-based approac h/, units of sp eec h /(e/.g/. w ords/) are represen ted in the same\nform as the sp eec h input itself/. The input is compared to the templates using some distance\nmetric th us iden tifying the b est matc h/. The problem of temp oral v ariabilit y is tac kled b y\ndynamic programming/. F or simple applications requiring minimal o v erhead/, this approac h\nhas b een quite successful/.\nIn the kno wledge based approac h prop osed in the /7/0/'s and early /8/0/'s/, h uman kno wledge\nis co ded in to exp ert systems/. Rule/-based systems had only limited success/, but in more\nsuccessful systems/, the kno wledge is in tegrated in to a sound mathematical approac h and\nthis additional kno wledge is found to impro v e the p erformance/.\nIn the sto c hastic approac h /(e/.g/. using hidden Mark o v mo dels or HMMs/)/, a template\npattern is represen ted at a higher lev el of abstraction b y a reference mo del th us allo wing\nsome generalization/. HMMs are based on a sound probabilistic framew ork that can mo del\nCHAPTER /1/. INTR ODUCTION /5\nthe uncertain t y and v ariabilit y inheren t in sp eec h recognition/. Since HMMs sim ultane/-\nously solv e the segmen tation and classi/\fcation problem/, they are particularly w ell suited\nfor con tin uous sp eec h recognition/. Most successful large/-v o cabulary systems to da y use the\nsto c hastic approac h/.\nThe most recen t dev elopmen t in sp eec h recognition is the connectionist approac h/. This\napproac h do es not require some of the often incorrect assumptions underlying the sto c has/-\ntic approac h/. Ev en though no large scale/, fully in tegrated connectionist system has b een\ndemonstrated/, this approac h holds considerable promise/, esp ecially in com bination with the\nsto c hastic approac h/.\nIn the rest of this thesis/, w e will assume that the sp eec h recognizer is built according to\nthe widely used sto c hastic approac h/. Nev ertheless/, the ideas of language mo deling presen ted\nin this thesis are also applicable to other approac hes/. A language mo del could/, for example/,\nb e used to rescore h yp otheses in a template/-based or kno wledge/-based approac h/.\n/1/./3 Incorp orating Natural Language Constrain ts in to Sp eec h\nRecognition\nGiv en an y one of the approac hes men tioned in the previous section/, a sp eec h recognizer can\niden tify a set of candidate w ords/, whic h are lik ely to corresp ond to a part of the signal/.\nSupp ose for example that w e ha v e recognized the w ords /\\He ate the/\" co v ering a certain\npart of the signal/, ho w can w e extend recognition b y another w ord/? Based on the acoustic\nprop erties of the w ords in the v o cabulary /, w e can/, for example/, iden tify the w ords /\\p eac h/\"\nand /\\teac h/\" as candidates/, b ecause they are v ery similar to the next stretc h of the signal/.\nHo w ev er/, w e can not iden tify exactly whic h one of the candidate w ords w as the one sp ok en/.\nW e can then use the linguistic con text to iden tify the w ord that is more lik ely to app ear next/.\nIn this example/, it is clear that the w ord /\\p eac h/\" is far more lik ely than the w ord /\\teac h/\"/.\nUsing these constrain ts imp osed b y con text is v ery imp ortan t for sp eec h recognition/. This\nis for example p oin ted out in /[/1/1/7 /, p/./3/3/]/: /\\W e kno w that/, in a real task/, the imp ortance\nof the language mo del is comparable to that of the acoustic mo dule in determining the\n/\fnal p erformance/\"/. In general/, this kind of reasoning in v olv es constrain ts on the next w ord\nimp osed b y syn tax/, seman tics or pragmatics/. These constrain ts are part of the domains\nof natural language pro cessing and linguistics/. In the follo wing/, w e will therefore lo ok at\ndi/\u000beren t w a ys of incorp orating natural language constrain ts in to sp eec h recognition/.\nCHAPTER /1/. INTR ODUCTION /6\nThere are man y di/\u000beren t w a ys of incorp orating natural language constrain ts in to a sp eec h\nrecognizer/. F ollo wing roughly a classi/\fcation suggested in /[/1/0/8 /]/, w e will presen t four di/\u000beren t\napproac hes for com bining a sp eec h recognizer and a natural language pro cessor/. W e will see\nho w eac h approac h acts in our example/, e/.g/./, ho w it c ho oses b et w een /\\p eac h/\" and /\\teac h/\"\nas p ossible con tin uations of the sen tence fragmen t /\\He ate the/\"/.\n/1/) Serial connection/. In this approac h/, the natural language pro cessor receiv es the most\nlik ely sen tence from the sp eec h recognizer and in terprets it further/. The adv an tage of\nthis approac h is that b oth systems ha v e no additional computational burden from the\n/\\in tegration/\"/. The disadv an tage is that there is almost no in teraction b et w een the\nt w o comp onen ts/. As a result/, the natural language pro cessor can not correct errors\nof the sp eec h recognizer/. This metho d is for example used in /[/1/3/2 /]/. In our example\nsen tence/, the sp eec h recognizer w ould ha v e to c ho ose b et w een /\\teac h/\" and /\\p eac h/\"\nindep enden tl y of the natural language pro cessor\n/4\n/.\n/2/) N /-b est sen tence in terface/. The sp eec h recognizer outputs the N b est scoring sen tences\n/(for N /= /1/, this is the serial in terface/)/. The natural language pro cessor c ho oses the\nsen tence that b est satis/\fes the natural language constrain ts/. The adv an tage is that\nthis allo ws some in teraction of the t w o/, while adding only some additional computa/-\ntional burden/. The size of N determines the tradeo/\u000b b et w een allo w ed in teraction and\nadditional burden/. One disadv an tage is that N ma y b e required to rise exp onen tially\nwith the length of the input sen tence/. This approac h is for example used in /[/1/3/2 /]/. In\nour example/, the sp eec h recognizer could use /\\p eac h/\" in one of the N /-b est sen tences\nand /\\teac h/\" in another and this w ould allo w the decision to b e tak en b y the natural\nlanguage pro cessor/.\n/3/) W ord lattice in terface/. the sp eec h recognizer pro duces a graph with p ossible starting\ntimes/, end times and recognition scores for an y w ord of the v o cabulary at an y time/.\nThe natural language pro cessor searc hes this graph for the most lik ely sen tence that\nsatis/\fes the natural language constrain ts/. The adv an tage of this approac h is the high\ndegree of in teraction b et w een the t w o comp onen ts/. The disadv an tage is/, the additional\ncomputational burden for b oth systems/. The sp eec h recognizer has to k eep trac k of\nand output man y h yp otheses/, rather than concen trating on the b est one/. The natural\n/4\nHo w ev er/, the sp eec h recognizer could use a v ery simple natural language pro cessor to /\fnd the most lik ely\nsen tence and this is further discussed after this classi/\fcation/.\nCHAPTER /1/. INTR ODUCTION /7\nlanguage pro cessor has to ev aluate man y more p ossible sen tences/. This metho d is\nused in /[/1/4/6 /] and /[/1/3/1 /]/. F or our example/, the sp eec h recognizer do es not mak e the\ndecision and b oth /\\teac h/\" and /\\p eac h/\" will app ear in the w ord lattice/. The natural\nlanguage pro cessor will then mak e the /\fnal decision/.\n/4/) P arallel connection/. In this approac h/, the constrain ts pro vided b y the natural lan/-\nguage pro cessor are used directly in the sp eec h recognizer to reduce the searc h space/.\nWithin this category /, w e can further distinguish the approac hes with resp ect to the\ncomplexit y of the natural language pro cessor/. The natural language pro cessor can b e\nv ery complex/, attempting to pro duce a parse and a seman tic represen tation of the sen/-\ntence/. Or it can b e v ery simplistic/, attempting only to iden tify whic h w ords are lik ely\nto app ear giv en the preceding w ord/. W e divide the whole sp ectrum in to t w o classes/,\ncomplex natural language pro cessors and simplistic natural language pro cessors/. The\nline b et w een the t w o classes can b e dra wn in man y w a ys /(e/.g/. whether the natural\nlanguage pro cessor attempts a parse or not/)/, but for our purp oses w e don/'t need to\nsp ecify exactly where w e dra w the line in order to con tin ue/.\na/) Complex natural language pro cessor/. The adv an tage of this approac h is that\nit allo ws considerable in teraction b et w een the t w o comp onen ts/. The constrain ts\npro vided b y the natural language pro cessor are used directly during recognition\nto rule out some of the w ord candidates/. One disadv an tage is the amoun t of\ncomputation required to c hec k the constrain ts pro vided b y the natural language\npro cessor for all w ord candidates in the acoustic searc h/. Another disadv an tage\nis that v ery complex natural language pro cessors can usually b e built only for\nlimited domains and this metho d is th us di/\u000ecult to use for unrestricted sp eec h/.\nAn example of use with a restricted domain can b e found in /[/1/3/4 /]/. Con text free\nrules are deriv ed automatically from sample sen tences and then appro ximated\nb y a probabilistic /\fnite state mac hine/. Another w a y of using this approac h is\npresen ted in /[/1/0/8 /]/. The natural language constrain ts are expressed in terms of\na /\fnite state mac hine/, that is in turn used directly b y the sp eec h recognizer/.\nHo w ev er/, since a t ypical natural language system will pro duce an enormous or\nev en in/\fnite n um b er of states/, only the parts that are curren tly searc hed b y the\nsp eec h recognizer are dynamically created b y the natural language pro cessor/.\nb/) Simplistic natural language pro cessor/. The adv an tage of this approac h is that\nCHAPTER /1/. INTR ODUCTION /8\nit allo ws considerable in teraction b et w een the t w o comp onen ts/. Moreo v er/, since\nthe natural language pro cessor is v ery simplistic/, it can e/\u000ecien tly score all the\nw ord candidates during the acoustic searc h/. The disadv an tage is that the nat/-\nural language pro cessor only captures v ery few constrain ts/, thereb y for example\nallo wing ungrammatical sequences of w ords/. W e will see man y examples of this\napproac h later on/.\nIn our example/, the kno wledge of the natural language pro cessor and the sp eec h rec/-\nognizer are com bined during the acoustic searc h to c ho ose b et w een the w ords /\\p eac h/\"\nand /\\teac h/\"/.\nF or more information ab out this issue/, the in terested reader can refer to /[/5/2 /]/, /[/7/5 /]/, /[/1/0/8 /]/,\n/[/1/1/3 /]/, /[/1/1/6 /] and /[/1/4/4 /]/. In this thesis/, w e will fo cus on the simplistic natural language pro ces/-\nsors of category /4b/)/, also called language mo dels/, and w e giv e four reasons for this c hoice/.\nFirst/, language mo dels are used in man y existing recognizers and this sho ws their great\npractical imp ortance/. Second/, language mo dels can b e used ev en if the serial approac hes of\ncategory /1/) or /2/) are c hosen/. In that case/, the sp eec h recognizer uses a language mo del to\narriv e at the N most lik ely sen tences /(for example/)/, whic h are then further pro cessed b y a\nmore complex natural language pro cessor/. Third/, if the task at hand do es not require the\nunderstanding of the utterance/, a parse ma y not b e necessary and language mo dels still\npro vide a w a y of incorp orating some natural language constrain ts in to the sp eec h recog/-\nnizer/. F ourth/, more complex natural language pro cessors /(as in /4a/) are mostly limited to\none sp eci/\fc domain and they are th us of limited use for unconstrained sp eec h recognition/.\nIt is imp ortan t to b e a w are of t w o di/\u000beren t subtasks sometimes lump ed together in the\nterm sp eec h recognition/: sp eec h understanding and sp eec h recognition /(prop er/)/. The goal\nof sp eec h understanding is to understand sp ok en language and to react to it in a meaning/-\nful manner/. Since this task is usually limited to a narro w domain/, more complex natural\nlanguage pro cessors can b e used/. An example of a sp eec h understanding task is that of un/-\nderstanding sp ok en queries to a database/. Con trary to that/, the task of sp eec h recognition\nis only to transcrib e sp eec h in to text/. Because this task do es not require understanding/,\nit can and should deal with unrestricted text/, not limited to a certain domain/. Therefore/,\nmore simplistic natural language pro cessors are commonly used/. An example of the sp eec h\nrecognition task is the phonetic t yp ewriter/, a device that is able to output a prin ted v er/-\nsion of a sp ok en con v ersation/. Since our w ork fo cuses on the simplistic natural language\nCHAPTER /1/. INTR ODUCTION /9\npro cessors/, it is mostly relev an t to sp eec h recognition/. But as p oin ted out ab o v e/, ev en a\nsp eec h understanding system with a complex natural language pro cessor connected in a\nserial manner migh t use a simplistic language mo del during recognition/.\nNo w that w e ha v e informally\n/5\npresen ted the fo cus of our w ork/, language mo dels for\nsp eec h recognition/, w e will giv e an o v erview of the rest of this thesis b y giving a summary\nof eac h c hapter/.\n/1/./4 Ov erview\n/1/./4/./1 Chapter /2/: Language Mo deling for Sp eec h Recognition\nIn c hapter t w o/, w e giv e an o v erview of the di/\u000beren t comp onen ts of a sp eec h recognizer/, de/-\nscrib e their in teraction/, de/\fne the task of the comp onen t cen tral to this thesis/, the language\nmo del more formally /, and review the most commonly used language mo dels/.\nAfter in tro ducing the di/\u000beren t comp onen ts of a sp eec h recognizer/, w e de/\fne the task of\na language mo del as the construction of one or more probabilit y distributions o v er all the\nw ords of a v o cabulary giv en the w ords that ha v e b een recognized so far/. In tuitiv ely /, the\nsp eec h recognizer uses this distribution to decide whic h w ords are lik ely to app ear next/, e/.g/.\nbased on the probabilit y distribution/, it c ho oses the w ord /\\p eac h/\" o v er /\\teac h/\" when the\npreceding sen tence fragmen t is /\\He ate the/\"/. As this example illustrates/, di/\u000beren t w ords are\nmore lik ely to app ear in di/\u000beren t con texts/. Therefore/, a language mo del usually has man y\ndi/\u000beren t distributions/, one for eac h con text/. In language mo deling/, con text often means\nthe t w o or three w ords preceding the w ord to predict/. T o sho w the usefulness of suc h a\nsimple con text/, let us de/\fne the con text as the immediately preceding w ord and consider\nwhat this en tails for the follo wing w ord/. Only nouns and adjectiv es are lik ely to app ear if\nthe immediately preceding w ords is for example /\\the/\"/. Th us/, ev en this simple de/\fnition of\ncon text can sev erely restrict the follo wing w ord/.\nDuring sp eec h recognition/, the language mo del only has to c ho ose a distribution accord/-\ning to the curren t con text and to lo ok up the probabilities of w ords in this distribution/. The\nimp ortan t task in constructing the language mo del is to determine/, prior to recognition/, the\nn um b er of con texts it di/\u000beren tiates and to construct a probabilit y distribution for eac h one\nof them/.\n/5\nW e will giv e a more formal description in the section de/\fning the task of the mo del /(section /2/./3/)/.\nCHAPTER /1/. INTR ODUCTION /1/0\nLanguage mo dels are usually describ ed in terms of frequency coun ts of their probabilit y\ndistributions/, but for the purp oses of this thesis/, it is more appropriate to describ e language\nmo dels at the more abstract lev el of probabilit y distributions and con texts/. This is b ecause\nw e are more in terested in the conceptually imp ortan t asp ects/, e/.g/. the w a y in whic h a\nmo del de/\fnes con text/, rather than in the details of a particular tec hnique of constructing\nprobabilit y distributions from frequency coun ts/. Therefore/, w e only giv e a brief description\nof ho w probabilit y distributions can b e estimated from frequency data/, often referred to as\ntr aining data /. The principle of this estimation is that of coun ting ho w often a certain ev en t\napp ears in a giv en con text in the training data and of dividing this coun t b y the n um b er of\no v erall o ccurrences of the con text/. F or example/, w e can estimate the probabilit y of ha ving\nsunshine tomorro w giv en that it w as raining to da y b y dividing the n um b er of times w e had\nsunshine giv en that it w as raining the previous da y b y the o v erall n um b er of rain y da ys/.\nW e then giv e a review of di/\u000beren t language mo dels in order to presen t the state of the art\nin language mo deling and to set the stage for the remainder of this thesis/. F or eac h mo del/,\nw e giv e the de/\fnition of con text it uses/, the n um b er of probabilities it needs to estimate and\nsome of its adv an tages and disadv an tages/. In particular/, w e presen t the class based mo dels\nor N /-p os mo dels/, in whic h w ords are group ed in to classes called parts of sp eec h/, whic h\nroughly resem ble their grammatical function/. In these class based mo dels/, the prediction of\nthe next w ord is a t w o step pro cess/: /\frst/, the part of sp eec h is predicted b y one comp onen t/,\nthen the w ord giv en the part of sp eec h is predicted b y a second comp onen t/.\n/1/./4/./2 Chapter /3/: Analysing and Impro ving Language Mo dels\nIn c hapter three/, whic h con tains the cen tral idea of this thesis/, w e prop ose to p erform error\nanalyses of language mo dels in order to impro v e the mo dels afterw ards/, de/\fne what w e\nmean b y /\\error or w eakness of a language mo del/\" and presen t a metho d to iden tify the\nw eaknesses of a giv en mo del/.\nW e b egin b y noting that error analysis of existing theories ab out the w orld often leads\nto impro v emen ts of these theories/. By analogy to this/, w e prop ose in this c hapter to analyze\nerrors of a language mo del in order to impro v e the mo del afterw ards/. But ho w can w e\nde/\fne an error of a language mo del/? The de/\fnition of an error should b e related to the\nmeasure used to ev aluate the p erformance of a language mo del/. If de/\fnition and measure\nare not related/, w e ma y still iden tify and then remo v e an error/, but this ma y not translate\nin to an impro v emen t in p erformance /(since error and p erformance measure are not related/)/.\nCHAPTER /1/. INTR ODUCTION /1/1\nBefore de/\fning an error/, w e therefore /\frst in tro duce the standard measure used to ev aluate\nlanguage mo dels/, called p erplexit y /.\nIn tuitiv ely /, the goal of a language mo del is to predict w ords in a giv en con text/. A\ngo o d mo del should therefore assign a high probabilit y to eac h w ord in a piece of text/.\nHence/, the a v erage probabilit y assigned to w ords in a testing text /| the geometric mean of\nprobabilities to b e more precise /| is a go o d measure for the qualit y of a language mo del/.\nP erplexit y /, the standard measure used to ev aluate language mo dels/, is just the recipro cal\nof the geometric mean of probabilities/. Besides explaining the p erplexit y in tuitiv ely /, w e\nalso deriv e the p erplexit y using metho ds from information theory and further discuss its\nadv an tages and disadv an tages/.\nSince the term err or do es not really apply to a language mo del/, w e prefer to use the term\nwe akness /. W e no w de/\fne a w eakness\n/6\nof a language mo del in terms of the logarithm of the\ntotal probabilit y /(L TP/) of a sequence of w ords/, a measure closely related to the p erplexit y /.\nMoreo v er/, for mo dels with sev eral comp onen ts /(for example the class based mo del w e use/)/,\nw e dev elop the metho d of probabilit y decomp osition/, whic h allo ws us to iden tify w eaknesses\nof the di/\u000beren t comp onen ts separately /.\nThe main idea of this c hapter /| applying error analysis to language mo dels /| applies to\nan y probabilistic mo del whose p erformance is measured in terms of p erplexit y /. W e conclude\nthis c hapter b y giving examples of mo dels to whic h our metho d of error analysis applies/.\n/1/./4/./3 Chapter /4/: Analyzing and Impro ving a Bi/-p os Language Mo del\nIn c hapter four/, w e apply the cen tral idea of this thesis/, our tec hnique of iden tifying w eak/-\nnesses of a language mo del presen ted in the previous c hapter/, to a commonly used bi/-p os\nlanguage mo del and rep ort the results/.\nIn order to apply the tec hnique of iden tifying w eaknesses of a language mo del to a\nconcrete mo del/, w e /\frst c ho ose a corpus /(the Lancaster/-Oslo/-Bergen corpus/)/, a mo del /(the\nbi/-p os mo del/) and v erify that the section of the corpus w e use con tains enough data to\ntrain our mo del/. This w ork prompts an in v estigation in to the issue of sample space/, the\nset of all p ossible ev en ts considered b y a mo del/. W e note that it is not meaningful to use\nthe p erplexit y measure to compare language mo dels that di/\u000ber in their underlying sample\nspaces/. Y et language mo dels are usually compared with the p erplexit y measure/, ev en though\n/6\nW e use the term w eakness as a tec hnical term and the in tuitions /, that still apply to our tec hnical use of\nit are discussed on page /4/2/.\nCHAPTER /1/. INTR ODUCTION /1/2\nthey sometimes di/\u000ber in their underlying sample spaces/, either due to di/\u000beren t v o cabularies\nor due to di/\u000beren t w a ys of dealing with unkno wn w ords/. W e also discuss p ossible solutions\nto the problem of di/\u000beren t sample spaces/.\nW e then apply our metho d of iden tifying w eaknesses of a language mo del to our c hosen\nbi/-p os mo del and rep ort three results of general in terest/. First/, a v ery small n um b er of\nclasses\n/7\nare iden ti/\fed as w eaknesses of the mo del/. W e b eliev e that these results are helpful\nfor future e/\u000borts to impro v e the mo del/, b ecause w e kno w on whic h classes w e should con/-\ncen trate our e/\u000borts/. Second/, unkno wn w ords are iden ti/\fed as w eaknesses/. This prompts\nthe dev elopmen t of a new mo deling of unkno wn w ords/, whic h impro v es the p erformance\nb y b et w een /1/4/% and /2/1/%/. Third/, the w ord comp onen t of our bi/-p os mo del is sho wn to b e\nat least as imp ortan t as the class comp onen t/. This has in teresting rami/\fcations for using\nprobabilistic con text free grammars for language mo deling/, an approac h that has recen tly\nreceiv ed a lot of atten tion/. Ev en though using probabilistic con text free grammars ma y\nresult in an impro v ed prediction of a class /(or part of sp eec h/)/, it is not lik ely to impro v e the\nprediction of the actual w ord giv en its class/. W e should therefore impro v e the w ord comp o/-\nnen t of a class based mo del ev en if probabilistic con text free grammars are b eing used/. The\nadditional insigh t gained from these results also sho w the usefulness of the cen tral idea of\nthis thesis/, the iden ti/\fcation and analysis of w eaknesses of language mo dels/.\nHo w can w e go ab out impro ving the w ord comp onen t of a class based language mo del/?\nThis question leads us to dev elop our generalized N /-p os mo del/, whic h is sho wn to b e a\ntrue generalization of the N /-gram and the N /-p os mo del/. Moreo v er/, it can incorp orate an y\nlinguistic kno wledge not restricted to the immediate con text of the w ord to b e predicted/.\nThis is exempli/\fed b y incorp orating a v ery simple kno wledge source in to our generalized N /-\np os mo del/. The results of this example also sho w that a considerable impro v emen t /(around\n/1/5/%/) is ac hiev ed in the prediction of w ords for whic h our generalized N /-p os mo del actually\ndi/\u000bers from the original N /-p os mo del/. Ho w ev er/, the o v erall impro v emen t is negligible/,\nb ecause the cases in whic h our simple kno wledge source can b e used are v ery rare in our\nexample/. This leads to a general discussion of what kno wledge w e should add to a language\nmo del /| an issue w e address in the next c hapter/.\n/7\nF or an in tuitiv e explanation of the use of classes see the summary of c hapter /2/.\nCHAPTER /1/. INTR ODUCTION /1/3\n/1/./4/./4 Chapter /5/: Adding Linguistic Kno wledge to Language Mo dels\nIn c hapter /\fv e/, w e motiv ate the addition of kno wledge to language mo dels/, dev elop di/\u000ber/-\nen t criteria to iden tify useful kno wledge/, and presen t metho ds to com bine kno wledge in a\nlanguage mo del/.\nW e b egin b y p oin ting out three reasons for w an ting to add kno wledge to a language\nmo del/. First/, w e w ould lik e to impro v e its p erformance/. Second/, if w e apply curren t sp eec h\nrecognition tec hnology to more complex tasks than the ones tac kled to da y /, the n um b er of\nacoustically confusable h yp otheses will increase/, and w e ma y w ell need a b etter language\nmo del in order to deal with the additional am biguit y /. Third/, adding kno wledge is more\nsatisfying than stic king to existing mo dels on psyc hological grounds b ecause h umans seem\nto use kno wledge to predict a w ord other than the kno wledge used in curren t mo dels/, namely\nthe immediately preceding t w o or three w ords/. Hence/, there is clearly a need for a language\nmo del whic h incorp orates more linguistic kno wledge/.\nOnce w e ha v e decided to add kno wledge to a language mo del/, the follo wing t w o questions\ncome to mind/. First/, what kno wledge should w e add/, and second/, ho w can w e c ombine\ndi/\u000beren t t yp es of kno wledge in a language mo del/. W e address b oth questions in turn/.\nRather than trying to giv e a necessarily incomplete list of t yp es of kno wledge that\nw e should add/, w e presen t four criteria that w e think should b e used to iden tify useful\nkno wledge/. First/, the kno wledge should restrict the n um b er of p ossible w ords/, otherwise\nit is not going to help in solving our task/. Second/, it should b e applicable often enough\nto b e of statistical signi/\fcance/. Third/, it should b e p ossible computationally to use this\nkno wledge in real time sp eec h recognition/. Finally /, w e should b e able to acquire and co de\nthis kno wledge for use with unrestricted text/.\nW e dev elop a classi/\fcation of p ossibly useful kno wledge and apply the criteria for iden/-\ntifying useful kno wledge to one t yp e of kno wledge that promises to b e useful for impro ving\nlanguage mo dels in general/.\nW e then mo v e on to the issue of com bining di/\u000beren t t yp es of kno wledge in a language\nmo del/. W e presen t three metho ds of com bining kno wledge and dev elop some of the adv an/-\ntages and disadv an tages w e see in eac h metho d/. F ollo wing that/, w e conclude that it is v ery\nunlik ely that w e will ha v e enough data to estimate distributions that dep end on sev eral\nkno wledge sources directly /, ev en with the a v ailabilit y of increasingly large corp ora/. There/-\nfore/, w e think that metho ds that com bine distributions from single kno wledge sources in a\nCHAPTER /1/. INTR ODUCTION /1/4\nmeaningful fashion will b e v ery useful and require further in v estigation/. One suc h metho d\nsho wn to b e v ery useful in recen t w ork is the maxim um en trop y metho d/, whic h will also b e\npresen ted brie/\ry in this c hapter/. It holds great promise for future w ork/.\nChapter /2\nLanguage Mo deling for Sp eec h\nRecognition\nIn the last c hapter/, w e describ ed in tuitiv ely ho w the topic of this thesis/, language mo deling\nfor sp eec h recognition/, relates to sp eec h recognition researc h in general/. In this c hapter/, w e\nwill mak e this relationship more precise b y in tro ducing the di/\u000beren t comp onen ts of a sp eec h\nrecognition system /(section /2/./1/)/, and/, after in tro ducing some notations in section /2/./2/, b y\nde/\fning the task of a language mo del more formally /(section /2/./3/)/. Since a language mo del\nconsists mainly of probabilit y distributions/, w e presen t the metho d of constructing and\nsmo othing probabilities distributions w e use for our w ork /(section /2/./4/)/. Giv en the notations\nin tro duced in section /2/./2 and ha ving seen the k ey issues of language mo deling/, w e then giv e\nan o v erview of the state of the art in language mo deling b y reviewing existing language\nmo dels /(section /2/./5/)/.\n/2/./1 The Comp onen ts of a Sp eec h Recognizer\nA sp eec h recognizer is a device that translates sp eec h in to written text/. As input/, it tak es\nthe acoustic signal recorded b y a microphone/. As output/, it pro duces a string of w ords\nin tended to corresp ond to the input/. The mapping from acoustic signal to a string of w ords\nis a complex task and it in v olv es sev eral stages/. T o illustrate this mapping in a simpli/\fed\nw a y /, w e will presen t a set of stages that are v ery similar to the ones used in one of the\n/\frst successful/, large v o cabulary /, sp eak er indep enden t/, con tin uous sp eec h recognizers/, the\n/1/5\nCHAPTER /2/. LANGUA GE MODELING F OR SPEECH RECOGNITION /1/6\n/- /- /-\nw ord\nstring\n/-\nLanguage\nMo del\nAcoustic\nMo del\nsp eec h\nPro cessor\nSignal\nV ector\nQuan tizer\nSearc h\nAlgorithm\nFigure /2/./1/: Comp onen ts of the SPHINX system\nSPHINX system /(/[/9/5 /]/)/.\nAs w e can see from /\fgure /2/./1/, the acoustic signal is /\frst giv en to a signal pro cessing\ncomp onen t/. This comp onen t p erforms sev eral transformations/, e/.g/. sampling the signal at\n/\fxed time in terv als/, reducing the noise etc/. As output/, it pro duces one /1/2 dimensional v ector\nof /\roating p oin t v alues p er time in terv al/. These v ectors are the input to the next comp onen t/,\nthe v ector quan tizer\n/1\n/, whic h compares eac h input v ector to stored protot yp e v ectors and\noutputs the index of the v ector that is closest to the input v ector/. F or a sequence of /1/2\ndimensional input v ectors/, it th us pro duces a sequence of in tegers/. This sequence constitutes\nthe input to the next comp onen t/, the searc h algorithm/. It is a time sync hronous algorithm /(a\nViterbi b eam searc h/, see /[/9/5 /]/)/, that compares the lik eliho o d of di/\u000beren t sequences of units of\nsp eec h/. In order to calculate these lik eliho o ds/, the searc h algorithm uses the acoustic mo del\nand the language mo del/. The acoustic mo del pro vides the algorithm with the lik eliho o d\nthat a unit of sp eec h /(phonemes and w ords in SPHINX/) corresp ond to parts of the giv en\nsequence of in tegers/. The language mo del pro vides the algorithm with the lik eliho o d of\no ccurrence of a unit of sp eec h giv en the previously iden ti/\fed units/. Based on these t w o\ncomp onen ts/, the searc h algorithm iden ti/\fes the most lik ely sequence of units of sp eec h and\n/1\nIn recen t y ears/, the tendency has b een to eliminate the v ector quan tizer b y ha ving con tin uous densit y\nhidden Mark o v Mo dels/.\nCHAPTER /2/. LANGUA GE MODELING F OR SPEECH RECOGNITION /1/7\nthis constitutes the recognized string of w ords/.\nAs an example of the in terpla y of the acoustic mo del and the language mo del/, supp ose\nw e ha v e recognized the sequence /\\He ate the/\" so far/. The acoustic mo del calculates the\nprobabilities that the w ords /\\teac h/\" corresp onds to a pre/\fx of the sequence of in tegers\nit receiv es as input/. Similarly /, it calculates the probabilities for the w ord /\\p eac h/\"/. The\nlanguage mo del calculates probabilities for the w ords /\\teac h/\" and /\\p eac h/\"/, giv en for example\nthat the last recognized w ord is an article/. Ev en if the acoustic mo del can not decide b et w een\n/\\teac h/\" and /\\p eac h/\" b ecause they are v ery similar acoustically /, the language mo del can ha v e\na clear preference for /\\p eac h/\" /(b ecause it is a noun and nouns are far more lik ely than v erbs\nto follo w articles/)/. Based on the probabilities of the acoustic mo del and the language mo del\ntogether/, w e can then c ho ose /\\p eac h/\" as the next recognized w ord/.\nThis thesis is concerned with the language mo del comp onen t of a sp eec h recognizer/.\nSimilar to the language mo del in SPHINX men tioned ab o v e/, w e can describ e a language\nmo del in general as follo ws/. A sp eec h recognizer has recognized a sequence of w ords in the\npast and is no w trying to extend this sequence b y another w ord/. Based on the acoustic\nsignal it receiv es/, it can iden tify a set of candidate w ords whose acoustic signal is v ery\nsimilar/. Ho w ev er/, based on the acoustic signal alone/, it can/'t iden tify precisely whic h of\nthese candidate w ords is the one that w as sp ok en/. It therefore uses a language mo del to\npic k the w ord that is more lik ely to app ear in this con text/. This lik eliho o d of app earance\nis formalized in terms of probabilities/: eac h w ord of the v o cabulary has a probabilit y of\napp earing next in a giv en con text/. W e can th us describ e the task of the language mo deling\nfor sp eec h recognition in tuitiv ely as the construction of a probabilit y distribution o v er all\nthe w ords of the v o cabulary /. As an example/, consider the w ords /\\He ate the/\" and t w o\ncandidate w ords /\\p eac h/\" and /\\teac h/\"/. Humans can easily iden tify the w ord /\\p eac h/\" as\nthe lik ely con tin uation of the sen tence fragmen t/. The probabilit y distribution constructed\nb y a language mo del should /(ideally/) giv e a higher probabilit y to /\\p eac h/\" than to /\\teac h/\"/,\nallo wing it to mak e the correct c hoice/.\n/2/./2 F requen tly Used Notations\nIn this section w e will in tro duce some notations that w e will use for the remainder of the\nthesis/. Giv en these notations/, w e can then de/\fne the task of a language mo del more formally\nin section /2/./3/.\nCHAPTER /2/. LANGUA GE MODELING F OR SPEECH RECOGNITION /1/8\n/- V /= f w\n/1\n/; /:/:/:/; w\nm\ng will denote the v o cabulary of a sp eec h recognizer\n/- l /; /1 /\u0014 l /\u0014 m will denote an index ranging o v er this v o cabulary\n/- W /= w /[/1/] /; /:/:/:/; w /[ n /] will denote a string of w ords of V /. In other w ords/: /8 i /: /9 l /( i /) /; /1 /\u0014\nl /( i /) /\u0014 m /: w /[ i /] /= w\nl /( i /)\n/- i/; /1 /\u0014 i /\u0014 n will denote an index ranging o v er the string of w ords\n/- w /[ i /1 /: i /2/]/: if i /1 /\u0014 i /2 it will b e a short form for w /[ i /1/] /; /:/:/:w /[ i /2/] /, else it will denote the\nempt y string\n/- p /( x j y /) will denote the conditional probabilit y of x giv en y\n/- let V\n/+\ndenote one or more sym b ols of V /; ar g max\nW\nf /( W /) will denote the W /2 V\n/+\nfor\nwhic h f /( W /) has the maxim um v alue\n/2\n/- p /( w /[ i /] /= w\nl\nj c /) will denote the probabilit y of the i\nth\nw ord in the sequence b eing the\nw ord w\nl\ngiv en the con text c\n/- p /( w /[ i /] j c /) will denote the probabilit y distribution o v er the v o cabulary giv en the con text\nc /. In order for p /( w /[ i /] j c /) to b e a probabilit y distribution/, it has to satisfy the follo wing\nt w o constrain ts/. Eac h probabilit y m ust b e b et w een /0 and /1/, and the sum of the\nprobabilities m ust b e /1/:\n/1/. /8 l /: /0 /\u0014 p /( w /[ i /] /= w\nl\nj c /) /\u0014 /1\n/2/.\nP\nl\np /( w /[ i /] /= w\nl\nj c /) /= /1\n/- A will denote the acoustic data giv en to the recognizer\n/- G /= f g\n/1\n/; /:/:/:/; g\nt\ng is a set of classes or parts of sp eec h /(w e will in tro duce the notion of\nw ord classes in section /2/./5/./3/)\n/- j/; /1 /\u0014 j /\u0014 t will denote an index ranging o v er the set of classes\n/- g /( w /) will denote the class of a w ord w\n/- g /( w /[ i /1 /: i /2/]/) /; i /1 /\u0014 i /2 is a short hand for g /( w /[ i /1/]/) /; /:/: /:/; g /( w /[ i /2/]/)\n/2\nIf there are sev eral W /'s for whic h f /( W /) has the maximal v alue/, ar g max\nW\nf /( W /) will denote one of them\nthat w as pic k ed randomly /.\nCHAPTER /2/. LANGUA GE MODELING F OR SPEECH RECOGNITION /1/9\n/2/./3 The T ask of a Language Mo del\nGiv en the notations in tro duced in the previous section/, w e can no w deriv e the task of a\nlanguage mo del more formally /.\nA go o d sp eec h recognizer should c ho ose the most lik ely string W\n/\u0003\n/, giv en the acoustic\ndata A /. This is expressed b y the follo wing form ula\nW\n/\u0003\n/= ar g max\nW\np /( W j A /) /: /(/2/./1/)\nBased on Ba y es/' form ula /(see for example /[/4/1 /, p/./1/5/0/]/)/, w e can rewrite the probabilit y from\nthe righ t hand side of /2/./1 according to the follo wing equation/:\np /( W j A /) /=\np /( W /) /\u0003 p /( A j W /)\np /( A /)\n/: /(/2/./2/)\np /( W /) is the probabilit y that the w ord sequence W is sp ok en/, p /( A j W /) is the probabilit y that\nthe acoustic signal A is observ ed when W is sp ok en and p /( A /) is the probabilit y of observing\nthe acoustic signal A /. Based on this form ula/, w e can rewrite the maximization of equation\n/2/./1 as\nW\n/\u0003\n/= ar g max\nW\np /( W /) /\u0003 p /( A j W /)\np /( A /)\n/: /(/2/./3/)\nSince p /( A /) is the same for all W /, the factor p /( A /) do es not in/\ruence the c hoice of W and\nmaximizing equation /2/./3 is equiv alen t to maximizing\nW\n/\u0003\n/= ar g max\nW\np /( W /) /\u0003 p /( A j W /) /: /(/2/./4/)\nThe comp onen t of the sp eec h recognizer that calculates p /( A j W /) is called the acoustic mo del/.\nThe comp onen t calculating p /( W /) is the language mo del/.\nWh y is maximizing equation /2/./4 easier than maximizing equation /2/./1/? Or/, in other\nw ords/, wh y did w e use Ba y es form ula to rewrite equation /2/./1/? F or equation /2/./1/, w e w ould\nneed to build a mo del for all p ossible acoustic signals A /. F or equation /2/./4/, w e need a mo del\nfor ev ery p ossible w ord sequence W /. Since A is a con tin uous signal and W is discrete/, the\nlatter is easier/.\nHo w can w e calculate p /( W /) for a giv en string W /? F ormally /, w e can decomp ose the\nprobabilit y of a sequence of w ords p /( W /) as the pro duct of probabilities of eac h w ord w /[ i /]\ngiv en the preceding w ords w /[/1 /: i /BnZr /1/]/:\np /( W /) /=\ni /= n\nY\ni /=/1\np /( w /[ i /] /= w\nl /( i /)\nj w /[/1 /: i /BnZr /1/]/) /(/2/./5/)\nCHAPTER /2/. LANGUA GE MODELING F OR SPEECH RECOGNITION /2/0\nThis decomp osition is appropriate for sp eec h recognition for the follo wing reason/. It\nallo ws us to ev aluate the probabilit y of a pre/\fx w /[/1 /: k /] /; /1 /\u0014 k /\u0014 n of W as the pro duct of\nprobabilities of the k w ords it con tains/:\np /( w /[/1 /: k /]/) /=\ni /= k\nY\ni /=/1\np /( w /[ i /] /= w\nl /( i /)\nj w /[/1 /: i /BnZr /1/]/) /(/2/./6/)\nThis is v ery useful when w e try to p erform the maximization in equation /2/./4/. Rather\nthan ha ving to construct a W co v ering the en tire signal b efore w e can ev aluate it with the\nlanguage mo del/, w e can no w ev aluate partial strings co v ering only parts of the signal as\nthey are constructed/. W e can th us prune the searc h space b y nev er expanding or ev aluating\nunlik ely partial strings\n/3\n/. Using equation /2/./6/, w e can no w precisely de/\fne the task of a\nlanguage mo del/.\nDe/\fnition /1 Given a set of c ontexts C /= f c\n/1\n/; /:/:/:/; c\np\ng /, the task of a language mo del is to\npr ovide a pr ob ability distribution p /( w /[ i /] j c\nk\n/) for e ach c ontext c\nk\n/; /1 /\u0014 k /\u0014 p and a way of\ncho osing a c ontext given the wor ds r e c o gnize d so far/.\nDuring recognition/, all the language mo del has to do is to determine whic h is the curren t\ncon text and to lo ok up the probabilities of w ords in the distribution for this con text/. This is\nfairly straigh t forw ard once the mo del has b een constructed/. The imp ortan t issue/, ho w ev er/,\nis to construct the language mo del prior to recognition/. This requires the de/\fnition of\nthe set of con texts and the estimation of a probabilit y distribution for eac h con text/. These\ncon texts can capture an y information ab out the w ords sp ok en so far/. Ho w ev er/, the language\nmo del m ust b e able to extract this information e/\u000ecien tly during recognition/. An example\nof suc h information is whether the sub ject of the curren t sen tence is animate or not/. The\nlanguage mo del m ust b e able to decide e/\u000ecien tly whether the sub ject of the curren t sen tence\nh yp othesis is animate or not in order to determine the curren t con text and therefore the\ndistribution it is going to use/.\nAs an example of a language mo del/, consider a v ery simplistic mo del that constructs\nonly one distribution/, indep enden t of con text/. The w ord /\\teac h/\" for example will therefore\nb e exp ected with the same probabilit y /, whether the previous w ords w ere /\\He lik es to/\" or\n/\\He ate the/\"/. This is clearly not a v ery go o d mo del since the constrain ts on the follo wing\n/3\nPruning has to b e done with care b ecause it can lead to the pruning of unlik ely partial strings/, that\nw ould b ecome more lik ely giv en the later/, y et unseen parts of the signal/.\nCHAPTER /2/. LANGUA GE MODELING F OR SPEECH RECOGNITION /2/1\nw ords v ary signi/\fcan tly with con text/. A b etter language mo del w ould therefore ha v e sev eral\ndistributions/, one for eac h con text it treats separately /.\nHere w e can see t w o con/\ricting in terests that in/\ruence the construction of a language\nmo del /(see /[/6/2 /]/)/. On the one hand/, the more di/\u000beren t con texts a language mo del can\ndi/\u000beren tiate/, the more distributions it has/, and the b etter it can mo del a language/. On the\nother hand/, eac h distribution needs to b e estimated from training data /(see section /2/./4/)/. The\nmore distributions it has/, the more data it needs/. In other w ords/, giv en a /\fxed amoun t of\ntraining data/, the more distributions a language mo del has/, the less accurate the estimates\nwill b e/. T rying to balance these con/\ricting goals is one of the di/\u000eculties of constructing\nlanguage mo dels and w e will encoun ter this problem again in section /2/./5/.\n/2/./4 Estimati on and Smo othing of Probabilit y Distributions\nfrom F requency Data\nBecause w e will b e using man y probabilit y distributions throughout the rest of this thesis/,\nw e need to examine the estimation and smo othing\n/4\nof probabilit y distributions based on\nfrequency data/. All of the probabilit y distributions are pro duced with similar tec hniques\nand once w e ha v e dealt with these issues here/, w e w on/'t need to address them separately for\neac h probabilit y distribution w e use/. This w a y /, w e can describ e di/\u000beren t language mo dels\non the more abstract lev el of probabilit y distributions/, rather than ha ving to describ e them\non the lev el of frequency data/, requiring man y length y form ulas/.\n/2/./4/./1 Estimation of Probabilit y Distributions from F requency Data\nHo w can w e estimate a probabilit y distribution/? As a simple example/, consider the tossing\nof a coin/. W e w ould lik e to kno w with what probabilit y it comes up head or tails and\nthis will b e its probabilit y distribution/. In tuitiv ely /, w e can estimate this distribution in the\nfollo wing manner/. Thro w the coin N times/, coun t the n um b er of times it comes up heads\nand tails/, and denote these n um b ers with H and T resp ectiv ely /. W e can then estimate the\nprobabilit y of the coin coming up heads or tails as\nH\nN\nor\nT\nN\nresp ectiv ely /.\nHo w can w e extend this to the more general issue of estimating probabilit y distributions\nof ev en ts in certain con texts/? A con text/, in the case of the coin tossing/, could b e the outcome\n/4\nSmo othing attempts to mak e the probabili ties dep end less on the particulariti es of the training data and\nto a v oid zero probabili ties for ev en ts that w ere nev er seen/. W e will see smo othing in more detail later on/.\nCHAPTER /2/. LANGUA GE MODELING F OR SPEECH RECOGNITION /2/2\nof the previous toss or the fact/, that the coin is lying head or tails up in our hand when w e\nthro w it/. Let E /= f e\n/1\n/; /:/:/:/; e\np\ng denote a set of ev en ts and let C /= f c\n/1\n/; /:/:/:/; c\nr\ng denote a set of\ncon texts/. As in section /2/./2/, w e will denote the probabilit y that ev en t e\nl\n/; /1 /\u0014 l /\u0014 p o ccurs\nin con text c\nk\n/; /1 /\u0014 k /\u0014 r as p /( E /= e\nl\nj C /= c\nk\n/)/. F urthermore/, w e will denote the probabilit y\ndistribution o v er all ev en ts in a giv en con text c\nk\nas p /( E j C /= c\nk\n/)/. Our goal is to estimate a\nprobabilit y distribution p /( E j C /= c\nk\n/) for all c\nk\n/; /1 /\u0014 k /\u0014 r /.\nIf w e follo w the example of the coin/, w e simply mak e a large n um b er of trials N and\ncoun t the n um b er of times eac h ev en t o ccurs in eac h con text/, denoted b y O /( E /= e\nl\nj C /= c\nk\n/)/.\nThe o ccurrence coun ts O /( E /= e\nl\nj C /= c\nk\n/) are often referred to as the tr aining data /. W e can\nthen calculate the n um b er of times con text c\nk\no ccurs/, denoted as O /( C /= c\nk\n/)/, as the sum of\nthe n um b er of times eac h ev en t o ccurs in that con text/:\nO /( C /= c\nk\n/) /=\nl /= p\nX\nl /=/1\nO /( E /= e\nl\nj C /= c\nk\n/) /: /(/2/./7/)\nThe sum of the n um b er of o ccurrences of eac h con text will b e the total n um b er of trials/:\nN /=\nk /= r\nX\nk /=/1\nO /( C /= c\nk\n/) /: /(/2/./8/)\nAs in the example of the coin/, w e can then get an estimate of p /( E /= e\nl\nj C /= c\nk\n/) b y dividing\nthe n um b er of times ev en t e\nl\no ccurred in con text c\nk\nb y the total n um b er of times con text\nc\nk\no ccurred/:\np /( E /= e\nl\nj C /= c\nk\n/) /=\nO /( E /= e\nl\nj C /= c\nk\n/)\nO /( C /= c\nk\n/)\n/=\nO /( E /= e\nl\nj C /= c\nk\n/)\nP\nl /= p\nl /=/1\nO /( E /= e\nl\nj C /= c\nk\n/)\n/: /(/2/./9/)\nF rom all p ossible v alues w e can estimate for p /( E /= e\nl\nj C /= c\nk\n/)/, the v alue estimated ab o v e\nis the one that has the highest lik eliho o d of pro ducing the observ ed data\n/5\n/. This metho d\nof estimation is therefore called maxim um lik eliho o d estimation /(MLE/)/. As p oin ted out in\n/[/1/4/0 /]/, the principle of maxim um lik eliho o d estimation w as /\frst prop osed b y Sir R/. A/. Fisher\nin /1/9/2/6 /(see for example /[/3/7/]/)/. Maxim um lik eliho o d estimation is a v ery simple metho d that\ncan b e used for a wide range of problems/. Ev en though more sophisticated metho ds are\na v ailable /(see for example /[/2/0 /]/)/, w e use the maxim um lik eliho o d estimation for reasons of\nsimplicit y for our w ork/.\n/5\nStrictly sp eaking/, it is the v alue that has the highest lik eliho o d of pro ducing the observ ed data giv en\nsome additional assumptions ab out the distribution of probabilit y v alues/.\nCHAPTER /2/. LANGUA GE MODELING F OR SPEECH RECOGNITION /2/3\nF rom no w on/, w e will denote the quotien t of the o ccurrences in equation /2/./9 b y the\nfrequency f /( E /= e\nl\nj C /= c\nk\n/)/:\nf /( E /= e\nl\nj C /= c\nk\n/) /=\nO /( E /= e\nl\nj C /= c\nk\n/)\nO /( C /= c\nk\n/)\n/=\nO /( E /= e\nl\nj C /= c\nk\n/)\nP\nl /= p\nl /=/1\nO /( E /= e\nl\nj C /= c\nk\n/)\n/: /(/2/./1/0/)\n/2/./4/./2 Smo othing of Probabilit y Distributions\nIn the case of a language mo del/, it is imp ortan t to a v oid zero probabilities for ev en ts that\nnev er o ccurred in the data/. The reason for this is that the sp eec h recognizer should correctly\ndeco de what the user said/. Since w e cannot prev en t the user from sa ying nonsensical or\nungrammatical w ords at some p oin t in the sen tence/, the language mo del should not giv e\na zero probabilit y to an y of the w ords at an y time/. If it did/, suc h a w ord could not b e\nrecognized ev en if it w as the one said b y the user/. If w e use the maxim um lik eliho o d\nestimation for ev en ts that nev er o ccurred/, they will receiv e a probabilit y estimate of zero\nb ecause their o ccurrence coun t is zero/. In order to a v oid this/, the probabilit y estimates ha v e\nto b e smo othed/. This results in giving a small probabilit y to unseen ev en ts and in reducing\nthe probabilit y of other ev en ts/.\nThe tec hniques often used for smo othing are the addition of a small constan t probabilit y\n/(see for example /[/8/5 /]/)/, deleted in terp olation /(/[/6/6 /]/)/, bac king o/\u000b /(/[/7/3 /]/)/, di/\u000beren t discoun ting\nmetho ds /(/[/1/1/0 /]/)/, Go o d/-T uring form ula /(/[/4/8 /]/) or enhanced Go o d/-T uring form ula /(/[/2/0 /]/)/. In\nc ho osing one of these metho ds for our w ork/, w e had t w o criteria/. First/, since smo othing is\nnot an issue of particular in terest to our w ork/, w e w ould lik e the metho d to b e fairly simple/.\nSecond/, in order to ensure that the metho d is acceptable to other researc hers/, the metho d\nshould b e used b y other researc hers in similar language mo dels/. Adding a small constan t\nprobabilit y satis/\fes b oth criteria and this is the metho d w e c hose/. W e will presen t it in the\nfollo wing/.\nSupp ose w e w an t to estimate p /( E /= e\nl\nj C /= c\nk\n/) based on the frequency coun ts f /( E /=\ne\nl\nj C /= c\nk\n/)/. In a /\frst approac h/, w e can simply use\np /( E /= e\nl\nj C /= c\nk\n/) /= f /( E /= e\nl\nj C /= c\nk\n/) /: /(/2/./1/1/)\nHo w ev er/, some of the ev en ts in E ma y nev er ha v e b een observ ed in a certain con text/, for\nexample ev en t e\np\nin con text c\nk\n/. Th us/, w e w ould obtain p /( E /= e\np\nj C /= c\nk\n/) /= /0/. As p oin ted\nout in the preceding paragraph/, w e should a v oid zero probabilities in language mo dels/. A\nCHAPTER /2/. LANGUA GE MODELING F OR SPEECH RECOGNITION /2/4\nsimple w a y to a v oid zero probabilities is to add a small constan t v alue v\n/1\nto all probabilities/:\np /( E /= e\nl\nj C /= c\nk\n/) /= f /( E /= e\nl\nj C /= c\nk\n/) /+ v\n/1\n/: /(/2/./1/2/)\nAdding v\n/1\nindeed a v oids zero probabilities/, but the sum of probabilities of all p ossible ev en ts\nis no w bigger than /1 and the resulting distribution is not a probabilit y distribution an y more\n/(see section /2/./2/)/:\nX\ne\nl\n/2 E\n/( f /( E /= e\nl\nj C /= c\nk\n/) /+ v\n/1\n/) /=\nX\ne\nl\n/2 E\nf /( E /= e\nl\nj C /= c\nk\n/) /+ j E j /\u0003 v\n/1\n/= /1 /+ j E j /\u0003 v\n/1\n/: /(/2/./1/3/)\nIn order to comp ensate for the extra probabilit y mass of j E j /\u0003 v\n/1\n/, w e will simply m ultiply\nthe frequencies f /( E /= e\nl\nj C /= c\nk\n/) with the constan t v alue v\n/2\n/= /1 /BnZr j E j /\u0003 v\n/1\n/:\np /( E /= e\nl\nj C /= c\nk\n/) /= v\n/2\n/\u0003 f /( E /= e\nl\nj C /= c\nk\n/) /+ v\n/1\n/: /(/2/./1/4/)\nW e can v erify that the sum of the probabilities of all w ords no w adds up to /1/:\nX\ne\nl\n/2 E\n/( v\n/2\n/\u0003 f /( E /= e\nl\nj C /= c\nk\n/) /+ v\n/1\n/) /= v\n/2\n/\u0003\nX\ne\nl\n/2 E\nf /( E /= e\nl\nj C /= c\nk\n/) /+ j E j /\u0003 v\n/1\n/(/2/./1/5/)\n/= v\n/2\n/\u0003 /1 /+ j E j /\u0003 v\n/1\n/= /1 /BnZr j E j /\u0003 v\n/1\n/+ j E j /\u0003 v\n/1\n/= /1 /: /(/2/./1/6/)\nTh us/, the smo othed estimate in equation /2/./1/4 constitutes a true probabilit y distribution\nand a v oids zero probabilities/.\n/2/./4/./3 Assumptions ab out Probabilit y Distributions in our W ork\nIn the rest of the thesis/, w e will often refer to probabilit y distributions and all of these\ncan b e estimated using the metho ds presen ted ab o v e/. F or example/, w e will denote with\np /( w /[ i /] /= w\nl\nj w /[ i /BnZr /1/]/) the probabilit y that the i\nth\nw ord of the sequence is w\nl\n/, giv en that the\nprevious w ord w /[ i /BnZr /1/] is w\nl /( i /BnZr /1/)\n/(see section /2/./2/)/. It is understo o d that this probabilit y is\nestimated appro ximately as presen ted ab o v e/. Summing up/, this estimation roughly w orks\nas follo ws/: coun t the n um b er of times w\nl /( i /BnZr /1/)\no ccurs in the training data/, coun t the n um b er\nof times it is follo w ed b y the w ord w\nl\n/, and then estimate the probabilit y as the quotien t\nof these t w o n um b ers/. Moreo v er/, to ensure that our results are easily repro ducible/, w e will\ngiv e the complete form ulas/, including the smo othing/, for the probabilit y distributions w e\nactually implemen ted/.\nCHAPTER /2/. LANGUA GE MODELING F OR SPEECH RECOGNITION /2/5\n/2/./5 Review of Existing Language Mo dels\nHa ving in tro duced the k ey issues of language mo deling/, w e will no w review man y commonly\nused language mo dels/. Ho w ev er/, rather than giving the often complicated form ula in terms\nof frequency coun ts/, w e will describ e eac h mo del on a more abstract lev el in terms of its\nprobabilit y distributions/. This w a y /, w e don/'t ha v e to consider estimation and smo othing\nissues/, but can fo cus on the follo wing conceptually imp ortan t issues/:\n/1/) W e sa w in section /2/./3 that the task of a language mo del is to pro vide a probabilit y\ndistribution for a set of suitably de/\fned con texts/. Ho w is the con text de/\fned or/, in\nother w ords/, on what do es the probabilit y distribution dep end in eac h mo del/? This is\na crucial p oin t of eac h mo del b ecause it sho ws whic h linguistic regularities /(e/.g/. those\nthat in v olv e only the preceding t w o w ords/) it can capture/. W e will pro vide an in tuitiv e\ndescription of the con texts as w ell as the form ula eac h mo del uses to instan tiate the\ngeneral p /( w /[ i /] /= w\nl\nj c /) with a sp eci/\fc con text c /.\n/2/) Ho w man y probabilities ha v e to b e estimated in eac h mo del/? This is imp ortan t b ecause\nit determines the amoun t of data needed to train the mo del/. This in turn determines\nthe situations and tasks in whic h eac h mo del can b e used/.\n/2/./5/./1 Con text Indep enden t Mo dels\nCon text indep enden t mo dels ha v e only one probabilit y distribution/. This distribution is\nused to assign probabilities to w ords/, indep enden t of the curren t con text/.\nThe most simplistic/, con text indep enden t mo del is the mo del that treats all w ords as\nb eing equiprobable/. Giv en the v o cabulary V /, this results in the follo wing form ula/:\np /( w /[ i /] /= w\nl\nj c /) /=\n/1\nj V j\n/(/2/./1/7/)\nThis mo del do es not ha v e an y probabilities to estimate and therefore do es not need an y\ntraining data/. Ev en though this mo del satis/\fes the requiremen ts of a language mo del/, it\nis of no use to a sp eec h recognizer b ecause all w ords receiv e the same probabilit y /. It will\ntherefore ha v e no in/\ruence on the ranking of the w ords/.\nA more sensible w a y to construct a con text indep enden t mo del is to estimate the prob/-\nabilit y of eac h w ord according to its frequency /, but indep enden t of con text/. This leads to\nthe follo wing mo del/:\np /( w /[ i /] /= w\nl\nj c /) /= p /( w /[ i /] /= w\nl\n/) /: /(/2/./1/8/)\nCHAPTER /2/. LANGUA GE MODELING F OR SPEECH RECOGNITION /2/6\nThis mo del also has only one distribution/, but it has j V j probabilities to estimate/. Ev en\nthough this mo del is v ery simple/, it is actually b eing used in a commercial sp eec h recognizer\n/(/[/5/6 /]/)/. Because it is a sp ecial case /( N /= /1/) of the N /-gram mo del /(see the next section/)/, it\nis sometimes referred to as the uni/-gram mo del/.\nThe adv an tage of the uni/-gram mo del is that it requires only v ery little training data/.\nIts disadv an tage is that the probabilit y of a w ord will alw a ys b e the same/, indep enden t of\nthe con text/.\n/2/./5/./2 N /-gram Mo dels\nThe previous mo dels had only one distribution/, indep enden t of con text/. F rom no w on/, w e\nwill mak e the probabilit y distributions dep end on con text/. The di/\u000beren t mo dels w e will see\nwill mostly di/\u000ber in the kind of con text they consider/.\nBefore lo oking at the general N /-gram mo del/, w e will consider the sp ecial case of the\nbi/-gram mo del/, where N /= /2/. When w e lo ok at a fragmen t of a sen tence/, e/.g/. /\\He ate\nthe/\"/, it is quite clear that certain w ords are not v alid con tin uations of the sen tence/. F or\nexample/, if the last w ord in our sen tence fragmen t/, namely /`the/'/, is b eing follo w ed b y a\nv erb/, it will not lead to a grammatical sen tence\n/6\n/. It is therefore in tuitiv ely app ealing to\nmak e the probabilit y distribution dep end on the previous w ord/. The con text is therefore\nsimply the preceding w ord/. Ev en though this captures only a v ery small amoun t of con text/,\nit do es capture the restrictions in the ab o v e example/. Moreo v er/, there is considerable\nempirical evidence from corpus linguistics that the immediate con text of man y w ords is\nv ery predictable /(see the discussion in section /5/./2/./3/)/. This is esp ecially true for /\fxed w ord\norder languages lik e English /(see /[/2/3 /, p/./3/2/]/)/, where the lo cal constrain ts are quite p o w erful/.\nMaking the probabilit y distribution dep end on the previous w ord leads to the follo wing\nform ula/:\np /( w /[ i /] /= w\nl\nj c /) /= p /( w /[ i /] /= w\nl\nj w /[ i /BnZr /1/]/) /: /(/2/./1/9/)\nIn the more general form of the mo del/, the so called N /-gram mo del/, the probabilit y of\nthe i\nth\nw ord of an input sen tence is made dep enden t on the preceding N /BnZr /1 w ords/. The\ncon text is therefore de/\fned b y the preceding N /BnZr /1 w ords/:\np /( w /[ i /] /= w\nl\nj c /) /= p /( w /[ i /] /= w\nl\nj w /[ i /BnZr N /+ /1 /: i /BnZr /1/]/) /: /(/2/./2/0/)\n/6\nEv en if a w ord can b e a noun and a v erb/, eac h o c curr enc e of a w ord has only one grammatical function/.\nTh us/, if the article is follo w ed b y an o ccurrence of a v erb/, it will not lead to a grammatical sen tence/.\nCHAPTER /2/. LANGUA GE MODELING F OR SPEECH RECOGNITION /2/7\nF or eac h N /BnZr /1 tuple of w ords of the v o cabulary /, the N /-gram mo del has a separate\nprobabilit y distribution and/, at a giv en p oin t in a sen tence/, the distribution it c ho oses is\ndetermined b y the previous N /BnZr /1 w ords/. There are j V j\nN /BnZr /1\ndi/\u000beren t N /BnZr /1 tuples and this is\nthe n um b er of distributions of the N /-gram mo del/. F or eac h distribution/, w e ha v e to estimate\nj V j probabilities/. This giv es a total of j V j\nN\nprobabilities to estimate/. F or a v o cabulary size\nof /1/0/,/0/0/0 w ords/, the n um b er of probabilities that need to b e estimated increases dramatically\n/(exp onen tially/) with N /. F or example/, for N /= /3/, w e ha v e /1/0\n/1/2\nprobabilities/. Therefore/, in\npractice/, N is usually tak en to b e t w o /(see for example /[/7/9 /]/, /[/3/4 /]/) or three /(/[/5 /]/, /[/1/3 /]/, /[/6/3 /]/,\n/[/2/7 /]/, /[/6/7 /]/, /[/6/5 /]/)/.\nThe adv an tage of the N /-gram mo del is that it captures all the information pro vided b y\nthe preceding N /BnZr /1 w ords/. Judging from its success/, this is quite an imp ortan t source of\ninformation/, esp ecially for /\fxed w ord order languages lik e English/. Its disadv an tage is the\nenormous amoun t of training data needed to train all the probabilities/. F or example/, in\n/[/1/3 /]/, sev eral h undred millions of w ords are used for training/.\nAs p oin ted out in /[/6/3 /]/, /\ffteen y ears after the /\frst use of a tri/-gram mo del in large\nv o cabulary sp eec h recognition /(/[/5 /]/)/, the tri/-gram mo del is still considered one of the b est\np erforming mo dels and it is used as a comp onen t in man y other mo dels/.\n/2/./5/./3 N /-p os Mo dels\nThe ma jor problem with the N /-gram mo dels is the amoun t of data required for training/.\nMoreo v er/, one can argue that some of the lo cal constrain ts dep end less on the iden tit y of\nthe previous w ords than on their grammatical function/. This leads to the idea of grouping\nw ords together in classes and making the probabilities dep end on these classes/. T raditionally /,\nthese classes are called parts of sp eec h /(p os/) in linguistics whic h explains the name of the\nN /-p os mo dels/. Within the class of N /-p os mo dels/, there are man y di/\u000beren t v arian ts/. In the\nfollo wing/, after starting with a v ery basic mo del/, w e will presen t t w o mo di/\fcations that lead\nto the mo del used in our implemen tation/.\nLet G /= f g\n/1\n/; /:/:/:/; g\nj\n/; /:/:/:g\nt\ng denote the set of classes/, let g /( w /) denote the class of a giv en w ord\nw and let g /( w /[ i /1 /: i /2/]/) /; /1 /\u0014 i /1 /\u0014 i /2 /\u0014 n b e a short form for g /( w /[ i /1/]/) /; g /( w /[ i /1 /+ /1/]/) /; /:/:/:/; g /( w /[ i /2 /]/)\n/7\n/. In the N /-p os mo del/, the probabilities dep end on the classes of the previous N /BnZr /1 w ords/.\n/7\nAll of these notations are also men tioned in the section on notations /(/2/./2/)/.\nCHAPTER /2/. LANGUA GE MODELING F OR SPEECH RECOGNITION /2/8\nTherefore/, the con text is de/\fned b y the preceding N /BnZr /1 classes/:\np /( w /[ i /] /= w\nl\nj c /) /= p /( w /[ i /] /= w\nl\nj g /( w /[ i /BnZr N /+ /1 /: i /BnZr /1/]/)/) /: /(/2/./2/1/)\nThis mo del has j G j\nN /BnZr /1\ndistributions and requires the estimation of j G j\nN /BnZr /1\n/\u0003 j V j proba/-\nbilities/. F or common v alues of j G j /= /2/0/0 /; j V j /= /1/0 /; /0/0/0 and N /= /3/, the N /-p os /(e/.g/. tri/-p os/)\nmo del has /8 /\u0003 /1/0\n/1/0\nprobabilities/. This is a signi/\fcan t reduction with resp ect to the tri/-gram\nmo del/.\nF urthermore/, one can argue that the class of the previous w ord mostly restricts the\nclass of the next w ord/, but not its iden tit y /. Hence/, w e can deriv e probabilities in a t w o/-step\npro cess/. First/, w e predict the class of the next w ord based on the classes of the previous\nN /BnZr /1 w ords/. Then/, w e predict the actual w ord giv en its class/, but indep enden t of preceding\nclasses/. This leads to the follo wing form ula\np /( w /[ i /] /= w\nl\nj c /) /= p /( g /( w /[ i /]/) j g /( w /[ i /BnZr N /+ /1 /: i /BnZr /1/]/)/) /\u0003 p /( w /[ i /] /= w\nl\nj g /( w /[ i /]/)/) /: /(/2/./2/2/)\nThis mo del has the same de/\fnition of a con text/, but it only has j G j\nN /BnZr /1\n/\u0003 j G j /+ j G j /\u0003 j V j\nfree parameters/. F or the same v alues of j G j and j V j /, this corresp onds to /1/0\n/8\nprobabilities/,\na further reduction compared to the previous form ula/.\nThe ab o v e mo del/, used for example in /[/1/5 /]/, /[/2/9 /]/, /[/3/0 /]/, /[/6/0 /]/, /[/7/8 /]/, /[/1/1/2 /]/, /[/1/5/6 /] and /[/1/5/5 /]/,\nrequires disjoin t classes/. Ho w ev er/, one w ord can b elong to sev eral classes/. F or example/, the\nw ord /`ligh t/' can b e a noun/, v erb or adjectiv e/. Hence/, the probabilit y of seeing the w ord\n/`ligh t/' is the probabilit y of seeing it as a noun plus the probabilit y of seeing it as a v erb plus\nthe probabilit y of seeing it as an adjectiv e/. This leads to the follo wing form ula/, where the\nprobabilities are summed o v er all p ossible classes in G /:\np /( w /[ i /] /= w\nl\nj c /) /=\nX\ng\nj\n/2 G\np /( g /( w /[ i /]/) /= g\nj\nj g /( w /[ i /BnZr N /+ /1 /: i /BnZr /1/]/)/) /\u0003 p /( w /[ i /] /= w\nl\nj g /( w /[ i /]/) /= g\nj\n/) /: /(/2/./2/3/)\nThis is equiv alen t to summing o v er all classes w /[ i /] can actually b elong to since the second\nterm in the form ula will b e zero for classes that do not con tain w /[ i /]/.\nIn order to relate the N /-p os mo del to the N /-gram mo del/, it is quite rev ealing to lo ok\nat the extreme cases of N /-p os mo dels/, e/.g/. at a mo del with only one class and at a mo del\nwith one class p er w ord /(see Figure /2/./2/)/. If a N /-p os mo del has only one class/, then kno wing\nthe classes of the N /BnZr /1 previous w ords do es not con tain an y information ab out the con text\nb ecause the last N /BnZr /1 w ords alw a ys b elong to the same single class/. Similarly /, the class\nCHAPTER /2/. LANGUA GE MODELING F OR SPEECH RECOGNITION /2/9\niden tical to\nuni/-gram\niden tical to\nN/-gram\n/-\nn um b er of\nclasses in a\nN/-p os mo del\n/1\n/2/5 /5/0/0\n/././. /././.\ncommonly used range\nof N/-p os mo dels\nV\nFigure /2/./2/: The relationship b et w een N /-gram and N /-p os mo del\nof the w ord to predict do es not con tain an y information ab out the w ord to predict or the\ncon text/, b ecause all w ords b elong to this class/. Th us/, since b oth factors in equation /2/./2/3\nare indep enden t of the con text/, the prediction of the next w ord will b e indep enden t of the\ncon text/. W e therefore obtain a con text indep enden t mo del with only one distribution/. This\nmo del is iden tical to the uni/-gram mo del of section /2/./5/./1/. A t the other extreme/, w e ha v e a\nmo del with a separate class p er w ord/. If a mo del has one class p er w ord/, then predicting\nthe w ord giv en its class b ecomes trivial/, b ecause eac h class con tains only one w ord/. In\nthis case/, de/\fning the con text in terms of the classes of the previous N /BnZr /1 w ords actually\nmeans de/\fning the con text in terms of the iden tit y of the previous N /BnZr /1 w ords/. Th us/,\nthe second factor in equation /2/./2/3 will alw a ys b e equal to one and the /\frst factor will b e\nthe prediction of the next w ord giv en the previous N /BnZr /1 w ords/. In other w ords/, w e obtain\nthe N /-gram mo del from section /2/./5/./2/. F rom these observ ations/, w e can see that the N /-p os\nmo del is somewhere b et w een the unigram and the N /-gram mo del/, dep ending on the n um b er\nof classes it uses/.\nThis mo del/, used for example in /[/8/5 /]/, /[/3/2 /]/, /[/6/2 /] and /[/1/4/7 /]/, has the same n um b er of\ndistributions and parameters as the previous mo del /{ the di/\u000berence is that the classes it\nuses ha v e to b e disjoin t/.\nThe adv an tage of the N /-p os mo del is that it requires far less training data than the\nN /-gram mo del/, while still considering the class information of the previous N /BnZr /1 w ords/.\nIts disadv an tage is that its distributions dep end on classes/, and not on particular w ords/.\nAs an example/, supp ose that the class AR TICLE con tains the singular article /\\a/\" as w ell\nas other articles lik e /\\the/\"/. In the case of a bi/-p os mo del/, w e will ha v e one distribution/,\ngiv en that the last w ord w as an article/. Ho w ev er/, if w e knew that the last w ord w as the\narticle /\\a/\"/, the distribution w ould b e signi/\fcan tly di/\u000beren t since it w ould not con tain plural\nnouns/. W e will come bac k to this in section /4/./4/./2/, page /8/4/. In general/, the p erformance\nCHAPTER /2/. LANGUA GE MODELING F OR SPEECH RECOGNITION /3/0\nof the N /-p os mo del is not as go o d as an N /-gram mo del trained on su/\u000ecien t data/, but it\nis b etter than an N /-gram mo del trained with insu/\u000ecien t data/. Here/, w e can see again the\ncon/\ricting in terests in constructing a language mo del that w e sa w on page /2/1/.\n/2/./5/./4 Decision T ree Based Mo dels\nIn all previous mo dels/, the n um b er of distributions is /\fxed indep enden tly of the particular/-\nities of the training data/. F or example/, in the tri/-p os mo del/, there is a separate distribution\nfor eac h pair of preceding classes/. This is done for all pairs/, ev en though some of the re/-\nsulting distributions ma y b e v ery similar/. As an example/, the distribution in the con texts\n/[v erb/, article/] and /[prep osition/, article/] migh t b e v ery similar to eac h other and ev en to the\ndistribution in the con text /[article/]/. This is a serious disadv an tage b ecause it leads to the\nconstruction of v ery similar distributions/, whic h do not result in impro v ed p erformance/.\nThe statistical tec hnique of decision trees can a v oid this problem/. It has b een used\nrecen tly for di/\u000beren t tasks in statistical natural language pro cessing /(/[/7 /]/, /[/9 /]/, /[/1/5/0 /]/, /[/1/4/8 /]/,\n/[/8/7 /]/, /[/1/0 /]/)/. A go o d in tro duction to a sp eci/\fc metho d for constructing decision trees/, called\nCAR T/, is giv en in /[/1/1 /]/. Other recen t algorithms are presen ted in /[/4/5/] and /[/1/6 /]/. More on\ndecision trees in general can b e found in /[/4/6 /]/. In the follo wing/, w e will brie/\ry outline the\nbasic idea and its application to language mo deling/.\nA decision tree con tains the probabilit y distributions of a language mo del and a metho d\nof iden tifying the distribution that should b e used in the curren t con text/. In suc h a tree/, eac h\nleaf con tains exactly one distribution and eac h in ternal no de con tains exactly one question\nab out the con text/. The follo wing metho d is used to /\fnd the distribution /(or the leaf /) that\nshould b e used in the curren t con text/. Starting at the ro ot no de/, w e lo ok at the question\ncon tained in the curren t no de/. Based on the answ er to this question/, w e mo v e to one of the\nc hildren/, making it the curren t no de/. This pro cess con tin ues un til w e arriv e at a leaf/. W e\nthen use the probabilit y distribution asso ciated with this leaf/.\nT o construct a decision tree/, w e start with only one no de/, the ro ot/, con taining only one\nprobabilit y distribution/. Giv en a prede/\fned set of p ossible questions ab out the con text/, w e\nc ho ose one that maximizes some criterion/, e/.g/. p erformance of the tree on test data/. This\nquestion is then placed at the ro ot no de/, the c hildren are created/, and separate distributions\nare estimated for eac h c hild/. This pro cess con tin ues recursiv ely un til some stopping criterion/,\ne/.g/. none of the questions lead to impro v emen ts/, is met/. A t eac h leaf/, the distribution is\nestimated in the follo wing manner/. Our training data consists of a set of data p oin ts/, e/.g/.\nCHAPTER /2/. LANGUA GE MODELING F OR SPEECH RECOGNITION /3/1\nthe o ccurrence of some w ord in a giv en con text/. Eac h of these data p oin ts starts out at the\nro ot no de and then/, b y answ ering the questions at eac h in ternal no de it encoun ters/, ends up\nat some leaf no de/. Once all data p oin ts ha v e reac hed the lea v es/, w e estimate a probabilit y\ndistribution for eac h leaf based on the frequencies of ev en ts in the data it con tains/.\nAs an example/, supp ose w e ha v e a set of training data con taining the iden tit y of a w ord\nw /[ i /]/, giv en the previous w ord w /[ i /BnZr /1/]/. One suc h data p oin t could b e /(w/[i/-/1/]/,w/[i/]/)/=/(the/,\nw eather/)/. Supp ose further/, that the set of p ossible questions w e can ask is /\\Is the part\nof sp eec h of the previous w ord g\nj\n/?/\"/, where g\nj\ncan b e an y part of sp eec h/. Our goal no w\nis to construct a language mo del based on this training data and on this set of questions/.\nW e will start out with one no de con taining only one distribution/. As for ev ery other leaf/,\nthis distribution is estimated from the frequencies of ev en ts in its data/. In this case/, the\ndistribution will corresp ond to the uni/-gram /(see section /2/./5/./1/)/, giving the relativ e frequency\nof eac h w ord/. F or eac h question w e can ask/, w e construct the c hildren/, their separate\ndistributions and measure the p erformance of the resulting language mo del on testing data/.\nW e then c ho ose the question that leads to the b est p erforming mo del and actually put this\nquestion in the ro ot no de/. W e then create t w o c hildren/, send eac h data p oin t to the righ t\nor left c hild/, dep ending on its answ er to the question w e just c hose/, and construct separate\ndistributions for the c hildren based on the frequency of ev en ts in their data/. W e can then\napply this pro cess recursiv ely to eac h c hild un til some stopping criterion is met/, e/.g/. none of\nthe questions leads to a further impro v emen t/. This terminates the pro cess and the resulting\ndecision tree is our language mo del/.\nAll the mo dels w e ha v e seen so far can b e represen ted in terms of a decision tree/. Decision\ntrees are therefore more general language mo dels/. Their adv an tage is that the n um b er of\ndistributions is not /\fxed in adv ance/, but it is determined b y the training data/. The n um b er\nof distributions is therefore in general more appropriate than if one of the previous mo dels\nis used/. Its disadv an tage is that the task of constructing the tree is computationally v ery\nexp ensiv e/. And ev en though the resulting mo del frequen tly has less distributions to store/,\nthe impro v emen t in p erformance is often relativ ely small /(see /[/7/]/, /[/1/5/0 /]/)/.\n/2/./5/./5 Dynamic/, Adaptiv e and Cac he/-Based Mo dels\nIn all of the mo dels w e ha v e seen so far/, the probabilit y distributions are estimated from\nthe training data and do not c hange further when the mo del is used on di/\u000beren t texts or on\ndi/\u000beren t p ortions of a text/. F or this reason/, they are called static language mo dels /(/[/6/7 /]/)/.\nCHAPTER /2/. LANGUA GE MODELING F OR SPEECH RECOGNITION /3/2\nHo w ev er/, in tuitiv ely /, it is v ery clear that some w ords are v ery /\\burst y/\" of nature/: they do\nnot o ccur in a large p ortion of the text/, but then o ccur frequen tly in one small section/.\nThere is also strong empirical evidence to supp ort this in tuition/. In /[/6/8 /] and /[/6/9 /]/, it is\nsho wn for three di/\u000beren t corp ora that frequencies of w ords v ary greatly b et w een di/\u000beren t\nt yp es of text/. Language mo dels that try to capture this short term /\ructuation of the o v erall\nfrequencies are called dynamic/, adaptiv e or cac he/-based language mo dels/.\nSo far/, there has b een v ery little w ork on dynamic language mo dels in the literature/.\nThe idea w as /\frst prop osed b y R/. Kuhn in /[/8/4 /]/, then dev elop ed in /[/8/5 /]/, /[/8/6 /] and /[/8/8 /]/. It w as\nfurther tested in /[/6/7 /]/, /[/7/9 /] and /[/2/7 /]/. Since all these approac hes ha v e a considerable degree\nof similarit y /, w e will only presen t one of them in more detail/.\nIn /[/6/7 /]/, the o ccurrences of the N most recen t w ords w /[ i /BnZr N /: i /BnZr /1/] /(e/.g/. N /= /1/0/0/0/) are\nconsidered as separate training data/. Based on this data/, separate unigram/, bi/-gram and tri/-\ngram frequency distributions are estimated/. They are com bined using one of the smo othing\nmetho ds to obtain a single dynamic tri/-gram estimate denoted b y p\ndy n\n/( w /[ i /] j w /[ i /BnZr /2 /: i /BnZr /1/]/)/.\nThis distribution assigns a non/-zero probabilit y to all the w ords that o ccurred within the last\nN w ords/. T o a v oid zero probabilities of the remaining w ords/, the mo del is com bined with the\nstatic tri/-gram mo del p\nsta\n/( w /[ i /] j w /[ i /BnZr /1 /: i /BnZr /1/]/) to giv e the com bined mo del p\ncom\n/( w /[ i /] j w /[ i /BnZr /2 /:\ni /BnZr /1/]/)/. This com bination is p erformed b y a linear in terp olation/:\np\ncom\n/( w /[ i /] j w /[ i /BnZr /2 /: i /BnZr /1/]/) /= /\u0015 /\u0003 p\ndy n\n/( w /[ i /] j w /[ i /BnZr /2 /: i /BnZr /1/]/) /+ /(/1 /BnZr /\u0015 /) /\u0003 p\nsta\n/( w /[ i /] j w /[ i /BnZr /2 /: i /BnZr /1/]/) /:\n/(/2/./2/4/)\nA w ell kno wn estimation algorithm/, the forw ard/-bac kw ard algorithm /(/[/6 /]/)/, is used to esti/-\nmate the in terp olation parameter /\u0015 /. /\u0015 ranges from /0/./0/7 to /0/./2/8 dep ending on the static\ntri/-gram mo del used and on the cac he size N /.\nUsing the com bined mo del/, the impro v emen t in p erformance of the language mo del\nranges b et w een /8/% and /2/3/%/. As rep orted in the same pap er/, for an isolated sp eec h rec/-\nognizer/, this leads to a reduction in error rates ranging from /5/% for shorter do cumen ts to\n/2/4/% for larger do cumen ts/. This is b ecause the cac he starts out empt y at the b eginning of\neac h do cumen t and it tak es some time b efore its estimates accurately re/\rect the particular\ndo cumen t/. Impro v emen ts of ab out the same size are rep orted in /[/8/5 /]/, /[/8/6/] and /[/7/9 /]/.\nCHAPTER /2/. LANGUA GE MODELING F OR SPEECH RECOGNITION /3/3\n/2/./6 Summ ary\nIn this c hapter/, w e ga v e an o v erview of the di/\u000beren t comp onen ts of a sp eec h recognizer/,\nde/\fned the task of the comp onen t cen tral to this thesis/, the language mo del/, and review ed\nthe most commonly used language mo dels/.\nW e b egan b y giving an o v erview of the di/\u000beren t comp onen ts of a sp eec h recognizer\ndesigned according to the sto c hastic approac h /(see section /1/./2/)/. W e brie/\ry explained the\ntasks of the signal pro cessor/, the v ector quan tizer/, the acoustic mo del and the language\nmo del and ho w they in teract to p erform the mapping from the acoustic signal to a string\nof w ords/. Moreo v er/, w e in tro duced man y of the notations used throughout this thesis/.\nAfter ha ving describ ed the task of the language mo del in tuitiv ely /, w e then de/\fned the\ntask of the language mo del more formally as follo ws/. Giv en a set of con texts/, the task of\na language mo del is to pro vide a probabilit y distribution for eac h con text and to pro vide a\nw a y of c ho osing a con text giv en the w ords recognized so far/.\nSince probabilit y distribution are used frequen tly in this thesis/, w e explained ho w prob/-\nabilit y distributions can b e estimated from frequency data using the maxim um lik eliho o d\ncriterion/. F urthermore/, w e brie/\ry addressed the issue of smo othing probabilit y distributions\nand presen ted a v ery simple smo othing tec hnique/, the addition of a small/, constan t baseline\nprobabilit y /. The estimation and smo othing metho ds that ha v e b een presen ted here are the\nones w e use for our w ork/.\nHa ving seen the ma jor issues in constructing language mo dels/, w e review ed man y existing\nlanguage mo dels /(con text indep enden t/, N /-gram/, N /-p os/, decision tree based and adaptiv e\nmo dels/)/. F or eac h language mo del presen ted/, w e fo cussed on t w o conceptually imp ortan t\nissues/: ho w do es the mo del de/\fne the con text c for its probabilit y distributions p /( w /[ i /] j c /)\nand ho w man y probabilities do es the mo del ha v e to estimate/. The /\frst p oin t is imp ortan t\nb ecause it determines whic h linguistic regularities /(e/.g/. the ones in v olving only the t w o\npreceding w ords/) the language mo del can capture/. The second p oin t is imp ortan t b ecause\nit determines the amoun t of data needed to train the mo del and therefore the situations in\nwhic h the mo del can b e used/.\nChapter /3\nAnalyzing and Impro ving\nLanguage Mo dels\nIn the last c hapter/, w e review ed man y existing language mo dels for sp eec h recognition/. Ev en\nthough some of these mo dels ma y ac hiev e go o d p erformance/, the t yp e of sp eec h recognition\ntasks w e can tac kle with existing sp eec h recognition tec hnology /, including the language\nmo dels/, is still limited/. Ho w can w e impro v e the language mo dels so that w e can tac kle\nmore complex sp eec h recognition tasks/? In this c hapter/, w e prop ose the cen tral idea of this\nthesis/, namely trying to analyze errors of existing language mo dels in order to subsequen tly\nimpro v e the mo dels/. W e /\frst presen t some in tuitions on impro ving language mo dels /(section\n/3/./1/) follo w ed b y a deriv ation of the standard measure used to ev aluate language mo dels\n/(section /3/./2/)/. Giv en a term closely related to this p erplexit y measure/, the logarithm of\nthe total probabilit y LT P /, w e then de/\fne a w eakness of a language mo del in terms of\nLT P /(section /3/./3/)/. F or mo dels with sev eral comp onen ts /(e/.g/. the class based mo dels/)/, w e\ndev elop the metho d of probabilit y decomp osition /(section /3/./4/) whic h allo ws us to analyze\nthe w eaknesses of the comp onen ts separately /. W e conclude this c hapter b y sho wing that\nthe idea of analyzing w eaknesses is applicable to man y probabilistic mo dels /(section /3/./5/)/.\n/3/4\nCHAPTER /3/. ANAL YZING AND IMPR O VING LANGUA GE MODELS /3/5\n/3/./1 In tuitions on Analysing and Impro ving Language Mo d/-\nels\nGenerally sp eaking/, theories ab out the w orld remain v alid as long as they correctly predict\nthe observ ed empirical data/. But when con tradictory evidence is found/, new theories are\nsometimes found naturally b y iden tifying and analyzing the errors of the old theory /. This\ncan b e tak en as a v ery simple/, in tuitiv e mo del of scien ti/\fc progress/. By analogy to this\nmo del of progress/, w e prop ose in this c hapter the cen tral idea of this thesis/, namely trying\nto analyze errors or w eaknesses of a language mo del in order to subsequen tly impro v e the\nmo del/.\nBefore dev eloping this line of though t further/, w e should try to /\fnd out whether some/-\nthing similar has b een tried b efore/. In the pro ceedings of the recen t ma jor conference in\nNorth America /(/[/5/7 /]/) and one of the main Europ ean conferences /(/[/3/5 /]/)/, w e did not /\fnd one\npap er that attempts an error analysis of a language mo del/. In all of the t ypical language\nmo deling literature /(e/.g/. pro ceedings of previous y ears/, w orkshops/, etc/./)/, w e ha v e not come\nacross a pap er that tries to tac kle the problem from this angle/. Therefore/, the curren t\nliterature on language mo deling sho ws an apparen t lac k of in terest in error analysis/. This\nis v ery surprising/, esp ecially when the recen t increase in w ork on the topic is tak en in to\naccoun t/.\nIn order to p erform an error analysis of a language mo del/, w e /\frst ha v e to de/\fne what\nconstitutes an error/. Ho w do w e de/\fne an error of a language mo del/? Rather than trying to\nde/\fne an error at this p oin t/, w e observ e that the de/\fnition of an error should b e related to\nthe p erformance measure used to ev aluate a language mo del/. If they are not related/, w e can\nstill iden tify and remo v e an error/, but b y doing so/, w e ma y not impro v e the p erformance of\nthe mo del b ecause the error is not related to the p erformance measure/. Before de/\fning an\nerror/, w e therefore in tro duce the standard measure used to ev aluate the p erformance of a\nlanguage mo del/.\nCHAPTER /3/. ANAL YZING AND IMPR O VING LANGUA GE MODELS /3/6\n/3/./2 Ev aluating a Language Mo del\n/3/./2/./1 A Simple Mathematical Measure for the Qualit y of a Language\nMo del\nSince the task of the mo del is to predict w ords/, it seems natural to ev aluate a mo del b y\nlo oking at the probabilities it giv es to w ords in a sample of text/. This text is referred to as\na testing text /. The geometric mean of these probabilities /(the /\\a v erage v alue/\"/) therefore\nseems lik e a go o d measure for the abilit y of the mo del to predict w ords/. P erplexit y /(see\n/[/6/2 /]/)/, the standard y ardstic k for comparing p erformances of language mo dels/, is just the\nrecipro cal of the geometric mean/.\nW e no w presen t the measure of p erplexit y in more detail/. As in tro duced in section /2/./2/,\nW /= w /[/1/] /; /:/:/:/; w /[ i /] /; /:/: /:/; w /[ n /] denotes a sequence of w ords/. Here/, the sequence is the sequence\nof w ords in the testing text/. Let c\nk /( i /)\nb e the con text the language mo del c ho oses for the\nprediction of the w ord w /[ i /] /(see section /2/./3/)/. F urthermore/, p /( w /[ i /] /= w\nl /( i /)\nj c\nk /( i /)\n/) denotes the\nprobabilit y assigned to the i\nth\nw ord b y the mo del/. The total probabilit y T P of the sequence\nis\nT P /= p /( W /) /= p /( w /[/1 /: n /]/) /=\ni /= n\nY\ni /=/1\np /( w /[ i /] /= w\nl /( i /)\nj c\nk /( i /)\n/) /: /(/3/./1/)\nThe p erplexit y P P w e just describ ed in tuitiv ely as the geometric mean of the probabilities\nis then\nP P /= /( T P /)\n/BnZr\n/1\nn\n/: /(/3/./2/)\nF or a large sample of text/, the total probabilit y T P can get extremely small/. Therefore/, from\na practical p oin t of view/, it is more con v enien t to use the logarithm of the total probabilit y\nLT P\nLT P /= l og\n/2\n/( T P /) /=\ni /= n\nX\ni /=/1\nl og\n/2\n/( p /( w /[ i /] /= w\nl /( i /)\nj c\nk /( i /)\n/) /(/3/./3/)\nand the logarithm of the p erplexit y LP\n/1\nLP /= l og\n/2\n/( P P /) /= /BnZr\n/1\nn\nl og\n/2\n/( T P /) /= /BnZr\n/1\nn\nLT P /: /(/3/./4/)\n/1\nBy analogy to T P and LT P /, w e w ould prefer to use the term LP P instead of LP /. Ho w ev er/, since LP\nis the term used b y man y other researc hers/, w e will b e using it as w ell/.\nCHAPTER /3/. ANAL YZING AND IMPR O VING LANGUA GE MODELS /3/7\n/3/./2/./2 Information/, En trop y and P erplexit y from an Information Theoretic\nP oin t of View\nIn this section/, w e will deriv e the logarithm of the probabilit y LP and the p erplexit y PP\nfrom an information theoretic p oin t of view as measures for the qualit y of a language mo del/.\nW e will also deriv e the term of en trop y /, whic h w e will use later in section /4/./2/./1/, page /6/4/.\nMost of the material here is tak en from /[/6/2 /, pp/./4/7/2/]/, /[/1/2/2 /, p/./6/,p/./5/4/] and /[/1/3/5 /]/.\nInformation theory is concerned with sources of information/. In simple terms/, a source of\ninformation is a device that outputs sym b ols c hosen from a /\fnite set V /= f x\n/1\n/; /:/:/:/; x\nl\ng kno wn\nto the observ er/. The sym b ols are c hosen according to a statistical la w underlying the device/.\nW e will write the probabilit y of observing sym b ol x\ni\nas p /( x\ni\n/)/. When an information source\noutputs a sym b ol/, it pro vides information b y remo ving the uncertain t y ab out the iden tit y\nof that sym b ol/. Th us/, a source pro vides more information if the uncertain t y ab out the\nnext sym b ol is greater/. Ho w can w e measure the amoun t of uncertain t y w e ha v e ab out the\nnext sym b ol/? If there is suc h a measure/, sa y H /( p /( x\n/1\n/; /:/:/:/; x\nl\n/)/)/, it is reasonable to require the\nfollo wing prop erties/:\n/1/) H /( p /( x\n/1\n/; /:/:/:/; x\nl\n/)/) should b e con tin uous in the p /( x\ni\n/)/.\n/2/) If all the p /( x\ni\n/) are equal /(e/.g/. p /( x\ni\n/) /=\n/1\nl\n/)/, then H /( p /( x\n/1\n/; /:/:/:/; x\nl\n/)/) should b e a monotonic\nincreasing function on l /. In other w ords/, if all sym b ols are equiprobable/, then there is\nmore uncertain t y if there are more sym b ols/.\n/3/) If the c hoice of the next sym b ol is brok en do wn in to t w o successiv e c hoices/, the original\nH /( p /( x\n/1\n/; /:/:/:/; x\nl\n/)/) should b e the w eigh ted sum of the H v alues of eac h c hoice/. The\nmeaning of this is illustrated in /\fgure /3/./1/. In case a/)/, w e ha v e three p ossibilitie s with\nprobabilities p /( x\n/1\n/) /=\n/1\n/2\n/; p /( x\n/2\n/) /=\n/1\n/3\nand p /( x\n/3\n/) /=\n/1\n/6\n/. In case b/)/, w e /\frst c ho ose b et w een\nt w o p ossibilities eac h of whic h has the probabilit y\n/1\n/2\n/, and if w e ha v e pic k ed the second\np ossibilit y /, w e will mak e another c hoice b et w een t w o p ossibilities with probabilities\n/2\n/3\nand\n/1\n/3\n/. W e require that\nH /(\n/1\n/2\n/;\n/1\n/3\n/;\n/1\n/6\n/) /= H /(\n/1\n/2\n/;\n/1\n/2\n/) /+\n/1\n/2\n/\u0003 H /(\n/2\n/3\n/;\n/1\n/3\n/) /: /(/3/./5/)\nIt is sho wn in /[/1/3/5 /, p/./1/1/6/]/, that the only H /( p /( x\n/1\n/; /:/:/:/; x\nl\n/)/) satisfying the three requiremen ts\nis of the form\nH /= /BnZr k\nX\np /( x\ni\n/) /\u0003 l og p /( x\ni\n/) /: /(/3/./6/)\nCHAPTER /3/. ANAL YZING AND IMPR O VING LANGUA GE MODELS /3/8\n/BnZr\n/BnZr\n/BnZr\n/BnZr\n/@\n/@\n/@\n/@\n/@\n/@\n/@\n/@/\u001a\n/\u001a\n/\u001a\n/\u001a\n/\u001a\n/\u001a\n/,\n/,\n/,\n/,\n/,\n/,\n/,\n/,\n/,\n/@\n/@\n/@\n/@\n/@\n/@\na/)\n/1\n/1\n/3\n/6\nb/)\n/1\n/1\n/1\n/2\n/1\n/2\n/1\n/2\n/1\n/2\n/2\n/3\n/1\n/3\n/3\n/6\nFigure /3/./1/: Breaking one c hoice in to t w o successiv e c hoices\nThe constan t k merely determines the c hoice of a unit of measure/. Quan tities of the form\nH /= /BnZr\nX\np /( x\ni\n/) /\u0003 l og p /( x\ni\n/) /(/3/./7/)\npla y a cen tral role in information theory as measures of information/, c hoice and uncertain t y /.\nH is also used as en trop y in statistical mec hanics/, where p /( x\ni\n/) is the probabilit y of a system\nb eing in cell i of its phase space /(see for example /[/1/4/5 /]/)/.\nOne w a y of understanding in tuitiv ely wh y the logarithm is used is to lo ok at the infor/-\nmation pro vided b y a source with l equiprobable sym b ols/. According to equation /3/./7/, the\ninformation con ten t of suc h a source is\nH /( X /) /=\ni /= l\nX\ni /=/1\n/1\nl\nl og l /=\n/1\nl\n/\u0003 l /\u0003 l og l /= l og l /: /(/3/./8/)\nIf the source outputs t w o sym b ols in a ro w/, w e should get t wice as m uc h information/.\nHo w ev er/, outputting t w o sym b ols is equiv alen t to a source outputting one of l\n/2\nsym b ols\nindep enden tly and with equal probabilit y /. The information con ten t of the second source\nshould therefore b e t wice the information con ten t of the /\frst/. Indeed/, b ecause w e use the\nlogarithm/, w e get\nl og l\n/2\n/= /2 /\u0003 l og l /(/3/./9/)\nThe logarithm therefore conforms to our in tuitions ab out the quan tities of information/.\nAnother w a y of understanding equation /3/./7 is to rewrite it as\nH /= /BnZr\nX\np /( x\ni\n/) /\u0003 l og p /( x\ni\n/) /=\nX\np /( x\ni\n/) /\u0003 l og\n/1\np /( x\ni\n/)\n/: /(/3/./1/0/)\nCHAPTER /3/. ANAL YZING AND IMPR O VING LANGUA GE MODELS /3/9\nIf X denotes a random v ariable /(our source/) o v er the set V /= f x\n/1\n/; /:/:/:/; x\nl\ng /, then H is in fact\nthe exp ected v alue of l og\n/1\np /( x\ni\n/)\n/, where\n/1\np /( x\ni\n/)\nis the uncertain t y asso ciated with sym b ol x\ni\n/. If\nx\ni\nis v ery unlik ely /, then\n/1\np /( x\ni\n/)\nis v ery big/, thereb y agreeing with our in tuition that unlik ely\nev en ts carry a great degree of uncertain t y /.\nAs one example of this de/\fnition/, consider the en trop y of a v ariable that can only tak e\none v alue/, of course with probabilit y one/:\nH /( X /) /= /1 /\u0003 l og\n/1\n/1\n/= /0 /: /(/3/./1/1/)\nSince the outcome is absolutely certain/, no information is pro vided b y the source/.\nThe logarithms in equation /3/./7 are usually tak en to the base t w o and in this case/, the\ninformation is measured in units of binary sym b ols /(bits/)/. F or example/, the information\npro vided b y a uniform source with t w o sym b ols is one bit/:\nI /= l og\n/2\n/2 /= /1 /: /(/3/./1/2/)\nThe fundamen tal theory of information theory states /(see /[/1/3/5 /, p/./5/9/, Theorem /9/]/) that\non the a v erage/, it tak es H bits to represen t a sym b ol put out b y a source of en trop y H /.\nF urthermore a source of en trop y H pro vides as m uc h information as a source that c ho oses\nit sym b ols indep enden tly /, with equal probabilit y /, from a v o cabulary size of\nl /= /2\nH\n/: /(/3/./1/3/)\nThis is b ecause/, according to equation /3/./8/, the en trop y H\n/0\nof the latter source is\nH\n/0\n/= l og /2\nH\n/= H /: /(/3/./1/4/)\nWhat ab out information sources that do not c ho ose their sym b ols indep enden tly of\nprevious sym b ols/? Let x /[ i /] denote the i\nth\nsym b ol output b y the source and let x /[ i /: j /] /; i /\u0014 j\nb e a short hand for x /[ i /] /; x /[ i /+ /1/] /; /: /:/:/; x /[ j /] /. F or this more general case/, the en trop y H is de/\fned\nas\nH /= /BnZr l im\nn /!/1\n/1\nn\nX\nx /[/1/: n /] /2 V\nn\np /( x /[/1 /: n /]/) l og p /( x /[/1 /: n /]/) /(/3/./1/5/)\nThere is a groups of sources/, called er go dic sources/, for whic h w e can simplify equation\n/3/./1/5/. Ev en though a rigorous de/\fnition of ergo dicit y\n/2\nis quite complex/, the general idea is\nsimple/.\n/2\nThe in terested reader can /\fnd more on ergo dicit y in /[/1/3/5 /, p/./4/7/] and a more rigorous de/\fnition in /[/4/0/]/.\nCHAPTER /3/. ANAL YZING AND IMPR O VING LANGUA GE MODELS /4/0\n/\\In an ergo dic pro cess ev ery sequence pro duced b y the pro cess is the same\nin statistical prop erties/. Th us the letter frequencies/, digram frequencies/, etc/./,\nobtained from particular sequences/, will/, as the lengths of the sequences increase/,\napproac h de/\fnite limits indep enden t of the particular sequence/. Actually /, this is\nnot true of ev ery sequence but the set for whic h it is false has probabilit y zero/.\nRoughly the ergo dic prop ert y means statistical homogeneit y/\" /(/[/1/3/5 /, p/./4/5/,/4/6/]/)\nIn the case of ergo dic sources/, equation /3/./1/5 reduces to\nH /= /BnZr l im\nn /!/1\n/1\nn\nl og p /( x /[/1 /: n /]/) /: /(/3/./1/6/)\nIn other w ords/, w e can estimate the en trop y H from long sequences of sym b ols that w ere\ngenerated b y the source/.\nHo w can w e apply information theory to language mo dels/? Language can b e seen as an\ninformation source whose output sym b ols are w ords from the v o cabulary V /= f w\n/1\n/; /:/:/:/; w\nm\ng\n/(see section /2/./2/)/. W e can use form ula /3/./1/6 to estimate the con ten t of information p er w ord\nin a large corpus of text/:\nH /=\n/1\nn\nl og p /( x /[/1 /: n /]/) /: /(/3/./1/7/)\nBut ho w can w e get the probabilities of sequences of w ords w /[/1 /: n /] of the language/, that w e\nneed for equation /3/./1/7/? W e can appro ximate them with the probabilities /^p /( w /[/1 /: n /]/) giv en\nb y the language mo del/. If w e replace the true probabilities p /( w /[/1 /: n /]/) of equation /3/./1/7/, with\ntheir appro ximations /^p /( w /[/1 /: n /]/) giv en b y the language mo del/, w e obtain the logarithm of\nthe probabilit y /(logprob/) LP that w e sa w in section /3/./2/:\nLP /= /BnZr\n/1\nn\nl og /^p /( w /[/1 /: n /]/) /: /(/3/./1/8/)\nIn tuitiv ely /, LP is a measure for the en trop y of our mo del for the language/. As p oin ted out\nin /[/6/2 /, p/./4/7/4/]/, w e can sho w that LP /\u0015 H if w e assume prop er ergo dic b eha vior of the source\ngenerating the text/. This is clear in tuitiv ely /, b ecause our mo del of the language can at most\nb e as go o d as the language itself/. F rom the view of the sp eec h recognizer/, LP measures the\ndi/\u000ecult y in recognizing sp eec h that w as generated b y the same source that generated the\ncorpus/. Th us/, LP is a v ery appropriate measure for the qualit y of a language mo del/.\nSimilar to equation /3/./1/5/, w e can sa y that the di/\u000ecult y of a sp eec h recognition task is\nalso giv en b y the p erplexit y P P /(see section /3/./2/)/:\nP P /= /2\nLP\n/= /^p /( w /[/1 /: n /]/) /: /(/3/./1/9/)\nCHAPTER /3/. ANAL YZING AND IMPR O VING LANGUA GE MODELS /4/1\nTh us/, the sp eec h recognition task with a language mo del of logprob LP can b e though t of\nas b eing as di/\u000ecult as the recognition of a language with P P equally lik ely w ords/.\n/3/./2/./3 Discussion of the Standard P erplexit y Measure\nThe ultimate measure for the p erformance of a sp eec h recognition system is its recognition\naccuracy /. Wh y then do w e w an t to measure the qualit y of a language mo del separately/?\nFirst/, b ecause it allo ws us to measure the qualit y of one comp onen t of the sp eec h recognizer/,\nthe language mo del/, indep enden tly of the c haracteristics of the other comp onen ts of the\nparticular sp eec h recognition system at hand/. Not only do es this mak e language mo dels of\ndi/\u000beren t sp eec h recognizers directly comparable/, but it also allo ws researc hers to w ork on\nthe t w o subtasks separately /, th us follo wing the w ell kno wn /\\divide and conquer/\" approac h/.\nSecond/, w e can measure the qualit y of language mo dels that are built for di/\u000beren t tasks/,\ne/.g/. w ord disam biguation and sp elling correction or text enco ding/.\nWhat do w e exp ect of a measure of the qualit y of a language mo del/, in particular with\nresp ect to sp eec h recognition/? Supp ose w e ha v e t w o language mo dels/, LM/1 and LM/2/, and\n/{ according to our p erformance measure /{ LM/1 is b etter than LM/2/. W e exp ect that in\ngeneral/, the recognition accuracy of a sp eec h recognizer that uses LM/1 will decrease if it\nuses LM/2 instead/. In other w ords/, the measure of the qualit y of the language mo del should\nb e highly correlated with the accuracy of an y sp eec h recognition system/.\nThe p erplexit y measure from the previous sections has b een sho wn to correlate w ell\nwith the recognition accuracy man y times/. Moreo v er/, it is a theoretically sound measure\nfor the amoun t of c hoice in a text generated b y the language mo del/. It is therefore a v ery\nappropriate measure to use for the ev aluating of language mo dels/. Ho w ev er/, it also has the\nfollo wing problems/:\n/1/) The p erplexit y measure do es not tak e the acoustic similarit y of the w ords in to ac/-\ncoun t/. Th us/, there is no p erfe ct correlation b et w een p erplexit y and recognition accu/-\nracy /. There ha v e b een cases rep orted in the literature /(see for example /[/2/1 /]/)/, where\na language mo del LM/1 with a higher p erplexit y than a mo del LM/2 leads to b etter\nrecognition accuracy /.\n/2/) The qualit y of the language mo del dep ends on the testing text/. If w e c ho ose a testing\ntext that is v ery di/\u000beren t from the text used to train the mo del/, the mo del will p erform\nv ery p o orly /. Ho w ev er/, this do es not really mean that the mo del is bad/, but that the\nCHAPTER /3/. ANAL YZING AND IMPR O VING LANGUA GE MODELS /4/2\ntesting text is v ery di/\u000beren t from the training text/. In fact/, the language mo del ma y\nha v e learned the statistical prop erties of the training text v ery w ell/.\n/3/) The language mo del will ultimately b e used to discriminate b et w een lik ely and unlik ely\nw ords/. It seems that for this purp ose/, the di/\u000ber enc e in probabilit y b et w een lik ely and\nunlik ely w ords is more imp ortan t than the absolute value of the probabilities/. In\ngeneral/, negativ e information is useful in language learning tasks /(see for example /[/4/2 /]\nand /[/4/3 /]/)/. It therefore seems appropriate for the language mo del to mak e use of a\n/\\false/\" text /(for example a sequence of w ords c hosen at random from a v o cabulary or\na p erm utation of an existing text/)/. W e could then for example measure the di/\u000berence\nin p erplexit y on the real and the /\\false/\" text/.\nEv en though there are problems with the p erplexit y measure/, w e c ho ose to use it for our\nw ork for the follo wing reasons/. First/, problem /1/) is v ery rare and in the large ma jorit y of\ncases/, the p erplexit y and the recognition accuracy ar e highly correlated/. Second/, problem\n/2/) is less sev ere if w e c ho ose a testing text that is quite similar to the training text /(and to\nthe task the language mo del is used for in the end/)/. This is common practice/. Third/, the\namoun t of w ork required to in v estigate problem /3/) is b ey ond the scop e of our w ork here/.\nLast/, but not least/, p erplexit y is still the only widely accepted measure for the qualit y of a\nlanguage mo del/.\n/3/./3 De/\fning and Iden tifying W eaknesses of Language Mo d/-\nels\nNo w that w e ha v e seen the standard measure used to ev aluate language mo dels/, w e can pro/-\nceed with our endea v our of de/\fning errors of language mo dels/. Since the language mo del\nconstructs a probabilit y distribution/, w e do not think that the w ord err or is appropriate/.\nInstead/, w e prefer to use the term w eakness/, b ecause the in tuitiv e notion of the term we ak/-\nness as something that should b e impro v ed is what w e w an t to express/. Ho w ev er/, in order\nto a v oid a future misunderstanding/, let us p oin t out that w e use the term w eakness in a\nsp ecial/, tec hnical sense for the follo wing reason/. The commonly used notion of w eakness\nimplies that the w eak part c an b e impro v ed/. In our usage/, this migh t not alw a ys b e p ossible/.\nEv en an in tuitiv ely /\\p erfect/\" language mo del will not b e able to predict the correct next\nCHAPTER /3/. ANAL YZING AND IMPR O VING LANGUA GE MODELS /4/3\nw ord with a probabilit y of one\n/3\n/. Since our de/\fnition of a w eakness will b e related to the\ninformation theoretic term of p erplexit y /, only a mo del that can predict the next w ord with\nabsolute certain t y /(e/.g/. with a probabilit y of /1/) w ould b e p erfect/. Th us/, an y mo del that do es\nnot ac hiev e this will still ha v e w eaknesses in the information theoretic sense/. So ev en the\nin tuitiv ely /\\p erfect/\" language mo del will still ha v e a w eakness according to our de/\fnition/.\nEv en though this migh t b e a theoretical dra wbac k of linking the de/\fnition of a w eakness to\ninformation theory /, the disadv an tage only app ears if the language mo del approac hes the in/-\ntuitiv ely /\\p erfect/\" mo del/. Ho w ev er/, curren t N /-gram mo dels ha v e arguably not y et reac hed\nthis state /(see /[/6/3 /] for a brief comparison b et w een the p erformance of a tri/-gram mo del and\na h uman guess/)/.\nW e can no w describ e a w eakness of a language mo del in terms of the logarithm of the\ntotal probabilit y LT P /, a term v ery closely related to the standard p erformance measure for\nlanguage mo dels /(see section /3/./2/)/. In tuitiv ely /, a w eakness of a language mo del is an y part\nof the mo del that causes a large fraction of the LT P /. In the follo wing/, w e will formalize\nthis in tuitiv e description/.\nLet us b egin b y again p oin ting out that the p erformance of a language mo del dep ends\non the testing text used to ev aluate the mo del/. This is one of the dra wbac ks of the standard\np erformance measure and it w as already men tioned in section /3/./2/./3/. As w e sa w in the\nb eginning of this c hapter/, our measure of w eakness should b e related to the p erformance\nmeasure/. It is th us v ery clear that our w eakness measure will also dep end on the testing\ntext/. In other w ords/, when w e sp eak of a w eakness of a language mo del/, this will alw a ys b e\nrelativ e to a giv en testing text/. As p oin ted out in section /3/./2/./3/, the idea is of course that\nthe testing text should b e v ery similar to the actual data the language mo del will ultimately\nb e used for/. If this is the case/, the ab o v e men tioned dra wbac k/, b oth for the p erformance\nmeasure and for our w eakness measure/, will b e less sev ere/.\nA language mo del is ev aluated on a testing text W /= w /[/1/] /:/:/:w /[ i /] /:/: /:w /[ n /] /(see section\n/3/./2/) and the probabilit y the language mo del assigns to w ord w /[ i /] is denoted b y p /( w /[ i /] /=\nw\nl /( i /)\nj c\nk /( i /)\n/)/. F urthermore/, the LT P of W is calculated as /(see equation /3/./3/)\nLT P /= l og\n/2\n/( T P /) /=\ni /= n\nX\ni /=/1\nl og\n/2\n/( p /( w /[ i /] /= w\nl /( i /)\nj c\nk /( i /)\n/)/) /: /(/3/./2/0/)\nIn the follo wing/, w e will denote the testing text W b y its index set I\nW\n/= f /1 /; /:/:/:/; n g /. This\n/3\nUnless w e assume an all/-kno wing oracle and den y the existence of a free will of the sp eak er/.\nCHAPTER /3/. ANAL YZING AND IMPR O VING LANGUA GE MODELS /4/4\nw a y /, w e can denote an y subset W /1 of w ords of W b y giving the subset of indices I\nW /1\n/\u0012 I\nW\n/.\nF or a giv en subset W /1/, w e can easily determine the LT P it causes /( LT P\nW /1\n/) b y summing\nup the logarithm of the probabilities of all the w ords in W /1/:\nLT P\nW /1\n/=\nX\ni /2 W /1\nl og\n/2\n/( p /( w /[ i /] /= w\nl /( i /)\nj c\nk /( i /)\n/)/) /: /(/3/./2/1/)\nGiv en LT P\nW /1\n/, w e can then calculate the fraction of LT P caused b y W /1 /( f\nW /1\n/) as\nf\nW /1\n/=\nLT P\nW /1\nLT P\n/: /(/3/./2/2/)\nDe/\fnition /2 The imp act of a subset W /1 /\u0012 W is the fr action f\nW /1\nof LT P the subset W /1\nc auses/.\nThe in tuitiv e idea b ehind this de/\fnition is that w e need to impro v e the language mo del/'s\nprediction of the w ords that ha v e a big impact/, if w e w an t to impro v e the o v erall p er/-\nformance signi/\fcan tly /. Similarly /, if a subset has a lo w impact/, impro ving the language\nmo del/'s prediction of the w ords in this subset will not lead to a signi/\fcan t impro v emen t in\np erformance/.\nGiv en the impact of a subset W /1 /\u0012 W /, w e will no w deriv e the part of the language mo del\nused in calculating the probabilit y of W /1 and the impact of this part/. A language mo del\ncon tains man y probabilit y distributions and eac h probabilit y distribution con tains man y\nprobabilities/. W e therefore sa y that a language mo del is made up of a set of probabilities\nS /= f p\n/1\n/; /:/:/:/; p\nl\ng /. F urthermore/, w e will call an y subset S /1 /\u0012 S a part of the mo del/. In order\nto calculate the probabilities of a subset W /1 of w ords /(e/.g/. p /( w /[ i /] /= w\nl /( i /)\nj c\nk /( i /)\n/) /; i /2 I\nW /1\n/)/,\nthe language mo del will use a subset S\nW /1\n/\u0012 S of its probabilities/. Giv en a subset W /1/, w e\ncan then de/\fne the part S\nW /1\nof the mo del as the subset of probabilities used to calculate\nthe probabilities of w ords in W /1/. The impact of a part S\nW /1\n/\u0012 S of a language mo del S\nis then giv en b y f\nW /1\n/, the fraction of LT P that W /1 causes/. Finally /, w e can no w de/\fne a\nw eakness of a language mo del/.\nDe/\fnition /3 A p art S\nW /1\nof a language mo del S /, de/\fne d by a subset W /1 of the testing text\nW /, is c al le d a we akness/, if S\nW /1\nhas a gr e at imp act/.\nThe in tuitiv e idea b ehind this de/\fnition is as follo ws/. If subset W /1 causes a large fraction\nof LT P /, then impro ving it is v ery imp ortan t/. This conforms to our in tuitiv e meaning of a\nw eakness as something that should b e impro v ed/.\nCHAPTER /3/. ANAL YZING AND IMPR O VING LANGUA GE MODELS /4/5\nIn the next c hapter/, w e will see what useful results w e obtain with this de/\fnition/. F or\nno w/, w e demonstrate the usefulness of this de/\fnition through an example/. W e de/\fne a\nsimple language/, a simple mo del of the language/, ev aluate the mo del on a simple testing\ntext and p erform the analysis of its w eaknesses/. W e can then v erify that/, in this case/, the\nw eaknesses iden ti/\fed b y our de/\fnition corresp ond to what w e w ould lik e to b e iden ti/\fed\nin tuitiv ely as w eaknesses/.\nConsider the language consisting of sequences of the four sym b ols f a/; b/; c/; d g /. Supp ose\nthat w e kno w nothing ab out this language in general and the frequencies of eac h sym b ol in\nparticular/. W e therefore c ho ose the most simplistic mo del in tro duced in section /2/./5/./1 as a\nmo del of the language/. This mo del treats all sym b ols as equiprobable and therefore assigns\nthe probabilit y\n/1\n/4\nto eac h of the sym b ols/, indep enden t of con text/. As testing text/, w e use\nthe string /\\b ddad/\"/. When w e ev aluate this mo del /(see section /3/./2/)/, w e obtain for its total\nprobabilit y /( T P /)/, p erplexit y /( P P /)/, logarithm of total probabilit y /( LT P /) and logarithm of\np erplexit y /( LP /)/:\nT P /= p /( W /) /= p /( w /[/1 /: /5/]/) /=\ni /=/5\nY\ni /=/1\np /( w /[ i /]/) /(/3/./2/3/)\n/= p /( b /) /\u0003 p /( d /) /\u0003 p /( d /) /\u0003 p /( a /) /\u0003 p /( d /) /=\n/1\n/4\n/\u0003\n/1\n/4\n/\u0003\n/1\n/4\n/\u0003\n/1\n/4\n/\u0003\n/1\n/4\n/(/3/./2/4/)\n/= /(\n/1\n/4\n/)\n/5\n/\u0019 /0 /: /0/0/0/9/8 /(/3/./2/5/)\nP P /= /( T P /)\n/BnZr\n/1\n/5\n/= /4 /(/3/./2/6/)\nLT P /= l og\n/2\n/( T P /) /= l og\n/2\n/(\n/1\n/4\n/) /+ l og\n/2\n/(\n/1\n/4\n/) /+ l og\n/2\n/(\n/1\n/4\n/) /+ l og\n/2\n/(\n/1\n/4\n/) /+ l og\n/2\n/(\n/1\n/4\n/) /(/3/./2/7/)\n/= /5 /\u0003 /( /BnZr /2/) /= /BnZr /1/0 /(/3/./2/8/)\nLP /= l og\n/2\n/( P P /) /= /BnZr\n/1\n/5\nLT P /= /2 /(/3/./2/9/)\n/(/3/./3/0/)\nAccording to the preceding discussion/, w e can no w try to iden tify the w eaknesses of the\nmo del/. Since there are four basic ev en ts distinguished b y the mo del/, it seems natural to\nlo ok at the imp ortance of the subsets A/; B /; C and D of W that are de/\fned b y all o ccurrences\nof eac h sym b ol in the testing text/. F or the subset A/, w e th us obtain the index set I\nA\n/= f /4 g /,\nthe set con taining the index of the only o ccurrence of the sym b ol /\\a/\"/. Similarly /, w e obtain\nI\nB\n/= f /1 g /; I\nC\n/= fg /; I\nD\n/= f /2 /; /3 /; /5 g /. The LT P caused b y subset A is simply the sum of the\nCHAPTER /3/. ANAL YZING AND IMPR O VING LANGUA GE MODELS /4/6\nlogarithms of the probabilities of all o ccurrences of /\\a/\"/:\nLT P\nA\n/=\nX\ni /2 I\nA\nl og\n/2\n/( p /( w /[ i /]/)/) /= l og\n/2\n/( p /( w /[/4/]/)/) /= l og\n/2\n/( p /( a /)/) /= l og\n/2\n/(\n/1\n/4\n/) /= /BnZr /2 /(/3/./3/1/)\nSimilarly /, w e obtain LT P\nB\n/; LT P\nC\nand LT P\nD\n/:\nLT P\nB\n/=\nX\ni /2 I\nB\nl og\n/2\n/( p /( w /[ i /]/)\n/= l og\n/2\n/( p /( w /[/1/]/)/) /= l og\n/2\n/( p /( b /)/) /= l og\n/2\n/(\n/1\n/4\n/) /= /BnZr /2\nLT P\nC\n/=\nX\ni /2 I\nC\nl og\n/2\n/( p /( w /[ i /]/)/) /= /0\nLT P\nD\n/=\nX\ni /2 I\nD\nl og\n/2\n/( p /( w /[ i /]/)/) /= l og\n/2\n/( p /( w /[/2/]/)/) /+ l og\n/2\n/( p /( w /[/3/]/)/) /+ l og\n/2\n/( p /( w /[/5/]/)/)\n/= /3 /\u0003 l og\n/2\n/( p /( b /)/) /= /3 /\u0003 l og\n/2\n/(\n/1\n/4\n/) /= /BnZr /6 /:\nW e then divide the LT P caused b y eac h subset b y the o v erall LT P /= /BnZr /1/0 to obtain the\nfraction of LT P caused b y eac h subset/:\nf\nA\n/=\nLT P\nA\nLT P\n/=\n/BnZr /2\n/BnZr /1/0\n/= /0 /: /2\nf\nB\n/=\nLT P\nB\nLT P\n/=\n/BnZr /2\n/BnZr /1/0\n/= /0 /: /2\nf\nC\n/=\nLT P\nC\nLT P\n/=\n/0\n/BnZr /1/0\n/= /0\nf\nD\n/=\nLT P\nD\nLT P\n/=\n/BnZr /6\n/BnZr /1/0\n/= /0 /: /6\nBecause all o ccurrences of the sym b ol /\\d/\" cause /6/0/% of the total LT P /(e/.g/. f\nD\n/= /0 /: /6/0/)/, w e\ncan see that the prediction of the sym b ol /\\d/\" is the most imp ortan t one for the p erformance\nof our mo del on this testing text/. But what do es this result imply for the mo del/? W e will\nno w iden tify the part of the mo del that is used in calculating the probabilities of the subset\nD /. In this example/, this will b e v ery straigh t forw ard/, but as w e will see in section /3/./4/, this\nis not alw a ys the case/.\nOur mo del has only one probabilit y distribution con taining four probabilities/. W e th us\nwrite our mo del S as S /= f p\na\n/; p\nb\n/; p\nc\n/; p\nd\ng /. The only probabilit y used in calculating the\nprobabilities of w ords in D is p\nd\n/. W e th us obtain the part of the mo del S\nD\n/= f p\nd\ng /.\nCHAPTER /3/. ANAL YZING AND IMPR O VING LANGUA GE MODELS /4/7\nThis part causes /6/0/% of the total LT P /(e/.g/. f\nD\n/= /0 /: /6/) and is therefore iden ti/\fed as a\nw eakness of the mo del/. In tuitiv ely /, if w e ha v e the c hance to increase one of the probabilities\nin S /= f p\na\n/; p\nb\n/; p\nc\n/; p\nd\ng /(and decrease others in return/)/, w e w ould c ho ose p\nd\n/, if w e w an t\nto signi/\fcan tly impro v e the o v erall p erformance of the mo del/. This is also the part that\nis iden ti/\fed as a w eakness using our de/\fnition and the results of our analysis therefore\ncorresp ond w ell to our in tuition/.\n/3/./4 Probabilit y Decomp osi ti on\nIn the last section/, w e de/\fned a w eakness of a language mo del as a part of the mo del that\nhas a great impact on the o v erall p erformance/. A part of the mo del w as in turn de/\fned\nas all the probabilities of the language mo del that are used in calculating the probabilities\nof a subset of w ords from the testing text/. In some language mo dels /(e/.g/. the N /-gram\nmo dels/)/, the probabilit y p /( w /[ i /] /= w\nl\nj c\nk /( i /)\n/) of the i\nth\nw ord is just the probabilit y v alue of the\nprobabilit y distribution for con text c\nk /( i /)\n/. In suc h mo dels/, w e can th us measure the impact\nof a single probabilit y v alue of the mo del and/, b y considering an y set of these probabilit y\nv alues/, the impact of an y subset of the mo del/. This allo ws a v ery /\fne/-grained analysis of\nthe language mo del/.\nHo w ev er/, there are language mo dels in whic h eac h probabilit y p /( w /[ i /] /= w\nl\nj c\nk /( i /)\n/) is cal/-\nculated from sev eral probabilit y v alues of the language mo del/. An example of suc h a mo del\nis the last class based mo del w e sa w in section /2/./5/./3/. In this mo del/, the probabilit y of seeing\nthe w ord /`ligh t/' is the probabilit y of seeing it as a noun plus the probabilit y of seeing it as\na v erb plus the probabilit y of seeing it as an adjectiv e/. The exact form ula of the mo del is\np /( w /[ i /] /= w\nl\nj w /[ i /BnZr /1/]/) /=\nX\ng\nj\n/2 G\np /( g /( w /[ i /]/) /= g\nj\nj g /( w /[ i /BnZr N /+ /1 /: i /BnZr /1/]/)/) /\u0003 p /( w /[ i /] /= w\nl\nj g /( w /[ i /]/) /= g\nj\n/) /:\n/(/3/./3/2/)\nW e can see that the probabilit y of the i\nth\nw ord is calculated as a sum of terms/, where\neac h term is the pro duct of t w o probabilities/. The subset of the mo del/, de/\fned b y just one\nw ord w /[ i /] therefore con tains man y probabilit y v alues /(as opp osed to just one in the N /-gram\nmo del/, for example/)/. The parts of the mo del iden ti/\fed as w eaknesses th us tend to b e large/.\nBut if w e w an t to impro v e suc h a large part of the mo del/, whic h of its probabilit y v alues are\nreally imp ortan t/? Is it for example the probabilit y v alues that predict the next class /(e/.g/.\np /( g /( w /[ i /]/) /= g\nj\nj g /( w /[ i /BnZr /1/]/)/)/) or is it the probabilit y v alues that predict the w ord giv en the\nCHAPTER /3/. ANAL YZING AND IMPR O VING LANGUA GE MODELS /4/8\nclass /(e/.g/. p /( w /[ i /] /= w\nl\nj g /( w /[ i /]/) /= g\nj\n/)/)/? In order to answ er this question/, w e will no w dev elop\na metho d called probabilit y decomp osition/. It will allo w us to divide the probabilit y of the\ni\nth\nw ord in to t w o parts/, the part used to predict the next class and the part used to predict\nthe next w ord giv en its class/.\nT o start with/, w e are giv en the sum of the form S /=\nP\ni\na\ni\n/\u0003 b\ni\nrepresen ting the bi/-p os\nform ula /(equation /3/./3/2/)/, where S corresp onds to p /( w /[ i /] /= w\nl\nj w /[ i /BnZr /1/]/)/, a\ni\nto p /( g /( w /[ i /]/) /=\ng\nj\nj g /( w /[ i /BnZr /1/]/)/) and b\ni\nto p /( w /[ i /] /= w\nl\nj g /( w /[ i /]/) /= g\nj\n/)/. W e therefore ha v e /0 /< a\ni\n/; b\ni\n/\u0014 /1/. In\nour o v erall analysis of the language mo del/, S will cause a certain p ercen tage f\nS\nof the total\nLT P /. Our goal is to split f\nS\nup in to t w o parts f\nA\nand f\nB\n/, the fractions of LT P caused\nb y the a\ni\n/'s and b\ni\n/'s resp ectiv ely /. This will allo w us to concen trate our e/\u000borts to impro v e\nthe mo del on the a\ni\n/'s or b\ni\n/'s/, dep ending on whic h one has a bigger impact on the o v erall\np erformance/. In order to split f\nS\nin to f\nA\nand f\nB\n/, w e need to kno w the p ercen tage of S\nthat is giv en b y the a\ni\n/'s and b\ni\n/'s/. Ho w can w e calculate that p ercen tage/? W e start with\nthe most simple case and supp ose that the sum is only o v er one term/, e/.g/. S /= a\n/1\n/\u0003 b\n/1\nwith\na\n/1\n/=\n/1\n/2\n/; b\n/1\n/=\n/1\n/4\n/; S /=\n/1\n/2\n/\u0003\n/1\n/4\n/=\n/1\n/8\n/. What p ercen tage p\nA\nof S is giv en b y a\n/1\n/? T o answ er this\nquestion/, w e /\frst need to tak e a closer lo ok at our in tuitiv e notion of p ercen tage/. Supp ose\nw e ha v e a sum S /= a\n/1\n/+ /:/:/: /+ a\nn\n/. When w e sa y a\n/1\nis y/*/1/0/0/% of S/, e/.g/.\na\n/1\nS\n/= y /, y is a measure\nof ho w man y a\n/1\n/'s mak e up the total S with resp ect to the op erator /`/+/'/. In fact\na\n/1\n/+ /:/:/: /+ a\n/1\n/(\n/1\ny\ntimes /) /= a\n/1\n/\u0003\n/1\ny\n/= a\n/1\n/\u0003\nS\na\n/1\n/= S/: /(/3/./3/3/)\nThe same should hold for a pro duct/. Supp ose w e ha v e S /= a\n/1\n/\u0003 /:/:/: /\u0003 a\nn\n/. If w e sa y a\n/1\nis\ny/*/1/0/0/% of P /, w e mean\na\n/1\n/\u0003 /:/:/: /\u0003 a\n/1\n/(\n/1\ny\ntimes /) /= a\n/1\ny\n/1\n/= P /: /(/3/./3/4/)\nBy solving\n/4\nthe last equation for y /, w e get the p ercen tage of P giv en b y a\n/1\n/:\nl og\n/2\n/( a\n/1\ny\n/1\n/) /= l og\n/2\n/( P /) /(/3/./3/5/)\n/1\ny\n/\u0003 l og\n/2\n/( a\n/1\n/) /= l og\n/2\n/( P /) /(/3/./3/6/)\ny /=\nl og\n/2\n/( a\n/1\n/)\nl og\n/2\n/( P /)\n/: /(/3/./3/7/)\n/4\nW e used the logarithm to the base t w o to solv e the equation/. But b ecause\nlog\na\n/( x /)\nlog\na\n/( y /)\n/=\nlog\nb\n/( x /)\nlog\nb\n/( y /)\n/, the notion\nof p ercen tage do es not dep end on the base used/.\nCHAPTER /3/. ANAL YZING AND IMPR O VING LANGUA GE MODELS /4/9\n/0\n/0/./2\n/0/./4\n/0/./6\n/0/./8\n/1\n/0/./2 /0/./3 /0/./4 /0/./5 /0/./6 /0/./7 /0/./8 /0/./9 /1\np\nA\na\n/1\np\nA\nFigure /3/./2/: The graph of p\nA\nTh us/, in our example/, the p ercen tage p\nA\nof S /=\n/1\n/8\ngiv en b y a\n/1\n/=\n/1\n/2\nis\np\nA\n/=\nl og\n/2\n/(\n/1\n/2\n/)\nl og\n/2\n/(\n/1\n/8\n/)\n/=\n/1\n/3\n/: /(/3/./3/8/)\nT o get a b etter in tuitiv e understanding of p\nA\n/, w e plot in /\fgure /3/./2 the relationship b et w een\np\nA\nand a\n/1\nfor S /=\n/1\n/8\n/;\n/1\n/8\n/< a\n/1\n/\u0014 /1/. W e limit a\n/1\nto the range\n/1\n/8\n/< a\n/1\n/\u0014 /1 b ecause only in this\nrange w e can /\fnd a b\n/1\nin the range /0 /< b\n/1\n/\u0014 /1 suc h that S /= a\n/1\n/\u0003 b\n/1\n/. Because the curv e is\nfalling/, w e can see that a smaller a\n/1\ncorresp onds to a higher p ercen tage/. Th us if w e ha v e\nS /=\n/1\n/8\n/= a\n/1\n/\u0003 b\n/1\nwith a\n/1\n/=\n/1\n/4\n/; b\n/1\n/=\n/1\n/2\nand a\n/0\n/1\n/=\n/1\n/8\n/; b\n/0\n/1\n/= /1/, then a\n/0\n/1\ncauses a higher p ercen tage\nof S than a\n/1\n/.\nHo w can w e calculate the p ercen tage p\nA\nof S giv en b y a\ni\nif the sum is o v er sev eral\nterms/? As an example/, consider S /= a\n/1\n/\u0003 b\n/1\n/+ a\n/2\n/\u0003 b\n/2\nwith a\n/1\n/=\n/1\n/2\n/; b\n/1\n/=\n/1\n/4\n/; a\n/2\n/=\n/1\n/2\n/; b\n/2\n/=\n/1\n/2\n/; S /=\n/1\n/2\n/\u0003\n/1\n/4\n/+\n/1\n/2\n/\u0003\n/1\n/2\n/=\n/3\n/8\n/: W e can /\frst determine the p ercen tage p\nA /1\nof a\n/1\n/\u0003 b\n/1\ngiv en b y a\n/1\n/(as\nab o v e/) and the p ercen tage p\nA /2\nof a\n/2\n/\u0003 b\n/2\ngiv en b y a\n/2\n/. Giv en p\nA /1\nand p\nA /2\n/, w e could obtain\nthe o v erall p ercen tage p\nA\nb y simply taking the a v erage/, e/.g/.\np\nA\n/=\n/1\n/2\n/\u0003 /( p\nA /1\n/+ p\nA /2\n/) /: /(/3/./3/9/)\nCHAPTER /3/. ANAL YZING AND IMPR O VING LANGUA GE MODELS /5/0\nHo w ev er/, since a\n/1\n/\u0003 b\n/1\nand a\n/2\n/\u0003 b\n/2\nmak e up di/\u000beren t p ortions of the total sum S /, it is more\nfair to w eigh p\nA /1\nand p\nA /2\naccording to the p ortion of S represen ted b y a\n/1\n/\u0003 b\n/1\nand a\n/2\n/\u0003 b\n/2\nresp ectiv ely /. This giv es the w eigh ted a v erage\np\nA\n/=\na\n/1\nb\n/1\nS\n/\u0003 p\nA /1\n/+\na\n/2\nb\n/2\nS\n/\u0003 p\nA /2\n/: /(/3/./4/0/)\nSimilarly /, w e can calculate the p ercen tage p\nB\nof S giv en b y b\ni\nas\np\nB\n/=\na\n/1\nb\n/1\nS\n/\u0003 p\nB /1\n/+\na\n/2\nb\n/2\nS\n/\u0003 p\nB /2\n/: /(/3/./4/1/)\nIn our example/, this giv es\np\nA\n/=\na\n/1\n/\u0003 b\n/1\nS\n/\u0003\nl og\n/2\n/( a\n/1\n/)\nl og\n/2\n/( a\n/1\nb\n/1\n/)\n/+\na\n/2\n/\u0003 b\n/2\nS\n/\u0003\nl og\n/2\n/( a\n/2\n/)\nl og\n/2\n/( a\n/2\nb\n/2\n/)\n/(/3/./4/2/)\n/=\n/1\n/8\n/3\n/8\n/\u0003\nl og\n/2\n/(\n/1\n/2\n/)\nl og\n/2\n/(\n/1\n/8\n/)\n/+\n/1\n/4\n/3\n/8\n/\u0003\nl og\n/2\n/(\n/1\n/2\n/)\nl og\n/2\n/(\n/1\n/4\n/)\n/(/3/./4/3/)\n/=\n/1\n/3\n/\u0003\n/1\n/3\n/+\n/2\n/3\n/\u0003\n/1\n/2\n/(/3/./4/4/)\n/=\n/4\n/9\n/(/3/./4/5/)\np\nB\n/=\na\n/1\n/\u0003 b\n/1\nS\n/\u0003\nl og\n/2\n/( b\n/1\n/)\nl og\n/2\n/( a\n/1\nb\n/1\n/)\n/+\na\n/2\n/\u0003 b\n/2\nS\n/\u0003\nl og\n/2\n/( b\n/2\n/)\nl og\n/2\n/( a\n/2\nb\n/2\n/)\n/(/3/./4/6/)\n/=\n/1\n/8\n/3\n/8\n/\u0003\nl og\n/2\n/(\n/1\n/4\n/)\nl og\n/2\n/(\n/1\n/8\n/)\n/+\n/1\n/4\n/3\n/8\n/\u0003\nl og\n/2\n/(\n/1\n/2\n/)\nl og\n/2\n/(\n/1\n/4\n/)\n/(/3/./4/7/)\n/=\n/1\n/3\n/\u0003\n/2\n/3\n/+\n/2\n/3\n/\u0003\n/1\n/2\n/(/3/./4/8/)\n/=\n/5\n/9\n/(/3/./4/9/)\n/(/3/./5/0/)\nKno wing that p\nA\nand p\nB\nare the p ercen tages of S giv en b y a\ni\nand b\ni\n/, w e can then simply\nwrite S /= A /\u0003 B with\nA /= S\np\nA\n/; B /= S\np\nB\n/: /(/3/./5/1/)\nIn our example/, w e th us get\nA /= S\np\nA\n/= /(\n/3\n/8\n/)\n/4\n/9\n/\u0019 /0 /: /6/4/6/6 /(/3/./5/2/)\nB /= S\np\nB\n/= /(\n/3\n/8\n/)\n/5\n/9\n/\u0019 /0 /: /5/7/9/9 /(/3/./5/3/)\nA /\u0003 B /= /(\n/3\n/8\n/)\n/4\n/9\n/\u0003 /(\n/3\n/8\n/)\n/5\n/9\n/=\n/3\n/8\n/: /(/3/./5/4/)\n/(/3/./5/5/)\nCHAPTER /3/. ANAL YZING AND IMPR O VING LANGUA GE MODELS /5/1\nW e ha v e no w decomp osed the v alue of S /=\n/3\n/8\nin to S /= A /\u0003 B with A /\u0019 /0 /: /6/4/6/6 and\nB /\u0019 /0 /: /5/7/9/9/. What is so sp ecial ab out these v alues/, among all the p ossible v alues for A and\nB /? They are sp ecial b ecause the p ercen tage of S giv en b y A is\n/4\n/9\n/, e/.g/. p\nA\n/=\nlog\n/2\n/( A /)\nlog\n/2\n/( B /)\n/=\n/4\n/9\n/,\nthe w eigh ted a v erage of the p ercen tages of a\ni\n/\u0003 b\ni\ncaused b y a\ni\n/.\nW e will no w extend this metho d to the general case/. Giv en S /=\nP\na\ni\n/\u0003 b\ni\n/, w e will\ndecomp ose S as follo ws/:\nS /= A /\u0003 B /(/3/./5/6/)\nA /= S\np\nA\n/(/3/./5/7/)\nB /= S\np\nB\n/(/3/./5/8/)\np\nA\n/=\nX\ni\na\ni\n/\u0003 b\ni\nS\n/\u0003\nl og\n/2\n/( a\ni\n/)\nl og\n/2\n/( a\ni\nb\ni\n/)\n/(/3/./5/9/)\np\nB\n/=\nX\ni\na\ni\n/\u0003 b\ni\nS\n/\u0003\nl og\n/2\n/( b\ni\n/)\nl og\n/2\n/( a\ni\nb\ni\n/)\n/: /(/3/./6/0/)\nW e can v erify that for this c hoice of A and B /, the p ercen tage of S caused b y A and B is\nindeed p\nA\nand p\nB\nresp ectiv ely and that the m ultiplication of A and B indeed giv es S /:\nl og\n/2\n/( A /)\nl og\n/2\n/( AB /)\n/=\nl og\n/2\n/( S\np\nA\n/)\nl og\n/2\n/( S /)\n/= p\nA\n/(/3/./6/1/)\nl og\n/2\n/( B /)\nl og\n/2\n/( AB /)\n/=\nl og\n/2\n/( S\np\nB\n/)\nl og\n/2\n/( S /)\n/= p\nB\n/(/3/./6/2/)\nA /\u0003 B /= S\np\nA\n/\u0003 S\np\nB\n/= S\nlog\n/2\n/( A /)\nlog\n/2\n/( AB /)\n/+\nlog\n/2\n/( B /)\nlog\n/2\n/( AB /)\n/= S/: /(/3/./6/3/)\n/(/3/./6/4/)\nGiv en this metho d of probabilit y decomp osition/, w e can no w replace the probabilit y\np /( w /[ i /] /= w\nl\nj c\nk /( i /)\n/) of the i\nth\nw ord with\np /( w /[ i /] /= w\nl\nj c\nk /( i /)\n/) /= S /= A\ni\n/\u0003 B\ni\n/(/3/./6/5/)\nF or one/, this will allo w us to lo ok at the fraction of LT P caused b y di/\u000beren t con texts/.\nMoreo v er/, b y only lo oking at all the A\ni\n/'s /(or B\ni\n/'s/)/, w e can no w treat eac h comp onen t as a\nseparate mo del/, and this w a y /, w e can analyze the w eaknesses of eac h comp onen t separately /.\nPlease note that w e can extend the metho d of probabilit y decomp osition to mo dels that\nha v e more than t w o comp onen ts/. Supp ose for example that a language mo del calculates\nthe probabilit y of the i\nth\nw ord as\np /( w /[ i /] /= w\nl\nj c\nk /( i /)\n/) /= S /=\nX\na\ni\n/\u0003 b\ni\n/\u0003 c\ni\n/(/3/./6/6/)\nCHAPTER /3/. ANAL YZING AND IMPR O VING LANGUA GE MODELS /5/2\nW e can then simply write\nS /= A /\u0003 B /\u0003 C /(/3/./6/7/)\nA /= S\np\nA\n/(/3/./6/8/)\nB /= S\np\nB\n/(/3/./6/9/)\nC /= S\np\nC\n/(/3/./7/0/)\np\nA\n/=\nX\ni\na\ni\n/\u0003 b\ni\n/\u0003 c\ni\nS\n/\u0003\nl og\n/2\n/( a\ni\n/)\nl og\n/2\n/( a\ni\nb\ni\nc\ni\n/)\n/(/3/./7/1/)\np\nB\n/=\nX\ni\na\ni\n/\u0003 b\ni\n/\u0003 c\ni\nS\n/\u0003\nl og\n/2\n/( b\ni\n/)\nl og\n/2\n/( a\ni\nb\ni\nc\ni\n/)\n/(/3/./7/2/)\np\nC\n/=\nX\ni\na\ni\n/\u0003 b\ni\n/\u0003 c\ni\nS\n/\u0003\nl og\n/2\n/( c\ni\n/)\nl og\n/2\n/( a\ni\nb\ni\n/) c\ni\n/: /(/3/./7/3/)\n/(/3/./7/4/)\n/3/./5 Applicabili t y\nThe idea of iden tifying w eaknesses of probabilistic mo dels b y measuring the amoun t of LT P\ncaused b y di/\u000beren t ev en ts is v ery general/. It applies to all probabilistic mo dels that deriv e\na score for a sequence of tok ens b y m ultiplying the probabilities of individual tok ens and\nthat are ev aluated using the p erplexit y measure/.\nExamples of mo dels to whic h our idea of iden tifying w eaknesses is applicable are all\nthe mo dels review ed in section /(/2/./5/)/, ranging from N/-gram o v er N/-p os to mo dels based on\ndecision trees/. F urthermore/, the idea is also directly applicable to mo dels that are based on\nunits di/\u000beren t from w ords/, suc h as syllable based language mo dels /(/[/1/0/5 /]/) and phone based\nlanguage mo dels /(/[/9/6 /]/, /[/1/3/0 /]/)/, or to mo dels lik e /[/4/7 /]/, where the language mo del is made\ndep enden t on the state of a LR parser/.\nIn general/, sp eec h recognition systems ha v e b een based on phonemes /(/[/1/4/2 /]/)/, diphones\n/(/[/1/0/3 /]/, /[/2/2 /]/, /[/1/]/, /[/1/3/3 /]/)/, syllables /(/[/5/5 /]/, /[/1/5/4 /]/, /[/4/4 /]/, demi/-syllable s /(/[/1/2/7 /]/, /[/1/2/4 /]/)/, and disyllables\n/(/[/1/3/7 /]/)/. Language mo dels can b e built on all of these lev els and the idea prop osed here is\napplicable to all of them/. As an example/, w e will sho w ho w the idea of iden tifying w eaknesses\ncan b e applied to a syllable based language mo del/.\nThe basic linguistic unit in Japanese sen tences is the syllable /(see /[/1/0/5 /]/)/, corresp onding\nroughly to a consonan t/-v o w el unit/. The syllable therefore constitutes a con v enien t unit for\nrecognition of Japanese sp eec h and is/, for example/, used in the Japanese phonetic t yp ewriter\nCHAPTER /3/. ANAL YZING AND IMPR O VING LANGUA GE MODELS /5/3\n/(see /[/7/4 /]/)/. Giv en the syllable s\n/1\n/; /:/:/:/; s\ni /BnZr /1\nrecognized so far/, the acoustic mo del can decide\nwhic h syllables are lik ely to corresp ond to the next stretc h of the acoustic signal/. Ho w ev er/,\nas in w ord based recognition/, the acoustic mo del ma y not b e able to decide based on the\nsignal alone whic h one of the lik ely syllables w as the one sp ok en/. The phonetic t yp ewriter\ntherefore uses a syllable based trigram language mo del to decide whic h syllables are lik ely to\napp ear next/, giv en that the syllables recognized so far are s\n/1\n/; /:/:/:/; s\ni /BnZr /1\n/. As usual/, this syllable\ntrigram language mo del is trained on large amoun ts of text and its p erformance is measured\non a testing text/. Ho w can w e impro v e suc h an existing syllable based language mo del/?\nF ollo wing the idea prop osed here/, w e can /\frst iden tify the w eaknesses of the syllable based\nlanguage mo del/. W e therefore should iden tify the syllables whose predictions accoun t for a\nlarge fraction of the LT P /. Once these syllables ha v e b een iden ti/\fed/, w e can lo ok at wh y\nthey accoun t for suc h a large fraction of LT P and/, more imp ortan tly /, ho w w e can impro v e\nthe language mo del to a v oid this w eakness/.\nAn example of a mo del to whic h our de/\fnition of w eakness is not applicable is a prob/-\nabilistic con text free grammar /(see for example /[/1/5/7 /]/)/. This is b ecause the probabilit y of a\nsequence of w ords is not obtained b y m ultiplying the probabilities of eac h w ord/.\nBesides language mo deling for sp eec h recognition/, N /-gram based probabilistic mo dels\nare also used in the area of optical and handwritten c haracter recognition /(/[/5/4 /]/, /[/1/2/3 /] and\n/[/1/3/8 /]/)/. Ho w ev er/, according to a recen t surv ey on optical c haracter recognition /(/[/5/8 /, p/./1/1/]/)/,\nthe N /-gram mo del do es not come under the name of language mo del/, but is referred to as\ncon textual pro cessing and is one part of the p ostpro cessing in optical c haracter recognition/.\nNev ertheless/, the principles are v ery similar to language mo dels in sp eec h recognition/. F or\nexample/, in /[/1/3/8 /]/, the Viterbi algorithm /(/[/1/4/9 /]/) is used as in sp eec h recognition/, to /\fnd\nthe b est sequence of letters according to the probabilistic scores pro vided b y t w o mo dels\n/(corresp onding to the acoustic mo del and the language mo del in sp eec h recognition/)/. The\nmo del that corresp onds to the language mo del is in fact a letter bi/-gram language mo del/. In\nother w ords/, it calculates the probabilit y of a sequence of letters z\n/1\n/; /:/:/:/; z\nn\nb y m ultiplying the\nprobabilities of eac h letter z\ni\n/, whic h only dep ends on the previous letter/. This is expressed\nin the follo wing form ula/:\np /( z\n/1\n/; /:/:/:/; z\nn\n/) /=\ni /= n\nX\ni /=/1\np /( z\ni\nj z\ni /BnZr /1\n/) /: /(/3/./7/5/)\nThe probabilities p /( z\ni\nj z\ni /BnZr /1\n/) of letter bi/-grams are estimated from the Bro wn corpus/. As\nwith the syllable/-based Japanese language mo del w e just sa w/, w e can apply our analysis of\nCHAPTER /3/. ANAL YZING AND IMPR O VING LANGUA GE MODELS /5/4\nw eaknesses to this letter based language mo del in order to subsequen tly impro v e the mo del/.\n/3/./6 Summ ary\nIn this c hapter/, whic h con tains the cen tral idea of this thesis/, w e prop osed to p erform\nanalyses of w eaknesses of language mo dels in order to impro v e the mo dels afterw ards/, de/\fned\nwhat w e mean t b y /\\w eakness of a language mo del/\" and presen ted a metho d to iden tify the\nw eaknesses of a giv en mo del/.\nW e b egan b y noting the widely accepted idea that impro ving an y kind of mo del or theory\nis usually easier once its shortcomings are kno wn/. Moreo v er/, in a v ery simple/, in tuitiv e mo del\nof scien ti/\fc progress/, the kno wledge of the errors of an existing theory is crucial/. In analogy\nto this mo del of progress/, w e prop osed in this c hapter to analyze w eaknesses of a language\nmo del in order to subsequen tly impro v e the mo del/.\nSince the measure of w eakness of a language mo del should b e related to the p erformance\nmeasure used to ev aluate the mo del/, w e turned to the standard p erplexit y measure used for\nev aluating language mo dels/. W e in tro duced p erplexit y in tuitiv ely as the recipro cal of the\ngeometric mean of probabilities assigned to the w ords in the testing text/, deriv ed the measure\nfrom an information theoretic p oin t of view and discussed its adv an tages and shortcomings/.\nGiv en the p erplexit y and the closely related logarithm of the total probabilit y LT P /, w e\nde/\fned a w eakness of a language mo del in terms of LT P\n/5\n/. A part of a language mo del\nde/\fned b y a subset of w ords of the testing text is called a w eakness if it causes a large\nfraction of the LT P /. This conforms to our in tuitiv e meaning of a w eakness as something\nthat should b e impro v ed/, b ecause if w e w an t to impro v e the mo del signi/\fcan tly /, it is v ery\nimp ortan t to impro v e the parts of the mo del that cause a large fraction of the LT P /. F or\nmo dels with separate comp onen ts /(e/.g/. the class based mo dels/)/, w e also dev elop ed the\nmetho d of probabilit y decomp osition/, whic h allo ws us to analyze the w eaknesses of eac h\ncomp onen t separately /.\nAfter ha ving de/\fned a w eakness of a language mo del/, w e noted that this de/\fnition\nis applicable to an y probabilistic mo del that deriv es a score for a sequence of sym b ols\nb y m ultiplying the probabilities of eac h sym b ol and that is ev aluated with the p erplexit y\nmeasure/. W e can th us apply our de/\fnition to almost all of the commonly used language\n/5\nF or a discussion of whic h of the commonly asso ciated in tuitions of the term w eakness also apply to our\ntec hnical use of the term/, see page /4/2/.\nCHAPTER /3/. ANAL YZING AND IMPR O VING LANGUA GE MODELS /5/5\nmo dels/, including mo dels based on units di/\u000beren t from w ords /(e/.g/. phonemes/, syllables/)/.\nAs an example/, w e sho w ed brie/\ry ho w w e can apply our de/\fnition to a Japanese syllable\ntri/-gram mo del and ho w this could help in impro ving the mo del/. Besides language mo deling\nfor sp eec h recognition/, our idea of analyzing w eaknesses is also applicable to other areas\nthat use N /-gram statistics/, e/.g/. handwriting and optical c haracter recognition/.\nChapter /4\nAnalyzing and Impro ving a Bi/-p os\nLanguage Mo del\nIn the last c hapter/, w e presen ted the main idea of this thesis/, namely to p erform an analysis\nof the w eaknesses of language mo dels/. Moreo v er/, w e de/\fned what w e mean b y a w eakness\nof a language mo del/. In this c hapter/, w e apply this de/\fnition and the idea of analyzing\nw eaknesses to a concrete language mo del/. F or that purp ose/, w e /\frst c ho ose a training and\ntesting corpus and a language mo del /(section /4/./1/)/. W e v erify that the training data w e use\nis su/\u000ecien t to train our mo del and this leads to a discussion of the issue of sample space\n/(section /4/./2/)/. W e then pro ceed with the analysis of the w eaknesses of our mo del and presen t\nthe results /(section /4/./3/)/. T rying to impro v e one of the iden ti/\fed w eaknesses leads to the\ndev elopmen t of the generalized N /-p os mo del /(section /4/./4/)/.\n/4/./1 Cho osing a Corpus and a Language Mo del\nThe corpus used to train and test a language mo del is the primary source of information\nused in the mo del/. It is crucial to the o v erall undertaking that the corpus con tains su/\u000ecien t\ndata to train the mo del/. W e could of course c ho ose a language mo del and then a corpus/,\nbut w e w ould ha v e no guaran tee that the corpus con tains enough training data/. Since the\ncorpus size determines the complexit y of the language mo del that can b e adequately trained\nwith this amoun t of data/, w e will /\frst c ho ose a corpus and then a mo del/.\n/5/6\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /5/7\n/4/./1/./1 Cho osing a Corpus\nA corpus is a collection of text in mac hine readable format/, often annotated with additional\ninformation/. These annotations can supply information related to/, for example/, the parts\nof sp eec h of eac h w ord/, the parse tree of eac h sen tence or proso dic information/.\nBefore fo cusing on a sp eci/\fc corpus for our w ork/, let us consider the wide range of\nexisting corp ora/. According to a recen t review of corp ora researc h and construction /(see\n/[/3/1 /]/)/, the three most commonly used corp ora are the Bro wn corpus /(see /[/8/2 /]/, /[/3/9 /]/, /[/8/1 /]/)/, the\nLancaster/-Oslo/-Bergen /(LOB/) corpus /(see /[/7/1 /]/, /[/7/0 /]/, /[/9/7 /]/)/, and the London/-Lund corpus /(see\n/[/1/4/3 /]/)/. But a large v ariet y of other corp ora exist/: the Lancaster Sp ok en English Corpus\n/(SEC/) /(see /[/8/0 /]/)/, the British National Corpus /(see /[/1/1/9 /]/)/, the W all Street Journal Corpus\n/(a v ailable from the A CL//DCI/)/, and the In ternational Corpus of English /(see /[/4/9 /]/) /{ just\nto name a few/. Most of these corp ora are a v ailable through institutions and initiativ es/,\nwhic h ha v e b een created recen tly to o v ersee the collection of linguistic data/, e/.g/. the data\ncollection initiativ e of the Asso ciation for Computational Linguistics /(see /[/9/9 /]/, /[/1/7 /]/, /[/1/5/2 /]/,\n/[/1/5/3 /]/)/, the Europ ean Corpus Initiativ e of the Asso ciation for Computational Linguistics and\nthe Linguistic Data Consortium/.\nIn c ho osing one of the a v ailable corp ora for our w ork/, w e will consider t w o criteria/.\nFirst/, the corpus should b e used for language mo deling b y other researc hers/. This mak es\nthe results more widely acceptable and repro ducible/. Second/, w e prefer a /\\small/\" corpus\n/(e/.g/. less than a million w ords/) for the follo wing three reasons/.\nFirst/, man y curren t sp eec h recognizers are in tended for a sp eci/\fc application domain\n/(e/.g/. medical texts/)/. As p oin ted out in /[/1/1/7 /]/, the p erformance of a language mo del in suc h\na sp eci/\fc domain of application is often b etter if w e train the language mo del on a small\ncorpus task from the domain of application than if the language mo del is trained on a large\ncorpus not sp eci/\fc to the domain/. W e therefore need to train language mo dels on small/,\ndomain sp eci/\fc corp ora\n/1\n/.\nSecond/, if w e /\fnd a tec hnique to impro v e a language mo del trained on a small corpus/, it\nis lik ely that this tec hnique is also applicable to language mo dels trained on large corp ora/.\nOn the other hand/, if w e /\fnd a tec hnique to impro v e a language mo del trained on a large\ncorpus/, this tec hnique migh t require the a v ailabilit y of a large amoun t of training data\n/1\nDomain sp eci/\fc corp ora are most lik ely v ery small/, b ecause corp ora are exp ensiv e to pro duce/. As an\nexample consider the TI Digit Corpus /(/[/9/8/]/)/, a collection of a large set of sp ok en digits/. It required an\nestimated /$/3/0/0/,/0/0/0 to /$/4/0/0/,/0/0/0 for its construction/.\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /5/8\nand the tec hnique migh t therefore not b e applicable to language mo dels trained on small\ncorp ora/.\nFinally /, from a practical p oin t of view/, it is usually easier to understand a complex\nproblem b y lo oking at simple instances of the problem/. In the case of a language mo del/, a\nsimple instance is a simple mo del/, requiring little data to b e stored/, handled and analyzed/.\nUsing these criteria/, the LOB corpus is an adequate c hoice/. It con tains ab out one million\nw ords and is small compared to/, for example/, the W all Street Journal Corpus /(/5/0 million\nw ords/) or the British National Corpus /(/1/0/0 million w ords/)/. Moreo v er/, man y researc hers\nw orking on language mo deling also use the LOB /(/[/8/5 /]/, /[/1/0/9 /]/, /[/3/4 /]/, /[/1/0/6 /]/, /[/7/8 /]/, /[/1/1/0 /]/)/.\nEv en though w e are building language mo dels for sp eec h recognition/, the corpus is\nconstructed from written text/. This is common practice/, mainly for practical reasons/. Large\nquan tities of written text are already in a format that can b e used for a corpus/, whereas the\ntranscription of sp ok en text is time consuming/, tedious and exp ensiv e/.\nThe LOB corpus is divided in to /5/0/0 samples of text/. Eac h sample con tains sligh tly\nmore than /2/0/0/0 w ords and eac h w ord is tagged with one of /1/5/3 p ossible syn tactic classes/.\nThese syn tactic classes corresp ond to the parts of sp eec h /(p os/) that w e men tioned when w e\nin tro duce the N /-p os mo dels /(see section /2/./5/./3/, page /2/7/)/. The samples are group ed in to /1/5\ndi/\u000beren t categories/, dep ending on the source of the text/. T able /4/./1 sho ws the /1/5 di/\u000beren t\ncategories and the n um b er of samples in eac h/. W e see that the corpus co v ers a wide range of\nEnglish prose/. An example of the corpus material can b e found in App endix A/. W e use the\n/\frst /5/0/,/0/0/0 w ords of sections A/1/-A/3/4 as training text and roughly /2/5/,/0/0/0 w ords /(sections\nA/3/5/-A/4/4/) as testing text/. In section /4/./2/./3/, w e justify wh y /5/0/,/0/0/0 w ords constitutes enough\ntraining data/.\nIt has b een rep orted in the literature /(for example /[/6/0 /]/, /[/7/8 /]/) that the n um b er of classes\nor tags a language mo del uses in/\ruences the p erformance of the mo del/. In order to mak e our\nresults less dep enden t on the n um b er of tags pro vided with the corpus/, w e therefore decided\nto use more than one set of tags/. W e could ha v e pro duced di/\u000beren t tagsets automatically /,\nas suggested in /[/6/2 /]/, /[/6/0 /]/, /[/7/8 /] and /[/1/1/1 /] /. Ho w ev er/, since this is not the main issue addressed\nin this thesis/, w e used the follo wing simple heuristic to construct four di/\u000beren t tagsets/. The\noriginal tagset con tains/, for example/, four di/\u000beren t tags for adv erbial nouns /(NR/, NR/$/,\nNRS/, NRS/$/) and t w elv e di/\u000beren t tags for prop er nouns/. W e construct smaller tagsets b y /,\nfor example/, merging all four tags for adv erbial nouns in to one tag /(NR/)/, or b y merging\nall t w elv e tags for prop er nouns in to one tag /(NP/)/. W e can then construct an ev en smaller\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /5/9\nCategory Description n um b er of samples\nA Press/: Rep ortage /4/4\nB Press/: Editorials /2/7\nC Press/: Reviews /1/7\nD Religion /1/7\nE Skills/, T rades and Hobbies /3/8\nF P opular Lore /4/4\nG Belles Lettres/, Biograph y and Essa ys /7/7\nH Miscellaneous /3/0\nJ Learned and Scien ti/\fc W ritings /8/0\nK General Fiction /2/9\nL Mystery and Detectiv e Fiction /2/4\nM Science Fiction /6\nN Adv en ture and W estern Fiction /2/9\nP Romance and Lo v e Story /2/9\nR Humour /9\nT able /4/./1/: The di/\u000beren t categories of the LOB corpus\ntagset b y merging the set w e just constructed for adv erbial nouns /(NR/) and prop er nouns\n/(NP/) in to one tag for nouns /(N/)/. Th us/, starting from the original tagset/, w e construct three\nother tagsets b y merging tags with the same pre/\fx/. The resulting three tagsets ha v e /8/8/, /4/2\nand /2/4 tags resp ectiv ely /. All four tagsets/, together with examples for eac h tag are sho wn in\napp endix B/.\n/4/./1/./2 Cho osing a Language Mo del\nIn recen t y ears/, a great n um b er of di/\u000beren t language mo dels ha v e b een dev elop ed /(see section\n/2/./5/)/. Most commonly used are bi/-gram/, tri/-gram/, bi/-p os and tri/-p os mo dels/. They di/\u000ber\nsigni/\fcan tly in their complexit y and in the amoun t of training data needed/. Whic h of the\nmo dels should w e use for our w ork/? As w as p oin ted out in the last section/, w e use a rather\nsmall corpus/. F urthermore/, the argumen t in fa v or of a simpler mo del making a complex\npro cess easier to understand is also v alid for the c hoice of language mo del/. W e therefore\nc hose the mo del requiring the least amoun t of training data/, the bi/-p os mo del/. As men tioned\nin section /2/./5/./3/, existing N/-p os mo dels furthermore di/\u000ber in the fact that the classes the\nmo dels use are o v erlapping or that they are m utually exclusiv e/. Since the classes in the\nLOB are o v erlapping/, w e c hose a mo del allo wing m ultiple class mem b ership/. F or example/,\nthe w ord /`ligh t/' can b e a noun/, v erb or adjectiv e dep ending on the con text in whic h it is\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /6/0\nused/.\nNo w that w e ha v e decided whic h of the mo dels review ed in section /2/./5 w e are going to\nuse for our w ork/, w e will presen t the c hosen mo del in more detail/, describing the probabilit y\ndistributions in terms of smo othed frequency coun ts/. But /\frst/, let us recall the mo del in\nterms of its probabilit y distributions/, as it w as presen ted in section /2/./5/./3/. The probabilit y\nthat the i\nth\nw ord w /[ i /] is the w ord w\nl\nis calculated as the sum /(o v er all classes g\nj\n/) of the\nprobabilities that the i\nth\nw ord w /[ i /] is w\nl\n/, where w\nl\napp ears with the particular class g\nj\n/. The\nprobabilit y of the w ord w\nl\napp earing with a particular tag g\nj\nis the probabilit y of ha ving this\ntag g\nj\n/( g /( w /[ i /]/) /= g\nj\n/) giv en the tag g /( w /[ i /BnZr /1/]/) of the previous w ord /( p /( g /( w /[ i /]/) /= g\nj\nj g /( w /[ i /BnZr /1/]/) /)/)\ntimes the probabilit y of ha ving w ord w\nl\ngiv en the tag g\nj\n/( p /( w /[ i /] /= w\nl\nj g /( w /[ i /]/) /= g\nj\n/)/)/. This\nis expressed more precisely b y the follo wing form ula /(see section /2/./5/./3/)/:\np /( w /[ i /] /= w\nl\nj w /[ i /BnZr /1/]/)\n/=\nX\ng\nj\n/2 G\np /( g /( w /[ i /]/) /= g\nj\nj g /( w /[ i /BnZr /1/]/)/) /\u0003 p /( w /[ i /] /= w\nl\nj g /( w /[ i /]/) /= g\nj\n/) /:\nAs sho wn in section /2/./4/, w e can estimate the probabilit y distributions p /( g /( w /[ i /]/) /=\ng\nj\nj g /( w /[ i /BnZr /1/]/)/) and p /( w /[ i /] /= w\nl\nj g /( w /[ i /]/) /= g\nj\n/) in terms of frequencies f /( g /( w /[ i /]/) /= g\nj\nj g /( w /[ i /BnZr /1/]/)/)\nand f /( w /[ i /] /= w\nl\nj g /( w /[ i /]/) /= g\nj\n/) of ev en ts/:\np /( w /[ i /] /= w\nl\nj w /[ i /BnZr /1/]/)\n/=\nX\ng\nj\n/2 G\nf /( g /( w /[ i /]/) /= g\nj\nj g /( w /[ i /BnZr /1/]/)/) /\u0003 f /( w /[ i /] /= w\nl\nj g /( w /[ i /]/) /= g\nj\n/) /:\nIn order to a v oid zero probabilities /(see section /2/./4/)/, w e ha v e to ensure that at least for one\ntag g\n/0\n/2 G /, b oth frequencies in form ula /4/./1 are di/\u000beren t from zero/. If w e supp ose that ev ery\nw ord w\nl\nof our v o cabulary o ccurred at least once in our training text/, then the o ccurrence of\nw ord w\nl\nhas one tag g\n/0\nasso ciated with it and the factor f /( w /[ i /] /= w\nl\nj g /( w /[ i /]/) /= g\n/0\n/) in form ula\n/4/./1 is di/\u000beren t from zero/. Ho w ev er/, if g\n/0\nnev er o ccurred after the previous tag g /( w /[ i /BnZr /1/]/)/,\nthe factor f /( g /( w /[ i /]/) /= g\n/0\nj g /( w /[ i /BnZr /1/]/)/) in form ula /4/./1 is zero/. In order to a v oid the second\nfactor to b e zero/, w e therefore smo oth the second distribution/. As suggested in /[/8/5 /]/, and\nas explained in section /2/./2/, w e add a small constan t probabilit y v alue of c\n/2\nand then use a\nmatc hing constan t c\n/1\nto ensure that the sum o v er the probabilities of all the w ords is one/:\np /( w /[ i /] /= w\nl\nj w /[ i /BnZr /1/]/)\n/=\nX\ng\nj\n/2 G\nf /( g /( w /[ i /]/) /= g\nj\nj g /( w /[ i /BnZr /1/]/)/) /\u0003 /( c\n/1\n/\u0003 f /( w /[ i /] /= w\nl\nj g /( w /[ i /]/) /= g\nj\n/) /+ c\n/2\n/) /:\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /6/1\nThe language mo del is ev aluated on a testing text/, whic h ma y con tain w ords that are not\npart of the v o cabulary V /. W e therefore ha v e to adjust form ula /4/./1 to deal with these so\ncalled unkno wn w ords /. W e again adopt the approac h tak en in /[/8/5 /] and treat ev ery unkno wn\nw ord as the o ccurrence of one sp ecial sym b ol/, sa y unk now n /. W e giv e a constan t probabilit y\nv alue of d to o ccurrence of this sym b ol and ha v e to m ultiply all other probabilities b y\n/(/1 /BnZr d /) in order to ensure that the sum of probabilities of all w ords in the v o cabulary plus\nthe probabilit y of the sym b ol unknown sums up to one/:\nP /( w /[ i /] /= w\nl\nj w /[ i /BnZr /1/]/)\n/=\n/8\n/>\n/>\n/>\n/<\n/>\n/>\n/>\n/:\n/(/1 /BnZr d /)\nP\ng\nj\n/2 G\n/[/( c\n/1\n/\u0003 f /( g /( w /[ i /]/) /= g\nj\nj g /( w /[ i /BnZr /1/]/)/) /+ c\n/2\n/)\n/\u0003 f /( w /[ i /] /= w\nl\nj g /( w /[ i /]/) /= g\nj\n/)/] if w\nl\n/2 V\nd otherwise /.\nF ollo wing /[/8/5 /]/, w e estimate the v alue of d b y T uring/'s form ula as the n um b er of unique w ords\nin the training text divided b y the total n um b er of w ords in the training text/. d decreases\nwhen the amoun t of training data increases and for our training text of /5/0/,/0/0/0 w ords/, w e\nobtain d /=\n/4/6/3/7\n/5/0/0/0/0\n/\u0019 /0 /: /0/9/3 /: F or c\n/2\n/, w e c hose the arbitrary v alue of /1/0\n/BnZr /4\nas suggested in /[/8/5 /]/.\nAs explained in section /2/./4/, w e ha v e to c ho ose c\n/1\n/= /1 /BnZr j G j /\u0003 c\n/2\n/, whic h giv es c\n/2\n/= /0 /: /9/9/5/8 for\nthe tagset with /4/2 tags/.\nIn form ula /4/./1/, w e need to kno w the tag of the w ord w /[ i /BnZr /1/] in order to calculate the\nprobabilit y of w ord w /[ i /]/, ev en if w ord w /[ i /BnZr /1/] is an unkno wn w ord/. W e could simply use\nthe tags pro vided with our tagged testing text/, but this w ould b e /`c heating/'/, b ecause w e\nw ould b e using a source of information external\n/2\nto the language mo del to mak e its task\neasier/. F ollo wing /[/8/5 /]/, our mo del uses a heuristic in order to assign a tag to eac h w ord and\nthe follo wing three cases can o ccur/:\n/1/) the w ord is unkno wn/, e/.g/. it did not o ccur in the training text and is not part of the\nv o cabulary /.\n/2/) the w ord alw a ys o ccurred with the same tag in the training text/.\n/3/) the w ord o ccurred with sev eral tags in the training text/.\nThe mo del deals with these cases as follo ws/:\n/2\nThe tags w ere assigned to w ords man ually or b y a separate program /(a tagger/)/, and they are therefore\nnot part of the information the language mo del can use/.\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /6/2\n/1/) the mo del c ho oses the tag g\nj\nwhic h has the highest v alue of f /( g /( w /[ i /]/) /= g\nj\nj g /( w /[ i /BnZr /1/]/) /)/,\ne/.g/. the tag that is most lik ely to follo w the preceding tag/.\n/2/) the mo del uses the unique tag of the w ord/.\n/3/) the mo del c ho oses the tag g\nj\nwhic h con tributes the largest probabilit y to the sum o v er\nall p ossible tags in equation /4/./1/.\n/4/./2 Di/\u000berences in Sample Spaces\nIn section /4/./1/, w e c hose a corpus and a language mo del for our w ork/. Before w e can\npro ceed with the analysis of the w eaknesses of the mo del/, it is imp ortan t to mak e sure\nthat there is su/\u000ecien t data in the corpus to train the mo del/. Otherwise/, the results w ould\nnot b e signi/\fcan t/. As w e will see in section /4/./2/./2/, the analysis of the in/\ruence of the\namoun t of training data on the mo del pro duces a coun ter/-in tuitiv e result/: as the amoun t\nof training data increases/, the p erformance of the mo del decreases/. In order to understand\nthis b eha viour/, it is useful to lo ok at the underlying statistical issue/, the issue of sample\nspace/.\nIn statistics /(/[/4/1 /]/)/, the w ord /`exp erimen t/' is used in a v ery wide sense/, and it refers\nto an y pro cess of observ ation or measuremen t/. The results obtained from an exp erimen t\nare called its outcomes/. The set of all p ossible outcomes for eac h exp erimen t is called the\nsample space/. Probabilities are deriv ed as the ratio of successful outcomes to all p ossible\noutcomes and the sum of the probabilities of all ev en ts in the sample space has to b e one/.\nIn a language mo del/, the exp erimen t is the observ ation of the iden tit y of the w ord that\no ccurs next in a giv en con text/. One outcome is the o ccurrence of one particular w ord and\nthe sample space is the set of all w ords that can b e observ ed/.\n/4/./2/./1 Di/\u000berences Due to the Mo deling of Unkno wn W ords\nAs w e ha v e seen on page /6/1/, a language mo del has to deal with unkno wn w ords/. In the\nliterature/, w e can /\fnd sev eral w a ys of approac hing the issue of unkno wn w ords and w e will\npresen t four di/\u000beren t mo dels/, M/1 to M/4/, in the follo wing paragraphs/. F or our purp ose/, it is\nnot crucial ho w probabilities are deriv ed exactly for these unkno wn w ords/, but rather what\nthe underlying sample space is/. W e will therefore only describ e the sample space for eac h\nof the four mo dels/.\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /6/3\nW e will denote the sample space b y S and the n um b er of w ords in the training and testing\ntext b y n\ntr ain\nand n\ntest\nresp ectiv ely /. Let V/(train/[i/]/) denote the v o cabulary deriv ed from the\n/\frst i w ords of training text and let V/(train/) b e the shorthand notation for V/(train/[ n\ntr ain\n/]/)/.\nSimilarly /, w e de/\fne V /( test /[ i /]/) and V /( test /)/. Mo dels/, that c hange v o cabularies during the\ntesting will ha v e a sequence of v o cabularies denoted b y V\n/0\n/; V\n/1\n/; /:/:/:/; V\ntest\nn\n/. F urthermore/, as\nin tro duced on page /6/1/, w e will denote the unkno wn sym b ol with unknown /. The four mo dels\nare/:\nM/1 Ev ery o ccurrence of an unkno wn w ord is treated as the o ccurrence of the sym b ol\nunknown /. This giv es the sample space S /=V/(train/) /[ f unknown g /. This mo del w as\nused b y R/. Kuhn and R/. de Mori in /[/8/5 /]/.\nM/2 All unkno wn w ords are again treated as one sp ecial unknown sym b ol/. Ho w ev er/, after\nan unkno wn w ord o ccurs/, it is added to the v o cabulary /. W e therefore get a sequence of\nv o cabularies V\n/0\n/=V/(train/)/, V\n/1\n/=V/(train/) /[ V/(test/[/1/]/)/, /./././, V\nn\ntest\n/=V/(train/) /[ V/(test/)/.\nThis corresp onds to a sequence of sample spaces S\ni\n/= V\ni\n/[ f unknown g /. S\n/0\nis equal to\nthe sample space of mo del M/1 and S\nn\ntest\nis equal to the sample space of mo del M/3/.\nThis adaptiv e mo del w as prop osed b y Jelinek et/. al in /[/6/5 /]/.\nM/3 The mo del that lo oks at the testing text in adv ance /(/[/8/3 /]/)/. All the w ords that w ould\nb e unkno wn are added to the v o cabulary b efore the testing b egins/, giving the sample\nspace S /=V/(train/) /[ V/(test/)/.\nM/4 The mo del that deriv es a probabilit y for an unkno wn w ord based on a c haracter b y\nc haracter probabilit y /. It th us distinguishes b et w een all p ossible unkno wn w ords/. This\ncorresp onds to a sample space S /=V/(train/) /[f /( s\n/1\n/; /:/:/:/; s\nk\n/) g for all k /\u0015 /1 where eac h\ns\ni\nis one of the /9/5 prin table ASCI I c haracters/. S therefore has an in/\fnite n um b er of\nelemen ts/. This mo del w as prop osed b y Bro wn et al/. in /[/1/3 /]/.\nRecall from section /3/./2/./1 that the qualit y of a language mo del is measured b y the p er/-\nplexit y /, the recipro cal of the geometric mean of the probabilities assigned to w ords in the\ntesting text/. W e think that it is not meaningful to compare probabilities that are based on\ndi/\u000beren t sample spaces and w e will illustrate this p oin t in three di/\u000beren t w a ys/.\nFirst/, consider the extreme case of M/1 trained on zero w ords of text/. This giv es a\nsample space that only con tains one elemen t/, the unknown sym b ol/. Since there are no other\nelemen ts/, the mo del giv es the probabilit y of /1 to this sym b ol/. All w ords of the testing text\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /6/4\nwill b e treated as o ccurrences of unknown and the text will b e reduced to a rep etition of this\nsym b ol/. It is clear that the mo del assigning the probabilit y of /1 to this sym b ol will /`ac hiev e/'\na p erplexit y of /1/, whic h w ould imply that this is a particularly go o d language mo del/!\nSecond/, consider the n um b er of di/\u000beren t w ords that are distinguished in the ab o v e\nsample spaces/, but that are not part of V/(train/)/. The last mo del/, M/4/, has a coun table\nin/\fnite n um b er of these w ords/, whereas the /\frst mo del/, M/1/, has only one/. If w e ha v e a\n/\fxed amoun t of probabilit y to allo cate to these unkno wn w ords/, then it is clear that in\nmo dels with man y unkno wn w ords/, eac h one will /`receiv e/' a v ery small probabilit y /. T o\nconclude from the high p erplexit y of suc h a mo del that it is w orse at mo deling the language\nis not really correct/, b ecause it solv es a di/\u000beren t task/.\nThird/, consider all the unkno wn w ords in the testing text/, that the mo del M/1 treats as the\none sym b ol unknown /. If w e treat all these as separate w ords/, then the sum of probabilities\nof all w ords will b e more than one/. The mo del therefore do es not construct a probabilit y\ndistribution and w e therefore can/'t compare these /`probabilities/' with probabilities of mo dels/.\nAll these examples illustrate the p oin t that mo dels with di/\u000beren t sample spaces are in\nfact solving di/\u000beren t tasks and can/'t b e compared using the standard p erplexit y measure/.\nThe amoun t b y whic h the p erplexit y results are distorted dep ends on exactly ho w di/\u000ber/-\nen t the sample spaces are and ho w m uc h of the total p erplexit y is caused b y these unkno wn\nw ords/. F or example/, if the mo del has a v ery big v o cabulary /, it has a higher co v erage of\nw ords in the testing text and the p erplexit y caused b y unkno wn w ords is v ery small/. In the\nexp erimen ts rep orted in /[/1/3 /]/, the unkno wn w ords only accoun t for roughly /5/% of the total\nen trop y /(see section /3/./2/./2/) /. If w e reduce the sample space b y using M/1/, M/2 or M/3 /(instead\nof M/4/)/, the en trop y could at most b e reduced b y these /5/%/. The distortion is so small /(at\nmost /5/%/)/, b ecause this mo del w as trained on appro ximately /5/8/3 million w ords/, it has a\nv o cabulary of roughly /2/9/3/,/0/0/0 en tries and only ab out /1/% of the tok ens in the testing text\nare unkno wn/. In order to see ho w big the distortion is in a mo del trained on less data/, w e\nimplemen ted and ran M/1 and M/4 on our /5/0/,/0/0/0 w ords of training text/. W e c hose M/1 and\nM/4 b ecause they di/\u000ber the most in the size of their sample spaces and hence the distortion\nshould b e bigger than b et w een other mo dels/. W e ran our bi/-p os mo del /(see section /4/./1/./2/)\nwith M/1 or M/4 as mo dels of unkno wn w ords/, using our testing text with ab out /1/4/% of\nunkno wn w ords/. The results are sho wn in /\fgure /4/./2/. The o v erall p erplexit y w as /2 /: /6 /\u0003 /1/0\n/2\nwhen using M/1 and /4 /: /6 /\u0003 /1/0\n/4\nwhen using M/4 /(see section /4/./3/./2/)/. This h uge di/\u000berence can b e\nundersto o d b y lo oking at the geometric mean of the probabilities assigned b y the t w o mo dels\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /6/5\nMo del p erplexit y a vg/. prob of unkno wn w ords\nM/1 /2 /: /6 /\u0003 /1/0\n/2\n/7 /: /4 /\u0003 /1/0\n/BnZr /2\nM/4 /4 /: /6 /\u0003 /1/0\n/4\n/9 /: /4 /\u0003 /1/0\n/BnZr /1/7\nT able /4/./2/: Comparison of mo dels M/1 and M/4\nto unkno wn w ords/. The /\frst mo del had a geometric mean of appro ximately /7 /: /4 /\u0003 /1/0\n/BnZr /2\n/, the\nsecond mo del of appro ximately /9 /: /3/9 /\u0003 /1/0\n/BnZr /1/7\n/. The impact on the o v erall p erplexit y of this\ndi/\u000berence in probabilit y is so m uc h bigger in our exp erimen t b ecause the unkno wn w ords\naccoun t for roughly /5/1/% of the p erplexit y when M/4 is used /(see section /4/./3/./2/)/.\nW e do not try to decide here what the /\\correct/\" sample space should b e/. This should\nb e done b y the researc h comm unit y in general and/, as one of the review ers of our pap er\n/[/1/4/7 /] p oin ted out/, researc hers seem to fa v or mo dels lik e M/4 b ecause these mo dels do not\nrequire researc hers to agree on a /\fxed v o cabulary /. Ho w ev er/, as long as di/\u000beren t mo dels\nha v e di/\u000beren t sample spaces/, one should k eep in mind the distortion this can cause to the\np erplexit y results when comparing language mo dels with this measure/.\nThe fo cus of this w ork is the analysis of the main part of the mo del/, not the prediction\nof unkno wn w ords/. F or the remainder of the w ork/, w e therefore w an t to c ho ose the mo del\nin whic h the prediction of unkno wn w ords pla ys the least imp ortan t role/. Hence/, w e will\nuse mo del M/1 for the remainder of this thesis/.\n/4/./2/./2 Di/\u000berences Due to Di/\u000beren t Amoun ts of T raining T ext\nIn our implemen tation of M/1 /(see equation /4/./1 for the exact equation/)/, w e made the follo wing\nobserv ation/: as the training text increases in size/, the p erformance\n/3\nof the mo del decreases/.\nThis can b e seen in Figure /4/./1/, whic h sho ws the logarithm of the total probabilit y LT P /(see\nsection /3/./2/./1/) of the testing text for di/\u000beren t sizes of training texts and a set of /4/2 tags/. LT P\ndecreases as the training size increases/. This b eha viour is coun ter/-in tuitiv e b ecause/, as the\ntraining text increases/, the mo del should get b etter at predicting the testing text and the\nLT P should increase/. In order to /\fnd the reason for this b eha viour/, w e lo ok at the parts of\nLT P /, that are caused b y non/-v o cabulary w ords/, LT P /( unk now n /)/, and b y v o cabulary w ords/,\n/3\nAs p oin ted out in section /3/./2/./1/, it is more con v enien t to use LT P instead of P P /. Moreo v er/, since LT P\nand P P measure the same prop ert y /, this do es not in/\ruence the results/.\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /6/6\n/-/1/4/0/0/0/0\n/-/1/2/0/0/0/0\n/-/1/0/0/0/0/0\n/-/8/0/0/0/0\n/-/6/0/0/0/0\n/-/4/0/0/0/0\n/-/2/0/0/0/0\n/0\n/0 /5/0/0/0 /1/0/0/0/0 /1/5/0/0/0 /2/0/0/0/0 /2/5/0/0/0 /3/0/0/0/0 /3/5/0/0/0 /4/0/0/0/0 /4/5/0/0/0 /5/0/0/0/0\nL TP\nn um b er of training w ords\nL TP\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002 /\u0002\nL TP/(kno wn/)\nb\nb\nb\nb\nb\nb\nb\nb\nb\nb\nb\nb b\nL TP/(unkno wn/)\ns\ns\ns\ns s\ns\ns\ns\ns\ns\ns\ns s\nFigure /4/./1/: The graphs for L TP /, L TP/(kno wn/) and L TP/(unkno wn/)\nLT P /( k now n /) separately/:\nLT P /=\nX\ni/;s/:t/:w /[ i /] /2 V\nl og\n/2\n/( P /( w /[ i /] /= w\nl /( i /)\nj w /[ i /BnZr /1/]/)/) /+\nX\ni/;s/:t/:w /[ i /] /6/2 V\nl og\n/2\n/( P /( w /[ i /] /= w\nl /( i /)\nj w /[ i /BnZr /1/]/)/)\n/= LT P /( k now n /) /+ LT P /( unk now n /)\nBoth are also sho wn in Figure /4/./1/. LT P /( unk now n /) increases and LT P /( k now n /) decreases\nas the training text gro ws/. W e explain this as follo ws/. As the training size increases/, more\nand more w ords that are unkno wn when a small training text is used/, b ecome kno wn/. Eac h\nof these w ords that is unkno wn and receiv es the probabilit y of d when a small training text\nis used/, will receiv e the probabilit y v alue according to the bi/-p os form ula when a larger\ntraining text is used/. It turns out that LT P /( k now n /) decreases more than LT P /( unk now n /)\nincreases/. This happ ens b ecause the probabilit y d our mo del assigns to unkno wn w ords is\nhigher than the a v erage probabilit y assigned to w ords in the v o cabulary /.\nThis b eha vior is again due to the fact that mo dels trained on di/\u000beren t amoun ts of text\nha v e di/\u000beren t underlying sample spaces/. W e already men tioned this problem in section\n/4/./2/./1/, when w e considered a mo del trained on zero w ords of text/, whic h w ould ha v e a\np erplexit y of one/. In ab o v e example/, the di/\u000berence in sample spaces leads to a distortion of\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /6/7\nthe probabilit y measure to the exten t that mo dels trained on less data p erform /`b etter/'/.\nOne w a y to solv e this problem is to adjust for some of the distortion caused b y di/\u000beren t\nsample spaces/. As w e men tioned earlier/, b y giving the probabilit y of d to ev ery unkno wn\nw ord/, the probabilities sum up to more than one/. F or example/, if w e ha v e r di/\u000beren t\nunkno wn w ords in the testing text and distinguish b et w een them/, the sum of the probabilities\nof the p ossible w ords will add up to /1 /+ /( r /BnZr /1/) /\u0003 d /, /1 /BnZr d for the w ords in the v o cabulary /, and\nan extra r /\u0003 d for all the unkno wn w ords/. Once the testing has b een completed/, w e can adjust\nfor this in the follo wing w a y /. Supp ose a language mo del is tested on a text that con tains\ns o ccurrences of these r di/\u000beren t unkno wn w ords/. If w e supp ose a uniform distribution of\nthe unkno wn w ords/, for example/, as a rough appro ximation/, w e can divide the probabilities\nof the unkno wn w ords b y r /. W e call this adjustmen t the adjusted logarithm of the total\nprobabilit y /, ALT P /, and similarly /, the adjusted p erplexit y /, AP P /:\nALT P /= ALT P /( k now n /) /+ ALT P /( unk now n /) /(/4/./1/)\n/=\nX\ni/;s/:t/:w /[ i /] /2 V\nl og\n/2\n/( p /( w /[ i /] /= w\nl /( i /)\nj w /[ i /BnZr /1/]/)/) /(/4/./2/)\n/+\nX\ni/;s/:t/:w /[ i /] /6/2 V\nl og\n/2\n/(\np /( w /[ i /] /= w\nl /( i /)\nj w /[ i /BnZr /1/]/)\nr\n/) /(/4/./3/)\n/= LT P /( k now n /) /+ LT P /( unk now n /) /+ s /\u0003 l og\n/2\n/(\n/1\nr\n/) /(/4/./4/)\n/= LT P /BnZr s /\u0003 l og\n/2\n/( r /) /(/4/./5/)\nAP P /= /(/2\nALT P\n/)\n/BnZr /1 /=n\n/(/4/./6/)\nThis will ensure that the probabilities sum up to one/, but it will not c hange the fact that\nd is /`allo cated/' to all unkno wn w ords/. In order to calculate AP P /, w e just calculate LT P\nas b efore and k eep a coun ter for s and r /. When w e reac h the end of the testing text/, w e\ncalculate ALT P and AP P from LT P b y adding the factor /BnZr s /\u0003 l og\n/2\n/( r /) /(see equation /4/./5/)/.\nFigure /4/./2 sho ws the adjusted logarithm of the total probabilit y /, ALT P /, and its decom/-\np osition in to ALT P /( k now n /) and ALT P /( unk now n /)/. W e can see that the ALT P increases\nas the training data increases/. The di/\u000berence LT P /BnZr ALT P exactly quan ti/\fes ho w m uc h a\nlanguage mo del is /`c heating/' b y allo cating d to ev ery unkno wn w ord/.\nLet us summarize the adv an tages and disadv an tages of the adjusted p erplexit y AP P /.\nFirst/, w e can appro ximately compare language mo dels with di/\u000beren t v o cabularies/. The\nAP P ensures that the mo dels agree in parts of their sample spaces/, namely /, that the mo dels\ndistinguish b et w een all the w ords that o ccur in the testing text/. Second/, from a practical\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /6/8\n/-/1/8/0/0/0/0\n/-/1/6/0/0/0/0\n/-/1/4/0/0/0/0\n/-/1/2/0/0/0/0\n/-/1/0/0/0/0/0\n/-/8/0/0/0/0\n/-/6/0/0/0/0\n/-/4/0/0/0/0\n/-/2/0/0/0/0\n/0 /5/0/0/0 /1/0/0/0/0 /1/5/0/0/0 /2/0/0/0/0 /2/5/0/0/0 /3/0/0/0/0 /3/5/0/0/0 /4/0/0/0/0 /4/5/0/0/0 /5/0/0/0/0\nAL TP\nn um b er of training w ords\nAL TP\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002 /\u0002\n/\u0002\n/\u0002\n/\u0002 /\u0002 /\u0002\nAL TP/(kno wn/)\nb\nb\nb\nb\nb\nb\nb\nb\nb\nb\nb\nb\nb\nAL TP/(unkno wn/)\ns\ns\ns\ns\ns\ns\ns\ns\ns\ns\ns\ns\ns\nFigure /4/./2/: The graphs for AL TP /, AL TP/(kno wn/) and AL TP/(unkno wn/)\np oin t of view/, w e can write a language mo del without ha ving to kno w the testing text/, whic h\nshould mak e it easier to run a language mo del on an y testing text and to compare existing\nlanguage mo dels/. Third/, w e can no w quan tify b y ho w m uc h a mo del is /`c heating/' due to its\nmo deling of unkno wn w ords/. One disadv an tage of this metho d is that it can not b e used to\nadjust the probabilities of w ords as one go es through the testing text /(e/.g/. during sp eec h\nrecognition/)/. The adjustmen t can only b e done once the complete testing text has b een\nseen/. Ho w ev er/, it is su/\u000ecien t if one w an ts to appro ximately compare t w o language mo dels\nwith di/\u000beren t v o cabularies/. Another disadv an tage is that mo dels trained on di/\u000beren t texts\ncan still di/\u000ber in their underlying sample spaces/. The AP P measure only ensures that they\ndistinguish b et w een all the w ords in the testing text/, but can of course not ensure that their\nsample spaces are iden tical/. In other w ords/, the t w o mo dels can still try to solv e di/\u000beren t\ntasks/.\nAnother w a y of solving the problem of di/\u000beren t sample spaces is to /\fx the v o cabulary\nindep enden tly of the training text/. This ensures that the underlying sample spaces are\niden tical and that the p erplexit y measure is used to ev aluate t w o mo dels that are trying\nto solv e the same task/. In order to /\fx the v o cabulary indep enden tly of the training text/,\nw e need to mo dify our mo del sligh tly /. If the v o cabulary is /\fxed in adv ance/, it ma y con tain\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /6/9\n/-/1/8/0/0/0/0\n/-/1/6/0/0/0/0\n/-/1/4/0/0/0/0\n/-/1/2/0/0/0/0\n/-/1/0/0/0/0/0\n/-/8/0/0/0/0\n/-/6/0/0/0/0\n/-/4/0/0/0/0\n/-/2/0/0/0/0\n/0\n/0 /5/0/0/0 /1/0/0/0/0 /1/5/0/0/0 /2/0/0/0/0 /2/5/0/0/0 /3/0/0/0/0 /3/5/0/0/0 /4/0/0/0/0 /4/5/0/0/0 /5/0/0/0/0\nL TP\nn um b er of training w ords\nL TP\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002 /\u0002 /\u0002\nL TP/(kno wn/)\nb\nb\nb\nb\nb\nb\nb\nb\nb\nb\nb\nb\nb\nL TP/(unkno wn/)\ns\ns s s s s s s s s s s s\nL TP/(unseen/)\n/3\n/3\n/3\n/3\n/3\n/3\n/3\n/3\n/3\n/3\n/3\n/3\n/3\nFigure /4/./3/: The graphs for L TP /, L TP/(kno wn/)/, L TP/(unkno wn/) and L TP/(unseen/) with /\fxed\nv o cabulary\nw ords that w ere nev er seen in the training text and w e will call these w ords unse en /. What\nprobabilities should w e assign to unse en w ords/? W e c hange the mo del so that it giv es a\nsmall/, arbitrarily c hosen v alue d\n/1\nto ev ery unseen w ord/, whic h leads to the follo wing mo del/:\np /( w /[ i /] /= w\nl\nj w /[ i /BnZr /1/]/)\n/=\n/8\n/>\n/>\n/>\n/>\n/>\n/<\n/>\n/>\n/>\n/>\n/>\n/:\n/(/1 /BnZr u /\u0003 d\n/1\n/BnZr d\n/2\n/)\nP\ng\nj\n/2 G\n/[/( c\n/1\n/\u0003 f /( g /( w /[ i /]/) /= g\nj\nj g /( w /[ i /BnZr /1/]/)/) /+ c\n/2\n/)\n/\u0003 f /( w /[ i /] /= w\nl\nj g /( w /[ i /] /= g\nj\n/)/] if w\nl\n/2 V and w\nl\nw as seen\nd\n/1\nif w\nl\n/2 V but w\nl\nunseen\nd\n/2\nif w\nl\n/6/2 V\nAll the constan ts that are part of the standard bi/-p os mo del/, for example c\n/1\n/; c\n/2\nand d\n/2\n/(whic h corresp onds to the former d /)/, are determined as men tioned in section /4/./1/./2/. F or\nthe new constan t d\n/1\ncorresp onding to the probabilit y of unse en w ords w e arbitrarily c hose\nd\n/1\n/= /1/0\n/BnZr /6\n/. The constan t u is set to the n um b er of unseen w ords in the v o cabulary and the\nsecond term u /\u0003 d\n/1\nis necessary to ensure that the probabilities of all w ords sum up to one/.\nFigure /4/./3 sho ws the LT P of the mo di/\fed mo del/. W e can see that the mo del impro v es\nas the size of the training text increases/. The mo di/\fed mo del therefore conforms to our\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /7/0\nin tuition/. Moreo v er/, since the v o cabulary is /\fxed in adv ance/, the LT P caused b y unkno wn\nw ords do es not c hange when the training text is c hanged/. Ho w ev er/, w e can also see that\nLT P has LT P /( unseen /)/, the en trop y caused b y unseen w ords/, as an additional comp onen t/.\nAs the the size of the training text increases/, more and more w ords that w ere previously\nunse en and receiv ed the probabilit y of d\n/1\nb ecome part of the v o cabulary and receiv e a\nprobabilit y according to the bi/-p os form ula/. This explains wh y LT P /( unseen /) increases and\nLT P /( k now n /) decreases as the training text gets bigger/. Since LT P /( unseen /) increases more\nthan LT P /( k now n /) decreases/, the o v erall e/\u000bect on LT P is an increase/, whic h corresp onds\nto an impro v emen t in the mo del/.\nSumming up/, w e ha v e no w seen t w o w a ys of dealing with di/\u000berences in sample spaces\ncaused b y di/\u000beren t amoun ts of training data/. W e can alleviate the problem of di/\u000beren t\nsample spaces b y using the adjusted p erplexit y measure or w e can a v oid the problem of\ndi/\u000beren t sample spaces en tirely b y /\fxing the v o cabulary indep enden tly of the training text/.\nIf it is not p ossible to agree on a common v o cabulary /(e/.g/. b ecause di/\u000beren t researc hers\nw orking in di/\u000beren t lo cations do not agree/)/, the AP P is a /\rexible w a y of making the results\ncomparable/, without ensuring iden tical sample spaces/. Ho w ev er/, from a theoretical p oin t\nof view/, /\fxing the v o cabulary in adv ance is more satisfying than using the AP P /. First/, it\nensures that the sample spaces are iden tical for all mo dels/. Second/, it mak es sense that the\nsample space should b e /\fxed b efore one starts to compare probabilities/. After all/, if one\nw an ts to compare probabilities tak en from probabilit y distributions/, the distributions should\nb e constructed o v er the same sample space/. Because it is preferable from a theoretical p oin t\nof view to /\fx the v o cabulary in adv ance and b ecause w e ha v e no problem in agreeing on a\ncommon v o cabulary /, w e will /\fx the v o cabulary based on the training text in the rest of this\nthesis/.\n/4/./2/./3 In/\ruence of the Amoun t of T raining Data on the P erformance of\nour Language Mo del\nAs men tioned in the b eginning of section /4/./2/, w e w an ted to ensure that our corpus con tains\nenough data to train our bi/-p os mo del/. When w e tried to measure the in/\ruence of the\namoun t of training data on the p erformance of the mo del/, the surprising results prompted\na discussion of the underlying issue of sample spaces/. No w that w e ha v e c hosen to /\fx the\nv o cabulary indep enden tly of the training text/, w e ha v e a meaningful w a y to measure the\nin/\ruence of the amoun t of training data on the p erformance of the mo del/. Figure /4/./4 sho ws\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /7/1\n/2/0/0\n/4/0/0\n/6/0/0\n/8/0/0\n/1/0/0/0\n/1/2/0/0\n/1/4/0/0\n/1/6/0/0\n/1/8/0/0\n/0 /5/0/0/0 /1/0/0/0/0 /1/5/0/0/0 /2/0/0/0/0 /2/5/0/0/0 /3/0/0/0/0 /3/5/0/0/0 /4/0/0/0/0 /4/5/0/0/0 /5/0/0/0/0\np erplexit y\nn um b er of training w ords\nPP/(/1/3/4 tags/)\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\nPP/(/8/8 tags/)\nb\nb\nb\nb\nb\nb\nb\nb\nb\nb\nb\nb\nb\nPP/(/4/2 tags/)\ns\ns\ns\ns\ns\ns\ns\ns\ns\ns\ns\ns\ns\nPP/(/2/4 tags/)\n/3\n/3\n/3\n/3\n/3\n/3\n/3\n/3\n/3\n/3\n/3\n/3\n/3\nFigure /4/./4/: The p erplexit y for di/\u000beren t amoun ts of training data and di/\u000beren t tagsets\nthe p erplexit y of the mo del for di/\u000beren t amoun ts of training data and for four di/\u000beren t\ntagsets /(as men tioned in section /4/./1/./1/)/. First/, w e can see that the mo dels do not con tin ue\nto impro v e signi/\fcan tly when the training data is increased from /3/0/,/0/0/0 to /5/0/,/0/0/0 w ords/.\nW e th us assume that all the mo dels are w ell trained after /5/0/,/0/0/0 w ords/. This justi/\fes wh y\nw e only use /5/0/,/0/0/0 w ords of training text in the remainder of this thesis/. Second/, w e can\nsee that the amoun t of training data should in/\ruence the c hoice of tagsets/. If/, in this case/,\nthe a v ailable training data con tained only /1/5/,/0/0/0 w ords/, then a smaller tagset /(e/.g/. the /2/4\ntags/) w ould lead to b etter results than a larger tagset and this con/\frms our in tuition/.\n/4/./3 W eaknesses of the Bi/-p os Mo del\nIn the follo wing/, w e will apply the metho d of iden tifying w eaknesses presen ted in c hapter /3\nto the c hosen bi/-p os mo del /(see page /6/9/)/.\nThe /\frst w eakness w e will iden tify is the prediction of the next w ord in a v ery small\nn um b er of con texts/. W e p erform a v ery detailed analysis of these con texts in order to\nunderstand wh y they constitute a w eakness/. Ev en though w e do not pro ceed in trying\nto impro v e this w eakness/, the information w e unco v er b y analyzing w eaknesses is already\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /7/2\nhelpful in sho wing where w e should concen trate future e/\u000borts/.\nThe second w eakness w e will iden tify is the prediction of unkno wn w ords/. As with the\nprevious w eakness/, this is helpful for future w ork/. Moreo v er/, w e actually try to impro v e this\nparticular w eakness and w e dev elop a new mo deling of unkno wn w ords/. This results in a\nreduction of the p erplexit y ranging b et w een /1/4/% and /2/1/%/.\nThe third w eakness w e will iden tify is the second factor in our bi/-p os form ula /(section\n/2/./5/./3/, equation /2/./2/3/)/, the prediction of the next w ord giv en its class/. This is imp ortan t b y\nitself/, b ecause it again iden ti/\fes a w eakness that needs to b e impro v ed for man y di/\u000beren t\nlanguage mo dels/, ev en for the recen tly used probabilistic con text free grammars/. T rying\nto actually impro v e this w eakness leads us to the dev elopmen t of a new generalized N /-p os\nmo del that w e will presen t at the end of this c hapter/.\n/4/./3/./1 Di/\u000beren t Con texts\nIn the bi/-p os mo del w e use /(see page /6/9/)/, the curren t con text consists simply of the tag of\nthe preceding w ord/. T o recall the e/\u000bect this de/\fnition of con text has/, supp ose w\n/1\nand w\n/2\nare the last w ords of t w o sequences of w ords /\u000b and /\f /. F urther supp ose that w\n/1\nand w\n/2\nha v e\nthe same tag /(or set of tags/)/. The probabilit y with whic h the mo del will exp ect to see a\ncertain w ord next will b e the same in b oth cases/, whether the preceding sequence w as /\u000b or\n/\f /. In other w ords/, the mo del distinguishes b et w een as man y con texts as it has classes or\ntags and it has a separate distribution for eac h of these con texts/.\nWhen w e analyze this mo del with resp ect to its w eaknesses/, it seems natural to ask what\nfraction of the LT P is caused b y eac h con text /(or tag g /)/. F or that purp ose/, w e group the\nelemen ts of the sum in equation /3/./3 /, page /3/6/, according to the preceding tag g /, pro duce\na separate sum for eac h /( LT P\ng\n/)/, and determine what fraction of the total LT P eac h tag\nrepresen ts/. W e can th us calculate the fraction of LT P caused b y eac h con text or b y eac h\npreceding tag/. In table /4/./3/, w e giv e the ten tags that accoun t for the biggest part of the\nLT P when the tagset with /4/2 tags w ere used/. In order to /\fnd out wh y these are the tags\ncausing most of the LT P /, w e p erformed a more detailed analysis of the /\frst three tags/.\nF or eac h tag g /, w e lo ok ed at the n um b er n\ng\nof times g o ccurred/, the LT P caused b y the\nprediction of the next w ord giv en that the last tag w as g /( LT P\ng\n/) and the a v erage LT P p er\nw ord giv en that the last tag w as g /( av g\ng\n/)/. The results are sho wn in the /\frst three columns\nof table /4/./4/. Moreo v er/, w e used the metho d of probabilit y decomp osition to split up LT P\ng\nin to the fraction caused b y the prediction of the next tag /( f\ntag\n/)/, the prediction of the next\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /7/3\nT ag Description F raction of LT P\nN noun /0/./1/6\nA T article /0/./1/3\nIN prep osition /0/./1/2\nV v erb /0/./0/8\nP pronoun /0/./0/7\nNP prop er noun /0/./0/5\n/, comma /0/./0/5\nJJ adjectiv e /0/./0/5\n/. p erio d /0/./0/5\nBE forms of to b e /0/./0/4\nT able /4/./3/: The ten tags of the preceding w ord causing the biggest fraction of the LT P when\nthe tagset with /4/2 tags is used\ng n\ng\nLT P\ng\nav g\ng\nf\ntag\nf\nw or d\nf\nr est\nN /4/2/2/9 /BnZr /2 /: /1 /\u0003 /1/0\n/4\n/-/4/./9 /0/./4/8 /0/./4/6 /0/./0/6\nA T /2/4/4/8 /BnZr /1 /: /7 /\u0003 /1/0\n/4\n/-/6/./9 /0/./2/9 /0/./6/1 /0/./1/0\nIN /2/9/2/5 /BnZr /1 /: /6 /\u0003 /1/0\n/4\n/-/5/./3 /0/./4/3 /0/./4/8 /0/./0/9\nT able /4/./4/: A detailed analysis of the three tags causing the highest fraction of LT P\nw ord giv en its tag /( f\nw or d\n/) and the rest /( f\nr est\n/)/. f\nr est\ncon tains for example the prediction of\nunkno wn w ords/. These three v alues are sho wn in column four/, /\fv e and six in table /4/./4/.\nW e can see that the tag N causes the largest fraction of LT P b ecause it o ccurs v ery\noften/. Ev en though it is relativ ely easy to predict the next w ord giv en that the last tag w as N\n/( av g\nN\nis the highest/)/, this is more than comp ensated b y its frequency of o ccurrence/. When\npredicting the next w ord/, ab out the same fraction of LT P is coming from the prediction of\nthe next tag /( f\ntag\n/) and the prediction of the next w ord giv en its tag /( f\nw or d\n/)/.\nThe tag AT o ccurs far less frequen tly /, but predicting the next w ord kno wing that the\nlast tag w as AT is v ery di/\u000ecult /( av g\nAT\nis the lo w est/)/. Moreo v er/, w e can see from columns\nfour and /\fv e that it is the prediction of the next w ord giv en its tag /( f\nw or d\n/) that accoun ts\nfor most of the LT P /(/6/1/%/)/. This is b ecause articles are often follo w ed b y so called op en\nclass w ords/. Op en class w ords are w ords lik e v erbs or nouns whic h b elong to a class with\na v ery large/, almost unlimited n um b er of mem b ers/. This con trasts with closed class w ords\nlik e articles/, whic h b elong to a class with a v ery small/, predetermined n um b er of mem b ers/.\nBecause the prediction of the actual w ord giv en its class is v ery di/\u000ecult for op en class\nw ords/, they accoun t for a large fraction of LT P and the same is true of tags who can often\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /7/4\ng f\nLT P\nn\ng\nLT P\ng\nav g\ng\nN /0/./2/4 /5/1/7 /BnZr /5 /: /0 /\u0003 /1/0\n/3\n/-/9/./7\nIN /0/./2/4 /1/4/7/9 /BnZr /5 /: /0 /\u0003 /1/0/3 /-/3/./4\nV /0/./1/1 /2/2/2 /BnZr /2 /: /3 /\u0003 /1/0\n/3\n/-/9/./6\n/, /0/./0/5 /4/4/7 /BnZr /1 /: /0 /\u0003 /1/0\n/3\n/-/2/./3/7\n/. /0/./0/5 /5/2/0 /BnZr /1 /: /0 /\u0003 /1/0\n/3\n/-/2/./0\nT able /4/./5/: The /\fv e tags causing the highest fraction of LT P giv en that the last tag w as N\nb e follo w ed b y op en class w ords /(b ecause predicting the next w ord is v ery hard/, if the next\nw ord b elongs to an op en class/)/.\nThe tag I N is more similar to the tag N in its b eha vior/. It accoun ts for a large fraction of\nthe LT P b ecause it o ccurs quite frequen tly /(more often than the tag AT /)/, but the prediction\nof the next w ord giv en that the last tag is I N is easier than for the tag AT /( av g\nI N\nis higher/)/.\nMoreo v er/, w e can see from table /4/./4 that f\ntag\nis highest if the last w ord w as a noun/.\nWh y is it so di/\u000ecult to predict the next tag in this particular con text/? In order to answ er\nthis question/, w e lo ok ed at the tags that follo w N and the /\fv e tags causing most of the\nLT P are sho wn in table /4/./5/. F or eac h tag g /, w e giv e the fraction of LT P /( f\nLT P\n/)/, giv en\nthat the last tag w as N /, it causes/, the n um b er n\ng\nof times g o ccurred after N /, the LT P\ncaused b y the prediction of the w ord with tag g giv en that the last tag w as N /( LT P\ng\n/) and\nthe a v erage LT P p er w ord giv en that the last tag w as N /( av g\ng\n/)/. W e can see that there is a\nwide range of tags that frequen tly follo w nouns/. Giv en a noun/, it is indeed hard to predict\nwhat the next tag will b e/. Information/, that w ould for example allo w us to predict b etter\nwhic h of the three most imp ortan t tags will come next/, w ould therefore b e v ery useful in\nimpro ving the mo del/.\nF rom the more detailed analysis based on table /4/./4/, w e can see that the con texts causing\nthe largest fraction of LT P are the ones that o ccur v ery frequen tly or that are often follo w ed\nb y op en class w ords/.\nW e can also see from table /4/./3/, that the /\frst four tags accoun t for /4/9/% of the LT P /. In\nother w ords/, ab out /4/9/% of the LT P is caused b y a fraction of roughly\n/4\n/4/2\n/\u0019 /0 /: /1/0 of the tags/.\nIn general/, Figure /4/./5 sho ws the relationship b et w een the fraction of LT P and the fraction\nof tags causing this fraction of LT P /. The graph sho ws clearly that a small n um b er of tags\ncauses a large fraction of LT P and that a large n um b er of tags only causes a small fraction\nof LT P /. Qualitativ ely /, this kind of graph o ccurs v ery often in natural language pro cessing\nand it is a t ypical example of a Zipf distribution /(/[/1/5/8 /]/)/. Other quan tities ha ving similar\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /7/5\n/0\n/0/./2\n/0/./4\n/0/./6\n/0/./8\n/1\n/0 /0/./2 /0/./4 /0/./6 /0/./8 /1\nfraction\nof\ntags\nfraction of L TP\n/1/3/4 tags\n/3\n/3\n/3\n/3 /3\n/3\n/3 /3\n/3\n/3\n/3\n/3\n/3\n/3\n/3\n/3\n/3\n/8/8 tags\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/4/2 tags\ns\ns\ns\ns\ns\ns\ns\ns\ns\ns\ns\ns\ns\ns\ns\n/2/4 tags\nb\nb\nb\nb\nb\nb\nb\nb\nb\nb\nb\nb\nb\nFigure /4/./5/: fraction of LT P caused b y fraction of tags\ndistributions can b e found at almost all lev els of language ranging from the phoneme to\nthe sen tence /(/[/3 /]/, /[/4 /]/)/. One can argue that the Zip/\fan nature of the graph sho ws that the\ntags are not w ell suited for a language mo deling task/. There is really not m uc h p oin t in\ndi/\u000beren tiating among most of the /5/0/% of the tags that only accoun t for /1/0/% of the LT P /.\nIn this section/, w e ha v e sho wn on whic h con texts w e should concen trate our e/\u000borts to\nimpro v e our bi/-p os mo del/. These con texts are the p oin ts in the text where the preceding\nw ord w as a noun/, an article or a prep osition/. The adv an tage of kno wing these con texts is\nthat w e can no w lo ok at eac h one of them in turn in order to analyze why the prediction\nof the follo wing w ord is so di/\u000ecult /(see for example /4/./5/) and w e can then try to impro v e\nour mo del on this sp eci/\fc p oin t/. It is clear that solving suc h a sp eci/\fc problem is m uc h\neasier than trying to someho w impro v e the language mo del in general/. Ho w ev er/, w e will\nnot explore this issue an y further at this p oin t/, mainly b ecause w e ha v e already sho wn the\nusefulness of iden tifying w eaknesses b y sho wing ho w it reduces the size of the problem at\nhand/. Moreo v er/, the additional understanding of the mo del obtained from this detailed\nanalysis also sho ws the usefulness of the cen tral idea of this thesis/, the iden ti/\fcation and\nanalysis of w eaknesses of language mo dels/.\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /7/6\nunkno wn w ord mo del total LT P LT P /(unkno wn/) LT P /(unkno wn/)// LT P\nM/1 /-/1/./3/0e/+/0/5 /-/7/./7/8e/+/0/3 /0/./0/5/9/9\nM/4 /-/2/./5/1e/+/0/5 /-/1/./2/9e/+/0/5 /0/./5/1/4\nT able /4/./6/: LT P caused b y unkno wn w ords when mo del M/1 and M/4 are used\n/4/./3/./2 Unkno wn W ords\nW e will no w mo v e to the second w eakness/, the prediction of unkno wn w ords/.\nOn page /6/9/, w e deriv e the form ula of the bi/-p os mo del w e use/. Our mo del giv es a constan t\nprobabilit y to unkno wn w ords and this is di/\u000beren t from the rest of the mo del/, whic h uses\nthe bi/-p os form ula/. This prompted us to measure ho w m uc h impact this separate part of\nthe mo del/, whic h is only used for unkno wn w ords/, has on the o v erall p erformance/.\nIn order to measure the impact of unkno wn w ords/, w e group the elemen ts of the sum\nin equation /3/./3/, page /3/6 in to t w o groups/, the terms that corresp ond to unkno wn w ords and\nthe rest/. W e calculate eac h sum separately and measure the fraction of the LT P caused\nb y unkno wn w ords/. The result /(sho wn in table /4/./6/) is that the unkno wn w ords accoun t for\nappro ximately /6/% of the total LT P /, indep enden t of the tagset used/, since the probabilit y\ngiv en to unkno wn w ords do es not dep end on the n um b er of tags/. If w e use a di/\u000beren t mo del\nfor the unkno wn w ords/, namely the mo del M/4 from section /4/./2/./1/, this fraction is as high as\n/5/1/%/. As in the preceding section/, w e ha v e no w iden ti/\fed a sp eci/\fc w eakness/, the mo deling\nof unkno wn w ords/. W e no w pro ceed with trying to /\fnd w a ys of impro ving the mo del with\nresp ect to this particular problem/.\nThe curren t mo del giv es a constan t probabilit y to unkno wn w ords/, indep enden t of the\ncurren t con text or the most lik ely tag of the next w ord/. Ho w ev er/, it is clear that the\nprobabilit y of the next w ord b eing unkno wn dep ends on the h yp othesized tag for the next\nw ord/. F or example/, it is clear in tuitiv ely that if the next w ord is an op en class w ord/, the\nw ord is m uc h more lik ely to b e unkno wn/. Hence/, rather than ha ving\nP /( w /[ i /] /= w\nl\nj w /[ i /BnZr /1/]/) /= d\n/2\nif w\nl\n/6/2 V /;\nw e w ould no w lik e to mak e d\n/2\ndep end on the supp osed tag g\nj\nof the next w ord/. This leads to\na form ula similar to the part of the mo del that deals with w ords in the v o cabulary/: /\frst/, w e\npredict a lik ely next tag /( c\n/1\n/\u0003 f /( g /( w /[ i /]/) /= g\nj\nj g /( w /[ i /BnZr /1/]/)/) /+ c\n/2\n/)/, then w e predict an unkno wn\nw ord giv e this tag /( d\ng\nj\n/)/. This leads to the follo wing form ula/:\nP /( w /[ i /] /= w\nl\nj w /[ i /BnZr /1/]/)\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /7/7\nn um b er of tags old mo del new mo del impro v emen t\n/2/4 /2/6/5 /2/2/9 /0/./1/4\n/4/2 /2/5/9 /2/1/8 /0/./1/6\n/8/8 /2/4/9 /1/9/6 /0/./2/1\n/1/3/4 /2/4/3 /1/9/2 /0/./2/1\nT able /4/./7/: The p erplexit y of the old and the new mo del\n/=\n/8\n/>\n/>\n/>\n/>\n/>\n/<\n/>\n/>\n/>\n/>\n/>\n/:\nP\ng\nj\n/2 G\n/[/(/1 /BnZr u /\u0003 d\n/1\n/BnZr d\ng\nj\n/)/( c\n/1\n/\u0003 f /( g /( w /[ i /]/) /= g\nj\nj g /( w /[ i /BnZr /1/]/)/) /+ c\n/2\n/)\n/\u0003 f /( w /[ i /] /= w\nl\nj g /( w /[ i /]/) /= g\nj\n/)/] if w\nl\n/2 V and w\nl\nw as seen\nd\n/1\nif w\nl\n/2 V but w\nl\nunseen\nP\ng\nj\n/2 G\nd\ng\nj\n/\u0003 /( c\n/1\n/\u0003 f /( g /( w /[ i /]/) /= g\nj\nj g /( w /[ i /BnZr /1/]/)/) /+ c\n/2\n/) if w\nl\n/6/2 V\nW e can also explain this mo deling of unkno wn w ords b y sa ying that the unkno wn w ord\nis a w ord of the v o cabulary that can app ear with all tags/. W e can estimate the v alues d\ng\nj\nof form ula /4/./7 from the training text using the same tec hnique that w as used to estimate d\n/2\n/(see page /6/1/)/. W e used T uring/'s form ula again and estimated d\ng\nj\nas the n um b er of unique\nw ords with tag g\nj\no v er the n um b er of w ords with tag g\nj\n/. W e sho w in App endix C that the\nsum of the probabilities is still equal to one/.\nThe p erplexit y of the new mo del is sho wn in T able /4/./7/. First/, w e can see that the\nimpro v emen t is substan tial for all sets of tags/, ranging b et w een /1/4/% and /2/1/%/. Second/, the\nimpro v emen t increases when the n um b er of tags increases/. This is b ecause for eac h tag/,\nw e ha v e a di/\u000beren t distribution for unkno wn w ords/. As the n um b er of tags increases/, the\ndistributions of unkno wn w ords can b ecome more and more sp eci/\fc/.\nIn this section/, w e iden ti/\fed another w eakness of our bi/-p os mo del/, the mo deling of\nunkno wn w ords/. W e then impro v ed our mo del with resp ect to this particular w eakness\nb y dev eloping a new mo deling for unkno wn w ords/. In this new mo del/, the probabilit y of\nseeing an unkno wn w ord dep ends on the con text /(lik e the prediction of v o cabulary w ords/)/,\nin particular/, on the h yp othesized tag for the next w ord/. This resulted in an impro v emen t\nin p erformance ranging b et w een /1/4/% and /2/1/%/, dep ending on the n um b er of tags used/.\nFinding a new mo deling of unkno wn w ords/, whic h results in a signi/\fcan t impro v emen t/, is an\nimp ortan t result b y itself/. But equally imp ortan t for us is that it sho ws that the iden ti/\fcation\nof a w eakness is/, at least in this case/, a /\frst step in impro ving our mo del/. F urthermore/, in this\nconcrete example/, the iden ti/\fcation of a w eakness do es lead to a subsequen t impro v emen t\nof the mo del/. This sho ws the usefulness of the cen tral idea of this thesis/, the iden ti/\fcation\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /7/8\nn b of tags unkno wn fact w ord p os\n/2/4 /0/./0/6 /0/./0/1 /0/./5/8 /0/./3/5\n/4/2 /0/./0/6 /0/./0/1 /0/./5/3 /0/./4/0\n/8/8 /0/./0/6 /0/./0/1 /0/./4/5 /0/./4/8\n/1/3/4 /0/./0/6 /0/./0/1 /0/./4/3 /0/./5/0\nT able /4/./8/: The LT P caused b y di/\u000beren t comp onen ts of the mo del\nand analysis of w eaknesses of language mo dels/.\n/4/./3/./3 Di/\u000beren t Comp onen ts\nW e will no w mo v e to the third w eakness/, the prediction of a w ord giv en its tag/.\nThe bi/-p os mo del from page /6/9 calculates the probabilities of w ords in a t w o step pro cess/.\nFirst/, it calculates the probabilit y of a tag/; then/, giv en the tag/, it calculates the probabilit y\nof a w ord/. It seems natural to measure ho w m uc h of the o v erall LT P is caused b y eac h of\nthese comp onen ts of the mo del/.\nT o b e more precise/, using the metho d of probabilit y decomp osition in tro duced on page\n/5/1/, w e measure the fraction of LT P caused b y unkno wn w ords /( d\n/2\n/)/, b y the factor for kno wn\nw ords /(/1 /BnZr u /\u0003 d\n/1\n/BnZr d\n/2\n/)/, b y the term for predicting the next tag /( p /( g /( w /[ i /]/) j g /( w /[ i /BnZr /1/]/)/)/)/, and\nb y the term for predicting the next w ords giv en its tag /( p /( w /[ i /] /= w\nl /( i /)\nj g /( w /[ i /]/)/) /)/. Recall from\npage /6/9 that unseen w ords are w ords of the v o cabulary that do not o ccur in the training\ntext/. Since w e /\fxed our v o cabulary based on the /5/0/,/0/0/0 w ords of text used to train the\nmo del/, all w ords of the v o cabulary do app ear in the training text/. Hence/, there are no\nunseen w ords and unseen w ords do not accoun t for an y fraction of LT P /. W e therefore do\nnot ha v e a column for unseen w ords in table /4/./8/. Ho w ev er/, the fraction of LT P eac h other\ncomp onen t causes is sho wn in table /4/./8/.\nA /\frst observ ation is that the LT P tends to shift from the w ord column to the p os\ncolumn as the n um b er of tags increases/. This is v ery understandable/. If w e ha v e only /2/4\ntags/, it is m uc h easier to predict whic h one of them will app ear next than if w e ha v e /4/2 tags/.\nBy the same tok en/, if w e ha v e only /2/4 tags/, more w ords will b elong to eac h tag and it is\nharder to predict the w ord giv en the tag than if w e use /4/2 tags/. In general/, as the n um b er\nof tags increases/, the prediction of the next tag will b e m uc h harder/, but giv en the tag of\nthe next w ord/, it will b e easier to predict the actual w ord/.\nA second observ ation from table /4/./8 is that the prediction of the next w ord/, giv en its\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /7/9\ntag/, is a v ery imp ortan t part of the bi/-p os mo del/, at least when w e consider the amoun t of\nLT P caused b y eac h part/. Dep ending on the n um b er of tags used/, it accoun ts for b et w een\n/3/5/% and /5/8/% of the LT P /. This is more than the prediction of the next part of sp eec h when\n/2/4 and /4/2 tags w ere used/. Hence/, the prediction of the w ords giv en the tag is at least as\nimp ortan t as predicting the next tag and this is a v ery imp ortan t fact to note for future\nresearc h/.\nThe observ ation that the prediction of the next w ord/, giv en its tag/, is v ery imp ortan t also\nsheds a di/\u000beren t ligh t on using probabilistic con text free grammars for language mo deling/.\nRecen tly /, a n um b er of researc hers ha v e in v estigated the use of probabilistic con text free\ngrammars /(PCF G/'s/) or sto c hastic con text free grammars /(SCF G/'s/) /(/[/8/9 /]/, /[/2/4 /]/, /[/6/4 /]/, /[/9/1 /]/,\n/[/9/2 /]/, /[/1/1/4 /]/, /[/1/5/7 /]/, /[/1/2/8 /]/) for language mo dels/. The use of these grammars can lead to b etter\nlanguage mo deling b y impro ving the prediction of the next tag/. Ho w ev er/, the problem of\npredicting the actual w ord giv en its tag will remain/. F urthermore/, these grammars rely on\nthe same mec hanisms for estimating the probabilities of w ords/, giv en their tags/, and there\nis therefore no reason to b eliev e that these grammars will do b etter at this prediction than\nstandard N/-p os mo dels/. Hence/, trying to impro v e the prediction of the w ord giv en its tag is\nan imp ortan t researc h area/, indep enden t of the kind of mo del used to predict the tag/, e/.g/.\nN/-p os or PCF G/.\nHo w could one go ab out impro ving this w eakness/, the prediction p /( w /[ i /] /= w\nl /( i /)\nj g /( w /[ i /]/) /=\ng\nj\n/) of the next w ord giv en its tag/? In order to answ er this question/, w e will /\frst tak e a closer\nlo ok at p /( w /[ i /] /= w\nl /( i /)\nj g /( w /[ i /]/) /= g\nj\n/) b y examining whic h tags g\nj\nwhic h accoun t for most of\nthe LT P caused b y this factor /( LT P\nw or d\n/)/. F or eac h tag g /, w e will lo ok at the n um b er n\ng\nof\ntimes g is the tag of a v o cabulary w ord\n/4\n/, the LT P caused b y the prediction of these w ords\ngiv en that their tag is g /( LT P\ng\n/)/, the a v erage LT P of these w ords /( av g\ng\n/)/, the fraction f\ng\nof LT P\nw or d\ncaused b y eac h tag and the fraction f\ng\n/( total /) of the total LT P caused b y eac h\ntag/. These n um b ers are giv en in table /4/./9/.\nW e can see that it is a lot harder to predict the w ord giv en its tag if the tag is an op en\nclass tag /(lo w av g\ng\nfor N /; V /; J J /)/. The only tag that is not an op en class tag in the table is\nthe tag I N /(prep osition/)/. Ev en though it is relativ ely easy to predict the prep osition /(e/.g/.\nhigh av g\nI N\n/)/, it causes a large fraction of LT P b ecause it o ccurs more than t wice as often\nas for example the follo wing tag J J /(adjectiv es/)/.\n/4\nW e do not w an t unkno wn w ords to in/\ruence the analysis of ho w di/\u000ecult it is to predict the w ord giv en\nthe tag/.\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /8/0\ng n\ng\nLT P\ng\nav g\ng\nf\ng\nf\ng\n/(total/)\nN /3/2/0/1 /BnZr /2 /: /4 /\u0003 /1/0\n/4\n/-/7/./4 /0/./3/5 /0/./1/8\nV /1/6/2/0 /BnZr /1 /: /1 /\u0003 /1/0\n/4\n/-/6/./5 /0/./1/5 /0/./0/8/1\nIN /2/4/0/6 /BnZr /6 /: /1 /\u0003 /1/0\n/3\n/-/2/./5 /0/./0/9 /0/./0/4/7\nJJ /9/3/9 /BnZr /6 /: /0 /\u0003 /1/0\n/3\n/-/6/./4 /0/./0/9 /0/./0/4/7\nT able /4/./9/: A detailed analysis of p /( w /[ i /] /= w\nl /( i /)\nj g /( w /[ i /]/) /= g\nj\n/) for the four tags causing the\nhighest fraction of LT P\nIn the ligh t of these results/, ho w can w e impro v e p /( w /[ i /] /= w\nl /( i /)\nj g /( w /[ i /]/) /= g\nj\n/)/? The\ncurren t mo del has only one distribution for eac h g\nj\n/. Ho w ev er/, it seems clear from our\nin tuition that this probabilit y dep ends on con text in the same manner as the prediction\nof p /( g /( w /[ i /]/) j g /( w /[ i /BnZr /1/]/)/)/. As an example/, in the con text /\\P eter talk ed to the NOUN/\"/, the\no v erall frequency of nouns is not a go o d indicator for the lik eliho o d of app earance of a noun\nin this particular con text/. Nouns lik e /\\money/\"/, that p eople usually do not talk to/, are v ery\nunlik ely to app ear/. The information useful in predicting p /( w /[ i /] /= w\nl /( i /)\nj g /( w /[ i /]/)/) could b e\nen tirely di/\u000beren t from the information used for predicting p /( g /( w /[ i /]/) j g /( w /[ i /BnZr /1/]/)/) /. W e will try\nto impro v e this particular w eakness in the next section /(section /4/./4/)/.\nIn this section/, w e used the metho d of probabilit y decomp osition in tro duced on page\n/5/1 to measure the impact of di/\u000beren t comp onen ts of our bi/-p os mo del on the o v erall p er/-\nformance/. W e ha v e sho wn that the prediction of the next w ord/, giv en its tag is at least as\nimp ortan t as the prediction of the next part of sp eec h/. Again/, this information is helpful\nfor future w ork b ecause it iden ti/\fes a w eakness of the mo del/. Moreo v er/, this information is\nnot only useful for impro ving our bi/-p os mo del/, but it also sho ws that the recen t in terest\nin probabilistic con text free grammars as language mo dels do es not address an imp ortan t\npart of the mo del at all/. The prediction of the w ord giv en its tag therefore is an imp ortan t\narea of researc h/, ev en if en tirely di/\u000beren t mo dels/, e/.g probabilistic con text free grammars/,\nare used to predict the next tag/. These results sho w again the usefulness of our de/\fnition\nof a w eakness and of the cen tral idea of this thesis/, the analysis of w eaknesses of language\nmo dels/, in general/.\n/4/./4 The Generalized N /-p os Mo del\nIn the last section /(section /4/./3/./3/)/, w e iden ti/\fed the prediction of p /( w /[ i /] /= w\nl /( i /)\nj g /( w /[ i /]/)/) as\none of the w eaknesses/. In this section/, w e will try to impro v e this w eakness b y in tro ducing\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /8/1\nthe generalized N /-p os mo del/. W e will /\frst describ e the idea b ehind the generalized N /-p os\nmo del in section /4/./4/./1/, follo w ed b y preliminary exp erimen ts that sho w its usefulness as a\nframew ork whic h can incorp orate man y kinds of linguistic information/.\n/4/./4/./1 In tro ducing the Generalized N /-p os Mo del\nW e can in tro duce a more general N /-p os mo del b y generalizing the follo wing t w o asp ects of\nthe N/-gram and N/-p os mo dels/. First/, rather than ha ving distributions based on the last\nw ord or the last tag/, w e can base these distributions on an y information ab out the w ords\nseen so far/. W e will co de this information b y a v ariable X /. F or example/, X could stand\nfor the last w ord/, in whic h case X w ould range o v er all p ossible w ords of the v o cabulary /.\nOr X could stand for the state of a simple parser and w e could th us mak e the distributions\ndep end on that information/. The second asp ect that is b eing generalized relates to the N/-\np os mo del/, in whic h the probabilit y of the next w ord is made dep enden t on the h yp othesized\ntag alone /(e/.g/. p /( w /[ i /] /= w\nl /( i /)\nj g /( w /[ i /]/) /= g\nj\n/)/)/. It is clear in tuitiv ely that the frequencies of\nnouns also v ary signi/\fcan tly with the con text/. The example w e already used in the previous\nsection /(section /4/./3/./3/) is that in the con text /\\ P eter talk ed to the NOUN/\"/, the nouns that\np eople usually talk to are more lik ely to app ear/. Moreo v er/, out in tuition indicates that the\nimmediate con text of the NOUN/, e/.g/. the w ords /\\to the/\"/, do not constrain the c hoice of\np ossible nouns as m uc h as the fact that NOUN is b eing talk ed to b y P eter/. In other w ords/,\nev en though the immediately preceding w ord is v ery useful in predicting the next tag/, it\nseems lik ely that the information useful in predicting the actual noun is further a w a y from\nthe w ord to predict/. This is for example men tioned in /[/3/6 /, p/./1/8/7/]/) and /[/1/4 /]/. Our generalized\nmo del should therefore allo w the prediction of the actual w ord to dep end on con textual\ninformation and it should b e p ossible for this information to b e di/\u000ber ent from the one used\nto predict the next tag/.\nNo w that w e ha v e seen the in tuitiv e idea b ehind our generalized mo del/, w e will presen t\nit in more detail/. The probabilit y of a sequence of w ords W will again b e decomp osed as a\npro duct of probabilities of eac h w ord /(see equation /2/./5/)/:\np /( W /) /=\ni /= n\nY\ni /=/1\np /( w /[ i /] /= w\nl /( i /)\nj w /[/1 /: i /BnZr /1/]/) /: /(/4/./7/)\nThen the probabilit y of eac h w ord will b e mo deled as\np /( w /[ i /] /= w\nl /( i /)\nj w /[/1 /: i /BnZr /1/]/) /= p /( w /[ i /] /= w\nl\nj X\n/1\n/; /:/:/:/; X\nr /+ s\n/)\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /8/2\n/=\nX\ng\nj\n/2 G\np /( g /( w /[ i /]/) /= g\nj\nj X\n/1\n/; /:/:/:/; X\nr\n/) /\u0003 p /( w /[ i /] /= w\nl\nj g /( w /[ i /]/) /= g\nj\n/; X\nr /+/1\n/; /:/:/:/; X\nr /+ s\n/)\nwhere the X\nj\n/'s/, /1 /\u0014 j /\u0014 r /+ s /, denote v ariables co ding some information a v ailable from\nthe w ords w\n/1\n/; /:/:/:/; w\ni /BnZr /1\nseen so far/. It is imp ortan t to v erify that the resulting probabilities\nconstitute a probabilit y distribution/. In other w ords/, w e ha v e to ensure that\nX\nw /2 V\np /( w /[ i /] /= w j X\n/1\n/; /:/:/:/; X\nr /+ s\n/) /= /1 /: /(/4/./8/)\nA t a giv en p oin t in the sen tence/, all the X\nj\n/'s ha v e a certain /\fxed v alue/. If the t w o comp onen t\nprobabilities are true probabilit y distributions for all com binations of v alues of X\nj\n/(and this\nis ensured if they are constructed as usual from frequency coun ts/)/, w e will sho w in App endix\nD that the sum will indeed b e one/.\nThe amoun t of training data needed b y the generalized N /-p os mo del dep ends of course\non the kno wledge enco ded b y the di/\u000beren t v ariables/. W e therefore can not mak e an y general\nstatemen t with resp ect to the amoun t of training data needed/. But as w e will see/, the mo del\ncan reduce to the N /-gram mo del or the N /-p os mo del and the amoun t of training data needed\nin these cases will b e similar to the training data required b y the N /-gram or N /-p os mo del/.\nW e will no w sho w that the generalized mo del reduces to N/-gram and N/-p os mo del for\na particular c hoice of v ariables X\nj\n/. If w e c hose s /= /0 /; r /= N /BnZr /1 and X\nj\n/= g /( w\ni /BnZr j\n/) /; j /=\n/1 /; /:/:/:/; N /BnZr /1/, then the generalized N/-p os mo del reduces to the standard N/-p os mo del/. As\nsho wn in App endix E/, it will also reduce to the N/-gram mo del for r /= N /BnZr /1 /; X\nr /+ j\n/= X\nj\n/=\nw\ni /BnZr j\n/; j /= /1 /; /:/:/:/; N /BnZr /1/. F or other c hoices of the v ariables/, w e obtain mo dels that can/'t b e\nconstructed from N/-gram or N/-p os/. This sho ws that it is a true generalization/. F urthermore/,\nsome of the v ariables could co de linguistically relev an t facts extending o v er a longer distance\nin the sen tence/, e/.g/. the sub ject of the sen tence or the fact whether the v erb is transitiv e/.\nIt therefore is a framew ork that allo ws more general linguistic kno wledge to b e captured/.\nThese p oin ts sho w the theoretical p oten tial of the generalized N/-p os mo del as a framew ork/.\nHo w ev er/, from a practical p oin t of view/, the generalized N/-p os mo del is only useful if\nw e /\fnd sources of information for the v ariables X\nj\nthat actually help impro v e the qualit y\nof the mo del signi/\fcan tly /. What information should the v ariables actually capture in order\nto impro v e the qualit y of the mo del/? The lac k of an answ er to this question corresp onds to\na lac k of kno wledge in this area/. A lot of w ork needs to b e done in order to /\fnd out what\nkind of information is useful for this purp ose/. In the next section /(section /4/./4/./2/)/, w e presen t\na small step in that direction b y describing the information w e exp erimen ted with so far/.\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /8/3\nIn the next c hapter /(c hapter /5/)/, w e discuss what kno wledge migh t b e useful for language\nmo dels in general/.\n/4/./4/./2 Using the Generalized N /-p os Mo del\nIn the usual bi/-p os mo del/, the probabilit y distribution for p /( w /[ i /] j g /( w /[ i /]/) /= N oun /) is con/-\nstructed b y coun ting the n um b er of o ccurrences of eac h noun in the training text/. As seen\nin section /2/./4/, an estimate for the probabilit y of a particular noun w is then obtained b y\ndividing the n um b er of times w o ccurred as a noun b y the total n um b er of noun o ccur/-\nrences/. The probabilit y distribution obtained in this manner can then b e used to calculate\nthe probabilities of w ords in a testing text/.\nIn all the exp erimen ts describ ed here/, w e construct one other distribution on top of the\none just men tioned/. A v ariable X with t w o v alues /(/\\general/\" and /\\sp eci/\fc/\"/) is up dated as\nw e mo v e through training or testing text/. W e will see later in exactly whic h situation w e\nwill ha v e X /= g ener al and X /= specif ic /. In tuitiv ely /, w e will ha v e X /= specif ic only if\nw e are in a v ery sp eci/\fc con text/, e/.g/. if the curren t noun phrase w as in tro duced with the\nprep osition /\\during/\"/. In all other situations/, w e will ha v e X /= g ener al /. After ha ving read\nthe training text/, w e coun t ho w often eac h noun o ccurred when X had the v alue /\\sp eci/\fc/\"/.\nW e then construct a sp eci/\fc probabilit y distribution b y dividing the n um b er of times eac h\nnoun o ccurred with X /= specif ic b y the total n um b er of times w e had X /= specif ic /.\nWhen w e go through the testing text/, w e use the normal distribution when X /= g ener al /,\nbut the sp eci/\fc distribution when X /= specif ic /. W e th us replace p /( w /[ i /] j g /( w /[ i /]/) /= N oun /)\nb y p /( w /[ i /] j g /( w /[ i /] /) /= N oun/; X /)/, where X has t w o p ossible v alues/.\nPlease note that this is not the same as ha ving t w o separate tags/, sa y Noun/-general\nand Noun/-sp eci/\fc/. Ha ving t w o separate tags w ould also c hange the factor p /( g /( w /[ i /]/) /=\ng\nj\nj g /( w /[ i /BnZr N /+ /1 /: i /BnZr /1/]/)/)/, requiring man y more separate distributions/. Using p /( w /[ i /] j g /( w /[ i /]/) /=\nN oun/; X /)/, ho w ev er/, allo ws us to ha v e a /\fner distinction for the prediction of the actual w ord/,\nwhile still preserving the same lev el of abstraction for the prediction of the next tag/.\nBefore w e describ e what information is co ded b y X /, w e w ould lik e to address the issue of\nsmo othing again/. The normal noun distribution is a t ypical example of a Zipf distribution\n/(/[/1/5/8 /]/)/. A v ery small n um b er of w ords o ccur v ery often and a v ery large n um b er of w ords\no ccur v ery rarely /. As an example/, w e sho w the fraction of nouns that o ccur less than\nx/; /1 /\u0014 x /\u0014 /2/0 /; times in our /5/0/,/0/0/0 w ords of training text in /\fgure /4/./6/. W e can see that\nmore than /5/0/% of the nouns o ccur only once/. If w e construct the sp eci/\fc distribution in\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /8/4\n/0 /: /0\n/0 /: /2\n/0 /: /4\n/0 /: /6\n/0 /: /8\n/1 /: /0\n/0 /5 /1/0 /1/5 /2/0\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002\n/\u0002 /\u0002\n/\u0002\n/\u0002 /\u0002\nFigure /4/./6/: F raction of nouns o ccurring less than x times in text\nthe manner describ ed ab o v e/, man y w ords will not ha v e o ccurred with X /= specif ic /, and\nthey w ould get a zero probabilit y /. In order to a v oid that/, w e obtain a com bined distribution\np\ncomb\n/( w /[ i /] j g /( w /[ i /]/) /= g\nj\n/) b y smo othing the sp eci/\fc distribution with the normal distribution\nin the follo wing manner\np\ncomb\n/( w /[ i /] jj g /( w /[ i /]/) /= g\nj\n/) /= /\u0015p /( w /[ i /] j g /( w /[ i /] /) /= g\nj\n/) /+ /(/1 /BnZr /\u0015 /) p /( w /[ i /] j g /( w /[ i /]/) /= g\nj\n/; X /= specif ic /) /:\nRather than using deleted in terp olation /(/[/6/6 /]/) to determine the b est v alue for /\u0015 /, w e tried\n/\u0015 /= /0 /: /1 /; /0 /: /2 /; /:/: /:/; /0 /: /9 and used the v alue of /\u0015 that giv es the b est p erformance on the testing\ntext/.\nExp erimen t /1/: X indicates whether the noun is lik ely to b e singular/. Initially /, X is\nset to /\\general/\"/. If w e come across /\\this/\"/, /\\a/\" or /\\an/\" in the text/, the follo wing noun\nis lik ely to b e singular and X is set to /\\sp eci/\fc/\"/. X retains this v alue as long as w e are\nlik ely to not ha v e /\fnished this noun phrase and is then reset to general/. W e did not ha v e\na parsed corpus a v ailable and used v ery primitiv e tec hniques to decide what constitutes a\nnoun phrase/. Roughly /, w e considered the noun phrase to b e ongoing as long as w e encoun ter\nadjectiv es/, adv erbs or nouns/.\nExp erimen t /2/: X w as used to indicate whether the noun phrase w as preceded b y the\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /8/5\nExp/. n b/. w ords in S p erpl/. on S sp eci/\fc p erpl on S impro v/. on S\nExp/1 /4/7/9 /1/6/6/5 /1/4/2/8 /0/./1/7/8\nExp/2 /1/6 /4/6/9 /3/9/6 /0/./1/5/6\nExp/3 /2/3/7 /1/3/6/5 /1/3/9/4 /-/0/./0/2/1\nT able /4/./1/0/: Results on the subset S of w ords where X/=sp eci/\fc\nprep osition /\\during/\"/. W e used the same mec hanism as ab o v e to determine the b oundaries\nof a noun phrase/.\nExp erimen t /3/: With a v ery lo w probabilit y /, X w as randomly set to sp eci/\fc/. This exp er/-\nimen t w as p erformed to see whether a random c hoice for the v alue of X w ould lead to an\nimpro v emen t/.\nExp erimen ts one and t w o mak e the distribution dep end on information that can b e\nsev eral w ords a w a y /, dep ending on the length of the noun phrase/. This sho ws that the gener/-\nalized N /-p os mo del is a framew ork that can incorp orate linguisticall y relev an t information/,\nindep enden t of its distance to the curren t w ord/.\n/5\nThe resulting c hange in p erplexit y on all the appro ximately /2/5/,/0/0/0 w ords of testing text\nis /0/./0/3/1/, /0/./0/0/0/1/2 and /-/0/./0/0/2/1 for the three exp erimen ts/. The c hange is v ery small and this is\ndue to the fact that the sp eci/\fc distribution w as used only a v ery small n um b er of times /(e/.g/.\nX /= specif ic in the testing text/)/. W e therefore measured the impro v emen t in p erplexit y\nonly on the w ords that actually use d the sp eci/\fc distribution /(denoted with S for subset\nin the table /4/./1/0/)/. W e can see that the impro v emen t in these cases is signi/\fcan t/, esp ecially\nwhen compared to exp erimen t /3 where X w as set randomly /.\nA more detailed analysis of the /4/7/9 cases of exp erimen t /1 sho ws that in /1/9/5 cases\n/(appro x/. /4/1/%/)/, the probabilit y assigned to the w ord using the sp eci/\fc distribution w as\nactually lo w er than the one from the general distribution/. These w ere w ords that did not\no ccur with X /= specif ic during training/, but o ccurred with X /= specif ic during testing/.\nAccording to the sp eci/\fc distribution/, they w ould get the probabilit y zero/, but thanks to the\nin terp olation/, they get a probabilit y b et w een zero and the v alue of the general distribution/.\n/5\nHo w ev er/, the linguistic information captured b y the v ariables should dep end on the con text/. If it do es\nnot/, then the information will b e the same in all con texts and it will therefore not c hange the probabili t y\ndistributions /. As an example/, tak e the fact that the sub ject is often follo w ed b y a v erb/. W e could try to\nuse this to increase the probabiliti es of v erbs after w e ha v e seen a sub ject /(assuming for the sak e of this\nargumen t/, that w e can iden tify sub jects/)/. Ev en though one could think that this is a con text indep enden t\nstatemen t/, it do es dep end on the con text in the sense used here/: if w e ha v e seen a sub ject/, then w e will\nincrease the probabilit y of v erbs/; if w e ha v e not seen a sub ject/, w e w on/'t/. The fact that w e will increase the\nprobabilit y of v erbs therefore dep ends on the curren t con text/.\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /8/6\nThis in terp olated v alue will alw a ys b e less or equal to that of the general distribution /(equal\nif /\u0015 /= /1/) and this is wh y w e actually do w orse when using the sp eci/\fc distribution for these\nw ords/.\nThe critical issue in these exp erimen ts is that of capturing seman tic restrictions on\np ossible nouns based on con text/. Ho w m uc h of this can w e capture b y a purely distributional\nanalysis/? Can w e someho w estimate a separate distribution for eac h X\nj\nand then com bine\nthem meaningfully so that w e will not ha v e to estimate a separate distribution for eac h\np ossible com bination of v alues of the X\nj\n/'s/? These are imp ortan t questions that w e will\naddress in the next c hapter /(c hapter /5/)/.\nAs stated in section /4/./4/./1/, w e had t w o reasons for in tro ducing the more general mo del/.\nOne w as to impro v e the qualit y of the language mo del/, the other to capture linguisticall y\nor in tuitiv ely more satisfying information/. W e sho w ed that the generalized N/-p os mo del\nis a framew ork that can indeed incorp orate linguistic information sev eral w ords a w a y and\nw e thereb y ac hiev e the second goal/. The /\frst goal of reducing the p erplexit y has not b een\nac hiev ed on an o v erall lev el/. Ho w ev er/, the small o v erall impro v emen t do es not re/\rect the\ncapabilities of the generalized N /-p os mo del/, but the usefulness of the v ery simple kno wledge\nsource w e used/. Moreo v er/, the exp erimen ts p erformed sho w the signi/\fcan t reduction in\np erplexit y in the cases where the generalized mo del w as actually used/. F urther researc h\nneeds to /\fnd other sources of information that are useful in reducing the p erplexit y in a\nlarger n um b er of cases and this issue will b e addressed in c hapter /5/. These could then\nb e added to the generalized N /-p os mo del in the same manner/, leading to a bigger o v erall\nimpro v emen t/. Th us/, the existence of kno wledge that can reduce the p erplexit y in a large\nn um b er of cases will ultimately decide on the practical usefulness of the generalized N /-p os\nmo del/. But this applies to an y mo del that tries to incorp orate more kno wledge/. If there\nis no kno wledge that will reduce the p erplexit y in a large n um b er of cases/, then the mo del\nwill not lead to a signi/\fcan t impro v emen t/.\n/4/./5 Summ ary\nIn this c hapter/, w e applied the cen tral idea of this thesis/, our tec hnique of iden tifying\nw eaknesses of a language mo del/, to a commonly used bi/-p os language mo del/, rep orted the\nresults/, and th us sho w ed the usefulness of p erforming analysis of w eaknesses of language\nmo dels/.\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /8/7\nW e started b y c ho osing the Lancaster/-Oslo/-Bergen corpus and the bi/-p os mo del for our\nw ork and w e v eri/\fed that the /5/0/,/0/0/0 w ords of the corpus w e use as training data is su/\u000ecien t\nto train our mo del/. The fact that the p erformance of our mo del initially decreased with an\nincrease in training data prompted a discussion of the issue of a sample space/, the set of all\np ossible ev en ts considered b y a mo del/. W e noted that it is not meaningful to compare t w o\nlanguage mo dels with the p erplexit y measure if they di/\u000ber in their underlying sample spaces/.\nY et language mo dels are usually compared with the p erplexit y measure/, ev en though they\ndo sometimes di/\u000ber in their sample spaces due to di/\u000beren t v o cabularies or due to di/\u000beren t\nw a ys of dealing with unkno wn w ords/. One w a y to solv e this problem is to agree on a common\nv o cabulary /. But if this is not p ossible/, w e prop ose to use the adjusted p erplexit y measure\nAP P /. It is a /\rexible w a y of making the results more comparable/, ev en if the underlying\nsample spaces are not iden tical/.\nW e then applied the idea of iden tifying w eaknesses of a language mo del to our bi/-p os\nmo del and rep orted the follo wing results/:\n/1/) W e iden ti/\fed three w eaknesses/, the prediction of w ords in a v ery small n um b er of\ncon texts/, the prediction of unkno wn w ords/, and the prediction of w ords giv en their\npart of sp eec h/. W e th us gained considerable additional insigh t in to the mo del/. This\ninsigh t is helpful in impro ving our mo del/, but it is also relev an t to other language\nmo dels/. The last w eakness/, in particular/, is imp ortan t with resp ect to the recen t\nin terest in probabilistic con text free grammars as language mo dels/. Ev en though\nprobabilistic con text free grammars migh t impro v e the prediction of the next part\nof sp eec h/, they are unlik ely to impro v e the prediction of the w ord giv en the part of\nsp eec h/. They therefore do not address this imp ortan t w eakness at all/.\n/2/) W e impro v ed one of the w eaknesses/, the prediction of unkno wn w ords/, b y in tro duc/-\ning a new mo del for unkno wn w ords/. This lead to an impro v emen t in the mo del/'s\np erformance ranging b et w een /1/4/% and /2/1/%/.\n/3/) In order to address the third w eakness/, the prediction of w ords giv en their part of\nsp eec h/, w e dev elop ed the generalized N /-p os mo del/. It can incorp orate linguistic in/-\nformation/, ev en if it extends o v er man y w ords/. Also/, the information used for the\nprediction of the w ord giv en the part of sp eec h in this mo del can b e di/\u000beren t from the\ninformation used to predict the part of sp eec h/. It is imp ortan t that the mo del allo ws\nfor this b ecause ev en though the immediate con text /(e/.g/. the t w o or three preceding\nCHAPTER /4/. ANAL YZING AND IMPR O VING A BI/-POS LANGUA GE MODEL /8/8\nw ords/) con tains a lot of information ab out the part of sp eec h of the next w ord/, w e can\nargue that useful seman tic information that restricts the actual w ord ma y b e further\na w a y /.\nWith these results/, w e ha v e sho wn the usefulness of our de/\fnition of w eaknesses and of\nanalyzing w eaknesses of a language mo dels in general/.\nChapter /5\nAdding Linguistic Kno wledge to\nLanguage Mo dels\nIn the previous c hapter/, one of the w eaknesses of our mo del prompted the dev elopmen t of\nthe generalized N /-p os mo del as a framew ork that can incorp orate kno wledge in to language\nmo dels/. In this c hapter/, w e will address the issue of adding kno wledge in more general\nterms/. W e b egin b y giving reasons for w an ting to add kno wledge to language mo dels /(section\n/5/./1/)/. W e then dev elop criteria to select kno wledge that will b e useful for a language mo del\n/(section /5/./2/)/. W e conclude this c hapter b y reviewing metho ds for com bining di/\u000beren t t yp es\nof kno wledge in a language mo del /(section /5/./3/)/.\n/5/./1 Reasons for Adding Linguistic Kno wledge to Language\nMo dels\nWh y do w e w an t to add more kno wledge to language mo dels than curren t mo dels con tain/?\nW e see the follo wing three reasons for attempting to add kno wledge/. First/, w e w ould lik e to\nimpro v e the p erformance of existing mo dels/. Our hop e is that adding kno wledge will lead\nto impro v ed p erformance/. Second/, adding more kno wledge ma y w ell b ecome a necessit y\nin the future/. Curren t sp eec h recognizers ac hiev e an acceptable recognition rate partly\nb ecause they w ork in a constrained domain with a limited v o cabulary /. As w e mo v e to more\ngeneral domains with larger v o cabularies/, the complexit y of the recognition task and the\nn um b er of acoustically confusable alternativ es increases/. A language mo del incorp orating\n/8/9\nCHAPTER /5/. ADDING LINGUISTIC KNO WLEDGE TO LANGUA GE MODELS /9/0\na broader range of linguistic kno wledge sources could rule out man y of these h yp otheses/,\nthereb y helping to cop e with the additional complexit y /. Third/, adding kno wledge is more\nsatisfying than stic king to existing mo dels on psyc hological grounds b ecause h umans seem to\nuse other kno wledge to predict a w ord than the kno wledge used b y curren t mo dels/, namely\nthe immediately preceding t w o or three w ords/. /\\This do es not mak e sense/!/\" is a reaction\nw e often ha v e when w e lo ok at the sen tence a sp eec h recognizer deems the most lik ely to b e\nsp ok en/. Sub ject and v erb do not agree/, prep ositions are not where they should b e/, the v erb\nis missing en tirely or a certain com bination of w ords is seman tically incorrect/. Tw o examples\nof suc h ob viously incorrect sen tences are sho wn b elo w/. They are tak en from a recognition\nsession of the SPHINX system /(see /[/9/5 /, p/./1/6/1/,p/./1/6/5/]/) in whic h a bi/-gram language mo del\nis used/. These utterances w ere from the resource managemen t task /(/[/1/1/8 /]/)/, whic h uses a\nconstrained syn tax for inquiring ab out na v al resources/. F or b oth examples/, w e will giv e the\nutterance sp ok en as w ell as the recognized sen tence/:\nCorrect/: Is the economic sp eed of apalac hicola less than that of the brunswic k\nBi/-gram/: Whose economic sp eec h of apalac hicola less than that of the brunswic k\nCorrect/: On what da y could dubuque arriv e in p ort at his maxim um sustained sp eed\nBi/-gram/: What w ould it tak e dubuque arriv e in p ort at his maxim um sustained sp eed\nIn these examples and in man y other cases/, h umans seem to notice almost without e/\u000bort the\nconstrain ts violated b y the prop osed output/. Adding these constrain ts to a sp eec h recognizer\nis therefore a v ery natural and tempting endea v our/.\nOnce w e ha v e decided to add linguistic kno wledge to a language mo del/, t w o questions\ncome to mind/. First/, what kno wledge should w e try to add/? Second/, ho w can w e com bine\nthe di/\u000beren t t yp es of kno wledge in a language mo del/? In the follo wing t w o sections/, w e will\ndeal with eac h question in turn/.\n/1\n/5/./2 What Kno wledge Should W e Add/?\nIn linguistics/, kno wledge ab out language can b e divided in to phonetics/, phonology /, proso dy /,\nmorphology /, syn tax/, seman tics and pragmatics/. The acoustic mo del of a sp eec h recognizer\ncaptures some of the phonological kno wledge and the part unique to sp eec h recognition/, the\nsignal pro cessing/. Morphology /, syn tax/, seman tics and pragmatics could all b e included in\nthe language mo del/. Therefore/, from an abstract p oin t of view/, there is a v ery wide range\n/1\nThe related question of ho w to obtain and to enco de the kno wledge is addressed in the /\frst section/.\nCHAPTER /5/. ADDING LINGUISTIC KNO WLEDGE TO LANGUA GE MODELS /9/1\nT yp e of T ask/-dep enden t Con v ersation/-dep enden t Sp eak er/-dep enden t Analysis/-dep enden t\nkno wledge kno wledge kno wledge kno wledge kno wledge\nPragmatic and A priori seman tic Concept subselection Psyc hological Concept subselection\nSeman tic kno wledge ab out the based on con v ersation mo del of the user based on partial\ntask domain sen tence recognition\nSyn tactic Grammar for the Grammar subselection Grammar subselection Grammar subselection\nlanguage based on topic based on sp eak er based on partial\nphrase recognition\nLexical Size and confusabilit y V o cabulary sub/- V o cabulary sub/- V o cabulary subselection\nof the v o cabulary selection based selection and ordering based on segmen tal\non topic based on sp eak er features\npreference\nPhonemic and Characteristics of Con textual Dialectal v ariations Phonemic subselection\nphonetic phones and phonemes v ariabilit y in of the sp eak er based on segmen tal\nof the language phonemic c haracter/- features\nistics\nP arametric and A priori kno wledge Adaptiv e noise V ariations resulting P arameter trac king\nacoustic ab out the transducer normalization from the size and based on previous\nc haracteristics shap e of v o cal tract parameters\nT able /5/./1/: Di/\u000beren t T yp es of Kno wledge /(tak en from Reddy and New ell/)\nof kno wledge that w e could incorp orate in to a language mo del/. Whic h kno wledge should\nw e add/?\nF or sp eec h recognition/, the kno wledge has b een classi/\fed in /[/1/2/0 /] according to t w o di/-\nmensions/: the lev el describ ed b y the kno wledge /(e/.g/. parametric/, phonemic/, lexical/, etc/./)\nand its range of v alidit y across di/\u000beren t situations /(e/.g/. a priori kno wledge/, task dep enden t\nkno wledge/, con v ersation dep enden t etc/./)/. This classi/\fcation is sho wn in table /5/./1/. One can\nsee that most of the kno wledge of the t w o lo w er ro ws /(e/.g/. parametric and phonemic/) is\ncaptured/, at least partially /, b y the acoustic mo del /(e/.g hidden Mark o v mo del/)/. But all the\nother t yp es of kno wledge could p oten tially b e useful for a language mo del/. Whic h kno wledge\nshould w e try to add/?\nT o address this question/, w e will /\frst presen t p ossible criteria for selecting useful kno wl/-\nedge /(section /5/./2/./1/)/. Then/, to giv e a structure to the space of p ossibly useful kno wledge/,\nw e will prop ose a classi/\fcation of p ossibly useful kno wledge /(section /5/./2/./2/)/. Finally /, w e will\nsho w for a concrete example ho w the criteria from section /5/./2/./1 can b e used to decide ab out\nits usefulness /(section /5/./2/./3/)/.\n/5/./2/./1 Criteria for Selecting Useful Kno wledge\nRather than giving a necessarily incomplete list of useful kno wledge/, w e will discuss some\ncriteria that w e think can b e used to iden tify useful kno wledge/. F or eac h criterion/, w e also\nsuggest ho w w e can ev aluate a giv en t yp e of kno wledge with resp ect to the criterion/:\n/1/) Restrictions of P ossible Choices/. The kno wledge m ust frequen tly b e able to restrict\nthe c hoice of w ords in a sen tence/. If this is not the case/, it will not impro v e the\nCHAPTER /5/. ADDING LINGUISTIC KNO WLEDGE TO LANGUA GE MODELS /9/2\nqualit y of the language mo del/, at least not with resp ect to the standard measure\nused to ev aluate language mo dels /(see section /3/./2/./1/)/. In order to /\fnd out whether a\ngiv en t yp e of kno wledge restricts the p ossible c hoices of w ords/, w e can for example\nuse in trosp ection/. Do h umans often seem to use the kno wledge to restrict the c hoice\nof w ords Do es the kno wledge create strong exp ectations ab out the w ords to come/?\nHo w ev er/, w e ha v e to b e careful in using in trosp ection/. A p oin t often made b y Jelinek\nand others /(see for example /[/8/5 /]/) is that in tuitiv e judgmen ts ha v e often b een misleading\nin the area of language mo dels/. It is alw a ys imp ortan t to v erify these in tuitions with\nreal data and to ha v e the parameters of the mo del b e trained rather than determined b y\nhand/. W e can formalize the restriction of p ossible c hoices using information theory /.\nThis is for example done in /[/1/2/2 /] to de/\fne the strength of the selection restriction\nb et w een predicates and argumen ts in terms of relativ e en trop y /. W e can use the same\nmetho d to measure the exten t to whic h a giv en t yp e of kno wledge restricts a w ord\nthat o ccurs later/.\nLet X denote a random v ariable enco ding the kno wledge under in v estigation and\nranging o v er the set f x\n/1\n/; /:/:/:/; x\nm\ng /. X can for example enco de the fact that the sub ject\nof the curren t sen tence is animate or not/. Let Y denote another random v ariable\nenco ding the iden tit y of the w ord that is b eing restricted b y X and ranging o v er the\nset f y\n/1\n/; /:/:/:/; y\nl\ng /. Y can for example enco de the iden tit y of the v erb in the sen tence/, in\nwhic h case the set of p ossible v alues w ould b e the set of all p ossible v erbs/. W e can\nno w measure the restriction X imp oses on Y b y lo oking at the di/\u000berence b et w een the\nprior distribution p /( Y /) and the p osterior distribution p /( Y j x\ni\n/)/. An appropriate w a y\nof measuring this di/\u000berence is to use the relativ e en trop y D /( p /( Y j x\ni\n/) /; p /( Y /)/)/, whic h is\nde/\fned as\nD /( p /( Y j x\ni\n/) /; p /( Y /)/) /=\nX\ny\nj\np /( y\nj\nj x\ni\n/) l og\np /( y\nj\nj x\ni\n/)\np /( y\nj\n/)\n/: /(/5/./1/)\nIf w e rewrite equation /5/./1 as\nD /( p /( Y j x\ni\n/) /; p /( Y /)/) /=\nX\ny\nj\np /( y\nj\nj x\ni\n/)/[ l og\n/1\np /( y\nj\n/)\n/BnZr l og\n/1\np /( y\nj\nj x\ni\n/)\n/] /; /(/5/./2/)\nw e can see that the relativ e en trop y measures in fact the a v erage di/\u000berence in infor/-\nmation /(see section /3/./2/./2/) pro vided b y the t w o distributions p /( Y /) and p /( Y j x\ni\n/)/. In\nfact/, the relativ e en trop y is a measure for the amoun t of information pro vided ab out\nthe random v ariable Y /(the w ord that will b e restricted/) b y an ev en t X /= x\ni\n/(the\nCHAPTER /5/. ADDING LINGUISTIC KNO WLEDGE TO LANGUA GE MODELS /9/3\nobserv ation of one v alue for a t yp e of kno wledge/)/. As men tioned in /[/1/2/2 /, p/./5/8/]/, it is\ndiscussed in /[/1/3/9 /] wh y this measure is the appropriate one to use in this case/.\nGiv en the measure of relativ e en trop y /, w e can no w quan tify ho w useful a certain t yp e\nof kno wledge X is for the prediction of some w ords Y /.\n/2 /3\n/2/) Error Analysis/. There is not m uc h p oin t in adding kno wledge that can impro v e the\nprediction of w ords that almost nev er o ccur or that only accoun t for a tin y fraction\nof the o v erall p erformance measure/. In order to /\fnd out whether a giv en t yp e of\nkno wledge has a signi/\fcan t impact on the o v erall qualit y /, w e can p erform the follo wing\nsteps/. First iden tify the w ord whose prediction will b e impro v ed b y the kno wledge/.\nSecond/, use the tec hnique of error analysis to determine the p ercen tage of the LT P\n/(see section /3/./3/) these w ords cause/. If this fraction is not signi/\fcan t then there is\nnot m uc h p oin t in adding this kno wledge/. This p oin t is separate from criterion /1/)\nfor the follo wing reason/. Ev en if a t yp e of kno wledge X con tains a lot of information\nab out Y /, the o v erall e/\u000bect this has on the p erformance of the language mo del ma y\nb e insigni/\fcan t /(see for example our exp erimen ts with the generalized N /-p os mo del\nin section /4/./4/./2/)/. F or example/, the gender of the sub ject and ob ject in the previous\nsen tence ma y ha v e a signi/\fcan t impact on the c hoice of pronouns in the subsequen t\nsen tence /(e/.g/. he/, she/, it/)/. Ho w ev er/, if the LT P caused b y all the pronouns is v ery\nsmall then impro ving the prediction will not lead to a signi/\fcan t o v erall impro v emen t/.\n/3/) Computational E/\u000bort/. The language mo dels discussed in this w ork /(see section /2/./3/)\nare used together with the acoustic mo del to narro w do wn the searc h space/. There/-\nfore/, man y thousands of h yp otheses need to b e ev aluated in a v ery short time/. This\nplaces sev ere computational restrictions on the kind of analysis that can b e used/. F or\nexample/, giv en the time required for parsing unrestricted English sen tences using the\ncurren t tec hnology /, it seems unlik ely that the language mo del could use a full parser\nfor unrestricted text/. Ho w ev er/, it is p ossible that future w ork/, for example in the area\n/2\nIt w ould b e v ery in teresting to measure/, for example/, whether the amoun t of information a grammar\npro vides ab out the part of sp eec h of the next w ord is signi/\fcan tl y higher than the amoun t of information\npro vided b y the immediately preceding parts of sp eec h/. This could help to explain wh y N /-p os or N /-gram\nmo dels are so v ery useful for language mo dels and whether a parser has the p oten tial to impro v e on this/.\n/3\nIn order to decide the usefulness of a giv en t yp e of kno wledge/, w e can also measure the p erformance of\nthe distribution p /( Y j x\ni\n/) on a testing text/. The di/\u000berence in the P P or LT P b et w een the distribution p /( Y /)\nand p /( Y j x\ni\n/) will b e a quan titativ e measure for ho w useful the kno wledge X is/. Ho w ev er/, this means that\nw e actually ha v e to implemen t the kno wledge X /, but w e are lo oking for criteria to select useful kno wledge\nsources b efore implemen ting the more promising ones/.\nCHAPTER /5/. ADDING LINGUISTIC KNO WLEDGE TO LANGUA GE MODELS /9/4\nof neural net w orks or partial parsing/, migh t c hange this situation /(see also p/. /1/0/4/)/.\nIn order to determine the computational e/\u000bort required for a giv en t yp e of kno wl/-\nedge/, w e can use the standard tec hniques of analyzing the computational complexit y\nof algorithms /(see for example /[/5/3 /, c hapter /1/2/]/)/, suc h as the theoretical w orst time\ncomplexit y of an algorithm/. Moreo v er/, if the giv en t yp e of kno wledge is also used in\nother areas /(e/.g/. natural language pro cessing/)/, w e can also use the actual time and\nspace requiremen ts of an algorithm as it is rep orted in the literature/.\n/4/) Kno wledge Acquisition and Co ding/. F or a general purp ose language mo del/, it is\nimp ortan t that the kno wledge can b e acquired and co ded for the use with unrestricted\ntext/. It ma y b e p ossible to describ e some kno wledge /(e/.g/. syn tax/) in terms of rules\nacquired b y hand/. F or others /(e/.g/. seman tics and pragmatics/)/, this migh t b e v ery\ndi/\u000ecult/.\nAs an example/, consider the restrictions/, often called selectional restrictions/, a v erb\nimp oses on its direct ob jects/, e/.g/. /`to drink X/'/. One w a y of capturing this w ould b e\nto organize ob jects in a hierarc h y of t yp es/. W e could imagine a t yp e corresp onding to\nall drink able ob jects and w e could then ha v e the restriction that the direct ob ject of\n/`drink/' b elongs to that t yp e/. As p oin ted out in /[/1/9 /]/, there are t w o main problems with\nresp ect to the task at hand/. First/, these t yp e hierarc hies are /\\large/, complicated and\nexp ensiv e to acquire b y hand/\"/. Moreo v er/, attempts at acquiring them automatically\nha v e b een only partially successful/. /\\Y et without a comprehensiv e hierarc h y /, it is\ndi/\u000ecult to use suc h classi/\fcations in the pro cessing of unrestricted text/\"/. Second/,\nev en if w e had these t yp e hierarc hies/, this w ould not b e su/\u000ecien t to predict patterns\nof usage in man y cases/. Ev en though p ean uts and p otato es ma y b e v ery similar and\ntherefore quite close to eac h other in the hierarc h y /(b oth are edible fo o ds that gro w\nunderground/)/, one t ypically /`bak es p otato es/' and /`roasts p ean uts/'/. A distribution/-\nbased analysis could capture these di/\u000berences automatically and /\\promises to do b etter\nat least on these t w o problems/\"/.\nOn the other hand/, suc h a hierarc h y can allo w generalizations that ma y b e hard to\ndescrib e with a distributional analysis/. In the ab o v e example/, w e migh t b e able to\ndisco v er from a few data p oin ts /(e/.g/. drink cola/, drink b eer/, /./././) that all direct ob jects\nof /`to drink/' b elong to the class of liquids or drink ables/. W e can then generalize the\nfact that the w ords actually seen are lik ely ob jects of /`to drink/' for all elemen ts of\nCHAPTER /5/. ADDING LINGUISTIC KNO WLEDGE TO LANGUA GE MODELS /9/5\nthis class/. With a distributional analysis/, man y data p oin ts w ould b e needed for eac h\nelemen t of that class in order to obtain the same e/\u000bect/. This is b ecause it can/'t\np erform generalizations based on seman tically meaningful classes/.\nT o /\fnd out whether a particular t yp e of kno wledge can b e acquired and used with\nunrestricted text/, w e can either lo ok through the existing literature or try to decide\non the issue ourselv es/.\n/5/./2/./2 Classi/\fcation of P ossibly Useful Kno wledge\nIn the previous section/, w e sa w di/\u000beren t criteria for judging the usefulness of a particular\nt yp e of kno wledge/. But to whic h t yp es of kno wledge are w e going to apply these criteria/?\nT o all the p oten tial sources of kno wledge/, e/.g/. morphology /, syn tax etc/./, men tioned on page\n/9/0/? T o help answ er this question/, w e will construct a classi/\fcation of all p ossible sources\nof kno wledge in this section/. Giv en this classi/\fcation/, w e can then k eep trac k of where the\ndi/\u000beren t t yp es of kno wledge fall in the classi/\fcation/, whic h parts of the classi/\fcation ha v e\nb een tried already and w e can construct a men tal image of the space of p ossible t yp es of\nkno wledge/.\nAccording to whic h measure are w e going to classify our p ossible t yp es of kno wledge/?\nThe most imp ortan t criterion that w e will use to select useful kno wledge is the /\frst one giv en\nin section /5/./2/./1/, the restriction of p ossible c hoices/. If a t yp e of kno wledge is not restricting\nthe c hoice of p ossible w ords/, it is not going to b e useful for our task/. An y kno wledge that w e\nw ould w an t to consider therefore m ust ha v e the prop ert y of restricting the c hoice of w ords/.\nTh us/, w e will classify all p ossible t yp es of kno wledge with resp ect to this prop ert y /, namely\naccording to the distance b et w een the origin of the kno wledge and the w ord it restricts/.\nThis guaran tees that our classi/\fcation will con tain all p ossibly useful t yp es of kno wledge/.\nMoreo v er/, w e also /\fnd it in tuitiv ely app ealing/. W e will no w giv e the resulting classi/\fcation/.\nF or eac h class/, w e will describ e an example of kno wledge falling in to the class/. F urthermore/,\nw e will men tion whether this class of kno wledge has b een used for language mo deling/:\n/1/) Kno wledge whose constrain ts do not extend across sen tences/. One example is gram/-\nmatical kno wledge/. Curren t language mo dels almost exclusiv ely use kno wledge from\nthis class/. Most often/, the restriction of grammaticalit y is appro ximated b y the con/-\nstrain ts pro vided b y w ords from the immediate con text/, e/.g/. the t w o or three preced/-\ning w ords/. Judging from the success of tec hniques using this immediate con text /, this\nCHAPTER /5/. ADDING LINGUISTIC KNO WLEDGE TO LANGUA GE MODELS /9/6\nm ust pro vide quite p o w erful constrain ts/, esp ecially for /\fxed w ord order languages lik e\nEnglish/.\n/2/) Kno wledge whose constrain ts extend across sen tences but remain within paragraphs/.\nOne example is the kno wledge of the gender of a noun phrase/. F or example/, the noun\nphrase /\\Mr/. Bak er/\" can not b e referred to as /\\she/\" in a subsequen t sen tence/. Recen tly\nused dynamic language mo dels /(see section /2/./5/./5/) capture the fact that w ords are more\nlik ely to app ear again if they app eared b efore in the curren t paragraph/.\n/3/) Kno wledge whose constrain ts extend across paragraphs but not across do cumen ts/.\nOne example is the kno wledge of the topic or con ten t of the curren t paragraph/. With\nresp ect to uses for language mo dels/, the remark of the previous class also applies here/.\n/4/) Kno wledge whose constrain ts extend across texts/. One example is the kno wledge of\na certain kind of v o cabulary or st yle of writing/. It has b een sho wn /(see for example\n/[/1/0/5 /]/, /[/6/8 /]/, /[/6/9 /]/)/, that the language used in di/\u000beren t genres is quite di/\u000beren t and this\nis actually used for a language mo del in /[/7/9 /]/.\n/5/./2/./3 The Usefulness of Collo cational Constrain ts\nIn the follo wing/, w e will apply the ab o v e /\fv e criteria to iden tify useful kno wledge from\nsection /5/./2/./1 to the kno wledge ab out collo cational constrain ts/. W e will use the term collo/-\ncation /\\quite broadly to include constrain ts on SV O /(sub ject v erb ob ject/) triples/, phrasal\nv erbs/, comp ound noun phrases/, and psyc holinguistic notions of w ords asso ciation /(e/.g/. do c/-\ntor//n urse/)/\" as suggested in /[/1/9 /, p/./7/5/]/. W e b egin b y reviewing w ork that suggests that\ncollo cational constrain ts are v ery frequen t and imp ortan t in language/. This is tak en as\nevidence that criteria one and t w o /(see section /5/./2/./1/) are partly satis/\fed/. Ho w ev er/, it do es\nnot replace a study that actually measures this e/\u000bect quan titativ ely as suggested in section\n/5/./2/./1/.\nIn /[/7/6 /]/, G/. Kjellmer classi/\fes com binations of w ords according to ho w m uc h v ariabilit y\nthey allo w/. The sp ectrum ranges from fossilized phrases /(Anno Domini/, aurora b orealis/) to\nsemi/-fossilized phrases /(b y and b y /, b y and large/, Ac hilles heel/, Ac hilles tendon/) or idioms\n/(ha v e a w eak//soft sp ot for/, do badly//w ell for/) to /\fnally v ariable phrases /(glass of w ater/, go\nto college/, his approac h to/, to b e app oin ted b y/)/. /\\So an y one who can b e said to b e pro/\fcien t\nin a language has command of a great n um b er of set phrases as w ell as skill in pro ducing\nCHAPTER /5/. ADDING LINGUISTIC KNO WLEDGE TO LANGUA GE MODELS /9/7\nacceptable v arian ts within the limits dra wn up b y the selectional rules/\" /(/[/7/6 /, p/./1/2/6/]/)/.\nIn /[/7/7 /]/, w ords are analyzed with resp ect to their tendency to form clusters/. /\\There is\na con tin uum in English w ords /(including names/)/, from those whose con textual compan y\nis en tirely predictable /(Angeles/, Fidel/) to those whose con textual compan y is en tirely un/-\npredictable /(therefore/)/, but the evidence indicates that most of the w ords are to b e found\nto w ards the Angeles end of the scale/\" /(/[/7/7 /, p/./1/7/2/]/)/.\n/4\nIn /[/2 /]/, a study of the phraseology of sp ok en English is motiv ated and presen ted/. /\\The\nnativ e sp eak er/'s abilit y to sp eak /\ruen tly and idiomatically can th us b e ascrib ed to his\ncommand of a great n um b er of suc h preassem bled phrases/. This means that linguistic\ncomp etence m ust include a large and imp ortan t phraseological comp onen t /././. whic h acts as\nan elastic link b et w een the lexicon and the pro ductiv e rules of grammar/\" /(/[/2 /, p/./3/]/)/.\nIn /[/1/3/6 /]/, the nature of lexical items and their relation to grammar is examined/, and t w o\nprinciples of in terpretations are stated in order to explain ho w meaning arises from text/:\n/1/) Principle of Choice/. A t eac h completion of a unit/, a c hoice op ens up and the only\nconstrain t is grammaticalit y /.\n/2/) Idiom Principle/. A language user has a large n um b er of semi/-preconstructed phrases\nthat constitute a single c hoice/, ev en though it in v olv es more w ords/.\nIt is argued that the second principle is at least as imp ortan t as the /\frst/, and one of the\nreasons for this is the follo wing/. It is noted that the more frequen t a w ord is/, the less w ell\nde/\fned its meaning b ecomes/. F or the most frequen t w ords/, w e are in fact talking ab out\nusages rather than meanings/. This is called progressiv e delexicalization/. Most normal text\nis made up of frequen t w ords and of the frequen t senses of less frequen t w ords/. This sho ws\nthat normal text is often delexicalized and it sho ws the use of the idiom principle/.\nIn /[/1/2/1 /]/, /`framew orks/' are prop osed in order to explain language patterning/. It is argued\nthat framew orks are v ery pro ductiv e and therefore deserv e closer examination/. The exam/-\nined framew orks are discon tin uous but do not extend o v er more than three w ords and can\ntherefore b e captured in a traditional N/-gram mo del/. Nev ertheless/, the idea of a framew ork\ncould b e extended to capture constructs that extend across this lo cal con text and could\ntherefore not b e captured b y a N /-gram approac h/.\n/4\nEv en though the immediate con text can b e captured w ell with for example a tri/-gram mo del/, it requires\nenormous amoun ts of data/. Moreo v er/, restrictions that extend o v er more than the t w o preceding w ords are\nnot captured/.\nCHAPTER /5/. ADDING LINGUISTIC KNO WLEDGE TO LANGUA GE MODELS /9/8\nIn /[/1/9/]/, the usefulness of collo cational constrain ts for natural language parsers is ex/-\namined/. The constrain ts pro vided b y syn tax as opp osed to collo cations are describ ed as\nfollo ws/:\n/\\Syn tactic constrain ts/, b y themselv es/, though are probably not v ery imp or/-\ntan t/. An y psyc holinguist kno ws that the in/\ruence of syn tax lexical retriev al is\nso subtle that y ou ha v e to con trol v ery carefully for all the factors that really\nmatter /(e/.g/./, w ord frequency /, w ord asso ciation norms/, etc/./)/. On the other hand/,\ncollo cational factors /(w ord asso ciations/) dominate syn tactic ones so m uc h that\ny ou can easily measure the in/\ruence of w ord frequency and w ord asso ciation\nnorms on lexical retriev al without careful con trols for syn tax/\" /(/[/1/9 /, pp/./7/9/-/8/0/]/)/.\nHo w ev er/, syn tax ma y b e necessary to capture stronger constrain ts/. /\\W e b e/-\nliev e that syn tax will ultimately b e a v ery imp ortan t source of constrain t/, but\nin a more indirect w a y /. As w e ha v e b een suggesting/, the real constrain ts will\ncome from w ord frequencies and collo cational constrain ts/, but these questions\nwill probably need to b e brok en out b y syn tactic con text/. Ho w lik ely is it for\nthis noun to conjoin with that noun/? Is this noun a t ypical sub ject of that v erb/?\nAnd so on/. In this w a y /, syn tax pla ys a crucial role in pro viding the relev an t\nrepresen tation for expressing these v ery imp ortan t constrain ts/, but crucially /, it\ndo es not pro vide v ery m uc h useful constrain t /(in the information theoretic sense/)\nall b y itself/./\" /(/[/1/9 /, p/./8/0/]/)\nIn /[/1/2/2 /]/, the notion of selectional restriction /(see page /9/4/) is formalized/. This is ac hiev ed\nb y using an information/-theoretic measure and it leads to the follo wing in terpretation of\nselection constrain ts/: /\\the strength of a predicate/'s selection for an argumen t is iden ti/\fed\nwith the quan tit y of information it carries ab out that argumen t/\" /(/[/1/2/2 /, p/.iv/]/)/. This allo ws\nus to measure quan titativ ely the strength of a selectional restriction/. Using a man ually con/-\nstructed hierarc h y of w ords /(/[/1/0/7 /]/, /[/8 /]/)/, this notion of selection restriction is used to p erform\nsyn tactic disam biguation of prep ositional phases/, co ordination and nominal comp ounds/.\nThe information in selectional restriction m ust therefore b e strong enough to p erform this\ndisam biguation/.\nHa ving seen evidence that collo cational constrain ts satisfy the /\frst t w o criteria of section\n/5/./2/./1/, w e no w brie/\ry address the third and fourth criterion/.\nWith resp ect to the third criterion/, computational e/\u000eciency /, w e note that since some of\nCHAPTER /5/. ADDING LINGUISTIC KNO WLEDGE TO LANGUA GE MODELS /9/9\nthe collo cational constrain ts require the iden ti/\fcation of sub ject/, v erb and ob ject/, a parser\nseems to b e needed/. As p oin ted out in section /5/./2/./1/, it is v ery unlik ely that w e will b e\nable to use a full parser for the kind of language mo del under consideration in this thesis/.\nHo w ev er/, with recen t progress in the area of partial parsing /(see /[/1/4/1 /]/, /[/5/0 /]/, /[/5/1 /]/, /[/2/6 /]/) it\nseems p ossible to get parts of a parse with far less computational e/\u000bort/. These parts of the\nparse could b e su/\u000ecien t for our purp ose/. Alternativ ely /, recen t w ork in the area of neural\nnet w orks /(see /[/5/9 /]/) migh t b e extended to pro vide an e/\u000ecien t solution in the future/.\nThe fourth criterion is the acquisition and co ding of the kno wledge for use with unre/-\nstricted text/. Collo cational constrain ts also p ose serious problems in that resp ect/. Progress/,\nfor example/, in the automatic acquisition of sub categorization frames /(see /[/1/2 /] and /[/1/0/1 /]/)\nor in the a v ailabilit y of fully parsed corp ora /(see /[/1/0/2 /]/)/, could again b e su/\u000ecien t to acquire\nand co de the kno wledge of collo cational constrain ts/.\nIn the ligh t of the evidence presen ted ab o v e/, w e b eliev e that collo cational constrain ts\nare a go o d candidate to b e included in a language mo del/. Ho w ev er/, w e suggest a further\nin v estigation of the exten t to whic h collo cation constrain ts that can not b e captured in the\nstandard N /-gram mo del/, quan titativ ely satisfy criteria one and t w o of section /5/./2/./1/.\n/5/./3 Ho w Can W e Com bine Useful Kno wledge in a Language\nMo del/?\nIn the last section/, w e sa w ho w w e could go ab out selecting useful kno wledge/. Once w e ha v e\ndecided on what kno wledge is useful/, w e ha v e to determine ho w to pro duce a probabilit y\ndistribution that dep ends on the c hosen kno wledge in a meaningful manner/. In section\n/5/./3/./1/, w e will presen t some traditional approac hes to this problem/. In section /5/./3/./2/, w e will\ndescrib e a v ery successful metho d that w as prop osed recen tly /.\n/5/./3/./1 T raditional Approac hes for Com bining Kno wledge in a Language\nMo del\nIn Hearsa y I I /(/[/3/3 /]/)/, a t ypical example of kno wledge based sp eec h recognition/, di/\u000beren t\nt yp es of kno wledge are com bined using a blac kb oard/, a dynamic global data structure/.\nDi/\u000beren t mo dules/, corresp onding to di/\u000beren t t yp es of kno wledge/, generate h yp otheses/, write\nh yp otheses on to the blac kb oard/, and ev aluate the plausibili t y of h yp otheses found on the\nCHAPTER /5/. ADDING LINGUISTIC KNO WLEDGE TO LANGUA GE MODELS /1/0/0\nblac kb oard/. This arc hitecture is used to com bine t yp es of kno wledge at di/\u000beren t lev els/, e/.g/.\nphoneme/, w ord and sen tence/, whic h are not necessarily represen ted in terms of probabilit y\ndistributions/. Ho w ev er/, in the case of the language mo del/, w e only need to com bine di/\u000beren t\nexisting probabilit y distributions and this do es not require the complicated async hronous\narc hitecture of a blac kb oard/. In the follo wing/, w e will presen t some simple mec hanisms to\ncom bine di/\u000beren t probabilit y distributions/.\nW e will enco de the ev en t to b e predicted /(e/.g/. the next w ord/) and the kno wledge used\n/(e/.g/. the preceding w ord/, the state of a parser/) in terms of v ariables and their v alues/. Let\nY denote the ev en t to b e predicted with a set of p ossible v alues f y\n/1\n/; /:/:/:/; y\nn\ng /. F ollo wing the\nterminology of decision trees /(see section /2/./5/./4/) and follo wing the sp eci/\fc//general example of\nthe generalized N/-p os mo del /(see section /4/./4/)/, w e will denote the kno wledge b y v ariables X\ni\nwith corresp onding v alues f x\ni /1\n/; /:/:/:/; x\niv\ni\ng /. W e will fo cus on the com bination of t w o v ariables\nX\n/1\nand X\n/2\n/, but the concepts can b e extended in a straigh t forw ard manner to sev eral\nv ariables/.\n/1/) Join t distribution/. F or eac h pair /( X\n/1\n/= x\n/1 i\n/; X\n/2\n/= x\n/2 j\n/)/, this metho ds estimates a\nseparate distribution from frequency data/.\n/2/) Decision trees/. This metho d estimates a separate distribution for a pair /( X\n/1\n/=\nx\n/1 i\n/; X\n/2\n/= x\n/2 j\n/) only if this signi/\fcan tly impro v es the qualit y of the mo del/.\n/3/) Deleted in terp olation/. This metho d constructs a separate distribution for eac h pair\n/( X\n/1\n/= x\n/1 i\n/; X\n/2\n/= x\n/2 j\n/)/, but not from frequency data of the join t distribution /(as metho d\none/)/. Instead/, it com bines the distributions of eac h v ariable according to /\u0015\nij\np /( Y /=\ny\nl\nj X\n/1\n/= x\n/1 i\n/) /+ /(/1 /BnZr /\u0015\nij\n/) p /( Y /= y\nl\nj X\n/2\n/= x\n/2 j\n/) /: F or eac h pair /( X\n/1\n/= x\n/1 i\n/; X\n/2\n/= x\n/2 j\n/)/, the\n/\u0015\nij\nis estimated to optimize some criterion/.\nW e will no w illustrate these three metho ds b y giving the probabilit y distributions eac h\nmetho d w ould calculate in an example/. In this example/, the v ariable Y whic h w e need to\npredict only has t w o v alues Y /2 f N /; R g /. Y indicates whether the next w ord is a noun\n/(v alue N /) or not /(v alue R /, R standing for rest/)/. The v ariable X\n/1\nalso has t w o v alues\nX\n/1\n/2 f Ad j/; R g /. X\n/1\nindicates whether the previous w ord w as an adjectiv e /(v alue Ad j /) or\nnot /(v alue R /)/. Similarly /, the v ariable X\n/2\nhas the t w o v alues X\n/2\n/2 f Ar t/; R g and it indicates\nwhether the second last w ord w as an article /(v alue Ar t /) or not /(v alue R /)/. The v alues of the\nprobabilit y distributions in this example w ere calculated from real data/, namely from our\nCHAPTER /5/. ADDING LINGUISTIC KNO WLEDGE TO LANGUA GE MODELS /1/0/1\nX\n/1\nX\n/2\np /( Y /= N /) p /( Y /= R /) distr/. name\nAdj Art /0/./8/4 /0/./1/6 p\n/1\nAdj R /0/./6/5 /0/./3/5 p\n/2\nR Art /0/./2/6 /0/./7/4 p\n/3\nR R /0/./2/5 /0/./7/5 p\n/4\nT able /5/./2/: The join t distribution for Y giv en X\n/1\nand X\n/2\nX\n/1\np /( Y /= N /) p /( Y /= R /)\nAdj /0/./7/2 /0/./2/8\nR /0/./2/5 /0/./7/5\nT able /5/./3/: The distribution for Y giv en X\n/1\ntraining text of /5/0/,/0/0/0 w ords/. The o v erall distribution for Y /, indep enden t of the v ariables\nX\n/1\nand X\n/2\nis p /( Y /= N /) /= /0 /: /2/8 and p /( Y /= R /) /= /0 /: /7/2/. In other w ords/, /2/8/% of the w ords in\nthe text are nouns/.\n/1/) Join t distribution/. This metho d directly samples the join t distribution and th us esti/-\nmates a separate distribution for the four com bination of v alues of X\n/1\nand X\n/2\n/. The\nresulting distributions are sho wn in table /5/./2/.\n/2/) Decision trees/. This metho d will estimate a separate distribution for a pair of v alues\nof X\n/1\nand X\n/2\nonly if the resulting distribution is signi/\fcan tly di/\u000beren t/. As w e can\nsee from table /5/./2/, the v alue of X\n/1\nhas a bigger impact on the distribution than the\nv alue of X\n/2\n/. Since v arying X\n/1\nwhile k eeping X\n/2\n/\fxed results in a large v ariation in\nprobabilities/, the decision tree metho d therefore estimates a separate distribution for\nthe t w o di/\u000beren t v alues of X\n/1\n/. These t w o resulting distributions are sho wn in table\n/5/./3/. F urthermore/, w e can see from table /5/./2/, that the distribution for X\n/1\n/= R do es not\nc hange signi/\fcan tly for the t w o v alues of X\n/2\n/. Ho w ev er/, the distribution for X\n/1\n/= Ad j\ndo es c hange signi/\fcan tly dep ending on the v alue of X\n/2\n/. The decision tree metho d\ncould therefore split the /\frst distribution in table /5/./3 in to t w o distributions/, leading\nto a total of three distributions sho wn in table /5/./4/.\n/3/) Deleted in terp olation/. This metho d com bines the t w o distributions of eac h v ariable\nsho wn in table /5/./5 to get four distributions p\n/0\n/1\n/; p\n/0\n/2\n/; p\n/0\n/3\nand p\n/0\n/4\nthat are used as appro xi/-\nmations of the four join t distributions sho wn in table /5/./2/. The in terp olation is linear\nCHAPTER /5/. ADDING LINGUISTIC KNO WLEDGE TO LANGUA GE MODELS /1/0/2\nX\n/1\nX\n/2\np /( Y /= N /) p /( Y /= R /)\nAdj Art /0/./8/4 /0/./1/6\nAdj R /0/./6/5 /0/./3/5\nR /(R or Art/) /0/./2/5 /0/./7/5\nT able /5/./4/: The distributions for Y giv en X\n/1\nand X\n/2\nconstructed b y the decision tree\nX\n/1\np /( Y /= N /) p /( Y /= R /) distr/. name\nAdj /0/./7/2 /0/./2/8 p\n/1 a\nR /0/./2/5 /0/./7/5 p\n/1 b\nX\n/2\np /( Y /= N /) p /( Y /= R /) distr/. name\nArt /0/./4/1 /0/./5/9 p\n/2 a\nR /0/./2/7 /0/./7/3 p\n/2 b\nT able /5/./5/: The distribution for Y giv en X\n/1\nand for Y giv en X\n/2\naccording to the four parameters /\u0015\naa\n/; /\u0015\nab\n/; /\u0015\nba\nand /\u0015\nbb\n/:\np\n/0\n/1\n/= /\u0015\naa\n/\u0003 p\n/1 a\n/+ /(/1 /BnZr /\u0015\naa\n/) /\u0003 p\n/2 a\np\n/0\n/2\n/= /\u0015\nab\n/\u0003 p\n/1 a\n/+ /(/1 /BnZr /\u0015\nab\n/) /\u0003 p\n/2 b\np\n/0\n/3\n/= /\u0015\nba\n/\u0003 p\n/1 b\n/+ /(/1 /BnZr /\u0015\nba\n/) /\u0003 p\n/2 a\np\n/0\n/4\n/= /\u0015\nbb\n/\u0003 p\n/1 b\n/+ /(/1 /BnZr /\u0015\nbb\n/) /\u0003 p\n/2 b\nSince v alue of X\n/1\nhas a bigger impact on the resulting distribution/, the v alues of the /\u0015 /'s\nwill tend to b e bigger than /0 /: /5/. W e will use the v alues /\u0015\naa\n/= /1 /; /\u0015\nab\n/= /0 /: /7/7 /; /\u0015\nba\n/= /0 /: /5\nand /\u0015\nbb\n/= /1 b ecause the resulting distributions sho wn in table /5/./6 are v ery go o d\nappro ximations to the join t distributions sho wn in table /5/./2/. In fact/, b y comparing\ntable /5/./2 and /5/./6/, w e can see that all appro ximations except for p\n/0\n/1\nare iden tical to the\njoin t distribution/.\nW e will no w discuss the adv an tages and disadv an tages of the three metho ds in general/.\ndistr/. name p /( Y /= N /) p /( Y /= R /)\np\n/0\n/1\n/0/./7/2 /0/./2/8\np\n/0\n/2\n/0/./6/5 /0/./3/5\np\n/0\n/3\n/0/./2/6 /0/./7/4\np\n/0\n/4\n/0/./2/5 /0/./7/5\nT able /5/./6/: The appro ximations to the join t distributions constructed b y deleted in terp olation\nCHAPTER /5/. ADDING LINGUISTIC KNO WLEDGE TO LANGUA GE MODELS /1/0/3\nOne adv an tage of the join t distribution metho d is its simplicit y /. It is v ery easy to im/-\nplemen t and it do es not require a lot of computational resources to construct the join t\ndistribution/. Moreo v er/, it directly samples the join t distribution/. This means that the\nmetho d will usually get closer to the true join t distribution than metho ds that try to ap/-\npro ximate it based on the comp onen t distributions /(e/.g/. the deleted in terp olation metho d/)/.\nOne big disadv an tage is the amoun t of data it requires/. F or eac h pair /( X\n/1\n/= x\n/1 i\n/; X\n/2\n/= x\n/2 j\n/)/,\nit tries to estimate the en tire distribution/. It therefore has to estimate j X\n/1\nj /\u0003 j X\n/2\nj /\u0003 j Y j\nprobabilities/. Esp ecially in the case of language mo dels/, where the n um b er of v alues for Y\nis v ery large/, this requires an enormous amoun t of data/. Consider for example/, the tri/-gram\nlanguage mo del presen ted in the review section /(section /2/./5/./2/)/. Y corresp onds to the next\nw ord/, X\n/1\nto the last w ord and X\n/2\nto the second last w ord/. It samples the join t distribution\ndirectly and ma y ha v e around /1/0\n/1/2\nparameters to estimate/. This requires man y million\nw ords of training text and ev en then/, the tri/-gram estimates are com bined with bi/-gram or\nuni/-gram estimates using the deleted in terp olation metho d/. Another disadv an tage of the\njoin t distribution metho d is that it constructs a separate distribution for all pairs/, ev en if\nsome of them will lead to v ery similar distributions/.\nThe main adv an tage of the decision tree metho d is that it only constructs the join t dis/-\ntribution for a pair /( X\n/1\n/= x\n/1 i\n/; X\n/2\n/= x\n/2 j\n/) if this will signi/\fcan tly impro v e the p erformance/.\nThis will usually result in few er distributions p erforming ab out as w ell as the join t distri/-\nbution metho d/. F or that reason/, the decision tree metho d can easily b e applied to man y\nv ariables at a time/. Whereas metho d one w ould not ha v e nearly enough data to sample the\no v erall join t distribution/, this metho d will only construct it at the p oin t where it results\nin a signi/\fcan t impro v emen t for the giv en data/. The main disadv an tage is the computa/-\ntional complexit y of the algorithm/. It is harder to implemen t and ma y require an enormous\namoun t of computation/. As stated in /[/1/6 /]/, the algorithm for /\fnding the optimal partitioning\nfor classi/\fcation and regression trees is exp onen tial in the n um b er of v alues of the v ariables\nX /. Ho w ev er/, a lo cally optimal partition can b e found in time linear in the n um b er of v alue\nfor X for eac h iteration/.\nOne of the adv an tages of the deleted in terp olation metho d is again its simplicit y /, making\nit easy to implemen t and fast to execute/. F urthermore/, it do es not require a lot of data\nb ecause it do es not attempt to directly sample the join t distribution/. Instead/, for eac h pair\n/( X\n/1\n/= x\n/1 i\n/; X\n/2\n/= x\n/2 j\n/)/, it appro ximates the join t distribution b y a linear com bination of the\nCHAPTER /5/. ADDING LINGUISTIC KNO WLEDGE TO LANGUA GE MODELS /1/0/4\nt w o comp onen ts/. F or eac h pair/, it therefore only has to estimate the one in terp olation pa/-\nrameter/. Its main disadv an tage is that the com bination of the t w o comp onen t distributions\nma y not get v ery close to the join t distributions/. In cases where there is enough data to\nsample the join t distribution directly /, this ma y lead to p o or p erformance/.\nAdding sev eral kno wledge sources to a language mo del is one instance of the general\ntrend to construct ric her probabilistic mo dels of language/. As p oin ted out in /[/1/2/2 /, p/./1/]/, this\napp ears to b e a trend in practical and theoretical w ork on language/. One example of this\ntrend is that the P enn T reebank is mo ving to w ards the annotation of text with predicate/-\nargumen t structure/, not only with surface linguistic structures /(see /[/1/0/2 /]/)/. Another is the\nuse of probabilistic mo dels for tagging /(see for example /[/1/8/]/, /[/2/8 /] and /[/9/0 /]/)/, parsing /(see for\nexample /[/9 /]/)/, and man y other applications/.\nHo w ev er/, one of the main problems with ric her probabilistic mo dels is the sparseness\nof data/. This is for example p oin ted out in /[/1/1/5 /, p/./1/8/3/]/: /\\It is w ell kno wn that a simple\ntabulation of frequencies of certain w ords participating in certain con/\fgurations/, for example\nof frequencies of pairs of a transitiv e main v erb and the head noun of its direct ob ject/, can\nnot b e reliably used for comparing the lik eliho o ds of di/\u000beren t alternativ e con/\fgurations/.\nThe problem is that for large enough corp ora/, the n um b er of p ossible join t ev en ts is m uc h\nlarge than the n um b er of ev en t o ccurrences in the corpus/, so man y ev en ts are seen rarely or\nnev er/, making their frequency coun ts unreliable estimates of their probabilities/\"/.\nF rom this p ersp ectiv e/, it seems unlik ely that w e will b e able to sample join t distributions\nfor man y di/\u000beren t t yp es of kno wledge/, ev en with ev er gro wing corp ora/. W e can therefore\nexp ect that approac hes that do not require this sampling /(lik e the deleted in terp olation\nmetho d/) will /\fnd widespread use/. One recen tly prop osed metho d/, that can in tegrate dif/-\nferen t t yp es of kno wledge/, will b e presen ted in the next section/. A second approac h/, whic h\nhas b een sho wn to in tegrate constrain ts from di/\u000beren t lev els on a smaller task /(see /[/7/2 /]/) /, is\nthe use of neural net w orks/. Their application to this task could b e the sub ject of a further\nstudy /.\n/5/./3/./2 The Maxim um En trop y Approac h to Com bining Kno wledge in a\nLanguage Mo del\nT rying to use di/\u000beren t probabilit y distributions to pro duce one com bined distribution is\na problem that app ears in man y tasks/. The maxim um en trop y principle from the area of\nstatistics /(see /[/2/5 /]/, /[/6/1 /]/) is a v ery general approac h to this problem/. Recen tly /, this approac h\nCHAPTER /5/. ADDING LINGUISTIC KNO WLEDGE TO LANGUA GE MODELS /1/0/5\nhas b een applied successfully to language mo deling /(see /[/9/3 /]/, /[/9/4 /]/, /[/1/2/6 /]/)/. A summary of this\nw ork is giv en b elo w/.\nThe maxim um en trop y approac h prop oses the follo wing t w o steps/:\n/1/) Rewrite the di/\u000beren t probabilit y estimates as constrain ts on the exp ectations of v arious\nfunctions/, that the com bined estimate has to satisfy /.\n/2/) F rom the set of all p ossible probabilit y distributions satisfying the constrain ts/, c ho ose\nthe one that has the highest en trop y /.\nSupp ose w e are trying to estimate a /(join t/) probabilit y function p /( X /= x /) /; x /= /( x\n/1\n/; /:/:/:/; x\nn\n/)/.\nUsing k constrain t functions f\ni\n/( x /) /; /1 /\u0014 i /\u0014 k /, w e can imp ose k constrain ts coming from dif/-\nferen t t yp es of kno wledge on the resulting distribution p /( X /= x /) in the follo wing manner/:\nX\nx\np /( X /= x /) f\ni\n/( x /) /= c\ni\n/; /1 /\u0014 i /\u0014 k /:\nAs an example/, if w e c ho ose\nf\ni\n/( x /) /=\n/8\n/<\n/:\n/1 if x /= x\n/0\n/0 else\n/;\nthe constrain t f\n/1\nimp oses that the v alue of p /( x\n/0\n/) is c\n/1\n/. In general/, f\ni\n/( x /) can of course b e of\na di/\u000beren t form th us allo wing more complicated constrain ts for p /( X /= x /)/. Giv en suc h a set\nof k consisten t constrain ts/, w e can then use the algorithm of generalized iterativ e scaling\n/(/[/2/5 /]/) to /\fnd the p /( X /= x /) satisfying these constrain ts that has the highest en trop y /. W e\ncan guaran tee that a unique function p /( X /= x /) exists and that it is of the form\nP /( X /= x /) /=\nY\ni\n/\u0016\nf /( x /)\ni\n/;\nwhere the /\u0016\ni\nare constan ts that ha v e to b e found b y the generalized iterativ e scaling algo/-\nrithm/.\nF or a mo del that com bines the distribution obtained b y the maxim um en trop y approac h\nwith a standard tri/-gram mo del/, a reduction in p erplexit y of ab out /3/0/% compared to the\nstandard tri/-gram mo del w as ac hiev ed/.\nThe adv an tages of the maxim um en trop y approac h are\n/1/) The maxim um en trop y principle is simple/, in tuitiv ely app ealing/, imp oses all of the\ngiv en constrain ts and do es not assume an ything else/.\nCHAPTER /5/. ADDING LINGUISTIC KNO WLEDGE TO LANGUA GE MODELS /1/0/6\n/2/) The maxim um en trop y principle is extremely general/. It can b e used for an y conceiv/-\nable linguistic or statistical phenomenon/.\n/3/) Information captured b y traditional language mo dels can b e incorp orated in to the\nmaxim um en trop y principle/.\n/4/) The generalized iterativ e scaling algorithm can b e adapted incremen tally /, th us allo wing\nthe addition of new constrain ts or the relaxation of old ones/.\n/5/) The generalized iterativ e scaling algorithm is guaran teed to con v erge to the unique\nsolution under assumptions that can b e met in practice/.\nThe w eaknesses of the maxim um en trop y approac h are\n/1/) The generalized iterativ e scaling algorithm is computationally v ery exp ensiv e\n/5\n/. F or\nthe exp erimen ts describ ed in /[/1/2/6 /]/, the algorithm ran in parallel on an a v erage of /1/5\nhigh p erformance w orkstations and it to ok three w eeks to complete/.\n/2/) Ev en though the algorithm is guaran teed to con v erge/, w e do not ha v e a theoretical\nb ound on its con v ergence rate/.\n/3/) When w e add constrain ts that are not satis/\fed b y the training data /{ and this is\nsometimes desirable /{ the theoretical results guaran teeing existence/, uniqueness and\ncon v ergence of the algorithm ma y not hold/.\nNev ertheless/, w e can see from the results presen ted in /[/1/2/6 /] that this is one of the\nrare times a standard tri/-gram mo del is outp erformed signi/\fcan tly and consisten tly /. This\napproac h therefore holds considerable promise for future w ork/.\n/5/./4 Summ ary\nIn this c hapter/, w e motiv ated the addition of kno wledge to language mo dels/, dev elop ed\ndi/\u000beren t criteria for iden tifying useful kno wledge/, and presen ted metho ds for com bining\nkno wledge in a language mo del/.\nW e b egan b y p oin ting out three reasons for w an ting to add kno wledge to a language\nmo del/. First/, w e w ould lik e to impro v e the mo del/'s p erformance/. Second/, if w e apply\n/5\nEv en though only the training part/, whic h can b e done o/\u000b line/, is exp ensiv e/, its computational complexit y\nma y prev en t the application of the algorithm to v ery large data sets for reasons of practicalit y /.\nCHAPTER /5/. ADDING LINGUISTIC KNO WLEDGE TO LANGUA GE MODELS /1/0/7\ncurren t sp eec h recognition tec hnology to more complex tasks than the ones tac kled to da y /,\nthe n um b er of acoustically confusable h yp otheses will increase/, and w e will need a b etter\nlanguage mo del in order to deal with this am biguit y /. Third/, adding kno wledge is more\nsatisfying than stic king to existing mo dels on psyc hological grounds b ecause h umans seem\nto use kno wledge to predict a w ord other than the kno wledge used in curren t mo dels/, namely\nthe immediately preceding t w o or three w ords/. Hence/, there is clearly a need for a language\nmo del whic h incorp orates more linguistic kno wledge/.\nOnce w e had decided to add kno wledge to a language mo del/, the follo wing t w o questions\ncame to mind/. First/, what kno wledge should w e add/, and second/, ho w can w e c ombine\ndi/\u000beren t t yp es of kno wledge in a language mo del/. W e addressed b oth issues in turn/.\nRather than trying to giv e a necessarily incomplete list of t yp es of kno wledge that\nw e should add/, w e presen ted four criteria that w e think should b e used to iden tify useful\nkno wledge/. First/, the kno wledge should restrict the n um b er of p ossible w ords/, otherwise\nit is not going to help in solving our task/. Second/, it should b e applicable often enough\nto b e of statistical signi/\fcance/. Third/, it should b e p ossible computationally to use this\nkno wledge in real time sp eec h recognition/. Finally /, w e should b e able to acquire and co de\nthis kno wledge for use with unrestricted text/. Moreo v er/, w e dev elop ed a classi/\fcation of\np ossibly useful kno wledge and applied the criteria for iden tifying useful kno wledge to one\nkind of kno wledge/, that promises to b e useful to impro v e language mo dels in general/.\nW e then mo v ed on to the issue of com bining di/\u000beren t kno wledge in a language mo del/. W e\npresen ted three metho ds for com bining kno wledge and dev elop ed some of the adv an tages and\ndisadv an tages w e see in eac h metho d/. F ollo wing that/, w e concluded that it is v ery unlik ely\nthat w e will ha v e enough data to estimate distributions that dep end on sev eral kno wledge\nsources directly /, ev en with the a v ailabilit y of increasingly large corp ora/. Therefore/, w e think\nthat metho ds that com bine distributions from single kno wledge sources in a meaningful\nfashion will b e v ery useful and require further in v estigation/. One metho d sho wn to b e v ery\nuseful in recen t w ork is the maxim um en trop y principle and it sho ws great promise for future\nw ork/.\nChapter /6\nSumm a ry of Results and F uture\nW ork\nThe main con tribution of this thesis is the idea of applying error analysis to language\nmo dels/. W e de/\fne what w e mean b y error or w eakness of a language mo del and w e sho w\nho w an analysis of w eaknesses is useful in impro ving a concrete mo del/. Th us/, in addition to\nthe concrete results w e obtained/, w e ha v e sho wn ho w one can go ab out impro ving language\nmo dels in general/. W e could therefore call this /\\meta language mo deling/\"/. In the remainder\nof this c hapter/, w e will giv e a more detailed summary of our results /(section /6/./1/)/. This will\nb e follo w ed b y p ossible extensions to our w ork /(section /6/./2/)/.\n/6/./1 Summ ary of Results\nIn this thesis/, w e set out to impro v e existing language mo dels for sp eec h recognition/. Since\nit is a widely accepted fact that kno wing the errors or w eaknesses of an y kind of mo del\nmak es it easier to impro v e the mo del/, w e prop osed to p erform an analysis of the w eaknesses\nof language mo dels/. W e de/\fned in general terms what w e mean b y a w eakness of a language\nmo del and analyzed the w eaknesses of a particular/, commonly used mo del/. This analysis\nled/, among other things/, to an impro v emen t in the mo del/'s p erformance ranging b et w een\n/1/4/% and /2/1/%/. This sho ws/, in a concrete case/, the usefulness of p erforming an analysis of\nw eaknesses of a language mo del/.\nIn order to analyze the w eaknesses of a language mo del/, w e /\frst needed to de/\fne what\n/1/0/8\nCHAPTER /6/. SUMMAR Y OF RESUL TS AND FUTURE W ORK /1/0/9\nw e mean b y a w eakness of a language mo del/. Giv en a part of the mo del/, w e measured its\nimpact on the o v erall p erformance of the mo del in terms of the p ercen tage of the L TP /, a\nmeasure closely related to the standard p erplexit y measure used to ev aluate the language\nmo del p erformance/. W e then de/\fned a w eakness of a mo del as a part of the mo del that has\na big impact on the o v erall p erformance/. Do es this de/\fnition conform to the in tuitions w e\nha v e ab out the w ord w eakness/? In tuitiv ely /, a w eakness should b e something that needs to\nb e impro v ed/. Giv en our de/\fnition/, w eaknesses cause a considerable fraction of the o v erall\np erformance measure and this means that impro ving them is imp ortan t for the o v erall\np erformance/. Our de/\fnition therefore agrees with our in tuition/. F urthermore/, the de/\fnition\nis directly applicable to almost all curren tly used language mo dels /(except probabilistic\ncon text free grammars/) and the calculations in v olv ed in iden tifying w eaknesses are v ery\nstraigh t forw ard/.\nIn order to sho w the usefulness of our de/\fnition of w eakness and of the analysis of\nw eaknesses in general/, w e p erformed this analysis on a commonly used bi/-p os language\nmo del/. W e c hose a corpus and a mo del for our w ork and v eri/\fed that our mo del is w ell\ntrained with the amoun t of training data w e use/. This led to the dev elopmen t of the\nadjusted p erplexit y measure AP P /, whic h ga v e us a /\rexible w a y of making results of mo dels\nwith di/\u000beren t sample spaces more comparable/. W e then analyzed the w eaknesses of our\nmo del giving the follo wing results/:\n/1/) W e iden ti/\fed three w eaknesses/, the prediction of w ords in a v ery small n um b er of\ncon texts/, the prediction of unkno wn w ords/, and the prediction of w ords giv en their\npart of sp eec h/. W e th us gained considerable additional insigh t in to the mo del/. This\ninsigh t is helpful in impro ving our mo del/, but it is also relev an t to other language\nmo dels/. The last w eakness in particular is imp ortan t with resp ect to the recen t in terest\nin probabilistic con text free grammars as language mo dels/. Ev en though probabilistic\ncon text free grammars migh t impro v e the prediction of the next part of sp eec h/, they\nare unlik ely to impro v e the prediction of the w ord giv en the part of sp eec h/. They\ntherefore do not address this imp ortan t w eakness at all/.\n/2/) W e impro v ed one of the w eaknesses/, the prediction of unkno wn w ords/, b y in tro ducing\na new mo deling for unkno wn w ords/. This leads to an impro v emen t of the mo del/'s\np erformance ranging b et w een /1/4/% and /2/1/%/.\n/3/) In order to address the third w eakness/, the prediction of w ords giv en their part of\nCHAPTER /6/. SUMMAR Y OF RESUL TS AND FUTURE W ORK /1/1/0\nsp eec h/, w e dev elop ed the generalized N /-p os mo del/. It can incorp orate linguistic infor/-\nmation/, ev en if it extends o v er man y w ords/, and the information used for the prediction\nof the w ord giv en the part of sp eec h in this mo del can b e di/\u000beren t from the informa/-\ntion used to predict the part of sp eec h/. It is imp ortan t that the mo del allo ws for this\nb ecause ev en though the immediate con text /(e/.g/. the t w o or three preceding w ords/)\ncon tains a lot of information ab out the part of sp eec h of the next w ord/, w e can argue\nthat useful seman tic information that restricts the actual w ord ma y b e further a w a y /.\n/4/) Our w ork/, in particular the generalized N /-p os mo del/, led us to the follo wing questions\nab out language mo dels in general/:\na/) what kno wledge should w e add to language mo dels in order to impro v e their\np erformance/?\nb/) ho w can w e com bine di/\u000beren t t yp es of kno wledge in a language mo del/?\nT o help answ er the /\frst question/, w e dev elop ed four criteria useful in deciding whether\na giv en t yp e of kno wledge is useful/. Rather than ha ving to implemen t all p ossible t yp es\nof kno wledge/, w e can th us select the more promising ones/. With resp ect to the second\nquestion/, w e presen ted and ev aluated some existing tec hniques that can b e used for\nthis task/.\n/6/./2 F uture W ork\nThe most immediate extension to our w ork is to try to impro v e the bi/-p os language mo del\nwith resp ect to the w eaknesses iden ti/\fed in section /4/./3/./1/. What information w ould w e need\nin order to decide on the tag that will follo w a noun/? Do es this kno wledge satisfy the criteria\nset out in section /5/./2/./1/? One p ossibilit y migh t b e to divide all the nouns in to t w o groups\n/(e/.g/. t w o tags/) /{ the nouns that are often follo w ed b y other nouns and the ones that are\nnot/.\nAnother ob vious extension is to apply the idea of analyzing w eaknesses of a language\nmo del to other existing mo dels/. This can b e done for N /-gram/, N /-p os/, decision tree based\nor cac he based mo dels/. F urthermore/, the idea is applicable to other languages /(e/.g/. F renc h/,\nGerman/, Japanese etc/./) as w ell as to language mo dels that are not based on the w ord lev el\n/(e/.g/. syllable/, phoneme/, etc/./)/. F or eac h mo del/, this can sho w whether their w eaknesses\nCHAPTER /6/. SUMMAR Y OF RESUL TS AND FUTURE W ORK /1/1/1\nare similar to the w eaknesses of our mo del and where the analysis of w eaknesses leads to\nimpro v emen t in these mo dels/. This extension should b e v ery straigh t forw ard and w e do\nnot an ticipate an y di/\u000eculties/.\nF urthermore/, w e could apply the criteria for judging the usefulness of one t yp e of kno wl/-\nedge to selectional restrictions/, a t yp e of kno wledge iden ti/\fed as p oten tially useful for lan/-\nguage mo dels/. W e could then decide whether it is w orth incorp orating selectional restriction\nin to language mo dels and whether w e can exp ect a signi/\fcan t impro v emen t in p erformance\nb y doing so/. T o this end/, w e migh t b e able to use the same data as in /[/1/2/2 /]/. Alternativ ely /,\nw e could use a sto c hastic tagger and a v ery primitiv e heuristic to iden tify /, for example/,\nv erbs and their direct ob jects/. This w ould allo w us to extract the necessary data from an y\ntext/, rather than b eing restricted to a fully parsed corpus/, and this w ould mak e the whole\npro cess v ery /\rexible/.\nMoreo v er/, w e could p erform a systematic study of the usefulness of di/\u000beren t t yp es of\nkno wledge /(e/.g/. phonological/, proso dic/, syn tactic/, seman tic/) for a language mo del/. Quan ti/-\ntativ e results of suc h a study w ould b e v ery v aluable to the researc h comm unit y b ecause it\ncould help in steering its future researc h e/\u000borts/. In order to p erform this kind of study /, w e\nw ould require a corpus annotated with man y di/\u000beren t kinds of information/. Dep ending on\nthe required amoun t of data/, this could b e hard to /\fnd at the momen t/, but w e think that\nit will surely b ecome a v ailable in the future/.\nBesides language mo deling for sp eec h recognition/, w e can also apply the idea of analyzing\nw eaknesses of probabilistic language mo dels to other areas that use N /-gram statistics/. One\nexample is handwriting and optical c haracter recognition/. Analyzing the w eaknesses of the\nmo dels and impro ving them afterw ards could lead to an impro v emen t in p erformance in\nthese areas/.\nApp endix A\nSample T ext\nIn this section/, w e giv e a sample of section A/0/1 of the LOB corpus/. Eac h item in the text\nis made up of t w o parts joined b y an underscore /(/` /'/)/. The /\frst part is the w ord itself /(for\nexample /`a/'/)/, and the second part is the tag asso ciated with this o ccurrence of the w ord\n/(for example /`A T/' for article/)/. An y item that has these t w o parts is part of the text and\nneeds to b e predicted b y the language mo del/. These items include quotes/, commas/, colons/,\nand other punctuation marks/. The tags are listed and explained brie/\ry in App endix B/. F or\nmore details on the form of the corpus see /[/7/1/]/)/.\nA/0/1 /2 /^ /*/'/_/*/' stop/_VB electing /_VB G life/_NN peers/_NNS /*/*/'/_/*/*/' /./_/.\nA/0/1 /3 /^ by/_IN Trevor/_NP Williams/_ NP /./_/.\nA/0/1 /4 /^ a/_AT move/_NN to/_TO stop/_VB /\\/0Mr/_NPT Gaitskell /_NP from/_IN\nA/0/1 /4 nominatin g/_ VBG any/_DTI more/_AP labour/_NN\nA/0/1 /5 life/_NN peers/_NNS is/_BEZ to/_TO be/_BE made/_VBN at/_IN a/_AT meeting/_N N\nA/0/1 /5 of/_IN labour/_NN /\\/0MPs/_NP TS tomorrow/_N R /./_/.\nA/0/1 /6 /^ /\\/0Mr/_NPT Michael/_NP Foot/_NP has/_HVZ put/_VBN down/_RP a/_AT\nA/0/1 /6 resolutio n/_ NN on/_IN the/_ATI subject/_N N and/_CC\nA/0/1 /7 he/_PP/3A is/_BEZ to/_TO be/_BE backed/_VBN by/_IN /\\/0Mr/_NPT Will/_NP\nA/0/1 /7 Griffiths /_N P /,/_/, /\\/0MP/_NPT for/_IN Manchester /_N P\nA/0/1 /8 Exchange/_ NP /./_/.\nA/0/1 /9 /^ though/_CS they/_PP/3A S may/_MD gather/_VB some/_DTI left/-wing /_J JB\nA/0/1 /9 support/_N N /,/_/, a/_AT large/_JJ majority /_NN\nA/0/1 /1/0 of/_IN labour/_NN /\\/0MPs/_NP TS are/_BER likely/_JJ to/_TO turn/_VB down/_RP\nA/0/1 /1/0 the/_ATI Foot/-Grif fi ths /_N P\nA/0/1 /1/1 resolutio n/_ NN /./_/.\nA/0/1 /1/2 /^ /*/'/_/*/' abolish/_V B Lords/_NPTS /*/*/'/_/*/*/' /./_/.\nA/0/1 /1/3 /^ /\\/0Mr/_NPT Foot/'s/_NP/$ line/_NN will/_MD be/_BE that/_CS as/_CS labour/_NN\n/1/1/2\nAPPENDIX A/. SAMPLE TEXT /1/1/3\nA/0/1 /1/3 /\\/0MPs/_NPT S opposed/_VBD the/_ATI\nA/0/1 /1/4 governmen t/_ NN bill/_NN which/_WDTR brought/_VB D life/_NN peers/_NNS into/_IN\nA/0/1 /1/4 existence /_N N /,/_/, they/_PP/3AS should/_MD\nA/0/1 /1/5 not/_XNOT now/_RN put/_VB forward/_R B nominees/_ NN S /./_/.\nA/0/1 /1/6 /^ he/_PP/3A believes/_ VB Z that/_CS the/_ATI House/_NP L of/_IN Lords/_NPTS\nA/0/1 /1/6 should/_MD be/_BE abolishe d/_V BN and/_CC that/_CS\nA/0/1 /1/7 labour/_NN should/_MD not/_XNOT take/_VB any/_DTI steps/_NNS which/_WDTR\nA/0/1 /1/7 would/_MD appear/_VB to/_TO /*/'/_/*/' prop/_VB up/_RP /*/*/'/_/*/*/' an/_AT\nA/0/1 /1/8 out/-dated /_J J instituti on /_NN /./_/.\nA/0/1 /1/9 /^ since/_IN /1/9/5/8/_CD /,/_/, /1/3/_CD labour/_NN life/_NN peers/_NNS and/_CC\nA/0/1 /1/9 peeresses /_N NS have/_HV been/_BEN created/_V BN /./_/.\nA/0/1 /2/0 /^ most/_AP labour/_NN sentiment /_NN would/_MD still/_RB favour/_VB the/_ATI\nA/0/1 /2/0 abolition /_N N of/_IN the/_ATI\nA/0/1 /2/1 House/_NPL of/_IN Lords/_NP TS /,/_/, but/_CC while/_CS it/_PP/3 remains/_V BZ\nA/0/1 /2/1 labour/_NN has/_HVZ to/_TO have/_HV an/_AT adequate/_J J\nA/0/1 /2/2 number/_NN of/_IN members/_ NNS /./_/.\nA/0/1 /2/3 /^ Africans/_ NNP S drop/_VB rivalry/_NN to/_TO fight/_VB Sir/_NPT Roy/_NP /./_/.\nA/0/1 /2/4 /^ by/_IN Dennis/_NP Newson/_NP /./_/.\nA/0/1 /2/5 /^ the/_ATI two/_CD rival/_JJB African/_JN P national ist /_J J parties/_N NS\nA/0/1 /2/5 of/_IN Northern/_NP Rhodesia/_ NP\nA/0/1 /2/6 have/_HV agreed/_VB N to/_TO get/_VB together/_RB to/_TO face/_VB the/_ATI\nA/0/1 /2/6 challenge /_N N from/_IN Sir/_NPT Roy/_NP\nA/0/1 /2/7 Welensky/_ NP /,/_/, the/_ATI federal/_JJ Premier/_NP T /./_/.\nA/0/1 /2/8 /^ delegates /_NN S from/_IN /\\/0Mr/_NPT Kenneth/_N P Kaunda/'s /_NP /$ united/_J J\nA/0/1 /2/8 national/_ JJ\nA/0/1 /2/9 independe nc e/_N N party/_NN /(/_/( /2/8/0/,/0/0/0/_ CD members/_NN S /)/_/) and/_CC /\\/0Mr/_NPT\nA/0/1 /2/9 Harry/_NP Nkumbula/'s /_NP /$\nA/0/1 /3/0 African/_J NP national/_J J congress/_NN /(/_/( /4/0/0/,/0/0/0/_CD /)/_/) will/_MD meet/_VB\nA/0/1 /3/0 in/_IN London/_NP today/_NR to/_TO\nA/0/1 /3/1 discuss/_V B a/_AT common/_J J course/_NN of/_IN action/_N N /./_/.\nA/0/1 /3/2 /^ Sir/_NPT Roy/_NP is/_BEZ violently/_ RB opposed/_VB N to/_IN Africans/_N NPS\nA/0/1 /3/2 getting/_V BG an/_AT elected/_J J\nA/0/1 /3/3 majority/_ NN in/_IN Northern/_ NP Rhodesia/_N P /,/_/, but/_CC the/_ATI\nA/0/1 /3/3 colonial/_ JJ Secretary/_ NP T /,/_/, /\\/0Mr/_NPT Iain/_NP\nA/0/1 /3/4 Macleod/_N P /,/_/, is/_BEZ insisting/_ VBG on/_IN a/_AT policy/_N N of/_IN\nA/0/1 /3/4 change/_NN /./_/.\nA/0/1 /3/5 /^ sir/_NN Roy/'s/_NP/$ united/_JJ federal/_JJ party/_NN is/_BEZ\nA/0/1 /3/5 boycottin g/_ VBG the/_ATI London/_NP talks/_NNS on/_IN\nA/0/1 /3/6 the/_ATI protector at e/'s /_N N/$ future/_NN /./_/.\nA/0/1 /3/7 /^ said/_VBD /\\/0Mr/_NPT Nkumbula/_ NP last/_AP night/_NN /:/_/: /^ /*/'/_/*/'\nAPPENDIX A/. SAMPLE TEXT /1/1/4\nA/0/1 /3/7 we/_PP/1AS want/_VB to/_TO discuss/_V B what/_WDT to/_TO do/_DO\nA/0/1 /3/8 if/_CS the/_ATI British/_JN P governmen t/_ NN gives/_VBZ in/_RP to/_IN Sir/_NPT\nA/0/1 /3/8 Roy/_NP and/_CC the/_ATI talks/_NNS fall/_VB\nA/0/1 /3/9 through/_R P /./_/. /^ there/_EX are/_BER bound/_VBN to/_TO be/_BE\nA/0/1 /3/9 demonstra ti ons /_NN S /./_/. /*/*/'/_/*/*/'\nA/0/1 /4/0 /^ all/_ABN revealed/_ VBN /./_/.\nA/0/1 /4/1 /^ yesterday /_NR Sir/_NPT Roy/'s/_NP/$ chief/_JJB aide/_NN /,/_/, /\\/0Mr/_NPT\nA/0/1 /4/1 Julius/_NP Greenfiel d/_N P /,/_/,\nA/0/1 /4/2 telephone d/_ VBD his/_PP/$ chief/_NN a/_AT report/_NN on/_IN his/_PP/$ talks/_NNS\nA/0/1 /4/2 with/_IN /\\/0Mr/_NPT Macmillan/_ NP at/_IN\nA/0/1 /4/3 Chequers/_ NP /./_/.\nA/0/1 /4/4 /^ /\\/0Mr/_NPT Macleod/_NP went/_VBD on/_RP with/_IN the/_ATI conference /_N N\nA/0/1 /4/4 at/_IN Lancaster/_N P House/_NPL\nA/0/1 /4/5 despite/_I N the/_ATI crisis/_NN which/_WD TR had/_HVD blown/_VBN up/_RP /./_/. /^\nA/0/1 /4/5 he/_PP/3A has/_HVZ now/_RN revealed/_ VBN his/_PP/$ full/_JJ\nA/0/1 /4/6 plans/_NNS to/_IN the/_ATI Africans/_NN PS and/_CC liberals/_N NS attending/_ VBG\nA/0/1 /4/6 /./_/.\nA/0/1 /4/7 /^ these/_DTS plans/_NNS do/_DO not/_XNOT give/_VB the/_ATI Africans/_N NP S\nA/0/1 /4/7 the/_ATI overall/_J JB majority/_N N they/_PP/3AS\nA/0/1 /4/8 are/_BER seeking/_V BG /./_/. /^ African/_J NP delegates/_ NN S are/_BER\nA/0/1 /4/8 studying/_ VB G them/_PP/3O S today/_NR /./_/.\nA/0/1 /4/9 /^ the/_ATI conferenc e/_ NN will/_MD meet/_VB to/_TO discuss/_V B the/_ATI\nA/0/1 /4/9 function/_ NN of/_IN a/_AT proposed/_ JJ\nA/0/1 /5/0 House/_NPL of/_IN Chiefs/_N PTS /./_/.\nApp endix B\nThe F our T agsets Used in Our\nExp erimen ts\nThe four tag sets used in our exp erimen ts are repro duced here/. The /\frst column con tains\nthe tag set as part of the LOB corpus distribution/. The second/, third/, fourth and /\ffth\ncolumn corresp ond to the tag sets with /1/3/5/, /8/8/, /4/2 and /2/4 tags eac h/. They w ere mostly\npro duced b y joining tags with the same pre/\fx and are th us dep enden t on the original set/.\nLOB /1/3/5 tags /8/8 tags /4/2 tags /2/4 tags example or explanation\n/! /! /! /! PU exclamation mark\n/&F O /&F O /&F O /&F O F form ula\n/&FW /&FW /&FW /&FW F foreign w ord\n/( /) /) /) BR left brac k et\n/) /( /( /( BR righ t brac k et\n/' /*/' /*/' /*/' BR b egin quote\n/*/' /*/*/' /*/*/' /*/*/' BR end quote\n/- /*/- /*/- /*/- PU dash\n/, /, /, /, PU comma\n/|/{ /|/{ /|/{ /|/{ PU dash\n/. /. /. /. PU full stop\n/././. /././. /././. /././. PU ellipsis\n/: /: /: /: PU colon\n/; /; /; /; PU semicolon\n/? /? /? /? PU question mark\nABL ABL AB AB A pre/-quali/\fer /(quite/)\nABN ABN AB AB A pre/-quan ti/\fer /(all/)\nABX ABX AB AB A double conjunction /(b oth/)\nAP AP AP AP A p ost/-determiner /(few/)\n/1/1/5\nAPPENDIX B/. THE F OUR T A GSETS USED IN OUR EXPERIMENTS /1/1/6\nAP/$ AP/$ AP AP A other/'s\nAPS APS APS AP A others\nAPS/$ APS/$ APS AP A others/'\nA T A T A T A T A T singular article /(a/)\nA TI A TI A T A T A T singular or plural article /(the/)\nBE BE BE BE BE b e\nBED BED BED BE BE w ere\nBEDZ BEDZ BEDZ BE BE w as\nBEG BEG BEG BE BE b eing\nBEM BM BEM BE BE am\nBEN BEN BEN BE BE b een\nBER BER BER BE BE are\nBEZ BEZ BEZ BE BE is\nCC CC CC CC C co/-ordinating conjunction /(and/)\nCD CD CD CD CD cardinal /(/2/)\nCD/$ CD/$ CD CD CD cardinal with genitiv e\nCD/-CD CD/-CD CD CD CD h yphenated pair of cardinals\nCD/1 CD/1 CD/1 CD CD one\nCD/1/$ CD/1/$ CD/1 CD CD one/'s\nCD/1S CD/1S CD/1 CD CD ones\nCDS CDS CD CD CD cardinal with plural /(tens/)\nCS CS CS CS C sub ordinating conjunction /(after/)\nDO DO DO DO DO do\nDOD DOD DOD DO DO did\nDOZ DOZ DOZ DO DO do es\nDT DT DT DT DT singular determiner /(another/)\nDT/$ DT/$ DT DT DT singular determiner with genitiv e\nDTI DTI DTI DT DT singular or plural determiner /(an y/)\nDTS DTS DTS DT DT plural determiner /(these/)\nDTX DTX DTX DT DT double conjunction /(either/)\nEX EX EX EX EX existen tial there\nHV HV HV HV HV ha v e\nHVD HVD HVD HV HV had\nHV G HV G HV G HV HV ha ving\nHVN HVN HVN HV HV past participle /(had/)\nHVZ HVZ HVZ HV HV has\nIN IN IN IN IN prep osition /(ab out/)\nJJ JJ JJ JJ JJ adjectiv e\nJJB JJB JJB JJ JJ attributiv e/-only adjectiv e /(c hief /)\nJJR JJR JJR JJ JJ comparativ e adjectiv e\nAPPENDIX B/. THE F OUR T A GSETS USED IN OUR EXPERIMENTS /1/1/7\nJJT JJT JJT JJ JJ sup erlativ e adjectiv e\nJNP JNP JJP JJ JJ adjectiv e with w ord/-initial capital /(English/)\nMD MD MD MD MD mo dal auxiliary /(can/)\nNC NC NC NC F cited w ord\nNN NN NN N N singular common noun\nNN/$ NN/$ NN N N singular common noun with genitiv e\nNNP NNP NNP N N sing/. c/. noun w/. w ord/-initial capital /(English/)\nNNP/$ NNP/$ NNP N N same as ab o v e with genitiv e\nNNPS NNPS NNP N N plural common noun with w ord/-initial capital\nNNPS/$ NNPS/$ NNP N N same as ab o v e with genitiv e\nNNS NNS NNS N N plural common noun\nNNS/$ NNS/$ NNS N N same as ab o v e with genitiv e\nNNU NNU NNU N N abbr/. unit of meas/. unmark ed for n um b er /(hr/)\nNNUS NNUS NNU N N abbreviated unit of measuremen t\nNP NP NP NP N singular prop er noun\nNP/$ NP/$ NP NP N same as ab o v e with genitiv e\nNPL NPL NPL NP N sing/. lo cativ e noun w/. w ord/-initial cap/. /(Abb ey/)\nNPL/$ NPL/$ NPL NP N same as ab o v e with genitiv e\nNPLS NPLS NPL NP N plural lo cativ e noun with w ord/-initial capital\nNPLS/$ NPLS/$ NPL NP N same as ab o v e with genitiv e\nNPS NPS NPS NP N plural prop er noun\nNPS/$ NPS/$ NPS NP N same as ab o v e with genitiv e\nNPT NPT NPT NP N sing/. titular noun w/. w ord/-initial cap/. /(Arc h bishop/)\nNPT/$ NPT/$ NPT NP N same as ab o v e with genitiv e\nNPTS NPTS NPT NP N plural titular noun with w ord/-initial capital\nNPTS/$ NPTS/$ NPT NP N same as ab o v e with genitiv e\nNR NR NR NR N singular adv erbial noun /(Jan uary/)\nNR/$ NR/$ NR NR N same as ab o v e with genitiv e\nNRS NRS NRS NR N plural adv erbial noun\nNRS/$ NRS/$ NRS NR N same as ab o v e with genitiv e\nOD OD OD OD OD ordinal /(/1st/)\nOD/$ OD/$ OD OD OD same as ab o v e with genitiv e\nPN PN PN P P nominal pronoun /(an yb o dy/)\nPN/$ PN/$ PN P P same as ab o v e with genitiv e\nPP/$ PP/$ PP P P p ossessiv e determiner /(m y/)\nPP/$/$ PP/$/$ PP P P p ossessiv e pronoun /(mine/)\nPP/1A PP/1A PP/1 P P p ersonal pronoun /(I/)\nPP/1AS PP/1AS PP/1 P P p ersonal pronoun /(w e/)\nPP/1O PP/1O PP/1 P P p ersonal pronoun /(me/)\nPP/1OS PP/1OS PP/1 P P p ersonal pronoun /(us/)\nAPPENDIX B/. THE F OUR T A GSETS USED IN OUR EXPERIMENTS /1/1/8\nPP/2 PP/2 PP P P p ersonal pronoun /(y ou/)\nPP/3 PP/3 PP/3 P P p ersonal pronoun /(it/)\nPP/3A PP/3A PP/3 P P p ersonal pronoun /(she/)\nPP/3AS PP/3AS PP/3 P P p ersonal pronoun /(they/)\nPP/3O PP/3O PP/3 P P p ersonal pronoun /(him/)\nPP/3OS PP/3OS PP/3 P P p ersonal pronoun /(them/)\nPPL PPL PPL P P singular re/\rexiv e pronoun\nPPLS PPLS PPL P P plural re/\rexiv e pronoun\nQL QL QL QL QL quali/\fer /(as/)\nQLP QLP QL QL QL p ost/-quali/\fer /(enough/)\nRB RB RB R R adv erb\nRB/$ RB/$ RB/$ R R same as ab o v e with genitiv e\nRBR RBR RBR R R comparativ e adv erb\nRBT RBT RBT R R sup erlativ e adv erb\nRI RI RI R R adv erb /(homograph of prep osition/: b elo w/)\nRN RN RN R R nominal adv erb\nRP RP RP R R adv erbial particle /(bac k/)\nTO TO TO TO TO in/\fnitiv al to\nUH UH UH UH UH in terjection\nVB VB VB V V base form of b erb\nVBD VBD VBD V V past tense of v erb\nVBG VBG VBG V V presen t participle/, gerund\nVBN VBN VBN V V past participle\nVBZ VBZ VBZ V V /3rd p erson singular of v erb\nWDT WDT WD P P WH/-determiner /(what/)\nWDTR WDTR WD P P WH/-determiner/, relativ e /(whic h/)\nWP WP WP P P WH/-pronoun /(who/)\nWP/$ WP/$ WP P P WH/-pronoun /(whose/)\nWP/$R WP/$R WP P P WH/-pronoun/, relativ e /(whose/)\nWP A WP A WP P P WH/-pronoun /(whoso ev er/)\nWPO WPO WP P P WH/-pronoun /(whom/)\nWPOR WPOR WP P P WH/-pronoun/, relativ e /(whom/)\nWPR WPR WP P P WH/-pronoun /(that/)\nWRB WRB WR P P WH/-adv erb /(ho w/)\nXNOT XNOT XNOT XNOT XNOT not\nZZ ZZ ZZ ZZ F letter of alphab et /(e/)\nApp endix C\nThe New Mo del of Unkno wn\nW ords\nIn this app endix/, w e will sho w that the new mo deling of unkno wn w ords/, as prop osed in\nsection /4/./3/./2/, ensures that the sum of probabilities of all p ossible w ords is /1/.\nAs stated in section /4/./3/./2/, the probabilit y of the w ord w\nl\n/, giv en the tag g /( w /[ i /BnZr /1/]/) of\nthe last w ord is\nP /( w /[ i /] /= w\nl\nj w /[ i /BnZr /1/]/)\n/=\n/8\n/>\n/>\n/>\n/>\n/>\n/<\n/>\n/>\n/>\n/>\n/>\n/:\nP\ng\nj\n/2 G\n/[/(/1 /BnZr u /\u0003 d\n/1\n/BnZr d\ng\nj\n/)/( c\n/1\n/\u0003 f /( g /( w /[ i /]/) /= g\nj\nj g /( w /[ i /BnZr /1/]/)/) /+ c\n/2\n/)\n/\u0003 f /( w /[ i /] /= w\nl\nj g /( w /[ i /]/) /= g\nj\n/)/] if w\nl\n/2 V and w\nl\nw as seen\nd\n/1\nif w\nl\n/2 V but w\nl\nunseen\nP\ng\nj\n/2 G\nd\ng\nj\n/\u0003 /( c\n/1\n/\u0003 f /( g /( w /[ i /]/) /= g\nj\nj g /( w /[ i /BnZr /1/]/)/) /+ c\n/2\n/) if w\nl\n/6/2 V\nwith c\n/1\n/= /1 /BnZr j G j /\u0003 c\n/2\n/. The follo wing calculation sho ws that the sum of the probabilities of all\nthe w ords in the v o cabulary plus the probabilit y of the generic unkno wn w ord /`UNKNO WN/'\nis equal to one/.\nS /=\nX\nw is unseen\nP /( w /) /+ P /(\n/0\nU N K N O W N\n/0\n/) /+\nX\nw /2 V and seen\nP /( w /)\n/= A /+ B /+ C\nA /= u /\u0003 d\n/1\n/:\nB /=\nX\ng\nj\n/2 G\nd\ng\nj\n/\u0003 /( c\n/1\n/\u0003 f /( g /( w /[ i /]/) /= g\nj\nj g /( w /[ i /BnZr /1/]/)/) /+ c\n/2\n/) /:\n/1/1/9\nAPPENDIX C/. THE NEW MODEL OF UNKNO WN W ORDS /1/2/0\nC /=\nX\nw /2 V and seen\nP /( w /[ i /] /= w j g /( w /[ i /BnZr /1/]/)\n/=\nX\nw /2 V and seen\nX\ng\nj\n/2 G\n/[/(/1 /BnZr u /\u0003 d\n/1\n/BnZr d\ng\nj\n/)/( c\n/1\n/\u0003 f /( g /( w /[ i /]/) /= g\nj\nj g /( w /[ i /BnZr /1/]/)/) /+ c\n/2\n/)\n/\u0003 f /( w /[ i /] /= w j g /( w /[ i /] /= g\nj\n/)/]\n/=\nX\ng\nj\n/2 G\n/(/1 /BnZr u /\u0003 d\n/1\n/BnZr d\ng\nj\n/) /\u0003 /( c\n/1\n/\u0003 f /( g /( w /[ i /]/) /= g\nj\nj g /( w /[ i /BnZr /1/]/)/) /+ c\n/2\n/)\n/\u0003\nX\nw /2 V and seen\nP /( w /[ i /] /= w j g /( w /[ i /BnZr /1/]/)\n/=\nX\ng\nj\n/2 G\n/(/1 /BnZr u /\u0003 d\n/1\n/BnZr d\ng\nj\n/) /\u0003 /( c\n/1\n/\u0003 f /( g /( w /[ i /]/) /= g\nj\nj g /( w /[ i /BnZr /1/]/)/) /+ c\n/2\n/) /:\nB /+ C\n/=\nX\ng\nj\n/2 G\nd\ng\nj\n/\u0003 /( c\n/1\n/\u0003 f /( g /( w /[ i /]/) /= g\nj\nj g /( w /[ i /BnZr /1/]/)/) /+ c\n/2\n/)\n/+\nX\ng\nj\n/2 G\n/(/1 /BnZr u /\u0003 d\n/1\n/BnZr d\ng\nj\n/) /\u0003 /( c\n/1\n/\u0003 f /( g /( w /[ i /]/) /= g\nj\nj g /( w /[ i /BnZr /1/]/)/) /+ c\n/2\n/)\n/=\nX\ng\nj\n/2 G\n/(/1 /BnZr u /\u0003 d\n/1\n/) /\u0003 /( c\n/1\n/\u0003 f /( g /( w /[ i /]/) /= g\nj\nj g /( w /[ i /BnZr /1/]/)/) /+ c\n/2\n/)\n/= /(/1 /BnZr u /\u0003 d\n/1\n/) /\u0003\n/0\n/@\nc\n/1\n/\u0003\nX\ng\nj\n/2 G\nf /( g /( w /[ i /]/) /= g\nj\nj g /( w /[ i /BnZr /1/]/)/) /+ j G j c\n/2\n/1\nA\n/= /(/1 /BnZr u /\u0003 d\n/1\n/) /\u0003 /( c\n/1\n/+ j G j c\n/2\n/)\n/= /(/1 /BnZr u /\u0003 d\n/1\n/) /:\nS /= A /+ B /+ C\n/= u /\u0003 d\n/1\n/+ /(/1 /BnZr u /\u0003 d\n/1\n/) /= /1 /:\nApp endix D\nThe Generalize d N /-p os Mo del /{\nP art I\nIn this app endix/, w e will sho w that the generalized N /-p os mo del in tro duced in section /4/./4/./1\nensures that the sum of the probabilities of all the w ords is one/.\nAs stated in section /4/./4/./1/, the probabilit y of the w ord w\nl\n/, giv en the v ariables X\n/1\n/; /:/:/:/; X\nr /+ s\nis\np /( w /[ i /] /= w\nl\nj X\n/1\n/; /:/:/:/; X\nr /+ s\n/)\n/=\nX\ng\nj\n/2 G\np /( g /( w /[ i /]/) /= g\nj\nj X\n/1\n/; /:/:/:/; X\nr\n/) /\u0003 p /( w /[ i /] /= w\nl\nj g /( w /[ i /]/) /= g\nj\n/; X\nr /+/1\n/; /:/:/:/; X\nr /+ s\n/)\nIn the follo wing/, w e will use p\n/1\n/( g\nj\nj X\nr\n/) as a short hand for p /( g /( w /[ i /]/) /= g\nj\nj X\n/1\n/; /:/:/:/; X\nr\n/) and\np\n/2\n/( w\nl\nj g\nj\n/; X\nr /+ s\n/) as a short hand for p /( w /[ i /] /= w\nl\nj g /( w /[ i /]/) /= g\nj\n/; X\nr /+/1\n/; /:/:/:/; X\nr /+ s\n/)/.\nW e assume that p\n/1\n/( g\nj\nj X\nr\n/) and p\n/2\n/( w\nl\nj g\nj\n/; X\nr /+ s\n/) are probabilit y distributions for all com/-\nbinations of v alues of v ariables X\nk\n/; /1 /\u0014 k /\u0014 r /+ s and tags g\nj\n/; /1 /\u0014 j /\u0014 t /. In other w ords/:\nj /= t\nX\nj /=/1\np\n/1\n/( g\nj\nj X\nr\n/) /= /1\nand\nl /= m\nX\nl /=/1\np\n/2\n/( w\nl\nj g\nj\n/; X\nr /+ s\n/) /= /1 /:\nThis is for example the case if p\n/1\n/( g\nj\nj X\nr\n/) and p\n/2\n/( w\nl\nj g\nj\n/; X\nr /+ s\n/) are constructed from frequency\ndata and it is in general a reasonable assumption/.\n/1/2/1\nAPPENDIX D/. THE GENERALIZED N /-POS MODEL /{ P AR T I /1/2/2\nW e can no w sho w that the sum S of the probabilities of all the w ords in the v o cabulary\nis one/:\nS /=\nX\nw\nl\n/2 V\nX\ng\nj\n/2 G\np\n/1\n/( g\nj\nj X\nr\n/) /\u0003 p\n/2\n/( w\nl\nj g\nj\n/; X\nr /+ s\n/)\n/=\nX\ng\nj\n/2 G\np\n/1\n/( g\nj\nj X\nr\n/)\nX\nw\nl\n/2 V\np\n/2\n/( w\nl\nj g\nj\n/; X\nr /+ s\n/)\n/=\nX\ng\nj\n/2 G\np\n/1\n/( g\nj\nj X\nr\n/)\n/= /1 /:\nApp endix E\nThe Generalize d N /-p os Mo del /{\nP art I I\nIn this app endix/, w e w an t to sho w that the generalized N /-p os mo del reduces to the N /-gram\nmo del/. F or ease of reference/, w e rep eat the form ula for the generalized mo del/:\np /( w /[ i /] /= w\nk\nj X\n/1\n/; /:/:/:/; X\nm\n/) /=\n/=\nX\ng\nj\n/2 G\np /( g /( w /[ i /]/) /= g\nj\nj X\n/1\n/; /:/:/:/; X\nn\n/) /\u0003 p /( w /[ i /] /= w\nk\nj g /( w /[ i /]/) /= g\nj\n/; X\nn /+/1\n/; /:/:/:/; X\nn /+ m\n/) /:\nIf w e c hose n /= N /BnZr /1 /; X\nn /+ l\n/= X\nl\n/= w\ni /BnZr l\n/; l /= /1 /; /:/:/:/; N /BnZr /1/, w e obtain\np /( w /[ i /] /= w\nk\nj X\n/1\n/; /:/:/:/; X\nm\n/) /=\n/=\nX\ng\nj\n/2 G\np /( g /( w /[ i /]/) /= g\nj\nj w /[ i /BnZr N /+ /1 /: i /BnZr /1/]/) /\u0003 p /( w /[ i /] /= w\nk\nj g /( w /[ i /]/) /= g\nj\n/; w /[ i /BnZr N /+ /1 /: i /BnZr /1/]/) /:\nIf w e further assume that the probabilities are estimated from frequency coun ts denoted\nb y the function f /(/)/, w e ha v e\np /( w /[ i /] /= w\nk\nj X\n/1\n/; /:/:/:/; X\nm\n/) /=\n/=\nX\ng\nj\n/2 G\np /( g /( w /[ i /]/) /= g\nj\nj w /[ i /BnZr N /+ /1 /: i /BnZr /1/]/) /\u0003 p /( w /[ i /] /= w\nk\nj g /( w /[ i /]/) /= g\nj\n/; w /[ i /BnZr N /+ /1 /: i /BnZr /1/]/) /=\n/=\nX\ng\nj\n/2 G\nf /( g /( w /[ i /]/) /= g\nj\n/; w /[ i /BnZr N /+ /1 /: i /BnZr /1/]/)\nf /( w /[ i /BnZr N /+ /1 /: i /BnZr /1/]/)\n/\u0003\nf /( w /[ i /] /= w\nk\n/; g /( w /[ i /]/) /= g\nj\n/; w /[ i /BnZr N /+ /1 /: i /BnZr /1/]/)\nf /( g /( w /[ i /]/) /= g\nj\n/; w /[ i /BnZr N /+ /1 /: i /BnZr /1/]/)\n/=\n/=\nX\ng\nj\n/2 G\nf /( w /[ i /] /= w\nk\n/; g /( w /[ i /]/) /= g\nj\n/; w /[ i /BnZr N /+ /1 /: i /BnZr /1/]/)\nf /( w /[ i /BnZr N /+ /1 /: i /BnZr /1/]/)\n/=\n/=\nf /( w /[ i /] /= w\nk\n/; w /[ i /BnZr N /+ /1 /: i /BnZr /1/]/)\nf /( w /[ i /BnZr N /+ /1 /: i /BnZr /1/]/)\n/=\n/= p /( w /[ i /] /= w\nk\nj w /[ i /BnZr N /+ /1 /: i /BnZr /1/]/) /:\n/1/2/3\nAPPENDIX E/. THE GENERALIZED N /-POS MODEL /{ P AR T I I /1/2/4\nSince the frequency coun ts are usually smo othed b efore they are used to estimate the\nprobabilities/, the generalized N /-p os mo del will not b e exactly the same as the N /-gram\nmo del/. Ho w ev er/, as these calculations sho w/, it will b e an appro ximation of it/, based on the\nsame dep endencies/.\nBiblio graph y\n/[/1/] M/. Adda/-Dec k er and J/. Mariani/. Collab oration b et w een analytical and global metho ds\nfor con tin uous sp eec h recognition/. In /7th F ASE Symp osium on A c oustics and Sp e e ch /,\npages /3/4/5/{/3/5/2/. Edin burgh/, /1/9/8/8/.\n/[/2/] B/. Alten b erg and M/. Eeg/-Olofsson/. Phraseology in sp ok en English/: Presen tation of a\npro ject/. In J/. Aarts and W/. Meijs/, editors/, The ory and Pr actic e in Corpus Linguistics /,\npages /1/{/2/6/. Editions Ro dopi/, B/.V/. Amsterdam/-A tlan ta/, /1/9/9/0/.\n/[/3/] G/. Altmann/. Prolegomena to Menzerath/'s la w/. Glottometric a /, /2/:/1/{/1/0/, /1/9/8/0/.\n/[/4/] G/. Altmann and M/. Sc hibb e/. Das Menzer athsche Gesetz in Informationsver arb eiten/-\nden Systemen /. Hildesheim/, Zueric h/, New Y ork/, /1/9/8/9/.\n/[/5/] L/. Bahl/, J/. Bak er/, P /. Cohen/, F/. Jelinek/, B/. Lewis/, and R/. Mercer/. Recognition of\na con tin uously read natural corpus/. In Pr o c e e dings of International Confer enc e on\nA c oustics/, Sp e e ch and Signal Pr o c essing /. T ulsa/, /1/9/7/8/.\n/[/6/] L/. Bahl/, F/. Jelinek/, and R/. Mercer/. A maxim um lik eliho o d approac h to con tin uous\nsp eec h recognition/. IEEE T r ansactions on Pattern A nalysis and Machine Intel ligenc e /,\n/5/(/2/)/:/1/7/9/{/1/9/0/, Marc h /1/9/8/3/.\n/[/7/] L/. R/. Bahl/, P /. F/. Bro wn/, P /. V/. de Souza/, and R/. L/. Mercer/. A tree/-based statistical\nlanguage mo del for natural language sp eec h recognition/. In A/. W aib el and K/./-F/.\nLee/, editors/, R e adings in Sp e e ch R e c o gnition /, pages /5/0/7/{/5/1/4/. Morgan Kaufmann/, San\nMateo/, CA/, /1/9/9/0/.\n/[/8/] R/. Bec kwith/, C/. F ellbaum/, D/. Gross/, and G/. Miller/. W ordnet/: A lexical database\norganized on psyc holinguistic principles/. In U/. Zernik/, editor/, L exic al A c quisition/:\nExploiting On/-Line R esour c es to Build a L exic on /, pages /2/1/1/{/2/3/2/. Erlbaum/, /1/9/9/1/.\n/1/2/5\nBIBLIOGRAPHY /1/2/6\n/[/9/] E/. Blac k/, F/. Jelinek/, J/. La/\u000bert y /, D/. Magerman/, R/. Mercer/, and S/. Rouk os/. T o w ards\nhistory/-based grammars/: Using ric her mo dels for probabilistic parsing/. In Pr o c e e dings\nof D ARP A Sp e e ch and Natur al L anguage Workshop /, pages /1/3/4/{/1/3/9/. Harriman/, New\nY ork/, F eb /1/9/9/2/.\n/[/1/0/] E/. Blac k/, F/. Jelinek/, J/. La/\u000bert y /, R/. Mercer/, and S/. Rouk os/. Decision tree mo dels\napplied to the lab eling of text with parts/-of/-sp eec h/. In Pr o c e e dings of D ARP A Sp e e ch\nand Natur al L anguage Workshop /, pages /1/1/7/{/1/2/1/. Harriman/, New Y ork/, F ebruary /1/9/9/2/.\n/[/1/1/] L/. Breiman/, J/. H/. F riedman/, R/. A/. Olshen/, and C/. J/. Stone/. Classi/\fc ation and R e/-\ngr ession T r e es /. W adsw orth/, Inc/./, Belmon t CA/, /1/9/8/4/.\n/[/1/2/] M/. R/. Bren t/. Automatic acquisition of sub categorization frames from un tagged text/. In\nPr o c e e dings of the A nnual Me eting of the Asso ciation for Computational Linguistics /,\npages /2/0/9/{/2/1/4/, /1/9/9/1/.\n/[/1/3/] P /. F/. Bro wn/, S/. A/. della Pietra/, V/. J/. della Pietra/, J/. C/. Lai/, and R/. L/. Mercer/. An\nestimate of an upp er b ound for the en trop y of English/. Computational Linguistics /,\n/1/8/(/1/)/:/3/1/{/4/0/, /1/9/9/2/.\n/[/1/4/] P /. F/. Bro wn/, V/. J/. della Pietra/, P /. V/. de Souza/, J/. C/. Lai/, and R/. L/. Mercer/. Class\nbased language mo dels/. T ec hnical rep ort/, IBM/, Y ork T o wn Heigh ts/, NJ/, /1/9/9/0/.\n/[/1/5/] H/. Cerf/-Danon and M/. El/-Beze/. Three di/\u000beren t probabilistic language mo dels/: Com/-\nparison and com bination/. In Pr o c e e dings of International Confer enc e on A c oustics/,\nSp e e ch and Signal Pr o c essing /, pages /2/9/7/{/3/0/0/, /1/9/9/1/.\n/[/1/6/] P /. A/. Chou/. Optimal partitioning for classi/\fcation and regression trees/. IEEE T r ans/-\nactions on Pattern A nalysis and Machine Intel ligenc e /, /1/3/(/4/)/:/3/4/0/{/3/5 /4/, /1/9/9/1/.\n/[/1/7/] K/. Ch urc h and M/. Lib erman/. A status rep ort on the A CL//DCI/. In Using c orp or a/:\nPr o c e e dings fr om the New OED Confer enc e /, pages /8/4/{/9/1/. The Univ ersit y of W aterlo o\nCen tre for the new OED and T ext Researc h/, W aterlo o/, On tario/, /1/9/9/0/.\n/[/1/8/] K/. W/. Ch urc h/. A sto c hastic parts program and noun phrase parser for unrestricted\ntext/. In Pr o c e e dings of International Confer enc e on A c oustics/, Sp e e ch and Signal\nPr o c essing /, pages /6/9/5/{/6/9/8/. Glasgo w/,Scotland/, /1/9/8/8/.\nBIBLIOGRAPHY /1/2/7\n/[/1/9/] K/. W/. Ch urc h/, W/. Gale/, P /. Hanks/, and D/. Hindle/. P arsing/, w ord asso ciations and\nt ypical predicate/-argumen t relations/. In Pr o c e e dings of D ARP A Sp e e ch and Natur al\nL anguage Workshop /, pages /7/5/{/8/1/. Cap e Co d/, Octob er/, /1/9/8/9/.\n/[/2/0/] K/. W/. Ch urc h and W/. A/. Gale/. A comparison of the enhanced Go o d/-Turing and\ndeleted estimation metho ds for estimating probabilities of English bigrams/. Computer/,\nSp e e ch and L anguage /, /5/:/1/9/{/5/4/, /1/9/9/1/.\n/[/2/1/] M/. Co dogno/, L/. Fissore/, A/. Martelli/, G/. Pirani/, and G/. V olpi/. Exp erimen tal ev aluation\nof Italian language mo dels for large/-dictionary sp eec h recognition/. In Pr o c e e dings\nof the Eur op e an Confer enc e on Sp e e ch T e chnolo gy /, pages v ol/./1/, /1/5/9/{/1/6/2/. Edin burgh/,\nSeptem b er /1/9/8/7/.\n/[/2/2/] A/. Colla/. Automatic diphone b o otstrapping for sp eak er/-adaptiv e con tin uous sp eec h\nrecognition/. In Pr o c e e dings of International Confer enc e on A c oustics/, Sp e e ch and\nSignal Pr o c essing /, pages /3/5/./2/./1/{/3/5/./2/./4/. San Diego/, USA/, /1/9/8/4/.\n/[/2/3/] B/. Comrie/. L anguage Universals and Linguistic T yp olo gy/: Syntax and Morpholo gy /.\nBasil Blac kw ell/, Oxford/, /1/9/8/1/.\n/[/2/4/] A/. Corazza/, R/. De Mori/, R/. Gretter/, and G/. Satta/. Computation of probabilities\nfor an island/-driv en parser/. IEEE T r ansactions on Pattern A nalysis and Machine\nIntel ligenc e /, /1/3/(/9/)/:/9/3/6/{/9/5/0 /, /1/9/9/1/.\n/[/2/5/] J/. N/. Darro c h and D/. Ratcli/\u000b/. Generalized iterativ e scaling for log/-linear mo dels/. The\nA nnals of Mathematic al Statistics /, /4/3/(/5/)/:/1/4/7/0/{/1/4 /8/0/, /1/9/7/2/.\n/[/2/6/] C/. G/. de Marc k en/. P arsing the LOB corpus/. In Pr o c e e dings of the /2/8\nth\nA nnual\nMe eting of the Asso ciation for Computational Linguistics /, pages /2/4/3/{/2/5/1/. Pittsburgh/,\nP A/, /1/9/9/0/.\n/[/2/7/] S/. della Pietra/, V/. della Pietra/, L/. Mercer/, and S/. Rouk os/. Adaptiv e language mo deling\nusing minim um discriminan t estimation/. In Pr o c e e dings of International Confer enc e\non A c oustics/, Sp e e ch and Signal Pr o c essing /, pages /6/3/3/{/6/3/6/, /1/9/9/2/.\n/[/2/8/] S/. J/. DeRose/. Grammatical category disam biguation b y statistical optimization/. Com/-\nputational Linguistics /, /1/4/:/3/1/{/3/9/, /1/9/8/8/.\nBIBLIOGRAPHY /1/2/8\n/[/2/9/] A/. Derouault and B/. Merialdo/. Natural language mo deling for phoneme/-to/-text tran/-\nscription/. IEEE T r ansactions on Pattern A nalysis and Machine Intel ligenc e /, /8/:/7/4/2/{\n/7/4/9/, /1/9/8/6/.\n/[/3/0/] P /. Dumouc hel/, V/. Gupta/, M/. Lennig/, and P /. Mermelstein/. Three probabilistic lan/-\nguage mo dels for a large/-v o cabulary sp eec h recognizer/. In Pr o c e e dings of International\nConfer enc e on A c oustics/, Sp e e ch and Signal Pr o c essing /, pages /5/1/3/{/5/1/6/, /1/9/8/9/.\n/[/3/1/] J/. Edw ards/. Surv ey of electronic corp ora and related resources for language re/-\nsearc hers/. In J/. Edw ards and M/. Lamp ert/, editors/, T alking Data/: T r anscription and\nCo ding in Disc ourse R ese ar ch /, pages /2/6/3/{/3/1/0/. Erlbaum/, London and Hillsdale/, NJ/,\nUSA/, /1/9/9/3/.\n/[/3/2/] M/. El/-B /\u0012 eze and A/./-M/. Derouault/. A morphological mo del for large v o cabulary sp eec h\nrecognition/. In Pr o c e e dings of International Confer enc e on A c oustics/, Sp e e ch and\nSignal Pr o c essing /, pages /5/7/7/{/5/8/0/. Albuquerque/, NM/, /1/9/9/0/.\n/[/3/3/] L/. D/. Erman and V/. R/. Lesser/. The Hearsa y/-I I sp eec h understanding system/: A\ntutorial/. In A/. W aib el and K/./-F/. Lee/, editors/, R e adings in Sp e e ch R e c o gnition /, pages\n/2/3/5/{/2/4/5/. Morgan Kaufmann Publishers/, Inc/, San Mateo/, California/, /1/9/9/0/.\n/[/3/4/] U/. Essen and V/. Stein biss/. Co o ccurrence smo othing for sto c hastic language mo deling/.\nIn Pr o c e e dings of International Confer enc e on A c oustics/, Sp e e ch and Signal Pr o c essing /,\npages /1/6/1/{/1/6/3/, /1/9/9/2/.\n/[/3/5/] Eurosp eec h/. Pr o c e e dings of the Eur op e an Confer enc e on Sp e e ch Communic ation and\nT e chnolo gy /. Berlin/, German y /, /1/9/9/3/.\n/[/3/6/] S/. P /. Finsc h/. Finding Structur e in L anguage /. Ph/.D/. Thesis/, Univ ersit y of Edin burgh/,\nScotland/, /1/9/9/3/.\n/[/3/7/] S/. R/. A/. Fisher/. Statistic al Metho ds and Scienti/\fc Infer enc e /. Edin burgh/, Oliv er and\nBo yd/, /2nd revised edition/, /1/9/5/9/.\n/[/3/8/] In T/. F orester/, editor/, Computers in the Human Context/: Information T e chnolo gy/,\nPr o ductivity and Pe ople /. The MIT Press/, Cam bridge/, Massac h usetts/, /1/9/8/9/.\nBIBLIOGRAPHY /1/2/9\n/[/3/9/] W/. F rancis/. Problems of assem bling and computerizing large corp ora/. In S/. Green/-\nbaum/, G/. Leec h/, and J/. Sv artvik/, editors/, Computer Corp or a in English L anguage\nR ese ar ch /, pages /7/{/2/4/. Norw egian Computing Cen tre for the Humanities/, Bergen/, /1/9/8/2/.\n/[/4/0/] M/. F rec het/. M /\u0013 etho des des fonctions arbitr air es/. The orie des /\u0013 ev /\u0013 enements en cha /^ /\u0010ne\ndans le c as d/'un nombr e /\fni d/' /\u0013 etats p ossibles /. Gauthier Villars/, P aris/, /1/9/3/8/.\n/[/4/1/] J/. E/. F reund/. Mo dern Elementary Statistics /. Pren tice/-Hall/, Englew o o d Cli/\u000bs/, New\nJersey /, /7th Edition/, /1/9/8/8/.\n/[/4/2/] K/./-S/. F u and T/. L/. Bo oth/. Grammatical inference/: In tro duction and surv ey /{ part /1/.\nIEEE T r ansactions on Pattern A nalysis and Machine Intel ligenc e /, /5/(/1/)/:/9/5/{/1/1/0/, /1/9/7/5/.\n/[/4/3/] K/./-S/. F u and T/. L/. Bo oth/. Grammatical inference/: In tro duction and surv ey /{ part\n/2/. IEEE T r ansactions on Pattern A nalysis and Machine Intel ligenc e /, /5/(/4/)/:/4/0/9/{/4/2 /3/,\n/1/9/7/5/.\n/[/4/4/] J/. Gauv ain/. A syllable based isolated w ord recognition exp erimen t/. In Pr o c e e dings\nof International Confer enc e on A c oustics/, Sp e e ch and Signal Pr o c essing /, pages /5/7/{/6/0/.\nT oky o/, /1/9/8/6/.\n/[/4/5/] S/. Gelfand/, C/. Ra vishank ar/, and E/. J/. Delp/. An iterativ e gro wing and pruning al/-\ngorithm for classi/\fcation tree design/. IEEE T r ansactions on Pattern A nalysis and\nMachine Intel ligenc e /, /1/3/(/2/)/:/1/6/3/{/1/7/4 /, /1/9/9/1/.\n/[/4/6/] S/. B/. Gelfand and E/. J/. Delp/. On tree structured classi/\fers/. In I/. K/. Sethi and A/. K/.\nJain/, editors/, A rti/\fcial Neur al Networks and Statistic al Pattern R e c o gnition/: Old and\nNew Conne ctions /, pages /5/1/{/7/0/. North Holland/, Amsterdam/, /1/9/9/1/.\n/[/4/7/] D/. Go ddeau and V/. Zue/. In tegrating probabilistic LR parsing in to sp eec h understand/-\ning systems/. In Pr o c e e dings of International Confer enc e on A c oustics/, Sp e e ch and\nSignal Pr o c essing /, pages /1/8/1/{/1/8/4/, /1/9/9/2/.\n/[/4/8/] I/. Go o d/. The p opulation frequencies of sp ecies and the estimation of p opulation\nparameters/. Biometrika /, /4/0/:/2/3/7/{/2/6/4/, /1/9/5/3/.\n/[/4/9/] S/. Green baum/. A new corpus of English/: ICE/. In J/. Sv artvik/, editor/, Dir e ctions in\nCorpus Linguistics /, pages /1/7/1/{/1/7/9/. New Y ork/: Mouton de Gruyter/, /1/9/9/2/.\nBIBLIOGRAPHY /1/3/0\n/[/5/0/] D/. Hindle/. User man ual of Fidditc h/, a deterministic parser/. T ec hnical Rep ort Memo/-\nrandum /7/5/9/0/-/1/4/2/, Na v al Researc h Lab oratory /, W ashington/, D/.C/./, /1/9/8/3/.\n/[/5/1/] D/. Hindle and M/. Ro oth/. Structural am biguit y and lexical relations/. In Pr o c e e dings\nof the /2/9\nth\nA nnual Me eting of the Asso ciation for Computational Linguistics /, pages\n/2/2/9/{/2/3/6/. Berk eley /, CA/, /1/9/9/1/.\n/[/5/2/] L/. Hirsc hman/, S/. Sene/\u000b/, D/. Go o dine/, and M/. Phillips/. In tegrating syn tax and se/-\nman tics in to sp ok en language understanding/. In Pr o c e e dings of D ARP A Sp e e ch and\nNatur al L anguage Workshop /, pages /3/6/6/{/3/7/1/. San Mateo/, CA/, /1/9/9/1/.\n/[/5/3/] J/. E/. Hop croft and J/. D/. Ullman/. Intr o duction to A utomata The ory/, L anguages and\nComputation /. Addison/-W esley /, Reading/, MA/, /1/9/7/9/.\n/[/5/4/] J/. Hull and S/. N/. Srihari/. Exp erimen ts in text recognition with binary N/-gram and\nViterbi algorithms/. IEEE T r ansactions on Pattern A nalysis and Machine Intel ligenc e /,\n/4/:/5/2/0/{/5/3/0/, /1/9/8/2/.\n/[/5/5/] M/. Hun t/, M/. Lennig/, and P /. Mermelstein/. Exp erimen ts in syllable/-based recognition\nof con tin uous sp eec h/. In Pr o c e e dings of International Confer enc e on A c oustics/, Sp e e ch\nand Signal Pr o c essing /, pages /8/8/0/{/8/8/3/. Den v er/, CO/, /1/9/8/0/.\n/[/5/6/] M/. J/. Hun t/. Personal Communic ation /. Co/-Director/, Dragon Systems UK L TD/./, /1/9/9/3/.\n/[/5/7/] ICASSP/. Pr o c e e dings of the International Confer enc e on A c oustics/, Sp e e ch and Signal\nPr o c essing /. Minneap olis/, MN/, /1/9/9/3/.\n/[/5/8/] S/. Imp edo v o/, L/. Otta viano/, and S/. Occ hinegro/. Optical c haracter recognition /{ a sur/-\nv ey /. In P /. S/./-P /. W ang/, editor/, Char acter /& Handwriting R e c o gnition /. Singap ore/;Riv er\nEdge/, N/.J/./; W orld Scien ti/\fc/, /1/9/9/1/.\n/[/5/9/] A/. Jain and A/. W aib el/. Robust connectionist parsing of sp ok en language/. In Pr o c e e d/-\nings of International Confer enc e on A c oustics/, Sp e e ch and Signal Pr o c essing /. Albu/-\nquerque/, NM/, April/, /1/9/9/0/.\n/[/6/0/] M/. Jardino and G/. Adda/. Automatic w ord classi/\fcation using sim ulated annealing/. In\nPr o c e e dings of International Confer enc e on A c oustics/, Sp e e ch and Signal Pr o c essing /,\npages /4/1/{/4/3/. Minneap olis/, MN/, /1/9/9/3/.\nBIBLIOGRAPHY /1/3/1\n/[/6/1/] E/. T/. Ja ynes/. Information theory and statistical mec hanics/. Physic al R eview /,\n/1/0/6/(/4/)/:/6/2/0/{/6/3 /0/, /1/9/5/7/.\n/[/6/2/] F/. Jelinek/. Self/-organized language mo deling for sp eec h recognition/. In A/. W aib el and\nK/./-F/. Lee/, editors/, R e adings in Sp e e ch R e c o gnition /, pages /4/5/0/{/5/0/6/. Morgan Kaufmann/,\nSan Mateo/, CA/, /1/9/9/0/.\n/[/6/3/] F/. Jelinek/. Up from trigram/! the struggle for impro v ed language mo dels/. In Eur op e an\nConfer enc e on Sp e e ch Communic ation and T e chnolo gy /, pages /1/0/3/7/{/1/0/4/0/, /1/9/9/1/.\n/[/6/4/] F/. Jelinek and J/. D/. La/\u000bert y /. Computation of the probabilit y of initial substring gener/-\nation b y sto c hastic con text/-free grammars/. Computational Linguistics /, /1/7/(/3/)/:/3/1/5/{/3/2 /3/,\n/1/9/9/1/.\n/[/6/5/] F/. Jelinek/, R/. Mercer/, and S/. Rouk os/. Classifying w ords for impro v ed statistical\nlanguage mo dels/. In Pr o c e e dings of International Confer enc e on A c oustics/, Sp e e ch\nand Signal Pr o c essing /, pages /6/2/1/{/6/2/4/. Albuquerque/, NM/, /1/9/9/0/.\n/[/6/6/] F/. Jelinek and R/. L/. Mercer/. In terp olated estimation of Mark o v source parameters\nfrom sparse data/. In E/. S/. Gelsema and L/. H/. Kanal/, editors/, Pattern R e c o gnition in\nPr actic e /, pages /3/8/1/{/3/9/7/. /1/9/8/0/.\n/[/6/7/] F/. Jelinek/, B/. Merialdo/, S/. Rouk os/, and M/. Strauss/. A dynamic language mo del for\nsp eec h recognition/. In Pr o c e e dings of D ARP A Sp e e ch and Natur al L anguage Workshop /,\npages /2/9/3/{/2/9/5/, F ebruary /1/9/9/1/.\n/[/6/8/] S/. Johansson/. Some observ ations on w ord frequencies in three corp ora of presen t/-\nda y English texts/. International R eview of Applie d Linguistics in L anguage T e aching /,\n/6/7/-/6/8/:/1/1/7/{/1/2/6/, /1/9/8/5/.\n/[/6/9/] S/. Johansson/. W ord frequency and text t yp e/: Some observ ations based on the LOB\ncorpus of British English texts/. Computing and the Humanities /, /1/9/:/2/3/{/3/6/, /1/9/8/5/.\n/[/7/0/] S/. Johansson/, E/. A t w ell/, R/. Garside/, and G/. Leec h/. The tagged LOB corpus user/'s\nman ual/. T ec hnical rep ort/, Norw egian Computing Cen tre for the Humanities/, Bergen/,\nNorw a y /, /1/9/8/6/.\nBIBLIOGRAPHY /1/3/2\n/[/7/1/] S/. Johansson/, G/. Leec h/, and H/. Go o dluc k/. Man ual of information to accompan y\nthe Lancaster/-Oslo/-Bergen corpus of British English for use with digital computers/.\nT ec hnical rep ort/, Bergen/: Norw egian Computing Cen tre for the Humanities/, /1/9/7/8/.\n/[/7/2/] M/. F/. S/. John and J/. L/. McClelland/. Learning and applying con textual constrain ts in\nsen tence comprehension/. A rti/\fcial Intel ligenc e /, /4/6/:/2/1/7/{/2/5/7/, /1/9/9/0/.\n/[/7/3/] S/. Katz/. Estimation of probabilities from sparse data for the language mo del com/-\np onen t of a sp eec h recognizer/. IEEE T r ansactions on A c oustics/, Sp e e ch and Signal\nPr o c essing /, /3/5/:/4/0/0/{/4/0/1/, /1/9/8/7/.\n/[/7/4/] T/. Ka w abata and et al/. Japanese phonetic t yp ewriter using HMM phone units and\nsyllable trigrams/. In International Confer enc e on Sp oken L anguage Pr o c essing /, pages\n/7/1/7/{/7/2/0/, /1/9/9/0/.\n/[/7/5/] K/. Kita and W/. H/. W ard/. Incorp orating LR parsing in to SPHINX/. In Pr o c e e dings of\nInternational Confer enc e on A c oustics/, Sp e e ch and Signal Pr o c essing /, pages /2/6/9/{/2/7/2/,\n/1/9/9/1/.\n/[/7/6/] G/. Kjellmer/. A min t of phrases/. In K/. Aijmer and B/. Alten b erg/, editors/, English\nCorpus Linguistics/: Studies in Honour of Jan Svartvik /, pages /1/1/1/{/1/2/7/. Longman/,\nLondon/, England/, /1/9/9/1/.\n/[/7/7/] G/. Kjellmer/. P atterns of collo cabilit y /. In J/. Aarts and W/. Meijs/, editors/, The ory and\nPr actic e in Corpus Linguistics /, pages /1/6/3/{/1/7/8/. Editions Ro dopi/, B/. V/./, Amsterdam/-\nA tlan ta/, /1/9/9/1/.\n/[/7/8/] R/. Kneser and H/. Ney /. Impro v ed clustering tec hniques for class/-based statistical lan/-\nguage mo delling/. In Eur op e an Confer enc e on Sp e e ch Communic ation and T e chnolo gy /,\npages /9/7/3/{/9/7/6/. Berlin/, German y /, Septem b er /1/9/9/3/.\n/[/7/9/] R/. Kneser and V/. Stein biss/. On the dynamic adaptation of sto c hastic language mo dels/.\nIn Pr o c e e dings of International Confer enc e on A c oustics/, Sp e e ch and Signal Pr o c essing /,\npages /5/8/6/{/5/8/9/. Minneap olis/, Minnesota/, USA/, /1/9/9/3/.\n/[/8/0/] G/. Kno wles and L/. La wrence/. Automatic in tonation assignmen t/. In R/. Garside/,\nG/. Leec h/, and G/. Sampson/, editors/, The Computational A nalysis of English/: A\nCorpus/-b ase d Appr o ach /. London/: Longman/, /1/9/8/7/.\nBIBLIOGRAPHY /1/3/3\n/[/8/1/] H/. Kucera/. Bro wn corpus/. In S/. Shapiro/, editor/, Encyclop e dia of A rti/\fc al Intel ligenc e /,\npages v ol/./1/, pp/. /1/2/8/{/1/3/0/. John Wiley /& Sons/, New Y ork/, NY/, /1/9/9/2/.\n/[/8/2/] H/. Kucera and W/. F rancis/. Computational A nalysis of Pr esent/-day A meric an English /.\nBro wn Univ ersit y Press/, Pro vidence/, RI/, /1/9/6/7/.\n/[/8/3/] R/. Kuhn/. Personal Communic ation /. Mon treal/, Canada/, /1/9/9/2/.\n/[/8/4/] R/. Kuhn/. Sp eec h recognition and the frequency of recen tly used w ords/: A mo di/\fed\nmark o v mo del for natural language/. In International Confer enc e on Computational\nLinguistics /, pages /3/4/8/{/3/5/0/. Budap est/, August /1/9/8/8/.\n/[/8/5/] R/. Kuhn and R/. De Mori/. A cac he/-based natural language mo del for sp eec h recog/-\nnition/. IEEE T r ansactions on Pattern A nalysis and Machine Intel ligenc e /, /1/2/(/6/)/:/5/7/0/{\n/5/8/3/, /1/9/9/0/.\n/[/8/6/] R/. Kuhn and R/. De Mori /. Corrections to /`a cac he/-based natural language mo del for\nsp eec h recognition/'/. IEEE T r ansactions on Pattern A nalysis and Machine Intel ligenc e /,\n/1/4/:/6/9/1/{/6/9/2/, /1/9/9/2/.\n/[/8/7/] R/. Kuhn and R/. De Mori/. Learning sp eec h seman tics with k eyw ord classi/\fcation/. In\nPr o c e e dings of International Confer enc e on A c oustics/, Sp e e ch and Signal Pr o c essing /,\npages /5/5/{/5/8/. Minneap olis/, MN/, /1/9/9/3/.\n/[/8/8/] J/. Kupiec/. Probabilistic mo dels of short and long distance w ord dep endencies in\nrunning text/. In Pr o c e e dings of D ARP A Sp e e ch and Natur al L anguage Workshop /,\npages /2/9/0/{/2/9/5/. F ebruary /, /1/9/8/9/.\n/[/8/9/] J/. Kupiec/. A trellis/-based algorithm for estimating the parameters of a hidden sto c has/-\ntic con text free grammar/. In Pr o c e e dings of D ARP A Sp e e ch and Natur al L anguage\nWorkshop /, pages /2/4/1/{/2/4/6/. Morgan Kaufmann/: San Mateo/, CA/, /1/9/9/1/.\n/[/9/0/] J/. Kupiec/. Robust part/-of/-sp eec h tagging using a hidden Mark o v mo del/. Computer/,\nSp e e ch and L anguage /, /6/:/2/2/5/{/2/4/2/, /1/9/9/2/.\n/[/9/1/] K/. Lari and S/. Y oung/. The estimation of sto c hastic con text free grammars using the\ninside/-outside algorithm/. Computer Sp e e ch and L anguage /, /4/:/3/5/{/5/6/, /1/9/9/0/.\nBIBLIOGRAPHY /1/3/4\n/[/9/2/] K/. Lari and S/. Y oung/. Applications of sto c hastic con text/-free grammars using the\ninside/-outside algorithm/. Computer Sp e e ch and L anguage /, /5/:/2/3/7/{/2/5/7/, /1/9/9/1/.\n/[/9/3/] R/. Lau/, R/. Rosenfeld/, and S/. Rouk os/. T rigger/-based language mo dels/: A maxim um\nen trop y approac h/. In Pr o c e e dings of International Confer enc e on A c oustics/, Sp e e ch\nand Signal Pr o c essing /, pages /4/5/{/4/8/. Minneap olis/, Minnesota/, USA/, /1/9/9/3/.\n/[/9/4/] R/. Lau/, R/. Rosenfeld/, and S/. Rouk os/. Adaptiv e language mo deling using the maxim um\nen trop y principle/. In Pr o c e e dings of D ARP A Sp e e ch and Natur al L anguage Workshop /,\nMarc h/, /1/9/9/3/.\n/[/9/5/] K/./-F/. Lee/. Large/-v o cabulary sp eak er indep enden t con tin uous sp eec h recognition/: The\nSPHINX system/. T ec hnical Rep ort CMU/-CS/-/8/8/-/1/4/8/, Ph/.D/. Thesis/, Computer Science\nDepartmen t/, Carnegie Mellon Univ ersit y /, Pittsburgh/, P A/, /1/9/8/8/.\n/[/9/6/] K/./-F/. Lee and H/./-W/. Hon/. Sp eak er/-indep endent phone recognition using hidden\nMark o v mo dels/. IEEE T r ansactions on A c oustics/, Sp e e ch and Signal Pr o c essing /,\n/3/7/(/1/1/)/:/1/6/4/1/{/1 /6/4/8 /, /1/9/8/9/.\n/[/9/7/] G/. Leec h and R/. Garside/. Running a grammar factory/: The pro duction of syn tactically\nanalysed corp ora or /`treebanks/'/. In S/. Johansson and A/./-B/. Stenstr/ o m/, editors/, English\nComputer Corp or a/: Sele cte d Pap ers and R ese ar ch Guide /, pages /1/5/{/3/3/. Mouton de\nGruyter/, /1/9/9/1/.\n/[/9/8/] Leonard/. In Pr o c e e dings of International Confer enc e on A c oustics/, Sp e e ch and Signal\nPr o c essing /, pages v ol/. /3/, /4/2/./1/1/./1/{/4/2/. /1/1/./4 /, /1/9/8/4/.\n/[/9/9/] M/. Lib erman/. T ext on tap/: The A CL//DCI/. In Pr o c e e dings of D ARP A Sp e e ch and\nNatur al L anguage Workshop /. San Mateo/, CA/, USA/, Octob er /1/9/8/9/.\n/[/1/0/0/] M/. Lib erman and V/. Zue/. Intr o duction to the Linguistic Data Consortium /. P art of an\nInformation P ac k age on the LDC/, /1/9/9/3/.\n/[/1/0/1/] C/. D/. Manning/. Automatic acquisition of a large sub categorization dictionary from\ncorp ora/. In Pr o c e e dings of the A nnual Me eting of the Asso ciation for Computational\nLinguistics /, pages /2/3/5/{/2/4/2/. Colum bus/, OH/, /1/9/9/3/.\n/[/1/0/2/] M/. P /. Marcus/, B/. San torini/, and M/. A/. Marcinkiewicz/. Building a large annotated\ncorpus of English/: The P enn T reebank/. Computational Linguistics /, /1/9/:/3/1/3/{/3/3/0/, /1/9/9/3/.\nBIBLIOGRAPHY /1/3/5\n/[/1/0/3/] J/. Mariani/. Reconnaissance de la parole con tin ue par diphonemes/. In Pr o c essus\nd/'Enc o dage et de D /\u0013 ec o dage Phon /\u0013 etiques /. Symp osium Galf/-Greco/, T oulouse/, /1/9/8/1/.\n/[/1/0/4/] J/. Mariani/. Recen t adv ances in sp eec h pro cessing/. In Pr o c e e dings of International Con/-\nfer enc e on A c oustics/, Sp e e ch and Signal Pr o c essing /, pages /4/2/9/{/4/4/0/. Glasgo w/, Scotland/,\n/1/9/8/9/.\n/[/1/0/5/] S/. Matsunaga/, T/. Y amada/, and K/. Shik ano/. T ask adaptation in sto c hastic language\nmo dels for con tin uous sp eec h recognition/. In Pr o c e e dings of International Confer enc e\non A c oustics/, Sp e e ch and Signal Pr o c essing /, pages /1/6/5/{/1/6/8/, /1/9/9/2/.\n/[/1/0/6/] F/. R/. McInnes/. An enhanced in terp olation tec hnique for con text/-sp eci/\fc probabilit y\nestimation in sp eec h and language mo delling/. In International Confer enc e on Sp oken\nL anguage Pr o c essing /. Ban/\u000b/, AB/, /1/9/9/2/.\n/[/1/0/7/] G/. Miller/. W ordnet/: An on/-line lexical database/. International Journal of L exic o gr aphy\n/(Sp e cial Issue/) /, /3/(/4/)/, /1/9/9/0/.\n/[/1/0/8/] H/. Murv eit and R/. Mo ore/. In tegrating natural language constrain ts in to HMM/-based\nsp eec h recognition/. In Pr o c e e dings of International Confer enc e on A c oustics/, Sp e e ch\nand Signal Pr o c essing /, pages /5/7/3/{/5/7/6/. Albuquerque/, NM/, /1/9/9/0/.\n/[/1/0/9/] H/. Ney and U/. Essen/. On smo othing tec hniques for bigram/-based natural language\nmo deling/. In Pr o c e e dings of International Confer enc e on A c oustics/, Sp e e ch and Signal\nPr o c essing /, pages /8/2/5/{/8/2/8/. T oron to/, ON/, /1/9/9/1/.\n/[/1/1/0/] H/. Ney and U/. Essen/. Estimating /`small/' probabilities b y lea ving/-one/-out/. In Eur o/-\np e an Confer enc e on Sp e e ch Communic ation and T e chnolo gy /, pages /2/2/3/9/{/2/2/4/2/. Berlin/,\nGerman y /, /1/9/9/3/.\n/[/1/1/1/] N/. Ney /, U/. Essen/, and R/. Kneser/. On structuring probabilistic dep endencies in sto c has/-\ntic language mo delling/. Computer/, Sp e e ch and L anguage /, /7/:/1/0/1/{/1/3/8/, /1/9/9/3/.\n/[/1/1/2/] A/. P aeseler and H/. Ney /. Con tin uous/-sp eec h recognition using a sto c hastic language\nmo del/. In Pr o c e e dings of International Confer enc e on A c oustics/, Sp e e ch and Signal\nPr o c essing /, pages /7/1/9/{/7/2/2/. Glasgo w/, Scotland/, /1/9/8/9/.\n/[/1/1/3/] D/. B/. P aul/. A CSR/-NL in terface sp eci/\fcation/. In Pr o c e e dings of D ARP A Sp e e ch and\nNatur al L anguage Workshop /, pages /2/0/3/{/2/1/4/. Cap e Co d/, MA/, Octob er /1/9/8/9/.\nBIBLIOGRAPHY /1/3/6\n/[/1/1/4/] F/. P ereira and Y/. Sc hab es/. Inside/-outside reestimation from partially brac k eted cor/-\np ora/. In Pr o c e e dings of the A nnual Me eting of the Asso ciation for Computational\nLinguistics /. New ark/, DE/, /1/9/9/2/.\n/[/1/1/5/] F/. P ereira/, N/. Tish b y /, and L/. Lee/. Distributional clustering of English w ords/. In\nPr o c e e dings of the A nnual Me eting of the Asso ciation for Computational Linguistics /,\npages /1/8/3/{/1/9/0/. Colum bus/, OH/, /1/9/9/3/.\n/[/1/1/6/] R/. Pieraccini and C/./-H/. Lee/. F actorization of language constrain ts in sp eec h recog/-\nnition/. In Pr o c e e dings of the A nnual Me eting of the Asso ciation for Computational\nLinguistics /, pages /2/9/9/{/3/0/6/, /1/9/9/1/.\n/[/1/1/7/] P /. Placew a y /, R/. Sc h w artz/, P /. F ung/, and L/. Nguy en/. The estimation of p o w erful lan/-\nguage mo dels from small and large corp ora/. In Pr o c e e dings of International Confer enc e\non A c oustics/, Sp e e ch and Signal Pr o c essing /, pages /3/3/{/3/6/. Minneap olis/, MN/, /1/9/9/3/.\n/[/1/1/8/] P /. Price/, W/. Fisher/, J/. Bernstein/, and D/. P allett/. A database for con tin uous sp eec h\nrecognition in a /1/0/0/0/-w ord domain/. In Pr o c e e dings of International Confer enc e on\nA c oustics/, Sp e e ch and Signal Pr o c essing /, /1/9/8/8/.\n/[/1/1/9/] R/. Quirk/. Corpus principles and design/. In J/. Sv artvik/, editor/, Dir e ctions in Corpus\nLinguistics /, pages /4/5/7/{/4/6/9/. Mouton de Gruyter/, New Y ork/, NY/, /1/9/9/2/.\n/[/1/2/0/] D/. R/. Reddy and A/. New ell/. Kno wledge and its represen tation in a sp eec h under/-\nstanding system/. In L/. W/. Gregg/, editor/, Know le dge and Co gnition /, pages /2/5/3/{/2/8/5/.\nL/. Erlbaum Asso c/./, W ashington/, D/.C/./, /1/9/7/4/.\n/[/1/2/1/] A/. Renouf and J/. M/. Sinclair/. Collo cational framew orks in English/. In K/. Aijmer and\nB/. Alten b erg/, editors/, English Corpus Linguistics/: Studies in Honour of Jan Svartvik /,\npages /1/2/9/{/1/4/3/. Longman/, London/, England/, /1/9/9/1/.\n/[/1/2/2/] P /. S/. Resnik/. Sele ction and Information/: A Class/-Base d Appr o ach to L exic al R elation/-\nships /. Ph/.D/. Thesis/, Computer and Information Science/, Univ ersit y of P ennsylv ania/,\nP A/, /1/9/9/3/.\n/[/1/2/3/] E/. Riseman and A/. Hanson/. A con textual p ostpro cessing system for error correction\nusing binary N/-grams/. IEEE T r ansactions on Computers /, /2/3/:/4/8/0/{/4/9/3/, /1/9/7/4/.\nBIBLIOGRAPHY /1/3/7\n/[/1/2/4/] A/. Rosen b erg/, L/. Rabiner/, S/. Levinson/, and J/. Wilp on/. A preliminary study on the\nuse of demi/-syllables in automatic sp eec h recognition/. In Pr o c e e dings of International\nConfer enc e on A c oustics/, Sp e e ch and Signal Pr o c essing /, pages /9/6/7/{/9/7/0/. FL/, /1/9/8/1/.\n/[/1/2/5/] R/. S/. Rosen b erg/. The So cial Imp act of Computers /. Academic Press/, Inc/, San Diego/,\nCA/, USA/, /1/9/9/2/.\n/[/1/2/6/] R/. Rosenfeld/. A h ybrid approac h to adaptiv e statistical language mo deling/. In Pr o/-\nc e e dings of D ARP A Sp e e ch and Natur al L anguage Workshop /, pages /7/6/{/8/1/. Princeton/,\nNJ/, Marc h/, /1/9/9/4/.\n/[/1/2/7/] G/. Rusk e and T/. Sc hotola/. The e/\u000eciency of demi/-syllable segmen tation in the recogni/-\ntion of sp ok en w ords/. In Pr o c e e dings of International Confer enc e on A c oustics/, Sp e e ch\nand Signal Pr o c essing /, pages /9/7/1/{/9/7/4/. FL/, /1/9/8/1/.\n/[/1/2/8/] Y/. Sc hab es/, M/. Roth/, and R/. Osb orne/. P arsing the Wall Street Journal with the inside/-\noutside algorithm/. In Pr o c e e dings of the Eur op e an Asso ciation for Computational\nLinguistics /, pages /3/4/1/{/3/4/7/. Utrec h t/, The Netherlands/, /1/9/9/3/.\n/[/1/2/9/] In K/. Sc hellen b erg/, editor/, Computer Studies/: Computers in So ciety /. An Ann ual Edi/-\ntion Publication/, /1/9/9/2/.\n/[/1/3/0/] R/. Sc h w artz/, Y/. Cho w/, O/. Kim ball/, S/. Roucos/, M/. Krasner/, and J/. Makhoul/. Con text/-\ndep enden t mo deling for acoustic/-phonetic recognition of con tin uous sp eec h/. In Pr o/-\nc e e dings of International Confer enc e on A c oustics/, Sp e e ch and Signal Pr o c essing /, /1/9/8/5/.\n/[/1/3/1/] R/. Sc h w artz and Y/./-L/. Cho w/. The N/-b est algorithm/: An e/\u000ecien t and exact pro cedure\nfor /\fnding the N most lik ely sen tence h yp otheses/. In Pr o c e e dings of International\nConfer enc e on A c oustics/, Sp e e ch and Signal Pr o c essing /, pages /8/1/{/8/4/. Albuquerque/,\nNM/, /1/9/9/0/.\n/[/1/3/2/] R/. Sc h w artz and Y/./-L/. Cho w/. The optimal N /-b est algorithm/: An e/\u000ecien t pro cedure\nfor /\fnding the top n sen tence h yp otheses/. In Pr o c e e dings of D ARP A Sp e e ch and\nNatur al L anguage Workshop /, Octob er/, /1/9/8/9/.\n/[/1/3/3/] R/. M/. Sc h w artz/, J/. Klo vstadt/, J/. Makhoul/, and J/. Sorensen/. A preliminary study\nof a phonetic v o co der based on a diphone mo del/. In Pr o c e e dings of International\nBIBLIOGRAPHY /1/3/8\nConfer enc e on A c oustics/, Sp e e ch and Signal Pr o c essing /, pages /3/2/{/3/5/. Den v er/, CO/,\n/1/9/8/0/.\n/[/1/3/4/] S/. Sene/\u000b/. TINA/: A probabilistic syn tactic parser for sp eec h understanding systems/. In\nPr o c e e dings of International Confer enc e on A c oustics/, Sp e e ch and Signal Pr o c essing /,\npages /7/1/1/{/7/1/4/. Glasgo w/, Scotland/, /1/9/8/9/.\n/[/1/3/5/] C/. E/. Shannon and W/. W ea v er/. The Mathematic al The ory of Communic ation /. The\nUniv ersit y of Illinois Press/, Urbana/, IL/, /1/9/6/4/.\n/[/1/3/6/] J/. M/. Sinclair/. Collo cation/: A progress rep ort/. In R/. Steele and T/. Threadgold/,\neditors/, L anguage T opics/: Essays in Honour of Michael Hal liday /, pages /3/1/9/{/3/3/1/.\nJohn Benjamins/, Philadelphi a/, P A/, /1/9/8/7/.\n/[/1/3/7/] H/. Singer and J/. L/. Gauv ain/. Connected sp eec h recognition using dissylable segmen/-\ntation/. In Pr o c e e dings of the A c oustic al So ciety of Jap an Confer enc e /, Octob er /1/9/8/8/.\n/[/1/3/8/] R/. Sinha and B/. Prasada/. Visual text recognition through con textual pro cessing/.\nPattern R e c o gnition /, /2/1/(/5/)/:/4/6/3/{/4/7/9 /, /1/9/8/8/.\n/[/1/3/9/] P /. Sm yth and R/. Go o dman/. An information theoretic approac h to rule induction\nfrom databases/. IEEE T r ansactions on Know le dge and Data Engine ering /, /4/(/4/)/:/3/0/1/{\n/3/1/6/, August /1/9/9/2/.\n/[/1/4/0/] C/. H/. Springer/, R/. E/. Herlih y /, R/. T/. Mall/, and R/. I/. Beggs/. Statistic al Infer enc e/,\nV olume Thr e e of the Mathematics for Management Series /. Ric hard D/. Irwin/, Inc/.\nHomew o o d/, IL/, Septem b er /1/9/6/8/.\n/[/1/4/1/] T/. Strzalk o wski/. TTP/: A fast and robust parser for natural language/. T ec hnical Rep ort\nPR OTEUS Pro ject Memorandum /#/4/3/, Deparmen t of Computer Science/, Couran t\nInstitute of Mathematical Sciences/, New Y ork Univ ersit y /, New Y ork/, NY/, /1/9/9/1/.\n/[/1/4/2/] N/. Sugam ura/, K/. Shik ano/, and S/. F urui/. Isolated w ord recognition using phoneme/-lik e\ntemplates/. In Pr o c e e dings of International Confer enc e on A c oustics/, Sp e e ch and Signal\nPr o c essing /, pages /7/2/3/{/7/2/6/. Boston/, MA/, /1/9/8/3/.\n/[/1/4/3/] J/. Sv artvik/. The London/-Lund corpus of sp ok en English/: Users man ual/. T ec hnical\nrep ort/, Departmen t of English/, Lund Univ ersit y /, Lund/, Sw eden/, /1/9/9/2/.\nBIBLIOGRAPHY /1/3/9\n/[/1/4/4/] T/. T ak eza w a/, K/. Kita/, J/. Hosak a/, and T/. Morimoto/. Linguistic constrain ts for con/-\ntin uous sp eec h recognition/. In Pr o c e e dings of International Confer enc e on A c oustics/,\nSp e e ch and Signal Pr o c essing /, pages /8/0/1/{/8/0/4/, /1/9/9/1/.\n/[/1/4/5/] R/. T olman/. Principles of Statistic al Me chanics /. Oxford/, Clarendon/, England/, /1/9/3/8/.\n/[/1/4/6/] M/. T omita/. An e/\u000ecien t w ord lattice parsing algorithm for con tin uous sp eec h recog/-\nnition/. In Pr o c e e dings of International Confer enc e on A c oustics/, Sp e e ch and Signal\nPr o c essing /, pages /1/5/6/9/{/1/5/7/2/, /1/9/8/6/.\n/[/1/4/7/] J/. P /. Ueb erla/. Analysing a simple language mo del /- some general conclusions for\nlanguage mo dels for sp eec h recognition/. Computer/, Sp e e ch and L anguage /, to app ear/,\n/1/9/9/4/.\n/[/1/4/8/] N/. V eilleux and M/. Ostendorf/. Probabilistic parse scoring with proso dic information/.\nIn Pr o c e e dings of International Confer enc e on A c oustics/, Sp e e ch and Signal Pr o c essing /,\npages /5/1/{/5/4/. Minneap olis/, MN/, /1/9/9/3/.\n/[/1/4/9/] A/. Viterbi/. Error b ounds for con v olutionalco des and an asymptotically optim um\ndeco ding algorithm/. IEEE T r ansactions on Information The ory /, /1/3/(/2/)/:/2/6/0/{/2 /6/9/, /1/9/6/7/.\n/[/1/5/0/] N/. W aegner and S/. Y oung/. A trellis/-based language mo del for sp eec h recognition/. In\nInternational Confer enc e on Sp oken L anguage Pr o c essing /, pages /2/4/5/{/2/4/8/. Ban/\u000b/, AB/,\n/1/9/9/2/.\n/[/1/5/1/] A/. W aib el and K/./-F/. Lee/. In tro duction/. In A/. W aib el and K/./-F/. Lee/, editors/, R e adings\nin Sp e e ch R e c o gnition /, pages /1/{/5/. Morgan Kaufmann Publishers/, Inc/, San Mateo/, CA/,\n/1/9/9/0/.\n/[/1/5/2/] D/. W alk er/. The ecology of language/. In Pr o c e e dings of the International Workshop on\nEle ctr onic Dictionaries /, pages /1/0/{/2/2/. Japan Electronic Dictionary Researc h Institute/,\nT oky o/, Japan/, /1/9/9/1/.\n/[/1/5/3/] D/. W alk er/. Dev eloping computational lexical resources/. In E/. F/. Kitta y and A/. Lehrer/,\neditors/, F r ames/, Fields and Contr asts/: New Essays in Semantic and L exic al Or gani/-\nzation /. La wrence Erlbaum Asso ciates/, Hillsdale/, NJ/,USA/, /1/9/9/2/.\nBIBLIOGRAPHY /1/4/0\n/[/1/5/4/] T/. W atanab e/. Segmen tation/-free syllable recognition in con tin uously sp ok en Japanese/.\nIn Pr o c e e dings of International Confer enc e on A c oustics/, Sp e e ch and Signal Pr o c essing /,\npages /3/2/0/{/3/2/3/. Boston/, MA/, /1/9/8/3/.\n/[/1/5/5/] P /. Witsc hel/. Constructing linguistic orien ted language mo dels for large v o cabulary\nsp eec h recognition/. In Eur op e an Confer enc e on Sp e e ch Communic ation and T e chnol/-\no gy /, pages /1/1/9/9/{/1/2/0/2/. Berlin/, German y /, /1/9/9/3/.\n/[/1/5/6/] P /. Witsc hel and G/. T/. Niedermair/. Exp erimen ts in dialogue con text dep enden t lan/-\nguage mo deling/. In /1/. Konfer enz V er arb eitung Nat / u rlicher Spr ache /, pages /3/9/5/{/3/9/9/.\nN / urn b erg/, German y /, /1/9/9/2/.\n/[/1/5/7/] J/. W righ t/. LR parsing of probabilistic grammars with input uncertain t y for sp eec h\nrecognition/. Computer Sp e e ch and L anguage /, /4/:/2/9/7/{/3/2/3/, /1/9/9/0/.\n/[/1/5/8/] G/. K/. Zipf/. The meaning/-frequency relationship of w ords/. Journal of Gener al Psy/-\ncholo gy /, /3/3/:/2/5/1/{/2/5/6/, /1/9/4/5/.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.8791849613189697
    },
    {
      "name": "Computer science",
      "score": 0.774024248123169
    },
    {
      "name": "Language model",
      "score": 0.722773551940918
    },
    {
      "name": "Strengths and weaknesses",
      "score": 0.6898735165596008
    },
    {
      "name": "Statistical model",
      "score": 0.589654266834259
    },
    {
      "name": "Natural language processing",
      "score": 0.5699816942214966
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5302376747131348
    },
    {
      "name": "Logarithm",
      "score": 0.4517410099506378
    },
    {
      "name": "Spoken language",
      "score": 0.41349127888679504
    },
    {
      "name": "Psychology",
      "score": 0.1351022720336914
    },
    {
      "name": "Mathematics",
      "score": 0.12469488382339478
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ]
}