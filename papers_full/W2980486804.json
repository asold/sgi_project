{
  "title": "MAuto: Automatic Mobile Game Testing Tool Using Image-Matching Based Approach",
  "url": "https://openalex.org/W2980486804",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A5031510588",
      "name": "J. Tuovenen",
      "affiliations": [
        "University of Oulu"
      ]
    },
    {
      "id": "https://openalex.org/A5068812101",
      "name": "Mourad Oussalah",
      "affiliations": [
        "University of Oulu"
      ]
    },
    {
      "id": "https://openalex.org/A5051336752",
      "name": "Panos Kostakos",
      "affiliations": [
        "University of Oulu"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2071751474",
    "https://openalex.org/W2048710758",
    "https://openalex.org/W6759057345",
    "https://openalex.org/W2033288554",
    "https://openalex.org/W2088749975",
    "https://openalex.org/W1812630525",
    "https://openalex.org/W2161963160",
    "https://openalex.org/W2164170598",
    "https://openalex.org/W2143058502",
    "https://openalex.org/W2091932246",
    "https://openalex.org/W2227887088",
    "https://openalex.org/W2615260684",
    "https://openalex.org/W4256148631",
    "https://openalex.org/W2016996406",
    "https://openalex.org/W2101800210",
    "https://openalex.org/W6648823533",
    "https://openalex.org/W2027999475",
    "https://openalex.org/W2473963657",
    "https://openalex.org/W2121507867",
    "https://openalex.org/W2146511370",
    "https://openalex.org/W2028916132",
    "https://openalex.org/W2963011053",
    "https://openalex.org/W2013856010",
    "https://openalex.org/W2055703785",
    "https://openalex.org/W2463553622",
    "https://openalex.org/W2249191673",
    "https://openalex.org/W2963943519",
    "https://openalex.org/W2620672884",
    "https://openalex.org/W6602708128",
    "https://openalex.org/W1976652907",
    "https://openalex.org/W1988737164",
    "https://openalex.org/W2053372694",
    "https://openalex.org/W4229614710",
    "https://openalex.org/W176206521",
    "https://openalex.org/W2141125339",
    "https://openalex.org/W2087248009",
    "https://openalex.org/W2617421253",
    "https://openalex.org/W2329807147",
    "https://openalex.org/W1541128807",
    "https://openalex.org/W78252639",
    "https://openalex.org/W2913429812",
    "https://openalex.org/W3101412407",
    "https://openalex.org/W4206737400",
    "https://openalex.org/W4297881286",
    "https://openalex.org/W2091501493",
    "https://openalex.org/W2025721496",
    "https://openalex.org/W1995362840"
  ],
  "abstract": null,
  "full_text": "Vol.:(0123456789)\nThe Computer Games Journal (2019) 8:215–239\nhttps://doi.org/10.1007/s40869-019-00087-z\n1 3\nRESEARCH\nMAuto: Automatic Mobile Game Testing Tool Using \nImage‑Matching Based Approach\nJ. Tuovenen1 · M. Oussalah1  · P . Kostakos1\nReceived: 16 May 2019 / Accepted: 10 October 2019 / Published online: 19 October 2019 \n© The Author(s) 2019\nAbstract\nThe exponential increase in the speed of the mobile industry has led to a decreas-\ning quality in many associated mobile apps. Besides, the number of distinct Android \ndevices reached thousands. This challenged the development of universally accepted \ntest applications that can run on all devices. This paper focuses on the development \nof a new mobile game testing framework, referred to, MAuto. MAuto records the \nuser actions in the game and replays the tests on any Android device. MAuto uses \nimage recognition, through AKAZE features, to record the test cases and the Appium \nframework to replay the user actions automatically. The feasibility of the developed \ntool has been demonstrated through testing on the Clash of Clans mobile game.\nKeywords Mobile game · Testing · Image recognition\n1 Introduction\nIn the era of mobile computing and internet technology, mobile gaming has seen \na huge increase, gaining almost all population groups. According to SmartInsights \n(Chaffey 2016), there are about 1.7 billion mobile users worldwide who averagely \nspend 3 h per day on their mobile phones. In another market study (see, Iqbal 2019) \nshowed that 33% of mobile phone users regularly play games on their phones where \n50% of them download apps to their phones.\nBesides, with the development of smartphones, fast mobile broadband and plat-\nform availability, mobile gaming has moved deeper into the broader culture of indi-\nviduals and communities. According to Intelligence Blog Sonders (2017), 62% of \nsmartphone users download game applications within a week after purchasing their \nphones, which is higher than any other downloaded applications. This generated \nmore than $60 billion of revenues, which are expected to exceed $100 billion by 2021 \n * M. Oussalah \n Mourad.Oussalah@oulu.fi\n1 Centre for Ubiquitous Computing, Faculty of Information Technology and Electrical \nEngineering, University of Oulu, PO Box 3500, 90114 Oulu, Finland\n216 The Computer Games Journal (2019) 8:215–239\n1 3\naccording to estimates raised by Newzo (2018). On the other hand, the ever-grow -\ning popularity of Android that constantly attract new developers and business, has \nunfortunately led to a wide discrepancy of distinct Android devices employed. As \nsuch, ensuring efficient and reliable testing of newly developed mobile game applica-\ntions becomes of paramount importance and one of the toughest challenges faced by \ngame developers, service providers as well as regulators. This is often referred to as \nquality assurance (QA) testing, which focuses on identifying technical problems with \nthe games. Although QA techniques appear almost at every stage of the software \ndevelopment lifecycle, starting from requirement eliciting to product deployment and \nmaintenance where a special interest is attributed to testing automation. The latter is \nan integral part of a continuous integration pipeline (Novak 2008) where simple auto-\nmated tests are used for basic program elements, such as individual class methods or \nseparate functions. Especially, test automation reduces the time, cost and resources, \nwhile enhancing the reliability through exposure to a large number of test cases that \ncannot be performed solely by human interaction in practice.\nCompared to traditional desktop-applications, test automation for mobile applica-\ntions bears additional challenges. First, through sandboxing, only limited access to \ninternal processes is provided, which challenges the developers to optimize resource \nallocation. Second, the general user interface navigation of mobile apps is vulner -\nable and hard to control due to uncertainty pervading the response time of the inter -\nface (s). This includes, for instance, grab  and hold like interactions. This problem \nis also referred to as the fragile test issue pointed out by Meszaros (2007). That is \nwhy it is recommended that the application functional logic should not be tested via \nthe application’s user-interface, although such rules are often violated by developers \nthemselves. Third, mobile devices are often in a steady movement, which can cause \nthe currently executed test-case for an app to break down. Fourth, often the com-\nplexity of the allocation task together with resource limitations cause a change in \nsize and resolution of the screen, which, in turn, makes any user-interface based test \nlikely to fail. Fifth, despite the effort to harmonize the software-hardware configura-\ntion in mobile platforms, the number of distinct configurations is sharply increasing, \nwhich makes the application of a single testbed very difficult. For instance, the num-\nber of distinct Android devices is exponentially increasing (e.g., more than 24,000 \ndistinct (Android) devices were reported in 2015\n1). Therefore, it is nearly impos-\nsible to test an application on every distinct device in a real environment and, at \nthe same time, provide the best user experience where the underlined application \nworks flawlessly on all other devices. Sixth, mobile games bear additional inherent \nfeatures that add extra difficulties. For instance, games involve a lot of graphics and \nother assets, which substantially increase the loading time. This, in turn, challenges \nthe efficiency of the resource allocation policy. Besides, games have inherent hooks \nthat are intended to make the player play the game again and again (Novak 2008). \nThis makes it difficult to automate the process of accessing the various passes of the \ngame. Finally, games bear a psychological factor referred to as fun factor (Novak \n2008). Indeed, even in the case of a bug-free scenario, the game can fail because \n1 https ://qz.com/47276 7/there -are-now-more-than-24000 -diffe rent-andro id-devic es/.\n217\n1 3The Computer Games Journal (2019) 8:215–239 \nthe players do not feel the fun factor so that their actions are random and not com-\nply with game rules. Because of its inherent subjectivity and vulnerability from one \nplayer to another, it is almost impossible to automate the fun factor in testing.\nDue to the above challenges and the lack of effective fully automated testing plat-\nforms, mobile app testing is still performed mostly manually, costing the develop-\ners and the industry significant amounts of effort, time, and money (Novak 2008; \nChoudhary et al. 2015; Kochhar et al. 2015). This requires attention from both the \nresearch community and practitioners. Although, setting up the test automation \nscheme would imply an additional investment, sometimes referred to as the “hump \nof pain” learning curve, the expected benefits gained from this process will return \nback such investment sooner or later (Crispin and Gregory 2011).\nIn this perspective, we present in this paper a new take on a mobile game applica-\ntion testing called MAuto. The latter aims to help the tester to create tests that work \nwith Android games. The tests can then be re-run on any other Android device. The \ntool records tests from user-interactions and exports them to Appium framework \n(Appium 2012) for playback. MAuto belongs to the class of image-based recogni-\ntion tests where AKAZE features (Alcantarilla et al. 2013) were used to recognize \nthe objects from the screenshots. When the user performs the recording, MAuto \ngenerates a test script that reproduces the recorded events. MAuto then uses Appium \nframework to perform the replay of the test script task. To validate the developed \nMAutol, tests are created with the tool for Hill Climb Racing mobile game and suc-\ncessfully executed. The rest of this paper is organized as follows. Section  2 reviews \nthe state of the art in the field of mobile testing. The description of the developed \nMAuto system is reported in Sect. 3 of this paper, while experimentation and exem-\nplification using Hill Climb Racing game are examined in Sect. 4. Section 5 summa-\nrizes the key findings and ways forward.\n2  State of Art\n2.1  Test Automation Pyramid\nThe traditional test automation pyramid introduced by Cohen (2006) is highlighted \nin Fig.  1. It consists of a three layer-pyramid corresponding to End-to-End (E2E) \ntest, an Integration test and a Unit test at the top of the hierarchy. The width of the \npyramid reflects the number of tests to be written in each layer (Knott 2015). Usu-\nally, manual testing is not part of Cohen’s test automation pyramid so it was drawn \nas a cloud on the top of the pyramid of Fig. 1 for illustration purposes only.\nMobile test automation tools are not yet good enough to support the traditional \ntest automation pyramid. Besides, mobile devices are armed with a variety of sen-\nsors (e.g., camera, accelerometer, gyroscope, infrared, GPS) and other distinguished \nfeatures (e.g., memory and CPU resources and various embedded software that \naccommodate current and future installed apps), which restrict the development of \nuniversally accepted testing tools (Knott 2015). We primarily focus on E2E testing \nbecause of its criticality.\n218 The Computer Games Journal (2019) 8:215–239\n1 3\n2.2  Types of Mobile Test Automation Tools\nWe distinguish five types of test automation modes: image-based, coordinate-based, \nOCR/text recognition, native object recognition, gesture record and replay (Knott \n2015).\n2.2.1  Image‑Based Tools\nThe key in this testing mode is to determine the type and location of icons/objects \non the screen to be matched with a set of predefined graphical elements of the game \ntaking into account the user’s actions and status of the game. More specifically, the \napplication objects and controls are stored as images, which are then compared to the \nobjects displayed on the screen to identify any potential matching. Once a match is \nfound, the pre-defined step can be re-executed. These frameworks can also associate \nspecific actions, such as clicks and text entry,  to the controls. Besides, every user-\ninterface (UI) object, which includes buttons, text boxes, selection lists, icons, among \nothers, has a set of properties that can be used to identify, define or validate the \nunderlying object (MacKenzie 2012). This provides the tester with useful and power-\nful tools for GUI-testing. As a result, the automation engineer achieves a high reus-\nable and good maintainable low-cost script development. Such a method is widely \naccepted in the field and recognized as the best practice according to test automation.\nHowever, it is acknowledged that the image recognition-like technique runs on \nelaborate and time-consuming pixel-comparison algorithms. Image-recognition \nautomation is also infeasible if application objects are dynamic. On the other hand, \nsuch tests can be fragile if the predefined graphical elements are not carefully \nFig. 1  Traditional test automation pyramid (Knott 2015)\n219\n1 3The Computer Games Journal (2019) 8:215–239 \nchosen. For instance, badly chosen algorithms or algorithm parameters can lead to \nflaky tests (Knott 2015). Therefore cautious analysis of the context is needed before \nthe application of image like technique.\n2.2.2  Coordinate‑Based Recognition\nIn this approach, user actions are captured and automated based on their associated \nx–y coordinates on the screen. This allows interactions with UI elements such as \nbuttons and images present at specific, pre-defined locations in the application UI to \nbe reproduced. However, if the screen orientation or object layout changes, scripts \nneed to be rewritten. Indeed, the test just blindly executes a given action on a given \ncoordinate so that whenever the screen size varies between devices under testing, \nthe test can be broken down easily (Knott 2015). Therefore, the approach is rarely \napplied in practice, and only very few tools provide coordinate-based identification.\n2.2.3  Optical Character/Text Recognition\nThe key in this approach is to identify the characters displayed on screens, e.g., \n“login” or “logout” button, by matching the text with the correct object on the \nscreen, to determine the relevant application control (s).\nHowever, OCR technology is dependent on the ability to identify the visible text, \nso that any blurring or screen resolution change may have a negative effect on the \nidentification accuracy. Also, such tools are not suitable to test user-interface ele-\nments that are not visible or are continuously changing. Untestable elements for \nOCR and text matching would include a list of options that are not visible such as \napplication controls that might have hidden text or dynamic text such as account bal-\nances or clocks that live-update. OCR recognition tools tend to be slower than other \ntypes of tools because they need to scan the whole screen for the text (Knott 2015). \nTherefore such techniques experience significant limits and, thereby, are commonly \nused in tandem with image-based recognition tools.\n2.2.4  Native Object Recognition\nNative object identification is based, first, on recognizing application object proper -\nties in the application code, such as ID, XPath, and, second, testing those elements. \nEspecially, native object recognition is one of the most widely used types of mobile \ntest automation tools where the UI objects are identified using the UI element tree. \nThere are many ways to access the UI elements, such as XML Path Language \n(XPath), Cascading Style Sheet (CSS) locators or the native object ID of the ele-\nment. With native object recognition, the developer can define the IDs or the loca-\ntors properly and build very robust tests. The biggest advantage of this approach is \nthat it does not depend on changes in the UI, orientation, resolution or the device \nitself (Knott 2015). The identification of programmatic objects makes this technique \nthe most resilient to changing code, and hence quite reliable, although, it requires \nmore effort and programming knowledge.\n220 The Computer Games Journal (2019) 8:215–239\n1 3\n2.2.5  Gesture Record and Replay (GRR)\nThe basis of this approach is to record screen interactions during manual testing, \nincluding every mouse movement, keystroke, and screenshot to be replicated later \non. This utility usually comes bundled as a record and a playback tool to enable \ntesters with no programming skills to record and replay the flow of a test case. The \ntest is primarily used for repetitive testing across various platforms and device mod-\nels. Since each recording is unique, this automation technique is only meaningful in \ncase of stable applications that do not involve important UI modifications. This con-\ncerns mainly quick and easy automation of unchanging flows. However, whenever \nthe environment becomes dynamic with external interruptions such as incoming \ntext and calls notifications or changes in orientation/layout, this approach has shown \nserious limitations. Many tools such as UFT and Perfecto\n2 have capture-and-replay \ncapabilities.\nFigure 2 describes the basic principle of R&R like technique (MacKenzie 2012). \nIn the record stage, the UI is connected to the business logic directly. When the \ntest is recorded, the signals from the UI are intercepted by the Recording Decorator. \nOnce the signals are stored, the decorator sends the signals to the business logic and \nthe AUT will continue as it would without the decorator. During the execution phase \nof the test, the UI is put on the sleep mode. Playback Driver reads the signals from \nthe container and sends them to the business logic.\nBesides, in practice, many test automation tools are a combination of the afore-\nmentioned types and they are not usually locked into a single object recognition \ntype. Every type has its pros/cons, so the developer has to choose the best approach \nthat fits his needs and constraints based on the mobile platform employed and the \nmobile game properties (Knott 2015). Adapted from Linares-Vásquez et al. (2017), \nTable  1 summarizes the main tools employed in Automation framework APIs, \nRecord & Replay Tools, Automated GUI-input generation tools.\nFig. 2  Record and replay using a recording decorator adapted from MacKenzie (2012)\n2 https ://www.aspir esys.com/White Paper s/QTPvs Selen ium.pdf.\n221\n1 3The Computer Games Journal (2019) 8:215–239 Table 1  Overview of automation frameworks and APIs\nName GUI-automation OS API automa-\ntion\nBlack box Test-case \nrecording\nCross-device \nsupport\nNatural language \ntest cases\nOpen source\nAutomation frameworks and APIs\n UIAutomator (2018) Yes No Either No Limited No Yes\n UIAutomation (iOS) (2015) Yes No No Yes Yes No Yes\n Espresso (2015) Yes No No No Limited No Yes\n Appium (2012) Yes No Yes Yes Limited No Yes\n Robotium (2010) Yes No Yes Yes Limited No Yes\n Roboelectric (2012) No Yes No No Yes No Yes\n Ranorex (2017) Yes No Yes Yes Yes No No\n Calabash (2012) Yes No No No No Yes Yes\n Quantum (2012) Yes N/A No N/A N/A Yes No\n Qmetry (2016) Yes N/A No N/A N/A Yes No\nGUI support Sensor support Root access \nrequired\nCross-device High-level test \ncases\nOpen source\nRecord and replay tools\n RERAN (Gomez et al. 2013) Yes No Yes No No Yes\n VALERA (Hu et al. 2015) Yes Yes Yes No No No\n Mosaic (Halpern et al. 2015) Yes No Yes Limited Yes Yes\n Barista (Fazzini et al. 2016) Yes No No Yes Yes No\n Robotium Recorder (2014) Yes No No Limited Yes No\n Xamarin Test Recorder (Xamarin \n2018)\nYes No No Yes Yes No\n ODBR (Moran et al. 2017a, b) Yes Yes Yes Limited Yes Yes\n SPAG-C (Lin et al. 2014) Yes No N/A N/A No No\n Espresso Recorder (2015) Yes No No Limited Yes Yes\n222 The Computer Games Journal (2019) 8:215–239\n1 3\nTable 1  (continued)\nTool name Instrumentation GUI exploration Types of events Replayable \ntest cases\nNL crash reports Emulators, devices Open source\nAutomated GUI-input generation tools\n Random-based input generation\n  Monkey (2016) No Random System, GUI, Text No No Both Yes\n  Dynodroid (Machiry et al. 2013) Yes Guided/random System, GUI, Text No No Emulators Yes\n  Intent Fuzzer (Sasnauskas and \nRegehr 2014)\nNo Guided/random System (Intents) No No N/A No\n  VANARSena (Ravindranath et al. \n2014)\nYes Random System, GUI, Text Yes No N/A No\n Systematic input generation\n  AndroidRipper (Amalfitano et al. \n2012)\nYes Systematic GUI, Text No No N/A Yes\n  ACTEve (Anand et al. 2012) Yes Systematic GUI No No Both No\n  A3E Depth-First (Azim and Neamtiu \n2013)\nYes Systematic GUI No No Both Yes\n  CrashScope (Moran et al. 2017a, b) No Systematic GUI, text, system Yes Yes Both No\n  Google RoboTest (2015) No Systematic GUI, text No Yes Devices No\n Model-based input generation\n  MobiGUItar (Amalfitano et al. 2014) Yes Model-based GUI, text Yes No N/A Yes\n  A3E Targeted (Azim and Neamtiu \n2013)\nYes Model-based GUI No No Both No\n  Swifthand (Choi et al. 2013) Yes Model-based GUI, text No No Both Yes\n  QUANTUM (Zaeem et al. 2014) Yes Model-based System, GUI Yes No N/A No\n  ORBIT (Yang et al. 2013) No Model-based GUI No No N/A No\n  MonkeyLab (Linares-Vásquez et al. \n2017)\nNo Model-based GUI, text Yes No Both No\n223\n1 3The Computer Games Journal (2019) 8:215–239 Table 1  (continued)\nTool name Instrumentation GUI exploration Types of events Replayable \ntest cases\nNL crash reports Emulators, devices Open source\n  Zhang and Rountev (2017) No Model-based GUI, text N/A N/A Both Yes\n Other types of input generation strategies\n  PUMA (Hao et al. 2014) Yes Programmable System, GUI, text No No Both Yes\n  JPF-Android (Van der Merwe et al. \n2014)\nNo Scripting GUI Yes No N/A Yes\n  CrashDroid (White et al. 2015) No Manual Rec/replay GUI, text Yes Yes Both No\n  Collider (Jensen et al. 2013) Yes Symbolic GUI Yes No N/A No\n  SIG-Droid (Mirzaei et al. 2015) No Symbolic GUI, text Yes No N/A No\n  Thor (Adamsen et al. 2015) Yes Test cases Test case events N/A No Emulators Yes\n  AppDoctor (Hu et al. 2014) Yes Multiple System, GUI2, text Yes No N/A No\n  EvoDroid (Mahmoud et al. 2014) No System/Evo GUI No No N/A No\n  Sapienz (Mao et al. 2016) Yes Search-based GUI, text, system Yes Yes Both Yes\n  Jabbarvand et al. (2016) Yes Search-based GUI, text, system Yes Yes Both Yes\n224 The Computer Games Journal (2019) 8:215–239\n1 3\nOn the other hand, one distinguishes noticeable tools that are of paramount \nimportance for the developers: Appium (2012) is an open-source test automation \nframework that can test native, hybrid and mobile web applications on Android, \niOS and Windows platforms. One special feature of Appium is that the develop-\ners do not have to modify the application binaries to test the application, because \nAppium uses vendor-provided automation frameworks. On the other hand, Appium \nuses WebDriver\n3 protocol to wrap the vendor-provided framework into a single API. \nWebDriver specifies a client–server protocol (known as the JSON Wire Protocol 4) \nfor the communication. The clients have been written in many major programming \nlanguages like Ruby, Python, and Java.\n5\nIn terms of software implementation, Appium sets up a server into the host \nmachine. The client, where the test logic is located, connects to the server. If the oper-\nating system of the device is Android, the server forwards the commands from the cli-\nent to the device via UI Automator frame-work (see, Fig. 3). On older Android devices \nthe server communicates with the device via Selendroid (Android API level < 17).\nClose alternative candidates to image-based automation tools are summarized \nbelow.\nSikuliX\n6: is a tool that automates everything on the screen. It is formally known as \nSikuli.7 It uses OpenCV image recognition to find the objects to click on the screen. \nSikuliX does not have support for mobile devices out of the box, but it is possible \nto make it work with simulators, emulators or VNC (virtual network computing) \nFig. 3  Appium on Android architecture\n3 http://docs.selen iumhq .org/proje cts/webdr iver/.\n4 https ://code.googl e.com/p/selen ium/wiki/JsonW irePr otoco l.\n5 http://appiu m.io/downl oads.\n6 http://www.sikul ix.com/.\n7 https ://githu b.com/sikul i/sikul i.\n225\n1 3The Computer Games Journal (2019) 8:215–239 \nsolutions where the mobile device screen can be accessed from the desktop (Yeh \net al. 2009; Chang et al. 2010).\nJAutomate (Alegroth et al. 2013): is a commercial tool combining image recog-\nnition with Record and Replay functionality. JAutomate does not support mobile \ndevices out of the box, but it is possible to make it work with simulators, emulators \nor VNC solutions where the mobile device screen can be accessed from the desktop.\nAlso, in terms of record and replay capability, one shall mention Testdroid \nRecorder\n8 a free plugin for Eclipse. 9 It is a record-and-replay tool that records user \nactions with the application under testing (AUT) and generates reusable Android \nJUnit, Robotium\n10 and ExtSolo 11 tests. The generated tests can be replayed \nafterward.\nRobotium Recorder12: is a commercial plugin for Android Studio, very similar to \nTestdroid Recorder and it can also record and replay Robotium tests.\nAppium GUI13: is a project which provides a graphical user interface (GUI) for \nAppium. There is an inspector which can tell information about the objects on the \nscreen and also a recorder that can record and replay Appium tests. Table  2 summa-\nrizes the aforementioned main application testing tools.\nNevertheless, despite the multiplicity of the mobile automation testing tools as \nhighlighted in Tables 1 and 2, the effectiveness of such tools was limited in practice, \nespecially when dealing with complex interactive mobile games as pointed out in \n[56–57], which motivates the proposed MAuto.\nTable 2  Review of main application testing tools\nDesktop support Mobile support Image recogni-\ntion capabili-\nties\nSikuliX X X\nTestdroid Recorder X\nRobotium Recorder X\nAppium GUI X\nJautomate X X\n8 https ://www.testt oolre view.de/en/testt ool-overv iew/tool-list/toold etail /642-testd roid_recor der.\n9 https ://eclip se.org/.\n10 https ://githu b.com/Robot iumTe ch/robot ium.\n11 https ://githu b.com/bitba r/robot ium-exten sions .\n12 http://robot ium.com/produ cts/robot ium-recor der.\n13 https ://githu b.com/appiu m/appiu m-dot-app.\n226 The Computer Games Journal (2019) 8:215–239\n1 3\n3  MAuto\n3.1  Motivation and Rationality\nUsually, the functionality of a mobile game is executed during the runtime stage in \na graphic container, e.g., OpenGL, to provide better graphics and interaction capa-\nbilities to users. Often, the container wraps all the functionality of a game. Thus, it \nis not possible to access the wrapped functionality to test it. Several methods have \nbeen developed to overcome this problem. The most common and effective tech-\nniques are (1) programming the container in a particular way to expose functionality \noutside the container and (2) implementing image recognition approaches to iden-\ntify functionality from the screen, which is then transmitted to the testing process.\nNevertheless, to use an image recognition-based technique, the user needs the \ngraphical representation of the object to find, e.g., buttons or game characters. \nSometimes the user can get those elements directly from the graphics designer, but \nthis is not always the case. Also, the game might change the environment and the \ncontext where the object is presented, e.g., shadows and lighting, which, in turn, will \naffect the success ratio of image recognition. Therefore, it is better to use the actual \ncontext from the game and take screenshots while playing the game.\nIn this sequel, screenshots are taken while the game is running in the mobile \ndevice and objects are extracted in a real context. Indeed, the screenshots are stored \nin the memory of the mobile device so that the user needs to transfer the image to \nhis/her machine. Once the screenshot becomes available in the user’s machine, the \nobject or partial image could be extracted from the screenshot. Finally, when every \nobject required to run the game is automatically extracted, the user can utilize these \nobjects to write the automation code that replays the sequence he played before.\nTo make the cycle above easier and faster for the user, we propose a tool called \nMAuto. Especially, MAuto will automatically take the screenshots and extract \nthe objects from those screenshots while the user is playing the game. Once the \nsequence is ready, MAuto will generate the Appium test code to replay the sequence. \nThe design and architecture of MAuto are detailed in the next section.\n3.2  General Architecture\nThe developed MAuto mobile is a new mobile game automatic testing tool, which \ntargets users without programming skills. Indeed, the tool enables generating an \nAppium test without a single line of code. From the mobile game categorization \ntechniques highlighted in Sect. 2, MAuto makes use of two of the above techniques: \nimage-based recognition and Record & Replay like technique. The image-based \napproach uses AKAZE features (accelerated KAZE features) (Alcantarilla et  al. \n2012).\nFrom an input–output perspective, MAuto overall architecture involves three ele-\nments; namely, user, browser and mobile device, and next, it generates a test script \nthat the user can run later on (see Fig. 4). Once the user has launched MAuto, all the \n227\n1 3The Computer Games Journal (2019) 8:215–239 \ninteractions between the user and the tool make use only the browser input. MAuto \ntakes care of the mobile device so that the user role is reduced to start MAuto and \ninteract with the application via a web browser. When the user performed the record-\ning task, MAuto will generate a test script that can reproduce the recorded events. \nMAuto itself is not able to replay the test, but the test script can be replayed with \nAppium.\nFig. 4  System overview highlighting its components: user, mobile and browser\nFig. 5  Architectural design\n228 The Computer Games Journal (2019) 8:215–239\n1 3\nIn Fig.  5 is described a more detailed view of the system. The two physical \ncomponents are a mobile device and the host machine. In the beginning, MAuto \ninstalls and launches the Application Under Test (AUT) and the VNC (Virtual \nNetwork Computing) server to the device. Then MAuto initiates the connection \nbetween the VNC server and the client. If a user-related event occurs, the VNC \nclient forwards this event together with its coordinates to MAuto. The latter cap-\ntures the screenshot from the mobile device, saves them in a separate database, \nand gives VNC client permission to continue its processing. VNC client sends the \nsame event to the VNC server. Next, the UI view from the mobile device will be \nFig. 6  MAuto sequence diagram\n229\n1 3The Computer Games Journal (2019) 8:215–239 \nupdated to the VNC client. When the user has finished his manipulations, MAuto \ngenerates the test script from the screenshots and events.\nIn summary, MAuto acts as an R&R tool where the tool records the user inter -\nactions and Appium is employed to replay the tests. The recording decorator is a \nmodified VNC viewer in the browser, while the replay driver is an Appium test \ntogether with the image recognition module.\nFigure 6 summarizes how the recording sequence interacts with MAuto tool. At \nfirst, the user launches the MAuto from the command line, which is also transmit-\nted to AUT so that MAuto installs the VNC client in the mobile device. Then it \ninstalls and executes AUT. MAuto runs an accessible webserver such that whenever \nthe recording task is ready, MAuto opens up the associated webpage. The user can \nthen visualize and monitor various UIs of the device so that the events associated \nwith the VNC client running on the browser can be monitored. Next, MAuto runs \na modified VNC client which will send the event (s) together with the associated \ncoordinates to MAuto. The latter saves the event, takes a screenshot from the screen \nof the mobile device and extracts the query image around the event coordinates.\nThe API call from the VNC client to MAuto is enabled after the corresponding \nimages have been processed. Then the VNC client passes the event to the device \nvia VNC protocol. In turn, the UI in the VNC client will be updated. This will \ncontinue until the user decides to stop the recording. Finally, when MAuto gets \nthe command to stop the recording, it generates the test script which can be used \nwith Appium to replay the test on any given device.\nMAuto stores the screenshots and query images to the session folder. The latter \nhas a CSV file where the events and image names are saved. The test script generator \nloads the CSV file from the disk and transforms it into Appium compatible test file.\n3.3  Image Recognition in MAuto\nMAuto calculates AKAZE features in the query image and the current screenshot. \nFast Explicit Diffusion schemes (FED) are used in AKAZE to speed up the fea-\nture detection in nonlinear scale-spaces. AKAZE also introduces Modified-Local \nDifference Binary (M-LDB) to preserve low computational demand and storage \nrequirements. Once both features (at the original image model and current image) \nare calculated, thresholding is employed to compare those features to ascertain \nwhether the query image is currently shown on the screen and ascertain its coordi-\nnates accordingly, see Fig.  7 for a detailed implementation description. Examples \nof experimental results using these features are reported in Sect.  4 of this paper, \nsee, e.g., Fig.  12 where green circles are the calculated features and the red lines \nare the matching features in both images. Especially, once the matching features \nare identified, we calculate the average coordinate from the inliers to get the coor -\ndinate of the query image in the screenshot.\n230 The Computer Games Journal (2019) 8:215–239\n1 3\n4  Exemplification and Evaluation\nMAuto tool has been tested and validated using Clash of Clans Android mobile \ngame (version 8.551.4) (available from Supercell 14). Clash of Clans (CoC) is \na mobile Massive Multiplayer Online Game (MMO/M-MOG) where the player \nbuilds a community, trains troops and attacks other players to earn assets. The \ngame has a tutorial that the player should pass to play the game. The tutorial \nguides the player to click certain elements to continue the game. Therefore, if \nthe tutorial can be passed without serious bugs, it is likely that the game works \nproperly. Besides, since the variations are quite limited in the tutorial, this makes \nit a good test subject for MAuto. During the recording phase of the MAuto, the \nbrowser pops up indicating the readiness to start the recording task as seen in \nFig. 8.\nFig. 7  Pseudo-code implementation of AKAZE based image recognition algorithm\n14 http://super cell.com/.\n231\n1 3The Computer Games Journal (2019) 8:215–239 \nThe first view which requires user interaction in Clash of Clans is the impor -\ntant notice view, see Fig.  9. This view is accessible using the native object rec-\nognition tool. The object’s resource ID is android:id/button3 whose associated \npackage is com.supercell.clashofclans.\nClash of Clans requires the user to select one Google Play account for the \ngame, accessible using the native object recognition as well (see Fig. 10).\nThe subsequent views are no longer accessible using native object recogni-\ntion, therefore, the use of image recognition is needed. See an example of this \nFig. 8  Browser’s view in case of Clash of Clans\nFig. 9  The first view which requires interaction in Clash of Clans\n232 The Computer Games Journal (2019) 8:215–239\n1 3\ntask in Fig.  11. The green circles are the calculated features and the red lines \nare the matching features in both images. Once we have the matching features, \nwe calculate the average coordinate from the inliers to get the coordinate of the \nquery image in the screenshot.\nAn example of a file created by MAuto in order to save the click coordinates, \nscreenshots and query images is shown in Fig.  12, while an example of Appium \nscript in MAuto is reported in the “ Appendix” of this paper.\nNext, the Record and Replay phase is carried out using Appium test script \nthat takes into account the image recognition based approach. An example of \nAppium test script for this purpose is shown in “ Appendix”.\nFig. 11  Example of image recognition using AKAZE features—query image is correctly matched from \nthe screen\nFig. 10  Google Play account selection\n233\n1 3The Computer Games Journal (2019) 8:215–239 \n5  Discussion\nIt is very time consuming and prone to errors for human testers to take regular \nscreenshots from the device, transfer them to the host machine and crop appropri-\nate query image. However, such a repetitive task can more efficiently be automated. \nMAuto is designed to do so and, thereby, decrease the amount of required manual \nwork. It can take the screenshots and transfer the images to the host machine auto-\nmatically. More specifically, MAuto crops the images properly and creates reusa-\nble tests through the appropriate use of Appium. To demonstrate its feasibility and \ntechnical soundness, MAuto was used to create automated test scripts for Clash of \nClans mobile game. Strictly speaking, although MAuto does not automate every -\nthing, still it can significantly improve the speed of test automation script creation. \nNevertheless, the selected query images have a huge impact on test stability on other \ndevices. Indeed, the query image must have a good layout for the AKAZE features \nto be matched appropriately on the screen. Figure 13a highlighted an example of the \nquery image of click where MAuto and AKAZE found only 4 features so that most \nlikely this query image cannot be found from the screen when the test is run (see \nFig. 13b).\nThe current version of MAuto has a predefined box to crop the query image from \nthe click-coordinate and, sometimes, the box size becomes too small to contain a \nusable number of features. To circumvent this limitation, the user needs to manually \ncrop a better query image from the screenshot to expect enhanced results.\nFig. 12  Example of MAuto output file\n234 The Computer Games Journal (2019) 8:215–239\n1 3\nInitially, the user should use native object recognition whenever possible. Indeed, \nnative object recognition is found to be relatively stable, thereby, once available, \nsuch an approach should be privileged over MAuto. The latter often takes shortly \nafter the native object recognition step.\nWhen creating the tests, the screenshot operation is quite slow. It can last few \nseconds to take the screenshot. This means the usability of the application in the \nbrowser is not the same as in the device without MAuto. It is also harder to play \ngames through the browser than in the device.\nMAuto cannot work with fast-paced mobile games, because it is too slow. It takes \nrelatively too much time to transfer the screenshot from a mobile device to the host. \nTherefore, it is almost impossible to play fast-paced games with MAuto, because the \ngame can end up in a couple of seconds without new inputs.\nMany recording tools do far better when native object recognition can be used \nin the application. If a native object recognition cannot be used, then MAuto will \ntake over. However, we should notice that it is not possible to give inputs to mobile \ndevice sensors through MAuto. This means that it is not possible to test directly \ngames that use sensor data. Although Appium has some support for sensor inputs, \nMAuto cannot record those inputs.\n6  Conclusion\nThis paper focused on mobile game testing. We reviewed the motivation, key \nmilestones and challenges pervading the development of automated mobile game \ntesting tools. Especially, we highlighted why mobile game testing and test auto-\nmation are harder than testing traditional mobile applications. One of the key \nreasons lies in the fact that native object recognition is less applicable to games \nwhere the use of additional object recognition methods, like image recognition, \nFig. 13  Example of bad AKAZE matching\n235\n1 3The Computer Games Journal (2019) 8:215–239 \nis necessary. Besides, the acknowledged fun factor renders traditional sequential \nlike approaches quite inefficient. A review of existing technologies revealed five \nkey approaches for mobile game testing: image-based, coordinate-based, OCR/\ntext recognition, native object recognition, and gesture record/replay. Image-\nbased recognition test has shown increased performance, although with limited \nscope.\nTools to create image-based recognition test scripts have not reached maturity yet \nand still are under development. This paper has introduced a testing tool, MAuto, \nto make it easier to create automated mobile game tests. The approach is based on \na fruitful combination of AKAZE features, Appium, record and replay, and native \nobject recognition. Evaluation and testing have been conducted using the Clash of \nClans game. It is stressed that a good choice of query images is required to make the \ntest stable for production use.\nMAuto is a very raw approach to solve challenging mobile automatic testing \nproblems. With some polishing, MAuto would work on slow games, but it does not \nwork with fast games that require rapid user interactions.\nMAuto allows the user to create Appium tests with image recognition without \ncoding a single line of code. The main target is to help the developers to test mobile \ngames, but the tool can be used for other application types as well.\nAs a perspective work, it would be a good idea to make the cropped image size \ndynamic. At the moment it is a static 10 pixel square around the click-coordinate. \nWhen cropping the query image we could calculate the number of features in the \nimage and if there are fewer features than 20 for example, then the algorithm should \nincrease the query image size and calculate the features again until the image has a \ngood amount of features. This would decrease the manual work the user has to do \nto fix the low-quality query images. It should be quite easy to add iOS support to \nMAuto as well. Appium works al-ready for Android and iOS. The corner problems \nare to find a way to take a screenshot from the iOS device and to find a quality \nVNC client for iOS. The image recognition solution MAuto can therefore easily be \nextended to an iOS environment.\nTo overcome the slowness of MAuto, especially in the recording phase, one \nsolution could be to compress the image in the mobile device and then send it to \nthe host machine. Another solution consists of tapping into the Android operating \nsystem and remove the VNC solution completely. From an input–output perspec-\ntive, MAuto takes the user inputs from a desktop browser, which is not an ideal way \nto interact with a mobile device. It would be better for instance to trap the inputs \ndirectly from the screen of the mobile device and transfer the clicks and images to \nthe host machine after the test has been recorded. Indeed, if the test recording would \nbe in the mobile device, MAuto might be able to trap the sensor inputs and write \nthose inputs to tests as well.\nAcknowledgements Open access funding provided by University of Oulu including Oulu University \nHospital. This work is partly supported by EU Horizon 2020 project Cutler (ID # 770469) on Coastal \nUrban Development through the Lenses of Resiliency and EU YoungRes (#823701) project on Youth \npolarization.\n236 The Computer Games Journal (2019) 8:215–239\n1 3\nCompliance with Ethical Standards \nConflict of interest No conflict of interest.\nOpen Access This article is distributed under the terms of the Creative Commons Attribution 4.0 Interna-\ntional License (http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, \nand reproduction in any medium, provided you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons license, and indicate if changes were made.\nAppendix: Example of Appium Script in MAuto\n\n237\n1 3The Computer Games Journal (2019) 8:215–239 \nReferences\nAdamsen, C. Q., Mezzetti, G., & Møller A. (2015). Systematic execution of android test suites in adverse \nconditions. In Proceedings of the International Symposium on Software Testing and Analysis \n(ISSTA) ISSTA’15, pp. 83–93.\nAlcantarilla, P. F., et al. (2013). Fast explicit diffusion for accelerated features in nonlinear scale spaces. \nIn British Machine Vision Conference, Bristol, BMVC.\nAlcantarilla, P., Bartoli, A., & Davison, A. (2012). Kaze features. In A. Fitzgibbon, S. Lazebnik, P. \nPerona, Y. Sato, & C. Schmid (Eds.), Computer vision—ECCV 2012. Lecture Notes in Computer \nScience (Vol. 7577, pp. 214–227). Berlin, Heidel-berg: Springer.\nAlegroth, E., Nass, M., & Olsson, H. (2013) JAutomate: A tool for system-and acceptance-test automa-\ntion. In IEEE sixth international conference on software testing, verification and validation (ICST), \npp. 439–446.\nAmalfitano, D., Fasolino, A. R., Tramontana, P., De Carmine, S., & Memon, A. M. (2012). Using GUI \nripping for automated testing of android applications. In Proceedings of the 27th IEEE/ACM inter -\nnational conference automated software engineering (ASE’12), pp. 258–261.\nAmalfitano, D., Fasolino, A. R., Tramontana, P., Ta, B. D., & Memon, A. (2014). Mobiguitar—A tool for \nautomated model-based testing of mobile apps. IEEE Software, 32, 53–59.\nAnand, S., Naik, M., Harrold, M. J., & Yang, H. (2012). Automated concolic testing of smartphone apps. \nIn Proceedings of the 20th ACM international symposium on the foundations of software engineer -\ning (FSE’12), North Carolina.\nAppium. (2012). Appium testing framework. Retrieved September 2019, from http://appiu m.io.\nAzim, T., & Neamtiu, I. (2013). Targeted and depth-first exploration for systematic testing of android \napps. In Proceedings of the ACM SIGPLAN international conference on object oriented program-\nming systems languages & applications, OOPSLA’13, pp. 641–660.\nCalabash. (2012). Calabash testing framework. http://calab a.sh.\nChaffey, D. (2016). Mobile marketing statistics compilation. Retrieved September 2019, from Smart \nInsights: http://www.smart insig hts.com/mobil e-marke ting/mobil e-marke ting-analy tics/mobil \ne-marke ting-stati stics /.\nChang, T. H., Yeh, T. & Miller, R. C. (2010). GUI testing using computer vision. In Proceedings of the \nSIGCHI conference on human factors in computing systems, CHI’10 (pp. 1535–1544). New York, \nNY: ACM. http://doi.org/10.1145/17533 26.17535 55.\nChoi, W., Necula, G., & Sen, K. (2013). Guided GUI testing of android apps with minimal restart and \napproximate learning. In OOPSLA’13, pp. 623–640.\nChoudhary, R. S., Gorla, A., & Orso A. (2015). Automated test input generation for android: Are we \nthere yet? (E). In Proceedings of the ACM/IEEE international conference on automated software \nengineering, pp. 429–440.\nCohen, M. (2006). Agile estimation and planning. Upper Saddle River: Prentice-Hall.\nCrispin, L., & Gregory, J. (2011). Agile testing A practical guide for testers and agile teams (7th ed.). \nBoston: Addison-Wesley.\nEsspresso. (2015). Esspresso testing framework. https ://googl e.githu b.io/andro id-testi ng-suppo rt-libra ry/\ndocs/espre sso/.\nFazzini, M., Freitas, E. N., Choudhary, S. R., & Orso, A. (2016). Barista: A technique for recording, \nencoding, and running platform independent android tests. In Proceedings of the IEEE international \nconference on software testing, verification and validation (ICST).\nGomez, L., Neamtiu, I., Azim, T., & Millstein, T. (2013). Reran: Timing- and touch-sensitive record and \nreplay for android. In Proceedings of the 33th international conference on software engineering \n(ICSE), pp. 72–81.\nGoogle Robo Test. (2015). Google firebase test lab robo test. Retrieved September 2019, from https ://\nfireb ase.googl e.com/docs/test-lab/robo-ux-test.\nHalpern, M., Zhu, Y., Peri, R., & Reddi, V. G. (2015). Mosaic: Cross-platform user-interaction record and \nreplay for the fragmented android ecosystem. In Proceedings of the IEEE international symposium \non performance analysis of systems and software, pp. 215–224.\nHao, S., Liu, B., Nath, S., Halfond, W., & Govindan, R. (2014). Puma: Programmable ui-automation for \nlarge-scale dynamic analysis of mobile apps. In MobiSys’14, pp. 204–217.\nHu, Y., Azim, T., & Neamtiu, I. (2015). Versatile yet lightweight record-and replay for android. In OOP-\nSLA’15, ser. OOPSLA 2015 (pp. 349–366). New York, NY: ACM.\n238 The Computer Games Journal (2019) 8:215–239\n1 3\nHu, G., Yuan, X., Tang, Y., & Yang, J. (2014). Efficiently, effectively detecting mobile app bugs with app \ndoctor. In EuroSys’14, pp. 18:1–18:15.\nIqbal, M. (2019). App downloaded and usage statistics, Business of Apps, August 7th 2019, https ://www.\nbusin essof apps.com/data/app-stati stics /. Accessed September 2019.\nJabbarvand, R., Sadeghi, A., Bagheri, H., & Malek, S. (2016). Energy-aware test-suite minimization \nfor android apps. In Proceedings of the international symposium on software testing and analysis \n(ISSTA), pp. 425–436.\nJensen, C. S., Prasad, M. R., & Moller, A. (2013). Automated testing with targeted event sequence gen-\neration. In Proceedings of the international symposium on software testing and analysis (ISSTA), \npp. 67–77.\nKnott, D. (2015). Hands-on mobile app testing: A guide for mobile testers and anyone involved in the \nmobile app business (1st ed.). New York: Addison-Wesley Professional.\nKochhar, P. S., Thung, F., Nagappan, N., Zimmermann, T., & Lo, D. (2015). Understanding the test auto-\nmation culture of app developers. In Proceedings of the 8th IEEE international conference on soft-\nware testing, verification and validation, Austria, ICST’15.\nLin, Y., Rojas, J. F., Chu, E., & Lai, Y. (2014). On the accuracy, efficiency, and reusability of automated \ntest oracles for android devices. IEEE Transactions on Software Engineering, 40(10), 2014.\nLinares-Vásquez, M., Moran, K., & Poshyvanyk, D. (2017). Continuous, evolutionary and large-scale: A \nnew perspective for automated mobile app testing. In Proceedings of the 33rd IEEE conference of \nsoftware maintenance and evolution (ICSME).\nMachiry, A., Tahiliani, R., & Naik, M. (2013). Dynodroid: An input generation system for android apps. \nIn Proceedings of the 9th joint meeting of the European Software Engineering Conference and \nthe ACM SIGSOFT Symposium on the Foundations of Software Engineering (ESEC/FSE’13), pp. \n224–234.\nMacKenzie, B. (2012). Top 10 mobile application testing automation tool requirements. Retreived April \n7, 2018 from http://north wayso lutio ns.com/blog/top-10-mobil e-appli catio n-testi ngaut omati on-tool-\nrequi remen ts/.\nMahmood, R., Mirzaei, N., & Malek, S. (2014). EvoDroid: Segmented evolutionary testing of android \napps. In Proceedings of the 10th Joint Meeting of the European Software Engineering Conference \nand the ACM SIGSOFT Symposium on the Foundations of Software Engineering (ESEC/FSE’14), \npp. 599–609.\nMao, K., Harman, M., & Jia, Y. (2016). Sapienz: Multi-objective automated testing for android appli-\ncations. In Proceedings of the 25th international symposium on software testing and analysis, pp. \n94–105.\nMeszaros, G. (2007). xUnit test patterns: Refactoring test code. Pearson Education.\nMirzaei, N., Bagheri, H., Mahmood, R., & Malek, S. (2015). Sig-droid: Automated system input genera-\ntion for android applications. In ISSRE’15, pp. 461–471.\nMonkey Testing. (2016). Android ui/application exerciser monkey. Retrieved September 2019, from \nhttp://devel oper.andro id.com/tools /help/monke y.html.\nMoran, K., Bonett, R., Bernal-Cárdenas, C., Otten, B., Park, D., & Poshyvanyk, D. (2017). On-device \nbug reporting for android applications. In Proceedings of the 4th IEEE/ACM international confer -\nence on mobile software engineering and systems (MobileSOFT).\nMoran, K., Linares-Vásquez, M., Bernal-Cárdenas, C., Vendome, C., & Poshyvanyk, D. (2017). Crash-\nscope: A practical tool for automated testing of android applications. In ICSE’17 Companion, pp. \n15–18.\nNewzo. (2018). Global Game Market Report, see sample by Tom Wijman at https ://newzo o.com/insig \nhts/artic les/globa l-games -marke t-reach es-137-9-billi on-in-2018-mobil e-games -take-half/. Accessed \nJune 2018.\nNovak, J. (2008). Game development essentials: An introduction. Game development essentials series, \nThomson/Delmar Learning.\nQmetry. (2016). Qmetry test automation framework. https ://qmetr y.githu b.io/qaf/.\nQuantum. (2012). Quantum. https ://commu nity.perfe ctomo bile.com/posts /12860 12-intro ducin g-quant \num-frame work.\nRanorex. (2017). Ranorex testing framework. http://www.ranor ex.com.\nRavindranath, L., Nath, S., Padhye, J., & Balakrishnan, H. (2014). Automatic and scalable fault detec-\ntion for mobile applications. In Proceedings of the 12th annual international conference on mobile \nsystems, applications, and services, MobiSys’14 (pp. 190–203). New York, NY: ACM. http://doi.\norg/10.1145/25943 68.25943 77.\n239\n1 3The Computer Games Journal (2019) 8:215–239 \nRoboelectric, (2012). Roboelectric testing framework. Retrieved September 2019, from http://robol ectri \nc.org.\nRobotium. (2010). Robotium testing. Retrieved September 2019, from https ://githu b.com/Robot iumTe ch/\nrobot ium.\nRobotium Recorder, (2014). Robotium recorder. https ://githu b.com/Robot iumTe ch/robot ium.\nSasnauskas, R., & Regehr, J. (2014). Intent fuzzer: Crafting intents of death. In Proceedings of the Joint \n12th International Workshop on Dynamic Analysis (WODA) and Workshop on Software and System \nPerformance (WODA & PERTEA), pp. 1–5.\nSonders, M. (2017). New mobile game statistics every game publisher should know in 2016. Retrieved \nfrom Survey Monkey: https ://www.surve ymonk ey.com/busin ess/intel ligen ce/mobil e-game-stati stics \n/. Accessed August 2017.\nUi-Automation iOS. (2015). Apple ui-automation documentation. Retrieved September 2019, from https \n://web.archi ve.org/web/20140 81219 5854/https ://devel oper.apple .com/libra ry/ios/docum entat ion/\nDevel operT ools/Refer ence/UIAut omati onRef /_index .html.\nUiautomator. (2018). Android uiautomator. http://devel oper.andro id.com/tools /help/uiaut omato r/index \n.html.\nVan der Merwe, H., Van der Merwe, B., & Visser, W. (2014). Execution and property specifications for \njpf-android. SIGSOFT Software Engineering Notes, 39(1), 1–5.\nWhite, M., Linares-Vásquez, M., Johnson, P., Bernal-Cárdenas, C., & Poshyvanyk, D. (2015). Generating \nreproducible and replayable bug reports from android application crashes. In ICPC’15.\nXamarin. (2018). Retrieved September 2019, from https ://www.jimbo bbenn ett.io/ui-testi ng-your-xamar \nin-apps/.\nYang, W., Prasad, M., Xie, T. (2013). A grey-box approach for automated GUI-model generation of \nmobile applications. In Proceedings of the 6th international conference on fundamental approaches \nto software engineering (FASE), pp. 250–265.\nYeh, T., Chang, T. H., & Miller R. C. (2009) Sikuli: Using GUI screenshots for search and automation. \nIn Proceedings of the 22nd annual ACM symposium on User Interface Software and Technology, \nUIST’09 (pp. 183–192). New York, NY: ACM. http://doi.org/10.1145/16221 76.16222 13.\nZaeem, R. N., Prasad M. R., & Khurshid S. (2014). Automated generation of oracles for testing user-\ninteraction features of mobile apps. In ICST’14, pp. 183–192.\nZhang, H., & Rountev, A. (2017). Analysis and testing of notifications in android wear applications. In \nProceedings of the 39th international conference on software engineering (ICSE), May 2017.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7815341949462891
    },
    {
      "name": "Android (operating system)",
      "score": 0.7805973291397095
    },
    {
      "name": "Mobile device",
      "score": 0.5095353722572327
    },
    {
      "name": "Human–computer interaction",
      "score": 0.43989232182502747
    },
    {
      "name": "Video game development",
      "score": 0.4129945635795593
    },
    {
      "name": "Game design",
      "score": 0.23856180906295776
    },
    {
      "name": "Operating system",
      "score": 0.20286095142364502
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98381234",
      "name": "University of Oulu",
      "country": "FI"
    }
  ]
}