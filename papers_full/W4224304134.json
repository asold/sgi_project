{
  "title": "Towards Lightweight Transformer Via Group-Wise Transformation for Vision-and-Language Tasks",
  "url": "https://openalex.org/W4224304134",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2126126454",
      "name": "Gen Luo",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2143179490",
      "name": "Yiyi Zhou",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2126149465",
      "name": "Xiaoshuai Sun",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A1484673654",
      "name": "Yan Wang",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2651443321",
      "name": "Liujuan Cao",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2105710542",
      "name": "Yongjian Wu",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2696333288",
      "name": "Feiyue Huang",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2137291500",
      "name": "Rongrong Ji",
      "affiliations": [
        "Xiamen University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2966683369",
    "https://openalex.org/W2986670728",
    "https://openalex.org/W6767494712",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2964067226",
    "https://openalex.org/W3002557610",
    "https://openalex.org/W6769120734",
    "https://openalex.org/W3034655362",
    "https://openalex.org/W6763643401",
    "https://openalex.org/W2990818246",
    "https://openalex.org/W3092767330",
    "https://openalex.org/W2970869018",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W6766904570",
    "https://openalex.org/W6771321870",
    "https://openalex.org/W6767211374",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W639708223",
    "https://openalex.org/W6768080748",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W6768851824",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W6771251021",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W6764409202",
    "https://openalex.org/W6783944145",
    "https://openalex.org/W6779163297",
    "https://openalex.org/W6771626834",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W6737664043",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W2883780447",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W2964350391",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2963125010",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2963518342",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W2561715562",
    "https://openalex.org/W6639432524",
    "https://openalex.org/W2251512949",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2915716523",
    "https://openalex.org/W6758657797",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W3035284526",
    "https://openalex.org/W2963170456",
    "https://openalex.org/W2949376505",
    "https://openalex.org/W2890531016",
    "https://openalex.org/W2965697393",
    "https://openalex.org/W2983141445",
    "https://openalex.org/W2963383024",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W3175095612",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W6794345597",
    "https://openalex.org/W6797533273",
    "https://openalex.org/W6798901973",
    "https://openalex.org/W2558535589",
    "https://openalex.org/W2963735856",
    "https://openalex.org/W2984121207",
    "https://openalex.org/W2779827764",
    "https://openalex.org/W2964345792",
    "https://openalex.org/W2964022527",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W6678262379",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W2506483933",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W6787972765",
    "https://openalex.org/W2963717374",
    "https://openalex.org/W6728881024",
    "https://openalex.org/W2963150162",
    "https://openalex.org/W2747623286",
    "https://openalex.org/W2964072591",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W6748270630",
    "https://openalex.org/W2760103357",
    "https://openalex.org/W6738893770",
    "https://openalex.org/W2998631105",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W3035682985",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W6752083267",
    "https://openalex.org/W2571175805",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W6763509872",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W3103022576"
  ],
  "abstract": "Despite the exciting performance, Transformer is criticized for its excessive parameters and computation cost. However, compressing Transformer remains as an open problem due to its internal complexity of the layer designs, i.e., Multi-Head Attention (MHA) and Feed-Forward Network (FFN). To address this issue, we introduce Group-wise Transformation towards a universal yet lightweight Transformer for vision-and-language tasks, termed as LW-Transformer. LW-Transformer applies Group-wise Transformation to reduce both the parameters and computations of Transformer, while also preserving its two main properties, i.e., the efficient attention modeling on diverse subspaces of MHA, and the expanding-scaling feature transformation of FFN. We apply LW-Transformer to a set of Transformer-based networks, and quantitatively measure them on three vision-and-language tasks and six benchmark datasets. Experimental results show that while saving a large number of parameters and computations, LW-Transformer achieves very competitive performance against the original Transformer networks for vision-and-language tasks. To examine the generalization ability, we apply LW-Transformer to the task of image classification, and build its network based on a recently proposed image Transformer called Swin-Transformer, where the effectiveness can be also confirmed.",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\nTowards Lightweight Transformer via Group-wise\nTransformation for Vision-and-Language Tasks\nGen Luo, Yiyi Zhou*, Xiaoshuai Sun, Member, IEEE, Yan Wang, Member, IEEE, Liujuan Cao, Yongjian Wu,\nFeiyue Huang, Rongrong Ji, Senior Member, IEEE\nAbstract‚ÄîDespite the exciting performance, Transformer is\ncriticized for its excessive parameters and computation cost.\nHowever, compressing Transformer remains as an open problem\ndue to its internal complexity of the layer designs, i.e., Multi-\nHead Attention (MHA) and Feed-Forward Network (FFN). To\naddress this issue, we introduce Group-wise Transformation to-\nwards a universal yet lightweight Transformer for vision-and-\nlanguage tasks, termed as LW-Transformer1. LW-Transformer\napplies Group-wise Transformation to reduce both the param-\neters and computations of Transformer, while also preserving\nits two main properties, i.e., the efÔ¨Åcient attention modeling on\ndiverse subspaces of MHA, and the expanding-scaling feature\ntransformation of FFN. We apply LW-Transformer to a set of\nTransformer-based networks, and quantitatively measure them\non three vision-and-language tasks and six benchmark datasets.\nExperimental results show that while saving a large number of\nparameters and computations, LW-Transformer achieves very\ncompetitive performance against the original Transformer net-\nworks for vision-and-language tasks. To examine the generaliza-\ntion ability, we also apply our optimization strategy to a recently\nproposed image Transformer called Swin-Transformer for image\nclassiÔ¨Åcation, where the effectiveness can be also conÔ¨Årmed.\nIndex Terms‚ÄîLightweight Transformer, Visual Question An-\nswering, Image Captioning, Reference Expression Comprehen-\nsion.\nI. I NTRODUCTION\nT\nRANSFORMER [1] has become the model of choice in\nthe Ô¨Åeld of natural language processing due to its superior\ncapability of modeling long-range dependencies and learning\neffective representations. Its great success has also attracted\nthe attention of the computer vision community [2]‚Äì[5]. At\npresent, Transformer and its variants have been applied to a\nvariety of vision-and-language tasks, such as visual question\nanswering (VQA) [2], [6], [7], image captioning [3], [8]‚Äì\n[10], and referring expression comprehension[4], [11], achiev-\ning dominant performance in multiple benchmarks. Besides,\nTransformer also yields a new trend of large-scale multi-modal\npretraining [12]‚Äì[21], which greatly promotes the development\nof joint vision-and-language learning.\nComing with the outstanding ability of long-distance depen-\ndency modeling, Transformer-based models are also known\nG. Luo, Y . Zhou, X. Sun, L. Cao and R. Ji are with the Media An-\nalytics and Computing Lab, Department of ArtiÔ¨Åcial Intelligence, Xiamen\nUniversity, China e-mail: (luogen@stu.xmu.edu.cn, zhouyiyi@xmu.edu.cn,\ncaoliujuan@xmu.edu.cn, xssun@xmu.edu.cn, rrji@xmu.edu.cn).\nY . Wang is with Microsoft, China e-mail: (yanwang@ee.columbia.edu ).\nY . Wu and F. Huang are with the Tencent Technology (Shanghai) Co., Ltd,\nChina e-mail: (littlekenwu@tencent.com, garyhuang@tencent.com).\n*Corresponding Author: Yiyi Zhou (E-mail: zhouyiyi@xmu.edu.cn)\n1Source codes are released https://github.com/luogen1996/LWTransformer.\nfor their extreme demand on computation power and memory\nspace. Such an issue will become more prominent when\napplying Transformer to vision-and-language tasks, where\nthe multi-modal network is typically built based on a large\nvisual backbone, e.g., ResNet [22] or Faster R-CNN [23]. For\ninstance, VL-BERT-large [18] needs to be trained on 8 GPUs\nin parallel for nearly 20 days, and its parameters are about\n340 millions, which takes about 1.2 GB of memory to deploy.\nThus, the large latency and memory footprint greatly hinder\nthe application of Transformer on the mobile devices.\nTo this end, the network compression for Transformer-based\nmodels has attracted signiÔ¨Åcant research attention [24]‚Äì[28].\nPopular directions include distillation [25], [27], layer prun-\ning [24] and quantization [26], [28]. Beyond these method-\nologies, one orthogonal direction is to directly optimize the\nTransformer layer itself [29]‚Äì[31], which has not been paid\nenough attention and is the focus of this paper. By designing\na more efÔ¨Åcient version of Transformer that still preserves the\naccuracy, one can directly train a model based on the improved\nstructure to avoid the training-distilling process [32], which\ncan be also combined with the popular compression methods\nto further improve their efÔ¨Åciency.\nHowever, designing an efÔ¨Åcient Transformer remains as\na challenging problem, mainly due to the unique structures\nof the Multi-Head Attention (MHA) and the Feed-Forward\nNetwork (FFN). First, as a key component in Transformer,\nMHA is used to capture the dependency of input features in\nvarious subspaces, upon which efÔ¨Åcient feature transforma-\ntions can be obtained [1]. In this case, these transformations\nin MHA not only map features to new semantic spaces but\nalso compute the attention scores using the softmax function.\nSecond, as a layer-to-layer connection design, FFN achieves\nefÔ¨Åcient transformations by expanding and scaling the feature\ndimensions, which inevitably involves a large number of\nparameters. Both designs pose a huge obstacle in reducing\nTransformer parameters.\nIn this paper, we introduce Group-wise Transformation to\nachieve a lightweight Transformer for vision-and-language\ntasks, termed LW-Transformer, which well addresses the afore-\nmentioned issues in a principled way. Group-wise Transforma-\ntion is an efÔ¨Åcient method used in many convolution neural\nnetworks (CNNs) [33]‚Äì[38]. It splits the input features into k\ngroups by channels to perform parameterized transformations,\nand then concatenates the transformed features as the output.\nThis group-wise operation can reduce the parameters and\ncomputations of the original transformation by k‚àí1\nk . Based on\nthis strategy, we propose the group-wise multi-head attention\narXiv:2204.07780v1  [cs.CV]  16 Apr 2022\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\nInput\nFeature Split\n(a) Multi-head attention and group-wise multi-head attention.\nX\nùêóùüè ùêóùüê\nùíç√óùíÖ\nùíç√óùíÖ\nùíå\nhead0\nQ\nK\nV head1\nQ\nK\nV head2\nQ\nK\nV head3\nQ\nK\nV\nAttention\n Attention\n Attention\n Attention\nAggregation\nùíç√óùíÖ\nùíç√óùíÖ\nhead0\nQ\nK\nV head1\nQ\nK\nV head2\nQ\nK\nV head3\nQ\nK\nV\nAttention\n Attention\n Attention\n Attention\nùíç√óùíÖ\nX\n(b) Feed-forward network and group-wise\nfeed-forward network.\nùíç√óùíÖ\nùíç√óùíÖ\nX\nExpanding\nProjection\nScaling\nProjection\nùíç√óùíÖ\nùíç√óùíÖ\nExpanding\nProjection\nScaling\nProjection\nùêóùüè ùêóùüê\nScaling\nProjection\nFeature Split\nAggregation\nX\nFig. 1: Illustration of Group-wise Multi-Head Attention (G-MHA) and Group-wise Feed-Forward Network (G-FFN). (a) The\nproposed G-MHA divides input feature into k groups, of which the multi-head attention is conducted, and the outputs are\nconcatenated to one feature. (b) G-FFN Ô¨Åsrt performs expanding projections and then conducts group-wise linear transformation.\n(G-MHA) and group-wise feed forward networks (G-FFN) for\nLW-Transformer.\nNotably, in addition to improve model compactness, LW-\nTransformer can well maintain the desired principles of the\noriginal Transformer. The effectiveness of Transformer mainly\nlies in its MHA design [1], which captures the diverse fea-\nture dependencies through different feature subspaces. These\nsubspaces are obtained by truncating the input features after\nlinear projections, as shown in Fig. 1. The principle of Group-\nwise transformation is essentially consistent with that of MHA,\nwhich also aims to learn more efÔ¨Åcient feature representations\nthrough more diverse feature spaces. In this case, the proposed\nG-MHA takes a step forward than MHA, which directly\nsplits input features before projections. More importantly, this\noperation does not change neither the feature dimension nor\nthe scaled-dot product for attention modeling. Meanwhile, G-\nFFN also maintains the expanding-scaling property of FFN\nwhile reducing the computation and parameters to a large\nextent, as shown in Fig. 1.\nDuring experiments, we build LW-Transformer based on the\nstructures of the default Transformer and its variants, including\nMCAN [2], LXMERT [19] and Swin-Transformer [39], and\nconduct extensive experiments on six benchmark datasets, i.e.,\nGQA [40], VQA2.0 [41], CLEVR [42], COCO [43], Ref-\nCOCO [44], RefCOCO+ [44], of three language-and-vision\ntasks, i.e., VQA [41], image captioning [43] and referring\nexpression comprehension (REC) [45]. We also test LW-\nTransformer on the large-scale and BERT-style pretrain-\ning [19] and the task of image classiÔ¨Åcation. The experimental\nresults show that with an overall reduction of up to 45%\nof parameters and 28% of computations 2, LW-Transformer\nstill achieves competitive performance against the default\nTransformer on six benchmark datasets of the three VL tasks\nand even obtains performance gains on some benchmarks, e.g.,\n+1% on GQA, +0.1% on COCO Captioning and +3.2% on\nRefCOCO, which well conÔ¨Årms its effectiveness for vision-\n2Under the setting of LWTransformer 1√ó.\nand-language tasks. In addition, its generalization ability is\nalso well validated on ImageNet and CIFAR-100. Compared\nwith Swin-Transformer, LW-Transformer can obtain the same\nperformance on CIFAR-100 while saving 42.4% computations\nand 43.3% parameters. On ImageNet, LW-Transformer can\neven obtain better performance with similar experimental cost\nto Swin-Transformer.\nAs a summary, our contributions are three-fold:\n‚Ä¢ We present the Ô¨Årst application of Group-wise transfor-\nmation to Transformer, and propose an efÔ¨Åcient Trans-\nformer network called LW-Transformer.\n‚Ä¢ On six benchmarks of three language-and-vision tasks,\nLW-Transoformer achieves competitive performance\nagainst the default Transformers with a reduction of up to\n45% parameters and 28% computations. Its generalization\nability is also validated on the recently proposed Swin-\nTransformer [39] for image classiÔ¨Åcation.\n‚Ä¢ We conduct extensive experiments to examine the opti-\nmizations of different designs in Transformer, which can\nprovide useful insights for its future optimization.\nII. R ELATED WORK\nDue to the superior capability of learning efÔ¨Åcient represen-\ntations, Transformer [1] is becoming popular in both natural\nlanguage processing [1], [29], [46], [47] and computer vi-\nsion [2], [3], [6]‚Äì[10]. A set of Transformer-based approaches\nhave been proposed to achieve the state-of-the-art performance\nin VQA [2], [6], [7], [48] and image captioning [3], [9], [10],\n[49]‚Äì[53]. For example, the recent X-Linear Transformer [49]\nintegrates bilinear pooling [54] and squeeze-excitation mod-\nule [55] into self-attention to selectively capitalize on visual\ninformation and to perform multimodal reasoning. With these\nnovel designs, X-Linear Transformer improves the SOTA\nperformance of image captioning on the CIDEr [56] metric by\n+1.6. Meanwhile, these Transformer-based networks also lead\nto a trend of large-scale language-and-vision pre-training [14]‚Äì\n[19], [21], [57], which break the best performance of various\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\ntasks, such as Visual Question Answering [58] and Image\nCaptioning [43]. In addition, a lot of vision transformers [39],\n[59] have been proposed recently, which shows the superior\nperformance on image classiÔ¨Åcation. For example, XCiT [59]\nachieves 86.0 % top-1 accuracy on ImageNet, outperforming\nconventional CNNs by a large margin. Despite the great\nsuccess, Transformer and its variants have been criticized for\nthe excessive parameters and high computation overhead. For\ninstance, the parameter sizes of BERT [13], ViLBERT [16]\nand LXMERT [19] are 85, 221 and 181 millions, respectively.\nSuch a large amount of parameters greatly hinder the appli-\ncations of Transformer and Transformer-based models on the\nmobile applications, where both the storage and computation\nresources are limited.\nSeveral recent advances [29]‚Äì[31] are committed to de-\nsign the efÔ¨Åcient Transformer networks. SpeciÔ¨Åcally, Lin-\nformer [31] reduces the computation cost of self-attention\nby compressing the sequence length of input features. Per-\nformer [30] applies the scalable kernel methods to approx-\nimate the scale-dot product attention to reduce computation\noverhead when the sequence length is very large. Tensorized\nTransformer [29] uses tensor decomposition to reduce the\nparameters of Q, K and V in self-attention. Compared with\nthese efÔ¨Åcient Transformers, the proposed LW-Transformer\nneither changes the length of input sequences nor the deÔ¨Ånition\nof attention modeling.\nThe principle of the introduced Group-wise Transformation\nis also related to the group convolution, which can be traced\nback to a set of classical convolutional neural networks [33]‚Äì\n[38], [60] like AlexNet [34]. The depth-wise separable convo-\nlution in Xception [33] generalized the method into Inception\nNetworks [36], [37]. ShufÔ¨ÇeNets [38] and MobileNets [35],\n[60] use the group-wise and depth-wise separable convolution\nto achieve lightweight deep networks with competitive per-\nformance. However, due to the huge difference between the\nprinciples of multi-head self-attention (MHA) and convolution,\nits application to Transformer networks is still left unexploited.\nExcept for improving model efÔ¨Åciency, the motivation of this\npaper is different from the mentioned CNNs. Existing methods\nlike Xception [33] use group-wise transformation to realize the\nmulti-branch structure, so as to increase the capacity of the\nmodel. In contrast, our application is to inherit the property\nof global dependency in self-attention, as discussed in the\nintroduction.\nIII. A PPROACH\nIn this section, we Ô¨Årst recap the main components of\nTransformer, introduce Group-wise Transformation, and then\ndescribe the proposed LW-Transformer in detail.\nA. Preliminary\nIn Transformer, each layer typically contains two main com-\nponents, i.e., Multi-Head Attentions (MHA) for dependency\nmodeling and Feed-Forward Network (FFN) for position-wise\ntransformations.\nMulti-Head Attention is used to capture the dependency\nbetween input features by the scaled dot-product attention [1],\nupon which efÔ¨Åcient representations are learned. SpeciÔ¨Åcally,\ngiven the transformed matrices of query Q ‚àà Rn√ód, key\nK ‚àà Rn√ód, and value V ‚àà Rn√ód, the scaled dot-product\nattention function is formulated as:\nAttention(Q,K,V) =Softmax(KT Q‚àö\nd\n)V. (1)\nHere, n and d denote the sequential length and the feature\ndepth, respectively. To further improve the representational\nability, Vaswaniet al. [1] extends Eq. 1 to a multi-head version,\ndenoted as multi-head attention (MHA), which consists h\nparalleled ‚Äúheads‚Äù to make the scale dot-product attention.\nSpeciÔ¨Åcally, given the input features X ‚àà Rn√ód, MHA is\nformulated as:\nMHA(X) = [head1,...,head h],\nwhere headi = Attention(Qi,Ki,Vi). (2)\nHere, Qi, Ki and Vi are the split features for the i-th head,\nand they are obtained by truncating the input features X after\nprojection. Meanwhile, a linear-merge projection is used after\nMHA, deÔ¨Åned as:\nO = MHA(X)WO, (3)\nwhere WO ‚ààRd√ód is the weight matrix and O ‚ààRn√ód is\nthe Ô¨Ånal output.\nFeed-Forward Network makes a position-wise and dense\ntransformation from the input features to the output ones,\nwhich keeps the non-linearity by expanding the hidden units\nin the ReLU layer, as shown in Fig. 1 (c). This design is\nbeneÔ¨Åcial to prevent manifold collapse caused by ReLU, as\nwell as to extend the model capacity [35]. The formulation of\nFFN can be written as\nFFN(X) =max(0,XW1 + b1)W2 + b2. (4)\nHere W1 ‚ààRd√ódf , W2 ‚ààRdf √ód, b1 ‚ààRdf and b2 ‚ààRd are\nweights and bias, and max(¬∑) is the ReLU activation. Typically,\ndf is a quadruple as large as d.\nB. Lightweight Transformer\nTo achieve the lightweight Transformer, we optimize its two\nmain designs, i.e., MHA and FFN, with Group-wise transfor-\nmation. In this subsection, we begin with the introduction of\nGroup-wise transformation, and then describe its deployment\non MHA and FFN in detail.\nGroup-wise transformation. As shown in Fig. 1, given the\ninput features X ‚ààRn√ód, we Ô¨Årst divide it into k groups.\nThose features of different groups are transformed by the func-\ntion œÑ(¬∑) and then concatenated again as one complete output.\nThe formulation of Group-wise Transformation, denoted as\nœÑGroup (¬∑), is deÔ¨Åned by\nœÑGroup (X) = [X‚Ä≤\n1,..., X‚Ä≤\nk],\nwhere X‚Ä≤\ni = œÑ(Xi; Œ∏) . (5)\nHere, Xi ‚àà Rn√ód\nk denotes the truncated features of the\ni-th group, [¬∑] denotes concatenation and œÑ(¬∑) can be any\nparameterized function, e.g., linear/ non-linear projections or\nmore complex modules like self-attention [1].\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\nWith the split-transform-concatenate design, the Group-\nwise Transformation can save k‚àí1\nk of parameters and k‚àí1\nk of\nthe computations. To further reduce the number of parameters,\nwe can also share the learnable weights for each group,\nconsistently resulting in k2‚àí1\nk2 reductions of parameters.\nGroup-wise Multi-Head Attention. SpeciÔ¨Åcally, we extend\nthe original Multi-Head Attention (MHA) in Transformer to a\ngroup-wise version (G-MHA), as shown in Fig. 1 (a).\nAs described in Sec. III-A, MHA obtains the multiple\nattentions via truncating Q, K and V after the projection of\nX. In this case, G-MHA can be considered as its extension\nin principle, which directly splits the input features before\nprojection. Concretely, the input features X ‚ààRn√ód are Ô¨Årst\ndivided into k splits Xi ‚ààRn√ód/k, based on which the multi-\nhead attentions are performed. Afterwards, those attention\noutputs are concatenated again. Then, G-MHA is deÔ¨Åned as:\nG-MHA(X) = [œÑ(X1),...,œÑ (Xk)],\nwhere œÑ(Xi) =MHA(Xi). (6)\nAs shown in Fig. 1 (a), the group operation of G-MHA is\nconducted on the feature dimension instead of the sequence\nlength. Therefore, each feature can still obtain its coupling\ncoefÔ¨Åcients to the others through the comparisons on the\ntruncated features, similar to the default MHA. To this end,\nthe attention patterns learned by G-MHA can be close to that\nof MHA in principle.\nNotably, we do not replace the linear-merge layer in MHA,\ni.e., Wo in Eq.2, which is founded to be important for expand-\ning the learning capacity of Transformer and facilitating the\ncross-group information exchange. Overall, G-MHA reduces\nthe parameter size from 4d2 to about 2d2, and the computa-\ntional complexity from O(4nd2 +n2d) to about O(nd2 +n2d).\nGroup-wise Feed-Forward Network. We further deploy\nGroup-wise Transformation to FFN, of which implementa-\ntion is illustrated in Fig. 1 (b). SpeciÔ¨Åcally, given an input\nX ‚àà Rn√ód, the group-wise feed-forward network (G-FFN)\nperforms non-linear projections to expand the feature size by\nH = max(0,XW1 + b1), (7)\nwhere W1 ‚àà Rd√ódf and b1 ‚àà Rdf . Then, the obtained\nhidden features H ‚ààRn√ódf are divided into k independent\nfeatures Hi ‚ààRn√ó\ndf\nk , which is further linearly transformed\nwith sharable weights. G-FFN can be formulated as:\nG-FFN(X) = [œÑ(H1),...,œÑ (Hk)],\nwhere œÑ(Hi) =HiW2 + b2, (8)\nwhere W2 ‚ààR\ndf\nk √ód\nk and b2 ‚ààR\nd\nk . Compared to the original\nFFN, G-FFN reduces the parameter size from O( d2) to O( d2\nk2 )\nand the complexity from O( nd2) to O( nd2\nk ).\nAs shown in Eq. 8 and Fig. 1 (b), G-FFN can still maintains\nthe expanding-scaling property of the default FFN. Notably,\nthe group-wise operation is only deployed on the last linear\nlayer, while the Ô¨Årst non-linear projection remains intact. To\nexplain, this design is beneÔ¨Åcial to protect the non-linearity of\nFFN and prevent the manifold collapse [35], therefore avoiding\nthe performance degeneration.\nTABLE I: Notations for different settings of LW-Transformer.\nNotations Description\nn√ó(the sufÔ¨Åx) The expanding multiples of projection dimensions\nin Q and K.\nmini (the sufÔ¨Åx) The mini version illustrated in Tab.III.\nk The number of groups in the transformations.\nWS Weight sharing.\nG-MHA Group-wise multi-head attention.\nG-FFN Group-wise feed-forward network.\nG-LML Group-wise linear-merge layer in MHA.\nG-IL Group-wise intermediate layer in FFN.\nIV. E XPERIMENTS\nTo validate the proposed LW-Transformer, we apply it to a\nset of Transformer and BERT-style models, and conduct exten-\nsive experiments on six benchmark datasets, i.e., VQA2.0 [41],\nGQA [40], CLEVR [42], MS-COCO [43], RefCOCO [44], Re-\nfCOCO+ [44], of three vision-and-language tasks, i.e., Visual\nQuestion Answering (VQA) [58], Image Captioning (IC) [43]\nand Referring Expression Comprehension (REC) [45]. To\nexamine its generalization ability, we build LW-Transformer\nbased on the newly proposed Swin-Transformer [39] for image\nclassiÔ¨Åcation.\nA. Deployed Networks\nTransformer: For VQA, IC and REC, we use the Transformer\nnetworks proposed in [1], [2] as our baseline model, which\nall follow a classical encoder-decoder structure [1]. In the\nbaseline, we set the d, df , dh deÔ¨Åned in Eq.1 - 2, as 512, 2,048,\n64, respectively. The number of attention heads his 8 for each\nTransformer layer, and both the encoder and decoder branches\nare composed of six Transformer layers. For simplicity, we de-\nnote the baseline network as Transformer. We further replace\nTransformer with the proposed LW-Transformer layers, and\nkeep the rest designs unchanged. We denote the compressed\nnetwork as LW-Transformer. In LW-Transformer, the basic\nsettings of d, df , dh are the same as Transformer. The number\nof groups is set to 2 by default, and each group has 4 attention\nheads, so the total number of attentions is kept as 8 as\nTransformer.\nBert-style Model : The deployed Bert-style model is the\nrecently proposed LXMERT [19], of which structure is slightly\ndifferent from the conventional Transformer network. It has\n9 and 6 Transformer encoder layers for the language and\nvision modelings, respectively, and 5 cross-modal Transformer\nlayers3 for the multi-modal interactions. During experiments,\nwe replace the encoder and decoder Transformer layers with\nthe proposed LW-Transformer layer. The model settings are\nsimilar to the standard bert [13], i.e., d =768, df = 3156,\ndh = 96, k= 2, h= 6. For simplicity, we denote the network\nwith our optimization methods as LW-LXMERT. The detailed\nexplanation of the notations is given in Tab. I.\nDuring experiments, we also examine the sensitivity in\nmapping dimensions of query Q and key K towards the\nmodel performance. Hence, we add an sufÔ¨Åx n√óafter LW-\nTransformer to indicate the change in attention dimensions of\n3Cross-modal Transformer layer has two MHA and FFN.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\nQ and K. For instance, dh of LW-Transformer1√óis 64, while\nthe one of LW-Transformer 3√ó is 192.\nSwin-Transformer: For image classiÔ¨Åcation, we use Swin-\nTransformer-tiny [39] as our baseline structure. Swin-\nTransformer-tiny is a hierarchical Transformer, which contains\n12 Transformer blocks in 4 stages. During experiments, we\nsimply replace the MHA and FFN of the original Swin-\nTransformer-tiny with our G-MHA and G-FFN. The modiÔ¨Åed\nmodel is denoted as LW-Transformer. To compare with the\noriginal Swin-Transformer under the same parameters, we also\nbuild a large model containing 16 Transformer blocks, namely\nLW-Transformer-large. Meanwhile, we remove weight-sharing\nin our model, since we Ô¨Ånd that it degrades the performance\nof image classiÔ¨Åcation.\nB. Datasets\nVisual Question Answering : We conduct experiments on\nthree vqa benchmark datasets, i.e., VQA2.0 [41], GQA [40]\nand CLEVR [42]. VQA2.0 contains about 1.1M image-\nquestion pairs from real word, in which there are 440K\nexamples for training, 210K for validation, and 450K for\ntesting. CLEVR is a synthetically generated dataset that aims\nto test the visual reasoning ability of models. It includes 700K\nand 150K examples for training and test, respectively. GQA\ncontains 22M questions over 140K images, which is designed\nto test the visual reasoning ability of models in real scenes. In\nterms of the evaluation metric, we use the VQA accuracy [58]\nfor VQA2.0, and the classiÔ¨Åcation accuracy for GQA and\nCLEVR.\nReferring Expression Comprehension : RefCOCO [45] and\nRefCOCO+ [45] datasets are used in our experiments. Both\ndatasets contain nearly 140K referring expressions for 50K\nbounding boxes of 20K images. The categories of TestA are\nabout people and the ones of TestB are about objects. Ref-\nCOCO has more descriptions related to the spatial relations,\nwhile RefCOCO+ excludes these spatial-related expressions\nand adds more appearance-related ones. Following [61]‚Äì[68],\nwe use the top-1 accuracy as the metric on both datasets.\nImage Captioning : COCO Captioning [43] contains more\nthan 120K images from MS-COCO [43], each of which is\nannotated with 5 different captions. We train and test our mod-\nels with the Karpathy splits [69]. BLEU [70], METEOR [71],\nROUGE [72], CIDEr [56] and SPICE [73] are used as the\nmetrics for evaluation.\nImage ClassiÔ¨Åcation : ImageNet-1K [74] is the most widely-\nused benchmark for image classiÔ¨Åcation, which contains\n1.28M training images and 50K validation images from 1,000\nclasses. Cifar-100 [75] is a benchmark containing 60,000 low-\nresolution images from 100 classes. In Cifar-100, there are\n50,000 images for training and 10,000 images for validation.\nC. Experiment Setups\nTransformer: For all datasets except CLEVR [42], we use the\nregional features from [81] as the visual inputs. On VQA2.0,\n3The parameters of the language and vision encoders, i.e., CNN and LSTM,\nare not counted.\nGQA and IC, all networks are trained for 13 epochs, 3 of\nwhich are for warming-up. The basic learning rate is set to 1e-\n4√ó\n‚àö\nk, where kdenotes the number of groups, and decayed on\n10 epochs and 12 epochs with a factor of 0.2. For CLEVR, we\nfollow the previous works [42], [82]‚Äì[85] to use grid features\nextracted by ResNet101 [22]. The model is trained for up to\n16 epochs and warmed up for 3 epochs. The basic learning\nrate is set to 4e-4 √ó\n‚àö\nk and decayed by 0.2 on 13 epochs and\n15 epochs. For IC, another 17 training epochs is further used\nfor the CIDEr-D [51], [56] optimization with a learning rate of\n5e-6. The Adam optimizer [86] is used to train all networks.\nBert-style Model : Following the settings of LXMERT [19],\nthe visual features are extracted by Faster R-CNN [23]\nwith ResNet101 [22] backbone pretrained on the Visual\nGenome [87]. The training procedures are divide into two\nsteps, i.e., pretraining and Ô¨Ånetune. We follow the default\nsetting of LXMERT to pre-train the model. The pretraining\ntakes 20 epochs overall, where the optimizer is Adam [86]\nwith a initial learning rate of 1e-4 √ó\n‚àö\nk. We then Ô¨Ånetune the\nmodel on VQA and GQA with a learning rate of 1e-5 √ó\n‚àö\nk\nfor 5 epochs.\nSwin-Transformer: Following the default settings of Swin-\nTransformer [39], we use the AdamW optimizer with an initial\nlearning rate of 0.001 and a weight decay of 0.05. We use\na cosine decay learning rate scheduler with 20 epochs of\nlinear warm-up. Models are trained for total 300 epochs with\n1024 batchsize. For ImageNet and Cifar-100, we adopt the\ninput image resolution of 224 √ó224. Following the training\nstrategy of Swin-Transformer, a set of data augmentations, i.e.,\nRandAugment, Mixup, Cutmix, random erasing and stochastic\ndepth, are applied to avoid overÔ¨Åtting.\nD. Experiment Results\n1) Ablation Study: We Ô¨Årst examine different designs of\nthe proposed LW-Transformer including group-wise multi-\nhead attention (G-MHA), group-wise feed-forward network\n(G-FFN) and weight sharing (WS), and also evaluate the\nsensitivity of hyper-parameters like the group number ( k) and\nthe dimensions of query and key in MHA (indicated by the\nsubÔ¨Åx n√óafter LW-Transformer, i.e., dh deÔ¨Åned in Eq. 2).\nThe results are reported in Tab. II.\nThe effect of group-wise transformation. The second\nblock of Tab. II illustrates the impact of G-MHA, G-FFN\nand WS on the parameter size, computation cost and over-\nall performance of LW-Transformer. From these results we\ncan see that while greatly reducing the model parameters\nand computations, each of these designs will hardly reduce\nmodel performance, and even help the model obtain slightly\nimprovements on COCO and RefCOCO. In addition, after\ndeploying all designs, i.e., LW-Transformer1√ó, the parameters\nand computations are reduced by 45.5% and 28.3%, respec-\ntively. However, the performance drop is still very marginal,\ne.g., -0.1% on VQA2.0, -0.08% on COCO and -0.12% on\nRefCOCO. Thus, we can conclude that group-wise designs\nin LW-Transformer can effectively reduce both parameter size\nand computation cost, while maintaining the performance.\nThe impact of expanding attention dimensions. We\nexplore the effects of expanding the attention dimensions\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6\nTABLE II: Ablation Study. These variations are conducted on VQA val set, COCO Caption Karpathy set and RefCOCO val\nset, respectively. WS refers to weight sharing and k is the number of groups. MAdds is multiplication-addition [60] used to\nindicate the computation cost.\nModel G-MHA G-FFN WS k #Params 3 MAdds VQA2.0 COCO* RefCOCO\nTransformer - - - - 44.1M 2.58G 67.17 117.0 80.8\nLW-Transformer 1 √ó ‚úì \u0017 \u0017 2 37.0M 2.21G 67.11 116.7 80.9\nLW-Transformer 1 √ó \u0017 ‚úì \u0017 2 37.7M 2.22G 67.15 117.1 81.0\nLW-Transformer 1 √ó ‚úì ‚úì \u0017 2 32.4M 1.85G 67.13 117.1 80.9\nLW-Transformer\n‚Ä†\n1 √ó ‚úì ‚úì ‚úì 2 24.0M 1.85G 67.10 116.9 80.7\nLW-Transformer 2 √ó ‚úì ‚úì ‚úì 2 26.3M 2.16G 67.14 117.1 80.8\nLW-Transformer\n‚Ä†\n3 √ó ‚úì ‚úì ‚úì 2 28.7M 2.46G 67.19 117.2 80.9\nLW-Transformer 3 √ó ‚úì ‚úì ‚úì 4 21.9M 1.83G 66.68 115.9 80.0\nLW-Transformer 3 √ó ‚úì ‚úì ‚úì 8 19.8M 1.51G 66.29 114.6 79.3\n* CIDEr is used as the metric. These results are before the CIDEr optimization stage.\n‚Ä† denotes the model for the further experiments.\nTABLE III: Effects of deploying Group-wise Linear-merge Layer(G-LML) and Group-wise Intermediate Layer(G-IL). The\ncomparisons are conducted on VQA val set, COCO Caption Karpathy set and RefCOCO val set.\nModel G-LML G-IL #Params MAdds VQA2.0 COCO* RefCOCO\nTransformer [1] - - 44.1M 2.58G 67.17 117.0 80.8\nLW-Transformer 1 √ó \u0017 \u0017 24.0M 1.85G 67.10 116.9 80.7\nLW-Transformer 1 √ó ‚úì \u0017 20.4M 1.69G 66.93 116.5 80.2\nLW-Transformer mini\n1 √ó \u0017 ‚úì 14.5M 1.50G 66.57 115.7 79.8\nLW-Transformer 1 √ó ‚úì ‚úì 11.0M 1.33G 66.36 114.8 79.3\n* CIDEr is used as the metric. These results are before the CIDEr optimization stage.\nTABLE IV: Comparisons of LW-Transformer mini\n1√ó with exist-\ning multi-modal fusion networks on VQA test-dev. All models\nare trained on train+val set and tested on test-dev set.\nModel #Params All Y/N Num Other\nMCB [54] 32M 61.23 79.73 39.13 50.45\nTucker [76] 14M 64.21 81.81 42.28 54.17\nMLB [77] 16M 64.88 81.34 43.75 53.48\nMFB [78] 24M 65.56 82.35 41.54 56.74\nMUTAN [76] 14M 65.19 82.22 42.10 55.94\nMFH [79] 48M 65.72 82.82 40.39 56.94\nBLOCK [80] 18M 66.41 82.86 44.76 57.30\nLW-Transformermini\n1√ó 14.5M 69.68 86.03 51.62 59.80\nof query and key in MHA, of which results are given in\nthe third block of Tab.II. Here, the sufÔ¨Åxes, i.e., 1√ó and\n3√ó, indicate the multipliers of dimension expansion. From\nthese results, we observe that expanding the MHA dimen-\nsion further improve the model performance, which helps\nLW-Transformer outperform the original Transformer on all\ndatasets with much fewer parameters. For instance, compared\nwith LW-Transformer1√ó, the efÔ¨Åciency of LW-Transformer 2√ó\nis relatively less signiÔ¨Åcant, but it still reduce 40% parameters\nand 16.3% computations, respectively. More importantly, it\nhas almost the same performance to the default Transformer.\nWe also notice that the increased parameter size is still small,\ne.g., +4.7M for 3√ó. In contrast, expanding the projection di-\nmensions signiÔ¨Åcantly increases the model size in the original\nMHA, e.g., +18.8M for 3√ó. To explain, the input features in G-\nMHA are Ô¨Årst truncated into multiple groups, which will leads\nto less parameters than the direct projection. This observation\nalso conÔ¨Årms another merit of Group-wise transformation.\nThe impact of group number. Next, we test LW-\nTransformer with different group numbers ( k), as reported\nin the last block of Tab. II . From this table, we notice\nthat although increasing the group number further reduces the\nmodel size, the performance degradation becomes relatively\nobvious. For this case, our understanding is that both the\nattention modeling and feature transformation require a certain\ncapacity to learn the large-scale language-and-vision data, e.g.,\nVQA2.0, and setting too small feature dimensions will be\ncounterproductive. In addition, since some layers are not in-\ncluded in our optimization scheme, e.g., the linear merge layer\nin MHA, the lower bound of LW-Transformer‚Äôs parameter\nsize is about 20M. After experiments, we Ô¨Ånd that a trade-\noff between the efÔ¨Åciency and the performance is k= 2.\n2) Fully Group-wise LW-Transformer: In LW-Transformer,\nGroup-wise transformation is not applied in two inner layers\nof Transformer, i.e., linear merge layer of MHA and the\nintermediate layer of FFN. In practice, we Ô¨Ånd that these\ntwo layers are important for keeping the model capacity and\nfacilitating the cross-group information exchange. In Tab. III,\nwe quantitatively validate the effects of deploying Group-wise\ntransformation on these two layers, denoted as Group-wise\nLinear-Merge Layer (G-LML) and Group-wise Intermediate\nLayer (GIL), respectively. From Tab. III, we can see that\nwhen G-LML and G-IL are deployed, the parameters and\ncomputations are further reduced by 29.5% and 19.7%, respec-\ntively, but the performance declines are relatively signiÔ¨Åcant,\ne.g., from 67.10 to 66.36 on VQA and from 116.9 to 114.8\non COCO. To explain, the completely group-wise structure\ndivides the network into two independent sub-networks with-\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7\nTABLE V: Comparisons of LW-Transformer between Group-wise Transformation (LW-Transformer) and efÔ¨Åcient transform-\ners [29]‚Äì[31]. The comparisons are conducted on VQA val set, COCO Caption Karpathy set and RefCOCO val set, respectively.\nMAdds i.e., multiplication-additions [60], is used to indicate the computation cost.\nModel #Params 3 MAdds VQA2.0 COCO* RefCOCO\nTransformer [1] 44.1M 2.58G 67.17 117.0 80.8\nTensorized Transformer [29] 27.5M 3.71G 67.03 116.3 80.6\nPerformer [30] 44.1M 3.36G 65.20 116.2 80.0\nLinformer [31] 44.2M 2.53G 64.20 - 80.3\nLW-Transformer 1 √ó 24.0M 1.85G 67.10 116.9 80.7\nLW-Transformer 3 √ó 28.7M 2.46G 67.19 117.2 80.9\n* CIDEr is used as the metric. These results are before the CIDEr\noptimization stage.\nTABLE VI: Comparisons of LW-Transformer and SOTAs on VQA tasks in single-model setting.\nVQA2.0 CLEVR GQA\nmodel test-dev model test model test-dev\nBottom-Up [81] 65.3 SAN [42] 76.7 Bottom-Up [81] 49.7\nMFH [79] 68.8 RN [84] 95.5 MAC [2] 54.1\nBAN [88] 70.0 FiLM [83] 97.7 LCGN [85] 57.1\nMCAN [2] 70.6 MAC [82] 98.9 BAN [88] 57.1\nTransformer [1] 70.6 Transformer [1] 98.4 Transformer [1] 57.4\nLW-Transformer 1 √ó 70.4 LW-Transformer 1 √ó 98.6 LW-Transformer 1 √ó 58.4\nLW-Transformer 3 √ó 70.5 LW-Transformer 3 √ó 98.7 LW-Transformer 3 √ó 57.5\nTABLE VII: Comparisons of LW-Transformer and SOTAs on REC tasks in single-model setting.\nRefCOCO RefCOCO+\nmodel testA testB model testA testB\nSpe+ Lis +Rl [89] 73.1 64.9 Spe+ Lis +Rl [89] 60.0 49.6\nDDPN [66] 80.1 72.4 DDPN [66] 70.5 54.1\nMattNet [65] 81.1 70.0 MattNet [65] 71.6 56.2\nNMTree [63] 81.2 70.1 NMTree [63] 72.0 57.5\nTransformer [1] 84.0 73.4 Transformer [1] 75.9 61.1\nLW-Transformer 1 √ó 84.2 73.7 LW-Transformer 1 √ó 75.9 61.0\nLW-Transformer 3 √ó 83.9 74.3 LW-Transformer 3 √ó 76.5 60.8\nout any cross-group information exchange, which ultimately\nresults in the performance degeneration. Based on these ob-\nservations, we therefore keep the linear-merge layer of MHA\nand the intermediate layer of FFN intact in LW-Transformer.\nIn addition, we also notice that retaining G-IL is the best\nchoice after the design introduced in the paper, where both\ncompactness and performance are satisfactory. Therefore, we\nfurther extend LW-Transformer to a mini version, i.e., deploy-\ning G-IL and keeping the linear-merge layer intact for cross-\ngroup information exchange, termed as LW-Transformer mini\nand compare it with the existing multi-modal fusion net-\nworks [54], [76]‚Äì[80], of which results are shown in Tab. IV.\nFrom Tab. IV, we Ô¨Ånd that even compared to state-of-the-art\nmethods, i.e., BLOCK [80], LW-Transformermini\n1√ó outperforms\nit by a large margin, while the number of parameters is much\nsmaller, i.e., 14.5M vs 18M. Such results greatly support\nthe effectiveness of LW-Transformer under the extremely\nlightweight settings. Meanwhile, it also suggests that the liner\nmerge layer (LML) is an important design in MHA, which\nneeds to be carefully considered during optimization.\n3) Comparisons with other EfÔ¨Åcient Transformers: In\nTab. V, we also compare LW-Transformer with existing ef-\nÔ¨Åcient transformers including Tensorized Transformer [29],\nPerformer [30] and Linformer [31]. In Tab. V, Tensorized\nTransformer applies Tucker Decomposition, a popular method\nin network compression [29], to decompose the projection\nweights in MHA. Performer [30] approximates the scaled-\ndot product attention by using scalable kernel methods. Lin-\nformer [31] uses additional projection layers to reduce the\nlength of the input sequence, which is hard to apply to\napply to the sequence-to-sequence generation tasks like Image\nCaptioning. So, we only report the results of Linformer on\nVQA2.0 and RefCOCO.\nFrom Tab. V, we can see that LW-Transformer obtains better\nperformance than the other compressing methods, and its per-\nformance gains are very distinct on VQA2.0. For these results,\nour understanding is that changing the original deÔ¨Ånition of\nself-attention will affect the effectiveness of Transformer to\nsome degree. For instance, Linformer reduces the length of\nthe features for attention, which inevitably leads to the loss of\ninformation for Ô¨Åne-grained tasks like VQA. The performance\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8\nTABLE VIII: Comparisons of LW-Transformer and SOTAs on Image Captioning tasks in single-model setting. The models\nare evaluated on the Karpathy test split.\nCOCO Captioning\nmodel Params BLEU-4 METEOR ROUGE CIDEr SPICE\nORT [9] 45M 38.6 28.7 58.4 128.3 22.6\nAoANet [3] 64M 38.9 29.2 58.8 129.8 22.4\nGCN-LSTM+HIP [52] - 39.1 28.9 59.2 130.6 22.3\nM2 Transformer [8] 33M 39.1 29.2 58.6 131.2 22.6\nX-Transformer [49] 138M 39.7 29.5 59.1 132.8 23.4\nTransformer [1] 44M 38.9 29.0 58.5 131.0 22.3\nLW-Transformer 1 √ó 24M 38.7 29.2 58.3 130.9 22.7\nLW-Transformer 3 √ó 29M 38.9 29.2 58.6 131.3 22.6\nTABLE IX: Comparisons between LW-Transformer and other Transformers on image classiÔ¨Åcation task. We use Swin-\nTransformer-tiny [39] as the baseline architecture. LW-Transformer-large denotes the model with more Transformer layers.\nModel groups params MAdds Cifar-100 ImageNet\ntop-1 top-5 top-1 top-5\nSwin-Transformer [39] - 28.3M 4.5G 77.9 94.5 81.2 95.5\nPerformer [30] - 27.5M 4.1G - - 79.0 94.2\nLW-Transformer1√ó 2 20.0M 3.3G 77.8 94.7 79.9 94.9\nLW-Transformer-large1√ó 2 27.0M 4.5G 78.8 95.2 81.5 95.7\nLW-Transformer1√ó 4 16.9M 2.6G 77.7 94.5 78.9 94.4\nTABLE X: A statistic summary of the effects of our solutions on 5 Transformer networks.\nMetrics\nVisual Question\nAnswering\nReffering Expression\nComprehension\nImage\nCaptioning\nV&L\nPre-training\nImage\nClassiÔ¨Åcation\nVQA2 CLEVR GQA RefCOCO RefCOCO+ MS-COCO GQA Cifar100\nPerformance Gains -0.1 +0.3 +1.0 +0.2 +0.0 -0.1 +0.6 -0.1\nParameter Reduction -45% -45% -45% -45% -45% -45% -45% -41%\nComputational Reduction -28.3% -28.3% -28.3% -28.3% -28.3% -28.3% -28.3% -42.2%\nTABLE XI: Comparisons between Bert-style models on the\ntest-dev splits of VQA2.0 and GQA. LW-LXMEERT is the\nmodel deployed with the proposed LW-Transformer layer.\nmodel VQA2.0 GQA #Params 3\nViLBERT [16] 70.6 - 221M\nVisualBERT [15] 70.8 - 85M\nUNITER [14] 72.3 - 85M\n12in1 [17] 72.6 60.1 221M\nLXMERT [19] 72.5 60.3 181M\nLW-LXMERT 1√ó (Ours) 71.6 60.6 100M\nLW-LXMERT 3√ó (Ours) 71.8 60.5 117M\nof Performer also suggests that there is still a gap between the\napproximated attention and the default one.\nSecondly, compared with the original structure, Performer\nand Tensorized Transformer greatly increases the computation\ncost (+30% and +43.8%), while our methods can reduce it\nby up to 28.3%. The merit of Linformer in computation\nis also very limited. To explain, Linformer and Performer\nare proposed for the tasks with very large input sequences,\ne.g., language modeling [90] and unidirectional/causal mod-\neling [91]. In unidirectional/causal modeling, the sequence\nlength of input features can be up to 12,288, so the com-\nputation reductions by Linformer and Performer can be up\nto 68.8% and 41.3%, respectively. In contrast, the sequence\nlength for VL tasks typically ranges from 30 to 100 [41],\n[43], [45], which are much smaller. Therefore, these efÔ¨Åcient\ntransformer will increase the amount of computations for VL\ntasks in contrast.\nFrom these observations, we can conclude that the efÔ¨Åciency\nof LW-Transformer is indeed obvious, especially considering\nits performance gains to other optimization methods. More\nimportantly, it can improve the efÔ¨Åciency of Transformer while\nmaintaining the original deÔ¨Ånition of attention modeling.\n4) Comparisons with the State-of-the-art Methods: We\nfurther compare LW-Transformer with SOTA methods on six\nbenchmark datasets of three multi-modal tasks, i.e., VQA, IC\nand REC. The results are given in Tab. VI - VIII.\nTab. VI shows the comparison on three widely-used bench-\nmark datasets of VQA, i.e., VQA2.0 [41], CLEVR [42]\nand GQA [40]. The Ô¨Årst observation from this table is that\nthe performance of the proposed LW-Transformer is very\ncompetitive on all three datasets. And it achieves new SOTA\nperformance on GQA, which is the largest VQA dataset for\nvisual reasoning. Besides, compared to these SOTA methods,\nLW-Transformer has an obvious advantage in terms of the\nparameter size. For instance, BAN-8 and MCAN have 93.4\nand 44.1 millions of parameters 3, respectively, while LW-\nTransformer1√ó and LW-Transformer3√ó only have 24.0 and\n28.7 millions of parameters, respectively. In addition, we\nobserve that LW-Transformer achieves better performance than\nTransformer except for VQA2.0, which has a strong language\nbias and requires a larger parameter size to accommodate the\ndata distribution [41]. These observations greatly conÔ¨Årm the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9\neffectiveness of LW-Transformer and the introduced Group-\nwise Transformation on the VQA task.\nOn REC, which is a task of exploring language-vision\nalignment, the advantages of LW-Transformer are more sig-\nniÔ¨Åcant, as shown in Tab. VII . Compare to SOTA methods\nlike NMTree [63], the performance gains of LW-Transformer\nare up to 6.1% (on RefCOCO+), greatly showing the gen-\neralization ability of LW-Transformer. We also observe that\nits performance is very close to that of Transformer, and\neven slightly better on RefCOCO, which once again conÔ¨Årms\nour argument that Group-wise transformation can compress\nTransformer while still keeping its performance.\nThe comparison on the language-and-vision generative task,\ni.e., IC, is given in Tab. VIII . As shown in this table, LW-\nTransformer is an excellent generative model. Its performance\nis very close to M2 Transformer, and even better on the\nmetrics of CIDEr and SPICE. In addition, the parameter size\nof LW-Transformer is smaller than that of M2 Transformer,\ni.e., 24 millions vs 33 millions. Considering M2 Transformer\nis already an efÔ¨Åcient model, these improvements greatly\nsuggest the effectiveness and efÔ¨Åciency of LW-Transformer.\nWhen compared to the SOTA method, i.e., X-Transformer,\nthe advantage of LW-Transformer in efÔ¨Åciency becomes more\nprominent. Compared with X-Transformer, LW-Transformer\nhas up to 79.0% fewer parameters, while the performance is\nonly reduced by 1.12% on CIDEr.\n5) Quantitative Results with the BERT-style Pre-training:\nWe further apply our optimization strategy to a BERT-style\nmodel, i.e., LXMERT [19], and compare it with a set of\nmethods pre-trained on the large-scale language-and-vision\ndata. The result is given in Tab. XI. For simplicity, we denote\nthe compressed LXMERT as LW-LXMERT. From this table,\nwe Ô¨Årst observe that after deploying G-MHA and G-FFN,\nthe parameter size of LXMERT is reduced by up to 44.8%,\nwhile the performance is marginally reduced on VQA2.0 and\neven better on GQA. We notice that compared to some SOTA\nmethods like 12in1 [17], the performance of LW-LXMERT\non VQA2.0 is slightly worse. As mentioned about, VQA2.0\nrequires a larger model capacity due to the issue of language\nbias [41]. On GQA, examples of which are more balanced\nand more challenging, LW-LXMERT achieves new SOTA\nperformance. Such results conÔ¨Årm again our argument that\nLW-Transformer can be applied to most Transformer-based\nnetworks, while maintaining their high performance.\n6) Results of Image ClassiÔ¨Åcation: To further examine\nthe generalization ability of LW-Transformer, we build it on\nthe recently proposed Swin-Transformer [39] and conduct\nadditional experiments on the task of image classiÔ¨Åcation, of\nwhich results are given in Tab. IX. In this table, we report\nthree LW-Transformer with different settings. The Ô¨Årst two are\nLW-Transformer1√ówith different group numbers, and the last\none is LW-Transformer1√ó-large, which has similar parameter\nsize as Swin-Transformer by adding more Transformer layers.\nFrom these results, we have some observations. Firstly,\non Cifar-100, LW-Transformer 1√ó hardly degrades the per-\nformance of Swin-Transformer. On top-5 accuracy , LW-\nTransformer1√ó even performs slightly better. These results\nare consistent with those in vision-and-language benchmarks.\nSecondly, on the large-scale ImageNet benchmark, LW-\nTransformer1√ó is slightly worse than Swin-Transformer. In\nthis regard, our assumption is that ImageNet has a higher\nrequirement for model capacity. Considering the saved ex-\nperimental expenditure, e.g., saving 40.3% parameters and\n42.2% MAdds when group number is 4, the performance\ndrop (-2.8%) is still acceptable. This assumption is validated\nin the results of LW-Transformer-large. It improves Swin-\nTransformer on Cifar-100 and ImageNet with similar parame-\nters and computations. Considering Swin-Transformer is care-\nfully designed for image classiÔ¨Åcation, these slightly improve-\nments well validate the effectiveness of our method. Lastly,\ncompared with Performer, LW-Transformer 1√ó has all-round\nadvantages. Retaining better performance, LW-Transformer1√ó\nmerits in the parameter size and the computation cost, which\ngreatly validates our motivation about the application of group-\nwise transformation. Overall, these results well support the\ngeneralization ability and efÔ¨Åciency of LW-Transformer for the\ntraditional computer vision tasks.\nE. Qualitative Experiment.\nTo gain deep insights into LW-Transformer, we visualize its\nattention maps and compare them with the ones of the default\nTransformer in Fig.2.\nIn Fig. 2 (a), we present the overall patterns of single- and\nmulti-modal attentions learned by Transformer and the pro-\nposed LW-Transformer. From these visualizations, we can see\nthat the global dependency patterns learned by two networks\nare roughly the same, which subsequently conÔ¨Årms the argu-\nment we made in this paper, i.e., group-wise transformation\nessentially inherits the principle of self-attention.\nIn addition to this observation, we capture some subtle\ndifferences between two networks and two multi-modal tasks.\nFor example, the attention patterns of VQA and REC are\nslightly different. As shown in Fig. 2 (a), the words indicating\nquestion type will be more important in VQA, e.g., ‚Äúis the ‚Äù\nand ‚Äú what color ‚Äù. In contrast, REC requires the model to\nfocus more on spatial information like ‚Äú left‚Äù or ‚Äú middle‚Äù.\nThis result further reÔ¨Çects the difference of model reasoning\nbetween two tasks. That is, VQA relies more on the language\nprior of question types to answer the question, and REC\nlocates the target instance based more on spatial information.\nNevertheless, we Ô¨Ånd that these different attention patterns can\nbe mastered by both LW-Transformer and Transformer.\nIn terms of vision-and-language alignment, the attention\nfocus of LW-Transformer is more concentrated, while the one\nof Transformer is more divergent. For instance, in the second\nexample of VQA, Transformer‚Äôs attention is Ô¨Çat, mainly\nfocusing on the incorrect text-image relationships (4-7). In\ncontrast, the attention of LW-Transformer is more accurate.\nThese properties is also reÔ¨Çected in the visualizations of\ncross-modal attentions in Fig.2.b. From the examples of\nFig. 2.b, we can see that LW-Transformer can obtain more\nprecise ‚Äúattention‚Äù than Transformer. SpeciÔ¨Åcally, in cross-\nmodal attention, Transformer and LW-Transformer focus on\nsimilar image regions. However, LW-Transformer will be more\naccurate in terms of the most attended regions (with red\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10\nWhat color blanket is the laptop sitting on?\n1 2 3\n1 2 3\nWhat is the woman doing with the cell phone?\n1 2 3\n1 2 3\nIs the potted plant on the side table?\n1 2\n1 2\n(a) Comparisons of attention maps between LWTransformer(first row) and Transformer (second row) on VQA val set and RefCOCO val set.\n(b) Visualizations of   the detailed vision-and-language alignments of LW-Transformer and Transformer. \nText-to-text \nattention\nImage-to-image \nattention\nText-to-image \nattention\nText-to-text \nattention\nImage-to-image \nattention\nText-to-image \nattention\nSaliency Regions\nQuestion: What color is the \nplant?  \nPrediction: green\nIndex of the prediction box: 3\nExp: Pink toilet left side\nthird down.\nSaliency Regions\nText-to-text \nattention\nImage-to-image \nattention\nText-to-image \nattention\nVQAREC\nIndex of the prediction box: 9\nExp: Guy with plaid shirt \nin middle with back straps.\nSaliency Regions\nText-to-text \nattention\nImage-to-image \nattention\nText-to-image \nattention\nQuestion: Is the boy playing\nbaseball?  \nPrediction: yes\nSaliency Regions\nLWTransformerTransformer\nLWTransformerTransformerLWTransformerTransformer\nLWTransformerTransformerLWTransformerTransformer\nFig. 2: Visualizations of the attention maps of the proposed LW-Transformer and the default Transformer. We visualize different\ntypes of attention maps in (a) and the detailed vision-and-language alignments in (b). These results are all from the last self-\nattention layer of two models. In (b), the red box denotes the most attended visual region.\nboxes), for example, the ‚Äúphone‚Äù in the second example. This\nobservation conÔ¨Årms the merits of LW-Transformer in vision-\nand-language alignment.\nV. C ONCLUSION\nIn this paper, we introduce the Group-wise Transformation\nto achieve a lightweight yet general Transformer network,\ntermed LW-Transformer. Compressing Transformer still re-\ntains challenging mainly due to its complex layer designs,\ni.e., MHA and FFN. The proposed LW-Transformer can well\nmaintain their main principles, i.e., the efÔ¨Åcient attention\nmodelings on diverse subspaces and the expanding and scaling\nfeature transformation, and reduce the parameter sizes and\ncomputation costs to a large extend. More importantly, the in-\ntention of group-wise transformation is consistent with MHA,\nwhich can ensure LW-Transformer to learn similar or even\nbetter attention patterns as the default Transformer. To validate\nour optimization strategy, we build LW-Transformer based on\nTransformer and a BERT-style model called LXMERT, and\nconduct extensive experiments on six benchmark datasets of\nthree multi-modal tasks. The experimental results show that\nwhile saving up to 45% of parameters and 28% of computation\ncosts, LW-Transformer can achieve almost or even better\nperformance than Transformer on these datasets. Meanwhile,\nthe generalization ability of LW-Transformer is also well\nvalidated on a newly proposed image Transformer called Swin-\nTransformer [39] for the task of image classiÔ¨Åcation. These\nresults greatly validate the effectiveness of LW-Transformer\nas well as our motivation.\nACKNOWLEDGMENT\nThis work is supported by the National Science\nFund for Distinguished Young Scholars (No.62025603),\nthe National Natural Science Foundation of China\n(No.U1705262, No. 62072386, No. 62072387, No.\n62072389, No.62002305, No.61772443, No.61802324\nand No.61702136), China Postdoctoral Science Foundation\n(2021T40397), Guangdong Basic and Applied Basic Research\nFoundation (No.2019B1515120049) and the Fundamental\nResearch Funds for the central universities (No. 20720200077,\nNo. 20720200090 and No. 20720200091).\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11\nREFERENCES\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù in NeurIPS,\n2017, pp. 5998‚Äì6008.\n[2] Z. Yu, J. Yu, Y . Cui, D. Tao, and Q. Tian, ‚ÄúDeep modular co-attention\nnetworks for visual question answering,‚Äù inCVPR, 2019, pp. 6281‚Äì6290.\n[3] L. Huang, W. Wang, J. Chen, and X.-Y . Wei, ‚ÄúAttention on attention for\nimage captioning,‚Äù in Proceedings of the IEEE International Conference\non Computer Vision , 2019, pp. 4634‚Äì4643.\n[4] Z. Yu, Y . Cui, J. Yu, D. Tao, and Q. Tian, ‚ÄúMultimodal uniÔ¨Åed\nattention networks for vision-and-language interactions,‚Äù arXiv preprint\narXiv:1908.04107, 2019.\n[5] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, ‚ÄúEnd-to-end object detection with transformers,‚Äù arXiv\npreprint arXiv:2005.12872, 2020.\n[6] P. Gao, Z. Jiang, H. You, P. Lu, S. C. Hoi, X. Wang, and H. Li, ‚ÄúDynamic\nfusion with intra-and inter-modality attention Ô¨Çow for visual question\nanswering,‚Äù in CVPR, 2019, pp. 6639‚Äì6648.\n[7] P. Gao, H. You, Z. Zhang, X. Wang, and H. Li, ‚ÄúMulti-modality latent\ninteraction network for visual question answering,‚Äù inProceedings of the\nIEEE International Conference on Computer Vision , 2019, pp. 5825‚Äì\n5835.\n[8] M. Cornia, M. Stefanini, L. Baraldi, and R. Cucchiara, ‚ÄúMeshed-\nMemory Transformer for Image Captioning,‚Äù in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2020.\n[9] S. Herdade, A. Kappeler, K. Boakye, and J. Soares, ‚ÄúImage captioning:\nTransforming objects into words,‚Äù in Advances in Neural Information\nProcessing Systems, 2019, pp. 11 135‚Äì11 145.\n[10] G. Li, L. Zhu, P. Liu, and Y . Yang, ‚ÄúEntangled transformer for image\ncaptioning,‚Äù in Proceedings of the IEEE International Conference on\nComputer Vision, 2019, pp. 8928‚Äì8937.\n[11] Z. Yu, Y . Cui, J. Yu, M. Wang, D. Tao, and Q. Tian, ‚ÄúDeep multimodal\nneural architecture search,‚Äù arXiv preprint arXiv:2004.12070 , 2020.\n[12] C. Alberti, J. Ling, M. Collins, and D. Reitter, ‚ÄúFusion of de-\ntected objects in text for visual question answering,‚Äù arXiv preprint\narXiv:1908.05054, 2019.\n[13] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBert: Pre-training\nof deep bidirectional transformers for language understanding,‚Äù arXiv\npreprint arXiv:1810.04805, 2018.\n[14] G. Li, N. Duan, Y . Fang, D. Jiang, and M. Zhou, ‚ÄúUnicoder-vl: A\nuniversal encoder for vision and language by cross-modal pre-training,‚Äù\narXiv preprint arXiv:1908.06066 , 2019.\n[15] L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang, ‚ÄúVisualbert:\nA simple and performant baseline for vision and language,‚Äù arXiv\npreprint arXiv:1908.03557, 2019.\n[16] J. Lu, D. Batra, D. Parikh, and S. Lee, ‚ÄúVilbert: Pretraining task-\nagnostic visiolinguistic representations for vision-and-language tasks,‚Äù\nin Advances in Neural Information Processing Systems , 2019, pp. 13‚Äì\n23.\n[17] J. Lu, V . Goswami, M. Rohrbach, D. Parikh, and S. Lee, ‚Äú12-in-1: Multi-\ntask vision and language representation learning,‚Äù 2019.\n[18] W. Su, X. Zhu, Y . Cao, B. Li, L. Lu, F. Wei, and J. Dai, ‚ÄúVl-bert:\nPre-training of generic visual-linguistic representations,‚Äù arXiv preprint\narXiv:1908.08530, 2019.\n[19] H. Tan and M. Bansal, ‚ÄúLxmert: Learning cross-modality encoder\nrepresentations from transformers,‚Äù arXiv preprint arXiv:1908.07490 ,\n2019.\n[20] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and\nQ. V . Le, ‚ÄúXlnet: Generalized autoregressive pretraining for language\nunderstanding,‚Äù in Advances in neural information processing systems ,\n2019, pp. 5754‚Äì5764.\n[21] L. Zhou, H. Palangi, L. Zhang, H. Hu, J. J. Corso, and J. Gao,\n‚ÄúUniÔ¨Åed vision-language pre-training for image captioning and vqa,‚Äù\narXiv preprint arXiv:1909.11059 , 2019.\n[22] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image\nrecognition,‚Äù in Proceedings of the IEEE conference on computer vision\nand pattern recognition , 2016, pp. 770‚Äì778.\n[23] S. Ren, K. He, R. Girshick, and J. Sun, ‚ÄúFaster r-cnn: Towards real-time\nobject detection with region proposal networks,‚Äù in Advances in neural\ninformation processing systems , 2015, pp. 91‚Äì99.\n[24] A. Fan, E. Grave, and A. Joulin, ‚ÄúReducing transformer depth on demand\nwith structured dropout,‚Äù 2019.\n[25] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, ‚ÄúDistilbert, a distilled\nversion of bert: smaller, faster, cheaper and lighter,‚Äù arXiv preprint\narXiv:1910.01108, 2019.\n[26] S. Shen, Z. Dong, J. Ye, L. Ma, Z. Yao, A. Gholami, M. W. Mahoney,\nand K. Keutzer, ‚ÄúQ-bert: Hessian based ultra low precision quantization\nof bert,‚Äù arXiv preprint arXiv:1909.05840 , 2019.\n[27] J. Y . Tian, A. P. Kreuzer, P.-H. Chen, and H.-M. Will, ‚ÄúWaldorf:\nWasteless language-model distillation on reading-comprehension,‚ÄùarXiv\npreprint arXiv:1912.06638, 2019.\n[28] O. Zafrir, G. Boudoukh, P. Izsak, and M. Wasserblat, ‚ÄúQ8bert: Quantized\n8bit bert,‚Äù arXiv preprint arXiv:1910.06188 , 2019.\n[29] X. Ma, P. Zhang, S. Zhang, N. Duan, Y . Hou, M. Zhou, and D. Song,\n‚ÄúA tensorized transformer for language modeling,‚Äù in NeurIPS, 2019,\npp. 2229‚Äì2239.\n[30] K. M. Choromanski, V . Likhosherstov, D. Dohan, X. Song, A. Gane,\nT. Sarlos, P. Hawkins, J. Q. Davis, A. Mohiuddin, L. Kaiser et al. ,\n‚ÄúRethinking attention with performers,‚Äù in International Conference on\nLearning Representations, 2020.\n[31] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, ‚ÄúLinformer:\nSelf-attention with linear complexity,‚Äù arXiv preprint arXiv:2006.04768,\n2020.\n[32] N. Kitaev, L. Kaiser, and A. Levskaya, ‚ÄúReformer: The efÔ¨Åcient trans-\nformer,‚Äù inInternational Conference on Learning Representations, 2019.\n[33] F. Chollet, ‚ÄúXception: Deep learning with depthwise separable convo-\nlutions,‚Äù in CVPR, 2017, pp. 1251‚Äì1258.\n[34] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImagenet classiÔ¨Åcation\nwith deep convolutional neural networks,‚Äù in Advances in neural infor-\nmation processing systems , 2012, pp. 1097‚Äì1105.\n[35] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,\n‚ÄúMobilenetv2: Inverted residuals and linear bottlenecks,‚Äù inCVPR, 2018,\npp. 4510‚Äì4520.\n[36] C. Szegedy, S. Ioffe, V . Vanhoucke, and A. A. Alemi, ‚ÄúInception-v4,\ninception-resnet and the impact of residual connections on learning,‚Äù in\nThirty-Ô¨Årst AAAI conference on artiÔ¨Åcial intelligence , 2017.\n[37] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, ‚ÄúRethinking\nthe inception architecture for computer vision,‚Äù in Proceedings of the\nIEEE conference on computer vision and pattern recognition , 2016, pp.\n2818‚Äì2826.\n[38] X. Zhang, X. Zhou, M. Lin, and J. Sun, ‚ÄúShufÔ¨Çenet: An extremely\nefÔ¨Åcient convolutional neural network for mobile devices,‚Äù in CVPR,\n2018, pp. 6848‚Äì6856.\n[39] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, ‚ÄúSwin transformer: Hierarchical vision transformer using shifted\nwindows,‚Äù arXiv preprint arXiv:2103.14030 , 2021.\n[40] D. A. Hudson and C. D. Manning, ‚ÄúGqa: A new dataset for real-world\nvisual reasoning and compositional question answering,‚Äù inCVPR, 2019,\npp. 6700‚Äì6709.\n[41] Y . Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh, ‚ÄúMaking\nthe v in vqa matter: Elevating the role of image understanding in\nvisual question answering,‚Äù in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , 2017, pp. 6904‚Äì6913.\n[42] J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei,\nC. Lawrence Zitnick, and R. Girshick, ‚ÄúClevr: A diagnostic dataset for\ncompositional language and elementary visual reasoning,‚Äù in CVPR,\n2017, pp. 2901‚Äì2910.\n[43] X. Chen, H. Fang, T.-Y . Lin, R. Vedantam, S. Gupta, P. Doll ¬¥ar, and\nC. L. Zitnick, ‚ÄúMicrosoft coco captions: Data collection and evaluation\nserver,‚Äù arXiv preprint arXiv:1504.00325 , 2015.\n[44] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg, ‚ÄúModeling\ncontext in referring expressions,‚Äù in European Conference on Computer\nVision. Springer, 2016, pp. 69‚Äì85.\n[45] S. Kazemzadeh, V . Ordonez, M. Matten, and T. Berg, ‚ÄúReferitgame:\nReferring to objects in photographs of natural scenes,‚Äù in EMNLP, 2014,\npp. 787‚Äì798.\n[46] D. R. So, C. Liang, and Q. V . Le, ‚ÄúThe evolved transformer,‚Äù arXiv\npreprint arXiv:1901.11117, 2019.\n[47] S. Sukhbaatar, E. Grave, P. Bojanowski, and A. Joulin, ‚ÄúAdaptive\nattention span in transformers,‚Äù arXiv preprint arXiv:1905.07799, 2019.\n[48] Y . Zhou, R. Ji, X. Sun, G. Luo, X. Hong, J. Su, X. Ding, and L. Shao,\n‚ÄúK-armed bandit based multi-modal network architecture search for vi-\nsual question answering,‚Äù in Proceedings of the 28th ACM International\nConference on Multimedia , 2020, pp. 1245‚Äì1254.\n[49] Y . Pan, T. Yao, Y . Li, and T. Mei, ‚ÄúX-linear attention networks for image\ncaptioning,‚Äù in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 2020, pp. 10 971‚Äì10 980.\n[50] X. Liu, H. Li, J. Shao, D. Chen, and X. Wang, ‚ÄúShow, tell and\ndiscriminate: Image captioning by self-retrieval with partially labeled\ndata,‚Äù in Proceedings of the European Conference on Computer Vision\n(ECCV), 2018, pp. 338‚Äì354.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12\n[51] S. Liu, Z. Zhu, N. Ye, S. Guadarrama, and K. Murphy, ‚ÄúImproved image\ncaptioning via policy gradient optimization of spider,‚Äù in Proceedings of\nthe IEEE international conference on computer vision , 2017, pp. 873‚Äì\n881.\n[52] T. Yao, Y . Pan, Y . Li, and T. Mei, ‚ÄúHierarchy parsing for image\ncaptioning,‚Äù in Proceedings of the IEEE/CVF International Conference\non Computer Vision , 2019, pp. 2621‚Äì2629.\n[53] J. Ji, Y . Luo, X. Sun, F. Chen, G. Luo, Y . Wu, Y . Gao, and R. Ji,\n‚ÄúImproving image captioning by leveraging intra-and inter-layer global\nrepresentation in transformer network,‚Äù 2021.\n[54] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and\nM. Rohrbach, ‚ÄúMultimodal compact bilinear pooling for visual question\nanswering and visual grounding,‚Äù inProceedings of the 2016 Conference\non Empirical Methods in Natural Language Processing , 2016, pp. 457‚Äì\n468.\n[55] J. Hu, L. Shen, and G. Sun, ‚ÄúSqueeze-and-excitation networks,‚Äù in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2018, pp. 7132‚Äì7141.\n[56] R. Vedantam, C. Lawrence Zitnick, and D. Parikh, ‚ÄúCider: Consensus-\nbased image description evaluation,‚Äù in Proceedings of the IEEE confer-\nence on computer vision and pattern recognition , 2015, pp. 4566‚Äì4575.\n[57] Y . Li, Y . Pan, T. Yao, J. Chen, and T. Mei, ‚ÄúScheduled sampling in\nvision-language pretraining with decoupled encoder-decoder network,‚Äù\narXiv preprint arXiv:2101.11562 , 2021.\n[58] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick,\nand D. Parikh, ‚ÄúVqa: Visual question answering,‚Äù in Proceedings of\nthe IEEE international conference on computer vision , 2015, pp. 2425‚Äì\n2433.\n[59] A. El-Nouby, H. Touvron, M. Caron, P. Bojanowski, M. Douze,\nA. Joulin, I. Laptev, N. Neverova, G. Synnaeve, J. Verbeek et al., ‚ÄúXcit:\nCross-covariance image transformers,‚ÄùarXiv preprint arXiv:2106.09681,\n2021.\n[60] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, and H. Adam, ‚ÄúMobilenets: EfÔ¨Åcient convo-\nlutional neural networks for mobile vision applications,‚Äù arXiv preprint\narXiv:1704.04861, 2017.\n[61] R. Hu, M. Rohrbach, J. Andreas, T. Darrell, and K. Saenko, ‚ÄúModeling\nrelationships in referential expressions with compositional modular\nnetworks,‚Äù in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2017, pp. 1115‚Äì1124.\n[62] R. Hu, H. Xu, M. Rohrbach, J. Feng, K. Saenko, and T. Darrell, ‚ÄúNatural\nlanguage object retrieval,‚Äù in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , 2016, pp. 4555‚Äì4564.\n[63] D. Liu, H. Zhang, F. Wu, and Z.-J. Zha, ‚ÄúLearning to assemble neural\nmodule tree networks for visual grounding,‚Äù in Proceedings of the IEEE\nInternational Conference on Computer Vision , 2019, pp. 4673‚Äì4682.\n[64] J. Liu, L. Wang, and M.-H. Yang, ‚ÄúReferring expression generation and\ncomprehension via attributes,‚Äù in Proceedings of the IEEE International\nConference on Computer Vision , 2017, pp. 4856‚Äì4864.\n[65] L. Yu, Z. Lin, X. Shen, J. Yang, X. Lu, M. Bansal, and T. L.\nBerg, ‚ÄúMattnet: Modular attention network for referring expression\ncomprehension,‚Äù in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , 2018, pp. 1307‚Äì1315.\n[66] Z. Yu, J. Yu, C. Xiang, Z. Zhao, Q. Tian, and D. Tao, ‚ÄúRethinking\ndiversiÔ¨Åed and discriminative proposal generation for visual grounding,‚Äù\nin Proceedings of the 27th International Joint Conference on ArtiÔ¨Åcial\nIntelligence, 2018, pp. 1114‚Äì1120.\n[67] G. Luo, Y . Zhou, X. Sun, L. Cao, C. Wu, C. Deng, and R. Ji, ‚ÄúMulti-\ntask collaborative network for joint referring expression comprehension\nand segmentation,‚Äù in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2020, pp. 10 034‚Äì10 043.\n[68] Y . Zhou, R. Ji, G. Luo, X. Sun, J. Su, X. Ding, C.-W. Lin, and Q. Tian,\n‚ÄúA real-time global inference network for one-stage referring expression\ncomprehension,‚Äù IEEE Transactions on Neural Networks and Learning\nSystems, 2021.\n[69] A. Karpathy and L. Fei-Fei, ‚ÄúDeep visual-semantic alignments for\ngenerating image descriptions,‚Äù in Proceedings of the IEEE conference\non computer vision and pattern recognition , 2015, pp. 3128‚Äì3137.\n[70] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, ‚ÄúBleu: a method\nfor automatic evaluation of machine translation,‚Äù in Proceedings of\nthe 40th annual meeting on association for computational linguistics .\nAssociation for Computational Linguistics, 2002, pp. 311‚Äì318.\n[71] S. Banerjee and A. Lavie, ‚ÄúMeteor: An automatic metric for mt evalua-\ntion with improved correlation with human judgments,‚Äù in Proceedings\nof the acl workshop on intrinsic and extrinsic evaluation measures for\nmachine translation and/or summarization , 2005, pp. 65‚Äì72.\n[72] C.-Y . Lin, ‚ÄúRouge: A package for automatic evaluation of summaries,‚Äù\nin ACL Workshop. Springer, 2004, pp. 382‚Äì398.\n[73] P. Anderson, B. Fernando, M. Johnson, and S. Gould, ‚ÄúSpice: Semantic\npropositional image caption evaluation,‚Äù in European Conference on\nComputer Vision. Springer, 2016, pp. 382‚Äì398.\n[74] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ‚ÄúImagenet:\nA large-scale hierarchical image database,‚Äù in 2009 IEEE conference on\ncomputer vision and pattern recognition . Ieee, 2009, pp. 248‚Äì255.\n[75] A. Krizhevsky, G. Hinton et al. , ‚ÄúLearning multiple layers of features\nfrom tiny images,‚Äù 2009.\n[76] H. Ben-Younes, R. Cadene, M. Cord, and N. Thome, ‚ÄúMutan: Multi-\nmodal tucker fusion for visual question answering,‚Äù in Proceedings of\nthe IEEE international conference on computer vision , 2017, pp. 2612‚Äì\n2620.\n[77] J.-H. Kim, K.-W. On, W. Lim, J. Kim, J.-W. Ha, and B.-T. Zhang,\n‚ÄúHadamard product for low-rank bilinear pooling,‚Äù arXiv preprint\narXiv:1610.04325, 2016.\n[78] Z. Yu, J. Yu, J. Fan, and D. Tao, ‚ÄúMulti-modal factorized bilinear pooling\nwith co-attention learning for visual question answering,‚Äù inProceedings\nof the IEEE international conference on computer vision , 2017, pp.\n1821‚Äì1830.\n[79] Z. Yu, J. Yu, C. Xiang, J. Fan, and D. Tao, ‚ÄúBeyond bilinear: Generalized\nmultimodal factorized high-order pooling for visual question answering,‚Äù\nin NeurIPS, 2018, pp. 5947‚Äì5959.\n[80] H. Ben-Younes, R. Cadene, N. Thome, and M. Cord, ‚ÄúBlock: Bilinear\nsuperdiagonal fusion for visual question answering and visual relation-\nship detection,‚Äù in Proceedings of the AAAI Conference on ArtiÔ¨Åcial\nIntelligence, vol. 33, 2019, pp. 8102‚Äì8109.\n[81] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and\nL. Zhang, ‚ÄúBottom-up and top-down attention for image captioning and\nvisual question answering,‚Äù in CVPR, 2018, pp. 6077‚Äì6086.\n[82] D. A. Hudson and C. D. Manning, ‚ÄúCompositional attention networks\nfor machine reasoning,‚Äù in ICLR, 2018.\n[83] E. Perez, F. Strub, H. De Vries, V . Dumoulin, and A. Courville, ‚ÄúFilm:\nVisual reasoning with a general conditioning layer,‚Äù in Thirty-Second\nAAAI Conference on ArtiÔ¨Åcial Intelligence , 2018.\n[84] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu,\nP. Battaglia, and T. Lillicrap, ‚ÄúA simple neural network module for\nrelational reasoning,‚Äù in Advances in neural information processing\nsystems, 2017, pp. 4967‚Äì4976.\n[85] R. Hu, A. Rohrbach, T. Darrell, and K. Saenko, ‚ÄúLanguage-conditioned\ngraph networks for relational reasoning,‚Äù in Proceedings of the IEEE\nInternational Conference on Computer Vision, 2019, pp. 10 294‚Äì10 303.\n[86] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù\narXiv preprint arXiv:1412.6980 , 2014.\n[87] R. Krishna, Y . Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,\nY . Kalantidis, L.-J. Li, D. A. Shammaet al., ‚ÄúVisual genome: Connecting\nlanguage and vision using crowdsourced dense image annotations,‚Äù\nInternational Journal of Computer Vision , vol. 123, no. 1, pp. 32‚Äì73,\n2017.\n[88] J.-H. Kim, J. Jun, and B.-T. Zhang, ‚ÄúBilinear attention networks,‚Äù in\nAdvances in Neural Information Processing Systems , 2018, pp. 1564‚Äì\n1574.\n[89] L. Yu, H. Tan, M. Bansal, and T. L. Berg, ‚ÄúA joint speaker-listener-\nreinforcer model for referring expressions,‚Äù in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition , 2017, pp.\n7282‚Äì7290.\n[90] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn, and\nT. Robinson, ‚ÄúOne billion word benchmark for measuring progress in\nstatistical language modeling,‚Äù arXiv preprint arXiv:1312.3005 , 2013.\n[91] N. Parmar, A. Vaswani, J. Uszkoreit, ≈Å. Kaiser, N. Shazeer, A. Ku, and\nD. Tran, ‚ÄúImage transformer,‚Äù arXiv preprint arXiv:1802.05751 , 2018.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13\nGen Luo is currently pursuing the phd‚Äôs degree in\nXiamen University. His research interests include\nvision-and-language interactions.\nYiyi Zhou received his Ph.D. degree supervised by\nProf. Rongrong Ji from Xiamen University, Chian,\nin 2019. He is a Post-doctoral Research Fellow of\nthe School of Informatics and a member of Media\nAnalytics and Computing (MAC) lab of Xiamen\nUniversity, China.\nXiaoshuai Sun (Senior Member, IEEE) received\nthe B.S. degree in computer science from Harbin\nEngineering University, Harbin, China, in 2007, and\nthe M.S. and Ph.D. degrees in computer science and\ntechnology from the Harbin Institute of Technology,\nHarbin, in 2009 and 2015, respectively. He was a\nPostdoctoral Research Fellow with the University\nof Queensland from 2015 to 2016. He served as\na Lecturer with the Harbin Institute of Technology\nfrom 2016 to 2018. He is currently an Associate\nProfessor with Xiamen University, China. He was a\nrecipient of the Microsoft Research Asia Fellowship in 2011.\nYan Wang got his PhD degree in Oct. 2015, under\nsupervision of Prof. Shih-Fu Chang. He had rich\nR&D experience in Adobe Research, Microsoft Re-\nsearch, and Facebook, with his algorithms integrated\nin Facebook Graph Search and Adobe Photoshop,\nand granted patents. He held the Olympic Torch of\nBeijing Olympic Games as a torchbearer in 2008,\nwon the Tech Draft (a nation-wide programming\nchallenge) in 2014, and is a certiÔ¨Åed airplane pi-\nlot. His current research interests include computer\nvision and machine learning.\nLiujuan Cao received her Bachelor‚Äôs, Master‚Äôs,\nand Ph.D. degrees from the School of Computer\nScience and Technology, Harbin Engineering Uni-\nversity, Harbin, China. She was a Visiting Re-\nsearcher with Columbia University from 2012 to\n2013. She joined Xiamen University in 2014. She\nis currently an Associate Professor with the School\nof Informatics, Xiamen University, Xiamen, China.\nShe has published more than 30 papers in the top\nand major tiered journals and conferences, including\nIEEE CVPR, Information Sciences, Neurocomput-\ning, Signal Processing, Digital Signal Processing, etc. Her research interests\ninclude covers pattern recognition, data mining, and computer vision. She is\nthe Finance Chair of IEEE MMSP 2015. She has been project PI for various\nprojects including NSFC, military projects, with over 1M RMB fundings.\nYongjian Wu received his master‚Äôs degree in com-\nputer science from Wuhan University, China, in\n2008. He is currently the expert researcher and the\ndirector of the Youtu Lab, Tencent Co., Ltd. His\nresearch interests include face recognition, image\nunderstanding, and large scale data processing.\nFeiyue Huang received his B.S. and Ph.D. de-\ngrees in computer science from Tsinghua University,\nChina, in 2001 and 2008, respectively. He is the\nexpert researcher and the director of the Tencent\nYoutu Lab. His research interests include image\nunderstanding and face recognition.\nRongrong Ji (Senior Member, IEEE) is currently\na Professor and the Director of the Intelligent\nMultimedia Technology Laboratory, School of In-\nformatics, Xiamen University, Xiamen, China. His\nwork mainly focuses on innovative technologies for\nmultimedia signal processing, computer vision, and\npattern recognition, with over 100 papers published\nin international journals and conferences. He is a\nmember of the ACM. He also serves as a program\ncommittee member for several Tier-1 international\nconferences. He was a recipient of the ACM Mul-\ntimedia Best Paper Award and the Best Thesis Award of Harbin Institute of\nTechnology. He serves as an Associate/Guest Editor for international journals\nand magazines, such as Neurocomputing, Signal Processing, Multimedia Tools\nand Applications, the IEEE Multimedia Magazine , and Multimedia Systems.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7003589272499084
    },
    {
      "name": "Computer science",
      "score": 0.6318094730377197
    },
    {
      "name": "Computation",
      "score": 0.5586677193641663
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4451848864555359
    },
    {
      "name": "Computer engineering",
      "score": 0.34797191619873047
    },
    {
      "name": "Algorithm",
      "score": 0.2686495780944824
    },
    {
      "name": "Engineering",
      "score": 0.1853160262107849
    },
    {
      "name": "Electrical engineering",
      "score": 0.138469398021698
    },
    {
      "name": "Voltage",
      "score": 0.1055443286895752
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I191208505",
      "name": "Xiamen University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210113369",
      "name": "Microsoft Research Asia (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    }
  ],
  "cited_by": 53
}