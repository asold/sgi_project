{
  "title": "Transformers Generalize Linearly",
  "url": "https://openalex.org/W3202908475",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5020894677",
      "name": "Jackson Petty",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A5071457812",
      "name": "Robert Frank",
      "affiliations": [
        "Yale University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2896556401",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2971044268",
    "https://openalex.org/W1989083219",
    "https://openalex.org/W2972515356",
    "https://openalex.org/W3104739822",
    "https://openalex.org/W2972733624",
    "https://openalex.org/W2154346509",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2971016963",
    "https://openalex.org/W2973154008",
    "https://openalex.org/W2798727047",
    "https://openalex.org/W2932637973",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W3168987555",
    "https://openalex.org/W3014415613",
    "https://openalex.org/W3042795397",
    "https://openalex.org/W2162263165"
  ],
  "abstract": "Natural language exhibits patterns of hierarchically governed dependencies, in which relations between words are sensitive to syntactic structure rather than linear ordering. While re-current network models often fail to generalize in a hierarchically sensitive way (McCoy et al.,2020) when trained on ambiguous data, the improvement in performance of newer Trans-former language models (Vaswani et al., 2017)on a range of syntactic benchmarks trained on large data sets (Goldberg, 2019; Warstadtet al., 2019) opens the question of whether these models might exhibit hierarchical generalization in the face of impoverished data.In this paper we examine patterns of structural generalization for Transformer sequence-to-sequence models and find that not only do Transformers fail to generalize hierarchically across a wide variety of grammatical mapping tasks, but they exhibit an even stronger preference for linear generalization than comparable recurrent networks",
  "full_text": "Transformers Generalize Linearly\nJackson Petty\nYale University\nDepartment of Linguistics\njackson.petty@yale.edu\nRobert Frank\nYale University\nDepartment of Linguistics\nbob.frank@yale.edu\nAbstract\nNatural language exhibits patterns of hierar-\nchically governed dependencies, in which rela-\ntions between words are sensitive to syntactic\nstructure rather than linear ordering. While re-\ncurrent network models often fail to generalize\nin a hierarchically sensitive way (McCoy et al.,\n2020) when trained on ambiguous data, the\nimprovement in performance of newer Trans-\nformer language models (Vaswani et al., 2017)\non a range of syntactic benchmarks trained\non large data sets (Goldberg, 2019; Warstadt\net al., 2019) opens the question of whether\nthese models might exhibit hierarchical gen-\neralization in the face of impoverished data.\nIn this paper we examine patterns of struc-\ntural generalization for Transformer sequence-\nto-sequence models and ﬁnd that not only do\nTransformers fail to generalize hierarchically\nacross a wide variety of grammatical mapping\ntasks, but they exhibit an even stronger prefer-\nence for linear generalization than comparable\nrecurrent networks.\n1 Introduction\nOne of the fundamental properties of human lan-\nguages is their sensitivity to relations among el-\nements that are not easily characterized in linear\nterms. In phenomena like subject-verb agreement\nor reﬂexive anaphora, the relationship between the\nagreeing verb and its agreement target or the re-\nﬂexive pronoun and its antecedent is not governed\nby linear properties like adjacency or recency, but\ninstead by the hierarchical organization of the sen-\ntence. Similarly, the relationship between related\nsentences, which are represented in some gram-\nmatical theories as transformational operations or\nas lexical rules in others, is also governed by hi-\nerarchical organization. English polar questions,\nfor instance, involve the fronting of an auxiliary\nverb in the corresponding declarative to a sentence-\ninitial position. Questions with complex subjects\nlike (1a) demonstrate that the verb that is fronted in\nsuch cases is the determined by hierarchical promi-\nnence (i.e., MOVE -MAIN yielding (1b)) and not\nlinear considerations (MOVE -FIRST yielding (1c)\nor MOVE -LAST yielding (1d)).\n(1) a. [The president who can smile] will lead\n[those who would sing].\nb. Will the president who can smile lead\nthose who would sing?\nc. * Can the president who smile will lead\nthose who would sing?\nd. * Would the president who can smile will\nlead those who sing?\nChomsky (1971) argues that, in spite of receiving\nlittle input of the form in (1b), which would unam-\nbiguously demonstrate the necessity for a hierar-\nchically governed dependency, children uniformly\ngeneralize the process of question formation in a\nhierarchical fashion. Such consistent behavior sug-\ngests that humans possess an inherent bias of some\nsort towards hierarchical generalization (though see\nAmbridge et al. (2008) and Perfors et al. (2011) for\narguments against this view). Replicating such a\nbias in generalization would indicate the ability to\nmimic patterns of human cognition and learning.\nPrevious investigations of recurrent neural\narchitectures have yielded some evidence for\nhierarchically-governed linguistic knowledge (Gu-\nlordava et al., 2018; Marvin and Linzen, 2018;\nHu et al., 2020). Even greater success has been\nachieved with neural networks the incorporate ex-\nplicit representation of syntactic structure (Kuncoro\net al., 2018). Architecturally-constrained models\nwhen trained without explicit information about\nsyntactic structure show only modest beneﬁts (Shen\net al., 2018; Kim et al., 2019; Merrill et al., 2019).\nHowever, all of these studies involve models that\nare trained on large quantities of text which may\nnot be impoverished in domains that these bench-\narXiv:2109.12036v1  [cs.CL]  24 Sep 2021\nmarks assess. As a result, it is unclear whether any\napparent hierarchical behavior reported in these\nworks is the effect of a bias for hierarchical gener-\nalization or the accumulation of patterns explicitly\nguided by the training data. McCoy et al. (2020)\ntake a different tack: the training data is carefully\ncontrolled so that hierarchical behavior can emerge\nonly if a model itself is biased to extract hierarchi-\ncal generalizations. Their experiments demonstrate\nthat recurrent neural network seq2seq models show\na clear preference for linear generalization.\nThe recently developed Transformer architec-\nture has led to revolutionary advances across many\nareas of natural language processing, including ma-\nchine translation and question answering (Vaswani\net al., 2017; Devlin et al., 2019). Transformer-\nbased models have also shown considerable suc-\ncess on benchmarks that appear to require the repre-\nsentation of hierarchical abstractions (Rogers et al.,\n2021; Goldberg, 2019; Warstadt et al., 2019). Fur-\nther, investigations of Transformers’ representa-\ntions of sentences (Hewitt and Manning, 2019; Lin\net al., 2019) point to encodings of hierarchical syn-\ntactic structure. Yet, for the reasons noted above,\nit is difﬁcult to conclude much about the inductive\nbias in the Transformer: they are trained on vast\ndatasets, leaving open the question of the impact of\ninductive bias as opposed to training data (Warstadt\nand Bowman (2020), but see Van Schijndel et al.\n(2019) for arguments that even massive data may\nnot be sufﬁcient). This paper contributes to our un-\nderstanding by examining the degree to which the\nTransformer architecture is biased toward hierarchi-\ncal generalization when the data underdetermine\nsuch generalization. Speciﬁcally, we study whether\nTransformers learning sequence-to-sequence map-\npings generalize in a structure sensitive way, and\ncompare their performance with recurrent models.\n2 Experiments\nOur experiments involve a variety of English-\nlanguage transduction tasks that highlight\nhierarchically-governed patterns. For each task,\nthe training data is ambiguous between a linear\nand hierarchical generalization. This allows us to\nevaluate performance on both a TEST set, drawn\nfrom the same distribution as the training set, and\na GEN set of data, that contains out-of-distribution\ndata consistent only with hierarchical patterns of\ngeneralization.\nWe compare transformer models with a number\nof recurrent architectures (LSTMs and GRUs with\nno attention, with additive attention (Bahdanau\net al., 2016), and with multiplicative attention (Lu-\nong et al., 2015)). Transformer models follow their\nusual implementation with self- and multi-headed\nattention. For each model type, we perform 10\nruns, initialized with different random initial seeds,\nand report median accuracy metrics. Recurrent\nunits are single-layer models, with hidden and em-\nbedding dimensions of 256. Transformers are 4-\nheaded, 3-layer models with hidden and embed-\nding dimensions of 128. All models are trained at\na learning rate of 0.01 using SGD optimization for\n100 epochs with early stopping.\n2.1 Polar Question Formation\nOur ﬁrst task involves the process of question for-\nmation discussed earlier. We borrow the formu-\nlation of this task from McCoy et al. (2020): the\ntraining dataset consists of an input sentence (a\nsimple declarative with relative clauses optionally\nmodifying the subject and object), a transformation\ntoken, D E C Lor Q U E S T, and an output sentence.\nThe transformation token speciﬁes what the form\nof the target output should be. Following the logic\nsurrounding example (1), examples with subject-\nmodifying relative clauses are never paired in the\ntraining data with the Q U E S Ttransformation token.\nAs a result, the network is not trained on sentences\nin which an auxiliary verb must be fronted past an\nintervening relative clause, and the target general-\nization is therefore ambiguous between something\nakin to MOVE -MAIN and MOVE -FIRST . While a\nnetwork that acquires the MOVE -FIRST generaliza-\ntion will succeed on the in-distribution TEST set\nconsisting of examples of the same structure as in\nthe training data, it will fail on the GEN set consist-\ning of input sentences with subject-relative clauses\nand the Q U E S Ttransformation.\nAll trained network types performed well on\nthe in-distribution TEST set, attaining mean full-\nsentence accuracies of at least 95%. In contrast,\nnone of the models succeeded on theGEN set in full\nsentence accuracy. Following McCoy et al. (2020),\nwe instead assess GEN set performance using the\nmore lenient metric of ﬁrst-word accuracy. Since\nthe GEN set includes only sentences with distinct\nauxiliary verbs in the main and relative clauses, the\nidentity of the ﬁrst output word reveals whether\nthe network has acquired a linear (MOVE -FIRST ) or\nhierarchical (MOVE -MAIN ) generalization. Results\nFigure 1: Proportion of ﬁrst-word predictions consis-\ntent with hierarchical generalization in the question\nGEN set. A ( +) denotes additive attention, ( ×), mul-\ntiplicative. Horizontal bars denote max, median, and\nmin values.\nare shown in Figure 1. As noted in McCoy et al.\n(2020), there is variation in performance among\nthe different types of recurrent networks: GRUs\nwith multiplicative attention achieved median ac-\ncuracy of 32.9%. Transformers exhibit the worst\nmedian performance among all architectures sur-\nveyed, with a median ﬁrst-word accuracy of just\n0.03% and virtually no variability across different\nrandom initializations. Instead, Transformer mod-\nels overwhelmingly predicted sequences consistent\nwith a linear MOVE -FIRST rule on the GEN set.\nThese results are robust across changes in learning\nrate.\n2.2 Tense Reinﬂection\nOur second mapping task, again borrowed from\nMcCoy et al. (2020) involves the reinﬂection of a\nsentence with a past tense verb into one with ei-\nther a past or present tense verb. Signiﬁcantly,\nthe English present tense involves structurally-\nconditioned agreement with the verb’s subject. In\ncomplex expressions like (2a), distractor nouns\nwith different number within the subject linearly\nseparate the verb from the subject, but the grammat-\nical agreement is nonetheless governed by a hierar-\nchical AGREE -SUBJECT relation (predicting (2b)).\nas opposed to an AGREE -RECENT relation (predict-\ning (2c)).\n(2) a. My newt near the elephants ran.\nb. My newt near the elephants runs.\nc. * My newt near the elephants run.\nOur datasets consist of past-tense English sentences\nas inputs, optionally with prepositional phrases or\nrelative clauses modifying the subject or object,\nalong with PRES and PAST transformation tokens\nthat indicate the form of the target output. For train-\ning and in-distribution test data, examples with the\nFigure 2: Proportion of linear and hierarchical predic-\ntions on the reinﬂection GEN set.\nPRES token do not have modiﬁed subjects, so that\nthe reinﬂection mapping is ambiguous between\nAGREE -SUBJECT and AGREE -RECENT . In con-\ntrast, the GEN set includes sentences where the two\nrules make different predictions (modiﬁed subjects\nwith distractor having distinct number). Results\nare shown in Figure 2. Like the recurrent architec-\ntures, Transformers systematically fail to exhibit\nhierarchical in favor of linear generalization.\n2.3 Negation\nOur third task involves the conversion of an af-\nﬁrmative sentence into a negative one. Negation\nrequires the insertion of the negative marker “not”\nimmediately prior to the main verb.\n(3) a. The bird will sing.\nb. The bird will not sing.\nWhen an adverbial clause is placed before or after\nthe main clause (4), the main verb is no longer\nconsistently the linearly ﬁrst or last verb in the\nsentence.\n(4) a. The bird will sing because the cat will swim.\nb. The bird will not sing because the cat will\nswim.\nc. Because the cat will swim the bird will not\nsing.\nOur dataset consists of afﬁrmative sentences, with\nadverbial clauses optionally preceding or following\nthe main clause. These are transformed either into\n(identical) afﬁrmatives or corresponding negatives.\nThe training and in-distribution test set excludes\nsentences with initial adverbial clauses that must\nbe mapped to negatives. As a result, this data set is\nambiguous between a linear NEG -FIRST generaliza-\ntion and a hierarchical NEG -MAIN . This ambiguity\nis resolved in theGEN set, which contains sentences\nwith preceding adverbials that must be converted\ninto negative sentences, following the NEG -MAIN\ngeneralization.\nAll models, including the Transformer, perform\nexceedingly well on in-distribution data, attaining\nnear-ceiling full-sentence accuracy on the TEST set.\nBy contrast, all models, again including the Trans-\nformer, fail uniformly on the GEN set, attaining\nnear-zero performance even using a more forgiv-\ning metric looking only at correct placement of the\nnegative marker. Closer examination of the model\noutputs on the GEN set reveals that networks of all\nsorts overwhelmingly produce predictions consis-\ntent with the linear generalization (NEG -FIRST ).\n2.4 Reﬂexive Anaphoric Interpretation\nOur ﬁnal task, similar to that of Kim and Linzen\n(2020) and Frank and Petty (2020), involves the\nsemantic parsing of a sequence into a predicate\ncalculus representation, as in (5).\n(5) Alice sees Bob →SEE (ALICE , BOB )\nFor entities whose meaning is context-independent,\nlike nouns or verbs, this task involves learning a\ncombination of token correspondence and form\ncomposition. As Frank and Petty (2020) note, re-\nﬂexive anaphora like “herself” present a challenge\nsince their meaning is not context-independent but\nrather conditioned on a linguistically-determined\nantecedent. In sentences with complex subjects,\nlike that in (6) with a prepositional phrase modiﬁer,\nthe identiﬁcation of the correct antecedent for the\nanaphor is conditioned not by the linear distance\nbetween a potential antecedent and the reﬂexive\nbut rather by the hierarchical relation between the\nantecedent and reﬂexive.\n(6) The boy by the king sees himself →\nSEE (BOY, BOY) ∧BY(BOY, KING )\nOur in-distribution data consists of sentences,\ntransitive and intransitive, paired with predicate\ncalculus representations of their meanings. Input\nsentences in this set may have complex subjects or\nthe reﬂexive objects (“himself” or “herself”), but\nnot both. As a result, the training and TEST data\ndoes not disambiguate whether the reﬂexive is co-\nreferent with the grammatical subject or the noun\nphrase immediately preceding the verb. The GEN\nset contains only sentences reﬂexive objects and\ncomplex subjects containing prepositional phrases,\nand therefore serves to distinguish between the lin-\near and hierarchical generalizations.\nAll models examined perform well on the TEST\nset, attaining median full sequence accuracy of\n100%. Results on the GEN set, as shown in Figure 3,\nare more varied. We categorize the predictions\nFigure 3: Proportion of reﬂexive-linear, subject-linear,\nand hierarchical predictions in the anaphora GEN set.\nmade by the network into three distinct classes:\nsubject-verb linear, where the model interprets the\nsubject of the verb as being the linearly most re-\ncent noun (incompatible with the training data);\nreﬂexive linear, where the model interprets the an-\ntecedent of the reﬂexive as being the linearly most\nrecent noun (compatible with the training set); and\nhierarchical, where the model correctly interprets\nboth the subject and antecedent in a manner consis-\ntent with the hierarchical structure of the sentence\n(also compatible with training). Transformers and\nGRU models overwhelming make predictions con-\nsistent with reﬂexive linearity. LSTMs are more\nvaried, with inattentive LSTMs attaining the high-\nest hierarchical scores of all network types with a\nmedian performance of 65.8%.\n3 Conclusion\nTransformers have shown great success on syntac-\ntic benchmarks. Is this because the architecture has\nuseful syntactic biases, or is it because cues to hier-\narchical structure are present in their training data?\nOur results ﬁnd no evidence for the former, suggest-\ning that their syntactic successes can mainly be at-\ntributed to their ability to leverage massive training\nsets rather than linguistically-relevant architectural\nbiases. Though the Transformer models studied\nhere were the best performers on in-distribution\ndata across all tasks, their strong preference for\nlinear over hierarchical generalization suggests an\nexplanation for their poor performance on tasks re-\nquiring structural generalization (Kim and Linzen,\n2020) despite their promise in other syntactically\nsensitive tasks. Finally, we note that the prefer-\nence we have observed for linear generalization is\nconsistent with previous theoretical work on the\n(limited) expressive power of Transformers (Hahn,\n2020; Merrill, 2019).\nReferences\nBen Ambridge, Caroline F. Rowland, and Julian M.\nPine. 2008. Is structure dependence an innate con-\nstraint? new experimental evidence from children’s\ncomplex-question production. Cognitive Science,\n32(1):222–255.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2016. Neural machine translation by jointly\nlearning to align and translate.\nNoam Chomsky. 1971. Problems of Knowledge and\nFreedom. Richard B. Russell lectures. Pantheon\nBooks.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nRobert Frank and Jackson Petty. 2020. Sequence-to-\nsequence networks learn the meaning of reﬂexive\nanaphora. In Proceedings of the Third Workshop on\nComputational Models of Reference, Anaphora and\nCoreference, pages 154–164, Barcelona, Spain (on-\nline). Association for Computational Linguistics.\nYoav Goldberg. 2019. Assessing BERT’s syntactic\nabilities. arXiv preprint arXiv:1901.05287.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of NAACL-HLT, page 1195–1205.\nMichael Hahn. 2020. Theoretical limitations of self-\nattention in neural sequence models. Transactions\nof the Association for Computational Linguistics,\n8:156–171.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4129–4138, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox,\nand Roger Levy. 2020. A systematic assessment\nof syntactic generalization in neural language mod-\nels. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics,\npages 1725–1744, Online. Association for Compu-\ntational Linguistics.\nNajoung Kim and Tal Linzen. 2020. COGS: A com-\npositional generalization challenge based on seman-\ntic interpretation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 9087–9105, Online. As-\nsociation for Computational Linguistics.\nYoon Kim, Alexander Rush, Lei Yu, Adhiguna Kun-\ncoro, Chris Dyer, and G ´abor Melis. 2019. Unsuper-\nvised recurrent neural network grammars. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers), pages 1105–1117,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nAdhiguna Kuncoro, Chris Dyer, John Hale, Dani Yo-\ngatama, Stephen Clark, and Phil Blunsom. 2018.\nLSTMs can learn syntax-sensitive dependencies\nwell, but modeling structure makes them better. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1426–1436, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nYongjie Lin, Yi Chern Tan, and Robert Frank. 2019.\nOpen sesame: Getting inside BERT’s linguistic\nknowledge. In Proceedings of the 2019 ACL Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP, pages 241–253, Florence,\nItaly. Association for Computational Linguistics.\nMinh-Thang Luong, Hieu Pham, and Christopher D.\nManning. 2015. Effective approaches to attention-\nbased neural machine translation. arXiv preprint\narXiv:1508.04025.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1192—-\n1202.\nR. Thomas McCoy, Robert Frank, and Tal Linzen.\n2020. Does syntax need to grow on trees? sources of\nhierarchical inductive bias in sequence-to-sequence\nnetworks. Transactions of the Association for Com-\nputational Linguistics, 8:125–140.\nWilliam Merrill. 2019. Sequential neural networks as\nautomata. B.S. thesis, Yale University.\nWilliam Merrill, Lenny Khazan, Noah Amsel, Yiding\nHao, Simon Mendelsohn, and Robert Frank. 2019.\nFinding hierarchical structure in neural stacks using\nunsupervised parsing. In Proceedings of the 2019\nACL Workshop BlackboxNLP: Analyzing and Inter-\npreting Neural Networks for NLP, pages 224–232,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nAmy Perfors, Joshua B Tenenbaum, and Terry Regier.\n2011. The learnability of abstract syntactic princi-\nples. Cognition, 118(3):306–338.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2021. A primer in bertology: What we know about\nhow bert works. Transactions of the Association for\nComputational Linguistics, 8:842–866.\nYikang Shen, Shawn Tan, Alessandro Sordoni, and\nAaron Courville. 2018. Ordered neurons: Integrat-\ning tree structures into recurrent neural networks.\narXiv preprint arXiv:1810.09536.\nMarten Van Schijndel, Aaron Mueller, and Tal Linzen.\n2019. Quantity doesn’t buy quality syntax with neu-\nral language models. In Proceedings of the Con-\nference on Empirical Methods in Natural Language\nProcessing and the International Joint Conference\non Natural Language Processing.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, undeﬁne-\ndukasz Kaiser, and Illia Polosukhin. 2017. Attention\nis all you need. In Proceedings of the 31st Interna-\ntional Conference on Neural Information Processing\nSystems, NIPS’17, page 6000–6010, Red Hook, NY ,\nUSA. Curran Associates Inc.\nAlex Warstadt and Samuel R. Bowman. 2020. Can\nneural networks acquire a structural bias from raw\nlinguistic data?\nAlex Warstadt, Yu Cao, Ioana Grosu, Wei Peng, Ha-\ngen Blix, Yining Nie, Anna Alsop, Shikha Bordia,\nHaokun Liu, Alicia Parrish, et al. 2019. Investigat-\ning BERT’s knowledge of language: Five analysis\nmethods with npis. In Proceedings of the Confer-\nence on Empirical Methods in Natural Language\nProcessing and the International Joint Conference\non Natural Language Processing, pages 2877—-\n2887.",
  "topic": "Generalization",
  "concepts": [
    {
      "name": "Generalization",
      "score": 0.7476283311843872
    },
    {
      "name": "Transformer",
      "score": 0.7278196215629578
    },
    {
      "name": "Computer science",
      "score": 0.6019927859306335
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4936326742172241
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4751812517642975
    },
    {
      "name": "Natural language processing",
      "score": 0.39150726795196533
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3518974483013153
    },
    {
      "name": "Algorithm",
      "score": 0.33347058296203613
    },
    {
      "name": "Mathematics",
      "score": 0.24821054935455322
    },
    {
      "name": "Engineering",
      "score": 0.06489142775535583
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I32971472",
      "name": "Yale University",
      "country": "US"
    }
  ],
  "cited_by": 10
}