{
  "title": "Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions",
  "url": "https://openalex.org/W4385570982",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2101834038",
      "name": "John Chung",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2162034732",
      "name": "Ece Kamar",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2073051687",
      "name": "Saleema Amershi",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2765407302",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4285113702",
    "https://openalex.org/W3010293452",
    "https://openalex.org/W2168573280",
    "https://openalex.org/W4220996411",
    "https://openalex.org/W3162670395",
    "https://openalex.org/W3014333092",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W3099617520",
    "https://openalex.org/W2891575196",
    "https://openalex.org/W3035542229",
    "https://openalex.org/W2245623202",
    "https://openalex.org/W3100268441",
    "https://openalex.org/W4285310604",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2114524997",
    "https://openalex.org/W2124332348",
    "https://openalex.org/W4226404919",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W3194325860",
    "https://openalex.org/W2094485533",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W4298857601",
    "https://openalex.org/W3115242847",
    "https://openalex.org/W4294554825",
    "https://openalex.org/W3206420877",
    "https://openalex.org/W2146388339",
    "https://openalex.org/W2966087730",
    "https://openalex.org/W3105190746",
    "https://openalex.org/W2967579878",
    "https://openalex.org/W2962721878",
    "https://openalex.org/W2949858875",
    "https://openalex.org/W4285210452",
    "https://openalex.org/W4312412605",
    "https://openalex.org/W4303614602",
    "https://openalex.org/W2964120993",
    "https://openalex.org/W3153451655",
    "https://openalex.org/W2952861497",
    "https://openalex.org/W2103490241",
    "https://openalex.org/W3035507081",
    "https://openalex.org/W2127058057",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4220832158",
    "https://openalex.org/W2148378803",
    "https://openalex.org/W3098341425",
    "https://openalex.org/W4287854464",
    "https://openalex.org/W2945289329",
    "https://openalex.org/W4292779060"
  ],
  "abstract": "Large language models (LLMs) can be used to generate text data for training and evaluating other models. However, creating high-quality datasets with LLMs can be challenging. In this work, we explore human-AI partnerships to facilitate high diversity and accuracy in LLM-based text data generation. We first examine two approaches to diversify text generation: 1) logit suppression, which minimizes the generation of languages that have already been frequently generated, and 2) temperature sampling, which flattens the token sampling probability. We found that diversification approaches can increase data diversity but often at the cost of data accuracy (i.e., text and labels being appropriate for the target domain). To address this issue, we examined two human interventions, 1) label replacement (LR), correcting misaligned labels, and 2) out-of-scope filtering (OOSF), removing instances that are out of the user's domain of interest or to which no considered label applies. With oracle studies, we found that LR increases the absolute accuracy of models trained with diversified datasets by 14.4%. Moreover, we found that some models trained with data generated with LR interventions outperformed LLM-based few-shot classification. In contrast, OOSF was not effective in increasing model accuracy, implying the need for future work in human-in-the-loop text data generation.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 575–593\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nIncreasing Diversity While Maintaining Accuracy: Text Data Generation\nwith Large Language Models and Human Interventions\nJohn Joon Young Chung\nUniversity of Michigan\njjyc@umich.edu\nEce Kamar\nMicrosoft Research\neckamar@microsoft.com\nSaleema Amershi\nMicrosoft Research\nsamershi@microsoft.com\nAbstract\nLarge language models (LLMs) can be used\nto generate text data for training and evalu-\nating other models. However, creating high-\nquality datasets with LLMs can be challenging.\nIn this work, we explore human-AI partner-\nships to facilitate high diversity and accuracy\nin LLM-based text data generation. We first\nexamine two approaches to diversify text gen-\neration: 1) logit suppression, which minimizes\nthe generation of languages that have already\nbeen frequently generated, and 2) temperature\nsampling, which flattens the token sampling\nprobability. We found that diversification ap-\nproaches can increase data diversity but often\nat the cost of data accuracy (i.e., text and labels\nbeing appropriate for the target domain). To ad-\ndress this issue, we examined two human inter-\nventions, 1) label replacement (LR), correcting\nmisaligned labels, and 2) out-of-scope filtering\n(OOSF), removing instances that are out of the\nuser’s domain of interest or to which no con-\nsidered label applies. With oracle studies, we\nfound that LR increases the absolute accuracy\nof models trained with diversified datasets by\n14.4%. Moreover, we found that some models\ntrained with data generated with LR interven-\ntions outperformed LLM-based few-shot classi-\nfication. In contrast, OOSF was not effective in\nincreasing model accuracy, implying the need\nfor future work in human-in-the-loop text data\ngeneration.\n1 Introduction\nTraining custom natural language classification\nmodels has become easier with many tools (e.g.,\nHuggingface1). However, data collection remains\na costly part of model building. For example, exist-\ning open-source datasets may not be usable if they\ndo not match the distribution of a model builder’s\ntarget domain or do not contain desired labels. In\nsuch cases, the model builder may need to collect\nand label new data which could be costly (e.g., in\n1https://huggingface.co/\nterms of the time and resources to scrape data or\npay people to generate or annotate new data).\nAdvances in generative large language mod-\nels (LLMs), such as GPT-3 (Brown et al., 2020),\npresent a novel approach for creating training data\nfor classification models (Yoo et al., 2021; Sahu\net al., 2022; Kumar et al., 2020). Model builders\ncan prompt an LLM with the domain of texts and\nlabels of interest and the LLM can quickly gener-\nate text data for the model builder’s needs. This\napproach allows model builders to acquire a large\namount of data even when they initially have no\nor few data instances. With the generated data, the\nmodel builder can train a separate affordable model\n(e.g., BERT (Devlin et al., 2019)) to perform the\nspecific task.\nWhile LLMs can directly support this classifica-\ntion task with few-shot learning, it might not be the\nbest option for every model builder—some might\nnot have enough resources (e.g., GPUs) or budget\n(e.g., credit for GPT-3) to run expensive models.\nOthers might be concerned about privacy or secu-\nrity issues when they use LLMs from external APIs\n(e.g., OpenAI API). In such cases, generating data\nfrom LLMs and training custom models could be\na more viable approach. Moreover, if we share\ngenerated datasets within the community, we can\nalso benefit those who do not have access to LLMs.\nLastly, we can also use generated datasets to test\nmodels. With these benefits of generating new text\ndatasets with LLMs, the practical concern is how\nto generate high-quality datasets.\nIn this work, we investigate human-AI partner-\nships to efficiently create high-quality datasets with\nLLM-based text generation. High-quality datasets\nshould have high diversity and coverage, informing\nthe extent of data that the model may encounter. At\nthe same time, the generated text should have high\naccuracy, being relevant to the model’s target task\nwhile having accurate accompanying labels. To\nthese ends, we first study two technical approaches\n575\nto diversify text generation (Section 3): 1) logit sup-\npression, which diversifies the generated texts by\ndecreasing the probability of sampling tokens that\nhave already appeared frequently in the previous\ngeneration, and 2) temperature sampling, which\nflattens the probability distribution of sampled to-\nkens to pick less likely texts. From an experiment\non eight classification tasks with GPT-3 as a text\ngenerator (Section 4), we found that diversifica-\ntion approaches can have mixed results. While\nincreasing data diversity, these approaches can hurt\naccuracy in generation and similarity to the original\ndatasets for the task.\nWe demonstrate that human interventions (Sec-\ntion 5) are the key to resolving these issues in text\ngeneration diversification. We examine human in-\nterventions of replacing inaccurate labels with ac-\ncurate ones (label replacement) and filtering out-\nof-scope data (out-of-scope data filtering). With\noracle studies (Section 6), we found that replac-\ning all incorrect labels increased model accuracy\nby 14.4% when we used both logit suppression\nand high temperature. This performance increase\nbrings in practical benefits—without label replace-\nment, the average accuracy of models trained with\nGPT-3-generated data was lower than that of GPT-3\nclassification with few-shot learning, but with 180\ninstances label-replaced, the models trained with\ngenerated data started to outperform GPT-3 few-\nshot classification. Out-of-scope data filtering had\nlimited utility in increasing model accuracy, possi-\nbly due to the negative impact of removing training\ninstances. We discuss how human interventions\ncan further facilitate the diversity and accuracy of\ntext data generation.\nOur contributions are:\n• A methodolgy that combines LLM generation\napproaches and human supervision for diver-\nsified and accurate data generation.\n• An experiment showing how text generation\ndiversification impacts the accuracy of trained\nmodels and other qualities of the data, such as\ndiversity and accuracy in the generation.\n• Oracle studies on how human effort to replace\nmisaligned labels and filter out-of-scope data\ninstances can impact the performance of mod-\nels trained on data generated with text diversi-\nfication.\n2 Related Work\n2.1 Text Data Generation for Model Training\nIn NLP, data augmentation, where data are multi-\nplied based on existing data, is one context where\ntext data are generated for model training. There\nwere many approaches, from replacing words with\nsynonyms (Wei and Zou, 2019; Zhang et al., 2015),\nto randomly editing texts (Wei and Zou, 2019), pre-\ndicting replaceable words (Ng et al., 2020), back-\ntranslating (Fadaee et al., 2017), generating label-\nflipped data (Zhou et al., 2022), or using reinforce-\nment learning to condition generation (Liu et al.,\n2020). Inspired by MixUp (Zhang et al., 2018),\nwhich mixes different examples in vision data, re-\nsearchers also blended texts to augment data (Guo\net al., 2020; Sun et al., 2020; Zhang et al., 2022).\nOther approaches generate texts by learning from\ndifferent datasets (Xia et al., 2020; Hou et al., 2018;\nChen et al., 2020; Yoo et al., 2019).\nRecently, with the generative capacity of LLMs,\nresearchers proposed generating datasets with zero\nor very few samples and training a separate model\nto serve the specific task (Kumar et al., 2020; Yoo\net al., 2021; Sahu et al., 2022; Yuan et al., 2021;\nHartvigsen et al., 2022). As this approach would\nextract information from large models, they would\nbe analogous to knowledge distillation (Phuong\nand Lampert, 2019; Hinton et al., 2015) or dataset\ndistillation (Wang et al., 2018; Cazenavette et al.,\n2022). LLM-generated data has also been used to\ntest other trained models (Ribeiro and Lundberg,\n2022; Perez et al., 2022). In this work, we extend\nthe previous work by investigating the generation\nof high-quality data with accurate diversification.\n2.2 Text Generation with LLMs\nAs the size of language models increases, re-\nsearchers found that LLMs can serve different\ngeneration tasks based on input prompts and ex-\namples (Brown et al., 2020). This approach can\nbe used to generate text data with instructional\nprompts and a few examples. However, for the\ngenerated data to be useful, diversity and cover-\nage should be ensured. Control of the sampling\ntemperature (Goodfellow et al., 2016) would be rel-\nevant, as it facilitates the unlikely generation, but\nit was not evaluated for the facilitation of diversity\nand coverage. Inspired by previous work on con-\ntrolling LLM generation, we examine human-AI\napproaches to steer data generation to have higher\ndiversity while securing accuracy in the alignment\n576\nof specified labels.\n2.3 Human-In-The-Loop\nHuman interventions are imperative to train high-\nperformance machine learning models, as people\ncurate datasets, configure model architectures, and\ntest the trained models. Researchers investigated\napproaches to make human interventions more\ninteractive in model training pipelines, by clos-\ning gaps between model training and data cura-\ntion (Fogarty et al., 2008; Amershi et al., 2009,\n2012; Levonian et al., 2022), humans extracting\nfeatures (Branson et al., 2010; Cheng and Bern-\nstein, 2015), interactively changing the error pat-\nterns (Kapoor et al., 2010; Talbot et al., 2009), or\ninteractively testing models (Wu et al., 2019; Yuan\net al., 2022; Ribeiro et al., 2020; Cabrera et al.,\n2021; Suh et al., 2019). Generative models intro-\nduce novel approaches to interactively tune and\nevaluate models by leveraging generated results as\ndata instances for training and testing (Ribeiro and\nLundberg, 2022). In this work, we explored har-\nnessing diversified and accurate datasets by com-\nbining LLM-based text generation and human in-\nterventions.\n3 Diversified Text Data Generation\nWe lay out the desired characteristics of the datasets\nfor model building. Then, we introduce approaches\nto generate diversified datasets with LLMs.\n3.1 Goals\nIdeal classification datasets need to have the fol-\nlowing characteristics: 1) Scoped: fall in the model\nbuilder’s domain of interest while classifiable with\nlabels of interest, 2) Label accurate: accompany\naccurate labels, and 3) Diverse: cover cases the\nmodel would encounter during test time. These\ngoals are difficult to achieve simultaneously but\nneed to be balanced. Only considering diversity,\nrandomly generating any text would be enough, but\nit would hurt scope and label accuracy. Likewise,\nonly considering the scope and label accuracy, gen-\nerating an accurate but limited variety of text would\nbe enough, but it would hurt the diversity.\n3.2 Diversifying Approaches\nWe introduce the setting to use LLM-based data\ngeneration for model training. Then, we lay out\ntwo approaches to promote diversity in text data\ngeneration. We also note their potential risks of\nharming the scope and accuracy.\nFigure 1: Examples of Diversification Approaches.\n3.2.1 Settings for Data Generation\nWhen prompting LLMs, we consider 1) a text type\nand 2) labels in the prompts. While there can be\nmany different prompts, in our paper, we used the\nfollowing prompt:\nWrite a movie review (text type) to cover all fol-\nlowing elements\nElements: positive sentiment (label)\nMovie review (text type): \"This is a great movie\"\n(A)\nModel builders can also prepend examples in the\nsame format. The generation process is iterative,\nand model builders can use intermediate data points\nas examples in later prompts. The model builders\ncan generate data until they reach the desired num-\nber of data points. With the generated data, the\nmodel builder would finetune a separate smaller\nmodel that serves the target task. With this ap-\nproach of finetuning a smaller model, there can be\na question of whether finetuning a separate model\nwould result in higher accuracy than using zero-\nshot or few-shot learning of the LLM. In the later\nstudy, we show the cases where finetuned smaller\nmodels perform better than the LLM.\n3.2.2 Logit Suppression\nLogit suppression is a diversification approach that\nsuppresses tokens that have already been generated\nfrequently in the intermediate dataset (Figure 1a).\nWith this approach, the generation pipeline logs\nthe frequency of tokens that have been generated\nso far. Then, to diversify the selection of tokens,\nlogit suppression decreases the probability of high-\nfrequency tokens. However, with this approach,\nsome tokens that could contribute to accurate gen-\neration can be suppressed.\n3.2.3 High Temperature\nThe temperature of sampling distribution (Good-\nfellow et al., 2016) controls how “flat” the token\nsampling probability is (the equation is explained\nin Appendix A). High temperature leads to “flatter”\ntoken sampling probabilities (Figure 1b), increas-\ning the probability of sampling “less likely” tokens\n577\nand diversifying generation. Similar to logit sup-\npression, extremely high temperatures can result in\ntokens irrelevant to the prompt, hurting accuracy in\ngeneration results.\n4 Experiment1: Diversified Text Data\nGeneration\nWe evaluated how diversification approaches im-\npact the diversity of the generated data and the\naccuracy of models trained with the dataset.\n4.1 Experiment Settings\n4.1.1 Tasks\nWe used tasks from eight datasets. SST-2 (Socher\net al., 2013) is a binary sentiment classification\ndataset from Rotten Tomatoes movie reviews.\nClickbait classification dataset (CB) (Chakraborty\net al., 2016) is news headlines labeled either click-\nbait or non-clickbait. CARER (Saravia et al., 2018)\nis Twitter statements labeled with one of the six\nemotion categories. PubMed 200k RCT (Dernon-\ncourt and Lee, 2017) has five classes regarding the\nroles of sentences in medical papers. The subjec-\ntivity dataset (SUBJ) is movie review texts labeled\nsubjective or objective (Pang and Lee, 2004). For-\nmality classification dataset ( FO) (Lahiri, 2015)\nhas labels on whether the text is formal or informal.\nHWU64 (Liu et al., 2021) is a dataset with hu-\nman utterances to chatbots, and we used 18 domain\nclasses for our experiments. Corpus of Linguistic\nAcceptability (COLA) (Warstadt et al., 2019) is\npublication texts with annotations on whether the\ntext is grammatically correct or not.\n4.1.2 Generation Method\nAs a generative LLM, we used the\ntext-davinci-002 model of GPT-3 through\nOpenAI API Access with Prompt A. We list the\nspecific text types and labels used for each dataset\nin Appendix B.1. The generation process was\niterative, with 20 data points generated with a\nsingle prompt for each API call. As a single\nprompt can only generate data instances for a\nsingle label, the generation process cycled through\nall considered labels while balancing the number\nof instances for each class. As our tasks dealt with\nshort text data, we limited the generation length\nto 100 tokens. We set the frequency penalty and\ntop p to 0.02 and 1, respectively. Except for SST-2,\nwe generated 5600 instances for a single training\ndataset. For SST-2, we generated 6922 data\npoints. We chose these numbers to ensure a low\ngeneration budget while having fair quality when\ntraining models. Specifically, with a maximum\nlength of 100 tokens for each generated instance,\nif the prompt includes examples for n classes, the\nnumber of required tokens for each instance would\nbe (100+30) × (n+1) (where 30 come from the\ninstructional prompts). With the generation pricing\nof $0.02/1000 tokens for text-davinci-002\nmodel, 5600 and 6922 instances resulted in\nmaximum spending of $14.56 × (n+1) and $17.80\n× (n+1), respectively. In our pilot tests, model\naccuracy saturated after these numbers of instances.\nFor the oracle training dataset, with which we\ncompared the quality of the datasets, we sampled\ninstances from the original training dataset for\nthe task. The test dataset was sampled from the\noriginal test dataset. We provide details on how we\nsampled these instances in Appendix B.2.\nGeneration Conditions In addition to logit sup-\npression and temperature sampling, we also con-\nsider example seeding , whether the generation\npipeline begins with an initial set of example in-\nstances. We can use multiple approaches simultane-\nously (e.g., using logit suppression and temperature\nsampling together), and how these approaches in-\nteract is also the scope of our questions. For a\nsingle combination of conditions, we generated\nthree datasets, as there could be some variance in\nthe results with the initial seeds and the examples\ngenerated initially.\nWe instantiated logit suppression with the logit\nbias function in OpenAI API Access2, which can\nincrease or decrease the probability of sampling to-\nkens. Every time we complete a single generation\niteration, we recorded the frequency of tokens gen-\nerated by GPT-3. As the OpenAI API only allows\n100 tokens for logit biasing, we suppressed only\nthe 100 most appeared tokens. Specifically, for the\nlogit bias weights, we multiplied the token appear-\nance ratio (in percentage) by -7.5 while capping the\nminimum weight at –7.5. For temperature sam-\npling, we used four temperature values, 0.3, 0.7,\n0.9, and 1.3. When seeding examples, we first ran-\ndomly sampled 18 examples from oracle training\ndata with a balanced number of labels. Only for\nPubMed, which has five classes, we used 15 seed\nexamples. We used sampled data points as an initial\nexample pool. With example seeding, from the first\n2https://beta.openai.com/docs/api-reference/\ncompletions/create#completions/create-logit_bias\n578\n0.4\n0.6\n0.8\n1.0\nModel Accuracy\n0.4\n0.6\n0.8\n1.0\nLabel Accuracy\n0.00\n0.05\n0.10\n0.15\n0.20\nDiversity\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nSimilarity\nOracle\nGPT Zero\nGPT Few\nBase Similarity\nT emp=0.3, Logit Sup=X\nT emp=0.7, Logit Sup=X\nT emp=0.9, Logit Sup=X\nT emp=1.3, Logit Sup=X\nT emp=0.3, Logit Sup=O\nT emp=0.7, Logit Sup=O\nT emp=0.9, Logit Sup=O\nT emp=1.3, Logit Sup=O\nExample=X\nExample=O\nFigure 2: Impact of logit suppression and high temperatures on model accuracy, label accuracy, diversity, and\nsimilarity to the oracle dataset, averaged across eight tasks. Bars without hatches start generation without examples\nwhile those with hatches start with few-shot generation. Throughout this paper, error bars indicate 95% confidence\ninterval.\ngeneration iteration, examples were randomly cho-\nsen from the pool. Without the seeding examples,\nwe completed the first cycle of generations as a\nzero-shot generation. After the first cycle, since we\nwould have generated data instances for all labels,\nwe added examples to the prompt. When adding\nexamples, we randomly sampled the examples for\nall labels, one example for each label.\n4.1.3 Training Method\nWith the generated data, we finetuned base size\nBERT (Devlin et al., 2019) classifiers with 109M\nparameters using pretrained weights from the Hug-\ngingface Transformer library (Wolf et al., 2020)\nwith a randomly initialized fully connected clas-\nsifier layer. For each dataset, we trained the five\ndifferent models with the same dataset. With three\ndatasets for each combination of approaches, it\nresulted in 15 models for a condition. While train-\ning, Adam optimizer was used, with a learning rate\nof 3e-5 and a warm-up period of 3 epochs. We\nadopted the early stopping with the patience of five\ntraining epochs. We used PyTorch and RTX A6000\nGPUs for training.\n4.2 Metrics\nWe compared the accuracies of models trained with\ngenerated data to 1) models trained with oracle\ndatasets (oracle model) and 2) GPT-3’s few-/zero-\nshot classifications ( text-davinci-002). For\nGPT-3 few-shot learning, we used 18 examples\n(15 only for PubMed) with the same number of\nexamples for each label. We also measured the\ndiversity of the dataset using Remote-Clique met-\nric (Rhys Cox et al., 2021), which is the average\nmean pairwise distances. Specifically, we embed-\nded generated data with BERT (Devlin et al., 2019),\nthen calculated the distances. We also evaluated\nlabel accuracy, which is the accuracy of the align-\nment between the generated texts and the specified\nlabels. For this metric, except for SST-2, we used\nthe oracle model as the evaluator. For SST-2, we\nused GPT-3 few-shot classification as the evalua-\ntor, as it has higher accuracy than the oracle model.\nWe also measured the similarity of the generated\ndataset to the oracle dataset with the average mean\npairwise distances between the two. For similarity,\nwe also used BERT to embed the generated texts.\n4.3 Results\nFigure 2 shows the results of the first experiment\nfor all tasks. The first column shows the model\naccuracy results. It also shows the accuracy of\nzero-shot and few-shot GPT-3 classification (gray\nsolid and dashed line, respectively) and the model\ntrained with the oracle training dataset (purple line).\nThe second column shows the label accuracy, and\nthe third column shows the diversity. The diversity\nplots also show the diversity of oracle datasets (pur-\nple line). The last column shows the similarity. It\nalso shows the base similarity (brown line), which\nis the average distance between all the different\ndatasets that we considered.\nFirst, to evaluate how diversity, label accuracy,\nand similarity impact model accuracy, we per-\nformed a linear regression analysis. The analysis\nshowed that label accuracy, diversity, and similarity\nare positively correlated with model accuracy, with\nsignificance (coef=.4797 and p<0.001 for label ac-\ncuracy, coef=.2260 and p<0.001 for diversity, and\ncoef=0.1980 and p<0.005 for similarity).\n579\nRegarding specific patterns, logit suppression in-\ncreased diversity while hurting the label accuracy\nand the similarity to the oracle dataset. High tem-\nperature increased diversity and decreased label\naccuracy, but to a smaller degree than logit sup-\npression. The application of each diversification\napproach increased the model accuracy, but when\nused together, the benefit did not add up. For in-\nstance, in Model Accuracy of Figure 2, each high\ntemperature (1.3, red light bars) and logit suppres-\nsion (dark blue bars) could increase the model ac-\ncuracy from when using a low temperature (0.3,\nlight blue bars). However, when using them to-\ngether (dark red bars), the resulting accuracy was\nnot much different from only using high temper-\natures (light red bars). It indicates that the effect\nof logit suppression has diminished by using high\ntemperatures and logit suppression together. Seed-\ning examples increases label accuracy and model\naccuracy. Examples also slightly increased diver-\nsity when used without logit suppression. Whether\nmodels trained with LLM-generated data would\nhave higher accuracy than zero- or few-shot learn-\ning of LLMs depends on the task. We provide a\ndetailed result on each task in Appendix C.\n5 Human Interventions to Fix Inaccurate\nText Generation\nThe first study shows that diversifying approaches\ncan have mixed effects, hurting the accuracy in gen-\neration. We propose two human interventions to\nimprove the generated data, based on issues that\nwe found from qualitatively analyzing the gener-\nated data. The first is label replacement (LR) ,\nswitching the misaligned label to the correct one.\nThe second is out-of-scope data filtering (OOSF),\nwhich removes instances that are outside the do-\nmain of interest and do not match any labels (OOS\ninstances).\nWhile LR and OOSF might facilitate accurate\ngeneration with diversifying approaches, inspect-\ning all data points can require a lot of effort. Hence,\nwe propose a simple way to scale the effort of the\nmodel builder, which is training a proxy model.\nWith this approach, model builders will first label\na small number of data points. Then, with those\nlabels, they will train binary classifiers as proxy\nmodels, where each learns about a single label (i.e.,\na label class from labels of interest or if the instance\nis out of scope). For unlabeled data points, proxy\nmodels can make inferences on behalf of the model\nbuilder. We introduced the specific implementation\nof this approach in Section 6.\n6 Experiment2: Human Interventions\nFor Diversifed Text Generation\nWe evaluated LR and OOSF. Except for adding LR\nand OOSF, we used the same tasks, datasets, train-\ning methods, and metrics as in Section 4. In this\nsection, we focus on reporting results for two tem-\nperature values, 0.3 and 1.3. We present the results\nwith the rest of the temperatures in Appendix E.\nAlso, in this section, when reporting, we merged\nconditions with and without example seeding.\n6.1 Experiment Settings\n6.1.1 Label Replacement\nFor LR, we conducted an oracle experiment. For\neach task, we used the highest accuracy model as\nthe oracle labeler. Therefore, we used oracle mod-\nels as a labeler, but only for SST-2, we used GPT-3\nfew-shot classification as a labeler. We conducted\nLR on the datasets generated in experiment 1.\nWe had two approaches for LR: 1) do LR to all\ndata points and 2) use proxy models with LR on\npartial data. For 1), we inspected all generated\ntexts with simulated labelers and replaced labels\nas the labelers predicted. For 2), we sampled a set\nof instances from the generated dataset, applied\nthe oracle labeler to them, and then trained proxy\nmodels with those data. Specifically, we sampled\n90, 180, or 270 data instances. When training, for\neach class, we trained a proxy model that performs\nbinary classification for the class. For each proxy\nmodel, the data instances labeled with the target\nlabel were used as positive instances, while the rest\nwere used as negative instances. We applied proxy\nmodels to the uninspected data to obtain confidence\nscores for each label. For each class, we calculated\nthe final score as follows:\nSf,i = Ss,i ∗ w + Sp,i ∗ (1 − w) (1)\nwhere for the class i, Sf,i is the final score, Sp,i\nis the confidence score of the proxy model, Ss,i is\nif the class is specified when generating the text (1\nwhen the class is specified, 0 otherwise), and w is\nthe weighting constant. We consideredSs,i as there\ncan be a chance that the proxy model is inaccurate\nand the correct labels are swapped. For our experi-\nment, we used w of 0.3. We chose the label with\nthe highest final score as the label to be replaced.\n580\nTask Ratio Task Ratio\nCARER 20.56% CB 1.39%\nCOLA 0.00% FO 0.56%\nHWU64 0.28% PubMed 1.11%\nSST-2 3.61% SUBJ 3.06%\nTable 1: Ratio of out-of-scope instances from 360 sam-\nples.\nTask Accuracy (std) Task Accuracy (std)\nCARER 94.93 (2.20) CB 100 (0.00)\nSST-2 97.18 (0.89) SUBJ 97.5 (1.04)\nTable 2: OOSF proxy model performance. Note that\nCB only had five OOS instances, with one used for test.\nFor training proxy models, we trained linear sup-\nport vector classifiers with a maximum iteration of\n10000 while using texts embedded with BERT (De-\nvlin et al., 2019) as input. We chose to train mul-\ntiple proxy models for each class over training a\nsingle proxy model for all classes, as it tends to\nbe more reliable in our pilots when there are many\nclasses. As the labeling of the proxy model de-\npends on the initial samples, for each generated\ndataset in experiment 1, we applied the approach\nfive times.\n6.1.2 Out-of-Scope Filtering\nWith OOSF, we first tried to understand how OOS\ninstances occur. Therefore, we sampled 360 data\ninstances for each task from the union of all the\ndatasets generated for the task. Then, an author\nserved as the oracle and annotated if they were\nOOS or not. Note that, as the definition of OOS\ninstance, we filtered those instances that are out-\nside the task domain or to which no label is appli-\ncable. We found that COLA, FO, HWU64, and\nPubMed have zero to four instances of OOS (Ta-\nble 1). For the later analysis, we only considered\nthe rest of the datasets, with at least five OOS in-\nstances. We present examples of OOS instances in\nAppendix D.1.\nWith the annotated data, we trained proxy mod-\nels to annotate the instances unseen by the author,\nwhich were binary linear support vector classifiers\nwith the maximum iteration of 10000 and BERT-\nembedded inputs. With the trained model, we did\nOOSF on the datasets generated in experiment 1.\nTable 2 shows the accuracy of the proxy model,\nwhen we divide the annotated data into training\nand test sets with an 8:2 ratio, with a split of ten\ntimes. Note that the perfect accuracy in CB is be-\ncause we identified only five OOS instances from\n1.0\n0.9\n0.8\n0.7\n0.6\n0.9\n0.85\n0.8\n0.75\n0.7\n0.65\n0.6\nFigure 3: Impact of label replacement on label accuracy\nand model accuracy. Throughout this paper, error areas\nindicate 95% confidence interval.\nour samples, which are extremely few.\nAfter applying LR or OOSF, we trained BERT\nmodels that serve the target task. For each dataset\nthat applied LR without proxy models or used\nOOSF, we ran the training five times. For each\ndataset that used LR with proxy models, since each\ndataset from experiment 1 has been label-replaced\nfive times, we ran training only once. With this\napproach, we acquired 15 model accuracy results\nfor each task and condition.\n6.2 Results\n6.2.1 Label Replacement\nLabel Accuracy and Model Accuracy in Figure 3\nshows the results with LR. It shows how model\naccuracy and label accuracy change with the num-\nber of instances inspected (x-axis). Other metrics,\ndiversity, and similarity would not change with LR,\nas it keeps the texts as they are. For model accuracy,\nwe also visualized the performance of oracle mod-\nels and the GPT-3 few-/zero-shot classification.\nLR increases the model accuracy and label ac-\ncuracy. Moreover, with more labels inspected,\nthe model accuracy and label accuracy further in-\ncreased. LR also added more values to logit sup-\npression. For example, without LR, using both\nhigh temperature (1.3) and logit suppression did\nnot have a comparative benefit over using only\nhigh temperature. However, with label replace-\nment, the addition of logit suppression started to\nbenefit the model accuracy when using high tem-\nperature. When doing LR with proxy models, the\nbenefit of logit suppression increased with more in-\nstances inspected, but with full LR, the size of this\ngap decreased a little bit. With LR of all instances,\nusing both high temperature and logit suppression\nincreased the absolute model accuracy by 17.8%,\ncompared to when using neither. It was greater than\n581\nFigure 4: The ratio of instances filtered with OOSF, and its impact on model accuracy, label accuracy, diversity, and\nsimilarity, in aggregation across all tasks. As we examined the effect of OOSF with LR, for model accuracy and\nlabel accuracy, numbers left to +OOS indicate how many instances are inspected with LR.\nthe increase from diversification approaches when\nLR was not used (9.4%). Furthermore, with high\ntemperature and logit suppression, using LR on all\ninstances could increase the absolute model accu-\nracy by 14.4% compared to not doing LR. When\na high temperature and logit suppression are used\ntogether, the model accuracy outperformed GPT3’s\nfew-shot classification when LR was done for 180\ninstances. Across tasks, we found that specific pat-\nterns on how diversification approaches and LR\nimpact the model accuracy can vary between tasks.\nWe provide details in Appendix E.1.\n6.2.2 Out-of-Scope Instances Filtering\nFigure 4 shows how many instances were filtered\nwith OOSF and how it affects model accuracy, la-\nbel accuracy, diversity, and similarity. We present\nmodel accuracy from both unbalanced and bal-\nanced data: when we balanced data, we used\ndatasets with the same number of instances across\ndifferent conditions by subsampling data with the\nsmallest size of the filtered dataset. It was because\nfiltering can make the number of instances different\nbetween conditions. For unbalanced data, we did\nnot balance the number of instances.\nOOSF either increases or maintains label accu-\nracy and similarity while decreasing or maintaining\ndiversity, but there was no unified pattern of how\nthey impact the model accuracy. There tend to be\nfew OOS-filtered instances without diversification\napproaches. For example, with a temperature of 0.3\nand without logit suppression, OOSF removed very\nfew data instances. Consequently, label accuracy,\ndiversity, and similarity remained the same with\nOOSF. Without diversification approaches, the ac-\ncuracy of trained models tends to be more unstable\nwith large confidence intervals. On the other hand,\nwith diversification approaches, OOSF removed\nmore instances, and hence there were slightly more\nchanges in label accuracy, diversity, and similarity,\nwith small increases in label accuracy and similar-\nity while decreasing diversity. However, in some\ncases, these changes were subtle or within the 95%\nconfidence intervals. Moreover, how the OOSF\nchanges the model accuracy depends on the spe-\ncific task and condition. We provide the OOSF\nresults for each task in Appendix E.2.\n7 Conclusion\nIn this work, we investigate approaches to harness\nLLMs and human efforts to generate text classi-\nfication datasets with high accuracy and diversity.\nWe study two text generation diversification ap-\nproaches, 1) logit suppression, which restrains gen-\nerating already frequently generated tokens, and 2)\nhigh temperature, which flattens the sampling prob-\nability of tokens. We found that they diversify text\ngeneration but hurt the accuracy in aligning speci-\nfied labels with the generated data. We experiment\nwith two human intervention approaches, 1) replac-\ning misaligned labels with more adequate ones, and\n2) filtering out-of-scope instances. We found that\nreplacing labels makes diversification approaches\nmore beneficial by increasing the accuracy of mod-\nels trained with the generated dataset. On the other\nhand, efficient filtering of out-of-scope instances\ndid not have a positive impact on the model accu-\nracy.\n8 Limitations\nOur implementation of proxy models applies those\nmodels after the whole data is generated. Due\nto this, in the resulting dataset, the number of in-\nstances can often be unbalanced between labels.\nSuch a limitation might be addressable by training\nproxy models from intermediate datasets with a\nsmaller number of instances, and using those mod-\nels while generating the rest of the dataset. As\nthe data become unbalanced during the generation,\n582\nthe generation pipeline can try to generate more\ninstances with labels that are a minority in the in-\ntermediate dataset. However, when we piloted this\napproach, we identified potential problems. First,\nintermediately trained proxy models could perform\nworse than those trained after all data are generated,\ndue to the lower diversity in intermediate data used\nto train proxy models. Second, if many data points\ngenerated with a specific label (label a) actually\nbelong to another label (label b), there can be cases\nwhere most instances of label b come from the\nprompt with label a. It can skew the linguistic pat-\nterns of instances within the dataset, as only a small\nnumber of texts for label b might have been from\nthe prompt with label b. Advanced approaches to\naddress these issues can be future work directions.\nOur implementation of efficient OOSF was not\neffective in increasing model accuracy. It might be\ndue to the negative impact of removing instances,\nsuch as filtering instances on the decision boundary.\nAs our study of OOSF was not complete, future\nwork is necessary. Applying OOSF to the entire\ngenerated dataset and seeing the impact of their\nremoval would be the first step. With a comprehen-\nsible understanding of OOSF, we would be able\nto design better OOSF strategies, such as filtering\ninstances with various criteria.\nIn this work, we only examined the\ntext-davinci-002 model of GPT-3. Al-\nthough we believe that the overall trends of results\nwould be similar for other models, examining\nother models with our approaches is a necessary\nfuture work. We also examined only one prompt\n(Prompt A), while there may be other options. In\nAppendix F, we present partial results on using\nanother prompt, showing that our approach is\ngeneralizable to other prompts. Combining human\ninterventions with automatic annotation error\ndetection (Klie et al., 2023) can be another future\ndirection.\n9 Ethics Statement\nLLM-generated text data could have replicated bi-\nases within the used LLM. Diversification might\nalleviate such issues, as it steers the LLM to gener-\nate texts that it considers less probable, but bias can\nstill exist after using the approach. More human\nintervention approaches can be a potential solution.\nFor example, the model builder can provide more\nspecific prompts and examples to counter the bi-\nased generation (Hartvigsen et al., 2022). However,\nthese approaches still would have limitations and\nhow these approaches would impact the data bias\nand the resulting model performance would need\nto be further researched.\nAcknowledgements\nWe want to thank Microsoft Research for support-\ning the work.\nReferences\nSaleema Amershi, James Fogarty, Ashish Kapoor, and\nDesney Tan. 2009. Overview based example selec-\ntion in end user interactive concept learning. In Pro-\nceedings of the 22nd Annual ACM Symposium on\nUser Interface Software and Technology, UIST ’09,\npage 247–256, New York, NY , USA. Association for\nComputing Machinery.\nSaleema Amershi, James Fogarty, and Daniel Weld.\n2012. Regroup: Interactive machine learning for\non-demand group creation in social networks. In\nProceedings of the SIGCHI Conference on Human\nFactors in Computing Systems, CHI ’12, page 21–30,\nNew York, NY , USA. Association for Computing\nMachinery.\nSteve Branson, Catherine Wah, Florian Schroff, Boris\nBabenko, Peter Welinder, Pietro Perona, and Serge\nBelongie. 2010. Visual recognition with humans\nin the loop. In Proceedings of the 11th European\nConference on Computer Vision: Part IV, ECCV’10,\npage 438–451, Berlin, Heidelberg. Springer-Verlag.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nÁngel Alexander Cabrera, Abraham J. Druck, Jason I.\nHong, and Adam Perer. 2021. Discovering and val-\nidating ai errors with crowdsourced failure reports.\nProc. ACM Hum.-Comput. Interact., 5(CSCW2).\nGeorge Cazenavette, Tongzhou Wang, Antonio Torralba,\nAlexei A. Efros, and Jun-Yan Zhu. 2022. Dataset\ndistillation by matching training trajectories. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition.\nAbhijnan Chakraborty, Bhargavi Paranjape, Sourya\nKakarla, and Niloy Ganguly. 2016. Stop clickbait:\n583\nDetecting and preventing clickbaits in online news\nmedia. In 2016 IEEE/ACM International Conference\non Advances in Social Networks Analysis and Mining\n(ASONAM), pages 9–16.\nJiaao Chen, Zichao Yang, and Diyi Yang. 2020. Mix-\nText: Linguistically-informed interpolation of hid-\nden space for semi-supervised text classification. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 2147–\n2157, Online. Association for Computational Lin-\nguistics.\nJustin Cheng and Michael S. Bernstein. 2015. Flock:\nHybrid crowd-machine learning classifiers. In Pro-\nceedings of the 18th ACM Conference on Computer\nSupported Cooperative Work & Social Computing,\nCSCW ’15, page 600–611, New York, NY , USA.\nAssociation for Computing Machinery.\nFranck Dernoncourt and Ji Young Lee. 2017. PubMed\n200k RCT: a dataset for sequential sentence clas-\nsification in medical abstracts. In Proceedings of\nthe Eighth International Joint Conference on Natu-\nral Language Processing (Volume 2: Short Papers),\npages 308–313, Taipei, Taiwan. Asian Federation of\nNatural Language Processing.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nMarzieh Fadaee, Arianna Bisazza, and Christof Monz.\n2017. Data augmentation for low-resource neural\nmachine translation. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 567–573,\nVancouver, Canada. Association for Computational\nLinguistics.\nJames Fogarty, Desney Tan, Ashish Kapoor, and Simon\nWinder. 2008. Cueflik: Interactive concept learning\nin image search. In Proceedings of the SIGCHI Con-\nference on Human Factors in Computing Systems ,\nCHI ’08, page 29–38, New York, NY , USA. Associa-\ntion for Computing Machinery.\nIan J. Goodfellow, Yoshua Bengio, and Aaron Courville.\n2016. Deep Learning. MIT Press, Cambridge, MA,\nUSA. http://www.deeplearningbook.org.\nDemi Guo, Yoon Kim, and Alexander Rush. 2020.\nSequence-level mixed sample data augmentation. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 5547–5552, Online. Association for Computa-\ntional Linguistics.\nThomas Hartvigsen, Saadia Gabriel, Hamid Palangi,\nMaarten Sap, Dipankar Ray, and Ece Kamar. 2022.\nToxiGen: A large-scale machine-generated dataset\nfor adversarial and implicit hate speech detection.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3309–3326, Dublin, Ireland.\nAssociation for Computational Linguistics.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network.\nYutai Hou, Yijia Liu, Wanxiang Che, and Ting Liu.\n2018. Sequence-to-sequence data augmentation for\ndialogue language understanding. In Proceedings\nof the 27th International Conference on Computa-\ntional Linguistics, pages 1234–1245, Santa Fe, New\nMexico, USA. Association for Computational Lin-\nguistics.\nAshish Kapoor, Bongshin Lee, Desney Tan, and Eric\nHorvitz. 2010. Interactive optimization for steer-\ning machine classification. In Proceedings of the\nSIGCHI Conference on Human Factors in Comput-\ning Systems, CHI ’10, page 1343–1352, New York,\nNY , USA. Association for Computing Machinery.\nJan-Christoph Klie, Bonnie Webber, and Iryna\nGurevych. 2023. Annotation Error Detection: An-\nalyzing the Past and Present for a More Coherent\nFuture. Computational Linguistics, 49(1):157–198.\nVarun Kumar, Ashutosh Choudhary, and Eunah Cho.\n2020. Data augmentation using pre-trained trans-\nformer models. In Proceedings of the 2nd Workshop\non Life-long Learning for Spoken Language Systems,\npages 18–26, Suzhou, China. Association for Com-\nputational Linguistics.\nShibamouli Lahiri. 2015. Squinky! a corpus of\nsentence-level formality, informativeness, and im-\nplicature.\nZachary Levonian, Chia-Jung Lee, Vanessa Murdock,\nand F. Maxwell Harper. 2022. Trade-offs in sampling\nand search for early-stage interactive text classifica-\ntion. In 27th International Conference on Intelligent\nUser Interfaces, IUI ’22, page 566–583, New York,\nNY , USA. Association for Computing Machinery.\nRuibo Liu, Guangxuan Xu, Chenyan Jia, Weicheng\nMa, Lili Wang, and Soroush V osoughi. 2020. Data\nboost: Text data augmentation through reinforcement\nlearning guided conditional generation. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n9031–9041, Online. Association for Computational\nLinguistics.\nXingkun Liu, Arash Eshghi, Pawel Swietojanski, and\nVerena Rieser. 2021. Benchmarking Natural Lan-\nguage Understanding Services for Building Conver-\nsational Agents, pages 165–183. Springer Singapore,\nSingapore.\n584\nNathan Ng, Kyunghyun Cho, and Marzyeh Ghassemi.\n2020. SSMBA: Self-supervised manifold based data\naugmentation for improving out-of-domain robust-\nness. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1268–1283, Online. Association for\nComputational Linguistics.\nBo Pang and Lillian Lee. 2004. A sentimental educa-\ntion: Sentiment analysis using subjectivity summa-\nrization based on minimum cuts. In Proceedings\nof the 42nd Annual Meeting of the Association for\nComputational Linguistics (ACL-04), pages 271–278,\nBarcelona, Spain.\nEthan Perez, Saffron Huang, Francis Song, Trevor Cai,\nRoman Ring, John Aslanides, Amelia Glaese, Nat\nMcAleese, and Geoffrey Irving. 2022. Red teaming\nlanguage models with language models.\nMary Phuong and Christoph Lampert. 2019. Towards\nunderstanding knowledge distillation. In Proceed-\nings of the 36th International Conference on Ma-\nchine Learning, volume 97 of Proceedings of Ma-\nchine Learning Research, pages 5142–5151. PMLR.\nSamuel Rhys Cox, Yunlong Wang, Ashraf Abdul, Chris-\ntian von der Weth, and Brian Y . Lim. 2021. Directed\ndiversity: Leveraging language embedding distances\nfor collective creativity in crowd ideation. In Pro-\nceedings of the 2021 CHI Conference on Human\nFactors in Computing Systems, CHI ’21, New York,\nNY , USA. Association for Computing Machinery.\nMarco Tulio Ribeiro and Scott Lundberg. 2022. Adap-\ntive testing and debugging of NLP models. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 3253–3267, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,\nand Sameer Singh. 2020. Beyond accuracy: Be-\nhavioral testing of NLP models with CheckList. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4902–\n4912, Online. Association for Computational Lin-\nguistics.\nGaurav Sahu, Pau Rodriguez, Issam Laradji, Parmida\nAtighehchian, David Vazquez, and Dzmitry Bah-\ndanau. 2022. Data augmentation for intent classi-\nfication with off-the-shelf large language models. In\nProceedings of the 4th Workshop on NLP for Conver-\nsational AI, pages 47–57, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nElvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang,\nJunlin Wu, and Yi-Shin Chen. 2018. CARER: Con-\ntextualized affect representations for emotion recog-\nnition. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3687–3697, Brussels, Belgium. Association\nfor Computational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nJina Suh, Soroush Ghorashi, Gonzalo Ramos, Nan-Chen\nChen, Steven Drucker, Johan Verwey, and Patrice\nSimard. 2019. Anchorviz: Facilitating semantic data\nexploration and concept discovery for interactive ma-\nchine learning. ACM Trans. Interact. Intell. Syst. ,\n10(1).\nLichao Sun, Congying Xia, Wenpeng Yin, Tingting\nLiang, Philip Yu, and Lifang He. 2020. Mixup-\ntransformer: Dynamic data augmentation for NLP\ntasks. In Proceedings of the 28th International Con-\nference on Computational Linguistics, pages 3436–\n3440, Barcelona, Spain (Online). International Com-\nmittee on Computational Linguistics.\nJustin Talbot, Bongshin Lee, Ashish Kapoor, and\nDesney S. Tan. 2009. Ensemblematrix: Interactive vi-\nsualization to support machine learning with multiple\nclassifiers. In Proceedings of the SIGCHI Conference\non Human Factors in Computing Systems, CHI ’09,\npage 1283–1292, New York, NY , USA. Association\nfor Computing Machinery.\nTongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and\nAlexei A Efros. 2018. Dataset distillation. arXiv\npreprint arXiv:1811.10959.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nJason Wei and Kai Zou. 2019. EDA: Easy data augmen-\ntation techniques for boosting performance on text\nclassification tasks. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 6382–6388, Hong Kong, China. As-\nsociation for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nTongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, and\nDaniel Weld. 2019. Errudite: Scalable, reproducible,\n585\nand testable error analysis. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 747–763, Florence, Italy.\nAssociation for Computational Linguistics.\nCongying Xia, Chenwei Zhang, Hoang Nguyen, Jiawei\nZhang, and Philip Yu. 2020. Cg-bert: Conditional\ntext generation with bert for generalized few-shot\nintent detection.\nKang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo\nLee, and Woomyoung Park. 2021. GPT3Mix: Lever-\naging large-scale language models for text augmen-\ntation. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021, pages 2225–2239,\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nKang Min Yoo, Youhyun Shin, and Sang-goo Lee. 2019.\nData augmentation for spoken language understand-\ning via joint variational generation. In Proceedings of\nthe Thirty-Third AAAI Conference on Artificial Intelli-\ngence and Thirty-First Innovative Applications of Ar-\ntificial Intelligence Conference and Ninth AAAI Sym-\nposium on Educational Advances in Artificial Intelli-\ngence, AAAI’19/IAAI’19/EAAI’19. AAAI Press.\nAnn Yuan, Daphne Ippolito, Vitaly Nikolaev, Chris\nCallison-Burch, Andy Coenen, and Sebastian\nGehrmann. 2021. Synthbio: A case study in faster\ncuration of text datasets. In Thirty-fifth Conference\non Neural Information Processing Systems Datasets\nand Benchmarks Track (Round 2).\nJun Yuan, Jesse Vig, and Nazneen Rajani. 2022. Isea:\nAn interactive pipeline for semantic error analysis\nof nlp models. In 27th International Conference on\nIntelligent User Interfaces, IUI ’22, page 878–888,\nNew York, NY , USA. Association for Computing\nMachinery.\nHongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and\nDavid Lopez-Paz. 2018. mixup: Beyond empirical\nrisk minimization. In International Conference on\nLearning Representations.\nLe Zhang, Zichao Yang, and Diyi Yang. 2022. TreeMix:\nCompositional constituency-based data augmentation\nfor natural language understanding. In Proceedings\nof the 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5243–5258,\nSeattle, United States. Association for Computational\nLinguistics.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In Proceedings of the 28th International\nConference on Neural Information Processing Sys-\ntems - Volume 1, NIPS’15, page 649–657, Cambridge,\nMA, USA. MIT Press.\nJing Zhou, Yanan Zheng, Jie Tang, Li Jian, and Zhilin\nYang. 2022. FlipDA: Effective and robust data aug-\nmentation for few-shot learning. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 8646–8665, Dublin, Ireland. Association for\nComputational Linguistics.\nA Equation for Temperature Sampling\nMathematically, with the temperature T and origi-\nnal probability of token, pi, the temperature sam-\npled probability of token i, fT (p)i, would be de-\nnoted as below:\nfT (p)i = p1/T\ni\nΣjp1/T\nj\n(2)\nB Experiment 1 Details\nB.1 Prompts Used in LLM Generation\nFor each task, we used prompt A with text types\nand labels as in Table 3. For example, for CB, a\nprompt can look like the below with examples:\nWrite a news headline to cover all following elements\nElements: valid news\nNews headline: \"Zach Johnson Wins Sony Open\"\n- - - - -\nWrite a news headline to cover all following elements\nElements: clickbait\nNews headline: \"10 Of The Biggest Lies We Were\nTold In 2015\"\n- - - - -\nWrite a news headline to cover all following elements\nElements: clickbait\nNews headline:\"\n(B)\nB.2 Sampling Oracle Dataset\nFor the oracle dataset, if there are more than 5600\ndata points in the original dataset (CB, CARER,\nHATE, COLA, HWU64, SUBJ), we subsampled\n5600 training data points. For SST2, we used all\n6922 instances from the original dataset. Note that\nthese numbers are the same as the number of gen-\nerated data instances. For FO, we used the original\ntraining dataset as is (with 3622 data instances),\nas there are fewer than 5600 instances. For test\ndatasets, from the same original dataset exclud-\ning instances used for the oracle dataset, we sam-\npled 2400 data points for CB, CARER, HATE, and\nHWU64. For FO, COLA, SUBJ, and SST-2, we\nused the original test datasets as there were fewer\nthan 2400 instances.\nC Results of the Experiment 1 on\nIndividual Dataset\nHere, we introduce the result of the first experiment\nfor individual tasks (Figure 5).\n586\n0.4\n0.6\n0.8\n1.0a) CARER\nModel Accuracy\n0.4\n0.6\n0.8\n1.0\nLabel Accuracy\n0.0\n0.1\n0.2\nDiversity\n0.8\n0.9\n1.0\nSimilarity\n0.4\n0.6\n0.8\n1.0b) CB\n0.4\n0.6\n0.8\n1.0\n0.0\n0.1\n0.2\n0.8\n0.9\n1.0\n0.4\n0.6\n0.8\n1.0c) COLA 0.4\n0.6\n0.8\n1.0\n0.0\n0.1\n0.2\n0.8\n0.9\n1.0\n0.4\n0.6\n0.8\n1.0d) FO\n0.4\n0.6\n0.8\n1.0\n0.0\n0.1\n0.2\n0.8\n0.9\n1.0\n0.4\n0.6\n0.8\n1.0e) HWU64 0.4\n0.6\n0.8\n1.0\n0.0\n0.1\n0.2\n0.8\n0.9\n1.0\n0.4\n0.6\n0.8\n1.0f) PubMed 0.4\n0.6\n0.8\n1.0\n0.0\n0.1\n0.2\n0.8\n0.9\n1.0\n0.4\n0.6\n0.8\n1.0g) SST2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.1\n0.2\n0.8\n0.9\n1.0\n0.4\n0.6\n0.8\n1.0h) SUBJ\n0.4\n0.6\n0.8\n1.0\n0.0\n0.1\n0.2\n0.8\n0.9\n1.0\nOracle\nGPT Zero\nGPT Few\nBase Similarity\nT emp=0.3, Logit Sup=X\nT emp=0.7, Logit Sup=X\nT emp=0.9, Logit Sup=X\nT emp=1.3, Logit Sup=X\nT emp=0.3, Logit Sup=O\nT emp=0.7, Logit Sup=O\nT emp=0.9, Logit Sup=O\nT emp=1.3, Logit Sup=O\nExample=X\nExample=O\nFigure 5: Impact of logit suppression and high temperatures on model accuracy, label accuracy, diversity, and\nsimilarity to the oracle dataset, for each task.\n587\nTask Text type Label →Label in prompts\nCARER emotional tweet joy →expressing joy, anger →expressing anger, fear →expressing fear,\nsadness →expressing sadness, love →expressing love, surprise →expressing surprise\nCB news headline non-clickbait →valid news, clickbait →clickbait\nCOLA sentence grammatically acceptable →grammatically correct sentence,\ngrammatically unacceptable →grammatically incorrect sentence\nFO sentence informal →informal, formal →formal\nHWU64 human utterance to\na chatbot\nnews →news, weather →weather, play →play, datetime →datetime, iot →iot,\ncooking →cooking, recommendation →recommendation, calendar →calendar,\nmusic →music, takeaway →takeaway, lists →list, transport →transport, qa →qa,\nsocial →social, general →general, alarm →alarm, email →email, audio →audio\nPubMed sentence from a\nmedical paper\nobjective → sentence about objective, methods → sentence about methods, results →\nsentence about results, conclusions →sentence about conclusions,\nbackground →sentence about background\nSST-2 movie review positive →positive sentiment, negative →negative sentiment\nSUBJ sentence from a\nmovie review\nobjective →objective statement, subjective →subjective statement\nTable 3: Text types and labels used in prompts.\nThe benefit of logit suppression for each task\ndepends on the combination of label accuracy, di-\nversity, and similarity. Tasks that have high base\nlabel accuracy tend to improve model accuracy\nmore with logit suppressions. For example, for CB\nand SST-2, those conditions with logit suppressions\nwere clear winners in model accuracy over other\ncombinations of approaches. For other tasks, where\noverall label accuracy tends to be lower, logit sup-\npression did not have large benefits. COLA was the\nextreme case where the label accuracy was about\n50% in binary classification, indicating that the per-\nformance of the LLM in generating label-accurate\ninstances was not better than random chance. In\nthis case, logit suppression resulted in almost no\nincrease in the model accuracy. Even in this case,\nlogit suppression could increase the diversity of the\ngenerated text. With PubMed, we could observe an\nexception of label accuracy increasing with logit\nsuppression when example seeding and high tem-\nperature (1.3) are not used (compare light and dark-\ncolored unhatched bars in PubMed’s Label Accu-\nracy from Figure 5, except for red bars). It was be-\ncause GPT-3 generates many similar errors without\nlogit suppression and seeding examples. Specifi-\ncally, without logit suppression, when prompted to\nwrite about the background sentence in a medical\npaper, GPT-3 generated many sentences starting\nwith “The purpose of this study was,” which is\nmore about the objective.\nFor temperature also, specific patterns on how\nit affected label accuracy, diversity, and similarity\ndiffer between tasks. In PubMed, without logit\nsuppression and example seeding, label accuracy\neven increased with higher temperatures, which\nwas against the general pattern. In this case, similar\nto what we found with logit suppression, the lack of\ndiversification approaches led to the generation of\nnarrowly populated error instances. CARER was\nanother case with the reversed trend: without logit\nsuppression and seeding examples, the mean diver-\nsity was higher with a temperature of 0.7 than with\na temperature of 1.3. It was because, with the high\ntemperature of 1.3, many sentences started with\n“I’m so,” (on average 3012 occurrences) which was\nless the case for the lower temperatures of 0.7 and\n0.9 (on average 841.5 occurrences). In CARER,\nwhen example seeding and logit suppression are\nnot used, label accuracy was also higher with the\ntemperature of 1.3 than with lower temperatures,\nalthough the means were within 95% confidence\nintervals. In this case, with lower temperatures of\n0.7 and 0.9, more instances started with “No matter\nwhat,” which continues with advice on what to do\nin emotional situations. For such cases, no label is\napplicable since they are not the self-expression of\nemotions (on average, 32 occurrences with a tem-\nperature of 1.3 and 682.7 occurrences with temper-\natures of 0.7 or 0.9). Note that these are examples\nof out-of-scope instances. Summarizing results of\nlogit suppression and temperature sampling, these\napproaches increased diversity while hurting the\nlabel accuracy, but specific patterns could vary be-\ntween tasks.\nThe utility of example seeding in label accuracy\nand model accuracy could also vary between tasks.\nFor example, in the extreme case of COLA, ex-\namples did not increase label accuracy and model\naccuracy. How seeding examples impact the gen-\neration of data similar to the oracle dataset also\n588\nTask Example Reason for filtering\nCARER No matter what life throws at you, always\nremember to find joy in the little things.\n#HappyThoughts\nNot a self-expression of emotion\nCB Valid News Not a news headline\nSST-2 Jurassic World Fallen Kingdom Only movie title\nSUBJ For what it’s worth, Incomplete sentence and unable to decide subjectivity\nTable 4: Examples of OOS instances.\n0 90 180 270 All\n0.4\n0.6\n0.8a) CARER\nModel Accuracy\n0 90 180 270 All\n0.6\n0.8\n1.0\nLabel Accuracy\n0 90 180 270 All\n0.6\n0.7\n0.8\n0.9\n1.0b) CB\nModel Accuracy\n0 90 180 270 All\n0.90\n0.95\n1.00\nLabel Accuracy\n0 90 180 270 All\n0.6\n0.7\n0.8c) COLA\n0 90 180 270 All\n0.6\n0.8\n1.0\n0 90 180 270 All\n0.6\n0.7\n0.8\n0.9d) FO\n0 90 180 270 All\n0.6\n0.7\n0.8\n0.9\n1.0\n0 90 180 270 All\n0.6\n0.8e) HWU64\n0 90 180 270 All\n0.6\n0.7\n0.8\n0.9\n1.0\n0 90 180 270 All\n0.4\n0.5\n0.6\n0.7f) PubMed\n0 90 180 270 All\n0.6\n0.7\n0.8\n0.9\n1.0\n0 90 180 270 All\n# of instances inspected\n0.8\n0.9g) SST2\n0 90 180 270 All\n# of instances inspected\n0.90\n0.95\n1.00\n0 90 180 270 All\n# of instances inspected\n0.4\n0.6\n0.8h) SUBJ\n0 90 180 270 All\n# of instances inspected\n0.00\n0.25\n0.50\n0.75\n1.00\nT emp=0.3\nT emp=0.7\nT emp=0.9\nT emp=1.3\nLogit Sup=X\nLogit Sup=O\nOracle\nGPT Few\nFigure 6: Impact of label replacement on model accuracy, label accuracy, for each task, on all temperature values.\nFigure 7: Impact of label replacement on model ac-\ncuracy, label accuracy, for all tasks aggregated, on all\ntemperature values.\ndepends on the task.\nFor CARER, HWU64, and PubMed in Figure 5,\nthere were cases where the model accuracy was\nhigher than the accuracy of GPT-3’s few-shot learn-\ning. Other tasks showed lower accuracy than GPT-\n3’s few-shot learning accuracy, indicating that GPT-\n3 few-shot classification can be a better alternative\nthan training a model with generated data if the\nmodel builder has a budget to continuously access\nGPT-3 and is willing to hand over data through\nAPI. In Section 6, we show that human interven-\ntions can be a way to make the data generation\napproach applicable in more tasks by increasing\nthe model accuracy higher than that of few-shot\nclassifications from GPT-3.\n589\nFigure 8: The ratio of instances filtered with OOSF, and its impact on model accuracy, label accuracy, diversity, and\nsimilarity, for all tasks aggregated, on all temperature values. As we examined the effect of OOSF with LR, for\nmodel accuracy and label accuracy, numbers left to +OOS indicate how many instances are inspected with LR.\nD Experiment 2 Details\nD.1 Examples of OOS instances.\nWe present examples of OOS instances in Table 4.\nE Results of the Experiment 2 on Varying\nTasks\nWe present the results of experiment 2 for individ-\nual tasks. Note that we also show results for all\ntemperature values (0.3, 0.7, 0.9, and 1.3).\nE.1 Label Replacement\nFigure 6 and 7 shows the LR result for individ-\nual tasks and whole tasks aggregated, respectively,\nwith all temperatures. First, there were cases where\nlogit suppression provided additional benefit upon\nhigh temperature only when LR was applied (com-\nparing thick and thin red lines in Model Accuracy\nof CARER, HWU64, and PubMed in Figure 6).\nSecond, for tasks that already have high accuracy\nwithout LR (CB and SST-2), LR either resulted in\nvery small model accuracy increases or even hurted\nthe accuracy. For example, in SST-2, the label ac-\ncuracy was already high without LR, and doing LR\nwith proxy models could even decrease the label\naccuracy and model accuracy. Third, without diver-\nsification approaches, there were also cases where\nLR did not increase model accuracy much while la-\nbel accuracy was greatly increased (thin blue lines\nin Model Accuracy of CARER, CB, FO, PubMed,\nSST2, SUBJ in Figure 6). It may show that fixing\nlabels is more beneficial when there is enough di-\nversity in the generated dataset. Fourth, CB, FO,\nand SUBJ were cases where models trained with\ngenerated data could outperform GPT-3’s few-shot\nclassification only with label replacement (some\ncolored lines go over gray dashed lines with LR in\nModel Accuracy of CB, FO, and SUBJ in Figure 6).\nAmong them, with FO, inspecting partial instances\ncould also turn the model accuracy higher than\nthat of GPT-3 few-shot classification. As expected,\nno approaches outperform oracle models as those\nmodels are used for LR. Fifth, for tasks with many\nclasses (CARER, HWU64, and PubMed), when us-\ning LR with proxy models, the performance tends\nto increase not much dramatically as the number\nof annotated instances increases (Model Accuracy\nof CARER, HWU64, and PubMed in Figure 6).\nHigher model accuracy leaps occurred when all\ninstances were inspected. It may indicate the diffi-\nculty of training accurate proxy models with many\nclasses to consider.\nE.2 Out-of-Scope Filtering\nFigure 8 and 9 shows the OOSF results with all\ntemperatures, for the aggregation of all tasks and\nindividual tasks, respectively. As mentioned in the\nmain text, it was difficult to find a general pattern of\nhow OOSF impacts the model accuracy. Consistent\npatterns were that OOSF tends to increase or main-\ntain label accuracy and similarity while decreasing\nor maintaining diversity.\nF Results on Prompt C\nOn two tasks (FO, HWU64), we conducted the\nexperiment with another instructional prompt:\nShow me a text type that has the following charac-\nteristics\nCharacteristics: label\ntext type: \"Generated text\"\n(C)\nWe measured model accuracy, label accuracy,\ndiversity, and similarity of generated datasets and\nalso investigated how label replacement impacts la-\nbel accuracy and model accuracy. The experiment\nsetting was the same as the main experiment we\nconducted, except the prompt used. The trend in\nthe results (Figure 10) was similar to that of the\nprompt A.\n590\n0.00\n0.25\n0.50\n0.75\n1.00a) CARER\nRatio of Unfiltered\n0 +OOS 180 +OOS All +OOS\n0.4\n0.6\n0.8\nUnbalanced Model Accuracy\n0 +OOS 180 +OOS All +OOS\n0.4\n0.6\n0.8\nBalanced Model Accuracy\n0 +OOS 180 +OOS All +OOS\n0.6\n0.8\n1.0\nLabel Accuracy\n+OOS\n0.100\n0.125\n0.150\n0.175\nDiversity\n+OOS\n0.76\n0.78\n0.80\n0.82\n0.84\nSimilarity\n0.00\n0.25\n0.50\n0.75\n1.00b) CB\n0 +OOS 180 +OOS All +OOS\n0.7\n0.8\n0.9\n0 +OOS 180 +OOS All +OOS\n0.7\n0.8\n0.9\n0 +OOS 180 +OOS All +OOS\n0.90\n0.95\n1.00\n+OOS\n0.15\n0.20\n+OOS\n0.75\n0.76\n0.77\n0.78\n0.79\n0.0\n0.5\n1.0c) SST2\n0 +OOS 180 +OOS All +OOS\n0.80\n0.85\n0.90\n0.95\n0 +OOS 180 +OOS All +OOS\n0.75\n0.80\n0.85\n0.90\n0.95\n0 +OOS 180 +OOS All +OOS\n0.90\n0.95\n1.00\n+OOS\n0.05\n0.10\n0.15\n+OOS\n0.76\n0.78\n0.80\n0.82\n0.84\n0.00\n0.25\n0.50\n0.75\n1.00d) SUBJ\n0 +OOS 180 +OOS All +OOS\n0.6\n0.7\n0.8\n0.9\n0 +OOS 180 +OOS All +OOS\n0.6\n0.7\n0.8\n0.9\n0 +OOS 180 +OOS All +OOS\n0.6\n0.8\n1.0\n+OOS\n0.10\n0.15\n0.20\n+OOS\n0.76\n0.78\n0.80\nBase Similarity\nT emp=0.3\nT emp=0.7\nT emp=0.9\nT emp=1.3\nLogit Sup=X\nLogit Sup=O\nOracle\nGPT Few\nT emp=0.3, Logit Sup=X\nT emp=0.7, Logit Sup=X\nT emp=0.9, Logit Sup=X\nT emp=1.3, Logit Sup=X\nT emp=0.3, Logit Sup=O\nT emp=0.7, Logit Sup=O\nT emp=0.9, Logit Sup=O\nT emp=1.3, Logit Sup=O\nFigure 9: The ratio of instances filtered with OOSF, and its impact on model accuracy, label accuracy, diversity,\nand similarity, for each task, on all temperature values. As we examined the effect of OOSF with LR, for model\naccuracy and label accuracy, numbers left to +OOS indicate how many instances are inspected with LR.\n0 90 180 270 All\n# of Instances Inspected\nwith label replacement\n0.75\n0.80\n0.85\n0.90\n0.95\nModel Accuracy\nGPT3 Few shot\nOracle\n0 90 180 270 All\n# of Instances Inspected\nwith label replacement\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nLabel Accuracy\nLow T emp (0.3)\nLogit Sup+Low T emp (0.3)\nHigh T emp (1.3)\nLogit Sup+High T emp (1.3)\n0.0\n0.1\n0.2\n0.3\n0.4a) FO\nDiversity\nOracle\nLow T emp (0.3)\nHigh T emp (1.3)\nLogit Sup+Low T emp (0.3)\nLogit Sup+High T emp (1.3)\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n1.05\nSimilarity\nBase Similarity\n0 90 180 270 All\n# of Instances Inspected\nwith label replacement\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nModel Accuracy\n0 90 180 270 All\n# of Instances Inspected\nwith label replacement\n0.6\n0.7\n0.8\n0.9\n1.0\nLabel Accuracy\n0.0\n0.1\n0.2\n0.3\n0.4b) HWU64\nDiversity\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n1.05\nSimilarity\nFigure 10: Result on prompt C.\n591\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 8. Limitations\n□\u0013 A2. Did you discuss any potential risks of your work?\nSection 9. Ethics Statement\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract and Section 1. Introduction\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNot applicable. Left blank.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNot applicable. Left blank.\nC □\u0013 Did you run computational experiments?\nLeft blank.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 4 and Appendix A\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n592\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 4 and Appendix A\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nFigure 1, 3 and 4 and Table 2\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 4 and Appendix A\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nSection 6. It was a human oracle study where one of the authors served the role of the oracle.\n□\u0017 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nIt was an oracle study by one of the authors on ﬁltering out-of-scope instances, and we followed the\ndeﬁnition we provided in Section 5.\n□\u0017 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nIt was an oracle study by one of the authors on ﬁltering out-of-scope instances.\n□\u0017 D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nIt was an oracle study by one of the authors on ﬁltering out-of-scope instances.\n□\u0017 D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nIt was an oracle study by one of the authors on ﬁltering out-of-scope instances.\n□\u0017 D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nIt was an oracle study by one of the authors on ﬁltering out-of-scope instances.\n593",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7474642395973206
    },
    {
      "name": "Oracle",
      "score": 0.6685513257980347
    },
    {
      "name": "Language model",
      "score": 0.5783144235610962
    },
    {
      "name": "Natural language generation",
      "score": 0.5137837529182434
    },
    {
      "name": "Text generation",
      "score": 0.5136508345603943
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46784600615501404
    },
    {
      "name": "Machine learning",
      "score": 0.4509236514568329
    },
    {
      "name": "Diversification (marketing strategy)",
      "score": 0.41201624274253845
    },
    {
      "name": "Data mining",
      "score": 0.37075722217559814
    },
    {
      "name": "Natural language",
      "score": 0.19506362080574036
    },
    {
      "name": "Software engineering",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    }
  ]
}