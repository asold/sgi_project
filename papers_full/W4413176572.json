{
  "title": "SoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks, and Mitigation",
  "url": "https://openalex.org/W4413176572",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5071591540",
      "name": "Yashothara Shanmugarasa",
      "affiliations": [
        "Commonwealth Scientific and Industrial Research Organisation",
        "Data61"
      ]
    },
    {
      "id": "https://openalex.org/A5075722123",
      "name": "Ming Ding",
      "affiliations": [
        "Commonwealth Scientific and Industrial Research Organisation",
        "Data61"
      ]
    },
    {
      "id": "https://openalex.org/A5063839073",
      "name": "M.A.P. Chamikara",
      "affiliations": [
        "Commonwealth Scientific and Industrial Research Organisation",
        "Data61"
      ]
    },
    {
      "id": "https://openalex.org/A5071175182",
      "name": "Thierry Rakotoarivelo",
      "affiliations": [
        "Commonwealth Scientific and Industrial Research Organisation",
        "Data61"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4405182239",
    "https://openalex.org/W3211753216",
    "https://openalex.org/W4319780902",
    "https://openalex.org/W4281806276",
    "https://openalex.org/W4206199121",
    "https://openalex.org/W3035261884",
    "https://openalex.org/W2781896769",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W4402683378",
    "https://openalex.org/W4389520705",
    "https://openalex.org/W4389523936",
    "https://openalex.org/W4367053831",
    "https://openalex.org/W3216260999",
    "https://openalex.org/W4408452289",
    "https://openalex.org/W4385573004",
    "https://openalex.org/W2792641098",
    "https://openalex.org/W4292793781",
    "https://openalex.org/W4393035858",
    "https://openalex.org/W2208157769",
    "https://openalex.org/W4399516266",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2535690855",
    "https://openalex.org/W4391107696",
    "https://openalex.org/W4385734162",
    "https://openalex.org/W4412130408",
    "https://openalex.org/W4391974622",
    "https://openalex.org/W4391407054",
    "https://openalex.org/W4392353733",
    "https://openalex.org/W4401857637",
    "https://openalex.org/W4404782219",
    "https://openalex.org/W4386211975",
    "https://openalex.org/W3027379683",
    "https://openalex.org/W4403486681",
    "https://openalex.org/W4389564820",
    "https://openalex.org/W4392449489",
    "https://openalex.org/W4385570888",
    "https://openalex.org/W4402671828",
    "https://openalex.org/W4390490761",
    "https://openalex.org/W4385572011",
    "https://openalex.org/W3096214574"
  ],
  "abstract": "Large language models (LLMs) are sophisticated artificial intelligence systems that enable machines to generate human-like text with remarkable precision. While LLMs offer significant technological progress, their development using vast amounts of user data scraped from the web and collected from extensive user interactions poses risks of sensitive information leakage. Most existing surveys focus on the privacy implications of the training data but tend to overlook privacy risks from user interactions and advanced LLM capabilities. This paper aims to fill that gap by providing a comprehensive analysis of privacy in LLMs, categorizing the challenges into four main areas: (i) privacy issues in LLM training data, (ii) privacy challenges associated with user prompts, (iii) privacy vulnerabilities in LLM-generated outputs, and (iv) privacy challenges involving LLM agents. We evaluate the effectiveness and limitations of existing mitigation mechanisms targeting these proposed privacy challenges and identify areas for further research.",
  "full_text": "arXiv:2506.12699v2  [cs.CR]  19 Jun 2025\nSoK: The Privacy Paradox of Large Language Models:\nAdvancements, Privacy Risks, and Mitigation\nYashothara Shanmugarasa\nyashothara.shanmugarasa@data61.csiro.au\nCSIRO‚Äôs Data61\nSydney, NSW, Australia\nMing Ding\nming.ding@data61.csiro.au\nCSIRO‚Äôs Data61\nSydney, NSW, Australia\nM.A.P. Chamikara\nchamikara.arachchige@data61.csiro.au\nCSIRO‚Äôs Data61\nMelbourne, Victoria, Australia\nThierry Rakotoarivelo\nthierry.rakotoarivelo@data61.csiro.au\nCSIRO‚Äôs Data61\nSydney, NSW, Australia\nABSTRACT\nLarge language models (LLMs) are sophisticated artificial intelli-\ngence systems that enable machines to generate human-like text\nwith remarkable precision. While LLMs offer significant technolog-\nical progress, their development using vast amounts of user data\nscraped from the web and collected from extensive user interactions\nposes risks of sensitive information leakage. Most existing surveys\nfocus on the privacy implications of the training data but tend to\noverlook privacy risks from user interactions and advanced LLM\ncapabilities. This paper aims to fill that gap by providing a compre-\nhensive analysis of privacy in LLMs, categorizing the challenges\ninto four main areas: (i) privacy issues in LLM training data, (ii)\nprivacy challenges associated with user prompts, (iii) privacy vul-\nnerabilities in LLM-generated outputs, and (iv) privacy challenges\ninvolving LLM agents. We evaluate the effectiveness and limita-\ntions of existing mitigation mechanisms targeting these proposed\nprivacy challenges and identify areas for further research.\nCCS CONCEPTS\n‚Ä¢ Security and privacy ‚ÜíPrivacy protections;\nKEYWORDS\nLarge Language Models, Privacy, Systematization of Knowledge\n1 INTRODUCTION\nArtificial intelligence (AI) stands as a groundbreaking frontier tech-\nnology, offering users the ability to simplify day-to-day tasks through\nautomation and intelligent querying. One of the rapidly advancing\nAI innovations is the Large Language Model (LLM), which has rev-\nolutionized natural language processing, empowering machines to\nproduce human-like text with exceptional precision [30].\nLLMs, trained on vast datasets and characterized by extensive\nparameters, master linguistic nuances to generate sensible, coher-\nent, and conversational responses to natural language queries [128].\nNotable platforms like OpenAI‚Äôs ChatGPT and Google‚Äôs Gemini,\nboasting millions of active users, have gained immense popularity.\nLeveraging prompt engineering [118], in-context learning capa-\nbilities [13], and retrieval-augmented generation [61], LLMs have\ndemonstrated adaptability to diverse contexts and can undertake\ntranslation, debugging, and storytelling tasks without requiring\ntraining or fine-tuning. While they perform impressively in var-\nious tasks, there are opportunities for improvement in address-\ning challenges such as hallucination, the need for domain-specific\nknowledge, and horizon cut-off.\n1.1 Privacy Concerns in Conventional\nAI/Machine Learning (ML) vs. LLMs\nDespite LLMs offering significant advancements and benefits, fair-\nness, reliability, bias, security, and privacy concerns are becoming\nincreasingly prominent [87]. This paper focuses on the multifaceted\nview of privacy in LLMs, which presents distinct challenges com-\npared to traditional AI privacy concerns related to models and data.\nPrivacy concerns in traditional AI focus on data privacy issues, such\nas unauthorized access to sensitive information or leakage through\nmodels via attacks like membership inference and model inversion\n[104]. In addition to these traditional risks, LLMs bring about new\ndimensions of privacy risks due to their advanced capabilities, such\nas a profound understanding of natural language context, human-\nlike text generation, contextual awareness in knowledge-rich fields,\nand a robust ability to follow instructions [126, 128].\nThe utilization of LLMs in commercial applications such as Ope-\nnAI‚Äôs chatGPT and Google‚Äôs Gemini raises strong privacy concerns,\nparticularly regarding the collection and use of personal data [20].\nLLMs are often trained on extensive datasets compiled from vari-\nous sources, including social media posts, websites, online articles,\nand books. There is a significant risk that this data, even when\nde-identified, can contain sensitive personal information [128]. De-\nspite the sharing of such data by individuals or organizations for\ndiverse agreed purposes, it is plausible that such data have been\nincorporated into the training data for building LLM without their\nexplicit consent, constituting a privacy breach. Besides, the interac-\ntive nature of LLMs poses additional privacy challenges. As users\nengage with LLMs through prompts, they may inadvertently dis-\nclose sensitive information. The sophisticated reasoning capabilities\nof LLMs can infer sensitive information about users even utilizing\nseemingly harmless user inputs [ 128]. Additionally, LLMs often\nintegrate functionalities that involve communication with external\nagents or third-party modules, extending the ecosystem to perform\nautomated tasks assigned by users. However, such integration can\nfurther complicate privacy concerns, as each interaction has the po-\ntential to generate and disseminate sensitive data across a network\nYashothara Shanmugarasa, Ming Ding, M.A.P. Chamikara, and Thierry Rakotoarivelo\nof interconnected systems, each with varying levels of security and\nadherence to privacy concerns.\n1.2 Motivation\nEvaluating privacy concerns in LLM systems is crucial for several\ncompelling reasons:\n‚Ä¢ Regulatory compliance . Regulations such as the General\nData Protection Regulation (GDPR) [89], the Health Insur-\nance Portability and Accountability Act (HIPAA) [ 1], or\nthe EU AI Act [90] impose laws on data protection and AI\nsafety, establish substantial penalties for non-compliance\nand empower individuals with rights to protect their sensi-\ntive information. Thus, organizations must adhere to such\nregulations when deploying LLM-based services.\n‚Ä¢ Inherent vulnerabilities and privacy risks in LLMs . Compared\nto traditional AI, LLMs introduce exacerbated vulnerabili-\nties, as they are often trained on vast amounts of potentially\nsensitive data. This requires comprehensive analyses and\nmethodologies to understand their strengths and weak-\nnesses, and mitigate their vulnerabilities.\n‚Ä¢ Advancement of privacy technologies for LLMs . Understand-\ning the privacy implications of LLMs facilitates evaluating\ncurrent technologies, such as differential privacy within the\ncontext of LLMs. Such insights are crucial for pinpointing\ngaps and driving innovation in developing robust mecha-\nnisms for privacy-preserving LLMs.\n‚Ä¢ Public perception and Trust . The widespread use of LLMs\nin consumer applications, particularly in critical sectors\nsuch as finance and healthcare, demands transparency and\nrobust privacy protection to maintain public trust, as pri-\nvacy violations within these domains can lead to significant\nfinancial and individual harms.\nHence, a thorough understanding of LLM systems‚Äô privacy chal-\nlenges and the development of practical solutions are essential for\ntheir ethical and safe deployment.\n1.3 The Overall Objectives\nThe objectives of this paper are three-fold. First, it comprehensively\nmaps the landscape of privacy issues associated with LLMs, ana-\nlyzing these multiple dimensions: the privacy implications of the\ntraining data used, the potential for sensitive information leakage\nthrough user interactions (i.e. prompts), and privacy vulnerabil-\nities specific to the deployment of LLMs (i.e., privacy breaches\nthrough LLM-generated output and privacy challenges involving\nLLM agents). Second, it provides a detailed overview of the cur-\nrent mitigation strategies and technologies employed to address\nthese identified privacy risks. Finally, it broadens the discussion by\nexamining how various privacy-preserving techniques can tackle\ndiverse privacy threats, while highlighting key research challenges\nand proposing avenues for future exploration.\nTo support these goals, we conducted a comprehensive review\nacross various sources, including the latest research papers, tuto-\nrials, dissertations, and magazines focused on LLMs and privacy.\nWe hope this work provides a holistic perspective for researchers,\npractitioners, and stakeholders engaged in the development and\ndeployment of LLM systems.\n1.4 State-of-the-art\nThe state-of-the-art studies on privacy in LLMs can be broadly\ncategorized into privacy and security challenges in LLM models and\ntraining data. Privacy in LLMs can be categorized further into two\nmain groups: privacy risks and defense mechanisms. The former\nfocuses on specific privacy and security challenges within the LLM\ndomain [14, 67, 70, 125], while the latter focuses on addressing\nthese challenges and the available defense mechanisms [ 20, 55,\n80, 120, 128]. While many surveys [ 80, 106, 128] predominantly\nfocus on the privacy implications of LLMs and their training data,\nthey often overlook the distinct privacy risks introduced by user\ninteractions and LLMs‚Äô advanced capabilities [14]. Consequently,\nwe could not identify any previous surveys or SoKs related to the\nprivacy challenges during LLM deployment and user interaction.\nThe significance of prompts cannot be overstated in LLM opera-\ntion, as they facilitate the customization of pre-trained LLMs for\ntask-specific purposes by appending a sequence of query texts[68,\n149]. However, these prompts can often contain user-sensitive in-\nformation, which can be inferred by LLM service providers, who\nleverage the LLMs‚Äô capabilities and access to personal data [108].\nRecent advancements in the privacy domain for LLMs offer a timely\nand highly relevant overview of this emerging research area [31].\nOur study delves into the latest approaches and techniques in this\ndomain, highlighting current research gaps and privacy challenges\nin LLMs across the categories: training data, prompts, LLM outputs,\nand LLM agents.\n1.5 Research Questions\nWe formulated the following research questions (RQs) to address\nthe aforementioned objectives of our study systematically.\n(1) RQ1: How do LLMs‚Äô advanced inferring capabilities and\ncontextual awareness impact user privacy across various\ndimensions, such as inference of sensitive information and\nmemorization of user inputs? This question aims to ex-\nplore the core capabilities of LLMs that may lead to privacy\nbreaches.\n(2) RQ2: What privacy challenges are associated with users\nactively interacting with LLMs? This question investigates\nhow users can inadvertently expose sensitive information\nwhen interacting with LLMs.\n(3) RQ3: What state-of-the-art solutions are currently employed\nto mitigate the privacy risks inherent in LLMs? This ques-\ntion investigates the effectiveness and limitations of the\nexisting privacy-preservation technologies for LLMs.\n(4) RQ4: What are the open challenges and possible future\ntrends to overcome these challenges? This question aims to\nhighlight ongoing challenges and predict future technolo-\ngies to preserve privacy in the context of LLMs.\n1.6 Scope\nThis paper focuses on the papers published after 2022, aligning with\nthe surge of LLMs. While we acknowledge the significant body of\nwork exploring training data privacy concerns, we deliberately\nchose not to delve extensively into them, as they have been com-\nprehensively covered in existing surveys [80, 106, 128]. Instead, our\nfocus lies on conducting a more in-depth analysis of other privacy\nSoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks, and Mitigation\nchallenges unique to LLMs that have not been sufficiently explored\nin the literature. Moreover, our study focuses exclusively on privacy\nissues within LLMs and does not address the broader security chal-\nlenges. The employed methodology to identify relevant literature\nand establish the categories is detailed in Appendix 6.1.\n2 OVERVIEW OF PRIVACY CHALLENGES IN\nLLM\nPrivacy in LLMs refers to protecting sensitive data that extends\nbeyond traditional Personally Identifiable Information (PII) to en-\ncompass confidential, proprietary, intellectual property, and con-\ntextual or behavioral data that could reveal personal attributes or\nidentities [52, 124]. This section outlines the key privacy challenges\nassociated with LLM systems. Our comprehensive analysis of the\nliterature reveals four main categories of privacy challenges: (i)\nprivacy issues related to LLM training data [ 36, 80], (ii) privacy\nchallenges in the interaction with LLM systems via user prompts in\nthe interaction with LLM systems [68, 146], (iii) privacy vulnerabili-\nties in LLM-generated outputs [86, 124], and (iv) privacy challenges\ninvolving LLM agents [39, 109]. Figure 1 shows the multi-faceted\nview of four identified privacy challenges in LLM systems.\n2.1 Privacy in LLM Training Data\nPrivacy concerns in LLMs primarily revolve around protecting the\ntraining data utilized in their development. This is a key issue in\ncommercial deployments of LLM systems, where they are trained\nusing personal data retrieved from various sources such as social\nmedia posts and websites [55]. This practice raises significant pri-\nvacy concerns as it often involves using individuals‚Äô data without\nconsent and contextual integrity principles when used outside their\nintended context [ 81]. Moreover, concerns arise regarding data\nstorage practices, with LLM service providers such as OpenAI stor-\ning personal information potentially conflicting with GDPR [74].\nBesides the privacy concern arising from the utilization of public\ndata and the data owner‚Äôs consent, LLM models are susceptible to\nsignificant privacy vulnerabilities, such as memorization and the\ninadvertent leakage of PII or confidential data [20].\n2.2 Privacy in Prompts\nThere are potential privacy leakages from user-provided input\nprompts during interactions with LLM systems [ 107]. The ever-\nincreasing inference capabilities of LLM systems and their wide-\nspread adoption across various domains have raised significant\nprivacy concerns through user prompts. User prompts are often\nexposed to the service providers, raising questions about whether\ncurrent LLM systems could breach users‚Äô privacy by inferring per-\nsonal attributes from the prompts. Despite data transmission and\nstorage encryption, LLM service providers retain the knowledge of\nthe actual data contents, undermining individuals‚Äô or entities‚Äô trust\nin these services.\nThe interactive utilization of LLM systems introduces a new\nset of privacy risks during inference time. LLM systems are fed\nwith diverse types of information from various sources in their\nprompts, potentially revealing more contextual data beyond the\ndirect sensitive data in the prompts. Recent findings in [107] have\ndemonstrated that modern LLM systems can be leveraged for highly\naccurate predictions of personal attributes, even from seemingly\ninnocuous data. Previously, human involvement was necessary\nfor inferring private attributes. However, these models are now\nswift and economical enough to automate such inferences at scale\n[108]. Simultaneously, users remain unaware of these concerns,\ninadvertently sharing texts containing information easily inferable\nby LLM systems. For evidence of privacy vulnerability via prompts,\nThe New York Times1 reported instances where personal informa-\ntion, including chatbot conversations and login credentials, was\nunintentionally exposed.\nGiven these challenges, we believe the research community is\nresponsible for fostering a paradigm shift in LLM-centered privacy\nresearch. New research should extend beyond examining privacy\nrisks associated only with models and data to explore privacy vul-\nnerabilities inherent in other aspects (e.g., user interactions) of\nLLMs. Prompts are one significant aspect as they directly reveal\nsubstantial information. These approaches should prioritize user-\nfriendliness, ensuring users can easily and quickly verify whether\nthey disclose private information in their prompts.\n2.3 Privacy in LLM-generated Outputs\nWithin the scope of privacy concerns regarding LLM-generated\noutputs, we delve into issues such as the retention, extraction, and\nretrieval of sensitive user data in outputs (i.e., memorizing sensitive\ninformation from user prompts) and the inadvertent inclusion of\nsensitive information in the LLM outputs.\nWe consider LLM-generated output privacy as a distinct aspect\nof LLM privacy due to the following reasons: 1) Users may enter\nsensitive information in prompts without expecting it to appear in\nthe output. However, LLMs may still include such information in\ntheir responses. This issue is prevalent in In-context Learning (ICL),\nwhere users‚Äô private data in prompts help adapt the model to spe-\ncific tasks with only black-box access. Due to the context-specific\nexamples in the prompts, the output can include labels of sensitive\ndata samples from the prompt data for ICL. 2) In commercialized\nLLM settings like the GPT store2, specialized LLM models are fine-\ntuned using private data through methods like ICL, fine-tuning,\nand Retrieval-Augmented Generation (RAG). These techniques inte-\ngrate user queries with demonstrations or relevant documents from\na knowledge base to enhance LLM-generated responses. However,\nthis customization poses a risk of exposing private information\nfrom a smaller user base to a broader audience. Although LLM\nproducts should ideally avoid generating harmful outputs such as\nsensitive information, legal, medical / health, financial advice, and\nmisinformation, they can be customized to produce such output. 3)\nEven when users employ techniques to protect sensitive informa-\ntion in input prompts, LLM-generated outputs remain vulnerable to\nexposure to malicious service providers, third-party tools, external\nparties, or hackers[129]. Service providers can potentially disclose\nsensitive information in output prompts by analyzing training data\nor accessing external sources, even with partially sensitive data\nfrom prompts. For example, using protection mechanisms at the\nuser end, users can restrict LLM systems from retaining sensitive\n1https://www.nytimes.com/2023/03/31/technology/chatgpt-italy-ban.html\n2https://openai.com/index/introducing-the-gpt-store/\nYashothara Shanmugarasa, Ming Ding, M.A.P. Chamikara, and Thierry Rakotoarivelo\nPrivacy in\nPrompts (Sec. 2.2)\nPrivacy in\nOutput (Sec. 2.3)\nUser\nLarge Language modelEmbeddings\nLayer 1\nTraining\nData\n...\nLayer n\nLLM generated\noutput\nTask 1\nTask n\n...\nTask Execution\nInteracting with\nthird party tools\nLLM Agents\nPrivacy in LLM\nAgents (Sec. 2.4)\nPrivacy in\nTraining data\n(Sec. 2.1)\nCollaborate with\nother agentsOutput to users\nDomain-\nspecific\nData\nTraining\nFine-tuning\nPrompting\nDate\nLocation\nContextual\nKnowledge Base\nMethod: RAG Enhancement\nAugmentation\nFigure 1: A Multi-Faceted View of the Four Identified Privacy Challenges in LLMs\ndata (e.g., passport identifier) in input prompts. However, LLM sys-\ntems may still inadvertently reveal sensitive information such as\nthe birth year or age in the output due to its processing [120].\n2.4 Privacy in LLM Agents\nRecent advances in LLM Systems have led to the development of\nagent-based solutions such as WebGPT [78], AutoGPT [94], and\nGPT Plugins (e.g., WebPilot [145]). As illustrated in Figure 2, in these\nsystems, a main LLM agent divides a user prompt into individual\ntasks and transmits them to secondary agents (which can also be\nLLM-based). These agents then leverage a range of powerful tools,\nincluding other agents and third-party applications (e.g., mobile\napps, web browsers, sensors, metaverse interfaces, code interpreters,\nand API plugins) to interact with external environments and carry\nout real-world tasks assigned by the main LLM agent.\nUserResponse\nPrompt\nMain LLMAgent\nMemory\nStorage\nKnowledgeLLM\nDecision making\nPlanning\nTask 1\nTask n\n...\nExecute\nExternal tools\nAgents\nCollaborate\nDigital world\nGenergic\nAPIs\nCommunicate \nUser Profile\nGeneralizing/ Transfer\nPhysical interaction with\nenvironment\nPhysical world\nVirtual world\nMetaverse interaction\nDigital twin\nGoal\nFigure 2: The Workflow of LLM Agents\nThe transition from text-based interactions to those involving\nsensitive data with agents operating in the real world raises signifi-\ncant privacy concerns [99]. For example, physical interaction-based\nLLM agents heighten privacy risks by collecting real-world data\nsuch as facial photos (cameras) and speech audio (microphones)\n[32]. A significant challenge in such an agent-based system is the\nrisk of unauthorized propagation of sensitive data across different\ncomponents of the system [67]. Indeed, agents autonomously inter-\nact with external tools, different data modalities, other AI models,\nand APIs, making tracking and controlling data flow difficult. Users\nmay provide private inputs through prompts, and when process-\ning their requests, an LLM agent may share their data with exter-\nnal tools or across interconnected systems without their explicit\nawareness. The agent‚Äôs ability to autonomously reason, execute\nmulti-step actions, and retrieve information from various sources\nfurther amplifies the privacy risks, as data may persist across in-\nteractions in ways not immediately apparent to users. LLM agents\npose risks of data persistence, where sensitive information from\npast interactions may resurface in future prompts, especially when\ncontextual memory is retained across sessions or users [67].\nHence, LLM agents extend beyond traditional LLM output pri-\nvacy concerns by persisting memory, unintended exposure across\nsessions and contexts, autonomously interacting with external tools,\nand operating in real-world physical interactions with environ-\nments [32]. Therefore, stricter and more diverse mitigation strate-\ngies are needed to address privacy concerns in LLM agents.\n3 PRIVACY CHALLENGES IN LLM AND\nMITIGATION\nThis section discusses privacy challenges under four broad cate-\ngories and reviews existing mitigation techniques in the literature.\n3.1 Privacy Issues in LLM Training Data and\nMitigation\nWe analyze privacy issues in LLM training data by distinguish-\ning between the causes, attack mechanisms, and consequences of\nprivacy vulnerabilities. Data memorization is one of the primary\ncauses of privacy risks, where models inadvertently retain and re-\nproduce sensitive information from training data. Privacy attacks\nact as mechanisms that adversaries use to exploit LLMs to extract\nsensitive information. These attacks can lead to consequences such\nas privacy leakage. For example, unauthorized access to personal\nor confidential data can compromise privacy. Figure 3 illustrates\nthese interrelated aspects of privacy risks in LLMs. Although prior\nSoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks, and Mitigation\nresearch works [20, 80, 106] extensively discuss privacy threats\nand mitigation, they have not examined this causal structure. This\nsection summarises key insights, emphasizing how data memoriza-\ntion contributes to privacy vulnerabilities and how various attack\ntechniques facilitate privacy leakages.\nData\nMemorization\nConsequences\nUnauthorized Access\nto Personal DataExtraction attacksInference Attacks\nGradient leakage\nattacks\nReconstruction\nattacks\nTraining data\nextraction attacks\nMembership\ninference attacks\nAttribute inference\nattacks\nUser inference\nattacks\nTraining on\nSensitive Datasets\nOverfitting\nInadequate\nRegularization\nCauses Attack Mechanisms\nFigure 3: The Privacy Challenges in LLM Training Data\n3.1.1 Potential Causes of Privacy Issues. Many privacy issues in\nLLMs arise due to data memorization, which can be exploited by\nprivacy attacks, ultimately resulting in privacy leakages [53]. Data\nmemorization is an intrinsic characteristic of many ML models, as\nthe training process involves retaining specific information from\ninput data to make accurate predictions [3]. However, while users\nexpect LLMs to generate novel content that is semantically similar\nto the training data, the verbatim memorization and reproduction\nof learned phrases can potentially expose sensitive data and breach\nprivacy [36, 80]. Hence, memorization is particularly problematic\nwhen LLMs are trained on personal information datasets.\nBeyond data memorization, privacy leakage can stem from train-\ning on sensitive datasets without proper sanitization [ 14]. Over-\nfitting and inadequate regularization (e.g., dropout, weight decay)\nfurther exacerbate the risk by making models more likely to retain\nand expose specific training data [54, 112].\nMitigation. A fundamental approach to mitigating privacy is-\nsues in LLM training data is to reduce data memorization, which is\nthe basis for many attacks. We first examine techniques designed\nto address the challenge of data memorization . One straightforward\napproach is to deduplicate training datasets, as data redundancy\ncan exacerbate model memorization. Even a 10-fold data duplica-\ntion can lead to a 1000-fold increase in memorization [51]. Another\nmethod is early detection of memorization during the training\nphase, enabling practitioners to take corrective actions such as\ndiscarding memorized points, reverting to a checkpoint, or halt-\ning the training process for adjustments [ 80]. The utilization of\nfiltering techniques such as the bloom filter method (which is a\nprobabilistic data structure used to detect and filter out memorized\nor sensitive data efficiently) [ 45] is another defense mechanism\nfor memorization, scanning the training datasets to check if the\nmodel‚Äôs next token forms an ùëõ-gram in the training set and select-\ning an alternate token by sampling from the model‚Äôs posterior if\nit does. However, this method does not guarantee privacy and can\nbe bypassed with plausible, minimally modified prompts, failing to\nprevent training data leakage entirely. Another promising approach\nfor mitigating memorization challenges is differential privacy (DP:\ndetailed in Table 1), which adds noise to data during training, pro-\nviding mathematical guarantees for privacy but reducing utility\n[150]. LLM editing offers yet another approach, in which neurons\ncorresponding to memorization and storage of specific training\ndata knowledge can be directly edited (i.e., by altering internal\nparameters) [15].\nA key consequence of data memorization is personal or confi-\ndential data leakage, which poses a significant privacy risk. While\ngeneral techniques such as deduplication [51, 60] can help mitigate\nmodel memorization and reduce the risk of personal data leakage,\nsome studies specifically target this issue by employing advanced\napproaches, such as fuzzy logic deduplication [13] to prevent the\nretention of sensitive information. Other methods to prevent pri-\nvacy leakage include data cleaning, PII scrubbing, and filtering with\nrestrictive terms of use [4, 124]. Data cleaning enhances privacy\nby correcting errors, implementing anonymization, and following\nsecure practices to protect sensitive information [85, 124]. PII scrub-\nbing filters [4, 23, 110] use Named Entity Recognition (NER) to tag\nand remove PII. However, these tools cannot guarantee complete\nremoval, and AI advancements can infer PII from non-PII data.\nTherefore, minimizing data retention and enforcing purging poli-\ncies are critical to reducing the risk of breaches and unauthorized\naccess. While most of these approaches are applied to smaller text\ncorpora, the use of extensive training data in LLMs presents challenges.\nMore empirical studies are needed to evaluate their effectiveness in\nsuch large-scale models with larger datasets.\nRecently, knowledge unlearning techniques [33, 130, 140] have\nbeen utilized in LLMs to force models to forget specific knowledge\nwithout requiring full retraining. These knowledge unlearning tech-\nniques involve randomly sampling data from the training corpus\nand performing gradient ascent, altering the direction during lan-\nguage modeling on target token sequences. However, the success of\nthis method depends heavily on specific target data and the domain\nof the data to be unlearned [106].\nAnother recent method for addressing PII leakage in LLMs is\nProPILE [56], a tool that allows data subjects to assess the inclu-\nsion and potential leakage of personal data in LLM systems during\ndeployment. ProPILE considers the linkability and structurability\nof data in inferring PII. Singh et al. [105] proposed a multi-faceted\nwhispered tuning approach by integrating PII redaction, DP, out-\nput filtering, and architectural improvements to enhance privacy\npreservation. Likewise, OpenAI utilizes filtering and fuzzy dedupli-\ncation techniques to remove PII from the corpora utilized for model\ntraining [13]. Despite these efforts, achieving complete prevention of\nprivacy leaks remains challenging. Hence, in-depth studies are neces-\nsary to design effective defense techniques for PII leakages on LLMs\nwhile maximizing user education and involvement.\n3.1.2 Potential Privacy Attacks. To exploit data memorization in\nLLM training data, various privacy attack mechanisms have been\ndeveloped, each employing different techniques and targeting spe-\ncific vulnerabilities. While LLMs utilize Deep Learning (DL) for\nmodel building, all traditional attacks on DL models may not di-\nrectly apply to LLMs due to limited access to model parameters\nand the prevalent use of application programming interfaces (APIs)\nin most LLMs. However, certain LLM service providers offer open-\nsource models (e.g., LLaMA, BERT, DeepSeek, Mistral 7B ), making\nDL-related attacks extended to the LLMs. Privacy attacks on LLM\ncan be broadly categorized into two classes: extraction attacks and\ninference attacks.\nExtraction Attacks. Extraction attacks involve adversaries at-\ntempting to extract sensitive information or insights from the LLM\nYashothara Shanmugarasa, Ming Ding, M.A.P. Chamikara, and Thierry Rakotoarivelo\nmodel by accessing model gradients, the training data, or adver-\nsarial prompting. LLMs may inadvertently capture and reproduce\nsensitive information in the training data, potentially raising pri-\nvacy concerns during the text generation process [ 80]. Various\ntypes of data extraction attacks include gradient leakage attacks\n[9, 22, 152], reconstruction attacks [ 8, 19, 69], and training data\nextraction attacks [14, 79, 127, 147]. Gradient leakage attacks ex-\nploit DL optimization algorithms, accessing gradients to leak sen-\nsitive data. However, this attack is challenging when the model\narchitecture is unknown (i.e., closed-source) [22]. Reconstruction\nattacks aim to recover private training data by analyzing model\nparameters, gradients, or generated output. Even without direct\naccess to LLMs, attackers can use shadow modeling with external\ndatasets and feedback from the target LLM [ 143]. Training data\nextraction attacks target sensitive information in training data by\nstrategically querying LLM systems via prompts to retrieve spe-\ncific examples, including personal information [14, 20]. Jailbreaking\n[117] and prompt injection [103] security attacks make LLMs more\nvulnerable to extraction attacks and introduce substantial privacy\nrisks with broad and potentially severe implications even without\ndirect model access. Jailbreaking attacks exploit both direct and\nindirect techniques to bypass the safety and alignment constraints\nof LLMs. These attacks often use manually or automatically crafted\nadversarial prompt templates designed to deceive the model into\ngenerating harmful content or revealing sensitive information, po-\ntentially including memorized data from its training corpus [70]. In\nterms of extracting private information, a multi-step jailbreaking\napproach has bypassed ChatGPT‚Äôs safety mechanisms, successfully\nextracting PII from ChatGPT and LLM-powered search engines\n[62]. Another study tested 3,700 templated jailbreak prompts, ana-\nlyzing their effectiveness across different LLMs [88]. Additionally,\nan automated framework, ReNeLLM, was proposed to generate\neffective jailbreak prompts using prompt rewriting and scenario\nnesting to infer private information [25].\nInference Attacks. Inference attacks aim to acquire knowledge\nor insights about a model or data characteristics by observing the\nmodel‚Äôs responses or behavior [128]. Common inference attacks\nin LLMs include membership inference attacks (MIA), attribute\ninference attacks, and user inference attacks [46, 75, 116].\nIn MIAs, an adversary attempts to predict whether a particular\ndata point (e.g., a particular sentence or document) is a member of\na target model‚Äôs training dataset [29] (Detailed in Table 1). MIA can\nbe conducted solely based on the text generated by LLM systems.\nWhile MIAs generally perform no better than random guessing\nfor LLM training data due to the vast datasets used [ 29], their\neffectiveness improves when small datasets are used for fine-tuning\nfor specific tasks, known as ‚ÄòUser Inference Attacks‚Äô [ 50]. These\nattacks are prevalent in fine-tuned LLMs using techniques such as\nfull fine-tuning, RAG, and ICL techniques [28, 37].\nUser inference attacks present a significant privacy risk forLLMs\nfine-tuned on a smaller group of user data , where an attacker tries to\ndetermine if a specific user participated in the fine-tuning process.\nMinimal user samples and black-box access to the fine-tuned model\nare sufficient for this attack to succeed [50]. This poses a privacy\nrisk if the fine-tuning task reveals sensitive information about users,\nsuch as a model fine-tuned exclusively on users with a rare disease.\nAttribute inference attacks (Detailed in Table 1) pose another\nprivacy risk for LLM systems, as adversaries can use these attacks to\nidentify users‚Äô attributes through prompts using publicly available\npartial data of users [ 107]. Robin et al. [ 107] demonstrated that\npersonal details can be inferred accurately from current LLMs.\nWhile prompt injection is often discussed in the literature as a\nbackdoor security attack, it can also be used for attribute inference.\nMitigation. Differential Privacy (DP) techniques have become\na popular approach in addressing such privacy attacks[66, 102, 105,\n137]. Besides DP, techniques to reduce memorization (discussed\nin Section 3.1.1) can also help prevent these privacy attacks. For\nexample, Jagielski et al. [47] discovered that using larger datasets\nduring fine-tuning or more extended training with non-sensitive\ndata could effectively mitigate privacy concerns by reducing the\nMIA. Furthermore, test-time defense and instruction processing are\nalso crucial for mitigating privacy attacks [128]. Test-time defenses\nfilter malicious inputs, detect abnormal queries, and post-process\nLLM outputs. Instruction pre-processing transforms user prompts\nto eliminate adversarial contexts or malicious intents [64, 96]. En-\nhancing LLM architectures can also improve safety by integrating\nLLMs with knowledge graphs [ 136] and cognitive architectures\n[97]. Furthermore, the LLM training process can employ robust\noptimization methods such as adversarial training [134] and robust\nfine-tuning [26] to prevent malicious text attacks.\nHowever, these techniques have limitations, such as challenges\nin effectively handling adversarial inputs, ensuring scalability in\nlarge models, and maintaining a balance between privacy and model\nperformance, particularly in dynamic, real-world scenarios.\nFederated learning (FL) may potentially solve privacy attacks\nin LLM systems by shifting processing from central servers to\nusers [95, 115]. However, its application to LLMs faces challenges\ndue to inadequate FL framework support, handling vast data and\ncomplex models, and optimizing communication and computational\nresources. Some studies [34, 131] have explored FL for LLMs, but its\nviability remains uncertain. Federated LLM fine-tuning may enable\nparameter-efficient fine-tuning with limited resources [58].\nFully Homomorphic Encryption (FHE) has been explored as a\nprivacy-preserving solution [92, 98, 142] for language models such\nas DistilBERT [101] (66M parameters) and BERT (110M parameters)\n[24]. While FHE has been applied to smaller models with limited\ndatasets [92, 98, 142], scaling it to larger models, such as GPT with\nmassive datasets, remains challenging due to the computational\ncomplexity. Outsourcing computation can also be securely con-\nducted for small language models (LM): for instance, an encrypted\nRNN text classifier was shown to predict on homomorphically\nencrypted input without accuracy loss [ 63], and such overhead\nremains manageable at this scale. With a compact model, full on-\ndevice inference becomes realistic, retaining user data locally and\nmitigating cloud-related vulnerabilities [71, 122, 132]. Conversely,\nLLMs with billions and trillions of parameters often demand cloud-\nbased infrastructures and massive datasets, introducing more com-\nplex privacy concerns.\nMitigation strategies for jailbreaking attacks are extensively stud-\nied in the literature and can be categorized into prompt-level and\nmodel-level approaches, depending on whether they modify the\nprotected LLM. Prompt-level defenses (e.g., input sanitization) [148]\nSoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks, and Mitigation\nfocus on filtering adversarial prompts using rule-based detection or\nclassifier-based approaches. Model-level defenses (e.g., adversarial\ntraining) [48] involve enhancing LLMs through safety training, re-\ninforcement learning with human feedback (RLHF), and adversarial\nfine-tuning to improve robustness against attacks. Constitutional\nAI frameworks [6] and automated moderation systems also help\nenforce ethical guidelines and restrict unintended model behaviors.\nTo protect private information from jailbreaking, prior work has\nexplored mostly prompt-level mitigation: defending with additional\nprompts and using a harmfulness classifier to filter malicious inputs\n[25, 88]. The study in [62] outlined potential approaches at both\nmodel-level defenses and prompt intention detection mechanisms\nto prevent the disclosure of private information.\nKey Takeaways\nLLM systems have shown an ability to memorize training\ndata, raising concerns about the unintentional disclosure of\nsensitive information and privacy breaches. Despite the ef-\nforts to address these issues with methods such as differen-\ntial privacy, knowledge unlearning, and test-time defenses,\nadversaries may still infer private data through adversar-\nial prompting. Researchers should focus on establishing\nresponsible practices that safeguard privacy while preserv-\ning the utility of LLMs trained on large-scale datasets.\n3.2 Privacy Challenges in the Interaction with\nLLM systems via Prompts and Mitigation\nThis section focuses on privacy challenges when users interact\nwith pre-built LLM systems via prompts. We identify three types\nof privacy challenges associated with prompts in the literature: the\ndirect leakage of sensitive data in prompts, the potential inference\nof sensitive information, and the leakage of contextual information\nfrom user devices and activity logs.\n3.2.1 The Direct Leakage of Sensitive Data in Prompts. Users might\ninadvertently reveal personal data through prompts when interact-\ning with the system, often due to a lack of awareness or reluctance\nto undertake additional efforts to minimize personal information\nin prompts. Even individuals with strong technical backgrounds,\noften in IT industries, reportedly leaked sensitive company data\nthrough ChatGPT prompts in a late 2022 incident3.\nThis challenge became more prominent with the integration of\nLLMs into interactive computing systems such as conversational\nagents, making them widely accessible for everyday tasks. Users\ntend to disclose more private information when engaging with\nLLM-based conversational agents due to the tools‚Äô high utility and\nhuman-like interactions [146]. Moreover, many users are uncon-\ncerned about revealing personal data in prompts, maybe under the\nmisconception that LLM companies no longer collect interaction\ndata, based on various published statements [120]. However, despite\nthe prevailing belief, often the company‚Äôs privacy policies indicate\nthat user prompts are periodically integrated into the training pro-\ncess by default [120, 146]. The extent of sensitive data revealed in\nuser prompts is evidenced in the study by Zhang et al. [ 146]. To\n3https://tinyurl.com/yny4mjdh\nidentify PII leakage in prompts, they analyzed real-world ChatGPT\nconversations. The study uncovered significant amounts of sensi-\ntive and personal data in LLM prompts, providing a comprehensive\noverview of user disclosure behaviors.\nProtecting personal data in prompts is imperative. Conversations\nbetween users and LLMs can become embedded in LLM parameters\nduring training, making them vulnerable to adversarial attacks.\nHence, data protection measures should be implemented at the user\nend to protect data from service providers and external parties.\nMitigation. A straightforward mitigation technique for poten-\ntial direct leakage of sensitive data is input validation and sanitiza-\ntion. Some approaches rely on LLM service providers to implement\nthese solutions, so their efficacy depends on the trustworthiness of\nthe providers or the LLM itself. Other methods acknowledge that\nprivacy vulnerabilities can arise from both LLM service providers\nand external parties, offering a safer solution than those relying\non the trustworthiness of the service providers. One sanitization\napproach involves using NER or predefined policies and rules to\nidentify and remove specific sensitive details in prompts. Research\nworks [17, 65, 68] and commercial products [41, 73] focus on prompt\nprivacy using this method.\nTo address the dilemma of potentially compromising prompt\nutility by removing all personal information from user prompts,\nEmojiCrypt [68] converts sensitive information in prompts into an\nencrypted format using emojis and mathematical operations. This\nmethod retains informative value for LLMs while making the data\nincomprehensible to humans. However, it relies on trusting the LLM\nfor encryption, mainly aiming to conceal information from other\nusers. Other constraints include a limited symbolic vocabulary and\nthe risk of introducing inaccuracies.\nAnother lightweight anonymization technique proposed in [17]\naims to protect prompt privacy through substitution or masking.\nThe approach involves two core techniques: hiding private enti-\nties in prompts using generative schemes and NER models and\nseeking private entities for de-anonymization. Both models are\nimplemented on the user‚Äôs system. After anonymizing the text with\nthe Hide model, the result is sent to the LLM, and the Seek model\nde-anonymizes the output. This approach commendably does not\ntrust the LLM.However, it relies on a small corpus or NER for identify-\ning sensitive data, which limits its ability to conceal other words and\nsentence structures. Additionally, replacing entity words in prompts\ncan challenge tasks that rely on precise semantics.\nAnother potential approach involves designing secure prompt\ntemplates and sensitive data redaction tools to minimize personal\ndata leakage [ 73]. This could include using structured data for\ninputs instead of free text. Variables can be safely inserted into\nthe prompt template without giving users direct control over the\nprompt structure, allowing for easy removal of sensitive data.\nAnother technique is a combination of small local LMs and re-\nmote LLMs. Hartmann et al. [43] proposed a privacy-preserving\nmechanism where a local small LM at the user‚Äôs end personalizes\noutputs without disclosing confidential information to the remote\nmodel. They explored three algorithms: (1) generating a high-level\ndescription of the query locally and using the remote LLM for few-\nshot examples, (2) creating a similar novel problem with new unla-\nbeled examples for the remote LLM, and (3) maintaining the query‚Äôs\nYashothara Shanmugarasa, Ming Ding, M.A.P. Chamikara, and Thierry Rakotoarivelo\nstructure while substituting private information with placehold-\ners. These methods may be domain-specific, rule, or NER-dependent,\nrequiring users to have sufficient knowledge to manage them.\nTextObfuscator [151] protects user privacy using text obfus-\ncation techniques. It learns private representations that obscure\noriginal words while retaining their functionality. This is achieved\nby identifying prototypes for each word and clustering functionally\nsimilar words around the same prototype. Random perturbations\nare then applied to these clusters, obscuring original words and\nmaintaining functionality. However, this approach requires signif-\nicant computational power on local machines, only protects word\nprivacy during inference.\nRecently, Ruoyan et al. [100] integrated FHE and provable se-\ncurity theory with parameter-efficient fine-tuning to propose a\nsecure inference scheme for LLMs, ensuring the protection of both\nuser-side inputs and server-side private parameters. Likewise, [139]\ndesigns efficient FHE counterparts for the core algorithmic building\nblocks of prompt ensembling. In [16, 21, 42], the authors explored\nprivacy-preserving inference using transformer models, incorporat-\ning FHE and secure multi-party computation to protect the input\ntext in user prompts. However, computing and communication effi-\nciency remain a challenge when handling multiple prompts.\n3.2.2 The Potential Inference of Sensitive Information. Aside from\ndirect leaks of sensitive data, the capabilities of LLMs enable even\nseemingly insignificant data to unveil sensitive information through\ninference [57]. Leveraging their advanced capabilities, LLMs can\ninfer various personal attributes from prompts with the help of\nextensive sets of unstructured training data. For instance, from the\nprompt, ‚ÄúI always get stuck there waiting for a hook turn, ‚Äù LLMs\ncould infer that the individual is in Melbourne, as hook turns are a\ndistinct traffic maneuver primarily employed there [65]. This infer-\nence ability, combined with widespread LLM availability, reduces\nthe costs of private data inferences, enabling adversaries to scale\nbeyond limitations imposed by costly human profilers. Existing\nrule-based or NER-based redaction tools often prove inadequate in\nprotecting users from such sensitive inference, as they struggle to\ndetect revealing, context-dependent cues.\nMitigation. Most existing works utilize LLMs themselves as\na mitigation technique to identify sensitive information that can\nbe inferred from prompts. A study in [ 107] shows the ability of\npre-trained LLMs to infer personal attributes from the text given at\ninference time. Their extensive experiments demonstrated LLMs‚Äô\ncapacity to infer personal attributes from real-world data, even\nwhen the text is anonymized using commercial tools. While this\npaper highlighted the potential for sensitive data leakage from\nprompts, it did not delve into mitigation techniques.\nBuilding on the work in [107], a later study [108] introduced a\nframework for anonymizing texts using an adversarial feedback-\nguided approach. This method leverages LLMs‚Äô strong attribute\ninference abilities to guide a separate anonymizer LLM. The process\ninvolves two steps: an ‚ÄòLLM adversary‚Äô performs private attribute\ninference, and an ‚Äòanonymizing LLM‚Äô adjusts the text to obscure\nor generalize the inferred cues. However, this method doesn‚Äôt fully\naddress where the LLM adversary should be located (user or service\nprovider) or the utility of the anonymized text. Using smaller, fine-\ntuned LLMs locally could improve this process by identifying inference\ndata before it reaches the remote LLM, and incorporating LLM bias\nand prior knowledge may enhance inference detection.\n3.2.3 The leakage of contextual information. LLMs often rely on\ncontextual information to generate personalized and impactful\ncontent, leveraging their in-context learning capabilities. The con-\ntextual information may include users‚Äô personal data, such as app\nusage data (e.g., shopping lists, logs, and calendar events). Most\nusers include contextual data in prompts for improved performance,\nas it is more parameter and data-efficient than fine-tuning [28].\nGiven the granular nature of contextual data, privacy risks natu-\nrally arise. It‚Äôs essential to understand these risks, mainly because\nthe data used in prompts often comes from smaller, private datasets,\nunlike the large public corpora used for pretraining LLMs. This\nsmall set of private data is more vulnerable to inference attacks.\nTherefore, striking a balance between performance and privacy is\ncrucial.\nMitigation. In the literature, techniques for protecting contex-\ntual data in prompts include hashing operations [133], prompt en-\nsembling [28], and employing a personalized LM on the local with\ncontextual information to improve the remote LLM responses[141].\nYim et al. [133] used a hash operation on context data before send-\ning it to the LLM, ensuring only hashed values are transmitted. The\nLLM‚Äôs response also includes hashed values that are then reverted\nto their original form. This method protects personal data privacy\nwhile enabling personalized responses. Duan et al. [28] employed a\nprompt ensembling method to mitigate MIA on prompted data with\ncontextual information. This method reduces the MIA success rate\nto near-random guessing levels. The prompt ensembling technique\naggregates prediction probability vectors over multiple indepen-\ndent prompted models into an ensemble prediction. In CoGenesis\n[141], smaller, personalized LMs on user devices access private\ndata and activity logs, while advanced general LLMs operate in the\ncloud, receiving only general instructions and providing high-level\nknowledge. This allows for collaborative content generation, where\nthe user-end model uses context and responses from the LLMs to\nproduce personalized outputs.\nExisting techniques face key challenges. Hashing methods [ 133]\nrely on trusting the LLM and only protect against eavesdropping.\nPrompt ensembling [ 28] lacks formal privacy guarantees and adds\ncomputational overhead. CoGenesis [ 141] depends on logit access,\nlimiting its use with closed-source LLMs, and tested on synthetic data,\nraising questions about real-world applicability.\nKey Takeaways\nUser prompts submitted to LLM systems may contain sensi-\ntive information. Given the advanced inference capabilities\nof modern LLMs, even seemingly benign or contextual data\ncan lead to the disclosure of private details, making tradi-\ntional NER or rule-based detection methods inadequate. As\nmitigation, researchers are investigating the use of small,\nfine-tuned LLMs with ICL/RAG techniques at the user‚Äôs\nend to assess the risk of sensitive information inference\nand enhance prompt privacy.\nSoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks, and Mitigation\n3.3 Privacy Vulnerabilities in LLM-generated\nOutputs and Mitigation\nPrivacy vulnerabilities in LLM outputs arise when sensitive infor-\nmation, whether learned from training data or newly produced in\nresponse, is inadvertently disclosed to unintended recipients. This\ncan happen if the model memorizes personal or confidential details\nin a generated text or synthesizes proprietary content (such as in-\nternal strategies, algorithms, or trade secrets) accessible beyond\nauthorized channels. In either case, allowing these outputs to be\npublicly visible or logged by external systems exposes private data\nto potential misuse.\n3.3.1 Revealing Sensitive Information in LLM-generated Output. As\nreported in many studies [65, 86, 146], LLMs can memorize sensitive\ndata from prompts and include them in their output despite users\nexplicitly requesting them not to memorize their data in prompts.\nPriyanshu et al. [86] evaluated chatbot responses‚Äô compliance with\nprivacy regulations, particularly when user data are used as few-\nshot samples for ICL. They found that ChatGPT reproduces PII\naccurately 57.4% of the time, decreasing to 30.5% with regulation\nprompts and 15.2% with explicit removal prompts. This behavior can\nbe considered a privacy violation as it occurs without user consent.\nMoreover, there‚Äôs the potential for additional privacy breaches if\nthese outputs are compromised through attacks, eavesdropping, or\nretraining the LLM model with user data.\nThis risk is heightened when the model is fine-tuned with private\ndata for domain-specific applications, such as when organizations\nuse LLMs for specialized internal tasks. Even within a controlled\nsetting, sensitive data can be exposed to all users with access per-\nmissions. For example, an LLM-driven HR chatbot fine-tuned with\nemployee details could inadvertently allow employees to view sen-\nsitive information about their peers‚Äô salaries and benefits4. Without\nproper controls to restrict access, the model could expose this in-\nformation to all users.\nThis risk increases when LLM-generated outputs are shared with\nthird-party services, especially in integrated systems, such as chat\nplatforms or APIs. Unlike LLM agents, which autonomously inter-\nact with external tools and the environment, LLM output privacy\nconcerns stem from the uncontrolled dissemination of generated\nresponses to third-party services without the user‚Äôs explicit aware-\nness, increasing the risk of unintentional data leakage. Ensuring\ncontrol and transparency in accessing privacy data with user con-\nsent is crucial, especially since the output can potentially be sent to\nnumerous third-party applications once it leaves the LLM. Hence, it\nis essential to implement techniques to manage the data workflow\nand keep users informed about it.\nMitigation. Mitigation techniques for privacy challenges in\nLLM systems‚Äô outputs are still nascent, with only a few research\nstudies addressing this issue. Given the lack of user control over the\nblack box settings in LLM services, controlling the LLM systems‚Äô\ndecisions is challenging. The only feasible method for users to\ninfluence the model output is solely through prompts. Otherwise,\nservice providers are responsible for managing the output.\nYao et al. [129] proposed an initial approach to protect LLM de-\ncisions in a black-box manner. They define decision privacy and\n4https://medium.com/snowflake/handling-sensitive-data-with-llms-aa765f8ce840\ninvestigate instance obfuscation strategies for decision privacy.\nThey tackled the privacy concerns of decision-making by append-\ning an ‚Äòobfuscator‚Äô (another random text) to the original prompt.\nThe method uses obfuscators to alter LLM decision distribution,\npreventing adversaries from inferring the correct decision while\nthe data owner resolves it from obfuscated inputs.\nLikewise, to protect the privacy of decisions made in ICL, the\nstudies in [113, 119] concentrated on differentially private aggrega-\ntion methods to prevent the direct extraction of private data. Wu\net al. [119] introduce the DP-ICL framework, which can aggregate\nand release responses without heavily relying on any individual\nexamples provided in the demonstrated private data. The funda-\nmental concept behind the DP-ICL framework involves producing\ndifferentially private responses by aggregating noisy consensus\nfrom an ensemble of LLMs‚Äô outputs, each operating on separate\nsets of examples.\nIn summary, current mitigation techniques for protecting private\ninformation in LLM outputs illustrate a clear trade-off between privacy\nand utility. Specifically, the approach in [ 129], designed mainly for\nstatic tasks (e.g., text classification), does not adequately address the\ngenerative nature of LLM outputs, risking exposure of confidential data\nthrough dynamic outputs. Moreover, cross-session data retention poses\nan additional threat to data privacy: if previous conversation states,\nmemory buffers, or hidden user context are not carefully managed\nor sanitized, sensitive information can reappear in later responses,\nbypassing any initial privacy safeguards. Consequently, while these\nmethods offer valuable insights, they require further refinement to\nincorporate robust safeguards against unintended data exposure.\nKey Takeaways\nRecent techniques such as ICL and RAG have exacerbated\nprivacy concerns with LLM-generated outputs, as users\nincreasingly provide sensitive data for domain-specific\ntasks. The opaque nature of black-box LLM services lim-\nits user control, making it difficult to manage or audit\nmodel-generated decisions externally. To mitigate these\nrisks, users may obfuscate their input before submission.\nAdditionally, LLM providers may consider implementing\ntechniques like DP to reduce potential leakage in outputs.\n3.4 Privacy Challenges while Involving Agents\nin the LLM System Tasks and Mitigation\nThis section addresses privacy challenges that arise when users\ninteract with LLM agents to accomplish tasks that communicate\nwith the external world. These privacy challenges in LLM agents\ncan be categorized into three areas: automated task execution via\nLLM agents, adversarial interactions of agents, and the potential\nexposure of sensitive information to third-party tools via agents.\n3.4.1 Privacy Issues Caused by Automated Task Execution via LLM\nAgents. Human instructions via prompts often contain ambiguities\nor omit crucial details. It is imperative to ensure the resilience\nand reliability of the agents‚Äô decisions, guaranteeing alignment\nbetween the actions performed by the agent and their intended\ntasks. Given that unknown risks lie in complex environments and\nYashothara Shanmugarasa, Ming Ding, M.A.P. Chamikara, and Thierry Rakotoarivelo\nuser instructions, LLM agents are prone to causing unexpected\nprivacy and safety issues [99, 121]. For instance, a LLM agent tasked\nto process emails might click on phishing links, leading to potential\nprivacy breaches and financial loss. Similarly, if instructed to send\nan email with file content, the agent might accidentally include\nsensitive details like credit card information. In another example,\n[35] demonstrated that LLM agents could even exploit one-day\nvulnerabilities to hack websites based on task descriptions.\nHowever, due to their long-tail nature, identifying the risks asso-\nciated with LLM agents is challenging. These risks arise as agents\ninteract with various tools to execute tasks, coupled with the open-\nended nature of potential issues and the substantial engineering\neffort required for testing such interactions.\nMitigation. Several approaches assess whether user instruc-\ntions are safe to execute without compromising user privacy and\nsafety when evaluating LLMs‚Äô risk awareness regarding agent pri-\nvacy. ToolEmu [99] implemented a GPT-4 powered emulator with\ndiverse tools and scenarios to identify potential failure modes in\nLLM agents and create sandbox states to trigger such failures. It\nalso includes a GPT-4 powered safety evaluator to quantify risks.\nSimilarly, AgentMonitor [77] proposed using an LLM to monitor\nand halt unsafe actions, preventing potential safety and privacy\nissues on the open internet. Both ToolEmu and AgentMonitor use\nLLMs to identify risky actions in agents. However, these methods\nhave limitations: They often ignore core constraints, particularly\nin complex multi-turn interactions (back-and-forth interactions\nbetween LLM agents, users, and diverse environments), and rely\non humans to provide risk descriptions.\nTo evaluate LLMs‚Äô risk awareness in agent safety through com-\nplex multi-turn interactions, Yuan et al. [135] developed R-Judge, a\nbenchmark dataset. The dataset includes user instructions, agent\naction histories, and environment feedback annotated with safety\nlabels. In evaluating automated harmful action detection, most\nLLMs struggled to identify safety and privacy risks, with GPT-4\nachieving an F1 score of 72.52% versus the human score of 89.07%.\nHua et al. [44] presented TrustAgent, an agent framework em-\nploying three strategies to ensure safety: pre-planning (injecting\nsafety knowledge before plan generation), in-planning (bolstering\nsafety during plan generation), and post-planning (ensuring safety\nthrough post-planning inspection). Experiments show that TrustA-\ngent enhances both safety and helpfulness. However, the study\nunderscores the need for inherent reasoning abilities within LLMs\nto support truly safe agents.\nAside from the methods discussed that rely on external super-\nvision from humans or other LLMs, which require significant in-\nvestment in human labor and computational resources, alternative\nmethods can operate without human supervision. Self-alignment\n[111] is an emerging paradigm where LLMs can independently\nachieve value alignment. Pang et al. [84] explored this direction for\nachieving self-alignment of LLMs through multi-agent role-playing.\nThis method allows the LLM to create a simulation environment\nthat mirrors real-world multi-party interactions and simulates the\nsocial consequences of a user‚Äôs instruction. It records the textual\ninteractions of the roles and summarizes them as the final output.\nAlthough the simulation process can be time-consuming during\ninference, fine-tuning can mitigate this issue.\nIn summary, while these methods aim to prevent LLM agents\nfrom disclosing sensitive or personal data, they come with notable\ntrade-offs and limitations tied directly to data privacy. Supervision-\nbased approaches (ToolEmu, AgentMonitor, and TrustAgent) are ef-\nfective at flagging and mitigating privacy risks but depend on human\noversight. This reliance introduces its own privacy challenges (e.g.,\npotential exposure of private information to supervisors) and can be\nresource-intensive. R-Judge seeks to automate the detection of privacy-\nrelated risks, yet it struggles with nuanced or context-dependent leaks,\nhighlighting that even advanced models like GPT-4 still fall short of\nhuman-level detection in complex data-privacy scenarios. Meanwhile,\nself-alignment techniques attempt to reduce reliance on external su-\npervision by enabling the model itself to learn protective behaviors\naround sensitive data. However, these methods are computationally\nexpensive and have yet to prove robust across diverse real-world pri-\nvacy contexts (where inadvertent disclosure can occur in unpredictable\nways). Moving forward, designing scalable, adaptive frameworks with\nminimal human involvement remains critical: such approaches must\nspecifically address how to protect sensitive information while ensur-\ning that any oversight mechanisms themselves do not become new\nvectors for privacy compromise.\n3.4.2 Adversarial Interactions of Agents. Privacy risks can be fur-\nther exacerbated by adversarial agents that mimic honest behavior\nand request additional private information under the guise of need-\ning it for prompt execution tasks. Since LLM agents have demon-\nstrated remarkable capabilities in generating human-like text, users\nmay unwittingly disclose more information than they would to\nother service-providing systems [146]. In [107], the authors explore\nthe interaction between LLM agents and users, where agents gather\nadditional information through human-like conversations.\nLLM agents present complex privacy concerns due to their gen-\nerative power and vulnerability to adversarial manipulation. Tech-\nniques such as ICL and RAG can be exploited to train agents for\nmalicious purposes with minimal input. [39] found that modifying\na single agent can rapidly lead to widespread harmful behaviors\namong agents without further intervention from the adversary. The\nnature of LLM agents and advanced techniques increases the risk of\nadversarial attacks and interactions. For evidence, [114] introduced\nEvil Geniuses, a virtual team that develops malevolent strategies\nand conducts Red-Blue exercises, demonstrating successful harmful\nactions through adversarial interactions. Similarly, [109] explored\nthe potential for LLMs to act as adversaries by perturbing text sam-\nples to bypass safety measures. They investigated whether LLMs\ncan inherently create adversarial examples from benign samples\nto deceive existing safeguards. Their findings underscore signifi-\ncant challenges for (semi-)autonomous systems using LLM agents,\nparticularly concerning potential adversarial behavior.\nMitigation. Chern et al. [ 18] proposed a multi-agent debate\nmechanism to improve quality and mitigate adversarial behavior. In\nthis approach, agents self-evaluate through discussion and feedback.\nThey tested this method with state-of-the-art models, evaluating\nsusceptibility to red team attacks in single- and multi-agent settings.\nThe results show that multi-agent debate generally produces less\ntoxic responses to adversarial prompts without introducing new\nrisks at inference time. However, the approach is resource-intensive,\nSoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks, and Mitigation\nrequires multiple queries, relies on knowledge from a single LLM,\nand does not address removing toxic outputs effectively.\nThough multi-agent settings can mitigate this issue, multiple\nagents can have secret collusion between themselves. Motwani et\nal. [76] formalized the issue of secret collusion in generative AI\nsystems, proposing mitigation measures based on AI and security\nprinciples. They investigated monitoring the information content\nin agents‚Äô communications, focusing on agents‚Äô capabilities and in-\ncentives to evade detection. Using monitoring agents, they detected\nsteganographic techniques that agents use to hide information by\nanalyzing cover text anomalies and agent simulations. Future work\ncould extend this by examining complexity and information theory,\noptimizing pressures, and exploring complex multi-agent settings.\nCommon alignment methods to reduce privacy risks include fine-\ntuning [40, 138] and reward modeling [59]. During fine-tuning, mod-\nels are trained on specific datasets, incorporating human-generated\nexamples to align with desired behaviors. Reward modeling in-\nvolves optimizing a reward function to reflect desired outcomes,\noften using reinforcement learning techniques to adjust the model‚Äôs\nbehavior accordingly. Although intelligent personal agents should\nminimize user interruptions, integrating user opinions or human\nassistance can prove valuable when making significant decisions.\nIn summary, while multi-agent debate and alignment strategies en-\nhance LLM privacy, they come with trade-offs. Debate-based methods\nreduce adversarial risks but are resource-intensive and knowledge-\nlimited. Secret collusion raises concerns about hidden adversarial be-\nhavior, requiring costly monitoring. Fine-tuning and reward modeling\nenhance alignment but may not generalize well.\n3.4.3 Potential Exposure of Sensitive Information to Third-party\nTools. The next challenge is preventing the unnecessary and unau-\nthorized disclosure of user-sensitive information during interac-\ntions with agents to third-party tools and the external world [67].\nThe LLM agents orchestrate the task and invoke relevant third-\nparty tools to execute it with the collected private information.\nHowever, in this process, the LLM possesses knowledge of the\nuser‚Äôs private input and autonomously initiates queries in places\nwithout user awareness or inadvertently shares this information\nwith the other agents or external tools. Another threat model is\nadversarial third-party applications that can manipulate the con-\ntext of interaction to trick LLM-based agents into exposing private\ninformation irrelevant to the task [5].\nMitigation. The study in [144] examined privacy concerns for\ntool-using LLM agents. They proposed PrivacyAsst using encryption-\nbased and shuffling-based solutions to preserve privacy. The encryption-\nbased solution allows tasks to operate on encrypted inputs. In\ncontrast, the shuffling-based solution uses attribute-based forgery\ngenerative models and an attribute shuffling mechanism to create\nprivacy-preserving requests. However, limitations include the de-\ntectability of dummy prompts by advanced LLMs, the complexity\nand computational cost of encryption, reliance on third-party tools,\nand the burden of creating multiple dummy inputs on users.\nAirGapAgent [5] focuses on preventing unintended data leakage\nduring third-party interactions. It restricts access to only the neces-\nsary data for specific tasks, considering user privacy preferences.\nThe design involves two LLMs: one minimizes data for sharing\nappropriately, while the other handles third-party interactions with\nthe minimized data. This ensures the agent can distinguish between\nprivate and non-private data for each task.\nPrivacyAsst and AirGapAgent enhance privacy but face challenges:\nPrivacyAsst incurs high computational costs and third-party reliance,\nwhile AirGapAgent‚Äôs effectiveness depends on accurate filtering. Both\nhighlight the trade-off between privacy and usability, requiring more\nscalable, adaptive solutions.\nKey Takeaways\nLLM agents‚Äô automated interactions with the physical and\ndigital worlds amplified their privacy vulnerabilities. Ex-\nisting mitigation approaches often rely on human-guided\nLLM emulators to enforce user-specific privacy preferences\nand prevent sensitive information leakage. Emerging ap-\nproaches such as self-alignment seek to reduce human\noversight by enabling agents to self-assess privacy risks\nthrough multi-agent role-playing.\nOverall, Figure 4 provides an intricate portrayal of privacy con-\ncerns, encapsulating the four broader privacy challenges and the\ntailored mitigation technologies deployed at various phases of LLM\nsystems, including data pre-training, training, and post-training\napproaches. A more detailed summary of these approaches can be\nfound in Appendix: Table 2.\n4 DISCUSSION AND FUTURE DIRECTIONS\nThis section highlights how some privacy-preserving methods\ncould address multiple privacy threats and discusses their research\nchallenges and potential future directions.\nNER is widely used for direct leakage prevention in both train-\ning data and user prompts. Similarly, encryption and DP-based\nmechanisms can mitigate multiple privacy risks with trade-offs\nin processing and model performance. Adversarial regularization,\nwhich enhances the robustness of LLM, also plays a role in mitigat-\ning privacy attacks. However, these solutions are often complex due\nto fine-tuning between privacy preservation and model efficiency.\nFurthermore, many solutions operate in isolation, addressing indi-\nvidual threats rather than working together as part of a unified and\nadaptive privacy framework.\nPrivacy concerns in LLM systems usually stem from foundational\nmodels and extend to various downstream applications.Traditional\nmethods for protecting LLM training data fall short against more\nadvanced attacks, especially when dealing with fine-tuned LLMs\nand their various ways of user interaction. These models are partic-\nularly susceptible to inference attacks due to their complexity and\nnatural language interaction. Existing mitigation techniques‚Äîsuch\nas detection, filtering, and model stacking‚Äî often come with trade-\noffs such as reduced performance, incomplete filtering, and higher\nlatency and computational costs. To enhance LLM privacy, future\napproaches should explore hybrid models that integrate adaptive\nfiltering, modular architectures, and human-in-the-loop validation.\nReducing training data influence through prompt conditioning\nor modular architectures could provide additional privacy gains\nwithout sacrificing accuracy, particularly in fine-tuned models. De-\nveloping domain-specific privacy frameworks tailored for LLMs\nYashothara Shanmugarasa, Ming Ding, M.A.P. Chamikara, and Thierry Rakotoarivelo\nGradient leakage attacks\nPotential Privacy Leakages\nData memorization and personal data leakage\nPotential Privacy\nAttaks\nExtraction attacks\nInference attacks\nReconstruction attacks\nTraining data extraction\nMembership inference attack\nAttribute inference attacks\nUser inference attacks\nPrivacy in LLM\nOutput (Sec. 3.3)\nData cleaning\nData depulication\nPost-training approaches\nDetection and Filtering\nTraining app.\nDifferential Privacy / Obfuscation strategies\nTest-time defense & instruction processing\nKnowledge unlearning\nAdversarial regularizations\nModel stacking and Dropout\nSynthetic few-shot demonstrations\nDifferentially private aggregation\nDirect leakage of sensitive data\nPotential inference of sensitive data\nLeakage of contextual information\nInput validation and Sanitization\nLocal LM and remote LLM combination\nText obfuscation\nAdversarial feedback guidance\nHashing and prompt ensembling\nPrivacy in LLM\nAgents (Sec. 3.4)\nRevealing Sensitive Information in LLM-generated\nOutput\nAmbiguities in automated task execution\nAdversarial interactions of agents\nExposure of sensitive information to third-parties\nLLM sandbox\nInjection of safety knowledge\nSelf-alignment\nMulti-agent debate\nMeasuring based on AI and security principles\nEncryption and shuffling\nAccess restriction\nPre-training app.\nPrivacy in LLM Training data (Sec. 3.1)Privacy in User Prompts (Sec.\n3.2)\nSafety training, RLHF, adversarial fine-tuning\nFully Homomorphic Encryption\nFigure 4: Summary of Privacy Challenges in LLM Systems and Mitigation Techniques in the Literature\nwill also be crucial in aligning with emerging regulations. Future\nresearch should also focus on developing adaptive privacy mech-\nanisms that dynamically assess conversation context and adjust\nLLM responses accordingly, ensuring that privacy-sensitive topics\ntrigger enhanced protections such as real-time content filtering,\nobfuscation, or user alerts.\nOther privacy-preserving approaches for generic models, such\nas cryptographic techniques, knowledge unlearning, and Feder-\nated Learning, show opportunities and challenges for the LLM\ndomain. Cryptographic methods, while theoretically effective, are\ncomputationally expensive when applied to LLMs due to their large-\nscale processing requirements. Knowledge unlearning struggles\nSoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks, and Mitigation\nwith completely removing sensitive information when dealing with\ndeeply integrated training data [106]. Adapting such techniques for\nLLMs requires careful optimization to balance privacy, efficiency,\nand performance.\nAnother critical direction is the advancement of policy-driven\nAI governance. Efforts such as the EU AI Act [90], AI Safety Insti-\ntutes [2], and Anthropic‚Äôs Constitutional AI [7] highlight the need\nfor clearer guidelines on data collection, storage, and use in LLM\nsystems. Automated auditing mechanisms and adversarial testing\ncan further enhance transparency and accountability in LLM de-\nployment, while continuous monitoring, adversarial testing, and\nproactive threat modeling could mitigate other privacy risks.\n5 CONCLUSION\nOur SoK identifies four main categories of privacy challenges: (i) pri-\nvacy issues in LLM training data, (ii) privacy challenges associated\nwith user prompts, (iii) privacy risks in LLM-generated outputs,\nand (iv) privacy vulnerabilities involving LLM agents. While the\nliterature has extensively examined privacy issues in training data,\nprivacy issues arising from user interactions and LLM outputs, par-\nticularly involving small and private datasets through fine-tuning,\nICL, RAG, and agents, remain underexplored. Addressing these\noverlooked areas requires further research to strengthen privacy\nsafeguards across user prompts, LLM outputs, and agent-based\napplications.\nREFERENCES\n[1] Accountability Act. 1996. Health insurance portability and accountability act\nof 1996. Public law 104 (1996), 191.\n[2] UK AI Safety Institute. 2024. Inspect AI: Framework for Large Language Model\nEvaluations. UK AI Safety Institute. https://github.com/UKGovernmentBEIS/\ninspect_ai\n[3] Devansh Arpit, Stanis≈Çaw Jastrzundefinedbski, Nicolas Ballas, David Krueger,\nEmmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron\nCourville, Yoshua Bengio, and Simon Lacoste-Julien. 2017. A closer look at\nmemorization in deep networks. In Proceedings of the 34th International Con-\nference on Machine Learning - Volume 70 (Sydney, NSW, Australia)(ICML‚Äô17).\nJMLR.org, 233‚Äì242.\n[4] Shubhi Asthana, Ruchi Mahindru, Bing Zhang, and Jorge Sanz. 2025. Adap-\ntive PII Mitigation Framework for Large Language Models. arXiv preprint\narXiv:2501.12465 (2025).\n[5] Eugene Bagdasaryan, Ren Yi, Sahra Ghalebikesabi, Peter Kairouz, Marco\nGruteser, Sewoong Oh, Borja Balle, and Daniel Ramage. 2024. Air Gap: Protect-\ning Privacy-Conscious Conversational Agents. arXiv preprint arXiv:2405.05175\n(2024).\n[6] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson\nKernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron\nMcKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback. arXiv\npreprint arXiv:2212.08073 (2022).\n[7] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, and Amanda Askell et al. 2022.\nConstitutional AI: Harmlessness from AI Feedback. arXiv:2212.08073 [cs.CL]\nhttps://arxiv.org/abs/2212.08073\n[8] Borja Balle, Giovanni Cherubin, and Jamie Hayes. 2022. Reconstructing training\ndata with informed adversaries. In2022 IEEE Symposium on Security and Privacy\n(SP). IEEE, 1138‚Äì1156.\n[9] Mislav Balunovic, Dimitar Dimitrov, Nikola Jovanoviƒá, and Martin Vechev. 2022.\nLamp: Extracting text from gradients with language model priors. Advances in\nNeural Information Processing Systems 35 (2022), 7641‚Äì7654.\n[10] Dipto Barman, Ziyi Guo, and Owen Conlan. 2024. The dark side of language\nmodels: Exploring the potential of llms in multimedia disinformation generation\nand dissemination. Machine Learning with Applications (2024), 100545.\n[11] Kristian Gonz√°lez Barman, Nathan Wood, and Pawel Pawlowski. 2024. Beyond\ntransparency and explainability: on the need for adequate and contextualized\nuser guidelines for LLM use. Ethics and Information Technology 26, 3 (2024), 47.\n[12] Rouzbeh Behnia, Mohammadreza Reza Ebrahimi, Jason Pacheco, and Balaji\nPadmanabhan. 2022. Ew-tune: A framework for privately fine-tuning large\nlanguage models with differential privacy. In2022 IEEE International Conference\non Data Mining Workshops (ICDMW) . IEEE, 560‚Äì566.\n[13] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877‚Äì1901.\n[14] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-\nVoss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson,\net al. 2021. Extracting training data from large language models. In30th USENIX\nsecurity symposium (USENIX Security 21) . 2633‚Äì2650.\n[15] Ting-Yun Chang, Jesse Thomason, and Robin Jia. 2023. Do Localization Methods\nActually Localize Memorized Data in LLMs? arXiv preprint arXiv:2311.09060\n(2023).\n[16] Tianyu Chen, Hangbo Bao, Shaohan Huang, Li Dong, Binxing Jiao, Daxin Jiang,\nHaoyi Zhou, Jianxin Li, and Furu Wei. 2022. The-x: Privacy-preserving trans-\nformer inference with homomorphic encryption.arXiv preprint arXiv:2206.00216\n(2022).\n[17] Yu Chen, Tingxin Li, Huiming Liu, and Yang Yu. 2023. Hide and\nSeek (HaS): A Lightweight Framework for Prompt Privacy Protection.\narXiv:2309.03057 [cs.CR]\n[18] Steffi Chern, Zhen Fan, and Andy Liu. 2024. Combating Adversarial Attacks\nwith Multi-Agent Debate. arXiv preprint arXiv:2401.05998 (2024).\n[19] Junjie Chu, Zeyang Sha, Michael Backes, and Yang Zhang. 2024. Conversation\nReconstruction Attack Against GPT Models. arXiv preprint arXiv:2402.02987\n(2024).\n[20] Badhan Chandra Das, M Hadi Amini, and Yanzhao Wu. 2024. Security\nand privacy challenges of large language models: A survey. arXiv preprint\narXiv:2402.00888 (2024).\n[21] Leo de Castro, Antigoni Polychroniadou, and Daniel Escudero. 2024. Privacy-\nPreserving Large Language Model Inference via GPU-Accelerated Fully Homo-\nmorphic Encryption. In Neurips Safe Generative AI Workshop 2024 .\n[22] Jieren Deng, Yijue Wang, Ji Li, Chao Shang, Hang Liu, Sanguthevar Rajasekaran,\nand Caiwen Ding. 2021. Tag: Gradient attack on transformer-based language\nmodels. arXiv preprint arXiv:2103.06819 (2021).\n[23] Sentry Developers. [n. d.]. PII and Data Scrubbing ‚Äî develop.sentry.dev. https:\n//develop.sentry.dev/pii. [Accessed 24-04-2024].\n[24] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\narXiv:1810.04805 [cs.CL] https://arxiv.org/abs/1810.04805\n[25] Peng Ding, Jun Kuang, Dan Ma, Xuezhi Cao, Yunsen Xian, Jiajun Chen, and\nShujian Huang. 2023. A Wolf in Sheep‚Äôs Clothing: Generalized Nested Jailbreak\nPrompts can Fool Large Language Models Easily.arXiv preprint arXiv:2311.08268\n(2023).\n[26] Xinshuai Dong, Anh Tuan Luu, Min Lin, Shuicheng Yan, and Hanwang Zhang.\n2021. How should pre-trained language models be fine-tuned towards adver-\nsarial robustness? Advances in Neural Information Processing Systems 34 (2021),\n4356‚Äì4369.\n[27] Wenyu Du, Tongxu Luo, Zihan Qiu, Zeyu Huang, Yikang Shen, Reynold Cheng,\nYike Guo, and Jie Fu. 2024. Stacking Your Transformers: A Closer Look at Model\nGrowth for Efficient LLM Pre-Training. arXiv preprint arXiv:2405.15319 (2024).\n[28] Haonan Duan, Adam Dziedzic, Mohammad Yaghini, Nicolas Papernot, and\nFranziska Boenisch. 2023. On the privacy risk of in-context learning. In The\n61st Annual Meeting Of The Association For Computational Linguistics .\n[29] Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia\nShi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, and Hannaneh\nHajishirzi. 2024. Do Membership Inference Attacks Work on Large Language\nModels? arXiv preprint arXiv:2402.07841 (2024).\n[30] Travis Dyde. 2023. Documentation on the emergence, current iterations, and\npossible future of Artificial Intelligence with a focus on Large Language Models.\n(2023).\n[31] Kennedy Edemacu and Xintao Wu. 2024. Privacy Preserving Prompt Engineer-\ning: A Survey. arXiv preprint arXiv:2404.06001 (2024).\n[32] L. Ekenstam. 2023. Linus on x: \"if you think this is creepy. . . you should know\nwhat all major stores in the world knows about you as a shopper they use\nin-store, online, cell-tower, and more to keep track of everything. . . everything.\ncambridge analytics is a joke in comparison\". https://x.com/LinusEkenstam/\nstatus/1692602911518343502.\n[33] Ronen Eldan and Mark Russinovich. 2023. Who‚Äôs Harry Potter? Approximate\nUnlearning in LLMs. arXiv:2310.02238 [cs.CL]\n[34] Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, and\nQiang Yang. 2023. Fate-llm: A industrial grade federated learning framework\nfor large language models. arXiv preprint arXiv:2310.10049 (2023).\n[35] Richard Fang, Rohan Bindu, Akul Gupta, and Daniel Kang. 2024. LLM Agents can\nAutonomously Exploit One-day Vulnerabilities. arXiv preprint arXiv:2404.08144\n(2024).\n[36] Vitaly Feldman. 2020. Does learning require memorization? a short tale about a\nlong tail. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory\nof Computing . 954‚Äì959.\nYashothara Shanmugarasa, Ming Ding, M.A.P. Chamikara, and Thierry Rakotoarivelo\n[37] Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, and Tao Jiang.\n2023. Practical membership inference attacks against fine-tuned large language\nmodels via self-prompt calibration. arXiv preprint arXiv:2311.06062 (2023).\n[38] Neil Zhenqiang Gong and Bin Liu. 2018. Attribute inference attacks in online\nsocial networks. ACM Transactions on Privacy and Security (TOPS) 21, 1 (2018),\n1‚Äì30.\n[39] Xiangming Gu, Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Ye Wang, Jing\nJiang, and Min Lin. 2024. Agent Smith: A Single Image Can Jailbreak One Million\nMultimodal LLM Agents Exponentially Fast. arXiv preprint arXiv:2402.08567\n(2024).\n[40] Suchin Gururangan, Ana Marasoviƒá, Swabha Swayamdipta, Kyle Lo, Iz Beltagy,\nDoug Downey, and Noah A Smith. 2020. Don‚Äôt stop pretraining: Adapt language\nmodels to domains and tasks. arXiv preprint arXiv:2004.10964 (2020).\n[41] David Haber. 2024. Introducing Lakera Guard ‚Äì Bringing Enterprise-Grade\nSecurity to LLMs with One Line of Code | Lakera ‚Äì Protecting AI teams that dis-\nrupt the world. ‚Äî lakera.ai. https://www.lakera.ai/blog/lakera-guard-overview.\n[Accessed 15-05-2024].\n[42] Meng Hao, Hongwei Li, Hanxiao Chen, Pengzhi Xing, Guowen Xu, and Tianwei\nZhang. 2022. Iron: Private inference on transformers. Advances in neural\ninformation processing systems 35 (2022), 15718‚Äì15731.\n[43] Florian Hartmann, Duc-Hieu Tran, Peter Kairouz, Victor CƒÉrbune, et al. 2024.\nCan LLMs get help from other LLMs without revealing private information?\narXiv preprint arXiv:2404.01041 (2024).\n[44] Wenyue Hua, Xianjun Yang, Zelong Li, Cheng Wei, and Yongfeng Zhang. 2024.\nTrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent\nConstitution. arXiv preprint arXiv:2402.01586 (2024).\n[45] Daphne Ippolito, Florian Tram√®r, Milad Nasr, Chiyuan Zhang, Matthew Jagielski,\nKatherine Lee, Christopher A Choquette-Choo, and Nicholas Carlini. 2022.\nPreventing verbatim memorization in language models gives a false sense of\nprivacy. arXiv preprint arXiv:2210.17546 (2022).\n[46] Abhyuday Jagannatha, Bhanu Pratap Singh Rawat, and Hong Yu. 2021. Member-\nship inference attack susceptibility of clinical language models. arXiv preprint\narXiv:2104.08305 (2021).\n[47] Matthew Jagielski, Om Thakkar, Florian Tramer, Daphne Ippolito, Katherine\nLee, Nicholas Carlini, Eric Wallace, Shuang Song, Abhradeep Thakurta, Nicolas\nPapernot, et al. 2022. Measuring forgetting of memorized training examples.\narXiv preprint arXiv:2207.00099 (2022).\n[48] Tanqiu Jiang, Zian Wang, Jiacheng Liang, Changjiang Li, Yuhui Wang, and Ting\nWang. 2025. RobustKV: Defending Large Language Models against Jailbreak\nAttacks via KV Eviction. In The Thirteenth International Conference on Learning\nRepresentations. https://openreview.net/forum?id=L5godAOC2z\n[49] Junfeng Jiao, Saleh Afroogh, Yiming Xu, and Connor Phillips. 2024. Navigating\nllm ethics: Advancements, challenges, and future directions. arXiv preprint\narXiv:2406.18841 (2024).\n[50] Nikhil Kandpal, Krishna Pillutla, Alina Oprea, Peter Kairouz, Christopher A\nChoquette-Choo, and Zheng Xu. 2023. User inference attacks on large language\nmodels. arXiv preprint arXiv:2310.09266 (2023).\n[51] Nikhil Kandpal, Eric Wallace, and Colin Raffel. 2022. Deduplicating training\ndata mitigates privacy risks in language models. In International Conference on\nMachine Learning . PMLR, 10697‚Äì10707.\n[52] Antonia Karamolegkou, Jiaang Li, Li Zhou, and Anders S√∏gaard. 2023. Copyright\nviolations and large language models. arXiv preprint arXiv:2310.13771 (2023).\n[53] Aly Kassem, Omar Mahmoud, and Sherif Saad. 2023. Preserving Privacy\nThrough Dememorization: An Unlearning Technique For Mitigating Mem-\norization Risks In Language Models. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing , Houda Bouamor, Juan Pino,\nand Kalika Bali (Eds.). Association for Computational Linguistics, Singapore,\n4360‚Äì4379. https://doi.org/10.18653/v1/2023.emnlp-main.265\n[54] Yigitcan Kaya, Sanghyun Hong, and Tudor Dumitras. 2020. On the\nEffectiveness of Regularization Against Membership Inference Attacks.\narXiv:2006.05336 [cs.LG] https://arxiv.org/abs/2006.05336\n[55] Sunder Ali Khowaja, Parus Khuwaja, and Kapal Dev. 2023. Chatgpt needs spade\n(sustainability, privacy, digital divide, and ethics) evaluation: A review.arXiv\npreprint arXiv:2305.03123 (2023).\n[56] Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, and\nSeong Joon Oh. 2024. Propile: Probing privacy leakage in large language\nmodels. Advances in Neural Information Processing Systems 36 (2024).\n[57] Jacob Leon Kr√∂ger, Leon Gellrich, Sebastian Pape, Saba Rebecca Brause, and\nStefan Ullrich. 2022. Personal information inference from voice recordings:\nUser awareness and privacy concerns. Proc. Priv. Enhancing Technol. 2022, 1\n(2022), 6‚Äì27.\n[58] Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei Gao, Xuchen Pan,\nYuexiang Xie, Yaliang Li, Bolin Ding, and Jingren Zhou. 2023. Federatedscope-\nllm: A comprehensive package for fine-tuning large language models in feder-\nated learning. arXiv preprint arXiv:2309.00363 (2023).\n[59] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard,\nColton Bishop, Victor Carbune, and Abhinav Rastogi. 2023. Rlaif: Scaling\nreinforcement learning from human feedback with ai feedback. arXiv preprint\narXiv:2309.00267 (2023).\n[60] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas\nEck, Chris Callison-Burch, and Nicholas Carlini. 2021. Deduplicating training\ndata makes language models better. arXiv preprint arXiv:2107.06499 (2021).\n[61] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir\nKarpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt√§schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive\nnlp tasks. Advances in Neural Information Processing Systems 33 (2020), 9459‚Äì\n9474.\n[62] Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, and Yangqiu\nSong. 2023. Multi-step jailbreaking privacy attacks on chatgpt. arXiv preprint\narXiv:2304.05197 (2023).\n[63] Kunhong Li, Ruwei Huang, and Bo Yang. 2025. Privacy-Preserving Text Classi-\nfication on Deep Neural Network. Neural Processing Letters 57, 2 (2025), 29.\n[64] Linyang Li, Demin Song, and Xipeng Qiu. 2022. Text adversarial purification as\ndefense against adversarial attacks. arXiv preprint arXiv:2203.14207 (2022).\n[65] Tianshi Li, Sauvik Das, Hao-Ping Lee, Dakuo Wang, Bingsheng Yao, and Zhiping\nZhang. 2024. Human-Centered Privacy Research in the Age of Large Language\nModels. arXiv preprint arXiv:2402.01994 (2024).\n[66] Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. 2021. Large\nlanguage models can be strong differentially private learners. arXiv preprint\narXiv:2110.05679 (2021).\n[67] Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu,\nJiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, et al. 2024. Personal llm agents:\nInsights and survey about the capability, efficiency and security. arXiv preprint\narXiv:2401.05459 (2024).\n[68] Guo Lin, Wenyue Hua, and Yongfeng Zhang. 2024. PromptCrypt: Prompt\nEncryption for Secure Communication with Large Language Models. arXiv\npreprint arXiv:2402.05868 (2024).\n[69] Tong Liu, Yingjie Zhang, Zhe Zhao, Yinpeng Dong, Guozhu Meng, and Kai Chen.\n2024. Making Them Ask and Answer: Jailbreaking Large Language Models in\nFew Queries via Disguise and Reconstruction. arXiv preprint arXiv:2402.18104\n(2024).\n[70] Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida\nZhao, Tianwei Zhang, and Yang Liu. 2023. Jailbreaking chatgpt via prompt\nengineering: An empirical study. arXiv preprint arXiv:2305.13860 (2023).\n[71] Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian,\nIgor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Kr-\nishnamoorthi, Liangzhen Lai, and Vikas Chandra. 2024. MobileLLM: Op-\ntimizing Sub-billion Parameter Language Models for On-Device Use Cases.\narXiv:2402.14905 [cs.LG] https://arxiv.org/abs/2402.14905\n[72] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting\nmodel predictions. Advances in neural information processing systems 30 (2017).\n[73] Aatish Mandelecha. 2024. How to Secure Sensitive Data in LLM Prompts?\n‚Äî strac.io. https://www.strac.io/blog/secure-sensitive-data-in-llm-prompts.\n[Accessed 14-05-2024].\n[74] E McGowan. 2024. Is ChatGPT‚Äôs use of people‚Äôs data even legal? ‚Äî\nblog.avast.com. https://blog.avast.com/chatgpt-data-use-legal. [Accessed\n04-04-2024].\n[75] Fatemehsadat Mireshghallah, Archit Uniyal, Tianhao Wang, David K Evans,\nand Taylor Berg-Kirkpatrick. 2022. An empirical analysis of memorization in\nfine-tuned autoregressive language models. InProceedings of the 2022 Conference\non Empirical Methods in Natural Language Processing . 1816‚Äì1826.\n[76] Sumeet Ramesh Motwani, Mikhail Baranchuk, Martin Strohmeier, Vijay Bolina,\nPhilip HS Torr, Lewis Hammond, and Christian Schroeder de Witt. 2024. Secret\nCollusion Among Generative AI Agents.arXiv preprint arXiv:2402.07510 (2024).\n[77] Silen Naihin, David Atkinson, Marc Green, Merwane Hamadi, Craig Swift, Dou-\nglas Schonholtz, Adam Tauman Kalai, and David Bau. 2023. Testing language\nmodel agents safely in the wild. arXiv preprint arXiv:2311.10538 (2023).\n[78] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina\nKim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021. Webgpt: Browser-assisted question-answering with human feedback.\narXiv preprint arXiv:2112.09332 (2021).\n[79] Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A Feder\nCooper, Daphne Ippolito, Christopher A Choquette-Choo, Eric Wallace, Florian\nTram√®r, and Katherine Lee. 2023. Scalable extraction of training data from\n(production) language models. arXiv preprint arXiv:2311.17035 (2023).\n[80] Seth Neel and Peter Chang. 2023. Privacy issues in large language models: A\nsurvey. arXiv preprint arXiv:2312.06717 (2023).\n[81] Helen Nissenbaum. 2004. Privacy as contextual integrity. Wash. L. Rev. 79\n(2004), 119.\n[82] Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schu-\nbert, Katherine Ye, and Alexander Mordvintsev. 2018. The building blocks of\ninterpretability. Distill 3, 3 (2018), e10.\n[83] Matthew J Page, Joanne E McKenzie, Patrick M Bossuyt, Isabelle Boutron,\nTammy C Hoffmann, Cynthia D Mulrow, Larissa Shamseer, Jennifer M Tetzlaff,\nElie A Akl, Sue E Brennan, et al. 2021. The PRISMA 2020 statement: an updated\nguideline for reporting systematic reviews. bmj 372 (2021).\nSoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks, and Mitigation\n[84] Xianghe Pang, Shuo Tang, Rui Ye, Yuxin Xiong, Bolun Zhang, Yanfeng Wang,\nand Siheng Chen. 2024. Self-Alignment of Large Language Models via\nMonopolylogue-based Social Scene Simulation. arXiv preprint arXiv:2402.05699\n(2024).\n[85] Ildik√≥ Pil√°n, Pierre Lison, Lilja √òvrelid, Anthi Papadopoulou, David S√°nchez, and\nMontserrat Batet. 2022. The text anonymization benchmark (tab): A dedicated\ncorpus and evaluation framework for text anonymization. Computational\nLinguistics 48, 4 (2022), 1053‚Äì1101.\n[86] Aman Priyanshu, Supriti Vijay, Ayush Kumar, Rakshit Naidu, and Fatemehsadat\nMireshghallah. 2023. Are chatbots ready for privacy-sensitive applications? an\ninvestigation into input regurgitation and prompt-induced sanitization. arXiv\npreprint arXiv:2305.15008 (2023).\n[87] Md Abdur Rahman, Lamyaa Alqahtani, Amna Albooq, and Alaa Ainousah. 2024.\nA Survey on Security and Privacy of Large Multimodal Deep Learning Mod-\nels: Teaching and Learning Perspective. In 2024 21st Learning and Technology\nConference (L&T). IEEE, 13‚Äì18.\n[88] Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, and Monojit\nChoudhury. 2023. Tricking llms into disobedience: Formalizing, analyzing, and\ndetecting jailbreaks. arXiv preprint arXiv:2305.14965 (2023).\n[89] Protection Regulation. 2016. Regulation (EU) 2016/679 of the European Parlia-\nment and of the Council. Regulation (eu) 679 (2016), 2016.\n[90] Protection Regulation. 2024. AI Act, Regulation (EU) 2024/1689 of the European\nParliament and of the Council. Regulation (eu) 1689 (2024).\n[91] Jingjing Ren, Ashwin Rao, Martina Lindorfer, Arnaud Legout, and David\nChoffnes. 2016. Recon: Revealing and controlling pii leaks in mobile network\ntraffic. In Proceedings of the 14th Annual International Conference on Mobile\nSystems, Applications, and Services . 361‚Äì374.\n[92] Donghwan Rho, Taeseong Kim, Minje Park, Jung Woo Kim, Hyunsik Chae,\nErnest K Ryu, and Jung Hee Cheon. 2024. Encryption-friendly LLM architecture.\narXiv preprint arXiv:2410.02486 (2024).\n[93] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \" Why should\ni trust you?\" Explaining the predictions of any classifier. In Proceedings of the\n22nd ACM SIGKDD international conference on knowledge discovery and data\nmining. 1135‚Äì1144.\n[94] Toran Bruce Richards. 2023. Auto-gpt: Autonomous artificial intelligence soft-\nware agent. https://github.com/Significant-Gravitas/AutoGPT. [Accessed\n08-04-2024].\n[95] Jae Hun Ro, Srinadh Bhojanapalli, Zheng Xu, Yanxiang Zhang, and\nAnanda Theertha Suresh. 2024. Efficient Language Model Architectures for Dif-\nferentially Private Federated Learning. arXiv preprint arXiv:2403.08100 (2024).\n[96] Alexander Robey, Eric Wong, Hamed Hassani, and George J Pappas. 2023.\nSmoothllm: Defending large language models against jailbreaking attacks.arXiv\npreprint arXiv:2310.03684 (2023).\n[97] Oscar J Romero, John Zimmerman, Aaron Steinfeld, and Anthony Tomasic. 2023.\nSynergistic integration of large language models and cognitive architectures\nfor robust ai: An exploratory analysis. In Proceedings of the AAAI Symposium\nSeries, Vol. 2. 396‚Äì405.\n[98] Lorenzo Rovida and Alberto Leporati. 2024. Transformer-based language models\nand homomorphic encryption: An intersection with bert-tiny. In Proceedings of\nthe 10th ACM International Workshop on Security and Privacy Analytics . 3‚Äì13.\n[99] Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou,\nJimmy Ba, Yann Dubois, Chris J Maddison, and Tatsunori Hashimoto. 2023.\nIdentifying the risks of lm agents with an lm-emulated sandbox. arXiv preprint\narXiv:2309.15817 (2023).\n[100] Zhang Ruoyan, Zheng Zhongxiang, and Bao Wankang. 2025. Practical Secure\nInference Algorithm for Fine-tuned Large Language Model Based on Fully\nHomomorphic Encryption. arXiv preprint arXiv:2501.01672 (2025).\n[101] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distil-\nBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108 (2019).\n[102] Weiyan Shi, Aiqi Cui, Evan Li, Ruoxi Jia, and Zhou Yu. 2021. Selective differential\nprivacy for language modeling. arXiv preprint arXiv:2108.12944 (2021).\n[103] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer\nSingh. 2020. Autoprompt: Eliciting knowledge from language models with\nautomatically generated prompts. arXiv preprint arXiv:2010.15980 (2020).\n[104] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017.\nMembership inference attacks against machine learning models. In 2017 IEEE\nsymposium on security and privacy (SP) . IEEE, 3‚Äì18.\n[105] Tanmay Singh, Harshvardhan Aditya, Vijay K Madisetti, and Arshdeep Bahga.\n2024. Whispered Tuning: Data Privacy Preservation in Fine-Tuning LLMs\nthrough Differential Privacy. Journal of Software Engineering and Applications\n17, 1 (2024), 1‚Äì22.\n[106] Victoria Smith, Ali Shahin Shamsabadi, Carolyn Ashurst, and Adrian Weller.\n2023. Identifying and mitigating privacy risks stemming from language models:\nA survey. arXiv preprint arXiv:2310.01424 (2023).\n[107] Robin Staab, Mark Vero, Mislav Balunovic, and Martin Vechev. 2024. Beyond\nMemorization: Violating Privacy via Inference with Large Language Models.\nIn The Twelfth International Conference on Learning Representations . https:\n//openreview.net/forum?id=kmn0BhQk7p\n[108] Robin Staab, Mark Vero, Mislav Balunoviƒá, and Martin Vechev. 2024. Large\nLanguage Models are Advanced Anonymizers. arXiv preprint arXiv:2402.13846\n(2024).\n[109] Lukas Struppek, Minh Hieu Le, Dominik Hintersdorf, and Kristian Kersting.\n2024. Exploring the Adversarial Capabilities of Large Language Models. arXiv\npreprint arXiv:2402.09132 (2024).\n[110] Nishant Subramani, Sasha Luccioni, Jesse Dodge, and Margaret Mitchell. 2023.\nDetecting personal information in training corpora: an analysis. In Proceedings\nof the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP\n2023). 208‚Äì220.\n[111] Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, and et al Chen.\n2024. Principle-driven self-alignment of language models from scratch with\nminimal human supervision. Advances in Neural Information Processing Systems\n36 (2024).\n[112] Jasper Tan, Blake Mason, Hamid Javadi, and Richard G. Baraniuk. 2022. Param-\neters or Privacy: A Provable Tradeoff Between Overparameterization and Mem-\nbership Inference. arXiv:2202.01243 [stat.ML] https://arxiv.org/abs/2202.01243\n[113] Xinyu Tang, Richard Shin, Huseyin A Inan, Andre Manoel, Fatemehsadat\nMireshghallah, Zinan Lin, Sivakanth Gopi, Janardhan Kulkarni, and Robert\nSim. 2023. Privacy-preserving in-context learning with differentially private\nfew-shot generation. arXiv preprint arXiv:2309.11765 (2023).\n[114] Yu Tian, Xiao Yang, Jingyuan Zhang, Yinpeng Dong, and Hang Su. 2023.\nEvil geniuses: Delving into the safety of llm-based agents. arXiv preprint\narXiv:2311.11855 (2023).\n[115] Minh N Vu, Truc Nguyen, Tre‚ÄôR Jeter, and My T Thai. 2024. Analysis of Privacy\nLeakage in Federated Large Language Models. arXiv preprint arXiv:2403.04784\n(2024).\n[116] Jeffrey G Wang, Jason Wang, Marvin Li, and Seth Neel. 2024. Pandora‚Äôs\nWhite-Box: Increased Training Data Leakage in Open LLMs. arXiv preprint\narXiv:2402.17012 (2024).\n[117] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2024. Jailbroken: How\ndoes llm safety training fail? Advances in Neural Information Processing Systems\n36 (2024).\n[118] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry\nGilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. 2023.\nA prompt pattern catalog to enhance prompt engineering with chatgpt. arXiv\npreprint arXiv:2302.11382 (2023).\n[119] Tong Wu, Ashwinee Panda, Jiachen T Wang, and Prateek Mittal. 2023. Privacy-\npreserving in-context learning for large language models. In The Twelfth Inter-\nnational Conference on Learning Representations .\n[120] Xiaodong Wu, Ran Duan, and Jianbing Ni. 2023. Unveiling security, privacy,\nand ethical concerns of chatgpt. Journal of Information and Intelligence (2023).\n[121] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming\nZhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023. The rise and potential\nof large language model based agents: A survey.arXiv preprint arXiv:2309.07864\n(2023).\n[122] Daliang Xu, Wangsong Yin, Xin Jin, Ying Zhang, Shiyun Wei, Mengwei Xu, and\nXuanzhe Liu. 2023. Llmcad: Fast and scalable on-device large language model\ninference. arXiv preprint arXiv:2309.04255 (2023).\n[123] Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You. 2024.\nTo repeat or not to repeat: Insights from scaling llm under token-crisis.Advances\nin Neural Information Processing Systems 36 (2024).\n[124] Biwei Yan, Kun Li, Minghui Xu, Yueyan Dong, Yue Zhang, Zhaochun Ren, and\nXiuzheng Cheng. 2024. On Protecting the Data Privacy of Large Language\nModels (LLMs): A Survey. arXiv preprint arXiv:2403.05156 (2024).\n[125] Haomiao Yang, Kunlan Xiang, Mengyu Ge, Hongwei Li, Rongxing Lu, and Shui\nYu. 2024. A comprehensive overview of backdoor attacks in large language\nmodels within communication networks. IEEE Network (2024).\n[126] Linyao Yang, Hongyang Chen, Zhao Li, Xiao Ding, and Xindong Wu. 2024.\nGive us the facts: Enhancing large language models with knowledge graphs\nfor fact-aware language modeling. IEEE Transactions on Knowledge and Data\nEngineering (2024).\n[127] Zhou Yang, Zhipeng Zhao, Chenyu Wang, Jieke Shi, Dongsun Kim, DongGyun\nHan, and David Lo. 2023. What do code models memorize? an empirical study\non large language models of code. arXiv preprint arXiv:2308.09932 (2023).\n[128] Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, and Yue Zhang.\n2024. A survey on large language model (llm) security and privacy: The good,\nthe bad, and the ugly. High-Confidence Computing (2024), 100211.\n[129] Yixiang Yao, Fei Wang, Srivatsan Ravi, and Muhao Chen. 2024. Privacy-\nPreserving Language Model Inference with Instance Obfuscation.arXiv preprint\narXiv:2402.08227 (2024).\n[130] Yuanshun Yao, Xiaojun Xu, and Yang Liu. 2023. Large language model unlearn-\ning. arXiv preprint arXiv:2310.10683 (2023).\n[131] Rui Ye, Wenhao Wang, Jingyi Chai, Dihan Li, Zexi Li, Yinda Xu, Yaxin Du,\nYanfeng Wang, and Siheng Chen. 2024. OpenFedLLM: Training Large Language\nModels on Decentralized Private Data via Federated Learning. arXiv preprint\narXiv:2402.06954 (2024).\nYashothara Shanmugarasa, Ming Ding, M.A.P. Chamikara, and Thierry Rakotoarivelo\n[132] Rongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shangguang Wang, and Mengwei\nXu. 2025. EdgeMoE: Empowering Sparse Large Language Models on Mobile\nDevices. IEEE Transactions on Mobile Computing (2025).\n[133] Keun Soo Yim. 2023. Privacy-friendly Personalization of LLM Responses Using\nHashed Entity Injection. Technical Disclosure Commons (2023).\n[134] Jin Yong Yoo and Yanjun Qi. 2021. Towards improving adversarial training of\nNLP models. arXiv preprint arXiv:2109.00544 (2021).\n[135] Tongxin Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang, Ruijie Zhao, Tian\nXia, Lizhen Xu, Binglin Zhou, Fangqi Li, Zhuosheng Zhang, et al . 2024. R-\nJudge: Benchmarking Safety Risk Awareness for LLM Agents. arXiv preprint\narXiv:2401.10019 (2024).\n[136] Ahtsham Zafar, Venkatesh Balavadhani Parthasarathy, Chan Le Van, Saad\nShahid, Arsalan Shahid, et al . 2023. Building trust in conversational ai: A\ncomprehensive review and solution architecture for explainable, privacy-aware\nsystems using llms and knowledge graph. arXiv preprint arXiv:2308.13534\n(2023).\n[137] Santiago Zanella-B√©guelin, Lukas Wutschitz, Shruti Tople, Victor R√ºhle, Andrew\nPaverd, Olga Ohrimenko, Boris K√∂pf, and Marc Brockschmidt. 2020. Analyzing\ninformation leakage of updates to natural language models. In Proceedings of\nthe 2020 ACM SIGSAC conference on computer and communications security .\n363‚Äì375.\n[138] Yifan Zeng, Yiran Wu, Xiao Zhang, Huazheng Wang, and Qingyun Wu. 2024. Au-\ntoDefense: Multi-Agent LLM Defense against Jailbreak Attacks. arXiv preprint\narXiv:2403.04783 (2024).\n[139] Jiawen Zhang, Kejia Chen, Zunlei Feng, Jian Lou, Mingli Song, Jian Liu, and\nXiaohu Yang. 2024. SecPE: Secure Prompt Ensembling for Private and Robust\nLarge Language Models. In ECAI.\n[140] Jinghan Zhang, Junteng Liu, Junxian He, et al . 2023. Composing parameter-\nefficient modules with arithmetic operation. Advances in Neural Information\nProcessing Systems 36 (2023), 12589‚Äì12610.\n[141] Kaiyan Zhang, Jianyu Wang, Ermo Hua, Biqing Qi, Ning Ding, and Bowen Zhou.\n2024. CoGenesis: A Framework Collaborating Large and Small Language Models\nfor Secure Context-Aware Instruction Following.arXiv preprint arXiv:2403.03129\n(2024).\n[142] Lexin Zhang, Changxiang Li, Qi Hu, Jingjing Lang, Sirui Huang, Linyue Hu,\nJingwen Leng, Qiuhan Chen, and Chunli Lv. 2023. Enhancing Privacy in Large\nLanguage Model with Homomorphic Encryption and Sparse Attention. Applied\nSciences 13, 24 (2023). https://doi.org/10.3390/app132413146\n[143] Ruisi Zhang, Seira Hidano, and Farinaz Koushanfar. 2022. Text revealer: Private\ntext reconstruction via model inversion attacks against transformers. arXiv\npreprint arXiv:2209.10505 (2022).\n[144] Xinyu Zhang, Huiyu Xu, Zhongjie Ba, Zhibo Wang, Yuan Hong, Jian Liu, Zhan\nQin, and Kui Ren. 2024. PrivacyAsst: Safeguarding User Privacy in Tool-Using\nLarge Language Model Agents. IEEE Transactions on Dependable and Secure\nComputing (2024).\n[145] Yao Zhang, Zijian Ma, Yunpu Ma, Zhen Han, Yu Wu, and Volker Tresp. 2024.\nWebPilot: A Versatile and Autonomous Multi-Agent System for Web Task\nExecution with Strategic Exploration. arXiv:2408.15978 [cs.AI] https://arxiv.\norg/abs/2408.15978\n[146] Zhiping Zhang, Michelle Jia, Bingsheng Yao, Sauvik Das, Ada Lerner, Dakuo\nWang, Tianshi Li, et al. 2023. \" It‚Äôs a Fair Game‚Äù, or Is It? Examining How Users\nNavigate Disclosure Risks and Benefits When Using LLM-Based Conversational\nAgents. arXiv preprint arXiv:2309.11653 (2023).\n[147] Zhexin Zhang, Jiaxin Wen, and Minlie Huang. 2023. Ethicist: Targeted training\ndata extraction through loss smoothed soft prompting and calibrated confidence\nestimation. arXiv preprint arXiv:2307.04401 (2023).\n[148] Zhexin Zhang, Junxiao Yang, Pei Ke, Fei Mi, Hongning Wang, and Minlie\nHuang. 2024. Defending Large Language Models Against Jailbreaking Attacks\nThrough Goal Prioritization. In Proceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers) . Association\nfor Computational Linguistics, Bangkok, Thailand, 8865‚Äì8887. https://doi.org/\n10.18653/v1/2024.acl-long.481\n[149] Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai,\nShuaiqiang Wang, Dawei Yin, and Mengnan Du. 2024. Explainability for large\nlanguage models: A survey. ACM Transactions on Intelligent Systems and Tech-\nnology 15, 2 (2024), 1‚Äì38.\n[150] Xuandong Zhao, Lei Li, and Yu-Xiang Wang. 2022. Provably confidential\nlanguage modelling. arXiv preprint arXiv:2205.01863 (2022).\n[151] Xin Zhou, Yi Lu, Ruotian Ma, Tao Gui, Yuran Wang, Yong Ding, Yibo Zhang,\nQi Zhang, and Xuan-Jing Huang. 2023. Textobfuscator: Making pre-trained\nlanguage model a privacy protector via obfuscating word representations. In\nFindings of the Association for Computational Linguistics: ACL 2023 . 5459‚Äì5473.\n[152] Ligeng Zhu, Zhijian Liu, and Song Han. 2019. Deep leakage from gradients.\nAdvances in neural information processing systems 32 (2019).\n6 APPENDICES\n6.1 Source Selection and Strategy\nWe conducted a literature review adhering to the Preferred Report-\ning Items for Systematic Reviews and Meta-Analyses (PRISMA)\nguidelines [83]. Our search spanned databases such as ACM, IEEE\nXplore, Springer, ScienceDirect, and Google Scholar, targeting re-\nsearch papers that explore various privacy challenges in LLM sys-\ntems published after 2022. The initial screening employed broad\nkeywords such as ‚Äòprivacy challenges in LLM OR ChatGPT, ‚Äô ‚Äòprivacy\nin LLM, ‚Äô ‚Äòprivacy attacks in LLM, ‚Äô ‚Äòresponsible LLM, ‚Äô and ‚Äòsecurity\nAND privacy challenges in LLM. ‚Äô We then refined our search strat-\negy by categorizing the identified challenges (such as ‚Äôprivacy in\nLLM agents‚Äô, ‚Äôprivacy in LLM prompts‚Äô, ‚Äòprivacy in LLM output\nOR decision‚Äô, and ‚Äòprivacy in LLM training data‚Äô) via screening the\nidentified challenges via initial search. Additionally, we used the\nmitigation methodologies found in the identified papers to search\nfor further relevant studies. Survey papers on training data privacy\nchallenges in LLMs were included as well.\nFigure 5 shows the PRISMA and snowball approach of the paper\nsearch and selection process. Our search yielded a total of 17,900\nrecords. We removed duplicates, screened titles and abstracts, and\napplied exclusion criteria, such as eliminating long-short repeti-\ntive papers, papers irrelevant to the focused area, security-related\npapers, those with no citations or published in non-A/A*/B con-\nferences, and technical papers focused on root techniques with\nmodifications. After this filtering, we were left with 101 records.\nWe also employed a snowballing technique by reviewing the bib-\nliographies of the identified papers to capture additional relevant\nstudies. Ultimately, our study comprised 116 papers categorized as\nfollows: Training data (n=51), Prompts (n=24), Outputs (n=8), LLM\nagents (n=24), Legal/copyright/bias (n=9), Survey papers (n=14),\ncopyright data (n=12), and Responsible LLM (n=7).\n# records identifiedthrough multiple keyword\ndb searches (n=17900)\n# records after duplicates\nremoved (n=12200)\n# records aftertitle and abstract\nscreenings and exclusionscenarios (n=113)\n# studies included inSoK\n(n =128) Training data (n=51)\nPrompts (n=24)\nOuputs (n=8)\nLLM agents (n=24)\nLegal / copyright / Bias\n(n=9)\nSurvey papers (n=14)\nCopyright data(n=12)\nResponsible LLM (n=7)\n# systematic reviewreference check\n\"snowballing\" (n=15)\n# records excluded\n(n=12087)\nFigure 5: PRISMA and Snowballing Approach for Paper\nSearching and Selection (# implies number of)\nSoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks, and Mitigation\n6.2 Glossary\nTable 1 lists the abbreviations used in the paper along with their\ndescriptions.\n6.3 Responsible Large Language Models\nConcerns and Privacy Implications\nThis section addresses responsible AI concerns in LLM systems,\nparticularly from user interactions and LLM systems‚Äô inherent\nblack-box nature. Responsible LLMs‚Äô concerns include ethical and\nlegal issues due to LLMs tendency to copy proprietary material,\ntransparency, reliability, explainability, accountability, fairness, and\nbias [52, 55].\nThe first challenge lies in the lack of transparency and interoper-\nability of LLM, with service providers managing user data without\nclear mechanisms for users to understand its use. Ensuring strin-\ngent data protection is crucial due to the rising number of data\nbreaches 5 and the absence of strict regulations. The black-box\nnature of LLMs complicates verifying data processes, heightening\nprivacy concerns.\nAnother concern within LLM systems is the perpetuation of\nunfair discrimination and representational harm through reinforc-\ning stereotypes and social biases [ 55]. The association of social\nidentities with LLM decisions often results in excluding specific\nsocial communities from the LLM‚Äôs outcomes. Furthermore, hate\nspeech generated by LLMs may incite violence and cause signif-\nicant offense. Despite efforts by many LLM service providers to\nremove such content from their training corpora, bias and unfair\ndiscrimination remain persistent issues in LLMs.\nNext, the potential for misinformation and disinformation is\nheightened by LLMs‚Äô ability to generate persuasive yet potentially\nfalse or misleading content. This capability can be exploited to\nspread misinformation, manipulate public opinion, or create fraud-\nulent materials, posing significant risks to public health, democracy,\nand social harmony [10].\nAccountability in the deployment and use of LLM systems presents\na critical issue, as it involves determining responsibility for the\nactions and outputs of these models [ 49]. When LLMs generate\nharmful or unethical content, such as misinformation or biased\nresponses, it can be challenging to pinpoint who is responsible‚Äîthe\ndevelopers, users, or the organizations deploying the models. This\nissue is compounded by the complexity and opacity of LLMs, which\ncan obscure the decision-making processes of the models. Estab-\nlishing precise accountability mechanisms, such as transparent\nreporting standards, ethical guidelines for development, and over-\nsight committees, is essential to ensure that all parties involved are\nheld responsible for the impacts of LLM technologies [11].\n6.3.1 Mitigation. Various approaches have been proposed to ad-\ndress responsible AI concerns, but they are still nascent for LLM\nsystems. Potential solutions include improving user-friendly pri-\nvacy policies and ethical data principles, conducting model au-\ndits, and using explainable AI techniques like Local Interpretable\nModel-agnostic Explanations (LIME) [ 93] and SHapley Additive\n5https://www.statista.com/statistics/290525/cyber-crime-biggest-online-data-\nbreaches-worldwide/\nexPlanations (SHAP) [72] to make LLM processes transparent. Fea-\nture visualization and activation maximization offer insights into a\nmodel‚Äôs learning process, though their application to LLMs with\nmillions of parameters is challenging. Additionally, it is crucial to\neducate users about the privacy implications of LLMs, their data\nusage policies, and the importance of protecting their privacy. Im-\nplementing mechanisms to quantify or explain privacy risks can\nfurther inform users of potential consequences. However, further\nwork is needed to evaluate the effectiveness of these solutions.\nIn conclusion, addressing these concerns requires a multi-faceted\napproach that encompasses technical strategies (such as algorithms\nfor detecting and mitigating biases), policy development (including\nprivacy protection regulations), ethical guidelines (like respecting\nintellectual property), and stakeholder engagement (with collabo-\nration among industry, academia, and regulatory bodies).\n6.4 Summary of Privacy Challenges and\nSolutions\nTable 2 summarizes the privacy challenges and the details of existing\nmitigation techniques proposed in the literature.\nYashothara Shanmugarasa, Ming Ding, M.A.P. Chamikara, and Thierry Rakotoarivelo\nTable 1: The Glossary Table\nAbbreviations Description\nArtificial Intelligence (AI) The science of making machines that can think like humans\nMachine Learning (ML) The capability of a machine to imitate intelligent human behavior\nLarge Language Model (LLM) Trained on vast datasets and characterized by extensive parameters to generate sensible responses\nGDPR General Data Protection Regulation\nHIPAA Health Insurance Portability and Accountability Act of 1996\nLLM systems user prompts Natural language instructions to describe the task and achieve the desired outcomes\nLLM agents A system endowed with intricate reasoning capabilities, enabling it to interact with other systems\nand perform actions\nPersonally Identifiable Information\n(PII)\nInformation that can be used to identify individuals\nResponsible AI (RAI) An approach to developing and deploying AI in an ethical, trustworthy, safe, and legal way\nFine-tuning in LLM The process of adjusting the parameters of a pre-trained large language model to a specific task\nor domain\nIn Context Learning (ICL) A method to adapt the LLM to a specific task by incorporating demonstrations into the prompt\nRetrieval Augmented Generation\n(RAG)\nA technique where an LLM retrieves relevant information from external sources to enhance its\ngenerated responses\nDifferential Privacy (DP) A formal mathematical framework for ensuring privacy in data analysis. It provides strong guar-\nantees that the inclusion or exclusion of any individual‚Äôs data in a dataset does not significantly\nimpact the output of a computation, thereby protecting individual privacy. It achieves this by\nintroducing controlled noise to statistical computations, making inferring private details about\nany single participant difficult. The strength of privacy protection is controlled by a parameter\ncalled epsilon (ùúñ)‚Äîa lower ùúñ provides stronger privacy but may reduce accuracy.\nKnowledge unlearning Knowledge unlearning is the process of selectively removing specific information from a trained\nmodel while preserving overall performance, which is crucial for privacy protection and regu-\nlatory compliance. Common approaches include exact unlearning (retraining without specific\ndata), influence-based methods, and fine-tuning to mitigate privacy risks and data poisoning.\nFederated Learning (FL) A machine learning approach where models are trained across multiple decentralized devices or\nservers, preserving data privacy by keeping data local\nMIA Membership Inference Attack: The objective of a MIA is to determine whether a specific data\npoint ùë• (a sentence or document in LLM context) was included in the training dataset Dof a\nmodel ùëÄ. This is achieved by computing a membership score ùëì (ùë•; ùëÄ), which is then compared\nagainst a predefined threshold to decide whether the target sample was part of the training data\nby analyzing its output [104].\nAttribute Inference Attack In an attribute inference attack, the adversary aims to predict sensitive attributes of an individual\nwhose data was used to train a target model, given that the adversary already knows other\nattributes of that individual [38].\nJailbreak attacks A jailbreak attack is an adversarial technique used to bypass the safety restrictions and ethical\nguardrails of LLMs. These attacks manipulate the model into generating responses that it is\ndesigned to avoid, such as harmful, biased, or restricted content. Jailbreak attacks exploit tech-\nniques like prompt engineering (deceptive inputs), role-playing (bypassing safeguards through\nscenarios), encoding tricks (obfuscation or special characters), and multi-turn exploits (gradual\nmanipulation) to evade AI restrictions.\nRLHF Reinforcement Learning from Human Feedback\nIntellectual Property (IP) It encompasses creations of the mind, including inventions and works of art, protected by law\nthrough patents, copyrights, trademarks, and trade secrets\nAPI Application Programming Interface\nNER Named Entity Recognition: This involves identifying and classifying named entities in a text\ninto predefined categories, such as the names of persons, organizations, locations, dates, times,\nand other entities.\nSoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks, and Mitigation\nTable 2: Summary of Privacy Challenges in LLM Systems and Solutions. Experimented on ‚ÄòOpen/closed source LLM system‚Äô: =\nClosed-source LLM systems, = Open source LLM systems, = Both. ‚ÄòThreat model‚Äô: = Trusting LLM service provider and\nconsider threats from external parties only, = Threat from both service provider and external parties. (This table continues\non the next page.)\nPrivacy\nChal-\nlenges\nSpecific Issues Solution Technical description Tested on\nOpen/closed\nsource LLM\nThreat\nmodel\nIn LLM\nTraining\ndata\nData mem-\norization\nand\npersonal\ndata\nleakage\nData deduplication Fuzzy deduplication techniques [13]\nDetection and Filtering Bloom filter [45], Filtering with restrictive terms\n[91], PII scrubbing filters and NER [23], Output\nfiltering [105]\nData cleaning Correcting errors and inconsistencies, implement-\ning anonymization, data minimization, and se-\ncure practices to protect sensitive information\n[13, 124]\nDifferential privacy Adding noise to data when training[ 12, 15, 66,\n105, 137, 150]\nKnowlege unlearning Force models to forget specific knowledge with-\nout requiring full retraining [33, 130, 140]\nPrivacy\nattacks\nModel stacking /\nDropout\nModel stacking: combines multiple base\nmodels[27], Dropout: regularization technique\nused in neural networks to prevent overfitting\n[123]\nTest time defense and\ninstruction processing\nFilters malicious inputs, detects abnormal queries,\nand post-processes LLM-generated output. [ 64,\n96]\nAdversarial regulariza-\ntions\nLLM training process employs robust optimiza-\ntion methods like adversarial training [134] and\nrobust fine-tuning [26] to prevent malicious text\nattacks\nPrompt-level ap-\nproaches\nFiltering adversarial prompts using rule-based\ndetection or classifier-based approaches [148]\nModel-level approaches Enhancing LLMs through safety training, RLHF,\nand adversarial fine-tuning [6, 48]\nIn\nPrompts\nDirect\nleakage\nInput validation and\nsanitization\nNER and encryption [68]\nNER and Substitution/masking [17]\nNER and obfuscation [73]\nLocal small LM and re-\nmote LLM combination\nUse NER to remove sensitive information be-\nfore sending data to remote LLM, and add again\nlocally[43]\nText obfuscation Obscure original word information while retain-\ning original word functionality [151]\nCryptographic ap-\nproach: Fully harmonic\nencryption\nEncrypting data in such a way that computations\ncan be performed on the encrypted data without\nneeding to decrypt it first [16, 42, 100, 139]\nInference of sen-\nsitive\nLocal small LM and re-\nmote LLM combination\nAdversarial feedback-guided approach using\nLLM [108]\nContextual\ninformation\nleakage\nHash operation Transmitting only hashed values of sensitive data\nand service providers can revert back [133]\n-\nPrompt ensembling Using multiple different prompts to hide actual\nprompt of the user[28]\nLocal small LM and re-\nmote LLM combination\nRemote LLMs provide high-level guidance and\nenhance the result in local LM with contextual\ninformation [141]\nYashothara Shanmugarasa, Ming Ding, M.A.P. Chamikara, and Thierry Rakotoarivelo\nIn LLM-\ngenerated\nOutputs\nRevealing\nSensitive\nInformation\nText obfuscation Appending an another random text to the original\nprompt [129]\n-\nDifferentially private\naggregation\nAggregate and release responses without relying\non any individual outputs [119]\nSynthetic few-shot demonstrations [113]\nTransparency in\ndecision making\nExplainable AI tech-\nniques\nFeature importance analysis[72], interactive ex-\nploration tools [82], and human-AI interpretation\n- -\nIn LLM\nAgents\nAmbiguities\nin automated\ntask\nexecution\nLLM sandbox LLM powered emulator and safety evaluator [77,\n99], Complex multi-turn interactions [135]\nInjecting safety knowl-\nedge in different stages\nSafety knowledge injection in pre-planning, in-\nplanning, and post-planning [44]\nSelf-alignment Self-alignment of LLMs through multi-agent role-\nplaying [84]\nAdversarial\ninteractions\nof agents\nMulti-agent debate\nmechanism\nSelf-evaluate through discussion and feedback\nMitigation measures\nbased on AI and secu-\nrity principles\nMonitoring agents‚Äô communication content [76]\nExposure of\nSensitive\nInformation\nto External\nTools\nEncryption and\nshuffling-based solu-\ntions\nOperate on encrypted inputs and attribute-based\nforgery generative models with shuffling mecha-\nnism [144]\nAccess restriction meth-\nods\nLimits access to necessary data based on user\nprivacy preferences [5]",
  "topic": "Internet privacy",
  "concepts": [
    {
      "name": "Internet privacy",
      "score": 0.6982667446136475
    },
    {
      "name": "Information privacy",
      "score": 0.6547781229019165
    },
    {
      "name": "Computer science",
      "score": 0.5703072547912598
    },
    {
      "name": "Privacy software",
      "score": 0.547021746635437
    },
    {
      "name": "Computer security",
      "score": 0.49370989203453064
    },
    {
      "name": "Privacy protection",
      "score": 0.44181931018829346
    }
  ]
}