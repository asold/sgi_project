{
  "title": "Gradient-based Adversarial Attacks against Text Transformers",
  "url": "https://openalex.org/W3158360872",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2106979855",
      "name": "Chuan Guo",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2526820065",
      "name": "Alexandre Sablayrolles",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A1140831031",
      "name": "Hervé Jégou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A302325417",
      "name": "Douwe Kiela",
      "affiliations": [
        "Meta (Israel)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2547875792",
    "https://openalex.org/W2966149470",
    "https://openalex.org/W3035736465",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2962818281",
    "https://openalex.org/W2949128310",
    "https://openalex.org/W2799194071",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2798966449",
    "https://openalex.org/W3025408396",
    "https://openalex.org/W2964153729",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3101449015",
    "https://openalex.org/W4293846201",
    "https://openalex.org/W9657784",
    "https://openalex.org/W2962718684",
    "https://openalex.org/W2996851481",
    "https://openalex.org/W3104423855",
    "https://openalex.org/W2964253222",
    "https://openalex.org/W2950734153",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1673923490",
    "https://openalex.org/W2735135478",
    "https://openalex.org/W2962700793",
    "https://openalex.org/W2963857521",
    "https://openalex.org/W2803831897",
    "https://openalex.org/W3023768479",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W3117433489",
    "https://openalex.org/W2607892599",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2964301649",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2964283260",
    "https://openalex.org/W3169948074",
    "https://openalex.org/W3104208618",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W3086385389",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W3044324512",
    "https://openalex.org/W2794557536",
    "https://openalex.org/W3169306793",
    "https://openalex.org/W2963859254",
    "https://openalex.org/W2950133940"
  ],
  "abstract": "We propose the first general-purpose gradient-based adversarial attack against transformer models. Instead of searching for a single adversarial example, we search for a distribution of adversarial examples parameterized by a continuous-valued matrix, hence enabling gradient-based optimization. We empirically demonstrate that our white-box attack attains state-of-the-art attack performance on a variety of natural language tasks, outperforming prior work in terms of adversarial success rate with matching imperceptibility as per automated and human evaluation. Furthermore, we show that a powerful black-box transfer attack, enabled by sampling from the adversarial distribution, matches or exceeds existing methods, while only requiring hard-label outputs.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5747–5757\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n5747\nGradient-based Adversarial Attacks against Text Transformers\nChuan Guo∗ Alexandre Sablayrolles∗ Hervé Jégou Douwe Kiela\nFacebook AI Research\nAbstract\nWe propose the ﬁrst general-purpose gradient-\nbased adversarial attack against transformer\nmodels. Instead of searching for a single ad-\nversarial example, we search for a distribu-\ntion of adversarial examples parameterized by\na continuous-valued matrix, hence enabling\ngradient-based optimization. We empirically\ndemonstrate that our white-box attack attains\nstate-of-the-art attack performance on a variety\nof natural language tasks, outperforming prior\nwork in terms of adversarial success rate with\nmatching imperceptibility as per automated\nand human evaluation. Furthermore, we show\nthat a powerful black-box transfer attack, en-\nabled by sampling from the adversarial distri-\nbution, matches or exceeds existing methods,\nwhile only requiring hard-label outputs.\n1 Introduction\nDeep neural networks are sensitive to small, often\nimperceptible changes in the input, as evidenced\nby the existence of so-called adversarial exam-\nples (Biggio et al., 2013; Szegedy et al., 2013).\nThe dominant method for constructing adversarial\nexamples deﬁnes an adversarial loss, which en-\ncourages prediction error, and then minimizes the\nadversarial loss over the input space with estab-\nlished optimization techniques. To ensure that the\nperturbation is hard to detect by humans, existing\nmethods also introduce a perceptibility constraint\ninto the optimization problem. Variants of this\ngeneral strategy have been successfully applied to\nimage and speech data (Madry et al., 2017; Carlini\nand Wagner, 2017, 2018).\nHowever, optimization-based search strategies\nfor obtaining adversarial examples are much more\nchallenging with text data. Attacks against contin-\nuous data types such as image and speech utilize\ngradient descent for superior efﬁciency, but the dis-\ncrete nature of natural languages prohibits such\n∗ ∗Equal contribution.\nﬁrst-order techniques. In addition, perceptibility\nfor continuous data can be approximated with L2-\nand L∞-norms, but such metrics are not readily ap-\nplicable to text data. To circumvent this issue, some\nexisting attack approaches have opted for heuris-\ntic word replacement strategies and optimizing by\ngreedy or beam search using black-box queries (Jin\net al., 2020; Li et al., 2020a,b; Garg and Ramakr-\nishnan, 2020). Such heuristic strategies typically\nintroduce unnatural changes that are grammatically\nor semantically incorrect (Morris et al., 2020a).\nIn this paper, we propose a general-purpose\nframework for gradient-based adversarial attacks,\nand apply it against transformer models on text\ndata. Our framework, GBDA (Gradient-based Dis-\ntributional Attack), consists of two key components\nthat circumvent the difﬁculties of gradient descent\nfor discrete data under perceptibility constraints.\nFirst, instead of constructing a single adversarial\nexample, we search for an adversarial distribu-\ntion. We instantiate examples with the Gumbel-\nsoftmax distribution (Jang et al., 2016), param-\neterized by a continuous-valued matrix of coef-\nﬁcients that we optimize with a vanilla gradient-\nbased method. Second, we enforce perceptibility\nand ﬂuency using BERTScore (Zhang et al., 2019)\nand language model perplexity, respectively, both\nof which are differentiable and can be added to the\nobjective function as soft constraints. The combi-\nnation of these two components enables powerful,\nefﬁcient, gradient-based text adversarial attacks.\nWe empirically demonstrate the efﬁcacy of\nGBDA against several transformer models. In ad-\ndition, we also evaluate under the transfer-based\nblack-box threat model by sampling from the opti-\nmized adversarial distribution and querying against\na different, potentially unknown target model. On\na variety of tasks including news/article categoriza-\ntion, sentiment analysis, and natural language in-\nference, our method achieves state-of-the-art attack\nsuccess rate, while preserving ﬂuency, grammatical\n5748\nblue berries\nred fruits\n0.72 0.89\n0.09 0.02\ntree\nplant\n0.89\n0.02 Target model\nLanguage model\nBERT Score\nGumbel samples\n(training)\nHard samples\n(attack)\nred berriestree\nAdversarial loss\nFluency constraint\nSimilarity constraint\nFigure 1: Overview of our attack framework. The parameter matrix Θ is used to sample a sequence of probability\nvectors ˜π1,..., ˜πn, which is forwarded through three (not necessarily distinct) models: (i) the target model for\ncomputing the adversarial loss, (ii) the language model for the ﬂuency constraint, and (iii) the BERTScore model\nfor the semantic similarity constraint. Due to the differentiable nature of each loss component and of the Gumbel-\nsoftmax distribution, our framework is fully differentiable, hence enabling gradient-based optimization.\ncorrectness, and a high level of semantic similarity\nto the original input.\nIn summary, the main contributions of our paper\nare as follows:\n1. We deﬁne a parameterized adversarial distribu-\ntion and optimize it using gradient-based methods.\nIn contrast, most prior work construct a single ad-\nversarial example using black-box search.\n2. By incorporating differentiable ﬂuency and se-\nmantic similarity constraints into the adversarial\nloss, our white-box attack produces more natural\nadversarial texts while achieving a new state-of-\nthe-art success rate.\n3. The adversarial distribution can be sampled efﬁ-\nciently to query different target models in a black-\nbox setting. This enables a powerful transfer attack\nthat matches or exceeds the performance of exist-\ning attacks. Compared to prior work that operate on\ncontinuous-valued outputs from the target model,\nthis transfer attack only requires hard labels.\n2 Background\nAdversarial examples constitute a class of robust-\nness attacks against neural networks. Let h: X→\nYbe a classiﬁer where X,Yare the input and out-\nput domains, respectively. Suppose that x ∈X is\na test input that the model correctly predicts as the\nlabel y = h(x) ∈Y. An (untargeted) adversarial\nexample is a sample x′∈X such that h(x′) ̸= y\nbut x′and x are imperceptibly close.\nThe notion of perceptibility is introduced so that\nx′preserves the semantic meaning of x for a hu-\nman observer. At a high level, x′constitutes an\nattack on the model’s robustness if a typical hu-\nman would not misclassify x′but the model hdoes.\nFor image data, since the input domain Xis a sub-\nset of the Euclidean space Rd, a common surro-\ngate for perceptibility is a distance metric such as\nthe Euclidean distance or the Chebyshev distance.\nIn general, one can deﬁne a perceptibility metric\nρ : X×X→ R≥0 and a threshold ϵ >0 so that\nx′is considered imperceptible to x if ρ(x,x′) ≤ϵ.\nSearch problem formulation. The process of\nﬁnding an adversarial example is typically mod-\neled as an optimization problem. For classiﬁcation,\nthe model h outputs a logit vector φh(x) ∈RK\nsuch that y= arg maxk φh(x)k. To encourage the\nmodel to misclassify an input, one can deﬁne an\nadversarial loss such as the margin loss:\nℓmargin(x,y; h) =\nmax\n(\nφh(x)y −max\nk̸=y\nφh(x)k + κ,0\n)\n, (1)\nso that the model misclassiﬁes x by a margin of\nκ >0 when the loss is 0. The margin loss has\nbeen widely used in attack algorithms for image\ndata (Carlini and Wagner, 2017).\nGiven an adversarial loss ℓ, the process of con-\nstructing an adversarial example can be cast as a\nconstrained optimization problem:\nmin\nx′∈X\nℓ(x′,y; h) subject to ρ(x,x′) ≤ϵ. (2)\nAn alternative formulation is to relax the constraint\ninto a soft constraint with λ> 0:\nmin\nx′∈X\nℓ(x′,y; h) + λ·ρ(x,x′), (3)\nwhich can then be solved using gradient-based opti-\nmizers if the constraint function ρis differentiable.\n5749\n2.1 Text Adversarial Examples\nAlthough the search problem formulation in Equa-\ntion 2 has been widely applied to continuous data\nsuch as image and speech, it does not directly apply\nto text data because (1) the data spaceXis discrete,\nhence not permitting gradient-based optimization;\nand (2) the constraint function ρis difﬁcult to de-\nﬁne for text data. In fact, both issues arise when\nconsidering attacks against any discrete input do-\nmain, but the latter is especially relevant for text\ndata due to the sensitivity of natural language. For\ninstance, inserting the word not into a sentence can\nnegate the meaning of the whole sentence despite\nhaving a token-level edit distance of 1.\nPrior work. Several attack algorithms have been\nproposed to circumvent these two issues, using\na multitude of approaches. For attacks that op-\nerate on the character level, perceptibility can\nbe approximated by the number of character ed-\nits, i.e., replacements, swaps, insertions and dele-\ntions (Ebrahimi et al., 2017; Li et al., 2018; Gao\net al., 2018). Attacks that operate on the word\nlevel adopt heuristics such as synonym substitu-\ntion (Samanta and Mehta, 2017; Zang et al., 2020;\nMaheshwary et al., 2020) or replacing words by\nones with similar word embeddings (Alzantot et al.,\n2018; Ren et al., 2019; Jin et al., 2020). More re-\ncent attacks have also leveraged masked language\nmodels such as BERT (Devlin et al., 2019) to\ngenerate word substitutions by replacing masked\ntokens (Garg and Ramakrishnan, 2020; Li et al.,\n2020a,b). Most of the aforementioned attacks fol-\nlow the common recipe of proposing character-\nlevel or word-level perturbations to generate a con-\nstrained candidate set and optimizing the adversar-\nial loss greedily or using beam search.\nShortcomings in prior work. Despite the\nplethora of attacks against natural language mod-\nels, their efﬁcacy remains subpar compared to at-\ntacks against other data modalities. Both character-\nlevel and word-level changes are still relatively\ndetectable, especially as such changes often intro-\nduce misspellings, grammatical errors, and other\nartifacts of unnaturalness in the perturbed text (Mor-\nris et al., 2020a). Moreover, prior attacks mostly\nquery the target model has a black-box and rely on\nzeroth-order strategies for minimizing the adversar-\nial loss, resulting in sub-optimal performance.\nFor instance, BERT-Attack (Li et al., 2020b)—\narguably the state-of-the-art attack against BERT—\nonly reduces the test accuracy of the target model\non the AG News dataset (Zhang et al., 2015) from\n95.1 to 10.6. In comparison, attacks against im-\nage models can consistently reduce the model’s\naccuracy to 0 on almost all computer vision\ntasks (Akhtar and Mian, 2018). This gap in per-\nformance raises the question of whether gradient-\nbased search can produce more ﬂuent and optimal\nadversarial examples on text data. In this work, we\nshow that our gradient-based attack can reduce the\nsame model’s accuracy from95.1 to 3.5 while be-\ning more semantically-faithful to the original text.\nOur result shows that using gradient-based search\nfor text adversarial examples can indeed close the\nperformance gap between vision and text attacks.\n2.2 Other Attacks\nWhile most works on adversarial attack on text fall\nwithin the formulation deﬁned at the beginning of\nsection 2, other notions of adversarial perturbation\nexist as well. One class of such attacks is known\nas universal adversarial triggers—a short snippet\nof text that when appended to any input, causes the\nmodel to misclassify (Wallace et al., 2019; Song\net al., 2020). However, such triggers often contain\nunnatural combinations of words or tokens, and\nhence are very perceptible to a human observer.\nOur work falls within the general area of adver-\nsarial learning, and many prior works in this area\nhave explored the notion of adversarial example\non different data modalities. Although the most\nprominent data modality by far is image, adversar-\nial examples can be constructed for speech (Carlini\nand Wagner, 2018) and graphs (Dai et al., 2018;\nZügner et al., 2018) as well.\n3 GBDA: Gradient-based Distributional\nAttack\nIn this section, we detail GBDA—our general-\npurpose framework for gradient-based text attacks\nagainst transformers. Our framework leverages\ntwo important insights: (1) we deﬁne a parameter-\nized adversarial distribution that enables gradient-\nbased search using the Gumbel-softmax (Jang et al.,\n2016); and (2) we promote ﬂuency and semantic\nfaithfulness of the perturbed text using soft con-\nstraints on both perplexity and semantic similarity.\n3.1 Adversarial Distribution\nLet z = z1z2 ···zn be a sequence of tokens where\neach zi ∈V is a token from a ﬁxed vocabulary\n5750\nV= {1,...,V }. Consider a distribution PΘ pa-\nrameterized by Θ ∈Rn×V , which yields samples\nz ∼PΘ by independently sampling each token\nzi ∼Categorical(πi), where πi = Softmax(Θi) is\na vector of token probabilities for the i-th token.\nWe aim to optimize the parameter matrix Θ so\nthat samples z ∼PΘ are adversarial examples for\nthe model h. To do so, we deﬁne the objective\nfunction for this goal as:\nmin\nΘ∈Rn×V\nEz∼PΘℓ(z,y; h), (4)\nwhere ℓis a chosen adversarial loss.\nExtension to probability vector inputs. The ob-\njective function in Equation 4 is non-differentiable\ndue to the discrete nature of the categorical distribu-\ntion. Instead, we propose a relaxation of Equation 4\nby ﬁrst extending the model hto take probability\nvectors as input, and then use the Gumbel-softmax\napproximation (Jang et al., 2016) of the categorical\ndistribution to derive the gradient.\nTransformer models take as input a sequence\nof tokens that are converted to embedding vectors\nusing a lookup table. Let e(·) be the embedding\nfunction so that the input embedding for the token\nzi is e(zi) ∈Rd for some embedding dimension\nd. Given a probability vector πi that speciﬁes the\nsampling probability of the token zi, we deﬁne\ne(πi) =\nV∑\nj=1\n(πi)je(j) (5)\nas the embedding vector corresponding to the prob-\nability vector πi. Note that if πi is a one-hot vec-\ntor corresponding to the token zi then e(πi) =\ne(zi). We extend the notation for an input se-\nquence of probability vectors π = π1 ···πn as\ne(π) = e(π1) ···e(πn) by concatenating the in-\nput embeddings.\nComputing gradients using Gumbel-softmax.\nExtending the model h to take probability vec-\ntors as input allows us to leverage the Gumbel-\nsoftmax approximation to derive smooth estimates\nof the gradient of Equation 4. Samples ˜π =\n˜π1 ··· ˜πn from the Gumbel-softmax distribution\n˜PΘ are drawn according to the process:\n(˜πi)j := exp((Θi,j + gi,j)/T)∑V\nv=1 exp((Θi,v + gi,v)/T)\n, (6)\nwhere gi,j ∼Gumbel(0,1) and T > 0 is a tem-\nperature parameter that controls the smoothness\nof the Gumbel-softmax distribution. As T →0,\nthis distribution converges towards the distribution\nCategorical(Softmax(Θi)).\nWe can now optimize Θ using gradient descent\nby deﬁning a smooth approximation of the objec-\ntive function in Equation 4:\nmin\nΘ∈Rn×V\nE˜π∼˜PΘ\nℓ(e(˜π),y; h), (7)\nThe expectation can be estimated using stochastic\nsamples of ˜π ∼ ˜PΘ.\n3.2 Soft Constraints\nBlack-box attacks based on heuristic replacements\ncan only constrain the perturbation by proposing\nchanges that fall within the constraint set, e.g., lim-\niting edit distance, replacing words by ones with\nhigh word embedding similarity, etc. In contrast,\nour adversarial distribution formulation can readily\nincorporate any differentiable constraint function\nas a part of the objective. We leverage this advan-\ntage to include both ﬂuency and semantic similar-\nity constraints in order to produce more ﬂuent and\nsemantically-faithful adversarial texts.\nFluency constraint with a language model.\nCausal language models (CLMs) are trained with\nthe objective of next token prediction by maximiz-\ning the likelihood given previous tokens. This al-\nlows the computation of likelihoods for any se-\nquence of tokens. More speciﬁcally, given a CLM\ng with log-probability outputs, the negative log-\nlikelihood (NLL) of a sequence z = z1 ···zn is\ngiven autoregressively by:\nNLLg(z) = −\nn∑\ni=1\nlog pg(zi |z1 ···zi−1),\nwhere log pg(zi |z1 ···zi−1) = g(z1 ···zi−1)zi\nis the cross-entropy between the delta distribution\non token zi and the predicted token distribution\ng(z1 ···zi−1) for i= 1,...,n .\nWe extend the deﬁnition of NLL to the setting\nwhere inputs are vectors of token probabilities by:\nNLLg(π) := −\nn∑\ni=1\nlog pg(πi |π1 ···πi−1)\n= −\nn∑\ni=1\nV∑\nj=1\n(πi)jg(e(π1) ···e(πi−1))j,\nwith log pg(πi | π1 ···πi−1) being the cross-\nentropy between the next token distribution\n5751\nπi and the predicted next token distribution\ng(e(π1) ···e(πi−1)). This extension coincides\nwith the NLL for a token sequence x when each πi\nis a delta distribution for the token xi.\nSimilarity constraint with BERTScore. Prior\nwork on word-level attacks often used context-\nfree embeddings such as word2vec (Mikolov et al.,\n2013) and GloVe (Pennington et al., 2014) or syn-\nonym substitution to constrain semantic similarity\nbetween the original and perturbed text (Alzantot\net al., 2018; Ren et al., 2019; Jin et al., 2020). These\nconstraints tend to produce out-of-context and un-\nnatural changes that alter the semantic meaning of\nthe perturbed text (Garg and Ramakrishnan, 2020).\nInstead, we propose to use BERTScore (Zhang\net al., 2019), a similarity score for evaluating text\ngeneration that captures the semantic similarity be-\ntween pairwise tokens in contextualized embed-\ndings of a transformer model.\nLet x = x1 ···xn and z = z1 ···zm be two\ntoken sequences and let g be a language model\nthat produces contextualized embeddings φ(x) =\n(u1,..., un) and φ(z) = ( v1,..., vm). The (re-\ncall) BERTScore between x and z is deﬁned as:\nRBERT(x,z) =\nn∑\ni=1\nwi max\nj=1,...,m\nu⊤\ni vj, (8)\nwhere wi := idf(xi)/∑n\ni=1 idf(xi) is the normal-\nized inverse document frequency of the token xi\ncomputed across a corpus of data. We can readily\nsubstitute z with a sequence of probability vectors\nπ = π1 ···πm as described in Equation 5 and use\nρg(x,π) = 1 −RBERT(x,π) as a differentiable\nsoft constraint.\nObjective function. We combine all the compo-\nnents in the previous sections into a ﬁnal objec-\ntive for gradient-based optimization. Our objective\nfunction uses the margin loss ( cf. Equation 1) as\nthe adversarial loss, and integrates the ﬂuency con-\nstraint with a causal language model g and the\nBERTScore similarity constraint using contextual-\nized embeddings of g:\nL(Θ) =E˜π∼˜PΘ\nℓ(e(˜π),y; h)\n+ λlm NLLg(˜π) + λsim ρg(x,˜π), (9)\nwhere λlm,λsim >0 are hyperparameters that con-\ntrol the strength of the soft constraints. We min-\nimize L(Θ) stochastically using Adam (Kingma\nand Ba, 2014) by sampling a batch of inputs from\n˜PΘ at every iteration.\n3.3 Sampling Adversarial Texts\nOnce Θ has been optimized, we can sample from\nthe adversarial distribution PΘ to construct adver-\nsarial examples. Since the loss function L(Θ) that\nwe optimize is an approximation of the objective\nin Equation 4, it is possible that some samples are\nnot adversarial even when L(Θ) is successfully\nminimized. Hence, in practice, we draw multiple\nsamples z ∼PΘ and stop sampling either when the\nmodel misclassiﬁes the sample or when we reach a\nmaximum number of samples.\nNote that this stage could technically allow us to\nadd hard constraints to the examples we generate,\ne.g., manually ﬁlter out adversarial examples that\ndo not seem natural. In our case, we do not add\nany extra hard constraint and only verify that the\ngenerated example is misclassiﬁed by the model.\nTransfer to other models. Since drawing from\nthe distribution PΘ could potentially generate an\ninﬁnite stream of adversarial examples, we can\nleverage these generated samples to query a target\nmodel that is different from h. This constitutes a\nblack-box transfer attack from the source model h.\nMoreover, our transfer attack does not require the\ntarget model to output continuous-valued scores,\nwhich most existing black-box attacks against trans-\nformers rely on (Jin et al., 2020; Garg and Ramakr-\nishnan, 2020; Li et al., 2020a,b). We demonstrate\nin subsection 4.2 that this transfer attack enabled\nby the adversarial distribution PΘ is very effective\nat attacking a variety of target models.\n4 Experiments\nIn this section, we empirically validate our attack\nframework on a benchmark suite of natural lan-\nguage tasks. Code to reproduce our results is open\nsourced on GitHub1.\n4.1 Setup\nTasks. We evaluate on several benchmark text\nclassiﬁcation datasets, including DBPedia (Zhang\net al., 2015) and AG News (Zhang et al., 2015) for\narticle/news categorization, Yelp Reviews(Zhang\net al., 2015) and IMDB (Maas et al., 2011) for bi-\nnary sentiment classiﬁcation, and MNLI (Williams\net al., 2017) for natural language inference.\nThe MNLI dataset contains two evaluation sets:\n1https://github.com/facebookresearch/\ntext-adversarial-attack\n5752\nGPT-2 XLM(en-de) BERT\nTask Clean Acc. Adv. Acc. Cosine Sim.Clean Acc. Adv. Acc. Cosine Sim.Clean Acc. Adv. Acc. Cosine Sim.\nDBPedia 99.2 5.2 0.91 99.1 7.6 0.80 99.2 7.1 0.80\nAG News 94.8 6.6 0.90 94.4 5.4 0.87 95.1 3.5 0.80\nYelp 97.8 2.9 0.94 96.3 3.4 0.93 97.3 4.4 0.94\nIMDB 93.8 7.6 0.98 87.6 0.1 0.97 93.0 1.8 0.96\nMNLI(m.) 81.7 2.8/11.0 0.82/0.88 76.9 5.4/13.1 0.84/0.86 84.6 5.5/11.3 0.82/0.86\nMNLI(mm.) 82.5 4.2/13.5 0.85/0.88 76.3 4.1/10.6 0.85/0.86 84.5 4.7/11.8 0.80/0.87\nTable 1: Result of white-box attack against three transformer models: GPT-2, XLM (en-de), and BERT. For MNLI,\nthe pair of numbers correspond to result for attacking the hypothesis/premise portions of the text. Our attack is\nable to reduce the target model’s accuracy to below 10% in almost all cases, while maintaining a high level of\nsemantic similarity (cosine similarity of higher than 0.8 using USE embeddings).\nTask Clean Acc. Attack Alg. Adv. Acc. # Queries Cosine Sim.\nAG News 95.1\nGBDA (ours)8.8 107 0.69BERT-Attack 10.6 213 0.63BAE 13.0 419 0.75TextFooler 12.6 357 0.57\nYelp 97.3\nGBDA (ours)2.6 43 0.83BERT-Attack 5.1 273 0.77BAE 12.0 434 0.90TextFooler 6.6 743 0.74\nIMDB 93.0\nGBDA (ours)8.5 116 0.92BERT-Attack 11.4 454 0.86BAE 24.0 592 0.95TextFooler 13.6 1134 0.86\nMNLI(m.) 84.6\nGBDA (ours)2.3/10.8 37/133 0.75/0.79BERT-Attack 7.9/11.919/44 0.55/0.68BAE 25.4/36.2 68/1200.88/0.88TextFooler 9.6/25.3 78/152 0.57/0.65\nMNLI(mm.) 84.5\nGBDA (ours)1.8/13.4 30/159 0.76/0.80BERT-Attack 7/13.724/43 0.53/0.69BAE 19.2/30.3 75/1100.88/0.88TextFooler 8.3/22.9 86/162 0.58/0.65\nTable 2: Evaluation of black-box model transfer attack\nfrom GPT-2 to ﬁnetuned BERT classiﬁers. Unlike the\nbaseline methods, our transfer attack does not require\ncontinuous-valued model outputs. See text for details.\nmatched (m.) and mismatched (mm.), correspond-\ning to whether the test domain is matched or mis-\nmatched with the training distribution.\nModels. We attack three transformer architec-\ntures with our gradient-based white-box attack:\nGPT-2 (Radford et al., 2019), XLM (Lample and\nConneau, 2019) (using the en-de cross-lingual\nmodel), and BERT (Devlin et al., 2019). For BERT,\nwe use ﬁnetuned models from TextAttack (Mor-\nris et al., 2020b) for all tasks except for DBPedia,\nwhere ﬁnetuned models are unavailable. For BERT\non DBPedia and GPT-2/XLM on all tasks, we ﬁne-\ntune a pretrained model to serve as the target model.\nThe soft constraints described in subsection 3.2\nutilizes a CLM g with the same tokenizer as the\ntarget model. For GPT-2 we use the pre-trained\nGPT-2 model without ﬁnetuning asg, and for XLM\nwe use the checkpoint obtained after ﬁnetuning\nusing the CLM objective. For masked language\nmodels such as BERT (Devlin et al., 2019), we\ntrain a causal language model gon WikiText-103\nusing the same tokenizer as the target model.\nBaselines. We compare against several recent at-\ntacks on text transformers: TextFooler (Jin et al.,\n2020), BAE (Garg and Ramakrishnan, 2020), and\nBERT-Attack (Li et al., 2020b). All baseline at-\ntacks are evaluated on ﬁnetuned BERT models\nfrom the TextAttack library (Morris et al., 2020b).\nSee subsection 4.2 for details of attack settings.\nHyperparameters. Our adversarial distribution\nparameter Θ is optimized using Adam (Kingma\nand Ba, 2014) with a learning rate of 0.3 and a\nbatch size of 10 for 100 iterations. The distribution\nparameters Θ are initialized to zero except Θi,j =\nC where xi = j is the i-th token of the clean\ninput. In practice we take C ∈12,15. We use\nλperp = 1 and cross-validate λsim ∈[20,200] and\nκ∈{3,5,10}using held-out data.\n4.2 Quantitative Evaluation\nWhite-box attacks. We ﬁrst evaluate the attack\nperformance under the white-box setting. Table 1\nshows the result of our attacks against GPT-2, XLM\n(en-de), and BERT on different benchmark datasets.\nFollowing prior work (Jin et al., 2020), for each\ntask, we randomly select 1000 inputs from the\ntask’s test set as attack targets. After optimizingΘ,\nwe draw up to 100 samples z ∼PΘ until the model\nmisclassiﬁes z. The model’s accuracy after attack\n(under the column “Adv. Acc.”) is the accuracy\nevaluated on the last of the drawn samples.\nOverall, our attack is able to successfully gener-\nate adversarial examples against all three models\nacross the ﬁve benchmark datasets. The test accu-\nracy can be reduced to below 10% for almost all\nmodels and tasks. Following prior work, we also\nevaluate the semantic similarity between the adver-\nsarial example and the original input using the co-\n5753\nTarget Model Task Clean Acc. Adv. Acc. # Queries Cosine Sim.\nALBERT\nAG News 94.7 7.5 84 0.68\nYelp 97.5 5.9 76 0.79\nIMDB 93.8 13.1 157 0.87\nRoBERTA\nAG News 94.7 10.7 130 0.67\nIMDB 95.2 17.4 205 0.87\nMNLI(m.) 88.1 4.1/15.1 63/179 0.69/0.76\nMNLI(mm.) 87.8 3.2/15.9 51/189 0.69/0.78\nXLNet\nIMDB 93.8 12.1 149 0.87\nMNLI(m.) 87.2 3.9/13.7 56/162 0.70/0.77\nMNLI(mm.) 86.8 1.7/14.4 32/171 0.70/0.78\nTable 3: Result of black-box model transfer attack from GPT-2 to\nother transformer models. Our attack is achieved by sampling from\nthe same adversarial distribution PΘ and is able to generalize to the\nthree target transformer models considered in this study.\n100 101 102 103\nNumber of queries\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6Test accuracy\nBERT\nRoBERTa\nALBERT\nsim = 50\nsim = 20\nsim = 10\nFigure 2: Effect of the parameter λsim on\ntransfer attack success rate. Lower λsim pro-\nduces more aggressive changes, but also gen-\neralizes better to different target models.\nTask Architecture Clean Acc. Adv. Acc. # Queries Cosine Sim.\nIMDB→Yelp\nGPT-2 97.8 22.1 280 0.83\nXLM(en-de) 96.3 7.0 94 0.81\nBERT 97.3 19.3 235 0.79\nGPT-2→BERT 97.3 26.1 319 0.83\nYelp→IMDB\nGPT-2 93.8 3.2 52 0.89\nXLM(en-de) 87.6 14.4 163 0.88\nBERT 93.0 15.4 192 0.88\nGPT-2→BERT 93.0 11.1 138 0.89\nTable 4: Evaluation of black-box\ndataset transfer attack on Yelp/IMDB.\nEven without access to the target\nmodel’s data distribution, it is still\npossible to execute the attack by us-\ning GBDA on a model trained for the\nsame task but a different training dis-\ntribution. See text for details.\nsine similarity of Universal Sentence Encoders (Cer\net al., 2018) (USE). Our attack is able to consis-\ntently maintain a high cosine similarity to the origi-\nnal input (higher than 0.8) in most cases.\nModel transfer attacks. We also evaluate our at-\ntack against prior work under the black-box setting\nby transferring across models. More speciﬁcally,\nfor each model and task, we randomly select 1000\ntest samples and optimize the adversarial distribu-\ntion PΘ on GPT-2. After optimizing Θ, we draw\nup to 1000 samples z ∼PΘ and evaluate them\non the target BERT model from the TextAttack\nlibrary (Morris et al., 2020b) until the model mis-\nclassiﬁes z. This attack setting is strictly more\nrestrictive than prior work because our query pro-\ncedure only requires the target model to output\na discrete label in order to decide when to stop\nsampling from PΘ, whereas prior work relied on a\ncontinuous-valued output score such as class prob-\nabilities.\nTable 2 shows the performance of our attack\nwhen transferred to ﬁnetuned BERT text classiﬁers.\nIn all settings, GBDA is able to reduce the target\nmodel’s accuracy to below that of BERT-Attack\nand BAE within similar or fewer number of queries.\nMoreover, the cosine similarity between the origi-\nnal input and the adversarial example is higher than\nthat of BERT-Attack.\nWe further evaluate our model transfer attack\nagainst three other ﬁnetuned transformer mod-\nels from the TextAttack library: ALBERT (Lan\net al., 2019), RoBERTa (Liu et al., 2019), and XL-\nNet (Yang et al., 2019). For this experiment, we\nuse the same Θ optimized on GPT-2 for each of\nthe target models. Table 3 reports the performance\nof our attack after randomly sampling up to 1000\ntimes from PΘ. The attack performance is compa-\nrable to that of the transfer attack against BERT in\nTable 2, which means our adversarial distribution\nPΘ is able to capture the common failure modes of\na wide variety of transformer models.\nDataset transfer attacks. The model transfer at-\ntack relies on the assumption that the adversary has\naccess to the target model’s training data. We relax\nthis assumption in the form of a dataset transfer\nattack where only the target model’s task is known.\nConcretely, we attack sentiment classiﬁers trained\non Yelp/IMDB by using a model trained on one\ndataset for optimizing Θ and drawing up to 1000\nsamples from PΘ to attack the target model trained\non the other dataset.\nTable 4 shows the result of the dataset transfer at-\ntack for different target model architectures. In all\nexcept for the case of GPT-2→BERT, the model\n5754\nAttackPrediction Text\nOriginalEntailment (83%)He found himself thinking in circles of worry and pulled himself back to his problem.\nHe got lost in loops of worry, but snapped himself back to his problem.\nGBDA Neutral (95%) He found himself thinking in circles of worry and pulled himself back to his problem.\nHe got lost in loops of hell, but snapped himself back to his problem.\nOriginalContradiction (78%)Steps are initiated to allow program board membership to reﬂect the clienteligible community and include\nrepresentatives from the funding community, corporations and other partners.\nThere isn’t a fair representation of board members on the program.\nGBDA Neutral (98%) Steps are initiated to allow program board membership to reﬂect the clienteligible community and include\nrepresentatives from the funding community, corporations and other partners.\nThere isn also a fair representation of board members on the program..\nOriginalContradiction (98%)Pesticide concentrations should not exceed USEPA’s Ambient Water Quality chronic criteria values where available.\nThere is no assigned value for maximum pesticide concentration in water.\nGBDA Entailment (86%)Pesticide concentrations should not exceed USEPA’s Ambient Water Quality chronic criteria values where available.\nThere is varying assigned value for maximum pesticide concentration in water.\nTable 5: Examples of successful adversarial texts on the MNLI dataset.\nAttack Prediction Text\nOriginal World (99%) Turkey a step closer to Brussels The European Commission is set to give the green light later today\nto accession talks with Turkey. EU leaders will take a ﬁnal decision in December.\nGBDA w/ ﬂuencyBusiness (100%)Turkey a step closer to Brussels The eurozone Union is set to give the green light later today to\naccession talks with Barcelona. EU leaders will take a ﬁnal decision in December.\nGBDA w/o ﬂuencyBusiness (77%)Turkey a step closer to Uber Thecom Commission is set to give the green light later today to\naccessrage negotiations with Turkey. EU leaders will take a ﬁnal decision in December.\nTable 6: Examples of adversarial text on AG News generated with and without the ﬂuency constraint. Without the\nﬂuency constraint, the constructed adversarial text tends to contain more nonsensical token combinations.\nused when optimizing PΘ has the same architec-\nture as the target model. In the last setting, we\nsimultaneously transfer between the model and the\ndataset. It is evident that the transfer attack remains\nsuccessful despite not having access to the target\nmodel’s training data. This result opens a practi-\ncal avenue of attack against real world systems as\nthe attacker requires very limited knowledge of the\ntarget model in order to succeed.\n4.3 Analysis\nSample adversarial texts. Table 5 shows exam-\nples of our adversarial attack on text. Our method\nintroduces minimal changes to the text, preserving\nmost of the original sentence’s meaning. Despite\nnot explicitly constraining replaced words to have\nthe same Part-Of-Speech tag, we observe that our\nsoft penalties make the adversarial examples obey\nthis constraint. For instance, in the ﬁrst and third ex-\namples of Table 5, \"worry\" is replaced with \"hell\"\nand \"no\" with \"varying\".\nEffect of λsim. Figure 2 shows the impact of the\nsimilarity constraint on transfer attack adversar-\nial accuracy for GPT-2 on AG News. Each color\ncorresponds to a different target model, whereas\nthe color shade (from light to dark) indicates the\nvalue of the constraint hyperparameter: λsim =\n50,20,10. A higher value of λsim reduces the ag-\ngressiveness of the perturbation, but also increases\nthe number of queries required to achieve a given\ntarget adversarial accuracy.\nImpact of the ﬂuency constraint. Table 6\nshows adversarial examples for GPT-2 on AG\nNews, generated with and without the ﬂuency con-\nstraint. We ﬁx all hyperparameters except for the\nﬂuency regularization constant λlm, and sample\nsuccessful adversarial texts from PΘ after Θ has\nbeen optimized. It is evident that the ﬂuency con-\nstraint promotes token combinations to form valid\nwords and ensure grammatical correctness of the\nadversarial text. Without the ﬂuency constraint, the\nadversarial text tends to contain nonsensical words.\nTokenization artifacts. Our attack operates en-\ntirely on tokenized inputs. However, the input to\nthe classiﬁcation system is often in raw text form,\nwhich is then tokenized before being fed to the\nmodel. Thus it is possible that we generate an ad-\nversarial example that, when converted to raw text,\nis not re-tokenized to the same set of tokens.\nConsider this example: our adversarial exam-\nple contains the tokens \"jui-\" and \"cy\", which de-\ncodes into \"juicy\", and is then re-encoded to \"juic-\n\" and \"y\". In practice, we observe that these re-\ntokenization artifacts are rare: the \"token error rate\"\nis around 2%. Furthermore, they do not impact\nadversarial accuracy by much: the re-tokenized\nexample is in fact still adversarial. One potential\n5755\nFigure 3: Web interface for the human evaluation experiment using Amazon Mechanical Turk.\nmitigation strategy is to re-sample from PΘ until\nthe sampled text is stable under re-tokenization.\nNote that all our adversarial accuracy results are\ncomputed after re-tokenization.\nRuntime. Our method relies on white-box opti-\nmization and thus necessitates forward and back-\nward passes through the attacked model, the lan-\nguage model and the similarity model, which in-\ncreases the per-query time compared to black-box\nattacks that only compute forward passes. How-\never, this is compensated by a much more efﬁcient\noptimization which brings the total runtime to 20s\nper generated example, on par with black-box at-\ntacks such as BERT-Attack (Li et al., 2020b).\nHuman evaluation. We further conduct a human\nevaluation study of our attacks to examine to what\nextent are adversarial texts generated by GBDA\ntruly imperceptible. Our interface is shown in Fig-\nure 3: We show annotators on Amazon Mechanical\nTurk two snippets of text—one is not modiﬁed,\nand the other one is adversarially corrupted—and\nthe annotator has to select which one is corrupted\nin less than 10 seconds. The clean text is sam-\npled from Yelp and the adversarial text is generated\nagainst BERT using either our method or BAE, our\nstrongest baseline. To ensure high quality of the\nannotations, we select annotators with more than\n1000 hits approved and with an approval rate higher\nthan 98%. The annotation itself is preceded by an\nonboarding with three simple examples that have\nto be correctly classiﬁed in order for the annotator\nto qualify for the task.\nAveraging across more than3000 samples, anno-\ntators are able to detect BAE examples in 78.04%\nof the cases, while detecting our examples in\n76.06% of the cases. This result shows that al-\nthough both GBDA and BAE produce detectable\nchanges, our method is slightly less perceptible\nthan BAE but the model accuracy after attack is\nsigniﬁcantly lower for our attack: 4.7% for GBDA\ncompared to 12.0% for BAE (cf. Tables 1 and 2).\n5 Conclusion and Future Work\nWe presented GBDA, a framework for gradient-\nbased white-box attack against text transformers.\nOur approach overcomes many ad-hoc constraints\nand limitations from the existing text attack litera-\nture by leveraging a novel adversarial distribution\nformulation, allowing end-to-end optimization of\nthe adversarial loss and ﬂuency constraints with\ngradient descent. This makes our method generic\nand potentially applicable to any model for token\nsequence prediction.\nLimitations. One clear limitation of GBDA is\nits restriction to only token replacements. Indeed,\nour adversarial distribution formulation using the\nGumbel-softmax does not trivially extend to to-\nken insertions and deletions. This limitation may\nadversely affect the naturalness of the generated ad-\nversarial examples. We hope to extend our frame-\nwork to incorporate a broader set of token-level\nchanges in the future.\nIn addition, the adversarial distribution PΘ is\nhighly over-parameterized. Despite most adversar-\nial examples requiring only a few token changes,\nthe distribution parameterΘ is of size n×V, which\nis especially excessive for longer sentences. Future\nwork may be able to reduce the number of parame-\nters without affecting attack performance.\n5756\nReferences\nNaveed Akhtar and Ajmal Mian. 2018. Threat of adver-\nsarial attacks on deep learning in computer vision: A\nsurvey. Ieee Access, 6:14410–14430.\nMoustafa Alzantot, Yash Sharma, Ahmed Elgohary,\nBo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang.\n2018. Generating natural language adversarial ex-\namples. arXiv preprint arXiv:1804.07998.\nBattista Biggio, Igino Corona, Davide Maiorca, Blaine\nNelson, Nedim Šrndi ´c, Pavel Laskov, Giorgio Giac-\ninto, and Fabio Roli. 2013. Evasion attacks against\nmachine learning at test time. In Joint European\nconference on machine learning and knowledge dis-\ncovery in databases, pages 387–402. Springer.\nNicholas Carlini and David Wagner. 2017. Towards\nevaluating the robustness of neural networks. In\n2017 ieee symposium on security and privacy (sp) ,\npages 39–57. IEEE.\nNicholas Carlini and David Wagner. 2018. Audio ad-\nversarial examples: Targeted attacks on speech-to-\ntext. In 2018 IEEE Security and Privacy Workshops\n(SPW), pages 1–7. IEEE.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St John, Noah Constant,\nMario Guajardo-Céspedes, Steve Yuan, Chris Tar,\net al. 2018. Universal sentence encoder. arXiv\npreprint arXiv:1803.11175.\nHanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang,\nJun Zhu, and Le Song. 2018. Adversarial attack on\ngraph structured data. In International conference\non machine learning, pages 1115–1124. PMLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJavid Ebrahimi, Anyi Rao, Daniel Lowd, and De-\njing Dou. 2017. Hotﬂip: White-box adversarial\nexamples for text classiﬁcation. arXiv preprint\narXiv:1712.06751.\nJi Gao, Jack Lanchantin, Mary Lou Soffa, and Yan-\njun Qi. 2018. Black-box generation of adversarial\ntext sequences to evade deep learning classiﬁers. In\n2018 IEEE Security and Privacy Workshops (SPW),\npages 50–56. IEEE.\nSiddhant Garg and Goutham Ramakrishnan. 2020.\nBAE: BERT-based adversarial examples for text\nclassiﬁcation. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 6174–6181, Online. As-\nsociation for Computational Linguistics.\nEric Jang, Shixiang Gu, and Ben Poole. 2016. Categor-\nical reparameterization with gumbel-softmax. arXiv\npreprint arXiv:1611.01144.\nDi Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter\nSzolovits. 2020. Is bert really robust? a strong base-\nline for natural language attack on text classiﬁcation\nand entailment. In Proceedings of the AAAI con-\nference on artiﬁcial intelligence , volume 34, pages\n8018–8025.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. arXiv preprint\narXiv:1901.07291.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nDianqi Li, Yizhe Zhang, Hao Peng, Liqun Chen, Chris\nBrockett, Ming-Ting Sun, and Bill Dolan. 2020a.\nContextualized perturbation for textual adversarial\nattack. arXiv preprint arXiv:2009.07502.\nJinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting\nWang. 2018. Textbugger: Generating adversarial\ntext against real-world applications. arXiv preprint\narXiv:1812.05271.\nLinyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue,\nand Xipeng Qiu. 2020b. BERT-ATTACK: Adver-\nsarial attack against BERT using BERT. InProceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n6193–6202, Online. Association for Computational\nLinguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAndrew Maas, Raymond E Daly, Peter T Pham, Dan\nHuang, Andrew Y Ng, and Christopher Potts. 2011.\nLearning word vectors for sentiment analysis. In\nProceedings of the 49th annual meeting of the as-\nsociation for computational linguistics: Human lan-\nguage technologies, pages 142–150.\nAleksander Madry, Aleksandar Makelov, Ludwig\nSchmidt, Dimitris Tsipras, and Adrian Vladu. 2017.\nTowards deep learning models resistant to adversar-\nial attacks. arXiv preprint arXiv:1706.06083.\nRishabh Maheshwary, Saket Maheshwary, and Vikram\nPudi. 2020. Generating natural language attacks\nin a hard label black box setting. arXiv preprint\narXiv:2012.14956.\n5757\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\nrado, and Jeffrey Dean. 2013. Distributed represen-\ntations of words and phrases and their composition-\nality. arXiv preprint arXiv:1310.4546.\nJohn Morris, Eli Liﬂand, Jack Lanchantin, Yangfeng\nJi, and Yanjun Qi. 2020a. Reevaluating adversar-\nial examples in natural language. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, pages 3829–3839, Online. Association for\nComputational Linguistics.\nJohn X Morris, Eli Liﬂand, Jin Yong Yoo, and Yan-\njun Qi. 2020b. Textattack: A framework for adver-\nsarial attacks in natural language processing. arXiv\npreprint arXiv:2005.05909.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language process-\ning (EMNLP), pages 1532–1543.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nShuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che.\n2019. Generating natural language adversarial ex-\namples through probability weighted word saliency.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n1085–1097, Florence, Italy. Association for Compu-\ntational Linguistics.\nSuranjana Samanta and Sameep Mehta. 2017. Towards\ncrafting text adversarial samples. arXiv preprint\narXiv:1707.02812.\nLiwei Song, Xinwei Yu, Hsuan-Tung Peng, and\nKarthik Narasimhan. 2020. Universal adversarial\nattacks with natural triggers for text classiﬁcation.\narXiv preprint arXiv:2005.00174.\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever,\nJoan Bruna, Dumitru Erhan, Ian Goodfellow, and\nRob Fergus. 2013. Intriguing properties of neural\nnetworks. arXiv preprint arXiv:1312.6199.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner,\nand Sameer Singh. 2019. Universal adversarial trig-\ngers for attacking and analyzing nlp. arXiv preprint\narXiv:1908.07125.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2017. A broad-coverage challenge corpus for\nsentence understanding through inference. arXiv\npreprint arXiv:1704.05426.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nYuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu,\nMeng Zhang, Qun Liu, and Maosong Sun. 2020.\nWord-level textual adversarial attacking as combina-\ntorial optimization. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 6066–6080, Online. Association\nfor Computational Linguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\nuating text generation with bert. arXiv preprint\narXiv:1904.09675.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. arXiv preprint arXiv:1509.01626.\nDaniel Zügner, Amir Akbarnejad, and Stephan Gün-\nnemann. 2018. Adversarial attacks on neural net-\nworks for graph data. In Proceedings of the 24th\nACM SIGKDD International Conference on Knowl-\nedge Discovery & Data Mining, pages 2847–2856.",
  "topic": "Adversarial system",
  "concepts": [
    {
      "name": "Adversarial system",
      "score": 0.8884456157684326
    },
    {
      "name": "Computer science",
      "score": 0.7286379933357239
    },
    {
      "name": "Parameterized complexity",
      "score": 0.7257839441299438
    },
    {
      "name": "Transformer",
      "score": 0.5613994002342224
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4353015422821045
    },
    {
      "name": "Algorithm",
      "score": 0.26333552598953247
    },
    {
      "name": "Engineering",
      "score": 0.0799863338470459
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    }
  ],
  "cited_by": 82
}