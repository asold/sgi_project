{
    "title": "Cross-Domain Data Augmentation with Domain-Adaptive Language Modeling for Aspect-Based Sentiment Analysis",
    "url": "https://openalex.org/W4385571507",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5084990301",
            "name": "Jianfei Yu",
            "affiliations": [
                "Nanjing University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5075130969",
            "name": "Qiankun Zhao",
            "affiliations": [
                "Nanjing University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5035534911",
            "name": "Rui Xia",
            "affiliations": [
                "Nanjing University of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3197893554",
        "https://openalex.org/W2901440799",
        "https://openalex.org/W3102449459",
        "https://openalex.org/W2757541972",
        "https://openalex.org/W2155120841",
        "https://openalex.org/W4283800335",
        "https://openalex.org/W4287890001",
        "https://openalex.org/W4221141206",
        "https://openalex.org/W2803777992",
        "https://openalex.org/W4210643350",
        "https://openalex.org/W2252024663",
        "https://openalex.org/W2889774592",
        "https://openalex.org/W2567698949",
        "https://openalex.org/W3100132436",
        "https://openalex.org/W2159457224",
        "https://openalex.org/W2799186171",
        "https://openalex.org/W3035529900",
        "https://openalex.org/W3174493546",
        "https://openalex.org/W1731081199",
        "https://openalex.org/W2985056549",
        "https://openalex.org/W2515059321",
        "https://openalex.org/W3211920512",
        "https://openalex.org/W2963274454",
        "https://openalex.org/W2187089797",
        "https://openalex.org/W4285688121",
        "https://openalex.org/W22861983",
        "https://openalex.org/W3034990686",
        "https://openalex.org/W2905325083",
        "https://openalex.org/W3098649723",
        "https://openalex.org/W2950488390",
        "https://openalex.org/W2952357537",
        "https://openalex.org/W2604356472",
        "https://openalex.org/W4313490656",
        "https://openalex.org/W2163302275",
        "https://openalex.org/W4288087680",
        "https://openalex.org/W3174432206",
        "https://openalex.org/W2261310161",
        "https://openalex.org/W2897908126",
        "https://openalex.org/W3152368098",
        "https://openalex.org/W3041133507",
        "https://openalex.org/W3104055036",
        "https://openalex.org/W2160660844",
        "https://openalex.org/W2251648804",
        "https://openalex.org/W4293167850",
        "https://openalex.org/W1882958252",
        "https://openalex.org/W2979611230",
        "https://openalex.org/W2153353890",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2514722822",
        "https://openalex.org/W2923978210",
        "https://openalex.org/W3104182623"
    ],
    "abstract": "Cross-domain Aspect-Based Sentiment Analysis (ABSA) aims to leverage the useful knowledge from a source domain to identify aspect-sentiment pairs in sentences from a target domain. To tackle the task, several recent works explore a new unsupervised domain adaptation framework, i.e., Cross-Domain Data Augmentation (CDDA), aiming to directly generate much labeled target-domain data based on the labeled source-domain data. However, these CDDA methods still suffer from several issues: 1) preserving many source-specific attributes such as syntactic structures; 2) lack of fluency and coherence; 3) limiting the diversity of generated data. To address these issues, we propose a new cross-domain Data Augmentation approach based on Domain-Adaptive Language Modeling named DA2LM, which contains three stages: 1) assigning pseudo labels to unlabeled target-domain data; 2) unifying the process of token generation and labeling with a Domain-Adaptive Language Model (DALM) to learn the shared context and annotation across domains; 3) using the trained DALM to generate labeled target-domain data. Experiments show that DA2LM consistently outperforms previous feature adaptation and CDDA methods on both ABSA and Aspect Extraction tasks. The source code is publicly released at https://github.com/NUSTM/DALM.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 1456â€“1470\nJuly 9-14, 2023 Â©2023 Association for Computational Linguistics\nCross-Domain Data Augmentation with Domain-Adaptive Language\nModeling for Aspect-Based Sentiment Analysis\nJianfei Yuâˆ—, Qiankun Zhaoâˆ—and Rui Xiaâ€ \nSchool of Computer Science and Engineering,\nNanjing University of Science and Technology, China\n{jfyu, kkzhao, rxia}@njust.edu.cn\nAbstract\nCross-domain Aspect-Based Sentiment Analy-\nsis (ABSA) aims to leverage the useful knowl-\nedge from a source domain to identify aspect-\nsentiment pairs in sentences from a target do-\nmain. To tackle the task, several recent works\nexplore a new unsupervised domain adapta-\ntion framework, i.e., Cross-Domain Data Aug-\nmentation (CDDA), aiming to directly generate\nmuch labeled target-domain data based on the\nlabeled source-domain data. However, these\nCDDA methods still suffer from several is-\nsues: 1) preserving many source-specific at-\ntributes such as syntactic structures; 2) lack of\nfluency and coherence; 3) limiting the diver-\nsity of generated data. To address these issues,\nwe propose a new cross-domain Data Augmen-\ntation approach based on Domain-Adaptive\nLanguage Modeling named DA 2LM, which\ncontains three stages: 1) assigning pseudo la-\nbels to unlabeled target-domain data; 2) unify-\ning the process of token generation and label-\ning with a Domain-Adaptive Language Model\n(DALM) to learn the shared context and an-\nnotation across domains; 3) using the trained\nDALM to generate labeled target-domain data.\nExperiments show that DA 2LM consistently\noutperforms previous feature adaptation and\nCDDA methods on both ABSA and Aspect Ex-\ntraction tasks. The source code is publicly re-\nleased at https://github.com/NUSTM/DALM.\n1 Introduction\nAs an important task in sentiment analysis, Aspect-\nBased Sentiment Analysis (ABSA) aims to extract\naspect terms from sentences and predict the senti-\nment polarity towards each aspect term (Liu, 2012;\nPontiki et al., 2016). For example, given a sentence\nâ€œThe screen is broken\", the aspect term is screen\nand its sentiment polarity is Negative. With the ad-\nvancements of deep learning techniques, a myriad\nof neural approaches have been proposed for ABSA\nâˆ—Equal contribution.\nâ€ Corresponding author.\nDecoderEncoder\nThe [MASK] was pretty great .\nO    B-ASP     O       O B-OP O\npizza The laptop was pretty great .\nO   B-ASP    O       O B-OP O\nb) Seq2Seq-based Cross-Domain Data Augmentation\nMasked source data (Restaurant): Generated target data (Laptop):\nMLMs\nOrder the [MASK] [MASK] positive,\nit's fantastic.\npanang   duck Order the glass touchpad positive,\nit's fantastic.\na) MLM-based Cross-Domain Data Augmentation\nMasked source data (Restaurant): Generated target data (Laptop):\nDecoder\nLabeled source \ndata (Restaurant)\nPseudo-labeled \ndata (Laptop)\nToshiba will not acknowledge \nthis laptop negative as a repair.\n......\n......\nTrain\nGenerated target data (Laptop):\nc) DALM-based Cross-Domain Data Augmentation (Ours)\nFigure 1: Comparison between different Cross-Domain Data\nAugmentation (CDDA) methods.\nand achieved promising results on several bench-\nmark datasets (Li et al., 2019a; He et al., 2019;\nChen and Qian, 2020b). However, these methods\nheavily rely on labeled data with fine-grained anno-\ntation, which is often time-consuming and expen-\nsive to obtain for many emerging domains.\nTo alleviate the reliance on labeled data, many\nprevious works resorted to unsupervised domain\nadaptation techniques, which aim to transfer knowl-\nedge from a resource-rich source domain to a tar-\nget domain only with unlabeled data (Blitzer et al.,\n2007; Pan et al., 2010; Zhuang et al., 2015). Most\nexisting domain adaptation methods on the ABSA\ntask focus on learning shared feature representa-\ntions across domains (Wang and Pan, 2018; Li et al.,\n2019c; Gong et al., 2020; Chen and Qian, 2021).\nAlthough these methods have obtained promising\nresults, their models are only trained on the source-\ndomain labeled data and thus insensitive to the\nimportant target-specific aspect and opinion terms.\nTo address this limitation, several recent stud-\nies have explored a new domain adaptation frame-\nwork named Cross-Domain Data Augmentation\n(CDDA), which aims to directly generate much\ntarget-domain labeled data based on the labeled\ndata from the source domain. These existing meth-\nods can be summarized into two groups: Masked\n1456\nLanguage Model (MLM)-based CDDA (Yu et al.,\n2021; Yang et al., 2022) and Sequence-to-Sequence\n(Seq2Seq)-based CDDA (Chen et al., 2021; Li\net al., 2022). As shown in Fig. 1(a) and Fig. 1(b),\nthe core idea behind existing CDDA methods is\nto first mask source-specific words in the source-\ndomain labeled data, followed by using either the\nwell-trained MLM or Seq2Seq models to automat-\nically generate target-specific words and labels in\nthe masked positions. Despite achieving significant\nimprovements over previous feature adaptation\nmethods, these CDDA approaches still have several\nshortcomings: 1) they only mask source-specific\nwords or phrases but preserve other source-specific\nattributes such as syntactic structures, which make\nthe distribution of the generated data different from\nthat of the real target-domain data; 2) replacing\nsource-specific words with target-specific words\nmay destruct the semantic meaning of the original\nsentence, making the generated data lack of fluency\nand coherence; 3) existing CDDA methods regard\neach source-domain sentence as the template, thus\nlimiting the diversity of the generated data.\nTo tackle these shortcomings, we propose a new\ncross-domain Data Augmentation approach based\non Domain-Adaptive Language Modeling named\nDA2LM, which consists of three stages, includ-\ning Domain-Adaptive Pseudo Labeling, Domain-\nAdaptive Language Modeling, and Target-Domain\nData Generation. Specifically, the labeled source\ndata and unlabeled target data are first leveraged\nto train a base domain adaptation model, which\nis then used for predicting pseudo labels of un-\nlabeled data in the target domain. Secondly, we\ndesign a novel Domain-Adaptive Language Model\n(DALM), and train it on the labeled source data and\npseudo-labeled target data to learn the transferable\ncontext and label across domains. Different from\nmost existing LMs, our DALM unifies the process\nof data generation and fine-grained annotation, aim-\ning to simultaneously generate the next token and\npredict the label of the current token at each time\nstep of the training stage. Finally, given the trained\nDALM, we employ it to generate many labeled\ntarget-domain data in an autoregressive manner\nwith a probability-based generation strategy.\nOur main contributions can be summarized as\nfollows:\nâ€¢ We propose a three-stage framework named\ncross-domain Data Augmentation with Domain\nAdaptive Language Modeling (DA2LM), which\ncan generate a large amount of labeled target-\ndomain data for the cross-domain ABSA task.\nâ€¢ Under the framework, we devise a new domain-\nadaptive language model, which unifies the pro-\ncess of data generation and labeling and captures\nthe domain-invariant context and annotation for\ntarget-domain data generation.\nâ€¢ Experiments on four benchmark datasets demon-\nstrate that our framework significantly outper-\nforms a number of competitive domain adapta-\ntion methods on both ABSA and Aspect Extrac-\ntion (AE) tasks. Further analysis on generated\ndata shows the superiority of our framework in\nterms of data distribution, diversity, and fluency.\n2 Related Work\n2.1 Aspect-Based Sentiment Analysis (ABSA)\nAs an important task in sentiment analysis, ABSA\nhas been extensively studied in the last decade. Ear-\nlier works mainly focus on two subtasks of ABSA,\ni.e., aspect extraction (AE) (Liu et al., 2015; Chen\nand Qian, 2020a) and aspect-based sentiment clas-\nsification (ASC) (Zhang et al., 2016; Chen et al.,\n2017; Sun et al., 2019; Wang et al., 2020). Re-\ncently, many supervised methods are proposed to\nsolve the two sub-tasks in an end-to-end manner,\nwhich either resort to multi-task learning to ex-\nploit the relations between AE and ASC (Luo et al.,\n2019; He et al., 2019; Chen and Qian, 2020b) or\nemploy a collapsed tagging scheme to combine\nAE and ASC into a unified label space and formu-\nlate the task as a sequence labeling problem (Wang\net al., 2018; Li et al., 2019a,b). Despite obtaining\npromising results on several benchmark datasets,\nthese methods suffer from the lack of annotated\ndata in many emerging domains. To alleviate this\nissue, we aim to propose an unsupervised domain\nadaptation method to generate sufficient labeled\ndata for ABSA in any target domain.\n2.2 Unsupervised Domain Adaptation\nIn the literature, a myriad of unsupervised domain\nadaptation methods have been proposed for coarse-\ngrained sentiment analysis (Zhuang et al., 2020),\nincluding pivot-based methods (Blitzer et al., 2007;\nYu and Jiang, 2016; Ziser and Reichart, 2018; Xi\net al., 2020), auto-encoders (Glorot et al., 2011;\nZhou et al., 2016), domain adversarial networks\n(Ganin and Lempitsky, 2015; Ganin et al., 2016;\nLi et al., 2018), and semi-supervised methods (He\n1457\net al., 2018; Ye et al., 2020). These methods pri-\nmarily focus on learning domain-invariant repre-\nsentations to alleviate the distribution discrepancy\nacross domains. Inspired by the success of these\nrepresentation-based methods, a few recent stud-\nies have adapted them to the cross-domain ABSA\ntask, in which the key idea is to learn a shared\nrepresentation for each word or aspect term across\ndomains (Ding et al., 2017; Wang and Pan, 2018,\n2019, 2020; Li et al., 2019c; Zeng et al., 2022; Chen\nand Qian, 2022). Moreover, Lekhtman et al. (2021)\nproposed a customized pre-training approach with\naspect category shift for the aspect extraction task.\nDespite obtaining promising results, the major\nlimitation of these aforementioned methods for\ncross-domain ABSA is that their models for the\nmain ABSA task is solely trained on the source-\ndomain labeled data. Thus, their models are insen-\nsitive to target-specific features. To address this\nissue, some studies have explored a Cross-Domain\nData Augmentation framework (CDDA) to directly\ngenerate much target-domain labeled data, includ-\ning MLM-based CDDA (Yu et al., 2021; Yang\net al., 2022) and Seq2Seq-based CDDA (Chen\net al., 2021; Li et al., 2022). However, the gen-\nerated data by these methods has several limita-\ntions including 1) preserving many source-specific\nattributes such as syntactic structures; 2) lack of\nfluency and diversity. Thus, in this work, we aim\nto propose a new data augmentation framework\nthat can generate fluent target-domain labeled data\nwithout any source-specific attributes.\n3 Methodology\n3.1 Problem Definition and Notations\nFollowing previous studies (Li et al., 2019c), we\nformulate ABSA and AE as a sequence label-\ning problem. Given a sentence with n words\nx = {w1,w2,...,w n}, the goal is to predict its\ncorresponding label sequence y = {y1,y2,...,y n},\nwhere yj âˆˆ{B-POS,I-POS,B-NEG,I-NEG,B-NEU,\nI-NEU,O}for ABSA and yj âˆˆ{B,I,O}for AE.\nIn this work, we focus on the unsupervised do-\nmain adaptation setting, in which the source do-\nmain has enough labeled data and the target domain\nonly has unlabeled data. Let DS = {(xs\ni ,ys\ni )}Ns\ni=1\ndenote a set of source-domain labeled data, and\nDT = {xt\ni}Nt\ni=1 a set of target-domain unlabeled\ndata. The goal is to leverage DS and DT to pre-\ndict the label sequences of test data from the target\ndomain.\n3.2 Overview\nAs illustrated in Figure 2, our Cross-Domain Data\nAugmentation framework contains three key stages,\nincluding 1) Domain-Adaptive Pseudo Labeling,\n2) Domain-Adaptive Language Modeling, and 3)\nTarget-Domain Data Generation. In the first stage,\nan aspect-aware domain adaptation model is trained\nto assign pseudo labels to unlabeled data in the tar-\nget domain. In the second stage, the labeled source\ndata and the pseudo-labeled target data are used\nto train a domain-adaptive language model, which\nintegrates data generation and sequence labeling\nin a unified architecture to capture the transfer-\nable context and annotation across domains. After\ntraining the DALM, the last stage uses probability-\nbased generation strategy to generate diverse target-\ndomain data with fine-grained annotations in an\nautoregressive manner.\n3.3 Domain-Adaptive Pseudo Labeling\nIn this stage, our goal is to assign the pseudo la-\nbels to each unlabeled data in the target domain.\nSince the data distribution of the source domain is\ndifferent from that of the target domain, directly\ntraining a classifier on the labeled source data to\npredict the pseudo labels of the unlabeled target\ndata will bring much noise. Thus, it is necessary\nto alleviate the domain discrepancy to improve the\nquality of pseudo-labels. Since aspect terms are\nshown to play a crucial role in ABSA (Gong et al.,\n2020), we attempt to explicitly minimize the dis-\ntance between source-domain and target-domain\naspect term representations via Maximum Mean\nDiscrepancy (MMD) (Gretton et al., 2012).\nSpecifically, given the labeled source data DS\nand the unlabeled target data DT , we first obtain\nthe aspect terms in DS via the gold labels and\nextract the aspect terms in DT based on a rule-\nbased algorithm named Double Propagation (Qiu\net al., 2011). Let us use xd = {wd\n1,wd\n2,...,w d\nn}\nto denote a source or target domain sentence and\nuse ad = {wd\ni ,...,w d\nj }to denote one of the as-\npect terms in the sentence, where dâˆˆ{s,t}. We\nthen employ a pre-trained BERT model to obtain\nthe hidden representation of the sentence Hd =\n{hd\n1,hd\n2,..., hd\nn}and the aspect term representation\nad = g(hd\ni ,..., hd\nj ), where hd âˆˆ Rr, r refers to\nthe hidden dimension, and g(Â·) denotes the mean-\npooling operation. Next, we propose an aspect-\nlevel MMD loss to alleviate the distribution dis-\n1458\nğ‘’ğ‘’âˆ’1\nğ‘¤ğ‘¤\nğ‘’ğ‘’âˆ’2\nğ‘¦ğ‘¦\n<BOS>\n+\n<BOL>\nO\nğ‘’ğ‘’0\nğ‘¤ğ‘¤\nğ‘’ğ‘’âˆ’1\nğ‘¦ğ‘¦\n[source]\nthis\n+\nO\n[source] O\nğ‘’ğ‘’1\nğ‘¤ğ‘¤\nğ‘’ğ‘’0\nğ‘¦ğ‘¦\nthis\npizza\n+\nO\nO\nğ‘’ğ‘’2\nğ‘¤ğ‘¤\nğ‘’ğ‘’1\nğ‘¦ğ‘¦\npizza\nis\n+\nO\nB-POS\nğ‘’ğ‘’3\nğ‘¤ğ‘¤\nğ‘’ğ‘’2\nğ‘¦ğ‘¦\nis\ngreat\n+\nB-POS\nO\nğ‘’ğ‘’4\nğ‘¤ğ‘¤\nğ‘’ğ‘’3\nğ‘¦ğ‘¦\ngreat\n<EOS>\n+\nO\nO\nDecoder\nğ‘’ğ‘’âˆ’1\nğ‘¤ğ‘¤\nğ‘’ğ‘’âˆ’2\nğ‘¦ğ‘¦\n<BOS>\n+\n<BOL>\nO\nğ‘’ğ‘’0\nğ‘¤ğ‘¤\nğ‘’ğ‘’âˆ’1\nğ‘¦ğ‘¦\n[target]\nthe\n+\nO\n[target] O\nğ‘’ğ‘’1\nğ‘¤ğ‘¤\nğ‘’ğ‘’0\nğ‘¦ğ‘¦\nthe\nkeyboard\n+\nO\nO\nğ‘’ğ‘’2\nğ‘¤ğ‘¤\nğ‘’ğ‘’1\nğ‘¦ğ‘¦\nkeyboard\nis\n+\nO\nB-POS\nğ‘’ğ‘’3\nğ‘¤ğ‘¤\nğ‘’ğ‘’2\nğ‘¦ğ‘¦\nis\na\n+\nB-POS\nO\nğ‘’ğ‘’4\nğ‘¤ğ‘¤\nğ‘’ğ‘’3\nğ‘¦ğ‘¦\na\ngood\n+\nO\nO\nğ‘’ğ‘’5\nğ‘¤ğ‘¤\nğ‘’ğ‘’4\nğ‘¦ğ‘¦\ngood\nchoice\n+\nO\nO\nğ‘’ğ‘’5\nğ‘¤ğ‘¤\nğ‘’ğ‘’4\nğ‘¦ğ‘¦\nchoice\n<EOS>\n+\nO\nO\nDecoder\nBase Model\nUnlabeled \ntarget data\nLabeled \nsource data\nPseudo-labeled \ntarget data\nDomain-Adaptive Pseudo Labeling1 Domain-Adaptive Language Modeling2\nTarget-Domain Data Generation3\nFigure 2: Overview of cross-domain Data Augmentation with Domain-Adaptive Language Modeling (DA2LM).\ncrepancy across domains as follows:\nLmmd = d2\nk(DS\na ,DT\na ) = 1\n(Nsa)2\nNs\naâˆ‘\ni,j\nk(as\ni ,as\nj)+\n1\n(Nta)2\nNt\naâˆ‘\ni,j\nk(at\ni,at\nj) âˆ’ 2\nNsaNta\nNs\naâˆ‘\ni\nNt\naâˆ‘\nj\nk(as\ni ,at\nj),\nwhere DS\na and DT\na respectively denote the sets of\naspect term representations in the source domain\nand the target domain, Ns\na and Nt\na refer to the num-\nber of aspect terms in the two domains, and k(Â·)\ndenotes the Gaussian Kernel function.\nMeanwhile, for each source sample, the hidden\nrepresentation Hs is fed into a Conditional Random\nField (CRF) layer to predict the label sequence for\nthe ABSA or AE task p(ys|Hs). The goal is to\nminimize the negative log-probability of the correct\nlabel sequence of each source-domain sample:\nLcrf = âˆ’\nNs\nâˆ‘\ni=1\nlog p(ys\ni |Hs\ni ). (1)\nThe CRF loss for the ABSA or AE task and the\naspect-level MMD loss are combined to train the\nbase model Cb:\nL= Lcrf + Î±Lmmd, (2)\nwhere Î±is the hyper-parameter.\nFinally, we use Cb to assign pseudo labels\nto each sample in DT , and obtain DPT =\n{(xpt\ni ,ypt\ni )}Nt\ni=1.\n3.4 Domain-Adaptive Language Modeling\nTo generate a large amount of target-domain la-\nbeled data with diverse syntactic structures, we\npropose a Domain-Adaptive Language Model\n(DALM), which leverages the labeled source data\nDS and the pseudo-labeled target dataDPT to learn\nthe shared distribution of words and labels across\ndomains. Since our DALM unifies the process of\nword generation and sequence labeling, at each\ntime step, we employ the current input token and\nthe predicted label at the previous step to simulta-\nneously maximize the probabilities of predicting\nthe next token and the label of the current token.\nSpecifically, for each sample (x,y) âˆˆ DS âˆª\nDPT , we first construct an input token se-\nquence, in which we insert a special token\nâŸ¨BOSâŸ© to denote the sentence beginning, fol-\nlowed by a domain-specific token (i.e., [source]\nor [target]) to distinguish the domain that x be-\nlongs to. Let xin = {âŸ¨BOSâŸ©,w0,w1,w2,...,w n}\ndenote the expanded input sentence, where\nw0 âˆˆ {[source],[target]}. Moreover, we con-\nstruct another input label sequence, denoted\nby yin = {âŸ¨BOLâŸ©,yâŸ¨BOSâŸ©,y0,y1,y2,...,y nâˆ’1},\nwhere âŸ¨BOLâŸ©denotes the initial state of the la-\nbel sequence, yâŸ¨BOSâŸ© is O, and yj refers to the\nlabel of wj. According to the input, the out-\nput token sequence is xout = {w0,w1,w2,...,w n,\nâŸ¨EOSâŸ©}. The output label sequence is yout =\n{yâŸ¨BOSâŸ©,y0,y1,y2,...,y n}. The top of Figure 2\n1459\nshows an example of two input and two output\nsequences for a sample from the source domain.\nNext, for the input token sequence xin, we em-\nploy a decoder such as LSTM and the pre-trained\nGPT-2 model (Radford et al., 2019) to get its hid-\nden representation as follows:\new\nâˆ’1,ew\n0 ,..., ew\nn = Decoder(wâˆ’1,w0,w1,...,w n),\nwhere wâˆ’1 denotes âŸ¨BOSâŸ©, ew\nt âˆˆRd is the token\nrepresentation, and dis the hidden dimension. For\nthe input label sequence yin, a label embedding\nlayer is used to get the label representation:\ney\nâˆ’2,..., ey\nnâˆ’1 = LabelEmb(yâˆ’2,yâˆ’1,...,y nâˆ’1),\nwhere yâˆ’2 and yâˆ’1 denote âŸ¨BOLâŸ©and yâŸ¨BOSâŸ©, and\ney\nt âˆˆRd. Next, at each time step t, we add ew\nt\nand ey\ntâˆ’1 to produce a token and label-aware repre-\nsentation (i.e., et = ew\nt + ey\ntâˆ’1), which is then fed\ninto two different full-connected softmax layers to\npredict the probabilities of the next token wt+1 and\nthe label yt as follows:\nP(wt+1|wâ‰¤t,yâ‰¤tâˆ’1) =Ïƒ(Wwet + bw), (3)\nP(yt|wâ‰¤t,yâ‰¤tâˆ’1) =Ïƒ(Wyet + bw), (4)\nwhere Ïƒ is the softmax function, and Wx âˆˆ\nR|Vx|Ã—d, Wy âˆˆR|Vy|Ã—d, and |Vx|and |Vy|are the\nvocabulary size and the label size. For each sample\n(x,y) âˆˆDS âˆªDPT , we optimize the parameters\nfor DALM by minimizing the combination of cross\nentropy losses for the output token sequence and\nlabel sequence as follows:\nL= Lw + Ly, (5)\nLw = âˆ’\nnâˆ‘\nt=âˆ’1\nlogP(wt+1|wâ‰¤t,yâ‰¤tâˆ’1), (6)\nLy = âˆ’\nnâˆ‘\nt=âˆ’1\nlogP(yt|wâ‰¤t,yâ‰¤tâˆ’1). (7)\n3.5 Target-Domain Data Generation\nAfter training the DALM, we employ it to generate\ntarget-domain data with fine-grained annotations in\nan autoregressive manner. As shown in the bottom\nof Figure 2, theâŸ¨BOSâŸ©token and the target-specific\ntoken [target] are fixed as the first two input tokens\nof the DALM, and âŸ¨BOLâŸ©and O are fixed as the\nfirst two input labels. Next, we adopt a probability-\nbased generation strategy to generate the following\ntokens and their corresponding labels.\nAt each time step t, we first rank all the tokens\nin Vx based on the probabilities computed by Eq. 3\nand pick top-ktokens as a candidate set Ct+1. We\nthen sample a token wt+1 from Ct+1 as the next to-\nken. As the candidate tokens in Ct+1 are predicted\nwith higher probabilities, the generated data are\ngenerally fluent and close to the real target-domain\ndata. Moreover, given the same context, the DALM\ncan choose a synonym as the next token due to the\nrandomness of sampling, which is conducive to\ndiversifying the generated data.\nNext, for the label generation at each time step t,\nwe directly select the label with the highest proba-\nbility computed by Eq. 4 as the label of the current\ntoken yt, which can ensure the quality of the gener-\nated label sequence.\nThe above process of token generation and label-\ning will be stopped when the next token is predicted\nas âŸ¨EOSâŸ©. Because of the randomness brought by\nsampling, the trained DALM can be used to gener-\nate any amount of labeled data. However, generat-\ning more data may lead to significant vocabulary\nredundancy of generated data. Thus, once the size\nof generated data equals to Ng, we will stop gener-\nating target-domain labeled data.\n3.6 Generated Data Filtering\nTo mitigate the presence of low-quality labels in the\ntarget data generated from the probability-based\ngeneration strategy, we introduce the following\nsteps for generated data filtering: 1) Delete data\nwith the illogical labels that violate the prefix order\nof the BIO tagging schema (e.g., having O before\nI in the AE task and having B-Positive before I-\nNeutral in the ABSA task); 2) Delete repetitive\ndata whose token and label sequences are the same,\nand only keep one of the duplicate samples; 3) Use\nthe base model Cb in Section 3.3 to predict the la-\nbel sequences of the generated sentences and delete\ndata whose label sequences are different from those\npredicted by Cb.\nLet us use Dg = {(xg\ni ,yg\ni )}Ng\ni=1 to denote the set\nof generated target-domain data. We then train a\nstandard BERT-CRF model (Li et al., 2019b) on\nDg, and use it to predict the label sequences of test\ndata from the target domain.\n4 Experiments\n4.1 Experimental Settings\nDatasets. To evaluate the effectiveness of the\nproposed DA2LM framework, we conduct experi-\n1460\nDataset Sentences Training Testing\nLaptop (L) 3845 3045 800\nRestaurant (R) 6035 3877 2158\nDevice (D) 3836 2557 1279\nService (S) 2239 1492 747\nTable 1: Basic statistics of the datasets.\nments on four benchmark datasets, namely Laptop\n(L), Restaurant (R), Device (D), and Service (S), as\nshown in Table 1. L contains data from the laptop\ndomain in SemEval 2014 (Pontiki et al., 2014). R\nis the union set of the restaurant data from SemEval\n2015 (Pontiki et al., 2015) and SemEval 2016 (Pon-\ntiki et al., 2016). D contains device data about 5\ndigital products (Hu and Liu, 2004). S contains\ndata from web services (Toprak et al., 2010).\nEvaluation. Following (Li et al., 2019c), we\nchoose 10 different source â†’target domain pairs\nfor experiments. L â†’D and D â†’L are removed\nsince the two domains are very similar. For each\ncross-domain pair, DA 2LM generates sufficient\ntarget-domain labeled data and then directly trains\na BERT-CRF classifier on the generated target-\ndomain data. We evaluate the model predictions\nbased on Micro-F1 under the exact match, which\nmeans that the predicted aspect-sentiment pairs are\nconsidered as correct only if they exactly match\nwith the gold aspect-sentiment pairs.\nParameter Setting. For the BERT-CRF model\nused in DA 2LM, we employ a domain-specific\nBERT-base model named BERT-Cross (Xu et al.,\n2019), which was post-trained on a large amount\nof Yelp and Amazon Electronic data (He and\nMcAuley, 2016). For Domain-Adaptive Pseudo La-\nbeling, the hyper-parameterÎ±in Eq. 2 is set as 0.01,\nand we adopt the Adam algorithm with a learn-\ning rate of 3e-5 to optimize the parameters. For\nDomain-Adaptive Language Modeling, we fine-\ntune the LSTM and the pre-trained language model\nGPT-2 (Radford et al., 2019) on DSâˆªDPT , and\nusing the Adam algorithm as the optimizer with\na learning rate of 3e-3 and 3e-4 respectively. For\nTarget-Domain Data Generation, we choose the\ntop-k tokens (i.e., k=100) as the candidate set and\nthe maximum number of generated data Ng is set\nto 10000 in token-sampling generation. All the ex-\nperiments are run on a single Nvidia 1080Ti GPU.\n4.2 Main Results\nTo show the effectiveness of our DA 2LM ap-\nproach, we consider the following competitive do-\nmain adaptation comparison systems for the cross-\ndomain ABSA task.\nâ€¢ BERT-NoDA (Kenton and Toutanova, 2019):\na baseline system without domain adaptation,\nwhich directly fine-tunes a BERT-base model\non labeled source-domain data.\nâ€¢ BERT-Cross (Xu et al., 2019): a domain-\nadaptive BERT-CRF model, in which the BERT-\nbase model was post-trained on a myriad of E-\ncommerce data and the full model is fine-tuned\non labeled source-domain data.\nâ€¢ UDA (Gong et al., 2020): a unified domain adap-\ntation approach that integrates feature-based\nand instance-based adaptation for cross-domain\nABSA.\nâ€¢ FMIM (Chen and Wan, 2022): a feature-\nbased domain adaptation method, using the fine-\ngrained mutual information maximization tech-\nnique.\nâ€¢ CDRG (Yu et al., 2021): a cross-domain re-\nview generation approach that exploits each la-\nbeled source-domain review to generate a la-\nbeled target-domain review based on masked\nlanguage models.\nâ€¢ GCDDA (Li et al., 2022): a generative cross-\ndomain data augmentation framework that lever-\nages a pre-trained sequence-to-sequence model\nBART to generate target-domain data with fine-\ngrained annotation.\nThe comparison results on the cross-domain\nABSA and AE task are reported in Table 2. For\nour proposed framework, we present the results\nof both LSTM and GPT-2-based DA2LM. We can\nobserve that our framework generally achieves the\nbest performance on most cross-domain pairs and\nDA2LM outperforms the state-of-the-art method\nby 1.86% and 0.90% on average for the ABSA and\nAE task respectively. We conjecture the reasons\nas follows. First, DA 2LM can directly generate\nnumerous high-quality target domain labeled data,\nthereby overcoming the sensitivity to source data\nin feature-based domain adaptation methods. Sec-\nond, there is still a considerable distribution dis-\ncrepancy between the generated data in previous\ncross-domain data augmentation methods and the\nreal target-domain data because these methods pre-\nserve source-specific attributes such as syntactic\nstructures. Moreover, since previous cross-domain\ndata augmentation methods are based on the word\nreplacement technology, the fluency and diversity\n1461\nTasks Methods Sâ†’R S â†’L S â†’D Râ†’S R â†’L R â†’D Lâ†’S L â†’R Dâ†’S D â†’R A VE\nABSA\nBERT-NoDA 49.85 33.08 35.97 27.63 32.69 32.45 27.77 37.38 31.87 42.74 35.14\nBERT-Cross 51.36 34.33 36.28 26.38 42.42 40.82 28.35 49.91 27.31 47.92 38.51\nUDA 52.04 35.41 38.06 30.76 46.00 40.81 30.34 49.97 33.28 50.72 40.74\nFMIM 49.46 31.83 32.46 40.59 39.26 33.11 41.61 57.02 40.76 55.68 42.21\nCDRG 52.93 33.33 36.14 43.07 44.70 30.82 41.51 57.77 40.30 53.18 43.38\nGCDDA 55.66 36.53 36.87 32.07 47.79 40.35 27.22 50.50 28.52 49.47 40.50\nDA2LM (LSTM) 56.26 36.54 39.80 40.38 42.49 40.55 35.93 59.47 33.55 57.28 44.22\nDA2LM (GPT-2) 58.64 36.97 40.28 40.44 42.91 41.28 36.84 60.39 35.75 58.98 45.24\nAE\nBERT-NoDA 57.72 40.33 39.69 31.21 38.38 35.15 31.44 41.11 34.46 45.79 39.53\nBERT-Cross 58.08 40.47 39.89 27.74 51.49 42.52 30.84 54.96 28.69 50.97 42.57\nUDA 57.98 42.44 40.24 35.29 57.58 43.07 33.96 54.79 35.78 53.85 45.50\nFMIM 57.43 39.14 35.26 47.60 50.57 36.11 51.68 68.67 49.53 61.64 49.76\nCDRG 60.20 39.49 38.59 49.97 55.50 34.89 51.07 68.63 43.19 57.51 49.90\nGCDDA 63.53 43.95 39.16 35.69 64.06 44.25 30.31 58.00 30.74 53.70 46.34\nDA2LM (LSTM) 63.63 44.39 42.39 43.38 57.12 43.64 39.44 67.24 36.16 62.66 50.00\nDA2LM (GPT-2) 65.78 44.96 43.24 43.41 54.55 44.29 41.06 68.72 38.20 63.86 50.80\nTable 2: Main results for Cross-Domain ABSA and AE based on Micro-F1. All results are based on our re-implementation.\nof generated data in these methods are inferior to\nour DA2LM approach.\nIn addition to the above observations, Table\n2 shows that LSTM-based DA 2LM is similar to\nGPT-2-based DA2LM and also outperforms previ-\nous domain adaptation methods on average, which\nimplies that our cross-domain data augmentation\nframework is robust and does not rely on the pre-\ntrained language model.\nFurthermore, as shown in Table 1 and Table 2,\nthe proposed model underperforms several baseline\nsystems when the source/target sample size ratio\nis larger than 1 (e.g., R â†’S, L â†’S, D â†’S, R\nâ†’L). We believe the reason of the performance\ndrop is as follows: when the number of target-\ndomain data is less than that of source-domain data,\nit will inevitably lead the Domain-Adaptive Lan-\nguage Model (DALM) to pay more attention to\nsource-domain data instead of target-domain data.\nHence, in the target-domain data generation pro-\ncess, the trained DALM may still generate source-\nspecific words, and thus bring negative effects to\nthe final performance.\n4.3 Ablation Study\nTo explore the effects of each component in\nDA2LM, we show the results of our ablation study\nin Table 3.\nFirstly, after removing the aspect-level MMD\nloss in the domain-adaptive pseudo labeling\n(DAPL) stage, the average performance on 10\ncross-domain pairs drops dramatically, which in-\ndicates that it is important to alleviate the domain\ndiscrepancy via the MMD loss in DAPL. Secondly,\nremoving the domain-adaptive language modeling\nMethods ABSA AE\nDA2LM 45.24 50.80\n- w/o MMD loss in DAPL 39.44 43.57\n- w/o DALM & DG 42.53 48.03\n- w/o source-domain data in DALM 43.82 50.16\n- w/o malposed generation 42.82 48.23\n- replace DALM with DAGA 44.23 50.40\nTable 3: Ablation studies of each component in DA 2LM.\nDAPL, DALM, and DG respectively denote Domain-Adaptive\nPseudo Labeling, Domain-Adaptive Language Modeling, and\ntarget-domain Data Generation. Ablation without malposed\ngeneration means that the next token and label are generated\nsimultaneously in one time step.\n(DALM) and target-domain data-generation (DG)\nstages decreases the average F1 score by 2.71 ab-\nsolute percentage points. This shows that automati-\ncally generating a large amount of target-domain\nlabeled data plays an indispensable role in our\nDA2LM framework. Thirdly, for the training of\nDALM, the removal of source-domain labeled data\nalso leads to a significant drop in the average F1\nscore. This implies that the source-domain data\nis indeed helpful for capturing domain-invariant\ncontext and annotation.\nMoreover, we remove the malposed generation\nstrategy, which means it does not take the current\ntoken into account when predicting the label of the\ncurrent token. As shown in Table 3, the perfor-\nmance of DA2LM drops dramatically since it gen-\nerates low-quality label sequences. Lastly, because\na language model-based data augmentation method\nDAGA (Ding et al., 2020) has shown success in\nstandard in-domain ABSA tasks, we propose to\nreplace DALM in our DA2LM framework with a\nvariant of DAGA, i.e., a language model trained\non source and target-domain data with linearized\n1462\nCriterion Methods Sâ†’R S â†’L S â†’D Râ†’S R â†’L R â†’D Lâ†’S L â†’R Dâ†’S D â†’R A VE\nDiversity\nCDRG 0.133 0.134 0.146 0.250 0.235 0.289 0.229 0.193 0.293 0.264 0.2165\nGCDDA 0.226 0.203 0.207 0.236 0.208 0.227 0.247 0.241 0.297 0.266 0.2362\nDA2LM 0.275 0.309 0.354 0.472 0.269 0.374 0.416 0.252 0.503 0.257 0.3487\nPerplexity\nCDRG 583.8 611.0 484.2 971.8 1106.9 971.5 567.5 620.9 625.4 697.0 724.00\nGCDDA 244.9 215.2 217.8 806.0 782.0 763.8 469.1 392.0 442.9 480.0 481.35\nDA2LM 362.8 237.4 214.9 182.1 257.8 254.9 204.8 389.8 200.6 360.3 266.53\nMMD\nSource 0.733 0.651 0.650 0.724 0.634 0.763 0.657 0.691 0.624 0.693 0.6819\nCDRG 0.603 0.697 0.576 0.604 0.552 0.631 0.631 0.622 0.556 0.617 0.6088\nGCDDA 0.800 0.541 0.559 0.772 0.547 0.561 0.759 0.567 0.603 0.600 0.6310\nDA2LM 0.560 0.566 0.498 0.548 0.487 0.559 0.597 0.533 0.677 0.535 0.5564\nTable 4: Comparison results between the generated data in DA2LM and those in CDRG and GCDDA.\n/uni0000005e/uni0000017d/uni000001b5/uni0000018c/uni00000110/uni0000011e/uni00000003/uni00000018/uni00000102/uni0000019a/uni00000102\n/uni00000064/uni00000102/uni0000018c/uni00000150/uni0000011e/uni0000019a/uni00000003/uni00000018/uni00000102/uni0000019a/uni00000102\n/uni00000027/uni0000011e/uni00000176/uni0000011e/uni0000018c/uni00000102/uni0000019a/uni0000011e/uni0000011a/uni00000003/uni00000018/uni00000102/uni0000019a/uni00000102/uni00000003/uni0000015d/uni00000176/uni00000003/uni00000012/uni00000018/uni0000005a/uni00000027\n/uni0000005e/uni0000017d/uni000001b5/uni0000018c/uni00000110/uni0000011e/uni00000003/uni00000018/uni00000102/uni0000019a/uni00000102\n/uni00000064/uni00000102/uni0000018c/uni00000150/uni0000011e/uni0000019a/uni00000003/uni00000018/uni00000102/uni0000019a/uni00000102\n/uni00000027/uni0000011e/uni00000176/uni0000011e/uni0000018c/uni00000102/uni0000019a/uni0000011e/uni0000011a/uni00000003/uni00000018/uni00000102/uni0000019a/uni00000102/uni00000003/uni0000015d/uni00000176/uni00000003/uni00000027/uni00000012/uni00000018/uni00000018/uni00000004\n/uni0000005e/uni0000017d/uni000001b5/uni0000018c/uni00000110/uni0000011e/uni00000003/uni00000018/uni00000102/uni0000019a/uni00000102\n/uni00000064/uni00000102/uni0000018c/uni00000150/uni0000011e/uni0000019a/uni00000003/uni00000018/uni00000102/uni0000019a/uni00000102\n/uni00000027/uni0000011e/uni00000176/uni0000011e/uni0000018c/uni00000102/uni0000019a/uni0000011e/uni0000011a/uni00000003/uni00000018/uni00000102/uni0000019a/uni00000102/uni00000003/uni0000015d/uni00000176/uni00000003/uni00000018/uni000000042/uni0000003e/uni00000044\nFigure 3: Visualization of the distribution discrepancy between the generated\ndata in different methods and the source/target-domain data on a cross-domain\npair S â†’R. Each point represents a sample.\nMethods ABSA AE\nDA2LM 45.24 50.80\nUDA 40.74 45.50\nDA2LM-UDA 42.02 47.30\nFMIM 39.31 49.26\nDA2LM-FMIM 45.94 53.79\nCDRG 43.38 49.90\nDA2LM-CDRG 45.71 52.99\nTable 5: Average results of replacing\nour base model in DAPL with existing\ndomain adaptation methods.\nlabels before each aspect term. For fair comparison,\nwe also employ GPT-2 (Radford et al., 2019) as the\npre-trained language model. As shown at the bot-\ntom of Table 3, replacing DALM with DAGA leads\nto a moderate performance drop, which proves the\nimportance of DALM in our DA2LM approach.\n4.4 Evaluation on Generated Data\nIn this subsection, we conduct additional experi-\nments to evaluate the quality of data generated by\nDA2LM and report the performance in Table 4.\nDiversity. Diversity denotes the percentage of\nunique aspect terms in all aspect terms. The results\nin Table 4 clearly show that DA 2LM can gener-\nate more aspect terms since other methods need\nto regard source-domain sample as the template.\nMoreover, our framework employs a probability-\nbased sampling strategy to generate the next token,\nwhich can improve the diversity of generated aspect\nterms.\nPerplexity. To evaluate the coherence of gener-\nated data, we further calculate the perplexity 1 of\ndata generated from each compared method based\non a pre-trained language model GPT-2. 2 In the\nfourth to sixth rows of Table 4, it is clear to see\n1https://huggingface.co/spaces/\nevaluate-measurement/perplexity\n2Note that different from Li et al. (2022) which uses 2 as\nthe base of the exponential function, we employ e as the base.\nthat the perplexity of our DA 2LM framework is\nsignificantly lower than that of other methods. This\nshows that for MLM-based and Seq2Seq-based\nCDDA methods, simply replacing source-specific\nattributes with target-specific attributes may break\nthe syntactic structure of the original sentence and\nthus the generated sentences are not coherent. In\ncontrast, our DA2LM framework relies on language\nmodeling to automatically generate tokens and their\ncorresponding labels in an autoregressive manner.\nMaximum Mean Discrepancy (MMD). MMD\nis used to measure the distribution distance between\nthe generated data in different methods and the real\ntarget-domain test data. The results in the last four\nrows show that the generated data in DA2LM are\nmuch closer to the target domain than other meth-\nods, which indicates DA 2LM can generate more\nauthentic target-domain data and better alleviate\nthe distribution discrepancy across domains.\nVisualization. To visually verify the superiority\nof our DA2LM framework, we further utilize t-SNE\n(Van der Maaten and Hinton, 2008) to perform a vi-\nsualization of the sentence representations obtained\nby a pre-trained language model BERT (Kenton\nand Toutanova, 2019). Figure 3 shows the visual-\nization result on a cross-domain pair S â†’R. As\nshown in Figure 3, the distribution of generated\ndata in CDRG and GCDDA is still similar to that\nof source-domain data because these methods still\n1463\npreserve many source-domain attributes including\ncontexts and syntactic structures. In contrast, there\nis almost no discrepancy between the generated\ndata in DA 2LM and the target-domain data, as\nshown in the right of Figure 3.\nThese observations demonstrate the advantage\nof DA2LM over previous CDDA methods in terms\nof diversity, fluency, and data distribution.\n4.5 Compatibility with Existing DA Methods\nTo show the compatibility of our DA2LM frame-\nwork, we replace the base model Cb in the first\nstage (i.e., domain-adaptive pseudo labeling) with\nother existing domain adaptation methods includ-\ning UDA (Gong et al., 2020), FMIM (Chen and\nWan, 2022) and CDRG (Yu et al., 2021).\nTable 5 shows the average results of different\nbase models with their DA 2LM variants on 10\nsource â†’target domain pairs for the cross-domain\nABSA task and the cross-domain AE task, respec-\ntively. Firstly, we can find that by using the target-\ndomain labeled data from our DA2LM framework,\nthe performance of existing domain adaptation\nmethods is generally boosted on average for cross-\ndomain ABSA and AE, which demonstrates the\nusefulness of our DA2LM framework and the ro-\nbustness of the generated target-domain data. Sec-\nondly, by comparing all DA2LM variants, we can\nobserve that DA2LM-FMIM consistently obtains\nthe best average performance on cross-domain\nABSA and AE. This suggests that our DA 2LM\nframework is compatible with any domain adap-\ntation method, and it can generally achieve better\nresults with better base models.\n5 Conclusion\nIn this paper, we proposed a cross-domain\nData Augmentation framework based on Domain-\nAdaptive Language Modeling (DA 2LM), which\ncontains three key stages to automatically gen-\nerate sufficient target-domain labeled data, in-\ncluding 1) Domain-Adaptive Pseudo Labeling,\n2) Domain-Adaptive Language Modeling, and 3)\nTarget-Domain Data Generation. Experiments on\nfour benchmark datasets show that our DA 2LM\nframework consistently outperforms the state-of-\nthe-art method for the cross-domain ABSA task.\nMoreover, further evaluation results demonstrate\nthe superiority of the generated data in terms of\ndiversity, fluency, and data distribution.\nLimitations\nDespite obtaining promising results, our proposed\napproach still has the following limitations.\nFirst, although our DA2LM approach can gener-\nate a large amount of target-domain data with high\ndiversity, the generated words are still limited by\nthe source-domain labeled data and target-domain\nunlabeled data. How to make the model generate\nnovel target-domain words is a challenging prob-\nlem to explore in the future.\nSecond, our DA 2LM model is primarily pro-\nposed for the ABSA and AE tasks, which are not\ndirectly applicable for the other information ex-\ntraction tasks with more than two elements, such\nas Aspect Sentiment Triplet Extraction (ASTE).\nTherefore, cross-domain data augmentation for\nmultiple-element information extraction tasks may\nbe a promising followup direction.\nEthics Statement\nWe conduct experiments on four publicly available\ndatasets, i.e., Laptop (L), Restaurant (R), Device\n(D), and Service (S). These datasets do not share\npersonal information and do not contain sensitive\ncontent that can be harmful to any individual or\ncommunity. Due to the lack of ethics and bias con-\nstraint in the data generation process, the generated\ndata from our trained Domain-Adaptive Language\nModel may contain sensitive and misleading con-\ntent. Therefore, it is necessary to manually check\nthese generated data when applying them to real-\nworld applications.\nAcknowledgements\nThe authors would like to thank the anonymous\nreviewers for their insightful comments. This work\nwas supported by the Natural Science Foundation\nof China (62076133 and 62006117), and the Nat-\nural Science Foundation of Jiangsu Province for\nYoung Scholars (BK20200463) and Distinguished\nYoung Scholars (BK20200018).\nReferences\nJohn Blitzer, Mark Dredze, and Fernando Pereira. 2007.\nBiographies, bollywood, boom-boxes and blenders:\nDomain adaptation for sentiment classification. In\nProceedings of ACL, pages 440â€“447.\nPeng Chen, Zhongqian Sun, Lidong Bing, and Wei Yang.\n2017. Recurrent attention network on memory for\n1464\naspect sentiment analysis. In Proceedings of EMNLP,\npages 452â€“461.\nShuguang Chen, Gustavo Aguilar, Leonardo Neves, and\nThamar Solorio. 2021. Data augmentation for cross-\ndomain named entity recognition. In Proceedings of\nEMNLP, pages 5346â€“5356.\nXiang Chen and Xiaojun Wan. 2022. A simple\ninformation-based approach to unsupervised domain-\nadaptive aspect-based sentiment analysis. arXiv\npreprint arXiv:2201.12549.\nZhuang Chen and Tieyun Qian. 2020a. Enhancing as-\npect term extraction with soft prototypes. In Proceed-\nings of EMNLP, pages 2107â€“2117.\nZhuang Chen and Tieyun Qian. 2020b. Relation-aware\ncollaborative learning for unified aspect-based senti-\nment analysis. In Proceedings of ACL, pages 3685â€“\n3694.\nZhuang Chen and Tieyun Qian. 2021. Bridge-based\nactive domain adaptation for aspect term extraction.\nIn Proceedings of ACL/IJCNLP, pages 317â€“327.\nZhuang Chen and Tieyun Qian. 2022. Retrieve-and-edit\ndomain adaptation for end2end aspect based senti-\nment analysis. IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, 30:659â€“672.\nBosheng Ding, Linlin Liu, Lidong Bing, Canasai Kru-\nengkrai, Thien Hai Nguyen, Shafiq Joty, Luo Si, and\nChunyan Miao. 2020. Daga: Data augmentation with\na generation approach for low-resource tagging tasks.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, pages 6045â€“6057.\nYing Ding, Jianfei Yu, and Jing Jiang. 2017. Recur-\nrent neural networks with auxiliary labels for cross-\ndomain opinion target extraction. In Proceedings of\nAAAI, volume 31.\nYaroslav Ganin and Victor Lempitsky. 2015. Unsu-\npervised domain adaptation by backpropagation. In\nProceedings of ICML, pages 1180â€“1189.\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,\nPascal Germain, Hugo Larochelle, FranÃ§ois Lavi-\nolette, Mario Marchand, and Victor Lempitsky. 2016.\nDomain-adversarial training of neural networks. The\nJournal of Machine Learning Research, 17(1):2096â€“\n2030.\nXavier Glorot, Antoine Bordes, and Yoshua Bengio.\n2011. Domain adaptation for large-scale sentiment\nclassification: a deep learning approach. In Proceed-\nings of ICML, pages 513â€“520.\nChenggong Gong, Jianfei Yu, and Rui Xia. 2020. Uni-\nfied feature and instance based domain adaptation for\naspect-based sentiment analysis. In Proceedings of\nEMNLP, pages 7035â€“7045.\nArthur Gretton, Karsten M Borgwardt, Malte J Rasch,\nBernhard SchÃ¶lkopf, and Alexander Smola. 2012.\nA kernel two-sample test. The Journal of Machine\nLearning Research, 13(1):723â€“773.\nRuidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel\nDahlmeier. 2018. Adaptive semi-supervised learning\nfor cross-domain sentiment classification. In Pro-\nceedings of EMNLP, pages 3467â€“3476.\nRuidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel\nDahlmeier. 2019. An interactive multi-task learn-\ning network for end-to-end aspect-based sentiment\nanalysis. In Proceedings of ACL, pages 504â€“515.\nRuining He and Julian McAuley. 2016. Ups and downs:\nModeling the visual evolution of fashion trends with\none-class collaborative filtering. In Proceedings of\nWWW, pages 507â€“517.\nMinqing Hu and Bing Liu. 2004. Mining and summa-\nrizing customer reviews. In Proceedings of the tenth\nACM SIGKDD international conference on Knowl-\nedge discovery and data mining, pages 168â€“177.\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nProceedings of NAACL-HLT, pages 4171â€“4186.\nEntony Lekhtman, Yftah Ziser, and Roi Reichart. 2021.\nDilbert: Customized pre-training for domain adapta-\ntion with category shift, with an application to aspect\nextraction. In Proceedings of EMNLP, pages 219â€“\n230.\nJunjie Li, Jianfei Yu, and Rui Xia. 2022. Generative\ncross-domain data augmentation for aspect and opin-\nion co-extraction. In Proceedings of NAACL, pages\n4219â€“4229.\nXin Li, Lidong Bing, Piji Li, and Wai Lam. 2019a. A\nunified model for opinion target extraction and target\nsentiment prediction. In Proceedings of AAAI, pages\n6714â€“6721.\nXin Li, Lidong Bing, Wenxuan Zhang, and Wai Lam.\n2019b. Exploiting bert for end-to-end aspect-based\nsentiment analysis. In Proceedings of the 5th Work-\nshop on Noisy User-generated Text (W-NUT 2019),\npages 34â€“41.\nZheng Li, Xin Li, Ying Wei, Lidong Bing, Yu Zhang,\nand Qiang Yang. 2019c. Transferable end-to-end\naspect-based sentiment analysis with selective adver-\nsarial learning. In Proceedings of EMNLP-IJCNLP,\npages 4590â€“4600.\nZheng Li, Ying Wei, Yu Zhang, and Qiang Yang. 2018.\nHierarchical attention transfer network for cross-\ndomain sentiment classification. In Proceedings of\nAAAI.\nBing Liu. 2012. Sentiment analysis and opinion mining.\nSynthesis Lectures on Human Language Technolo-\ngies, 5(1):1â€“167.\n1465\nPengfei Liu, Shafiq Joty, and Helen Meng. 2015. Fine-\ngrained opinion mining with recurrent neural net-\nworks and word embeddings. In Proceedings of\nEMNLP, pages 1433â€“1443.\nHuaishao Luo, Tianrui Li, Bing Liu, and Junbo Zhang.\n2019. Doer: Dual cross-shared rnn for aspect term-\npolarity co-extraction. In Proceedings of ACL, pages\n591â€“601.\nSinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang\nYang, and Zheng Chen. 2010. Cross-domain senti-\nment classification via spectral feature alignment. In\nProceedings of WWW 2010, pages 751â€“760.\nMaria Pontiki, Dimitrios Galanis, Haris Papageorgiou,\nIon Androutsopoulos, Suresh Manandhar, AL Mo-\nhammad, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing\nQin, OrphÃ©e De Clercq, et al. 2016. Semeval-2016\ntask 5: Aspect based sentiment analysis. Proceed-\nings of the 10th international workshop on semantic\nevaluation (SemEval 2016), pages 19â€“30.\nMaria Pontiki, Dimitrios Galanis, Harris Papageorgiou,\nSuresh Manandhar, and Ion Androutsopoulos. 2015.\nSemeval-2015 task 12: Aspect based sentiment analy-\nsis. In Proceedings of the 9th international workshop\non semantic evaluation (SemEval 2015), pages 486â€“\n495.\nMaria Pontiki, Dimitris Galanis, John Pavlopoulos, Har-\nris Papageorgiou, Ion Androutsopoulos, and Auresh\nManandhar. 2014. Semeval-2014 task 4: Aspect\nbased sentiment analysis. In Proceedings of the 8th\ninternational workshop on semantic evaluation (Se-\nmEval 2014), pages 27â€“35.\nGuang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.\n2011. Opinion word expansion and target extrac-\ntion through double propagation. Computational\nlinguistics, 37(1):9â€“27.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nChi Sun, Luyao Huang, and Xipeng Qiu. 2019. Uti-\nlizing bert for aspect-based sentiment analysis via\nconstructing auxiliary sentence. In Proceedings of\nNAACL-HLT, pages 380â€“385.\nCigdem Toprak, Niklas Jakob, and Iryna Gurevych.\n2010. Sentence and expression level annotation of\nopinions in user-generated discourse. In Proceedings\nof the 48th Annual Meeting of the Association for\nComputational Linguistics, ACL 2010 , pages 575â€“\n584.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. The Journal of Machine\nLearning Research, 9(11).\nFeixiang Wang, Man Lan, and Wenting Wang. 2018.\nTowards a one-stop solution to both aspect extraction\nand sentiment analysis tasks with neural multi-task\nlearning. In Proceedings of IJCNN, pages 1â€“8. IEEE.\nKai Wang, Weizhou Shen, Yunyi Yang, Xiaojun Quan,\nand Rui Wang. 2020. Relational graph attention net-\nwork for aspect-based sentiment analysis. In Pro-\nceedings of ACL, pages 3229â€“3238.\nWenya Wang and Sinno Jialin Pan. 2018. Recursive\nneural structural correspondence network for cross-\ndomain aspect and opinion co-extraction. In Proceed-\nings of ACL, pages 2171â€“2181.\nWenya Wang and Sinno Jialin Pan. 2019. Transferable\ninteractive memory network for domain adaptation\nin fine-grained opinion extraction. In Proceedings of\nAAAI, pages 7192â€“7199.\nWenya Wang and Sinno Jialin Pan. 2020. Syntacti-\ncally meaningful and transferable recursive neural\nnetworks for aspect and opinion extraction. Compu-\ntational Linguistics, 45(4):705â€“736.\nDongbo Xi, Fuzhen Zhuang, Ganbin Zhou, Xiaohu\nCheng, Fen Lin, and Qing He. 2020. Domain adap-\ntation with category attention network for deep sen-\ntiment analysis. In Proceedings of The Web Confer-\nence 2020, pages 3133â€“3139.\nHu Xu, Bing Liu, Lei Shu, and S Yu Philip. 2019. Bert\npost-training for review reading comprehension and\naspect-based sentiment analysis. In NAACL-HLT,\npages 2324â€“2335.\nLinyi Yang, Lifan Yuan, Leyang Cui, Wenyang Gao,\nand Yue Zhang. 2022. Factmix: Using a few labeled\nin-domain examples to generalize to cross-domain\nnamed entity recognition. In Proceedings of COL-\nING, pages 5360â€“5371.\nHai Ye, Qingyu Tan, Ruidan He, Juntao Li, Hwee Tou\nNg, and Lidong Bing. 2020. Feature adaptation of\npre-trained language models across languages and\ndomains with robust self-training. In Proceedings of\nEMNLP, pages 7386â€“7399.\nJianfei Yu, Chenggong Gong, and Rui Xia. 2021. Cross-\ndomain review generation for aspect-based sentiment\nanalysis. In Findings of the Association for Com-\nputational Linguistics: ACL/IJCNLP 2021 , pages\n4767â€“4777.\nJianfei Yu and Jing Jiang. 2016. Learning sentence\nembeddings with auxiliary tasks for cross-domain\nsentiment classification. In Proceedings of EMNLP,\npages 236â€“246.\nYushi Zeng, Guohua Wang, Haopeng Ren, and Yi Cai.\n2022. Enhance cross-domain aspect-based sentiment\nanalysis by incorporating commonsense relational\nstructure (student abstract). In Proceedings of AAAI,\npages 13105â€“13106.\nMeishan Zhang, Yue Zhang, and Duy-Tin V o. 2016.\nGated neural networks for targeted sentiment analy-\nsis. In Proceedings of AAAI, pages 3087â€“3093.\n1466\nGuangyou Zhou, Zhiwen Xie, Xiangji Huang, and Tingt-\ning He. 2016. Bi-transferring deep neural networks\nfor domain adaptation. In Proceedings of ACL, pages\n322â€“332.\nFuzhen Zhuang, Xiaohu Cheng, Ping Luo, Sinno Jialin\nPan, and Qing He. 2015. Supervised representation\nlearning: Transfer learning with deep autoencoders.\nIn Proceedings of IJCAI.\nFuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi,\nYongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing\nHe. 2020. A comprehensive survey on transfer learn-\ning. Proceedings of the IEEE, 109(1):43â€“76.\nYftah Ziser and Roi Reichart. 2018. Pivot based lan-\nguage modeling for improved neural domain adap-\ntation. In Proceedings of NAACL-HLT, pages 1241â€“\n1251.\nA Appendix\nA.1 Case Study and Error Analysis\nIn this section, we select several representative ex-\namples generated by different methods to demon-\nstrate the effectiveness of our DA2LM framework.\nCase Study. Table 6 shows several examples of\nCDRG, GCDDA and DA2LM on a cross-domain\npair L â†’R. Firstly, we can observe that the MLM-\nbased approach CDRG and the Seq2Seq-based\napproach GCDDA fail to replace some source-\nspecific words such as â€œlaptopâ€ and â€œMiscrosoft of-\nficeâ€ with target-specific words. Besides, it is clear\nthat the generated target-domain data in CDRG and\nGCDDA are lack of fluency, coherence, and diver-\nsity, because they both generate target-domain data\nbased on a source template sentence by replacing\nwords. In contrast, our DA2LM approach can gen-\nerate much more diverse target-domain data due to\nthe randomness of sampling. Moreover, because\nthe DALM in our framework is based on the lan-\nguage model, it is not surprising that the sentences\ngenerated in DA2LM are generally fluent and co-\nherent.\nError Analysis. Furthermore, we also manually\nverify the label correctness of the target-domain\ndata generated from our DA2LM framework, and\nshow two generated samples with incorrect labels\nat the bottom of Table 6. We find that DA2LM is\nprone to identify a target-specific attribute as an as-\npect term, even if it is not the target of the sentiment\nexpression (e.g., â€œrestaurantsâ€) or is an incomplete\naspect term (e.g., â€œsakeâ€). We conjecture the reason\nis our adoption of a rule-based algorithm to obtain\nthe target-domain aspect terms to minimize the dis-\ntance between source-domain and target-domain\naspect term representations in Section 3.3, which\nmay result in the noise in the pseudo-labeled target\ndata for Aspect Term Extraction. However, the re-\nsults and analysis in Section 4.5 demonstrate that\nour DA2LM framework is generally compatible\nwith various domain adaptation methods and has\nthe potential to deliver better performance when\nemployed in conjunction with more powerful base\nmodels.\nA.2 Detailed Evaluation on the Compatibility\nwith Existing DA Methods\nTable 7 and Table 8 show the detailed comparison\nresults of different base models with their DA2LM\nvariants on all domain-pairs for the cross-domain\nABSA task and the cross-domain AE task. We can\nobserve that the variants of our DA2LM show con-\nsistent improvements over different base models\non most domain pairs for both tasks.\n1467\nExamples\nSource The [engineering design]positive and [warranty]positive are superiorâ€“covers damage from dropping the laptop.\nCDRG The [wait service]positive and [flavoring]positive are superiorâ€“keep distract from dropping the laptop.\nGCDDA The [engineering design]positive and [service]positive are superiorâ€“covers damage from dropping the food.\nSource There is no [cd drive]negative on the computer, which defeats the purpose of keeping files on a cd.\nCDRG There is no [fire place]negative on the computer, which defeats the purpose of keeping files on a cd.\nGCDDA There is no [cheese plate]negative in the menu, which defeats the purpose of keeping files on a cd.\nSource Itâ€™s [applications]positive are terrific, including the replacements for [Microsoft office]positive.\nCDRG Itâ€™s [drinks]positive are terrific, including the noodles for [cheeses]positive.\nGCDDA Itâ€™s [salads]positive are terrific, including the replacements for [Microsoft office]positive.\nDA2LM\nwe always have a delicious [meal]positive and always leave feeling satisfied. âœ“\nthe [prices]positive were exceptionally reasonable for the [appetizers]positive and [food]positive we ordered. âœ“\nthe [stuff tilapia]negative was horridtasted like cardboard. âœ“\nthe place is a bistro which means, simple [dishes]positive served efficiently in a bustling [atmosphere]positive. âœ“\nthe [food]positive was adequate, but the [restaurant]negative was too tiny. âœ“\nbut, i think citysearch is a great place to find [restaurants]positive. âœ—\ntheir [sake]positive list was extensive, but we were looking for purple haze, which wasnâ€™t listed. âœ—\nTable 6: Examples of different methods on a cross-domain pair L â†’R. For baseline systems, text chunks in blue\nindicate the replaced target-specific attributes and text chunks in red indicate the remaining source-specific attributes\nin generated target-domain data. For our DA2LM approach, âœ“ and âœ— indicate that the generated label sequences are\ncorrect and incorrect, respectively.\nMethods Sâ†’R S â†’L S â†’D Râ†’S R â†’L R â†’D Lâ†’S L â†’R Dâ†’S D â†’R A VE\nDA2LM 58.64 36.97 40.28 40.44 42.91 41.28 36.84 60.39 35.75 58.98 45.24\nUDA 52.04 35.41 38.06 30.76 46.00 40.81 30.34 49.97 33.28 50.72 40.74\nDA2LM-UDA 56.05 35.15 40.45 26.40 45.78 44.18 28.43 53.28 37.90 52.57 42.02\nFMIM 49.46 31.83 32.46 40.59 39.26 33.11 41.61 57.02 40.76 55.68 42.21\nDA2LM-FMIM 54.05 32.36 35.57 47.01 41.78 38.93 45.80 59.66 47.66 56.62 45.94\nCDRG 52.93 33.33 36.14 43.07 44.70 30.82 41.51 57.77 40.30 53.18 43.38\nDA2LM-CDRG 56.81 34.10 38.43 45.06 44.85 30.11 49.44 61.02 40.56 56.80 45.71\nTable 7: Compatibility with existing domain adaptation methods for Cross-Domain ABSA.\nMethods Sâ†’R S â†’L S â†’D Râ†’S R â†’L R â†’D Lâ†’S L â†’R Dâ†’S D â†’R A VE\nDA2LM 65.78 44.96 43.24 43.41 54.55 44.29 41.06 68.72 38.20 63.86 50.80\nUDA 57.98 42.44 40.24 35.29 57.58 43.07 33.96 54.79 35.78 53.85 45.50\nDA2LM-UDA 62.42 42.12 42.84 32.29 59.84 46.60 31.69 58.23 41.07 55.85 47.30\nFMIM 57.43 39.14 35.26 47.60 50.57 36.11 51.68 68.67 49.53 61.64 49.76\nDA2LM-FMIM 62.37 41.90 38.43 52.98 56.24 42.29 55.63 70.95 53.46 63.63 53.79\nCDRG 60.20 39.49 38.59 49.97 55.50 34.89 51.07 68.63 43.19 57.51 49.90\nDA2LM-CDRG 64.20 41.78 41.58 52.81 59.16 34.88 56.32 71.29 46.18 61.66 52.99\nTable 8: Compatibility with existing domain adaptation methods for Cross-Domain Aspect Extraction (AE).\n1468\nACL 2023 Responsible NLP Checklist\nA For every submission:\nâ–¡\u0013 A1. Did you describe the limitations of your work?\nSection Limitations\nâ–¡\u0013 A2. Did you discuss any potential risks of your work?\nSection Ethics Statement\nâ–¡\u0013 A3. Do the abstract and introduction summarize the paperâ€™s main claims?\nAbstract and senction Introduction\nâ–¡\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB â–¡\u0013 Did you use or create scientiï¬c artifacts?\nWe use the pre-trained language model GPT-2 as mentioned in Section 3.\nâ–¡\u0013 B1. Did you cite the creators of artifacts you used?\nIn Section 3 named Methodology.\nâ–¡\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nWe use publicly available pretrained language models and datasets from previous works.\nâ–¡\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciï¬ed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nIn Section 3, we discuss in detail how to use the scientiï¬c artifact. And we introduce the intended use\nof our framework in Section 1.\nâ–¡\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiï¬es individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nThe data we use is based on publicly available datasets, which have been checked and pre-\nprocessedby previous works.\nâ–¡\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nWe describe the key stages and settings in Section 3 and Section 4 in detail.\nâ–¡\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiï¬cant, while on small test sets they may not be.\nWe describe the dataset we use in Section 4.\nC â–¡\u0013 Did you run computational experiments?\nIn Section 4.\nâ–¡\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nWe describe the parameters setting and computing infrastructure in Section 4.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1469\nâ–¡\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nWe describe the experiment setup in Section 4.\nâ–¡\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nWe describe them in Section 4.\nâ–¡\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nWe describe them in Section 4.\nD â–¡\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\nâ–¡ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\nâ–¡ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participantsâ€™ demographic\n(e.g., country of residence)?\nNo response.\nâ–¡ D3. Did you discuss whether and how consent was obtained from people whose data youâ€™re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\nâ–¡ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\nâ–¡ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n1470"
}