{
    "title": "Assessing the Syntactic Capabilities of Transformer-based Multilingual Language Models",
    "url": "https://openalex.org/W3173829323",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5043850175",
            "name": "Laura Pérez-Mayos",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3161365484",
            "name": "Alba Táboas García",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2003600963",
            "name": "Simon Mille",
            "affiliations": [
                "Pompeu Fabra University"
            ]
        },
        {
            "id": "https://openalex.org/A1828080704",
            "name": "Leo Wanner",
            "affiliations": [
                "Pompeu Fabra University",
                "Institució Catalana de Recerca i Estudis Avançats"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6631349028",
        "https://openalex.org/W2888922637",
        "https://openalex.org/W2963751529",
        "https://openalex.org/W3104235057",
        "https://openalex.org/W4385681388",
        "https://openalex.org/W4288351520",
        "https://openalex.org/W2972896975",
        "https://openalex.org/W3034510440",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W2531882892",
        "https://openalex.org/W2918996109",
        "https://openalex.org/W2891399254",
        "https://openalex.org/W3031001133",
        "https://openalex.org/W2946359678",
        "https://openalex.org/W3088059392",
        "https://openalex.org/W3035579820",
        "https://openalex.org/W2979736636",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W3037115370",
        "https://openalex.org/W2996728628",
        "https://openalex.org/W2962961857",
        "https://openalex.org/W3168987555",
        "https://openalex.org/W3015766957",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W3035064549",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3088382025",
        "https://openalex.org/W3018732874",
        "https://openalex.org/W2921890305",
        "https://openalex.org/W2549835527",
        "https://openalex.org/W2964303116",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2964204621",
        "https://openalex.org/W4289552613"
    ],
    "abstract": "Comunicació presentada a Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, celebrat de l'1 al 6 d'agost de 2021 de manera virtual.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3799–3812\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3799\nAssessing the Syntactic Capabilities of Transformer-based Multilingual\nLanguage Models\nLaura P´erez-Mayos1, Alba T´aboas Garc´ıa1, Simon Mille1, Leo Wanner2,1\n1 TALN Research Group, Pompeu Fabra University, Barcelona, Spain\n2 Catalan Institute for Research and Advanced Studies (ICREA), Barcelona, Spain\n{laura.perezm|alba.taboas|simon.mille|leo.wanner}@upf.edu\nAbstract\nMultilingual Transformer-based language\nmodels, usually pretrained on more than\n100 languages, have been shown to achieve\noutstanding results in a wide range of cross-\nlingual transfer tasks. However, it remains\nunknown whether the optimization for differ-\nent languages conditions the capacity of the\nmodels to generalize over syntactic structures,\nand how languages with syntactic phenomena\nof different complexity are affected. In this\nwork, we explore the syntactic generalization\ncapabilities of the monolingual and multilin-\ngual versions of BERT and RoBERTa. More\nspeciﬁcally, we evaluate the syntactic gener-\nalization potential of the models on English\nand Spanish tests, comparing the syntactic\nabilities of monolingual and multilingual\nmodels on the same language (English),\nand of multilingual models on two different\nlanguages (English and Spanish). For English,\nwe use the available SyntaxGym test suite;\nfor Spanish, we introduce SyntaxGymES, a\nnovel ensemble of targeted syntactic tests in\nSpanish, designed to evaluate the syntactic\ngeneralization capabilities of language models\nthrough the SyntaxGym online platform.\n1 Introduction\nTransformer-based neural models such as BERT\n(Devlin et al., 2019), RoBERTa (Liu et al., 2019b),\nDistilBERT (Sanh et al., 2019), XLNet (Yang et al.,\n2019), etc. are excellent learners. They have been\nshown to capture a range of different types of lin-\nguistic information, from morphological (Edmis-\nton, 2020) over syntactic (Hewitt and Manning,\n2019) to lexico-semantic (Joshi et al., 2020). A\nparticularly signiﬁcant number of works study the\ndegree to which these models capture and general-\nize over (i.e., learn to instantiate correctly in differ-\nent contexts) syntactic phenomena, including, e.g.,\nsubject-verb agreement, long distance dependen-\ncies, garden path constructions, etc. (Linzen et al.,\n2016; Marvin and Linzen, 2018; Futrell et al., 2019;\nWilcox et al., 2019a). However, most of these\nworks focus on monolingual models, and, if the\ncoverage of syntactic phenomena is considered sys-\ntematically and in detail, it is mainly for English, as,\ne.g., (Hu et al., 2020a). This paper aims to shift the\nattention from monolingual to multilingual models\nand to emphasize the importance to also consider\nthe syntactic phenomena of languages other than\nEnglish when assessing the generalization potential\nof a model. More speciﬁcally, it systematically as-\nsesses how well multilingual models are capable to\ngeneralize over certain syntactic phenomena, com-\npared to monolingual models, and how well they\ncan do it not only for English, but also for Spanish.\nMultilingual models such as mBERT (multilin-\ngual BERT, (Devlin et al., 2019)), XLM (Lample\nand Conneau, 2019) and XLM-R (Conneau et al.,\n2020) proved to achieve outstanding performance\non cross-lingual language understanding tasks, in-\ncluding on low-resource languages for which only\nlittle training data is available. However, these\nmodels face the risk of running into what Conneau\net al. (2020) refer to as “curse of multilinguality”:\nadding languages to the model increases the per-\nformance on low-resource languages up to a point,\nafter which the overall performance on monolin-\ngual and cross-lingual benchmarks degrades. The\nquestion is thus whether, and if yes to what degree,\nthis degradation affects the syntactic generalization\npotential of multilingual models across languages.\nThe reason to extend the evaluation to other lan-\nguages (in our case, Spanish) is that many existing\nsyntactic phenomena such as determiner and ad-\njective agreement within the noun phrase, subject\npro-drop, or ﬂexible word order – to name only a\nfew – are not prominent or do not exist in English,\nwhile in Spanish all of them do.\nOur evaluation methodology is similar to that by\n3800\nHu et al. (2020a), who test 20 model type combi-\nnations and data sizes on 34 English syntactic test\nsuites, and ﬁnd substantial differences in the syn-\ntactic generalization performance across different\nmodels. We draw upon their tests to test the syn-\ntactic generalization potential of monolingual and\nmultilingual transformer-based models for English,\nand upon the Spanish SyntaxGym introduced in\nthis paper for Spanish. Tu run the tests, we use the\nSyntaxGym toolkit (Gauthier et al., 2020).\nOur results show that, indeed, there is a substan-\ntial difference between the syntactic generalization\npotential of monolingual and multilingual mod-\nels. But this difference depends on the language:\nWhile for English monolingual models (BERT and\nRoBERTa) offer a higher syntactic generalization\nthan multilingual models (mBERT and XLM-R),\nthis is not the case for Spanish, for which multi-\nlingual models (XLM-R) generalize better. Fur-\nthermore, multilingual models do not generalize\nequally well across languages, with mBERT gener-\nalizing, in general, better in English and XLM-R\nbetter in Spanish. Our experiments also show that\nit depends on the language how well a multilingual\nmodel captures a speciﬁc syntactic phenomenon\nsuch as, e.g., Agreement, Center-embedding or Gar-\nden Path.\nThe remainder of the paper is structured as fol-\nlows. Section 2 introduces the work that is related\nto ours in terms of the evaluation methodology and,\nin particular, in terms of the assessment of multi-\nlingual language models. Section 3 describes the\nEnglish test suites, and presents the novel Spanish\nSyntaxGym test suites. Section 4 details the mod-\nels that we tested and outlines how we use them to\nevaluate the probability of a text sequence. Section\n5 offers a detailed analysis of the syntactic gener-\nalization abilities of the monolingual and multilin-\ngual versions of BERT and RoBERTa, and Section\n6 summarizes the implications that our work has\nfor the use of multilingual language models.\n2 Related Work\nOur work on the evaluation of the capability of\nmonolingual and multilingual transformer-based\nLMs to capture syntactic information is in line with\na number of previous works, including, e.g., those\nthat are based on psycholinguistic experiments, fo-\ncusing on highly speciﬁc measures of language\nmodeling performance and allowing to distinguish\nmodels with human-like representations of syntac-\ntic structure (Linzen et al., 2016; Lau et al., 2017;\nGulordava et al., 2018; Marvin and Linzen, 2018;\nFutrell et al., 2019). Supervised probing models\nhave been used to test for the presence of a wide\nrange of linguistic phenomena (Conneau et al.,\n2018; Hewitt and Manning, 2019; Liu et al., 2019a;\nTenney et al., 2019; V oita and Titov, 2020; Elazar\net al., 2020). Warstadt et al. (2020) isolate speciﬁc\nphenomena in syntax, morphology, and semantics,\nﬁnding that state-of-the-art models struggle with\nsome subtle semantic and syntactic phenomena,\nsuch as negative polarity items and extraction is-\nlands.\nRecently, a number of works also address the\ncross-language assessment of models. Hu et al.\n(2020b) introduces XTREME, a multi-task bench-\nmark for evaluating the cross-lingual generalization\ncapabilities of multilingual representations across\n40 languages and 9 tasks. They show that while\nXLM-R reduces the difference between the perfor-\nmance on the English test set and all other lan-\nguages compared to mBERT for tasks such as\nXQuAD and MLQA, it does not have the same\nimpact on structured prediction tasks such as PoS\nand NER. Mueller et al. (2020) introduces a set of\nsubject-verb agreement tests, showing that mBERT\nperforms better than English BERT on Sentential\nComplements, Short VP Coordination, and Across\na Prepositional Phrase, but worse on Within-an-\nObject Relative Clause, Across-an-Object Relative\nClause and in Reﬂexive Anaphora Across a Rel-\native Clause, and offers high syntactic accuracy\non English, but noticeable deﬁciencies on other\nlanguages, most notably on those that do not use\nLatin script, as also noted by Hu et al. (2020b). On\nthe same line, R ¨onnqvist et al. (2019) concludes\nthat mBERT is not able to substitute a well-trained\nmonolingual model in challenging tasks.\nAs already mentioned in Section 1, Hu et al.\n(2020a) assembled a set of English syntactic tests\nin order to assess the syntactic generalization po-\ntential of a number of different neural LMs (LSTM,\nON-LSTM, RNNG and GPT-2). The tests are ac-\ncessible through the SyntaxGym toolkit (Gauthier\net al., 2020); cf. also Section 3.1. Our methodology\nis analogous, although our objective is different.\nRather than comparing the performance of several\nmonolingual models, we contrast the performance\nof monolingual and multilingual transformer-based\nmodels. Furthermore, while their only test suite\nsource is the English SyntaxGym, we create and\n3801\nuse also a Spanish SyntaxGym; cf. Section 3.2.\n3 Test Suites\nFor the English test suites, we used SyntaxGym,1\nan online platform that compiles a variety of lin-\nguistic tests used by Hu et al. (2020a) to assess the\nsyntactic coverage of language models. It contains\n34 suites, grouped into 6 different so-calledcircuits,\na classiﬁcation based on what is required from the\nmodels to process the targeted constructions. For\nthe Spanish test suites, we created SyntaxGymEs,\nadapting 11 of the existing suites for English and\nbuilding 15 new ones, including a whole new cir-\ncuit. In what follows, we ﬁrst introduce the original\nEnglish SyntaxGym and then present in detail the\nnovel SyntaxGymEs.\n3.1 SyntaxGym for English\nThe tests in the SyntaxGym designed by Hu et al.\n(2020a) (henceforth also referred to as “English\nSyntaxGym”) are based on the notion of surprisal.\nA sequence of words is given to a language model,\nwhich assigns a probability to each of the following\ncandidate words. Given the syntactic properties of\nthe considered language, some candidate words\nare less surprising than others, and so should be\npredicted by a language model. For instance, after\nthe sequence The cat, the inﬂected word sleeps\nshould be less surprising than sleep.\nEach test consists of a list of ITEMS that vary in\na controlled way according to a set ofCONDITIONS\ndetermined by the experimental design. The other\nmain component is a series of PREDICTIONS com-\nparing surprisal values in speciﬁc regions of the\nitems across conditions. If the relevant syntactic\ngeneralization has been learned by the model, the\npredictions should hold.\nMoreover, some tests have versions with MODI -\nFIERS , in which additional clauses or phrases have\nbeen embedded inside each item. These modiﬁers\nincrease the linear distance between two co-varying\nitems, making the task harder. Sometimes they also\ninclude a distractor word in the middle of a syn-\ntactic dependency, which can lead the models to\nmisinterpret the dependency.\nThe test suites are arranged in terms of the fol-\nlowing circuits:\n•Agreement: Morphosyntactic phenomena that\noccur when the features of an item constrain an-\nother item to adopt a speciﬁc form. This is a\n1http://syntaxgym.org/\nmarginal phenomenon in English, so the original\ncircuit only includes 3 test suites on Subject-verb\nnumber agreement, all of them with modiﬁers (Mar-\nvin and Linzen, 2018).\n•Licensing: A construction’s need for the pres-\nence of a licensor to allow its occurrence in a sen-\ntence. The circuit consists of 4 suites on Negative\npolarity items (2 of them with modiﬁers) and 6\non Reﬂexive pronouns (all of them with modiﬁers),\nalso from Marvin and Linzen (2018).\n•Center embedding: Subordinate clauses that\nsit in the middle of their superordinate clause, cre-\nating nested dependencies. This circuit contains 2\ntest suites: Center embedding and Center embed-\nding with modiﬁer, from Wilcox et al. (2019a).\n•Long-distance dependencies (LDDs): LDDs\noccur when two constituents that are syntactically\nrelated do not appear adjacent to one another, but at\na longer distance from one another. The circuit in-\ncludes 6 suites on Filler-gap dependencies (2 with\nmodiﬁers and 4 addressing extraction and hierar-\nchy) from Wilcox et al. (2018) and Wilcox et al.\n(2019b), and 2 suites on Cleft structure that were\nﬁrst introduced in (Hu et al., 2020a).\n•Gross syntactic expectation: Expectation for\na large syntactic structure usually induced by sub-\nordinating adverbs or conjunctions. 4 test suites on\nSubordination (from Futrell et al. (2018), 3 of them\nwith modiﬁers) constitute the circuit.\n•Garden path effects : Effects that emerge\nwhen an incorrect but locally likely parse needs\nto be abandoned in favor of the correct one, once\na speciﬁc word appears in the sentence. Two\nsuch effects are considered in this circuit: Main\nverb/reduced relative clause (MVRR) and NP/Z\ngarden paths, with respectively 2 and 4 suites, all\nfrom Futrell et al. (2018).\n3.2 SyntaxGymES: SyntaxGym for Spanish\nFor Spanish, we expand the tests in (Hu et al.,\n2020a) so as to cover language-speciﬁc phenom-\nena. In this section, we detail which of the original\ntests we retained, which ones we modiﬁed, and\nwhich ones we added within each original circuit.\nA whole new circuit regarding the linear order of a\nsentence’s basic constituents was also added, since\nﬂexibility in this respect is a characteristic that dis-\ntinguishes Spanish (and other Romance languages)\nfrom English. For a more detailed description with\nexamples and predictions, see the Supplementary\nMaterial; upon acceptance of the paper, Syntax-\n3802\nGymES will be published in the SyntaxGym plat-\nform http://syntaxgym.org/.\n3.2.1 Notation\nWe follow the usual notations in linguistic literature.\nAn asterisk ‘*’ preceding an example signals that\nthe sentence is ungrammatical, it violates some\nprinciple or constraint. A question mark ‘?’ is used\nto indicate a marginal sentence, i.e., a sentence\nthat is grammatical but very uncommon or that\nrequires a non-straightforward interpretation. The\nexclamation mark ‘!’ indicates a highly difﬁcult\nsentence to process for the human mind.\n3.2.2 Agreement\nUnlike English, Spanish is a morphologically rich\nlanguage, and as such it presents many morpho-\nsyntactic phenomena related to agreement. For this\nreason, out of the six original circuits, Agreement\nwas the one that underwent the most changes.\nRegarding verbal agreement (constraints im-\nposed on the verb by the subject), we adapted\ntwo existing test suites, Subject-Verb Agreement\nwith Object Relative Clause and Subject-Verb\nAgreement with Subject Relative Clause , and\ncreated a new one, Basic Subject-Verb Agree-\nment, in which both person and number features\nwere taken into consideration.\n(1) T´u\nyou.2SG\ncocinas\ncook.2SG\n(2) * T´u\nyou.2SG\ncocinais/cocino/cocinan\ncook.2PL/1SG/3PL\nAs for nominal agreement (constraints that a\nnoun’s gender and number features can impose\non the form of other words in the sentence), we\nalso created several new test suites: Determinant-\nNoun Agreement simply pairs a noun with the\nfour possible forms of the deﬁnite article ( el, la,\nlos, las), while Adjective-Noun Agreement pairs\na noun with the four possible forms of an adjec-\ntive that modiﬁes it (we excluded articles to avoid\nproviding extra information).\n(3) La\nthe\ntienda\nstore\nvende\nsells\ndiscos\ndisc.M.PL\nusados\nused.M.PL\n(4) * La\nthe\ntienda\nstore\nvende\nsells\ndiscos\ndisc.M.PL\nusados/usado/usadas/usada\nused.M.PL/M.SG/F.PL/F.SG\nIn addition to these two suites, we built similar\nones for Attribute Agreement in copulative con-\nstructions, to which we added two versions with\nobject or subject relative clauses as modiﬁers, and\nalso for Predicative Agreement in constructions\nwith subject or object predicative complement. The\nonly difference here is that the two words that must\nagree are not adjacent anymore. In terms of predic-\ntions, the verb/noun with matching features should\nhave a lower surprisal than the others, and the\nverb/noun that matches only one feature should\nhave a lower surprisal that the one that doesn’t\nmatch any.\n3.2.3 Center Embedding\nFor this circuit, we adapted to Spanish the two exist-\ning test suites in English, creating Center Embed-\nding and Center Embedding with PP modiﬁer .\nIn the basic suite, a relative clause is center em-\nbedded after the subject of the main clause. Verb\ntransitivity and subject-verb plausibility are used\nto test if the models are capable of retaining the\nrelevant information and predicting the verbs in the\ncorrect order.\n3.2.4 Gross Syntactic Expectation\nFrom the four original suites in this circuit, we\nadapted three of them: Subordination, and two of\nits versions with modiﬁers, Subordination with\nObject Relative Clause and Subordination with\nSubject Relative Clause . Given a sentence that\nstarts with a typically subordinating adverb or con-\njunction, these suites test the models’ ability to\nmaintain the expectation for the onset of a matrix\nclause for as long as the subordinate one lasts.\n3.2.5 Long-distance Dependencies\nFiller-gap dependencies are an example of LDDs.\nThey occur when a phrase (the ﬁller) is realized\nsomewhere in the sentence, but is semantically in-\nterpreted at some other point (the gap). For this\ncircuit, we created a Basic Filler-Gap Dependen-\ncies test and adapted from the original English\ncircuit a version that includes modiﬁers, Filler-\nGap Dependencies with Three Sentencial Em-\nbeddings. Embedding three sentences between\nﬁller and gap makes the task more challenging. We\nalso adapted to Spanish the novel Pseudo-Cleft\nStructures suite introduced in (Hu et al., 2020a).\n3.2.6 Garden Path Effects\nThe Garden Path effect can be created by several\nsyntactic ambiguities that differ cross-linguistically.\nThe Main Verb/Reduced Relative garden path ef-\nfect was the subject of two suites in the original\n3803\nEnglish circuit, but it does not translate to Span-\nish, so those suites were not included in Spanish\nSyntaxGym.\nOn the other hand, the ambiguity responsible\nfor NP/Z also holds for Spanish. Here, an NP is\ninitially interpreted as the object in a subordinate\nclause when it actually is the subject of the main\nclause (the subordinate clause having a Zero/null\nobject). The ambiguity can be prevented with a\ncomma, but also by placing an overt object in the\nsubordinate clause, as is done in NP/Z Garden\nPath Effect (with Overt Object), or by substitut-\ning its verb with a pure intransitive verb, as is done\nin NP/Z Garden Path Effect (with Intransitive\nVerb). Both suites correspond to Spanish adapta-\ntions of the two original suites regarding this effect.\n(5) !Mientras ella le ´ıa sus manuscritos se\nvolaron por la ventana.\n!’While she read her manuscripts went out\nthe window.’\n(6) Mientras ella [dorm ´ıa]/[le´ıa un li-\nbro]/[le´ıa,] sus manuscritos se volaron por\nla ventana.\n’While she [slept]/[read a book]/[read,] her\nmanuscripts went out the window.’\n3.2.7 Licensing\nNegative polarity items (NPIs), like any or ever\nin English, are examples of words that need to\nbe licensed by negation. Since Spanish NPIs do\nnot function exactly in the same way, we took the\noriginal NPI Licensing test as inspiration and cre-\nated two new suites: Negative Polarity Items and\nNPIs and Polarity Agreement.\nConstructions with verbs in subjunctive mood\nalso require the presence of a licensor. In Spanish,\na verb expressing feelings (e.g. of joy, surprise,\npleasantness) in the main clause, creates the ex-\npectation for subjunctive mood in the subordinate\nclause. This was the basis for a new test suite: Sub-\njunctive Mood and Verbs that Express Feeling.\n(7) Espero\n(I)hope\nque\nthat\nma˜nana\ntomorrow\nllueva/*llover´a.\nrain.SUB /will.rainIND\n’I hope it [rains]/[will rain] tomorrow.’\nThe other new suite in this circuit, Subjunctive\nMood, Negation and Belief Verbs , relies on the\nfact that belief verbs can also license subjunctive\nmood, but only when combined with negation:\n(8) No\nNEG\ncreo\n(I)believe\nque\nthat\nma˜nana\ntomorrow\nllueva/*llover´a.\nrain.SUB /will.rain.IND\n‘I don’t think it [rain]/[will rain] tomorrow.’\n(9) Creo\n(I)believe\nque\nthat\nma˜nana\ntomorrow\nno\nNEG\nllover´a/*llueva.\nwill.rain.IND /rain.SUB\n’I think it [won’t]/[don’t] rain tomorrow.’\n3.2.8 Linearization\nOne of the main syntactic distinctions between lan-\nguages is constituent order within the sentence.\nBut, in addition to the canonical order in which\nthese elements appear, languages also differ in their\nﬂexibility to alter that order. Spanish allows some\nﬂexibility, which was the basis for three new test\nsuites.\nFor Subject–Auxiliary Verb–Main Verb Lin-\nearization, the possibility to postpone the subject\nis compared with the rigidity of the relation be-\ntween main and auxiliary verb, which must be ad-\njacent and do not allow inversion:\n(10) Juan [ha comido]/*[comido ha].\n’John [has eaten]/[eaten has].’\n(11) Ha [comido Juan]/*[Juan comido].\n*’Has [eaten John]/[John eaten].’\nIn the Subject–Verb–Object Linearization\ntest, we compare the phenomenon in afﬁrmative\nversus interrogative sentences. In Spanish, word\norder ﬂexibility holds for afﬁrmative sentences, but\nnot for interrogative ones, where subject-verb in-\nversion is compulsory:\n(12) Ana compr´o un libro. / Compr ´o un libro\nAna.\n’Ann bought a book. / Bought a book Ann.’\n(13) ¿Qu ´e compr´o Ana? / *¿Qu´e Ana compr´o?\n’What did Ann buy? / *What Ana did buy?’\nWord order variations also appear within the NP,\nas captured by the Noun-Adjective and Noun-PP\nLinearization test. Contrary to English, Spanish\nadjectives usually come after the noun. But again,\nthe language allows for some ﬂexibility and they\ncan be swapped. This possibility, however, does\nnot apply to other noun modiﬁers like prepositional\nphrases:\n3804\n(14) Construy´o una [mesa robusta]/[robusta\nmesa].\n’He built a [sturdy table]/[table sturdy].’\n(15) Construy´o una [mesa de madera]/*[de\nmadera mesa].\n’He built a [wooden table]/*[table\nwooden].’\n4 Experiments\nWe test the base cased versions of BERT and\nmBERT (Devlin et al., 2019), RoBERTa (Liu et al.,\n2019b) and XLM-R (Conneau et al., 2020) on\nthe English SyntaxGym and BETO (Canete et al.,\n2020), mBERT and XLM-R on the Spanish Syn-\ntaxGym. To run the experiments, we use the Syn-\ntaxGym toolkit (Gauthier et al., 2020).\n4.1 Experimental Setup\nThe SyntaxGym test suites are designed from the\nperspective of sentence generation, i.e., with the\nhypothesis that if a model has correctly learned\nsome relevant syntactic generalization, it should\nassign higher probability to grammatical and nat-\nural continuations of sentences. This requires ask-\ning the models to predict the next token given\na context of previous tokens, in a left-to-right\ngenerative fashion. However, BERT-based and\nRoBERTa-based families of models (in our case,\nBERT and mBERT on the one side, and RoBERTa\nand XLM-R on the other side) are bidirectional,\nthey are trained with a masked language modeling\nobjective to predict a word given its left and right\ncontext. In this work, we follow Wang and Cho\n(2019)’s sequential sampling procedure to evalu-\nate the probability of a text sequence, encoding\nunidirectional context in the forward direction. To\ncompute the probability distribution for a sentence\nwith N tokens, we start with a sequence of N + 2\ntokens: a begin of sentence token plus N + 1mask\ntokens, where the last mask corresponds to the\nend of sentence token. For each token position\ni in [1, N], we compute the probability distribu-\ntion over the vocabulary given the left context of\nthe original sequence, and select the probability\nassigned by the model to the original word.\nFor example, in an agreement test with the sen-\ntence ‘The girls run fast.’, a model that has properly\nlearned agreement should assign a higher proba-\nbility to run than to runs for the third word. In\norder to test it, we feed the tokens sequence [[bos]\n[The] [girls] [mask] [mask] [mask] [mask]] to the\nAverage SG performance\nModel English Spanish\nBERT 77.80 —\nRoBERTa 82.04 —\nmBERT 77.55 72.31\nXLM-R 71.84 78.50\nBETO — 67.92\nTable 1: Average SG score by model class for the En-\nglish and Spanish tests.\nFigure 1: Performance accuracy across English circuits\nmodel, and compare the probabilities assigned by\nthe model to run and runs for position 4.\n4.2 Results of the experiments\nThis section summarizes the results of our experi-\nments that aim to: (i) contrast the performance of\nmonolingual and multilingual models on English\nand Spanish and (ii) provide insights on the perfor-\nmance of the multilingual models across languages.\nTable 1 shows the average SyntaxGym (SG) per-\nformance of the evaluated monolingual and multi-\nlingual models on the English and Spanish Syntax-\nGyms. Figures 1 and 2 zoom in on the performance\nof the tested models with respect to speciﬁc circuits\nfor English and Spanish respectively.\nSix of the English test suites (Center Embedding,\nCleft structure, MVRR, NPZ-Verb, NPZ-Object,\nSubordination) and ﬁve of the Spanish test suites\n(Attribute Agreement, Basic Subject-Verb Agree-\nment, Subordination, Center Embedding, Basic\nFiller-Gap Dependencies) include tests with and\nwithout modiﬁers, i.e,. intervening content inserted\nbefore the critical region. Figures 3 and 4 show the\nmodels’ average scores in these test suites, with-\nout modiﬁers (dark bars) and with modiﬁers (light\nbars), evaluating how robust each model is with\n3805\nFigure 2: Performance accuracy across Spanish circuits\nFigure 3: Models average English SG score in Center\nEmbedding, Cleft structure, MVRR, NPZ-Verb, NPZ-\nObject and Subordination, with and without modiﬁers.\nrespect to the corresponding content.\n5 Discussion\nLet us assess in detail the results of the exper-\niments from above. In what follows, we com-\npare the performance of monolingual with the per-\nformance of multilingual models and analyze the\ncross-language performance of multilingual mod-\nels, as well as the stability of the individual models\nwith respect to modiﬁers.\n5.1 Monolingual vs multilingual models\nRoBERTa shows an overall higher performance\nthan the other models for English (Table 1). This\nis not surprising since it is trained on 10 times\nmore data than BERT, and it has been shown to\nimprove over BERT in many NLU tasks. How-\never, while mBERT does not seem to lose perfor-\nmance compared to BERT, XLM-R loses around\n10 points compared to RoBERTa. As XLM-R is\nspeciﬁcally designed to offer a more balanced per-\nformance across languages, with a special focus\non low-resource languages, it appears natural that\nFigure 4: Models average Spanish SG score in At-\ntribute Agreement, Subject-Verb Agreement, Subordi-\nnation, Center Embedding and Filler-Gap Dependen-\ncies, with and without modiﬁers.\nit loses some performance on high-resource lan-\nguages such as English. For Spanish, the multilin-\ngual models clearly outperform the monolingual\nmodel. This is likely due to the fact that while\nBETO and mBERT are of comparable size and\nare trained with the same amount of data (16GB),\nBETO is only trained with a Masked Language\nModeling (MLM) objective, and mBERT is trained\non MLM and Next Sentence Prediction (NSP). On\nthe other hand, XLM-R is also only trained on\nMLM, but it is trained on more than 2TB of data,\n53 GB corresponding to Spanish data.\nRoBERTa outperforms all other models in all the\nEnglish circuits (cf. Figure 1), except in Gross Syn-\ntactic State, in which BERT-based models clearly\noutperform RoBERTa-based models, and the multi-\nlingual model outperforms the monolingual one in\nboth families. Intuitively, we believe that the NSP\ntraining objective of BERT-based models helps\nthem to better understand the relation between two\nsentences, and this knowledge can also be applied\nto the relation between two clauses (which is the ba-\nsis of the Gross Syntactic State circuit). Comparing\nthe BERT and RoBERTa model families, it is inter-\nesting to notice that while RoBERTa outperforms\nXLM-R in all circuits except Gross Syntactic State,\nBERT only outperforms mBERT in 3 of them.\nInterestingly, all models seem to struggle with\nAgreement in English. This observation is aligned\nwith Mueller et al. (2020)’s hypothesis that lan-\nguage models learn better hierarchical syntactic\ngeneralizations in morphologically complex lan-\nguages (such as, e.g., Spanish), which frequently\nprovide overt cues to syntactic structure, than in\nmorphologically simpler languages (such as, e.g.,\nEnglish). Indeed, the fact that XLM-R offers the\n3806\nlowest performance may be related to the fact that\nthe model has been more exposed to more complex\nlanguages than the others. For Long Distance De-\npendencies, BERT-based models show a low perfor-\nmance compared to RoBERTa-based models. This\nmight be due to the different training procedures\nadopted in both model families (i.e., that RoBERTa\ndoes not include the Next Sentence Prediction task\n(as BERT does) and introduces dynamic masking).\nOn the other hand, in speciﬁc circuits for Span-\nish (cf. Figure 2) XLM-R outperforms the other\ntwo models in 5 out of 7 circuits. As observed for\nEnglish, the BERT-based models struggle with the\nLong Distance Dependencies tests, and mBERT\noffers an outstanding performance in Gross Syn-\ntactic State. The monolingual model, BETO, is\noutperformed by mBERT in 4 out of 7 tests, and\nby XLM-R in all 6 out of 7 tests. As mentioned\nbefore, these differences may be related to the fact\nthat, unlike BERT, BETO is not trained with the\nNSP objective; but also to the difference in training\ndata size: 16GB for BETO vs. more than 2TB (of\nwhich 53GB of Spanish data) for XLM-R.\nAll models offer a low performance in the new\nLinearization test for Spanish. A more in-depth\ninvestigation is necessary to explain this. The test\nhas been designed with literary Peninsular Spanish\nin mind, and it is possible that the training data\nmay not contain enough samples that show the\ntargeted word order varieties, or may contain data\nfrom American Spanish sources, which may show\ndifferences in canonical word order with respect to\nPeninsular Spanish.\n5.2 Cross-language multilingual models\nperformance\nAs shown in Table 1, multilingual models do not\nsyntactically generalize equally well in both lan-\nguages. While mBERT offers a better generaliza-\ntion in English, outperforming XLM-R by almost 6\npoints, XLM-R generalizes better in Spanish, out-\nperforming mBERT by 6 points. This observation\ncorroborates our intuition that XLM-R sacriﬁces\nperformance in high-resource languages (e.g., En-\nglish, with 300GB of training data) to be able to of-\nfer a more balanced performance across languages\n(e.g., Spanish, with 53GB of training data).\nComparing Figures 1 and 2, we observe improve-\nments in the Spanish tests for XLM-R in 4 out of 6\ncircuits, particularly noticeable in Agreement and\nCenter Embedding, while it loses around 10 points\nin Long Distance Dependencies. On the other hand,\nmBERT also shows a big improvement in the Span-\nish tests in Agreement, while it loses performance\nin Garden Path Effects, Licensing and Long Dis-\ntance Dependencies.\n5.3 Model stability with respect to modiﬁers\nSince modiﬁers increase the linear distance be-\ntween the elements in a dependency structure, thus\nmaking the task more demanding, stability in this\nrespect indicates that models have robustly learnt\nthe appropriate syntactic generalization and do not\ndepend that much on adjacency. Figures 3 and\n4 show the models’ average scores in those test\nsuites that have two versions: without modiﬁers\n(dark bars) and with modiﬁers (light bars). As was\nintuitively expected, all the models offer a higher\nperformance in the tests without modiﬁers. While\nfor English the multilingual models are the less af-\nfected, for Spanish BETO seems to be more robust\nthan the multilingual models, even though it offers\na lower performance.\n6 Conclusions\nIn this paper, we assessed the syntactic general-\nization potential of selected transformer-based lan-\nguage models on English and Spanish. We have\nshown that multilingual models do not generalize\nequally well across languages: mBERT generalizes\nbetter for phenomena in English, while XLM-R\ndoes it better for phenomena in Spanish. We have\nalso shown that the answer to the question whether\nmonolingual or multilingual models generalize bet-\nter is equally language-speciﬁc: the monolingual\nRoBERTa generalizes better on English, while the\nmultilingual XLM-R generalizes better on Spanish.\nWhile it is possible that the multilingual abstrac-\ntions captured by XLM-R become useful for mor-\nphologically rich languages such as Spanish, this\ndifference may also be related to the difference in\nthe amount of training data used to train BETO and\nXLM-R, and therefore it is possible that a monolin-\ngual model trained with a comparable amount of\ndata could outperform the multilingual models.\nThe performance of all models is affected by the\npresence of modiﬁers, which shows that the com-\nplexity of the syntactic structure is still a challenge.\nIn general, each syntactic phenomenon deserves\nattention. For instance, Agreement in English is\nhard to learn, given the scarcity of cues (especially\nif compared to a morphologically rich language),\n3807\nand so is Linearization in Spanish.\nAs far as the nature of the training procedures of\nthe models is concerned, the lack of Next Sentence\nPrediction (NSP) objective in the RoBERTa model\nfamily seems to harm BETO, but not XLM-R; this\nsuggests that the performance of BETO may be\nimproved with (much) more training data. It also\nseems to harm in the case of the Gross Syntactic\nState circuit, suggesting that RoBERTa-based mod-\nels may also beneﬁt from complementary training\nobjectives in their pretraining procedure.\nOverall, our experiments have also shown the\nimportance of testing models on a wider range of\nlanguages, in particular, morphologically rich ones.\nAs part of our future work, we plan to expand fur-\nther SyntaxGymES and develop SyntaxGyms for a\nnumber of other selected languages. Also, careful\nexamination of a wider range of material is neces-\nsary to ensure that important phenomena are not\nleft out, so as to assess the actual coverage of the\ntest suites.\nAcknowledgments\nThis work has been partially funded by the Eu-\nropean Commission via its H2020 Research Pro-\ngram under the contract numbers 779962, 786731,\n825079, and 870930.\nReferences\nJos´e Canete, Gabriel Chaperon, Rodrigo Fuentes, and\nJorge P´erez. 2020. Spanish pre-trained BERT model\nand evaluation data. PML4DC at ICLR, 2020.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Lo¨ıc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $&!#* vector: Probing\nsentence embeddings for linguistic properties. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2126–2136, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nDaniel Edmiston. 2020. A systematic analysis of mor-\nphological content in BERT models for multiple lan-\nguages. arXiv preprint arXiv:2004.03032v1.\nYanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav\nGoldberg. 2020. When Bert forgets how to POS:\nAmnesic probing of linguistic properties and MLM\npredictions. arXiv preprint arXiv:2006.00995.\nRichard Futrell, Ethan Wilcox, Takashi Morita, and\nRoger Levy. 2018. RNNs as psycholinguistic sub-\njects: Syntactic state and grammatical dependency.\narXiv preprint arXiv:1809.01329.\nRichard Futrell, Ethan Wilcox, Takashi Morita, Peng\nQian, Miguel Ballesteros, and Roger Levy. 2019.\nNeural language models as psycholinguistic sub-\njects: Representations of syntactic state. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 32–42, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nJon Gauthier, Jennifer Hu, Ethan Wilcox, Peng Qian,\nand Roger Levy. 2020. Syntaxgym: An online\nplatform for targeted evaluation of language models.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics: System\nDemonstrations, pages 70–76.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers) , pages 1195–1205, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4129–4138, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox,\nand Roger Levy. 2020a. A systematic assessment\nof syntactic generalization in neural language mod-\nels. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics ,\npages 1725–1744, Online. Association for Compu-\ntational Linguistics.\n3808\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020b. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual gener-\nalisation. In Proceedings of the 37th International\nConference on Machine Learning , volume 119 of\nProceedings of Machine Learning Research , pages\n4411–4421. PMLR.\nMandar Joshi, Kenton Lee, Yi Luan, and Kristina\nToutanova. 2020. Contextualized representations us-\ning textual encyclopedic knowledge. arXiv preprint\narXiv:2004.12006.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. arXiv preprint\narXiv:1901.07291.\nJey Han Lau, Alexander Clark, and Shalom Lappin.\n2017. Grammaticality, acceptability, and probabil-\nity: A probabilistic view of linguistic knowledge.\nCognitive Science, 41(5):1202–1241.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of lstms to learn syntax-\nsensitive dependencies. Transactions of the Associa-\ntion for Computational Linguistics, 4:521–535.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202,\nBrussels, Belgium. Association for Computational\nLinguistics.\nAaron Mueller, Garrett Nicolai, Panayiota Petrou-\nZeniou, Natalia Talmina, and Tal Linzen. 2020.\nCross-linguistic syntactic evaluation of word predic-\ntion models. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5523–5539, Online. Association for\nComputational Linguistics.\nSamuel R ¨onnqvist, Jenna Kanerva, Tapio Salakoski,\nand Filip Ginter. 2019. Is multilingual BERT ﬂu-\nent in language generation? In Proceedings of the\nFirst NLPL Workshop on Deep Learning for Natural\nLanguage Processing, pages 29–36, Turku, Finland.\nLink¨oping University Electronic Press.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a distilled version\nof BERT: smaller, faster, cheaper and lighter.ArXiv,\nabs/1910.01108.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Sam Bowman, Dipanjan Das,\nand Ellie Pavlick. 2019. What do you learn from\ncontext? probing for sentence structure in contextu-\nalized word representations. In International Con-\nference on Learning Representations.\nElena V oita and Ivan Titov. 2020. Information-\ntheoretic probing with minimum description length.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 183–196, Online. Association for Computa-\ntional Linguistics.\nAlex Wang and Kyunghyun Cho. 2019. BERT has\na mouth, and it must speak: BERT as a Markov\nrandom ﬁeld language model. In Proceedings of\nthe Workshop on Methods for Optimizing and Eval-\nuating Neural Language Generation , pages 30–36,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R\nBowman. 2020. Blimp: The benchmark of linguis-\ntic minimal pairs for english. Transactions of the As-\nsociation for Computational Linguistics, 8:377–392.\nEthan Wilcox, Roger Levy, and Richard Futrell. 2019a.\nHierarchical representation in neural language mod-\nels: Suppression and recovery of expectations. In\nProceedings of the 2019 ACL Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 181–190, Florence, Italy. As-\nsociation for Computational Linguistics.\nEthan Wilcox, Roger Levy, Takashi Morita, and\nRichard Futrell. 2018. What do RNN language\nmodels learn about ﬁller–gap dependencies? In\nProceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 211–221, Brussels, Belgium.\nAssociation for Computational Linguistics.\nEthan Wilcox, Peng Qian, Richard Futrell, Miguel\nBallesteros, and Roger Levy. 2019b. Structural su-\npervision improves learning of non-local grammati-\ncal dependencies. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3302–3312, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural\nInformation Processing Systems , volume 32, pages\n5753–5763. Curran Associates, Inc.\n3809\nA Spanish SyntaxGym: Description of\nTest Suites\nThis appendix lists and describes all the test suites\ncompiled for Spanish SyntaxGym. Each test con-\nsists of a list of ITEMS that vary in a controlled\nway according to a set of CONDITIONS determined\nby the experimental design. A series of PREDIC -\nTIONS compare surprisal values at speciﬁc regions\nof the items across conditions. Some tests have\nversions with MODIFIERS that increase the linear\ndistance between two co-varying items, making the\ntask more demanding.\nThe test suites are arranged in terms ofcircuits of\nrelated syntactic phenomena. Each of the following\nsections corresponds to one of these circuits.\nNotation. An asterisk * signals an ungrammati-\ncal sentence, a question mark ? indicates a marginal\nsentence (grammatical but very uncommon or re-\nquiring a difﬁcult interpretation), an exclamation\npoint ! denotes high processing difﬁculty.\nA.1 Agreement\nAgreement is a morpho-syntactic phenomenon that\noccurs when the features of an item constrain an-\nother item to adopt a speciﬁc form.\n•Basic Subject-Verb Agreement. New suite.\nSpanish ﬁnite verbs in any tense/mood have six\ninﬂected forms according to person and number\nfeatures. The verb’s features the subject’s, other-\nwise the result is ungrammatical.\n(16) T´u\nyou.2SG\ncocinas\ncook.2SG\n(17) * T´u\nyou.2SG\ncocinais/cocino/cocinan\ncook.2PL/1SG/3PL\nPredictions: The surprisal at the verb region is ex-\npected to be lower when it matches the subject than\nin any other condition. It is also expected to be\nlower when at least one of the features (person or\nnumber) agrees than when both disagree.\n•Subject-Verb Agreement with Subject Rel-\native Clause. Adapted from English. This test\nfocuses on number agreement. The subject relative\nclause includes a distractor NP differing in number\nwith the subject.\n(18) El\nthe.SG\nfontanero\nplumber\nque\nthat\nayud´o\nhelped.3SG\na\nto\nlos\nthePL\nalba˜niles\nbricklayers\ntrabaja/*trabajan\nwork.3SG/3PL\nlos\nthe\ns´abados.\nsaturdays.\n’The plumber who helped the bricklayers\nworks/*work on saturdays.’\n(19) Los\nthe.PL\nfontaneros\nplumbers\nque\nthat\nayudaron\nhelped.3SG\nal\nto.thePL\nalba˜nil\nbricklayer\n*trabaja/trabajan\nwork.3PL/3SG\nlos\nthe\ns´abados.\nsaturdays.\n’The plumbers who helped the bricklayer\n*works/work on saturdays.’\nPredictions: A successful model should place\nhigher probability to the verb agreeing with the\nsubject (instead of the distractor) both in singular\nand in plural.\n•Subject-Verb Agreement with Object Rela-\ntive Clause. Adapted from English. Equal to the\nprevious one, but with an object relative clause.\nNominal agreement was the basis for the follow-\ning 6 new test suites. All of them share the same\npredictions: the surprisals should be lower when\nboth gender and number features in the second\nword of the agreement relation match those in the\nﬁrst word. They should also be lower when only\none of the features agrees than when both disagree.\n•Determiner-Noun Agreement. New suite.\nThe four possible forms of the deﬁnite article are\npaired with different nouns.\n(20) El/*La/*Los/*Las\nthe.M.SG/*F.SG/*M.PL/*F.PL\ngato\ncat\n•Adjective-Noun Agreement. New suite. The\ntest pairs a noun with the four possible forms of\nan adjective that modiﬁes it (we used constructions\nwithout determiner to avoid providing the models\nwith extra information).\n(21) La\nthe\ntienda\nstore\nvende\nsells\ndiscos\ndiscs\nusados/*usado/*usadas/*usada\nused.M.PL/M.SG/F.PL/F.SG\n’The store sells second-hand discs.’\n•Attribute Agreement. New suite. Here, a\nnoun is paired with and adjective through a copu-\nlative construction. This suite has 2 versions with\nobject or subject relative clauses as modiﬁers.\n(22) El\nthe\npiso\nﬂat\nest´a\nis\nvac´ıo/*vac´ıa/*vac´ıos/*vac´ıas\nempty.M.SG/*F.SG/*M.PL/*F.PL\n•Predicative Agreement. New suite. The sub-\nject or the object is paired with an adjective func-\ntioning as a predicative complement.\n(23) Los\nthe\nni˜nos\nchildren\nllegaron\narrived\ncansados/*cansado/*cansadas/*cansada\ntired.M.PL/*M.SG/*F.PL/*F.SG\n’The children arrived tired.’\n3810\nA.2 Center Embedding\nA center embedded clause is a subordinate clause\nthat sits in the middle of its superordinate clause,\ncreating nested dependencies that may be challeng-\ning for the models.\n•Center Embedding. Adapted from English.\nA relative clause is center embedded after the\nsubject of the main clause. Verb transitivity and\nsubject-verb plausibility are used to test if the mod-\nels are capable of retaining the relevant information\nand predicting the verbs in the correct order.\n(24) La tormenta que el capit ´an [cape ´o\namain´o]/?[amain´o cape´o].\n’The storm the captain [weathered\nabated]/?[abated weathered].’\nPrediction: The surprisal of the combination of\nverbs should be smaller when their relative order\ncreates a plausible sentence than when it creates an\nimplausible one.\n•Center Embedding with modiﬁer . In the\nversion with modiﬁer, a prepositional phrase is in-\nserted after the subject of the subordinate clause.\nA.3 Gross Syntactic State\nExpectation for a large syntactic structure at some\npoint within the sentence.\n•Subordination. Adapted from English. A\nsentence starting with a subordinate clause creates\nthe expectation for the onset of a matrix clause for\nas long as the subordinate one lasts.\n(25) ?(Mientras) ella miraba los resultados, el\ndoctor entr´o en la habitaci´on.\n’While she looked at the results, the doctor\nentered the room.’\n(26) (*Mientras) ella miraba los resultados.\n’(*While) she looked at the results.’\nPredictions: The surprisal for the lack of a second\nclause should be higher when there is a subordi-\nnating conjunction or adverb than where there is\nnot. But having two clauses joined by a conjunc-\ntion/adverb should be less surprising than their jux-\ntaposition.\n•Subordination with Object Relative Clause\nand Subordination with Subject Relative\nClause. Adapted from English. Versions of the\nprevious suite but with a modiﬁer.\nA.4 Long-distance Dependencies\nLDDs occur when two syntactically related groups\ndo not appear adjacent to one another but at a longer\ndistance from one another.\n•Basic Filler-Gap Dependencies. New suite,\na simpliﬁed version of the existing FGD tests for\nEnglish. FGDs occur when a phrase (the ﬁller) is\nrealized somewhere in the sentence but is semanti-\ncally interpreted at some other point (the gap).\n(27) Yo s´e [lo que]/*que tu amigo tir´o al suelo.\n’I know what/*that your friend threw .’\n(28) Yo s´e *[lo que]/que tu amigo tir´o una col-\nilla al suelo.\n’I know *what/that your friend threw a\ncigarette butt.’\nPredictions: The overt object should be more sur-\nprising when there is a ﬁller when there is not. We\nalso expect lower surprisal when the sentence has\na ﬁller later followed by gap than when it has a\nconjunction instead but the gap remains.\n•Filler-Gap Dependencies with Three Sen-\ntencial Embeddings. Adapted from English. It\nis a version of the previous test that includes a mod-\niﬁer (three sentential embeddings) between ﬁller\nand gap. This makes the task more challenging.\nThe predictions, though, remain the same.\n•Pseudo-Cleft Structures Adapted from En-\nglish. A pseudo-cleft or wh-cleft is formed by\na wh-element extracting content from a relative\nclause joined by a copula to a constituent that pro-\nvides the content requested by the wh-element. The\nextracted constituent can be a NP or a VP. In the\nVP case, the verb in the relative clause must be an\ninﬂected form of ‘hacer’ (‘to do’).\n(29) Lo que t´u difundiste/?hiciste fue un rumor.\n’What you spread/*did was a rumor.’\n(30) Lo que t´u *difundiste/hiciste fue conﬁrmar\nun rumor.\n’What you *spread/did was conﬁrm a ru-\nmor.’\nPredictions: The surprisal should be lower for the\nextracted VP when the verb in the relative clause is\na light verb (hacer – ‘to do’) than when it is not, but\nit should be higher for the extracted NP when the\nverb is light than when it is semantically heavier\nand matches the NP. In addition, the difference in\nthe ﬁrst case should be more important than in the\n3811\nsecond one. This happens because the light verb\nadmits a wider range of objects, whereas in the ﬁrst\ncase, one of the options is syntactically incorrect.\nA.5 Garden Path Effects\nGarden-path effects emerge when an incorrect but\nlocally likely parse needs to be abandoned in favor\nof the correct one. In the NP/Z garden path, an NP\nis initially interpreted as the object in a subordinate\nclause, but when the main verb appears, this NP\nshould be reinterpreted as its subject. The effect\ncan be prevented by adding a comma, but also by\nplacing an overt object in the subordinate clause,\nor by substituting its verb with a purely intransitive\none. These are the basis for the next two suites.\n•NP/Z Garden Path Effect (Overt Object).\n• NP/Z Garden Path Effect (Intransitive\nVerb). Both adapted from English.\n(31) !Mientras ella le ´ıa sus manuscritos se\nvolaron por la ventana.\n!’While she read her manuscripts went out\nthe window.’\n(32) Mientras ella [dorm ´ıa]/[le´ıa un li-\nbro]/[le´ıa,] sus manuscritos se volaron por\nla ventana.\n’While she [slept]/[read a book]/[read,] her\nmanuscripts went out the window.’\nPredictions: The main verb should be more sur-\nprising in the garden path condition than when the\neffect has been prevented either by the comma or by\ninterfering with the verb. Moreover, the difference\nin surprisal should be bigger when the comma is\nessential to solve the garden path effect than when\nit is not.\nA.6 Licensing\nIn natural language, some words or constructions\nneed the presence of a licensor to allow their oc-\ncurrence in a sentence. This happens with NPIs\n(Negative polarity items) and subjunctive mood,\nfor instance.\n•Negative Polarity Items and Polarity Agree-\nment. New suite. In Spanish, NPIs that follow the\nverb (such as nunca ’never’,nadie ’nobody’, and\nnada ’nothing’) need to be licensed by negation.\nThis ‘double negative’ does not result in an afﬁr-\nmative, it is a sort of polarity agreement.\n(33) Yo\nI\nno\nNEG\nbebo\ndrink\nnunca/?siempre.\nnever/always\n’I never drink./I don’t drink always.’\n(34) Yo bebo *nunca/siempre.\n’I *ever/always drink.’\nPredictions: We expect the surprisals in both agree-\ning conditions (negative-NPI, positive-PPI) to be\nlower than in any of the non-agreeing conditions\n(negative-PPI, positive-NPI).\n•Negative Polarity Items . New suite. NPIs\nalso need to be in the scope of the negation to\nbe licensed by it. This suite compares between a\nnegative particle that “commands” the NPI and one\nthat doesn’t.\n(35) T´u,\nYou,\ncomo\nas\nno\nNEG\nmirabas\nlooked\npor\nby\nla\nthe\nventana,\nwindow,\n*(no)\nNEG\nhas\nhave\nvisto\nseen\na\nat\nnadie.\nnobody\n’As you weren’t looking through the win-\ndow, you have *(not) seen anybody.’\n(36) T´u,\nYou,\ncomo\nas\nmirabas\nlooked\npor\nby\nla\nthe\nventana,\nwindow,\n*(no)\nNEG\nhas\nhave\nvisto\nseen\na\nat\nnadie.\nnobody\n’As you were looking through the window,\nyou have *(not) seen anybody.’\nPredictions: The NPI should be more surprising\nwhen there isn’t a negative particle that commands\nit, independently of the presence of another one\nthat does not command it.\n•Subjunctive Mood and Verbs that Express\nFeeling. New suite. Feeling verbs that introduce a\nsubordinate clause serve as licensors for subjunc-\ntive mood, whereas other type of verbs do not.\n(37) Espero\n(I)hope\nque\nthat\nma˜nana\ntomorrow\nllueva/*llover´a.\nrain.SUB /will.rain.IND\n’I hope it rains/*[will rain] tomorrow.’\n(38) S´e\n(I)know\nque\nthat\nma˜nana\ntomorrow\n*llueva/llover´a.\nrain.SUB /will.rain.IND\n’I know it [will rain]/rains tomorrow.’\nPredictions: Subjunctive mood should be less sur-\nprising than indicative mood when the verb in the\nmain clause expresses feelings. But when it doesn’t,\nsubjunctive should be more surprising than indica-\ntive mood. Moreover, subjunctive mood should\nalso be more surprising with a feeling verb than\nwith a non-feeling verb.\n• Subjunctive Mood, Negation and Belief\nVerbs. New suite. Belief verbs can also license\nsubjunctive mood, but only when combined with\nnegation.\n3812\n(39) No\nNEG\ncreo\nbelieve\nque\nthat\nma˜nana\ntomorrow\nllueva/*llover´a.\nrain.SUB /will.rain.IND\n’I don’t think it rains/[will rain] tomorrow.’\n(40) Creo\n(I)believe\nque\nthat\nma˜nana\ntomorrow\nno\nNEG\n*llueva/llover´a.\nrain.SUB /will.rain.IND\n’I think it rains/[won’t rain] tomorrow.’\nPredictions: The subordinate verb should be less\nsurprising in subjunctive than in indicative mood\nwhen the main clause is negated. However, the\ncontrary should hold when the subordinate clause\nis negated but the main one is not. In addition,\nsubjunctive mood should be less surprising when\nthe negation is in the main clause than when it is in\nthe subordinate clause.\nA.7 Linearization\nConstituent order is commonly used in linguistics\nas a way to classify languages. But, in addition\nto the canonical order in which elements appear,\nlanguages also differ in their ﬂexibility to alter that\norder.\n•Subject – Auxiliary Verb – Main Verb Lin-\nearization. New suite. Subject-verb order admits\ninversion in Spanish but main and auxiliary verb\ndo not and they must be adjacent.\n(41) Juan ha comido. / Ha comido Juan\n’John has eaten. / Has eaten John.’\n(42) *Juan comido ha. / *Ha Juan comido.\n’John eaten has. / Has John eaten.’\nPredictions: The postposed subject should be less\nsurprising than any of the alterations involving aux-\niliary and main verb. The canonical SV order, how-\never, should be less surprising than postposing the\nsubject, and the difference in this case should be\nless important than the differences in the ﬁrst two\ncases.\n•Subject – Verb – Object Linearization. New\ntest. In Spanish, word order ﬂexibility holds for\nafﬁrmative sentences but not for interrogative ones,\nwhere subject-verb inversion is compulsory.\n(43) Ana compr´o un libro/Compr´o un libro Ana.\n’Ann bought a book. / Bought a book Ann.’\n(44) ¿Qu ´e compr´o Ana? / ¿Qu´e Ana compr´o?\n’What did Ana buy? / ’What Ana did buy?’\nPredictions: A postposed subject in an afﬁrmative\nsentence should be less surprising than lack of SV\ninversion in an interrogative one. The canonical SV\norder in the afﬁrmative sentence, however, should\nbe less surprising than postposing the subject, and\nthe difference in this case should be less important\nthan the difference in the ﬁrst one.\n• Noun-Adjective and Noun-PP Lineariza-\ntion. New suite. Spanish adjectives usually come\nafter the noun, but this order can be inverted. Other\nnoun modiﬁers like prepositional phrases cannot.\n(45) Construy´o una [mesa robusta]/[robusta\nmesa].\n’He built a [sturdy table]/[table sturdy].’\n(46) Construy´o una [mesa de madera]/*[de\nmadera mesa].\n’He built a [wooden table]/*[table\nwooden].’\nPredictions: A PP preceding the noun should be\nmore surprising than one following it. An adjective\npreceding the noun should also be more surprising\nthan one following it, but the difference in this case\nshould be less important than in the ﬁrst one."
}