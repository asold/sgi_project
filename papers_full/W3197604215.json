{
  "title": "Contextual Density Ratio for Language Model Biasing of Sequence to Sequence ASR Systems",
  "url": "https://openalex.org/W3197604215",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5019869698",
      "name": "Jesús Andrés-Ferrer",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5014480570",
      "name": "D. Albesano",
      "affiliations": [
        "5T Torino (Italy)"
      ]
    },
    {
      "id": "https://openalex.org/A5109588257",
      "name": "Puming Zhan",
      "affiliations": [
        "Burlington College"
      ]
    },
    {
      "id": "https://openalex.org/A5003960312",
      "name": "Paul Vozila",
      "affiliations": [
        "Burlington College"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3015686596",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W3162639446",
    "https://openalex.org/W2963240019",
    "https://openalex.org/W3096273170",
    "https://openalex.org/W2972625221",
    "https://openalex.org/W2962826786",
    "https://openalex.org/W2739427748",
    "https://openalex.org/W1915251500",
    "https://openalex.org/W3097794466",
    "https://openalex.org/W2939111082",
    "https://openalex.org/W2150355110",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W2886319145",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W3008037978",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2577366047",
    "https://openalex.org/W2005708641",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2888779557",
    "https://openalex.org/W3097239815",
    "https://openalex.org/W2143612262",
    "https://openalex.org/W3161375121",
    "https://openalex.org/W3016234571",
    "https://openalex.org/W3152221657"
  ],
  "abstract": "End-2-end (E2E) models have become increasingly popular in some ASR tasks\\nbecause of their performance and advantages. These E2E models directly\\napproximate the posterior distribution of tokens given the acoustic inputs.\\nConsequently, the E2E systems implicitly define a language model (LM) over the\\noutput tokens, which makes the exploitation of independently trained language\\nmodels less straightforward than in conventional ASR systems. This makes it\\ndifficult to dynamically adapt E2E ASR system to contextual profiles for better\\nrecognizing special words such as named entities. In this work, we propose a\\ncontextual density ratio approach for both training a contextual aware E2E\\nmodel and adapting the language model to named entities. We apply the\\naforementioned technique to an E2E ASR system, which transcribes doctor and\\npatient conversations, for better adapting the E2E system to the names in the\\nconversations. Our proposed technique achieves a relative improvement of up to\\n46.5% on the names over an E2E baseline without degrading the overall\\nrecognition accuracy of the whole test set. Moreover, it also surpasses a\\ncontextual shallow fusion baseline by 22.1 % relative.\\n",
  "full_text": "Contextual Density Ratio for Language Model Biasing of Sequence to\nSequence ASR Systems\nJes´us Andr´es-Ferrer1,2, Dario Albesano1,3, Puming Zhan4, Paul Vozila4\nNuance Communications, Inc., 2Valencia, Spain, 3Torino, Italy,4Burlington, MA, USA\n{jesusandres.ferrer, dario.albesano, puming.zhan, paul.vozila}@nuance.com\nAbstract\nEnd-2-end (E2E) models have become increasingly popular in\nsome ASR tasks because of their performance and advantages.\nThese E2E models directly approximate the posterior distri-\nbution of tokens given the acoustic inputs. Consequently, the\nE2E systems implicitly deﬁne a language model (LM) over the\noutput tokens, which makes the exploitation of independently\ntrained language models less straightforward than in conven-\ntional ASR systems. This makes it difﬁcult to dynamically\nadapt E2E ASR system to contextual proﬁles for better recog-\nnizing special words such as named entities. In this work, we\npropose a contextual density ratio approach for both training a\ncontextual aware E2E model and adapting the language model\nto named entities. We apply the aforementioned technique to\nan E2E ASR system, which transcribes doctor and patient con-\nversations, for better adapting the E2E system to the names in\nthe conversations. Our proposed technique achieves a relative\nimprovement of up to 46.5% on the names over an E2E baseline\nwithout degrading the overall recognition accuracy of the whole\ntest set. Moreover, it also surpasses a contextual shallow fusion\nbaseline by 22.1 % relative.\nIndex Terms: speech recognition, end-to-end, sequence-to-\nsequence, language model, shallow fusion, density ratio\n1. Introduction\nIn recent years, End-to-end (E2E) systems [1, 2, 3] have been\ngreatly adopted for ASR because of their performance and sim-\nplicity. In conventional ASR systems, several components,\nsuch as the pronunciation lexicon, the language models, and the\nacoustic model, are optimized independently. In contrast, E2E\nsystems integrate all components of conventional ASR systems\nin a single neural network by directly approximating the poste-\nrior probability given the acoustic features.\nDespite the direct posterior approximation simplifying\nmany aspects such as training, decoding and deployment, these\nadvantages do not come without drawbacks. Speciﬁcally, one\nof the disadvantages is the difﬁculty of exploiting external lan-\nguage models. Since the E2E models directly approximate the\nposterior token distribution, there is no straightforward way to\nintegrate them, which generates recognition problems for rare\nevents among other problems. These tail probability events\ncannot be expected to be comprehensively observed in train-\ning. The selection of the output units such as word pieces [4]\nor characters instead of full words could theoretically mitigate\nthese problems, but in practice this is not the case [5].\nSeveral early attempts were proposed to integrate external\nlanguage models (LM) into E2E systems, ranging from shal-\nlow or deep fusion [6, 7], to cold fusion [8] among many oth-\ners [9, 10]. In the ASR domain, [7] shown that shallow fusion\n1Both authors contributed equally to the work\nwith same output units in both LM and E2E worked best to in-\ncorporate an external language model in some tasks. However,\none of the disadvantages of shallow fusion comes from the fact\nthat most of the E2E systems [1, 2, 11] arguably already incor-\nporate an internal LM [12, 13, 14]. Recently, a density ratio\napproach [15] was proposed to better integrate external LM by\nremoving the internal LM contribution of E2E models while de-\ncoding. In this initial work [15], authors reported improvements\nover shallow fusion for domain adaptation tasks. Further works\nmodiﬁed the architecture [14] or made some assumptions [16]\nto better approximate the implicit LM of E2E systems in com-\nbination with density ratio.\nIn many tasks, ASR systems are expected to recognize sin-\ngletons or infrequent words such as contact list names or other\nnamed entities. These entities may not be seen during train-\ning, and even for those which are observed, the training data is\nclearly neither representative of the particular distribution nor\nof the relevant set associated with the application context for a\ngiven utterance. Consequently, even if the E2E model is using\nBPE [4] or graphemenes as output units, the system struggles to\nrecognize them. Several approaches have been introduced in the\nliterature to contextualize E2E models to named entities such as\nsong names or contact lists. In [17] and [18], an additional at-\ntention set input is proposed for both attention and recurrent\nneural transducer (RNN-T) models respectively. Shallow fu-\nsion approach [19] was applied surpassing state-of-the-art con-\nventional models. More recently [5], shallow fusion was used\nin combination with special tokens to delimit class-based en-\ntity names together with a mapping to transform rare words to\ncommon words through pronunciation.\nIn this paper, we propose contextual density ratio for con-\ntextualizing E2E models so that the internal E2E LM is dynam-\nically adapted to a priori known named entities. The proposed\ntechnique builds upon both density ratio [15] and class-based\nLM tags [5] to contextualize the E2E models. During training,\nwe introduce special tokens to enclose known named entities so\nthat the E2E system learns to predict when a named entity is\nspoken by the different statistical clues obtained from both the\nacoustics and the internal LM. We also approximate an inter-\nnal E2E language model by training an independent LM with\nthe transcriptions on which the E2E system was trained. Dur-\ning decoding, we apply density ratio within the named entities\nsegments identiﬁed by the E2E model to dynamically adapt and\ncontextualize the system’s LM. We show that the proposed con-\ntextual density ratio (CDR) reduces names recognition errors\nover a E2E baseline with and without contextual shallow fu-\nsion [19] by 46.5% and 22.1 % respectively.\nThe paper is organized as follows. We ﬁrst brieﬂy review\ndensity ratio in section 2.1 to pave the way for the following sec-\ntion 2.2 where the proposed contextual density ratio is detailed.\nIn section 3, we apply the proposed technique to a doctor-to-\npatient conversation task for better recognizing both doctors and\narXiv:2206.14623v1  [eess.AS]  29 Jun 2022\npatient names. Finally, in section 4, we conclude with some re-\nﬂections.\n2. Proposed approach\nIn this section we brieﬂy summarize thecontextual density ratio\napproach. We ﬁrst review the density ratio [15], and then we\npropose our extension to the contextual biasing scenario.\n2.1. Density ratio (DR)\nNeural network E2E models such as [1, 20], directly approxi-\nmate the probability of a target token sequence y = yJ\n1 , given\nacoustic features x = xT\n1 , as follows:\np(y|x) ≈pe2e(y|x) (1)\nwhere I is the length of the input acoustic features along time\nand J the length of a possible token sequence; and where p\ndenotes the actual probability of the token sequence given the\ninput acoustic features and pe2e the approximated model distri-\nbution.\nIn density ratio, we wish to adapt the posterior in-domain\n(ID) distribution p(y|x) to a new out-of-domain (OOD) poste-\nrior, q(y|x). Via standard noisy channel decomposition (Bayes’\nlaw) and under assumption that acoustics of both domains are\nsimilar, p(x|y) ≈q(x|y), we decompose the OOD posterior\nas:\nq(y|x) = q(x|y)q(y)\nq(x) = p(x|y)q(y)\nq(x) (2)\nUtilizing the same noisy channel decomposition for ID distri-\nbution p(x|y) and rearranging terms, we obtain:\np(x|y) = p(y|x)p(x)\np(y) (3)\nPlugging eq (3) into eq. (2) yields:\nq(y|x) = p(x|y)q(y)\nq(x) = p(y|x) q(y)\np(y)\np(x)\nq(x) (4)\nNote that, eq. (4) has a marginal LM ratio q(y)/p(y) and an\nacoustics density ratio p(x)/q(x), which does not modify to-\nken sequences scores. We obtain the ﬁnal density ratio (DR)\napproach by approximating distributions with models:\nq(y|x) =pe2e(y|x)qOOD(y)\npID(y)\np(x)\nq(x) (5)\nwhich approximates the acoustic distribution with both an ID\nlanguage and E2E models, pID and pe2e; and shifts the posterior\nwith an OOD language model, qOOD, up to a constant ratio on\nthe acoustics. Note that pID corresponds to the marginal distri-\nbution obtained from E2E posterior,pe2e; in practice we take the\nE2E model training transcriptions as approximately sampling\nfrom this distribution.\nFinally, when performing the beam search we apply\nweights to balance out the importance of each score, yielding\nthe search score:\nscore(y; x) = logpe2e(y|x)−α·log pID(y) +β·log qOOD(y)\n(6)\n2.2. Contextual density ratio (CDR)\nDensity ratio (DR) [15] can bias the distribution for the full ut-\nterance, but we would like to adapt the language model of E2E\nsystems to be able to recognize named entities such as contact\nlist names, or client names. These named entities may occur at\nany position inside a utterance and often only span a few words.\nIn order to do so, we extend the token vocabulary with spe-\ncial words for both start and end of entities, <ne> and </ne>.\nDuring training, our E2E model inserts those tags in\nbetween known named entities, for instance, the next\npatient <ne> be e th o ve n </ne> is\nlosing his hear ing . The system learns to predict\nwhen those named entities will occur, and during beam search\nor decoding, the system hypothesizes those special tokens as if\nthey were standard tokens. Although we focus on a single tag\nfor simplicity in this work, we could add as many specialized\ntags as desired,\nIn the previous section, we reviewed the DR approach for\nfull utterance biasing. However, in practice, each token distri-\nbution on eq. (5) is approximated autoregressively as follows:\nq(y|x) = p(x)\nq(x)\n∏\nt\npe2e(yt|x,y<t)qOOD(yt|y<t)\npID(yt|y<t) (7)\nBecause of the new tokens added to the E2E vocabulary, we\nknow the positions where the named entities will occur. For\nsimplicity, we assume a single named entity occurrence at po-\nsition (tb,te). Then, the token posterior probability of an utter-\nance with a named entity is decomposed by:\nqne(y|x) =\n\n∏\nt<tb\npe2e(yt|x,y<t)\n\n\n·pe2e(<ne>|x,y<tb)\nte∏\nt=tb+1\npe2e(yt|y<t,x)qNE(yt|yt−1\ntb )\npID(yt|y<t)\np(x)\nq(x)\n∏\nt>te\npe2e(yt|x,y<t)\n(8)\nwhere recall yte = </ne>. Note that the in-domain (ID) and\nnamed entity (NE) LMs condition on different context. It is\nworth mentioning that the acoustic density ratio, p(x)/q(x),\nmay introduce a bias when comparing hypotheses with different\nnumber of named entities. However, assuming acoustics are\nsimilar, this ratio should approach 1 and we veriﬁed in the next\nsection that this is not a problem in practice.\nDuring beam search or decoding, each token is scored with\nscore(yt) := score(yt; y<t,x) computed as:\nscore(yt)=\n\n\n\nlog pe2e(yt|... ) + log\nqNE(yt|yt−1\ntb )β\npID(yt|y<t)α ,ne(y<t)\nlog pe2e(yt|... ), o.w.\n(9)\nwhere we omit y<t,x from pe2e(yt|... ). The notation ne(y<t)\nindicates whether there is an open named entity tag in the pre-\nvious tokens that has not yet been closed and in such a case,\ntb := tb(y<t) is the position of the start tag token, <ne>. In\npractice, LM distributions are weighted by αand β to smooth\nthem.\nFigure 1 depicts a small segment of an utterance with a\nnamed entity. The E2E model scores each token independently\nof whether it is within a named entity tag, in contrast to both\nModel mr <ne> moz art </ne>\nE2E p(mr |hello ,x) p(<ne>|..., mr ,x) p(moz|..., mr ,<ne>,x) p(art |..., moz,x) p(</ne>|..., art ,x)\nID 1 1 p(moz|..., mr ,<ne>) p(art |..., moz) p(</ne>|..., art )\nNE 1 1 q(moz|<ne>) q(art |<ne>,moz) q(</ne>|<ne>,moz,art )\nFigure 1: An example of how the different components of contextual density ratio score a hypothesis. The ﬁnal hypothesis score is\ncomputed as log(e2e) −α·log(ID) +β·log(NE). Note how the different language model components track the context differently\naccordingly to their goal. The ID LM, is tracking full context from the beginning of the utterance in contrast with the NE LM which\nonly tracks the LM from the most recent named entity start tag,<ne>.\nin-domain (ID) and LM named-entity (NE) LM. Since the ID\nLM is subtracted to counteract the internal E2E LM, it is track-\ning the context of the full utterance, whereas the NE LM, only\ntracks the context within the contextual named entity tags.\n3. Experiments\n3.1. Data set\nWe apply the proposed contextual density ratio to a conversa-\ntional speech transcription task (doctor-patient conversations).\nAll speech data are ﬁeld data, from which we computed 80 ﬁl-\nter bank features from the input audio. For this study, the train\nand test set consisted of 1.5K hours and 26.8 hours respectively\nfrom several medical domains.\nWe focused on person names as named entities and auto-\nmatically marked them in both training and testing (references)\nby intersecting the transcriptions with a list of 173K English\nnames. During training we enclosed occurrences of such names\nbetween tags ( <ne>and </ne>) and during testing we ex-\ntracted for each conversation both doctor and patient names\nfrom conversation utterances. In total 5% of the utterances in\nthe test set have names.\n3.2. Models\nThe E2E model is a fairly standard E2E attention model sim-\nilar to LAS [1]. Speciﬁcally, our model follows the same ar-\nchitecture of the speaker independent system from LASA [20].\nOur model has a set of 3 convolutional neural network (CNN)\nlayers which are conﬁgured to produce a 768-dimensional em-\nbedding for each input frame. The 3 CNN layers have a kernel\nsize of 3 and do not perform any time reduction. Subsequently,\nthere is a pyramid LAS-like encoder composed of 6 bidirec-\ntional Long Short-Term Memory (bLSTM) [21] layers with 768\nhidden units per layer and direction. The encoder performs in-\nput decimation by a factor of 2 after every other layer. The total\ntime reduction is 8x with respect to the ﬁlter banks inputs. The\ndecoder component uses 2 unidirectional LSTM [22] layers of\n1536 units per layer. We use Bahdanau attention in an archi-\ntecture similar to Luong [23, 20] on the attention component,\nwith a single head with 1536 nodes. We apply dropout [24]\nto regularize the layers and SpecAugment [25] to both regular-\nize the model and as data augmentation. We use a single time\nband of length 40 frames and a frequency band of up to 20 ﬁl-\nters. The ﬁnal softmax layer predicts the posterior probability of\n2.5K word-pieces [4] computed on the training set. The model\naccounts for a total of 143.7M parameters.\nIn order to approximate the internal E2E language model\nfor the contextual density ratio technique, we use the exact\ntranscriptions of the utterances on which the E2E model was\ntrained, together with the very same 2.5K word-pieces lexicon.\nWe trained the language model with truncated backpropagation\nthrough time (BPTT) [26] to better approximate the internal\nE2E language model. We settled for a language model with\n512 embedding projection followed by 2 unidirectional LSTM\nlayers of 1536 each and a ﬁnal down projection back to 512 log-\nits followed by the softmax layer of 2.5K outputs. The internal\nLM accounts for a total of 31.99M parameters. We regularized\nthe model with small weight-decay and dropout. For decoding,\nboth the αand β parameters were tuned on a different domain\nadaptation density ratio setup and both equal to 0.1. This en-\nsures that search parameters are not over-trained to the speciﬁcs\nof the contextual density ratio task or the test set. In addition,\nbecause of the attention models, we used a coverage score [27]\nof 0.3 optimized in conjunction with αand β.\nFor the contextual LMs, we initialized them with the same\nin-domain LSTM based language model and trained them un-\ntil convergence on the speciﬁc list of names extracted per each\nconversation.\n3.3. Results\nIn order to measure the different systems, we report the word\nerror rate (WER), as well as the WER within the name tags\n(WERT)1. This allow us to evaluate both overall performance\nof the system as well as the speciﬁc improvements on name\nrecognition.\nFirst, we trained both an E2E baseline model without tags\nand an E2E model with the name tags ( <ne>and </ne>).\nSecond line of Table 1 shows that adding the tags slightly hurts\nthe model performance, compared to the ﬁrst line, but the dif-\nference is small and within training repetition variance (+/- 0.1).\nThen, we added contextual biasing by either shallow fusion [5]\nor contextual density ratio in the realistic scenario, in which we\nbuild a context biasing language model per conversation. As\nreported in the last 2 lines of Table 1, in both cases, we recover\npart of the small drop in accuracy while signiﬁcantly improving\nname performance. Adding a contextual biasing shallow fusion\ncomponent reduces name errors by 31.3 % relative. Contextual\ndensity ratio biasing reduces name errors by 46.5 % relative\nwith respect to the E2E baseline and 22.1 % compared to shal-\nlow fusion. Note that if all names were wrongly recognized by\nsubstitutions, this will account for only 0.43% of the whole test\nset WER.\nTable 2, compares the precision and the recall on the con-\ntextual tags themselves for the 3 systems with tags from Ta-\nble 1. Both contextual density ratio and contextual shallow fu-\nsion improve the tagging precision and recall over the baseline,\nwith contextual density ratio being better than contextual shal-\nlow fusion. The improvements in terms of precision and re-\ncall are small compared to the WERT improvements from Ta-\nble 1, and consequently the name WERT improvement cannot\nbe attributed as a side effect of better tagging. We hypothesize\n1We aligned hypotheses and references with tags and computed the\nerrors aligned with the references tags.\nTable 1: Word Error Rate (WER) and Word Error Rate within\nthe named entities Tags (WERT) for several systems. Both con-\ntextual systems based on either SF or DR, extend the E2E model\nwith contextual tags and are based on full conversations.\nSystem WER WERT\nE2E without contextual tags 14.94 n.a.\nE2E with contextual tags 15.10 44.1\n+ contextual SF 15.04 30.3\n+ contextual DR (CDR) 15.00 23.6\nTable 2: Precision and recall computed on the contextual tags\nthemselves for the different contextual systems on table 1.\nSystem Precision Recall\nE2E with contextual tags 78.5 79.4\n+ contextual SF 80.7 82.6\n+ contextual DR (CDR) 82.0 83.5\ninstead that the a priori known entity values for the conversa-\ntion, allow the contextually biased techniques to better detect\nthe span for such entities, in particular when linguistic contex-\ntual cues are insufﬁcient.\n3.4. Oracle and noisy names list experiments\nWe studied the proposed contextual DR (CDR) in several sce-\nnarios. Initially, we evaluated the oracle best case where we\nknow whether a given utterance has a name and the name itself\nin such a case. For those utterances with names, a contextual\nname LM is trained per utterance.\nThis oracle setup bounds the potential improvements with\nrespect to the more realistic experiments from the previous sec-\ntion, and measures the effect of the acoustic ratio in Eq. (8) be-\ncause no utterance without names is decoded with the biasing\ncomponent. CDR applied to full conversations, is close to the\noracle case as shown in ﬁrst row of Table 3. On the other ex-\ntreme, a single contextual name LM was trained on all names in\nthe full test set as if the conversation level information was lost.\nThe second row of Table 3 shows that using a single LM has a\nlarger negative effect than using conversation-speciﬁc LMs, yet\nit is still better than the E2E baseline\nFollowing the worst case experiment from Table 3, we stud-\nied the effect of extending the entity (or name) lists per each\nconversation, by adding an increasing number of randomly sam-\npled distracting names (from 1 to 256) unrelated to the conver-\nsation. As shown in Figure 2, CDR is relatively robust to noisy\nnames, only starting to degrade to 26.0% WERT at 256 dis-\ntracting names. Speciﬁcally, for 16 distracting names we obtain\na WERT of 23.9 which is very close to 23.6 of the conversation\nlevel LMs in Table 1.\nFinally, the entity list per conversation, which is used to\ntrain the CDR contextual LMs, was extended with 16 adversar-\nial names so as to further investigate the robustness of CDR.\nThese names are randomly selected from a pool of 170K names\n(and their combinations) according to their character lexico-\ngraphical Levensthein distance, in {1,2,4}, from any of the ac-\ntual conversation names. Note that for Levensthein distance 4,\nwe can drop part of the name (e.g. in case of John Smith the\nresulting adversarial name can be Smith). Table 4 assesses how\nadversarial examples degrade the performance of the CDR bi-\nasing; it is still relatively robust and much better than that of\nthe baseline system. Note that the performance drop for edit\nTable 3: WERT precision and recall for a single LM per utter-\nance (oracle case) and a single LM for the full test set dropping\nconversation level information.\nSystem WERT Precision Recall\n1 LM per utterance (Oracle) 20.1 82.6 85.1\n1 LM for full test set 37.3 78.5 79.7\nTable 4: WERT for 16 names with different Levensthein dis-\ntances, applied to per-conversation CDR case.\nLevensthein distance random 4 2 1\nWERT 23.9 28.2 30.5 28.3\ndistance 4 may seem unexpected, but it is in part a side ef-\nfect of random sampling being able to drop full short names\nor surnames via deletions, in contrast to 2 or 1, where names\nare rarely dropped if at all. We intentionally allowed for full\nname deletions so as to make the list insidiously difﬁcult.\n 20\n 25\n 30\n 35\n 40\n 45\n 50\n 1  2  4  16  64  256\nContextual Density Ratio\nE2E baseline\nFigure 2: WERT (y-axis) at increasing number of per-\nconversation randomly sampled distracting names (log-scale).\n4. Conclusions\nIn this paper, a contextual density ratio for contextual language\nmodel biasing was proposed. This technique was applied to\nthe task of name recognition in doctor-patient conversations.\nThe proposed approach improves name recognition up to 46.5%\nwith respect to a standard E2E system or 22.1% relative with\nrespect to contextual shallow fusion. Moreover, the technique\ndoes not degrade the system performance on utterances without\nnames signiﬁcantly and has no major side effects.\nThe behaviour of the proposed technique was studied by\nadding random distracting and adversarial names to the bias-\ning name list. Contextual density ratio is robust to noise, being\nmore sensitive to similar names. As future work, we want to im-\nprove the technique in those adversarial conditions, for instance\nby also taking into account the phonetics.\n5. References\n[1] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend\nand spell: A neural network for large vocabulary conversational\nspeech recognition,” in Proc. of ICASSP . Shanghai, China:\nIEEE, 2016, pp. 4960–4964.\n[2] A. Graves, A. Mohamed, and G. Hinton, “Speech recognition with\ndeep recurrent neural networks,” in2013 IEEE International Con-\nference on Acoustics, Speech and Signal Processing , May 2013,\npp. 6645–6649.\n[3] E. Battenberg, J. Chen, R. Child, A. Coates, Y . G. Y . Li, H. Liu,\nS. Satheesh, A. Sriram, and Z. Zhu, “Exploring neural transducers\nfor end-to-end speech recognition,” in Proc. of IEEE Automatic\nSpeech Recognition and Understanding Workshop (ASRU). Ok-\ninawa, Japan: IEEE, 2017, pp. 206–213.\n[4] R. Sennrich, B. Haddow, and A. Birch, “Neural machine\ntranslation of rare words with subword units,” in Proceedings of\nthe 54th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) . Berlin, Germany:\nAssociation for Computational Linguistics, Aug. 2016, pp. 1715–\n1725. [Online]. Available: https://www.aclweb.org/anthology/\nP16-1162\n[5] R. Huang, O. Abdel-Hamid, X. Li, and G. Evermann, “Class\nLM and word mapping for contextual biasing in end-to-end\nASR,” in Interspeech 2020, 21st Annual Conference of the\nInternational Speech Communication Association, Virtual Event,\nShanghai, China, 25-29 October 2020, H. Meng, B. Xu, and T. F.\nZheng, Eds. ISCA, 2020, pp. 4348–4351. [Online]. Available:\nhttps://doi.org/10.21437/Interspeech.2020-1787\n[6] C ¸ . G¨ulc ¸ehre, O. Firat, K. Xu, K. Cho, L. Barrault, H.-C.\nLin, F. Bougares, H. Schwenk, and Y . Bengio, “On using\nmonolingual corpora in neural machine translation,” arXiv\ne-prints, vol. abs/1503.03535, Mar. 2015. [Online]. Available:\nhttps://arxiv.org/abs/1503.03535\n[7] A. Kannan, Y . Wu, P. Nguyen, T. N. Sainath, Z. Chen, and\nR. Prabhavalkar, “An analysis of incorporating an external lan-\nguage model into a sequence-to-sequence model,” in 2018 IEEE\nInternational Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), 2018, pp. 1–5828.\n[8] A. Sriram, H. Jun, S. Satheesh, and A. Coates, “Cold fusion:\nTraining seq2seq models together with language models,” in\nInterspeech 2018, 19th Annual Conference of the International\nSpeech Communication Association, Hyderabad, India, 2-6\nSeptember 2018, B. Yegnanarayana, Ed. ISCA, 2018, pp. 387–\n391. [Online]. Available: https://doi.org/10.21437/Interspeech.\n2018-1392\n[9] C. Shan, C. Weng, G. Wang, D. Su, M. Luo, D. Yu, and L. Xie,\n“Component fusion: Learning replaceable language model com-\nponent for end-to-end speech recognition system,” in ICASSP\n2019 - 2019 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), 2019, pp. 5361–5635.\n[10] S. Kim, Y . Shangguan, J. Mahadeokar, A. Bruguier, C. Fuegen,\nM. L. Seltzer, and D. Le, “Improved neural language model\nfusion for streaming recurrent neural network transducer,”\nCoRR, vol. abs/2010.13878, 2020. [Online]. Available: https:\n//arxiv.org/abs/2010.13878\n[11] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and\nY . Bengio, “End-to-end attention-based large vocabulary speech\nrecognition,” in 2016 IEEE International Conference on\nAcoustics, Speech and Signal Processing, ICASSP 2016,\nShanghai, China, March 20-25, 2016 . IEEE, 2016, pp. 4945–\n4949. [Online]. Available: https://doi.org/10.1109/ICASSP.2016.\n7472618\n[12] M. Ghodsi, X. Liu, J. Apfel, R. Cabrera, and E. Weinstein, “Rnn-\ntransducer with stateless prediction network,” in ICASSP 2020 -\n2020 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), 2020, pp. 7049–7053.\n[13] R. Prabhavalkar, Y . He, D. Rybach, S. Campbell, A. Narayanan,\nT. Strohman, and T. N. Sainath, “Less is more: Improved\nRNN-T decoding using limited label context and path merging,”\nCoRR, vol. abs/2012.06749, 2020. [Online]. Available: https:\n//arxiv.org/abs/2012.06749\n[14] E. Variani, D. Rybach, C. Allauzen, and M. Riley, “Hybrid\nautoregressive transducer (HAT),” in 2020 IEEE International\nConference on Acoustics, Speech and Signal Processing, ICASSP\n2020, Barcelona, Spain, May 4-8, 2020 . IEEE, 2020,\npp. 6139–6143. [Online]. Available: https://doi.org/10.1109/\nICASSP40776.2020.9053600\n[15] E. McDermott, H. Sak, and E. Variani, “A density ratio\napproach to language model fusion in end-to-end automatic\nspeech recognition,” CoRR, vol. abs/2002.11268, 2020. [Online].\nAvailable: https://arxiv.org/abs/2002.11268\n[16] Z. Meng, S. Parthasarathy, E. Sun, Y . Gaur, N. Kanda, L. Lu,\nX. Chen, R. Zhao, J. Li, and Y . Gong, “Internal language model\nestimation for domain-adaptive end-to-end speech recognition,”\nin SLT 2021. IEEE, January 2021.\n[17] G. Pundak, T. N. Sainath, R. Prabhavalkar, A. Kannan, and\nD. Zhao, “Deep context: End-to-end contextual speech recog-\nnition,” in 2018 IEEE Spoken Language Technology Workshop\n(SLT), 2018, pp. 418–425.\n[18] M. Jain, G. Keren, J. Mahadeokar, G. Zweig, F. Metze, and\nY . Saraf, “Contextual RNN-T for Open Domain ASR,” in\nProc. Interspeech 2020 , 2020, pp. 11–15. [Online]. Available:\nhttp://dx.doi.org/10.21437/Interspeech.2020-2986\n[19] D. Zhao, T. N. Sainath, D. Rybach, P. Rondon, D. Bhatia, B. Li,\nand R. Pang, “Shallow-Fusion End-to-End Contextual Biasing,”\nin Proc. Interspeech 2019 , 2019, pp. 1418–1422. [Online].\nAvailable: http://dx.doi.org/10.21437/Interspeech.2019-1209\n[20] F. Weninger, F. Mana, R. Gemello, J. Andr ´es-Ferrer, and P. Zhan,\n“Semi-supervised learning with data augmentation for end-to-end\nASR,” in Interspeech 2020, 21st Annual Conference of the In-\nternational Speech Communication Association, Virtual Event,\nShanghai, China, 25-29 October 2020, H. Meng, B. Xu, and T. F.\nZheng, Eds. ISCA, 2020, pp. 2802–2806.\n[21] A. Graves, N. Jaitly, and A. Mohamed, “Hybrid speech recogni-\ntion with deep bidirectional lstm,” in2013 IEEE Workshop on Au-\ntomatic Speech Recognition and Understanding , 2013, pp. 273–\n278.\n[22] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”\nNeural computation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[23] T. Luong, H. Pham, and C. D. Manning, “Effective approaches\nto attention-based neural machine translation,” in Proceedings of\nthe 2015 Conference on Empirical Methods in Natural Language\nProcessing. Lisbon, Portugal: Association for Computational\nLinguistics, Sep. 2015, pp. 1412–1421. [Online]. Available:\nhttps://www.aclweb.org/anthology/D15-1166\n[24] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, “Dropout: A simple way to prevent neural net-\nworks from overﬁtting,” Journal of Machine Learning Research,\nvol. 15, no. 1, pp. 1929–1958, 2014.\n[25] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph,\nE. D. Cubuk, and Q. V . Le, “SpecAugment: A Simple\nData Augmentation Method for Automatic Speech Recognition,”\nin Proc. Interspeech 2019 , 2019, pp. 2613–2617. [Online].\nAvailable: http://dx.doi.org/10.21437/Interspeech.2019-2680\n[26] P. J. Werbos, “Backpropagation through time: what it does and\nhow to do it,”Proceedings of the IEEE, vol. 78, no. 10, pp. 1550–\n1560, 1990.\n[27] J. Chorowski and N. Jaitly, “Towards better decoding and lan-\nguage model integration in sequence to sequence models,” in\nProc. of INTERSPEECH . Stockholm, Sweden: ISCA, 2017,\npp. 523–527.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8038069009780884
    },
    {
      "name": "Language model",
      "score": 0.6660410761833191
    },
    {
      "name": "Sequence (biology)",
      "score": 0.6410232782363892
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.527237594127655
    },
    {
      "name": "Baseline (sea)",
      "score": 0.5239030122756958
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4801454246044159
    },
    {
      "name": "Speech recognition",
      "score": 0.47606131434440613
    },
    {
      "name": "Natural language processing",
      "score": 0.44842416048049927
    },
    {
      "name": "Programming language",
      "score": 0.07770901918411255
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    }
  ]
}