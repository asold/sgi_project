{
  "title": "Multimodal diagnosis model of Alzheimer’s disease based on improved Transformer",
  "url": "https://openalex.org/W4391023547",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1974703667",
      "name": "Yan Tang",
      "affiliations": [
        "Central South University",
        "Guangxi Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2130649126",
      "name": "Xing Xiong",
      "affiliations": [
        "Central South University"
      ]
    },
    {
      "id": "https://openalex.org/A2372057633",
      "name": "Gan Tong",
      "affiliations": [
        "Central South University"
      ]
    },
    {
      "id": "https://openalex.org/A2099853141",
      "name": "Yuan Yang",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2095945326",
      "name": "Hao Zhang",
      "affiliations": [
        "Central South University"
      ]
    },
    {
      "id": "https://openalex.org/A1974703667",
      "name": "Yan Tang",
      "affiliations": [
        "Guangxi Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2130649126",
      "name": "Xing Xiong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2372057633",
      "name": "Gan Tong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099853141",
      "name": "Yuan Yang",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2095945326",
      "name": "Hao Zhang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4286433291",
    "https://openalex.org/W3216798277",
    "https://openalex.org/W4311442013",
    "https://openalex.org/W2126927216",
    "https://openalex.org/W2171831801",
    "https://openalex.org/W2025581624",
    "https://openalex.org/W2109276615",
    "https://openalex.org/W4307248062",
    "https://openalex.org/W4283156058",
    "https://openalex.org/W4377103877",
    "https://openalex.org/W2523104495",
    "https://openalex.org/W4292466656",
    "https://openalex.org/W2044565330",
    "https://openalex.org/W2894188065",
    "https://openalex.org/W3207018230",
    "https://openalex.org/W2890991727",
    "https://openalex.org/W2942882625",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4312349930",
    "https://openalex.org/W4313007769",
    "https://openalex.org/W4226036172",
    "https://openalex.org/W2038003677",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W2097214564",
    "https://openalex.org/W2127384340",
    "https://openalex.org/W2165278596",
    "https://openalex.org/W2054756808",
    "https://openalex.org/W2442065749",
    "https://openalex.org/W2147619660",
    "https://openalex.org/W2071546430",
    "https://openalex.org/W2157848968",
    "https://openalex.org/W2614542815",
    "https://openalex.org/W2555500380",
    "https://openalex.org/W1998710995",
    "https://openalex.org/W2134610285"
  ],
  "abstract": "Abstract Purpose Recent technological advancements in data acquisition tools allowed neuroscientists to acquire different modality data to diagnosis Alzheimer’s disease (AD). However, how to fuse these enormous amount different modality data to improve recognizing rate and find significance brain regions is still challenging. Methods The algorithm used multimodal medical images [structural magnetic resonance imaging (sMRI) and positron emission tomography (PET)] as experimental data. Deep feature representations of sMRI and PET images are extracted by 3D convolution neural network (3DCNN). An improved Transformer is then used to progressively learn global correlation information among features. Finally, the information from different modalities is fused for identification. A model-based visualization method is used to explain the decisions of the model and identify brain regions related to AD. Results The model attained a noteworthy classification accuracy of 98.1% for Alzheimer’s disease (AD) using the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset. Upon examining the visualization results, distinct brain regions associated with AD diagnosis were observed across different image modalities. Notably, the left parahippocampal region emerged consistently as a prominent and significant brain area. Conclusions A large number of comparative experiments have been carried out for the model, and the experimental results verify the reliability of the model. In addition, the model adopts a visualization analysis method based on the characteristics of the model, which improves the interpretability of the model. Some disease-related brain regions were found in the visualization results, which provides reliable information for AD clinical research.",
  "full_text": "Multimodal diagnosis model of Alzheimer’s \ndisease based on improved Transformer\nYan Tang1,3, Xing Xiong2, Gan Tong2, Yuan Yang4 and Hao Zhang1* \nBackground\nAlzheimer’s disease (AD) is a major neurocognitive impairment, which is the most com -\nmon cause of dementia in people over the age of 65 [1]. It is usually manifested in the \nchanges in memory, abstract thinking, judgment, behavior, and emotion, and finally \ninterferes with the physical control of the body [2]. However, the diagnosis of AD often \nrequires physicians to use various clinical methods including medical history, mental \nstatus tests, physical and neurological exams, diagnostic tests, and brain imaging [3]. \nAbstract \nPurpose: Recent technological advancements in data acquisition tools allowed \nneuroscientists to acquire different modality data to diagnosis Alzheimer’s disease (AD). \nHowever, how to fuse these enormous amount different modality data to improve \nrecognizing rate and find significance brain regions is still challenging.\nMethods: The algorithm used multimodal medical images [structural magnetic reso-\nnance imaging (sMRI) and positron emission tomography (PET)] as experimental data. \nDeep feature representations of sMRI and PET images are extracted by 3D convolution \nneural network (3DCNN). An improved Transformer is then used to progressively learn \nglobal correlation information among features. Finally, the information from differ-\nent modalities is fused for identification. A model-based visualization method is used \nto explain the decisions of the model and identify brain regions related to AD.\nResults: The model attained a noteworthy classification accuracy of 98.1% for Alzhei-\nmer’s disease (AD) using the Alzheimer’s Disease Neuroimaging Initiative (ADNI) data-\nset. Upon examining the visualization results, distinct brain regions associated with AD \ndiagnosis were observed across different image modalities. Notably, the left parahip-\npocampal region emerged consistently as a prominent and significant brain area.\nConclusions: A large number of comparative experiments have been carried \nout for the model, and the experimental results verify the reliability of the model. In \naddition, the model adopts a visualization analysis method based on the characteristics \nof the model, which improves the interpretability of the model. Some disease-related \nbrain regions were found in the visualization results, which provides reliable informa-\ntion for AD clinical research.\nKeywords: Alzheimer’s disease, Deep learning, Multimodal medical images, 3DCNN, \nTransformer, Visualization\nOpen Access\n© The Author(s) 2024. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco mmons. org/ publi \ncdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nRESEARCH\nTang et al. BioMedical Engineering OnLine            (2024) 23:8  \nhttps://doi.org/10.1186/s12938-024-01204-4\nBioMedical Engineering\nOnLine\n*Correspondence:   \nhao@csu.edu.cn\n1 School of Electronic \nInformation, Central South \nUniversity, Changsha 410008, \nHunan, People’s Republic \nof China\n2 School of Computer Science \nand Engineering, Central South \nUniversity, Changsha 410008, \nHunan, People’s Republic \nof China\n3 Guangxi Key Lab \nof Multi-source Information \nMining & Security, Guangxi \nNormal University, Guilin 541004, \nGuangxi, People’s Republic \nof China\n4 Department of Bioengineering, \nUniversity of Illinois Urbana-\nChampaign, Grainger College \nof Engineering, Urbana, IL, USA\nPage 2 of 18Tang et al. BioMedical Engineering OnLine            (2024) 23:8 \nTherefore, a computer-aided AD diagnosis is in urgent need of objective and efficient \nmethods.\nMedical imaging technology is a powerful tool to identify the progression of brain \ndiseases. More specifically, magnetic resonance imaging (MRI) and positron emission \ntomography (PET) can assist in diagnosing the disease and monitoring its progress [4]. \nStructural MRI (sMRI) can well-quantify brain tissue atrophy in patients with AD [5]. \nKLöppel et  al. [6] generated the gray matter density map of the brain using the sMRI \nimages of subjects and realized the identification of AD using the support vector \nmachine (SVM). The PET can monitor the changes in glucose metabolism in the human \nbody [7]. Wen et  al. [8] extracted PET image features and identified AD from healthy \ncontrols by logistic regression. For the features of a single modality, the observed feature \ninformation usually only is provided from a certain perspective. The feature information \nof multiple modalities can realize a more comprehensive study of human brain. There -\nfore, developing AD diagnosis models based on multimodal medical images has become \na new trend. A few recent studies show that multimodal brain imaging data results have \nbetter performance than single-modal data [9–11].\nSome AD-related networks have been discovered and new insights have been provided \nfor the pathological mechanisms of AD with seed-based methods such as hippocampus \nvolume, regional cortical thickness, and temporal lobe. For example, Ardekani et al. pro-\nposed a method of segmenting the hippocampal region to identify AD [12]. Williamson \net al. introduced a connectivity analysis to identify sex-specific AD biomarkers based on \nhippocampal–cortical functional connectivity [13]. However, mounting evidence indi -\ncates that neurodegenerative processes, even if they are highly localized, are associated \nwith disease-specific alterations across the whole brain [14, 15]. Thus, the abnormal pat-\nterns of AD across the whole-brain scale have yet to be investigated.\nRecently, the deep learning approach has attracted a lot of attention to exploring new \nimaging biomarkers for AD diagnosis and prediction, which requires no prior knowl -\nedge to extract biologically meaningful information from subtle changes. Zhang et  al. \n[16] proposed a deep learning framework based on gray matter slices from sMRI. This \nframework combines slices with attention mechanisms and achieves AD classification \nthrough residual networks. However, this slice-based approach leads to the loss of spa -\ntial information in 3D brain images, thereby affecting the classification performance. \nTherefore, Feng et al. [17] proposed to use a 3DCNN to extract features from MRI and \nPET images. They cascade these features and then use a stacked bidirectional recurrent \nneural network (SBi-RNN) to obtain further semantic information for identification. \nHowever, SBi-RNN has the problem of gradient explosion or gradient disappearance. \nTo address this challenge, Feng et al. [18] proposed to use a 3DCNN to extract features \nfrom MRI and PET images. They use fully stacked bidirectional long short-term mem -\nory (FSBi-LSTM) to extract all the spatial information from the feature maps and further \nimprove the performance. However, there are direct or indirect connections between \ndifferent feature maps, and these global correlations are ignored by the above research.\nThe transformer model was first proposed by Vaswani et  al. [19]. It has a powerful \ncapability of global information integration. The self-attention mechanism in it can \nquickly obtain the global correlation between input features without stacking many lay -\ners like CNN, and these computations are all parallel [19]. Thus, it can effectively capture \nPage 3 of 18\nTang et al. BioMedical Engineering OnLine            (2024) 23:8 \n \nthe non-local relationships among all input feature maps. This mechanism also makes \nthe model more interpretable. Vision Transformer (ViT) is a pioneering work of trans -\nformer in the field of computer vision, it and its variants have shown excellent perfor -\nmance in various image-related tasks [20–22]. However, the original ViT only deals with \n2-D images, and the input is the sequence of linear embeddings of image patches [20]. In \nour scenario, the input is 3-D medical images, and directly patching would destructing \nthe connection among brain areas. Hence, we employ 3DCNN to extract features that \nserve as input to the Transformer for AD diagnosis. However, the features extracted by \n3DCNN in the initial stage of model training are not representative, and learning their \nglobal information is meaningless. Therefore, we optimize the transformer by gradually \nintroducing a self-attention mechanism to help model training focus more on feature \nextraction in the initial stage.\nIn summary, we proposed a network model based on 3DCNN and Transformer for AD \ndiagnosis. In specific, an improved Transformer is used to learn the correlation informa-\ntion among features, which are extracted from medical images by 3DCNN. The informa-\ntion from different modalities is fused and identified through the fully connected layer. \nWe conducted extensive experiments on the publicly available ADNI dataset, and the \nmodel demonstrated excellent performance. In addition, we performed interpretability \nanalysis of the model’s decisions using visualization methods based on its characteristics \nand identified several brain regions associated with AD.\nResults\nPerformance of the proposed method\nWe obtained the identification performance with the ACC of 98.10% (precision 99.09%, \nSPE 96.75%, SEN 96.75%, F1 97.81%, AUC 98.35%) in AD diagnosis (see Table 1).\nFirst, we employed permutation tests to assess the statistical significance of the tenfold \ncross-validation results. For each tenfold cross-validation, the identification labels of the \ntraining data were randomly permuted 1000 times. The null hypothesis is that identifier \ncannot learn to predict labels based on the given training set. Experiments show that for \neach training, usually the training set converges or even overfits, while the validation set \ndoes not converge. With accuracy as the statistic, the result (see Fig.  1) of permutation \ndistribution of the estimate revealed that this identifier learned the relationship between \nthe data and the labels with a risk of being wrong with a probability of lower than 0.01.\nThen, we conducted a series of comparative experiments as follows. To demonstrate \nthe superiority of multi-modal fusion, we implemented two single-mode variants of our \nTable 1 Network performance in different settings (mean ± standard deviation, %)\nACC  accuracy, PRE precision, SPE specificity, SEN recall/sensitivity, F1S F1 score\nMethods ACC PRE SPE SEN F1S AUC \nProposed method 98.10 ± 2.46 99.09 ± 2.87 96.75 ± 5.28 95.82 ± 5.03 97.81 ± 2.87 98.35 ± 2.14\nOnly sMRI 91.91 ± 5.96 91.05 ± 10.49 92.72 ± 8.67 90.91 ± 9.41 90.31 ± 6.16 91.33 ± 5.99\nOnly PET 87.14 ± 7.12 85.4 ± 10.31 88.62 ± 8.59 85.05 ± 15.61 83.99 ± 9.80 87.43 ± 6.61\nTypical Transformer 94.32 ± 4.01 93.57 ± 7.76 93.33 ± 6.34 92.25 ± 7.36 93.40 ± 4.25 95.05 ± 3.11\nWithout Transformer 92.86 ± 7.12 90.60 ± 10.31 92.20 ± 8.59 92.03 ± 15.61 90.78 ± 9.80 93.11 ± 6.62\nWithout 3DCNN 90.01 ± 3.52 83.44 ± 8.77 89.46 ± 5.05 93.02 ± 7.59 87.45 ± 4.22 90.06 ± 3.48\nPage 4 of 18Tang et al. BioMedical Engineering OnLine            (2024) 23:8 \nmethod (only sMRI and only PET). Thus, the model was unchanged except for the mul -\ntimodal fusion part. We trained the model with single-mode data (sMRI or PET) and \ntake the results of a tenfold cross-validation. The results are also shown in Table  2, from \nwhich we can find that the multimodal fusion method significantly improved the per -\nformance. Compared with the sMRI only variant, the accuracy was improved by 6.19%, \nwhile the improvement over the PET only variant is 10.96%.\nIn addition, compared with the classical Transformer, our improved solution also \nshows better performance (3.78% higher in accuracy). When the model only included \nthe 3DCNN by excluding the Transformer part, the accuracy of identification is 5.24% \nlower. We also tried to remove the 3DCNN part from the model. We followed the \napproach in ViT [20]: split the 3D image into patches and added positional encoding \nbefore inputting them into the Transformer. The results show that the identification \naccuracy of the model without 3DCNN is 90.01%, which verified the importance of fea -\nture extraction using 3DCNN. The ROC curves of different methods are shown in Fig. 2.\nVisualization results\nWe obtained the features with the highest weight for sMRI and PET respectively through \nthe above method. The highest weighted features for sMRI and PET were 145th and \n133rd features, respectively. We input these features into the decoding network to obtain \nthe key brain regions that make significant contributions to these features. Here, the \nhigher the value of pixels is, the more important it is in the identification process. Thus, \nthe value of pixels in the top 1% and cluster size > 100 remained (see Fig.  3). The most \nimportant brain regions were identified.\nResults show that selected regions refer to the right inferior frontal gyrus, the right \ncerebellum posterior lobe, the left middle temporal gyrus, the right fusiform, and the \nFig. 1 Permutation distribution of the estimate\nTable 2 Cluster distribution statistics of sMRI 142nd feature deconvolution map in brain regions (5 \nclusters)\nRegion MNI coordinates Peak intensity Voxels\nx y z\nRight fusiform 34 − 4 − 43 2.89 72\nRight cerebellum posterior lobe 39 − 78 − 35 2.25 71\nLeft middle temporal gyrus − 51 − − 22 2.31 71\nLeft parahippocampal − 22 − 8 − 22 2.14 69\nRight inferior frontal gyrus 32 36 − 10 2.24 154\nPage 5 of 18\nTang et al. BioMedical Engineering OnLine            (2024) 23:8 \n \nleft parahippocampal in sMRI [see Table  2 and Fig.  3 (top)]. As for PET, the selected \nregions include the left medial frontal gyrus, the left superior frontal, and the left para -\nhippocampal [see Table 3 and Fig. 3 (bottom)].\nFig. 2 ROC curves of different methods\nFig. 3 Clustering results (cluster size > 100) of remaining pixels (top 1%) in MRI (top) and PET (bottom) \nimages\nPage 6 of 18Tang et al. BioMedical Engineering OnLine            (2024) 23:8 \nDiscussion\nRecent technological advancements in data acquisition tools allowed neuroscientists \nto acquire data in a different modality. These data are huge in amount and complex \nin nature. It is an enormous challenge for data scientists to identify intrinsic charac -\nteristics of neurological big data and infer meaningful conclusions from these data. \nMining such an enormous amount of data for pattern recognition requires sophisti -\ncated data-intensive machine learning techniques. Classical data mining techniques \ncould be ineffective when problems get increasingly complicated. We propose a \nrelatively lightweight model, which can efficiently extract meaningful features from \nmedical images. It combines the characteristics of brain images, which can effi -\nciently solve the correlation information between features, and fuse the information \nfrom different modal medical images. Our method increased accuracy, and the use \nof meaningful information.\nComparison with previous methods\nWe compared our proposed method with previous AD diagnosis methods based \non multimodal data, which used sMRI and PET images from the ADNI dataset as \nexperimental data. To ensure a fair comparison, we reproduced their models using \nthe parameters provided in their literature and conducted comparative experiments \non the same dataset. As shown in Table  4, our proposed method outperforms the \nmethods proposed by Feng et  al. [17, 18], who used SBi-RNN and FSBi-LSTM to \nlearn the correlation information between features, and Li et  al. [23], who used a \nVGG-like network to mine multimodal image information. Our approach, which uti -\nlizes an improved Transformer to progressively learn global information of features, \nachieves superior performance in terms of accuracy.\nTable 3 Cluster distribution statistics of PET 133rd feature deconvolution map in brain regions (3 \nclusters)\nRegion MNI coordinates Peak intensity Voxels\nx y z\nLeft parahippocampal − 17 − 4 − 28 1.65 30\nLeft medial frontal gyrus − 1 − 3 59 1.39 84\nLeft superior frontal − 20 − 1 62 1.37 46\nTable 4 Comparison with previous research on AD diagnosis (mean ± standard deviation, %)\nACC  accuracy, PRE precision, SPE specificity, SEN recall/sensitivity, F1S F1 score\nMethods ACC Precision SPE SEN F1S AUC \n3DCNN + SBi-RNN [17] 93.33 ± 5.59 93.67 ± 7.22 94.91 ± 5.90 91.81 ± 8.01 92.52 ± 6.16 93.43 ± 5.56\n3DCNN + FSBi-LSTM [18] 94.76 ± 5.70 95.78 ± 7.21 96.78 ± 5.53 93.76 ± 9.35 94.38 ± 6.12 94.85 ± 5.38\n3D PMNet [23] 96.19 ± 4.92 98.89 ± 3.51 98.89 ± 2.78 93.19 ± 7.99 95.84 ± 5.40 96.47 ± 4.67\nProposed method 98.10 ± 2.46 99.09 ± 2.87 96.75 ± 5.28 95.82 ± 5.03 97.81 ± 2.87 98.35 ± 2.14\nPage 7 of 18\nTang et al. BioMedical Engineering OnLine            (2024) 23:8 \n \nVisualization analysis\nThe brain consists of many brain regions responsible for different tasks. However, not \nall brain regions are related to AD. The most obvious pathological feature of AD is the \nloss of neurons, which is mainly manifested in the development of brain atrophy from \nAD signal area (such as the hippocampus and the temporal lobe) to the whole cor -\ntex [ 24]. Therefore, we try to find these brain regions by utilizing a different method \nfrom the traditional method of shielding brain ROIs, which may ignore some poten -\ntial brain regions related to AD.\nWe made full use of the characteristics of the model to realize visualization and \nfound the brain regions related to AD, which may help to better understand the \npotential pathogenesis of AD. Based on the visualization method in [20], we can \nobtain the weight of each feature through the attention matrix. According to the \nstudy of Zeiler et al., they realized the visualization of the model by deconvoluting the \nfeature map output by convolution [25], and we can also do that on the final features.\nMany studies suggest the temporal lobe transforms sensation into derived meaning \nto properly maintain visual memory, language understanding, and emotional asso -\nciation [26]. Brain atrophy in AD patients is symmetric and primarily affects medial \ntemporal lobe structures [27]. Fusiform is part of the temporal lobe of the brain, this \nregion is critical for face and body recognition. Convicted et al. [28] found that in AD, \nthe volume of the temporal lobe is reduced, and the atrophy of the fusiform gyrus is \nthe most obvious. Vidoni et al. [29] found that people with cognitive impairment had \nincreased fusiform cortex engagement in visual coding tasks. Chang et al. [30] stud -\nied the relationships between regional amyloid burden and GM volume in AD and \nfound pathological co-variance between the fusiform gyrus and para-hippocampus, \nand inferior temporal gyrus. Our structural findings are consistent with these previ -\nous studies and suggest that the etiology and mechanism of AD may be closely related \nto temporal lobe abnormalities.\nThe cerebellum plays an important role in motor function, controlling muscle ten -\nsion and balance. It is a generally neglected area in the study of AD. However, there is \nincreasing evidence that it is also involved in cognitive processing, emotion, and emo -\ntion regulation. The findings of Thomann et al. [31] confirmed that cognitive ability in \nAD patients was significantly associated with the volume of the posterior cerebellar \nlobe. Thus, we speculate that the aberrant cerebellar regions may be partially involved \nin the sluggishness and cognitive decline of AD.\nThe frontal lobe and hippocampus may be related to cognition and memory. \nAccording to the recent studies reported, the frontal lobe is responsible for logic, \nregulating behavior, complex planning, and learning. Alzheimer’s disease gradually \ndamages the frontal lobe as the disease progresses. Laakso et al. [32] found that the \nvolume of the hippocampus and left frontal lobe in the AD group was significantly \nsmaller than that in CN subjects, and the decrease in left hippocampal volume was \nrelated to the decrease in MMSE score and the impairment of language memory. \nOur results for PET showed that the superior frontal gyrus, middle frontal gyrus, and \nparahippocampal may be subject to damage. Our results are consistent with previous \nstudies. Especially, abnormal regions in the left hippocampus appeared in both sMRI \nand PET, which may suggest that the abnormality in this region is particularly related \nPage 8 of 18Tang et al. BioMedical Engineering OnLine            (2024) 23:8 \nto AD. Our results might lead to an improved understanding of the underlying patho -\ngenesis of the disease and provide valuable information for further research on AD.\nConclusion\nIn this study, we proposed a framework based on 3DCNN and an improved Transformer \nfor the diagnosis of Alzheimer’s disease based on multimodal medical images. These \npromising results indicated that AD-related brain disorders can be precisely examined \nwith multimodal medical images and deep learning techniques. We also strengthened \nthe clinical interpretation of our proposed method through the visualization method, \nwhich may provide additional information to facilitate the diagnosis of AD.\nMethods\nFeature learning‑based 3DCNN\nThe CNN has a powerful capability of local feature extraction. However, most CNN \nframework is designed for processing 2D images. For 3D brain images, they are usually \nprocessed into 2D slices, which will lose spatial information. To efficiently extract the \nabundant spatial information of 3D brain images, we adopt the 3D convolution kernel \nin this work. We alternatively stack the convolutional layers and pooling layers to get the \nmulti-level features of multimodality brain images, as shown in Fig. 1.\nIn specific, the input image is convolved with a list of kernel filters in a convolutional \nlayer. Then, a batch normalization layer is added between the activation function and \nconvolution layer to improve the efficiency of training and avoid overfitting. We choose \nrectified linear unit (ReLU) as the activation function. Formally, we define the 3D convo-\nlution operation as follows:\nwhere x, y, and z represent the voxel positions of a 3D image. W l\nkj is the weight of the jth \n3D kernel which connects the kth feature map of layer l1 with the jth feature map of \nlayer l, F l−1\nk  is the kth feature map of the (l1)th layer, and b l\nj is the bias term of the jth fea-\nture map of the lth layer. ReLU is employed as the activation function after the convolu -\ntion of each layer. Finally, the output F l\nj is obtained by summation of the response maps \nof different convolution kernels, which denotes the jth 3D feature map of the lth layer. To \nobtain more efficient and compact features, a max Max-pooling is used to down-sample \nthe feature map after the convolution layer. Through the above operations, we can finally \nget a series of feature maps with rich 3D spatial information.\nProgressive learning of global feature information based on improved Transformer\nTraditionally, the full connection (FC) layer is used to integrate the information of fea -\nture maps for the final identification. However, it just simply connects all neurons and \ncannot effectively take advantage of the spatial information from all feature maps.\nTherefore, we choose to replace the FC layer with the encoder layer of the Transformer \nas in ViT [20]. However, unlike ViT, the input of the transformer module is not the image \npatches, but the feature maps extracted by 3DCNN. According to [33], the convolution \n(1)\nFl\nj(x,y,z) = ReLU(b l\nj +\n∑\nk\n∑\nδx\n∑\nδy\n∑\nδy\nF l−1\nk (x + δx,y + δy,z + δz) ∗ W l\nkj(δx,δy,δz)),\nPage 9 of 18\nTang et al. BioMedical Engineering OnLine            (2024) 23:8 \n \noperation itself has the ability to encode the position information. Therefore, we remove \nthe position embedding mechanism replaced it with convolutional operations to per -\nform positional encoding. Then, an encoder module of the Transformer is used to learn \nglobal correlation information between inputs.\nThe encoder block of the Transformer contains a multi-head self-attention (MSA) \nlayer, and a feed-forward network (FFN) [19]. The normalization layer is applied before \nevery block and residual connections are used after every block, as shown in Fig. 2.\nMuti‑head self‑attention\nThe Self-attention (SA) mechanism is an important component of the transformer \nencoder block, which reduces the dependence on external information and is better at \ncapturing the internal correlation of features [19]. This mechanism mainly solves the \nproblem of long-distance dependence by calculating the interaction among embeddings. \nIt allows each position in the sequence to attend to all other positions, enabling the \nmodel to consider the interdependencies between different elements. In simple terms, \nthe self-attention mechanism calculates attention weights by computing the dot prod -\nuct between the query vector and the key vectors. Then, by scaling these weights and \napplying them to the value vectors, global correlation information between features is \nobtained. Finally, residual connections and feed-forward networks are used to enhance \nthis information.\nUnlike the classical SA module, we use convolutional operations instead of conven -\ntional linear mappings. Convolutional operations can preserve spatial information in the \nfeatures and have fewer parameters than linear mappings, which can improve the com -\nputational efficiency of the model. Furthermore, 1 × 1×1 convolutions can also be used \nfor positional encoding to help the Transformer differentiate the importance of different \npositions in the sequence during attention computation, as shown in Fig. 3.\nHere, we use a convolutional kernel of size 1 × 1 × 1 to transform high-level features \ninto Query(Q), Key(K), and Value(V) matrices in Fig.  3. Then, the Q, K, and V matrices \nare used to compute the attention weights, just like in the Transformer model. The cal -\nculation formula for a single-head self-attention is shown as follows:\nHere, X represents the input features with a total of N samples, d represents the \ndimension of the features, and Conv represents a 3D convolution operation that maps \nthe high-level features to Q, K, and V matrices using a 1 × 1 × 1 convolution kernel. In \nthe calculation of attention weights, first, the inner product of the query and the key \n(QKT) are computed. Then, it is scaled by dividing it by the square root of the dimen -\nsion of the query and key (√d). Finally, the SoftMax operation is applied to obtain the \nattention weights. The attention weights are then used to weight the values, and their \nweighted sum yields the final output. A single-head SA layer has limited capability to \nfocus on a specific entity (or several entities). Thus, several self-attention heads are used \nin MSA layers to allow the learning of different kinds of interdependencies. The calcula -\ntion formula for multi-head self-attention (MSA) module is shown as follows:\n(2)Attention(Q,K,V ) = SoftMax\n(QK T\n√\nd\n)\nV,\nwhere Q = Conv 1(X),K = Conv 2(X), V = Conv 3(X).\nPage 10 of 18Tang et al. BioMedical Engineering OnLine            (2024) 23:8 \nAfter MSA, there is a residual connection to preserve the original information of the \ninput features. Here, we represent the features after residual connection as\nwhere O is the output feature, X the input feature, and α a learnable scalar. Initially, we \nset the value of α to 0, so that the self-attention mechanism is masked at the beginning \nof model training, allowing the 3DCNN to focus on local feature extraction. As train -\ning progresses, the value of α increases, and the model starts to learn global correlation \ninformation between features. The maximum value of α is 1.\nFeed‑forward networks\nAfter each layer passes through attention, there will be an FFN, which is used for spa -\ntial transformation. The FFN contains two linear transformation layers with the ReLU \nactivation function. The FNN performs dimension expansion/reduction and nonlinear \ntransformation on each token to enhance the representation ability of the tokens, thus \nincreasing the performance ability of the model:\nwhere \n  is the weight of the first layer, which projects X into a higher dimen -\nsion D. \n  is the weight of the second layer, and b1 and b2 are the biases.\nThe output of the transformer layer is transformed linearly through the MLP layer, and \nfinally identified by the SoftMax function.\nNetwork model framework\nIn this experiment, we use 3DCNN to extract features of sMRI and PET. To extract the \ndifferential information of sMRI and PET, we built and trained a 3DCNN network for \nsMRI and PET respectively, while they share the same network structure. We obtained \n200 features with dimensions of 2 × 2 × 2 after applying 3DCNN. Each feature repre -\nsents one part of the brain. Then, the encoder block of the Transformer is used to extract \ninteractive information among various features instead of the traditional FC layer. Here, \nsMRI and PET feature also shared the same transformer module. Finally, the learned \ninformation was concatenated and further passed to MLP for disease diagnosis. The \noverall framework of the network model is shown in Fig.  4. The 3DCNN and the Trans -\nformer framework were simultaneously trained in the end-to-end framework.\nModel visualization\nGiven the complexity and high risk of medical decision-making, model interpretation is \nparticularly important for medical imaging applications. Incorrect diagnosis or failure to \ndetect diseases could be detrimental to patients, and therefore, it is necessary to explain \nthe reasons behind the decisions made by deep learning models.\nIt has been verified that through the relevance of a feature to identification, the identi -\nfiable power of the feature can be quantitatively measured by the attention matrix. Then, \n(3)\nMultiHead(X ) = Concat(head1, head2, ..., headh )\nwhere head i = Attention(Conv i\n1(X ), Convi\n2(X ), Convi\n3(X ))\n(4)X′= αO + X,\n(5)FFX(X) = max(0,XW 1+b1)W 2+b2,\nPage 11 of 18\nTang et al. BioMedical Engineering OnLine            (2024) 23:8 \n \nthe important brain region can be obtained by decoding the feature through a deconvo -\nlution network [25]. The process of visualization is shown in Fig. 5.\nGet the feature with the highest weight\nThe Attention Rollout is used to compute the attention map from output tokens to input \nfeatures. In specific, since the residual connections in the self-attention and FFN layers \nFig. 4 The architecture of deep 3D CNNs denoted with the sizes of each layer’s input, convolution, max \npooling, and output layers and the numbers and sizes of generated feature maps. C is a convolutional layer, \nthe P is max pooling layer, @ is the number of filters such as 15@ 3 × 3 × 3 is 15 filters whose size are 3 × 3 × 3 \nand P 2 × 2 × 2 is pooling layers, with a size of 2 × 2 × 2. The number below each layer represents the shape \nof the feature\nFig. 5 The struct of transformer encoder\nPage 12 of 18Tang et al. BioMedical Engineering OnLine            (2024) 23:8 \nof the Transformer modules (as shown in Fig.  2) play an important role in connecting \nthe corresponding positions across different layers, we add extra weights to represent \nthe residual connections for computing the attention rollout map as follows:\nIn the formula, A represents the attention matrix considering the residual connec -\ntions, W represents the original attention matrix, and I represent the identity matrix. \nConsidering that the residual connection parameter α after the MSA layer approaches \n1 (0.996) in the later stages of training, the weight of the residual is set to 0.5. Finally, to \ncalculate the attention from the feature input layer to the output layer, we recursively \nmultiply the attention matrices of previous layers in all subsequent layers. The formula \nfor calculating the attention rollout for the ith layer is shown below.\n˜A(l i) represents the attention calculation of the ith layer during the attention rollout \nprocess, which is multiplied with the layer attention matrix A through matrix multipli -\ncation. Each row of the matrix represents the attention weight between a feature and \nother features. Then, the average of all attention matrices is taken along the row and \ncolumn dimensions, resulting in a 200-dimensional vector. This vector indicates the con-\ntribution weights of the 200 different features of the same modality to the classification \nresults. The feature with the highest weight is then selected for subsequent visualization \nresearch.\nFeature decoding\nTo find out the relevant brain regions for AD diagnosis, two deconvolution networks \n(for sMRI and PET) are trained whose structures were mirror images of the 3DCNN \nparts. Deconvolution networks can restore the features extracted by 3DCNN to the \noriginal image. Thus, we transform the features with the highest weight into pixels using \nthe trained deconvolution networks for analysis.\nData and preprocessing\nIn this experiment, we used the open-access sMRI and PET datasets from Alzheimer’s \nDisease Neuroimaging Initiative (ADNI) database 1. ADNI is multicenter research to \nsearch for clinical, imaging, genetic, and biochemical biomarkers for the discovery of \nAD. We used the 18F-FDG-PET and sMRI data downloaded from ADNI with each pair \nof FDG-PET and sMRI for the same subject captured at the same time. All sMRI scans \n(T1-weighted MP-RAGE sequence at 1.5 T) used in our work were acquired from 1.5 T \nscanners and typically consisted of 256 × 256 × 176 voxels with a size of approximately \n1 mm × 1 mm × 1.2 mm. The PET images have many different specifications, but they \nwere finally processed into a unified format.\n(6)A = 0.5W + 0.5I.\n(7)˜A(l i) =\n{\nA(l i)˜A(l i−1 ) if i > j\nA(l i) if i = j\n1 https:// adni. loni. usc. edu.\nPage 13 of 18\nTang et al. BioMedical Engineering OnLine            (2024) 23:8 \n \nOur dataset consists of 210 subjects, consisting of 88 AD subjects and 122 cogni -\ntively normal (CN) subjects. The male to female ratio is 106/104. The age of the subjects \nranges from 56 to 92, and there is no difference in age and gender between AD and CN \nsubjects (p = 0.0513). Some previous studies did not consider the balance of gender and \nage, so the features extracted from the data may not be related to disease, which may be \nrelated to gender or age, so we strictly controlled for their balance. The characteristics of \nthe subjects are summarized in Table 5.\nFor the sMRI data, we conducted Anterior Commissure (AC) – Posterior Commissure \n(PC) reorientation via MIPAV software. 2 Tissue intensities inhomogeneity is then cor -\nrected using the N3 algorithm [34]. Skull stripping, cerebellum removal, and three main \ntissues [gray matter (GM), white matter (WM), and cerebrospinal fluid (CSF)] segmen -\ntation were conducted via the Cat12 tool of SPM12 3. Existing research shows that GM \ndemonstrated higher relatedness to AD [35, 36]. Therefore, we chose the GM masks in \nthis work. Finally, we used the hierarchical attribute matching mechanism for the elastic \nregistration (HAMMER) algorithm [37] to spatially register the GM masks to the tem -\nplate of the Montreal Neurological Institute (MNI) 152 [38].\nFor the PET, first, we realigned them to the mean image. Then, we registered it to the \ncorresponding sMRI image. Finally, in common with sMRI images, they were registered \nto the MNI152 brain atlas.\nFinally, all the sMRI and PET data were smoothed to a common resolution of 8-mm \nfull-width at half-minimum. And they were all down-sampled to 64 × 64 × 64.\nExperiment settings\nThe ranges of pixel values of each sMRI or PET are different, hence, we normalized the \npreprocessed sMRI and PET images to the same range for a subject. We used min–max \nnormalization to scale all pixel values into 0–1 as follows:\nwhere z is the normalized pixel values for sMRI or PET.\nAs shown in Fig.  1, the 3DCNN part consisted of 5 stacked convolutional and max-\npooling layers. A separate convolution layer was used as the last layer. A batch normali -\nzation layer and an ReLU activation function were added after each convolution layer. \nWe set all convolutional layer strides to 2 and padding was set to be the same as layer \n(8)z = x − min (x)\nmax (x) − min (x) ,\nTable 5 Characteristics of the subjects in the ADNI dataset (mean ± standard deviation)\nAD CN\nGender (M/F) 46/42 60/62\nAge (years) 75.43 ± 8.20 77.42 ± 6.48\nMMSE 22.32 ± 2.67 28.93 ± 1.32\nGlobal CDR 0.86 ± 0.31 0.10 ± 0.20\n2 https:// mipav. cit. nih. gov.\n3  www. fil. ion. ucl. ac. uk/ spm/.\nPage 14 of 18Tang et al. BioMedical Engineering OnLine            (2024) 23:8 \ninput. The structure of 3DCNN for sMRI and PET The structure of 3DCNN used to \nextract MRI and pet features is the same, but they do not share parameters. In the Trans-\nformer part, we chose the encoder block of the framework, but the number of heads \nis set at 4. To avoid overfitting, we just stacked two layers of the transformer encoder \nblock. Then two linear transformations were performed in the MLP part, and a dropout \nwith probability of 0.1 was performed after each linear transformation. We chose Adam \noptimizer (with default parameters) to optimize the model parameters and categorical \ncross-entropy as the loss function, which is suitable for identification tasks. We set the \nbatch size to 11, the number of epochs to 60, and the learning rate to  10−4. The learning \nrate was decaying every 20 epochs, and the decay factor was set to 0.1. We set the ran -\ndom number seed for experiment debugging.\nA tenfold cross-validation algorithm was adopted to evaluate the identification perfor-\nmance. In specific, all samples were randomly divided into 10 portions to evenly distrib -\nuted AD and CN data in every portion. Then, samples from two portions were used as \nthe testing data (21 subjects) and the validation data (21 subjects) respectively, while the \nrest were utilized as the training data (168 subjects). The cross-validation algorithm was \napplied and the final identification accuracy was obtained by averaging the results of 10 \ntests.\nIn the cross-validation scheme, the model parameters and features were not neces -\nsarily the same across all loops. Several parameters were used to evaluate the identifica -\ntion performance, including accuracy (ACC), precision (PRE), specificity (SPE), recall/\nsensitivity (SEN), F1 score (F1S), and area under receive operation curve (AUC). PRE \nindicated how many of the positive values predicted by the model are positive. F1 is the \nharmonic average of accuracy and recall/sensitivity, which was a comprehensive evalua -\ntion index. AUC can intuitively evaluate the quality of the identifier (Figs. 6, 7, 8).\nFor deconvolution networks in visualization, we set the batch size to 20, the num -\nber of epochs to 3000, and the learning rate to  10−4. The learning rate was decay \nevery 500 epochs with the decay factor of 0.5. Adam optimizer was used to speed up \nFig. 6 Convolution-based self-attention mechanism\nPage 15 of 18\nTang et al. BioMedical Engineering OnLine            (2024) 23:8 \n \ntraining. We utilized the nearest neighbor interpolation algorithm as the up-sampling \nalgorithm and the mean square error (MSE) loss as the loss function, which could \nbetter measure the reconstruction error.\nFig. 7 The framework of network model\nFig. 8 Visualization framework\nPage 16 of 18Tang et al. BioMedical Engineering OnLine            (2024) 23:8 \nAbbreviations\nAD  Alzheimer’s disease\nsMRI  Structural magnetic resonance imaging\nPET  Positron emission tomography\n3DCNN  Three-dimensional convolutional neural network\nADNI  Alzheimer’s disease neuroimaging initiative\nSBi-RNN  Stacked bidirectional recurrent neural network\nFSBi-LSTM  Stacked bidirectional long short-term memory\nViT  Vision Transformer\nReLU  Rectified linear unit\nFC  Full connection\nMSA  Multi-head self-attention\nFFN  Feed-forward network\nCN  Cognitively normal\nAC  Anterior commissure\nPC  Posterior commissure\nGM  Gray matter\nWM  White matter\nCSF  Cerebrospinal fluid\nMNI  Montreal neurological institute\nACC   Accuracy\nPRE  Precision\nSPE  Specificity\nSEN  Sensitivity\nF1S  F1 score\nAUC   Area under receive operation curve\nMSE  Mean square error\nAcknowledgements\nThis work was supported in part by the High-Performance Computing Center of Central South University.\nAuthor contributions\nAll authors contributed to the study conception and design.\nFunding\nThe author would like to thank the Research Fund of the Guangxi Key Lab of Multi-source Information Mining and Secu-\nrity [Grant number MIMS20-08] for their supports.\nAvailability of data and materials\nData collection and sharing for this project were funded by the Alzheimer’s Disease Neuroimaging Initiative (ADNI). \nThis investigation was led by Michael W. Weiner (Michael.Weiner@ucsf.edu) and the complete list of collaborators can \nbe found at https:// adni. loni. usc. edu/ wp- conte nt/ uploa ds/ how_ to_ apply/ ADNI_ Ackno wledg ement_ List. pdf. ADNI \nis funded by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, and \nthrough generous contributions from the following: AbbVie, Alzheimer’s Association; Alzheimer’s Drug Discovery \nFoundation; Araclon Biotech; BioClinica, Inc.; Biogen; Bristol-Myers Squibb Company; CereSpir, Inc.; Cogstate; Eisai Inc.; \nElan Pharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; F. Hoffmann-La Roche Ltd and its affiliated company \nGenentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer Immunotherapy Research & Development, \nLLC.; Johnson & Johnson Pharmaceutical Research & Development LLC.; Lumosity; Lundbeck; Merck & Co., Inc.; Meso \nScale Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis Pharmaceuticals Corporation; Pfizer Inc.; \nPiramal Imaging; Servier; Takeda Pharmaceutical Company; and Transition Therapeutics. The Canadian Institutes of \nHealth Research is providing funds to support ADNI clinical sites in Canada. Private sector contributions are facilitated by \nthe Foundation for the National Institutes of Health (www. fnih. org). The grantee organization is the Northern California \nInstitute for Research and Education, and the study is coordinated by the Alzheimer’s Therapeutic Research Institute at \nthe University of Southern California. ADNI data are disseminated by the Laboratory for Neuro Imaging at the University \nof Southern California. The dataset of this paper was obtained from Alzheimer’s Disease Neuroimaging Initiative (ADNI4).\nDeclarations\nEthics approval and consent to participate\nThis study was approved by the Research Ethics Committee of Central South University and The Laboratory for Neuro \nImaging at the University of California. The authors have agreed to participate in this work.\nConsent for publication\nThe publication of this work was approved by Central South University and The Laboratory for Neuro Imaging at the \nUniversity of California.\nCompeting interests\nThe authors declare that they have no conflict of interest.\n4 https:// adni. loni. usc. edu/.\nPage 17 of 18\nTang et al. BioMedical Engineering OnLine            (2024) 23:8 \n \nReceived: 5 April 2023   Accepted: 8 January 2024\nReferences\n 1. Gauthier S, Webster C, Servaes S, Morais J, Rosa-Neto P . World Alzheimer report 2022: life after diagnosis: navigating \ntreatment, care and support. London: Alzheimer’s Disease International London; 2022.\n 2. Javeed A, Dallora AL, Berglund JS, Anderberg P . An intelligent learning system for unbiased prediction of dementia \nbased on autoencoder and adaboost ensemble learning. Life. 2022;12(7):1097.\n 3. Loddo A, Buttau S, Di Ruberto C. Deep learning based pipelines for Alzheimer’s disease diagnosis: a comparative \nstudy and a novel deep-ensemble method. Comput Biol Med. 2022;141: 105032.\n 4. Shoeibi A, Khodatars M, Jafari M, Ghassemi N, Moridian P , Alizadesani R, Ling SH, Khosravi A, Alinejad-Rokny H, \nLam H. Diagnosis of brain diseases in fusion of neuroimaging modalities using deep learning: a review. Inf Fus. \n2022;93:85–117.\n 5. McEvoy LK, Fennema-Notestine C, Roddey JC, Hagler DJ Jr, Holland D, Karow DS, Pung CJ, Brewer JB, Dale AM. Alz-\nheimer disease: quantitative structural neuroimaging for detection and prediction of clinical and structural changes \nin mild cognitive impairment. Radiology. 2009;251(1):195–205.\n 6. Klöppel S, Stonnington CM, Chu C, Draganski B, Scahill RI, Rohrer JD, Fox NC, Jack CR Jr, Ashburner J, Frackowiak RS. \nAutomatic classification of MR scans in Alzheimer’s disease. Radiology. 2008;131(3):681–9.\n 7. Ferreira LK, Busatto GF. Neuroimaging in Alzheimer’s disease: current role in clinical practice and potential future \napplications. Clinics. 2011;66:19–24.\n 8. Wen L, Bewley M, Eberl S, Fulham M, Feng D. Classification of dementia from FDG-PET parametric images using data \nmining. In: 2008 . New York: IEEE; 2008. p. 412–5.\n 9. Rallabandi VS, Seetharaman K. Deep learning-based classification of healthy aging controls, mild cognitive impair-\nment and Alzheimer’s disease using fusion of MRI-PET imaging. Biomed Signal Process Control. 2023;80: 104312.\n 10. Qiu S, Miller MI, Joshi PS, Lee JC, Xue C, Ni Y, Wang Y, Anda-Duran D, Hwang PH, Cramer JA. Multimodal deep learn-\ning for Alzheimer’s disease dementia assessment. Nat Commun. 2022;13(1):1–17.\n 11. Shukla A, Tiwari R, Tiwari S. Alzheimer’s disease detection from fused PET and MRI modalities using an ensemble \nclassifier. Mach Learn Knowl Extr. 2023;5(2):512–38.\n 12. Ardekani BA, Bermudez E, Mubeen AM, Bachman AH. Initiative AsDN: prediction of incipient Alzheimer’s disease \ndementia in patients with mild cognitive impairment. J Alzheimers Dis. 2017;55(1):269–81.\n 13. Williamson J, Yabluchanskiy A, Mukli P , Wu DH, Sonntag W, Ciro C, Yang Y. Sex differences in brain functional con-\nnectivity of hippocampus in mild cognitive impairment. Front Aging Neurosc. 2022;14: 959394.\n 14. Brooks DJ, Pavese N. Imaging biomarkers in Parkinson’s disease. Prog Neurobiol. 2011;95(4):614–28.\n 15. Tang Y, Liu B, Yang Y. Wang C-m, Meng L, Tang B-s, Guo J-f. Identifying mild-moderate Parkinson’s disease using \nwhole-brain functional connectivity. Clin Neurophysiol. 2018;129(12):2507–16.\n 16. Zhang Y, Teng Q, Liu Y, Liu Y, He X. Diagnosis of Alzheimer’s disease based on regional attention with sMRI gray mat-\nter slices. J Neurosci Methods. 2022;365: 109376.\n 17. Feng C, Elazab A, Yang P , Wang T, Lei B, Xiao X. 3D convolutional neural network and stacked bidirectional recurrent \nneural network for Alzheimer’s disease diagnosis. In: Predictive intelligence in medicine. Berlin: Springer; 2018.\n 18. Feng C, Elazab A, Yang P , Wang T, Zhou F, Hu H, Xiao X, Lei B. Deep learning framework for Alzheimer’s disease diag-\nnosis via 3D-CNN and FSBi-LSTM. IEEE Access. 2019;7:63605–18.\n 19. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I. Attention is all you need. Adv \nNeural Inf Process Syst. 2017. https:// doi. org/ 10. 48550/ arXiv. 1706. 03762.\n 20. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Dehghani M, Minderer M, Heigold G, \nGelly S. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv. 2020. https:// doi. org/ 10. \n48550/ arXiv. 2010. 11929.\n 21. Liu Z, Hu H, Lin Y, Yao Z, Xie Z, Wei Y, Ning J, Cao Y, Zhang Z, Dong L.. Swin transformer v2: Scaling up capacity \nand resolution In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition; 2022. p \n12009–19.\n 22. Dong X, Bao J, Chen D, Zhang W, Yu N, Yuan L, Chen D, Guo B. Cswin transformer: a general vision transformer \nbackbone with cross-shaped windows: In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern \nRecognition; 2022. p 12124–34.\n 23. Li C, Song L, Zhu G, Hu B, Liu X, Wang Q 2022. Alzheimer’s level classification by 3D PMNet using PET/MRI multi-\nmodal images. In 2022 IEEE International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA). \nNew York: IEEE: p 1068–73.\n 24. Frisoni GB, Fox NC, Jack CR, Scheltens P , Thompson PM. The clinical use of structural MRI in Alzheimer disease. Nat \nRev Neurol. 2010;6(2):67–77.\n 25. Zeiler MD, Fergus R. Visualizing and understanding convolutional networks. Berlin: In European conference on \ncomputer vision. Springer; 2014.\n 26. Dickerson BC, Sperling RA. Functional abnormalities of the medial temporal lobe memory system in mild cognitive \nimpairment and Alzheimer’s disease: insights from functional MRI studies. Neuropsychologia. 2008;46(6):1624–35.\n 27. Chan D, Fox NC, Scahill RI, Crum WR, Whitwell JL, Leschziner G, Rossor AM, Stevens JM, Cipolotti L, Rossor MN. Pat-\nterns of temporal lobe atrophy in semantic dementia and Alzheimer’s disease. Ann Neurol. 2001;49(4):433–42.\n 28. Convit A, De Leon M, Tarshish C, De Santi S, Tsui W, Rusinek H, George A. Specific hippocampal volume reductions in \nindividuals at risk for Alzheimer’s disease. Neurobiol Aging. 1997;18(2):131–8.\n 29. Vidoni ED, Thomas GP , Honea RA, Loskutova N, Burns JM. Evidence of altered corticomotor system connectivity in \nearly-stage Alzheimer’s disease. J Neurol Phys Ther. 2012;36(1):8.\nPage 18 of 18Tang et al. BioMedical Engineering OnLine            (2024) 23:8 \n 30. Chang Y-T, Huang C-W, Chen N-C, Lin K-J, Huang S-H, Chang W-N, Hsu S-W, Hsu C-W, Chen H-H, Chang C-C. Hip-\npocampal amyloid burden with downstream fusiform gyrus atrophy correlate with face matching task scores in \nearly stage Alzheimer’s disease. Frontiers aging neurosci. 2016;8:145.\n 31. Thomann PA, Schläfer C, Seidl U, Dos Santos V, Essig M, Schröder J. The cerebellum in mild cognitive impairment \nand Alzheimer’s disease—a structural MRI study. J Psychiatr Res. 2008;42(14):1198–202.\n 32. Laakso M, Partanen K, Riekkinen P , Lehtovirta M, Helkala E-L, Hallikainen M, Hanninen T, Vainio P , Soininen H. Hip-\npocampal volumes in Alzheimer’s disease, Parkinson’s disease with and without dementia, and in vascular demen-\ntia: an MRI study. Neurology. 1996;46(3):678–81.\n 33. Islam MA, Jia S, Bruce ND. How much position information do convolutional neural networks encode. arXiv preprint. \narXiv:200108248. 2020\n 34. Sled JG, Zijdenbos AP , Evans AC. A nonparametric method for automatic correction of intensity nonuniformity in \nMRI data. IEEE Trans Med Imaging. 1998;17(1):87–97.\n 35. Zhang J, Liu M, An L, Gao Y, Shen D. Alzheimer’s disease diagnosis using landmark-based features from longitudinal \nstructural MR images. IEEE J Biomed Health Inform. 2017;21(6):1607–16.\n 36. Liu M, Zhang J, Yap P-T, Shen D. View-aligned hypergraph learning for Alzheimer’s disease diagnosis with incom-\nplete multi-modality data. Med Image Anal. 2017;36:123–34.\n 37. Shen D, Davatzikos C. HAMMER: hierarchical attribute matching mechanism for elastic registration. IEEE Trans Med \nImaging. 2002;21(11):1421–39.\n 38. Lancaster JL, Tordesillas-Gutiérrez D, Martinez M, Salinas F, Evans A, Zilles K, Mazziotta JC, Fox PT. Bias between MNI \nand Talairach coordinates analyzed using the ICBM-152 brain template. Hum Brain Mapp. 2007;28(11):1194–205.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6821177005767822
    },
    {
      "name": "Interpretability",
      "score": 0.6644752621650696
    },
    {
      "name": "Visualization",
      "score": 0.6103570461273193
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6034227609634399
    },
    {
      "name": "Neuroimaging",
      "score": 0.5531837940216064
    },
    {
      "name": "Modality (human–computer interaction)",
      "score": 0.49660974740982056
    },
    {
      "name": "Modalities",
      "score": 0.4764059782028198
    },
    {
      "name": "Parahippocampal gyrus",
      "score": 0.4231169819831848
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.40179210901260376
    },
    {
      "name": "Machine learning",
      "score": 0.3963684141635895
    },
    {
      "name": "Neuroscience",
      "score": 0.2085840106010437
    },
    {
      "name": "Temporal lobe",
      "score": 0.14323532581329346
    },
    {
      "name": "Psychology",
      "score": 0.14181748032569885
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Epilepsy",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139660479",
      "name": "Central South University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    }
  ]
}