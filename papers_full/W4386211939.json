{
  "title": "Evaluation of a Large Language Model to Identify Confidential Content in Adolescent Encounter Notes",
  "url": "https://openalex.org/W4386211939",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2200861510",
      "name": "Naveed Rabbani",
      "affiliations": [
        "Palo Alto University",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2941277441",
      "name": "Conner Brown",
      "affiliations": [
        "Lucile Packard Children's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2560287309",
      "name": "Michael Bedgood",
      "affiliations": [
        "Children's Hospital of Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2536771456",
      "name": "Rachel L Goldstein",
      "affiliations": [
        "Palo Alto University",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2252105161",
      "name": "Jennifer L. Carlson",
      "affiliations": [
        "Stanford University",
        "Palo Alto University"
      ]
    },
    {
      "id": "https://openalex.org/A2078812167",
      "name": "Natalie M Pageler",
      "affiliations": [
        "Stanford University",
        "Palo Alto University"
      ]
    },
    {
      "id": "https://openalex.org/A2999044050",
      "name": "Keith E. Morse",
      "affiliations": [
        "Palo Alto University",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2200861510",
      "name": "Naveed Rabbani",
      "affiliations": [
        "Palo Alto University",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2941277441",
      "name": "Conner Brown",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2560287309",
      "name": "Michael Bedgood",
      "affiliations": [
        "Children's Hospital of Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2536771456",
      "name": "Rachel L Goldstein",
      "affiliations": [
        "Stanford University",
        "Palo Alto University"
      ]
    },
    {
      "id": "https://openalex.org/A2252105161",
      "name": "Jennifer L. Carlson",
      "affiliations": [
        "Palo Alto University",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2078812167",
      "name": "Natalie M Pageler",
      "affiliations": [
        "Palo Alto University",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2999044050",
      "name": "Keith E. Morse",
      "affiliations": [
        "Palo Alto University",
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4362522866",
    "https://openalex.org/W3091545942",
    "https://openalex.org/W4368227661",
    "https://openalex.org/W4323826290",
    "https://openalex.org/W4229369881"
  ],
  "abstract": "Abstract Introduction In adolescent care, information sharing through patient portals can lead to unintentional disclosures to patients’ guardians around protected health topics such as mental health, sexual health, and substance use. A persistent challenge facing pediatric health systems is configuring systems to withhold confidential information recorded as free text in encounter notes. This study evaluates the accuracy of a proprietary large language model (LLM) in identifying content relating to adolescent confidentiality in such notes. Methods A random sample of 300 notes were selected from outpatient adolescent encounters performed at an academic pediatric health system. The notes were manually reviewed by a group of pediatricians to identify confidential content. A proprietary LLM, GPT-3.5 (OpenAI, San Francisco, CA), was prompted using a “few-shot learning” method to identify the confidential content within these notes. Two primary outcomes were considered: (1) the ability of the LLM to determine whether a progress note contains confidential content and (2) its ability to identify the specific confidential content within the note. Results Of the 300 sampled notes, 91 (30%) contained confidential content. The LLM was able to classify whether an adolescent progress note contained confidential content with a sensitivity of 97% (88/91), specificity of 18% (37/209), and positive predictive value of 34% (88/260). Only 40 of the 306 manually reviewed excerpts (13%) were accurately derived from the original note (ie. contained no hallucinations), 22 (7%) of which represented the note’s actual confidential content. Discussion A proprietary LLM achieved a high sensitivity in classifying whether adolescent encounter notes contain confidential content. However, its low specificity and poor positive predictive value limit its usefulness. Furthermore, an alarmingly high fraction of confidential note excerpts proposed by the model contained hallucinations. In its current form, GPT-3.5 cannot reliably identify confidential content in free-text adolescent progress notes.",
  "full_text": "Evaluation of a Large Language Model to Identify Confidential Content in Adolescent\nEncounter Notes\nNaveed Rabbani, MDa; Conner Brown, BSb; Michael Bedgood, MDc; Rachel L. Goldstein, MDa;\nJennifer L. Carlson, MDa; Natalie M. Pageler, MD MEda; Keith E. Morse, MD MBAa\nAffiliations:\na. Department of Pediatrics, Stanford University, Palo Alto, CA, USA\nb. Information Services, Lucile Packard Children’s Hospital, Palo Alto, CA, USA\nc. Department of Emergency and Transport Medicine, Children’s Hospital Los Angeles, Los\nAngeles, CA, USA\nCorresponding Author:Naveed Rabbani, Department of Pediatrics, Stanford University School\nof Medicine; 453 Quarry Road, MC 5660, Palo Alto, CA 94304; nrabbani@stanford.edu\nShort title:Evaluating an LLM for Adolescent Confidentiality\nConflict of Interest Disclosures:Rachel Goldstein reports that she is a Nexplanon Clinical\ntrainer for Organon & Co.\nFunding/Support: No funding was secured for this study.\nData Sharing Agreement: The adolescent encounter notes are not publicly available as they\ncontain protected health information. However, the model prompt is shared as supplementary\nmaterial.\nHuman Subjects Statement: The presented work was performed as part of health information\ntechnology operations improvement at our institution and does not qualify as human subjects\nresearch.\nWord Count:600 words\nAbbreviations: LLM = Large Language Model\nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted August 28, 2023. ; https://doi.org/10.1101/2023.08.25.23294372doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nAbstract\nIntroduction: In adolescent care, information sharing through patient portals can lead to\nunintentional disclosures to patients' guardians around protected health topics such as mental\nhealth, sexual health, and substance use. A persistent challenge facing pediatric health systems is\nconfiguring systems to withhold confidential information recorded as free text in encounter\nnotes. This study evaluates the accuracy of a proprietary large language model (LLM) in\nidentifying content relating to adolescent confidentiality in such notes.\nMethods: A random sample of 300 notes were selected from outpatient adolescent encounters\nperformed at an academic pediatric health system. The notes were manually reviewed by a group\nof pediatricians to identify confidential content. A proprietary LLM, GPT-3.5 (OpenAI, San\nFrancisco, CA), was prompted using a \"few-shot learning\" method to identify the confidential\ncontent within these notes. Two primary outcomes were considered: (1) the ability of the LLM to\ndetermine whether a progress note contains confidential content and (2) its ability to identify the\nspecific confidential content within the note.\nResults: Of the 300 sampled notes, 91 (30%) contained confidential content. The LLM was able\nto classify whether an adolescent progress note contained confidential content with a sensitivity\nof 97% (88/91), specificity of 18% (37/209), and positive predictive value of 34% (88/260).\nOnly 40 of the 306 manually reviewed excerpts (13%) were accurately derived from the original\nnote (ie. contained no hallucinations), 22 (7%) of which represented the note's actual confidential\ncontent.\nDiscussion: A proprietary LLM achieved a high sensitivity in classifying whether adolescent\nencounter notes contain confidential content. However, its low specificity and poor positive\npredictive value limit its usefulness. Furthermore, an alarmingly high fraction of confidential\nnote excerpts proposed by the model contained hallucinations. In its current form, GPT-3.5\ncannot reliably identify confidential content in free-text adolescent progress notes.\nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted August 28, 2023. ; https://doi.org/10.1101/2023.08.25.23294372doi: medRxiv preprint \nIntroduction\nIn adolescent care, information sharing through patient portals can lead to unintentional\ndisclosures to patients’ guardians around protected health topics like mental health, sexual\nhealth, and substance use. Maintaining confidentiality in the wake of the 21st Century Cures Act\nis crucial to providing equitable care to this population1.\nPediatric institutions have employed a variety of methods to prevent such unintentional\ndisclosures, including configuring specific types of medical information from being released to\npatient portals2. A persistent challenge is withholding confidential information recorded as free\ntext in visit notes, up to 21% of which have been shown to contain confidential information3.\nElectronic health records lack functionality to identify and filter such information, and while\nnatural language processing algorithms have shown promise4, there is no widely accessible\nsolution.\nThe emergence of large language models (LLMs) offers a potential solution. LLMs are capable\nof summarizing large bodies of text and extracting relevant clinical information5. The ability of\nLLMs to accurately identify the presence of confidential content in clinical notes has not been\nstudied.  We evaluate the accuracy of a proprietary LLM in identifying content related to mental\nhealth, sexual health, and substance use in adolescent progress notes.\nMethods\nA random sample of 300 visit notes were selected from outpatient adolescent encounters\nperformed at an academic pediatric health system between January 1, 2016 and December 31,\nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted August 28, 2023. ; https://doi.org/10.1101/2023.08.25.23294372doi: medRxiv preprint \n2019. The notes were manually reviewed by a group of pediatricians specifically trained in\nadolescent confidentiality laws to identify confidentiality disclosures3,6.\nA proprietary LLM, GPT-3.5 (OpenAI, San Francisco, CA), was prompted to identify\nconfidential content within these notes. The LLM was accessed via a secure computing platform\napproved for use with protected health information. Two primary outcomes were considered: (1)\nthe ability of the LLM to determine whether a progress note contains confidential content (note\nclassification); and (2) its ability to identify the specific confidential content within the note\n(excerpt extraction). For the note classification task, sensitivity, specificity, and positive\npredictive value were calculated, including 95% confidence intervals assuming a binomial\ndistribution. For excerpt extraction, the model’s proposed excerpts were compared to the original\nnote content to verify accuracy, as LLMs are liable to “hallucinations.” Hallucinations are model\noutputs that–while plausible–are incorrect or unrelated to the input. Excerpts were also reviewed\nby a physician to evaluate whether they correctly refer to the actual confidential concepts within\nthe note.\nFigure 1 illustrates the structure of the model prompt. A “few-shot learning” approach was used,\nwhere the model is given both definitions and examples of confidential content derived from real\nclinical notes. The prompt is available in the Supplement.\nResults\nOf the 300 sampled notes, 91 (30%) contained confidential content. The LLM was able to\nclassify whether an adolescent progress note contained confidential content with a sensitivity of\nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted August 28, 2023. ; https://doi.org/10.1101/2023.08.25.23294372doi: medRxiv preprint \n97% (88/91), specificity of 18% (37/209), and positive predictive value of 34% (88/260) (Table\n1). The LLM proposed a total of 768 confidential excerpts, 306 of which were from a note with\nconfidential content and were reviewed. Only 40 excerpts (13%) were accurately derived from\nthe original note (ie. 87% of excerpts contained a hallucination), and 22 (7%) of excerpts\nreflected the note’s actual confidential content.\nDiscussion\nA proprietary LLM achieved a high sensitivity in classifying whether adolescent notes contain\nconfidential content. However, its low specificity and poor positive predictive value limit its\nusefulness in hospital operations. Furthermore, the proportion of excerpts containing\nhallucinations is alarmingly high, such that the output of the model cannot be used for regulatory\npurposes. With this prompt structure, OpenAI’s GPT-3.5 is not able to reliably identify\nconfidential content in free-text adolescent progress notes.\nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted August 28, 2023. ; https://doi.org/10.1101/2023.08.25.23294372doi: medRxiv preprint \nFigure Legends\nFigure 1. Architecture of the GPT prompt. There are five portions of the prompt provided to the\nlarge language model. “Instructions” provide a general description of the problem; “Context”\nprovides the model with further details; “Task” explicitly defines the specific task; “Output”\nspecifies the desired formatting of the model’s response; “Sample” provides the clinical note to\nbe interpreted by the model. The right column shows text from the actual prompt provided to the\nsystem.\nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted August 28, 2023. ; https://doi.org/10.1101/2023.08.25.23294372doi: medRxiv preprint \nTables\nTable 1. Performance results from GPT-3.5’s ability to identify confidential content in adolescent\nvisit notes. Brackets report 95% confidence intervals.\nConfidential content note classification performance\nSensitivity 97% (88/91) [91% - 93%]\nSpecificity 18% (37/209) [13% - 24%]\nPositive Predictive Value 34% (88/260) [28% - 40%]\nConfidential content excerpt performance\nExcerpt is accurately derived from note (no\nhallucination)\n13% (40/306)\nExcerpt correctly identifies confidential\ncontent from the note\n7% (22/306)\nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted August 28, 2023. ; https://doi.org/10.1101/2023.08.25.23294372doi: medRxiv preprint \nReferences\n1. Pasternak RH, Alderman EM, English A. 21st Century Cures Act ONC Rule: Implications\nfor Adolescent Care and Confidentiality Protections. Pediatrics. 2023 Apr 1;151(Suppl 1).\n2. Parsons CR, Hron JD, Bourgeois FC. Preserving privacy for pediatric patients and families:\nuse of confidential note types in pediatric ambulatory care. J Am Med Inform Assoc. 2020\nNov 1;27(11):1705–10.\n3. Bedgood M, Rabbani N, Brown C, Goldstein R, Carlson JL, Steinberg E, et al. The\nPrevalence of Confidential Content in Adolescent Progress Notes Prior to the 21st Century\nCures Act Information Blocking Mandate. Appl Clin Inform. 2023 Mar;14(2):337–44.\n4. Rabbani N, Bedgood M, Brown C, Steinberg E, Goldstein RL, Carlson JL, et al. A Natural\nLanguage Processing Model to Identify Confidential Content in Adolescent Clinical Notes.\nAppl Clin Inform. 2023 May;14(3):400–7.\n5. Lee P, Goldberg C, Kohane I. The AI Revolution in Medicine: GPT-4 and Beyond. Pearson\nEducation, Limited; 2023.\n6. Sharko M, Jameson R, Ancker JS, Krams L, Webber EC, Rosenbloom ST. State-by-State\nVariability in Adolescent Privacy Laws. Pediatrics. 2022 Jun 1;149(6).\nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted August 28, 2023. ; https://doi.org/10.1101/2023.08.25.23294372doi: medRxiv preprint \nContributors Statement Page\nNaveed Rabbani: conceptualization, methodology, formal analysis, investigation, data curation,\nwriting - original draft, writing - review & editing, project administration\nConner Brown: methodology, software, data curation, formal analysis, writing - review & editing\nMichael Bedgood, Rachel Goldstein, and Jennifer Carlson: methodology, data curation, writing -\nreview & editing\nNatalie Pageler: methodology, writing - review & editing\nKeith Morse: conceptualization, methodology, investigation, data curation, writing - original\ndraft, writing - review & editing, supervision\nAll authors approve the final manuscript as submitted and agree to be accountable for all aspects\nof the work.\nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted August 28, 2023. ; https://doi.org/10.1101/2023.08.25.23294372doi: medRxiv preprint \nInstructions\nGeneral instructions and \nbackground of the task to be \ncompleted by model.\n“The following is a multi-label \nclassification task for confidential \ninformation found in progress notes at a \nlarge pediatric hospital…”\nContext\nAdditional information to help the \nmodel complete the task. Includes \ndefinitions and real-life examples \nof confidential content.\n“Confidential information topics: mental \nhealth, sexual and reproductive health, \ndrugs and alcohol.\nMental health: words or phrases…\n\"PHQ-9 Screen Score: 7\", \"mood changes, \nanxiety\"…”\nTask\nSpecific command to be completed \nby model.\n“Label the following sample as containing \nany of the following confidential \ninformation topics or not. Provide a list of \nany topics which are present.”\nOutput\nSpecifies formatting of the model’s \nanswer.\n“Label: <comma delimited list of \nconfidential information topics or NONE>\nExcerpt: <words or phrases from the sample \ncontaining confidential information topics>”\nSample\nProgress note content to be \nanalyzed.\n“This is a 17-year-old male who presents \nfor…”\nGPT Prompt Architecture\nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted August 28, 2023. ; https://doi.org/10.1101/2023.08.25.23294372doi: medRxiv preprint ",
  "topic": "Confidentiality",
  "concepts": [
    {
      "name": "Confidentiality",
      "score": 0.9271925687789917
    },
    {
      "name": "Patient confidentiality",
      "score": 0.6591172218322754
    },
    {
      "name": "Adolescent health",
      "score": 0.4886060357093811
    },
    {
      "name": "Mental health",
      "score": 0.44666677713394165
    },
    {
      "name": "Health care",
      "score": 0.4272494614124298
    },
    {
      "name": "Medicine",
      "score": 0.39271408319473267
    },
    {
      "name": "Internet privacy",
      "score": 0.37730515003204346
    },
    {
      "name": "Psychology",
      "score": 0.36966413259506226
    },
    {
      "name": "Computer science",
      "score": 0.3343771696090698
    },
    {
      "name": "Computer security",
      "score": 0.2416176199913025
    },
    {
      "name": "Psychiatry",
      "score": 0.21837395429611206
    },
    {
      "name": "Nursing",
      "score": 0.17823147773742676
    },
    {
      "name": "Political science",
      "score": 0.1034591794013977
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1334016132",
      "name": "Lucile Packard Children's Hospital",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1336910626",
      "name": "Children's Hospital of Los Angeles",
      "country": "US"
    }
  ],
  "cited_by": 2
}