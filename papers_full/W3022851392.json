{
    "title": "BERT-kNN: Adding a kNN Search Component to Pretrained Language Models for Better QA",
    "url": "https://openalex.org/W3022851392",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2027506410",
            "name": "Nora Kassner",
            "affiliations": [
                "Ludwig-Maximilians-Universität München"
            ]
        },
        {
            "id": "https://openalex.org/A2035156685",
            "name": "Hinrich Schütze",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3027879771",
        "https://openalex.org/W2950339735",
        "https://openalex.org/W2571859396",
        "https://openalex.org/W2963494889",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W2983915252",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2785611959",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W3039017601",
        "https://openalex.org/W3018732874",
        "https://openalex.org/W2912924812",
        "https://openalex.org/W2911430044",
        "https://openalex.org/W2963855739",
        "https://openalex.org/W2509019445",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2953356739",
        "https://openalex.org/W3007672467",
        "https://openalex.org/W2970986510",
        "https://openalex.org/W3033176962",
        "https://openalex.org/W2593864460",
        "https://openalex.org/W2995154514"
    ],
    "abstract": "Khandelwal et al. (2020) use a k-nearest-neighbor (kNN) component to improve language model performance. We show that this idea is beneficial for open-domain question answering (QA). To improve the recall of facts encountered during training, we combine BERT (Devlin et al., 2019) with a traditional information retrieval step (IR) and a kNN search over a large datastore of an embedded text collection. Our contributions are as follows: i) BERT-kNN outperforms BERT on cloze-style QA by large margins without any further training. ii) We show that BERT often identifies the correct response category (e.g., US city), but only kNN recovers the factually correct answer (e.g., \"Miami\"). iii) Compared to BERT, BERT-kNN excels for rare facts. iv) BERT-kNN can easily handle facts not covered by BERT's training set, e.g., recent events.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3424–3430\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n3424\nBERT-kNN: Adding a kNN Search Component to Pretrained Language\nModels for Better QA\nNora Kassner, Hinrich Sch¨utze\nCenter for Information and Language Processing (CIS)\nLMU Munich, Germany\nkassner@cis.lmu.de\nAbstract\nKhandelwal et al. (2020) use a k-nearest-\nneighbor (kNN) component to improve lan-\nguage model performance. We show that\nthis idea is beneﬁcial for open-domain ques-\ntion answering (QA). To improve the recall\nof facts encountered during training, we com-\nbine BERT (Devlin et al., 2019) with a tra-\nditional information retrieval step (IR) and a\nkNN search over a large datastore of an embed-\nded text collection. Our contributions are as\nfollows: i) BERT-kNN outperforms BERT on\ncloze-style QA by large margins without any\nfurther training. ii) We show that BERT often\nidentiﬁes the correct response category (e.g.,\nUS city), but only kNN recovers the factu-\nally correct answer (e.g., “Miami”). iii) Com-\npared to BERT, BERT-kNN excels for rare\nfacts. iv) BERT-kNN can easily handle facts\nnot covered by BERT’s training set, e.g., re-\ncent events.\n1 Introduction\nPretrained language models (PLMs) like BERT\n(Devlin et al., 2019), GPT-2 (Radford et al., 2019)\nand RoBERTa (Liu et al., 2019) have emerged as\nuniversal tools that not only capture a diverse range\nof linguistic, but also (as recent evidence seems to\nsuggest) factual knowledge.\nPetroni et al. (2019) introduced LAMA (LAn-\nguage Model Analysis) to test BERT’s perfor-\nmance on open-domain QA and therefore inves-\ntigate PLMs’ capacity to recall factual knowledge\nwithout the use of ﬁnetuning. Since the PLM train-\ning objective is to predict masked tokens, ques-\ntion answering tasks can be reformulated as cloze\nquestions; e.g., “Who wrote ‘Ulysses’?” is refor-\nmulated as “[MASK] wrote ‘Ulysses’.” In this\nsetup, Petroni et al. (2019) show that, on QA, PLMs\noutperform baselines trained on automatically ex-\ntracted knowledge bases (KBs).\nFigure 1: BERT-kNN interpolates BERT’s prediction\nfor question q with a kNN-search. The kNN search\nruns in BERT’s embedding space, comparing the em-\nbedding of q with the embeddings of a retrieved sub-\nset of a large text collection: Pairs of a word w in the\ntext collection and the BERT embedding of w’s con-\ntext ( BERT(s)) are stored in a key-value datastore.\nAn IR step is used to deﬁne a relevant subset of the\nfull datastore (yellow). BERT(q) (red) is BERT’s\nembedding of the question. The kNN search runs be-\ntween BERT(q) and BERT(s) and the correspond-\ning distance d and word w is returned (orange). Fi-\nnally, BERT’s predictions (blue) are interpolated with\nthis kNN search result.\nStill, given that PLMs have seen more text than\nhumans read in a lifetime, their performance on\nopen-domain QA seems poor. Also, many LAMA\nfacts that PLMs do get right are not “recalled” from\ntraining, but are guesses instead (Poerner et al.,\n2019). To address PLMs’ poor performance on\nfacts and choosing BERT as our PLM, we introduce\nBERT-kNN.\nBERT-kNN combines BERT’s predictions with\na kNN search. The kNN search runs in BERT’s\nembedding space, comparing the embedding of the\nquestion with the embeddings of a retrieved subset\nof a large text collection. The text collection can\nbe BERT’s training set or any other suitable text\ncorpus. Due to its kNN component and its resulting\nability to directly access facts stated in the searched\ntext, BERT-kNN outperforms BERT on cloze-style\n3425\nDataset BERT-base BERT-large ERNIE Know-BERT E-BERT BERT-kNN\nLAMA 27.7 30.6 30.4 31.7 36.2 39.4\nLAMA-UHN 20.6 23.0 24.7 24.6 31.1 34.8\nTable 1: Mean P@1 on LAMA and LAMA-UHN on the TREx and GoogleRE subsets for BERT-base, BERT-\nlarge, ERNIE (Zhang et al., 2019), KnowBert (Peters et al., 2019), E-BERT (Poerner et al., 2019) and BERT-kNN.\nBERT-kNN performs best.\nQA by large margins.\nA schematic depiction of the model is shown\nin Figure 1. Speciﬁcally, we use BERT to embed\neach token’s masked contextsin the text collection\n(BERT(s)). Each pair of context embedding and\ntoken is stored as a key-value pair in a datastore.\nTesting for a cloze question q, the embedding of q\n(BERT(q)) serves as query to ﬁnd the kcontext-\ntarget pairs in the subset of the datastore that are\nclosest. The ﬁnal prediction is an interpolation of\nthe kNN search and the PLM predictions.\nWe ﬁnd that the kNN search over the full dat-\nstore alone does not obtain good results. Therefore,\nwe ﬁrst query a separate information retrieval (IR)\nindex with the original question qand only search\nover the most relevant subset of the full datastore\nwhen ﬁnding the k-nearest-neighbors of BERT(q)\nin embedding space.\nWe ﬁnd that the PLM often correctly predicts the\nanswer category and therefore the correct answer is\noften among the top k-nearest-neighbors. A typical\nexample is “Albert Einstein was born in [MASK]”:\nthe PLM knows that a city is likely to follow and\nmaybe even that it is a German city, but it fails to\npick the correct city. On the other hand, the top-\nranked answer in the kNN search is “Ulm” and so\nthe correct ﬁller for the mask can be identiﬁed.\nBERT-kNN sets a new state-of-the-art on the\nLAMA cloze-style QA dataset without any fur-\nther training. Even though BERT-kNN is based on\nBERT-base, it also outperforms BERT-large. The\nperformance gap between BERT and BERT-kNN\nis most pronounced on hard-to-guess facts. Our\nmethod can also make recent events available to\nBERT without any need of retraining: we can sim-\nply add embedded text collections covering recent\nevents to BERT-kNN’s datastore.\nThe source code of our experiments is avail-\nable under: https://github.com/norakassner/\nBERT-kNN.\n2 Data\nThe LAMA dataset is a cloze-style QA dataset that\nallows to query PLMs for facts in a way analogous\nto KB queries. A cloze question is generated using\na subject-relation-object triple from a KB and a\ntemplatic statement for the relation that contains\nvariables X and Y for subject and object; e.g, “X\nwas born in Y”. The subject is substituted for X\nand [MASK] for Y . In all LAMA triples, Y is a\nsingle-token answer.\nLAMA covers different sources: The\nGoogleRE1 set covers the relations “place\nof birth”, “date of birth” and “place of death”.\nTREx (ElSahar et al., 2018) consists of a subset of\nWikidata triples covering 41 relations. ConceptNet\n(Li et al., 2016) combines 16 commonsense rela-\ntions among words and phrases. The underlying\nOpen Mind Common Sense corpus provides\nmatching statements to query the language model.\nSQuAD (Rajpurkar et al., 2016) is a standard\nquestion answering dataset. LAMA contains a\nsubset of 305 context-insensitive questions. Unlike\nKB queries, SQuAD uses manually reformulated\ncloze-style questions which are not based on a\ntemplate.\nWe use SQuAD and an additional 305 Concept-\nNet queries for hyperparamter search.\nPoerner et al. (2019) introduce LAMA-UHN, a\nsubset of LAMA’s TREx and GoogleRE questions\nfrom which easy-to-guess facts have been removed.\nTo test BERT-kNN’s performance on unseen\nfacts, we collect Wikidata triples containing TREx\nrelations from Wikipedia pages created January–\nMay 2020 and add them to the datastore.\n3 Method\nBERT-kNN combines BERT with a kNN search\ncomponent. Our method is generally applicable to\nPLMs. Here, we use BERT-base-uncased (Devlin\net al., 2019). BERT is pretrained on the BookCor-\npus (Zhu et al., 2015) and the English Wikipedia.\nDatastore. Our text collection C is the 2016-\n12-21 English Wikipedia.2 For each single-token\nword occurrence w in a sentence in C, we com-\n1https://code.google.com/archive/p/\nrelation-extraction-corpus/\n2dumps.wikimedia.org/enwiki\n3426\nDataset Statistics Model\nFacts Rel BERT kNN BERT\n-kNN\nGoogleRE 5527 3 9.8 51.1 48.6\nTREx 34039 42 29.1 34.4 38.7\nConceptNet 11153 16 15.6 4.7 11.6\nSQuAD 305 - 14.1 25.5 24.9\nunseen 34637 32 18.8 21.5 27.1\nTable 2: Mean P@1 for BERT-base, kNN and their\ninterpolation (BERT-kNN) for LAMA subsets and un-\nseen facts. BERT results differ from Petroni et al.\n(2019) where a smaller vocabulary is used.\nConﬁguration P@1\nhidden layer 12 36.8\nhidden layer 11 39.4\nhidden layer 10 34.7\nhidden layer 11 (without IR) 26.9\nTable 3: Mean P@1 on LAMA (TREx, GoogleRE sub-\nsets) for different context embedding strategies. Top:\nThe context embedding is represented by the embed-\nding of the masked token in different hidden layers.\nBest performance is obtained using BERT’s hidden\nlayer 11. Bottom: We show that BERT-kNN’s per-\nformance without the additional IR step drops signif-\nicantly. We therefore conclude that the IR step is an\nessential part of BERT-kNN.\npute the pair (c,w) where cis a context embedding\ncomputed by BERT. To be speciﬁc, we mask the\noccurrence of win the sentence and use the embed-\nding of the masked token. We store all pairs (c,w)\nin a key-value datastore Dwhere cserves as key\nand was value.\nInformation Retrieval.We ﬁnd that just using\nthe datastore Ddoes not give good results (see re-\nsult section). We therefore use Chen et al. (2017)’s\nIR system to ﬁrst select a small subset of D us-\ning a keyword search. The IR index contains all\nWikipedia articles. An article is represented as a\nbag of words and word bigrams. We ﬁnd the top\n3 relevant Wikipedia articles using TF-IDF search.\nFor KB queries, we use the subject to query the IR\nindex. If the subject has its dedicated Wikipedia\npage, we simply use this. For non-knowledge base\nqueries, we use the cloze-style questionq([MASK]\nis removed).\nInference. During testing, we ﬁrst run the IR\nsearch to identify the subset D′of D that corre-\nsponds to the relevant Wikipedia articles. For the\nkNN search, qis embedded in the same way as the\ncontext representations cin D: we set BERT(q)\nto the embedding computed by BERT for [MASK].\nWe then retrieve the k= 128nearest-neighbors of\nFigure 2: Mean P@1, P@5, P@10 on LAMA for orig-\ninal BERT and BERT-kNN.\nBERT(q) in D′. We convert the distances (Eu-\nclidean) between BERT(q) and the kNNs to a\nprobability distribution using softmax. Since a\nword wcan occur several times in kNN, we com-\npute its ﬁnal output probability as the sum over all\noccurrences.\nIn the ﬁnal step, we interpolate kNN’s (weight\n0.3) and BERT’s original predictions (weight 0.7).\nWe optimize hyperparameters on dev. See supple-\nmentary for details.\nEvaluation. Following Petroni et al. (2019) we\nreport mean precision at rank r (P@r). P@r is 1\nif the top rpredictions contain the correct answer,\notherwise it returns 0. To compute mean preci-\nsion, we ﬁrst average within each relation and then\nacross relations.\n4 Results and Discussion\nTable 1 shows that BERT-kNN outperforms BERT\non LAMA. It has about 10 precision point gain over\nBERT, base and large. Recall that BERT-kNN uses\nBERT-base. The performance gap between original\nBERT and BERT-kNN becomes even larger when\nevaluating on LAMA-UHN, a subset of LAMA\nwith hard-to-guess facts.\nIt also outperforms entity-enhanced versions of\nBERT (see related work) – ERNIE (Zhang et al.,\n2019), KnowBert (Peters et al., 2019) and E-BERT\n(Poerner et al., 2019) – on LAMA.\nTable 2 shows that BERT-kNN outperforms\nBERT on 3 out of 4 LAMA subsets. BERT pre-\nvails on ConceptNet; see discussion below. Huge\ngains are obtained on the GoogleRE dataset. Fig-\nure 2 shows precision at 1, 5 and 10. BERT-kNN\nperforms better than BERT in all three categories.\nTable 3 compares different context embedding\nstrategies. BERT’s masked token embedding of\n3427\nQuery and True Answer Generation\nGoogle\nRE\nhans gefors was born in [MASK]. BERT-kNN: stockholm (0.36), oslo (0.15), copenhagen (0.13)\nTrue: stockholm BERT: oslo (0.22), copenhagen (0.18), bergen (0.09)\nkNN: stockholm (1.0), lund (0.00), hans (0.00)\nTREx\nregiomontanus works in the ﬁeld of [MASK]. BERT-kNN: astronomy (0.20), mathematics (0.13), medicine (0.06)\nTrue: mathematics BERT: medicine (0.09), law (0.05), physics (0.03)\nkNN: astronomy (0.63), mathematics (0.36), astronomical (0.00)\nConcept\nNet\nears can [MASK] sound. BERT-kNN: hear (0.27), detect (0.23), produce (0.06)\nTrue: hear BERT: hear (0.28), detect (0.06), produce (0.04)\nkNN: detect (0.77), hear (0.14), produce (0.10)\nSquad\ntesla was in favour of the [MASK] current type. BERT-kNN: alternating (0.39), electric (0.18), direct (0.11)\nTrue: ac BERT: electric (0.28), alternating (0.18), direct (0.11)\nkNN: alternating (0.87), direct (0.12), ac (0.00)\nTable 4: Examples of generation for BERT-base, kNN, BERT-kNN. The last column reports the top three tokens\ngenerated together with the associated probability (in parentheses).\nhidden layer 11 performs best. We also show the\nnecessity of the IR step by running a kNN search\nover all Wikipedia contexts, which results in preci-\nsion lower than original BERT. To run an efﬁcient\nkNN search over all contexts instead of the relevant\nsubset identiﬁed by the IR step, we use the FAISS\nlibary (Johnson et al., 2017).\nTable 2 also shows that neither BERT nor kNN\nalone are sufﬁcient for top performance, while the\ninterpolation of the two yields optimal results. In\nmany cases, BERT and kNN are complementary.\nkNN is worse than BERT on ConceptNet, presum-\nably because commonsense knowledge like “birds\ncan ﬂy” is less well-represented in Wikipedia than\nentity triples and also because relevant articles are\nharder to ﬁnd by IR search. We keep the interpola-\ntion parameter constant over all datasets. Table 4\nshows that kNN often has high conﬁdence for cor-\nrect answers – in such cases it is likely to dominate\nless conﬁdent predictions by BERT. The converse\nis also true (not shown). Further optimization could\nbe obtained by tuning interpolation per dataset.\nBERT-kNN answers facts unseen during pre-\ntraining better than BERT, see Table 2. BERT was\nnot trained on 2020 events, so it must resort to\nguessing. Generally, we see that BERT’s knowl-\nedge is mainly based on guessing as it has seen\nWikipedia during training but is not able to recall\nthe knowledge recovered by kNN.\nTable 4 gives examples for BERT and BERT-\nkNN predictions. We see that BERT predicts the\nanswer category correctly, but it often needs help\nfrom kNN to recover the correct entity within that\ncategory.\n5 Related work\nPLMs are top performers for many tasks, includ-\ning QA (Kwiatkowski et al., 2019; Alberti et al.,\n2019; Bosselut et al., 2019). Petroni et al. (2019)\nintroduced the LAMA QA task to probe PLMs’\nknowledge of facts typically modeled by KBs.\nThe basic idea of BERT-kNN is similar to Khan-\ndelwal et al. (2020)’s interpolation of a PLM and\nkNN for language modeling. In contrast, we ad-\ndress QA. We introduce an IR step into the model\nthat is essential for good performance. Also, our\ncontext representations differ as we use embed-\ndings of the masked token.\nGrave et al. (2016) and Merity et al. (2017), in-\nter alia, also make use of memory to store hidden\nstates. They focus on recent history, making it\neasier to copy rare vocabulary items.\nDRQA (Chen et al., 2017) is an open-domain\nQA model that combines an IR step with a neural\nreading comprehension model. We use the same IR\nmodule, but our model differs signiﬁcantly. DRQA\ndoes not predict masked tokens, but extracts an-\nswers from text. It does not use PLMs nor a kNN\nmodule. Most importantly, BERT-kNN is fully un-\nsupervised and does not require any extra training.\nSome work on knowledge in PLMs focuses on\ninjecting knowledge into BERT’s encoder. ERNIE\n(Zhang et al., 2019) and KnowBert (Peters et al.,\n2019) are entity-enhanced versions of BERT. They\nintroduce additional encoder layers that are inte-\ngrated into BERT’s original encoder by expensive\nadditional pretraining. Poerner et al. (2019) injects\nfactual entity knowledge into BERT’s embeddings\nwithout pretraining by aligning Wikipedia2Vec en-\ntity vectors (Yamada et al., 2016) with BERT’s\nwordpiece vocabulary. This approach is also lim-\nited to labeled entities. Our approach on the other\nhand is not limited to labeled entities nor does it re-\nquire any pretraining. Our approach is conceptually\ndifferent from entity-enhanced versions of BERT\nand could potentially be combined with them for\n3428\neven better performance. Also, these models ad-\ndress language modeling, not QA.\nThe combination of PLMs with an IR step/kNN\nsearch has attracted a lot of recent research interest.\nThe following paragraph lists concurrent work:\nPetroni et al. (2020) also combine BERT with an\nIR step to improve cloze-style QA. They do not use\na kNN search nor an interpolation step but feed the\nretrieved contexts into BERT’s encoder. Guu et al.\n(2020) augment PLMs with a latent knowledge re-\ntriever. In contrast to our work they continue the\npretraining stage. They jointly optimize the masked\nlanguage modeling objective and backpropagate\nthrough the retrieval step. Lewis et al. (2020); Izac-\nard and Grave (2020) leverage retrieved contexts\nfor better QA using ﬁnetuned generative models.\nThey differ in that the latter fuse evidence of mul-\ntiple contexts in the decoder. Joshi et al. (2020)\nintegrate retrieved contexts into PMLs for better\nreading comprehension.\n6 Conclusion\nThis work introduced BERT-kNN, an interpolation\nof BERT predictions with a kNN search for unsu-\npervised cloze-style QA. BERT-kNN sets a state-\nof-the-art on LAMA without any further training.\nBERT-kNN can be easily enhanced with knowl-\nedge about new events that are not covered in the\ntraining text used for pretraining BERT.\nIn future work, we want to exploit the utility\nof the kNN component for explainability: kNN\npredictions are based on retrieved contexts, which\ncan be shown to users to justify an answer.\nAcknowledgements\nThis work has been funded by the German Federal\nMinistry of Education and Research (BMBF) under\nGrant No. 01IS18036A. The authors of this work\ntake full responsibility for its content.\nReferences\nChris Alberti, Kenton Lee, and Michael Collins. 2019.\nA BERT baseline for the natural questions. ArXiv,\nabs/1901.08634.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. COMET: Commonsense transformers for au-\ntomatic knowledge graph construction. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 4762–4779,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1870–\n1879, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nHady ElSahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon S. Hare, Fr ´ed´erique\nLaforest, and Elena Simperl. 2018. T-rex: A large\nscale alignment of natural language with knowledge\nbase triples. In Proceedings of the Eleventh Inter-\nnational Conference on Language Resources and\nEvaluation, LREC 2018, Miyazaki, Japan, May 7-12,\n2018.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2016. Improving neural language models with a con-\ntinuous cache. ICLR, abs/1612.04426.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\nsupat, and Ming-Wei Chang. 2020. REALM:\nRetrieval-augmented language model pre-training.\narXiv preprint arXiv:2002.08909.\nGautier Izacard and Edouard Grave. 2020. Lever-\naging passage retrieval with generative models for\nopen domain question answering. arXiv preprint\narXiv:2007.01282.\nJeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2017.\nBillion-scale similarity search with gpus. arXiv\npreprint arXiv:1702.08734.\nMandar Joshi, Kenton Lee, Yi Luan, and Kristina\nToutanova. 2020. Contextualized representations us-\ning textual encyclopedic knowledge. arXiv preprint\narXiv:2004.12006.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough Memorization: Nearest Neighbor Language\nModels. In International Conference on Learning\nRepresentations (ICLR).\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Al-\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\nNatural questions: A benchmark for question an-\nswering research. Transactions of the Association\nfor Computational Linguistics, 7:453–466.\n3429\nPatrick Lewis, Ethan Perez, Aleksandara Piktus,\nFabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich Kttler, Mike Lewis, Wen tau Yih,\nTim Rocktschel, Sebastian Riedel, and Douwe\nKiela. 2020. Retrieval-augmented generation for\nknowledge-intensive nlp tasks. arXiv preprint\narXiv:2005.11401.\nXiang Li, Aynaz Taheri, Lifu Tu, and Kevin Gimpel.\n2016. Commonsense knowledge base completion.\nIn Proceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1445–1455, Berlin, Germany.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In International Conference on Learning Repre-\nsentations (ICLR).\nMatthew E. Peters, Mark Neumann, Robert Logan, Roy\nSchwartz, Vidur Joshi, Sameer Singh, and Noah A.\nSmith. 2019. Knowledge enhanced contextual word\nrepresentations. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 43–54, Hong Kong, China. Associ-\nation for Computational Linguistics.\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim\nRockt¨aschel, Yuxiang Wu, Alexander H. Miller, and\nSebastian Riedel. 2020. How context affects lan-\nguage models’ factual predictions. In Automated\nKnowledge Base Construction.\nFabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nNina Poerner, Ulli Waltinger, and Hinrich Sch ¨utze.\n2019. Bert is not a knowledge base (yet): Fac-\ntual knowledge vs. name-based reasoning in unsu-\npervised qa. ArXiv, abs/1911.03681.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nIkuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and\nYoshiyasu Takefuji. 2016. Joint learning of the em-\nbedding of words and entities for named entity dis-\nambiguation. In Proceedings of The 20th SIGNLL\nConference on Computational Natural Language\nLearning, pages 250–259, Berlin, Germany. Associ-\nation for Computational Linguistics.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 1441–1451, Florence, Italy. Association\nfor Computational Linguistics.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In The IEEE International Con-\nference on Computer Vision (ICCV).\n3430\nA Data\nLAMA and LAMA-UHN can be downloaded from:\nhttps://dl.fbaipublicfiles.com/LAMA/\nFor TREx unseen, we downloaded the latest\nWikidata and Wikipedia dump from:\nhttps://dumps.wikimedia.org/\nwikidatawiki/entities/wikipedia_en/\nlatest-all.json.bz2\nand\nhttps://dumps.wikimedia.org/enwiki/\nlatest/enwiki-latest-pages-articles.xml.\nbz2.\nWe ﬁlter for TREx relations and only consider\nfacts which have a Wikipedia page created after\nJanuary 1st 2020. We only consider relations with\n5 questions or more. We add the additional embed-\nded Wikipedia articles to the datastore.\nB Inference\nThe probability of the kNN search for word wis\ngiven by:\npkNN (w|q) ∼∑\n(cw,w)∈kNN e−d(BERT (q),cw)/l.\nThe ﬁnal probability of BERT-kNN is the\ninterpolation of the predictions of BERT and the\nkNN search:\npBERT −kNN (q) = λpkNN (q)+(1 −λ)pBERT (q),\nwith\nqquestion,\nBERT(q) embedding q,\nwtarget word,\nsw context of w,\ncw = BERT(s) embedded context,\nddistance,\nldistance scaling,\nλinterpolation parameter.\nC Hyperparameters\nHyperparameter optimization is done with the\n305 SQuAD questions and additional randomly\nsampled 305 ConceptNet questions. We remove\nthe 305 ConceptNet questions from the test set.\nWe run the hyperparameter search once.\nWe run a grid search for the following hyperparam-\neters:\nNumber of documents N = [1, 2, 3, 4, 5],\nInterpolation λ= [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\nNumber of NN k= [64, 128, 512],\nDistance scaling l= [5, 6, 7, 8, 9, 10, 11, 12].\nThe optimal P@1 was found for:\nNumber of documents N = 3,\nInterpolation parameter λ= 0.3,\nNumber of NN k= 128,\nDistance scaling l= 6.\nD kNN without IR\nTo enable a kNN search over the full datastore\nwe use FAISS index (Johnson et al., 2017). We\ntrain the index using 1M randomly sampled keys\nand 40960 number of clusters. Embeddings are\nquantized to 64 bytes. During inference the index\nlooks up 64 clusters.\nE Computational Infrastructure\nThe creation of the datastore is computationally\nexpensive but only a single forward pass is needed.\nThe datastore creation is run on a server with 128\nGB memory, Intel(R) Xeon(R) CPU E5-2630 v4,\nCPU rate 2.2GHz, number of cores 40(20), 8x\nGeForce GTX 1080Ti. One GPU embedds 300\ncontexts/s. The datastore includes 900M contexts.\nEvaluation is run on a server with 128 GB\nmemory, Intel(R) Xeon(R) CPU E5-2630 v4, CPU\nrate 2.2GHz, number of cores 40(20). Evaluation\ntime for one query is 2 s but code can be optimized\nfor better performance."
}