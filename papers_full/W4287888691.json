{
  "title": "Efficient Hierarchical Domain Adaptation for Pretrained Language Models",
  "url": "https://openalex.org/W4287888691",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2004970051",
      "name": "Alexandra Chronopoulou",
      "affiliations": [
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A2108007937",
      "name": "Matthew Peters",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2105646908",
      "name": "Jesse Dodge",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2971277088",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3205949070",
    "https://openalex.org/W2111362445",
    "https://openalex.org/W3153805297",
    "https://openalex.org/W2970925270",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3190540921",
    "https://openalex.org/W1905522558",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W3176618728",
    "https://openalex.org/W3105421296",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W3100353583",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3213241618",
    "https://openalex.org/W3146885639",
    "https://openalex.org/W2142746600",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W2963211188",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W4385567093",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W3034640977",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2971292190",
    "https://openalex.org/W4297803344",
    "https://openalex.org/W1953829803",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2117278770",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964303773"
  ],
  "abstract": "Alexandra Chronopoulou, Matthew Peters, Jesse Dodge. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1336 - 1351\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nEfﬁcient Hierarchical Domain Adaptation\nfor Pretrained Language Models\nAlexandra Chronopoulou1∗, Matthew E. Peters2, Jesse Dodge2\n1Center for Information and Language Processing, LMU Munich, Germany\n2Allen Institute for Artiﬁcial Intelligence, Seattle, W A\nachron@cis.lmu.de\n{matthewp,jessed}@allenai.org\nAbstract\nThe remarkable success of large language\nmodels has been driven by dense models\ntrained on massive unlabeled, unstructured cor-\npora. These corpora typically contain text\nfrom diverse, heterogeneous sources, but infor-\nmation about the source of the text is rarely\nused during training. Transferring their knowl-\nedge to a target domain is typically done by\ncontinuing training in-domain. In this paper,\nwe introduce a method to permit domain adap-\ntation to many diverse domains using a com-\nputationally efﬁcient adapter approach. Our\nmethod is based on the observation that tex-\ntual domains are partially overlapping, and\nwe represent domains as a hierarchical tree\nstructure where each node in the tree is asso-\nciated with a set of adapter weights. When\ncombined with a frozen pretrained language\nmodel, this approach enables parameter shar-\ning among related domains, while avoiding\nnegative interference between unrelated ones.\nExperimental results with GPT-2 and a large\nfraction of the 100 most represented websites\nin C4 show across-the-board improvements in-\ndomain. We additionally provide an inference\ntime algorithm for a held-out domain and show\nthat averaging over multiple paths through the\ntree enables further gains in generalization,\nwhile adding only a marginal cost to inference.\n1 Introduction\nPretrained language models (PLMs) (Peters et al.,\n2018; Devlin et al., 2019; Liu et al., 2019; Radford\net al., 2019), trained on massive general-domain\ncorpora, have enabled great progress in many natu-\nral language processing (NLP) benchmarks (Wang\net al., 2018). Nonetheless, continuing pretraining\n(as a dense model) a PLM on a narrower domain\n(Han and Eisenstein, 2019; Lee et al., 2019) is ben-\neﬁcial, although computationally expensive (Ma-\nronikolakis and Schütze, 2021), which indicates\n∗ Work done while an intern at AllenAI.\n6\n3 4\nscientiﬁc\narticles\nreviews\nfrontiersin.org journals.plos.org\n7\nbooking.com\n5\n1 2\nyelp.com\nFigure 1: We model domains as a hierarchical tree\nstructure that associates adapters with nodes, allowing\nparameter sharing among related domains. Internet do-\nmains appear as leaf nodes. During training, we acti-\nvate the adapters along the path to a leaf to specialize a\nlanguage model to the domain corresponding to it.\nthat domain-relevant data is important for down-\nstream tasks. Sparse models that use mixtures of\nexperts (Lepikhin et al., 2021) have recently been\nproposed to allow efﬁcient training.\nPrior work typically assumes that individual do-\nmains are distinct, and models them accordingly.\nFor example, Gururangan et al. (2020, 2021) train\none model for each textual domain, either in a\ndense or sparse manner. This is related to data\nselection (Moore and Lewis, 2010; Axelrod et al.,\n2011; Plank and van Noord, 2011), which aims to\nselect the best matching data for a new domain.\nThis process does not scale to multiple domains\nefﬁciently, as the parameters grow linearly with the\ndomains. It also does not allow sharing represen-\ntations among related domains during training, as\neach domain is modeled with a separate set of pa-\nrameters. At the other extreme, training one model\non all domains as is common during unsupervised\npretraining does not account for their similarities\nand differences and might hinder the model’s gen-\neralization ability due to negative interference.\nAs an alternative, we start with the observation\nthat the term “domain” typically denotes a distribu-\ntion over language characterizing a given topic or\ngenre, and that domains are partially overlapping.\n1336\nFor example, a sentiment model processing hotel\nreviews could be expected to beneﬁt by also in-\ncluding data from restaurant reviews, which might\nin turn beneﬁt from cooking recipes, but combing\nhotel reviews and recipes may be detrimental.\nWe want to model the relations between domains\nand selectively share information, so that we allow\npositive transfer and avoid negative interference.\nTo this end, we propose a data-driven approach to\nmodeling domains that automatically clusters them\nin a tree using PLM representations. We then in-\ntroduce an efﬁcient method that specializes a PLM\nin a number of domains leveraging their hierar-\nchical structure. Our approach allows parameter\nsharing among related domains using adapters (Re-\nbufﬁ et al., 2017; Houlsby et al., 2019), which are\nlightweight layers, added after each transformer\n(Vaswani et al., 2017) layer. Each node in the tree\nis associated with a separate set of adapters, that are\nonly activated for a particular domain. For instance,\ndata from BOOKING .COM activates parameters in\nnodes 3, 6, and 7, allowing parameter sharing with\nthe highly related YELP.COM through nodes 6 and\n7 (Figure 1).\nWe verify the efﬁcacy of our approach in two set-\ntings. First, we manually deﬁne a tree structure, us-\ning websites as the leaves. In this ﬁrst few-domain\nsetting, our method outperforms prior work includ-\ning single and multi-domain adapters added to GPT-\n2 (Radford et al., 2019) when tested in-domain. We\nfurther show that our method generalizes better to\nheld-out websites than the baselines.\nWe then scale our model to a many-domain set-\nting across almost 100 websites. We induce the\nhierarchical structure in an unsupervised way using\nrepresentations from GPT-2 with a Gaussian Mix-\nture Model (GMM) (Aharoni and Goldberg, 2020)\nand hierarchical clustering, similar to Das Gupta\net al. (2015). In this way, the clusters model tex-\ntual domains and the GMM provides a mechanism\nto automatically ﬁnd the closest training websites\nto any held-out website. Empirical results show\nacross-the-board improvements over strong base-\nlines when evaluated in-domain. We also show\nthat an efﬁcient inference-time algorithm that aver-\nages over multiple paths through the tree improves\ngeneralization when tested on held-out websites.1\n1Our code is publicly available at github.com/alexandra-\nchron/hierarchical-domain-adaptation.\n2 Hierarchical Representation of\nDomains\nIn this section, we provide a formal problem deﬁ-\nnition and the intuition for a hierarchical ordering\nof domains. We then describe how we add a hierar-\nchical structure to a PLM and present the training\nprocess. Additionally, we show how a path in the\ntree is selected to evaluate the in-domain and out-of-\ndomain sets. We ﬁnally discuss the computational\ncost of our approach compared to the baselines and\nour experimental setup.\n2.1 Problem Deﬁnition\nWe formulate the task as follows: given a PLM, we\naim at ﬁne-tuning it in the task of language model-\ning using adapter modules, on a corpus consisting\nof k corpora for domain adaptation. The model\nis trained to minimize the cross-entropy loss on\nsentences from all kcorpora, then is evaluated on\nboth in-domain and out-of-domain test sets.\n2.2 Textual domains and provenance\nAs there is no commonly-accepted deﬁnition of a\n“domain” in text (Plank, 2016), we take a practi-\ncal approach and use the provenance of a piece of\ntext (that is, the website from which the text was\nscraped) as a proxy for textual domain. To model\nhow similar different textual domains are to each\nother, we ﬁt a Gaussian Mixture Model (GMM)\nusing PLM representations (Aharoni and Goldberg,\n2020) using a small sample of text from each do-\nmain. After ﬁtting this GMM, for some clusters\nthere is a one-to-one correspondence between one\ncluster and text from one website, while for other\nclusters there is a one-to-many correspondence be-\ntween one cluster and text from multiple, similar\nwebsites (see §4).\n2.3 Hierarchical Structure\nDomains generally overlap with each other and\nhave different degrees of granularity. A model that\nencodes them should both capture domain-speciﬁc\nand general-domain information. To this end, we\npropose representing the data as a tree. An example\nof a tree structure is shown in Figure 1. Text from\nspeciﬁc websites is encoded in the leaf nodes (such\nas FRONTIERSIN .ORG , JOURNALS .PLOS .ORG ),\nwhile more general-domain knowledge is encoded\nin the upper nodes (SCIENTIFIC ARTICLES ).\n1337\n2.4 Model Architecture\nAssuming a corpus with data from ndomains, we\nconsider the setting where we have a pretrained\nmodel M. We want to use M to adapt to nnew\ndomains. To this end, we can leverage adapters.\nAdapter layer. Adapters are typically added to\nmodel M in each transformer layer and are trained\nto a task, while M remains unchanged. An adapter\nuses as input the output of the previous layer. It is\nformally deﬁned as WU ReLU(WD LN(hi)) +hi\n,where hi is the output of the i-th layer, of dimen-\nsion m, LN is a layer-normalization (Ba et al.,\n2016), WD is a down-projection in Rm×d, and\nWU is an up-projection in Rd×m, and dis the bot-\ntleneck dimension of the adapter module.\nSingle Adapters. To adapt to ndomains, one so-\nlution is to train nadapters (per transformer layer),\none for each domain. The number of parameters\nadded from single adapters grows linearly with the\nnumber of domains (O(n)).\nMulti-Domain Adapters. Another solution is to\nadd just one set of adapters to model M. The\nadapter weights will be updated based on data from\nall n domains. This is a dense model that does\nnot permit modular training. For ndomains, the\nnumber of parameters added is constant.\nHierarchical Adapters. We propose associating\neach of the nodes in a tree that represents domains\nwith a set of adapters and adding them to M. This\nsparse model adds parameters that scale logarith-\nmically (O(log(n)) with the number of domains\nbecause of the binary tree structure (Figure 1).\nWhile Houlsby et al. (2019) insert adapters but\nre-train layer normalization parameters of M and\nBapna and Firat (2019) introduce new layer normal-\nization parameters for every adapter, we introduce\njust one set of layer normalization parameters in\neach transformer layer and these parameters are\nshared between all adapters of a transformer layer.\n2.5 Training & Computational Cost\nWhen our input consists of data from a particular\ndomain, we only update the adapter layers of the\npath that leads to this domain (Figure 1).\nSupposing we have a mini-batch from FRON -\nTIERSIN .ORG , the hidden state hi of the i-th layer\nis the input of adapter1\ni (the adapter of node 1\nfor transformer layer i). hi is also the input of\nadapter5\ni (parent) and adapter7\ni (root). Their out-\nputs y1\ni ,y5\ni ,y7\ni are averaged. The ﬁnal representa-\ntion yi is the input to the next transformer layer.\nUsing this simple training process, we allow\nsharing between related domains. Upper nodes\nin the tree are updated more often than leaves, thus\nthey are better trained and encode more domain-\ngeneral knowledge. More precisely, the root node\nof the hierarchical model in Figure 1 is updated for\neach sequence, but the leaf nodes are only updated\nusing sequences from the associated domain.\nIn terms of computation, although our model\nadds a large number of trainable parameters ( to-\ntal parameters), only a small fraction of them is\nused for each forward pass (active parameters), as\nshown in Table 1. At inference time, to evaluate\nperformance on a domain using the tree of Figure 1,\nour approach with a single path uses 126M parame-\nters (GPT-2 has 112M and the adapters of each path\naccount for 14M parameters). When we average\ntwo paths, 23M parameters are added to GPT-2.\nKaplan et al. (2020) provided a detailed break-\ndown of compute cost for transformer LMs. For\na model with N non-embedding parameters, the\napproximate cost of a forward pass is 2N ﬂops per\ntoken. Extending their calculations to our setting,\nfor a model with Llayers, model dimension dmodel,\nadapter bottleneck size d, a single adapter adds\n4Ldmodeldﬂops per forward pass over the cost of\nrunning GPT-2. Our hierarchical method requires\nrunning T adapters per layer per forward pass,\nwhere T is the average tree depth. For the many-\ndomain setting in §4 with L = 12, dmodel = 768,\nd= 64, N = 84M, T = 8, this gives an increase\nof ~11% ﬂops over GPT-2. At inference time, using\ntwo paths (§4.5) the increase is 22% over GPT-2.\nFor fair comparison between our method and\nthe baselines, we scale the adapter size so that our\nproposed model and the multi-domain adapters (the\nmost related baseline) use the same number of ﬂops.\nFollowing the previous paragraph, the adapter sizes\nin our hierarchical model are smaller by a factor of\n1/T then those in the baseline models (Table 1).\n2.6 In-domain/Out-of-domain Evaluation\nAt inference time, we need to deﬁne which path\nshould be activated for each domain. When we per-\nform in-domain evaluation, this is straightforward.\nWe always activate the path that leads to the node\nthat is assigned to this speciﬁc domain.\nFor out-of-domain evaluation, we need to ﬁnd\nthe path that better ﬁts the held-out domain. We\ncan also use multiple paths, as the computational\ncost is small. We describe in detail how we run\n1338\nFew-Domain Many-Domain\nSetup Setup\nHierarchical (ours)\nAdapter Size 256 64\n# Adapters 7 49\nAverage path length 3 8\nTotal parameters 33M 58M\nActive parameters 14M 9.5M\nNumber of updates - root 22K 11K\nNumber of updates - leaf 5.5K 400\nMulti-domain\nAdapter Size 768 512\n#Adapters 1 1\nAverage path length 1 1\nTotal parameters 14M 9.5M\nActive parameters 14M 9.5M\nNumber of updates 22K 11K\nTable 1: Parameters used by our approach and the\nmulti-domain adapters. The few-domain and many-\ndomain setup are explained in §3 and §4 respectively.\nout-of-domain evaluation in the following two sec-\ntions, which present a manually deﬁned (§3) and an\nautomatically created hierarchical structure (§4).\n2.7 Experimental Setup\nWe use GPT-2 (12 transformer layers; hidden size\n768) as the pretrained model. GPT-2 has a vocabu-\nlary of 50,264 BPE (Sennrich et al., 2016) tokens\nand 112M parameters. Our code is built with Py-\nTorch (Paszke et al., 2017), using the HuggingFace\nlibrary (Wolf et al., 2020). We run all experiments\non NVIDIA A100 GPUs with 40GB of RAM. We\nsplit our corpora in 800-token sequences. Models\nare trained with the Adam optimizer (Kingma and\nBa, 2015) with an initial learning rate of 1e−3 and\nwe accumulate gradients over 2 updates.\n3 Hierarchical Domain Adaptation with\na Manually Created Tree\nIn this section, we implement the model described\nin the prior section for a very limited number of\ndomains (few-domain setup), to comprehensively\nexamine design choices and verify the performance,\nbefore moving to a large-scale setting in §4.\n3.1 Data\nWe select four websites to be represented by leaf\nnodes in our tree: two that contain scientiﬁc arti-\ncles (FRONTIERSIN .ORG , JOURNALS .PLOS .ORG )\nand two that contain reviews (BOOKING .COM and\nYELP.COM ). We use text from the released version\n(Dodge et al., 2021) of C4 (Raffel et al., 2020), a\nweb-scale corpus of English data; the ﬁrst three\ninternet domains are some of the largest sources of\ntext in C4. We also useYELP.COM , a publicly avail-\nable dataset. Dataset sizes (training/evaluation) are\nshown in Appendix A.1.\n3.2 Approach\nWe use the hierarchical structure shown in Figure\n1, with two leaf nodes representing scientiﬁc arti-\ncles sharing a parent, two leaf nodes representing\nreviews sharing a parent, and a single grandparent\nshared by the two parents. This tree structure was\nmanually chosen using domain knowledge. We use\na pretrained GPT-2 model as our base model, and\nadd one set of adapters per node in the tree (one\nadapter per transformer layer for each node). We\nfreeze the weights of GPT-2 and train the adapters\non language modeling of the domains of interest.\nThe training process is explained in detail in §2.5.\n3.3 Experimental Setup\nOur hierarchical model adds 7 sets of adapters to\nGPT-2, one for each node in the tree. Each adapter\nhas a bottleneck dimension d of 256. For each\ntraining step, one path through the tree is active (so,\n3 adapters) depending on which domain of text is\nrepresented in the current batch (see §2.5). Active\nnodes are used in the forward pass and updated in\nthe backward pass (during training), while those\nthat are not active are not used in the computation.\nWe evaluate two baselines: a multi-domain\nadapter, trained on all in-domain data, and sin-\ngle adapters, each trained on data from a different\nwebsite. We ensure that the hierarchical model uses\nthe same amount of compute for a forward pass as\nthe multi-domain adapter baseline (using d= 768\nand 1 adapter/path). We also train each model to an\nequal amount of data from each domain. Results\nare shown after 20 epochs of training (22K steps).\nGPT-2 single multi hierarchical\nadapters adapters adapters\nfrontiersin 22.2 16.1 15.8 15.5\njournals 24.5 16.6 16.3 15.8\nbooking 29.7 9.7 9.9 9.2\nyelp 36.2 24.3 25.3 23.8\naverage 27.7 15.8 15.9 15.2\nTable 2: In-domain evaluation perplexity for the few-\ndomain setting (§3). Hierarchical adapters consistently\nprovide better scores compared to the baselines.\n3.4 In-Domain Results\nIn-domain evaluation scores are presented in Table\n2. Our model clearly surpasses the multi-domain\n1339\nGPT-2 single multi hierarchical\nadapters adapters adapters\nncbi 20.5 18.2 17.6 17.3\nlink.springer 27.7 24.5 22.7 22.6\nscholars.duke 22.7 20.1 20.3 19.9\ntechcrunch 27.7 27.1 26.3 27.1\nmedium 29.1 30.0 27.9 28.5\ntripadvisor 41.3 36.6 34.1 26.0\nlonelyplanet 35.5 27.1 24.3 25.3\naverage 29.2 26.2 24.8 23.8\nTable 3: Out-of-domain evaluation perplexity for the\nsmall setting (§3). For the hierarchical model, 2 paths\nthrough the tree are used for the evaluation. The hierar-\nchical model on average outperforms the baselines.\nadapter baseline in all domains. On average, hier-\narchical adapters lower the perplexity by 0.7 com-\npared to multi-domain adapters. Compared to just\nevaluating GPT-2, our model yields a large im-\nprovement, conﬁrming prior work that suggests\nthat further training a PLM in-domain is highly ef-\nfective. Single adapters perform roughly equivalent\nto multi-domain adapters in this scenario.\n3.5 Out-of-Domain Results\nWe perform evaluation on 7 unseen domains, some\nof which represent similar textual domains to our\nin-domain data, while others are quite different.\nFor example, NCBI , LINK .SPRINGER , and SCHOL -\nARS .DUKE contain text from scientiﬁc documents,\nsimilar to two of our in-domain sources of text,\nbut TECHCRUNCH and MEDIUM are quite dissim-\nilar to the in-domain text. All models outperform\nthe baseline of just evaluating GPT-2, as shown in\nTable 3. We hypothesize that the pretraining data\nfrom GPT-2, which has not been publicly released,\nhad a somewhat different distribution to C4, and\nthus further training on any data from C4 seems\nto improve performance. The best out-of-domain\nresults are obtained with hierarchical adapters.\nHowever, which set of single adapters we should\nuse to evaluate a held-out domain is not obvious.\nFor example, to evaluate on LONELYPLANET , it\nintuitively makes sense to use adapters trained on a\nreviews/travelling domain (BOOKING or YELP ), but\nfor LINK .SPRINGER , the model trained on a scien-\ntiﬁc articles (FRONTIERSIN or JOURNALS ) might\nbe more suitable. We have no a priori criterion to\nchoose the most appropriate model. This is also\ntrue for our proposed model. We show the best\nevaluation scores using single adapters in Table 3\n(full evaluation in Appendix A.2).\nFor the hierarchical adapter model, we show\n1 path 2 pathsjournals frontiers booking yelpscience reviews\nncbi 17.6 18.7 34.8 26.0 17.3 26.3link.springer 23.3 23.3 37.0 33.1 22.6 31.8scholars.duke20.7 20.7 35.5 29.4 19.9 28.8techcrunch 27.7 27.9 34.8 32.8 27.1 29.4medium 29.4 29.4 35.9 36.2 28.5 30.6tripadvisor 47.9 47.9 37.038.1 45.6 26.0lonelyplanet 39.6 40.0 25.538.9 38.5 25.3\naverage 29.5 29.7 34.4 33.5 28.5 28.3\nTable 4: Out-of-domain evaluation of the hierarchical\nmodel using different paths. The left part of the table\nshows scores using a single path. The right part shows\nresults using the average of two paths, corresponding\nto either the scientiﬁc articles or the reviews domain.\nevaluation scores using various paths in Table 4.\nAs expected, using a single path, the hierarchical\nmodel performs best leveraging the path of a web-\nsite that is most similar to the unseen website. For\nexample, the best evaluation score for NCBI is ob-\ntained with the path that leads to JOURNALS , while\nthe best score for TRIPADVISOR using the path that\nleads to BOOKING . Using two paths (either the\npaths of FRONTIERS and JOURNALS , or BOOKING\nand YELP ), results generally improve. For science\nor technology websites, using the paths of the sci-\nence domain considerably boosts the hierarchical\nmodel’s performance. For reviews/travelling web-\nsites, using both paths of the reviews domain is\nbeneﬁcial. This conﬁrms our intuition that the hi-\nerarchical structure proposed adequately models\ndomains, preventing negative transfer.\nComparing our hierarchical adapters to multi-\ndomain adapters, using a single path, hierarchical\nadapters perform worse than multi-domain adapters\n(average scores of columns 1-4 in Table 4 are\nworse than the average score of column 3 in Ta-\nble 3). However, with a second path active, hier-\narchical adapters outperform all other approaches\n(Table 3). This highlights an advantage: they are\nextensible even after training, allowing for ﬂex-\nible performance-efﬁciency trade-offs that dense\napproaches (like multi-domain adapters) do not.\n4 Hierarchical Domain Adaptation with\nan Automatically Created Tree\nIn this section, we scale our approach to a many-\ndomain setup, using a larger set of domains, and\nthus a much larger hierarchy, adding more adapters\nin our model. In the previous section, we manually\nselected a tree based on our domain knowledge, but\nin this section we automatically create a tree using\n1340\n5714181129116231719822401013241520216321\n2625\n28\n33\n29\n39\n36\n42\n27\n43\n3231\n37\n30\n35\n40\n44\n38\n45\n34\n41\n46\n47\n48\ncity-data.com\nbaltimoresun.com\nchicagotribune.com\nmedium.comexpress.co.ukdeviantart.comanswers.sap.comgsmarena.com\nnpr.org\nglassdoor.com\ninstructables.com\nfoxnews.comcsmonitor.com\npcworld.com\nlonelyplanet.comlibrarything.comfrontiersin.org\nandroidheadlines.com\nign.comsi.com\nprweb.com\ndailymail.co.uk\nforums.macrumors.com\nentrepreneur.com\nwired.com\naljazeera.com\ninsiderpages.com\neventbrite.com\noreilly.com\nlink.springer.com\nFigure 2: Dendrogram obtained from agglomerative clustering based on the average KL divergences of the GMMs.\nThis diagram illustrates the hierarchical structure of 30 of the most high-resource websites on C4. The leaf nodes\ncorrespond to the cluster centers and are mapped to the websites they assign the highest probability to.\nunsupervised methods. We leverage domain clus-\nters obtained using Gaussian Mixture Models and\nhierarchical clustering and provide an algorithm for\nout-of-domain evaluation, leveraging the ﬂexibility\nof hierarchical adapters, that can be combined to\nimprove performance with a minimal cost.\n4.1 Data\nAs a training and evaluation corpus, we use data\nfrom C4. Speciﬁcally, we use text from 30 web-\nsites as our training corpus and we perform out-of-\ndomain evaluation of our model and the baselines\non 38 other websites. All websites used belong to\nthe top 100 sites in C4 (details in Appendix A.1).\n4.2 Approach\nWe want to create a hierarchical structure that repre-\nsents relations between domains. To this end, we ﬁt\na Gaussian Mixture Model (GMM) and then use an\nagglomerative clustering algorithm on the GMM.\nA GMM assumes that all data points are generated\nfrom a mixture of a kGaussian distributions and\ndeﬁnes the probability for data points to belong to\nany of these distributions. We consider a GMM\nto be suitable choice because it accounts for the\nuncertainty of cluster assignment and provides soft\nassignments that we use at inference.\nSimilar to Aharoni and Goldberg (2020), we gen-\nerate contextual representations of 1K sequences\n(uniformly sampled) from each of our 30 training\nwebsites using GPT-2. We use PCA for dimension-\nality reduction. We then ﬁt a GMM with 30 compo-\nnents to our data (i.e., 30 Gaussians/clusters). After\nthat, we ﬁnd the Gaussian which assigns highest\nprobability to text from each website, and remove\nany Gaussian which does not assign the highest\nprobability to any website (it can be the case that\ntext from more than one websites could be drawn\nby the same Gaussian). The websites and their\ncorresponding clusters are shown in Figure 2.\nFor hierarchical clustering, we use the sym-\nmetrized Kullback-Leibler (KL) divergence as a\ndistance metric. Suppose we have two multivariate\nnormal distributions (means µ0,µ1, covariance ma-\ntrices Σ0,Σ1) obtained by the GMM. To measure\nthe difference between the two distributions, if they\nhave the same dimension N, we compute the KL\ndivergence. Because it is asymmetric, we cannot\nuse it to measure the distance between distributions,\nso we compute the symmetrized version as follows:\nDKL(N0∥N1) = 1\n2tr\n(\nΣ−1\n1 Σ0) + ln\n(det Σ1\ndet Σ0\n))\n+ 1\n2\n((µ1 −µ0)TΣ−1\n1 (µ1 −µ0) −N) (1)\nDKLsym(N0,N1) =\n1\n2 (DKL(N0∥N1) +DKL(N1∥N0)) (2)\nUsing Equation 2 as a distance metric, we use\nagglomerative clustering to infer the structure of\nour data. We start from 25 clusters, computed by\n1341\n0.0\n5.0\n10.0\n15.0\n20.0\n25.0\n30.0\n35.0\n40.0\n45.0\n50.0\n55.0\n60.0\nign.com\ninsiderpages.com \neventbrite.com\nandroidheadlines.com\nlink.springer.comLibrarything.comcsmonitor.comcity-data.com\nforums.macrumors.com\nglassdoor.com\noreilly.compcworld.comexpress.co.uk\nanswers.sap.com\nprweb.com\ninstructables.com\ndeviantart.com\nentrepreneur.com\nsi.com\ngsmarena.com\nwired.com\nmedium.com\nbaltimoresun.com\nnpr.org\nfrontiersin.org\nchicagotribune.com\nfoxnews.comaljazeera.comdailymail.co.uklonelyplanet.com\naverage\ngpt2 multi-domain hierarchical\nFigure 3: In-domain evaluation perplexity. Hierarchical adapters consistently outperform the multi-domain adapter\non all websites used during training.\nthe GMM (5 are ignored because do not assign a\nhigh probability to data samples from any website,\nsee Appendix A.3 for the confusion matrix). The\nclustering algorithm leads to a tree (see Figure 2).\nNodes 0-24 correspond to the clusters of the GMM.\nEach website is assigned to a speciﬁc cluster.\n4.3 Experimental Setup\nFor PCA, we use 100 dimensions. For the hierar-\nchical clustering, we use distances computed using\nthe symmetrized KL divergence. We get a tree of\n49 nodes, shown in Figure 2. We add 49 adapters\nto GPT-2, one for each node. For a single training\nstep, just one path in the tree is active (as in §3).\nIn this set of experiments, we used our compu-\ntational budget to compare against our strongest\nbaseline, multi-domain adapters, as that provided\nthe most competitive results in §3. Comparing\nagainst single adapters could be relevant but we\nfocus on our strongest baseline, as single adapters\nhave shown to be less able to generalize to held-\nout domains. We train both our hierarchical model\nand the multi-domain adapter baseline for 4 epochs\n(11K steps), using 1 GPU per model and stopping\nafter 51 hours. We oversample the low-resource\ndomains to avoid overﬁtting. We use d= 64 for\nhierarchical adapters, as the average path length is\n8 and d= 512 for the multi-domain adapter, since\nit adds just 1 adapter/transformer layer (Table 1).\n4.4 In-Domain Results\nOur in-domain results are shown in Figure 3. To\nevaluate our model in-domain, we use the path\nthat leads to the cluster that assigns the highest\nprobability to the domain of interest (the same as\nduring training). For example, to evaluate the per-\nformance of the model on PCWORLD , we use the\npath that leads to cluster 4. The average path length\nin the tree is 8, so we “activate” 8 adapters on av-\nerage at every training step and also for in-domain\nevaluation. Our approach consistently outperforms\nmulti-domain adapters, yielding +1.3 on average in\nterms of perplexity (see Appendix A.4 for details).\n4.5 Out-of-Domain Results\nWe perform out-of-domain evaluation on 38 held-\nout websites (dataset sizes in Appendix A.1). We\nwant to automatically ﬁnd the best path in the tree\nfor a held-out website. To this end, we use the\nﬁtted GMM to assign probabilities to data from the\nheld-out websites. We intuitively want to place a\nheld-out website close to similar training websites,\nso that it can beneﬁt from positive transfer.\nTo do that, for a given out-of-domain websitei,\nwe assume we have a set ofNsequences (in our ex-\nperiments N = 1,000) that we can use to ﬁnd the\nbest path; this path is used to evaluate the rest of the\ndata from this website (e.g., for computing perplex-\nity). Following a similar procedure to our training\nregime, we use GPT-2 to encodeNsequences, then\nuse the ﬁtted GMM to ﬁnd the probability assigned\nto each of the N vectors by each cluster (i.e., each\nleaf node). The single best path leads to the leaf\nnode that corresponds to cluster m, where mas-\nsigns the highest probability to the largest fraction\nof the N sequences from website i. The second\nbest path through the tree leads to cluster nthat\nassigns highest probability to the second-most num-\nber of the N sequences from website i. Thus, using\nthe GMM clusters and the hierarchical structure,\n1342\nwithout training more parameters, we are able to\nevaluate out-of-domain data using the adapters that\nwere trained on the most related domains. This is\nsimilar to the “cached” setting in Gururangan et al.\n(2021), and it does require a held-out set of N se-\nquences that are only used for ﬁnding the best path\nthrough the tree (and not for computing perplexity).\nThis is realistic setting when one has a signiﬁcant\namount of data from a single source, and we leave\nother approaches (e.g., ﬁnding the best path for\nevery input sequence individually) to future work.\nWe show in Table 5 results of the out-of-domain\nevaluations. Our hierarchical adapter model outper-\nforms the baseline of just evaluating GPT-2. We no-\ntice that using a single path, our approach provides\nworse results compared to multi-domain adapters.\nIn this evaluation, the multi-domain adapters and\nthe hierarchical model have the same number of\nactive parameters, but the adapters in the hier-\narchical model are trained on less data (except\nthe adapter associated with the root, which has\nthe same number of updates as the multi-domain\nadapter but is signiﬁcantly smaller). However, by\nhaving two paths through the tree active, the hi-\nerarchical adapter model leverages its modularity\nand surpasses multi-domain adapters, obtaining an\nimprovement of +0.6 in terms of perplexity.\nAt inference time, our approach with a single\nactive path uses 122M parameters (112M of GPT-2\nand ~10M parameters for a path of average length).\nWhen two paths are active, at most 132M parame-\nters are used. The overhead is thus quite small; if\nthe two paths have some overlap, this computation\nis potentially signiﬁcantly less. On average, active\nparameters in our model are trained on less data\nthan multi-domain adapters (e.g., leaf nodes only\nsee on average 400 updates, as shown in Table 1).\n5 Related work\nOur approach draws on prior work in domain adap-\ntion and efﬁcient language model ﬁne-tuning.\nDomain Adaptation. A large research area in\nNLP is domain adaptation (Jiang and Zhai, 2007;\nDaumé III, 2007). Training a masked language\nmodel on data from a speciﬁc domain (Lee et al.,\n2020; Beltagy et al., 2019) or ﬁne-tuning a PLM\nusing data from the target task (Howard and Ruder,\n2018) or the target domain (Rietzler et al., 2020;\nHan and Eisenstein, 2019) has shown to be helpful\nto mitigate the domain shift between train and test\ndata distributions of the same task. Gururangan\nOut-of-domain GPT-2 multi hierarchy hierarchy\nscores adapters 1 path 2 paths\nreuters.com 20.9 16.0 16.4 16.3\nibtimes.co.uk 24.3 19.5 19.7 19.5\nbbc.com 23.6 19.1 18.9 18.7\ntripadvisor.com 40.4 34.8 35.9 33.8\ncnet.com 26.8 23.3 22.2 22.9\ntelegraph.co.uk 30.9 23.6 24.5 22.2\ntheatlantic.com 28.5 23.6 23.8 23.6\nfoxbusiness.com 22.9 17.5 19.9 18.2\nthesun.co.uk 26.8 19.9 19.9 18.2\nnydailynews.com 24.5 19.3 19.5 18.2\ndailystar.co.uk 20.7 13.9 12.2 12.2\nfastcompany.com 27.9 21.3 21.5 20.9\nnypost.com 26.3 18.9 18.9 18.7\nbusinessinsider.com 24.3 20.5 20.7 20.9\ndeadline.com 33.1 26.3 33.1 26.8\nbreitbart.com 22.9 16.9 17.8 17.1\ntechcrunch.com 27.7 21.5 21.8 20.1\nnme.com 28.2 20.1 23.8 20.5\nfool.com 23.8 22.2 22.4 22.2\nﬁnance.yahoo.com 22.6 20.1 20.3 20.1\nyoutube.com 15.3 14.2 14.4 13.5\nncbi.nlm.nih.gov 20.7 18.5 18.4 18.2\nscholars.duke.edu 22.6 20.7 20.3 20.3\ninquisitr.com 22.4 17.5 16.4 16.4\nsimple.wikipedia.org 22.2 19.5 20.5 19.5\nkickstarter.com 26.6 24.0 24.8 22.2\nmashable.com 27.1 22.0 22.0 21.8\nbooking.com 29.7 22.9 24.5 22.0\netsy.com 28.8 26.3 26.8 24.5\nﬁneartamerica.com 25.5 26.6 26.6 24.5\ngithub.com 32.8 30.3 30.6 30.6\njournals.plos.org 23.3 20.1 20.1 18.2\nitunes.apple.com 34.8 28.8 33.1 30.0\nagreatertown.com 44.7 40.0 39.6 35.9\npremium.wpmudev.org 31.527.7 30.0 27.7\nhomestars.com 34.1 29.4 28.2 28.2\nreference.com 28.5 24.5 25.3 24.5\ncnbc.com 21.1 17.6 18.4 17.6\naverage 26.8 22.3 23.0 21.7\nTable 5: Out-of-domain evaluation perplexity. With 1\npath, our hierarchical model performs worse than the\nbaseline. However, using paths of the 2 closest clusters\nto a held-out website, our approach yields better results.\nWe show the paths used in detail in Appendix A.3.\net al. (2020) showed that a PLM can further im-\nprove by ﬁne-tuning on data from a domain that is\nrelated to the domain of the task (DAPT). While\nthis work suggests ﬁne-tuning a different model to\nthe domain of each task, our approach trains a sin-\ngle model to adapt to all domains. Also, although\nDAPT does not permit parameter sharing between\ndomains, our hierarchical adapter model leverages\ndomain similarities to improve adaptation.\nDomain expert mixture (DEMix) layers (Guru-\nrangan et al., 2021) that condition a LM on the\ndomain of input text have been recently proposed.\nDEMix layers replace feed-forward layers in a\ntransformer and each of them is updated only us-\ning data from a speciﬁc domain. Then, a modular\nLM is trained from scratch. On the contrary, we\n1343\nuse a PLM and only train adapter layers on the\ntarget domains. Since each feed-forward layer is\nreplaced with a mixture of experts, the parameters\nadded grow linearly with the domains. In our ap-\nproach, however, the number of parameters grows\nlogarithmically, due to the hierarchical structure.\nAdapters. Efﬁcient ﬁne-tuning using adapters (Re-\nbufﬁ et al., 2017; Houlsby et al., 2019) is preva-\nlent in many NLP taks, such as machine transla-\ntion (Bapna and Firat, 2019), cross-lingual trans-\nfer (Pfeiffer et al., 2020) and dependency parsing\n(Üstün et al., 2020). Adapters can be trained on a\nsingle task or language (Pfeiffer et al., 2020), but\nalso multilingually (Stickland et al., 2021). They\nhave also been used to infuse factual and linguistic\n(Wang et al., 2021) as well as general-purpose and\ncommonsense knowledge (Lauscher et al., 2020)\ninto a PLM. To the best of our knowledge, we are\nthe ﬁrst to use them in a hierarchical structure for\ndomain adaptation.\nEfﬁcient ﬁne-tuning methods for PLMs. Be-\nsides adapters, multiple other parameter-efﬁcient\nmethods to adapt general-purpose PLMs to speciﬁc\ntasks have been recently proposed. Preﬁx tuning\n(Li and Liang, 2021), low-rank matrix approxima-\ntion (Hu et al., 2022), as well as ﬁne-tuning only\nthe bias terms of a PLM (Zaken et al., 2021) are\nsome of the lightweight alternatives to ﬁne-tuning\nthe entire PLM. He et al. (2022) show that these\nﬁne-tuning methods can be seen as modiﬁcations\nto some speciﬁc hidden states of PLMs and can\nthus be recast. We use adapters in this work, but\nour method could possibly also beneﬁt from other\nparameter-efﬁcient approaches.\n6 Conclusion & Future Work\nIn this paper, we present a novel approach for efﬁ-\ncient domain adaptation on multiple domains using\nhierarchical adapters that encode the similarities\nand differences of domains, allowing parameter\nsharing but avoiding negative transfer. We start\nwith a manually deﬁned tree and then scale to a\nlarge tree, created in an unsupervised way. We\nalso provide an evaluation-time algorithm that can\ncombine paths to best adapt to an unseen domain.\nIn the future, we would like to investigate a more\nefﬁcient evaluation-time approach, using only a\nfew tokens of an unseen domain. It would also be\ninteresting to extend our model to a multi-lingual\nsetup. Finally, we would like to use our method to\ncontrol language generation of PLMs, in order to\navoid generating hate speech or toxic text.\n7 Limitations and Risks\nOur work uses generative pretrained language mod-\nels. As such models are trained on large datasets\nfrom text in the Internet, they encode biases that\ncould harm marginalized populations (Bender et al.,\n2021). The specialized language model we propose\ncould be used for propaganda or hate speech gen-\neration, same as any other language model. How-\never, our hierarchical adapter model permits adding\nmodular components and we believe that it could\npotentially be used to detoxify language genera-\ntion, following Liu et al. (2021). This is in line\nwith recent work on sparse models (Gururangan\net al., 2021; Artetxe et al., 2021).\nAcknowledgements\nWe thank the AllenNLP team and other researchers\nat the Allen Institute for AI for their thoughtful\ncomments. We also thank Alex Fraser, Dario Sto-\njanovski, and Suchin Gururangan for helpful dis-\ncussions about this work.\nReferences\nRoee Aharoni and Yoav Goldberg. 2020. Unsupervised\ndomain clusters in pretrained language models. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 7747–\n7763, Online. Association for Computational Lin-\nguistics.\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor\nMihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin,\nJingfei Du, Srinivasan Iyer, Ramakanth Pasunuru,\nGiri Anantharaman, Xian Li, Shuohui Chen, Halil\nAkin, Mandeep Baines, Louis Martin, Xing Zhou,\nPunit Singh Koura, Brian O’Horo, Jeff Wang, Luke\nZettlemoyer, Mona Diab, Zornitsa Kozareva, and\nVes Stoyanov. 2021. Efﬁcient large scale language\nmodeling with mixtures of experts.\nAmittai Axelrod, Xiaodong He, and Jianfeng Gao.\n2011. Domain adaptation via pseudo in-domain data\nselection. In Proceedings of the 2011 Conference on\nEmpirical Methods in Natural Language Processing,\npages 355–362, Edinburgh, Scotland, UK. Associa-\ntion for Computational Linguistics.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-\nton. 2016. Layer normalization.\nAnkur Bapna and Orhan Firat. 2019. Simple, scalable\nadaptation for neural machine translation. In Pro-\nceedings of the Conference on Empirical Methods in\nNatural Language Processing and the International\n1344\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1538–1548.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientiﬁc text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3615–\n3620, Hong Kong, China. Association for Computa-\ntional Linguistics.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\nFAccT ’21, page 610–623, New York, NY , USA. As-\nsociation for Computing Machinery.\nMithun Das Gupta, Srinidhi Srinivasa, J. Madhukara,\nand Meryl Antony. 2015. Kl divergence based ag-\nglomerative clustering for automated vitiligo grad-\ning. In 2015 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 2700–2709.\nHal Daumé III. 2007. Frustratingly easy domain adap-\ntation. In Proceedings of the 45th Annual Meeting of\nthe Association of Computational Linguistics, pages\n256–263, Prague, Czech Republic. Association for\nComputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, pages 4171–4186.\nJesse Dodge, Maarten Sap, Ana Marasovi ´c, William\nAgnew, Gabriel Ilharco, Dirk Groeneveld, Margaret\nMitchell, and Matt Gardner. 2021. Documenting\nlarge webtext corpora: A case study on the colos-\nsal clean crawled corpus. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1286–1305, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nSuchin Gururangan, Mike Lewis, Ari Holtzman,\nNoah A. Smith, and Luke Zettlemoyer. 2021. Demix\nlayers: Disentangling domains for modular lan-\nguage modeling.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nXiaochuang Han and Jacob Eisenstein. 2019. Unsu-\npervised domain adaptation of contextualized em-\nbeddings for sequence labeling. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4238–4248, Hong Kong,\nChina. Association for Computational Linguistics.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2022. Towards a\nuniﬁed view of parameter-efﬁcient transfer learning.\nIn International Conference on Learning Represen-\ntations.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for NLP.\nIn Proceedings of the International Conference on\nMachine Learning, Proceedings of Machine Learn-\ning Research, pages 2790–2799.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. LoRA: Low-rank adaptation of\nlarge language models. In International Conference\non Learning Representations.\nJing Jiang and ChengXiang Zhai. 2007. Instance\nweighting for domain adaptation in NLP. In Pro-\nceedings of the 45th Annual Meeting of the Associ-\nation of Computational Linguistics, pages 264–271,\nPrague, Czech Republic. Association for Computa-\ntional Linguistics.\nJared Kaplan, Sam McCandlish, T. J. Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeff Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. ArXiv,\nabs/2001.08361.\nDiederick P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations.\nAnne Lauscher, Olga Majewska, Leonardo F. R.\nRibeiro, Iryna Gurevych, Nikolai Rozanov, and\nGoran Glavaš. 2020. Common sense or world\nknowledge? investigating adapter-based knowledge\ninjection into pretrained transformers. In Proceed-\nings of Deep Learning Inside Out (DeeLIO): The\nFirst Workshop on Knowledge Extraction and Inte-\ngration for Deep Learning Architectures, pages 43–\n49, Online. Association for Computational Linguis-\ntics.\n1345\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2019. BioBERT: a pre-\ntrained biomedical language representation model\nfor biomedical text mining. Bioinformatics,\n36(4):1234–1240.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. 2020. Biobert: a pre-trained biomed-\nical language representation model for biomedical\ntext mining. Bioinformatics, 36(4):1234–1240.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,\nDehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. 2021.\n{GS}hard: Scaling giant models with conditional\ncomputation and automatic sharding. In Interna-\ntional Conference on Learning Representations.\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n4582–4597, Online. Association for Computational\nLinguistics.\nAlisa Liu, Maarten Sap, Ximing Lu, Swabha\nSwayamdipta, Chandra Bhagavatula, Noah A.\nSmith, and Yejin Choi. 2021. DExperts: Decoding-\ntime controlled text generation with experts and anti-\nexperts. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 6691–6706, Online. Association for Computa-\ntional Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nAntonis Maronikolakis and Hinrich Schütze. 2021.\nMultidomain pretrained language models for green\nNLP. In Proceedings of the Second Workshop\non Domain Adaptation for NLP , pages 1–8, Kyiv,\nUkraine. Association for Computational Linguistics.\nRobert C. Moore and William Lewis. 2010. Intelligent\nselection of language model training data. In Pro-\nceedings of the ACL 2010 Conference Short Papers,\npages 220–224, Uppsala, Sweden. Association for\nComputational Linguistics.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming\nLin, Alban Desmaison, Luca Antiga, and Adam\nLerer. 2017. Automatic differentiation in pytorch.\nIn NIPS 2017 Workshop on Autodiff.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654–7673, Online. Association for Computa-\ntional Linguistics.\nBarbara Plank. 2016. What to do about non-standard\n(or non-canonical) language in nlp. In Proceed-\nings of the 13th Conference on Natural Language\nProcessing, KONVENS 2016, Bochum, Germany,\nSeptember 19-21, 2016, volume 16 ofBochumer Lin-\nguistische Arbeitsberichte.\nBarbara Plank and Gertjan van Noord. 2011. Effec-\ntive measures of domain similarity for parsing. In\nProceedings of the 49th Annual Meeting of the As-\nsociation for Computational Linguistics: Human\nLanguage Technologies, pages 1566–1576, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nSylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea\nVedaldi. 2017. Learning multiple visual domains\nwith residual adapters. In Advances in Neural In-\nformation Processing Systems.\nAlexander Rietzler, Sebastian Stabinger, Paul Opitz,\nand Stefan Engl. 2020. Adapt or get left behind:\nDomain adaptation through BERT language model\nﬁnetuning for aspect-target sentiment classiﬁcation.\nIn Proceedings of the 12th Language Resources\nand Evaluation Conference, pages 4933–4941, Mar-\nseille, France. European Language Resources Asso-\nciation.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\n1346\nAsa Cooper Stickland, Xian Li, and Marjan\nGhazvininejad. 2021. Recipes for adapting\npre-trained monolingual and multilingual models to\nmachine translation. In Proceedings of the Confer-\nence of the European Chapter of the Association for\nComputational Linguistics, pages 3440–3453.\nAhmet Üstün, Arianna Bisazza, Gosse Bouma, and\nGertjan van Noord. 2020. UDapter: Language adap-\ntation for truly Universal Dependency parsing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 2302–2315, Online. Association for Computa-\ntional Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xu-\nanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang,\nand Ming Zhou. 2021. K-Adapter: Infusing Knowl-\nedge into Pre-Trained Models with Adapters. In\nFindings of the Association for Computational Lin-\nguistics: ACL-IJCNLP 2021, pages 1405–1418, On-\nline. Association for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nElad Ben Zaken, Shauli Ravfogel, and Yoav Gold-\nberg. 2021. Bitﬁt: Simple parameter-efﬁcient\nﬁne-tuning for transformer-based masked language-\nmodels. CoRR, abs/2106.10199.\n1347\nA Appendix\nA.1 Corpus description\nIn Table 8, we present the sizes of the training and\nevaluation corpora used for the many-domain setup.\nOnly one corpus is used for the few-domain but\nnot the many-domain experimental setting, namely\nYELP.COM 2. This corpus has 684M training to-\nkens and 20M evaluation tokens. We randomly\nsub-sample 53M training tokens of this corpus for\nour ﬁrst, few-domain setup, as we want to train a\nbalanced model. Extensive documentation of the\ncorpus is available from Dodge et al. (2021). We\nuse the C4 corpus in accordance with the terms of\nuse3.\nA.2 Few-Domain Setup\nIn Table 3, we present out-of-domain evaluation\nperplexities for the ﬁrst experimental setup. For\nthe single adapters model, we present the best-\nperforming models in the table. In order to allow\nfor an exhaustive comparison, we also present the\nevaluation results of all the trained single adapter\nmodels on all held-out websites in Table 6. We see,\nfor example, that when evaluating on the traveling\nwebsite TRIPADVISOR , the single adapter model\nthat was trained on either BOOKING or YELP pro-\nvides the lowest perplexity scores, conﬁrming our\nintuition that for a held-out website, we should use\na model trained on a very similar domain.\nsingle adapters trained on baseline\nbooking yelp frontiers journals GPT-2\nncbi 20.1 20.1 19.7 18.2 20.5\nlink.springer 27.1 27.1 24.5 24.5 27.7\nscholars.duke 22.2 22.2 22.2 20.1 22.7\ntechcrunch 27.1 27.1 27.1 27.1 27.7\nmedium 30.0 30.0 30.0 30.0 29.1\ntripadvisor 36.6 36.6 49.4 49.4 41.3\nlonelyplanet 30.0 27.1 40.4 40.4 35.5\nTable 6: Out-of-domain evaluation of single adapters\nin the few-domain setup ( §3). We evaluate every set\nof single adapters in 7 different websites. The best re-\nsults for every out-of-domain website (underlined) are\nshown in Table 3.\nA.3 Many-Domain Setup\nConfusion Matrix. Figure 4 depicts the confusion\nmatrix of the GMM. We can observe visually that\nsome clusters assign a high probability to multiple\ninternet domains, while others remain empty. This\nshows that the intuition have for what a domain is\n2www.yelp.com/dataset\n3commoncrawl.org/terms-of-use/\nIn-domain scores GPT-2 multi hierarchical\nadapters adapters\nign.com 30.0 25.5 23.8\ninsiderpages.com 30.0 19.7 18.4\neventbrite.com 34.5 27.4 25.5\nandroidheadlines.com 21.8 17.1 16.0\nlink.springer.com 27.9 22.6 21.5\nlibrarything.com 29.4 17.6 16.9\ncsmonitor.com 29.4 25.8 24.8\ncity-data.com 36.2 31.2 30.3\nforums.macrumors.com 37.0 27.7 26.0\nglassdoor.com 20.7 7.9 7.5\noreilly.com 27.4 21.5 20.5\npcworld.com 24.3 19.7 18.9\nexpress.co.uk 22.2 15.0 14.0\nanswers.sap.com 60.3 34.5 30.3\nprweb.com 25.8 20.1 18.9\ninstructables.com 32.8 28.2 26.6\ndeviantart.com 42.5 33.1 31.2\nentrepreneur.com 26.3 22.0 20.9\nsi.com 22.2 17.3 16.4\ngsmarena.com 56.3 34.5 31.2\nwired.com 30.0 24.3 23.8\nmedium.com 29.1 23.1 22.6\nbaltimoresun.com 27.1 20.9 20.1\nnpr.org 22.2 18.0 17.5\nfrontiersin.org 22.0 18.4 17.2\nchicagotribune.com 27.1 21.1 20.7\nfoxnews.com 22.2 15.3 14.9\naljazeera.com 22.2 17.8 17.1\ndailymail.co.uk 27.1 21.1 20.7\nlonelyplanet.com 35.5 19.5 17.1\naverage 30.0 22.3 21.0\nTable 7: In-domain evaluation perplexity for the many-\ndomain setup (we note that the hierarchical model uses\na single path).\ndoes not correspond exactly to the cluster obtained\nby an unsupervised, data-driven approach. Our\nvisualization is based on publicly available code4.\nOut-of-domain Evaluation. As mentioned in\n§4.5, to run evaluation on a given out-of-domain\nwebsite i, we use two paths of the trained hierar-\nchical model. The ﬁrst path leads to the leaf node\nthat corresponds to clusterm(with massigning the\nhighest probability to the largest fraction of N se-\nquences from website i) and the second path leads\nto cluster n, where nassigns the highest probabil-\nity to the second-most number of the N sequences.\nWe present the clusters mand n(and the websites\nthey were mapped to during training) in Table 9.\nA.4 Experimental Details\nBecause we wanted to keep a modest computa-\ntional budget, we did not perform multiple training\nruns for the hierarchical models and the baselines.\nResults are reported over a single run.\n4github.com/roeeaharoni/unsupervised-domain-clusters\n1348\nTrain (Eval.) Tokens Train (Eval.) Tokens\nTRAININGCORPUS\nfrontiersin.org 38M (6M)\nEVALUATIONCORPUS\njournals.plos.org 53M (6M)\nchicagotribune.com 31M (4M) fool.com 34M (4M)\nlink.springer.com 28M (4M) businessinsider.com 32M (4M)\naljazeera.com 26M (3M) theatlantic.com 30M (4M)\ninstructables.com 25M (3M) booking.com 30M (4M)\nnpr.org 25M (3M) kickstarter.com 26M (3M)\ndailymail.co.uk 25M (3M) telegraph.co.uk 25M (3M)\ncsmonitor.com 23M (3M) cnet.com 24M (3M)\nbaltimoresun.com 23M (3M) ncbi.nlm.nih.gov 23M (3M)\ncity-data.com 22M (3M) foxbusiness.com 23M (3M)\nforums.macrumors.com 22M (3M) cnbc.com 20M (2M)\nmedium.com 22M (3M) ibtimes.co.uk 18M (2M)\nfoxnews.com 22M (3M) reuters.com 17M (2M)\nsi.com 18M (2M) bbc.com 17M (2M)\nwired.com 18M (2M) nypost.com 15M (2M)\nprweb.com 17M (2M) nydailynews.com 14M (2M)\nexpress.co.uk 16M (2M) fastcompany.com 14M (2M)\nentrepreneur.com 16M (2M) mashable.com 14M (2M)\nandroidheadlines.com 14M (2M) thesun.co.uk 13M (2M)\npcworld.com 14M (2M) techcrunch.com 13M (2M)\ngsmarena.com 12M (2M) inquisitr.com 13M (2M)\neventbrite.com 11M (1M) youtube.com 11M (1M)\nign.com 10M (1M) itunes.apple.com 11M (1M)\noreilly.com 9M (1M) breitbart.com 10M (1M)\ndeviantart.com 9M (1M) etsy.com 10M (1M)\ninsiderpages.com 8M (1M) github.com 10M (1M)\nlonelyplanet.com 6M (1M) agreatertown.com 9M (1M)\nanswers.sap.com 6M (1M) premium.wpmudev.org 9M (1M)\nglassdoor.com 4M (500K) deadline.com 9M (1M)\nlibrarything.com 3M (500K) dailystar.co.uk 9M (1M)\nreference.com 7M (1M)\nscholars.duke.edu 7M (1M)\ntripadvisor.com 7M (1M)\nsimple.wikipedia.org 6M (1M)\nnme.com 5M (1M)\nhomestars.com 3M (500K)\nﬁneartamerica.com 2M (500K)\nTable 8: Domains that make up our in-domain (training) and out of-domain (evaluation) corpus for the large setup,\nincluding the size of our training and evaluation data. All data is extracted from C4 (Raffel et al., 2020).\n1349\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29\nCluster\nwww.ign.com\nwww.insiderpages.com\nwww.eventbrite.com\nwww.androidheadlines.com\nlink.springer.com\nwww.librarything.com\nwww.csmonitor.com\nwww.city-data.com\nforums.macrumors.com\nwww.glassdoor.com\nwww.oreilly.com\nwww.pcworld.com\nwww.express.co.uk\nanswers.sap.com\nwww.prweb.com\nwww.instructables.com\nwww.deviantart.com\nwww.entrepreneur.com\nwww.si.com\nwww.gsmarena.com\nwww.wired.com\nmedium.com\nwww.baltimoresun.com\nwww.npr.org\nwww.frontiersin.org\nwww.chicagotribune.com\nwww.foxnews.com\nwww.aljazeera.com\nwww.dailymail.co.uk\nwww.lonelyplanet.com\nInternet Domain\n872 0 0 9 1 9 0 0 0 7 3 0 0 0 6 5 4 0 1 0 58 6 0 9 0 0 0 0 2 8\n8 969 0 0 0 0 0 3 5 6 0 0 0 0 0 2 3 1 0 0 0 1 0 0 0 0 0 0 0 2\n0 754 0 0 1 137 0 2 0 4 0 0 0 0 3 1 1 75 0 0 1 16 0 0 0 0 0 0 1 4\n1 0 0 847 1 0 0 0 0 0 3 0 0 0 3 1 0 3 0 1 136 4 0 0 0 0 0 0 0 0\n0 0 0 0 762 1 10 0 0 5 11 0 0 0 0 0 0 0 0 0 0 12 0 0 194 0 0 0 0 5\n4 0 0 0 0 785 15 3 1 72 2 0 0 0 2 0 16 5 0 0 0 84 0 0 0 0 0 1 2 8\n1 1 0 0 5 102 213 174 0 1 1 0 0 0 2 13 0 14 11 0 17 5 0 43 0 0 0 370 26 1\n2 3 0 0 0 3 145 761 3 3 0 0 0 0 4 39 3 8 0 0 7 15 0 4 0 0 0 0 0 0\n2 0 0 7 6 1 6 22 848 3 10 0 0 4 10 11 1 1 0 8 6 52 0 0 0 0 0 2 0 0\n0 4 0 0 1 0 1 0 0 886 1 0 0 0 0 1 0 102 0 0 0 3 0 1 0 0 0 0 0 0\n0 0 0 2 97 0 1 0 4 0 810 0 0 2 16 1 0 32 0 0 1 34 0 0 0 0 0 0 0 0\n11 0 0 487 14 0 2 0 2 0 172 0 0 0 10 2 0 11 0 0 263 20 0 4 0 0 0 1 1 0\n0 0 0 1 2 0 2 0 0 0 0 0 868 0 10 4 0 0 0 0 0 0 0 0 0 0 0 5 108 0\n0 0 0 0 1 1 0 0 0 5 3 0 0 982 1 0 0 0 0 0 0 7 0 0 0 0 0 0 0 0\n0 219 0 1 68 46 14 20 0 2 31 0 0 0 91 18 0 427 0 0 27 18 0 6 0 0 0 0 9 3\n1 0 0 0 30 0 6 1 42 0 12 0 0 1 4 877 5 0 0 0 2 19 0 0 0 0 0 0 0 0\n10 0 0 0 0 5 24 7 20 1 0 0 0 0 21 17 849 1 0 0 4 39 0 0 0 0 0 0 2 0\n0 1 0 8 2 13 12 22 0 0 21 0 0 0 12 3 0 802 0 0 61 31 0 6 0 0 0 3 3 0\n0 0 0 0 0 0 1 0 0 0 0 0 126 0 1 0 0 0 769 0 1 0 0 0 0 0 0 0 102 0\n1 0 0 59 0 0 0 0 40 0 1 0 0 0 0 1 0 0 0 890 2 6 0 0 0 0 0 0 0 0\n21 3 0 62 68 29 185 24 3 6 43 0 0 0 17 28 1 59 1 0 280 126 0 15 0 0 0 15 14 0\n6 3 0 2 66 28 39 19 6 3 289 0 0 5 33 14 6 219 15 0 29 185 0 5 0 0 0 26 2 0\n0 6 0 1 1 137 28 315 0 2 0 0 0 0 1 16 0 12 189 0 11 0 0 34 0 0 0 36 211 0\n1 1 0 0 13 129 94 124 0 6 1 0 0 0 4 16 0 10 5 0 13 6 0 434 0 0 0 105 38 0\n0 0 0 0 424 0 0 0 0 6 3 0 0 0 7 0 0 0 0 0 0 0 0 0 560 0 0 0 0 0\n1 11 0 2 2 118 46 268 0 6 0 0 0 0 1 15 0 10 147 0 23 7 0 39 0 0 0 63 236 5\n0 0 0 2 35 3 30 43 0 2 0 0 0 0 24 11 0 2 85 0 6 5 0 15 0 0 0 322 415 0\n0 0 0 0 2 7 55 7 0 0 0 0 8 0 2 0 0 1 4 0 5 0 0 1 0 0 0 823 84 1\n0 0 0 3 14 23 43 17 0 0 0 0 65 0 18 16 0 1 12 0 4 4 0 2 0 0 0 95 669 14\n0 1 0 0 0 3 18 0 0 12 0 0 0 0 14 15 0 0 0 0 0 0 0 0 0 0 0 8 0 929\nFigure 4: Confusion Matrix. The x-axis depicts the clusters that the internet domains are assigned to. If no data\nsamples are added to a cluster (for example, cluster 2), the corresponding Gaussian distribution is not used for\nthe hierarchical clustering. The y-axis depicts the internet domains used for training. The cluster numbers shown\nhere are not the exact ones shown in the ﬁnal dendrogram, but one can easily observe that, for example, the same\ncluster (in this example, cluster 7) assigns the highest probability to CITY-DATA.COM , BALTIMORESUN .COM and\nCHICAGOTRIBUNE .COM . This is mirrored in Figure 2 of the main paper.\n1350\nHeld-out Website Path to Clusterm Train. Website of Clusterm Path to Clustern Train. Website of Clustern\nreuters.com 14 aljazeera.com 15 npr.org\nibtimes.co.uk 9 dailymail.co.uk 14 aljazeera.com\nbbc.com 9 dailymail.co.uk 3 express.co.uk\ntripadvisor.com 5 insiderpages.com 22 lonelyplanet.com\ncnet.com 18 wired.com 17 androidheadlines.com\ntelegraph.co.uk 9 dailymail.co.uk 14 aljazeera.com\ntheatlantic.com 0 city-data.com 14 aljazeera.com\nfoxbusiness.com 11 prweb.com 15 npr.org\nthesun.co.uk 9 dailymail.co.uk 3 express.co.uk\nnydailynews.com 9 dailymail.co.uk 6 si.com\ndailystar.co.uk 3 express.co.uk 9 dailymail.co.uk\nfastcompany.com 1 entrepreneur.com 18 wired.com\nnypost.com 9 dailymail.co.uk 6 si.com\nbusinessinsider.com 1 entrepreneur.com 18 wired.com\ndeadline.com 8 librarything.com 9 dailymail.co.uk\nbreitbart.com 14 aljazeera.com 0 city-data.com\ntechcrunch.com 18 wired.com 1 entrepreneur.com\nnme.com 8 librarything.com 9 dailymail.co.uk\nfool.com 1 entrepreneur.com 15 npr.org\nﬁnance.yahoo.com 1 entrepreneur.com 15 npr.org\nyoutube.com 11 prweb.com 15 npr.org\nncbi.nlm.nih.gov 4 link.springer.com 19 frontiersin.org\nscholars.duke.edu 4 link.springer.com 19 frontiersin.org\ninquisitr.com 9 dailymail.co.uk 18 wired.com\nsimple.wikipedia.org 10 csmonitor.com 8 librarything.com\nkickstarter.com 16 deviantart.com 18 wired.com\nmashable.com 18 wired.com 9 dailymail.co.uk\nbooking.com 5 insiderpages.com 22 lonelyplanet.com\netsy.com 13 instructables.com 5 insiderpages.com\nﬁneartamerica.com 16 deviantart.com 13 instructables.com\ngithub.com 7 oreilly.com 2 answers.sap.com\njournals.plos.org 4 link.springer.com 19 frontiersin.org\nitunes.apple.com 20 gsmarena.com 8 librarything.com\nagreatertown.com 22 lonelyplanet.com 5 insiderpages.com\npremium.wpmudev.org 2 answers.sap.com 7 oreilly.com\nhomestars.com 5 insiderpages.com 13 instructables.com\nreference.com 13 instructables.com 10 csmonitor.com\ncnbc.com 15 npr.org 1 entrepreneur.com\nTable 9: The two paths used for evaluation of the hierarchical adapter model on each held-out website.\n1351",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8098177909851074
    },
    {
      "name": "Domain adaptation",
      "score": 0.7325746417045593
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.6504935026168823
    },
    {
      "name": "Language model",
      "score": 0.523308515548706
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.48424026370048523
    },
    {
      "name": "Natural language processing",
      "score": 0.41629740595817566
    },
    {
      "name": "Artificial intelligence",
      "score": 0.41466760635375977
    },
    {
      "name": "Psychology",
      "score": 0.06282469630241394
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I8204097",
      "name": "Ludwig-Maximilians-Universität München",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210156221",
      "name": "Allen Institute for Artificial Intelligence",
      "country": "US"
    }
  ]
}