{
  "title": "Conformer: Convolution-augmented Transformer for Speech Recognition",
  "url": "https://openalex.org/W3025165719",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4364850335",
      "name": "Gulati, Anmol",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4223153741",
      "name": "Qin, James",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4223153740",
      "name": "Chiu, Chung-Cheng",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4281510514",
      "name": "Parmar, Niki",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2102355683",
      "name": "Zhang Yu",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2552959557",
      "name": "Yu, Jiahui",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2018778232",
      "name": "Han Wei",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2189289017",
      "name": "Wang Shi-bo",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1474586170",
      "name": "Zhang Zheng-dong",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1993644842",
      "name": "Wu, Yonghui",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2993384157",
      "name": "Pang, Ruoming",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963542740",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2972818416",
    "https://openalex.org/W2952180055",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W2948981900",
    "https://openalex.org/W1828163288",
    "https://openalex.org/W3019527251",
    "https://openalex.org/W2932319281",
    "https://openalex.org/W2962760690",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W3016010032",
    "https://openalex.org/W2112739286",
    "https://openalex.org/W2928941594",
    "https://openalex.org/W2963970792",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2798858969",
    "https://openalex.org/W2963414781",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2981581604",
    "https://openalex.org/W2767286248",
    "https://openalex.org/W2981857663",
    "https://openalex.org/W3015194534",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W1995562189",
    "https://openalex.org/W2994771587",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W2937843571",
    "https://openalex.org/W2979636403",
    "https://openalex.org/W3021469861",
    "https://openalex.org/W1964175594"
  ],
  "abstract": "Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.",
  "full_text": "Conformer: Convolution-augmented Transformer for Speech Recognition\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo\nWang, Zhengdong Zhang, Yonghui Wu, Ruoming Pang\nGoogle Inc.\n{anmolgulati, jamesqin, chungchengc, nikip, ngyuzh, jiahuiyu, weihan, shibow, zhangzd,\nyonghui, rpang}@google.com\nAbstract\nRecently Transformer and Convolution neural network (CNN)\nbased models have shown promising results in Automatic\nSpeech Recognition (ASR), outperforming Recurrent neural\nnetworks (RNNs). Transformer models are good at captur-\ning content-based global interactions, while CNNs exploit lo-\ncal features effectively. In this work, we achieve the best of\nboth worlds by studying how to combine convolution neural\nnetworks and transformers to model both local and global de-\npendencies of an audio sequence in a parameter-efﬁcient way.\nTo this regard, we propose the convolution-augmented trans-\nformer for speech recognition, named Conformer. Conformer\nsigniﬁcantly outperforms the previous Transformer and CNN\nbased models achieving state-of-the-art accuracies. On the\nwidely used LibriSpeech benchmark, our model achieves WER\nof 2.1%/4.3% without using a language model and 1.9%/3.9%\nwith an external language model on test/testother. We also\nobserve competitive performance of 2.7%/6.3% with a small\nmodel of only 10M parameters.\nIndex Terms: speech recognition, attention, convolutional neu-\nral networks, transformer, end-to-end\n1. Introduction\nEnd-to-end automatic speech recognition (ASR) systems based\non neural networks have seen large improvements in recent\nyears. Recurrent neural networks (RNNs) have been the de-\nfacto choice for ASR [1, 2, 3, 4] as they can model the temporal\ndependencies in the audio sequences effectively [5]. Recently,\nthe Transformer architecture based on self-attention [6, 7] has\nenjoyed widespread adoption for modeling sequences due to its\nability to capture long distance interactions and the high train-\ning efﬁciency. Alternatively, convolutions have also been suc-\ncessful for ASR [8, 9, 10, 11, 12], which capture local context\nprogressively via a local receptive ﬁeld layer by layer.\nHowever, models with self-attention or convolutions each\nhas its limitations. While Transformers are good at modeling\nlong-range global context, they are less capable to extract ﬁne-\ngrained local feature patterns. Convolution neural networks\n(CNNs), on the other hand, exploit local information and are\nused as the de-facto computational block in vision. They learn\nshared position-based kernels over a local window which main-\ntain translation equivariance and are able to capture features like\nedges and shapes. One limitation of using local connectivity is\nthat you need many more layers or parameters to capture global\ninformation. To combat this issue, contemporary work Con-\ntextNet [10] adopts the squeeze-and-excitation module [13] in\neach residual block to capture longer context. However, it is still\nlimited in capturing dynamic global context as it only applies a\nglobal averaging over the entire sequence.\nRecent works have shown that combining convolution and\nFeed Forward Module\nMulti-Head Self Attention\nModule\nConvolution Module\n1/2 x\n1/2 x\nFeed Forward Module\nDropout\nConformer Blocks\nLinear\nSpecAug\n10 ms rate\n10 ms rate\nConvolution\nSubsampling\n40 ms rate\nx N\n40 ms rate\n40 ms rate\n+ \n+ \n+ \n+ \nLayernorm\nFigure 1: Conformer encoder model architecture.Conformer\ncomprises of two macaron-like feed-forward layers with half-\nstep residual connections sandwiching the multi-headed self-\nattention and convolution modules. This is followed by a post\nlayernorm.\nself-attention improves over using them individually [14]. To-\ngether, they are able to learn both position-wise local features,\nand use content-based global interactions. Concurrently, papers\nlike [15, 16] have augmented self-attention with relative posi-\ntion based information that maintains equivariance. Wu et al.\n[17] proposed a multi-branch architecture with splitting the in-\nput into two branches: self-attention and convolution; and con-\ncatenating their outputs. Their work targeted mobile applica-\ntions and showed improvements in machine translation tasks.\nIn this work, we study how to organically combine con-\nvolutions with self-attention in ASR models. We hypothesize\nthat both global and local interactions are important for being\nparameter efﬁcient. To achieve this, we propose a novel combi-\nnation of self-attention and convolution will achieve the best of\nboth worlds – self-attention learns the global interaction whilst\nthe convolutions efﬁciently capture the relative-offset-based lo-\ncal correlations. Inspired by Wu et al. [17, 18], we introduce\na novel combination of self-attention and convolution, sand-\nwiched between a pair feed forward modules, as illustrated in\nFig 1.\nOur proposed model, named Conformer, achieves state-of-\nthe-art results on LibriSpeech, outperforming the previous best\npublished Transformer Transducer [7] by 15% relative improve-\narXiv:2005.08100v1  [eess.AS]  16 May 2020\nLayernorm Glu \nActivation \nPointwise \nConv BatchNorm Swish \nActivation \n1D \nDepthwise \nConv \nPointwise \nConv Dropout + \nFigure 2: Convolution module.The convolution module contains a pointwise convolution with an expansion factor of 2 projecting the\nnumber of channels with a GLU activation layer, followed by a 1-D Depthwise convolution. The 1-D depthwise conv is followed by a\nBatchnorm and then a swish activation layer.\nment on the testother dataset with an external language model.\nWe present three models based on model parameter limit con-\nstraints of 10M , 30M and 118M. Our 10M model shows an im-\nprovement when compared to similar sized contemporary work\n[10] with 2.7%/6.3% on test/testother datasets. Our medium\n30M parameters-sized model already outperforms transformer\ntransducer published in [7] which uses 139M model parameters.\nWith the big 118M parameter model, we are able to achieve\n2.1%/4.3% without using language models and 1.9%/3.9% with\nan external language model.\nWe further carefully study the effects of the number of at-\ntention heads, convolution kernel sizes, activation functions,\nplacement of feed-forward layers, and different strategies of\nadding convolution modules to a Transformer-based network,\nand shed light on how each contributes to the accuracy improve-\nments.\n2. Conformer Encoder\nOur audio encoder ﬁrst processes the input with a convolution\nsubsampling layer and then with a number of conformer blocks,\nas illustrated in Figure 1. The distinctive feature of our model is\nthe use of Conformer blocks in the place of Transformer blocks\nas in [7, 19].\nA conformer block is composed of four modules stacked\ntogether, i.e, a feed-forward module, a self-attention module,\na convolution module, and a second feed-forward module in\nthe end. Sections 2.1, 1, and 2.3 introduce the self-attention,\nconvolution, and feed-forward modules, respectively. Finally,\n2.4 describes how these sub blocks are combined.\n2.1. Multi-Headed Self-Attention Module\nWe employ multi-headed self-attention (MHSA) while integrat-\ning an important technique from Transformer-XL [20], the rel-\native sinusoidal positional encoding scheme. The relative po-\nsitional encoding allows the self-attention module to general-\nize better on different input length and the resulting encoder is\nmore robust to the variance of the utterance length. We use pre-\nnorm residual units [21, 22] with dropout which helps training\nand regularizing deeper models. Figure 3 below illustrates the\nmulti-headed self-attention block.\nLayernorm\nMulti-Head Attention with\nRelative Positional\nEmbedding\nDropout + \nFigure 3: Multi-Headed self-attention module.We use multi-\nheaded self-attention with relative positional embedding in a\npre-norm residual unit.\n2.2. Convolution Module\nInspired by [17], the convolution module starts with a gating\nmechanism [23]—a pointwise convolution and a gated linear\nunit (GLU). This is followed by a single 1-D depthwise convo-\nlution layer. Batchnorm is deployed just after the convolution\nto aid training deep models. Figure 2 illustrates the convolution\nblock.\n2.3. Feed Forward Module\nThe Transformer architecture as proposed in [6] deploys a feed\nforward module after the MHSA layer and is composed of two\nlinear transformations and a nonlinear activation in between. A\nresidual connection is added over the feed-forward layers, fol-\nlowed by layer normalization. This structure is also adopted by\nTransformer ASR models [7, 24].\nWe follow pre-norm residual units [21, 22] and apply layer\nnormalization within the residual unit and on the input before\nthe ﬁrst linear layer. We also apply Swish activation [25] and\ndropout, which helps regularizing the network. Figure 4 illus-\ntrates the Feed Forward (FFN) module.\n2.4. Conformer Block\nOur proposed Conformer block contains two Feed Forward\nmodules sandwiching the Multi-Headed Self-Attention module\nand the Convolution module, as shown in Figure 1.\nThis sandwich structure is inspired by Macaron-Net [18],\nwhich proposes replacing the original feed-forward layer in the\nTransformer block into two half-step feed-forward layers, one\nbefore the attention layer and one after. As in Macron-Net, we\nemploy half-step residual weights in our feed-forward (FFN)\nmodules. The second feed-forward module is followed by a\nﬁnal layernorm layer. Mathematically, this means, for input xi\nto a Conformer block i, the output yi of the block is:\n˜xi = xi + 1\n2FFN(xi)\nx′\ni = ˜xi + MHSA( ˜xi)\nx′′\ni = x′\ni + Conv(x′\ni)\nyi = Layernorm(x′′\ni + 1\n2FFN(x′′\ni ))\n(1)\nwhere FFN refers to the Feed forward module, MHSA refers to\nthe Multi-Head Self-Attention module, and Conv refers to the\nConvolution module as described in the preceding sections.\nOur ablation study discussed in Sec 3.4.3 compares the\nMacaron-style half-step FFNs with the vanilla FFN as used in\nprevious works. We ﬁnd that having two Macaron-net style\nfeed-forward layers with half-step residual connections sand-\nwiching the attention and convolution modules in between pro-\nvides a signiﬁcant improvement over having a single feed-\nforward module in our Conformer architecture.\nThe combination of convolution and self-attention has been\nstudied before and one can imagine many ways to achieve\nLayernorm Linear \nLayer Dropout Linear \nLayer \nSwish \nActivation Dropout + \nFigure 4: Feed forward module.The ﬁrst linear layer uses an expansion factor of 4 and the second linear layer projects it back to the\nmodel dimension. We use swish activation and a pre-norm residual units in feed forward module.\nthat. Different options of augmenting convolutions with self-\nattention are studied in Sec 3.4.2. We found that convolution\nmodule stacked after the self-attention module works best for\nspeech recognition.\n3. Experiments\n3.1. Data\nWe evaluate the proposed model on the LibriSpeech [26]\ndataset, which consists of 970 hours of labeled speech and\nan additional 800M word token text-only corpus for building\nlanguage model. We extracted 80-channel ﬁlterbanks features\ncomputed from a 25ms window with a stride of 10ms. We use\nSpecAugment [27, 28] with mask parameter (F = 27), and ten\ntime masks with maximum time-mask ratio (pS = 0.05), where\nthe maximum-size of the time mask is set topS times the length\nof the utterance.\n3.2. Conformer Transducer\nWe identify three models, small, medium and large, with 10M,\n30M, and 118M params, respectively, by sweeping different\ncombinations of network depth, model dimensions, number of\nattention heads and choosing the best performing one within\nmodel parameter size constraints. We use a single-LSTM-layer\ndecoder in all our models. Table 1 describes their architecture\nhyper-parameters.\nFor regularization, we apply dropout [29] in each residual\nunit of the conformer, i.e, to the output of each module, before\nit is added to the module input. We use a rate of Pdrop = 0.1.\nVariational noise [5, 30] is introduced to the model as a regu-\nlarization. A ℓ2 regularization with 1e−6 weight is also added\nto all the trainable weights in the network. We train the models\nwith the Adam optimizer [31] with β1 = 0.9, β2 = 0.98 and\nϵ = 10−9 and a transformer learning rate schedule [6], with\n10k warm-up steps and peak learning rate 0.05/\n√\nd where d is\nthe model dimension in conformer encoder.\nWe use a 3-layer LSTM language model (LM) with width\n4096 trained on the LibriSpeech langauge model corpus with\nthe LibriSpeech960h transcripts added, tokenized with the 1k\nWPM built from LibriSpeech 960h. The LM has word-level\nperplexity 63.9 on the dev-set transcripts. The LM weight λ\nfor shallow fusion is tuned on the dev-set via grid search. All\nmodels are implemented with Lingvo toolkit [32].\n3.3. Results on LibriSpeech\nTable 2 compares the (WER) result of our model on Lib-\nriSpeech test-clean/test-other with a few state-of-the-art mod-\nels include: ContextNet [10], Transformer transducer [7], and\nQuartzNet [9]. All our evaluation results round up to 1 digit\nafter decimal point.\nWithout a language model, the performance of our\nmedium model already achieve competitive results of 2.3/5.0\non test/testother outperforming the best known Transformer,\nLSTM based model, or a similar sized convolution model. With\nthe language model added, our model achieves the lowest word\nTable 1: Model hyper-parameters for Conformer S, M, and L\nmodels, found via sweeping different combinations and choos-\ning the best performing models within the parameter limits.\nModel Conformer\n(S)\nConformer\n(M)\nConformer\n(L)\nNum Params (M) 10.3 30.7 118.8\nEncoder Layers 16 16 17\nEncoder Dim 144 256 512\nAttention Heads 4 4 8\nConv Kernel Size 32 32 32\nDecoder Layers 1 1 1\nDecoder Dim 320 640 640\nTable 2: Comparison of Conformer with recent published mod-\nels. Our model shows improvements consistently over various\nmodel parameter size constraints. At 10.3M parameters, our\nmodel is 0.7% better on testother when compared to contempo-\nrary work, ContextNet(S) [10]. At 30.7M model parameters our\nmodel already signiﬁcantly outperforms the previous published\nstate of the art results of Transformer Transducer [7] with 139M\nparameters.\nMethod #Params (M) WER Without LM WER With LM\ntestclean testother testclean testother\nHybrid\nTransformer [33] - - - 2.26 4.85\nCTC\nQuartzNet [9] 19 3.90 11.28 2.69 7.25\nLAS\nTransformer [34] 270 2.89 6.98 2.33 5.17\nTransformer [19] - 2.2 5.6 2.6 5.7\nLSTM 360 2.6 6.0 2.2 5.2\nTransducer\nTransformer [7] 139 2.4 5.6 2.0 4.6\nContextNet(S) [10] 10.8 2.9 7.0 2.3 5.5\nContextNet(M) [10] 31.4 2.4 5.4 2.0 4.5\nContextNet(L) [10] 112.7 2.1 4.6 1.9 4.1\nConformer (Ours)\nConformer(S) 10.3 2.7 6.3 2.1 5.0\nConformer(M) 30.7 2.3 5.0 2.0 4.3\nConformer(L) 118.8 2.1 4.3 1.9 3.9\nerror rate among all the existing models. This clearly demon-\nstrates the effectiveness of combining Transformer and convo-\nlution in a single neural network.\n3.4. Ablation Studies\n3.4.1. Conformer Block vs. Transformer Block\nA Conformer block differs from a Transformer block in a\nnumber of ways, in particular, the inclusion of a convolution\nblock and having a pair of FFNs surrounding the block in the\nMacaron-style. Below we study these effects of these differ-\nences by mutating a Conformer block towards a Transformer\nblock, while keeping the total number of parameters unchanged.\nTable 3 shows the impact of each change to the Conformer\nblock. Among all differences, convolution sub-block is the most\nimportant feature, while having a Macaron-style FFN pair is\nalso more effective than a single FFN of the same number of\nparameters. Using swish activations led to faster convergence\nin the Conformer models.\nTable 3: Disentangling Conformer.Starting from a Conformer\nblock, we remove its features and move towards a vanilla Trans-\nformer block: (1) replacing SWISH with ReLU; (2) remov-\ning the convolution sub-block; (3) replacing the Macaron-style\nFFN pairs with a single FFN; (4) replacing self-attention with\nrelative positional embedding [20] with a vanilla self-attention\nlayer [6]. All ablation study results are evaluated without the\nexternal LM.\nModel\nArchitecture\ndev\nclean\ndev\nother\ntest\nclean\ntest\nother\nConformer Model 1.9 4.4 2.1 4.3\n– SWISH + ReLU 1.9 4.4 2.0 4.5\n– Convolution Block 2.1 4.8 2.1 4.9\n– Macaron FFN 2.1 5.1 2.1 5.0\n– Relative Pos. Emb. 2.3 5.8 2.4 5.6\n3.4.2. Combinations of Convolution and Transformer Modules\nWe study the effects of various different ways of combining the\nmulti-headed self-attention (MHSA) module with the convolu-\ntion module. First, we try replacing the depthwise convolution\nin the convolution module with a lightweight convolution [35],\nsee a signiﬁcant drop in the performance especially on the dev-\nother dataset. Second, we study placing the convolution mod-\nule before the MHSA module in our Conformer model and ﬁnd\nthat it degrades the results by 0.1 on dev-other. Another pos-\nsible way of the architecture is to split the input into parallel\nbranches of multi-headed self attention module and a convolu-\ntion module with their output concatenated as suggested in [17].\nWe found that this worsens the performance when compared to\nour proposed architecture.\nThese results in Table 4 suggest the advantage of placing\nthe convolution module after the self-attention module in the\nConformer block.\nTable 4: Ablation study of Conformer Attention Convolution\nBlocks. Varying the combination of the convolution block with\nthe multi-headed self attention: (1) Conformer architecture; (2)\nUsing Lightweight convolutions instead of depthwise convolu-\ntion in the convolution block in Conformer; (3) Convolution be-\nfore multi-headed self attention; (4) Convolution and MHSA in\nparallel with their output concatenated [17].\nModel Architecture dev\nclean\ndev\nother\nConformer 1.9 4.4\n– Depthwise conv + Lightweight convolution 2.0 4.8\nConvolution block before MHSA 1.9 4.5\nParallel MHSA and Convolution 2.0 4.9\n3.4.3. Macaron Feed Forward Modules\nInstead of a single feed-forward module (FFN) post the atten-\ntion blocks as in the Transformer models, the Conformer block\nhas a pair of macaron-like Feed forward modules sandwiching\nthe self-attention and convolution modules. Further, the Con-\nformer feed forward modules are used with half-step residuals.\nTable 5 shows the impact of changing the Conformer block to\nuse a single FFN or full-step residuals.\nTable 5: Ablation study of Macaron-net Feed Forward mod-\nules. Ablating the differences between the Conformer feed for-\nward module with that of a single FFN used in Transformer\nmodels: (1) Conformer; (2) Conformer with full-step residuals\nin Feed forward modules; (3) replacing the Macaron-style FFN\npair with a single FFN.\nModel\nArchitecture\ndev\nclean\ndev\nother\ntest\nclean\ntest\nother\nConformer 1.9 4.4 2.1 4.3\nSingle FFN 1.9 4.5 2.1 4.5\nFull step residuals 1.9 4.5 2.1 4.5\n3.4.4. Number of Attention Heads\nIn self-attention, each attention head learns to focus on different\nparts of the input, making it possible to improve predictions\nbeyond the simple weighted average. We perform experiments\nto study the effect of varying the number of attention heads from\n4 to 32 in our large model, using the same number of heads\nin all layers. We ﬁnd that increasing attention heads up to 16\nimproves the accuracy, especially over the devother datasets, as\nshown in Table 6.\nTable 6: Ablation study on the attention heads in multi-headed\nself attention.\nAttention\nHeads\nDim per\nHead\ndev\nclean\ndev\nother\ntest\nclean\ntest\nother\n4 128 1.9 4.6 2.0 4.5\n8 64 1.9 4.4 2.1 4.3\n16 32 2.0 4.3 2.2 4.4\n32 16 1.9 4.4 2.1 4.5\n3.4.5. Convolution Kernel Sizes\nTo study the effect of kernel sizes in the depthwise convolu-\ntion, we sweep the kernel size in {3,7,17,32,65}of the large\nmodel, using the same kernel size for all layers. We ﬁnd that the\nperformance improves with larger kernel sizes till kernel sizes\n17 and 32 but worsens in the case of kernel size 65, as show\nin Table 7. On comparing the second decimal in dev WER, we\nﬁnd kernel size 32 to perform better than rest.\nTable 7: Ablation study on depthwise convolution kernel sizes.\nKernel\nsize\ndev\nclean\ndev\nother\ntest\nclean\ntest\nother\n3 1.88 4.41 1.99 4.39\n7 1.88 4.30 2.02 4.44\n17 1.87 4.31 2.04 4.38\n32 1.83 4.30 2.03 4.29\n65 1.89 4.47 1.98 4.46\n4. Conclusion\nIn this work, we introduced Conformer, an architecture that\nintegrates components from CNNs and Transformers for end-\nto-end speech recognition. We studied the importance of each\ncomponent, and demonstrated that the inclusion of convolution\nmodules is critical to the performance of the Conformer model.\nThe model exhibits better accuracy with fewer parameters than\nprevious work on the LibriSpeech dataset, and achieves a new\nstate-of-the-art performance at 1.9%/3.9% for test/testother.\n5. References\n[1] C.-C. Chiu, T. N. Sainath, Y . Wu, R. Prabhavalkar, P. Nguyen,\nZ. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Goninaet al., “State-\nof-the-art speech recognition with sequence-to-sequence models,”\nin 2018 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP). IEEE, 2018, pp. 4774–4778.\n[2] K. Rao, H. Sak, and R. Prabhavalkar, “Exploring architectures,\ndata and units for streaming end-to-end speech recognition with\nrnn-transducer,” in2017 IEEE Automatic Speech Recognition and\nUnderstanding Workshop (ASRU). IEEE, 2017, pp. 193–199.\n[3] Y . He, T. N. Sainath, R. Prabhavalkar, I. McGraw, R. Alvarez,\nD. Zhao, D. Rybach, A. Kannan, Y . Wu, R. Pang, Q. Liang,\nD. Bhatia, Y . Shangguan, B. Li, G. Pundak, K. C. Sim, T. Bagby,\nS.-Y . Chang, K. Rao, and A. Gruenstein, “Streaming End-to-end\nSpeech Recognition For Mobile Devices,” inProc. ICASSP, 2019.\n[4] T. N. Sainath, Y . He, B. Li, A. Narayanan, R. Pang, A. Bruguier,\nS.-y. Chang, W. Li, R. Alvarez, Z. Chen, and et al., “A streaming\non-device end-to-end model surpassing server-side conventional\nmodel quality and latency,” inICASSP, 2020.\n[5] A. Graves, “Sequence transduction with recurrent neural net-\nworks,”arXiv preprint arXiv:1211.3711, 2012.\n[6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\n2017.\n[7] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, and\nS. Kumar, “Transformer transducer: A streamable speech recog-\nnition model with transformer encoders and rnn-t loss,” inICASSP\n2020-2020 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP). IEEE, 2020, pp. 7829–7833.\n[8] J. Li, V . Lavrukhin, B. Ginsburg, R. Leary, O. Kuchaiev, J. M. Co-\nhen, H. Nguyen, and R. T. Gadde, “Jasper: An end-to-end convo-\nlutional neural acoustic model,”arXiv preprint arXiv:1904.03288,\n2019.\n[9] S. Kriman, S. Beliaev, B. Ginsburg, J. Huang, O. Kuchaiev,\nV . Lavrukhin, R. Leary, J. Li, and Y . Zhang, “Quartznet: Deep\nautomatic speech recognition with 1d time-channel separable con-\nvolutions,”arXiv preprint arXiv:1910.10261, 2019.\n[10] W. Han, Z. Zhang, Y . Zhang, J. Yu, C.-C. Chiu, J. Qin, A. Gulati,\nR. Pang, and Y . Wu, “Contextnet: Improving convolutional neural\nnetworks for automatic speech recognition with global context,”\narXiv preprint arXiv:2005.03191, 2020.\n[11] T. N. Sainath, A.-r. Mohamed, B. Kingsbury, and B. Ramabhad-\nran, “Deep convolutional neural networks for lvcsr,” in2013 IEEE\ninternational conference on acoustics, speech and signal process-\ning. IEEE, 2013, pp. 8614–8618.\n[12] O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, L. Deng, G. Penn,\nand D. Yu, “Convolutional neural networks for speech recogni-\ntion,” IEEE/ACM Transactions on audio, speech, and language\nprocessing, vol. 22, no. 10, pp. 1533–1545, 2014.\n[13] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,”\nin Proceedings of the IEEE conference on computer vision and\npattern recognition, 2018, pp. 7132–7141.\n[14] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V . Le, “Attention\naugmented convolutional networks,” in Proceedings of the IEEE\nInternational Conference on Computer Vision , 2019, pp. 3286–\n3295.\n[15] B. Yang, L. Wang, D. Wong, L. S. Chao, and Z. Tu, “Convolu-\ntional self-attention networks,” arXiv preprint arXiv:1904.03107,\n2019.\n[16] A. W. Yu, D. Dohan, M.-T. Luong, R. Zhao, K. Chen, M. Norouzi,\nand Q. V . Le, “Qanet: Combining local convolution with\nglobal self-attention for reading comprehension,” arXiv preprint\narXiv:1804.09541, 2018.\n[17] Z. Wu, Z. Liu, J. Lin, Y . Lin, and S. Han, “Lite transformer with\nlong-short range attention,” arXiv preprint arXiv:2004.11886 ,\n2020.\n[18] Y . Lu, Z. Li, D. He, Z. Sun, B. Dong, T. Qin, L. Wang, and\nT.-Y . Liu, “Understanding and improving transformer from a\nmulti-particle dynamic system point of view,” arXiv preprint\narXiv:1906.02762, 2019.\n[19] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,\nM. Someki, N. E. Y . Soplin, R. Yamamoto, X. Wang et al., “A\ncomparative study on transformer vs rnn in speech applications,”\narXiv preprint arXiv:1909.06317, 2019.\n[20] Z. Dai, Z. Yang, Y . Yang, J. Carbonell, Q. V . Le, and R. Salakhut-\ndinov, “Transformer-xl: Attentive language models beyond a\nﬁxed-length context,” 2019.\n[21] Q. Wang, B. Li, T. Xiao, J. Zhu, C. Li, D. F. Wong, and L. S. Chao,\n“Learning deep transformer models for machine translation,” in\nProceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics. Association for Computational Lin-\nguistics, Jul. 2019, pp. 1810–1822.\n[22] T. Q. Nguyen and J. Salazar, “Transformers without tears:\nImproving the normalization of self-attention,” arXiv preprint\narXiv:1910.05895, 2019.\n[23] Y . N. Dauphin, A. Fan, M. Auli, and D. Grangier, “Language\nmodeling with gated convolutional networks,” in Proceedings of\nthe 34th International Conference on Machine Learning-Volume\n70. JMLR. org, 2017, pp. 933–941.\n[24] L. Dong, S. Xu, and B. Xu, “Speech-transformer: a no-recurrence\nsequence-to-sequence model for speech recognition,” in 2018\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). IEEE, 2018, pp. 5884–5888.\n[25] P. Ramachandran, B. Zoph, and Q. V . Le, “Searching for activa-\ntion functions,”arXiv preprint arXiv:1710.05941, 2017.\n[26] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-\nrispeech: an asr corpus based on public domain audio books,”\nin 2015 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP). IEEE, 2015, pp. 5206–5210.\n[27] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D.\nCubuk, and Q. V . Le, “Specaugment: A simple data augmen-\ntation method for automatic speech recognition,” arXiv preprint\narXiv:1904.08779, 2019.\n[28] D. S. Park, Y . Zhang, C.-C. Chiu, Y . Chen, B. Li, W. Chan, Q. V .\nLe, and Y . Wu, “Specaugment on large scale datasets,” arXiv\npreprint arXiv:1912.05533, 2019.\n[29] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, “Dropout: A simple way to prevent neural net-\nworks from overﬁtting,” Journal of Machine Learning Research,\nvol. 15, no. 56, pp. 1929–1958, 2014.\n[30] K.-C. Jim, C. L. Giles, and B. G. Horne, “An analysis of noise\nin recurrent neural networks: convergence and generalization,”\nIEEE Transactions on neural networks , vol. 7, no. 6, pp. 1424–\n1438, 1996.\n[31] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti-\nmization,”arXiv preprint arXiv:1412.6980, 2014.\n[32] J. Shen, P. Nguyen, Y . Wu, Z. Chen, and et al., “Lingvo: a modu-\nlar and scalable framework for sequence-to-sequence modeling,”\n2019.\n[33] Y . Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar,\nH. Huang, A. Tjandra, X. Zhang, F. Zhang et al., “Transformer-\nbased acoustic modeling for hybrid speech recognition,” arXiv\npreprint arXiv:1910.09799, 2019.\n[34] G. Synnaeve, Q. Xu, J. Kahn, T. Likhomanenko, E. Grave,\nV . Pratap, A. Sriram, V . Liptchinsky, and R. Collobert, “End-to-\nend asr: from supervised to semi-supervised learning with modern\narchitectures,” 2019.\n[35] F. Wu, A. Fan, A. Baevski, Y . N. Dauphin, and M. Auli, “Pay\nless attention with lightweight and dynamic convolutions,” arXiv\npreprint arXiv:1901.10430, 2019.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8171144723892212
    },
    {
      "name": "Computer science",
      "score": 0.7066457271575928
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5866729021072388
    },
    {
      "name": "Language model",
      "score": 0.4979739189147949
    },
    {
      "name": "Speech recognition",
      "score": 0.4774101972579956
    },
    {
      "name": "Artificial neural network",
      "score": 0.44883793592453003
    },
    {
      "name": "Artificial intelligence",
      "score": 0.433878630399704
    },
    {
      "name": "Sequence labeling",
      "score": 0.4274536669254303
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3211584687232971
    },
    {
      "name": "Engineering",
      "score": 0.09129726886749268
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ]
}