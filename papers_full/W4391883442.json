{
  "title": "Simultaneous instance pooling and bag representation selection approach for multiple-instance learning (MIL) using vision transformer",
  "url": "https://openalex.org/W4391883442",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2174094312",
      "name": "Muhammad Waqas",
      "affiliations": [
        "National University of Computer and Emerging Sciences",
        "The University of Texas MD Anderson Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A2618493346",
      "name": "Muhammad Atif Tahir",
      "affiliations": [
        "National University of Computer and Emerging Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5093942684",
      "name": "Muhammad Danish Author",
      "affiliations": [
        "United Arab Emirates University"
      ]
    },
    {
      "id": "https://openalex.org/A4221192752",
      "name": "Sumaya Al Maadeed",
      "affiliations": [
        "Qatar University"
      ]
    },
    {
      "id": "https://openalex.org/A2127992686",
      "name": "Ahmed Bouridane",
      "affiliations": [
        "University of Sharjah"
      ]
    },
    {
      "id": "https://openalex.org/A2099029284",
      "name": "Jia Wu",
      "affiliations": [
        "The University of Texas MD Anderson Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A2174094312",
      "name": "Muhammad Waqas",
      "affiliations": [
        "The University of Texas MD Anderson Cancer Center",
        "National University of Computer and Emerging Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2618493346",
      "name": "Muhammad Atif Tahir",
      "affiliations": [
        "National University of Computer and Emerging Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5093942684",
      "name": "Muhammad Danish Author",
      "affiliations": [
        "United Arab Emirates University"
      ]
    },
    {
      "id": "https://openalex.org/A4221192752",
      "name": "Sumaya Al Maadeed",
      "affiliations": [
        "Qatar University"
      ]
    },
    {
      "id": "https://openalex.org/A2127992686",
      "name": "Ahmed Bouridane",
      "affiliations": [
        "University of Sharjah"
      ]
    },
    {
      "id": "https://openalex.org/A2099029284",
      "name": "Jia Wu",
      "affiliations": [
        "The University of Texas MD Anderson Cancer Center"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2746791238",
    "https://openalex.org/W4283832242",
    "https://openalex.org/W3085685449",
    "https://openalex.org/W4285092723",
    "https://openalex.org/W4210462352",
    "https://openalex.org/W2399463307",
    "https://openalex.org/W2560886373",
    "https://openalex.org/W2158177997",
    "https://openalex.org/W2343974644",
    "https://openalex.org/W1606858007",
    "https://openalex.org/W2036424610",
    "https://openalex.org/W2181519955",
    "https://openalex.org/W2997512706",
    "https://openalex.org/W2133288557",
    "https://openalex.org/W3183684402",
    "https://openalex.org/W4292103624",
    "https://openalex.org/W4307569623",
    "https://openalex.org/W4390481277",
    "https://openalex.org/W2313501142",
    "https://openalex.org/W2110119381",
    "https://openalex.org/W2312404985",
    "https://openalex.org/W2135607475",
    "https://openalex.org/W2336727000",
    "https://openalex.org/W2074992691",
    "https://openalex.org/W2098239572",
    "https://openalex.org/W101414327",
    "https://openalex.org/W2016545623",
    "https://openalex.org/W3135638847",
    "https://openalex.org/W4362468840",
    "https://openalex.org/W2155226385",
    "https://openalex.org/W2740426294",
    "https://openalex.org/W3200870442",
    "https://openalex.org/W2531897166",
    "https://openalex.org/W3170720946",
    "https://openalex.org/W2963495494",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W3131350578",
    "https://openalex.org/W2111574404",
    "https://openalex.org/W2991430586",
    "https://openalex.org/W2981026798",
    "https://openalex.org/W2952744660",
    "https://openalex.org/W2048827870",
    "https://openalex.org/W1857789879",
    "https://openalex.org/W4241727697",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W4302367531",
    "https://openalex.org/W3014096773"
  ],
  "abstract": "Abstract In multiple-instance learning (MIL), the existing bag encoding and attention-based pooling approaches assume that the instances in the bag have no relationship among them. This assumption is unsuited, as the instances in the bags are rarely independent in diverse MIL applications. In contrast, the instance relationship assumption-based techniques incorporate the instance relationship information in the classification process. However, in MIL, the bag composition process is complicated, and it may be possible that instances in one bag are related and instances in another bag are not. In present MIL algorithms, this relationship assumption is not explicitly modeled. The learning algorithm is trained based on one of two relationship assumptions (whether instances in all bags have a relationship or not). Hence, it is essential to model the assumption of instance relationships in the bag classification process. This paper proposes a robust approach that generates vector representation for the bag for both assumptions and the representation selection process to determine whether to consider the instances related or unrelated in the bag classification process. This process helps to determine the essential bag representation vector for every individual bag. The proposed method utilizes attention pooling and vision transformer approaches to generate bag representation vectors. Later, the representation selection subnetwork determines the vector representation essential for bag classification in an end-to-end trainable manner. The generalization abilities of the proposed framework are demonstrated through extensive experiments on several benchmark datasets. The experiments demonstrate that the proposed approach outperforms other state-of-the-art MIL approaches in bag classification.",
  "full_text": "ORIGINAL ARTICLE\nSimultaneous instance pooling and bag representation selection\napproach for multiple-instance learning (MIL) using vision transformer\nMuhammad Waqas1,2 • Muhammad Atif Tahir1 • Muhammad Danish Author3 • Sumaya Al-Maadeed4 •\nAhmed Bouridane5 • Jia Wu2\nReceived: 10 April 2023 / Accepted: 14 January 2024 / Published online: 16 February 2024\n/C211 The Author(s) 2024\nAbstract\nIn multiple-instance learning (MIL), the existing bag encoding and attention-based pooling approaches assume that the\ninstances in the bag have no relationship among them. This assumption is unsuited, as the instances in the bags are rarely\nindependent in diverse MIL applications. In contrast, the instance relationship assumption-based techniques incorporate the\ninstance relationship information in the classiﬁcation process. However, in MIL, the bag composition process is com-\nplicated, and it may be possible that instances in one bag are related and instances in another bag are not. In present MIL\nalgorithms, this relationship assumption is not explicitly modeled. The learning algorithm is trained based on one of two\nrelationship assumptions (whether instances in all bags have a relationship or not). Hence, it is essential to model the\nassumption of instance relationships in the bag classiﬁcation process. This paper proposes a robust approach that generates\nvector representation for the bag for both assumptions and the representation selection process to determine whether to\nconsider the instances related or unrelated in the bag classiﬁcation process. This process helps to determine the essential\nbag representation vector for every individual bag. The proposed method utilizes attention pooling and vision transformer\napproaches to generate bag representation vectors. Later, the representation selection subnetwork determines the vector\nrepresentation essential for bag classiﬁcation in an end-to-end trainable manner. The generalization abilities of the pro-\nposed framework are demonstrated through extensive experiments on several benchmark datasets. The experiments\ndemonstrate that the proposed approach outperforms other state-of-the-art MIL approaches in bag classiﬁcation.\nKeywords Multiple-instance learning (MIL) /C1Vision transformers /C1Attention-based pooling /C1Bag representation selection\n1 Introduction\nThe multiple-instance learning (MIL) approach is a case of\nweakly supervised learning [ 1]. This learning approach is\nused where labeling cost is a major restriction for anno-\ntating every data instance [ 2]. In MIL, the data are repre-\nsented as bags with multiple instances, with only one label\nfor each bag. Unlike supervised learning, the labels of the\ninstances are not available in the training process. The\nmodel in MIL is trained using weak bag-wise labels rather\nthan instance-wise labels. The case of supervised learning\nand MIL is shown in Fig. 1a and b, respectively. In MIL,\nthe primary objective is to develop a model that predicts\nthe label of the test bag using training bags and corre-\nsponding labels. The application of MIL is common in\nimage segmentation [ 3], medical image classiﬁcation [ 4],\nand others [ 5–7].\nThe MIL approaches can be categorized based on the\nclassiﬁcation granularity: the bag-space level classiﬁcation\napproaches [ 8], which compute the distance between the\nbags or apply maximum margin approach to train the\nclassiﬁers; embedding-space classiﬁcation [ 9, 10], where\nan entire bag is transformed into a ﬁxed-size vector rep-\nresentation and applies a simple single instance classiﬁca-\ntion algorithms; instance-space classiﬁcation [ 11], where\nthe score for each instance is computed, and the bag label is\nobtained based on the instance scores. The studies in\n[12, 13] show that the ﬁrst two categories are robust in bag\nclassiﬁcation compared to the last category. However, the\nbag-space and embedding-space classiﬁcation approaches\ncannot identify the key instances (the instances that trigger\nthe bag label) [ 13]. Identifying key instances in the bag is\nExtended author information available on the last page of the article\n123\nNeural Computing and Applications (2024) 36:6659–6680\nhttps://doi.org/10.1007/s00521-024-09417-3(0123456789().,-volV)(0123456789().,- volV)\nessential as these instances play a vital role in the bag\nclassiﬁcation process and model interpretability.\nFurthermore, in the context of MIL, the bags consist of\nmultiple instances, and the goal is to classify the bags\nbased on their contents. However, the difﬁculty arises when\nthe bags in the training set and testing come from a dif-\nferent distribution [ 14]. Previous MIL studies assume that\nthe instances of the bag in the training and testing data are\nsampled from the same distribution (either related or\nindependent). However, this assumption is often violated in\nreal-world tasks [ 9, 13, 15–19].\nFor example, the case of MIL image classiﬁcation is\nillustrated in Fig. 2, where the image is considered a bag,\nand the extracted patches are considered instances. The\ninstances related to the Fox concept are positive instances;\ninstances related to other objects like cars and buildings are\nnegative instances. Figure 2 illustrates the dissimilarity\nbetween different training bag distributions, where the\ntraining set contains images of the animal of interest in\nnatural settings. However, some images in the training and\ntesting set may be captured in a diverse environment or\ncontain other similar animals.\nIn such cases, the instances in the bag may or may not\nhave a relationship, and it can be challenging to ascertain\nthe presence or absence of any underlying instance rela-\ntionships. Therefore, determining the relationship between\nthe instances in the bag becomes important to model per-\nformance, and the presumption of a speciﬁc instance\nrelationship could potentially hinder the performance of the\nclassiﬁcation algorithm. In order to obtain better general-\nization, the classiﬁer must distinguish between instances\nrelated to the fox concept, different animal species, other\nobjects inside the bag, and their relationship. Thus, deter-\nmining the relationship or independence of instances in the\nbag may enhance the classiﬁcation process.\nMIL algorithms [ 9, 13, 15–19] are developed based on\none of two assumptions: whether instances have a rela-\ntionship or not. However, it is not theoretically guaranteed\nthat instances in all bags follow the same assumption.\nAdditionally, existing MIL algorithms do not explicitly\naccount for the bag-wise relationship assumption. As a\nresult, their performance could be improved since weak\nbag-level labels provide only limited supervision.\nFor example, to identify the essential instances in the\nbags, a weighted average bag pooling operation is proposed\nusing attention-based deep neural networks (AbDMIL)\n[13], where end-to-end trainable architectures are used to\ngenerate attention-based weights for each instance. The\nconcept of attention pooling is further investigated in Shi\net al. [ 15] by incorporating the attention loss mechanism.\nHowever, the existing attention-based pooling approaches\n[13, 15] and bag encoding strategies [ 9, 17] are based on\nthe assumption that instances in the bag are independent\nand that no relationship exists between the instances of the\nbag. In this assumption, the relationship between the\nFig. 1 Supervised learning (SL) vs Multiple-Instance learning (MIL),\na shows the example of instance classiﬁcation setup followed in SL,\nwhere every data instance is labeled. The MIL bag classiﬁcation\napproach is shown in b where the instances are grouped in bags, and\nthe labels are provided at bag level\n6660 Neural Computing and Applications (2024) 36:6659–6680\n123\ninstances of the bags is ignored, which may result in\nneglecting the information in the bag [ 20, 21].\nOn the other hand, the assumption of relationship\nbetween instances is natural and may present a superior\ndescription of the data [ 22]. Considering the different\nimage patches as interrelated is more meaningful than\nassuming the opposite, speciﬁcally in multiple-instance\nimage classiﬁcation scenarios. The assumption of instance\nrelation is also considered for MIL problems by Zhou\net al. [ 16]. However, these techniques mainly focus on the\nstructural properties of the bag, and the instance relation-\nships are modeled in terms of graph kernel learning.\nAdditionally, this process is not end-to-end trainable.\nIn this paper, we propose the idea of generating bag\nrepresentation vectors based on both assumptions and\nintroduce the bag representation selection process to select\na suitable representation for each bag, which addresses the\nlimitation of the instance relationship assumption in\nexisting MIL algorithms.\nIn the proposed algorithm, we incorporate bag-wise\ninstance relationship assumption in the classiﬁcation pro-\ncess by considering bags with varying instances as a batch,\nand bag representation vectors are generated for each bag\nbased on the assumption of interaction and independence.\nWe obtain information about the relationship between\ninstances in a bag by using a vision transformer architec-\nture to model the dependencies among them. Furthermore,\nthe representation vectors for independent assumptions are\nderived from the mean, max, average, and attention pool-\ning operations [13], which do not consider the relationship\nof instances.\nIn addition, we propose a differentiable representation\nselection network to decide whether to consider instance\nrelationships in the classiﬁcation process for each bag. We\nrefer to the proposed approach as a vision transformer-\nbased instance weighting and representation selection\nsubnetwork (ViT-IWRS).\nThe major contributions of the paper are:\n• The vision transformer (ViT)-based approach is pro-\nposed to model the relationship between the instances\nof the bag. This process helps to generate a bag\nrepresentation vector by considering the instance\nrelationship.\n• To select informative bag representation from sets of\ngenerated bag representation vectors, a differentiable\nrepresentation selection subnetwork (RSN) is proposed.\n• The weight-sharing approach is presented for simulta-\nneous instance weight learning and bag classiﬁcation\nfor ViT. This method helps to strengthen the relation-\nship between the loss and instance weighting processes.\nTo demonstrate the generalization ability of the proposed\napproach, the experiments are performed on multiple types\nof data from different MIL application domains. For binary\nclassiﬁcation, ﬁve benchmark datasets are used: Musk1 and\nMusk2 [ 23] datasets for molecular activity predictions;\nFox, Elephant, and Tiger datasets for image classiﬁcation.\nFor multi-class classiﬁcation two datasets are used: multi-\nple-instance MNIST (MIL-MNIST) [ 13] dataset for hand-\nwritten digit classiﬁcation; MIL-based CIFAR-10 datasets\n[15] for object recognition. Additionally, the experiments\nare also conducted for real-world Colon Cancer detection\nhistopathology dataset [ 24].\nThe remainder of the paper is organized into the fol-\nlowing sections: Sect. 2 presents the literature review.\nSection 3 explains the proposed methodology for (ViT-\nIWRS). The experimental setup is given in Sect. 4. The\nFig. 2 The example of distribution change where the training\nexamples are from distribution. a Shows the positive training\nexample captured in different settings, and b presents the negative\ntesting example. Green boxes mark the positive instances, while the\nnegative instances are shown in red boxes\nNeural Computing and Applications (2024) 36:6659–6680 6661\n123\nobtained results are discussed in Sect. 5, which follows the\nconclusive Sect. 6.\n2 Literature review\nThis section presents a summary of MIL algorithms in the\nliterature. The MIL algorithms are divided into two cate-\ngories: Classical MIL techniques and Neural network-\nbased techniques. These categories are discussed in detail\nin the following subsections.\n2.1 Classical MIL techniques\nClassical MIL techniques can also be categorized into bag-\nspace and instance-space algorithms. The instance-space\nalgorithms classify each instance in the bag individually\nand aggregate the instance labels to determine the bag label\n[11, 25]. Thus, these algorithms identify the key instances\nin the bag (instances that triggered the bag label). However,\nthe unavailability of instance-level labels complicates the\nlearning problem.\nTo tackle the complexity of the learning process,\nAndrews et al. [ 26] proposed two support vector machine\n(SVM)-based solutions to solve MIL problems: Mi-SVM\nfor instance-space classiﬁcation and MI-SVM for bag-\nspace classiﬁcation. Diversity Density (DD) and nearest\nneighbor approach for real-valued target in MIL are pro-\nposed in Amar et al. [ 27], and a similar approach com-\nbining diversity density and expectation-maximization\n(EM) is proposed in Zhang and Goldman [ 28]. These\nalgorithms address MIL problems by assigning bag labels\nto the instances and training an instance-space model.\nHowever, these methods often fail when a complicated\nrelationship between instances determines the bag label.\nRandom subspace clustering and instance selection\napproach (RSIS) is proposed in Carbonneau et al. [ 29],\nwhere key instances are selected from positive bags. The\nselected instances are then used in the instance-space\nensemble learning approach. However, the instance selec-\ntion procedure in RSIS results in class imbalance problems\nand negatively affects performance. The constructive\nclustering ensemble (CCE) [ 30] approach performs\ninstance clustering to obtain a binary vector representation\nfor the bag. The bit value in the binary vector determines\nthe bag link to the clusters. However, the performance of\nCCE is comparatively low.\nBag-space techniques do not require access to instance\nlabels, although they are not as explainable as instance-\nlevel approaches. For example, the graph-based kernel\napproach (mi-Graph) [ 16] transforms the bag into a graph\nrepresentation and employs a distance function to compare\nbags. Embedding space methods for bag classiﬁcation\nadopt a ﬁxed-size embedding vector used for bag classiﬁ-\ncation. For example, Zhou et al. proposed two bag\nencoding techniques for MIL using Fisher vector encoding\n(miFV) and locally aggregated descriptors (miVLAD) [ 9].\nThe miFV and miVLAD keep essential bag-level infor-\nmation in generated bag encodings with the help of dic-\ntionary learning. However, the bag-space classiﬁcation\nalgorithms lack any mechanism to learn appropriate feature\nrepresentation. Other conventional MIL algorithms include\nsemi-supervised SVMs for MIL (MissSVM) [ 31], MIL\nwith randomized trees [ 32], and many others [ 7].\n2.2 Neural network-based MIL techniques\nThis section introduces the related work based on neural\nnetwork (NN) architectures for MIL. Traditionally, neural\nnetworks (NN) for MIL perform instance-level classiﬁca-\ntion [33]. The convolution neural networks (CNN) are also\nused in MIL for feature extraction through multiple con-\nvolution layers [ 34–36]. The best candidate search and\ninstance positioning with the global max-pooling operation\napproach are explored in Hoffman [ 37]. However, the\nmax-pooling is not robust enough to ﬁnd the inﬂuential\ninstance, especially in the bag classiﬁcation approach [ 15].\nTo overcome the limitation of max-pooling, the concept\nof Noisy or [ 38], LSE, and generalized mean are intro-\nduced in Shi et al. [ 39]. However, these operators are non-\ntrainable. In contrast, the use of an adaptive pooling\napproach and a fully connected network is proposed in Liu\net al. [ 40]. MIL-based pooling approaches, e.g., mean and\nmax-pooling operations, are proposed in Wang et al. [ 41],\nwhich is designed to extract features and perform back-\npropagation with the support of maximum response of\ninstance feature extraction layers.\nContrary to the above discussed techniques, the atten-\ntion-based pooling approach is considered as a kind of\nweighted average of instances in which the weights of the\ninstances are obtained by trainable attention layers [ 42].\nThis technique has been applied in several real-world\nproblems, such as image classiﬁcation and captioning [ 43].\nHowever, limited attention-based studies are available in\nthe literature related to MIL. Attention-based instance\npooling approach in Ilse [ 13] proposed two-layer (AbD-\nMIL) and three-layer (Gated-AbDMIL) networks to attain\ninstance weights. This approach focuses on binary classi-\nﬁcation problems and uses an additional layer for bag\nclassiﬁcation. The loss-based attention (LBA) approach\n[15] proposed a weight-sharing approach among fully\nconnected layers and attention layers. However, the atten-\ntion pooling techniques [ 13, 15] assume no dependence\namong instances in the bag. Unlike previous attention-\nbased techniques, the proposed ViT-IWRS generates sev-\neral bag representations based on both assumptions and\n6662 Neural Computing and Applications (2024) 36:6659–6680\n123\nselects the suitable bag representation for the classiﬁcation\nprocess.\n3 Proposed methodology\nThe proposed ViT-IWRS consists of four steps. In the ﬁrst\nstep, we propose a vision transformer-based approach to\nidentify the dependencies between the bag instances. This\nprocess transforms input instances into latent representa-\ntions using an embedding network and provides the latent\ntransformation as input to a transformer encoder. The\nencoding process involves a multi-head-self-attention pro-\ncess that captures the global dependencies between the\ninstances in the bag. With the output of the encoding\nprocess, we compute the weights for the bag instances in\nthe second step. The weighting process ensures the\nassignment of higher weights to the essential instances in\nthe bag. The process of instance embedding and trans-\nformer encoding is shown in Fig. 3a, while the process of\ninstance weighting is illustrated in Fig. 3b.\nThe third step of the proposed approach involves gen-\nerating bag representation vectors from instance weights\nfor both instance relationship assumptions using encoder\noutputs and latent representations. Weights assigned to\ninstances determine the composition of the representation\nvector and ensure that informative instances are repre-\nsented more prominently. Figure 3c illustrates the vector\nrepresentation generation process. As a ﬁnal step, the\nrepresentation selection subnetwork (RSN) selects the ﬁnal\nbag representation vector from a set of generated bag\nrepresentation vectors. The RSN and bag classiﬁcation\nprocess function is shown in Fig. 3d. In the following\nsubsection, we present problem formulation, a brief dis-\ncussion of the vision transformer, and each step of the\nproposed approach in detail.\n3.1 Problem formulation\nIn binary MIL classiﬁcation problem, for a given bag Bi ¼\nxi;1; xi;2; xi;3; ... ; xi;mi\n/C8/C9\nof mi total instances with d\ndimensions, where xi;j represents jth instance of ith bag.\nThe objective is to predict a bag target label Yi 2f 1; 0g.\nThe prediction of bag label depends on the corresponding\nset of instance-level labels yi;1; yi;2; ... ; yi;m\n/C8/C9\n, where\nyi;j 2f 1; 0g. The instance-level labels remain unknown\nwhile the model training and Yi for binary classiﬁcation is\nobtained as:\nYi ¼ 0 iff Pm\nj¼1 yi;j ¼ 0\n1 otherwise :\n/C26\nð1Þ\nNeural Computing and Applications (2024) 36:6659–6680 6663\n123\nIn this paper, we concentrate on bag-level classiﬁcation for\nbinary and multi-class MIL applications. Therefore, a\nrepresentation vector is generated for the bag of instances\nand the model classiﬁes the bag representation vector\ninstead of individual instances.\nGiven a bag representation vector and corresponding\nbag label, the model generates a K/C0 dimensional vector of\nclass scores s\nK , where K represents the number of classes.\nIn this case, the bag label is determined by:\nYi ¼ argmaxK/C0 1\nk¼0 f ðsÞk\n/C16/C17\n; ð2Þ\nwhere f ðsÞi ¼ exp siðÞPK/C0 1\nj¼0 exp s jðÞ\nis Softmax function that squashes\nthe score vector sk in the range between (0, 1) and all the\nresulting elements add up to 1 and are interpreted as class\nprobabilities.\n3.2 Vision transformer\nThe Vision Transformer (ViT) is inspired by the concept of\ntransformers in language processing models and can be\nseen as an alternative to the convolutional neural network\n(CNN) [ 44]. Vision Transformers (ViT) takes 1D patch\nembeddings as input. Therefore, the image is transformed\ninto a sequence of two-dimensional ﬂattened patches, and a\ntrainable linear projection converts the generated patches to\none-dimensional vectors. The projected image patches are\ncalled patch embeddings. A learnable embedding called\nclass token is also prepended to patch embeddings. More-\nover, the positional embeddings which are added to pre-\nserve the positional information of patches in the image.\nTransform encoder [ 45] combines multi-head self-at-\ntention (MHSA) blocks with multi-layer perceptrons\n(MLP). Before each block, layer normalization (LN) is\napplied, and residual connections are used after each block.\nThere are two layers of MLP and GELU nonlinearity in the\ntransformer encoder. The details of the transformer encoder\nand MHSA process are shown in Fig. 4. Vision transfer\nemploys one or more stacked transformer encoder blocks\nin the encoding generation process. The generated class\ntoken from the last transformer encoder block is then\nemployed for classiﬁcation using a classiﬁcation head. The\nclassiﬁcation head consists of MLP with one hidden layer.\n3.3 Vision transformer for bag encoding in MIL\nIn MIL, the objective is classify a given bag Bi ¼\nxi;1; xi;2; xi;3; ... ; xi;mi\n/C8/C9\nof mi instances, where xi;j 2 R1/C2d.\nIn this case, the ViT can be employed to generate robust\nbag embeddings and determine dependencies among the\nbag instances. The self-attention in the transformer\nencoding process can allow instances in the bag to interact\nwith each other. It can provide essential details about the\nrelationship of instances in the bag, which can be used to\ngenerate a robust representation vector for the bag.\nAt ﬁrst, each instance x\ni;jin the bag Bi is transformed\ninto a latent representation hi;j using an embedding net-\nwork. The process of instance embedding corresponds to\nthe patch embedding process in standard ViT settings.\nHowever, the embedding network can consist of multi-\nlayer perceptron (MLP) or convolution layers, depending\nupon the nature of the data. We used a similar design for\nthe embedding network as previously used by Shi\net al. [ 15] and Ilse et al. [ 13]. The details about the\nembedding network design are discussed in Sect. 5.9.1.W e\nrefer to the generated latent instance representation h\ni;j as\ninstance embeddings. Similarly, the embeddings for all the\ninstances in the bag Bi are grouped and referred to as bag\nembeddings H½0/C138\ni ¼ hi;1; hi;2; ... :hi;mi\n/C8/C9\n. Afterward, the\ngenerated bag embeddings are prepended with a learnable\nclass token hi;0 and denoted by\nH0½0/C138\ni ¼ hi;0; hi;1; hi;2; ... :hi;mi\n/C8/C9\n.\nThe class token aggregates global information from the\nentire bag, and it allows the model to make high-level\ndecisions based on the overall content rather than relying\nsolely on local instance information. The class token is\ntypically fed into a classiﬁcation head for image classiﬁ-\ncation tasks. In the case of MIL, the class token diversiﬁes\nthe set of generated vector representations for the bag. The\nclassiﬁcation token is learnable embedding and can capture\nglobal dependencies and relationships in the bag. Thus, the\nclassiﬁcation token can be used as an additional bag rep-\nresentation vector. It can be used as an input for the rep-\nresentation selection network.\nThe generated bag embeddings serve as input to the\nencoder. At the start of the training process, the class token\nis randomly initialized and learned during the training\nprocess. The length of the class token is the same as the\nlength of the instance embedding in the bags. The class\ntoken is used in the MHSA process in the same way as\nother instance embeddings of the bag and accumulates\ninformation from other instance embeddings [ 44]. Here,\nthe positional embeddings are not used as bag representa-\ntion follows a permutation invariant structure. The ViT\nencodes the given bag embeddings H\n0½0/C138\ni as:\nbFig. 3 The Proposed ViT-IWRS framework. The top row in this\nblock represents 3 different input bags (red, green, and blue) with a\ndifferent number of instances (3, 4 and 5). Block (a) illustrates\ninstance embedding and the transformer encoding process. The\ninstance selection mechanism is shown in (b). The bag representation\ngeneration block is presented in (c). The representation weighting and\nbag classiﬁcation process is shown in (d)\n6664 Neural Computing and Applications (2024) 36:6659–6680\n123\nH0½0/C138\ni ¼ hi;0; hi;1; hi;2; ... hi;mi\n/C8/C9\n;\nH0½‘/C0 1/C138\ni ¼ MHSA LN Hl/C0 1\ni\n/C0/C1/C0/C1\nþ H½‘/C0 1/C138\ni ;‘ ¼ 1... L\nH0½‘/C138\ni ¼ MLP LN H0½‘/C0 1/C138\ni\n/C16/C17/C16/C17\nþ H0½‘/C0 1/C138\ni ;‘ ¼ 1... L\n8\n>>\n>\n<\n>>>:\nð3Þ\nWhere ‘ represents the index of the transformer encoder\nblock, and L denotes the depth or the total number of\nencoder blocks. Discussion related to the depth of ViT and\nthe number of heads in MHSA is presented in Sect. 5.9.4.\nAdditionally, the generated output of the encoding process\nis denoted by H\n0½L/C138\ni ¼ h½L/C138\ni;0; h½L/C138\ni;1; h½L/C138\ni;2... ::h½L/C138\ni;mi\nhi\nwhere h½L/C138\ni;j\nand h½L/C138\ni;0 denote the output of the last transformer encoder\nblock for the corresponding input instance embedding hi;j\nand hi;0, respectively.\nLater, H0½L/C138\ni is used to generate bag representation vec-\ntors with the assumption of related instances, and H½0/C138\ni is\nused to generate bag representation vectors without\ninstance relationship assumption, respectively. The process\nof instance embedding and bag encoding using ViT is\nillustrated in Fig. 3a.\n3.4 Instance weight computation\nIn this step, the weight for each instance in the bag is\ncomputed using the attention approach [ 13, 15]. This pro-\ncess highlights essential instances from the bag and assigns\na higher weight to the informative instance. Later, the\ninstances in the bag are pooled using a weighted average\noperation to obtain representation vectors for the bag. In\nthis study, the weights of the transformer classiﬁcation\nhead are shared to learn instance weight and bag repre-\nsentation vector classiﬁcation simultaneously. This process\nhelps to enhance the connection between the loss and\ninstance weighting process.\nLet W 2 R\nd/C2K be a weight matrix and b 2 RK be a bias\nvector of classiﬁcation head f( : ). Given the output of the\nlast transformer encoder block H0½L/C138\ni the weights for the\ninstance in the bag Bi are computed as:\n8\n1 /C20j /C20mi\nai;j ¼\nPK/C0 1\nc/C0 0 exp h½L/C138\ni;j wc þ bc\n/C16/C17\nPmi\nt¼1\nPK/C0 1\nc¼0 exp h½L/C138\ni;t wc þ bc\n/C16/C17 ; ð4Þ\nwhere wc 2 Rd is cth column vector of W and bc /C26 b is\ncorresponding bias. The obtained weights are then used to\ngenerate bag representation vectors in the next step. The\nprocess of weight computation is illustrated in Fig. 3b.\nFig. 4 The vision transformer block is shown in ( a), while the process of multi-head self-attention [ 45] is illustrated in ( b)\nNeural Computing and Applications (2024) 36:6659–6680 6665\n123\n3.5 Computation of bag representation vectors\nAfter obtaining the weights of the instance in the bag, the\nnext step is to compute bag representation vectors. This\nprocess transforms the bag with a variable number of\ninstances to a manageable vector representation and\ntransforms the MIL problem into a classical supervised\nlearning problem. To classify the bags, one of the obtained\nvectors is selected using the representation selection\nsubnetwork.\nGiven H\n0½L/C138\ni ¼ h½L/C138\ni;0; h½L/C138\ni;1; h½L/C138\ni;2... ::h½L/C138\ni;mi\nhi\nand weights of\ninstances ai the representation vector for the bag Bi are\ncomputed as:\nwi ¼\nXmi\nj¼1\nai;j /C1h½L/C138\ni;j : ð5Þ\nThe computed bag representations wi involves the output of\nthe transformer encoder, and h½L/C138\ni;0 is learned class token.\nThe learning process of these vectors considers all the\ninstances in the bag. Thus, these vectors incorporate the\ninformation related to the relationship of instances in the\nbag B\ni.\nAdditionally, bag representation vectors without\nassuming instance relationship are obtained based on the\nbag embeddings H½0/C138\ni ¼ hi;1; hi;2; ... hi;mi\n/C8/C9\nas:\nxi ¼\nXmi\nj¼1\nai;j /C1h½0/C138\ni;j ;\nmaxi ¼ max\n1 /C20j /C20mi\nH½0/C138\ni\n/C16/C17\n;\nli ¼ 1\nmi\nXmi\nj¼1\nh½0/C138\ni;j ;\n8\n>>>>\n>\n>\n>>>\n>\n<\n>>>>\n>\n>>>\n>\n>:\nð6Þ\nwhere the x\ni; li; maxi represent the attention weighted\naverage [ 13], mean, and max representation vectors,\nrespectively. The computation of these representation\nvectors does not incorporate any dependencies or rela-\ntionships between the instances of the bag. Therefore,\nx\ni; li; maxi are based on the assumption of unrelated\ninstances of Bi. Figure 3c shows the representation vector\ngeneration process.\n3.6 Representation selection subnetwork (RSN)\nThe instance in the bag can either be related or unrelated.\nTherefore, the representation vector generated by a correct\ndistribution assumption will provide critical information to\nthe classiﬁer. In this case, RSN aims to select one of the\nrepresentation vectors, which is most informative for the\nbag classiﬁcation. RSN performs hard selection using\nGumbel SoftMax in an end-to-end approach [ 46]. This\nprocess is analogous to computing the softmax over a\nstochastically sampled set of points. The Gumbel-Max\nTrick separates the deterministic and stochastic parts of the\nsampling process using the reparameterization trick\n[46, 47]. It computes the log probabilities of given scores in\nthe distribution and adds some noise to them from the\nGumbel distribution. Finally, the argmax function is\napplied to ﬁnd the class with the maximum value for each\nrepresentation vector and generate a one-hot vector for use\nby the rest of the neural network.\nAt First, the previously computed n representation\nvectors for the bag B\ni are combined to form a representa-\ntion matrix R ¼ h½L/C0 1/C138\ni;cls ; wi; li; maxi; xi\nhi\n2 Rn/C2d, where d\ndenotes the length of representation vectors. Afterward, the\nrepresentation matrix R is given as input to RSN ( R),\nwhich outputs the score vector r 2 Rn/C21 and representation\nselection code u ¼ u1; u2; ... ; unðÞ are computed as:\nui ¼\nexp log riðÞ þ giðÞ\ns\n/C16/C17\nPn\nj¼1 exp\nlog riðÞ þ gjðÞ\ns\n/C18/C19 ; ð7Þ\nwhere gi /C24 Gumbel ð0; 1Þ¼/C0 logð/C0 logðqÞÞ; q /C24 Uni-\nform (0, 1). Additionally, s 2ð 0; 1Þ is the temperature\nparameter, which determines the degree of approximation\nfor u in relation to a one-hot vector. A smaller value of s\nresults in a harder u, whereas a higher s leads to a smoother\nu. The obtained u is further used to generate a one-hot\nvector as:\ni\nH ¼ arg max\ni\nuifg ;\ne/C3 ¼ OneHot iH/C0/C1\n;\nð8Þ\nwhere iH denotes sampled index and e/C3 represents the one-\nhot vector with the iH the element being 1. Afterward, the\nbag representation vector for the bag Bi is selected as:\nvi ¼ RT e/C3 : ð9Þ\nThe selected bag representation vector vi is then used to\nclassify the bag label by classiﬁcation head f(:)a s\nYi ¼ f viðÞ : ð10Þ\nFurthermore, the details related to the number of layers in\nRSN are discussed in Sect. 5.9.2.\n3.7 Loss function\nThis section presents the loss function for the training of\nViT-IWRS. The proposed loss scheme is derived from the\nconcept of cross-entropy (CE) loss [15]. CE is a measure of\ndissimilarity between the true and predicted label.\n6666 Neural Computing and Applications (2024) 36:6659–6680\n123\nGiven a representation vector v for the training bag Bi,\nand corresponding label Yi 2f 0; 1; /C1/C1/C1; K /C0 1g, where K\ndenotes the number of classes. Let f( : ) represent a neural\nnetwork and zi ¼ f ðvÞ2 RK be the class score vector for\nBi. The estimated class probability of Bi belonging to the k-\nth class can be computed by using softmax function:\nqk\ni ¼ exp zk\ni\n/C0/C1\nPK/C0 1\nc¼0 exp zc\niðÞ\n; ð11Þ\nwhere exp ð:Þ represents the exponential function. For\nmulti-class classiﬁcation, the loss function can be written\nas:\nCE ¼/C0\nXK/C0 1\nc¼0\npc\ni log qc\ni ; ð12Þ\nwhere pc\ni 2f 0; 1gK denote the true probability of the bag\nBi belonging to the cth class, and qc\ni is the estimated\nprobability.\nThe target vector p is one-hot encodings in multi-class\nclassiﬁcation. In this case, if Bi belongs to the k-th class,\nthere is only one element pk\ni in the target vector which is\nnot zero. So, only the positive class contributes to the loss\ncomputation process. Discarding the elements of the\nsummation which are zero due to target labels in equation\n(12), the loss function can be written as:\nCE ¼/C0 log exp zk\ni\n/C0/C1\nPk/C0 1\nc¼0 exp zc\niðÞ\n !\n: ð13Þ\nSuppose that the training bag Bi belongs to the kth class. In\nthis case, given the output of ViT H0½L/C138\ni ¼\nh½L/C138\ni;0; h½L/C138\ni;1; h½L/C138\ni;2... ::h½L/C138\ni;mi\nhi\n, the weights of instances ai, and\ncorresponding bag representation vector v, the loss for the\nbag Bi is computed as:\nL1 ¼/C0 log exp vwk þ bk/C0/C1\nPK/C0 1\nc¼0 exp vwc þ bcðÞ\n !\n; ð14Þ\nL2 ¼\nXmi\nj¼1\n/C0 log\nexp h½L/C138\ni;j wk þ bk\n/C16/C17\nPK/C0 1\nc¼0 exp h½L/C138\ni;c wc þ bc\n/C16/C17\n0\n@\n1\nAai;j\n0\n@\n1\nA;\nð15Þ\nLoss ¼ L1 þ kL2: ð16Þ\nwhere wc 2 Rd is cth column vector of weight matrix W\nand bc is corresponding bias for classiﬁcation head f(:) .\nThe ﬁrst term of the loss function focuses on bag clas-\nsiﬁcation loss, while the second one captures the attention\nloss, and k is a non-negative hyperparameter to balance\nbetween bag and attention loss. The discussion related to\nthe impact of k is given in Sect. 5.9.3.\nThe term L1 ! 0 if any one instance in a bag Bi belongs\nto the kth class. However, in this case, it is not theoretically\nguaranteed that only one instance belongs to the kth class in\nthe bag [ 15]. Therefore, it results in a high false negative\nrate for the instances in the positive bags. To address this\nissue, the L2 term is added to the objective function. This\nterm ensures that more than one instance with higher\nweights contributes to the label. Furthermore, the L2 term\nis inspired by the fact that the weight of instance x\ni;j\nbecome approximately zero when yi;j 6¼ Yi.\n4 Experimental setup\nThis Section introduces the datasets used for experiments\nalong with relevant evaluation measures. Additionally, a\ncomparative analysis of existing methods is also provided.\n4.1 Details of datasets and evaluation measure\nThe performance of ViT-IWRS is evaluated using different\ndatasets for binary and multi-class classiﬁcation problems.\nThese datasets have been used to assess the performance of\nMIL algorithms in the literature and cover a range of MIL\napplication domains, such as molecular activity prediction,\nimage classiﬁcation, object detection, and medical image\nclassiﬁcation. The details of these datasets are given below.\n4.1.1 Benchmark MIL datasets\nThe experiments are conducted on ﬁve MIL datasets rela-\nted to binary classiﬁcation problems: Musk1, Musk2,\nElephant, Tiger, and Fox. These datasets are related to\nbinary classiﬁcation problems. The ﬁrst two datasets\n(Musk1 and Musk2) cover the application of MIL for\nmolecular drug activity predictions [23]. These datasets are\ncomposed of molecular conformations of multiple shapes.\nThe bag is formed based on the shape similarity, and the\ndrug’s effect is observed if one or more conformations are\nattached to the targeted bindings. The later three datasets:\nElephant, Tiger, and Fox, are related to image classiﬁcation\n[26]; features of image segments constitute the bags in\nthese datasets. The positive bags hold one or more\ninstances related to the animal of interest while the nega-\ntive bags contain other animals. The details of these data-\nsets are shown in Table 1.\n4.1.2 MIL-based MNIST dataset\nIn addition to the existing benchmark MIL dataset, an\nadditional dataset for multi-class classiﬁcation is created\nfrom well-known MNIST digits (MIL-MINST) for digit\nNeural Computing and Applications (2024) 36:6659–6680 6667\n123\nclassiﬁcation [ 48]. The dataset consists of gray-scale digit\nimages of size 28 /C2 28, and the images are randomly\nselected to form a bag where each digit represents an\ninstance. In this problem, we have used a labeling approach\nsimilar to [ 15], where bags with the target digits {’3’, ’5’,\n’9’} are labeled {’1’, ’2’, ’3’} accordingly and if a bag does\nnot include any of the target digits, it is labeled as ’0’. in\nthe training process, the model is trained for 50, 100, 150,\n200, 300, and 400 generated training bags, respectively,\nwhile the performance is evaluated on 1000 test bags.\n4.1.3 MIL-based CIFAR-10 dataset\nWe construct more challenging MIL datasets for multi-\nclass classiﬁcation using images from the CIFAR-10\ndataset for object recognition MIL application [ 49]. The\nCIFAR-10 dataset contains 60000 images divided into ten\nclasses, each image is of size 32 /C2 32, and classes are\ncompletely mutually exclusive. We employed a similar\napproach previously used in Shi et al. [ 15] to evaluate the\nperformance of ViT-IWRS on this dataset. The bags are\nformed by treating images as instances, and bags are nor-\nmally distributed with a mean bag size of 10 and a variance\nof 2, respectively. The target classes are set to {’airplane’,\n’automobile’, ’bird’}, and associated with the labels {’1’,\n’2’, ’3’} accordingly. The bags related to target classes at\nmost contain images from one of these three classes. The\ntraining sets are built with 500 and 5000 bags, while the\ntest set is created with 1000 bags.\n4.1.4 Colon cancer dataset\nDetecting cancerous regions in hematoxylin and eosin (H\n&E) stained whole-slide images (WSI) are vital in clinical\nsettings [ 50]. These images, also called digital pathology\nslides, can occupy several gigabytes of storage space [ 51].\nPresently, supervised approaches require pixel-level\nannotations, which demand signiﬁcant time from patholo-\ngists. A successful solution to reduce pathologists’ work-\nload is to use weak slide levels. For this study, we\nconducted experiments on colon cancer histopathology\nimages [24] to test the efﬁciency of ViT-IWRS.\nThis dataset consists of 100 H&E images belonging to\nbinary classes. These images feature a range of tissue\nappearances, including both normal and malignant regions.\nEvery image has been marked with the majority of nuclei\nfor each cell with a total of 22,444 nuclei and class labels\nsuch as epithelial, inﬂammatory, ﬁbroblast, and miscella-\nneous. Every WSI represents a bag with several 27 /C227\npatches. The bag is labeled as positive if it has one or more\nnuclei from the epithelial class.\n4.1.5 Evaluation measure\nWe evaluate the performance of the proposed ViT-IWRS\nin terms of bag classiﬁcation accuracy. The experiments on\nbenchmark datasets are performed using ﬁve runs of\n10-fold cross-validation, and average performance is\nreported. For the MIL-based MNIST dataset, the experi-\nments are performed with 1000 test bags and different\nnumbers of training bags (50, 100, 150, 200, 300, and 400).\nThe experiments are repeated 50 times for each train and\ntest set, and average results are compared with existing\nstate-of-the-art techniques. Similarly, the experiments are\nrepeated thirty times with different training and testing data\nfor MIL-based CIFAR-10 datasets, and average perfor-\nmance is reported. On the Colon Cancer dataset, we per-\nformed a 5-fold cross-validation, and average results are\npresented.\n4.2 Methods used for comparative study\nThe proposed approach is compared with several state-of-\nthe-art attention-based approaches and other benchmark\nbag-level classiﬁcation techniques. The methods for per-\nformance comparison are selected based on good perfor-\nmance and the wide range of MIL solutions they offer.\nSome of the methods are brieﬂy discussed below.\n• MIL NN [ 41]: This study proposes trainable pooling\noperators for MIL. In this work, the bag-level classi-\nﬁcation technique (MI-NET) directly produces the bag\nlabel. The instance-level classiﬁcation technique (mi-\nNET) pools instance-level scores to produce the bag\nlabel. The pooling approach based on the residual\nconnection ( MI-NET RC) is also proposed.\n• Ranking Loss-based Simple MIL (ESMIL) [ 52]: This\npaper presents a novel approach to differentiate\nTable 1 The details of MIL\nbenchmark datasets Datasets Positive bags Negative bags Total bags Total instances\nTiger 100 100 200 1220\nElephant 100 100 200 1391\nFox 100 100 200 1320\nMusk1 47 45 92 476\nMuks2 63 39 102 6598\n6668 Neural Computing and Applications (2024) 36:6659–6680\n123\nbetween positive and negative bags by a simple\npairwise bag-level ranking loss function. The proposed\nobjective function ensures that the model assigns a\nhigher score to the positive bags. Instead of using a\nthreshold-based decision function, the proposed\napproach penalizes the network when it generates a\nlower score for positive bags compared to negative\nbags.\n• Attention-based Deep MIL (AbDMIL) [ 13]: This work\nproposed an attention approach to identify the weights\nof the instances in the bag. The authors proposed two\narchitectures for attention-based pooling to solve MIL\nbinary classiﬁcation problem.\n• Loss-based Attention (LBA) [ 15]: This method extends\nthe concepts of (AbDMIL) [11] and introduces collab-\norative training for attention and classiﬁcation layers of\nthe network.\n• Multiple-instance SVM (MI-SVM and mi-SVM) [ 26]:\nIn this study, two algorithms mi-SVM and MI-SVM\nextend the use of SVM to solve multiple-instance\nlearning problems. The MI-SVM maximizes the bag\nmargin while SVM updates the hyper-plane based on\nthe instance label assignments.\n• Classiﬁer Ensemble with constructive clustering (CCE)\n[30]: This method represents the entire bag of instances\nfrom a binary vector, employing clustering and adopt-\ning an ensemble learning-based classiﬁcation approach.\nThe binary vector entries are set to 1 if any bag instance\nis a part of the cluster. Additionally, the clustering and\nmodels are trained on different data representations.\n• Multiple instances (Fisher Vector and VLAD) [ 9]:\nThese methods are based on bag encoding generation\ntechniques. These techniques are inspired by the widely\nused Fisher vector (FV) and VLAD encoding schemes\nfor image classiﬁcation\n5 Results and discussion\nIn this Section, we present the results and discuss the\nperformance of the proposed (ViT-IWRS )approach. First,\nwe compare the performance of the proposed approach\nwith state-of-the-art (SOTA) attention-based pooling\napproaches for MIL classiﬁcation problems, including\nAbDMIL [ 13], Gated-AbDMIL [ 53], and loss-based\nattention (LBA) [ 15]. Later, the proposed approach is\ncompared to benchmark bag classiﬁcation approaches.\n5.1 Comparison with SOTA attention-based\npooling approaches\nThe comparison of the ViT-IWRS with three SOTA\nattention techniques LBA [ 15] and AbDMIL [ 13]i s\ndepicted in Fig. 5. Similar to the proposed ViT-IWRS, the\nalgorithms estimate the weights of the instances using the\nattention mechanism and generate a representation vector\nfor the bag. However, these techniques do not consider the\nrelationship of instances in the bag. These approaches are\nimplemented, and reproduced results are reported. The\nproposed ViT-IWRS achieves better results in all ﬁve\ndatasets. For the Fox dataset, the proposed approach\nachieved 62.5% accuracy compared to the 60.5% and\n59.5% accuracy achieved by LBA [ 15] and AbDMIL [ 13],\nrespectively. Similarly, the ViT-IWRS approach attained\n84.5% accuracy for the Tiger dataset, superior to the pre-\nvious results of 83% by LBA [ 15]. In the case of the\nElephant dataset, the proposed approach attained 87.4%\naccuracy.\nFor Musk1 and Musk2 datasets, the ViT-IWRS\napproach achieved 89.5% and 87.6% compared to the\nprevious best performance of 88.6% and 87.3% accuracy,\nrespectively. Overall, the performance of ViT-IWRS is\nsuperior to the counterpart attention-based techniques on\nall ﬁve benchmark datasets. The proposed ViT-IWRS is\nrobust enough to ascertain the association among the\ninstances. With the help of the RSN network, it can provide\nsuperior bag encoding.\nThe experimental results show that the prior assumption\nof instance relationship in the bag restricts the performance\nof AbDMIL and LBA. On the contrary, the proposed ViT-\nIWRS generates several bag representations without prior\nassumption of instance selection and simultaneously\nselects the informative vector through RSN. This ability\ngenerates a more effective vector representation for the bag\nand improves the model’s generalization ability.\n5.2 Comparison with benchmark techniques\nPerformance comparison of ViT-IWRS with benchmark\ntechniques is given in Table 2. ViT-IWRS outperformed\nthe performance of existing benchmark techniques on\nElephant, Tiger, and Fox datasets. ViT-IWRS produced\n62.5% accuracy for the Fox dataset compared to the\nhighest 86.2% accuracy by MI-Net [ 41]. For the Elephant\ndataset, 87.4% accuracy outperformed the previous best\naccuracy of 62.1% accuracy of miFV [ 9]. Similarly, the\nViT-IWRS produced 84.5% accuracy on the Tiger dataset\nand surpassed the previous best performance of 83.6%\naccuracy reported by MI-Net-RC [ 41].\nNeural Computing and Applications (2024) 36:6659–6680 6669\n123\nIn the case of Musk1 and Musk2 datasets, the ViT-\nIWRS produced comparable accuracy to several bag clas-\nsiﬁcation approaches. The Musk1 and Musk2 datasets are\ncomposed of molecular conformations with a small number\nof bags. It is usually difﬁcult for neural networks to per-\nform well as benchmark methods. Additionally, in the\nMusk1 and Musk2 datasets, molecular data follow a\nstructure and can be represented using graphs; therefore,\nthe graph representation-based techniques [ 16] are more\nsuitable for these types of datasets. Thus, the performance\nof ViT-IWRS is limited in these datasets. However, in the\ncase of image datasets, the ViT-IWRS performs consider-\nably better than the benchmark approaches.\n5.3 ViT-IWRS VS benchmark MIL techniques\nBenchmark MIL techniques such as mi-Net and MI-Net\n[41] adopt trainable pooling operations to generate vector\nrepresentation for the bag. However, the proposed pooling\noperation considers the equal contribution of instances in\nthe bag. Additionally, these techniques do not account for\nthe instance relationship information in the pooling pro-\ncess. The bag encoding approaches such as miFV and\nFig. 5 The performance analysis of ViT-IWRS with SOTA attention-based MIL techniques, a shows the comparison on Musk1 and Musk2\ndatasets, while the performance comparison for image-related MIL dataset is given in ( b)\nTable 2 The performance\ncomparison of proposed ViT-\nIWRS with benchmark MIL\ntechniques, the best accuracy is\nhighlighted by boldface and\nitalicized, while the second-best\nperformance for each dataset is\nmarked as simple boldface\nAlgorithms Accuracy ondDatasets\nElephant Tiger Fox Musk1 Musk2\nmi-SVM [26] 82.2 78.4 58.2 87.4 83.6\nMI-SVM [26] 81.4 84.0 57.8 77.9 84.3\nSimple-MI [9] 80.1 ± (8.2) 77.8 ± (9.2) 54.6 ± (9.3) 83.2 ± (12.3) 85.3 ± (11.1)\nEM-DD [28] 77.1 ± (9.8) 73.0 ± (10.1) 60.9 ± (10.1) 84.9 ± (9.8) 86.9 ± (10.8)\nMI-Wrapper [54] 82.7 ± (8.8) 77.0 ± (9.2) 58.2 ± (10.2) 84.9 ± (10.6) 79.6 ± (10.6)\nCCE [30] 79.3 ± (7.5) 76.0 ± (12.0) 59.9 ± (13.7) 83.1 ± (2.5) 71.3 ± (2.4)\nAPR [23] 75.19 ± (1.3) 55.8 ± (1.1) 53.2 ± (1.4) 92.4 – (2.7) 89.20 ± (3.0)\nCitation-kNN [55] 82.6 ± (1.0) 78.8 ± (1.3) 58.2 ± (1.1) 90.3 ± (1.3) 83.7 ± (2.3)\nMI-Graph [16] 85.1 ± (7.0) 81.9 ± (1.6) 61.2\n± (2.8) 90.0 ± (3.8) 90.1 – (3.8)\nRSIS [29] 84.6 ± (1.0) 82.5 ± (2.3) 61.1 ± (2.0) 88.8 ± (2.3) 89.5 ± (2.6)\nmiVLAD [9] 85.0 ± (8.0) 81.0 ± (9.0) 62.0 ± (10.0) 87.1 ± (9.5) 87.2 ± (9.7)\nmiFV [56] 85.2 ± (8.0) 81.3 ± (7.0) 62.1 ± (10.0) 90.9 ± (8.0) 88.4 ± (9.0)\nmi-Net [41] 85.8 ± (3.6) 82.4 ± (3.7) 61.3 ± (3.5) 88.9 ± (3.9) 85.2 ± (4.5)\nMI-NET [41] 86.2 ± (2.5) 83.0 ± (2.2) 62.2 ± (2.2) 88.7 ± (4.1) 85.9 ± (4.5)\nESMIL [52] 82.5 ± (3.0) 82.7 ± (4.0) 61.7 ± (4.5) 87.8 ± (3.5) 88.2 ± (5.0)\nProposed ViT-IWRS 87.4 – (3.2) 84.5 – (3.5) 62.5 – (4.0) 89.5 – (7.5) 87.6 – (5.9)\n6670 Neural Computing and Applications (2024) 36:6659–6680\n123\nmiVLAD [56] are based on dictionary learning techniques\nusing the instance clustering process and incorporate all the\ninstances of the bag in the encoding process. However,\nthese techniques do not incorporate any instance weighting\ntechnique in the encoding process which may affect the\nperformance of generated encoding. Likewise, Simple-MI\n[9] computes the instance-wise mean vector for the bag. In\ncomparison with these algorithms, ViT-IWRS tackles the\nrelationship assumption with instance weighing and bag\nrepresentation selection process.\nRSIS [ 29] adopts a random subspace hard clustering\napproach to select a candidate instance from positive bags\nwhile the instances from negative bags are sampled ran-\ndomly. The selected instances are classiﬁed using an\nensemble learning technique in ambient space. However,\nthe adopted instance selection process in RSIS results in a\nclass imbalance problem. Similarly, CCE [ 30] groups\ntraining instances into c clusters and generates a c/C0 di-\nmensional binary vector representation for the bag. The ith\nbits in the representation vector are set to one if corre-\nsponding bag instances are part of ith cluster. The proposed\nViT-IWRS generates a robust bag representation vector by\nincorporating the information presented in all instances of\nthe bag with different weights. Additionally, the generated\nbag representation vector using ViT-IWRS offers more\ninformation in the classiﬁcation process than the classiﬁ-\ncation of instances in ambient space or binary vector\ngenerated by RSIS [ 29] and CCE [ 30].\nMoreover, ESMIL [ 52] uses a ranking loss mechanism\nto assign a score to each instance in the bags. The proposed\nranking loss function ensures that the highest-scoring\ninstance in a positive bag receives a higher score than the\nhighest-scoring instance in a negative bag. ESMIL distin-\nguishes between positive and negative bags based on the\nhighest-scoring instances from the bag of each category,\nand this process helps to maximize the AUC score. How-\never, ESMIL ignores the contribution of other instances in\nthe bag classiﬁcation process. Additionally, the adopted\ntraining process lacks the ability to learn an efﬁcient score\nfunction for bag classiﬁcation. This property is essential for\nbag-level classiﬁcation, and the selection of a suboptimal\nscoring function affects the model’s generalization ability.\nIn contrast, ViT-IWRS assigns higher weights to the\ninstances in the bag, which induces bag labels and gener-\nates a robust bag representation vector by combining the\ninstance relationship and weighted impact of the instances.\nThis ability helps to learn an efﬁcient scoring function for\nbag-level classiﬁcation.\nSimilarly, Mi-Graph [ 16] assumes instances of the bag\nhave a relationship and adopts a graph kernel learning\ntechnique to transform a given bag into an undirected\nweighted graph. The nodes in the generated graph represent\ninstances of the bag, and if the distance between the two\nnodes is smaller than a preset threshold, then a weighted\nedge is established between the nodes. The weight of the\nedge expresses the afﬁnity of the two nodes. This approach\nis useful where details of the bag structure play an essential\nrole in the bag classiﬁcation process. In contrast, ViT-\nIWRS models instance dependencies through the MHSA\nprocess and simultaneously incorporates bag-wise instance\nrelationship assumption in the classiﬁcation process.\n5.4 Performance comparison on MIL-MNIST\ndataset\nFor the multi-class classiﬁcation problem, the MIL-\nMNIST dataset is generated. We used a bag generation\napproach similar to the one used in LBA [15] and AbDMIL\n[13]. The performance of the ViT-IWRS is compared with\nSOTA attention-based approaches, including LBA [ 15],\nAbDMIL, and Gated-AbDMIL [ 13]. The two approaches,\nAbDMIL and Gated-AbDMIL, were extended with Soft-\nmax output to support multi-class classiﬁcation problems.\nThe bag classiﬁcation is also performed for max-instance,\nmean-instance, max-instance embedding, and mean-in-\nstance embedding. The max-embedding and mean-em-\nbedding are computed by the output of the previously\ndiscussed embedding network. The bag classiﬁcation\nresults in Table 3 show that the proposed ViT-IWRS pro-\nduces better performance in most cases, especially in the\ncase of large training sets of 150, 200, 300, and 400 bags,\nrespectively.\n5.5 Comparison on MIL-based CIFAR-10 dataset\nTo better evaluate the performance of the proposed ViT-\nIWRS, a larger and more challenging dataset is created\nTable 3 The performance comparison of ViT-IWRS with SOTA\nattention techniques on MIL-MNIST dataset. The best accuracy is\nhighlighted in boldface and italicized, while the second-best perfor-\nmance for each dataset is marked in simple boldface\nAccuracy using different training set\nAlgorithms 50 100 150 200 300 400\nInstance-(max) 47.7 75 84.6 88.7 89.2 89.9\nInstance-(mean) 58.7 77.4 86.5 91.7 91.9 92.2\nEmbedding-(max) 63.5 79.6 87.9 91.8 92.5 92.8\nEmbedding-(mean) 52.8 77.4 86.9 92.0 92.3 92.6\nAbDMIL [13] 75.3 87.5 91.89 3 .89 4 .3 95.5\nGated-AbDMIL [13] 72 86.9 91.1 93 93.8 94.5\nLBA [15] 75.9 89.0 91.7 93.9 94.39 5 .7\nProposed ViT-IWRS 76.18 8 .2 92.5 94.2 95.6 96.5\nNeural Computing and Applications (2024) 36:6659–6680 6671\n123\nbased on CIFAR-10. The performance of ViT-IWRS is\ncompared with SOTA methods, including LBA [ 15],\nAbDMIL [ 13], and Gated-AbDMIL [ 13], previously used\nfor MIL-MNIST. The experiments are conducted for 500,\nand 5000 randomly generated training bags. Additionally,\nthe experimental results of this dataset are presented in\nTable 4. The results show that ViT-IWRS surpasses the\nprevious best performance of LBA and produces 3.1% and\n1.5% improved performance on 500 training bags and 5000\nbags, respectively. The experimental results indicate that\nthe proposed ViT-IWRS is robust in determining the\ndependencies among the bag instances in complex and\nchallenging situations involving large datasets.\n5.6 Performance comparison on colon cancer\ndataset\nWe have evaluated the performance of ViT-IWRS algo-\nrithms on a real-life colon cancer dataset with weak\nlabeling. Our comparison includes state-of-the-art tech-\nniques such as AbDMIL [ 13], Gated-AbDMIL [ 13], LBA\n[15], and ESMIL [ 52], as well as instance-level and\nembedding level max and mean pooling operations. The\nresults show the effectiveness of ViT-IWRS on this dataset.\nBased on the results shown in Fig. 6, it is evident that the\nproposed ViT-IWRS outperforms other state-of-the-art\ntechniques. ViT-IWRS obtained 92.4% bag-level classiﬁ-\ncation accuracy compared to the previous best of 90.3% by\nLBA [ 15]. ViT-IWRS achieves this by effectively\nmanaging Global and Local information about the bag.\nFurthermore, the representation selection process ensures\nthat only the necessary bag representation vector is used in\nthe classiﬁcation process.\n5.7 Statistical validation\nIn this work, we evaluate the statistical signiﬁcance of ViT-\nIWRS on MIL benchmark datasets using the Wilcoxon-\nsigned rank test with a 95% conﬁdence interval [ 57, 58].\nUsing statistical analysis, this test determines if there is a\nsubstantial difference between two related groups. This\ntechnique is preferable when the normality or equal vari-\nance assumptions are violated. These methods are tested\nusing the same train-test distribution as ViT-IWRS.\nTable 5 shows the p/C0 values for the Musk1 and Musk2\ndatasets. A p-value below 0.05 indicates that ViT-IWRS is\nstatistically better than LBA [ 15], AbDMIL [ 13], Gated-\nAbDMIL [ 13], and ESMIL [ 52]. Likewise, in the case of\nthe Musk2 dataset, ViT-IWRS is statistically signiﬁcant\ncompared to AbDMIL and Gated-AbDMIL. Table 6 shows\nthe p-values for the Elephant, Tiger, and Fox datasets. The\nproposed ViT-IWRS is statistically signiﬁcant for the Fox\ndataset compared to LBA [ 15], AbDMIL [ 13], Gated-\nAbDMIL [ 13], and ESMIL [ 52]. Similarly, for the Tiger\nand Elephant datasets, the ViT-IWRS is statistically better\nthan AbDMIL [13], Gated-AbDMIL [13], and ESMIL [52].\nThe proposed ViT-IWRS showed statistical signiﬁcance in\ncomparison with AbDMI [ 13] and Gated-AbDMIL [ 13].\nSimilarly, ViT-IWRS exhibited statistical signiﬁcance over\nESMIL [ 52] and LBA [ 15] on four and two datasets,\nrespectively.\nWe also used the Friedman rank test [ 59, 60] to assess\nthe overall performance of various algorithms and compare\ntheir performance across various datasets. This statistical\ntest is designed to assess whether there are statistically\nsigniﬁcant differences among the means of three or more\nrelated groups. It involves ranking the data within each\ngroup and assigning a rank to each algorithm. In this\nranking, the best algorithm is assigned the lowest rank,\nwhile the algorithm with the worst performance is assigned\nthe highest rank. The rankings of the proposed and com-\npared methods are determined with 95% signiﬁcance and a\ncritical distance diagram is plotted to illustrate the results\nin Fig. 7. As shown in Fig. 7, the proposed ViT-IWRS\nachieved the lowest rank (most important) among all\ncompared techniques. This indicates that the performance\nof ViT-IWRS is superior to the compared methods.\nTable 4 The experimental results on MIL-BASED CIFAR-10 dataset.\nThe best accuracy is highlighted in boldface and italicized, while the\nsecond-best performance for each dataset is marked in simple\nboldface\nAlgorithms Performance in accuracy\n500 (Training bags) 5000 (Training bags)\nAbDMIL [13] 41.8 51.7\nGated-AbDMIL [13] 39.8 49.1\nLBA [15] 45.75 1 .9\nProposed ViT-IWRS 49.5 54.8\nFig. 6 The performance analysis of ViT-IWRS with SOTA attention-\nbased MIL techniques on Colon Cancer histopathology dataset\n6672 Neural Computing and Applications (2024) 36:6659–6680\n123\n5.8 Time efficiency comparison\nIn this paper, the time efﬁciency of the proposed ViT-\nIWRS is empirically evaluated on ﬁve benchmark MIL\ndatasets. The time costs of training do not include the time\nfor data preparation. The proposed ViT-IWRS is compared\nwith state-of-the-art counterparts, including LBA [ 15],\nAbDMIL [13], Gated-AbDMIL [13], and ESMIL [52]. The\nalgorithms are trained for 100 Epochs, and the average\ntraining time in the log scale is shown in Fig. 8. All the\nexperiments are conducted on a machine with a Core i7\n3.10 GHz CPU, RTX 3060 GPU, and 16GB of main\nmemory.\nCompared to AbDMIL, Gated-AbDMIL, and LBA, the\ntraining process for ViT-IWRS is more time-consuming.\nThis is because ViT uses a self-attention mechanism with\nquadratic complexity, making it more computationally\nexpensive than traditional attention algorithms. Notably,\nTable 5 The obtained p/C0 values of Wilcoxon-signed ranked test for Musk1 and Musk2 datasets\nMusk1 Musk2\nIs ViT-IWRS statistically signiﬁcant? (if p\\0.05) p-\nvalues\nIs ViT-IWRS statistically signiﬁcant? (if p\\0.050) p-\nvalues\nAlgorithms\nAbDMIL Yes 0.0242 Yes 0.0414\nGated-\nAbDMIL\nYes 0.0079 Yes 0.0331\nLBA Yes 0.0465 No 0.4696\nESMIL Yes 0.0054 No 0.6001\nTable 6 The obtained p/C0 values of Wilcoxon-signed ranked test for Elephant, Tiger, and Fox datasets\nFox Tiger Elephant\nIs ViT-IWRS\nstatistically\nsigniﬁcant?\n(if p\\0.05)\np/C0 values Is ViT-IWRS\nstatistically\nsigniﬁcant?\n(if p\\0.05)\np/C0 values Is ViT-IWRS\nstatistically\nsigniﬁcant?\n(if p\\0.05)\np/C0 values\nAlgorithms\nAbDMIL Yes 0.003 Yes 0.019 Yes 0.0002\nGated-AbDMIL Yes 0.004 Yes 0.017 Yes 0.0044\nLBA Yes 0.009 No 0.125 No 0.0750\nESMIL Yes 0.048 Yes 0.130 Yes 0.0001\nFig. 7 Critical distance diagram comparing the proposed ViT-IWRS\nagainst various MIL algorithms with a 95% conﬁdence interval. The\ndiagram’s top line shows the algorithm’s average rank, with the most\nimportant rank at the left and the least signiﬁcant rank at the right.\nThe two algorithms are not considerably different if they are not\nconnected by bold line\nNeural Computing and Applications (2024) 36:6659–6680 6673\n123\nViT-IWRS requires less training time than ESMIL, which\ninvolves a pairwise loss strategy, necessitating the adjust-\nment of network weights across all pairs of positive and\nnegative bags.\nHowever, ViT-IWRS outperforms state-of-the-art algo-\nrithms on all types of datasets in terms of bag classiﬁcation\nperformance. This outcome underscores the proposed\napproach’s effectiveness and ability to surpass the capa-\nbilities of current state-of-the-art techniques.\n5.9 Parameter sensitivity analysis\nThis section discusses the impact of different hyperpa-\nrameters related to ViT-IWRS on performance. There are\nFig. 8 The time efﬁciency analysis of ViT-IWRS with SOTA attention-based MIL techniques. The time comparisons on the Elephant, Tiger, and\nFox datasets are shown in ( a–c). The time comparison of the Musk1 and Musk2 datasets is illustrated in ( d) and ( e), respectively\n6674 Neural Computing and Applications (2024) 36:6659–6680\n123\nseveral parameters related to ViT-IWRS, such as the size\nof the RSN, the number of blocks, and the number of heads\nin ViT blocks. These parameters are tuned one at a time.\nWhile tuning one parameter, the other parameters are kept\nﬁxed. Initially, the number of transformer encoder blocks\nand layers in RSN is set to two, and the number of heads in\nMHSA is ﬁxed to four, respectively. The details of the\nhyperparameters related to model training are given in\nTable 7. The details of the embedding network are also\npresented in this section.\n5.9.1 Embedding network\nThe proposed ViT-IWRS ﬁrst transforms the bag instance\nto a latent representation using an embedding network. We\nadopted a similar setting for embedding networks as pre-\nviously used in AbDMIL [ 13] and LBA [ 15]. The\nembedding network for benchmark datasets mainly con-\nsists of fully connected layers. In contrast, the MIL-MNIST\nand MIL-based CIFAR-10 datasets network comprises\nconvolution layers with other related operations based on\nthe LeNet5 architecture [ 61]. The details of the networks\nfor the benchmark dataset and MIL-MNIST dataset are\ngiven in Table 8.\n5.9.2 Layers in representation selection subnetworks (RSN)\nThis subnetwork comprises one or more fully connected\nlayers, whereas the network’s last layer consists of a single\noutput neuron. The network learns a nonlinear represen-\ntation selection function using a continuous output vector\nduring training and generates a discretized one-hot vector\nin the testing. The layers in this subnetwork depend on the\ndataset representation diversity. The initial RSN comprises\na fully connected layer with ReLU activation and dropout\noperation. Later, the layers to RSN are added with Tanh(:)\nfollowed by the dropout operation. The experiments show\nthat two subnetwork layers are preferred for Musk1, Ele-\nphant, and Tiger datasets. Whereas, for Musk2 and Fox\ndatasets, tree layer RSN is preferred. However, increasing\nthe number of layers can result in overﬁtting. Furthermore,\nthe number of layers for the MIL-MNIST, MIL-BASED\nCIFAR-10, and Colon Cancer datasets is set to one\nthroughout the experiments. The detailed analysis of RSN\nsize is given in Table in 9.\n5.9.3 Analysis of term k in loss function\nThe loss function presented in Sect. 3.7 comprises L1 and\nL2, where k is a hyperparameter. The value of k plays a\nsigniﬁcant role in the model performance and interpreta-\ntion. As discussed previously, the L1 term in the loss\nfunction can be decreased to a small value even when only\none instance shares the label with the bag; when k ¼ 0, the\nL2 term is removed from the objective, the model only\nfocuses on the bag loss resulting in a low instance recall\nand may negatively affect the classiﬁcation performance.\nWe evaluated the impact of k on MIL-MNIST datasets of\n50 training and 1000 testing bags, respectively. Figure 9\nshows the performance of ViT-IWRS with\nk 2 0; 10\n/C0 3; 10/C0 2; 10/C0 1; 1; 1; 10;½/C138 . The experiments\ndemonstrate the effectiveness of k in the loss function. The\npositive value of k between 1 and 10 improves the\ninstances recall and bag classiﬁcation performance.\n5.9.4 Analysis for ViT depth and attention heads\nThe ViT depth and the number of attention heads are the\nessential parameters in the proposed approach. First, we\nﬁxed the number of attention heads to four and the impact\nof ViT depth. Later, the best-performing depth is used to\nanalyze the inﬂuence of attention heads. The experiments\nshow that a depth of 3 is preferred for the Musk1 and\nMusk2 datasets, respectively, while the number of heads\nTable 7 The details of hyperparameters used in the training of ViT-IWRS\nHyperparameter Benchmark dataset MIL-MNIST dataset MIL CIFAR-10\nDATASET\nk 221\nOptimizer Adam Adam Adam\nBetas (0.9,0.999) (0.9,0.999) (0.9, 0.999)\nLearning-rate 0.0005 0.0005 0.01\nEpochs 150 100 100\nWeight decay 0.0005 0.0001 0.0001\nBatch size 1 1 1\nStopping criteria Lowest validation error and loss Lowest validation error and loss Lowest validation error and loss\nNeural Computing and Applications (2024) 36:6659–6680 6675\n123\nfrom 2 and 4 can produce better performance. This is due\nto the nature of the datasets. Additionally, where the\nstructure information of the instances is important in\naddition to the instance relationship, adding ViT blocks and\nincreasing the number of heads does not improve perfor-\nmance. For the fox, tiger, and Elephant datasets, 3, 2 and 3\nblocks and 4 heads tend to perform well, respectively. It\nshows that these instances inside these datasets are highly\nrelated, and existing SOTA attention-based approaches do\nnot consider this relation. The analysis of depth is shown in\nFig. 10, and the analysis of the number of heads in MHSA\nis illustrated in Fig. 11, respectively. Furthermore, for the\nMIL-MNIST dataset, the depth is set to 1 and the number\nof heads is set to 4 throughout the experiments.\n5.10 Ablation study\nThe proposed ViT-IWRS consists of two essential pro-\ncessing blocks: the transformer encoding and RSN blocks.\nThese blocks are shown in Fig. 3a and d, respectively. The\ncontribution of these two blocks to overall model perfor-\nmance is validated in the section. The performance of these\ntwo blocks is observed on the Musk1 dataset for binary\nclassiﬁcation and the MIL-MNIST dataset for multi-class\nTable 8 The details of\nembedding network for\nbenchmark and MIL-MNIST\ndatasets. The parameters of\nconvolution layers are\nconstituted as\nConvolution(a,b,c,d), where a,\nb, c, and d represent kernel size,\nstride, padding and the number\nof kernels, respectively\nLayer numbers Network Details\nLayer details for Benchmark Dataset\n1 FC-256 ? Activation() ? Dropuout()\n2 FC-128 ? Activation() ? Dropout()\n3 FC-64 ? Activation() ?Dropout()\nLayer details for MIL-MNIST Dataset\n1 Convolution (5,1,0,20) ? Activation()\n2 Max-pool (2,2)\n3 Convolution (5, 1, 0, 50) ? Activation()\n4 Max-pool (2,2)\n5 FC-500 ? Activation()\nLayer Details for MIL-based CIFAR-10 Dataset\n1 Convolution (5, 3, 0, 20) ? Activation()\n2 Max-pool (2,2)\n3 Convolution (5, 1, 0, 50) ? Activation()\n4 Max-pool (2,2)\n5 FC-500 ? Activation() ? Dropout()\n6 FC-500 ? Activation()\nLayer Details for Colon Cancer Dataset\n1 Convolution (4, 1, 0, 36) ? Activation()\n2 Max-pool (2,2)\n3 Convolution (3, 1, 0, 48) ? Activation()\n4 Max-pool (2,2)\n5 FC-500 ? Activation() ? Dropout()\nTable 9 Analysis of layers in RSN\nNumber of layer Musk1 Musk2 Elephant Fox Tiger\n1 87.9 85.7 83.9 50.9 82.5\n2 88.5 85.9 85.4 60.5 83.2\n3 87.2 86.1 83.9 61.2 82.4\n4 86.5 84.3 84.0 59.5 82.5\n5 86.3 83.6 84.2 58.0 81.5\nThe bold value in the table shows the highest performance achieved\non a particular dataset using the corresponding number of layers in\nRSN\nFig. 9 The analysis of the term k in loss function\n6676 Neural Computing and Applications (2024) 36:6659–6680\n123\nclassiﬁcation problems. Additionally, the experiments on\nthe MIL-MNIST dataset are performed 30 times using a\ntraining set of 50 bags and a test set of 1000 bags, and the\naverage performance is presented.\n5.10.1 Effect of RSN block\nIn order to verify the impact of RSN, we replace this block\nwith a simple average operation that computes the feature-\nwise average of the representation matrix R. Later, the\naveraged vector is used for classiﬁcation. The experimental\nresults in Table 10 show that the removal of RSN from the\nproposed ViT-IWRS results in performance degradation.\nTherefore, the use of the RSN block is essential to achieve\nimproved results.\n5.10.2 Effect of transformer encoding\nIn order to verify the impact of transformer encoding, we\nsimply apply max and attention pooling on the output of\nthe embedding network to obtain a bag representation\nvector. Afterward, the generated output vector is used for\nthe classiﬁcation process. This process is analogous to\nexisting AbDMIL and LBA algorithms. The experimental\nFig. 10 The analysis of transformer depth. The depth analysis for Musk1, Musk2, Elephant, Fox, and Tiger datasets is illustrated from ( a–e),\nrespectively\nFig. 11 The analysis of the number of MHSA heads in transformer encoder. The analysis of attention heads for Musk1, Musk2, Elephant, Fox,\nand Tiger datasets is given from ( a–e), respectively\nNeural Computing and Applications (2024) 36:6659–6680 6677\n123\nresults in Table 10 show that the removal of Transformer\nEncoding and RSN from the proposed ViT-IWRS results in\nperformance degradation. Therefore, the use of this block\nis essential to attain improved results.\n6 Conclusion\nIn this work, we presented the application for a vision\ntransformer for simultaneous instance weighting and bag\nencoding processes for MIL. The existing MIL algorithms\npresumed that the instances in the bag are either related or\nunrelated. However, this assumption may not apply to all\nbags in the dataset.\nThe proposed approach avoids the instance relationship\nassumption in a two-stage process. In the ﬁrst stage, several\nbag representation vectors are generated for both relation-\nship assumptions. In the second stage, the network decides\nwhether to consider instances to be related or not using the\nrepresentation selection module in the classiﬁcation pro-\ncess. The experimental results show that the selection\nsubnetwork robustly selects bag representation vectors in\nthe bag classiﬁcation process in an end-to-end trainable\napproach. The experiments are performed on diverse\ndatasets related to images and molecular activity. The\nproposed approach outperformed several state-of-the-art\nattention pooling and benchmark bag classiﬁcation tech-\nniques. Additionally, the proposed ViT-IWRS provides\nmodel interpretations for vision transformer architecture\nthrough an attention-based instance weighting approach.\nThus, the proposed approach is suited for image classiﬁ-\ncation, object detection, and high-risk MIL applications,\nsuch as computer-aided diagnostic and clinical decision\nsupport.\nAlthough the proposed approach produces promising\nresults on several datasets related to images, this approach\nis less computationally expensive as compared to existing\npooling techniques. Furthermore, the performance of ViT-\nIWRS is effective when labels are entirely dependent on\nthe structural properties of the instances, such as molecular\ndatasets. The proposed loss function can be further exten-\nded to handle multi-instance multi-target regression prob-\nlems, such as Drug Discovery and Environmental\nMonitoring. In the future, we intend to explore the appli-\ncation of the proposed approach to multiple-instance and\nmultiple-label learning (MIML) tasks and incorporate the\nstructural details of the bag into the self-attention process.\nAcknowledgement This publication was jointly supported by Qatar\nUniversity QUHI-CENG-22/23-548. The ﬁndings achieved herein are\nsolely the responsibility of the authors. Open Access funding pro-\nvided by the Qatar National Library.\nData availability The datasets generated during and/or analyzed\nduring the current study are publically available at http://www.uco.es/\ngrupos/kdis/momil/.\nDeclarations\nConflict of interest Authors declare no conflict of interest.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate\nif changes were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unless\nTable 10 Details of ablation study, the performance is presented in classiﬁcation accuracy\nAblation study design Performance in\naccuracy\nImpact\nMusk1 MIL-\nMNIST\nEffect of RSN block\n(RSN network block removed from the model, and representation\nvectors in R are combined by applying average pooling operation)\n87.70 73.90 The model’s performance degrades when the RSN\nnetwork is replaced with an average operation.\nEffect of Transformer Encoding block. (Transformer encoding block\nand RSN network block is removed from the model, and the output\nof Embedding network is transformed to a vector representation\nusing simple Max-pooling)\n86.2 63.5 The model’s performance degrades when the RSN\nnetwork is replaced with an average operation.\nEffect of Transformer Encoding block. (Transformer encoding block\nand RSN network block is removed from the model, and the output\nof Embedding network is transformed to vector representation using\nattention pooling pooling)\n88.4 75.3 The model’s performance degrades when the\nTransformer encoding and RSN is removed from\nmodel.\nPerformance of proposed ViT-IWRS with all proposed blocks 89.50 76.10 ViT-IWRS achieves improved performance with all\nproposed blocks\n6678 Neural Computing and Applications (2024) 36:6659–6680\n123\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\nReferences\n1. Zhou Z-H (2018) A brief introduction to weakly supervised\nlearning. Natl Sci Rev 5(1):44–53\n2. Li M, Li X, Jiang Y, Zhang J, Luo H, Yin S (2022) Explainable\nmulti-instance and multi-task learning for COVID-19 diagnosis\nand lesion segmentation in CT images. Knowl-Based Syst\n252:109278\n3. Liu Y, Wu YH, Wen P, Shi Y, Qiu Y, Cheng MM (2020) Lev-\neraging instance-, image-and dataset-level information for\nweakly supervised instance segmentation. IEEE Trans. Pattern\nAnal. Mach. Intell. 44(3):1415–1428\n4. Zhang Y, Liu S, Qu X, Shang X (2022) Multi-instance discrim-\ninative contrastive learning for brain image representation.\nNeural Comput Appl. https://doi.org/10.1007/s00521-022-07524-\n7\n5. Antwi-Bekoe E, Liu G, Ainam J-P, Sun G, Xie X (2022) A deep\nlearning approach for insulator instance segmentation and defect\ndetection. Neural Comput Appl 34(9):7253–7269\n6. Wang K, Liu J, Gonza ´lez D (2017) Domain transfer multi-in-\nstance dictionary learning. Neural Comput Appl 28:983–992\n7. Carbonneau M-A, Cheplygina V, Granger E, Gagnon G (2018)\nMultiple instance learning: a survey of problem characteristics\nand applications. Pattern Recogn 77:329–353\n8. Cheplygina V, Tax DM, Loog M (2015) Dissimilarity-based\nensembles for multiple instance learning. IEEE Trans Neural\nNetw Learn Syst 27(6):1379–1391\n9. Wei X-S, Wu J, Zhou Z-H (2016) Scalable algorithms for multi-\ninstance learning. IEEE Trans Neural Netw Learn Syst\n28(4):975–987\n10. Perronnin F, Sa´nchez J, Mensink T (2010) Improving the ﬁsher\nkernel for large-scale image classiﬁcation. In: European Confer-\nence on Computer Vision, pp. 143–156. Springer\n11. Ramon J, De Raedt L (2000) Multi instance neural networks. In:\nProceedings of the ICML-2000 Workshop on Attribute-value and\nRelational Learning, pp. 53–60\n12. Kandemir M, Hamprecht FA (2015) Computer-aided diagnosis\nfrom weak supervision: a benchmarking study. Comput Med\nImaging Graph 42:44–50\n13. Ilse M, Tomczak J, Welling M (2018) Attention-based deep\nmultiple instance learning. In: International conference on\nmachine learning, pp. 2127–2136. PMLR\n14. Zhang W-J, Zhou Z-H (2014) Multi-instance learning with dis-\ntribution change. In: Proceedings of the AAAI conference on\nartiﬁcial intelligence, vol. 28\n15. Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based\nattention for deep multiple instance learning. In: Proceedings of\nthe AAAI conference on artiﬁcial intelligence, vol. 34,\npp. 5742–5749\n16. Zhou Z-H, Sun Y-Y, Li Y-F (2009) Multi-instance learning by\ntreating instances as non-IID samples. In: Proceedings of the 26th\nannual international conference on machine learning,\npp. 1249–1256\n17. Waqas M, Tahir MA, Qureshi R (2021) Ensemble-based instance\nrelevance estimation in multiple-instance learning. In: 2021 9th\nEuropean workshop on visual information processing (EUVIP),\npp. 1–6. IEEE\n18. Waqas M, Tahir MA, Qureshi R (2023) Deep Gaussian mixture\nmodel based instance relevance estimation for multiple instance\nlearning applications. Appl Intell 53(9):10310–10325\n19. Waqas M, Tahir MA, Khan SA (2023) Robust bag classiﬁcation\napproach for multi-instance learning via subspace fuzzy cluster-\ning. Expert Syst Appl 214:119113\n20. Shao Z, Bian H, Chen Y, Wang Y, Zhang J, Ji X et al (2021)\nTransmil: transformer based correlated multiple instance learning\nfor whole slide image classiﬁcation. Adv Neural Inf Process Syst\n34:2136\n21. Waqas M, Khan Z, Ahmed SU, Raza A (2023) MIL-Mixer: a\nrobust bag encoding strategy for Multiple Instance Learning (mil)\nusing MLP-Mixer. In 2023 18th IEEE International Conference\non Emerging Technologies (ICET) 22–26\n22. Wei X-S, Zhou Z-H (2016) An empirical study on image bag\ngenerators for multi-instance learning. Mach Learn 105(2):155–198\n23. Dietterich TG, Lathrop RH, Lozano-Pe ´rez T (1997) Solving the\nmultiple instance problem with axis-parallel rectangles. Artif\nIntell 89(1–2):31–71\n24. Sirinukunwattana K, Raza SEA, Tsang Y-W, Snead DR, Cree IA,\nRajpoot NM (2016) Locality sensitive deep learning for detection\nand classiﬁcation of nuclei in routine colon cancer histology\nimages. IEEE Trans Med Imaging 35(5):1196–1206\n25. Raykar VC, Krishnapuram B, Bi J, Dundar M, Rao RB (2008)\nBayesian multiple instance learning: automatic feature selection\nand inductive transfer. In: Proceedings of the 25th international\nconference on machine learning, pp. 808–815\n26. Andrews S, Tsochantaridis I, Hofmann T (2002) Support vector\nmachines for multiple-instance learning. In: NIPS, vol. 2, p. 7\n27. Amar RA, Dooly DR, Goldman SA, Zhang Q (2001) Multiple-\ninstance learning of real-valued data. In: ICML, pp. 3–10.\nCiteseer\n28. Zhang Q, Goldman S (2001) EM-DD: An improved multiple-\ninstance learning technique. In: Dietterich T, Becker S, Ghahra-\nmani Z(ed) Advances in neural information processing systems.\nMIT Press, 14. https://proceedings.neurips.cc/paper_ﬁles/paper/\n2001/ﬁle/e4dd5528f7596dcdf871aa55cfccc53c-Paper.pdf\n29. Carbonneau M-A, Granger E, Raymond AJ, Gagnon G (2016)\nRobust multiple-instance learning ensembles using random sub-\nspace instance selection. Pattern Recogn 58:83–99\n30. Zhou Z-H, Zhang M-L (2007) Solving multi-instance problems\nwith classiﬁer ensemble based on constructive clustering. Knowl\nInf Syst 11(2):155–170\n31. Zhou Z-H, Xu J-M (2007) On the relation between multi-instance\nlearning and semi-supervised learning. In: Proceedings of the 24th\ninternational conference on machine learning, pp. 1167–1174\n32. Leistner C, Saffari A, Bischof H (2010) Miforests: Multiple-in-\nstance learning with randomized trees. In: European conference\non computer vision, pp. 29–42. Springer\n33. Li CH, Gondra I, Liu L (2012) An efﬁcient parallel neural net-\nwork-based multi-instance learning algorithm. J Supercomput\n62(2):724–740\n34. Waqas M, Khan Z, Anjum S, Tahir MA (2020) Lung-wise\ntuberculosis analysis and automatic CT report generation with\nhybrid feature and ensemble learning. In: CLEF (Working Notes)\n35. Abro WA, Aicher A, Rach N, Ultes S, Minker W, Qi G (2022)\nNatural language understanding for argumentative dialogue sys-\ntems in the opinion building domain. Knowl-Based Syst\n242:108318\n36. Hanif M, Waqas M, Muneer A, Alwadain A, Tahir MA, Raﬁ M\n(2023) Deepsdc: deep ensemble learner for the classiﬁcation of\nsocial-media ﬂooding events. Sustainability 15(7):6049\n37. Hoffman J, Pathak D, Darrell T, Saenko K (2015) Detector dis-\ncovery in the wild: joint multiple instance and representation\nlearning. In: Proceedings of the IEEE conference on computer\nvision and pattern recognition, pp. 2883–2891\nNeural Computing and Applications (2024) 36:6659–6680 6679\n123\n38. Zhang C, Platt J, Viola P (2005 ) Multiple instance boosting for\nobject detection. In: Weiss J, Sch\\‘‘{o}lkopf B, Platt J(ed) Advances\nin neural information processing systems. MIT Press, 18\n39. Shi X, Xing F, Xu K, Xie Y, Su H, Yang L (2017) Supervised\ngraph hashing for histopathology image retrieval and classiﬁca-\ntion. Med Image Anal 42:117–128\n40. Liu Y, Chen H, Wang Y, Zhang P (2021) Power pooling: an\nadaptive pooling function for weakly labelled sound event\ndetection. In: 2021 International joint conference on neural net-\nworks (IJCNN), pp. 1–7. IEEE\n41. Wang X, Yan Y, Tang P, Bai X, Liu W (2018) Revisiting mul-\ntiple instance neural networks. Pattern Recogn 74:15–24\n42. Li G, Li C, Wu G, Ji D, Zhang H (2021) Multi-view attention-\nguided multiple instance detection network for inter-\npretable breast cancer histopathological image diagnosis. IEEE\nAccess 9:79671–79684\n43. Wang F, Jiang M, Qian C, Yang S, Li C, Zhang H, Wang X, Tang\nX (2017) Residual attention network for image classiﬁcation. In:\nProceedings of the IEEE conference on computer vision and\npattern recognition, pp. 3156–3164\n44. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X,\nUnterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S\net al.(2020) An image is worth 16x16 words: transformers for\nimage recognition at scale. arXiv preprint arXiv:2010.11929\n45. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez\nAN, Kaiser Ł, Polosukhin I (2017) Attention is all you need.\nAdvances in neural information processing systems. Curran\nAssociates, Inc., 30\n46. Jang E, Gu S, Poole B (2017) Categorical Reparametrization with\nGumbel-Softmax. In: Proceedings international conference on\nlearning representations (ICLR). https://openreview.net/pdf?id=\nrkE3y85ee\n47. Li X-C, Zhan D-C, Yang J-Q, Shi Y (2021) Deep multiple\ninstance selection. Sci China Inf Sci 64(3):1–15\n48. LeCun Y, Cortes C, Burges C (2010) Mnist handwritten digit\ndatabase. ATT Labs [Online]. Available: http://yann.lecun.com/\nexdb/mnist2\n49. Krizhevsky A, Hinton G (2009) Learning multiple layers of\nfeatures from tiny images. Technical report, University of\nToronto. https://www.cs.toronto.edu/*kriz/learning-features-\n2009-TR.pdf\n50. Ghaznavi F, Evans A, Madabhushi A, Feldman M (2013) Digital\nimaging in pathology: whole-slide imaging and beyond. Annu\nRev Pathol 8:331–359\n51. Dimitriou N, Arandjelovic´ O, Caie PD (2019) Deep learning for\nwhole slide image analysis: an overview. Front Med 6:264\n52. Asif A et al (2019) An embarrassingly simple approach to neural\nmultiple instance classiﬁcation. Pattern Recogn Lett 128:474–479\n53. Hahn M (2020) Theoretical limitations of self-attention in neural\nsequence models. Trans Assoc Comput Linguist 8:156–171\n54. Frank E, Xu X (2008) Applying propositional learning algorithms\nto multi-instance data. Working paper series, Department of\ncomputer science, The University of Waikato. https://books.goo\ngle.com/books?id=5eaGzgEACAAJ\n55. Wang J, Zucker J-D (2000) Solving multiple-instance problem: a\nlazy learning approach. International Conference on Machine\nLearning. 1:1119–1126. https://api.semanticscholar.org/Corpu\nsID:13896348\n56. Wei X-S, Wu J, Zhou Z-H (2014) Scalable multi-instance\nlearning. In: 2014 IEEE international conference on data mining,\npp. 1037–1042. IEEE\n57. Wilcoxon F (1992) Individual comparisons by ranking methods.\nIn: Kotz S, Johnson NL (eds) Breakthroughs in statistics:\nmethodology and distribution. Springer, Berlin, pp 196–202\n58. Conover WJ (1999) Practical nonparametric statistics, vol 350.\nWiley, New York\n59. Demsˇar J (2006) Statistical comparisons of classiﬁers over mul-\ntiple data sets. J Mach Learn Res 7:1–30\n60. Friedman M (1937) The use of ranks to avoid the assumption of\nnormality implicit in the analysis of variance. J Am Stat Assoc\n32(200):675–701\n61. LeCun Y, Bottou L, Bengio Y, Haffner P (1998) Gradient-based\nlearning applied to document recognition. Proc IEEE\n86(11):2278–2324\nPublisher’s Note Springer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nAuthors and Afﬁliations\nMuhammad Waqas1,2 • Muhammad Atif Tahir1 • Muhammad Danish Author3 • Sumaya Al-Maadeed4 •\nAhmed Bouridane5 • Jia Wu2\n& Muhammad Waqas\nwaqas.sheikh@nu.edu.pk; mwaqas@mdanderson.org\nMuhammad Atif Tahir\natif.tahir@nu.edu.pk\nMuhammad Danish Author\nk190887@nu.edu.pk\nSumaya Al-Maadeed\nSalali@qu.edu.qa\nAhmed Bouridane\nabouridane@sharjah.ac.ae\nJia Wu\njiawu11@mdanderson.com\n1 FAST School of Computing, National University of\nComputer Emerging Science (FAST-NUCES), Karachi,\nPakistan\n2 Department of Imaging Physics, The University of Texas MD\nAnderson Cancer Center, Houston, TX, USA\n3 College of information technology, United Arab Emirates\nUniversity, Abu Dhabi, United Arab Emirates\n4 Department of Computer Science and Engineering, Qatar\nUniversity, Doha, Qatar\n5 Cybersecurity and Data Analytics Research Center,\nUniversity of Sharjah, Sharjah, UAE\n6680 Neural Computing and Applications (2024) 36:6659–6680\n123",
  "topic": "Computational Science and Engineering",
  "concepts": [
    {
      "name": "Computational Science and Engineering",
      "score": 0.8295942544937134
    },
    {
      "name": "Pooling",
      "score": 0.7573189735412598
    },
    {
      "name": "Computer science",
      "score": 0.7122095227241516
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7121143341064453
    },
    {
      "name": "Machine learning",
      "score": 0.6003109812736511
    },
    {
      "name": "Representation (politics)",
      "score": 0.4819062054157257
    },
    {
      "name": "Feature learning",
      "score": 0.48055532574653625
    },
    {
      "name": "Transformer",
      "score": 0.4382660388946533
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.4316960275173187
    },
    {
      "name": "Feature selection",
      "score": 0.42706021666526794
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.35442936420440674
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}