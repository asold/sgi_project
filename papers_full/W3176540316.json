{
  "title": "Multi-Document Transformer for Personality Detection",
  "url": "https://openalex.org/W3176540316",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2205372220",
      "name": "Feifan Yang",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2099425862",
      "name": "Xiaojun Quan",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2165821485",
      "name": "Yun-Yi Yang",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2133840351",
      "name": "Jianxing Yu",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2205372220",
      "name": "Feifan Yang",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2099425862",
      "name": "Xiaojun Quan",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2165821485",
      "name": "Yun-Yi Yang",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2133840351",
      "name": "Jianxing Yu",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3011686870",
    "https://openalex.org/W2096282930",
    "https://openalex.org/W6680018895",
    "https://openalex.org/W2573314583",
    "https://openalex.org/W3037031145",
    "https://openalex.org/W3034828027",
    "https://openalex.org/W3035156228",
    "https://openalex.org/W2759869292",
    "https://openalex.org/W2768262152",
    "https://openalex.org/W6736575291",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W2806246579",
    "https://openalex.org/W2963631426",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W1973871257",
    "https://openalex.org/W3003186568",
    "https://openalex.org/W2597655663",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2763585929",
    "https://openalex.org/W2489406233",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3035294872",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2134111694",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3169322305",
    "https://openalex.org/W2963248507",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2950336186",
    "https://openalex.org/W2962922117",
    "https://openalex.org/W2960748659",
    "https://openalex.org/W2119595472"
  ],
  "abstract": "Personality detection aims to identify the personality traits implied in social media posts. The core of this task is to put together information in multiple scattered posts to depict an overall personality profile for each user. Existing approaches either encode each post individually or assemble posts arbitrarily into a new document that can be encoded sequentially or hierarchically. While the first approach ignores the connection between posts, the second tends to introduce unnecessary post-order bias into posts. In this paper, we propose a multi-document Transformer, namely Transformer-MD, to tackle the above issues. When encoding each post, Transformer-MD allows access to information in the other posts of the user through Transformer-XL’s memory tokens which share the same position embedding.Besides, personality is usually defined along different traits and each trait may need to attend to different post information, which has rarely been touched by existing research. To address this concern, we propose a dimension attention mechanism on top of Transformer-MD to obtain trait-specific representations for multi-trait personality detection. We evaluate the proposed model on the Kaggle and Pandora MBTI datasets and the experimental results show that it compares favorably with baseline methods.",
  "full_text": "Multi-Document Transformer for Personality Detection\nFeifan Yang, Xiaojun Quan*, Yunyi Yang, Jianxing Yu\nSchool of Data and Computer Science, Sun Yat-sen University, China\nfyangff6, yangyy37g@mail2.sysu.edu.cn, fquanxj3, yujx26g@mail.sysu.edu.cn\nAbstract\nPersonality detection aims to identify the personality traits\nimplied in social media posts. The core of this task is to put\ntogether information in multiple scattered posts to depict an\noverall personality proﬁle for each user. Existing approaches\neither encode each post individually or assemble posts arbi-\ntrarily into a new document that can be encoded sequentially\nor hierarchically. While the ﬁrst approach ignores the connec-\ntion between posts, the second tends to introduce unnecessary\npost-order bias into posts. In this paper, we propose a multi-\ndocument Transformer, namely Transformer-MD, to tackle\nthe above issues. When encoding each post, Transformer-MD\nallows access to information in the other posts of the user\nthrough Transformer-XL’s memory tokens which share the\nsame position embedding. Besides, personality is usually de-\nﬁned along different traits and each trait may need to attend\nto different post information, which has rarely been touched\nby existing research. To address this concern, we propose a\ndimension attention mechanism on top of Transformer-MD\nto obtain trait-speciﬁc representations for multi-trait person-\nality detection. We evaluate the proposed model on the Kag-\ngle and Pandora MBTI datasets and the experimental results\nshow that it compares favorably with baseline methods.\nIntroduction\nPersonality detection is a fundamental task in psychology\nwith wide applications in areas such as public health (Fried-\nman and Kern 2014), personalized medicine (Matz et al.\n2017), and mental health (Bagby et al. 1995). Besides, it can\nprovide insightful information for many natural language\nprocessing (NLP) tasks. For example, Fung et al. (2016)\ndeveloped a virtual interactive dialogue system by incorpo-\nrating the user’s emotion, sentiment, and personality. Lynn\net al. (2017) proposed a general NLP task by adapting user\nfactors such as age, gender, and personality traits.\nTraditional questionnaire-based approaches to personal-\nity detection are time-consuming and laborious. Whereas in\nthe era of social media, users generate a tremendous number\nof posts containing their behavior trace every day, provid-\ning new possibilities for personality detection research (Xue\net al. 2018; Keh, Cheng et al. 2019; Jiang, Zhang, and Choi\n* Xiaojun Quan is the corresponding author.\nCopyright © 2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nEncoder\n!\"#\n!$#\nPost1 Postn\n…\nEncoder\n…\nEncoder\nEncoder\nPost2\n≠\nPostn Post1\n… Encoder\nPostn-1\nPost1\nPost2\nPostn\n … MBTI\nMBTI\nMBTI\nFigure 1: Existing approaches to combine multiple posts for\npersonality detection. (a) encodes each post individually. (b)\nencodes posts by integrating them into a new document with\narbitrary order, where “[ ]” means the concatenation of posts\nand “6=” means an inconsistent result for the posts contain-\ning the same personality information.\n2020). The typical setting of this task is: for each user, multi-\nple scattered posts are provided, and the objective is to piece\ntogether information in the posts into a comprehensive user\npersonality proﬁle.\nExisting approaches to automatically combining multi-\nple posts for personality detection can be broadly divided\ninto two categories. Firstly, as shown in Figure 1(a), each\npost is ﬁrst encoded independently and then averaged into\nthe user representation (Hernandez and Knight 2017; Keh,\nCheng et al. 2019). This approach simply ignores the fact\nthat the posts of a user may not be of equal importance and\nthey may also need to interact to complement the personal-\nity proﬁle. Secondly, as shown in Figure 1(b), the posts are\nintegrated into a long ﬂat sequence with an arbitrary post\norder and then encoded hierarchically (Lynn, Balasubrama-\nnian, and Schwartz 2020; Xue et al. 2018) or sequentially\n(Zhou et al. 2019; Jiang, Zhang, and Choi 2020; Yang et al.\n2019b). However, models trained in this way may learn the\nextra post-order bias, resulting in inconsistent results of each\nTheThi rty-Fi fth AAA ICon ferenceon A rti fi ci al Intellig ence(AAAI-21)\n14221\nuser1 and affecting the generalization ability of the models.\nBesides, personality is deﬁned in terms of different dimen-\nsions (traits) and different post information is likely to con-\ntribute to different dimensions, which has rarely been men-\ntioned by existing research.\nIn this paper, we propose a Multi-Document Trans-\nformer model, namely Transformer-MD, to overcome the\nabove limitations. Transformer-MD is a post-order-agnostic\nencoder based on Transformer-XL (Dai et al. 2019) and ini-\ntialized by the pre-trained XLNet (Yang et al. 2019b). When\nencoding each post, it allows access to other post informa-\ntion of the same user through memory tokens that share the\nsame position embedding. Moreover, we deﬁne a personality\ntrait-speciﬁc attention, dimension attention (DA), on top of\nTransformer-MD, which allows each personality dimension\nto focus on speciﬁc post information they care most about.\nThe contributions of this work can be summarized as:\n• We propose a novel post-order-agnostic encoder to put to-\ngether the posts of a user to depict an overall personality\nproﬁle without introducing the post-order bias.\n• We propose a dimension attention mechanism to focus\neach personality dimension on relevant post information.\n• We provide thorough analyses and discussions to demon-\nstrate the effectiveness of the proposed model.\nRelated Work\nMost previous works on personality detection rely on hand-\ncrafted feature engineering (Yarkoni 2010; Schwartz et al.\n2013; Amirhosseini and Kazemian 2020). They may use\nvarious psycholinguistic attributes extracted by Linguistic\nInquiry and Word Count (Pennebaker, Francis, and Booth\n2001) or statistical features extracted by bag-of-words mod-\nels. Obviously, these methods are limited by their capability\nin extracting many useful features (Lynn, Balasubramanian,\nand Schwartz 2020). Recently, deep learning methods have\nbeen widely applied and become mainstream for personality\ndetection. Essentially, personality detection can be regarded\nas a multi-document multi-label classiﬁcation task, which is\nrelated to the following domains.\nMulti-Document Encoding\nMulti-document encoding has been extensively studied, yet\nwe will only review those related to personality detection\nfrom four perspectives: single encoder, graph encoder, hier-\narchical network, and concatenation encoder.\nThe single-encoder approach is predominately adopted\nin personality detection, which encodes each post indepen-\ndently with a single model. Hernandez and Knight (2017)\nand Tandera et al. (2017) used LSTM to encode each post\nwith the GloVe embeddings. Keh, Cheng et al. (2019) used\nBERT to encode each post. However, these methods gener-\nally ignore the dependency between posts, which is detri-\nmental to integrate the scattered personality information\nacross different posts. An alternative approach is to use\na graph encoder, modeling the interaction between posts\n1Given the same content of posts, the personality type they rep-\nresent should be consistent whatever order they are integrated in.\nby graph neural networks. This approach has been widely\ninvestigated in multi-document evidence reasoning tasks\n(Zhao et al. 2019; Zhou et al. 2019; Ding et al. 2019; Liu\net al. 2020). Unlike Wikipedia evidence documents, there\nare no hyperlinks or priori personality-links between user\nposts, making it non-trivial to directly apply such models\nfor personality detection.\nThe hierarchical-network approach tries to encode the\nposts hierarchically. Amirhosseini and Kazemian (2020)\nproposed to encode each post ﬁrst via a gated recurrent unit\n(GRU), and then passed the encodings to a second GRU to\nrealize the interaction between posts. Xue et al. (2018) ﬁrst\ndesigned an AttRCNN module to encode each post and then\nused a convolutional neural network for the interaction be-\ntween posts. The ﬁnal strategy is the concatenation encoder\nwhich concatenates the user’s posts into a new document in\na certain order and then encodes it with a sequence model\nsuch as BERT (Zhou et al. 2019; Jiang, Zhang, and Choi\n2020) and XLNet (Yang et al. 2019b). Intuitively, both of the\nhierarchical-network and concatenation-encoder approaches\nintroduce extra post-order bias into the posts, impairing the\ngeneralization ability of models.\nMulti-Dimension Classiﬁcation\nOne of the challenges for multi-dimension classiﬁcation is to\nallow different dimensions to focus on different information.\nLynn, Balasubramanian, and Schwartz (2020) attempted to\naddress this by training an appropriative model for each di-\nmension, which ignores the interaction between personality\ndimensions. In other multi-label classiﬁcation tasks, Yang\net al. (2018, 2019a) adopted an attention mechanism to au-\ntomatically extract informative features for each category.\nVu, Nguyen, and Nguyen (2020) also proposed an attention-\nbased method to obtain label-speciﬁc vectors that represent\nuseful clinical text fragments relating to certain international\nclassiﬁcation of disease diagnosis codes. In this paper, we\nexplored a dimension attention for personality detection.\nApproach\nIn this section, we ﬁrst formulate the problem of personality\ndetection and then provide the details of our Transformer-\nMD model and the dimension attention module.\nProblem Deﬁnition\nGiven a set X = fx1;x2;\u0001\u0001\u0001 ;xngof nposts from a user,\nwhere xi = [ti1;ti2;\u0001\u0001\u0001 ;tik] is i-th post with ktokens, per-\nsonality detection can be formulated as a user-level classiﬁ-\ncation problem. The model takes X as input and produces a\nuser representation U 2Rd, or U 2Rt\u0002d as in our model,\nwhere dis the hidden size and tis the number of personality\ntraits. Based on U, tpersonality traits Y = fy1;y2;\u0001\u0001\u0001 ;ytg\nare predicted by tclassiﬁers individually, where yi is chosen\nfrom a trait-speciﬁc label set fyi1;yi2;\u0001\u0001\u0001 ;yimg, e.g., t= 4\nand m= 2 in the MBTI taxonomy. We also refer to the traits\ntechnically as personality dimensions in this paper.\nMulti-Document Transformer\nThe proposed Transformer-MD is inspired by Transformer-\nXL, attempting to encode and integrate multiple posts of\n14222\nPost-Independent Representation\nt(n-1)1\n…\n…CLS1 SEP…Post\n1\n…\n…\n…\n…\nLow-LevelEncoderMulti-DocumentFusionLayerHigh-LevelEncoder\nPost-Related Representation\n…\nPost\n2 Post\n3\nPost\nn\nMemory TokensPost\nnTokensType\nPost\nn\n-\n1\n… …………\n…………\n…………\nCLS1CLS2 CLSn-1… CLSn tn1…SEPPOS0POS0 POS0… POS0 …POS1 POSk-1\nSEG1SEG1 SEG1… SEG0 …SEG0 SEG0\nTokenPositionSegment\nt11\n…\n…CLS2 SEP…\n…\nt21\n…\n…CLS3 SEP…\n…\nt31\n…\n…SEP…\n…\nCLSn-1\nSEP…\nCLS3POS0SEG1\nCLSn tn1\nFigure 2: Overview of our Transformer-MD, which consists of a low-level encoder, a multi-document fusion layer and a high-\nlevel encoder, distinguished by different colors. All the posts of a user are encoded in parallel but we only show the whole\nprocess of Postn. Three types of embedding used in the model are listed in the upper left corner.\na user in a post-order-agnostic manner. The purpose of\nTransformer-XL is to relieve the limit of Transformer on\ninput size to enable learning of long-distance dependency\nbeyond a ﬁxed length without disrupting temporal coher-\nence (Dai et al. 2019). Speciﬁcally, Transformer-XL divides\na long document into multiple ﬁxed-length segments and en-\ncodes them with a segment-level recurrence mechanism. Af-\nter encoding a segment, Transformer-XL stores its hidden\nstates in a memory bank and reuses them for future seg-\nments. Inspired by this mechanism, we treat each post as a\nsegment. While processing each post, we store the rest posts\nfrom the same user in the memory, so that dependencies be-\ntween posts can be modeled as in Transformer-XL.\nNevertheless, a user may have dozens to hundreds of\nposts, and each post may contain dozens of tokens, so it\nis unrealistic to put all of them into memory. Inspired by\nthe two-stage encoding scheme of Transformer-XH (Zhao\net al. 2019), we pre-encode each post separately for several\ninitial layers of Transformer-XL, in which post information\nwill be aggregated into their respective CLS tokens. Then,\nwe put only the CLS tokens instead of all the post tokens\ninto memory and model the dependency between posts in the\nremaining layers of Transformer-XL. Besides, Transformer-\nXL encodes memory tokens sequentially for semantic coher-\nence between segments, implemented by position embed-\ndings. However, posts are scattered and submitted by users\nrandomly, and there is no natural order between them for\npersonality detection. Therefore, we make the memory to-\nkens of all posts to share the same position embedding so\nthat posts can interact without introducing post-order bias.\nAs shown in Figure 2, Transformer-MD can be decomposed\ninto a low-level encoder, a multi-document fusion layer, and\na high-level encoder.\nLow-level encoder is composed of several bottom layers\n(9 in this paper) of Transformer-XL, aiming to learn post-\nindependent representation for each post by encoding them\nindividually. In this way, the information of a post will be\naggregated into its CLS token. This encoder is similar to\nTransformer-XL while encoding the ﬁrst segment without\nmemory. Formally, for the i-th post, this encoder updates\nthe representations Hi layer by layer as follows. At layer l,\nit computes the representation of each token by gathering\ninformation from the other tokens in the post:\nQl\ni;Kl\ni;V l\ni = Hl\u00001\ni WT\nq ;Hl\u00001\ni WT\nk ;Hl\u00001\ni WT\nv (1)\nHl\ni = TransformerLayer\n\u0000\nQl\ni;Kl\ni;V l\ni\n\u0001\n(2)\nIn Equation (1) and (4), WT\nq , WT\nk and WT\nv represent learn-\nable parameters for the query, key and value in Transformer\n(Vaswani et al. 2017), respectively. In Equation (2) and (5),\nTransformerLayercontains a multi-head self-attention and\na feed-forward network (Vaswani et al. 2017). Particularly,\nH0\ni in the ﬁrst layer is set to the embeddings of xi.\nMulti-document fusion layer is used to construct memory\ntokens for the high-level encoder. Formally, for thei-th post\nto be encoded in layerl, we concatenate the CLS representa-\ntions Ml\u00001\ni = fHl\u00001\n1;cls;\u0001\u0001\u0001 ;Hl\u00001\ni\u00001;cls;Hl\u00001\ni+1;cls;\u0001\u0001\u0001 ;Hl\u00001\nn;clsg\n2R(n\u00001)\u0002d of the other posts from the preceding layer to\nconstruct the memory. In doing this, the information of the\nother posts from the same user is stored in the memory. Be-\nsides, we allow the position embedding of each CLS to be\nshared in the memory so that the post order is ignored. We\nalso add segment embeddings to distinguish the post from\nthe other posts, as shown in Figure 2.\nHigh-level encoder is composed of the last few layers (3 in\nthis paper) of Transformer-XL. It aims to learn a post-related\nrepresentation for each post by selectively collecting infor-\nmation from the other posts in the memory with the multi-\nhead self-attention of Transformer. This process is similar\nto Transformer-XL while encoding the second segment with\nmemory storing the ﬁrst segment. Formally, for thei-th post,\nthis encoder updates the representation Hi layer by layer as\nfollows. At layer l, it computes the representation of each\ntoken by gathering information from the other tokens in the\n14223\nTransformer-MD\nPost1\n…………\n……\nDimension \nA\nttention\n…………\n……\nPost2\nPostn\nHn\nytUt\ny2\ny1\nU2\nU1\nH2\nH1\nFigure 3: Overview of our dimension attention module.\npost and the memory:\nHl\u00001\ni;mem = [Ml\u00001\ni \u000eHl\u00001\ni ] (3)\nQl\ni;Kl\ni;V l\ni = Hl\u00001\ni WT\nq ;Hl\u00001\ni;memWT\nk ;Hl\u00001\ni;memWT\nv (4)\nHl\ni = TransformerLayer\n\u0000\nQl\ni;Kl\ni;V l\ni\n\u0001\n(5)\nIn Equation (3), \u000edenotes the concatenation operation of\ntwo hidden states.\nDimension Attention\nAs described above, Transformer-MD put multiple posts\ntogether to obtain the post-related representations H 2\nRn\u0002d\u0002k for a user. It is also mentioned that different parts\nof a post may contribute to different personality dimensions.\nTherefore, we develop a dimension attention module to fur-\nther reﬁne the representation H and obtain t dimension-\nspeciﬁc vectors U = [ U1;U2;\u0001\u0001\u0001 ;Ut]. The architecture of\nthis module is shown in Figure 3. Following Lin et al. (2017)\nand Vu, Nguyen, and Nguyen (2020), we employ a multi-\nlayer perceptron to compute the weights as:\nHr = reshape (H) 2Rd\u0002(n\u0002k) (6)\nZr = tanh (WtHr) 2Rdt\u0002(n\u0002k) (7)\nAr = softmax (WaZr) 2Rt\u0002(n\u0002k) (8)\nHere, reshape in Equation (6) is a matrix deformation\nfunction, Wt 2Rdt\u0002d in Equation (7) and Wa 2Rt\u0002dt in\nEquation (8) are learnable parameters, where dt is a hyper-\nparameter to be tuned. The i-th row of the attention matrix\nAi\nr corresponds to the weights of the i-th dimension, which\nis then multiplied by the hidden states Hr to produce the\ndimension-speciﬁc representation Ui as:\n[U1;U2;\u0001\u0001\u0001 ;Ut] = [A1\nrHT\nr ;A2\nrHT\nr ;\u0001\u0001\u0001 ;At\nrHT\nr ]\nObjective Function\nFor each personality dimension i, we pass Ui to a single-\nlayer feed-forward network followed by a softmax function\nto generate the probabilities \u0016yi for this dimension. Then, the\ncross-entropy loss is used to measure the loss and the tdi-\nmensions are jointly optimized in a general approach:\nLoss(X;Y;\u0012 ) = 1\nt\ntX\ni=1\n\u0000yi log \u0016yi\nDataset T\nypes Train Validation Test\nKaggle\nI vs. E 4011 vs.\n1194 1326 vs. 409 1339 vs. 396\nS vs. N 727 vs. 4478 222 vs. 1513 248 vs. 1487\nT vs. F 2410 vs. 2795 791 vs. 944 780 vs. 955\nP vs. J 3096 vs. 2109 1063 vs. 672 1082 vs. 653\nPandora\nI vs. E 4278\nvs. 1162 1427 vs. 386 1437 vs. 377\nS vs. N 610 vs. 4830 208 vs. 1605 210 vs. 1604\nT vs. F 3549vs. 1891 1120 vs. 693 1182 vs. 632\nP vs. J 3211 vs. 2229 1043 vs. 770 1056 vs. 758\nTable 1: Statistics of the Kaggle and Pandora datasets.\nExperiments\nDatasets\nFollowing previous studies (Hernandez and Knight 2017;\nKeh, Cheng et al. 2019; Gjurkovi ´c et al. 2020), we conduct\nexperiments on the Kaggle2 and Pandora3 MBTI personality\ndatasets. While the former has 40-50 posts for each user, the\nlater has dozens to hundreds and is annotated based on self-\ndiagnoses of user (Gjurkovi´c et al. 2020). MBTI personality\ntype divides people’s personality into four dimensions, each\ncontaining two aspects: Introversion vs. Extroversion (I vs.\nE), Sensing vs. iNtuition (S vs. N), Think vs. Feeling (T\nvs. F), and Perception vs. Judging (P vs. J). Statistics of the\ndatasets are presented in Table 1. As Hernandez and Knight\n(2017), we remove the words (e.g., “INTP”) that match per-\nsonality labels from all posts to avoid information leaks.\nThen, we randomly split them into a 60-20-20 proportion\nfor training, validation, and testing, respectively. Due to the\nimbalanced distribution of labels, we use the macro-F1 met-\nric for evaluation.\nBaselines\nTo evaluate the proposed model intensively, we employ the\nfollowing mainstream models as baselines for comparison:\n• SVM and XGBoost (Amirhosseini and Kazemian\n2020): Posts are ﬁrstly concatenated into a new document,\nand then SVM or XGBoost is applied based on TF-IDF\nfeatures extracted from the document.\n• GloVe-LSTMmean (Tandera et al. 2017; Hernandez\nand Knight 2017): LSTM is adopted to independently en-\ncode each post with GloVe embeddings and then the mean\nof the post embeddings is taken to represent the user.\n• GRU-MLABERT (Lynn, Balasubramanian, and\nSchwartz 2020): Based on BERT embeddings, this model\nuses two-level GRUs to generate the post encodings and\nthe user representation, respectively.\n• BERTconcat (Zhou et al. 2019; Jiang, Zhang, and Choi\n2020): This method simply concatenates the posts of a\nuser into a long document and encodes it by BERT.\n2https://www.kaggle.com/datasnaek/mbti-type\n3https://psy.takelab.fer.hr/datasets/all/\n14224\nMethods Kaggle Pandora\nI vs. E S vs. N T vs. F P vs. J Average I vs. E S vs. N T vs. F P vs. J Average\nSVM 53.34 47.75 76.72 63.03 60.21 44.74 46.92 64.62 56.32 53.15\nXGBoost 56.67 52.85 75.42 65.94 62.72 45.99 48.93 63.51 55.55 53.50\nGloVe-LSTMmean 57.82 57.87 69.97 57.01 60.67 48.01 52.01 63.48 56.21 54.93\nGRU-MLABERT\u00004 64.75 60.24 75.17 62.89 65.76 54.60 49.19 61.82 53.64 54.81\nBERTconcat 58.33 53.88 69.36 60.88 60.61 54.22 49.15 58.31 53.14 53.71\nBERTconcat\u00005 61.72 58.74 71.72 59.83 63.00 53.32 49.94 60.46 55.31 54.76\nXLNetconcat 60.65 54.50 71.98 56.00 60.78 50.49 49.59 58.10 54.09 53.07\nBERTcls\u0000mean 63.50 55.34 78.55 66.06 65.86 53.35 50.56 64.06 56.83 56.20\nXLNetcls\u0000mean 62.53 61.93 77.19 64.84 66.62 52.66 48.75 68.66 57.14 56.80\nBERTDA 65.67 61.28 79.19 66.52 68.17 54.88 55.49 66.71 59.01 59.02\nXLNetDA 65.99 63.80 78.53 66.44 68.69 55.49 57.50 65.04 60.80 59.71\nTransformer-MDcls\u0000mean 67.80 63.67 78.83 64.62 68.73 54.78 55.51 67.28 59.89 59.37\nTransformer-MDDA 66.08 69.10 79.19 67.50 70.47 55.26 58.83 69.03 60.57 60.92\nTable 2: Overall results of different models in macro-F1(%), where the best results are shown in bold.\n• BERTcls\u0000mean (Keh, Cheng et al. 2019): BERT is used\nto encode each post individually, and the average of the\nCLS representations is used for the user representation.\n• XLNetconcat: This model is similar to BERTconcat\nbut uses XLNet to encode the concatenated document.\n• XLNetcls\u0000mean: This model is similar to\nBERTcls\u0000mean but uses XLNet as the post encoder.\nImplementation Details\nWe use Pytorch (Paszke et al. 2019) to implement all the\ndeep learning models on four 2080Ti GPU cards. For train-\ning, we use the Adam (Kingma and Ba 2014) optimizer with\nan initial learning rate \u000b= 2e-5 and a mini-batch size of 24.\nFollowing previous work, we set the max number of posts to\n50 for each user and the max length to 70 for each post. For\nGloVe-LSTM, we use the 300-dimensional GloVe word em-\nbeddings (Pennington, Socher, and Manning 2014) and set\nthe hidden size to 300. For BERT and XLNet, their parame-\nters are initialized by the bert-base-cased (Devlin et al. 2018)\nand xlnet-base-cased (Yang et al. 2019b) models, respec-\ntively. The hidden sizedt of our dimension attention module\nis set to 768. After training a ﬁxed number of epochs, we se-\nlect the model with the best macro-F1 on the validation set\nand evaluate its performance on the test set.\nOverall Performance\nThe overall results are shown in Table 2, in which the\nmodels are organized into two groups. The ﬁrst group are\nthe existing models (from SVM to BERTcls\u0000mean) and\nvariants (XLNetcls\u0000mean and XLNetconcat), while the sec-\nond group are models based on our Transformer-MD or\nDA module or both. We can observe that our ﬁnal model\n(Transformer-MDDA) achieves the highest average macro-\nF1 score, outperforming the existing state-of-the-art (SOTA)\nbaseline (BERTcls\u0000mean) by 4.61 and 4.72 on Kaggle and\nPandora, respectively. What’s more, in the same pre-trained\nsetting, our model also outperforms the variant of the SOTA\nbaseline (XLNetcls\u0000mean) by 3.85 and 4.12 on Kaggle and\nPandora, respectively. The results verify the effectiveness\nof our model in personality detection. Besides, we also\npresent a variant of our model by removing the DA mod-\nule, and the results (Transformer -MDcls\u0000mean) show that\nthis novel multi-document encoder alone still outperforms\nthe SOTA baseline (BERTcls\u0000mean) by 2.87 on Kaggle and\n3.17 on Pandora, and outperform the variant of SOTA base-\nline (XLNetcls\u0000mean) by 2.11 on Kaggle and 2.57 on Pan-\ndora, respectively, demonstrating that the post-related rep-\nresentations (encoded by Transformer-MD) are better than\npost-independent representations (encoded by BERT or XL-\nNet) for personality detection. A notable phenomenon of\nTransformer-MDcls\u0000mean is that the results for different\npersonality dimensions vary more obviously than our ﬁnal\nmodel. We speculate the reason is that the former repre-\nsents the whole personality dimensions with a shared rep-\nresentation which contains more information for the simple\ndimensions (“I/E” and “T/F”) than the difﬁcult dimensions\n(“S/N” and “P/J”). By contrast, our DA module can gener-\nate a speciﬁc representation for each dimension to alleviate\nthis problem. To further verify this, we implement the DA\nmodule with XLNet and BERT as the post encoder, respec-\ntively, and the results (BERTDA and XLNetDA) in Table 2\nconﬁrm that the performances are also improved.\nEffect of Post Order\nFrom Table 2 we can note that the models without post or-\nder (subscripted by “cls-mean”) are superior to those with\npost order (subscripted by “concat”). To further examine the\nnegative effect of post order, we take XLNet and BERT as\nan example and train XLNetconcat and BERTconcat in the\noriginal post order until they converge. We then run them on\nthe training set again with a new random post order 5 times\nand record the average scores. Ideally, given the same con-\ntent of posts, the personality type they represent should be\nconsistent no matter what order they are integrated in. How-\never, the results shown in Figure 4 suggest that it is not the\n14225\nFigure 4: Results of study on post order on Kaggle. Red\nbars denote the performance of XLNetconcat/BERTconcat\ntrained until converge on the training set and green bars de-\nnote the test set. Blue bars are the average performance of\nﬁve random post orders on the training set.\ncase, as the performance of XLNetconcat and BERTconcat\non the training set are greatly reduced after altering the ini-\ntial post order. This implies that the initial models tend to\noverﬁt the extra post-order bias for each user, affecting their\ngeneralization ability. A feasible remedy for this problem is\nto concatenate posts in different orders to generate differ-\nent new documents and to reduce the post-order bias. As the\nresults (BERTconcat-5) shown in Table 2, there are indeed\ncertain amounts of improvement observed when each set of\nposts from the same users are organized into different docu-\nments in random orders. However, with each user containing\ndozens to hundreds of posts, the number of combinations is\nexponentially increasing. Thus, an order-agnostic approach\nis more desirable for personality detection.\nAnalysis of Transformer-MD\nAblation Study We conduct an ablation study to investigate\nthe effects of the multi-document fusion layer (MFL), the\nshared position embedding, and the segment embedding in\nTransformer-MD. As shown in Table 3, the performance of\nthe model drops visibly after removing any of them. Specif-\nically, the performance drops by 1.78 when the MFL is re-\nmoved, demonstrating that this module contributes consid-\nerably to personality detection. Moreover, when the shared\nposition embedding or the segment embedding is removed,\nthe performance drops by 0.99 and 1.00 respectively, which\nshows that it is crucial for the MFL module to ignore the\npost order bias and distinguish between different posts when\nputting them together for personality detection.\nEffect of Post Connection One of the roles of our Trans-\nformer-MD is to model the connection between posts. To\nverify the effectiveness, we investigate the performance of\nour model while keeping only the top \u0015percent of connec-\ntions between posts learned by our model (in terms of atten-\ntion weights on memory tokens). As the results show in Fig-\nure 5, the improvements of Transformer-MD over the base-\nlines mainly come from the 60% links, while the remaining\n40% add little impact to the performance or even cause a\nslight degradation. This demonstrates that multi-head self-\nattention can effectively establish positive post connections\nand ignores useless or noisy ones by assigning different\nMethods macro-F1\n(%)\nI vs.E S vs.\nN T vs.F P vs.J Av\ne.\nTransformer-\nMDDA 66.08 69.10 79.19 67.50 70.47\n–MFL 65.99 63.80\n78.53 66.44 68.69\n–share-position 66.13 66.70\n79.03 66.05 69.48\n–segment 65.12 66.99\n79.70 66.07 69.47\nTable 3: Results of ablation study of Transformer-MD on\nKaggle, where “–” denotes the removal of a component.\nFigure 5: Performance of Transformer-MD on Kaggle when\nkeeping only the top-\u0015percent links of posts.\nweights. Another interesting observation is that the top 20%\nlinks can improve the model performance quickly, implying\nthat high-quality connections are extremely predictive.\nCase Study To further study what kind of connections are\npositive, we examine the memory attention weights gener-\nated by Equation (3)-(5) for a user with INFP personality\ntypes in the Kaggle dataset. We then pick out the posts with\nhigh-weight links and visualize them. As shown in Figure\n6, positive connections are those having similar emotions,\nthemes, or viewpoints which ensure the information in mul-\ntiple posts can be put together to depict a proﬁle.\nEffect of High-Level Encoder Layers We also investigate\nhow the number of layers for the low-level and high-level\nencoders affects the performance. To this end, we change\nthe number of layers of the high-level encoder from 0 to\n6 and plot the results in Figure 7. We can observe that\nTransformer-MDDA achieves the best performance when\nthis number is set to 3. This implies that Transformer-MD\nneeds about 9 layers to aggregate post information into CLS\nand another 3 layers to model the connection between posts.\nGranularity of Dimension Attention\nOur DA module gathers information at word-level rather\nthan post-level, for word-level contains more information\nfor multi-dimension personality detection. To verify this, we\ncompare the DA module with a post-level attention method\n(CLS-Attn) that gathers information from only the CLS to-\nkens of all posts. As shown in Table 4, our DA module out-\nperforms the other approaches across all the post encoders.\n14226\nFrustrated, drained, lonely, bitter. All the good stuff that comes \nfrom excessive solitude. And I've only just begun my break! Ah, \nthis is not going to be fun. Aching already.\nI \nlisten to metal and classical and read fantasy (preferably the \ndarker, grittier kind). Not sure if that's common for 4w5s.\nIt's pathetic, but deep down I have this childish hope that \nsomeone will come along and rescue me. The years show me \njust how wrong that idea is. \nLiving on my own would probably be pretty awesome, though \nI'm sure there would be downsides that I'm overlooking. As \nlong as I had friends nearby it would be great.\nYeah, supposedly this is the 's natural defense against people \nthey don't like/don't want to be around. When my wall is up I \nact in a more formal/impersonal way.\nOf course, I don't think that metal heads are the only creative \npeople around. Even within that study it showed there were \nother genres of music that creative people are drawn to. \nUbiquitous Sloth Yeah, The Mantle is great. Did you check out \ntheir latest album? It was really good too. Narcissist Duuuude, \nOctober Falls. Ja listen to The Womb Of Primordial Nature?\nI wouldn't really call Agalloch post metal, but they have some \npost rock influences. Check them out, they're amazing.\nI don't really think I intimidate anyone - I'm too friendly most of \nthe time, and if someone bothers me I'll try and work it out in my \nhead or avoid them. In a confrontation I'll try to...\nFigure 6: Links of posts established by Transformer-MD.\nFigure 7: Performance of Transformer-MDDA with differ-\nent numbers of layers for the high-level encoder on Kaggle.\nError Analysis\nThe results in Table 2 show that our model achieves the best\nperformance in all personality dimensions except “I/E”. We\nﬁnally conduct an error analysis using Transformer-MD DA\non the Kaggle dataset. Speciﬁcally, we record the macro-F1\nscores of this model on the validation set during training. As\nshown in Figure 8, the model converges faster in the “I/E”\nand “T/F” personality dimensions than in “S/N” and “P/J”.\nThe best epoch range for “I/E” and “T/F” is from 4 to 7 but\n7 to 10 for “S/N” and “P/J”. Besides, the “T/F” dimension\nnot only converges fast but also keeps good performance in\nthe later epochs. The “S/N”, “T/F” and “P/J” personality di-\nmensions all achieve the best performance at the 9-th epoch,\nbut “I/E” is slightly worse in this setting than its best epoch\nMethods Macro-F1 (%)\nBERT XLNet Transformer-MD\nCLS-Mean 65.86 66.62 68.73\nCLS-Attn 66.37 67.19 69.56\nDA 68.17 68.69 70.47\nTable 4: Results of mean (CLS-Mean), word-level attention\n(DA) and post-level attention (CLS-Attn) on Kaggle.\nFigure 8: Performance of Transformer-MDDA in different\npersonality dimensions as the epoch increases.\n(7-th). This phenomenon may indicate that personality di-\nmensions have different training difﬁculties. The reason why\nTransformer-MDDA performs less satisﬁed in the “I/E” di-\nmension is probably that the model overﬁts this dimension\nquickly compared with the other dimensions.\nConclusion\nIn this paper, we proposed a multi-document Transformer,\nTransformer-MD, for personality detection. Transformer-\nMD intends to put together information in different posts to\ndepict a personality proﬁle for each user without introducing\npost orders. To this aim, it ﬁrst encodes each post indepen-\ndently to obtain post-independent representations. Then, it\ngenerates post-related representations by disorderly fusing\ninformation from other posts. On top of Transformer-MD,\nwe further proposed a dimension attention mechanism to\ngenerate a trait-speciﬁc representation for each personality\ndimension. Experimental results on two datasets show that\ncombining Transformer-MD and dimension attention leads\nto a model that outperforms the baselines signiﬁcantly.\nAcknowledgments\nWe thank the anonymous reviewers for their constructive re-\nviews. This work was supported by the Fundamental Re-\nsearch Funds for the Central Universities (No.19lgpy220),\nthe Program for Guangdong Introducing Innovative and En-\ntrepreneurial Teams (No.2017ZT07X355), and the National\nNatural Science Foundation of China (No. 61906217).\nReferences\nAmirhosseini, M. H.; and Kazemian, H. 2020. Machine\nLearning Approach to Personality Type Prediction Based on\n14227\nthe Myers–Briggs Type Indicator®. Multimodal Technolo-\ngies and Interaction 4(1): 9.\nBagby, R. M.; Joffe, R. T.; Parker, J. D. A.; Kalemba, V .;\nand Harkness, K. L. 1995. Major Depression and the Five-\nFactor Model of Personality. Journal of Affective Disorders\n38(2-3): 89–95.\nDai, Z.; Yang, Z.; Yang, Y .; Carbonell, J.; Le, Q.; and\nSalakhutdinov, R. 2019. Transformer-XL: Attentive Lan-\nguage Models beyond a Fixed-Length Context. In Proceed-\nings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics, 2978–2988. Florence, Italy: Associ-\nation for Computational Linguistics. doi:10.18653/v1/P19-\n1285.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805 .\nDing, M.; Zhou, C.; Chen, Q.; Yang, H.; and Tang, J. 2019.\nCognitive graph for multi-hop reading comprehension at\nscale. arXiv preprint arXiv:1905.05460 .\nFriedman, H. S.; and Kern, M. L. 2014. Personality, Well-\nBeing, and Health*. Annual Review of Psychology 65(1):\n719.\nFung, P.; Dey, A.; Siddique, F. B.; Lin, R.; Yang, Y .; Bert-\nero, D.; Wan, Y .; Chan, R. H. Y .; and Wu, C.-S. 2016. Zara:\nA virtual interactive dialogue system incorporating emotion,\nsentiment and personality recognition. In Proceedings of\nCOLING 2016, the 26th International Conference on Com-\nputational Linguistics: System Demonstrations, 278–281.\nGjurkovi´c, M.; Karan, M.; Vukojevi ´c, I.; Bo ˇsnjak, M.; and\nˇSnajder, J. 2020. PANDORA Talks: Personality and Demo-\ngraphics on Reddit. arXiv preprint arXiv:2004.04460 .\nHernandez, R.; and Knight, I. 2017. Predicting Myers-\nBridge Type Indicator with text classiﬁcation. In Proceed-\nings of the 31st Conference on Neural Information Process-\ning Systems, Long Beach, CA, USA, 4–9.\nJiang, H.; Zhang, X.; and Choi, J. D. 2020. Automatic Text-\nBased Personality Recognition on Monologues and Multi-\nparty Dialogues Using Attentive Networks and Contextual\nEmbeddings (Student Abstract). In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, volume 34, 13821–\n13822.\nKeh, S. S.; Cheng, I.; et al. 2019. Myers-Briggs Personal-\nity Classiﬁcation and Personality-Speciﬁc Language Gener-\nation Using Pre-trained Language Models. arXiv preprint\narXiv:1907.06333 .\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980 .\nLin, Z.; Feng, M.; Santos, C. N. d.; Yu, M.; Xiang, B.; Zhou,\nB.; and Bengio, Y . 2017. A structured self-attentive sentence\nembedding. arXiv preprint arXiv:1703.03130 .\nLiu, Z.; Xiong, C.; Sun, M.; and Liu, Z. 2020. Fine-\ngrained Fact Veriﬁcation with Kernel Graph Attention Net-\nwork. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, 7342–7351.\nOnline: Association for Computational Linguistics. doi:\n10.18653/v1/2020.acl-main.655.\nLynn, V .; Balasubramanian, N.; and Schwartz, H. A. 2020.\nHierarchical Modeling for User Personality Prediction: The\nRole of Message-Level Attention. In Proceedings of the\n58th Annual Meeting of the Association for Computational\nLinguistics, 5306–5316.\nLynn, V .; Son, Y .; Kulkarni, V .; Balasubramanian, N.; and\nSchwartz, H. A. 2017. Human Centered NLP with User-\nFactor Adaptation. In Proceedings of the 2017 Conference\non Empirical Methods in Natural Language Processing ,\n1146–1155. Copenhagen, Denmark: Association for Com-\nputational Linguistics. doi:10.18653/v1/D17-1119.\nMatz, S. C.; Kosinski, M.; Nave, G.; and Stillwell, D. J.\n2017. Psychological targeting as an effective approach\nto digital mass persuasion. Proceedings of the National\nAcademy of ences of the United States of America 12714–\n12719.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.;\net al. 2019. Pytorch: An imperative style, high-performance\ndeep learning library. arXiv preprint arXiv:1912.01703 .\nPennebaker, J. W.; Francis, M. E.; and Booth, R. J. 2001.\nLinguistic inquiry and word count: LIWC 2001. Mahway:\nLawrence Erlbaum Associates 71(2001): 2001.\nPennington, J.; Socher, R.; and Manning, C. D. 2014. Glove:\nGlobal vectors for word representation. In Proceedings of\nthe 2014 conference on empirical methods in natural lan-\nguage processing (EMNLP), 1532–1543.\nSchwartz, H. A.; Eichstaedt, J. C.; Kern, M. L.; Dziurzyn-\nski, L.; Ramones, S. M.; Agrawal, M.; Shah, A.; Kosinski,\nM.; Stillwell, D.; Seligman, M. E.; et al. 2013. Personality,\ngender, and age in the language of social media: The open-\nvocabulary approach. PloS one 8(9): e73791.\nTandera, T.; Suhartono, D.; Wongso, R.; Prasetio, Y . L.; et al.\n2017. Personality prediction system from facebook users.\nProcedia computer science 116: 604–611.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems 30: 5998–6008.\nVu, T.; Nguyen, D. Q.; and Nguyen, A. 2020. A Label At-\ntention Model for ICD Coding from Clinical Text. arXiv\npreprint arXiv:2007.06351 .\nXue, D.; Wu, L.; Hong, Z.; Guo, S.; Gao, L.; Wu, Z.; Zhong,\nX.; and Sun, J. 2018. Deep learning-based personality\nrecognition from text posts of online social networks. Ap-\nplied Intelligence 48(11): 4232–4246.\nYang, P.; Luo, F.; Ma, S.; Lin, J.; and Sun, X. 2019a. A Deep\nReinforced Sequence-to-Set Model for Multi-Label Classi-\nﬁcation. In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, 5252–5258. Flo-\nrence, Italy: Association for Computational Linguistics. doi:\n10.18653/v1/P19-1518.\n14228\nYang, P.; Sun, X.; Li, W.; Ma, S.; Wu, W.; and Wang, H.\n2018. SGM: sequence generation model for multi-label clas-\nsiﬁcation. arXiv preprint arXiv:1806.04822 .\nYang, Z.; Dai, Z.; Yang, Y .; Carbonell, J.; Salakhutdinov,\nR. R.; and Le, Q. V . 2019b. Xlnet: Generalized autoregres-\nsive pretraining for language understanding. In Advances in\nneural information processing systems, 5753–5763.\nYarkoni, T. 2010. Personality in 100,000 words: A large-\nscale analysis of personality and word use among bloggers.\nJournal of research in personality 44(3): 363–373.\nZhao, C.; Xiong, C.; Rosset, C.; Song, X.; Bennett, P.; and\nTiwary, S. 2019. Transformer-XH: Multi-Evidence Reason-\ning with eXtra Hop Attention. In International Conference\non Learning Representations.\nZhou, J.; Han, X.; Yang, C.; Liu, Z.; Wang, L.; Li, C.;\nand Sun, M. 2019. GEAR: Graph-based evidence aggre-\ngating and reasoning for fact veriﬁcation. arXiv preprint\narXiv:1908.01843 .\n14229",
  "topic": "Personality",
  "concepts": [
    {
      "name": "Personality",
      "score": 0.738806962966919
    },
    {
      "name": "Transformer",
      "score": 0.7162067890167236
    },
    {
      "name": "Computer science",
      "score": 0.6857700347900391
    },
    {
      "name": "Trait",
      "score": 0.6150171756744385
    },
    {
      "name": "ENCODE",
      "score": 0.5763261318206787
    },
    {
      "name": "Big Five personality traits",
      "score": 0.5635442733764648
    },
    {
      "name": "Embedding",
      "score": 0.5442067384719849
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3671914339065552
    },
    {
      "name": "Information retrieval",
      "score": 0.3227608799934387
    },
    {
      "name": "Psychology",
      "score": 0.20253252983093262
    },
    {
      "name": "Social psychology",
      "score": 0.17998585104942322
    },
    {
      "name": "Engineering",
      "score": 0.09398773312568665
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ]
}