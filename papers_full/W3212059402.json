{
  "title": "HiTRANS: A Hierarchical Transformer Network for Nested Named Entity Recognition",
  "url": "https://openalex.org/W3212059402",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2101675635",
      "name": "Zhiwei Yang",
      "affiliations": [
        "Jilin University",
        "Hong Kong Baptist University",
        "Jilin Province Science and Technology Department"
      ]
    },
    {
      "id": "https://openalex.org/A2010371457",
      "name": "Jing Ma",
      "affiliations": [
        "Hong Kong Baptist University"
      ]
    },
    {
      "id": "https://openalex.org/A2679801196",
      "name": "Hechang Chen",
      "affiliations": [
        "Jilin University"
      ]
    },
    {
      "id": "https://openalex.org/A2126876975",
      "name": "Yunke Zhang",
      "affiliations": [
        "Jilin University"
      ]
    },
    {
      "id": "https://openalex.org/A2100158142",
      "name": "Yi Chang",
      "affiliations": [
        "Jilin University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2804221886",
    "https://openalex.org/W2047477415",
    "https://openalex.org/W2949663785",
    "https://openalex.org/W2950281195",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2803609931",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2892252202",
    "https://openalex.org/W2163107094",
    "https://openalex.org/W3034744126",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3090015871",
    "https://openalex.org/W2949257977",
    "https://openalex.org/W2963568202",
    "https://openalex.org/W2250710764",
    "https://openalex.org/W3105491236",
    "https://openalex.org/W2952594430",
    "https://openalex.org/W3035625205",
    "https://openalex.org/W2971019153",
    "https://openalex.org/W2998456908",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2515248967",
    "https://openalex.org/W3102603416",
    "https://openalex.org/W3035543689",
    "https://openalex.org/W2963571583",
    "https://openalex.org/W2891570996",
    "https://openalex.org/W2407338347",
    "https://openalex.org/W3034732033",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "Nested Named Entity Recognition (NNER) has been extensively studied, aiming to identify all nested entities from potential spans (i.e., one or more continuous tokens). However, recent studies for NNER either focus on tedious tagging schemas or utilize complex structures, which fail to learn effective span representations from the input sentence with highly nested entities. Intuitively, explicit span representations will contribute to NNER due to the rich context information they contain. In this study, we propose a Hierarchical Transformer (HiTRANS) network for the NNER task, which decomposes the input sentence into multi-grained spans and enhances the representation learning in a hierarchical manner. Specifically, we first utilize a two-phase module to generate span representations by aggregating context information based on a bottom-up and top-down transformer network. Then a label prediction layer is designed to recognize nested entities hierarchically, which naturally explores semantic dependencies among different spans. Experiments on GENIA, ACE-2004, ACE-2005 and NNE datasets demonstrate that our proposed method achieves much better performance than the state-of-the-art approaches.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 124–132\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n124\nHiTRANS : A Hierarchical Transformer Network\nfor Nested Named Entity Recognition\nZhiwei Yang1,2,3,4, Jing Ma2, Hechang Chen3,4,∗, Yunke Zhang3, Yi Chang3,4,∗\n1 College of Computer Science and Technology, Jilin University, Changchun, China\n2 Department of Computer Science, Hong Kong Baptist University, Hong Kong, China\n3 School of Artiﬁcial Intelligence, Jilin University, Changchun, China\n4 Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education\nyangzw18@mails.jlu.edu.cn, majing@comp.hkbu.edu.cn,\nchenhc@jlu.edu.cn, zhangyk19@mails.jlu.edu.cn, yichang@jlu.edu.cn\nAbstract\nNested Named Entity Recognition (NNER)\nhas been extensively studied, aiming to iden-\ntify all nested entities from potential spans\n(i.e., one or more continuous tokens). How-\never, recent studies for NNER either focus\non tedious tagging schemas or utilize com-\nplex structures, which fail to learn effective\nspan representations from the input sentence\nwith highly nested entities. Intuitively, ex-\nplicit span representations will contribute to\nNNER due to the rich context information\nthey contain. In this study, we propose\na Hierarchical Transformer (HiT RANS ) net-\nwork for the NNER task, which decomposes\nthe input sentence into multi-grained spans\nand enhances the representation learning in a\nhierarchical manner. Speciﬁcally, we ﬁrst uti-\nlize a two-phase module to generate span rep-\nresentations by aggregating context informa-\ntion based on a bottom-up and top-down trans-\nformer network. Then a label prediction layer\nis designed to recognize nested entities hi-\nerarchically, which naturally explores seman-\ntic dependencies among different spans. Ex-\nperiments on GENIA, ACE-2004, ACE-2005\nand NNE datasets demonstrate that our pro-\nposed method achieves much better perfor-\nmance than the state-of-the-art approaches.\n1 Introduction\nNamed entity recognition (NER) is an essential\ntask in the research of natural language processing,\nwhich aims to detect and classify text spans into\ncorresponding semantic categories, e.g., Person\n(PER), Organization (ORG), and Location (LOC),\nfrom a chunk of text. Most existing studies focus\non ﬂat NER, i.e., without nested entities, by se-\nquence labeling methods (Yang et al., 2020; Yoon\net al., 2019). However, named entities are generally\nnested with each other in the real world (Finkel\nand Manning, 2009). For example, in Figure 1, the\n∗Corresponding authors.\nFigure 1: An example of nested named entities. The\nshort entities with red labels are nested in long entities\nwith blue labels, respectively, in a hierarchical manner.\nentity “St. Louis” with Label “CITY” is nested in\n“St. Louis Cardinals” with Label “SPORTSTEAM”.\nThis poses a major technical challenge to the pre-\nvious methods and thus a more robust model for\nnested NER (NNER) is urgently desirable.\nPrevious literature for NNER can be roughly\ncategorized into three types: 1) hypergraph-\nbased models focus on designing a complex hy-\npergraph structure to obtain an expressive tag-\nging schema (Straková et al., 2019; Katiyar and\nCardie, 2018; Lu and Roth, 2015), which are\ntime-consuming when encountering ambiguous\nschemas; 2) span-based models tend to detect can-\ndidate spans from an input sentence ﬁrst, and then\ntrain a classiﬁer to predict entity categories (Luo\nand Zhao, 2020; Zheng et al., 2019). However,\nit is hard to get a complete meaning of the sen-\ntence because each text span contains only a part\nof the semantics, and errors may propagate to the\nprediction stage if the span boundary is divided\nincorrectly at the ﬁrst stage; and 3) layered-based\nmodels are proposed to utilize layered structures\nto deal with NNER based on the divide and con-\nquer strategy (Jue et al., 2020; Xia et al., 2019; Ju\net al., 2018). However, it merely breaks down the\ncomplex problem into several smaller subtasks and\npays little attention to the hierarchical representa-\ntion learning for multi-grained named entities.\nTo this end, we propose a novel hierarchical\ntransformer network (HiTRANS ) to recognize more\nnamed entities (either nested or not) for a given sen-\ntence, where we capture the dependencies of adja-\ncent candidate spans and utilizes an attention mech-\n125\nanism to enhance the representation of text spans.\nMore speciﬁcally, the input of our proposed method\ncombines character-level, word-level, and sentence-\nlevel representations, which are obtained by three\nembedding networks, respectively. We then pro-\npose a two-phase span generation model (SGM)\non top of the multi-level representations, which\nhierarchically aggregates adjacent spans based on\ntransformer mechanism at each layer. The SGM\nincludes a bottom-up and a top-down structure to\nenhance the representation learning of each candi-\ndate span which is ﬁnally fed into a label prediction\nlayer to assign an entity class for the span. As a re-\nsult, nested entities are comprehensively contained\nin the candidate spans and the representation learn-\ning is further enhanced based on both multi-level\nembedding and the hierarchical transformer mech-\nanisms. Experimental results on FOUR datasets\ndemonstrate that HiTRANS establishes new state-\nof-the-art performances, which veriﬁes the effec-\ntiveness of our proposed framework. The main\ncontributions of this work are as follows:\n• We propose a novel hierarchical transformer\nframework (Hi TRANS ) for NNER, which\nis superior in modeling the nested relations\namong multi-grained named entities and learn-\ning more effective representations.\n• Entity representation learning is formulated\nas a two-phase span generation, which aggre-\ngates context information of adjacent spans\nin a bottom-up and top-down manner respec-\ntively. The span representation is enhanced by\nmulti-level features and context information.\n• The overall superiority of our Hi TRANS is\nvalidated across four benchmarks comparing\nwith state-of-the-art methods. Visualization\nand case study conducted on top of the out-\nputs from each layer further shows an in-depth\nunderstanding of our method.\n2 Related Work\nWe brieﬂy review some prior works closely related\nto ours from three perspectives: hypergraph-based,\nspan-based, and layered-based approaches.\nHypergraph-based approaches obtain expressive\ntagging schemas for NNER (Lu and Roth, 2015;\nWang and Lu, 2018). However, the hypergraph\nrequires speciﬁc modules designed to prevent the\nspurious structure of hypergraphs. Muis and Lu\n(2017) introduced mention separators to facilitate\nmulti-graph representation. Katiyar and Cardie\n(2018) further improved the result using features ex-\ntracted from a recurrent neural network. Recently,\nStraková et al. (2019) proposed two competitive\nneural networks using a linearized scheme. How-\never, more expressive and unambiguous schemas\nwill inevitably cause higher time complexity.\nSpan-based methods achieve promising results\nfor NNER (Tan et al., 2020; Zheng et al., 2019;\nSohrab and Miwa, 2018), which explicitly enumer-\nate all possible spans from input sentences, which\nwill be fed into a classiﬁer for category prediction\nbased on multitask learning. Lin et al. (2019) pro-\nposed a sequence-to-nuggets architecture to recog-\nnize nested entities with semantic central words. Li\net al. (2020) extracted answer spans from a passage\nthrough a given question. Luo and Zhao (2020)\nproposed a novel bipartite ﬂat-graph network to\nlearn the dependencies of inner spans. But most\nof these methods generally break input sequences\ninto fragments, leading to inferior semantics.\nLayered-based models are recently proposed,\ne.g., Finkel and Manning (2009) constructed a syn-\ntactic constituency tree to transform each sentence\ninto a tree,Wang et al. (2018) proposed a transition-\nbased model by mapping a sentence with nested\nmentions to a designated forest, Fisher and Vlachos\n(2019) and Ju et al. (2018) dynamically stacked\nmultiple ﬂat NER layers from inside to outside,\nShibuya and Hovy (2020) introduced a decoding\nmethod that iteratively recognizes entities in an\noutside-to-inside way, Jue et al. (2020) and Xia\net al. (2019) utilized a layered model to recursively\nidentify named entity candidates based on a hi-\nerarchical structure, which is suitable for NNER.\nHowever, few of them emphasize on learning more\neffective span representations, failing to recognize\nnested named entities in more complex sentences.\nThe core idea of our work is inspired to en-\nhance representation learning for more complex\nsentences. We propose to leverage the representa-\ntion power of transformer based on a hierarchical\nstructure for improving NNER. Particularly, pre-\ntrained word embeddings such as GloVe (Penning-\nton et al., 2014), and pre-trained sentence-level\nembedding such as BERT (Devlin et al., 2019) and\nALBERT (Lan et al., 2020) have proven to be ef-\nfective to NER. In this paper, we will apply both\nkinds of embeddings besides character embeddings\nto further improve the performance.\n126\nFigure 2: An overview of our HiT RANS model. (a) The character-level embedding (CL-EMB), word-level em-\nbedding (WL-EMB), and sentence-level embedding (SL-EMB) are concatenated for better representations. (b)\nThere are two phases in span generation model (SGM) network, which iteratively generate span representations\nfor each layer by merging adjacent spans in a bottom-up and a top-down manner, respectively. “P” denotes the\npadding when employing CNN and the outputs “*t spans\" denote the representations of candidate spans at each\nlayer, where !is set to 6 in the above example. (c) During hierarchical label prediction (HLP), the same labeling\nnetwork, e.g., !4, is employed in each layer. (d) Output entities. Different layers are displayed in different colors.\n3 Our Proposed Method\nPrior hypergraph-based and span-based methods\nfor NNER suffer from ambiguous schemas or\nerrors propagation in complex sentences, thus\nlayered-based models are proposed to decompose\nthe problem into several smaller subtasks. How-\never, for NNER, learning effective representations\nand modeling inter-entity dependencies is still a\nsubstantial challenge. In this study, we hypothesis\nthat nested entities in the same context are comple-\nmentary and the text representation at multi-level\ncould improve NNER.\nGiven an input sentence Sis composed of a\nsequence of words, i.e., S= {F1,F2,...,F |S|},\nwhere |S|denotes the number of words. For the\nNNER task, each word F8 is associated with mul-\ntiple BIO2-format 1 labels Y8 = {Y1\n8,Y2\n8,..., Y!\n8 },\nwhere !denotes the maximum nesting depth. Note\nthat if != 1, a word F8 is associated with one cate-\ngorical label, which is regarded as ﬂat NER. There-\nfore, we formulate NNER as a multi-layer predic-\ntion problem. Speciﬁcally, the topmost layer is\nprocessed as ﬂat NER, and other layers merely us-\ning \u0017−{2;0BB}and $labels to recognize complete\nentities from text spans. For each layer, it modeled\nas sequence labeling, that is, 5; : e1e2 ··· e) →\n1B-, I-, 0indicate the beginning, intermediate, and outer\nposition of an entity, respectively, and 2;0BB indicates a cat-\negorical label takes from a pre-deﬁned tag set, e.g., Person,\nLocation, or Organization.\nH1H2 ··· H), where ei indicates the representation\nof a text span (i.e., one or more continuous words)\niteratively generated from the previous layer, )\nindicates the number of spans in the ;-th layer\n(1 ≤; ≤!).\nIn the following subsections, we will introduce\nour proposed HiTRANS , which consists of three\nparts: Multi-level Representation, Span Generation\nModel, and Hierarchical Label Prediction. Figure\n2 gives an overview of our framework.\n3.1 Multi-level Representation\nTo better capture the semantic information of a\nsentence, we learn token representations from mul-\ntiple levels, e.g., character level, word level, and\nsentence level. As Figure 2 (a) shows, given a\nsentence composed of a sequence of words S=\n{F1,F2,...,F |S|}and 289 denotes the 9-th char-\nacter within the 8-th word F8. For the 8-th word, the\nmulti-level representation is represented as follows:\nx8 = [x2\n8; xF\n8 ; xB\n8] (1)\nwhere x2\n8 denotes the character-level representa-\ntion within F8. As each word can be regarded as a\ncharacter sequence, randomly initialized character\nembeddings are encoded by a bidirectional LSTM\nlayer (Zheng et al., 2019) to capture sequential fea-\ntures in the context, then we use the last hidden\nstate as x2\n8. xF\n8 denotes the word-level representa-\ntion obtained from GloVe (Pennington et al., 2014)\n127\nfor the 8-th word F8; and xB\n8 denotes the sentence-\nlevel representation obtained from pretrained lan-\nguage model, e.g., BERT and ALBERT. And [; ]\ndenotes concatenation. Furthermore, a dense layer\nis applied to reduce the embedding dimension, i.e.,\nx8 →e8. Thus, we can obtain the span representa-\ntion in the ;-th layer as Hl = {e1,e2,..., eT}, where\n) is the span number.\nIn order to learn more effective span repre-\nsentations in Figure 2 (b), we further adapt the\nmulti-head attention mechanism from Transformer\n(Vaswani et al., 2017) in each layer of HiTRANS , as\nillustrated in Figure 3. Speciﬁcally, in the;-th layer,\nHiTRANS ﬁrst transforms the multi-level represen-\ntation Hl into multiple subspaces with different\nlinear projections:\nQℎ,Kℎ,Vℎ = H;W&\nℎ,H;W \nℎ ,H;W+\nℎ (2)\nwhere {Qℎ,Kℎ,Vℎ}are respectively the query,\nkey, and value representations with trainable pa-\nrameters {W&\nℎ,W \nℎ ,W+\nℎ }corresponding to the ℎ-\nth head. Then, the attention functions are applied\nto reﬁne the span representations.\nHl\nℎ = softmax(\nQhKh)\n√3ℎ\n)Vℎ (3)\nwhere Hl\nℎ is the ℎ-th head with 3ℎ as the dimension.\nFurthermore, we concatenate the output representa-\ntions of all these heads with the residual connection\nto capture global semantic information in parallel,\nwhich is as follows:\nH; = [H;\n1; H;\n2; ... ; H;\n=]W$ +H; (4)\nwhere H; ∈ℝT×=3ℎ is the ﬁnal span representation\nin the ;-th layer, =is the number of parallel heads,\nand W$ is a trainable parameter. For example, H1\nindicates the reﬁned span representations for the\nﬁrst layer at Phase 1 of Figure 2 (b).\n3.2 Span Generation Model\nTo extract nested entities from nested-structure sen-\ntences, we design a hierarchical span generation\nmodel (SGM) consisting of two phases to generate\ncandidate spans for the NNER, as shown in Figure\n2 (b). Speciﬁcally, the two phases are composed of\n!layers that respectively generate candidate spans\nin a Bottom-Up and Top-Down manner (i.e., BU-\nSGM and TD-SGM) in sequence. In each layer\nof SGM, a convolution neural network (CNN) is\nﬁrstly utilized to aggregate two adjacent spans for\nFigure 3: Detailed structure of candidate span genera-\ntion for Layer 4. (a) the reﬁned span representations\nfrom Layer 3 at Phase 1; (b) the reﬁned the representa-\ntions from Layer 5 at Phase 2.\nthe next layer which generates all possible ﬂat en-\ntities as candidates for further prediction. Then a\nmulti-head attention layer is utilized to enhance\nthe representation learning of each candidate. The\ndetails of each component will be described below.\nBU-SGM. The core idea of BU-SGM network\nis to generate feature vectors for candidate spans\nby recursively stacking convolutional neural net-\nworks from the bottom layer to the top layer as\nshown at Phase 1 of SGM in Figure 2 (b). Specif-\nically, the generated span representations in the\nﬁrst layer correspond to 1-token entities. As for\nhigher layers, a CNN with a kernel size of 2 is\niteratively applied to generate continuous ;-token\nspan representations from the (;−1)-th layer in a\nbottom-up manner, which avoids breaking the con-\nsecutive context. The span representations in the\n;-th layer can be obtained in a bottom-up manner:\nˆHl =\n{\nf(H;) , ; = 1\nf(Conv(ˆHl−1)) , 1 <; ≤! (5)\nwhere f(·)indicates the shorthand of Equation (4),\nH; denotes the reﬁned multi-level representation\nobtained from the ﬁrst layer, and ˆHl denotes span\nrepresentations in Layer ; generated iteratively\nfrom Layer ;−1. It is noted that stacking CNN will\nlead to the length reduced by 1 in each layer. Be-\nsides, a ReLU and Norm layer is applied to obtain\nthe ﬁnal span representations.\nTD-SGM. In the opposite direction, as long\nentities at higher-layer are closely related to short\nentities at lower-layer in the same context, high-\nlevel features can contribute to identifying entities\nin lower layers by providing additional background\ninformation, which is complementary with low-\nlevel features. Therefore, TD-SGM network aims\n128\nto propagate the higher-layer information to lower\nlayers in a top-down manner, which is initialized\nby 0 and guided with the output from the corre-\nsponding layer of Phase 1. Speciﬁcally, the span\nrepresentations generated at Phase 2 is iteratively\nobtained by stacking CNNs (with a kernel size of 2)\nwith proper zero-paddings in a top-down manner.\nFor example, as Figure 2 (b) shows, the span repre-\nsentations in Layer 4 at Phase 2, i.e.{e\n′\n1,e\n′\n2,e\n′\n3}, are\ngenerated from the span representations in Layer 5,\ni.e., {0,e\n′\n1,e\n′\n2,0}, which are obtained by concate-\nnating the span representations in Layer 5 at Phase\n1 and Phase 2, and then padding with zeros. Simi-\nlarly, the span representations in the ;-th layer can\nbe generated in a top-down manner as follows:\nˇH; =\n{\nf([0; ˆH;]) , ; = !\nf(Conv\n′\n([ˇH;+1; ˆH;+1])) , 1 ≤; < !(6)\nwhere ˇH; denotes span representations in Layer ;\ngenerated from Layer ;+1, 0 denotes zero tensor\nfor initializing the top-layer representation.\n3.3 Hierarchical Label Prediction\nTo recognize named entities from candidate spans,\na hierarchical label prediction (HLP) network is\nintroduced, as shown in Layer 4, Figure 2 (c). First,\nthe outputs of Phase 1 and Phase 2 are concate-\nnated as the ﬁnal candidate span representations\nto combine bidirectional features into a global in-\nformative representation. Formally, the ﬁnal span\nrepresentations in the ;-th layer are as follows:\nH; = [ˆHl; ˇH;] (7)\nAs BiLSTM networks can make full use of the\ncontext information at a higher level, we employ\na BiLSTM and a linear layer to predict labels for\ncandidate spans in a hierarchical manner. As we\nhave obtained complete candidate spans, e.g., {1-\ntoken spans, 2-token spans, ..., !-token spans},\nbased on the attention weights in the SGM module,\nwe can easily classify them into a proper category.\nThe predicted labels for the span representations in\nthe ;-th layer is obtained as follows:\nH; = BiLSTM(H;U1 +b1) (8)\nY; = argmax(H;U2 +b2) (9)\nwhere U1,U2,b1,and b2 are trainable parameters,\nY; is the predicted labels of the ;-th layer. The total\noutput of the L layers is Y = {Y1,Y2,..., YL}.\n3.4 Model Training\nWe prepare the gold labels in a hierarchical manner,\ntherefore, each layer of the proposed model could\nbe simpliﬁed as a multi-class classiﬁcation task in\nany layer of bottom !−1 layers and a ﬂat NER\ntask in the topmost layer. During training, our\nmodel predicts the distribution of entity semantic\nlabels for each layer. Finally, we compute the cross-\nentropy loss as follows:\nL= −\nÕ\n(ˆY;)log(Y;) (10)\nwhere ˆY; and Y; denote the true distribution and\npredicted distribution of entity semantic labels, re-\nspectively. Lis the summation of the loss from\nall layers. Our complete training procedure for\nHiTRANS is shown in Algorithm 1.\nAlgorithm 1Pseudocode of HiTRANS .\nInput: A sequence of words S= {F1,F2,...,F |S|};\nThe number of layers !(; ∈!)\nOutput: Entity labels of L layers Y = {Y1,Y2,..., YL}.\n1: for numbers of training iterations do\n2: Multi-level Embedding x8 = [x2\n8; xF\n8 ; xB\n8]\n3: Attention-reﬁned representation for the ;−th layer\nusing Equation (2) and (3)\n4: initializing ˆH1 ←Equation (4)\n[Span Generation]\n5: for ;:=1 to !step 1 do\n6: BU-SGM: ˆHl ←Equation (5)\n7: for ;:=L to 1 step 1 do\n8: TD-SGM: ˇHl ←Equation (6)\n[Hierarchical Label Prediction]\n9: for ;:=1 to !step 1 do\n10: H; = [ˆHl; ˇH;]\n11: H; ←BiLSTM by Equation (8)\n12: Y; ←Equation (9)\n13: end for\n14: return Entity labels Y\n4 Experiments\n4.1 Datasets and Baseline Methods\nTo verify the effectiveness of HiTRANS , we con-\nduct our experiments on four NNER datasets: GE-\nNIA (Kim et al., 2003), ACE-2004 (Doddington\net al., 2004), ACE-2005 (Walker et al., 2006), and\nNNE (Ringland et al., 2019). We adopt the prepro-\ncess of (Finkel and Manning, 2009) and (Lu and\nRoth, 2015). For GENIA, we use GENIA v3.0.2\ncorpus to construct the dataset and split it into\n81%:9%:10% for training, development, and test-\ning, respectively. For ACE-2004 and ACE-2005,\nwe split the Train/Develop/Test set following the\npreprocess as previous studies (Lu and Roth, 2015;\nZheng et al., 2019; Jue et al., 2020). For NNE, we\n129\nItem GENIA ACE-2005 NNE ACE-2004\nTrain Develop Test Train Develop Test Train Develop Test Train Develop Test\nTotal sentences 15022 1669 1855 6198 742 809 7285 968 1058 43457 1989 3762\nNested sentences 3222 328 448 2718 294 388 2797 352 339 28606 1292 2489\nTotal entities 47006 4461 5596 22195 2514 3034 24700 3218 3029 248136 10463 21196\nNested entities 8382 818 1212 10157 1092 1417 9946 1191 1179 206618 8487 17670\nMax length 20 20 15 57 35 42 49 31 27 16 15 15\nPercentage 18% 18% 22% 46% 43% 47% 40% 37% 39% 83% 81% 83%\nTable 1: The statistics of datasets. A nested sentence denotes the sentence containing any nested entity.\nModel GENIA ACE-2004 ACE-2005 NNE\nP(%) R(%) F1(%) P(%) R(%) F1(%) P(%) R(%) F1(%) P(%) R(%) F1(%)\nRevisited (Katiyar and Cardie, 2018) 79.80 68.20 73.60 73.60 71.80 72.70 70.60 70.40 70.50 - - -Linearization (Straková et al., 2019) - - 76.40 - - 77.10 - - 75.40 - - -Exhaustive (Sohrab and Miwa, 2018) 93.20 64.00 77.10 70.40 62.50 66.20 75.20 58.00 65.50 - - -Boundary-aware (Zheng et al., 2019) 74.00 76.10 75.00 74.40 74.10 74.20 76.40 71.20 73.70 89.10 89.30 89.20Sequence2nuggets (Lin et al., 2019) 75.80 73.90 74.80 - - - 76.20 73.60 74.90 - - -Boundary-enhanced (Tan et al., 2020) 78.90 72.70 75.70 78.10 72.80 75.30 77.10 74.20 75.60 - - -BiFlaG (Luo and Zhao, 2020) 77.40 74.60 76.00 - - - 75.00 75.20 75.10 - - -Layered (Ju et al., 2018) 78.50 71.30 74.70 - - - 74.20 70.30 72.20 - - -Second-best (Shibuya and Hovy, 2020) 76.30 74.70 75.50 - - - 83.00 82.40 82.70 - - -Multi-grained (Xia et al., 2019) - - - 81.70 77.40 79.50 79.00 77.30 78.20 - - -Merge (Fisher and Vlachos, 2019) - - 76.44 - - 77.08 - - 75.36 - - -Pyramid (Jue et al., 2020) 78.60 77.02 77.78 81.14 79.42 80.27 80.01 78.85 79.42 93.44 93.95 93.70\nMerge (Fisher and Vlachos, 2019) - - 78.20 - - 84.33 - - 83.42 - - -MRC (Li et al., 2020) 78.56 73.94 76.18 87.39 86.09 86.73 86.90 86.50 86.70 - - -Boundary-enhanced (Tan et al., 2020) 79.20 77.40 78.30 85.80 84.80 85.30 83.80 83.90 83.90 - - -Pyramid (Jue et al., 2020) - - - 87.71 87.78 87.74 85.30 87.40 86.34 94.30 95.07 94.68\nHiTRANS 78.57 79.5979.0888.10 87.5787.8886.48 87.6287.0494.62 94.8594.74\nTable 2: Experiment results on the test set of four benchmarks compared to the state-of-the-art methods. Methods\nlisted in the lower part of the table are based on the pretrained language model.\nuse the original dataset split and pre-processing.\nThere are 5/7/7/114 different entity types in GE-\nNIA, ACE-2004, ACE-2005, and NNE datasets,\nrespectively. For evaluation, we employ micro-\naveraged precision (P), recall (R), and F1. Table 1\nlists the concerned data statistics of each dataset.\nWe comprehensively compare our proposed\nmodel with the state-of-the-art baselines, which\ncould be categorized into three groups as follows:\n• Hypergraph-based methods: These obtain\nexpressive tagging schemas for NER, includ-\ning Revisited Model (Katiyar and Cardie,\n2018), and Linearization model (Straková\net al., 2019).\n• Span-based methods: They achieve a decent\nperformance by enumerating possible regions\nof an input sequence for classiﬁcation, in-\ncluding Exhaustive Model (Sohrab and Miwa,\n2018), Boundary-aware (Zheng et al., 2019),\nSequence2nuggets (Lin et al., 2019), MRC (Li\net al., 2020), Boundary-enhanced (Tan et al.,\n2020), and BiFlaG (Luo and Zhao, 2020).\n• Layered-based methods: These methods ap-\nply hierarchical structures to iteratively extract\nnamed entities in order, including Layered\nModel (Ju et al., 2018), Merge (Fisher and\nVlachos, 2019), Second-best Model (Shibuya\nand Hovy, 2020), Multi-grained Model (Xia\net al., 2019), and Pyramid (Jue et al., 2020).\n4.2 Experimental Settings\nWe obtain the character-level representation en-\ncoded by BiLSTM, and word-level representation\nfrom the 100-dimensional pre-trained word em-\nbedding GloVe (Pennington et al., 2014), which\nis trained in 6B tokens. For sentence-level em-\nbeddings, we use the BERT and ALBERT embed-\ndings to further improve the NNER. For ACE-2004,\nACE-2005, and NNE datasets, the dimensions of\ncharacter-level embedding, word-level embedding,\nsentence-level embedding are set by default to 30,\n100, and 5120 (1024+4096), respectively. As for\nthe GENIA dataset, we obtain word embedding\nfrom pretrained embedding Pubmed trained on\nbiomedical corpus (Chiu et al., 2016), setting the\ndimension of word-level embeddings to 200. The\noutput dimension of the multi-level representation\nand the hidden size of bidirection LSTM are set\nto 200. The number of parallel heads :is set to 8.\n130\nThe number of layers !is set to 16, which exceeds\nthe length of most entities and the batch size is\nempirically set to 32. We use SGD optimizer for\ntraining our model with learning rate set to 0.02,\nand the dropout rate is set to 0.4 to avoid overﬁtting.\nAll of our experiments are performed on the same\nmachine. We repeat these experiments 5 times, and\nreport the average performance on the test set.\n4.3 Results and Analysis\nTable 2 shows the overall results compared with the\nbaseline methods by groups. Overall, hypergraph-\nbased methods achieve decent results depending on\nthe expressive tagging schema; however, ambigu-\nity and high time complexity are hardly inevitable.\nSpan-based methods improve the performance of\nNNER; however, they may break the continuous-\nstructure of the context. To alleviate the problem,\nlayered-based models further improve the ﬁnal per-\nformance with hierarchical layers, however, span\nrepresentations are oversimpliﬁed. In addition, the\nmethods incorporated with the pretrained language\nmodel, e.g., BERT and ALBERT, generally outper-\nform previous methods, which take the advantage\nof capturing sentence-level features from context.\nAs shown in Table 1, we can observe that there\nare 22%, 47%, 39%, and 83% in the test set of\nGENIA, ACE-2004, ACE-2005, and NNE, respec-\ntively, which contain nested entities to different de-\ngrees. Table 2 shows that our proposed HiTRANS\nachieves the state-of-the-art results on GENIA 2,\nACE-2004, ACE-2005, and NNE datasets, which\nveriﬁes the effectiveness of HiTRANS for NNER.\nBesides, HiTRANS outperforms other baselines on\nNNE dataset containing 114 categories of entities,\nwhich further validates the superiority in recogniz-\ning nested entities from complex sentences.\nFrom the tendency, span-based methods and\nlayered-based methods draw more attention than\nhypergraph-based methods in recent years, which\nprobably because they effectively balance the per-\nformance and efﬁciency. In summary, the overall\nperformance of the HiTRANS demonstrates its su-\nperiority in NNER, which beneﬁts from the hierar-\nchical span representation.\n4.4 Ablation Study\nAs shown in Table 3, we present the experimen-\ntal results of our proposed model on ACE-2005.\n2We reproduced the results using their implement code (Li\net al., 2020), which only obtains 76.18% F1 score, rather than\n83.75% F1 score.\nSetting P(%) R(%) F1(%)\nwithout CL-EMB 86.28 87.42 86.85\nwithout WL-EMB 84.80 87.65 86.20\nwithout SL-EMB 80.46 76.76 78.56\nwithout MHA 85.32 87.32 86.31\nwithout Phase 2 85.86 87.19 86.52\nHiTRANS 86.48 87.62 87.04\nTable 3: Ablation study on ACE-2005. MHA denotes\nthe multi-head attention.\nThe multi-level features (i.e., CL-EMB, WL-EMB,\nand SL-EMB) obtained from character-level, word-\nlevel, and sentence-level are essential for the ﬁnal\nperformance. Particularly, the sentence-level fea-\nture improves the performance by a large margin,\nwhich may because the language model usually\nhas a large number of parameters to learn a better\nrepresentation. Besides, Hi TRANS without WL-\nEMB has a slight increase in recall, but a decrease\nin precision, which indicates that the word-level\nfeature contributes to select the correct entity from\ncandidate spans. The residual multi-head attention\n(MHA) contributes to the ﬁnal performance as well,\nwhich could be due to the reﬁned span representa-\ntions in each layer. In addition, Hi TRANS model\nwith two phases shows better performance, which\nmay because phase 2 can further propagate infor-\nmation in a top-down manner. We only remove\nPhase 2 for ablation studies, since Phase 1 need to\ntake original multi-level representations as input.\nIn all, our Hi TRANS achieves 87.04% F1 score,\nwhich indicates that all components contribute to\nthe effectiveness and the whole framework has su-\nperior in achieving the overall performance.\nSentence These problems multiplied when the New\nEngland chain Stop n’ Shop acquired Giant.\nGold Label\nNew England: [LOC]; the New\nEngland chain: [ORG]; the New England\nchain Stop n’ Shop :[ORG]; Giant: [ORG]\nExhaustive the New England chain Stop n’ Shop :[ORG];\nNew England chain: [ORG]; Giant: [ORG]\nLayered the New England chain Stop n’ Shop :[ORG];\nGiant: [ORG]\nBoundary-awareNew England: [LOC]; Giant: [ORG];\nthe New England chain Stop n’ Shop :[ORG]\nPyramid\nthe New England chain Stop n’ Shop :[ORG];\nthe New England chain: [ORG]; New\nEngland chain: [ORG]; New England: [LOC];\nn’ Shop :[ORG]; Giant: [ORG];\nOur model\nGiant: [ORG]; New England: [LOC];\nthe New England chain: [ORG];\nthe New England chain Stop n’ Shop :[ORG]\nTable 4: A case study of the NNER.\n131\nFigure 4: The conﬁdences of entities in different layers.\n4.5 Case Study and Visualization\nTable 4 shows a case study comparing our model\nwith Exhaustive (Sohrab and Miwa, 2018), Lay-\nered (Ju et al., 2018), Boundary-aware (Zheng\net al., 2019), and Pyramid (Jue et al., 2020) models,\nwhich are more germane and representative. In\nthis example, there is an entity “the New England\nchain Stop n’ shop” containing the entity “the New\nEngland chain\", which also has an entity “New\nEngland\" nested in it. Our proposed model recog-\nnizes all potential entities of different-length in a\nﬁne-to-coarse manner. Exhaustive gets the wrong\ntoken of entity heads and misses the token “the\"\nin entities, and Layered merely extracts outer enti-\nties. Compared with the Pyramid model detecting\nwrong spans, our HiTRANS can extract both inner\nand outer entities more precisely in a hierarchical\nmanner. It demonstrates that HiTRANS contributes\nto the performance of NNER, which may due to\nthe hierarchical transformer reﬁnes span represen-\ntations in each layer. Furthermore, the hierarchical\nlabel prediction model has the advantage of identi-\nfying nested named entities by incorporating both\nsemantic dependencies.\nFor in-depth analysis of Hi TRANS , we visual-\nize the result of the predictions in each layer with\nmasking. Owing to space limit, only the ﬁrst four\nlayers are shown in Figure 4. From the input sen-\ntence, “his” is correctly recognized as entities of\n1-token with 0.43 conﬁdence in Layer 1, \"Saddam\nHussein\" and \"his Henchmen\" are recognized as\nentities of 2-token with 0.29 and 0.31 conﬁdence\nin Layer 2, respectively. Likewise, other spans of\n;-token in Layer ; ∈ ! are assigned with differ-\nent conﬁdences. In a word, we can observe that\nthe recognized entities of different lengths are as-\nsigned with higher conﬁdences than others in each\nlayer, which contributes to distill truth named enti-\nties from candidate spans and further validates the\neffectiveness of our HiTRANS for the NNER.\nFigure 5: Parameter sensitivity analysis of HiT RANS .\nThe out-of-memory problem occurs when the number\nof layers is set to 32 (i.e., 25) on the NNE and GENIA\ndataset, as shown at the left.\n4.6 Parameter Sensitivity Analysis\nTwo primary parameters, i.e., the number of layers\nand batch size, are selected to verify the impact\nof parameters on the effectiveness of Hi TRANS .\nThe number of layers denotes how many layers\nused in the hierarchical model and the batch size\ncontrols the size of allocated resources. To study\nuncertainty in the output of our HiTRANS , we adopt\nthe single-parameter sensitivity analysis by varying\none parameter while ﬁxing the others each time. As\nFigure 5 shows, when the number of layers and the\nbatch size change, especially when the number of\nlayers is greater than 4, and the batch size is greater\nthan 4, HiTRANS still maintains high performance\non these four benchmark datasets. Although the\nnumber of layers is related to the maximum nesting\ndepth, the results demonstrate that Hi TRANS is\nnot sensitive to parameter settings and has superior\nperformance and robustness in NNER.\n5 Conclusion\nThis paper presents a novel HiTRANS framework,\nwhich learns effective span representations for la-\nbel prediction of nested entities in a hierarchical\nmanner. The proposed framework iteratively gener-\nates candidate span representations by aggregating\nadjacent features and further reﬁnes them based\non a bottom-up and top-down transformer network.\nMoreover, a candidate span is further recognized\nas a named entity sequentially, leveraging the se-\nmantic dependency of adjacent spans. Extensive\nexperimental results demonstrate that Hi TRANS\nachieves the state-of-the-art performances on GE-\nNIA, ACE-2004, ACE-2005 and NNE datasets.\nAcknowledgments\nThis work is partially supported by National Nat-\nural Science Foundation of China through grants\nNo.61976102, No.U19A2065, and No.61902145,\nand HKBU direct grant (Ref. AIS 21-22/02).\n132\nReferences\nBilly Chiu, Gamal Crichton, Anna Korhonen, and\nSampo Pyysalo. 2016. How to train good word em-\nbeddings for biomedical nlp. In BioNLP, pages 166–\n174.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT, pages 4171–4186.\nGeorge R Doddington, Alexis Mitchell, Mark A Przy-\nbocki, Lance A Ramshaw, Stephanie M Strassel, and\nRalph M Weischedel. 2004. The automatic content\nextraction (ace) program-tasks, data, and evaluation.\nIn Lrec, volume 2, pages 837–840. Lisbon.\nJenny Rose Finkel and Christopher D Manning. 2009.\nNested named entity recognition. In EMNLP, pages\n141–150.\nJoseph Fisher and Andreas Vlachos. 2019. Merge and\nlabel: A novel neural network architecture for nested\nner. In ACL, pages 5840–5850.\nMeizhi Ju, Makoto Miwa, and Sophia Ananiadou.\n2018. A neural layered model for nested named en-\ntity recognition. In NAACL-HLT, pages 1446–1459.\nW ANG Jue, Lidan Shou, Ke Chen, and Gang Chen.\n2020. Pyramid: A layered model for nested named\nentity recognition. In ACL, pages 5918–5928.\nArzoo Katiyar and Claire Cardie. 2018. Nested named\nentity recognition revisited. In NAACL-HLT, pages\n861–871.\nJ-D Kim, Tomoko Ohta, Yuka Tateisi, and Jun’ichi\nTsujii. 2003. Genia corpus—a semantically anno-\ntated corpus for bio-textmining. Bioinformatics,\n19(suppl_1):i180–i182.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In ICLR.\nXiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong\nHan, Fei Wu, and Jiwei Li. 2020. A uniﬁed MRC\nframework for named entity recognition. In ACL,\npages 5849–5859.\nHongyu Lin, Yaojie Lu, Xianpei Han, and Le Sun.\n2019. Sequence-to-nuggets: Nested entity men-\ntion detection via anchor-region networks. In ACL,\npages 5182–5192.\nWei Lu and Dan Roth. 2015. Joint mention extrac-\ntion and classiﬁcation with mention hypergraphs. In\nEMNLP, pages 857–867.\nYing Luo and Hai Zhao. 2020. Bipartite ﬂat-graph net-\nwork for nested named entity recognition. In ACL,\npages 6408–6418.\nAldrian Obaja Muis and Wei Lu. 2017. Labeling gaps\nbetween words: Recognizing overlapping mentions\nwith mention separators. In EMNLP, pages 2608–\n2618.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In EMNLP, pages 1532–1543.\nNicky Ringland, Xiang Dai, Ben Hachey, Sarvnaz\nKarimi, Cecile Paris, and James R Curran. 2019.\nNne: A dataset for nested named entity recognition\nin english newswire. In ACL, pages 5176–5181.\nTakashi Shibuya and Eduard Hovy. 2020. Nested\nnamed entity recognition via second-best sequence\nlearning and decoding. Transactions of the Associa-\ntion for Computational Linguistics, 8:605–620.\nMohammad Golam Sohrab and Makoto Miwa. 2018.\nDeep exhaustive model for nested named entity\nrecognition. In EMNLP, pages 2843–2849.\nJana Straková, Milan Straka, and Jan Hajic. 2019. Neu-\nral architectures for nested ner through linearization.\nIn ACL, pages 5326–5331.\nChuanqi Tan, Wei Qiu, Mosha Chen, Rui Wang, and\nFei Huang. 2020. Boundary enhanced neural span\nclassiﬁcation for nested named entity recognition.\nIn AAAI, pages 9016–9023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS, pages 5998–6008.\nC Walker, S Strassel, J Medero, and K Maeda. 2006.\nAce 2005 multilingual training corpus. Progress of\nTheoretical Physics Supplement, 110(110):261–276.\nBailin Wang and Wei Lu. 2018. Neural segmental hy-\npergraphs for overlapping mention recognition. In\nEMNLP, pages 204–214.\nBailin Wang, Wei Lu, Yu Wang, and Hongxia Jin. 2018.\nA neural transition-based model for nested mention\nrecognition. In EMNLP, pages 1011–1017.\nCongying Xia, Chenwei Zhang, Tao Yang, Yaliang Li,\nNan Du, Xian Wu, Wei Fan, Fenglong Ma, and\nPhilip Yu. 2019. Multi-grained named entity recog-\nnition. In ACL, pages 1430–1440.\nZhiwei Yang, Hechang Chen, Jiawei Zhang, Jing Ma,\nand Chang Yi. 2020. Attention-based multi-level\nfeature fusion for named entity recognition. In IJ-\nCAI, pages 3594–3600.\nWonjin Yoon, Chan Ho So, Jinhyuk Lee, and Jaewoo\nKang. 2019. Collabonet: collaboration of deep neu-\nral networks for biomedical named entity recogni-\ntion. BMC bioinformatics, 20(10):249.\nChangmeng Zheng, Yi Cai, Jingyun Xu, Ho-fung Le-\nung, and Guandong Xu. 2019. A boundary-aware\nneural model for nested named entity recognition. In\nEMNLP, pages 357–366.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8309056758880615
    },
    {
      "name": "Transformer",
      "score": 0.7722951173782349
    },
    {
      "name": "Sentence",
      "score": 0.5631389021873474
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4759480953216553
    },
    {
      "name": "Natural language processing",
      "score": 0.4171850085258484
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210134929",
      "name": "Jilin Province Science and Technology Department",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I194450716",
      "name": "Jilin University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I141568987",
      "name": "Hong Kong Baptist University",
      "country": "HK"
    }
  ]
}