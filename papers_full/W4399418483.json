{
    "title": "CLIPping the Deception: Adapting Vision-Language Models for Universal Deepfake Detection",
    "url": "https://openalex.org/W4399418483",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2476675712",
            "name": "Sohail Ahmed Khan",
            "affiliations": [
                "University of Bergen"
            ]
        },
        {
            "id": "https://openalex.org/A2760104377",
            "name": "Duc-Tien Dang-Nguyen",
            "affiliations": [
                "University of Bergen"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4225323055",
        "https://openalex.org/W4312453532",
        "https://openalex.org/W4312753047",
        "https://openalex.org/W2963767194",
        "https://openalex.org/W4375869427",
        "https://openalex.org/W6931405348",
        "https://openalex.org/W3180355996",
        "https://openalex.org/W2950352474",
        "https://openalex.org/W3173126908",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W4312651322",
        "https://openalex.org/W6797179183",
        "https://openalex.org/W2962770929",
        "https://openalex.org/W3035574324",
        "https://openalex.org/W2112796928",
        "https://openalex.org/W3185341429",
        "https://openalex.org/W3019200173",
        "https://openalex.org/W4386071953",
        "https://openalex.org/W2962974533",
        "https://openalex.org/W4312933868",
        "https://openalex.org/W2982058372",
        "https://openalex.org/W4281485151",
        "https://openalex.org/W3034577585",
        "https://openalex.org/W2963800363",
        "https://openalex.org/W3198675127",
        "https://openalex.org/W3198377975",
        "https://openalex.org/W2962793481",
        "https://openalex.org/W3183392865",
        "https://openalex.org/W4389370799",
        "https://openalex.org/W3125803510",
        "https://openalex.org/W2909336075",
        "https://openalex.org/W3174807077"
    ],
    "abstract": "The recent advancements in Generative Adversarial Networks (GANs) and the emergence of Diffusion models have significantly streamlined the production of highly realistic and widely accessible synthetic content. As a result, there is a pressing need for effective general purpose detection mechanisms to mitigate the potential risks posed by deepfakes. In this paper, we explore the effectiveness of pre-trained vision-language models (VLMs) when paired with recent adaptation methods for universal deepfake detection. Following previous studies in this domain, we employ only a single dataset (ProGAN) in order to adapt CLIP for deepfake detection. However, in contrast to prior research, which rely solely on the visual part of CLIP while ignoring its textual component, our analysis reveals that retaining the text part is crucial. Consequently, the simple and lightweight Prompt Tuning based adaptation strategy that we employ outperforms the previous SOTA approach by 5.01% mAP and 6.61% accuracy while utilizing less than one third of the training data (200k images as compared to 720k). To assess the real-world applicability of our proposed models, we conduct a comprehensive evaluation across various scenarios. This involves rigorous testing on images sourced from 21 distinct datasets, including those generated by GANs-based, diffusion-based and commercial tools. Code and pre-trained models: https://github.com/sohailahmedkhan/CLIPping-the-Deception",
    "full_text": null
}