{
    "title": "Visual cognition in multimodal large language models",
    "url": "https://openalex.org/W4406400870",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A3207783209",
            "name": "Luca M. Schulze Buschoff",
            "affiliations": [
                "Max Planck Institute for Biological Cybernetics",
                "Institute of Bioinformatics and Systems Biology"
            ]
        },
        {
            "id": "https://openalex.org/A5092048108",
            "name": "Elif Akata",
            "affiliations": [
                "University of Tübingen",
                "Institute of Bioinformatics and Systems Biology",
                "Max Planck Institute for Biological Cybernetics"
            ]
        },
        {
            "id": "https://openalex.org/A1898836778",
            "name": "Matthias Bethge",
            "affiliations": [
                "University of Tübingen"
            ]
        },
        {
            "id": "https://openalex.org/A2117491939",
            "name": "Eric Schulz",
            "affiliations": [
                "Max Planck Institute for Biological Cybernetics",
                "Institute of Bioinformatics and Systems Biology"
            ]
        },
        {
            "id": "https://openalex.org/A3207783209",
            "name": "Luca M. Schulze Buschoff",
            "affiliations": [
                "Max Planck Institute for Biological Cybernetics",
                "Helmholtz Zentrum München",
                "Institute of Bioinformatics and Systems Biology"
            ]
        },
        {
            "id": "https://openalex.org/A5092048108",
            "name": "Elif Akata",
            "affiliations": [
                "Max Planck Institute for Biological Cybernetics",
                "Institute of Bioinformatics and Systems Biology",
                "Helmholtz Zentrum München",
                "University of Tübingen"
            ]
        },
        {
            "id": "https://openalex.org/A1898836778",
            "name": "Matthias Bethge",
            "affiliations": [
                "University of Tübingen"
            ]
        },
        {
            "id": "https://openalex.org/A2117491939",
            "name": "Eric Schulz",
            "affiliations": [
                "Helmholtz Zentrum München",
                "Max Planck Institute for Biological Cybernetics",
                "Institute of Bioinformatics and Systems Biology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W4360836968",
        "https://openalex.org/W6838461927",
        "https://openalex.org/W4392773006",
        "https://openalex.org/W7014595873",
        "https://openalex.org/W4297677272",
        "https://openalex.org/W4323655724",
        "https://openalex.org/W6800751262",
        "https://openalex.org/W4386827556",
        "https://openalex.org/W4388000629",
        "https://openalex.org/W6853108898",
        "https://openalex.org/W2002170215",
        "https://openalex.org/W4300402905",
        "https://openalex.org/W4362458506",
        "https://openalex.org/W4386042678",
        "https://openalex.org/W2963305465",
        "https://openalex.org/W2930789748",
        "https://openalex.org/W2847827297",
        "https://openalex.org/W2962802170",
        "https://openalex.org/W6712034760",
        "https://openalex.org/W2806280256",
        "https://openalex.org/W3096013785",
        "https://openalex.org/W2671811403",
        "https://openalex.org/W4233071853",
        "https://openalex.org/W1982966536",
        "https://openalex.org/W2059100041",
        "https://openalex.org/W2507135045",
        "https://openalex.org/W2789580031",
        "https://openalex.org/W2625439778",
        "https://openalex.org/W2055904739",
        "https://openalex.org/W2110597679",
        "https://openalex.org/W2143891888",
        "https://openalex.org/W1965831205",
        "https://openalex.org/W1511739467",
        "https://openalex.org/W40029570",
        "https://openalex.org/W2128152674",
        "https://openalex.org/W2120380598",
        "https://openalex.org/W2799950781",
        "https://openalex.org/W2102630578",
        "https://openalex.org/W2498626120",
        "https://openalex.org/W2522500382",
        "https://openalex.org/W4307844977",
        "https://openalex.org/W2765191723",
        "https://openalex.org/W3163675517",
        "https://openalex.org/W6887904745",
        "https://openalex.org/W2909534157",
        "https://openalex.org/W2188746823",
        "https://openalex.org/W244491643",
        "https://openalex.org/W3136535094",
        "https://openalex.org/W2010931158",
        "https://openalex.org/W2609031512",
        "https://openalex.org/W6982639720",
        "https://openalex.org/W2963965884",
        "https://openalex.org/W2594035753",
        "https://openalex.org/W4301435598",
        "https://openalex.org/W6748203849",
        "https://openalex.org/W2145780367",
        "https://openalex.org/W2123713131",
        "https://openalex.org/W4318919287",
        "https://openalex.org/W4385571689",
        "https://openalex.org/W4385292963",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4385430086",
        "https://openalex.org/W4366850553",
        "https://openalex.org/W4401042756",
        "https://openalex.org/W4387378202",
        "https://openalex.org/W3004346089",
        "https://openalex.org/W6981194214",
        "https://openalex.org/W4211017739",
        "https://openalex.org/W3096397561",
        "https://openalex.org/W2937454300",
        "https://openalex.org/W4389518675",
        "https://openalex.org/W4401024343",
        "https://openalex.org/W4283080831",
        "https://openalex.org/W4281690148",
        "https://openalex.org/W2619383789",
        "https://openalex.org/W6713645886",
        "https://openalex.org/W2964138343",
        "https://openalex.org/W3015965768",
        "https://openalex.org/W2904925972",
        "https://openalex.org/W4366822724",
        "https://openalex.org/W3045548659",
        "https://openalex.org/W6772383348",
        "https://openalex.org/W4405140008",
        "https://openalex.org/W4312480274",
        "https://openalex.org/W4399893213",
        "https://openalex.org/W4402671548",
        "https://openalex.org/W3108490213",
        "https://openalex.org/W4386740843",
        "https://openalex.org/W3160638507",
        "https://openalex.org/W4292213411",
        "https://openalex.org/W3198599617",
        "https://openalex.org/W3185341429",
        "https://openalex.org/W4385262478",
        "https://openalex.org/W6853344003",
        "https://openalex.org/W3172887788",
        "https://openalex.org/W6851949647",
        "https://openalex.org/W4381827075",
        "https://openalex.org/W4319322519",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W3035965352",
        "https://openalex.org/W6969259846",
        "https://openalex.org/W3003257820",
        "https://openalex.org/W2011301426",
        "https://openalex.org/W3150635270",
        "https://openalex.org/W2749069611",
        "https://openalex.org/W6949209083",
        "https://openalex.org/W3099878876"
    ],
    "abstract": "Abstract A chief goal of artificial intelligence is to build machines that think like people. Yet it has been argued that deep neural network architectures fail to accomplish this. Researchers have asserted these models’ limitations in the domains of causal reasoning, intuitive physics and intuitive psychology. Yet recent advancements, namely the rise of large language models, particularly those designed for visual processing, have rekindled interest in the potential to emulate human-like cognitive abilities. This paper evaluates the current state of vision-based large language models in the domains of intuitive physics, causal reasoning and intuitive psychology. Through a series of controlled experiments, we investigate the extent to which these modern models grasp complex physical interactions, causal relationships and intuitive understanding of others’ preferences. Our findings reveal that, while some of these models demonstrate a notable proficiency in processing and interpreting visual data, they still fall short of human capabilities in these areas. Our results emphasize the need for integrating more robust mechanisms for understanding causality, physical dynamics and social cognition into modern-day, vision-based language models, and point out the importance of cognitively inspired benchmarks.",
    "full_text": "Nature Machine Intelligence | Volume 7 | January 2025 | 96–106 96\nnature machine intelligence\nArticle\nhttps://doi.org/10.1038/s42256-024-00963-y\nVisual cognition in multimodal large \nlanguage models\n \nLuca M. Schulze Buschoff    1,2,4 , Elif Akata    1,2,3,4, Matthias Bethge3 & \nEric Schulz1,2\nA chief goal of artificial intelligence is to build machines that think like \npeople. Yet it has been argued that deep neural network architectures fail \nto accomplish this. Researchers have asserted these models’ limitations in \nthe domains of causal reasoning, intuitive physics and intuitive psychology. \nYet recent advancements, namely the rise of large language models, \nparticularly those designed for visual processing, have rekindled interest in \nthe potential to emulate human-like cognitive abilities. This paper evaluates \nthe current state of vision-based large language models in the domains of \nintuitive physics, causal reasoning and intuitive psychology. Through a \nseries of controlled experiments, we investigate the extent to which these \nmodern models grasp complex physical interactions, causal relationships \nand intuitive understanding of others’ preferences. Our findings reveal that, \nwhile some of these models demonstrate a notable proficiency in processing \nand interpreting visual data, they still fall short of human capabilities in \nthese areas. Our results emphasize the need for integrating more robust \nmechanisms for understanding causality, physical dynamics and social \ncognition into modern-day, vision-based language models, and point out \nthe importance of cognitively inspired benchmarks.\nPeople are quick to anthropomorphize, attributing human character-\nistics to non-human agents1. The tendency to anthropomorphize has \nonly intensified with the advent of large language models (LLMs) 2. \nLLMs apply deep learning techniques to generate text3, learning from \nvast datasets to produce responses that can be startlingly human-like4. \nAstonishingly, these models cannot only generate text. When scaled up \nto bigger training data and architectures, other, so-called ‘emergent \nabilities’ appear5,6. The current models can, for example, pass the bar \nexam7, write poems8, compose music9 and assist in programming and \ndata analysis tasks10. As a result, the line between human and machine \ncapabilities is increasingly blurred11,12. People not only interact with \nthese systems as if they were humans13, but they also start to rely on \nthem for complex decision-making14, artistic creation15 and personal \ninteractions16. It is, therefore, natural to ask: Have we built machines \nthat think like people?\nJudging whether or not artificial agents can mimic human thought \nis at the core of cognitive science 17,18. Therein, researchers have long \ndebated the capabilities of artificially intelligent agents19–21. In a seminal \npaper, Lake and colleagues22 proposed core domains to consider when \nmaking such judgements. Published during the height of the deep \nlearning revolution23, the authors focused on domains that were easy \nfor people but difficult for deep learning models: intuitive physics, \ncausal reasoning and intuitive psychology.\nResearch on intuitive physics has studied how people perceive and \ninterpret physical phenomena24–26. Past work on this topic has empha-\nsized that humans possess an innate ability to predict and understand \nthe physical properties of objects and their interactions27, even from \na young age28, a notion sometimes summarized as a ‘physics engine’ \nin people’s heads 29. This understanding includes concepts such as \ngravity30, inertia31 and momentum32. Some of the most canonical tasks \nReceived: 29 November 2023\nAccepted: 25 November 2024\nPublished online: 15 January 2025\n Check for updates\n1Max Planck Institute for Biological Cybernetics, Tübingen, Germany. 2Institute for Human-Centered AI, Helmholtz Munich, Oberschleißheim,  \nGermany. 3University of Tübingen, Tübingen, Germany. 4These authors contributed equally: Luca M. Schulze Buschoff, Elif Akata.  \n e-mail: lucaschulzebuschoff@gmail.com\nNature Machine Intelligence | Volume 7 | January 2025 | 96–106\n 97\nArticle https://doi.org/10.1038/s42256-024-00963-y\nquestions. We submitted them to some of the currently most advanced \nLLMs. T o evaluate whether the LLMs show human-like performance in \nthese domains, we follow the approach outlined in ref. 73: we treat the \nmodels as participants in psychological experiments. This allows us to \ndraw direct comparisons with human behaviour on these tasks. Since \nthe tasks are designed to test abilities in specific cognitive domains, \nthis comparison allows us to investigate in which domains multimodal \nLLMs perform similar to humans, and in which they don’t. Our results \nshowed that these models can, at least partially, solve these tasks. In \nparticular, two of the largest currently available models, OpenAI’s \nGenerative Pre-trained Transformer (GPT-4) and Anthropic’s Claude-3 \nOpus, managed to perform robustly above chance in two of the three \ndomains. Yet crucial differences emerged. First, none of the models \nmatched human-level performance in any of the domains. Second, \nnone of the models fully captured human behaviour, leaving room \nfor domain-specific models of cognition such as the Bayesian models \noriginally proposed for the tasks.\nRelated work\nThere have been a large number of studies on reasoning abilities in \nLLMs74–76. Previous studies have focused, among others, on testing \nLLMs’ cognitive abilities in model-based planning73, analogical rea-\nsoning tests 77, exploration tasks 78, systematic reasoning tests 79,80, \npsycholinguistic completion studies81 and affordance understanding \nproblems82. In this sense, our contribution can be seen as a part of a \nlarger movement in which researchers use methods from the behav-\nioural sciences to understand black box machine learning models83–85. \nHowever, most of the previous studies did not investigate multimodal \nLLMs but rather remained in the pure language domain. Although \nthere have been recent attempts to investigate vision LLMs’ cognitive \nfeatures, including their reaction to visual illusions86 as well as how \nthey solve simple intelligence tasks87, we investigate the proposed core \ncomponents of cognition in these models.\nPrevious work has also looked at how LLMs solve cognitive tasks \ntaken from the same domains that we have looked at. In intuitive \nphysics, Zečević and colleagues88 found that LLMs performed poorly \nin a task using language descriptions of physical scenarios. Zhang \nand colleagues 89 extracted programs from text produced by LLMs \nto improve their physical reasoning abilities. Finally, Jassim and col-\nleagues 90 proposed a new benchmark for evaluating multimodal \nLLMs’ understanding of situated physics. In causal reasoning, Binz \nin this domain involve testing people’s judgements about the stability \nof block towers33,34. These tasks have made their way into machine learn-\ning benchmarks35,36, where they are used to test the intuitive physical \nunderstanding of neural networks (see ref. 37 for an overview of previ-\nous work on building models with human-like physical knowledge).\nResearch on causal reasoning has studied how individuals infer \nand think about cause–effect relationships 38–40. Past work on this \ntopic has proposed that humans possess an intuitive capacity to infer, \nunderstand and predict causal relationships in their environment41–44, \noftentimes described using Bayesian models of causal learning 45,46. \nThis cognitive ability encompasses recognizing patterns47,48, inferring \ncauses from interventions49,50, and predicting future events based on \nhypothetical events 51. Canonical tasks in this domain often involve \nassessing individuals’ ability to infer causal relationships, for example, \nwhen judging the responsibility of one object causing other objects’ \nmovement52,53. Causal reasoning remains a challenge, even for current \nmachine learning approaches54,55.\nResearch on intuitive psychology has explored how individuals \ninfer, understand and interpret social phenomena and mental states \nof other agents56,57. Past work on this topic has emphasized the concept \nthat humans possess an inherent ability to infer and reason about the \nmental states58,59, intentions and emotions of others, often referred to \nas a ‘theory of mind’60,61. This ability has been modelled as a Bayesian \ninference problem62–64. Canonical tasks in this domain often involve \nassessing individuals’ capacity to predict actions based on understand-\ning others’ perspectives or intentions, such as determining agents’ \nutility functions based on their actions in a given environment 65,66. It \nis the subject of ongoing debates whether modern algorithms show \nany form of intuitive psychology67–69.\nLake and colleagues argued that some of these abilities act as \n‘start-up software’ , because they constitute cognitive capabilities \npresent early in development. Moreover, they proposed that these \nso-called ‘intuitive theories’70,71 need to be expressed explicitly using \nthe calculus of Bayesian inference72, as opposed to being learned from \nscratch, for example, via gradient descent. However, with the increase \nin abilities of current neural networks, in particular LLMs, we pondered: \nCan LLMs, in particular vision LLMs, sufficiently solve problems from \nthese core domains?\nT o address this question, we took canonical tasks from the \ndomains of intuitive physics, causal reasoning and intuitive psychol-\nogy that could be studied by providing images and language-based \nTask-speci/f_ic prompt\nconstruction \nContext description\nTask description\nVisual cognitive tasks and datasets\nIntuitive physics Intuitive psychologyCausal reasoning\nLerer et al. 98\nEvaluation\nBasic visual queries\nTask prompts\nVisual stimuli\n+\nFuyu-8B\n8 billion parameters\nOtter\n7 billion parameters\nLlama Adapter\n7 billion parameters\nGPT-4V\n1.7 trillion parameters estimated\nClaude 3 Opus\n2 trillion parameters estimated\nb\na\nMLLM\nOutput re/f_inement\nZhou et al. 100\n Jara-Ettinger et al. 103\n Wu et al. 104\nGerstenberg et al. 52\nMultimodal LLMsc\nFig. 1 | Overview of domains, tasks, approach and models. a, Example images for \nthe different experiments. Each experiment was taken from one of three cognitive \ndomains: intuitive physics, causal reasoning and intuitive psychology. b, General \napproach. For every query, an image was submitted to the model, and different \nquestions were asked about the image, that is, we performed visual question \nanswering. c, Used multimodal LLMs and their size. MLLM, multimodal LLM.\nNature Machine Intelligence | Volume 7 | January 2025 | 96–106 98\nArticle https://doi.org/10.1038/s42256-024-00963-y\nand Schulz 73 showed that GPT-3 failed at simple causal reasoning \nexperiments, while Kosoy and colleagues91 showed that LLMs cannot \nlearn human-like causal over-hypotheses. In research on intuitive \npsychology, Kosinski argued that theory of mind might have emerged \nin LLMs68 which has been criticized other researchers69. Akata and col-\nleagues showed that GTP-4 plays repeated games very selfishly and \ncould not pick up on simple conventions such as alternating between \noptions16. Finally, Gandhi and colleagues92 proposed a framework for \nprocedurally generating theory of mind evaluations and found that \nGPT-4’s abilities mirror human inference patterns, although less reli-\nable, while all other LLMs struggled.\nMany of the past studies on LLMs have fallen risk to appearing in \nnew models’ training sets. Recent work has recognized this issue and, \nin turn, evaluated language models on many problem variations to \nminimize training set effects93. Our work differs from these approaches \nas current models could not have just memorized solutions to the given \nproblems because these problems require higher level reasoning. \nFurthermore, the human data and ground truth are most commonly \nstored in additional data files, which first have to be extracted and \nmatched to the respective images to be used for model training. Since \nthis requires data wrangling that cannot easily be automated and the \nnumber of stimuli to gain is so small, it is extremely unlikely that these \nstimuli together with the ground truth were entered into the training \nset of any of the investigated models.\nResults\nWe tested five different models on three core components for \nhuman-like intelligence as outlined in ref. 22 (Fig. 1a). The models we \nused are vision LLMs, which are multimodal models that integrate \nimage processing capabilities into LLMs94,95 (Fig. 1c). These models \nallow users to perform visual question answering96,97: users can upload \nan image and ask questions about it, which the model interprets and \nresponds to accordingly.\nT o test the three core components, we used tasks from the cogni-\ntive science literature that could be studied in vision LLMs via visual \nquestion answering. For every task, we queried the visual reasoning \nabilities of the LLMs with tasks of increasing complexity. First, we asked \nabout simple features of the shown images such as the background \ncolour or the number of objects shown. Afterwards, we submitted \nquestions taken from the cognitive science experiments. We report \nresults based on comparisons with the ground truth as well as the dif-\nferent models’ matches to human data.\nIntuitive physics with block towers\nT o test the intuitive physics capabilities of the different LLMs, we used \nphotographs depicting wooden block towers from ref. 98 (see Sup-\nplementary Fig. 1 for an example). We first asked models to determine \nthe background colour of the image. All four models achieved almost \nperfect accuracy (Fig. 2a). We then asked models to state the colour \nof blocks from top to bottom. Here, the performance of most models \nexcept for GPT-4V and Claude-3 deteriorated (Fig. 2b). Please note that \nthe first two tasks are fairly trivial for humans and we would expect \nhuman performance to be at 100% (the background colour is always \nwhite and images featured two, three or four blocks in primary colours).\nT o test the models’ physical reasoning abilities, we asked them to \ngive a binary stability judgement of the depicted block towers. Here, \nonly GPT-4V and Claude-3 performed slightly above chance (Fig. 2c; \nfor GPT-4V, Fisher’s exact test yielded an odds ratio of 2.597 with a \none-sided P value of 0.028). None of the other models performed sig-\nnificantly above chance (the second best performing model, Claude-3, \nhad an odds ratio of 2.016, with a one-sided P value of 0.078). Human \nparticipants were also not perfect but showed an average accuracy of \n65.608%.\nFinally, we determined the relationship between models’ and \nhumans’ stability judgements using a Bayesian logistic mixed effects \nregression. We compute a Bayesian R2 for each regression model based \non draws from the modelled residual variances 99. We then take the \nsquare root of this Bayesian R2 and multiply it with the sign of the main \nregression coefficient to arrive at a pseudo r value. Around this pseudo \nr value we plot the square root of the 95% percentiles for the R2 value \n(Fig. 2d). We found that GPT-4V was the only model that showed a rela-\ntion to human judgements, with a regression coefficient of 1.15 (95% \ncredible interval (95% CI) 1.04, 1.27) and an R2 value of 0.066. However, \nthe regression coefficient between individual humans and the mean \nover humans was still larger, with a coefficient of 1.46 (95% CI 1.41, 1.52) \nand an R2 value of 0.354.\nCausal reasoning with Jenga\nT o test the models’ causal reasoning capabilities, we used synthetic \nimages from refs. 100,101, which depicted block towers that were stable \nbut might collapse if one of the blocks was removed (see Supplemen-\ntary Fig. 2 for an example). We started by asking the models to count the \nblocks in the image. The images in this task displayed a larger number of \nblocks (ranging from 6 to 19), which made the basic counting task signifi-\ncantly more challenging than in the previous section. Models’ responses \nBackground colour\nBlock colour\nfrom top to bottom Stability Stability\n100\na c db\n75\n50\nPercentage correct\nFuyu\nOtter\nAdapter\nGPT-4V\nClaude-3\nFuyu\nOtter\nAdapter\nGPT-4V\nClaude-3\nFuyu\nOtter\nAdapter\nGPT-4V\nClaude-3\nHumans\nFuyu\nOtter\nAdapter\nGPT-4V\nClaude-3\nHumans\n25\n0\n100\n75\n50\nPercentage correct 25\n0\n100 1.00\n0.75\n0.50\n0.25\n0\n–0.25\n–0.50\n–0.75\n–1.00\n75\n50\nPercentage correct\nPseudo r to humans\n25\n0\nFig. 2 | Results for five vision LLMs for tasks of increasing complexity given \nimages of real block towers. a–c, We first ask for the background colour in the \nimage (a) (images were taken from ref. 98), then the colour of blocks from top to \nbottom (b) and finally a binary stability rating for the block towers (c). d, The last \nplot shows the square root of the R2 value for the Bayesian logistic mixed effects \nregression between models and human participants. Bars in plots a–c show \npercentage of correct answers with error bars given by the standard deviation \nof a binomial distribution (n = 100). Bars in plot d show the square root of the R2 \nvalues for Bayesian logistic mixed effects regressions with error bars given by the \nsquare root of the 95% percentiles for this R2 value (n = 10,700, number of images \ntimes number of human participants).\nNature Machine Intelligence | Volume 7 | January 2025 | 96–106\n 99\nArticle https://doi.org/10.1038/s42256-024-00963-y\napproximated the ground truth, albeit rarely matching it exactly. There-\nfore, we report the mean absolute distance to the ground truth instead \nof the percentage of correct answers (Fig. 3a). The models’ performance \nhighlighted the challenging nature of this task, with the best performing \nmodel (Claude-3) still being on average more than one block off.\nWe continued by querying the models for the number of blocks \nthat would fall if a specific block was removed from the scene (Fig. 3b,c). \nWe established a baseline performance represented by a horizontal \nline in Fig. 3b,c, which corresponds to a random agent that gave the \nmean between 0 and the number of blocks in each image as its’ predic-\ntion, essentially behaving like a uniform distribution over the possible \nnumber of blocks that could fall. Notably, both GPT-4V and Fuyu-8B \nsurpassed the random baseline, their performance levels being close \nto the human results reported in ref. 100 , which is depicted by the \nrightmost bar in the plot. However, GPT-4V still diverges significantly \nfrom the average over human participants (t(42) = 2.59, P < 0.05).\nFinally, we asked the models to rate the responsibility of a specific \nblock for the stability of the other blocks (Fig. 3d). Notably, all mod -\nels except for GPT-4V gave constant ratings for this task (Fuyu and \nClaude-3 always responded with 100, while Otter and LLaMA-Adapter \nV2 always responded with 50). The regression coefficient for GPT-4V \nwith human values is 0.16 (95% CI 0.10, 0.21) with an R2 value of 0.027. \nThe human-to-human regression has a coefficient of 0.54 (95% CI 0.45, \n0.63) and an R2 value of 0.268.\nCausal reasoning with Michotte\nFor the second test for causal reasoning abilities, we ran an experiment \nfrom ref. 52 that is based on the classic Michotte launching paradigm102. \nIt uses simple synthetic two-dimensional (2D) depictions of two balls \nlabelled ‘ A’ and ‘B’ with arrows showing their trajectories in front of \na white background (see Supplementary Fig. 3 for an example). We \nstarted by asking the models to determine the background colour of the \nimage (Fig. 4a). Most models perform fine with the exception of Fuyu, \nwhich always answers ‘pink’ (probably since pink is mentioned as the \ncolour of the gate in the prompt). Then, we asked models to infer the \ntrajectory of ball movement. This proved challenging for most models \n(Fig. 4b), which is surprising given that the prompt explicitly mentions \nthat the arrows in the stimuli depict the trajectory of the balls and the \nballs always move from right to left.\nWe then queried the models for their agreement on a scale from \n0 to 100 with the following questions: either ‘Ball B went through the \nmiddle of the gate’ (if ball B entered the gate) or ‘Ball B completely \nmissed the gate’ (if ball B missed the gate) (Fig. 4c). No model per -\nforms close to the human results reported in ref. 52 . The best per -\nforming model is Fuyu with a regression coefficient of 0.26 (95% CI \n−0.08, 0.61) and an R 2 value of 0.067. Interestingly, Claude-3 shows \na negative relationship with human judgements, with a regression \ncoefficient of −0.22 (95% CI −0.39, −0.06) and an R 2 value of 0.076. \nThe human-to-human regression coefficient is 0.85 (95% CI 0.69, 1.03) \nwith an R2 value of 0.556.\nFinally, we asked the models for their agreement on a scale from \n0 to 100 with the counterfactual question of whether ‘Ball B would \nhave gone through the gate had Ball A not been present in the scene’ \n(Fig. 4d). Notably, the closed-source models perform worse than some \nopen-source models for both tasks. Here, Fuyu is again the best per-\nforming model with a regression coefficient of 0.42 (95% CI 0.28, 0.57) \nand an R2 value of 0.185. Pseudo r  values for LLaMA-Adapter V2 and \nGPT-4V are missing, since the former gave only non-valid answers and \nlatter always responded with 100. The human-to-human regression \ncoefficient is 0.85 (95% CI 0.76, 0.93) with an R2 value of 0.698.\nIntuitive psychology with the astronaut task\nAs a first test for the intuitive psychology understanding of the dif -\nferent LLMs, we used synthetic images depicting an astronaut on a \ncoloured background from ref. 103  (see Supplementary Figs. 4 and \n5 for an example). The images featured different terrains and care \npackages. Depending on which terrain the astronaut crossed or which \ncare package they chose to pick up, it was possible to infer the costs \nassociated with the terrains and rewards associated with the care \npackages.\nAgain, we first tasked models with determining the background \ncolour of the images. Here, the performance of the models was worse \ncompared with the intuitive physics dataset (Fig. 5a), which might \nbe due to the fact that the background colour here was not uniform \n(Supplementary Fig. 5). We then asked models to count the number of \ncare packages in the scene. Most models except for GPT-4V struggled \nhere (Fig. 5b).\nAfterwards, we asked them to infer the costs associated with the \ndifferent terrains (Fig. 5c) and the rewards associated with different \ncare packages (Fig. 5d). All models only showed weak relations with \nthe average over human participants in their judgements about the \ncosts and rewards associated with the environment. The regression \nCounting\na b c dNumber of\nfalling blocks\nNumber of\nfalling blocks\nResponsibility\nratings\nFuyu\nOtter\nAdapter\nGPT-4V\nClaude-3\nFuyu\nOtter\nAdapter\nGPT-4V\nClaude-3\nHumans\nFuyu\nOtter\nAdapter\nGPT-4V\nClaude-3\nHumans\nFuyu\nOtter\nAdapter\nGPT-4V\nClaude-3\nHumans\n10\n8\n6\n4\n2\n0\nMean absolute distance\nto ground truth\nMean absolute distance\nto ground truth\n10\n8\n6\n4\n2\n0\nMean absolute distance\nto humans\n10\n8\n6\n4\n2\n0\nPseudo r to humans\n1.00\n0.75\n0.50\n0.25\n0\n–0.25\n–0.50\n–0.75\n–1.00\nFig. 3 | Results for Jenga causal reasoning experiment. a–d, We first ask for \nthe number of blocks in the image (a), then we ask for the number of blocks that \nwould fall if a specific block is removed and compute the absolute distance to \nthe ground truth (b) as well as the absolute distance to human judgements (c) \nand finally a rating between 0 and 100 for how responsible a specific block is for \nthe stability of the tower (d). The causal reasoning experiment was taken from \nref. 100. For the responsibility ratings, all LLMs except for GPT-4V give constant \nratings: Fuyu and Claude-3 always respond with 100, while Otter and LLaMA-\nAdapter V2 always respond with 50. Bars in plots a and b show absolute distance \nto ground truth with error bars given by the standard error of the mean (n = 42). \nBars in plot c show the distance to human answers with errors bars again given by \nthe standard error of the mean (n = 41). Bars in plot d show the square root of the \nR2 values for Bayesian logistic mixed effects regressions with error bars given by \nthe square root of the 95% percentiles for this R2 value (n = 1,470).\nNature Machine Intelligence | Volume 7 | January 2025 | 96–106 100\nArticle https://doi.org/10.1038/s42256-024-00963-y\ncoefficients of the models with the z-scaled mean over human partici-\npants ranged from −0.24 to 0.16 with R2 values between 0.025 to 0.04 \nfor cost questions, and from −0.02 to 0.39 (Claude-3, 95% CI 0.11, 0.66) \nwith R2 values between 0.015 and 0.110 for reward questions.\nIntuitive psychology with the help or hinder task\nThe new intuitive psychology dataset we added is taken from ref. 104. \nThis task shows a simple 2D depiction of two agents in a grid environ-\nment (see Supplementary Fig. 6 for an example). On each time step, \nthe agents can move up, down, left or right, or stay in place, but can-\nnot move through walls or boxes. The red agent has the objective of \nreaching a star in ten time steps. If the agent runs out of time they fail. \nThe blue agent has the objective of either helping or hindering the red \nagent by pushing or pulling boxes around.\nWe first asked models to determine the background colour in the \nscene and to determine the number of boxes in the scene (Fig. 6a,b). The \nclosed-source models are able to perfectly determine the background \ncolour (always white) but they nonetheless struggle with determining \nthe number of boxes in the scene (always 1, 2 or 3). Model answers for \nthe counting task ranged from 1 to 4, with only LLaMA-Adapter V2 giv-\ning constant responses of 2.\nWe then asked the models whether the blue agent tried to help or \nhinder the red agent (Fig. 6c). Here, Otter shows the highest regression \ncoefficient with human answers with 0.19 (95% CI 0.13, 0.25) and an R2 \nvalue of 0.038. Claude-3 shows a negative relationship with human \nanswers with a coefficient of −0.25 (95% CI −0.31, −0.20) and an R2 value of \n0.066. No model showed coefficients even close to the human-to-human \ncoefficient of 0.93 (95% CI 0.90, 0.96) with an R2 value of 0.858.\nFinally, we asked the model whether the red agent would have \nsucceeded in reaching the star, had the blue agent not been there. We \nshow the square root of the R 2 for the Bayesian linear mixed effects \nregression with 95% percentiles in Fig. 6d. Interestingly, the results \nhere flip, with Otter now showing a stronger negative relationship \nwith a coefficient of −0.40 (95% CI −0.47, −0.33) and an R 2 value of \n0.161 (this makes sense, since this task is essentially a counterfactual \nsimulation question similar to Fig. 4d, where Otter already showed a \nnegative relation to human judgements). GPT-4V and Claude-3 both \nshow small positive regression coefficients with humans answers: 0.15 \n(95% CI 0.09, 0.21) with an R2 value of 0.025, and 0.17 (95% CI 0.11, 0.23) \nwith an R2 value of 0.032, respectively. Again, no model coefficient is \nclose to the human-to-human coefficient of 0.83 (95% CI 0.80, 0.87) \nwith an R2 value of 0.688.\nColour\nFuyu\nAdapter\nGPT-4V\nClaude-3\nOtter\nFuyu\nAdapter\nGPT-4V\nClaude-3\nOtter\nFuyu\nAdapter\nGPT-4V\nClaude-3\nHumans\nOtter\nFuyu\nAdapter\nGPT-4V\nClaude-3\nHumans\nOtter\nPercentage correct\n100\n80\n60\n40\n20\n0\nPercentage correct\n100\n80\n60\n40\n20\n0\nPseudo r to humans\n1.00\n0.75\n0.50\n0.25\n0\n–0.25\n–0.50\n–0.75\n–1.00\nPseudo r to humans\n1.00\nCounterfactualOutcomeTrajectory\n0.75\n0.50\n0.25\n0\n–0.25\n–0.50\n–0.75\n–1.00\na b c d\nFig. 4 | Results for Michotte causal reasoning experiment. a–d, We first ask for \nthe background colour in the image (a), then the direction of ball movement  \n(b), a judgement between 0 and 100 on whether ball ‘B’ goes through the gate (c) \nand finally a counterfactual judgement between 0 and 100 on whether ball ‘B’ \nwould have gone through the gate, had ball ‘ A’ not been present in the scene (d). \nThe causal reasoning experiment was taken from ref. 52. Bars in plots a and b show \npercentage of correct answers with error bars given by the standard deviation of a \nbinomial distribution (n = 18). Bars in plots c and d show the square root of the R2 \nvalues for Bayesian logistic mixed effects regressions with error bars given by the \nsquare root of the 95% percentiles for this R2 value (n = 252 and 234, respectively).\nBackground colour Number of boxes Cost Rewarddcba\n100\n80\n60\n40\n20\n0\nFuyu\nOtter\nAdapter\nGPT-4V\nClaude-3\nFuyu\nOtter\nAdapter\nGPT-4V\nClaude-3\nFuyu\nOtter\nAdapter\nGPT-4V\nClaude-3\nFuyu\nOtter\nAdapter\nGPT-4V\nClaude-3\nPercentage correct\n100\n80\n60\n40\n20\n0\nPercentage correct\n1.00\n0.75\n0.50\n0.25\n0\n–0.25\n–0.50\n–0.75\n–1.00 Pseudo r to humans\n1.00\n0.75\n0.50\n0.25\n0\n–0.25\n–0.50\n–0.75\n–1.00\nPseudo r to humans\nFig. 5 | Results on astronaut task for intuitive psychology. a,b, Again, we first \nask for the background colour (a) and the number of boxes in the scene (b). \nc,d, Models are then asked to make inferences about the costs (c) and rewards \n(d) in an environment depending on the path an agent has taken. The tasks for \nintuitive psychology were taken from ref. 103. Regression coefficients for Fuyu \nand LLaMA-Adapter V2 are missing as they always responded with constant \nratings for either cost or reward questions. Bars in plots a and b show percentage \nof correct answers with error bars given by the standard deviation of a binomial \ndistribution (n = 16). Bars in plots c and d show the square root of the R2 values for \nBayesian logistic mixed effects regressions with error bars given by the square \nroot of the 95% percentiles for this R2 value (n = 81 and 70, respectively).\nNature Machine Intelligence | Volume 7 | January 2025 | 96–106\n 101\nArticle https://doi.org/10.1038/s42256-024-00963-y\nDiscussion\nWe started by asking whether, with the rise of modern LLMs, research-\ners have created machines that—at least to some degree—think like \npeople. T o address this question, we took four recent multimodal LLMs \nand probed their abilities in three core cognitive domains: intuitive \nphysics, causal reasoning and intuitive psychology.\nIn intuitive physics and causal reasoning, the models managed \nto solve some of the given tasks and GPT-4V showed a slight match \nwith human data. However, while they performed well in some tasks, \nthe models did not show a conclusive match with human data for the \ncausal reasoning experiments. Finally, in the intuitive psychology tasks, \nnone of the models showed a strong match with human data. Thus, an \nappropriate answer to the question motivating our work would be ‘No’ , \nor—perhaps more optimistically—‘Not quite’ .\nAlthough we have tried our best to give all models a fair chance \nand set up the experiments in a clean and replicable fashion, some \nshortcomings remain that should be addressed in future work. First of \nall, we have tested only a handful of multimodal models on just three \ncognitive domains. While we believe that the used models and tasks \nprovide good insights into the state-of-the-science of LLMs’ cognitive \nabilities, future studies should look at more domains and different \nmodels to further tease apart when and why LLMs can mimic human \nreasoning. For example, it would be interesting to see whether scale is \nthe only important feature influencing model performance105,106. Cur-\nrently, our evidence suggests that even smaller models, for example, \nFuyu, with its 8 billion parameters, can sometimes perform as well as \nGPT-4V in some tasks. Additionally, we applied all models out of the \nbox and without further fine-tuning. Future studies could attempt to \nfine-tune multimodal LLMs to better align with cognitive data 107 and \nassess whether this improves their reasoning abilities more gener -\nally. Similar to other recent work108, we found that many models were \nalready constrained in their basic visual processing. While the more \npowerful closed-source models performed more robustly on simple \nscene understanding tasks, we found that they still failed simple ques-\ntions that would be trivial for human observers. Thus, we think that the \nmodels’ weak performance in some domains can partially be explained \nby their poor basic visual processing capabilities.\nAnother shortcoming of the current work is the simplicity of the \nused stimuli. While the block towers used in our first study were delib-\nerately designed to be more realistic98 than commonly used psycho-\nlogical stimuli33, this was not true for the experiments in the other two \ndomains. For the intuitive psychology experiments, in particular, we \nwould expect the models to perform better if the stimuli contained \nmore realistic images of people, which has been shown to work better \nin previous studies109. Interestingly, using more realistic stimuli can \nalso change people’s causal judgements 110; how realistic the stimuli \nused in cognitive experiments should be remains an open question111.\nOn a related point, we used only static images in our current \nexperiments, which severely limits the breadth and level of detail of \nthe questions we could ask. For example, some of the most canonical \ntasks investigating people’s causal reasoning abilities involve videos of \ncolliding billiard balls52. As future LLMs will probably be able to answer \nquestions about videos 112, these tasks represent the next frontier of \ncognitively inspired benchmarks.\nFor the comparisons with human data, we used the participant \ndata collected in the original studies for all experiments, except for \nthe intuitive physics task, and assessed the correspondence between \nmodels and these data via a Bayesian mixed effects regression and R2 \nvalues. Future work could expand on this approach by collecting new \ndata from human participants choosing which of the model’s judge -\nments they prefer. This could lead to a more detailed comparison, \nsimilar to what has been proposed to discriminate among deep learning \nmodels for human vision113 and language114.\nA crucial weakness of most studies using LLMs is that they can \nbe sensitive to specific prompts115–117. While we have attempted to use \nprompts that elicited good behaviour, thereby giving LLMs a chance to \nperform well, future work could try to further optimize these prompts \nusing available methods 118–120, while also assessing how the models \nrespond to paraphrased versions of the same tasks. We present an \nexploratory analysis of the effects of response constraints and context \ncomplexity on human behaviour in the intuitive psychology astronaut \ntask in Supplementary Fig. 7. While response constraints and context \ncomplexity both influence model outputs, we also find that small vari-\nations to prompts on a character level can impact model behaviour, \nprobably due to tokenization. Taken together, this shows that evalu-\nations of visual LLMs are not only dependent on the specific models \nand experiments used, but also on the prompts and probably even \nhow these prompts are tokenized. While it could be possible to further \nengineer the used prompts, we believe that our current approach was \nsufficient to showcase these models’ abilities.\nOur work has shown that multimodal LLMs have come a long \nway, showing some correspondence to human behaviour and often \nperforming above chance. Moreover, machine learning researchers \nhave put forward various ideas about how to close the remaining gap \nBackground coloura b c dNumber of boxes Intention Counterfactual\n100\n80\n60\n40\n20\n0\nPercentage correct\n100\n80\n60\n40\n20\n0\nPercentage correct\n1.00\n0.75\n0.50\n0.25\n0\n–0.25\n–0.50\n–0.75\n–1.00\nPseudo r to humans\n1.00\n0.75\n0.50\n0.25\n0\n–0.25\n–0.50\n–0.75\n–1.00\nPseudo r to humans\nFuyu\nOtter\nAdapter\nGPT-4V\nClaude-3\nFuyu\nOtter\nAdapter\nGPT-4V\nClaude-3\nFuyu\nOtter\nAdapter\nGPT-4V\nClaude-3\nHumans\nFuyu\nOtter\nAdapter\nGPT-4V\nClaude-3\nHumans\nFig. 6 | Results on help or hinder task for intuitive psychology. a–d, We first ask \nfor the background colour in the image (a), then the number of boxes in the scene \n(b), a judgement between 0 and 100 on whether an agent in the scene tried to \nhinder the other agent (c) and finally a counterfactual judgement between 0 and \n100 on whether an agent in the scene would have successfully reached the goal, \nhad the other agent not been present (d). The intuitive psychology dataset was \nfrom taken from ref. 104. Bars in plots a and b show percentage of correct answers \nwith error bars given by the standard deviation of a binomial distribution (n = 24). \nBars in plots c and d show the square root of the R2 values for Bayesian logistic \nmixed effects regressions with error bars given by the square root of the 95% \npercentiles for this R2 value (n = 1,200).\nNature Machine Intelligence | Volume 7 | January 2025 | 96–106 102\nArticle https://doi.org/10.1038/s42256-024-00963-y\nbetween humans and machines121, including self-supervised learning122, \ntranslating from natural into probabilistic languages123 or grounding \nLLMs in realistic environments124. This continuous evolution in models’ \ncapabilities necessitates a re-evaluation of the metaphors and tools we \nuse to understand them. We believe that cognitive science can offer \ntools, theories and benchmarks to evaluate how close we have come \nto ‘building machines that learn and think like people’ .\nMethods\nCode\nThe open-source models were installed per the instructions on \ntheir related GitHub or Huggingface repositories and evaluated on \na Slurm-based cluster with a single A100. For the results reported as \nGPT-4V, we used the public ChatGPT interface and the OpenAI appli-\ncation programming interface (API), specifically the November 2023 \nrelease of gpt4-vision-preview model which is available via the com -\npletions endpoint. For Claude-3, we used the Anthropic API. Code for \nreplicating our results is available on GitHub (github.com/lsbuschoff/\nmultimodal). All models were evaluated in Python using PyT orch125. \nAdditional analyses were carried out using NumPy 126, Pandas127 and \nSciPy128. Matplotlib129 and Seaborn130 were used for plotting. Bayesian \nmixed effects models were computed using brms131 in R132.\nModels\nOpen-source. Fuyu is an 8 billion parameter multimodal text and image \ndecoder-only transformer. We used the Huggingface implementation \nwith standard settings and without further fine-tuning (available at \nhttps://huggingface.co/adept/fuyu-8b ). The maximum number of \ngenerated tokens was set to 8 and responses were parsed by hand. \nOtter is a multimodal LLM that supports in-context instruction tuning \nand is based on the OpenFlamingo model. We used the Huggingface \nimplementation of OTTER-Image-MPT7B (available at https://hugging-\nface.co/luodian/OTTER-Image-MPT7B), again with standard settings \nand without fine-tuning. The maximum number of generated tokens \nwas left at 512 and responses were parsed by hand. For LLaMA-Adapter \nV2, which adds adapters into LLaMA’s transformer to turn it into an \ninstruction-following model, we used the GitHub implementation of \nllama-adapter-v2-multimodal7b with standard settings and again with-\nout further fine-tuning (available at https://github.com/OpenGVLab/\nLLaMA-Adapter/tree/main/llama_adapter_v2_multimodal7b ). The \nmaximum number of generated tokens was left at 512 and responses \nwere parsed by hand.\nClosed-source.  We initially queried GPT-4V through the ChatGPT \ninterface, since the OpenAI API was not publicly available at the outset \nof this project. The intuitive psychology task responses were collected \nusing the gpt4-vision-preview model variant after its November 2023 \nrelease in the API. We set the maximum number of generated tokens \nfor a given prompt to 1 to get single numerical responses. All other \nparameters were set to their default values. Note that this model does \nnot currently feature an option for manually setting the temperature, \nand the provided documentation does not specify what the default \ntemperature is. We query Claude-3 using the Anthropic API. We use \nthe model version claude-3-opus-20240229 with a temperature of zero \nand the maximum number of new tokens between 3 and 6 depending \non the task.\nDatasets\nIntuitive physics with block towers. We tested the intuitive physical \nunderstanding of the models using images from ref. 98 . The photos \ndepict a block tower consisting of coloured wooden blocks in front of \na white fabric (see Supplementary Fig. 1 for an example). The images \nare of size 224 × 244. In the dataset, there are a total of 516 images of \nblock towers. We tested the models on 100 randomly drawn images. We \nfirst tested the models on their high-level visual understanding of the \nscenes: we tasked them with determining the background colour and \nthe number of blocks in the image. T o test their physical understanding, \nwe tested them on the same task as the original study: we asked them \nto give a binary rating on the stability of the depicted block towers. For \nthe first two tasks, we calculated the percentage of correct answers for \neach of the models. For the third task, we calculated a Bayesian linear \nmixed effects regression between human and model answers.\nDue to the limited sample size of the original human experiment, \nwe reran the human experiment from ref. 98 on Prolific with 107 partici-\npants (55 female and 52 male native English speakers with a mean age of \n27.73 (s.d. = 4.21)). All participants agreed to take part in the study and \nwere informed about the general purpose of the experiment. Experi-\nments were performed in accordance with the relevant guidelines \nand regulations approved by the ethics committee of the University \nof Tübingen. Participants first saw an example trial, followed by 100 \ntest images. In a two-alternative forced choice paradigm, participants \nwere asked whether the block tower in a given image was stable or not \nstable. They were paid £1.50 and the median time they took to com -\nplete the experiment was 08:08 min, making the average base reward \n£11.07 per hour. Additionally, they received a bonus payment of up to \n£1 depending on their performance (1 penny for each correct answer).\nCausal reasoning with Jenga. For the first causal reasoning experi -\nment, we used images from ref. 100. The images show artificial block \nstacks of red and grey blocks on a black table (see Supplementary \nFig. 2 for an example). The dataset consists of 42 images on which we \ntested all models. We again first tested the models on their high-level \nvisual understanding of the scene and therefore tasked them with \ndetermining the number of blocks in the scene. The ground truth \nnumber of blocks in the scenes ranged from 6 to 19. Since this task is \nrather challenging due to the increased number of blocks, we do not \nreport the percentage correct as for the intuitive physics dataset, but \nthe mean over the absolute distance between model predictions and \nthe ground truth for each image (Fig. 3a).\nT o test the causal reasoning of the models, we adopted the tasks \nperformed in the original study 100,101. We asked models to infer how \nmany red blocks would fall if the grey block was removed. For this \ncondition, Zhou and colleagues100 collected data from 42 participants. \nWe again report the absolute distance between model predictions \nand the ground truth for each image (Fig. 3b). We calculate a random \nbaseline which uses the mean between 0 and the number of blocks \nfor each specific image as the prediction. We also ask the models for \na rating between 0 and 100 for how responsible the grey block is for \nthe stability of the tower. Here, data for 41 human participants were \npublicly available. For both the number of blocks that would fall if the \ngrey block was removed, and its responsibility for the stability of the \ntower, we calculate the mean Pearson correlation to human participants \nfrom the original study (Fig. 3c).\nCausal reasoning with Michotte. For the second test for causal rea-\nsoning abilities, we used a task from ref. 52. It features 18 images which \nshow a 2D view of two balls and their trajectories on a flat surface (see \nSupplementary Fig. 3 for an example). This experiment is a variation of \nthe classic Michotte launching paradigm102, used to test visual causal \nperception. We again first tested the models on their high-level visual \nunderstanding of the scene: we first asked them to determine the \nbackground colour (Fig. 4a) and then the direction of ball movement \n(Fig. 4b) from the two options ‘left to right’ or ‘right to left’ (the balls \nalways moved from right to left).\nT o test the causal reasoning of the models, we adopted the tasks \nperformed in the original study. We asked models about the actual out-\ncome of the scene: ‘Did ball A enter the gate?’ As in the original experi-\nments, models had to indicate their agreement with this statement on \na scale from 0 (not at all) to 100 (completely). We then also asked the \ncounterfactual question: ‘Would ball A have entered the gate had it not \nNature Machine Intelligence | Volume 7 | January 2025 | 96–106\n 103\nArticle https://doi.org/10.1038/s42256-024-00963-y\ncollided with ball B?’ The original authors52 collected the responses of \n14 participants in the ‘outcome’ condition and 13 participants in the \n‘counterfactual’ condition. We here report the regression between \nmodel and human responses (Fig. 4c,d).\nIntuitive psychology with astronaut images task. T o test the intui-\ntive psychology of the different LLMs, we used stimuli from ref. 103 . \nThis part consisted of three different experiments, each consisting of \n16, 17 and 14 images showing a 2D depiction of an astronaut and care \npackages in different terrains (see Supplementary Figs. 4 and 5 for \nan example). T o check their high-level understanding of the images, \nwe again asked the models to determine the background colour of \nthe images. Since this background colour is not uniform, we counted \nboth ‘Pink’ and ‘Purple’ as correct answers. We report the percentage \nof correct answers for the background colour in Fig. 5a.\nIn accordance with the original study, analyses for the intuitive \npsychological capabilities of the models are split into cost questions \n(passing through a terrain is associated with a cost for the agent) and \nreward questions (collecting a care package yields some sort of reward \nfor the agent). We pooled cost and reward questions over all three \nexperiments and reported the mean Pearson correlation with the data \nof 90 human participants collected in ref. 103 (Fig. 5b,c). This heuristic \ncalculates the costs and rewards associated with the environment from \nthe amount of time an agent spends in each terrain and which care \npackage the agent collects.\nIntuitive psychology with the help or hinder task. The second intui-\ntive psychology experiment is taken from ref. 104 . It consists of 24 \nimages showing a 2D depiction of two agents in a grid world (see Sup-\nplementary Fig. 6 for an example). T o check the models’ basic under-\nstanding of the images, we again asked the models to determine the \nbackground colour of the images and the number of boxes in the scene. \nWe report the percentage of correct answers for both tasks in Fig. 6a,b.\nWe then asked the models whether the blue agent tried to help or hin-\nder the red agent on a scale from ‘definitely hinder RED’ (0) to ‘definitely \nhelp RED’ (100), with the midpoint ‘unsure’ (50). We show the regression \nto human judgements in Fig. 6c. Finally, we asked the model the coun-\nterfactual question if the red agent would have succeeded in reaching \nthe star had the blue agent not been there on a scale from ‘not at all’ (0) \nto ‘very much’ (100)? The original authors collected the responses of 50 \nparticipants for each of the two conditions (‘intention’ and ‘counterfac-\ntual’). We show the mixed linear regression coefficients between model \nand human answers for all models with 95% credible intervals in Fig. 6d.\nReporting summary\nFurther information on research design is available in the Nature \nPortfolio Reporting Summary linked to this article.\nData availability\nAll data used in our experiments are available on GitHub (github.com/\nlsbuschoff/multimodal). We have used subsets of openly available data-\nsets from Lerer et al. (https://github.com/facebookarchive/UET orch/\nissues/25#issuecomment-235688223)98, Gerstenberg et al. (https://\ngithub.com/tobiasgerstenberg/eye_tracking_causality)52, Zhou et al. \n(https://github.com/cicl-stanford/mental_jenga)100, Wu et al. (https://\ngithub.com/cicl-stanford/counterfactual_agents)104 and Jara-Ettinger \net al. (https://osf.io/uzs8r/)103.\nCode availability\nAll code needed to reproduce our results is available on GitHub (github.\ncom/lsbuschoff/multimodal; and via the Zenodo repository at https://\ndoi.org/10.5281/zenodo.14050104 (ref. 133)). We use openly available \nimplementations of all LLMs except for GPT-4V and Claude-3. The code \nincludes instructions on how to install and evaluate these LLMs. All \nprompts are listed in the Supplementary Information.\nReferences\n1. Mitchell, M. Artificial Intelligence: A Guide for Thinking Humans \n(Penguin, 2019).\n2. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: pre-training \nof deep bidirectional transformers for language understanding. \nIn Proc. 2019 Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Human Language \nTechnologies, Volume 1 (Long and Short Papers) (eds Burstein, J. et al.)  \n4171–4186 (Association for Computational Linguistics, 2019).\n3. Vaswani, A. et al. Attention is all you need. In Proc. Advances in \nNeural Information Processing Systems 30 (NIPS 2017) (eds Guyon, \nI. et al.) 5998–6008 (2017).\n4. Brown, T. et al. Language models are few-shot learners. In Proc. \nAdvances in Neural Information Processing Systems 33 (NeurIPS \n2020) (eds Larochelle, H. et al.) 1877–1901 (Curran Associates, \n2020).\n5. Bubeck, S. et al. Sparks of artificial general intelligence: \nearly experiments with GPT-4. Preprint at https://arxiv.org/\nabs/2303.12712 (2023).\n6. Wei, J. et al. Emergent abilities of large language models. Trans. \nMach. Learn. Res. https://openreview.net/forum?id=yzkSU5zdwD \n(2022).\n7. Katz, D. M., Bommarito, M. J., Gao, S. & Arredondo, P. GPT-4 passes \nthe bar exam. Phil. Trans. R. Soc. A 382, 2270 (2024).\n8. Sawicki, P. et al. On the power of special-purpose gpt models \nto create and evaluate new poetry in old styles. In Proc. 14th \nInternational Conference on Computational Creativity (ICCC’23) \n(eds Pease, A. et al.) 10–19 (Association for Computational \nCreativity, 2023).\n9. Borsos, Z. et al. Audiolm: a language modeling approach to audio \ngeneration. IEEE/ACM Trans. Audio Speech Lang. Process 31, \n2523–2533 (2023).\n10. Poldrack, R. A., Lu, T. & Beguš, G. Ai-assisted coding: experiments \nwith GPT-4. Preprint at https://arxiv.org/abs/2304.13187 (2023).\n11. Kasneci, E. et al. ChatGPT for good? On opportunities and \nchallenges of large language models for education. Learn. \nIndivid. Differ. 103, 102274 (2023).\n12. Bommasani, R. et al. On the opportunities and risks of foundation \nmodels. Preprint at https://arxiv.org/abs/2108.07258 (2021).\n13. Elkins, K. & Chun, J. Can GPT-3 pass a writer’s Turing test? J. Cult. \nAnal. 5, 2 (2020).\n14. Dell’Acqua, F. et al. Navigating the Jagged Technological Frontier: \nField Experimental Evidence of the Effects of AI on Knowledge \nWorker Productivity and Quality. Harvard Business School \nTechnology & Operations Mgt. Unit Working Paper (Harvard \nBusiness School, 2023).\n15. Bašić, Ž., Banovac, A., Kružić, I. & Jerković, I. Better by you, better \nthan me? ChatGPT-3.5 as writing assistance in students’ essays. \nHumanit. Soc. Sci. Commun. 10, 750 (2023).\n16. Akata, E. et al. Playing repeated games with large language \nmodels. Preprint at https://arxiv.org/abs/2305.16867 (2023).\n17. Simon, H. A. Cognitive science: the newest science of the \nartificial. Cogn. Sci. 4, 33–46 (1980).\n18. Rumelhart, D. E. et al. Parallel Distributed Processing, Vol.1 (MIT \nPress, 1987).\n19. Wichmann, F. A. & Geirhos, R. Are deep neural networks adequate \nbehavioral models of human visual perception? Annu. Rev. Vis. \nSci. 9, 501–524 (2023).\n20. Bowers, J. S. et al. On the importance of severely testing deep \nlearning models of cognition. Cogn. Syst. Res. 82, 101158 (2023).\n21. Marcus, G. Deep learning: a critical appraisal. Preprint at  \nhttps://arxiv.org/abs/1801.00631 (2018).\n22. Lake, B. M., Ullman, T. D., Tenenbaum, J. B. & Gershman, S. J. \nBuilding machines that learn and think like people. Behav. Brain \nSci. 40, e253 (2017).\nNature Machine Intelligence | Volume 7 | January 2025 | 96–106 104\nArticle https://doi.org/10.1038/s42256-024-00963-y\n23. Sejnowski, T. J. The Deep Learning Revolution (MIT, 2018).\n24. Smith, K. A., Battaglia, P. W. & Vul, E. Different physical intuitions \nexist between tasks, not domains. Comput. Brain Behav. 1, 101–118 \n(2018).\n25. Bates, C. J., Yildirim, I., Tenenbaum, J. B. & Battaglia, P. Modeling \nhuman intuitions about liquid flow with particle-based simulation. \nPLoS Comput. Biol. 15, e1007210 (2019).\n26. Battaglia, P. et al. Computational models of intuitive physics. Proc. \nAnnu. Meet. Cogn. Sci. Soc. 34, 32–33 (2012).\n27. Bramley, N. R., Gerstenberg, T., Tenenbaum, J. B. & Gureckis, T. M. \nIntuitive experimentation in the physical world. Cogn. Psychol. \n105, 9–38 (2018).\n28. Ullman, T. D. & Tenenbaum, J. B. Bayesian models of conceptual \ndevelopment: learning as building models of the world. Annu. \nRev. Dev. Psychol. 2, 533–558 (2020).\n29. Ullman, T. D., Spelke, E., Battaglia, P. & Tenenbaum, J. B. Mind \ngames: game engines as an architecture for intuitive physics. \nTrends Cogn. Sci. 21, 649–665 (2017).\n30. Hamrick, J., Battaglia, P. & Tenenbaum, J. B. Probabilistic internal \nphysics models guide judgments about object dynamics. Proc. \nAnnu. Meet. Cogn. Sci. Soc. 33, 1545–1550 (2011).\n31. Mildenhall, P. & Williams, J. Instability in students’ use of  \nintuitive and Newtonian models to predict motion: the critical \neffect of the parameters involved. Int. J. Sci. Educ. 23, 643–660 \n(2001).\n32. Todd, J. T. & Warren, W. H. Jr Visual perception of relative mass in \ndynamic events. Perception 11, 325–335 (1982).\n33. Battaglia, P. W., Hamrick, J. B. & Tenenbaum, J. B. Simulation as an \nengine of physical scene understanding. Proc. Natl Acad. Sci. USA \n110, 18327–18332 (2013).\n34. Hamrick, J. B., Battaglia, P. W., Griffiths, T. L. & Tenenbaum, J. B. \nInferring mass in complex scenes by mental simulation. Cognition \n157, 61–76 (2016).\n35. Bakhtin, A., van der Maaten, L., Johnson, J., Gustafson, L. & \nGirshick, R. PHYRE: a new benchmark for physical reasoning. \nIn Proc. Advances in Neural Information Processing Systems \n32 (NeurIPS 2019) (eds Wallach, H. et al.) 5082–5093 (Curran \nAssociates, 2019).\n36. Riochet, R. et al. Intphys: a framework and benchmark for \nvisual intuitive physics reasoning. Preprint at https://arxiv.org/\nabs/1803.07616 (2018).\n37. Schulze Buschoff, L. M., Schulz, E. & Binz, M. The acquisition of \nphysical knowledge in generative neural networks. In Proc. 40th \nInternational Conference on Machine Learning (eds Krause, A. \net al.) 202, 30321–30341 (JMLR, 2023).\n38. Waldmann, M. The Oxford Handbook of Causal Reasoning (Oxford \nUniv. Press, 2017).\n39. Cheng, P. W. From covariation to causation: a causal power \ntheory. Psychol. Rev. 104, 367 (1997).\n40. Holyoak, K. J. & Cheng, P. W. Causal learning and inference as \na rational process: the new synthesis. Annu. Rev. Psychol. 62, \n135–163 (2011).\n41. Pearl, J. Causality (Cambridge Univ. Press, 2009).\n42. Griffiths, T. L. & Tenenbaum, J. B. Theory-based causal induction. \nPsychol. Rev. 116, 661 (2009).\n43. Lagnado, D. A., Waldmann, M. R., Hagmayer, Y. & Sloman, S. A. in \nCausal Learning: Psychology, Philosophy, and Computation (eds \nGopnik, A. and Schulz, L.) 154–172 (Oxford Univ. Press, 2007).\n44. Carey, S. On the Origin of Causal Understanding (Clarendon Press/\nOxford Univ. Press, 1995).\n45. Gopnik, A. et al. A theory of causal learning in children: causal \nmaps and Bayes nets. Psychol. Rev. 111, 3 (2004).\n46. Lucas, C. G. & Griffiths, T. L. Learning the form of causal \nrelationships using hierarchical bayesian models. Cogn. Sci. 34, \n113–147 (2010).\n47. Bramley, N. R., Gerstenberg, T., Mayrhofer, R. & Lagnado, D. A. \nTime in causal structure learning. J. Exp. Psychol. Learn. Mem. \nCogn. 44, 1880 (2018).\n48. Griffiths, T. L. & Tenenbaum, J. B. Structure and strength in causal \ninduction. Cogn. Psychol. 51, 334–384 (2005).\n49. Schulz, L., Kushnir, T. & Gopnik, A. in Causal Learning: Psychology, \nPhilosophy, and Computation (eds Gopnik, A. and Schulz, L.) \n67–85 (Oxford Univ. Press, 2007).\n50. Bramley, N. R., Dayan, P., Griffiths, T. L. & Lagnado, D. A. \nFormalizing Neurath’s ship: approximate algorithms for online \ncausal learning. Psychol. Rev. 124, 301 (2017).\n51. Gerstenberg, T. What would have happened? Counterfactuals, \nhypotheticals and causal judgements. Philos. Trans. R. Soc. B 377, \n20210339 (2022).\n52. Gerstenberg, T., Peterson, M. F., Goodman, N. D., Lagnado, D. \nA. & Tenenbaum, J. B. Eye-tracking causality. Psychol. Sci. 28, \n1731–1744 (2017).\n53. Gerstenberg, T., Goodman, N. D., Lagnado, D. A. & Tenenbaum, \nJ. B. A counterfactual simulation model of causal judgments for \nphysical events. Psychol. Rev. 128, 936 (2021).\n54. Jin, Z. et al. CLadder: Assessing causal reasoning in language \nmodels. In Proc. Advances in Neural Information Processing \nSystems 36 (NeurIPS 2023) (eds Oh, A. et al.) 31038–31065 \n(Curran Associates, 2023).\n55. Dasgupta, I. et al. Causal reasoning from meta-reinforcement \nlearning. Preprint at https://arxiv.org/abs/1901.08162 (2019).\n56. Baker, C. L. & Tenenbaum, J. B. in Plan, Activity, and Intent \nRecognition: Theory and Practice (eds Sukthankar, G. et al.) \n177–204 (Morgan Kaufmann, 2014).\n57. Jern, A. & Kemp, C. A decision network account of reasoning \nabout other people’s choices. Cognition 142, 12–38 (2015).\n58. Vélez, N. & Gweon, H. Learning from other minds: an optimistic \ncritique of reinforcement learning models of social learning. Curr. \nOpin. Behav. Sci. 38, 110–115 (2021).\n59. Spelke, E. S., Bernier, E. P. & Skerry, A. Core Social Cognition \n(Oxford Univ. Press, 2013).\n60. Baker, C., Saxe, R. & Tenenbaum, J. Bayesian theory of mind: \nmodeling joint belief-desire attribution. Proc. Annu. Meet. Cogn. \nSci. Soc. 33, 2469–2474 (2011).\n61. Frith, C. & Frith, U. Theory of mind. Curr. Biol. 15, R644–R645 (2005).\n62. Saxe, R. & Houlihan, S. D. Formalizing emotion concepts within a \nBayesian model of theory of mind. Curr. Opin. Psychol. 17, 15–21 \n(2017).\n63. Baker, C. L. et al. Intuitive theories of mind: a rational approach \nto false belief. Proc. Annu. Meet. Cogn. Sci. Soc. 28, 69k8c7v6 \n(2006).\n64. Shum, M., Kleiman-Weiner, M., Littman, M. L. & Tenenbaum, J. \nB. Theory of minds: understanding behavior in groups through \ninverse planning. In Proc. 33rd AAAI Conference on Artificial \nIntelligence 6163–6170 (Curran Associates, 2019).\n65. Baker, C. L., Jara-Ettinger, J., Saxe, R. & Tenenbaum, J. B. Rational \nquantitative attribution of beliefs, desires and percepts in human \nmentalizing. Nat. Hum. Behav. 1, 0064 (2017).\n66. Zhi-Xuan, T. et al. Solving the baby intuitions benchmark with a \nhierarchically Bayesian theory of mind. Preprint at https://arxiv.\norg/abs/2208.02914 (2022).\n67. Rabinowitz, N. et al. Machine theory of mind. In Proc. 35th \nInternational Conference on Machine Learning (eds Dy, J. & Krause, \nA.) 80, 4218–4227 (JMLR, 2018).\n68. Kosinski, M. Theory of mind may have spontaneously emerged in \nlarge language models. Preprint at https://arxiv.org/abs/ \n2302.02083 (2023).\n69. Ullman, T. Large language models fail on trivial alterations to \ntheory-of-mind tasks. Preprint at https://arxiv.org/abs/2302.08399 \n(2023).\nNature Machine Intelligence | Volume 7 | January 2025 | 96–106\n 105\nArticle https://doi.org/10.1038/s42256-024-00963-y\n70. Schulz, L. The origins of inquiry: inductive inference and exploration \nin early childhood. Trends Cogn. Sci. 16, 382–389 (2012).\n71. Ullman, T. D. On the Nature and Origin of Intuitive Theories: \nLearning, Physics and Psychology. PhD thesis, Massachusetts \nInstitute of Technology (2015).\n72. Tenenbaum, J. B., Kemp, C., Griffiths, T. L. & Goodman, N. D. How \nto grow a mind: statistics, structure, and abstraction. Science 331, \n1279–1285 (2011).\n73. Binz, M. & Schulz, E. Using cognitive psychology to understand \nGPT-3. Proc. Natl Acad. Sci. USA 120, e2218523120 (2023).\n74. Huang, J. & Chang, K. C.-C. Towards reasoning in large language \nmodels: a survey. In Findings of the Association for Computational \nLinguistics: ACL 2023 (eds Rogers, A. et al.) 1049–1065 \n(Association for Computational Linguistics, 2023).\n75. Sawada, T. et al. ARB: Advanced reasoning benchmark for large \nlanguage models. Preprint at https://arxiv.org/abs/2307.13692 \n(2023).\n76. Wei, J. et al. Chain-of-thought prompting elicits reasoning in \nlarge language models. In Proc. Advances in Neural Information \nProcessing Systems 35 (NeurIPS 2022) (eds Koyejo, S. et al.) \n24824–24837 (Curran Associates, 2022).\n77. Webb, T., Holyoak, K. J. & Lu, H. Emergent analogical reasoning in \nlarge language models. Nat. Hum. Behav. 7, 1526–1541 (2023).\n78. Coda-Forno, J. et al. Inducing anxiety in large language models \nincreases exploration and bias. Preprint at https://arxiv.org/\nabs/2304.11111 (2023).\n79. Eisape, T. et al. A systematic comparison of syllogistic reasoning \nin humans and language models. In Proc. 2024 Conference of \nthe North American Chapter of the Association for Computational \nLinguistics: Human Language Technologies (Volume 1: Long \nPapers) (eds Duh, K. et al.) 8425–8444 (Association for \nComputational Linguistics, 2024).\n80. Hagendorff, T., Fabi, S. & Kosinski, M. Human-like intuitive \nbehavior and reasoning biases emerged in large language models \nbut disappeared in ChatGPT. Nat. Comput. Sci. 3, 833–838 (2023).\n81. Ettinger, A. What BERT is not: Lessons from a new suite of \npsycholinguistic diagnostics for language models. Trans. Assoc. \nComput. Linguist. 8, 34–48 (2020).\n82. Jones, C. R. et al. Distrubutional semantics still can’t account for \naffordances. Proc. Annu. Meet. Cogn. Sci. Soc. 44, 482–489 (2022).\n83. Rahwan, I. et al. Machine behaviour. Nature 568, 477–486 (2019).\n84. Schulz, E. & Dayan, P. Computational psychiatry for computers. \niScience 23, 12 (2020).\n85. Rich, A. S. & Gureckis, T. M. Lessons for artificial intelligence from \nthe study of natural stupidity. Nat. Mach. Intell. 1, 174–180 (2019).\n86. Zhang, Y., Pan, J., Zhou, Y., Pan, R. & Chai, J. Grounding visual \nillusions in language: do vision-language models perceive \nillusions like humans? In Proc. 2023 Conference on Empirical \nMethods in Natural Language (eds Bouamor, H. et al.) 5718–5728 \n(Association for Computational Linguistics, 2023).\n87. Mitchell, M., Palmarini, A. B. & Moskvichev, A. Comparing \nhumans, GPT-4, and GPT-4v on abstraction and reasoning tasks. \nPreprint at https://arxiv.org/abs/2311.09247 (2023).\n88. Zečević, M., Willig, M., Dhami, D. S. & Kersting, K. Causal \nparrots: large language models may talk causality but are \nnot causal. Trans. Mach. Learn. Res. https://openreview.net/\npdf?id=tv46tCzs83 (2023).\n89. Zhang, C., Wong, L., Grand, G. & Tenenbaum, J. Grounded \nphysical language understanding with probabilistic programs \nand simulated worlds. Proc. Annu. Meet. Cogn. Sci. Soc. 45, \n3476–3483 (2023).\n90. Jassim, S. et al. GRASP: a novel benchmark for evaluating \nlanguage grounding and situated physics understanding in \nmultimodal language models. Preprint at https://arxiv.org/\nabs/2311.09048 (2023).\n91. Kosoy, E. et al. Towards understanding how machines can learn \ncausal overhypotheses. Proc. Annu. Meet. Cogn. Sci. Soc. 45, \n363–374 (2023).\n92. Gandhi, K., Fränken, J.-P., Gerstenberg, T. & Goodman, N. D. \nUnderstanding social reasoning in language models with \nlanguage models. In Proc. 37th International Conference on Neural \nInformation Processing Systems (NIPS ’23) (eds Oh, A. et al.) \n13518–13529 (Curran Associates, 2024).\n93. Srivastava, A. et al. Beyond the imitation game: quantifying and \nextrapolating the capabilities of language models. Preprint at \nhttps://arxiv.org/abs/2206.04615 (2022).\n94. Baltrušaitis, T., Ahuja, C. & Morency, L.-P. Multimodal machine \nlearning: a survey and taxonomy. IEEE Trans. Pattern Anal. Mach. \nIntell. 41, 423–443 (2018).\n95. Reed, S. et al. Generative adversarial text to image synthesis. In \nProc. 33rd International Conference on Machine Learning (eds \nBalcan, M. F. & Weinberger, K. Q.) 48, 1060–1069 (JMLR, 2016).\n96. Wu, Q. et al. Visual question answering: a survey of methods and \ndatasets. Comput. Vis. Image Underst. 163, 21–40 (2017).\n97. Manmadhan, S. & Kovoor, B. C. Visual question answering: a \nstate-of-the-art review. Artif. Intell. Rev. 53, 5705–5745  \n(2020).\n98. Lerer, A., Gross, S. & Fergus, R. Learning physical intuition of \nblock towers by example. In Proc. 33rd International Conference \non Machine Learning (eds Balcan, M. F. & Weinberger, K. Q.) 48, \n430–438 (JMLR, 2016).\n99. Gelman, A., Goodrich, B., Gabry, J. & Vehtari, A. R-squared for \nBayesian regression models. Am. Stat. 73, 307–309 (2019).\n100. Zhou, L., Smith, K. A., Tenenbaum, J. B. & Gerstenberg, T. \n Mental Jenga: a counterfactual simulation model of causal \njudgments about physical support. J. Exp. Psychol. Gen. 152, 2237 \n(2023).\n101. Gerstenberg, T., Zhou, L., Smith, K. A. & Tenenbaum, J. B.  \nFaulty towers: A hypothetical simulation model of physical \nsupport. Proc. Annu. Meet. Cogn. Sci. Soc. 39, 409–414  \n(2017).\n102. Michotte, A. The Perception of Causality (Basic Books, 1963).\n103. Jara-Ettinger, J., Schulz, L. E. & Tenenbaum, J. B. The naïve \nutility calculus as a unified, quantitative framework for action \nunderstanding. Cogn. Psychol. 123, 101334 (2020).\n104. Wu, S. A., Sridhar, S. & Gerstenberg, T. A computational model \nof responsibility judgments from counterfactual simulations \nand intention inferences. Proc. Annu. Meet. Cogn. Sci. Soc. 45, \n3375–3382 (2023).\n105. Sutton, R. The bitter lesson. Incomplete Ideas http://www.\nincompleteideas.net/IncIdeas/BitterLesson.html (2019).\n106. Kaplan, J. et al. Scaling laws for neural language models. Preprint \nat https://arxiv.org/abs/2001.08361 (2020).\n107. Binz, M. & Schulz, E. Turning large language models into \ncognitive models. In Proc. 12th International Conference on \nLearning Representations (ICLR) https://openreview.net/\nforum?id=eiC4BKypf1 (OpenReview, 2024).\n108. Rahmanzadehgervi, P., Bolton, L., Taesiri, M. R. & Nguyen, A. T. \nVision language models are blind. In Proc. Asian Conference on \nComputer Vision (ACCV) 18–34 (Computer Vision Foundation, \n2024).\n109. Ju, C., Han, T., Zheng, K., Zhang, Y. & Xie, W. Prompting \nvisual-language models for efficient video understanding. In Proc. \nComputer Vision – ECCV 2022: 17th European Conference (eds \nAvidan, S. et al.) 105–124 (Springer, 2022).\n110. Meding, K., Bruijns, S. A., Schölkopf, B., Berens, P. & Wichmann, F. A.  \nPhenomenal causality and sensory realism. Iperception 11, \n2041669520927038 (2020).\n111. Allen, K. R. et al. Using games to understand the mind. Nat. Hum. \nBehav. 8, 1035–1043 (2024).\nNature Machine Intelligence | Volume 7 | January 2025 | 96–106 106\nArticle https://doi.org/10.1038/s42256-024-00963-y\n112. Maaz, M., Rasheed, H., Khan, S. & Khan, F. Video-ChatGPT: \nTowards detailed video understanding via large vision and \nlanguage models. In Proc. 62nd Annual Meeting of the Association \nfor Computational Linguistics (Volume 1: Long Papers)  \n(eds Ku, L.-W. et al.) 12585–12602 (Association for Computational \nLinguistics, 2024).\n113. Golan, T., Raju, P. C. & Kriegeskorte, N. Controversial stimuli: \npitting neural networks against each other as models of human \ncognition. Proc. Natl Acad. Sci. USA 117, 29330–29337 (2020).\n114. Golan, T., Siegelman, M., Kriegeskorte, N. & Baldassano, C. Testing \nthe limits of natural language models for predicting human \nlanguage judgements. Nat. Mach. Intell. 5, 952–964 (2023).\n115. Reynolds, L. & McDonell, K. Prompt programming for large \nlanguage models: beyond the few-shot paradigm. In Extended \nAbstracts of the 2021 CHI Conference on Human Factors in \nComputing Systems (eds Kitamura, Y. et al.) 314 (Association for \nComputing Machinery, 2021).\n116. Strobelt, H. et al. Interactive and visual prompt engineering for \nad-hoc task adaptation with large language models. IEEE Trans. \nVis. Comput. Graph. 29, 1146–1156 (2022).\n117. Webson, A. & Pavlick, E. Do prompt-based models really \nunderstand the meaning of their prompts? In Proc. 2022 \nConference of the North American Chapter of the Association for \nComputational Linguistics: Human Language Technologies (eds \nCarpuat, M. et al.) 2300–2344 (Association for Computational \nLinguistics, 2022).\n118. Liu, P. et al. Pre-train, prompt, and predict: a systematic survey \nof prompting methods in natural language processing. ACM \nComput. Surv. 55, 195 (2023).\n119. Gu, J. et al. A systematic survey of prompt engineering on \nvision-language foundation models. Preprint at https://arxiv.org/\nabs/2307.12980 (2023).\n120. Coda-Forno, J. et al. Meta-in-context learning in large language \nmodels. In Proc. Advances in Neural Information Processing \nSystems 36 (NeurIPS 2023) (eds Oh, A. et al.) 65189–65201 \n(Curran Associates, 2023).\n121. Geirhos, R. et al. Partial success in closing the gap between \nhuman and machine vision. In Proc. Advances in Neural \nInformation Processing Systems 34 (NeurIPS 2021) (eds Ranzato, \nM. et al.) 23885–23899 (Curran Associates, 2021).\n122. Balestriero, R. et al. A cookbook of self-supervised learning. \nPreprint at https://arxiv.org/abs/2304.12210 (2023).\n123. Wong, L. et al. From word models to world models: translating \nfrom natural language to the probabilistic language of thought. \nPreprint at https://arxiv.org/abs/2306.12672 (2023).\n124. Carta, T. et al. Grounding large language models in interactive \nenvironments with online reinforcement learning. In Proc. 40th \nInternational Conference on Machine Learning (eds Krause, A. \net al.) 202, 3676–3713 (JMLR, 2023).\n125. Paszke, A. et al. Pytorch: An imperative style, high-performance \ndeep learning library. In Proc. Advances in Neural Information \nProcessing Systems 32 (NeurIPS 2019) (eds Wallach, H. et al.) \n8026–8037 (Curran Associates, 2019).\n126. Harris, C. R. et al. Array programming with NumPy. Nature 585, \n357–362 (2020).\n127. Pandas Development Team. pandas-dev/pandas: Pandas. Zenodo \nhttps://doi.org/10.5281/zenodo.3509134 (2020).\n128. Virtanen, P. et al. SciPy 1.0: fundamental algorithms for scientific \ncomputing in Python. Nat. Methods 17, 261–272 (2020).\n129. Hunter, J. D. Matplotlib: a 2D graphics environment. Comput. Sci. \nEng. 9, 90–95 (2007).\n130. Waskom, M. L. seaborn: statistical data visualization. J. Open \nSource Softw 6, 3021 (2021).\n131. Bürkner, P.-C. brms: an R package for Bayesian multilevel models \nusing Stan. J. Stat. Softw. https://doi.org/10.18637/jss.v080.i01 \n(2017).\n132. R Core Team. R: a language and environment for statistical \ncomputing (R Foundation for Statistical Computing, 2021).\n133. Schulze Buschoff, L. M. et al. lsbuschoff/multimodal: First release. \nZenodo https://doi.org/10.5281/zenodo.14050104 (2024).\nAcknowledgements\nWe thank M. Thalmann and C. Demircan for helpful discussions on the \nstatistical analysis. This work was supported by the Max Planck Society \n(L.M.S.B. and E.S.), the Volkswagen Foundation (L.M.S.B. and E.S.), the \nGerman Federal Ministry of Education and Research (BMBF): Tübingen \nAI Center, FKZ: grant no. 01IS18039A (E.A. and M.B.), and the Deutsche \nForschungsgemeinschaft (DFG, German Research Foundation) under \nGermany’s Excellence Strategy—EXC 2064/1—grant no. 390727645 \n(E.A. and M.B.).\nAuthor contributions\nL.M.S.B. and E.S. conceived the study. L.M.S.B. and E.A. conducted \nthe VLM experiments. E.A. conducted the human experiment. L.M.S.B. \nanalysed the results with input from E.S. L.M.S.B., E.A. and E.S. wrote \nthe manuscript with input from M.B.\nFunding\nOpen access funding provided by Helmholtz Zentrum München - \nDeutsches Forschungszentrum für Gesundheit und Umwelt (GmbH).\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary information The online version contains supplementary \nmaterial available at https://doi.org/10.1038/s42256-024-00963-y.\nCorrespondence and requests for materials should be addressed to \nLuca M. Schulze Buschoff.\nPeer review information Nature Machine Intelligence thanks Taylor \nWebb and Michael Frank for their contribution to the peer review of \nthis work.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\n© The Author(s) 2025\n\n\n"
}