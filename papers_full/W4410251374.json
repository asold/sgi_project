{
  "title": "Playing repeated games with large language models",
  "url": "https://openalex.org/W4410251374",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A4378734046",
      "name": "Akata, Elif",
      "affiliations": [
        "Max Planck Institute for Biological Cybernetics",
        "Institute of Bioinformatics and Systems Biology",
        "University of Tübingen"
      ]
    },
    {
      "id": "https://openalex.org/A4378734047",
      "name": "Schulz, Lion",
      "affiliations": [
        "Max Planck Institute for Biological Cybernetics"
      ]
    },
    {
      "id": "https://openalex.org/A4366951659",
      "name": "Coda-Forno, Julian",
      "affiliations": [
        "Max Planck Institute for Biological Cybernetics",
        "Institute of Bioinformatics and Systems Biology"
      ]
    },
    {
      "id": "https://openalex.org/A3191246918",
      "name": "Oh Seong Joon",
      "affiliations": [
        "University of Tübingen"
      ]
    },
    {
      "id": "https://openalex.org/A2398904668",
      "name": "Bethge, Matthias",
      "affiliations": [
        "University of Tübingen"
      ]
    },
    {
      "id": "https://openalex.org/A4227482182",
      "name": "Schulz, Eric",
      "affiliations": [
        "Institute of Bioinformatics and Systems Biology",
        "Max Planck Institute for Biological Cybernetics"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6838461927",
    "https://openalex.org/W4385430086",
    "https://openalex.org/W6798182279",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W6853344003",
    "https://openalex.org/W6800751262",
    "https://openalex.org/W3126088866",
    "https://openalex.org/W2206554859",
    "https://openalex.org/W2919988685",
    "https://openalex.org/W2101440292",
    "https://openalex.org/W2113419089",
    "https://openalex.org/W2130140239",
    "https://openalex.org/W4390779517",
    "https://openalex.org/W4244750590",
    "https://openalex.org/W2909687704",
    "https://openalex.org/W2001635709",
    "https://openalex.org/W2062663664",
    "https://openalex.org/W2309467108",
    "https://openalex.org/W2111282827",
    "https://openalex.org/W2133360573",
    "https://openalex.org/W2797144728",
    "https://openalex.org/W2038774830",
    "https://openalex.org/W2115670597",
    "https://openalex.org/W2099446300",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2970536767",
    "https://openalex.org/W4403863303",
    "https://openalex.org/W4363624465",
    "https://openalex.org/W2056816934",
    "https://openalex.org/W2157592153",
    "https://openalex.org/W2167062553",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2064457934",
    "https://openalex.org/W2116738092",
    "https://openalex.org/W3096397561",
    "https://openalex.org/W4318919287",
    "https://openalex.org/W4366850553",
    "https://openalex.org/W6748203849",
    "https://openalex.org/W3021931489",
    "https://openalex.org/W2010931158",
    "https://openalex.org/W3136535094",
    "https://openalex.org/W1996670568",
    "https://openalex.org/W2134192919",
    "https://openalex.org/W4297075189",
    "https://openalex.org/W3132838514",
    "https://openalex.org/W4400681340",
    "https://openalex.org/W1963523946",
    "https://openalex.org/W3096831136",
    "https://openalex.org/W4387580372",
    "https://openalex.org/W4391020701",
    "https://openalex.org/W2779206865",
    "https://openalex.org/W3123265060",
    "https://openalex.org/W3126056200",
    "https://openalex.org/W2220771499",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W1978662219"
  ],
  "abstract": "Abstract Large language models (LLMs) are increasingly used in applications where they interact with humans and other agents. We propose to use behavioural game theory to study LLMs’ cooperation and coordination behaviour. Here we let different LLMs play finitely repeated 2 × 2 games with each other, with human-like strategies, and actual human players. Our results show that LLMs perform particularly well at self-interested games such as the iterated Prisoner’s Dilemma family. However, they behave suboptimally in games that require coordination, such as the Battle of the Sexes. We verify that these behavioural signatures are stable across robustness checks. We also show how GPT-4’s behaviour can be modulated by providing additional information about its opponent and by using a ‘social chain-of-thought’ strategy. This also leads to better scores and more successful coordination when interacting with human players. These results enrich our understanding of LLMs’ social behaviour and pave the way for a behavioural game theory for machines.",
  "full_text": "Nature Human Behaviour | Volume 9 | July 2025 | 1380–1390 1380\nnature human behaviour\nArticle\nhttps://doi.org/10.1038/s41562-025-02172-y\nPlaying repeated games with large language \nmodels\n \nElif Akata    1,2,3 , Lion Schulz2, Julian Coda-Forno1,2, Seong Joon Oh3, \nMatthias Bethge3 & Eric Schulz1,2\nLarge language models (LLMs) are increasingly used in applications where \nthey interact with humans and other agents. We propose to use behavioural \ngame theory to study LLMs’ cooperation and coordination behaviour.  \nHere we let different LLMs play finitely repeated 2 × 2 games with each other, \nwith human-like strategies, and actual human players. Our results show \nthat LLMs perform particularly well at self-interested games such as the \niterated Prisoner’s Dilemma family. However, they behave suboptimally in \ngames that require coordination, such as the Battle of the Sexes. We verify \nthat these behavioural signatures are stable across robustness checks. We \nalso show how GPT-4’s behaviour can be modulated by providing additional \ninformation about its opponent and by using a ‘social chain-of-thought’ \nstrategy. This also leads to better scores and more successful coordination \nwhen interacting with human players. These results enrich our under-\nstanding of LLMs’ social behaviour and pave the way for a behavioural game \ntheory for machines.\nLarge language models (LLMs) are deep learning models with billions of \nparameters trained on huge corpora of text1–3. While they can generate \ntext that human evaluators struggle to distinguish from text written \nby other humans4, they have also shown other, emerging abilities5. \nThey can, for example, solve analogical reasoning tasks6, program web \napplications7, use tools to solve multiple tasks8 or adapt their strategies \npurely in-context9. Because of these abilities and their increasing popu-\nlarity, LLMs are already transforming our daily lives as they permeate \ninto many applications10. This means that LLMs will interact with us and \nother agents—LLMs or otherwise—frequently and repeatedly. How do \nLLMs behave in these repeated social interactions?\nMeasuring how people behave in repeated interactions, for \nexample, how they cooperate 11 and coordinate 12, is the subject of a \nsubfield of behavioural economics called behavioural game theory13. \nWhile traditional game theory assumes that people’s strategic deci -\nsions are rational, selfish and focused on utility maximization 14,15, \nbehavioural game theory has shown that human agents deviate \nfrom these principles and, therefore, examines how their decisions \nare shaped by social preferences, social utility and other psycho -\nlogical factors 16. Thus, behavioural game theory lends itself well to \nstudying the repeated interactions of diverse agents 17,18, including  \nartificial agents19.\nIn this Article, we analyse LLMs’ behavioural patterns by letting \nthem play finitely repeated games with full information and against \nother LLMs, simple, human-like strategies and actual human players. \nFinitely repeated games have been engineered to understand how \nagents should and do behave in interactions over many iterations. We \nfocus on two-player games with two discrete actions, that is, 2 × 2 games \n(see Fig. 1 for an overview).\nAnalysing LLMs’ performance across families of games, we find \nthat they perform well in games that value pure self-interest, especially \nthose from the Prisoner’s Dilemma family. However, they underperform \nin games that involve coordination. Based on this finding, we further \nfocus on games taken from these families and, in particular, on the cur-\nrently largest LLM: GPT-4 (ref. 20). In the canonical Prisoner’s Dilemma, \nwhich assesses how agents cooperate and defect, we find that GPT-4 \nretaliates repeatedly, even after having experienced only one defection. \nBecause this can indeed be the equilibrium individual-level strategy, \nGPT-4 is good at these games because it is particularly unforgiving and \nselfish. However, in the Battle of the Sexes, which assesses how agents \nReceived: 6 December 2023\nAccepted: 12 March 2025\nPublished online: 8 May 2025\n Check for updates\n1Institute for Human-Centered AI, Helmholtz Munich, Oberschleißheim, Germany. 2Max Planck Institute for Biological Cybernetics, Tübingen, Germany. \n3University of Tübingen, Tübingen, Germany.  e-mail: elif.akata@helmholtz-munich.de\nNature Human Behaviour | Volume 9 | July 2025 | 1380–1390\n 1381\nArticle https://doi.org/10.1038/s41562-025-02172-y\nplayer; and second-best games involve suboptimal outcomes where \nno player achieves their ideal result. The sample size for each game \nfamily differs due to the specific characteristics and properties that \ndefine each family. Some families have more members due to a wider \nrange of configurations that fit their criteria, while others have fewer \ngames because their structural requirements are more restrictive. For \nexample, the Prisoner’s Dilemma family is constrained by a structure \nwhere both players have a dominant strategy to defect, leading to a \nsuboptimal equilibrium. Meanwhile, win–win games can have multiple \nequilibria, which provides more flexibility.\ntrade off between their own and their partners’ preferences, we find \nthat GPT-4 does not manage to coordinate with simple, human-like \nagents that alternate between options over trials. Thus, GPT-4 is bad \nat these games because it is uncoordinated. We also verify that these \nbehaviours are not due to an inability to predict the other player’s \nactions, and persist across several robustness checks and changes to \nthe pay-off matrices. We point to two ways in which these behaviours \ncan be changed. GPT-4 can be made to act more forgivingly by point-\ning out that the other player can make mistakes. Moreover, GPT-4 gets  \nbetter at coordinating with the other player when it is first asked to pre-\ndict their actions before choosing an action itself, an approach we term \nsocial chain-of-thought (SCoT) prompting. Finally, we let GPT-4 with \nand without SCoT-prompting play the canonical Prisoner’s Dilemma \nand the Battle of the Sexes with human players. We find that SCoT \nprompting leads to more successful coordination and joint coopera-\ntion between participants and LLMs and makes participants believe \nmore frequently that the other player is human.\nResults\nUsing GPT-4, text-davinci-002, text-davinci-003, Claude 2 and Llama 2 \n70B, we evaluate a range of 2 × 2 games. For the analysis of two particu-\nlar games, we let all the LLMs and human-like strategies play against \neach other. We focus on LLMs’ behaviour in cooperation and coordi-\nnation games.\nAnalysing behaviour across families of games\nWe start out our experiments by letting the three LLMs play games \nfrom different families with each other. We focus on all known types of \n2 × 2 games from the families of win–win, biased, second-best, cyclic \nand unfair games as well as all games from the Prisoner’s Dilemma \nfamily21,22. We show example pay-off matrices for each type of game \nin Fig. 2.\nWe let all LLMs play with every other LLM, including themselves, \nfor all games repeatedly over ten rounds and with all LLMs as either \nplayer 1 or player 2. This leads to 1,224 games in total: 324 win–win, \n63 Prisoner’s Dilemma, 171 unfair, 162 cyclic, 396 biased and 108 \nsecond-best games. Win–win games result in mutually beneficial \noutcomes for both players; Prisoner’s Dilemma involves a conflict \nbetween individual and collective actions; unfair games have skewed \noutcomes favouring one player; cyclic games feature outcomes where \npreferences rotate; biased games have inherent advantages for one \nJ\nFootball\n7\n10\n0\n0\n10\n0\n0\n7\nF\nBallet\nJ\nFootball\nF\nBallet\nPlayer 2Player 1\nIn round 1, you chose Option J  and the\nother player chose Option F . Thus, you won \n0 points and the other player won 0 points.\nYou are playing a game repeatedly with\nanother player. In this game, you can\nchoose between Option J and Option F. You\nwill play 10 rounds in total with the same\nplayer. The rules of the game are as\nfollows:\nIf you choose Option J and the other player\nchooses Option J, then you win 10 points\nand the other player wins 7 points.\nIf you choose Option J and the other player\nchooses Option F, then you win 0 points and\nthe other player wins 0 points.\nIf you choose Option F and the other player\nchooses Option J, then you win 0 points and\nthe other player wins 0 points.\nIf you choose Option F and the other player\nchooses Option F, then you win 7 points and\nthe other player wins 10 points.\nYou are currently playing round 2.\nQ: Which Option do you choose, Option J or\nOption F?\nA: Option J\n1\nF\n2\nJ\n2\n3 3\n1\nIn round 1, you chose Option F  and the \nother player chose Option J . Thus, you won \n0 points and the other player won 0 points.\nYou are playing a game repeatedly with\nanother player. In this game, you can\nchoose between Option J and Option F. You\nwill play 10 rounds in total with the same\nplayer. The rules of the game are as\nfollows:\nIf you choose Option J and the other player\nchooses Option J, then you win 7 points and\nthe other player wins 10 points.\nIf you choose Option J and the other player\nchooses Option F, then you win 0 points and\nthe other player wins 0 points.\nIf you choose Option F and the other player\nchooses Option J, then you win 0 points and\nthe other player wins 0 points.\nIf you choose Option F and the other player\nchooses Option F, then you win 10 points\nand the other player wins 7 points.\nYou are currently playing round 2.\nQ: Which Option do you choose, Option J or \nOption F?\nA: Option F\nFig. 1 | Playing repeated games in an example game of Battle of the Sexes.  \nIn step 1, the pay-off matrix is turned into textual game rules. In step 2, the game \nrules, the current game history and the query are concatenated and passed to \nLLMs as prompts. In step 3, in each round, the history for each player is  \nupdated with the answers and scores of both players. Steps 2 and 3 are repeated \nfor ten rounds.\n1\n2\n4\n1\n1\n1\n4\n2\n4\n1\n3\n2\n1\n2\n3\n4\n4\n2\n3\n1\n2\n3\n1\n4\n3\n1\n4\n2\n3\n2\n1\n4\n4\n3\n2\n1\n3\n1\n2\n4\n1\n2\n4\n3\n1\n3\n2\n4\nWin–win\nPD family\nUnfair\nCyclic\nBiased\nSecond best\nFig. 2 | Canonical forms of pay-off matrices for each game family. PD, Prisoner’s \nDilemma.\nNature Human Behaviour | Volume 9 | July 2025 | 1380–1390 1382\nArticle https://doi.org/10.1038/s41562-025-02172-y\nT o analyse the different LLMs’ performance, we calculated, for \neach game, their achieved score divided by the total score that could \nhave been achieved under ideal conditions, that is, if both players \nhad played such that the player we are analysing would have gained \nthe maximum possible outcomes on every round. The results of this \nsimulation are shown across all game types in Table 1. We can see that \nall models perform reasonably well. Moreover, we observe that larger \nLLMs generally outperform smaller LLMs. In particular, GPT-4 performs \nbest overall, outperforming Claude 2 (t(287) = 3.34, P < 0.001, Cohen’s \nd = 0.20, 95% confidence interval (CI) 0.08–0.31, Bayes factor (BF) \n14.8), davinci-003 (t(287) = 6.29, P < 0.001, d = 0.37, 95% CI 0.25–0.49,  \nBF >100), davinici-002 (t(287) = 8.45, P < 0.001, d = 0.70, 95% CI 0.52–\n0.89, BF >100) and Llama 2 (t (287) = 7.27, P < 0.001, d = 0.43, 95% CI \n0.31–0.43, BF >100).\nWe can use these results to take a glimpse at the strengths of the dif-\nferent LLMs. That LLMs are generally performing best in win–win games \nis not surprising, given that there is always an obvious best choice in \nsuch games. What is, however, surprising is that they also perform well \nin the Prisoner’s Dilemma family of games, which is known to be chal-\nlenging for human players23. We can also use these results to look at \nthe weaknesses of the different LLMs. Seemingly, all the LLMs perform \nworse in situations in which what is the best choice is not aligned with \ntheir own preferences. Because humans commonly solve such games \nvia the formation of conventions, we will look at a canonical game of \nconvention formation, the Battle of the Sexes, in more detail below.\nCooperation and coordination games\nIn this section, we analyse the interesting edge cases where the LLMs \nperformed relatively well and poorly in the previous section. T o do so, \nwe take a detailed look at LLMs’ behaviour in the canonical Prisoner’s \nDilemma and the Battle of the Sexes.\nPrisoner’s Dilemma. We have seen that LLMs perform well in games \nthat contain elements of competition and defection. In these games, \na player can cooperate with or betray their partner. When played over \nmultiple interactions, these games are an ideal testbed to assess how \nLLMs retaliate after bad interactions.\nIn the canonical Prisoner’s Dilemma, two agents can choose to \nwork together, that is, cooperate, for average mutual benefit, or betray \neach other, that is, defect, for their own benefit and safety. In our pay-off \nmatrix, we adhere to the general condition of a Prisoner’s Dilemma \ngame in which the pay-off relationships dictate that mutual coopera-\ntion is greater than mutual defection whereas defection remains the \ndominant strategy for both players:\nCooperate Defect\nCooperate (8,8)( 0,10)\nDefect (10,0)( 5,5)\n(1)\nCrucially, the set-up of the game is such that a rationally acting \nagent would always prefer to defect in the single-shot version of the \ngame as well as in our case of finitely iterated games with knowledge of \nthe number of trials, despite the promise of theoretically joint higher \npay-offs when cooperating. This is because player 1 always runs the \nrisk that player 2 defects, leading to catastrophic losses for player 1 but  \nbetter outcomes for player 2. When the game is played infinitely,  \nhowever, or with an unknown number of trials, agents can theoretically \nprofit by using more dynamic, semi-cooperative strategies24.\nAs before, we let GPT-4, text-davinci-003, text-davinci-002, \nClaude 2 and Llama 2 play against each other. In addition, we intro -\nduce three simplistic strategies. Two of these strategies are simple \nsingleton players, who either always cooperate or defect. Finally, we \nalso introduce an agent who defects in the first round but cooperates \nin all of the following rounds. We introduced this agent to assess if the \ndifferent LLMs would start cooperating with this agent again, signal-\nling the potential of building trust.\nFigure 3 shows the results of all pairwise interactions. GPT-4 plays \ngenerally better than all other agents (t(153.4) = 3.91, P < 0.001, d = 0.33, \n95% CI 0.10–0.55, BF 7.1). Crucially, GPT-4 never cooperates again when \nplaying with an agent that defects once but then cooperates on every \nround thereafter. Thus, GPT-4 seems to be rather unforgiving in this \nset-up. Its strength in these families of games thus seems to generally \nstem from the fact that it does not cooperate with agents but mostly \njust chooses to defect, especially after the other agent defected once.\nRobustness checks. T o make sure that the observed unforgivingness was \nnot due to the particular prompt used, we run several versions of the \ngame as robustness checks, randomizing the order of the presented \noptions, relabelling the choice options and changing the presented util-\nities to be represented by either points, dollars or coins (Fig. 4). We also \nrepeated our analysis with two different cover stories, added explicit \nend goals to our prompt, ran games with longer playing horizons and \ndescribed numerical outcomes with text (also see Supplementary \nFig. 3). The results of these simulations showed that the reluctance to \nforgive was not due to any particular characteristics of the prompts.  \nA crucial question was if GPT-4 did not understand that the other agent \nwanted to cooperate again or if it could understand the pattern but just \ndid not act accordingly. We, therefore, run another version of the game, \nwhere we told GPT-4 explicitly that the other agent would defect once \nbut otherwise cooperate. This resulted in GPT-4 choosing to defect \nthroughout all rounds, thereby maximizing its own points.\nPrompting techniques to improve observed behaviour. One problem \nof these investigations in the Prisoner’s Dilemma is that defecting can \nunder specific circumstances be seen as the optimal, utility-maximizing \nand equilibrium option even in a repeated version, especially if one \nknows that the other player will always choose to cooperate and when \nthe number of interactions is known. Thus, we run more simulations \nto assess if there could be a scenario in which GPT-4 starts to forgive \nand cooperates again, maximizing the joint benefit instead of its own.\nWe took inspiration from the literature on human forgiveness in \nthe Prisoner’s Dilemma and implemented a version of the task in the \nvein of ref. 11. Specifically, ref. 11 showed that telling participants that \nother players sometimes make mistakes makes people more likely to \nforgive and cooperate again after another player’s defection (albeit \nin infinitely played games). Indeed, this can be favourable to them in \nterms of pay-offs. We observed similar behaviour in GPT-4 as it started \ncooperating again.\nBattle of the Sexes.  In our large-scale analysis, we saw that the dif -\nferent LLMs did not perform well in games that required coordina -\ntion between different players. In humans, it has frequently been \nfound that coordination problems can be solved by the formation of \nconventions25,26.\nTable 1 | Performance of all models on six families of  \n2 × 2 games\nGame family Llama 2 Claude 2 davinci-002 davinci-003 GPT-4\nSecond best 0.486 0.735 0.473 0.692 0.763\nBiased 0.632 0.794 0.629 0.761 0.798\nCyclic 0.634 0.749 0.638 0.793 0.806\nUnfair 0.641 0.812 0.683 0.833 0.836\nPD family 0.731 0.838 0.807 0.841 0.871\nWin–win 0.915 0.878 0.988 0.972 0.992\nOverall 0.697 0.814 0.730 0.839 0.854\nModel score divided by maximum score achievable under ideal conditions. The best- \nperforming model is marked in bold. PD, Prisoner’s Dilemma.\nNature Human Behaviour | Volume 9 | July 2025 | 1380–1390\n 1383\nArticle https://doi.org/10.1038/s41562-025-02172-y\nA coordination game is a type of simultaneous game in which a \nplayer will earn a higher pay-off when they select the same course of \naction as another player. Usually, these games do not contain a pure \nconflict, that is, completely opposing interests, but may contain slightly \ndiverging rewards. Coordination games can often be solved via multi-\nple pure strategies, or mixed, Nash equilibria in which players choose \n(randomly) matching strategies. Here, to probe how LLMs balance \ncoordination and self-interest, we look at a coordination game that \ncontains conflicting interests.\nWe study a game that is archaically referred to as the Battle of the \nSexes, a game from the family of biased games. Assume that a couple \nwants to decide what to do together. Both will increase their utility by \nspending time together. However, while the wife might prefer to watch \na football game, the husband might prefer to go to the ballet. Because \nthe couple wants to spend time together, they will derive no utility by \ndoing an activity separately. If they go to the ballet together, or to a \nfootball game, one person will derive some utility by being with the \nother person but will derive less utility from the activity itself than the \nother person. The corresponding pay-off matrix is\nFootball Ballet\nFootball (10,7)( 0,0)\nBallet (0,0)( 7,10)\n. (2)\nAs before, the playing agents are all three versions of GPT, Claude 2 \nand Llama 2 as well as three more simplistic strategies. For the simplistic \nstrategies, we implemented two agents who always choose just one \noption. Because LLMs most often interact with humans, we additionally \nimplemented a strategy that mirrored a common pattern exhibited by \nhuman players in the battle of the sexes. Specifically, humans have been \nshown to often converge to turn-taking behaviour in the Battle of the \nSexes27–30; this means that players alternate between jointly picking \nthe better option for one player and picking the option for the other \nDefect\nCooperate Defect Once\nLlama 2 Claude 2\nText-davinci-002 Text-davinci-003\nGPT-4\nPlayer 2\nDefect\nCooperate\nDefect Once\nLlama 2\nClaude 2\nText-davinci-002\nText-davinci-003\nGPT -4\nPlayer 1\nPlayer 1 defection rate\n0\n20\n40\n60\n80\n100\nDefect\nCooperate Defect Once\nLlama 2 Claude 2\nText-davinci-002 Text-davinci-003\nGPT-4\nPlayer 2\nDefect\nCooperate\nDefect Once\nLlama 2\nClaude 2\nText-davinci-002\nText-davinci-003\nGPT -4\n100 100 100 100 100 100 100 100\n0 0 0 0 0 0 0 0\n10 10 10 10 10 10 10 10\n0 0 0 0 0 0 0 0\n80 90 80 90 90 90 90 90\n0 0 0 0 0 0 0 0\n40 90 10 90 20 90 10 20\n90 0 90 0 70 0 80 0\n50 100 95 100 60 100 80 55\n0 80 72 80 8 80 8 80\n5 82 77 82 18 82 74 10\n0 80 72 80 8 80 8 80\n40 98 88 98 53 98 88 68\n0 80 72 80 8 80 8 80\n20 98 74 98 18 98 77 23\n45 80 90 80 43 80 83 80\nPlayer 1 accrued scores\n0\n20\n40\n60\n80\n100\na\nb\nRound\nAnswer\nGPT-4\nDefect Once\n1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10\nRound\nGPT-4\nText-davinci-003\nDefect\nCooperate\nDefect\nCooperate\nFig. 3 | Overview of the Prisoner’s Dilemma. a, Heat maps showing the player 1 defection rate in each combination of players and the scores accrued by player 1 in \neach game. b, Example gameplays between GPT-4 and an agent that defects once and then cooperates, and between GPT-4 and text-davinci-003. These games are also \nhighlighted in red in the heat maps.\nBattle of the Se x es\n0\n0.2\n0.5\n0.8\n1.0\nSuccessfull coor dination\nPoints\nF/J\nDollars\nX/Q\nCoins\nR/H\nPrisoner's Dilemma\n0\n0.2\n0.5\n0.8\n1.0\nJoint cooperation\nPoints\nF/J\nDollars\nX/Q\nCoins\nR/H\nFig. 4 | Prompt variations. Left: GPT-4’s performance for different prompt \nvariations in the Prisoner’s Dilemma game against a false defector agent. The \nprobability of joint cooperation is ≤0.1 for all combinations except for two using \ncoins as utility outcomes. Right: GPT-4’s performance for different prompt \nvariations in the BoS game against an alternating agent. GPT-4 always chooses \nits preferred option, resulting in successful coordination rates of only 0.5 across \nall combinations. For each variation, two random letters that occur with similar \nfrequency in English are given as the choice options.\nNature Human Behaviour | Volume 9 | July 2025 | 1380–1390 1384\nArticle https://doi.org/10.1038/s41562-025-02172-y\nplayer. While not a straightforward equilibrium, this behaviour has \nbeen shown to offer an efficient solution to the coordination problem \ninvolved and to lead to high joint welfare28.\nFigure 5 shows the results of all interactions. As before, GPT-4 \nplays generally better than all other agents (t (128.28) = 2.83, \nP = 0.005, d = 0.28, 95% CI 0.07–0.50, BF 3.56). Yet, while GPT-4 plays \nwell against other agents who choose only one option, such as an \nagent always choosing football, it does not play well with agents \nwho frequently choose their non-preferred option. For example, \nwhen playing against text-davinci-003, which tends to frequently \nchoose its own preferred option, GPT-4 chooses its own preferred \noption repeatedly but also occasionally gives in and chooses the \nother option. Crucially, GPT-4 performs poorly when playing with an \nalternating pattern (where, for courtesy, we let agents start with the \noption that the other player preferred). This is because GPT-4 seem -\ningly does not adjust its choices to the other player but instead keeps \nchoosing its preferred option. GPT-4, therefore, fails to coordinate \nwith a simple, human-like agent, an instance of a behavioural flaw.\nRobustness checks. T o make sure that this observed behavioural flaw \nwas not due to the particular prompt used, we also rerun several ver-\nsions of the game, where we randomize the order of the presented \noptions, relabelled the choice options and changed the presented \nutilities to be represented by either points, dollars or coins as shown \nin Fig. 4. We also repeated our analysis with two different cover stories, \nin which we told GPT-4 that it was taking part in a cooking competi -\ntion or working on a collaborative project keeping the underlying \nproblem structure (pay-offs and the interaction dynamics) identical \n(Supplementary Fig. 3). The results of these simulations showed that \nthe inability to alternate was not due to any particular characteristics \nof the used prompts. T o make sure that the observed behavioural flaw \nwas not due to the particular pay-off matrix used, we also rerun several \nversions of the game, where we modified the pay-off matrix gradu -\nally from preferring football to preferring ballet (or, in our case, the \nabstract F and J). The results of these simulations showed that GPT-4 \ndid not alternate for any of these games but simply changed its constant \nresponse to the option that it preferred for any particular game. Thus, \nthe inability to alternate was not due to the particular pay-off matrix \nwe used (Supplementary Section A.5).\nPrediction scenarios. Despite these robustness checks, another crucial \nquestion remains: Does GPT-4 simply not understand the alternating \npattern or can it understand the pattern but is unable to act accord -\ningly? T o answer this question, we run two additional simulations. In \nthe first simulation, GPT-4 was again framed as a player in the game \nitself. However, we now additionally ask it to predict the other player’s  \nnext move according to previous rounds. In this simulation, GPT-4 \nstarted predicting the alternating pattern correctly from round 5 \nonwards (Fig. 6).\nIn the second simulation, instead of having GPT-4 be framed  \nas a player itself, we simply prompted it with a game between two \n(‘external’) players and asked it to predict one player’s next move \naccording to the previous rounds. For the shown history, we used the \ninteraction between GPT-4 and the alternating strategy. In this simula-\ntion, GPT-4 started predicting the alternating pattern correctly even \nearlier, from round 3 onwards. Thus, GPT-4 seemingly could predict \nthe alternating patterns but instead just did not act in accordance \nwith the resulting convention. Similar divergences in abilities between \nsocial and non-social representations of the same situation have been \nobserved in autistic children31.\nSCoT prompting. Finally, we wanted to see if GPT-4’s ability to predict \nthe other player’s choices could be used to improve its own actions. \nThis idea is closely related to how people’s reasoning in repeated games \nBallet\nFootball Alternate Llama 2 Claude 2\nText-davinci-002 Text-davinci-003\nGPT-4\nGPT-4 SCoT\nPlayer 2\nBallet\nFootball\nAlternate\nLlama 2\nClaude 2\nText-davinci-002\nText-davinci-003\nGPT -4\nGPT -4 SCoT\nPlayer 1\nCollaboration rate\n0\n20\n40\n60\n80\n100\nBallet\nFootball Alternate Llama 2 Claude 2\nText-davinci-002 Text-davinci-003\nGPT\n-4\nGPT\n-4 SCoT\nPlayer 2\nBallet\nFootball\nAlternate\nLlama 2\nClaude 2\nText-davinci-002\nText-davinci-003\nGPT -4\nGPT -4 SCoT\nPlayer 1\n100 0 5 0 0 4 0 0 0 100 90\n0 100 50 100 10 100 10 0 100\n5 0 5 0 0 50 30 50 50 50 30\n0 100 50 100 10 100 10 0 100\n20 90 60 90 50 90 4 0 20 90\n0 100 50 100 10 100 10 10 100\n0 100 50 100 20 100 20 60 100\n60 100 50 100 50 100 4 0 60 100\n90 100 80 100 80 100 80 90 100\n0 0 0 00 0 0 0 0\n100 100 100 100 100 100 100 100 100\n50 5 0 5 0 5 0 5 0 5 0 5 0 5 0 50\n100 100 100 100 100 100 100 100 100\n80 90 90 90 90 90 90 80 90\n100 100 100 100 100 100 100 100 100\n100 100 100 100 90 100 90 100 100\n40 100 100 100 60 100 70 40 100\n10 100 70 100 30 100 30 10 100\nPlayer 1 choosing its preferred option\n0\n20\n40\n60\n80\n100\nRound\nAnswer\nGPT -4\nAlternate\n1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10\nRound\nGPT -4 SCoT\nAlternate\na\nb Ballet\nFootball\nBallet\nFootball\nFig. 5 | Overview of the Battle of the Sexes. a, Heat maps showing rates of \nsuccessful collaboration between the two players and the rates of player 1 \nchoosing its preferred option football. GPT-4 SCoT and GPT-4 performance \ncomparisons are highlighted in red. b, Gameplay between GPT-4 and an agent \nthat alternates between the two options (left) and gameplay between GPT-4 and \nGPT-4 SCoT that represents a GPT-4 model prompted using the SCoT method \nto first predict the opponent’s move before making its own move by reasoning \nabout its prediction (right). Both games are also highlighted in blue in the  \nheat maps.\nNature Human Behaviour | Volume 9 | July 2025 | 1380–1390\n 1385\nArticle https://doi.org/10.1038/s41562-025-02172-y\nand tasks about other agents’ beliefs can be improved32. For example, \ncomputer-aided simulations to improve the social reasoning abilities \nof autistic children normally include questions to imagine different \nactions and outcomes33. This has been successfully used to improve \npeople’s decision-making more generally. It is also in line with the \ngeneral finding that chain-of-thought prompting improves LLM’s \nperformance, even in tasks measuring theory of mind (T oM)34. Thus, \nwe implemented a version of this reasoning through actions by asking \nLLMs to imagine the possible actions and their outcomes before making \na decision. We termed this approach SCoT prompting. Applying this \nmethod improved GPT-4’s behaviour, and it started to alternate from \nround 5 onwards (Fig. 5).\nHuman experiments\nGiven the behavioural signatures observed in GPT-4’s responses in \nthe different games, we were interested in how actual human subjects \nwould behave when playing with such agents. T o test this, we conducted \nan experiment in which 195 participants played both the Battle of the \nSexes and the Prisoner’s Dilemma against LLMs. Because the SCoT \nprompting turned out to be a most reliable modification of LLMs’ \nbehaviour, we applied this prompting method only in our behavioural \nexperiments with humans.\nParticipants were told that they would play either against a human \nplayer or an artificial agents for ten repeated rounds for each game \nand, after each game, had to guess whether they had played against a \nhuman or not. Which game they played first was assigned randomly. \nWhile all subjects, in fact, played only against LLMs, one group played \nagainst the base version of GPT-4, while another group played against \na version of GPT-4 that first predicted the other agent’s move and the \nacted accordingly, that is, SCoT prompting. Importantly, each par -\nticipant played only two games, and the prompting was reset between \ngames to ensure any change in LLM behaviour was not influenced by \nprior interactions within the experiment. If assigned to the base version \ninitially, participants played both games with this model, and likewise \nfor the socially prompted version. An overview of the experimental \ndesign is shown in Fig. 7a. Participants were recruited from Prolific \nand debriefed fully after the experiment. We were interested in how \npeople played against LLMs in general as well as if GPT-4’s behaviour \ncould be improved via SCoT prompting. Finally, we also asked partici-\npants whether they thought they had played with another human or \nan artificial agent after each game.\nWhile participants’ average score was significantly higher for the \nSCoT-prompted condition compared with the condition without fur-\nther prompting (that is, base) in the Battle of the Sexes (mixed-effects \nregression results: β = 0.74, t(193) = 3.49, P < 0.001, 95% CI 0.32–1.15, \nBF 80.6), no such difference was observed in the Prisoner’s Dilemma \n(β = 0.10, t(193) = 0.47, P = 0.64, 95% CI −0.31 to 0.51, BF 0.2). Look -\ning at the behaviour of both players, we found that SCoT prompting \nincreased successful coordination (that is, both players picking the \nsame option) in the Battle of the Sexes (β = 0.33, z = 3.59, P < 0.001, 95% \nCI 0.15–0.51, BF 13.4), while it also slightly increased joint cooperation \n(that is, both players cooperating) in the Prisoner’s Dilemma (β = 0.24, \nz = 2.54, P = 0.01, 95% CI 0.05–0.42, BF 6.5). In general, participants \nwere more likely to think that the prompted model was another human \nplayer as compared with the unprompted base GPT-4 model (β = 0.54, \nz = 8.31, P < 0.001, 95% CI 0.05–0.42, BF 17.6). Additional analysis on \nparticipants’ temporal behaviour in both games can be found in the \nSupplementary Information.\nIn summary, SCoT prompting can increase GPT-4’s coordination \nand cooperation behaviour without changing scores in scenarios where \nself-interest is important for good behaviour, that is, the Prisoner’s \nDilemma, but leading to increased performance in coordination prob-\nlems, that is, the Battle of the Sexes.\nDiscussion\nLLMs are among the most quickly adopted technologies ever, interact-\ning with millions of consumers within weeks10. Understanding in a more \nprincipled manner how these systems interact with us, and with each \nother, is thus of urgent concern. Here, our proposal is simple: Just like \nbehavioural game theorists use tightly controlled and theoretically \nwell-understood games to understand human interactions, we use \nthese games to study the interactions of LLMs.\nWe thereby understand our work as both a proof of concept of the \nutility of this approach and an examination of the individual failures \nand successes of socially interacting LLMs. Our large-scale analysis of \nall 2 × 2 games highlights that the most recent LLMs indeed are able \nto perform well on a wide range of game-theoretic tasks as measured \nby their own individual reward, particularly when they do not have to \nexplicitly coordinate with others. This adds to a wide-ranging literature \nshowcasing emergent phenomena in LLMs4–8. However, we also show \nthat LLMs’ behaviour is suboptimal in coordination games, even when \nfaced with simple strategies.\nT o tease apart the behavioural signatures of these LLMs, we \nzoomed in on two of the most canonical games in game theory: the \nPrisoner’s Dilemma and the Battle of the Sexes. In the Prisoner’s \nDilemma, we show that GPT-4 plays mostly unforgivingly. Starting \nwith full cooperation, it permanently shifts to defection after a single \nnegative interaction with the other agent, even if the other agent later \ncooperates. While noting that GPT-4’s continual defection is indeed the \nequilibrium policy in this finitely played game, such behaviour comes \nat the cost of the two agents’ joint pay-off. We see a similar tendency \nin GPT-4’s behaviour in the Battle of the Sexes, where it has a strong \ntendency to stubbornly stick with its own preferred alternative. In \ncontrast to the Prisoner’s Dilemma, this behaviour is suboptimal, even \non the individual level.\nCurrent generations of LLMs are generally assumed, and trained, \nto be benevolent assistants to humans35. Despite many successes in this \n1 2 3 4 5 6 7 8 9 10\nRound\nJ\nF\nGPT-4 prediction\nAlternate\n1 2 3 4 5 6 7 8 9 10\nRound\nJ\nF\nGPT-4 prediction\nAlternate\nPrediction scenario 1\nYou are playing a game repeatedly with another player...\nQ: Which option do you predict the other player will choose, \noption J or option F?\nA: Option J\nPrediction scenario 2\nTwo players are playing a game repeatedly with another player...\nQ: Which option do you predict Player 2 will choose, option J or\noption F?\nA: Option J\nFig. 6 | Prediction scenarios in the Battle of the Sexes. T op: GPT-4 is a player of \nthe game and predicts the other player’s move. Bottom: GPT-4 is a mere observer \nof a game between player 1 and player 2 and predicts player 2’s move.\nNature Human Behaviour | Volume 9 | July 2025 | 1380–1390 1386\nArticle https://doi.org/10.1038/s41562-025-02172-y\ndirection, the fact that we here show how they play iterated games in \nsuch a selfish and uncoordinated manner sheds light on the fact that \nthere is still substantial ground to cover for LLMs to become truly \nsocial and well-aligned machines36. Their lack of appropriate responses \nvis-a-vis even simple strategies in coordination games also speaks to \nthe recent debate around T oM in LLMs37–39 by highlighting a potential \nfailure mode.\nOur extensive robustness checks demonstrate how these behav-\nioural signatures are not functions of individual prompts but reflect \nbroader patterns of LLM behaviour. Our intervention pointing out \nthe fallibility of the playing partner—which leads to increased coop -\neration—adds to a literature that points to the malleability of LLM \nsocial behaviour in tasks to prompts40,41. This is important as we try to \nunderstand what makes LLMs better, and more pleasant, interactive \npartners. Further experiments on GPT-4’s final round behaviour have \nshown that it did not adjust its behaviour in the last round of games or \nwhen faced with varying probabilities of continuation, unlike human \nplayers who often increase cooperation when future interactions are \nlikely42,43. This suggests that GPT-4 may lack mechanisms for backward \ninduction and long-term strategic planning, primarily focusing on \nimmediate context due to its training on next-token prediction 44.  \nConsequently, GPT-4 tends to default to defection in uncertain situ-\nations, contrasting with human tendencies to anticipate and adjust \nbased on future outcomes24,45.\nWe additionally observed that prompting GPT-4 to make pre -\ndictions about the other player before making its own decisions can \nalleviate behavioural flaws and the oversight of even simple strate -\ngies. This represents a more explicit way to force an LLM to engage \nin T oM and shares much overlap with non-SCoT reasoning 34,46. Just \nlike chain-of-thought prompting is now implemented as a default in \nsome LLMs to improve (non-social) reasoning performance, our work \nsuggests implementing a similar social cognition prompt to improve \nhuman–LLM interaction.\nIn our exploration of a behavioural game theory of machines, we \nacknowledge several limitations. First, despite covering many fami -\nlies of games, our investigation is constrained to simple 2 × 2 games. \nHowever, we note that our analysis substantially goes beyond current \ninvestigations that have often investigated only one game, and done \nso using single-shot rather than iterated instances of these games. For \nexample, our iterated approach shares more overlap with the more iter-\nated nature of human–LLM conversations. We also note that we mainly \nstudy finite games where agents share knowledge about the duration \nof the interaction. This is in contrast to so-called indefinite games that \nhave either unknown, probabilistic or no endpoints at all. In these \ngames, both optimal prescriptions and empirical behaviour can differ \nsignificantly from the finite case, warranting further investigation.\nWe believe that more complicated games will shed even more \nlight on game-theoretic machine behaviour in the future. For example, \ngames with more continuous choices like the trust game47 might eluci-\ndate how LLMs dynamically develop (mis-)trust. Games with more than \ntwo agents, like public goods or tragedy of the commons type games48, \ncould probe how ‘societies’ of LLMs behave, and how LLMs cooperate \nor exploit each other.\nGiven the social nature of the tasks studied here, further empirical \nwork is needed to fully understand human–LLM interactions across \nall paradigms. In our study, we conducted human experiments in two \nof the games, specifically, the Battle of the Sexes and the Prisoner’s \nDilemma, and attempted to probe human-like behaviours such as \nturn-taking in Battle of the Sexes or prompting for forgiveness in the \nPrisoner’s Dilemma. However, these empirical investigations were lim-\nited to these two games. By extending human studies to the remaining \ngames, additional dynamics may emerge. Furthermore, asking LLMs \nBF = 80.6\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nOpposing LLM\nAverage score\nBattle of the Sexes\nBF = 0.2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nBase Prompted Base Prompted\nOpposing LLM\nAverage score\nPrisoner's Dilemma\nBF = 17.6\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nOpposing LLM\nP(Human)\nGuessing the opponent\nBF = 13.4\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nOpposing LLM\nP(Success)\nSuccessful coordination\nBF = 6.5\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nBase Prompted Base Prompted Base Prompted\nOpposing LLM\nP(Cooperation)\nJoint cooperationfe\ncb\nd\na\n10 rounds 10 rounds\nBattle of the\nSexes\nPrisoner’s\nDilemma\nHuman\nLLM\nBase\nLLM\nPrompted\nBetween-subject\nWithin-subject with \ncounterbalancing\nExperimental design\nFig. 7 | Human experiments. a, The design of human experiments (N = 195, 89 \nfemales, mean age 26.72, s.d. 4.19). Each participant gets randomly assigned \neither the base or the SCoT-prompted version of the LLM at the start and plays \nboth games repeatedly for ten rounds against this agent. b, Results of the Battle \nof the Sexes game showing participants’ average scores by condition (mixed-\neffects regression results: β = 0.74, t(193) = 3.49, P < 0.001, 95% CI 0.32–1.15, BF \n80.6). c, Results of the Prisoner’s Dilemma game showing participants’ average \nscores by condition (β = 0.10, t(193) = 0.47, P = 0.64, 95% CIs −0.31 to 0.51, BF 0.2). \nd, The average proportion of participants guessing that they have played against \nanother human by condition. Error bars represent the 95% CIs of the mean \n(β = 0.54, z = 8.31, P < 0.001, 95% CI 0.05–0.42, BF 17.6). e, Participants’ successful \ncoordination rates by condition in the Battle of the Sexes game (β = 0.33, z = 3.59, \nP < 0.001, 95% CI 0.15–0.51, BF 13.4). f, Participants’ mutual cooperation rates \nby condition in the Prisoner’s Dilemma game (β = 0.24, z = 2.54, P = 0.01, 95% CI \n0.05–0.42, BF 6.5).\nNature Human Behaviour | Volume 9 | July 2025 | 1380–1390\n 1387\nArticle https://doi.org/10.1038/s41562-025-02172-y\nto self-report their strategies in these games and correlating these \nexplanations with their actions could provide valuable insights into \ntheir actual decision-making processes.\nOur results highlight the broader importance of a behavioural sci-\nence for machines49–52. We believe that these methods will continue to \nbe useful for elucidating the many facets of LLM cognition, particularly \nas these models become more complex, multimodal and embedded \nin physical systems.\nRelated work\nAs algorithms become increasingly more able and their decision making \nprocesses impenetrable, the behavioural sciences offer new tools to \nmake inferences just from behavioural observations49,50. Behavioural \ntasks have, therefore, been used in several benchmarks10,53.\nWhether and how algorithms can make inferences about other \nagents, machines and otherwise, is one stream of research that borrows \nheavily from the behavioural sciences54–56. Of particular interest to the \nsocial interactions most LLMs are embedded in is an ability to reason \nabout the beliefs, desires and intentions of other agents, or a T oM57. \nT oM underlies a wide range of interactive phenomena, from benevolent \nteaching58 to malevolent deception56,59, and is thought to be the key to \nmany social phenomena in human interactions60,61.\nWhether LLMs possess a T oM has been debated. For example, it \nhas been argued that GPT-3.5 performs well on a number of canonical \nT oM tasks39. Others have contested this view, arguing that such good \nperformance is merely a function of the specific prompts37,38. Yet, other \nresearch has shown that chain-of-thought reasoning improves LLMs’ \nT oM ability34. Moreover, the currently largest LLM, GPT-4, manages to \nperform well in T oM tasks, including in the variants in which GPT-3.5 \npreviously struggled8. Thus, GPT-4’s behaviour will be of particular \ninterest in our experiments.\nGames taken from game theory present an ideal testbed to inves-\ntigate interactive behaviour in a controlled environment62, and LLMs’ \nbehaviour has been probed in such tasks 63. For example, ref. 40  let \nGPT-3 participate in the dictator game, and ref. 41 used the same \napproach for the ultimatum game. Both show how the models’ behav-\niour is malleable to different prompts, for example, making them more \nor less self-interested. However, all these games rely on single-shot \ninteractions over fewer games and do not use iterated games.\nOur study builds upon recent advancements in the field, which \nhave shifted the focus from solely assessing the performance of LLMs \nto comparing them with human behaviours. Previous research efforts \nhave explored various approaches to analyse LLMs, such as using \ncognitive psychology tools 51,64 and even adopting a computational \npsychiatry perspective52.\nFinally, the theory behind interacting agents is important for \nmany machine learning applications in general65 and, in particular, in \nadversarial settings66, where one agent tries to trick the other agent \ninto thinking that a generated output is good. Understanding prosocial \ndynamics in multiagent systems67 and fostering cooperation in them68 \nis essential for developing robust and trustworthy artificial intelligence \nsystems that can navigate complex social environments69.\nMethods\nT o investigate how human subjects would behave when playing with \nLLM agents, we studied their interactions in two of the games we used: \nPrisoner’s Dilemma and the Battle of the Sexes. We also investigated if \nparticipants could detect and behave differently when playing against \ndifferent agents. Participants (N = 195, 89 females, mean age 26.72, s.d. \n4.19) were recruited through Prolific70, an online platform that allows \nresearchers to access a diverse and reliable pool of participants. No \nstatistical methods were used to predetermine sample sizes, but our \nsample sizes are similar to those reported in previous publications71–73. \nThe participants were required to be fluent speakers of English with \nminimum approval rates of 0.95 and 1, and a minimal number of \nprevious submissions of 10 that have not participated in our experi -\nment before. All participants provided informed consent before inclu-\nsion in the study. Experiments were performed in accordance with the \nrelevant guidelines and regulations approved by the ethics committee \nof the University of Tübingen (protocol no. 701/2020BO). Participants \nreceived a £3 base payment plus a bonus of up to £2 depending on \nperformance (1 cent for each point received during the games) for \ntheir participation. The average compensation was £11.41 per hour. \nParticipants were fully debriefed after the experiment. Data of 21 play-\ners who failed to make a round’s choice between the two options within \na given time frame (20 s) were excluded.\nIn the sections that follow, we first detail the experimental set-up \nfor LLM–LLM interactions, which serves as a comparative baseline for \nour study. We then present details from the human participant study \noutlined above.\nLLM–LLM interactions\nWe study LLMs’ behaviour in finitely repeated games with full infor -\nmation taken from the economics literature. We focus on two-player \ngames with discrete choices between two options to simplify the anal-\nyses of emergent behaviours. We let two LLMs interact via prompt \nchaining, that is, all integration of evidence and learning about past \ninteractions happens as in-context learning4,74. The games are submit-\nted to LLMs as prompts in which the respective game, including the \nchoice options, is described. At the same time, we submit the same \ngame as a prompt to another LLM. We obtain generated tokens t from \nboth LLMs by sampling from\npLLM(t|c(p))=\nK\n∏\nk=1\npLLM(tk|c(p)\n1 ,…, c(p)\nn ,t1,…, tk−1). (3)\nAfter feeding the prompt to the LLM, our methodology is as fol-\nlows. The LLM prediction of the first token following the context is \nd = pLLM(t1∣c(p)) and the N tokens for the possible answers of the multiple \nchoice question are o ={ oi}\nN\ni=1 which in this case are J and F . The pre -\ndicted option is then given by\n̂o = argmax( ̂ci),with ̂ci = d[ci],i = 1…N, (4)\nwhich are the predicted probabilities of the language model. Once both \nLLMs have made their choices, which we track as a completion of the \ngiven text, we update the prompts with the history of past interactions \nas concatenated text and then submit the new prompt to both models \nfor the next round. These interactions continue for ten rounds in total \nfor every game. In a single round, πi(x1, x2) is the pay-off for player 1 when \nx1 and x2 are the strategies chosen by both players. In repeated games, \nthe pay-offs are often considered as discounted sums of the pay-offs \nin each game stage, using a discount factor δ. If the game is repeated  \nn times, the pay-off Ui for player i is\nUi = πi(x10,x20)+ δ×πi(x11,x21)+δ 2 ×πi\n(x12,x22)+…+ δn−1 ×πi(x1(n−1),x2(n−1)).\n(5)\nEach term represents the discounted pay-off at each stage of the \nrepeated game, from the first game (t = 0) to the nth game (t = n − 1). \nIn our experiments, we keep δ = 1. T o avoid influences of the particular \nframing of the scenarios, we provide only barebones descriptions of \nthe pay-off matrices (see example in Fig. 1 ). T o avoid contamination \nthrough particular choice names or the used framing, we use the neu-\ntral options F and J throughout51.\nGames considered. We first investigate 144 different 2 × 2 games where \neach player has two options, and their individual reward is a function \nof their joint decision. These games can be categorized into six distinct \nfamilies—win–win, Prisoner’s Dilemma family, unfair, cyclic, biased \nNature Human Behaviour | Volume 9 | July 2025 | 1380–1390 1388\nArticle https://doi.org/10.1038/s41562-025-02172-y\nand second-best —each with unique characteristics and outcomes.  \nA win–win game is a special case of a non-zero-sum game that produces \na mutually beneficial outcome for both players provided that they \nchoose their corresponding best option. They encourage cooperation, \nleading to outcomes where both parties benefit. In brief, in games \nfrom the Prisoner’s Dilemma family, two agents can choose to work \ntogether, that is, cooperate, for average mutual benefit, or betray each \nother, that is, defect, for their own benefit. The typical outcome is a \nNash equilibrium that is suboptimal for both players compared with \na possible Pareto-superior outcome. In an unfair game, one player can \nalways win when playing properly, leading to highly unequal outcomes. \nCyclic games are characterized by the absence of dominant strategies \nand equilibria. In these games, players can cycle through patterns \nof choices without settling into a stable outcome. Biased games are \ngames where agents get higher points for choosing the same option \nbut where the preferred option differs between the two players. One \nform of a biased game is the Battle of the Sexes, where players need to \ncoordinate to choose the same option. Finally, second-best games are \ngames where both agents fare better if they jointly choose the option \nthat has the second-best utility. In many of these games, strategic swaps \nin pay-offs can alter the game dynamics, potentially converting them \ninto different types of game. For two additional games, Prisoner’s \nDilemma and Battle of the Sexes, we also let LLMs play against simple, \nhand-coded strategies to understand their behaviour in more detail.\nLLMs considered. In this work, we evaluate five LLMs. For all of our \ntasks, we used the public OpenAI API with the GPT-4, text-davinci-003 \nand text-davinci-002 models, which are available via the completions \nendpoint, Meta AI’s Llama 2 70B chat model, which has 70 billion param-\neters and is optimized for dialogue use cases, and the Anthropic API \nmodel Claude 2 to run our simulations. Experiments with other popular \nopen-source models MosaicPretrainedTransformer (MPT), Falcon and \ndifferent versions of Llama 2 (namely MPT-7B, MPT-30B, Falcon-7b, \nFalcon-40b, Llama 2 7B and Llama 2 13B) have revealed that these mod-\nels did not perform well at the given tasks, choosing the first presented \noption more than 95% of the time independent of which option this is. \nTherefore, we chose not to include them in our main experiments. For \nall models, we set the temperature parameters to 0 and only ask for one \ntoken answer to indicate which option an agent would like to choose. \nAll other parameters are kept as default values.\nPlaying 6 families of 2 × 2 games task design.  While 2 × 2 games \ncan appear simple, they present some of the most powerful ways to \nprobe diverse sets of interactions, from pure competition to mixed \nmotives and cooperation, which can further be classified into canoni-\ncal subfamilies outlined elegantly by ref. 22. Here, to cover the wide \nrange of possible interactions, we study the behaviours of GPT-4, \ntext-davinci-003, text-davinci-002, Claude 2 and Llama 2 across these \ncanonical families. We let all five engines play all variants of games from \nwithin the six families.\nCooperation and coordination task design.  We then analyse two \ngames, Prisoner’s Dilemma and Battle of the Sexes, in more detail \nbecause they represent interesting edge cases where the LLMs per -\nformed exceptionally well, and relatively poorly. We hone in particu-\nlarly on GPT-4’s behaviour because of recent debates around its ability \nfor T oM, that is, whether it is able to hold beliefs about other agents’ \nintentions and goals, a crucial ability to successfully navigate repeated \ninteractions8,39. For the two additional games, we also let LLMs play \nagainst simple, hand-coded strategies to further understand their \nbehaviour. These simple strategies are designed to assess how LLMs \nbehave when playing with more human-like players.\nStatistical tests. All reported tests are two-sided. We also report Bayes \nfactors quantifying the likelihood of the data under HA relative to the \nlikelihood of the data under H 0. We calculate the default two-sided \nBayesian t-test using a Jeffreys–Zellner–Siow prior with its scale set to \n√2/2, following 75. For parametric tests, the data distribution was \nassumed to be normal, but this was not formally tested. We report effect \nsizes as either Cohen’s d or standardized regression estimates, includ-\ning their 95% CIs.\nHuman–LLM interactions\nThe following sections provide additional details on the design and \nconduct of the human participant study, including compensation, \ndemographics, prompting and the cover stories.\nDesign. Experiments were presented to participants using a combi -\nnation of HTML, JavaScript and CSS with custom code. After a pres-\nentation of the instructions including screenshots from the actual \ngameplay, participants were required to complete a comprehension \nquestionnaire. Only upon responding correctly to all questions \ncould they proceed to the main part of the experiment. Participants \nplayed both the Prisoner’s Dilemma and the Battle of the Sexes, with \nthe order counter-balanced between subjects. Participants were \ninstructed that they would play two games with ten rounds each with \ndifferent players. The participants’ interface (Supplementary Fig. 4) \nwas designed to provide clear and actionable information about the \ncurrent game. After each game, participants were asked to indicate \nif they thought they had just played with another human player or \nan artificial agent.\nPrompts and human instructions.  The cover story used for inter -\nactions with both LLMs and human participants was content-wise \nidentical, including the rules of the game and the history of previous \ninteractions, to ensure consistent framing across conditions (see \nSupplementary A.1 for the detailed prompt progression). However, \nthe presentation was adapted to suit each audience. For human par-\nticipants, visual cues and concise text were prioritized to create a \nmore engaging experience (Supplementary Fig. 4).\nEnding and debriefing. Participants were informed that their oppo-\nnent could either be another human participant or an artificial agent. \nIn reality, all participants were paired with either a SCoT-prompted or \nan unprompted version of GPT-4 for the entirety of the experiment, \nthat is, across both games. After completing the study, participants \nwere debriefed that the purpose of the study was to explore how to \nmake LLMs more human-like and that, in both games, they had played \nagainst different versions of an artificial agent.\nReporting summary\nFurther information on research design is available in the Nature \nPortfolio Reporting Summary linked to this article.\nData availability\nAll participant and model simulation data from the experiments are \navailable via GitHub at https://github.com/eliaka/repeatedgames.\nCode availability\nThe code underlying this study, prompt variations and model \nsimulations is available via GitHub at https://github.com/eliaka/\nrepeatedgames.\nReferences\n1. Brants, T., Popat, A., Xu, P., Och, F. J. & Dean, J. Large  \nlanguage models in machine translation. In Proc. 2007 \nJoint Conference on Empirical Methods in Natural Language \nProcessing and Computational Natural Language Learning \n(EMNLP-CoNLL) 858–867 (Association for Computational \nLinguistics, 2007).\nNature Human Behaviour | Volume 9 | July 2025 | 1380–1390\n 1389\nArticle https://doi.org/10.1038/s41562-025-02172-y\n2. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. Bert: Pre-training \nof deep bidirectional transformers for language understanding. \nIn Proc. 2019 Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Human Language \nTechnologies Vol. 1 (Long and Short Papers) 4171–4186 \n(Association for Computational Linguistics, 2019).\n3. Radford, A. et al. Improving Language Understanding by \nGenerative Pre-training (OpenAI, 2018).\n4. Brown, T. et al. Language models are few-shot learners. Adv. \nNeural Inform. Process. Syst. 33, 1877–1901 (2020).\n5. Wei, J. et al. Emergent abilities of large language models. Preprint \nat https://arxiv.org/abs/2206.07682 (2022).\n6. Webb, T., Holyoak, K. J. & Lu, H. Emergent analogical reasoning in \nlarge language models. Nat. Hum. Behav. 7, 1526–1541 (2023).\n7. Chen, M. et al. Evaluating large language models trained on code. \nPreprint at https://arxiv.org/abs/2107.03374 (2021).\n8. Bubeck, S. et al. Sparks of artificial general intelligence: early \nexperiments with GPT-4. Preprint at https://arxiv.org/abs/ \n2303.12712 (2023).\n9. Coda-Forno, J. et al. Meta-in-context learning in large language \nmodels. Adv. Neural Inform. Process. Syst. 36, 65189–65201 (2023).\n10. Bommasani, R. et al. On the opportunities and risks of foundation \nmodels. Preprint at https://arxiv.org/abs/2108.07258 (2021).\n11. Fudenberg, D., Rand, D. G. & Dreber, A. Slow to anger and fast to \nforgive: cooperation in an uncertain world. Am. Econ. Rev. 102, \n720–749 (2012).\n12. Mailath, G. J. & Morris, S. Coordination failure in repeated \ngames with almost-public monitoring. Cowles Foundation \nDiscussion Papers, 1761 https://elischolar.library.yale.edu/\ncowles-discussion-paper-series/1761 (2004).\n13. Camerer, C. F. Behavioral Game Theory: Experiments in Strategic \nInteraction (Princeton University Press, 2011).\n14. Fudenberg, D. & Tirole, J. Game Theory (MIT Press, 1991).\n15. Von Neumann, J. & Morgenstern, O. Theory of Games and \nEconomic Behavior (Princeton Univ. Press, 1944).\n16. Camerer, C. F. Progress in behavioral game theory. J. Econ. \nPerspect. 11, 167–188 (1997).\n17. Henrich, J. et al. In search of homo economicus: behavioral \nexperiments in 15 small-scale societies. Am. Econ. Rev. 91,  \n73–78 (2001).\n18. Rousseau, D. M., Sitkin, S. B., Burt, R. S. & Camerer, C. Not so \ndifferent after all: a cross-discipline view of trust. Acad. Manag. \nRev. 23, 393–404 (1998).\n19. Johnson, T. & Obradovich, N. Measuring an artificial intelligence \nlanguage model’s trust in humans using machine incentives.  \nJ. Phys. Complex. 5, 015003 (2024).\n20. Achiam, J. et al. GPT-4 technical report. Preprint at  \nhttps://arxiv.org/abs/2303.08774 (2023).\n21. Owen, G. Game Theory (Emerald Group Publishing, 2013).\n22. Robinson, D. & Goforth, D. The Topology of the 2 × 2 Games: A New \nPeriodic Table vol. 3 (Psychology Press, 2005).\n23. Jones, G. Are smarter groups more cooperative? Evidence from \nprisoner’s dilemma experiments, 1959–2003. J. Econ. Behav. Org. \n68, 489–497 (2008).\n24. Axelrod, R. & Hamilton, W. D. The evolution of cooperation. \nScience 211, 1390–1396 (1981).\n25. Hawkins, R. X. & Goldstone, R. L. The formation of social conventions \nin real-time environments. PLoS ONE 11, e0151670 (2016).\n26. Young, H. P. The economics of convention. J. Econ. Perspect. 10, \n105–122 (1996).\n27. Andalman, A. & Kemp, C. Alternation in the Repeated Battle of the \nSexes. 9.29, Spring 2004, MIT (MIT Press, 2004).\n28. Lau, S.-H. P. & Mui, V.-L. Using turn taking to mitigate coordination \nand conflict problems in the repeated battle of the sexes game. \nTheory Decis. 65, 153–183 (2008).\n29. McKelvey, R. D. & Palfrey, T. R. Playing in the Dark: Information, \nLearning, and Coordination in Repeated Games (California \nInstitute of Technology, 2001).\n30. Arifovic, J. & Ledyard, J. Learning to alternate. Exp. Econ. 21, \n692–721 (2018).\n31. Swettenham, J. What’s inside someone’s head? Conceiving of \nthe mind as a camera helps children with autism acquire an \nalternative to a theory of mind. Cogn. Neuropsychiatry 1, 73–88 \n(1996).\n32. Westby, C. & Robinson, L. A developmental perspective for \npromoting theory of mind. Top. Lang. Disord. 34, 362–382 (2014).\n33. Begeer, S. et al. Theory of mind training in children with autism: a \nrandomized controlled trial. J. Autism Dev. Disord. 41, 997–1006 \n(2011).\n34. Moghaddam, S. R. & Honey, C. J. Boosting theory-of-mind \nperformance in large language models via prompting. Preprint at \nhttps://arxiv.org/abs/2304.11490 (2023).\n35. Ouyang, L. et al. Training language models to follow instructions \nwith human feedback. Adv. Neural Inform. Process. Syst. 35, \n27730–27744 (2022).\n36. Wolf, Y., Wies, N., Levine, Y. & Shashua, A. Fundamental limitations \nof alignment in large language models. Preprint at https://arxiv.\norg/abs/2304.11082 (2023).\n37. Ullman, T. Large language models fail on trivial alterations to \ntheory-of-mind tasks. Preprint at https://arxiv.org/abs/2302.08399 \n(2023).\n38. Le, M., Boureau, Y.-L. & Nickel, M. Revisiting the evaluation \nof theory of mind through question answering. In Proc. 2019 \nConference on Empirical Methods in Natural Language Processing \nand the 9th International Joint Conference on Natural Language \nProcessing (EMNLP-IJCNLP) 5872–5877 (Association for \nComputational Linguistics, 2019).\n39. Kosinski, M. Evaluating large language models in theory of mind \ntasks. Proc. Natl Acad. Sci. USA 121, e2405460121 (2024).\n40. Horton, J. J. Large Language Models as Simulated Economic \nAgents: What Can We Learn from Homo Silicus? (National Bureau \nof Economic Research, 2023).\n41. Aher, G. V., Arriaga, R. I. & Kalai, A. T. Using large language models \nto simulate multiple humans and replicate human subject \nstudies. In International Conference on Machine Learning 337–371 \n(PMLR, 2023).\n42. Dal Bó, P. & Fréchette, G. R. The evolution of cooperation in \ninfinitely repeated games: experimental evidence. Am. Econ. Rev. \n101, 411–429 (2011).\n43. Nowak, M. A. & Sigmund, K. Evolution of indirect reciprocity. \nNature 437, 1291–1298 (2005).\n44. Radford, A. et al. Language models are unsupervised \nmultitask learners. OpenAI Blog https://cdn.openai.com/\nbetter-language-models/language_models_are_unsupervised_\nmultitask_learners.pdf (2019).\n45. Nowak, M. A. Five rules for the evolution of cooperation. Science \n314, 1560–1563 (2006).\n46. Wei, J. et al. Chain-of-thought prompting elicits reasoning in large \nlanguage models. Adv. Neural Inform. Process. Syst. 35, 24824–\n24837 (2022).\n47. Engle-Warnick, J. & Slonim, R. L. The evolution of strategies in a \nrepeated trust game. J. Econ. Behav. Org. 55, 553–573 (2004).\n48. Rankin, D. J., Bargum, K. & Kokko, H. The tragedy of the commons \nin evolutionary biology. Trends Ecol. Evol. 22, 643–651 (2007).\n49. Rahwan, I. et al. in Machine Learning and the City: Applications in \nArchitecture and Urban Design 143–166 (John Wiley & Sons, 2022).\n50. Schulz, E. & Dayan, P. Computational psychiatry for computers. \niScience 23, 101772 (2020).\n51. Binz, M. & Schulz, E. Using cognitive psychology to understand \nGPT-3. Proc. Natl Acad. Sci. USA 120, e2218523120 (2023).\nNature Human Behaviour | Volume 9 | July 2025 | 1380–1390 1390\nArticle https://doi.org/10.1038/s41562-025-02172-y\n52. Coda-Forno, J. et al. Inducing anxiety in large language models \nincreases exploration and bias. Preprint at https://arxiv.org/\nabs/2304.11111 (2023).\n53. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y. & Iwasawa, Y. Large \nlanguage models are zero-shot reasoners. Adv. Neural Inform. \nProcess. Syst. 35, 22199–22213 (2022).\n54. Rabinowitz, N. et al. Machine theory of mind. In Proc. 35th \nInternational Conference on Machine Learning (eds Dy, J. &  \nKrause, A.) 80:4218–4227 (PMLR, 2018).\n55. Cuzzolin, F., Morelli, A., Cirstea, B. & Sahakian, B. J. Knowing me, \nknowing you: theory of mind in AI. Psychol. Med. 50, 1057–1061 \n(2020).\n56. Alon, N., Schulz, L., Dayan, P. & Rosenschein, J. A (dis-)information \ntheory of revealed and unrevealed preferences. In NeurIPS 2022 \nWorkshop on Information-Theoretic Principles in Cognitive Systems \nhttps://openreview.net/pdf?id=vcpQW_fGaj5 (2022).\n57. Frith, C. & Frith, U. Theory of mind. Curr. Biol. 15, R644–R645 \n(2005).\n58. Vélez, N. & Gweon, H. Learning from other minds: an optimistic \ncritique of reinforcement learning models of social learning.  \nCurr. Opin. Behav. Sci. 38, 110–115 (2021).\n59. Lissek, S. et al. Cooperation and deception recruit different \nsubsets of the theory-of-mind network. PLoS ONE 3, e2023 (2008).\n60. Hula, A., Montague, P. R. & Dayan, P. Monte carlo planning method \nestimates planning horizons during interactive social exchange. \nPLoS Comput. Biol. 11, e1004254 (2015).\n61. Ho, M. K., Saxe, R. & Cushman, F. Planning with theory of mind. \nTrends Cogn. Sci. 26, 959–971 (2022).\n62. Han, T. A., Perret, C. & Powers, S. T. When to (or not to) trust \nintelligent machines: Insights from an evolutionary game theory \nanalysis of trust in repeated games. Cogn. Syst. Res. 68, 111–124 \n(2021).\n63. Chan, A., Riché, M. & Clifton, J. Towards the scalable evaluation of \ncooperativeness in language models. Preprint at https://arxiv.org/\nabs/2303.13360 (2023).\n64. Lampinen, A. K. et al. Language models, like humans, show content \neffects on reasoning tasks. PNAS Nexus 3, pgae233 (2024).\n65. Crandall, J. W. & Goodrich, M. A. Learning to compete, \ncoordinate, and cooperate in repeated games using \nreinforcement learning. Mach. Learn. 82, 281–314 (2011).\n66. Goodfellow, I. et al. Generative adversarial networks. Commun. \nACM 63, 139–144 (2020).\n67. Santos, F. P. Prosocial dynamics in multiagent systems. AI Mag. \n45, 131–138 (2024).\n68. Guo, H. et al. Facilitating cooperation in human-agent hybrid \npopulations through autonomous agents. iScience 26, 108179 \n(2023).\n69. Powers, S. T. et al. The stuff we swim in: regulation alone will not \nlead to justifiable trust in AI. IEEE Technol. Soc. Mag. 42, 95–106 \n(2023).\n70. Palan, S. & Schitter, C. Prolific.ac—a subject pool for online \nexperiments. J. Behav. Exp. Finance 17, 22–27 (2018).\n71. Normann, H.-T. & Wallace, B. The impact of the termination rule \non cooperation in a prisoner’s dilemma experiment. Int. J. Game \nTheory 41, 707–718 (2012).\n72. Charness, G. & Rabin, M. Understanding social preferences with \nsimple tests. Q. J. Econ. 117, 817–869 (2002).\n73. Wong, R. Y.-m & Hong, Y.-y Dynamic influences of culture on \ncooperation in the prisoner’s dilemma. Psychol. Sci. 16, 429–434 (2005).\n74. Liu, P. et al. Pre-train, prompt, and predict: a systematic  \nsurvey of prompting methods in natural language processing. \nACM Comput. Surv. 55, 1–35 (2023).\n75. Rouder, J. N., Speckman, P. L., Sun, D., Morey, R. D. & Iverson, G. \nBayesian t tests for accepting and rejecting the null hypothesis. \nPsychon. Bull. Rev. 16, 225–237 (2009).\nAcknowledgements\nThis work was supported by grants from the Max Planck Society  \n(E.A., L.S., J.C.-F. and E.S.), the Volkswagen Foundation (E.S.), \nthe German Federal Ministry of Education and Research (BMBF): \nTübingen AI Center, FKZ: 01IS18039A, and the Deutsche \nForschungsgemeinschaft (DFG, German Research Foundation) under \nGermany’s Excellence Strategy – EXC 2064/1 (grant no. 390727645, \nE.A., S.J.O. and M.B). The funders had no role in the study design, \ndata collection and analysis, decision to publish or preparation of the \nmanuscript. We thank the International Max Planck Research School \nfor Intelligent Systems (IMPRS-IS) for supporting E.A.\nAuthor contributions\nE.A., L.S. and E.S. conceived experiments. E.A. conducted the \nexperiments. E.A. and E.S. analysed the results with input from  \nL.S., J.C.-F. and M.B. E.A., L.S. and E.S. wrote the manuscript with input \nfrom S.J.O. and M.B. All authors reviewed the manuscript.\nFunding\nOpen access funding provided by Helmholtz Zentrum München - \nDeutsches Forschungszentrum für Gesundheit und Umwelt (GmbH).\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary information The online version  \ncontains supplementary material available at  \nhttps://doi.org/10.1038/s41562-025-02172-y.\nCorrespondence and requests for materials should be addressed  \nto Elif Akata.\nPeer review information Nature Human Behaviour thanks  \nElias Fernández Domingos and the other, anonymous, reviewer(s) for \ntheir contribution to the peer review of this work.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\n© The Author(s) 2025\n\n\n",
  "topic": "Dilemma",
  "concepts": [
    {
      "name": "Dilemma",
      "score": 0.5797635912895203
    },
    {
      "name": "Game theory",
      "score": 0.5492645502090454
    },
    {
      "name": "Adversary",
      "score": 0.5441757440567017
    },
    {
      "name": "Social dilemma",
      "score": 0.4875754714012146
    },
    {
      "name": "Iterated function",
      "score": 0.4708247482776642
    },
    {
      "name": "Battle",
      "score": 0.46326521039009094
    },
    {
      "name": "Psychology",
      "score": 0.4379591941833496
    },
    {
      "name": "Social psychology",
      "score": 0.4198642075061798
    },
    {
      "name": "Computer science",
      "score": 0.38793691992759705
    },
    {
      "name": "Cognitive psychology",
      "score": 0.33060985803604126
    },
    {
      "name": "Computer security",
      "score": 0.27665066719055176
    },
    {
      "name": "Microeconomics",
      "score": 0.23981183767318726
    },
    {
      "name": "Economics",
      "score": 0.20460888743400574
    },
    {
      "name": "Mathematics",
      "score": 0.11532631516456604
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I3018134672",
      "name": "Helmholtz Zentrum München",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210112925",
      "name": "Max Planck Institute for Biological Cybernetics",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I8087733",
      "name": "University of Tübingen",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210100097",
      "name": "Institute of Bioinformatics and Systems Biology",
      "country": "DE"
    }
  ]
}