{
  "title": "Cross-View Gait Recognition Using Pairwise Spatial Transformer Networks",
  "url": "https://openalex.org/W3008732063",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2112140162",
      "name": "Chi Xu",
      "affiliations": [
        "Osaka University",
        "Osaka Research Institute of Industrial Science and Technology",
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A272119250",
      "name": "Yasushi Makihara",
      "affiliations": [
        "Osaka Research Institute of Industrial Science and Technology",
        "Osaka University"
      ]
    },
    {
      "id": "https://openalex.org/A2068388382",
      "name": "Xiang Li",
      "affiliations": [
        "Osaka University",
        "Osaka Research Institute of Industrial Science and Technology",
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2094158490",
      "name": "Yasushi Yagi",
      "affiliations": [
        "Osaka University",
        "Osaka Research Institute of Industrial Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2107596476",
      "name": "Jianfeng Lu",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2112140162",
      "name": "Chi Xu",
      "affiliations": [
        "Osaka University",
        "Nanjing University of Science and Technology",
        "Osaka Research Institute of Industrial Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A272119250",
      "name": "Yasushi Makihara",
      "affiliations": [
        "Nanjing University of Science and Technology",
        "Osaka Research Institute of Industrial Science and Technology",
        "Osaka University"
      ]
    },
    {
      "id": "https://openalex.org/A2068388382",
      "name": "Xiang Li",
      "affiliations": [
        "Nanjing University of Science and Technology",
        "Osaka University",
        "Osaka Research Institute of Industrial Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2094158490",
      "name": "Yasushi Yagi",
      "affiliations": [
        "Osaka Research Institute of Industrial Science and Technology",
        "Osaka University",
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2107596476",
      "name": "Jianfeng Lu",
      "affiliations": [
        "Osaka Research Institute of Industrial Science and Technology",
        "Nanjing University of Science and Technology",
        "Osaka University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2904739449",
    "https://openalex.org/W2148568366",
    "https://openalex.org/W2018331988",
    "https://openalex.org/W2788751553",
    "https://openalex.org/W2878579911",
    "https://openalex.org/W2781155895",
    "https://openalex.org/W2120526922",
    "https://openalex.org/W6680709798",
    "https://openalex.org/W2154171558",
    "https://openalex.org/W6675575696",
    "https://openalex.org/W2016053056",
    "https://openalex.org/W2138621090",
    "https://openalex.org/W6674330103",
    "https://openalex.org/W1975517671",
    "https://openalex.org/W2798896071",
    "https://openalex.org/W2151239861",
    "https://openalex.org/W6618372016",
    "https://openalex.org/W2071207147",
    "https://openalex.org/W2608706973",
    "https://openalex.org/W6631943919",
    "https://openalex.org/W6677106874",
    "https://openalex.org/W2085058513",
    "https://openalex.org/W1896402587",
    "https://openalex.org/W2407362091",
    "https://openalex.org/W2949024437",
    "https://openalex.org/W2963301258",
    "https://openalex.org/W2964014798",
    "https://openalex.org/W2963854019",
    "https://openalex.org/W2739099888",
    "https://openalex.org/W2606794139",
    "https://openalex.org/W2739325416",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W6637242042",
    "https://openalex.org/W2898496484",
    "https://openalex.org/W2887139759",
    "https://openalex.org/W2096619076",
    "https://openalex.org/W2161236525",
    "https://openalex.org/W2563705555",
    "https://openalex.org/W2149527187",
    "https://openalex.org/W2975535607",
    "https://openalex.org/W2126680226",
    "https://openalex.org/W2116208049",
    "https://openalex.org/W2138218001",
    "https://openalex.org/W2038118117",
    "https://openalex.org/W1971239273",
    "https://openalex.org/W2510190030",
    "https://openalex.org/W2760814882",
    "https://openalex.org/W2807461033",
    "https://openalex.org/W2098605450",
    "https://openalex.org/W2072510697",
    "https://openalex.org/W1954814386",
    "https://openalex.org/W1561782558",
    "https://openalex.org/W2104035329",
    "https://openalex.org/W2080982032",
    "https://openalex.org/W2322772590",
    "https://openalex.org/W2149581961",
    "https://openalex.org/W6734522614",
    "https://openalex.org/W2517225990",
    "https://openalex.org/W6733961436",
    "https://openalex.org/W2068715223",
    "https://openalex.org/W2104769793",
    "https://openalex.org/W2059901520",
    "https://openalex.org/W1181052087",
    "https://openalex.org/W1967979390",
    "https://openalex.org/W2035725550",
    "https://openalex.org/W2761326642",
    "https://openalex.org/W2075060084",
    "https://openalex.org/W2113651538",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2104335344",
    "https://openalex.org/W2593229017",
    "https://openalex.org/W2593351588",
    "https://openalex.org/W2952793010",
    "https://openalex.org/W2138618573",
    "https://openalex.org/W2308045930",
    "https://openalex.org/W1665214252",
    "https://openalex.org/W603908379",
    "https://openalex.org/W2095705004"
  ],
  "abstract": "In this paper, we propose a pairwise spatial transformer network (PSTN) for cross-view gait recognition, which reduces unwanted feature mis-alignment due to view differences before a recognition step for better performance. The proposed PSTN is a unified CNN architecture that consists of a pairwise spatial transformer (PST) and subsequent recognition network (RN). More specifically, given a matching pair of gait features from different source and target views, the PST estimates a non-rigid deformation field to register the features in the matching pair into their intermediate view, which mitigates distortion by registration compared with the case of direct deformation from the source view to target view. The registered matching pair is then fed into the RN to output a dissimilarity score. Although registration may reduce not only intra-subject variations but also inter-subject variations, we can still achieve a good trade-off between them using a loss function designed to optimize recognition accuracy. Experiments on three publicly available gait datasets demonstrate that the proposed method yields superior performance for both verification and identification scenarios by combining any gait recognition network benchmarks with the PST.",
  "full_text": "260 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. 31, NO. 1, JANUARY 2021\nCross-View Gait Recognition Using Pairwise\nSpatial Transformer Networks\nChi Xu , Yasushi Makihara, Xiang Li , Yasushi Yagi , Member, IEEE, and Jianfeng Lu\nAbstract— In this paper, we propose a pairwise spatial\ntransformer network (PSTN) for cross-view gait recognition,\nwhich reduces unwanted feature mis-alignment due to view\ndifferences before a recognition step for better performance. The\nproposed PSTN is a uniﬁed CNN architecture that consists of a\npairwise spatial transformer (PST) and subsequent recognition\nnetwork (RN). More speciﬁcally, given a matching pair of gait\nfeatures from different source and target views, the PST estimates\na non-rigid deformation ﬁeld to register the features in the\nmatching pair into their intermediate view, which mitigates\ndistortion by registration compared with the case of direct\ndeformation from the source view to target view. The registered\nmatching pair is then fed into the RN to output a dissimilarity\nscore. Although registration may reduce not only intra-subject\nvariations but also inter-subject variations, we can still achieve\na good trade-off between them using a loss function designed\nto optimize recognition accuracy. Experiments on three publicly\navailable gait datasets demonstrate that the proposed method\nyields superior performance for both veriﬁcation and identi-\nﬁcation scenarios by combining any gait recognition network\nbenchmarks with the PST.\nIndex Terms — Pairwise spatial transformer, convolutional\nneural network, gait recognition, cross-view.\nI. I NTRODUCTION\nG\nAIT is one popular behavioral biometric modality that\ncan be used to authenticate a person from his/her\nwalking style. Compared with other physiological biometrics\n(e.g., DNA, ﬁngerprints, irises, and faces), it exhibits unique\nadvantages in applications such as surveillance and crimi-\nnal investigation using cameras (e.g., closed-circuit television\nManuscript received July 9, 2019; revised December 7, 2019; accepted\nFebruary 15, 2020. Date of publication February 21, 2020; date of current\nversion January 7, 2021. This work was supported in part by the JSPS\nGrants-in-Aid for Scientiﬁc Research (A) under Grant JP18H04115, in part\nby the Jiangsu Provincial Science and Technology Support Program under\nGrant BE2014714, in part by the Programme of Introducing Talents of\nDiscipline to Universities under Grant B13022, and in part by the Priority\nAcademic Program Development of Jiangsu Higher Education Institutions.\nThis article was recommended by Associate Editor S. Gao.(Corresponding\nauthor: Chi Xu.)\nChi Xu and Xiang Li are with the School of Computer Science and\nEngineering, Nanjing University ofScience and Technology, Nanjing 210094,\nChina, and also with the Institute of Sc ientiﬁc and Industrial Research,\nOsaka University, Osaka 567-0047, Japan (e-mail: xuchisherry@gmail.com;\nlixiangmzlx@gmail.com)\nYasushi Makihara and Yasushi Yagi are with the Institute of Scientiﬁc\nand Industrial Research, Osaka University, Osaka 567-0047, Japan (e-mail:\nmakihara@am.sanken.osaka-u.ac.jp; yagi@am.sanken.osaka-u.ac.jp).\nJianfeng Lu is with the School of Computer Science and Engineer-\ning, Nanjing University of Scie nce and Technology, Nanjing 210094,\nChina (e-mail: lujf@mail.njust.edu.cn).\nColor versions of one or more of the ﬁgures in this article are available\nonline at https://ieeexplore.ieee.org.\nDigital Object Identiﬁer 10.1109/TCSVT.2020.2975671\nFig. 1. GEI examples from the same subject with different view angles.\nObvious intra-subject variations exist among the GEI features from different\nobservation view angles.\n(CCTV)) because it works even for a subject captured at a\nlong distance from a camera without his/her cooperation (i.e.,\na person at a low image resolution) and also is difﬁcult to keep\non concealing and impersonating in daily life. Gait recognition\nhas therefore been of great importance for many applications\nin surveillance, forensics, and criminal investgation [1]–[3].\nInvolving uncooperative subjects, however, makes gait\nrecognition more easily affected by various covariates, includ-\ning view [4], [5], clothing [6], [7], and walking speed [8],\n[9]. Among these covariates, view variation is one of the most\ncommon challenging factors and often exists in real applica-\ntions (e.g., CCTV footage captured from different observation\nview angles). As shown in Fig. 1, the view changes raise large\nintra-subject variations in widely used appearance-based gait\nfeatures, such as the gait energy image (GEI) [10], which may\ndrastically degrade the performance of gait recognition.\nExtensive studies [4], [5], [11]–[16] on cross-view gait\nrecognition mainly fall into two categories: the generative\napproaches and discriminativ e approaches. The generative\napproaches generally transform gait features from one view\n(e.g., gallery view) to a different view (e.g., probe view) [4],\n[13], or transform features from different views into a com-\nmon canonical view (e.g., side view) [11], [17]. However,\nthese approaches do not guarantee optimal recognition accu-\nracy [14], [15] because they essentially consider not recog-\nnition accuracy but the quality of transformed gait features.\nBy contrast, discriminative approaches mainly aim at learn-\ning view-invariant subspaces or metrics to directly optimize\nthe discrimination capability without undertaking registration\namong features from various views, such as traditional linear\ndiscriminant analysis (LDA) [10] and rank support vector\nmachine (rank SVM) [18]. It is, however, difﬁcult to ﬁnd a\nrobust subspace or metric for non-aligned features, particularly\nfor the case of large view differences.\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information,\nsee https://creativecommons.org/licenses/by/4.0/\nXU et al.: CROSS-VIEW GAIT RECOGNITION USING PSTNs 261\nRecently, by introducing convolutional neural\nnetwork (CNN) frameworks, the performance of cross-view\ngait recognition has been promoted increasingly further [5],\n[14]–[16], [19]–[23]. The great success of CNN-based\nmethods is partly because of the max-pooling layers, which\nallow the networks to be somewhat spatially invariant to the\nintra-subject variations in the body parts that result from view\ndifferences [15]. On the other hand, the max pooling layer\nmay wash out subtle inter-subject variation (e.g., a slight\nback contour difference caused by body shape variation),\nand hence we need to consider the trade-off between\nspatial displacement invaria nce caused by view variation\nand maintaining subtle inter-subject variation. In fact, [15]\nsuggested maintaining the subtle inter-subject variation by\ntaking the difference at the low level before going through\nthe pooling layer, when the view angle difference is small.\nThis implies that CNN-based gait recognition accuracy may\nfurther improve by incorpora ting a preceding r egistration\nprocess for a matching pair of gait features, in a similar\nmanner to face recognition accuracy improving as a result\nof image registration in advance for pose variation [24], [25]\nand expression variation [26].\nThe spatial transformer network (STN) [27] is one such\nregistration technique, and includes a spatial transformer (ST)\nmodule that explicitly performs spatial transformation on input\nfeatures. Because the ST is typically used as a sub-network\nof the entire STN designed for main tasks such as object\nclassiﬁcation [27], [28] and face recognition [29]–[31], the ST\nis trained to provide registration parameters that are suitable\nfor the main task. The conventional ST often takes a sin-\ngle input and regresses afﬁne transformation parameters to\ntransform the input features to a canonical view or pose for\nthe main task. Whereas such an architecture is suitable for\nclassiﬁcation tasks, such as digit recognition [27], it is not\nnecessarily suitable for matching tasks, such as cross-view gait\nrecognition. For example, assume that the canonical view is set\nto side view 90\n◦, whereas a matching pair is observed from 0◦\nand 30◦. In this case, it is infeasible to transform gait features\nfrom 0◦ and 30◦ into those from the canonical view, that is,\n90◦, and hence, a direct application of the conventional ST is\nan unsuitable choice for cross-view gait recognition.\nWe therefore propose a uniﬁed pairwise spatial transformer\nnetwork (PSTN) that contains a pairwise spatial trans-\nformer (PST) module for cross-view gait recognition, which\ntakes a pair of probe and gallery gait features as the input\nin the network architecture. Instead of transforming a single\ninput feature into a canonical view using afﬁne transformation,\nthe features in the input pair from different views are both\nregistered into their intermediate view via the learned appro-\npriate non-rigid transformation by the proposed PSTN, where\nthe intra-subject differences caused by the view variations are\nwell suppressed while maintaining the inter-subject differences\nsimultaneously. The contributions of this work are four-fold.\nA. PST for Cross-View Matching\nRather than transforming the single input into a general\ncanonical view in the conventional ST [27], the proposed\nPST transforms a pair of inputs (i.e., probe and gallery)\ninto their intermediate vie w between the probe view and\ngallery view, which avoids unnecessary large distortion. To the\nbest of our knowledge, this is the ﬁrst time that geometric\nfeature registration has been introduced into a CNN-based gait\nrecognition framework.\nB. Managing Arbitrary View Combination Without Knowing\nthe View Information in Advance With a Single Uniﬁed\nCNN Model\nUnlike existing generative a pproaches that require view\ninformation in advance, the proposed method does not require\nview information throughout the training and testing processes,\nbecause the PST is trained to output a suitable transformation\nﬁeld for each input pair without view information.\nC. A Uniﬁed CNN Framework That Involves Both Generative\nand Discriminative Models\nThe proposed PSTN is composed of a PST and subsequent\nrecognition network (RN), which is a uniﬁed CNN framework\nthat involves both generative and discriminative models, and\nhence the optimal transformation for the main recognition task\nthat achieves a trade-off betweenintra-subject variations and\ninter-subject variations is predicted by the proposed PSTN,\nunlike traditional generative methods, which only aim to\nreduce the intra-subject differences in feature generation rather\nthan optimizing recognition accuracy.\nD. State-of-the-Art Performance Among GEI-Based Methods\non Three Publicly Available Datasets\nWe evaluated the proposed method on three publicly avail-\nable gait datasets: the OU-ISIR Gait Database, Multi-View\nLarge Population Dataset (OU-MVLP) [32], OU-ISIR Gait\nDatabase, Large Population Dataset (OULP) [33], and CASIA\nGait Database, Dataset B (CASIA-B) [34]. OU-MVLP is the\nworld’s largest gait database with wide view variation, which\nenables a more statistically reliable performance evaluation,\nwhereas OULP and CASIA-B are datasets widely used for\nexisting cross-view gait recognition studies. By combining the\nproposed PST with any gait recognition network benchmarks\nas the following RN, the proposed PSTN achieves performance\nimprovement on the three datasets, which yields state-of-\nthe-art accuracy in the ﬁeld of GEI-based methods for both\nveriﬁcation and identiﬁcation scenarios.\nII. R\nELATED WORK\nA. Generative Approaches to Cross-View Gait Recognition\nGenerative approaches to cross-view gait recognition\ncan be divided into two categories: geometry-based and\nexample-based. Some geometry-based approaches construct\na three-dimensional (3D) human model for gallery subjects\nfrom two-dimensional (2D) images for multiple views using\na model-ﬁtting [35], [36] or visual intersection method [37],\nwhereas some methods [11], [38] project gait templates into a\ncanonical view (i.e., side view) based on the assumption that\nthe human body is well approximated as a planar object on a\nsagittal plane. These methods are, however, only applicable for\n262 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. 31, NO. 1, JANUARY 2021\ncooperative scenarios with multiple calibrated cameras, or only\nwork for the case in which view differences from the side view\nare small, where the aforementioned assumption holds [39].\nMost generative approaches belong to the example-based\ncategory, which learns the transformation between different\nviews based on the training set. Makiharaet al. [4] proposed\na view transformation model (VTM) that applies singular\nvalue decomposition (SVD) on frequency-domain features.\nSubsequently, a variety of VTM-based methods were proposed\nto improve performance using different algorithms, such as\nsupport vector regression (SVR) [12], [13] and multi-layer\nperceptron [40], and using 3D training gait models [41]. The\naforementioned methods may corrupt the geometric continuity\nof the human body that results in non-humanoid generated\ngait features; hence, El-Alfyet al. [39] proposed a geometric\nview transformation model (GVTM), which is the only model\nthat considers both geometric deformation and example-based\nlearning to avoid possible corruption in appearance-based gait\nfeatures.\nHowever, generative methods all require view information\nto construct a transformation for each view combination in\nadvance. Additionally, they only ensures the optimal gener-\nation of gait features rather than recognition accuracy. Fur-\nthermore, the learned transformation is generic across the\npopulation, which also fails to represent subject individuality\nin the transformation.\nB. Discriminative Approaches to Cross-View Gait\nRecognition\nDifferent from generative a pproaches, discriminative\napproaches use machine learning techniques to directly\noptimize the discriminatio n capability without feature\nregistration. One typical method is to apply traditional\nLDA after dimension reduction using principal component\nanalysis [10]. Rather than LDA, Lu and Tan [42] used\nuncorrelated discriminant simplex analysis; Mansuret al. [43]\nused multi-view discriminant analysis (MvDA); and\nMartin-Felez and Xiang [18] exploited rank SVM. Zhang\net al. [44] proposed discriminative projection with list-wise\nconstraints and rectiﬁcation (DPLCR) using a new gait\nrepresentation called the gait individuality image (GII).\nInstead of learning a common r elatively view-invariant\nsubspace or metric, canonical correlation analysis (CCA) [45]\nwas introduced to project features on two latent subspaces with\nmaximal correlation. Kusakunniranet al. [46] further applied\ncorrelated motion co-clustering (CMCC) to solve the weakly\ncorrelated problem in global gait features, and Xinget al.[47]\nproposed complete canonical correlation analysis (C3A) to\nimprove the performance of CCA for high-dimensional fea-\ntures. Unlike the aforementioned CCA-based methods that\nneed view information, unitary linear view-invariant discrim-\ninative projection (ViDP) [48] transfers features into latent\nspace without knowing the view angles.\nAlthough discriminative approaches generally achieve better\nresults than generative approaches, most of them still work\npoorly for the case of large view variations because it is quite\nchallenging to ﬁnd robust view-invariant subspaces or metrics\nwith high generalizations for such non-aligned gait features.\nC. CNN-Based Approaches to Cross-View Gait Recognition\nTo date, CNN-based approaches have achieved state-of-the-\nart performance in cross-viewgait recognition, where various\ninput features and network structures have been discussed.\nWu et al. [19] used raw silhouette images and Wolfet al. [49]\nexploited spatio-temporal features as inputs. Reference [23]\ndirectly inputted RGB images todisentangle appearance and\npose features, and used the latter one for subsequent recog-\nnition. However, most CNN-based approaches [5], [14], [15],\n[20], [50] directly feed the GEI into the networks. GEINet,\ndesigned by Shiraga et al. [14], is a typical network and has\na similar structure to AlexNet [51]. Some approaches [5],\n[20] have demonstrated that CNN models with two inputs,\nwhere the similarities between the two inputs are learned\nto discriminate whether they are from the same subject\nor different subjects, achieve better performance than the\naforementioned one-input networks. In [15], different CNN\narchitectures were explored to consider the type of recognition\ntask (i.e., veriﬁcation and identiﬁcation) and degree of view\ndifferences. Zhang et al.. [22] proposed a joint network to\ncombine the advantages of using a single input and a pair\nof inputs with the quintuplet loss function. A recent work\nnamed GaitSet [21] regarded gait as a set of independent\nframes, without considering the order information of silhouette\nframes in the gait sequence, which achieved prominent gait\nrecognition performance.\nAlthough these approaches have achieved promising results,\nthe networks that only consider discrimination learning with-\nout feature registration are still limited in their ability to\nbe invariant to large spatial displacement on the inputs by\nonly using convolutional layers and local pooling layers [27],\nwhich may also wash out subtle personal gait characteristics\n(e.g., body shape and walking style) simultaneously.\nSome approaches have used generative adversarial networks\n(GAN), which generate a gait feature of the common canonical\nview (i.e., side view) [50] or generate a probe feature of the\nsame view as that of a gallery feature after detecting the view\nangles [16]. However, GAN-based methods encounter the\nfollowing two limitations: First, GAN models need to optimize\nboth the feature generation quality and recognition accuracy\nsimultaneously, and hence they suffer from hyperparameter\ntuning. Second, the generated gait feature may be corrupted\nbecause GAN is not a geometric but an example-based\napproach, and hence the geometric continuity of the human\nbody is never considered.\nD. STN\nRecently, Jaderberg et al. [27] proposed a differentiable\nmodule, the ST, to explicitly perform parameterized spatial\ntransformation inside the network by inserting it into any\nexisting CNN architecture, which improves the invariance to\nsigniﬁcantly large spatial displacement for the network. The\noptimal transformation for the main task (e.g., classiﬁcation) is\nadaptively learned based only on a loss function for the main\ntask in an end-to-end manner, without any extra supervision\nor modiﬁcation to the objective function, unlike the GANs’\noptimization of a loss function, which considers both feature\ngeneration and the performanceof the main task alternately.\nXU et al.: CROSS-VIEW GAIT RECOGNITION USING PSTNs 263\nFig. 2. Overview of the proposed PSTN framework, which contains a PST and RN. Two networks are designed for two types of gait recognition scenarios\naccording to [15]. (a) is for the gait veriﬁcation task using contrastive loss with a pair of input GEIs, and (b) is for the gait identiﬁcation task usingtriplet\nloss with triplet input GEIs.\nWith the illustration of state-of-the-art performance for digit\nrecognition in the original paper of STN [27], STN has\nbeen extended to many applications, such as face recognition.\nWhereas [29] adopted afﬁne transformation, [52] compared\nthe performance of ST based on three types of homogeneous\ntransformations for face alignment. Shi and Jain [31] designed\nan ST-based attention network to extract both global and\nlocal features, and Wu et al. [30] proposed a recursive ST\nto hierarchically model a more complex transformation by\nperforming afﬁne transformation in divided local regions.\nTo render non-rigid transformation, [28] introduced diffeomor-\nphisms into ST, which results in a ‘squariﬁcation’ effect and\nbetter performance for face veriﬁcation.\nThe aforementioned ST modules mainly aim at transforming\na single input image into a common canonical view, which\ncannot manage gait feature transformation well when the\ncanonical view is not in-between the probe and gallery views\nor the view difference between the probe and gallery views is\nconsiderably large, where the gait behavior in one view cannot\nbeen seen in another view.\nIII. G\nAIT RECOGNITION USING PSTN\nA. Overview\nAn overview of the proposed PSTN framework is shown\nin Fig. 2. Following most CNN-based approaches [5], [14],\n[15], [20], [50], GEI is adopted as the input gait feature,\nwhich is most widely used for gait recognition studies because\nit is effective despite its simplicity. Given raw gait videos,\nsilhouette sequences can be ﬁrst extracted using segmenta-\ntion methods (e.g., background subtraction-based graph-cut\nsegmentation [53], recent state-of-the-art deep learning-based\nsemantic segmentation methods, such as ReﬁneNet [54], fol-\nlowed by a boundary reﬁnement method such as Dense-\nCRF [55] since the semantic segmentation methods extracts\noften larger segments than the actual object).\n1 The height\nis then normalized and the region center of the silhouettes\n1We used extracted silhouette sequences released by each dataset provider\nin our experiments.\nis registered, and the gait period is detected based on the\nautocorrelation of the size-normalized and registered silhouette\nsequences [4]. Thus, GEI is obtained by averaging the silhou-\nettes over one gait period, which reﬂects both the static and\ndynamic (e.g., arm swing and leg motion) gait information.\nThe proposed CNN framework is composed of two parts:\nPST and RN. Given an input pair of probe and gallery GEIs,\ninstead of a common transformation used by existing genera-\ntive approaches, an appropriate sample-dependent geometric\ntransformation is regressed by the PST to transform both\nthe probe and gallery GEIs from different views into their\nintermediate view, which are further fed into the subsequent\nRN to obtain the ﬁnal dissimilarity between this pair. Sim-\nilar to [15], we design two networks for two types of gait\nrecognition scenarios: veriﬁcation (i.e., one-to-one matching)\nand identiﬁcation (i.e., one-to-many matching). Whereas the\nveriﬁcation network uses the contrastive loss, the network for\nidentiﬁcation adopts the triplet loss using triplet GEIs, which\nis similar to the Siamese network [20], where the parameters\nfor PST and RN are shared, respectively.\nDetails for the PST and RN are given in the following\nsections.\nB. PST\nSimilar to the conventional ST [27], the proposed PST also\nconsists of three components, that is, the localization network,\ngrid generator, and sampler, which is shown in Fig. 3. Instead\nof afﬁne transformation used in many studies, a non-rigid\ntransformation based on free-form deformation (FFD) [56] is\nused for the PST because the FFD is suitable for reﬂecting\nthe transformation of non-rigid objects (e.g., human body)\nbecause of its high ﬂexibility. Moreover, the FFD also retains\ngeometric continuity in adjacent regions [56], and hence never\ncorrupts the personalized gait characteristics, whereas such\ncorruption in feature generation may easily occur in existing\nexample-based generative approaches [4], [13].\nFor example, in case where a test subject walks in\nan extraordinary manner (e.g., extremely large arm swing\nor heavy stoop), or owns an extraordinary body shape\n264 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. 31, NO. 1, JANUARY 2021\n(e.g., extremely fat or thin), which are extraordinary gait\nfeatures never included in the training samples, example-based\ngenerative approaches may easily make these extraordinary\n(or distinctive) gait characteristics disappear and may generate\nmore common gait features instead. Moreover, assuming that a\ntraining sample unfortunately contains noise in the background\nregion (e.g., over-segmented isolated foreground regions in\nbackground area in GEI) and that a test sample resembles\nto the training sample with noise by chance, the noise may\npop-out for the generated gait feature for the test sample.\nBy contrast, using the geometric transformation with a spa-\ntially smooth warping ﬁeld, the isolated noisy foreground\nregions never pop-out newly and extraordinary (or distinctive)\ngait features are likely to be kept to some extent thanks to the\nproperty of the geometric continuity, which is more beneﬁcial\nfor the subsequent recognition task.\nTherefore, we adopt FFD-based geometric transformation\nfor the PST module, and consider transforming a pair of probe\nand gallery GEIs from different views into their intermediate\nview to avoid unnecessary distortion. During the training of\nthe entire PSTN, the PST is supervised by the loss of the\nfollowing RN, which aims to learn a transformation that\nachieves a trade-off between intra-subject and inter-subject\nvariations, and further leads to optimal recognition perfor-\nmance. We describe an overview of the FFD framework and\ndetails of the three components in the following.\n1) Overview of FFD: We ﬁrst introduce the general repre-\nsentation of FFD in this section. To represent FFD, we ﬁrst\nallocate a set of grid-type control points on the GEI [56],\n[57], as shown in Fig. 4. More speciﬁcally, given source GEI\nG\ns ∈ RH×W ,w h e r eH and W are the height and width\nof the image, respectively, we set nW control points with\ninterval \u0002x = (W − 1)/(nW − 1) and nH control points\nwith interval \u0002y = (H − 1)/(nH − 1) for the horizontal\nand vertical directions, respectively. The spatial position of the\n(i, j)-th control point located in thei-th column and j-th row\nis denoted by pi,j =[ i\u0002x, j\u0002y]T (i = 0,..., nW − 1, j =\n0,..., nH − 1).\nWe then deﬁne a set of 2D displacement vectors that indicate\na deformation on the control points asu, where the displace-\nment vector on the(i, j)-th control point is denoted byui,j =\n[ui,j ,v i,j ]T ∈ R2. Therefore, the position of the (i, j)-th\ncontrol point is transformed topi,j + ui,j . The entire warping\nﬁeld throughout image F(u) is obtained using interpolation\nfrom the displacement vectors on the control points, which\nrepresents the coordinate correspondence between the source\nGEI and transformed GEI for each pixel. Finally, the trans-\nformed GEI is obtained asGt = Gs ◦ F(u),w h e r e◦ denotes\na transformation operator. In practice, we usually implement a\nbackward (or inverse) warping ﬁeld (i.e., that from the target\nimage to the source image) instead of a forward warping ﬁeld\n(i.e., that from the source image to the target image) when we\ntry to transform a source image to a target image [58].2\n2) Localization Network: Given an input pair of probe\nand gallery GEIs, the localization network regresses a\n2Readers may refer to page 25—26 and page 31—35 in Chapter 3 of [58]\nfor more details.\nFig. 3. Structure of the proposed PST, which is composed of the localization\nnetwork, grid generator, and sampler. In the localization network, C, FC, and\npool denote the convolution layer, fullyconnected layer, and pooling layer,\nrespectively. The digits written before C represent the ﬁlter size with the\nstride, whereas those after C representthe number of ﬁlters. The digits after\nFC represent the number of output neurons. This notation is used to illustrate\nthe network structure throughout this paper.\nFig. 4. Illustration of the FFD framework.\ntransformation parameter vector, that is, a set of displacement\nvectors on the control pointsu. The transformation parameter\nvector u is used to deﬁne an inverse warping ﬁeld from\nthe intermediate view to a probe view and gallery view,\nas described later.\nThe localization network is then designed as a simple\nCNN whose input is a subtraction image of the original GEI\npair and output is transformation parameter vector u.N o t e\nthat both the original probe and gellery GEI can be the\nminuend for the input of subtraction image. More speciﬁcally,\nthe localization network constitutes two convolutional layers,\ntwo pooling layers, and two fullyconnected layers (see Fig. 3).\nA max pooling with 2 × 2 pixels with stride 2 is used\nfor the pooling layers, and the rectiﬁed linear unit (ReLU)\nactivation function [59] is used for all convolution layers and\nthe ﬁrst fully connected layer. Additionally, local response\nnormalization (LRN) [60] is applied before the max-pooling\nlayers. Transformation parameter vectoru is regressed by the\nlast fully connected layer.\n3) Grid Generator: After obtaining transformation parame-\nter vector u from the localization network, the grid generator\nproduces a warping ﬁeld, which describes the deformation on\neach pixel. To avoid unnecessary large distortion, we consider\ntransforming both probe and gallery GEIs from the original\nviews to their intermediate view, where the transformation\nbetween the intermediate view to each probe and gallery\nview is symmetrical to each other. Note that the intermediate\nview does not indicate the physically exact medial view, but\nan apparent intermediate view derived from the symmetric\nXU et al.: CROSS-VIEW GAIT RECOGNITION USING PSTNs 265\nFig. 5. Illustration of the opposite pair of warping ﬁeldsF(u) and F(−u).\ntransformation for the following two reasons. First, because\nthe training data could be generally collected under arbitrary\nviews, it is infeasible to prepare enough training samples from\nthe physically exact medial view, which are necessary for\nconstructing the transformation from the probe/gallery view to\nthe physically exact medial view. Second, because the warping\nﬁelds from a probe and a gallery to the physically exact medial\nview is asymmetric, two different warping ﬁelds are required\nfor estimation, which increases the number of parameters for\na STN. On the other hand, if the apparent intermediate view\nis adopted for the proposed PST, only a pair of GEIs without\nany view constraint (or information) is required for the training\nprocess, and only a single warping ﬁeld is needed for estima-\ntion thanks to the property of the symmetric transformation,\nwhich saves the number of parameters.\nWe therefore deﬁne warping ﬁeld F(u) from the GEI of\nthe intermediate view (referred to as the intermediate GEI)\nto the input probe GEI using piecewise linear interpolation\nfrom the displacement vectors o n the control points, and\ndeﬁne a reverse version,F(−u), as the warping ﬁeld from the\nintermediate GEI to the input gallery GEI. Please also note\nthat we deﬁne the intermediate view so as that deformation\nvectors from the intermediate view to the probe and gallery\nare just opposite each other, i.e.,u and −u.\nAssume there is a GEI at a virtual intermediate view with\nthe originally regular red grid, as shown in Fig. 5. The probe\nGEI is obtained by applying the warping ﬁeldF(u),w h e r et h e\nspatial position of the(i, j)-th control pointp\ni,j is transformed\nto the corresponding position in the warped green grid by\nthe displacement vectoru\ni,j (i.e., the transformed position of\nthe control point is pi,j + ui,j ). Similarly, the gallery GEI\nis obtained by applying the warping ﬁeldF(−u),w h e r et h e\nposition of the same control point is transformed to that in\nthe warped blue grid by the displacement vector−ui,j (i.e.,\nthe transformed position of the control point ispi,j − ui,j ),\nwhich is just the opposite ofui,j . Thus, an entirely symmetric\ntransformation between the intermediate view to each probe\nand gallery view is guaranteed, which is an important aspect\nfor constructing deformation between two different states.\nMore speciﬁcally, the displacement at position(x, y) on the\nentire warping ﬁeldF(u) is denoted byf\nx,y =[ f u\nx,y, f v\nx,y ]T ∈\nR2 (x = 0,..., W − 1, y = 0,..., H − 1). First, two general\nweighting functions are deﬁned as w0(x) = 1 − (x −\u0004 x\u0005)\nand w1(x) = x −\u0004 x\u0005,w h e r e\u0004·\u0005 is the ﬂoor function, and\ntwo variables are deﬁned as ¯x = x/\u0002x and ¯y = y/\u0002y.\nDisplacement f x,y is then computed using piecewise linear\ninterpolation from its four neighboring control points as\nf x,y =\n1∑\nk=0\n1∑\nl=0\nwk (¯x)wl (¯y)u\u0004¯x\u0005+k,\u0004¯y\u0005+l . (1)\nThe displacement for each pixel on warping ﬁeldF(−u) is\nobtained in the same manner.\nOnce the grid generator is deﬁned as above, the gradient\nof f d\nx,y (d ∈{ u,v }) is also computed with respect to each\nd\u0004¯x\u0005+k,\u0004¯y\u0005+l used during the back-propagation process as\n∂f d\nx,y\n∂d\u0004¯x\u0005+k,\u0004¯y\u0005+l\n= wk (¯x)wl (¯y), (2)\nwhere k,l ∈{ 0,1}.\n4) Sampler: As the ﬁnal procedure in the PST, the sampler\ngenerates the output image pairs by transforming each of input\nprobe and gallery GEIs, respectively. Let the input probe and\ngallery GEIs beGi\np, Gi\ng ∈ RH×W , and transformed probe and\ngallery GEIs be Go\np, Go\ng ∈ RH×W ,w h e r eGi\np = Go\np ◦ F(u),\nGi\ng = Go\ng ◦ F(−u). For pixel (xo\np, yo\np) of the output Go\np, its\ncorresponding source coordinates in the input are obtained as\n(xi\np, yi\np) = (xo\np + f u\nxop,yop\n, yo\np + f v\nxop,yop\n),w h e r ef u\nxop,yop\nand f v\nxop,yop\nare the horizontal and vertical di splacements, respectively,\nfor this pixel according to warping ﬁeldF(u). The intensity\nvalue I o\nxop ,yop\nat position (xo\np, yo\np) is sampled using bilinear\ninterpolation as\nI o\nxop ,yop =\n1∑\nk=0\n1∑\nl=0\nwk (xi\np)wl (yi\np)I i\n\u0004xip\u0005+k,\u0004yip \u0005+l , (3)\nwhere I i\n\u0004xip \u0005+k,\u0004yip\u0005+l is the intensity at position (\u0004xi\np\u0005+\nk,\u0004yi\np\u0005+ l) on original probe Gi\np, which indicates the four\nnearest pixels to spatial location(xi\np, yi\np).\nThe partial derivatives ofI o\nxop,yop\nwith respect to its related\ndisplacements f u\nxop ,yop\nand f v\nxop,yop\nare given as\n∂I o\nxop,yop\n∂f u\nxop,yop\n=\n∂I o\nxop,yop\n∂xip\n=\n1∑\nk=0\n1∑\nl=0\nck wl (yi\np)I i\n\u0004xip\u0005+k,\u0004yip \u0005+l ,\n∂I o\nxop,yop\n∂f v\nxop,yop\n=\n∂I o\nxop,yop\n∂yip\n=\n1∑\nk=0\n1∑\nl=0\ncl wk (xi\np)I i\n\u0004xip\u0005+k,\u0004yip \u0005+l , (4)\nwhere the coefﬁcients ck and cl are deﬁned as\nck =\n{\n1 k = 1\n−1 k = 0, cl =\n{\n1 l = 1\n−1 l = 0. (5)\nThe forward and back-propagation are executed similarly\nfor the gallery. Consequently, the loss gradients are enabled to\nﬂow back throughout the entire network, from the RN to the\nlocalization network in PST.\nC. RNs Considering the Recognition Scenarios and View\nVariation Degree\nThe transformed probe and gallery GEI pairs from the PST\nare subsequently fed into the RN for discriminative feature\nlearning. Because the proposedPST could be freely combined\nwith any CNN model, we choose four state-of-the-art network\n266 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. 31, NO. 1, JANUARY 2021\nFig. 6. RNs with four different architectures. Thetransformed probe and gallery GEI pairs are used as the inputs. L2 indicates the L2 norm/distance ofthe\none/two output from the FC4 layer. diff/2in directly takes the differencebetween the input GEI pair/output pair ofthe FC4 layer, whereas LB*/MT* usesp a i r\nﬁlters to compute the pixel-wise weighted sum of the two inputs/features at the C3 layer. 2in is a Siamese network, and MT* also shares parameters in the\nﬁrst two convolutional layers.\narchitectures for gait recognition as the RNs to investigate the\nperformance of PSTN.\nThe structures of the four RNs, diff, 2in [15], LB*, and\nMT*, are shown in Fig. 6. As introduced in Section III-C2,\ncontrastive loss and triplet loss are more suitable for gait\nveriﬁcation and identiﬁcation, respectively [15]; therefore,\nwe modify the structures of the original LB and MT [5] by\nreplacing cross-entropy loss with contrastive loss and triplet\nloss, respectively, and changing the output dimension of the\nlast fully connected layer accordingly (denoted by LB* and\nMT*). As a result, the four RNs share the same basic architec-\nture, which is composed of three convolutional layers and two\nmax-pooling layers with 2×2 pixels with stride 2, in addition\nto a fully connected layer. The ReLU activation function [59]\nis applied to all convolution layers and fully connected layers.\nThe LRN [60] is used before the max-pooling layers and the\ndropout technique [61] is used for the FC4 layer. Finally,\nthe L2 norm/distance of the one/two output from the FC4 layer\nis computed, which is also cons idered as the dissimilarity\nscore between the input pair of probe and gallery GEIs during\nthe test stage. The differences among these four RNs will be\ndiscussed in detail in Section III-C1.\n1) RNs Considering the View V ariation Degree:To suc-\ncessfully recognize that the input GEI pairs are from the\nsame subject or different subjects, it is necessary to con-\nsider a trade-off between the inter-subject differences and\nintra-subject differences that result from the view variations;\nhence, different network architectures are required that depend\non the degree of appearance change caused by different view\nangles [15].\nThe structures diff and LB* match (i.e., determines the\ndifference) the input transformed GEI pair at the initial stage\n(i.e., ﬁrst layer) of the networks, which makes the models more\nsensitive to local differences that arise from both intra-subject\nand inter-subject variations, and hence they are more suitable\nfor the case of relatively smaller view variations, where\nthe inter-subject variations are larger than the intra-subject\ndifferences. Whereas diff directly takes the difference between\nthe input pair before feeding it into the network, LB* simulates\nthe subtraction by calculating the pixel-wise weighted sum\nusing the pair ﬁlters [5].\nBy contrast, 2in and MT* compare the two inputs in the\nhigher level of the networks, and hence allow more spatially\ninvariant features to be extracted before the matching process.\nTherefore, 2in and MT* are more favorable for the case\nof relatively larger view variations because the intra-subject\nspatial differences are larger than the inter-subject variations\nbecause of the large view differences. Similar to the difference\nbetween diff and LB*, 2in computes the subtraction between\nthe outputs of the last FC4 layer with a Siamese network [20]\n(i.e., weights are shared between two columns), whereas MT*\ntakes the weighted differencesbetween the features learned at\nthe C3 layer, which is also based on the Siamese network at\nthe bottom two convolutional layers.\n2) RNs Considering the Recognition Scenario: Two sce-\nnarios are considered in gait recognition: veriﬁcation and\nidentiﬁcation. In the veriﬁcation scenario, a pair of probe\nand gallery GEIs is provided to assess whether the GEIs are\nfrom the same subject by comparing their dissimilarity score\nwith an acceptance threshold. Regarding gait identiﬁcation,\na probe is compared with all the galleries to locate the genuine\nGEI derived from the same subject as the probe, which\nis determined by calculating the smallest dissimilarity score\nusing the nearest neighbor classiﬁer.\nTo successfully discriminate the input GEI pairs in the ver-\niﬁcation scenario, it is necessary for the absolute dissimilarity\nscores of the same subject pairs to be smaller than those of\nthe different subject pairs, which coincides with the deﬁnition\nof the contrastive loss function as [62]\nL\ncon = 1\n2N\nN∑\nn=1\n(αn d2\nn + (1 − αn ) max(margin − dn,0)2), (6)\nXU et al.: CROSS-VIEW GAIT RECOGNITION USING PSTNs 267\nwhere N is the number of training GEI pairs and dn is\nthe dissimilarity score (i.e., L2 norm/distance of the one/two\noutputs from the FC4 layer) of then-th GEI pair.αn is set to\none when the GEIs in then-th pair originate from the same\nsubject, and zero otherwise. Consequently, the contrastive loss\nfunction is suitable to be used for the proposed PSTN in the\ngait veriﬁcation scenario.\nIn the identiﬁcation scenario, the dissimilarity score between\nthe probe and genuine (i.e., true match in the gallery) GEI\nis required to be relatively smaller than that of the probe\nand imposters (i.e., false match in the gallery) to obtain a\ncorrect match. To achieve this, a triplet that characterizes a\nrelative dissimilarity ranking order for the three GEI images,\nthat is, probe, genuine, and imposter, is adopted as the input\nof the entire proposed framework, which is similar to [15].\nCorrespondingly, two parallel PSTs and RNs with respectively\nshared weights are designed for the input genuine pair and\nimposter pair, as shown on the right of Fig. 2. A triplet loss\nfunction is used for the proposed PSTN, which is deﬁned\nas [63]\nL\ntri = 1\n2N\nN∑\nn=1\nmax(margin − dn\nimp + dn\ngen,0)2, (7)\nwhere N is the number of training GEI triplets, dn\nimp is\nthe dissimilarity score of the imposter pair, anddn\ngen is that\nof the genuine pair for the n-th input triplet. As a result,\ndn\ngen(n = 1,..., N) is trained to be relatively smaller than\ndn\nimp, which satisﬁes the requirement of an accurate match for\nthe identiﬁcation task.\nD. Training Process\nTo boost the performance of the proposed PSTN, we ﬁrst\npre-trained the PST part using a subset of the whole training\nset without any additional training data, and then used the\npre-trained model as the initialization for the PST part to\nﬁne-tune the whole PSTN.\nMore speciﬁcally, the PST part was ﬁrst pre-trained using\nonly the same subject pairs from two randomly selected\nview angles, which aimed to optimize the deformation effects\napplied by the PST. Concretely speaking, we minimized the\ndifferences between the transformed training GEI pairs using\nthe Euclidean loss. To smooth the output images, we addition-\nally deﬁned a regularizer loss with coefﬁcientλ to ensure the\nspatial consistency between displacements at adjacent control\npoints [57]. Thus, the Euclidean loss and regularizer loss with\nits coefﬁcient constituted the total loss for pre-training the PST.\nOnce the PST is pre-trained, we ﬁrst used the pre-trained\nPST model to initialize the weights of the PST part in the\nPSTN, and then ﬁne-tuned the entire PSTN only with the\ncontrastive/triplet loss introduced in Section III-C2 in an end-\nto-end manner, where the deformationsF(u) and F(−u) were\nagain simultaneously modiﬁed to obtain optimal recognition\naccuracy.\nIV . E\nXPERIMENTS\nA. Datasets\nWe evaluated the proposed method on three pub-\nlicly available datasets: OU-MVLP [32], OULP [33], and\nCASIA-B [34].\nOU-MVLP is currently the world ’s largest gait database\nwith wide view variation, and was collected in conjunction\nwith an experience-based long-run exhibition at a science\nmuseum (i.e., Miraikan). The dataset contains 10,307 subjects\ncaptured from 14 view angles, ranging from 0\n◦ to 90◦ and\n180◦ to 270◦ in 15◦ intervals. Examples of GEIs from each\nview angle can be found in Fig. 1. Two sequences (i.e., probe\nand gallery sequences) are provided for each subject from each\nview angle. This dataset was used for all our experiments to\nmake the performance evaluation more statistically reliable.\nFollowing the protocol of the dataset [32], 5,153 subjects\nwere used for training, and the other disjoint 5,154 subjects\nwere used for testing. Based on the perspective projection\nassumption [64], GEIs with view angles over 180\n◦ were\nﬂipped right-to-left to roughly align the walking direction in\nthe GEIs, which is easier to estimate (i.e., leftward or right-\nward) compared with the exact view angle estimation [15].\nIn the training phase, GEIs from all view angles were fed\ninto the PSTN simultaneously, and in the test stage, following\n[15], performance was evaluated for each combination of four\ntypical views: 0\n◦,3 0◦,6 0◦, and 90◦.\nThe OULP dataset is the second largest gait dataset and\nconsists of over 4,000 subjects with four different views:\n55\n◦,6 5◦,7 5◦, and 85◦. Similar to OU-MVLP , both probe\nand gallery sequences are provided for each view angle.\nTo compare our results with the benchmarks [14], [39], [43],\n[65], [66] in Section IV-F, the same subset that they used\nwas chosen, which comprised 1,912 subjects. The subset was\nfurther divided into two disjoint sets of equal size, which were\nused for training and testing separately.\nThe CASIA-B dataset includes a relatively small number\nof subjects, that is, 124, but also has a wide view variation.\nEleven view angles ranging from 0\n◦ to 180◦ in 18◦ intervals\nwith six normal walking sequences (NM #01-06) per view are\nprovided for each subject. This dataset allows us to evaluate\nperformance for low-quality gaitsilhouettes with segmentation\nerrors. The same protocol protocol as [5], [16] was used for the\nexperiment in Section IV-G. Speciﬁcally, the ﬁrst 74 subjects\nwere used for training, and performance was evaluated using\nthe remaining 50 subjects, where four sequences (NM #01-04)\nwere chosen as galleries, whereas the other two sequences\n(NM #05-06) were used as probes.\nB. Implementation Details\nWe initialized the weight parameters of all layers using\nXavier’s algorithm [67], except for the last fully connected\nlayer in the PST, which was initialized to zero. The bias\nterms were all set to zero initially. The momentum for weights\nand bias terms was 0.9, and the weight decay was zero. The\nnetwork parameters were trained using the stochastic gradient\ndescent algorithm [68] with a min-batch size of 600. Basically,\nthe initial learning rate was set to 0.001 for the PST and\n0.01 for the RN, which were both divided by 10 four times\nduring the training stage. The proportion of dropping neurons\nwas set to 0.5 for the dropout technique applied in the last\nlayer of the RN. The hyperparameters of the margin in Eqs. (6)\nand (7) were both set to 3 empirically.\n268 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. 31, NO. 1, JANUARY 2021\nFig. 7. Examples of transformed GEIs from the ﬁne-tuned PSTN (PST-LB*) andpre-trained PST. The ﬁrst and second row show the learned transformations\nfor a genuine pair and imposter pair, respectively. (a) Original probe GEI from 30◦. (b) Probe warping ﬁeld learned by pre-trained PST. (c) Probe warping ﬁeld\nlearned by PSTN. (d) Transformed probe using pre-trained PST. (e) Transformed probe using PSTN. (f) Original gallery GEI from 90◦. (g) Gallery warping\nﬁeld learned by pre-trained PST. (h) Gallery warping ﬁeld learned by PSTN. (i) Transformed gallery using pre-trained PST. (j) Transformed gallery using\nPSTN. (k) Absolute difference image and corresponding Euclidean distance between original GEIs (a) and (f). (l) Absolute difference image and Euclidean\ndistance between transformed GEIs (d) and (i) using pre-trained PST. (m) Absolute difference image and Euclidean distance between transformed GEIs(e) and\n(j) using PSTN.\nRegarding the constitution of training sample pairs for the\ngait veriﬁcation task, we used all the same subject pairs by\nrandomly selecting two view angles, and randomly chose parts\nfrom all different subject pairs to maintain the ratio of the\nnumber of same subject pairs to that of different subject\npairs equal to 1: 9. To make up the training triplets for the\nidentiﬁcation scenario, we randomly selected approximately\n30 million triplets from all possible sample combinations.\nConsidering the quite limited training subjects in the CASIA-B\ndataset, we used all the pairs\n3 and triplets to increase the\namount of training data.\nTo evaluate the recognition performance, we computed the\nequal error rate (EER) of the false acceptance rate and false\nrejection rate to measure accuracy in the veriﬁcation scenario,\nand used the rank-1 identiﬁcation rate as the evaluation crite-\nrion for the identiﬁcation scenario.\nC. Visualization of PST\nWe ﬁrst visualize the transformation learned by the proposed\nPST module using the examples of a genuine pair and imposter\npair with the same probe to illustrate the sample-dependence of\nthe transformation. We choose a case of relatively larger view\nvariation, where the probe and two galleries are from 30\n◦ and\n90◦, respectively. To better understand the trade-off between\nthe intra-subject and inter-subject differences, we show both\nthe transformation learned by the pre-trained PST and the\nentire ﬁne-tuned PSTN (we used LB* as the RN as an\nexample, denoted by PST-LB*) in Fig. 7.\nComparing the two corresponding learned warping ﬁelds for\nthe genuine pair and imposter pair (Fig. 7(b)(c) and (g)(h)),\nit is obvious that the learned transformations vary for dif-\nferent sample pairs, which makes it possible to generate a\ntransformation that represents individual gait characteristics\nusing a single CNN model rather than a common deformation\ntypically applied by traditional methods [4], [39]. Because of\nthe view variation, both the original GEI pairs show large\nimage differences (Fig. 7(k)). By transforming the original\nGEIs with the learned deformation ﬁelds into the intermediate\n3The same subject pairs were duplicated to keep the ratio of the number of\nsame subject pairs to that of different subject pairs still equal to 1: 9.\nview (Fig. 7(d)(e)(i)(j)), such differences are reduced by\nboth the pre-trained PST (Fig. 7(l)) and ﬁne-tuned PSTN\n(Fig. 7(m)).\nAs a result, the pre-trained PST signiﬁcantly reduces the\nintra-subject difference of the genuine pair by transforming\nthe probe and gallery into almost the same intermediate view.\nThe imposter pair, however, is still even more similar than\nthe genuine pair after the transformation, which degrades the\ndiscrimination capability (e.g., body shape change between\nFigs. 7(a) and (d) or (f) and (i) in the second row), and may\nfurther result in a false match in the subsequent recognition\ntask (larger difference for the genuine pair than the imposter\npair in Fig. 7(l)). This means that the pre-trained PST alone\nmay risk overly registering the GEI pairs regardless of the\ndifference derived from the intra-subject view variation or the\ninter-subject variation.\nBy contrast, the proposed PSTN ﬁne-tuned by the recog-\nnition loss (i.e., contrastive/triplet loss) somewhat weakens\nthe registration effect and hence makes dissimilarity measures\nboth for the genuine pair and imposter pair larger than\nthose by the pre-trained PST. Importantly, we notice that the\ndissimilarity for the imposter pair increases more than that\nfor the genuine pair by changing the pre-trained PST to the\nﬁne-tuned PSTN, and consequently the dissimilarity for the\nimposter pair becomes larger than that for the genuine pair\n(Fig. 7(m)). This implies that the PSTN ﬁne-tuned by the main\nrecognition task works to reduce the intra-subject difference\ncaused by view variation while not unnecessarily reducing\nthe inter-subject difference (e.g., body shape change observed\nbetween Figs. 7(a) and (d) or (f) and (i)), which are two\nconﬂicting aspects; that is, the ﬁne-tuned PSTN achieves a\ngood trade-off between reducing the intra-subject difference\ncaused by view variations and maintaining the inter-subject\ndifference, unlike the pre-trained PST, which tends to overly\nregister, and the baseline without PST, which does nothing\nregarding registration.\nD. Effect of PST\nTo conﬁrm the effectiveness of the proposed PST mod-\nule, we compared the recognition performance between the\nXU et al.: CROSS-VIEW GAIT RECOGNITION USING PSTNs 269\nTABLE I\nCOMPARISON BETWEEN THE RNSW /O AND W/ A PST MODULE FOR BOTH VERIFICA TION AND IDENTIFICATION SCENARIOS USING OU-MVLP. E ACH\nROW IN THE TABLE SHOWS THE RESULTS OF THE PURE RN (BEFORE SLASH ) AND THAT COMBINED WITH THE PST (AFTER SLASH ). BOLD\nAND ITALIC BOLD INDICATE THE BEST AND SECOND -BEST RESULTS FOR EACH ANGULAR DIFFERENCE ,R ESPECTIVEL Y.T HIS FONT\nCONVENTION IS USED TO INDICATE PERFORMANCE THROUGHOUT THIS PAPER\nRNs without and with the PST module (i.e., PSTN) for the\nsame parameter settings. More speciﬁcally, for the veriﬁcation\nscenario, we used the EER to evaluate all the four RNs\nintroduced in Section III-C1, that is, diff, 2in, LB*, and MT*,\nand those combined with the PST, which are denoted by\nPST-diff, PST-2in, PST-LB*, and PST-MT*, respectively. For\nthe identiﬁcation scenario, we computed the rank-1 identiﬁca-\ntion rate to compare the performance of parallel RNs using\nthe triplet loss, that is, 2diff, 3in [15], 2LB* and 2MT*,\nwith each including the PST module as PST-2diff, PST-4in,\nPST-2LB*, and PST-2MT*, respectively. Similar to [5], [15],\nwe additionally created a score-level fusion between a pair of\nnetworks suitable for small and large view variation (e.g., diff\nand 2in) to further improve the recognition accuracy, which\nwas performed by simply averaging the L2 distances output\nfrom the individual CNN models.\nWe conducted the experiments on OU-MVLP because of its\nstatistical reliability in terms of both the number of subjects\nand view variations. Following [15], we also report the mean\nresults for each angular difference based on the full combi-\nnations of four typical view angles, 0\n◦,3 0◦,6 0◦, and 90◦,\nin addition to the total mean results for each method, as shown\nin Table I.\nComparing the results of the RNs with the corresponding\nPSTNs, it is clear that the recognition performance improves\nwhen the proposed PST module is combined with any RN\nfor both the veriﬁcation and identiﬁcation scenarios, which\ndemonstrates the effectiveness of integrating the PST into a\nCNN framework for cross-view gait recognition. The proposed\nPSTN gains more signiﬁcant improvement for the case of\nlarger view variation (e.g., 60\n◦ and 90◦ view difference), for\nTABLE II\nEER (%) OF PST-LB* +PST-2in (BEFORE SLASH ) AND RANK -1\nIDENTIFICATION RATE (%) OF PST-2LB* +PST-4in (AFTER\nSLASH ) FOR EACH INDIVIDUAL COMBINATION OF FOUR\nTYPICAL VIEW ANGLES .P ROBE AND GALLERY\nARE DENOTED BY P AND G, RESPECTIVEL Y\nwhich it is more difﬁcult for the pure RNs to extract spatially\ninvariant features. By registering both the input GEIs from two\nviews into an appropriate intermediate view, the entire PSTNs\ncan be more invariant to large spatial displacements raised by\nconsiderable view variations. Additionally, it is interesting to\nﬁnd that the proposed PST also slightly improves the results\nfor the same view case, where the original GEI pairs with some\nposture change (e.g., looking down in the probe but walking\nnormally in the gallery sequence) are also effectively aligned\nby the PST for better matching.\nOn the other hand, the models with the high-level matching\nstructure (e.g., PST-2in) achieve better performance than the\nlow-level matching structure (e.g., PST-diff) for the case\nof larger view differences, whereas the latter models are\nmore effective than the former in the scenario of smaller\nview variations, which is consistent with the analysis in\nSection III-C1, in addition to insight from [15]. Additionally,\n270 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. 31, NO. 1, JANUARY 2021\nTABLE III\nCOMPARISON OF THE PROPOSED METHOD WITH OTHER\nSTATE-OF -THE -ART METHODS ON OU-MVLP\nit is understandable that the improvement for PST-diff is\nrelatively larger than that for PST-2in because the former is\nmore sensitive to spatial variations and hence easier to improve\nby involving spatial transformation before feature learning.\nFinally, the fusions of high-level and low-level matching\nnetworks all obtain better results than those of using a single\nCNN model. Given that the fusions of PST-LB*+ PST-2in\nand ST-2LB*+ PST-4in yield the best performance for veri-\nﬁcation and identiﬁcation, respectively, we also provide their\nEER/rank-1 identiﬁcation rates for each individual combina-\ntion of four view angles in Table II, and only report the\nresults based on this four related networks for the following\nexperiments.\nE. Comparison on OU-MVLP\nIn this section, the proposed method is compared with\nthe state-of-the-art methods on OU-MVLP. In addition to the\nbenchmark of the generative approach, that is, VTM [4]\n4\nand one typical discriminative approach, that is, LDA [65],\nwe also provide the results of the baseline, that is, direct\nmatching (DM) between the original GEI image pairs, and\n4The exact view angle information is required for VTM to generate a\ntransformation model for each view pair.\nTABLE IV\nCOMPARISON OF THE PROPOSED METHOD AND OTHER\nSTATE-OF -THE -ART METHODS ON OULP.\nGALLERY VIEW IS FIXED TO 85◦\nthose of the state-of-the art CNN-based methods, that is,\nGEINet [14], original LB and MT [5], in addition to the\nresults of diff/2diff, 2in/3in, and the fusion of them,5 which\nare all originally from [15]. The mean EERs and rank-1\nidentiﬁcations for each angular difference in addition to the\ntotal mean over all the combinations of four view angles are\nshown in Table III.\nThe CNN-based methods clearlyoutperform the traditional\ngenerative and discriminative approaches both in terms of ver-\niﬁcation and identiﬁcation scenarios. The networks that apply\ncontrastive loss (e.g., diff) and triplet loss (e.g., 2diff) obtain\nbetter results than those using cross-entropy loss (e.g., original\nLB), particularly for the case of larger view differences.\nAmong all methods using a single CNN model, the pro-\nposed PST-LB*/2LB* and PST-2in/4in yield the best perfor-\nmance for recognition under smalland large view variations,\n5We did not compare with [21] because it takes a set of silhouette images\nas the inputs, where the PST module cannot be directly applied since both\nview registration and phase registration for each frame need to be considered;\ntherefore, we mainly focus on the comparison with GEI-based methods.\nXU et al.: CROSS-VIEW GAIT RECOGNITION USING PSTNs 271\nTABLE V\nCOMPARISON OF THE PROPOSED METHOD AND OTHER STATE-OF -THE -ART METHODS ON CASIA-B. T HE MEAN RESULT OVER ALL\n10 GALLERY VIEWS FOR EACH PROBE VIEW IS SHOWN ,W HERE THE IDENTICAL VIEW IS EXCLUDED FOR THE GALLERY\nrespectively, whereas their fusion achieves the best results for\nall degrees of view angle difference.\nF . Comparison on OULP\nIn addition to the DM, LDA, and CNN-based methods\nmentioned in the previous section, the proposed method was\nalso compared with several additional benchmarks of tradi-\ntional methods: generalized multiview LDA (GMLDA) [66],\nMvDA [43], and GVTM [39] using OULP . The performance\nwas evaluated using the same settings as [39], [43], where the\ngallery view was ﬁxed to 85◦, whereas the probe included all\nfour view angles. We implemented the original LB/MT [5] and\ndiff/2in/2diff/3in [15] for the performance evaluation using this\nprotocol.\nAs shown in Table IV, the proposed PSTN achieves\nmuch better results than the GVTM, which is the generative\napproach using a common non-rigid deformation between\neach pair of different view angles. Therefore, an automat-\nically learned sample-dependent transformation is demon-\nstrated to be more effective than a common one for achieving\nhigh recognition accuracy. Because OULP contains relatively\nsmall view variations (i.e., maximum of 30\n◦), which is\nmore suitable for using networks with a low-level matching\nstructure, those matching the sample pairs at a high level\nachieve relatively worse performance. Generally, most of the\nCNN-based methods obtain almost saturated performance (less\nthan 1% EER and almost 100% rank-1 rate), and the proposed\nPST-LB*+PST-2in and PST-2LB*+PST-4in yield the smallest\nEER and highest rank-1 identiﬁcation rate, respectively.\nG. Comparison on CASIA-B\nWe ﬁnally compared the methods on CASIA-B, which\nis another dataset that includes wide view variation, but a\nsmall number of subjects with low-quality silhouette images.\nConsidering the large amount of network parameters in our\nmodels, we used the protocol containing relatively more train-\ning data, i.e., 74 training subjects, which was also utilized\nin [5] and [16]. To compare with the proposed method,\nwe implemented diff/2in and 2diff/3in using the same proto-\ncol. Additionally, we selected the state-of-the-art traditional\nmethod, which was also evaluated using the same dataset\nsettings, that is, ViDP [48], and several typical networks in [5],\nin addition to a GAN-based method, that is, Multi-task GAN\n(MGAN) [16], for comparison in the identiﬁcation scenario\nonly because of the lack of EER results in their original papers.\nSimilar to the other methods, we evaluated all 11 probe\nviews, whereas the gallery view set that corresponded to each\nprobe view contained the other 10 views, that is, we only\nexcluded the identical view (i.e., the same view) from the\ntotal 11 views. The mean results over all 10 gallery views\nfor each probe view are shown in Table V. Because of\nthe small size of the training set, the improvements of the\nproposed methods are not as large as that on OU-MVLP .\n272 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. 31, NO. 1, JANUARY 2021\nParticularly, the networks with a high-level matching structure\nsometimes cannot outperform those with a low-level matching\nstructure (e.g., PST-4in vs . PST-2LB*) because they may\nfail to extract sufﬁciently invariant features using a small\ntraining set. As a result, the proposed PST-LB* + PST-2in\nachieves the best performance for the veriﬁcation scenario.\nConsidering the small number of test subjects (i.e., 50),\nthe proposed GEI-based PST-2LB* + PST-4in still obtains\ncompetitive results for identiﬁcation scenario compared with\nGEI Ensemble and GEI-temporal Ensemble [5], which is a\nfusion of ﬁve networks using GEI, and a fusion of eight\nnetworks using GEI and/or temporal information, respectively.\nV. C\nONCLUSION\nIn this paper, a PSTN for cross-view gait recognition was\npresented, which combines a PST for spatial transformation\nand an RN for discrimination learning in a uniﬁed frame-\nwork. To the best of our knowledge, this is the ﬁrst time\nthat geometric feature registration has been integrated into\na CNN architecture in the gait recognition community. To\navoid unnecessary large distortion, an input matching pair\nof GEIs from different views were both registered into their\nintermediate view with a non-rigid deformation ﬁeld predicted\nby the PST, and further fed into the subsequent RN to obtain\nthe dissimilarity score. A loss function for optimizing the\nmain recognition task was designed for the PSTN, which\nmade the learned transformation realize a good trade-off\nbetween maintaining inter-subject variations and suppressing\nintra-subject variations that resulted from the view differences\nto achieve optimal recognition accuracy. Experiments on three\npublicly available datasets demonstrated the effectiveness of\nthe proposed method, which achieved state-of-the-art perfor-\nmance among GEI-based methods in both the veriﬁcation and\nidentiﬁcation scenarios.\nOne important future research avenue is the extension\nof the PST module considering both view registration and\nphase registration for each frame, in conjunction with the\nrecent silhouette-based recogn ition networks, such as [21].\nBecause geometric feature registration is also applicable to\nother covariates, such as posture change caused by the walk-\ning/running speed, a future research direction is to evaluate\nthe performance of the proposed PSTN under other covariates\nin gait recognition. Rather than using a simple score-level\nfusion of networks with low-level and high-level matching\nstructures, a uniﬁed model that combines these two structures\nto accommodate different degrees of view variation is also\nworth investigating in the future. Additionally, considering\nthat the sample-dependent transformation predicted by the\nPST may contain discriminative individualities, including the\ntransformation parameters in subsequent feature learning may\nimprove the ﬁnal recognition performance, and this remains\nfuture work.\nA\nCKNOWLEDGMENT\nThe authors would like to thank Dr. Noriko Takemura for\nproviding help and valuable suggestions for the implementa-\ntions and experiments and thank Maxine Garcia, PhD, from\nEdanz Group (www.edanzediting.com/ac) for editing a draft\nof this manuscript.\nREFERENCES\n[1] I. Bouchrika, M. Goffredo, J. Carter, and M. Nixon, “On using gait\nin forensic biometrics,” J. Forensic Sci., vol. 56, no. 4, pp. 882–889,\nMay 2011.\n[2] H. Iwama, D. Muramatsu, Y . Makihara, and Y . Yagi, “Gait veriﬁcation\nsystem for criminal investigation,” IPSJ Trans. Comput. Vis. Appl. ,\nvol. 5, pp. 163–175, Oct. 2013.\n[3] N. Lynnerup and P. K. Larsen, “Gait as evidence,”IET Biometrics,v o l .3 ,\nno. 2, pp. 47–54, Jun. 2014.\n[4] Y . Makihara, R. Sagawa, Y . Mukaigawa, T. Echigo, and Y . Yagi,\n“Gait recognition using a view transformation model in the frequency\ndomain,” inProc. 9th Eur. Conf. Comput. Vis., Graz, Austria, May 2006,\npp. 151–163.\n[5] Z. Wu, Y . Huang, L. Wang, X. Wang, and T. Tan, “A comprehensive\nstudy on cross-view gait based human identiﬁcation with deep CNNs,”\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 39, no. 2, pp. 209–226,\nFeb. 2017.\n[6] M. Altab Hossain, Y . Makihara, J. Wang, and Y . Yagi, “Clothing-\ninvariant gait identiﬁcation using pa rt-based clothing categorization\nand adaptive weight control,” Pattern Recognit. , vol. 43, no. 6,\npp. 2281–2291, Jun. 2010.\n[7] X. Li, Y . Makihara, C. Xu, D. Muramatsu, Y . Yagi, and M. Ren, “Gait\nenergy response function for clothing-invariant gait recognition,” in\nProc. 13th Asian Conf. Comput. Vis. (ACCV), Taipei, Taiwan, Nov. 2016,\npp. 257–272.\n[8] Y . Guan and C.-T. Li, “A robust speed-invariant gait recognition system\nfor walker and runner identiﬁcation,” in Proc. Int. Conf. Biomet-\nrics (ICB), Jun. 2013, pp. 1–8.\n[9] C. Xu, Y . Makihara, X. Li, Y . Yagi, and J. Lu, “Speed invariance vs.\nStability: Cross-speed gait recognition using single-support gait energy\nimage,” inProc. 13th Asian Conf. Comput. Vis. (ACCV), Taipei, Taiwan,\nNov. 2016, pp. 52–67.\n[10] J. Han and B. Bhanu, “Individual recognition using gait energy image,”\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 28, no. 2, pp. 316–322,\nFeb. 2006.\n[11] A. Kale, A. K. R. Chowdhury, and R. Chellappa, “Towards a view\ninvariant gait recognition algorithm,” inProc. IEEE Conf. Adv. Video\nSignal Based Surveill., Jul. 2003, pp. 143–150.\n[12] W. Kusakunniran, Q. Wu, J. Zhang, and H. Li, “Support vector regres-\nsion for multi-view gait recogn ition based on local motion feature\nselection,” in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern\nRecognit., San Francisco, CA, USA, Jun. 2010, pp. 1–8.\n[13] W. Kusakunniran, Q. Wu, J. Zhang, and H. Li, “Gait recognition\nunder various viewing angles based on correlated motion regression,”\nIEEE Trans. Circuits Syst. Video Technol., vol. 22, no. 6, pp. 966–980,\nJun. 2012.\n[14] K. Shiraga, Y . Makihara, D. Muramatsu, T. Echigo, and Y . Yagi,\n“GEINet: View-invariant gait recognition using a convolutional neural\nnetwork,” inProc. Int. Conf. Biometrics (ICB), Jun. 2016, pp. 1–8.\n[15] N. Takemura, Y . Makihara, D. Muramatsu, T. Echigo, and Y . Yagi,\n“On Input/Output architectures for convolutional neural network-based\ncross-view gait recognition,”IEEE Trans. Circuits Syst. Video Technol.,\nvol. 29, no. 9, pp. 2708–2719, Sep. 2019.\n[16] Y . He, J. Zhang, H. Shan, and L. Wang, “Multi-task GANs for\nview-speciﬁc feature learning in gait recognition,” IEEE Trans. Inf.\nForensics Security, vol. 14, no. 1, pp. 102–113, Jan. 2019.\n[17] F. Jean, R. Bergevin, and A. B. Albu, “Computing and evaluating\nview-normalized body part trajectories,” Image Vis. Comput., vol. 27,\nno. 9, pp. 1272–1284, Aug. 2009, doi:10.1016/j.imavis.2008.11.009.\n[18] R. Martín-Félez and T. Xiang, “Uncooperative gait recognition by\nlearning to rank,” Pattern Recognit., vol. 47, no. 12, pp. 3793–3806,\nDec. 2014. [Online]. Available: h ttp://www.sciencedirect.com/\nscience/article/pii/S0031320314002325\n[19] Z. Wu, Y . Huang, and L. Wang, “Learning representative deep features\nfor image set analysis,” IEEE Trans. Multimedia , vol. 17, no. 11,\npp. 1960–1968, Nov. 2015.\n[20] C. Zhang, W. Liu, H. Ma, and H. Fu, “Siamese neural network based gait\nrecognition for human identiﬁcation,” inProc. IEEE Int. Conf. Acoust.,\nSpeech Signal Process. (ICASSP), Mar. 2016, pp. 2832–2836.\n[21] H. Chao, Y . He, J. Zhang, and J. Feng, “GaitSet: Regarding gait as a\nset for cross-view gait recognition,” inProc. AAAI Conf. Artif. Intell.,\nvol. 33, Jul. 2019, pp. 8126–8133.\nXU et al.: CROSS-VIEW GAIT RECOGNITION USING PSTNs 273\n[22] K. Zhang, W. Luo, L. Ma, W. Liu, and H. Li, “Learning\njoint gait representation via quintuplet loss minimization,” in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019,\npp. 4695–4704.\n[23] Z. Zhang et al., “Gait recognition via disentangled representation learn-\ning,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR),\nJun. 2019, pp. 4705–4714.\n[24] X. Zhu, Z. Lei, X. Liu, H. Shi, and S. Z. Li, “Face alignment across\nlarge poses: A 3D solution,” inProc. IEEE Conf. Comput. Vis. Pattern\nRecognit. (CVPR), Jun. 2016, pp. 146–155.\n[25] A. Jourabloo and X. Liu, “Pose-invariant face alignment via CNN-based\ndense 3D model ﬁtting,” Int. J. Comput. Vis. , vol. 124, no. 2,\npp. 187–203, Apr. 2017.\n[26] V . Naresh Boddeti, M.-C. Roh, J. Shin, T. Oguri, and T. Kanade,\n“Face alignment robust to pose, expressions and occlusions,” 2017,\narXiv:1707.05938. [Online]. Available: http://arxiv.org/abs/1707.05938\n[27] M. Jaderberg, K. Simonyan, A. Zisserman, and K. kavukcuoglu,\n“Spatial transformer networks,” in Proc. Adv. Neural Inf. Process.\nSyst., C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and\nR. Garnett, Eds. New York, NY , USA: Curran Associates, 2015,\npp. 2017–2025. [Online]. Available: http://papers.nips.cc/paper/5854-\nspatial-transformer-networks.pdf\n[28] N. S. Detlefsen, O. Freifeld, an d S. Hauberg, “Deep diffeomorphic\ntransformer networks,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit., Jun. 2018, pp. 4403–4412.\n[29] X. Peng, N. Ratha, and S. Pankanti, “Learning face recognition from\nlimited training data using deep neural networks,” in Proc. 23rd Int.\nConf. Pattern Recognit. (ICPR), Dec. 2016, pp. 1442–1447.\n[30] W. Wu, M. Kan, X. Liu, Y . Yang, S. Shan, and X. Chen, “Recursive\nspatial transformer (ReST) for alignment-free face recognition,” inProc.\nIEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017, pp. 3792–3800.\n[31] Y . Shi and A. Jain, “Improving face recognition by exploring local\nfeatures with visual attention,” in Proc. Int. Conf. Biometrics (ICB),\nFeb. 2018, pp. 247–254.\n[32] N. Takemura, Y . Makihara, D. Muramatsu, T. Echigo, and Y . Yagi,\n“Multi-view large population gait dataset and its performance evaluation\nfor cross-view gait recognition,”IPSJ Trans. Comput. Vis. Appl., vol. 10,\nno. 1, pp. 1–14, Feb. 2018.\n[33] H. Iwama, M. Okumura, Y . Makihara, and Y . Yagi, “The OU-ISIR\ngait database comprising the large population dataset and performance\nevaluation of gait recognition,” IEEE Trans. Inf. Forensics Security,\nvol. 7, no. 5, pp. 1511–1521, Oct. 2012.\n[34] S. Yu, D. Tan, and T. Tan, “A framework for evaluating the effect of\nview angle, clothing and carrying condition on gait recognition,” inProc.\n18th Int. Conf. Pattern Recognit. (ICPR), Hong Kong, vol. 4, Aug. 2006,\npp. 441–444.\n[35] G. Zhao, G. Liu, H. Li, and M. Pietikainen, “3D gait recognition\nusing multiple cameras,” inProc. IEEE Int. Conf. Autom. Face Gesture\nRecognit., Apr. 2006, pp. 529–534.\n[36] H.-D. Yang and S.-W. Lee, “Reconstruction of 3D human body pose for\ngait recognition,” inProc. IAPR Int. Conf. Biometrics 2006, Jan. 2006,\npp. 619–625.\n[37] G. Shakhnarovich, L. Lee, and T. D arrell, “Integrated face and gait\nrecognition from multiple views,” inProc. the IEEE Comput. Soc. Conf.\nComput. Vis. Pattern Recognit.. CVPR, vol. 1, Dec. 2001, pp. I–I.\n[38] M. Goffredo, I. Bouchrika, J. N. Carter, and M. S. Nixon, “Self-\ncalibrating view-invariant gait biometrics,” IEEE Trans. Syst., Man,\nCybern. B. Cybern., vol. 40, no. 4, pp. 997–1008, Aug. 2010.\n[39] H. El-Alfy, C. Xu, Y . Makihara, D. Muramatsu, and Y . Yagi, “A geo-\nmetric view transformation model using free-form deformation for\ncross-view gait recognition,” in Proc. 4th IAPR Asian Conf. Pattern\nRecognit. (ACPR)\n, Nov. 2017.\n[40] W. Kusakunniran, Q. Wu, J. Zhang, and H. Li, “Cross-view and\nmulti-view gait recognitions based on view transformation model\nusing multi-layer perceptron,” Pattern Recognit. Lett., vol. 33, no. 7,\npp. 882–889, May 2012.\n[41] D. Muramatsu, A. Shiraishi, Y .Makihara, M. Z. Uddin, and Y . Yagi,\n“Gait-based person recognition using arbitrary view transformation\nmodel,” IEEE Trans. Image Process. , vol. 24, no. 1, pp. 140–154,\nJan. 2015.\n[42] J. Lu and Y .-P. Tan, “Uncorrelated discriminant simplex analysis for\nview-invariant gait signal computing,”Pattern Recognit. Lett., vol. 31,\nno. 5, pp. 382–393, Apr. 2010.\n[43] A. Mansur, Y . Makihara, D. Muramatsu, and Y . Yagi, “Cross-view\ngait recognition using view-dependent discriminative analysis,” inProc.\nIEEE Int. Joint Conf. Biometrics, Sep. 2014, pp. 1–8.\n[44] Z. Zhang, J. Chen, Q. Wu, and L. Shao, “GII representation-based\ncross-view gait recognition by discriminative projection with list-wise\nconstraints,” IEEE Trans. Cybern., vol. 48, no. 10, pp. 2935–2947,\nOct. 2018.\n[45] K. Bashir, T. Xiang, and S. Gong, “Cross view gait recognition using\ncorrelation strength,” inProc. Procedings Brit. Mach. Vis. Conf., 2010,\npp. 1–11.\n[46] W. Kusakunniran, Q. Wu, J. Zhang, H. Li, and L. Wang, “Recognizing\ngaits across views through correlated motion co-clustering,”IEEE Trans.\nImage Process., vol. 23, no. 2, pp. 696–709, Feb. 2014.\n[47] X. Xing, K. Wang, T. Yan, and Z. Lv, “Complete canonical correla-\ntion analysis with application to multi-view gait recognition,” Pattern\nRecognit., vol. 50, pp. 107–117, Feb. 2016.\n[48] M. Hu, Y . Wang, Z. Zhang, J. J. Little, and D. Huang, “View-invariant\ndiscriminative projection for multi-view gait-based human identiﬁca-\ntion,” IEEE Trans. Inf. Forensics Security, vol. 8, no. 12, pp. 2034–2045,\nDec. 2013.\n[49] T. Wolf, M. Babaee, and G. Rigoll,“Multi-view gait recognition using\n3D convolutional neural networks,” in Proc. IEEE Int. Conf. Image\nProcess. (ICIP), Sep. 2016, pp. 4165–4169.\n[50] S. Yu, H. Chen, E. B. G. Reyes, and N. Poh, “GaitGAN: Invariant gait\nfeature extraction using generative adversarial networks,” inProc. IEEE\nConf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), Jul. 2017,\npp. 532–539.\n[51] A. Krizhevsky, I. Sutskever, andG. E. Hinton, “ImageNet classiﬁcation\nwith deep convolutional neural networks,”Commun. ACM, vol. 60, no. 6,\npp. 84–90, May 2017.\n[52] Y . Zhong, J. Chen, and B. Huang, “Toward end-to-end face recognition\nthrough alignment learning,”IEEE Signal Process. Lett., vol. 24, no. 8,\npp. 1213–1217, Aug. 2017.\n[53] Y . Makihara and Y . Yagi, “Silhouette extraction based on iterative\nspatio-temporal local color transformation and graph-cut segmentation,”\nin Proc. 19th Int. Conf. Pattern Recognit., Tampa, FL, USA, Dec. 2008,\npp. 1–4.\n[54] G. Lin, A. Milan, C. Shen, and I. Reid, “ReﬁneNet: Multi-path\nreﬁnement networks for high-resolution semantic segmentation,” in\nProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017,\npp. 1925–1934.\n[55] P. Krähenbühl and V . Koltun, “Efﬁcient inference in fully connected\nCRFs with Gaussian edge potentials,” inProc. Adv. Neural Inf. Process.\nSyst., J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and\nK. Q. Weinberger, Eds. New York, NY , USA: Curran Associates, 2011,\npp. 109–117.\n[56] T. W. Sederberg and S. R. Parry, “Free-form deformation of solid\ngeometric models,” ACM SIGGRAPH Comput. Graph., vol. 20, no. 4,\npp. 151–160, Aug. 1986, doi:10.1145/15886.15903.\n[57] Y . Makihara, D. Adachi, C. Xu, and Y . Yagi, “Gait recognition by\ndeformable registration,” in\nProc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit. Workshops (CVPRW), Jun. 2018, pp. 561–571.\n[58] J. Chaki and N. Dey, A Beginner’s Guide to Image Preprocess-\ning Techniques (Intelligent Signal Processing and Data Analysis).\nBoca Raton, FL, USA: CRC Press, 2018. [Online]. Available:\nhttps://books.google.co.jp/books?id=Dfp0DwAAQBAJ\n[59] V . Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted\nBoltzmann machines,” in Proc. 27th Int. Conf. Int. Conf. Mach.\nLearn. (ICML). Madison, WI, USA: Omnipress, 2010, pp. 807–814.\n[Online]. Available: http://dl.acm.org/citation.cfm?id=3104322.3104425\n[60] A. Karpathy, G. Toderici, S. S hetty, T. Leung, R. Sukthankar, and\nL. Fei-Fei, “Large-scale video classiﬁcation with convolutional neural\nnetworks,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. ,\nJun. 2014, pp. 1725–1732.\n[61] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, “Dropout: A simpleway to prevent neural networks\nfrom overﬁtting,” J .M a c h .L e a r n .R e s ., vol. 15, pp. 1929–1958,\nJun. 2014.\n[62] R. Hadsell, S. Chopra, and Y . LeCun, “Dimensionality reduction by\nlearning an invariant mapping,” inProc. IEEE Comput. Soc. Conf. Com-\nput. Vis. Pattern Recognit. (CVPR), vol. 2, Jun. 2006, pp. 1735–1742.\n[63] J. Wang et al., “Learning ﬁne-grained image similarity with deep rank-\ning,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.Washington,\nDC, USA: IEEE Computer Society, Jun. 2014, pp. 1386–1393,\ndoi: 10.1109/CVPR.2014.180.\n[64] Y . Makihara, R. Sagawa, Y . Mukaigawa, T. Echigo, and Y . Yagi, “Which\nreference view is effective for gait identiﬁcation using a view trans-\nformation model?” in Proc. IEEE Comput. Soc. Workshop Biometrics,\nNew York, NY , USA, Jun. 2006.\n274 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY , VOL. 31, NO. 1, JANUARY 2021\n[65] N. Otsu, “Optimal linear and nonlinear solutions for least-square dis-\ncriminant feature extraction,” inProc. 6th Int. Conf. Pattern Recognit.,\n1982, pp. 557–560.\n[66] A. Sharma, A. Kumar, H. Daume, and D. W. Jacobs, “Generalized\nmultiview analysis: A discriminative latent space,” inProc. IEEE Conf.\nComput. Vis. Pattern Recognit., Jun. 2012, pp. 2160–2167.\n[67] X. Glorot and Y . Bengio, “Understanding the difﬁculty of training deep\nfeedforward neural networks,” in Proc. 13th Int. Conf. Artif. Intell.\nStatist. (AISTATS), vol. 9, May 2010, pp. 249–256.\n[68] L. Bottou and O. Bousquet, “The tradeoffs of large scale learning,”\nin Proc. 20th Int. Conf. Neural Inf. Process. Syst. (NIPS) , 2007,\npp. 161–168.\nChi Xu received the B.S. degree in computer\nscience and technology from the Nanjing Univer-\nsity of Science and Tec hnology (NUST), China,\nin 2012, where she is currently pursuing the Ph.D.\ndegree in pattern recognition and intelligent sys-\ntem. Since January 2016, she has been a Visiting\nResearcher with the Institute of Scientiﬁc and Indus-\ntrial Research, Osaka University, Japan. Her research\ninterests are gait recognition, machine learning, and\nimage processing.\nYasushi Makihara received the B.S., M.S., and\nPh.D. degrees in engineering from Osaka Univer-\nsity in 2001, 2002, and 2005, respectively. He was\nappointed specially as an Assistant Professor (full-\ntime), an Assistant Professor, and an Associate Pro-\nfessor from the Institute of Scientiﬁc and Industrial\nResearch, Osaka University, in 2005, 2006, and\n2014, respectively, where he is currently a Professor\nwith the Institute for Advanced Co-Creation Studies.\nHis research interests are computer vision, pattern\nrecognition, and image processing, including gait\nrecognition, pedestrian detection, morphing, and temporal super resolution.\nHe is a member of IPSJ, IEICE, RSJ, and JSME. He has obtained several\nhonors and awards, including the Second International Workshop on Bio-\nmetrics and Forensics (IWBF 2014), the IAPR Best Paper Award, the 9th\nIAPR International Conference on Biometrics (ICB 2016), the Honorable\nMention Paper Award, and the Commendation for Science and Technology\nby the Minister of Education, Culture, Sports, Science and Technology, Prizes\nfor Science and Technology, Research Category, in 2014. He has served as\nan Associate Editor-in-Chief for theIEICE Transactions on Information and\nSystems, an Associate Editor for theIPSJ Transactions on Computer Vision\nand Applications (CV A), a Program Co-Chair for the 4th Asian Conference\non Pattern Recognition (ACPR 2017), and an Area Chair for ICCV 2019,\nCVPR 2020, and ECCV 2020.\nXiang Li received the B.S. degree in computer\nscience and technology from the Nanjing Univer-\nsity of Science and Tec hnology (NUST), China,\nin 2012, where he is currently pursuing the Ph.D.\ndegree. Since January 2016, he has been a Visiting\nResearcher with the Institute of Scientiﬁc and Indus-\ntrial Research, Osaka University, Japan. His research\ninterests are gait recognition, image processing, and\nmachine learning.\nYasushi Yagi (Member, IEEE) received the Ph.D.\ndegree from Osaka University in 1991. In 1985,\nhe joined the Product Development Laboratory, Mit-\nsubishi Electric Corporation, where he worked on\nrobotics and inspections. He became a Research\nAssociate in 1990, a Lecturer in 1993, an Asso-\nciate Professor in 1996, and a Professor in 2003 at\nOsaka University. He was also the Director of the\nInstitute of Scientiﬁc and Industrial Research, Osaka\nUniversity, from 2012 to 2015, where he was the\nExecutive Vice President from 2015 to 2019. He is\ncurrently a Professor with the Institute of Scientiﬁc and Industrial Research,\nOsaka University. His research interests are computer vision, medical engi-\nneering, and robotics. He is a fellow of IPSJ and a member of IEICE\nand RSJ. He was awarded the ACM VRST2003 Honorable Mention Award,\nthe IEEE ROBIO2006 Finalist of T. J. Tan Best Paper in Robotics, the IEEE\nICRA2008 Finalist for Best Vision Paper, the MIRU2008 Nagao Award, and\nthe PSIVT2010 Best Paper Award. International conferences for which he has\nserved as the Chair include: FG1998 asthe Financial Chair, OMINVIS2003\nas the Organizing Chair ROBIO2006as the Program Co-Chair, ACCV2007 as\nthe Program Chair, PSVIT2009 as the Financial Chair, ICRA2009 as the\nTechnical Visit Chair, ACCV2009 as the General Chair, ACPR2011 as the\nProgram Co-Chair, and ACPR2013 as the General Chair. He has also served\nas an Editor for the IEEE ICRA Conference Editorial Board from 2007 to\n2011. He is also an Editorial Member of IJCV and the Editor-in-Chief of\nIPSJ Transactions on Computer Vision and Applications.\nJianfeng Lu received the B.S. degree in computer\nsoftware and the M.S. and Ph.D. degrees in pattern\nrecognition and intelligent systems from the Nanjing\nUniversity of Science and Technology, China. He is\ncurrently a Professor with the Nanjing University\nof Science and Technology. His research interests\ninclude image processing, pattern recognition, and\ndata mining.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7501859664916992
    },
    {
      "name": "Pairwise comparison",
      "score": 0.729363739490509
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6269368529319763
    },
    {
      "name": "Transformer",
      "score": 0.6228404641151428
    },
    {
      "name": "Gait",
      "score": 0.5898774266242981
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5761780142784119
    },
    {
      "name": "Matching (statistics)",
      "score": 0.5189840197563171
    },
    {
      "name": "Feature extraction",
      "score": 0.49836301803588867
    },
    {
      "name": "Computer vision",
      "score": 0.40765783190727234
    },
    {
      "name": "Engineering",
      "score": 0.12978091835975647
    },
    {
      "name": "Mathematics",
      "score": 0.1219601035118103
    },
    {
      "name": "Voltage",
      "score": 0.0775679349899292
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Physiology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}