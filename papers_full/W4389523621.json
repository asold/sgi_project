{
  "title": "InsightPilot: An LLM-Empowered Automated Data Exploration System",
  "url": "https://openalex.org/W4389523621",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2744477633",
      "name": "Ma, Pingchuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108875397",
      "name": "Ding Rui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1881628833",
      "name": "Wang Shuai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105084222",
      "name": "Shi Han",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1642755225",
      "name": "Zhang Dong-mei",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4381326944",
    "https://openalex.org/W4385270135",
    "https://openalex.org/W2014267653",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4378770815",
    "https://openalex.org/W2508656285",
    "https://openalex.org/W2513272121",
    "https://openalex.org/W4303649020",
    "https://openalex.org/W3174775648",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W2903606913",
    "https://openalex.org/W4226516675",
    "https://openalex.org/W3139497941",
    "https://openalex.org/W1995887163",
    "https://openalex.org/W4289494028",
    "https://openalex.org/W4387355353",
    "https://openalex.org/W3031617143"
  ],
  "abstract": "Exploring data is crucial in data analysis, as it helps users understand and interpret the data more effectively. However, performing effective data exploration requires in-depth knowledge of the dataset, the user intent and expertise in data analysis techniques. Not being familiar with either can create obstacles that make the process time-consuming and overwhelming. To address this issue, we introduce InsightPilot, an LLM (Large Language Model)-based, automated data exploration system designed to simplify the data exploration process. InsightPilot features a set of carefully designed analysis actions that streamline the data exploration process. Given a natural language question, InsightPilot collaborates with the LLM to issue a sequence of analysis actions, explore the data and generate insights. We demonstrate the effectiveness of InsightPilot in a user study and a case study, showing how it can help users gain valuable insights from their datasets.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 346‚Äì352\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nInsightPilot: An LLM-Empowered Automated Data Exploration System\nPingchuan Ma\nHKUST\nRui Ding‚Ä†\nMicrosoft Research\nShuai Wang‚Ä†\nHKUST\nShi Han\nMicrosoft Research\nDongmei Zhang\nMicrosoft Research\nAbstract\nExploring data is crucial in data analysis, as it\nhelps users understand and interpret the data\nmore effectively. However, performing effec-\ntive data exploration requires in-depth knowl-\nedge of the dataset, the user intent and expertise\nin data analysis techniques. Not being familiar\nwith either can create obstacles that make the\nprocess time-consuming and overwhelming.\nTo address this issue, we introduce InsightPi-\nlot, an LLM (Large Language Model)-based,\nautomated data exploration system designed to\nsimplify the data exploration process. Insight-\nPilot features a set of carefully designed analy-\nsis actions that streamline the data exploration\nprocess. Given a natural language question, In-\nsightPilot collaborates with the LLM to issue a\nsequence of analysis actions, explore the data\nand generate insights. We demonstrate the ef-\nfectiveness of InsightPilot in a user study and a\ncase study, showing how it can help users gain\nvaluable insights from their datasets.\n1 Introduction\nExploratory data analysis (EDA) is a demand-\ning task that extracts meaningful insights from\ndata (Komorowski et al., 2016; Jebb et al., 2017;\nDevore, 2007). Data exploration is a critical step\nin data analysis. In general, it involves a series of\ndata analysis operations, such as filtering, sorting,\nand grouping, to discover patterns in data. Usu-\nally, the process is iterative and interactive, and\nthe user needs to manually explore the data back-\nand-forth to gain insights. This process is often\ntime-consuming and requires considerable domain\nknowledge and expertise. Below, we present an\nexample to illustrate a data exploration process.\nExample. Using a student performance dataset\nfrom multiple schools (Figure 1), an education an-\nalyst, Alice, conducts EDA to comprehend trends\nin math performance. After considerable manual\ndata filtering and sorting, Alice captures an upward\ntrend by plotting math scores over time. Alice then\nputs in more effort into manual data filtering for\ncomparing student performance across schools A,\nB, and C. Finally, she observes that both schools A\nand B illustrate an increasing trend while school C\nhas an outlier in 2020. Alice is curious and decides\nto investigate the outlier. She spends even more\ntime exploring the data back-and-forth, filtering\nand grouping by various variables until she finally\nfinds that when excluding ‚Äútake-home‚Äù exams, the\noutlier for school C in 2020 is no longer present.\nAlice notes this finding and concludes that the out-\nlier is caused by a policy change of the exam form\nin school C in 2020.\nAlice‚Äôs manual data sifting for insights is effort-\nintensive and time-consuming, highlighting the\nneed for an efficient automated data exploration\nsystem to simplify the process.\nExisting Solutions. To date, a number of data ex-\nploration systems have been proposed in the data\nmanagement and data mining community (Bar El\net al., 2020; Chanson et al., 2022; Personnaz et al.,\n2021; Cao et al., 2023). In general, these systems\nleverage a heuristic score function to identify the\n‚Äúbest‚Äù data exploration sequence (a series of data\nanalysis operations). While these systems show po-\ntential, they exhibit key limitations. ‚ûä User Intent\nIgnorance: Existing tools are designed for gen-\neral exploration and often fail to incorporate user\nintent. For instance, an analyst may be interested\nin understanding the economics-related factors but\nreceive insights about demographics. ‚ûã Dataset\nCharacteristic Ignorance: They overlook dataset\ncharacteristics, often providing irrelevant insights.\nFor instance, in a flight delay dataset, a correlation\nbetween flight delays and weather might yield in-\nsights but irrelevant factors like time and weather\ndoes not make sense in the context. They fall short\nin delivering a direct answers to the user question.\nRecently, large language models (LLM) have\nshown promising potential in understanding user\n346\nStudent School Score Year Subject Exam Form\nBob A 87 2018 Math Multi-Choice\nTom B 67 2018 History Open-book\n‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶\n‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶\n‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶\nraw data\n70\n75\n80\n85\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\nScore\n70\n75\n80\n85\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\nScore\n70\n75\n80\n85\nAll Exc. Take-home Exam\nScore\n2019\n2020\nSubject=Math has an increasing trend \nover years.\nWhen Subject=Math, most School have an \nincreasing trend while School=C has an \noutlier in Year=2020.\nExam Form=Take-home explains the outlier in Year=2020.\nFigure 1: An example of data exploration.\nintent and generating actions to achieve user-\nspecified goals (Yao et al., 2022). In this regard,\nwe anticipate that LLM can be leveraged to drive\nthe data exploration process. However, there are\nseveral challenges that impede the adoption. ‚ûå\nHallucination: Due to the infamous hallucination\nissue (Ji et al., 2023), LLMs often generate un-\nreliable contents and are thus not mature for pro-\nduction use. ‚ûç Overwhelming Context Window:\nA dataset may contain millions of cells, which is\noverwhelming for LLMs to process.\nOur Solution. To address these challenges, we\npropose InsightPilot, a system that automates data\nexploration using LLMs. This system facilitates\nexploration through the synergy of an LLM and an\ninsight engine, which integrates three production-\nquality insight discovery tools: QuickInsight (Ding\net al., 2019), MetaInsight (Ma et al., 2021), and\nXInsight (Ma et al., 2023) (detailed in Sec. 5.1).\nThese tools offers a unified insight representation,\nenabling the LLM to engage coherently. The in-\nsight engine provides the LLM with accurate and\nreliable insights, avoiding hallucination. Further-\nmore, the insight engine presents a concise abstrac-\ntion of the dataset to alleviate the overwhelming\ncontext window issue. In InsightPilot, users input\nhigh-level queries, like ‚Äú show me the interesting\ntrend in mathematics scores for students‚Äù. Then,\nthe InsightPilot employ an LLM to interact with\nthe insight engine using a set of carefully designed\nanalysis actions to streamline common data explo-\nration tasks. These actions serve as a coherent\ntransition to chain up insights and generate a data\nexploration sequence to answer the user‚Äôs question.\nFinally, InsightPilot summarizes the results using\nnatural language together with charts that are un-\nderstandable to non-technical users.\nContributions. In summary, we make the follow-\ning contributions: We propose InsightPilot, an au-\ntomated system for data exploration that employs\nLLMs to drive the exploration process.InsightPilot\nstreamlines the exploration process by interacting\nwith an insight engine using a set of carefully de-\nsigned analysis actions. We conduct a user study\nand a case study to demonstrate the effectiveness\nof InsightPilot in real-world scenarios.\n2 Related Work\nText-to-SQL. To date, text-to-SQL is the most pop-\nular approach to enabling natural language inter-\nface to database. It translates users‚Äô utterances into\nSQL queries for relational databases and has been\nstudied by both database and NLP communities for\nseveral decades (Yu et al., 2018; Kim et al., 2020;\nMa and Wang, 2022). Recent studies have shown\nthat with LLM, text-to-SQL can now be augmented\nto support non-SQL enquiries such as entity extrac-\ntion (Cheng et al., 2023). However, in EDA, users‚Äô\nintents are often more complex than simple SQL\nqueries. EDA generally involves more complicated\nuser intents and goes beyond the expressiveness\nof basic SQL queries. We take a step further by\nusing an LLM and analysis actions in InsightPilot,\nto produce natural and coherent data exploration\nsequences that accurately address users‚Äô questions.\nThis innovation provides an important complement\nto the existing text-to-SQL approach.\nAnalytics Model in OLAP. Traditionally, users\ninteract with OLAP (online analytical process-\ning) systems with a set of pre-defined operators\n(e.g., drill-down and roll-up) (Vassiliadis and Sel-\nlis, 1999). Recently, there is a surge of interest\nin developing analytics models with higher-level\nabstraction and automation to facilitate complex\nOLAP needs (Vassiliadis et al., 2019). In Insight-\nPilot, we use ‚Äúanalysis actions‚Äù to describe such\nhigh-level abstractions.\n3 Preliminaries\nIn this section, we introduce the preliminaries of\nexploratory data analysis.\nData Model. Let D := {X1, ¬∑¬∑¬∑ , Xn}represents\nmulti-dimensional data comprising n attributes,\nwhere each attribute Xi is either a dimension or\na measure. A dimension Xi is a categorical at-\ntribute that can be used to group data. A measure\nXi is a numerical attribute that can be used to per-\nform aggregation operations. In InsightPilot, filter\nis the basic unit of data operations. Given a multi-\ndimensional data D and a dimension X, a filter\npi = X = xi (e.g., ‚ÄúSubject=Math‚Äù) implies an\nequality assertion to X such that the value of X\n347\nwill equal xi. A subspace is a conjunction of fil-\nters on disjoint dimensions (e.g., ‚ÄúSubject = Math\nAND Year = 2019‚Äù). A breakdown dimension\nis the dimension where the group-by operation is\nperformed. Given a measure M, users may per-\nform aggregation operations (such as SUM and AVG\nin SQL) over some records for M.\nAnalysis Entity (AE). An AE is defined as a 3-\ntuple AE := ‚ü®agg(M), S, B‚ü©, where M is a mea-\nsure with an aggregation function agg applied, S is\na subspace, andB is a breakdown dimension. It can\nbe interpreted as an equivalent SQL query that per-\nforms aggregation operations over a set of records\nfor the measure M in a subspace S, grouped by the\nbreakdown dimension B. For instance, the AE\n‚ü®AVG(Score), Subject = Math, Year‚ü©is equiva-\nlent to the SQL query SELECT AVG(Score) FROM Table\nWHERE Subject = Math GROUP BY Year.\nData Insight. A basic data insight is represented as\na 3-tuple ‚ü®AE, Type, Property‚ü©. Here, AE denotes\nan analysis entity, Type specifies the insight‚Äôs kind\n(e.g., trend, outlier), and Property encapsulates ad-\nditional outputs from insight mining algorithms,\nlike extreme points for a unimodality pattern. We\ncategorize insights into basic insights, directly de-\nrived from data (e.g., trend insights), and com-\npound insights, which build upon other insights.\nA meta-insight, for instance, summarizes several\nsimilar insights (e.g., sales trends across various\ncities). Both insight categories can be expressed as\nthe 3-tuple format. Throughout this paper, \"insight\"\npertains to both types. We have crafted templates to\narticulate these insights in user-friendly language,\naccompanied by visualizations.\n4 Problem Definition\nIn this section, we define the problem of generat-\ning a sequence of data insights to address users‚Äô\nanalysis intents.\nAnalysis Actions. An analysis action is defined as a\ntransition from one data insight to several other data\ninsights (which may also be no insight or solely\none insight). In our context, an action represents\na reasonable data analysis operation with the ex-\nisting knowledge (e.g., user question, dataset, and\nexplored insights). This is defined as a function\nAA : Insight ‚ÜíInsight‚àó, where Insight‚àódenotes\nthe set of all possible data insights. In addition, we\ndefine two special analysis actions, namely, AAinit\nand AAback. AAinit that takes a dataset as input and\nprovides a set of initial insights and AAback that\nbacktracks to the last state and returns the insights\ngenerated by the preceding analysis action.\nData Exploration Sequence. A data exploration\nsequence is defined as a series of data insights in-\nterconnected by analysis actions. It is represented\nas S= ‚ü®AAinit, Insight1, AA1, ¬∑¬∑¬∑ , AAn, ‚ä•‚ü©, where\nInsighti is a data insight picked from the output\nof the preceding analysis action AAi‚àí1, AAi is an\nanalysis action, and ‚ä•symbolizes the termination.\nEach analysis action AAi takes the preceding data\ninsight Insighti‚àí1 as input and produces multiple\ninsights to be picked for the next analysis action.\nGenerating Final Answer. Given a user question\nQ and the data exploration sequence S, we define\nthe problem of generating a final answer as a func-\ntion FA : Q √óflatten(S) ‚ÜíA, where A is the\nfinal answer to the user question Q and flatten(S)\nis the top-k of all data insights generated in the\ndata exploration sequence (including unpicked in-\nsights). The final answer A can be obtained by any\ndocument QA techniques over flatten(S).\nApplication Scope. In InsightPilot, we focus on\nthe class of data analysis tasks are expressed as\nfuzzy and high-level tasks. These are tasks where\nthe user‚Äôs intent is not explicitly clear or the analy-\nsis objective is often complex, requiring multiple\nsteps to fully address. For example, a fuzzy high-\nlevel task could be ‚Äú Analyze sales performance\nover the past year.‚Äù The exact steps required to\nanswer this question are not specified, and a vari-\nety of different analysis actions and insights may\nbe required to provide a comprehensive answer.\nNote that while we focus on this specific class of\nquestions, the unique feature of InsightPilot can be\nintegrated with other systems (e.g., text-to-SQL)\nand support diverse data analysis scenarios.\n5 InsightPilot Design\nFigure 2 depicts the overview ofInsightPilot. Over-\nall, InsightPilot constitutes a pipeline of three com-\nponents: (1) a user interface that enables users to\nissue inquiries in natural language, and also depicts\nanalysis results in texts and charts; (2) a LLM that\ndrives the exploration process by selecting appro-\npriate insights and analysis intents based on the\ncontext (e.g., user question, dataset domain knowl-\nedge, and current exploration state); (3) an insight\nengine that executes analysis action, generates in-\nsights, and presents results in natural language.\nWorking Example. In Figure 2, InsightPilot is il-\nlustrated using the example from Figure 1. A user\n348\nUser\nUser Inquiry\n Report\nInsightPilot\nSchool A has #1 ScoreIncreasing Math Score‚Ä¶\n UnderstandSummarize\nInsight Engine\nLLMInsight Selectionùê¥ùê¥!\"!# Analysis ActionSelectionSummary by SchoolSummary by Exam Form\nActionExecution\nUnderstandExplain\nExplanation by Exam Form‚Ä¶\nActionExecution\nInsight Selection ‚Ä¶Analysis ActionSelection\nFigure 2: Pipeline of InsightPilot. Selected insights/analysis actions are highlighted in red.\nposes a query: ‚Äúshow me any interesting trend in\nmathematics scores for students‚Äù. The insight en-\ngine then generates initial insights with AAinit. One\nsuch insight might be ‚Äú School A has the Rank#1\naverage score.‚Äù Based on the user‚Äôs question, the\nLLM identifies the most pertinent insight, such\nas ‚Äúthe mathematics scores of students have been\nincreasing over time‚Äù, using predefined prompts\n(refer Sec. 5.2). After choosing the insight, we pre-\npare potential analysis actions and the LLM selects\nan appropriate one, in this case, compare (details in\nSec. 5.1). Executing this action, the insight engine\nsummarizes the math scores trend across schools.\nIt observes: ‚Äúmost schools show rising math scores,\nexcept for an outlier in 2020 for school C.‚Äù To delve\ndeeper, the LLM continues to select insights and\nactions, eventually querying the engine to ‚Äúexplain\nthe 2020 outlier for school C‚Äù.\nInteractions continue until the LLM completes\nits exploration (i.e., choose ‚ä•as the next action)\nor hits the token size limit. Once done, insights\nare translated to natural language for the prompt.\nGiven the typically large number of insights, in-\nsight ranking becomes crucial. The insight engine\nthen presents the top-K insights, which the LLM\ncondenses into a coherent report. This report and\nthe top-K insights (in the form of charts) are then\ndisplayed to the user via the interface.\n5.1 Analysis Action\nEvery time the LLM selects an insight and an anal-\nysis action, the insight engine will execute the\naction and generate new insights. Currently, we\nhave prepared four analysis actions for the LLM\nto select: understand, summarize, compare, and\nexplain. These actions are in accordance with\nthree insight discovery solutions, namely Quick-\nInsight (Ding et al., 2019) for understand, MetaIn-\nsight (Ma et al., 2021) for summarize and compare,\nand XInsight (Ma et al., 2023) for explain. We now\nelaborate on the design of these analysis actions.\n‚ûÄ Understand. This action is designed to help\nusers understand the high-level patterns in the data.\nIn particular, it attempts to enumerate all possi-\nble AEs (see definition in Sec. 3) under the AE\nof input insight, applies the insight mining algo-\nrithm (e.g., trend detection) on each AE to iden-\ntify basic insight and transforms them into human-\nunderstandable natural language.\n‚ûÅ Summarize. This action aims to view an input\ninsight from various angles. Starting with a basic\ninsight (like a trend), it employs a specific insight\nmining algorithm on the AEs of the input to verify\nthe presence of the primary insight type and prop-\nerty (e.g., an increasing trend) across each AE. If\nconsistent across all AEs, the output is ‚Äúthe basic in-\nsight type and property are universal among AEs‚Äù.\nIf not, it‚Äôs ‚Äúthe basic insight type and property are\npresent in some AEs‚Äù. Using the example insight\nof an ‚Äúincreasing math score trend‚Äù, the outcome\ncould be ‚Äúmost schools show rising math scores,\nbarring school C‚Äù or ‚Äúin most subjects, scores have\nrisen over time‚Äù. These compound insights can be\nfurther explored by subsequent analysis actions.\n‚ûÇ Compare. This action shares a similar design\nwith summarize. It is designed to compare the input\ninsight from different neighbors. In particular, it\nstarts with a basic insight (e.g., a trend) and then ap-\nplies the particular insight mining algorithm on the\nneighboring AEs. Given an input insight ‚Äú the in-\ncreasing trend of the mathematics scores in school\nA‚Äù, it will generate a comparison of the trend re-\ngarding different schools, e.g., ‚Äú the mathematics\nscores of students in school A and B have been\nincreasing over time, except school C.‚Äù This com-\nparison is also represented by a compound insight.\n‚ûÉ Explain. This action is designed to explain the\ninsight that reveals a difference or an outlier in the\ndata. It supports both basic insights (e.g., a outlier\ninsight or a change point insight) and compound in-\nsights (e.g., a summary insight with an exceptional\ncase). Given the input insight with difference or\noutlier, it will identify a subspace that is responsi-\nble for the outcome using causal inference and then\nconstitute a new compound insight to encode the\ncause. For example, given ‚Äúthe outlier of the mathe-\nmatics scores in 2020 for school C‚Äù, it will identify\nexplanations such as ‚Äúthe difference on the mathe-\n349\nmatics scores of students in school C between 2019\nand 2020 is caused by Exam Form=Take-home.\nWhen excluding Exam Form=Take-home, 2020 is\nno longer an outlier.‚Äù\n5.2 Prompt Engineering\nTo deliver a self-contained presentation, we de-\nscribe the design of our prompt engineering tech-\nniques. In general, any agent-based prompt tem-\nplate (e.g., ReACT (Yao et al., 2022)) can be used\nto instantiate InsightPilot. We prepare separate\nprompt templates for different stages of Insight-\nPilot for selecting insights and analysis actions,\nand for finalizing the answer. Routine instructions\nare used in the prompt to improve the usefulness,\nclarity, and coherence of the LLM outputs.\n5.3 Insight Ranking\nConsider the final answer generation phase detailed\nin Sec. 4. The insight engine often yields an over-\nwhelming number of insights, sometimes reach-\ning hundreds within a single exploration sequence.\nGiven the LLM‚Äôs capacity, it is infeasible to process\nall these insights. Thus, we prioritize by extract-\ning the top- K insights. Notably, the value of K\nsurpasses the count of selected insights in the explo-\nration sequence (Insight ‚ààS), ensuring the chosen\ninsights are encompassed within the top-K. Next,\nwe present three schemes to rank top-K insights.\nRedundant Insight Elimination. Trivial insights\ncan be eliminated if they are entailed by a more\ninformative insight. For example, if three insight\n#1: ‚ÄúSchool=A has the highest mathematics score,‚Äù\n#2: ‚ÄúSchool=A has the highest score in 2022,‚Äù and\n#3 ‚ÄúSchool=A has the highest mathematics score\nin 2022‚Äù are generated, we can exclude the third in-\nsight since it trivially derives from the first two. En-\nlightened by this observation, we propose to elimi-\nnate insights that are entailed by other insights. In\nparticular, such elimination is achieved by identify-\ning insights by looping through every possible pair\nof insights and checking if there exists a dominator\nto make one of them trivial.\nSemantic Similarity-based Elimination. To fur-\nther decrease the number of insights for the LLM\nto handle, we employ semantic similarity. We iden-\ntify the top-K‚Ä≤(K‚Ä≤‚â´K) insights most relevant to\nthe user‚Äôs question using an embedding model that\ntransforms text inputs into vector representations.\nBy calculating the cosine similarity between each\ninsight‚Äôs vector and the user‚Äôs question, we rank the\ninsights, selecting the top-K‚Ä≤most relevant ones.\nThis method not only reduces the LLM‚Äôs process-\ning load, but it also ensures the retained insights\nalign closely with the user‚Äôs question.\nDiversity-aware Reranking. After applying the\nabove two strategies, we obtain the top-K‚Ä≤insights.\nThen, we seek to re-rank them according to their\ndiversity to provide the top-K insight. In the con-\ntext of recommending a set of insights, the goal is\nto select insights with high individual scores and\nlow redundancy. This can be thought of as maxi-\nmizing the total usefulness of the selected insights.\nTo achieve this, we leverage the second-order ap-\nproximated ranking algorithm as explained in (Ma\net al., 2021) to determine the order.\n6 Evaluation\nImplementation. We implement our tool based on\nthe codebases of QuickInsight, MetaInsight, and\nXInsight, adding an additional 1.8K lines of C#\ncode and 1.5K lines of JavaScript code. We use\n‚Äúgpt-3.5-turbo‚Äù as our language model and ‚Äútext-\nembedding-ada-002‚Äù is used to generate embed-\ndings. Both models are provided by OpenAI.\nInsightPilotCode InterpreterPandas Agent\nRelevance 4.50¬±0.76 4.08¬±0.86 1.92¬±1.00\nCompleteness 4.67¬±0.55 3.54¬±1.00 1.12¬±0.33\nUnderstandability4.46¬±0.64 4.25¬±0.83 1.62¬±0.90\nTable 1: Results of the User Study\nUser Study. We conduct a user study to simulate\nthe real-world application ofInsightPilot, highlight-\ning its unique advantages over existing solutions\nlike OpenAI Code Interpreter (OpenAI, 2023) and\nLangchain Pandas Agent (Langchain, 2023), both\nbeing state-of-the-art in their domains. We ex-\nplored but excluded text-to-SQL models like Flan-\nT5 (Chung et al., 2022) and Anthropic Claude (An-\nthropic, 2023), due to their inability to provide di-\nrect answers or load the datasets. Four independent\ndata science participants are recruited for the study.\nThey are given two datasets and asked to raise three\nquestions each within the InsightPilot application\nscope, resulting in 24 groups of comparisons (4\nparticipants √ó2 datasets √ó3 questions). They\nscore the systems on Relevance, Completeness and\nUnderstandability (scale of 1 to 5).\nResults are reported in Table 1. InsightPilot\nconsistently outperforms the others in all three met-\nrics, showcasing its capability in offering relevant,\ncomplete and understandable responses. Specifi-\ncally, InsightPilot is notably better than the Code\n350\nData Overview\nUser Inquiry\nReport Generated by LLM\nPrioritize Relevant Insight\nInsights Discovered by Insight Engine\nFigure 3: User interface of InsightPilot.\nInterpreter and Pandas Agent in completeness (p-\nvalue < 0.05). Upon examining the competitors‚Äô\nresponses, we found they often provide an ad-hoc\nanswer to a specific region of the dataset. For exam-\nple, when inquiring about differences in car sales\nbetween Mazda and Toyota, competitors reveal\nonly the overall difference, whereas InsightPilot\nfurther analyzes various breakdowns, identifying\nthat ‚ÄúToyota‚Äôs Corolla model accounts for a larger\npercentage of sales compared to Mazda‚Äôs models.‚Äù\nCase Study. We demonstrate InsightPilot‚Äôs use in\nFigure 3, showcasing a portion of its output (due to\nspace limit). The user is using a car sales dataset\nand enquiries ‚Äú I want to know the overall trend\nof Toyota.‚Äù InsightPilot first identifies that ‚ÄúToy-\nota has a decreasing trend on its sales over the\nyears‚Äù and then dives into two representative mod-\nels of Toyota, namely ‚ÄúToyota Corolla‚Äù and ‚ÄúToy-\nota Camry.‚Äù It identifies that ‚ÄúCorolla‚Äù and ‚ÄúCamry‚Äù\nconstitute two top-selling models of Toyota and the\nsales of ‚ÄúCamry‚Äù has a strong correlation with the\noverall Sales of Toyota. Therefore, InsightPilot\nconcludes that the Camry is the key driver of Toy-\nota‚Äôs sales. Afterwards, InsightPilot further com-\npares Toyota with Honda and identifies that they\nare the top-two brands for subcompact cars while\nToyota leading the way. Besides, InsightPilot also\nlooks into the sales of Toyota in different years and\nobtains other interesting insights.\n7 Discussion\nAction-wise Performance. The efficacy of Insight-\nPilot, an automated data analytics tool, hinges on\nthe comprehensive design of each action and its ac-\ncurate execution. While we introduce four actions\nrooted in common data analysis techniques, it is vi-\ntal to note that InsightPilot‚Äôs innovation is not tied\nto specific action designs. Instead, its uniqueness\nlies in leveraging LLM for data exploration.\nComprehensive Assessment. To validate Insight-\nPilot‚Äôs effectiveness, it is essential to evaluate it\nacross diverse real-life datasets and dimensions,\nsuch as keyword preservation. Nonetheless, In-\nsightPilot often produces open-ended responses,\nmaking manual evaluation crucial for assessing an-\nswer quality. These hurdles make it challenging to\nefficiently evaluate InsightPilot‚Äôs performance in\na comprehensive manner. An LLM-based evalu-\nation framework could potentially streamline this\nprocess (Wang et al., 2023; Li et al., 2023).\n8 Conclusion\nIn this paper, we introduce InsightPilot, an LLM-\nempowered automated data exploration system. By\nseamlessly integrating LLM with state-of-the-art\ninsight engines, InsightPilot streamlines data analy-\nsis into a coherent exploration sequence. Effective\nfor real-world datasets, it allows users to derive\ninsights via natural language inquiries. InsightPi-\nlot equips even non-technical individuals to benefit\nfrom data analysis, bolstering efficiency and data-\ndriven decision-making.\nAcknowledgement\nRui Ding and Shuai Wang are the corresponding\nauthors.\n351\nReferences\nAnthropic. 2023. Anthropic claude chat. https://\nclaude.ai/chat/.\nOri Bar El, Tova Milo, and Amit Somech. 2020. Auto-\nmatically generating data exploration sessions using\ndeep reinforcement learning. In Proceedings of the\n2020 ACM SIGMOD International Conference on\nManagement of Data, pages 1527‚Äì1537.\nYukun Cao, Xike Xie, and Kexin Huang. 2023. Learn\nto explore: on bootstrapping interactive data explo-\nration with meta-learning. In 2023 IEEE 39th Inter-\nnational Conference on Data Engineering (ICDE) ,\npages 1720‚Äì1733. IEEE.\nAlexandre Chanson, Nicolas Labroche, Patrick Marcel,\nStefano Rizzi, and Vincent t‚ÄôKindt. 2022. Automatic\ngeneration of comparison notebooks for interactive\ndata exploration. In EDBT, pages 2‚Äì274.\nZhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu\nLi, Rahul Nadkarni, Yushi Hu, Caiming Xiong,\nDragomir Radev, Mari Ostendorf, Luke Zettlemoyer,\net al. 2023. Binding language models in symbolic\nlanguages. International Conference on Learning\nRepresentations.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nJay Devore. 2007. Making sense of data: A practical\nguide to exploratory data analysis and data mining.\nRui Ding, Shi Han, Yong Xu, Haidong Zhang, and\nDongmei Zhang. 2019. Quickinsights: Quick and au-\ntomatic discovery of insights from multi-dimensional\ndata. In ACM SIGMOD International Conference on\nManagement of Data.\nAndrew T Jebb, Scott Parrigon, and Sang Eun Woo.\n2017. Exploratory data analysis as a foundation of\ninductive research. Human Resource Management\nReview, 27(2):265‚Äì276.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):1‚Äì38.\nHyeonji Kim, Byeong-Hoon So, Wook-Shin Han, and\nHongrae Lee. 2020. Natural language to sql: Where\nare we today? VLDB.\nMatthieu Komorowski, Dominic C. Marshall, Justin D.\nSalciccioli, and Yves Crutain. 2016. Exploratory\nData Analysis, pages 185‚Äì203. Springer International\nPublishing, Cham.\nLangchain. 2023. Langchain pandas dataframe\nagent. https://python.langchain.com/docs/\nintegrations/toolkits/pandas.\nZongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan\nWu, Tianxiang Li, Shuai Wang, Cuiyun Gao, and\nYang Liu. 2023. Split and merge: Aligning posi-\ntion biases in large language model based evaluators.\narXiv preprint arXiv:2310.01432.\nPingchuan Ma, Rui Ding, Shi Han, and Dongmei Zhang.\n2021. Metainsight: Automatic discovery of struc-\ntured knowledge for exploratory data analysis. In\nACM SIGMOD International Conference on Man-\nagement of Data.\nPingchuan Ma, Rui Ding, Shuai Wang, Shi Han, and\nDongmei Zhang. 2023. Xinsight: explainable data\nanalysis through the lens of causality. In ACM SIG-\nMOD International Conference on Management of\nData.\nPingchuan Ma and Shuai Wang. 2022. Mt-teql: Eval-\nuating and augmenting neural nlidb on real-world\nlinguistic and schema variations. VLDB.\nOpenAI. 2023. Openai code interpreter.\nhttps://chat.openai.com/?model=\ngpt-4-code-interpreter .\nAur√©lien Personnaz, Sihem Amer-Yahia, Laure Berti-\nEquille, Maximilian Fabricius, and Srividya Subra-\nmanian. 2021. Balancing familiarity and curiosity\nin data exploration with deep reinforcement learning.\nIn Fourth Workshop in Exploiting AI Techniques for\nData Management, pages 16‚Äì23.\nPanos Vassiliadis, Patrick Marcel, and Stefano Rizzi.\n2019. Beyond roll-up‚Äôs and drill-down‚Äôs: An inten-\ntional analytics model to reinvent olap. Information\nSystems, 85:68‚Äì91.\nPanos Vassiliadis and Timos Sellis. 1999. A survey\nof logical models for olap databases. ACM Sigmod\nRecord, 28(4):64‚Äì69.\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai\nLin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\n2023. Large language models are not fair evaluators.\narXiv preprint arXiv:2305.17926.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. arXiv preprint arXiv:2210.03629.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-\ning Yao, Shanelle Roman, et al. 2018. Spider: A\nlarge-scale human-labeled dataset for complex and\ncross-domain semantic parsing and text-to-sql task.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n3911‚Äì3921.\n352",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8496838212013245
    },
    {
      "name": "Process (computing)",
      "score": 0.7184929251670837
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5608526468276978
    },
    {
      "name": "Natural language",
      "score": 0.48930594325065613
    },
    {
      "name": "Data exploration",
      "score": 0.48408448696136475
    },
    {
      "name": "Data science",
      "score": 0.45277953147888184
    },
    {
      "name": "Data modeling",
      "score": 0.43787679076194763
    },
    {
      "name": "Data mining",
      "score": 0.34616780281066895
    },
    {
      "name": "Artificial intelligence",
      "score": 0.24922627210617065
    },
    {
      "name": "Software engineering",
      "score": 0.21689575910568237
    },
    {
      "name": "Visualization",
      "score": 0.14580267667770386
    },
    {
      "name": "Programming language",
      "score": 0.07818460464477539
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ],
  "cited_by": 50
}