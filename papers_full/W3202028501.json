{
  "title": "Understanding and Overcoming the Challenges of Efficient Transformer Quantization",
  "url": "https://openalex.org/W3202028501",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3169978166",
      "name": "Yelysei Bondarenko",
      "affiliations": [
        "Qualcomm (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2105724237",
      "name": "Markus Nagel",
      "affiliations": [
        "Qualcomm (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2895361001",
      "name": "Tijmen Blankevoort",
      "affiliations": [
        "Qualcomm (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2964164125",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2950297794",
    "https://openalex.org/W2809624076",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2469490737",
    "https://openalex.org/W2950784811",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3105645800",
    "https://openalex.org/W1999085092",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2963273111",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3035183452",
    "https://openalex.org/W2805493160",
    "https://openalex.org/W4287867774",
    "https://openalex.org/W3017022649",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W3100083812",
    "https://openalex.org/W2921077359",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2896718327",
    "https://openalex.org/W3170233084",
    "https://openalex.org/W4287812978",
    "https://openalex.org/W3022969335",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2963122961",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2982479999",
    "https://openalex.org/W3020212829",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W4287777801",
    "https://openalex.org/W4287667694",
    "https://openalex.org/W2911300548",
    "https://openalex.org/W2916954108",
    "https://openalex.org/W3035232708",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3170437634",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W2981751377",
    "https://openalex.org/W2947946877",
    "https://openalex.org/W2949118787",
    "https://openalex.org/W2286365479",
    "https://openalex.org/W2970896726",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W2889847962",
    "https://openalex.org/W2524428287",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W2787513570",
    "https://openalex.org/W3024171804",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W3118568258",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W2912168260",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2903260438",
    "https://openalex.org/W2982041622",
    "https://openalex.org/W2972918064",
    "https://openalex.org/W2950458216",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W2242818861",
    "https://openalex.org/W2786951478",
    "https://openalex.org/W3103754749",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3000514857",
    "https://openalex.org/W4322588812",
    "https://openalex.org/W4287118909",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W2884150179",
    "https://openalex.org/W3017947878",
    "https://openalex.org/W4288337707",
    "https://openalex.org/W2982615700",
    "https://openalex.org/W3091156754",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3035251378",
    "https://openalex.org/W1841592590",
    "https://openalex.org/W2998218113",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3024366773"
  ],
  "abstract": "Transformer-based architectures have become the de-facto standard models for a wide range of Natural Language Processing tasks. However, their memory footprint and high latency are prohibitive for efficient deployment and inference on resource-limited devices. In this work, we explore quantization for transformers. We show that transformers have unique quantization challenges – namely, high dynamic activation ranges that are difficult to represent with a low bit fixed-point format. We establish that these activations contain structured outliers in the residual connections that encourage specific attention patterns, such as attending to the special separator token. To combat these challenges, we present three solutions based on post-training quantization and quantization-aware training, each with a different set of compromises for accuracy, model size, and ease of use. In particular, we introduce a novel quantization scheme – per-embedding-group quantization. We demonstrate the effectiveness of our methods on the GLUE benchmark using BERT, establishing state-of-the-art results for post-training quantization. Finally, we show that transformer weights and embeddings can be quantized to ultra-low bit-widths, leading to significant memory savings with a minimum accuracy loss. Our source code is available at https://github.com/qualcomm-ai-research/transformer-quantization.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7947–7969\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n7947\nUnderstanding and Overcoming the Challenges of\nEfﬁcient Transformer Quantization\nYelysei Bondarenko, Markus Nagel, Tijmen Blankevoort\nQualcomm AI Research∗\n{ybond, markusn, tijmen}@qti.qualcomm.com\nAbstract\nTransformer-based architectures have become\nthe de-facto standard models for a wide\nrange of Natural Language Processing tasks.\nHowever, their memory footprint and high la-\ntency are prohibitive for efﬁcient deployment\nand inference on resource-limited devices.\nIn this work, we explore quantization for\ntransformers. We show that transformers have\nunique quantization challenges – namely, high\ndynamic activation ranges that are difﬁcult to\nrepresent with a low bit ﬁxed-point format.\nWe establish that these activations contain\nstructured outliers in the residual connections\nthat encourage speciﬁc attention patterns,\nsuch as attending to the special separator\ntoken. To combat these challenges, we\npresent three solutions based on post-training\nquantization and quantization-aware training,\neach with a different set of compromises for\naccuracy, model size, and ease of use. In\nparticular, we introduce a novel quantization\nscheme – per-embedding-group quantization.\nWe demonstrate the effectiveness of our\nmethods on the GLUE benchmark using\nBERT, establishing state-of-the-art results for\npost-training quantization. Finally, we show\nthat transformer weights and embeddings\ncan be quantized to ultra-low bit-widths,\nleading to signiﬁcant memory savings with\na minimum accuracy loss. Our source\ncode is available at https://github.\ncom/qualcomm-ai-research/\ntransformer-quantization.\n1 Introduction\nRecently, transformer architectures have shown re-\nmarkable improvement in many Natural Language\nProcessing (NLP) tasks and beyond. Based on\nthe original Transformer (Vaswani et al., 2017),\nlanguage models pre-trained from large corpora\nof unlabeled text, such as BERT (Devlin et al.,\n2019), RoBERTa (Liu et al., 2019), XLNet (Yang\n∗Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.\net al., 2019), Transformer-XL (Dai et al., 2019),\nGPT family (Radford et al., 2018, 2019; Brown\net al., 2020), have become an indispensable build-\ning block in modern NLP pipelines. They are also\nincreasingly adopted in other areas, including vi-\nsion (Carion et al., 2020; Dosovitskiy et al., 2020;\nChen et al., 2020; Girdhar et al., 2019) and au-\ndio (Dong et al., 2018; Child et al., 2019).\nDespite cutting edge results in many applica-\ntions, pre-trained transformer-based models are\nextremely large, sometimes exceeding billions of\nparameters. Hence, efﬁcient deployment of these\nmodels on resource-constrained embedded systems,\nand even sometimes in data centers, has become\nan important problem due to high latency and pro-\nhibitively large memory footprint and energy con-\nsumption.\nOne effective method to tackle this problem is\nneural network quantization. Quantization reduces\nmemory consumption by using low-bit precision\nfor weight and activation tensors. Is also reduces\ninference time, and improves energy efﬁciency by\nemploying low-bit ﬁxed-point arithmetic instead of\nﬂoating-point arithmetic (Horowitz, 2014).\nQuantization, however, is not free. It introduces\nadditional noise in the network that can lead to\na drop in the model’s performance. While prior\nwork has demonstrated the feasibility of integer-\nonly inference for computer vision models (Lin\net al., 2016; Jacob et al., 2018; Krishnamoorthi,\n2018; Zhang et al., 2018; Choukroun et al., 2019;\nDong et al., 2019; Esser et al., 2019; Nagel et al.,\n2019, 2020), there is relatively little work done\non quantizing NLP models (Wang et al., 2018b;\nXu et al., 2018), and speciﬁcally on transformer\nmodels.\nUnderstanding the challenges of transformer\nquantization and designing a robust and easy-to-\nuse quantization pipeline for them constitute the\nprimary goal of this paper. The contributions of\nour work include:\n7948\n• We show that standard 8-bit post-training\nquantization techniques lead to a signiﬁcant\nperformance degradation for transformer en-\ncoder models.\n• We conduct a systematic study to identify\nthe underlying reason that precludes efﬁcient\ntransformer quantization. We ﬁnd that the\nmain bottleneck is a considerable mismatch\nbetween the different dynamic ranges of acti-\nvation tensors in the residual connections. Fur-\nther analysis shows that these activation ten-\nsors contain structured outliers that facilitate\nspeciﬁc attention patterns in deeper encoder\nlayers, such as attending to the special[SEP]\ntoken. We highlight that this issue is inherent\nto many architectures and pre-training objec-\ntives.\n• Based on these ﬁndings, we propose a set of\nsolutions with different trade-offs to overcome\nthe dynamic range problem, including tech-\nniques based on post-training, mixed preci-\nsion, and quantization-aware training. In par-\nticular, we introduce a new per-embedding-\ngroup quantization scheme, which solves the\nactivation quantization issue without a signif-\nicant compute overhead or increase in com-\nplexity.\n• Finally, we show that weights and embeddings\nin BERT-like models can be quantized to ultra-\nlow (2-4) bits, reducing the memory footprint\nby more than 8 ×with a minimal accuracy\nloss.\nWe evaluate our proposed solutions on eight\ndifferent NLP tasks from the well-known GLUE\nbenchmark. Our techniques set a new state-of-\nthe-art of post-training quantization and per-tensor\nquantization-aware training for the BERT model.\nTo the best of our knowledge, this is the ﬁrst work\nfor the BERT-like transformer quantization with\na strong focus on post-training quantization. The\npresented method is not exclusive to BERT and is\neasily applicable to other pre-trained transformer\nmodels.\n2 Background and related work\nEfﬁcient Transformers Making transformer\nmodels more efﬁcient in terms of memory and\ncomputation time is an active area of research. A\ngood survey paper is Tay et al. (2020). Most prior\nwork focuses on architectural changes that speed\nup self-attention, which is the most expensive op-\neration crucial for efﬁcient processing of long se-\nquences of tokens or pixels. Notable examples\ninclude ones that apply ﬁxed (Child et al., 2019;\nBeltagy et al., 2020) or learned (Kitaev et al., 2020)\nsparsity patterns to the otherwise dense attention\nmatrix, while others introduce efﬁcient approxima-\ntions based on low-rank (Wang et al., 2020b) or\nkernel methods (Katharopoulos et al., 2020; Choro-\nmanski et al., 2020). Some of the complemen-\ntary efforts in this area are compact and fast ar-\nchitectures by design (Sun et al., 2020; Iandola\net al., 2020), weight sharing (Dehghani et al., 2018;\nLan et al., 2019), parameter reuse across multiple\ndownstream tasks (Houlsby et al., 2019; Stickland\nand Murray, 2019), knowledge distillation (Sanh\net al., 2019; Jiao et al., 2020), neural architecture\nsearch (Guo et al., 2019; Wang et al., 2020a), prun-\ning (Sanh et al., 2020; Prasanna et al., 2020), and\nbetter pre-training (Liu et al., 2019; Clark et al.,\n2020).\nQuantization One of the most powerful ways to\ndecrease the computational time and memory con-\nsumption of neural networks is quantization, which\nuses low-bit representations for weight and/or acti-\nvation tensors. When moving from 32 to 8 bits, the\nmemory overhead of storing tensors decreases by a\nfactor of 4, while the computational cost for matrix\nmultiplication reduces quadratically by a factor of\n16. Low-bit ﬁxed-point representations, such as\nINT8, further reduce the energy consumption since\nthe ﬁxed-point operations are more efﬁcient than\ntheir ﬂoating-point counterparts (Horowitz, 2014).\nHowever, exact latency improvements and energy\nsavings are highly dependent on the target hard-\nware. Therefore, we focus in this work on achiev-\ning high memory and compute reduction while\nmaintaining acceptable model accuracy and do not\nmeasure actual on-device performance gains. We\nwill cover relevant basics of quantization here, for\na more comprehensive overview of neural network\nquantization please refer to Nagel et al. (2021).\nA commonly used scheme for quantization is\nuniform afﬁne or asymmetric quantization (Zhou\net al., 2016; Hubara et al., 2017; Krishnamoorthi,\n2018) because it allows for efﬁcient implementa-\ntion of ﬁxed-point arithmetic. It is deﬁned by bit-\nwidth b ∈N, scale factor s ∈R+, and zero-point\nz ∈ Z. We simulate the quantization process\nin ﬂoating-point according to Jacob et al. (2018).\n7949\nQuantizing a real-valued tensor x is performed by\nﬁrst mapping it to an unsigned integer grid:\nx(Z) = clip\n(⌊x\ns\n⌉\n+ z; 0, 2b −1\n)\n, (1)\nIt is possible to approximately recover the real-\nvalued input x through an operation that is often\nreferred to as de-quantization:\nˆx := q (x; s, z, b) =s\n(\nx(Z) −z\n)\n≈x. (2)\nIn the case of symmetric quantization, we restrict\nthe quantization grid to be symmetric around z.\nIt is common to have a single set of quantization\nparameters per tensor, known as per-tensor quanti-\nzation. One could also increase the quantization\ngranularity by deﬁning separate quantizers for in-\ndividual segments of a tensor. This will improve\nthe accuracy of a network, but at the cost of an\nadditional compute and memory overhead.\nAn important class of quantization methods\nis post-training quantization (PTQ) algorithms,\nwhich take a pre-trained FP32 network and con-\nvert it directly into a ﬁxed-point network without\nthe need for the original training pipeline (Krish-\nnamoorthi, 2018). A vital step in the PTQ process\nis ﬁnding good quantization ranges for each quan-\ntizer. One way of doing this is static range estima-\ntion, which determines quantization parameters for\nthe network by passing a few batches of calibration\ndata through the model before inference. It yields\nmore efﬁcient inference since all the quantization\nparameters are known in advance and ﬁxed. Sev-\neral of the most common range estimators include:\ncurrent min-max or simply min-max, uses the full\ndynamic range of the tensor (Zhou et al., 2016;\nWu et al., 2018b; Zhu et al., 2020);\nrunning min-max uses exponential moving av-\nerage of the min and max over multiple\nbatches (Krishnamoorthi, 2018);\nMSE ﬁnds quantization parameters that minimize\nmean squared error between quantized and\nﬂoating-point tensors (Choukroun et al., 2019;\nBanner et al., 2018).\nAn alternative to PTQ is to train a neural net-\nwork with the simulated quantization operations in\nthe network, known as quantization-aware train-\ning (QAT, Jacob et al. 2018; Gupta et al. 2015;\nKrishnamoorthi 2018). It allows the model to\nbetter adapt to the introduced quantization noise\ncompared to PTQ, at the cost of longer train-\ning times, the need for labeled data and doing a\nhyper-parameter search. Gradients through the non-\ndifferentiable quantization step are usually approxi-\nmated using the straight-through estimator (Bengio\net al., 2013). Ranges for both weights and activa-\ntions can be set using PTQ range estimators or\nlearned jointly with the weights during training, as\nin Esser et al. (2019); Jain et al. (2019).\nFinally, it is possible to assign different bit-\nwidths for different layers or parts of the network,\na technique known as mixed precision (Lin et al.,\n2016; Wu et al., 2018a; Zhou et al., 2018; Dong\net al., 2019; Wang et al., 2019; van Baalen et al.,\n2020).\nTransformer quantization Junczys-Dowmunt\net al. (2018) applied knowledge distillation and\n8-bit post-training quantization to speed up trans-\nformer models for neural machine translation.\nBhandare et al. (2019) also applied 8-bit post-\ntraining quantization to the transformer model for\nmachine translation and demonstrated how to uti-\nlize specialized hardware to accelerate the infer-\nence process.\nZafrir et al. (2019) proposed an 8-bit quantiza-\ntion scheme for BERT-like models and achieves\ncompression of up to 25% of the original model\nsize. Shen et al. (2020) applies mixed-precision\nquantization on BERT, where they assign a dif-\nferent precision to different layers according to\ntheir sensitivity deﬁned by Hessian information.\nKim et al. (2021) proposed a fully integer-only\narithmetic inference scheme based on second-order\npolynomial approximations for GELU, Softmax,\nand LayerNorm non-linearities. Some examples of\ntransformer-based model quantization with alterna-\ntive quantization schemes include Fan et al. (2020);\nDarvish Rouhani et al. (2020).\nNote that all the mentioned approaches for\nBERT-like transformer quantization employ some\nform of QAT and either do not discuss PTQ alter-\nnatives or only use them as weak baselines.\n3 Problem investigation\nFirst, we investigate what happens when we ap-\nply standard 8-bit post-training quantization to\nthe BERT model and evaluate it on eight down-\nstream tasks from the GLUE benchmark (Wang\net al., 2018a). To quantize ﬁne-tuned models, we\nuse uniform afﬁne quantization with static range\n7950\nConﬁguration CoLA SST-2 MRPC STS-B QQP MNLI QNLI RTE GLUE\nFP32 57.27 93.12 88.36 89.09 89.72 84.91 91.58 70.40 83.06\nW8A8 54.74 92.55 88.53 81.02 83.81 50.31 52.32 64.98 71.03\nW32A8 56.70 92.43 86.98 82.87 84.70 52.80 52.44 53.07 70.25\nW8A32 58.63 92.55 88.74 89.05 89.72 84.58 91.43 71.12 83.23\nTable 1: Post-training quantization results on development sets of the GLUE benchmark (except WNLI). The\nmetrics for these tasks can be found in the GLUE paper (Wang et al., 2018a); in all cases, higher is better. FP32\nbaseline is trained by the authors from the pre-trained checkpoint, see Appendix B.1 for details. We report a\nmedian over 5 runs with different random seeds.\nestimation, as described in Section 2. We quan-\ntize all layer’s weights and activations (both input\nand output). We follow a typical setup with sym-\nmetric weight and asymmetric activation quantiza-\ntion (Bhalgat et al., 2020). We try several choices\nfor range estimation for both weights and activa-\ntions and report the best conﬁguration per task,\nbased on its metric (see Appendix B.2 for details).\nIn Table 1, we present the results for joint (W8A8),\nactivation-only (W32A8), and weight-only quanti-\nzation (W8A32). We note that there is a signiﬁcant\nperformance degradation for joint 8-bit quantiza-\ntion. We can also see that weight quantization\nincurs almost no error on its own and that most\ndegradation is due to activation quantization. Fi-\nnally, some tasks seem to be more robust to quanti-\nzation than others.\nTo ﬁnd which part of the network is the most\nproblematic, we perform an ablation study in which\nwe do not quantize speciﬁc activations. The results\nare summarized in Table 2. By far, the smallest\nperformance drop is when we do not quantize the\nresidual sum after the feed-forward network (FFN,\nsee Figure 1). Furthermore, the issue seems to be\nthe most pronounced for deeper encoder layers (10\nand 11).\nTo understand why quantizing the residual FFN\nsum is so detrimental, we look at activation tensors\nin the problematic 11th layer. First, from Figure 2a,\nwe note that FFN’s input and output have radically\ndifferent dynamic ranges (note the scale for y-axes)\ndue to strong outliers in the output tensor. Apply-\ning per-tensor quantization for the FFN’s residual\nsum is likely to cause a notable error because of\nthe following trade-off between the range and the\nprecision. On the one hand, using the full dynamic\nrange for the FFN’s output will lead to a very coarse\nquantization of its input. On the other hand, using\nhigher precision for the input will cause informa-\ntion loss in the output due to aggressive clipping\nof its range. We also notice a correlation of out-\nliers with special [SEP] tokens. In addition to\nthat, from Figure 2b, we observe that only a few\nembedding dimensions are consistently responsible\nfor these outliers across many data points.\nIn Appendix D, we show that this is the case\nfor all layers of BERT-base and all GLUE tasks.\nFurthermore, we show that a similar issue is also\npresent in multiple architectures and training objec-\ntives, including pre-trained BERT-large, RoBERTa,\nand DistilRoBERTa (Sanh et al., 2019), and Mo-\nbileBERT (Sun et al., 2020).\nFurther analysis suggests that structured outliers\nin the FFN’s residual connections lead to structured\noutliers in query-key multiplications in speciﬁc at-\ntention heads in the next attention layer, causing\nmost of the tokens to attend to the special [SEP]\ntoken. See Appendix A for more details on this.\n4 Methodology\nIn this section, we introduce our proposed tech-\nniques for BERT-like model quantization. Moti-\nvated by our ﬁndings from Section 3, we consider\nthree ways of efﬁcient BERT quantization – post-\ntraining mixed precision, a new per-embedding-\ngroup activation quantization, and quantization-\naware training. Each of these three methods comes\nwith its own set of trade-offs, which is why we\npresent all three. The reader can pick an appro-\npriate solution for their practice. As before, we\nemploy uniform afﬁne quantization and static acti-\nvation ranges, which are either estimated in PTQ\nor learned during QAT, as described in Section 2.\nMixed precision PTQ As seen in Section 3, not\nall parts of BERT are equally sensitive to the quanti-\nzation noise. Thus, selecting a higher bit-width for\nsensitive tensors can lead to better accuracy while\nefﬁciently keeping all the other tensors in 8-bit or\n7951\nQuantized activations STS-B MNLI QNLI RTE\nnone (FP32 model) 89.09 84.91 91.58 70.40\nall 62.64 42.67 50.74 48.74\nall, except softmax input 70.92 42.54 51.84 48.74\nall, except sum of embeddings 67.57 46.82 51.22 51.26\nall, except self-attention output 70.47 46.57 50.98 50.90\nall, except softmax output 72.83 50.35 50.23 49.46\nall, except residual connections after FFN 81.57 82.56 89.73 67.15\nsame as above, but for layers 10, 11 only 79.40 81.24 88.03 63.90\nTable 2 & Figure 1: Left: Leave-one-out analysis for activation quantizers on problematic GLUE tasks. We set\nall weights to FP32 and use current min-max (with a batch size of 1) range estimator for activations. We report\nmedian score over 5 runs with different random seeds. Right: A schematic illustration of the attention layer in\nBERT. Hidden activation tensor is denoted byx. ⊕is an element-wise addition. A problematic residual connection\nsum after feed-forward network is highlighted in red.\n(a)\n (b)\nFigure 2: Full-precision FFN input (top row) and output (bottom row) in 11th layer of BERT. (a) Per-token ranges\nfor ﬁrst data sequence in the MNLI development set. (b) Visualization of outliers across embedding dimension for\nthe ﬁrst ten data sequences in the MNLI development set. Dark grey color indicates values that exceed six standard\ndeviations from the mean of the activation tensor.\nlower.\nFirst, we consider 16-bit activation quantization\nfor problematic activation tensors, such as the resid-\nual sum tensor after the feed-forward network. It\nwill provide a model with sufﬁcient precision to\nrepresent both FFN’s input and output, as well as\ntheir sum. Additionally, given the observation from\nTable 1, that the BERT model seems to be quite\nresilient to 8-bit weight quantization, we also con-\nsider the effect of low-bit (2-4) weight and token\nembedding quantization, which reduces the model\nsize by more than 8×with a minimal loss in accu-\nracy.\nPer-embedding-group PTQ As discussed in\nSection 2, another way of improving the perfor-\nmance of the quantized model is to increase the\nquantization granularity. Based on our observation\nfrom Figure 2b, that the most problematic outliers\nin activation tensors are in few designated embed-\nding dimensions, we consider having distinct quan-\ntization parameters for individual embedding di-\nmensions or groups of embedding dimensions, as\nshown in Figure 3.\nWe start by describing per-embedding activa-\ntion quantization. In BERT-like models, an inter-\nmediate hidden activation tensor x has a shape\n(B, T, d), where B is the batch size, T is the se-\nquence length, and d is the number of embedding\ndimensions (d = 768for BERT-base, Devlin et al.\n2019). Inspired by per-channel weight quantiza-\ntion (Krishnamoorthi, 2018), we can have distinct\nscaling factors and zero-points per embedding di-\nmension instead of having two scalars for the whole\ntensor. In this case, we can collectively denote the\n7952\n(a) Per-tensor\n (b) Per-embedding\n (c) Per-embedding-group\nFigure 3: An overview for several choices of activation quantization granularity. The color indicates quantization\nparameter sharing. In all cases we assume per-tensor weight quantization.\nquantization parameters by vectors s, z ∈Rd. The\nrest of the quantization machinery works as before,\nincluding range estimation, with the only differ-\nence that equations (1) and (2) are now with broad-\ncasting along the last dimension. The proposed\nscheme should alleviate the activation quantization\nissue since the outlier embedding dimensions will\nno longer dominate the ranges of other embedding\ndimensions.\nNote, however, that full per-embedding activa-\ntion quantization will lead to a more expensive\ncomputational graph. To illustrate why, consider a\nmatrix-vector multiplication Wx, which in case of\nper-tensor quantization (and assuming z = 0) for\nboth weights and activations can be computed as\nfollows:\nˆWˆx =\n(\nsw ·W(Z)\n)(\nsx ·x(Z)\n)\n= swsx ·\n( d∑\nj=1\nW(Z)\nij x(Z)\nj\n)\ni\n. (3)\nA crucial detail here is that we can factor a common\nfactor swsx out of the summation. The sum is then\nefﬁciently calculated using integer-only arithmetic.\nIn case of per-embedding activation quantization\n(ˆx = sx ⊙x(Z)), the matrix-vector multiplication\nbecomes instead:\nˆWˆx = sw ·\n( d∑\nj=1\nsx\nj ·W(Z)\nij x(Z)\nj\n)\ni\n. (4)\nHere it is no longer possible to take the scaling\nfactor out of the summation and perform a single\nre-scaling of the result. Instead, one has to perform\nrepeated intermediate re-scalings on the accumula-\ntor.\nTo alleviate the overhead of constant re-scaling,\nwe introduce per-embedding-group (PEG) quan-\ntization, where we split the activation tensor into\nK evenly sized groups along the embedding di-\nmension and share quantization parameters among\nelements in the same group:\nˆx =\n[\nsx\n1 ·\n[\nx(Z)\n1 . . .x(Z)\nd/K\n]\n, sx\n2 ·\n[\nx(Z)\nd/K+1 . . .\n]\n,\n. . . ,sx\nK ·\n[\n. . .x(Z)\nd\n]]\n, (5)\nwhere [···] denotes concatenation. Thus the re-\nquired re-scaling operations are signiﬁcantly re-\nduced from d to K.\nThe proposed scheme might not be natively sup-\nported on all target device, but there is an efﬁcient\nway to implement it using only per-tensor quantiza-\ntion. Before quantizing the output of the ﬁrst Lay-\nerNorm (Figure 1), we split the output tensor based\non embedding groups into K individual tensors.\nWe also accordingly split columns of the ﬁrst Lin-\near layer and rows of the second layer and decom-\npose them into K smaller Linear layers each. The\noutputs of the ﬁrst set of layers are elementwise-\nsummed, and the outputs of the second set of layers\nare concatenated before the residual sum. With this\nfunctional equivalent rewriting, all operations can\nbe performed using standard per-tensor quantiza-\ntion.\nTo ensure all outliers end up in the same group,\nwe employ a deterministic range-based permu-\ntation of the embedding dimensions. Similar to\nrange estimation for the activation quantization,\nwe pass some calibration data through the un-\nquantized network and record the dynamic range\nrj := max(x:,:,j) −min(x:,:,j) for each embed-\nding dimension j. Next, we deﬁne K evenly sized\ngroups based on indices in arg sort(r). During the\nrange estimation phase, we determine a separate\nquantization range for each group. The sorting and\ngrouping need to happen only once before the range\nestimation phase and deployment to the target.\n7953\nPEG quantization with permutation can still\nbe simulated on hardware that only supports per-\ntensor operations. First, we can share the same per-\nmutation for FFN’s input, output and sum since we\nexpect the outliers in the output dominate the ones\nfrom the input. Second, we use the permutation-\nequivariant properties of LayerNorm and linear lay-\ners (weights are permuted accordingly before in-\nference). We ﬁrst permute the output of the ﬁrst\nLayerNorm, proceed as described above, and then\napply inverse permutation before the next Layer-\nNorm.\nNote that the PEG quantization has a negligible\nmemory overhead, introducing only d + 2·3 ·K\nextra parameters per attention layer (permutation\nindices and scale & zero points per group for FFN’s\ninput, output, and sum), which is less than 0.04%\nof the total size of BERT-base model.\nQuantization-aware training Finally, we con-\nsider a variant of QAT with learnable ranges for\nboth weights and activations by adapting the pro-\ncedure from Esser et al. (2019); Jain et al. (2019)\nfor BERT-like transformer models. Simulating the\nquantization process during ﬁne-tuning allows the\nmodel to adapt to quantization noise and often sig-\nniﬁcantly increases performance compared to post-\ntraining quantization.\nCriterion MP-PTQ PEG-PTQ QAT\nPost-training ✓ ✓ \u0015\nPer-tensor ✓ \u0015 ✓\nSame bit-width \u0015 ✓ ✓\nTable 3: Comparison between proposed techniques\n(MP = mixed precision, PEG = per-embedding-group).\nComparison of methods We summarize differ-\nent trade-offs for the proposed techniques in Ta-\nble 3. As discussed in Section 2, usually PTQ meth-\nods are preferred over QAT algorithms since they\nare faster and require either no data or only a small\ncalibration dataset. Additionally, they typically re-\nquire almost no hyperparameter tuning, enabling\neasy and computationally efﬁcient quantization. Al-\nlocating a higher bit-width to certain parts of the\nnetwork will reduce the efﬁciency gain from quan-\ntization, because higher bit-width layers are more\ncomputationally expensive. It is also not supported\nby all target hardware. Per-embedding-group quan-\ntization has a smaller granularity compared to per-\ntensor quantization. It leads to a minor amount of\nextra compute (and potential latency) due to the\nadditional summation and re-quantization that oc-\ncurs and might not be supported natively on every\nﬁxed-point platform. Meanwhile, we have shown\na way to simulate this scheme on a hardware that\nonly support per-tensor quantization operations.\n5 Experiments\nIn this section, we evaluate the proposed quanti-\nzation techniques for the BERT model on GLUE\ndownstream tasks.\nExperimental setup In all experiments, we use\nuniform afﬁne quantization – symmetric weights,\nasymmetric activations – with the static activation\nrange setting, as discussed in Section 2. We quan-\ntize all layer’s weights and activations. For 8-bit\nweight quantization, we use the best range settings\nfound in the experiment from Section 3, which can\nbe found in Appendix B.2. However, for low (<8)\nbit weight and token embedding quantization, we\nalways use the MSE range estimator, as recom-\nmended by Choukroun et al. (2019); Banner et al.\n(2018). We set activation ranges based on min and\nmax from a single input sequence. For PTQ exper-\niments, we report the median score over ﬁve runs\nwith different random seeds.\nFor QAT experiments, we initialize all quanti-\nzation parameters from the PTQ setup described\nabove. Similarly to full-precision ﬁne-tuning, we\nuse Adam (Kingma and Ba, 2014) and a maximum\nsequence length of 128, with padding using a spe-\ncial [PAD] token for shorter sequences. We use a\ntypical learning rate schedule from the transformer\nliterature (Devlin et al., 2019; Liu et al., 2019; Lan\net al., 2019) – a linear warmup for the ﬁrst 10% of\ntraining steps followed by a linear decay to zero.\nWe perform a hyper-parameter search over the max-\nimum learning rate, batch size, number of epochs,\nand the self-attention dropout rate for every task\nand report the best median score over three runs\nwith different random seeds. For reproducibility,\nwe included more details on the search space and\nselected hyper-parameters in Appendix B.3.\nMixed precision PTQ First, we present the re-\nsults for mixed precision post-training quantization\n(MP-PTQ) in Table 4, where we start from 8-bit\nactivations and progressively keep more and more\noperations in 16-bit precision. We see that for clas-\nsiﬁcation tasks (MNLI, QNLI, RTE), it is sufﬁcient\nto keep a few of the most problematic parts in 16-bit\n7954\nMethod STS-B MNLI QNLI RTE\nFP32 89.09 84.91 91.58 70.40\nW8A8 PTQ 79.78 45.60 51.73 64.98\nMP-PTQ* 85.41 82.20 88.38 66.43\nMP-PTQ*† 85.27 82.67 90.41 68.95\nMP-PTQ*†‡ 88.00 82.67 90.41 68.95\nTable 4: Mixed precision post-training quantization re-\nsults for BERT-base on development sets of the prob-\nlematic GLUE tasks. *Uses 16-bit residual FFN sum.\n†Uses 16-bit FFN input and output. ‡Uses 16-bit ﬁnal\noutput (using MSE range estimator).\n#groups, K STS-B MNLI QNLI RTE\nFP32 89.09 84.91 91.58 70.40\n1 (= per-tensor) 79.78 45.60 51.73 64.98\n768 (= per-embd.) 87.87 80.97 90.66 69.31\n768 (only FFN)* 87.92 81.00 90.68 68.59\n6 (only FFN) 87.26 80.51 89.82 68.59\n3 (only FFN) 85.96 76.43 80.74 66.06\n3 + P(only FFN) 87.92 80.64 91.07 69.31\n6 + P(only FFN) 87.92 81.25 91.07 69.31\nTable 5: Per-embedding-group activation quantization\nPTQ results for BERT-base on development sets of the\nproblematic GLUE tasks. *Per-embedding-group quan-\ntization is applied only to FFN’s input, output, and\nresidual sum (all the rest – per-tensor). “ + P” – Uses\nrange-based permutation.\nto get good performance. For the STS-B regres-\nsion task, it is also necessary to keep the output in\nhigher precision to close the gap with FP32 model\nperformace.\nIn conclusion, by only keeping 22% of the ac-\ntivations in 16-bit1, we can achieve performance\nclose to FP32, while all other activations and all\nweights are in 8-bit for efﬁcient inference.\nPer-embedding-group PTQ Next, we inves-\ntigate the effectiveness of the proposed per-\nembedding-group post-training activation quantiza-\ntion, depending on the number of groups K. The\nresults are summarized in Table 5. Per-embedding\nactivation quantization signiﬁcantly improves per-\nformance, even when only applied to problematic\nparts of the network. Surprisingly, we can also\nrecover most of the performance degradation with\nonly K = 3groups (size 256 each), especially if\n136 out of 161 activation quantizers for BERT-base\nwe apply range-based permutation to ensure all the\noutliers end up in the same group. A small number\nof groups is essential since it limits the number of\nre-scalings required, enabling efﬁcient execution\non resource constraint devices.\nComparison of proposed methods We summa-\nrize the results for all of our proposed techniques\nand compare them to several related methods from\nthe literature in Table 6. We use the same setup\nas described above. Unless otherwise stated, all\nresults use 8-bit per-tensor quantization for both\nweights and activations. For mixed precision (MP-\nPTQ), we use the best setup from the ablation\nstudy before. For per-embedding-group quanti-\nzation (PEG-PTQ), we use K = 6 groups with\nrange-based permutation for all tasks and only ap-\nply it to FFN’s input, output, and the sum.\nTo summarize, all the proposed techniques\nsolved the dynamic range problem, enabling ef-\nﬁcient transformer quantization with minimum ac-\ncuracy loss. Our PTQ results strongly outperform\nresults from the literature, while our assumptions in\nmixed precision are milder than ones of Q8BERT,\nwhich keeps all non-linearities in FP32. Our per-\ntensor QAT results are also on par or outperform\nresults from the literature, which uses ﬁner quan-\ntization granularity and keeps certain parts of the\nnetwork in FP32.\nLow-bit weight and token embeddings Given\nthe robustness of the BERT model to 8-bit weight\nquantization, we investigate the effect of low-bit\nweight and token embedding quantization and sum-\nmarize the results in Table 7.\nWe see that even in the post-training regime, it\nis possible to achieve low-bit weight quantization\nwith acceptable performance degradation, espe-\ncially when combined with AdaRound (Nagel et al.,\n2020), a technique for learning optimal rounding.\nQAT recovers most of the performance, even with\nquantized activations. Furthermore, we can push\ntoken embeddings to 2-bits with less than a 0.8%\ndrop in terms of the GLUE score. This reduces the\nmodel size by8.85×compared to the original FP32\ncheckpoint and can signiﬁcantly increase inference\nspeed and reduce the energy consumption on re-\nsource constraint devices. More detailed results,\nincluding per-task scores and comparison to results\nfrom the literature, can be found in Appendix C.\n7955\nMethod CoLA SST-2 MRPC STS-B QQP MNLI QNLI RTE GLUE\nFP32 baseline 57.27 93.12 88.36 89.09 89.72 84.91 91.58 70.40 83.06\nOur W8A8 PTQ 54.74 92.55 88.53 81.02 83.81 50.31 52.32 64.98 71.03\nOur W8A{8,16} MP-PTQ 58.63 92.66 88.74 88.00 89.40 82.67 90.41 68.95 82.43\nOur W8A8 PEG-PTQ 59.43 92.66 88.53 87.92 89.42 81.25 91.07 69.31 82.45\nOur W8A8 QAT 61.27 93.00 88.80 88.95 89.44 83.74 90.48 70.40 83.26\nQ8BERT W8A8 PTQ*† 56.74 91.04 87.88 ‡ 87.66‡ 84.98‡ – 89.34 63.32 80.13§\nQ8BERT W8A8 QAT* 58.48 92.24 89.56 ‡ 89.04‡ 87.96‡ – 90.62 68.78 82.38§\nQ-BERT W8A8 QATψ – 92.88 – – – 83.87 – – –\nTable 6: 8-bit quantization results for BERT-base on development sets of the GLUE benchmark (except WNLI).\nThe metrics for these tasks can be found in the GLUE paper (Wang et al., 2018a); in all cases, higher is better.\nWe compare against Q8BERT (Zafrir et al., 2019) and Q-BERT (Shen et al., 2020). Note that these papers start\nfrom FP32 baselines with slightly different scores. *Uses FP32 Softmax, GELU and LayerNorm. †Uses dynamic\nactivation quantization. ‡Reports F1 score for MRPC, QQP and Pearson Correlation for STS-B, instead of the\ncombined metrics. §A macro-average without a score for the MNLI task. ψUses group-wise per-channel weight\nquantization with 128 groups and keeps the last fully-connected layer in FP32.\nMethod Memory\nreduction\nGLUE\nFP32 baseline ×1.00 83.06\nW6A32 PTQ ×5.33 81.41\nW4A32 PTQ ×8.00 72.31\nW4A32 AdaRound (PTQ) ×8.00 81.46\nW4A32 QAT ×8.00 82.95\nW4A8 QAT ×8.00 82.64\nW4A8, 2-bit embd. QAT ×8.85 82.29\nTable 7: Low-bit weight & token embedding quanti-\nzation results for for BERT-base on development sets\nof the GLUE benchmark. For AdaRound optimization\n(Nagel et al. 2020, our impl.), we used 1024 random\ndata sequences and 104 iterations with default hyper-\nparameters from the paper.\n6 Conclusions\nIn this paper, we explored quantization for BERT-\nlike transformers. We showed that these models\nhave unique quantization challenges – namely, high\ndynamic activations ranges that are difﬁcult to rep-\nresent with a low bit ﬁxed-point format. These\nactivations contain structured outliers in the resid-\nual connections that encourage speciﬁc model be-\nhavior, such as attending to the special [SEP] to-\nken. Motivated by our ﬁndings, we proposed three\nsolutions, one based on mixed precision quantiza-\ntion, a novel per-embedding-group quantization,\nand quantization-aware training. Each of these\nmethods has its own set of trade-offs in terms of\naccuracy, ease of use, and model size. Our tech-\nniques overcome the dynamic range issues and set\na new state-of-the-art for PTQ and per-tensor QAT\non GLUE downstream tasks. Finally, we achieved\n4-bit weight and 2-bit token embedding quantiza-\ntion with less than 0.8% drop in terms of GLUE\nscore, leading to signiﬁcant memory and compute\nsavings.\nReferences\nRon Banner, Yury Nahshan, Elad Hoffer, and Daniel\nSoudry. 2018. Post-training 4-bit quantization of\nconvolution networks for rapid-deployment. arXiv\npreprint arXiv:1810.05723.\nIz Beltagy, Matthew E Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150.\nYoshua Bengio, Nicholas Léonard, and Aaron\nCourville. 2013. Estimating or propagating gradi-\nents through stochastic neurons for conditional com-\nputation. arXiv preprint arXiv:1308.3432.\nYash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen\nBlankevoort, and Nojun Kwak. 2020. Lsq+: Im-\nproving low-bit quantization through learnable off-\nsets and better initialization. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition Workshops, pages 696–697.\nAishwarya Bhandare, Vamsi Sripathi, Deepthi\nKarkada, Vivek Menon, Sun Choi, Kushal Datta,\nand Vikram Saletore. 2019. Efﬁcient 8-bit quantiza-\ntion of transformer neural machine language trans-\nlation model. arXiv preprint arXiv:1906.00532.\n7956\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve,\nNicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. 2020. End-to-end object detection with\ntransformers. In European Conference on Computer\nVision, pages 213–229. Springer.\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu,\nHeewoo Jun, David Luan, and Ilya Sutskever. 2020.\nGenerative pretraining from pixels. In International\nConference on Machine Learning, pages 1691–1703.\nPMLR.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. arXiv preprint\narXiv:1904.10509.\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, Tamas Sar-\nlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, et al. 2020. Rethinking attention\nwith performers. arXiv preprint arXiv:2009.14794.\nYoni Choukroun, Eli Kravchik, Fan Yang, and Pavel\nKisilev. 2019. Low-bit quantization of neural net-\nworks for efﬁcient inference. In ICCV Workshops,\npages 3009–3018.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286, Florence, Italy. Association\nfor Computational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than genera-\ntors. arXiv preprint arXiv:2003.10555.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nBita Darvish Rouhani, Daniel Lo, Ritchie Zhao, Ming\nLiu, Jeremy Fowers, Kalin Ovtcharov, Anna Vino-\ngradsky, Sarah Massengill, Lita Yang, Ray Bittner,\nAlessandro Forin, Haishan Zhu, Taesik Na, Prerak\nPatel, Shuai Che, Lok Chand Koppaka, XIA SONG,\nSubhojit Som, Kaustav Das, Saurabh T, Steve Rein-\nhardt, Sitaram Lanka, Eric Chung, and Doug Burger.\n2020. Pushing the limits of narrow precision infer-\nencing at cloud scale with microsoft ﬂoating point.\nIn Advances in Neural Information Processing Sys-\ntems, volume 33, pages 10271–10281. Curran Asso-\nciates, Inc.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Łukasz Kaiser. 2018. Univer-\nsal transformers. arXiv preprint arXiv:1807.03819.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith.\n2020. Fine-tuning pretrained language models:\nWeight initializations, data orders, and early stop-\nping. arXiv preprint arXiv:2002.06305.\nLinhao Dong, Shuang Xu, and Bo Xu. 2018. Speech-\ntransformer: a no-recurrence sequence-to-sequence\nmodel for speech recognition. In 2018 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 5884–5888. IEEE.\nZhen Dong, Zhewei Yao, Amir Gholami, Michael W\nMahoney, and Kurt Keutzer. 2019. Hawq: Hessian\naware quantization of neural networks with mixed-\nprecision. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision , pages 293–\n302.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\nAn image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint\narXiv:2010.11929.\nSteven K Esser, Jeffrey L McKinstry, Deepika Bablani,\nRathinakumar Appuswamy, and Dharmendra S\nModha. 2019. Learned step size quantization. arXiv\npreprint arXiv:1902.08153.\nAngela Fan, Pierre Stock, Benjamin Graham, Edouard\nGrave, Rémi Gribonval, Hervé Jégou, and Armand\nJoulin. 2020. Training with quantization noise\nfor extreme model compression. arXiv preprint\narXiv:2004.07320.\nRohit Girdhar, Joao Carreira, Carl Doersch, and An-\ndrew Zisserman. 2019. Video action transformer\nnetwork. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition ,\npages 244–253.\nYong Guo, Yin Zheng, Mingkui Tan, Qi Chen, Jian\nChen, Peilin Zhao, and Junzhou Huang. 2019.\nNat: Neural architecture transformer for accu-\nrate and compact architectures. arXiv preprint\narXiv:1910.14488.\n7957\nSuyog Gupta, Ankur Agrawal, Kailash Gopalakrish-\nnan, and Pritish Narayanan. 2015. Deep learning\nwith limited numerical precision. In International\nconference on machine learning , pages 1737–1746.\nPMLR.\nM. Horowitz. 2014. 1.1 computing’s energy problem\n(and what we can do about it). In 2014 IEEE Inter-\nnational Solid-State Circuits Conference Digest of\nTechnical Papers (ISSCC), pages 10–14.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for nlp.\nIn International Conference on Machine Learning ,\npages 2790–2799. PMLR.\nItay Hubara, Matthieu Courbariaux, Daniel Soudry,\nRan El-Yaniv, and Yoshua Bengio. 2017. Quantized\nneural networks: Training neural networks with low\nprecision weights and activations. The Journal of\nMachine Learning Research, 18(1):6869–6898.\nForrest Iandola, Albert Shaw, Ravi Krishna, and Kurt\nKeutzer. 2020. SqueezeBERT: What can computer\nvision teach NLP about efﬁcient neural networks?\nIn Proceedings of SustaiNLP: Workshop on Simple\nand Efﬁcient Natural Language Processing , pages\n124–135, Online. Association for Computational\nLinguistics.\nBenoit Jacob, Skirmantas Kligys, Bo Chen, Meng-\nlong Zhu, Matthew Tang, Andrew Howard, Hartwig\nAdam, and Dmitry Kalenichenko. 2018. Quanti-\nzation and training of neural networks for efﬁcient\ninteger-arithmetic-only inference. In Proceedings of\nthe IEEE Conference on Computer Vision and Pat-\ntern Recognition, pages 2704–2713.\nSambhav R Jain, Albert Gural, Michael Wu, and Chris\nDick. 2019. Trained uniform quantization for accu-\nrate and efﬁcient neural network inference on ﬁxed-\npoint hardware. arXiv preprint arXiv:1903.08066 ,\n6.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2020. TinyBERT: Distilling BERT for natural lan-\nguage understanding. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n4163–4174, Online. Association for Computational\nLinguistics.\nMarcin Junczys-Dowmunt, Kenneth Heaﬁeld, Hieu\nHoang, Roman Grundkiewicz, and Anthony Aue.\n2018. Marian: Cost-effective high-quality neu-\nral machine translation in c++. arXiv preprint\narXiv:1805.12096.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and François Fleuret. 2020. Transformers are\nrnns: Fast autoregressive transformers with linear\nattention. In International Conference on Machine\nLearning, pages 5156–5165. PMLR.\nSehoon Kim, Amir Gholami, Zhewei Yao, Michael W\nMahoney, and Kurt Keutzer. 2021. I-bert:\nInteger-only bert quantization. arXiv preprint\narXiv:2101.01321.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. arXiv\npreprint arXiv:2001.04451.\nRaghuraman Krishnamoorthi. 2018. Quantizing deep\nconvolutional networks for efﬁcient inference: A\nwhitepaper. arXiv preprint arXiv:1806.08342.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nHector Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In\nThirteenth International Conference on the Princi-\nples of Knowledge Representation and Reasoning .\nCiteseer.\nDarryl Lin, Sachin Talathi, and Sreekanth Anna-\npureddy. 2016. Fixed point quantization of deep\nconvolutional networks. In International conference\non machine learning, pages 2849–2858. PMLR.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nMarkus Nagel, Rana Ali Amjad, Mart Van Baalen,\nChristos Louizos, and Tijmen Blankevoort. 2020.\nUp or down? adaptive rounding for post-training\nquantization. In International Conference on Ma-\nchine Learning, pages 7197–7206. PMLR.\nMarkus Nagel, Mart van Baalen, Tijmen Blankevoort,\nand Max Welling. 2019. Data-free quantization\nthrough weight equalization and bias correction. In\nProceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 1325–1334.\nMarkus Nagel, Marios Fournarakis, Rana Ali Am-\njad, Yelysei Bondarenko, Mart van Baalen, and\nBlankevoort Tijmen. 2021. A white paper on\nneural network quantization. arXiv preprint\narXiv:2106.08295.\nSai Prasanna, Anna Rogers, and Anna Rumshisky.\n2020. When bert plays the lottery, all tickets are\nwinning. arXiv preprint arXiv:2005.00561.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\n7958\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nVictor Sanh, Thomas Wolf, and Alexander M Rush.\n2020. Movement pruning: Adaptive sparsity by ﬁne-\ntuning. arXiv preprint arXiv:2005.07683.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W Mahoney, and Kurt\nKeutzer. 2020. Q-bert: Hessian based ultra low\nprecision quantization of bert. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence , vol-\nume 34, pages 8815–8821.\nAsa Cooper Stickland and Iain Murray. 2019. Bert\nand pals: Projected attention layers for efﬁcient\nadaptation in multi-task learning. In International\nConference on Machine Learning, pages 5986–5995.\nPMLR.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. MobileBERT:\na compact task-agnostic BERT for resource-limited\ndevices. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2158–2170, Online. Association for Computa-\ntional Linguistics.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020. Efﬁcient transformers: A survey.\narXiv preprint arXiv:2009.06732.\nMart van Baalen, Christos Louizos, Markus Nagel,\nRana Ali Amjad, Ying Wang, Tijmen Blankevoort,\nand Max Welling. 2020. Bayesian bits: Uni-\nfying quantization and pruning. arXiv preprint\narXiv:2005.07093.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of the 31st International\nConference on Neural Information Processing Sys-\ntems, pages 6000–6010.\nJesse Vig. 2019. A multiscale visualization of atten-\ntion in the transformer model. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics: System Demonstrations , pages\n37–42, Florence, Italy. Association for Computa-\ntional Linguistics.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018a.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nHanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai,\nLigeng Zhu, Chuang Gan, and Song Han. 2020a.\nHAT: Hardware-aware transformers for efﬁcient nat-\nural language processing. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7675–7688, Online. As-\nsociation for Computational Linguistics.\nKuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song\nHan. 2019. Haq: Hardware-aware automated quan-\ntization with mixed precision. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 8612–8620.\nPeiqi Wang, Xinfeng Xie, Lei Deng, Guoqi Li, Dong-\nsheng Wang, and Yuan Xie. 2018b. Hitnet: Hybrid\nternary recurrent neural network. In Proceedings of\nthe 32nd International Conference on Neural Infor-\nmation Processing Systems, pages 602–612.\nSinong Wang, Belinda Li, Madian Khabsa, Han\nFang, and Hao Ma. 2020b. Linformer: Self-\nattention with linear complexity. arXiv preprint\narXiv:2006.04768.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nBichen Wu, Yanghan Wang, Peizhao Zhang, Yuan-\ndong Tian, Peter Vajda, and Kurt Keutzer. 2018a.\nMixed precision quantization of convnets via differ-\nentiable neural architecture search. arXiv preprint\narXiv:1812.00090.\nShuang Wu, Guoqi Li, Feng Chen, and Luping Shi.\n2018b. Training and inference with integers in deep\nneural networks. arXiv preprint arXiv:1802.04680.\nChen Xu, Jianqiang Yao, Zhouchen Lin, Wenwu\nOu, Yuanbin Cao, Zhirong Wang, and Hong-\nbin Zha. 2018. Alternating multi-bit quantiza-\ntion for recurrent neural networks. arXiv preprint\narXiv:1802.00150.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nOﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8bert: Quantized 8bit bert.\narXiv preprint arXiv:1910.06188.\n7959\nDongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and\nGang Hua. 2018. Lq-nets: Learned quantization for\nhighly accurate and compact deep neural networks.\nIn Proceedings of the European conference on com-\nputer vision (ECCV), pages 365–382.\nShuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou,\nHe Wen, and Yuheng Zou. 2016. Dorefa-net:\nTraining low bitwidth convolutional neural net-\nworks with low bitwidth gradients. arXiv preprint\narXiv:1606.06160.\nYiren Zhou, Seyed-Mohsen Moosavi-Dezfooli, Ngai-\nMan Cheung, and Pascal Frossard. 2018. Adap-\ntive quantization for deep neural network. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, volume 32.\nFeng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu,\nYanfei Wang, Zhelong Li, Xiuqi Yang, and Junjie\nYan. 2020. Towards uniﬁed int8 training for con-\nvolutional neural network. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 1969–1979.\n7960\nA Why do these outliers exist?\nTo better understand why transformer models learn\nthese peculiar outliers, we look at what happens\nwith those outliers when they proceed to the next at-\ntention layer. We visualize the attention mechanism\nfor one of the attention heads in the problematic\n11th layer of BERT-base in Figure 4. We can see\nthat most of the tokens in this attention head attend\nto special [SEP] tokens. Furthermore, from Fig-\nure 4b we see a similar consistent vertical pattern\n(indicated by black arrows) as we saw from the\nper-embedding graphs for FFN’s input and output\nactivation tensors (see Figure 2b in paper). It means\nthe attention mechanism generates such queries and\nkey vectors that the decision of attending to special\nseparator tokens is determined by only a few desig-\nnated neurons. It suggests that structured outliers\nin residual connections lead to structured outliers\nin query-key multiplications, causing most tokens\nto attend to the separator token.\nClark et al. (2019) has shown that in BERT-\nlike transformer models, attending to the spe-\ncial [SEP] token is essentially a “no-op” for at-\ntention heads that cannot extract patterns they were\ntrained to look for from the speciﬁc passage of text.\nClark et al. (2019) also showed that such behavior\nis quite common: often, more than a half of the\nhead’s attention is on special tokens, speciﬁcally in\ndeeper layers.\nWe hypothesize that such an attention pattern\nseems to be a useful one to obtain a good predictive\nperformance, while the structured outliers merely\nhelp to facilitate this behavior. These outliers caus-\ning such a high dynamic range for activations likely\nemerged as a result of speciﬁc architectural choices\n(e.g., large fully-connected layers) and long pre-\ntraining with no explicit activation regularization\napplied.\nB Experimental details\nB.1 FP32 ﬁne-tuning details\nWe use pre-trained checkpoint for BERT-base\n(uncased, 109M parameters) from HuggingFace\nrepository (Wolf et al., 2020). We follow\na standard ﬁne-tuning practices from (Devlin\net al., 2019) and https://github.com/\nhuggingface/transformers. Each data\npoint is tokenized and truncated to the maximum\nsequence length of 128. Shorter sequences are\npadded to the same length of 128 using a spe-\ncial [PAD] token. We ﬁne-tune for 3 epochs using\nAdam for all tasks. Learning rate is initially set to\nits maximum value and is linearly decayed to zero\nby the end of ﬁne-tuning. We tune the batch size\nand maximum value of learning rate individually\nper task from the following search space:\n• batch size: {32, 64} for bigger tasks (QQP,\nMNLI, QNLI) and {8, 16, 32, 64} for the rest,\n• learning rate: {2,3,4,5}e-5.\nWe repeat every experiment 5 times with different\nrandom seeds and select the conﬁguration with the\nbest median score on the development set for the\nrespective task. These conﬁgurations are shown\nTask Learning rate Batch size\nCoLA 2e-05 32\nSST-2 2e-05 16\nMRPC 5e-05 8\nSTS-B 4e-05 32\nQQP 4e-05 64\nMNLI 2e-05 16\nQNLI 2e-05 32\nRTE 3e-05 8\nTable 8: Hyper-parameters for FP32 BERT-base ﬁne-\ntuning on GLUE downstream tasks.\nin Table 8. Quantization is always applied to the\nmedian checkpoint for the respective task.\nWe exclude the problematic WNLI\ntask (Levesque et al., 2012), as it has rela-\ntively small dataset and shows an unstable\nbehaviour (Dodge et al., 2020), in particular due\nto several issues with the way the dataset was\nconstructed2.\nB.2 Range setting for 8-bit post-training\nquantization\nWe select the best range estimators from the fol-\nlowing search space:\n• weights: {min-max, MSE};\n• activations: {current min-max, running min-\nmax, MSE}.\nFor activations, we also select the best batch size\nand number of batches from {1,4,16} (except cur-\nrent min-max, for which only a single batch is\n2See https://gluebenchmark.com/faq for details.\n7961\n(a)\n (b)\nFigure 4: Visualization of the attention pattern in 6th attention head of 11th layer in BERT-base, computed on ﬁrst\ndata sequence from the MNLI development set. (a) visualization of the attention weights (attention probabilities)\nbetween the tokens. Thickness of line between the query vector qi of i-th token on the left and key vector kj\nof j-th token on the right is proportional to exp\n(\nqi·kj√\nd\n)\n. To generate this ﬁgure, we used “head view” from the\nBertViz library (Vig, 2019). (b) a “deconstruction” of the behavior on the left in terms of elementwise query-key\nvector multiplications. Values in red indicate high positive values, while values in blue indicate negative values.\nWe used “neuron view” from the BertViz library to generate this ﬁgure.\nused). For running min-max, we use the momen-\ntum coefﬁcient of 0.9. We repeat every experiment\n5 times with different random seeds and select\nthe conﬁguration with the best median score on\nthe development set for the respective task. Best\nTask Weights Activations (bs, nb)\nCoLA min-max running min-max (1, 4)\nSST-2 MSE running min-max (4, 16)\nMRPC MSE running min-max (16, 16)\nSTS-B min-max running min-max (1, 16)\nQQP min-max running min-max (16, 16)\nMNLI min-max running min-max (1, 16)\nQNLI min-max running min-max (1, 16)\nRTE MSE current min-max (1)\nTable 9: Best range estimators for post-training quanti-\nzation of BERT-base on GLUE tasks (bs = batch size,\nnb = number of batches).\nconﬁgurations for joint weight and activation 8-bit\npost-training quantization are listed in Table 9.\nB.3 W8A8 QAT hyper-parameters\nHyper-parameters for W8A8 quantization-aware\ntraining are listed in Table 10.\nB.4 W4A8 QAT hyper-parameters\nHyper-parameters for W4A8 quantization-aware\ntraining are listed in Table 11.\nTask LR BS E D\nCoLA { 2, 3, 4}e-5 { 16, 32} { 3, 6} { 0, 0.1}\nSST-2 {1, 2, 3}e-5 {16, 32} { 3, 6} {0, 0.1}\nMRPC { 1, 2, 3}e-5 { 8} { 3, 6} {0, 0.1}\nSTS-B {2, 4, 8}e-5 { 16, 32} {3, 6} {0, 0.1}\nQQP { 4, 5, 6}e-5 { 32} { 3} {0, 0.1}\nMNLI { 2, 3, 4}e-5 { 16, 32} { 3} {0, 0.1}\nQNLI {2, 3, 4}e-5 {16, 32} { 3} { 0, 0.1}\nRTE {1, 3, 5}e-5 { 8} {3, 6} {0, 0.1}\nTable 10: Hyper-parameters for 8-bit QAT for BERT-\nbase on development sets of the GLUE benchmark (LR\n= maximum learning rate, BS = batch size, E = number\nof epochs, D = self-attention dropout rate). Values in\nbold indicate the best conﬁguration.\nC Detailed results for low-bit weight and\nembedding quantization\nDetailed results for low-bit weight and token em-\nbedding quantization for BERT-base on develop-\nment sets of the GLUE benchmark (including per-\ntask scores) are summarized in Table 12.\nD Additional graphs from problem\ninvestigation\nD.1 Per-embedding outliers for all layers in\nBERT-base\nWe visualize per-embedding outliers in FFN’s input\nand output for all layers in BERT-base computed on\nﬁrst ten data sequences from the development set\n7962\nTask LR BS E\nCoLA { 2, 3, 4}e-5 { 16, 32} { 3, 6}\nSST-2 {2, 3, 4}e-5 {16, 32} { 3, 6}\nMRPC { 2, 3, 4}e-5 { 8} { 3, 6}\nSTS-B {4, 6, 8}e-5 {16, 32} {3, 6}\nQQP {4, 5.5, 7}e-5 { 32} { 3, 6}\nMNLI {3, 4, 5}e-5 { 16, 32} { 3, 6}\nQNLI { 2, 3, 4}e-5 { 16, 32} { 3, 6}\nRTE {3, 4, 5}e-5 { 8} {3, 6}\nTable 11: Hyper-parameters for W4A8 QAT for BERT-\nbase on development sets of the GLUE benchmark (LR\n= maximum learning rate, BS = batch size, E = number\nof epochs). Values in bold indicate the best conﬁgura-\ntion.\nof MNLI (Figure 5), STS-B (Figure 6) and MRPC\n(Figure 7). We see that only a few designated em-\nbedding dimensions generate outliers across many\ndata points. It suggests that such behavior is already\npre-determined by the weights and embeddings of\nthe pre-trained BERT model.\nD.2 Activation tensors for different\narchitectures\nWe shot that dynamic range issue is present in mul-\ntiple architectures and training objectives:\n• BERT-base in Figure 8,\n• BERT-large in Figure 9,\n• RoBERTa-base in Figure 10,\n• DistilRoBERTa-base in Figure 11,\n• MobileBERT-base in Figure 12.\nIn all cases, we used pre-trained checkpoints from\nHuggingFace library (Wolf et al., 2020).\n7963\nMethod CoLA SST-2 MRPC STS-B QQP MNLI QNLI RTE GLUE\nFP32 baseline 57.27 93.12 88.36 89.09 89.72 84.91 91.58 70.40 83.06\nOur W8A32, 6-bit embd. PTQ 58.65 92.32 88.36 89.06 89.74 84.62 91.32 68.95 82.88\nOur W8A32, 4-bit embd. PTQ 57.43 92.32 88.79 89.02 89.70 84.58 91.43 68.59 82.73\nOur W8A32, 2-bit embd. PTQ 57.22 92.32 86.99 88.87 89.63 84.45 91.47 68.23 82.40\nOur W6A32 PTQ 56.23 91.86 86.70 87.76 88.94 82.39 88.83 68.23 81.41\nOur W4A32 PTQ 43.06 90.83 84.90 83.07 79.37 68.16 79.68 50.18 72.31\nOur W4A32 AdaRound (PTQ) 54.56 92.32 87.53 87.91 88.30 81.61 90.17 69.31 81.46\nOur W4A32 QAT 58.31 92.49 87.58 89.02 89.78 84.29 91.40 70.76 82.95\nOur W4A8 QAT 57.22 92.32 87.77 89.13 89.64 83.69 91.29 70.04 82.64\nOur W4A8, 2-bit embd. QAT 56.08 91.74 87.59 89.19 89.56 83.68 90.79 69.67 82.29\nQ-BERT W4A8 QAT* – 85.67 – – – 76.85 – – –\nQ-BERT W4A8 QAT*† – 92.66 – – – 84.03 – – –\nTable 12: Low-bit weight and token embedding quantization results for BERT-base on development sets of the\nGLUE benchmark. We compare against Q-BERT (Shen et al., 2020). Note that this work starts from FP32\nbaselines with slightly different scores. *Keeps the last fully-connected layer in full precision. †Uses group-wise\nper-channel weight quantization with 128 groups (of size 6 each).\n7964\nFigure 5: Visualization of activation tensor outliers in BERT-base FFN’s input and output across embedding di-\nmension for the ﬁrst ten data sequences in the MNLI development set. Dark grey color indicates values that exceed\nsix standard deviations from the mean of the activation tensor.\n7965\nFigure 6: Visualization of activation tensor outliers in BERT-base FFN’s input and output across embedding dimen-\nsion for the ﬁrst ten data sequences in the STS-B development set. Dark grey color indicates values that exceed six\nstandard deviations from the mean of the activation tensor.\n7966\nFigure 7: Visualization of activation tensor outliers in BERT-base FFN’s input and output across embedding dimen-\nsion for the ﬁrst ten data sequences in the MRPC development set. Dark grey color indicates values that exceed\nsix standard deviations from the mean of the activation tensor.\n7967\n(a) input\n (b) output\nFigure 8: Activation distributions of FFN’s input (a) and output (b) in second to the last layer for BERT-base,\nevaluated on ﬁrst ten data sequences from development sets of GLUE downstream tasks (full-precision). In each\nsub-plot, left-to-right, top-to-bottom: CoLA, MNLI, MRPC →QNLI, QQP, RTE→SST-2, STS-B, WNLI.x-axis:\nindex of data sequence. y-axis: the range (note the scales are different for the input and the output).\n(a) input\n (b) output\nFigure 9: Activation distributions of FFN’s input (a) and output (b) in second to the last layer for BERT-large,\nevaluated on ﬁrst ﬁve data sequences from development sets of GLUE downstream tasks (full-precision). In each\nsub-plot, left-to-right, top-to-bottom: CoLA, MNLI, MRPC →QNLI, QQP, RTE→SST-2, STS-B, WNLI.x-axis:\nindex of data sequence. y-axis: the range (note the scales are different for the input and the output).\n7968\n(a) input\n (b) output\nFigure 10: Activation distributions of FFN’s input (a) and output (b) in second to the last layer for RoBERTa-base,\nevaluated on ﬁrst ﬁve data sequences from development sets of GLUE downstream tasks (full-precision). In each\nsub-plot, left-to-right, top-to-bottom: CoLA, MNLI, MRPC →QNLI, QQP, RTE→SST-2, STS-B, WNLI.x-axis:\nindex of data sequence. y-axis: the range (note the scales are different for the input and the output).\n(a) input\n (b) output\nFigure 11: Activation distributions of FFN’s input (a) and output (b) in second to the last layer for DistilRoBERTa-\nbase, evaluated on ﬁrst ﬁve data sequences from development sets of GLUE downstream tasks (full-precision). In\neach sub-plot, left-to-right, top-to-bottom: CoLA, MNLI, MRPC →QNLI, QQP, RTE →SST-2, STS-B, WNLI.\nx-axis: index of data sequence. y-axis: the range (note the scales are different for the input and the output).\n7969\n(a) input\n (b) output\nFigure 12: Activation distributions of FFN’s input (a) and output (b) in second to the last layer for MobileBERT-\nbase, evaluated on ﬁrst ten data sequences from development sets of GLUE downstream tasks (full-precision). In\neach sub-plot, left-to-right, top-to-bottom: CoLA, MNLI, MRPC →QNLI, QQP, RTE →SST-2, STS-B, WNLI.\nx-axis: index of data sequence. y-axis: the range (note the scales are different for the input and the output).",
  "topic": "Quantization (signal processing)",
  "concepts": [
    {
      "name": "Quantization (signal processing)",
      "score": 0.6889837980270386
    },
    {
      "name": "Computer science",
      "score": 0.6817563772201538
    },
    {
      "name": "Transformer",
      "score": 0.5745789408683777
    },
    {
      "name": "Computer engineering",
      "score": 0.4569741487503052
    },
    {
      "name": "Residual",
      "score": 0.42902565002441406
    },
    {
      "name": "Algorithm",
      "score": 0.3385149836540222
    },
    {
      "name": "Computer hardware",
      "score": 0.3242078423500061
    },
    {
      "name": "Electrical engineering",
      "score": 0.17787861824035645
    },
    {
      "name": "Engineering",
      "score": 0.1306121051311493
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I19268510",
      "name": "Qualcomm (United Kingdom)",
      "country": "GB"
    }
  ]
}