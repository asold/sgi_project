{
  "title": "BERT2Code: Can Pretrained Language Models be Leveraged for Code Search?",
  "url": "https://openalex.org/W3152617716",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287337532",
      "name": "Ishtiaq, Abdullah Al",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2908989527",
      "name": "Hasan, Masum",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4284856113",
      "name": "Haque, Md. Mahim Anjum",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287337533",
      "name": "Mehrab, Kazi Sajeed",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287337531",
      "name": "Muttaqueen, Tanveer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225965230",
      "name": "Hasan, Tahmid",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743110461",
      "name": "Iqbal Anindya",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2752799963",
      "name": "Shahriyar, Rifat",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964150020",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2971499595",
    "https://openalex.org/W2973529529",
    "https://openalex.org/W1572063013",
    "https://openalex.org/W2156387975",
    "https://openalex.org/W2402619042",
    "https://openalex.org/W2805788202",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2009125891",
    "https://openalex.org/W1560851690",
    "https://openalex.org/W2907636479",
    "https://openalex.org/W2131744502",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2911550516",
    "https://openalex.org/W3100105221",
    "https://openalex.org/W2968179027",
    "https://openalex.org/W2087309216",
    "https://openalex.org/W2180107243",
    "https://openalex.org/W2884276923",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W2766736793",
    "https://openalex.org/W2794601162",
    "https://openalex.org/W2783455372",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3045289171",
    "https://openalex.org/W2994865335",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2990362151",
    "https://openalex.org/W2100591395",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "Millions of repetitive code snippets are submitted to code repositories every day. To search from these large codebases using simple natural language queries would allow programmers to ideate, prototype, and develop easier and faster. Although the existing methods have shown good performance in searching codes when the natural language description contains keywords from the code, they are still far behind in searching codes based on the semantic meaning of the natural language query and semantic structure of the code. In recent years, both natural language and programming language research communities have created techniques to embed them in vector spaces. In this work, we leverage the efficacy of these embedding models using a simple, lightweight 2-layer neural network in the task of semantic code search. We show that our model learns the inherent relationship between the embedding spaces and further probes into the scope of improvement by empirically analyzing the embedding methods. In this analysis, we show that the quality of the code embedding model is the bottleneck for our model's performance, and discuss future directions of study in this area.",
  "full_text": "BERT2Code: Can Pretrained Language Models\nbe Leveraged for Code Search?\nAbdullah Al Ishtiaq⋆, Masum Hasan⋆, Md. Mahim Anjum Haque,\nKazi Sajeed Mehrab, Tanveer Muttaqueen, Tahmid Hasan,\nAnindya Iqbal, and Rifat Shahriyar\nBangladesh University of Engineering and Technology, Dhaka, Bangladesh\n1505080.aai@ugrad.cse.buet.ac.bd, masum@ra.cse.buet.ac.bd, {mahimanzum,\nksmehrab, tanveer.mutta}@gmail.com, {tahmidhasan, anindya,\nrifat}@cse.buet.ac.bd\nMillions of repetitive code snippets are submitted to code repositories every\nday. To search from these large codebases using simple natural language queries\nwould allow programmers to ideate, prototype, and develop easier and faster.\nAlthough the existing methods have shown good performance in searching codes\nwhen the natural language description contains keywords from the code [21],\nthey are still far behind in searching codes based on the semantic meaning of\nthe natural language query and semantic structure of the code. In recent years,\nboth natural language and programming language research communities have\ncreated techniques to embed them in vector spaces. In this work, we leverage\nthe eﬃcacy of these embedding models using a simple, lightweight 2-layer neural\nnetwork in the task of semantic code search. We show that our model learns the\ninherent relationship between the embedding spaces and further probes into the\nscope of improvement by empirically analyzing the embedding methods. In this\nanalysis, we show that the quality of the code embedding model is the bottleneck\nfor our model’s performance, and discuss future directions of study in this area.\n1 Introduction\nSince the inception of computers, researchers have been dreaming of program-\nming them with natural language instructions only [38]. Although this problem\nis far from solved, a subset of this problem, ‘Semantic Code Search’, has gained\noverwhelming traction in recent years [8,13,17,18,21,35,41].\nSemantic code search refers to searching for a source code with a natural\nlanguage query by utilizing the inherent meaning of both the source code and\nthe query. It has a signiﬁcant impact on a wide range of computer science appli-\ncations. For example, searching for source code on websites like Stack Overﬂow\nis an integral part of modern software development [30,37]. Easily ﬁnding the\nrelevant code snippet can remarkably reduce time, eﬀort, and project cost. Thus,\nfor decades, researchers have been trying to search source codes automatically\n[32].\n⋆ These authors contributed equally to this work.\narXiv:2104.08017v1  [cs.SE]  16 Apr 2021\n2 Abdullah Al Ishtiaq et al.\nWith the rapid advancement of deep learning in recent years, there have been\nmany attempts to use neural network based code search methods [8,13,17,21].\nHowever, these methods often fail to capture the semantic meanings of the query\nand the source code, and instead heavily rely on common tokens between the\ntwo [8,21,35].\nIn recent years, there has been tremendous advancement in embedding mod-\nels of natural language and programming language. They have shown notable\nperformance in capturing the semantic meanings of sentences and the source\ncodes in embedding spaces [2,11,13,33]. In addition, translating from one sen-\ntence embedding space to another has shown promising results in Neural Machine\nTranslation [9]. We have incorporated this idea to semantic code search by lever-\naging the eﬃcacy of embedding models. In this work, we propose BERT2Code – a\nsimple neural network based code search method that utilizes the generalization\ncapabilities of the state-of-the-art pretrained natural language and source code\nembedding models, namely Sentence-BERT [33], Code2Vec [2], and CodeBERT\n[13]. We show that our method can capture the inherent relationship between\nthe two embedding spaces to a reasonable extent and achieve 15.478% MRR in\ncode search. With manual and statistical analysis of the embedding models, we\nﬁnd that leveraging the full potential of the embeddings requires code embedding\nmodels with better quality and more generalization capability.\nWe organize our paper by ﬁrst deﬁning relevant techniques and terminologies\nused throughout our paper (Section 2). In the section that follows, we describe\nour model in detail (Section 3). In the next section, we outline the results and\nﬁndings from our study (Section 4). We conclude the paper by discussing related\nworks from the literature (Section 5) and with a brief summary of our work in\nthe conclusion (Section 6).\n2 Background\nIn this section, we describe the necessary concepts and technologies relevant to\nour work.\n2.1 Sentence Embedding\nSentence Embedding is referred to as the vector representations of sentences.\nSeveral classical and neural approaches have been proposed for generating sen-\ntence embeddings over the years. [3,11,23,26,29,33,42].\nBidirectional Encoder Representations from Transformers (BERT) [11] is\na Transformer [39] based language model that is trained for two tasks of predict-\ning masked tokens and predicting whether two given sentences are consecutive\nsentences or not.\nThese cleverly designed tasks allow the model to be trained on large volumes\nof easily accessible unlabeled data from the internet, and consequently, with\nsuﬃcient amount of data, the model learns to create rich semantic latent repre-\nsentations of the input texts. With ﬁnetuning, these representations can be used\nBERT2Code 3\nto achieve state-of-the-art performance and even above human-level performance\n[11] in numerous text analysis tasks.\nBased on the latent text representations created by BERT, Reimers et al.\n[33] has proposed a sentence embedding method named Sentence-BERT or\nSBERT, by performing a pooling operation on the output of BERT and ﬁne-\ntuning BERT using siamese and triplet network structures. This resulted in\nupdated sentence embeddings that are more meaningful and can be compared\nusing cosine-similarity. Regardless of its name, this method is also capable of\nembedding multiple sentences.\n2.2 Code Embedding\nThe idea of generating embeddings or vectors from source code has showed\nencouraging results recently. This is largely due to the success of deep learning\nbased latent representations of source codes.\nCode2Vec [2] is a major contribution in the area of code embedding models.\nIt represents code snippets as continuous distributed vectors or code embeddings.\nA given code snippet is decomposed into its abstract syntax tree and the paths\nin the tree are represented by vectors which are then aggregated into a single em-\nbedding using the attention mechanism [5]. The authors trained their model with\n12 million Java methods for the end task of predicting corresponding method\nnames. Their model is able to generalize further than the original target and\neven predict method names that are unobserved in the training data.\nCodeBERT [13] is another recent contribution in the concerned area. It\nuses exactly the same architecture as RoBERTa-base [28] along with multi-layer\nbidirectional Transformer [39], and is pretrained using both unimodal and bi-\nmodal data from the CodeSearchNet Challenge dataset [21]. It has shown to\noutperform the baseline models of the challenge, and improves the performance\nin code document generation task as well.\n2.3 Similarity Search\nSimilarity search is the problem of ﬁnding a set of objects that are the most\nsimilar to a given object. For embeddings or vectors, the problem boils down to\nﬁnding vectors from a given dataset that are closest to the query-vector in terms\nof Euclidean or cosine distance. Similarity search using vectors works particularly\nwell because the vector representation of objects is designed to produce similar\nvectors for similar objects [16,26].\nThe FAISS1 library, developed by Facebook AI Research, can eﬃciently per-\nform similarity search and clustering on embeddings or vectors. This work has\nproven to largely reduce the time required for searching the nearest neighbors of\na vector in a high dimensional embedding space when GPU is available [22]. In\naddition, this library is highly scalable and, therefore, one can easily perform a\nk-nearest-neighbor search within a reasonable amount of time even in a dataset\nof size in billions.\n1 https://github.com/facebookresearch/faiss\n4 Abdullah Al Ishtiaq et al.\n3 Methodology\nIn this section, we describe the dataset used, the techniques to create the em-\nbeddings of natural language and code, the architecture of the neural network,\nand the process of training and evaluating the network.\n3.1 Problem Deﬁnition\nGiven a natural language (NL) query, and a code snippet corpus, the task is\nto ﬁnd the code snippet that best matches the NL query in terms of semantic\nmeanings.\nFor a programming language LP , we consider a K1 dimensional embedding\nspace SP , a subset of IRK1 , where every point in the vector space represents a\nprogram P written in language LP . VP is an embedding of a program P, and VP\nis referred to as a code vector. Program P is also referred to as a source code.\nSimilarly, for natural language LN, we consider a K2 dimensional embedding\nspace SN, a subset of IRK2 , where every point in the vector space represents a\ndescription N of a program in natural language LN. VN is an embedding of a\nnatural language statement N, and VN is referred to as an NL vector.\nGiven the embedding spaces SP and SN, we aim to ﬁnd a transformation T\nthat maps every vector in space SN to its programming language equivalent to\nSP with a very small number of data samples in SN and SP . Given a natural\nlanguage description N, we ﬁnd its vector representation VN and by applying\ntransformation T, we obtain a code vector ˆVP . After that, we ﬁnd its closest n\ncode embeddings {VP1 ,...,V Pn} from a predeﬁned set of code vectors where n ϵ\nN is an arbitrary constant. Finally, we return the codes {P1,...,P n} associated\nwith the code vectors.\n3.2 Dataset\nIn this study, we build a model to map a natural language query to its pro-\ngramming language counterpart. Hence, our task requires a dataset where each\ndatapoint consists of a source code and a Natural Language (NL) query that\ndescribes the functionality of the source code in adequate detail. Upon exploring\nthe literature, we ﬁnd the CodeSearchNet Challenge [21] dataset to be of the\nmost suitable for our use-case. The dataset contains in total 2.3 million code-\ndescription pairs from 6 diﬀerent programming languages. However, in our study,\nwe only limit our scope to 543k given samples from Java programming language.\nWe use original train-valid-test split from the dataset. The detailed process of\nprocurement and cleaning of the dataset is described in the original article [21].\n3.3 Generating Sentence Embeddings\nIn order to generate sentence embeddings, we have used Sentence-BERT (SBERT)\n[33]. Our used model employs the BERT LARGE model, uses the MEAN pooling\nBERT2Code 5\n  \nNL:     Check   if   string   is   a   valid   Byte   .   \nprivate     static     final     boolean     checkByte ( String    s)   throws   AttributeBadValueException{   \n    try    {   \n      //   Byte.parseByte()   can't   be   used   because   values   >   127   are   allowed   \n      short     val    =    Short . parseShort (s);   \n      if (DebugValueChecking)   {   \n        log . debug ( \"Attribute.checkByte()   -   string:   '\"    +   s   +    \"'     value:   \"    +   val);   \n     }   \n      if (val   >    0xFF    ||   val   <    0 )    return     false ;   \n      else   return     true ;   \n   }    catch    ( NumberFormatException     e )   {   \n      throw     new     AttributeBadValueException ( \"`\"    +   s   +    \"'   is   not   a   Byte   value.\" );   \n   }   \n}   \nFig. 1.Sample natural language description and source code pair from the CodeSearch-\nNet [21] dataset\nstrategy on the output, and is ﬁnetuned with Natural Language Inference (NLI)\ndatasets [6,40] using a 3-way softmax classiﬁer objective function. We use this\nparticular model because it shows the greatest average accuracy in generating\nmeaningful sentence embeddings [33]. Creating the NL vectors using pre-trained\nSBERT has taken 4.4s on an average per 1000 samples in a workstation with\nIntel Core i5-9600k 3.70 GHz CPU and NVIDIA GeForce RTX 2070 SUPER\nGPU. All of the following computations are also done in the same workstation.\n3.4 Generating Code Embeddings\nAt ﬁrst, we use Code2Vec [2] to generate code embeddings from our data. Each of\nthe raw Java methods from the dataset is passed through the Code2Vec prepro-\ncessor. Then, we pass the preprocessed source programs through the pretrained\nCode2Vec model and generate the corresponding code vectors. Generating these\ncode vectors has taken 2030s on an average per 1000 samples.\nIn addition to Code2Vec, we have generated code vectors using the Code-\nBERT [13] model which can be used in other downstream tasks. Generating the\ncode vectors in this case has taken 44.25s on an average per 1000 samples.\n3.5 Training a Neural Network to Transform NL Vectors to Code\nVectors\nOnce the natural language vectors and their corresponding code vectors are\nproduced, we train a feed-forward neural network [34] as the transformation\nfunction T between an NL vector and a code vector. The neural network has 2\nhidden layers with size 1280 and 896 respectively. While the size of the input\nlayer is 1024, and the size of the output layer is 384 in case of Code2Vec and\n6 Abdullah Al Ishtiaq et al.\n768 in case of CodeBERT. Both of the hidden layers use ReLU [15,19], a non-\nlinear activation function, and the output layer uses linear activation. As the loss\nfunction for our network, we use Euclidean distance with max-margin approach.\nIn other words, over the training period, the network tries to reduce the L2\ndistance between the network prediction and the original code vector while,\nsimultaneously, trying to increase distances from other datapoints in the code\nembedding space. We use constant learning rate of 10 −5 and the batch size is\nkept at 16. We have also used early stopping mechanism [25] in order to prevent\nover-ﬁtting. The training continued for 207 and 170 epochs at 5h 37m 40s and\n4h 22m 32s for the two networks respectively. The network is implemented with\nthe PyTorch Framework [31]. The training procedure is visualized in Figure 2.\n \nfloat\n \nsquare(\nfloat \nx){\n \nreturn\n \nx*x;\n \n}\nInput \nLayer \n(1024)\nHidden \nLayer \n1 \n(1280)\nHidden \nLayer \n2\n(896)\nOutput \nLayer\n(384 \n/ \n768) \nReLU\nReLU\nLinear\nInput \nEmbedding \n(1024)\nOutput \nEmbedding\n(384 \n/ \n768)\nSimple \nNeural \nNetwork\nCalculate \nthe \nsquare \nof \na \ngiven \nnumber\nBERT\nCode2Vec \n/ \nCodeBERT\nFig. 2.Training a feed-forward neural network to transform natural language embed-\nding from BERT to code embedding (BERT ﬁgure [1])\n3.6 Predicting Code for Unseen Query\nWhen the training phase is complete, any new natural language query is, at ﬁrst,\nconverted into an NL vector. Then, using a trained neural network, we transform\nthe NL vector into a code vector prediction. We then ﬁnd n code snippets that\nhave code vectors from a given corpus that are closest in terms of L2 distance to\nthe predicted vector, where nis any suitable number. For accelerating this step,\nwe have used FAISS [22]. The prediction methodology is presented in Figure 3.\n3.7 Evaluation Criteria\nWe evaluate the networks’ performance with the test set of CodeSearchNet chal-\nlenge [21] that were unseen during the training phase. In this case, we ﬁx a set\nfor each test sample with 999 distractor datapoints in the same way as the orig-\ninal paper. Then, for each query in this set, we ﬁnd the closest code vectors and\nBERT2Code 7\ncorresponding source codes, and calculate the Mean Reciprocal Rank (MRR)\nscore [10], which is used by several related works [8,13,21,35]. The Results from\nthis endeavor are described in Section 4.\nAll \nCode \nVectors\nSimilarity \nSearch\nInput \nLayer \n(1024)\nHidden \nLayer \n1 \n(1280)\nHidden \nLayer \n2\n(896)\nOutput \nLayer \n(384 \n/ \n768)\nReLU\nReLU\nLinear\nFind the multiplication \nof \ntwo \ngiven \nnumbers\nBERT\nInput \nEmbedding \n(1024)\nOutput \nEmbedding\n(384 \n/ \n768)\n Simple Neural Network\nCode2Vec \n/ \nCodeBERT\n \nAll \nSouce \nCodes\nFig. 3.Searching network prediction for a new query from all code embeddings in our\ndataset\n4 Results\nThe results obtained from BERT2Code are shown in Table 1. In our experiment,\nBERT2Code model with Code2Vec [2] embeddings have performed better than\nwith CodeBERT [13] embeddings in semantic code search. It is also evident that\nour model learns the inherent relationship between the embedding spaces as it\nperforms almost 20 times better in the best setting than a random model.\nTable 1.Semantic code search with BERT2Code\nMethod Output Layer Trainable Epochs MRR (%)\nSize Parameters\nBERT2Code With 384 2,804,224 207 15.478Code2Vec Embeddings\nBERT2Code With 768 3,148,672 170 9.986CodeBERT Embeddings\nRandom Model - - - 0.74\n4.1 Implications of the Results\nIn recent years, pretrained embedding models have helped researchers achieve\noutstanding performances in numerous natural language processing tasks [14,27].\nIn particular, Cheng and Callison-Burch [9] has shown that a simple feed-forward\nneural network can perform considerably well in Neural Machine Translation\nusing BERT pretrained models [11] and achieved a near SOTA performance in\nthe Multi30k English-to-German translation dataset [12].\n8 Abdullah Al Ishtiaq et al.\nWe adopted a similar approach by trying to leverage available state-of-the-\nart embedding methods for NL queries and code snippets with a lightweight\nneural network. Our study ﬁnds that the model is technologically feasible, can\nlearn the inherent semantic relationship between NL queries and code snippets,\nand can often ﬁnd good code suggestions. However, at this moment, it is not\nthe best approach to code search [13,21]. With these ﬁndings, we argue that as\nnatural language embeddings have already proven to be readily useful in similar\napproaches [9], in our case, code embeddings can be inferred the bottleneck for\nour method.\n4.2 Quality of Code and Sentence Embeddings\nOur simple, and lightweight neural network based code search method largely\ndepends on the embedding models’ capability to capture the semantic meanings\nof source codes and NL queries. We evaluate the code and the sentence embed-\ndings by manually rating queries and source codes based on semantic similarity\nand then comparing these manual scores with the distances in embedding spaces.\nWe perform this analysis on the DeepCom dataset [20] to free ourselves from\nany bias from previous experiments. We generate embeddings with SBERT [33]\nand with Code2Vec [2] (as it performed better than CodeBERT [13]) models\nfor the 505,188 code-NL datapoints from DeepCom. We sample 150 pairs of\ndatapoints that have low Euclidean distance code embedding space and another\n150 pairs of datapoints that have low Euclidean distance in the NL embedding\nspace. We then give a similarity score for the two source codes and another\nsimilarity score for the NL queries for each of the 300 pairs. The scores are\nintegers ranging between 0 to 10. The scoring is done twice independently by\ntwo authors based on the their manual observation. Their ratings for the source\ncodes and the NL queries have 0.8422 and 0.8760 Pearson’s correlation [36]\nrespectively. The scores from the two authors are averaged to get our manual\nsemantic similarity score.\nTable 2.Comparison between Manual Similarity Scores and the Corresponding Dis-\ntances in Embedding Spaces in Pearson’s Correlation Coeﬃcient and p-value\nMeasured Variable Pairs Correlation p-value\nManual NL Similarity NL Vectors L2 Distance -0.772 < 10−6\nManual Code Similarity Code Vectors L2 Distance -0.444 < 10−6\nFrom Table 2, the code vectors have a lower correlation coeﬃcient with the\nmanual scores than NL vectors. We calculate thep-values for the similarity scores\nwith the null hypotheses that the manual similarity scores do not correlate with\nthe NL and the code embedding similarities. Both the p-values are less than 10−6\nwhich reject the null hypotheses and show that the correlations are statistically\nsigniﬁcant. This ﬁnding supports our assumption that embeddings can be used\nfor learning complex relations between source codes and NL queries. However,\nBERT2Code 9\nthe signiﬁcantly low correlation for the code embeddings similarity indicates\nthat better code embedding methods than Code2Vec [2] and CodeBERT [13]\nare necessary to exploit the capability of our method fully.\n5 Related Works\nOur results indicate that better code embeddings are needed to be readily us-\nable in semantic code search. Recently, Kang et al. [24] has also found that\nthe Code2Vec [2] embeddings fail to generalize in the tasks of code comments\ngeneration, code authorship identiﬁcation, and code clones detection.\nBriem et al. [7] used distributed representations by Code2Vec [2] in the task\nof bug detection. Arumugam [4] trained a Code2Vec bag-of-paths embedding\nmodel for the CodeSearchNet Challenge [21].\nGu et al. [17] introduced CODEnn, which jointly embeds descriptions and\nsource codes into a single high-dimensional vector space. Sachdev et al. [35]\nproposed Neural Code Search (NCS), a method to extract a sequence of natural\nlanguage tokens from a code snippet that forms a code document. Embedding\nUniﬁcation (UNIF) [8] is a supervised extension of the NCS. The idea of both\nthese methods is to extract natural language components from the method and\nidentiﬁer names, and use them to match with the natural language query.\nThe CodeSearchNet Challenge [21] initiated an open challenge for code search,\nand publicly released a dataset collected from GitHub. Additionally, they re-\nleased several baseline code search methods. Feng et al. [13] proposed Code-\nBERT, a BERT [11] model trained with both natural language and source code\nfrom the CodeSearchNet dataset [21], and has shown to signiﬁcantly outperform\nthe baseline models of the challenge.\n6 Conclusion\nIn this work, we propose BERT2Code, a simple neural network architecture\nthat performs semantic code search by utilizing pre-trained natural language\nand code embedding models. We show that our proposed method learns to map\nnatural language embeddings to code embeddings to a reasonable extent. There\nare multiple ﬁndings of our work. Firstly, we show that it is possible to utilize\npre-trained embedding models to semantic code search. Secondly, We manually\nanalyze the embedding methods and show that better code embedding methods\nare needed for better results of our work. Through this work, we would like to\ndraw the attention of the research community to build more generalizable code\nembedding models.\nReferences\n1. Alammar, J.: The illustrated bert, elmo, and co. (how nlp cracked transfer learning)\n– jay alammar – visualizing machine learning one concept at a time. http://\njalammar.github.io/illustrated-bert/ (2018), (Accessed on 07/21/2020)\n10 Abdullah Al Ishtiaq et al.\n2. Alon, U., Zilberstein, M., Levy, O., Yahav, E.: Code2vec: Learning distributed\nrepresentations of code. Proc. ACM Program. Lang. 3(POPL) (Jan 2019), https:\n//doi.org/10.1145/3290353\n3. Artetxe, M., Schwenk, H.: Massively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transactions of the Association for Com-\nputational Linguistics 7(0), 597–610 (2019), https://transacl.org/ojs/index.\nphp/tacl/article/view/1742\n4. Arumugam, L.: Semantic code search using Code2Vec: A bag-of-paths model. Mas-\nter’s thesis, University of Waterloo (2020)\n5. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning\nto align and translate. arXiv preprint arXiv:1409.0473 (2014)\n6. Bowman, S.R., Angeli, G., Potts, C., Manning, C.D.: A large annotated corpus\nfor learning natural language inference. In: Proceedings of the 2015 Conference\non Empirical Methods in Natural Language Processing. pp. 632–642. Association\nfor Computational Linguistics, Lisbon, Portugal (Sep 2015),https://www.aclweb.\norg/anthology/D15-1075\n7. Briem, J.A., Smit, J., Sellik, H., Rapoport, P.: Using distributed representation of\ncode for bug detection. arXiv preprint arXiv:1911.12863 (2019)\n8. Cambronero, J., Li, H., Kim, S., Sen, K., Chandra, S.: When deep learning met\ncode search. In: Proceedings of the 2019 27th ACM Joint Meeting on European\nSoftware Engineering Conference and Symposium on the Foundations of Software\nEngineering. p. 964–974. ESEC/FSE 2019, Association for Computing Machinery,\nNew York, NY, USA (2019), https://doi.org/10.1145/3338906.3340458\n9. Cheng, J., Callison-Burch, C.: Bilingual is at least monolingual (balm): A\nnovel translation algorithm that encodes monolingual priors. arXiv preprint\narXiv:1909.01146 (2019)\n10. Craswell, N.: Mean Reciprocal Rank, pp. 1703–1703. Springer US, Boston, MA\n(2009), https://doi.org/10.1007/978-0-387-39940-9_488\n11. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep\nbidirectional transformers for language understanding. In: Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers).\npp. 4171–4186. Association for Computational Linguistics, Minneapolis, Minnesota\n(Jun 2019), https://www.aclweb.org/anthology/N19-1423\n12. Elliott, D., Frank, S., Sima’an, K., Specia, L.: Multi30K: Multilingual English-\nGerman image descriptions. In: Proceedings of the 5th Workshop on Vision and\nLanguage. pp. 70–74. Association for Computational Linguistics, Berlin, Germany\n(Aug 2016), https://www.aclweb.org/anthology/W16-3210\n13. Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu,\nT., Jiang, D., Zhou, M.: CodeBERT: A pre-trained model for programming and\nnatural languages. In: Findings of the Association for Computational Linguistics:\nEMNLP 2020. pp. 1536–1547. Association for Computational Linguistics, Online\n(Nov 2020), https://www.aclweb.org/anthology/2020.findings-emnlp.139\n14. Ge, L., Moh, T.: Improving text classiﬁcation with word embedding. In: 2017\nIEEE International Conference on Big Data (Big Data). pp. 1796–1805 (2017).\nhttps://doi.org/10.1109/BigData.2017.8258123\n15. Glorot, X., Bordes, A., Bengio, Y.: Deep sparse rectiﬁer neural networks. In: Gor-\ndon, G., Dunson, D., Dud´ ık, M. (eds.) Proceedings of the Fourteenth International\nConference on Artiﬁcial Intelligence and Statistics. Proceedings of Machine Learn-\ning Research, vol. 15, pp. 315–323. JMLR Workshop and Conference Proceedings,\nBERT2Code 11\nFort Lauderdale, FL, USA (11–13 Apr 2011), http://proceedings.mlr.press/\nv15/glorot11a.html\n16. Grzegorczyk, K.: Vector representations of text data in deep learning. arXiv\npreprint arXiv:1901.01695 (2019)\n17. Gu, X., Zhang, H., Kim, S.: Deep code search. In: Proceedings of the 40th Interna-\ntional Conference on Software Engineering. p. 933–944. ICSE ’18, Association for\nComputing Machinery, New York, NY, USA (2018), https://doi.org/10.1145/\n3180155.3180167\n18. Gu, X., Zhang, H., Zhang, D., Kim, S.: Deep api learning. In: Proceedings of the\n2016 24th ACM SIGSOFT International Symposium on Foundations of Software\nEngineering. p. 631–642. FSE 2016, Association for Computing Machinery, New\nYork, NY, USA (2016), https://doi.org/10.1145/2950290.2950334\n19. Hahnloser, R.H., Sarpeshkar, R., Mahowald, M.A., Douglas, R.J., Seung, H.S.:\nDigital selection and analogue ampliﬁcation coexist in a cortex-inspired silicon\ncircuit. Nature 405(6789), 947–951 (2000)\n20. Hu, X., Li, G., Xia, X., Lo, D., Jin, Z.: Deep code comment generation. In: 2018\nIEEE/ACM 26th International Conference on Program Comprehension (ICPC).\npp. 200–20010. IEEE (2018)\n21. Husain, H., Wu, H.H., Gazit, T., Allamanis, M., Brockschmidt, M.: Codesearch-\nnet challenge: Evaluating the state of semantic code search. arXiv preprint\narXiv:1909.09436 (2019)\n22. Johnson, J., Douze, M., J´ egou, H.: Billion-scale similarity search with gpus. IEEE\nTransactions on Big Data (2019)\n23. Joulin, A., Grave, E., Bojanowski, P., Mikolov, T.: Bag of tricks for eﬃcient text\nclassiﬁcation. In: Proceedings of the 15th Conference of the European Chapter\nof the Association for Computational Linguistics: Volume 2, Short Papers. pp.\n427–431. Association for Computational Linguistics, Valencia, Spain (Apr 2017),\nhttps://www.aclweb.org/anthology/E17-2068\n24. Kang, H.J., Bissyand´ e, T.F., Lo, D.: Assessing the generalizability of code2vec\ntoken embeddings. In: Proceedings of the 34th IEEE/ACM International Confer-\nence on Automated Software Engineering. p. 1–12. ASE ’19, IEEE Press (2019),\nhttps://doi.org/10.1109/ASE.2019.00011\n25. Kim, Y.: Convolutional neural networks for sentence classiﬁcation. In: Proceedings\nof the 2014 Conference on Empirical Methods in Natural Language Processing\n(EMNLP). pp. 1746–1751. Association for Computational Linguistics, Doha, Qatar\n(Oct 2014), https://www.aclweb.org/anthology/D14-1181\n26. Le, Q., Mikolov, T.: Distributed representations of sentences and documents. In:\nProceedings of the 31st International Conference on International Conference on\nMachine Learning - Volume 32. p. II–1188–II–1196. ICML’14, JMLR.org (2014)\n27. Lilleberg, J., Zhu, Y., Zhang, Y.: Support vector machines and word2vec for text\nclassiﬁcation with semantic features. In: 2015 IEEE 14th International Conference\non Cognitive Informatics Cognitive Computing (ICCI*CC). pp. 136–140 (2015).\nhttps://doi.org/10.1109/ICCI-CC.2015.7259377\n28. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,\nZettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized BERT pretraining\napproach. CoRR abs/1907.11692 (2019), http://arxiv.org/abs/1907.11692\n29. Mikolov, T., Sutskever, I., Chen, K., Corrado, G., Dean, J.: Distributed representa-\ntions of words and phrases and their compositionality. In: Proceedings of the 26th\nInternational Conference on Neural Information Processing Systems - Volume 2.\np. 3111–3119. NIPS’13, Curran Associates Inc., Red Hook, NY, USA (2013)\n12 Abdullah Al Ishtiaq et al.\n30. Nie, L., Jiang, H., Ren, Z., Sun, Z., Li, X.: Query expansion based on crowd\nknowledge for code search. IEEE Transactions on Services Computing 9(5), 771–\n783 (2016)\n31. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,\nT., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,\nDeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai,\nJ., Chintala, S.: Pytorch: An imperative style, high-performance deep learning\nlibrary. In: Wallach, H., Larochelle, H., Beygelzimer, A., d 'Alch´ e-Buc, F., Fox, E.,\nGarnett, R. (eds.) Advances in Neural Information Processing Systems 32, pp.\n8024–8035. Curran Associates, Inc. (2019), http://papers.neurips.cc/paper/\n9015-pytorch-an-imperative-style-high-performance-deep-learning-library.\npdf\n32. Paul, S., Prakash, A.: A framework for source code search using program patterns.\nIEEE Transactions on Software Engineering 20(6), 463–475 (1994)\n33. Reimers, N., Gurevych, I.: Sentence-BERT: Sentence embeddings using Siamese\nBERT-networks. In: Proceedings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP). pp. 3982–3992. Association for\nComputational Linguistics, Hong Kong, China (Nov 2019), https://www.aclweb.\norg/anthology/D19-1410\n34. Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning Internal Representations\nby Error Propagation, p. 318–362. MIT Press, Cambridge, MA, USA (1986)\n35. Sachdev, S., Li, H., Luan, S., Kim, S., Sen, K., Chandra, S.: Retrieval on source\ncode: A neural code search. In: Proceedings of the 2nd ACM SIGPLAN Inter-\nnational Workshop on Machine Learning and Programming Languages. p. 31–41.\nMAPL 2018, Association for Computing Machinery, New York, NY, USA (2018),\nhttps://doi.org/10.1145/3211346.3211353\n36. Sedgwick, P.: Pearson’s correlation coeﬃcient. Bmj 345, e4483 (2012)\n37. Singer, J., Lethbridge, T., Vinson, N., Anquetil, N.: An examination of software\nengineering work practices. In: Proceedings of the 1997 Conference of the Centre\nfor Advanced Studies on Collaborative Research. p. 21. CASCON ’97, IBM Press\n(1997)\n38. Turing, A.M.: 1. the imitation game. In: Eckert, M. (ed.) Theories of Mind: An\nIntroductory Reader, p. 51. Rowman & Littleﬁeld (2006)\n39. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\nKaiser, u., Polosukhin, I.: Attention is all you need. In: Proceedings of the 31st\nInternational Conference on Neural Information Processing Systems. p. 6000–6010.\nNIPS’17, Curran Associates Inc., Red Hook, NY, USA (2017)\n40. Williams, A., Nangia, N., Bowman, S.: A broad-coverage challenge corpus for sen-\ntence understanding through inference. In: Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies, Volume 1 (Long Papers). pp. 1112–1122.\nAssociation for Computational Linguistics, New Orleans, Louisiana (Jun 2018),\nhttps://www.aclweb.org/anthology/N18-1101\n41. Yao, Z., Peddamail, J.R., Sun, H.: Coacor: Code annotation for code retrieval\nwith reinforcement learning. In: The World Wide Web Conference. p. 2203–2214.\nWWW ’19, Association for Computing Machinery, New York, NY, USA (2019),\nhttps://doi.org/10.1145/3308558.3313632\n42. Zhang, W., Yoshida, T., Tang, X.: A comparative study of tf*idf, lsi and multi-\nwords for text classiﬁcation. Expert Syst. Appl. 38(3), 2758–2765 (Mar 2011),\nhttps://doi.org/10.1016/j.eswa.2010.08.066",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8886884450912476
    },
    {
      "name": "Embedding",
      "score": 0.6591540575027466
    },
    {
      "name": "Code (set theory)",
      "score": 0.6338366270065308
    },
    {
      "name": "Natural language",
      "score": 0.6153041124343872
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.6024038195610046
    },
    {
      "name": "Bottleneck",
      "score": 0.5057156085968018
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47816258668899536
    },
    {
      "name": "Language model",
      "score": 0.45064622163772583
    },
    {
      "name": "Natural language processing",
      "score": 0.4488697648048401
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.42450472712516785
    },
    {
      "name": "Scope (computer science)",
      "score": 0.4107283651828766
    },
    {
      "name": "Information retrieval",
      "score": 0.3810594081878662
    },
    {
      "name": "Programming language",
      "score": 0.3394879698753357
    },
    {
      "name": "Embedded system",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 6
}