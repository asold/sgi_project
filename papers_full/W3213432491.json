{
  "title": "Investigating Negation in Pre-trained Vision-and-language Models",
  "url": "https://openalex.org/W3213432491",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2994846147",
      "name": "Radina Dobreva",
      "affiliations": [
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A2102014928",
      "name": "Frank Keller",
      "affiliations": [
        "University of Edinburgh"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3034995113",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3104142662",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W2138028057",
    "https://openalex.org/W2963530300",
    "https://openalex.org/W3034837210",
    "https://openalex.org/W3099843385",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W1501375624",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W3106784008",
    "https://openalex.org/W3176751053",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3159900299",
    "https://openalex.org/W2963115613",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2946417913"
  ],
  "abstract": "Pre-trained vision-and-language models have achieved impressive results on a variety of tasks, including ones that require complex reasoning beyond object recognition. However, little is known about how they achieve these results or what their limitations are. In this paper, we focus on a particular linguistic capability, namely the understanding of negation. We borrow techniques from the analysis of language models to investigate the ability of pre-trained vision-and-language models to handle negation. We find that these models severely underperform in the presence of negation.",
  "full_text": "Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 350–362\nOnline, November 11, 2021. ©2021 Association for Computational Linguistics\n350\nInvestigating Negation in Pre-trained Vision-and-language Models\nRadina Dobreva and Frank Keller\nInstitute for Language, Cognition and Computation\nSchool of Informatics\nUniversity of Edinburgh\nr.dobreva@ed.ac.uk, keller@inf.ed.ac.uk\nAbstract\nPre-trained vision-and-language models have\nachieved impressive results on a variety of\ntasks, including ones that require complex rea-\nsoning beyond object recognition. However,\nlittle is known about how they achieve these\nresults or what their limitations are. In this\npaper, we focus on a particular linguistic ca-\npability, namely the understanding of negation.\nWe borrow techniques from the analysis of lan-\nguage models to investigate the ability of pre-\ntrained vision-and-language models to handle\nnegation. We ﬁnd that these models severely\nunderperform in the presence of negation.\n1 Introduction\nVision-and-language models have made a lot of\nprogress on complex tasks, going beyond recogni-\ntion and towards reasoning over the two modalities\n(Zellers et al., 2019; Suhr et al., 2019). Following\nthe success of pre-trained language models such as\nBERT (Devlin et al., 2019) on a range of language\ntasks, recent advances in vision-and-language have\ninvolved the introduction of pre-trained models\n(e.g., UNITER, Chen et al. 2020, VisualBERT, Li\net al. 2019, ViLBERT, Lu et al. 2019, LXMERT,\nTan and Bansal 2019). These models achieve im-\npressive results, topping task leaderboards and im-\nproving over previous approaches by a large mar-\ngin. However, as with pre-trained language models,\nit is not clear how and why these models perform\nas well as they do, what information they learn and\nuse in their predictions, or what their limitations\nare. While a large body of research has focused on\nthe interpretation of pre-trained language models\n(e.g., Clark et al. 2019; Tenney et al. 2019; Rogers\net al. 2020; Elazar et al. 2021), such work has been\nmore limited for vision-and-language models.\nThis paper focuses on a particular linguistic ca-\npability, namely the ability to understand negation.\nNegation is universal across languages (Zeijlstra,\n2007) and is very important for interpreting the\nFigure 1: Examples from the NLVR2 corpus. The in-\nput to the model consists of two images and a sentence.\nThe model’s task is to predict whether the sentence is\ntrue of the images or not. The top example shows an\ninstance where the sentence is true and the bottom one,\nfalse. Image from (Suhr et al., 2019).\nmeaning and determining the truth value of a state-\nment. Vision-and-language models have applica-\ntions in real-world scenarios where it is crucial to\nunderstand negation, as this is a behaviour expected\nby humans interacting with them.\nFor the purposes of this analysis, we focus on a\nparticular vision-and-language task, Natural Lan-\nguage Visual Reasoning for Real (NLVR2) (Suhr\net al., 2019). For this task, the model is presented\nwith two images and a sentence and has to predict\nwhether that sentence is true of the images or not\n(see Figure 1). Since this task involves the pre-\ndiction of a truth value, it lends itself well to the\nexploration of negation. We consider two popular\npre-trained vision-and-language models, ﬁnetuned\nfor the task: LXMERT (Tan and Bansal, 2019) and\nUNITER (Chen et al., 2020). Details on the data\nand models are in Section 3.\nIn order to investigate the performance of these\n351\nmodels on examples that contain negation in a con-\ntrolled manner, we annotated a portion of the orig-\ninal NLVR2 test set to create minimally differing\ninstances. That is, we created pairs of instances\nwhere the images remain the same and the sen-\ntences only differ in the presence or absence of\nnegation (see Section 3 for details). We show that\nboth models under consideration perform worse on\nthe negated examples, compared to the correspond-\ning non-negated examples (Section 4). We also use\ncausal mediation analysis (Pearl, 2001; Vig et al.,\n2020) on UNITER to examine the contributions of\nspeciﬁc neurons to the ﬁnal predictions (Section 5).\nOur results show that the effects of negation are\nmainly seen in the upper layers of the model. We\nrelease the new test set and code for our exper-\niments at https://github.com/radidd/\nvision-and-language-negation.\n2 Background\n2.1 Negation\nNegation is related to the notion of polarity: a\nclause such as “It is raining” is said to have pos-\nitive polarity, whereas a clause such as “It is not\nraining” has negative polarity (Pullum and Huddle-\nston, 2002). Positive polarity is usually structurally\nsimpler, whereas negative polarity is marked by\nwords or afﬁxes. Pullum and Huddleston (2002)\ncategorise negation based on several different prop-\nerties, but for our purposes we focus on the distinc-\ntion between verbal and non-verbal negation. In the\nformer, the negation marker is attached to the verb\n(“I did not see anything at all”), while in the latter,\nthe negation marker is attached to a dependent of\nthe verb (“I saw nothing at all”).\nPrevious analysis of language models has shown\nthat they are not able to handle negation. Pre-\ntrained language models fail to make correct predic-\ntions in the presence of negation or even to distin-\nguish between positive and negative sentences (Et-\ntinger, 2020; Kassner and Schütze, 2020). Hossain\net al. (2020) ﬁnetune several different pre-trained\nlanguage models for natural language inference\nand show that performance on instances containing\nnegation deteriorates.\n2.2 Vision-and-language models\nFollowing the success of pre-trained language\nmodels (Devlin et al., 2019), multimodal tasks\nsuch as visual question answering (Antol et al.,\n2015) and visual commonsense reasoning (Zellers\net al., 2019) have also recently been approached\nusing a pretrain-and-ﬁnetune method. Pre-trained\nvision-and-language models fall into two cat-\negories: single-stream models (e.g., Li et al.\n2019; Chen et al. 2020) and two-stream models\n(e.g., Lu et al. 2019; Tan and Bansal 2019). While\ntwo-stream models have separate encoders for\nthe visual and textual modalities and then jointly\nprocess them using a third encoder, single-stream\nmodels process both the textual and visual features\ntogether from the start.\nWhile these models have been very successful,\npushing state-of-the-art results up across tasks, not\nmuch is known about their capabilities and limita-\ntions. From that perspective, the closest works to\nours are by Li et al. (2020) and Cao et al. (2020).\nLi et al. (2020) analyse attention heads in the\nmodel and show that words in the text are cor-\nrectly mapped to image regions which correspond\nto them. Cao et al. (2020) probe pre-trained vision-\nand-language models and report similar observa-\ntions. Their work focuses more on the ground-\ning aspect of vision-and-language models, whereas\nour work focuses more on linguistic and reasoning\nabilities. Cao et al. (2020) also show that mul-\ntimodal pre-trained models learn some linguistic\nknowledge, however, they do not analyse this any\ndeeper than providing results on several probing\ntasks, none of which involves negation.\n2.3 Causal mediation analysis\nLanguage-only BERT has been targeted by a lot of\nrecent analysis work focusing on different capabili-\nties of the model (Clark et al., 2019; Tenney et al.,\n2019; Rogers et al., 2020; Elazar et al., 2021). A re-\ncent proposal is the application of causal mediation\nanalysis (Pearl, 2001) to better understand neural\nNLP models (Vig et al., 2020).\nCausal mediation analysis aims to measure the\neffect of intermediate variables (“mediators”) on a\nresponse variable. Pearl (2001) deﬁnes a natural\ndirect effect (NDE) and a natural indirect effect\n(NIE), see Figure 2. NDE refers to the direct ef-\nfect of a particular value of an input variable X\non the value of a response variable Y , without the\nintervention of a mediator Z. More speciﬁcally,\nwe can measure the effect of an intervention, or\nchange, of the input variable (X = x − →X = x∗),\nwhile keeping Z to its value without the interven-\ntion. NIE refers to the indirect effect of the input\nvariable X on Y via the intermediate variable Z.\n352\nx y\nz\nMediator\n- Model components\nResponse variable\n- Gender bias/syntactic \nagreement\nControl variable\n- Text edits\nDirect effect\nIndirect effect\nFigure 2: Graphical model of the indirect and direct\neffect. Figure adapted from Vig et al. (2020).\nSpeciﬁcally, we can ﬁx X to its original value, but\nchange the value of the intermediate variable Z to\nits value under the intervention X = x∗.\nVig et al. (2020) apply causal mediation analy-\nsis to the study of gender bias in large pre-trained\nlanguage models. They treat model components\n– speciﬁcally neurons and attention heads – as\nintermediate variables (Figure 2). They make\nchanges to the input text by switching from gender-\nambiguous to gender-unambiguous input and mea-\nsure the effects of these changes on the amount\nof bias the model exhibits. This kind of analysis\ncan show whether speciﬁc model components are\ncausally responsible for a speciﬁc outcome. In-\ndeed, Vig et al. (2020) show that gender bias ef-\nfects are sparse and concentrated in a handful of\nattention heads in the middle layers of the model.\nMore recently, Finlayson et al. (2021) apply causal\nmediation analysis to the problem of subject-verb\nagreement in language models. They look at neu-\nrons in the models and ﬁnd that some models learn\ntwo different mechanisms to resolve agreement for\ndifferent sentence structures.\n3 Data and models\n3.1 Models\nFor our experiments we used two vision-and-\nlanguage models, both based on the Transformer\narchitecture: LXMERT (Tan and Bansal, 2019)\nwhich is a two-stream model and UNITER (Chen\net al., 2020), which is a single-stream model. For\nUNITER, we experiment with two different vari-\nants of applying the model to NLVR2: paired with\nattention and triplet. The paired with attention vari-\nant encodes the two images separately and then\ncombines the representations with an additional at-\ntention layer, whereas the triplet variant encodes\nboth images together from the start (see Chen et al.\n(2020) for details).\n3.2 NLVR2 dataset\nThe dataset used for all experiments is NLVR2\n(Suhr et al., 2019), which consists of pairs of\nimages, a sentence describing each pair and a\nTrue/False tag indicating whether the sentence is\ntrue of the image pair. This dataset requires joint\nreasoning over the two modalities and is more com-\nplex than image captioning datasets due to the kind\nof language it contains – for instance, it requires\ncomparison and counting abilities.\nThe authors of NLVR2 present statistics of the\noccurrence of different linguistic phenomena in a\nportion of their development set. Their analysis\nshows that 9.6% of the samples they consider con-\ntain negation. This statistic is not available for the\ntest and the training set, so we calculated it based\non a short list of negation words (see Appendix A).\nThe results are as follows:\n• Training set: 7192/86373 samples (8.33%)\n• Development set: 630/6982 samples (9.02%)\n• Test set: 589/6967 samples (8.45%)\nThis shows that negation is not very common in\nthe dataset. As a preliminary experiment, we tested\nthe models’ performance on the samples identiﬁed\nto contain negation in the original development\nand test set and compared it to the performance on\nsamples which do not contain negation. Results are\nshown in Table 1. All three models show a drop\nin performance on the samples containing negation\nfor both the development and test set, compared\nto non-negated samples. The drop in performance\nvaries between 1.7 points and 7 points.\nThere is no more detailed analysis of the types\nof negation present in the dataset, for example\nwhether it is verbal or non-verbal. This means\nthat there is no way to use the existing data for a\nmore ﬁne-grained analysis of negation. The exist-\ning dataset also cannot be used reliably to make per-\nformance comparisons between negated and non-\nnegated examples. This is because it is possible\nthat other factors, such as sentence length or com-\nplexity of the reasoning required (e.g., counting,\ncomparison between the two images), are inﬂuenc-\ning performance. Therefore, we manually created\na test set of minimally differing pairs by adding\nnegation to the original data.\n3.3 Negation test set\nIn order to investigate whether vision-and-language\nmodels perform differently in the presence of nega-\ntion, we annotated a portion of the NLVR2 test set\n353\nLXMERT UNITER paired−attn UNITERtriplet\nneg. non-neg. neg. non-neg. neg. non-neg.\nDev set 71.43 74.92 74.29 77.38 70.00 71.68\nTest set 67.74 74.71 74.87 77.89 67.57 73.61\nTable 1: Accuracy on samples which contain negation and samples which do not contain negation from the original\ndevelopment and test set.\nNegation type o/n Example count\nVerbal (content) o At least one person is wearing a hat. 91\nn At least one person is not wearing a hat. 94\nVerbal (existential) o The left image contains exactly two dogs. 108\nn The left image does not contain exactly two dogs. 108\nNP (nonexistential) o All the cars are facing towards the left. 28\nn Not all the cars are facing towards the left. 29\nNP (existential) o All the marmots are on rocks. 55\nn None of the marmots are on rocks. 55\nNP (number-to-none) o There are a total of four people in the gym. 72\nn There are no people in the gym. 72\nSentence-wide o there are sled dogs moving toward the camera 83\nn It is not true that there are sled dogs moving toward the camera 83\nTable 2: Example negated sentences and counts for each category. “o” stands for original, “n” stands for negated.\nto create instances of negation. An important re-\nquirement for the annotation was that every negated\ninstance has a ﬂipped label compared to the origi-\nnal.1 It should also be noted that while negation is\na complex phenomenon, here we only consider ab-\nsolute negators (e.g., no, not, nobody, nothing), and\nwe do not consider approximate negators (e.g., few,\nlittle, barely) or afﬁxal negators (e.g., the preﬁxes\nun-, in-, non-, see Pullum and Huddleston (2002).\nThe negation categories below are constrained by\nthese two considerations.\nFirst, we identiﬁed appropriate samples for nega-\ntion. We selected the samples in pairs, where both\nsamples belonging to a pair have the same images,\nbut different sentences and True/False labels. This\nwas done so that the labels remain balanced in the\nnew negation test set. See Appendix B for more\ncriteria for sample selection.\nNext, we created negated versions of each sam-\nple in such a way that the new sample has a ﬂipped\nlabel according to the annotator’s judgement. It is\nimportant to keep in mind that the truth value of an\nexample depends on the images and the sentence;\n1The ﬂipped label requirement was in place to aid the anal-\nysis. It is less straightforward to evaluate pairs with the same\nlabel, since we would not be able to tell if the model is doing\nanything differently with and without the negation. Future\nwork could include analysis of examples where negation does\nnot change the label.\nif we just look at the sentence on its own, adding\nnegation does not ﬂip the truth value in all cases.\nThe annotator also recorded the type of negation for\neach example as belonging to one of six categories\n(see Table 2 and Appendix C for examples):\nVerbal negation:\n• Content negation: where negation is attached\nto a verb expressing an action (“is not stand-\ning”) or a characteristic (“don’t have black\nseats”).\n• Existential negation: where the negated verb\nrelates to the existence of a predicate (“the im-\nage doesn’t include . . . ”, “there aren’t . . . ”).\nNoun phrase negation:\n• Non-existential: the resulting negated NP\ndoes not deny the existence of an object, for\nexample “not all birds”, “no more than three\nbirds”.\n• Existential negation: denies the existence of\nan object in a noun phrase (“no dogs”, “neither\nimage”).\n• Number-to-none negation: similar to existen-\ntial negation, but the original NP contains a\nnumeral (“at least two dogs”, “a total of ﬁve\nbirds”) and the negated version denies the ex-\nistence of the object (“no dogs”, “no birds”).\n354\nSentence-wide negation: negating the full sen-\ntence by appending “It is not the case that . . . ” or\n“It is not true that . . . ” at the start.\nThe annotation was done with minimal possible\nperturbations of the original sentences to allow\nfor a fairer comparison between the original and\nthe negated samples. For some of the original\nsamples it was possible to create several different\nnegated ones. Those were kept to a maximum of\nthree and in practice few samples had more than\ntwo possible negations. Table 2 shows the number\nof samples belonging to each category as well as\nthe number of corresponding original samples. In\ntotal there are 441 negated samples created from\n400 original samples.\nIt should be noted that there are some differences\nbetween the existing negated examples in the orig-\ninal NLVR2 test set and the examples in our nega-\ntion test set. Firstly, we had to keep the negated sen-\ntences grammatical and minimally different, which\nrestricted the variety of negation structures and\ntypes. Second, people likely use negation differ-\nently when they are describing images in a natural\nsetting, than when it is added to existing descrip-\ntions artiﬁcially. See Appendix D for a further dis-\ncussion of the differences between our test set and\nnegation present in the original NLVR2 test set.\n4 Experimental results\n4.1 Experimental setup\nWe followed the instructions in the respective\nrepositories for LXMERT 2 and UNITER 3 to\nobtain the pre-trained checkpoints and ﬁnetune\nthem for the NLVR2 task. For both versions of\nUNITER we used the UNITER-base pre-trained\ncheckpoint. LXMERT was ﬁnetuned for four\nepochs and achieved an accuracy of 74.61%\nand 74.12% on the development and test sets\nrespectively. UNITERpaired-attn was ﬁnetuned for\nseven epochs and achieved an accuracy of 77.10%\nand 77.64% on the development and test sets.\nUNITERtriplet was ﬁnetuned for 10 epochs and\nachieved accuracies of 71.53% on the development\nand 73.10% on the test set.\n4.2 Results on the negation test set\nTable 3 shows the results on the negation test set by\ncategory. For comparison, we have provided accu-\nracy on the original samples corresponding to the\n2https://github.com/airsplay/lxmert\n3https://github.com/ChenRocks/UNITER\nnegated samples in each category. As can be seen\nfrom the table, all models perform worse on the\nnegation samples, across all categories. The only\nexception to this is UNITERtriplet on the NP (nonex-\nistential) category, where the score for the negative\nsamples is higher than that for the corresponding\npositive ones. Between the three models, overall\nthe performance of UNITER paired-attn is highest,\nfollowed by UNITERtriplet and LXMERT.\nLooking at speciﬁc categories, LXMERT seems\nto struggle the most with verbal negation, while\nboth versions of UNITER perform better, achiev-\ning scores between 14 and 20 points higher than\nLXMERT. The scores for the three NP nega-\ntion types are more varied between models, with\nUNITERtriplet outperforming the others on the\nnumber-to-none and nonexistential categories and\nUNITERpaired-attn outperforming on the existen-\ntial category. Since the negated forms of the NP\n(number-to-none) category and the NP (existen-\ntial) category are very similar, it is surprising that\nthere is such a big gap in performance (around 20\npoints) for two of the models. All models struggle\nsigniﬁcantly with sentence-wide negation. One ex-\nplanation for this could be that the models do not\nencounter the phrases used to create these samples\nin the training data and ignore them.\nTable 4 shows accuracy of the negated samples,\nsplit by whether the model predicts the correspond-\ning original sample correctly or not. A higher score\non the originally correct examples indicates that\nthe model is potentially able to handle negation,\ni.e., it learns that negation inverts the truth value. A\nhigher score on the originally incorrect examples is\nless clear – since there are only two categories, the\nmodel can achieve this by simply ignoring nega-\ntion and outputting the same prediction as for the\noriginal. Looking at the originally correct category,\nUNITERpaired-attn outperforms both other models\nacross categories, except for the sentence-wide cat-\negory. For the sentence-wide category, LXMERT\noutperforms both versions of UNITER by more\nthan 16 points. Turning to the originally incor-\nrect category, here all models perform much bet-\nter across negation categories, with the exception\nof NP (existential) negation. The highest score\noverall is obtained by UNITERtriplet, followed by\nLXMERT and UNITERpaired-attn.\n355\nLXMERT UNITER paired−attn UNITERtriplet\nnegative positive negative positive negative positive\nVerbal (content) 28.72 69.23 43.62 73.63 43.62 71.43\nVerbal (existential) 30.56 82.41 50.0 77.77 44.44 66.66\nNP (nonexistential) 44.83 67.86 48.28 64.29 55.17 50.0\nNP (existential) 34.55 80.0 50.91 85.45 32.73 87.27\nNP (number-to-none) 54.17 72.22 51.39 77.77 55.56 76.39\nSentence-wide 38.55 66.27 31.33 69.87 38.55 65.06\nOverall 36.96 73.5 45.35 76.5 44.22 71.5\nTable 3: Accuracy on the negation test set and the corresponding non-negated (positive) examples.\nLXMERT UNITER paired−attn UNITERtriplet\no. correct o. incorrect o. correct o. incorrect o. correct o. incorrect\nVerbal (content) 15.38 58.62 40.58 52.0 30.3 75.0\nVerbal (existential) 21.35 73.68 46.43 62.5 29.17 75.0\nNP (nonexistential) 30.0 77.78 42.11 60.0 40.0 71.43\nNP (existential) 36.36 27.27 55.32 25.0 27.08 71.43\nNP (number-to-none) 46.15 75.0 51.79 50.0 47.27 82.35\nSentence-wide 25.45 64.29 8.62 84.0 9.26 93.1\nOverall 27.38 63.79 40.54 60.19 29.35 79.39\nTable 4: Accuracy for the negated examples for whose original (unnegated) version the model makes a cor-\nrect/incorrect prediction (“o. correct”/“o. incorrect”).\n5 Causal mediation analysis\nWe will now take a closer look at the effect of\nadding negation and the contributions of speciﬁc\nneurons to model predictions using causal medi-\nation analysis. This type of analysis can help us\nmeasure the effect of a change in the input (adding\nnegation) on the output beyond looking at accu-\nracy only. We also apply mediation analysis to\nﬁnd out if speciﬁc neurons in the model contribute\nto a change in the output more than others. We\ncontinue to work with the negation categories de-\nscribed previously and aim to discover if the results\nfrom this analysis correspond to the observed dif-\nferences in accuracy between the categories and\nwhether neuron effects differ by category.\nSimilarly to Finlayson et al. (2021), we are work-\ning with a task which has correct and incorrect\noutputs (in their case continuations, in ours, la-\nbels). We therefore follow their deﬁnitions. We\nwant to measure the ability of the model to predict\nthe correct True/False label given the images and\nthe sentence. Therefore, we deﬁne our response\nvariable as:\ny(u, l) = pθ(lincorrect|u)\npθ(lcorrect|u)\nwhere u are the images and the text, which can ei-\nther match (utrue) or not (ufalse), and l is the label.\nThis equation takes the following form depending\non whether the gold label is True or False:\ny(utrue, l) = pθ(lfalse|utrue)\npθ(ltrue|utrue)\ny(ufalse, l) = pθ(ltrue|ufalse)\npθ(lfalse|ufalse)\nThe value for y is small (y <1) when the model\nassigns the correct label, and large (y >1) when\nthe model assigns the incorrect label.\nFollowing Vig et al. (2020) and Finlayson et al.\n(2021), we deﬁne two do-operations: (a) negate:\nnegate the original sentence so that the truth value\nchanges, and (b) null: no change. We also deﬁne\nyx(u, l) to be the value of y when we apply the op-\neration x to the context u. This takes the following\nvalues under the negate operation for each of the\npossible values of u:\nynegate(utrue, l) = pθ(lfalse|ufalse)\npθ(ltrue|ufalse)\nynegate(ufalse, l) = pθ(ltrue|utrue)\npθ(lfalse|utrue)\nIn order to measure the change in the response\nvariable under the intervention (negation), we\ndeﬁne the unit-level (per one original/negated pair)\n356\ntotal effect as:\nTE (negate, null; y, u, l) =\nynegate(u, l) −ynull(u, l)\nynull(u, l) =\nynegate(u, l)\nynull(u, l) −1\nAs Finlayson et al. (2021) state, this quantity\nmeasures the margin between the probability of\nthe correct and incorrect answers under an inter-\nvention. However, unlike Finlayson et al. (2021),\nwe analyse the originally correct and originally\nincorrect examples separately. When the model\npredicts the original example correctly, a larger\ntotal effect under the intervention could indicate a\nbetter handling of negation, as it suggests a higher\nprobability of the correct label under negation.\nWhen the original example is predicted incorrectly,\nit is less clear what the total effect indicates. While\na larger total effect suggests a move towards the\ncorrect prediction under negation, this does not\nnecessarily mean negation is handled better. A\nsmaller total effect suggests a move toward the\nincorrect label, but this means ﬂipping the label,\nwhich is desired behaviour.\nWe calculate the average total effect across all\nexample pairs, for each negation type and for two\ndifferent sizes of UNITER:\nTE (negate, null; y) =\nEu,l\n[ynegate(u, l)\nynull(u, l) −1\n]\nFor the mediation analysis, we focus on the effects\nof individual neurons from the representation of\nthe [CLS] token on the response variable y. In the\nfollowing deﬁnition, z refers to a single neuron.\nWe measure the natural indirect effect (NIE) of\na change in the input X on the response variable\ny, with respect to a mediator z. As mentioned\nin Section 2.3, this is done by ﬁxing the input to\nits value without the intervention, but changing\nthe value of the mediator z to its value under the\nintervention. In this case, we set the input to its\nvalue without negation (i.e. the original images-\nsentence pair which does not contain negation), but\nset the value of z (the neuron) to the value it would\ntake if the input was the negated version of the\nimages-sentence pair. The population level NIE\nthen is deﬁned as:\nNIE (negate, null; y, z) =\nEu,l\n[ynull,znegate(u,l)(u, l)\nynull(u, l) −1\n]\nAs we are concerned with the effects of spe-\nciﬁc mediators (the neurons) on the output, we do\nnot calculate the natural direct effect (NDE) which\nmeasures the effect of the input without the inter-\nvention of a mediator. We also do not consider\nintervening on attention heads in this work, as the\nlength of the original and the negated sentences is\ndifferent.\n5.1 Results\nFor this analysis we use the triplet version of\nUNITER (Chen et al., 2020). We perform the anal-\nysis on the base and large versions of the model\n(see Appendix E for accuracy results of the large\nmodel). We use UNITER-base and UNITER-large\nto refer to the two sizes of the triplet model.\nTotal effects We report the total effect for each\nnegation type for both model sizes, separately for\nthe originally correct and originally incorrect exam-\nples (Figure 3). Looking at the originally correct\nexamples (left side of Figure 3), we observe that\nthe total effects of UNITER-base are in most cases\nseveral orders of magnitude larger than those of\nUNITER-large. This difference between models\ncould mean that the two models assign probabili-\nties differently, i.e., the base model assigns more\nextreme probabilities to the correct answers, while\nthe large model assigns more moderate probabili-\nties. Both models show similar patterns, with the\nlowest total effects being observed for the sentence-\nwide and the verbal (content) negation categories\nand the highest for the NP (nonexistential) and ver-\nbal (existential) categories. This pattern does not\nshow any apparent correlation with the accuracies\nreported in the previous section – a higher total ef-\nfect does not necessarily mean a higher accuracy. It\nis possible that for categories with higher accuracy\nbut comparatively lower total effects the change in\nprobability is moderate, but meaningful – enough\nto ﬂip the label. At the same time, it could be that\nthe categories with a high total effect contain ex-\namples with extreme probabilities which skew the\naverage.\nThe right part of the ﬁgure shows the total effects\nof examples for which the model incorrectly pre-\ndicts the original. A larger total effect indicates a\n357\nVerbal (content)\nVerbal (existential)NP (nonexistential)\nNP (existential)\nNP (number-to-none)\nSentence-wide\nOverall\n0\n100\n101\n102\n103\n104\n105\n106\nT otal effect\n4289.92\n74565.7\n407879\n6014.93\n10759.8\n66.49\n40819.8\n16.93\n193.61\n450.15\n85.75 62.99\n10.53\n97.72\nOriginally correct examples\nUNITER-base\nUNITER-large\nVerbal (content)\nVerbal (existential)NP (nonexistential)\nNP (existential)\nNP (number-to-none)\nSentence-wide\nOverall\n0\n100\n101\n102\n103\n104\n105\n106\nT otal effect\n-0.39\n0.95 1.06\n-0.73\n3.87\n0.43\n0.85\n-0.44 -0.42 -0.15\n-0.66\n1.09\n-0.42 -0.19\nOriginally incorrect examples\nUNITER-base\nUNITER-large\nFigure 3: Total effects by correctness of the original (non-negated) example.\n0 2 4 6 8 10 12\nLayer index\n0.0\n0.01\n0.02\n0.03\n0.04\n0.05Indirect effect of top neurons\nUNITER-base\nVerbal (content)\nVerbal (existential)\nNP (nonexistential)\nNP (existential)\nNP (number-to-none)\nSentence-wide\n0 5 10 15 20 25\nLayer index\n0.0\n0.01\n0.02\n0.03\n0.04\n0.05Indirect effect of top neurons\nUNITER-large\nVerbal (content)\nVerbal (existential)\nNP (nonexistential)\nNP (existential)\nNP (number-to-none)\nSentence-wide\nFigure 4: Natural indirect effect of the top 5% of neurons in each layer per negation category. Shaded area\nrepresents the standard deviation.\nlarger change in the probability moving towards the\ncorrect prediction for the negated sample, whereas\na smaller (negative) total effect indicates a larger\nchange moving in the direction of the incorrect pre-\ndiction for the negated sample. These changes are\nmore difﬁcult to interpret for the originally incor-\nrect samples. A larger total effect could indicate\nthat the model is handling negation, however, it is\nnot clear why that would be the case if the original\nsample was incorrectly predicted. A negative total\neffect indicates that the prediction probability is\nmoving towards the opposite label from the one\npredicted for the original which could also be in-\nterpreted as the model handling the negation, since\na change of label is expected. All total effects are\nsmaller than the ones observed for the originally\ncorrect examples, which is expected given the ob-\nserved high accuracies – the model prediction does\nnot change a lot under the intervention and the total\neffect is closer to zero.\nNatural indirect effects Figure 4 shows the\nNIEs of the top 5% of neurons with the highest\nNIE in each layer, for each negation category. We\nobserve that for both models, the NIEs are approxi-\nmately zero in the lower layers and become larger\nin the upper layers. The NIEs are similar between\nnegation types, however, for both models we ob-\nserve the highest NIEs for the NP (existential) type.\nWe do not see any differences in patterns based on\nthe negation category, which suggests we do not\nhave evidence for a different treatment of the dif-\nferent categories by the model. We also compared\nthe NIEs of examples split by whether the origi-\nnal/negated one is correctly predicted by the model,\nhowever, we do not observe notable differences\n(Appendix F).\nPrevious studies on language models have shown\n358\nthat syntactic information is stored in the middle\nlayers, task-speciﬁc information is stored in the\nupper layers and there are conﬂicting conclusions\nregarding semantic information being found in the\nupper layers, or throughout the model (see Rogers\net al. (2020) for an overview). It is unclear whether\nthese patterns hold for vision-and-language mod-\nels. However, if they do, our results show that the\nmodels do not process negation on a syntactic level\n– at least not in terms of the neurons. The observed\neffects in the upper layers suggest that negation\nmay be semantically processed in some way. Fur-\nther investigation is required to draw more deﬁnite\nconclusions.\n6 Conclusion\nOur work shows that pre-trained vision-and-\nlanguage models ﬁnd it difﬁcult to handle nega-\ntion, which is a ﬁnding that is consistent with pre-\nvious work on pre-trained language models. Us-\ning a manually created set of minimally differing\npairs we show that two vision-and-language mod-\nels (LXMERT and UNITER) fail to reach good\nperformance in the presence of negation. We con-\nduct causal mediation analysis on the neurons of\none model and ﬁnd that the main effects of nega-\ntion are found in the upper layers. However, these\neffects are small and do not seem to correlate well\nwith model accuracy.\nWhile causal mediation analysis is a useful anal-\nysis tool, it is not straightforward to apply it to all\nmodels and to the analysis of attention when the\ninput is of different length in the base case and un-\nder the intervention. In the future we would like\nto extend our analysis to other model variations\n(e.g., the UNITERpaired-attn model which processes\nthe images separately from each other), and to at-\ntention heads.\nAcknowledgements\nThis work was supported in part by the UKRI Cen-\ntre for Doctoral Training in Natural Language Pro-\ncessing, funded by the UKRI (grant EP/S022481/1)\nand the University of Edinburgh, School of Infor-\nmatics and School of Philosophy, Psychology &\nLanguage Sciences.\nReferences\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C. Lawrence Zitnick,\nand Devi Parikh. 2015. VQA: Visual Question An-\nswering. In 2015 IEEE International Conference on\nComputer Vision (ICCV), pages 2425–2433.\nJize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-\nChun Chen, and Jingjing Liu. 2020. Behind the\nscene: Revealing the secrets of pre-trained vision-\nand-language models. In Computer Vision – ECCV\n2020, pages 565–580, Cham. Springer International\nPublishing.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. UNITER: UNiversal Image-\nTExt Representation Learning. In European Confer-\nence on Computer Vision, pages 104–120. Springer.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What Does BERT\nLook at? An Analysis of BERT’s Attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286, Florence, Italy. Association\nfor Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nYanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav\nGoldberg. 2021. Amnesic Probing: Behavioral Ex-\nplanation with Amnesic Counterfactuals. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:160–175.\nAllyson Ettinger. 2020. What BERT is Not: Lessons\nfrom a New Suite of Psycholinguistic Diagnostics\nfor Language Models. Transactions of the Associa-\ntion for Computational Linguistics, 8(0):34–48.\nMatthew Finlayson, Aaron Mueller, Sebastian\nGehrmann, Stuart Shieber, Tal Linzen, and Yonatan\nBelinkov. 2021. Causal Analysis of Syntactic\nAgreement Mechanisms in Neural Language Mod-\nels. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics\nand the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long\nPapers), pages 1828–1843, Online. Association for\nComputational Linguistics.\nMd Mosharaf Hossain, Venelin Kovatchev, Pranoy\nDutta, Tiffany Kao, Elizabeth Wei, and Eduardo\nBlanco. 2020. An Analysis of Natural Language In-\nference Benchmarks through the Lens of Negation.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9106–9118, Online. Association for Computa-\ntional Linguistics.\n359\nNora Kassner and Hinrich Schütze. 2020. Negated and\nMisprimed Probes for Pretrained Language Models:\nBirds Can Talk, But Cannot Fly. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics , pages 7811–7818, On-\nline. Association for Computational Linguistics.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. VisualBERT: A\nSimple and Performant Baseline for Vision and Lan-\nguage. In Arxiv.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2020. What does BERT\nwith vision look at? In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 5265–5275, Online. Association\nfor Computational Linguistics.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Ste-\nfan Lee. 2019. ViLBERT: Pretraining Task-\nAgnostic Visiolinguistic Representations for Vision-\nand-Language Tasks. In Advances in Neural Infor-\nmation Processing Systems, volume 32. Curran As-\nsociates, Inc.\nJudea Pearl. 2001. Direct and indirect effects. In\nProceedings of the Seventeenth Conference on Un-\ncertainty in Artiﬁcial Intelligence , UAI’01, page\n411–420, San Francisco, CA, USA. Morgan Kauf-\nmann Publishers Inc.\nGeoffrey K. Pullum and Rodney Huddleston. 2002.\nNegation. In The Cambridge Grammar of the En-\nglish Language, page 785–850. Cambridge Univer-\nsity Press.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A Primer in BERTology: What We Know\nAbout How BERT Works. Transactions of the Asso-\nciation for Computational Linguistics, 8:842–866.\nAlane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang,\nHuajun Bai, and Yoav Artzi. 2019. A Corpus for\nReasoning about Natural Language Grounded in\nPhotographs. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 6418–6428, Florence, Italy. Associa-\ntion for Computational Linguistics.\nHao Tan and Mohit Bansal. 2019. LXMERT: Learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5100–5111, Hong Kong, China. Association for\nComputational Linguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT Rediscovers the Classical NLP Pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\nSharon Qian, Daniel Nevo, Yaron Singer, and Stu-\nart Shieber. 2020. Investigating Gender Bias in Lan-\nguage Models Using Causal Mediation Analysis. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 12388–12401. Curran Associates,\nInc.\nHedde Zeijlstra. 2007. Negation in natural language:\nOn the form and meaning of negative elements.Lan-\nguage and Linguistics Compass, 1(5):498–518.\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin\nChoi. 2019. From recognition to cognition: Visual\ncommonsense reasoning. In 2019 IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), pages 6713–6724.\n360\nA Negation word list\nnot, isn’t, aren’t, doesn’t, don’t, can’t, cannot,\nshouldn’t, won’t, wouldn’t, no, none, nobody, noth-\ning, nowhere, neither, nor, never, without\nB Sample selection\nDuring the annotation samples with the following\nproperties were discarded:\n• Samples which already contain negation.\n• Samples with inappropriate or unpleasant im-\nages.\n• Samples which are too similar to already an-\nnotated samples. Because of the way NLVR2\nwas created, there are sample pairs that are\nvery similar to each other – those were dis-\ncarded to increase the diversity of the negation\ntest set.\n• Samples with sentences containing typos and\nother errors.\n• Samples for which there is no way to negate\nthe sentences and ﬂip the label, and at the\nsame time keep them grammatically correct.\nC Negation examples with images\nSee Figure 5.\nD Negation types in original data\nWe annotated the negation types of 100 examples\nfrom the NLVR2 training set which were automati-\ncally determined to contain negation, as well as 50\nfrom the development and test sets each. Results\nare shown in Table 5. We do not distinguish be-\ntween NP (existential) and NP (number-to-none),\nsince there are no “original” examples to com-\npare with. The “Other” category contains mostly\nnegated adjectives. The original negated examples\ndiffer from our annotation in several ways. First,\nthe sentence-wide type is not found in the original\nTrain Dev Test\nVerbal (content) 20 28 22\nVerbal (existential) 3 2 10\nNP (existential) 45 40 46\nNP (nonexistential) 31 32 26\nOther 2 2 4\nTable 5: Percentage of examples that contain each of\nthe negation types. Note that some examples contain\nmore than one type, so the percentages do not add up\nto 100.\ndata. This was conﬁrmed by a simple search across\nthe whole dataset for the phrases which compose\nthis negation type (“It is not the case that . . . ”, “It\nis not true that . . . ”). Second, there are very few\ninstances of verbal (existential) negation in the orig-\ninal data, whereas this type is very over-represented\nin our annotation. Finally, NP (nonexistential) is\nvery under-represented in our annotation compared\nto the original data.\nE UNITER-large triplet results\nSee Table 6.\nF NIE per correctness category\nFigure 6 shows the NIEs of each negation type for\nUNITER-base, comparing between correctness cat-\negories. The four correctness categories reﬂect the\ncorrectness of the original sample without the inter-\nvention and the correctness of the negated sample.\nNIE can be negative, which indicates a change to-\nwards the incorrect label. Here, we look at both the\ntop 5% neurons and the bottom 5% neurons to ﬁnd\nout if some neurons speciﬁcally contribute to in-\ncorrect predictions. The patterns we see are largely\nsimilar between the six negation types. The largest\nNIEs are observed in cases where both the origi-\nnal and the negated samples are predicted wrong.\nHowever, this result may be unreliable due to the\nsmall sample size (recall from Section 4 that a\nlarge percentage of the originally incorrect samples\nare predicted correctly when negated, therefore the\nnumber of “i-i” examples is small). The other cor-\nrectness categories all exhibit similar NIEs with the\nhighest values concentrated in the upper layers (8\nto 12). We expected to see higher NIE for the “c-c”\ncategory, however, that is not the case.\n361\nOriginal: At least one monkey is sitting in a tree \nin the image on the left.\nNegated: At least one monkey is not sitting in a \ntree in the image on the left.\nTrue\nFalse\n(a) Verbal (content)\nOriginal: There are three pandas.\nNegated: There aren’t three pandas.\nTrue\nFalse\n (b) Verbal (existential)\nOriginal: Every dog is wearing a collar.\nNegated: Not every dog is wearing a collar.\nFalse\nTrue\n(c) NP (nonexistential)\nOriginal: A dog is resting its head on something.\nNegated: No dog is resting its head on something.\nTrue\nFalse\n (d) NP (existential)\nOriginal: Four or fewer television screens \nare visible.\nNegated: No television screens are visible.\nTrue\nFalse\n(e) NP (number-to-none)\nOriginal: Three or fewer goats are visible.\nNegated: It is not true that three or fewer \ngoats are visible.\nFalse\nTrue\n (f) Sentence-wide\nFigure 5: Examples for each negation type with images and labels.\nUNITERtriplet−large\nnegative positive o.correct o.incorrect\nVerbal (content) 43.62 74.76 36.62 65.22\nVerbal (existential) 54.63 68.52 51.35 61.76\nNP (existential) 56.36 85.46 59.57 37.5\nNP (number-to-none) 58.33 76.38 54.55 70.59\nNP (nonexistential) 48.28 60.71 33.33 72.73\nSentence-wide 37.35 72.29 21.67 78.26\nOverall 49.43 74.0 43.38 66.38\nTable 6: Results on the negation test set.\n362\n0 2 4 6 8 10 12\nLayer index\n-0.05\n-0.04\n-0.03\n-0.02\n-0.01\n0.0\n0.01\n0.02\n0.03\n0.04\n0.05\nIndirect effect\nUNITER-base Verbal (content) negation\nc-c\nc-i\ni-c\ni-i\nc-c\nc-i\ni-c\ni-i\n0 2 4 6 8 10 12\nLayer index\n-0.05\n-0.04\n-0.03\n-0.02\n-0.01\n0.0\n0.01\n0.02\n0.03\n0.04\n0.05\nIndirect effect\nUNITER-base Verbal (existential) negation\nc-c\nc-i\ni-c\ni-i\nc-c\nc-i\ni-c\ni-i\n0 2 4 6 8 10 12\nLayer index\n-0.05\n-0.04\n-0.03\n-0.02\n-0.01\n0.0\n0.01\n0.02\n0.03\n0.04\n0.05\nIndirect effect\nUNITER-base NP (nonexistential) negation\nc-c\nc-i\ni-c\ni-i\nc-c\nc-i\ni-c\ni-i\n0 2 4 6 8 10 12\nLayer index\n-0.05\n-0.04\n-0.03\n-0.02\n-0.01\n0.0\n0.01\n0.02\n0.03\n0.04\n0.05\nIndirect effect\nUNITER-base NP (existential) negation\nc-c\nc-i\ni-c\ni-i\nc-c\nc-i\ni-c\ni-i\n0 2 4 6 8 10 12\nLayer index\n-0.05\n-0.04\n-0.03\n-0.02\n-0.01\n0.0\n0.01\n0.02\n0.03\n0.04\n0.05\nIndirect effect\nUNITER-base NP (number-to-none) negation\nc-c\nc-i\ni-c\ni-i\nc-c\nc-i\ni-c\ni-i\n0 2 4 6 8 10 12\nLayer index\n-0.05\n-0.04\n-0.03\n-0.02\n-0.01\n0.0\n0.01\n0.02\n0.03\n0.04\n0.05\nIndirect effect\nUNITER-base Sentence-wide negation\nc-c\nc-i\ni-c\ni-i\nc-c\nc-i\ni-c\ni-i\nFigure 6: Natural indirect effect of the top (solid line) and bottom (dashed line) 5% of neurons in each layer. The\nﬁgure shows the NIEs by correctness category: “c-c”: both original and negated sample are correctly predicted;\n“c-i”: original is correctly predicted and negated is incorrectly predicted; “i-c”: original is incorrectly predicted,\nnegated is correctly predicted; “i-i”: both are incorrectly predicted.",
  "topic": "Negation",
  "concepts": [
    {
      "name": "Negation",
      "score": 0.9043644666671753
    },
    {
      "name": "Computer science",
      "score": 0.7597805261611938
    },
    {
      "name": "Focus (optics)",
      "score": 0.711661696434021
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.6995661854743958
    },
    {
      "name": "Natural language processing",
      "score": 0.6044144034385681
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5990307331085205
    },
    {
      "name": "Language understanding",
      "score": 0.550489604473114
    },
    {
      "name": "Object (grammar)",
      "score": 0.5444821119308472
    },
    {
      "name": "Language model",
      "score": 0.48156288266181946
    },
    {
      "name": "Programming language",
      "score": 0.17116019129753113
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98677209",
      "name": "University of Edinburgh",
      "country": "GB"
    }
  ]
}