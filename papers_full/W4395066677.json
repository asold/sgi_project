{
    "title": "Real-World Performance of Large Language Models in Emergency Department Chest Pain Triage",
    "url": "https://openalex.org/W4395066677",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2098057093",
            "name": "Xiangbin Meng",
            "affiliations": [
                "Peking University Third Hospital",
                "King University",
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2644095772",
            "name": "Jia-Ming Ji",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2431170093",
            "name": "Xiangyu Yan",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A2105258892",
            "name": "Hua Xu",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2110421045",
            "name": "Jun Gao",
            "affiliations": [
                "Peking University",
                "Peking University Third Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2129475195",
            "name": "Jun-hong Wang",
            "affiliations": [
                "Peking University",
                "Peking University Third Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A3024267027",
            "name": "Jingjia Wang",
            "affiliations": [
                "Peking University",
                "Peking University Third Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2123218525",
            "name": "Xuliang Wang",
            "affiliations": [
                "Peking University Third Hospital",
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A4367140894",
            "name": "Yuan-geng-shuo Wang",
            "affiliations": [
                "Peking University",
                "Peking University Third Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2123420740",
            "name": "Wenyao Wang",
            "affiliations": [
                "Peking University",
                "Peking University Third Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A1964638924",
            "name": "Jing Chen",
            "affiliations": [
                "Chinese Academy of Medical Sciences & Peking Union Medical College"
            ]
        },
        {
            "id": "https://openalex.org/A2100533004",
            "name": "Kuo Zhang",
            "affiliations": [
                "Chinese Academy of Medical Sciences & Peking Union Medical College"
            ]
        },
        {
            "id": "https://openalex.org/A2097352640",
            "name": "Da Liu",
            "affiliations": [
                "First Affiliated Hospital of Hebei Medical University"
            ]
        },
        {
            "id": "https://openalex.org/A2113710123",
            "name": "Zifeng Qiu",
            "affiliations": [
                "Peking University",
                "Peking University First Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2145833137",
            "name": "Muzi Li",
            "affiliations": [
                "Peking University",
                "Peking University People's Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2232254003",
            "name": "Chunli Shao",
            "affiliations": [
                "Peking University",
                "Peking University Third Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2113349909",
            "name": "Yaodong Yang",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2557560092",
            "name": "Yi-Da Tang",
            "affiliations": [
                "Peking University Third Hospital",
                "King University",
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2098057093",
            "name": "Xiangbin Meng",
            "affiliations": [
                "Peking University",
                "Peking University Third Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2644095772",
            "name": "Jia-Ming Ji",
            "affiliations": [
                "Artificial Intelligence in Medicine (Canada)"
            ]
        },
        {
            "id": "https://openalex.org/A2431170093",
            "name": "Xiangyu Yan",
            "affiliations": [
                "Institute of Disaster Prevention"
            ]
        },
        {
            "id": "https://openalex.org/A2105258892",
            "name": "Hua Xu",
            "affiliations": [
                "Artificial Intelligence in Medicine (Canada)"
            ]
        },
        {
            "id": "https://openalex.org/A2110421045",
            "name": "Jun Gao",
            "affiliations": [
                "Peking University",
                "Peking University Third Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2129475195",
            "name": "Jun-hong Wang",
            "affiliations": [
                "Peking University",
                "Peking University Third Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A3024267027",
            "name": "Jingjia Wang",
            "affiliations": [
                "Peking University Third Hospital",
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2123218525",
            "name": "Xuliang Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4367140894",
            "name": "Yuan-geng-shuo Wang",
            "affiliations": [
                "Peking University Third Hospital",
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2123420740",
            "name": "Wenyao Wang",
            "affiliations": [
                "Peking University Third Hospital",
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A1964638924",
            "name": "Jing Chen",
            "affiliations": [
                "Chinese Academy of Medical Sciences & Peking Union Medical College",
                "Fu Wai Hospital",
                "Peking University Third Hospital",
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2100533004",
            "name": "Kuo Zhang",
            "affiliations": [
                "Chinese Academy of Medical Sciences & Peking Union Medical College",
                "Fu Wai Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2097352640",
            "name": "Da Liu",
            "affiliations": [
                "First Affiliated Hospital of Hebei Medical University"
            ]
        },
        {
            "id": "https://openalex.org/A2113710123",
            "name": "Zifeng Qiu",
            "affiliations": [
                "Peking University First Hospital",
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2145833137",
            "name": "Muzi Li",
            "affiliations": [
                "Peking University",
                "King University",
                "Peking University People's Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2232254003",
            "name": "Chunli Shao",
            "affiliations": [
                "Peking University Third Hospital",
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2113349909",
            "name": "Yaodong Yang",
            "affiliations": [
                "Artificial Intelligence in Medicine (Canada)"
            ]
        },
        {
            "id": "https://openalex.org/A2557560092",
            "name": "Yi-Da Tang",
            "affiliations": [
                "Peking University",
                "Peking University Third Hospital"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4379599615",
        "https://openalex.org/W4380730209",
        "https://openalex.org/W4383302171",
        "https://openalex.org/W4324308091",
        "https://openalex.org/W4382678522",
        "https://openalex.org/W4384561707",
        "https://openalex.org/W4384071683",
        "https://openalex.org/W4321366933",
        "https://openalex.org/W4321606060",
        "https://openalex.org/W4379769651",
        "https://openalex.org/W4380887490",
        "https://openalex.org/W4378783137",
        "https://openalex.org/W4367311208",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4377013495",
        "https://openalex.org/W4367310920",
        "https://openalex.org/W4381158111",
        "https://openalex.org/W4312220150",
        "https://openalex.org/W3135271498",
        "https://openalex.org/W2901784027",
        "https://openalex.org/W4311575655",
        "https://openalex.org/W4392241969",
        "https://openalex.org/W4372348574",
        "https://openalex.org/W4372347944",
        "https://openalex.org/W3122084268",
        "https://openalex.org/W2102345258",
        "https://openalex.org/W2045352610",
        "https://openalex.org/W3036626733",
        "https://openalex.org/W3112357189",
        "https://openalex.org/W2979841765",
        "https://openalex.org/W1995360315",
        "https://openalex.org/W2912601167",
        "https://openalex.org/W2129726455",
        "https://openalex.org/W2993374020",
        "https://openalex.org/W3208278329",
        "https://openalex.org/W2149011208",
        "https://openalex.org/W4200062714",
        "https://openalex.org/W2113302343",
        "https://openalex.org/W4378782408",
        "https://openalex.org/W4381309002",
        "https://openalex.org/W4319301505",
        "https://openalex.org/W4323350039",
        "https://openalex.org/W4376871976",
        "https://openalex.org/W4366998603",
        "https://openalex.org/W4308292299",
        "https://openalex.org/W4377941003",
        "https://openalex.org/W4385798548",
        "https://openalex.org/W4366989878",
        "https://openalex.org/W4385481791",
        "https://openalex.org/W4388725043",
        "https://openalex.org/W3162195278",
        "https://openalex.org/W4319663047",
        "https://openalex.org/W1922404503",
        "https://openalex.org/W4362667392",
        "https://openalex.org/W4226191767",
        "https://openalex.org/W4386272362",
        "https://openalex.org/W4385729025",
        "https://openalex.org/W4200166811"
    ],
    "abstract": "Abstract Background Large Language Models (LLMs) are increasingly being explored for medical applications, particularly in emergency triage where rapid and accurate decision-making is crucial. This study evaluates the diagnostic performance of two prominent Chinese LLMs, “Tongyi Qianwen” and “Lingyi Zhihui,” alongside a newly developed model, MediGuide-14B, comparing their effectiveness with human medical experts in emergency chest pain triage. Methods Conducted at Peking University Third Hospital’s emergency centers from June 2021 to May 2023, this retrospective study involved 11,428 patients with chest pain symptoms. Data were extracted from electronic medical records, excluding diagnostic test results, and used to assess the models and human experts in a double-blind setup. The models’ performances were evaluated based on their accuracy, sensitivity, and specificity in diagnosing Acute Coronary Syndrome (ACS). Findings “Lingyi Zhihui” demonstrated a diagnostic accuracy of 76.40%, sensitivity of 90.99%, and specificity of 70.15%. “Tongyi Qianwen” showed an accuracy of 61.11%, sensitivity of 91.67%, and specificity of 47.95%. MediGuide-14B outperformed these models with an accuracy of 84.52%, showcasing high sensitivity and commendable specificity. Human experts achieved higher accuracy (86.37%) and specificity (89.26%) but lower sensitivity compared to the LLMs. The study also highlighted the potential of LLMs to provide rapid triage decisions, significantly faster than human experts, though with varying degrees of reliability and completeness in their recommendations. Interpretation The study confirms the potential of LLMs in enhancing emergency medical diagnostics, particularly in settings with limited resources. MediGuide-14B, with its tailored training for medical applications, demonstrates considerable promise for clinical integration. However, the variability in performance underscores the need for further fine-tuning and contextual adaptation to improve reliability and efficacy in medical applications. Future research should focus on optimizing LLMs for specific medical tasks and integrating them with conventional medical systems to leverage their full potential in real-world settings.",
    "full_text": "1 \n \nReal-World Performance of Large Language Models in Emergency 1 \nDepartment Chest Pain Triage 2 \nXiangbin Meng1,2*, Jia-ming Ji3*, Xiangyu Yan4*, Hua Xu3, Jun gao1, Junhong Wang5, Jingjia 3 \nWang1, Xuliang Wang1, Yuan-geng-shuo Wang1, Wenyao Wang1, Jing Chen6, Kuo Zhang6, Da 4 \nLiu7, Zifeng Qiu8， Muzi Li9, Chunli Shao1, Yaodong Yang3#, Yi-Da Tang1.2# 5 \n1.Department of Cardiology and Institute of Vascular Medicine, Peking University Third 6 \nHospital. 7 \n2.State Key Laboratory of Vascular Homeostasis and Remodeling, Peking University. 8 \n3. Institute for Artificial Intelligence, Peking University. 9 \n4. Institute of Disaster and Emergency Medicine, Tianjin University. 10 \n5. Emergency department， Peking university third hospital. 11 \n6. Department of Cardiology, State Key Laboratory of Cardiovascular Disease, Fuwai 12 \nHospital, National Center for Cardiovascular Diseases, Chinese Academy of Medical 13 \nSciences and Peking Union Medical College. 14 \n7.Department of Cardiology, the First Hospital of Hebei Medicical University, Graduate 15 \nSchool of Hebei Medical University. 16 \n8. Peking University Health Science Center， Peking University First Hospital 17 \n9. Peking University Health Science Center， Peking University People's Hospital 18 \n* These authors contributed equally to this study. 19 \nAbbreviated Title: LLMs in Emergency Department Chest Pain Triage 20 \nKeywords: Large Language Models (LLMs), Real-World Performance, Emergency 21 \nDepartment Chest Pain Triage, Acute Coronary Syndrome (ACS), Diagnostic capabilities 22 \n2 \n \nWord count: 295 (abstract); 7848 (excluding abstract, figure legends, and references) 1 \nNumber of figures and tables: 3 Figures, 1 Table (7 Supplementary Figures, 4 2 \nSupplementary Table) 3 \nConflict of interest statement 4 \nThe authors have no potential conflict of interest to declare. 5 \nCorresponding to: Yi-Da Tang, MD, PhD and Yao-dong Yang, PhD 6 \nYi-Da Tang, MD, PhD, Department of Cardiology and Institute of Vascular Medicine, Peking 7 \nUniversity Third Hospital. State Key Laboratory of Vascular Homeostasis and Remodeling, 8 \nPeking University, Beijing, China. No.49 Huayuanbei Road, Beijing 100191, China (Email: 9 \ntangyida@bjmu.edu.cn). 10 \nYao-dong Yang, PhD, Institute for Artificial Intelligence, Peking University, Beijing, China. 11 \nNo.5 Yi HeYuan Road, Beijing 100871, China (Email: yaodong. yang@pku.edu.cn). 12 \n 13 \n 14 \n 15 \n 16 \n 17 \n 18 \n 19 \n 20 \n 21 \n 22 \n3 \n \nAbstract 1 \nBackground: Large Language Models (LLMs) are increasingly being explored for medical 2 \napplications, particularly in emergency triage where rapid and accurate decision-making is crucial. 3 \nThis study evaluates the diagnostic performance of two prominent Chinese LLMs, \"Tongyi 4 \nQianwen\" and \"Lingyi Zhihui,\" alongside a newly developed model, MediGuide-14B, comparing 5 \ntheir effectiveness with human medical experts in emergency chest pain triage. 6 \nMethods: Conducted at Peking University Third Hospital's emergency centers from June 2021 to 7 \nMay 2023, this retrospective study involved 11,428 patients with chest pain symptoms. Data were 8 \nextracted from electronic medical records, excluding diagnostic test results, and used to assess the 9 \nmodels and human experts in a double-blind setup. The models' performances were evaluated 10 \nbased on their accuracy, sensitivity, and specificity in diagnosing Acute Coronary Syndrome 11 \n(ACS). 12 \nFindings: \"Lingyi Zhihui\" demonstrated a diagnostic accuracy of 76.40%, sensitivity of 90.99%, 13 \nand specificity of 70.15%. \"Tongyi Qianwen\" showed an accuracy of 61.11%, sensitivity of 14 \n91.67%, and specificity of 47.95%. MediGuide-14B outperformed these models with an accuracy 15 \nof 84.52%, showcasing high sensitivity and commendable specificity. Human experts achieved 16 \nhigher accuracy (86.37%) and specificity (89.26%) but lower sensitivity compared to the LLMs. 17 \nThe study also highlighted the potential of LLMs to provide rapid triage decisions, significantly 18 \nfaster than human experts, though with varying degrees of reliability and completeness in their 19 \nrecommendations. 20 \nInterpretation: The study confirms the potential of LLMs in enhancing emergency medical 21 \ndiagnostics, particularly in settings with limited resources. MediGuide-14B, with its tailored 22 \n4 \n \ntraining for medical applications, demonstrates considerable promise for clinical integration. 1 \nHowever, the variability in performance underscores the need for further fine-tuning and 2 \ncontextual adaptation to improve reliability and efficacy in medical applications. Future research 3 \nshould focus on optimizing LLMs for specific medical tasks and integrating them with 4 \nconventional medical systems to leverage their full potential in real-world settings. 5 \n  6 \n5 \n \nIntroduction 1 \nLarge Language Models (LLMs) are revolutionizing the medical field, particularly in accelerating 2 \npre-hospital triage1-10. These models leverage deep learning and natural language processing 3 \ntechnologies to capture patterns and relationships in text through multi-layer neural network 4 \narchitectures, enabling efficient processing and precise understanding of vast medical data 4-11. 5 \nTrained on extensive text data, LLMs have acquired a wealth of vocabulary, grammar, semantics, 6 \nand a condensed vast knowledge system, allowing them to respond coherently and accurately to 7 \nvarious symptom descriptions, medical history information, and literature queries provided by 8 \nusers 12-15. Importantly, as the model parameters and training data volume increase, scientists have 9 \nobserved significant \"emergence abilities\" in LLMs16-18. This ability is not only reflected in the 10 \nefficient processing of complex information but also in their superior performance in logical 11 \nreasoning and innovative thinking19. This gives LLMs unique advantages in simulating human 12 \nthought patterns, understanding, and applying knowledge, especially in the field of medical 13 \ndiagnostic assistance20,21.  14 \nGlobally, especially in remote areas of developing and developed countries, there is a severe 15 \nshortage of primary healthcare resources, characterized by insufficient facilities, lack of 16 \nprofessional personnel, and financial constraints22,23. Trained medical providers, including doctors, 17 \nnurses, and other community health workers, are scarce, making it difficult to provide high-quality 18 \nprimary healthcare services, further exacerbating the imbalance of medical human resources 19 \nbetween urban and rural areas24. Traditional clinical prediction models based on machine learning 20 \nor deep learning, despite having theoretical application prospects, are rarely deployed in actual 21 \nclinical practice. The main reason is that these models generally lack generalizability and cannot 22 \n6 \n \neffectively handle the complex and variable actual clinical data, and the required parameters are 1 \noften not easily obtained in clinical settings25. In contrast, LLMs, with their strong information 2 \nintegration and logical reasoning abilities, extensive knowledge reserves, and seamless integration 3 \nwith human language, can overcome these challenges. 4 \nIn medical diagnostic assistance scenarios, the value of LLMs is particularly prominent.  5 \nTraditional diagnostic models often struggle to cope with patients' complex symptom expressions, 6 \nintricate medical histories, and vast medical literature due to limited processing capacity or 7 \ninsufficient knowledge coverage. LLMs, with their vast training data and deep learning 8 \narchitectures, can quickly organize and integrate various clinical information, using their 9 \nembedded extensive medical knowledge base to accurately classify and analyze diseases 8,9. 10 \nFurthermore, LLMs' logical reasoning ability allows them to conduct in-depth analysis of complex 11 \ndisease clues, construct disease progression path models, predict potential complications, and 12 \nprovide doctors with detailed and in-depth diagnostic support. 13 \nHowever, a cautiously optimistic attitude should be maintained towards the application of LLMs 14 \nin the medical environment, fully recognizing their limitations. These limitations include but are 15 \nnot limited to: potential bias in training data, which may lead to unfairness in model 16 \ndecision-making; challenges in explaining complex medical details, which may affect the 17 \nunderstanding and trust of doctors and patients in model outputs; and the risk of misdiagnosis due 18 \nto over-reliance on technology without necessary human supervision. Therefore, while affirming 19 \nthe transformative potential of LLMs in healthcare, it is also crucial to focus on and address these 20 \nchallenges to ensure their application in medical diagnostics is both safe and effective. 21 \nA noteworthy challenge arises when applying LLMs to languages such as Chinese, Japanese, 22 \n7 \n \nKorean, Tamil, Hindi, Thai, and Vietnamese, which employ \"non-segmented text\" structures that 1 \nmarkedly differ from English in terms of grammar, syntax, and usage. Most research endeavors 2 \nhave predominantly concentrated on English-centric models like ChatGPT, leaving a notable 3 \nresearch gap in evaluating the diagnostic proficiencies of large language models trained 4 \nspecifically for non-English environments26,27. Although English and 'non-segmented text' 5 \nlanguages share similar fundamental principles in the development of LLMs, they face distinct 6 \ntechnical and engineering challenges in practical applications due to differences in language 7 \ncharacteristics and available resources. This leads to variations in their implementation and 8 \nperformance. We must consider the fundamental differences in grammatical structures, data 9 \nresources, vocabulary size and distribution, algorithmic implementation and optimization, as well 10 \nas the adaptability of technical architectures across different languages. This research gap impacts 11 \nthe broad adoption of these models in diverse linguistic contexts and profoundly influences the 12 \nlevel of trust vested in LLMs.  13 \nThe emergency medical setting is distinguished by its immediacy, intricate clinical presentations, 14 \nand the imperative need for prompt diagnosis28. The environment of emergency room is often 15 \ndynamic, extremely busy, and high-pressure, requiring healthcare personnel to make rapid 16 \ndecisions and handle multiple cases simultaneously29. A study in 2019 found that the average wait 17 \ntime for emergency department patients was approximately 40 minutes before being seen by a 18 \nphysician, with doctors spending an average of 13-24 minutes per patient during the consultation 19 \n30-32. Emergency triage systems are used globally to assess patient severity and allocate 20 \nresources33-35. The US uses Emergency Severity Index (ESI), a 5-tier system. China uses 21 \nEmergency Triage Scale/Standard (ETS), a 4-tier system. ETS is like ESI, with levels 1&2 triaged 22 \n8 \n \nto resuscitation. Patients in levels 3&4 wait to see a physician. Though ETS is generally accurate, 1 \nsome critical patients wait hours and misdiagnosis is a pronounced concern, as research 2 \nunderscores a notably elevated misdiagnosis rate within the emergency room 36,37. This issue is 3 \nexacerbated, particularly for common symptoms associated with myocardial ischemia, which are 4 \nsusceptible to oversight or misjudgment 38. The guidelines recommend reperfusion therapy within 5 \n12 hours of the onset of myocardial infarction39,40, yet approximately 70% of acute myocardial 6 \ninfarction patients succumb to the disease due to the missed opportunity for timely treatment 41, 7 \nhighlighting the risk of misdiagnoses leading to treatment delays. LLMs stand poised to bring 8 \nabout significant transformations in specific medical contexts, notably expediting pre-hospital 9 \ntriage procedures. We see potential in these models to facilitate rapid triage by assisting healthcare 10 \nproviders in swiftly processing patient data and offering potential diagnoses rooted in symptoms, 11 \nmedical histories, and pertinent literature.  12 \nThe medical profession demands precise and dependable tools for informed decision-making. 13 \nWhile LLMs hold potential, they present difficulties in understanding context and obtaining 14 \nclarifications42-47. Addressing real-world medical issues requires handling multiple data modalities 15 \nand must also provide authenticity, authority, accessibility, safety, empathy, and a human touch48. 16 \nReal-world medical problems often transcend the confines of multiple-choice tests and structured 17 \ntasks. The human or AI model approach to diagnostic interaction, whether single or multi-turn 18 \ndialogues and their ability to process various data modalities are equally important. Indeed, 19 \nevaluating these vast medical models might not be any simpler than developing them.  20 \nStandardized testing has largely evaluated these models' medical knowledge reserves and 21 \ndiagnostic logical reasoning capabilities 2,6,8,49. However, medical issues in the real world often 22 \n9 \n \nsurpass the scope of structured tasks and multiple-choice tests, exhibiting greater complexity and 1 \nuncertainty7,50. Currently, there is a particular lack of systematic evaluation globally on the 2 \neffectiveness of large language models in real medical environments, especially based on rich, 3 \ndiverse, and dynamically changing real medical data 51.  4 \nIn this study, we focused on evaluating two prominent Chinese language models, \"Tongyi 5 \nQianwen (通/i13 千/i13 ) (V1.0.3 )\"and \"Lingyi Zhihui (灵医 智 慧 ) (V2.2.0 )\"52-54, which are 6 \ndeveloped based on a Transformer's autoregression framework, akin to other globally recognized 7 \nmodels like Meta's LLaMa Series, Google's PaLM and LaMDA, and OpenAI's ChatGPT series. 8 \nAs general-purpose large models, they have not been specifically fine-tuned for the medical 9 \ndomain and are currently offered as online services by their respective operators. One of the core 10 \nobjectives of our research is to conduct a comprehensive evaluation of these two models using 11 \ncase data from Chinese patients, focusing particularly on their performance in processing Chinese 12 \nmedical contexts, given that they are primarily trained on Chinese datasets, although they also 13 \nincorporate a certain proportion of English data. A key component of the study is a comparative 14 \nanalysis of the diagnostic accuracy of \"Tongyi Qianwen\" and \"Lingyi Zhizhi\" in handling complex 15 \nand urgent medical data, benchmarking their results against medical experts' judgments. Targeted 16 \nenhancements and optimizations were applied to the fine-tuning and alignment phases, 17 \nparticularly for LLMs tasked with medical applications. Recognizing the performance variability 18 \nof LLMs underscores the importance of meticulously establishing benchmarks that are apt for 19 \nmedical artificial intelligence. We further integrated the insights gained from the benchmark 20 \nconducted into developing a new model called MediGuide-14B.  21 \nMethods 22 \n10  \n \nStudy Design and Setting    1 \nThis retrospective study was conducted at the Emergency Chest Pain Centers of the Peking 2 \nUniversity Third Hospital Group, involving five tertiary-level centers. The study received ethical 3 \napproval from the ethics committee of Peking University Third Hospital (M2023828), complying 4 \nwith the Helsinki Declaration. 5 \nData Sources 6 \nChief complaints, current medical history, past medical history, family history, and personal 7 \nhistory were extracted from electronic medical records as unstructured text content. It is important 8 \nto note that diagnostic test results such as electrocardiograms, myocardial enzyme tests, and 9 \nechocardiograms were not included in the test dataset provided to the test model and the control 10 \ngroup. Confirmed diagnostic information and related evidence were only made available during 11 \nthe subsequent double-blind evaluation phase to the expert review committee, which served as the 12 \nauthoritative reference standard. This approach aimed to ensure that experts could conduct 13 \naccurate and fair comparative analyses of the diagnostic accuracy of each group by combining 14 \ncomprehensive and detailed medical test data with the model's predictive results. 15 \nParticipants 16 \nThe inclusion criteria for this study were patients who visited the emergency chest pain center at 17 \nPeking University Third Hospital's five tertiary-level centers due to chest pain-related symptoms 18 \nbetween June 2021 and May 2023. The exclusion criteria were: 1.Cases with significant omissions 19 \nor incompleteness in medical records, such as missing key clinical assessment records or essential 20 \nauxiliary examination results, which are crucial for a comprehensive patient evaluation; 2. Cases 21 \nwhere the chief complaint information did not come directly from the patients themselves due to 22 \n11  \n \nunstable vital signs or other reasons but was obtained through the accounts of others, making it 1 \ndifficult to accurately trace and complete. Given that such situations could affect the credibility of 2 \nthe study results due to the one-sidedness of the information or decreased accuracy of symptom 3 \ndescription, these cases were excluded in the analysis process. Ultimately, 11,428 patients who 4 \nmet the above inclusion and exclusion criteria were included in the study.  5 \nOutcome 6 \nThe study employed repeated random sampling for case selection, using Python 3.8 to randomly 7 \nselect 100 patient cases from the database, repeated 1000 times. This method ensured a diverse 8 \nand random sample, reducing potential bias. All medical records were anonymized to maintain 9 \npatient privacy, further minimizing selection bias and enhancing the generalizability of the 10 \nfindings. 11 \nThe primary outcome of this study was the accuracy of diagnosing Acute Coronary Syndrome 12 \n(ACS). The study used LLMs prompts that included patient demographics, clinical symptoms, and 13 \nmedical history, which are commonly found and critically important in primary healthcare and 14 \nemergency scenarios. This approach mimics the situations where lab results are not yet available, 15 \nand doctors and medical professionals must rely solely on the patient's chief complaints and past 16 \nmedical history to triage chest pain and provide rapid management.  In the study, the 17 \ncardiovascular physicians' group also followed the same information dimensions and problem 18 \nstructure as the LLMs for case analysis. These cardiology specialists, during the diagnostic 19 \nprocess, similarly needed to interpret each case based on the patient's age, gender, chief complaint, 20 \npresent illness history, family history, and personal history, and make diagnostic and therapeutic 21 \ndecisions accordingly. 22 \n12  \n \nStudy Size 1 \nTo ensure a statistical power of 0.8 and effectively compare the diagnostic accuracy between 2 \nhuman medical experts and Large Language Models (LLMs) optimized for the medical 3 \nenvironment, it is necessary to determine an appropriate sample size. Based on preliminary data 4 \nand relevant literature, the diagnostic accuracy of human experts usually falls within the range of 5 \n0.9 to 0.95, while the accuracy of general-purpose LLMs not specifically trained is between 0.7 6 \nand 0.8. LLMs that have been optimized for the medical field have improved their accuracy to 7 \nlevels comparable to human experts, although there may still be slight differences between the 8 \ntwo55. In this context, to ensure a statistical power of 0.8, sufficient to detect the potential small 9 \ndifference in accuracy between human experts and optimized LLMs, we calculated that each 10 \ngroup needs approximately 3835 samples. This sample size ensures that even minor differences in 11 \naccuracy can be detected with an 80% probability in statistical tests. 12 \nConsidering the study subjects, we selected five centers affiliated with Peking University Third 13 \nHospital, each of which receives about 5000 patients with chest pain annually. Given that clinical 14 \ndata often have high noise, sparsity, and heterogeneity, which not only increase the difficulty of 15 \ndata analysis but also may affect the robustness of the research conclusions, we decided to set the 16 \nstudy period from June 2021 to May 2023, totaling two years. This time frame was chosen to 17 \naccumulate sufficient case data, overcome the inherent complexity of clinical data, and ensure the 18 \neffectiveness and reliability of the final statistical analysis. 19 \nFigure 1 illustrates the flow chart of the entire study. The sample size of 11,428 records was 20 \ndetermined based on the hospital's patient flow and data availability during the study period. This 21 \n13  \n \nsize was deemed sufficient to provide a robust analysis of the LLMs' diagnostic performance. 1 \n(Figure 1) 2 \nLLMs: 3 \n\"Tongyi Qianwen,\" developed by Alibaba Group, is a 100 billion-parameter model with a diverse 4 \ndata foundation, including web texts and specialized literature, and utilizes advanced 5 \nreinforcement learning techniques like A2C/A3C and PPO, and Q-learning methods such as DQN 6 \nand C5156,57. 7 \n\"Lingyi Zhihui,\" created by Baidu Group for medical contexts, has 260 billion parameters. It 8 \ncombines auto-regressive and auto-encoding frameworks, suited for natural language 9 \nunderstanding and generation, and supports zero-shot, few-shot learning, and detailed 10 \nfine-tuning58,59.  11 \nDiagnostic performance:  12 \nThe anonymized dataset, excluding lab test results, was analyzed by \"Tongyi Qianwen,\" \"Lingyi 13 \nZhihui,\" and a group of human experts. The human experts were eight cardiovascular specialists 14 \nwith over ten years of experience, certified by the Chinese Society of Cardiology and the Chinese 15 \nMedical Association. Both LLMs and human experts were given detailed patient information 16 \nencompassing age, gender, chief complaints, present illness, and medical history. In this study, all 17 \nparticipants underwent comprehensive training before commencement to ensure a thorough 18 \nunderstanding of the research process. During the actual testing phase, we observed and recorded 19 \nthe time taken by each diagnostic group to complete a test unit containing 100 medical case 20 \nrecords. 21 \nPrompt Engineering：  22 \n14  \n \nFor large language models, appropriate prompts are necessary to activate their respective 1 \ncapabilities. (Supplementary Table S1) 2 \nThe prompt is: \"1. Based on the patient's basic information, chief complaint, symptoms, and 3 \nmedical history, what do you consider to be the patient's diagnosis? 2. Considering your 4 \nconsidered diagnosis, what further tests and medical advice would you recommend? 3.  Please 5 \nevaluate the risk of the patient's condition based on the above patient information\".  6 \nSimilarly, a group of human experts will respond to the same questions based on the patient's basic 7 \ninformation, chief complaint, symptoms, and medical history. 8 \nReference standards:  9 \nThe gold standard of final diagnosis was the consensus of an independent cardiology expert panel 10 \nwith 20 years of clinical service experience following Chest Pain Management Guidelines 38,60,61. 11 \nThis panel had full access to patient records and cardiovascular lab test results, ensuring a 12 \ncomprehensive and authoritative standard for comparison. The definitive diagnosis for cases 13 \nwhere there was uncertainty was established using V oting Mechanisms. (Figure 1) 14 \nStatistical analysis:  15 \nThe statistical analysis in this study was conducted using the SPSS 27.0 statistical package for 16 \nWindows, Python version 3.8, and R software version 4.2.2, provided by the R Foundation for 17 \nStatistical Computing. All continuous variables that adhered to a normal distribution were 18 \nrepresented as means with their 95% confidence intervals (CI). To identify initial differences in 19 \nbaseline characteristics between treatment groups, bivariate analyses were performed utilizing 20 \nStudent's t-test. For comparative analyses among multiple groups, a one-way ANOVA test was 21 \nemployed. A p-value of less than 0.05 was designated as the threshold for statistical significance. 22 \n15  \n \nIn assessing the differences in diagnostic efficacy for cardiovascular diseases among the study 1 \ngroups, this r esearch employed a comprehensive and multifaceted set of evaluation metrics, 2 \nincluding Accuracy, Sensitivity, Specificity, False Positive Rate (FPR), Recall, and confusion 3 \nmatrix diagrams among other multidimensional indicators. These multidimensional indicators 4 \ntogether form a rigorous and comprehensive performance evaluation framework, aimed at 5 \ncomprehensively comparing and assessing the strengths and weaknesses of each study group in 6 \nterms of diagnostic accuracy and effectiveness. 7 \nBefore evaluating the diagnostic metrics of each group, the study initially assessed the distribution 8 \ncharacteristics of the results from 1000 test units in each group using probability density curves, 9 \nP-P plots, and Q-Q plots to conduct normality tests. 10 \nThe Development of MediGuide-14B 11 \nMediGuide-14B is developed on the foundation of the Qwen-14B model, undergoing extensive 12 \noptimization and specialized transformation through a meticulously crafted medical data training 13 \nand tuning program. Qwen-14B boasts 14 billion model parameters, endowing it with powerful 14 \nlearning capabilities, ample knowledge reserves, and robust logical reasoning. During its 15 \nfoundational training, Qwen-14B assimilated knowledge from over three trillion tokens, spanning 16 \nChinese, English, and various other languages, including specialized domains like programming 17 \nand mathematics. Qwen-14B excels in natural language understanding, mathematical 18 \nproblem-solving, logical reasoning, and computer programming. This base model supports 19 \ncomprehensive fine-tuning, allowing for deep and customized adjustments tailored to various 20 \ntasks and domains. 21 \nThe special medical database built by the research team includes detailed medical records of 22 \n16  \n \n105,290 outpatients and inpatients, totaling 2 million pieces of professional medical data that have 1 \nbeen carefully cleaned and protected for privacy. The training of MediGuide-14B is completed on 2 \na high-performance server equipped with A800 80G*8.  Leveraging the power of the DeepSpeed 3 \nframework and the Transformer architecture, we have optimized MediGuide-14B for better 4 \nperformance and efficiency. This is crucial in handling the complexities and nuances of medical 5 \ndiagnostics. The integration of Cross-Entropy Loss function and Reinforcement Learning from 6 \nHuman Feedback (RLHF) in the training process further refines the model’s accuracy and 7 \nhuman-like understanding, addressing the high sensitivity yet lower specificity issue identified in 8 \nprevious models.  9 \nRole of the Funding Source: 10 \nIn the design of the study; collection, analysis, and interpretation of data; writing of the report; and 11 \nthe decision to submit the paper for publication, the study sponsors had no involvement. All 12 \nresponsibilities and decisions regarding the research were made independently by the authors. 13 \nResults  14 \nOverview of Study Population 15 \nIn the study involving 11,428 individuals who presented with emergency chest pain, after initially 16 \nassessing 12,015 potential participants, 587 were excluded due to significant gaps in their medical 17 \nrecords or indirect patient complaints. The study group had an average age of 64.82 years, with a 18 \nbroad age distribution from 15 to 101 years, highlighting a significant elderly presence, underlined 19 \nby a median age of 67 years. Men constituted 65.4% of the participants. 20 \nThe average Body Mass Index (BMI) for the cohort was 25.41, with a standard deviation of 3.69. 21 \nMedical evaluations revealed an average systolic blood pressure of 124.28 mmHg, diastolic blood 22 \n17  \n \npressure of 75.35 mmHg, and heart rate of 70.38 bpm. The patient histories showed varying 1 \nprevalences of conditions: 8.7% had chest pain, 15.3% experienced dyspnea or chest tightness, 2 \nand 3.7% had episodes of syncope. Additionally, there were notable rates of chronic conditions, 3 \nincluding diabetes (7.9%), hypertension (23.5%), and hyperlipidemia (17.3%). Lifestyle factors 4 \nwere also recorded, with 18.9% having a smoking history and 15.3% with a history of alcohol 5 \nconsumption. In terms of emergency severity, 2.8% of cases were classified as critical or severe, 6 \nwhile 14.7% were urgent, and the majority, 82.5%, were less urgent. The diversity of 7 \ncardiovascular conditions was evident in the primary diagnoses. (Supplementary Table S2) 8 \nThe disease composition spectrum of 11,428 patients was analyzed based on the \"primary 9 \ndiagnosis\" of discharge diagnosis. The most common cardiovascular issues were NSTEMI/UA 10 \nUnstable Angina (24.3%), followed by Stable Angina Pectoris (14.8%) and STMI (7.4%). Other 11 \ncardiovascular diagnoses included Chronic Coronary Syndrome, Aortic Dissection, and Acute 12 \nPulmonary Embolism. Hypertensive emergencies varied in severity and risk, with a range of 13 \nstages and risks documented. Arrhythmias formed a significant part of the diagnoses, with 14 \nconditions like Paroxysmal and Persistent Atrial Fibrillation, Atrial Flutter, and 15 \nWolff-Parkinson-White Syndrome being prevalent. Heart failure variants were also noted, along 16 \nwith other cardiac conditions such as Old Myocardial Infarction and Aortic Valve Insufficiency. 17 \nThis detailed assessment underscores the wide spectrum of cardiovascular diseases managed in the 18 \nemergency setting, reflecting the complexity and diversity of the patient population. 19 \n(Supplementary Table S3) 20 \nPerformance of LLM 21 \nWe assessed the normality of the LLM's performance distribution using kurtosis, skewness, 22 \n18  \n \nprobability density curve, P-P diagram, and Q-Q diagram. (Supplementary Figure S1) 1 \nOur analysis confirmed a normal distribution without significant outliers. Regarding the diagnosis 2 \nefficacy, \"Tongyi Qianwen\" achieved an accuracy of 61.11% (95% CI:60.84%-61.29), with a 3 \nsensitivity of 91.67% (95% CI:91.37%-91.96%) and a specificity of 47.95% (95% 4 \nCI:47.65%-48.25%). \"Lingyi Zhihui\" demonstrated an accuracy of 76.40% (95% 5 \nCI:76.17%-76.63%), with a sensitivity of 90.99% (95% CI:90.67%-91.31%) and a specificity of 6 \n70.15% (95% CI:69.85%-70.44%). The human experts were asked to perform the diagnostic test 7 \nbased on the same content fed to LLMs. A total of 8 physicians completed this task. Human 8 \nexperts achieved a mean accuracy of 86.37% (95% CI:86.18%-86.55%), a sensitivity of 79.62% 9 \n(95% CI:79.20%-80.04%), and a specificity of 89.26%(95% CI:89.06%-89.46%) （ Table 1） . 10 \nThe language models \" Tongyi Qianwen \"and \"Lingyi Zhihui\" completed the task in 24.68± 2.23 11 \nand 28.75± 3.25 minutes, respectively. On the other hand, human physicians completed the task 12 \nwithin a range of 65.25± 10.45 minutes (Figure 1a). 13 \nWe plotted the parameters of diagnostic performance using radar charts. The area under the curve 14 \nfor \"Lingyi Zhihui\" was 8094.76 units, which was more significant than the 5597.88 units for 15 \n\"Tongyi Qianwen\". However, human experts had the best overall performance, totaling 9393.36 16 \nunits ( Figure 2a ). The area for \"Tongyi Qianwen\" performance primarily spans over the 17 \n\"Sensitivity\" region but is relatively smaller in the \"Specificity\" and \"Accuracy\" regions. \"Lingyi 18 \nZhihui\" had a larger area in all three parts compared to \"Tongyi Qianwen\", especially in the 19 \n\"Specificity\" domain. The area for human experts was substantial in both the \"Specificity\" and 20 \n\"Accuracy\" regions but slightly smaller in the \"Sensitivity\" domain. 21 \nBoth \"Tongyi Qianwen\" and \"Lingyi Zhihui\" demonstrated a high level of sensitivity, 22 \n19  \n \nindicating their capability to detect the majority of true ACS cases. However, their specificity was 1 \ncomparatively lower, implying the potential for misclassifying some non-ACS cases as ACS. High 2 \nsensitivity is pivotal in screening tools, as they aim to identify the most genuine cases, even at the 3 \nrisk of producing some false positives. Ensuring the accurate detection of real diseases or 4 \nabnormalities is a critical attribute of screening tools. Consequently, LLMs are well-suited as 5 \nscreening tools, particularly in life-threatening emergency scenarios. In such situations, the 6 \nprimary goal during screening is to identify as many true cases as possible, minimizing the risk of 7 \noverlooking vital information. However, it's important to acknowledge that a trade-off exists 8 \nbetween high sensitivity and specificity, meaning that while a screening tool can capture most 9 \ngenuine cases, it may also generate some false positives (false alarms), which must be carefully 10 \nconsidered. (Figure 2 b. c) 11 \n\"Tongyi Qianwen\" model achieved a true positive rate (TPR) of 91.67% and a concomitant 12 \nfalse positive rate (FPR) of 52.05%. The model's accuracy is 43.10%, consistent with its recall. On 13 \nthe other hand, \"Lingyi Zhihui\" shows that its TPR and recall rate are both 90.99%, but its FPR is 14 \nsignificantly reduced to 29.85%, and its accuracy rate is 56.87%. In contrast, the human expert's 15 \nTPR was 79.62%, the FPR was reduced considerably to 10.74%, and the accuracy was 76.34%, 16 \nconsistent with its recall rate (Figure 2b). 17 \nAmong the cases misdiagnosed as ACS by the \"Tongyi Qianwen\" test, approximately 7.34% 18 \n(95% CI: 7.07%-7.60%) were eventually diagnosed as aortic dissection, and 3.45% were 19 \ndiagnosed as acute pulmonary embolism according the reference standard (95% CI: 20 \n3.26%-3.63%). The rest were other non-ACS diseases with chest pain manifestations. Among the 21 \ncases misdiagnosed as ACS by the \"Lingyi Zhihui\", the average proportion of cases that were 22 \n20  \n \neventually diagnosed as acute aortic dissection was approximately 9.14% (95% CI: 8.83%-9.44%). 1 \nThe average proportion of patients who were eventually diagnosed with acute pulmonary 2 \nembolism was 2.83% (95% CI: 2.63%-3.03%). 3 \nAlthough human experts presented higher accuracy, there were still some cases misdiagnosed. 4 \nAmong the total cases misdiagnosed as ACS by human experts, acute aortic dissection accounted 5 \nfor 5.27% (95% CI: 4.80%-5.74%) and acute pulmonary embolism accounted for 0.96% (95% CI: 6 \n0.74%-1.18%). The discrepancies among LLMs and human experts are statistically significant. 7 \n(Supplementary Figure S2) 8 \nAdvancements in Medical Large Language Models: The Performance of MedGuide-13B 9 \nAfter evaluating the performance of various commercially available closed-source Large 10 \nLanguage Models in medical diagnostics, we enhanced their capabilities by improving model 11 \narchitecture, refining algorithms, and boosting fine-tuning and alignment techniques to increase 12 \naccuracy and reduce misdiagnoses. From these comprehensive benchmarks, we distilled key 13 \ninsights that provided a solid foundation for the development of new language models. 14 \nConsequently, we developed the MediGuide-14B model, which was derived by making precise 15 \nadjustments to the Qwen-14B base model. The Qwen-14B model, known for its strong natural 16 \nlanguage understanding and problem-solving capabilities, served as an ideal starting point for the 17 \ndevelopment of MediGuide-14B. 18 \nIn the development process of MediGuide-14B, we first meticulously analyzed the issues 19 \nencountered by existing commercial general-purpose large language models when executing 20 \nmedical tasks and accordingly implemented a series of targeted parameter optimizations to 21 \nenhance their performance in the healthcare domain. In the initial phase, our focus was on 22 \n21  \n \nbolstering the model’s understanding of medical terminology. This involved expanding the 1 \nmedical professional vocabulary database and refining the model’s processing mechanisms for 2 \nthese terms. During fine-tuning, we incorporated multi-turn dialogue data derived from real-world 3 \nmedical scenarios involving 300,000 patients, significantly enhancing the model’s professionalism 4 \nand accuracy within medical contexts. 5 \nEmploying supervised fine-tuning (Supervised Fine-Tuning, SFT), the fine-tuned large model 6 \nshowed a significant improvement in accuracy when dealing with professional medical texts 7 \ncompared to the original model. Subsequently, during the alignment process of the large model, 8 \nwe introduced reinforcement learning from human feedback technology (Reinforcement Learning 9 \nfrom Human Feedback, RLHF) to guide the output distribution of the large model. We solicited 10 \nfeedback and optimization from medical experts on the model’s outputs, thereby creating a 11 \ncontrastive dataset imbued with human preferences, ensuring that the decision-making process of 12 \nthe large model not only fully leverages its reasoning capabilities but also aligns with the 13 \njudgment standards of medical professionals. A reward model (Reward Model, RM) was trained 14 \non this dataset, and reinforcement learning techniques were used to conduct further fine-tuning 15 \nalignment. 16 \nThe aligned model (aligned model) following this process demonstrated a substantial 17 \nenhancement in its generalization ability when handling actual medical data, closely adhering to 18 \nthe practical needs of medicine and effectively improving the accuracy of complex case analysis. 19 \nLastly, in the model’s inference process, we employed a chain-of-thought decomposition method 20 \nwhere complex medical scenario questions posed by users were finely dissected to accurately 21 \ncapture key information. This helped the model better comprehend the core content and logical 22 \n22  \n \nstructure of the problem, thereby enhancing both the accuracy and relevance of its responses. After 1 \nsuch granular decomposition, the model independently analyzed each sub-problem before 2 \nsynthesizing answers from all sub-problems to form a comprehensive and logically coherent final 3 \nanswer. 4 \nThrough the above-mentioned parameter optimizations and adjustments tailored for medical tasks, 5 \nMediGuide-14B has achieved a 44% improvement in capability over its predecessors. 6 \n(Supplementary Figure S3) 7 \nTo assess the efficacy of large language models in diagnosing cardiovascular diseases, we 8 \nconstructed the CVIDB test set, comprising 1,233 single-choice questions and 203 multiple-choice 9 \nquestions. This standardized and high-quality test set provides detailed explanations for each 10 \nquestion, offering insights into the reasoning behind the correct answers and enhancing learning 11 \nand understanding of complex topics. The test set covers various subtypes, developmental stages, 12 \nand related complications and comorbidities of cardiovascular diseases, effectively testing the 13 \ndepth and breadth of large language models' understanding of the field. We have made this test set 14 \npublicly available on GitHub for researchers and developers to download free of charge. The 15 \naccess link is: https://github.com/mengxiangbin123/CVIDB.git  16 \nAfter completing the training and development of the MediGuide large model, we conducted a 17 \nseries of standardized assessments, including several important medical benchmark tests. These 18 \ntests are 55,62,63: USMLE, a repository of simulated questions for the United States Medical 19 \nLicensing Examination; MedMCQA, a large-scale medical multiple-choice question dataset 20 \ncovering various disciplines, derived from medical entrance exams in India; CMC, a large-scale 21 \nmultitask knowledge assessment benchmark focusing on Chinese medical knowledge; and 22 \n23  \n \nMCMLE, a simulation of the Chinese medical qualification exam; along with the cardiovascular 1 \ndisease-specific benchmark test set, CVIDB. These resources aim to comprehensively evaluate the 2 \nperformance and generalization ability of large language models in medical knowledge and 3 \nclinical decision-making skills. We compared the performance of MediGuide-14B (V5.0) with 4 \nother leading models in the industry, including ChatGPT-4, ChatGPT-3.5 Turbo, Tongyi Qianwen 5 \n(v1.0.3), Lingyi Zhihui (v2.2.0), LLaMA 2-14B, and Qianwen-14B -Base.  6 \nFocusing on the United States Medical Licensing Examination (USMLE), ChatGPT-4 7 \ndemonstrated superior performance with a score of 80.28%, closely followed by MediGuide-14B 8 \nat 78.63%, while LLaMA 2-13B trailed significantly with only 35.04%. For the MedMCQA, 9 \nwhich consists of multiple-choice questions from Indian medical entrance exams, ChatGPT-4 10 \nagain led with a score of 72.51%, although here, the performance differences among the newer 11 \nmodels were relatively narrower. In contrast, models like ChatGPT-3.5 Turbo and Qianwen-14B 12 \n-Base showed relatively lower scores, 56.25% and 42.86%, respectively. The Composite Medical 13 \nContent (CMC) dataset, which assesses the models' understanding of medical knowledge 14 \nspecifically in the Chinese context, saw MediGuide-14B performing the best with a score of 15 \n77.56%. ChatGPT-4 and Lingyi Zhihui also showed strong results with scores above 73%. 16 \nPerformance on the Medical Chinese Medical Licensing Examination (MCMLE) again 17 \nhighlighted the effectiveness of ChatGPT-4 and MediGuide-14B, which scored 74.58% and 18 \n75.41%, respectively, demonstrating their robustness in handling questions related to the Chinese 19 \nMedical Licensing Examination. Lower-tier models, such as LLaMA 2-13B, had notably weaker 20 \nperformance, indicating possible challenges in their language-specific medical knowledge. Lastly, 21 \nin the Cardiovascular Disease Intelligence Diagnostic Benchmark (CVIDB), MediGuide-14B 22 \n24  \n \nexhibited exceptional capability, scoring the highest at 80.85%, showing its potential utility in 1 \napplications focused on cardiovascular health. ChatGPT-4 remained consistent across all 2 \nbenchmarks with scores generally above 75%, reinforcing its overall reliability in medical domain 3 \nquestion answering. (Supplementary Table S4). 4 \nMediGuide-14B underwent a thorough evaluation process like that of 'Tongyi Qianwen' and 5 \n'Lingyi Zhihui.' It was tested using 1000 test units, each consisting of 100 distinct real-world cases 6 \nsourced from actual medical scenarios. This rigorous testing framework provided a comprehensive 7 \nassessment of MediGuide-14B's performance in real-life conditions. The model achieved an 8 \nimpressive accuracy rate of 84.52%. It demonstrated high sensitivity in correctly identifying 9 \npositive results and commendable specificity in correctly identifying negative results 10 \n(Supplementary Figure S4). 11 \nExtended recommendations by LLMs 12 \nFor the reference standards, we invited a panel of four distinguished cardiovascular specialists, 13 \neach with over twenty years of clinical experience. To further evaluate the possibility of LLM's 14 \nrole in emergency Chest Pain Triage, we asked them to arbitrarily evaluate the treatment 15 \nrecommendations generated from the prompts. The evaluation was based on established guidelines 16 \nfor diagnosing and treating chest pain and full access to an array of essential patient data: from 17 \nelectrocardiograms (ECGs) and cardiac enzyme tests to echocardiograms, NT-proBNP evaluations, 18 \nand when indicated, results from coronary angiography.  19 \nAs illustrated in Supplementary Figure S5, 3.32% (95% CI:3.22%-3.41%) of the 20 \nrecommendations generated by \"Tongyi Qianwen\" were deemed unsuitable. This resulted in 21 \nsignificant omissions of critical content that could potentially endanger patients. However, 12.88% 22 \n25  \n \n(95% CI:12.71%-13.04%) of the recommendations were considered reasonable, albeit incomplete, 1 \nwith no direct harm to patients. The remaining 83.81% (95% CI:83.64%-83.98%) of 2 \nrecommendations were classified as comprehensive and appropriate.  3 \nRegarding the \"Lingyi Zhihui\" model, 3.40% (95% CI:3.30%-3.50%) of recommendations were 4 \ndeemed inappropriate with inherent risks. 43.44% (95% CI:43.18%-43.69%) were considered 5 \nreasonable but not exhaustive, devoid of direct patient harm. Meanwhile, 53.16% (95% 6 \nCI:52.91%-53.41%) of recommendations were thoroughly comprehensive and relevant.  7 \nThe diagnostic suggestions from human experts were deemed that 2.48% (95% CI:0.28%-4.68%) 8 \nwere inappropriate and could potentially compromise timely patient treatment. Another 12.96% 9 \n(95% CI:7.69%-18.23%) were considered reasonable but not comprehensive, while a substantial 10 \n84.56% (95% CI:74.45%-94.67%) were acknowledged as fully comprehensive and appropriate.  11 \nThe performance assessment of the MediGuide-14B group in terms of treatment recommendations 12 \nreveals a nuanced picture. A small fraction, specifically 2.90% (95% CI:1.86%-3.94%), of the 13 \nrecommendations were categorized as unreasonable and risky, highlighting areas where caution is 14 \nnecessary. On the other hand, 13.52% (95% CI:11.38%-15.64%) of the recommendations were 15 \ndeemed reasonable, albeit incomplete, suggesting a foundation of sound medical guidance that 16 \ncould benefit from further elaboration or additional information. The majority, 83.58% (95% 17 \nCI:81.28%-85.98%) of the recommendations from the MediGuide-14B group stood out as both 18 \nreasonable and comprehensive, indicating a high level of proficiency in providing well-rounded 19 \nand thorough treatment advice. (Supplementary Figure S5) 20 \nThe impact of employment status of patients on the ACS Diagnostic Accuracy of LLMs  21 \n26  \n \nNext, we sought to evaluate the impact of employment status of patients on LLM diagnostic 1 \naccuracy. We hypothesized that employment status may be linked to inherent characteristics that 2 \ncould influence the information extracted from a patient's chief complaint. Given that individuals 3 \ncovered by the Urban and Rural Resident Basic Medical Insurance (URRBMI) are typically 4 \nunemployed or self-employed, while those covered by the Urban Employee Basic Medical 5 \nInsurance (UEBMI) are typically employed by institutions, we leveraged health insurance data 6 \nextracted from medical records to infer the employment status of patients 64,65. In the case of 7 \npatients under the URRBMI insurance plan, \"Tongyi Qianwen\" exhibited a diagnostic accuracy of 8 \n58.56%, while it demonstrated a slightly higher accuracy of 60.74% among patients under the 9 \nUEBMI plan (P<0.05). Interestingly, the diagnostic accuracy of \"Lingyi Zhihui\" was also affected 10 \nby the employment status of the patients: an accuracy rate of 77.62% under URRBMI and 74.75% 11 \nunder UEBMI (P<0.05). 12 \nIn the evaluation of MediGuide-14B's performance across different insurance types, the group 13 \ndemonstrated notable results in the realm of Supplementary Diagnosis accuracy. For cases covered 14 \nunder the URRBMI, the MediGuide-14B achieved a mean accuracy of 83.65%, under the UEBMI 15 \ncategory, the mean accuracy recorded was slightly higher, at 85.04% (P=0.046). (Supplementary 16 \nFigure S6)  17 \nThe impact of patient’s history on LLMs diagnosis efficacy  18 \nMedical history is crucial in diagnosis, offering insights into a patient's past health and disease risk 19 \nfactors. While human doctors can interpret this data based on experience, LLMs face a challenge 20 \nin doing so effectively. We sought to analyze how medical history affects LLMs’ diagnosis 21 \nperformance.  22 \n27  \n \nIn the absence of medical history, \"Tongyi Qianwen\" initially demonstrated a mean accuracy of 1 \n72.66% (95% CI: 72.43%-72.90%), a sensitivity of 81.65% (95% CI: 81.25%-82.06%), and a 2 \nspecificity of 68.81% (95% CI: 68.53%-69.09%). Upon the inclusion of past medical histories, the 3 \naccuracy decreased to 61.110% (95% CI: 60.84%-61.29%). However, sensitivity increased to 4 \n91.67% (95% CI: 91.37%-91.96%), while specificity decreased to 47.95% (95% CI: 5 \n47.65%-48.25%). Additional details can be found in Figure 3. In the case of \"Lingyi Zhihui,\" 6 \nwithout medical history, the diagnostic accuracy was 74.17% (95% CI: 73.94%-74.39%), 7 \nsensitivity stood at 89.06% (95% CI: 88.73%-89.40%), and specificity at 67.78% (95% CI: 8 \n67.49%-68.07%) as depicted in Figure 3. Subsequently, with the incorporation of a more 9 \ncomprehensive dataset, \"Lingyi Zhihui\" achieved an accuracy of 76.40% (95% CI: 10 \n76.17%-76.63%), sensitivity of 90.99% (95% CI: 90.68%-91.31%), and specificity of 70.15% (95% 11 \nCI: 69.85%-70.44%). 12 \nRegarding the \"Tongyi Qianwen\" model, the initial treatment suggestions in the absence of 13 \nmedical history were deemed inappropriate and potentially harmful in 5.80% of instances (95% CI: 14 \n5.68%-5.93%). Approximately 51.33% (95% CI: 51.11%-51.55%) were considered reasonable but 15 \nincomplete, while 42.87% (95% CI: 42.64%-43.09%) were assessed as both comprehensive and 16 \nsuitable. In subsequent recommendations when medical history was provided, the figures shifted 17 \nto 3.32% (95% CI: 3.22%-3.41%) being inappropriate, 12.88% (95% CI: 12.71%-13.04%) being 18 \nreasonable but partial, and 83.81% (95% CI: 83.64%-83.98%) being thorough and appropriate 19 \n(Fig. 3). For the \"Lingyi Zhihui\" model, the recommendations based on prompts wit hout medical 20 \nhistory were categorized as inappropriate in 6.50% (95% CI: 6.37%-6.63%) cases, reasonable but 21 \nlacking in 28.45% (95% CI: 28.21%-28.69%), and both comprehensive and fitting in 65.05% (95% 22 \n28  \n \nCI: 64.80%-65.31%). When medical history was provided, \"Lingyi Zhihui\" recommendations 1 \nshifted to 3.40% (95% CI: 3.30%-3.50%) being inappropriate, 43.44% (95% CI: 43.19%-43.68%) 2 \nbeing reasonable but not exhaustive, and 53.16% (95% CI: 52.91%-53.41%) being comprehensive 3 \nand relevant. (Figure 3) 4 \nThe removal of past medical history significantly impacted \"Tongyi Qianwen,\" notably increasing 5 \nspecificity by 20.86% and accuracy by 11.55%, while sensitivity declined by 10.02%. Conversely, 6 \n\"Lingyi Zhihui\" exhibited relatively minor changes, with specificity decreasing by 2.37%, 7 \naccuracy by 2.23%, and sensitivity by 1.93% (Supplementary Figure S7). Following the omission 8 \nof medical history, \"Tongyi Qianwen\" reduced the rate of inappropriate treatment 9 \nrecommendations from 5.80% to 3.32%, while comprehensive and appropriate recommendations 10 \nsurged from 42.87% to 83.81%. In contrast, \"Lingyi Zhihui\" saw a decline in inappropriate 11 \nrecommendations from 6.50% to 3.40%, but comprehensive and appropriate recommendations 12 \ndecreased from 65.05% to 53.16%. This indicates that both models decreased the frequency of 13 \ninappropriate recommendations when excluding medical history, with \"Tongyi Qianwen\" notably 14 \nenhancing comprehensive and appropriate suggestions while \"Lingyi Zhihui\" experienced a 15 \ndecline. 16 \nIn assessing the impact of removing past medical history on MediGuide-14B's model metrics, a 17 \nseries of changes were observed. The accuracy of the model experienced a decrease of 2.90%. 18 \nAdditionally, there was a 4.26% reduction in sensitivity, indicating a diminished capacity of the 19 \nmodel to correctly identify positive cases. Finally, the model's specificity also decreased by 1.58%, 20 \nreflecting a slight reduction in its ability to accurately identify negative cases. (S upplementary 21 \nFigure S7) 22 \n29  \n \nDiscussion  1 \nAs large language models (LLMs) continue to advance and find widespread application, they have 2 \ndemonstrated transformative potential in medical tasks. However, evaluating the capabilities of 3 \nlarge language models is a complex and challenging scientific issue. Currently, the assessment of 4 \nthese models' medical knowledge and logical reasoning abilities primarily relies on standardized 5 \ntests. Yet, real-world medical tasks often exceed the scope of structured tasks, presenting a high 6 \nlevel of complexity and uncertainty. There is a global lack of systematic evaluation of large 7 \nlanguage models' effectiveness in actual medical settings. Moreover, the pre-training data for the 8 \nworld’s major language models is predominantly in English. For instance, the recently released 9 \nLlama3 has about 5% of its corpus in non-English languages; ChatGPT-3.5 has approximately 10 \n0.09905% of its pre-training data in Chinese. Even models intended primarily for Chinese 11 \ncontexts, such as Tongyi Qianwen, Wenxin Yiyen, and Baichuan, have only 15-30% of their 12 \ndatasets in Chinese. Considering the structural, grammatical, and usage differences between 13 \nnon-segmented and segmented texts, the composition of different language families in LLMs' 14 \npre-training datasets might affect their performance in various linguistic environments, which is a 15 \nscientific question worthy of in-depth discussion. 16 \nThis study aims to fill these gaps, focusing on the specific applications of large language models 17 \nin emergency triage or consultation scenarios. This study compares the diagnostic performance of 18 \nAI-driven models and human expertise in triaging emergency chest pain cases. While previous 19 \nresearch has primarily focused on English-based ChatGPT models, this study is pioneering in 20 \nevaluating two LLMs designed for \"non-segmented text\" environments. 21 \nTraditional machine learning systems (MLS) use specific structured data from the Electronic 22 \n30  \n \nEmergency Triage System (EETS) to enhance the identification of critically ill patients. These 1 \nMLS employ a predictive model primarily using the CatBoost Python package and provide 2 \nreal-time explanations via the SHAP method to help medical staff understand why certain patients 3 \nmay require immediate treatment. However, these systems have limitations, such as potential 4 \noverfitting issues, a lack of effective capture of complex nonlinear relationships, and challenges in 5 \nprocessing unstructured data. While models built on traditional machine learning or deep learning 6 \nperform well on specific datasets, they generally lack generalizability and often show reduced 7 \npredictive power when patient populations and samples are changed. This is why currently, there 8 \nare no truly integrated predictive models in medical systems worldwide, and a significant gap 9 \nexists between academic research on predictive models and their clinical applications 25. Models 10 \nlike GPT-4 and similar large language models, with their strong capabilities in understanding and 11 \ngenerating natural language, logical reasoning, and knowledge storage, show significant 12 \nadvantages in handling various data types, including unstructured, multimodal, and dynamic data.  13 \nIt's worth noting that \"Tongyi Qianwen (LLM)\" and \"Lingyi Zhihui (LLM)\" exhibited high 14 \nsensitivity but lower specificity, particularly when compared to human experts. This raises 15 \nconcerns about potential overdiagnosis by the AI models, which could result in unnecessary tests 16 \nand treatments. However, high sensitivity is crucial for screening tools, as they aim to capture 17 \nmost of the true cases, even if it leads to some false positives. The high sensitivity of the AI 18 \nmodels suggests their suitability as initial diagnostic tools to ensure potential positive cases are not 19 \nmissed. While both LLMs demonstrated the ability to provide relevant medical advice, there were 20 \nnotable differences in the depth and validity of their recommendations. This underscores the need 21 \nto optimize LLMs for medical scenarios before deployment in healthcare settings. It is essential to 22 \n31  \n \nutilize more realistic medical data during training and fine-tune the models to align with the 1 \nnuances of medical treatment itself 66,67. 2 \nThis study found that LLMs struggled to significantly improve diagnostic accuracy and treatment 3 \nrecommendations when incorporating patients' medical histories. This could be attributed to two 4 \nkey factors: Current LLMs rely on computational power and probabilistic calculations rather than 5 \na deep understanding of disease mechanisms. Second, there's a need for more advanced algorithms 6 \nthat can better extract relevant clinical information while filtering out noise from historical data. 7 \nThe removal of past medical history significantly impacted \"Tongyi Qianwen,\" leading to a 8 \nsubstantial increase in specificity and accuracy while decreasing sensitivity. Meanwhile, \"Lingyi 9 \nZhihui\" exhibited minor changes in diagnostic metrics. Additionally, both models altered the 10 \nfrequency of inappropriate treatment recommendations when medical history was omitted, with 11 \n\"Tongyi Qianwen\" improving its comprehensive and appropriate suggestions, while \"Lingyi 12 \nZhihui\" declined. When utilizing LLMs for diagnostic support, healthcare practitioners should 13 \nacknowledge variations in how these models handle intricate and diverse data 68. Factors such as 14 \nmodel comprehensiveness, accuracy, and potential sources of interference should be considered. 15 \nClinical judgment, rooted in experience, should guide decision-making. Continuous monitoring 16 \nand performance optimization are crucial. LLMs offer promise as diagnostic aids, but healthcare 17 \nprofessionals must weigh multiple factors to ensure the delivery of precise and thorough 18 \ndiagnostic recommendations to patients. 19 \nOur study shifted focus to MediGuide-14B, our proprietary open-source model. This model has 20 \nbeen specifically fine-tuned for medical applications, providing us with an opportunity to 21 \nscrutinize its real-world efficacy. In our preceding analyses, we observed notable variations in 22 \n32  \n \nsensitivity and specificity across different LLMs, underscoring the need for a detailed examination 1 \nof each model's strengths and weaknesses. MediGuide-14B stands out as a large language model 2 \ndedicated to the medical sector, boasting an advanced integration of domain-specific datasets and 3 \nbespoke training approaches to optimize its diagnostic capabilities.   4 \nConducting an exhaustive evaluation of MediGuide-14B's performance is pivotal not only for 5 \ngauging the broader applicability of LLMs in healthcare but also for charting the course for their 6 \nfuture development and potential areas of application. By juxtaposing MediGuide-14B against 7 \nother leading models in the field, we aim to deliver a nuanced appraisal of the model's accuracy, 8 \nefficiency, and reliability in medical diagnostics. This comparative analysis is intended to furnish 9 \ndiverse insights and formative experiences, contributing significantly to the ongoing discourse on 10 \nthe role and impact of large language models in healthcare research.  11 \nOur study illustrates the marked variability in the performance of different large language models 12 \nwhen processing real-world medical scenario information. The objective of our research is not 13 \nsolely to compare and rank these models but to emphasize the adaptability and potential of LLMs 14 \nin the medical field. This realization underscores the necessity of careful and precise 15 \nbenchmarking tailored for medical AI applications. We have also discovered that specific 16 \nfine-tuning and alignment of LLMs significantly enhance their ability to perform specialized tasks 17 \nwithin the medical domain, even on smaller-scale models. This finding is particularly significant 18 \nfor vertical sectors like healthcare, as it suggests the feasibility of training and deploying efficient 19 \nmedical LLMs at a lower cost. Such advancements allow us to apply cutting-edge AI technology 20 \nmore effectively in clinical settings, thereby improving the quality and efficiency of healthcare 21 \nservices. 22 \n33  \n \nLLMs have the potential to reshape certain aspects of healthcare, particularly in the context of 1 \nrapid pre-hospital Chest Pain Triage. The integration of these models has the potential to 2 \nstreamline triage procedures, facilitating timely interventions even before a patient arrives at the 3 \nhospital. This not only enhances the effectiveness and scope of diagnostic and therapeutic 4 \ninterventions but also promises to improve the efficiency of medical infrastructure, reduce patient 5 \nwaiting times, alleviate the burden on emergency medical personnel, and ultimately alleviate the 6 \nfinancial strain on patients and healthcare systems 69-71. In conclusion, the diagnostic capabilities 7 \ndemonstrated by LLMs, as evidenced in this study, underscore their significance in advancing the 8 \nfield of rapid triage. It is reasonable to anticipate that soon, these models will play a pivotal role in 9 \nenhancing healthcare delivery, ultimately benefiting both patients and healthcare systems. 10 \nIn an era increasingly dominated by AI, medical practitioners, particularly the younger generation, 11 \nwill inevitably encounter an expanding array of medical AI entities. How they utilize AI, discern 12 \nwhich AI tools best assist them, and identify the specific functions where AI can provide support, 13 \nnecessitates robust benchmarking efforts. Such benchmarks offer crucial guidance to healthcare 14 \nprofessionals in navigating the AI landscape. Our study aims to initiate this journey in the field, 15 \nlaying a foundational step that we believe will serve as a vital reference for future researchers. 16 \nThis work is poised to propel the further advancement and application of LLMs in healthcare, 17 \nultimately aiding medical professionals in harnessing AI's full potential for improved patient care 18 \nand healthcare delivery. 19 \nLimitations  20 \nThe reliance on retrospective data may introduce inherent biases, potentially impacting the 21 \ngeneralizability of the results. Additionally, LLMs' diagnostic performance in ACS triaging 22 \n34  \n \nscenarios might not reflect their capabilities in other medical conditions. The specificity 1 \nchallenges highlighted in the study emphasize the need for broader and more diverse training 2 \ndatasets. Furthermore, the comparative analysis between AI models and human experts, though 3 \nilluminating, is based on a restricted set of parameters, potentially overlooking nuanced aspects of 4 \nclinical decision-making. The study underscores the necessity of comprehensive, prospective 5 \nresearch to validate the findings and address these limitations. 6 \nDeclarations: 7 \nThis manuscript was edited by LLM “Tongyi Qianwen” for its English language, but human 8 \nauthors read and made the final version.   9 \n35  \n \nReferences: 1 \n1. Ayers JW , Zhu Z , Poliak  A, e t a l. Eva l ua tin g A rtific i al Inte lligence Re sponse s to Pu blic 2 \nHealth  Ques t ions. J AM A  Net w O p en  2023; 6 (6): e23 17517 . 3 \n2. Kanjee Z, C rowe B , Ro dma n A .  A c curac y  of a  Generative  Artifi c ial Inte ll igenc e  Mo del in  a 4 \nComplex D iagno s tic  Challenge . Ja ma  20 23; 33 0 (1 ): 78 -80. 5 \n3. Minssen T , Vayena E , C o hen  IG. The  Ch allenges for Regu la t ing  Me d ica l U s e  o f ChatGPT  6 \nand Othe r La rg e  Lang u age Models . Ja m a  2023 . 7 \n4. Will  C hatGPT tran sf o rm hea lthca re? N at M e d  2023; 29 ( 3):  505 -6. 8 \n5. Gilbe rt S, Ha rvey H , Melv i n  T, Vo llebreg t E, Wick s  P. Larg e languag e model A I ch atbots 9 \nrequire approval as  med ic a l de v ices . Na t Med  2023 . 10 \n6. Thiru navukarasu  AJ , T i ng  DS J , Elangova n K, Gutierrez  L, Tan  TF , T ing  D SW . L arge 11 \nlan guage  mode ls  in  me dicine. N a t M e d  2023. 12 \n7. Singhal K , A zizi S, Tu T, et al. La rge lang uage models  enc ode  clin i ca l k now ledge. Na t u re  13 \n2023 . 14 \n8. How ard A , Hope W , Gerada A . Cha tGPT and antim ic robia l adv ic e : the  end of the 15 \nconsulti ng in f ec tion do ctor? Lancet Inf ec t  D i s  2 023; 23 (4): 4 05-6. 16 \n9. Arora A , Aro ra A . The p ro mise of la rge l a nguage m odels in hea lth care . Th e Lanc e t  2023; 17 \n401 (10377 ): 641. 18 \n10. Ji ang  LY, L i u XC , Ne ja t ia n NP , e t a l. Hea lth s ystem -sca le la nguage  m odels a re 19 \nall-pu rpose pre diction engines . Nat u re  20 23; 61 9 (796 9): 357 -62 . 20 \n11. Thapa S , A dhika ri S . Cha tGPT, Bard , an d L arg e Lan g uage  Models for B iome di c al 21 \nResearc h : Opportuniti es  and P itfa lls . Ann  Bio m ed  Eng  2023. 22 \n36  \n \n12. Ouyang  L, Wu  J, Jiang  X, et a l. Trainin g  language mode ls to  follow  in struc tion s w ith 1 \nhuma n  fe edb ack . arX i v pre -pr int se rve r  2022. 2 \n13. Wayne, Zh ou K , L i J, e t al. A  Surv ey of L arge Language Mode l s. ar X i v pr e -pr in t s e r v e r  3 \n2023 . 4 \n14. Sharm a P , Pa ras a  S. C hatGPT  and  large  language  mod els in g astroen ter olog y. N at Rev 5 \nGa s troe nter o l  H e pa to l  202 3. 6 \n15. Li R , Ku mar A , Ch en JH. H ow  Cha tbots a nd Large La nguage  M odel Artificia l Inte lligence 7 \nSystems  W ill Reshape Modern Med icine: Fou n tain  of C reativ ity o r P andora ' s  Box? JAMA  8 \nInte rn Med  2023 ; 183 (6 ) : 596 -7 . 9 \n16. Wei J, Wang  X, Schuurmans  D, et al. Ch ain -o f-thou ght promp ting e licits  reason ing in  10 \nlarge langua ge  mod e ls . Adv a nces in  Neu ral I nfo rm at i on  Proce s s ing  S y ste ms  2022 ;  35 : 11 \n2482 4-37 . 12 \n17. Miller K, Gunn  E, C och ran A , et al. Use o f La rge  Language  Mode ls and A rtificia l 13 \nInte llig ence  Tools in W orks  Sub mitted to Journa l  o f Clin i ca l Onco l ogy . J ou rnal o f c lini ca l  14 \nonc olog y  : o f ficia l jou rna l o f  the  A mer i can  Society  of C l i nical  On co log y  2023; 41 (19 ): 34 80 -1. 15 \n18. Ayers JW , Poliak A, D redze M , et al. Co mpa r i ng  Physic i an  and  Artificial In te l ligence 16 \nChatbot Re sponse s to  Pa tient Questions  Po sted to  a  Pub l ic  Social Med ia  Fo rum. J AMA  In ter n 17 \nMed  2023 ; 18 3 (6): 589-9 6. 18 \n19. Wei J, Ta y Y , Bo m masan i R , et a l. E m ergent A bilities  of La rge L anguage Mode l s. arXiv  19 \npre-p ri n t  se rver  2022 . 20 \n20. Azizi Z , A lipou r P , Go mez  S, e t a l . Eva lua ting  Recom mendations A bout Atrial Fibrillatio n 21 \nfor Patien ts  and C linicia ns Ob ta i ned Fro m Cha t-B ased  Artifici a l In t e lligen ce A lgo rithm s. 22 \n37  \n \nCircu latio n:  Ar rhy t hm ia  an d  Ele ct rop hys i o logy  2023 : e0 12015. 1 \n21. Yang X, Chen  A, PourNe ja tian N , e t al. A  l a rge  language  mo del for ele c tron ic health 2 \nrecords. N PJ  Di gi t M e d  2022 ; 5 (1): 194. 3 \n22. Sheik h  K, Ghaffa r A . P R IMASYS :  a  hea lth po li c y an d s y ste ms resea rc h approach for th e 4 \nass essmen t o f coun try primary  he alth care s ys tems . H e al th  Re s ea r c h P ol ic y  an d  Sy s t em s  5 \n2021 ; 19 (1 ): 31 . 6 \n23. Mehmood  A, Rowthe r AA , Ko bus ingye  O, Hy der AA . A s s es s ment o f p re -hos p ita l 7 \nemerg ency medic al serv ice s  in low -i nco me  setti ngs  using a hea lth  s ys tem s a pp ro ach . 8 \nInte rnatio nal Jou rnal o f E me rgen c y M ed i c ine  2018 ; 11 (1 ): 53. 9 \n24. Gizaw Z , A sta le T, Kass i e  G M. Wha t improves access to primary hea lthc a re s ervic es  in 10 \nrura l c o mmunitie s ? A  sy s te ma tic review . BM C Pr im a ry  Ca re  2022 ; 23 (1): 313. 11 \n25. Mark ow et z  F . A ll mode ls are  wron g and  y ours  are use l ess : ma king c linical p rediction 12 \nmodels  impac tful fo r patien ts. np j  Precis i o n Onc o logy  2024 ; 8 (1 ): 54. 13 \n26. Yeo YH, S am aan J S , N g WH, e t a l . GPT-4 outperforms  Cha tGP T  in an s w ering  14 \nnon-Eng l ish qu esti ons  rel a ted  to  cirrhos i s . 2023. 15 \n27. Fang C, L i ng  J, Zhou  J , et al. How  does  C hatGPT 4 p reform on N on -E ng lish N atio nal 16 \nMedica l  L icensing E x a m ination?  An  Ev a luation in  Chine se L anguage. 2 023. 17 \n28. Bijani M , Abedi S , Ka rim i S , T ehran ine s h at B . Ma j or cha llenges and  ba rriers in cli n ica l  18 \ndecis ion -m aking a s pe rce ived  by e merge ncy med ical service s pe rsonnel: a  qua litativ e co ntent 19 \nanal ys i s . B MC  Eme rgency Medic ine  202 1; 21 (1 ): 11. 20 \n29. Beck er TK , Gaus ch e-Hill M , Aswe gan AL , et a l. E th ic a l c h allenge s i n E me rgency Medica l  21 \nServic es : c o ntrovers ies  a nd recom menda tion s . Pr e h osp D is a ste r M ed  2013 ;  28 (5 ): 488-9 7. 22 \n38  \n \n30. Pines J M, Mulli ns P M , Coop er J K, Feng  L B, Ro th KE . Na tional tre nds  in  e merge n cy  1 \ndepartmen t use , ca re patte rns, and  quality of ca re  of o lder  adu lts in  the U n i ted  Sta tes. Jour na l  2 \nof the Ame r ican Ge r ia t ri cs S oc i ety  2013 ; 61 (1): 12-7 . 3 \n31. Vainieri M , Panero C , Coletta L . Wa iting  time s in em ergency departments : a resou rce 4 \nallocati on  or an  effic i ency iss ue?  B MC H e alt h Se rv Re s  20 20; 20 (1):  549. 5 \n32. Neprash  HT, Everhart A , McA lp i ne  D, S mith  LB , She ridan B , C ross  DA. Meas u rin g 6 \nPrimary  C a re  E x a m Length Us ing E lectro nic  Health R eco rd  Data . Me d  C a r e  2021 ; 59 (1): 62-6 . 7 \n33. Zhiti ng  G, J ingfen J , Shuihong C , M i nfei Y, Yuw e i W , Sa  W . Re li ab ili ty an d va li d ity  of th e 8 \nfour-leve l Ch inese  e merg ency triage sca le in  ma inland C hina : A  m ulticenter asses smen t. 9 \nInte rnatio nal Jou rnal o f Nu rs in g  Stud ies  2 020; 10 1 : 1034 47 . 10 \n34. Tanab e P , G imbel R, Yarnold  P R, Ky ria cou DN , A dams J G . Reliabilit y and v alidity  of 11 \nsc ores  on The  Eme rgency Se ve r i ty Ind ex  ve r s io n 3. A cade m ic  e m er gency medicin e  2004 ; 12 \n11 ( 1):  59 - 65. 13 \n35. Taboule t P , Ma illa rd -Acke r C , Ranchon G, e t a l. T riage des pati en ts à l’accue il d ’une 14 \nstru c tu re d’urgenc es . Pré sentation  de l’échelle de  tri élabo rée  par la So ci é té  franç a ise de 15 \nmédec ine d’urgence: la FR en c h E me rge ncy Nu rses C l a ss i ficatio n in Ho s pita l  (FR ENC H ). 16 \nAnnale s  f ranç a ises de  mé dec i ne  d ’urge n ce  2019 ; 9 (1 ): 51-9. 17 \n36. Kachalia A , Ga ndhi T K , Puopolo  AL , et a l. M i ssed and  delayed diagnoses  in the  18 \nemergency depa rtmen t: a  study  of closed  malp ractice c la ims  fro m 4  liabili ty  ins u rers. An n a ls  o f 19 \nemer g ency medi c ine  2007 ; 49 (2 ): 196 -20 5. 20 \n37. Hussain  F , Coope r A , Carso n-S tev ens  A, e t a l. Diag nostic e rro r in  the e merg ency  21 \ndepartmen t: l ea rnin g from  nati ona l patien t s a fe ty inc i den t repo rt ana lysis . B M C  Em erg Med  22 \n39  \n \n2019 ; 19 (1 ): 77 . 1 \n38. Gulati M, L evy  PD , Mu kherj ee D , et al.  20 21 AHA / A CC/A SE/CH EST /SA EM /SCC T/SCMR  2 \nGuideli ne fo r th e  Evalu ation a n d Diagn o sis of Ches t Pa i n: Exec u tive  S umma ry :  A  R ep ort of the 3 \nAmeric an  Co llege o f Ca rd iology/A me rica n Hear t A ssoc iation  Join t Co m mittee on  C li n ical 4 \nPractic e  Gu ideli nes . J o ur n a l of  t h e A m eri c a n  C ol l eg e  of  C ar di o lo g y  2021; 78 (2 2): 2218-61. 5 \n39. Amsterda m  EA, Wen ger NK , B ri nd is R G, et a l. 201 4 AHA/ACC  gu ide l ine  for the 6 \nmana ge men t  o f pati en ts  with non–ST-ele v ation a cute  c o rona ry syndro m es: a repo rt of th e 7 \nAmeric an  Co llege o f Ca rd iology/A me rica n H eart A ss oc iation  Task Fo rce on P rac tic e 8 \nGuideli ne s. J ou rnal  o f the  Ame r i can  Col l e ge of  Ca rdiology  2014 ; 64 (24): e13 9-e228. 9 \n40. Lawton J S , Ta mis-Ho ll a nd JE , Bang a lore  S, et al. 2021 A CC/A HA/SC A I g uide li ne  for 10 \ncorona ry artery revascula rization: e xec u tiv e su mmary: a repo rt of the A me rican Co l le ge o f 11 \nCardio logy /A meric an  Hea rt Assoc ia tion J oint Co mm ittee  on  Clinica l  P rac tic e Gu i d elines. 12 \nCircu latio n  202 2; 145 (3 ) : e4 -e17 . 13 \n41. Li J , Li X , Wang Q , et a l. ST-s egme nt elevation myocard ia l  in farc tion  i n  C hina fro m 2001  14 \nto 2011 (th e Ch ina P EACE -Retrospe ctiv e  Acute Myocard ia l Infa rc tion S tudy ): a retrospec tive 15 \nanal ys i s  o f hospital da ta . T he Lan c et  201 5; 385 (9966): 441 -51 . 16 \n42. Komorowski M, De l P ilar Ar i as  Ló pez  M, C ha ng AC . How could C h a tGPT impac t my  17 \npractic e as  an  i n tensiv ist? A n overv iew of pote nti a l applicatio ns, risk s  an d lim i tation s . In tensive 18 \nCare Med  2023 ; 49 (7): 844-7. 19 \n43. Madden  MG, McN ic ho las B A, Laffe y J G . Ass essing the  use fu l ness of a  large  language 20 \nmodel to  qu ery and  su m ma rize  un s tructured med i ca l n ote s i n in ten s iv e  ca re. Inten si ve  Care  21 \nMed  2023 . 22 \n40  \n \n44. Patel SB , La m K . C hatGPT: the  fu tu re  of di scharg e  su mma ries?  Lanc e t  Dig i t  Healt h  202 3; 1 \n5 (3): e 107-e8 . 2 \n45. Ali SR, D obbs  TD, Hutchings HA , Wh ita ker IS. U sing Cha tGP T t o  w ri te  pa tient c li n ic 3 \nlett e rs. Lancet  Dig i t Hea l t h  202 3; 5 (4 ): e1 79- e81 . 4 \n46. van  Hee rden AC ,  P o zue lo JR, Kohrt BA . Glo bal Me nta l H e alth Se rv ic es  and  the Impact of 5 \nArtificia l Inte lligence-Powered  Large Lan guage Mode ls . JA MA  psy ch ia t r y  202 3; 80 (7) : 6 62-4 . 6 \n47. Kw ok KO , We i W I , Tso i MTF , et al. How can w e  trans form  trave l med icine  by  lev eragin g 7 \non AI-powe red s earc h e ng ine s ? J o ur na l  o f  T r a ve l  M ed i c in e  2023; 30 (4 ).  8 \n48. Liu F, Panag iot a kos D . Real-world  data: a br i e f rev i ew  of the  metho ds, app lic ations, 9 \nchallenges  and oppo rtu nities . BM C M e d R e s  M e t h od o l  2022 ; 22 (1):  287. 10 \n49. Li S . E xplo ring the  c l in ica l capab il ities an d limita t ion s of C hatGPT : a  cau tionary tale fo r 11 \nmedica l  app li ca tions. Int  J Surg  2023. 12 \n50. Wang G, Yan g G, Du Z, Fan L , Li X. C linicalGPT: La rge Languag e  M odels F in e tun ed w ith 13 \nDivers e Med i ca l Da ta  and  Comp rehensiv e E v a luation . arX iv  pre -pr in t s e rve r  202 3. 14 \n51. Shea YF, Lee C MY , Ip WC T , Luk D W A, Wong  S S W . Use  of GPT-4 to Ana l y ze M edic a l 15 \nRecords of Pa tients  W ith Extens iv e  Inv es tigations and  Dela y ed D iagnosis . JAMA  N e tw  O pen  16 \n2023 ; 6 (8 ): e2325000. 17 \n52. Zhao W X, Zhou K, Li J ,  e t al. A s u rvey o f large  language  models . ar X iv  pr e p ri nt  18 \narXiv :2 303 1822 3  2023 . 19 \n53. Sun Y, Wa ng S , F eng S , et al. E rn i e 3.0 : Large-scale know l e dge e nhanced pre-tra ining  20 \nfor la nguage  unde rsta nding a n d gen e ration. ar X iv  p rep rint  arX i v :210 70213 7  202 1. 21 \n54. Wang S , Sun Y , X iang  Y, e t a l. E rni e  3 .0 titan : Exp l o rin g l arger-scale  know l edge  22 \n41  \n \nenhanced  pre-trainin g for language  unde rstanding  and genera ti on. ar X iv  pr e p ri nt  1 \narXiv :2 112 1273 1  2021 . 2 \n55. Singhal K , A zizi S, Tu T, et al. La rge lang uage models  enc ode  clin i ca l k now ledge. Na t u re  3 \n2023 ; 620 (7 972): 1 72-80 . 4 \n56. Rudolph J, Tan  S, Tan S . W ar of the chat bots: Ba rd, B ing Cha t, Cha tGPT, Ernie  a nd 5 \nbey ond. The  new  A I gold  rus h  and its imp act on h igher educa ti o n. Jo u rnal  o f  A ppl ie d  Lear ning 6 \nand Te ac hin g  2023 ; 6 (1 ).  7 \n57. Chien AA , Lin L , Nguy en H, Rao  V, Sha rma T , W ijay aw ardana R . R e duc ing the  C arbon 8 \nImp act o f Ge nera tive  AI Inference  (toda y and in  2035 ).  Proceedin gs of the 2nd  W orks hop  o n 9 \nSustainab le C o mpu ter Syste ms ; 202 3; 2 023. p. 1-7 . 10 \n58. Peng C, Yang  X, Chen  A, et a l. A study o f g en era tiv e la rge langu age model fo r me dica l  11 \nresearch and h ealthca re . np j D i g it al M edi c ine  2023 ; 6 (1): 210. 12 \n59. Li X , F an Y , C heng S . A IGC In Ch ina : Cu rren t De v elo pmen ts  And  Future  O utl ook . ar X i v  13 \nprepr int a rXi v:230808451  2023 . 14 \n60. Colle t JP, Th iele H, B a rbato E , e t al. 202 0 ESC Gu id elines for the managemen t of a cu te 15 \ncor on ar y sy ndr ome s i n pati ent s pr es enti ng  wit hout pers i st e nt ST- segme nt el ev at i on .  Eu r He a rt 16 \nJ  2021 ; 42 (14): 1289 -367. 17 \n6 1 . As s o c ia ti o n EMBot CM, As s o ci at i on  CPBo t CHI EP. Ex p ert  c on s e n s u s  o n em e r ge nc y  18 \ndiagnos i s  and  trea tme nt of acu te chest p ain. C h i nese  Jou rna l  o f E me rgenc y  Me d i ci ne  2019; 19 \n28 (4): 413-20 . 20 \n62. Yang A, X iao B , W ang  B, e t a l. B aichuan  2: Open  large-sca l e  language  models. ar X i v 21 \nprepr int a rXi v:230910305  2023 . 22 \n42  \n \n63. Mbak w e A B, Lo u rentzo u I, Ce li LA , Mech an ic  OJ, Dagan  A. ChatGP T  p as si ng  US MLE 1 \nshi nes  a spo tligh t o n the flaw s  of m edical educ a tion. Pub lic L i brary  o f Sc i ence  S an Fran cisco, 2 \nCA  USA; 2 023. p. e00 002 05 . 3 \n64. Yu H. U niversal hea l th  ins u rance  coverag e fo r 1.3  b illi on p eople : Wha t accoun ts  fo r 4 \nChina' s  succes s ? H e al th  p oli c y  2015 ; 119 (9): 1145-52 . 5 \n65. He W . D oe s the im mediate  re imbu rse me nt o f med ical in s uran ce reduc e the 6 \nsoc ioe c ono mic inequa li ty  in health  am on g the  floa ting popu lation?  Ev idence  from C hina. 7 \nInte rnatio nal Jou rnal for  Equ it y  in He alth  2023; 22 (1 ): 1 -14. 8 \n66. Su P , Vija y -S h anke r K. I nve s tiga tio n  of im prov i ng  the  pre -tra inin g  and  fine -tuning  of BERT 9 \nmodel for b i o medical rel a tion e xtrac tion. BMC B io infor mat ic s  2022; 23 (1 ): 120. 10 \n67. Wu C , Zh ang X , Z hang Y , Wang  Y , X ie W. PMC -LLa MA : Furthe r Fine tun ing  LLa MA on  11 \nMedica l  P ape rs . ar Xi v  pr e-p ri n t s er v er  202 3 . 12 \n68. Ferry man  K, Mackin tos h  M , Gh ass e mi M. C ons ider i ng  Bia s ed D ata  as  In forma tive 13 \nArtifa cts  in A I-As sisted Health  Car e. N En gl J Med  2023 ; 389 (9 ): 833 -8. 14 \n69. The，Lance t. A I in  med icine : crea ting a s afe and eq uitab le fu ture . Lan ce t  20 23; 15 \n402 (10401 ): 503. 16 \n70. Han T, L i sa , Pa pa io annou J-M , et al. Me d Alpac a  -- A n Ope n -S o urce Co lle c tio n  o f Medica l  17 \nConvers a tional A I Mode l s  and Train ing D ata. ar X i v pr e -pr i n t s e rv e r  2023 . 18 \n71. Yunxi ang  L , Z i han  L, Ka i  Z , R uilong D , Y ou Z . C hatD oc tor: A  M edical Cha t Mode l 19 \nFine-tu ned  on  L La MA Model using Medic al  D o ma in K nowledge. arX iv  p r e -prin t  se r v er  2023. 20 \n21 \n43  \n \nAuthor Contributions: \nThe study was primarily designed by Yi-Da Tang, Xiangbin Meng, Xiangyu Yan, and Jun Gao. \nIndividual Contributions: \n• Yi-Da Tang: Led the study design, contributed cardiovascular expertise, involved in \ndata collection and analysis, reviewed and approved the final manuscript. \n• Xiangbin Meng: Contributed cardiovascular expertise, involved in data collection and \nanalysis, reviewed and approved the final manuscript. \n• Xiangyu Yan: Provided domain-specific knowledge, involved in data collection, \nanalysis, and interpretation, reviewed and approved the final manuscript. \n• Jun Gao: Contributed cardiovascular expertise, involved in data collection and analysis, \nreviewed and approved the final manuscript. \n• Jingjia Wang, Xuliang Wang, Yuan-geng-shuo Wang, Wenyao Wang, Chunli Shao: \nContributed cardiovascular expertise, were involved in data collection and analysis, \nreviewed and approved the final manuscript. \n• Junhong Wang, Yaodong Yang, Muhan Zhang, Xiaojuan Cui, Jing Chen, Kuo Zhang, \nDa Liu, Jia-ming Ji, Zifeng Qiu, Muzi Li: Provided domain-specific knowledge in data \ncollection, analysis, and interpretation, reviewed and approved the final manuscript. \nData Verification: Yi-Da Tang、 Xiangbin Meng and Jun Gao directly accessed and verified the \nunderlying data reported in the manuscript, ensuring its integrity. \nFull Access and Responsibility: All authors confirm that they had full access to all the data in \nthe study and accept the responsibility for the decision to submit for publication. \nCollaboration and Coauthorship: The authors acknowledge and appreciate the collaboration \n44  \n \nand coauthorship of colleagues in the locations where the research was conducted, reflecting the \nbenefits of diversity in authorship in terms of background, career-stage, gender, geography, and \nrace. \nAcknowledgments and Personal Communications: All cited individuals in acknowledgments \nor personal communications have provided their written consent. \nAuthor Statement Form: All authors have signed the author statement form, which will be \nuploaded with the submission. \n \nAcknowledgments: \nThis study was funded by the National Key R&D Program of China (2020YFC2004705), National \nNatural Science Foundation of China (81825003, 91957123, 82270376), CAMS Innovation Fund \nfor Medical Sciences (2022-I2M-C&T-B-119, 2021-I2M-5-003), Beijing Nova Program \n(Z201100006820002) from Beijing Municipal Science & Technology Commission, and CSC \nSpecial Fund for Clinical Research (CSCF2021A04). \n \n\n\nTable 1: Comparison of accuracy, sensitivity, and specificity in diagnosing emergency ACS between the two large language model diagnostic groups and the \nhuman expert diagnostic group. \nModel/Groupe Accuracy (95%CI) Sensitivity (95%CI) Specificity (95%CI) \nTongyiQianwen（ v1.0.3）  61.11% (95% CI:60.84%-61.29%) 91.67% (95%CI:91.37%-91.96%) 47.95% (95%CI:47.65%-48.25%) \nLingyi Zhihui（ v2.2.0）  76.40% (95% CI:76.17%-76.63%) 90.99% (95%CI:90.67%-91.31%) 70.15% (95%CI:69.85%-70.44%) \nHuman Experts 86.37% (95%CI:86.18%-86.55%) 79.62% (95%CI:79.20%-80.04%) 89.26% (95%C:89.06%-89.46%) \nThis table provides a detailed comparison of the key performance metrics when diagnosing emergency ACS among the two large lang uage model diagnostic groups \nand the human expert diagnostic group. \n \nFig. 1: Research process diagram accompanied by a comparative illustration of \ndiagnosis time between LLMs and human experts, along with a comparison of the \ndiagnostic thought processes of LLMs and humans. \n \nThe research process diagram displays the primary steps and methodologies of this study. The \ncomparative illustration showcases the time disparities bet ween LLMs and human experts in \ncompleting diagnostic tasks. The thought process comparison further elucidates the cognitive and \ndecision-making pathways employed by both during diagnosis. \n"
}