{
    "title": "Trear: Transformer-based RGB-D Egocentric Action Recognition",
    "url": "https://openalex.org/W3119588134",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5100460306",
            "name": "Xiangyu Li",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A5023247075",
            "name": "Yonghong Hou",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A5042680345",
            "name": "Pichao Wang",
            "affiliations": [
                "Alibaba Group (United States)",
                "Bellevue Hospital Center"
            ]
        },
        {
            "id": "https://openalex.org/A5066142993",
            "name": "Zhimin Gao",
            "affiliations": [
                "Zhengzhou University"
            ]
        },
        {
            "id": "https://openalex.org/A5081346568",
            "name": "Mingliang Xu",
            "affiliations": [
                "Zhengzhou University"
            ]
        },
        {
            "id": "https://openalex.org/A5100695040",
            "name": "Wanqing Li",
            "affiliations": [
                "University of Wollongong"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2970608575",
        "https://openalex.org/W2156303437",
        "https://openalex.org/W2949170850",
        "https://openalex.org/W2963177663",
        "https://openalex.org/W2963563276",
        "https://openalex.org/W2963097942",
        "https://openalex.org/W2963986337",
        "https://openalex.org/W1947050545",
        "https://openalex.org/W2443602434",
        "https://openalex.org/W2432964524",
        "https://openalex.org/W2963082988",
        "https://openalex.org/W1993229407",
        "https://openalex.org/W2342662179",
        "https://openalex.org/W2964222622",
        "https://openalex.org/W2085735683",
        "https://openalex.org/W2591961134",
        "https://openalex.org/W2963601560",
        "https://openalex.org/W2418822399",
        "https://openalex.org/W2774319743",
        "https://openalex.org/W1993666393",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W1895914852",
        "https://openalex.org/W2805434138",
        "https://openalex.org/W2309561466",
        "https://openalex.org/W2082497897",
        "https://openalex.org/W2149276562",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W2507009361",
        "https://openalex.org/W2716916105",
        "https://openalex.org/W2142194269"
    ],
    "abstract": "In this paper, we propose a \\textbf{Tr}ansformer-based RGB-D \\textbf{e}gocentric \\textbf{a}ction \\textbf{r}ecognition framework, called Trear. It consists of two modules, inter-frame attention encoder and mutual-attentional fusion block. Instead of using optical flow or recurrent units, we adopt self-attention mechanism to model the temporal structure of the data from different modalities. Input frames are cropped randomly to mitigate the effect of the data redundancy. Features from each modality are interacted through the proposed fusion block and combined through a simple yet effective fusion operation to produce a joint RGB-D representation. Empirical experiments on two large egocentric RGB-D datasets, THU-READ and FPHA, and one small dataset, WCVS, have shown that the proposed method outperforms the state-of-the-art results by a large margin.",
    "full_text": "Trear: Transformer-based RGB-D Egocentric Action Recognition\nXiangyu Li, Yonghong Hou, Pichao Wang, Zhimin Gao, Mingliang Xu, and Wanqing Li*‚Ä† ‚Ä° ¬ß ¬∂\nAbstract\nIn this paper, we propose a Transformer-based RGB-D\negocentric action recognition framework, called Trear. It\nconsists of two modules, inter-frame attention encoder and\nmutual-attentional fusion block. Instead of using optical\nÔ¨Çow or recurrent units, we adopt self-attention mechanism\nto model the temporal structure of the data from different\nmodalities. Input frames are cropped randomly to miti-\ngate the effect of the data redundancy. Features from each\nmodality are interacted through the proposed fusion block\nand combined through a simple yet effective fusion oper-\nation to produce a joint RGB-D representation. Empirical\nexperiments on two large egocentric RGB-D datasets, THU-\nREAD and FPHA, and one small dataset, WCVS, have\nshown that the proposed method outperforms the state-of-\nthe-art results by a large margin.\n1. Introduction\nWith the popularity of the wearable equipment (e.g.\nGoPro and VR helmet), recognition of human activities\nfrom egocentric videos has attracted much attention due\nto its wide research and practical applications, such as\nRobotics, VR/AR, etc. Recently, deep learning is widely\napplied to many computer vision tasks with promising re-\nsults which promotes researchers to employ Convolutional\nNeural Networks (CNNs) or Recurrent Neural Networks\n(RNNs) in egocentric/third-person view action recognition\n[1, 2, 3, 4, 5, 6]. While promising, most of these meth-\nods are based on the single RGB modality, do not take\nthe combination of multiple heterogeneous modalities, e.g.\nRGB and depth, into consideration. However, each modal-\nity has its own characteristic. Depth modality carries rich\n*Corresponding authors: Zhimin Gao and Pichao Wang\n‚Ä†X. Li and Y . Hou are with School of Electronic Informa-\ntion Engineering, Tianjing University, Tianjin, China (e-mail: lixi-\nangyu 1008@tju.edu.cn; houroy@tju.edu.cn).\n‚Ä°P. Wang is with DAMO Academy, Alibaba Group (U.S.), Bellevue,\nUSA (email: pichaowang@gmail.com).\n¬ßZ. Gao and M. Xu are with School of Information Engi-\nneering, Zhengzhou University, Zhengzhou, China (e-mail: ie-\ngaozhimin@zzu.edu.cn; iexumingliang@zzu.edu.cn).\n¬∂W. Li is with Advanced Multimedia Research Lab, University of Wol-\nlongong, Wollongong, Australia (email: wanqing@uow.edu.au).\n3D structure information, shows insensitive to the illumi-\nnation changes, and it lacks the vital texture appearance\ninformation, while RGB modality is vice versa. For con-\nventional third person action recognition, RGB-D based\nmethods [9, 10, 11] have been widely proposed to exploit\nthe complementary characteristics of both modalities, while\nfor egocentric action recognition, there are still few stud-\nies [12]. This paper focuses on RGB-D based egocentric\naction recognition and explores a novel framework to learn\na conjoint representation of both modalities.\nCompared to the third person action recognition, egocen-\ntric action is more Ô¨Åne-grained and it requires to classify\nboth the motion performed by the subject and objects being\nmanipulated (e.g. close juice bottle, close milk, pour milk,\npour wine, etc.). Thus, it‚Äôs essential to encode the spatial-\ntemporal relation information of the action clips. Previous\nworks either utilize the optical Ô¨Çow to exploit motion in-\nformation via a two-stream network [13] or adopt a Convo-\nlutional Long Short-Term Memory (ConvLSTM) network\nfor spatio-temporal encoding [1, 2]. However, they either\ncan only model short-term motion or only consider tempo-\nral structure sequentially as the activity progresses. Based\non this observation, the recently proposed Transformer [14]\ninspires us to employ it in RGB-D egocentric action recog-\nnition due to its strong capability of sequence modeling in\nNLP (e.g. language translation) tasks, parallelness in pro-\ncessing the input, and ability in building long-range depen-\ndencies through self-attention mechanism.\nThis paper proposes a novel transformer-based egocen-\ntric action recognition framework. It consists of two mod-\nules, inter-frame attention encoder and mutual-attentional\nfusion block. Data from each modality is Ô¨Årst encoded\nthrough an attention encoder to build an intra-modality tem-\nporal structure, and then features are incorporated through\nthe fusion block to produce a cross-modal representation.\nFor the inter-frame attention encoder, we adopt a standard\ntransformer encoder consisting of self-attention and feed-\nforward layers. By mimicking the language translation task,\neach sampled image (or depth map) in an action video is\ntreated as a word and dependencies with other words is\nconstructed using a self-attention mechanism. Due to the\ncontext redundancy among the sampled images, it‚Äôs inefÔ¨Å-\ncient to conduct attention calculation. Thus, we propose to\ncrop regions randomly from each image, where the differ-\n1\narXiv:2101.03904v1  [cs.CV]  5 Jan 2021\nent regions are interacted through the encoder to enhance\nthe spatial correlation. Further, a mutual-attentional fu-\nsion block is proposed to learn joint representation for clas-\nsiÔ¨Åcation. In this block, self-attention layer is extended\nto a mutual-attention layer, where features from different\nmodalities interact. Features after going through mutual-\nattention layer are fused via a simple operation to pro-\nduce the cross-modal representation for classiÔ¨Åcation. Our\nmethod is extensively evaluated on two large RGB-D ego-\ncentric datasets, THU-READ [12] and FPHA [15], and a\nsmall WCVS [16] dataset. Experiments results show that\nthe proposed method achieves the state-of-the-art results\nand outperforms the existing methods by a large margin.\nOur contributions can be summarized as follows:\n‚Ä¢ Transformer encoder is adopted to model the temporal\ncontextual information over the action period of each\nmodality;\n‚Ä¢ A mutual-attentional feature fusion block is proposed\nto learn a conjoint feature representation for classiÔ¨Åca-\ntion;\n‚Ä¢ The proposed method achieves the state-of-the-art re-\nsults on three standard RGB-D egocentric datasets.\n2. Related Work\nThird Person RGB-D Action Recognition RGB-D\nbased action recognition has attracted much attention and\nmany works have been reported to exploit the complemen-\ntary nature of RGB and depth modalities. Kong et al. [11]\npropose to project features from different modalities into\nshared space and learn RGB-Depth features for recogni-\ntion. Wang et al. [10] take the scene Ô¨Çow as input and\npropose a new representation called Scene Flow to Action\nMap (SFAM) for action recognition. Instead of treating\nRGB and depth as separate channels, Wang et al. [9] pro-\npose to train a single CNN (called c-ConvNet) for RGB-\nD action recognition and a ranking loss is utilized to en-\nhances the discriminative power of the learned features.\nLiu et al. [33] propose a multimodal feature fusion strat-\negy to exploit geometric and visual features within the de-\nsigned spatio-temporal LSTM unit for skeleton-based ac-\ntion recognition. Shahroudy et al. [34] adopt a deep auto-\nencoder based nonlinear common component analysis net-\nwork to discover the shared and informative components of\ninput RGB+D signals. For more methods, readers are re-\nferred to the survey paper [17]. The above mentioned meth-\nods are mainly based on the third-person datasets, and the\nÔ¨Årst-person action recognition has generated renewed inter-\nest due to the development of wearable cameras.\nFirst Person Action Recognition The early methods\nutilize semantic cues (object detection, hand pose and gaze\ninformation) to assist the egocentric action recognition. For\nexample, a hierarchical model is presented by [18] to ex-\nploit a joint representation of objects, hands and actions. Li\net al. [19] design a series egocentric cues for action recog-\nnition containing hand pose, head movement and gaze di-\nrection. The advance of the deep learning has led to the de-\nvelopment of methods based on CNNs and RNNs. Several\nmethods adopt two-stream structure [13] as the basic con-\nÔ¨Åguration and modify it to Ô¨Åt different purpose. Ma et al.\n[3] redesign the appearance stream for hand segmentation\nand object localization. Singh et al. [4] propose a com-\npact two-stream network which uses semantic cues. For\ntemporal encoding, LSTM and ConvLSTM are employed\nin [1, 2]. However these methods are all based on single\nRGB modality, and there are few works on RGB-D egocen-\ntric action recognition. Tang et al. [12] propose a multi-\nstream network to incorporate features from RGB, depth\nand optical Ô¨Çow using Cauchy estimator and orthogonality\nconstraint. Garcia-Hernando et al. [15] release a RGB-D\negocentric dataset with hand pose annotation, however they\ndo not propose any method based on RGB and depth. This\npaper focuses on Ô¨Årst person action recognition using RGB\nand depth modalities.\nTransformer Transformer [14], a fully-attentional ar-\nchitecture, has achieved state-of-the-art results than RNN or\nLSTM based methods for sequence modeling problem e.g.\nmachine translation and language modeling. Apart from\nNLP tasks, transformer has also been employed in some\ncomputer vision tasks such as image generation [20] and\nhuman action localization [21]. Inspired by there works,\nwe use transformer for intra-modality temporal modeling\nand cross-modality feature fusion.\n3. Proposed Method\nIn this section, we Ô¨Årst give an overview of the proposed\nframework. Then both inter-frame transformer encoder and\nmutual-attentional feature fusion block will be described in\ndetail.\n3.1. Overview\nThe proposed method is developed for egocentric action\nrecognition from heterogeneous RGB and depth modalities.\nAs shown in Fig. 1, the proposed method contains two\nparts, two transformer encoders and a mutual-attentional fu-\nsion block. The network takes aligned RGB frames and\ndepth maps as input, which are Ô¨Årst converted into two se-\nquences of feature embeddings. Then both sequence fea-\ntures are fed to the transformer encoders to model the tem-\nporal structure respectively. Features obtained from the en-\ncoders interact through the cross-modality block and then\nfused to produce the cross-modality representation. The\nconjoint features are processed through the linear layer\nto get per-frame classiÔ¨Åcation and then averaged over the\nframes of an action clip as the Ô¨Ånal recognition result.\n2\nrf1\nrF1\ndF1\ndF2\ndF3\ndF4\ndf1\ndf2\ndf3\ndf4\nrF2\nrF3\nrF4\nrF '\n1\nrF '\n2\nrF '\n3\nrF '\n4\nMutual-attentional Feature Fusion\ndF '\n1\ndF '\n2\ndF '\n3\ndF '\n4\nPer-frame classification\n))4,1((, ÔÉéiF c\ni\nAveraged classification\nrf2\nrf3\nrf4\nInter-frame Transformer Encoder Inter-frame Transformer Encoder\nCNN+Position Encoding CNN+Position Encoding\nFigure 1: The illustration of the proposed framework. It takes four RGB frames and the corresponding depth maps as input, which are\nprocessed by two encoders respectively. Features from each modality are interacted and incorporated through the mutual-attentional block\nto produce the cross-modal or joint representation. The Ô¨Ånal classiÔ¨Åcation is the average of each frames which produced by the joint\nrepresentation.\n3.2. Inter-frame Transformer Encoder\nAs shown in Fig. 1, the two transformer encoders pro-\ncess both RGB and depth data respectively which form a\ntwo-stream structure. Since both streams are composed of\nthe same network conÔ¨Åguration (not weight-shared), here\nwe just describe the RGB stream in detail. Given a se-\nquence with k RGB frames sampled from an action clip\n{fr\n1 , fr\n2 , ..., fr\nk }, we conduct average pooling on the fea-\nture maps of each frames to produce the feature embeddings\n{Fr\n1 , Fr\n2 , ..., Fr\nk }with size dmodel = 512. In order to en-\ncode the position information of each frame in the sequence,\nwe utilize the position encoding proposed by Vaswani et al.\n[14], which adopting sine and cosine functions of different\nfrequencies:\nPE(pos,2i) = sin(pos/100002i/dmodel ) (1)\nPE(pos,2i+1) = cos(pos/100002i/dmodel ) (2)\nwhere pos is the position and i is the dimension. This func-\ntion is chosen for the hypothesis that the model can easily\nlearn to attend by relative positions, since for any Ô¨Åxed off-\nset k, PEpos+k can be represented as a linear function of\nPEpos. The positional encodings have the same dimension\ndmodel as the embeddings, so that the two can be summed.\nThe remaining architecture essentially follows the standard\nTransformer which can be seen in Fig. 2. After obtain-\ning the feature embeddings, multi-head attention is applied\nto them. SpeciÔ¨Åcally, features are Ô¨Årst mapped to a series\nvectors query (Q) and key (K) of dimension dk, and value\n(V ) of dimension dv using different learned linear projec-\ntion. Then the dot-product of the Q vector and K vector\nare calculated through softmax function to get the attention\nweight, and have a weighted sum of V . For each head, the\nprocess can be presented by\nHeadi = Softmax(QKT\n‚àödk\n)V (3)\nThe concatenation of each head‚Äôs output is followed by a\ngroup of operations containing dropout, residual connection\nand LayerNorm (LN) [22].\nf‚Ä≤r = LN(Fr + Dropout(Concat(Headi))) (4)\nF‚Ä≤r = LN(f‚Ä≤r + Dropout(FFN(f‚Ä≤r))) (5)\nwhere Fr is the matrices packing of input feature embed-\ndings {Fr\n1 , Fr\n2 , ..., Fr\nk }, f‚Ä≤r is the intermediate feature dur-\ning process and F‚Ä≤r denotes the features after transformer\nencoder. Feed-Forward Networks (FFN) is composed of\ntwo convolution layers with kernel size being1. Notice that\nthe whole process is based on the matrices calculation.\nInstead of using recurrent units, the inter-frame depen-\ndencies is modeled using self-attention mechanism. In or-\nder to enhance the spatial correlation, we adopt a simple\nyet effective image cropping operation. As we know, most\n3\nùêªùëíùëéùëë‡¨µ‚Üí‡Øú\nConcat LN\nDropout Residual¬†ConnectionMatMultiply\nFFN\nMulti‚ÄêHead¬†Attention\nrF '\n1\nrF '\n2\nQ\nK\nV\nLinear¬†project\nSoftmax\nrF1\nrF2\nLN\nFigure 2: The architecture of the inter-frame transformer encoder.\nFor simplicity, the number of feature embeddings are set to 2.\nMulti-head attention is Ô¨Årst applied on the feature embeddings.\nThen the output of each head are concatenated and pass through\nseries operations contains residual connection, drop and Layer\nNormalization (LN). FFN denotes Feed-Forward Network. Note\nthat the calculation in the process is matrix operation which pack-\ning the embeddings.\nRGB-based action recognition methods will apply the data\naugmentation (resize, crop, Ô¨Çip, etc.) randomly on the in-\nput images. For image cropping, as shown in Fig. 3, the\nRGB images are sampled as a certain interval from the ac-\ntion clip (4 frames for simplify). Fig. 3a shows the normal\ncrop manner used in most methods [1, 2], where the yel-\nlow boxes indicate the cropped region of the original im-\nage, and the region are same for all the images. Fig. 3b\nindicates the random cropping used in our method. For ev-\nery sampled RGB image, we extract the region randomly.\nThe red boxes show that the cropped regions can be lo-\ncated in image everywhere. In short, normal cropping only\ntakes a Ô¨Åxed region of action videos into consideration dur-\ning one training iteration, while our random cropping vice\nthe verse. The random cropping has following advantages:\n1) the egocentric action clips always short and have small\nrange of motion, resulting plenty of inter-frames context re-\ndundancy. Comparing to crop same regions for all images,\nour cropping manner can augment the randomness of in-\nput data. 2) since the transformer is applied to model the\ninter-frame relationship, the repeated regions context will\nresult in the inefÔ¨Åciency of attention calculation. Differ-\nent cropped regions can effectively raise efÔ¨Åciencies and\nenhance the inter-frame spatial correlation. Results in Ta-\nble 5 shows the effectiveness of the proposed simple data\noperation.\n3.3. Mutual-attentional Feature Fusion\nDue to the feature variations in different modalities,\nit‚Äôs essential to learn a joint representation of the RGB\nand depth modalities. We propose a cross-modality block\nto interact features from both modalities using mutual-\nattentional mechanism. As shown in Fig. 4, the proposed\n(a) Cropping same region of all frames.\n(b) Cropping regions randomly of each sampled frames.\nFigure 3: Different data cropping manner, (a) yellow boxes in-\ndicates that regions are cropped at the same location as used in\nother methods. (b) red boxes indicates that regions are cropped\nrandomly as adoped in our method.\nmodule contains two parts, mutual-attention layer and fea-\nture fusion operation. The intermediate feature embeddings\nof both modalities from inter-frames encoder can be repre-\nsented as {F‚Ä≤r\n1 , F‚Ä≤r\n2 , ..., F‚Ä≤r\nk },\n{\nF‚Ä≤d\n1 , F‚Ä≤d\n2 , ..., F‚Ä≤d\nk\n}\n, where r\nand d represent RGB and depth. Qr (Qd), Kr (Kd) and V r\n(V d) matrices are computed following the standard trans-\nformer. Then the mutual-attention is applied to retrieve the\ninformation from context vectors (key Qd and value V d) of\ndepth stream related to query vector Qr of RGB stream and\nvice the verse. SpeciÔ¨Åcally, it calculates the RGB feature\nattention in depth modality and depth feature attention in\nRGB modality, and produces corresponding cross-modality\nfeatures {F‚Ä≤‚Ä≤r\n1 , F‚Ä≤‚Ä≤r\n2 , ..., F‚Ä≤‚Ä≤r\nk }and\n{\nF‚Ä≤‚Ä≤d\n1 , F‚Ä≤‚Ä≤d\n2 , ..., F‚Ä≤‚Ä≤d\nk\n}\nre-\nspectively. Then the dropout, residual connection and Lay-\nerNorm operations are also employed sequentially. After\nmutual-attention layer, features of each frame from both\nmodalities are fused via simply feature addition operation\nand used for per-frame classiÔ¨Åcation. The Ô¨Ånal classiÔ¨Åca-\ntion are the average of per-frame results. The whole process\ncan be presented as follows\n{\nF‚Ä≤‚Ä≤r\ni , F‚Ä≤‚Ä≤d\ni\n}\n= MutAtten(F‚Ä≤r\ni , F‚Ä≤d\ni ), i‚àà(1, k) (6)\nFc\ni = F‚Ä≤‚Ä≤r\ni + F‚Ä≤‚Ä≤d\ni (7)\nThe mutual-attention mechanism builds the interaction\namong different modalities, and features incorporated after\nsuch layers can beneÔ¨Åt from the narrow modality discrep-\nancy than fusing the features extracted from modalities di-\nrectly. Although such a co-attentional mechanism has been\nutilized in some vision-and-language tasks [23], e.g. visual\nquestion answer (VQA) and visual commonsense reason-\ning (VCR). However, the co-attentional used in our method\nhas two differences, 1) the two modalities data RGB and\ndepth are restricted aligned, RGB frames and depth maps\nare one-to-one correspondence. For vision-and-language\n4\nrF'\ndF'\nLinear¬†project Linear¬†project\nSoftmax\nSoftmax\nMutual‚Äêattention¬†layer\nSequential\nOperations\nSequential\nOperations\nFeature¬†\nAddition\nrF ''\ndF ''\ncFdQ\ndK\ndV\nrQ\nrK\nrV\nFigure 4: Illustration of the proposed mutual-attentional block. It\nconsists of a mutual-attention layer and feature fusion operation.\nFeature embeddings from RGB and depth modalities are fed to the\nattention layer to exchange the information. Then the features are\npassing through the sequential operations similar to transformer\nencoder and then fused together to get the joint representation\ntasks, words and visual inputs often suffer from mismatch\nissue, which affect the attention computation among inputs.\n2) the modality gap between visual feature and word em-\nbedding are much complexity. While both RGB and depth\nare image-level visual feature, which make them interacted\nwith each other through mutual-attention layer more effec-\ntively and straightforward.\n4. Experiments\nThe proposed method is extensively evaluated on three\nstandard RGB-D action recognition benchmark datasets,\nthe large THU-READ [12] and FPHA [15] datasets and the\nsmall WCVS [16] dataset. Ablation studies and attention\nmaps visualization are also reported to demonstrate the ef-\nfectiveness of the proposed method.\n4.1. Implementation Detail and Training\nThe proposed framework consists of two parallel streams\ncorresponding to the RGB and depth modalities. They inter-\nact through the fusion block. Both inter-frame attention en-\ncoders share the same structure, and ResNet-34 pre-trained\non the ImageNet dataset is adopted as the feature encoder.\nIn order to reduce the computation cost, the number of en-\ncoder is set to 1. The number of the heads for attention\ncalculation in both inter-frame block and mutual-attention\nblock is set to 8. Notice that, we found that the number of\nheads for mutual-attention set to 2 can perform slightly bet-\nter than 8 heads on THU-READ dataset, and we take the\nbetter results for THU-READ dataset comparison.\nThe experiments are conducted on the Pytorch frame-\nwork with a single TitanX GPU. The networks are trained\nfor 50 epochs with batch-size of 4 on all the three datasets.\nThe initial learning rate is set to 0.0001 and the learning rate\nis decayed by a factor of 0.1 after 30 epochs. Adam opti-\nmizer is used to train all networks. For the input data, we\nselect 32 frames from each action clip, uniformly sampled\nin time. Images are Ô¨Årst resized to 256, and then randomly\ncropped to 224 √ó224 for training. The center crop is used\nfor testing. The depth data are Ô¨Årst normalized to (0-1) and\nthen copied into a 3-channel input so that the depth stream\ncan directly utilize the pre-trained weight of ResNet-34.\n4.2. Datasets\nTHU-READ The THU-READ [12] dataset is cur-\nrent the largest RGB-D egocentric dataset which consists\nof 40 different actions performed by 8 subjects. The RGB\nand depth data are collected by Primesense Carmine cam-\nera, which is a RGB-D sensor released by Primesense. It\ncontains 1920 videos with each subject repeating each ac-\ntion for 3 times. We adopt the released leave-one-split-out\ncross validation protocol, which divides the 8 subjects into 4\ngroups and uses 3 splits for training and the rest for testing.\nFPHA The FPHA (First-Person Hand Action) [15]\ndataset collected with Intel RealSense SR300 RGB-D cam-\nera on the subject‚Äôs shoulder. It contains 1175 sequences\nbelonging to 45 action categories performed by 6 subjects.\nThe dataset also has accurate hand pose annotation. It is\nseparated into 1:1 setting for training and validation at video\nlevel with 600 sequences and 575 sequences respectively.\nWCVS Wearable Computer Vision Systems (WCVS)\n[16] dataset which is captured by RGB-D camera mounted\non a helmet that contains three levels of action recognition.\nThe Level 1 consists of two action categories, manipulation\nand non-manipulation. Level 2 subdivides the two action\ninto 4 and 6 classiÔ¨Åcations respectively. Although Level 3\ncontains Ô¨Åne-grained actions, the recording frequency is too\nlow to train a classiÔ¨Åer. Following the [16, 12], we adopt\nLevel 2 with 4 action classes to evaluate our method. The\ndataset is performed by 4 subjects in 2 scenarios. The large\nintra-class variations pose a great challenge to recognition.\nCross-subject evaluation metrics is adopted in this paper.\n4.3. Results and Comparison with the State-of-the-\nart\nAs shown in Table 1, the compared methods mainly con-\ntain hand-crafted feature based methods: HOG [24] and\nHOF [25], and deep learning-based methods: TSN [27]\nand MDNN [12]. In the cases of single modality, RGB-\nbased methods perform better than depth-based methods\nbecause of the vital texture features that RGB modality car-\nries. BeneÔ¨Åted from the transformer, our method can explic-\nitly model the intra-modality temporal structure and outper-\nform others on the THU-READ and WCVS datasets. In the\ncases of multi-modality, TSN exploits optical Ô¨Çow modality\nto process the motion information and treats depth and RGB\nmodality as separate channels for late score fusion. MDNN\n5\nMethods Modality THU-READ (%) WCVS(%)\nHOG [24] Depth 45.83 50.61\nHOF [25] Depth 43.96 41.25\nDepth Stream [26] Depth 34.06 58.47\nTSN [27] Depth 65.00 59.32\nHOG [24] RGB 39.93 52.14\nHOF [25] RGB 46.27 48.50\nAppearance Stream [26] RGB 41.90 60.36\nTSN [27] RGB 73.85 66.02\nTSN [27] RGB + Flow 78.23 67.05\nTSN [27] RGB + Flow + Depth 81.67 70.09\nMDNN [12] RGB + Flow + Depth + Hand 62.92 67.04\nTrear (Ours) Depth 76.04 63.72\nTrear (Ours) RGB 80.42 68.27\nTrear (Ours) RGB+Depth 84.90 71.49\nTable 1: Results obtained by the proposed ‚ÄúTrear‚Äù and comparison with the state-of-the-art methods on THU-READ and WCVS datasets.\nThe results are the average of the 4 splits and 5 subjects respectively.\nMethods Modality Accuracy\nTwo stream-color [28] RGB 61.56\nH+O [29] RGB 82.43\nHOG2-depth [30] Depth 59.83\nHON4D [31] Depth 70.61\n2-layer LSTM [15] Pose 80.14\nGram Matrix [32] Pose 85.39\nTwo stream [13] RGB + Flow 75.30\nHOG2-depth+pose [30]Depth + Pose 66.78\nTrear (Ours) Depth 92.17\nTrear (Ours) RGB 94.96\nTrear (Ours) RGB+Depth 97.04\nTable 2: Results obtained by ‚ÄúTrear‚Äù and comparisons with the\nstate-of-the-art methods on the FPHA dataset . Pose represents\nthe hand pose modality.\nemploys a multi-stream network and deploys Cauchy esti-\nmator and orthogonality constraint to assist egocentric ac-\ntion recognition. Our method achieves the state-of-the-art\nresults, indicating that the learned conjoint cross-modal rep-\nresentation produced by mutual-attention block can effec-\ntively exploit the complementary nature of both modalities.\nSince FPHA can be adopted as hand pose estimation\nbenchmark, thus hand pose annotations are given as a\nknown modality and can be used for action recognition as\nwell. It can be seen from Table 2, methods based on hand\npose outperform most of those based on RGB and/or depth.\nSince the egocentric video mostly contains the hands and\nMethods THU-READ FPHA WCVS\nResNet-34 79.60 89.16 64.58\nResNet-34+Encoder 84.58 94.96 68.27\nTable 3: Ablation study for the Inter-frame Transformer Encoder\non THU-READ (CS4), FPHA and WCVS datasets.\ninteracted objects, the hand pose feature contributes signif-\nicantly to the recognition performance. Tekin et al. based\non this characteristic, [29] develop a uniÔ¨Åed framework that\ncan estimate 3D hand, object poses and action category\nfrom RGB data. Two-stream [13] utilizes the optical Ô¨Çow to\nexploit short-term motion information and [15] introduces\nthe temporal information vis recurrent units (LSTM). Ben-\neÔ¨Åt from the proposed intra-frame encoder, our method can\nprocess the input frames parallel and build the context cor-\nrelation of the action clip without using Ô¨Çow and recur-\nrent unit. In short, our method (single modality or both\nRGB-D) outperforms all other methods by a large margin,\ndemonstrating the effectiveness of both transformer encoder\nand mutual-attention block in Ô¨Åne-grained egocentric action\nrecognition.\n4.4. Ablation Studies\nIn order to verify the effectiveness of the proposed inter-\nframe Transformer encoder and fusion block, ablation stud-\nies are conducted on the THU-READ, FPHA and WCVS\ndatasets. Since the inter-frame Transformer encoder in our\nframework is composed of CNN and Transformer encoder,\nwe conduct an the ablation study for the ResNet-34 and\n6\nMethods THU-REA FPHA WCVS\nSingle Modality\nDepth 77.50 92.17 63.72\nRGB 84.58 94.96 68.27\nRGB+D Feature Fusion\nConcatenation 86.25 94.43 70.16\nMultiplication 86.67 96.00 69.60\nAddition 86.67 94.09 69.59\nRGB+D Mutual-attentional Feature Fusion\nConcatenation 86.67 95.30 70.23\nMultiplication 85.83 97.04 69.58\nAddition 88.33 96.34 71.50\nTable 4: Ablation study of the proposed mutual-attention fusion\nblock with different fusion manners on THU-READ (CS4), FPHA\nand WCVS dataset.\nthe encoder, as shown in the Table 3. The results shows\nthat the Transformer encoder can effectively model the tem-\nporal structure and can improve the performance signiÔ¨Å-\ncantly. As shown in Table 4, RGB modality contributes\nmore signiÔ¨Åcantly to the recognition than depth modality\nbecause of the needed textual information. Directly fusing\nfeatures from both modalities for recognition even produces\nslightly worse results than using RGB alone, especially on\nthe FPHA dataset. This is probably due to the neglect of\nthe modality discrepancy. The proposed mutual-attentional\nblock can effectively mitigate such an issue, in which the\nfeatures from different modalities can exchange the infor-\nmation through the mutual-attention layer to reduce the fea-\nture variations. Then the cross-modal features are fused\nto produce the conjoint feature representation. The results\nalso show that the addition fusion performs better than con-\ncatenation and multiplication fusion on THU-READ and\nWCVS and a slightly worse in FPHA. In addition, Table 5\nshows the results of different image cropping methods, in-\ndicating that random cropping used in the proposed method\nimproves the performance signiÔ¨Åcantly.\n4.5. Attention Map Visualization\nAs shown in Fig. 5, the attention maps in inter-frame\ntransformer encoder and mutual-attentional layer are visu-\nalized respectively. The action is the ‚Äùdrink mug‚Äù with\n32 sampled frames, which conducts ‚Äùdrink‚Äù process twice,\n0 ‚àí14 frames is the Ô¨Årst ‚Äùdrink‚Äù and 15 ‚àí26 is the second\n‚Äùdrink‚Äù. Fig. 5a represents the inter-frame temporal atten-\ntion weight in the RGB stream transformer encoder. It can\nbe seen that the encoder can accurately capture the ‚Äùdrink-\ning‚Äù moments and most of frames are correlated to the two\nmoments. However, the correlation to the actions ‚ÄùPick up\nMethods Random Crop Crop same region\nTHU-READ\nDepth 77.50 73.75\nRGB 84.85 82.08\nRGB+Depth 88.33 86.67\nFPHA\nDepth 92.17 88.00\nRGB 94.96 91.65\nRGB+Depth 97.04 95.65\nTable 5: Ablation study for the data crop manner on THU-READ\n(CS4) and FPHA datasets.\n12:\n21:\n21\n(a) Attention map in RGB stream transformer en-\ncoder.\n12:\n24:\n8:\n16:\n(b) Attention map in mutual-attentional layer.\nFigure 5: Attention maps in both inter-frames attention and\nmutual-attentional layers. The vertical axis denotes the query vec-\ntors and the horizontal axis represents the context vectors. The\naction indicates ‚Äùdrink mug‚Äù in FPHA dataset.\nmug‚Äù and ‚Äùput down mug‚Äù are weak which mainly because\nof the appearance change are small in these actions. Fig. 5b\nshows the attention map in mutual-attentional layer. From\nthe Ô¨Ågure, we can see that the proposed co-attention mecha-\nnism can exploit the complementary characteristics of both\nmodality and model the complete action conduction pro-\ncess, contains ‚ÄùPick up mug ‚àídrinking ‚àíput down mug‚Äù\ntwice.\n5. Conclusion\nIn this paper, we present a novel framework for egocen-\ntric RGB-D action recognition. It consists of two modules,\n7\ninter-frame transformer encoder and the mutual-attentional\ncross-modality feature fusion block. The temporal informa-\ntion is encoded in each modality through the self-attention\nmechanism. Features from different modalities can ex-\nchange information via the mutual-attention layer and fused\nto become the conjoint cross-modal representation. Experi-\nmental results on three RGB-D egocentric datasets demon-\nstrates the effectiveness of the proposed method.\nAcknowledgment\nThis work was supported in part by the National Natural\nScience Foundation of China (Grant numbers: 61906173,\n61822701).\nReferences\n[1] S. Sudhakaran, S. Escalera, and O. Lanz, ‚ÄúLsta: Long\nshort-term attention for egocentric action recogni-\ntion,‚Äù inProceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2019, pp. 9954‚Äì\n9963.\n[2] S. Sudhakaran and O. Lanz, ‚ÄúAttention is all\nwe need: nailing down object-centric attention\nfor egocentric activity recognition,‚Äù arXiv preprint\narXiv:1807.11794, 2018.\n[3] M. Ma, H. Fan, and K. M. Kitani, ‚ÄúGoing deeper\ninto Ô¨Årst-person activity recognition,‚Äù in Proceedings\nof the IEEE Conference on Computer Vision and Pat-\ntern Recognition, 2016, pp. 1894‚Äì1903.\n[4] S. Singh, C. Arora, and C. Jawahar, ‚ÄúFirst person ac-\ntion recognition using deep learned descriptors,‚Äù in\nProceedings of the IEEE Conference on Computer Vi-\nsion and Pattern Recognition, 2016, pp. 2620‚Äì2628.\n[5] S. Yan, J. S. Smith, W. Lu, and B. Zhang, ‚ÄúMulti-\nbranch attention networks for action recognition in\nstill images,‚ÄùIEEE Transactions on Cognitive and De-\nvelopmental Systems, vol. 10, no. 4, pp. 1116‚Äì1125,\n2018.\n[6] H. Lee, M. Jung, and J. Tani, ‚ÄúRecognition of visu-\nally perceived compositional human actions by multi-\nple spatio-temporal scales recurrent neural networks,‚Äù\nIEEE Transactions on Cognitive and Developmental\nSystems, vol. 10, no. 4, pp. 1058‚Äì1069, 2018.\n[7] S. A. W. Talha, M. Hammouche, E. Ghorbel,\nA. Fleury, and S. Ambellouis, ‚ÄúFeatures and classi-\nÔ¨Åcation schemes for view-invariant and real-time hu-\nman action recognition,‚Äù IEEE Transactions on Cog-\nnitive and Developmental Systems, vol. 10, no. 4, pp.\n894‚Äì902, 2018.\n[8] D. K. Vishwakarma and K. Singh, ‚ÄúHuman activ-\nity recognition based on spatial distribution of gradi-\nents at sublevels of average energy silhouette images,‚Äù\nIEEE Transactions on Cognitive and Developmental\nSystems, vol. 9, no. 4, pp. 316‚Äì327, 2017.\n[9] P. Wang, W. Li, J. Wan, P. Ogunbona, and X. Liu,\n‚ÄúCooperative training of deep aggregation networks\nfor rgb-d action recognition,‚Äù in Thirty-Second AAAI\nConference on ArtiÔ¨Åcial Intelligence, 2018.\n[10] P. Wang, W. Li, Z. Gao, Y . Zhang, C. Tang, and\nP. Ogunbona, ‚ÄúScene Ô¨Çow to action map: A new rep-\nresentation for rgb-d based action recognition with\nconvolutional neural networks,‚Äù inProceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, 2017, pp. 595‚Äì604.\n[11] Y . Kong and Y . Fu, ‚ÄúBilinear heterogeneous informa-\ntion machine for rgb-d action recognition,‚Äù in Pro-\nceedings of the IEEE conference on computer vision\nand pattern recognition, 2015, pp. 1054‚Äì1062.\n[12] Y . Tang, Z. Wang, J. Lu, J. Feng, and J. Zhou, ‚ÄúMulti-\nstream deep neural networks for rgb-d egocentric ac-\ntion recognition,‚Äù IEEE Transactions on Circuits and\nSystems for Video Technology , vol. 29, no. 10, pp.\n3001‚Äì3015, 2018.\n[13] K. Simonyan and A. Zisserman, ‚ÄúTwo-stream convo-\nlutional networks for action recognition in videos,‚Äù in\nAdvances in neural information processing systems ,\n2014, pp. 568‚Äì576.\n[14] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, ≈Å. Kaiser, and I. Polosukhin,\n‚ÄúAttention is all you need,‚Äù in Advances in neural in-\nformation processing systems, 2017, pp. 5998‚Äì6008.\n[15] G. Garcia-Hernando, S. Yuan, S. Baek, and T.-K.\nKim, ‚ÄúFirst-person hand action benchmark with rgb-\nd videos and 3d hand pose annotations,‚Äù in Proceed-\nings of the IEEE conference on computer vision and\npattern recognition, 2018, pp. 409‚Äì419.\n[16] M. Moghimi, P. Azagra, L. Montesano, A. C. Murillo,\nand S. Belongie, ‚ÄúExperiments on an rgb-d wearable\nvision system for egocentric activity recognition,‚Äù in\nProceedings of the IEEE Conference on Computer Vi-\nsion and Pattern Recognition Workshops , 2014, pp.\n597‚Äì603.\n[17] P. Wang, W. Li, P. Ogunbona, J. Wan, and S. Escalera,\n‚ÄúRgb-d-based human motion recognition with deep\nlearning: A survey,‚Äù Computer Vision and Image Un-\nderstanding, vol. 171, pp. 118‚Äì139, 2018.\n8\n[18] A. Fathi, A. Farhadi, and J. M. Rehg, ‚ÄúUnderstand-\ning egocentric activities,‚Äù in 2011 international con-\nference on computer vision . IEEE, 2011, pp. 407‚Äì\n414.\n[19] Y . Li, Z. Ye, and J. M. Rehg, ‚ÄúDelving into egocentric\nactions,‚Äù in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , 2015, pp.\n287‚Äì295.\n[20] N. Parmar, A. Vaswani, J. Uszkoreit, ≈Å. Kaiser,\nN. Shazeer, A. Ku, and D. Tran, ‚ÄúImage transformer,‚Äù\narXiv preprint arXiv:1802.05751, 2018.\n[21] R. Girdhar, J. Carreira, C. Doersch, and A. Zisserman,\n‚ÄúVideo action transformer network,‚Äù inProceedings of\nthe IEEE Conference on Computer Vision and Pattern\nRecognition, 2019, pp. 244‚Äì253.\n[22] J. L. Ba, J. R. Kiros, and G. E. Hinton, ‚ÄúLayer normal-\nization,‚Äù arXiv preprint arXiv:1607.06450, 2016.\n[23] J. Lu, D. Batra, D. Parikh, and S. Lee, ‚ÄúVilbert: Pre-\ntraining task-agnostic visiolinguistic representations\nfor vision-and-language tasks,‚Äù inAdvances in Neural\nInformation Processing Systems, 2019, pp. 13‚Äì23.\n[24] H. Wang, M. M. Ullah, A. Klaser, I. Laptev, and\nC. Schmid, ‚ÄúEvaluation of local spatio-temporal fea-\ntures for action recognition,‚Äù in BMCV, 2009.\n[25] I. Laptev, M. Marszalek, C. Schmid, and B. Rozen-\nfeld, ‚ÄúLearning realistic human actions from movies,‚Äù\nin 2008 IEEE Conference on Computer Vision and\nPattern Recognition. IEEE, 2008, pp. 1‚Äì8.\n[26] K. Simonyan and A. Zisserman, ‚ÄúVery deep convo-\nlutional networks for large-scale image recognition,‚Äù\narXiv preprint arXiv:1409.1556, 2014.\n[27] L. Wang, Y . Xiong, Z. Wang, Y . Qiao, D. Lin, X. Tang,\nand L. Van Gool, ‚ÄúTemporal segment networks: To-\nwards good practices for deep action recognition,‚Äù in\nEuropean conference on computer vision. Springer,\n2016, pp. 20‚Äì36.\n[28] C. Feichtenhofer, A. Pinz, and A. Zisserman, ‚ÄúCon-\nvolutional two-stream network fusion for video action\nrecognition,‚Äù in Proceedings of the IEEE conference\non computer vision and pattern recognition, 2016, pp.\n1933‚Äì1941.\n[29] B. Tekin, F. Bogo, and M. Pollefeys, ‚ÄúH+ o: UniÔ¨Åed\negocentric recognition of 3d hand-object poses and in-\nteractions,‚Äù inProceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , 2019, pp.\n4511‚Äì4520.\n[30] E. Ohn-Bar and M. M. Trivedi, ‚ÄúHand gesture recog-\nnition in real time for automotive interfaces: A\nmultimodal vision-based approach and evaluations,‚Äù\nIEEE transactions on intelligent transportation sys-\ntems, vol. 15, no. 6, pp. 2368‚Äì2377, 2014.\n[31] O. Oreifej and Z. Liu, ‚ÄúHon4d: Histogram of ori-\nented 4d normals for activity recognition from depth\nsequences,‚Äù in Proceedings of the IEEE conference\non computer vision and pattern recognition, 2013, pp.\n716‚Äì723.\n[32] X. Zhang, Y . Wang, M. Gou, M. Sznaier, and\nO. Camps, ‚ÄúEfÔ¨Åcient temporal sequence comparison\nand classiÔ¨Åcation using gram matrix embeddings on\na riemannian manifold,‚Äù in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recogni-\ntion, 2016, pp. 4498‚Äì4507.\n[33] J. Liu and S. Amir and D. Xu and A. Kot and\nG. Wang,‚ÄúSkeleton-based action recognition using\nspatio-temporal lstm network with trust gates‚Äù, in\nIEEE transactions on pattern analysis and machine\nintelligence, 2017, pp. 3007‚Äì3021.\n[34] S. Amir and T. Ng and Y . Gong and G. Wang,‚ÄúDeep\nmultimodal feature analysis for action recognition in\nrgb+ d videos‚Äù, in IEEE transactions on pattern anal-\nysis and machine intelligence, 2017, pp. 1045‚Äì1058.\n9"
}