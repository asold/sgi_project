{
  "title": "TransEM: Residual Swin-Transformer Based Regularized PET Image Reconstruction",
  "url": "https://openalex.org/W4280622762",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2052737604",
      "name": "Rui Hu",
      "affiliations": [
        "Zhejiang University",
        "State Key Laboratory of Modern Optical Instruments"
      ]
    },
    {
      "id": "https://openalex.org/A2096466688",
      "name": "Huafeng Liu",
      "affiliations": [
        "Zhejiang University",
        "State Key Laboratory of Modern Optical Instruments",
        "Jiaxing University"
      ]
    },
    {
      "id": "https://openalex.org/A2052737604",
      "name": "Rui Hu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096466688",
      "name": "Huafeng Liu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2270346996",
    "https://openalex.org/W2071189636",
    "https://openalex.org/W2069629287",
    "https://openalex.org/W3110968147",
    "https://openalex.org/W1963933039",
    "https://openalex.org/W1985396125",
    "https://openalex.org/W2014351445",
    "https://openalex.org/W2041114617",
    "https://openalex.org/W2002480337",
    "https://openalex.org/W3047729951",
    "https://openalex.org/W3092370246",
    "https://openalex.org/W2970280802",
    "https://openalex.org/W2963385325",
    "https://openalex.org/W3037333647",
    "https://openalex.org/W3004829055",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W1946620893",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2017162022",
    "https://openalex.org/W2039284087",
    "https://openalex.org/W2154744699",
    "https://openalex.org/W2156612134",
    "https://openalex.org/W2798538010",
    "https://openalex.org/W2055483062",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4253515568",
    "https://openalex.org/W2899663614"
  ],
  "abstract": null,
  "full_text": "TransEM: Residual Swin-Transformer based\nregularized PET image reconstruction\nRui Hu1 and Huafeng Liu1,2,3(\u0000 )\n1 State Key Laboratory of Modern Optical Instrumentation, Department of Optical\nEngineering,Zhejiang University, Hangzhou 310027, China\nliuhf@zju.edu.cn\n2 Jiaxing Key Laboratory of Photonic Sensing & Intelligent Imaging, Jiaxing 314000,\nChina\n3 Intelligent Optics & Photonics Research Center, Jiaxing Research Institute,\nZhejiang University, Jiaxing 314000, China\nAbstract. Positron emission tomography (PET) image reconstruction\nis an ill-posed inverse problem and suﬀers from high level of noise due to\nlimited counts received. Recently deep neural networks especially convo-\nlutional neural networks (CNN) have been successfully applied to PET\nimage reconstruction. However, the local characteristics of the convo-\nlution operator potentially limit the image quality obtained by current\nCNN-based PET image reconstruction methods. In this paper, we pro-\npose a residual swin-transformer based regularizer (RSTR) to incorpo-\nrate regularization into the iterative reconstruction framework. Speciﬁ-\ncally, a convolution layer is ﬁrstly adopted to extract shallow features,\nthen the deep feature extraction is accomplished by the swin-transformer\nlayer. At last, both deep and shallow features are fused with a residual\noperation and another convolution layer. Validations on the realistic 3D\nbrain simulated low-count data show that our proposed method outper-\nforms the state-of-the-art methods in both qualitative and quantitative\nmeasures.\nKeywords: Positron Emission Tomography (PET)· image reconstruc-\ntion · model-based deep learning· Transformer.\n1 INTRODUCTION\nPositron Emission Tomography (PET) is one of the irreplaceable tools of func-\ntional imaging, which is wildly used in oncology, cardiology, neurology and med-\nical research [1]. However, PET images usually suﬀer from high level of noise\ndue to many physical degradation factors and the ill-conditioning of PET recon-\nstruction problem.\nTo reconstruct high-quality PET images, lots of works have been proposed\noverthelastfewdecades,whichcanberoughlydividedintoﬁvecategories:1)tra-\nditional analytic methods such as ﬁltered back-projection (FBP [2]) and itera-\ntive methods like maximum-likelihood expectation maximization (ML-EM [3]);\narXiv:2205.04204v2  [eess.IV]  24 Oct 2022\n2 R. Hu and H. Liu\nMulti-Head\nSelf-Attention\nLayer Norm\nLayer Norm\nMLP\nConv\nConv\nSwin-transformer Layer\nEM\nRSTR\nPixel to Pixel \nFusion\nEM\nRSTR\nPixel to Pixel \nFusion\nEM\nRSTR\nPixel to Pixel \nFusion\nBlock 1 Block 2 Block n\n0x\nnx\nSinogram\nY\n…\nResidual Swin-transformer Regularizer\nFig. 1.The overall ﬂow-chart of proposed method. Speciﬁcally, TransEM is composed\nof n blocks. Each block contains EM for image updating, RSTR for regularization and\na pixel to pixel fusion operation.\n2)prior-incorporative methods; 3)image post-processing (denoising) methods; 4)\nPenalized Log-Likelihood (PLL) methods and 5)deep learning based methods.\nThe FBP algorithm is based on the central slice theorem, which can rapidly\nﬁnish the reconstruction but suﬀers from heavy noise due to the lack of mod-\neling of physical properties. Iterative algorithms, such as ML-EM modeled the\nphysical properties and improved image quality. However, the excessive noise\npropagation from the measurements is the biggest disadvantage of ML solution.\nTo further improve the image quality, prior-incorporative reconstruction meth-\nods, image post-processing methods and PLL methods have been introduced.\nThe performance of PLL methods [4,5,6] and prior-incorporative methods like\nkernel methods [7] are closely related to the hyper-parameters that are often\nhand-crafted before reconstruction. Post-processing is an eﬀective way to reduce\nnoise such as BM3D [8], non-local mean (NLM) [9] and gaussian ﬁlter. However,\nthese methods usually tend to be over-smoothing and time-consuming.\nDeep learning (DL) techniques especially supervised learning techniques have\nrecently drawn much attention and shown promising results in PET image recon-\nstruction[10].Amongthem,directlearning,DL-basedpost-denoisingandmodel-\nbased learning are three mainstream approaches. Direct learning [11] methods\nusually learn the mapping from sinogram to the PET image through deep neu-\nral networks (DNN). Because there are no physical constraints, direct learning\nmethods are extremely data-hungry and sometimes unstable. DL-based post-\nTransEM 3\ndenoising methods [12] are simple to implement, but the ﬁnal results are very\nsensitive to the pre-reconstruction algorithms.\nBy unrolling an iterative reconstruction algorithm, model-based learning\nshows inspiring results and good interpretability, which has been a promising\ndirection. Gong et al. proposed an unrolled network based on 3D U-net and\nalternating direction method of multipliers (ADMM) [13]. Mehranianet al.pro-\nposed a forward backward splitting algorithm for Poisson likelihood and unrolled\nthe algorithm into a recurrent neural network with several blocks [14]. Limet al.\nunrolled the block coordinate descent (BCD) algorithm with U-net [15]. All these\nmethods adopt convolutional neural networks (CNN) to assist in reconstruction.\nHowever, a convolution operator has a local receptive ﬁeld [16], giving rise to\nthat CNNs cannot process long-range dependencies unless passing through a\nlarge number of layers. while when layer number increases, the feature reso-\nlution and ﬁne details may be lost, which limits the quality of reconstructed\nimages. For this issue, the Transformer [17] is noticed for its strong ability in\nmodeling long-range dependencies of the data and tremendous success in the lan-\nguage domain. Recently, it has also demonstrated promising results in computer\nvision.\nIn this paper, we propose a residual swin-transformer [18] based regular-\nizer (RSTR) along with the ML-EM iterative framework, called TransEM, to\nreconstruct the standard-dose image from low count sinogram. As one of the\nmodel-based learning methods (MoDL), TransEM does not need a large train-\ning dataset and achieves state-of-the-art results in realistic 3D brain simulation\ndata.\n2 METHODS AND MATERIALS\n2.1 Problem formulation\nIn PET image reconstruction from sinogram data, The measured datay can be\nwell modeled by a Poisson noise model given by:\ny ∼Poisson{y} s.t. y = Ax + b (1)\nwhere y ∈RI is the mean of the measured datay ∈RI with yi representing the\ni-th detector bin,x ∈RJ is the unknown activity distribution image withxj\nrepresentingj-th voxel.b ∈RI denotes the expectation of scatters and randoms.\nI is the number of detector pairs andJ is the number of pixels.A ∈RI×J is\nsystem response matrix withAij representing the probabilities of detecting an\nemission from voxelj at detectori.\nLike many other under-determined inverse problem, the unknown imagex\ncan be estimated from a Bayesian perspective:\nˆx = arg max\nx\nL(y|x) −βR(x) (2)\nL(y|x) =\n∑\ni\nyilog yi −yi (3)\n4 R. Hu and H. Liu\nwhere L(y|x) is the Poisson log-likelihood function of measured sinogram data,\nR(x) is the regularization term,β is the parameter that controls the regulariza-\ntion.\nThe forward-backward splitting(FBS) algorithm [19] and optimization trans-\nfer method can be used to solve Eq. (2). FBS algorithm is used to split the\nobjection function into two terms:\nrk = xk−1 −αβ∇R(xk−1) (4)\nxk = arg max\nx\nL(x|y) − 1\n2α||x −rk||2 (5)\nwhere Eq. (4) is a gradient descent update with step size ofα and k denotes\nk-th iteration. In original FBSEM [14], the Eq. (4) was replaced by a Residual\nCNN [20] unit, while the performance of CNN-based regularizer in long-range\ndependencies is limited due to their localized receptive ﬁelds, which limits the\nqualityoftheimagesobtained.Toaddressthisissue,weproposedaresidualswin-\ntransformer based regularizer (RSTR) to replace the gradient descent update in\nEq. (4):\nrk = RSTR(xk−1) (6)\nEq. (5) can be reformulated with optimize transfer [21] method and EM surro-\ngate [22]:\nxk = arg max\nx\n∑\nj\nˆxk\nj,EM ln(xj) −xj − 1\n2α∑\niAij\n(xk\nj −rk\nj)\n2\n(7)\nand ˆxk\nj,EM is given by ML-EM [3] algorithm:\nˆxk\nj,EM = xk−1\nj\n1∑\niAij\n∑\ni\nAij\nyi\nyi\n(8)\nsetting the derivative of Eq. (7) to zero, the following closed-form solution can\nbe obtained:\nxk\nj =\n2xk\nj,EM\n1 −\nrk\nj\nα∑\ni Aij\n+\n√\n(1 −\nrk\nj\nα∑\ni Aij\n)\n2\n+ 4\nxk\nj,EM\nα∑\ni Aij\n(9)\nit can be viewed as a pixel to pixel fusion between regularized reference image\nrk\nj and ML-EM resultxk\nj,EM. The parameterα was learned from training data.\nThe whole reconstruction workﬂow called TransEM is shown in Fig. 1. The\nTransEM was unrolled to n blocks, where each block consists of two separate\nsteps and a pixel to pixel fusion operation. The two separate steps are a EM\nstep for image update from measured sinogram data and a deep learning step\nfor prior learning using proposed residual swin transformer based regularizer\n(RSTR) in image domain.\nTransEM 5\n2.2 Residual swin-transformer regularizer\nAs shown in Fig. 1, the RSTR is a residual block with a Swin Transformer\nLayer (STL) [18] and two convolutional layers. At ﬁrst, a3 ×3 convolutional\nlayer is used to extract the shallow feature, then a STL is used to extract deep\nfeatures. At last, another 3 ×3 convolutional layer is used to aggregate the\nshallow and deep features with a residual learning operation. STL is based on\noriginal Transformer layer and multi-head self-attention (MSA), the input of\nsize H×W ×C is ﬁrstly reshaped to a feature map with size ofHW\nM2 ×M2 ×C\naccording to the shifted window mechanism. Then the standard self-attention\nseparately for each window is calculated. After that, a multi-layer perceptron\n(MLP) with GELU [23]activation are used. Besides, the residual connection is\napplied for both modules and the LayerNorm (LN) is added before MLP and\nMSA.\nThe whole process of RSTR is formulated as:\nX1 = Conv3×3(Input)\nX2 = MSA(LN(X1)) +X1\nX3 = MLP(LN(X2)) +X2\nOutput= Conv3×3(X3) +X0\n(10)\n2.3 Implementation details and Reference Methods\nThe TransEM was unrolled with ordered subsets (OS) acceleration and imple-\nmented using Pytorch 1.7 on a NVIDIA RTX 3090. The number of unrolled\nBlocks is 60 (10 iterations and 6 subsets). The windowsize of STL (M) is 4.\nAdam [24] optimizer and Mean square error (MSE) loss between the network\noutputs and the label images were used during training. The image x0 was\ninitialized with values of one. The proposed TransEM was compared with con-\nventional ordered subsets expectation maximization (OSEM [25]), maximum a\nposterior probability expectation maximization algorithm (MAPEM [26]), Deep-\nPET [27] and FBSEM [14]. For both OSEM and MAPEM, 10 iterations and 6\nsubsets were adopted. The quadratic penalty was used for MAPEM and theβ\nwas set to 0.005. Both DeepPET and FBSEM were trained with MSE loss and\nAdam optimizer. The learning rate was 5e-5, batch size was 4.\n3 EXPERIMENT AND RESULTS\n3.1 Experimental evaluation\nTwenty 3D brain phantoms from BrainWeb [28] were used to simulate 2D18F\nFDG PET images with the resolution and matrix size of 2.086×2.086×2.031\nmm3 and 344×344×127 acquired from a Siemens Biograph mMR. For each\nphantom, 10 noncontinuous slices were selected from each of the three orthogo-\nnal views to generate high count sinograms which were used to reconstruct the\n6 R. Hu and H. Liu\nOSEM DeepPET FBSEM ProposedMAPEM\nLabel\nSlice 4\nSlice 15\nSlice 27\nFig. 2.Reconstruction results of OSEM, MAPEM, DeepPET, FBSEM and proposed\nTransEM on three orthogonal views of one test brain sample.\nlabel images and low count sinograms with size of 172×252. The system ma-\ntrix was simulated with Siddon projection [29]. For high count,5 ∗106 counts\nand point spread function (PSF) modeling with 2.5mmfull width at half maxi-\nmum (FWHM) Gaussian kernels were used, while5 ∗105 counts on average and\nPSF of 4mm were used for low count. The high dose label images were recon-\nstructed from high count sinogram using OSEM algorithm with 10 iterations\nand 6 subsets. Besides, ﬁfteen hot spheres of radius ranging from 2mm to 8mm\nwere inserted into all phantoms. TransEM has trained with 17 brain samples\n(510 slices) to map low count sinogram to high dose label PET images, and 2\nbrain samples (60 slices) for testing and 1 brain sample (30 slices) for valida-\ntion. To assess reconstruction quality, quantitative comparisons were performed\nagainst high dose label images. Both references and reconstructed images were\nnormalized to a maximum of 1. Peak signal to noise ratio (PSNR), structural\nsimilarity index (SSIM [30]) and mean contrast recovery coeﬃcients (MCRC)\nwere calculated.\nMCRC = 1\nN\nN∑\nn=1\nIa\nItrue\n(11)\nwhere N is the number of pictures which contains the simulated tumors,Iais\nthe average uptake of all the tumor areas in the test phantom.\n3.2 Results\nFig. 2 shows three orthogonal views of the reconstructed brain PET images\nusing diﬀerent methods. It can be observed that the conventional OSEM algo-\nrithm suﬀers from high level noise. MAPEM reduces noise but always shows\nTransEM 7\nO\nSEMM APEMD eepPETF BSEMP roposed121\n62\n02\n42\n8 \nP\nSNR(dB)\n(a) PSNR\nO\nSEMM APEMD eepPETF BSEMP roposed0.700\n.750\n.800\n.850\n.900\n.951\n.00SSIM (b) SSIM\nFig. 3.Quantitative image quality(PSNR, SSIM) comparison among diﬀerent methods\n.\nFBSEM Proposed\nHigh counts DeepPET\nSlices’ direction of \ntraining data\nSlices’ direction of \ntest data\nFig. 4.Robustness analysis on the diﬀerence slices’ direction between the training set\nand test set. In this experiment, the training slices are selected from the transverse\nplane, while the test slices are from the sagittal plane.\nover-smooth, resulting in losses of detailed information. As one of the direct\nlearning methods, DeepPET performed not so good. One possible reason is that\nDeepPET is extremely data-hungry, so poor performance on a small dataset is\nexpected. The FBSEM has a better noise reduction compared to the traditional\nmethod OSEM, MAPEM and direct learning method DeepPET, but also has\nsome noises showing up in diﬀerent regions and some structural information is\nnot well recovered. As seen, the proposed TransEM revealed more cortex struc-\ntures and preserved edges well compared to other methods. The quantitative\nresults on the test set are demonstrated in Fig. 3 where our proposed method\nachieves the highest scores among all the methods.\n3.3 Robustness analysis\nBesides, to analyze the robustness of the proposed TransEM on diﬀerent low\ncount levels, we have trained DeepPET, FBSEM, and TransEM on 1/4, 1/100\ndownsampled data. The training label is reconstructed by OSEM with high\ncount(5e6) data. Each experiment involves retraining and testing. As shown in\nTable 1, including 1/10 downsampled data results mentioned above, TransEM\nbeats all comparison methods at diﬀerent counts except DeepPET in 1/100\n8 R. Hu and H. Liu\nTable 1.The PSNR SSIM and MCRC of the test set with diﬀerent counts level.\nMethod Counts=1.25e6 (1/4) Counts=5e5 (1/10) Counts=5e4 (1/100)\nPSNR SSIM MCRC PSNR SSIM MCRC PSNR SSIM MCRC\nMLEM 19.97 ±2.69 0.86±0.02 0.6852 19.24±2.34 0.84±0.03 0.5109 15.03±1.93 0.77±0.03 0.1662\nMAPEM 22.35±2.26 0.88±0.02 0.8187 22.30±2.25 0.86±0.02 0.7983 17.04±2.23 0.79±0.03 0.3838\nDeepPET 20.74±2.05 0.82±0.04 0.7005 21.77±2.13 0.84±0.04 0.6813 20.69±2.8320.69±2.8320.69±2.83 0.82±0.05 0.66900.66900.6690\nFBSEM 22.52 ±2.00 0.88±0.01 0.8448 22.94±1.84 0.88±0.02 0.8518 19.16±2.35 0.82±0.03 0.5681\nProposed 22.61±2.0022.61±2.0022.61±2.00 0.90±0.010.90±0.010.90±0.01 0.85780.85780.8578 23.10±1.8623.10±1.8623.10±1.86 0.89±0.020.89±0.020.89±0.02 0.87180.87180.8718 20.10±2.47 0.84±0.030.84±0.030.84±0.03 0.5765\nWithout RC\nWith RC\nSlice 5 Slice 24 Slice 43\n(a) Ablation study on RC\n0\n0.2\n0.4\n0.6\n0.8\n1\n12\n14\n16\n18\n20\n22\n24\n0 12 24 36 48 60 72 84 96\nPSNR(dB)\nNumber of unrolled blocks\nPSNR SSIM CRC (b) Ablation study on Unrolled blocks\nFig. 5.Ablation study on diﬀerent settings of TransEM.\ndownsampled situation, while we would like to emphasize that it looks like Deep-\nPET got pretty good PSNR and MCRC, in ultra-low count situation, due to the\nlack of physical constraints, the over-ﬁtting of DeepPET is severe and the results\nare not very reliable which is proved true when we trained the three learning\nmethods with transverse slices and tested with sagittal slices. We selected train-\ning slices from the transverse plane and test slices from the sagittal plane to test\nthe generalization ability of three learning-based methods as shown in Fig. 4. It\ncan be observed that the generalization ability of DeepPET is poor.\n3.4 Ablation study and discussion\nFigure 5(a) shows two residual connection (RC) variants outside STL in RSTR.\nWithout residual connection, the training step is easily falling into sub-optimal\nsolution and is diﬃcult to convergence. The signiﬁcance of RC also lies in the\ncomparison of reconstruction results.In TransEM proposed in this paper, most of\nthe parameters are learned from training data, however, the number of unrolled\nblocks is hand-crafted. In this section, the sensitivity of the number of unrolled\nblocks is analyzed. Due to the limitation of hardware and image size, the number\nof subsets that we chose is 6, so the number of unrolled blocks is multiples of six.\nWhen the number is 60, the TransEM achieves the best performance as shown in\nFig 5(b), so the number of unrolled blocks is 60 in the experiment in this paper.\nTransEM 9\n4 CONCLUSIONS\nIn this work, we proposed a model-based deep learning method by unrolling the\nEMalgorithmwithresidualswin-transformerregularizerforlow-dosePETimage\nreconstruction. Simulated human brain data were used in the evaluation. Both\nquantitative and qualitative results show that the proposed TransEM performs\nbetter than the FBSEM, DeepPET as well as traditional OSEM and MAPEM\nregarding PSNR, SSIM and MCRC. Because lack of clinical PET data currently,\nfuture work will focus on more clinical evaluations.\nAcknowledgements This work was supported in part by the Talent Program\nof Zhejiang Province (2021R51004) and by the National Natural Science Foun-\ndation of China (U1809204).\nReferences\n1. Gunn, R., Slifstein, M., Searle, G. , Price, J.: Quantitative imaging of protein targets\nin the human brain with PET.Physics in Medicine and Biology. 60, R363-R411\n(2015)\n2. Brooks , Rodney, A.: Statistical limitations in X-ray reconstructive tomography.\nMedical Physics. 3, 237-240 (1976)\n3. Shepp, L. , Vardi, Y.: Maximum likelihood reconstruction for emission tomography.\nIEEE Transactions on Medical Imaging. 1, 113-122 (1982)\n4. Xie, N., Gong, K., Guo, N., Qin, Z., Wu, Z., Liu, H. , Li, Q.: Penalized-Likelihood\nPET Image Reconstruction Using 3D Structural Convolutional Sparse Coding.\nIEEE Transactions on Biomedical Engineering. 69, 4-14 (2022)\n5. Chen, S., Liu, H., Shi, P. , Chen, Y.: Sparse representation and dictionary learn-\ning penalized image reconstruction for positron emission tomography.Physics in\nMedicine and Biology. 60, 807-823 (2015)\n6. Chen, S., Liu, H., Hu, Z., Zhang, H., Shi, P. , Chen, Y.: Simultaneous Reconstruction\nand Segmentation of Dynamic PET via Low-Rank and Sparse Matrix Decomposi-\ntion. IEEE Transactions on Biomedical Engineering. 62, 1784-1795 (2015)\n7. Wang, G. , Qi, J.: PET image reconstruction using kernel method.IEEE Transac-\ntions on Medical Imaging. 34, 61-71 (2014)\n8. Feruglio, P., Vinegoni, C., Gros, J., Sbarbati, A. , Weissleder, R.: Block matching\n3D random noise ﬁltering for absorption optical projection tomography.Physics in\nMedicine and Biology. 55, 5401 (2010)\n9. Dutta, J., Leahy, R. , Li, Q.: Non-local means denoising of dynamic PET images.\nPlos One. 8, e81390 (2013)\n10. Reader, A., Corda, G., Mehranian, A., Costa-Luis, C., Ellis, S. , Schnabel,: J.\nDeep learning for PET image reconstruction.IEEE Transactions on Radiation and\nPlasma Medical Sciences. 5, 1-25 (2020)\n11. Wang, B. , Liu, H.: FBP-Net for direct reconstruction of dynamic PET images.\nPhysics in Medicine and Biology. 65, 235008 (2020)\n12. Cui, J., Gong, K., Guo, N., Wu, C., Meng, X., Kim, K., Zheng, K., Wu, Z., Li-Fu,\nXu, B., Zhu, Z., Tian, J., Liu, H. , Li, Q.: PET image denoising using unsupervised\ndeep learning. European Journal of Nuclear Medicine and Molecular Imaging. 46\npp. 2780 - 2789 (2019)\n10 R. Hu and H. Liu\n13. Gong, K., Guan, J., Kim, K., Zhang, X., Yang, J., Seo, Y., El Fakhri, G., Qi,\nJ. , Li, Q.: Iterative PET image reconstruction using convolutional neural network\nrepresentation. IEEE Transactions on Medical Imaging. 38, 675-685 (2018)\n14. Mehranian, A. , Reader, A.: Model-Based Deep Learning PET Image Reconstruc-\ntion Using Forward–Backward Splitting Expectation–Maximization.IEEE Trans-\nactions on Radiation and Plasma Medical Sciences. 5, 54-64 (2020)\n15. Lim, H., Chun, I., Dewaraja, Y. , Fessler, J.: Improved low-count quantitative\nPETreconstructionwithaniterativeneuralnetwork. IEEE Transactions on Medical\nImaging. 39, 3512-3522 (2020)\n16. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Un-\nterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.: An image is\nworth 16x16 words: Transformers for image recognition at scale.ArXiv Preprint\narXiv:2010.11929. (2020)\n17. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser,\nŁ. , Polosukhin, I.: Attention is All you Need. Advances in Neural Information\nProcessing Systems. 30 (2017)\n18. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S. , Guo, B.: Swin trans-\nformer: Hierarchical vision transformer using shifted windows.Proceedings of the\nIEEE/CVF International Conference on Computer Vision. pp. 10012-10022 (2021)\n19. Combettes, P. , Pesquet, J.: Proximal splitting methods in signal processing.Fixed-\npoint Algorithms for Inverse Problems in Science and Engineering. pp. 185-212\n(2011)\n20. He, K., Zhang, X., Ren, S. , Sun, J.: Deep residual learning for image recognition.\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\npp. 770-778 (2016)\n21. Wang, G. , Qi, J.: Penalized likelihood PET image reconstruction using patch-\nbased edge-preserving regularization.IEEE Transactions on Medical Imaging. 31,\n2194-2204 (2012)\n22. Lange, K., Hunter, D. , Yang, I.: Optimization transfer using surrogate objective\nfunctions. Journal of Computational and Graphical Statistics. 9, 1-20 (2000)\n23. Hendrycks, D. , Gimpel, K.: Gaussian error linear units (gelus).ArXiv Preprint\narXiv:1606.08415. (2016)\n24. Kingma, D. , Ba, J.: Adam: A method for stochastic optimization.ArXiv Preprint\narXiv:1412.6980. (2014)\n25. Hudson, H. , Larkin, R.: Accelerated image reconstruction using ordered subsets\nof projection data.IEEE Transactions on Medical Imaging. 13, 601-609 (1994)\n26. De Pierro, A.: A modiﬁed expectation maximization algorithm for penalized like-\nlihood estimation in emission tomography.IEEE Transactions on Medical Imaging.\n14, 132-137 (1995)\n27. Häggström, I., Schmidtlein, C., Campanella, G. , Fuchs, T.: DeepPET: A deep\nencoder–decoder network for directly solving the PET image reconstruction inverse\nproblem. Medical Image Analysis. 54 pp. 253-262 (2019)\n28. Cocosco, C., Kollokian, V., Kwan, R., Pike, G. , Evans, A.: BrainWeb: Online\nInterface to a 3D MRI Simulated Brain Database.NeuroImage. 5 pp. 425 (1997)\n29. Siddon, R.: Fast calculation of the exact radiological path for a three-dimensional\nCT array.Medical Physics. 12, 252-255 (1985)\n30. Wang, Z., Bovik, A., Sheikh, H. , Simoncelli, E.: Image quality assessment: from\nerror visibility to structural similarity.IEEE Transactions on Image Processing. 13,\n600-612 (2004)",
  "topic": "Residual",
  "concepts": [
    {
      "name": "Residual",
      "score": 0.7776221036911011
    },
    {
      "name": "Computer science",
      "score": 0.7494885325431824
    },
    {
      "name": "Convolutional neural network",
      "score": 0.68462073802948
    },
    {
      "name": "Iterative reconstruction",
      "score": 0.6446791887283325
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6038891673088074
    },
    {
      "name": "Convolution (computer science)",
      "score": 0.5571959614753723
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.5378420352935791
    },
    {
      "name": "Inverse problem",
      "score": 0.4593956172466278
    },
    {
      "name": "Image quality",
      "score": 0.44686853885650635
    },
    {
      "name": "Algorithm",
      "score": 0.42546674609184265
    },
    {
      "name": "Transformer",
      "score": 0.42190998792648315
    },
    {
      "name": "Feature extraction",
      "score": 0.4113519489765167
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4069752097129822
    },
    {
      "name": "Computer vision",
      "score": 0.34399911761283875
    },
    {
      "name": "Artificial neural network",
      "score": 0.31381291151046753
    },
    {
      "name": "Image (mathematics)",
      "score": 0.26296842098236084
    },
    {
      "name": "Mathematics",
      "score": 0.13709479570388794
    },
    {
      "name": "Voltage",
      "score": 0.0848655104637146
    },
    {
      "name": "Physics",
      "score": 0.06357699632644653
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4391767668",
      "name": "State Key Laboratory of Modern Optical Instruments",
      "country": null
    },
    {
      "id": "https://openalex.org/I76130692",
      "name": "Zhejiang University",
      "country": "CN"
    }
  ]
}