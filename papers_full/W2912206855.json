{
  "title": "Extracting Syntactic Trees from Transformer Encoder Self-Attentions",
  "url": "https://openalex.org/W2912206855",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A339531207",
      "name": "David Mareček",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2144311903",
      "name": "Rudolf Rosa",
      "affiliations": [
        "Charles University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2606134370",
    "https://openalex.org/W2579343286",
    "https://openalex.org/W1965680834",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W116785612",
    "https://openalex.org/W2963651521",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2563574619",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963403868"
  ],
  "abstract": "This is a work in progress about extracting the sentence tree structures from the encoder’s self-attention weights, when translating into another language using the Transformer neural network architecture. We visualize the structures and discuss their characteristics with respect to the existing syntactic theories and annotations.",
  "full_text": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 347–349\nBrussels, Belgium, November 1, 2018.c⃝2018 Association for Computational Linguistics\n347\nExtracting Syntactic Trees from Transformer Encoder Self-Attentions\nDavid Mareˇcek and Rudolf Rosa\nInstitute of Formal and Applied Linguistics\nFaculty of Mathematics and Physics\nCharles University, Prague, Czechia\n{marecek,rosa}@ufal.mff.cuni.cz\n1 Introduction\nInterpreting neural networks is a popular topic,\nand there are many works focusing on analyz-\ning networks with respect to learning syntax (Shi\net al., 2016; Linzen et al., 2016; Blevins et al.,\n2018).\nIn particular, Vaswani et al. (2017) showed that\nthe self-attentions in their Transformer architec-\nture may be directly interpreted as syntactic de-\npendencies between tokens. However, there is\na potential problem in the fact that the atten-\ntion mechanism on deeper layers operates on the\nprevious-layer neurons, which already comprise\nmixed information from multiple source tokens.\nOur goal is to infer source sentence tree struc-\ntures form the encoder’s self-attention energies\nused in the Transfomer neural machine translation\n(NMT) system. We would like to visualize how\nthe self-attention mechanism connects individual\nwords (or wordpieces) of the sentence, to cre-\nate various tree structures (e.g. constituency trees,\nundirected trees, dependency trees), and to discuss\ntheir characteristics with respect to the existing\nsyntactic theories and annotations. We would also\nlike to discuss results across various languages and\nnatural language processing (NLP) tasks.\nIn this abstract, we present our preliminary re-\nsults, analyzing the encoder in English-to-German\nNMT within the NeuralMonkey toolkit (Helcl and\nLibovick´y, 2017). We introduce aggregation of\nself-attention through layers to get a distribution\nover the input tokens for each encoder position\nand layer (Section 2). We then propose algorithms\nfor constructing two types of syntactic trees (Sec-\ntions 3 and 4), apply them to 42 sentences sampled\nfrom PennTB (Marcus et al., 1993), and compare\nthe resulting structures to established syntax anno-\ntation styles, such as that of PennTB, UD (Nivre\net al., 2016), or PDT (B¨ohmov´a et al., 2003).\nas_\na_\nresult_\n,_\nthe_\nlink_\nbetween_\nthe_\nfutur\nes_\nand_\nstock_\nmarkets_\nrip\nped_\napart_\n._\nas_\na_\nresult_\n,_\nthe_\nlink_\nbetween_\nthe_\nfutur\nes_\nand_\nstock_\nmarkets_\nrip\nped_\napart_\n._\nFigure 1: Aggregated encoder’s self-attentions after the\n6th layer. Each column contains a distribution over the\nsource wordpieces for one encoder position.\n2 Aggregated self-attention visualization\nWe use the default setting: encoder is composed\nof 6 layers, each consisting of a 16-head self-\nattention mechanism and a fully connected feed-\nforward network, both bridged by residual connec-\ntions. Each position in one layer can attend to all\npositions in the previous layer; the attention to the\nsame position is boosted by the residual connec-\ntion. When translating a single sentence by Trans-\nformer, we would like to capture how much each\ninput token affects each particular position on each\nlayer in the encoder. This is done by aggregating\nthe attention distributions through the layers. For\neach layer, we collect the self-attention distribu-\ntion to the previous layer and add +1 to the same-\nposition attention for the residual connection. The\noutput is then normalized. So far, we take the at-\ntention distribution as the average attention over\nall the 16 heads.\n348\nX\nX\nX\napartripped\nX\nX\nX\nmarketsstock\nand\nX\nX\nfuturesthe\nX\nX\nbetweenlink\nthe\nX\nX\n,result\nX\naas\nAs a result , the link between the futur es and stock market rip ped apart\nx x x\nx\nx x\nx\nx x x x x x x x\nFigure 2: A binary constituency tree and an undirected tree, generated by the proposed algorithms.\n3 Constituency trees extraction\nIn Figure 1, we can see that the self-attention\nmechanism is quite strong within phrases. That\nled us to an idea of extracting phrase-structure\ntrees from that. We deﬁne the score of a con-\nstituent with span from position i to position j as\nscore(i, j) =\n∑\nx∈[i,...,j]\n∑\ny∈[i,...,j] w[x, y]\nj −i + 1 ,\nwhere w[x, y] is the attention weight of the token\ny in the position x. We then build a binary con-\nstituency tree by recurrently splitting the sentence.\nWhen splitting a phrase with span (i, j), we look\nfor a position k maximizing the scores of the two\nresulting phrases:\narg max\nk\n(score(i, k) ·score(k + 1, j)) .\nWe also rejoin wordpieces into words, assigning\nzero scores to constituents separating pieces of a\nsingle word. One example is shown in Figure 2.\nWhen compared to PennTB, clauses, noun\nphrases, or shorter verb phrases are often well rec-\nognized. The differences are mainly inside them 1\nand in composing them together forming clauses.\n4 Undirected trees extraction\nFirst, for each pair of tokens i, j, we calculate a\ncoattention score, expressing how common it is\nfor the tokens to be attended to at the same time:\nscore(i, j) =\n∑\nm∈[1,N−1]\nw[m, i] ·w[m, j]\n1This is also caused by very ﬂat noun phrases representa-\ntions in PennTB compared to our binary branching.\nWe then construct an undirected tree2 maximiz-\ning the coattention scores along its edges, using\nthe algorithm of Kruskal (1956); see the bottom\ntree in Figure 2. We have found the resulting trees\nto bear surprising similarities to standard syntactic\ndependency trees (which, however, are directed).\nFor example, we observe many ﬂat treelets, re-\nsembling headed syntactic phrases; the “phrase\nheads” ( bold) are mostly content words, while\nthe function words are mostly attached as leaf\nnodes (as in UD). We hypothesize that the encoder\ntries to concentrate the representation of the whole\nphrase onto the position of a single token – ideally\none that already carries a lot of meaning.\nFurthermore, the phrase treelets are then typi-\ncally connected to each other via these heads (as\nin UD), and/or via a sort of connector tokens at\nphrase boundaries (underlined), such as commas,\nconjunctions, or prepositions (as in PDT).\n5 Future work\nIn future, we would like to (1) analyze how the\ntrees evolve through layers, (2) employ unsuper-\nvised or supervised selection of “more syntactic”\nheads, and (3) perform the experiments on more\nlanguage pairs; especially, we hope that transla-\ntion into multiple languages could push the en-\ncoder to use a more syntactic internal representa-\ntion.\n2 We also tried to construct directed graphs, (i.e. a stan-\ndard dependency tree), where the edges could be directly\nviewed as dependencies, but we did not ﬁnd any sensible way\nof deﬁning the coattention scores assymetrically; rather than\nparent-child dependencies, many relations seem to be more\ngeneral, without a clear direction.\n349\nReferences\nTerra Blevins, Omer Levy, and Luke Zettlemoyer.\n2018. Deep RNNs encode soft hierarchical syntax.\nIn Proceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 14–19, Melbourne, Australia.\nAssociation for Computational Linguistics.\nAlena B¨ohmov´a, Jan Hajiˇc, Eva Hajiˇcov´a, and Barbora\nHladk´a. 2003. The Prague dependency treebank. In\nTreebanks, pages 103–127. Springer.\nJindˇrich Helcl and Jind ˇrich Libovick ´y. 2017. Neural\nMonkey: An open-source tool for sequence learn-\ning. The Prague Bulletin of Mathematical Linguis-\ntics, (107):5–17.\nJoseph B Kruskal. 1956. On the shortest spanning sub-\ntree of a graph and the traveling salesman problem.\nProceedings of the American Mathematical society,\n7(1):48–50.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. Transactions of the\nAssociation for Computational Linguistics, 4:521–\n535.\nMitchell P. Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a large annotated\ncorpus of English: The Penn treebank. Comput. Lin-\nguist., 19(2):313–330.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Yoav Goldberg, Jan Haji ˇc, Christopher Man-\nning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,\nNatalia Silveira, Reut Tsarfaty, and Daniel Zeman.\n2016. Universal Dependencies v1: A multilingual\ntreebank collection. In Proceedings of the 10th In-\nternational Conference on Language Resources and\nEvaluation (LREC 2016), pages 1659–1666, Por-\ntoro, Slovenia. European Language Resources As-\nsociation.\nXing Shi, Inkit Padhi, and Kevin Knight. 2016. Does\nstring-based neural MT learn source syntax? In\nEMNLP, pages 1526–1534.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 6000–6010. Curran As-\nsociates, Inc.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7951796054840088
    },
    {
      "name": "Computer science",
      "score": 0.7825614213943481
    },
    {
      "name": "Encoder",
      "score": 0.7634214162826538
    },
    {
      "name": "Sentence",
      "score": 0.633594274520874
    },
    {
      "name": "Natural language processing",
      "score": 0.6165086627006531
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5777007937431335
    },
    {
      "name": "Architecture",
      "score": 0.4927563965320587
    },
    {
      "name": "Artificial neural network",
      "score": 0.4770438075065613
    },
    {
      "name": "Engineering",
      "score": 0.11222106218338013
    },
    {
      "name": "Electrical engineering",
      "score": 0.10516357421875
    },
    {
      "name": "Voltage",
      "score": 0.06266424059867859
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I21250087",
      "name": "Charles University",
      "country": "CZ"
    }
  ]
}