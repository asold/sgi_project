{
  "title": "Event Grounded Criminal Court View Generation with Cooperative (Large) Language Models",
  "url": "https://openalex.org/W4394737017",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Yue, Linan",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2108859322",
      "name": "Liu Qi",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A1894482195",
      "name": "Zhao Li-li",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2010922498",
      "name": "Wang Li",
      "affiliations": [
        "Hefei University"
      ]
    },
    {
      "id": "https://openalex.org/A2254487848",
      "name": "Gao, Weibo",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A5077334659",
      "name": "AN Yanqing",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4367628274",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2981133359",
    "https://openalex.org/W4285120549",
    "https://openalex.org/W3034827881",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2973230187",
    "https://openalex.org/W4367000087",
    "https://openalex.org/W4384895072",
    "https://openalex.org/W2740833370",
    "https://openalex.org/W3156716744",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W3156829117",
    "https://openalex.org/W3117435054",
    "https://openalex.org/W3099641295",
    "https://openalex.org/W4385573085",
    "https://openalex.org/W2962940365",
    "https://openalex.org/W4394711241",
    "https://openalex.org/W3156831596",
    "https://openalex.org/W3155791841",
    "https://openalex.org/W4317209756",
    "https://openalex.org/W2890026792",
    "https://openalex.org/W2997429021",
    "https://openalex.org/W3103181983",
    "https://openalex.org/W1593271688"
  ],
  "abstract": "With the development of legal intelligence, Criminal Court View Generation\\nhas attracted much attention as a crucial task of legal intelligence, which\\naims to generate concise and coherent texts that summarize case facts and\\nprovide explanations for verdicts. Existing researches explore the key\\ninformation in case facts to yield the court views. Most of them employ a\\ncoarse-grained approach that partitions the facts into broad segments (e.g.,\\nverdict-related sentences) to make predictions. However, this approach fails to\\ncapture the complex details present in the case facts, such as various criminal\\nelements and legal events. To this end, in this paper, we propose an Event\\nGrounded Generation (EGG) method for criminal court view generation with\\ncooperative (Large) Language Models, which introduces the fine-grained event\\ninformation into the generation. Specifically, we first design a LLMs-based\\nextraction method that can extract events in case facts without massive\\nannotated events. Then, we incorporate the extracted events into court view\\ngeneration by merging case facts and events. Besides, considering the\\ncomputational burden posed by the use of LLMs in the extraction phase of EGG,\\nwe propose a LLMs-free EGG method that can eliminate the requirement for event\\nextraction using LLMs in the inference phase. Extensive experimental results on\\na real-world dataset clearly validate the effectiveness of our proposed method.\\n",
  "full_text": "Event Grounded Criminal Court View Generation with\nCooperative (Large) Language Models\nLinan Yue\nState Key Laboratory of Cognitive\nIntelligence, University of Science and\nTechnology of China\nHefei, China\nlnyue@mail.ustc.edu.cn\nQi Liuâˆ—\nState Key Laboratory of Cognitive\nIntelligence, University of Science and\nTechnology of China & Institute of\nArtificial Intelligence, Hefei\nComprehensive National Science\nCenter\nHefei, China\nqiliuql@ustc.edu.cn\nLili Zhao\nState Key Laboratory of Cognitive\nIntelligence, University of Science and\nTechnology of China\nHefei, China\nliliz@mail.ustc.edu.cn\nLi Wang\nByteDance\nHangzhou, China\nwl063@mail.ustc.edu.cn\nWeibo Gao\nState Key Laboratory of Cognitive\nIntelligence, University of Science and\nTechnology of China\nHefei, China\nweibogao@mail.ustc.edu.cn\nYanqing An\nState Key Laboratory of Cognitive\nIntelligence, University of Science and\nTechnology of China\nHefei, China\nanyq@mail.ustc.edu.cn\nABSTRACT\nWith the development of legal intelligence, Criminal Court View\nGeneration has attracted much attention as a crucial task of legal\nintelligence, which aims to generate concise and coherent texts that\nsummarize case facts and provide explanations for verdicts. Exist-\ning researches explore the key information in case facts to yield\nthe court views. Most of them employ a coarse-grained approach\nthat partitions the facts into broad segments (e.g., verdict-related\nsentences) to make predictions. However, this approach fails to cap-\nture the complex details present in the case facts, such as various\ncriminal elements and legal events. To this end, in this paper, we\npropose an Event Grounded Generation (EGG) method for criminal\ncourt view generation with cooperative (Large) Language Models,\nwhich introduces the fine-grained event information into the gener-\nation. Specifically, we first design a LLMs-based extraction method\nthat can extract events in case facts without massive annotated\nevents. Then, we incorporate the extracted events into court view\ngeneration by merging case facts and events. Besides, considering\nthe computational burden posed by the use of LLMs in the extrac-\ntion phase of EGG, we propose a LLMs-free EGG method that can\neliminate the requirement for event extraction using LLMs in the\ninference phase. Extensive experimental results on a real-world\ndataset clearly validate the effectiveness of our proposed method.\nCode is available at https://github.com/yuelinan/Codes-of-EGG.\nâˆ—Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0431-4/24/07\nhttps://doi.org/10.1145/3626772.3657698\nCCS CONCEPTS\nâ€¢ Applied computing â†’Law.\nKEYWORDS\nCourt View Generation, Event Extraction, Large Language Model\nACM Reference Format:\nLinan Yue, Qi Liu, Lili Zhao, Li Wang, Weibo Gao, and Yanqing An. 2024.\nEvent Grounded Criminal Court View Generation with Cooperative (Large)\nLanguage Models. In Proceedings of the 47th International ACM SIGIR Con-\nference on Research and Development in Information Retrieval (SIGIR â€™24),\nJuly 14â€“18, 2024, Washington, DC, USA. ACM, New York, NY, USA, 10 pages.\nhttps://doi.org/10.1145/3626772.3657698\n1 INTRODUCTION\nThe remarkable success of deep neural networks has stimulated\nthe exploration of legal intelligence applications [18, 20, 34, 42â€“44].\nAmong these applications, Criminal Court View Generation [36, 40]\nhas garnered increasing attention as a foundational facet of legal\nintelligence. As depicted in Figure 1(a), the objective of criminal\ncourt view generation is to produce a coherent text, referred to as\na court view, which serves as a concise representation of the case\nfacts and offers an explanation for the rendered verdicts, such as\ncharges and sentencing. The automated generation of court views\nhas the potential to alleviate the workload of legal professionals\nwhile providing legal assistance to laymen [34, 40].\nThe existing approaches in the field can be categorized into\ntwo groups: domain-specific models [ 11, 36] and large language\nmodels (LLMs) [23, 30]. Several domain-specific models [ 34, 40]\ncommonly generate court views by leveraging key information\n(e.g., crime circumstances [38â€“40]) extracted from the case facts\nusing legal knowledge. For instance, C3VG [40], a court view gen-\neration model that has demonstrated promising results, explicitly\ncategorizes crime circumstances in the case facts into two broad\ntypes: verdict-related circumstances and sentencing-related ones.\narXiv:2404.07001v3  [cs.CL]  16 Apr 2024\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Linan Yue, Qi Liu, Lili Zhao, Li Wang, Weibo Gao, and Yanqing An\n\u0002\u0002\nFact description: It was found that Bob and Tom \nhad a verbal altercation while chatting online. \nThen, Bob and Tom both led more than ten people \nto meet at a park and then exchanged blows. \nDuring the fight, Bob was armed with an iron \nspanner and injured Tomâ€¦ \nIt was also found that Bob compensated Tom's \neconomic loss of $10,000 and obtained Tom's \nunderstanding.\nCourt view: Our court hold that Bob and Tom had \na verbal dispute and gathered people to fight. In the \ncourse of the fight, Bob injured Tom with an iron \nspanner, which constituted the crime of gathering a \ncrowd for a fight. It was also found that Bob \nactively compensated Tom for his losses and \nobtained his understanding, which was a mitigating \npunishment according to the law.\nCoarse-grained Verdict-Sentencing Information(a) The overview of C3VG\n(b) The overview of EGG\nLLMs-based \nExtractor\nVerdicts-related: It was found that Bob and Tom had a \nverbal altercation while chatting online. Then, Bob and \nTom both led more than ten people to meet at a park and \nthen exchanged blows. During the fight, Bob was armed \nwith an iron spanner and injured Tommy\u0001\nSentencing-related: \nIt was also found that Bob compensated Tom's economic \nloss of $10,000 and obtained Tom's understanding.\nFact description: It was found that Bob and Tom \nhad a verbal altercation while chatting online . \nThen, Bob and Tom both led more than ten people \nto meet at a park and then exchanged blows. \nDuring the fight, Bob was armed with an iron \nspanner and injured Tomâ€¦ \nIt was also found that Bob compensated Tom's \neconomic loss of $10,000 and obtained Tom's \nunderstanding.\nEvent-related Question1: \nWhat is the cause of the crime? \nAnswer:\nBob and Tom had a verbal altercation while chatting online\nEvent:\nThe cause of the crime is Bob and Tom had a verbal \naltercation while chatting online.\nFine-grained Events Information\nEvent-related Question2: \nWhat is the amount of compensation? \nAnswer: \n$10,000 \nEvent: \nThe amount of compensation is $10,000.\nExtractor\nGenerator\nCourt view:  Our court hold that Bob and Tom \nengaged in a verbal dispute and gathered people to \nfight. During the fight, Bob injured Tom with an \niron spanner. However, the court also found that \nBob actively compensated Tom for his losses and \nobtained his understanding, which was considered a \nmitigating factor according to the law.\nPLMs-based \nGenerator\nFigure 1: Schematic of C3VG and our method EGG presented in this paper.\nSubsequently, it employs pre-trained language models (PLMs) (e.g.,\nBART1 [13]) to generate court views based on these two types of\ninformation. Nevertheless, the components comprising the case\nfacts are highly intricate. As illustrated in Figure 1(b), case facts en-\ncompass various criminal elements2 (i.e., legal events), represented\nby the underlined tokens in the fact description. Consequently,\nthe adoption of a coarse-grained domain-specific approach that\npartitions the facts into two segments proves to be inadequate.\nFurthermore, considering that court view generation is essen-\ntially a text generation task, it is plausible to fine-tune LLMs [37]\n(e.g., Baichuan-7B [2]) for court view generation. However, as evi-\ndent from the experimental findings presented in Table 2, simple\nfine-tuning of LLMs does not yield satisfactory results. This could\nbe attributed to the intricacy of the fact descriptions, necessitat-\ning the incorporation of additional legal knowledge. In this regard,\na straightforward approach is to substitute PLMs with LLMs in\ndomain-specific models. Nonetheless, domain-specific models of-\nten involve the collaborative training of multiple PLMs, which poses\na significant computational burden on LLMs.\nTo this end, in this paper, we aim to develop a method which\nincorporates fined-grained event information into the court view\ngeneration by leveraging the collaboration between LLMs and PLMs\nin domain-specific models. The overview of our proposed method is\npresent in Figure 1(b) and is two-fold: (1) extracting the fine-grained\nevent of the case fact and (2) generating court views based on the\nidentified events.\nHowever, it is a non-trivial problem. Although available legal\nevent extraction datasets contain substantial annotated data [7, 35],\nthey primarily focus on annotating which information belongs to\nevents in each legal document within specific case types. This ap-\nproach not only necessitates extensive professional effort but also\nrequires re-annotation of vast amounts of legal documents when\nencountering new case types, thereby serving as a major bottleneck\nfor practical applications of legal event extraction. Therefore, it is\ncrucial to devise a strategy that can extract events with minimal hu-\nman annotation and demonstrate good generalization capabilities\nacross different case types.\n1https://github.com/yuelinan/C3VG/tree/main/bart_based_c3vg\n2https://en.wikipedia.org/wiki/Element_(criminal_law)\nTo tackle the challenge mentioned above, we propose an Event\nGrounded Generation (EGG) method for criminal court view gen-\neration with Cooperative (Large) Language Models following an\nextract-generate framework:\nâ€¢In the extraction phase , we design aLLMs-based event extrac-\ntor. Specifically, we first fine-tune LLMs with the publicly available\nlegal QA dataset CJRC [6] (an extractive QA dataset like SQuAD\n[25]). This fine-tuning process enables the LLMs to extract perti-\nnent answers from the original text based on a given legal question\n(i.e., the prompt). After the extractor is trained, we label each case\ntype with several event-related questions. For example, as shown\nin Figure 1(b), for a case type ofMobbing, we label the event-related\nquestions (e.g. â€œWhat is the cause of the crime? â€ and â€œWhat is the\namount of compensation? â€.) Importantly, we label these questions\nonly for the case type itself and not for individual case facts. When\ndealing with a specific case fact related to the crime ofMobbing, we\nutilize the pre-defined event-related questions for theMobbing case\ntype. By prompting the trained LLMs-based event extractor with\nthe labeled questions, we extract events for each question based on\nthe given case fact. It is worth noting that our labeled event-related\nquestions are not present in the CJRC dataset, thus making our\nextraction method a zero-shot event extraction approach. Next,\nwe combine the question and answer to get the events (e.g., the\nfine-grained events information in Figure 1(b)). In summary, this\napproach only necessitates the annotation of relevant questions\nfor each case type, with an average of 9 questions per case type. In\ncomparison to previous methods, our proposed event extraction\napproach significantly reduces the annotation time required.\nâ€¢In the generation phase , we splice the facts and events to-\ngether to form a new text input, which is then fed into the PLMs-\nbased generator to yield the court views.\nAdditionally, taking into consideration the computational burden\nposed by the use of LLMs in the extraction phase of EGG, we\nrecognize the need to enhance its practical applicability for both\nlaymen and professionals. To address this, we propose an LLMs-\nfree EGG method, referred to as EGGğ‘“ğ‘Ÿğ‘’ğ‘’ , which eliminates the\nrequirement of events during the inference phase. Specifically, in\nthe training process, we still employ LLMs to extract events in\nthe extraction phase. However, in the generation phase, instead of\nEvent Grounded Criminal Court View Generation with Cooperative (Large) Language Models SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nCJRC\nParagraph: After hearing, it was found that the defendant Bob had an argument with \nTommy in front of the hotel due to the problem of moving the car. Tommy fought with \nBob and was injured during the fight, and died that night after an ineffective \nresuscitation. YES/NO\nQuestion1: Why the defendant Bob had an argument with Tommy?\nAnswer1: due to the problem of moving the car\nQuestion2: Whether the defendant is dead?\nAnswer2: YES\n(a)\nInstruction Input\u0002\u0001Assuming you are a judge, please answer the [Question1] â€œWhy the \ndefendant Bob had an argument with Tommy?â€ based on the following facts of the case. \nPlease note that the answer must be extracted from the facts of the case, which are \nprovided below:\n[Paragraph] After hearing, it was found that the defendant Bob had an argument with \nTommy in front of the hotel due to the problem of moving the car. Tommy fought with \nBob and was injured during the fight, and died that night after an ineffective \nresuscitation. YES/NO\nInstruction Output: due to the problem of moving the car\nInstruction1\n(b)\nInstruction2\nInstruction Input\u0002\u0001Assuming you are a judge, please answer the [Question2] â€œWhether \nthe defendant is dead?â€ based on the following facts of the case. Please note that the \nanswer must be extracted from the facts of the case, which are provided below:\n[Paragraph] After hearing, it was found that the defendant Bob had an argument with \nTommy in front of the hotel due to the problem of moving the car. Tommy fought with \nBob and was injured during the fight, and died that night after an ineffective \nresuscitation. YES/NO\nInstruction Output: YES\n(c)\nFigure 2: (a). An example from the CJRC dataset, including\ntwo questions. (b). An example from the processed instruc-\ntion dataset based on the Question1 in (a). (c). An example\nfrom the processed instruction dataset based on the Ques-\ntion2 in (a).\nmerging the event and fact as input to the generator, we leverage\nthe event as auxiliary information to assist the model in generating\nthe court view based solely on the fact. To achieve this, we encode\nthe fact and event separately using the fact and event encoders.\nSubsequently, we design a contrastive learning module to facilitate\nthe fact encoder in capturing co-occurrence signals with the event\nthrough contrastive constraints. Finally, we generate the court view\nbased on the fact. Importantly, during the inference phase,EGGğ‘“ğ‘Ÿğ‘’ğ‘’\nno longer requires any event information. It solely relies on the\ncase fact to generate the court view without the need for event\nextraction. This modification aims to improve the practicality and\nusability of the EGG method for legal professionals and individuals\nwithout legal expertise.\nIn summary, the major contributions of this paper are:\nâ€¢We propose an Event Grounded Generation (EGG) method for\ncriminal court view generation with cooperative (large) language\nmodels, which first introduces the fine-grained event information\ninto the court view generation.\nâ€¢We propose a low data resource approach to achieve a zero-shot\nlegal event extraction with LLMs.\nâ€¢To alleviate the computational burden in EGG during inference\nthat employs LLMs, we propose a LLMs-free EGG method based\non the contrastive constraint.\nâ€¢Extensive experiments on a real-world dataset validate the effec-\ntiveness of our method by comparing it with several competi-\ntive methods.\n2 RELATED WORK\nCourt View Generation. The remarkable success in neural net-\nworks provokes the legal intelligence [3, 15, 21, 27, 43, 44]. Among\nthem, court view generation has achieved increasing attention\n[36, 40]. Specifically, [36] were the first to formulate the task of\ncourt view generation and explored the use of charges to enhance\nthe generation process, allowing the model to focus on verdict-\nrelated information within the case facts. [ 11] proposed a court\nview generation approach that involved masking key tokens in a\ntemplate and subsequently employing a question-answering (QA)\nmethod to fill in these masked tokens. [34] integrated legal judg-\nment prediction with court view generation, enabling the simultane-\nous generation of judgment results and court views. [40] designed\nan extract-generate framework that categorized case facts into two\ntypes, namely verdict-related and sentencing-related information,\nusing an extractor. The generated court views were then based on\nthe extracted information. Despite the promising results achieved\nby these existing methods, they have overlooked the incorporation\nof fine-grained event information present in case facts. This limita-\ntion highlights the need to consider and leverage event information\nfor more comprehensive and accurate court view generation.\nLarge Language Model in Legal AI. Large Language Models\n(LLMs) such as ChatGPT [23] and LLaMA [30] have exhibited im-\npressive performance across various complex tasks and have made\na significant impact on society. In the realm of legal AI, researchers\nhave been combining LLMs with legal tasks [8, 32, 41]. One notable\nexample is Lawyer LLaMA [10], which underwent continual pre-\ntraining on an extensive legal corpus to systematically acquire legal\nknowledge. The model was then fine-tuned using legal instruction\ndata, enabling it to apply its legal knowledge to specific scenarios.\nThis approach leverages the power of LLMs to enhance the effec-\ntiveness of legal AI tasks. Another approach, ChatLaw [5], explored\nthe use of larger base models to improve the logical reasoning\ncapabilities of legal models. By leveraging the increased capacity\nand capabilities of larger models, ChatLaw aimed to enhance the\nmodelâ€™s ability to perform complex legal reasoning tasks. Privacy\nconcerns in the legal domain are addressed by FedJudge [37], which\nadopts Federated Learning during the instruction tuning process.\nThis approach ensures the privacy of legal data by training the\nmodel on local devices and only sharing aggregated updates, rather\nthan sharing raw data. In this paper, the focus is specifically on\nutilizing LLMs to achieve legal event extraction.\nLegal Event Extraction. In the field of legal event extraction,\nnumerous studies [7, 14, 16, 28, 35] have involved annotating legal\nevent types for each legal document. Notably, [35] have annotated\nover 8,000 legal documents with 108 event types. However, this\nmanual annotation process is labor-intensive and time-consuming.\nFurthermore, when encountering new legal event types, it becomes\nnecessary to label additional data, making existing datasets less\nreliable. It is crucial to develop an event extraction method that\nminimizes the reliance on extensive manual annotation.\n3 EVENT GROUNDED GENERATION FOR\nCRIMINAL COURT VIEW\n3.1 Problem Definition\nHere, we explore the problem of criminal court view generation.\nWe first clarify the definitions of the terms as follows:\nFact description ğ‘¥ = {ğ‘¥1,ğ‘¥2,...,ğ‘¥ ğ‘›}is the identified facts in a\ncase including several events, where ğ‘¥ğ‘– denotes the i-th token.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Linan Yue, Qi Liu, Lili Zhao, Li Wang, Weibo Gao, and Yanqing An\nIt was found that on the evening of 23 June 2015, Bob and\nTommy had a verbal altercation while chatting online. At\naround 12:00 the following day, Bob and Tommy both led more\nthan ten people to meet at a park and then exchanged blows.\nDuring the fight, Bob was armed with an iron spanner and\ninjured Tommy, who was involved in the fight. It was\nforensically determined that Tommy's injuries were of the first\ndegree of minor injury. It was also found that Bob's family\ncompensated Tommy's economic loss of $10,000 and obtained\nTommy's understanding.\n \n Whether the defendant obtained the victim's understanding?\nWhat was the defendant's instrument of injury to the victim?\nWhat was the cause of the defendant's fight?\nWhat kind of injuries did the defendant cause?\nHow much money did the defendant pay the victim?\nWhether the defendant surrendered himself?\n1\n2\n3\n4\n5\n6\nAnnotated questions\nThe input Answers\n \nThe defendant obtained the victim's understanding\nThe tool of the crime is an iron spanner\nCause of crime is Bob and Tommy had a verbal\naltercation while chatting online\nInjuries is the first degree of minor injury\nAmount of compensation is $10,000\nThe defendant did not surrender.\n1\n2\n3\n4\n5\n6\n \nYes.\nan iron spanner\nBob and Tommy had a verbal altercation while\nchatting online\nthe first degree of minor injury\n$10,000\nNo.\n1\n2\n3\n4\n5\n6\nEvent\nFigure 3: The process of event extraction. Among them, the input ğ‘¥ is a legal document in the â€œcrime of Affrayâ€ case type, and\nthe questions ğ‘â€²is annotated for the â€œcrime of Affrayâ€ case type (not only for the input ğ‘¥).\nEvent set ğ‘’ = {ğ‘’1,ğ‘’2,...,ğ‘’ ğ‘˜}consists of ğ‘˜ events of the fact,\nwhere ğ‘’ğ‘– =\n\b\nğ‘’1\nğ‘–,ğ‘’2\nğ‘–,...,ğ‘’ ğ‘š\nğ‘–\n\t\ncontains ğ‘štokens and each event is a\nsubsequence of the fact.\nCourt view is the summary of the fact which consists of the\ncharge ğ‘ and rationales ğ‘Ÿ = {ğ‘Ÿ1,ğ‘Ÿ2,...,ğ‘Ÿ ğ‘¡}. Among them, the ratio-\nnale is concluded from the fact in order to determine and support\nthe judgment results, such as sentencing. In this work, we assume\nthe charge is available, and we only focus on generating rationales\nin court views, where the charge can be easily obtained by the judge\nor the charge prediction systems [39, 42, 43].\nThen, based on the above definitions, our problem is defined as:\nProblem 1 (Court View Generation). Given the case fact ğ‘¥, our\ngoal is first to extract several events ğ‘’ from the case fact, and then\ngenerate the rationales ğ‘Ÿ in court views, where the gold events are\nunavailable.\n3.2 Architecture of EGG\nOur proposed Event Grounded Generation (EGG) for criminal court\nview method consists of two phases, cascading the event extractor\nand the court view generator . Specifically, in the extraction phase, we\nfirst train a LLMs-based QA model which can extract a subsequence\nof the text input as the answer to the prompts (or questions). After\nthe model is trained, we consider this model as the event extractor\nto select several events from the case fact by introducing annotated\nlegal event-related questions. Finally, we employ a PLMs-based\ncourt view generator to generate court views by merging the fact\nand event as the new text input.\n3.2.1 Event extractor. Existing legal event extraction datasets\n[7, 35] mainly focus on annotating each case under different case\ntypes (i.e., different charges). However, this annotation requires\nsignificant and expensive professional labor. Meanwhile, when\nfacing a new case type, it commonly needs to be re-labeled. To this\nend, we develop a zero-shot LLMs-based legal event extractor.\nSpecifically, we implement the extractor with a publicly avail-\nable legal QA dataset CJRC [6], which consists of the paragraph ğ‘,\nquestion ğ‘and answer ğ‘as shown in Figure 2(a). Among them, the\nanswer ğ‘is a part of paragraph ğ‘. Besides, to answer the question\nabout YES or NO, CJRC adds â€œYES/NOâ€ at the end of the paragraph.\nBased on CJRC, we train a legal LLMs-based model Mto extract\nanswers from the paragraph according to the questions.\nIn detail, we begin by transforming the original CJRC dataset into\nan instruction dataset [29], denoted as D, where each instruction\ndata has the form of {ğ¼ğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡ : ğ¼ğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘‚ğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ }. Fig-\nure 2(b) and Figure 2(c) illustrate the specific format of the prompt,\ntask-specific instruction, and ground truth in our instruction dataset.\nNext, we utilize the instruction tuning method to fine-tune the base\ngenerative LLMs to extract answers from paragraphs. To address\nthe computational and time constraints associated with directly\nfine-tuning the entire LLM, we employ the parameter-efficient fine-\ntuning technique for training the extractor. Specifically, we employ\nthe LoRA [9] method which involves freezing the pre-trained model\nparameters and introducing trainable rank decomposition matrices\ninto each layer of the Transformer architecture [31]. Finally, the\nlearning objective can be computed as:\nLğ‘’ = âˆ’\n|ğ‘¦|âˆ‘ï¸\nğ‘¡=1\nlog \u0000ğ‘ƒÎ˜+Î˜ğ¿ (ğ‘¦ğ‘¡ |ğ‘š,ğ‘¦<ğ‘¡)\u0001 , (1)\nwhereğ‘šandğ‘¦represent theInstruction Input and Instruction Output,\nğ‘¦ğ‘¡ denotes the ğ‘¡-th token of ğ‘¦, ğ‘¦<ğ‘¡ is the tokens before ğ‘¦ğ‘¡, and Î˜\nrepresents the frozen LLMs parameters and Î˜ğ¿ is the trainable\nLoRA parameters (Î˜ğ¿ â‰ªÎ˜).\nAfter the LLMs-based extractor is trained, we annotate several\nquestions for each case type, where each question ğ‘â€²is related to\nthe event in case facts ğ‘¥. It is important to note that we label the\nquestions only for the case type and not for each individual case fact.\nFor instance, if we have a case fact related to the crime ofAffray, we\nutilize the previously labeled questions forAffray, such as the cause\nof the crime, tools of the crime, and whether to surrender. Then, as\nshown in Figure 3, we promote the trained LLMMto answer these\nevent-related questions and obtain the corresponding answers ğ‘â€²\n(i.e., ğ‘â€²= M(ğ‘¥,ğ‘â€²)). Finally, we combine the obtained answersğ‘â€²\nand the corresponding questionsğ‘â€²to obtain the eventğ‘’. It is worth\nemphasizing that the labeled event-related questions used in this\nprocess are not present in the CJRC dataset. This characteristic\ndistinguishes our extraction method as a zero-shot event extraction\napproach, as it successfully extracts event information from case\nEvent Grounded Criminal Court View Generation with Cooperative (Large) Language Models SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\ndecoderencoder\nBART\nencoder\nActivate for both \ntesting and training\nActivate only  \nfor training\nFigure 4: Architecture of EGG free.\nfacts using questions that were not available in the training data.\nThis zero-shot capability increases the versatility and adaptability\nof our approach to handle new or unseen case types.\n3.2.2 Court view generator. Previous models generate court\nviews based solely on case fact. In this section, ourcourt view gener-\nator designs a strategy to incorporate extracted event information\ninto the fact to yield more plausible court views, where we adopt\nthe BART [13] as our backbone by considering the advantages of\nthe current PLMs. Specifically, we merge the event and fact descrip-\ntions to form new input ğ‘¥â€²of the court view generator . In practice,\nlimited by the maximum length of the PLMs, we enforce the events\nto be placed before facts (i.e., ğ‘¥â€²= ğ‘’|||ğ‘¥), where â€œ|||â€ represents the\nprocess of mergers.\n3.3 Training and Inference\nIn this section, we describe the training loss in our proposed method\nEGG. Specifically, in the extraction phase, we employ Eq(1) to train\nour LLMs-based event extractor. In the generation phase, we adopt\nthe negative log-likelihood loss to optimize the generator:\nLğ‘Ÿ = âˆ’\n|ğ‘Ÿ|âˆ‘ï¸\nğ‘¡=1\nlog \u0000ğ‘ƒÎ˜ğµ\n\u0000ğ‘Ÿğ‘¡ |ğ‘¥â€²,ğ‘Ÿ<ğ‘¡\n\u0001\u0001 , (2)\nwhere Î˜ğµ is the trainable BART parameters, ğ‘Ÿğ‘¡ denotes the ğ‘¡-th\ntoken of ğ‘Ÿ and ğ‘Ÿ<ğ‘¡ is the tokens before ğ‘Ÿğ‘¡.\nDuring the inference phase, given a description of case fact, we\nfirst use the LLMs-based event extractor to extract the events from\nthe case fact. Then, we generate the court view based on both the\nfacts and events.\n4 EGG free: LLMS-FREE EGG WITH\nCONTRASTIVE CONSTRAINTS\nIndeed, the use of LLMs for event extraction in the extraction phase\nof EGG can lead to increased computational burden during the\ninference phase. This limitation hampers the practical application\nof the model in real-world scenarios. To overcome this challenge,\nwe propose an LLMs-free EGG method that employs contrastive\nconstraints, enabling court view generation without the need for\nevent information during the inference phase.\n4.1 Architecture of EGG free\nDuring training, EGGğ‘“ğ‘Ÿğ‘’ğ‘’ follows the extractor-generator frame-\nwork. Specifically, in the extraction phase, similar to EGG, we still\nemploy LLM to extract events. In the generation phase, unlike the\nprevious EGG of combining event and fact as inputs to the gen-\nerator, we use event as a kind of auxiliary information to assist\nthe model in generating court views based on fact. In particular, as\nshown in Figure 4, given the fact and event, we first employ the fact\nencoder and event encoder to encode both fact ğ‘¥ and event ğ‘’as the\ncorresponding representations â„ğ‘¥ âˆˆRğ‘‘ and â„ğ‘’ âˆˆRğ‘‘, where ğ‘‘ is\nthe dimensional size. Then, we feed the fact representation into the\ndecoder for court view generation. In practice, we use the encoder\nand decoder of BART to achieve the above implementation.\nSubsequently, to enable the fusion of event information into\nEGGğ‘“ğ‘Ÿğ‘’ğ‘’ , we employ a novel contrastive learning strategy during\nthe training phase. This strategy aims to teach the fact encoder to\nmemorize the co-occurrence event signals within its parameters,\nallowing the fact encoder to inject event clues into fact representa-\ntions during the inference phase.\nIn particular, during the training phase, as shown in Figure 4, we\nadjust the parameters of the fact encoder based on the event encoder\nto maximize the mutual information between the case fact and\nevent. To achieve this objective, for a training fact representation\nâ„ğ‘¥, we build its positive sample set using its corresponding eventâ„ğ‘’\n(referred to â„+ğ‘’), i.e., N+= {â„+ğ‘’}, and its negative sample set Nâˆ’=\nNğ‘ğ‘ğ‘¡ğ‘â„\\N+where Nğ‘ğ‘ğ‘¡ğ‘â„ denotes each batch of event samples.\nTo teach the fact encoder to memorize the co-occurrence event\nsignals, we define the contrastive loss following the concept of\nInfoNCE [22]. The contrastive loss is formulated as:\nLğ‘ = âˆ’Eâ„+ğ‘’âˆˆN+\nï£®ï£¯ï£¯ï£¯ï£¯ï£°\nlog\nexp (sim(â„ğ‘¥,â„+\nğ‘’)\nğœ )\nexp (sim(â„ğ‘¥,â„+ğ‘’)\nğœ )+Ã\nâ„âˆ’ğ‘’ âˆˆNâˆ’exp (sim(â„ğ‘¥,â„âˆ’ğ‘’ )\nğœ )\nï£¹ï£ºï£ºï£ºï£ºï£»\n,\n(3)\nwhere sim(â„ğ‘¥,â„ğ‘’)represents the similarity measure between the\nfact representation â„ğ‘¥ and the event representation â„ğ‘’, and ğœ is a\ntemperature parameter that controls the sharpness of the proba-\nbility distribution. Besides, we set the fact encoder and the event\nencoder to share parameters to save GPU memory. According to\nour experiments, separate encoders and shared encoders do not\nhave a significant difference in the generation performance.\n4.2 Training and Inference\nIn the training process, since EGGğ‘“ğ‘Ÿğ‘’ğ‘’ uses the same extractor as\nEGG, we use Eq(1) to train the extractor. Besides, the final objective\nof the generator in EGGğ‘“ğ‘Ÿğ‘’ğ‘’ is defined as:\nLEGGğ‘“ğ‘Ÿğ‘’ğ‘’ = Lğ‘Ÿ +ğ›½Lğ‘, (4)\nwhere ğ›½ is the adjusted hyperparameter.\nDuring the inference phase, since the fact encoder learns to\ncapture co-occurrence signals with the event through contrastive\nconstraints, EGGğ‘“ğ‘Ÿğ‘’ğ‘’ can ignore the event as the input, enabling\nthe generation of contextually relevant court views based solely on\nthe case fact.\nBy leveraging contrastive constraints, our proposed method elim-\ninates the reliance on LLMs for event extraction in the inference\nphase. This approach significantly reduces the computational bur-\nden, making the model more suitable for real-world applications.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Linan Yue, Qi Liu, Lili Zhao, Li Wang, Weibo Gao, and Yanqing An\nTable 1: The statistics of datasets.\nCJO Results\n# Sample 62,939\n# Types of cases 62\n# Avg. Length of fact description 458.1\n# Avg. Length of court view description 130.9\n# Avg. Annotated questions of event 9.0\n# Avg. Length of annotated questions of event 10.8\n# Avg. Length of all events in a case 83.4\nCJRC Results\n# Sample 20,000\n# Avg. Length of paragraph 501.8\n# Avg. Length of question 16.5\n5 EXPERIMENTS\nTo evaluate the effectiveness of EGG, we conduct experiments to\nanswer the following research questions:\nâ€¢RQ1: How effective are EGG and EGGğ‘“ğ‘Ÿğ‘’ğ‘’ in improving the\nperformance of event extraction and court view generation?\nâ€¢RQ2: How efficient is EGGğ‘“ğ‘Ÿğ‘’ğ‘’ during the inference phase?\nâ€¢RQ3: What are the performances of EGG by the length of court\nviews?\nâ€¢RQ4: How do EGG and EGGğ‘“ğ‘Ÿğ‘’ğ‘’ perform in human evaluation?\nâ€¢RQ5: What is the court view generated by EGG to a specific\ncase fact?\n5.1 Datasets\nIn the extraction phase, we adopt the criminal cases in CJRC3 [6]\nas the training data, where we process CJRC into the format of\nan instruction dataset. Figure 2 is an example from the CJRC and\nthe instruction dataset. In the generation phase, following [40], we\nconduct experiments on CJO 4, where CJO is collected from the\npublished legal documents in China Judgments Online5. Detailed\ndataset statistics are shown in Table 1. Among them, since there ex-\nist 62 types of cases, we ask three law expects to annotate questions\nfor each case type, for a total of 558 questions.\n5.2 Experimental Setup\nIn this section, we present the detailed experimental setup of our\nproposed EGG. First, in the extraction phase, we adopt Baichuan-7B\n[2] as the backbone of the LLMs-based event extractor M. Then,\nwe employ the LoRA to parameter-efficient fine-tune it on the\ninstruction dataset. For training, we adopt an AdamW optimizer\n[19] with an initial learning rate of 1e-5, then we set the maximum\nsequence length as 512 and the batch size as 4. Besides, the rank\nof LoRA is set to 4. In the generation phase, we employ BART [13]\nto generate the court views. We set the learning rate to 1e-4 and\nthe batch size to 8, and ğ›½in EGGğ‘“ğ‘Ÿğ‘’ğ‘’ to 1. For evaluation, we adopt\nmacro-average F1 as our metric to evaluate the performance of\nthe LLMs-based event extractor Min the test set of CJRC. Besides,\nsince there exist no gold events in CJO, we assume that the better\nthe generated court views perform, the more effective events are\nextracted. To this end, to evaluate the performance of the generation,\n3https://github.com/china-ai-law-challenge/CAIL2019\n4https://github.com/bigdata-ustc/C3VG\n5https://wenshu.court.gov.cn\nwe adopt ROUGE [17] and BLEU [24] as the metrics. Among them,\nwe report F1 scores of ROUGE-1, ROUGE-2, and ROUGE-L, and we\nkeep the result of BLEU-1, BLEU-2 and BLEU-N (i.e., an average\nscore of BLEU-1, BLEU2, BLEU-3, and BLEU-4).\n5.3 Comparison methods\nIn this section, to evaluate the generated court view, we employ\nthree type of baselines. First, we compare EGG with several tradi-\ntional baselines:\nâ€¢AttS2S [1] is an attention-based sequence-to-sequence model,\nfollowing an encoder-decoder framework.\nâ€¢PGN [26] employs a pointer network to solve the out of vocabu-\nlary (OOV) problem in the text generation.\nâ€¢Transformer [31] has been widely implemented to generate\ntexts.\nâ€¢Label-AttS2S [36] is designed to generate court views by intro-\nducing the charge semantics into AttS2S.\nâ€¢C3VG [40] separates the case fact into two parts with an extract-\ngenerate framework to generate the court views.\nThe above baselines are implemented with GRU [ 4] or trans-\nformer. For a fair comparison, the results of the above baselines are\ndirectly taken from [40].\nBesides, since the pre-training models have promoted the text\ngeneration in recent years, we introduce several approaches based\non the pre-training models:\nâ€¢BART [13] is a Transformer-based pre-training sequence-to-\nsequence model, which achieves promising results in text gen-\neration. In this paper, BART(Fact) denotes BART takes the case\nfact as the input. BART(Event) represents taking the extracted\nevent as the input.\nâ€¢C3VG with BART [40] implements C3VG with BART as the\nbackbone.\nFinally, we also compare LLMs baselines with EGG:\nâ€¢Baichuan-7B [2] is a large language model which achieves com-\npetitive results in Chinese intelligence tasks.\nâ€¢Baichuan-7B(Fact) employs LoRA to fine-tune Baichuan-7B by\ntaking the case fact as the input with the form of the instruction\ndataset. Among them, the Instruction Input is: â€œAssuming you are\na judge, please summarize the facts of the case: [the description\nof case facts]â€, and the Instruction Output is the court views.\n5.4 Performance on Event Extraction and Court\nView Generation (RQ1)\n5.4.1 Results of event extraction. In this section, we report the\nmacro-average F1 to evaluate the performance of the LLMs-based\nextraction model Min the test set of CJRC. After statistics, the\nmacro-average F1 is 84.6 which performs better than the original\nmacro-average F1 (82.9) reported in the paper of CJRC [6] which em-\nploys BERT [12] to achieve the answer extraction. This observation\ndemonstrates the effectiveness of LLMs on extraction. However,\nour goal is employing the trained Mto predict the potential events\nin our court view data CJO. Therefore, the F1 scores in the test set of\nCJRC fail to illustrate the effectiveness of our extraction sufficiently.\nTo further evaluate the extracted events, we consider that the better\nEvent Grounded Criminal Court View Generation with Cooperative (Large) Language Models SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nTable 2: Results of court view generation. â€œ*â€: results obtained from C3VG [40].\nModels ROUGE (â†‘) BLEU ( â†‘) Bert-S (â†‘)R-1 R-2 R-L B-1 B-2 B-N\nAttS2Sâˆ— 58.7 38.9 59.4 50.5 41.0 38.0 â€“\nPGNâˆ— 59.3 37.0 59.8 50.2 39.6 36.7 â€“\nTransformerâˆ— 59.9 39.6 60.9 50.8 41.3 38.1 â€“\nLabel-AttS2Sâˆ— 47.0 31.4 52.8 38.7 31.6 29.4 â€“\nC3VGâˆ— 60.1 40.5 62.5 52.1 43.5 40.6 â€“\nBART(Event) 66.04 50.46 54.01 52.16 47.95 46.76 81.93BART(Fact) 74.96 58.34 62.44 54.94 52.55 51.46 85.59C3VG with BART 75.59 64.22 65.11 56.16 53.58 52.71 85.61\nBaichuan-7B 57.43 37.76 38.65 58.96 53.92 52.01 72.84Baichuan-7B(Fact) 74.05 60.25 61.69 69.48 66.02 64.71 82.58\nEGGğ‘“ğ‘Ÿğ‘’ğ‘’ 75.23 64.41 64.24 57.19 54.34 53.41 85.86\nEGG 76.86 65.15 65.90 56.92 54.43 53.59 86.54\nTable 3: Inference speed of Baichuan-7B and EGG.\nMethods Inference Speed\nBaichuan-7B(Fact) 1.00Ã—\nEGGğ‘“ğ‘Ÿğ‘’ğ‘’ 0.71Ã—\nEGG 8.72Ã—\nthe generated court view performs, the more effective the extracted\nevents will be. The corresponding results are shown in section 5.4.2.\n5.4.2 Results of court view generation. To validate the effec-\ntiveness of EGG, we first compare it with several baselines. As\nshown in Table 2, we find all methods that exploit PLMs outper-\nform the traditional baselines implemented with GRU or trans-\nformer, which demonstrates the effectiveness of PLMs. Then, we\ncan observe that both EGG and EGGğ‘“ğ‘Ÿğ‘’ğ‘’ perform better than other\nbaselines in most metrics, which indicates our EGG can gener-\nate more plausible court views. Specifically, compared with C3VG\nwith BART, which groups the original fact into two type para-\ngraphs to generate court views, our EGG significantly outperforms\nit. This observation demonstrates that incorporating fine-grained\nevents into court view generation is more effective than employing\ncoarse-grained paragraphs. We also implement BART by taking\nfact and event as the text input, respectively. From the results, we\nobserve that BART(fact) surpasses BART(Event) by a large margin,\nillustrating that there exist several events are not extracted. These\nobservations prove that it is necessary to design an incorporated\nstrategy to combine the case facts with the event information to\ngenerate court views.\nBesides, we observe that Baichuan-7B without instruction tuning\nperforms well on the legal task in the zero-shot setting, which indi-\ncates that Baichuan-7B has already possessed court view abilities\nthrough training on a large amount of data. However, its results are\nstill worse than the fine-tuned model (Baichuan-7B(Fact)), which\nalso shows the necessity of fine-tuning LLMs to the court view\ngeneration. Although Baichuan-7B(Fact) achieves promising re-\nsults on BLEU than EGG, it still performs worse on ROUGE and\nBert-S. Meanwhile, EGG does not fine-tune LLMs in the generation\nphase, which further illustrates the effectiveness of EGG. Finally,\nwe analyse the difference between EGG andEGGğ‘“ğ‘Ÿğ‘’ğ‘’ . From Table 2,\nEGG performs better than EGGğ‘“ğ‘Ÿğ‘’ğ‘’ , which indicates that it is more\neffective to combine case facts and events directly and explicitly\non the data side than to introduce events implicitly into the model\nstructure. However, from the point of view of inference speed and\ncomputational resources occupied by the model, EGGğ‘“ğ‘Ÿğ‘’ğ‘’ is faster\nand occupies fewer computational resources, yet achieves simi-\nlar results to EGG. This observation illustrates the effectiveness\nof EGGğ‘“ğ‘Ÿğ‘’ğ‘’ which employs the contrastive learning constraint to\nincorporate event information into the learning of factual repre-\nsentations. In section 5.5, we will further illustrate the efficiency of\ninference in EGGğ‘“ğ‘Ÿğ‘’ğ‘’ .\n5.5 Efficiency of Inference in EGG free (RQ2)\nIn this section, we present the results of our experiments com-\nparing the inference speed of our proposed EGGğ‘“ğ‘Ÿğ‘’ğ‘’ with other\nbaselines. The hardware setup for the experiments consists of 12\ncores of Intel(R) Xeon(R) Gold 5317 CPU and a single 40G NVIDIA\nA100 Tensor Core GPU. The findings are summarized in Table 3.\nOur proposed EGGğ‘“ğ‘Ÿğ‘’ğ‘’ achieves an impressive decoding speed,\napproximately 12 times the speed achieved by EGG, which utilizes\nLLM for event extraction. It is worth noting that EGG has the slow-\nest inference speeds. Besides, although the difference between the\nnumber of parameters in EGG and Baichuan-7B is not significant,\nsince there are multiple events for a single case, EGG often needs\nto perform multiple event extractions, and thus is slower than\nBaichuan-7B. This observation highlights that EGGğ‘“ğ‘Ÿğ‘’ğ‘’ strikes\na balance between efficiency and effectiveness, making it well-\nsuited for resource-constrained users. The results demonstrate that\nEGGğ‘“ğ‘Ÿğ‘’ğ‘’ offers a practical solution for legal event extraction, pro-\nviding efficient performance while maintaining effectiveness. Its\nsuitability for resource-constrained users makes it a valuable option\nin real-world applications.\n5.6 Performance by the Length of Court Views\n(RQ3)\nIn this section, we focus on investigating the generation perfor-\nmance of court views based on their length. We sample examples\nfrom the test set of CJO, where the real court views have lengths\nranging from 50 to 120 tokens. We then predict and evaluate the\ngenerated court views by comparing them with the outputs ofEGG\nand C3VG with BART using ROUGE-L and BLEU-4 scores. The find-\nings, as illustrated in Figure 5, reveal that bothEGG and C3VG with\nBART experience a degradation in performance as the length of\ncourt views increases. However, we observe that ourEGG achieves\nthe best performance when the court view length is between 60\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Linan Yue, Qi Liu, Lili Zhao, Li Wang, Weibo Gao, and Yanqing An\n60 70 80 90 100 110\nCourt View Length\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95ROUGE-L\nEGG\nC3VG\n60 70 80 90 100 110\nCourt View Length\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90BLEU-4\nEGG\nC3VG\nFigure 5: Model performance by the length of court views. Among them, C3VG is implemented with BART.\nTable 4: Detailed scoring standards for human annotators.\nScoreUsefulness Fluency\n1 No Use. The generated texts are useless for an-\nswering questions.\nNonsense.\n2 Almost useless. Almost all generated texts are use-\nless.\nVery unfluent.\n3 Half of them are useful. About half of the gener-\nated texts are useful for answering questions.\nPartial fluent.\n4 Highly useful. Most generated texts are useful to\nanswer the questions.\nHighly fluent.\n5 Exactly. Generated texts are useful for me to get\nthe correct answer.\nVery fluent.\nand 70 tokens, with both ROUGE-L and BLEU-4 scores surpass-\ning 90. Furthermore, our method outperforms C3VG with BART\nacross all court view lengths, indicating the effectiveness of incor-\nporating fine-grained event information into court view generation.\nThis suggests that by considering the specific event details in the\ngeneration process, our approach can produce more accurate and\nhigher-quality court views compared to existing methods.\n5.7 Human Evaluation (RQ4)\nTable 2 highlights that both EGG and EGGğ‘“ğ‘Ÿğ‘’ğ‘’ exhibit lower BLEU\nscores compared to Baichuan-7B(fact), prompting the need to in-\nvestigate the performance of generated court views. To gain further\ninsights, a human evaluation is conducted on the court views gener-\nated by EGG and Baichuan-7B(fact). In this evaluation, a total of 100\nexamples are sampled, and three annotators with expertise in both\ncomputer science and law are asked to evaluate the generated court\nviews based on two metrics: Usefulness and Fluency. Each metric\nis scored on a scale from 1 (lowest) to 5 (highest), with specific\nscoring standards provided in Table 4. The experimental results are\npresented in Table 5. The results indicate that all models achieve\npromising scores in terms of Fluency, indicating that the generated\ncourt views are fluent and well-formed. Additionally, it is observed\nthat EGG and EGGğ‘“ğ‘Ÿğ‘’ğ‘’ outperform Baichuan-7B(fact) in terms of\nUsefulness. This finding further illustrates the effectiveness of incor-\nporating fine-grained event information into court view generation.\nBy considering the specific event details, our models generate court\nviews that are deemed more useful by human evaluators.\n5.8 Case Study (RQ5)\nAn example of extracted events and generated court views is shown\nin Figure 6. Specifically, firstly, the type of case in this fact is inten-\ntional injury . Then, Event-related Questions show all questions\nTable 5: Human evaluation on generated texts.\nMethods Usefulness Fluency\nBaichuan-7B(fact) 3.76 4.43\nEGGğ‘“ğ‘Ÿğ‘’ğ‘’ 3.98 4.63\nEGG 4.09 4.68\ndesigned for the crime of intentional injury. It is worth noting that\nthe designed questions are the same for any fact which belongs\nto intentional injury . In the Answers, we present the answers ex-\ntracted from the fact description according to the questions by the\nLLMs-based extractor. Afterward, we post-process the questions\nand answers to obtain the corresponding Events.\nNext, we present 7 court views generated by EGG and baselines:\nâ€¢We can find although C3VG generates court views well, it\nfails to generate the court views about obtaining the victimâ€™s under-\nstanding. Conversely, EGG can generate more plausible court views.\nBesides, EGG also yields injured another person with a knife\nwhich has been described in fact description but not in the real\ncourt view. This observation indicates EGG can generate several\nkey information, which is ignored by the real court view.\nâ€¢Besides, compared to BART(Event), we find it generates several\nunfaithful court views (the underlined) which do not exist in the\ncase fact, illustrating that it is unfeasible to generate court views\nbased solely on events, and we need to combine the fact and event\nfor the generation. Then, although BART(Fact) has generated court\nviews well, there are several omissions it generates compared to\nother methods. For example, EGG yields the injuries of the defen-\ndant is minor, however, BART(Fact) only generates the defendant\nhas injuries (i.e., intentionally injured another personâ€™s body ).\nâ€¢Moreover, we can get a fluent court view by directly prompting\nBaichuan-7B (without fine-tuning). However, we tend to obtain\nsome redundant information (e.g., â€œIf you do not accept this judgment,\n... , directly to the [Province] Intermediate Peopleâ€™s Court ... â€). This\ninformation has nothing to do with the court view, and may even\ninvolve some private information, such as [Province] Intermediate\nPeopleâ€™s Court, where we have a privacy treatment for [Province] to\nshow this example. Meanwhile, Baichuan-7B(fact) is not as accurate\nas EGG in yielding information about surrender, where Baichuan-\n7B(fact) only describes the defendantâ€™s surrender, while EGG also\ndescribes how the defendant surrendered (â€œthe victim learned that\nothers had called the police and waited at the scene to be arrested â€).\nThese observations demonstrate that incorporating fine-grained\nevent information into the court view generation is effective.\nEvent Grounded Criminal Court View Generation with Cooperative (Large) Language Models SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nThetrialfoundthatat19:00onJune24,2017,thedefendantQianxxwasinvitedbythevictimZhangxxtodrinkatthexxCountyxxTownshipHotel.Duringthattime,QianxxandLixx,whowasatthesametable,hadaverbalaltercationandQianxxlefttherestaurantalone.ThetwopartiesagreedbytelephonetomeetonthehighwaynearQianxx'shouse.Afterthemeeting,QianxxandZhangxxweredrinkingataroadsidekioskwhenanotherargumentbrokeout,andZhangxxpushedQianxx,whothenusedakitchenknifehehadpickeduponthegroundtocutZhangxx'sbackandchestandcuthisfingers.TheextentofZhang'sinjurieswasassessedtobesecond-degreeminorinjuries.Afterlearningthatthepolicehadbeencalled,Qianxxwaitedatthescenetobearrestedand,uponarrival,truthfullyconfessedtothefactsofhiscrime.ItwasalsofoundthatthedefendantQianxxcompensatedthevictimZhangxxfordamagestotallingRMB4,000,andobtainedZhangxx'sunderstandingYes/NO.1Whatwasthedefendantâ€˜sinstrumentofinjurytothevictim?2Whatkindofinjuriesdidthedefendantcause?3Whatwasthecauseofthedefendant'sfight?4Howmuchmoneydidthedefendantpaythevictim?5Whetherthedefendantsurrenderedhimself?6Whetherthedefendantobtainedthevictim'sunderstanding?7Whetherthedefendantmadeatruthfulconfessiontothecrime?1akitchenknife2TheextentofZhangâ€™sinjurieswasassessedtobesecond-degreeminorinjuries3anotherargumentbrokeout,andZhangxxpushedQianxx4RMB4,0005Yes6Yes7YesThetoolofthecrimeisakitchenknife.InjuriesistheextentofZhang'sinjurieswasassessedtobesecond-degreeminorinjuries.Causeofcrimeisanotherargumentbrokeout,andZhangxxpushedQianxx.AmountofcompensationisRMB4,000.Thedefendantsurrenderedhimself.Thedefendantobtainedthevictim'sunderstanding.Thedefendantmadeatruthfulconfessiontothecrime.ThecourtheldthatQianxxintentionallyinjuredthehealthofothers,causingonepersonsecond-degreeminorinjuries,hisbehaviorhasconstitutedthecrimeofintentionalinjury.ThedefendantQianxxwaswaitingforarrestatthesceneknowingthatothershadreportedthecrime,andafterarrivingatthecase,hetruthfullyconfessedthefactsofhiscrime,andwasaself-surrender,whichcanbealighterorlesserpunishmentaccordingtothelaw.Afterthecrimewascommitted,thedefendantQianxxcompensatedthevictimforthelossandobtainedanunderstanding,sohecouldbegivenalighterpunishment.ThecourtheldthatQianxxintentionallyinjuredanotherpersonwithaknife,causingminorinjuriestooneperson,andhisconductconstitutedthecrimeofintentionalinjury.Thedefendantwasarmedwithaknifetocommittheactofinjury,andcouldbepunishedwithdiscretionaryseverity.Afterthecrime,thevictimlearnedthatothershadcalledthepoliceandwaitedatthescenetobearrested,andafterarrivingatthecase,hetruthfullyconfessedthefactsofhiscrime,whichcouldbealighterpunishmentaccordingtothelaw.Besides,hecompensatedthevictimandcouldbepunishedlessseverely.ThecourtheldthatthedefendantQianxxintentionallyinjuredanotherperson'sbody,causingonepersonminorinjuries,hisbehaviorhasconstitutedthecrimeofintentionalinjury.ThedefendantQianxx,afterlearningthatothershadcalledthepolice,waitedatthescenetobearrested,andafterarrivingatthecase,confessedtruthfullytothefactsofhiscrime,andwasaself-surrender,whichcouldbepunishedlessseverely.ThecourtheldthatQianxx,togetherwithothers,randomlyassaultedothers,causingminorinjuriestooneperson,thecircumstancesarebad,hisbehaviorhasconstitutedthecrimeofintentionalinjury.Inthejointcrime,thevictim,Zhangxx,wasgenerallyatfaultandcouldbepunishedlessseverelyatdiscretion.Thedefendantconfessedthefactsofhiscrimetruthfullyafterhisreturntothecourt,andhecanbepunishedlightlyaccordingtothelaw.ThecourtheldthatQianxxdisregardedthenationallawandintentionallyinjuredanotherperson'sbody,hisbehaviorhasconstitutedthecrimeofintentionalhomicide.Thedefendantwasinformedthatothershadcalledthepoliceandwaitedforarrestatthescene.Afterarrivingatthecase,hetruthfullyconfessedtothefactsofhiscrimeandwasaself-surrender,whichcanbealighterpunishmentaccordingtothelaw.Hecompensatedthevictimandcouldbepunishedlessseverely.ThiscourtheldthatthedefendantQianxxintentionalinjurytothebodyofanotherpersontocauseminorinjurieshasconstitutedthecrimeofintentionalinjury.Thepublicprosecutionauthoritiesaccusedofthefactsareclear,theevidenceisindeedsufficient,thechargesareestablished.DefendantQianxxvoluntarilysurrendered,truthfullyconfessedhiscrime,isthesurrender,canbemitigatedorreducedpunishment.DefendantQianxxactivelycompensatedthevictimâ€˜seconomiclosses,obtainedtheunderstandingofthevictim,maybeappropriatetomitigatehispunishment.Accordingtothedefendantâ€™scriminalcircumstancesandrepentance,pronouncementofprobationhasnosignificantadverseimpactonthecommunityinwhichhelives,soaccordingtothelawtoapplyprobation.Accordingly,inaccordancewiththeprovisionsofArticle234(1),Article67(1),andArticle72oftheCriminalLawofthePeopleâ€˜sRepublicofChina,theverdictisasfollowsDefendantQianxxisguiltyofintentionalinjuryandsentencedtosixmonthsoffixed-termimprisonmentandoneyearofprobation.(Theperiodofprobationshallbecalculatedfromthedatewhenthejudgmentisfinalized.)Ifyoudonotacceptthisjudgment,youmay,withintendaysfromtheseconddayofreceiptofthejudgment,appealthroughthiscourtordirectlytothe[Province]IntermediatePeopleâ€™sCourt.Inthecaseofawrittenappeal,oneoriginalandtwocopiesofthestatementofappealshallbesubmitted.ThiscourtheldthatthedefendantQianxxintentionallyinjuresanotherperson'sbodytocauseaminorinjury,hisbehaviorhasconstitutedthecrimeofintentionalinjury.Thepublicprosecutionauthoritiesaccusedofthefactsareclear,theevidenceisindeedsufficient,thecrimeisestablished.DefendantQianxxvoluntarilysurrenderedtothepoliceandconfessedtohiscrimeafterarrivingatthecrime,soheisaself-surrender,andcanbemitigatedoralleviatedinaccordancewiththelaw.DefendantQian\u0001xxactivelycompensatedthevictim'sfinanciallosses,obtainedtheunderstandingofthevictim,maybeappropriatetomitigatepunishment.\nFactDescription\nEvent-relatedQuestionsAnswersEventsRealCourtView(groundtruth)\nC3VG\nBART(Event)\nBART(Fact)\nEGG\nBaichuan-7B\nBaichuan-7B(Fact)\nFigure 6: An example of extracted events and generated court views.\n6 DISCUSSION\nEthical Discussion. Court view generation has gained significant\nattention as a core task in legal intelligence. Based on the exper-\nimental results, EGG demonstrates the ability to generate more\naccurate court views. Additionally, EGGğ‘“ğ‘Ÿğ‘’ğ‘’ achieves a balance\nbetween modeling effectiveness and inference efficiency, making it\nsuitable for users with limited computational resources. However,\nit is important to note that our model does not replace the work\nof judges. Instead, our aim is to assist judges in organizing court\nviews and alleviate their workload. The final court views must be\ndetermined and decided upon by the judges themselves [ 33, 40].\nOur work serves to provide judges with a tool to streamline the\nprocess of collating court views and reduce their workload stress.\nLimitations. When extracting events, we simply post-process\nthe extracted answers and questions to obtain the corresponding\nevents. However, when dealing with complex relationships among\nevents, such as causality, a more advanced approach is needed. One\npossible solution is to construct an event graph that represents the\nrelationships among events. An event graph is a graphical repre-\nsentation where events are nodes, and the relationships between\nevents are represented by edges. By incorporating this event graph\ninto the court view generation process, the model can better capture\nand understand the complex relationships among events. We will\nleave it as the future work.\n7 CONCLUSION\nIn this paper, we proposed an Event Grounded Generation (EGG)\nmethod for criminal court view generation with cooperative (Large)\nLanguage Models, cascading the event extractor and the court view\ngenerator. To be specific, EGG first employed a trained LLMs-based\nlegal event extractor to select several events in the case fact without\nmassive annotated events. Then, in the court view generator, we\nincorporated these events into the court view generation by merg-\ning the case fact and event as the new input. Besides, to alleviate\nthe computational burden in EGG during inference that employs\nLLMs, we further proposed a LLMs-free EGG method based on\nthe contrastive constraint. This enhancement enables court view\ngeneration without requiring event information during the infer-\nence phase. Experimental results on a real-world dataset clearly\ndemonstrated the effectiveness of our proposed method.\nAcknowledgements. This research was supported by grants\nfrom the National Natural Science Foundation of China (Grants No.\n62337001, 623B1020) and the Fundamental Research Funds for the\nCentral Universities.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Linan Yue, Qi Liu, Lili Zhao, Li Wang, Weibo Gao, and Yanqing An\nREFERENCES\n[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine\ntranslation by jointly learning to align and translate. Proceedings of International\nConference on Learning Representations (ICLR) (2014).\n[2] BaiChuan-Inc. 2023. A large-scale 7B pretraining language model developed by\nBaiChuan-Inc. https://github.com/baichuan-inc/Baichuan-7B.\n[3] Jiangui Chen, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yiqun Liu, Yixing Fan,\nand Xueqi Cheng. 2023. A Unified Generative Retriever for Knowledge-Intensive\nLanguage Tasks via Prompt Learning. In Proceedings of the 46th International\nACM SIGIR Conference on Research and Development in Information Retrieval .\n1448â€“1457.\n[4] Kyunghyun Cho, Bart Van MerriÃ«nboer, Caglar Gulcehre, Dzmitry Bahdanau,\nFethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase\nrepresentations using RNN encoder-decoder for statistical machine translation.\nIn Proceedings of the 2014 Conference on EMNLP .\n[5] Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. 2023. ChatLaw:\nOpen-Source Legal Large Language Model with Integrated External Knowledge\nBases. ArXiv (2023).\n[6] Xingyi Duan, Baoxin Wang, Ziyue Wang, Wentao Ma, Yiming Cui, Dayong Wu,\nShijin Wang, Ting Liu, Tianxiang Huo, Zhen Hu, et al . 2019. Cjrc: A reliable\nhuman-annotated benchmark dataset for chinese judicial reading comprehension.\nIn China National Conference on Chinese Computational Linguistics . Springer, 439â€“\n451.\n[7] Yi Feng, Chuanyi Li, and Vincent Ng. 2022. Legal Judgment Prediction via Event\nExtraction with Constraints. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers) . Association\nfor Computational Linguistics.\n[8] Wanwei He, Jiabao Wen, Lei Zhang, Hao Cheng, Bowen Qin, Yunshui Li, Feng\nJiang, Junying Chen, Benyou Wang, and Min Yang. 2023. HanFei-1.0. https:\n//github.com/siat-nlp/HanFei.\n[9] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large\nLanguage Models. In International Conference on Learning Representations .\n[10] Quzhe Huang, Mingxu Tao, Zhenwei An, Chen Zhang, Cong Jiang, Zhibin Chen,\nZirui Wu, and Yansong Feng. 2023. Lawyer LLaMA Technical Report. ArXiv\n(2023).\n[11] Weijing Huang, Xianfeng Liao, Zhiqiang Xie, Jiang Qian, Bojin Zhuang, Shaojun\nWang, and Jing Xiao. 2020. Generating Reasonable Legal Text through the\nCombination of Language Modeling and Question Answering. In Proceedings of\nthe 29th International Joint Conference on Artificial Intelligence (IJCAI) .\n[12] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. Bert:\nPre-training of deep bidirectional transformers for language understanding. In\nProceedings of naacL-HLT , Vol. 1. 2.\n[13] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman\nMohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART:\nDenoising Sequence-to-Sequence Pre-training for Natural Language Generation,\nTranslation, and Comprehension. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics .\n[14] Chuanyi Li, Yu Sheng, Jidong Ge, and Bin Luo. 2019. Apply Event Extraction\nTechniques to the Judicial Field. In Adjunct Proceedings of the 2019 ACM Interna-\ntional Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of\nthe 2019 ACM International Symposium on Wearable Computers .\n[15] Haitao Li, Qingyao Ai, Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Zheng Liu, and Zhao\nCao. 2023. Constructing Tree-based Index for Efficient and Effective Dense Re-\ntrieval. In Proceedings of the 46th International ACM SIGIR Conference on Research\nand Development in Information Retrieval . 131â€“140.\n[16] Qingquan Li, Qifan Zhang, Junjie Yao, and Yingjie Zhang. 2020. Event Extraction\nfor Criminal Legal Text. In 2020 IEEE International Conference on Knowledge\nGraph (ICKG).\n[17] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries.\nIn Text Summarization Branches Out . Association for Computational Linguistics.\n[18] Yifei Liu, Yiquan Wu, Yating Zhang, Changlong Sun, Weiming Lu, Fei Wu, and\nKun Kuang. 2023. ML-LJP: Multi-Law Aware Legal Judgment Prediction. In\nProceedings of the 46th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval . 1023â€“1034.\n[19] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regulariza-\ntion. In 7th International Conference on Learning Representations, ICLR 2019, New\nOrleans, LA, USA, May 6-9, 2019 . OpenReview.net.\n[20] Bingfeng Luo, Yansong Feng, Jianbo Xu, Xiang Zhang, and Dongyan Zhao. 2017.\nLearning to Predict Charges for Criminal Cases with Legal Basis. InProceedings of\nthe 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP) .\n2727â€“2736.\n[21] Yixiao Ma, Yunqiu Shao, Yueyue Wu, Yiqun Liu, Ruizhe Zhang, Min Zhang, and\nShaoping Ma. 2021. LeCaRD: a legal case retrieval dataset for Chinese law system.\nIn Proceedings of the 44th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval .\n[22] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning\nwith contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).\n[23] OpenAI. 2023. Introducing ChatGPT. OpenAI Blogs (2023). https://openai.com/\nblog/chatgpt\n[24] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a\nmethod for automatic evaluation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computational Linguistics . 311â€“318.\n[25] Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Donâ€™t Know:\nUnanswerable Questions for SQuAD. In Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics (Volume 2: Short Papers) .\n[26] Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get To The Point:\nSummarization with Pointer-Generator Networks. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational Linguistics. (ACL) .\n[27] Yunqiu Shao, Yueyue Wu, Yiqun Liu, Jiaxin Mao, Min Zhang, and Shaoping Ma.\n2021. Investigating user behavior in legal case retrieval. In Proceedings of the 44th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval.\n[28] Shirong Shen, Guilin Qi, Zhen Li, Sheng Bi, and Lusheng Wang. 2020. Hierarchical\nChinese Legal event extraction via Pedal Attention Mechanism. In Proceedings of\nthe 28th International Conference on Computational Linguistics .\n[29] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos\nGuestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An\nInstruction-following LLaMA model. https://github.com/tatsu-lab/stanford_\nalpaca.\n[30] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lam-\nple. 2023. LLaMA: Open and Efficient Foundation Language Models. arXiv\npreprint arXiv:2302.13971 (2023).\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing systems (2017).\n[32] Yunhe Wang, Hanting Chen, Yehui Tang, Tianyu Guo, Kai Han, Ying Nie, Xu-\ntao Wang, Hailin Hu, Zheyuan Bai, Yun Wang, et al. 2023. PanGu: Enhancing\nLanguage Model Architectures via Nonlinearity Compensation. arXiv preprint\narXiv:2312.17276 (2023).\n[33] Yiquan Wu, Kun Kuang, Yating Zhang, Xiaozhong Liu, Changlong Sun, Jun Xiao,\nYueting Zhuang, Luo Si, and Fei Wu. 2020. De-Biased Courtâ€™s View Generation\nwith Causality. In Proceedings of the 2020 Conference on EMNLP .\n[34] Yiquan Wu, Yifei Liu, Weiming Lu, Yating Zhang, Jun Feng, Changlong Sun,\nFei Wu, and Kun Kuang. 2022. Towards Interactivity and Interpretability: A\nRationale-based Legal Judgment Prediction Framework. (2022), 4787â€“4799.\n[35] Feng Yao, Chaojun Xiao, Xiaozhi Wang, Zhiyuan Liu, Lei Hou, Cunchao Tu,\nJuanzi Li, Yun Liu, Weixing Shen, and Maosong Sun. 2022. LEVEN: A Large-\nScale Chinese Legal Event Detection Dataset. In Findings of the Association for\nComputational Linguistics: ACL 2022 .\n[36] Hai Ye, Xin Jiang, Zhunchen Luo, and Wenhan Chao. 2018. Interpretable charge\npredictions for criminal cases: Learning to generate court views from fact de-\nscriptions. Proceedings of the 2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics(NAACL) (2018).\n[37] Linan Yue, Qi Liu, Yichao Du, Weibo Gao, Ye Liu, and Fangzhou Yao. 2023.\nFedjudge: Federated legal large language model. arXiv preprint arXiv:2309.08173\n(2023).\n[38] Linan Yue, Qi Liu, Binbin Jin, Han Wu, and Yanqing An. 2024. A Circumstance-\naware Neural Framework for Explainable Legal Judgment Prediction. IEEE\nTransactions on Knowledge and Data Engineering (2024).\n[39] Linan Yue, Qi Liu, Binbin Jin, Han Wu, Kai Zhang, Yanqing An, Mingyue Cheng,\nBiao Yin, and Dayong Wu. 2021. NeurJudge: A Circumstance-aware Neural\nFramework for Legal Judgment Prediction. InProceedings of the 44th International\nACM SIGIR Conference on Research and Development in Information Retrieval .\n[40] Linan Yue, Qi Liu, Han Wu, Yanqing An, Li Wang, Senchao Yuan, and Dayong Wu.\n2021. Circumstances enhanced criminal court view generation. In Proceedings\nof the 44th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval . 1855â€“1859.\n[41] Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shu-\njun Liu, Yuxuan Zhou, Yao Xiao, Song Yun, Wei Lin, et al. 2023. Disc-lawllm:\nFine-tuning large language models for intelligent legal services. arXiv preprint\narXiv:2309.11325 (2023).\n[42] Han Zhang, Zhicheng Dou, Yutao Zhu, and Ji-Rong Wen. 2023. Contrastive Learn-\ning for Legal Judgment Prediction. ACM Transactions on Information Systems 41,\n4 (2023), 1â€“25.\n[43] Haoxi Zhong, Zhipeng Guo, Cunchao Tu, Chaojun Xiao, Zhiyuan Liu, and\nMaosong Sun. 2018. Legal Judgment Prediction via Topological Learning. In\nProceedings of the 2018 Conference on EMNLP .\n[44] Haoxi Zhong, Yuzhong Wang, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, and\nMaosong Sun. 2020. Iteratively questioning and answering for interpretable legal\njudgment prediction. In Proceedings of the 34th AAAI Conference on Artificial\nIntelligence, Vol. 34. 1250â€“1257.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6407566070556641
    },
    {
      "name": "Event (particle physics)",
      "score": 0.5043665170669556
    },
    {
      "name": "Computer security",
      "score": 0.4180513024330139
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    }
  ]
}