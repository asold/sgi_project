{
    "title": "Revisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models",
    "url": "https://openalex.org/W4400779789",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2111539822",
            "name": "Isadora Campregher Paiva",
            "affiliations": [
                "University of Amsterdam"
            ]
        },
        {
            "id": "https://openalex.org/A2938644925",
            "name": "Josephine Diecke",
            "affiliations": [
                "University of Zurich"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4214744264",
        "https://openalex.org/W2921858050",
        "https://openalex.org/W2159276920",
        "https://openalex.org/W4323521900",
        "https://openalex.org/W4286009750",
        "https://openalex.org/W4286846183",
        "https://openalex.org/W2065869119",
        "https://openalex.org/W1569298189",
        "https://openalex.org/W2565799483",
        "https://openalex.org/W3126970061",
        "https://openalex.org/W4362703233",
        "https://openalex.org/W4210797512",
        "https://openalex.org/W2099813784",
        "https://openalex.org/W1539653163",
        "https://openalex.org/W2049529728",
        "https://openalex.org/W2946517888",
        "https://openalex.org/W2113459411",
        "https://openalex.org/W2251958472",
        "https://openalex.org/W2038784157",
        "https://openalex.org/W4322618977",
        "https://openalex.org/W2997064598",
        "https://openalex.org/W4361866031",
        "https://openalex.org/W4385395702",
        "https://openalex.org/W4231748204",
        "https://openalex.org/W4210827551",
        "https://openalex.org/W4375870056",
        "https://openalex.org/W2771976988",
        "https://openalex.org/W3191193109",
        "https://openalex.org/W614021763",
        "https://openalex.org/W411138465",
        "https://openalex.org/W249400579",
        "https://openalex.org/W4238023786",
        "https://openalex.org/W3141474827",
        "https://openalex.org/W3089331698",
        "https://openalex.org/W4251720972"
    ],
    "abstract": "Film reviews are an obvious area for the application of sentiment analysis, but while this is common in the field of computer science, it has been mostly absent in film studies. Film scholars have quite rightly been skeptical of such techniques due to their inability to grasp nuanced critical texts. Recent technological developments have, however, given us cause to re-evaluate the usefulness of automated sentiment analysis for historical film reviews. The release of ever more sophisticated Large Language Models (LLMs) has shown that their capacity to handle nuanced language could overcome some of the shortcomings of lexicon-based sentiment analysis. Applying it to historical film reviews seemed logical and promising to us. Some of our early optimism was misplaced: while LLMs, and in particular ChatGPT, proved indeed to be much more adept at dealing with nuanced language, they are also difficult to control and implement in a consistent and reproducible way – two things that lexicon-based sentiment analysis excels at. Given these contrasting sets of strengths and weaknesses, we propose an innovative solution which combines the two, and has more accurate results. In a two-step process, we first harness ChatGPT’s more nuanced grasp of language to undertake a verbose sentiment analysis, in which the model is prompted to explain its judgment of the film reviews at length. We then apply a lexicon-based sentiment analysis (with Python’s NLTK library and its VADER lexicon) to the result of ChatGPT’s analysis, thus achieving systematic results. When applied to a corpus of 80 reviews of three canonical Weimar films ( Das Cabinet des Dr. Caligari , Metropolis and Nosferatu ), this approach successfully recognized the sentiments of 88.75% of reviews, a considerable improvement when compared to the accuracy rate of the direct application of VADER to the reviews (66.25%). These results are particularly impressive given that this corpus is especially challenging for automated sentiment analysis, with a prevalence of macabre themes, which can easily trigger falsely negative results, and a high number of mixed reviews. We believe this hybrid approach could prove useful for application in large corpora, for which close reading of all reviews would be humanly impossible.",
    "full_text": "ARTICLE \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-\nBased Sentiment Analysis with Large Language Models \nIsadora Campregher Paiva1\n, Josephine Diecke2\n1 University of Amsterdam, 2 University of Zurich \nKeywords: Weimar cinema, sentiment analysis, ChatGPT, film reviews, LLM, NLP \nhttps://doi.org/10.22148/001c.118497 \nJournal of Cultural Analytics \nVol. 9, Issue 4, 2024 \nFilm reviews are an obvious area for the application of sentiment analysis, but \nwhile this is common in the field of computer science, it has been mostly absent \nin film studies. Film scholars have quite rightly been skeptical of such \ntechniques due to their inability to grasp nuanced critical texts. Recent \ntechnological developments have, however, given us cause to re-evaluate the \nusefulness of automated sentiment analysis for historical film reviews. The \nrelease of ever more sophisticated Large Language Models (LLMs) has shown \nthat their capacity to handle nuanced language could overcome some of the \nshortcomings of lexicon-based sentiment analysis. Applying it to historical film \nreviews seemed logical and promising to us. Some of our early optimism was \nmisplaced: while LLMs, and in particular ChatGPT, proved indeed to be much \nmore adept at dealing with nuanced language, they are also difficult to control \nand implement in a consistent and reproducible way – two things that lexicon-\nbased sentiment analysis excels at. Given these contrasting sets of strengths and \nweaknesses, we propose an innovative solution which combines the two, and \nhas more accurate results. In a two-step process, we first harness ChatGPT’s \nmore nuanced grasp of language to undertake a verbose sentiment analysis, in \nwhich the model is prompted to explain its judgment of the film reviews at \nlength. We then apply a lexicon-based sentiment analysis (with Python’s NLTK \nlibrary and its VADER lexicon) to the result of ChatGPT’s analysis, thus \nachieving systematic results. When applied to a corpus of 80 reviews of three \ncanonical Weimar films (Das Cabinet des Dr. Caligari, Metropolis and \nNosferatu), this approach successfully recognized the sentiments of 88.75% of \nreviews, a considerable improvement when compared to the accuracy rate of the \ndirect application of VADER to the reviews (66.25%). These results are \nparticularly impressive given that this corpus is especially challenging for \nautomated sentiment analysis, with a prevalence of macabre themes, which can \neasily trigger falsely negative results, and a high number of mixed reviews. We \nbelieve this hybrid approach could prove useful for application in large corpora, \nfor which close reading of all reviews would be humanly impossible. \nIntroduction  \nIn the field of film studies, the majority of scholarly discourse revolves \naround a small number of canonized films. The reputation of such works \ncan sometimes seem self-evident, particularly as the films become older than \nthe living memory of scholars. Let us take as an example the Weimar classic \nCampregher Paiva, Isadora, and Josephine Diecke. “Revisiting Weimar Film Reviewers’\nSentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language\nModels.”Journal of Cultural Analytics, vol. 9, no. 4, July 2024,\nhttps://doi.org/10.22148/001c.118497.\nDas Cabinet des Dr. Caligari (1920). In the 2008 compendium Weimar \nCinema: An Essential Guide to Classic Films of the Era Stefan Andriopoulos \ndeclares that “immediately after its release, The Cabinet of Dr. Caligari \nwas acclaimed a masterpiece of German expressionist cinema” (13). This \nstatement is seemingly so uncontroversial as to dispense the need for citation. \nYet previous work by film historians who engaged with primary sources \nreveals a more contentious reception. In German Expressionist Film, John \nBarlow affirms that the film received considerable disapproval from those \neager to assign fixed definitions of the cinematic arts in contrast to the \ntheatrical (29). In Dr Caligari at the Folies-Bergère, Kristin Thompson cites \na number of original sources in order to conclude that “[t]he French \nintellectual response to Caligari was mixed” (152). Later, she makes a broader \nstatement about the film’s reception beyond this specific context, contending \nthat “[s]uch views – for, against, and mixed – have characterized discussions \nof Caligari ever since” (154). Scholars focusing on the German and American \ncritical response to the film found a rosier picture. Siegfried Kracauer states \nthat German reviewers “were unanimous in praising Caligari as the first work \nof art on the screen” (71), while David Robinson finds that “[t]he German \ncritics, almost without exception, ranged from favorable to ecstatic” (56), and \n“[t]he American press was by and large as enthusiastic as the German critics” \n(58). \nThese scholars have likely reached such conclusions based on close readings of \nprimary sources, their divergence mostly caused by having access to different \nmaterial, particularly when focusing on different countries. Yet, it is hard to \nknow exactly what sources they accessed. As is typically the case for film \nhistorians, they cite only excerpts of, at most, a handful of reviews to make \ntheir points. Even if we can reasonably assume that they read many more \nreviews to reach their overarching conclusions, most of this material is left \nunacknowledged. That this means of reporting on qualitative research is the \nstandard in historical work is understandable given that, for much of history, \naccess to primary sources was mostly limited to what could be found in \nphysical archives. \nThough the digitization of archival materials is far from complete, sources \nlike The Media History Digital Library (MHDL) have revolutionized archival \nresearch, not only by granting access to a treasure trove of digitized materials, \nbut by making their texts fully searchable. This development sets the stage for \nmore in-depth analysis of collections with computer assistance, as highlighted \nby Acland and Hoyt in The Arclight Guidebook to Media History and the \nDigital Humanities. We contend that, while film historians have embraced \nthis newfound wealth of resources, our methodologies have not sufficiently \nadjusted to the new possibilities they confer. This article aims to serve as \na proof of concept demonstrating the potential of digital methods for the \nanalysis of historical film reception. \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 2\nFirstly, since we maintain that digitalization means we can afford – and \neven have the responsibility – to be more transparent regarding our corpora, \nwe are publishing our corpus and go into detail as to how it was built. \nWhile publishing corpora is common in quantitative analysis, we believe that \nqualitative research would also be better served by this, since sharing even \na small corpus of the entirety of one’s primary sources can make clear how \nmany reviews one had access to and allow readers to potentially build on \none’s work or dispute one’s conclusions. \nSecondly, we believe that there is underutilized potential for “distant reading” \n(Moretti)1\n when it comes to film reception. In film studies, analysis of film \nreviews have been mostly limited to close reading. While this method yields \nmuch more nuanced and rich analyses, it comes at the expense of breadth \nof coverage. If we are only interested in the reception of canonized films, \ndone one film and one country at a time, that is not a problem, but we \nbelieve there is much to be gained from going beyond that – not as an \nalternative methodology to supplant close reading, but to complement it. \nFollowing Denbo and Fraistat, we argue for a scalable approach, as “only by \nconnecting the distant with the close can the potential of digital (…) analytics \nto address questions about culture be fully realized” (170). This approach \nis not confined solely to qualitative or quantitative methods, automated or \nmanual processes; instead, it permits an iterative exploration involving both \nzooming in and zooming out of our data. Consequently, it avoids strict \nadherence to a single analytical level, allowing the examination of research \nobjects and questions from diverse perspectives. \nWhile the application of distant reading techniques to film reviews has been \nmostly absent in film historical work, that is not to say that the application \nof such methods to film reviews hasn’t been done at all. Sentiment analysis \nof film reviews is a common topic in computer science papers (Maas et al.; \nLu and Wu; Chen et al.). A particularly popular approach has been to use \nlexicon-based sentiment analysis. This is characterized by assigning texts an \noverall sentiment (typically positive or negative) through the use of pre-built \ndictionaries containing words and their associated sentiment scores. These \ntechniques were developed especially to deal with vast amounts of consumer-\ngenerated data on the internet, such as product reviews. Film historians have \nseemingly – and, as we shall see, quite rightly – been skeptical of such \ntechniques. The use of dictionaries which give a set sentiment value to a \nword regardless of the context in which it is used creates severe issues when it \ncomes to nuanced critical texts, to the extent that the technique seemed to be \nultimately useless for our purposes. \nWhile film studies has been undergoing a digital turn in the last 15 years, most of this has been dedicated to (moving) image analysis, such as \naverage shot lengths (Tsivian and Gunars), film colors (Flueckiger and Halter) and even what some have termed “distant viewing” (Arnold \nand Tilton) or “distant watching” (Howanitz). \n1 \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 3\nRecent technological developments have given us cause to re-evaluate the \nusefulness of automated sentiment analysis for historical film reviews. The \nrelease of ever more sophisticated Large Language Models (LLMs) – most \nfamously of ChatGPT in November 2022 – has shown that their capacity \nto handle nuanced language could overcome some of the shortcomings of \nlexicon-based sentiment analysis. In light of the increasing use of LLMs \nin academic contexts (Bukar et al.; Hariri; Shen et al.; Sudirjo et al.), we \nwere keen to assess the potential of ChatGPT and HuggingChat (an open \nsource alternative) models for our research. The integration of LLMs into \nthe domains of Digital Humanities and Computational Social Sciences \nintroduces novel opportunities, capitalizing particularly on their apparent \ntext comprehension capabilities alongside iterative inquiry methods (Ziems \net al.). As we shall elaborate, some of our early optimism was misplaced: \nwhile LLMs, and in particular ChatGPT, proved indeed to be much more \nadept at dealing with nuanced language, they are also difficult to control and \nimplement in a consistent and reproducible way—two things that lexicon-\nbased sentiment analysis excels at. \nGiven these contrasting sets of strengths and weaknesses, we propose an \ninnovative solution which combines the two, and has remarkably more \naccurate results. In a two-step process, we first harness ChatGPT’s more \nnuanced grasp of language to undertake a verbose sentiment analysis, in \nwhich the model is prompted to explain its judgment of the film reviews at \nlength. We then apply lexicon-based sentiment analysis with Python’s NLTK \nlibrary and its VADER (Valence Aware Dictionary for Sentiment Reasoning) \nlexicon to the result of ChatGPT’s analysis, thus achieving systematic results. \nThe primary appeal of such an approach is typically seen to be its capacity \nfor analysis of large corpora which go beyond what a human could feasibly \nread. Before one may use such an approach on a large corpus, however, it is \nnecessary to test its reliability, comparing the automated results to those of \ncareful manual annotation. The corpus for this test must be smaller and one \nwhich is intimately familiar to the authors. For this, we will use a corpus we \nprepared of 80 historical film reviews of three canonical Weimar films: Das \nCabinet des Dr. Caligari (1920), Nosferatu (1922) and Metropolis (1927). \nIn the next section, we will present this corpus, going over how it was \nselected and what it contains. Following that, we will explain and compare the \ndifferent sentiment analysis methods we employed—encompassing ternary \nmanual annotation (“positive”, “negative” and “mixed”), binary manual \nannotation (“positive” and “negative”) with LLM support, lexicon-based \nsentiment analysis with VADER and a hybrid model which integrates \nChatGPT outputs with VADER sentiment analysis. We will show that these \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 4\napproaches have variable accuracy2\n rates and ideal applications. We found \nthat no purely automated approach reached perfect accuracy when compared \nto human annotation, though the two-step hybrid approach of ChatGPT-\nVADER showed a remarkable improvement when compared to lexicon-based \nsentiment analysis and could prove useful for large-scale application to film \nreviews. \nCorpus Selection and Research Questions      \nOur first task was to locate film reviews pertaining to our chosen canonical \nWeimar films. To achieve this, we primarily utilized the online resources \navailable through the MHDL and archive.org, as they offer a wealth of \nhistorical materials such as trade journals and fan magazines in English, \nGerman, French, and Spanish. We initially believed we would be able to \ngather the reviews for a large number of Weimar films automatically through \nweb scraping, and thus create a corpus worthy of being called “big data”. This \nprocess was revealed to be far more complicated than initially anticipated. For \nstarters, finding materials that actually relate to the films we were searching \nfor is far from trivial. Film titles which are made up of common words, such \nas “Metropolis”, “Passion” (the English title for Ernst Lubitsch’s Madame \nDubarry) or even letters, like “M”, make it difficult to identify the films \nwithout considerable manual work. Moreover, even if the word in question \nwas indeed referring to the film we were interested in, instances of mere \nmentions, such as in lists of screenings, are a common occurrence, but offered \nlimited value for sentiment analysis. Even harder would be to distinguish \nreviews from ads (by definition very positive) or plot summaries.3\n It is \nalso difficult to automatically recognize when a review starts and ends in \na text—even for humans, the placement of text in a page feels somewhat \narbitrary in many of the historical sources we are working with, with breaks \nin the columns at odd places and gaps of sometimes dozens of pages in the \nmiddle of a review. To make matters even worse, the quality of the Optical \nCharacter Recognition (OCR) of the texts is often subpar, affecting not \nonly the quality of the digital text (which would make automated sentiment \nanalysis of it problematic), but also undermining the reliability of the search \nitself. This OCR issue was particularly salient in relation to German texts \nobtained from the MHDL. \nThese challenges underscore the limitations of current technology when it \ncomes to historical sources that were neither born-digital nor were digitized \nin a way that facilitates large-scale analysis. There is a reason why so much \nIn the course of this paper, we will use the word “accuracy” to compare the results of the automated methods with those of our own \nmanual annotations. Those are, of course, ultimately subjective judgments, and so the term must be understood in that context. \nEven while reading the reviews carefully, this still proved difficult later. Indeed, as Eric Hoyt details in Ink Stained Hollywood, his history of \nHollywood trade papers from 1915-1935, exhibitors were often rightly skeptical of the integrity of reviews in the trades. We ultimately \ndecided to excluse only those items that made no attempts to disguise that they were ads and keeping all texts that followed a review format, \nthat is, which provided at least a semblance of critical evaluation. \n2 \n3 \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 5\nof the scholarly work done on film reviews has been done on born-digital \nmaterial that is easily scrapable, like IMDb reviews (Shaukat et al.; Amulya et \nal.), rather than historical ones. \nWe subsequently altered our expectations and decided to create a corpus \nmanually, and focus on fewer films. We picked Das Cabinet des Dr. Caligari, \nNosferatu and Metropolis because those are some of the most canonized \nWeimar films according to several metrics.4\n They have generated a \nconsiderable amount of film historical research that has – as seen in the \nintroduction regarding Caligari – not led to a consensus about their \nreception, and we felt that openly publishing a collection of clean reviews \nwhich, though small when compared to big data, is still significantly larger \nthan what most scholars have accessed to make their judgments, would still \nbe a helpful contribution to the field. \nWe thus dedicated ourselves to a manual collection of reviews, eventually \nestablishing a corpus of 80 reviews distributed as follows: 38 (47%) reviews \nfor Caligari, 25 (31%) for Metropolis, and 17 (21%) for Nosferatu (Figure 1). \nWhile the majority of these (56%) were still obtained through the MHDL, \nother sources also proved important (Figure 2). A little-known website by \nthe independent scholar Olaf Brill called filmhistoriker.de proved particularly \nhelpful (contributing 32% of reviews) due to its provision of clean digital \ntext of German-language Weimar-era film reviews split by movie. The third \nlargest source of reviews were books, which reprinted historical reviews \nin full (Minden and Bachmann; Kaes et al.). Furthermore, our quest to \nbroaden our collection of non-English language reviews led us to explore \nalternative sources, such as from our fourth online resource, filmportal.de \n(4%), maintained by the Deutsches Filminstitut & Filmmuseum (DFF). \nUnfortunately, filmportal holds very few reviews per film, and often only as \nan image with no OCR at all. While we applied OCR ourselves to those \nmaterials using the open-source software Tesseract, this was often futile when \nit came to poorly scanned and maintained sources, particularly those written \nin the German gothic script known as Fraktur. \nFour languages are represented in our corpus: approximately the same \nnumber of reviews were written in English (34) and German (32), but there \nare also some in French (10) and Spanish (4) (Figure 3). Regarding the \ncountry of publication, a large number of reviews were published in Germany \n(33), followed by the USA (29), France (10), Spain (1) and Switzerland \n(1) (Figure 4). It is important to note that publication locations are not \nneatly aligned with languages. For instance, the magazine Cine-Mundial, \nwhich has four reviews in the dataset, presents a complex case of a Spanish-\nFor instance, these films constitute the top three most commonly assigned Weimar films in college syllabi, as derived from OpenSyllabus \n(Open Syllabus). Furthermore, they hold positions within the top 250 films featured in Sight & Sound’s esteemed “The 100 Greatest Films \nof All Time” list from 2012 (Sight and Sound). \n4 \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 6\nFigure 1. Distribution of Reviews by Film. \nFigure 2. Distribution of Reviews by Source. \nlanguage publication targeted at the Spanish and Latin American market, but \npublished in the USA. To ensure linguistic consistency for further processing, \nwe translated all non-English language reviews to English. This was done \nwith a combination of DeepL and manual checking of the results, as we \nare proficient in these four languages. While it would have been possible to \nuse different models for different languages, we felt that building a uniform \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 7\nFigure 3. Reviews by Language. \nFigure 4. Reviews by Country. \ncorpus would be preferable to ensure consistency and comparability of the \nreviews. The comparison of sentiment analysis results between translated and \nnon-translated reviews indicated that translation did not alter the outcome. \nTo manage the collected information, we established entries within the open-\nsource reference management software Zotero. The subsequent steps within \nZotero encompassed creating entries enriched with bibliographical details. We \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 8\ndiligently verified OCR content, which sometimes meant manually typing a \nreview from scratch and eradicated duplicates, resulting in a refined collection \nof 80 reviews, each with accurate bibliographical information. For improved \norganization and analysis, we exported the Zotero data and used Open Refine \nfor a final stage of data cleaning. Our final dataset, published here as an Excel \ntable, was constructed by incorporating all the gathered data and metadata \nfrom the individual reviews. It includes metadata for the reviews, their full \ntext in their original language and English translation, and the results of each \nof the sentiment analysis steps we took, which we will detail in the following \nsections. \nManual Annotation   \nOnce we had a clean corpus, we conducted a manual sentiment analysis \nto establish a baseline for the succeeding computer-assisted phases. We \ndeliberated over which sentiment categories were most useful and reflective \nof our objectives, and would afford the best basis for comparison with \nautomated methods. The spectrum from positive to negative is the one \ntypically used in sentiment analysis, so this seemed more fitting for \ncomparison purposes than a star-rating, and also easier to assign. Inside that \nspectrum, we chose to include a mixed category, as it would allow for a \nrecognition of reviews that are not easily categorizable in a binary, which \nare abundant in the corpus. We tried to characterize the review’s overall \nsentiment, marking it as mixed only when there were substantial arguments \nmade both in favor and against the picture. In other words, if a review was \noverwhelmingly positive or negative, minor comments to the contrary were \nnot sufficient to get a “mixed” judgment. \nLooking at the results of the manual sentiment analysis of our individual \nfilms (Figure 5), the following pattern emerges: Caligari’s reception was \noverwhelmingly positive, with 29 reviews (76.3%) marked as positive, 5 \n(13.2%) as negative and 4 (10.5%) as mixed. Metropolis, on the other hand, \nreceived mostly (52%) mixed reviews: 13 out of 25. This is followed by \n7 positive reviews (28%) and 5 negative reviews (20%). Within the smaller \ncorpus of reviews for Nosferatu, the reception was categorized as positive in 8 \ncases (47.1%), mixed in 6 cases (35.3%), and negative in 3 cases (17.6%). \nBinary Manual Annotation with ChatGPT Support       \nBecause of the high number of mixed reviews, we decided to add a separate \ncolumn containing a binary classification (either positive or negative) of the \nreviews.5\n Even mixed reviews typically lean slightly more to one side or the \nother, and having a binary rating in addition to the tripartite classification \nwould allow us to better check the automated methods’ accuracy. The \nSee column “Binary_Judgment”. 5 \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 9\nFigure 5. Sentiment Counts of Manual Annotation. \nlexicon-based VADER model has a tendency towards polarization, so we \nwanted to be able to check when its classification was only slightly off the \nmark or very much so. For this binary annotation, we used ChatGPT in a \nmanual and iterative manner to help us decide if a mixed review was mostly \npositive or mostly negative. The model thus served as an inter-rater reliability \nmeasure, in which one of the raters is not human. \nThis was the initial prompt used on all 23 “mixed” reviews: “The following is \na film review. Would you say it is mostly positive or mostly negative regarding \nthe film?”.6\n The answers typically contain around 130 words, and start with a \nsentence declaring “The film review appears to be mostly [positive/negative] \nregarding the film”, followed by statements justifying this judgment with \ndirect quotes of the review’s relevant passages. This structure could also vary \nseemingly at random: in one case, ChatGPT’s answer came in the form of \nnumbered bullet points (ID “TM6H4KLS”). \nFor the purposes of this binary manual annotation, we read through each \nof ChatGPT’s responses carefully, and we did not default to ChatGPT’s \njudgments, but rather used it as a separate rater and conversed with it when \nnecessary to find an assessment we felt was fair. A compelling example of \nthis arose in Herbert Ihering’s review “Der Metropolisfilm - Ufa-Palast am \nZoo (Vorbericht)”, published on January 11, 1927, in the early edition of the \nBerliner Börsen-Courier (ID “TIRHYW52”). As indicated by the title, this is \nnot a full-length review, but a Vorbericht (preliminary report) of Metropolis’ \nBerlin premiere. We felt that this distinction caused ChatGPT’s response to \nbe slightly off the mark, so we followed up our initial prompt with an extra \nquestion: \nChatGPT’s answers are recorded in the column “ChatGPT Binary Answers”. 6 \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 10\nUser: \nThe following is a film review. Would you say it is mostly \npositive or mostly negative regarding the film? \n“A great film premiere. A lot of audience applause for Fritz \nLang, the director, for Karl Freund, the operator, for the actors \nAlfred Abel, Heinrich George and Brigitte Helm. The film \nitself? A great effort with brilliant technical details for a banal, \nno longer viable idea. A future city and a stuffy past text.” \nChat GPT: \nThe film review appears to be mostly positive in its assessment of \ntechnical aspects and the people involved in the film, such as the \ndirector, operator, and actors. However, it also contains a negative \nnote regarding the film’s idea, describing it as “banal” and “no \nlonger viable.” So, it’s a mixed review with both positive and \nnegative elements. \nUser: \nThe first portion of the review is actually referring to the \npremiere. The reviewer’s opinion of the film itself is: “A great \neffort with brilliant technical details for a banal, no longer viable \nidea. A future city and a stuffy past text.” How would you \ncharacterize that, as mostly positive or mostly negative? \nChatGPT: \nI apologize for the confusion earlier. Based on the review’s \nassessment of the film itself, it is mostly negative. The reviewer \nmentions that the film has “brilliant technical details,” which is \na positive aspect. However, they also describe the film’s idea as \n“banal” and “no longer viable,” which are negative criticisms. So, \noverall, the review leans more towards the negative side. \nChatGPT’s original answer is not exactly wrong, and indeed, a human might \nhave easily made the same judgment. There is a reason, after all, why we \nhad originally marked this review as “mixed”, as it is not a clear-cut case. \nWe felt, however, that the reviewer’s sentiment regarding the film seemed to \ndiffer from what the public at the premiere had demonstrated, and therefore \npointed to the specific section of this short text that dealt with the film’s \nquality as perceived by Ihering. The model’s answer changed accordingly, \nmatching our own perception. \nOur judgment (and, after some prodding, ChatGPT’s) of this Ihering’s \npreliminary report seems to have been correct, since the author wrote a \nlengthier piece that day for the late edition of the same paper (ID \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 11\n“TAKRH7XK”), where he makes his mostly negative opinion of the film \nclear . Indeed, this evening review was unanimously perceived as negative by \nall the sentiment analysis methods we employed. \nOverall, we found that ChatGPT performed admirably at the task of helping \nus to assign binary sentiments to the reviews. ChatGPT’s output reliably \nsummarizes the positively and negatively rated arguments in brief statements, \noften quoting relevant passages directly, and assigns a general sentiment based \non the review’s overall tone. Because LLMs are designed to predict the most \nlikely next word, they are particularly adept at producing text that represents \nwhat an average person might interpret as the overall sentiment of a review. \nFor that reason, the model’s analysis is unlikely to be truly novel or especially \ninsightful, but its very averageness makes it a useful tool as a sanity check for \na human coder. \nOne disadvantage of this method is that it is not perfectly reproducible – \nwere we to run the exact same prompt on our reviews again, ChatGPT’s \nresponses would not be the exact same. Then again, neither would a human’s. \nWhile the exact wording would change, we did find that the overall sentiment \njudgments remained consistent in multiple runs of the same prompt and \nreview combination. \nLLM-only Sentiment Analysis    \nGiven the excellent results we had with ChatGPT’s analysis of the reviews, \none might think getting it to perform a one-word sentiment analysis would \nbe trivially easy. If ChatGPT can deliver well-argued and nuanced analysis of \na review’s overall sentiment, shouldn’t the labeling of reviews as “positive”, \n“mixed” or “negative” be a far simpler task? Unfortunately, that is not the \ncase. ChatGPT and other LLMs are surprisingly resistant to following strict \nrules in their output and remaining consistent. We experimented with several \ndifferent prompts trying to make the model behave in a dependable manner, \nto no avail. Changes could be minute – such as varying capitalization of the \nwords and random addition of punctuation after a word – to severe – going \noff on a verbose analysis of the review such as the ones that had been useful \nin the previous section, but were not at all what we are hoping for at this \nstage. \nAttempts to use open-source LLMs unfortunately had even worse results \nthan ChatGPT. HuggingChat, a free and open-source LLM developed by \nHuggingFace, was not only unable to keep results concise and consistent in \nform (with similar issues regarding unexpected verboseness and inconsistent \nwriting style), but the sentiments themselves proved to be inconsistent (Table \n1). Changing the wording of the prompt hardly helped to make the results \nmore formally consistent, and to make matters worse, the actual judgments \nwere also erratic, as one can see by comparing the results with the same \nreviews and a slightly reworded prompt. In Table 1, “prompt 1” was “The \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 12\nTable 1. HuggingChat’s inconsistent one-word sentiment analysis. \nID ID prompt 1 prompt 1 prompt 2 prompt 2 \n2MBUUSRR negative Mixed. \nL33Y2KDI mixed Negative \n7PQP3M9K Mixed. positive \nUSR5PKR8 positive/mixed positive \nfollowing is a film review: {text}. Assess the sentiment of this review in one \nword (positive, mixed, negative)”. Prompt 2 was “The following is a film \nreview: {text}. Assess the reviewer’s judgment of the film in a single word \n(positive, mixed, negative).” \nLexicon-based Sentiment Analysis of Reviews with VADER        \nConfronted with such poor results, we decided to undertake a lexicon-based \nsentiment analysis of the reviews. Though we had low expectations for the \nquality of the assessments of the reviews with this method, we knew that at \nleast its output would be consistent and reproducible, avoiding the pitfalls we \nhad encountered attempting to generate consistent one-word sentiments with \nLLMs. \nRooted in the field of Natural Language Processing (NLP), lexicon-based \nsentiment analysis is a popular technique for extracting the emotional \npolarity of text, typically assigned either a score of -1 to +1, or words such \nas positive, negative, or neutral (Wankhade et al. et al.; Rebora). The method \nrelies on predefined dictionaries of words and phrases that have associated \nsentiment scores. A text is thus analyzed as a collection of words, and the \nsentiment of the text as a whole is essentially an average of the individual \nwords it contains. To be fair to this method, the way the words are arranged \nin the text does affect their value. The VADER model takes into account, \nfor instance, if a word is in all caps (+ or - 0.733 of the word’s base \nvalue), punctuation (for instance, + or - 0.292 to the sentence’s score for \neach exclamation mark), and modifiers like negation, softening and contrast \n(vaderSentiment.py). For instance, in sentences containing the word “but”, \nsentiment-bearing words before the “but” have their valence reduced by 50%, \nwhile those after the “but” increase their base values by 50% (ibid.). \nThese solutions are ultimately not enough to fully grasp the nuance of \nlanguage. Lexicon-based models are known for their inability to capture \ncontextual intricacies, irony, and sarcasm (Maynard and Greenwood; Farias \nand Rosso). The lexicon-based model struggles in particular with a corpus \nsuch as ours, which contains horror films like Caligari and Nosferatu. A word \nlike “horrifying” is considered very negative by VADER (vader_lexicon.txt), \nwith a valence of -2.7 (individual words are measured in a scale of -4 to +4). \nBut in the context of a horror movie, that may well be a positive thing, as the \nfilm delivers the thrills it promises. \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 13\nOne may well argue that VADER was built to analyze Social Media Text \n(Hutto and Gilbert) and a different lexicon should be used to work on \nhistorical film reviews. A more tailored lexicon might slightly improve the \nquality of our results, but it would certainly not solve the overarching \nproblem of a lexicon-based approach when applied to nuanced texts, however. \nWhat to do, for instance, with words like horrific? When attached to \nsomeone’s acting or directing, it should rightly be judged as negative, after all. \nWe ultimately chose VADER because it is the most widely used open source \nmodel for sentiment analysis due to its simplicity, speed, interpretability and \nlack of token limits (allowing longer texts). We could thus not only run it \non our corpus with no fine-tuning, the same could be said for anyone who \nwanted to follow our model. \nWe applied VADER to our corpus of reviews, hoping to see just how accurate \nit could be when compared to our manual analysis.7\n The resulting polarity \nscores range from -1 to +1.8\n We also assigned sentiment values to those scores, \nin which values from -0.2 to +0.2 are mixed, while those below this band \nare negative, and above it are positive.9\n We considered increasing the mixed \ninterval in number because in practice, no scores were between these two \nnumbers in this application, but decided against it because it did not improve \nthe model’s accuracy. \nGiven the lack of mixed cases in practice in the VADER sentiments, we \nmeasured the model’s accuracy leniently: if the VADER sentiment matched \neither the original manual sentiment (negative, mixed or positive) or the \nmanual binary sentiment (negative or positive), we considered its judgment \nto be accurate.10\n We will go over the results of this method in depth when we \ncompare it to our better performing hybrid model, but suffice it to say for \nnow that the results of this direct application were unsatisfactory – VADER’s \ndirect sentiment analysis diverged from the human sentiment 27 times, that \nis, in 33.75 % of cases. \nHybrid Model: Lexicon-based Sentiment Analysis of Chat-GPT        \noutputs  \nAs previously discussed, we were impressed with the ability of LLMs, and \nparticularly ChatGPT, to work with the nuanced language of historical film \nreviews and output well-argued analyses of even mixed reviews to indicate \nwhether they leaned more towards positive or negative aspects. Yet, the raw \npower of LLMs proved difficult to harness in a consistent and reliable way for \nSee code in “sentiment_analysis.ipynb”. \nResults recorded in column “VADER_review_score”. \nSee column “VADER_review_sentiment”. \nSee code in “results_analysis.ipynb”. \n7 \n8 \n9 \n10 \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 14\na one-word sentiment analysis of each review. The employment of a lexicon-\nbased sentiment analysis had almost exactly the opposite characteristics: its \nresults were clean, consistent and reproducible – they just happened to be \nwrong a third of the time. \nGiven these contrasting sets of strengths and weaknesses, we devised an \ninnovative alternative approach, which combines the two methods for a \nfully automated sentiment analysis which has more accurate results, with \nan error rate of only 11.25%. We started from the realization that when \nwe used ChatGPT to support manual analysis, it would typically output \na relatively short text with declarative sentences which would accurately \ncondense the sentiment of the reviews. The LLM-generated responses often \ninclude concise statements like “This film review appears to be [positive, \nnegative, mixed],” and highlight the portions of the review which led it to \ncome to this assessment, removing all extraneous information. We speculated \nthat, when compared to the nuanced and complex language of the original \nreviews, the simplistic ChatGPT outputs would likely be much easier for \nlexicon-based sentiment analysis models like VADER to classify correctly. \nChatGPT’s output is also more similar to what VADER was trained to work \non – short texts in modern internet speech – compared to film reviews from \nthe 1920s. Essentially, this would be an indirect sentiment analysis of the film \nreviews: rather than running the VADER model on the reviews directly, we \nwould run it on the LLM-generated analysis of those reviews. This hybrid, \ntwo-step approach, would have most of the advantages of a lexicon-based \nsentiment analysis—systematic and fully automated—and would harness at \nleast some of the advantages of LLMs’ more nuanced grasp of language. \nOnce more, we tried to use HuggingChat for this portion,11\n particularly \nbecause we would have to pay12\n to run this number of reviews through \nChatGPT’s API, but found HuggingChat’s output to be subpar. The results \nto the prompt “The following is a film review. Would you characterize it as \nnegative or positive? {review_text}”13\n include responses like “I’m sorry, I am \nunable to understand your request. Can you please rephrase or provide more \ncontext?” (ID “DLNP7CTM”), which is why we abandoned this avenue. \nGiven these results, we again turned to ChatGPT. We did not use, as we had \nbefore, ChatGPT’s interface on the OpenAI website to run this portion of \nour experiment, utilizing its API instead to make this process fully automated \nthrough Python14\n. Since we wanted to maximize the accuracy of results and \nSee code “huggingChat_API_reviews.ipynb.” \nFor a small corpus like ours, this was ultimately not an issue, since running ChatGPT for all the tasks we used it for during the period of \nthis paper only cost US$0.32. \nThe results are recorded in the column “HuggingChat API Answers” of the Reviews Corpus table. \nSee code “GPT_API_all_reviews.ipynb.” To avoid cherry-picking, we ran this code only once, with the column “ChatGPT API Answers” of \nour corpus table containing the results. \n11 \n12 \n13 \n14 \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 15\navoid, to the best of our ability, that the sometimes dark subject-matter of \nthe films in question would generate falsely “negative” results, we applied \nthe ensuing prompt to all the reviews: “The following is a film review. Is it \nnegative or positive? Explain your reasons, and keep in mind that negative \nemotions may be warranted depending on the genre of the film, and do not \nnecessarily signify a negative review. Review: {text}”.15\n We ran this request on \nonly once, to avoid cherry-picking the best gene \nWe manually checked ChatGPT’s output and found that this prompt proved \neffective, producing well-reasoned summaries of each review’s overall \nsentiment. We subsequently ran the VADER model on the ChatGPT output, \nfollowing exactly the same steps as what had been used directly on the film \nreviews.16\n The analysis of the results will be discussed below, as part of a \ncomparison and evaluation of all the methods we utilized. \nEvaluation of Sentiment Analysis Methods      \nComparing the results of the two automated sentiment analysis methods \nleaves little question as to which is more accurate. With a deviation rate of \nonly 9 cases (11.25%) from the manual judgments, the VADER sentiment \nanalysis of ChatGPT outputs proved to be considerably more reliable than \nthe results of VADER when applied directly, which erred in 27 cases \n(33.75%). Most of VADER-only’s mistakes (17, or 62.9%) were due to \nincorrectly assigning a positive sentiment to a review that was originally \nmixed or negative (Figure 6). VADER-ChatGPT’s mistakes were much more \nbalanced, with five false positives and four false negatives. \nThe direct and indirect VADER analyses yield a score ranging from -1 for \nnegative to +1 for positive.17\n Both methods had a tendency towards extremes. \nThough we assigned a mixed score between -0.2 and +0.2, in practice, mixed \njudgments were never observed in the VADER-only analysis and occurred \nonly three times in the VADER-ChatGPT analysis. Interestingly, in all three \nof these cases, the manual judgment had indeed been categorized as mixed. In \nother words, not a single review was falsely categorized as mixed. \nExamination of Interpretation Discrepancies per film       \nIn total, there are 30 reviews out of 80 for which at least one of the \nautomated sentiments diverge from the manually annotated sentiment (either \nbinary or tertiary), that is, 37.5% of the time. In order to gain insight into the \nreasons behind the divergences, we will hone in on those 30 cases below. It is \nhelpful to closely examine the deviations within individual films and reviews. \nThe resulting output was saved in the column “ChatGPT API Answers”. \nSee code “sentiment_analysis.ipynb”. \nSee columns “VADER_review_score” and “VADER_ChatGPT_score”. \n15 \n16 \n17 \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 16\nFigure 6. Comparison of Automated Methods’ Accuracy. \nFigure 7. Heat Table of Deviating Results for Caligari. \nThrough the use of heatmap visualizations (Figures 7-9), we can immediately \nidentify cases where reviews are classified as negative (red), mixed (yellow), \nor positive (green). Numerical scores follow that scale in a gradient, with \nnormalized values from -1 to +1. \ncaligari  \nThere were 10 instances (26%) where VADER deviated from our manual \nanalyses of Caligari. As we shall see, this was the best result out of the \nthree films. The ChatGPT-VADER model only made two mistakes, and in \nboth cases VADER-only had made the same mistake. In the case of Caligari \nreviews, the direction of mistakes is fairly even – there were four false positives \nand 6 false negatives in the VADER-only model, and one of each for the two \nmistakes of the hybrid model (Figure 7). We will proceed to examine these \ntwo exceptional cases more closely. \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 17\nThe first case pertains to the review “Ein expressionistischer Film” by Herbert \nIhering, published on February 29, 1920, in the Berliner Börsen-Courier (ID \n“PJSEZGQ4”). Despite the manual encodings indicating a negative stance, \nboth the VADER-only and VADER-ChatGPT models assigned the review \na positive sentiment. This was one of the few cases in which we believe \nChatGPT indubitably made a mistake, rather than displaying an acceptable \ndivergence of judgment from ours, since the reviewer leaves little question \nas to his negative opinion of the film. Closely examining the review and \nChatGPT’s output, it becomes evident that statements originally framed in \na negative context, critiquing the untapped potentials of the medium and \ncinematic expressionism, were interpreted by the model in a positive light : \nChatGPT: \nThe review can be considered positive. The reviewer \nacknowledges the significance of expressionism in film and \npraises the film’s depiction of a “sane reality” being opposed by \nthe idea of “sick unreality”. […] \nThis is actually a reversal of the reviewer’s opinion, who criticizes the film’s \nuse of expressionism only because of its setting: \nIt is telling that Carl Mayer and Hans Janowitz rendered their \nphotoplay The Cabinet of Dr. Caligari expressionistically only \nbecause it is set in an insane asylum. It opposes the notion of \na sick unreality to the notion of a healthy reality. […] In other \nwords, insanity becomes the excuse for an artistic idea. \nIhering’s argumentation is intricate and he doesn’t belabor his points by \nstating them explicitly, which would help explain why the model struggled, \nbut it is clear in context that the author is a proponent of expressionism and \nfinds the film’s casual use of it as an aesthetic to signify madness offensive. \nThe second case of misattribution was of Roland Schacht’s critique published \non March 14, 1920, in the Freie Deutsche Bühne (ID “VNXHI9SG”), which \nwe manually coded as positive, but was assessed as negative by both VADER \nalone and VADER-ChatGPT. Interestingly, an inspection of ChatGPT’s \nresponse is closer to the mark, suggesting a mixed sentiment: \nChatGPT: \nBased on the information provided, it is difficult to determine \nwhether the film review is negative or positive. The review \nprimarily focuses on the plot and artistic elements of the film, \nsuch as the set design and performances. However, the reviewer \ndoes mention some disappointments with certain aspects of \nthe film’s narrative and stylization. Overall, the review seems \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 18\nto acknowledge the artistic significance of the film while also \nhighlighting its flaws. Therefore, it can be seen as a balanced \nreview rather than entirely negative or positive. \nA potential explanation for the undetected positive essence of the review \ncould lie in the fact that the author’s fundamental admiration for Caligari \nis somewhat obscured within the middle portion, following an extensive plot \nsummary. Given the film’s subject matter, the plot summary is littered with \nwords that VADER scores as negative, such as murderer (-3.6), horrible (-2.5) \nand death (-2.9) (vader_lexicon.txt). \nIt is also true that the reviewer makes some criticisms of the film, such as that \nits twist ending undermines the audience’s initial response, but he follows \nthat criticism with this rhapsody: \nBut this complaint is as good as negligible in view of the \nother great artistic significance of this film as a whole. For the \nfirst time, the film is fundamentally lifted out of the realm of \nphotography into the pure sphere of the work of art; for the \nfirst time, the emphasis is fundamentally placed not on the what \nof the brutal and exciting events, but on the how; for the first \ntime, not a vulgar illusionistic, but an artistic effect is striven \nfor. (697, ID “VNXHI9SG”) \nSchacht’s recurring rhetorical use of “for the first time” not only highlights \nCaligari’s novelty, but hails it as a watershed moment for film as an artform. \nThis level of effusiveness is a common feature in Caligari’s reviews, and is \nsadly lost in a process of sentiment analysis that weighs things simply as \n“positive”, “mixed” or “negative”. \nmetropolis  \nMetropolis proved to be the most challenging case for the model, with \ndeviations from the manual analyses in 12 out of 25 reviews, 48% (Figure \n8). The model seems to have struggled in particular with Metropolis because \nof the large number of mixed reviews (52%, see Figure 5) that skew negative \nin the binary assessment. Out of the mistakes the VADER-only model \nmade concerning Metropolis, they were all misidentifications of the review as \npositive. \nIt is little wonder that Metropolis was ultimately the film with the largest \nnumber of errors, since the movie garnered reviews that seem almost tailor-\nmade to challenge lexicon-based sentiment analysis models. The majority \nof Metropolis reviews commend various technical aspects of the film, but \nultimately condemn it for its poor plot and characters. A paradigmatic \nexample is offered by this excerpt from Donald Beaton’s review in The Film \nSpectator (ID “KIUF2ZGM”): \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 19\nFigure 8. Heat table of Deviating Results for Metropolis. \nTechnically, Metropolis is a great picture. The sets were \nmarvelous, and the mob direction was good. The whole picture \nshowed wonderfully painstaking care in the production, but \ngood production is not enough to make a good picture. \nThis sentence presents four positive aspects, yet the significance of the \nnegative aspect clearly outweighs them. Lexicon-based sentiment analysis \nis fundamentally incapable of capturing such nuance, since it offers the \nsentiment of a text by adding up – with some small adjustments – the \nindividual sentiment scores of words and then averaging them out. In a \ncase like Beaton’s review, the high quantity of positive words will ultimately \ndeliver a positive result, despite the author’s clear negative feelings about the \npicture as a whole. \nIn Figure 8, we can see that both the VADER-only method and the hybrid \napproach wrongly categorized Beaton’s rather scathing review as positive. \nIf we inspect ChatGPT’s output, however, we find it to be a remarkably \naccurate summary of Beaton’s – mostly negative – thoughts: \nThe review is negative. The reviewer acknowledges the technical \naspects of the film, such as the marvelous sets and good \ndirection, but criticizes the lack of story and mediocre acting. \nThe reviewer also states that the idea of the film was wrong and \npoints out inconsistencies in the plot. They ultimately attribute \nthe credit for the film’s success to the technicians and the person \nresponsible for the set design and machinery. \nIt is worth reiterating that both of the methods we employed use VADER, so \nthe flaws of a lexicon-based sentiment analysis are present in both of them. \nOur implementation of ChatGPT as an intermediate step served simply to \nalleviate this problem of naive quantification, but it did not solve it. It is \nworth noting, however, that the hybrid model did manage to lower the \nscore of Beaton’s review from an astonishingly high 0.98 to a moderately \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 20\nFigure 9. Heat Table of Deviating Results for Nosferatu. \npositive 0.50, and similar reductions happened in other cases (Beaton, ID \n“KIUF2ZGM”; Anonymous, “Production”, ID “EIRLBFNR”; Haas, ID \n“AXSMZ7XF”). \nA closer inspection of these instances show that ChatGPT’s responses were \nalso mischaracterized by VADER, as in all three cases ChatGPT correctly and \nunambiguously categorized the reviews as negative. The chatbot’s analysis of \nthe other two reviews where the hybrid model diverged from the manual \nencoding (Bartlett apud Minden and Bachmann ID “D4AR9KYS”; Barry, \nID “CZWZ5UV7”) still display a level of nuance that makes calling them \n“incorrect” feel disingenuous. While their judgments differ from our manual \njudgments, we do not substantially disagree with the LLM’s outputs, finding \nthat they could well have been written by another human who simply \nweighed the reviews slightly differently than we did. \nnosferatu  \nWith 8 out of 17 (47%), almost half of film reviews for Nosferatu were \ninaccurately assessed by one of the two models (Figure 9). As was the case \nwith Metropolis, this mostly pertains to reviews classified as mixed in our \noriginal manual analysis. \nInterestingly, the misassessments of the automated methods do not align in \nany of the cases. While VADER incorrectly labeled four reviews as negative \nand two reviews as positive, VADER-ChatGPT identified two reviews as false \nnegatives. However, in both of those cases, the initial manual analysis was \noriginally mixed and was only categorized as positive through the enforced \nbinary classification. \nIn contrast to the errors in Metropolis, most misidentifications (6 out of 8) \nin the reviews of Nosferatu erred on the negative side. Admittedly, even those \nreviews often carried significantly negative comments (which is why so many \nof them had initially been labeled as mixed). Upon closer examination of \nthe two outcomes marked as incorrect negatives in the VADER-ChatGPT \nsentiment analysis, the intricacies of each case become evident. This \ncomplexity is also intertwined with the intended readership of each review. \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 21\nIn the review “Film Reviews - Nosferatu the Vampire,” published on \nDecember 12, 1929, in Variety (ID “5YA546KU”), the anonymous author \nconveys a combination of sentiments regarding the anticipated success of \nNosferatu. On the one hand, the review highlights the film’s directorial and \nartistic aspects, primarily crediting Murnau’s directing talent and consistently \nattributing the “[s]killfully mounted and directed” (26) work to him: \nMurnau proved his directorial artistry in “Sunrise” for Fox \nabout three years ago,18\n but in this picture he’s a master artisan \ndemonstrating not only a knowledge of the subtler side of \ndirecting but in photography. One shot of the sun cracking at \ndawn is an eye filler. Among others of extremely imaginative \nbeauty is one which takes in a schooner sailing in a rippling \nstream photographed in such a manner that it has the Illusion \nof color and an enigmatic weirdness that’s more perplexing than \nthe ghost action of the players. (Ibid.) \nOn the other hand, the review also stresses some shortcomings, especially \nconcerning titling, acting, and the adaptation of Bram Stoker’s Dracula for \nthe screenplay. It characterizes Nosferatu as “a depressive piece of art,” and \nraises concerns about its potential unsuitability for mainstream theatrical \naudiences. One aspect that must be considered when analyzing (historical) \nfilm reviews is that not all of them have the same objectives. This review \nwas published in Variety, a trade journal, and its language, particularly its \nemphasis on being a “risky exhibit,” (26) appears to be directed specifically at \ncinema owners. \nIn our manual judgment, we found this review to be mixed, but leaning \npositive when pressed because the reviewer does emphasize the film’s artistic \nmerits and overall quality. It is interesting to note that this review was so \nambivalent that ChatGPT’s itself went back and forth on its judgment. Its \nassessment of the review when we used it as an assistant for the binarization \nprocess19\n was that the review was mostly positive – a judgment we agreed with \nat the time. Ultimately, this case is so ambivalent that both of its responses are \nin the realm of acceptable subjective divergence which another human coder \nmight also make. \nThough Nosferatu was released in Germany in 1922, it was only released in the US in 1929, buoyed in part by Murnau’s success with his \nAmerican-made Sunrise in 1927. By the time of Nosferatu’s release in the US, sound film had already begun taking over the screens, which \ncertainly did not help the film’s American reviews, which mostly - as is the case here - seemed unaware that it was an older film and found it \nto be a step back for the director. \nSee column “ChatGPT Binary Answers”. \n18 \n19 \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 22\nIn the second instance where VADER-ChatGPT incorrectly attributed a \nnegative sentiment, the distinction between positive and negative attitudes \nwas also unclear. Robert Desnos’ published this snippet of a review of \nNosferatu in May 1929 in the journal La Revue du cinéma (ID \n“CEHEIWR7”): \nHowever, an admirable film, far superior to Caligari in terms \nof direction, went unnoticed: Nosferatu the Vampire, where no \ninnovation was arbitrary, and where everything was sacrificed to \npoetry and nothing to art. But, driven by the will of directors \noblivious to their role, German artists, obliged to fill an absurd \nlandscape with derisory gestures, soon gave them ridiculous \nimportance. \nOverall, the reviewer has a positive view of Nosferatu in terms of direction \nand its focus on poetry but criticizes the perceived lack of innovation and \nthe actions of German artists in the film. What carries more weight in \nthis context, the positive or the negative statements? And what serves as \nan indicator? In the manual analysis, the tendency towards positivity is \nattributed to the first sentence, incorporating markers like “[h]owever” and \n“went unnoticed,” which aims to communicate to readers that the film was \nundeservedly overlooked by a wider audience – similar to what’s stated in the \nVariety review. The follow-up sentence, however, is undeniably negative. At \nfirst glance, this seems to be a critique of Nosferatu, but careful consideration \nindicates that the criticism is targeted at other German films in the Caligari \nmold, which the author dislikes for (unlike Nosferatu) allegedly displaying \narbitrary innovation and absurd landscapes. This level of intricate \nargumentation is even difficult for a human to parse, so it is little wonder that \nChatGPT struggled with it. \nConclusion  \nAs we demonstrated above, determining the sentiment of intricate and \nnuanced film reviews can be a challenge, even for human experts with \ndomain knowledge. This underscores the limitations of adopting a simplistic \n(binary) approach to sentiment. Film scholars used to qualitative research \nmay well question the utility of such an endeavor. We find that the value \nis two-fold. The first is the testing of hypotheses that have already been \ndeveloped with close reading, so as to provide stronger evidence in favor of \nthem, or as to refute them. For instance, most people with even a passing \nknowledge of Metropolis’ contemporary reception could tell you that critics \nfound the film spectacular in its technical aspects, but ridiculous in its plot \nand social message. Establishing that 52% of Metropolis reviews were mixed \nis but a confirmation of that – mundane, perhaps, but still worthwhile. This \napplication certainly feels more useful when there is scholarly debate about a \nfilm’s reception, such as in the case of Caligari, sketched in the introduction. \nOur analysis showed that though not unanimous, the film was mostly (76%) \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 23\nreceived with glowing reviews, and the positive reception could be seen across \ncountries and publication types. The second – and more important – value \nof distant reading methods is when it comes to large corpora. This would \nallow us to get away from the canon and make judgments about films that \nfew – if any – scholars would otherwise dedicate themselves to researching \nthrough the use of close reading. Given a clean corpus, we could answer, with \na reasonable degree of certainty, large questions such as “were German movies \nwell received internationally?” and we could break down their reception in \nseveral countries without focusing only on a small and skewed sample of \nfilms. \nIt is important to acknowledge that access to large datasets of clean historical \nsources that can be seamlessly integrated in such automated processes can \nbe the tallest hurdle when it comes to implementing such methods. Though \ndigital methods in general and LLMs in particular are routinely touted for \ntheir potential time-saving advantages, this is often a false promise. While \nmost other papers that undertake sentiment analysis of film reviews utilize \nborn-digital corpora like IMDb reviews, we went through considerable \nlengths to (manually) obtain, clean and categorize digitized historical sources \nthat were diverse in language, country of origin and publication type. \nIdentifying the correct films in full-text search, distinguishing between mere \nmentions of films, advertisements, and actual reviews, defining the \nboundaries of individual reviews on the page and handling the quality issues \nof OCR were all extremely time-consuming, even for our ultimately small \ncorpus of 60 reviews. \nWe believe that current technological developments in the realm of machine \nlearning will continue to yield gains in those areas, increasing, for instance, \nthe quality of OCR and facilitating the identification of texts related to \nparticular movies. These developments are not only helpful for quantitative \nanalysis (sometimes wrongly seen as synonymous with “digital methods”), \nbut also for qualitative analysis. In addition to better quality of textual \ndata and search capabilities, close reading itself is a method that can gain \nfrom interaction with LLMs. We were impressed with ChatGPT’s ability to \nreliably summarize the main arguments of a review, and felt that even in the \ninstances in which we disagreed with its responses, it was not out of the realm \nof possibility for a human to have reached those same conclusions. Although \nLLMs are unlikely to offer entirely novel or exceptionally insightful analyses, \nthey can serve as valuable sparring partners and as easily implementable \ninter-rater reliability measures. While at the point of writing open source \nalternatives like HuggingChat did not yet match the capabilities of private \nmodels, we are optimistic about the potential for such models to improve \nover time. \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 24\nDespite the impressive capabilities of LLMs – proprietary or not – to handle \nnuanced text, these models proved themselves to be unreliable for tasks \nrequiring consistency and reproducibility. When it comes to sentiment \nanalysis, lexicon-based models deliver such consistency, but are unfortunately \nhighly inaccurate. The contrasting advantages and disadvantages of these two \nmethods led us to propose a combination of the two, running a lexicon-\nbased model not on the film reviews directly, but on the analysis of those \nreviews made by ChatGPT. It is worth reiterating that our method is still \nultimately a lexicon-based model, so the weaknesses of that approach are still \nvery much present. The model struggles in particular with a corpus such as \nours, which contains film reviews with an abundance of negatively coded \nwords (“murder”, “horrifying,” etc.) without them necessarily indicating a \nnegative review of the film. This is likely one of the reasons why Metropolis \n– a visually dazzling science fiction film – had so many reviews that were \nwrongly identified as positive, in contrast to the other two movies, which are \npioneering examples of the horror genre. \nDespite the challenges provided by such a corpus, our hybrid model, \ncombining ChatGPT’s nuanced handling of language with lexicon-based \nsentiment analysis, delivered respectable results. We believe that combining \nmachine learning methods with symbolic (human-readable) programming is \na promising avenue when it comes to automating complex tasks, and we \nexpect further improvements in this area in the coming years., While there \nare inherent challenges associated with adopting a digital approach to the \nanalysis of historical film-related texts, we firmly believe that the benefits it \noffers are substantial. While not mandatory, the use of digital methods for \nanalyzing film reviews holds the potential to significantly enhance the quality \nof evidence, especially when dealing with larger datasets. It’s important to \nnote that these digital methodologies are not meant to replace qualitative \napproaches, but to complement them. By broadening our scope to include \na more diverse range of film reviews, extending beyond the foundational \nelements of the Weimar film canon, we can more effectively address specific \ncomparative questions. \nFurthermore, the process of constructing and sharing an explicit corpus \nhas compelled us to be exceptionally precise and transparent about our \nmethodology – a level of detail not commonly encountered in the writing \nof qualitative historical papers. Although this paper primarily focuses on \ngenerating quantitative insights, such as the percentage of positive reviews, \nthe sharing of our complete dataset and code enables others to critique and \nleverage our work for various purposes, including qualitative analyses. We \nhope the methodology outlined in this article will serve as a framework for \nthose interested in automated analysis of film reviews and inspire further \nresearch at the intersection of digital methods and film history. \nData repository: https://doi.org/10.7910/DVN/8NINQK \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 25\nSubmitted: March 20, 2024 EST. Accepted: May 21, 2024 EST. Published: July 18, 2024 EST. \nThis is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 \nInternational License (CCBY-4.0). View this license’s legal deed at http://creativecommons.org/licenses/\nby/4.0 and legal code at http://creativecommons.org/licenses/by/4.0/legalcode for more information. \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 26\nworks cited \nAcland, Charles R., and Eric Hoyt. The Arclight Guidebook to Media History and the Digital \nHumanities. Reframe, 2016. \nAmulya, K., et al. “Sentiment Analysis on IMDB Movie Reviews Using Machine Learning and \nDeep Learning Algorithms.” 2022 4th International Conference on Smart Systems and Inventive \nTechnology (ICSSIT), IEEE, 2022, pp. 814–19. DOI.org (Crossref), https://doi.org/10.1109/\nICSSIT53264.2022.9716550. \nAndriopoulos, Stefan. “Suggestion, Hypnosis, and Crime: Robert Wiene’s The Cabinet of Dr. \nCaligari (1920).” Weimar Cinema: An Essential Guide to Classic Films of the Era, edited by Noah \nIsenberg, Columbia University Press, 2009, pp. 13–32. \nAnonymous. “Film Reviews - Nosferatu the Vampire.” Variety, vol. XCVII, no. 11, Dec. 1929, pp. \n26–30. \n---. “Production Weak in Human Qualities.” The Film Spectator, vol. 4, no. 1, Sept. 1927, p. 5. \nArnold, Taylor, and Lauren Tilton. “Distant Viewing: Analyzing Large Visual Corpora.” Digital \nScholarship in the Humanities, vol. 34, no. Supplement_1, Dec. 2019, pp. i3–16. DOI.org \n(Crossref), https://doi.org/10.1093/llc/fqz013. \nAscari, Maurizio. “The Dangers of Distant Reading: Reassessing Moretti’s Approach to Literary \nGenres.” Genre, vol. 47, no. 1, Apr. 2014, pp. 1–19. DOI.org (Crossref), https://doi.org/10.1215/\n00166928-2392348. \nBarlow, John D. German Expressionist Film. Twayne, 1982. \nBarry, Iris. “The Cinema: Metropolis.” The Spectator, no. 540, 26 Mar. 1927. \nBeaton, Donald. “As They Appeal to a Youth.” The Film Spectator, vol. 4, no. 4, Oct. 1927, p. 17. \nBukar, Umar, et al. “Text Analysis of Chatgpt as a Tool for Academic Progress or Exploitation.” \nPreprint, SSRN, 2023. DOI.org (Crossref), https://doi.org/10.2139/ssrn.4381394. \nChen, Cheng, et al. “Sentiment Analysis of Animated Film Reviews Using Intelligent Machine \nLearning.” Computational Intelligence and Neuroscience, edited by Qiangyi Li, vol. 2022, July \n2022, pp. 1–8. DOI.org (Crossref), https://doi.org/10.1155/2022/8517205. \nCiotti, Fabio. “Distant Reading in Literary Studies: A Methodology in Quest of Theory.” Testo e \nSenso, no. 23, Dec. 2021, pp. 195–213. \nDenbo, Seth, and Neil Fraistat. “Diggable Data, Scalable Reading and New Humanities \nScholarship.” 2011 Second International Conference on Culture and Computing, 2011, pp. \n169–70. IEEE Xplore, https://doi.org/10.1109/Culture-Computing.2011.49. \nDesnos, Robert. “Scénarios.” La Revue du cinéma, no. 3, May 1929. \nElsaesser, Thomas. Weimar Cinema and After: Germany’s Historical Imaginary. Digital Edition, \nRoutledge, 2009. \nFarias, D. I. Hernández, and P. Rosso. “Irony, Sarcasm, and Sentiment Analysis.” Sentiment \nAnalysis in Social Networks, Elsevier, 2017, pp. 113–28. DOI.org (Crossref), https://doi.org/\n10.1016/B978-0-12-804412-4.00007-3. \nFlueckiger, Barbara, and Gaudenz Halter. “Methods and Advanced Tools for the Analysis of Film \nColors in Digital Humanities.” Digital Humanities Quarterly, vol. 14, no. 4, 2020, https://\ndoi.org/10.5167/uzh-197994. \nHariri, Walid. Unlocking the Potential of ChatGPT: A Comprehensive Exploration of Its Applications, \nAdvantages, Limitations, and Future Directions in Natural Language Processing. 2023. DOI.org \n(Datacite), https://doi.org/10.48550/ARXIV.2304.02017. \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 27\nHowanitz, Gernot. “Distant Waching. Ein quantitativer Zugang zu YouTube-Videos.” Proceedings \nof the 2nd Annual Conference of the Association ‘Digital Humanities Im Deutschsprachigen Raum’ \n(DHd 2015), Graz, 23.–27 Feb. 2015. \nHoyt, Eric. Ink Stained Hollywood: The Triumph of American Cinema’s Trade Press. University of \nCalifornia Press, 2022. \nHutto, C., and Eric Gilbert. “VADER: A Parsimonious Rule-Based Model for Sentiment Analysis \nof Social Media Text.” Proceedings of the International AAAI Conference on Web and Social \nMedia, vol. 8, no. 1, May 2014, pp. 216–25. ojs.aaai.org, https://doi.org/10.1609/\nicwsm.v8i1.14550. \nHutto, C. J. vaderSentiment/Vader_lexicon.Txt. 1 Apr. 2022, https://github.com/cjhutto/\nvaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt. \nIhering, Herbert. “Der Metropolisfilm - Ufa-Palast am Zoo.” Berliner Börsen-Courier, Late edition, \n11 Jan. 1927, p. 2. \n---. “Der Metropolisfilm - Ufa-Palast am Zoo (Vorbericht).” Berliner Börsen-Courier, Early edition, \n11 Jan. 1927, p. 5. \n---. “Ein expressionistischer Film.” Berliner Börsen-Courier, Early edition, no. 101, 29 Feb. 1920, p. \n8. \nKaes, Anton, et al., editor. The Weimar Republic Sourcebook. 1., Univ. of California Press, 1995, \nhttps://doi.org/10.1525/9780520909601. \nKracauer, Siegfried. From Caligari to Hitler: A Psychological History of the German Film. Edited by \nLeonardo Quaresima, Princeton University Press, 2004. \nLandry, Lionel. “Le Cabinet du Docteur Caligari.” Cinéa, no. 27, Nov. 1921, p. 8. \nLu, Kui, and Jiesheng Wu. “Sentiment Analysis of Film Review Texts Based on Sentiment \nDictionary and SVM.” Proceedings of the 2019 3rd International Conference on Innovation in \nArtificial Intelligence, ACM, 2019, pp. 73–77. DOI.org (Crossref), https://doi.org/10.1145/\n3319921.3319966. \nMaas, Andrew L., et al. “Learning Word Vectors for Sentiment Analysis.” The 49th Annual Meeting \nof the Association for Computational Linguistics: Human Language Technologies, 2011, https://\nai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf. \nMaynard, Diana, and Mark Greenwood. “Who Cares about Sarcastic Tweets? Investigating the \nImpact of Sarcasm on Sentiment Analysis.” Proceedings of the Ninth International Conference on \nLanguage Resources and Evaluation (LREC’14), European Language Resources Association \n(ELRA), 2014, pp. 4238–43, http://www.lrec-conf.org/proceedings/lrec2014/pdf/67_Paper.pdf. \nMinden, Michael, and Holger Bachmann, editors. Fritz Lang’s Metropolis: Cinematic Visions of \nTechnology and Fear. Camden House, 2002. \nMoretti, Franco. Distant Reading. Verso, 2013. \nOpen Syllabus. “OS Movie Lab.” Open Syllabus, 2021, https://blog.opensyllabus.org/movie-lab/. \nOrndorff, Marguerite. “[25] METROPOLIS (Paramount).” The Educational Screen, vol. VII, no. 2, \nApr. 1928, p. 68. \nPrimorac, A., et al. “Distant Reading Two Decades on: Reflections on the Digital Turn in the \nStudy of Literature.” Digital Studies/Le Champ Numerique, 2023. ora.ox.ac.uk, https://\nora.ox.ac.uk/objects/uuid:acbba668-71d1-4f8e-9cfc-fda801214682. \nRebora, Simone. “Sentiment Analysis in Literary Studies. A Critical Survey.” Digital Humanities \nQuarterly, vol. 017, no. 2, July 2023. \nRobinson, David. Das Cabinet des Dr. Caligari. BFI Pub, 1997. \nSchacht, Roland. “Caligari.” Freie Deutsche Bühne (Berlin), no. 29, 14 Mar. 1920, pp. 695–98. \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 28\nShaukat, Zeeshan, et al. “Sentiment Analysis on IMDB Using Lexicon and Neural Networks.” SN \nApplied Sciences, vol. 2, no. 2, Feb. 2020, p. 148. DOI.org (Crossref), https://doi.org/10.1007/\ns42452-019-1926-x. \nShen, Yongliang, et al. HuggingGPT: Solving AI Tasks with ChatGPT and Its Friends in Hugging \nFace. 2023. DOI.org (Datacite), https://doi.org/10.48550/ARXIV.2303.17580. \nSight and Sound. “The 100 Greatest Films of All Time.” BFI, 28 June 2021, https://\nwww.bfi.org.uk/sight-and-sound/polls/greatest-films-all-time-2012. \nSudirjo, Frans, et al. “Application of ChatGPT in Improving Customer Sentiment Analysis for \nBusinesses.” Jurnal Teknologi Dan Sistem Informasi Bisnis, vol. 5, no. 3, July 2023, pp. 283–88. \nDOI.org (Crossref), https://doi.org/10.47233/jteksis.v5i3.871. \nThompson, Kristin. “Dr. Caligari at the Folies-Bergère, or, The Successes of an Early Avant-Garde \nFilm.” The Cabinet of Dr. Caligari: Texts, Contexts, Histories, edited by Mike Budd, Rutgers \nUniversity Press, 1990. \nTopal, Kamil, and Gultekin Ozsoyoglu. “Movie Review Analysis: Emotion Analysis of IMDb \nMovie Reviews.” 2016 IEEE/ACM International Conference on Advances in Social Networks \nAnalysis and Mining (ASONAM), 2016, pp. 1170–76. IEEE Xplore, https://doi.org/10.1109/\nASONAM.2016.7752387. \nTsivian, Yuri, and Gunars Civjans. Cinemetrics: Movie Measurement and Study Tool Database. \n2005, https://cinemetrics.uchicago.edu/database.php. \nUnderwood, Ted. “A Genealogy of Distant Reading.” Digital Humanities Quarterly, vol. 011, no. \n2, June 2017. \nWankhade, Mayur, et al. “A Survey on Sentiment Analysis Methods, Applications, and Challenges.” \nArtificial Intelligence Review, vol. 55, no. 7, Oct. 2022, pp. 5731–80. Springer Link, https://\ndoi.org/10.1007/s10462-022-10144-1. \nZiems, Caleb, et al. Can Large Language Models Transform Computational Social Science? 2023. \nDOI.org (Datacite), https://doi.org/10.48550/ARXIV.2305.03514. \nRevisiting Weimar Film Reviewers’ Sentiments: Integrating Lexicon-Based Sentiment Analysis with Large Language Models\nJournal of Cultural Analytics 29"
}