{
  "title": "Exploring the applicability of large language models to citation context analysis",
  "url": "https://openalex.org/W4402775249",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Nishikawa, Kai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3173775181",
      "name": "Koshiba Hitoshi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2908693395",
    "https://openalex.org/W2019753053",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2057704743",
    "https://openalex.org/W4383605161",
    "https://openalex.org/W4389519042",
    "https://openalex.org/W1602102908",
    "https://openalex.org/W4292337090",
    "https://openalex.org/W4361807267",
    "https://openalex.org/W3173450924",
    "https://openalex.org/W2808556605",
    "https://openalex.org/W4387846655",
    "https://openalex.org/W4323697401",
    "https://openalex.org/W4206950723",
    "https://openalex.org/W4389523908",
    "https://openalex.org/W2801161192",
    "https://openalex.org/W4364320763",
    "https://openalex.org/W2036868766",
    "https://openalex.org/W3133726972",
    "https://openalex.org/W4322495771",
    "https://openalex.org/W4393746812",
    "https://openalex.org/W4379255800",
    "https://openalex.org/W4385819901",
    "https://openalex.org/W4389523957",
    "https://openalex.org/W4366850587",
    "https://openalex.org/W4402580364",
    "https://openalex.org/W4379539276",
    "https://openalex.org/W2975166839",
    "https://openalex.org/W4247957072",
    "https://openalex.org/W3197503028",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3128323043",
    "https://openalex.org/W2136213468",
    "https://openalex.org/W4386875911"
  ],
  "abstract": null,
  "full_text": "Exploring the applicability of Large Language Models\nto citation context analysis\nKai NISHIKAWA1,2 ∗ Hitoshi KOSHIBA2\n1 Institute of Library, Information and Media Science, University of Tsukuba\n2 National Institute of Science and Technology Policy (NISTEP)\nAbstract\nUnliketraditionalcitationanalysis—whichassumesthatallcitationsinapaperareequivalent—\ncitation context analysis considers the contextual information of individual citations. However,\ncitation context analysis requires creating large amounts of data through annotation, which hinders\nthe widespread use of this methodology. This study explored the applicability of Large Language\nModels (LLMs) —particularly ChatGPT— to citation context analysis by comparing LLMs and\nhumanannotationresults. TheresultsshowthattheLLMsannotationisasgoodasorbetterthanthe\nhumanannotationintermsofconsistencybutpoorintermsofpredictiveperformance. Thus,having\nLLMsimmediatelyreplacehumanannotatorsincitationcontextanalysisisinappropriate. However,\nthe annotation results obtained by LLMs can be used as reference information when narrowing the\nannotation results obtained by multiple human annotators to one, or LLMs can be used as one of\nthe annotators when it is difficult to prepare sufficient human annotators. This study provides basic\nfindings important for the future development of citation context analyses.\nkeywords:\nScientometrics, Citation Context Analysis, Annotation,\nLarge Language Model (LLM), ChatGPT\n∗Email: knishikawa@slis.tsukuba.ac.jp\n1\narXiv:2409.02443v2  [cs.DL]  10 Sep 2024\n1 Introduction\nQuantitative analysis focusing on citation relationships among papers assumes that all citations are\nessentiallyandimplicitlyequivalent[e.g.,BornmannandDaniel,2008,Dingetal.,2014,Lin,2018].\nIn contrast, citation context analysis has been proposed to consider the contextual information of\nindividualcitations,suchasthelocationofthecitationandthesemanticcontentofthetextcontaining\nthe citation. Although citation context analysis is expected to provide complementary findings to\nthe traditional quantitative citation analysis, it has the drawback that the cost of creating the data\nnecessary for the analysis is significant. Therefore, it is difficult to conduct studies that require a\nlarge amount of data, for example, analyzing differences in citation context trends among multiple\ndisciplines.\nIn the citation context analysis, data are created by determining the contextual characteristics of\neach citation using the text surrounding the citation in the citing paper. There are two ways to create\ndata: manual data processing, in which a human annotator manually creates data, and automatic\ndata processing, in which data are created using machine learning and other techniques [Tahamtan\nand Bornmann, 2019]. However, because the latter method often uses supervised learning—which\nrequirestrainingdata—itisnecessarytocreatelargedatasetsusinghumanannotators. Thehighcost\nof this annotation work is an obstacle to the development of citation context analysis.\nHowever, with the recent development of Large Language Models (LLMs) such as GPT [Brown\net al., 2020a], some studies have attempted to perform general annotation tasks on behalf of human\nannotators [e.g., Dorantes-Gilardi et al., 2022, He et al., 2023, Pangakis et al., 2023, Reiss, 2023,\nRytting et al., 2023]. These studies clarify that LLMs sometimes outperform human annotators\nhired through crowdsourcing and can produce more data in a more time- and cost-efficient manner.\nHowever, the performance of LLMs annotation varies depending on the specific task, even for the\nsame text classification, and it is not necessarily clear whether LLMs can immediately automate the\nannotation process.\nTo the best of our knowledge, no study has focused on whether LLMs can substitute for human\nannotatorsinscientificpapers. Becauseascientificpaperisaspecializedtextwithitsownformatting\nandwritingstyleandcontainsalargeamountofspecializedterminology,annotationsseemdifferent\nfrom general annotations that can be easily crowdsourced, as focused on in previous studies. In\nfact,incitationcontextanalysis,annotationsareoftenperformedbyresearchersorgraduatestudents\nemployed as research assistants (RA), who are accustomed to reading articles and are required to be\nfamiliar with a schema and manual for annotation through a certain amount of training. Therefore,\nit is unclear whether the findings of previous studies can be applied to the citation context analysis.\nThis study aims to explore the applicability of LLMs to citation context analysis. Specifically,\nwewillexaminethefollowingbyhavingLLMsperformannotationtaskssimilartothoseperformed\nby human annotators in Nishikawa [2023], a previous study on citation context analysis:\n1. Can LLMs replace humans for annotations in citation context analysis?\n2. How can LLMs be effectively utilized in citation context analysis?\nThe results of this study indicate that the annotation results of LLMs are comparable to or better\nthanthoseofhumansintermsofconsistencybutpoorintermsofpredictiveperformance. Therefore,\nit is not appropriate to allow current LLMs to perform annotations associated with citation context\nanalysis on behalf of humans. However, the annotation results obtained by LLMs can be used as\nreferenceinformationwhennarrowingtheannotationresultsobtainedbymultiplehumanannotators\nto one, or LLMs can be used as one of the annotators if securing a sufficient number of human\nannotators is difficult. This study provides basic findings important for the future development of\ncitation context analyses.\nInthefollowingsection,weprovidealiteraturereviewfollowedbythemethodsandresultsofthe\nexperiments. Subsequently,basedontheexperimentalresults,wediscusswhetherLLMscanreplace\nhuman annotators. Next, we examine whether LLMs can be applied to citation context analysis\nbeyond replacing humans. Finally, we present our conclusions.\n2\n2 Literature Review\n2.1 Citation Context Analysis\nCitation context analysis is also referred to as citation content analysis. Although some studies\ndistinguish between both [e.g., Tahamtan and Bornmann, 2019], we use the term citation context\nanalysis as including citation content analysis. When conducting citation context analysis, the first\nstepistosetaschemathatdefinesthecategorizationofcitations. Categoriesandpossiblevalues(also\ncalled classes) for each category are often set arbitrarily by researchers according to their research\npurposes. Zhang et al. [2013] divided these into syntactic and semantic categories, with the former\nrepresented by citation location and the latter by citation purpose (also called citation function or\ncitation motivation) and citation sentiment.\nNext, a dataset is created by classifying the citations to be analyzed based on the schema. This\nstage corresponds to a task known as annotation, coding, or citation classification. As mentioned\npreviously, there are two dataset creation methods: human annotators (coders), machine learning,\nand other techniques [Tahamtan and Bornmann, 2019]. The former is a costly method in terms of\nboth time and money, whereas supervised learning is often used in the latter, especially for semantic\ncategories that require human annotation [Iqbal et al., 2021]. In addition, the distribution of classes\nhas been reported to be highly skewed for many categories [e.g., Kunnath et al., 2023, Nishikawa,\n2023], which is another reason for the need for larger datasets for analysis.\nFinally, the created dataset is analyzed for individual research purposes. However, because both\nhumanandmachinemethodsrequirecostlyhumanannotations,studiesrequiringlargedatasets,such\nascomparisonsofcitationrelationshipsamongmultipledisciplines,arenotwelldeveloped. Thefew\nexceptionsthatmakeinter-disciplinecomparisonsareachievedbylimitingthenumberofcategories,\ntopics, or disciplines focused on [Chang, 2013, Lin, 2018, Zhang et al., 2021, Wang et al., 2021,\nNishikawa,2023]. Inotherwords,thecostofannotationmustbereducedtoallowamoreflexibleor\nlarge-scale study design for citation context analysis.\n2.2 Annotation by LLMs\nAnnotationreferstothetextclassificationinnaturallanguageprocessingtasks. Manypreviousstudies\nhavecomparedtheresultsofmultiplemodelsandversionsofLLMstoevaluatetheirperformancein\ntextclassification[cf.,Changetal.,2023]. SeveralstudieshavefocusedonthepotentialofLLMsas\nsubstitutesforannotation,comparingtheannotationresultsobtainedbymultiplemodelsandversions\nof LLMs [He et al., 2023, Kuzman et al., 2023, Pangakis et al., 2023, Reiss, 2023], or more directly\ncomparing human and LLMs annotations [Dorantes-Gilardi et al., 2022, He et al., 2023, Rytting\net al., 2023]. The specific tasks performed in these studies are as diverse as classifying topics for\nsocial networking posts [Dorantes-Gilardi et al., 2022], classifying websites as news [Reiss, 2023],\nand classifying article genres for news article headlines [Rytting et al., 2023]. In studies comparing\nhuman and LLM annotations, several texts that are the subject of these tasks are common and do\nnot require expertise to read. Therefore, crowd workers are often employed as human annotators in\nadditiontotrainedannotators,suchasgraduatestudents,whenmakingcomparisonsbetweenhumans\nand LLMs.\nAlthough the cost of annotation is significantly lower with LLMs than with crowd workers\n[Dorantes-Gilardi et al., 2022], the question remains as to whether the quality of annotation results\nwithLLMsishighenoughtobeusedforanalysis. Theresultsofannotationtasksareoftenevaluated\nin terms of their consistency (also known as reliability) and prediction performance. Inter-coder\nagreementisoftenusedasaconsistencymetric,whereasaccuracyorF1isoftenusedasaperformance\nmetric. The findings of previous studies indicate that LLMs are superior or comparable to human\nannotators, particularly cloud workers, in terms of consistency and performance. Meanwhile, it has\nalso been noted thatthe consistency and performance of LLMsannotation work can vary depending\non the attributes of texts and categories [Pangakis et al., 2023, Reiss, 2023]. Thus, no consensus\nhas been reached on whether current LLMs, such as ChatGPT, can immediately replace human\nannotators.\nAdditionally,fewstudieshaveappliedLLMstocitationclassification. Zhangetal.[2023]argued\nthat LLMs can contribute to citation context analysis by automating citation classification; however,\nthe applicability of LLMs classification is currently unclear and should be addressed in the future.\nKunnath et al. [2023] compared the performance of LLMs citation classification when performing\n3\nparameter updating using multiple methods on the public datasets ACL-ARC [Jurgens et al., 2018]\nandACT2[NambanoorKunnathetal.,2022]. Theresultsshowhighperformancewhenusingsome\nof the methods and that the zero-shot performance of GPT3.5 is high when targeting multiple fields\n(ACT2) but low when targeting a single field (ACL-ARC). However, Kunnath et al. [2023] did not\ncomparehumanannotatorstoLLMs,nordiditfocusontheapplicabilityofLLM-generateddatafor\ncitation context analysis.\nTothebest ofourknowledge, itis notyetclearwhether LLMscanreplacehumans inannotating\npaper, a special type of text that requires expertise in reading and understanding. In other words,\nit remains to be seen whether LLMs can be used in applied citation context analysis research, in\nwhich researchers create and analyze data that categorize individual citations for their own research\npurposes.\n3 Methods\n3.1 Task and Data\nMany categories are used in citation context analysis, and annotations vary widely according to the\ncategories used. We let LLMs perform annotation on the two categories used in Nishikawa [2023]:\ncitation purpose and citation sentiment. Descriptions are presented in Table 1.\nTable 1:Citation purpose and citation sentiment\nCategories Description\nCitation Purpose The type of purpose for citing a cited paper\n5 classes:1. Background, 2. Comparison, 3. Criti-\ncize, 4. Evidence, 5. Use\nCitation Sentiment The mental attitude of the author of a citing paper\nwhen citing a cited paper\n3 classes:1. Positive, 2. Negative, 3. Neutral\nThis study focuses on the same tasks as those in Nishikawa [2023] for the following three\nreasons. First, Nishikawa [2023] simply organizes categories and their classes (also called values)\nafter reviewing previous studies on citation context analysis. Second, the manual used for human\nannotators is publicly available and can be annotated using LLMs under the same conditions as\nhumans. Third,becausethegoldstandarddatausedintheanalysisarepubliclyavailable[Nishikawa\nand Monjiyama, 2023], the predictive performance of the annotation results from the LLMs can be\nevaluated. Regardingthelastreason,inNishikawa[2023],thedatausedintheanalysiswereprepared\nusing the following procedure:\n1. Twoannotators,aresearcherandagraduatestudentemployedasaresearchassistant,indepen-\ndently annotated all data according to the manual.\n2. After the initial annotation, the annotators explained why they had determined a value for a\ncitation in which the results did not match1.\n3. Finally,datafromtheannotatorwiththelowestnumberofvaluemodificationsforeachcategory\nwere used in the analysis.\nWhile Nishikawa [2023] set six categories, this study focuses only on citation purpose and\nsentiment for the following reasons. First, citation purpose and sentiment are the main categories\naddressed in many previous studies that have conducted citation context analyses [Lyu et al., 2021].\nInaddition,automatingtheirannotationsisdifficultbecauseitisnecessarytounderstandthesemantic\ncontent of the surrounding text in which the target-cited paper is mentioned to determine the class.\nHowever, the classification of other categories that correspond to syntactic categories [Zhang et al.,\n2013] can be automated relatively easily because they can be processed without understanding the\nmeaning of the text. Therefore, we believe that citation purpose and sentiment would particularly\nbenefit from automation, and we address only these two categories in this study.\n1This phase is called “discussion” [Lin, 2018].\n4\nInthisstudy,weallowedtheLLMstoannotatethesametextsthatwerethesubjectofannotation\nfor citation purpose and sentiments in Nishikawa [2023]. Nishikawa [2023] created 1,174 data by\nhuman annotation for citation purpose and citation sentiment, respectively. However, we annotated\n181 of them with LLMs for which the text of the citing papers could be obtained in the Journal\nArticle Tag Suite (JATS) -XML format. We have limited the annotation target to papers in JATS-\nXML format because they are expected to be almost free from errors and labor involved in text\nextraction. Therefore, if LLMs annotation is successful, it can be easily applied on a large scale. In\naddition, many of the papers that are the target of annotation in Nishikawa [2023] are available only\nin PDF format. In such cases, it is often difficult to accurately and automatically extract text and\ncitationinformationfromPDFs. Therefore,whenevaluatingtheannotationresults,theperformance\noftheLLMsandtheextractionaccuracyofthetargetdatamustbeconsidered. Manualtextextraction\nfrom PDFs can avoid this problem but at a higher cost. For these reasons, we limited the scope of\nthis study to articles that could be collected in JATS-XML format.\n3.2 Type of LLMs\nTheLLMsannotationwasperformedusingtheAPIprovidedbyOpenAPIInc. TheLLMmodelused\ninthisstudywas gpt3.5-turbo-0310. Thetemperature,whichistheparameterforcreativityor\nthe randomness of replies, was set to0.5 2.\n3.3 Prompts\nIn LLMs, any task can be executed by providing instructions (prompts) in a natural language, and\nthe task results can differ depending on the expression of the prompt. For example, to have LLMs\nprepare a summary of a certain paper, there are multiple patterns of possible prompts, such as\n“Please summarize the paper given below,” “Please give a brief summary of the paper,” and “Please\nsummarize the paper in about 200 words,” and the results may differ among them. In addition to\nthese differences in expression, using certain techniques such as few-shot [Brown et al., 2020b] or\nchain-of-thought [Wei et al., 2023] can change the results3.\nTherefore, in this study, we set up multiple patterns of prompts based on the manual used by\nhuman annotators in Nishikawa [2023] but with almost the same content. Because specific prompts\ndiffer depending on the citation purpose and sentiment, the following describes the prompt patterns\nfor each. The prompts are included in Online Resource 1.\n3.3.1 Prompt patterns for citation purposes\nThe manual for citation contains the following elements: 1. types of possible classes, 2. definitions\nfor each class, 3. procedures for annotation, and 4. Keywords and example sentences based on\nclass determination [Nishikawa, 2023]. Correspondingly, in addition to the basic instructions, four\npatterns of prompts for citation purpose were established, including the following elements:\n1. Types of class only (Simple)\n2. Types of class and their definition (Basic)\n3. Types of class, their definitions, and procedures for annotation (Precise)\n4. Typesofclass,definitions,annotationprocedures,andkeywordsandexamplesentences(Full)\nThe fourth pattern (Full), which includes all elements, is almost identical to the manual used in\nNishikawa [2023]. However, the original manual included instructions that did not directly affect\nthe annotation results, such as the handling of the files to be worked on. This study excluded such\ninstructions from the prompts. In addition, although the original manual instructs annotators to\nconsider “the title of the section in which the citation in question is being made,” in this experiment,\nwe excluded that part of the annotation to reduce the time and effort required to extract the title.\n2According to the official explanation (https://platform.openai.com/docs/api-reference/chat/create, Last\naccess:2023/May/06), it takes a value between 0 and 2, and defaults to 1. A value of about 0.2 always returns almost the same\nresponse, while a value of about 0.8 returns a random result. Considering these factors, we set an intermediate value this time.\n3Furthermore,dependingontheparametersoftheLLMsandthenatureofthedata,theresultsmaydifferevenforthesame\nprompt\n5\nClassify the type of purpose for which the author of the following text is citing the target. The type of citation purpose category is Background, Comparison, Criticize, Evidence, or Use. The label of the target literature is Target: onwards, up to a new line. The thesis is the text after Text:. Just report the classification result.Target: {Labels of cited papers in the text to be annotated} Text:{A paragraph to be annotated} \nPrompt for citation purpose (Simple): \nFigure 1:An example of prompt\nPredictTruePositive(TP)FalseNegative(FN)FalsePositive(FP)TrueNegative(TN)\nActualPositiveNegative\nPositiveNegative\nFigure 2:Definitions of indicators for performance\nMoreover,althoughthepublishedversionsofthemanualandtargetpaperswerewritteninEnglish,\nthe original manual was written in Japanese. Therefore, eight prompt patterns were established by\nwritingtheabovefourpatternsinJapaneseandEnglish. Forexample,thesimplestpatternofprompts\n(Simple, EN) containing only the type of class is shown in Figure1.\n3.3.2 Prompt patterns for citation sentiment\nAlthough the elements included in the original manual for citation sentiment are the same as those\nfor citation purpose, there are no instructions regarding procedures for annotation. Therefore, three\npatterns of prompts are set in citation sentiment, except “Precise,” as follows:\n1. Types of class only (Simple)\n2. Types of class and their definition (Basic)\n3. Types of class, their definitions, and keywords and example sentences (Full)\nFinally, six prompt patterns were set by writing the above in Japanese and English. Patterns\ncontaining all the elements (Full) are generally identical to those in the original manual; however,\nas in the case of citation purpose, it excludes instructions that do not directly affect the annotation\nresults contained in the original manual.\n3.4 Evaluation Metrics\nAsmentionedintheLiteratureReview,severalstudieshaveevaluatedtheresultsofLLMsannotations\nin terms of consistency and predictive performance. These two perspectives were also used in this\nstudy, where we use Nishikawa and Monjiyama [2023] as the gold standard. In addition, multiple\napplicablemetricsexistforbothperspectives. Weusedthesimpleagreementratebetweenannotators\nand Cohen’s kappa for consistency and the metrics shown in Figure2 for performance.\n6\n4 Experiments\n4.1 Distribution of Data\nTable2andTable3showthedistributionsofthetargetsannotatedinthisstudy. InNishikawa[2023],\nthe analysis unit was a citation pair, which is a pair of citing papers and one of the papers cited by it,\nall of which were related to either renewable energy (SDG7) or climate change (SDG13). Both the\nciting and cited papers were classified as Natural Sciences (NS) or Social Sciences and Humanities\n(SSH), and the following four patterns of relationships between disciplines were set: NS citing NS\n(NS-NS),NScitingSSH(NS-SSH),SSHcitingSSH(SSH-SSH),andSSHcitingNS(SSH-NS).As\nmentioned in the Methods section, this study used some of these as targets for annotation. Table 2\nshows the distribution of citation pairs for each of the patterns used in this study, and Table 3 shows\nthe distribution of the citing papers by research topic, that is SDG7 or 13.\nTable 4 and Table 5 present the gold standards for annotation used in this study. In other words,\ntheyareaselectionofannotationresultsforcitationpurposeandsentimentsfromNishikawa[2023],\nand their citing papers are available in the JATS-XML format.\nTable 2:Distribution of citation pairs by patterns\ncite_pattern n %\nNS-NS 32 17.7%\nNS-SSH 71 39.2%\nSSH-NS 41 22.7%\nSSH-SSH 37 20.4%\nTotal 181 100.0%\nTable 3:Distribution of citing papers by topics\nSDGs n %\nSDG7 130 71.8%\nSDG13 51 28.2%\nTotal 181 100.0%\nTable 4:Distribution of the gold standard on citation purpose\nPurpose n %\nBackground 128 70.7%\nComparison 2 1.1%\nCriticize 12 6.6%\nEvidence 30 16.6%\nUse 9 5.0%\nTotal 181 100.0%\n4.2 Consistency\nFirst,wecomparedtheannotationresultsofthehumanannotatorsinNishikawa[2023]andChatGPT\ninthisstudyintermsofconsistency. ChatGPTwasgivenaprompt(Full,EN)thatwasnearlyidentical\ntothoseinthemanualsusedinNishikawa[2023]andwasaskedtoannotatetwiceeachofthecitation\npurpose and citation sentiments for all 181 data. Table 6 shows the simple agreement rate and\nCohen’s kappa for each of the annotation results by ChatGPT and the results at the time the two\nannotatorsindependentlyannotatedinNishikawa[2023],i.e.,beforethe“discussion.” Itcanbeseen\nin Table 6 that ChatGPT is more consistent than humans with respect to both citation purpose and\ncitation sentiment.\nNext,wecomparedtheconsistencyoftheannotationresultsusingChatGPTwithallthepatterns\nof prompts described in the Methods section. Using eight patterns for citation purpose and six for\n7\nTable 5:Distribution of the gold standard on citation sentiment\nSentiment n %\nPositive 32 17.7%\nNeutral 121 66.9%\nNegative 28 15.5%\nTotal 181 100.0%\nTable 6:Comparison of consistency between humans and ChatGPT\nNishikawa\n(2023) GPT Nishikawa\n(2023) GPT\nSimple Agreement\nRate (%) 71.8% 90.1% 75.7% 91.2%\nCohen's Kappa 0.29 0.77 0.45 0.72\nPurpose Sentiment\ncitationsentimentprompts, wehadChatGPTannotateall181datapointstwiceeach. Table7shows\nthe number of cases in which the results at each prompt did not agree, the simple agreement rate,\nand Cohen’s kappa for citation purpose.\nTable 7:Consistency by prompts for citation purpose\nPatterns LNG n Agreement Kappa \nSimple (JP) 37 79.6% 0.58\nSimple (EN) 60 66.9% 0.40\nBasic (JP) 20 89.0% 0.74\nBasic (EN) 17 90.6% 0.80\nPrecise (JP) 21 88.4% 0.71\nPrecise (EN) 8 95.6% 0.86\nFull (JP) 17 90.6% 0.81\nFull (EN) 18 90.1% 0.77\n(N=181)\nAs shown in Table 7, for citation purpose, the highest consistency was found in the prompt\n(Precise, EN) that provided types of classes, their definitions, and annotation procedures in English,\nwith eight cases (4.4%) differing between the first and second prompts and95.6% remaining con-\nsistent. The lowest consistency was for the simple prompt in English (Simple, EN), with 30 cases\n(33.1%) differing and a simple agreement rate of66.9%. Prompts other than this pattern exceeded\ntheagreementratesofthehumanannotators. Interestingly,theprompt,includingallelements(full),\nwhich is similar to the original manual for humans, was less consistent. This finding suggests that\nconsistency does not necessarily increase with more detailed instruction.\nTable 8 summarizes the consistency of the annotation results for each prompt for citation sen-\ntiment. The table shows that the highest consistency was for the prompt that gave only the types\nof classes in Japanese (Simple, JP), with one case (0.6%) differing, and the lowest consistency was\nfound in the prompt (Full, EN), with 16 cases (8.8%) differing. However, all patterns outperform\nthe agreement rate by human annotators, and the consistency of the prompt Simple is significantly\nhigher than for citation purpose. In addition, focusing only on the prompts written in English, the\nmore detailed and specific the instructions, the less consistent they become.\n4.3 Predictive Performance\nFirst, we see the overall performance of annotation using ChatGPT. As mentioned above, ChatGPT\nwas asked to annotate each prompt pattern twice, but here, we have taken the results of the first\nannotation. The accuracy of the results obtained by ChatGPT when given a prompt with the same\n8\nTable 8:Consistency by prompts for citation sentiment\nPatterns LNG n Agreement Kappa \nSimple (JP) 1 99.4% 0.91\nSimple (EN) 3 98.3% 0.79\nPrecise (JP) 7 96.1% 0.65\nPrecise (EN) 9 95.0% 0.77\nFull (JP) 7 96.1% 0.80\nFull (EN) 16 91.2% 0.72\n(N=181)\ncontent as the manual used in Nishikawa [2023] was61.3% for citation purpose and64.6% for\ncitation sentiment.\nNext, we look at the performance of the annotation results from the prompts with the highest\nconsistency for each citation purpose and citation sentiment. For citation purpose, because the\nprompt that gave types of classes, their definitions, and annotation procedures in English (Precise,\nEN) were the most consistent, the relationship between the results of the first annotation with this\npattern (Predict) and the gold standard (Actual) is summarized in Table 9.\nTable 9:Predictive performance for citation purpose\nPurpose BKG CMP CRT EVS Use ttl\nBackground (BKG) 107 2 3 16 0 128\nComparison (CMP) 1 0 0 1 0 2\nCriticize (CRT) 11 0 0 1 0 12\nEvidence (EVS) 22 2 0 6 0 30\nUse 7 0 0 2 0 9\nTotal(ttl) 148 4 3 26 0 181\nPredict\nActual\nTable 9 shows, for example, that when the correct answer is “Background (BKG),” ChatGPT\ncorrectlypredictedBKG,i.e.,TruePositive,in107instances. However,aconsistentnumberoferrors\nare observed, including 16 cases where ChatGPT incorrectly predicted “Evidence (EVS)” instead\nof the correct BKG and 22 cases where ChatGPT predicted BKG, but the correct answer was EVS.\nMoreover, none of the “Compare (CMP),” “Criticize (CRT),” and “Use” were correctly predicted,\neven though they were originally infrequent classes.\nSimilarly, Table 10 summarizes the relationship between the results of the first annotation and\nthe prompt (Simple, EN), which provided only types of classes in English and was most consistent\nin the case of citation sentiment and the gold standard. The table shows, for example, that there are\n120 cases where the correct answer is “Neutral (NT)” and it is correctly predicted as NT. However,\nthere are some discrepancies, such as 24 cases where the correct answer was “Positive (PG),” even\nthough it was predicted as NT.\nTable 10:Predictive performance for citation sentiment\nSentiment PG NT NG ttl\nPositive (PG) 3 29 0 32\nNeutral (NT) 1 120 0 121\nNegative (NG) 0 24 4 28\nTotal(ttl) 4 173 4 181\nPredict\nActual\nThusfar,wehaveexaminedtheresultsofthefirstannotationbasedonasingleprompt. However,\nin the case of human annotation, it is common to create a single dataset for analysis, or in other\nwords, a gold standard, by narrowing down the annotation results from multiple annotators through\nsome means. Based on this, we had ChatGPT annotate using prompts of all types explained in the\n9\nMethodssection;then,wecreatedasingledatasetbyintegratingtheresultsandcomparedthedataset\nwith the gold standard. A majority vote was employed as the method of integration.\nIn Table 11, for citation purpose, the relationship between the ChatGPT dataset and the gold\nstandard is summarized. Although there was a slight increase in the number of correct answers for\nthe EVS in Table 11, the results were not significantly different from those shown in Table 9.\nTable 11:Predictive performance of ChatGPT’s multiple annotations for citation purpose\nPurpose BKG CMP CRT EVS Use ttl\nBackground (BKG) 103 1 2 21 1 128\nComparison (CMP) 1 0 0 0 1 2\nCriticize (CRT) 11 0 0 1 0 12\nEvidence (EVS) 15 1 0 13 1 30\nUse 6 0 0 3 0 9\nTotal(ttl) 136 2 2 38 3 181\nPredict\nActual\nSimilarly,forcitationsentiment,Table12showstherelationshipbetweentheresultsofChatGPT’s\nannotation, merged into one by majority vote, and the gold standard. Table 12 shows that the\nperformance for the prompt (Simple, EN) in Table 10 and the performance when integrating the\nannotation results from all prompts are nearly comparable.\nTable 12:PredictiveperformanceofChatGPT’smultipleannotationsforcitationsentiment\nSentiment PG NT NG ttl\nPositive (PG) 3 29 0 32\nNeutral (NT) 1 119 1 121\nNegative (NG) 0 23 5 28\nTotal(ttl) 4 171 6 181\nPredict\nActual\n4.4 Discussion of Experimental Results\nThe results of the experiments indicate that while ChatGPT outperforms human annotators in terms\nof consistency, it does not produce high-quality data in terms of predictive performance. Based on\ntheresultspresentedinTable9to12,itcanbesaidthatChatGPTdoesnotpredictthecorrectanswer\nwell, even though the gold standard created in Nishikawa [2023] was originally highly skewed from\nclass to class. For example, of the 148 cases in Table 9 predicted to be BKG, the number of cases\nthat were actually BKG was 107 (72.3%). Considering that the proportion of BKG in the correct\ndata was70.7% as shown in Table 9, the improvement rate is1.6% compared to the hypothetical\ncase where ChatGPT always predicts any class as a BKG. In addition, it should be noted that there\nwere a certain number of cases that were predicted as BKG but were not actually BKG, that is, false\nnegatives, and those that were predicted as a class other than BKG but were actually BKG, that is\nfalse negatives.\nAs for citation sentiment, Table 10 shows that all those predicted as PG are actually PG, which\nis a good prediction in terms of the small number of false negatives. However, this number is only\n4 out of 28 total actual PG, which is not large. The same was true for NG. For NT, which accounts\nforthemajorityofcases,120ofthe173casespredictedasNTareactuallyNT,buttheimprovement\nrateis 1.5% comparedwiththehypotheticalcaseinwhichChatGPTalwayspredictstheclassasNT.\nIt should also be noted that approximately30% of the cases were falsely predicted to be PG or NG.\nEvenifweconsiderthepredictedresultsasPGorNG,thispercentageisonlyapproximately 4.4% of\nthe total. In other cases, the annotation results are unreliable; therefore, human review is inevitable.\nConversely, the results of the experiments in this study, which showed poor performance while\nmaintaining a certain level of consistency, may indicate differences in how ChatGPT and human\nannotators “interpret” texts. Thus, we considered how ChatGPT interprets texts by examining the\ntexts that were actually the target of the annotation in cases where ChatGPT failed to predict the\nclasses correctly or where the results were inconsistent across multiple annotations.\n10\nThe results of the examination suggest that there is no explicit expression of the relationship\nbetween the target cited paper and its surrounding sentences in the texts for which ChatGPT makes\nerroneous or inconsistent annotations. For example, ChatGPT predicted the class as “Criticism\n(CRT)” for the following text [Borie et al., 2019, p.207]4, but the correct class was \"Background\n(BKG)\".\n(...) This use of technical devices in an attempt to suppress political debates has been\nwidely documented elsewhere (e.g. Latour, 2004; Lupton and Mather, 1997). At an\nextreme this use of GIS and mapping reinforces and aggravates existing divides and\ninequalities.\nIn this text, the target-cited paper was Lupton and Mather [1997]. Although the text includes\nwords that would provide a basis for predicting the class as CRT (“widespread” and “reinforces and\nexacerbatesexistingdivisionsandinequalities”),itis“thisuseoftechnicaldevices/GISandmapping,”\nnot Lupton and Mather [1997], that is the target of “criticism” in this text. Because Lupton and\nMather [1997] seems to have been cited to provide background information on the research topic\nof the citing paper, the correct class here is the BKG. This would have been relatively easy for\nhuman annotators to determine, but it would have been difficult for ChatGPT to do so because the\nrelationship between the cited and citing papers was not explicitly stated as words. In other words,\nit is suggested that ChatGPT interprets text using only explicit words and does not consider implied\ncontexts. Note that this pattern is often seen in texts where ChatGPT fails to correctly predict the\nclass, but there are exceptions in which this pattern does not successfully explain the reason for the\ninterpretation.\nFrom the above, it can be said that the annotation results of ChatGPT are inadequate in terms\nof performance, and it is problematic to use the dataset created by ChatGPT for analysis. In other\nwords, the experimental results of this study clarify that it is difficult to use the current ChatGPT as\na substitute for human annotators in citation context analyses.\n5 Consideration of LLM Use Case in Citation Context Analysis\n5.1 Support for Human Annotators\nThus far, we have evaluated the LLM from the viewpoint of consistency and predictive performance\nwhen instructions are given based on a manual for humans. Consequently, the annotation results\nof the LLM are not likely to be as good as those of humans in terms of performance, and it was\nsuggested that human annotation is necessary for unknown data.\nHowever, it may be possible to use LLMs not as a complete replacement for human annotation,\nbut as a support for human annotators. If we can believe that “what LLMs predict as PG is actually\nPG,” as in the case of PG in Table 9—i.e., if the prediction performance for at least some classes is\nsufficientlyhigh—wecanletLLMsannotatethemonbehalfofhumanannotators. Moreover,aswith\nhuman annotations, it is also possible that some of those predicted to be in the same class include\nthose predicted with confidence, whereas others do not. In this case, it may be possible to reduce\nthe cost of annotation by having low-confidence prediction results annotated again by a human and\nadopting LLMs’ annotation results of the LLMs for high-confidence prediction results.\nIn general, the number of possible classes affects the text classification performance, and the\nannotation performance by LLMs depends on the type of class predicted [Pangakis et al., 2023].\nTherefore,theperformanceofLLMsannotationcanbeimprovedbychangingthenumberofclasses\nand their characteristics. In light of the above, we reviewed prompts for using LLMs to support\nhuman annotators and examined the possibility of their use.\n5.1.1 Citation Purpose\n“Background(BKG)”accountsforabout 70% ofthegoldstandard,followedby“Evidence”atabout\n16%. We thus reconfigured the annotation for citation purpose as a three-class classification task,\nadding “Other” to these two classes. Table 13 presents the annotation results for ChatGPT5.\n4It is the citaion pair (NS-SSH) in SDG13.\n5The prompts used for the following tasks in this section are shown in Online Resource 2.\n11\nTable 13:PredictivePerformanceofChatGPT’sAnnotationforCitationPurposewhenthe\nThird Class is \"Other\"\nPurpose BKG EVS OTH SUM\nBackground (BKG) 65 63 0 128\nEvidence (EVS) 3 27 0 30\nOther (OTH) 14 9 0 23\nSUM 82 99 0 181\nPredict\nActual\nAs shown in the table, none of the cases were classified as “Other,” and the predicted results are\neither“Background”or“Evidence.” ComparedtoTable9andTable11,thenumberofcasespredicted\nas “Evidence” increased, and the performance of annotation generally worsened. In addition, the\nconsistency (simple agreement rate) of the two annotations is91.7%. One possible reason for this\nchange in annotation trends is that ChatGPT was influenced by the literal meaning of the name of\nthe newly created class “Other” and avoided annotating that broad and ambiguous class. We thus\nreplaced “Other” with “General” or “Pending,” respectively, and let ChatGPT annotate again.\nAs shown in Table 14 and 15, although some cases belonging to the third class can be observed,\ntheoveralltrendisthesameasthatshowninTable13. Inaddition,theconsistency(simpleagreement\nrate) of the results of the two annotations was86.2% and 91.7%.\nTable 14:PredictivePerformanceofChatGPT’sAnnotationforCitationPurposewhenthe\nThird Class is \"General\"\nPurpose BKG EVS GEN ttl\nBackground (BKG) 37 71 20 128\nEvidence (EVS) 0 30 0 30\nGeneral (GEN) 5 16 2 23\nTotal(ttl) 42 117 22 181\nPredict\nActual\nTable 15:PredictivePerformanceofChatGPT’sAnnotationforCitationPurposewhenthe\nThird Class is \"Pending\"\nPurpose BKG EVS PDG ttl\nBackground (BKG) 9 113 6 128\nEvidence (EVS) 0 29 1 30\nPending (PDG) 1 21 1 23\nTotal(ttl) 10 163 8 181\nPredict\nActual\nTheaboveexperimentswereconductedhopingthatreducingthenumberofclasseswouldimprove\nthe annotation performance; however, the results showed that the performance degraded. We also\nchanged the third-class names to account for the possibility that the name might affect annotation;\nhowever, performance did not improve. However, if the literal meaning of the class name affects\nthe annotation, it is possible that the existing classes “Background” and “Evidence” also affected\nthe annotation trend of ChatGPT, apart from their operative definition. Therefore, we let ChatGPT\nannotate again by replacing “Background” and “Evidence” with just “BKG” and “EVS,” which are\nsequencesofsymbolswithoutmeaningaswords. Moreover,thenameofthethirdclasswaschanged\nto UKN. The annotation results for these changes are listed in Table 16. The overall trend was the\nsame as that shown in Table 13. In addition, the consistency (simple agreement rate) of the results\nfor the two annotations is86.2%.\nThe experiments thus far have shown that reducing the number of classes or changing class\nnames does not improve the annotation performance. Therefore, we attempted a different strategy,\nhaving GPT perform a binary classification for each class. Specifically, we let ChatGPT predict\nwhether it was a BKG (PB or NB) or an EVS (PE or NE) and examined the relationship between\n12\nTable 16: Predictive Performance of ChatGPT’s Annotation for Citation Purpose with\nMeaningless Class Names\nPurpose BKG EVS UKN ttl\nBackground (BKG) 107 21 0 128\nEvidence (EVS) 22 8 0 30\nOther (OTH) 19 4 0 23\nTotal(ttl) 148 33 0 181\nPredict\nActual\ntheircombinationandthegoldstandard. Althoughwereplacedtheoriginalannotationwithabinary\nclassification, there was no relationship, as shown in Table 17. In addition, the consistency (simple\nagreement rate) of the results of the two annotations was90.6% and91.7%.\nTable 17: Predictive Performance of ChatGPT’s Annotation for Citation Purpose when\nBinary Classification for Each Class\nPurpose NBNE PBPE NBPE PBNE ttl\nBackground (BKG) 6 68 9 45 128\nEvidence (EVS) 0 17 3 10 30\nOther (OTH) 4 9 1 9 23\nTotal(ttl) 10 94 13 64 181\nPredict\nActual\nFinally, we reorganized the annotation into the simplest binary classification: BKG (BKG or\nUKN).Table18showsthatofthe43casespredictedasBKG,38( 88.4%)wereBKG,whichishighly\naccurate. Although the number of cases in which ChatGPT answered correctly was small, these\ndata could potentially be used in the analysis. In this case, the consistency (simple agreement rate)\nbetween the two annotations is91.2%.\nTable 18: Predictive Performance of ChatGPT’s Annotation for Citation Purpose when\nBinary Classification\nPurpose BKG UKN ttl\nBackground (BKG) 38 90 128\nEvidence (EVS) 2 28 30\nOther (OTH) 3 20 23\nTotal(ttl) 43 138 181\nPredict\nActual\n5.1.2 Citation Sentiment\nUnlikecitationpurpose, citationsentimentwasoriginallyathree-classclassificationtask, anditwas\ndifficulttoreducethenumberofclassesanyfurther. Wethusattemptedadifferentapproach: adding\na class.\nAs mentioned previously, some of the predicted results from the LLMs may contain different\nconfidence levels. If so, the annotation performance could be improved by distinguishing between\nthose predicted with high and low confidence. Therefore, we first added a class named “Pending\n(PD)” and let those with low confidence be classified there. As shown in Table 19, some are\nclassified as “Pending,” but the number is small, 30. Furthermore, what is actually a “Positive” may\nbe classified as the opposite, “Negative,” which means that the performance is deteriorating.\nNext, we had both those that were actually neutral and those predicted with low confidence be\nclassified in the current “Neutral,” and we have renamed only this class to UKN, a name without\nmeaning. As shown in Table 20, while the number of those predicted as “Positive” and “Negative”\nhas increased, the number of errors was too large to use the results for analysis.\n13\nTable 19:Predictive Performance of ChatGPT’s Annotation for Citation Sentiment when\nAdding \"Pending\"\nSentiment PG NT NG PD ttl\nPositive (PG) 14 17 0 1 32\nNeutral (NT) 19 78 0 24 121\nNegative (NG) 2 16 5 5 28\nTotal(ttl) 35 111 5 30 181\nPredict\nActual\nPD: Pending\nTable 20:Predictive Performance of ChatGPT’s Annotation for Citation Sentiment when\nChanging \"Neutral\"\nSentiment PG UN NG ttl\nPositive (PG) 15 8 9 32\nNeutral (NT) 19 57 45 121\nNegative (NG) 1 8 19 28\nTotal(ttl) 35 73 73 181\nPredict\nActual\nUN: Unknown\n5.2 Other Use Cases\nTheresultsoftheexperimentsthusfarindicatethatitisdifficulttouseLLMstoperformannotations\npartiallyon behalfof humans. However, they alsosuggest thatthereis roomfor LLMstobe utilized\nin citation context analysis.\nPangakis et al. [2023] categorizes the use cases of LLMs in general annotation work as follows:\n1. Confirming the quality of human-labeled data\n2. Identifying cases to prioritize for human review\n3. Producing labeled data to finetune and validate a supervised classifier\n4. Classifying the entire corpus directly\nThis paper shows that cases other than Case 1 are difficult to apply to citation context analysis. For\nCase4,aswehaveseenintheExperimentssection,poorpredictiveperformancemakesitproblematic\nto use the LLM-generated data for the analysis. For the same reason, the use of LLMs in Case 3\nshould be avoided. In addition, Case 2 is a use case related to the partial substitution of annotation\nwork, butasdiscussedthusfarinthissection, therearealsoconcernsabouttheuseofLLMsforthis\npurpose.\nHowever, Case 1 seems open for consideration. This use case implies examining the quality of\nhuman annotation results by comparing the annotation results of humans and the LLM. Putting this\nintothecontextofcitationcontextanalysis,apossibleuseofLLMsistousetheresultsoftheLLMs\nannotation as reference information when narrowing down the data produced by multiple human\nannotators to a single set of data to be used in the analysis. As shown in the Experiments section,\nalthough the performance of the annotation results by the LLM was low, the consistency was more\nstablethanthatofthehumanannotators. TheLLMcanalsooutputreasonsforitsdecisions. Because\nofthesecharacteristics,theLLMcanbeviewedasanannotatorwithcriteriaandtendenciesdifferent\nfrom those of human annotators.\nInotherwords,theannotationresultsofLLMsandthereasonsfortheirdecisionscanbeutilizedas\nreference information in the process of the aforementioned “discussion”[Lin, 2018], which narrows\ndown multiple datasets to a single one. In “discussion,” human annotators try to maintain the\nobjective correctness of the data by explaining the reasons for their decisions of annotations to\neach other, taking care not to persuade the other, and voluntarily revising their own work results if\nnecessary. Atthistime,usingLLMsdataasreferenceinformationfromathird-partystandpointmay\nreduce the possibility of a particular human annotator’s subjectivity having a significant impact on\nthe “discussion.”\n14\nAdditionally, hiring human annotators is generally expensive in terms of time and money, which\nsometimesforcestheuseofdatafromasingleannotatorforanalysis[Ryttingetal.,2023]. Particularly\ninthecaseofcitationcontextanalysis,annotatorsmusthavemoreadvancedskillsbecausethetextto\nbeannotatedisaspecialone,ascientificpaper. Thismakesitmorechallengingtosecureasufficient\nnumber of annotators compared with general annotations. In these situations, one option would be\nto introduce LLMs as one of the annotators to avoid using the data generated by a single annotator.\n6 Conclusion\nThisstudyaimedtoexploretheapplicabilityofLLMstocitationcontextanalysis. Theresultsrevealed\nthat ChatGPT, at least in its current version, cannot annotate with sufficiently high performance to\nreplace human annotators for the major categories in citation context analysis: citation purpose and\nsentiment. ItwasalsofoundtobedifficulttohaveChatGPTpartiallyannotatedonbehalfofhumans,\nsuchasbyusingCatGPTannotationresultsforspecificclassesandhavingahumanannotatetherest.\nHowever, because the ChatGPT annotation results have a certain consistency, it may be possible\nto view LLMs as annotators who interpret differently than humans. This suggests the following\ntwo possible use cases of LLMs in citation context analysis. First, the annotation results obtained\nby LLMs can be used as reference information when narrowing the annotation results obtained by\nmultiple human annotators to one. Second, it is possible to use LLMs as the Nth annotators when\nsecuring the number of human annotators is difficult.\nFuture researchers attempting to utilize LLMs for citation context analysis can refer to LLMs’\nlimitations and use cases clarified in this study. In contrast to previous studies that verified the\nperformance of LLMs in general annotation tasks and proposed use cases, the findings of this study\narenovelinthattheyexaminedtheirapplicabilityandusecasesinthespecifictaskofcitationcontext\nanalysis.\nHowever,thisstudyhadseverallimitations. Inthepresentstudy,wefocusedon gpt3-turbo-0310,\nwhich exhibited the best performance at the time of the experiment. However,gpt4, which is be-\nlieved to perform better thangpt3-turbo-0310, is now available to the public. We also tried an\nexperiment using this new model and found that it is worse thangpt3-turbo-0310 in terms of\npredictive performance, as shown in Online Resource 3; however, further new models may emerge\nin the future that will allow LLMs to perform in a way that overturns the conclusions of this study.\nTherefore, the findings of this study represent a snapshot of the potential applications of LLMs and\nshould be analyzed continuously following future technological trends.\n15\nAcknowledgements\nThispreprinthasnotundergonepeerreview(whenapplicable)oranypost-submissionimprovements\nor corrections. The Version of Record of this article is published in Scientometrics, and is available\nonline athttps://doi.org/10.1007/s11192-024-05142-9\nDeclarations\nCompeting Interests\nThe authors have no competing interests to declare that are relevant to the content of this article.\nFunding\nNo funding was received for conducting this study.\n16\nReferences\nMaudBorie,MarkPelling,GinaZiervogel,andKeithHyams. Mappingnarrativesofurbanresilience\nin the global south. Global Environmental Change, 54:203–213, jan 2019. doi: 10.1016/j.\ngloenvcha.2019.01.001.\nLutz Bornmann and Hans-Dieter Daniel. What do citation counts measure? a review of stud-\nies on citing behavior. Journal of Documentation, 64(1):45–80, jan 2008. doi: 10.1108/\n00220410810844150.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielM.Ziegler,\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. Language models are few-shot learners.arXiv (preprint), 2020a.\ndoi: 10.48550/arXiv.2005.14165.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielM.Ziegler,\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. Language models are few-shot learners.arXiv (preprint), 2020b.\ndoi: 10.48550/arXiv.2005.14165.\nYu-Wei Chang. A comparison of citation contexts between natural sciences and social sciences and\nhumanities. Scientometrics, 96(2):535–553, feb 2013. doi: 10.1007/s11192-013-0956-1.\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaĳie Zhu, Hao Chen, Xiaoyuan\nYi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S Yu, Qiang Yang,\nand Xing Xie. A survey on evaluation of large language models.arXiv (preprint), 2023. doi:\n10.48550/arXiv.2307.03109.\nYingDing,GuoZhang,TamyChambers,MinSong,XiaolongWang,andChengxiangZhai.Content-\nbased citation analysis: The next generation of citation analysis.Journal of the Association for\nInformation Science and Technology, 65(9):1820–1833, jun 2014. doi: 10.1002/asi.23256.\nRodrigoDorantes-Gilardi,AuroraA.Ramírez-Álvarez,andDianaTerrazas-Santamaría. Theroleof\nhighly intercited papers on scientific impact: the mexican case.Applied Network Science, 7(1),\naug 2022. doi: 10.1007/s41109-022-00497-5.\nXingwei He, Zhenghao Lin, Yeyun Gong, A-Long Jin, Hang Zhang, Chen Lin, Jian Jiao, Siu Ming\nYiu, Nan Duan, and Weizhu Chen. AnnoLLM: Making large language models to be better\ncrowdsourced annotators.arXiv (preprint), 2023. doi: 10.48550/arXiv.2303.16854.\nSehrish Iqbal, Saeed-Ul Hassan, Naif Radi Aljohani, Salem Alelyani, Raheel Nawaz, and Lutz\nBornmann. Adecadeofin-textcitationanalysisbasedonnaturallanguageprocessingandmachine\nlearning techniques: an overview of empirical studies.Scientometrics, 126(8):6551–6599, jun\n2021. doi: 10.1007/s11192-021-04055-1.\nDavid Jurgens, Srĳan Kumar, Raine Hoover, Dan McFarland, and Dan Jurafsky. Measuring the\nevolutionofascientificfieldthroughcitationframes. Trans. Assoc. Comput. Linguist.,6:391–406,\nDecember 2018. doi: 10.1162/tacl_a_00028.\nSuchetha N Kunnath, David Pride, and Petr Knoth. Prompting strategies for citation classification.\nIn Proceedings of the 32nd ACM International Conference on Information and Knowledge Man-\nagement, CIKM ’23, pages 1127–1137, New York, NY, USA, October 2023. Association for\nComputing Machinery. doi: 10.1145/3583780.3615018.\n17\nTaja Kuzman, Nikola Ljubešić, and Igor Mozetič. ChatGPT: Beginning of an end of manual\nannotation? use case of automatic genre identification.arXiv preprint arXiv:2303.03953, 2023.\ndoi: 10.48550/arXiv.2303.03953.\nChi-ShiouLin. Ananalysisofcitationfunctionsinthehumanitiesandsocialsciencesresearchfrom\nthe perspective of problematic citation analysis assumptions.Scientometrics, 116(2):797–813,\nmay 2018. doi: 10.1007/s11192-018-2770-2.\nM.LuptonandC.Mather.‘theanti-politicsmachine’: GISandthereconstructionofthejohannesburg\nlocalstate. Political Geography,16(7):565–580,sep1997.doi: 10.1016/S0962-6298(96)00060-1.\nDongqingLyu,XuanminRuan,JuanXie,andYingCheng. Theclassificationofcitingmotivations: a\nmeta-synthesis. Scientometrics,126(4):3243–3264,feb2021. doi: 10.1007/s11192-021-03908-z.\nSuchetha Nambanoor Kunnath, Valentin Stauber, Ronin Wu, David Pride, Viktor Botev, and Petr\nKnoth. ACT2: A multi-disciplinary semi-structured dataset for importance and purpose classi-\nfication of citations. In Nicoletta Calzolari, Frédéric Béchet, Philippe Blache, Khalid Choukri,\nChristopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mar-\niani, Hélène Mazo, Jan Odĳk, and Stelios Piperidis, editors,Proceedings of the Thirteenth Lan-\nguage Resources and Evaluation Conference, pages 3398–3406, Marseille, France, June 2022.\nEuropean Language Resources Association.\nKai Nishikawa. How and why are citations between disciplines made? a citation context analysis\nfocusing on natural sciences and social sciences and humanities.Scientometrics, 128(5):2975–\n2997, feb 2023. doi: 10.1007/s11192-023-04664-y.\nKaiNishikawaandMieMonjiyama. Dataonacitationcontextanalysisfocusingonnaturalsciences\nand social sciences and humanities, 2023.\nNicholas Pangakis, Samuel Wolken, and Neil Fasching. Automated annotation with generative ai\nrequires validation.arXiv (preprint), 2023. doi: 10.48550/arXiv.2306.00176.\nMichaelV.Reiss. Testingthereliabilityofchatgptfortextannotationandclassification: Acautionary\nremark. arXiv (preprint), 2023. doi: 10.48550/arXiv.2304.11085.\nChristopher Michael Rytting, Taylor Sorensen, Lisa Argyle, Ethan Busby, Nancy Fulda, Joshua\nGubler, andDavidWingate. Towardscodingsocialsciencedatasetswithlanguagemodels. arXiv\n(preprint), 2023. doi: 10.48550/arXiv.2306.02177.\nImanTahamtanandLutzBornmann. Whatdocitationcountsmeasure? anupdatedreviewofstudies\non citations in scientific documents published between 2006 and 2018.Scientometrics, 121(3):\n1635–1684, sep 2019. doi: 10.1007/s11192-019-03243-4.\nShiyunWang,JinMao,KunLu,YujieCao,andGangLi. Understandinginterdisciplinaryknowledge\nintegration through citance analysis: A case study on eHealth.Journal of Informetrics, 15(4):\n101214, nov 2021. doi: 10.1016/j.joi.2021.101214.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc\nLe, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models.\narXiv (preprint), 2023. doi: 10.48550/arXiv.2201.11903.\nChengzhiZhang,LifanLiu,andYuzhuoWang. Characterizingreferencesfromdifferentdisciplines:\nAperspectiveofcitationcontentanalysis. Journal of Informetrics, 15(2):101134, may2021. doi:\n10.1016/j.joi.2021.101134.\nGuo Zhang, Ying Ding, and Staša Milojević. Citation content analysis (CCA): A framework for\nsyntacticandsemanticanalysisofcitationcontent. Journal of the American Society for Information\nScience and Technology, 64(7):1490–1503, may 2013. doi: 10.1002/asi.22850.\nYangZhang,YufeiWang,KaiWang,QuanZSheng,LinaYao,AdnanMahmood,WeiEmmaZhang,\nandRongyingZhao. Whenlargelanguagemodelsmeetcitation: Asurvey. arXiv (preprint),2023.\ndoi: 10.48550/arXiv.2309.09727.\n18\nA Online Resource 1. Prompts used in Experiments\nNote:\nThefollowingshowsonlythepromptsgiventoChatGPTthatwerewritteninEnglish.\nIn each prompt, the actual data to be annotated is inserted in the area enclosed in { }.\nA.1 Citation Purpose\nA.1.1 Simple\nClassifythetypeofpurposeforwhichtheauthorofthefollowingtextiscitingthetarget.\nThe type of citation purpose category is Background, Comparison, Criticize, Evidence,\nor Use.\nThe label of the target literature is Target: onwards, up to a new line. The thesis is the\ntext after Text:.\nJust report the classification result.\nTarget: { }\nText:\n{ }\nA.1.2 Basic\nClassifythetypeofpurposeforwhichtheauthorofthefollowingtextiscitingthetarget.\nThe type of citation purpose category is Background, Comparison, Criticize, Evidence,\nor Use based on the following manual.\nManual:\n———–\nIf the target is cited to present or summarize general background information about the\nresearch theme or topic of the text, you classify the text as Background.\nIf the target is cited to compare results or methods between the text and the target or\nbetween other cited papers, you classify the text as Comparison.\nIfthetargetiscitedtoprovidesomeevaluationorreviewofthetext,youclassifythetext\nas Criticize. Both positive and negative evaluations are included here.\nIf the target is cited to support or validate the author’s claims, decisions (e.g., choice\nof methodology), interpretations, judgments, opinions, etc., you classify the text as\nEvidence.\nIfthetargetiscitedtousemethods, models, data, software, concepts, theories, hypothe-\nses, etc. presented in the target, you classify the text as Use.\n———–\nThe label of the target literature is Target: onwards, up to a new line.\nThe thesis is the text after Text:. Just report the classification result.\nTarget: { }\nText:\n{ }\nA.1.3 Precise\nClassifythetypeofpurposeforwhichtheauthorofthefollowingtextiscitingthetarget\nbased on the following manual.\nThe type of citation purpose category is Background, Comparison, Criticize, Evidence,\nor Use.\nManual:\n———–\nThe classification procedure is as follows:\n1. Readthesentencecontainingthetargetfirst. Ifthecategorycanbeclearlydetermined\nby reading the sentence containing the target, the category is determined at that point.\n19\n2. If you are not sure about the decision based on the sentence containing the target\nalone,readtheonesentencebeforeandafterit. Ifthecategorycanbeclearlydetermined\nbyreadingthesentencesbeforeandafterthesentencecontainingthetarget,thecategory\nis determined at that point.\n3. If the category cannot be determined after reading one sentence before or after the\nsentence containing the target, read the sentences in order from the beginning of the\nparagraph. If the category can be clearly determined, the category is determined at that\npoint. If the target is cited to present or summarize general background information\nabout the research theme or topic of the text, you classify the text as Background.\nIf the target is cited to compare results or methods between the text and the target or\nbetween other cited papers, you classify the text as Comparison.\nIfthetargetiscitedtoprovidesomeevaluationorreviewofthetext,youclassifythetext\nas Criticize. Both positive and negative evaluations are included here.\nIf the target is cited to support or validate the author’s claims, decisions (e.g., choice\nof methodology), interpretations, judgments, opinions, etc., you classify the text as\nEvidence.\nIfthetargetiscitedtousemethods, models, data, software, concepts, theories, hypothe-\nses, etc. presented in the target, you classify the text as Use.\n———–\nThe label of the target literature is Target: onwards, up to a new line.\nThe thesis is the text after Text:. Just report the classification result.\nTarget: { }\nText:\n{ }\nA.1.4 Full\nClassifythetypeofpurposeforwhichtheauthorofthefollowingtextiscitingthetarget\nbased on the following manual.\nThe type of citation purpose category is Background, Comparison, Criticize, Evidence,\nor Use.\nI will give you the text later.\nManual:\n———–\nThe classification procedure is as follows:\n1. Readthesentencecontainingthetargetfirst. Ifthecategorycanbeclearlydetermined\nby reading the sentence containing the target, the category is determined at that point.\n2. If you are not sure about the decision based on the sentence containing the target\nalone,readtheonesentencebeforeandafterit. Ifthecategorycanbeclearlydetermined\nbyreadingthesentencesbeforeandafterthesentencecontainingthetarget,thecategory\nis determined at that point.\n3. If the category cannot be determined after reading one sentence before or after the\nsentence containing the target, read the sentences in order from the beginning of the\nparagraph. If the category can be clearly determined, the category is determined at that\npoint.\nIf the target is cited to present or summarize general background information about the\nresearch theme or topic of the text, you classify the text as Background.\nIf the sentences around the target contain the following keywords, the text is considered\nBackground.\nKeywords of Background: “overview”, “review”, “summarize”\nThe following are the criteria for classifying the text as Background.\nCriteria of Background: “When the target is cited to summarize information on recent\nresearch trends”, “If the text simply introduces or refer to the target on its research\ntopic”, “When the target is cited to state general or overall information (e.g., policy\ntrends, relevant research areas or theories, etc.)”, “If the text does not fall into any other\ncategory”\n20\nIf the target is cited to compare results or methods between the text and the target or\nbetween other cited papers, you classify the text as Comparison.\nIf the sentences around the target contain the following keywords, the text is considered\nComparison.\nKeywordsofComparison: “although”,“compare”,“comparison”,“contrast”,“however”,\n“in contrast”, “on the contrary”, “on the other hand”, “while”\nThe following are the criteria for classifying the text as Comparison.\nCriteria of Comparison: “When cited to compare the results of the citing paper with\nthose of previous studies and claim the superiority of the citing paper’s results”, “When\ncomparingtwopreviousstudiesandpointingouttheadvantagesordisadvantagesofone\nstudy over the other”\nIf the sentences around the target are similar to the following examples, the text is\nconsidered Comparison.\nAn example of Comparison: “However, while neurobiology posits that the rewarding\nproperties of social behavior may have evolved to facilitate group cohesion and cooper-\nation [4], our model suggests that polarization (as opposed to cohesion) across groups\nmay be a side-effect of these rewarding properties.”\nIfthetargetiscitedtoprovidesomeevaluationorreviewofthetext,youclassifythetext\nas Criticize. Both positive and negative evaluations are included here.\nThe following are the criteria for classifying the text as Criticize.\nCriteria of Criticize: “When the target is cited to evaluate the contribution or advantage\nof the text”, “When the text is cited to point out a weakness or wrong in the text”\nIf the sentences around the target are similar to the following examples, the text is\nconsidered Criticize.\nAn example of Criticize: “The method in [4] reports a high result for the Media-lab\ndataset but does this using a dataset-specific SE and so it not a universal method.”\nIf the target is cited to support or validate the author’s claims, decisions (e.g., choice\nof methodology), interpretations, judgments, opinions, etc., you classify the text as\nEvidence.\nIf the sentences around the target contain the following keywords, the text is considered\nEvidence.\nKeywords of Evidence: “aligns with”, “be consistent with”, “indicate to us”, “similar\nto”, “support”, “therefore”, “thus”\nThe following are the criteria for classifying the text as Evidence.\nCriteria of Evidence: “When the target is cited to support the text’s author’s claim,\nhypothesis, or decision”, “When the target is cited to justify the methodology of the\ntext’s research or previous research the text’s author supports”, “When the target is cited\nto justify the assumptions and limitations of the text”, “When the text’s author proposes\na future research direction, the author cites the target to support his proposal”\nIf the sentences around the target are similar to the following examples, the text is\nconsidered Evidence.\nAnexampleofEvidence: “Ourfindingsemphasizethatbuildingdigitalseizingcapabil-\nities are contingent on pacing strategic actions, which aligns with dynamic capabilities\nresearch in hypercompetitive contexts [4].”\nIfthetargetiscitedtousemethods, models, data, software, concepts, theories, hypothe-\nses, etc. presented in the target, you classify the text as Use.\nIf the sentences around the target contain the following keywords, the text is considered\nUse.\nKeywords of Use: “based on”, “be carried over”, “provided by”, “use”\nThe following are the criteria for classifying the text as Use.\nCriteriaofUse: “Whenthetargetiscitedtouseadatasetpresentedinthetarget”,“When\nthetargetiscitedtouseamethodproposedordevelopedinthetarget”,“Whentheauthor\nof the text cites a definition on a concept or theory presented in the target”\nIf the sentences around the target are similar to the following examples, the text is\nconsidered Use.\n21\nAnexampleofUse: “OurArabicpart-of-speechtaggerusesthesimplifiedPATBtagset\nproposed by [4].”\n———–\nThe label of the target literature is Target: onwards, up to a new line.\nThe thesis is the text after Text:. Just report the classification result.\nTarget: { }\nText:\n{ }\nA.2 Citation Sentiment\nA.2.1 Simple\nClassify the text into Positive, Negative, or Neutral.\nThe label of the target literature is Target: onwards, up to a new line.\nThe thesis is the text after Text:. Just report the classification result.\nTarget: { }\nText:\n{ }\nA.2.2 Precise\nClassify the text into Positive, Negative, or Neutral based on the following manual.\nManual:\n—–\nYou classify the sentence in which the target is cited. If the sentence is divided into\nmultiple clauses by conjunctions, etc., and whether the sentence is Positive or Negative\ndiffersdependingontheclauses,youshouldreadandclassifyonlytheclausecontaining\nthe target.\nIf the target is cited in a sentence with a positive meaning, you classify it as Positive.\nIf the target is cited in a sentence with a negative meaning, you classify it as Negative.\nIfthetargetiscitedinasentencewithneitherPositivenorNegativemeaning,youclassify\nit as Neutral.\n—–\nThe label of the target literature is Target: onwards, up to a new line. The thesis is the\ntext after Text:. Just report the classification result.\nTarget: { }\nText:\n{ }\nA.2.3 Full\nClassify the text into Positive, Negative, or Neutral based on the following manual.\nManual:\n—–\nYou classify the sentence in which the target is cited. If the sentence is divided into\nmultiple clauses by conjunctions, etc., and whether the sentence is Positive or Negative\ndiffersdependingontheclauses,youshouldreadandclassifyonlytheclausecontaining\nthe target.\nIfthetargetiscitedinasentencewithapositivemeaning,youclassifythetextasPositive.\nSentences containing the following keywords are considered Positive.\nExamples of Positive keywords: “be able to...”, “best”, “can...”, “could”, “develop”,\n“enhance”, “important”, “promote”, “robustly”, “support”, “well”\nThe following sentences are examples of sentences that are considered Positive.\nExample of Positive sentences: “The best known and simplest stochastic representation\nfordiscretegeophysicaltimeseriesistheAR(1)model(Ghiletal. 2002;Brethertonand\nBattisti 2000).”\n22\nExample of Positive sentences: “These patterns find empirical support in Popp and\nNewell’s (2012) study of firm-level R&D spending and patents.”\nExample of Positive sentences: “Although national and transnational connections may\nbe necessary to secure access to resources and technical expertise, it is argued that local\nparticipation in the governance of social-ecological systems provides legitimacy (Bier-\nmann and Gupta 2011, Dryzek and Stevenson 2011), accommodates diverse interests\nandvalues(Brown2003,Lebeletal. 2006),andtapslocalecologicalknowledge(Berkes\nand Folke 2002, Gerhardinger et al. 2009, Raymond et al. 2010).”\nIf the target is cited in a sentence with a negative meaning, you classify the text as\nNegative.\nSentences containing the following keywords are considered Negative.\nExamples of Negative keywords: “but”, “despite”, “even though”, “however”, “ignore”,\n“less”, “nevertheless”, “problematic”, “suffer”, “undermine”\nThe following sentences are examples of sentences that are considered Negative.\nExample of Negative sentences: “When women are unable to obtain sufficient water for\nmenstrualablutionsorhygiene(e.g.,cleaningmenstrualcloths),theymaysufferextreme\nstigma and humiliation (Rashid and Michaud 2000:54).”\nExampleofNegativesentences: “Theneedforbetterempiricalinformationaboutenergy-\nefficiency R&D is well known but difficult to solve due to lack of disaggregated data\n(although see on the contrary Popp (2002) and Popp and Newell (2012)).”\nExampleofNegativesentences: “Determinationofthefunctionsoffungalspecies,which\ntypically requires their isolation in pure culture and the study of their effects on defined\nsubstrates, has well-documented limitations [19–21].”\nIfthetargetiscitedinasentencewithneitherPositivenorNegativemeaning,youclassify\nthe text as Neutral.\n—–\nThe label of the target literature is Target: onwards, up to a new line. The thesis is the\ntext after Text:. Just report the classification result.\nTarget: { }\nText:\n{ }\n23\nB Online Resource 2. Prompts used in Consideration of LLM\nUse Case in Citation Context Analysis\nNote:\nBelow is each prompt used in Consideration of LLM Use Case in Citation Context\nAnalysis. Each prompt is named with the corresponding table number in the article. In\neach prompt, the actual data to be annotated is inserted in the area enclosed in { }.\nB.1 Citation Purpose\nB.1.1 Table 13\nPleaseclassifythetypeofpurposeforwhichtheauthorofthefollowingtextiscitingthe\ntarget.\nThe type of citation purpose category is Background, Evidence, or Others based on the\nfollowing manual.\nManual:\n—\nIf the target is cited to present or summarize general background information about the\nresearch theme or topic of the text, you classify the text as Background.\nIf the target is cited to support or validate the author’s claims, decisions (e.g., choice\nof methodology), interpretations, judgments, opinions, etc., you classify the text as\nEvidence.\nIfthetypeofcitationpurposecategoryisneitherBackgroundnorEvidence,youclassify\nthe text as Others.\n—\nThe label of the target literature is Target: onwards, up to a new line.\nThe thesis is the text after Text:. Just report the classification result.\nTarget: { }\nText:\n{ }\nB.1.2 Table 14\nPleaseclassifythetypeofpurposeforwhichtheauthorofthefollowingtextiscitingthe\ntarget.\nThe type of citation purpose category is General, Background or Evidence based on the\nfollowing manual.\nManual:\n—\nUsually annotate as ’General’.\nFor example, 1. a comparison of results or methods between the text and the subject or\nother cited papers.\n2. some kind of evaluation or review of the text.\n3. use of methods, models, data, software, concepts, theories, hypotheses, etc.\nAll other items that are not background, evidence, etc., that cannot definitely be said to\nbe background or evidence, or that cannot be classified in one specific category, etc., all\nbelong to the ’General’ category.\nInaddition,Ifthetargetiscitedtopresentorsummarizegeneralbackgroundinformation\nabout the research theme or topic of the text, you classify the text as Background.\nIf the target is cited to support or validate the author’s claims, decisions (e.g., choice\nof methodology), interpretations, judgments, opinions, etc., you classify the text as\nEvidence.\n—\nThe label of the target literature is Target: onwards, up to a new line.\n24\nThethesisisthetextafterText:. Please,returnonlytheclassificationresultsinoneword.\nTarget: { }\nText:\n{ }\nB.1.3 Table 15\nPleaseclassifythetypeofpurposeforwhichtheauthorofthefollowingtextiscitingthe\ntarget.\nThe type of citation purpose category is Pending, Background or Evidence based on the\nfollowing manual.\nManual:\n—\nWherever possible, judge the case as ’Pending’.\nAnythingthatcannotdefinitelybecategorisedasBackgroundorEvidence,orthatcannot\nbe placed in a specific category, belongs to ’Pending’. In addition,\nIf the target is cited to present or summarize general background information about the\nresearch theme or topic of the text, you classify the text as Background.\nIf the target is cited to support or validate the author’s claims, decisions (e.g., choice\nof methodology), interpretations, judgments, opinions, etc., you classify the text as\nEvidence.\n—\nThe label of the target literature is Target: onwards, up to a new line. The thesis is the\ntext after Text:. Please, return only the classification results in one word.\nTarget: { }\nText:\n{ }\nB.1.4 Table 16\nPlease provide a classification of the citation in the text.\nClassification target is only one citation.\nThe label of the target citation is [Target:] onwards, up to a new line. The text is after\n[Text:].\nThe criteria for classification are as follows.\nThere are three classes: UKN, BKG and EVS.\nThe criteria for classification are given below.\nBasically, classified as UKN.\nThis option (i.e. UKN) is extremely strongly recommended.\nTry to select this option whenever possible.\nWhere the purpose of the citation is presumed to be background, such as including\npresentorsummarizegeneralbackgroundinformationabouttheresearchthemeortopic,\nit is classified as BKG.\nThis option (i.e. BKG) is second most strongly recommended.\nWherethepurposeofthecitationispresumedtobeevidence,suchassupportorvalidate\nthe author’s claims, decisions (e.g., choice of methodology), interpretations, judgments,\nopinions,etc.,andwherethereisnoroomforanyotherinterpretationatall,itisclassified\nas EVS.\nThis option (i.e. EVS) is deprecated, avoid as much as possible.\nPlease, return only the classification results in just one word.\nTarget: { }\nText:\n{ }\n25\nB.1.5 Table 17\nPlease provide a classification of the citation in the text.\nClassification target is only one citation.\nThe label of the target citation is [Target:] onwards, up to a new line. The text is after\n[Text:].\nThe criteria for classification are as follows.\nThere are two types of classes.\nBasically, classified as NB.\nHowever, where the purpose of the citation is presumed to be background, such as\nincluding present or summarize general background information about the research\ntheme or topic, it is classified as PB.\nPlease, return only the classification results in just one word.\nTarget: { }\nText:\n{ }\nB.1.6 Table 18\nPlease provide a classification of the citation in the text.\nClassification target is only one citation.\nThe label of the target citation is [Target:] onwards, up to a new line. The text is after\n[Text:].\nThe criteria for classification are as follows.\nThere are 2 classes: UKN and BKG.\nThe criteria for classification are given below.\nBasically, classified as UKN.\nThis category includes, for example, citations as evidence, criticism, comparison, dis-\ncussion, etc.\nTry to select this option whenever possible.\nThe other hands, Where the purpose of the citation is presumed to be background, such\nas including present or summarize general background information about the research\ntheme or topic, it is classified as BKG.\nPlease, return only the classification results in just one word.\nTarget: { }\nText:\n{ }\nB.2 Citation Sentiment\nB.2.1 Table 19\nPleaseclassifythetextintoPositive,Negative,NeutralorPendingbasedonthefollowing\nmanual.\nManual:\n—-\nYou classify the sentence in which the target is cited.\nWherever possible, judge the case as ’Pending’.\nIf the target is cited in a sentence with a definitely positive meaning, you classify it as\n’Positive’.\nIf the target is cited in a sentence with a definitely negative meaning, you classify it as\n’Negative’.\nIf the target is cited in a sentence with a definitely neutral meaning, you classify it as\n’Neutral’.\n—-\n26\nThe label of the target literature is Target: onwards, up to a new line.\nThe thesis is the text after Text:. Just report the classification result.\nTarget: { }\nText:\n{ }\nB.2.2 Table 20\nPlease provide a classification of the citation in the text.\nClassification target is only one citation.\nThe label of the target citation is [Target:] onwards, up to a new line. The text is after\n[Text:].\nThe criteria for classification are as follows.\nThere are 3 classes: Positive, Negative and UKN.\nThe criteria for classification are given below.\nBasically, classify them as UKN.\nThis category also includes the classification result of neutral.\nHowever,ifthetargetcitationispresumedtobecitedpositivelyornegatively,outputthe\npresumption.\nPlease, return only the classification results in just one word.\nTarget: { }\nText:\n{ }\n27\nC Online Resource 3. Examples of Annotations with GPT4\nNote:\nAt the start of the experiment (May 2023), of the models available to the general\npublicviatheAPI,themostcapablemodelwas gpt-3.5-turbo andwasthereforeused\nin this part.\nHowever, GPT4 has also been available since July 2023. As GPT4 is generally\nconsideredtohavehigherperformancethan gpt-3.5-turbo,inthefollowing,weshow\nthe results of having GPT4 annotated.\nSpecifically, we had GPT4 annotated in the same way as discussed in Experiments.\nHowever, only English prompts were used.\nC.1 Citation purposes\nThe annotation results for citation purpose are summarized in Table 21 through Table 24.\nTable 21:Citation purposes: Simple (English)\nPurpose BKG CMP CRT EVS Use ttl\nBackground (BKG) 35 8 4 56 26 129\nComparion (CMP) 0 2 0 0 0 2\nCriticize (CRT) 5 1 0 2 4 12\nEvidence (EVS) 4 2 0 19 5 30\nUse 0 0 0 2 7 9\nTotal (ttl) 44 13 4 79 42 181\nPredict\nActual\nTable 22:Citation purposes: Basic (English)\nPurpose BKG CMP CRT EVS Use ttl\nBackground (BKG) 41 18 4 39 26 128\nComparion (CMP) 0 2 0 0 0 2\nCriticize (CRT) 4 6 0 1 1 12\nEvidence (EVS) 5 6 0 15 4 30\nUse 0 0 0 3 6 9\nTotal (ttl) 50 32 4 58 37 181\nPredict\nActual\nTable 23:Citation purposes: Precise (English)\nPurpose BKG CMP CRT EVS Use ttl\nBackground (BKG) 28 10 6 45 39 128\nComparion (CMP) 0 1 0 0 1 2\nCriticize (CRT) 4 3 0 1 4 12\nEvidence (EVS) 3 3 0 14 10 30\nUse 0 0 0 1 8 9\nTotal (ttl) 35 17 6 61 62 181\nPredict\nActual\nIn the article, we used the annotation results from the prompt (Precise, EN) with the highest\nconsistency in order to evaluate the predictive performance ofgpt-3.5-turbo. In the follow-up\nstudy described above, Table 23 shows the result for the same prompt.\nWhile the prediction results using GPT3 were concentrated on \"Background (BKG)\" and \"Evi-\ndence(EVS)\",thenumberofotherclassesincreasedrelativelymorewhenGPT4wasused. However,\nthe performance is rather degraded due to the skewed distribution of the gold standard in the first\nplace. Compared to Table 9 in the article, which summarizes the results fromgpt-3.5-turbo,\nTable 23 shows a decrease in the number of predicted BKG, which may be the reason for the lower\nperformance. Ontheotherhand,theimbalanceintheannotationresultshasbeenimproved,resulting\nin some cases where \"Criticize (CRT)\" and \"Use\" were correctly predicted. In particular, none were\nestimated to be Use whengpt-3.5-turbo was used, but their number increased significantly with\nTable 23.\n28\nTable 24:Citation purposes: Precise & examples (English)\nPurpose BKG CMP CRT EVS Use ttl\nBackground (BKG) 51 4 3 22 48 128\nComparion (CMP) 0 1 0 0 1 2\nCriticize (CRT) 6 3 0 0 3 12\nEvidence (EVS) 4 0 0 13 13 30\nUse 0 0 0 0 9 9\nTotal (ttl) 61 8 3 35 74 181\nPredict\nActual\nFrom the above, it can be seen that the annotation results vary widely depending on the LLM\nmodel used. However, in any case, the performance was not sufficient, suggesting that human\nannotation is still necessary for citation context analysis.\nC.2 Citation sentiment\nAswellascitationpurposes,theannotationresultsforcitationsentimentaresummarizedinTable25\nthrough Table 27.\nTable 25:Citation sentiment: Simple (English)\nSentiment PG NT NG ttl\nPositive (PG) 5 27 0 32\nNeutral (NT) 6 114 1 121\nNegative (NG) 0 22 6 28\nTotal(ttl) 11 163 7 181\nPredict\nActual\nTable 26:Citation sentiment: Basic (English)\nSentiment PG NT NG ttl\nPositive (PG) 14 18 0 32\nNeutral (NT) 18 101 2 121\nNegative (NG) 0 16 12 28\nTotal(ttl) 32 135 14 181\nPredict\nActual\nIn the article, we used the annotation results from the prompt (Simple, EN) with the highest\nconsistency in order to evaluate the predictive performance ofgpt-3.5-turbo. In the follow-up\nstudy described above, Table 25 shows the result for the same prompt. The general trends shown in\nTable 25 are similar to those in Table 10 in the article, which suggests that there is no significant\ndifferencebetween gpt-3.5-turbo andTable25otherthanaslightdecreaseinperformanceofthe\nlatter.\n29\nTable 27:Citation sentiment: Precise (English)\nSentiment PG NT NG ttl\nPositive (PG) 27 3 2 32\nNeutral (NT) 40 56 24 120\nNegative (NG) 2 4 22 28\nTotal(ttl) 69 63 48 181\nPredict\nActual\n30",
  "topic": "Citation",
  "concepts": [
    {
      "name": "Citation",
      "score": 0.6467106342315674
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6256893873214722
    },
    {
      "name": "Computer science",
      "score": 0.584376335144043
    },
    {
      "name": "Citation analysis",
      "score": 0.5660801529884338
    },
    {
      "name": "Data science",
      "score": 0.4446325898170471
    },
    {
      "name": "Information retrieval",
      "score": 0.4026535749435425
    },
    {
      "name": "Linguistics",
      "score": 0.34690356254577637
    },
    {
      "name": "Library science",
      "score": 0.21889954805374146
    },
    {
      "name": "History",
      "score": 0.1819133162498474
    },
    {
      "name": "Archaeology",
      "score": 0.08557116985321045
    },
    {
      "name": "Philosophy",
      "score": 0.0765233039855957
    }
  ],
  "institutions": []
}