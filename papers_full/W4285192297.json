{
  "title": "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
  "url": "https://openalex.org/W4285192297",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2114515055",
      "name": "Guo Yue",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A1979858332",
      "name": "Yang, Yi",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A3118478475",
      "name": "Abbasi, Ahmed",
      "affiliations": [
        "University of Notre Dame"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3173673804",
    "https://openalex.org/W3004561114",
    "https://openalex.org/W3105882417",
    "https://openalex.org/W2972413484",
    "https://openalex.org/W2971307358",
    "https://openalex.org/W2888161220",
    "https://openalex.org/W2963078909",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W1945616565",
    "https://openalex.org/W3174082143",
    "https://openalex.org/W2963116854",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2802105481",
    "https://openalex.org/W3166986030",
    "https://openalex.org/W3204712960",
    "https://openalex.org/W3173610337",
    "https://openalex.org/W3013770059",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2989344603",
    "https://openalex.org/W3035591180",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W3093211917",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3173465197",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W3035241006",
    "https://openalex.org/W2603766943",
    "https://openalex.org/W3131157458",
    "https://openalex.org/W4206292552",
    "https://openalex.org/W2769358515",
    "https://openalex.org/W3134354193",
    "https://openalex.org/W2921633540",
    "https://openalex.org/W2791170418",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3213670254",
    "https://openalex.org/W2889624842",
    "https://openalex.org/W3155655882",
    "https://openalex.org/W2949969209",
    "https://openalex.org/W2725155646",
    "https://openalex.org/W3185212449",
    "https://openalex.org/W3206487987",
    "https://openalex.org/W2963457723",
    "https://openalex.org/W2950866572",
    "https://openalex.org/W2954275542",
    "https://openalex.org/W3035296331",
    "https://openalex.org/W2926555354",
    "https://openalex.org/W4288029087",
    "https://openalex.org/W2950018712"
  ],
  "abstract": "Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to fine-tune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed Auto-Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models' understanding abilities, as shown using the GLUE benchmark.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 1012 - 1023\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nAuto-Debias: Debiasing Masked Language Models with Automated\nBiased Prompts\nYue Guo1, Yi Yang 1, Ahmed Abbasi 2\n1 The Hong Kong University of Science and Technology\n2 University of Notre Dame\nyguoar@connect.ust.hk imyiyang@ust.hk aabbasi@nd.edu\nAbstract\nHuman-like biases and undesired social stereo-\ntypes exist in large pretrained language mod-\nels. Given the wide adoption of these models\nin real-world applications, mitigating such bi-\nases has become an emerging and important\ntask. In this paper, we propose an automatic\nmethod to mitigate the biases in pretrained lan-\nguage models. Different from previous debi-\nasing work that uses external corpora to ﬁne-\ntune the pretrained models, we instead directly\nprobe the biases encoded in pretrained models\nthrough prompts. Speciﬁcally, we propose a\nvariant of the beam search method to automat-\nically search for biased promptssuch that the\ncloze-style completions are the most different\nwith respect to different demographic groups.\nGiven the identiﬁed biased prompts, we then\npropose a distribution alignment loss to miti-\ngate the biases. Experiment results on stan-\ndard datasets and metrics show that our pro-\nposed Auto-Debias approach can signiﬁcantly\nreduce biases, including gender and racial bias,\nin pretrained language models such as BERT,\nRoBERTa and ALBERT. Moreover, the im-\nprovement in fairness does not decrease the\nlanguage models’ understanding abilities, as\nshown using the GLUE benchmark.\n1 Introduction\nPretrained language models (PLMs), such as\nmasked language models (MLMs), have achieved\nremarkable success in many natural language pro-\ncessing (NLP) tasks (Devlin et al., 2019; Liu et al.,\n2019; Lan et al., 2020; Brown et al.). Unfor-\ntunately, pretrained language models, which are\ntrained on large human-written corpora, also in-\nherit human-like biases and undesired social stereo-\ntypes (Caliskan et al., 2017; Bolukbasi et al., 2016;\nBlodgett et al., 2020). For example, in the ﬁll-in-\nthe-blank task, BERT (Devlin et al., 2019) substi-\ntutes [MASK] in the sentence “The man/woman\nhad a job as [MASK]” with “manager/receptionist”\nrespectively, reﬂecting occupational gender bias.\nThe human-like biases and stereotypes encoded in\nPLMs are worrisome as they can be propagated or\neven ampliﬁed in downstream NLP tasks such as\nsentiment classiﬁcation (Kiritchenko and Moham-\nmad, 2018), co-reference resolution (Zhao et al.,\n2019; Rudinger et al., 2018), clinical text classiﬁca-\ntion (Zhang et al., 2020) and psychometric analysis\n(Abbasi et al., 2021; Ahmad et al., 2020).\nHowever, although it is important to mitigate\nbiases in PLMs, debiasing masked language mod-\nels such as BERT is still challenging, because the\nbiases encoded in the contextualized models are\nhard to identify. To address this challenge, previous\nefforts seek to use additional corpora to retrieve the\ncontextualized embeddings or locate the biases and\nthen debias accordingly. For example, Liang et al.\n(2020); Kaneko and Bollegala (2021); Garimella\net al. (2021) use external corpora to locate sen-\ntences containing the demographic-speciﬁc words\n(e.g., man and women) or stereotype words (e.g.,\nmanager and receptionist) and then use different\ndebiasing losses to mitigate the biases.\nUsing external corpora to debias PLMs heav-\nily relies on the quality of the corpora. Empirical\nresults show that different corpora have various\neffects on the debiasing results: some external cor-\npora do mitigate the bias, while others introduce\nnew biases to the PLMs (Garimella et al., 2021;\nLiang et al., 2020). This is because the corpora\nused for debiasing may not have enough cover-\nage of the biases encoded in the PLMs. Neverthe-\nless, our understanding of how to quantitatively\nassess the level of biases in a corpus remains lim-\nited (Blodgett et al., 2020).\nMitigating biases in PLMs without external cor-\npora is an open research gap. Recent work in\nlanguage model prompting shows that through\ncloze-style prompts, one can probe and analyze\nthe knowledge (Petroni et al., 2019), biases (May\net al., 2019) or toxic content (Ousidhoum et al.,\n2021) in PLMs. Motivated by this, instead of refer-\n1012\nhe   man     boy  father ....she woman girl  mother  ......          ..          ..          ..         ..          ... [   ][T][T][T][MASK]\nMasked LM..          ..          ..          ..         ..          ... Target concept wordspromptsp([MASK]|prompt, [he])p([MASK]|prompt, [she])Step2:debias\nMinimize JS-DivergenceBiased prompts searchtheof                     the   …                    …           …      …professional    united    real    ……                    …           …      …transformed\nStep1: search for biases\nMaximizeJS-Divergence\nPLACE-\nHOLDER\nFigure 1: The Auto-Debias framework. In the ﬁrst stage, our approach searches for the biased promptssuch that\nthe cloze-style completions (i.e., masked token prediction) have the highest disagreement in generating stereo-\ntype words. In the second stage, the language model is ﬁne-tuned by minimizing the disagreement between the\ndistributions of the cloze-style completions.\nring to any external corpus, we directly use cloze-\nstyle prompts to probe and identify the biases in\nPLMs. But what are the biases in a PLM? Our\nidea is motivated by the assumption that a fair NLP\nsystem should produce scores that are independent\nto the choice of identities mentioned in the text\n(Prabhakaran et al., 2019). In our context, we pro-\npose automatically searching for “discriminative”\nprompts such that the cloze-style completions have\nthe highest disagreement in generating stereotype\nwords (e.g., manager/receptionist) with respect to\ndemographic words (e.g., man/woman). The auto-\nmatic biased promptsearch also minimizes human\neffort.\nAfter we obtain the biased prompts, we probe\nthe biased content with such prompts and then\ncorrect the model bias. We propose an equal-\nizing loss to align the distributions between the\n[MASK] tokens predictions, conditioned on the\ncorresponding demographic words. In other words,\nwhile the automatically crafted biased prompts\nmaximize the disagreement between the predicted\n[MASK] token distributions, the equalizing loss\nminimizes such disagreement. Combining the auto-\nmatic prompts generation and the distribution align-\nment ﬁne-tuning, our novel method, Auto-Debias\ncan debias the PLMs without using any external\ncorpus. Auto-Debias is illustrated in Figure 1.\nIn the experiments, we evaluate the perfor-\nmance of Auto-Debias in mitigating gender and\nracial biases in three popular masked language\nmodels: BERT, ALBERT, and RoBERTa. More-\nover, to alleviate the concern that model debias-\ning may worsen a model’s performance on natu-\nral language understanding (NLU) tasks (Meade\net al., 2021), we also evaluate the debiased mod-\nels on GLUE tasks. The results show that our\nproposed Auto-Debias approach can effectively\nmitigate the biases while maintaining the capa-\nbility of language models. We have released\nthe Auto-Debias implementation, debiased mod-\nels, and evaluation scripts at https://github.\ncom/Irenehere/Auto-Debias.\n2 Related Works\nAs NLP models are prevalent in real-world appli-\ncations, a burgeoning body of literature has inves-\ntigated human-like biases in NLP models. Bias in\nNLP systems can stem from training data (Dixon\net al., 2018), pre-trained word embeddings or can\nbe ampliﬁed by the machine learning models. Most\nexisting work focuses on the bias in pre-trained\nword embeddings due to their universal nature\n(Dawkins, 2021). Prior work has found that tra-\nditional static word embeddings contain human-\nlike biases and stereotypes (Caliskan et al., 2017;\nBolukbasi et al., 2016; Garg et al., 2018; Manzini\net al., 2019; Gonen and Goldberg, 2019). Debias-\ning strategies to mitigate static word embeddings\nhave been proposed accordingly (Bolukbasi et al.,\n2016; Zhao et al., 2018; Kaneko and Bollegala,\n2019; Ravfogel et al., 2020).\nContextualized embeddings such as BERT have\nbeen replacing the traditional static word embed-\ndings. Researchers have also reported similar\nhuman-like biases and stereotypes in contextual\n1013\nembedding PLMs (May et al., 2019; Kurita et al.,\n2019; Tan and Celis, 2019; Hutchinson et al.,\n2020; Guo and Caliskan, 2021; Wolfe and Caliskan,\n2021) or in the text generation tasks (Schick et al.,\n2021; Sheng et al., 2019). Compared to static\nword embeddings, mitigating the biases in con-\ntextualized PLMs is more challenging since the\nrepresentation of a word usually depends on the\nword’s context. Garimella et al. (2021) propose to\naugment the pretraining corpus with demographic-\nbalanced sentences. Liang et al. (2020); Cheng\net al. (2021) suggest removing the demographic-\ndirection from sentence representations in a post-\nhoc fashion. However, augmenting the pretrain-\ning corpus is costly and post-hoc debiasing does\nnot mitigate the intrinsic biases encoded in PLMs.\nTherefore, recent work has proposed to ﬁne-tune\nthe PLMs to mitigate biases by designing different\ndebiasing objectives (Kaneko and Bollegala, 2021;\nGarimella et al., 2021; Lauscher et al., 2021). They\nrely on external corpora, and the debiasing results\nbased on these external corpora vary signiﬁcantly\n(Garimella et al., 2021). Moreover, Garimella et al.\n(2021) ﬁnd that existing debiasing methods are gen-\nerally ineffective: ﬁrst, they do not generalize well\nbeyond gender bias; second, they tend to worsen a\nmodel’s language modeling ability and its perfor-\nmance on NLU tasks. In this work, we propose a\ndebiasing method that does not necessitate refer-\nring to any external corpus. Our debiased models\nare evaluated on both gender and racial biases, and\nwe also evaluate their performance on NLU tasks.\n3 Auto-Debias: Probing and Debiasing\nusing Prompts\nWe propose Auto-Debias, a debiasing technique for\nmasked language models that does not entail ref-\nerencing external corpora. Auto-Debias contains\ntwo stages: First, we automatically craft the biased\nprompts, such that the cloze-style completions have\nthe highest disagreement in generating stereotype\nwords with respect to demographic groups. Sec-\nond, after we obtain the biased prompts, we debias\nthe language model by a distribution alignment\nloss, with the motivation that the prompt comple-\ntion results should be independent to the choice of\ndifferent demographic-speciﬁc words.\n3.1 Task Formulation\nLet Mbe a Masked Language Model (MLM),\nand V be its vocabulary. The language model\npre-trained with human-generated corpus con-\ntains social bias towards certain demographic\ngroups. To mitigate the bias, we have two types\nof words: target conceptswhich are the paired to-\nkens related to demographic groups (e.g., he/she,\nman/woman), and attribute wordswhich are the\nstereotype tokens with respect to the target con-\ncepts (e.g., manager, receptionist). We denote\nthe target concepts as a set of m-tuples of words\nC = {(c(1)\n1 , c(1)\n2 , .., c(1)\nm ), (c(2)\n1 , c(2)\n2 , .., c(2)\nm ), ...}.\nFor example, in the two-gender debiasing task, the\ntarget concepts are {(he,she), (man,woman),...}. In\nthe three-religion debiasing task, the target con-\ncepts are {(judaism,christianity,islam), (jew, chris-\ntian,muslim), ...}. We omit the superscript of Cif\nwithout ambiguity. We denote the set of attribute\nwords as W.\nAn MLM can be probed by cloze-style prompts.\nFormally, a prompt xprompt ∈V∗is a sequence of\nwords with one masked token [MASK] and one\nplaceholder token. We use xprompt(c) to denote the\nprompt with which the placeholder is ﬁlled with\na target concept c. For example, given xprompt =\n“[placeholder] has a job as [MASK]”, we\ncan ﬁll in the placeholder with the target concept\n\"she\" and obtain\nxprompt(she) =she has a job as [MASK].\nGiven a prompt and a target concept xprompt(c)\nas the input of M, we can obtain the predicted\n[MASK] token probability as\np([MASK] = v|M, xprompt(c))\n= exp(M[MASK](v|xprompt(c)))∑\nv′ ∈Vexp(M[MASK](v\n′\n|xprompt(c)))\n(1)\nwhere v ∈ V. Prior literature has used this\n[MASK] token completion task to assess MLM\nbias (May et al., 2019). To mitigate the bias in\nan M, we hope that the output distribution pre-\ndicting a [MASK] should be conditionally inde-\npendent on the choice of any target concept in the\nm-tuple (c1, c2, ..., cm). Therefore, for different\nci ∈(c1, c2, ..., cm), our goal to debias Mis to\nmake the conditional distributions p([MASK] =\nv|M, xprompt(ci)) as similar as possible.\n3.2 Finding Biased Prompts\nThe ﬁrst stage of our approach is to generate\nprompts that can effectively probe the bias from\nM, so that we can remove such bias in the sec-\nond stage. One straightforward way to design such\n1014\nAlgorithm 1: Biased Prompt Search\ninput : Language model M, candidate vocabulary V′, target words C, stereotype words W,\nprompt length PL, beam width K.\noutput :Generated Biased Prompts P\n1 P←{} ;\n2 Candidate prompts Pcan ←V′;\n3 for l ←1 to PL do\n4 Pgen ←top-Kx∈Pcan {JSD (p([MASK]|xprompt(ci), M), i∈{1, 2, ..m})};\n5 // where xprompt(ci) =ci ⊕x ⊕[MASK] and we only consider the probability of the attribute\nwords Win the [MASK] position\n6 P←P∪P gen;\n7 Pcan ←{x ⊕v|∀x ∈Pgen, ∀v ∈V′}\n8 end\nprompts is by manual generation. For example, “A\n[placeholder] has a job as [MASK]” is such\nbiased prompts as it generates different mask token\nprobabilities conditioned on the placeholder word\nbeing man or woman. However, handcrafting such\nbiased prompts at scale is costly and the models\nare highly sensitive to the crafted prompts.\nTo address the problem, we propose biased\nprompt search, as described in Algorithm 1, a vari-\nant of the beam search algorithm, to search for the\nmost discriminative, or in other words, the most bi-\nased prompts with respect to different demographic\ngroups. Our motivation is to search for the prompts\nthat have the highest disagreement in generating\nattribute words Win the [MASK] position. We use\nJensen–Shannon divergence (JSD), which is a sym-\nmetric and smooth Kullback–Leibler divergence\n(KLD), to measure the agreement between distri-\nbutions. In the case of the two-gender debiasing\n(male/female) task, JSD measures the agreement\nbetween the two distributions.\nThe JSD among distributions p1, p2, ..pm is de-\nﬁned as\nJSD (p1, p2, ..., pm)\n= 1\nm\n∑\ni\nKLD(pi||p1 + p2 + ... + pm\nm ), (2)\nwhere the Kullback–Leibler divergence(KLD) be-\ntween two distributions pi, pj is computed as\nKLD(pi||pj) =∑\nv∈Vpi(v)log( pi(v)\npj(v) ).\nAlgorithm 1 describes our algorithm for search-\ning biased prompts. The algorithm ﬁnds the se-\nquence of tokens x from the search space to craft\nprompts, which is ﬁrstly the candidate vocabulary\nspace1, and then, after the ﬁrst iteration, the con-\n1We could use the entire V as the search space, but it\ncatenation of searched sequences and candidate\nvocabulary. Speciﬁcally, during each iteration, for\neach candidate x in the search space, we construct\nthe prompt as xprompt(ci) = ci ⊕x ⊕[MASK],\nwhere ⊕is the string concatenation, for ci in an m-\ntuple (c1, c2, ..., cm). Given the prompt xprompt(ci),\nMpredicts the [MASK] token distribution over\nattribute words W(e.g. manager, receptionist,...):\np([MASK] = v|M, xprompt(ci)), v∈W.\nNext, we compute the JSD score between\np([MASK] = v|M, xprompt(ci)) for each ci ∈\n(c1, c2, ..., cm), and select the prompts with high\nscores — indicating large disagreement between\nthe [MASK] predictions for the given target con-\ncepts. The algorithm ﬁnds the top K prompts\nxprompt from the search space in each iteration step,\nand the procedure repeats until the prompt length\nreaching the pre-deﬁned threshold. We merge all\nthe generated prompts as the ﬁnal biased prompts\nset P.\n3.3 Fine-tuning MLM with Prompts\nAfter we obtain the biased prompts, we ﬁne-tune\nMto correct the biases. Speciﬁcally, given an m-\ntuple of target words (c1, c2, ..., cm) and a biased\nprompt xprompt, we expect Mto be unbiased in\nthe sense that p([MASK] = v|M, xprompt(ci)) =\np([MASK] = v|M, xprompt(cj)) for any ci, cj ∈\n(c1, c2, ..., cm). This equalizing objective is moti-\nvated by the assumption that a fair NLP system\nshould produce scores that are independent to the\nchoice of the target concepts in our context, men-\ncontains punctuations, word pieces and meaningless words.\nTherefore, instead of using the vocabulary V, we use\nthe 5,000 highest frequency words in Wikipedia as the\nsearch space. https://github.com/IlyaSemenov/\nwikipedia-word-frequency\n1015\ntioned in the text (Prabhakaran et al., 2019).\nTherefore, given a prompt xprompt, our equal-\nizing loss aims to minimize the disagreement be-\ntween the predicted [MASK] token distributions.\nSpeciﬁcally, it is deﬁned as the Jensen-Shannon\ndivergence (JSD) between the predicted [MASK]\ntoken distributions:\nloss(xprompt) =\n∑\nk\nJSD (p(k)\nc1 , p(k)\nc2 , .., p(k)\ncm ) (3)\nwhere p(k)\nci = p([MASK] = v|M, xprompt(c(k)\ni )),\nfor v in a certain stereotyped word list. And the\ntotal loss is the average over all the prompts in the\nprompt set P.\nDiscussion: Another perspective for Auto-Debias\nis that the debiasing method resembles adversarial\ntraining (Goodfellow et al., 2014; Papernot et al.,\n2017). In the ﬁrst step, Auto-Debias searches for\nthe biased prompts by maximizing disagreement\nbetween the masked language model (MLM) com-\npletions. In the second step, Auto-Debias lever-\nages the biased prompts to ﬁne-tune the MLM, by\nminimizing disagreement between the MLM com-\npletions. Taken together, Auto-Debias corrects the\nbiases encoded in the MLM without relying on any\nexternal corpus. Overcoming the need to manually\nspecify biased prompts would also make the entire\ndebiasing pipeline more objective.\nRecent research has adopted the adversarial train-\ning idea to remove biases from sensitive features,\nrepresentations and classiﬁcation models (Zhang\net al., 2018; Elazar and Goldberg, 2018; Beutel\net al., 2017; Han et al., 2021). Our work differs\nfrom this line of research in two ways. First, our\nwork aims to mitigate biases in the PLMs. Sec-\nond, the crafted biased prompts are not adversarial\nexamples.\n4 Debiasing Performance\nWe evaluate the performance of Auto-Debias in\nmitigating biases in masked language models.\nDebiasing strategy benchmarks. We consider\nthe following debiasing benchmarks. Based on\nwhich stage the debiasing technique applies to, the\nbenchmarks can be grouped into three categories.\n• Pretraining: CDA is a data augmentation\nmethod that creates a gender-balanced dataset\nfor language model pretraining (Zmigrod\net al., 2019). Dropout is a debiasing method\nby increasing the dropout parameters in the\nPLMs (Webster et al., 2020);\n• Post-hoc: Sent-Debias is a post-processing\ndebias work that removing the estimated\ngender-direction from the sentence represen-\ntations (Liang et al., 2020). FairFil uses a\ncontrastive learning approach to correct the\nbiases in the sentence representations (Cheng\net al., 2021);\n• Fine-tuning: Context-Debias proposes to de-\nbias PLM by a loss function that encour-\nages the stereotype words and gender-speciﬁc\nwords to be orthogonal (Kaneko and Bolle-\ngala, 2021). DebiasBERT proposes to use the\nequalizing loss to equalize the associations of\ngender-speciﬁc words (Garimella et al., 2021).\nBoth works essentially ﬁne-tune the parame-\nters in PLMs.\nOur proposed Auto-Debias approach belongs to\nthe ﬁne-tuning category. It does not require any ex-\nternal corpus compared to the previous ﬁne-tuning\ndebiasing approaches.\nPretrained Models . In the experiments, we\nconsider three popular masked language models:\nBERT (Devlin et al., 2019), ALBERT (Lan et al.,\n2019) and RoBERTa (Liu et al., 2019). We im-\nplement BERT, ALBERT, and RoBERTa using\nthe Huggingface Transformers library (Wolf et al.,\n2020).\nBias Word List. Debiasing approaches leverage\nexisting hand-curated target concepts and stereo-\ntype word lists to identify and mitigate biases in\nthe PLMs. Those word lists are often developed\nbased on concepts or methods from psychology or\nother social science literature, to reﬂect cultural\nand cognitive biases. In our experiments, we aim\nto mitigate gender or racial biases. Following prior\ndebiasing approaches, we obtain the gender con-\ncept/stereotype word lists used in (Kaneko and Bol-\nlegala, 2021)2 and racial concept/stereotype word\nlists used in (Manzini et al., 2019)3.\nEvaluating Biases: SEAT. Sentence Embedding\nAssociation Test (SEAT) (May et al., 2019) is\na common metric used to assess the biases in\nthe PLM embeddings. It extends the standard\nstatic word embedding association test (WEAT)\n(Caliskan et al., 2017) to contextualized word em-\nbeddings. SEAT leverages simple templates such\nas “This is a[n] <word>” to obtain individual\n2https://github.com/kanekomasahiro/\ncontext-debias/\n3https://github.com/TManzini/\nDebiasMulticlassWordEmbedding/\n1016\nSEAT-6 SEAT-6b SEAT-7 SEAT-7b SEAT-8 SEAT-8b avg.\nBERT 0.48 0.11 0.25 0.25 0.40 0.64 0.35\n+CDA(Zmigrod et al., 2019) 0.46 -0.19 -0.20 0.40 0.12 -0.11 0.25\n+Dropout(Webster et al., 2020) 0.38 0.38 0.31 0.40 0.48 0.58 0.42\n+Sent-Debias(Liang et al., 2020) -0.10 -0.44 0.19 0.19 -0.08 0.54 0.26\n+Context-Debias(Kaneko and Bollegala, 2021) 1.13 - 0.34 - 0.12 - 0.53\n+FairFil(Cheng et al., 2021) 0.18 0.08 0.12 0.08 0.20 0.24 0.15\n+Auto-Debias (Our approach) 0.09 0.03 0.23 0.28 0.06 0.16 0.14\nALBERT 0.36 0.18 0.50 0.09 0.33 0.25 0.28\n+CDA(Zmigrod et al., 2019) -0.24 -0.02 0.26 0.31 -0.49 0.47 0.30\n+Dropout(Webster et al., 2020) -0.31 0.09 0.53 -0.01 0.32 0.14 0.24\n+Context-Debias(Kaneko and Bollegala, 2021) 0.18 - -0.05 - -0.77 - 0.33\n+Auto-Debias (Our approach) 0.07 0.15 0.21 0.23 0.16 0.23 0.18\nRoBERTa 1.61 0.72 -0.14 0.70 0.31 0.52 0.67\n+Context-Debias(Kaneko and Bollegala, 2021) 1.27 - 0.86 - 1.14 - 1.09\n+Auto-Debias (Our approach) 0.16 0.02 0.06 0.11 0.42 0.40 0.20\nTable 1: Gender debiasing results of SEAT on BERT, ALBERT and RoBERTa. Absolute values closer to 0 are\nbetter. Auto-Debias achieves better debiasing performance. The results of Sent-Debias, Context-Debias, FairFil\nare from the original papers. CDA, Dropout are reproduced from the released model (Webster et al., 2020). \"-\"\nmeans the value is not reported in the original paper.\nStereo Anti-stereo Overall\nBERT 55.06 62.14 57.63\n+Auto-Debias 52.64 58.44 54.92\nALBERT 54.72 60.19 56.87\n+Auto-Debias 43.58 54.47 47.86\nRoBERTa 62.89 42.72 54.96\n+Auto-Debias 53.53 44.08 49.77\nTable 2: Gender debiasing performance on CrowS-\nPairs. An ideally debiased model should achieve a\nscore of 50%. Auto-Debias mitigates the overall bias\non all three models.\nword’s context-independent embeddings, which\nallows measuring the association between two\ndemographic-speciﬁc words (e.g., man and woman)\nand stereotypes words (e.g., career and family). An\nideally unbiased model should exhibit no differ-\nence between the demographic-speciﬁc words and\ntheir similarity to the stereotype words. We report\nthe effect size in the SEAT evaluation. Effect size\nwith an absolute value closer to 0 indicates lower\nbiases. In the experiment, following prior work\n(Liang et al., 2020; Kaneko and Bollegala, 2021),\nwe use SEAT 6, 6b, 7, 7b, 8, and 8b for measuring\ngender bias. Also, we use SEAT 3, 3b, 4, 5, and 5b\nfor measuring racial bias. The SEAT test details, in-\ncluding the bias types and demographic/stereotype\nword associations, are presented in Appendix A.\nExperiment Setting. In our prompt searching al-\ngorithm 1, we set the maximum biased prompt\nlength PL as ﬁve and beam search width K as\n100. In total, we automatically generate 500 biased\nprompts for debiasing each model. In the gender\ndebias experiments, we use BERT-base-uncased,\nRoBERTa-base, and ALBERT-large-v2. In the\nracial debiasing experiments, we use BERT-base-\nuncased and ALBERT-base-v2. We use different\nALBERT models in the two experiments to allow\na fair comparison with existing benchmarks. We\ndo not debias RoBERTa-base in the race experi-\nment because it has a pretty fair score in the SEAT\nmetric. All Auto-Debias models are trained for 1\nepoch with AdamW (Loshchilov and Hutter, 2019)\noptimizer and 1e−5 learning rate. All models are\ntrained on a single instance of NVIDIA RTX 3090\nGPU card. For gender and race experiments, we\nrun Auto-Debias separately on each base model\nﬁve times and report the average score for the eval-\nuation metrics4.\n4.1 Mitigating gender bias\nSEAT.We report gender debiasing results in Table\n1, leading to several ﬁndings. First, our proposed\nAuto-Debias approach can meaningfully mitigate\ngender bias on the three tested masked language\nmodels BERT, ALBERT, and RoBERTa, in terms\nof the SEAT metric performance. For example,\nthe average SEAT score of the original BERT, AL-\nBERT, and RoBERTa is 0.35, 0.28, and 0.67, re-\nspectively. Auto-Debias can substantially reduce\nthe score to 0.14, 0.18, and 0.20. Second, Auto-\nDebias is more effective in mitigating gender biases\ncompared to the existing state-of-the-art bench-\n4The SEAT score is based on the average ofabsolute value.\n1017\nPrompt Length Generated Prompts\n1 substitute, premier, united, became, liberal, major, acting, professional, technical, against, political\n2 united domestic, substitute foreign, acting ﬁeld, eventual united, professional domestic, athletic and\n3 professional domestic real, bulgarian domestic assisted, former united free, united former inside\n4 eventual united reading and, former united choice for, professional domestic central victoria\n5 united former feature right and, former united choice for new, eventual united reading and\nTable 3: Examples of prompts generated by Biased Prompt Search (BERT model, for gender).\nSEAT-3 SEAT-3b SEAT-4\nBERT -0.10 0.37 0.21\n+Auto-Debias 0.25 0.19 0.12\nALBERT 0.60 0.29 0.53\n+Auto-Debias 0.10 0.12 0.19\nSEAT-5 SEAT-5b avg.\nBERT 0.16 0.34 0.23\n+Auto-Debias 0.15 0.17 0.18\nALBERT 0.40 0.46 0.46\n+Auto-Debias 0.26 0.19 0.17\nTable 4: Mitigating racial biases in BERT and AL-\nBERT. RoBERTa is excluded because it barely exhibits\nracial bias in terms of the SEAT metric.\nmarks. BERT is the most studied model in prior\nwork, so we include the state-of-the-art debiasing\nnumbers reported in existing benchmark papers.\nWe can see that Auto-Debias achieves the lowest\naverage SEAT score in all three pretrained model\nexperiments. For example, in SEAT-6 and SEAT-\n6b, where we examine the association between\nmale/female names/terms and career/family terms,\nAuto-Debias achieves SEAT scores that are close to\n0, indicating the debiased model can almost elimi-\nnate the gender bias in the career/family direction.\nThird, we observe that Auto-Debias, while achiev-\ning the lowest average SEAT score, is also rela-\ntively stable on SEAT score across different tasks.\nConversely, benchmark debiasing approaches have\nhigh variance across tasks, which is consistent with\nrecent empirical ﬁndings (Meade et al., 2021). This\nindicates that Auto-Debias is a more stable and gen-\neralizable in terms of its debiasing performance.\nCrowS-Pairs. In addition to the word associa-\ntion test, we also evaluate debiasing performance\nusing the Crowdsourced Stereotype Pairs bench-\nmark (CrowS-Pairs) (Nangia et al., 2020). This\ndataset contains a set of sentence pairs that are in-\ntended to be minimally distant, semantically speak-\ning, except that one sentence in each pair is con-\nsidered to be more indicative of stereotyping than\nthe other. The CrowS-Pairs benchmark metric mea-\nsures the percentage of sentence pairs in which the\nlanguage model assigns a higher likelihood to the\nsentence deemed to be more stereotyping. An ideal\nmodel is expected to achieve a score of 50%.\nTable 2 shows the debiasing performance on\nCrowS-Pairs (gender subset) for BERT, ALBERT,\nand RoBERTa. The original model’s stereotype\nscores are also presented in the table for direct\nreference. Note that a score closer to 50 is pre-\nferred, as it implies that the model assigns equal\nprobability to male and female sentences. In the\nBERT and RoBERTa models, Auto-Debias reduces\nthe language models’ bias and assigns more equal\nlikelihood to the sentences in both gender groups.\nInterestingly, in ALBERT, for the sentences in\nthe dataset that demonstrate stereotypes ( Stereo),\nAuto-Debias even over-corrects the stereotypes:\nit slightly prefers the historically disadvantaged\ngroups. Overall, Auto-Debias can reduce the bi-\nases in all three models.\nBiased prompts. We present some examples\nof the generated biased prompts in Table 3. Al-\nthough the biased prompts from Auto-Debias are\nnot grammatical, which is expected in the case of\nautomatically generated prompts (Shin et al., 2020;\nZhong et al., 2021), they do contain stereotype re-\nlated tokens such as professional, political, and\nliberal. Also, the automated biased generation can\nminimize human effort and may scale well.\n4.2 Mitigating racial bias\nMitigating non-gender biases is a challenging task\nin debiasing research. Meade et al. (2021) empiri-\ncally show that some of the debiasing techniques\nconsidered in our benchmarks generalize poorly\nin racial debiasing. One of the challenges could\nbe the ambiguity of words (white, black) in differ-\nent contexts. Therefore, the counterfactual data-\naugmentation approach or the ﬁne-tuning approach\nrelying on external corpora may be less effective.\nIn this experiment, we evaluate Auto-Debias’s\nperformance in mitigating racial biases in the PLMs\nand evaluate the performance using SEAT 3, 3b, 4,\n5, and 5b tests. Table 4 reports the SEAT score on\n1018\nCoLA SST-2 MRPC STS-B QQP MNLI QNLI RTE WNLI\nBERT 0.53 0.92 0.88 0.87 0.90 0.84/0.85 0.92 0.58 0.55\n+Auto-Debias 0.52 0.92 0.89 0.88 0.91 0.84/0.85 0.91 0.60 0.56\nALBERT 0.59 0.92 0.91 0.91 0.91 0.88/0.87 0.92 0.74 0.55\n+Auto-Debias 0.58 0.94 0.91 0.90 0.91 0.87/0.87 0.92 0.75 0.47\nRoBERTa 0.52 0.94 0.89 0.88 0.91 0.88/0.87 0.93 0.61 0.56\n+Auto-Debias 0.46 0.94 0.89 0.87 0.91 0.88/0.87 0.93 0.61 0.56\nTable 5: GLUE test results on the original and the gender-debiased PLMs. Auto-Debias can mitigate the bias while\nalso maintaining the language modeling capability.\nthe original and debiased BERT and ALBERT. The\nRoBERTa model is excluded because it barely ex-\nhibits racial biases in the SEAT test with an average\nscore of 0.05. We do not include other debiasing\nbenchmarks in Table 4 because most benchmark\npapers do not focus on racial debiasing. Thus, we\nfocus on comparing the Auto-Debias performance\nagainst the original models.\nWe can see from Table 4 that Auto-Debias\ncan meaningfully mitigate the racial biases in\nterms of the SEAT metric. Note that the racial\nSEAT test examines any association difference\nbetween European-American/African American\nnames/terms and the stereotype words (pleasant vs.\nunpleasant). For example, on BERT, Auto-Debias\nconsiderably mitigates the racial bias in 4 out of\n5 SEAT sub-tests, and the overall score is reduced\nfrom 0.23 to 0.18. On ALBERT, Auto-Debias also\nsigniﬁcantly mitigates the bias in all subsets.\n5 Does Auto-Debias affect downstream\nNLP tasks?\nMeade et al. (2021) ﬁnd that the previous debi-\nasing techniques often come at a price of wors-\nened performance in downstream NLP tasks, which\nimplies that prior work might over-debias. Our\nwork instead directly probes the bias encoded\nin PLM, alleviating the concern of over-debias.\nIn this section, we evaluate the gender debiased\nBERT/ALBERT/RoBERTa on the General Lan-\nguage Understanding Evaluation (GLUE) bench-\nmark (Wang et al., 2019), to examine the capa-\nbilities of the language models. The results are\nreported in Table 5. The racial-debiased PLM mod-\nels achieve similar GLUE scores.\nAuto-Debias performs on par with the base mod-\nels on most natural language understanding tasks.\nThere is only one exception: CoLA dataset. CoLA\nevaluates linguistic acceptability, judging whether\na sentence is grammatically correct. Our method\nadjusts the distribution of words using prompts,\nwhich may affect the grammatical knowledge con-\ntained in PLMs. But overall speaking, Auto-Debias\ndoes not adversely affect the downstream perfor-\nmance. Taking the results together, we see that\nAuto-Debias can alleviate the bias concerns while\nalso maintaining language modeling capability.\n6 Discussion\nPrompts have been an effective tool in probing the\ninternal knowledge relations of language models\n(Petroni et al., 2019), and they can also reﬂect the\nstereotypes encompassed in PLMs (Ousidhoum\net al., 2021; Sheng et al., 2019). Ideally, when\nprompted with different demographic targets and\npotential stereotype words, a fair language model’s\ngenerated predictions should be equally likely. Our\nmethod shows that, from the other direction, im-\nposing fairness constraints on the prompting results\ncan effectively promote the fairness of a language\nmodel.\nWe also observe a trade-off between efﬁciency\nand equity: tuning with more training steps, more\nprompts and more target words leads to a fairer\nmodel (which can even make the SEAT score very\nclose to 0), however, it comes at the price of harm-\ning the language modeling ability. Over-tuning\nmay harm the internal language patterns. It is im-\nportant to strike a balance between efﬁciency and\nequity with appropriate ﬁne-tuning.\nAlso, in order not to break the desirable con-\nnections between targets and attributes, carefully\nselecting the target words and stereotyped attribute\nwords is crucial. However, acquiring such word\nlists is difﬁcult and depends on the downstream ap-\nplications. Some prior work establishes word lists\nbased on theories, concepts, and methods from psy-\nchology and other social science literature(Kaneko\nand Bollegala, 2021; Manzini et al., 2019). How-\never, such stereotyped word lists are usually lim-\n1019\nited, are often contextualized, and offer limited\ncoverage. Moreover, word lists about other pro-\ntected groups, such as the groups related to edu-\ncation, literacy, or income, or even intersectional\nbiases (Abbasi et al., 2021), are still missing. One\npromising method to acquire such word lists is to\nprobe related words from a pre-trained language\nmodel, for example, “the man/woman has a job as\n[MASK]” yields job titles that reﬂect the stereo-\ntypes. We leave such probing-based stereotype\nword-list generation as an important and open fu-\nture direction.\n7 Conclusion\nIn this work, we propose Auto-Debias, a frame-\nwork and method for automatically mitigating the\nbiases and stereotypes encoded in PLMs. Com-\npared to previous efforts that rely on external cor-\npora to obtain context-dependent word embeddings,\nour approach automatically searches for biased\nprompts in the PLMs. Therefore, our approach\nis effective, efﬁcient, and is perhaps also more\nobjective than prior methods that rely heavily on\nmanually crafted lists of stereotype words. Experi-\nmental results on standard benchmarks show that\nAuto-Debias reduces gender and race biases more\neffectively than prior efforts. Moreover, the debi-\nased models also maintain good language model-\ning capability. Bias in NLP systems can stem from\ndifferent aspects such as training data, pretrained\nembeddings, or through ampliﬁcation when ﬁne-\ntuning the machine learning models. We believe\nthis work contributes to the emerging literature\nthat sheds light on practical and effective debiasing\ntechniques.\nAcknowledgement\nThis work was funded in part through U.S. NSF\ngrant IIS-2039915 and an Oracle for Research grant\nentitled \"NLP for the Greater Good.\"\nReferences\nAhmed Abbasi, David Dobolyi, John P Lalor,\nRichard G Netemeyer, Kendall Smith, and Yi Yang.\n2021. Constructing a psychometric testbed for fair\nnatural language processing. In Proceedings of\nEMNLP, pages 3748–3758.\nFaizan Ahmad, Ahmed Abbasi, Jingjing Li, David G\nDobolyi, Richard G Netemeyer, Gari D Clifford, and\nHsinchun Chen. 2020. A deep learning architec-\nture for psychometric natural language processing.\nACM Transactions on Information Systems (TOIS),\n38(1):1–29.\nAlex Beutel, Jilin Chen, Zhe Zhao, and Ed H Chi. 2017.\nData decisions and theoretical implications when\nadversarially learning fair representations. arXiv\npreprint arXiv:1707.00075.\nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and\nHanna M. Wallach. 2020. Language (technology) is\npower: A critical survey of \"bias\" in NLP. In Pro-\nceedings of the 58th Annual Meeting of the Associ-\nation for Computational Linguistics, ACL 2020, On-\nline, July 5-10, 2020, pages 5454–5476. Association\nfor Computational Linguistics.\nTolga Bolukbasi, Kai-Wei Chang, James Y . Zou,\nVenkatesh Saligrama, and Adam Tauman Kalai.\n2016. Man is to computer programmer as woman\nis to homemaker? debiasing word embeddings.\nIn Advances in Neural Information Processing Sys-\ntems 29: Annual Conference on Neural Informa-\ntion Processing Systems 2016, December 5-10, 2016,\nBarcelona, Spain, pages 4349–4357.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, and Pranav et al. Shyam. Language\nmodels are few-shot learners. In Advances in Neural\nInformation Processing Systems, pages 1877–1901.\nCurran Associates, Inc.\nAylin Caliskan, Joanna J Bryson, and Arvind\nNarayanan. 2017. Semantics derived automatically\nfrom language corpora contain human-like biases.\nScience, 356(6334):183–186.\nPengyu Cheng, Weituo Hao, Siyang Yuan, Shijing Si,\nand Lawrence Carin. 2021. Fairﬁl: Contrastive neu-\nral debiasing method for pretrained text encoders. In\nProceedings of ICLR.\nHillary Dawkins. 2021. Marked attribute bias in nat-\nural language inference. In Findings of the Associ-\nation for Computational Linguistics: ACL-IJCNLP\n2021, pages 4214–4226.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171–4186. Association for Computa-\ntional Linguistics.\nLucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain,\nand Lucy Vasserman. 2018. Measuring and mitigat-\ning unintended bias in text classiﬁcation. In Pro-\nceedings of the 2018 AAAI/ACM Conference on AI,\nEthics, and Society, pages 67–73.\nYanai Elazar and Yoav Goldberg. 2018. Adversarial\nremoval of demographic attributes from text data. In\nProceedings of EMNLP, pages 11–21.\n1020\nNikhil Garg, Londa Schiebinger, Dan Jurafsky, and\nJames Zou. 2018. Word embeddings quantify\n100 years of gender and ethnic stereotypes. Pro-\nceedings of the National Academy of Sciences ,\n115(16):E3635–E3644.\nAparna Garimella, Akhash Amarnath, Kiran Ku-\nmar, Akash Pramod Yalla, N Anandhavelu, Niyati\nChhaya, and Balaji Vasan Srinivasan. 2021. He is\nvery intelligent, she is very beautiful? on mitigat-\ning social biases in language modelling and gener-\nation. In Findings of the Association for Computa-\ntional Linguistics: ACL-IJCNLP 2021, pages 4534–\n4545.\nHila Gonen and Yoav Goldberg. 2019. Lipstick on a\npig: Debiasing methods cover up systematic gender\nbiases in word embeddings but do not remove them.\nIn NAACL), pages 609–614.\nIan J Goodfellow, Jonathon Shlens, and Christian\nSzegedy. 2014. Explaining and harnessing adversar-\nial examples. arXiv preprint arXiv:1412.6572.\nWei Guo and Aylin Caliskan. 2021. Detecting emer-\ngent intersectional biases: Contextualized word em-\nbeddings contain a distribution of human-like biases.\nIn Proceedings of the 2021 AAAI/ACM Conference\non AI, Ethics, and Society, pages 122–133.\nXudong Han, Timothy Baldwin, and Trevor Cohn.\n2021. Decoupling adversarial training for fair nlp.\nIn Findings of the ACL, pages 471–477.\nBen Hutchinson, Vinodkumar Prabhakaran, Emily\nDenton, Kellie Webster, Yu Zhong, and Stephen De-\nnuyl. 2020. Social biases in nlp models as barriers\nfor persons with disabilities. In ACL, pages 5491–\n5501.\nMasahiro Kaneko and Danushka Bollegala. 2019.\nGender-preserving debiasing for pre-trained word\nembeddings. In Proceedings of ACL, pages 1641–\n1650.\nMasahiro Kaneko and Danushka Bollegala. 2021. De-\nbiasing pre-trained contextualised embeddings. In\nProc. of the 16th European Chapter of the Associa-\ntion for Computational Linguistics (EACL).\nSvetlana Kiritchenko and Saif Mohammad. 2018. Ex-\namining gender and race bias in two hundred sen-\ntiment analysis systems. In Proceedings of LREC,\npages 43–53.\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,\nand Yulia Tsvetkov. 2019. Measuring bias in con-\ntextualized word representations. arXiv preprint\narXiv:1906.07337.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nAnne Lauscher, Tobias Lüken, and Goran Glavaš. 2021.\nSustainable modular debiasing of language models.\narXiv preprint arXiv:2109.03646.\nPaul Pu Liang, Irene Mengze Li, Emily Zheng,\nYao Chong Lim, Ruslan Salakhutdinov, and Louis-\nPhilippe Morency. 2020. Towards debiasing sen-\ntence representations. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decou-\npled weight decay regularization. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nThomas Manzini, Yao Chong Lim, Yulia Tsvetkov, and\nAlan W Black. 2019. Black is to criminal as cau-\ncasian is to police: Detecting and removing multi-\nclass bias in word embeddings. In NAACL.\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\nBowman, and Rachel Rudinger. 2019. On measur-\ning social biases in sentence encoders. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, NAACL-\nHLT 2019, Minneapolis, MN, USA, June 2-7, 2019,\nVolume 1 (Long and Short Papers), pages 622–628.\nAssociation for Computational Linguistics.\nNicholas Meade, Elinor Poole-Dayan, and Siva Reddy.\n2021. An empirical survey of the effectiveness of de-\nbiasing techniques for pre-trained language models.\narXiv preprint arXiv:2110.08527.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020. Crows-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, Online, November 16-20,\n2020, pages 1953–1967. Association for Computa-\ntional Linguistics.\nNedjma Djouhra Ousidhoum, Xinran Zhao, Tianqing\nFang, Yangqiu Song, and Dit Yan Yeung. 2021.\nProbing toxic content in large pre-trained language\nmodels. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\n1021\nand the 11th International Joint Conference on Nat-\nural Language Processing.\nNicolas Papernot, Patrick McDaniel, Ian Goodfellow,\nSomesh Jha, Z Berkay Celik, and Ananthram Swami.\n2017. Practical black-box attacks against machine\nlearning. In Proceedings of the 2017 ACM on Asia\nconference on computer and communications secu-\nrity, pages 506–519.\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton\nBakhtin, Yuxiang Wu, Alexander H Miller, and Se-\nbastian Riedel. 2019. Language models as knowl-\nedge bases? In Proceedings of EMNLP.\nVinodkumar Prabhakaran, Ben Hutchinson, and Mar-\ngaret Mitchell. 2019. Perturbation sensitivity analy-\nsis to detect unintended model biases. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 5740–5745.\nShauli Ravfogel, Yanai Elazar, Hila Gonen, Michael\nTwiton, and Yoav Goldberg. 2020. Null it out:\nGuarding protected attributes by iterative nullspace\nprojection. In ACL, pages 7237–7256.\nRachel Rudinger, Jason Naradowsky, Brian Leonard,\nand Benjamin Van Durme. 2018. Gender bias in\ncoreference resolution. In Proceedings of NAACL,\npages 8–14.\nTimo Schick, Sahana Udupa, and Hinrich Schütze.\n2021. Self-diagnosis and self-debiasing: A pro-\nposal for reducing corpus-based bias in nlp. arXiv\npreprint arXiv:2103.00453.\nEmily Sheng, Kai-Wei Chang, Prem Natarajan, and\nNanyun Peng. 2019. The woman worked as a\nbabysitter: On biases in language generation. In\nEMNLP, pages 3407–3412.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nEliciting knowledge from language models with au-\ntomatically generated prompts. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020, pages 4222–4235. Associ-\nation for Computational Linguistics.\nYi Chern Tan and L Elisa Celis. 2019. Assessing so-\ncial and intersectional biases in contextualized word\nrepresentations. arXiv preprint arXiv:1911.01485.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th\nInternational Conference on Learning Representa-\ntions, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019. OpenReview.net.\nKellie Webster, Xuezhi Wang, Ian Tenney, Alex Beu-\ntel, Emily Pitler, Ellie Pavlick, Jilin Chen, Ed Chi,\nand Slav Petrov. 2020. Measuring and reducing\ngendered correlations in pre-trained models. arXiv\npreprint arXiv:2010.06032.\nThomas Wolf, Julien Chaumond, Lysandre Debut, Vic-\ntor Sanh, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Morgan Funtowicz, Joe Davison, Sam\nShleifer, et al. 2020. Transformers: State-of-the-\nart natural language processing. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations,\npages 38–45.\nRobert Wolfe and Aylin Caliskan. 2021. Low fre-\nquency names exhibit bias and overﬁtting in con-\ntextualizing language models. arXiv preprint\narXiv:2110.00672.\nBrian Hu Zhang, Blake Lemoine, and Margaret\nMitchell. 2018. Mitigating unwanted biases with\nadversarial learning. In Proceedings of the 2018\nAAAI/ACM Conference on AI, Ethics, and Society,\npages 335–340.\nHaoran Zhang, Amy X Lu, Mohamed Abdalla,\nMatthew McDermott, and Marzyeh Ghassemi. 2020.\nHurtful words: quantifying biases in clinical con-\ntextual word embeddings. In proceedings of the\nACM Conference on Health, Inference, and Learn-\ning, pages 110–120.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cot-\nterell, Vicente Ordonez, and Kai-Wei Chang. 2019.\nGender bias in contextualized word embeddings. In\nProceedings of NAACL-HLT, pages 629–634.\nJieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kai-\nWei Chang. 2018. Learning gender-neutral word\nembeddings. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning, Brussels, Belgium, October 31 - November 4,\n2018, pages 4847–4853. Association for Computa-\ntional Linguistics.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: learning vs. learning\nto recall. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2021, Online, June 6-11,\n2021, pages 5017–5033. Association for Computa-\ntional Linguistics.\nRan Zmigrod, Sabrina J Mielke, Hanna Wallach, and\nRyan Cotterell. 2019. Counterfactual data augmen-\ntation for mitigating gender stereotypes in languages\nwith rich morphology. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 1651–1661.\n1022\nA Appendix: SEAT Test Details\nWe present more information on the SEAT tests\nthat are used in the experiments, in Table 6.\nB Appendix: Target Word Lists\nWe provide details about the gender and racial word\nlists used in the debiasing experiments.\nFor gender, we use the target concept words and\nstereotype words listed in (Kaneko and Bollegala,\n2021).\nFor race, we use the target concept words and\nstereotype words listed in (Manzini et al., 2019),\nwith a slight modiﬁcation on the target concept\nwords. We present the racial concept word lists\nbelow:\nAfrican American: black, african, black, africa,\nafrica, africa, black people, african people, black\npeople, the africa\nEuropean American: caucasian, caucasian,\nwhite, america, america, europe, caucasian peo-\nple, caucasian people, white people, the america\nBias type Test Demographic-speciﬁc words Stereotype words\nRacial\nSEAT-3 European-American/African American names Pleasant vs. Unpleasant\nSEAT-3b European-American/African American terms Pleasant vs. Unpleasant\nSEAT-4 European-American/African American names Pleasant vs. Unpleasant\nSEAT-5 European-American/African American names Pleasant vs. Unpleasant\nSEAT-5b European-American/African American terms Pleasant vs. Unpleasant\nGender\nSEAT-6 Male vs. Female names Career vs. Family\nSEAT-6b Male vs. Female terms Career vs. Family\nSEAT-7 Male vs. Female terms Math vs. Arts\nSEAT-7b Male vs. Female names Math vs. Arts\nSEAT-8 Male vs. Female terms Science vs. Arts\nSEAT-8b Male vs. Female names Science vs. Arts\nTable 6: The SEAT test details, extended from (Caliskan et al., 2017).\n1023",
  "topic": "Debiasing",
  "concepts": [
    {
      "name": "Debiasing",
      "score": 0.9869735836982727
    },
    {
      "name": "Computer science",
      "score": 0.8191693425178528
    },
    {
      "name": "Language model",
      "score": 0.7037250399589539
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6950898766517639
    },
    {
      "name": "Task (project management)",
      "score": 0.6789906024932861
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5482221245765686
    },
    {
      "name": "Natural language processing",
      "score": 0.5315936207771301
    },
    {
      "name": "Machine learning",
      "score": 0.47585365176200867
    },
    {
      "name": "Psychology",
      "score": 0.10320219397544861
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Cognitive science",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I107639228",
      "name": "University of Notre Dame",
      "country": "US"
    }
  ]
}