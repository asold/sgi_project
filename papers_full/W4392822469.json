{
  "title": "Towards Personalized Evaluation of Large Language Models with An Anonymous Crowd-Sourcing Platform",
  "url": "https://openalex.org/W4392822469",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2375549972",
      "name": "Cheng, Mingyue",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A1979946437",
      "name": "Zhang Hao",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2374247389",
      "name": "Yang Ji-qian",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2108859322",
      "name": "Liu Qi",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A1964094874",
      "name": "Li Li",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2107139585",
      "name": "Huang Xin",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2026576832",
      "name": "Song Liwei",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A1965783116",
      "name": "Li Zhi",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2128312296",
      "name": "Huang, Zhenya",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A167579018",
      "name": "Chen, Enhong",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2798881875",
    "https://openalex.org/W2335612402",
    "https://openalex.org/W1991055526"
  ],
  "abstract": "Large language model evaluation plays a pivotal role in the enhancement of its capacity. Previously, numerous methods for evaluating large language models have been proposed in this area. Despite their effectiveness, these existing works mainly focus on assessing objective questions, overlooking the capability to evaluate subjective questions which is extremely common for large language models. Additionally, these methods predominantly utilize centralized datasets for evaluation, with question banks concentrated within the evaluation platforms themselves. Moreover, the evaluation processes employed by these platforms often overlook personalized factors, neglecting to consider the individual characteristics of both the evaluators and the models being evaluated. To address these limitations, we propose a novel anonymous crowd-sourcing evaluation platform, BingJian, for large language models that employs a competitive scoring mechanism where users participate in ranking models based on their performance. This platform stands out not only for its support of centralized evaluations to assess the general capabilities of models but also for offering an open evaluation gateway. Through this gateway, users have the opportunity to submit their questions, testing the models on a personalized and potentially broader range of capabilities. Furthermore, our platform introduces personalized evaluation scenarios, leveraging various forms of human-computer interaction to assess large language models in a manner that accounts for individual user preferences and contexts. The demonstration of BingJian can be accessed at https://github.com/Mingyue-Cheng/Bingjian.",
  "full_text": "Towards Personalized Evaluation of Large Language Models with\nAn Anonymous Crowd-Sourcing Platform\nMingyue Cheng1, Hao Zhang1, Jiqian Yang1, Qi Liu1*, Li Li1, Xin Huang1, Liwei Song1,\nZhi Li2, Zhenya Huang1, Enhong Chen1\n1Anhui Province Key Laboratory of Big Data Analysis and Application, University of Science and Technology of China\n& State Key Laboratory of Cognitive Intelligence, Hefei, China,\n2 Shenzhen International Graduate School, Tsinghua University, Shenzhen, China\n{mycheng,qiliuql,huangzhy,cheneh}@ustc.edu.cn,{zh2001,yangjq,lili0516,wuli_error,songliv}@mail.ustc.edu.cn,\nzhilizl@sz.tsinghua.edu.cn\nABSTRACT\nLarge language model evaluation plays a pivotal role in the enhance-\nment of its capacity. Previously, numerous methods for evaluating\nlarge language models have been proposed in this area. Despite their\neffectiveness, these existing works mainly focus on assessing ob-\njective questions, overlooking the capability to evaluate subjective\nquestions which is extremely common for large language mod-\nels. Additionally, these methods predominantly utilize centralized\ndatasets for evaluation, with question banks concentrated within\nthe evaluation platforms themselves. Moreover, the evaluation pro-\ncesses employed by these platforms often overlook personalized\nfactors, neglecting to consider the individual characteristics of both\nthe evaluators and the models being evaluated. To address these\nlimitations, we propose a novel anonymous crowd-sourcing evalu-\nation platform, BingJian, for large language models that employs\na competitive scoring mechanism where users participate in rank-\ning models based on their performance. This platform stands out\nnot only for its support of centralized evaluations to assess the\ngeneral capabilities of models but also for offering an open evalua-\ntion gateway. Through this gateway, users have the opportunity to\nsubmit their questions, testing the models on a personalized and\npotentially broader range of capabilities. Furthermore, our platform\nintroduces personalized evaluation scenarios, leveraging various\nforms of human-computer interaction to assess large language\nmodels in a manner that accounts for individual user preferences\nand contexts. The demonstration of BingJian can be accessed at\nhttps://github.com/Mingyue-Cheng/Bingjian.\nCCS CONCEPTS\n‚Ä¢ Computing methodologies ‚ÜíNatural language processing .\nQi Liu is corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nWWW ‚Äô24 Companion, May 13‚Äì17, 2024, Singapore, Singapore\n¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0172-6/24/05\nhttps://doi.org/10.1145/3589335.3651243\nKEYWORDS\nLarge Language Model, Personalized Evaluation, Crowdsourcing\nPlatform\nACM Reference Format:\nMingyue Cheng1, Hao Zhang1, Jiqian Yang1, Qi Liu1*, Li Li1, Xin Huang1, Li-\nwei Song1,, Zhi Li2, Zhenya Huang1, Enhong Chen1. 2024. Towards Person-\nalized Evaluation of Large Language Models with An Anonymous Crowd-\nSourcing Platform. In Companion Proceedings of the ACM Web Conference\n2024 (WWW ‚Äô24 Companion), May 13‚Äì17, 2024, Singapore, Singapore. ACM,\nNew York, NY, USA, 4 pages. https://doi.org/10.1145/3589335.3651243\n1 INTRODUCTION\nThe advent of Large Language Models (LLMs) has marked a signifi-\ncant milestone in the journey toward Artificial General Intelligence\n(AGI) [5, 7], opening new frontiers in our ability to process and un-\nderstand complex human languages at an unprecedented scale. As\nthese models become increasingly sophisticated, their evaluation\ntranscends traditional paradigms [9], challenging the very notion\nof a singular, definitive ground truth. In the realm of large language\nmodel development, the absence of a clear-cut benchmark necessi-\ntates a reimagined approach to evaluation‚Äîone that accommodates\nthe nuanced and multifaceted nature of tasks these models are de-\nsigned to tackle. This shift underscores the critical role of evaluation\nmethodologies in not only benchmarking current capabilities but\nalso in driving the evolution of model sophistication. The quality\nand depth of these evaluation mechanisms, therefore, directly influ-\nence the trajectory of large language model advancements, making\nit imperative to explore and refine our evaluative frameworks to\nkeep pace with the rapid advancements in this domain.\nIn the quest to evaluate LLMs, researchers are diligently working\nto gauge an expansive range of model capabilities, from coding pro-\nficiency to domain-specific expertise. These efforts [1, 6, 11] play\na crucial role in refining evaluation methodologies and shedding\nlight on the complex competencies of LLMs. Yet, in spite of these\nadvances, current methods face significant shortcomings that de-\nmand attention. One major issue is the dependence on centralized\ndatasets, which narrows the evaluation to a set of predetermined\nchallenges and fails to encompass decentralized, real-world prob-\nlems. Additionally, most evaluation frameworks do not adequately\nconsider the integration of personalized user data [ 3, 10], an es-\nsential factor that could provide deeper insights into how models\nperform across varied user interactions. These challenges highlight\nthe necessity for inventive evaluation approaches that not only\nbroaden the scope of problem collection to include decentralized\narXiv:2403.08305v1  [cs.CL]  13 Mar 2024\nWWW ‚Äô24 Companion, May 13‚Äì17, 2024, Singapore, Singapore\nMingyue Cheng1, Hao Zhang1, Jiqian Yang1, Qi Liu1*, Li Li1, Xin Huang1, Liwei Song1,\nZhi Li2, Zhenya Huang1, Enhong Chen1\nCentralized Evaluation \nDecentralized Evaluation \nAnswer1\nAnswer2 Dataset\nV ote\nKnowledge Database\nRandomlySelect\nCandidateLLMsUser\nAge: 24Gender: MaleEducation: ...Profession: ...Occupation: ......\nAnonymization\nModel\nA\nModel\nB\nGeneration /Discrimination\nELO Rating SystemPersonalization Analysis\nAnalysis & Visualization\nPersonalized AI Services\nFair EvaluationApplication \nUser Profile\nFigure 1: The illustration of evaluation pipeline of BingJian platform.\nissues but also factor in individual user contexts, thereby making\nthe evaluation results more relevant and applicable.\nTo address the prevailing challenges in the evaluation of LLMs,\nwe design a platform named BingJian aimed at facilitating com-\nprehensive model assessment. On this platform, responses from\ndifferent models are presented to users, who then are encouraged\nto select the most appropriate answer. To ensure a fair evaluation\nof model capabilities, BingJian employs an ELO rating system [8]\nthat adjusts model scores based on user selections. Furthermore,\nBingJian is designed as an open, crowdsourced platform. Just as\nImageNet [2] significantly advanced the field of computer vision by\nconstructing a high-quality image dataset through crowdsourced\nannotations, we aim to revolutionize the evaluation of large lan-\nguage models. Traditional objective assessments [4] fall short of\ncapturing the full capabilities of LLMs. By incorporating human\ncrowdsourcing evaluations, we introduce the most authentic form\nof human feedback. Humans assess models based on various intan-\ngible aspects, such as the quality of generated text, knowledge con-\nveyed, and presentation style, offering a comprehensive evaluation\nbeyond quantifiable metrics. Naturally, a crowdsourced platform\ncan gather a wide variety of evaluation results. These outcomes are\nclosely related not only to objective facts but also to the personalized\ninformation of the evaluators. To this end, our platform compiles a\ncomprehensive dataset of evaluator personalization information.\nWe aim to delve into this personal data to uncover the cognitive\nrelationships between humans and LLMs. This endeavor provides\na more holistic depiction of the evaluation results for LLMs, reveal-\ning how the individual characteristics of evaluators influence their\ninteractions with and assessments of these models. By exploring\nthese nuanced relationships, we can enhance our understanding\nof model performance from a perspective that integrates human\nsubjectivity, thereby enriching the evaluation process.\n2 THE PROPOSED BINGJIAN\nIn this section, we introduce our BingJian platform, as depicted in\nFigure 1. Initially, we establish a comprehensive question database\nCentralized Evaluation \nDecentralized Evaluation\nTarget Capability to EvaluationAnalysis&Visualization\nFigure 2: Centralized and decentralized evaluation interface.\nencompassing a wide array of domains, along with gathering a\nsuite of large language models for assessment. Following this, we\ndevelop interfaces incorporating both centralized and decentralized\napproaches to evaluation, enabling the collection of varied data\nsets pivotal for evaluating the multifaceted capabilities of LLMs.\nNext, we provide a detailed introduction to the BingJian platform,\nfocusing on the interface design, the evaluation process, and the\ndata analysis and visualization.\n2.1 Interface Overview\nOn the login page, we first encourage users to fill out their profile\ninformation, including age, gender, profession, and educational\nbackground, to facilitate subsequent analysis of personalized large\nlanguage model evaluation results. Then, as shown in Figure 2,\nthe BingJian evaluation interface primarily consists of two parts:\ncentralized evaluation and decentralized evaluation.\n2.1.1 Centralized Evaluation. The central evaluation is to evalu-\nate the performance of LLM on a constructed question set, which\ncovers general knowledge in various domains such as natural sci-\nences, humanities, economics, etc. In each interaction, the users\nmay choose a question displayed on the page. Then, the system\nwill randomly invoke two different models to answer the question\nTowards Personalized Evaluation of Large Language Models with An Anonymous Crowd-Sourcing Platform WWW ‚Äô24 Companion, May 13‚Äì17, 2024, Singapore, Singapore\nand anonymously display their answers along with the explana-\ntions and analyses, on the evaluation interface. Such an anonymous\nmechanism could help eliminate bias against different LLMs to\nsome extent, effectively ensuring the fairness of the evaluation.\nFurthermore, we also integrate a question-recommendation sys-\ntem. Based on users‚Äô past question browsing history, we select\nquestions that align with their interests to push to them while\nensuring, as much as possible, that users do not encounter the\nsame question twice. From the perspective of user experience, this\napproach significantly enhances the engaging nature of the crowd-\nsourced evaluation process. When considered from an evaluation\nstandpoint, this strategy aims to broaden users‚Äô evaluation activi-\nties across various related disciplines, thereby minimizing the risk\nof inaccurate assessments. This personalized and dynamic ques-\ntion recommendation not only caters to the users‚Äô preferences but\nalso enriches the evaluation dataset by capturing a wider spectrum\nof user interactions and responses, leading to more robust and\ncomprehensive insights into model performance.\n2.1.2 Decentralized Evaluation. Due to the continuous iteration\nand updating of LLMs, the dataset in the constructed question bank\nmay be incorporated into the model‚Äôs training corpus in the present\nor future, leading to biases in the evaluation results. To address this\nchallenge, we further design the decentralized evaluation module\nshown in the right column of Figure 2. Users can input custom\nquestions into the dialogue box and then click the button below to\nevaluate the generated answers. This feature greatly alleviates the\npotential issue of question leakage in the evaluation nowadays and\nfurther improves the fairness of the leader board while supporting\nthe open-domain question.\n2.2 Crowdsource Evaluation\nJust as ImageNet revolutionized computer vision research by involv-\ning a large number of contributors in labeling images, our platform\naims to harness the power of crowdsourcing to establish a compre-\nhensive evaluation process and database for large language models.\nNext, we outline the specific evaluation process following the three\nprimary data collection goals, respectively. Through the implemen-\ntation of crowdsourced evaluation, our objective is twofold: firstly,\nto objectively delineate the capabilities of numerous LLMs, and sec-\nondly, to foster the development of a benchmarking system through\nhuman-computer interaction assessments that will propel further\nadvancements in LLM technologies. This benchmark will serve as\na valuable resource for researchers and developers alike, offering a\nstandardized framework against which the progression of model\ncapabilities can be rigorously tested and compared.\n2.2.1 General Knowledge Mastery. To evaluate the general knowl-\nedge mastery of the models, we have created question banks in\nvarious domains such as nature, science, the humanities, and eco-\nnomics. The multiple-choice questions will be presented to LLM\nwith the following prompt: For the following questions, please\ngive the correct option and explanation. <Question>, (A) <An-\nswer1>, (B) <Answer2>, (C) <Answer3>, (D) <Answer4>.Then,\nthe models are required to provide the correct options for the given\nquestions. By matching their responses with the correct answers,\nwe can calculate the accuracy of the model‚Äôs answers and also\npreliminary conclude its capabilities over various domains.\nImplicit EvaluationExplicit Evaluation\nBefore Vote\nAfter Vote\nFigure 3: Evaluation process of generative ability.\n2.2.2 Generative Ability. In addition to the problem answer, ex-\nplainability is also an important evaluation indicator. As shown in\nFigure 3, users are encouraged to score the model‚Äôs generative abil-\nity based on the answers and their analysis. The evaluation mainly\ninvolves a comparison (i.e., \"JUST AS GOOD\", \"A IS BETTER\", \"B IS\nBETTER\", \"JUST AS BAD\") of the outputs generated by different\nmodels. Besides, it also includes a quantitative evaluation of the\nmodel‚Äôs generative capacity, as assessed using an exscoring system\nranging from 1 to 5. Furthermore, given the substantial amount\nof user profile information collected, we highlight that this data\ncan be utilized to analyze the correlation between the model‚Äôs re-\nsponses and personalized user profiles, providing potential research\nopportunities in the realm of personalized AI services. For instance,\ncertain user groups may be more inclined to prefer professional\nexplanations, while others may favor imaginative responses.\n2.2.3 Discriminate Ability. Evaluating the discriminate ability of\nlarge language models is essential to ensuring their reliability, ad-\ndressing biases, and benchmark performance, providing valuable\ninsights into their generalization capabilities in real-world applica-\ntions. In this vein, our evaluation framework goes beyond simply\nassessing the generative prowess of large language models. We\nalso scrutinize their ability to evaluate and judge different answers\nby engaging them in a comparative analysis. Specifically, for a\ngiven question, our system employs a double-blind method where\nit randomly selects two models, referred to as A and B, to provide\nanswers without revealing their identities. These responses are then\ndisplayed on the user interface.Finally, users are invited to partici-\npate by scoring the responses given by models C and D to evaluate\nthe discriminating ability of the large language models. Through\nthis multi-tiered evaluation process, we can identify strengths and\nweaknesses in the models‚Äô abilities to discriminate between high-\nand low-quality responses. Such insights are instrumental for itera-\ntive improvements, leading to more sophisticated and reliable AI\nsystems. Ultimately, by enhancing the discriminative capabilities\nof LLMs, we can better tailor them to a variety of applications,\nensuring that they not only produce content that is engaging and\ninformative but also critically sound and contextually appropriate.\n2.3 Analysis & Visualization\n2.3.1 ELO Rating Mechaminsm. Inspired by the renowned chess\nranking system, the Elo Rating System (ELO) provides a dynamic\nand intuitive framework for gauging relative strengths. We initial-\nize models with ELO ratings, in which winners gain ELO points\nWWW ‚Äô24 Companion, May 13‚Äì17, 2024, Singapore, Singapore\nMingyue Cheng1, Hao Zhang1, Jiqian Yang1, Qi Liu1*, Li Li1, Xin Huang1, Liwei Song1,\nZhi Li2, Zhenya Huang1, Enhong Chen1\nFigure 4: Visualization of correlation between the model‚Äôs\nresponses and personalized user group profiles.\nwhile losers lose points, ensuring a continuous adjustment of model\nrankings based on relative model performance. To be specific, it\nupdates a participant‚Äôs rating (ùëÖ‚Ä≤) based on the outcome (ùëÜ) of each\nevaluation record, which can be expressed as:\nùëÖ‚Ä≤= ùëÖ+ùêæ ¬∑(ùëÜ‚àíùê∏),\nwhere ùëÖ‚Ä≤is the updated ELO rating, ùëÖ is the pre-match ELO rat-\ning, ùêæ is a constant determining the rating change magnitude, ùëÜ\nis the match outcome (1 for a win, 0 for a loss, 0.5 for a draw)\nand ùê∏is the expected outcome calculated using a logistic function,\ni.e. ùê∏ = 1\n1+10((ùëÖùêµ‚àíùëÖùê¥)/400),in which ùëÖùê¥ and ùëÖùêµ are the initial ELO\nratings of the two participants. The logistic function ensures that\nthe expected outcome aligns with the participants‚Äô relative ratings,\nmaking ELO a dynamic and reliable measure of skill in various\ncompetitive scenarios. This innovative methodology allows for a\nfair and nuanced assessment of the model‚Äôs capability, transcending\ntraditional evaluation metrics.\n2.3.2 Crowd Analysis. To delve into the relationship between the\nresponses generated by the model and the diverse backgrounds of\nthe user groups, we embarked on a comprehensive visual analysis,\nas delineated in Figure 4. This analysis scrutinized the evaluation\ndata through the lens of various demographic dimensions, such as\nage, gender, profession, occupation, and educational attainment.\nOur information collection requires user approval. Our objective in\nthis exploratory endeavor is to detect and understand patterns that\ncould inform and enhance the customization of services provided by\nlarge language models. For example, we might discover that specific\ndemographic segments have a predilection for responses that are\nsteeped in professional jargon or technical detail, while others\nmight demonstrate a preference for responses that are more creative\nor narrative in nature. Moreover, by examining the assessment\nresults from these diverse demographic vantage points, we can\nidentify unique opportunities for research into tailored AI services.\nThis granular analysis not only aids in refining the user experience\nbut also serves as a foundational step towards the development of AI\nsystems that are sensitive to the nuanced needs and preferences of\ndifferent user groups. By integrating these insights into the iterative\ndesign of large language models, we can move closer to achieving\na level of personalized interaction that mirrors the adaptive and\ndiscerning nature of human communication.\n3 CONCLUSION\nThis paper introduced a personalized, anonymized crowd-sourcing\nplatform for evaluating the capacity of large language models, pro-\nviding users with both centralized and decentralized evaluation\nentry points. Users are enabled to assess models‚Äô generative and\ndiscriminative capabilities within this framework. Moreover, the\nplatform conducts a comprehensive analysis of users‚Äô personalized\ninformation in conjunction with model evaluation results, utilizing\nvisual statistical charts to display relevant profile information. This\ninnovative approach not only enriches the evaluation landscape\nby incorporating a human-centric perspective but also paves the\nway for a more nuanced understanding of model capabilities across\ndiverse user backgrounds and preferences. The ongoing expansion\nof model integrations underscores our commitment to offer a robust\nand dynamic evaluation environment, poised to adapt and evolve\nwith the advancing frontiers of large language model technologies.\nAcknowledgements. This research was supported by grants\nfrom the National Key Research and Development Program of China\n(Grant No. 2021YFF0901003), the National Natural Science Foun-\ndation of China (Grants No. 62337001, U20A20229), and the Fun-\ndamental Research Funds for the Central Universities. This work\nalso thanks the support of funding of SC5290005194. We thank the\nHefei Artificial Intelligence Computing Center of Hefei Big Data\nAsset Operation Co., Ltd. for providing computational resources\nfor this project.\nREFERENCES\n[1] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao\nChen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al . 2023. A survey on\nevaluation of large language models. ACM Transactions on Intelligent Systems\nand Technology (2023).\n[2] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet:\nA large-scale hierarchical image database. In 2009 IEEE conference on computer\nvision and pattern recognition . Ieee, 248‚Äì255.\n[3] Jingtao Ding, Fuli Feng, Xiangnan He, Guanghui Yu, Yong Li, and Depeng Jin.\n2018. An improved sampler for bayesian personalized ranking by leveraging\nview data. In Companion Proceedings of the The Web Conference 2018 . 13‚Äì14.\n[4] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun\nSu, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al . 2023. C-eval: A\nmulti-level multi-discipline chinese evaluation suite for foundation models.arXiv\npreprint arXiv:2305.08322 (2023).\n[5] Junzhe Jiang, Shang Qu, Mingyue Cheng, and Qi Liu. 2023. Reformulating\nSequential Recommendation: Learning Dynamic User Interest with Content-\nenriched Language Modeling. arXiv preprint arXiv:2309.10435 (2023).\n[6] Jiatong Li, Rui Li, and Qi Liu. 2023. Beyond Static Datasets: A Deep Interaction\nApproach to LLM Evaluation. arXiv preprint arXiv:2309.04369 (2023).\n[7] Yucong Luo, Mingyue Cheng, Hao Zhang, Junyu Lu, and Enhong Chen. 2023. Un-\nlocking the potential of large language models for explainable recommendations.\narXiv preprint arXiv:2312.15661 (2023).\n[8] Radek Pel√°nek. 2016. Applications of the Elo rating system in adaptive educational\nsystems. Computers & Education 98 (2016), 169‚Äì179.\n[9] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R\nBowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural\nlanguage understanding. arXiv preprint arXiv:1804.07461 (2018).\n[10] Tong Zhao, Julian McAuley, and Irwin King. 2014. Leveraging social connections\nto improve personalized ranking for collaborative filtering. In Proceedings of the\n23rd ACM international conference on conference on information and knowledge\nmanagement. 261‚Äì270.\n[11] Yan Zhuang, Qi Liu, Yuting Ning, Weizhe Huang, Rui Lv, Zhenya Huang, Guan-\nhao Zhao, Zheng Zhang, Qingyang Mao, Shijin Wang, et al . 2023. Efficiently\nMeasuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective.arXiv\npreprint arXiv:2306.10512 (2023).",
  "topic": "Crowd sourcing",
  "concepts": [
    {
      "name": "Crowd sourcing",
      "score": 0.9028782248497009
    },
    {
      "name": "Computer science",
      "score": 0.768363356590271
    },
    {
      "name": "World Wide Web",
      "score": 0.4126833975315094
    },
    {
      "name": "Human‚Äìcomputer interaction",
      "score": 0.41155701875686646
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    }
  ],
  "cited_by": 4
}