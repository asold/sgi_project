{
    "title": "Enhancing the LLM-Based Robot Manipulation Through Human-Robot Collaboration",
    "url": "https://openalex.org/W4399767172",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2846069892",
            "name": "Liu, Haokun",
            "affiliations": [
                "Nagoya University"
            ]
        },
        {
            "id": "https://openalex.org/A2666797692",
            "name": "Zhu Yao-nan",
            "affiliations": [
                "The University of Tokyo"
            ]
        },
        {
            "id": "https://openalex.org/A2150407836",
            "name": "Kato Kenji",
            "affiliations": [
                "National Center for Geriatrics and Gerontology"
            ]
        },
        {
            "id": "https://openalex.org/A2749664772",
            "name": "Tsukahara Atsushi",
            "affiliations": [
                "National Center for Geriatrics and Gerontology"
            ]
        },
        {
            "id": "https://openalex.org/A2743340483",
            "name": "Kondo Izumi",
            "affiliations": [
                "National Center for Geriatrics and Gerontology"
            ]
        },
        {
            "id": "https://openalex.org/A2750429633",
            "name": "Aoyama Tadayoshi",
            "affiliations": [
                "Nagoya University"
            ]
        },
        {
            "id": "https://openalex.org/A2229972896",
            "name": "Hasegawa Yasuhisa",
            "affiliations": [
                "Nagoya University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4385245566",
        "https://openalex.org/W6850625674",
        "https://openalex.org/W4388720459",
        "https://openalex.org/W2963037989",
        "https://openalex.org/W6791353385",
        "https://openalex.org/W4390874575",
        "https://openalex.org/W4383108457",
        "https://openalex.org/W4383172072",
        "https://openalex.org/W4383097638",
        "https://openalex.org/W4389666038",
        "https://openalex.org/W4383108753",
        "https://openalex.org/W3087853581",
        "https://openalex.org/W3006069020",
        "https://openalex.org/W4383220103",
        "https://openalex.org/W4387375948",
        "https://openalex.org/W130216483",
        "https://openalex.org/W2136719407",
        "https://openalex.org/W6680971464",
        "https://openalex.org/W4321607911",
        "https://openalex.org/W4322718191"
    ],
    "abstract": "Large Language Models (LLMs) are gaining popularity in the field of robotics.\\nHowever, LLM-based robots are limited to simple, repetitive motions due to the\\npoor integration between language models, robots, and the environment. This\\npaper proposes a novel approach to enhance the performance of LLM-based\\nautonomous manipulation through Human-Robot Collaboration (HRC). The approach\\ninvolves using a prompted GPT-4 language model to decompose high-level language\\ncommands into sequences of motions that can be executed by the robot. The\\nsystem also employs a YOLO-based perception algorithm, providing visual cues to\\nthe LLM, which aids in planning feasible motions within the specific\\nenvironment. Additionally, an HRC method is proposed by combining teleoperation\\nand Dynamic Movement Primitives (DMP), allowing the LLM-based robot to learn\\nfrom human guidance. Real-world experiments have been conducted using the\\nToyota Human Support Robot for manipulation tasks. The outcomes indicate that\\ntasks requiring complex trajectory planning and reasoning over environments can\\nbe efficiently accomplished through the incorporation of human demonstrations.\\n",
    "full_text": "This is the Accepted Version of the Article Published in IEEE Robotics and Automation Letters, vol. 9, no. 8, pp. 6904-6911, Aug. 2024.\nFinal Version Available at DOI:https://doi.org/10.1109/LRA.2024.3415931\nMultimedia Available at https://youtu.be/q9G6akvytqU\nEnhancing the LLM-Based Robot Manipulation\nThrough Human-Robot Collaboration\nHaokun Liu, Yaonan Zhu ∗, Kenji Kato, Atsushi Tsukahara, Izumi Kondo,\nTadayoshi Aoyama, and Yasuhisa Hasegawa\nAbstract— Large Language Models (LLMs) are gaining popular-\nity in the field of robotics. However, LLM-based robots are limited\nto simple, repetitive motions due to the poor integration between\nlanguage models, robots, and the environment. This paper pro-\nposes a novel approach to enhance the performance of LLM-based\nautonomous manipulation through Human-Robot Collaboration\n(HRC). The approach involves using a prompted GPT-4 language\nmodel to decompose high-level language commands into sequences\nof motions that can be executed by the robot. The system also\nemploys a YOLO-based perception algorithm, providing visual\ncues to the LLM, which aids in planning feasible motions within\nthe specific environment. Additionally, an HRC method is proposed\nby combining teleoperation and Dynamic Movement Primitives\n(DMP), allowing the LLM-based robot to learn from human\nguidance. Real-world experiments have been conducted using\nthe Toyota Human Support Robot for manipulation tasks. The\noutcomes indicate that tasks requiring complex trajectory planning\nand reasoning over environments can be efficiently accomplished\nthrough the incorporation of human demonstrations.\nI. I NTRODUCTION\nThe concept of autonomous robots capable of interacting\nwith humans via natural language has captivated the field of\nrobotics research for years. Recently, the emergence of Large\nLanguage Models (LLMs), powered by Transformer technol-\nogy, has presented a practical approach to realize this vision\n[1]. Over the years, LLMs have emerged as a key technology\nin machine learning, demonstrating remarkable capabilities in\nlanguage processing [2]. Notably, LLMs have shown consider-\nable promise in areas such as human-robot interaction and task\nplanning [3], [4].\nFor LLMs to effectively guide robot behavior in real-world\nscenarios, integrating environmental perception is crucial. This\nsynergy enables the robot to autonomously interpret and act\nupon user instructions in real-world scenarios. Key technologies\nthat facilitate this include computer vision algorithms like You\nThis work was supported in part by JST SICORP, Japan, under Grant\nJPMJSC2305; and in part by JSPS KAKENHI under Grant JP24K17236;\nand in part by NCGG under Chojuiryou Kenkyukaihatsuhi Nos. 19–5, 21-21.\n(∗Corresponding author: Yaonan Zhu)\nHaokun Liu is with Department of Mechanical Systems Engineering, Nagoya\nUniversity, Nagoya 464-8603, Japan (e-mail: haokun@robo.mein.nagoya-\nu.ac.jp).\nYaonan Zhu is with the School of Engineering, The University of\nTokyo, Tokyo 113-8656, Japan, and Department of Micro-Nano Mechani-\ncal Science and Engineering, Nagoya University, Nagoya 464-8603, Japan\n(yaonan.zhu@weblab.t.u-tokyo.ac.jp).\nTadayoshi Aoyama and Yasuhisa Hasegawa are with Department of Micro-\nNano Mechanical Science and Engineering, Nagoya University, Nagoya 464-\n8603, Japan (e-mail: {aoyama, hasegawa}@mein.nagoya-u.ac.jp).\nKenji Kato, Atsushi Tsukahara and Izumi Kondo are with National Center\nfor Geriatrics and Gerontology, Obu, Aichi, 474-8511, Japan (e-mail: {kk0724,\ntsukahara, ik7710}@ncgg.go.jp).\nDigital Object Identifier (DOI): https://doi.org/10.1109/LRA.2024.3415931\nWaiting for command Find the oven and open it…\nU: I want to warm up my lunch!L: Use microwave oven or gas?U: Microwave oven.\nLLM(prompted)\nEnvironmentPerception\nUserRobot Successyes\nHuman teleoperator\nno\nRe-perform the failed motion in teleoperation modeInstruct and save a new skill\nGeneratemotion functions\nSubtask 1\ndefinecallback()move_to_position(microwave_handle)gripper_control(close)base_cycle_move()gripper_control(open)Basiclibrary\nDMPlibrary\nSubtasks \nFig. 1. An overview of an LLM-based Human-Robot Collaboration System,\nfeaturing user interaction, a basic library for pre-programmed motion functions,\nand a DMP library for adaptive motion function generation and storage to\naccomplish a complex real-world task. (e.g. “warm up my lunch”)\nOnly Look Once (YOLO) [5], and innovative approaches such\nas Contrastive Language-Image Pre-Training (CLIP) [6] and\nthe Segment Anything Model (SAM) [7]. The combination\nof LLMs with advanced computer vision has spurred the\ndevelopment of sophisticated robotic manipulation platforms\ncapable of executing complex, precise actions based on user\ncommands given by natural languages.\nDespite the impressive natural language-based interaction and\nlogical reasoning capabilities of LLMs, existing research in the\nreal-world LLM-based robot systems primarily focuses on basic\nand straightforward task planning such as picking and placing\n[8], [9], the more intricate, long-horizon tasks, such as warming\nand roasting food, which require high-level reasoning and mo-\ntion planning, have often been overlooked. These shortcomings\nare primarily due to the following challenges: (i) The prevalent\nmethod for LLM-based robot control is through generating\ncodes that trigger robots’ motion. Undoubtedly, this approach\nis a straightforward way to accomplish robot manipulation\nvia LLM. However, this approach falters when dealing with\ncomplex trajectories, as it lacks the advanced capability to in-\ntelligently modify motion commands to meet specific, dynamic\nenvironment requirements [8]. This limitation is a significant\nhurdle in enhancing robotic autonomy and responsiveness. (ii)\nIn current LLM-based human-robot interaction frameworks,\nhuman commands are typically restricted to single-instance\ninputs, limiting the scope for human continuous supervision\nand instructions. This constraint hinders the potential of these\nsystems for complex, dynamic real-world tasks. Thus, intuitive\nand seamless Human-Robot Collaboration (HRC) is essential\nto extend the channels of input and enhance the capabilities of\nLLM-based robots.\n© 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including\nreprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or\nreuse of any copyrighted component of this work in other works.\narXiv:2406.14097v2  [cs.RO]  1 Jul 2024\nThis is the Accepted Version of the Article Published in IEEE Robotics and Automation Letters, vol. 9, no. 8, pp. 6904-6911, Aug. 2024.\nFinal Version Available at DOI:https://doi.org/10.1109/LRA.2024.3415931\nMultimedia Available at https://youtu.be/q9G6akvytqU\nTo address the challenges aforementioned, this paper pro-\nposes a novel LLM-based robot manipulation framework com-\nbined with HRC, as illustrated in Fig. 1. This system advances\nbeyond simple LLM-based autonomy by integrating a teleop-\neration system. This feature allows user input during the au-\ntonomous process, ensuring human guidance for complex tasks.\nThrough Dynamic Movement Primitives (DMP), the system\ncaptures and stores trajectory data from manual teleoperation.\nThese recorded trajectories can then be saved in the motion\nlibrary and reused, promoting task-specific autonomy and bol-\nstering the system’s learning efficiency. The contributions of\nthis paper are as follows:\n1) A GPT4-based LLM system has been developed to fa-\ncilitate task planning for complex, long-horizon tasks.\nThe LLM selects motion functions from the motion\nlibrary according to the natural language commands. The\nselected functions are then integrated with environmental\ninformation, perceived through a YOLOv5-based percep-\ntion module, enabling the autonomous execution of a wide\nrange of tasks.\n2) The proposed LLM system adopts a hierarchical planning\nframework by utilizing the prompt function of GPT4. The\nLLM can dissect complex tasks into sub-tasks, further\nbreak down the sub-tasks into several motion functions,\nand execute each motion function sequentially.\n3) A teleoperation-based HRC framework for motion\ndemonstration is proposed. By integrating with DMP, the\nframework allows the LLM-based robot to learn from\nhuman demonstrations, thereby augmenting its motion\ncapabilities.\n4) The proposed HRC framework significantly enhances\nthe capabilities of the LLM-based system in executing\ncomplex tasks. These tasks, often requiring intricate tra-\njectory planning and reasoning over environments, can\nbe efficiently accomplished through the incorporation of\nhuman demonstrations.\nII. R ELATED WORKS\nA. Robotics Manipulation with Natural Language\nLLMs show great potential in the study of autonomous\nrobotics control. Liang et al. [10] proposed “Code as Policies”,\na method using the language model to generate robot policy\ncode from natural language commands and controlling the robot\nmotion by executing the generated code. Valuable enhancements\nand optimizations have been carried out based on the contribu-\ntions of Liang and other related pioneers. In [11], the authors\npresent the SoftGPT, a language model that combines graph\nrepresentations and LLM-based dynamics for efficient soft\nobject manipulation and learning from human demonstrations in\ndomestic scenes. Besides, a multisensory approach is proposed\nin [12] to enhance the robot’s understanding and execution of\nnatural language instructions for robot manipulation. We also\nutilize the LLM to assist in generating the necessary code for\nrobot operation. In this case, the LLM is solely responsible for\nselecting the motion functions.\nB. VR-based Teleoperation System\nIn 1940, the first robot teleoperation system was proposed\nby Goertz [13]. Nowadays, tons of robot teleoperation systems\nwith great manipulability and intelligence have been developed.\nNakanishi et al. [14] proposed an intuitive teleoperation system\nusing a VR device. In addition, Zhu et al. present a force\nfeedback system and a shared control framework to improve\nthe transparency and manipulability of teleoperation [15], [16].\nIn [17], the authors introduce a robotic avatar system featur-\ning immersive 3D visualization and advanced force-feedback\ntelemanipulation. In this paper, we use the teleoperation system\nas an instructive bridge between the human supervisor and the\nLLM-based robot manipulation system.\nC. Dynamic Movement Primitives for Trajectory Learning\nDynamic Movement Primitives (DMP) is a mathematical\nframework used in robotics and machine learning to model\nand reproduce complex joint behaviors. This approach was first\nproposed in [18], and updated by [19], the authors introduce\nDMP as a method for modeling complex behaviors in robotics,\ndemonstrating their flexibility, adaptability, and potential for\nintegration with statistical learning methods.\nAfter plenty of years of development, DMP has been con-\nsidered a classic approach in the field of robotic trajectory\nlearning. In 2013s, Paraschos et al. [20] proposed an improved\nmovement primitives approach named Probabilistic movement\nprimitives (ProMP), which allow for blending motions, adapt-\ning to task variables, and co-activating movement primitives\nwith advantages of probabilistic functions. Recently, in [21],\nthe authors propose ProDMP, a method combining DMP and\nProMP for smooth, statistics-informed trajectory generation,\nintegrated with a deep neural network for enhanced learning\nand functionality. In our architecture, we combine DMP with\nour teleoperation system to accomplish the one-shot task in-\nstructions.\nIII. M ETHODS\nOur proposed method combines an LLM with environmen-\ntal information within the Robot Operating System (ROS) to\nconstruct an LLM-based autonomous system. To enhance the\ncapabilities of the LLM-based system in executing complex\ntasks, we also adopt an HRC method to guide robot motion with\nhuman demonstration. This integration enables the translation\nof human commands into specific robotic motions. The system\nutilizes two primary libraries for motion execution: the basic\nlibrary and the DMP library.\nBasic library: This library includes pre-programmed mo-\ntion functions. The LLM selects motion functions based on\nthe task requirements and integrates these functions with\nenvironmental information to generate Pythonic code. This\ncode controls the robot to perform motions corresponding\nto the tasks.\nDMP library : This library, developed using DMP tech-\nnology, stores updated motion function sequences for\nsub-tasks (short-horizon tasks). These motion function\nsequences are updated through the user interface which\nleverages teleoperation.\n© 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including\nreprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or\nreuse of any copyrighted component of this work in other works.\nThis is the Accepted Version of the Article Published in IEEE Robotics and Automation Letters, vol. 9, no. 8, pp. 6904-6911, Aug. 2024.\nFinal Version Available at DOI:https://doi.org/10.1109/LRA.2024.3415931\nMultimedia Available at https://youtu.be/q9G6akvytqU\nTo accomplish long-horizon tasks, the LLM can extract\nupdated one-shot sub-tasks from the DMP library and combine\nthem with other zero-shot sub-tasks composed by the motion\nfunctions from the basic library.\nA. Large Language Model for Autonomous Robot Manipulation\nIn our methodology, we employ the GPT-4 Turbo network\n(the temperature is set to 0) to formulate the robot control\nhub. This hub is dedicated to converting user input into motion\nfunctions that can be integrated into Pythonic code. To ensure\nthe model’s output meets specific requirements, a special cus-\ntomization process is undertaken by providing the prompt. To\nclarify the LLM’s duty, we provide the prompt “Your mission\nis to control the robot to complete the task assigned by the\nuser, utilizing the motion functions from the basic library”.\nAlso, the prompts introducing the components of the basic\nlibrary are provided as follows: “(i) 'move to position()' controls\nthe robot end effector to reach the desired pose by updating\njoint angles through the calculation of inverse kinematics. (ii)\n'base cycle move()' controls the base of the robot to make\na circle movement according to the given angle and radius.\nAdditionally, the function includes the motion to fully open the\ndoor. (iii) ' close move()' is used to close doors with vertical\ndoor axes. The scope of motion is decided by the size of the\nobject. (iv) ' gripper control()' controls the opening and closing\nof the gripper and can be used to control the degree of opening\nand closing depending on the size of the object.” Through this\ntypical prompt, the LLM can generalize and perform similar\ntasks without relying on additional related prompts, illustrating\nthat while prompt design is crucial, it does not constitute the\npredominant workload in operations.\nAdditionally, to ensure the LLM executes motion func-\ntions properly, providing detailed prompts is important.\nFor example, customization inputs should include instruc-\ntive examples, such as “When I ask you to put the ap-\nple on the plate, you can arrange motion functions like\nthis 'move to position(init)', 'move to position(apple)', 'grip-\nper control(close low)', 'move to position(init)', ' move to -\nposition(plate)', ' gripper control(open)', ' move to position(-\ninit)'.”\nMoreover, to address ambiguity or vagueness in commands,\nthe system can be enhanced with additional prompts for cus-\ntomization. To give an example of such prompts, “If multiple\nobjects share the same name as the target, you could cue the\nuser by asking, ‘There are multiple objects share the same name,\nwhich one do you prefer?’”\nBased on these customizations, the LLM is enabled to select\nand utilize motion functions from the basic library in response\nto user requirements, achieving 99.4% executability (Table I).\nBesides, a hierarchical strategy of task planning is applied.\nThe hierarchical task planning strategy in our system enables\nhandling complex, long-horizon tasks in the real world. For\nexample, for a task like heating food, the LLM devises a\ndetailed plan broken down into specific sub-tasks with motion\nfunctions, but not a long sequence of motion functions is\nprone to errors. To classify different tasks, the prompt “As\nyou receive a task, first consider if the task can be divided\nFig. 2. Illustration of how the LLM understands, classifies, decomposes, and\nexecutes different tasks.\ninto sub-tasks, if so divide them into sub-tasks with motion\nfunctions, if not, directly output the motion functions” is given\nto the LLM. As illustrated in Fig. 2, the LLM translates short-\nhorizon tasks directly into motion functions MoF (t), whereas\nlong-horizon tasks are divided into sub-tasks, each comprising\nmultiple motion functions PN\ni MoF (ti).\nIn practice, user commands for long-horizon tasks, are illus-\ntrated in Fig.3 and processed through the following steps:\n1) The user issues a command (e.g., Clean the table).\n2) The LLM interprets this command and classifies it to a\nlong-horizon task. Based on the classification, this task\ncan be divided into several sub-tasks, such as the task\nPut the left cup in storage.\n3) For the first sub-task, the LLM parses and maps it to the\nmotion functions contained within the basic library.\n4) The environmental perception system analyzes sensory\ndata and publishes the spatial coordinates of relevant\nentities within the operational environment, such as bot-\ntle1 position and bowl position in Fig. 3.\n5) The processed sensory data and the motion function\nsequence are integrated into an executable Pythonic code.\nBy executing this Pythonic code, the robot is controlled\nto fulfill the first sub-task. The remaining sub-tasks will\nbe executed following the aforementioned process.\nB. Environmental Perception\nIn the environmental perception part, we extract the target\nobjects’ names, quantities, and coordinates in the image by the\nrobot’s Xtion depth camera based on YOLOv5. The names and\nquantities of these objects are used to dynamically update the\nprompts for the LLM, which is used for LLM’s motion planning\nin response to user commands. The coordinates of the target to\nthe depth camera can be calculated as follows:\nPc\nobj = d · K−1 · i (1)\nwhere i =\n\u0002u v 1\u0003T\nrepresents the homogeneous coordi-\nnates in the image, corresponding to the target object’s image\ncoordinate point (u, v). The term d denotes the depth value at\nthe point (u, v) collected from the depth map, and K refers to\nthe intrinsic matrix of the depth camera. The position of the\nobject relative to the depth camera is denoted by Pc\nobj.\nThe world coordinate of the target is given by the following\ntransformation:\nPw\nobj = Tw\nc · Pc\nobj (2)\nwhere Pw\nobj represents the position of the object in world\ncoordinates, and Pc\nobj denotes its position in camera coordinates.\nThe transformation matrix Tw\nc encapsulates the rotation Rw\nc and\ntranslation tw\nc matrix from the camera coordinate system to the\nworld coordinate system.\n© 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including\nreprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or\nreuse of any copyrighted component of this work in other works.\nThis is the Accepted Version of the Article Published in IEEE Robotics and Automation Letters, vol. 9, no. 8, pp. 6904-6911, Aug. 2024.\nFinal Version Available at DOI:https://doi.org/10.1109/LRA.2024.3415931\nMultimedia Available at https://youtu.be/q9G6akvytqU\ndefinecallback()move_to_position(init)move_to_position(cup1)gripper_control(close)move_to_position(init)    \nBasic Librarymove_to_position()gripper_control()base_cycle_move()Sub-task1\nEnvironment Perceptioncup1_position, cup2 _position,bottle _position, bowl _position\nLLMCatchthe cup on the left side\nclose_move()\nEncodingMotion functionsselection\nEnvironment informationintegration\nExecutable Code\nPublish information:\nUser Input\nClean the table\nSub-tasksMotion functions selections and EncodingsExecutions\n...\n...\n...\nFig. 3. An overview of LLM-based autonomy for a real-world long-horizon task. This process encompasses sub-task identification, motion function selection\nfrom the basic library, environment perception integration, and executable code generation.\nThe calculated target positions are continuously published\nand labeled with their names. For the ongoing task, the LLM\nintegrates these calculated positions with motion functions,\nguiding the robot’s movements as illustrated in Fig. 3.\nOn the other hand, to efficiently handle complex scenarios\ninvolving multiple same-name targets and pre-tasks (obstacle re-\nmoval), two specialized algorithms are proposed. (i) Algorithm\n1 streamlines the object selection in cluttered environments.\nIt processes the same-named objects detected by YOLOv5,\nsorting and labeling them from spatial left to right within each\nobject category. By integrating the detected objects and their\nquantities into the prompt, the LLM can interpret commands\nfor specific objects. For example, the task “Pick the middle\ncup” is achieved by picking the appropriately labeled object\n‘cup2’, with the position information provided by Algorithm 1\n(e.g., ‘cup2’ in a sorted sequence of three cups). (ii) Algorithm\n2 addresses obstacle removal before task execution. When the\nLLM executes a task, it extracts the targets’ names involved\nand forwards their names to Algorithm 2. Algorithm 2 then\nevaluates the workspace of each target, identifying any obstacles\nand returning a list of the obstacles to the LLM. Upon receiving\nthis list, the original task is temporarily paused and the LLM\ninitiates a new task to remove the identified obstacles. Only after\nthese obstacles are successfully cleared does the LLM resume\nthe execution of the planned motion sequence for the primary\nobjective.\nC. Human-Robot Collaboration\nAlthough LLMs can assist in basic tasks when integrated with\nenvironmental perception, their limitations become apparent in\ncomplex, real-world scenarios. For example, while the LLM\nsuccessfully operates a microwave with a normal design, it\ncould struggle with opening an oven door featuring a horizontal\naxis design. This difficulty stems from the LLM’s limited\ncapabilities in code generation, particularly in handling a con-\nstrained motion library and understanding complex trajectories.\nFurthermore, even though GPT-4 can interpret environments\nthrough image input, it still relies on human guidance for\naccurate decision-making in diverse and unfamiliar situations.\nAlgorithm 1 Sort and Label Objects Detected\n1: Input: List of objects O detected by YOLOv5\n2: Output: List of labeled objects L\n3: Initialize L = []\n4: for each unique object name n in O do\n5: Filter objects in O with name n, store in On\n6: Sort On from left to right based on their positions, store in\nOn sorted\n7: for each object o in On sorted do\n8: Label o with its index in On sorted\n9: Add labeled o to L\n10: end for\n11: end for\n12: Return L\nAlgorithm 2 Identify Obstacles for Task Execution\n1: Input: Target coordinates TCoord , robot base coordinates\nRCoord, workspace objects WObjs\n2: Output: Coordinates sequence of obstacles ObsCoord\n3: Initialize ObsCoord as an empty list\n4: Define an equilateral triangular detection space with one vertex at\nRCoord and the base midpoint at TCoord\n5: if WObjs is empty then\n6: Return ‘No objects’\n7: end if\n8: for each object Obj in WObjs do\n9: Check if Obj is within the triangular detection space\n10: if Obj is inside then\n11: Add Obj’s coordinates to ObsCoord\n12: end if\n13: end for\n14: Return ObsCoord\nTo address these challenges, we advocate for a synergistic\nframework of human-robot collaboration that is grounded in\nLLM-based autonomy. In this proposed framework, users are\ncapable of proactively interceding and seamlessly instructing\nthe robot by leveraging teleoperation. The instructed motions\ncan be used to replace the original motion functions in the basic\nlibrary and the updated motion functions sequences are saved\nin the DMP library through the user interface shown in Fig. 5.\nThe teleoperation system [14] includes a headset and a\n© 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including\nreprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or\nreuse of any copyrighted component of this work in other works.\nThis is the Accepted Version of the Article Published in IEEE Robotics and Automation Letters, vol. 9, no. 8, pp. 6904-6911, Aug. 2024.\nFinal Version Available at DOI:https://doi.org/10.1109/LRA.2024.3415931\nMultimedia Available at https://youtu.be/q9G6akvytqU\nDMPLibrary  “open_oven_handle” \ndmp_pub(open_oven_handle) dmp_pub(open_oven_handle_ex)\nmove_to_position(oven_handle)gripper_control(close)base_cycle_move()\n123\nUIReplace13\nDelete2BasicLibraryInput is processed by LLM and named the task as “open_oven_handle”\nFirst-timeInput\nOpen the oven\nHuman-robot collaboration\nOne-shot task reusing\n+\nInstruction with manual teleoperation\n1\n 2\n 3\n 4 Same taskReuse\nOpen the oven\nmove_to_position(oven_handle)gripper_control(close)base_cycle_move()® ®\nTheLLM processedand named the task as “open_oven_handle”Execute the updated motion functions under the task name “open_oven_handle” in the DMP library\nExecution\nOriginal motion functions sequence\nUpdated motion functions sequence\nFig. 4. An overview of the LLM-based autonomy with Human-Robot Collaboration in sub-task (short-horizon task). The LLM processes user input to select\nmotion functions from the basic Library. These selected motions are subsequently modified through the user interface with teleoperation. The updated motion\nfunctions are stored in the DMP Library with a specific name such as “open oven handle” (The LLM captures the action “open” and the target “oven handle”,\nthen integrates them as “open oven handle”) for future application (same task re-input or reusing in the long-horizon task), resulting in successful one-shot task\nexecution.\ncontroller for the operator. The headset provides stereo-vision\nfeedback, replicating a real-world view as seen through the\nrobot’s eyes, and mirrors the user’s head movements in the\nrobot’s head movements. The controller, held in the user’s\nhand, maps the user’s hand movements to the robot’s hand\nmovements. These setups ensure that operators can intuitively\nguide the robot, as illustrated in Fig. 4.\nThese custom motions, instructed by the operator through\nthe teleoperation system, are then captured and integrated into\nthe robot’s capabilities using DMP technology, as updated by\nIjspeert [19]. DMP technology is crucial for capturing and\nstoring these tailored motion trajectories in the DMP library,\nthereby enriching the robot’s repertoire for diverse tasks. The\ncore principle of DMP is encapsulated in the following equa-\ntions:\n¨y = α(β(g − y) − ˙y) +f(x)(g − y) (3)\nwhere y represents the system’s state, g is the goal, α and β are\nthe constant gain terms. This equation describes the behavior\nof the trajectory. For the forcing term f(x):\nf(x) =\nPN\ni=1 ψiwix\nPN\ni=1 ψi\n(4)\nwhere ψi are Gaussian basis functions, wi the corresponding\nweights, and x is the phase variable from the canonical system\nthat decays non-linearly. The weights wi are derived from the\ntraining trajectory using weighted linear regression, and N\nrepresents the number of Gaussian basis functions, fixed at 15 in\nour experiments. DMP achieves trajectory storage and replay by\nlearning the weights wi from a demonstrated trajectory achieved\nby teleoperation. Once learned, these weights define the shape\nof the forcing function f(x), which, when combined with the\nspring-damper system in Equation (3), produces the desired\nmovement trajectory. By varying the goal g, start position y,\nand phase x, the DMP can generate different trajectories while\nstill maintaining the learned movement pattern.\nStop the process\nCommunication\n1\n2\n3\n4Teleoperation\nFig. 5. An overview of the usage of the user interface. A user interface is\nused for supervising and intervening in a robot’s task sequence, including the\ncommunication between the user and the system and the steps for modifying\nrobotic motions through teleoperation.\nBy leveraging teleoperation for trajectory recording, the sys-\ntem enables an LLM-based robot to improve previously prob-\nlematic tasks by accessing human-defined motion trajectories\nstored in the DMP library.\nThe HRC process in the real world, which is conducted\nthrough the user interface shown in Fig. 5, is illustrated in Fig.\n4, following the steps as follows:\n1) The user begins the process by issuing a command, such\nas “Open the oven”.\n2) The LLM could process the command, map it into a set\nof basic motions that exist in the robot’s basic library,\nand integrate it with environment information which is\nthe same process in Fig. 3.\n3) However, in Fig. 4, the original motion sequence, suitable\nfor a door with a vertical axis, will fail to open the\nhorizontally hinged oven door. The user notices it, stops\nthe process, and updates motion functions via the user\ninterface and teleoperation shown in Fig. 5.\n4) The updated new motion functions sequence is then\n© 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including\nreprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or\nreuse of any copyrighted component of this work in other works.\nThis is the Accepted Version of the Article Published in IEEE Robotics and Automation Letters, vol. 9, no. 8, pp. 6904-6911, Aug. 2024.\nFinal Version Available at DOI:https://doi.org/10.1109/LRA.2024.3415931\nMultimedia Available at https://youtu.be/q9G6akvytqU\n(a) HSR joint configuration\nCabinet (b) Environment setup\n(c) Oven\n (d) Microwave\n(e) Apple\n (f) Bowl\n (g) Cups\nFig. 6. HSR joint configuration, experiment environment setup, and objects\nutilized in the experiments\nsaved in the DMP library and the new motion trajectory\ninstructed by manual teleoperation can be called by the\nfunction 'dmp pub'. These data are preserved in the DMP\nlibrary named “open oven handle” which is named by the\nLLM.\n5) When the task “Open the oven” is retrieved, the LLM\nnames the task as “open oven handle” again, extracts\nthe updated motion functions sequence from the task’s\nsegment in the DMP library, and enables the robot to\nperform the task as depicted in Fig. 4.\nIV. E XPERIMENTS AND DISCUSSION\nThe HRC framework based on the LLM has undergone real-\nworld testing on the Human Support Robot (HSR) from Toyota.\nThe HSR has a total of 10-Degree-of-Freedom (DoF), including\na 3-DoF mobile base, a 5-DoF arm (4-DoF for rotating joints\nand 1-DoF for a torso lift joint) with a gripper, and a 2-\nDoF head. Oculus’s VR device is used for teleoperation. The\nexperiment objects are shown in Fig. 6.\nThe experiments were set in a well-lit kitchen environment as\nshown in Fig. 6 (b), where the robot was tasked with performing\na variety of domestic tasks initiated through natural language\ncommands provided by users. The experiments aim to evaluate\nthe performance of the system for routine zero-shot tasks and\nmore intricate and specialized one-shot tasks.\nA. Zero-shot Basic Tasks\nIn this part, we conducted two basic tasks Put&Stack, and\nOpen, and two long-horizon tasks Warm up and Clean.\n1) Put&Stack: The Put motions position an item relative\nto a target and are of two types: above or inside the target\nlocation. The customized LLM adds a suffix to specify the target\ncontext—' above' for placement 10mm higher than the target,\nand ' inside' for insertion, such as 'object above'. The motion\nfunctions for Put&Stack are as follows:\n# Move to the first object’s position\nmove to position(object1)\n# Close gripper\ngripper control(close)\n# Move to the second object’s above position\nmove to position(object2 above/inside)\n# Open gripper\ngripper control(open)\n2) Open: The effect of this motion Open is opening the door,\nbut only for the door with the axis perpendicular to the ground.\n# Move to the door handle’s position\nmove to position(microwave handle)\n# Close gripper\ngripper control(close)\n# Open the door\nbase cycle move(radius door2axis)\n# Open gripper\ngripper control(open)\n3) Close: This motion is used to close the door, also only\nfor the door with the axis perpendicular to the ground.\n# Close the door\nclose move(object)\n4) Power on: This motion includes the function to move to\nthe target and rotate the waist to power the device.\n# Move to the object\nmove to position(microwave knob)\n# Rotate waist to power on the object\nrotate waist(degree)\n5) Warm up: This task is a long-horizon task to warm up\nthe apple which contains several sub-tasks.\n# Generate sub-tasks\nsub-tasks = ['open the microwave', 'put the apple\ninto the microwave', 'close the microwave', 'power\non the microwave']\n# Execute the functions of the sub-task in order\nfor sub-task in sub-tasks:\nexecute(sub-task)\n6) Clean the table: This task is a long-horizon task. The\nenvironmental information including the objects on the table is\ngenerated to integrate into executable code. In the below case,\nwe assume that there are one cup and two bottles.\n# Generate sub-tasks\nsub-tasks = ['put the cup in the storage', 'put the\nfirst bottle in the storage', 'put the second bottle\nin the storage' ]\n# Execute the functions of the sub-task in order\nfor sub-task in sub-tasks:\nexecute(sub-task)\n© 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including\nreprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or\nreuse of any copyrighted component of this work in other works.\nThis is the Accepted Version of the Article Published in IEEE Robotics and Automation Letters, vol. 9, no. 8, pp. 6904-6911, Aug. 2024.\nFinal Version Available at DOI:https://doi.org/10.1109/LRA.2024.3415931\nMultimedia Available at https://youtu.be/q9G6akvytqU\nB. One-shot DMP-based Tasks\nIn this section, we conducted two one-shot DMP-based tasks\nand a long-horizon task “Roast apple (HRC)”.\n1) Open the oven: The motion function 'base cycle mov-\ne()' in our basic library is not effective for the door with the hor-\nizontal axis. To address this, we teach it through teleoperation\nand save it in the DMP library.\n# Move to the oven’s handle with updated pose\nmove to position(oven handle)\n↓ Replace\ndmp publish(open oven handle)\n# Delete the gripper control(close)\ngripper control(close)\n# Open the oven with updated trajectory\nbase cycle move(radius door2axis)\n↓ Replace\ndmp publish(open oven handle ex)\n2) Close the oven: Due to the special construction of the\noven in contrast to the microwave oven, closing the oven\ndoor also requires instruction to replace the motion function\n'close move(object)'.\n# Close the oven with new trajectory\nclose move(object)\n↓ Replace\ndmp publish(close oven)\n3) Open the cabinet: The cabinet has a press-pull structure\n(To open it, the cabinet door must first be pushed inward.),\nwhich makes it impossible for the LLM to control the robot\nto open it in the zero-shot. (The position of the cabinet is\ndetermined by its relative location to the microwave.)\n#The new motion to press the cabinet\nmove to position(cabinet)\n↓ Replace\ndmp publish(open cabinet)\n# Delete the gripper control(close)\ngripper control(close)\n# The new motion to pull the cabinet\nbase cycle move(radius door2axis)\n↓ Replace\ndmp publish(open cabinet ex)\n4) Roast the apple: This task aims to determine whether\nthe sub-tasks instructed by the DMP can be repurposed for\nrelatively long-horizon tasks.\n# Generate sub-tasks (one-shot DMP-based tasks are\nbold letter)\nsub-tasks = ['open the oven', 'put the apple into\noven', 'close the oven', 'power on the oven']\n# Execute the functions of the sub-task in order\nfor sub-task in sub-tasks:\nexecute(sub-task)\nC. Discussion\nAll of the zero-shot basic tasks and the one-shot DMP-based\ntasks are indicated in Table 1 to evaluate their performance.\nA total of seven distinct tasks were assessed, with each task\nundergoing 23 trials, totaling 161 trials.\nTABLE I\nEXECUTABILITY , FEASIBILITY , AND SUCCESS RATES OF LLM- BASED\nAUTONOMY AND HUMAN -ROBOT COLLABORATION\nTasks Num of trials Executability Feasibility Success rate\nPut&Stack 23 100.0% 100.0% 91.3%\nOpen microwave 23 100.0% 100.0% 82.6%\nOpen oven (HRC) 23 100.0% 100.0% 91.3%\nOpen cabinet (HRC) 23 100.0% 100.0% 87.0%\nClean table 23 100.0% 95.7% 87.0%\nWarm up apple 23 100.0% 100.0% 60.9%\nRoast apple (HRC) 23 95.7% 87.0% 56.5%\nTotal 161 99.4% 97.5% 79.5%\nTABLE II\nCOMPARISON OF EXECUTABILITY , FEASIBILITY , AND SUCCESS RATES FOR\nSUB-TASKS WITH AND WITHOUT HRC\nTasks Num of trials Executability Feasibility Success rate\nOpen oven 1 100.0% 0.0% 0.0%\nOpen oven (HRC) 23 100.0% 100.0% 91.3%\nOpen cabinet 1 100.0% 0.0% 0.0%\nOpen cabinet (HRC) 23 100.0% 100.0% 87.0%\nThe “Executability” score reflects whether the integrated\nPythonic code follows the predefined format and is executable.\nThis score is quantitatively measured as the ratio of tasks for\nwhich the code is successfully executed to the total number of\ntasks assessed.\nThis score almost achieved 100%, the only exception was\nnoted in the task “Roast apple (HRC)”, where executability\nslightly decreased to 95.7%. This was attributed to the LLM’s\noccasional generation of sub-tasks without assigning corre-\nsponding motion functions, leading to non-encodable responses.\nHowever, this issue was not observed in subsequent experi-\nments, suggesting its rarity and isolating it as a singular incident\nwithin the scope of our testing (one failure in 161 experiments).\nThis indicates that the instance of reduced executability for\n“Roast Apple (HRC)” was an anomaly rather than an indication\nof a systemic flaw.\nThe “Feasibility” score addresses whether the motion func-\ntions arranged by the LLM could accomplish the tasks under\nthe assumption that there are no motion errors in robot control\nand no spatial errors in object recognition. To quantify this,\nthis score is calculated as the percentage of tasks that meet the\nestablished standard out of the total number of tasks attempted.\nThis score highlights the indispensable role of HRC, as\nevidenced in Table II. Specifically, tasks like “Open oven” and\n“Open cabinet,” initially deemed infeasible with a Feasibility\nof 0% (impossible to be finished with motion functions from\nthe basic library), witnessed remarkable improvements upon\nintegrating HRC, underscoring HRC’s enhancement on task\nexecution.\nAdditionally, certain tasks such as “Clean table” and “Roast\napple (HRC)” scored 95.7% and 87.0%, respectively. The\n“Clean table” task’s diminished score can be attributed to the\nYOLO’s occasional oversight of objects during environmental\nscanning, which results in passing the incomplete environment\n© 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including\nreprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or\nreuse of any copyrighted component of this work in other works.\nThis is the Accepted Version of the Article Published in IEEE Robotics and Automation Letters, vol. 9, no. 8, pp. 6904-6911, Aug. 2024.\nFinal Version Available at DOI:https://doi.org/10.1109/LRA.2024.3415931\nMultimedia Available at https://youtu.be/q9G6akvytqU\ninformation to the LLM. For “Roast apple (HRC)”, the is-\nsue arose from incorrect DMP function calls, exemplified by\nthe LLM mistakenly activating 'dmp pub(close oven handle)'\ninstead of 'dmp pub(close oven)'. Such inaccuracies, driven by\ntoken variability, can dilute the precision of motions reproduced\nfrom the DMP library, particularly in complex tasks prone to\nsynonymous token substitution.\nThe “Success rate” score, which indicates the percentage of\ntasks completed in the real world, varied depending on the com-\nplexity of the task. To investigate the cause of the reduction in\ntasks “Warm up apple” and “Roast apple (HRC)”, experiments\nto test the performance of YOLO were conducted. In these\nexperiments, the center of YOLO’s bounding box for an object\nwas utilized to represent its position. Additionally, AR markers\nwere placed at the absolute center of the object’s surface facing\nthe robot as a ground truth position. Discrepancies between\nthe object positions detected by YOLO and the ground truth\npositions indicated by the AR markers were measured over 5\nseconds.\nFor specific tasks, such as placing a 0.063m high apple into a\n0.086m high oven with a margin of 0.0115m and a discrepancy\nrange of 0.0095m to 0.0122m, and a median discrepancy of\n0.01m to 0.012m, the margin falls within the discrepancy range.\nThis issue also occurs in other tasks like rotating the knob,\nopening the microwave door fully, and stacking the cups, which\ncan affect the success rate of task execution. The errors of\nenvironment perception stem from YOLOv5’s bounding box\ninaccuracies, leading to slightly variable coordinates for target\nobjects and occasionally exceeding the necessary margins for\nprecise manipulation. Furthermore, even though the success\nrates of all sub-tasks in our experiments are above 80%, the\nsuccess rates of the long-horizon tasks will inevitably decrease\ndue to error accumulation.\nV. C ONCLUSION\nThis research advances robot autonomy through an interface\nlinking the LLM with robotic systems, enabling task execution\nthrough natural language. Moreover, a novel LLM-based hier-\narchical planning framework efficiently manages long-horizon\ntasks, showcasing advanced task planning. Furthermore, the in-\ncorporation of teleoperation and DMP into the HRC framework\nenables the enhancement of LLM-based robotic manipulation\nthrough human demonstrations.\nExperimental results show the effectiveness of the proposed\nmethod: an average success rate of 79.5%, with 99.4% ex-\necutability and 97.5% feasibility across various tasks. These\nresults underscore the system’s robustness in translating lan-\nguage commands into robot motions and integrating operator\ninstructions to accomplish unachievable tasks, making signif-\nicant strides toward improving the performance of the LLM-\nbased robot with the complexities of real-world task demands.\nHowever, to address a limitation of current LLM-based robots\nthat rely heavily on visual inputs, future research will focus\non integrating LIDAR-derived point clouds and tactile sensing\ntechnologies to enhance the proposed LLM-based robot perfor-\nmance in real-world environments.\nREFERENCES\n[1] A. Vaswani et al. , “Attention is all you need,” arXiv preprint\narXiv:1706.03762, 2023.\n[2] H. Touvron et al., “Llama: Open and efficient foundation language\nmodels,” arXiv preprint arXiv:2302.13971, 2023.\n[3] M. Ahn et al., “Do as i can and not as i say: Grounding language in\nrobotic affordances,” in arXiv preprint arXiv:2204.01691, 2022.\n[4] J. Wu et al., “Tidybot: Personalized robot assistance with large language\nmodels,” Autonomous Robots, 2023.\n[5] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:\nUnified, real-time object detection,” arXiv preprint arXiv:1506.02640,\n2016.\n[6] A. Radford et al., “Learning transferable visual models from natural\nlanguage supervision,” arXiv preprint arXiv:2103.00020, 2021.\n[7] A. Kirillov et al., “Segment anything,” arXiv preprint arXiv:2304.02643,\n2023.\n[8] I. Singh et al., “Progprompt: Generating situated robot task plans using\nlarge language models,” in 2023 IEEE International Conference on\nRobotics and Automation (ICRA). IEEE, 2023, pp. 11 523–11 530.\n[9] Z. Zhao, W. S. Lee, and D. Hsu, “Differentiable parsing and visual\ngrounding of natural language instructions for object placement,” in 2023\nIEEE International Conference on Robotics and Automation (ICRA) .\nIEEE, 2023, pp. 11 546–11 553.\n[10] J. Liang et al., “Code as policies: Language model programs for em-\nbodied control,” in 2023 IEEE International Conference on Robotics and\nAutomation (ICRA). IEEE, 2023, pp. 9493–9500.\n[11] J. Liu, Z. Li, W. Lin, S. Calinon, K. C. Tan, and F. Chen, “Softgpt:\nLearn goal-oriented soft object manipulation skills by generative pre-\ntrained heterogeneous graph transformer,” in 2023 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS), 2023, pp. 4920–\n4925.\n[12] W. Wang, X. Li, Y . Dong, J. Xie, D. Guo, and H. Liu, “Natural language\ninstruction understanding for robotic manipulation: a multisensory per-\nception approach,” in 2023 IEEE International Conference on Robotics\nand Automation (ICRA), 2023, pp. 9800–9806.\n[13] R. C. Goertz, “Manipulator systems developed at anl,” in Proceedings,\nThe 12th Conference on Remote Systems Technology, 1964, pp. 117–136.\n[14] J. Nakanishi, S. Itadera, T. Aoyama, and Y . Hasegawa, “Towards the\ndevelopment of an intuitive teleoperation system for human support robot\nusing a vr device,” Advanced Robotics, vol. 34, no. 19, pp. 1239–1253,\n2020.\n[15] Y . Zhu, T. Aoyama, and Y . Hasegawa, “Enhancing the transparency\nby onomatopoeia for passivity-based time-delayed teleoperation,” IEEE\nRobotics and Automation Letters, vol. 5, no. 2, pp. 2981–2986, 2020.\n[16] Y . Zhu, B. Jiang, Q. Chen, T. Aoyama, and Y . Hasegawa, “A shared\ncontrol framework for enhanced grasping performance in teleoperation,”\nIEEE Access, vol. 11, pp. 69 204–69 215, 2023.\n[17] C. Lenz et al., “Nimbro wins ana avatar xprize immersive telepresence\ncompetition: Human-centric evaluation and lessons learned,” International\nJournal of Social Robotics, pp. 1–25, 2023.\n[18] S. Schaal, “Dynamic movement primitives-a framework for motor control\nin humans and humanoid robotics,” in Adaptive motion of animals and\nmachines. Springer, 2006, pp. 261–280.\n[19] A. J. Ijspeert, J. Nakanishi, H. Hoffmann, P. Pastor, and S. Schaal,\n“Dynamical movement primitives: learning attractor models for motor\nbehaviors,” Neural computation, vol. 25, no. 2, pp. 328–373, 2013.\n[20] A. Paraschos, C. Daniel, J. R. Peters, and G. Neumann, “Probabilistic\nmovement primitives,” Advances in neural information processing sys-\ntems, vol. 26, 2013.\n[21] G. Li, Z. Jin, M. V olpp, F. Otto, R. Lioutikov, and G. Neumann, “Prodmp:\nA unified perspective on dynamic and probabilistic movement primitives,”\nIEEE Robotics and Automation Letters, vol. 8, no. 4, pp. 2325–2332,\n2023.\n© 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including\nreprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or\nreuse of any copyrighted component of this work in other works."
}