{
    "title": "How Large Language Models are Transforming Machine-Paraphrased Plagiarism",
    "url": "https://openalex.org/W4304194491",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4224451961",
            "name": "Wahle, Jan Philip",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4223315846",
            "name": "Ruas, Terry",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4304229260",
            "name": "Kirstein, Frederic",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3162852365",
            "name": "Gipp, Bela",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2794557536",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W2963551569",
        "https://openalex.org/W3044021949",
        "https://openalex.org/W3176456866",
        "https://openalex.org/W2016172157",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W2001771035",
        "https://openalex.org/W4319811928",
        "https://openalex.org/W3015453090",
        "https://openalex.org/W2798935874",
        "https://openalex.org/W3098267758",
        "https://openalex.org/W3125358881",
        "https://openalex.org/W2147528976",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3174519801",
        "https://openalex.org/W2963991775",
        "https://openalex.org/W2531908596",
        "https://openalex.org/W4299567010",
        "https://openalex.org/W4213122582",
        "https://openalex.org/W2119298903",
        "https://openalex.org/W4226053440",
        "https://openalex.org/W3102273025",
        "https://openalex.org/W3100501376",
        "https://openalex.org/W4234918518",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2086668169",
        "https://openalex.org/W3034287667",
        "https://openalex.org/W2970559004",
        "https://openalex.org/W2133012565",
        "https://openalex.org/W2251882135",
        "https://openalex.org/W3211481821",
        "https://openalex.org/W4206732210",
        "https://openalex.org/W2962965405",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W2996573939",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4293350112",
        "https://openalex.org/W1614298861"
    ],
    "abstract": "The recent success of large language models for text generation poses a severe threat to academic integrity, as plagiarists can generate realistic paraphrases indistinguishable from original work. However, the role of large autoregressive transformers in generating machine-paraphrased plagiarism and their detection is still developing in the literature. This work explores T5 and GPT-3 for machine-paraphrase generation on scientific articles from arXiv, student theses, and Wikipedia. We evaluate the detection performance of six automated solutions and one commercial plagiarism detection software and perform a human study with 105 participants regarding their detection performance and the quality of generated examples. Our results suggest that large models can rewrite text humans have difficulty identifying as machine-paraphrased (53% mean acc.). Human experts rate the quality of paraphrases generated by GPT-3 as high as original texts (clarity 4.0/5, fluency 4.2/5, coherence 3.8/5). The best-performing detection model (GPT-3) achieves a 66% F1-score in detecting paraphrases.",
    "full_text": "Related papers at https://jpwahle.com/pub/ and https://gipp.com/pub/\nJ. P. Wahle, T. Ruas, F. Kirstein, and B. Gipp, “How Large Language Models are Transforming Machine-\nParaphrased Plagiarism”, in Proceedings of the 2022 Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP).\nClick to download: BibTeX\nHow Large Language Models are Transforming\nMachine-Paraphrased Plagiarism\nJan Philip Wahle\n *, Terry Ruas*, Frederic Kirstein♠*, Bela Gipp*\n*Georg-August-Universität Göttingen, Germany\n♠Mercedes-Benz Group AG, Germany\nwahle@gipplab.org\nAbstract\nThe recent success of large language models\nfor text generation poses a severe threat to aca-\ndemic integrity, as plagiarists can generate re-\nalistic paraphrases indistinguishable from orig-\ninal work. However, the role of large autore-\ngressive transformers in generating machine-\nparaphrased plagiarism and their detection is\nstill developing in the literature. This work ex-\nplores T5 and GPT-3 for machine-paraphrase\ngeneration on scientiﬁc articles from arXiv,\nstudent theses, and Wikipedia. We evaluate\nthe detection performance of six automated so-\nlutions and one commercial plagiarism detec-\ntion software and perform a human study with\n105 participants regarding their detection per-\nformance and the quality of generated exam-\nples. Our results suggest that large models can\nrewrite text humans have difﬁculty identify-\ning as machine-paraphrased (53% mean acc.).\nHuman experts rate the quality of paraphrases\ngenerated by GPT-3 as high as original texts\n(clarity 4.0/5, ﬂuency 4.2/5, coherence 3.8/5).\nThe best-performing detection model (GPT-3)\nachieves a 66% F1-score in detecting para-\nphrases. We make our code, data, and ﬁndings\npublicly available for research purposes.1\n1 Introduction\nParaphrases are texts that convey the same mean-\ning while using different words or sentence struc-\ntures (Bhagat and Hovy, 2013). Paraphrasing plays\nan important role in related language understand-\ning problems (e.g., question answering (McCann\net al., 2018), summarization (Rush et al., 2015)),\nbut it can also be misused for academic plagiarism.\nAcademic plagiarism is serious misconduct as its\nperpetrators can unjustly advance their careers, ob-\ntain research funding that could be better spent,\nand make science less reliable if their misbehavior\nremains undetected (Meuschke, 2021).\n1https://github.com/jpwahle/\nemnlp22-transforming\nOriginal Text\n...\nOn April 29, 2017 , Bill Gates partnered with\nSwiss tennis legend Roger Federer in playing the\n“Match for Africa” 4, a noncompetitive tennis match\nat a sold-out Key Arena in Seattle.\nThe event was in support of Roger Federer\nFoundation’s charity efforts in Africa.\n...\nParaphrased using GPT-3\n...\nBill Gates teamed up with Swiss tennis player\nRoger Federer to play in the “Match for Africa 4” on\nApril 29, 2017 .\nThe noncompetitive tennis match at a sold-out\nKey Arena in Seattle was in support of\nRoger Federer Foundation’s charity efforts in Africa.\n...\nTable 1: Example excerpt from a Wikipedia article and\nits paraphrased versions using GPT-3. Important key-\nwords are highlighted in boldfont and color. Autore-\ngressive paraphrasing with GPT-3 keeps the same mes-\nsage while generating text with the original structure.\nThe original example used is 3747-ORIG-44.txt.\nParaphrasing tools can be used to generate convinc-\ning plagiarized texts with minimum effort. Most\nof these tools (e.g., SpinBot2, SpinnerChief3) use\nrelatively rudimentary heuristics, such as word re-\nplacements with synonyms, and they already de-\nceive plagiarism detection software (Wahle et al.,\n2022a). However, these tools scratch the surface of\nthe possibilities compared to what large neural lan-\nguage models can achieve in producing convincing\nhigh-quality paraphrases (Zhou and Bhat, 2021).\nNotably, large autoregressive language models with\nbillions of parameters, such as GPT-3 (Brown et al.,\n2020), make paraphrase plagiarism effortless yet\nexceedingly difﬁcult to spot.\nSo far, large language models have found little ap-\n2https://spinbot.com/\n3https://spinnerchief.com/\n1\narXiv:2210.03568v3  [cs.CL]  10 Nov 2022\nplication in plagiarism detection. As language mod-\nels are already easily accessible for applications\nsuch as software development4 or accounting5, us-\ning language models for machine-paraphrasing will\nbecome as easy as a click of a button soon. There-\nfore, the number of machine-plagiarized texts will\nincrease dramatically in the upcoming years. To\ncounteract this problem, we need robust solutions\nbefore models are widely misused.\nIn this study, we generate machine-paraphrased text\nwith GPT-3 and T5 (Raffel et al., 2020) to compose\na dataset for testing against automatically generated\nparaphrasing. We test different conﬁgurations of\nmodel size, training schemes, and selection criteria\nfor generating paraphrases. To understand how hu-\nmans perceive machine-paraphrased text, we also\nperformed an extensive study with 105 participants\nassessing their detection performance and quality-\nof-text assessments against existing automated de-\ntection methods. We show that while humans can\nspot paraphrasing of online tools and smaller au-\ntoencoding models, large autoregressive models\nprove to be a more complex challenge as they can\ngenerate human-like text containing the same key\nideas and messages from their original counterparts\n(see Table 1 for an example). Popular paid plagia-\nrism detection software (e.g. PlagScan6, Turnitin7)\nis already deceived by rudimentary paraphrasing\nmethods and large language models make this task\neven more challenging. We also test the models\nused for the generation, which show the highest\nperformance in detecting machine-paraphrased pla-\ngiarism.\nTo summarize our contributions:\n• We present a dataset with machine-\nparaphrased text from T5 and GPT-3 based\non original work from Wikipedia, arXiv,\nand student theses to train and evaluate\nmachine-paraphrased plagiarism.\n• We explore the human ability to detect para-\nphrase through three experiments, focusing\non (1) the detection difﬁculty of paraphras-\ning methods, (2) the quality of examples, and\n(3) the accuracy of humans in distinguishing\nbetween paraphrased and original texts.\n4https://copilot.github.com/\n5https://openai.com/blog/openai-api/\n6https://www.plagscan.com/en/\n7https://turnitin.com\n• We empirically test plagiarism detection soft-\nware (i.e., PlagScan) against machine learn-\ning methods and neural language models (au-\ntoencoding and autoregressive) in detecting\nmachine-paraphrased plagiarism.\n• We show that paraphrases from GPT-3 pro-\nvide the most realistic plagiarism cases that\nboth humans and automated detection solu-\ntions fail to spot, while the model itself is\nthe best-tested candidate for detecting para-\nphrases.\n2 Related Work\nPlagiarism Detection: Plagiarism describes the\nuse of ideas, concepts, words, or structures without\nproper source acknowledgment (Meuschke, 2021).\nPlagiarism datasets are limited to the number of\nreal plagiarism cases known. With the recent\nsuccess of artiﬁcial intelligence in natural lan-\nguage processing (NLP) applications, paraphrase\ngeneration and plagiarism detection methods\nincreasingly rely on dense text representations and\nmachine learning classiﬁers (Foltýnek et al., 2019).\nMachine learning methods often fail to detect\nsubstantial paraphrasing from neural language\nmodels (Wahle et al., 2021). In particular, large\nautoregressive language models (e.g., GPT-3)\ncan generate paraphrased content almost indis-\ntinguishable from original work (Witteveen and\nAndrews, 2019). However, these models are still\ninsufﬁciently explored in the domain of plagiarism\ndetection, even though their impact on the ﬁeld is\nalready being discussed (Dehouche, 2021).\nMachine-Paraphrase Detection: Machine-\nparaphrasing can be described as the automatic\ngeneration of text that is semantically close to its\nsource and written in other words (Bhagat and\nHovy, 2013). Machine-paraphrasing experiences a\ngrowing research interest from NLP for learning\nsemantic representations and related applications\n(Rush et al., 2015; McCann et al., 2018). However,\nparaphrasing can be used in plagiarism detection to\ndeceive humans and thus needs detection solutions\nto prevent it (Foltýnek et al., 2019).\nLexical substitution is a common paraphrase mech-\nanism used by plagiarists (Barrón-Cedeño et al.,\n2013). Many online paraphrasing tools also use\nsynonym replacements and other lexical perturba-\ntions to paraphrase text automatically (Foltýnek\net al., 2020a). (Foltýnek et al., 2020b) showed that\nmachine-learning classiﬁers (e.g., Support Vector\nMachine) could easily detect paraphrasing from\npopular online paraphrasing tools such as SpinBot.\n(Wahle et al., 2021) proposed a benchmark with\nparaphrased examples from autoencoding models\n(e.g., BERT(Devlin et al., 2019), RoBERTa(Liu\net al., 2019)), showing that neural language models\ncan generate more challenging paraphrasing than\ntraditional online tools (e.g., SpinnerChief, Spin-\nBot). In a follow-up study, (Wahle et al., 2022a)\nevaluate neural language models (e.g., BERT) on\nparaphrased texts from SpinnerChief, another in-\ndependent paid online paraphrasing tool. Their\nmain ﬁnding was that neural language models out-\nperform machine learning techniques and can ob-\ntain super-human performance in all test cases.\n(Foltýnek et al., 2020b; Wahle et al., 2021, 2022a)\nresults show that synonym replacements are simple\nto detect with state-of-the-art neural language mod-\nels. However, none of these studies explore using\nlarge autoregressive models in their experiments.\nSo far, only a few studies have analyzed the im-\npacts of plagiarism using autoregressive models.\nSeq2Seq models were ﬁrst used by (Prakash et al.,\n2016) with stacked residual LSTM networks to gen-\nerate paraphrases. (Witteveen and Andrews, 2019)\ntrain GPT-2 to generate paraphrased versions of a\nsource text and select paraphrased candidates with\nthe highest similarity according to universal sen-\ntence encoder(Cer et al., 2018) embeddings and\nlow word overlap when compared to their original\ncounterparts. (Biderman and Raff, 2022) show that\nGPT-J (Wang and Komatsuzaki, 2021), a smaller\nversion of GPT-3 with six billion parameters, can\nplagiarize student programming assignments that\nare not detected by MOSS8, a popular plagiarism\ndetection tool. The scaling of models allows for the\ngeneration of text indistinguishable from human\nwriting (Brown et al., 2020). In addition, the mod-\nels’ increase in size and consequentially their per-\nformance (Kaplan et al., 2020) have the potential\nto make the paraphrase detection task even more\ndifﬁcult.\n3 Methodology\nThis study focuses on understanding how hu-\nmans and machines perceive large autoregressive\nmachine-generated paraphrase examples. There-\n8https://theory.stanford.edu/~aiken/moss/\nfore, we ﬁrst generate machine-paraphrased text\nwith different model sizes of GPT-3 and T5. We\nthen generate a dataset composed of 200,000 exam-\nples from arXiv (20,966), Wikipedia (39,241), and\nstudent graduation theses (5,226) using the best\nconﬁguration of both models.\nWe investigate how humans and existing detection\nsolutions perceive this newly automated form of\nplagiarism. In our human experiments, we compare\nparaphrased texts generated in this study to existing\ndata that use paid online paraphrasing tools and\nautoencoding language models to paraphrase their\ntexts. Finally, we evaluate commercial plagiarism\ndetection software, machine-learning classiﬁers,\nand neural language model-based approaches to\nthe machine-paraphrase detection task.\n3.1 Paraphrase Generation\nMethod: We generate candidate versions of para-\ngraphs using prompts and human paraphrases as\nexamples in a few-shot style prediction (Table 2).\nWe provide the model with the maximum number\nof human paraphrased examples that ﬁt its context\nwindow with a maximum of 2048 tokens total. For\nboth models, we use their default conﬁguration.\nParaphrasing models’ goal is to mimic human para-\nphrases. Instead of manually engineering suitable\nprompts for the task, we use AutoPrompt (Shin\net al., 2020) to determine task instructions based on\nthe model’s gradients. As suggested by the authors,\nwe place the predict-token at the end of our prompt.\nOne example of a generated prompt was “Rephrase\nthe following sentence.” As humans tend to shorten\ntext when paraphrasing, we limit the maximum\nnumber of generated tokens concerning the origi-\nnal version to 90%, which is the approximate ratio\nof human plagiarism fragments in (Barrón-Cedeño\net al., 2013). Table 2 provides an example of the\nmodel’s input/output when generating paraphrases.\nCandidate Selection: Paraphrases that are similar\nto their source are of limited value as they have\nrepetitive patterns, while those with high linguistic\ndiversity often make models more robust (Qian\net al., 2019). The quality of paraphrases is typically\nevaluated using three dimensions of quality (i.e.,\nclarity, coherence, and ﬂuency), where high-quality\nparaphrases are those with high semantic similarity\nand high lexical and syntactic diversity (McCarthy\net al., 2009; Zhou and Bhat, 2021). We aim to\nchoose high-quality examples semantically close to\nParaphrase Generation Example\nRephrasethefollowingparagraphwhilekeepingitsmeaning:\nOriginal:Mydayhasbeenprettygood.\nParaphrased:Todaywasa goodday.\n⋯\nOriginal:Thispaperanalysestwoparaphrasingmethods.\nParaphrased:Weanalyzetwomethodsinthisstudy.\nOriginal:Thistextwaswrittenbya machine.\nParaphrased:Thissentencehasbeengeneratedartiﬁcially.\nTable 2: Example of generating paraphrased plagiarism\nwith few-shot learning. As input the model receives a\nprompt and human paraphrase example pairs . After\ninserting the to-be-paraphrased sentence , the model\nthen generates a paraphrased version as the output.\nthe original content without reusing the exact words\nand structures (Witteveen and Andrews, 2019).\nIn this paper, we choose generated candidates that\nmaximize their semantic similarity against their\noriginal counterparts while minimizing their count-\nbased similarity. We select the Pareto-optimal can-\ndidate that minimizes ROUGE-L and BLEU (i.e.,\npenalizing the exact usage of words compared to\nthe original version) and maximizes BERTScore\n(Zhang et al., 2019) and BARTScore9 (Yuan et al.,\n2021) (i.e., encouraging a similar meaning com-\npared to the original version). Table 3 provides an\nexample for generated paraphrases and their scores.\nWhile examples with high count-based similarity\nusually convey the same essential message (e.g.,\nOut 1 and Out 2), they also share a similar sen-\ntence structure and word usage. Examples with\nhigh semantic similarity and lower count-based\nsimilarity (e.g., Out 3) state the same meaning but\nrephrase the sentence with novel structure and sim-\nilar words describing the same idea.\nDataset Creation: To provide data for common\nsources of academic plagiarism (i.e., scientiﬁc arti-\ncles), we paraphrase the original examples of the\nmachine paraphrase corpus (MPC) (Wahle et al.,\n2022a) which is mainly composed of publications\non arXiv, Wikipedia, and student’s graduation the-\nses. As human-authored examples, we sample\nequally from two of the most popular paraphrase\ndatasets, i.e., P4P and PPDB 2.0 (Zhou and Bhat,\n2021). The P4P database (Barrón-Cedeño et al.,\n2013) is composed of realistic plagiarism cases\nwith the paraphrase phenomena they contain (e.g.,\n9We use the large model version for both metrics.\nmorphology-based, syntax-based, lexicon-based),\nand the PPDB 2.0 database (Pavlick et al., 2015)\nis a large-scale paraphrase corpus extracted with\nbilingual pivoting from which we extract the high-\nquality phrasal and lexical subsets.\n3.2 Human Evaluation\nOur human study aims to understand how partic-\nipants perceive machine-paraphrased plagiarism\ncompared to original work and human-paraphrased\ntext. We used Amazon’s Mechanical Turk (AMT)\nservice to obtain human assessments for para-\nphrased text classiﬁcation. Additionally, we asked\nexperts that actively published in the plagiarism\ndetection domain over the past ﬁve years. To have\nadequate statistical power in our analyses (Card\net al., 2020), we included a total of 105 participants\n(see Appendix A.1 for details on demographic in-\nformation about participants).\nIn the ﬁrst part of the human study (Q2 in Sec-\ntion 4), 50 participants are provided with a mutually\nexclusive choice of whether a text was machine-\nparaphrased or original and a text ﬁeld to justify\ntheir reasoning. In the second part (Q3 in Sec-\ntion 4), 50 participants from AMT and ﬁve experts\nfrom the research community were provided with\na mutually exclusive choice of 5 points on a Likert\nscale for each of the three parameters of clarity,\nﬂuency, and coherence. For the ﬁrst experiment,\neach participant evaluated ﬁve texts for ﬁve models\nresulting in 1,250 text evaluations. For the second\nexperiment, each participant evaluated ten texts for\nthree parameters, totaling 1,340 text evaluations.\nFollowing common best practices on AMT (Berin-\nsky et al., 2012), evaluators had to have over a\n95% acceptance rate, be in the United States, and\nhave completed over 1,000 successful tasks. We\nexcluded evaluators’ assessments if their explana-\ntions were directly copied text from the task (> 90%\ntext match), did not match their classiﬁcation, or\nwere short, vague, or otherwise non-interpretable.\nAcross experiments, 138 assessments (≈10%) were\nrejected and not included in the experiments.\n4 Research Questions & Experiments\nQ1: How does model size inﬂuence the quality of\ngenerated paraphrases?\nA. We ask this question to underline the problem’s\nurgency as recently released models have a large\nnumber of parameters. Figure 1 shows the inﬂu-\nBERTSc. BARTSc. Rouge-L BLEU\nIn: Later in his career, Gates has pursued many business and\nphilanthropic endeavors.\n- - - -\nOut 1: Later, his time was allocated to business and philanthropic\nendeavors.\n0.79 0.74 0.55 0.63\nOut 2: Later in his career, Gates focused on business and charity. 0.84 0.83 0.64 0.51\nOut 3*: Gates focused on business and charitable efforts later in\nhis career.\n0.83 0.85 0.35 0.49\nTable 3: Candidate selection of machine-generated paraphrases with an example from (Witteveen and Andrews,\n2019). We choose the Pareto-optimal example that maximizes semantic similarity (BERTScore, BARTScore) and\nminimizes word overlap (ROUGE-L, BLEU). ∗Selected example in boldface.\nence of model size on the similarity scores of gen-\nerated candidates against their original candidates\non 500 random examples from the PPDB dataset.\nWe test the 220M, 770M, 3B, and 11B versions of\nT5 and the 350M, 1.3B, 6.7B, and 175B versions\nof GPT-3 (also known as Ada, Babbage, Curie,\nand Davinci in the OpenAI API 10 respectively).\nWith the increasing number of parameters, both\nmodels’ semantic similarity scores (BERTScore,\nBARTScore) also rise. T5 shows the highest in-\ncrease when extending the model from 3 billion\nparameters to 11 billion. GPT-3 (175B) reaches\nits overall highest semantic similarity, generating\nsentences with similar meanings compared to the\nsource. Model’s generated candidates also have\nhigher count-based scores on average as they often\nrepeat text from the source. As described before,\nwe try to sample candidates with low word-count\nscores to avoid repetition of words.\nWe conclude that scaling models’ size positively in-\nﬂuences their performance at the task of paraphras-\ning, which agrees with previous research (Kaplan\net al., 2020). While the limits and details of scaling\nmodels are still unknown, boosting their computing\npower will allow for more human-like texts to be\nproduced.\nQ2. Can humans identify whether a text is original,\nor machine-paraphrased?\nA. This question is inspired by the Turing (1950)\nTest to differentiate machines from humans. To\nanswer this question, we asked participants to as-\nsess whether texts were machine-generated (see\nAppendix A.3 for more details). We compared\noriginal work to an online paraphrasing tool\n(SpinnerChief), two auto-encoding models (BERT,\n10https://openai.com/api/\nFigure 1: Paraphrasing similarity scores for a sample\nof the dataset with different model sizes of GPT-3 and\nT5.\nRoBERTa), and two large auto-regressive mod-\nels (T5, GPT-3). As examples, we sampled 30\nmachine-generated paragraphs for each model and\ntheir corresponding 30 original texts with an equal\nweighting between the three sources (Wikipedia,\narXiv, and student theses). We performed a\nBonferroni-corrected two-sided T-Test to test for\nstatistical signiﬁcance compared to a control model.\nAs the control model, we chose SpinnerChief with\nits default paraphrasing frequency as it was the\nmost difﬁcult-to-detect online paraphrasing tool\ntested in (Wahle et al., 2022a). Participants re-\nceived individual text examples with three annota-\ntion options: “machine-paraphrased”, “original”,\nand “I don’t know”. Participants were not shown\naligned examples (i.e., an original and its para-\nphrased version) to avoid memorization effects.\nTable 4 shows the mean human accuracy (i.e.,\nthe ratio of correct assignments to non-neutral as-\nsignments per participant) in detecting machine-\nparaphrased text. The results show that humans can\nadequately detect the control model with 82% accu-\nracy on average (where 50% is a chance level per-\nformance). In contrast, human accuracy at detect-\ning paraphrases produced by autoencoding models\nwas signiﬁcantly lower, ranging from 61% to 71%\nover all participants. Plagiarism cases generated\nby large autoregressive models were usually hardly\nabove chance (53% for GPT-3 and 56% for T5).\nFor more information on the annotator agreement,\nplease see Appendix A.2. Human abilities to detect\nmachine-paraphrased text appear to decrease with\nincreasing model size and are particularly challeng-\ning for autoregressive models as they can change\nsentence structure and word order instead of single\nword replacements. Our ﬁndings on human detec-\ntion against autoregressive models corroborate with\nrecent results (Clark et al., 2021), challenging the\ncommon choice of humans as the gold standard.\nQ3. How similar are machine-generated para-\nphrases to human-paraphrases?\nA. We sampled 500 examples pairs (i.e., orig-\ninal, human-paraphrased) from the PPDB cor-\npus and paraphrased half of the original ver-\nsions with GPT-3 (175B) and the other half\nwith T5 (11B). As a proxy for similarity be-\ntween originals, human-paraphrased, and machine-\nparaphrased examples, we calculated their similar-\nity using BERTScore. The average BERTScore\nbetween human-paraphrases and originals (76%)\nis lower than between machine-generated para-\nphrases and originals (79%). The similarity be-\ntween human-paraphrases and machine-generated\nparaphrases is highest (81%). This result suggest\nthat machine-generated paraphrases are typically\ncloser to the human paraphrases than to the original,\nwhich we assume is due to the model’s objective\nto mimic human behavior, which are provided as\ngeneration examples.\nQ4. How do humans assess the quality of machine-\nparaphrased plagiarism?\nA. We asked human annotators to score gener-\nated paraphrases according to their clarity, ﬂuency,\nand coherence (Zhou and Bhat, 2021) (see Ap-\npendix A.3 for more details about the questions).\nAs quality assessments are challenging to evalu-\nate, we increased the requirements for participants.\nWe asked the second group of 50 participants that\nrequired to have a higher education degree (bach-\nelor’s, master’s, or Ph.D. degree). We also asked\nadditional ﬁve experts that have published at least\ntwo peer-reviews papers on plagiarism detection in\nthe last ﬁve years. Each participant annotated ten\nrandomly drawn examples on a Likert scale from 1\nto 5 regarding clarity, ﬂuency, and coherence (Zhou\nand Bhat, 2021).\nTable 5 shows the average rating for all 55 partici-\npants While original contents achieve the highest\nrating for all three dimensions, the largest version\nof GPT-3 achieves similar ratings. SpinnerChief’s\nquality of paraphrases is signiﬁcantly lower. BERT\nachieves convincing results as well, also because\nthe frequency of word changes (15%) for synonyms\nis lower than SpinnerChief’s (50%), and therefore\ngenerates examples closer to the original text.\nFluency was rated highest for all models, while clar-\nity and coherence were the lowest. We assume that\nas source sentences come from diverse scientiﬁc\nﬁelds, they might already be difﬁcult to understand;\nthus, paraphrasing can confuse readers when tech-\nnical terms are used wrong. For more information\non annotator agreement and the relation between\nexperts and their educational degree, please see\nAppendix A.2.\nQ5. How do existing detection methods identify\nparaphrased plagiarism?\nA. To test the detection performance of automated\nplagiarism detection solutions, we evaluate ﬁve\nmethods and compare them to random guesses and\na human baseline. We presume automated detec-\ntion solutions can identify paraphrases better than\nhumans as (Ippolito et al., 2020) showed that large\nlanguage models are optimized to fool humans at\nthe expense of introducing statistical anomalies\nwhich automated solutions can spot. As a de-facto\nsolution for plagiarism detection, we test PlagScan,\none of the best-performing systems, in a compre-\nhensive test conducted by the European Network\nfor Academic Integrity (Foltýnek et al., 2020a).\nWe test a combination of naïve bayes classiﬁer and\nword2vec (Mikolov et al., 2013), and three autoen-\ncoding transformers: BERT (Devlin et al., 2019),\nRoBERTa(Liu et al., 2019), and Longformer (Belt-\nagy et al., 2020) which are the best performing\nmodels in machine-paraphrase detection of (Wahle\net al., 2021, 2022a). Additionally, we evaluate the\nlargest versions of T5 and GPT-3 using few-shot\nprediction.\nAs paraphrasing models, we choose SpinnerChief;\nMean accuracy 95% Conﬁdence\nInterval (low, hi)\ntcompared to\ncontrol (p-value)\n“I don´t know\"\nassignments\nSpinnerChief (Control) 82% 76%-89% - 2.8 %\nBERT 67% 63%–71% 14.2 (1 e-11) 4.9%\nRoBERTa 65% 61%–70% 18.1 (1 e-29) 5.5%\nT5 11B 56% 51%–59% 16.6 (1 e-16) 7.1%\nGPT-3 175B 53% 49%–55% 19.2 (1e-34) 7.2%\nTable 4: Human accuracy in identifying whether parapgraphs of scientiﬁc papers from the arXiv subset are\nmachine-paraphrased. Human performance ranges from 82% on the control model to 53% on GPT-3 175B. This\ntable compares mean accuracy of with ﬁve paraphrasing models and shows the results of a two-sample T-Test\nbetween each model and the SpinnerChief control model according to (Wahle et al., 2022a). Lowest scores are in\nboldface.\nClarity Fluency Coherence\nOriginal 3.98 ( ±0.78) 4.21 (±0.81) 3.81 (±0.92)\nSpinnerChief 2.52 (±1.15) 2.94 (±1.19) 2.83 (±1.23)\nBERT 3.45 ( ±.1.29) 3.34 (±0.90) 3.73(±1.22)\nGPT-3 3.92 (±0.97) 3.60(±1.02) 3.72 (±1.07)\nTable 5: Average scores on a Likert-scale from 1 to 5 of\nmachine-generated plagiarism on the Wikipedia test set.\nEach example is judged by 50 participants with a bach-\nelor’s, master’s, or PhD degree and ﬁve experts in the\nplagiarism detection community. Standard deviation is\nshown in parenthesis. Highest scores are in boldface.\nthe best performing paid online paraphrasing tool\ntested in (Wahle et al., 2022a). Spinnerchief at-\ntempts to change every fourth word with a syn-\nonym. We use BERT as an autoencoding baseline\nand set the masking probability to 15% as in (Wahle\net al., 2021). As a large autoregressive model, we\nuse GPT-3 175B, the best model for automated\nsimilarity metrics and deceiving humans.\nTable 6 shows the average F1-macro except for\nthe human baseline, which shows accuracy. For\nPlagScan, we assume positive examples when\nthe text-match is greater than 50%. Looking at\nparaphrased plagiarism of SpinnerChief, humans\nreach between 79% and 85% accuracy on average.\nPlagScan achieves results up to 7% over the ran-\ndom baseline for Wikipedia articles but achieves\nclose to random performance for student theses.\nAs in (Wahle et al., 2022a), we assume PlagScan\nindexes Wikipedia and arXiv but not student the-\nses used in the MPC. Neural approaches based on\nnaïve bayes reach between 58% and 67% F1-macro\nscores while autoencoding models achieve up to\n67% - 78% (Longformer). Large autoregressive\nmodels achieve peak scores of 85% (T5 11B) and\n87% (GPT-3 175B) on SpinnerChief’s paraphrases.\nResults of detection models on BERT paraphrasing\nshow similar patterns to SpinnerChief, as autoen-\ncoding models also replace masked words with syn-\nonyms. While detection results are generally lower\nfor humans and PlagScan, autoencoding models\nimprove by a signiﬁcant margin. As pointed out in\nsimilar studies (Zellers et al., 2019; Wahle et al.,\n2021), models generating the paraphrased content\nare typically the best to detect it. The similarity\nin the architecture of the autoencoding models al-\nlows BERT, RoBERTa, and Longformer for the\nlargest performance increase over SpinnerChief.\nStill, large autoregressive models achieve the best\nresults in detecting machine-paraphrasing of BERT\noverall, with over 80% F1-score for GPT-3.\nWhen looking at paraphrasing of GPT-3, all mod-\nels detect paraphrases signiﬁcantly worse. Hu-\nmans, plagiarism detection software, and autoen-\ncoders can hardly achieve better results than ran-\ndom chance, which underlines how convincing\nparaphrased texts from large autoregressive models\nare. T5 and GPT-3 can achieve low, but reasonable\nresults between 60% - 63% (T5) and 64% - 66%\n(GPT-3) F1-macro.\nWhile detection results on large autoregressive\nparaphrasing seem low, models were not explic-\nitly trained on the task and are predicted based on\nprevious ﬁne-tuning on other data (upper part) or\nnot ﬁne-tuning (lower part). We assume GPT-3\nis the best detection solution because it generated\nthe paraphrased texts. Therefore, we see T5 as a\nbaseline when autoregressive paraphrasing models\nSpinnerChief BERT GPT-3\nModel arXiv Theses Wiki arXiv Theses Wiki arXiv Theses Wiki\nRandom 51.72 53.23 49.21 51.90 50.24 48.28 50.61 50.30 49.77\nHuman Baseline† 83.25 79.32 84.96 68.93 63.41 69.08 55.74 50.60 52.82\nPlagScan†† 55.07 49.29 57.10 57.73 50.22 59.04 49.28 48.90 50.19\nw2v + NB 65.89 58.24 66.83 62.12 59.96 63.38 52.78 51.01 51.15\nBERT 64.59 63.59 57.45 80.83 74.74 83.21 52.44 50.89 52.59\nRoBERTa 66.00 58.24 58.94 70.41 68.99 72.18 53.14 49.90 53.81\nLongformer 78.34 74.82 67.11 65.18 65.72 69.98 54.70 50.84 53.99\nT5 11B 82.92∗∗ 83.45∗∗ 79.92∗∗ 84.66∗∗ 78.09∗∗ 82.37∗∗ 59.80∗∗ 61.42∗∗ 62.72∗∗\nGPT-3 175B 83.20∗∗ 82.11∗∗ 79.68∗∗ 87.21∗∗ 81.02∗∗ 84.48∗∗ 66.52∗∗ 64.38∗∗ 65.79∗∗\nTable 6: F1-Macro scores of detection models for text paraphrased by SpinnerChief, BERT, and GPT-3. Numbers\nin boldface are the overall best result. ∗∗Results are statistically signiﬁcant using random and permutation tests\n(Dror et al., 2018) with p < 0.05. †Accuracy calculated as in Table 4. ††F1-score when text-match is greter than\n50%.\nare unknown.\nIn general, neural detection models reach their high-\nest performance for Wikipedia articles which we\nassume is due to their pre-training data contain-\ning Wikipedia examples. Student theses pose the\nmost challenging scenario for both humans and\nneural approaches, as it contains challenging ex-\namples and is written by non-native English as a\nsecond language speakers. Across experiments,\nPlagScan is not able to reliably identify machine-\nparaphrasing. Large autoregressive models make\nit challenging for PlagScan to ﬁnd text matches\nas phrasal and lexical substitutions can change the\nwords with synonyms and the order of words. The\nautomatic detection results on paraphrasing of GPT-\n3 are alarming as many of the most used models fail\nto detect its paraphrases. Even though the absolute\nresults of GPT-3 and T5 are low, they can perform\nbetter than humans at the detection task. Therefore,\nwe assume that, similar to (Vahtola et al., 2021),\nthere exist statistical abnormalities and patterns that\nautomated solutions can leverage to increase their\ndetection performance.\n5 Epilogue\nConclusion: We generated machine-paraphrased\nplagiarism using large autoregressive models up to\n175 billion parameters convincing paraphrased ex-\namples that deceived humans and plagiarism detec-\ntion solutions. We tested the human ability to detect\nmachine-generated paraphrases of large models\nand compared their assessments to well-established\nonline tools. We evaluated one plagiarism detection\nsoftware, one traditional machine-learning model,\nthree autoencoding, and two large autoregressive\nmodels detecting machine-paraphrased examples.\nDespite some limitations, our results suggest that\nlarge language models may increase the number\nof automated plagiarism cases through convincing\nparaphrasing of original work.\nFuture Work: This study is an initial step toward\nunderstanding how large language models can fos-\nter illicit activities in the scientiﬁc domain. We\nplan to further examine the similarities and dif-\nferences between human- and machine-generated\nparaphrases to understand whether humans have\ndifﬁculties in detecting paraphrases in general.\nWhen looking at participants’ justiﬁcations for clas-\nsifying machine-generated paraphrases, we plan to\nanalyze common terms and highlights to ﬁnd pos-\nsible markers for classiﬁcation decisions. Over the\nscope of English, our approach could be applied\nto other languages and even generate paraphrases\nfrom one language to another using multilinugal\nmodels and data. Finally, as academic plagiarism\nmainly relies on scientiﬁc articles, we want to ex-\ntend our study to large scientiﬁc corpora with high\nvariation across domains and venues (Lo et al.,\n2020; Wahle et al., 2022b).\nLimitations\nAlthough our experiments explore how human and\nautomated solutions struggle to identify machine-\nparaphrased examples from large language models,\nwe did not detail the similarities and differences be-\ntween human- and machine-generated paraphrases.\nComparing human paraphrases and machine para-\nphrases - qualitatively and automatically - would\nallow for a better understanding of what makes\nparaphrasing so challenging. As the classiﬁcation\nfrom our language models currently does not pro-\nvide references or sources for their results, these\nmodels can only be used as a support tool to iden-\ntify sentences and paragraphs for more detailed\ndeliberation. While our study has the above lim-\nitations, the focus of this study was to underline\nthe urgency of the problem of machine-generated\nplagiarism to promote better detection solutions in\nthe future.\nEthics Statement\nPlagiarism is illegal, unethical, and morally un-\nacceptable in all countries (Kumar and Tripathi,\n2013). While the binary classiﬁcation of machine-\nparaphrased examples in this study can indicate\nhow automated detection solutions would point out\npotential plagiarism cases, a team of experts should\nmake a ﬁnal decision on such cases. False-positive\ncases of wrongly accused researchers could ruin\ntheir careers forever. Therefore, all cases should be\ncarefully evaluated before any ﬁnal verdict. As this\nstudy and related work show (Clark et al., 2021),\nhumans are unreliable enough for paraphrase de-\ntection in the age of large neural language models.\nThe difﬁculty of machine-paraphrase identiﬁcation\nmakes legal decisions on plagiarism cases partic-\nularly complex. We presume paraphrasing with\nlanguage models will lead to more plagiarists get-\nting unnoticed when using large models to gener-\nate their paraphrases. One exciting approach to\ngain transparency would rely on reconstructing the\nmodel’s potential inputs (Tu et al., 2017; Niu et al.,\n2019) given the paraphrased version and classify-\ning original candidates using a hybrid approach\nconsidering text-match and semantic features. We\nadopted a binary classiﬁcation in gender for our hu-\nman evaluation, which we plan to improve in future\nwork so it can be more inclusive. Therefore, gender\nmight not represent the natural diversity included\nin our dataset.\nReferences\nAlberto Barrón-Cedeño, Marta Vila, M. Martí, and\nPaolo Rosso. 2013. Plagiarism Meets Paraphrasing: In-\nsights for the Next Generation in Automatic Plagiarism\nDetection. Computational Linguistics, 39(4):917–947.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer.\nAdam J. Berinsky, Gregory A. Huber, and Gabriel S.\nLenz. 2012. Evaluating online labor markets for ex-\nperimental research: Amazon.com’s mechanical turk.\nPolitical Analysis, 20(3):351–368.\nRahul Bhagat and Eduard Hovy. 2013. Squibs: What is\na paraphrase? Computational Linguistics, 39(3):463–\n472.\nStella Biderman and Edward Raff. 2022. Neu-\nral Language Models are Effective Plagiarists.\nArXiv220107406 Cs.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. 2020. Language\nModels are Few-Shot Learners. In Advances in Neural\nInformation Processing Systems , volume 33, pages\n1877–1901. Curran Associates, Inc.\nDallas Card, Peter Henderson, Urvashi Khandelwal,\nRobin Jia, Kyle Mahowald, and Dan Jurafsky. 2020.\nWith Little Power Comes Great Responsibility. In Pro-\nceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP) , pages\n9263–9274, Online. Association for Computational\nLinguistics.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St. John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-\nHsuan Sung, Brian Strope, and Ray Kurzweil. 2018.\nUniversal sentence encoder.\nElizabeth Clark, Tal August, Soﬁa Serrano, Nikita\nHaduong, Suchin Gururangan, and Noah A. Smith.\n2021. All that’s ‘human’ is not gold: Evaluating hu-\nman evaluation of generated text. In Proceedings of\nthe 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Volume\n1: Long Papers) , pages 7282–7296, Online. Associa-\ntion for Computational Linguistics.\nN Dehouche. 2021. Plagiarism in the age of massive\nGenerative Pre-trained Transformers (GPT-3). Ethics.\nSci. Environ. Polit., 21:17–23.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of Deep\nBidirectional Transformers for Language Understand-\ning. ArXiv181004805 Cs.\nRotem Dror, Gili Baumer, Segev Shlomov, and Roi Re-\nichart. 2018. The hitchhiker’s guide to testing statis-\ntical signiﬁcance in natural language processing. In\nProceedings of the 56th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long\nPapers), pages 1383–1392, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nJoseph L. Fleiss and Jacob Cohen. 1973. The equiva-\nlence of weighted kappa and the intraclass correlation\ncoefﬁcient as measures of reliability. Educational and\nPsychological Measurement, 33(3):613–619.\nTomáš Foltýnek, Dita Dlabolová, Alla Anohina-\nNaumeca, Salim Razı, Július Kravjar, Laima Kamzola,\nJean Guerrero-Dib, Özgür Çelik, and Debora Weber-\nWulff. 2020a. Testing of Support Tools for Plagiarism\nDetection. ArXiv200204279 CsDL.\nTomáš Foltýnek, Norman Meuschke, and Bela Gipp.\n2019. Academic Plagiarism Detection: A System-\natic Literature Review. ACM Computing Surveys ,\n52(6):112:1–112:42.\nTomáš Foltýnek, Terry Ruas, Philipp Scharpf, Norman\nMeuschke, Moritz Schubotz, William Grosky, and Bela\nGipp. 2020b. Detecting Machine-Obfuscated Plagia-\nrism. In Anneli Sundqvist, Gerd Berget, Jan Nolin,\nand Kjell Ivar Skjerdingstad, editors, Sustainable Digi-\ntal Communities, volume 12051 LNCS, pages 816–827.\nSpringer International Publishing, Cham.\nDaphne Ippolito, Daniel Duckworth, Chris Callison-\nBurch, and Douglas Eck. 2020. Automatic Detection\nof Generated Text is Easiest when Humans are Fooled.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics , pages 1808–\n1822, Online. Association for Computational Linguis-\ntics.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B. Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei.\n2020. Scaling laws for neural language models.\nRanjeet Kumar and RC Tripathi. 2013. An analysis\nof automated detection techniques for textual similar-\nity in research documents. International Journal of Ad-\nvanced Science and Technology, 56(9):99–110.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta:\nA robustly optimized bert pretraining approach. Cite\narxiv:1907.11692.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-\nney, and Daniel Weld. 2020. S2ORC: The semantic\nscholar open research corpus. In Proceedings of the\n58th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 4969–4983, Online. Associa-\ntion for Computational Linguistics.\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong,\nand Richard Socher. 2018. The natural language\ndecathlon: Multitask learning as question answering.\narXiv preprint arXiv:1806.08730.\nPhilip M McCarthy, Rebekah H Guess, and Danielle S\nMcNamara. 2009. The components of paraphrase eval-\nuations. Behavior Research Methods, 41(3):682–690.\nNorman Meuschke. 2021. Analyzing Non-Textual Con-\ntent Elements to Detect Academic Plagiarism . Ph.D.\nthesis, University of Konstanz, Dept. of Computer and\nInformation Science. Doctoral Thesis.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efﬁcient Estimation of Word Representa-\ntions in Vector Space. ArXiv13013781 Cs.\nXing Niu, Weijia Xu, and Marine Carpuat. 2019. Bi-\ndirectional differentiable input reconstruction for low-\nresource neural machine translation. In Proceedings of\nthe 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 442–448, Minneapolis, Minnesota. Asso-\nciation for Computational Linguistics.\nEllie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch,\nBenjamin Van Durme, and Chris Callison-Burch. 2015.\nPPDB 2.0: Better paraphrase ranking, ﬁne-grained en-\ntailment relations, word embeddings, and style clas-\nsiﬁcation. In Proceedings of the 53rd Annual Meet-\ning of the Association for Computational Linguistics\nand the 7th International Joint Conference on Natu-\nral Language Processing (Volume 2: Short Papers) ,\npages 425–430, Beijing, China. Association for Com-\nputational Linguistics.\nAaditya Prakash, Sadid A. Hasan, Kathy Lee, Vivek\nDatla, Ashequl Qadir, Joey Liu, and Oladimeji Farri.\n2016. Neural paraphrase generation with stacked resid-\nual lstm networks.\nLihua Qian, Lin Qiu, Weinan Zhang, Xin Jiang, and\nYong Yu. 2019. Exploring diverse expressions for para-\nphrase generation. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP) ,\npages 3173–3182, Hong Kong, China. Association for\nComputational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the Limits\nof Transfer Learning with a Uniﬁed Text-to-Text Trans-\nformer. ArXiv191010683 Cs Stat.\nAlexander M. Rush, Sumit Chopra, and Jason Weston.\n2015. A neural attention model for abstractive sentence\nsummarization. In Proceedings of the 2015 Conference\non Empirical Methods in Natural Language Process-\ning, pages 379–389, Lisbon, Portugal. Association for\nComputational Linguistics.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting Knowledge from Language Models with Au-\ntomatically Generated Prompts. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pages 4222–4235,\nOnline. Association for Computational Linguistics.\nZhaopeng Tu, Yang Liu, Lifeng Shang, Xiaohua Liu,\nand Hang Li. 2017. Neural machine translation with\nreconstruction. AAAI Conference on Artiﬁcial Intelli-\ngence.\nA. M. Turing. 1950. I.—COMPUTING MACHINERY\nAND INTELLIGENCE. Mind, LIX(236):433–460.\nTeemu Vahtola, Mathias Creutz, Eetu Sjöblom, and\nSami Itkonen. 2021. Coping with noisy training data la-\nbels in paraphrase detection. In Proceedings of the Sev-\nenth Workshop on Noisy User-generated Text (W-NUT\n2021), pages 291–296, Online. Association for Compu-\ntational Linguistics.\nJan Philip Wahle, Terry Ruas, Tomáš Foltýnek, Nor-\nman Meuschke, and Bela Gipp. 2022a. Identifying\nMachine-Paraphrased Plagiarism. In Malte Smits, edi-\ntor, Information for a Better World: Shaping the Global\nFuture, volume 13192, pages 393–413. Springer Inter-\nnational Publishing, Cham.\nJan Philip Wahle, Terry Ruas, Norman Meuschke, and\nBela Gipp. 2021. Are Neural Language Models Good\nPlagiarists? A Benchmark for Neural Paraphrase De-\ntection. In Proceedings of the ACM/IEEE Joint Confer-\nence on Digital Libraries (JCDL).\nJan Philip Wahle, Terry Ruas, Saif M. Mohammad, and\nBela Gipp. 2022b. D3: A massive dataset of scholarly\nmetadata for analyzing the state of computer science\nresearch. In Proceedings of The 13th Language Re-\nsources and Evaluation Conference, Marseille, France.\nEuropean Language Resources Association.\nBen Wang and Aran Komatsuzaki. 2021. GPT-\nJ-6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/\nmesh-transformer-jax.\nSam Witteveen and Martin Andrews. 2019. Paraphras-\ning with large language models. In Proceedings of the\n3rd Workshop on Neural Generation and Translation ,\npages 215–220, Hong Kong. Association for Computa-\ntional Linguistics.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text generation.\nAdvances in Neural Information Processing Systems ,\n34:27263–27277.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending Against Neural Fake\nNews. ArXiv190512616 Cs.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\nuating text generation with bert. arXiv preprint\narXiv:1904.09675.\nJianing Zhou and Suma Bhat. 2021. Paraphrase gener-\nation: A survey of the state of the art. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing , pages 5075–5086, Online\nand Punta Cana, Dominican Republic. Association for\nComputational Linguistics.\nA Human Study\nA.1 Demographic Information of\nParticipants\nParticipants were given a choice to consent to pro-\nviding additional anonymous information, includ-\ning - but not limited to - gender, age, nationality,\nbirth country, current country of residence, ﬁrst lan-\nguage, and current education level11. Out of all 105\nparticipants, 99 provided demographic information.\nFor all participants, we received their total number\nof completed tasks and the time taken to complete\nour questions. The average time to rate ten ex-\namples was 8.07 ( ±6.82) minutes. The average\nnumber of total successful tasks for participants\nwas 1200 (±590).\nThe majority of tasks in this study were performed\nwithin 3 - 14 minutes (95% of mass in the interval\nof [µ−2σ, µ+2σ]). Three participants took signif-\nicantly longer (23, 27, and 43 minutes), and their\nratings were considered outliers on the distribution.\nAge & Gender: Participants were 24 years old\non average (18 - 41). There was no signiﬁcant\ndifference in age between men and women with\na two-sided T-Test (p=0.87). Figure 2 shows age\ndistribution by gender. The majority of participants\nwere younger than 25 years old.\n0 10 20 30\nFemale\nMale\nFigure 2: Distribution of age by gender of participants.\nEducation & First Language: Most participants\nfrom Q4 had a bachelor’s degree (68%). The re-\nmainder had a master’s degree (24%) or Ph.D. de-\ngree (8%).\nUnsurprisingly, as all participants reside in the US,\nmost of them (78%) had English as their ﬁrst lan-\nguage. The remainder had Chinese, Spanish, Viet-\nnamese, Russian, or Arabic as their ﬁrst language.\nA.2 Agreement\nThe inter-annotator agreement according to Fleiss\nKappa (Fleiss and Cohen, 1973) of participants for\nQ2 was κ= 0.84.\nThe inter-annotator agreement of the ﬁve experts\nin Q4 was κ = 0.66 and for the remaining 50\n11The complete list of attributes is available in our dataset.\nEnglishChineseSpanish Arabic Russian\nVietnamese\n0\n20\n40\n60\nFigure 3: Distribution of ﬁrst languages of participants.\nparticipants in Q4 it was κ= 0.79.\nThe agreement between the expert group and the\nAMT group was κ = 0.41, showing that experts\ndeviate strongly from average raters with a higher\neducation degree.\nWhen looking at participants with a Ph.D. and\na bachelor’s degree, assessments of paraphrasing\nquality deviated more κ= 0.57 than within the re-\nspective groups of participants with a Ph.D. degree\nκ= 0.79 and a master’s degreeκ= 0.77.\nA.3 Details on Questions\nFor the experiments in Q2, participants were asked\nthe following question:\nQuestion: Do you think the above example was\nmachine-paraphrased (which means a machine\nrewrote some human-authored text) then choose\n“machine-paraphrased”. If you think a human wrote\nthe example, please choose “original”. If you can-\nnot assign the example to either category, please\nchoose “I don’t know”.\nFor the experiments in Q4, participants were given\nthe following three instructions with the option to\nrate on a scale from one to ﬁve.\nInstruction 1: The ﬁrst question is about ﬂuency,\nwhich refers to the ability to write grammatically\ncorrectly and clearly. Does it sound like a native\nspeaker wrote it (high rating), or does it sound like\nsomeone who just learned English (low rating)?\nInstruction 2: The second question is about clarity,\nwhich refers to the presentation of content and its\nexplanation. Is the content easy to follow (high\nrating), or is it complicated and hard to understand\n(low rating)?\nInstruction 3: The third question is about coher-\nence, which refers to the consistency of content\nthroughout the paragraph. Is the content following\na common central idea (high rating), or is the text\njumping from one (random) idea to another (low\nrating)?"
}