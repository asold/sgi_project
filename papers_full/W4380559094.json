{
  "title": "Weakly-Supervised Scientific Document Classification via Retrieval-Augmented Multi-Stage Training",
  "url": "https://openalex.org/W4380559094",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2066410461",
      "name": "Ran Xu",
      "affiliations": [
        "Emory University"
      ]
    },
    {
      "id": "https://openalex.org/A2096181728",
      "name": "Yue Yu",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2110628446",
      "name": "Joyce Ho",
      "affiliations": [
        "Emory University"
      ]
    },
    {
      "id": "https://openalex.org/A2277406968",
      "name": "Carl Yang",
      "affiliations": [
        "Emory University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2104049510",
    "https://openalex.org/W2405884322",
    "https://openalex.org/W2604227417",
    "https://openalex.org/W3188983256",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W3034588688",
    "https://openalex.org/W2890931111",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W3171416056",
    "https://openalex.org/W1995258314",
    "https://openalex.org/W3166913490",
    "https://openalex.org/W3155316532",
    "https://openalex.org/W4382317982",
    "https://openalex.org/W4287854750",
    "https://openalex.org/W4385573358",
    "https://openalex.org/W3172399575",
    "https://openalex.org/W6600195515",
    "https://openalex.org/W4229009238",
    "https://openalex.org/W4385573188",
    "https://openalex.org/W4281749642",
    "https://openalex.org/W4226144573",
    "https://openalex.org/W3200496214",
    "https://openalex.org/W4324016655",
    "https://openalex.org/W4389524236",
    "https://openalex.org/W2148972377",
    "https://openalex.org/W4378782608",
    "https://openalex.org/W4394652053",
    "https://openalex.org/W4221142221",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W3035324702",
    "https://openalex.org/W4226216229",
    "https://openalex.org/W3118062200",
    "https://openalex.org/W4385572829",
    "https://openalex.org/W3106109117",
    "https://openalex.org/W4307536106",
    "https://openalex.org/W4385572673",
    "https://openalex.org/W4320813768",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4367047045",
    "https://openalex.org/W4221150928",
    "https://openalex.org/W3101606352",
    "https://openalex.org/W2108281845",
    "https://openalex.org/W4283723760"
  ],
  "abstract": "Scientific document classification is a critical task for a wide range of applications, but the cost of collecting human-labeled data can be prohibitive. We study scientific document classification using label names only. In scientific domains, label names often include domain-specific concepts that may not appear in the document corpus, making it difficult to match labels and documents precisely. To tackle this issue, we propose WanDeR, which leverages dense retrieval to perform matching in the embedding space to capture the semantics of label names. We further design the label name expansion module to enrich its representations. Lastly, a self-training step is used to refine the predictions. The experiments on three datasets show that WanDeR outperforms the best baseline by 11.9%. Our code will be published at https://github.com/ritaranx/wander.",
  "full_text": "Weakly-Supervised Scientific Document Classification via\nRetrieval-Augmented Multi-Stage Training\nRan Xuâˆ—\nEmory University\nAtlanta, GA, USA\nran.xu@emory.edu\nYue Yuâˆ—\nGeorgia Institute of Technology\nAtlanta, GA, USA\nyueyu@gatech.edu\nJoyce C. Ho\nEmory University\nAtlanta, GA, USA\njoyce.c.ho@emory.edu\nCarl Yangâ€ \nEmory University\nAtlanta, GA, USA\nj.carlyang@emory.edu\nABSTRACT\nScientific document classification is a critical task for a wide range\nof applications, but the cost of obtaining massive amounts of human-\nlabeled data can be prohibitive. To address this challenge, we pro-\npose a weakly-supervised approach for scientific document classifi-\ncation using label names only. In scientific domains, label names\noften include domain-specific concepts that may not appear in the\ndocument corpus, making it difficult to match labels and documents\nprecisely. To tackle this issue, we proposeWanDeR, which lever-\nages dense retrieval to perform matching in the embedding space to\ncapture the semantics of label names. We further design the label\nname expansion module to enrich the label name representations.\nLastly, a self-training step is used to refine the predictions. The\nexperiments on three datasets show that WanDeRoutperforms the\nbest baseline by 11.9% on average. Our code will be published at\nhttps://github.com/ritaranx/wander.\nKEYWORDS\nScientific Document Classification, Weak Supervision, Retrieval\nACM Reference Format:\nRan Xu, Yue Yu, Joyce C. Ho, and Carl Yang. 2023. Weakly-Supervised\nScientific Document Classification via Retrieval-Augmented Multi-Stage\nTraining. In Proceedings of the 46th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval (SIGIR â€™23), July 23â€“27,\n2023, Taipei, Taiwan. ACM, New York, NY, USA, 6 pages. https://doi.org/10.\n1145/3539618.3592085\n1 INTRODUCTION\nScientific document classification aims to assign scientific literature\nto pre-defined categories, supporting various applications [5, 25, 40].\nRecently, pretrained language models (PTLMs) have demonstrated\nâˆ—Ran and Yue contributed equally to this research.\nâ€ Corresponding Author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSIGIR â€™23, July 23â€“27, 2023, Taipei, Taiwan\nÂ© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-9408-6/23/07. . . $15.00\nhttps://doi.org/10.1145/3539618.3592085\nLabelNamesRetrievedDocumentsUnlabeledDocumentsPseudo-labeledDocuments\nRankWord1 packet2 traffic3 protocol4 service5 routingâ€¦â€¦RankWord1 routing2 gateway3 architecture4 netconf5 virtualizationâ€¦â€¦\nnetworking and internet architectureroutingLocalGlobal\nClassifierInitialize\nRankWord1 routing2 packet3 traffic4 gateway5 protocolâ€¦â€¦\nStage-II:LabelNameExpansion\nStage-I:DenseRetrieval\nStage-III:Self-training !\"#$%&'$()*+,$=1,/\"0!+1,/\"0\"\nFigure 1: Framework of WanDeR.\nimpressive performance in document classification [1, 8]. However,\nthey often require a large number of annotations for fine-tuning,\nwhich restricts their deployment in real-world applications. While\npractitioners cannot afford to label many documents, it is often\neasier for them to provide category-descriptive label names asweak\nsupervision for each class [16, 32â€“34]. Motivated by this, we focus\non scientific document classification under the setting where only\nthe label name for each class as well as the unlabeled corpus are\navailable [17]. This task is challenging as the label names can be\nshort and succinct, often containing a few words only. How to mine\nclass-relevant knowledge with weak supervision is nontrivial.\nThere exist plenty of studies on automatic document categoriza-\ntion using class-relevant keywords only [16, 17, 22, 23, 31]. These\nmethods first leverage the keywords as input to extract relevant\ndocuments from the unlabeled data with hard matching. Then, they\nenlarge the set of keywords with masked language modeling [17]\nor embedding similarity [22], and use it to generate pseudo labels\nfor unlabeled data. Although these methods achieve competitive\nperformance, they mainly focus on tasks fromgeneral domains such\nas news and reviews. For these tasks, the keywords can be com-\nmonly used words (e.g. â€˜ Good/Badâ€™ for movie reviews), and they\ncan be matched with many examples from the unlabeled corpus.\nHowever, for scientific documents, the label names can either be too\ndomain-specific, or contain multiple concepts [7, 38]. As a result,\nthey often have limited coverage over the corpus, which causes\nperformance degradation when directly applying prior techniques\non weakly-supervised learning to the scientific domain (Sec. 2.2).\nIn this work, we proposeWanDeR(Weakly-supervised Scientific\nText Classification using Dense Retrieval), a multi-stage training\narXiv:2306.07193v1  [cs.CL]  12 Jun 2023\nSIGIR â€™23, July 23â€“27, 2023, Taipei, Taiwan Ran Xu, Yue Yu, Joyce C. Ho, and Carl Yang\nframework for weakly supervised text classification using dense\nretrieval (DR), as shown in Figure 1. In DR, both queries and docu-\nments are represented as dense vectors, and the relevance between\nthem is calculated via similarity metrics (e.g. dot product) [ 15].\nThis makes DR an ideal choice to tackle the above challenges, as\nit captures the semantics for different classes and circumvents the\nmismatch issue since some label names never appear in the corpus.\nTo incorporate DR into the framework, we regard label names as\nqueries, and retrieve the most relevant documents from the unla-\nbeled corpus for each class (Stage-I, Sec. 3.1). In this way, we create\nan initial set of pseudo-labeled documents, which can be used to\nfine-tune the PTLM for the target task.\nAlthough Stage-I is able to extract relevant documents, their\nperformance can be less satisfactory as label names are insufficient\nto capture all the class-specific information. To overcome this draw-\nback, in Stage-II, we expand the label names with the extracted\nkeywords using local and global information (Sec. 3.2). Specifically,\nwe first adopt the TF-IDF algorithm [12] on the retrieved documents\nto select the top-ranked words from the local corpus. Besides, as\nthe PTLM captures the generic linguistic knowledge, we use it to\ncalculate the embedding similarity between the candidate words\nand the label names as the global score. The local and global in-\nformation is connected via an ensemble ranking module, and we\naugment the label name for each class by selecting the word with\nthe highest score. The above expansion step is repeated multiple\ntimes to enrich the query [9] and help the DR model retrieve more\nrelevant documents from the corpus.\nTo leverage all unlabeled data to further improve the perfor-\nmance, an additional step is to harvest self-training [17, 26] (Stage-\nIII, Sec. 3.3) to refine the PTLM classifier by bootstrapping over\nhigh-confident examples and improve its generalization ability.\nWe verify the effectiveness of WanDeR by conducting exper-\niments on three datasets and show that our model outperforms\nthe previous weakly-supervised approaches by a large margin. Our\nanalysis further confirms the advantage of leveraging dense re-\ntrieval for tackling the limited coverage issue of label names as well\nas the efficacy of multi-stage training for improving performance.\n2 PRELIMINARIES\n2.1 Problem Definition\nOur weakly-supervised scientific document classification with ğ¶\nclasses is defined as follows. The input is a training corpus X=\b\nğ‘‘1,ğ‘‘2,...,ğ‘‘ |X|\n\t\nof documents without any labels. In addition, for\neach class ğ‘ (1 â‰¤ğ‘ â‰¤ğ¶), a label-specific name ğ‘¤ğ‘ is given, which\nconsists of one or a few words. We aim to learn a classifier ğ‘“(ğ‘¥; ğœƒ):\nXâ†’Y . Here Xdenotes all samples and Y= {1,2,Â·Â·Â· ,ğ¶}is the\nlabel set. While there exist works on multi-label classification or\nmetadata-aware classification [10, 37], we focus on the basic setting\nby assuming (1) each document only belongs to one category and\n(2) no other metadata information is available.\n2.2 Challenges for Scientific Text Classification\nWhile existing weakly supervised methods [17, 22] achieve com-\npetitive performance on general-domain datasets, applying them\ndirectly to scientific datasets often causes performance degrada-\ntion. To illustrate this, we use AGNews [35] as the general-domain\n70\n75\n80\n85\nPrecision\n1\n2\n3\n4\n5\nCoverage\nAGNews MeSH\n(a) Prec./Coverage (in %).\n0 1 2 3 4 5 6 7 8 9 100\n5\n10\n15\n20Coverage (in %) (b) Per-class Coverage.\nAGNews MeSH20\n40\n60\n80F1 Score (in %)\nFS X-Class LOTClass (c) Performance.\nFigure 2: Pilot Studies. FS means fully-supervised model.\ndataset and MeSH [5] as the scientific dataset. The average preci-\nsion (i.e. the portion of correctly matched examples) and coverage\n(i.e. the portion of examples that can be matched by label names)\nfor label names are shown in Figure 2a. We observe that for the\nscientific domain, the precision and coverage decline by 6% and 37%\nrespectively. Moreover, the results of per-class coverage (presented\nin Figure 2b) indicate that the label distribution is more imbalanced\nfor scientific data. For MeSH, there are 4 out of 11 classes where the\nlabel name cannot match any examples from the unlabeled corpus.\nThese two issues prevent the previous weakly-supervised models\n[17, 22] from performing well. As shown in Figure 2c, gaps to the\nperformance of the fully-supervised model are much larger for\nscientific datasets (36%) than general-domain datasets (8%), which\nindicates that these advanced techniques cannot resolve the unique\nchallenges that exist in the scientific domain.\n3 METHOD\nFrom the analysis in the above section, we conclude that it is neces-\nsary to propose techniques beyond hard matching to better harvest\nthe semantic label name information. Towards this goal, we present\nour framework WanDeRin Figure 1, a multi-stage training scheme\nbased on dense retrieval, to perform document classification using\nlabel names only. The three stages are detailed below.\n3.1 Stage-I: Dense Retrieval with Label Names\nDirectly using the label-indicative keywords to extract documents\nis sub-optimal for scientific documents, due to their limited cov-\nerage and inferior ability to capture the class-related semantics.\nMotivated by this, we propose to leverage dense retrieval (DR) [15]\nto effectively retrieve the most relevant documents. Specifically, DR\nrepresents the input information (â€œqueryâ€)ğ‘and target corpus (â€œdoc-\numentâ€) ğ‘‘ in the continuous embedding space as ğ‘”(ğ‘; ğœ™),ğ‘”(ğ‘‘; ğœ™)\nrespectively, whereğ‘”(Â·; ğœ™)is the dense retrieval model withğœ™being\nthe parameter of ğ‘”. Then, DR matches queries and documents via\napproximate nearest neighbor (ANN) using the relevance score\nğ‘Ÿ(ğ‘,ğ‘‘; ğœ™)= âŸ¨ğ‘”(ğ‘; ğœ™),ğ‘”(ğ‘‘; ğœ™)âŸ©, where âŸ¨Â·,Â·âŸ©is the dot product. Next,\nwe introduce the approach to train the DR model as well as leverage\nDR to extract documents from the corpus X.\nâ–¡ Task-adaptive DR Model Pretraining. To pretrain a DR model\nğ‘”(Â·; ğœ™)in an unsupervised fashion, we continuously pretrain the\nlanguage model on the corpus X, using the contrastive learning\nwidely adopted in recent research [11, 13, 30, 36]. Specifically, for\neach document ğ‘‘ğ‘– âˆˆX, we first split each document into multi-\nple sentences and randomly sample two sentences ğ‘‘ğ‘–,1,ğ‘‘ğ‘–,2 as the\nWeakly-Supervised Scientific Document Classification via Retrieval-Augmented Multi-Stage Training SIGIR â€™23, July 23â€“27, 2023, Taipei, Taiwan\npositive pair. The training objective of contrastive learning forğ‘‘ğ‘– is\nâ„“CL = âˆ’log exp \u0000ğœ Â·âŸ¨ğ‘”(ğ‘‘ğ‘–,1; ğœ™),ğ‘”(ğ‘‘ğ‘–,2; ğœ™)âŸ©\u0001\nÃ\nğ‘—=1,2\nÃ\nğ‘‘âˆ’âˆˆDâˆ’\nğ‘–\nexp\n\u0010\nğœ Â·âŸ¨ğ‘”(ğ‘‘ğ‘–,ğ‘—; ğœ™),ğ‘”(ğ‘‘âˆ’\nğ‘– ; ğœ™)âŸ©\n\u0011, (1)\nwhere ğ‘‘âˆ’\nğ‘– âˆˆD âˆ’\nğ‘– are the in-batch negatives, and ğœ = 0.01 is the\nparameter for temperature. Contrastive pretraining improves both\nthe alignment and uniformity for embeddings of sequences [14, 21,\n27, 28, 39], which can better support the retrieval task.\nâ–¡ Document Retrieval using Label Names. With the DR model,\nwe aim to extract an initial set of labeled data for each class by\nfeeding the label names (as queries) to the DR model. The initial\nretrieved document set Dğ‘– for the ğ‘–-th class can be written as\nDğ‘– = Top-ğ‘˜ANN\nğ‘‘âˆˆXğ‘Ÿ(ğ‘¤ğ‘–,ğ‘‘; ğœ™), (2)\nwhere ğ‘˜ is the number of retrieved examples, and the label of the\nretrieved document is determined by the category of the label name.\nIn this way, we get rid of the challenge brought by those infrequent\nlabel names and provide a flexible way to encode the label-related\nsemantics. All retrieved examples D= âˆªğ¶\nğ‘–=1Dğ‘– are then used for\nclassification, which will be discussed in the following part.\nâ–¡ Training Classifiers with Retrieved Text. With the retrieved\ndocument set D, one can simply finetune a classifier ğ‘“(Â·; ğœƒ)with\nthe standard cross-entropy loss:\nmin\nğœƒ\nE(ğ‘¥ğ‘– ,ğ‘¦ğ‘– )âˆˆD â„“CE (ğ‘“(ğ‘¥ğ‘–; ğœƒ),ğ‘¦ğ‘–). (3)\nThe fine-tuned model is used for target classification tasks.\n3.2 Stage-II: Expand Label Names with Local\nand Global Information\nOne drawback of the above stage is that the label names are often\ntoo abstract to fully represent the semantics information for classes.\nAs such, the retrieved documents still contain label noise, which\nhurts the downstream performance. To tackle this, we propose to\nautomatically extract class-related keywords to expand the label\nname, by using both local information from the retrieved documents\nand global information from the general pretrained models.\nâ–¡ Local Information for Keyword Extraction. To identify the\nclass-related keywords, we assume terms that appear frequently\nwithin documents from a specific class while infrequently for other\nclasses are more likely to be class-indicative words for that class.\nInspired by TF-IDF [12], we measure the indicativeness of word ğ‘¤\nfor class ğ‘ from the retrieved document Das\nğ¿ğ‘¤,ğ‘ = tfğ›¼\nğ‘¤,ğ‘ Â·log (1 +ğ´/tfğ‘¤)Â·cntğ‘¤,ğ‘ . (4)\nHere tfğ‘¤,ğ‘, cntğ‘¤,ğ‘ stands for the frequency and occurrence time of\nword ğ‘¤ within documents from class ğ‘and tfğ‘¤, is the frequency of\nğ‘¤in corpus, ğ´is the average number of words per class. In this way,\nwords appear commonly in the class-related documents (higher\ntfğ‘¤,ğ‘) while being less generic (lower tfğ‘¤) will receive higher score.\nFor each class, we extract ğ‘šwords with the highest score as the\ncandidate set C.1\nâ–¡ Global Information for Keyword Semantics. The above step\nonly considers the word occurrence in the local corpus, without\nmodeling the semantic information. An ideal keyword, however,\n1We omit words that already appeared in label names during the expansion (stage-II).\nshould also have a closer meaning to the label name. Motivated\nby this, we leverage the PTLM to transfer the global knowledge\nfrom pretraining corpora and encode the contextual information\nfor each word. We calculate the embeddings of both label names\nand candidate words by averaging the output of all tokens from\nthe last layer of PTLM â„(Â·;ğœ“). For word ğ‘¤ âˆˆC from the candidate\nset of class ğ‘, the global score is calculated between ğ‘¤ and the label\nname ğ‘¤ğ‘ using the embedding similarity as\nğºğ‘¤,ğ‘ = cos (â„(ğ‘¤;ğœ“),â„(ğ‘¤ğ‘;ğœ“)). (5)\nâ–¡ Ensemble Reranking. To effectively combine the local and\nglobal information, we sort candidate words ğ‘¤ âˆˆCğ‘– for ğ‘–-th class\nusing the score ğ¿ğ‘¤,ğ‘,ğºğ‘¤,ğ‘, respectively. Then, each word ğ‘¤ will\nhave two ranks asrankğ¿,ğ‘(ğ‘¤)and rankğº,ğ‘(ğ‘¤). We rerank the words\nusing the ensemble score based on Reciprocal Rank Fusion (RRF) [6]:\nscoreğ‘¤,ğ‘ = 1/rankğº,ğ‘(ğ‘¤)+1/rankğ¿,ğ‘(ğ‘¤). (6)\nFor each class, we add one word with the highest score to expand\nthe label name. For expansion, we simply concatenate the previous\nlabel name and the newly identified word for enrichment [2].\nâ–¡ Iterative Label Name Expansion. The above process can be\nconducted multiple times. In each iteration, we first use local and\nglobal scores to detect the expanded words using Eq. (4)â€“(6) and\nenrich the label names. Then, we use the expanded label names as\nqueries to update the retrieved documents Dwith Eq. (2) as we\nexpect the quality of Dwill improve by incorporating additional\nclass-indicative words. With the updated D, more relevant words\ncan be extracted to enrich the class information. The above iteration\nis repeated 5 times, and the retrieved documents after the final\niteration can be used to train another classifier using Eq. (3).\n3.3 Stage-III: Refine Classifier with Self-training\nThe pseudo-labeled samples in Stage-II are only from the top re-\ntrieved documents with the expanded label names. To generalize\nits current knowledge to the whole unlabeled corpus, self-training\nis adopted to bootstrap the model on the entire unlabeled cor-\npus [17, 29] as\nmin\nğœƒ\nE(ğ‘¥,eğ‘¦)âˆˆX 1\nn\n[ğ‘“(ğ‘¥; ğœƒ)]eğ‘¦ > ğ›¾\no\nÃ—â„“CE (ğ‘“(ğ‘¥; ğœƒ),eğ‘¦), (7)\nwhere eğ‘¦ = argmax ğ‘“(ğ‘¥; ğœƒ)is the hard pseudo label, ğ›¾ is the con-\nfidence threshold. With self-training, the model is refined by its\nhigh-confident predictions to improve generalization ability. Stage-\nIII stops when less than 1% of samples change their labels, and the\nfinal model can be used to classify any document.\n4 EXPERIMENTS\n4.1 Experiment Setups\nâ–¡ Datasets. We conduct experiments on three datasets from mul-\ntiple domains including MeSH [5], arXiv-CS [4], arXiv-Math [4].\nThe statistics and label names for each dataset are shown in Table 1\nand 2. For arXiv-CS and arXiv-Math, we select papers from years\n2017-2020 as the training set, 2021-2022 as the test set, and use the\ntopic from the main category as the label.\nâ–¡ Baselines. We compareWanDeRwith these baselines: (1)IR [20]\nleverages TF-IDF to assign labels for documents. (2) Dataless [3]\nuses Wikipedia to embed labels and documents. Each document is\nSIGIR â€™23, July 23â€“27, 2023, Taipei, Taiwan Ran Xu, Yue Yu, Joyce C. Ho, and Carl Yang\nTable 1: Label Name Information. Label names in blue never appear in the corpus.\nDataset Label Names\nMeSH Cardiovascular diseases, Chronic kidney disease, HIV/AIDS, Diabetes (mellitus), Chronic respiratory diseases, Digestive diseases, Hepatitis A/B/C/E,\nMental disorders, Musculoskeletal disorders, Neoplasms (cancer), Neurological disorders\narXiv-MathNumerical Analysis, Algebraic Geometry, Functional spaces, Number Theory, Complex Variable , Differential Geometry, Combinatorics, Operator Algebra,\nRepresentation Theory, Statistics Theory, Topological Geometry, Rings and Algebra, Probability , Dynamical System, Optimization and Control, Logic\narXiv-CS\nDatabase, Computation and Language, Information Theory, Computational Geometry, Cryptography and Security, System and Control, Game Theory,\nData Structures and Algorithm, Human-Computer Interaction, Machine Learning, Information Retrieval, Programming Languages, Software Engineering,\nNetworking and Internet Architecture, Artificial Intelligence, Social and Information Networks, Distributed, Parallel, and Cluster Computing, Robotics,\nComputer Vision and Pattern Recognition, Logic in Computer Science\nTable 2: Dataset statistics.\nDataset Domain # Train # Test # Class # OOV Avg. Len.\nMeSH BioMedical 16.3k 3.5k 11 4 (36%) 254.3\narXiv-Math Mathematics 62.5k 6.3k 16 3 (19%) 214.4\narXiv-CS Computer Science 75.7k 5.1k 20 5 (25%) 188.2\nTable 3: Performance on three datasets. Bold and blue indi-\ncate the best and second-best results for each dataset. Macro-\nF1 is the main metric as the label distribution is imbalanced.\nMethod MeSH arXiv-Math arXiv-CS\nMi-F1 Ma-F1 Mi-F1 Ma-F1 Mi-F1 Ma-F1\nFully Supervised 90.5Â±0.3 90.3Â±0.2 80.6Â±0.4 79.1Â±0.3 83.0Â±0.2 78.2Â±0.4\nIR [20] 40.6 37.6 27.8 22.9 24.5 22.8\nDataless [3] 36.1 26.8 18.9 13.4 20.5 18.2\nSentenceBERT [18] 68.6 66.0 48.9 41.1 50.7 47.7\nLOTClass [17] 57.9 Â±1.7 44.9Â±1.6 43.8Â±2.0 35.2Â±1.5 51.5Â±1.4 47.1Â±1.8\nX-Class [22] 55.2 Â±1.4 54.4Â±1.8 46.5Â±1.4 39.1Â±1.4 60.6Â±1.2 51.6Â±1.3\nFastClass [24] 78.5 Â±1.3 78.1Â±1.1 53.5Â±1.3 44.5Â±1.2 59.8Â±0.8 50.5Â±0.9\nWanDeR 82.0Â±0.4 81.9Â±0.4 58.0Â±0.8 51.9Â±0.7 65.6Â±0.8 58.9Â±0.6\nGainÎ” 3.5(4.4%) 3.8(4.9%) 4.5(8.4%) 7.4(16.6%) 5.0(8.2%) 7.3(14.1%)\nWanDeR(Stage-I) 76.6Â±1.0 75.6Â±0.8 56.4Â±1.4 49.8Â±0.9 61.8Â±1.1 54.7Â±1.2\nWanDeR(Stage-II) 79.9Â±0.6 80.2Â±0.7 57.1Â±1.1 51.0Â±1.0 64.6Â±1.0 58.1Â±0.6\nclassified to the label with the highest similarity. (3)SentenceBERT\n[18] is trained on NLI data to embed labels and documents for clas-\nsification. (4) LOTClass [17] and (5) X-Class [22] are two meth-\nods that use PTLMs for label-name-only text classification by us-\ning masked language modeling or pretrained representations. (6)\nFastClass [24] uses SentenceBERT to extract initial labeled exam-\nples, then selects an optimal subset for classifier training.\nâ–¡ Implementations. We use the pre-trained SciBERT [1] as the\nbackbone for WanDeRand baselines. The retrieval modelğ‘”(Eq. (1))\nand PTLM â„ (Eq. (5)) are initialized from SciBERT, and ğ‘” is pre-\ntrained on the corpus Xfor 5 epochs. Note that to avoid infor-\nmation leakage, only the training set is used for pretraining . The\nmaximum length is set to 512. For Stage-I and II, we finetuneğ‘“(Â·; ğœƒ)\nfor 5 epochs with Adam as the optimizer and set the batch size\nand learning rate to 32 and 2e-5. Other hyperparameters include\nğœ in Eq. (1), ğ‘˜ for ANN in Eq. (2), ğ›¾ in Eq. (7), ğ‘šin Sec. 3.2. We set\nğœ = 0.01,ğ‘š = 100,ğ‘˜ = 100,ğ›¾ = 0.8 without tuning.\n4.2 Experiment Results\nâ–¡ Main Experiments. We report both Macro-F1 and Micro-F1\nscores for WanDeRand baselines in Table 3. The mean and variance\nover 5 runs are calculated when fine-tuning is used. We observe\nthat WanDeRconsistently achieves the best performance on three\ndatasets, with an average gain of 11.9%. In contrast, X-Class and\nLOTClass, which achieve strong results on general-domain tasks,\n20\n40\n60\n80Macro F1\nMeSH\n20\n30\n40\n50\n60\narXiv-CS\nSciBERT\nBM25\nContriever\nWANDER\n(a) Study of Retrievers\n25 50 100 150 20060\n65\n70\n75\n80\n85Macro F1\nBest baseline (MeSH)\n50\n55\n60\n65\n70\n75\nBest baseline (arXiv-CS)\nMeSH\narXiv-CS (b) Study on ğ‘˜\n0.5 0.6 0.7 0.8 0.9\n76\n78\n80\n82Macro F1\nw/o stage-III (MeSH)\n58\n60\n62\n64\nw/o stage-III (arXiv-CS)\nMeSH\narXiv-CS (c) Study on ğ›¾\nFigure 3: Studies of Different Retrieval Models and Hyperpa-\nrameters (Best View in Colors).\n0 1 2 3 4 5\nIterations\n70\n80\n90Acc. of Pseudo Labels\nBest baseline (MeSH)\nBest baseline (arXiv-CS)\nMeSH, w/o Local\nMeSH, w/o Global\nMeSH, WANDER\narXiv-CS, w/o Local\narXiv-CS, w/o Global\narXiv-CS, WANDER\n(a) Pseudo Label Accuracy\n0 1 2 3 4 5\nIterations\n60\n70\n80F1 Score\nBest baseline (MeSH)\nBest baseline (arXiv-CS)\nMeSH, w/o Local\nMeSH, w/o Global\nMeSH, WANDER\narXiv-CS, w/o Local\narXiv-CS, w/o Global\narXiv-CS, WANDER (b) Macro-F1 Score\nFigure 4: Study on Effects of Local and Global Information.\nfail to perform well on the scientific domain, as they cannot handle\nthe challenges mentioned in Sec. 2.2. Moreover, traditional base-\nlines, such as IR, and Dataless, are inferior to other methods using\nPTLMs, indicating their limited ability for modeling scientific text.\nAlthough SentenceBERT and FastClass use extra labeled data for\nembedding learning, distribution shifts exist between the labeled\ndata and scientific corpus. They also fail to expand the label names\nfor enriching representations, leading to sub-optimal performance.\nâ–¡ Effect of Multi-stage Training. The bottom two rows in Ta-\nble 3 show the performance of WanDeRafter Stage-I and II, which\njustifies that all three stages contribute to the final performance.\nMoreover, WanDeR outperforms all baselines even without self-\ntraining (Stage-III), indicating that it can retrieve a small set of\nhigh-quality data to support downstream tasks sufficiently.\n4.3 Ablation and Hyperparameter Studies\nâ–¡ Study of DR Models. To illustrate the effect of task-adaptive con-\ntrastive learning (TAPT) for DR model pretraining, we substitute\nğ‘”(Â·)with other models including BM25 [19], SciBERT [1] without\nTAPT, the strong unsupervised DR modelContriever [13], and com-\npare the performance in Figure 3a. Overall, our model achieves the\nWeakly-Supervised Scientific Document Classification via Retrieval-Augmented Multi-Stage Training SIGIR â€™23, July 23â€“27, 2023, Taipei, Taiwan\nTable 4: Case Study on expanded keywords for three tasks.\nDataset Class Expanded Keyword\nMeSH Diabetes insulin, glucose, diabetic, metformin, glycemicMeSH Mental Disorders depression, anxiety, antidepressant, schizophrenia, moodMeSH Neoplasms tumor, carcinoma, cell, tumour, chemotherapy\narXiv-Math Combinatorics graph, combinatorial, vertex, edge, bipartitearXiv-Math Number theory prime, integer, modulo, odd, divisorarXiv-Math Statistics theory estimation, sample, regression, treatment, inference\narXiv-CS Information theory entropy, channel, shannon, capacity, decoderarXiv-CS Machine Learning classifier, classification, boosting, ensemble, treearXiv-CS Game Theory player, equilibrium, nash, payoff, strategy\nbest performance, which justifies the need for TAPT as it effectively\nreduces the distribution shifts and also produces better embeddings.\nInstead, using sparse retrieval model (BM25) yields undesirable\nperformance as it cannot understand label names well.\nâ–¡ Effect of Hyperparameters. We study the effect of ğ‘˜ and ğ›¾ in\nWanDeRon MeSH and arXiv-CS, as shown in Figure 3b and 3c. We\nobserve that the performance first increases with larger ğ‘˜ as the\nmodel benefits from more retrieved examples. When ğ‘˜ reaches 100,\nthe performance remains stable, as too many retrieved examples\nintroduce label noise and diminish the performance gain. We also\nrun experiments with different thresholds ğ›¾. The result indicates\nthat the model performance is insensitive toğ›¾, and the self-training\ncomponent leads to performance gain in most studied regions.\nâ–¡ Effect of Local and Global Information. Figure 4 illustrates\nthe performance of WanDeRand its variants over 5 expansion itera-\ntions. Overall, we observe that removing local or global information\nhurts the performance, since these two modules provide comple-\nmentary information. Combining these two terms together results\nin better pseudo labels and improves downstream performance.\n4.4 Case Studies\nWe present a case study in Table 4 to showcase that WanDeR\nis able to discover class-related keywords to expand label names.\nTake diabetes as an example, it is often related to high glucose\nlevel and glycemic index. Besides, insulin and metformin are used as\ntreatments for diabetes. Moreover, takemachine learning as another\nexample, it is applied to classifiation tasks. Boosting, ensemble, tree\nare all techniques to tackle machine learning problems. These all\nindicate that WanDeRcan enrich the semantics of label names.\n5 CONCLUSION\nWe proposeWanDeR, a multi-stage training framework for weakly-\nsupervised scientific document classification with label name only.\nWe leverage dense retrieval to go beyond hard matching and har-\nness the semantics of label names. In addition, we propose a label\nname expansion module to enrich its representations, and use self-\ntraining to improve the modelâ€™s generalization ability. Experiments\non three datasets demonstrate that WanDeRoutperforms the base-\nlines by 11.9% on average. For future works, we plan to extend\nWanDeRto other scenarios such as multi-label classification.\nACKNOWLEDGEMENTS\nWe thank the anonymous reviewers and area chairs for the valuable\nfeedbacks. This research was partially supported by the internal\nfunds and GPU servers provided by the Computer Science De-\npartment of Emory University. JH was supported by NSF grants\nIIS-1838200 and IIS-2145411.\nREFERENCES\n[1] Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A Pretrained Language\nModel for Scientific Text. In EMNLP-IJCNLP. 3615â€“3620.\n[2] Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and Stephen Robertson. 2008. Selecting\ngood expansion terms for pseudo-relevance feedback. In SIGIR. 243â€“250.\n[3] Ming-Wei Chang, Lev-Arie Ratinov, Dan Roth, and Vivek Srikumar. 2008. Impor-\ntance of Semantic Representation: Dataless Classification.. In AAAI. 830â€“835.\n[4] Colin B Clement, Matthew Bierbaum, Kevin P Oâ€™Keeffe, and Alexander A Alemi.\n2019. On the Use of ArXiv as a Dataset. arXiv preprint arXiv:1905.00075 (2019).\n[5] Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S Weld.\n2020. SPECTER: Document-level Representation Learning using Citation-\ninformed Transformers. In ACL. 2270â€“2282.\n[6] Gordon V Cormack, Charles LA Clarke, and Stefan Buettcher. 2009. Reciprocal\nrank fusion outperforms condorcet and individual rank learning methods. In\nSIGIR. 758â€“759.\n[7] Hejie Cui, Jiaying Lu, Yao Ge, and Carl Yang. 2022. How Can Graph Neural\nNetworks Help Document Retrieval: A Case Study on CORD19 with Concept\nMap Generation. In ECIR.\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nNAACL-HLT.\n[9] Fernando Diaz, Bhaskar Mitra, and Nick Craswell. 2016. Query Expansion with\nLocally-Trained Word Embeddings. InACL.\n[10] Soumyajit Ganguly and Vikram Pudi. 2017. Paper2vec: Combining graph and\ntext information for scientific paper representation. In ECIR. 383â€“395.\n[11] Luyu Gao and Jamie Callan. 2022. Unsupervised Corpus Aware Language Model\nPre-training for Dense Passage Retrieval. In ACL. 2843â€“2853.\n[12] Maarten Grootendorst. 2022. BERTopic: Neural topic modeling with a class-based\nTF-IDF procedure. arXiv preprint arXiv:2203.05794 (2022).\n[13] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-\njanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised Dense Infor-\nmation Retrieval with Contrastive Learning. TMLR (2022).\n[14] Xuan Kan, Hejie Cui, Joshua Lukemire, Ying Guo, and Carl Yang. 2022. Fbnetgen:\nTask-aware gnn-based fmri analysis via functional brain network generation. In\nMIDL.\n[15] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey\nEdunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-\nDomain Question Answering. In EMNLP. 6769â€“6781.\n[16] Yu Meng, Jiaming Shen, Chao Zhang, and Jiawei Han. 2018. Weakly-supervised\nneural text classification. In CIKM. 983â€“992.\n[17] Yu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong, Heng Ji, Chao Zhang, and\nJiawei Han. 2020. Text classification using label names only: A language model\nself-training approach. EMNLP (2020).\n[18] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings\nusing Siamese BERT-Networks. In EMNLP-IJCNLP. 3982â€“3992.\n[19] Stephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance frame-\nwork: BM25 and beyond. Foundations and Trends in Information Retrieval 3, 4\n(2009), 333â€“389.\n[20] Bruno Trstenjak, Sasa Mikac, and Dzenana Donko. 2014. KNN with TF-IDF based\nframework for text categorization. Procedia Engineering 69 (2014), 1356â€“1364.\n[21] Tongzhou Wang and Phillip Isola. 2020. Understanding contrastive representation\nlearning through alignment and uniformity on the hypersphere. In ICML.\n[22] Zihan Wang, Dheeraj Mekala, and Jingbo Shang. 2021. X-Class: Text Classification\nwith Extremely Weak Supervision. In NAACL. 3043â€“3053.\n[23] Zihan Wang, Tianle Wang, Dheeraj Mekala, and Jingbo Shang. 2023. A Benchmark\non Extremely Weakly Supervised Text Classification: Reconcile Seed Matching\nand Prompting Approaches. arXiv preprint arXiv:2305.12749 (2023).\n[24] Tingyu Xia, Yue Wang, Yuan Tian, and Yi Chang. 2022. FastClass: A Time-Efficient\nApproach to Weakly-Supervised Text Classification. EMNLP (2022).\n[25] Yi Xie, Yuqing Sun, and Elisa Bertino. 2021. Learning domain semantics and\ncross-domain correlations for paper recommendation. In SIGIR. 706â€“715.\n[26] Ran Xu, Yue Yu, Hejie Cui, Xuan Kan, Yanqiao Zhu, Joyce Ho, Chao Zhang, and\nCarl Yang. 2023. Neighborhood-Regularized Self-Training for Learning with Few\nLabels. In AAAI, Vol. 37.\n[27] R. Xu, Y. Yu, C. Zhang, M. K Ali, JC. Ho, and C. Yang. 2022. Counterfactual and\nfactual reasoning over hypergraphs for interpretable clinical predictions on ehr.\nIn Machine Learning for Health . PMLR, 259â€“278.\n[28] Yi Yang, Hejie Cui, and Carl Yang. 2022. Pre-train Graph Neural Networks for\nBrain Network Analysis. In IEEE-Big Data .\n[29] Yue Yu, Lingkai Kong, Jieyu Zhang, Rongzhi Zhang, and Chao Zhang. 2022.\nAcTune: Uncertainty-Based Active Self-Training for Active Fine-Tuning of Pre-\ntrained Language Models. In NAACL. 1422â€“1436.\n[30] Yue Yu, Chenyan Xiong, Si Sun, Chao Zhang, and Arnold Overwijk. 2022. COCO-\nDR: Combating Distribution Shifts in Zero-Shot Dense Retrieval with Contrastive\nand Distributionally Robust Learning. In EMNLP. 1462â€“1479.\nSIGIR â€™23, July 23â€“27, 2023, Taipei, Taiwan Ran Xu, Yue Yu, Joyce C. Ho, and Carl Yang\n[31] Yue Yu, Simiao Zuo, Haoming Jiang, Wendi Ren, Tuo Zhao, and Chao Zhang. 2021.\nFine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-\nRegularized Self-Training Approach. In NAACL. 1063â€“1077.\n[32] Jieyu Zhang, Yue Yu, Yinghao Li, Yujing Wang, Yaming Yang, Mao Yang, and\nAlexander Ratner. 2021. WRENCH: A Comprehensive Benchmark for Weak\nSupervision. In NeurIPS.\n[33] Rongzhi Zhang, Rebecca West, Xiquan Cui, and Chao Zhang. 2022. Adaptive\nMulti-view Rule Discovery for Weakly-Supervised Compatible Products Predic-\ntion. In KDD. 4521â€“4529.\n[34] Rongzhi Zhang, Yue Yu, Pranav Shetty, Le Song, and Chao Zhang. 2022. PRBoost:\nPrompt-Based Rule Discovery and Boosting for Interactive Weakly-Supervised\nLearning. In ACL.\n[35] Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level Convolu-\ntional Networks for Text Classification. In NIPS.\n[36] Yu Zhang, Hao Cheng, Zhihong Shen, Xiaodong Liu, Ye-Yi Wang, and Jianfeng\nGao. 2023. Pre-training Multi-task Contrastive Learning Models for Scientific\nLiterature Understanding. arXiv preprint arXiv:2305.14232 (2023).\n[37] Yu Zhang, Bowen Jin, Qi Zhu, Yu Meng, and Jiawei Han. 2023. The Effect of\nMetadata on Scientific Literature Tagging: A Cross-Field Cross-Model Study. In\nWWW. 1626â€“1637.\n[38] Yu Zhang, Yu Meng, Xuan Wang, Sheng Wang, and Jiawei Han. 2022. Seed-Guided\nTopic Discovery with Out-of-Vocabulary Seeds. InNAACL. 279â€“290.\n[39] Yanqiao Zhu, Yichen Xu, Hejie Cui, Carl Yang, Qiang Liu, and Shu Wu. 2022.\nStructure-enhanced heterogeneous graph contrastive learning. In SDM.\n[40] Yuchen Zhuang, Yinghao Li, Junyang Zhang, Yue Yu, Yingjun Mou, Xiang Chen,\nLe Song, and Chao Zhang. 2022. ReSel: N-ary Relation Extraction from Scientific\nText and Tables by Learning to Retrieve and Select. InEMNLP. 730â€“744.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7449849247932434
    },
    {
      "name": "Stage (stratigraphy)",
      "score": 0.6049351096153259
    },
    {
      "name": "Training (meteorology)",
      "score": 0.5865249037742615
    },
    {
      "name": "Information retrieval",
      "score": 0.5291006565093994
    },
    {
      "name": "Document classification",
      "score": 0.495364785194397
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4806995987892151
    },
    {
      "name": "Co-training",
      "score": 0.4761490821838379
    },
    {
      "name": "Training set",
      "score": 0.4458777606487274
    },
    {
      "name": "Semi-supervised learning",
      "score": 0.16963598132133484
    },
    {
      "name": "Meteorology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}