{
  "title": "Heterogeneous Graph Transformer",
  "url": "https://openalex.org/W3010021337",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3161703423",
      "name": "Hu, Ziniu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2584817349",
      "name": "Dong, Yuxiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3176362604",
      "name": "Wang Kuansan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2004370211",
      "name": "Sun, Yizhou",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2786915849",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2098711168",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2624431344",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2604314403",
    "https://openalex.org/W1975563293",
    "https://openalex.org/W2610002097",
    "https://openalex.org/W1973435495",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W2963581908",
    "https://openalex.org/W1932742904",
    "https://openalex.org/W2965857891",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2085814658",
    "https://openalex.org/W2970066309",
    "https://openalex.org/W103340358",
    "https://openalex.org/W2911286998",
    "https://openalex.org/W2743104969",
    "https://openalex.org/W2022322548",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2918342466",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W2952205826",
    "https://openalex.org/W2983466427"
  ],
  "abstract": "Recent years have witnessed the emerging success of graph neural networks (GNNs) for modeling structured data. However, most GNNs are designed for homogeneous graphs, in which all nodes and edges belong to the same types, making them infeasible to represent heterogeneous structures. In this paper, we present the Heterogeneous Graph Transformer (HGT) architecture for modeling Web-scale heterogeneous graphs. To model heterogeneity, we design node- and edge-type dependent parameters to characterize the heterogeneous attention over each edge, empowering HGT to maintain dedicated representations for different types of nodes and edges. To handle dynamic heterogeneous graphs, we introduce the relative temporal encoding technique into HGT, which is able to capture the dynamic structural dependency with arbitrary durations. To handle Web-scale graph data, we design the heterogeneous mini-batch graph sampling algorithm---HGSampling---for efficient and scalable training. Extensive experiments on the Open Academic Graph of 179 million nodes and 2 billion edges show that the proposed HGT model consistently outperforms all the state-of-the-art GNN baselines by 9%--21% on various downstream tasks.",
  "full_text": "Heterogeneous Graph Transformer\nZiniu Hu∗\nUniversity of California, Los Angeles\nbull@cs.ucla.edu\nYuxiao Dong\nMicrosoft Research, Redmond\nyuxdong@microsoft.com\nKuansan Wang\nMicrosoft Research, Redmond\nkuansanw@microsoft.com\nYizhou Sun\nUniversity of California, Los Angeles\nyzsun@cs.ucla.edu\nABSTRACT\nRecent years have witnessed the emerging success of graph neu-\nral networks (GNNs) for modeling structured data. However, most\nGNNs are designed for homogeneous graphs, in which all nodes\nand edges belong to the same types, making them infeasible to\nrepresent heterogeneous structures. In this paper, we present the\nHeterogeneous Graph Transformer (HGT) architecture for mod-\neling Web-scale heterogeneous graphs. To model heterogeneity,\nwe design node- and edge-type dependent parameters to charac-\nterize the heterogeneous attention over each edge, empowering\nHGT to maintain dedicated representations for different types of\nnodes and edges. To handle dynamic heterogeneous graphs, we in-\ntroduce the relative temporal encoding technique into HGT, which\nis able to capture the dynamic structural dependency with arbitrary\ndurations. To handle Web-scale graph data, we design the hetero-\ngeneous mini-batch graph sampling algorithm—HGSampling—for\nefficient and scalable training. Extensive experiments on the Open\nAcademic Graph of 179 million nodes and 2 billion edges show\nthat the proposed HGT model consistently outperforms all the\nstate-of-the-art GNN baselines by 9%–21% on various downstream\ntasks. The dataset and source code of HGT are publicly available at\nhttps://github.com/acbull/pyHGT.\nKEYWORDS\nGraph Neural Networks; Heterogeneous Information Networks;\nRepresentation Learning; Graph Embedding; Graph Attention\nACM Reference Format:\nZiniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. 2020. Hetero-\ngeneous Graph Transformer. In Proceedings of The Web Conference 2020\n(WWW ’20), April 20–24, 2020, Taipei, Taiwan. ACM, New York, NY, USA,\n11 pages. https://doi.org/10.1145/3366423.3380027\n1 INTRODUCTION\nHeterogeneous graphs have been commonly used for abstracting\nand modeling complex systems, in which objects of different types\n∗This work was done when Ziniu was an intern at Microsoft Research.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nWWW ’20, April 20–24, 2020, Taipei, Taiwan\n© 2020 Association for Computing Machinery.\nACM ISBN 978-1-4503-7023-3/20/04.\nhttps://doi.org/10.1145/3366423.3380027\nFigure 1: The schema and meta relations of Open Academic\nGraph (OAG). Given a Web-scale heterogeneous graph, e.g., an\nacademic network, HGT takes only its one-hop edges as input\nwithout manually designing meta paths.\ninteract with each other in various ways. Some prevalent instances\nof such systems include academic graphs, Facebook entity graph,\nLinkedIn economic graph, and broadly the Internet of Things net-\nwork. For example, the Open Academic Graph (OAG) [28] in Figure\n1 contains five types of nodes: papers, authors, institutions, venues\n(journal, conference, or preprint), and fields, as well as different\ntypes of relationships between them.\nOver the past decade, a significant line of research has been ex-\nplored for mining heterogeneous graphs [17]. One of the classical\nparadigms is to define and use meta paths to model heterogeneous\nstructures, such as PathSim [18] and metapath2vec [3]. Recently,\nin view of graph neural networks’ (GNNs) success [7, 9, 22], there\nare several attempts to adopt GNNs to learn with heterogeneous\nnetworks [14, 23, 26, 27]. However, these works face several issues:\nFirst, most of them involve the design of meta paths for each type of\nheterogeneous graphs, requiring specific domain knowledge; Sec-\nond, they either simply assume that different types of nodes/edges\nshare the same feature and representation space or keep distinct\nnon-sharing weights for either node type or edge type alone, mak-\ning them insufficient to capture heterogeneous graphs’ properties;\nThird, most of them ignore the dynamic nature of every (hetero-\ngeneous) graph; Finally, their intrinsic design and implementation\nmake them incapable of modeling Web-scale heterogeneous graphs.\nTake OAG for example: First, the nodes and edges in OAG could\nhave different feature distributions, e.g., papers have text features\nwhereas institutions may have features from affiliated scholars, and\ncoauthorships obviously differ from citation links; Second, OAG\nhas been consistently evolving, e.g., 1) the volume of publications\ndoubles every 12 years [4], and 2) the KDD conference was more\nrelated to database in the 1990s whereas more to machine learning\nin recent years; Finally, OAG contains hundreds of millions of nodes\narXiv:2003.01332v1  [cs.LG]  3 Mar 2020\nWWW ’20, April 20–24, 2020, Taipei, Taiwan Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun\nand billions of relationships, leaving existing heterogeneous GNNs\nnot scalable for handling it.\nIn light of these limitations and challenges, we propose to study\nheterogeneous graph neural networks with the goal of maintaining\nnode- and edge-type dependent representations, capturing network\ndynamics, avoiding customized meta paths, and being scalable to\nWeb-scale graphs. In this work, we present the Heterogeneous\nGraph Transformer (HGT) architecture to deal with all these issues.\nTo handle graph heterogeneity, we introduce the node- and edge-\ntype dependent attention mechanism. Instead of parameterizing\neach type of edges, the heterogeneous mutual attention in HGT is\ndefined by breaking down each edge e = (s, t)based on its meta\nrelation triplet, i.e., ⟨node type of s, edge type of e between s &\nt, node type of t⟩. Figure 1 illustrates the meta relations of hetero-\ngeneous academic graphs. In specific, we use these meta relations\nto parameterize the weight matrices for calculating attention over\neach edge. As a result, nodes and edges of different types are al-\nlowed to maintain their specific representation spaces. Meanwhile,\nconnected nodes in different types can still interact, pass, and aggre-\ngate messages without being restricted by their distribution gaps.\nDue to the nature of its architecture, HGT can incorporate informa-\ntion from high-order neighbors of different types through message\npassing across layers, which can be regarded as “soft” meta paths.\nThat said, even if HGT take only its one-hop edges as input without\nmanually designing meta paths, the proposed attention mechanism\ncan automatically and implicitly learn and extract “meta paths” that\nare important for different downstream tasks.\nTo handle graph dynamics, we enhance HGT by proposing the\nrelative temporal encoding (RTE) strategy. Instead of slicing the\ninput graph into different timestamps, we propose to maintain all\nthe edges happening in different times as a whole, and design the\nRTE strategy to model structural temporal dependencies with any\nduration length, and even with unseen and future timestamps. By\nend-to-end training, RTE enables HGT to automatically learn the\ntemporal dependency and evolution of heterogeneous graphs.\nTo handle Web-scale graph data, we design the first hetero-\ngeneous sub-graph sampling algorithm—HGSampling—for mini-\nbatch GNN training. Its main idea is to sample heterogeneous sub-\ngraphs in which different types of nodes are with similar propor-\ntions, since the direct usage of existing (homogeneous) GNN sam-\npling methods, such as GraphSage [7], FastGCN [1], and LADIES [29],\nresults in highly imbalanced ones regarding to both node and edge\ntypes. In addition, it is also designed to keep the sampled sub-graphs\ndense for minimizing the loss of information. With HGSampling,\nall the GNN models, including our proposed HGT, can train and\ninfer on arbitrary-size heterogeneous graphs.\nWe demonstrate the effectiveness and efficiency of the proposed\nHeterogeneous Graph Transformer on the Web-scale Open Aca-\ndemic Graph comprised of 179 million nodes and 2 billion edges\nspanning from 1900 to 2019, making this the largest-scale and\nlongest-spanning representation learning yet performed on hetero-\ngeneous graphs. Additionally, we also examine it on domain-specific\ngraphs: the computer science and medicine academic graphs. Exper-\nimental results suggest that HGT can significantly improve various\ndownstream tasks over state-of-the-art GNNs as well as dedicated\nheterogeneous models by 9–21%. We further conduct case studies\nto show the proposed method can indeed automatically capture the\nimportance of implicit meta paths for different tasks.\n2 PRELIMINARIES AND RELATED WORK\nIn this section, we introduce the basic definition of heteroge-\nneous graphs with network dynamics and review the recent devel-\nopment on graph neural networks (GNNs) and their heterogeneous\nvariants. We also highlight the difference between HGT and existing\nattempts on heterogeneous graph neural networks.\n2.1 Heterogeneous Graph Mining\nHeterogeneous graphs [17] (a.k.a., heterogeneous information\nnetworks) are an important abstraction for modeling relational data\nfor many real-world complex systems. Formally, it is defined as:\nDefinition 1. Heterogeneous Graph: A heterogeneous graph\nis defined as a directed graph G = (V, E, A, R)where each node\nv ∈V and each edge e ∈E are associated with their type mapping\nfunctions τ(v): V →A and ϕ(e): E →R, respectively.\nMeta Relation. For an edge e = (s, t)linked from source node s to\ntarget node t, its meta relation is denoted as ⟨τ(s), ϕ(e), τ(t)⟩. Natu-\nrally, ϕ(e)−1 represents the inverse of ϕ(e). The classical meta path\nparadigm [17–19] is defined as a sequence of such meta relation.\nNotice that, to better model real-world heterogeneous networks,\nwe assume that there may exist multiple types of relations between\ndifferent types of nodes. For example, in OAG there are different\ntypes of relations between the author and paper nodes by consid-\nering the authorship order, i.e., “the first author of”, “the second\nauthor of”, and so on.\nDynamic Heterogeneous Graph. To model the dynamic nature\nof real-world (heterogeneous) graphs, we assign an edge e = (s, t)\na timestamp T , when node s connects to node t at T . If s appears\nfor the first time, T is also assigned to s. s can be associated with\nmultiple timestamps if it builds connections over time.\nIn other words, we assume that the timestamp of an edge is\nunchanged, denoting the time it is created. For example, when a\npaper published on a conference at time T , T will be assigned to\nthe edge between the paper and conference nodes. On the contrary,\ndifferent timestamps can be assigned to a node accordingly. For\nexample, the conference node “WWW” can be assigned any year.\nWWW @1994 means that we are considering the first edition of\nWWW, which focuses more on internet protocol and Web infras-\ntructure, while WWW @2020 means the upcoming WWW, which\nexpands its research topics to social analysis, ubiquitous computing,\nsearch & IR, privacy and society, etc.\nThere have been significant lines of research on mining heteroge-\nnous graphs, such as node classification, clustering, ranking and\nrepresentation learning [3, 17–19], while the dynamic perspective\nof HGs has not been extensively explored and studied.\n2.2 Graph Neural Networks\nRecent years have witnessed the success of graph neural net-\nworks for relational data [7, 9, 22]. Generally, a GNN can be regarded\nas using the input graph structure as the computation graph for\nHeterogeneous Graph Transformer WWW ’20, April 20–24, 2020, Taipei, Taiwan\nmessage passing [6], during which the local neighborhood informa-\ntion is aggregated to get a more contextual representation. Formally,\nit has the following form:\nDefinition 2. General GNN Framework: Suppose Hl [t]is the\nnode representation of node t at the (l)-th GNN layer, the update\nprocedure from the (l-1)-th layer to the (l)-th layer is:\nHl [t]← Aggregate\n∀s ∈N (t), ∀e ∈E(s, t)\n\u0012\nExtract\n\u0010\nHl−1[s]; Hl−1[t], e\n\u0011\u0013\n(1)\nwhere N (t)denotes all the source nodes of nodet and E(s, t)denotes\nall the edges from node s to t.\nThe most important GNN operators are Extract(·) and Aggregate(·).\nExtract(·) represents the neighbor information extractor. It extract\nuseful information from source node’s representationHl−1[s], with\nthe target node’s representationHl−1[t]and the edgee between the\ntwo nodes as query. Aggregate(·) gather the neighborhood informa-\ntion of souce nodes via some aggregation operators, such as mean,\nsum, and max, while more sophisticated pooling and normalization\nfunctions can be also designed.\nVarious (homogeneous) GNN architectures have been proposed\nfollowing this framework. Kipf et al. [9] propose graph convolu-\ntional network (GCN), which averages the one-hop neighbor of each\nnode in the graph, followed by a linear projection and non-linear\nactivation operations. Hamilton et al. propose GraphSAGE that\ngeneralizes GCN’s aggregation operation fromaverage to sum, max\nand a RNN unit . Velickovi et al. propose graph attention network\n(GAT) [22] by introducing the attention mechanism into GNNs,\nwhich allows GAT to assign different importance to nodes within\nthe same neighborhood.\n2.3 Heterogeneous GNNs\nRecently, studies have attempted to extend GNNs for modeling\nheterogeneous graphs. Schlichtkrull et al. [14] propose the rela-\ntional graph convolutional networks (RGCN) to model knowledge\ngraphs. RGCN keeps a distinct linear projection weight for each\nedge type. Zhang et al. [27] present the heterogeneous graph neural\nnetworks (HetGNN) that adopts different RNNs for different node\ntypes to integrate multi-modal features. Wang et al. [23] extend\ngraph attention networks by maintaining different weights for dif-\nferent meta-path-defined edges. They also use high-level semantic\nattention to differentiate and aggregate information from different\nmeta paths.\nThough these methods have shown to be empirically better than\nthe vanilla GCN and GAT models, they have not fully utilized the\nheterogeneous graphs’ properties. All of them use either node type\nor edge type alone to determine GNN weight matrices. However,\nthe node or edge counts of different types can vary greatly. For\nrelations that don’t have sufficient occurrences, it’s hard to learn\naccurate relation-specific weights. To address this, we propose to\nconsider parameter sharing for a better generalization. Given an\nedge e = (s, t)with its meta relation as ⟨τ(s), ϕ(e), τ(t)⟩, if we use\nthree interaction matrices to model the three corresponding ele-\nments τ(s), ϕ(e), and τ(t)in the meta relation, then the majority of\nweights could be shared. For example, in “the first author of” and\n“the second author of” relationships, their source and target node\ntypes are both author to paper, respectively. In other words, the\nknowledge about author and paper learned from one relation could\nbe quickly transferred and adapted to the other one. Therefore, we\nintegrate this idea with the powerful Transformer-like attention\narchitecture, and propose Heterogeneous Graph Transformer.\nTo summarize, the key differences between HGT and existing\nattempts include:\n(1) Instead of attending on node or edge type alone, we use the\nmeta relation ⟨τ(s), ϕ(e), τ(t)⟩to decompose the interaction\nand transform matrices, enabling HGT to capture both the\ncommon and specific patterns of different relationships using\nequal or even fewer parameters.\n(2) Different from most of the existing works that are based on\ncustomized meta paths, we rely on the nature of the neural\narchitecture to incorporate high-order heterogeneous neigh-\nbor information, which automatically learns the importance\nof implicit meta paths.\n(3) Most previous works don’t take the dynamic nature of (het-\nerogeneous) graphs into consideration, while we propose\nthe relative temporal encoding technique to incorporate tem-\nporal information by using limited computational resources.\n(4) None of the existing heterogeneous GNNs are designed for\nand experimented with Web-scale graphs, we therefore pro-\npose the heterogeneous Mini-Batch graph sampling algo-\nrithm designed for Web-scale graph training, enabling ex-\nperiments on the billion-scale Open Academic Graph.\n3 HETEROGENEOUS GRAPH TRANSFORMER\nIn this section, we present the Heterogeneous Graph Trans-\nformer (HGT). Its idea is to use the meta relations of heteroge-\nneous graphs to parameterize weight matrices for the heteroge-\nneous mutual attention, message passing, and propagation steps.\nTo further incorporate network dynamics, we introduce a relative\ntemporal encoding mechanism into the model.\n3.1 Overall HGT Architecture\nFigure 2 shows the overall architecture of Heterogeneous Graph\nTransformer. Given a sampled heterogeneous sub-graph (Cf. Sec-\ntion 4), HGT extracts all linked node pairs, where target node t is\nlinked by source node s via edge e. The goal of HGT is to aggregate\ninformation from source nodes to get a contextualized representa-\ntion for target node t. Such process can be decomposed into three\ncomponents: Heterogeneous Mutual Attention , Heterogeneous Mes-\nsage Passing and Target-Specific Aggregation.\nWe denote the output of the (l)-th HGT layer as H(l), which is\nalso the input of the (l+1)-th layer. By stacking L layers, we can get\nthe node representations of the whole graph H(L), which can be\nused for end-to-end training or fed into downstream tasks.\n3.2 Heterogeneous Mutual Attention\nThe first step is to calculate the mutual attention between source\nnode s and target node t. We first give a brief introduction to the\ngeneral attention-based GNNs as follows:\nHl [t]← Aggregate\n∀s ∈N (t), ∀e ∈E(s, t)\n\u0010\nAttention(s, t)·Message(s)\n\u0011\n(2)\nWWW ’20, April 20–24, 2020, Taipei, Taiwan Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun\nFigure 2: The Overall Architecture of Heterogeneous Graph Transformer. Given a sampled heterogeneous sub-graph with t as\nthe target node, s1 & s2 as source nodes, the HGT model takes its edges e1 = (s1, t)& e2 = (s2, t)and their corresponding meta relations\n< τ(s1), ϕ(e1), τ(t)> & < τ(s2), ϕ(e2), τ(t)> as input to learn a contextualized representation H(L)for each node, which can be used for\ndownstream tasks. Color decodes the node type. HGT includes three components: (1) meta relation-aware heterogeneous mutual attention,\n(2) heterogeneous message passing from source nodes, and (3) target-specific heterogeneous message aggregation.\nwhere there are three basic operators: Attention, which estimates\nthe importance of each source node; Message, which extracts the\nmessage by using only the source node s; and Aggregate, which\naggregates the neighborhood message by the attention weight.\nFor example, the Graph Attention Network (GAT) [22] adopts\nan additive mechanism as Attention, uses the same weight for\ncalculating Message, and leverages the simple average followed by\na nonlinear activation for the Aggregate step. Formally, GAT has\nAttentionGAT (s, t)= Softmax\n∀s ∈N (t)\n\u0012\n®a\n\u0010\nW Hl−1[t]∥W Hl−1[s]\n\u0011\u0013\nMessageGAT (s)= W Hl−1[s]\nAggregateGAT (·)= σ\n\u0010\nMean(·)\n\u0011\nThough GAT is effective to give high attention values to important\nnodes, it assumes thats and t have the same feature distributions by\nusing one weight matrixW . Such an assumption, as we’ve discussed\nin Section 1, is usually incorrect for heterogeneous graphs, where\neach type of nodes can have its own feature distribution.\nIn view of this limitation, we design the Heterogeneous Mu-\ntual Attention mechanism. Given a target nodet, and all its neigh-\nbors s ∈N (t), which might belong to different distributions, we\nwant to calculate their mutual attention grounded by their meta\nrelations, i.e., the ⟨τ(s), ϕ(e), τ(t)⟩triplets.\nInspired by the architecture design of Transformer [21], we map\ntarget node t into a Query vector, and source nodes into a Key vec-\ntor, and calculate their dot product as attention. The key difference\nis that the vanilla Transformer uses a single set of projections for all\nwords, while in our case each meta relation should have a distinct\nset of projection weights. To maximize parameter sharing while\nstill maintaining the specific characteristics of different relations,\nwe propose to parameterize the weight matrices of the interac-\ntion operators into a source node projection, an edge projection,\nand a target node projection. Specifically, we calculate the h-head\nattention for each edge e = (s, t)(See Figure 2 (1)) by:\nAttentionHGT (s, e, t)= Softmax\n∀s ∈N (t)\n\u0010\n∥\ni ∈[1, h]\nATT-headi (s, e, t)\n\u0011\n(3)\nATT-headi (s, e, t)=\n\u0010\nKi (s)W ATT\nϕ(e) Qi (t)T\n\u0011\n·\nµ⟨τ (s), ϕ(e), τ (t)⟩\n√\nd\nKi (s)= K-Lineari\nτ (s)\n\u0010\nH(l−1)[s]\n\u0011\nQi (t)= Q-Lineari\nτ (t)\n\u0010\nH(l−1)[t]\n\u0011\nFirst, for the i-th attention head ATT-headi (s, e, t), we project the\nτ(s)-type source node s into the i-th Key vector Ki (s)with a linear\nprojection K-Lineari\nτ (s) : Rd →R\nd\nh , where h is the number of\nattention heads and d\nh is the vector dimension per head. Note that\nK-Lineari\nτ (s)is indexed by the source node s’s type τ(s), meaning\nthat each type of nodes has a unique linear projection to maximally\nmodel the distribution differences. Similarly, we also project the\ntarget node t with a linear projection Q-Linear i\nτ (t)into the i−th\nQuery vector.\nNext, we need to calculate the similarity between the Query\nvector Qi (t)and Key vector Ki (s). One unique characteristic of\nheterogeneous graphs is that there may exist different edge types\n(relations) between a node type pair, e.g., τ(s)and τ(t). Therefore,\nunlike the vanilla Transformer that directly calculates the dot prod-\nuct between the Query and Key vectors, we keep a distinct edge-\nbased matrix W ATT\nϕ(e) ∈R\nd\nh ×d\nh for each edge type ϕ(e). In doing so,\nthe model can capture different semantic relations even between\nHeterogeneous Graph Transformer WWW ’20, April 20–24, 2020, Taipei, Taiwan\nthe same node type pairs. Moreover, since not all the relation-\nships contribute equally to the target nodes, we add a prior tensor\nµ ∈R|A|×|R|×|A| to denote the general significance of each meta\nrelation triplet, serving as an adaptive scaling to the attention.\nFinally, we concatenate h attention heads together to get the\nattention vector for each node pair. Then, for each target node t,\nwe gather all attention vectors from its neighborsN (t)and conduct\nsoftmax, making it fulfill Í\n∀s ∈N (t)AttentionHGT (s, e, t)= 1h×1.\n3.3 Heterogeneous Message Passing\nParallel to the calculation of mutual attention, we pass informa-\ntion from source nodes to target nodes (See Figure 2 (2)). Similar\nto the attention process, we would like to incorporate the meta\nrelations of edges into the message passing process to alleviate the\ndistribution differences of nodes and edges of different types. For a\npair of nodes e = (s, t), we calculate its multi-head Message by:\nMessageHGT (s, e, t)= ∥\ni ∈[1, h]\nMSG -headi (s, e, t) (4)\nMSG -headi (s, e, t)= M-Lineari\nτ (s)\n\u0010\nH(l−1)[s]\n\u0011\nW MSG\nϕ(e)\nTo get thei-th message head MSG -headi (s, e, t), we first project the\nτ(s)-type source node s into the i-th message vector with a linear\nprojection M-Lineari\nτ (s): Rd →R\nd\nh . It is then followed by a matrix\nW MSG\nϕ(e) ∈R\nd\nh ×d\nh for incorporating the edge dependency. The final\nstep is to concat allh message heads to get theMessageHGT (s, e, t)\nfor each node pair.\n3.4 Target-Specific Aggregation\nWith the heterogeneous multi-head attention and message cal-\nculated, we need to aggregate them from the source nodes to the\ntarget node (See Figure 2 (3)). Note that the softmax procedure in\nEq. 3 has made the sum of each target node t’s attention vectors\nto one, we can thus simply use the attention vector as the weight\nto average the corresponding messages from the source nodes and\nget the updated vector eH(l)[t]as:\neH(l)[t]= ⊕\n∀s ∈N (t)\n\u0010\nAttentionHGT (s, e, t)·MessageHGT (s, e, t)\n\u0011\n.\nThis aggregates information to the target node t from all its neigh-\nbors (source nodes) of different feature distributions.\nThe final step is to map target node t’s vector back to its type-\nspecific distribution, indexed by its node type τ(t). To do so, we\napply a linear projection A-Linearτ (t)to the updated vector eH(l)[t],\nfollowed by residual connection [8] as:\nH(l)[t]= A-Linearτ (t)\n\u0010\nσ\u0000eH(l)[t]\u0001\u0011\n+ H(l−1)[t]. (5)\nIn this way, we get thel-th HGT layer’s outputH(l)[t]for the target\nnode t. Due to the “small-world” property of real-world graphs,\nstacking the HGT blocks for L layers (L being a small value) can\nenable each node reaching a large proportion of nodes—with differ-\nent types and relations—in the full graph. That is, HGT generates\na highly contextualized representation H(L)for each node, which\ncan be fed into any models to conduct downstream heterogeneous\nnetwork tasks, such as node classification and link prediction.\nFigure 3: Relative Temporal Encoding (RTE) to model graph\ndynamic. Nodes are associated with timestamps T (·). After the\nRTE process, the temporal augmented representations are fed to\nthe HGT model.\nThrough the whole model architecture, we highly rely on using\nthe meta relation—⟨τ(s), ϕ(e), τ(t)⟩—to parameterize the weight\nmatrices separately. This can be interpreted as a trade-off between\nthe model capacity and efficiency. Compared with the vanilla Trans-\nformer, our model distinguishes the operators for different relations\nand thus is more capable to handle the distribution differences in\nheterogeneous graphs. Compared with existing models that keep a\ndistinct matrix for each meta relation as a whole, HGT’s triplet pa-\nrameterization can better leverage the heterogeneous graph schema\nto achieve parameter sharing. On one hand, relations with few oc-\ncurrences can benefit from such parameter sharing for fast adapta-\ntion and generalization. On the other hand, different relationships’\noperators can still maintain their specific characteristics by using a\nmuch smaller parameter set.\n3.5 Relative Temporal Encoding\nBy far, we present HGT—a graph neural network for modeling\nheterogeneous graphs. Next, we introduce the Relative Temporal\nEncoding (RTE) technique for HGT to handle graph dynamic.\nThe traditional way to incorporate temporal information is to\nconstruct a separate graph for each time slot. However, such a pro-\ncedure may lose a large portion of structural dependencies across\ndifferent time slots. Meanwhile, the representation of a node at\ntime t might rely on edges that happen at other time slots. There-\nfore, a proper way to model dynamic graphs is to maintain all the\nedges happening at different times and allow nodes and edges with\ndifferent timestamps to interact with each other.\nIn light of this, we propose the Relative Temporal Encoding\n(RTE) mechanism to model the dynamic dependencies in heteroge-\nneous graphs. RTE is inspired by Transformer’s positional encoding\nmethod [15, 21], which has been shown successful to capture the\nsequential dependencies of words in long texts.\nSpecifically, given a source node s and a target node t, along\nwith their corresponding timestamps T (s)and T (t), we denote the\nrelative time gap ∆T (t, s)= T (t)−T (s)as an index to get a relative\ntemporal encoding RT E(∆T (t, s)). Noted that the training dataset\nWWW ’20, April 20–24, 2020, Taipei, Taiwan Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun\nwill not cover all possible time gaps, and thusRT Eshould be capable\nof generalizing to unseen times and time gaps. Therefore, we adopt\na fixed set of sinusoid functions as basis, with a tunable linear\nprojection T-Linear∗: Rd →Rd as RT E:\nBase \u0000∆T (t, s), 2i\u0001 = sin\n\u0010\n∆Tt, s /10000\n2i\nd\n\u0011\n(6)\nBase \u0000∆T (t, s), 2i + 1\u0001 = cos\n\u0010\n∆Tt, s /10000\n2i+1\nd\n\u0011\n(7)\nRT E\u0000∆T (t, s)\u0001 = T-Linear\n\u0010\nBase(∆Tt, s )\n\u0011\n(8)\nFinally, the temporal encoding relative to the target nodet is added\nto the source node s’ representation as follows:\nbH(l−1)[s]= H(l−1)[s]+ RT E\u0000∆T (t, s)\u0001 (9)\nIn this way, the temporal augmented representation bH(l−1)will\ncapture the relative temporal information of source node s and\ntarget node t. The RTE procedure is illustrated in the Figure 3.\n4 WEB-SCALE HGT TRAINING\nIn this section, we present HGT’s strategies for training Web-\nscale heterogeneous graphs with dynamic information, including\nan efficient Heterogeneous Mini-Batch Graph Sampling algorithm—\nHGSampling—and an inductive timestamp assignment method.\n4.1 HGSampling\nThe full-batch GNN [9] training requires the calculation of all\nnode representations per layer, making it not scalable for Web-scale\ngraphs. To address this issue, different sampling-based methods [1,\n2, 7, 29] have been proposed to train GNNs on a subset of nodes.\nHowever, directly using them for heterogeneous graphs is prone to\nget sub-graphs that are extremely imbalanced regarding different\nnode types, due to that the degree distribution and the total number\nof nodes for each type can vary dramatically.\nTo address this issue, we propose an efficient Heterogeneous\nMini-Batch Graph Sampling algorithm—HGSampling—to enable\nboth HGT and traditional GNNs to handle Web-scale heterogeneous\ngraphs. HGSampling is able to 1) keep a similar number of nodes\nand edges for each type and 2) keep the sampled sub-graph dense\nto minimize the information loss and reduce the sample variance.\nAlgorithm 1 outlines the HGSampling algorithm. Its basic idea\nis to keep a separate node budget B[τ]for each node type τ and\nto sample an equal number of nodes per type with an importance\nsampling strategy to reduce variance. Given nodet already sampled,\nwe add all its direct neighbors into the corresponding budget with\nAlgorithm 2, and add t’s normalized degree to these neighbors in\nline 8, which will then be used to calculate the sampling probability.\nSuch normalization is equivalent to accumulate the random walk\nprobability of each sampled node to its neighborhood, avoiding the\nsampling being dominated by high-degree nodes. Intuitively, the\nhigher such value is, the more a candidate node is correlated with\nthe currently sampled nodes, and thus should be given a higher\nprobability to be sampled.\n∗For simplicity, we denote a linear projection L : Ra →Rb as a function to conduct\nlinear transformation to vector x ∈Ra as: L(x)= W x+b, where matrixW ∈Ra+b\nand bias b ∈Rb . W and b are learnable parameters for L.\nAlgorithm 1 Heterogeneous Mini-Batch Graph Sampling\nRequire: Adjacency matrix A for each ⟨τ(s), ϕ(e), τ(t)⟩relation\npair; Output node Set OS ; Sample number n per node type;\nSample depth L.\nEnsure: Sampled node set NS ; Sampled adjacency matrix ˆA.\n1: NS ←OS // Initialize sampled node set as output node set.\n2: Initialize an empty Budget B storing nodes for each node type\nwith normalized degree.\n3: for t ∈NS do\n4: Add-In-Budget(B, t, A, NS ) // Add neighbors of t to B.\n5: end for\n6: for l ←1 to L do\n7: for source node type τ ∈B do\n8: for source node s ∈B[τ]do\n9: prob(l−1)[τ][s]← B[τ ][s]2\n∥B[τ ]∥2\n2\n// Calculate sampling prob-\nability for each source node s of node type τ.\n10: end for\n11: Sample n nodes {ti }n\ni=1 from B[τ]using prob(l−1)[τ].\n12: for t ∈{ti }n\ni=1 do\n13: OS [τ].add(t)// Add node t into Output node set.\n14: Add-In-Budget(B, t, A, NS ) // Add neighbors of t to B.\n15: B[τ].pop(t)// Remove sampled node t from Budget.\n16: end for\n17: end for\n18: end for\n19: Reconstruct the sampled adjacency matrix ˆA among the sam-\npled nodes OS from A.\n20: return OS and ˆA;\nAfter the budget is updated, we then calculate the sampling\nprobability in Algorithm 1 line 9, where we calculate the square of\nthe cumulative normalized degree of each node s in each budget.\nAs proved in [29], using such sampling probability can reduce the\nsampling variance. Then, we sample n nodes in type τ by using the\ncalculated probability, add them into the output node set, update\nits neighborhood to the budget, and remove it out of the budget\nin lines 12–15. Repeating such procedure for L times, we get a\nsampled sub-graph with L depth from the initial nodes. Finally, we\nreconstruct the adjacency matrix among the sampled nodes. By\nusing the above algorithm, the sampled sub-graph contains a similar\nnumber of nodes per type (based on the separate node budget), and\nis sufficiently dense to reduce the sampling variance (based on the\nnormalized degree and importance sampling), making it suitable\nfor training GNNs on Web-scale heterogeneous graphs.\n4.2 Inductive Timestamp Assignment\nTill now we have assumed that each node t is assigned with\na timestamp T (t). However, in real-world heterogeneous graphs,\nmany nodes are not associated with a fixed time. Therefore, we\nneed to assign different timestamps to it. We denote these nodes as\nplain nodes. For example, the WWW conference is held in both 1974\nand 2019, and the WWW node in these two years has dramatically\ndifferent research topics. Consequently, we need to decide which\ntimestamp(s) to attach to the WWW node.\nHeterogeneous Graph Transformer WWW ’20, April 20–24, 2020, Taipei, Taiwan\nFigure 4: HGSampling with Inductive Timestamp Assignment.\nAlgorithm 2 Add-In-Budget\nRequire: Budget B storing nodes for each type with normal-\nized degree; Added node t; Adjacency matrix A for each\n⟨τ(s), ϕ(e), τ(t)⟩relation pair; Sampled node set NS .\nEnsure: Updated Budget B.\n1: for each possible source node type τ and edge type ϕ do\n2: ˆDt ←1 /len\n\u0010\nA⟨τ, ϕ, τ (t)⟩[t]\n\u0011\n// get normalized degree of\nadded node t regarding to ⟨τ, ϕ, τ(t)⟩.\n3: for source node s in A⟨τ, ϕ, τ (t)⟩[t]do\n4: if s has not been sampled (s < NS ) then\n5: if s has no timestamp then\n6: s.time = t.time // Inductively inherit timestamp.\n7: end if\n8: B[τ][s]← B[τ][s]+ ˆDt // Add candidate node s to\nbudget B with target node t’s normalized degree.\n9: end if\n10: end for\n11: end for\n12: return Updated Budget B\nThere also exist event nodes in heterogeneous graphs that have\nan explicit timestamp associated with them. For example, the pa-\nper node should be associated with its publication behavior and\ntherefore attached to its publication date.\nWe propose an inductive timestamp assignment algorithm to\nassign plain nodes timestamps based on event nodes that they are\nlinked with. The algorithm is shown in Algorithm 2 line 6. The idea\nis that plan nodes inherit the timestamps from event nodes. We\nexamine whether the candidate source node is an event node. If\nyes, like a paper published at a specific year, we keep its timestamp\nfor capturing temporal dependency. If no, like a conference that\ncan be associated with any timestamp, we inductively assign the\nassociated node’s timestamp, such as the published year of its paper,\nto this plain node. In this way, we can adaptively assign timestamps\nduring the sub-graph sampling procedure.\n5 EVALUATION\nIn this section, we evaluate the proposed Heterogeneous Graph\nTransformer on three heterogeneous academic graph datasets. We\nconduct the Paper-Field prediction, Paper-Venue prediction, and\nAuthor Disambiguation tasks. We also take case studies to demon-\nstrate how HGT can automatically learn and extract meta paths\nthat are important for downstream tasks†.\n5.1 Web-Scale Datasets\nTo examine the performance of the proposed model and its real-\nworld applications, we use the Open Academic Graph (OAG) [16,\n20, 28] as our experimental basis. OAG consists of more than 178\nmillion nodes and 2.236 billion edges—the largest publicly available\nheterogeneous academic dataset. In addition, all papers in OAG are\nassociated with their publication dates, spanning from 1900 to 2019.\nTo test the generalization of the proposed model, we also con-\nstruct two domain-specific subgraphs from OAG: the Computer\nScience (CS) and Medicine (Med) academic graphs. The graph sta-\ntistics are listed in Table 1, in which P–A, P–F, P–V, A–I, and P–P\ndenote the edges between paper and author, paper and field, paper\nand venue, author and institute, and the citation links between two\npapers.\nBoth the CS and Med graphs contain tens of millions of nodes\nand hundreds of millions of edges, making them at least one mag-\nnitude larger than the other CS (e.g., DBLP) and Med (e.g., Pubmed)\nacademic datasets that are commonly used in existing heteroge-\nneous GNN and heterogeneous graph mining studies. Moreover,\nthe three datasets used are far more distinguishable than previously\nwide-adopted small citation graphs used in GNN studies, such as\n†The dataset and code are publicly available at https://github.com/acbull/pyHGT.\nWWW ’20, April 20–24, 2020, Taipei, Taiwan Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun\nDataset #nodes #edges #papers #authors #fields #venues #institutes #P-A #P-F #P-V #A-I #P-P\nCS 11,732,027 107,263,811 5,597,605 5,985,759 119,537 27,433 16,931 15,571,614 47,462,559 5,597,606 7,190,480 31,441,552\nMed 51,044,324 451,468,375 21,931,587 28,779,507 289,930 25,044 18,256 85,620,479 149,728,483 21,931,588 28,779,507 165,408,318\nOAG 178,663,927 2,236,196,802 89,606,257 88,364,081 615,228 53,073 25,288 300,853,688 657,049,405 89,606,258 167,449,933 1,021,237,518\nTable 1: Open Academic Graph (OAG) Statistics.\nCora, Citeseer, and Pubmed [9, 22], which only contain thousands\nof nodes.\nThere are totally five node types: ‘Paper’, ‘Author’, ‘Field’, ‘Venue’,\nand ‘Institute’. The ‘Field’ nodes in OAG are categorized into six\nlevels from L0 to L5, which are organized with a hierarchical tree.\nTherefore, we differentiate the ‘Paper–Field’ edges corresponding\nto the field level.\nIn addition, we differentiate the different author orders (i.e., the\nfirst author, the last one, and others) and venue types (i.e., journal,\nconference, and preprint) as well. Finally, the ‘Self’ type corre-\nsponds to the self-loop connection, which is widely added in GNN\narchitectures. Except the ‘Self’ relationship, which are symmetric,\nall other relation types ϕ have a reverse relation type ϕ−1.\n5.2 Experimental Setup\nTasks and Evaluation. We evaluate the HGT model on four dif-\nferent real-world downstream tasks: the prediction of Paper–Field\n(L1), Paper–Field (L2), and Paper–Venue, and Author Disambigua-\ntion. The goal of the first three node classification tasks is to predict\nthe correct L1 and L2 fields that each paper belongs to or the venue\nit is published at, respectively. We use different GNNs to get the\ncontextual node representation of the paper and use a softmax out-\nput layer to get its classification label. For author disambiguation,\nwe select all the authors with the same name and their associated\npapers. The task is to conduct link prediction between these papers\nand candidate authors. After getting the paper and author node\nrepresentations from GNNs, we use a Neural Tensor Network to\nget the probability of each author-paper pair to be linked.\nFor all tasks, we use papers published before the year 2015 as\nthe training set, papers between 2015 and 2016 for validation, and\npapers between 2016 and 2019 as testing. We choose NDCG and\nMRR, which are two widely adopted ranking metrics [10, 11], as\nthe evaluation metrics. All models are trained for 5 times and, the\nmean and standard variance of test performance are reported.\nBaselines. We compare HGT with two classes of state-of-art graph\nneural networks. All baselines as well as our own model, are imple-\nmented via the PyTorch Geometric (PyG) package [5].\nThe first class of GNN baselines is designed for homogeneous\ngraphs, including:\n•Graph Convolutional Networks (GCN) [ 9], which simply\naverages the neighbor’s embedding followed by linear pro-\njection. We use the implementation provided in PyG.\n•Graph Attention Networks (GAT) [22], which adopts multi-\nhead additive attention on neighbors. We use the implemen-\ntation provided in PyG.\nThe second class considered is several dedicated heterogeneous\nGNNs as baselines, including:\n•Relational Graph Convolutional Networks (RGCN) [14], which\nkeeps a different weight for each relationship, i.e., a relation\ntriplet. We use the implementation provided in PyG.\n•Heterogeneous Graph Neural Networks (HetGNN) [27], which\nadopts different Bi-LSTMs for different node type for aggre-\ngating neighbor information. We re-implement this model\nin PyG following the authors’ official code.\n•Heterogeneous Graph Attention Networks (HAN) [23] de-\nsign hierarchical attentions to aggregate neighbor informa-\ntion via different meta paths. We re-implement this model\nin PyG following the authors’ official code.\nIn addition, to systematically analyze the effectiveness of the\ntwo major components of HGT, i.e., Heterogeneous weight pa-\nrameterization (Heter) and Relative Temporal Encoding (RTE), we\nconduct an ablation study, but comparing with models that remove\nthese components. Specifically, we use −Heter to denote models\nthat uses the same set of weights for all meta relations, and use\n−RT Eto denote models that doesn’t include relative temporal en-\ncoding. By considering all the permutations, we have: HGT−RT E\n−Heter ,\nHGT+RT E\n−Heter , HGT−RT E\n+Heter and HGT+RT E\n+Heter\n‡.\nWe use our HGSampling algorithm proposed in Section 4 for\nall baseline GNNs to handle the large-scale OAG graph. To avoid\ndata leakage, we remove out the links we aim to predict (e.g., the\nPaper-Field link as the label) from the sub-graph.\nInput Features. As we don’t assume the feature of each node\ntype belongs to the same distribution, we are free to use the most\nappropriate features to represent each type of nodes. For each paper,\nwe use a pre-trained XLNet [24, 25] to get the representation of each\nword in its title. We then average them weighted by each word’s\nattention to get the title representation for each paper. The initial\nfeature of each author is then simply an average of his/her published\npapers’ representations. For the field, venue, and institute nodes,\nwe use the metapath2vec model [3] to train their node embeddings\nby reflecting the heterogeneous network structures.\nThe homogeneous GNN baselines assume the node features be-\nlong to the same distribution, while our feature extraction doesn’t\nfulfill this assumption. To make a fair comparison, we add an adap-\ntation layer between the input features and all used GNNs. This\nmodule simply conducts different linear projections for nodes of\ndifferent types. Such a procedure can be regarded to map hetero-\ngeneous data into the same distribution, which is also adopted in\nliterature [23, 27].\nImplementation Details. We use 256 as the hidden dimension\nthroughout the neural networks for all baselines. For all multi-head\nattention-based methods, we set the head number as 8. All GNNs\nkeep 3 layers so that the receptive fields of each network are exactly\n‡Unless other stated, HGT refers to HGT+RT E\n+Heter .\nHeterogeneous Graph Transformer WWW ’20, April 20–24, 2020, Taipei, Taiwan\nGNN Models GCN [9] RGCN [14] GAT [22] HetGNN [27] HAN [23] HGT−RT E\n−Heter HGT+RT E\n−Heter HGT−RT E\n+Heter HGT+RT E\n+Heter\n# of Parameters 1.69M 8.80M 1.69M 8.41M 9.45M 3.12M 3.88M 7.44M 8.20M\nBatch Time 0.46s 1.24s 0.97s 1.35s 2.27s 1.11s 1.14s 1.48s 1.50s\nCS\nPaper–Field (L1) NDCG .608±.062 .603 ±.065 .622 ±.071 .612 ±.063 .618 ±.058 .662±.051 .689 ±.042 .705 ±.036 .718±.014\nMRR .679±.069 .683 ±.056 .694 ±.065 .689 ±.060 .691 ±.051 .751±.036 .779 ±.027 .799 ±.023 .823±.019\nPaper–Field (L2) NDCG .344±.021 .322 ±.053 .357 ±.058 .346 ±.071 .352 ±.051 .362±.048 .371 ±.043 .379 ±.047 .403±.041\nMRR .353±.053 .340 ±.061 .382 ±.057 .373 ±.051 .388 ±.065 .394±.072 .397 ±.064 .414 ±.076 .439±.078\nPaper–Venue\nNDCG .406±.081 .412 ±.076 .437 ±.082 .431 ±.074 .449 ±.072 .456±.069 .461 ±.066 .468 ±.074 .473±.054\nMRR .215±.066 .216 ±.105 .239 ±.089 .245 ±.069 .254 ±.074 .258±.085 .265 ±.090 .275 ±.089 .288±.088\nAuthor\nDisambiguation\nNDCG .826±.039 .835 ±.042 .864 ±.051 .850 ±.056 .859 ±.053 .867±.048 .875 ±.046 .886 ±.048 .894±.034\nMRR .661±.045 .665 ±.054 .694 ±.052 .668 ±.061 .688 ±.049 .703±.036 .712 ±.032 .727 ±.038 .732±.038\nMed\nPaper–Field (L1) NDCG .560±.056 .571 ±.061 .584 ±.076 .598 ±.068 .607 ±.054 .654±.048 .667 ±.045 .683 ±.037 .709±.029\nMRR .465±.055 .470 ±.082 .493 ±.069 .509 ±.054 .575 ±.057 .620±.066 .642 ±.062 .659 ±.055 .688±.048\nPaper–Field (L2) NDCG .334±.035 .337 ±.051 .344 ±.063 .342 ±.048 .350 ±.059 .359±.053 .365 ±.047 .374 ±.050 .384±.046\nMRR .337±.061 .343 ±.063 .370 ±.058 .373 ±.061 .379 ±.052 .385±.071 .397 ±.069 .408 ±.071 .417±.074\nPaper–Venue\nNDCG .377±.059 .383 ±.062 .388 ±.065 .412 ±.057 .416 ±.068 .421±.083 .432 ±.078 .446±.083 .445±.085\nMRR .211±.045 .217 ±.058 .244 ±.091 .259 ±.072 .271 ±.056 .277±.081 .282 ±.085 .288 ±.074 .291±.062\nAuthor\nDisambiguation\nMRR .776±.042 .779 ±.048 .828 ±.044 .824 ±.058 .834 ±.056 .838±.047 .844 ±.041 .864 ±.043 .871±.040\nNDCG .614±.051 .625 ±.049 .663 ±.046 .659 ±.061 .667 ±.053 .683±.055 .691 ±.046 .708 ±.041 .718±.043\nOAG\nPaper–Field (L1) NDCG .508±.141 .511 ±.128 .534 ±.103 .543 ±.084 .544 ±.096 .571±.089 .578 ±.086 .595 ±.089 .615±.084\nMRR .556±.136 .565 ±.105 .610 ±.096 .616 ±.076 .622 ±.092 .649±.081 .657 ±.078 .675 ±.082 .702±.081\nPaper–Field (L2) NDCG .318±.074 .328 ±.046 .339 ±.049 .336 ±.062 .342 ±.051 .350±.045 .354 ±.046 .358 ±.052 .367±.048\nMRR .322±.067 .332 ±.052 .348 ±.045 .350 ±.053 .358 ±.049 .362±.057 .369 ±.058 .371 ±.064 .378±.071\nPaper–Venue\nNDCG .302±.066 .313 ±.051 .317 ±.057 .309 ±.071 .327 ±.062 .334±.058 .341 ±.059 .353 ±.064 .355±.062\nMRR .194±.070 .193 ±.047 .196 ±.052 .192 ±.059 .214 ±.067 .229±.061 .233 ±.060 .243 ±.048 .247±.061\nAuthor\nDisambiguation\nNDCG .738±.042 .755 ±.048 .797 ±.044 .803 ±.058 .821 ±.056 .835±.043 .841 ±.041 .847 ±.043 .852±.048\nMRR .612±.064 .619 ±.057 .645 ±.063 .649 ±.052 .660 ±.049 .668±.059 .674 ±.058 .683 ±.066 .688±.054\nTable 2: Experimental results of different methods over the three datasets.\nthe same. All baselines are optimized via the AdamW optimizer [13]\nwith the Cosine Annealing Learning Rate Scheduler [12]. For each\nmodel, we train it for 200 epochs and select the one with the lowest\nvalidation loss as the reported model. We use the default parameters\nused in GNN literature and donot tune hyper-parameters.\n5.3 Experimental Results\nWe summarize the experimental results of the proposed model\nand baselines on three datasets in Table 2. All experiments for the\nfour tasks are evaluated in terms of NDCG and MRR.\nThe results show that in terms of both metrics, the proposed\nHGT significantly and consistently outperforms all baselines for\nall tasks on all datasets. Take, for example, the Paper–Field ( L1)\nclassification task on OAG, HGT achieves relative performance\ngains over baselines by 15–19% in terms of NDCG and 18–21% in\nterms of MRR (i.e., the performance gap divided by the baseline\nperformance). When compared to HAN—the best baseline for most\nof the cases, the average relative NDCG improvements of HGT on\nthe CS, Med and OAG datasets are 11%, 10% and 8%, respectively.\nOverall, we observe that on average, HGT outperforms GCN,\nGAT, RGCN, HetGNN, and HAN by 20% for the four tasks on all\nthree large-scale datasets. Moreover, HGT has fewer parameters\nand comparable batch time than all the heterogeneous graph neu-\nral network baselines, including RGCN, HetGNN, and HAN. This\nsuggests that by modeling heterogeneous edges according to their\nmeta relation schema, we are able to have better generalization\nwith fewer resource consumption.\nWWW ’20, April 20–24, 2020, Taipei, Taiwan Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun\nAblation Study. The core component in HGT are the heteroge-\nneous weight parameterization (Heter) and Relative Temporal En-\ncoding (RTE). To further analyze their effects, we conduct an abla-\ntion study by removing them from HGT. Specifically, the model that\nremoves heterogeneous weight parameterization, i.e., HGT+RT E\n−Heter ,\ndrops 4% of performance compared with the full model HGT+RT E\n+Heter .\nBy removing RTE (i.e., HGT−RT E\n+Heter ), the performance has a 2% drop.\nThe ablation study shows the significance of parameterizing with\nmeta relations and using Relative Temporal Encoding.\nIn addition, we also try to implement a baseline that keeps a\nunique weight matrix for each relation. However, such a baseline\ncontains too many parameters so that our experimental setting\ndoesn’t have enough GPU memory to optimize it. This also indicates\nthat using the meta relation to parameterize weight matrices can\nachieve competitive performance with fewer resources.\n5.4 Case Study\nTo further evaluate how Relative Temporal Encoding (RTE) can\nhelp HGT to capture graph dynamics, we conduct a case study\nshowing the evolution of conference topic. We select 100 confer-\nences in computer science with the highest citations, assign them\nthree different timestamps, i.e., 2000, 2010 and 2020, and construct\nsub-graphs initialized by them. Using a trained HGT, we can get\nthe representations for these conferences, with which we can calcu-\nlate the euclidean distances between them. We select WWW, KDD,\nand NeurIPS as illustration. For each of them, we pick the top-5\nmost similar conferences (i.e., the one with the smallest euclidean\ndistance) to show how the conference’s topics evolve over time.\nAs shown in Table 3, these venues’ relationships have changed\nfrom 2000 to 2020. For example, WWW in 2000 was more related to\nsome database conferences, i.e., SIGMOD and VLDB, and some net-\nworking conferences, i.e., NSDI and GLOBECOM. However, WWW\nin 2020 would become more related to some data mining and infor-\nmation retrieval conferences (KDD, SIGIR, and WSDM), in addition\nto SIGMOD and GLOBECOM. Also, KDD in 2000 was more related\nto traditional database and data mining venues, while in 2020 it\nwill tend to correlate with a variety of topics, i.e. machine learning\n(NeurIPS), database (SIGMOD), Web (WWW), AI (AAAI), and NLP\n(EMNLP). Additionally, our HGT model can capture the difference\nbrought by new conferences. For example, NeurIPS in 2020 would\nrelate with ICLR, which is a newly organized deep learning confer-\nence. This case study shows that the relative temporal encoding can\nhelp capture the temporal evolution of the heterogeneous academic\ngraphs.\n5.5 Visualize Meta Relation Attention\nTo illustrate how the incorporated meta relation schema can\nbenefit the heterogeneous message passing process, we pick the\nschema that has the largest attention value in each of the first\ntwo HGT layers and plot the meta relation attention hierarchy\ntree in Figure 5. For example, to calculate a paper’s representation,\n⟨Paper, is_published _at, Venue, is_published _at−1, Paper⟩, ⟨Paper,\nhas_L2_f ield_of , Field,has_L5_f ield_of −1, Paper⟩, and ⟨Institute,\nis_af f iliated_with −1, Author,is_f irst_author _of , Paper⟩are the\nthree most important meta relation sequences, which can be re-\ngarded as meta paths PVP, PFP, and IAP, respectively. Note that\nVenue Time Top −5 Most Similar Venues\nWWW\n2000 SIGMOD, VLDB, NSDI, GLOBECOM, SIGIR\n2010 GLOBECOM, KDD, CIKM, SIGIR, SIGMOD\n2020 KDD, GLOBECOM, SIGIR, WSDM, SIGMOD\nKDD\n2000 SIGMOD, ICDE, ICDM, CIKM, VLDB\n2010 ICDE, WWW, NeurIPS, SIGMOD, ICML\n2020 NeurIPS, SIGMOD, WWW, AAAI, EMNLP\nNeurIPS\n2000 ICCV, ICML, ECCV, AAAI, CVPR\n2010 ICML, CVPR, ACL, KDD, AAAI\n2020 ICML, CVPR, ICLR, ICCV, ACL\nTable 3: Temporal Evolution of Conference Similarity.\nFigure 5: Hierarchy of the learned meta relation attention.\nthese meta paths and their importance are automatically learned\nfrom the data without manual design. Another example of calcu-\nlating an author node’s representation is shown on the right. Such\nvisualization demonstrates that Heterogeneous Graph Transformer\nis capable of implicitly learning to construct important meta paths\nfor specific downstream tasks, without manual customization.\n6 CONCLUSION\nIn this paper, we propose the Heterogeneous Graph Transformer\n(HGT) architecture for modeling Web-scale heterogeneous and dy-\nnamic graphs. To model heterogeneity, we use the meta relation\n⟨τ(s), ϕ(e), τ(t)⟩to decompose the interaction and transform matri-\nces, enabling the model to have the similar modeling capacity with\nfewer resources. To capture graph dynamics, we present the relative\ntemporal encoding (RTE) technique to incorporate temporal infor-\nmation using limited computational resources. To conduct efficient\nand scalable training of HGT on Web-scale data, we design the het-\nerogeneous Mini-Batch graph sampling algorithm—HGSampling.\nWe conduct comprehensive experiments on the Open Academic\nGraph, and show that the proposed HGT model can capture both\nheterogeneity and outperforms all the state-of-the-art GNN base-\nlines on various downstream tasks.\nIn the future, we will explore whether HGT is able to generate\nheterogeneous graphs, e.g., predict new papers and their titles, and\nwhether we can pre-train HGT to benefit tasks with scarce labels.\nAcknowledgements. We would like to thank Xiaodong Liu for\nhelpful discussions. This work is partially supported by NSF III-\n1705169, NSF CAREER Award 1741634, NSF#1937599, Okawa Foun-\ndation Grant, and Amazon Research Award.\nHeterogeneous Graph Transformer WWW ’20, April 20–24, 2020, Taipei, Taiwan\nREFERENCES\n[1] Jie Chen, Tengfei Ma, and Cao Xiao. 2018. FastGCN: Fast Learning with Graph\nConvolutional Networks via Importance Sampling. In ICLR’18.\n[2] Jianfei Chen, Jun Zhu, and Le Song. 2018. Stochastic Training of Graph Convolu-\ntional Networks with Variance Reduction. In ICML. 941–949.\n[3] Yuxiao Dong, Nitesh V Chawla, and Ananthram Swami. 2017. metapath2vec:\nScalable Representation Learning for Heterogeneous Networks. In KDD ’17 .\n[4] Yuxiao Dong, Hao Ma, Zhihong Shen, and Kuansan Wang. 2017. A Century of\nScience: Globalization of Scientific Collaborations, Citations, and Innovations. In\nKDD ’17 . ACM, 1437–1446.\n[5] Matthias Fey and Jan Eric Lenssen. 2019. Fast Graph Representation Learning\nwith PyTorch Geometric. ICLR 2019 Workshop: Representation Learning on Graphs\nand Manifolds (2019).\n[6] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E.\nDahl. 2017. Neural Message Passing for Quantum Chemistry. InICML. 1263–1272.\n[7] William L. Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive Represen-\ntation Learning on Large Graphs. In NeurIPS’17.\n[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual\nLearning for Image Recognition. In CVPR’16.\n[9] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with\nGraph Convolutional Networks. In ICLR’17.\n[10] Hang Li. 2014. Learning to Rank for Information Retrieval and Natural Language\nProcessing, Second Edition . Morgan & Claypool Publishers. https://doi.org/10.\n2200/S00607ED2V01Y201410HLT026\n[11] Tie-Yan Liu. 2011. Learning to Rank for Information Retrieval . Springer.\n[12] Ilya Loshchilov and Frank Hutter. 2017. SGDR: Stochastic Gradient Descent with\nWarm Restarts. In ICLR’17.\n[13] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization.\nIn ICLR’19.\n[14] Michael Sejr Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg,\nIvan Titov, and Max Welling. 2018. Modeling Relational Data with Graph Convo-\nlutional Networks. In ESWC’2018.\n[15] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-Attention with\nRelative Position Representations. In NAACL-HLT. 464–468.\n[16] Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Paul Hsu,\nand Kuansan Wang. 2015. An Overview of Microsoft Academic Service (MAS)\nand Applications. In WWW Companion 2015 .\n[17] Yizhou Sun and Jiawei Han. 2012. Mining Heterogeneous Information Networks:\nPrinciples and Methodologies . Morgan & Claypool Publishers.\n[18] Yizhou Sun, Jiawei Han, Xifeng Yan, Philip S. Yu, and Tianyi Wu. 2011. Pathsim:\nMeta path-based top-k similarity search in heterogeneous information networks.\nIn VLDB ’11 .\n[19] Yizhou Sun, Brandon Norick, Jiawei Han, Xifeng Yan, Philip S. Yu, and Xiao\nYu. 2012. Integrating meta-path selection with user-guided object clustering in\nheterogeneous information networks. In KDD’12.\n[20] Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. 2008. Arnet-\nminer: extraction and mining of academic social networks. In KDD.\n[21] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In NeurIPS’17.\n[22] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLiò, and Yoshua Bengio. 2018. Graph Attention Networks. In ICLR’18.\n[23] Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S.\nYu. 2019. Heterogeneous Graph Attention Network. In KDD’19. 2022–2032.\n[24] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,\nAnthony Moi, Pierric Cistac, Tim Rault, RÃľmi Louf, Morgan Funtowicz, and\nJamie Brew. 2019. Transformers: State-of-the-art Natural Language Processing.\narXiv:cs.CL/1910.03771\n[25] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdi-\nnov, and Quoc V. Le. 2019. XLNet: Generalized Autoregressive Pretraining for\nLanguage Understanding. In NeurIPS’19.\n[26] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim.\n2019. Graph Transformer Networks. In NeurIPS’19.\n[27] Chuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, and Nitesh V.\nChawla. 2019. Heterogeneous Graph Neural Network. In WWW’19.\n[28] Fanjin Zhang, Xiao Liu, Jie Tang, Yuxiao Dong, Peiran Yao, Jie Zhang, Xiaotao\nGu, Yan Wang, Bin Shao, Rui Li, and Kuansan Wang. 2019. OAG: Toward Linking\nLarge-scale Heterogeneous Entity Graphs. In KDD’19.\n[29] Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu.\n2019. Layer-Dependent Importance Sampling for Training Deep and Large Graph\nConvolutional Networks. In NeurIPS’19.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7352749705314636
    },
    {
      "name": "Scalability",
      "score": 0.6645475625991821
    },
    {
      "name": "Theoretical computer science",
      "score": 0.5212821960449219
    },
    {
      "name": "Graph",
      "score": 0.48130738735198975
    },
    {
      "name": "Homogeneous",
      "score": 0.44522637128829956
    },
    {
      "name": "Transformer",
      "score": 0.42799991369247437
    },
    {
      "name": "Heterogeneous network",
      "score": 0.4106210172176361
    },
    {
      "name": "Distributed computing",
      "score": 0.3465350270271301
    },
    {
      "name": "Mathematics",
      "score": 0.13538885116577148
    },
    {
      "name": "Combinatorics",
      "score": 0.09080579876899719
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Wireless",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Wireless network",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 30
}