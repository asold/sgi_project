{
    "title": "An interpretable transformer network for the retinal disease classification using optical coherence tomography",
    "url": "https://openalex.org/W4323038853",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2139330743",
            "name": "Jingzhen He",
            "affiliations": [
                "Qilu Hospital of Shandong University"
            ]
        },
        {
            "id": "https://openalex.org/A2107096172",
            "name": "Junxia Wang",
            "affiliations": [
                "Shandong Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2344080423",
            "name": "Zeyu Han",
            "affiliations": [
                "Shandong University"
            ]
        },
        {
            "id": "https://openalex.org/A2102675153",
            "name": "Jun Ma",
            "affiliations": [
                "Southeast University"
            ]
        },
        {
            "id": "https://openalex.org/A2132459240",
            "name": "Chongjing Wang",
            "affiliations": [
                "China Academy of Information and Communications Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2048831535",
            "name": "Meng Qi",
            "affiliations": [
                "Shandong Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2139330743",
            "name": "Jingzhen He",
            "affiliations": [
                "Qilu Hospital of Shandong University"
            ]
        },
        {
            "id": "https://openalex.org/A2107096172",
            "name": "Junxia Wang",
            "affiliations": [
                "Shandong Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2344080423",
            "name": "Zeyu Han",
            "affiliations": [
                "Weihai Science and Technology Bureau"
            ]
        },
        {
            "id": "https://openalex.org/A2102675153",
            "name": "Jun Ma",
            "affiliations": [
                "Southeast University"
            ]
        },
        {
            "id": "https://openalex.org/A2132459240",
            "name": "Chongjing Wang",
            "affiliations": [
                "China Academy of Information and Communications Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2048831535",
            "name": "Meng Qi",
            "affiliations": [
                "Shandong Normal University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3122431193",
        "https://openalex.org/W4225716664",
        "https://openalex.org/W3207206329",
        "https://openalex.org/W2012372812",
        "https://openalex.org/W2075206985",
        "https://openalex.org/W2016795705",
        "https://openalex.org/W2078920062",
        "https://openalex.org/W3012661000",
        "https://openalex.org/W2952405950",
        "https://openalex.org/W3091449858",
        "https://openalex.org/W3073352144",
        "https://openalex.org/W3158315073",
        "https://openalex.org/W4313588240",
        "https://openalex.org/W4292771491",
        "https://openalex.org/W2906751147",
        "https://openalex.org/W2985431718",
        "https://openalex.org/W4225400594",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3035253074",
        "https://openalex.org/W3112051848",
        "https://openalex.org/W2788633781",
        "https://openalex.org/W2907620831",
        "https://openalex.org/W2965325581",
        "https://openalex.org/W2966909379",
        "https://openalex.org/W2946303964",
        "https://openalex.org/W3082431377",
        "https://openalex.org/W2984413259",
        "https://openalex.org/W3211369467",
        "https://openalex.org/W4224029245",
        "https://openalex.org/W4312932358",
        "https://openalex.org/W4296218651",
        "https://openalex.org/W4220959735",
        "https://openalex.org/W2954996726",
        "https://openalex.org/W2295107390",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2899156985",
        "https://openalex.org/W2963847595",
        "https://openalex.org/W4308782099",
        "https://openalex.org/W4280605623",
        "https://openalex.org/W3176923149"
    ],
    "abstract": "Abstract Retinal illnesses such as age-related macular degeneration and diabetic macular edema will lead to irreversible blindness. With optical coherence tomography (OCT), doctors are able to see cross-sections of the retinal layers and provide patients with a diagnosis. Manual reading of OCT images is time-consuming, labor-intensive and even error-prone. Computer-aided diagnosis algorithms improve efficiency by automatically analyzing and diagnosing retinal OCT images. However, the accuracy and interpretability of these algorithms can be further improved through effective feature extraction, loss optimization and visualization analysis. In this paper, we propose an interpretable Swin-Poly Transformer network for performing automatically retinal OCT image classification. By shifting the window partition, the Swin-Poly Transformer constructs connections between neighboring non-overlapping windows in the previous layer and thus has the flexibility to model multi-scale features. Besides, the Swin-Poly Transformer modifies the importance of polynomial bases to refine cross entropy for better retinal OCT image classification. In addition, the proposed method also provides confidence score maps, assisting medical practitioners to understand the models’ decision-making process. Experiments in OCT2017 and OCT-C8 reveal that the proposed method outperforms both the convolutional neural network approach and ViT, with an accuracy of 99.80% and an AUC of 99.99%.",
    "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2023) 13:3637  | https://doi.org/10.1038/s41598-023-30853-z\nwww.nature.com/scientificreports\nAn interpretable transformer \nnetwork for the retinal disease \nclassification using optical \ncoherence tomography\nJingzhen He 1*, Junxia Wang 2, Zeyu Han 3, Jun Ma 4, Chongjing Wang 5 & Meng Qi 2*\nRetinal illnesses such as age-related macular degeneration and diabetic macular edema will lead \nto irreversible blindness. With optical coherence tomography (OCT), doctors are able to see cross-\nsections of the retinal layers and provide patients with a diagnosis. Manual reading of OCT images is \ntime-consuming, labor-intensive and even error-prone. Computer-aided diagnosis algorithms improve \nefficiency by automatically analyzing and diagnosing retinal OCT images. However, the accuracy and \ninterpretability of these algorithms can be further improved through effective feature extraction, \nloss optimization and visualization analysis. In this paper, we propose an interpretable Swin-Poly \nTransformer network for performing automatically retinal OCT image classification. By shifting the \nwindow partition, the Swin-Poly Transformer constructs connections between neighboring non-\noverlapping windows in the previous layer and thus has the flexibility to model multi-scale features. \nBesides, the Swin-Poly Transformer modifies the importance of polynomial bases to refine cross \nentropy for better retinal OCT image classification. In addition, the proposed method also provides \nconfidence score maps, assisting medical practitioners to understand the models’ decision-making \nprocess. Experiments in OCT2017 and OCT-C8 reveal that the proposed method outperforms both the \nconvolutional neural network approach and ViT, with an accuracy of 99.80% and an AUC of 99.99%.\nThe number of patients suffering from retinal illness has increased dramatically in recent  years1,2. Age-related \nmacular degeneration (AMD) and diabetic macular edema (DME) are two frequent retinal disorders that can \nlead to lifelong blindness. AMD, which comes in two forms: dry AMD and wet AMD, is the most prevalent cause \nof blindness in people over 65. Patients with dry AMD present drusen on the retina, and most patients with \nwet AMD show choroidal neovascularization (CNV) 3. DME is a diabetic complication that causes structural \nalterations in the retinal neurovascular systems, resulting in visual  loss4. It is caused by a rupture in the retinal \nvessel walls, which results in the accumulation of fluid and proteins in the  retina5. According to survey statistics, \nabout 25% of diabetic retinopathy patients develop to  DME6. With early identification and treatment, the course \nof fundus disease can be delayed.\nOptical coherence tomography (OCT) is a sophisticated ophthalmic imaging technique to display the cross-\nsection of retina layers. It has the advantages of being non-contact, non-invasive, and rapid  imaging7. Ophthal-\nmologists regard OCT as one of the most important tools for the quantification, analysis, and treatment design \nof retinal diseases. However, there are certain difficulties in manually diagnosing retinal OCT images. First, as \nthe number of patients grows year by year, relying solely on qualified medical professionals to make diagnoses \nwill no longer be sufficient to meet the diagnostic and therapeutic  requirements8. Second, the characteristics \nof certain lesions are not readily obvious, leading to misinterpretation and missed diagnoses. Moreover, a large \nnumber of patients have gone undiagnosed in the early stages of the disease due to a lack of medical care in some \nlocations, which will cause disease aggravations.\nComputer-aided diagnosis (CAD) is an effective method to address these problems. Although some break -\nthroughs have been obtained in the field of classification of retinal OCT images, there are some challenges \nof design well-performing machine learning CAD systems, such as complicated feature selection and high \nOPEN\n1Department of Radiology, Qilu Hospital of Shandong University, Jinan 250012, China. 2School of Information \nScience and Engineering, Shandong Normal University, Jinan 250358, China. 3School of Mathematics and \nStatistics, Shandong University, Weihai 264209, China. 4School of Cyber Science and Engineering, Southeast \nUniversity, Nanjing 211189, China. 5China Academy of Information and Communications Technology, \nBeijing 100191, China. *email: hjzhhjzh@163.com; qimeng@sdnu.edu.cn\n2\nVol:.(1234567890)Scientific Reports |         (2023) 13:3637  | https://doi.org/10.1038/s41598-023-30853-z\nwww.nature.com/scientificreports/\ncomputational cost. In recent years, deep learning has developed rapidly and has shown brilliant performance \nin the field of computer vision. Deep learning has become the mainstream algorithm for retinal OCT image \nclassification. It uses convolutional neural layers to automatically learn image features from low level to high \nlevel, which overcomes the shortcomings of manual feature extraction.\nSeveral scholars have explored the application of convolutional neural networks (CNNs) for the automatic \ndiagnosis of OCT images. Perdomo et al.9 developed an OCT-Net to classify normal retina and three common \nretinal diseases. The proposed network extracted and displayed information that was interpretable for clinical \ndiagnosis. Kamran et al.10 proposed a retinal disease classification framework consisting of two joint networks, \nwhich combine supervised and unsupervised approaches to improve the robustness and accuracy of identifying \nretinal diseases. In addition, Rajagopalan et al.11 trained a deep learning-based fully automatic diagnosis system \nand used the Kuan filter to remove speckle noise from the input image, which provided higher classification \naccuracy for large public OCT datasets. Song et al. 12 proposed a depth inference mechanism for the diagnosis \nof glaucoma, which combined OCT and visual field (VF) examination to effectively utilize complementary \ninformation from different modalities. jin et al. 13 proposed to improve the performance and interpretability of \ntraditional DL models by implementing segmentation based on prior human knowledge. Vidal et al.14 transforms \nbinary masks into photorealistic OCT images using image-to-image generative adversarial networks. Based on \nthe clinical relationship between retinal shape and the presence of DME fluid, this method generates pathologi-\ncal and non-pathological samples by changing the dichroic mask morphology. Previous works have shown that \nthe deep learning method achieved a matching or exceeding performance to that of ophthalmologists with \nsignificant clinical  experience15,16.\nThe evolution of network architectures in natural language processing (NLP) has promoted computer image \nprocessing from CNN to the sequence network Transformer. Vision Transformer (ViT) has become the most \nprevalent architecture in computer vision. Designed for sequence modeling and transduction activities, ViT is \nnotable for its use of self-attention based on windows to model long-range dependencies in the whole image. \nWen et al.17 recently applied the ViT framework to OCT images for auxiliary diagnosis of ocular abnormalities. \nThey employed CNN to extract local features and the ViT to consider the image’s global information, resulting \nin an increase in overall accuracy, sensitivity, and specificity. Their proposed method illustrates the advantages \nof ViT for modeling global dependencies.\nHowever, due to domain differences, converting the Transformer from natural language processing (NLP) to \ncomputer vision presents two obstacles. On the one hand, ViT cannot capture features at multiple scales, because \nthe language is not affected by scale changes. On the other hand, image pixels have a larger resolution than text \nwords, resulting in an exponential rise in computation. Fortunately, Liu et al.18 introduced the Swin Transformer, \na hierarchical vision transformer that increased computational efficiency by using a shifted-window strategy. \nThey also developed a Patch Merging method for flexibly synthesizing small patches into large patches, thereby \nwidening the perceptual field and providing feature information on multiple scales.\nIn this paper, inspired by the Swin Transformer, we proposed an automatic diagnosis network Swin-poly \nTransformer for classifying OCT images into different categories. Figure 1 depicts examples from eight categories \nof fundus diseases. Furthermore, We adopt the PolyLoss as a loss function, which adjusts polynomial coefficients \nautomatically for better retinal OCT image classification. In addition, the visual interpretation method is adopted \nin the inference stage to improve the model’s interpretability. We utilize the post-hoc interpretation method \nScore-CAM19 to generate confidence score maps, which highlight the discriminative features and thereby assist \nclinicians to understand the model’s decision-making.\nIn summary, the contributions of this work are as follows:\n• In this paper, We propose the Swin-poly Transformer that combines the multi-scale features and the Poly \nloss to improve the performance of automatic retinal OCT classification.\nFigure 1.  Examples of OCT images in eight classes, including AMD, CNV , CSR, DME, DR, drusen, MH and \nNormal.\n3\nVol.:(0123456789)Scientific Reports |         (2023) 13:3637  | https://doi.org/10.1038/s41598-023-30853-z\nwww.nature.com/scientificreports/\n• To intuitively understand the suggested model decision, we generate a heatmap based on the Score-CAM \nand apply it to the original image to highlight the tumor region.\n• The suggested strategy achieves state-of-the-art performance in OCT2017, outperforming both the ViT \nnetwork approach and convolutional neural network approach, with an accuracy of 99.80% and an Area \nUnder Curve of 99.99%.\nRelated work\nSeveral studies have looked into using deep learning algorithms to identify OCT images. Lu et al.15 and Bhadra \net al.20 trained a deep multi-layered CNN to classify OCT images into healthy, dry AMD, wet AMD, and DME cat-\negories. Kermany et al.21,22 and 23 investigated the application of deep transfer learning for the automatic diagnosis \nof diabetic retinopathy in OCT images. Das et al.24 introduced a multi-scale deep feature fusion (MDFF) network \nto contribute discriminative features and complementary information to the classifier. Huang et al.25 suggested \na layer-guided CNN (LGCNN) for identifying normal retina and three common types of macular pathologies \n(CNV , DME and Druse). It employed an effective segmentation network to build retinal layer segmentation maps \nand then integrate the information from two lesion-related layers to improve OCT classification. Kim and  Tran26 \nimplemented a CNN-based ensemble learning model through several CNNs to further improve classification \nperformance. Similarly, Alqudah et al.27 trained a CNN classification model on a large number of OCT images \nfor distinguishing five types of retinal diseases, which achieved an overall accuracy of 0.953.\nRecently, there have been new advances in OCT image classification based on deep learning. Saleh et al.28 and \nSubramanian et al.29 explored the transfer learning of pre-trained CNN networks to diagnose retinal disorders. \nThe accuracy and robustness of transfer learning with CNN for retinal disease classification is demonstrated by \ncomparison with other classifiers and human experts. Wen et al. 17 proposed a lesion-localization convolution \ntransformer (LLCT) network. It combines both convolution and self-attention to classify ophthalmic diseases and \nlocalize the retinal lesions. This design takes advantage of CNN’s extracting local features and the transformer’s \nconsideration of global context and dynamic attention, accurately classifying and localizing retinal lesions. In \naddition, Saleh et al. 30,31 developed a multi-criteria decision platform to investigate how to evaluate diagnostic \nmodels for retinal diseases and to enable the decision model to select the appropriate diagnostic model. The \nplatform uses an entropy technique with ideal solution similarity ranking and employed nine quantitative crite-\nria to evaluate models, facilitating reliable and fast diagnosis. Karthik and  Mahadevappa32 proposed a modern \ndiagnosis system for OCT image classification. They replace the residual connection in three ResNet architectures \nwith EdgeEn block and cross-activation for increasing the contrast of the derivatives to generate sharper features, \nsuccessfully increasing the classification accuracy. In this work, we propose to employ a transformer network \nthat combines the multi-scale features and the Poly loss to improve the performance of automatic retinal OCT \nclassification.\nMaterials and methods\nMaterials. We use the retinal OCT image datasets  OCT201721 and OCT-C8 33 to evaluate the proposed \nmethod. We follow the original data division strategy and use the handout method to split the training, valida-\ntion set, and test sets. The first dataset consists of 109,312 images, where 108,312 images are used for training, \n32 for validation, and 968 for testing. In the training set, there are 37,205 retinal OCT images with CNV , 11,348 \nimages with DME, 8616 images with drusen, and 26,315 normal images in the training set. In the validation \nand test sets, 8 and 242 OCT images were included in each category, respectively. The second dataset OCT-\nC8 consists of 24,000 images and is divided into eight categories: Age-related macular degeneration (AMD), \nChoroidal Neovascularisation (CNV), Diabetic macular edema (DME), Drusen, Macular Hole (MH), Diabetic \nRetinopathy (DR), Central Serous Retinopathy (CSR) and one for healthy classes. Where 25,600 images are \nused for training, 2800 for validation, and 2800 for testing. Each category includes 3200 for training and 350 for \nvalidation and testing respectively.\nData preprocessing and augmentation are performed prior to model training. Deep learning models are a \ndata-driven way to learn task-related features. These models are based on the assumption that training data and \ntest data have the same distribution. In a real scenario, this hypothesis holds only when the sample size is large \nenough. However, collecting numerous labeled medical images is difficult compared to natural  images1 because \nlabeling medical images requires a lot of time and effort from experienced experts. Numerous works have proven \nthat data augmentation is an effective method to improve the diversity of training data, which contributes to \nenhancing the generalization and stability of the  model34. Additionally, the features’ scale and rotation invari-\nance are not captured by the CNN model. Therefore, data augmentation methods, including random rotation, \nflipping and mirroring, are adopted to increase the diversity of training images. Furthermore, to match the input \nof the model, all images are resized to 224 × 224 and normalized to [0, 1]. Finally, converting data into tensors \nand sending them to the proposed model.\nOverall framework. We present a Swin-Poly Transformer network, which combines Swin  Transformer18 \nand PolyLoss, for the automatic diagnosis of retina diseases in OCT images. Moreover, the proposed method pro-\nvides visual interpretation based on the score-CAM method. The pipeline of the proposed method is depicted in \nFig. 2. Specifically, in the training stage, random data augmentation is performed on the training set to improve \nthe generalization ability of the model. After that, the enhanced images are fed into Swin Transformer in batches \nfor weights and parameters learning. Furthermore, PolyLoss is employed in this work to automatically adjust \npolynomial coefficients for better retinal OCT image classification. Based on the prediction, score-CAM gener-\nates a visual explanation to help understand the model’s decision-making.\n4\nVol:.(1234567890)Scientific Reports |         (2023) 13:3637  | https://doi.org/10.1038/s41598-023-30853-z\nwww.nature.com/scientificreports/\nSwin Transformer for multi-scale feature representation. The Transformer architecture and \nits adaptation on image  classification35 performs global self-attention by establishing a relationship between \none token and all others. However, in contrast to convolutional neural networks, induction biases, i.e., two-\ndimensional neighborhood structure (locality) and translational equivalence, are lost in  Vit18. Specifically, the \ntwo neighborhood structure describes the neighboring regions with similar features in an image. Translational \nequivalence means that objectives in an image should get the same result (labels) no matter where they are \nmoved. Scholars have demonstrated that the lack of inductive bias breaks down when the amount of data is large \n enough36. However, access to millions of labeled medical images is difficult due to privacy and ethical require-\nments. Moreover, the pixel resolution in images is much higher than the length of words in text paragraphs, \nresulting in an increase in the amount of computation. Therefore, in this work, we investigate the use of the Swin \nTransformer to express the multi-scale feature representation in OCT images. It can reduce the computational \ncomplexity of self-attention by exploiting the prior knowledge of induction bias in ViT.\nArchitecture of Swin Transformer. An overview of the Swin Transformer is presented in Fig. 3. A patch partition \nmodule first splits an input image of 224 × 224 into non-overlapping patches of size 4 × 4 . Each patch is treated \nas a ”token” , and the patch tokens are projected to the C dimension using a linear embedding layer. Following \nthat, two successive Swin Transformer blocks with self-attention computation are applied to these patch tokens \nto control the number of tokens, as shown in Fig. 3b. A ”stage” is the combination of the linear embedding layer \nand the Swin Transformer blocks. The design of the Swin Transformer is similar to the layer structure of a CNN, \nwhere the resolution of each stage is halved and the number of channels is doubled. To produce hierarchical \nrepresentations, the Swin-Transformer reduces the number of tokens by merging patch layers as the network \ngets deeper. An example of hierarchical representation is illustrated in Fig. 3c.\nSwin Transformer block. There are two units in the Swin Transformer block. Each unit consists of two nor -\nmalization layers (LayerNorm), a self-attention module, and a multilayer perceptron (MLP) layer. In the Swin \nTransformer block, the standard multi-head self attention (MSA) module in ViT is replaced with two succes-\nsive Swin Transformer modules, the window multi-head self attention (W-MSA) module and shifted window \nmulti-head self attention (SW-MSA) module, as illustrated in Fig.  3b. Each unit consists of two normalization \nlayers (LayerNorm), a self-attention module, and an MLP layer. The first unit uses the Window MSA (W-MSA) \nmodule, while the second unit uses the shifted Window MSA (SW-MSA) module. LayerNorm layers are added \nbefore each MSA module and each MLP layer, and the residual connection is employed after each module.\nThe Swin Transformer conducts self-attention on windows to reduce computational complexity. While in \nViT, standard MSA is used for global attention. The relationship between each patch is computed based on all \nother patches. However, the computational complexity is quadratic because of the enormous number of patches, \nFigure 2.  The overall framework of the proposed method.\n5\nVol.:(0123456789)Scientific Reports |         (2023) 13:3637  | https://doi.org/10.1038/s41598-023-30853-z\nwww.nature.com/scientificreports/\nmaking it unsuitable for high-resolution images. For effective modeling, Swin Transformer uses the W-MSA for \ncalculating self-attention within a local window. Where a window is a set of patches that uniformly and non-\noverlappingly split the entire image. Assuming that each window contains M × M patches, the computational \ncomplexities of the global MSA module and W-MSA in an image of h× w patches are as follows.\nwhere h× w represents the number of patches in whole images, and C is the channel of patches channel. In Eq. \n(1), the complexity is quadratic to patch number h× w . While in Eq. ( 2), the complexity of the latter is linear \nwhen M is fixed (set to 7 by default). For a large h× w , global self-attention computation is generally unafford-\nable, whereas window-based self-attention is scalable.\nShifted window for self‑attention. However, the window-based self-attention (W-MSA) lacks cross-window \nconnections, which limits the model’s modeling capabilities. In order to introduce the cross-window connection \nwhile maintaining efficient computation of non-overlapping windows, a shift window partitioning method is \nproposed in the Swin Transformer block. Figure  3d illustrates the shifted window partitioning strategy. In the \nl-th layer of the Swin Transformer, we use the window partitioning strategy for calculating the local attention. \nThe 8 × 8 feature map is uniformly divided into 2 × 2 windows of size 4 × 4 ( M = 4 ). Then, the next layer l + 1 \nadopts the window partitioning configuration from the front layer to generate new windows, by replacing the \nwindow \n(⌊ M\n2\n⌋\n,\n⌊ M\n2\n⌋)\n pixels from the regular partitioned window. The self-attention computation of the new \nwindow crosses the boundary of the previous window in layer l, providing a connection between them. By using \nthe shifted window partitioning strategy, the successive Swin Transformer blocks are calculated as:\n(1)�(MSA) =4hwC2 + 2(hw)2C\n(2)�(W − MSA) =4hwC2 + 2M 2hwC\n(3)ˆzl = W − MSA\n(\nLN\n(\nzl−1\n))\n+ zl−1\nFigure 3.  (a) The overall architecture of Swin Transformer, which is adapted from Liu et al.18. (b) Two \nsuccessive Swin Transformer blocks. (c) The hierarchical structure of Swin Transformer for extracting multi-\nscale feature representation. (d) An illustration of the shifted window strategy for computing self-attention in \nthe Swin Transformer architecture.\n6\nVol:.(1234567890)Scientific Reports |         (2023) 13:3637  | https://doi.org/10.1038/s41598-023-30853-z\nwww.nature.com/scientificreports/\nwhere ˆz l and z l represent the output features of the W-MSA module and MLP in the l layer, ˆz l and z l represent the \noutput features of the W-MSA module and MLP in the l layer. The shift window partitioning method introduces \nthe connection between adjacent non-overlapping windows in the previous layer, which helps to establish the \nrelationship of the model.\nThe window partitioning strategy produces multiple new windows of different sizes, and some of the new \nwindows are smaller than M × M . To calculate self-attention, one typical method is to fill all windows into \nM × M . This method, however, will result in a rise in the number of windows. As shown in Fig. 3d, the number \nof windows increases from 2 × 2 to 3 × 3 after the window transformation strategy, which obviously increases the \ncalculation cost of the model. To alleviate this problem, Swin Transformer proposes an efficient batch computa-\ntion approach of cyclic shifting toward the top-left direction, as illustrated in Fig.  4. After shifting, the window \ncomputed in batches may consist of several windows in the feature map that are not adjacent to each other. \nTherefore, to confine the calculation of self-attention to each sub-window, a masking method is applied. With \nthe cyclic shift, the number of batch windows remains the same as the number of regular window divisions, thus \nimproving computational efficiency.\nLoss function. In this paper, PolyLoss is used to optimize the OCT classification model. PolyLoss is pro-\nposed by Leng et al. 37, which provides a framework for understanding and refining the commonly used cross-\nentropy loss. It allows the importance of multiple polynomial bases to be easily modified based on the targeting \ntasks and datasets. As a result, we use the PolyLoss in this study to automatically change polynomial coefficients \nfor better retinal OCT image classification.\nApplying the Taylor expansion, the cross entropy loss in the bases of (1 − P t)j can be decomposed as\nThe Eq. (7) can be further condensed in the form of ∑∞\nj=1 αj(1 − P t)j , where αj ∈ R+ is the polynomial coef-\nficient and P t is the prediction probability of the target category label. Each polynomial base (1 − P t)j is weighted \nby a corresponding polynomial coefficient αj , allowing us to easily adjust the importance of different bases for \nvarious applications. The PolyLoss is equivalent to the cross-entropy loss when αj = 1/j for all j.\nLeng et al., propose perturbing the leading polynomial coefficients in cross-entropy to reduce the number of \nαj . They substitute the j − th polynomial coefficient in cross entropy loss 1/j with 1/j + εj.\nwhere j ∈ [−1/j ,∞) . N is the number of leading term coefficients to be tuned. PolyLoss experiments found that \ntuning the first polynomial term yields the largest significant gain. As a result, the Eq. (8) can be reduced to:\n(4)zl = MLP\n(\nLN\n(\nˆzl\n))\n+ˆzl,\n(5)ˆzl+1 = SW − MSA\n(\nLN\n(\nzl\n))\n+ zl,\n(6)zl+1 = MLP\n(\nLN\n(\nˆzl+1\n))\n+ˆzl+1\n(7)L Poly =− log(P t) =\n∞∑\nj=1\n1/j(1 − P t)j = (1 − P t) + 1/2(1 − P t) +···\n(8)\nLPoly = (ε1 + 1)(1 − Pt) +···+(εN + 1/N )(1 − Pt)N\n  \nperturbed byεj\n+ 1/(N + 1)(1 − Pt)N +1 +···  \nsame as L CE\n(9)=− log(Pt) +\nN∑\nj=1\nεj(1 − P t)j\n(10)L Poly =− log(Pt) + ε1(1 − P t)\nFigure 4.  Efficient batch computation approach for self-attention in shifted window partitioning.\n7\nVol.:(0123456789)Scientific Reports |         (2023) 13:3637  | https://doi.org/10.1038/s41598-023-30853-z\nwww.nature.com/scientificreports/\nIn this paper, we set ε1 = 2 following the configuration on ImageNet image classification.\nScore-CAM for visual interpretation. Although deep learning has been widely applied in a variety of \nscenarios such as medical image analysis and consultation assistance, the majority of existing deep learning \nnetworks are black box models with low interpretability. However, medical applications have a great demand for \nthe interpretability of deep learning models due to the involvement of ethics and life health. Therefore, decisions \nregarding artificial intelligence applications should be supported by rationales and explanations. Some scholars \nhave proposed post-hoc methods to explain the predicted behavior after the training is completed, such as Sali-\nency  Maps38, guided backpropagation (GuidedBP) 39 and class activation mapping (CAM) 40. In this work, we \nintroduce Score-CAM, a robust and reliable interpretation method, to provide a fair interpretation of the deci-\nsion process. Score-CAM treats the importance of features as a function of the confidence level, thus getting rid \nof the dependence on gradients.\nDefinition: Increase of confidence Given a general function Y = f(X ) that takes an input vector \nX =[ x0 ,x1 ,... ,xn]⊤ and outputs a scalar Y. For a known baseline input Xb , the contribution ci of xi,(i ∈ n − 1]) \ntowards Y is the change of the output by replacing the i − th entry in Xb with x i . Formally,\nwhere Hi is a vector with the same shape of Xb but for each entry h j in H i , h j = I[i = j] and ◦ denotes Hadamard \nProduct.\nWe define the trained Swin Transformer as Y = f(X ) that outputs a class probability scalar Y . We pick the \nsecond normalization layer in the last Swin Transformer block and the corresponding activation as A . Denote \nthe kth channel of activation A as Ak . Therefore, the contribution score Ak towards Y is defined as\nwhere\nUp(·) represents the operation that upsamples Ak into the input size. In this way, each upsampled activation map \nnot only presents the most relevant spatial location to the internal activation map but also can be used directly \nas a mask to disturb the input image. s(·) is a normalization function that maps each element in the activation \nmap matrix into [0, 1], which generates a smoother mask Hk . The normalization function s(·) is represented as\nThen, the final visualization is obtained by a linear combination of weights and activation mappings. In addi-\ntion, ReLU is also applied to the linear combination of mappings, since we are only interested in those features \nthat have a positive impact on the category of interest.\nFinally, we show the visualization in the form of heatmap and apply it to the input image for explaining the \ndecision process.\nImplement details. The experiments are conducted on Linux Ubuntu 16.04, Python 3.6, and Pytorch \n1.11.0. Models are trained on an NVIDIA Tesla V100 GPU. We initialize the weights with Xavier  initialization41 \nand optimize them during training with the Adam optimizer using β1 = 0.900 . The initial learning rate is 2e−4 \nand then decays into 1e−5 lastly. All of the OCT images are resized to 224 × 224 . The batch size was set to 32. \nWe train each model for 200 epochs. The model at the last epoch is used to evaluate performance. Moreover, \nfor the dataset OCT2017, we adopt the weight loss strategy to alleviate the incorrect prediction caused by class \nimbalance.\nEvaluation of classification models. For evaluating the classification performance, we apply the softmax \nmethod to convert logits into class probabilities, and then take the highest probability value as the predicted \ncategory. Accuracy, precision, recall, and F1-score are used as evaluation metrics. The formulas of evaluation \nmetrics are as follows.\n(11)ci = f(X b ◦ H i) − f(X b)\n(12)C(Ak) = f(X ◦ H k) − f(Xb)\n(13)H k = s(Up(Ak))\n(14)s(A k) = A k − minA k\nmaxA k − minA k\n(15)V Score−CAM = ReLU\n(∑\nk\nαc\nkAk\nl\n)\n(16)Accuracy= TP + TN\nTP + FP + TN + FN\n(17)Precision= TP\nTP + FP\n(18)Recall= TP\nTP + FN\n8\nVol:.(1234567890)Scientific Reports |         (2023) 13:3637  | https://doi.org/10.1038/s41598-023-30853-z\nwww.nature.com/scientificreports/\nWhere TP , TN, FP , and FN represent the number of true positives, true negatives, false positives, and false nega-\ntives, respectively. For the four classes OCT classification, TP is defined as the number of cases correctly identified \nas a category, TN as the number of negative cases correctly identified as a negative class by the model, FP as the \nnumber of negative samples incorrectly identified as positive classes, and FN as the number of positive cases \nincorrectly identified as negative categories. In addition, the area under curve (AUC) is an additional metric for \nfurther evaluate the proposed method. The larger the AUC, the closer the prediction is to the true label.\nResults\nResults on each category. In order to observe micro performance, we report the performance of several \nnetworks across each category of OCT2017 and OCT-C8. Table  1 shows the performance of  LLCT17, Vision \nTransformer (ViT), Swin Transformer and our method. For dataset OCT2017, we observe that ViT outperforms \nLLCT in our setting, demonstrating the effectiveness of ViT for the task of OCT image classification. In addition, \nthe performance on CNV and drusen images is further improved when Swin Transformer is used, which means \nthat hierarchical multi-scale features contribute to better predictions. Swin Transformer obtained 1.0000 on four \nmetrics (accuracy, precision, recall and F1 score) for DME and normal images, demonstrating the model’s ability \nto identify DME and normal images. Moreover, the PolyLoss leads to a further increase in classification accu-\nracy, recall, F1-score, and AUC. The suggested method’s average accuracy, precision, recall F1-Score, and AUC \nare 0.9980, 0.9980, 0.9980, 0.9980, and 0.9999, respectively, slightly outperforming the LLCT’s 0.0095, 0.0197, \n0.0215, 0.0057 and 0.0321. Although there is a small improvement in evaluation values, this improvement is \nvisible in the dataset OCT2017, as all evaluation metrics are close to 1. The proposed Swin-Poly Transformer \nachieves the best performance on four metrics, suggesting the effectiveness of the proposed method. Similarly, \nwe validate the proposed method on OCT-C8. For dataset OCT-C8, similarly, the proposed method surpasses \nViT and Swin-VIT to achieve the best average performance. We find that Vit, Swin-Vit and our method all \nachieve high accuracy on AMD. The proposed method achieves performance close to 1 in the four categories of \nAMD, CSR, DR, and MH. Combining CNN with transformers offers a viable improvement direction for local \nand global feature fusion. All in all, the proposed method takes the best performance on average results.\nWe compare the floating-point operations per second (FLOPs), numbers of model parameters and inference \ntime of VGG16, ViT, and our methods. The FLOPS of VGG16, ViT, and our methods are 15.4 G, 1.1 G and 4.5 G \nrespectively. The Parameters of the three methods are 13.8 M, 22.1 M and 27.5 M. In the inference stage, predict-\ning an image spend 2.72 ms, 5.9 ms and 12.6 ms. Although the inference time of our method is greater than that \nof VGG16 and ViT, for an OCT image, this speed is still satisfactory compared to manual reading.\nVisualization. Further, we investigate the model decision-making mechanism in OCT2017. We use the \npost-hoc explanation approach Score-CAM19 to visualize the evidence of prediction. Score-CAM is a gradient-\nfree visual interpretation method, where the importance of activation is encoded by the global contribution of \nthe corresponding input instead of the local sensitivity (gradient information). We perform an interpretation \nexperiment on 968 test images to see which regions contributed the most to the neural network’s prediction \nprognosis.\nFigure 5 shows confidence score maps of the prediction results in OCT2017 and OCT-C8. The heat map \nhighlights the regions that are connected with the target category. The redder the color, the higher the correlation \nwith the predicted category. As can be seen in this figure, the score-CAM clearly shows the regions of interest. \nWe notice that lesion regions are rendered as redder in the disease OCT images, for example, the first three rows \nof Fig. 5a and b right, Fig. 5b left, i.e., abnormal regions are given higher scores. In normal images (the last row \nof Fig. 5a and b right), the model pays more attention to the whole retina. These phenomena are consistent with \nclinical diagnosis, as ophthalmologists also identify diseases by looking at abnormal regions in OCT images.\nDiscussion\nWe develop a Swin-Poly Transformer network to automatically and accurately identify retinal disease types. Using \nOCT images, we investigate the performance improvement of the Swin-Transformer model for retinal abnormal-\nity classification using multi-scale feature representation and loss optimization. Further, visual interpretation \nanalysis is performed to determine whether the lesion areas of the model match the clinical diagnostic features.\nIn this paper, we compare the proposed method in dataset OCT2017, including ViT, Swin Transformer and \nWen et al.17 in Table 1. ViT converts an image to several sequence tokens and then employs Multi-Head Self-\nAttention to model long-range dependencies between tokens. This structure considers the image’s global infor-\nmation, leading to an increase in overall accuracy, sensitivity, and specificity (Table 2). Specifically, Wen et al.17 \nuse the customized feature maps generated by CNN as the input of the self-attention network, exploiting local \ndetails from the CNN and global contextual and dynamic attention from the Transformer. In our experimental \nsetting, the overall F1-score values for ViT, Swin Transformer, and Swin-Poly Transformer are 0.9907, 0.9970, \nand 0.9980 respectively. The performance of the Swin Transformer outperforms the ViT because of the utiliza-\ntion of multi-scale features. Swin Transformer shifts the window partition and then builds connections between \nadjacent non-overlapping Windows, thus combining low-level and high-level features. Furthermore, the Poly \nloss further improves the performance by refining the cross-entropy loss using Taylor expansion. It modifies \na large number of polynomial bases according to the specific task and dataset to regulate the relevance of each \nbasis. In particular, the Swin-Poly Transformer shows an AUC value of 0.9999, demonstrating the effectiveness \n(19)F 1 − score =2 · precision· recall\nprecision+ recall\n9\nVol.:(0123456789)Scientific Reports |         (2023) 13:3637  | https://doi.org/10.1038/s41598-023-30853-z\nwww.nature.com/scientificreports/\nof the proposed method. Experiments show that the accurate diagnosis provided by the proposed Swin-Poly \nTransformer can contribute to precision medicine.\nWe further compare the average performance of the Swin-Poly Transformer and other algorithms, includ-\ning CNN and Transformer-based networks. We explore the performance of CNNs in OCT2017 from multiple \nperspectives, including general training (Lu et al.15 and Bhadra et al.20), transfer learning (Kermany et al.21, Li at \nal.22 and Islam et al. 23), multi-scale/layer-guided feature fusion  (MDFF24 and  LGCNN25), and ensemble learn-\ning (Kim and  Tran26). All results are shown in Table  2. From Table 2, we find CNN  networks24,25,43 are useful \nTable 1.  Experimental results on OCT image classification. Significant values are in [bold].\nDataset Method Class Accuracy Precision Recall F1-Score AUC \nOCT2017\nLLCT17\nCNV 0.9810 0.9350 0.9940 0.9760 0.9960\nDME 0.9960 0.9860 0.9960 0.9950 0.9970\nDrusen 0.9810 0.9960 0.9280 0.9990 0.9190\nNormal 0.9960 0.9960 0.9880 0.9990 0.9590\nAverage 0.9885 0.9783 0.9765 0.9923 0.9678\nViT\nCNV 0.9800 0.9878 1.0000 0.9938 0.9993\nDME 0.9880 0.9918 0.9959 0.9938 0.9990\nDrusen 0.9920 0.9917 0.9876 0.9897 0.9985\nNormal 1.0000 0.9916 0.9793 0.9855 0.9999\nAverage 0.9900 0.9907 0.9907 0.9907 0.9992\nSwin Transformer\nCNV 1.0000 0.9881 1.0000 0.9940 0.9999\nDME 1.0000 1.0000 1.0000 1.0000 0.9995\nDrusen 0.9880 1.0000 0.9880 0.9940 0.9998\nNormal 1.0000 1.0000 1.0000 1.0000 1.0000\nAverage 0.9970 0.9970 0.9970 0.9970 0.9998\nOurs\nCNV 1.0000 0.9960 1.0000 0.9980 1.0000\nDME 0.9960 1.0000 0.9960 0.9980 0.9996\nDrusen 1.0000 0.9960 1.0000 0.9980 1.0000\nNormal 0.9960 1.0000 0.9960 0.9980 1.0000\nAverage 0.9980 0.9980 0.9980 0.9980 0.9999\nOCT-C8\nVit\nAMD 1.0000 0.9972 1.0000 0.9886 1.0000\nCNV 0.8657 0.8511 0.8657 0.8584 0.9845\nCSR 0.9886 0.9971 0.9943 0.9957 0.9999\nDME 0.7771 0.8576 0.7743 0.8138 0.9720\nDR 0.9971 0.9886 0.9886 0.9886 0.9991\nDrusen 0.7429 0.7424 0.7000 0.7206 0.9543\nMH 0.9914 0.9915 0.9943 0.9929 0.9995\nNormal 0.7571 0.7053 0.8000 0.7497 0.9686\nAverage 0.8896 0.8913 0.8896 0.8898 0.9847\nSwin-Vit\nAMD 1.0000 1.0000 1.0000 1.0000 1.0000\nCNV 0.8516 0.8493 0.9657 0.9037 0.9947\nCSR 0.9821 1.0000 0.9971 0.9986 1.0000\nDME 0.9122 0.9324 0.9057 0.9188 0.9933\nDR 0.9836 0.9859 1.0000 0.9929 1.0000\nDrusen 0.9327 0.9708 0.76 0.8526 0.9882\nMH 0.9812 1.0000 0.9886 0.9943 1.0000\nNormal 0.9257 0.8583 0.9514 0.9024 0.9954\nAverage 0.9461 0.9496 0.9461 0.9454 0.9965\nOurs\nAMD 1.0000 1.0000 1.0000 1.0000 1.0000\nCNV 0.9489 0.9389 0.9571 0.9477 0.9937\nCSR 1.0000 1.0000 1.0000 1.0000 1.0000\nDME 0.9439 0.9512 0.9457 0.9484 0.9919\nDR 1.0000 0.9972 1.0000 0.9986 0.9999\nDrusen 0.9200 0.9580 0.9114 0.9341 0.9888\nMH 1.0000 1.0000 0.9971 0.9986 0.9998\nNormal 0.9563 0.9254 0.9571 0.9410 0.9958\nAverage 0.9711 0.9713 0.9711 0.9710 0.9962\n10\nVol:.(1234567890)Scientific Reports |         (2023) 13:3637  | https://doi.org/10.1038/s41598-023-30853-z\nwww.nature.com/scientificreports/\nTable 2.  Performance of several deep learning networks for classification on OCT2017 and OCT-C8. \nSignificant values are in [bold].\nDataset Method Accuracy Recall Precision F1-score\nOct2017\nInception  V342 0.9660 0.9780 0.9740 0.9760\nKermany et al.21 0.9610 0.9612 0.9610 0.9610\nKaymak et al.43 0.9710 0.9960 – –\nMDFF24 0.9960 0.9960 0.9960 0.9960\nLGCNN25 0.8990 – – –\nIslam et al.23 0.9860 – 0.9950 –\nLi et al.22 0.9860 0.9780 0.9940 0.9859\nBhadra et al.20 0.9969 0.9969 0.9969 0.9968\nKim et al.26 0.9890 0.9890 0.9960 0.9915\nSaleh et al.28 0.9850 0.9700 0.9700 0.9700\nLLCT17 0.9770 0.9770 0.9920 0.9844\nViT35 0.9907 0.9907 0.9907 0.9907\nSwin  Transformer18 0.9970 0.9970 0.9970 0.9970\nOurs 0.9980 0.9980 0.9980 0.9980\nOCT-C8\nKarthik et al.32 (ResNet34 based) 0.9240 0.9200 0.9300 0.9200\nKarthik et al.32 (ResNet50 based) 0.9030 0.9100 0.9100 0.9100\nKarthik et al.32 (ResNet101 based) 0.8450 0.8600 0.8600 0.8600\nSubramanian et al.33 (VGG16 based) 0.9721 0.9725 0.9713 0.9725\nViT 0.8896 0.8913 0.8896 0.8898\nSwin-Transformer 0.9461 0.9496 0.9461 0.9454\nOurs 0.9712 0.9713 0.9713 0.9710\nFigure 5.  Confidence score maps on (a) OCT2017 and (b) OCT-C8 of proposed Swin-Poly Transformers.\n11\nVol.:(0123456789)Scientific Reports |         (2023) 13:3637  | https://doi.org/10.1038/s41598-023-30853-z\nwww.nature.com/scientificreports/\nalgorithms for OCT image classification, achieving satisfactory results in OCT2017. Among the CNN-based \nalgorithms, Bhadra et al.20 achieve the best performance with an accuracy of 0.9969, a recall of 0.9969, a preci-\nsion of 0.9969 and an F1-score of 0.9968. These phenomena prove that with enough samples, CNNs are able to \ncapture the subtle differences in each category of fundus OCT images in real  scenes44. For Transformer-based \nbackbones, the Swin Transformer outperforms the ViT on four metrics, suggesting the effectiveness of extract-\ning multi-scale features using a multi-scale hierarchical strategy. Finally, the proposed method achieves the best \nperformance with accuracy, recall, and precision of 0.9980, which indicates that the combination of multi-scale \nfeatures and Poly loss benefits the performance improvement. We show the loss and accuracy curves in Fig. 6a. \nIn the figure, the training loss first decreases gradually and then reaches equilibrium, indicating that the Swin-\nPoly Transformer has been fitted on the training data.\nFurthermore, we verify the effectiveness of the Swin-Poly Transformer on another dataset, OCT-C8. All \nresults are shown in Table 2. The proposed Swin-Poly Transformer exceeds the three ResNet-based models pro-\nposed by Karthik et al32. Moreover, the proposed Swein-Poly transformer achieves comparable performance to \nSubramanian et al.29 and further improves the interpretability of the model. Particularly, the proposed method \nexceeds the classical ViT in four evaluation indexes respectively. In addition, the accuracy, recall, accuracy, and \nF1 scores of Swin-Transformer using vanilla were 0.9461, 0.9496, 0.9461, and 0.9454, respectively. The proposed \nSwin-Poly Transformer achieves an accuracy of 0.9712, a recall of 0.9713, a precision of 0.9713, and an F1-score of \n0.9710, which are 2.52%, 1.17%, 2.49% and 2.56% higher than Swin Transformer, respectively. The performance \nof the proposed method on OCT-C8 proves that the Swin-Poly Transformer is an effective algorithm for OCT \nimage recognition. We show the training and validation accuracy curves in Fig. 6b. It can be found in the figure \nthat Swin Transformer converges faster than ViT. The proposed Swin-Poly Transformer and Swin Transformer \nhave comparable performance on the validation set. Furthermore, the accuracy of the Swin-Poly Transformer is \nhigher than that of the Swin Transformer on test data. Additionally, in the first 50 epochs, the accuracy curve of \nthe Swin-Poly Transformer is smoother than Swin Transformer on the training set. These phenomena suggest \nthat using Poly loss contributes to boosting the generalization and robustness.\nObserving intermediate layers facilitates revealing learned features and understanding the mechanism of \ndecision-making45. Vision interpretability is an evolving area with the potential to help the developer and medi-\ncal participant better understand how models work and gain new insights into revealing predictive  failures46. \nIn this paper, the gradient-free interpretation method Score-CAM is used to visualize the region of interest. We \ndiscover that the suggested model highlights abnormal areas of the image. The confidence score map displays \nthe region around the anomaly in addition to the lesion of interest, indicating that contextual information about \nthe immediate environment may be useful for prediction. The model appears to focus on the entire retinal layer \nfor normal images, demonstrating its flexibility in learning complicated and representative features. Overall, \nthese visualization results are remarkable and intuitive, confirming that the proposed model can appropriately \nidentify regions of interest.\nIn this work, we propose an effective Swin-Poly Transformer for identifying normal OCT images and retinal \nabnormities. The Swin-Poly Transformer network has the potential to transform the currently limited classifica-\ntion model into a more analytical and flexible system, combing radiographic imaging, biological data and clinical \nreports. These approaches contribute to augmenting other emerging technologies, such as liquid biopsy; provid-\ning complementary information to guide clinical decision-making. However, despite the promising progress, the \nchallenge of effectively integrating these computer-assisted diagnostic tools into regular practice remains. Perhaps \nmost pressing is the need for extensive data sharing to build large, well-labeled datasets to develop a robust and \nscalable model. In future work, on the one hand, we expect to utilize complementing information from several \nmodalities to simulate real diagnostic scenarios by combining multi-tasking or collaborative learning. On the \nother hand, we believe that intra- and inter-institutional data sharing will encourage models to perform better \nin real situations.\nFigure 6.  (a) The loss and accuracy curves of the proposed model in OCT2017. (b) The accuracy curves of \ndifferent models on OCT-C8.\n12\nVol:.(1234567890)Scientific Reports |         (2023) 13:3637  | https://doi.org/10.1038/s41598-023-30853-z\nwww.nature.com/scientificreports/\nData availability\nThe dataset analyzed during the current study is available in the Kaggle at https:// www. kaggle. com/ pault imoth \nymoon ey/ kerma ny2018.\nReceived: 22 June 2022; Accepted: 2 March 2023\nReferences\n 1. Li, T. et al. Applications of deep learning in fundus images: A review. Med. Image Anal. 69, 101971 (2021).\n 2. Zhao, J. et al. Emerging trends and research foci in artificial intelligence for retinal diseases: Bibliometric and visualization study. \nJ. Med. Internet Res. 24, e37532 (2022).\n 3. Kim, J. & Tran, L. Retinal disease classification from oct images using deep learning algorithms. In 2021 IEEE Conference on \nComputational Intelligence in Bioinformatics and Computational Biology (CIBCB) 1–6 (IEEE, 2021).\n 4. Schmitz-Valckenberg, S., Holz, F . G., Bird, A. C. & Spaide, R. F . Fundus autofluorescence imaging: Review and perspectives. Retina \n28, 385–409 (2008).\n 5. Varma, R. et al. Prevalence of and risk factors for diabetic macular edema in the united states. JAMA Ophthalmol. 132, 1334–1340 \n(2014).\n 6. Ciulla, T. A., Amador, A. G. & Zinman, B. Diabetic retinopathy and diabetic macular edema: Pathophysiology, screening, and \nnovel therapies. Diabetes Care 26, 2653–2664 (2003).\n 7. Fercher, A. F ., Drexler, W ., Hitzenberger, C. K. & Lasser, T. Optical coherence tomography—principles and applications. Rep. Prog. \nPhys. 66, 239–303 (2003).\n 8. Tsuji, T. et al. Classification of optical coherence tomography images using a capsule network. BMC Ophthalmol. 20, 1–9 (2020).\n 9. Perdomo, O. et al. Classification of diabetes-related retinal diseases using a deep learning approach in optical coherence tomog -\nraphy. Comput. Methods Programs Biomed. 178, 181–189 (2019).\n 10. Kamran, S. A., Tavakkoli, A. & Zuckerbrod, S. L. Improving robustness using joint attention network for detecting retinal degen-\neration from optical coherence tomography images. In 2020 IEEE International Conference On Image Processing (ICIP) 2476–2480 \n(IEEE, 2020).\n 11. Rajagopalan, N., Narasimhan, V ., Kunnavakkam-Vinjimoor, S. & Aiyer, J. Deep cnn framework for retinal disease diagnosis using \noptical coherence tomography images. J. Ambient Intell. Human. Comput. 12, 7569–7580 (2021).\n 12. Song, D. et al. Deep relation transformer for diagnosing glaucoma with optical coherence tomography and visual field function. \nIEEE Trans. Med. Imaging 40, 2392–2402 (2021).\n 13. Jin, K. et al. ierm: An interpretable deep learning system to classify epiretinal membrane for different optical coherence tomography \ndevices: A multi-center analysis. J. Clin. Med. 12, 400 (2023).\n 14. Vidal, P . L., de Moura, J., Novo, J., Penedo, M. G. & Ortega, M. Image-to-image translation with generative adversarial networks \nvia retinal masks for realistic optical coherence tomography imaging of diabetic macular edema disorders. Biomed. Signal Process. \nControl 79, 104098 (2023).\n 15. Lu, W . et al. Deep learning-based automated classification of multi-categorical abnormalities from optical coherence tomography \nimages. Transl. Vis. Sci. Technol. 7, 41–41 (2018).\n 16. Li, F . et al. Deep learning-based automated detection of retinal diseases using optical coherence tomography images. Biomed. Opt. \nExpress 10, 6204–6226 (2019).\n 17. Wen, H. et al. Towards more efficient ophthalmic disease classification and lesion location via convolution transformer index \nterms. Comput. Methods Programs Biomed. 2022, 106832 (2022).\n 18. Liu, Z. et al. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International \nConference on Computer Vision 10012–10022 (2021).\n 19. Wang, H. et al. Score-cam: Score-weighted visual explanations for convolutional neural networks. In Proceedings of the IEEE/CVF \nconference on computer vision and pattern recognition workshops 24–25 (2020).\n 20. Bhadra, R. & Kar, S. Retinal disease classification from optical coherence tomographical scans using multilayered convolution \nneural network. In 2020 IEEE Applied Signal Processing Conference (ASPCON) 212–216 (IEEE, 2020).\n 21. Kermany, D. S. et al. Identifying medical diagnoses and treatable diseases by image-based deep learning. Cell  172, 1122–1131 \n(2018).\n 22. Li, F ., Chen, H., Liu, Z., Zhang, X. & Wu, Z. Fully automated detection of retinal disorders by image-based deep learning. Graefes \nArch. Clin. Exp. Ophthalmol. 257, 495–505 (2019).\n 23. Islam, K. T., Wijewickrema, S. & O’Leary, S. Identifying diabetic retinopathy from oct images using deep transfer learning with \nartificial neural networks. In 2019 IEEE 32nd International Symposium on Computer ‑Based Medical Systems (CBMS) 281–286 \n(IEEE, 2019).\n 24. Das, V ., Dandapat, S. & Bora, P . K. Multi-scale deep feature fusion for automated classification of macular pathologies from oct \nimages. Biomed. Signal Process. Control 54, 101605 (2019).\n 25. Huang, L., He, X., Fang, L., Rabbani, H. & Chen, X. Automatic classification of retinal optical coherence tomography images with \nlayer guided convolutional neural network. IEEE Signal Process. Lett. 26, 1026–1030. https:// doi. org/ 10. 1109/ LSP . 2019. 29177 79 \n(2019).\n 26. Kim, J. & Tran, L. Ensemble learning based on convolutional neural networks for the classification of retinal diseases from optical \ncoherence tomography images. In 2020 IEEE 33rd International Symposium on Computer‑Based Medical Systems (CBMS) 532–537 \n(IEEE, 2020).\n 27. Alqudah, A. M. Aoct-net: A convolutional network automated classification of multiclass retinal diseases using spectral-domain \noptical coherence tomography images. Med. Biol. Eng. Comput. 58, 41–53 (2020).\n 28. Saleh, N., Abdel-Wahed, M. & Salaheldin, A. M. Transfer learning-based platform for detecting multi-classification retinal disorders \nusing optical coherence tomography images. Int. J. Imaging Syst. Technol. 32, 740–752 (2022).\n 29. Subramanian, M. et al. Diagnosis of retinal diseases based on bayesian optimization deep learning network using optical coherence \ntomography images. Comput. Intell. Neurosci. 2022, 56 (2022).\n 30. Saleh, N., Wahed, M. A. & Salaheldin, A. M. Computer-aided diagnosis system for retinal disorder classification using optical \ncoherence tomography images. Biomed. Eng. 2022, 459 (2022).\n 31. Salaheldin, A. M., Abdel Wahed, M. & Saleh, N. Machine learning-based platform for classification of retinal disorders using \noptical coherence tomography images. In Artificial Intelligence and Sustainable Computing 269–283 (Springer, 2022).\n 32. Karthik, K. & Mahadevappa, M. Convolution neural networks for optical coherence tomography (oct) image classification. Biomed. \nSignal Process. Control 79, 104176 (2023).\n 33. Subramanian, M., Shanmugavadivel, K., Naren, O., Premkumar, K. & Rankish, K. Classification of retinal oct images using deep \nlearning. Int. Conf. Comput. Commun. Inf. 1–7, 2022. https:// doi. org/ 10. 1109/ ICCCI 54379. 2022. 97409 85 (2022).\n 34. Shorten, C. & Khoshgoftaar, T. M. A survey on image data augmentation for deep learning. J. Big Data 6, 1–48 (2019).\n 35. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv: 2010. 11929 (2020).\n13\nVol.:(0123456789)Scientific Reports |         (2023) 13:3637  | https://doi.org/10.1038/s41598-023-30853-z\nwww.nature.com/scientificreports/\n 36. Matsoukas, C., Haslum, J. F ., Söderberg, M. & Smith, K. Is it time to replace cnns with transformers for medical images? arXiv:  \n2108. 09038 (2021).\n 37. Leng, Z. et al. Polyloss: A polynomial expansion perspective of classification loss functions. arXiv:  2204. 12511 (2022).\n 38. Simonyan, K., Vedaldi, A. & Zisserman, A. Deep inside convolutional networks: Visualising image classification models and sali-\nency maps. CoRR 2014, 25 (2014).\n 39. Springenberg, J. T., Dosovitskiy, A., Brox, T. & Riedmiller, M. Striving for simplicity: The all convolutional net. arXiv:  1412. 6806 \n(2014).\n 40. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A. & Torralba, A. Learning deep features for discriminative localization. In Proceedings \nof the IEEE conference on computer vision and pattern recognition, 2921–2929 (2016).\n 41. Glorot, X. & Bengio, Y . Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth \nInternational Conference on Artificial Intelligence and Statistics 249–256 (JMLR Workshop and Conference Proceedings, 2010).\n 42. Szegedy, C., Vanhoucke, V ., Ioffe, S., Shlens, J. & Wojna, Z. Rethinking the inception architecture for computer vision. In Proceed‑\nings of the IEEE Conference on Computer Vision and Pattern Recognition 2818–2826 (2016).\n 43. Kaymak, S. & Serener, A. Automated age-related macular degeneration and diabetic macular edema detection on oct images using \ndeep learning. In 2018 IEEE 14th International Conference on Intelligent Computer Communication and Processing (ICCP) 265–269 \n(IEEE, 2018).\n 44. Kareem, F . Q. & Abdulazeez, A. M. Ultrasound medical images classification based on deep learning algorithms: A review. Fusion \nPract. Appl. 3, 29–42 (2021).\n 45. Gilpin, L. H. et al. Explaining explanations: An overview of interpretability of machine learning. 2018 IEEE 5th International \nConference on Data Science and Advanced Analytics (DSAA) 80–89 (2018).\n 46. Wang, J. et al. Information bottleneck-based interpretable multitask network for breast cancer classification and segmentation. \nMed. Image Anal. 83, 102687 (2023).\nAcknowledgements\nThis research is supported by the Shandong Provincial Natural Science Foundation (ZR2021MH237) China; \nShandong Provincial Natural Science Foundation joint Fund (ZR2021LZL011); National Natural Science Foun-\ndation of China (61902225).\nAuthor contributions\nJ.H. conceived the experiments, J.W . and Z.H. conducted the experiments and wrote the original draft, J.M. and \nC.W . analyzed the results, and M.Q modified the manuscript. All authors reviewed the manuscript.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to J.H. or M.Q.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023"
}