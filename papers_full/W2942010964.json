{
  "title": "Joint Intensity Transformer Network for Gait Recognition Robust Against Clothing and Carrying Status",
  "url": "https://openalex.org/W2942010964",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2068388382",
      "name": "Xiang Li",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A272119250",
      "name": "Yasushi Makihara",
      "affiliations": [
        "Osaka Research Institute of Industrial Science and Technology",
        "Osaka University"
      ]
    },
    {
      "id": "https://openalex.org/A2112140162",
      "name": "Chi Xu",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2094158490",
      "name": "Yasushi Yagi",
      "affiliations": [
        "Osaka Research Institute of Industrial Science and Technology",
        "Osaka University"
      ]
    },
    {
      "id": "https://openalex.org/A2130645578",
      "name": "Mingwu Ren",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2068388382",
      "name": "Xiang Li",
      "affiliations": [
        "Osaka Research Institute of Industrial Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A272119250",
      "name": "Yasushi Makihara",
      "affiliations": [
        "Osaka Research Institute of Industrial Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2112140162",
      "name": "Chi Xu",
      "affiliations": [
        "Osaka Research Institute of Industrial Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2094158490",
      "name": "Yasushi Yagi",
      "affiliations": [
        "Osaka Research Institute of Industrial Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2130645578",
      "name": "Mingwu Ren",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2803380720",
    "https://openalex.org/W2802841625",
    "https://openalex.org/W2745659361",
    "https://openalex.org/W2887929752",
    "https://openalex.org/W6734522614",
    "https://openalex.org/W2151066636",
    "https://openalex.org/W2739325416",
    "https://openalex.org/W2587215467",
    "https://openalex.org/W2407362091",
    "https://openalex.org/W2757611655",
    "https://openalex.org/W2068222852",
    "https://openalex.org/W2556892663",
    "https://openalex.org/W2102742858",
    "https://openalex.org/W2133956100",
    "https://openalex.org/W2080982032",
    "https://openalex.org/W6681582814",
    "https://openalex.org/W2091451699",
    "https://openalex.org/W2072615210",
    "https://openalex.org/W2963589138",
    "https://openalex.org/W2107253934",
    "https://openalex.org/W2085058513",
    "https://openalex.org/W1896402587",
    "https://openalex.org/W2059901520",
    "https://openalex.org/W2510190030",
    "https://openalex.org/W2517225990",
    "https://openalex.org/W2760814882",
    "https://openalex.org/W2322772590",
    "https://openalex.org/W2030401579",
    "https://openalex.org/W2118036996",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W6674330103",
    "https://openalex.org/W2157364932",
    "https://openalex.org/W2072510697",
    "https://openalex.org/W2515018387",
    "https://openalex.org/W1975517671",
    "https://openalex.org/W2138621090",
    "https://openalex.org/W2126680226",
    "https://openalex.org/W2542803194",
    "https://openalex.org/W2899378177",
    "https://openalex.org/W2011058239",
    "https://openalex.org/W2287234120",
    "https://openalex.org/W2754666677",
    "https://openalex.org/W1561782558",
    "https://openalex.org/W2026800967",
    "https://openalex.org/W2154624311",
    "https://openalex.org/W1974775426",
    "https://openalex.org/W2068715223",
    "https://openalex.org/W2104475159",
    "https://openalex.org/W2104035329",
    "https://openalex.org/W2115203491",
    "https://openalex.org/W2154663802",
    "https://openalex.org/W2151458682",
    "https://openalex.org/W2154171558",
    "https://openalex.org/W2149516292",
    "https://openalex.org/W2158461740",
    "https://openalex.org/W1984031350",
    "https://openalex.org/W2149527187",
    "https://openalex.org/W1972580115",
    "https://openalex.org/W6618372016",
    "https://openalex.org/W2807461033",
    "https://openalex.org/W2040270931",
    "https://openalex.org/W2807624910",
    "https://openalex.org/W603908379",
    "https://openalex.org/W1986895792",
    "https://openalex.org/W2950094539",
    "https://openalex.org/W2593351588",
    "https://openalex.org/W2950096404",
    "https://openalex.org/W2563705555",
    "https://openalex.org/W2146278756",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2095705004"
  ],
  "abstract": "Clothing and carrying status variations are the two key factors that affect the performance of gait recognition because people usually wear various clothes and carry all kinds of objects, while walking in their daily life. These covariates substantially affect the intensities within conventional gait representations such as gait energy images. Hence, to properly compare a pair of input gait features, an appropriate metric for joint intensity is needed in addition to the conventional spatial metric. We therefore propose a unified joint intensity transformer network for gait recognition that is robust against various clothing and carrying statuses. Specifically, the joint intensity transformer network is a unified deep learning-based architecture containing three parts: a joint intensity metric estimation net, a joint intensity transformer, and a discrimination network. First, the joint intensity metric estimation net uses a well-designed encoder-decoder network to estimate a sample-dependent joint intensity metric for a pair of input gait energy images. Subsequently, a joint intensity transformer module outputs the spatial dissimilarity of two gait energy images using the metric learned by the joint intensity metric estimation net. Third, the discrimination network is a generic convolution neural network for gait recognition. In addition, the joint intensity transformer network is designed with different loss functions depending on the gait recognition task (i.e., a contrastive loss function for the verification task and a triplet loss function for the identification task). The experiments on the world's largest datasets containing various clothing and carrying statuses demonstrate the state-of-the-art performance of the proposed method.",
  "full_text": "3102 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY , VOL. 14, NO. 12, DECEMBER 2019\nJoint Intensity Transformer Network for\nGait Recognition Robust Against\nClothing and Carrying Status\nXiang Li , Yasushi Makihara, Chi Xu , Yasushi Yagi , Member, IEEE, and Mingwu Ren\nAbstract— Clothing and carrying status variations are the\ntwo key factors that affect the performance of gait recognition\nbecause people usually wear various clothes and carry all kinds\nof objects, while walking in their daily life. These covariates\nsubstantially affect the intensi ties within conventional gait\nrepresentations such as gait energy images. Hence, to properly\ncompare a pair of input gait features, an appropriate metric\nfor joint intensity is needed in addition to the conventional\nspatial metric. We therefore propose a uniﬁed joint intensity\ntransformer network for gait recognition that is robust against\nvarious clothing and carrying statuses. Speciﬁcally, the joint\nintensity transformer network is a uniﬁed deep learning-based\narchitecture containing three parts: a joint intensity metric\nestimation net, a joint intensity transformer, and a discrimination\nnetwork. First, the joint intensity metric estimation net uses a\nwell-designed encoder-decoder network to estimate a sample-\ndependent joint intensity metric for a pair of input gait energy\nimages. Subsequently, a joint intensity transformer module\noutputs the spatial dissimilarity of two gait energy images using\nthe metric learned by the joint intensity metric estimation net.\nThird, the discrimination network is a generic convolution neural\nnetwork for gait recognition. In addition, the joint intensity\ntransformer network is designed with different loss functions\ndepending on the gait recognition task (i.e., a contrastive loss\nfunction for the veriﬁcation task and a triplet loss function for\nthe identiﬁcation task). The experiments on the world’s largest\ndatasets containing various clothing and carrying statuses demon-\nstrate the state-of-the-art performance of the proposed method.\nIndex Terms— Joint intensity transformer network, joint inten-\nsity metric learning, gait recognition.\nI. I NTRODUCTION\nG\nAIT is an important biomet ric that cannot be replaced\nby other biometrics (e.g., ﬁngerprints, vein patterns,\nManuscript received September 20, 2018; revised February 20, 2019;\naccepted April 16, 2019. Date of publication April 22, 2019; date of current\nversion July 31, 2019. This work was supported in part by the Japan Society\nfor the Promotion of Science Grants-in-Aid for Scientiﬁc Research (A)\nJP18H04115, in part by the National R&D Program for Major Research\nInstruments under Grant 61727802, and i n part by the National Natural\nScience Foundation of China under G rant 61703209. The associate editor\ncoordinating the review of this manuscript and approving it for publication\nwas Dr. Julian Fierrez. (Corresponding author: Xiang Li.)\nX. Li and C. Xu are with the School of Computer Science and Engi-\nneering, Nanjing University of Sc ience and Technology, Nanjing 210094,\nChina, and also with the Institute of Sc ientiﬁc and Industrial Research,\nOsaka University, Osaka 567-0046, Japan (e-mail: lixiangmzlx@gmail.com;\nxuchisherry@gmail.com).\nY . Makihara and Y . Yagi are with the Institute of Scientiﬁc and\nIndustrial Research, Osaka University, Osaka 567-0046, Japan (e-mail:\nmakihara@am.sanken.osaka-u.ac.jp; yagi@am.sanken.osaka-u.ac.jp).\nM. Ren is with the School of Computer Sc ience and Engineering, Nanjing\nUniversity of Science and Technology, Nanjing 210094, China (e-mail:\nrenmingwu@mail.njust.edu.cn).\nDigital Object Identiﬁer 10.1109/TIFS.2019.2912577\nirises, or the face), because it is available even at a long dis-\ntance with low image resolution. Because it is an unconscious\nbehavior, people usually do not conceal their gait intentionally.\nTherefore, gait-based human recognition has attracted increas-\ningly more attention by researchers for many applications\nsuch as surveillance systems, forensics, and criminal investiga-\ntions [1]–[3]. The approaches to gait recognition in the litera-\nture are divided into two main categories: model-based [4]–[7]\nand appearance-based [8]–[12] approaches. While the former\none usually requires high-resolution videos to ﬁt a human\nmodel, the latter is more popular for relatively low-resolution\nvideos.\nFor appearance-based approaches, the gait representations\nmainly include motion-based features [13], [14] and silhouette-\nbased features (such as gait energy images (GEIs) [10],\nfrequency-domain features [15], chrono-gait images [16], and\nGabor GEIs [17]), where the latter type is more popular\nbecause of its simple yet effective properties. In particular,\nGEIs, also known as average silhouettes [18], are widely used\nin many studies. However, these appearance-based gait repre-\nsentations are easily changed by many covariates (e.g., view,\nclothing, and carrying status) resulting in large intrasubject dif-\nference, which greatly affect s the performance of recognition.\nAlthough most researchers mainly investigate view angle\nvariations [19]–[24], clothing and carrying status variations are\nalso very common in our daily life. This is because people usu-\nally walk while carrying different kinds of bags or other items.\nMoreover, they often change their clothes as the temperature\nchanges. Therefore, gait recognition techniques robust against\nclothing and carrying status are also of great importance.\nTraditional approaches to maintain the robustness of gait\nrecognition against covariates fall into two families: spatial\nmetric learning-based approaches and intensity transformation-\nbased approaches. The former one concentrates on learning\nmore discriminant features from original spaces and con-\ntaining whole-based metric learning approaches (such as lin-\near discriminant analysis (LDA) [10], discriminant analysis\nwith tensor representation (DATER) [25], the random sub-\nspace method (RSM) [26], [27]), and part-based approa-\nches [28]–[30], which decompose the holistic features into\nmultiple body part-dependent features, then enhance or atten-\nuate parts based on how they are inﬂuenced by covariates.\nHowever, spatial positions are inﬂuenced by covariates such as\nclothing and carrying status quite differently depending on the\ninstance. Hence, it is insufﬁcien t to deal with these covariates\nusing spatial metric learning techniques alone.\n1556-6013 © 2019 I EEE. Translations and c ontent mining are perm itted for academic research only. Personal use is also permitted,\nbut republication/redistribution r equires IEEE permission. See http://www.ieee.org/publicat ions_standards/publications/rights/index.html for more information.\nLI et al.: JITN FOR GAIT RECOGNITION ROBUST AGAINST CLOTHING AND CARRYING STATUS 3103\nFig. 1. Meaning of joint intensity metric learning. The joint intensity\nmetric learning is proposed to ﬁnd a proper metric that can reduce the\ndissimilarity of the joint intensities that come from intrasubject clothes/carried\nobject difference while enhancing the dissimilarity of the joint intensities\nthat come from intersubject motion difference. Because a conventional joint\nintensity metric (i.e., l\n1-norm), that returns a large dissimilarity for the\nintrasubject clothes/carried object difference (e.g., intensity level 255 vs. 0)\nand a small dissimilarity for the intersubject motion difference (e.g., intensity\nlevel 120 vs. 150), may result in a false match.\nIn contrast, intensity transformation-based approaches focus\nmore on the feature representation aspect. They transform the\nintensity values of an original gait feature (gait energies in\nthe case of GEI) into more discriminative values to increase the\nrobustness against covariates. Because clothing and carrying\nstatus variations mainly affect the static components of human\ngait (e.g., a backpack and coat w ill change the torso and limb\nshapes) and partly affect the dynamic components of human\ngait (e.g., a dress can hide the leg motion and a handbag will\naffect the hand motion) during people’s walking period, inten-\nsity transformation-based approaches are generally designed to\nenhance the effect of dynamic components while reducing the\neffect of static components. Typical approaches include hand-\ncrafted transformations like gait entropy image (GEnI) [11]\nand masked GEIs [12] as well as training-based transformation\nsuch as gait energy response functions [31], [32]. Recently,\ninstead of transforming a single GEI, Makihara et al. [33]\nproposed a joint intensity metric learning-based method that\nfocused on the joint intensity transformation of a pair of GEIs,\nwhich reduces the large intras ubject differences and leverages\nthe subtle intersubject differences, as shown in Fig. 1. Through\ntransforming gait energies, these intensity transformation-\nbased approaches show their unique advantages dealing with\nthe variations compared with spatial metric learning. Addition-\nally, they can be easily combined with spatial metric learning\ntechniques to boost performance.\nIn recent years, thanks to the great success of deep learning\ntechniques, many approaches [14], [21]–[24], [34]–[41] have\nbeen proposed in the gait recogn ition community that signiﬁ-\ncantly improve on the performan ce of traditional approaches.\nHowever, they all employ various types of spatial metric\nlearning while ignoring intensity metric learning. Currently,\nthere are no deep learning-based methods that employ intensity\nmetric learning.\nThere is an existing work called the spatial transformer\nnetwork [42] that regresses afﬁne transformation parameters\nfor spatial transformation to distorted digits. Inspired by this,\nwe propose a new architecture to deal with the joint intensity\ntransformation of a pair of GEIs for joint intensity metric\nlearning. Compared with [33], which learns a ﬁxed joint\nintensity metric using a framework consisting of a linear\nsupport vector machine (SVM), the proposed method utilizes\ndeep learning networks to learn a sample-dependent joint\nintensity metric for intensity transformation that is more suit-\nable for various clothing and carrying status types (appearing\nin different positions depending on the instance). For example,\nin the case of a relatively small variation, an incrementally\nmodulated joint intensity metric is estimated, whereas for\na large variation such as a large carried object, the large\nintrasubject difference is strongly suppressed while subtle\nmotion differences are strongly enhanced. Finally, through the\nlearned sample-dependent joint intensity metric, the proposed\nmethod can adaptively handle the intrasubject differences of\nthe same subject pair caused by clothing and carrying status\nvariations as well as the inters ubject differences of different\nsubject pairs caused by motion difference, which results in\nbetter recognition performance.\nIn this paper, we propose the uniﬁed joint intensity trans-\nformer network (JITN) for gait recognition that is robust\nagainst various clothing and carrying statuses and takes both\nspatial and intensity metric learning into consideration. To the\nbest of our knowledge, this is the ﬁrst work integrating\njoint intensity metric learning into a deep learning-based\nframework. Speciﬁcally, JITN is a uniﬁed CNN-based archi-\ntecture containing three parts, i.e., a joint intensity metric\nestimation net (JIMEN), a joint intensity transformer, and\na discrimination network (DN). More details are given in\nSection III. The contributions of this paper are summarized as\nfollows:\nA. A Uniﬁed CNN-Based Method Considering Both Joint\nIntensity and Spatial Metric Learning\nThe proposed JITN is a uniﬁed CNN-based method consid-\nering both joint intensity and spatial metric learning. It con-\ntains a JIMEN, a joint intensity transformer, and a DN, where\nthe JIMEN is a well-designed encoder-decoder network to esti-\nmate the joint intensity metric, the joint intensity transformer is\na transformation module, and the DN is a generic convolution\nneural network to learn the spatial metric. They are jointly\ntrained from end to end.\nB. Sample-Dependent Joint Intensity Metric for Intensity\nTransformation\nUnlike the ﬁxed joint intensity metric learned in [33],\nthe joint intensity metric learned by the JIMEN performs a\nsample-dependent transformation based on the input pairs,\nwhich is more suitable for dealing with all kinds of\nvariations in clothing and carrying status than traditional\napproaches, which perform common sample-independent\ntransformations.\nC. State-of-the-Art Performance\nWe achieve state-of-the-art performance on gait recognition\nunder variations in clothing and carrying status on four pub-\nlicly available gait databases: the OU-ISIR Large Population\nGait database with real-life carried objects (OU-LP-Bag) [43],\n3104 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY , VOL. 14, NO. 12, DECEMBER 2019\nthe OU-ISIR Gait database, Large Population dataset with bags\nβ version (OU-LP-Bag β) [33], the OU-ISIR Gait Database,\nTreadmill Dataset B (OUTD-B) [44] and the TUM Gait from\nAudio, Image and Depth Database (TUM-GAID) [45].\nII. R ELATED WORK\nA. Spatial Metric Learning-Based Approaches\nSpatial metric learning-base d approaches concentrate on\nimproving performance by learning a feature space from the\noriginal appearance-based feat ures that is more discriminant\nand robust against the covariates. There are two further cate-\ngories within this family: whole-based [10]–[12], [25]–[27],\n[46] and part-based approaches [28]–[30], [47].\nFor the whole-based approaches, the holistic appearance-\nbased features are projected in to a discriminative space to\nmake them more robust against the covariate conditions. For\nexample, Han and Bhanu [10] applied LDA to real and syn-\nthesized GEI templates to reduce intraclass variations to some\nextent. A RSM framework that combines multiple inductive\nbiases also was proposed in [26], [27].\nThe part-based approaches decompose the holistic\nappearance-based features in to multiple body part-dependent\nfeatures and enhance the parts effective for recognition while\nattenuating the parts affected by the covariate conditions.\nThis is because variations such as clothing and carrying\nstatus usually affect not the whole gait but only certain\nparts, and a decrease in accuracy is derived mainly from\nthe affected parts. Thus, the part-based approaches have the\npotential to achieve better accuracy by appropriate treatment\nof the affected body parts (e.g., reducing the weights of the\naffected body parts for recogn ition). For example, in [28],\nthe human body was divided into eight sections based on\nanatomical knowledge and the effect of clothing variations\nwas mitigated by adaptively assigning larger and smaller\nweights to the affected and unaff ected sections, respectively.\nIwashita et al. [29] divided the human body into several\nareas equally and then estimated a comparison weight for\neach area. Weights were based on the similarity between\nthe extracted features and those in the database for standard\nclothing.\nB. Intensity Transformation-Based Approaches\nIntensity transformation-based approaches transform the\nintensity values of an original gait feature into more dis-\ncriminative values to increase the robustness against changes\nof the covariate conditions. For example, Bashir et al. [11]\ncomputed the GEnI using the Shannon entropy of the fore-\nground probability at each pixel (i.e., the gait energy in the\nGEI). A GEnI encodes the randomness of pixel values in the\nsilhouette images over a complete gait cycle, thereby captur-\ning more motion information (dynamic components) rather\nthan static information, which improves robustness against\nshape changes (e.g., clothing and carrying status). Masked\nGEI [12] is another intensity transformation-based approach\nthat keeps the dynamic components as their original values but\nzero-pads the static components (i.e., almost all foreground\nand almost all background parts) using a certain threshold.\nInstead of using hand-crafted transformation, Li et al. [31]\nproposed a gait energy response function that transformed\nintensities in a data-driven way. Recently, Makihara et al. [33]\nproposed a joint intensity transformation-based method that\nfocused on the joint intensity transformation of a pair images\ninstead of a single one. Speciﬁcally, the joint intensity met-\nric was alternately learned in conjunction with a spatial\nmetric in a framework based on a linear SVM. However,\nthese approaches all use traditional methods (hand-crafted\ndesign or linear optimization) to perform sample-independent\ntransformations.\nC. Deep Learning-Based Approaches\nMany studies on deep learning-based gait recognition have\nbeen published recently [14], [21]–[24], [34]–[41]. For exam-\nple, the work [39] is a survey on deep learning for biometrics\nincluding gait. Wolf et al. [21] designed a 3D CNN model\nthat regarded raw silhouettes from each gait sequence as\na spatiotemporal input. Battistone et al. [38] proposed a\ntime-based graph deep learning approach to jointly exploit\nthe temporal information and skeleton data extracted from\nsilhouettes. Shiraga et al. [22] designed an eight-layered CNN\nnetwork called GEINet using averaged silhouettes (i.e., GEI).\nThese networks all regard gait recognition as person classiﬁ-\ncation from the same gait class. In addition, Wu et al. [23]\ndesigned multiple networks with two input GEIs (i.e., a pair\nconsisting of a probe ( query ) and gallery ( enrollment )G E I )\nby considering layers to start the comparison of the input pair.\nTakemura et al. [24] discussed input/output architectures for\nCNN-based gait recognition. Their networks attempt to learn\nthe similarity between input GEIs, then determine whether\nthey come from the same person or not. In [36], a stacked\nauto-encoder was used to ﬁnd invariant gait features that were\nrobust against multiple covariates. In [37], [41], generative\nadversarial networks were utilized for generating feature maps\nwithout covariates. Except for silhouette-based feature GEIs,\nmotion features (e.g., optical ﬂow maps) were also used in\nsome approaches [14], [34]. Additionally, apart from person\nauthentication or identiﬁcation, Liu et al. [40] also investigated\ndeep learning-based approaches on gait-based gender recog-\nnition under clothing and carry ing status variations. All of\nthese approaches achieved signiﬁcant improvements compared\nwith traditional approaches. However, they all employ various\ntypes of spatial metric learning while ignoring intensity metric\nlearning, which is another helpful technique for dealing with\nthe covariates of clothing and carrying status.\nIII. G\nAIT RECOGNITION USING JITN\nA. Overview\nIn this paper, we choose to use the GEI feature, which is\nthe most widely employed gait representation in many works\nincluding traditional approaches [10], [26]–[28], [33] and deep\nlearning-based approaches [22]–[24], [35]. To generate a GEI,\ngiven a raw video sequence of a subject, we ﬁrst extract human\nsilhouettes using a background subtraction-based graph-cut\nsegmentation [48] or recent deep learning-based semantic seg-\nmentation methods such as ReﬁneNet [49]; second, we obtain\nLI et al.: JITN FOR GAIT RECOGNITION ROBUST AGAINST CLOTHING AND CARRYING STATUS 3105\nFig. 2. Joint intensity transformer network (JITN) for gait recognition. (a ) Overview of the proposed JITN framework, which consists of a joint inten sity\nmetric estimation net (JIMEN), joint intensity transformer, and a discri mination network (DN) trained from end- to-end. Conv, DeConv, ReLU, Norm, M AX-\npooling, dropout, and Fc denote the convol utional layer, deconvolutional (transposed convol utional) layer, ReLU activation layer, normalizatio n layer, max\npooling layer, dropout layer, and fully connected la yer respectively. The L2 norm is the L2 norm of the output feature at the previous Fc layer. The numb ers\nin brackets written after Conv, DeConv, and MAX-pooling indicate (kernel height × kernel width / stride). (b) For different recognition tasks, we deﬁne\ndifferent JITN architectures. The left architecture is for the veriﬁcation task with a contrastive loss function, and the right architecture is for t he identiﬁcation\ntask with a triplet loss function.\nsize-normalized and registered silhouettes [15] based on the\nextracted region’s height and center; third, we detect a\ngait period by maximizing the auto-correlation of the size-\nnormalized and registered silhouettes; ﬁnally, we average the\nsilhouettes over one gait period to obtain a GEI.\nThe network architecture of the proposed JITN is shown\nin Fig. 2 (a). It consists of a JIMEN, joint intensity transformer,\nand a DN trained from end-to-end. Given a probe and gallery\nGEI pair, we ﬁrst estimate the joint intensity metric using\nthe JIMEN. Then, the joint intensity transformer generates\nthe spatial dissimilarity feature map of the original probe and\ngallery using the estimated joint intensity metric. Finally, for\nspatial metric learning, the spatial dissimilarity feature map\nis fed into a DN, which outputs the ﬁnal dissimilarity of the\nprobe and gallery. More details of the modules are given in\nthe rest of the section.\nB. Joint Intensity Transformer\nIn this subsection, we ﬁrst introduce the concept of joint\nintensity metric learning, which was proposed in [33]. Given\na pair of gray-scale images I\nP and IG with a resolution of H ×\nW (height by width), their dissimilarity measure, incorporating\nthe joint intensity metric, is represented as\nD(I P ,IG ; wI ) =\nH∑\ni=1\nW∑\nj=1\nwI (I P\ni,j , I G\ni,j ), (1)\nwhere wI (I P\ni,j , I G\ni,j ) is a spatially independent dissimilarity\nmetric for joint intensity (I P\ni,j , I G\ni,j ) at position (i, j).I no t h e r\nwords, wI ∈ R(Imax+1)×(Imax+1) can be regarded as a two-\ndimensional look-up table from intensity pairs to dissimilar-\nities, where I\nmax is the maximum gray value and usually is\n3106 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY , VOL. 14, NO. 12, DECEMBER 2019\n255 for 8-bit gray images. For example, thel1-norm is a typical\njoint intensity metric, i.e., wI (I P\ni,j , I G\ni,j ) =| I P\ni,j − I G\ni,j |.\nUsing this metric, we can more ﬂexibly design joint inten-\nsities, whereas traditional linear or quadratic metrics (e.g., the\nl1-norm or Mahalanobis distance) are limited to monotonically\nincreasing metrics as the absolute difference of joint intensities\nincreases. Although monotonically increasing properties are\ngenerally reasonable and exploited in most image matching\nalgorithms, they are not always suitable under certain circum-\nstances in gait recognition.\nGiven joint intensity metric w\nI , along with input probe GEI\nIP and gallery GEI IG , a joint intensity transformer outputs\na dissimilarity map T , whose value Ti,j at position (i, j) is\nwritten as\nTi,j = wI,(pi,j ,gi,j ) =\nImax∑\nk=0\nImax∑\nl=0\nδk,pi,j δl,gi,j wI,(k,l), (2)\nwhere δa,b is Kronecker’s delta. Note that this computation\nis also regarded as forward propagation in the proposed deep\nneural network framework.\nFor computing a backward propagation of the loss through\nthis joint intensity transformer, we deﬁne the partial derivative\nof T\ni,j with respect to wI,(k,l) as follows:\n∂Ti,j\n∂wI,(k,l)\n= δk,pi,j δl,gi,j . (3)\nWe further consider a downsampled joint intensity metric\n(e.g., 32 × 32) instead of a joint intensity metric with the full\nsize of intensities (i.e., 256 × 256), because a joint intensity\nmetric that is larger in size may need complex regression\nmodels and a relatively long time for computation. To do this,\nwe introduce N control points distributed over intensity levels\nfrom 0 to Imax in both the probe and gallery at a certain interval\n( Imax\nN−1 ) and estimate the weights at intermediate intensities by\nbilinear interpolation from the adjacent control points. Note\nthat bilinear inter polation is carried out along the diagonal\nand anti-diagonal directions b ecause the joint intensity metric\n(e.g., the l\n1-norm) is usually symmetric along the diagonal\ndirection, as shown in Fig. 3.\nSuppose wd\nI ∈ RN×N is the downsampled joint intensity\nmetric. Given a position (k,l) of the original joint intensity\nmetric, we compute the weight wI,(k,l) by bilinear interpola-\ntion from a quadruplet of ad jacent control points in wd\nI as\nwI,(k,l) = ck,l ((1 − ak,l )wd\nI,(mk ,nl ) + ak,l wd\nI,(mk +1,nl +1))\n+(1 − ck,l )((1 − bk,l )wd\nI,(mk ,nl +1)\n+bk,l wd\nI,(mk +1,nl +2)), (4)\nwhere mk =⌊ k/N⌋, nl =⌊ l/N⌋,a n d ⌊.⌋ is a ﬂoor function.\nThe coefﬁcients a, b,a n d c can be easily computed as ak,l =\n(k/N − mk + l/N − nl )/2, bk,l = ak,l − 1/2, and ck,l =\n1 + k/N − mk − (l/N − nl ).\nWe can rearrange Eq. (2) by replacing wI,(k,l) with Eq. (4).\nIn addition, the partial derivative of Ti,j with respect to wI,(k,l)\n(see Eq. (3)) turns out to be four partial derivatives of Ti,j\nwith respect to four adjacent control points of the down-\nsampled joint intensity metric (i.e., wd\nI,(mk ,nl ), wd\nI,(mk ,nl +1),\nFig. 3. Interpolation of ori ginal joint intensity metric wI,(k,l) using four\nsurrounding downsampled joint intensity metrics wd\nI,(mk ,nl ), wd\nI,(mk ,nl +1),\nwd\nI,(mk +1,nl +1),a n dwd\nI,(mk +1,nl +2) with coefﬁcients ak,l , bk,l ,a n dck,l for\nthe bilinear interpolation. Note that the interpolation is carried out along the\ndiagonal and anti-diagonal directions.\nwd\nI,(mk +1,nl +1),a n d wd\nI,(mk +1,nl +2)), which are easily obtained\nby Eq. (4) as follows:\n∂Ti,j\n∂wd\nI,(mk ,nl )\n= δk,pi,j δl,gi,j ck,l (1 − ak,l ),\n∂Ti,j\n∂wd\nI,(mk ,nl +1)\n= δk,pi,j δl,gi,j ck,l ak,l ,\n∂Ti,j\n∂wd\nI,(mk +1,nl +1)\n= δk,pi,j δl,gi,j (1 − ck,l )(1 − bk,l ),\n∂Ti,j\n∂wd\nI,(mk +1,nl +2)\n= δk,pi,j δl,gi,j (1 − ck,l )bk,l . (5)\nC. JIMEN\nWe design a JIMEN to estimate the joint intensity metric,\nas shown on the left side of Fig. 2 (a). It takes a pair of GEIs\nand outputs the joint intensity metric. Speciﬁcally, the JIMEN\nis an encoder-decoder framework in which the encoder ﬁrst\ntakes a difference image of a probe and gallery GEI, then\nlearns the effective subspace feature through four convolu-\ntional layers; the decoder ﬁrst takes the output feature of the\nencoder, then generates a two-dimensional representation fea-\nture as the joint intensity metric through four deconvolutional\n(transposed convolutional) layers. In this network, the ReLU\nactivation function is used for all convolutional layers and the\nﬁrst three deconvolutional layers, the local response normal-\nization (LRN) [50] is used for the normalization layers, and a\nmax pooling strategy is chosen for the pooling layers. Unlike\nSTN [42], which adopts a fully connected layer as the ﬁnal\nregression layer and outputs a vector of spatial deformation\nparameters (e.g., afﬁne transfo rmation parameters), we design\nan encoder-decoder framework because it can regress the two-\ndimensional joint intensity metric well.\nD. DN\nAfter the joint intensity transformation of a pair of GEIs\nusing the joint intensity transformer, their spatial dissimilarity\nimage is fed into a generic DN to learn the spatial metric,\nLI et al.: JITN FOR GAIT RECOGNITION ROBUST AGAINST CLOTHING AND CARRYING STATUS 3107\nas shown on the right side of Fig. 2 (a). A DN shares the same\narchitecture as the diff net in [24]. Speciﬁcally, a DN has three\nconvolutional layers followed by a ReLU activation layer and\nLRN normalization layer. It also has one fully connected layer\nthat has a 52-dimensional feature. Subsequently, the L2 layer\ncalculates the L2 norm of the 52-dimensional feature as\nthe ﬁnal dissimilarity of the input pair of GEIs. To avoid\noverﬁtting, a dropout technique (with a ratio of 0.5) [51] is\napplied after the third convolutional layer. The DN is designed\nto reduce the ﬁnal dissimilarity of a same-subject pair while\nincreasing the ﬁnal dissimilarity of a different-subject pair\nthrough the contrastive and triplet loss functions, which are\nintroduced in the next subsection.\nE. Networks for Different Gait Recognition Tasks\nGait recognition includes two kinds of tasks: gait veriﬁca-\ntion and gait identiﬁcation. For the veriﬁcation task, a probe\nand gallery pair is compared and then it is determined whether\nthey come from the same subject or different subjects. If their\ndissimilarity is lower than a certain acceptance threshold, they\nare judged to be from the same subject and vice versa. For the\nidentiﬁcation task, a probe is compared with all the galleries\nto ﬁnd the same subject that appears in the probe. We usually\ncalculate the dissimilarities between the probe and all the\ngalleries, then use a nearest neighbor classiﬁer to ﬁnd the\nsubject with the smallest dissimilarity.\nReferring to [24], different gait recognition tasks have\ntheir own suitable network ar chitectures and loss function,\ni.e., a Siamese network with contrastive loss for the veriﬁcation\ntask and a triplet network with triplet loss for the identiﬁcation\ntask. Therefore, we design dif ferent networks depending on\ndifferent gait recognition tasks in the training phase, as shown\nin Fig. 2 (b). More speciﬁcally, for the veriﬁcation task,\nwe choose a contrastive loss function [52] as the loss function\nof the proposed JITN framework, which is deﬁned as follows:\nL\ncont = 1\n2C\nC∑\ni=1\nyi d2\ni + (1 − yi )max(margin − di ,0)2, (6)\nwhere C is the number of GEI pairs for training, di is the\ndissimilarity score in the L2 norm layer of the i-th pair\nof GEIs, and yi is equal to 1 if the i-th pair is the same\nsubject pair and 0 otherwise. Using Eq. (6), the network trains\nits parameters such that the dissimilarity scores of the same\nsubject pairs are always smaller than those of different subject\npairs, which is suitable for the veriﬁcation scenario.\nFor the identiﬁcation task, we choose a triplet loss func-\ntion [53] as the loss function of the proposed JITN framework.\nTriplet GEIs called query , positi ve,a n d negati ve are fed\ninto the network, where positi ve is of the same subject as\nthat of query and negati ve is of a different subject from that\nof query . Then, a triplet loss function is deﬁned as follows:\nL\ntrip = 1\n2C\nC∑\ni=1\nmax(margin − d−\ni + d+\ni ,0)2, (7)\nwhere C is the number of triplets for training, d−\ni is the\ndissimilarity score in the L2 norm layer between query and\nFig. 4. Examples of GEI with seven annotated carrying status labels\nfrom OU-LP-Bag. The ﬁrst row shows subjects with a carrying status and\nthe second row shows subjects without carrying status.\nnegati ve,a n d d+\ni is the dissimilarity score in the L2 norm\nlayer between query and positi ve. Using Eq. (7), the network\ntrains its parameters so that the dissimilarity score of query\nand positi ve is always smaller than the dissimilarity score\nof query and all negati ve GEIs, which is suitable for the\nidentiﬁcation scenario.\nIV . EXPERIMENTS\nA. Datasets\nWe use four publicly available databases,1 OU-LP-Bag [43],\nOU-LP-Bag β [33], OUTD-B [44], and TUM-GAID [45], for\nthe experiments.\nOU-LP-Bag is currently the wor ld’s largest gait database\nwith real-life carried object s. The data were collected in\nconjunction with an experience-based demonstration of video-\nbased gait analysis at a science museum [54]. It includes a\ntotal of 62,528 subjects with seven annotated carrying status\nlabels (i.e., NoCO for no ca rried objects, SbCO for objects\ncarried on the side bottom, SmCO for objects carried on\nthe side middle, FrCO for obj ects carried in front, BaCO\nfor objects carried in back , MuCO for objects carried in\nmultiple locations, and CpCO for objects carried changing\nfrom one location to another). Some typical GEI examples\ncan be seen in Fig. 4. Each subject was captured three times\nto produce walking sequences. The ﬁrst sequence ( A\n1) can be\nwith or without carried objects (that is, some participants did\nnot hold any objects), the other two ( A2 and A3) are without\ncarried objects. Among all subjects, 58,199 subjects that have\na sample both in A1 and either A2 or A3 were chosen for\nthe experiments. The chosen subjects were randomly divided\ninto two subsets: a training set (29,097 subjects) and a test\nset (29,102 subjects). The test set is further divided into a\ngallery set and a probe set. Considering the uncooperative-\nsubject condition in real scenarios, Uddin et al. [43] introduced\nboth cooperative and uncooperative settings in the test set.\nFor the cooperative setting, samples from A\n2 or A3 are in the\ngallery set, while samples from A1 are in the probe set. For\nthe uncooperative setting, samples are randomly assigned into\nthe gallery and probe sets.\n1OU-LP-Bag, OU-LP-Bag β and OUTD-B are available at\nhttp://www.am.sanken.osaka-u.ac.jp/BiometricDB/index.html; TUM-GAID is\navailable at https://www.mmk.ei.tum.de/en/misc/tum-gaid-database/\n3108 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY , VOL. 14, NO. 12, DECEMBER 2019\nThe OU-LP-Bag β is the beta version of OU-LP-Bag.\nThere are 2,070 subjects in the dataset and each subject\nhas two sequences, one with carried objects and the other\nwithout carried objects. The whole dataset is divided into three\nsubsets: a training set, gallery set, and probe set. The training\nset contains 2,068 sequences of 1,034 subjects, while the\nremaining disjoint 1,036 subjects are included in the gallery\nand probe sets. The gallery set comprises sequences without\ncarried objects, while the probe set has sequences with carried\nobjects.\nThe OUTD-B has the largest number of clothing variations\n(up to 32). It is divided into three subsets: a training set, gallery\nset, and probe set. In the training set, there are 446 sequences\nof 20 subjects with a range of 15 to 28 different combinations\nof clothing. The gallery and probe sets constitute a testing\nset that comprises 48 subjects, which are disjoint from the\n20 subjects in the training set. The gallery contains only\nstandard clothing types (e.g., regular pants and full shirt),\nwhile the probe set includes 856 sequences of other clothing\ntypes.\nThe TUM-GAID simultaneously contains RGB video, depth\nand audio data with 305 subjects walking under four condi-\ntions: normal walking ( N), carrying a backpack ( B), wearing\ncoating shoes (S), and elapsed time (TN −TB −TS ) collected\nat January and April which may also exist changes in clothing\nor lighting condition. All subjects contain ten sequences: six\nnormal walking (N1−N6), two backpack variation (B1−B2),\nand two shoes variation ( S1 − S2). 32 subjects among them\ncontain additional ten sequences under elapsed time variation,\nnamely TN 1−TN 6, TB 1 − TB 2, TS 1−TS 2. Following the\nprotocol of the original paper [45], the whole dataset is divided\ninto three subsets: a training set with 100 subjects, a validation\nset with 50 subjects, and a test set with 155 subjects. Half of\nthe subjects under elapsed time variation is included in the test\nset and another half is included in the training and validation\nset. In the test set, N1 − N4 are set as the gallery set, while\nN5 − N6, B1 − B2, S1 − S2, TN 5 − TN 6, TB\n1 − TB 2, and\nTS 1 − TS 2 are set as six different probe sets.\nB. Implementation Details\nWe use Xavier’s algorithm to initialize the weight parame-\nters of all layers except for th e last deconvolutional layer in\nJIMEN, which is separately set to initialize the joint intensity\nmetric. The bias parameters are all set to the constant zero. The\nmomentum for all layers is 0.9. We set the initial learning rate\nto 0.01 and divide it by 10 four times during the training phase.\nThe proposed network is train ed for a total of 0.1 million\niterations using the stochastic gradient descent algorithm with\na mini-batch size of 300. We implement the whole framework\nusing Caffe [55] on a NVIDIA GeForce GTX TITAN X\nGPU with 12 G memory. The hyper-parameter margins in\nEqs. (6) and (7) are experimentally set to three.\nAs for the joint intensity metric, which is regressed by\nJIMEN, we set the number N of control points distributed\nover the intensity levels to 32 and obtain a downsampled\njoint intensity metric with a size of 32 × 32. Regarding the\ninitialization, we set it be the signed l\n1-norm, i.e., wI,(k,l) =\nk −l. To do so, we ﬁrst set the weight and bias parameters of\nthe last deconvolutional layer in JIMEN to zero, which forces\nthe output to zero; then, we add a dummy layer initialized by\nthe signed l1-norm that has the same size as the joint intensity\nmetric.\nRegarding the sampling problem for training, we uncooper-\natively choose subjects from the training set; that is, we do not\nﬁx the gallery to be a subject with clothing or carried objects.\nBasically, for the veriﬁcation task, we choose all the same and\ndifferent subject pairs in the training set and then duplicate the\nsame subject pairs so that their number is one-ninth that of the\ndifferent subject pairs; for the identiﬁcation task, we basically\nchoose all the triplets in the training set. However, for the\nlargest database OU-LP-Bag, there are billions of untrackable\npairs and triplets. Thus, we simply randomly choose from\nall pairs and triplets while keeping the total number to about\n10 million.\nC. Evaluation Metrics\nWe refer to the standards deﬁned in ISO/IEC 19795-1 on\nbiometric performance testing and reporting [56] for evalua-\ntion metrics. For the veriﬁcatio n task, a detection error trade-\noff (DET) curve, which indicates a trade-off between false\nnon-match rate (FNMR) and false match rate (FMR) when\nan acceptance threshold changes, is employed. Speciﬁcally,\nFNMR is the proportion of genuine attempts that are falsely\ndeclared not to match a template of the same subject and\nFMR is the proportion of the imposter attempts that are falsely\ndeclared to match a template of another subject. In addition,\nwe also calculate the equal error rate (EER), where FNMR is\nequal to FMR. For the identiﬁcation task, a cumulative match\ncharacteristic (CMC) curve, w hich shows the identiﬁcation\nrates of actual subjects included within each of the ranks,\nis employed. In addition, we calculate the rank-1 identiﬁcation\nrate, denoted as Rank-1.\nD. Learned Joint Intensity Metric\nIn this subsection, we analyze the learned joint intensity\nmetrics and show some typical comparison examples using the\nlearned metrics in Fig. 5. As me ntioned before, the learned\njoint intensity metrics are sample-dependent trained by the\nproposed network (see Figs. 5 (i) and (j)), which is more\nsuitable for dealing with all kinds of variations in clothing\nand carrying status than traditional approaches [31], [33],\nwhich perform common sample-independent transformations.\nNote that for learned joint intensity metrics, darker or brighter\nregions indicate an enhancement of the metrics, while grayer\nregions (i.e., a gray value close to 127) indicates a degra-\ndation of the metrics. We also calculated the difference\nbetween the learned joint intensity metrics and the initial\none to show the changes, and then colored these changes\nin Fig. 5 (k) and (l); that is, the blue and yellow regions\nrepresent the degradation and enhancement of the metrics,\nrespectively.\nWhen compared with the initial joint intensity metric (i.e.,\nthe signed l\n1-norm, see Fig. 5 (h)), for true match pairs,\nthe learned joint intensity metrics show more degradation\nnear the top-right and bottom-le ft corners, which is mainly\nLI et al.: JITN FOR GAIT RECOGNITION ROBUST AGAINST CLOTHING AND CARRYING STATUS 3109\nFig. 5. Comparison examples for four probes with different types of carri ed objects (i.e., FrCO, BaCO, MuCO, and NoCO from top to bottom) and their\ncorresponding learned joint intensity metric. (a) Probe. (b) Gallery (genuine). (c) Gallery (imposter). (d) and (e) Spatial dissimilarity with the absolute difference\nfor a true match pair (probe vs. genuine) and false match pair (probe vs. im poster), respectively. (f) and (g) Spatial dissimilarity with absolute lea rned joint\nintensity metrics for a true match pair (probe vs. genuine) and false match pair (probe vs. imposter), respectively. (h) Initial joint intensity metr ic, i.e., signed\nl1-norm. (i) and (j) Learned joint intensity metrics for a true match pair (pr obe vs. genuine) and false match pair (p robe vs. imposter). (k) and (l) Chang es in\nthe initial and learned joint intensity metrics for a true match pair (probe vs. genuine) and false match pair (probe vs. imposter), where the blue and y ellow\nregions represent a decline and enhancement of met rics on the joint intensity pairs, respectively.\nderived from the intrasubject carrying status variations. This\nindicates that the learned joint intensity metrics can success-\nfully suppress the effect of carrying status, which causes\nlarge intrasubject difference. In contrast, for false match pairs,\nthe learned joint intensity metrics show more enhancements\nnear the diagonal line, which is mainly derived from the\nmotion differences (e.g., leg or hand motions). This indicates\nthat the learned joint intensity metrics can enhance the effect of\nmotion differences, which causes small intersubject difference.\nThese conclusions can also be arrived at from the examples\n(see Figs. 5 (f) and (g)). The spatial dissimilarity of the true\nmatch pairs decreases, especially in the regions of carried\nobjects, while the spatial dissimilarity of false match pairs\nincreases especially in the regions of leg motion.\nWhen the metrics are compared for the four different\ntypes of carried objects, for lar ge variations such as FrCO,\nBaCO, and MuCO, the learned joint intensity metrics strongly\nsuppress the large intrasubject difference and enhance the\nsubtle motion difference. In contrast, for very small variations\nsuch as NoCO, a less drastically modulated joint intensity\nmetric is yielded because the metric changes are relatively\nsmaller than the other three types of large carried objects (see\nFigs. 5 (k) and (l), where the results in the bottom row have\nlighter yellow and blue colors than those in the upper three\nrows).\nTherefore, with the learned joint intensity metrics, we get\na smaller spatial dissimilarity for true match pairs regardless\nof carried objects in various locations (e.g., front, back, both\nfront and back, or even with no carried objects) and a larger\nspatial dissimilarity for false match pairs, which successfully\nmitigates the effect of carrying status and leads to the correct\nrecognition results.\nE. Comparison on OU-LP-Bag\nIn this subsection, we evaluate the robustness of\nthe proposed method against real-life carried objects on\nOU-LP-Bag. The state-of-the-art methods for comparison\nconsist of traditional methods (direct matching (DM) of\nGEIs [10], spatial metric learning-based approaches such as\nGEI w/ LDA [57], GEI w/ RSVM [58], and intensity\ntransformation-based approaches such as GERF [31]) and\nrecent deep learning-based methods (GEINet [22], the Siamese\nGEINet (SIAME) [59], LB [23], and diff/2diff [24]). The pro-\nposed method and the compared deep learning-based bench-\nmarks were trained from scratch on this dataset with the same\nprotocol. The network parameters were set to the defaults\ngiven in their original papers. Of all the methods, diff/2diff,\nproposed by Takemura et al. [24], has the architecture that\nis most similar to that of the proposed method, which uses\ncontrastive loss and triplet loss function. Takemura et al.\nsimply take the pair-wise difference at the beginning of the\nnetwork and proceed as the proposed method does with\na ﬁxed signed l\n1-norm as the joint intensity metric. Thus,\nthe comparison between the proposed method and diff/2diff\nreﬂects the effect of joint intensity metric learning well.\nWe show the DET and CMC curves of all methods in Fig. 6.\nWe also show the EER and Rank-1 of each method in Table I.\nThe results show that the proposed method achieves the best\nperformance both in the veriﬁca tion and identiﬁcation scenar-\nios for both cooperative and uncooperative settings. Moreover,\nit outperforms traditional methods by a large margin. Although\nLB [23] achieves the second best Rank-1, which is only 0.05%\nlower than the proposed method in a cooperative setting,\nit faces a clear decrease of nearly 4% for Rank-1 in an\nuncooperative setting, which is more likely in a real scene.\n3110 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY , VOL. 14, NO. 12, DECEMBER 2019\nFig. 6. DET and ROC curves for the com parison experiments on OU-LP-Bag\nin both the (a) cooperative setting and (b) uncooperative setting. The left side\nshows the DET curves and the right side shows the ROC curves.\nTABLE I\nEER\nAND RANK -1 [%] R ESULTS FOR THE COMPARISON EXPERIMENTS\nON OU-LP-B AG IN BOTH COOPERATIVE AND UNCOOPERATIVE SET-\nTINGS .F OR DIFF /2DIFF , THE EER R ESULT IS FROM THE “DIFF ”\nMETHOD ,W HILE THE RANK -1 R ESULT IS FROM “2DIFF ”\nMETHOD .B OLD AND ITALIC BOLD FONTS INDICATE THE\nBEST AND SECOND -BEST RESULTS ,R ESPECTIVELY .\nTHIS CONVENTION IS CONSISTENT THROUGHOUT\nTHIS PAPER\nIn contrast, the proposed method only shows a slight decline\nof 0.41% for Rank-1. Besides, for the EER, the proposed\nmethod clearly outperforms LB with a 0.4% lower EER\nin both cooperative and uncooperative settings. Moreover,\nwhen compared with diff/2diff, the proposed method clearly\noutperforms it with a 0.1 % lower EER and 1.3% higher\nRank-1, which demonstrates the effectiveness of the joint\nintensity metric learning in the proposed framework.\nWe also evaluated the robustness of the proposed method\nagainst different types of carried objects. We choose diff/2diff\nFig. 7. EER and Rank-1 results for the proposed method and baseline\nmethod (diff/2diff) under different carried object conditions on OU-LP-Bag.\n(a) Cooperative setting and (b) uncooperative setting. The left side shows the\nEER results and the right side shows the Rank-1 results.\nas the baseline method, which is a CNN-based method that\nonly considers spatial metric learning. The results are shown\nin Fig. 7. We can see that the proposed method shows lower\nEERs and higher Rank-1 rates for almost all types of carried\nobjects in both cooperative and uncooperative settings. In case\nof more difﬁcult types such as FrCO, MuCO, and CpCO,\nthe improvements of the proposed method are substantial,\nwhich is because the joint intensity metric learning can handle\nrelatively large carried objects well regardless of their carried\nlocations. For NoCO (no carried objects), the improvement is\nsubtle because, without carried objects, the proposed method\nacts more like the baseline method and considers no joint\nintensity metric learning. Through this analysis, we conﬁrm\nthe ﬂexibility of proposed method, which can effectively learn\nsample-dependent joint intensity metrics and handle all kinds\nof carrying statuses.\nF . Comparison on OU-LP-Bag β\nIn this subsection, we evaluate the robustness of the pro-\nposed method against carrying status on OU-LP-Bag β.T h e\ncomparison benchmarks are mainly from [33] and also include\nrecent state-of-the-art deep learning-based methods (LB [23]\nand diff/2diff [24]). Because of the relatively small number\nof training samples of OU-LP-Bag β, for deep learning-\nbased methods, we ﬁne-tuned deep models that were pre-\ntrained on the OU-LP-Bag dataset. To do so, we ﬁrst set\na smaller learning rate of 0.001 and tuned all layers of\nthese networks. Figure 8 an d Table II show the results\nof all methods. Here, the proposed method achieves the\nbest performance. In contrast with JIS-ML proposed by\nMakihara et al. [33], which was the ﬁrst to introduce joint\nintensity metric learning and integrated it with spatial metric\nlearning in a traditional linear SVM framework, the pro-\nposed method successfully integrates joint intensity metric\nlearning in a uniﬁed CNN-based framework trained from\nLI et al.: JITN FOR GAIT RECOGNITION ROBUST AGAINST CLOTHING AND CARRYING STATUS 3111\nFig. 8. DET and ROC curves for th e comparison experiments on\nOU-LP-Bag β. The left side shows the DET curve and the right side shows\nthe ROC curve.\nTABLE II\nEER AND RANK -1 [%] R ESULTS FOR THE COMPARISON\nEXPERIMENTS ON OU-LP-B AG β\nend to end. The proposed approach yields a much higher\nperformance.\nG. Comparison on OUTD-B\nIn this subsection, we evaluate the robustness of the pro-\nposed method against various clothing types on OUTD-B,\nwhich has the largest variation of labeled clothing. Similar to\nOU-LP-Bag β, for the deep learning-based methods, we used\nmodels pre-trained on the OU-LP-Bag dataset and applied\nthe same ﬁne-tuning strategy. The results of all comparison\nmethods are shown in Fig. 9 and Table III. It can be seen that\nthe proposed method achieves the best EER and the second\nbest Rank-1 of all methods. Although Gabor +RSM-HDF [27]\nobtains the best Rank-1, we note it has several weaknesses:\n1) it cannot be used for the veriﬁcation task because of its\nmajority voting scheme for al l galleries and 2) because it\nrequires multiple samples per gallery to compute the within-\nclass scatter from the gallery set, it cannot be used in datasets\nwith a single sample per gallery (e.g., OU-LP-Bag β). There-\nfore, the proposed method is promising because of its wide\nrange of applications both in identiﬁcation and veriﬁcation\nscenarios as well as its stat e-of-the-art performance.\nH. Comparison on TUM-GAID\nIn this subsection, we evaluate the robustness of the\nproposed method on TUM-GAID dataset, which contains\nFig. 9. DET and ROC curves for comparison experiments on OUTD-B. The\nleft side shows the DET curve and the right side shows the ROC curve.\nTABLE III\nEER AND RANK -1 [%] R ESULTS FOR THE COMPARISON EXPERIMENTS\nON OUTD-B. N/A AND -M EAN NOT APPLICABLE AND\nNOT PROVIDED ,R ESPECTIVEL Y\nscenarios where both clothing and carrying status change. The\nmethod [14] shows state-of-the-art results with relatively low-\nresolution input features (i.e., 60 ×60). For fair comparison,\nwe use the same resolution to show the performance of the\nproposed method under circumstances with low-resolution\nGEIs. To adapt the change of input size, we slightly adjust the\nkernel size of the fourth convolutional layer of JIMEN be 3×3.\nWe still use ﬁne-tuning strategy for the TUM-GAID dataset\non the models, which were pre-trained on the OU-LP-Bag\ndataset where the features are also ﬁrst resized to the low\nresolution. All 150 subjects from the training and validation\nsets are used to generate training pairs/triplets in the ﬁne-\ntuning stage. The GEI features are extracted from tracked\n3112 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY , VOL. 14, NO. 12, DECEMBER 2019\nTABLE IV\nRANK -1 [%] R ATE F OR THE COMPARISON EXPERIMENTS ON TUM-GAID.\nN , B, S, TN , TB , AND TS REPRESENT DIFFERENT\nPROBE SETS WITH DIFFERENT COV ARIATES\ndepth image sequences by the method described in [45], and\nthen resized to 60×60 (the original height-to-width ratio of the\nsubjects is kept). Because the GEIs are not as well aligned\nas other datasets, we employ an additional registration step\n(i.e., shift the probe in both horizontal and vertical directions\nto minimize the l\n1-norm between the gallery and the shifted\nprobe) to GEI pairs both in the training and test stages for\nbetter performance. The results of all comparison methods are\nshown in Table IV. Only the performance in the identiﬁcation\ntask is presented because few works reported their results in\nthe veriﬁcation task. From the results, the proposed method\nachieves very competitive performance compared with the\nbest state-of-the-art method [14], which implies the proposed\nmethod can handle the change of the resolution of the input\nimages and ensure its good performance.\nI. Analysis of the Effects of Noise\nReviewing the process of GEI feature extraction, the quality\nof GEI features highly depends on the segmentation results\nof human silhouettes from raw video sequences. Although\nrecent deep learning-based methods help to improve the\nsegmentation results, there may still exist somewhat over-\nsegmented or under-segmented parts (i.e., noise) near the\nboundaries of the human silhouettes. Therefore, robustness\nto such noise is very important for real world applications.\nSince current databases contain no noise variation, we use\nsimulated noise data on the silhouettes for the experiment.\nSpeciﬁcally, the OUTD-B dataset is chosen due to its relatively\nhigh-quality silhouettes. The noise is shaped into a circle with\nrandom radius from 2 to 5 pixels. The center of the noise\ncircle is assumed to appear at the boundary pixels of the\nsilhouettes, and we random decide it be over-segmented or\nunder-segmented within each circle (e.g., suppose intensity\nvalues 0 and 255 belong to the background and human part,\nrespectively; if over-segmented, all the pixels in the circle are\nset to be 255; if under-segmented, all the pixels are set to be 0).\nFigure 10 shows some examples of the noise data with three\ndifferent appearance frequencies (i.e., 0.01, 0.05, and 0.1),\nwhich is deﬁned as the ratio between the number of noise\nand the number of boundary pixels. Obviously, larger noise\nappearance frequency results in worse silhouettes and GEIs.\nAs for the performance evaluation against noise, we simply\nassume a setting where both probe and gallery contain the\nsame type of noise. We prepare two models: one is trained\nwithout any noise; another is trained together with noise. The\nFig. 10. Examples of some selected silhouettes and their GEIs without and\nwith noise. There are three different a ppearance frequencies (i.e., 0.01, 0.05,\nand 0.1) of the noise, which represent different degrees that affected by the\nnoise.\nTABLE V\nEER AND RANK -1 [%] R ESULTS OF THE PROPOSED METHOD\nAGAINST NOISE UNDER TWO MODELS TRAINED W /O\nAND W /N OISE ON OUTD-B\nresults on both models are shown in Table V. In case of\nmodels trained without noise, the proposed method shows\nits robustness to moderate and small noise (i.e., 0.05 and\n0.01 appearance frequencies). Moreover, even for very large\nnoise (i.e., 0.1 appearance frequency), it still achieves better\nperformance than most benchmarks that use the test set\nwithout noise. Additionally, if the models are trained together\nwith noise, the proposed method could increase its robustness\nagainst noise and show much better results.\nJ. Stability Analysis\nIn this subsection, we analyze the stability of the proposed\nmethod in terms of the performance on the test set. We choose\nthe OU-LP-Bag dataset for this experiment because it has the\nlargest number of test samples (29,102 subjects). The whole\ntest set is randomly divided into ﬁve equally disjoint gallery\nand probe sets. We use the same models as section IV-E\nfor evaluation, which are trained on the whole training set.\nTable VI shows the mean value and standard deviation of the\nperformance for the comparison methods. From the results,\nthe proposed method is still superior to other benchmarks\neven if we consider the uncertainty (i.e., mean ± standard\ndeviation).\nLI et al.: JITN FOR GAIT RECOGNITION ROBUST AGAINST CLOTHING AND CARRYING STATUS 3113\nTABLE VI\nEER AND RANK -1 [%] R ESULTS OF THE PROPOSED METHOD COMPARED\nWITH TWO OTHER BENCHMARKS UNDER COOPERATIVE SETTING ON\nOU-LP-B AG. Avg AND Std REPRESENT THE MEAN VALUE AND\nSTANDARD DEVIATION OF THE RESULTS ON FIVE EQUALLY\nDISJOINT GALLERY AND PROBE SETS ,R ESPECTIVELY\nV. C ONCLUSION\nIn this paper, we proposed a uniﬁed joint intensity trans-\nformer network for gait recognition that is robust against vari-\nous clothing and carrying status. To the best of our knowledge,\nthis is the ﬁrst work integrating joint intensity metric learning\ninto a deep learning-based framework. Speciﬁcally, JITN is\na uniﬁed CNN-based architecture containing three parts: a\nJIMEN, a joint intensity transformer, and a DN. Additionally,\nit is designed with different loss functions depending on the\ngait recognition task. Experimental results using four publicly\navailable datasets demonstrate the state-of-the-art performance\nof the proposed method compared with other state-of-the-art\nmethods.\nWe use two different network structures with the con-\ntrastive/triplet losses for the veriﬁcation/identiﬁcation tasks,\nrespectively, and this might be prohibited when a memory\nusage is limited (e.g., an embedded system). We will therefore\nconsider to train a uniﬁed model by combining the veriﬁca-\ntion/identiﬁcation losses in a multi-task setting which reduces\nmemory usage. Additionally, because the proposed method\nmainly focuses on the joint intensity transformation to deal\nwith clothing and carrying status covariates, we will consider\nhow to modify it to cope with cross-view gait recognition,\nwhich performs a spatial transformation that handles the large\nspatial displacement caused by view angle changes. Moreover,\nthe combination of both joint intensity and spatial transfor-\nmation for all covariates remains another direction for future\nwork.\nA\nCKNOWLEDGMENT\nThe authors would like to thank Md. Zasim Uddin\nfor providing part of the results in the experiments.\nThey also thank Kim Moravec, PhD, from Edanz Group\n(www.edanzediting.com/ac) for editing a draft of this man-\nuscript. Moreover, they would like to thank all the reviewers\nfor their valuable comments.\nR\nEFERENCES\n[1] I. Bouchrika, M. Goffredo, J. Carter, and M. Nixon, “On using gait in\nforensic biometrics,” J. Forensic Sci., vol. 56, no. 4, pp. 882–889, 2011.\n[2] H. Iwama, D. Muramatsu, Y . Makihara, and Y . Yagi, “Gait veriﬁcation\nsystem for criminal investigation,” IPSJ Trans. Comput. Vis. Appl. ,\nvol. 5, pp. 163–175, Oct. 2013.\n[3] N. Lynnerup and P. K. Larsen, “Gait as evidence,” IET Biometrics,v o l .3 ,\nno. 2, pp. 47–54, Jun. 2014.\n[4] R. Urtasun and P. Fua, “3D tracking for gait characterization and recog-\nnition,” in Proc. 6th IEEE Int. Conf. Autom. Face Gesture Recognit. ,\nMay 2004, pp. 17–22.\n[5] D. K. Wagg and M. S. Nixon, “On aut omated model-based extraction\nand analysis of gait,” in Proc. 6th IEEE Int. Conf. Autom. Face Gesture\nRecognit., May 2004, pp. 11–16.\n[6] C. Yam, M. Nixon, and J. N. Carter, “Automated person recognition by\nwalking and running via model-based approaches,” Pattern Recognit.,\nvol. 37, no. 5, pp. 1057–1072, 2004.\n[7] G. Zhao, G. Liu, H. Li, and M. Pietikainen, “3D gait recognition using\nmultiple cameras,” in Proc. 7th Int. Conf. Autom. Face Gesture Recognit.\n(FGR), Apr. 2006, pp. 529–534.\n[8] S. Sarkar, J. Phillips, Z. Liu, I. Vega, P. G. ther, and K. Bowyer,\n“The humanid gait challenge problem: Data sets, performance, and\nanalysis,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 27, no. 2,\npp. 162–177, Feb. 2005.\n[9] L. Wang, T. Tan, H. Ning, and W. Hu, “Silhouette analysis-based gait\nrecognition for human identiﬁcation,” IEEE Trans. Pattern Anal. Mach.\nIntell., vol. 25, no. 12, pp. 1505–1518, Dec. 2003.\n[10] J. Han and B. Bhanu, “Individual recognition using gait energy image,”\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 28, no. 2, pp. 316–322,\nFeb. 2006.\n[11] K. Bashir, T. Xiang, and S. Gong, “Gait recognition using gait entropy\nimage,” in Proc. Int. Conf. Crime Detection Prevention , Dec. 2009,\npp. 1–6.\n[12] K. Bashir, T. Xiang, and S. Gong, “Gait recognition without subject\ncooperation,” Pattern Recognit. Lett. , vol. 31, no. 13, pp. 2052–2060,\n2010.\n[13] F. M. Castro, M. J. Marín-Jiménez, N. G. Mata, and R. Muñoz-Salinas,\n“Fisher motion descriptor for multiview gait recognition,” Int. J. Pattern\nRecognit. Artif. Intell. , vol. 31, no. 1, 2017, Art. no. 1756002.\n[14] M. J. Marín-Jiménez, F. M. Castro, N. Guil, F. de la Torre, and\nR. Medina-Carnicer, “Deep multi-task learning for gait-based biomet-\nrics,” in Proc. IEEE Int. Conf. Image Process. (ICIP) , Sep. 2017,\npp. 106–110.\n[15] Y . Makihara, R. Sagawa, Y . Mukaigawa, T. Echigo, and Y . Yagi,\n“Gait recognition using a view transformation model in the frequency\ndomain,” in Proc. 9th Eur. Conf. Comput. Vis., Graz, Austria, May 2006,\npp. 151–163.\n[16] C. Wang, J. Zhang, L. Wang, J. Pu, and X. Yuan, “Human identiﬁcation\nusing temporal information preserving gait template,” IEEE Trans.\nPattern Anal. Mach. Intell., vol. 34, no. 11, pp. 2164–2176, Nov. 2012.\n[17] D. Tao, X. Li, X. Wu, and S. J. Ma ybank, “General tensor discriminant\nanalysis and Gabor features for gait recognition,” IEEE Trans. Pattern\nAnal. Mach. Intell. , vol. 29, no. 10, pp. 1700–1715, Oct. 2007.\n[18] Z. Liu and S. Sarkar, “Simplest re presentation yet for gait recognition:\nAveraged silhouette,” in Proc. 17th Int. Conf. Pattern Recognit. ,v o l .1 ,\nAug. 2004, pp. 211–214.\n[19] W. Kusakunniran, Q. Wu, J. Zha ng, H. Li, and L. Wang, “Recognizing\ngaits across views through correlated motion co-clustering,” IEEE Trans.\nImage Process., vol. 23, no. 2, pp. 696–709, Feb. 2014.\n[20] M. Hu, Y . Wang, Z. Zhang, J. J. Little, and D. Huang, “View-invariant\ndiscriminative projection for multi-view gait-based human identiﬁca-\ntion,” IEEE Trans. Inf. Forensics Security\n, vol. 8, no. 12, pp. 2034–2045,\nDec. 2013.\n[21] T. Wolf, M. Babaee, and G. Rigoll, “Multi-view gait recognition using\n3D convolutional neural networks,” in Proc. IEEE Int. Conf. Image\nProcess. (ICIP), Sep. 2016, pp. 4165–4169.\n[22] K. Shiraga, Y . Makihara, D. Muramatsu, T. Echigo, and Y . Yagi,\n“GEINet: View-invariant gait recognition using a convolutional neural\nnetwork,” in Proc. Int. Conf. Biometrics (ICB) , Halmstad, Sweden,\nJun. 2016, pp. 1–8.\n[23] Z. Wu, Y . Huang, L. Wang, X. Wang, and T. Tan, “A comprehensive\nstudy on cross-view gait based human identiﬁcation with deep CNNs,”\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 39, no. 2, pp. 209–226,\nFeb. 2017.\n[24] N. Takemura, Y . Makihara, D. Muramatsu, T. Echigo, and Y . Yagi, “On\ninput/output architectures for convolutional neural network-based cross-\nview gait recognition,” IEEE Trans. Circuits Syst. Video Technol.,t ob e\npublished.\n[25] D. Xu, S. Yan, D. Tao, L. Zhang, X. Li, and H.-J. Zhang, “Human\ngait recognition with matrix representation,” IEEE Trans. Circuits Syst.\nVideo Technol., vol. 16, no. 7, pp. 896–903, Jul. 2006.\n[26] Y . Guan, C.-T. Li, and Y . Hu, “Robust clothing-invariant gait recog-\nnition,” in Proc. 8th Int. Conf. Intell. Inf. Hiding Multimedia Signal\nProcess. (IIH-MSP), Jul. 2012, pp. 321–324.\n3114 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY , VOL. 14, NO. 12, DECEMBER 2019\n[27] Y . Guan, C. T. Li, and F. Roli, “On reducing the effect of covariate\nfactors in gait recognition: A classiﬁer ensemble method,” IEEE Trans.\nPattern Anal. Mach. Intell. , vol. 37, no. 7, pp. 1521–1528, Jul. 2015.\n[28] M. A. Hossain, Y . Makihara, J. Wang, and Y . Yagi, “Clothing-invariant\ngait identiﬁcation using part-based clothing categorization and adaptive\nweight control,” Pattern Recognit. , vol. 43, no. 6, pp. 2281–2291,\nJun. 2010.\n[29] Y . Iwashita, K. Uchino, and R. Kurazume, “Gait-based person iden-\ntiﬁcation robust to changes in appearance,” Sensors, vol. 13, no. 6,\npp. 7884–7901, 2013.\n[30] M. Rokanujjaman, M. S. Islam, M. A. Hossain, M. R. Islam,\nY . Makihara, and Y . Yagi, “Effectiv e part-based gait identiﬁcation\nusing frequency-domain gait entropy features,” Multimedia Tools Appl.,\nvol. 74, no. 9, pp. 3099–3120, May 2015.\n[31] X. Li, Y . Makihara, C. Xu, D. Muramatsu, Y . Yagi, and M. Ren,\n“Gait energy response function for clothing-invariant gait recognition,”\nin Proc. Asian Conf. Comput. Vis. Cham, Switzerland: Springer, 2016,\npp. 257–272.\n[32] X. Li, Y . Makihara, C. Xu, D. Muramatsu, Y . Yagi, and M. Ren, “Gait\nenergy response functions for gait r ecognition against various clothing\nand carrying status,” Appl. Sci., vol. 8, no. 8, p. 1380, 2018.\n[33] Y . Makihara, A. Suzuki, D. Muramatsu, X. Li, and Y . Yagi, “Joint\nintensity and spatial metric learning for robust gait recognition,” in Proc.\n30th IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jul. 2017,\npp. 5705–5715.\n[34] F. M. Castro, M. J. Marín-Jiménez, N. Guil, S. López-Tapia, and\nN. P. de la Blanca, “Evaluation of CNN architectures for gait recognition\nbased on optical ﬂow maps,” in Proc. Int. Conf. Biometrics Special\nInterest Group (BIOSIG) , 2017, pp. 1–5.\n[35] C. Zhang, W. Liu, H. Ma, and H. Fu, “ Siamese neural network based gait\nrecognition for human identiﬁcation,” in Proc. IEEE Int. Conf. Acoust.,\nSpeech Signal Process. (ICASSP) , Mar. 2016, pp. 2832–2836.\n[36] S. Yu, H. Chen, Q. Wang, L. Shen, and Y . Huang, “Invariant feature\nextraction for gait recognition using only one uniform model,” Neuro-\ncomputing, vol. 239, pp. 81–93, May 2017.\n[37] S. Yu, H. Chen, E. B. G. Reyes, and N. Poh, “GaitGAN: Invariant gait\nfeature extraction using generative adversarial networks,” in Proc. IEEE\nConf. Comput. Vis. Pattern Recognit. (C VPR) Workshops , Jul. 2017,\npp. 30–37.\n[38] F. Battistone and A. Petrosino, “TGLSTM: A time based graph deep\nlearning approach to gait recognition,” Pattern Recognit. Lett. ,t ob e\npublished.\n[39] K. Sundararajan and D. L. Woodard, “Deep learning for biometrics:\nAs u r v e y , ”ACM Comput. Surv., vol. 51, Jul. 2018, Art. no. 65.\n[40] T. Liu, X. Ye, and B. Sun, “Clothing and carrying invariant gait-\nbased gender recognition,” Proc. SPIE , vol. 10836, Oct. 2018,\nArt. no. 108360X.\n[41] Y . He, J. Zhang, H. Shan, and L. Wang, “Multi-task GANs for view-\nspeciﬁc feature learning in gait recognition,” IEEE Trans. Inf. Forensics\nSecurity, vol. 14, no. 1, pp. 102–113, Jan. 2019.\n[42] M. Jaderberg, K. Simonyan, A. Z isserman, and K. Kavukcuoglu, “Spa-\ntial transformer networks,” in Advances in Neural Information Process-\ning Systems , C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama,\nand R. Garnett, Eds. New York, NY , USA: Curran Associates, 2015,\npp. 2017–2025.\n[43] M. Z. Uddin et al., “The OU-ISIR large population gait database with\nreal-life carried object and its performance evaluation,” IPSJ Trans.\nComput. Vis. Appl. , vol. 10, no. 1, p. 5, May 2018.\n[44] Y . Makihara et al., “The OU-ISIR gait database comprising the treadmill\ndataset,” IPSJ Trans. Comput. Vis. Appl. , vol. 4, pp. 53–62, Apr. 2012.\n[45] M. Hofmann, J. Geiger, S. Bachmann, B. Schuller, and G. Rigoll, “The\nTUM gait from audio, image and depth (GAID) database: Multimodal\nrecognition of subjects and traits,” J. Vis. Commun. Image Represent. ,\nvol. 25, no. 1, pp. 195–206, Jan. 2014.\n[46] S. Lee, Y . Liu, and R. Collins, “Shape variation-based frieze pattern for\nrobust gait recognition,” in Proc. IEEE Conf. Comput. Vision Pattern\nRecognit., Minneapolis, MN, USA, Jun. 2007, pp. 1–8.\n[47] N. V . Boulgouris and Z. X. Chi, “Human gait recognition based on\nmatching of body components,” Pattern Recognit. , vol. 40, no. 6,\npp. 1763–1770, 2007.\n[48] Y . Makihara and Y . Yagi, “Silhouetteextraction based on iterative spatio-\ntemporal local color transformation and graph-cut segmentation,” in\nProc. 19th Int. Conf. Pattern Recognit. , Tampa, FL, USA, Dec. 2008,\npp. 1–4.\n[49] G. Lin, A. Milan, C. Shen, and I. D. Reid. (2016). “ReﬁneNet: Multi-\npath reﬁnement networks for high-resolution semantic segmentation.”\n[Online]. Available: https://arxiv.org/abs/1611.06612\n[50] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classiﬁcation\nwith deep convolutional neural networks,” in Advances in Neural Infor-\nmation Processing Systems , F. Pereira, C. J. C. Burges, L. Bottou, and\nK. Q. Weinberger, Eds. New York, NY , USA: Curran Associates, 2012,\npp. 1097–1105.\n[51] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, “Dropout: A simple way to prevent neural networks\nfrom overﬁtting,” J. Mach. Learn. Res. , vol. 15, no. 1, pp. 1929–1958,\n2014.\n[52] R. Hadsell, S. Chopra, and Y . LeCun, “Dimensionality reduction by\nlearning an invariant mapping,” inProc. IEEE Conf. Comput. Vis. Pattern\nRecognit. (CVPR), vol. 2, Jun. 2006, pp. 1735–1742.\n[53] J. Wang et al. (2014). “Learning ﬁne-grained image similarity with deep\nranking.” [Online]. Available: https://arxiv.org/abs/1404.4661\n[54] Y . Makihara et al. , “Gait collector: An automatic gait data collection\nsystem in conjunction with an experience-based long-run exhibition,”\nin Proc. 8th IAPR Int. Conf. Biometrics (ICB) , Halmstad, Sweden,\nJun. 2016, pp. 1–8.\n[55] Y . Jia et al. (2014). “Caffe: Convolutional architecture for fast feature\nembedding.” [Online]. Available: https://arxiv.org/abs/1408.5093\n[56] Information Technology—Biom etric Performance Testing and\nReporting—Part 1: Principles and Framework , Standard ISO/IEC\n19795-1:2006(en) and ISO/IEC JTC 1/SC 37, Int. Org. Standardization,\nGeneva, Switzerland, 2006.\n[57] N. Otsu, “Optimal linear and nonlinear solutions for least-square dis-\ncriminant feature extraction,” in Proc. 6th Int. Conf. Pattern Recognit. ,\n1982, pp. 557–560.\n[58] R. Martín-Félez and T. Xiang, “Uncooperative gait recognition by\nlearning to rank,” Pattern Recognit., vol. 47, no. 12, pp. 3793–3806,\nDec. 2014.\n[59] S. Chopra, R. Hadsell, and Y . LeCun, “Learning a similarity metric\ndiscriminatively, with application to face veriﬁcation,” in Proc. IEEE\nComput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR) ,v o l .1 ,\nJun. 2005, pp. 539–546.\n[60] K. Liu, Y . Q. Cheng, and J. Y . Yang, “Algebraic feature extraction for\nimage recognition based on an optimal discriminant criterion,” Pattern\nRecognit., vol. 26, no. 6, pp. 903–911, 2006.\n[61] X. Li, S. J. Maybank, S. Yan, D. Tao, and D. Xu, “Gait components\nand their application to gender recognition,” IEEE Trans. Syst., Man,\nCybern. C, Appl. Rev. , vol. 38, no. 2, pp. 145–155, Mar. 2008.\n[62] H. Aggarwal and D. K. Vishwakarma, “Covariate conscious approach\nfor gait recognition based upon zernike moment invariants,” IEEE\nTrans. Cogn. Develop. Syst. , vol. 10, no. 2, pp. 397–407,\nJun. 2018.\n[63] S. Lombardi, K. Nishino, Y . Maki hara, and Y . Yagi, “Two-point gait:\nDecoupling gait from body shape,” in Proc. IEEE Int. Conf. Comput.\nVis., Dec. 2013, pp. 1041–1048.\n[64] D. Xu, S. Yan, L. Zhang, H.-J. Zhang, Z. Liu, and H.-Y . Shum,\n“Concurrent subspaces analysis,” in Proc. IEEE Comput. Soc. Conf.\nComput. Vis. Pattern Recognit., Jun. 2005, pp. 203–208.\n[65] Y . Guan, X. Wei, C.-T. Li, G. L. Marcialis, F. Roli, and M. Tistarelli,\n“Combining gait and face for tackling the elapsed time challenges,”\nin Proc. IEEE 6th Int. Conf. Biometrics, Theory, Appl. Syst. (BTAS) ,\nSep./Oct. 2013, pp. 1–8.\n[66] F. M. Castro, M. J. Marín-Jiménez, N. Guil, and N. P. de la Blanca,\n“Automatic learning of gait signa tures for people identiﬁcation,”\nin Proc. Int. Work-Conf. Artif. Neural Netw. (IWANN) , 2017,\npp. 257–270.\nXiang Li received the B.S. degree in computer\nscience and technology from the Nanjing University\nof Science and Technology (NUST), China, in 2012,\nwhere he is currently pursuing the Ph.D. degree.\nSince 2016, he has been with the Institute of Sci-\nentiﬁc and Industrial Resear ch, Osaka University,\nas a Visiting Researcher. His research interests are\ngait recognition, image processing, and machine\nlearning.\nLI et al.: JITN FOR GAIT RECOGNITION ROBUST AGAINST CLOTHING AND CARRYING STATUS 3115\nYasushi Makihara received the B.S., M.S., and\nPh.D. degrees in engineering from Osaka Univer-\nsity, in 2001, 2002, and 2005, respectively. He is\ncurrently an Assistant Professor with the Institute of\nScientiﬁc and Industrial Res earch, Osaka University.\nHis research interests are gait recognition, morphing,\nand temporal super-resolution. He is a member of the\nI P S J ,R J S ,a n dJ S M E .\nChi Xu received the B.S. degree in computer sci-\nence and technology from the Nanjing University of\nScience and Technology (NUST), China, in 2012,\nwhere she is currently pursuing the Ph.D. degree\nin pattern recognition and intelligent systems. Since\n2016, she has been with the Institute of Scientiﬁc\nand Industrial Research, Osaka University, Japan,\nas a Visiting Researcher. Her research interests\nare gait recognition, machine learning, and image\nprocessing.\nYasushi Yagi (M’91) received the Ph.D. degree\nfrom Osaka University, in 1991. In 1985, he joined\nthe Product Development Laboratory, Mitsubishi\nElectric Corporation, where he worked on robotics\nand inspections. He became a Research Associate,\nin 1990, a Lecturer, in 1993, an Associate Profes-\nsor, in 1996, and a Professor, in 2003, at Osaka\nUniversity. He was also the Director of the Insti-\ntute of Scientiﬁc and Industrial Research, Osaka\nUniversity, from 2012 to 2015, where he is the\nExecutive Vice President. His research interests are\ncomputer vision, medical engineering, and robotics. He is a fellow of the\nIPSJ and a member of the IEICE and RSJ. He was a recipient of the\nACM VRST2003 Honorable Mention Awa rd, the IEEE ROBIO2006 Finalist\nof T.J. Tan Best Paper in Robotics, the IEEE ICRA2008 Finalist for Best\nVision Paper, the MIRU2008 Nagao A ward, and the PSIVT2010 Best Paper\nAward. He has served as the Chair for International conferences, including the\nFG1998 (Financial Chair), OMINVIS 2003 (Organizing Chair), ROBIO2006\n(Program Co-Chair), ACCV2007 (Pr ogram Chair), PSVIT2009 (Financial\nChair), ICRA2009 (Technical Visit C hair), ACCV2009 (General Chair),\nACPR2011 (Program Co-Chair), and ACPR2013 (General Chair). He has\nalso served as the Editor for the IEEE ICRA Conference Editorial Board\n(2007–2011). He is the Editorial Member of the IJCV and the Editor-in-Chief\nof the IPSJ Transactions on Computer Vision and Applications .\nMingwu Ren received the Ph.D. degree in pattern\nrecognition and intelligent systems from the Nan-\njing University of Science and Technology (NUST),\nNanjing, China, in 2001. He is currently a Pro-\nfessor with the School of Computer Science and\nEngineering, NUST. His cu rrent research interests\ninclude computer vision, image processing, and pat-\ntern recognition.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6775702834129333
    },
    {
      "name": "Transformer",
      "score": 0.6402442455291748
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5969178676605225
    },
    {
      "name": "Metric (unit)",
      "score": 0.5486479997634888
    },
    {
      "name": "Gait",
      "score": 0.5339081883430481
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4207799434661865
    },
    {
      "name": "Gait analysis",
      "score": 0.4179246425628662
    },
    {
      "name": "Computer vision",
      "score": 0.37711426615715027
    },
    {
      "name": "Engineering",
      "score": 0.20811009407043457
    },
    {
      "name": "Physical medicine and rehabilitation",
      "score": 0.07661613821983337
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I36399199",
      "name": "Nanjing University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210138169",
      "name": "Osaka Research Institute of Industrial Science and Technology",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I98285908",
      "name": "The University of Osaka",
      "country": "JP"
    }
  ]
}