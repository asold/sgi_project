{
  "title": "Automatic Fake News Detection in Political Platforms - A Transformer-based Approach",
  "url": "https://openalex.org/W3186039758",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5053939840",
      "name": "Shaina Raza",
      "affiliations": [
        "Toronto Metropolitan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6721616661",
    "https://openalex.org/W3120421331",
    "https://openalex.org/W3135114606",
    "https://openalex.org/W2945569946",
    "https://openalex.org/W3038130333",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2927732725",
    "https://openalex.org/W2604264634",
    "https://openalex.org/W6773411823",
    "https://openalex.org/W3048227452",
    "https://openalex.org/W2977830790",
    "https://openalex.org/W3001895040",
    "https://openalex.org/W6767244118",
    "https://openalex.org/W2788235048",
    "https://openalex.org/W2954365773",
    "https://openalex.org/W6948116018",
    "https://openalex.org/W2890801081",
    "https://openalex.org/W6779317912",
    "https://openalex.org/W2808228212",
    "https://openalex.org/W6777684697",
    "https://openalex.org/W2949397667",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3109746361",
    "https://openalex.org/W6748518621",
    "https://openalex.org/W2901272442",
    "https://openalex.org/W6779127638",
    "https://openalex.org/W2903981179",
    "https://openalex.org/W6763240421",
    "https://openalex.org/W3041406295",
    "https://openalex.org/W3070920170",
    "https://openalex.org/W2154851992",
    "https://openalex.org/W3120176075",
    "https://openalex.org/W2971008823",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3091471597",
    "https://openalex.org/W4287648121",
    "https://openalex.org/W3022435406",
    "https://openalex.org/W2969762015",
    "https://openalex.org/W3001172084",
    "https://openalex.org/W2964216663",
    "https://openalex.org/W3121309663",
    "https://openalex.org/W2977526300",
    "https://openalex.org/W3023672669",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2887167794",
    "https://openalex.org/W2153635508",
    "https://openalex.org/W3034020579",
    "https://openalex.org/W3131645232",
    "https://openalex.org/W2994850640",
    "https://openalex.org/W3030648110",
    "https://openalex.org/W3104097132",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3034503922",
    "https://openalex.org/W2107598941",
    "https://openalex.org/W2136903812",
    "https://openalex.org/W2906971970",
    "https://openalex.org/W4221155794",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W2790166049"
  ],
  "abstract": "The dynamics and influence of fake news on Twitter during the 2020 US presidential election remains to be clarified. Here, we use a dataset related to 2020 U.S Election that consists of news articles and tweets on those articles. Therefore, it is extremely important to stop the spread of fake news before it reaches a mass level, which is a big challenge. We propose a novel fake news detection framework that can address this challenge. Our proposed framework exploits the information from news articles and social contexts to detect fake news. The proposed model is based on a Transformer architecture, which can learn useful representations from fake news data and predicts the probability of a news as being fake or real. Experimental results on real-world data show that our model can detect fake news with higher accuracy and much earlier, compared to the baselines.",
  "full_text": "Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE), pages 68‚Äì78\nAugust 5‚Äì6, 2021, ¬©2021 Association for Computational Linguistics\n68\n \n \n \nAbstract \nThe dynamics and influence of fake news \non Twitter during the 2020 US presidential \nelection remains to be clarified. Here, we \nuse a dataset related to 2020 U.S Election \nthat consists of news articles and tweets on \nthose articles. Therefore, it is extremely  \nimportant to stop the spread of fake news \nbefore it reaches a mass level, which is a \nbig challenge. We propose a novel fake \nnews detection framework that can address \nthis challenge. Our proposed framework \nexploits the information from news articles \nand social contexts to detect fake news. The \nproposed model is based on a Transformer \narchitecture, which can learn useful \nrepresentations from fake news data and \npredicts the probability of a news as being \nfake or real. Experimental results on real -\nworld data show that our model can detect \nfake news with higher accuracy and much \nearlier, compared to the baselines. \n1 Introduction \nFake news refers to false or misleading information \nthat appears as real news (Zhou & Zafarani, 2020). \nFake news can be broadly categorized as either \nmisinformation (unintentional false information) \nor disinformation (deliberate false information) . \nRecent social and political events, such as 2020 \nUnited States presidential election, have seen an \nincrease in fake news (E. Chen et al., 2021) . \nAccording to a report by First Draft News 1 , \nAmerica‚Äôs current disinformation crisis is the result \nof more than two decades of corruption in \ncountry‚Äôs information ecosystem. There are many \n \n1 https://firstdraftnews.org/latest/fake-news-complicated/ \nfactors to blame for this social and political \nmisinformation. For example, the role of social \nmedia that is u nregulated, lack of investment in \npublic media, downfall of local news outlets, and \nemergence of hyper-partisan online outlets. \nAn information (news) ecosystem consists of \npublishers (news media that publish the news \narticle), information (news content) an d users \n(Anderson, 2016). Initially, the news comes from \nthe publishers. Then, it goes to the news websites, \nfrom where it goes to the users who share news on \ndifferent platforms (blogs, social media, etc.). If the \nnews is fake, some users may find it more \nsensational and interesting to comment on and \nshare over their networks. The existence of the bots \nin social media makes it even worse , who spread \nmisinformation through multiple channels to urge \npeople believe the fake news. Therefore, it \nbecomes crucial to stop the fake news before it \nreaches to a broad audience. In this paper, we aim \nto effectively detect the fake news. \nGenerally, the content of fake news is vague and \nmisleading (C. Liu et al., 2019) . According to a \nresearch (Horne & Adalƒ±, 2017), the content of fake \nnews consists of certain patterns, such as excessive \nuse of capital letters, punctuations, or emotion -\nbearing words, which gives us clues about a news \nbeing fake or real. However, if the content of news \nis not sufficient, then the social contexts may be \nuseful to assess the veracity (truthfulness) of news. \nThe social contexts (Shu et al., 2019)  refers to \nusers‚Äô interactions, such as , comments, shares, \nlikes, followers-followees relations etc., that are \nhelpful to determine if a news fake or real.  \nSometimes, even the ve rified accounts in social \nmedia are involved in the propagation of fake news \nAutomatic Fake News Detection in Political Platforms ‚Äì \n A Transformer-based Approach  \n \n \n \nShaina Raza \nDepartment of Computer Science, Ryerson University \nshaina.raza@ryerson.ca \n \n \n \n \n \n69\n \n \n \n(Shahi et al., 2021) . In this work, we plan to \nconsider both news content and social contexts to \ndetect fake news. \nGenerally, a news item is represented by a news \nID or news title, which is not sufficient to capture \nthe patterns of fake news. There are many \nimportant pieces of information that may be more \nuseful. For example, a news body or news source \ncould be (at times) more convincing in persuading \nreaders to believe something, so, we need to pay \ncloser attention to such information . We refer to \nsuch auxiliary information as side (metadata) \ninformation. The side information associated with \na news article can be news body, source, time of \npublication, topics etc. In this work, we pla n to \nconsider different side information related to news. \nWe also consider embedded tweets on news \narticles, which provide us additional information to \ndetermine the veracity of news. \nAccording to a research, the fake  news spreads \nwithin minutes once plan ted (V osoughi et al., \n2018). For example, the fake news that Elon \nMusk‚Äôs Tesla team is inviting people to give any \namount ranging from 0.1 to 20 bitcoins in \nexchange for double the amount, resulted in a loss \nof millions of dollars within the first few minutes2. \nSo, it is critical to detect fake news early on before \nit spreads. In this work, we plan to early detect the \nfake news within few minutes after its propagation. \nIn recent years, the Transformer-based models \n(V aswani et al., 2017) have gained significant \npopularity in different NLP tasks, such as text \nclassification, detection methods . These models \nusually input whole lexical da ta as one piece of \ninformation or document, without considering any \nside information (Wu et al., 2020). In addition, the \ntemporal information is not considered (by default) \nin these models. To better utilize the strengths of \nTransformer-based models for fake news \ndetection, it is important to  include heterogenous \ninformation (main, side and temporal information) \nto build a classification mo del. In this work, we \nbuild a novel Transformer model that considers \nheterogenous information for the task of fake news \ndetection. Throughout this paper, we refer to the \nmain information as the news headline , and we \nrefer to side information as consisting of news-\nrelated features, social co ntexts (tweets), and \ntemporal information. \nWe summarize our contributions as: \n \n2 https://www.bbc.com/news/technology-56402378 \n‚Ä¢ We propose a novel Transformer model that \nconsiders news content and associated side \ninformation for the fake news detection task.  \n‚Ä¢ We incorporate heterogenous side information \nin our model. In addition to only lexical data \n(as in typical Transformers), we also consider \nthe non-lexical (numeric, categorical) data. We \nuse the multi -head attention mechanism to \nattend to different parts of such information.  \n‚Ä¢ We propose to detect fake new early within \nfew minutes after it is planted.  For that, we \nutilize the position encoding (Devlin et al., \n2018) in the Transformer model that helps us \nto achieve our goal of early detection . The \nposition encoding represents the words‚Äô order \nin a model, i.e., the value of a word (content) \nand its temporal position in a sentence.  \nWe evaluate our system by running experiments \non real-world data, which consists of news articles \nfrom various sources and social contexts from \nTwitter. Using an ablation study, we find that \nincluding both news content and social contexts is \nbeneficial in detecting fake news patterns.  The \ninclusion of more side information proves very \nuseful as indicated in the results. We also show that \nour proposed model can detect fake news earlier \nand with greater accuracy than baselines. \nThe rest o f the paper is organized as follows. \nSection 2 is about Methodology. Section 3 is for \nExperiment, and Section 4  is for Experimental \nResults and Analysis. The Related Work is covered \nin Section 5. Section 6 is the Conclusion and \nrecommendations for the future work. \n2  Methodology \nProblem Definition: Given news and associated \nside information (news-related, social contexts and \ntemporal information), the task is to determine if a \nnews item is fake or real.  \nWe consider the fake news detection task as a \nbinary classification problem (news as fake or \nreal). We also consider a multiclass classification \n(news as fake, real or mixed) in the experiments.  \nProposed Model: In our work, we modify the \nstructure of pre -trained Bidirectional Encode r \nRepresentations from Transformers (BERT)  \n(Devlin et al., 20 18) to add side information  (in \naddition to main information) . The same \nmethodology can also be applied on other \nTransformers (RobertA, XLNet, BART, T5 etc.).  \n70\n \n \n \n \n \n \n \nFigure 1: Proposed Model Faker \nWe represent each news item N by its title (main \ninformation) and side information. ( Temporal, \nnews-related, and social contexts). We believe that \nhaving more information is always beneficial. For \ninstance, the author and source provide us with \npartisan information (political party as belonging \nto right or left wing). The temporal information is \nuseful for determining whether a fake news is \nalready spread or just released. Similarly, social \ncontexts (tweets) give us additional information \nabout users‚Äô reactions on news. \nWe present a novel Transformer-based model, \nFaker, as shown in Figure 1. The input to model is \nnews items and associated side information. Each \nnews item N has a sequence of words, i.e., ùëÅ =\n{ùëõ1, ùëõ2, ‚Ä¶ , ùëõùëô}  where l is length. For each news, \nwe have the accompanying side information, i.e., \nùëÜùêº = {ùë†ùëñ1, ùë†ùëñ2, ‚Ä¶ , ùë†ùëñùëô} . In our work, we consider \ndifferent types (lexical and non -lexical) of side \ninformation, whereas our main information is \ntextual. We use ‚Äòword‚Äô as a general term to \nrepresent any word from N or feature from SI. \nThe first layer in Faker is the embedding layer. \nThe input to the embedding layer is the sequence \nof words from each input N or SI. The [CLS] token \nis added at the start of the sequence and is later used \nfor the class label prediction. We utilize the token \nand segment embedding from the BERT model to \nrepresent the syntax and semantics of each word. \nSimilar to (Q. Chen et al., 2019), we also assume \nthat temporal order exists in sequences. So, we use \nposition encoding (V aswani et al., 2017) to capture \nthe chronological information in the sequences. In \nour case, the position value of each word is decided \nby the timestamp of news publication. \nThe output from the embedding layer is then fed \ninto the next twelve layers in the first Encoder \nblock. After the encoding process, we get the \noutput vector for each word from  news. The \ncontextualized representation after the first \nEncoder block is ùëÅÃÉ = {ùëõÃÉ1, ùëõÃÉ2  ‚Ä¶ , ùëõÃÉùëô} for the news \nand ùëÜùêºÃÉ = {ùë†ùëñÃÉ1, ùë†ùëñÃÉ 2, ‚Ä¶ , ùë†ùëñÃÉ ùëô} for the side information \n(ùë†ùëñÃÉ   comes from ùë†ùëñ , the dot above i under ùë†ùëñÃÉ   is \n\n71\n \n \n \nhidden under the tilde ~). Each word vector from \nùëÅÃÉ and ùëÜùêºÃÉ  is then passed to a Fusion Block. \nFusion Block: Inside the Fusion Block, we \nrepresent each piece of information (lexical or non-\nlexical) from ùëÅÃÉ and ùëÜùêºÃÉ  with a token (word). The x \nis a textual word, nu is numeric word (feature) and \nc is a categorical word. \nInspired by the gating mechanism introduced in \n(Wang et al., 2018), we first take each feature from \nthe non-lexical data (nu and c) and combine them \nusing a gating mechanism to produce a new non-\nlexical vector h, as shown in Equation (1): \n‚Ñé = ùëîùëê ‚äô (ùëäùëêùëê) + ùëîùëõùë¢ ‚äô (ùëäùëõùë¢ùëõùë¢) + ùëè‚Ñé (1) \nwhere c is categorical feature, nu is numerical \nfeature, W denotes a weight matrix, ùëè  denotes a \nscalar bias, and ùëîùëê  and ùëîùëõùë¢  are the gating vector \nfor c and nu respectively. We may refer to ùëîùëñ as a \ngating vector for a non-lexical feature i. The ùëîùëñ is \nfused with x using an activation function R. Then it \ngoes into h. The ùëîùëñ is defined in Equation (2): \nùëîùëñ = ùëÖ(ùëäùëîùëñ [ùëñ || ùë•] + ùëèùëñ ) (2) \nOnce, we get the h, we use a weighted \nsummation between the lexical vector  x and the \ncombined non-lexical vector h to produce a fused \nsequence m, as shown in Equation (3): \nùëö = ùë• + ùõº‚Ñé (3) \nwhere x is text feature, Œ± is a normalizing factor to \ndampen the magnitude of h representation within a \nrange. The Œ± is shown in Equation (4): \nùõº = min (\n||ùë•||2\n||‚Ñé||2\n‚àó ùõΩ, 1) (4) \nwhere the ||ùë•||2 and ||‚Ñé||2 denote the ùëô2-norms of \nùë• and ‚Ñé, and hyperparameter ùõΩ is selected during \nthe validation process. Subsequently, an attention \nis applied over the lexical and non-lexical vectors \nto produce the final fused representation ùëõÃÖ . The \noutput from each Fusion Block is ùëõÃÖùëñ  and is \ncalculated for each word from the input sequence. \nThe new sequence ùëÅÃÖ = {ùëõÃÖùê∂ùêøùëÜ, ùëõÃÖ1, ùëõÃÖ2, ‚Ä¶ , ùëõÃÖùëô}  is \nthen fed as input to the next Encoder block. We \napply the Encoder layers of our model on this \nsequence ùëõÃÖ .  At the end of the second Encoder \nblock, we get the sequence ùëõÃø =\n{ùëõÃøùê∂ùêøùëÜ, ùëõÃø1, ùëõÃø2, ‚Ä¶ , ùëõÃøùëô}. The first element in ùëõÃø is the \n[CLS] token that has the necessary information to \npredict the class {real, fake} label. Therefore, the \nùëõÃøùê∂ùêøùëÜ  goes through a final transformation to \nproduce a value which can be used to predict a \nclass label. \n \n3 https://mediabiasfactcheck.com/ \n3 Experiment \nFake news data : We use the NELA -GT-2020 \ndataset (Horne, Benjamin; Gruppi, 2021) , which \ncovers a broad set of events, including the COVID- \n19 pandemic and the 2020 U.S. Presidential \nElection. In this work, we only consider the 2020 \nU.S. Election event-based data, which consists of \n294,504 related news articles across 403 sources  \nbetween January 1st, 2020 and December 31st,  \n2020. The source -level ground truth labels are \ncollected from the Media Bias/Fact Check \n(MBFC)3 website.  \nThe dataset also includes over 400,000 \nembedded Tweets found in news articles, which we \nalso employ in our research. Table 1 shows the \nfeatures of US Elections data that we use. \nWe use article IDs to create sequences based on \navailable features (in Table 1). The embedded \ntweet text is also included in the sequence. Each \nsequence record is grouped by article ID and  is \nsorted according to publication timestamp. The \nactual news articles are not labeled.  \nThe dataset only provides us the ground truth \nlabels (0 - reliable, 1 - mixed, 2 - unreliable) at \nsource-level. These source -level labels are \nobtained from MBFC, which c onsiders the \ndimensions of veracity based on a factuality \n(credibility) and on conspiracy sources. We use the \ndistant supervision (Mintz et al., 2009) to assign a \nlabel to each news story. In that, first we take the \ndistant (weak) labels provided to each news source \nand use a weighted scheme to label each news \narticle. The intuition of distant labeling is that the \ntraining labels at source -level may be imprecise \nand partial but can be used to create a strong \nFeature Description Format \nArticle ID* Article identifier  Integer \nNews title   Headline of news  Text  \nNews source * News Source (e.g., \nCNN, theonion) \nCategorical  \nNews content * News Body Text  \nAuthor * Author of article Categorical  \nURL * URL of the article Text  \nPublication \ntimestamp* \nPublication time as \nunix timestamp \nInteger  \nTweet ID * ID of tweet  Integer \nEmbedded \ntweet* \nRaw data from \ntweets (on news)  \nText  \nTable 1:  Dataset features, * is side information \n72\n \n \n \npredictive model. This approach is also suggested \nin the NELA -GT-18 paper (N√∏rregaard et al., \n2019) and has shown promising results in a recent \nwork (Horne et al., 2019). \nAfter doing the labeling, we get around 37k \nlabels as ‚Äòfake‚Äô, 12.5k labels as ‚Äòreal‚Äô and 32k \nlabels as ‚Äòmixed‚Äô. To handle the data imbalance \nproblem in in the dataset,  we use the under -\nsampling technique (Drummond et al., 2003) , in \nwhich the majority class is made closer to the \nminority class, by removing records from the \nmajority class.  Initially, we tried the SMOTE \ntechnique, in which the distribution of minority \nclass is increased by replicating some records, but \ndue to limited memory, we opt for under-sampling. \nEvaluation Metrics: To assess model perform, we \nuse accuracy ACC, precision Prec, recall Rec and \nF1-score F1, and area under curve AUC. \nCompared to ACC, AUC is usually better at \nranking predictions because AUC evaluates model \nperformance across all possible thresholds. We \ntreat the fake news detection as a binary \nclassification problem using labels {‚ÄòReal‚Äô, \n‚ÄòFake‚Äô}, and as multiclass classification using \nlabels {‚ÄòReal‚Äô, Fake‚Äô and ‚ÄòMixed‚Äô}. \nComparison Methods: For the baselines, we use: \nFake-news detection methods \n‚Ä¢ TriFN (Shu et al., 2019): A matrix factorization \nmethods that exploits user, news and publisher \nrelationships for fake news detection.   \n‚Ä¢ Declare (Popat et al., 2020): A neural network  \nthat assesses the credibility of claims on news. \n‚Ä¢ Grover (Zellers et al., 2019) : a neural \nframework to detect fake news. \nTransformer-based methods  \n‚Ä¢ BERT (Devlin et al., 2018) : Bidirectional \nEncoder Representations from Transformers. \n‚Ä¢ GPT-2 (Radford et al., 2019): Generative Pre-\ntrained Transformer model. \n‚Ä¢ VGCN-BERT  (Lu et al., 2020): Transformer-\nbased model that uses BERT with Graph \nConvolutional Network for text classification \nOther methods \n‚Ä¢ SVM (Chang & Lin, 2011) : Support Vector \nMachine model for text classification.  \n‚Ä¢ DeepWalk (Perozzi et al., 2014): Embedding-\nbased deep neural model for text classification.  \nExperimental Settings and Hyperparameters:  \nAll the experiments are conducted on the GPUs \nprovided by Google Colab Pro. We implemented \nour model using TensorFlow. The sequences are \ncreated in a chronological order of a news \npublication timestamp. We temporally split the \ntime-ordered data (by timestamps) for model \ntraining. We use the last 15% of the \nchronologically sorted data as the test set, the \nsecond to last 15% of the data as the validation set \nand the initial 75% of the data as the train set. The \nknown labels are used as the ground truth for \nmodel training and evaluation. \nIn the final settings, we  choose the following \nhyperparameters: the news stories and tweets are \non average 500 words , so we choose a sequence \nlength of 500 token. We use padding if the length \nis shorter  and truncation if it is greater . The \ndimensionality is set to be 768. Larger batch sizes \ndid not work at our end due to memory limitation. \nSo, we choose the batch size to be 8. The dropout \nrate is set be 0.25, epochs 10, learning rate 1e-3 and \nAdam optimizer is chosen for optimization. \n4 Experimental Results and Analysis \nWe present the results of binary and multi class \nclassification using ACC, F1 -score (harmonic \nmean precision and recall) and AUC in Table 2. \n4.1 Binary Classification Results \nAccording to the results shown in Table 2, our \nproposed method Faker consistently outperforms \nall other methods in inferring binary classification \nlabels (for the evaluation metrics ACC, F1-score, \nand AUC).  For example, our proposed model  \nFaker‚Äôs accuracy score in inferring news articles is \n20-30% higher than that of the state-of-the-art fake \nnews detection models (TriFN, Declare, and \nGrover), as well as Transformer -based models \n(BERT, GPT-2, and VGCN -BERT), and other \nmethods (SVM and DeepWalk).  \nTriFN outperforms other fake news detection \nbaselines (Declare, Grover) in term s of overall \nperformance. This is most likely because when we \nuse both social contexts and news content, we get \nbetter patterns for detecting fake news. \nAmong the Transformer based models, the \ngeneral performance of BERT and VGCN-BERT \nis better than GPT -2. The BERT model is more \nsuited to generative (text generation) tasks, \nwhereas the GPT -2 model is better suited to \nautoregressive (time-series) tasks. The fake news \nand Transformer -based baselines have \noutperformed the simple machine learning (SVM) \nand neural baseline (DeepWalk).  \n73\n \n \n \n4.2 Multi-label Classification Results \nIn addition to the simplified binary classification, \nwe infer instance labels using the original 3-class \nlabel space, as shown in Table 2. \nThe results show that our proposed model Faker \nconsistently outperforms all the models on \nmulticlass classification on the quality metrics:  \nACC, F1-score and AUC. Similar to the results of \nbinary classification, the general performance of \nTriFN is better than other fake news baselines. The \nBERT-based models (in general) performs better \nthan GPT-2, which outperform simple baselines. \nIn terms of efficiency, the benefits of Faker are \nfar more pronounced in the binary classification \nsetting. This is most likely due to the fact that when \nthe ‚Äòmixed' label is removed, the models are better \nable to identify the instances as real or false.  \n4.3 Sampling Ratio \n We sample the training set, which is controlled by \na sampling ratio parameter Œ∏ ‚àà {0.2, 0.4, 0.6, 0.8, \n1.0}. Here, Œ∏ = 0.2 denotes 20% and Œ∏ = 1.0 means \n100% of training instances used. We have shown \nthe results with sample ratio of 1.0 in Table 2. For \nthe other ratios, we show results in Appendices.  \nThe results in Figure 4 in Appendix A show that \nour proposed model Faker consistently \noutperforms baselines in inferring binary labels by \n5-30%.  Figure 5 in Appendix B results also show \nthat Faker‚Äôs scores during multiclass classification \nis consistently higher than other baselines for all \nvalues of Œ∏. Overall, the F1 -score and AUC of \nFaker is significantly higher in the multi -label \nclassification compared to other approaches.  \n4.4 Precision-Recall of Binary Classification \n We also test model perform on a small subset of \n4000 instances for binary classification in Table 3. \n Actual Fake Actual Real \nPredicted Fake 2008 110 \nPredicted Real 37 1845 \nTable 3:  Confusion Matrix of Sample data \n \nThe results in Table 3 show that Faker accuracy is \n96.3%. We get the precision 94.8%, which means \nthat we have a few false positives (news is real but \npredicted as fake) and we can correctly predict a \nlarge portion of true positives (i.e., news is fake and \npredicted as fake). We also get the recall value of \n98.81%, which shows that we have much more true \npositives than false negatives. Generally, a false \nnegative (news is fake but predicted as real) is a \nworse error than a false positive in fake news \ndetection. In our ex periment, we get less false \nnegatives than the false positives (which are also \nfewer). Our F1-score is 96.46%, which is also high. \n4.5 Effectiveness of Early Detection \nIn this experiment, we compare the performance of \nour model and  baselines on early fake news \ndetection. We follow the methodology of Liu and \nWu (Y . Liu & Wu, 2018) to define a propagation \npath for each news story. The idea is that any \nobservation data after the detection deadline cannot \nbe used for training. According to the research in \nfake news detection, the fake news usually takes \nless than an hour to spread. Therefore, we choose \nminutes as the unit for the detection deadlines. \n \nFigure 2: AUC of models on detection deadlines \nThe results in Figure 2 shows that, in general, the \nmodels perform better when the detection deadline \ndelayed. This is shown with the overall better \nperformance of those methods in later detection \ndeadlines (except for SVM). This probably shows \nthat more data obviously helps us to better classify \nthe truth. Our proposed Faker model consistently \n0.1\n0.3\n0.5\n0.7\n0.9\n15 min 30 min 60 min 120 min\nAUC\nFaker TriFN Grover\nDeclare BERT VGCN-BERT\nGPT-2 SVM DeepWalk\nModel/ \nMetric TriFN Grover Declare BERT VGCN-\nBERT GPT2 SVM Deep \nWalk Faker \nBinary Classification \nACC 0.695 0.602 0.579 0.690 0.652 0.602 0.459 0.620 0.824 \nF1 0.660 0.598 0.552 0.612 0.635 0.609 0.468 0.610 0.768 \nAUC 0.698 0.678 0.577 0.619 0.632 0.648 0.430 0.542 0.804 \nMulticlass Classification \nACC 0.675 0.582 0.559 0.660 0.650 0.582 0.400 0.519 0.810 \nF1 0.640 0.580 0.540 0.591 0.605 0.589 0.456 0.598 0.750 \nAUC 0.680 0.660 0.563 0.601 0.632 0.636 0.420 0.529 0.780 \nTable 2: Results of all models using Binary and Multiclass classification \n74\n \n \n \nachieves the best AUC for all the detection \ndeadlines. Faker also achieve good performance \neven in the early stage after the news is released. \nThe ability of Faker to detect early is attributed \nto its position-aware mechanism, which learns the \nhidden patterns from the sequences of news data \nand tweets, and then classify the news articles. \nUsing position encoding, the ranking position of \neach data point in a time -ordered sequence is \nconsidered. The model, then, pays more attention \nto those data points that reflect the truthfulness of \nthe news article with respect to a temporal pattern. \nFor example, the ranking position of a data point \nmight give us an important clue as to whether a \nconcerned news article is fake in the recent time. \n4.6 Ablation Study \nIn ablation study, we remove a key feature \ncomponent from a model one a time and \ninvestigate its impact on performance. Due to \nlimited space, we just show the AUC performance \nof reduced variants with binary classification. In \nour experiments, we tested many variants of Faker \nbut mention the important ones below: \nFaker: Original model with news, tweets and side \ninformation. \nFaker(n): Faker with only news -related \ninformation - removing social contexts (tweets). \nFaker(s) Faker with only social contexts. \nFaker(h-): Original Faker with headline removed \nfrom news content \nFaker(b-): Original Faker with body removed \nFaker(so-): Faker with news source removed. \nThe results are shown as in Figure 3: \n \nFigure 3: AUC of Faker‚Äôs variants \nFrom the results in Figure 3, we see that when we \nremove the social contexts as in Faker(n), the \nmodel performance is impacted but the model \nperformance is impacted more when we remove \nthe news content as in Faker (s). This probably \nshows that news related information is very \nimportant to learn the patterns of fake news. \nHowever, together both news and social contexts \ngives us more accurate results, as demonstrated by \nhighest AUC of Faker. The Faker(n) variant also \nappears to indicate that the body text is not entirely \nresponsible for the overall performance, but it is \npretty close to the default system with all features. \nThe results also show that model performance is \nimpacted more when we remove the news body, \ncompared to the removal of the headline or the \nsource of the news. This is seen with the lower \naccuracy of Faker(b -) compared to both the \nFaker(h-) and Faker(so-). This shows that headline \nand sour ce are important, but news body alone \ncarries more information. Between source and \nheadline, the source seems to be more informative, \nthis is perhaps related to the partisan information. \nWe also test different setting, for example, \nnumber of layers, dropout rate, number of heads, \nbatch size, and removing certain embedding, such \nas positional embeddings. With all these \nexperiments, we find that our current setup is the \nbest for achieving our goals. \n5 Related Work \nFollowing the 2016 election, Google, Twitter, and \nFacebook all took steps to combat fake news. \nFacebook and Twitter also allow users to mark \nnews stories as fake. A marked news story usually \nthen goes through a manual fact-checking process. \nManual fact-checking is inefficient for detecting \nfake news ea rly because it is a time -consuming \nprocess, and it is also not scalable to handle a large \nvolume of fake news online. In this paper, we look \nat automated methods for detecting fake news. \nThe automatic fake news detection methods can \nbe broadly categorized as: content-based and social \ncontexts-based methods. Most of the existing \ncontent-based detection methods (Horne & Adalƒ±, \n2017; Przybyla, 2020;  Zellers et al., 2019)  use \nstyle-based features (e.g., sentence segmentation, \ntokenization, bag-of-words, latent topics, and POS \ntagging) or linguistic features (e.g., frequencies of \nwords, case schemes, context -free grammar and \nsyntax etc.,) from news articles to detect fake news. \nOne challenge of content -based techniques is \nthat fake news style, platform, and topics are \nchanging constantly. Models trained on one dataset \nmay perform poorly on a new dataset with a \ndifferent content, style, or language. Furthermore, \nbecause the target variables in fa ke news change \nover time, certain labels become obsolete, while \nothers must be re -labeled. These algorithms also \nnecessitate a massive amount of training data to \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFaker Faker(n) Faker(s) Faker (h-) Faker (b-) Faker (so-)\nAUC\n75\n \n \n \ndetect fake news. By the time these methods gather \nenough data, fake news has spread far enough. \nTo solve the issues of content -based methods, \nthe researchers begin focusing on social contexts to \ndetect fake news. The existing social contexts -\nbased approaches are categorized into two types: \n(i) stance -based methods, and (ii) propagation -\nbased methods. The stance -based approaches \nexploit the users‚Äô viewpoints from social media \nposts to determine the truth (De Maio et al., 2020; \nY . Liu & Wu, 2020; Nakamura et al., 2020; Shu et \nal., 2019). The propagation-based methods (Huang \net al., 2020; Jiang et al., 2019; Y . Liu & Wu, 2018; \nQian et al., 2018) utilize the information related to \nthe dissemination of fake news, e.g., how users \nspread it. These methods use techniques such as \ngraphs and multi-dimensional points for fake news \ndetection (Huang et al., 2020; Y . Liu & Wu, 2018).  \nWhile social context methods are useful when \nthere is a lack of news content, they also introduce \nadditional challenges. Gathering social contexts, \nfor example, is a broad topic. The data for social \ncontexts is not only large, but also incomplete, \nnoisy, and unstructured, which may render existing \ndetection algorithms ineffective. \nFake news detection is a subtask of text \nclassification (C. Liu et al., 2019), which is solved \nby various machine learning and deep learning \nmethods. Some work (Y . Liu & Wu, 2018) uses \nRNN and CNN networks to build propagation \npaths for detecting the fake news. Some other work \n(Shu et al., 2019)  uses matrix factorization \nmethods to detect fake news. A few works (Zellers \net al., 2019)  use LSTM networks on users‚Äô \ncomments to explain if a news is real or fake. A few \nworks (Nguyen et al., 2020)  also uses graph \nnetworks to propose an explainable fake news \ndetection system. \nIn recent years, there has been a greater focus in \nNLP research using the Transformer models, such \nas BERT (Devlin et al., 2018) . BERT is used in \nsome fake news detection models (Jwa et al., 2019; \nC. Liu et al., 2019; Vijjali et al., 2020) to classify \nthe news as real or fake. Despite the robust design \nproposed in these models, a few limitations are \nnoted. First, these models do not consider a richer \nset of features from the news items and social \ncontexts. Second, the focus in these methods is not \non early fake news detection.  \nThe inclusion of temporal information is \nimportant to early detect fake news (Y . Liu & Wu, \n2020). Also the inclusion of  side (meta-data) \ninformation related to news or social contexts  is \nimportant to understand the nature of fake news \ndata (Shu et al., 2019) . Recently, an exploratory \nstudy (Shahi et al., 2021)  on fake news gives us \nmore new insights about the timeline of \nmisinformation. In our work, we consider both the \ntemporal and side information to detect fake news. \nThe existing works on fake news focus either on \nnews content or social contexts to detect fake news, \nwe consider both in our work. Compared to some \nprevious works (Nguyen et al., 2020; Popat et al., \n2020; Shu et al., 2019)  that consider both these \naspects, we include a wider set of news-related as \nwell as social context (tweets). A few works (Y . Liu \n& Wu, 2020 ; Shu et al., 2019)  propose early \ndetection of fake news. Compared to these \nmethods, we can detect fake detect the fake news \nmuch earlier (i.e., after a few minutes of news \npropagation). Compared to the previous works, we \nconsider the latest state -of-the-art neural \narchitectures (Transformers). \n6 Conclusion and Recommendations \nIn our work, we propose a Transformer-based \narchitecture for fake news detection. We utilize the \nnews content and social contexts to detect the \npatterns of fake news. We also early detect the fake \nnews through a position -aware encoding.  We \nachieve higher performance compared to the \nbaselines, which shows the usefulness o f our \nproposed approach. In addition to fake news \ndetection, this model can also serve for general \nclassification tasks. \nTo further improve the proposed method , a \nrecommendation is to consider more social \ncontexts, such as friends‚Äô networks, propagation \npaths and implicit users‚Äô feedbacks. It would also \nbe very useful to consider malicious social media \nusers‚Äô profiles and their activities. Another \nrecommendation is to combat data and concept \ndrifts. It would also be very useful to understand \nthe tactics of fake news producers in real -time \nscenarios. Furthermore, data labelling scheme can \nbe investigated because of the possibility of \nincorrectly labelled data, which may lead to data \nbiases (Kishore Shahi, 2020). A possible extension \nof this work is to mitigate those biases.  We also \nwant to break filter bubbles and burst echo \nchambers created due to the spread of fake news. \n76\n \n \n \nReferences  \nAnderson, C. W. (2016). News ecosystems. The SAGE \nHandbook of Digital Journalism, 410‚Äì423. \nChang, C.-C., & Lin, C.-J. (2011). LIBSVM: a library \nfor support vector machines. ACM Transactions on \nIntelligent Systems and Technology (TIST), 2(3), 1‚Äì\n27. \nChen, E., Chang, H., Rao, A., Lerman, K., Cowan, G., \n& Ferrara, E. (2021). COVID -19 misinformation \nand the 2020 US presidential election. The Harvard \nKennedy School Misinformation Review. \nChen, Q., Zhao, H., Li, W., Hua ng, P., & Ou, W. \n(2019). Behavior sequence transformer for E -\ncommerce recommendation in Alibaba. \nProceedings of the ACM SIGKDD International \nConference on Knowledge Discovery and Data \nMining.  \nDe Maio, C., Fenza, G., Gallo, M., Loia, V ., & V olpe, \nA. (2020) . Cross -relating heterogeneous Text \nStreams for Credibility Assessment. IEEE \nConference on Evolving and Adaptive Intelligent \nSystems, 2020-May.  \nDevlin, J., Chang, M. W., Lee, K., & Toutanova, K. \n(2018). BERT: Pre -training of deep bidirectional \ntransformers for language understanding. ArXiv \nPreprint ArXiv:1810.04805. \nDrummond, C., Holte, R. C., & others. (2003). C4. 5, \nclass imbalance, and cost sen sitivity: why under -\nsampling beats over -sampling. Workshop on \nLearning from Imbalanced Datasets II, 11, 1‚Äì8. \nHorne, Benjamin; Gruppi, M. (2021). NELA -GT-\n2020: A Large Multi-Labelled News Dataset for The \nStudy of Misinformation in News Articles. ArXiv \nPreprint ArXiv:2102.04567.  \nHorne, B. D., & Adalƒ±, S. (2017). This just in: Fake \nnews packs a lot in title, uses simpler, repetitive \ncontent in text body, more similar to satire than real \nnews. ArXiv Preprint ArXiv:1703.09398 \nHorne, B. D., N√òrregaard, J., & Ada li, S. (2019). \nRobust fake news detection over time and attack. \nACM Transactions on Intelligent Systems and \nTechnology, 11(1).  \nHuang, Q., Zhou, C., Wu, J., Liu, L., & Wang, B. \n(2020). Deep spatial‚Äìtemporal structure learning for \nrumor detection on Twitter. Neural Computing and \nApplications, August.  \nJiang, S., Chen, X., Zhang, L., Chen, S., & Liu, H. \n(2019). User-characteristic enhanced model for fake \nnews detection in social media. CCF International \nConference on Natural Language Processing and \nChinese Computing, 634‚Äì646. \nJwa, H., Oh, D., Park, K., Kang, J. M., & Lim, H. \n(2019). exBAKE: Automatic fake news detection \nmodel based on Bidirectional Encoder \nRepresentations from Transformers (BERT). \nApplied Sciences (Switzerland), 9(19), 4062.  \nKaliyar, R. K., G oswami, A., Narang, P., & Sinha, S. \n(2020). FNDNet ‚Äì A deep convolutional neural \nnetwork for fake news detection. Cognitive Systems \nResearch, 61, 32‚Äì44.  \nKishore Shahi, G. (2020). AMUSED: An Annotation \nFramework of Multi -modal Social Media Data. \narXiv preprint arXiv:2010.00502. \nLiu, C., Wu, X., Yu, M., Li, G., Jiang, J., Huang, W., & \nLu, X. (2019). A Two-Stage Model Based on BERT \nfor Short Fake News Detection. Lecture Notes in \nComputer Science (Including Subseries Lecture \nNotes in Artificial Intelligence and Lecture Notes in \nBioinformatics), 172‚Äì183.  \nLiu, Y ., & Wu, Y . F. B. (2018). Early detection of fake \nnews on social media through propagation path \nclassification with recurrent and convolutional \nnetworks. 32nd AAAI Conference on Artificial \nIntelligence, AAAI 2018, 354‚Äì361. \nLiu, Y ., & Wu, Y . F. B. (2020). FNED: A Deep Network \nfor Fake News Early Detection on Social Media. \nACM Transactions on Information Systems, 38(3).  \nLu, Z., Du, P., & Nie, J. Y . (2020). VGCN -BERT: \nAugmenting BERT with Graph Embedding for Text \nClassification. In Lecture Notes in Computer \nScience (including subseries Lecture Notes in \nArtificial Intelligence and Lecture Notes in \nBioinformatics): Vol. 12035 LNCS.  \nMintz, M., Bills, S., Snow, R., & Jurafsky, D. (2009). \nDistant supervision for relation extraction without \nlabeled data. Proceedings of the Joint Conference of \nthe 47th Annual Meeting of the ACL and the 4th \nInternational Joint Conference on Natural \nLanguage Processing of the AFNLP, 1003‚Äì1011. \nMohammadrezaei, M., Shiri, M. E., & Rahmani, A. M. \n(2018). Identifying Fake Accounts on Social \nNetworks Based on Graph Analysis and \nClassification Algorithms. Security and \nCommunication Networks, 2018.  \nNakamura, K., Levy, S., & Wang, W. Y . (2020). \nr/Fakeddit: A new multimodal benc hmark dataset \nfor fine-grained fake news detection. In LREC 2020 \n- 12th International Conference on Language \nResources and Evaluation, Conference \nProceedings.  \nNguyen, V .-H., Nakov, P., & Kan, M. -Y . (2020). \nFANG: Leveraging Social Context for Fake News \nDetection Using Graph Representation. \nProceedings of the 29th ACM International \n77\n \n \n \nConference on Information & Knowledge \nManagement, 1165-1174 \nN√∏rregaard, J., Horne, B. D., & Adalƒ±, S. (2019). \nNELA-GT-2018: A large multi -labelled news \ndataset for the study of mi sinformation in news \narticles. Proceedings of the 13th International \nConference on Web and Social Media, ICWSM \n2019, Icwsm, 630‚Äì638.  \nPerozzi, B., Al -Rfou, R., & Skiena, S. (2014). \nDeepwalk: Online learning of social \nrepresentations. Proceedings of the 20t h ACM \nSIGKDD International Conference on Knowledge \nDiscovery and Data Mining, 701‚Äì710. \nPopat, K., Mukherjee, S., Yates, A., & Weikum, G. \n(2020). Declare: Debunking fake news and false \nclaims using evidence -aware deep learning. In \nProceedings of the 2018 Co nference on Empirical \nMethods in Natural Language Processing, EMNLP \n2018.  \nPrzybyla, P. (2020). Capturing the Style of Fake News. \nProceedings of the AAAI Conference on Artificial \nIntelligence, 34(01), 490‚Äì497.  \nQian, F., Gong, C., Sharma, K., & Liu, Y . (20 18). \nNeural user response generator: Fake news \ndetection with collective user intelligence. IJCAI \nInternational Joint Conference on Artificial \nIntelligence, 2018-July, 3834‚Äì3840.  \nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., \n& Sutskever, I. (2019) . Language models are \nunsupervised multitask learners. OpenAI Blog, 1(8), \n9. \nShahi, G. K., Dirkson, A., & Majchrzak, T. A. (2021). \nAn exploratory study of COVID-19 misinformation \non Twitter. Online Social Networks and Media,  \n100104 \nShu, K., Wang, S., & Liu , H. (2019). Beyond news \ncontents: The role of social context for fake news \ndetection. WSDM 2019 - Proceedings of the 12th \nACM International Conference on Web Search and \nData Mining, 9, 312‚Äì320.  \nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., \nJones, L., Gomez, A. N., Kaiser, ≈Å., & Polosukhin, \nI. (2017). Attention is all you need. Advances in \nNeural Information Processing Systems , 5998 ‚Äì\n6008. \nVijjali, R., Potluri, P., Kumar, S., & Teki, S. (2020). \nTwo stage transformer model for covid -19 fake \nnews detection and fact checking. ArXiv Preprint \nArXiv:2011.13253. \nV osoughi, S., Roy, D., & Aral, S. (2018). The spread of \ntrue and false news online. Science, 359(6380), \n1146‚Äì1151. \nWang, Y ., Shen, Y ., Liu, Z., Liang, P. P., Zadeh, A., & \nMorency, L. P. (2018). Words Can Shift: \nDynamically Adjusting Word Representations \nUsing Nonverbal Behaviors. In Proceedings of the \nAAAI Conference on Artificial Intelligence, 7216-\n7223. \nWu, F., Qiao, Y ., Chen, J.-H., Wu, C., Qi, T., Lian, J., \nLiu, D., Xie, X., Gao, J., Wu, W., & Zhou, M. \n(2020). MIND: A Large -scale Dataset for News \nRecommendation. Proceedings of the 58th Annual \nMeeting of the Association for Computational \nLinguistics, 3597‚Äì3606.  \nYang, S., Shu, K., Wang, S., Gu, R., Wu, F., & Liu, H. \n(2019). Unsupervised fake news detection on social \nmedia: A generative approach. Proceedings of the \nAAAI Conference on Artificial Intelligence, 33(01), \n5644‚Äì5651. \nZellers, R., Holtzman, A., Rashkin, H., Bi sk, Y ., \nFarhadi, A., Roesner, F., & Choi, Y . (2019). \nDefending against neural fake news. In \narXivpreprint arXiv:1905.12616. \nZhou, X., & Zafarani, R. (2020). A Survey of Fake \nNews: Fundamental Theories, Detection Methods, \nand Opportunities. ACM Computing Surveys, 53(5).  \n \n  \n78\n \n \n \nAppendix A. Binary Classification Sampling \nRatios \n \nFigure 4 (a): Binary Classification Accuracy \n \n \nFigure 4 (b): Binary Classification F1-score \n \n \nFigure 4 (c): Binary Classification AUC \nAppendix B. Multiclass Classification \nSampling Ratios \n \nFigure 5(a): Multiclass Classification ACC \n \n \nFigure 5(b): Binary Classification F1-score \n \n \nFigure 5(c): Multi-label Classification AUC \n \n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.2 0.4 0.6 0.8\nACCURACY\nSAMPLING RATIO\nFaker TriFN Grover\nDeclare BERT VGCN-BERT\nGPT-2 SVM DeepWalk\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.2 0.4 0.6 0.8\nF1-SVORE\nSAMPLING RATIO\nFaker TriFN Grover\nDeclare BERT VGCN-BERT\nGPT-2 SVM DeepWalk\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.2 0.4 0.6 0.8\nF1-SVORE\nSAMPLING RATIO\nFaker TriFN Grover\nDeclare BERT VGCN-BERT\nGPT-2 SVM DeepWalk\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.2 0.4 0.6 0.8\nACCURACY\nSAMPLING RATIO\nFaker TriFN\nGrover Declare\nBERT VGCN-BERT\nGPT-2 SVM\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.2 0.4 0.6 0.8\nF1-SCORE\nSAMPLING RATIO\nFaker TriFN\nGrover Declare\nBERT VGCN-BERT\nGPT-2 SVM\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.2 0.4 0.6 0.8\nAUC\nSAMPLING RATIO\nFaker TriFN\nGrover Declare\nBERT VGCN-BERT\nGPT-2 SVM",
  "topic": "Fake news",
  "concepts": [
    {
      "name": "Fake news",
      "score": 0.7980165481567383
    },
    {
      "name": "Computer science",
      "score": 0.7493796348571777
    },
    {
      "name": "Exploit",
      "score": 0.7005103826522827
    },
    {
      "name": "Social media",
      "score": 0.6199641227722168
    },
    {
      "name": "Transformer",
      "score": 0.525118887424469
    },
    {
      "name": "Presidential election",
      "score": 0.5170262455940247
    },
    {
      "name": "Big data",
      "score": 0.44950026273727417
    },
    {
      "name": "Architecture",
      "score": 0.4312414824962616
    },
    {
      "name": "Internet privacy",
      "score": 0.3257802426815033
    },
    {
      "name": "Politics",
      "score": 0.31254398822784424
    },
    {
      "name": "Computer security",
      "score": 0.2779240608215332
    },
    {
      "name": "World Wide Web",
      "score": 0.26919272541999817
    },
    {
      "name": "Political science",
      "score": 0.1961427927017212
    },
    {
      "name": "Data mining",
      "score": 0.1835295855998993
    },
    {
      "name": "Engineering",
      "score": 0.07815665006637573
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I530967",
      "name": "Toronto Metropolitan University",
      "country": "CA"
    }
  ],
  "cited_by": 17
}