{
    "title": "PSLT: A Light-Weight Vision Transformer With Ladder Self-Attention and Progressive Shift",
    "url": "https://openalex.org/W4362692084",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2584344947",
            "name": "Gaojie Wu",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2248194359",
            "name": "Wei-Shi Zheng",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2122795086",
            "name": "Yutong Lu",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A1993201802",
            "name": "Qi Tian",
            "affiliations": [
                "Huawei Technologies (China)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6798046796",
        "https://openalex.org/W2998508940",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W2949736877",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W6757817989",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W6764990469",
        "https://openalex.org/W2204750386",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6745136726",
        "https://openalex.org/W6797790494",
        "https://openalex.org/W2097117768",
        "https://openalex.org/W4313170858",
        "https://openalex.org/W6800092589",
        "https://openalex.org/W6802648153",
        "https://openalex.org/W6838836003",
        "https://openalex.org/W6639102338",
        "https://openalex.org/W2752782242",
        "https://openalex.org/W6795463671",
        "https://openalex.org/W6794345597",
        "https://openalex.org/W6811093309",
        "https://openalex.org/W4313007769",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W2931208564",
        "https://openalex.org/W4214614183",
        "https://openalex.org/W2963842104",
        "https://openalex.org/W4312960790",
        "https://openalex.org/W6779989637",
        "https://openalex.org/W6795901243",
        "https://openalex.org/W6735531217",
        "https://openalex.org/W4312986923",
        "https://openalex.org/W2964137095",
        "https://openalex.org/W2963446712",
        "https://openalex.org/W6796931752",
        "https://openalex.org/W2928165649",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W6637373629",
        "https://openalex.org/W2549139847",
        "https://openalex.org/W3173635859",
        "https://openalex.org/W2984145721",
        "https://openalex.org/W3151130473",
        "https://openalex.org/W4214736485",
        "https://openalex.org/W2986093954",
        "https://openalex.org/W2967515867",
        "https://openalex.org/W4214493665",
        "https://openalex.org/W6753421600",
        "https://openalex.org/W6839178539",
        "https://openalex.org/W6696085341",
        "https://openalex.org/W4214588794",
        "https://openalex.org/W3160694286",
        "https://openalex.org/W6784333009",
        "https://openalex.org/W6631943919",
        "https://openalex.org/W2601564443",
        "https://openalex.org/W2963150697",
        "https://openalex.org/W6797153837",
        "https://openalex.org/W2910628332",
        "https://openalex.org/W4214713996",
        "https://openalex.org/W2737258237",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W6684191040",
        "https://openalex.org/W4214636423",
        "https://openalex.org/W6799772243",
        "https://openalex.org/W4312856857",
        "https://openalex.org/W6787972765",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W3172752666",
        "https://openalex.org/W6729956949",
        "https://openalex.org/W4312849330",
        "https://openalex.org/W2963125010",
        "https://openalex.org/W2916798096",
        "https://openalex.org/W3139445856",
        "https://openalex.org/W3096533519",
        "https://openalex.org/W6788135285",
        "https://openalex.org/W6737664043",
        "https://openalex.org/W6804624231",
        "https://openalex.org/W4214709605",
        "https://openalex.org/W2982083293",
        "https://openalex.org/W6805224724",
        "https://openalex.org/W2963163009",
        "https://openalex.org/W6839204056",
        "https://openalex.org/W6752515464",
        "https://openalex.org/W6762718338",
        "https://openalex.org/W3177349073",
        "https://openalex.org/W6802010497",
        "https://openalex.org/W6795308139",
        "https://openalex.org/W3109269658",
        "https://openalex.org/W4287203089",
        "https://openalex.org/W2884822772",
        "https://openalex.org/W3167597877",
        "https://openalex.org/W3171087525",
        "https://openalex.org/W4312820606",
        "https://openalex.org/W4287257983",
        "https://openalex.org/W3175544090",
        "https://openalex.org/W1533861849",
        "https://openalex.org/W3159663321",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2951104886",
        "https://openalex.org/W3190492058",
        "https://openalex.org/W2953937638",
        "https://openalex.org/W4320036918",
        "https://openalex.org/W3118608800",
        "https://openalex.org/W3190216403",
        "https://openalex.org/W3157528469",
        "https://openalex.org/W2765407302",
        "https://openalex.org/W4312950730",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W2598634450",
        "https://openalex.org/W4312977443",
        "https://openalex.org/W2884561390",
        "https://openalex.org/W4309845474",
        "https://openalex.org/W2553303224",
        "https://openalex.org/W3164208409",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W2963840672",
        "https://openalex.org/W3206810688",
        "https://openalex.org/W4287236877",
        "https://openalex.org/W4287330514",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W4283320581",
        "https://openalex.org/W4287025584",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W4286910290",
        "https://openalex.org/W4295308583",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4313160444",
        "https://openalex.org/W4287324101",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W4297775537",
        "https://openalex.org/W4281756776",
        "https://openalex.org/W4287274255",
        "https://openalex.org/W3035314311"
    ],
    "abstract": "Vision Transformer (ViT) has shown great potential for various visual tasks due to its ability to model long-range dependency. However, ViT requires a large amount of computing resource to compute the global self-attention. In this work, we propose a ladder self-attention block with multiple branches and a progressive shift mechanism to develop a light-weight transformer backbone that requires less computing resources (e.g., a relatively small number of parameters and FLOPs), termed Progressive Shift Ladder Transformer (PSLT). First, the ladder self-attention block reduces the computational cost by modelling local self-attention in each branch. In the meanwhile, the progressive shift mechanism is proposed to enlarge the receptive field in the ladder self-attention block by modelling diverse local self-attention for each branch and interacting among these branches. Second, the input feature of the ladder self-attention block is split equally along the channel dimension for each branch, which considerably reduces the computational cost in the ladder self-attention block (with nearly [Formula: see text] the amount of parameters and FLOPs), and the outputs of these branches are then collaborated by a pixel-adaptive fusion. Therefore, the ladder self-attention block with a relatively small number of parameters and FLOPs is capable of modelling long-range interactions. Based on the ladder self-attention block, PSLT performs well on several vision tasks, including image classification, objection detection and person re-identification. On the ImageNet-1 k dataset, PSLT achieves a top-1 accuracy of 79.9% with 9.2 M parameters and 1.9 G FLOPs, which is comparable to several existing models with more than 20 M parameters and 4 G FLOPs. Code is available at https://isee-ai.cn/wugaojie/PSLT.html.",
    "full_text": "1\nPSLT: A Light-weight Vision Transformer with\nLadder Self-Attention and Progressive Shift\nGaojie Wu, Wei-Shi Zheng*, Yutong Lu, Qi Tian\n*Corresponding author: Wei-Shi Zheng.\nProject page with code: https://isee-ai.cn/wugaojie/PSLT.html\nSubmission date: 08-Jul-2022 to IEEE Transaction on Pattern Analysis\nand Machine Intelligence\nFor reference of this work, please cite:\nGaojie Wu, Wei-Shi Zheng, Yutong Lu and Qi Tian. ‚ÄúPSLT: A Light-weight Vision Transformer with Ladder Self-Attention\nand Progressive Shift‚Äù. IEEE Transaction on Pattern Analysis and Machine Intelligence, 2023.\nBib:\n@article{wu2023pslt,\ntitle = {PSLT: A Light-weight Vision Transformer with Ladder Self-Attention and Progressive Shift},\nauthor = {Gaojie Wu, Wei-Shi Zheng, Yutong Lu and Qi Tian},\njournal = {{IEEE}Transaction on Pattern Analysis and Machine Intelligence},\nyear = {2023}\n}\narXiv:2304.03481v1  [cs.CV]  7 Apr 2023\nSUBMISSION TO IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 2\nPSLT: A Light-weight Vision Transformer with\nLadder Self-Attention and Progressive Shift\nGaojie Wu, Wei-Shi Zheng*, Yutong Lu and Qi Tian\nAbstract‚ÄîVision Transformer (ViT) has shown great potential for various visual tasks due to its ability to model long-range\ndependency. However, ViT requires a large amount of computing resource to compute the global self-attention. In this work, we\npropose a ladder self-attention block with multiple branches and a progressive shift mechanism to develop a light-weight transformer\nbackbone that requires less computing resources (e.g. a relatively small number of parameters and FLOPs), termed Progressive Shift\nLadder Transformer (PSLT). First, the ladder self-attention block reduces the computational cost by modelling local self-attention in\neach branch. In the meanwhile, the progressive shift mechanism is proposed to enlarge the receptive Ô¨Åeld in the ladder self-attention\nblock by modelling diverse local self-attention for each branch and interacting among these branches. Second, the input feature of the\nladder self-attention block is split equally along the channel dimension for each branch, which considerably reduces the computational\ncost in the ladder self-attention block (with nearly 1\n3 the amount of parameters and FLOPs), and the outputs of these branches are then\ncollaborated by a pixel-adaptive fusion. Therefore, the ladder self-attention block with a relatively small number of parameters and\nFLOPs is capable of modelling long-range interactions. Based on the ladder self-attention block, PSLT performs well on several vision\ntasks, including image classiÔ¨Åcation, objection detection and person re-identiÔ¨Åcation. On the ImageNet-1k dataset, PSLT achieves a\ntop-1 accuracy of 79.9% with 9.2M parameters and 1.9G FLOPs, which is comparable to several existing models with more than 20M\nparameters and 4G FLOPs. Code is available at https://isee-ai.cn/wugaojie/PSLT.html.\nIndex Terms‚ÄîMultimedia Information Retrieval, Light-weight Vision Transformer, Ladder Self-Attention.\n!\n1 I NTRODUCTION\nConvolutional neural networks (CNNs) have shown\nconsiderable promise as general-purpose backbones for\ncomputer vision tasks. Since AlexNet [1] was proposed\nfor the ImageNet image classiÔ¨Åcation challenge [2], CNN\narchitectures have become increasingly powerful, with more\ncareful designs [3], [4], [5], deeper connections [6], [7] and\nwider dimensions [8]. Thus, CNNs appear to be indispens-\nable for various computer vision tasks. CNNs are successful\ndue to the inductive bias implied in convolutional com-\nputations, and this inductive bias ensures that CNNs are\ngeneralizable as investigated in [9], [10].\nAnother prevalent architecture that has shown extraor-\ndinary achievements in computer vision tasks is the vision\ntransformer. In the transformer, which was Ô¨Årst proposed\nfor sequence modelling and transduction tasks in natural\nlanguage processing [11], [12], features can interact globally\nby modelling attention of long-range dependency. Vision\nTransformer (ViT) models global interaction by computing\nattention among the feature map according to the projected\nquery, key and value of each pixel. ViT has shown these\nachievements due to the considerable amount of computing\nresource in the model. However, models with large number\n* Corresponding author\n‚Ä¢ Gaojie Wu and Yutong Lu are with the School of Computer Science and\nEngineering, Sun Yat-sen University, Guangzhou 510275, China. E-mail:\nwugj7@mail2.sysu.edu.cn, yutong.lu@nscc-gz.cn.\n‚Ä¢ Wei-Shi Zheng is with the School of Data and Computer Science,\nSun Yat-sen University, Guangzhou 510275, China, with Peng Cheng\nLaboratory, Shenzhen 518005, China, and also with the Key Labora-\ntory of Machine Intelligence and Advanced Computing (Sun Yat-sen\nUniversity), Ministry of Education, China. E-mail: wszheng@ieee.org\n/zhwshi@mail.sysu.edu.cn.\n‚Ä¢ Qi Tian is with the Cloud & AI BU, Huawei, China\n(tian.qi1@huawei.com).\nof parameters are difÔ¨Åcult to deploy in edge-computing\ndevices with limited memory storage and computing re-\nsources, such as FPGAs. And models with large number\nof FLOPs always require much time for inference, because\nFLOPs measures the number of Ô¨Çoat-point operations for\ninference.\nIn this work, we aim to develop a light-weight vision\ntransformer backbone with a relatively small number of\nparameters and FLOPs. Existing methods [13], [14], [15],\n[16] have focused on reducing the number of Ô¨Çoat-point\noperations by evolving the form of computing self-attention;\nhowever, the receptive Ô¨Åeld in the window-based self-\nattention block is restricted in most of these methods [15],\n[16], and only pixels divided in the same windows can\ninteract with each other in the window-based self-attention\nblock. Therefore, the interactions between pixels in different\nwindows cannot be modelled in one block.\nThus, we propose a light-weight ladder self-attention\nblock with multiple branches and a progressive shift mech-\nanism is introduced to enlarge the receptive Ô¨Åeld explicitly.\nThe expansion of the receptive Ô¨Åeld for the ladder self-\nattention block is accomplished according to the following\nstrategy. First, diverse local self-attentions are modelled\nby steering pixels at the same spatial position to model\ninteractions with pixels in diverse windows for different\nbranches. Second, the progressive shift mechanism trans-\nmits the output features of the current branch to the sub-\nsequent branch. The self-attention in the subsequent branch\nis computed with the participation of the output features\nin the current branch, allowing interactions among features\nin different windows in the two branches. As a result,\nthe ladder self-attention block with the progressive shift\nmechanism can model long-range interactions among pixels\nSUBMISSION TO IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 3\ndivided in different windows. In addition, in the ladder\nself-attention block, each branch only takes an equal pro-\nportion of input channels of the block, which considerably\nreduces the number of parameters and FLOPs. And all the\nchannels in these branches are aggregated in the proposed\npixel-adaptive fusion module to generate the output of the\nladder self-attention block. Based on the above designs, we\ndeveloped a light-weight general-purpose backbone with a\nrelatively small number of parameters.\nThe overall framework of our model is shown in Figure\n1. According to the conclusion of [9], [10] that the convo-\nlution in the early stages helps the model learn better and\nfaster. PSLT adopts light-weight convolutional blocks in the\nearly stages and the ladder self-attention blocks in the latter\nstages. Our PSLT has several important characteristics:\n1) PSLT uses light-weight ladder self-attention blocks,\nwhich greatly reduce the number of trainable pa-\nrameters and FLOPs. The ladder self-attention block\nÔ¨Årst divides the input feature map into several equal\nproportions along the channel axis. Then, each part\nof the feature map is sent to an individual branch to\ncompute the self-attention similarity.\n2) The ladder-self attention block is designed to ac-\nquire large receptive Ô¨Åeld, requiring relatively small\nnumber of computing resources. PSLT models lo-\ncal attention on each branch for computing efÔ¨Å-\nciency; and more importantly, without introducing\nextra parameters, PSLT adopts the progressive shift\nmechanism to model interactions among pixels in\ndifferent windows to enlarge the receptive Ô¨Åeld of\neach ladder self-attention block. The progressive\nshift mechanism forms a block shaped like a ladder.\nOur proposed PSLT achieves excellent performance on\nvisual tasks such as image classiÔ¨Åcation, object detection\nand person re-identiÔ¨Åcation with a relatively small number\nof trainable parameters. With less than 10 million parame-\nters and 2G FLOPs, PSLT achieves a top-1 accuracy of 79.9%\non ImageNet-1k image classiÔ¨Åcation at input resolution\n224 √ó224 without pretraining on extra large dataset.\nWe notice that recently there exist light-weight trans-\nformers [17], [18], [19] that combine local interaction (convo-\nlution) and global self-attention in one block. In this work,\nwe provide another perspective, and our PSLT is capable\nof modelling long-range interaction by effectively incorpo-\nrating diverse local self-attentions and evolving the self-\nattention of the latter branches in the ladder self-attention\nblock with a relatively small number of parameters and\nFLOPs.\nIn summary, our contributions are as follows:\n‚Ä¢ We propose a light-weight ladder self-attention block\nwith multiple branches and considerably less param-\neters. A pixel-adaptive fusion module is developed\nto aggregate features from multiple branches with\nadaptive weights along both the spatial and channel\ndimensions.\n‚Ä¢ We propose a progressive shift mechanism in the\nladder self-attention block to enlarge the receptive\nÔ¨Åeld. The progressive mechanism not only allows\nmodelling interaction among pixels in different win-\ndows for long-range dependencies, but also reduces\nthe number of parameters necessary to obtain the\nvalue for computing self-attention in the following\nbranch.\n‚Ä¢ We develop a general-purpose backbone (PSLT) with\na relatively small number of parameters. PSLT is\nconstructed with light-weight convolutional blocks\nand the ladder self-attention blocks, improving its\ngeneralization ability.\n2 R ELATED WORK\nA comparison of various models is shown in Table 1. Our\nPSLT was manually developed and has a relatively small\nnumber of parameters, and PSLT does not utilize a search\nphase before training. In the following section, we detail\nsome related works.\n2.1 Convolutional Neural Networks\nSince the development of AlexNet [1], CNNs have become\nthe most popular general-purpose backbones for various\ncomputer vision tasks. More effective and deeper convo-\nlutional neural networks have been proposed to further\nimprove the model capacity for feature representation, such\nas VGG [6], ResNet [3], DenseNet [7] and ResNext [5].\nHowever, neural networks with large scales are difÔ¨Åcult\nto deploy in edge devices, which have limited memory\nresources. Thus, various works have focused on designing\nlight-weight neural networks with less trainable parameters.\n- Light-weight CNN. To decrease the number of trainable\nparameters, MobileNets [20], [21], [22] substitute the stan-\ndard convolution operation with a more efÔ¨Åcient combi-\nnation of depthwise and pointwise convolution. ShufÔ¨ÇeNet\n[23] uses group convolution and channel shufÔ¨Çe to further\nsimplify the model. The manual design of neural networks\nis time consuming, and automatically designing convolu-\ntional neural architectures has shown great potential for\ndeveloping high-performance neural networks [24], [25],\n[26]. EfÔ¨ÅcientNet [27] investigates the model width, depth\nand resolution with a neural architecture search [24], [25],\n[28].\nAlthough CNNs can effectively model local interactions,\nadaptation to input data is absent in standard convolution.\nUsually, the adaptive methods are capable of improving\nmodel capacity by the dynamic weights or receptive Ô¨Åelds\nwith respective to different input images. RedNet [29] pro-\nduces dynamic weights for distinct input data. Deformable\nconvolution [30] produces an adaptive location for the con-\nvolution kernels. Our proposed backbone (PSLT) inherits\nthe virtue of convolution that implies inductive bias when\nmodelling local interactions, and PSLT takes advantage of\nself-attention [11] to model long-range dependency and\nadapt to input data with dynamic weights.\n2.2 Vision Transformer\nTransformers [11], [12] are widely used in natural language\nprocessing, and the transformer architecture for computer\nvision tasks has evolved since ViT [31] was proposed for im-\nage classiÔ¨Åcation. ViT has one-stage structure that produces\nfeatures with only one scale. PiT [39] adopts depthwise\nconvolution [20] to decrease the spatial dimension. CrossViT\nSUBMISSION TO IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 4\nTABLE 1\nComparison of various general-purpose backbones. ‚ÄúT‚Äù indicates that\nthe model adopts the transformer architecture. ‚ÄúAdaptive‚Äù indicates that\nthe model can adapt to the input image. ‚ÄúLight‚Äù denotes that the model\nhas a relatively small number of parameters and FLOPs. ‚ÄúNAS‚Äù\ndenotes that the model needs another search process before training\nfrom scratch.\nModel Architecture Adaptive Light NAS\nVGG [6] CNN \u0017 \u0017 \u0017\nResNet [3] CNN \u0017 \u0017 \u0017\nDenseNet [7] CNN \u0017 \u0017 \u0017\nMobileNet [20] CNN \u0017 \u0013 \u0017\nShufÔ¨ÇeNet [23] CNN \u0017 \u0013 \u0017\nEfÔ¨ÅcientNet CNN \u0013 \u0017 \u0013\nRedNet [29] CNN \u0013 \u0017 \u0017\nViT [31] T \u0013 \u0017 \u0017\nPS-ViT [32] T \u0013 \u0017 \u0017\nSwin [15] T \u0013 \u0017 \u0017\nVOLO [33] T \u0013 \u0017 \u0017\nPVT [14] T \u0013 \u0017 \u0017\nMobileViT [19] T \u0013 \u0013 \u0017\nEfÔ¨ÅcientFormer [34] T \u0013 \u0013 \u0013\nCoATNet CNN+T \u0013 \u0017 \u0017\nCvT [35] CNN+T \u0013 \u0017 \u0017\nCMT [13] CNN+T \u0013 \u0017 \u0017\nConformer [36] CNN+T \u0013 \u0017 \u0017\nBossNet [26] CNN+T \u0013 \u0017 \u0013\nLeViT [37] CNN+T \u0013 \u0013 \u0017\nMobile-Former [38] CNN+T \u0013 \u0013 \u0017\nPSLT (Ours) CNN+T \u0013 \u0013 \u0017\n[40] models multi-scale features via small and large patch\nembedding sizes. Recent transformer architectures [13], [14],\n[15], [37], [41] have adopted hierarchical structures like\npopular CNNs to yield multi-scale features through patch\nmerging to progressively reduce the number of patches. To\nmitigate the issue that the tokenization in ViT [42] may de-\nstruct the object structure, PS-ViT [32] locates discriminative\nregions by iteratively sampling tokens.\nSince self-attention requires a large amount of comput-\ning resources to model long-range interactions, recent works\n[15], [16], [43] have approximated global interaction by mod-\nelling local and long-range dependency. SwinTransformer\n[15] divides the feature map into multiple windows, self-\nattention is only conducted for interactions among pixels in\nthe same window, and the shift operation is proposed for\nlong-range interactions. VOLO [33] generates the outlook\nattention matrix for a local window. DAT [44] generates de-\nformable attention similar to deformable convolution [30].\nTwins [45] Ô¨Årst reduces the size of the feature map and then\nconducts self-attention and upsamples the feature map to\nits original size. PVT [14], ResT [46] and CMT [13] maintain\nthe size of the query while reducing the size of the key\nand value to process the self-attention operation. CSWin [47]\nadopts the cross-shaped window self-attention mechanism\nfor computing efÔ¨Åciency.\n- CNN + Transformer The combination of convolution and\nself-attention has shown great potential in computer vision\ntasks. CoATNet [9] investigates the best method for allo-\ncating the convolutional blocks and transformer blocks in\nTABLE 2\nComparison of light-weight vision transformers. ‚ÄúT‚Äù indicates that the\nmodel adopts the transformer architecture. ‚ÄúLocal‚Äù indicates the\nmodelling of local interaction. ‚ÄúGlobal‚Äù indicates the modelling of\nlong-range interaction. ‚ÄúGSA‚Äù denotes that the model adopts global\nself-attention. ‚ÄúLSA‚Äù denotes that the model adopts local self-attention.\nModel Architecture Local Global\nMobile-Former [38] CNN+T CNN GSA\nMobileViT [19] CNN+T CNN GSA\nEdgeViT [17] CNN+T CNN GSA\nEdgeNext [18] CNN+T CNN GSA\nPSLT (Ours) CNN+T LSA LSA\ndifferent stages to achieve the optimal model generalization\nability and capacity. BossNet [26] uses a neural architecture\nsearch method [24] to automatically determine the best\nmethod to combine the convolutional block and transformer\nblock. LeViT [37] proposes a hybrid neural network for\nfast inference image classiÔ¨Åcation. LeViT utilizes convolu-\ntion with a kernel size of 3 to halve the feature size four\ntimes and downsamples during the self-attention operation\nto reduce the number of Ô¨Çoat-point operations. CvT [35]\nintroduces depthwise and pointwise convolution to replace\nthe fully connected layer, yielding the query, key and value\nin the multi-head self-attention mechanism. Conformer [36]\nuses a dual network structure with convolution and self-\nattention for enhanced representation learning. CMT [13]\nalso introduces depthwise and pointwise convolution in\nmulti-head self-attention mechanism, and an inverted resid-\nual FFN is used in place of a standard FFN for improvement.\nMPViT [48] combines the convolution and multi-scale self-\nattention for multi-scale representation.\n- Light-weight vision transformers. The achievements of\nvision transformers rely on the large number of comput-\ning resource (parameters and FLOPs). Mobile-Former [38]\nleverages the advantages of MobileNet for local processing\nand the advantages of the transformer for global interaction.\nEdgeViT [17] and EdgeNeXt [18] also combine local interac-\ntion (convolution) and global self-attention. MobileViT [19]\nis also a light-weight general-purpose vision transformer for\nmobile devices. MobileViT leverages a multi-scale sampler\nfor efÔ¨Åcient training. LVT [49] also develops enhanced self-\nattention for computing efÔ¨Åciency.\nIn this work, we propose a light-weight ladder self-\nattention block requiring a relatively small number of pa-\nrameters and FLOPs. Instead of combining convolution and\nglobal self-attention for local and global interactions in a\nblock, our ladder self-attention block enlarges the receptive\nÔ¨Åeld by modelling and interacting the diverse local self-\nattentions, as shown in Table 2. And modelling local self-\nattention is more computationally efÔ¨Åcient than modelling\nglobal self-attention. Then we develop a light-weight trans-\nformer backbone with the ladder self-attention block in the\nlast two stages and light-weight convolutional blocks in the\nÔ¨Årst two stages, and the allocation of self-attention blocks\nand convolutional blocks is proved with better performance\nin CoAtNet [9].\nSUBMISSION TO IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 5\n3 P ROGRESSIVE SHIFT LADDER TRANSFORMER\n(PSLT)\n3.1 Overall Architecture\nIn this work, we aim to develop a light-weight trans-\nformer backbone and expand the receptive Ô¨Åeld of the basic\nwindow-based self-attention block. We propose a ladder\nself-attention block with multiple branches. Each branch\ntakes an equal proportion of input features and adopts\nthe window-based self-attention, considerably reducing the\nnumber of parameters and Ô¨Çoat-point operations in the\nladder self-attention block.\nA progressive shift mechanism is formed to enlarge the\nreceptive Ô¨Åeld of the ladder self-attention block with the\nfollowing strategy. Each branch shifts the obtained features\nin different directions and divides the shifted features into\nmultiple windows. In this manner, the output features of\neach branch can be aggregated from diverse windows.\nFurthermore, the latter branch takes the output feature of\nthe previous branch as input for computing self-attention,\nallowing pixels in different windows of various branches\nto interact with one another. With the progressive shift\nmechanism, the light-weight ladder self-attention block is\ncapable of modelling long-range interactions. Because the\noutput feature of each branch is integrated with pixels in\ndifferent spatial windows, a pixel-adaptive fusion module is\ndeveloped to effectively integrate the output features of all\nthe branches with adaptive weights along both the spatial\nand channel dimensions.\nBased on the above considerations, we Ô¨Ånally develop\na light-weight general-purpose backbone with a relatively\nsmall number of trainable parameters based on the pro-\nposed ladder self-attention block, termed Progressive Shift\nLadder Transformer (PSLT). An overview of PSLT is shown\nin Figure 1. PSLT leverages the advantages of convolution\nfor local interaction and self-attention for long-range inter-\naction. An input image is passed through a stem convolu-\ntion layer and four stages consisting of convolutional blocks\nor the proposed ladder self-attention blocks. The output\nfeature of the Ô¨Ånal stage is sent to the global average pooling\nlayer and classiÔ¨Åer.\nTo ensure that PSLT is applicable to various computer vi-\nsion tasks, we adopt a four-stage architecture following the\nSwin Transformer [15] to yield the hierarchical features. To\nimprove the model generalization ability and capacity, PSLT\nadopts light-weight convolutional blocks in MobileNetV2\n[21] with a squeeze-and-excitation (SE) block [50] in the\nÔ¨Årst two stages and ladder self-attention in the Ô¨Ånal two\nstages. In the Ô¨Ånal two stages, the proposed light-weight\nladder self-attention blocks are applied to model long-range\ninteractions. Note that the input feature is downsampled\nat the beginning of each stage, and these stages jointly\nyield multi-scale features similar to prevalent convolutional\narchitectures. With these hierarchical representations, PSLT\ncan be easily applied as the backbone model in existing\nframeworks for various computer vision tasks. For image\nclassiÔ¨Åcation, the output feature of the Ô¨Ånal stage, with\nabstract semantics representing the global information, is\ntransmitted to the global average pooling layer and classi-\nÔ¨Åer.\nIn addition, in contrast to splitting the input RGB image\ninto non-overlapping patches (Patch Partition) for the initial\nrepresentation, as is performed in most commonly used\nvision transformer architectures, PSLT applies the popular\nstem with three convolution layers for feature extraction, as\nsplitting the image into non overlapping patches may divide\nthe same part of an object into different patches. The 3 √ó3\nconvolution in the Ô¨Årst process is capable of modelling local\ninteractions with the implied inductive bias.\nDifferent from existing light-weight transformer [38],\nwhich greatly reduces the number of parameters in back-\nbones and introduces multiple parameters in the classiÔ¨Å-\ncation head, PSLT has less parameters in the classiÔ¨Åcation\nhead. Our experiments show that PSLT uses only 6% (0.57 M\nof 9.2 M) of the parameters in the classiÔ¨Åcation head, which\nis considerably lower than Mobile-Former-294M [38] using\n40% (4.6 M of 11.4 M) of the parameters in the classiÔ¨Åcation\nhead of the two fully connected layers.\n3.2 Ladder Self-Attention Block\nTo model interaction among pixels in different windows, we\npropose a ladder self-attention block with multiple branches\nand model long-range interaction among pixels in different\nbranches through the progressive shift mechanism. Since\nthe feature maps are integrated in diverse spatial windows\nin each branch of the ladder self-attention block, a pixel-\nadaptive fusion module is developed for effective feature\nfusion among these branches. The difference of processing\nfeature maps in these branches allows the proposed block to\nobtain diverse information, and the proposed pixel-adaptive\nfusion module is capable of efÔ¨Åciently aggregating the di-\nverse information. In the following section, we describe the\ndetails of the ladder self-attention block.\n3.2.1 Progressive Shift for the Ladder Self-Attention Block\nThe ladder self-attention block divides the input feature\nmap into several equal proportions along the channel di-\nmension and sends these features to multiple branches.\nTake the ladder self-attention block in Figure 1 for example;\nthe input feature map with d channels is divided into\nthree parts, each with d\n3 channels for each branch. PSLT\nadopts window-based self-attention to only compute sim-\nilarity among pixels in the same windows, thus ensuring\ncomputing efÔ¨Åciency. Different from SwinTransformer [15]\nthat stacks even number of window-based self-attention\nblocks to enlarge the receptive Ô¨Åeld with shift operation,\nour PSLT adopts the progressive shift mechanism to model\ndiverse local interactions in each branch and to interact in-\nformation among these branches, which explicitly enlarges\nthe receptive Ô¨Åeld of each ladder self-attention block. In this\nmanner, PSLT acquires large receptive Ô¨Åeld with a relatively\nsmall number of parameters and FLOPs.\nFor long-range interaction, PSLT proposes a progressive\nshift mechanism for the ladder self-attention block. First, the\ninput features in the Ô¨Årst branch are processed by the PSA\nwith the W-MHSA without the shift operation, as shown\nin Figure 3 (a). And the features of the other two branches\nare shifted differently to model diverse local interactions,\nnamely pixels from two windows in one branch may be\ndivided in the same window in the other branches. The\nSUBMISSION TO IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 6\nInput Image\nStem\nMBV2+SE \nblock √ó7\nLadder self-attention    \nblock √ó10\nLadder self-attention  \nblock √ó3\n2√ó2 Conv stride=2\nGlobal Avg Pool\nClassifier\nstage1-2 stage3 stage4\nLayerNorm\nPSW-MHSA\nLayerNrom\nLight FFN\nPSA\nInput Image\nPatch Partition\nSwin Transformer \nblock √ó2\nSwin Transformer\nblock √ó2\nSwin Transformer  \nblock √ó2\nstage1 stage2 stage3 (6 blocks) stage4\nLayerNorm\n(S)W-MHSA\nLayerNrom\nFFN\nSwin Transformer block \nInput Image\nPatch Partition\nLayerNorm\nMHSA\nLayerNrom\nFFN\nTransformer Encoder\nTransformer block √ó12\nPosition Embedding\nclass token\nClassifier Classifier\n(a) ViT-S (b) Swin-T (C) PSLT\n1√ó1 Conv \n3√ó3 DWConv\n1√ó1 Conv\nùëë2\nùëë2/4\nùëë2/4\nùëë2\nLight FFN\nShift 1\nPixel-Adaptive Fusion Module\nLadder self-attention block\nùëë1/3 ùëë1/3\nùëë1/3\nPSA\nPSA\nPSA Shift 1\nShift 2\nShift 2\nFig. 1. Illustration of prevalent transformer architectures for image classiÔ¨Åcation. (a) ViT [31] with only one stage. (b) Swin Transformer [15], which\nuses window-based multi-head self-attention and multi-stage blocks. (c) Our proposed PSLT adopts light-weight ladder self-attention (SA) blocks\nin the Ô¨Ånal two stages to model long-range dependency of pixels divided in different windows in the same block, and the light-weight convolutional\nblocks (MobileNetV2 [21] block with a squeeze-and-excitation (SE) block [50]) are adopted in the Ô¨Årst two stages. The details of PSLT are described\nin Section 3, and the structures of the PSW-MHSA and pixel-adaptive fusion module are shown in Figure 3. ‚ÄúShift 1‚Äù and ‚ÄúShift 2‚Äù denotes shift\noperations with different directions. ‚Äúd1‚Äù and ‚Äúd2‚Äù denote the number of channels, andd1 = 3d2.\nBlock i\nBlock i+1\n(a) W-MHSA (b) PSW-MHSA\naggregation\nFig. 2. Illustration of the window partition for pixel interactions in each\nblock. (a) The original W-MHSA generates windows according to only\none strategy to model local attention in each block. (b) The PSW-MHSA\nproduces different windows with multiple strategies and fuses these\nfeatures to aggregate information from diverse spatial windows.\nshift direction or stride of the two branches is different from\neach other to model long-range interactions. An illustration\nof modelling interaction in different windows is shown in\nFigure 2. Second, the progressive shift manner transmits\nthe output feature of the current branch to the subsequent\nbranch. As shown in Figure 3 (b), the PSW-MHSA takes\nboth the output feature of the previous branch and the\ncurrent input feature as input for self-attention computation\nto model interactions among the branches.\nMore speciÔ¨Åcally, the PSW-MHSA applies the same shift\nand window partition operations to both the input feature\nof the current branch and the output feature of the previous\nbranch. A 1 √ó1 convolution is applied to the input fea-\nture to yield the query and key to compute the similarity\namong the feature points. Instead of yielding the value in\nthe same manner, the PSW-MHSA takes the output of the\nprevious branch as the value directly for the self-attention\ncomputation. The PSW-MHSA progressively delivers the\noutput features of the branches, allowing it to model long-\nrange dependency because after the local interactions are\nmodelled, the pixels constrained in the divided windows\nof the current branch can interact with pixels outside the\nwindow in the following branches.\nInstead of consecutive connection of local self-attention\nand shifted local self-attention for long-range interaction\n[15], our PSW-MHSA enables building backbone with arbi-\ntrary number of ladder self-attention blocks instead of even\nnumber of blocks. In this way, the ladder self-attention block\ncan model long-range interactions among pixels in different\nwindows with the help of multiple branches, and the train-\nSUBMISSION TO IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 7\nInput\n1x1 Conv 1x1 Conv\nQ K\nAttentionV\n1x1 Conv\n‚àô\nMLP\nShift\nWindow \nPartition\nInput\n1x1 Conv 1x1 Conv\nQ K\nAttention\nPSA\nMLP\nWindow \nPartition\nShift\nWPShift\nW-MHSA\nùëë1\nùëë1/2\nùëë1/2\n‚àô\n+PSW-MHSA\nLadder self-attention block\n(a) W-MHSA (b) PSW-MHSA\nInput Input\nùëë1 ùëë1 ùëë1 ùëë1/2 ùëë1/2\nConcatenate\nFC\nFC\nWeights\nFC\n‚àô\nPixel-Adaptive Fusion Module\n(c) Pixel-Adaptive Fusion Module\nùëë1/2 ùëë1/2\nùëë1\nùëë1/2\nùëë1\nFig. 3. Example of detailed blocks in PSLT. (a) Window-based multi-head self-attention in the Swin Transformer [15]. (b) The ladder self-attention\nblock with two branches for example. The progressive shift mechanism transmits output feature of the previous branch to the subsequent branch,\nwhich further enlarges the receptive Ô¨Åeld. Only the PSW-MHSA is illustrated in detail; LayerNrom and Light FFN are shown in Figure 1 and omitted\nhere for simplicity. ‚ÄúWP‚Äù indicates the window partition. (c) The pixel-adaptive fusion module in the ladder self-attention block. ‚ÄúWeights‚Äù denotes the\nadaptive weights along the channel and spatial dimensions, which is the output of the second fully connected layer. The details of the PSW-MHSA\nare described in Section 3.2.1.\ning cost (number of trainable parameters) is decreased .\nFormally, the ladder self-attention blocks are computed\nas\nÀÜOt = PSW-MHSA(It,Ot‚àí1)\nPSW-MHSA(It,Ot‚àí1) = softmax(QIt KIt‚àö\nd\n)Ot‚àí1 + It\nOt = LFFN(LN( ÀÜOt))\n¬ØO= PAFM(Ot),t = 0,1,2,....\n(1)\nThe output ( ÀÜOt) of the PSW-MHSA in the t-th branch is\ncomputed according to the input feature of the t-th branch\n(It) and the output feature of the (t‚àí1)-th branch ( Ot‚àí1).\nIn the PSW-MHSA, the query ( QIt ) and the key ( KIt ) are\nprojected from It, and Ot‚àí1 is directly applied as the value.\nThe Ô¨Årst branch is computed as O0 = MHSA(I0), as shown\nin Figure 3 (a). Then, the Light FFN (LFFN) and layer norm\n(LN) are applied to produce the output of the t-th branch\n(Ot). Finally, a pixel-adaptive fusion module (PAFM) is\ndeveloped to generate the output of the ladder self-attention\nblock ( ¬ØO) according to the outputs of all branches.\n- Complexity analysis. Our intention is to build a light-\nweight backbone, and the PSW-MHSA is proposed for mod-\nelling long-range dependency. Here, we roughly compare\nthe number of trainable parameters and computation com-\nplexity of the W-MHSA and PSW-MHSA. Taking the input\nof h√ów √ód1 as an example, each window has M √óM\nnon-overlapping patches, and B branches are contained in\nthe PSW-MHSA. Then, the number of trainable parameters\nin the W-MHSA and PSW-MHSA are:\nœÜ(W-MHSA) = 4d2\n1\nœÜ(PSW-MHSA) = 3d2\n1\nB + d2\n1\nB2 .\n(2)\nThe computational complexity of the W-MHSA and PSW-\nMHSA are:\n‚Ñ¶(W-MHSA) = 4hwd2\n1 + 2M2hwd1\n‚Ñ¶(PSW-MHSA) = 3hwd2\n1\nB + hwd2\n1\nB2 + 2M2hwd1.\n(3)\nAs can be seen, when B = 1, the PSW-MHSA has the same\nnumber of trainable parameters and complexity as the W-\nMHSA. When B >1, the number of trainable parameters\nand the computation complexity are both considerably re-\nduced (Bis set to 3 by default).\n3.2.2 Light FFN\nAs presented in Figure 1 (b), the output of the MHSA is\nprocessed by the FFN. To further decrease the number of\ntrainable parameters and Ô¨Çoat-point operations in PSLT, a\nLight FFN is proposed to replace the original FFN, as shown\nin Figure 1 (c). In contrast to the original FFN, which uses\nSUBMISSION TO IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 8\na fully connected layer with d channels for both the input\nand output features, the Light FNN Ô¨Årst projects the input\nwith d2 channels to a narrower feature with d2\n4 channels.\nThen, a depthwise convolution is applied to model local\ninteractions, and a pointwise convolution is adopted to\nrestore the channels.\n- Complexity analysis. Taking the input of an FFN layer\nof size h√ów√ód2 as an example, the number of trainable\nparameters of the FFN and LFFN are:\nœÜ(FFN) = d2\nœÜ(LFFN) = d2\n2\n2 + 9d2\n4 .\n(4)\nThe computational complexity of the FFN and LFFN are:\n‚Ñ¶(FFN) = hwd2\n2\n‚Ñ¶(LFFN) = hwd2\n2\n2 + 9hwd2\n16 .\n(5)\nIn general, the number of input feature channels d2 is\nsubstantially larger than 9\n2 ; thus, the LFFN is more compu-\ntationally economical than the FFN, as it has less trainable\nparameters and Ô¨Çoat-point operations.\n3.2.3 Pixel-Adaptive Fusion Module\nPSLT divides the input feature map equally along the\nchannel dimension for the multiple branches to decrease\nthe number of parameters and computation complexity. To\nmodel long-range dependency, the progressive shift mech-\nanism in the ladder self-attention block divides the same\npixel into different windows in each branch. Because the\noutput features of the branches are produced by integrating\nfeature information in different windows, PSLT proposes a\npixel-adaptive fusion module to efÔ¨Åciently aggregate multi-\nbranch features with adaptive weights along the spatial and\nchannel dimensions.\nMore speciÔ¨Åcally, the output features of all the branches\nare concatenated and sent to two fully connected layers to\nyield the weights for each pixel. The weights indicate the\nimportance of the features from all the branches for each\npixel in the feature map. To ensure that the number of train-\nable parameters is not increased, the Ô¨Årst fully connected\nlayer halves the number of channels. The features are then\nmultiplied by the weights and then sent to a fully connected\nlayer for feature fusion along the channel dimension.\nDifferent from the conventional squeeze-and-excitation\nblock [50] measuring only the importance of the channels,\nwe introduce the pixel-adaptive fusion module to integrate\nthe output features of all branches with adaptive weights in\nboth the spatial and channel dimensions for two reasons.\nFirst, because the pixels in each branch are divided into\ndifferent windows, the pixels at various spatial locations\ncontain distinct information. Second, along the channel\ndimension, the input feature map is split equally for each\nbranch, so the pixel information at the same spatial location\nis different among the branches. The experimental results\nshow that our proposed pixel-adaptive fusion module is\nmore suitable for feature fusion than the squeeze-and-\nexcitation block [50].\nTABLE 3\nSpeciÔ¨Åcation for PSLT. ‚Äúconv2d‚Äù and ‚ÄúMBV2Block‚Äù indicate the\nstandard convolutional operation and the light-weight block in the\nMobileNetV2 [21] with the squeeze-and-excitation layer [50]. ‚Äúconv2d‚Üì‚Äù\nand ‚ÄúMBV2Block+SE‚Üì‚Äù denote the operation or block for downsampling\nwith stride 2. ‚ÄúFC‚Äù denotes the fully connected layer. PSLT has a total\nof 9.223M parameters according to the table.\nStage Input Block #OutStride#Params\nStem 2242 √ó3\n1122 √ó36\n3 √ó3 conv2d‚Üì\n3 √ó3 conv2d √ó2\n36\n36\n2\n1 0.018M\n1 1122 √ó36\n562 √ó72\nMBV2Block+SE‚Üì\nMBV2Block+SE √ó2\n72\n72\n2\n1 0.125M\n2\n562 √ó72\n282 √ó144\n282 √ó144\nMBV2Block+SE‚Üì\nMBV2Block+SE √ó2\nMBV2Block+SE‚Üì\n144\n144\n288\n2\n1\n2\n0.817M\n3 142 √ó288Ladder SA block √ó10 288 1 3.879M\n4 142 √ó288\n72 √ó576\n2 √ó2 conv2d‚Üì\nLadder SA block √ó3\n576\n576\n2\n1 3.808M\nhead 72 √ó576\n12 √ó576\n7 √ó7, pool\nFC\n576\n1000\n1\n1 0.576M\n3.3 Network SpeciÔ¨Åcation\nTable 3 shows the architecture of PSLT, which contains\n9.2M trainable parameters for image classiÔ¨Åcation on the\nImageNet dataset [2]. PSLT stacks the light-weight convo-\nlutional blocks (MBV2Block+SE) in the Ô¨Årst two stages and\nthe proposed ladder self-attention blocks in the Ô¨Ånal two\nstages. PSLT starts with three 3 √ó3 convolution as stem to\nproduce the input feature of the Ô¨Årst stage. In both stage\n1 and stage 2, the Ô¨Årst convolutional block has a stride of\n2 to downsample the input feature map. The Ô¨Ånal block in\nstage 2 is utilized to downsample the input feature map of\nstage 3. A 2 √ó2 convolution is applied to downsample the\nfeature map of stage 4. Each time the size of the feature map\nis halved, the number of feature channels doubles.\nFinally, the classiÔ¨Åer head uses global average pooling\non the output of the last block. The globally pooled feature\npasses through a fully connected layer to yield an image\nclassiÔ¨Åcation score. Without the classiÔ¨Åer head, PSLT with\nmulti-scale features can easily be applied as a backbone in\nexisting methods for various visual tasks.\n- Detailed conÔ¨Åguration. Following SwinTransformer [15],\nthe window size is set to 7 √ó7, and the window partition\nand position embedding is produced with the mechanism\nsimilar to [15]. The number of heads in PSW-MHSA is set\nto 4 and 8 in the last two stages respectively. The stride in\nthe shift operation (shown in Figure 1 (c)) in PSW-MHSA\nis set to 3. The shift operation is similar to that in [15], but\nthe shift direction is different from each other in the last two\nbranches.\nSUBMISSION TO IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 9\n4 E XPERIMENT\nIn this section, we evaluate the effectiveness of the proposed\nPSLT by conducting experiments on several tasks, including\nImageNet-1k image classiÔ¨Åcation [2], COCO objection detec-\ntion [51] and Market-1501 person re-identiÔ¨Åcation [52].\n4.1 Image ClassiÔ¨Åcation\n4.1.1 ImageNet Results\n- Experimental setting. We evaluate our proposed PSLT\non ImageNet [2] classiÔ¨Åcation. The ImageNet dataset has\n1,000 classes, including 1.2 million images for training and\n50 thousand images for validation. We train our PSLT on\nthe training set, and the top-1 accuracy on the validation\nset and the ImageNet-V2 [53] is reported. For a fair com-\nparison, we follow Mobile-Former [38] when performing\nthe ImageNet classiÔ¨Åcation experiments. The images were\nrandomly cropped to 224 √ó224. The data augmentation\nmethods include horizontal Ô¨Çipping [54], mix-up [55], auto-\naugmentation [56] and random erasing [57]. Our PSLT was\ntrained from scratch on 8 A6000 GPUs with label smoothing\n[58] using the AdamW [59] optimizer for 450 epochs with\na cosine learning rate decay. The distillation is not adopted\nfor training. The initial learning rate was set to 6√ó10‚àí4, and\nthe weight decay was set to 0.025 by default. For images in\nthe validation set, we adopted the center crop, with images\ncropped to 224 √ó224 for evaluation.\n- Experimental results. Table 4 shows a comparison of the\nperformance of our proposed PSLT and the performance\nof several convolution-based models and transformer-based\nmodels. The Ô¨Årst block shows the performance of our\nproposed PSLT. The second block in Table 4 presents the\nperformance of models with less than 10M parameters,\nincluding light-weight convolutional neural networks and\nrecently proposed transformer-based models. Compared to\nan efÔ¨Åcient convolution-based model with a similar num-\nber of parameters (ShufÔ¨ÇeNetV2 + WeightNet [61]), PSLT\nachieves a signiÔ¨Åcantly higher top-1 accuracy (79.9% vs.\n75.0%) with slightly fewer parameters (9.2M vs. 9.6M) but\nmore FLOPs (1.9G vs. 307M). With a similar number of\nparameters (9.2M vs. 9.5M) and FLOPs (1.9G vs. 2.0G),\nPSLT achieves a top-1 accuracy that is 2% higher than the\npure transformer-based DeiT-2G (79.9% vs. 77.6%). Several\nrecent works have focused on leveraging the advantages\nof CNNs and transformer-based architectures to improve\nmodel generalization ability and capacity, including LeViT\n[37] and CMT [13]. It should be noted that our proposed\nPSLT (trained for 450 epochs without distillation) achieves a\nhigher top-1 accuracy (79.9% vs. 78.6%) than LeViT (trained\nfor 1,000 epochs with distillation) with a similar number\nof parameters. With a similar number of parameters (4.3M\nvs. 4.2M) and FLOPs (876M vs. 0.6G), PSLT-Tiny achieves\na slightly higher top-1 accuracy (74.9% vs. 74.4%) than\nEdgeViT-XXS [17]. And PSLT achieves comparable perfor-\nmance to MPViT-T [41] with a similar number of FLOPs.\nAlthough our PSLT shows slightly inferior performance\ncompared to the EfÔ¨ÅcientNet (79.9% vs. 80.1%) with simi-\nlar number of parameters, the EfÔ¨ÅcientNet is obtained by\nthe carefully designed neural architecture search algorithm,\nwhich is time-consuming. And our PSLT achieves higher\nperformance than the EfÔ¨ÅcientNet on the experiments of\nCOCO for object detection in Table 6 and instance segmenta-\ntion in Table 7. Furthermore, our PSLT still achieves higher\nperformance than EfÔ¨ÅcientFormer-L1 which automatically\nÔ¨Ånds efÔ¨Åcient transformers with similar neural architecture\nsearch algorithm to EfÔ¨ÅcientNet.\nCompared to those recently developed lightweight net-\nworks [17], [18], [19], [37], [48], our PSLT provides a different\nperspective for acting as an effective backbone with a rela-\ntively small number of parameters and FLOPs. The above\nexperimental results demonstrate that PSLT with diverse\nlocal self-attentions for long-range dependency is capable\nof achieving comparable performance, and modelling the\nlocal self-attention requires less computing resources than\nmodelling the global self-attention.\nThe third block in Table 4 shows the performance of\nmodels with 10M ‚àº20M parameters. PSLT outperforms\npure transformer-based architectures with a similar number\nof parameters and FLOPs (9.2M/1.9G), including PvT-Tiny\n(13.2M/1.9G) and DeiT-2G (6.5M/2.0G), and PSLT achieves\na slightly higher top-1 accuracy than Swin-2G (79.9% vs.\n79.2%) with slightly fewer parameters. PSLT slightly outper-\nforms CNN-based models and transformer-based models,\nwhere PSLT achieves a higher top-1 accuracy than Mobile-\nFormer (79.9% vs. 79.3%) with signiÔ¨Åcantly fewer parame-\nters (9.2M vs. 14.0M) and more FLOPs (1.9G vs. 508M), and\nPSLT achieves a top-1 accuracy that is 3% higher than ConT-\nS (79.9% vs. 76.5%) with a similar number of parameters and\nFLOPs. PSLT also achieves comparable performance to the\nsearched light-weight model EfÔ¨ÅcientFormer-L1 [34] (79.9%\nvs. 79.2%), where PSLT requires a relatively smaller number\nof parameters (9.2M vs. 12.2M) and FLOPs (1.9G vs. 2.4G).\nThe last block in Table 4 shows the performance of mod-\nels with more than 20M parameters for reference, including\nthe commonly used ResNet and its evolved architectures.\nCompared to the pure transformer models, PSLT achieves\na comparable top-1 accuracy to PVT-S (79.9% vs. 79.8%)\nwith nearly 50% fewer parameters (9.2M vs. 24.5M) and\nFLOPs (1.9G vs. 3.8G). PSLT-Large with smaller amount of\nparameters and FLOPs (16.0M/3.4G) also outperforms re-\ncent transformer structures, including DeiT-S (22.0M/4.6G),\nPS-ViT-B/10 (21.3M/3.1G) and BossNet-T0 (3.4G). And the\nPSLT-Large also achieves a slightly higher top-1 accuracy\nthan Swin [15] (81.5% vs. 81.3%) with fewer parameters\n(16.0M vs. 29.0M) and FLOPs (3.4G vs. 4.5G).\nThe ImageNet classiÔ¨Åcation experiments show that our\nproposed PSLT outperforms both convolution-based and\ntransformer-based models with similar number of parame-\nters. These results validate the effectiveness of the proposed\nladder self-attention block.\n4.1.2 CIFAR Results\n- Experimental setting. The CIFAR-10/100 [70] datasets\ncontain 10/100 classes, including 50,000 images for training\nand 10,000 images for testing, with an image resolution of\n32 √ó32. We trained PSLT from scratch on the training set,\nand the top-1 accuracy on the test set is reported. PSLT was\ntrained for 300 epochs using the AdamW [59] optimizer\nwith cosine learning rate decay using an initial learning rate\nof 5 √ó10‚àí4 and a weight decay of 0.025.\n- Experimental results. Table 5 shows the image classiÔ¨Å-\ncation performance of the vision transformer models on\nSUBMISSION TO IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 10\nTABLE 4\nImage classiÔ¨Åcation performance on the ImageNet without pretraining. Models with similar number of parameters are presented for comparison.\n‚ÄúInput‚Äù indicates the scale of the input images. ‚ÄúTop-1‚Äù (‚ÄúV2 Top-1‚Äù) denotes the top-1 accuracy on ImageNet (ImageNet-V2). ‚Äú#Params‚Äù refers to\nthe number of trainable parameters. ‚Äú#FLOPs‚Äù is calculated according to the corresponding input size. ‚Äú+‚Äù indicates that the performance is cited\nfrom Mobile-Former [38]. ‚Äú*‚Äù indicates that the model was trained with distilling from an external teacher. ‚Äú‚Ä°‚Äù denotes extra techniques are applied,\nsuch as multi-scale sampler [19] and EMA [2]. ‚ÄúPAcc‚Äù (‚ÄúFAcc‚Äù) is the ratio of Top-1 accuracy to the number of parameters (FLOPs).\nModel NAS Input #Params #FLOPs Top-1 PAcc (%/M) FAcc (%/G) V2 top-1\nPSLT-Tiny(Ours) \u0017 2242 4.3M 876M 74.9% 17.42 85.5 63.0%\nPSLT(Ours) \u0017 2242 9.2M 1.9G 79.9% 8.68 42.05 68.6%\nPSLT-Large(Ours) \u0017 2242 16.0M 3.4G 81.5% 5.1 24.0 70.3%\nMobileNetV3 [22] \u0013 2242 4.0M 155M 73.3% 18.33 472.9 ‚Äì\nEdgeViT-XXS [17] \u0017 2242 4.1M 0.6G 74.4% 18.14 124 ‚Äì\nMobile-Former [38] \u0017 2242 4.6M 96M 72.8% 15.83 758.3 ‚Äì\nMobileViTv2-S [19]‚Ä° \u0017 2562 4.9M 1.8G 78.1% 15.94 43.39 ‚Äì\nLVT [49] \u0017 2242 5.5M 0.9G 74.8% 13.6 83.11 ‚Äì\nMobileViT-S [19]‚Ä° \u0017 2562 5.6M 2.0G 78.4% 14 39.2 ‚Äì\nEdgeNeXt-S [18]‚Ä° \u0017 2242 5.6M 965M 78.8% 14.07 81.66 ‚Äì\nMPViT-T [48] \u0017 2242 5.8M 1.6G 78.2% 13.48 48.88 ‚Äì\nHRFormer-T [60] \u0017 2242 8.0M 1.8G 78.5% 9.81 43.61 ‚Äì\nLeViT [37]‚àó \u0017 2242 9.2M 406M 78.6% 8.54 193.6 66.6%\nEfÔ¨ÅcientNet [27] \u0013 2242 9.2M 1.0G 80.1% 8.71 80.1 68.8%\nRedNet-26 [29] \u0017 2242 9.2M 1.7G 75.9% 8.25 44.65 ‚Äì\nCMT-Ti [13] \u0017 1922 9.5M 0.6G 79.2% 8.34 132 ‚Äì\nShufÔ¨ÇeV2+Weight [61] \u0017 2242 9.6M 307M 75.0% 7.81 244.3 ‚Äì\nConT-S [62] \u0017 2242 10.1M 1.5G 76.5% 7.57 51 ‚Äì\nCoat-Lite mini [63] \u0017 2242 11.0M 2.0G 79.1% 7.19 39.55 ‚Äì\nShunted-T [64] \u0017 2242 11.5M 2.1G 79.8% 6.94 38 ‚Äì\nGC ViT-XXT [65] \u0017 2242 12.0M 2.1G 79.6% 6.63 37.9 ‚Äì\nPoolFormer-S12 [66] \u0017 2242 12.0M ‚àº4.2G 77.2% 6.43 18.38 ‚Äì\nEfÔ¨ÅcientFormer-L1 [34] \u0013 2242 12.2M 2.4G 79.2% 6.49 33 ‚Äì\nSwin-2G [15]+ \u0017 2242 12.8M 2.0G 79.2% 6.19 39.6 ‚Äì\nPvT-Tiny [14] \u0017 2242 13.2M 1.9G 75.1% 5.69 39.53 ‚Äì\nResT-Small [46] \u0017 2242 13.7M 1.9G 79.6% 5.81 41.89 ‚Äì\nMobile-Former [38] \u0017 2242 14.0M 508M 79.3% 5.66 156.1 ‚Äì\nHRNet-W18 [67] \u0017 2242 21.3M 4.0G 76.8% 3.56 19.2 ‚Äì\nPS-ViT-B/10 [32] \u0017 2242 21.3M 3.1G 80.6% 3.78 26 ‚Äì\nA-ViT-S [68] \u0017 2242 22.0M 3.6G 78.6% 3.57 21.83 ‚Äì\nDeiT-S [69] \u0017 2242 22.0M 4.6G 79.8% 3.63 17.35 68.5%\nBossNet-T0 [26] \u0013 2242 - - 3.4G 80.8% ‚Äì 23.76 ‚Äì\nPVT-S [14] \u0017 2242 24.5M 3.8G 79.8% 3.24 21 ‚Äì\nRes2Net-50 [4] \u0017 2242 25.0M 4.2G 78.0% 3.12 18.57 ‚Äì\nResNet-50 [3] \u0017 2242 25.6M 4.1G 76.2% 2.98 18.59 ‚Äì\nRedNet-101 [29] \u0017 2242 25.6M 4.7G 79.1% 3.09 16.83 ‚Äì\nVOLO [33] \u0017 2242 27.0M 6.8G 84.2% 3.12 12.38 ‚Äì\nCrossFormer-T [16] \u0017 2242 27.8M 2.9G 81.5% 2.93 28.1 ‚Äì\nSwin [15] \u0017 2242 29.0M 4.5G 81.3% 2.80 18.07 ‚Äì\nWRN [8] \u0017 2242 68.9M ‚Äì 78.1% 1.13 ‚Äì ‚Äì\nViT-B/16 [31] \u0017 3842 86M 55.5G 77.9% 0.91 1.4 67.5%\nthe CIFAR-10 and CIFAR-100 datasets. As shown in the\ntable, our PSLT achieves comparable performance com-\npared to the commonly-used convolutional neural networks\nwith a small amount of parameters. Compared with WRN\n[8], PSLT achieves comparable performance on the CI-\nFAR10/100 with similar amount of parameters but higher\ntop-1 accuracy on ImageNet with much less parameters in\nTable 4. Compared to the vision transformers, our PSLT\nshows higher model generalization ability with fewer train-\ning images, where PSLT achieves relatively higher top-1\naccuracy on both CIFAR-10 (95.43% vs. 95.0%) and CIFAR-\n100 (79.1% vs. 78.63%) than MOA-T with less trainable\nparameters (8.7M vs. 30M).\n4.2 Objection Detection\n- Experimental setting. We conduct objection detection ex-\nperiments on COCO 2017 [51], which contains 118k images\nin the training set and 5k images in the validation set\nof 80 classes. Following PVT [14], which uses standard\nsettings to generate multi-scale feature maps, we evaluate\nSUBMISSION TO IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 11\nTABLE 5\nImage classiÔ¨Åcation performance of the models on the CIFAR-10/100\nwithout pretraining. The top-1 accuracy rate on CIFAR-10/100 is\nreported. ‚Äú+‚Äù denotes that results were taken from [71]. ‚ÄúFLOPs‚Äù is not\nreported for the compared methods in literatures.\nModel Input #Params CIFAR-10 CIFAR-100\nResNet [3] 322 19.4M 92.07% ‚Äì\nWRN-40-4 [8] 322 8.9M 95.47% 78.82%\nResNeXt-29 [5] 322 68.1M 96.43% 82.69%\nDenseNet [7] 322 27.2M 94.17% 76.58%\nPVT-T [14]+ 322 13M 91.0% 72.80%\nSwin [15]+ 322 27.5M 94.41% 78.07%\nMOA-T [71] 322 30M 95.0% 78.63%\nPSLT (Ours) 322 8.7M 95.43% 79.1%\nour proposed PSLT as a backbone with two standard de-\ntectors: RetinaNet [73] (one-stage) and Mask R-CNN [74]\n(two-stage). During training, our backbone (PSLT) was Ô¨Årst\ninitialized with the pretrained weights on ImageNet [2], and\nthe newly added layers were initialized with Xavier [75].\nOur PSLT was trained with a batch size of 16 on 8 GeForce\n1080Ti GPUs. We adopt the AdamW [59] optimizer with an\ninitial learning rate of 1√ó10‚àí4. Following the common PVT\nsettings, we adopted a 1√óor 3√ótraining schedule (12 or 36\nepochs) to train the detection models. The shorter side of the\ntraining images was resized to 800 pixels, while the longer\nside of the images is set to a maximum of 1,333 pixels. The\nshorter side of the validation images was Ô¨Åxed to 800 pixels.\nWith the 3√ótraining schedule, the shorter side of the input\nimage was randomly resized in the range of [640, 800].\n- Experimental results. Table 6 shows the performance of\nthe backbones on COCO val2017; the backbones were initial-\nized with pretrained weights on ImageNet using RetinaNet\nfor object detection. As shown in the table, with a similar\nnumber of trainable parameters, PSLT achieves comparable\nperformance. With the 1√ótraining scheduler, PSLT outper-\nforms Mobile-Former, where PSLT achieving a higher AP\n(41.2 vs. 38.0) with slightly more parameters (19.1 M vs. 17.9\nM) and FLOPs (192 G vs. 181G). The last block of Table\n6 presents models with more than 30M parameters; PSLT\nachieves higher AP (41.2 vs. 40.4) than PVT-S with consid-\nerably fewer parameters (19.1 M vs. 34.2M) and FLOPs (192\nG vs. 226 G). Compared with the commonly used backbone\nResNet-50, PSLT achieves a considerably higher AP (41.2 vs.\n35.3) with nearly 50% less trainable parameters (19.1 M vs.\n37.7 M) and fewer FLOPs (192 G vs. 239 G). With the 3√ó\ntraining scheduler, PSLT shows higher performance with a\nsimilar number of parameters, where PSLT achieving a 1.7-\npoint higher AP than PVT-S (43.9 vs. 42.2) with considerably\nfewer parameters.\nSimilar results are shown in Table 7, including segmen-\ntation experiments on Mask R-CNN. With the 1√ótrain-\ning scheduler, PSLT outperforms ResNet-50, where PSLT\nachieving 2.8 points higher box AP (40.8 vs. 38.0) and 2.9\npoints higher mask AP (37.3 vs. 34.4) with fewer parameters\n(28.9M vs. 44.2M) and FLOPs (211 G vs. 253 G). PSLT shows\ncomparable performance to PVT-S (40.8 vs. 40.4 box AP and\n37.3 vs. 37.8 mask AP) with considerably fewer parameters\n(28.9M vs. 44.1M) and FLOPs (211G vs. 305G). Similar\nresults are observed with the 3√ótraining scheduler; PSLT\noutperforms PVT-T, where PSLT achieving 2.9 points higher\nAPb and 1.5 points higher APm with a smaller number\nof parameters (28.9M vs. 32.9M) and FLOPs (211 G vs. 240\nG). Moreover, PSLT achieves comparable performance to\nResNet-50, which has more than 40M parameters.\nFor a fair comparison with Swin [15], we implement our\nPSLT-Large with the same experimental setting as Swin [15]\nwith Sparse R-CNN [72] framework (in Table 6) and with\nMask R-CNN [74] framework (in Table 7). Our PSLT-Large\nachieves comparable performance with less parameters and\nFLOPs.\n4.3 Semantic Segmentation\n- Experimental setting. Semantic segmentation experiments\nare conducted with a challenging scene parsing dataset,\nADE20K [76]. ADE20K contains 150 Ô¨Åne-grained semantic\ncategories with 20210, 2000 and 3352 images in the training,\nvalidation and test sets, respectively. Following PVT [14],\nwe evaluate our proposed PSLT as a backbone on the basis\nof the Semantic FPN [77], a simple segmentation method\nwithout dilated convolution [78]. In the training phase, the\nbackbone was Ô¨Årst initialized with the pretrained weights\non ImageNet [2], and the newly added layers were initial-\nized with Xavier [75]. Our PSLT was trained for 80k itera-\ntions with a batch size of 16 on 4 GeForce 1080Ti GPUs. We\nadopted the AdamW [59] optimizer with an initial learning\nrate of 2 √ó10‚àí4. The learning rate follows the polynomial\ndecay rate with a power of 0.9. The training images were\nrandomly resized and cropped to 512 √ó512, and the images\nin the validation set were rescaled, with the shorter side set\nto 512 pixels during testing.\n- Experimental results. Table 8 shows the semantic segmen-\ntation performance on the ADE20K validation set for back-\nbones initialized with pretrained weights on ImageNet with\nSematic FPN [77]. As shown in the table, PSLT outperforms\nthe ResNet-based models [3], where PSLT achieving a 2.3%\npoints higher than ResNet-50 (39.0% vs. 36.7%) with nearly\nhalf the number of parameters (13.0M vs. 28.5M) and 20%\nless FLOPs than ResNet-50 (32.0G vs. 45.6G). With almost\nthe same number parameters and FLOPs, PSLT achieves a\n2.3% points higher than PVT-T (39.0% vs 36.7%).\nFor a fair comparison with Swin [15], we implement\nour PSLT-Large with the same experimental setting as Swin\n[15] with UperNet [79] framework. Our PSLT-Large achieves\ncomparable performance with less parameters and FLOPs.\n4.4 Person Re-identiÔ¨Åcation\n- Experimental setting. Person re-identiÔ¨Åcation (ReID) ex-\nperiments are conducted with Market-1501 [52], a dataset\nthat contains 12,936 images of 751 people in the training set\nand 19,732 images of 750 different people in the validation\nset. All the images in the Market-1501 dataset were captured\nwith six cameras. We trained our PSLT on the training set\nto classify each person, similar to an image classiÔ¨Åer with\n751 training classes. The output feature of the Ô¨Ånal stage\nin PSLT was utilized to compute the similarity between\nthe query image and the gallery images for validation, and\nthe rank-1 accuracy and mean average precision (mAP) are\nSUBMISSION TO IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 12\nTABLE 6\nObject detection performance on COCO. The AP value on the validation set is reported. ‚ÄúMS‚Äù denotes that multi-scale training [14] was used. ‚Äú+‚Äù\nindicates that the result is taken from [14]. FLOPs is calculated at input resolution 1280 √ó800. ‚Äú*‚Äù denotes that the model is trained based on\nSparse R-CNN [72] following [15].\nBackbone #Params #FLOPs RetinaNet 1x RetinaNet 3x + MS\nAP AP50 AP75 APS APM APL AP AP50 AP75 APS APM APL\nPSLT(Ours) 19.1M 192G 41.2 61.3 44.0 25.5 45.0 54.8 43.9 64.3 46.9 27.6 47.6 56.9\nMobile-Former [38] 17.9M 181G 38.0 58.3 40.3 22.9 41.2 49.7 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì\nEfÔ¨ÅcientNet [27] 20.0M 151.2G 40.5 60.5 43.1 22.2 44.8 56.6 42.4 62.0 45.6 24.1 46.1 58.3\nResNet-18 [3]+ 21.3M 168G 31.8 49.6 33.6 16.3 34.2 43.2 35.4 53.9 37.6 19.5 38.2 46.8\nPVT-T [14] 23.0M 221G 36.7 56.9 38.9 22.6 38.8 50.0 39.4 59.8 42.0 25.5 42.0 52.1\nRedNet-50 [29] 27.8M 210G 38.3 58.2 40.5 21.1 41.8 50.9 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì\nConT-M [62] 27.0M 217G 37.9 58.1 40.2 23.0 40.6 50.4 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì\nPVT-S [14] 34.2M 226G 40.4 61.3 43.0 25.0 42.9 55.7 42.2 62.7 45.0 26.2 45.2 57.2\nResNet-50 [3]+ 37.7M 239G 36.3 55.3 38.6 19.3 40.0 48.8 39.0 58.4 41.8 22.4 42.8 51.6\nPSLT-Large (Ours)‚àó 97.6M 147.7G ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì 48.2 67.2 52.8 32.3 51.1 62.4\nSwin [15]‚àó 110.0M 172.0G ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì 47.9 67.3 52.3 ‚Äì ‚Äì ‚Äì\nTABLE 7\nObject detection and instance segmentation performance on COCO. APb and APm denote the bounding box AP and mask AP on the validation\nset, respectively. ‚ÄúMS‚Äù denotes that multi-scale training [14] was used. ‚Äú+‚Äù indicates that the result was taken from [14]. FLOPs is calculated at\ninput resolution 1280 √ó800.\nBackbone #Params #FLOPs Mask R-CNN 1x Mask R-CNN 3x + MS\nAPb APb\n50 APb\n75 APm APm\n50 APm\n75 APb APb\n50 APb\n75 APm APm\n50 APm\n75\nPSLT (Ours) 28.9M 211G 40.8 61.8 44.9 37.3 59.0 39.9 42.7 63.1 47.1 38.9 60.5 41.9\nEfÔ¨ÅcientNet [27] 30.2M 169.3G 40.1 61.2 43.5 36.4 58.0 38.9 42.1 63.3 45.5 38.0 60.2 40.6\nRedNet-50 [29] 34.2M 224G 40.2 61.4 43.7 36.1 58.1 38.2 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì\nResNet-18 [3]+ 31.2M ‚Äì ‚Äì 34.0 54.0 36.7 31.2 51.0 32.7 36.9 57.1 40.0 33.6 53.9 35.7\nPVT-T [14] 32.9M 240G 36.7 59.2 39.3 35.1 56.7 37.3 39.8 62.2 43.0 37.4 59.8 39.9\nPVT-S [14] 44.1M 305G 40.4 62.9 43.8 37.8 60.1 40.3 43.0 65.3 46.9 39.9 62.5 42.8\nResNet-50 [3]+ 44.2M 253G 38.0 58.6 41.4 34.4 55.1 36.7 41.0 61.7 44.9 37.1 58.4 40.1\nPSLT-Large (Ours) 35.6M 242.3G 44.1 65.9 48.3 39.6 62.6 42.4 46.7 68.0 51.5 41.8 65.0 45.2\nSwin [15] 48.0M 267.0G 43.7 66.6 47.7 39.8 63.3 42.7 46.0 68.1 50.3 41.6 65.1 44.9\nTABLE 8\nSemantic segmentation performance of different backbones on the\nADE20K. The mean intersection over union (MIoU) is reported. ‚Äú+‚Äù\nindicates that the result is taken from [14]. ‚Äú*‚Äù denotes that the model is\ntrained based on UperNet [79] following [15].\nBackbone Semantic FPN\n#Params Input FLOPs mIoU\nPSLT (Ours) 13.0M 5122 32.0G 39.0%\nResNet-18 [3]+ 15.5M 5122 32.2G 32.9%\nPVT-T [14] 17.0M 5122 33.2G 35.7%\nResNet-50 [3]+ 28.5M 5122 45.6G 36.7%\nPVT-S [14] 28.2M 5122 44.5G 39.8%\nPSLT-Large(Ours)‚àó 47.8M 5122 229.9G 46.1%\nSwin [15]‚àó 60M 5122 945G 46.1%\nreported. For a fair comparison, we follow Bag of Tricks [80]\nto perform the ReID experiments. The data augmentation\ntechniques that we apply to the training images include\nhorizontal Ô¨Çipping, central padding, randomly cropping to\n256√ó128, normalizing by subtracting the channel mean and\ndividing by the channel standard deviation, and random\nerasing [57]. The data augmentation techniques applied to\nthe test images include resizing to 256 √ó128 and normal-\nizing by subtracting the channel mean and dividing by the\nTABLE 9\nPerformance of the models on the Market-1501 when the models are\nÔ¨Årst pretrained on ImageNet. ‚ÄúRank-1‚Äù indicates the rank-1 accuracy\nrate. ‚ÄúmAP‚Äù denotes the mean Average Precision. ‚ÄúFLOPs‚Äù is not\nreported for the compared methods in literatures.\nModel Input #Params Rank-1 mAP\nOSNet [81] 256 √ó128 2.2M 94.8% 84.9\nCDNet [82] 256 √ó128 1.8M 95.1% 86.0\nAuto-ReID [83] 256 √ó128 11.4M 94.5% 85.1\nTransReID [84] 256 √ó128 ‚àº50M 94.7% 86.8\nPCB (+RPP) [85] 256 √ó128 ‚àº26M 93.8% 81.6\nVPM [86] 256 √ó128 ‚àº26M 93.0% 80.8\nBagofTrick [80] 256 √ó128 ‚àº26M 94.5% 85.9\nPSLT (Ours) 256 √ó128 9.0M 94.3% 86.2\nchannel standard deviation.\nThe proposed PSLT was trained for 360 epochs under the\nguidance of the cross-entropy loss and triplet loss [87] using\nthe Adam optimizer. Similarly, PSLT adopts the BNNeck\n[80] structure to compute the triplet loss. The output feature\nafter the global average pooling layer Ô¨Årst passed through\na BatchNorm layer before being sent to the classiÔ¨Åer. The\nfeature before the BatchNorm layer was used to compute\nthe triplet loss, and the output score of the classiÔ¨Åer was\nSUBMISSION TO IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 13\nTABLE 10\nComparison of model inference. ‚ÄúMem‚Äù denotes the peak memory for\nevaluation. ‚ÄúFPS‚Äù is the number of images processed for one second.\nModel #Params #FLOPs Mem FPS Top-1\nPSLT-Tiny 4.3M 876M 1.9G 669.2 74.9%\nPSLT 9.2M 1.9G 2.1G 638.2 79.9%\nPSLT-Large 16.0M 3.4G 2.3G 557.7 81.5%\nDeiT-S [69] 22.0M 4.6G 1.8G 1456.9 79.8%\nSwin [15] 29.0M 4.5G 2.5G 755.2 81.3%\nSwin√ó3 [15] 247.2M 38.1G 4.8G 272.4 ‚Äì\nused to compute the cross-entropy loss for training. For val-\nidation, the feature after the BatchNorm layer was adopted\nto compute the similarity. A batch size of 64 was applied\nduring training. The initial learning rate is set to 1.5 √ó10‚àí4,\nand the learning rate was decayed by a factor of 10 at epochs\n150, 225 and 300. The weight decay was set to 5 √ó10‚àí4.\n- Experimental results. Table 9 shows the performance on\nMarket-1501 of models that were pretrained on ImageNet.\nThe models in the Ô¨Årst block are designed speciÔ¨Åcally for\nperson ReID. The second block presents the performance\nof general-purpose backbones with improvement method-\nologies on Market-1501. PSLT outperforms some methods\nwith general-purpose backbones. PSLT achieves a higher\nrank-1 accuracy (94.3% vs. 93.8%) and mAP (86.2 vs. 81.6)\nthan PCB (+RPP) with fewer parameters (9.0M vs. 26M). In\naddition, PSLT achieves comparable performance to Auto-\nReID, which is automatically designed for person ReID.\nCompared to Auto-ReID, PSLT achieves a higher mAP\n(86.2 vs. 85.1) and the same rank-1 accuracy with fewer\nparameters (9.0M vs. 11.4M). PSLT also shows comparable\nperformance to OSNet, with a slightly higher mAP (86.2 vs.\n84.9) and slightly lower rank-1 accuracy (94.3% vs. 94.8%).\nHowever, PSLT performs worse than speciÔ¨Åcally designed\nstate-of-the-art models (CDNet and TransReID).\nAs can be seen, our PSLT achieves comparable per-\nformance to general-purpose models in the second block\nand slightly inferior performance to the models speciÔ¨Åcally\ndesigned for ReID in the Ô¨Årst block. The experimental results\ndemonstrate that our PSLT is generalizable to the person re-\nidentiÔ¨Åcation tasks.\n4.5 Discussion\nWe have shown our model performance in Table 4 and our\nPSLT achieves comparable top-1 accuracy with relative less\nparameters and FLOPs. Usually, the FLOPs is adopted to\nmeasure the total Ô¨Çoat operations for processing one image.\nIntuitively, the model with lower FLOPs can achieve higher\nFPS which measures the amount of processed images in\none second. However, according to Table 10, the FPS for\nthe listed three kinds of models seems counter-intuitive. For\nexample, the Swin contains relatively less FLOPs than DeiT-\nS but processes nearly a half amount of images as the DeiT-\nS does in one second. The same phenomenon occurs when\ncomparing our PSLT-Large and Swin. The main reason is\nthat the computing optimization is not implemented for\nSwin and our PSLT-Large directly on the accelerators such\nas GPUs, including the window partition in Swin and PSLT-\nLarge and the computation of multiple branches in our\nTABLE 11\nAblation study on the ImageNet without pretraining. ‚Äú#Branches‚Äù\nindicates the number of branches in the ladder self-attention block.\nModel #Branches #Params #FLOPs Top-1\nPSLT 2 11.0M 2.1G 80.2%\nPSLT 3 9.2M 1.9G 79.9%\nPSLT 4 8.4M 1.8G 79.0%\nPSLT 6 7.5M 1.7G 77.9%\nTABLE 12\nAblation study on the ImageNet without pretraining. ‚ÄúShift‚Äù denotes that\nthe shift operation was used in the ladder self-attention block to model\ndiverse local self-attentions. ‚ÄúFD‚Äù indicates whether progressively\ntransmitting features is adopted. ‚ÄúPAFM‚Äù indicates the pixel-adaptive\nfusion module.\nModel Shift FD PAFM #Params #FLOPs Top-1\nPSLT \u0013 \u0013 \u0013 9.2M 1.9G 79.9%\n‚Äì ‚Äì \u0017 \u0013 \u0013 9.2M 1.9G 79.3%\n‚Äì ‚Äì \u0013 \u0017 \u0013 9.7M 2.0G 79.2%\nPSLT-Large. Furthermore, we examine the FPS of Swin √ó3\nwith 3 times of the original channel dimension, and Ô¨Ånd\nthat the FLOPs is nearly 9 times of the Swin but the FPS\nis only 1/3 of the Swin. Thus, more direct optimization on\nthe computing hardware (GPU) for each transformer model\ncould essential for the FPS. However, the optimization of the\ncomputing hardware for fast inference can be investigated\nin the future but is not the main scope of our work.\n5 A BLATION STUDY\nIn this section, we conduct ImageNet classiÔ¨Åcation exper-\niments to demonstrate that the ladder self-attention block\nand progressive shift mechanism in the proposed PSLT\nare effective. Here, all models were trained with an input\nimage resolution of 2242 following the experimental settings\ndescribed in Section 4.1.1.\n- Number of branches in the ladder self-attention block.\nAs shown in Section 3.2, the ladder self-attention block\ncontains multiple branches, and more branches denotes that\nthe developed backbone has less trainable parameters. In the\nladder self-attention block with 6 branches, the shift stride\nin each branch is set to 3 (with two directions), 1 (with two\ndirections), 5. The blocks with less branches are constructed\nsimilarly to the above. Table 11 shows the performance of\nPSLT with various number of branches in the ladder self-\nattention blocks. Although the model with more branches\nrequires less computing resources, the performance of the\nmodel gets worse. For balance, we set the number of\nbranches as 3 by default.\n- Shift operation in the ladder self-attention block. As\ndescribed in Section 3.2.1, PSLT adopts the shift operation\nin each branch of the ladder self-attention block to model\nlong-range interactions. The third line in Table 12 shows the\nperformance of PSLT without the shift operation,i.e., all shift\noperations in Figures 1 and 3 are removed. The experimental\nresults show that with the shift operation, PSLT achieves a\nhigher top-1 accuracy (79.9% vs. 79.3%) without increasing\nthe number of training parameters or FLOPs.\nSUBMISSION TO IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 14\nTABLE 13\nAblation study of FFN layer on ImageNet without pre-training. ‚ÄúLFFN‚Äù\ndenotes our Light FNN layer.\nModel FFN #Params #FLOPs Top-1\nPSLT LFFN 9.2M 1.9G 79.9%\n‚Äì ‚Äì FFN 12.8M 2.3G 79.7%\nTABLE 14\nAblation study of the fusion module for the multiple branches on the\nImageNet without pretraining. ‚ÄúModule‚Äù indicates the method for feature\nfusion. ‚ÄúWeight‚Äù denotes the existence of the adaptive weights for\nfeature fusion. ‚ÄúPointwise‚Äù indicates the existence of a fully connected\nlayer for fusing the features along the channel dimension. ‚ÄúAW‚Äù\ndenotes that only the Ô¨Årst two fully connected layers are preserved in\nthe pixel-adaptive fusion module. ‚ÄúConcat‚Äù indicates that the output\nfeatures of the multiple branches are concatenated. ‚ÄúSE‚Äù denotes that\nthe SE [50] block is adopted.\nModule Weight Pointwise #Params #FLOPs Top-1\nPAFM \u0013 \u0013 9.2M 1.9G 79.9%\nFC \u0017 \u0013 7.4M 1.7G 78.8%\nAW \u0013 \u0017 7.4M 1.7G 78.6%\nConcat \u0017 \u0017 5.6M 1.5G 77.3%\nSE [50] \u0013 \u0013 9.2M 1.7G 79.0%\n- Feature delivery in the ladder self-attention block. For\nlong-range interactions, PSLT progressively delivers fea-\ntures of previous branch, where the latter branch takes the\noutput features of the previous branch as its value when\ncomputing self-attention. Here, we investigate whether de-\nlivering the output features is effective. A multi-branch\nblock is adopted, and the shift operation is still used for each\nbranch, i.e., only the horizontal connections in Figure 1 are\nremoved, and the PSW-MHSA in each branch is replaced\nwith the W-MHSA in Figure 3. The last line in Table 12\npresents the performance of the model without horizontal\ndelivery. Since the value of self-attention in each branch\nneeds to be computed from the input feature, the number\nparameters in the model without horizontal delivery is\nlarger than the PSLT. With the horizontal connections in\nthe ladder self-attention block, PSLT achieves a higher top-1\naccuracy (79.9% vs. 79.2%) with slightly fewer parameters\n(9.2M vs. 9.7M).\nBy modelling diverse local interactions and interacting\namong the branches, the progressive shift mechanism is\ncapable of enlarging the receptive Ô¨Åeld of the ladder self-\nattention block. The last two lines in Table 12 show perfor-\nmance of models without the two strategies respectively.\nObviously, PSLT with the progressive shift mechanism\nachieves higher performance. Besides, with similar amount\nof computing resources, PSLT also outperforms the Swin-2G\nas shown in Table 4, where Swin-2G is implemented in the\nsame way as SwinTransformer [15]. The above experimental\nresults show that the progressive shift mechanism improves\nmodel capacity with large receptive Ô¨Åeld.\n- Light FFN layer. For further cutting down the computing\nbudgets, PSLT proposes the Light FNN (LFFN) layer to\nreplace the original FFN layer in self-attention block. Table\n13 shows the comparison between PSLT and the model\nwith FFN (only Light FNN layer in PSLT is replaced with\nthe original FFN layer), and PSLT achieves comparable\nTABLE 15\nAblation study of the proposed modules for PSLT on the ImageNet\nwithout pretraining. ‚ÄúV1‚Äù and ‚ÄúV2‚Äù denote the top-1 accuracy on\nImageNet and ImageNet-V2 validation sets respectively.\nModule LSA LFFN PAFM #Params #FLOPs V1 V2\nViT-B [31] \u0017 \u0017 \u0017 86M 55.5G 77.9% 67.5%\nLSA \u0013 \u0017 \u0017 9.2M 2.0G 77.2% 64.8%\n+LFFN \u0013 \u0013 \u0017 5.6M 1.5G 77.3% 64.6%\n+PAFM \u0013 \u0013 \u0013 9.2M 1.9G 79.9% 68.6%\nperformance, where PSLT with LFFN achieves comparable\ntop-1 accuracay (79.9% vs. 79.7%) with relatively smaller\namount of parameters (9.2M vs. 12.8M) and FLOPs (1.9G\nvs. 2.3G).\n- Pixel-Adaptive Fusion Module. As described in Section\n3.2.3, to effectively integrate the features of each branch in\nthe ladder self-attention block, PSLT adopts a pixel-adaptive\nfusion module. In this module, the weight of each pixel\nin the feature map is Ô¨Årst multiplied by the feature; then,\nthe weighted features are integrated in a fully connected\nlayer to fuse the features along the channel dimension.\n‚ÄúAW‚Äù and ‚ÄúFC‚Äù denote the module without one of the\ntwo steps, respectively. The experimental results in Table\n14 demonstrate the effectiveness of our proposed pixel-\nadaptive fusion module.\nFurthermore, we implemented another module (‚ÄúSE‚Äù\nin Table 14) for comparison, which only computes the\nweights along the channel dimension (a pointwise following\na squeeze-and-excitation layer). The model with only chan-\nnel adaptive weights achieves top-1 accuracy of 79.0% with\n9.2M parameters and 1.7G FLOPs. The experimental results\ndemonstrate that adaptive weights in both the spatial and\nchannel dimensions signiÔ¨Åcantly improves performance.\n- Stacking the proposed modules. As described in Section\n3.1, our proposed PSLT is composed of the proposed lad-\nder self-attention block, Light FFN and the pixel-adaptive\nfusion module. Table 15 shows the performance of mod-\nels by stacking the proposed modules one by one, which\ndemonstrates the effectiveness of the modules. Compared\nto the ViT, although applying our ladder self-attetion (LSA)\nand Light FFN (LFFN) does not improve the performance,\nthe number of parameters and FLOPs are largely reduced,\nwhere the parameters have been reduced by 9 times and\n15 times respectively, and the FLOPs have been reduced\nby 27 times and 37 times respectively. The pixel-adaptive\nfusion module (PAFM) improves the model performance by\nintegrating information of all branches in the block. The top-\n1 accuracy of our PSLT and DeiT-S on both the ImageNet and\nImageNet-V2 validation set conforms to the linear Ô¨Åt in [53].\nOur PSLT (including DeiT-S) achieves smaller improvement over\nViT-B on ImageNet-V2 validation (2% and 1% top-1 accuracy\nimprovement on ImageNet and ImageNet-V2 validation set, re-\nspectively). Such a phenomenon has been similarly observed in\n[37], [69]. As described in [88], this shows the model generaliza-\ntion ability is challenged on different validation set.\n6 C ONCLUSION\nIn this work, we propose a ladder self-attention block with\nmultiple branches requiring a relatively small number of\nSUBMISSION TO IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 15\nparameters and FLOPs. To improve the computation efÔ¨Å-\nciency and enlarge the receptive Ô¨Åeld of the ladder self-\nattention block, the input feature map is split into sev-\neral parts along the channel dimension, and a progressive\nshift mechanism is proposed for long-range dependency\nby modelling diverse local self-attention on each branch\nand interacting among these branches. A pixel-adaptive\nfusion module is Ô¨Ånally designed to integrate features from\nmultiple branches with adaptive weights along both the\nspatial and channel dimensions. Furthermore, the ladder\nself-attention block adopts a Light FFN layer to decrease\nthe number of trainable parameters and Ô¨Çoat-point opera-\ntions. Based on the above designs, we develop a general-\npurpose backbone (PSLT) with a relatively small number of\nparameters and FLOPs. Overall, PSLT applies convolutional\nblocks in the early stages and ladder self-attention blocks\nin the latter stages. PSLT achieves comparable performance\nwith existing methods on several computer vision tasks,\nincluding image classiÔ¨Åcation, object detection and person\nre-identiÔ¨Åcation.\nOur work explores a new perspective to model long-\nrange interaction by effectively utilizing diverse local self-\nattentions, while there are recent works [17], [18], [38], [48]\nimplement effective methods to combine the convolution\nand global self-attention. We Ô¨Ånd that modelling local self-\nattention is more environmental-friendly than modelling\nglobal self-attention.\nFor future work, we will investigate the direct comput-\ning optimization of our PSLT for fast inference. And we will\ninvestigate a new computation manner for self-attention\nbecause the similarity computation is time consuming. For\nexample, we will investigate whether simple addition is\npowerful enough to model interactions in the self-attention\ncomputation. PSLT shows comparable performance with-\nout elaborately selecting the hyperparameters, such as the\nnumber of blocks, the channel dimension or the number\nof heads in the ladder self-attention blocks. Furthermore,\nwe can automatically select parameters that signiÔ¨Åcantly\nimprove model performance with neural architecture search\ntechniques.\nACKNOWLEDGMENT\nThis work was supported partially by the NSFC\n(U21A20471, U1911401, U1811461), Guangdong NSF Project\n(No. 2023B1515040025, 2020B1515120085). The correspond-\ning author and principal investigator for this paper is Wei-\nShi Zheng.\nREFERENCES\n[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImagenet classiÔ¨Åca-\ntion with deep convolutional neural networks,‚Äù Advances in neural\ninformation processing systems, vol. 25, 2012.\n[2] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, and M. Bernstein, ‚ÄúImagenet\nlarge scale visual recognition challenge,‚Äù International Journal of\nComputer Vision, vol. 115, no. 3, pp. 211‚Äì252, 2015.\n[3] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for\nimage recognition,‚Äù in 2016 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2016, pp. 770‚Äì778.\n[4] S.-H. Gao, M.-M. Cheng, K. Zhao, X.-Y. Zhang, M.-H. Yang, and\nP . Torr, ‚ÄúRes2net: A new multi-scale backbone architecture,‚ÄùIEEE\ntransactions on pattern analysis and machine intelligence, vol. 43, no. 2,\npp. 652‚Äì662, 2019.\n[5] S. Xie, R. Girshick, P . Doll ¬¥ar, Z. Tu, and K. He, ‚ÄúAggregated\nresidual transformations for deep neural networks,‚Äù in Proceedings\nof the IEEE conference on computer vision and pattern recognition, 2017,\npp. 1492‚Äì1500.\n[6] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional\nnetworks for large-scale image recognition,‚Äù arXiv preprint\narXiv:1409.1556, 2014.\n[7] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger,\n‚ÄúDensely connected convolutional networks,‚Äù in Proceedings of the\nIEEE conference on computer vision and pattern recognition , 2017, pp.\n4700‚Äì4708.\n[8] S. Zagoruyko and N. Komodakis, ‚ÄúWide residual networks,‚Äù arXiv\npreprint arXiv:1605.07146, 2016.\n[9] Z. Dai, H. Liu, Q. Le, and M. Tan, ‚ÄúCoatnet: Marrying convolution\nand attention for all data sizes,‚Äù Advances in Neural Information\nProcessing Systems, vol. 34, 2021.\n[10] T. Xiao, M. Singh, E. Mintun, T. Darrell, P . Doll¬¥ar, and R. Girshick,\n‚ÄúEarly convolutions help transformers see better,‚Äù Advances in\nNeural Information Processing Systems , vol. 34, pp. 30 392‚Äì30 400,\n2021.\n[11] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, ≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù\nAdvances in neural information processing systems , vol. 30, 2017.\n[12] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,‚Äù arXiv preprint arXiv:1810.04805, 2018.\n[13] J. Guo, K. Han, H. Wu, C. Xu, Y. Tang, C. Xu, and Y. Wang, ‚ÄúCmt:\nConvolutional neural networks meet vision transformers,‚Äù arXiv\npreprint arXiv:2107.06263, 2021.\n[14] W. Wang, E. Xie, X. Li, D.-P . Fan, K. Song, D. Liang, T. Lu, P . Luo,\nand L. Shao, ‚ÄúPyramid vision transformer: A versatile backbone\nfor dense prediction without convolutions,‚Äù in Proceedings of the\nIEEE/CVF International Conference on Computer Vision , 2021, pp.\n568‚Äì578.\n[15] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,\n‚ÄúSwin transformer: Hierarchical vision transformer using shifted\nwindows,‚Äù in Proceedings of the IEEE/CVF International Conference\non Computer Vision, 2021, pp. 10 012‚Äì10 022.\n[16] W. Wang, L. Yao, L. Chen, D. Cai, X. He, and W. Liu, ‚ÄúCrossformer:\nA versatile vision transformer based on cross-scale attention,‚Äù\narXiv e-prints, pp. arXiv‚Äì2108, 2021.\n[17] J. Pan, A. Bulat, F. Tan, X. Zhu, L. Dudziak, H. Li, G. Tz-\nimiropoulos, and B. Martinez, ‚ÄúEdgevits: Competing light-weight\ncnns on mobile devices with vision transformers,‚Äù arXiv preprint\narXiv:2205.03436, 2022.\n[18] M. Maaz, A. Shaker, H. Cholakkal, S. Khan, S. W. Zamir, R. M.\nAnwer, and F. S. Khan, ‚ÄúEdgenext: EfÔ¨Åciently amalgamated cnn-\ntransformer architecture for mobile vision applications,‚Äù arXiv\npreprint arXiv:2206.10589, 2022.\n[19] S. Mehta and M. Rastegari, ‚ÄúMobilevit: light-weight, general-\npurpose, and mobile-friendly vision transformer,‚Äù arXiv preprint\narXiv:2110.02178, 2021.\n[20] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, and H. Adam, ‚ÄúMobilenets: EfÔ¨Åcient\nconvolutional neural networks for mobile vision applications,‚Äù\narXiv preprint arXiv:1704.04861, 2017.\n[21] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,\n‚ÄúMobilenetv2: Inverted residuals and linear bottlenecks,‚Äù in 2018\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2018, pp. 4510‚Äì4520.\n[22] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan,\nW. Wang, Y. Zhu, R. Pang, V . Vasudevan et al. , ‚ÄúSearching for\nmobilenetv3,‚Äù in Proceedings of the IEEE International Conference on\nComputer Vision, 2019, pp. 1314‚Äì1324.\n[23] X. Zhang, X. Zhou, M. Lin, and J. Sun, ‚ÄúShufÔ¨Çenet: An extremely\nefÔ¨Åcient convolutional neural network for mobile devices,‚Äù in2018\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2018, pp. 6848‚Äì6856.\n[24] B. Zoph and Q. Le, ‚ÄúNeural architecture search with reinforce-\nment learning,‚Äù in ICLR 2017 : International Conference on Learning\nRepresentations 2017, 2017.\n[25] Z. Guo, X. Zhang, H. Mu, W. Heng, Z. Liu, Y. Wei, and J. Sun,\n‚ÄúSingle path one-shot neural architecture search with uniform\nsampling,‚Äù in European Conference on Computer Vision . Springer,\n2020, pp. 544‚Äì560.\n[26] C. Li, T. Tang, G. Wang, J. Peng, B. Wang, X. Liang, and X. Chang,\n‚ÄúBossnas: Exploring hybrid cnn-transformers with block-wisely\nSUBMISSION TO IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 16\nself-supervised neural architecture search,‚Äù in Proceedings of the\nIEEE/CVF International Conference on Computer Vision , 2021, pp.\n12 281‚Äì12 291.\n[27] M. Tan and Q. V . Le, ‚ÄúEfÔ¨Åcientnet: Rethinking model scaling\nfor convolutional neural networks,‚Äù in International Conference on\nMachine Learning, 2019, pp. 6105‚Äì6114.\n[28] H. Liu, K. Simonyan, and Y. Yang, ‚ÄúDarts: Differentiable architec-\nture search,‚Äù in ICLR 2019 : 7th International Conference on Learning\nRepresentations, 2019.\n[29] D. Li, J. Hu, C. Wang, X. Li, Q. She, L. Zhu, T. Zhang, and\nQ. Chen, ‚ÄúInvolution: Inverting the inherence of convolution for\nvisual recognition,‚Äù in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2021, pp. 12 321‚Äì12 330.\n[30] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei,\n‚ÄúDeformable convolutional networks,‚Äù in Proceedings of the IEEE\ninternational conference on computer vision, 2017, pp. 764‚Äì773.\n[31] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly\net al. , ‚ÄúAn image is worth 16x16 words: Transformers for image\nrecognition at scale,‚Äù arXiv preprint arXiv:2010.11929, 2020.\n[32] X. Yue, S. Sun, Z. Kuang, M. Wei, P . H. Torr, W. Zhang, and D. Lin,\n‚ÄúVision transformer with progressive sampling,‚Äù in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision, 2021, pp.\n387‚Äì396.\n[33] L. Yuan, Q. Hou, Z. Jiang, J. Feng, and S. Yan, ‚ÄúVolo: Vision\noutlooker for visual recognition,‚Äù arXiv preprint arXiv:2106.13112 ,\n2021.\n[34] Y. Li, G. Yuan, Y. Wen, E. Hu, G. Evangelidis, S. Tulyakov, Y. Wang,\nand J. Ren, ‚ÄúEfÔ¨Åcientformer: Vision transformers at mobilenet\nspeed,‚Äù arXiv preprint arXiv:2206.01191, 2022.\n[35] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang,\n‚ÄúCvt: Introducing convolutions to vision transformers,‚Äù inProceed-\nings of the IEEE/CVF International Conference on Computer Vision ,\n2021, pp. 22‚Äì31.\n[36] Z. Peng, W. Huang, S. Gu, L. Xie, Y. Wang, J. Jiao, and Q. Ye,\n‚ÄúConformer: Local features coupling global representations for\nvisual recognition,‚Äù in Proceedings of the IEEE/CVF International\nConference on Computer Vision, 2021, pp. 367‚Äì376.\n[37] B. Graham, A. El-Nouby, H. Touvron, P . Stock, A. Joulin, H. J¬¥egou,\nand M. Douze, ‚ÄúLevit: a vision transformer in convnet‚Äôs clothing\nfor faster inference,‚Äù in Proceedings of the IEEE/CVF International\nConference on Computer Vision, 2021, pp. 12 259‚Äì12 269.\n[38] Y. Chen, X. Dai, D. Chen, M. Liu, X. Dong, L. Yuan, and\nZ. Liu, ‚ÄúMobile-former: Bridging mobilenet and transformer,‚Äù\narXiv preprint arXiv:2108.05895, 2021.\n[39] B. Heo, S. Yun, D. Han, S. Chun, J. Choe, and S. J. Oh, ‚ÄúRethinking\nspatial dimensions of vision transformers,‚Äù in Proceedings of the\nIEEE/CVF International Conference on Computer Vision , 2021, pp.\n11 936‚Äì11 945.\n[40] C.-F. R. Chen, Q. Fan, and R. Panda, ‚ÄúCrossvit: Cross-attention\nmulti-scale vision transformer for image classiÔ¨Åcation,‚Äù in Proceed-\nings of the IEEE/CVF international conference on computer vision, 2021,\npp. 357‚Äì366.\n[41] H. Fan, B. Xiong, K. Mangalam, Y. Li, Z. Yan, J. Malik, and\nC. Feichtenhofer, ‚ÄúMultiscale vision transformers,‚Äù in Proceedings\nof the IEEE/CVF International Conference on Computer Vision , 2021,\npp. 6824‚Äì6835.\n[42] L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, Z.-H. Jiang, F. E. Tay,\nJ. Feng, and S. Yan, ‚ÄúTokens-to-token vit: Training vision trans-\nformers from scratch on imagenet,‚Äù in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2021, pp. 558‚Äì567.\n[43] Q. Yu, Y. Xia, Y. Bai, Y. Lu, A. L. Yuille, and W. Shen, ‚ÄúGlance-and-\ngaze vision transformer,‚Äù Advances in Neural Information Processing\nSystems, vol. 34, 2021.\n[44] Z. Xia, X. Pan, S. Song, L. E. Li, and G. Huang, ‚ÄúVision transformer\nwith deformable attention,‚Äù in Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition , 2022, pp. 4794‚Äì\n4803.\n[45] X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and\nC. Shen, ‚ÄúTwins: Revisiting the design of spatial attention in vision\ntransformers,‚Äù Advances in Neural Information Processing Systems ,\nvol. 34, 2021.\n[46] Q. Zhang and Y.-B. Yang, ‚ÄúRest: An efÔ¨Åcient transformer for visual\nrecognition,‚Äù Advances in Neural Information Processing Systems ,\nvol. 34, pp. 15 475‚Äì15 485, 2021.\n[47] X. Dong, J. Bao, D. Chen, W. Zhang, N. Yu, L. Yuan, D. Chen, and\nB. Guo, ‚ÄúCswin transformer: A general vision transformer back-\nbone with cross-shaped windows,‚Äù in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2022, pp.\n12 124‚Äì12 134.\n[48] Y. Lee, J. Kim, J. Willette, and S. J. Hwang, ‚ÄúMpvit: Multi-\npath vision transformer for dense prediction,‚Äù arXiv preprint\narXiv:2112.11010, 2021.\n[49] C. Yang, Y. Wang, J. Zhang, H. Zhang, Z. Wei, Z. Lin, and\nA. Yuille, ‚ÄúLite vision transformer with enhanced self-attention,‚Äù\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2022, pp. 11 998‚Äì12 008.\n[50] J. Hu, L. Shen, S. Albanie, G. Sun, and E. Wu, ‚ÄúSqueeze-and-\nexcitation networks,‚Äù in 2018 IEEE/CVF Conference on Computer\nVision and Pattern Recognition, vol. 42, no. 8, 2018, pp. 2011‚Äì2023.\n[51] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P . Perona, D. Ramanan,\nP . Doll¬¥ar, and C. L. Zitnick, ‚ÄúMicrosoft coco: Common objects in\ncontext,‚Äù in European conference on computer vision. Springer, 2014,\npp. 740‚Äì755.\n[52] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, ‚ÄúScalable\nperson re-identiÔ¨Åcation: A benchmark,‚Äù in 2015 IEEE International\nConference on Computer Vision (ICCV), 2015, pp. 1116‚Äì1124.\n[53] B. Recht, R. Roelofs, L. Schmidt, and V . Shankar, ‚ÄúDo imagenet\nclassiÔ¨Åers generalize to imagenet?‚Äù in International Conference on\nMachine Learning. PMLR, 2019, pp. 5389‚Äì5400.\n[54] C. Szegedy, W. Liu, Y. Jia, P . Sermanet, S. Reed, D. Anguelov,\nD. Erhan, V . Vanhoucke, and A. Rabinovich, ‚ÄúGoing deeper with\nconvolutions,‚Äù in Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2015, pp. 1‚Äì9.\n[55] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz,\n‚Äúmixup: Beyond empirical risk minimization,‚Äù arXiv preprint\narXiv:1710.09412, 2017.\n[56] E. D. Cubuk, B. Zoph, D. Mane, V . Vasudevan, and Q. V . Le,\n‚ÄúAutoaugment: Learning augmentation strategies from data,‚Äù in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2019, pp. 113‚Äì123.\n[57] Z. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang, ‚ÄúRandom\nerasing data augmentation,‚Äù in Proceedings of the AAAI conference\non artiÔ¨Åcial intelligence, vol. 34, no. 07, 2020, pp. 13 001‚Äì13 008.\n[58] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna,\n‚ÄúRethinking the inception architecture for computer vision,‚Äù in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2016, pp. 2818‚Äì2826.\n[59] I. Loshchilov and F. Hutter, ‚ÄúDecoupled weight decay regulariza-\ntion,‚Äù arXiv preprint arXiv:1711.05101, 2017.\n[60] Y. Yuan, R. Fu, L. Huang, W. Lin, C. Zhang, X. Chen, and J. Wang,\n‚ÄúHrformer: High-resolution transformer for dense prediction,‚Äù\narXiv preprint arXiv:2110.09408, 2021.\n[61] N. Ma, X. Zhang, J. Huang, and J. Sun, ‚ÄúWeightnet: Revisiting\nthe design space of weight networks,‚Äù in European Conference on\nComputer Vision. Springer, 2020, pp. 776‚Äì792.\n[62] H. Yan, Z. Li, W. Li, C. Wang, M. Wu, and C. Zhang, ‚ÄúContnet:\nWhy not use convolution and transformer at the same time?‚ÄùarXiv\npreprint arXiv:2104.13497, 2021.\n[63] W. Xu, Y. Xu, T. Chang, and Z. Tu, ‚ÄúCo-scale conv-attentional\nimage transformers,‚Äù in Proceedings of the IEEE/CVF International\nConference on Computer Vision, 2021, pp. 9981‚Äì9990.\n[64] S. Ren, D. Zhou, S. He, J. Feng, and X. Wang, ‚ÄúShunted\nself-attention via multi-scale token aggregation,‚Äù arXiv preprint\narXiv:2111.15193, 2021.\n[65] A. Hatamizadeh, H. Yin, J. Kautz, and P . Molchanov, ‚ÄúGlobal\ncontext vision transformers,‚Äù arXiv e-prints, pp. arXiv‚Äì2206, 2022.\n[66] W. Yu, M. Luo, P . Zhou, C. Si, Y. Zhou, X. Wang, J. Feng, and\nS. Yan, ‚ÄúMetaformer is actually what you need for vision,‚Äù arXiv\npreprint arXiv:2111.11418, 2021.\n[67] K. Sun, B. Xiao, D. Liu, and J. Wang, ‚ÄúDeep high-resolution rep-\nresentation learning for human pose estimation,‚Äù in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2019, pp. 5693‚Äì5703.\n[68] H. Yin, A. Vahdat, J. M. Alvarez, A. Mallya, J. Kautz, and\nP . Molchanov, ‚ÄúA-vit: Adaptive tokens for efÔ¨Åcient vision trans-\nformer,‚Äù in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2022, pp. 10 809‚Äì10 818.\n[69] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\nH. J ¬¥egou, ‚ÄúTraining data-efÔ¨Åcient image transformers & distil-\nlation through attention,‚Äù in International Conference on Machine\nLearning. PMLR, 2021, pp. 10 347‚Äì10 357.\nSUBMISSION TO IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 17\n[70] A. Krizhevsky, G. Hinton et al., ‚ÄúLearning multiple layers of fea-\ntures from tiny images,‚ÄùHandbook of Systemic Autoimmune Diseases,\n2009.\n[71] K. Patel, A. M. Bur, F. Li, and G. Wang, ‚ÄúAggregating\nglobal features into local vision transformer,‚Äù arXiv preprint\narXiv:2201.12903, 2022.\n[72] P . Sun, R. Zhang, Y. Jiang, T. Kong, C. Xu, W. Zhan, M. Tomizuka,\nL. Li, Z. Yuan, C. Wang et al. , ‚ÄúSparse r-cnn: End-to-end object\ndetection with learnable proposals,‚Äù in Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , 2021, pp.\n14 454‚Äì14 463.\n[73] T.-Y. Lin, P . Goyal, R. Girshick, K. He, and P . Doll ¬¥ar, ‚ÄúFocal loss\nfor dense object detection,‚Äù in Proceedings of the IEEE international\nconference on computer vision, 2017, pp. 2980‚Äì2988.\n[74] K. He, G. Gkioxari, P . Doll ¬¥ar, and R. Girshick, ‚ÄúMask r-cnn,‚Äù in\nProceedings of the IEEE international conference on computer vision ,\n2017, pp. 2961‚Äì2969.\n[75] X. Glorot and Y. Bengio, ‚ÄúUnderstanding the difÔ¨Åculty of training\ndeep feedforward neural networks,‚Äù in Proceedings of the thirteenth\ninternational conference on artiÔ¨Åcial intelligence and statistics . JMLR\nWorkshop and Conference Proceedings, 2010, pp. 249‚Äì256.\n[76] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba,\n‚ÄúScene parsing through ade20k dataset,‚Äù in Proceedings of the IEEE\nconference on computer vision and pattern recognition , 2017, pp. 633‚Äì\n641.\n[77] A. Kirillov, R. Girshick, K. He, and P . Doll ¬¥ar, ‚ÄúPanoptic feature\npyramid networks,‚Äù in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2019, pp. 6399‚Äì6408.\n[78] F. Yu and V . Koltun, ‚ÄúMulti-scale context aggregation by dilated\nconvolutions,‚Äù arXiv preprint arXiv:1511.07122, 2015.\n[79] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun, ‚ÄúUniÔ¨Åed perceptual\nparsing for scene understanding,‚Äù in Proceedings of the European\nconference on computer vision (ECCV), 2018, pp. 418‚Äì434.\n[80] H. Luo, Y. Gu, X. Liao, S. Lai, and W. Jiang, ‚ÄúBag of tricks\nand a strong baseline for deep person re-identiÔ¨Åcation,‚Äù in 2019\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\nWorkshops (CVPRW), 2019, pp. 0‚Äì0.\n[81] K. Zhou, Y. Yang, A. Cavallaro, and T. Xiang, ‚ÄúOmni-scale feature\nlearning for person re-identiÔ¨Åcation,‚Äù in 2019 IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV), 2019, pp. 3702‚Äì3712.\n[82] H. Li, G. Wu, and W.-S. Zheng, ‚ÄúCombined depth space based\narchitecture search for person re-identiÔ¨Åcation,‚Äù in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2021, pp. 6729‚Äì6738.\n[83] R. Quan, X. Dong, Y. Wu, L. Zhu, and Y. Yang, ‚ÄúAuto-reid:\nSearching for a part-aware convnet for person re-identiÔ¨Åcation,‚Äù in\n2019 IEEE/CVF International Conference on Computer Vision (ICCV) ,\n2019, pp. 3749‚Äì3758.\n[84] S. He, H. Luo, P . Wang, F. Wang, H. Li, and W. Jiang, ‚ÄúTransreid:\nTransformer-based object re-identiÔ¨Åcation,‚Äù in Proceedings of the\nIEEE/CVF International Conference on Computer Vision , 2021, pp.\n15 013‚Äì15 022.\n[85] Y. Sun, L. Zheng, Y. Yang, Q. Tian, and S. Wang, ‚ÄúBeyond part\nmodels: Person retrieval with reÔ¨Åned part pooling (and a strong\nconvolutional baseline),‚Äù in Proceedings of the European conference\non computer vision (ECCV), 2018, pp. 480‚Äì496.\n[86] Y. Sun, Q. Xu, Y. Li, C. Zhang, Y. Li, S. Wang, and J. Sun, ‚ÄúPerceive\nwhere to focus: Learning visibility-aware part-level features for\npartial person re-identiÔ¨Åcation,‚Äù in Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , 2019, pp. 393‚Äì\n402.\n[87] A. Hermans, L. Beyer, and B. Leibe, ‚ÄúIn defense of the triplet loss\nfor person re-identiÔ¨Åcation.‚Äù arXiv preprint arXiv:1703.07737, 2017.\n[88] V . Shankar, R. Roelofs, H. Mania, A. Fang, B. Recht, and\nL. Schmidt, ‚ÄúEvaluating machine accuracy on imagenet,‚Äù in In-\nternational Conference on Machine Learning. PMLR, 2020, pp. 8634‚Äì\n8644.\nGaojie Wu is currently a Ph.D candidate from\nschool of computer science and engineering,\nSun Y at-sen University. His research interests\nmainly focus on neural architecture search and\ncomputer vision.\nWei-Shi Zheng is now a full Professor with Sun\nY at-sen University. Dr. Zheng received his Ph.D.\ndegree in Applied Mathematics from Sun Y at-\nsen University in 2008. His research interests\ninclude person/object association and activity\nunderstanding in visual surveillance, and the\nrelated large-scale machine learning algorithm.\nEspecially, Dr. Zheng has active research on\nperson re-identiÔ¨Åcation in the last Ô¨Åve years. He\nhas ever joined Microsoft Research Asia Y oung\nFaculty Visiting Programme. He has ever served\nas area chairs of CVPR, ICCV, BMVC and IJCAI. He is an IEEE MSA TC\nmember. He is an associate editor of the Pattern Recognition Journal.\nHe is a recipient of the Excellent Y oung Scientists Fund of the National\nNatural Science Foundation of China, and a recipient of the Royal\nSociety-Newton Advanced Fellowship of the United Kingdom.\nYutong Lu is Professor in Sun Y at-sen Uni-\nversity (SYSU), Director of National super-\ncomputing center in Guangzhou. Her exten-\nsive research and development experience has\nspanned several generations of domestic su-\npercomputers in China. Her continuing research\ninterests include HPC, Cloud computing, stor-\nage and Ô¨Åle system, and advanced programming\nenvironment. At present, she is devoted to the\nresearch and implementation of system and ap-\nplication for the convergence of HPC, Bigdata\nand AI on supercomputer.\nQi Tian (Fellow, IEEE) received the B.E. de-\ngree in electronic engineering from Tsinghua\nUniversity and the Ph.D. degree in electrical and\ncomputer engineering (ECE) from the University\nof Illinois at Urbana‚ÄìChampaign (UIUC), Cham-\npaign, IL, USA. His Google citation is 50000+,\nwith H- index 101. He was a Visiting Chaired\nProfessor with the Center for Neural and Cog-\nnitive Computation, Tsinghua University, and a\nLead Researcher with the Media Computing\nGroup, Microsoft Research Asia (MSRA). He is\ncurrently the Chief Scientist of artiÔ¨Åcial intelligence with Huawei Cloud\n& AI, a Changjiang Chaired Professor of the Ministry of Education, an\nOverseas Outstanding Y outh, and an Overseas Expert by the Chinese\nAcademy of Sciences."
}