{
    "title": "A Large-Scale Study of Language Models for Chord Prediction",
    "url": "https://openalex.org/W2795825978",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A4287193189",
            "name": "Korzeniowski, Filip",
            "affiliations": [
                "Johannes Kepler University of Linz"
            ]
        },
        {
            "id": "https://openalex.org/A4287942598",
            "name": "Sears, David R. W.",
            "affiliations": [
                "Texas Tech University"
            ]
        },
        {
            "id": "https://openalex.org/A4281357259",
            "name": "Widmer, Gerhard",
            "affiliations": [
                "Johannes Kepler University of Linz"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6765720210",
        "https://openalex.org/W6713772116",
        "https://openalex.org/W6697136871",
        "https://openalex.org/W2537159808",
        "https://openalex.org/W2160484251",
        "https://openalex.org/W6631117800",
        "https://openalex.org/W6659421274",
        "https://openalex.org/W6732858653",
        "https://openalex.org/W6697297547",
        "https://openalex.org/W2395935897",
        "https://openalex.org/W6639340343",
        "https://openalex.org/W2079580423",
        "https://openalex.org/W6645928866",
        "https://openalex.org/W1989216934",
        "https://openalex.org/W6712324461",
        "https://openalex.org/W6607333740",
        "https://openalex.org/W6639497327",
        "https://openalex.org/W2964199361",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W1982446897",
        "https://openalex.org/W2407022421",
        "https://openalex.org/W3122695829",
        "https://openalex.org/W2577760017",
        "https://openalex.org/W2304209433",
        "https://openalex.org/W2398360960",
        "https://openalex.org/W2172140247",
        "https://openalex.org/W2296557957",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2950577311"
    ],
    "abstract": "We conduct a large-scale study of language models for chord prediction. Specifically, we compare N-gram models to various flavours of recurrent neural networks on a comprehensive dataset comprising all publicly available datasets of annotated chords known to us. This large amount of data allows us to systematically explore hyper-parameter settings for the recurrent neural networks---a crucial step in achieving good results with this model class. Our results show not only a quantitative difference between the models, but also a qualitative one: in contrast to static N-gram models, certain RNN configurations adapt to the songs at test time. This finding constitutes a further step towards the development of chord recognition systems that are more aware of local musical context than what was previously possible.",
    "full_text": "A LARGE-SCALE STUDY OF LANGUAGE MODELS\nFOR CHORD PREDICTION\nFilip Korzeniowski⋆ David R. W. Sears† Gerhard Widmer⋆\n⋆ Department of Computational Perception, Johannes Kepler University, Linz, Austria\n† College of Visual & Performing Arts, Texas Tech University, Lubbock, USA\nABSTRACT\nWe conduct a large-scale study of language models for chord predic-\ntion. Speciﬁcally, we compare N-gram models to various ﬂavours\nof recurrent neural networks on a comprehensive dataset compris-\ning all publicly available datasets of annotated chords known to us.\nThis large amount of data allows us to systematically explore hyper-\nparameter settings for the recurrent neural networks—a crucial step\nin achieving good results with this model class. Our results show\nnot only a quantitative difference between the models, but also a\nqualitative one: in contrast to static N-gram models, certain RNN\nconﬁgurations adapt to the songs at test time. This ﬁnding consti-\ntutes a further step towards the development of chord recognition\nsystems that are more aware of local musical context than what was\npreviously possible.\nIndex Terms— Language Modelling, Chord Prediction, Recur-\nrent Neural Networks\n1. INTRODUCTION\nChord recognition is a long-standing topic in the music information\nretrieval community. Steady advances notwithstanding, results have\nseemed to stagnate in recent years. The study in [1] offers possi-\nble explanations, including invalid harmonic assumptions, limita-\ntions of evaluation measures, conﬂicting problem deﬁnitions, and\nthe subjectivity inherent to this task. All of these criticisms are cer-\ntainly valid, and in our view, the latter two bear the greatest potential\nto hamper further improvements. However, at present there is still\na large performance gap between human annotators (measured via\ninter-annotator agreement) and automatic systems [1], although they\nare subjected to the same constraints.\nWe can divide a chord recognition system into two parts:\nhearing (acoustic modelling, feature extraction) and understand-\ning (combining observations, ﬁnding a meaningful chord sequence).\nResearch has focused mainly on the former by ﬁnding better features\nand acoustic models [2–4], while only a few works have explored\nimprovements of the latter using temporal models [5–7]. This trend\nwas reinforced through the insight that existing temporal models do\nlittle but smooth the noisy predictions of the acoustic model over\ntime, i.e. they mainly model the chord’s duration [8, 9].\nThus, we suggest another possible explanation for the perfor-\nmance gap: current state-of-the-art chord recognition systems lack\na meaningful understanding of harmony and its development in mu-\nsic. As shown in [10], such an understanding can only derive from\nThis work is supported by the European Research Council (ERC) un-\nder the EU’s Horizon 2020 Framework Programme (ERC Grant Agreement\nnumber 670035, project “Con Espressione”).\nName Ref. No. Pieces No. Chords\nBeatles [11] 180 12 646\nJay Chou [12] 29 3 356\nMcGill Billboard [13] 742 70 197\nQueen [14] 20 2 265\nRobbie Williams [15] 65 6 513\nRock [16] 201 18 343\nRWC [17] 100 12 726\nUS Pop 2002 [18] 195 23 309\nWeimar Jazz [19] 291 18 179\nZweieck [14] 18 1 822\nTotal 1 841 169 356\nUnique 1 766 161 796\nTable 1. Individual datasets used to create the compound dataset in\nthis study. For the Rock corpus, we used the annotations by Temper-\nley rather than those by De Clerq.\nsequential models that operate at higher temporal levels than those\nbased on audio frames.\nThe present work addresses this issue by exploring the capabili-\nties of automatically learned chord language models to predict chord\nsequences. To our knowledge, this paper constitutes the ﬁrst at-\ntempt to systematically ﬁnd and evaluate such models. We have seen\nwork on optimising low-level and/or Markovian temporal models in\ne.g. [9], and work on low-level non-Markovian models in e.g. [5],\nbut as argued in [10], at the level of audio frames, there is little to\nimprove upon the current state-of-the-art.\nOur work compares recurrent neural networks (RNNs) with\nhigher-order N-gram models. We evaluate these models inde-\npendently based on how well they predict chord sequences. The\nquestion on how to integrate them into a complete chord recognition\nsystem is left for future work.\n2. DATA\nWe compiled a comprehensive set of chord annotations to perform\na large-scale evaluation of different language models for chord pre-\ndiction. To our knowledge, our compound dataset consists of all\ntime-aligned chord annotations that are publicly available. Table 1\nprovides detailed information about the data. In total, we have 1 841\nsongs from a variety of genres and decades, with a focus on pop/rock\nbetween 1950 and 2000. After we remove duplicate songs and merge\nconsecutive identical annotations, the dataset consists of 1 766 songs\ncontaining 161 796 unique chord annotations.\narXiv:1804.01849v1  [cs.LG]  5 Apr 2018\nThe dataset is miniscule compared to those available for natural\nlanguage processing: e.g., the NYT section of the English Giga-\nword dataset (of which only a subset of 6.4 million words is used\nin [20] for training language models) consists of 37 million words.\nTherefore, we leverage domain knowledge to generate additional,\nsemi-artiﬁcial chord sequences. Assuming that chord progressions\nare independent of musical key (as in Roman numeral analysis), and\nthat musical keys are uniformly distributed (a simplifying assump-\ntion), we transpose each chord sequence to all possible keys during\ntraining. This step increases the amount of training sequences by\na factor of 12; however, the artiﬁcially created data are highly cor-\nrelated with the existing data, and thus are not equivalent to truly\nhaving 12 times as much data available. We will refer to this process\nas data augmentation in the remainder of this paper.\nWe focus on the major/minor chord vocabulary, and follow the\nstandard mapping as described in [9]: chords with a minor 3 rd in\nthe ﬁrst interval are considered minor, the rest major. This mapping\nresults in a vocabulary of size 25 ( 12 root notes ×{maj,min}and\nthe “no-chord” class). Bearing in mind the reduced vocabulary and\nthe more repetitive nature of chord progressions compared to spoken\nlanguage, we feel that the size of this dataset after data augmentation\nis appropriate for training and evaluating models for chord predic-\ntion.\n3. EXPERIMENTAL SETUP\nOur experiments evaluate how well various models predict chords.\nThis amounts to measuring the average probability the model assigns\nto the (correct) upcoming chord symbol, given the ones it has already\nobserved in a sequence. More formally, given a sequence of chords\ny = (y1,...,y K), the model M yields PM (yk |yk−1\n1 ) for each k.\n(We denote y1,...,y k−1 as yk−1\n1 .) The probability of the complete\nchord sequence can then be computed as\nPM (y) = PM (y1) ·ΠK\nk=2PM\n(\nyk |yk−1\n1\n)\n. (1)\nThe higher PM (y), the better M models y. To measure how\nwell M models all chord sequences in a dataset Y, we next compute\nthe average log-probability assigned to its sequences:\nL(M,Y) = 1\nNY\n∑\ny∈Y\nlog (PM (y)) , (2)\nwhere NY is the total number of chord symbols in the dataset.\nThis equation corresponds to the negative cross-entropy between the\nmodel’s distribution and the data distribution.\nAll models are trained and tested on the compound dataset. We\nuse 20% of the data for testing, and 80% for training. 15% of the\ntraining data is held out for validation. All splits are stratiﬁed by\ndataset.\n4. N-GRAM LANGUAGE MODELS\nN-gram language models are Markovian probabilistic models that\nassume a ﬁxed-length history (of length N −1) in order to predict\nthe next symbol. Hence, they assume PM (yk |yk−1\n1 ) = PM (yk |\nyk−1\nk−N+1). This ﬁxed-length history allows the probabilities to\nbe stored in a table, with its entries computed using maximum-\nlikelihood estimation (i.e., by counting occurrences in the training\nset).\nWith larger N, the sparsity of the probability table increases\nexponentially due to the ﬁnite number of N-grams in the training\n/uni00000088/uni00000089/uni0000008a/uni0000008b/uni0000008c/uni000000a4\nN\n/uni0000008a/uni00000057/uni00000087\n/uni00000089/uni00000057/uni0000008c\n/uni00000089/uni00000057/uni00000087\n/uni00000088/uni00000057/uni0000008c\n/uni00000088/uni00000057/uni00000087\n(M, )\n/uni0000001a/uni00000038/uni00000021/uni0000002d/uni00000032\n/uni0000001c/uni00000021/uni000000a3/uni0000002d/uni00000026/uni00000021/uni0000003b/uni0000002d/uni00000033/uni00000032\n/uni0000001a/uni00000038/uni00000021/uni0000002d/uni00000032/uni00000054/uni00000002/uni00000003/uni0000003c/uni0000002b/uni00000031/uni00000027/uni00000032/uni0000003b/uni00000027/uni00000026\n/uni0000001c/uni00000021/uni000000a3/uni0000002d/uni00000026/uni00000021/uni0000003b/uni0000002d/uni00000033/uni00000032/uni00000054/uni00000002/uni00000003/uni0000003c/uni0000002b/uni00000031/uni00000027/uni00000032/uni0000003b/uni00000027/uni00000026\nFig. 1. Average log-probability of all evaluated N-gram models.\nset. We solve this problem using Lidstone smoothing, which adds a\npseudo-count αto each possible N-gram. We determine the value\nof αfor each model using the validation set.\n4.1. Model Selection\nWe ﬁnd the best N-gram model by selecting the one with best re-\nsults on the validation set. To this end, we evaluate models with\nN ∈{1,2,3,4,5,6}, with and without data augmentation. N = 1\ncorresponds to a model that predicts chords just by their frequency\nin the training data, N = 2 to a model that could be deployed in a\nsimple ﬁrst-order Hidden Markov Model.\nFigure 1 presents the results of the models on the training and\nvalidation sets. With data augmentation, the best result is achieved\nby a 5-gram model (α= 0.3).\n5. RECURRENT NEURAL LANGUAGE MODELS\nRecurrent Neural Networks (RNNs, see [21] for an overview) are\npowerful models designed for sequential modelling tasks. In their\nsimplest form, RNNs transform input sequences xK\n1 to an output\nsequence oK\n1 through a non-linear projection into a hidden layerhK\n1 ,\nparameterised by weight matrices Whx, Whh and Woh:\nhk = σh (Whxxk + Whhhk−1) (3)\nok = σo (Wohhk) , (4)\nwhere σh and σo are the activation functions for the hidden layer\n(e.g. the sigmoid function), and the output layer (e.g. the softmax),\nrespectively. We left out bias terms for simplicity.\nTheir use as language models was proposed in [20]. For this\npurpose, the input at each time step k is a vector representation of\nthe preceding symbol yk−1. We call this the chord embedding, and\ndenote it v(yk−1). In the simplest case, this is a one-hot vector. The\nnetwork’s outputok is then interpreted as the conditional probability\nover the next chord symbol PM\n(\nYk |yk−1\n1\n)\n. During training, the\ncategorical cross-entropy between the ok and the true chord symbol\nis minimised by adapting the weight matrices in Eq. 3 and 4 using\nstochastic gradient descent and back-propagation through time. Fig-\nure 2 provides an overview of the model structure.\nEach output ok depends on all previous inputsyk−1\n1 through the\nrecurrent connection in the hidden layer. This allows the network to\nconsider all previous chords when computing PM (yk |yk−1\n1 ). In\nFig. 2. A simple RNN-based language model. We can easily stack\nmore recurrent hidden layers or add skip-connections between the\ninput and each hidden layer or the output.\npractice, this capacity is limited because of the limited size of the\nhidden layer, and the fragile learning procedure of back-propagation\nthrough time, which faces the well-known problems of exploding\nand vanishing gradients.\n5.1. Chord Embeddings\nAs mentioned earlier, we need to represent chord classes as vectors\nto use them in the RNN language model framework. In this work,\nwe explore three possibilities: 1) using the one-hot encoding of the\nclass, 2) using a ﬁxed-length vector that is learned jointly with the\nlanguage model, and 3) learning an embedding using the word2vec\nskip-gram model [22] before training the language model itself. In\nthis case, the chord embeddings are optimised to predict the neigh-\nbouring chords.\n5.2. Model Selection\nRNNs have more hyper-parameters compared to N-gram models:\nwe can set the number of hidden layers, the size of each hidden\nlayer, the type and dimensionality of the input chord embedding,\nthe activation function for each hidden layer, whether to use skip-\nconnections, and ﬁnally, we can decide to use simple RNNs or more\nadvanced hidden layer structures such as Long Short-Term Memory\n(LSTM) [23] or Gated Recurrent Units (GRU) [24]. Additionally,\nthe training procedure has its own hyper-parameters, such as learn-\ning rate or mini-batch size. Table 2 presents the hyper-parameter\nspace we sampled from.\nWe ﬁx the following hyper-parameters for all training runs: we\napply data augmentation, employ stochastic gradient descent with a\nbatch size of 4 and the ADAM update rule [25], set all hidden layers\nto the same (but variable) size, and stop training if the validation loss\ndoes not improve within 15 epochs.\nThe large number of hyper-parameters prevents us from con-\nducting an exhaustive search for the optimal architecture. Instead,\nwe use Hyperband [26], a bandit-based black-box hyper-parameter\noptimisation scheme, to ﬁnd good conﬁgurations for each considered\nRNN type.\nIn total, we considered 128 conﬁguration for each RNN type\n(384 different models). The best RNN setup used one-hot input en-\ncoding, skip connections, and Nh = 5, Dh = 256, lr = 2.5e−4.\nThe best GRU also used one-hot input encoding, but no skip connec-\ntions, and Nh = 3, Dh = 512, lr = 1e−3. The best LSTM used a\nword2vec input encoding with 16 dimensions, skip connections, and\nNh = 3, Dh = 512, lr= 1e−3.\nHyper Parameter Sample Space\nEmbedding Size {4,8,16,24}\nEmbedding Type {one-hot,word2vec,learned}\nNo. Hidden Layers Nh ∈{1,2,3,4,5}\nHidden Layer Size Dh ∈{128,256,512,1024}\nSkip Connections {yes,no}\nLearning Rate lr=\n{\n{0.001,0.0005} Nh ≤3\n{0.0005,0.00025} else\nLearning Rate (GRU) lr=\n{\n{0.005,0.001} Nh ≤3\n{0.001,0.0005} else\nTable 2. Hyper-parameter space we sampled from to ﬁnd good\nmodel conﬁgurations for each of the RNN types (simple, LSTM,\nGRU). Possible learning rate values were determined on a limited\nnumber of preliminary experiments.\n5-gram RNN GRU LSTM\nVal. 1.830 1.465 1.328 1.302\nVal. (ﬁne-tuned) — 1.417 1.290 1.272\nTest 1.874 1.387 1.244 1.249\nTable 3. −L(M,Y) (lower is better) on the validation and test sets\nfor the best model of each model class. All RNN-based models out-\nperform the 5-gram model, with the GRU yielding the highest avg.\nlog-probability. Statistically equivalent results are marked in bold.\nIn a ﬁnal step, we further improve the learned models by a ﬁne-\ntuning stage. Here, we take the best models found for each RNN\ntype, and re-start training from the epoch that gave the best results\non the validation set, but use 1/10 of the original learning rate. As\nshown in Table 3, this step slightly improves the models.\n6. RESULTS AND DISCUSSION\nWe compare the best model of each RNN type with the 5-gram base-\nline. To this end, we compute L(M,Y) according to Eq. 2 on the\ntest set, and present the results in Tab. 3 and Fig. 3. We see the RNN-\nbased models easily outperform the 5-gram model, with the LSTM\nand GRU models performing best. Statistical signiﬁcance was deter-\nmined by a paired t-test with Bonferroni correction.\nRecurrent neural networks have the capability to remember in-\nformation over time. We thus call them dynamic models, compared\nto the static characteristic of standard models based on N-grams.\nTo investigate if the RNN-based language models can leverage this\ncapability, we examine the development of their prediction quality—\nthe log-probability of the correct chord—over the progression of\nsongs.\nThis is a function of both model capacity and song complexity:\ngiven a static model, and repetitive songs that do not change much\nover time, the log-probabilities assigned to the chords should remain\napproximately constant; however, if e.g. chords in interludes tend to\ndeviate more from standard chord progressions, static models would\nyield lower log-probabilities in these cases. On the other hand, if a\ndynamic model could remember chord progressions from the past,\nthe predictions would improve over time for very repetitive songs.\n/uni00000005/uni00000027/uni00000021/uni0000003b/uni000000a3/uni00000027/uni00000039\n/uni0000006c/uni00000089/uni0000008c/uni000000a4/uni0000008c/uni0000006d\n/uni0000000e/uni00000021/uni00000040/uni00000002/uni00000006/uni0000002c/uni00000033/uni0000003c\n/uni0000006c/uni000000a4/uni0000008e/uni0000008a/uni0000006d\n/uni00000011/uni00000024/uni0000000b/uni0000002d/uni000000a3/uni000000a3/uni00000002/uni00000005/uni0000002d/uni000000a3/uni000000a3/uni00000023/uni00000033/uni00000021/uni00000038/uni00000026\n/uni0000006c/uni00000088/uni0000008a/uni000000a4/uni0000008b/uni0000008e/uni0000006d\n/uni00000017/uni0000003c/uni00000027/uni00000027/uni00000032\n/uni0000006c/uni0000008c/uni0000008a/uni0000008a/uni0000006d\n/uni00000018/uni0000001d/uni00000006\n/uni0000006c/uni00000089/uni0000008b/uni0000008b/uni0000008e/uni0000006d\n/uni00000018/uni00000033/uni00000023/uni00000023/uni0000002d/uni00000027/uni00000002/uni0000001d/uni0000002d/uni000000a3/uni000000a3/uni0000002d/uni00000021/uni00000031/uni00000039\n/uni0000006c/uni00000088/uni0000008c/uni0000008d/uni000000a5/uni0000006d\n/uni00000018/uni00000033/uni00000024/uni00000030\n/uni0000006c/uni0000008a/uni0000008c/uni000000a5/uni0000008c/uni0000006d\n/uni0000001b/uni00000019/uni00000002/uni00000016/uni00000033/uni00000036\n/uni0000006c/uni0000008b/uni0000008c/uni0000008b/uni00000088/uni0000006d\n/uni0000001d/uni00000027/uni0000002d/uni00000031/uni00000021/uni00000038/uni00000002/uni0000000e/uni00000021/uni00000041/uni00000041\n/uni0000006c/uni0000008a/uni000000a5/uni00000089/uni0000008a/uni0000006d\n/uni00000020/uni0000003e/uni00000027/uni0000002d/uni00000027/uni00000024/uni00000030\n/uni0000006c/uni0000008a/uni0000008c/uni000000a5/uni0000006d\n/uni00000006/uni00000033/uni00000031/uni00000036/uni00000033/uni0000003c/uni00000032/uni00000026\n/uni0000006c/uni0000008a/uni0000008a/uni0000008d/uni0000008d/uni0000008b/uni0000006d\n/uni00000087/uni00000057/uni00000087\n/uni00000087/uni00000057/uni0000008c\n/uni00000088/uni00000057/uni00000087\n/uni00000088/uni00000057/uni0000008c\n/uni00000089/uni00000057/uni00000087\n/uni00000089/uni00000057/uni0000008c\n(M, )\n/uni0000008c/uni00000066/uni0000000b/uni00000038/uni00000021/uni00000031/uni00000018/uni00000012/uni00000012/uni0000000b/uni00000018/uni0000001b/uni00000010/uni00000019/uni0000001a/uni00000011\nFig. 3. −L(M,Y) (lower is better) of the best model for each model class on the compound test set, and split up into the individual datasets.\nThe numbers in parentheses show the number of chord predictions in each set. Whiskers are 95% conﬁdence intervals computed using\nbootstrapping. We observe a similar pattern for each set, with the LSTM and GRU performing equally on most of the datasets. We also see\nthat chords are easier to predict in some datasets (e.g. “Rock”), while more difﬁcult in others, (e.g. “RWC”).\n/uni00000089/uni00000087/uni0000008b/uni00000087/uni000000a4/uni00000087/uni000000a5/uni00000087\nk\n/uni00000088/uni00000057/uni0000008e\n/uni00000088/uni00000057/uni000000a5\n/uni00000088/uni00000057/uni0000008d\n/uni00000088/uni00000057/uni000000a4\n/uni00000088/uni00000057/uni0000008c\n/uni00000088/uni00000057/uni0000008b\n/uni00000088/uni00000057/uni0000008a\n/uni00000088/uni00000057/uni00000089\n/uni00000088/uni00000057/uni00000088\n(k; M, )\n/uni0000008c/uni00000066/uni0000000b/uni00000038/uni00000021/uni00000031\n/uni00000018/uni00000012/uni00000012\n/uni0000000b/uni00000018/uni0000001b\n/uni00000010/uni00000019/uni0000001a/uni00000011\nFig. 4. Avg. cumulative log-probability per chord step\nL≤(k; M,Y). The predictions of the static 5-gram model worsen\nover time, which indicates that chord progressions later in the songs\ndeviate more from general patterns than in the beginning. The LSTM\nand GRU models do not suffer from this problem, however, and\npredictions even improve after chord 40. We conjecture that these\nmodels better remember previously seen chords and patterns, which\nenables them to automatically adapt as a song progresses.\nTo evaluate this quantitatively, we ﬁrst selected from the test\nset songs that contained at least 100 chords, which left us with 136\npieces. We then computed the average cumulative log-probability of\nchords up to a position in a song as\nL≤(k; M,Y) = 1\nk|Y|\n∑\ny∈Y\nk∑\n˜k=1\nlog\n[\nPM\n(\ny˜k |y\n˜k−1\n1\n)]\n. (5)\nNote that Eq. 5 converges to the objective in Eq. 2 with increasingk.\nFigure 4 presents the results for each model. To our surprise, the\nperformance of the static 5-gram model drops signiﬁcantly during\nthe ﬁrst 20 chords and continues to fall until around chord 60, af-\nter which it stagnates. This indicates that the chord progressions in\nthe beginning of the songs are easier to predict by a static model—\ni.e., more closely follow the general chord progressions the model\nlearned, while those later in the songs deviate more from common\npatterns.\nIn comparison, the RNN-based models do not suffer during the\nﬁrst 20 chords. Although they show similar behaviour during the\nﬁrst 20 chords, the LSTM and GRU models also behave differently\nlater in the songs: both the GRU and the LSTM improve from around\nchord 40, whereas the performance of the simple RNN continues to\ndrop until around chord 60 (similarly to the 5-gram model, but on a\nhigher level).\nSince predicting chords later in the songs is more difﬁcult than\nat the beginning, but the LSTM and the GRU are only negligibly\naffected by this (they almost recover to their performance at the be-\nginning), we argue that both the GRU and the LSTM models are\nbetter capable of adapting to the current song. One explanation for\nthis might be that these models privilege intra-song statistics during\nprediction. Thus, rather than learning a global, generic model trained\non the statistics of an entire corpus, we conjecture that these models\nalso acquire and apply knowledge about the immediate past.\n7. CONCLUSION\nWe presented a comprehensive evaluation of chord language models,\nwith a focus on RNN-based architectures. We discovered the best\nperforming hyper-parameters, and trained and evaluated the models\non a large compound dataset consisting of various genres. Our re-\nsults show that 1) all RNN-based models outperform N-gram mod-\nels, 2) gated RNN cells such as the LSTM cell or the GRU outper-\nform simple RNNs, and 3) that both LSTM and GRU networks seem\nto adapt their predictions to what they observed in the past.\nWe conjecture that to improve the recently stagnant chord recog-\nnition results, we need models with a better understanding of mu-\nsic than has been demonstrated in previous state-of-the-art systems.\nThis work is a ﬁrst step towards this goal. Future research needs\nto deal with modelling chord durations appropriately, and with inte-\ngrating such models with frame-wise acoustic chord classiﬁers.\n8. REFERENCES\n[1] Eric J. Humphrey and Juan P. Bello, “Four Timely Insights\non Automatic Chord Estimation,” in Proceedings of the 16th\nInternational Society for Music Information Retrieval Confer-\nence (ISMIR), M´alaga, Spain, 2015.\n[2] Eric J. Humphrey and Juan P. Bello, “Rethinking Automatic\nChord Recognition with Convolutional Neural Networks,” in\n11th International Conference on Machine Learning and Ap-\nplications (ICMLA), Boca Raton, USA, 2012.\n[3] Filip Korzeniowski and Gerhard Widmer, “Feature Learning\nfor Chord Recognition: The Deep Chroma Extractor,” in Pro-\nceedings of the 17th International Society for Music Informa-\ntion Retrieval Conference (ISMIR), New York, USA, 2016.\n[4] Yushi Ueda, Yuki Uchiyama, Takuya Nishimoto, Nobutaka\nOno, and Shigeki Sagayama, “HMM-based approach for au-\ntomatic chord detection using reﬁned acoustic features,” in\nInternational Conference on Acoustics Speech and Signal Pro-\ncessing (ICASSP), Dallas, USA, 2010.\n[5] Nicolas Boulanger-Lewandowski, Yoshua Bengio, and Pas-\ncal Vincent, “Audio chord recognition with recurrent neural\nnetworks,” in Proceedings of the 14th International Society\nfor Music Information Retrieval Conference (ISMIR), Curitiba,\nBrazil, 2013.\n[6] Matthias Mauch and Simon Dixon, “Approximate note tran-\nscription for the improved identiﬁcation of difﬁcult chords,” in\nProceedings of the 11th International Society for Music Infor-\nmation Retrieval Conference (ISMIR), Utrecht, Netherlands,\n2010.\n[7] Johan Pauwels and Jean-Pierre Martens, “Combining Musico-\nlogical Knowledge About Chords and Keys in a Simultaneous\nChord and Local Key Estimation System,” Journal of New\nMusic Research, vol. 43, no. 3, pp. 318–330, 2014.\n[8] Ruofeng Chen, Weibin Shen, Ajay Srinivasamurthy, and Parag\nChordia, “Chord recognition using duration-explicit hidden\nMarkov models,” in Proceedings of the 13th International\nSociety for Music Information Retrieval Conference (ISMIR),\nPorto, Portugal, 2012.\n[9] Taemin Cho and Juan P. Bello, “On the Relative Impor-\ntance of Individual Components of Chord Recognition Sys-\ntems,” IEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing, vol. 22, no. 2, pp. 477–492, 2014.\n[10] Filip Korzeniowski and Gerhard Widmer, “On the Futility of\nLearning Complex Frame-Level Language Models for Chord\nRecognition,” in Proceedings of the AES International Confer-\nence on Semantic Audio, Erlangen, Germany, 2017.\n[11] Christopher Harte, Towards Automatic Extraction of Harmony\nInformation from Music Signals, Dissertation, Department of\nElectronic Engineering, Queen Mary, University of London,\nLondon, United Kingdom, 2010.\n[12] Junqi Deng and Yu-Kwong Kwok, “Automatic Chord estima-\ntion on seventhsbass Chord vocabulary using deep neural net-\nwork,” in International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), Shanghai, China, 2016.\n[13] John Ashley Burgoyne, Jonathan Wild, and Ichiro Fujinaga,\n“An Expert Ground Truth Set for Audio Chord Recognition\nand Music Analysis.,” inProceedings of the 12th International\nSociety for Music Information Retrieval Conference (ISMIR),\nMiami, USA, 2011.\n[14] Matthias Mauch, Chris Cannam, Matthew Davies, Simon\nDixon, Christopher Harte, Sefki Kolozali, Dan Tidhar, and\nMark Sandler, “OMRAS2 metadata project 2009,” in Late\nBreaking Demo of the 10th International Conference on Music\nInformation Retrieval (ISMIR), Kobe, Japan, 2009.\n[15] Bruno Di Giorgi, Massimiliano Zanoni, Augusto Sarti, and\nStefano Tubaro, “Automatic chord recognition based on the\nprobabilistic modeling of diatonic modal harmony,” in Pro-\nceedings of the 8th International Workshop on Multidimen-\nsional Systems, Erlangen, Germany, 2013.\n[16] Trevor de Clercq and David Temperley, “A corpus analysis\nof rock harmony,” Popular Music, vol. 30, no. 01, pp. 47–70,\n2011.\n[17] Masataka Goto, Hiroki Hashiguchi, Takuichi Nishimura, and\nRyuichi Oka, “RWC Music Database: Popular, Classical and\nJazz Music Databases.,” in Proceedings of the 3rd Interna-\ntional Conference on Music Information Retrieval (ISMIR),\nParis, France, 2002.\n[18] Daniel P. W. Ellis, Adam Berenzweig, and Brian\nWhitman, “The ”uspop2002” Pop Music data set,”\nhttps://labrosa.ee.columbia.edu/projects/\nmusicsim/uspop2002.html, 2003.\n[19] The Jazzomat Research Project, “Weimar Jazz Database\n(WJazzD),” http://jazzomat.hfm-weimar.de/\ndbformat/dboverview.html, 2016.\n[20] Tomas Mikolov, Martin Karaﬁ ´at, Luk´as Burget, Jan Cernock´y,\nand Sanjeev Khudanpur, “Recurrent neural network based lan-\nguage model,” in INTERSPEECH, Chiba, Japan, 2010.\n[21] Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, and\nYoshua Bengio, “How to Construct Deep Recurrent Neural\nNetworks,” in Proceedings of the Second International Con-\nference on Learning Representations (ICLR), Banff, Canada,\n2014.\n[22] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean,\n“Efﬁcient estimation of word representations in vector space,”\narXiv:1301.3781, 2013.\n[23] Sepp Hochreiter and J ¨urgen Schmidhuber, “Long Short-Term\nMemory,” Neural Computation, vol. 9, no. 8, pp. 1735–1780,\n1997.\n[24] Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-\ndanau, and Yoshua Bengio, “On the Properties of Neu-\nral Machine Translation: Encoder-Decoder Approaches,”\narXiv:1409.1259, 2014.\n[25] Diederik Kingma and Jimmy Ba, “Adam: A method for\nstochastic optimization,” arXiv:1412.6980, 2014.\n[26] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Ros-\ntamizadeh, and Ameet Talwalkar, “Hyperband: A Novel\nBandit-Based Approach to Hyperparameter Optimization,”\narXiv:1603.06560, 2016."
}