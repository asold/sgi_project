{
  "title": "Gender Bias in Masked Language Models for Multiple Languages",
  "url": "https://openalex.org/W4229026816",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2000996583",
      "name": "Masahiro Kaneko",
      "affiliations": [
        "Tokyo Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2785979757",
      "name": "Aizhan Imankulova",
      "affiliations": [
        "Walmart (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2029493321",
      "name": "Danushka Bollegala",
      "affiliations": [
        "University of Liverpool",
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2030501650",
      "name": "Naoaki Okazaki",
      "affiliations": [
        "Tokyo Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3047171714",
    "https://openalex.org/W2997588435",
    "https://openalex.org/W3114950584",
    "https://openalex.org/W2913897682",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W4285184612",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W3035379020",
    "https://openalex.org/W3008110149",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4232537966",
    "https://openalex.org/W2964183365",
    "https://openalex.org/W2419539795",
    "https://openalex.org/W2972413484",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W2973088264",
    "https://openalex.org/W2942160782",
    "https://openalex.org/W3097202947",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2963078909",
    "https://openalex.org/W2972586109",
    "https://openalex.org/W3103187652",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W2962735107",
    "https://openalex.org/W3102725307",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3096266342",
    "https://openalex.org/W3004532592",
    "https://openalex.org/W4241439391",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2949969209",
    "https://openalex.org/W2972697496",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3155655882",
    "https://openalex.org/W3172415559",
    "https://openalex.org/W3153899207",
    "https://openalex.org/W2889624842",
    "https://openalex.org/W4385681388",
    "https://openalex.org/W3201341200",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970408399",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3105882417",
    "https://openalex.org/W3095105395",
    "https://openalex.org/W3035102548",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W4311398160",
    "https://openalex.org/W3194744716",
    "https://openalex.org/W3023547440",
    "https://openalex.org/W2982388861",
    "https://openalex.org/W3120756248"
  ],
  "abstract": "Masahiro Kaneko, Aizhan Imankulova, Danushka Bollegala, Naoaki Okazaki. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2740 - 2750\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nGender Bias in Masked Language Models for Multiple Languages\nMasahiro Kaneko1 Aizhan Imankulova2 Danushka Bollegala3,4∗ Naoaki Okazaki1\n1Tokyo Institute of Technology 2CogSmart Co., Ltd.\n3University of Liverpool 4Amazon\nmasahiro.kaneko@nlp.c.titech.ac.jp\naizhan.imankulova@cogsmart-global.com\ndanushka@liverpool.ac.uk okazaki@c.titech.ac.jp\nAbstract\nMasked Language Models (MLMs) pre-trained\nby predicting masked tokens on large corpora\nhave been used successfully in natural language\nprocessing tasks for a variety of languages. Un-\nfortunately, it was reported that MLMs also\nlearn discriminative biases regarding attributes\nsuch as gender and race. Because most stud-\nies have focused on MLMs in English, the\nbias of MLMs in other languages has rarely\nbeen investigated. Manual annotation of eval-\nuation data for languages other than English\nhas been challenging due to the cost and diffi-\nculty in recruiting annotators. Moreover, the\nexisting bias evaluation methods require the\nstereotypical sentence pairs consisting of the\nsame context with attribute words (e.g. He/She\nis a nurse). We propose Multilingual Bias Eval-\nuation (MBE) score, to evaluate bias in various\nlanguages using only English attribute word\nlists and parallel corpora between the target lan-\nguage and English without requiring manually\nannotated data. We evaluated MLMs in eight\nlanguages using the MBE and confirmed that\ngender-related biases are encoded in MLMs\nfor all those languages. We manually created\ndatasets for gender bias in Japanese and Rus-\nsian to evaluate the validity of the MBE. The\nresults show that the bias scores reported by\nthe MBE significantly correlates with that com-\nputed from the above manually created datasets\nand the existing English datasets for gender\nbias.\n1 Introduction\nMasked Language Models (MLMs) (Devlin et al.,\n2019), which are pre-trained on large corpora, have\nbeen used successfully in natural language process-\ning tasks for various languages (Conneau and Lam-\nple, 2019; Martin et al., 2020; Conneau et al., 2020).\nUnfortunately, it has been reported that MLMs also\n∗Danushka Bollegala holds concurrent appointments as\na Professor at University of Liverpool and as an Amazon\nScholar. This paper describes work performed at the Univer-\nsity of Liverpool and is not associated with Amazon.\nlearn social biases regarding attributes such as gen-\nder, religion, and race (Kurita et al., 2019; Dev\net al., 2020; Kaneko and Bollegala, 2021a; Bender\net al., 2021). The bias in MLMs is evaluated by\nthe imbalance of the likelihood between pairs of\nsentences associated with an attribute that has a\ncommon context (e.g. He/She is a nurse). Nadeem\net al. (2021) masked the modified tokens (e.g. He,\nShe), and Nangia et al. (2020) masked the unmod-\nified tokens (e.g. is, a, nurse) one word at a time\nand calculated the likelihood from their predictions\nto evaluate the bias. Kaneko and Bollegala (2021c)\nevaluated the bias using the average of the likeli-\nhoods of all tokens without masking the MLM.\nDespite the numerous studies of social bias in\nMLMs covering English, social biases in MLMs\nfor other languages remain understudied (Lewis\nand Lupyan, 2020; Liang et al., 2020; Bartl et al.,\n2020; Zhao et al., 2020). To realise the diverse\nand inclusive social and cultural impact of AI, we\nbelieve it is important to establish tools for detect-\ning and mitigating unfair social biases in MLMs,\nnot only for English but for all languages. How-\never, the significant manual annotation effort, the\ncosts and difficulties in recruiting qualified anno-\ntators remain major challenges when creating bias\nevaluation benchmarks for target languages. For\nexample, existing bias evaluation benchmarks such\nas CrowS-Pairs (CP; Nangia et al., 2020) and Stere-\noSet ( SS; Nadeem et al., 2021) require human-\nwritten sentences (or pairs of sentences) eliciting\ndifferent types of social biases expressed in the tar-\nget language. However, scaling up this approach\nto all languages is challenging because recruiting\na sufficiently large pool of annotators to cover the\ndifferent types of social biases in those languages\nis difficult. Because of the above-mentioned chal-\nlenges, bias evaluation datasets and studies outside\nEnglish remain under-developed.\nTo address this problem, we propose Multilin-\n2740\nFigure 1: The bias evaluation method using a parallel corpus between English and the target language and an\nEnglish female and male words list. Matrix values in Step 2 are the similarities between male and female sentences.\ngual Bias Evaluation (MBE) score1, a social bias\nevaluation method that can be used to evaluate bi-\nases in pre-trained MLMs for a target language\nwithout requiring significant manual annotations\nfor that language. MBE can perform equivalent\nbias evaluation using only existing parallel corpora\nand lists of English masculine (e.g. he, his, father,\nson etc.) and feminine (e.g. she, her, mother, daugh-\nter, etc.) words, without requiring any manually\nannotated sentences for social biases in the target\nlanguage. Although MBE require parallel corpora,\nsuch sources already exist for numerous language\npairs2 or can be automatically mined with less ef-\nfort compared to annotating bias evaluation data\n(Artetxe and Schwenk, 2019b,a). As a concrete ex-\nample, we evaluate the proposed method for gender\nbias, which exists in many languages. Extending\nthe proposed method to other types of social biases\nbeyond gender biases is deferred to future work. As\nshown in Figure 1, MBE first (shown in Step 1) ex-\ntracts target language sentences containing female\nwords (female sentences) and sentences containing\nmale words (male sentences) from a parallel corpus\nbetween English and the target language using a\nlist of gender words in English. Second, (shown\nin Step 2) MBE calculates the likelihoods for each\nof the extracted female and male sentences in the\ntarget language using the given MLM under evalua-\ntion. Finally, (shown in Step 3) MBE compares the\nlikelihoods between each female sentence and all\nmale sentences considering all pairwise combina-\ntions, and increment a count by 1 if the likelihood\nof the male sentence is greater than that of the fe-\nmale sentence, and 0 otherwise.\nAs for Step 1, we do not require any knowledge\n1Our code and dataset: https://github.com/\nkanekomasahiro/bias_eval_in_multiple_mlm\n2https://www.clarin.eu/\nresource-families/parallel-corpora\nabout the target language or manual annotations\nbecause we use only the existing English attribute\nword lists and parallel corpora between English\nand the target language. This is attractive from a\ndata availability point of view, which makes our\nproposed method easily extendable to different lan-\nguages. Kaneko and Bollegala (2021c) found that\nthe frequency of the words associated with the male\ngender to be significantly higher than that for the fe-\nmale gender in the data used to train MLMs. They\nshowed that these frequency-related biases are en-\ncoded into MLMs, and independently of whether a\nsentence contains a stereotypical or antistereotyp-\nical context, an MLM that is biased towards the\nmale gender, on average, assigns higher likelihood\nscores to sentences that contain masculine words\nthan feminine words3. Inspired by this finding, in\nStep 2, we calculate the likelihood scores assigned\nby an MLM under evaluation to sentences that con-\ntain male and female related words in different\ncontexts (e.g. He is a baseball playerand She is a\nnurse). Step 3 of our proposed method performs\nthe computation of the bias score considering the\nsimilarity between the contexts in sentence pairs\nthat contain male and female related words. The\nmore similar the sentence pairs are, the more sim-\nilar the estimates would be compared to the bias\nevaluation measures that require identical contexts.\nTherefore, we weight the bias score estimates by\nthe similarity of the sentence pairs using the sen-\ntence representations obtained from the MLM un-\nder evaluation. We ignore dissimilar sentence pairs\nand compute the bias score from the similar sen-\n3Nangia et al. (2020) define stereotypical sentence to be a\ncase where an advantaged group (in the case of gender bias\nmale is considered as the advantaged group, whereas female is\nthe disadvantaged group) is associated with a pleasant attribute\n(e.g. The man is intelligent) or a disadvantaged group is\nassociated with an unpleasant attribute (e.g. The woman is\ncareless).\n2741\ntence pairs, which is defined as the percentage of\nsentence pairs where the sentence containing the\nmasculine words is assigned a higher likelihood\nthan the sentence containing the feminine words.\nBias in MLM is thought to depend on both the\nMLM and the evaluation data, so in this paper, we\nare investigating both using two corpora for English\nMLMs. Using the proposed method, we evalu-\nated gender bias in MLMs in eight other languages:\nGerman, Japanese, Arabic, Spanish, Portuguese,\nRussian, Indonesian, and Chinese. Prior work in-\nvestigating social biases in MLMs for English have\nshown that different types and levels of biases are\nshown by different MLMs even for the same lan-\nguage (Kaneko and Bollegala, 2021a,c; Dev et al.,\n2020). We defer covering different MLMs across\nmultiple languages to future work and focus on es-\ntablishing MBE as an evaluation measure that can\nbe used for such a study.\nOur evaluations show that all MLMs learn\ngender-related biases in all languages studied.\nTo further validate MBE, we conduct a meta-\nevaluation where we use an existing manually an-\nnotated English bias evaluation dataset and two\nadditional datasets we annotate in this work cov-\nering gender-related biases in Japanese and Rus-\nsian languages. The bias scores computed using\nMBE show significantly high correlations with hu-\nman bias annotations on both datasets (CP and SS),\nshowing its validity for multiple languages as a\ngender bias evaluation method. Furthermore, we\nshow that MBE is superior to methods using ma-\nchine translations to evaluate bias in non-English\nlanguages. We also show that bias evaluation meth-\nods based on templates and word lists significantly\noverestimate the bias in MLMs due to the unnatu-\nralness of the created templates. Our analyses on\nthe effects of English names on gender information\nand preservation of gender information in parallel\ncorpora suggest that bias can be evaluated reason-\nably even with some loss of gender information.\n2 Related Work\nIn the study of bias in English MLMs, May et al.\n(2019) and Kurita et al. (2019) use a pair of ar-\ntificial sentences created using manually written\ntemplates. However, template-based evaluation is\nproblematic because it uses an artificial context\nthat does not reflect the natural usage and distribu-\ntion of words in the target language. To solve this\nproblem, Nadeem et al. (2021) and Nangia et al.\n(2020) manually created bias evaluation datasets,\nSS and CP, respectively, with stereotypical and\nantistereotypical sentence-pairs with identical con-\ntexts, except the attribute words. However, recent\nwork has pointed out various issues in CP and SS\ndatasets and has argued that they may not provide\neffective measurements of stereotyping (Blodgett\net al., 2021). In this study, (social) bias is defined as\nthe tendency towards outputting sentences about a\nparticular advantageous or disadvantageous group,\nsuch as males or females, given the same context by\nan MLM. However, these benchmarks are currently\nthe most commonly used benchmarks for bias eval-\nuation in MLMs, so we also use them in this work.\nWe note that MBE is independent of any bias eval-\nuation benchmark datasets. Our focus in this paper\nis on evaluating gender bias in multiple languages\nand not on comparing or proposing novel debiasing\nmethods. However, for the completion of the dis-\ncussion, we note that methods for debiasing MLMs\nusing sentence vectors from MLMs (Bommasani\net al., 2020) and lists of English male and female\nwords has been studied (Sedoc and Ungar, 2019;\nKaneko and Bollegala, 2021a; Dev et al., 2020;\nZhou et al., 2022).\nIn prior work on MLMs, social biases for lan-\nguages other than English have rarely been inves-\ntigated. Ahn and Oh (2021) investigated ethnic\nbias in monolingual MLM in six languages by ex-\ntending the templates to other languages using ma-\nchine translation. The biases of MLMs have been\nevaluated using templates for English and Chinese\n(Liang et al., 2020) and for English and German\n(Bartl et al., 2020). Zhao et al. (2020) investigated\nthe gender bias of a classifier that predicts the oc-\ncupation from resumes using multilingual word\nembeddings and multilingual MLM embedding in\nSpanish, German and French. They evaluated bias\nby using machine translation on the English data,\nwhen an MLM is used to create feature represen-\ntations in a specific task. However, this setting is\ndifferent from that of our study, where we evaluate\nthe bias of MLMs independently of a specific task.\nMoreover, the above studies do not discuss or pro-\npose methods on how to create evaluation data that\ncan be applied to many languages.\nFollowing the pioneering work by Bolukbasi\net al. (2016) that proposed a bias evaluation and de-\nbiasing methods, various studies have investigated\nsocial biases in English (Caliskan et al., 2017; Zhao\net al., 2018; Kaneko and Bollegala, 2019, 2021b;\n2742\nDev and Phillips, 2019). Unlike the contextual\nword embeddings produced by MLMs, evaluating\nsocial biases in static word embeddings is relatively\nless complicated because it can often be done using\nword lists without requiring annotated sentences.\nIn static word embeddings, bias has been investi-\ngated in various languages besides English due to\nthis ease of annotating evaluation data. Lauscher\nand Glavaš (2019) translated the English word lists\ninto six languages and evaluated the bias of the\nword embeddings. Zhou et al. (2019) proposed an\nevaluation metric for languages that require gen-\nder morphological agreement, such as in Spanish\nand French. Friedman et al. (2019) quantified the\ngender bias of word embeddings to understand cul-\ntural contexts with large-scale data, and used it to\ncharacterize the statistical gender gap in education,\npolitics, economics, and health in US states and\nseveral countries. Bansal et al. (2021) proposed\na debiasing method by constructing the same bias\nspace for multiple languages, and adapted it to\nthree Indian languages. Other bias studies have\nbeen conducted for specific languages (Takeshita\net al., 2020; Pujari et al., 2019; Sahlgren and Ols-\nson, 2019; Chávez Mulsa and Spanakis, 2020), but\nthey are not easily transferable to novel languages.\n3 Bias Evaluation for Multiple Languages\nOur proposed MBE score evaluates the gender bias\nof the target language under evaluation in three\nsteps (see Figure 1). In Step 1, we first define the\nset of English sentences E and the set of target lan-\nguage sentences T of the parallel corpus, where N\nis the data size, and (ei,ti) is a parallel sentence\npair. Let Vf (e.g. she, woman, female) be the list of\nfemale words and Vm (e.g. he, man, male) be the\nlist of male words in English. We then extract sen-\ntences that contain a female or a male word from E.\nSentences that contain both male and female words\nare excluded. Let us denote the set of sentences\nextracted for a female or a male word wby Φ(w).\nLet Ef = ⋃\nw∈Vf Φ(w) and Em = ⋃\nw∈Vm Φ(w)\nbe the sets of sentences containing respectively all\nof the male and female words. The set of sentences\nin the target language of the source sentences in-\ncluded in Ef and Em is denoted by Tf and Tm,\nrespectively. It is assumed that gender information\nis retained in the parallel corpus, and whether this\nis actually the case is verified later.\nIn Step 2, we compute the likelihood for the\nfull sentences in Tf and Tm. Let us consider a\ntarget sentence T = w1,w2,...,w |T|, containing\nlength |T| sequence of tokens wi. We calculate\nthe likelihood with All Unmasked Likelihood with\nAttention weights ( AULA; Kaneko and Bollegala,\n2021c) which evaluates the bias by considering\nthe weight of MLM attention as the importance of\ntokens. Given an MLM with pre-trained parameters\nθ, which we must evaluate it for its gender bias, let\nus denote the probability PMLM(wi|T; θ) assigned\nby the MLM to a token wi conditioned on all the\ntokens of T. AULA predicts all of the tokens in T\nusing the attention weights to evaluate social biases\nconsidering the relative importance of words in a\nsentence, which is given by (1).\nA(T) := 1\n|T|\n|T|∑\ni=1\nαi log PMLM(wi|T; θ) (1)\nHere, αi is the average of all multi-head attentions\nassociated with wi.\nIn Step 3, by comparing the likelihoods of fe-\nmale and male sentences returned by AULA, we\ncalculate the bias score as the weighted average of\nthe similarities of contexts using the sentence repre-\nsentations produced by the MLM under evaluation.\nSpecifically, We use the percentage of male (Tm)\nsentences (e.g. He is a baseball player) preferred\nby the MLM over female (Tf ) ones (e.g. She is a\nnurse) to define the corresponding multilingual bias\nevaluation measure (MBE bias score) as follows:\n100 ×\n∑\nTm∈Tm\n∑\nTf ∈Tf\nC(Tm, Tf )I(A(Tm) > A(Tf ))\n∑\nTm∈Tm\n∑\nTf ∈Tf\nC(Tm, Tf )\n(2)\nHere, I is the indicator function, which returns 1 if\nits argument is True and 0 otherwise. C(Tm,Tf )\nuses the average of the last layer in MLM for all to-\nkens except special tokens to compute the sentence\nembeddings of Tm and Tf respectively and com-\nputes the cosine similarity of these embeddings.\nAccording to this evaluation measure, values close\nto 50 indicate that the MLM under evaluation is\nneither females nor males biased, hence, it can be\nregarded as unbiased. On the other hand, values be-\nlow 50 indicate a bias towards the male group and\nabove 50 towards the female group. We report a\nstatistically significant difference comparing to the\nmodel with randomly assigned results of the indi-\ncator function I in Equation 2 with the McNemar’s\ntest (p< 0.05). For each sentence, the presence or\nabsence of bias is predicted by two methods, MLM\n2743\nLang TED News\nGerman 4.7K 2.1K\nJapanese 6.2K 1.8K\nArabic 7.0K 1.7K\nSpanish 7.1K 17.3K\nPortuguese 5.7K 2.2K\nRussian 6.7K 3.9K\nIndonesian 2.9K 0.5K\nChinese 6.8K 3.4K\nTable 1: The total number of male and female sentences\nextracted from the parallel data for each language.\nand Random. The McNemar’s test was used by\nclassifying into four categories: only MLM was\nbiased, only random was biased, both were unbi-\nased, and both were biased. We use the statistically\nsignificant difference to determine if there is a bias.\n4 Gender Bias in Masked Language\nModels\nWe use two parallel corpora, the TED2020 v1 cor-\npus in the spoken language domain (TED)4 and the\nGlobalV oices corpus in the news domain (News)5.\nTable 1 shows the total number of extracted male\nand female sentences for each language. Except\nfor Spanish, the News corpus is smaller than the\nTED corpus for all languages. In particular, the\nIndonesian news corpus is an extremely low re-\nsource. For the list of female and male words\nin English, we use the list created by Bolukbasi\net al. (2016)6 in addition to the female and male\nnames in CP (Nangia et al., 2020). The extracted\nmale and female sentences were downsampled to\ncreate sets of an equal number of sentences. We\nexperimented on the GeForce RTX 2080 Ti us-\ning the transformers7 implementation with\ndefault settings (Wolf et al., 2020). All evaluations\nare completed within 10 minutes.\nWe used Masked Language Models (MLMs) in\neight languages for our experiments: Japanese 8,\nGerman9 (Chan et al., 2020), Arabic 10 (Antoun\n4https://opus.nlpl.eu/TED2020.php\n5https://opus.nlpl.eu/\nGlobalVoices-v2017q3.php\n6https://github.com/uclanlp/gn_glove/\ntree/master/wordlist\n7https://github.com/huggingface/\ntransformers\n8https://huggingface.co/cl-tohoku/\nbert-base-japanese-whole-word-masking\n9https://huggingface.co/deepset/\ngbert-base\n10https://huggingface.co/aubmindlab/\nbert-base-arabertv01\nLang MBE(TED) MBE(News)\nGerman 54.69 ‡ 55.12‡\nJapanese 54.52 ‡ 50.99\nArabic 55.72 ‡ 54.39‡\nSpanish 51.44 ‡ 51.69‡\nPortuguese 53.07 ‡ 54.99‡\nRussian 54.59 ‡ 51.00\nIndonesian 52.38 ‡ 50.52\nChinese 52.86 ‡ 51.80‡\nTable 2: The bias score of MLMs using MBE in dif-\nferent languages. ‡ indicates statistically significant\ndifference at p< 0.05.\net al., 2020), Spanish11 (Cañete et al., 2020), Por-\ntuguese12 (Souza et al., 2020), Russian13, Indone-\nsian14 and Chinese15 (Cui et al., 2020).\nTable 2 shows the bias scores of the proposed\nMBE method for the TED and News corpora for\nthe MLMs considered. Here, the significant dif-\nference is evaluated against the MBE score of a\nrandomly assigned indicator function. Overall, we\nsee gender-related biases are reported in all cases.\nIn particular, significant biases are shown in the\nNews corpus for all languages except Japanese,\nRussian and Indonesian. Moreover, the different\nlevels of biases reported for Russian and Japanese\nbetween TED and News corpora indicate that bias\nevaluations are affected not only by the MLMs but\nalso the corpora used. It is known that the bias ten-\ndency of MLMs changes depending on the training\ndata (Babaeianjelodar et al., 2020), and similarly,\nthe bias evaluation of MLMs is affected by the eval-\nuation corpus. Because MBE can evaluate bias in\nvarious domains as long as there are parallel cor-\npora. It can also capture corpus-dependent biases,\nunlike existing methods requiring manually created\ndomain-specific sentence pairs.\n5 Meta-Evaluation\nWe perform a meta-evaluation to validate MBE\nscores against human bias ratings. In §5.1 we mea-\nsure the correlation between MBE scores and ex-\nisting measures on CP and SS, which are manu-\n11https://huggingface.co/dccuchile/\nbert-base-spanish-wwm-uncased\n12https://huggingface.co/neuralmind/\nbert-base-portuguese-cased\n13https://huggingface.co/blinoff/\nroberta-base-russian-v0\n14https://huggingface.co/cahya/\nbert-base-indonesian-522M\n15https://huggingface.co/hfl/\nchinese-bert-wwm-ext\n2744\nShf MBE\nCP\nSpearman 0.06 0.41\nPearson 0.05 0.63†\nDirection 0.54 0.72\nDiff 4.06 2.36\nSS\nSpearman 0.21 0.41\nPearson 0.04 0.62†\nDirection 0.54 0.72\nDiff 6.66 5.04\nTable 3: Bias scores computed using Shf and the MBE\nmethods for English MLMs in CP and SS. Correla-\ntion between the original and proposed evaluation rep-\nresented by Spearman and Pearson correlation coeffi-\ncients. † indicates significant correlation at p <0.05.\nDirection is the percentage of agreement for direction\nof the bias score between original and proposed evalu-\nations. Diff is the mean of the difference between the\nbias scores of the original and proposed methods.\nally annotated bias evaluation benchmarks for En-\nglish. In §5.2 to compare the evaluation methods\nin the target languages using MBE and manually\nannotated data, we manually translate the CP into\nthe Japanese and Russian, which demonstrate high\ncorpus-specific biases according to Table 2.\n5.1 Gender Bias Evaluation Using Manually\nAnnotated Data in English\nTo validate MBE scores using human bias ratings,\nwe use CP and SS datasets forEnglish. As baseline\nmethod we use Shf, which shuffles the sets of male\nand female sentences and randomly pair sentences\nfrom this set. Shf is used to show the usefulness\nof comparing the likelihoods of male and female\nsets. In the existing evaluation method using man-\nually annotated sentence pairs, the bias score is\ncalculated for stereotypical Ss (e.g. He is a doc-\ntor) and anti-stereotypical Sa (e.g. She is a doctor)\nsentences with identical contexts as follows:\n100\nN\n∑\nSs,Sa\nI(A(Ss) >A(Sa)) (3)\nwhere N is the total number of sentences. We use\nthis bias score as an upper bound score to com-\npare against it the results for Shf and MBE using\nthe rank correlations (Spearman and Pearson), the\nagreement of the direction of bias between female\nand male directions ( Direction), where the bias\nscores above 50 indicate a bias towards the male\ndirection and that below 50 towards the female di-\nrection, and the difference of the bias scores (Diff )\nfrom the results of the method using manual an-\nnotation. In the proposed method and Shf, for the\ngender bias data of CP and SS, we extract sen-\ntences containing male and female words for each\nsentence, instead of sentence pairs, and use them\nfor evaluation using Equation 3.\nAs English MLMs, we use BERT16, multilingual\nBERT17 (Devlin et al., 2019), RoBERTa 18 (Liu\net al., 2019), ALBERT 19 (Lan et al., 2019), Dis-\ntilBERT20, DistilRoBERTa21 (Sanh et al., 2019),\nConvBERT22 (Jiang et al., 2020), XLM 23 (Con-\nneau and Lample, 2019), and Deberta24 (He et al.,\n2020). Since BERT and RoBERTa each use two\nmodels of different sizes, we use a total of 11 mod-\nels. We report the averaged results over the above\n11 models.\nTable 3 shows that MBE has high performance\nin all evaluations. Performance of Shf highlights\nthe importance of comparing male against female\nsentences in sentence pairs.\n5.2 Gender Bias Evaluation Using Manually\nAnnotated Data in Japanese and Russian\nTo validate MBE scores, which does not require\nevaluation data with identical context, nor manual\ncreation of evaluation data in the target languages\nother than English, we use the following methods:\nHT: Native speakers manually translated all 262\nsentence pairs in CP into Japanese and Russian\nand apply Equation 3. This human translated (HT)\nbaseline can be seen as an upper bound for bias\nevaluation compared to MBE, which does not re-\nquire translated examples. Lower difference from\nthese bias scores in this human-translated (HT)\nmethod would indicate a more reliable bias eval-\nuation measure. Note that, it is not appropriate\nto compare the bias score calculated using the En-\nglish MLMs with the bias score calculated using\nthe Japanese MLMs because we are evaluating dif-\n16https://huggingface.co/\nbert-base-cased and https://huggingface.\nco/bert-large-uncased\n17https://huggingface.co/\nbert-base-multilingual-uncased\n18https://huggingface.co/roberta-base\nand https://huggingface.co/roberta-large\n19https://huggingface.co/albert-base-v2\n20https://huggingface.co/\ndistilbert-base-cased\n21https://huggingface.co/\ndistilroberta-base\n22https://huggingface.co/YituTech/\nconv-bert-medium-small\n23https://huggingface.co/\nxlm-mlm-100-1280\n24https://huggingface.co/microsoft/\ndeberta-xlarge-v2\n2745\nMLM Bias score Diff\nHT(Japanese)\nbase-subword 52.67 ‡ -\nlarge-subword 56.87 ‡ -\nbase-char 48.47 ‡ -\nlarge-char 55.73 ‡ -\nMT(Japanese)\nbase-subword 49.24 -3.43\nlarge-subword 52.67 ‡ -4.20\nbase-char 54.20 ‡ 5.73\nlarge-char 45.80 ‡ 9.93\nMBE(Japanese)\nbase-subword 54.89 ‡ 2.22\nlarge-subword 55.85 ‡ -1.02\nbase-char 52.69 ‡ 4.22\nlarge-char 50.60 -5.13\nTmp(Japanese)\nbase-subword 88.31 ‡ 35.64\nlarge-subword 82.13 ‡ 25.26\nbase-char 64.63 ‡ 16.16\nlarge-char 45.40 ‡ -10.33\nTable 4: The CP bias scores for manually translated CP\nto Japanese and bias scores for machine translated CP\nand the proposed method MBE. Diff shows the differ-\nence between MT, MBE and Tmp bias scores and HT\nbias scores, respectively. ‡ indicates statistically signifi-\ncant difference at p< 0.05.\nferent models. Therefore, we calculate the bias\nscore in Equation 3 using the data translated into\nJapanese and Russian.\nMBE: Here, we let MBE(Japanese) and\nMBE(Russian) be the MBE scores computed\nusing the Equation 2 and parallel data created\nabove by manually translating original (English)\nCP dataset into Japanese and Russian for Step 1,\nrespectively.\nMT: As an alternative to costly manual transla-\ntions, we use Google Machine Translation method\n(MT)25 to translate sentence pairs in CP sharing\nidentical contexts into each target language and\napply Equation 3.\nTmp: Although it requires some knowledge about\nthe target language, one can create templates in the\ntarget language for both genders such as “[Gen-\nder]は[Occupation]です” ([Gender] is a/an [Oc-\ncupation]) in Japanese, and fill in male and female\nword pairs, and occupation words as in “彼/彼女は\n医者です” (He/She is a doctor) to create an equal\nnumber of sentences as the evaluation data for\nEquation 3. In the template-based method (Tmp),\nfive word pairs were used for Japanese and Rus-\nsian following prior work by Kurita et al. (2019)26.\n25In July 2021, we translated CP data using google spread-\nsheet function: https://support.google.com/\ndocs/answer/3093331?hl\n26Japanese: 彼:彼女, 男:女, 父:母, 兄:姉, 叔父:叔母. Rus-\nMLM Bias score Diff\nHT(Russian) wiki&nwes 46.95 ‡ -\nsubtitle&sns 48.85 ‡ -\nMT(Russian) wiki&nwes 49.62 2.67\nsubtitle&sns 50.38 1.53\nMBE(Russian) wiki&nwes 46.05 ‡ -0.90\nsubtitle&sns 48.82 ‡ -0.03\nTmp(Russian) wiki&nwes 34.87 ‡ -12.1\nsubtitle&sns 63.51 ‡ 14.7\nTable 5: The CP bias scores for manually translated CP\nto Russian and bias scores for machine translated CP and\nthe proposed method MBE. Diff shows the difference\nbetween MT, MBE and Tmp bias scores and HT bias\nscores, respectively. ‡ indicates statistically significant\ndifference at p< 0.05.\nThe templates were “[Gender] は[Occupation]で\nす。” and “[Gender]は[Occupation]に興味があ\nる。” in Japanese and “[Gender] - [Occupation].”\nand “[Gender] - [Occupation] поспециа\nльности.” were used for Russian. We ex-\ntracted respectively 644 and 154 occupation words\nfor Japanese and Russian from Wikipedia27. Fol-\nlowing prior work by Kurita et al. (2019), we gener-\nated respectively 6400 and 1500 template sentences\nfor Japanese and Russian, and evaluated them using\nsentence pairs with identical contexts.\nFor Japanese MLMs, we evaluate four Japanese\nBERT models (base-subword28, large-subword29,\nbase-char30, large-char31), subword-based and\ncharacter-based, with base and large sizes. For Rus-\nsian, we use two MLMs – one trained on Wikipedia\nand news data ( wiki&news)32 and the other on\nOpenSubtitles (Lison and Tiedemann, 2016) and\nSNS data (Shavrina and Shapovalova, 2017). For\nJapanese and Russian, we use the difference of the\nbias scores instead of the correlation coefficients\nsian: Он:Она, Мужчина:Женщина, П\nапа:Мама, Брат:Сестра, Дядя:\nТетя (English: He:She, Man:Woman, Father:Mather,\nBrother:Sister, Uncle:Aunt)\n27https://ja.wikipedia.org/wiki/職業一覧\nand https://ru.wikipedia.org/wiki/Кате\nгория:Профессии\n28https://huggingface.co/cl-tohoku/\nbert-base-japanese-v2\n29https://huggingface.co/cl-tohoku/\nbert-large-japanese\n30https://huggingface.co/cl-tohoku/\nbert-base-japanese-char-v2\n31https://huggingface.co/cl-tohoku/\nbert-large-japanese-char\n32https://huggingface.co/DeepPavlov/\nrubert-base-cased\n2746\nMLM Bias score Diff\nHTname(Japanese)\nbase-subword 52.29 ‡ -0.38\nlarge-subword 54.58 ‡ -2.29\nbase-char 48.47 ‡ 0.00\nlarge-char 53.44 ‡ -2.29\nHTname(Russian) wiki&news 47.33 ‡ 0.38\nsubtitle&sns 48.09 ‡ -0.76\nTable 6: The difference between the bias score for the\noriginal data and the bias score for the CP data trans-\nlated into Japanese and Russian with the names of peo-\nple replaced by Japanese and Russian, respectively. ‡\nindicates statistically significant difference at p< 0.05.\nwith HT because the number of publicly available\npre-trained MLMs is smaller than that of English.\nTables 4 and 5 show the bias scores of HT, MT,\nTmp and MBE and their differences measured\nagainst HT for Japanese and Russian MLMs, re-\nspectively. We see that the difference between the\nbias scores of HT and MBE are smaller than that\nfor MT, indicating that MBE closely approximates\nthe human bias ratings in HT than other alterna-\ntives. Moreover, we see that the direction of bias is\nreversed for base-char, large-char, and subtitle&sns\ncompared to HT. Note that we can not directly com-\npare Tmp with other methods due to the difference\nin evaluation data. However, as one of the previous\nbias evaluation methods, Tmp overestimates the\nbiases of MLMs, especially for Japanese subwords.\nThis is because simple artificial templates often\nover-emphasize gender biases compared to natu-\nral sentences, Interestingly, MBE is more accurate\nthan MT when evaluating gender biases. Further\ninvestigations revealed that MT model itself could\nproduce gender-biased translations, thereby adding\nnoise to the translated sentences.\n6 Bias in Personal Names\nOne of the most significant differences in the fre-\nquency of words used in each language that affects\ngender bias is the names of people. In bias evalu-\nation, male and female names are used to identify\nthe gender (Caliskan et al., 2017; Romanov et al.,\n2019). However, when names are transliterated\nfrom English to the target language, those translit-\nerated names might be infrequent in the target lan-\nguage and might not be gender representative. To\nstudy the effect of this issue on gender bias evalua-\ntion, we conduct the following experiment. First,\nfor the Japanese and Russian target languages, we\nreplace the transliterated English names in the CP\n0\n25\n50\n75\n100\nGerman Portuguese Japanese Russian\nFemale Male\nFigure 2: Percentage of manually translated sentences\npreserving gender information from English News data.\ndata with native Japanese and Russian names of\nthe same gender. Next, we compare the bias scores\nwith those before the replacement in Tables 4 and\n5. We extracted the top 10 most popular names\namong Japanese33 and Russians34 for both genders,\nand randomly substituted them with the translit-\nerated English names. For example, we rewrite\n“シェリーはナースです” → “美咲はナースで\nす” (“Shelly is a nurse” → “Misaki is a nurse”).\nTable 6 shows the MBE score for\nJapanese (HT name(Japanese)), and Russian\n(HTname(Russian)) after the name replacement and\nthe corresponding differences w.r.t. original bias\nscores shown in Tables 4 and 5). We can see that\nthe bias scores of the Japanese base models and all\nthe Russian models are almost the same compared\nto respective values in Tables 4 and 5. The large\nmodels for Japanese differ by about -2.29, which\nis lower than the baseline in the table. Moreover,\nthe direction of the bias has not changed in both\nlanguages compared to respective directions in\nTables 4 and 5. These results suggest that the bias\ncan be evaluated reasonably even when English\nnames are transliterated into a target language.\n7 Preserving Gender in Parallel Corpora\nStep 1 of the proposed method requires that gender\ninformation in English (source) sentence matches\nthat with the target translation in the parallel data.\nTo test for this, we examine the proportion of sen-\ntences in which the corresponding translated words\nof English “she” and “he” appear to determine\nwhether female or male gender information is re-\ntained. We use the News corpus and select Japanese\n33https://www3.nhk.or.jp/news/special/\nsakusakukeizai/articles/20181127.html\n34https://znachenie-tajna-imeni.\nru/top-100-zhenskih-imen/ and\nhttps://znachenie-tajna-imeni.ru/\ntop-100-muzhskih-imen/\n2747\nand Russian, which had no bias, and German and\nPortuguese, which had significant biases (Table 2).\nFigure 2 shows the percentage of sentences\nwhere gender was retained for male and female\nsentences in the target languages35. For German,\nPortuguese, and Russian, gender is retained in more\nthan 80% of the sentences. This suggests that when\nthe the percentage of gender-preserved sentences is\nlarge, it does not affect the MBE score. In Japanese,\ngender information is retained in only about 60%\nof sentences, which is much lower than in other\nlanguages. This may be because Japanese is a null-\nsubject language that allows independent clauses\nto omit explicit subjects. In fact, in some cases,\ngender words were omitted in the parallel corpus,\nfor example “He owns a grocery store and runs a\nmotorcycle rental business.” was translated to “自\n分の食料品店を持ち、レンタルバイクビジ\nネスも営んでいる。(Owns a grocery store and\nruns a rental motorcycle business.)”. Contrarily,\nfrom the results in Table 4, MBE(Japanese) can de-\ntect the bias better than other methods. The reason\nmay be that even if the gender words are omitted\nif the context is composed of words that often co-\noccur with male and female words, it is possible\nthat it complements the gender information. In\nfact, Bolukbasi et al. (2016) show that words that\nco-occur with male and female words retain gen-\nder information. The results also show that gender\npreservation is not heavily biased in either the male\nor female direction, based on the small difference\nbetween percentages for male and female sentences\nfor each language. This suggests that the bias in the\npreservation of gender information may not affect\nthe evaluation of the proposed method.\n8 Conclusion\nIn this paper, we showed that a bias evaluation data\nand evaluation of MLMs for discriminatory bias\ncan be systematically created as long as there is a\nparallel corpus of English and the target language\nand a list of female and male words in English. Our\nmeta-evaluation proved that the proposed multilin-\ngual bias evaluation method could perform correct\nevaluation comparing against method using manu-\nally created data, at least for Russian, Japanese, and\nEnglish. The experimental results show that gender\nbias exists in all eight languages of our experiments.\nWe also showed that the proposed method is supe-\n35This is a conservative underestimate of gender preserva-\ntion, because gender words can be translated by paraphrasing.\nrior to the methods that use machine translation to\ntranslate the English bias evaluation data into the\ntarget language and the methods that use templates\nand word lists.\nAcknowledgements\nThis paper is based on results obtained from a\nproject, JPNP18002, commissioned by the New\nEnergy and Industrial Technology Development\nOrganization (NEDO).\nReferences\nJaimeen Ahn and Alice Oh. 2021. Mitigating language-\ndependent ethnic bias in BERT. InProceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 533–549, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020.\nAraBERT: Transformer-based model for Arabic lan-\nguage understanding. In OSACT4, pages 9–15. Euro-\npean Language Resource Association.\nMikel Artetxe and Holger Schwenk. 2019a. Margin-\nbased parallel corpus mining with multilingual sen-\ntence embeddings. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 3197–3203, Florence, Italy. Asso-\nciation for Computational Linguistics.\nMikel Artetxe and Holger Schwenk. 2019b. Massively\nMultilingual Sentence Embeddings for Zero-Shot\nCross-Lingual Transfer and Beyond. Transactions of\nthe Association for Computational Linguistics, 7:597–\n610.\nMarzieh Babaeianjelodar, Stephen Lorenz, Josh Gordon,\nJeanna Matthews, and Evan Freitag. 2020. Quanti-\nfying gender bias in different corpora. In WWW,\nWWW ’20, page 752–759. Association for Comput-\ning Machinery.\nSrijan Bansal, Vishal Garimella, Ayush Suhane, and\nAnimesh Mukherjee. 2021. Debiasing multilingual\nword embeddings: A case study of three indian lan-\nguages. In Proceedings of the 32nd ACM Conference\non Hypertext and Social Media, pages 27–34.\nMarion Bartl, Malvina Nissim, and Albert Gatt. 2020.\nUnmasking contextual stereotypes: Measuring and\nmitigating BERT’s gender bias. In GeBNLP, pages\n1–16. Association for Computational Linguistics.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In FAccT, FAccT ’21, page 610–623.\nAssociation for Computing Machinery.\n2748\nSu Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu,\nRobert Sim, and Hanna Wallach. 2021. Stereotyp-\ning Norwegian salmon: An inventory of pitfalls in\nfairness benchmark datasets. In ACL-IJCNLP, pages\n1004–1015. Association for Computational Linguis-\ntics.\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou,\nVenkatesh Saligrama, and Adam T Kalai. 2016. Man\nis to computer programmer as woman is to home-\nmaker? debiasing word embeddings. In NeurIPS,\nvolume 29. Curran Associates, Inc.\nRishi Bommasani, Kelly Davis, and Claire Cardie. 2020.\nInterpreting Pretrained Contextualized Representa-\ntions via Reductions to Static Embeddings. In COL-\nING, pages 4758–4781. Association for Computa-\ntional Linguistics.\nAylin Caliskan, Joanna J Bryson, and Arvind Narayanan.\n2017. Semantics derived automatically from lan-\nguage corpora contain human-like biases. Science,\n356(6334):183–186.\nJosé Cañete, Gabriel Chaperon, Rodrigo Fuentes, Jou-\nHui Ho, Hojin Kang, and Jorge Pérez. 2020. Span-\nish pre-trained bert model and evaluation data. In\nPML4DC.\nBranden Chan, Stefan Schweter, and Timo Möller. 2020.\nGerman’s next language model. In COLING, pages\n6788–6796. International Committee on Computa-\ntional Linguistics.\nRodrigo Alejandro Chávez Mulsa and Gerasimos\nSpanakis. 2020. Evaluating bias in Dutch word em-\nbeddings. In GeBNLP, pages 56–71. Association for\nComputational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In ACL,\npages 8440–8451. Association for Computational\nLinguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In NeurIPS, vol-\nume 32. Curran Associates, Inc.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin\nWang, and Guoping Hu. 2020. Revisiting pre-trained\nmodels for Chinese natural language processing. In\nEMNLP: Findings, pages 657–668. Association for\nComputational Linguistics.\nSunipa Dev, Tao Li, Jeff M Phillips, and Vivek Srikumar.\n2020. On measuring and mitigating biased inferences\nof word embeddings. In AAAI, volume 34, pages\n7659–7666.\nSunipa Dev and Jeff Phillips. 2019. Attenuating bias in\nword vectors. In AISTATS, pages 879–887. PMLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL, pages 4171–4186. Association\nfor Computational Linguistics.\nScott Friedman, Sonja Schmer-Galunder, Anthony\nChen, and Jeffrey Rye. 2019. Relating word embed-\nding gender biases to gender gaps: A cross-cultural\nanalysis. In GeBNLP, pages 18–24. Association for\nComputational Linguistics.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention. arXiv preprint\narXiv:2006.03654.\nZi-Hang Jiang, Weihao Yu, Daquan Zhou, Yunpeng\nChen, Jiashi Feng, and Shuicheng Yan. 2020. Con-\nvbert: Improving bert with span-based dynamic con-\nvolution. Advances in Neural Information Process-\ning Systems, 33.\nMasahiro Kaneko and Danushka Bollegala. 2019.\nGender-preserving debiasing for pre-trained word\nembeddings. In COLING, pages 1641–1650. Associ-\nation for Computational Linguistics.\nMasahiro Kaneko and Danushka Bollegala. 2021a. De-\nbiasing pre-trained contextualised embeddings. In\nEACL, pages 1256–1266. Association for Computa-\ntional Linguistics.\nMasahiro Kaneko and Danushka Bollegala. 2021b.\nDictionary-based debiasing of pre-trained word em-\nbeddings. In EACL, pages 212–223. Association for\nComputational Linguistics.\nMasahiro Kaneko and Danushka Bollegala. 2021c. Un-\nmasking the mask–evaluating social biases in masked\nlanguage models. arXiv preprint arXiv:2104.07496.\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,\nand Yulia Tsvetkov. 2019. Measuring bias in con-\ntextualized word representations. In GeBNLP, pages\n166–172. Association for Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nAnne Lauscher and Goran Glavaš. 2019. Are we consis-\ntently biased? multidimensional analysis of biases in\ndistributional word vectors. In *SEM, pages 85–91.\nAssociation for Computational Linguistics.\nMolly Lewis and Gary Lupyan. 2020. Gender stereo-\ntypes are reflected in the distributional structure of\n25 languages. Nature human behaviour, 4(10):1021–\n1028.\nSheng Liang, Philipp Dufter, and Hinrich Schütze. 2020.\nMonolingual and multilingual reduction of gender\nbias in contextualized representations. In COLING,\n2749\npages 5082–5093. International Committee on Com-\nputational Linguistics.\nPierre Lison and Jörg Tiedemann. 2016. OpenSub-\ntitles2016: Extracting large parallel corpora from\nmovie and TV subtitles. In LREC, pages 923–929.\nEuropean Language Resources Association (ELRA).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Suárez, Yoann Dupont, Laurent Romary, Éric\nde la Clergerie, Djamé Seddah, and Benoît Sagot.\n2020. CamemBERT: a tasty French language model.\nIn ACL, pages 7203–7219. Association for Computa-\ntional Linguistics.\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\nBowman, and Rachel Rudinger. 2019. On measuring\nsocial biases in sentence encoders. In NAACL, pages\n622–628. Association for Computational Linguistics.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\nStereoSet: Measuring stereotypical bias in pretrained\nlanguage models. In ACL-IJCNLP, pages 5356–\n5371, Online. Association for Computational Lin-\nguistics.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020. CrowS-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models. In EMNLP, pages 1953–1967. As-\nsociation for Computational Linguistics.\nArun K. Pujari, Ansh Mittal, Anshuman Padhi, Anshul\nJain, Mukesh Jadon, and Vikas Kumar. 2019. Debias-\ning gender biased hindi words with word-embedding.\nIn ACAI, ACAI 2019, page 450–456. Association for\nComputing Machinery.\nAlexey Romanov, Maria De-Arteaga, Hanna Wal-\nlach, Jennifer Chayes, Christian Borgs, Alexandra\nChouldechova, Sahin Geyik, Krishnaram Kenthapadi,\nAnna Rumshisky, and Adam Kalai. 2019. What’s\nin a name? Reducing bias in bios without access to\nprotected attributes. In NAACL, pages 4187–4195.\nAssociation for Computational Linguistics.\nMagnus Sahlgren and Fredrik Olsson. 2019. Gender\nbias in pretrained Swedish embeddings. In NoDaL-\niDa, pages 35–43. Linköping University Electronic\nPress.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nJoão Sedoc and Lyle Ungar. 2019. The role of protected\nclass word lists in bias identification of contextual-\nized word representations. In GeBNLP, pages 55–61.\nAssociation for Computational Linguistics.\nTatiana Shavrina and Olga Shapovalova. 2017. To\nthe methodology of corpus construction for machine\nlearning:«taiga» syntax tree corpus and parser. Cor-\npora, pages 78–84.\nFábio Souza, Rodrigo Nogueira, and Roberto Lotufo.\n2020. BERTimbau: pretrained BERT models for\nBrazilian Portuguese. In BRACIS.\nMasashi Takeshita, Yuki Katsumata, Rafal Rzepka, and\nKenji Araki. 2020. Can existing methods debias\nlanguages other than English? first attempt to an-\nalyze and mitigate Japanese word embeddings. In\nGeBNLP, pages 44–55. Association for Computa-\ntional Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nEMNLP: System Demonstrations, pages 38–45. As-\nsociation for Computational Linguistics.\nJieyu Zhao, Subhabrata Mukherjee, Saghar Hosseini,\nKai-Wei Chang, and Ahmed Hassan Awadallah. 2020.\nGender bias in multilingual embeddings and cross-\nlingual transfer. In COLING, pages 2896–2907. As-\nsociation for Computational Linguistics.\nJieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and\nKai-Wei Chang. 2018. Learning gender-neutral word\nembeddings. In EMNLP, pages 4847–4853. Associa-\ntion for Computational Linguistics.\nPei Zhou, Weijia Shi, Jieyu Zhao, Kuan-Hao Huang,\nMuhao Chen, Ryan Cotterell, and Kai-Wei Chang.\n2019. Examining gender bias in languages with\ngrammatical gender. In EMNLP-IJCNLP, pages\n5276–5284. Association for Computational Linguis-\ntics.\nYi Zhou, Masahiro Kaneko, and Danushka Bollegala.\n2022. Sense embeddings are also biased-evaluating\nsocial biases in static and contextualised sense em-\nbeddings. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics,\nDublin. Association for Computational Linguistics.\n2750",
  "topic": "Okazaki fragments",
  "concepts": [
    {
      "name": "Okazaki fragments",
      "score": 0.8583822846412659
    },
    {
      "name": "Computer science",
      "score": 0.6389426589012146
    },
    {
      "name": "Computational linguistics",
      "score": 0.5639554262161255
    },
    {
      "name": "Linguistics",
      "score": 0.48479098081588745
    },
    {
      "name": "Natural language processing",
      "score": 0.39859458804130554
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38726526498794556
    },
    {
      "name": "Philosophy",
      "score": 0.10829561948776245
    },
    {
      "name": "Medicine",
      "score": 0.06669986248016357
    },
    {
      "name": "Immunohistochemistry",
      "score": 0.0
    },
    {
      "name": "Eukaryotic DNA replication",
      "score": 0.0
    },
    {
      "name": "Proliferating cell nuclear antigen",
      "score": 0.0
    },
    {
      "name": "Internal medicine",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I114531698",
      "name": "Tokyo Institute of Technology",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I1330693074",
      "name": "Walmart (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210089985",
      "name": "Amazon (Germany)",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I146655781",
      "name": "University of Liverpool",
      "country": "GB"
    }
  ],
  "cited_by": 25
}