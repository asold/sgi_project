{
  "title": "TabTranSELU: A transformer adaptation for solving tabular data",
  "url": "https://openalex.org/W4393104014",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2264231934",
      "name": "Yuchen Mao",
      "affiliations": [
        "University of Edinburgh"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6600605178",
    "https://openalex.org/W6600213211",
    "https://openalex.org/W4287114832",
    "https://openalex.org/W4302009014",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W1678356000",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4315588382",
    "https://openalex.org/W3170720134",
    "https://openalex.org/W4287554008"
  ],
  "abstract": "Tabular data are most prevalent datasets in real world, yet the integration of deep learning algorithms in tabular data often garners less attention despite their widespread utilization in other field. This phenomenon could be attributed to the dominance of the classical algorithms in their simplicity and interpretability, and the superior performance of the gradient boosting tree models in tabular data. In this paper, a simple yet affective adaptation of the Transformer architecture tailored specifically for tabular data is presented, not only achieving good performance but also retains a high degree of explain ability. The model encodes both continuous and categorical features, alongside their respective names, and feed them into an enhanced Transformer structure enriched with Scaled Exponential Linear Unit activation. Through rigorous experimentation, our model not only outperforms classical algorithms and similar Transformer-based counterparts, but also are comparable to the performance of gradient boosting tree models.",
  "full_text": "TabTranSELU: A transformer adaptation for solving tabular \ndata \nYuchen Mao \nSchool of Mathematics, University of Edinburgh, Old College, South Bridge, \nEdinburgh EH8 9YL. United Kingdom \ns2045776@ed.ac.uk \nAbstract. Tabular data are most prevalent datasets in real world, yet the integration of deep \nlearning algorithms in tabular data often garners less attention despite their wides pread \nutilization in other field. This phenomenon could be attributed to the dominance of the classical \nalgorithms in their simplicity and interpretability, and the superior performance of the gradient \nboosting tree models in tabular data. In this paper, a  simple yet affective adaptation of the \nTransformer architecture tailored specifically for tabular data is presented, not only achieving \ngood performance but also retains a high degree of explain ability. The model encodes both \ncontinuous and categorical f eatures, alongside their respective names, and feed them into an \nenhanced Transformer structure enriched with Scaled Exponential Linear Unit activation. \nThrough rigorous experimentation, our model not only outperforms classical algorithms and \nsimilar Transformer-based counterparts, but also are comparable to the performance of gradient \nboosting tree models.  \nKeywords: SELU, Tabular Data, Transformer, Deep Learning, Embedding. \n1.  Introduction \nTabular data is an essential foundation across diverse fields, prominent ly in critical domains like \nhealthcare, factory settings, advertising, and finance. Its inherent structure streamlines various benefits, \nsuch as the efficient storage, retrieval, and analysis of multifaceted information. Often employed in \nregression and cl assification tasks, tabular data predicts categorical or numerical outcomes based on \ndata rows. This seemingly straightforward process can yield valuable and critical insights for \nprofessionals in various sectors, shaping informed decisions. \nWhile for class ification and regression tasks involving tabular data, the landscape has been \npredominantly shaped by models rooted in gradient tree boosting techniques since 2016. This dominance \nprimarily stems from their robustness and efficient training times. Exemplif ied by well -known \nimplementations like XGBoost, LightGBM, and CatBoost, these models, derived from the gradient \nboosting methodology, form a pivotal aspect of the machine learning field [1 -4]. In machine learning \ncompetitions, gradient boosting tree models are particularly prevalent due to their capability to achieve \nexceptional performance when the hyper parameters are properly tuned. \nAnother facet of machine learning consists of those traditional algorithms, excelling in their \nsimplicity, speed, and resour ce efficiency. Notable examples include Logistic Regression, Support \nVector Machines (SVM), and k-nearest neighbours. excels at its simplicity, speed. They are frequently \nProceedings of the 4th International Conference on Signal Processing and Machine LearningDOI: 10.54254/2755-2721/51/20241174\nÂ© 2024 The Authors. This is an open access article distributed under the terms of the Creative Commons Attribution License 4.0(https://creativecommons.org/licenses/by/4.0/).\n81\nemployed in sectors such as manufacturing, healthcare, and settings where prediction accuracy is not \nthe goal, but the prediction speed is of paramount importance and computational resources are valued \nassets. \nAs the tasks surrounding tabular data is dominate by the traditional and gradient boosting tree \nmachine learning algorithms, the rea lm of deep learning has striven to bridge the gap in performance \nfor tasks around tabular data, concurrently. To begin with, the Multilayer Perceptron (MLP) model was \nserved as a baseline. Then, various deep learning architectures tailored to tabular data have been \nformulated, novel models have drawn inspiration from diverse fields; for instance, the adoption of \nResNet-like architectures from computer vision and the Transformer architectures from natural language \nprocessing [5, 6].  \nIn this paper, we introdu ce a model that attains prediction accuracy comparable to boosting tree \nalgorithms while maintaining feature interpretability. Achieving this synergy involves encoding \ncontinuous features, embedding categorical names, and their respective features. These p rocessed \nfeatures are then directed injected into a modified Transformer module for prediction. Our adapted \nTransformer module is meticulously designed to cater to the nuances of tabular data. Given tabular \ndataâ€™s inherent sparsity and the presence of nega tive values, specific modifications have been \nimplemented. Also, this customized Transformer module features a solitary layer of encoder and \ndecoder. Moreover, the combination of ReLU and layer normalization layers is replaced with an SELU \nactivation layer, effectively preserving the latent information [7].  \nFurthermore, the specially crafted embedding layer serves the dual purpose of encoding feature \nnames and category names, thereby revealing the intricate interconnections that exist between these \ncategories and features. With the growing emphasis on safeguarding the privacy of personal and \nbusiness data, approaches of withholding the disclosure of actual feature or category names and \nreplacing them with pseudo-labelled character representations are being applied, as demonstrated in the \nrecent Kaggle ICR competition. The interpretability provided by the embedding layer not only addresses \nthis challenge of anonymity but also contributes to a more thorough comprehension of the intricate \nrelationships between f eatures and categories within specialized datasets, thereby enhancing their \noverall interpretability. \nFinally, in literature review section, related papers are researched and discussed. In the model section, \nthe structure and design of model has been explai ned. In experiment section, experiment on various \ndatasets is being conducted and the result of our model are being compared to various existing models. \nIn Feature interpretability section, the unique property of our model that allows feature and category \ninterpretability is introduced. Finally, in conclusion section, the result and conclusion of this paper is \npresented. \n2.  Literature Review \nThe primary attention for related paper resides on the models which originated from the Transformer \narchitecture, as it serves as the foundational architecture underlying our work.  \nOriginating as a solution for NLP translation tasks, the Transformer â€™s core structure encompasses \nencoder and decoder layers comprising self -attention mechanisms and feedforward neural networks \n(FFNNs). After the introduction of Transformer, it quickly gains it â€™s popularity, and the swift \nproliferation of the Transformer â€™s popularity in the NLP domain has permeated the broader landscape \nof deep learning. Notably, computer vision tasks have given rise to variations of Transformer structure, \nsuch as the Vision Transformer (ViT) [8]. In alignment with this trend, the field of tabular data analysis \nhas witnessed the emergence of various adaptations of the Transformer. \nWhen Transformer first emerges, model  with simple adaptation that only involves modification of \ninput data that allows the input data to be fed into transformer architecture is introduced. The FT -\nTransformer employs a straightforward yet effective approach [9]. It starts by tokenizing and embedding \nthe categorical features. These embeddings are then combined with the continuous features, which have \nbeen processed through a dense layer. This approach enables smooth integration of input features into \nthe Transformer architecture. On the other hand, the Tab -Transformer adopts an approach reminiscent \nProceedings of the 4th International Conference on Signal Processing and Machine LearningDOI: 10.54254/2755-2721/51/20241174\n82\nof wide and deep models [10]. It utilizes embeddings to process categorical features, which are \nsubsequently fed into a Transformer layer. At the final dense layer, these embedded representations are \ncombined with the continuous features to form a prediction. These adaptations represent instances of \nsimplistic yet impactful integration of the Transformer model within the tabular data context. \nNotably, a subset of models further augments the fundamental Transformer architecture, yielding \nconsiderable enhancements in tabular data tasks. For instance, the SAINT model incorporates self -\nattention mechanisms from Transformers while introducing inter-sample attention, effectively capturing \nrow-wise relationships and used neighbour -based classification for final prediction [11]. ExcelFormer \namalgamates Transformer attention mechanisms with FFNNs, proposing AiuM and DiaM modules that \nfacilitate feature representation updates and interactions [12]. TransTab, which shares its lineage with \nthe Transformer architecture, accommodates variable column tables, thus accommodating pre training \nand transfer learning [13]. Finally, Ta bPFN, gaining traction in recent Kaggle competitions, harnesses \nthe power of synthetic data and prior data fitting to pre train Transformer -like models, showcasing \nprowess particularly on small datasets [14]. \nWhile the aforementioned models manifest commendable utility, our research endeavours unveil yet \nanother innovative adaptation of the Transformer model tailored to tasks in the tabular domain. Our \nmodel is meticulously crafted to address the intricacies and challenges inherent in this context. By \nmodifying the input features to a more favoured representation and feeding it into the modified \ntransformer architecture, where ReLU and layer norm are replaced with SELU activation, for better \nhandling of tabular data. \n3.  The Model \nThe model is constructed upon the Transformer architecture, and we have adapted this architecture to \nmake it more suitable for addressing classification and regression problems involving tabular data. The \noverall model architecture is structured as follows: (1) Input processing layers, (2)  Transformer-like \nlayers, and (3) Dense layer. A detailed model architecture is shown in Figure 1. \n \nFigure 1. The detailed modelâ€™s architecture. \n3.1.  Input Processing Layers \nThe first layer before the transformer like structures are the input processing layer. Our model treats any \ntabular data as decomposition of two elements: categorical element and numerical element. The \nseparation of categorical element and numerical element is driven by its inherent differences. The \nnumerical element encapsulates a continuous ra nge of values, while the categorical element is discrete \nin natural and lack of mathematical meanings. Additionally, in this paper, categorical elements undergo \na direct tokenization and processing through an embedding layer, which is similar to natural la nguage \nprocessing tasks, where a unique vector dimension of 64 has been trained to represent each category. In \ncontrast, to align the numerical element to the categorical vector dimension of 64, a modified positional \nProceedings of the 4th International Conference on Signal Processing and Machine LearningDOI: 10.54254/2755-2721/51/20241174\n83\nencoding is used, wherein the numerical  value itself is adopted as the positional parameter. This \napproach ensures that each encoded value preserves the same numerical meaning from the input, while \nsimultaneously exhibiting variations across different instances. \n3.1.1.  Categorical elements. A categoric al element can be categorical features, binary features, and \ncolumn names. An embedding matrix is created with size of categorical elementsÃ—64. Each categorical \nelement is tokenized and matched to a corresponding row of the embedding matrix to generate a \ncategorical feature embedding vector of size 64. \n3.1.2.  Numerical elements. The numerical element is any continuous features. A Gaussian noise with \nstandard deviation of 0.2 is first been applied to each numerical element mimicking noise in the datasets \nand provide more augmentation, so that the model is more robust. Then, to allow continuous features \nmatch the shape of categorical feature embedding, the disturbed numerical e lement has been feed to \npositional encoding. Since numerical element itself have an underlying aspect of position and we need \nto expand the numerical element to the same shape of categorical feature embedding, with each position \nstill holds the information of the original data but varies for different position of the vector. The modified \npositional encoding is as follows: \n ğ‘·ğ’†(ğ’—ğ’‚ğ’ğ’–ğ’†, ğŸğ’Š) = ğ‘ºğ’Šğ’ (\nğ’—ğ’‚ğ’ğ’–ğ’†\nğŸğŸğŸğŸ\nğŸğ’Š\nğ’…ğ’ğ’ğ’…ğ’†ğ’\n)  ğ’‚ğ’ğ’… ğ‘·ğ’†(ğ’—ğ’‚ğ’ğ’–ğ’†, ğŸğ’Š + ğŸ) = ğ‘ºğ’Šğ’ (\nğ’—ğ’‚ğ’ğ’–ğ’†\nğŸğŸğŸğŸ\nğŸğ’Š+ ğŸ\nğ’…ğ’ğ’ğ’…ğ’†ğ’\n) (1) \nFinally, the embedded categorical column names and numerical column names are being added to \nthe corresponding categorical or numerical vectors. The categorical or numerical vectors are \nconcatenated to form the final input to the transformer-like layers. \n3.2.  Transformer-like Layers \nTo tailor the Transformer architecture to the characteristics of Tabular data, several modifications are \napplied to the architecture. \n3.2.1.  Transformers The original implementation of Transformer [13] consists of an encoder and \ndecoder, each composed of several identical layers. Each layer consists of a multi -head self-attention \nlayer followed by a feed-forward layer. Additionally, layer normalization and residual connections are \nused to stabilize training.  \nSelf-attention is a mechanism in the Transformer architecture that enables each position in a sequence \nto attend to other positions, capturing dependencies between different elements in the sequence. \nSpecifically, a self -attention layer consists of three components: Key  ğŠâ€ˆ âˆˆ ğ‘ğ¦â€ˆÃ—ğ¤, Query  ğ âˆˆ ğ‘ğ¦Ã—ğ¤, \nand Value ğ• âˆˆ ğ‘ğ¦Ã—ğ¯. Each component is being applied to the input. Query and Key are multiplied to \nproduce attention scores, which are used to compute a weighted sum of Values. Mathematically, for a \nsequence of input vectors X, the self-attention operation can be expressed as: \n Attention(ğ‘„, ğ¾, ğ‘‰) = softmax (\nğ‘„ğ¾ğ‘‡\n(ğ‘‘ğ‘˜)\n1\n2\n) ğ‘‰      (2) \nHere, Q, K, and V are the transformed Query, Key, and Value matrices, and ğğ¤ is the dimension of \nthe Key vectors. The softm ax function normalizes the attention scores, determining how much each \nposition contributes to the final output.  \nIn practice, when we implement this structure on tabular data, the attention between features can then \nbe captured and applied to the input features to determine the final classification target. \nTo tailor the Transformer architecture to the characteristics of Tabular data, this paper has embraced \nthe principle of Occam â€™s Razor, advocating for minimal unnecessary multiplication of entities. As a \nresult, we have streamlined the Transformer â€™s architecture to have only one encoder and one decoder \nlayer. Furthermore, we have excluded the masked layer from the multi-head attention within the decoder \nProceedings of the 4th International Conference on Signal Processing and Machine LearningDOI: 10.54254/2755-2721/51/20241174\n84\nlayer. While this masked layer is crucial for preventing the model from accessing future information in \nNatural Language Processing (NLP) tasks, its absence is justified for Tabular data, where there â€™s no \ninherent ordering. Also, in the context of training with tabular data, the presence of masked layer \ninadvertently restricts preceding data from accessing the subsequent data. Consequently, earlier data \nreceive fewer contextual information, leading to potentially compromised learning outcomes. Hence, \nthe masked layer is removed in the decoder module. Lastly, th e combinations of ReLU and layer \nnormalization have been replaced with a SELU activation layer. \n3.2.2.  SELU Here, we present the rationale behind the decision to substitute the combination of \nnormalization layer and ReLU activation with SELU activation function in our proposed neural network \narchitecture. The substitution is motivated by two key reasons. \nâš« Addressing the â€œDying ReLUâ€ problem \nThe Rectified Linear Unit (ReLU) activation function, mathematically defined as \n ğ’‡(ğ’™) = ğ’ğ’‚ğ’™(ğŸ, ğ’™). (3) \nis commonly used due to its simplicity and effectiveness in many mac hine learning tasks. However, \nReLU suffers from the â€œDying ReLUâ€ problem, where the problem arises when certain neurons become \ninactive, consistently producing zero outputs. Given that tabular data inherently differs from computer \nvision (CV) and natural language processing (NLP) tasks, where ReLU has found widespread adoption, \ntabular data often contains a substantial number of negative values. The abundance of negative values \nwill inevitably worsen the dying ReLU problem. \nAdditionally, the architecture pro posed, condensed to a single layer of encoder and decoder, is \nparticularly susceptible to the Dying ReLU problem, since there exists less neurons. The excessive \npresence of negative values in tabular data, coupled with the reduced model depth, would lead t o a \nsignificant loss of latent information during training. To prevent this issue and retain the valuable \ninformation present in negative values, we opted to eliminate the use of ReLU activations. \nâš« Leveraging SELU for Self-Normalization: \nThe SELU activation f unction (Scaled Exponential Linear Units) are activation function that induces \nself-normalization. Mathematically expressed as: \n ğ’‡(ğ’™) = { ğ€ğ’™ ğ’Šğ’‡ ğ’™ > ğŸ\nğ€ğœ¶(ğ’†ğ’™ âˆ’ ğŸ)ğ’Šğ’‡ğ’™ â‰¤ ğŸ  ğ’‡ğ’ğ’“ ğ€ â‰ˆ ğŸ. ğŸğŸ“ğŸğŸ•, ğœ¶ â‰ˆ ğŸ. ğŸ”ğŸ•ğŸ‘ğŸ‘. (4) \nSELU activations are equipp ed with self-normalization property, where the input features are been \npushed to zero mean and unit variance. Unlike ReLU, SELU retains both positive and negative values \nwithin the data, affectively avoiding the Dying ReLU problem. Furthermore, our decisio n to utilize \nSELU activations is bolstered by findings from the original SELU paper, which indicate that SELU \noutperforms layer normalization used in Transformer architecture and batch normalization, especially \nwhen dealing with small perturbations and high variances. \n3.3.  Final dense layer \nThe final layer serves as an extractor, retrieving pertinent information from the latent output of the \ntransformer like structure. For Regression tasks, a dense layer is used. For classification tasks, a SoftMax \nactivation is employed after the dense layer, creating an output size that corresponds precisely to the \ndimensions of the target data. \nProceedings of the 4th International Conference on Signal Processing and Machine LearningDOI: 10.54254/2755-2721/51/20241174\n85\n4.  Experiments \n4.1.  Data \nWe evaluate our model and baseline models on 2 publicly available classification datasets from UCI \nrepository (Dua and Gra ff 2017) and Kaggle (Kaggle, Inc 2017). For each dataset, the data is divided \ninto training and testing set, with a split of 80%/20%. The Categorical data is processed with Label \nencoding. For our model, the categorical column name and continuous column name are being extracted \nand Label encoded. For our model, the input data are continuous features, label encoded categorical \nfeatures, continuous column names, and categorical column names.   The description of datasets are \nshown in table 1. \nTable 1. Datasets description. \nDataset abbreviation Samples Features \nAdult Census Income AC 48842 14 \nBank Marketing BM 45211 17 \n4.2.  Model Setup \nFor each dataset, our model and five baseline model are trained and evaluated. I ncluding Logistic \nRegression, Random Forest Classifier, MLP Classifier, XGBoost Classifier, and tab transformer. The \ntab transformer is built with TensorFlow, XGBoost Classifier is obtained from XGBoost package, and \nall other models are taken from Sklearn module. The hyper parameters are not tuned. For our model, we \nset a batch size of 64, learning rate of 0.005, with an exponential decay of learning rate with a decay \nstep of 20 and decay rate of 0.9. The model is then trained for 3 epochs for each dataset.  Then, each \nmodel has been trained and evaluated for 10 times, the subsequent predicted results are being evaluated \nagainst the testing target by accuracy rating for classification task. The resulting evaluations are shown \nin table 2, where the datasets ar e been mentioned with their abbreviations. The model proposed in this \npaper is highlighted in grey, and the datasets with the best accuracy score has been coloured with red. \nTable 2. Model Performance Comparison \nModel AC BM \nOur Model 0.867 Â± 0.004 0.831 Â± 0.006 \nLogistic Regression 0.807 Â± 0.005 0.790 Â± 0.007 \nRandom Forest 0.857 Â± 0.005 0.847 Â± 0.005 \nXGBoost 0.869 Â± 0.004 0.853 Â± 0.007 \nMulti-layer Perception 0.730 Â± 0.119 0.753 Â± 0.041 \nTab-Transformer 0.845 Â± 0.002 0.815 Â± 0.007 \n \nIn table 2, when compared with traditional machine learning models, our model consistently \noutperforms their score. While compared to the similar Neural Network models, our model surpasses \nthe baseline Multi-layer Perception model by a drastic 10%. Comparing to the Tab-transformer model, \nwhich is similar to ours â€™s as it inherent it â€™s structure from Transformer, our model still consistently \noutperforms Tab-transformer by an average of 2% for classification tasks. Even when compared with \nGBDT boosting tree models, such as XGBoost, we only lose merely by 0.2% of accuracy, and the gap \nis narrowing as the datasetâ€™s size increasing. \n4.3.  Ablation Experiments \nThe performance comparison of our model in the cases of with and without SELU activation are \npresented in table 3. \nProceedings of the 4th International Conference on Signal Processing and Machine LearningDOI: 10.54254/2755-2721/51/20241174\n86\nTable 3. Model Performance Comparison between our model with and without SELU \nModel AC BM \nOur Model with SELU 0.867 Â± 0.004 0.831 Â± 0.006 \nOur Model without SELU 0.862 Â± 0.004 0.827 Â± 0.006 \n \nAs anticipated, the incorporation of the SELU activation function effectively tackled the issue of \ndying ReLU units, thereby retaining a greater amount of latent information and consistently enhancing \nthe final prediction by 0.4%. \nIn summary, aligning all a spects of the SELU activation layer shows a clear advantage over the \ncombination of layer normalization and ReLU layers, which also further simplified our overall \narchitecture. This strategic choice aims to enhance training effectiveness and information re tention, \nthereby improving the model performance on Tabular data. \n5.  Feature Interpretability \nTo visually see the Interpretability of the trained embedding layer, a PCA analysis is applied to the \nembedding layer of the model trained on the adult capital income dataset. In this analysis, the embedding \nvectors of dimension 64 are condensed into x and y positions. \n \n \n \nFigure 2. The PCA analysis of embedding vector \nof category under education column. \n Figure 3. The PCA analysis of \nembedding vector of columns. \nFigure 2 illustrates the PCA analysis of embedding of education categories, revealing a distinct trend \nbetween the label position and the level of education. The labels range from preschool, the lowest \neducation level (upper left), to Doctorate, the highest level of edu cation (lower right). Other categories \nalso exist similar relationships between its position and its underlying meaning. \nFigure 3 showcases another PCA analysis of embedding of feature names. This analysis provides \nclear indications of relationships, such a s the connection between â€œeducation-numâ€ and education, as \nwell as the relationship between â€œcapital gainâ€ and education, race, and marital status. \nIndeed, the trained model â€™s embeddings unveil hidden relationships between features. Even when \nthe feature and category names are concealed, these results can offer the user an intuitive understanding \nof the datasets. \n6.  Conclusion \nIn this paper, a novel model rooting from transformer model adapted specifically to tabular data is \npresented. The model consists of three  parts, input processing part, Transformer -like part, and a final \ndense layer. For the transformer-like part of the model, all combination of ReLU and layer normalization \nare being replaced with SELU activation, and the attention mask are being removed to supply the model \nwith more latent information. Additionally, a specially designed input processing layer are used to \nhandle the tabular data. Empirical experiments have consistently demonstrated that the model \noutperforms traditional machine learning algor ithms. Even when compared to similar Transformer -\nProceedings of the 4th International Conference on Signal Processing and Machine LearningDOI: 10.54254/2755-2721/51/20241174\n87\nbased networks, our model exhibits superior performance. Notably, the model also showcases \nperformance comparable to the state -of-the-art gradient boosting tree models. To further validate our \napproach, an a blation experiments is conducted. In the experiments, our model consistently \noutperformed other cases where SELU activation was not used, thus affirming the correctness and \neffectiveness of our modifications in the Transformer part. Furthermore, one distin ctive feature of our \nmodel is its ability to offer enhanced interpretability for any given data. The embedding layer within the \ntrained model unveils underlying relationships within categorical features, and continuous and category \nfeature names. This capability proves valuable when categorical and feature names are anonymized or \nwhen users are unfamiliar with the domain from which the data originates. Finally, for future work, \nfurther adaptation or modification of the model to accommodate small-sized datasets could be explored. \nReferences  \n[1] Chen T and Guestrin C 2016 XGBoost: A scalable tree boosting system Proceedings of the 22nd \nacm sigkdd international conference on knowledge discovery and data mining 785-94 \n[2] Ke G, Meng Q, Finley T, Wang T, Chen W, Ma W, Ye Q and Liu TY 2017 Lightgbm: A highly \nefficient gradient boosting decision tree Advances in neural information processing systems , \n30 \n[3] Prokhorenkova L, Gusev G, Vorobev A, Dorogush A and Gulin A 2018 CatBoost: unbiased \nboosting with categorical features Advances in neural information processing systems 31 \n[4] Friedman J H 2001 Greedy function approximation: a gradient boosting machine Annals of \nstatistics 1189-232 \n[5] He K, Zhang X, Ren S and Sun J 2016 Deep residual learning for image recognition Proceedings \nof the IEEE conference on computer vision and pattern recognition 770-8. \n[6] Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Å and Polosukhin I \n2017 Attention is all you need Advances in neural information processing systems 30 \n[7] Klambauer G, Unterthiner T, Mayr A and Hochreiter S 2017 Self -normalizing neural networks \nAdvances in neural information processing systems 30 \n[8] Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Dehghani M , \nMinderer M, Heigold G, Gelly S et al 2020 An image is worth 16x16 words: Transformers for \nimage recognition at scale arXiv preprint arXiv:2010.11929 \n[9] Gorishniy Y, Rubachev I, Khrulkov V and Babenko A 2021 Revisiting deep learning models for \ntabular data Advances in Neural Information Processing Systems 34 18932-43 \n[10] Huang X, Khetan A, Cvitkovic M and Karnin Z 2020 Tabtransformer: Tabular data modeling \nusing contextual embeddings arXiv preprint arXiv:2012.06678 \n[11] Somepalli G, Goldblum M, Schwarzschild A, Bruss CB and Goldstein T 2021 Saint: Improved \nneural networks for tabular data via row attention and contrastive pre -training arXiv preprint \narXiv:2106.01342 \n[12] Chen J, Yan J, Chen DZ and Wu J 2023 ExcelFormer: A Neural Network Surpassing GBDTs on \nTabular Data arXiv preprint arXiv:2301.02819 \n[13] Wang Z and Sun J 2022 Transtab: Learning transferable tabular transformers across table \nAdvances in Neural Information Processing Systems 35 2902-15 \n[14] Hollmann N, MÃ¼ ller S, Eggensperger K and Hutter F 2022 Tabpfn: A transformer that sol ves \nsmall tabular classification problems in a second arXiv preprint arXiv:2207.01848 \n \nProceedings of the 4th International Conference on Signal Processing and Machine LearningDOI: 10.54254/2755-2721/51/20241174\n88",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.7502090334892273
    },
    {
      "name": "Computer science",
      "score": 0.6900173425674438
    },
    {
      "name": "Categorical variable",
      "score": 0.5776947736740112
    },
    {
      "name": "Gradient boosting",
      "score": 0.5734980702400208
    },
    {
      "name": "Transformer",
      "score": 0.5422827005386353
    },
    {
      "name": "Boosting (machine learning)",
      "score": 0.49386054277420044
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4804498851299286
    },
    {
      "name": "Decision tree",
      "score": 0.45099616050720215
    },
    {
      "name": "Machine learning",
      "score": 0.4236152172088623
    },
    {
      "name": "Data mining",
      "score": 0.3516545295715332
    },
    {
      "name": "Algorithm",
      "score": 0.3201245665550232
    },
    {
      "name": "Engineering",
      "score": 0.1139509379863739
    },
    {
      "name": "Voltage",
      "score": 0.08971387147903442
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Random forest",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98677209",
      "name": "University of Edinburgh",
      "country": "GB"
    }
  ],
  "cited_by": 8
}