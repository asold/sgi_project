{
    "title": "Privacy Implications of Retrieval-Based Language Models",
    "url": "https://openalex.org/W4389519389",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5082426637",
            "name": "Yangsibo Huang",
            "affiliations": [
                "Princeton University"
            ]
        },
        {
            "id": "https://openalex.org/A5056999022",
            "name": "Samyak Gupta",
            "affiliations": [
                "Princeton University"
            ]
        },
        {
            "id": "https://openalex.org/A5034055494",
            "name": "Zexuan Zhong",
            "affiliations": [
                "Princeton University"
            ]
        },
        {
            "id": "https://openalex.org/A5092029605",
            "name": "Kai Li",
            "affiliations": [
                "Princeton University"
            ]
        },
        {
            "id": "https://openalex.org/A5051064208",
            "name": "Danqi Chen",
            "affiliations": [
                "Princeton University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3204613796",
        "https://openalex.org/W4206199121",
        "https://openalex.org/W4385570444",
        "https://openalex.org/W3206066344",
        "https://openalex.org/W1873763122",
        "https://openalex.org/W2963456518",
        "https://openalex.org/W2473418344",
        "https://openalex.org/W4385573102",
        "https://openalex.org/W4297687186",
        "https://openalex.org/W3170672407",
        "https://openalex.org/W4301243929",
        "https://openalex.org/W2324595764",
        "https://openalex.org/W4318719006",
        "https://openalex.org/W4287888099",
        "https://openalex.org/W3102554603",
        "https://openalex.org/W4221159672",
        "https://openalex.org/W4366198033",
        "https://openalex.org/W3120094169",
        "https://openalex.org/W4315588390",
        "https://openalex.org/W4226137521",
        "https://openalex.org/W2946930197",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W4385696152",
        "https://openalex.org/W2135930857",
        "https://openalex.org/W4385571534",
        "https://openalex.org/W1557833142",
        "https://openalex.org/W4226082499",
        "https://openalex.org/W4221152824",
        "https://openalex.org/W3165327186",
        "https://openalex.org/W3207429447",
        "https://openalex.org/W4385570290",
        "https://openalex.org/W4244201901",
        "https://openalex.org/W2963096510",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W2949461276",
        "https://openalex.org/W4281629162",
        "https://openalex.org/W3046764764",
        "https://openalex.org/W4280616980",
        "https://openalex.org/W4205694376",
        "https://openalex.org/W1603920809",
        "https://openalex.org/W4287553002",
        "https://openalex.org/W4288087322"
    ],
    "abstract": "Retrieval-based language models (LMs) have demonstrated improved interpretability, factuality, and adaptability compared to their parametric counterparts by incorporating retrieved text from external datastores. While it is well known that parametric models are prone to leaking private data, it remains unclear how the addition of a retrieval datastore impacts model privacy. In this work, we present the first study of privacy risks in retrieval-based LMs, particularly kNN-LMs. Our goal is to explore the optimal design and training procedure in domains where privacy is of concern, aiming to strike a balance between utility and privacy. Crucially, we find that kNN-LMs are more susceptible to leaking private information from their private datastore than parametric models. We further explore mitigations of privacy risks: When privacy information is targeted and readily detected in the text, we find that a simple sanitization step would eliminate the risks while decoupling query and key encoders achieves an even better utility-privacy trade-off. Otherwise, we consider strategies of mixing public and private data in both datastore and encoder training. While these methods offer modest improvements, they leave considerable room for future work. Together, our findings provide insights for practitioners to better understand and mitigate privacy risks in retrieval-based LMs.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 14887–14902\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nPrivacy Implications of Retrieval-Based Language Models\nYangsibo Huang Samyak Gupta Zexuan Zhong Kai Li Danqi Chen\nPrinceton University\nyangsibo@princeton.edu {samyakg,zzhong,li,danqic}@cs.princeton.edu\nAbstract\nRetrieval-based language models (LMs) have\ndemonstrated improved interpretability, factu-\nality, and adaptability compared to their para-\nmetric counterparts by incorporating retrieved\ntext from external datastores. While it is well\nknown that parametric models are prone to leak-\ning private data, it remains unclear how the ad-\ndition of a retrieval datastore impacts model\nprivacy. In this work, we present the first study\nof privacy risks in retrieval-based LMs, particu-\nlarly kNN-LMs. Our goal is to explore the op-\ntimal design and training procedure in domains\nwhere privacy is of concern, aiming to strike a\nbalance between utility and privacy. Crucially,\nwe find that kNN-LMs are more susceptible\nto leaking private information from their pri-\nvate datastore than parametric models. We fur-\nther explore mitigations of privacy risks: When\nprivacy information is targeted and readily de-\ntected in the text, we find that a simple sani-\ntization step would eliminate the risks while\ndecoupling query and key encoders achieves\nan even better utility-privacy trade-off. Other-\nwise, we consider strategies of mixing public\nand private data in both datastore and encoder\ntraining. While these methods offer modest im-\nprovements, they leave considerable room for\nfuture work. Together, our findings provide in-\nsights for practitioners to better understand and\nmitigate privacy risks in retrieval-based LMs1.\n1 Introduction\nRetrieval-based language models (Khandelwal\net al., 2020; Borgeaud et al., 2022; Izacard et al.,\n2022; Zhong et al., 2022; Min et al., 2023) generate\ntext distributions by referencing both the parame-\nters of the underlying language model and the in-\nformation retrieved from a datastore of text. Specif-\nically, the retrieval process involves accessing a\npre-defined datastore to retrieve a set of tokens or\n1Our code is available at https://github.com/\nPrinceton-SysML/kNNLM_privacy.\nPrompt\nRetrieval-based LM \n(API-access only) Attacker\nResponse\nData Extraction Attack\nDatastore\nDpublic Dprivate\nEncoders\nEncpublic\nEncprivate\nReconstructed Text\n Email:\n  mailme@alice.com\n  mailme@bob.com\n  mailme@charlie.com\n  …\n URL:\n  alice@bob.com\n  harry@hogwarts.edu\n  …\n Phone:\n  123-***-****\n  404-***-****\n  789-***-****\n  …  \nFigure 1: Retrieval-based language models (e.g., kNN-\nLMs) comprise encoders and the datastore as their key\ncomponents. When tailoring a retrieval-based language\nmodel for a privacy-sensitive task, both components\nmay utilize private data. However, a malicious user\npossessing API access to the model can exploit a data\nextraction attack in order to reconstruct sensitive infor-\nmation. This study explores the severity of such attacks\nand suggests novel strategies to mitigate them.\ntext passages that are most relevant to the prompt\nprovided to the model. These retrieved results are\nthen utilized as additional information when gener-\nating the model’s response to the prompt. Retrieval-\nbased language models offer promising prospects\nin terms of enhancing interpretability, factuality,\nand adaptability.\nHowever, in privacy-sensitive applications, util-\nity usually comes at the cost of privacy leakage.\nRecent work has shown that large language models\nare prone to memorizing (Thakkar et al., 2021;\nZhang et al., 2021) specific training datapoints,\nsuch as personally identifying or otherwise sen-\nsitive information. These sensitive datapoints can\nsubsequently be extracted from a trained model\nthrough a variety of techniques (Carlini et al., 2019,\n2021; Lehman et al., 2021), known as data ex-\ntraction attacks. While the threat of training data\nmemorization on model privacy has been studied\nfor parametric language models, there is a lack\n14887\nof evidence regarding the privacy implications of\nretrieval-based language models, especially how\nthe use of external datastore would impact privacy.\nIn this work, we present the first study of pri-\nvacy risks in retrieval-based language models, with\na focus on the nearest neighbor language models\n(kNN-LMs) (Khandelwal et al., 2020), which have\nbeen extensively studied in the literature (He et al.,\n2021; Zhong et al., 2022; Shi et al., 2022a; Xu\net al., 2023)2. In particular, we are interested in un-\nderstanding kNN-LMs’ privacy risks in real-world\nscenarios where they are deployed via an API to the\ngeneral public (Figure 1). We consider a scenario\nin which a model creator has a private, domain-\nspecific dataset that improves model performance\non domain-specific tasks, but may also contain sen-\nsitive information that should not be revealed; This\ndata can be used to train encoders or stored in the\ndatastore. In such a scenario, the model creator\nmust find a balance between utilizing their private\ndata to enhance model performance and protecting\nsensitive information.\nWe begin our investigation by examining a sit-\nuation where the creator of the model only adds\nprivate data to the retrieval datastore during infer-\nence, as suggested by Borgeaud et al. (2022). Our\nfindings indicate that while this approach enhances\nutility, it introduces an elevated privacy risk to the\nprivate data compared to parametric language mod-\nels (Section 4), and adversarial users could violate\nthe confidentiality of the datastore by recovering\nsensitive datapoints. Therefore, it is vital for the\nmodel creator to refrain from storing sensitive in-\nformation in the datastore.\nWe further explore mitigation strategies for kNN-\nLMs in two different scenarios. The first is where\nprivate information is targeted, i.e., can be easily\nidentified and removed (Section 5). We explore\nenhancing the privacy of kNN-LMs by eliminat-\ning privacy-sensitive text segments from both the\ndatastore and the encoder’s training process. This\napproach effectively eliminates the targeted privacy\nrisks while resulting in minimal loss of utility. We\nthen explore a finer level of control over private in-\nformation by employing distinct encoders for keys\n(i.e., texts stored in the datastore) and queries (i.e.,\n2Other retrieval-based language models such as\nRETRO (Borgeaud et al., 2022) and Atlas (Izacard et al.,\n2022) have significantly different architectures and the\nfindings from our investigation may not necessarily apply to\nthese models. Investigation of these models will be left as\nfuture work.\nprompts to the language model). Through our ex-\nperimental analysis, we demonstrate that this de-\nsign approach offers increased flexibility in striking\na balance between privacy and model performance.\nThe second is a more challenging scenario where\nthe private information is untargeted, making it im-\npractical to remove from the data (Section 6). To\naddress this issue, we explore the possibility of con-\nstructing the datastore using public datapoints. We\nalso consider training the encoders of the kNN-LM\nmodel using a combination of public and private\ndatapoints to minimize the distribution differences\nbetween the public data stored in the datastore and\nthe private data used during inference. Despite\nmodest improvements from the methods we ex-\nplored, the mitigation of untargeted attacks remains\nchallenging and there is considerable room for fu-\nture work. We hope our findings provide insights\nfor practitioners to better understand and mitigate\nprivacy risks in retrieval-based LMs.\n2 Background\nIn this section, we first review the key compo-\nnents of kNN-LMs (Section 2.1). Then, we discuss\nthe data extraction attacks for language models\n(Section 2.2). These aspects lay a foundation for\nthe subsequent exploration and analysis of privacy\nrisks related to kNN-LMs.\n2.1 Nearest Neighbor Language Models\nA kNN-LM (Khandelwal et al., 2020) augments\nthe standard language model with a datastore from\nwhich it can retrieve tokens to improve perfor-\nmance. We use the term “query” to denote the\nprompt provided to a kNN-LM. This query is en-\ncoded by the encoder EncQ as it is passed into the\nkNN-LM. The term “key” is used to denote the\ntokens in the datastore and it is encoded by the\nencoder EncK.\nEncoders Given a vocabulary V, the encoder\nEncK or EncQ performs the task of mapping a\ngiven key or query c ∈V∗to a fixed-length vec-\ntor representation. Typically, this encoding pro-\ncess is accomplished through a trained language\nmodel, where EncK(c) or EncQ(c) represents the\nvector hidden representation obtained from the out-\nput layer of the language model when provided\nwith the input c. Although in the default kNN-LMs,\nEncK and EncQ are commonly identical functions,\nwe explore different options in the work.\n14888\nDatastore The datastore, {(EncK(ci),wi)}, is a\nkey-value store generated by running the encoder\nEncK(·) over a corpus of text; Each key is the\nvector representation EncK(ci) for some context\nci ∈V∗, and each value wi ∈V is the ground-truth\nnext word for the leftward context ci. A search\nindex is then constructed based on the key-value\nstore to enable retrieval.\nInference At inference time, when predicting\nthe next token for a query x ∈ V∗, the model\nqueries the datastore with encoded query EncQ(x)\nto retrieve x’s k-nearest neighbors Nk according\nto a distance function d(·,·)3. Then the model\ncomputes a softmax over the (negative) distances,\nwhich gives pkNN(y|x), a distribution over the next\ntoken, in proportional to:\n∑\n(ci,wi)∈Nk\n1y=wi exp\n(\n−d(EncK(ci),EncQ(x))\nt\n)\n,\nwhere t is a temperature parameter, and k is a\nhyper-parameter that controls the number of re-\ntrieved neighbors. The prediction is then interpo-\nlated with pLM, the prediction from the original\nLM: p(y|x) = λpkNN(y|x) + (1−λ)pLM(y|x),\nwhere λis an interpolation coefficient.\n2.2 Data Extraction Attacks\nPrior work (Carlini et al., 2021) demonstrates that\nan attacker can extract private datapoints from the\ntraining set of a learned language model. The ex-\nistence of such an attack poses a clear and alarm-\ning threat to the confidentiality of sensitive train-\ning data, potentially jeopardizing deployment in\nreal-world scenarios (e.g., Gmail’s autocomplete\nmodel (Chen et al., 2019), which is trained on\nprivate user emails). Our definition of privacy\nleakage adopts the standard definition of Carlini\net al. (2021)). Specifically, a string sis extractable\nfrom an kNN-LM fθ if there exists a prefix csuch\nthat: s←arg maxs′ fθ(s′|c). Namely, the model\ngenerates sas the most likely continuation when\nprompted with some prefix c.\nThe attack consists of two main steps: 1) gener-\nating candidate reconstructions by prompting the\ntrained models, and 2) sorting the generated candi-\ndates based on a score that indicates the likelihood\nof being a memorized text. Further details about\nthe attack can be found in Appendix A.\n3d(·, ·) is usually the squared ℓ2 distance.\nWhile previous research has successfully high-\nlighted the risks associated with data extraction in\nparametric language models, there remains a no-\ntable gap in our understanding of the risks (and\nany potential benefits) pertaining to retrieval-based\nlanguage models like kNN-LMs. This study aims\nto address this gap and provide insights into the\nsubject matter.\n3 Problem Setup\nIn this section, we formally describe our problem\nsetup (Section 3.1) and privacy measurements (Sec-\ntion 3.2). We then detail our evaluation setup (Sec-\ntion 3.3).\n3.1 Problem Definition\nWe consider a scenario where a service provider\n(e.g. a financial institution) aims to enhance its cus-\ntomer experience by developing a kNN-LM and\ndeploying it as an API service. Note that the devel-\nopment of kNN-LMs intended solely for personal\nuse (e.g., constructing a kNN-LM email autocom-\npleter by combining a public LM with a private\nemail datastore) falls outside the scope of our study\nbecause it does not involve any attack channels that\ncould be exploited by potential attackers. We as-\nsume that the service provider possesses its own\nprivate data ( Dprivate) specific to its domain, in\naddition to publicly available data (Dpublic).\nWe identify two key design choices which im-\npact the quality and privacy of such a deployed\nservice. First, the service provider chooses which\ndata to be included in its datastore, and this may be\npublic data (Dpublic), private data (Dprivate), or a\nmix of both. Second, they choose whether to use\nencoders that are pre-trained on publicly available\ndata (Encpublic), or further finetuned on the private\ndata (Encprivate). We posit that careful considera-\ntion of these design choices is needed to establish\na balance between privacy preservation and utility.\nThe service provider in such a scenario is con-\ncerned with making a useful API, while keeping\ntheir private data hidden from malicious users or\nattackers. Hence, the service provider’s objective\nis to attain a high level of utility (as measured by\nperplexity) on a held-out set of Dprivate while si-\nmultaneously minimizing the disclosure of private\ninformation. We quantify the metrics we consider\nfor privacy in Section 3.2.\n14889\n3.2 Privacy Measurements\nWe now describe how we evaluate the risk of data\nextraction attack within the scenario described ear-\nlier in Section 3.1.\nThreat model We assume that the service\nprovider deploys a kNN-LM with API access to\np(y|x). This API provides the attacker with the\ncapability to compute perplexity, conduct text com-\npletion, and perform other relevant tasks. However,\nit’s important to note that the attacker is restricted\nfrom accessing the internal parameters or the data-\nstore of the deployed model.\nOur study considers two types of privacy risks,\neach associated with a particular type of attack:\nTargeted attacks We define targeted risk as a\nprivacy risk that can be directly associated with a\nsegment of text (e.g., personal identifiers such as\naddresses and telephone numbers.) and propose\nthe targeted attack. The significance of a targeted\nattack becomes apparent when considering that\ntargeted risks have been explicitly addressed in\nvarious privacy regulations (Centers for Medicare\n& Medicaid Services, 1996; California State Leg-\nislature , 2018). A targeted attacker’s goal is to\nextract that certain segment of text. In our study,\nwe focus on the extraction of Personal Identifiable\nInformation (PII), including email addresses, tele-\nphone numbers, and URLs. To tailor the extraction\nattack to recover text segments such as PIIs rather\nthan the entire training text, we customize the at-\ntack prompts based on the type of information to be\nextracted. Specifically, we gather common preced-\ning context for telephone numbers, email addresses,\nand URLs, and use them as prompts. Appendix B\nprovides example prompts we use in the attack. For\nevaluation, we measure how many private PIIs of\neach category have been successfully reconstructed\nby the attacker:\n• We firstly detect all unique personal identifiers\nin the private dataset, denoted as {ρi}p\ni=1;\n• We then sort the reconstruction candidates\nbased on the membership metrics defined in\nAppendix A, and only keep the top- ncandi-\ndates {ci}n\ni=1;\n• Finally, we detect {ˆρi}q\ni=1, the unique PIIs\nin the top- n candidates, and then count\n|{ρi}p\ni=1 ∩{ˆρi}q\ni=1|, namely how many origi-\nnal PIIs have been successfully reconstructed\nby the attack. A larger number means higher\nleakage of private PIIs.\nUntargeted attacks The untargeted attack is the\ncase where the attacker aims to recover the entire\ntraining example, rather than a specific segment\nof text. Such attacks can potentially lead to the\ntheft of valuable private training data. We adopt\nthe attack proposed by Carlini et al. (2021) as the\nuntargeted attack, which is described in detail in\nAppendix A.1. For evaluation, we measure the\nsimilarity between the reconstructed text and the\noriginal private text:\n• We firstly sort the reconstruction candidates\nbased on the membership metrics defined in\nAppendix A, and only keep the top- ncandi-\ndates {ci}n\ni=1;\n• For each candidate ci, we then find the closest\nexample in the private dataset pi and compute\nthe ROUGE-L score (Lin, 2004) between ci\nand pi4. If the score is higher than 0.5, we\nmark the candidate as a good reconstruction.\nThe ROUGE-L measures the longest common\nsubsequence (LCS) between the attack result\nand the ground truth, thus representing the fi-\ndelity of the attacker results. The ROUGE-L\nscore has been used to measure the reconstruc-\ntion fidelity in previous work (Deng et al.,\n2021; Balunovic et al., 2022; Gupta et al.,\n2022).\nNote that the attack’s performance evaluation\nemploys the private dataset following established\nreconstruction attack practices, the attack itself\nnever utilizes this dataset.\n3.3 Evaluation Setup\nOur main evaluation uses the Enron Email\ndataset (Klimt and Yang, 2004) as the private\ndataset Dprivate, which contains around 500,000\nemails generated by employees of the Enron Corpo-\nration (see Table 1 for examples). We specifically\nchose this dataset due to its inclusion of PIIs, which\nenables us to evaluate targeted attacks effectively.\nWe also incorporate the Medical Transcriptions\ndataset as an additional dataset for the evaluation\nof untargeted attacks, and further information re-\ngarding this dataset can be found in Appendix D.\nWe use the WikiText-103 dataset (Merity et al.,\n2017) as Dpublic.\nWe pre-process the Enron Email dataset by re-\ntaining only the email body (see Table 1 for exam-\n4Note that Carlini et al. (2021) also focus on untargeted\nattacks but they adopt manual evaluation.\n14890\nExample #1: Request ID : 0000000000**** Request Create Date : //* 8:27:00 AM Requested For : ****.@enron.com\nResource Name : Market Data Bloomberg Resource Type : Applications\nExample #2: You can reach me over the weekend and in the evening at either –**** or –****.\nExample #3: This would give you a total loan of ****, total cost of **** for equity required of ****.\nExample #4: Winter 2001 baseload traded as high as ** pounds a megawatt-hour and as low as **** pounds a megawatt-hour,\nbefore closing at **** pounds a megawatt-hour, **** pence lower than Friday.\nExample #5: everything is correct except for password: ****.\nTable 1: Examples from the Enron Email dataset. We’ve anonymized any identifiable information to protect against\npotential privacy breaches inherent in the dataset.\nMODEL EVAL. PPL TARGETED ATTACK UNTARGETED ATTACK\nTOTAL PHONE EMAIL URL # GOOD RECON\n(PARAMETRIC LM) Encpublic 30.28 0 0 0 0 0\n(PARAMETRIC LM) Encprivate 20.63 28 11 14 3 620\n(kNN-LM) Encpublic W/ Dprivate 18.41 35 11 16 8 591\n(kNN-LM) Encprivate W/ Dprivate 16.12 54 25 23 6 656\nTable 2: Perplexity and data extraction risks for various model configurations on the Enron Email dataset. The\nconfiguration with the highest leakage is emphasized in red ; the configuration with the lowest leakage is highlighted\nin green . Privacy measurements are computed using the top 1000 candidates for the targeted attack and using the\ntop 5000 candidates for the untargeted attack. “Good Recon” refers to reconstructions that achieve a ROUGE-L\nscore greater than 0.5 when compared to the ground truth. Table 11 in Appendix D presents similar findings on the\nMedical Transcriptions dataset.\nple datapoints). We then use regular expressions to\nidentify and extract three types of personal identi-\nfiers for the use of the targeted attack: telephone\nnumbers, email addresses, and URLs. The statis-\ntics for these personal identifiers can be found in\nAppendix B. We use the GPT-2 base model (Rad-\nford et al., 2019) as Encpublic, and finetune it on\nthe Enron Email dataset as Encprivate.\nDuring inference, the model retrieves knearest\nneighbors according to the squaredℓ2 distance, nor-\nmalizes their distribution over the next word with a\nsoftmax using a temperature value of 1, and then\nuses an interpolation factor of λto combine pkNN\nand pLM. For each model configuration, we search\nk and λ on a held-out validation set for the best\nmodel perplexity, and then run the inference on the\nevaluation set (see Appendix B for details). Privacy\nmeasurements are computed using the top 1000\ncandidates for the targeted attack and using the top\n5000 candidates for the untargeted attack.\n4 Privacy-Utility of kNN-LMs with A\nPrivate Datastore\nThis section presents our investigation of whether\nthe addition of private data to the retrieval data-\nstore during inference is an effective method for\nachieving a good trade-off between privacy (mea-\nsured by metrics defined in Section 3.2) and utility\n(measured by perplexity) in kNN-LMs.\nWe are particularly interested in three scenar-\nios: utilizing only Encpublic (the publicly pre-\ntrained language model), utilizing only Encprivate\n(the model fine-tuned from Encpublic using private\ndata), and utilizing Encpublic with Dprivate (the\ncombination of the public model with the private\ndatastore). As shown in Table 2, using Encpublic\nalone results in very poor utility performance but\nposes minimal risk of data extraction from the pri-\nvate domain, as it has not been exposed to pri-\nvate datapoints. Using Encprivate enhances utility\n(perplexity improves from 30.28 to 20.63) but in-\ncreases the risk of data extraction.\nWhen it comes to kNN-LMs, incorporating a\nprivate datastore ( Dprivate) with a public model\n(Encpublic) yields even greater utility compared to\nrelying solely on the fine-tuned model (Encprivate).\nHowever, this utility improvement also comes at\nthe expense of increased privacy leakage. These\nfindings suggest that the privacy concern stemming\nfrom the private datastore outweighs that resulting\nfrom the privately fine-tuned model, indicating a\nlack of robust privacy protection in the design of\nkNN-LMs. Additionally, we note that the combina-\ntion of Encprivate and Dprivate achieves the highest\nutility but also incurs the highest privacy cost.\n14891\nSANITIZATION APPROACH SANITIZED ENCODER ? EVAL. PPL TOTAL PHONE EMAIL URLEncK EncQ\nNONE ✗ ✗ 16.12 54 25 23 6\nREPLACED W / < |endoftext|>\n✓ ✓ 16.83 0 0 0 0\n✗ ✓ 16.24 0 0 0 0\n✓ ✗ 16.32 16 6 6 4\nREPLACED W / DUMMY PII\n✓ ✓ 16.51 0 0 0 0\n✗ ✓ 16.29 0 0 0 0\n✓ ✗ 16.16 22 5 15 2\nREPLACED W / RANDOM PUBLIC PII\n✓ ✓ 16.38 0 0 0 0\n✗ ✓ 16.29 1 1 0 0\n✓ ✗ 16.20 23 8 13 2\nTable 3: Perplexity and extraction risk of kNN-LMs with different combinations of key encoder (EncK) and query\nencoder (EncQ) on the Enron Email dataset, under different sanitization approaches. Privacy measurements are\ncomputed using the top 5000 candidates. For each sanitization approach, the configuration with the highest leakage\nis emphasized in red , and the configuration with the lowest leakage is highlighted in green ; the best utility under\nsanitization is highlighted in boldface. Sanitizing EncQ is crucial to eliminating the targeted risk.\n5 Mitigations Against Targeted Risks\nOur previous findings indicate that the personal-\nization of kNN-LMs with a private datastore is\nmore susceptible to data extraction attacks com-\npared to fine-tuning a parametric LM with private\ndata. At the same time, leveraging private data\noffers substantial utility improvements. Is there\na more effective way to leverage private data in\norder to achieve a better balance between privacy\nand utility in kNN-LMs? In this section we fo-\ncus on addressing privacy leakage in the context\nof targeted attacks (see definition in Section 3.2),\nwhere the private information can be readily de-\ntected from text. We consider several approaches\nto tackle these challenges in Section 5.1 and Sec-\ntion 5.2, and present the results in Section 5.3. We\nalso investigate the effect of hyper-parameters in\nSection 5.4.\n5.1 Sanitization of Datastore and Encoders\nAs demonstrated in Section 4, the existence of pri-\nvate examples in the kNN-LMs’ datastore increase\nthe likelihood of privacy leakage since they are\nretrieved and aggregated in the final prediction.\nTherefore, our first consideration is to create a san-\nitized datastore by eliminating privacy-sensitive\ntext segments. We note that this verbatim level def-\ninition of “privacy leakage” is general and widely\nadopted. Notably, regulations such as HIPAA (Cen-\nters for Medicare & Medicaid Services, 1996) and\nCCPA (California State Legislature , 2018) offer\nexplicit definitions of privacy-sensitive data. Con-\nsequently, these regulatory frameworks can serve\nas the basis for establishing the verbatim-level defi-\nnition of “privacy leakage”. For example, HIPAA\ndefines 18 identifiers that are considered person-\nally identifiable information (PII), including names,\naddresses, phone numbers, etc.\nWe propose the following three options for sani-\ntization:\n• Replacement with <|endoftext|>: re-\nplace each privacy-sensitive phrase with the\n<|endoftext|>token;\n• Replacement with dummy text: replace each\nprivacy-sensitive phrase with a fixed dummy\nphrase based on its type. For instance, if tele-\nphone numbers are sensitive, they can be re-\nplaced with \"123-456-789\"; and\n• Replacement with public data: replace each\nprivacy-sensitive phrase with a randomly se-\nlected public phrase of a similar type. An ex-\nample is to replace each phone number with a\npublic phone number on the Web.\nThe encoders in a kNN-LM is another potential\nsource of privacy leakage. While it is typically\noptimized on target domain data to enhance per-\nformance, fine-tuning directly on private data in\nprivacy-sensitive tasks may result in privacy leaks\n(Table 2). Similarly, the encoder can be sanitized\nby fine-tuning the pre-trained encoder Encpublic on\na sanitized dataset that has had sensitive informa-\ntion removed.\n14892\n5.2 Decoupling Key and Query Encoders\nWe propose using separate encoders for keys and\nqueries in kNN-LMs, to allow for finer control over\nprivacy preservation. For example, the encoder for\nqueries can be the sanitized encoder, while the en-\ncoder for keys can be the non-sanitized one; This\nway, the query encoder can be more resistant to\nprivacy leakage, while the keys encoder can pro-\nvide better query results. While it is not a common\npractice in kNN-LMs, we view the separation of\nkey and query encoders as a promising approach\nto reduce the discrepancy between the prompt and\nthe datastore, and reduce privacy leakage.\nThe privacy risk of a kNN-LM can also be im-\npacted by its hyper-parameters such as the number\nof neighbors k, and the interpolation coefficient λ.\nIt is important to consider these hyper-parameters\nin the customization of the kNN-LMs to ensure\nthat the privacy-utility trade-off is well managed.\n5.3 Results of Sanitization and Decoupling\nEncoders\nAs demonstrated in Table 3, applying sanitization\nto both the encoder and the datastore effectively\neliminates privacy risk, resulting in no person-\nally identifiable information (PII) being extracted.\nAmong the three methods, the strategy of replacing\nPII with random public information for sanitization\nyields the highest utility. It achieves a perplexity\nof 16.38, which is only marginally worse than the\nperplexity of 16.12 achieved by the non-sanitized\nprivate model.\nTable 3 also demonstrates that utilizing separate\nencoders for keys and queries enhances the model’s\nutility compared to using the same sanitized en-\ncoder for both. Specifically, we observe that when\nusing the non-sanitized encoder for the query and\nthe sanitized encoder for the key, privacy risks re-\nmain high due to the potential leakage from the\npLM. On the other hand, using the non-sanitized\nencoder for the key and the sanitized encoder for\nthe query effectively eliminates privacy risk while\nstill maintaining a high level of utility. This finding\nhighlights the importance of sanitizing the query\nencoder in kNN-LMs.\n5.4 Effect of Hyper-parameters\nWe finally analyze the impact of key hyper-\nparameters on utility and privacy risks in kNN-\nLMs, using Dprivate as datastore and Encprivate for\nboth EncK and EncQ. First, we vary λ, the inter-\n0.0 0.2 0.4\nλ\n0\n20\n40\n60\n80\n100# Total PIIs\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\nPerplexity\n101 103\nk\n0\n20\n40\n60\n80\n100# Total PIIs\n14\n16\n18\n20\n22\n24\nPerplexity\nFigure 2: Effect of the interpolation coefficient (λ) and\nthe number of nearest neighbors (k) on kNN-LMs’ util-\nity (measured by perplexity, the blue curve) and privacy\nrisk (measured by the number of reconstructed PIIs in\nthe targeted attack, the green curve). We use Encprivate\nas encoders and Dprivate as the datastore.\npolation coefficient, and observe that increasing λ\ndecreases perplexity but increases privacy risk (see\nFigure 2). This highlights the trade-off between\naccuracy and privacy, indicating that optimizing\nboth factors simultaneously is challenging through\nλadjustment alone.\nkis the number of nearest neighbors in a kNN-\nLM. As shown in Figure 2, increasing the value\nof kimproves perplexity as it allows considering\nmore nearest neighbors. We also notice that using\na larger kdecreases privacy risk as the model be-\ncomes less influenced by a limited group of private\nnearest neighbors. Together, increasing kseems to\nsimultaneously enhance utility and reduce risk.\n6 Mitigations Against Untargeted Risks\nIn this section, we explore potential methods to\nmitigate untargeted risks in kNN-LMs, which is a\nmore challenging setting due to the opacity of the\ndefinition of privacy. It is important to note that\nthe methods presented in this study are preliminary\nattempts, and fully addressing untargeted risks in\nkNN-LMs still remains a challenging task.\n6.1 Methods\nConsidering that storing Dprivate in the datastore is\nthe primary cause of data leakage (as discussed in\nSection 4), and the challenge of sanitizing private\ndata in the face of untargeted risks, we propose the\nfollowing approaches to leverage public data for\nmitigating these risks.\nAdding public data to datastore The quality of\nthe retrieved neighbors plays a crucial role in the\nperformance and accuracy of kNN-LMs. Although\nit is uncommon to include public datapoints that\nare not specifically designed for the task or domain\n14893\nDATASTORE EVAL. PPL # GOOD RECONNpub Npriv\n0 ALL 16.12 656\n0 0 20.63 620 (↓5.5%)\nALL\n0 21.46 561 (↓14.5%)\n5, 000 21.58 579 (↓11.7%)\n10, 000 21.23 595 (↓9.3%)\n50, 000 19.16 617 (↓5.9%)\nALL 19.02 632 (↓3.7%)\nTable 4: Perplexity and data extraction risks for kNN-\nLMs with different numbers of public (Npub) and pri-\nvate (Npriv) examples in the datastore. The encoder in\nuse is the privately fine-tuned encoder Encprivate. Pri-\nvacy measurements are computed using the top 5000\ncandidates. Risk reduction (%) compared to the first row\nis annotated. Table 12 in Appendix D presents similar\nfindings on the Medical Transcriptions dataset.\ninto kNN-LMs’ datastore, it could potentially aid\nin reducing privacy risks in applications that priori-\ntize privacy. This becomes particularly relevant in\nlight of previous findings, which suggest substan-\ntial privacy leakage from a private datastore.\nFine-tuning encoders on private data with DP-\nSGD Differentially private stochastic gradient de-\nscent (DP-SGD) (Abadi et al., 2016) is a recipe for\ntraining a deep learning model with differential\nprivacy (Dwork et al., 2006b) guarantee to protect\nprivacy leakage. It operates by modifying the mini-\nbatch stochastic optimization process through the\nuse of per-example gradient clipping and Gaussian\nnoise injection (See Appendix C for details).\nFine-tuning encoders on a mixture of public and\nprivate data However, adding public data can\npotentially lead to a decrease in retrieval perfor-\nmance as there is a distribution gap between the\npublic data (e.g., Web Crawl data) used to con-\nstruct the datastore and the private data (e.g., email\nconversations) used for encoder fine-tuning. To\naddress this issue, we propose further fine-tuning\nthe encoder on a combination of public and private\ndata to bridge the distribution gap and improve re-\ntrieval accuracy. The ratio for combining public\nand private datasets will be determined empirically\nthrough experimentation.\nSimilarly to Section 5.2, we could also employ\nseparate encoders for keys and queries in the con-\ntext of untargeted risks, which allows for more\nprecise control over privacy preservation.\n6.2 Experimental Results\nWe mainly present our findings using the Enron\nEmail dataset. In Appendix B, we provide results\nDATA EncK EncQ\nEVAL. # GOOD\nSTORE PPL RECON\nDprivate Encprivate Encprivate 16.12 656\nDpublic\nEncprivate Encprivate 21.46 561 (↓14.5%)\nEncpublic Encpublic 33.60 0 (↓100%)\nEncprivate Encpublic 31.57 54 (↓91.8%)\nEncpublic Encprivate 22.90 451 (↓31.3%)\nEncDP EncDP 22.83 540 (↓16.9%)\nEncprivate EncDP 21.57 603 (↓7.2%)\nEncDP Encprivate 21.62 594 (↓8.6%)\nEncmixed Encmixed 21.10 601 (↓8.4%)\nEncprivate Encmixed 21.12 545 (↓16.9%)\nEncmixed Encprivate 21.43 498 (↓24.1%)\nTable 5: Perplexity and data extraction risks for kNN-\nLMs with different encoders for keys ( EncK) and\nqueries (EncQ). Risk reduction ( %) compared to the\nfirst row is annotated. Privacy measurements are com-\nputed using the top 5000 candidates. The EncDP en-\ncoder is fine-tuned using DP-SGD with privacy budget\nε = 10.0. The Encmixed encoder is fine-tuned using\na mix of public and private data points. Results sug-\ngest that using different encoders for keys and queries\nor Encmixed can improve the privacy-utility trade-off.\nTable 13 in Appendix D presents similar findings on the\nMedical Transcriptions dataset.\nfrom the Medical Transcriptions dataset, and those\nfindings align with our main findings.\nTable 4 demonstrates that when a privately fine-\ntuned model Encprivate serves as the encoder, re-\nplacing the private datastore Dprivate with a public\none Dpublic in kNN-LMs considerably lowers the\nprivacy risk. Furthermore, when using Encprivate\nand Dpublic, the risk level is slightly lower than\nwhen using the standard language model with\nEncprivate because the model’s final response has\nbeen interpolated with non-sensitive information,\nwhich helps to reduce privacy risks.\nUsing a public datastore reduces privacy risk but\nalso results in a sudden drop in utility. If more\nstringent utility requirements but less strict privacy\nconstraints are necessary, adding a few private ex-\namples to the public datastore, as shown in Table 4,\nmay also be a suitable solution.\nTable 5 demonstrates that using different en-\ncoders for keys (EncK) and queries (EncQ) is more\neffective in achieving a desirable balance between\nprivacy and utility when using Dpublic as the datas-\ntore. Specifically, using Encprivate to encode keys\nand Encpublic to encode queries significantly re-\nduces the risk of data extraction with only a slight\ndecrease in perplexity.\nWe also note that fine-tuning the encoder using\nDP-SGD only helps slightly reduce the extraction\n14894\nrisk, despite the relatively strict privacy budgetε=\n10.0. This is because due to the existence of a\nprivate datastore, each inference query in the kNN-\nLM process incurs supplementary privacy costs,\nleading to the final kNN-LM model not satisfying\nthe ε-Differential Privacy criteria.\nWe further try fine-tuning the encoder using a\ncombination of public and private data, which re-\nsults in Encmixed. The training dataset comprises\nthe entire set of private data of size Npriv and\nNpriv ×rpublic data, where rtakes values from\n{0.01,0.02,0.05,0.1,0.2,0.5,1.0}. We present at-\ntack results using r= 0.05 as it achieves the best\nperplexity. As shown in Table 5, when the encoder\nis fine-tuned using a combination of public and\nprivate data, the perplexity can be enhanced from\n21.46 to 21.10 while simultaneously reducing pri-\nvacy risk. This is because Encmixed helps close the\ndistribution gap between private and public data\nthus improving the retrieval results. Similarly, us-\ning separate EncK and EncQ also helps further\nreduce the privacy risk.\n7 Related Work\n7.1 Retrieval-based Language Models\nRetrieval-based language models (Khandelwal\net al., 2020; Borgeaud et al., 2022; Izacard et al.,\n2022; Zhong et al., 2022; Min et al., 2023; Shi\net al., 2023) have been widely studied in recent\nyears. These models not only rely on encoder for-\nward running but also leverage a non-parametric\ncomponent to incorporate more knowledge from an\nexternal datastore during inference. The retrieval\nprocess starts by using the input as a query, and\nthen retrieving a set of documents (i.e., sequences\nof tokens) from a corpus. The language model\nfinally incorporates these retrieved documents as\nadditional information to make its final prediction.\nWhile the deployment of retrieval-based language\nmodels has been shown to lead to improved perfor-\nmance on various NLP tasks, including language\nmodeling and open-domain question answering, it\nalso poses concerns about data privacy.\n7.2 Privacy Risks in Language Models\nLanguage models have been shown to tend to mem-\norize (Carlini et al., 2019; Thakkar et al., 2021;\nZhang et al., 2021; Carlini et al., 2023; Asai et al.,\n2023) their training data and thus can be prompted\nto output text sequences from its training data (Car-\nlini et al., 2021), as well as highly sensitive informa-\ntion such as personal email addresses (Huang et al.,\n2022) and protected health information(Lehman\net al., 2021; Pan et al., 2020). Recently, the mem-\norization effect in LMs has been further exploited\nin the federated learning setting (Kone ˇcn`y et al.,\n2016), where in combination with the information\nleakage from model updates (Melis et al., 2019;\nHuang et al., 2020), the attacker is capable of recov-\nering private text in federated learning (Gupta et al.,\n2022). To mitigate privacy risks, there is a grow-\ning interest in making language models privacy-\npreserving (Yu et al., 2022; Li et al., 2022; Shi\net al., 2022b; Yue et al., 2023; Cummings et al.,\n2023) by training them with a differential privacy\nguarantee (Dwork et al., 2006b; Abadi et al., 2016)\nor with various anonymization approaches (Naka-\nmura et al., 2020; Biesner et al.).\nAlthough previous research has demonstrated\nthe potential risks of data extraction in parametric\nlanguage models, our study is the first investigation\nof the privacy risks associated with retrieval-based\nlanguage models; we also propose strategies to\nmitigate them. The closest effort is Arora et al.\n(2022), which explores the privacy concerns of\nusing private data in information retrieval systems\nand provides potential mitigations. However, their\nwork is not specifically tailored to the context of\nretrieval-based language models.\n8 Conclusion\nThis work presents the first study of privacy risks in\nretrieval-based language models, specifically focus-\ning on kNN-LMs. Our objective is to investigate\ndesigns and training methodologies for kNN-LMs\nthat strike a better privacy-utility trade-off.\nThere are several conclusions from our investi-\ngation. First, our empirical study reveals that in-\ncorporating a private datastore in kNN-LMs leads\nto increased privacy risks (both targeted and un-\ntargeted) compared to parametric language mod-\nels trained on private data. Second, for targeted\nattacks, our experimental study shows that sanitiz-\ning kNN-LMs to remove private information from\nboth the datastore and encoders, and decoupling\nthe encoders for keys and queries can eliminate the\nprivacy risks without sacrificing utility, achieving\nperplexity of 16.38 (vs. 16.12). Third, for untar-\ngeted attacks, our study shows that using a public\ndatastore and training the encoder on a combina-\ntion of public and private data can reduce privacy\nrisks at the expense of reduced utility by 24.1%,\nwith perplexity of 21.12 (vs. 16.12).\n14895\nLimitations\nWe discuss the limitations of this work as follows.\n• The current study mainly demonstrates the\nprivacy implications of nearest neighbor lan-\nguage models, but there are many other vari-\nants of retrieval-based language models, such\nas RETRO (Borgeaud et al., 2022) and At-\nlas (Izacard et al., 2022). Further study is\nneeded to understand privacy implications of\nthese models and whether our findings apply.\n• In the current study, we use WikiText-103\nas the public domain for Enron Email, and\nPubMed-Patients for Medical Transcriptions.\nWhile we believe that these choices of public\ndatasets are realistic, it is important to recog-\nnize that this selection may restrict the gener-\nalizability of our findings. We acknowledge\nthis limitation and leave the exploration of al-\nternative options for the public dataset as a\ndirection for future work.\n• Furthermore, an unexplored aspect of our\nstudy is the potential combination of pro-\nposed strategies, such as decoupling keys and\nquery encoders, with more diverse privacy-\npreserving techniques.\nAcknowledgement\nThis project is supported by an NSF CAREER\naward (IIS-2239290), a Sloan Research Fellowship,\na Meta research grant, and a Princeton SEAS Inno-\nvation Grant. We would like to extend our sincere\nappreciation to Dan Friedman, Jiayi Geng, Zirui\nWang, Alexander Wettig, and Zhiyuan Zeng for\ntheir valuable comments and feedback on earlier\nversions of this work.\nReferences\nMartin Abadi, Andy Chu, Ian Goodfellow, H Bren-\ndan McMahan, Ilya Mironov, Kunal Talwar, and\nLi Zhang. 2016. Deep learning with differential pri-\nvacy. In Proceedings of the 2016 ACM SIGSAC con-\nference on computer and communications security,\npages 308–318.\nSimran Arora, Patrick Lewis, Angela Fan, Jacob Kahn,\nand Christopher Ré. 2022. Reasoning over public\nand private data in retrieval-based systems. arXiv\npreprint arXiv:2203.11027.\nAkari Asai, Sewon Min, Zexuan Zhong, and Danqi\nChen. 2023. Retrieval-based language models and\napplications. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 6: Tutorial Abstracts), pages 41–46.\nMislav Balunovic, Dimitar Dimitrov, Nikola Jovanovi´c,\nand Martin Vechev. 2022. Lamp: Extracting text\nfrom gradients with language model priors. Ad-\nvances in Neural Information Processing Systems,\n35:7641–7654.\nDavid Biesner, Rajkumar Ramamurthy, Robin Sten-\nzel, Max Lübbering, Lars Hillebrand, Anna Ladi,\nMaren Pielka, Rüdiger Loitz, Christian Bauckhage,\nand Rafet Sifa. Anonymization of german finan-\ncial documents using neural network-based language\nmodels with contextual word representations. In-\nternational Journal of Data Science and Analytics,\npages 1–11.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International Conference on Ma-\nchine Learning (ICML). PMLR.\nCalifornia State Legislature . 2018. The California\nConsumer Privacy Act of 2018 (CCPA). Online at\nhttps://cppa.ca.gov/regulations/pdf/cppa_act.pdf.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\nKatherine Lee, Florian Tramer, and Chiyuan Zhang.\n2023. Quantifying memorization across neural lan-\nguage models. In International Conference on Learn-\ning Representations (ICLR).\nNicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej\nKos, and Dawn Song. 2019. The secret sharer: Eval-\nuating and testing unintended memorization in neu-\nral networks. In 28th USENIX Security Symposium\n(USENIX Security 19), pages 267–284.\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ulfar\nErlingsson, et al. 2021. Extracting training data from\nlarge language models. In 30th USENIX Security\nSymposium (USENIX Security 21), pages 2633–2650.\n14896\nCenters for Medicare & Medicaid Services. 1996.\nThe Health Insurance Portability and Account-\nability Act of 1996 (HIPAA). Online at\nhttp://www.cms.hhs.gov/hipaa/.\nMia Xu Chen, Benjamin N Lee, Gagan Bansal, Yuan\nCao, Shuyuan Zhang, Justin Lu, Jackie Tsay, Yinan\nWang, Andrew M Dai, Zhifeng Chen, et al. 2019.\nGmail smart compose: Real-time assisted writing. In\nProceedings of the 25th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining,\npages 2287–2295.\nRachel Cummings, Damien Desfontaines, David Evans,\nRoxana Geambasu, Matthew Jagielski, Yangsibo\nHuang, Peter Kairouz, Gautam Kamath, Sewoong\nOh, Olga Ohrimenko, et al. 2023. Challenges to-\nwards the next frontier in privacy. arXiv preprint\narXiv:2304.06929.\nJieren Deng, Yijue Wang, Ji Li, Chenghong Wang,\nChao Shang, Hang Liu, Sanguthevar Rajasekaran,\nand Caiwen Ding. 2021. TAG: Gradient attack on\ntransformer-based language models. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, pages 3600–3610, Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nCynthia Dwork, Krishnaram Kenthapadi, Frank McSh-\nerry, Ilya Mironov, and Moni Naor. 2006a. Our data,\nourselves: Privacy via distributed noise generation.\nIn EUROCRYPT, pages 486–503.\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and\nAdam Smith. 2006b. Calibrating noise to sensitivity\nin private data analysis. In Theory of cryptography\nconference. Springer.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018.\nHierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889–898, Melbourne, Australia. Association\nfor Computational Linguistics.\nSamyak Gupta, Yangsibo Huang, Zexuan Zhong,\nTianyu Gao, Kai Li, and Danqi Chen. 2022. Recov-\nering private text in federated learning of language\nmodels. In Advances in Neural Information Process-\ning Systems (NeurIPS).\nJunxian He, Graham Neubig, and Taylor Berg-\nKirkpatrick. 2021. Efficient nearest neighbor lan-\nguage models. In Empirical Methods in Natural\nLanguage Processing (EMNLP).\nJie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang.\n2022. Are large pre-trained language models leaking\nyour personal information? In Findings of Empirical\nMethods in Natural Language Processing (EMNLP).\nYangsibo Huang, Zhao Song, Danqi Chen, Kai Li, and\nSanjeev Arora. 2020. Texthide: Tackling data pri-\nvacy in language understanding tasks. In Findings of\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough Memorization: Nearest Neighbor Language\nModels. In International Conference on Learning\nRepresentations (ICLR).\nBryan Klimt and Yiming Yang. 2004. The enron corpus:\nA new dataset for email classification research. InEu-\nropean Conference on Machine Learning. Springer.\nJakub Koneˇcn`y, H Brendan McMahan, Felix X Yu, Pe-\nter Richtárik, Ananda Theertha Suresh, and Dave\nBacon. 2016. Federated learning: Strategies for im-\nproving communication efficiency. In NIPS Work-\nshop on Private Multi-Party Machine Learning.\nEric Lehman, Sarthak Jain, Karl Pichotta, Yoav Gold-\nberg, and Byron C Wallace. 2021. Does bert pre-\ntrained on clinical notes reveal sensitive data? In\nNorth American Chapter of the Association for Com-\nputational Linguistics (NAACL).\nXuechen Li, Florian Tramer, Percy Liang, and Tatsunori\nHashimoto. 2022. Large language models can be\nstrong differentially private learners. In International\nConference on Learning Representations (ICLR).\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nLuca Melis, Congzheng Song, Emiliano De Cristofaro,\nand Vitaly Shmatikov. 2019. Exploiting unintended\nfeature leakage in collaborative learning. In 2019\nIEEE symposium on security and privacy (SP). IEEE.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In International Conference on Learning Repre-\nsentations (ICLR).\nSewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-\ntau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer.\n2023. Nonparametric masked language modeling. In\nFindings of Association for Computational Linguis-\ntics (ACL).\nYuta Nakamura, Shouhei Hanaoka, Yukihiro No-\nmura, Naoto Hayashi, Osamu Abe, Shuntaro Yada,\nShoko Wakamiya, and Eiji Aramaki. 2020. Kart:\nPrivacy leakage framework of language models\npre-trained with clinical records. arXiv preprint\narXiv:2101.00036.\nArvind Narayanan and Vitaly Shmatikov. 2008. Robust\nde-anonymization of large sparse datasets. In 2008\nIEEE Symposium on Security and Privacy (sp 2008).\nIEEE.\n14897\nXudong Pan, Mi Zhang, Shouling Ji, and Min Yang.\n2020. Privacy risks of general-purpose language\nmodels. In 2020 IEEE Symposium on Security and\nPrivacy (SP).\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nWeijia Shi, Julian Michael, Suchin Gururangan, and\nLuke Zettlemoyer. 2022a. Nearest neighbor zero-\nshot inference.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\nWen-tau Yih. 2023. Replug: Retrieval-augmented\nblack-box language models. In Empirical Methods\nin Natural Language Processing (EMNLP).\nWeiyan Shi, Aiqi Cui, Evan Li, Ruoxi Jia, and Zhou Yu.\n2022b. Selective differential privacy for language\nmodeling. In North American Chapter of the Associ-\nation for Computational Linguistics (NAACL).\nOm Dipakbhai Thakkar, Swaroop Ramaswamy, Rajiv\nMathews, and Francoise Beaufays. 2021. Under-\nstanding unintended memorization in language mod-\nels under federated learning. In Proceedings of the\nThird Workshop on Privacy in Natural Language Pro-\ncessing, pages 1–10, Online. Association for Compu-\ntational Linguistics.\nFrank F Xu, Uri Alon, and Graham Neubig. 2023. Why\ndo nearest neighbor language models work? In Inter-\nnational Conference on Machine Learning (ICML).\nDa Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi,\nHuseyin A Inan, Gautam Kamath, Janardhan Kulka-\nrni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz,\net al. 2022. Differentially private fine-tuning of lan-\nguage models. In International Conference on Learn-\ning Representations (ICLR).\nXiang Yue, Huseyin A Inan, Xuechen Li, Girish Ku-\nmar, Julia McAnallen, Huan Sun, David Levitan, and\nRobert Sim. 2023. Synthetic text generation with\ndifferential privacy: A simple and practical recipe.\nIn Association for Computational Linguistics (ACL).\nChiyuan Zhang, Daphne Ippolito, Katherine Lee,\nMatthew Jagielski, Florian Tramèr, and Nicholas Car-\nlini. 2021. Counterfactual memorization in neural\nlanguage models. arXiv preprint arXiv:2112.12938.\nZexuan Zhong, Tao Lei, and Danqi Chen. 2022. Train-\ning language models with memory augmentation. In\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nA Training Data Extraction Attack\nA.1 Untargeted Attack\nCarlini et al. (2021) proposes the first attack that\ncan extract training data from a trained language\nmodel. The attack consists of two steps: 1) gen-\nerate candidate reconstructions via prompting the\ntrained models, and 2) sort the generated candi-\ndates using a score that implies the possibility of\nbeing a memorized text.\nGenerate candidate reconstructions The at-\ntacker generates candidates for reconstructions via\nquerying the retrieval-augmented LM’s sentence\ncompletion API with contexts. Following Carlini\net al. (2021)5, we randomly select chunks from a\nsubset of Common Crawl 6 to feed as these con-\ntexts.\nSort candidates by calibrated perplexity The\nsecond step is to perform membership inference\non candidates generated from the previous step.\nWe are using the calibrated perplexity in our study,\nwhich has been shown to be the most effective\nmembership metric among all tested ones by Car-\nlini et al. (2021).\nThe perplexity measures how likely the LM\nis to generate a piece of text. Concretely, given\na language model fθ and a sequence of tokens\nx = x1,...,x l, Perplexity(fθ,x) is defined as\nthe exponentiated average negative log-likelihood\nof x:\nexp\n(\n−1\nn\nl∑\ni=1\nlog fθ(xi |x1,...,x i−1)\n)\n(1)\nA low perplexity implies a high likelihood of the\nLM generating the text; For a retrieval-augmented\nLM, this may result from the LM has been trained\non the text or has used the text in its datastore.\nHowever, perplexity may not be a reliable indica-\ntor for membership: common texts may have very\nlow perplexities even though they may not carry\nprivacy-sensitive information. Previous work (Car-\nlini et al., 2019, 2021) propose to filter out these\nuninteresting (yet still high-likelihood samples) by\ncomparing to a second LM which never sees the\n5They have empirically show that sampling conditioned on\nInternet text is the most effective way to identify memorized\ncontent, compared with top-n sampling (Fan et al., 2018) and\ntemperature-base sampling (see Section 5.1.1 in their paper).\n6Common Crawl is a nonprofit organization that crawls the\nweb and freely provides its archives and datasets to the public.\nSee their webpage for details: http://commoncrawl.org/\n14898\nPhone Email URL\nIf you have questions, please feel free to\ngive me a call at\nFor more information, send email to The site can be found at\nPlease advise or call me at For more information please email us at For more information, visit\nPlease call us at Suggestions and feedback are welcome at Please visit our web site at\nI can be reached at For more information please email us at Visit our home page at\nIf you have any questions, please call Please forward this e-mail to For more details go to\nTable 6: Example extraction prompts for different types of PIIs.\nMODEL EVAL. PPL TARGETED ATTACK UNTARGETED ATTACK\nTOTAL PHONE EMAIL URL # GOOD RECON\nkNN-LM Encprivate W/ Dprivate 16.12 54 25 23 6 656\nkNN-LM, EncDP,ε=5.0 W/ Dprivate 29.17 7 3 1 3 54\nkNN-LM, EncDP,ε=5.0 W/ Dprivate 17.55 44 21 17 6 651\nTable 7: Perplexity and data extraction risks for original and DP models on the Enron Email dataset.\nprivate dataset. Specifically, given a piece of text\nxand the target model fθ, and the reference LM\nfref\nθ , the calibrated perpelxity computes the ratio\nPerplexity(fθ,x)/Perplexity(fref\nθ ,x).\nA.2 Targeted Attack\nThe untargeted attack has demonstrated the fea-\nsibility of recovering an entire sentence from the\ndeployed retrieval-augmented LM. However, it is\npossible that only a small segment of a sentence\ncontains sensitive information that can act as per-\nsonal identifiers, and thus be of interest to the at-\ntacker. Therefore, we also consider the type of\nattack which specifically targets this type of infor-\nmation.\nWe define personal identifiers and describe the\nattack method and evaluation subsequently.\nA.2.1 Definition of Personal Identifiers\nPersonal Identifiable Information (PII) refers to\nany data that can be used to identify a specific indi-\nvidual, such as date of birth, home address, email\naddress, and telephone number. PII is considered\nsensitive information and requires proper protec-\ntion to ensure privacy.\nThe exact definition of PII can vary depend-\ning on the jurisdiction, country, and regulations\nin place. One of the clearest definitions of PII\nis provided by Health Insurance Portability and\nAccountability Act (HIPAA) (Centers for Medi-\ncare & Medicaid Services, 1996), which includes\nname, address, date, telephone number, fax num-\nber, email address, social security number, medical\nrecord number, health plan beneficiary number, ac-\ncount number, certificate or license number, vehicle\nidentifiers and serial numbers, web URL, IP Ad-\ndress, finger or voice print, photographic image,\nand any other characteristic that could uniquely\nidentify the individual. In our study, we focus on\nthree frequently investigated PII in previous lit-\nerature (Huang et al., 2022; Carlini et al., 2021),\nincluding email addresses, telephone numbers, and\nURLs.\nA.2.2 The Attack\nIt’s important to note that our approach differs from\nthe work of Huang et al. (2022), which aims to re-\nconstruct the relationship between PIIs and their\nowners7. Instead, our study focuses on reconstruct-\ning the actual valuesof PIIs. This is because, even\nif the attacker cannot determine the relationship\nthrough the current attack, the reconstruction of\nPIIs is already considered identity theft8. Further,\nthe attacker can use the linkage attack(Narayanan\nand Shmatikov, 2008) with the aid of publicly avail-\nable information to determine the relationship be-\ntween PIIs and their owners.\nPII extraction. Similar to the training data ex-\ntraction attack, the PII extraction attack consists of\ntwo steps: 1) generate candidate reconstructions,\nand 2) sort them using membership metrics.\nTo tailor the attack to recover personal identifi-\nable information rather than the entire training text,\nwe customize the attack prompts based on the type\nof information to be extracted.\n7This threat model requires additional information about\nthe presence of the owners in the dataset.\n8https://en.wikipedia.org/wiki/Identity_theft.\n14899\nPII # EMAILS CONTAINING PII # UNIQUE VALUES\nEMAIL 1,013 758\nPHONE 1,921 1,621\nURL 1,641 1,396\nTable 8: Number of PIIs in the Enron Email dataset.\nMODEL EVAL. PPL\n(PARAMETRIC LM) Encpublic,WikiText 31.42\n(PARAMETRIC LM) Encpublic,PubMed 23.83\n(kNN-LM) Encpublic,WikiText W/ Dprivate 14.75\n(kNN-LM) Encpublic,PubMed W/ Dprivate 9.70\nTable 9: Evaluation perplexity of parametric LMs and\nkNN-LMs with different choices of Dpublic. Using\nPubMed as Dpublic results in better perplexity.\nB Experimental Details\nHyper-parameters For each model con-\nfiguration, we search hyper-parameters\nk ∈ { 64,128,256,512,1024,2048} and\nλ ∈ {0.1,0.2,0.3,0.4,0.5} on a held-out\nvalidation set for the best model perplexity.\nPIIs in Enron Email dataset We use regular\nexpressions to identify and extract three types of\npersonal identifiers from the Enron Email training\ndataset for the use of the targeted attack, including\ntelephone numbers, email addresses, and URLs.\nTable 8 provides statistics for these personal identi-\nfiers.\nPrompts for the targeted attack We gather\ncommon preceding context for telephone num-\nbers, email addresses, and URLs, and use them\nas prompts for the targeted attack. Table 6 provides\nexample prompts we use in the attack.\nAttack parameters For the untargeted attack, we\ngenerate 100,000 candidates, and for the targeted\nattack, we generate 10,000 candidates. We use\nbeam search with repetition penalty = 0.75 for the\ngeneration.\nC Defending the Untargeted Attack with\nDifferential Privacy\nC.1 Differential privacy and DP Stochastic\nGradient Descent\nDifferential privacy (DP)(Dwork et al., 2006b,a)\nis a mathematical framework for ensuring the pri-\nvacy of individuals in datasets. It can provide a\nstrong guarantee of privacy by allowing data to be\nanalyzed without revealing sensitive information\nabout any individual in the dataset. Formally, a ran-\ndomized algorithm Ais (ε,δ)-DP if for any two\nneighboring datasets Dand D′(i.e., datasets that\ndiffer by a single individual’s data), and any subset\nSof outputs, it holds that\nPr[A(D) ∈S] ≤eε ·Pr[A(D′) ∈S] +δ.\nHere, ε ∈R>0,δ ∈[0,1) are privacy parameters\nquantifying the privacy guarantee of the algorithm.\nDP Stochastic Gradient Descent (DP-\nSGD) (Abadi et al., 2016) is a recipe for\ntraining a deep learning model with DP by\nmodifying the mini-batch stochastic optimization\nprocess through the use of per-example gradient\nclipping and Gaussian noise injection. When\ntraining an ML model f parameterized by θwith\nthe per-example loss function ℓ(·,·)9 on dataset\nD, each optimization step t involves randomly\nsampling a mini-batch Bt. Given Bt, DP-SGD\nstarts by computing the per-example gradient for\neach (xi,yi) ∈Bt, where xi is the feature vector\nand yi is the corresponding label, as follows:\ngt(xi,yi) ←∇θtℓ(fθt(xi),yi) .\nIt then clips the gradient ℓ2-norm to a maximum\nℓ2-norm of C:\n[gt(xi,yi)]C := gt(xi,yi)/max(1,∥gt(xi,yi)∥2\nC ).\nFinally, it produces the private gradient ˆgt by\ninjecting Gaussian noise into the sum of the clipped\nper-example gradients:\nˆgt ← 1\n∥Bt∥\n(∑\ni[gt(xi,yi)]C + N\n(\n0,σ2C2I\n))\n,\nwhere (0,σ2C2I) is a Gaussian distribution with\nmean 0 and covariance σ2C2I, and the noise multi-\nplier σis computed from (ε,δ) by inverse privacy\naccounting (e.g., Abadi et al. (2016)).\nC.2 Results\nWe also evaluate whether DP can mitigate extrac-\ntion risks in kNN-LMs. Specifically, we fine-tune\nthe pre-trained LM on the private dataset with DP-\nSGD. We vary the privacy budget ε and fix the\nfailure probability δ to be 1/N , where N is the\nnumber of training examples. It’s important to ac-\nknowledge that due to the utilization of a private\ndatastore, each inference query in the kNN-LM\n9The specific loss depends on the particular task and model\n(e.g. cross-entropy loss for classification)\n14900\nExample #1: PAST MEDICAL HISTORY:, He has difficulty climbing stairs, difficulty with airline seats, tying shoes, used to public seating, and lifting objects off\nthe floor. He exercises three times a week at home and does cardio. He has difficulty walking two blocks or five flights of stairs. Difficulty with snoring. He has\nmuscle and joint pains including knee pain, back pain, foot and ankle pain, and swelling. He has gastroesophageal reflux disease...\nExample #2: HISTORY OF PRESENT ILLNESS: ,This is a 55-year-old female with a history of stroke, who presents today for followup of frequency and urgency\nwith urge incontinence. This has been progressively worsening, and previously on VESIcare with no improvement. She continues to take Enablex 50 mg and has not\nnoted any improvement of her symptoms. The nursing home did not do a voiding diary. She is accompanied by her power of attorney...\nExample #3: EXAM: , Ultrasound examination of the scrotum.,REASON FOR EXAM: , Scrotal pain.,FINDINGS: ,Duplex and color flow imaging as well as real\ntime gray-scale imaging of the scrotum and testicles was performed. The left testicle measures 5.1 x 2.8 x 3.0 cm. There is no evidence of intratesticular masses.\nThere is normal Doppler blood flow. The left epididymis has an unremarkable appearance. There is a trace hydrocele...\nExample #4: TESTICULAR ULTRASOUND,REASON FOR EXAM: ,Left testicular swelling for one day.,FINDINGS: ,The left testicle is normal in size and\nattenuation, it measures 3.2 x 1.7 x 2.3 cm. The right epididymis measures up to 9 mm. There is a hydrocele on the right side. Normal flow is seen within the testicle\nand epididymis on the right.,The left testicle is normal in size and attenuation, it measures 3.9 x 2.1 x 2.6 cm...\nExample #5: PHYSICAL EXAMINATION: , The patient is a 63-year-old executive who was seen by his physician for a company physical. He stated that he was in\nexcellent health and led an active life. His physical examination was normal for a man of his age. Chest x-ray and chemical screening blood work were within normal\nlimits. His PSA was elevated.,IMAGING:,Chest x-ray: Normal.,CT scan of abdomen and pelvis: No abnormalities...\nTable 10: Examples from the Medical Transcriptions dataset.\nMODEL EVAL. PPL # GOOD RECON\n(PARAMETRIC LM) Encpublic 23.83 0\n(PARAMETRIC LM) Encprivate 12.00 769\n(kNN-LM) Encpublic W/ Dprivate 9.70 122\n(kNN-LM) Encprivate W/ Dprivate 6.61 812\nTable 11: Perplexity and data extraction risks for various model configurations with the medical transcription dataset.\nThe configuration with the highest leakage is emphasized in red ; the configuration with the lowest leakage is\nhighlighted in green . Privacy measurements are computed using the top 5000 candidates for the untargeted attack.\nDATASTORE EVAL. PPL # GOOD RECONNpub Npriv\n0 ALL 6.61 812\n0 0 12.00 769 (↓5.3%)\nALL\n0 11.32 759 (↓6.5%)\n1, 000 10.54 773 (↓4.8%)\n2, 000 9.29 787 (↓3.1%)\n3, 000 8.26 799 (↓1.6%)\nALL 6.84 804 (↓1.0%)\nTable 12: Perplexity and data extraction risks for kNN-\nLMs with different numbers of public (Npub) and pri-\nvate (Npriv) examples in the datastore. The evaluation\nuses the medical transcription dataset. The encoder in\nuse is the privately fine-tuned encoder Encprivate. Pri-\nvacy measurements are computed using the top 5000\ncandidates.\nprocess incurs supplementary privacy costs, lead-\ning to the final kNN-LM model not satisfying the\n(ε,δ)-Differential Privacy criteria.\nAs demonstrated in Table 7, when εis set to 5.0,\nthe model showcases minimal utility alongside a\nmarginal privacy risk. Conversely, withεraised to\n10.0, the utility closely resembles that of utilizing\nEncprivate in conjunction Dprivate, while concur-\nrently slightly reducing the associated risk. These\nresults suggest that DP is also a viable alternative in\nimproving the utility-privacy trade-off inkNN-LM.\nEncK EncQ EVAL. PPL # GOOD RECON\nDprivate W/ Encprivate 6.61 812\nEncprivate Encprivate 11.32 759 (↓6.5%)\nEncpublic Encpublic 22.55 0 (↓100%)\nEncprivate Encpublic 23.01 7 (↓99.1%)\nEncpublic Encprivate 11.36 689 (↓15.1%)\nEncmixed Encmixed 12.32 773 (↓4.8%)\nEncprivate Encmixed 12.47 707 (↓12.9%)\nEncmixed Encprivate 12.36 692 (↓14.8%)\nTable 13: Perplexity and data extraction risks for kNN-\nLMs with different encoders for keys ( EncK) and\nqueries (EncQ). The datastore in use is the public data-\nstore Dpublic. The evaluation uses the medical tran-\nscription dataset. Privacy measurements are computed\nusing the top 5000 candidates. The Encmixed encoder is\nfine-tuned using a mix of public and private data points.\nResults suggest that using different encoders for keys\nand queries or Encmixed can potentially improve the\nprivacy-utility trade-off.\nD Untargeted Attacks on Medical\nTranscriptions Dataset\nWe primarily showcase our findings using the En-\nron Email dataset in the main paper, as its inclu-\nsion of personally identifiable information (PII)\nenables us to effectively evaluate both targeted and\nuntargeted attacks. To validate our findings, we\nhereby replicate our experiments specifically for\n14901\nuntargeted attacks on the Medical Transcriptions\ndataset.\nD.1 Experimental setup\nThe Medical Transcriptions dataset 10 contains\n5,000 medical transcriptions for various medical\nspecialties. We use a subset of 4,500 examples\nfor training and the rest for evaluation. Table 10\nprovides examples from the dataset. When using\nthe Medical Transcriptions dataset as Dprivate, we\nopt for PubMed-Patient 11 as Dpublic, rather than\nWikiText. This choice is motivated by the fact\nthat PubMed-Patient exhibits closer semantic align-\nment with the Medical Transcriptions, leading to\nenhanced utility in our evaluation. Further insights\nand justifications regarding this dataset selection\ncan be found in Table 9. We use the GPT-2 base\nmodel (Radford et al., 2019) as Encpublic, and fine-\ntune it on the Medical Transcriptions dataset as\nEncprivate.\nD.2 Results\nThe preliminary findings presented in Table 11\nalign with the observations outlined in Section 4,\nhighlighting a deficiency in the robustness of pri-\nvacy protection within the design of kNN-LMs.\nSpecifically, these results indicate that the pri-\nvacy concern stemming from the private datastore\nDprivate outweighs that resulting from the privately\nfine-tuned model Encprivate.\nOne approach to address this issue is to incor-\nporate a public datastore (Section 6), which helps\nmitigate privacy risks - A small number of private\nexamples can be introduced to the public datas-\ntore, striking a balance between utility and privacy\nconsiderations. Table 12 demonstrates the effec-\ntiveness of this approach, offering a promising com-\npromise.\nWe also observe on the Medicial Transcriptions\ndataset that separating the key and query encoders\nyields better results in striking a favorable trade-off\nbetween privacy and utility. As shown in Table 5,\nemploying distinct encoders, e.g., Encprivate for\nencoding keys and Encpublic for encoding queries,\nsubstantially diminishes the likelihood of data ex-\ntraction while only marginally affecting perplexity.\n10https://www.kaggle.com/datasets/tboyle10/\nmedicaltranscriptions\n11PubMed-Patients is derived from 167,000 patient sum-\nmaries extracted from case reports in PubMed Central. Fur-\nther details about this dataset can be found at: https://\nhuggingface.co/datasets/zhengyun21/PMC-Patients\n14902"
}