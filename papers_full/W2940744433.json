{
  "title": "Generating Long Sequences with Sparse Transformers",
  "url": "https://openalex.org/W2940744433",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222630523",
      "name": "Child, Rewon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2131432370",
      "name": "Gray Scott",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227412476",
      "name": "Radford, Alec",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226591326",
      "name": "Sutskever, Ilya",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2906625520",
    "https://openalex.org/W2763421725",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W2886490473",
    "https://openalex.org/W2953318193",
    "https://openalex.org/W2519091744",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W1793121960",
    "https://openalex.org/W2963139417",
    "https://openalex.org/W2962942158",
    "https://openalex.org/W2787214294",
    "https://openalex.org/W2773781902",
    "https://openalex.org/W2964122153",
    "https://openalex.org/W2338908902",
    "https://openalex.org/W2949427019",
    "https://openalex.org/W2962776038",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2953331651",
    "https://openalex.org/W2778792233",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2724346673",
    "https://openalex.org/W2952276042",
    "https://openalex.org/W2950946978",
    "https://openalex.org/W2594961016",
    "https://openalex.org/W2891815651"
  ],
  "abstract": "Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \\sqrt{n})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.",
  "full_text": "Generating Long Sequences with Sparse Transformers\nRewon Child 1 Scott Gray 1 Alec Radford 1 Ilya Sutskever 1\nAbstract\nTransformers are powerful sequence models, but\nrequire time and memory that grows quadrati-\ncally with the sequence length. In this paper we\nintroduce sparse factorizations of the attention\nmatrix which reduce this to O(n√n). We also\nintroduce a) a variation on architecture and initial-\nization to train deeper networks, b) the recompu-\ntation of attention matrices to save memory, and\nc) fast attention kernels for training. We call net-\nworks with these changes Sparse Transformers,\nand show they can model sequences tens of thou-\nsands of timesteps long using hundreds of layers.\nWe use the same architecture to model images,\naudio, and text from raw bytes, setting a new state\nof the art for density modeling of Enwik8, CIFAR-\n10, and ImageNet-64. We generate unconditional\nsamples that demonstrate global coherence and\ngreat diversity, and show it is possible in principle\nto use self-attention to model sequences of length\none million or more.\n1. Introduction\nEstimating complex, high-dimensional data distributions is\na central problem in unsupervised learning, as many down-\nstream applications of interest involve generation of text,\nimages, audio, and other data. Additionally, it is believed to\nbe a key component of unsupervised representation learning.\nRecently, neural autoregressive models have achieved im-\npressive results in this domain, achieving state-of-the-art in\nmodeling natural language (Jozefowicz et al., 2016) (Rad-\nford et al., 2018) (Dai et al., 2018), raw audio (Van Den Oord\net al., 2016) (Mehri et al., 2016), and images (Oord et al.,\n2016) (Menick & Kalchbrenner, 2018) (Salimans et al.,\n2017) (Reed et al., 2017) (Chen et al., 2017).\nThese methods decompose a joint probability distribution\ninto a product of conditional ones. Modeling these condi-\ntional distributions is extremely challenging, however, as\nthey contain many complex, long-range dependencies and\nrequire a suitably expressive model architecture to learn\nthem.\nArchitectures based off CNNs (Oord et al., 2016) have made\nFigure 1.Unconditional samples from our neural autoregressive\nmodel on ImageNet 64 and a classical music dataset. We used the\nsame self-attention based architecture for audio, images, and text.\nThe samples above were generated with softmax temperature 1.0,\nand had lengths 12,288 and 65,536. Audio samples be listened to at\nhttps://openai.com/blog/sparse-transformer\ngreat progress in this direction, but require signiﬁcant depth\nto expand their receptive ﬁeld. To address this, WaveNet\n(Van Den Oord et al., 2016) introduced dilated convolutions,\nwhich allowed the network to model long-range dependen-\ncies in a logarithmic number of layers.\nSeparately, the Transformer (Vaswani et al., 2017) has been\nshown to excel on many natural language tasks, which may\nbe in part due to its ability to model arbitrary dependencies\nin a constant number of layers. As each self-attention layer\nhas a global receptive ﬁeld, the network can allocate rep-\nresentational capacity to the input regions for which it is\narXiv:1904.10509v1  [cs.LG]  23 Apr 2019\nGenerating Long Sequences with Sparse Transformers\nmost useful. Thus the architecture may be more ﬂexible\nat generating diverse data types than networks with ﬁxed\nconnectivity patterns.\nHowever, the memory and computational requirements of\nsuch networks grows quadratically with sequence length,\nwhich excludes their use on long sequences.\nThe main contribution of this work is to introduce several\nsparse factorizations of the attention matrix, which scale\nas O(np√n) with the sequence length without sacriﬁcing\nperformance. These work by separating the full attention\ncomputation into several faster attention operations which,\nwhen combined, can approximate the dense attention oper-\nation. We use this to apply self-attention to sequences of\nunprecedented length.\nAdditionally, we introduce several other changes to the\nTransformer, including:\n• A restructured residual block and weight initialization\nto improve training of very deep networks\n• A set of sparse attention kernels which efﬁciently com-\npute subsets of the attention matrix\n• Recomputation of attention weights during the back-\nwards pass to reduce memory usage\nWe empirically validate that models augmented in this man-\nner can achieve state-of-the-art compression and generation\nof natural language, raw audio, and natural images. The\nsimplicity of the architecture leads us to believe it may be\nuseful for many problems of interest.\n2. Related Work\nThe most related work involves other techniques for scaling\nup autoregressive generative models. For images, (Reed\net al., 2017) models conditional independence between the\npixels in order to generate many locations in parallel, and\n(Menick & Kalchbrenner, 2018) imposes an ordering and\nmulti-scale upsampling procedure to generate high ﬁdelity\nsamples. (Parmar et al., 2018) uses blocks of local attention\nto apply Transformers to images. For text, (Dai et al., 2018)\nintroduces a state reuse ”memory” for modeling long-term\ndependencies. And for audio, in addition to (Van Den Oord\net al., 2016), (Mehri et al., 2016) used a hierarchical struc-\nture and RNNs of varying clock-rates to use long contexts\nduring inference, similar to (Koutnik et al., 2014). (Huang\net al., 2018) apply Transformers to MIDI generation with\nan efﬁcient relative attention.\nOur work is simpler than many of the techniques above and\ncan be applied equally across images, text, and audio. Many\nof the above techniques are orthogonal to ours, moreover,\nand could be used in conjunction with ours.\nOutside of generative modeling, there are several works\nrelevant to improving the efﬁciency of attention based off\nchunking (Chiu & Raffel, 2017) or using ﬁxed length repre-\nsentations (Britz et al., 2017). Other works have investigated\nattention with multiple ”hops”, such as (Sukhbaatar et al.,\n2015) and (Gehring et al., 2017).\nIt is worth noting that the Gated Pixel CNN (Oord et al.,\n2016) and WaveNet (Van Den Oord et al., 2016) use multi-\nplicative interactions in their networks, which are related to\nself-attention.\n3. Background\nWe consider the task of autoregressive sequence gener-\nation, where the joint probability of a sequence x =\n{x1,x2,...,x n}is modeled as the product of conditional\nprobability distributions and parameterized by a network θ.\np(x) =\nn∏\ni=1\np(xi|x1,...,x i−1; θ) (1)\nWe treat images, text, and audio as a sequence of discrete\ntokens, typically raw bytes. The network θtakes in the se-\nquence of tokens and outputs a categorical distribution over\nthe vpossible values of the next token using the softmax\nfunction, where vis the size of the vocabulary. The training\nobjective is to maximize the log-probability of the data with\nrespect to θ.\nA simple and powerful choice for model θis a Transformer\n(Vaswani et al., 2017) in decoder-only mode, as demon-\nstrated by (Radford et al., 2018) and (Liu et al., 2018). These\nmodels transform the input sequence with blocks of mul-\ntihead self-attention over the entire sequence, followed by\ndense transformations over each sequence element. The self-\nattention portion of the network must computenweightings\nfor each of nelements, however, which can quickly become\nintractable as the sequence length grows.\nIn the following sections, we describe our modiﬁcations to\nthe Transformer architecture which make it more suitable\nfor modeling long sequences.\n4. Factorized Self-Attention\nSparse Transformers separate the full self-attention opera-\ntion across several steps of attention, as visualized in Figure\n3(b) and 3(c). To motivate our approach, we ﬁrst perform\na qualitative assessment of attention patterns learned by a\nstandard Transformer on an image dataset.\nGenerating Long Sequences with Sparse Transformers\nFigure 2.Learned attention patterns from a 128-layer network on CIFAR-10 trained with full attention. White highlights denote attention\nweights for a head while generating a given pixel, and black denotes the autoregressive mask. Layers are able to learn a variety of\nspecialized sparse structures, which may explain their ability to adapt to different domains. a) Many early layers in the network learn\nlocally connected patterns, which resemble convolution. b) In layers 19 and 20, the network learned to split the attention across a\nrow attention and column attention, effectively factorizing the global attention calculation. c) Several attention layers showed global,\ndata-dependent access patterns. d) Typical layers in layers 64-128 exhibited high sparsity, with positions activating rarely and only for\nspeciﬁc input patterns.\n(a) Transformer\n (b) Sparse Transformer (strided)\n (c) Sparse Transformer (ﬁxed)\nFigure 3.Two 2d factorized attention schemes we evaluated in comparison to the full attention of a standard Transformer (a). The top\nrow indicates, for an example 6x6 image, which positions two attention heads receive as input when computing a given output. The\nbottom row shows the connectivity matrix (not to scale) between all such outputs (rows) and inputs (columns). Sparsity in the connectivity\nmatrix can lead to signiﬁcantly faster computation. In (b) and (c), full connectivity between elements is preserved when the two heads are\ncomputed sequentially. We tested whether such factorizations could match in performance the rich connectivity patterns of Figure 2.\nGenerating Long Sequences with Sparse Transformers\n4.1. Qualitative assessment of learned attention\npatterns\nWe visualized the attention patterns learned by a 128-layer\nself-attention network on CIFAR-10, and present several\nexamples in Figure 2. Visual inspection showed that most\nlayers had sparse attention patterns across most data points,\nsuggesting that some form of sparsity could be introduced\nwithout signiﬁcantly affecting performance. Several layers\n(Figure 2c) clearly exhibited global patterns, however, and\nothers exhibited data-dependent sparsity (Figure 2d), both\nof which would be impacted by introducing a predetermined\nsparsity pattern into all of the attention matrices.\nIn this paper, we restricted our investigation to a class of\nsparse attention patterns that have connectivity between all\npositions over several steps of attention. These methods can\nbe more efﬁcient than full attention while still providing\nglobal context to any given position. We aimed to empiri-\ncally validate the performance of these factorized patterns\non a range of tasks, given that they are unable to learn the\nexact same mappings as those in Figure 2. We present the\nformulation of factorized attention below.\n4.2. Factorized self-attention\nA self-attention layer maps a matrix of input embeddings\nX to an output matrix and is parameterized by a connectiv-\nity pattern S = {S1,...,S n}, where Si denotes the set of\nindices of the input vectors to which the ith output vector\nattends. The output vector is a weighted sum of transforma-\ntions of the input vectors:\nAttend(X,S) =\n(\na(xi,Si)\n)\ni∈{1,...,n}\n(2)\na(xi,Si) = softmax\n(\n(Wqxi)KT\nSi√\nd\n)\nVSi (3)\nKSi =\n(\nWkxj\n)\nj∈Si\nVSi =\n(\nWvxj\n)\nj∈Si\n(4)\nHere Wq, Wk, and Wv represent the weight matrices which\ntransform a given xi into a query, key, or value, and dis\nthe inner dimension of the queries and keys. The output at\neach position is a sum of the values weighted by the scaled\ndot-product similarity of the keys and queries.\nFull self-attention for autoregressive models deﬁnes Si =\n{j : j ≤i}, allowing every element to attend to all previous\npositions and its own position.\nFactorized self-attention instead has p separate attention\nheads, where the mth head deﬁnes a subset of the indices\nA(m)\ni ⊂ {j : j ≤ i}and lets Si = A(m)\ni . We are\nchieﬂy interested in efﬁcient choices for the subset A, where\n|A(m)\ni |∝ p√n.\nAdditionally, for the time being we consider valid choices\nof A, where all input positions are connected to all future\noutput positions across the psteps of attention.\nFor every j ≤ipair, we set every Asuch that ican attend\nto jthrough a path of locations with maximum length p+ 1.\nSpeciﬁcally, if (j,a,b,c,...,i ) is the path of indices, then\nj ∈A(1)\na , a∈A(2)\nb , b∈A(3)\nc , and so forth.\nThese two criteria allow us keep the ability of Transformers\nto propagate signals from arbitrary input positions to arbi-\ntrary output positions in a constant number of steps, while\nreducing the total effective computation to O(np√n). We\nalso note that softening the validity criterion (for instance,\nhaving a series of only locally connected layers) may be a\nuseful inductive bias for certain domains.\nIn this work, we explore two factorizations forp= 2, which\nwe describe in the following section, though we note that\nthe same techniques can be easily extended to higher dimen-\nsions.\n4.3. Two-dimensional factorized attention\nA natural approach to deﬁning a factorized attention pattern\nin two dimensions is to have one head attend to the previous\nllocations, and the other head attend to every lth location,\nwhere lis the stride and chosen to be close to √n, a method\nwe call strided attention.\nFormally, A(1)\ni = {t,t + 1,...,i }for t = max(0 ,i −l)\nand A(2)\ni = {j : (i−j) mod l = 0}. This pattern can be\nvisualized in Figure 3(b).\nThis formulation is convenient if the data naturally has a\nstructure that aligns with the stride, like images or some\ntypes of music. For data without a periodic structure, like\ntext, however, we ﬁnd that the network can fail to properly\nroute information with the strided pattern, as spatial coor-\ndinates for an element do not necessarily correlate with the\npositions where the element may be most relevant in the\nfuture.\nIn those cases, we instead use a ﬁxed attention pattern (Fig-\nure 3(c)), where speciﬁc cells summarize previous locations\nand propagate that information to all future cells.\nFormally, A(1)\ni = {j : (⌊j/l⌋= ⌊i/l⌋)}, where the brackets\ndenote the ﬂoor operation, and A(2)\ni = {j : j mod l ∈\n{t,t + 1,...,l }, where t= l−cand cis a hyperparameter.\nConcretely, if the stride is 128 and c = 8, then all future\npositions greater than 128 can attend to positions 120-128,\nall positions greater than 256 can attend to 248-256, and so\nforth.\nA ﬁxed-attention pattern with c= 1 limits the expressivity\nof the network signiﬁcantly, as many representations in\nGenerating Long Sequences with Sparse Transformers\nthe network are only used for one block whereas a small\nnumber of locations are used by all blocks. We instead\nfound choosing c ∈{8,16,32}for typical values of l ∈\n{128,256}to perform well, although it should be noted that\nthis increases the computational cost of this method by cin\ncomparison to the strided attention.\nAdditionally, we found that when using multiple heads,\nhaving them attend to distinct subblocks of length cwithin\nthe block of size lwas preferable to having them attend to\nthe same subblock.\nIn the subsequent section, we describe how to incorporate\nfactorized attention into the Sparse Transformer architec-\nture.\n5. Sparse Transformer\nHere we fully describe the Sparse Transformer architecture,\nwhich is a modiﬁed version of the Transformer (Vaswani\net al., 2017).\n5.1. Factorized attention heads\nStandard dense attention simply performs a linear transfor-\nmation of the attend function deﬁned in Equation 2:\nattention(X) = Wp ·attend(X,S) (5)\nwhere Wp denotes the post-attention weight matrix. The\nsimplest technique for integrating factorized self-attention\nis to use one attention type per residual block, and interleave\nthem sequentially or at a ratio determined as a hyperparam-\neter:\nattention(X) = Wp ·attend(X,A(r mod p)) (6)\nHere ris the index of the current residual block and pis the\nnumber of factorized attention heads.\nA second approach is to have a single head attend to the\nlocations of the pixels that both factorized heads would\nattend to, which we call a merged head:\nattention(X) = Wp ·attend(X,\np⋃\nm=1\nA(m)) (7)\nThis is slightly more computationally intensive, but only\nby a constant factor. A third approach is to use multi-head\nattention (Vaswani et al., 2017), wherenh attention products\nare computed in parallel, then concatenated along the feature\ndimension:\nattention(X) = Wp\n(\nattend(X,A)(i)\n)\ni∈{1,...,nh}\n(8)\nembed\nlinear\nsoftmax\nnorm\nnorm\nnorm\ndropout\ndropout\nattention\nfeed-forward\n. . . \nFigure 4.Diagram depicting one residual block of the Sparse Trans-\nformer. The shaded background indicates tensors which are check-\npointed (Chen et al., 2016) and stored in GPU memory. The other\ntensors, including the attention weights and feedforward network\nactivations, are recomputed during the calculation of gradients,\nreducing memory usage substantially.\nHere, the A can be the separate attention patterns, the\nmerged patterns, or interleaved as in Eq. 2. Also, the di-\nmensions of the weight matrices inside the attend function\nare reduced by a factor of 1/nh, such that the number of\nparameters are invariant across values of nh.\nWe typically ﬁnd multiple heads to work well, though for\nextremely long sequences where the attention dominates the\ncomputation time, it is more worthwhile to perform them\none at a time and sequentially.\n5.2. Scaling to hundreds of layers\nWe found that Transformers were difﬁcult to train with\nmany layers, as noted by (Al-Rfou et al., 2018). Instead\nof incorporating auxillary losses, we adopted the following\nGenerating Long Sequences with Sparse Transformers\narchitectural changes.\nFirst, we use the pre-activation residual block of (He et al.,\n2016), deﬁning a network of N layers in the following way:\nH0 = embed(X,We) (9)\nHk = Hk−1 + resblock(Hk−1) (10)\ny= softmax(norm(HN )Wout) (11)\nwhere embed is a function we describe in the next section,\nWout is a weight matrix, and resblock(h) normalizes the\ninput to the attention block and a positionwise feedforward\nnetwork in the following way:\na(H) = dropout(attention(norm(H))) (12)\nb(H) = dropout(ﬀ(norm(H+ a(H)))) (13)\nresblock(H) = a(H) + b(H) (14)\nThe norm function denotes Layer Normalization (Ba et al.,\n2016), and ﬀ(x) = W2 f(W1x+ b1) + b2. Our choice of\nf is the Gaussian Error Linear Unit (Hendrycks & Gimpel,\n2016), f(X) = X⊙sigmoid(1.702 ·X), as used in (Rad-\nford et al., 2018). The output dimension of W1 is 4.0 times\nthe input dimension, unless otherwise noted.\nObserve that HN is the sum of N applications of functions\naand b, and thus each function block receives a gradient\ndirectly from the output layer . We scale the initialization\nof W2 and Wp in Eq. 5 by 1√\n2N to keep the ratio of input\nembedding scale to residual block scale invariant across\nvalues of N.\n5.3. Modeling diverse data types\nIn addition to the embedding of input symbols, positional\nembeddings are typically used in Transformers and other\nlocation-agnostic architectures to encode the spatial relation-\nships of data (Gehring et al., 2017), (Parmar et al., 2018).\nWe found using learned embeddings which either encoded\nthe structure of the data or the factorized attention patterns\nwere important for performance of our models.\nWe added either nemb = ddata or nemb = dattn embed-\ndings to each input location, where ddata refers to the num-\nber of dimensions of the data, and dattn is the number of\ndimensions of the factorized attention. If xi is the one-hot\nencoded ith element in the sequence, and o(j)\ni represents\nthe one-hot encoded position of xi in the jth dimension\n(1 ≤j ≤nemb), then:\nembed(X,We) =\n\nxiWe +\nnemb∑\nj=1\no(j)\ni Wj\n\n\nxi∈X\n(15)\nFor images, we used data embeddings, where ddata = 3\nfor the row, column, and channel location of each input\nbyte. For text and audio, we used two-dimensional attention\nembeddings, where dattn = 2 and the index corresponds to\neach position’s row and column index in a matrix of width\nequal to the stride.\n5.4. Saving memory by recomputing attention weights\nGradient checkpointing has been shown to be effective in\nreducing the memory requirements of training deep neural\nnetworks (Chen et al., 2016), (Gruslys et al., 2016). It is\nworth noting, however, that this technique is particularly\neffective for self-attention layers when long sequences are\nprocessed, as memory usage is high for these layers relative\nto the cost of computing them.\nUsing recomputation alone, we are able to train dense atten-\ntion networks with hundreds of layers on sequence lengths\nof 16,384, which would be infeasible on modern hardware\notherwise.\nIn our experiments, we recompute the attention and feed-\nforward blocks during the backwards pass. To simplify\nour implementation, we do not apply dropout within the\nattention blocks, as in (Vaswani et al., 2017), and instead\nonly apply it at the end of each residual addition, as seen in\nFigure 4.\n5.5. Efﬁcient block-sparse attention kernels\nThe sparse attention masks in 3(b) and 3(c) can be efﬁciently\ncomputed by slicing out sub-blocks from the query, key, and\nvalue matrices and computing the product in blocks. Atten-\ntion over a local window can be computed as-is, whereas\nattention with a stride of kcan be computed by transposing\nthe matrix and computing a local window. Fixed attention\npositions can be aggregated and computed in blocks.\nIn order to ease experimentation, we implemented a set of\nGPU kernels which efﬁciently perform these operations.\nThe softmax operation is fused into a single kernel and\nalso uses registers to eliminate loading the input data more\nthan once, allowing it to run at the same speed as a simple\nnonlinearity. The upper triangle of the attention matrix\nis never computed, moreover, removing the need for the\nnegative bias term of (Vaswani et al., 2017) and halving the\nnumber of operations to be performed.\n5.6. Mixed-precision training\nWe store network weights in single-precision ﬂoating-point,\nbut otherwise compute network activations and gradients in\nhalf-precision, as in (Micikevicius et al., 2017). This acceler-\nates our training due to the usage of Tensor Core operations\non the V100 GPU. During the gradient calculation, we use\nGenerating Long Sequences with Sparse Transformers\nFigure 5.Unconditional samples from ImageNet 64x64, generated with an unmodiﬁed softmax temperature of 1.0. We are able to learn\nlong-range dependencies directly from pixels without using a multi-scale architecture.\ndynamic loss scaling to reduce numerical underﬂow, and\nwe communicate half-precision gradients when averaging\nacross multiple GPUs. When sampling, we cast the queries\nand keys to single-precision, as the query-key product can\nsometimes overﬂow the max value of half-precision.\n6. Training\nWe use the Adam optimizer with a linear warmup of 5000\niterations and a gradient clipping of 1.0, both of which we\nfound important for model stability. We use a weight decay\npenalty of 0.01. We annealed the learning rate according to\na cosine decay as in (Radford et al., 2018). We train on 8\nV100 GPUs unless otherwise noted.\nAll embeddings are of a constant dimension d, usually one\nof {256,512,1024}. By default, all linear transforms are to\nthe same dimension, with the exception of the feed-forward\nnetwork, which projects the input to 4d, unless we use\n“half-size” transformations, where it is 2d. Additionally,\nsometimes we halve the size of the query and key transfor-\nmations.\nWe initialize the token embeddingWe from N(0,0.125√\nd ) and\nthe position embeddings from N(0, 0.125√dnemb\n). Within the\nattention and feedforward components, all biases are initial-\nized to 0 and all weights are initialized from N(0,0.125√din\n)\nwhere din is the fan-in dimension. The weight matrix for\nthe output logits was initialized to 0.\n7. Experiments\nWe empirically test our architecture on density modeling\ntasks including natural images, text, and raw audio. A\nsummary of the results is available in Table 1. We found\nthat, in addition to running signiﬁcantly faster than full\nattention, sparse patterns also converged to lower error, as\nshown in Table 2. This may point to a useful inductive bias\nfrom the sparsity patterns we introduced, or an underlying\noptimization issue with full attention.\n7.1. CIFAR-10\nWe train strided Sparse Transformers on CIFAR-10 images\nrepresented as sequences of 3072 bytes. Models have 2\nheads, 128 layers, d= 256, half-size feedforward network\nand query-key projections, and are trained for 120 epochs\nwith a learning rate of 0.00035 and a dropout rate of 0.25\nuntil validation error stops decreasing.\nWe use 48000 examples for training and 2000 examples for\nvalidation, evaluating the performance of our best models on\nGenerating Long Sequences with Sparse Transformers\nTable 1.Summary of our ﬁndings for density modeling tasks. Re-\nsults are reported in bits per byte, which is equivalent to bits per\ndim for image tasks. M refers to millions of parameters.\nModel Bits per byte\nCIFAR-10\nPixelCNN (Oord et al., 2016) 3.03\nPixelCNN++ (Salimans et al., 2017) 2.92\nImage Transformer (Parmar et al., 2018) 2.90\nPixelSNAIL (Chen et al., 2017) 2.85\nSparse Transformer 59M (strided) 2.80\nEnwik8\nDeeper Self-Attention (Al-Rfou et al., 2018) 1.06\nTransformer-XL 88M (Dai et al., 2018) 1.03\nTransformer-XL 277M (Dai et al., 2018) 0.99\nSparse Transformer 95M (ﬁxed) 0.99\nImageNet 64x64\nPixelCNN (Oord et al., 2016) 3.57\nParallel Multiscale (Reed et al., 2017) 3.7\nGlow (Kingma & Dhariwal, 2018) 3.81\nSPN 150M (Menick & Kalchbrenner, 2018) 3.52\nSparse Transformer 152M (strided) 3.44\nClassical music, 5 seconds at 12 kHz\nSparse Transformer 152M (strided) 1.97\nthe test set. The model achieves 2.80 bits per dim (2.798 ±\n0.004 over seeds 1, 2, 3) versus the previous 2.85 state of\nthe art (Chen et al., 2017). We also compare performance of\ndifferent attention patterns in Table 2. The strided attention\nreaches the lowest error in the shortest amount of time,\nsurpassing the error of dense attention at 2.82 bits per dim.\n7.2. Text\nIn order to assess Sparse Transformers on datasets without\na strong two-dimensional structure, we trained models on\nthe EnWik8 dataset, which represents the ﬁrst 108 bytes\nof Wikipedia and contains a great degree of variability in\nperiodic structure. We trained with a context length of\n12,288, which is longer than previous approaches.\nWe trained on the ﬁrst 90 million tokens and reserved the last\n10 million for validation and test. We used 30-layer ﬁxed\nSparse Transformers with 8 heads, d= 512, and a dropout\nrate of 0.40. We trained for 80 epochs until validation loss\nstopped decreasing. We used a stride of 128, c= 32, and\nmerged the factorized attention heads.\nOur best model reached 0.99 bits per dim (0.992 ±0.001\nover seeds 1, 2, 3), surpassing the 1.03 state-of-the-art for\na similarly-sized Transformer-XL (Dai et al., 2018) and\nmatching the 0.99 of a model trained with more than double\nTable 2.Sparse patterns showed increased speed and also better\nloss on the datasets where we could compare both, which may\npoint to a useful inductive bias in the patterns we learned or an\nunderlying optimization issue with full attention.\nModel Bits per byte Time/Iter\nEnwik8 (12,288 context)\nDense Attention 1.00 1.31\nSparse Transformer (Fixed) 0.99 0.55\nSparse Transformer (Strided) 1.13 0.35\nCIFAR-10 (3,072 context)\nDense Attention 2.82 0.54\nSparse Transformer (Fixed) 2.85 0.47\nSparse Transformer (Strided) 2.80 0.38\nTable 3.We observe increased compression of Enwik8 with longer\ncontexts, suggesting the Sparse Transformer can effectively incor-\nporate long-term dependencies.\nMinimum context length during evaluation Bits per byte\n6,144 tokens 0.9952\n9,216 tokens 0.9936\n10,752 tokens 0.9932\n11,904 tokens 0.9930\n12,096 tokens 0.9922\n12,160 tokens 0.9908\nthe number of parameters. Strided attention failed to do well\non this dataset, whereas ﬁxed patterns were able to recover\nand surpass the performance of dense attention, as listed in\nTable 2.\nAdditionally, during evaluation of the test set, we modiﬁed\nthe minimum context length the network could use by evalu-\nating fewer tokens in parallel. We saw monotonic increases\nin performance with more tokens used, up to 12,160 out\nof the 12,288 tokens used for training (see Table 3), which\nsuggests the network is effectively incorporating long-term\ndependencies.\n7.3. ImageNet 64x64\nIn order to test the ability of the model to learn long range\ndependencies and scale to a large dataset, we train on the\nversion of downsampled ImageNet released by (Oord et al.,\n2016) and evaluate on the validation set. We used a 48 layer\nstrided Sparse Transformer with 16 attention heads and d\n= 512, totaling 152 million parameters. We used a stride\nof 128, a dropout of 0.01, and trained for 70 epochs, which\ntook 7 days on 64 V100 GPUs.\nOur model achieves a loss of3.44 bits per dim (3.437 across\n1 run), in comparison to the previous 3.52 (Menick & Kalch-\nbrenner, 2018).\nGenerating Long Sequences with Sparse Transformers\nAdditionally, we generate unconditional samples (Figure\n5) at an unmodiﬁed softmax temperature of 1.0, from the\nmodel and from one trained with twice the layers (300M\nparameters total). We include here samples from the 300M\nparameter model. On visual assessment we ﬁnd no artifacts\nfrom the sparsity patterns and see evidence of long-term\nstructure in most images.\n7.4. Classical music from raw audio\nTo test the extent to which Sparse Transformers are able\nto scale to very long contexts, we trained models on the\nclassical music dataset released by (Dieleman et al., 2018).\nAs details of the dataset processing are unavailable, we omit\nany direct comparison to other work and instead study what\nsize of Sparse Transformer we can train with increasing\ncontext size. For each sequence length, we attempted to\ntrain the largest model which could entirely ﬁt into 16GB\nV100 accelerators without model parallelism.\nOverall, we found that increasing the sequence length by a\nfactor of 4 requires a reduction in model capacity of approx-\nimately 4\n√\n4 = 8. Thus we found we could use factorized\nself-attention on sequences over 1 million timesteps long,\nalbeit with extremely few parameters (3 million).\nSamples are available for sequences of length 65,536, which\ncorrespond to around 5 seconds of generated audio at 12kHz.\nThe samples clearly demonstrate global coherence over the\nsampled period, and exhibit a variety of play styles and\ntones, swapping from rhythmic playing to forceful. To\nlisten to samples, visit https://openai.com/blog/\nsparse-transformer. Sample quality quickly de-\ngrades for greater sequence lengths due to reduced model\ncapacity.\nTable 4.Performance of a strided Sparse Transformer on a classical\naudio dataset (µ-law encoded at 12 kHz) as a function of sequence\nlength and model size.\nSequence length Parameters Bits per byte\n65,536 152M 1.97\n262,144 25M 2.17\n1,048,576 3M 2.99\n8. Conclusion\nWe introduced Sparse Transformers and showed they attain\nequivalent or better performance on density modeling of\nlong sequences than standard Transformers while requiring\nsigniﬁcantly fewer operations. This performance is state-\nof-the-art in images and text and is easily adaptable to raw\naudio. The model demonstrates usage of long-term context\nand generates globally coherent samples.\n9. Acknowledgements\nWe would like to thank Ashish Vaswani for insightful dis-\ncussions during the genesis of the project. We also thank\nJoshua Meier and Mark Chen for helpful discussions, and\nJohannes Otterbach, Prafulla Dhariwal, and David Luan for\nfeedback on drafts of this paper.\nReferences\nAl-Rfou, R., Choe, D., Constant, N., Guo, M., and Jones,\nL. Character-level language modeling with deeper self-\nattention. arXiv preprint arXiv:1808.04444, 2018.\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.\narXiv preprint arXiv:1607.06450, 2016.\nBritz, D., Guan, M. Y ., and Luong, M.-T. Efﬁcient attention\nusing a ﬁxed-size memory representation. arXiv preprint\narXiv:1707.00110, 2017.\nChen, T., Xu, B., Zhang, C., and Guestrin, C. Training\ndeep nets with sublinear memory cost. arXiv preprint\narXiv:1604.06174, 2016.\nChen, X., Mishra, N., Rohaninejad, M., and Abbeel, P.\nPixelsnail: An improved autoregressive generative model.\narXiv preprint arXiv:1712.09763, 2017.\nChiu, C.-C. and Raffel, C. Monotonic chunkwise attention.\narXiv preprint arXiv:1712.05382, 2017.\nDai, Z., Yang, Z., Yang, Y ., Cohen, W. W., Carbonell, J., Le,\nQ. V ., and Salakhutdinov, R. Transformer-xl: Language\nmodeling with longer-term dependency. 2018.\nDieleman, S., van den Oord, A., and Simonyan, K. The chal-\nlenge of realistic music generation: modelling raw audio\nat scale. In Advances in Neural Information Processing\nSystems, pp. 8000–8010, 2018.\nGehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin,\nY . N. Convolutional sequence to sequence learning.arXiv\npreprint arXiv:1705.03122, 2017.\nGruslys, A., Munos, R., Danihelka, I., Lanctot, M., and\nGraves, A. Memory-efﬁcient backpropagation through\ntime. In Advances in Neural Information Processing\nSystems, pp. 4125–4133, 2016.\nHe, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in\ndeep residual networks. arXiv preprint arXiv:1603.05027,\n2016.\nHendrycks, D. and Gimpel, K. Bridging nonlinearities and\nstochastic regularizers with gaussian error linear units.\narXiv preprint arXiv:1606.08415, 2016.\nGenerating Long Sequences with Sparse Transformers\nHuang, C.-Z. A., Vaswani, A., Uszkoreit, J., Shazeer, N.,\nHawthorne, C., Dai, A. M., Hoffman, M. D., and Eck,\nD. An improved relative self-attention mechanism for\ntransformer with application to music generation. arXiv\npreprint arXiv:1809.04281, 2018.\nJozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., and\nWu, Y . Exploring the limits of language modeling.arXiv\npreprint arXiv:1602.02410, 2016.\nKingma, D. P. and Dhariwal, P. Glow: Generative ﬂow\nwith invertible 1x1 convolutions. In Advances in Neural\nInformation Processing Systems, pp. 10236–10245, 2018.\nKoutnik, J., Greff, K., Gomez, F., and Schmidhuber, J. A\nclockwork rnn. arXiv preprint arXiv:1402.3511, 2014.\nLiu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepa-\nssi, R., Kaiser, L., and Shazeer, N. Generating\nwikipedia by summarizing long sequences.arXiv preprint\narXiv:1801.10198, 2018.\nMehri, S., Kumar, K., Gulrajani, I., Kumar, R., Jain, S.,\nSotelo, J., Courville, A., and Bengio, Y . Samplernn: An\nunconditional end-to-end neural audio generation model.\narXiv preprint arXiv:1612.07837, 2016.\nMenick, J. and Kalchbrenner, N. Generating high ﬁdelity im-\nages with subscale pixel networks and multidimensional\nupscaling. arXiv preprint arXiv:1812.01608, 2018.\nMicikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen,\nE., Garcia, D., Ginsburg, B., Houston, M., Kuchaev, O.,\nVenkatesh, G., et al. Mixed precision training. arXiv\npreprint arXiv:1710.03740, 2017.\nOord, A. v. d., Kalchbrenner, N., and Kavukcuoglu,\nK. Pixel recurrent neural networks. arXiv preprint\narXiv:1601.06759, 2016.\nParmar, N., Vaswani, A., Uszkoreit, J., Kaiser,Ł., Shazeer,\nN., and Ku, A. Image transformer. arXiv preprint\narXiv:1802.05751, 2018.\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever,\nI. Improving language understanding by genera-\ntive pre-training. URL https://s3-us-west-2. ama-\nzonaws. com/openai-assets/research-covers/language-\nunsupervised/language understanding paper. pdf, 2018.\nReed, S., Oord, A. v. d., Kalchbrenner, N., Colmenarejo,\nS. G., Wang, Z., Belov, D., and de Freitas, N. Paral-\nlel multiscale autoregressive density estimation. arXiv\npreprint arXiv:1703.03664, 2017.\nSalimans, T., Karpathy, A., Chen, X., and Kingma, D. P.\nPixelcnn++: Improving the pixelcnn with discretized lo-\ngistic mixture likelihood and other modiﬁcations. arXiv\npreprint arXiv:1701.05517, 2017.\nSukhbaatar, S., Weston, J., Fergus, R., et al. End-to-end\nmemory networks. In Advances in neural information\nprocessing systems, pp. 2440–2448, 2015.\nVan Den Oord, A., Dieleman, S., Zen, H., Simonyan, K.,\nVinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and\nKavukcuoglu, K. Wavenet: A generative model for raw\naudio. CoRR abs/1609.03499, 2016.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-\ntion is all you need. In Advances in Neural Information\nProcessing Systems, pp. 5998–6008, 2017.",
  "topic": "Initialization",
  "concepts": [
    {
      "name": "Initialization",
      "score": 0.8636572360992432
    },
    {
      "name": "Quadratic growth",
      "score": 0.7265004515647888
    },
    {
      "name": "Computer science",
      "score": 0.6850135326385498
    },
    {
      "name": "Byte",
      "score": 0.5627768635749817
    },
    {
      "name": "Transformer",
      "score": 0.5323646068572998
    },
    {
      "name": "Sequence (biology)",
      "score": 0.5024726390838623
    },
    {
      "name": "Sparse matrix",
      "score": 0.4994473457336426
    },
    {
      "name": "Architecture",
      "score": 0.4691530466079712
    },
    {
      "name": "Parallel computing",
      "score": 0.4472709596157074
    },
    {
      "name": "Algorithm",
      "score": 0.43813735246658325
    },
    {
      "name": "Theoretical computer science",
      "score": 0.4339140057563782
    },
    {
      "name": "Coherence (philosophical gambling strategy)",
      "score": 0.41037479043006897
    },
    {
      "name": "Artificial intelligence",
      "score": 0.32208335399627686
    },
    {
      "name": "Mathematics",
      "score": 0.19995087385177612
    },
    {
      "name": "Computer hardware",
      "score": 0.11269265413284302
    },
    {
      "name": "Programming language",
      "score": 0.09029144048690796
    },
    {
      "name": "Engineering",
      "score": 0.08200010657310486
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Gaussian",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}