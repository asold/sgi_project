{
  "title": "Efficient Compressing and Tuning Methods for Large Language Models: A Systematic Literature Review",
  "url": "https://openalex.org/W4409269162",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5065358792",
      "name": "Gun Il Kim",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A5110671375",
      "name": "Suk Min Hwang",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A5067151609",
      "name": "Beakcheol Jang",
      "affiliations": [
        "Yonsei University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3137147200",
    "https://openalex.org/W3034368386",
    "https://openalex.org/W3162922479",
    "https://openalex.org/W4402683925",
    "https://openalex.org/W2106956101",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4386187806",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W4381233128",
    "https://openalex.org/W4394782456",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W4404918643",
    "https://openalex.org/W2623399293",
    "https://openalex.org/W4401042709",
    "https://openalex.org/W398859631",
    "https://openalex.org/W4239019441",
    "https://openalex.org/W3183859557",
    "https://openalex.org/W2607303097",
    "https://openalex.org/W4285306484",
    "https://openalex.org/W4299527668",
    "https://openalex.org/W2606555609",
    "https://openalex.org/W2560674852",
    "https://openalex.org/W2751343396",
    "https://openalex.org/W2930957955",
    "https://openalex.org/W4241886172",
    "https://openalex.org/W4389519153",
    "https://openalex.org/W4234493895",
    "https://openalex.org/W2489487449",
    "https://openalex.org/W4321637333",
    "https://openalex.org/W4241901867",
    "https://openalex.org/W3138154797",
    "https://openalex.org/W2750779823",
    "https://openalex.org/W2768663569"
  ],
  "abstract": "Efficient compression and tuning techniques have become indispensable in addressing the increasing computational and memory demands of large language models (LLMs). While these models have demonstrated exceptional performance across a wide range of natural language processing tasks, their growing size and resource requirements pose significant challenges to accessibility and sustainability. This survey systematically reviews state-of-the-art methods in model compression, including compression techniques such as knowledge distillation, low-rank approximation, parameter pruning, and quantization, as well as tuning techniques such as parameter-efficient fine-tuning and inference optimization. Compression techniques, though well-established in traditional deep learning, require updated methodologies tailored to the scale and dynamics of LLMs. Simultaneously, parameter-efficient fine-tuning, exemplified by techniques like Low-Rank Adaptation (LoRA) and query tuning, emerges as a promising solution for adapting models with minimal resource overhead. This study provides a detailed taxonomy of these methods, examining their practical applications, strengths, and limitations. Critical gaps are identified in scalability, and the integration of compression and tuning strategies, signaling the need for unified frameworks and hybrid approaches to maximize efficiency and performance. By addressing these challenges, this survey aims at guiding researchers toward sustainable, efficient, and accessible LLM development, ensuring their broader applicability across diverse domains while mitigating resource constraints.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8838062286376953
    },
    {
      "name": "Programming language",
      "score": 0.44826751947402954
    },
    {
      "name": "Natural language processing",
      "score": 0.32028043270111084
    }
  ]
}