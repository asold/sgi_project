{
  "title": "Digital forgetting in large language models: a survey of unlearning methods",
  "url": "https://openalex.org/W4406299883",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2116052834",
      "name": "Alberto Blanco-Justicia",
      "affiliations": [
        "Universidad Rovira i Virgili"
      ]
    },
    {
      "id": "https://openalex.org/A4292672667",
      "name": "Najeeb Jebreel",
      "affiliations": [
        "Universidad Rovira i Virgili"
      ]
    },
    {
      "id": "https://openalex.org/A4295845953",
      "name": "Benet Manzanares-Salor",
      "affiliations": [
        "Universidad Rovira i Virgili"
      ]
    },
    {
      "id": "https://openalex.org/A2025964949",
      "name": "David Sánchez",
      "affiliations": [
        "Universidad Rovira i Virgili"
      ]
    },
    {
      "id": "https://openalex.org/A2681033523",
      "name": "Josep Domingo-Ferrer",
      "affiliations": [
        "Universidad Rovira i Virgili"
      ]
    },
    {
      "id": "https://openalex.org/A2694406834",
      "name": "Guillem Collell",
      "affiliations": [
        "Huawei Technologies (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2146201562",
      "name": "Kuan Eeik Tan",
      "affiliations": [
        "Huawei Technologies (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2116052834",
      "name": "Alberto Blanco-Justicia",
      "affiliations": [
        "Universidad Rovira i Virgili"
      ]
    },
    {
      "id": "https://openalex.org/A4292672667",
      "name": "Najeeb Jebreel",
      "affiliations": [
        "Universidad Rovira i Virgili"
      ]
    },
    {
      "id": "https://openalex.org/A4295845953",
      "name": "Benet Manzanares-Salor",
      "affiliations": [
        "Universidad Rovira i Virgili"
      ]
    },
    {
      "id": "https://openalex.org/A2025964949",
      "name": "David Sánchez",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2681033523",
      "name": "Josep Domingo-Ferrer",
      "affiliations": [
        "Universidad Rovira i Virgili"
      ]
    },
    {
      "id": "https://openalex.org/A2694406834",
      "name": "Guillem Collell",
      "affiliations": [
        "Huawei Technologies (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2146201562",
      "name": "Kuan Eeik Tan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2473418344",
    "https://openalex.org/W2951939640",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W2920807444",
    "https://openalex.org/W3154155772",
    "https://openalex.org/W4389519496",
    "https://openalex.org/W4401043305",
    "https://openalex.org/W2909212904",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W2989743967",
    "https://openalex.org/W2982673177",
    "https://openalex.org/W3157562165",
    "https://openalex.org/W4383993628",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W3186138538",
    "https://openalex.org/W4389524389",
    "https://openalex.org/W2962881743",
    "https://openalex.org/W4389518784",
    "https://openalex.org/W4287888682",
    "https://openalex.org/W3201174429",
    "https://openalex.org/W3175115403",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4392902780",
    "https://openalex.org/W4385567201",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W3105882417",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W2951583236",
    "https://openalex.org/W3035241006",
    "https://openalex.org/W2963457723",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W2963378725",
    "https://openalex.org/W1690606251",
    "https://openalex.org/W3035252911",
    "https://openalex.org/W4404293797",
    "https://openalex.org/W2535690855",
    "https://openalex.org/W3034600233",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2952328691",
    "https://openalex.org/W2562898622",
    "https://openalex.org/W2503516461",
    "https://openalex.org/W4402671223",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W4385571042",
    "https://openalex.org/W2920114910",
    "https://openalex.org/W4389524330",
    "https://openalex.org/W2795435272",
    "https://openalex.org/W4385571741",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W2963825865",
    "https://openalex.org/W6600388300",
    "https://openalex.org/W2963526187",
    "https://openalex.org/W3100560913"
  ],
  "abstract": "Abstract Large language models (LLMs) have become the state of the art in natural language processing. The massive adoption of generative LLMs and the capabilities they have shown have prompted public concerns regarding their impact on the labor market, privacy, the use of copyrighted work, and how these models align with human ethics and the rule of law. As a response, new regulations are being pushed, which require developers and service providers to evaluate, monitor, and forestall or at least mitigate the risks posed by their models. One mitigation strategy is digital forgetting: given a model with undesirable knowledge or behavior, the goal is to obtain a new model where the detected issues are no longer present. Digital forgetting is usually enforced via machine unlearning techniques, which modify trained machine learning models for them to behave as models trained on a subset of the original training data. In this work, we describe the motivations and desirable properties of digital forgetting when applied to LLMs, and we survey recent works on machine unlearning. Specifically, we propose a taxonomy of unlearning methods based on the reach and depth of the modifications done on the models, we discuss and compare the effectiveness of machine unlearning methods for LLMs proposed so far, and we survey their evaluation. Finally, we describe open problems of machine unlearning applied to LLMs and we put forward recommendations for developers and practitioners.",
  "full_text": "Accepted: 13 December 2024 / Published online: 13 January 2025\n© The Author(s) 2025\nAlberto Blanco-Justicia, Najeeb Jebreel, Benet Manzanares-Salor, Josep Domingo-Ferrer, Guillem Collell \nand Kuan Eeik Tan contributed equally to this work.\nExtended author information available on the last page of the article\nDigital forgetting in large language models: a survey of \nunlearning methods\nAlberto Blanco-Justicia1 · Najeeb Jebreel1 · Benet Manzanares-Salor1 · \nDavid Sánchez1 · Josep Domingo-Ferrer1 · Guillem Collell2 · Kuan Eeik Tan2\nArtificial Intelligence Review (2025) 58:90\nhttps://doi.org/10.1007/s10462-024-11078-6\nAbstract\nLarge language models (LLMs) have become the state of the art in natural language pro -\ncessing. The massive adoption of generative LLMs and the capabilities they have shown \nhave prompted public concerns regarding their impact on the labor market, privacy, the \nuse of copyrighted work, and how these models align with human ethics and the rule \nof law. As a response, new regulations are being pushed, which require developers and \nservice providers to evaluate, monitor, and forestall or at least mitigate the risks posed by \ntheir models. One mitigation strategy is digital forgetting: given a model with undesirable \nknowledge or behavior, the goal is to obtain a new model where the detected issues are no \nlonger present. Digital forgetting is usually enforced via machine unlearning techniques, \nwhich modify trained machine learning models for them to behave as models trained on \na subset of the original training data. In this work, we describe the motivations and desir -\nable properties of digital forgetting when applied to LLMs, and we survey recent works \non machine unlearning. Specifically, we propose a taxonomy of unlearning methods based \non the reach and depth of the modifications done on the models, we discuss and compare \nthe effectiveness of machine unlearning methods for LLMs proposed so far, and we survey \ntheir evaluation. Finally, we describe open problems of machine unlearning applied to \nLLMs and we put forward recommendations for developers and practitioners.\nKeywords Large language models · Machine unlearning · Privacy · Copyright · \nTrustworthy AI\n1 Introduction\nLarge language models (LLMs) have become the state of the art in most if not all natural \nlanguage processing (NLP) and natural language understanding (NLU) tasks. Since the pub-\nlication of the transformer architecture by Vaswani et al ( 2017), several authors and com -\n et al. [full author details at the end of the article]\n1 3\nA. Blanco-Justicia et al.\npanies have made use of (variations of) this architecture to tackle tasks such as translation, \nsummarization, question answering, sentiment analysis, or text generation.\nWhereas the original transformer consisted of an encoder-decoder architecture, varia -\ntions of this have been introduced in subsequent works. Namely, encoder-only architectures \nsuch as BERT (Devlin et al 2018) –used mostly for text classification–, decoder-only archi-\ntectures such as GPT (Radford et al 2018) – for text generation–, and encoder-decoder archi-\ntectures, such as T5 (Raffel et al 2020) –for text-to-text processing, such as translation–.\nThe training of LLMs occurs in different phases. These phases include a self-supervised \npre-training phase using text data collected from several sources (and sometimes uncu -\nrated), with tasks or training objectives that depend on the type of LLM being trained (e.g., \nmasked language modeling and next word prediction); a fine-tuning phase, where labeled \ndata are used to train specific tasks, such as sentiment analysis, conversational text genera -\ntion, code generation, or summarization; an additional optional round of fine-tuning using \nreinforcement learning from human feedback (RLHF), in which human annotators help \nfine-tune the model by providing feedback on the model responses; and a final phase for \ngenerative models, where specific prompts can be passed to the model to guide it and condi-\ntion its future responses.\nThe announcement and publication of ChatGPT by OpenAI in November 2022 (among \nother large language, image, video, and audio generative models) released the capabilities \nof LLMs to the general public. Since then, several issues have been raised, mainly related \nto the alignment of such models with societal values and the rule of law. Such concerns \nhave elicited political reaction at an international level, with governments and supranational \nentities reaching agreements such as the G7 Hiroshima AI Process (Group of Seven (G7) \n2023), and adopting executive and legislative actions such as the President Biden’s Execu-\ntive Order on the Safe, Secure, and Trustworthy Development and Use of AI (Joseph R. \nBiden Jr. 2023), and the European Union’s AI Act (European Parliament 2024).\nThese recommendations and regulations, along with privacy regulations like the Euro -\npean Union’s General Data Protection Regulation (GDPR) (European Parliament and \nCouncil of the European Union, 2016), require developers and entities that deploy large \n(generative) models to document, monitor, inform about, and solve any potential risks dur-\ning the development and operation of the models. Such risks include the impact of LLMs \non several areas: the labor market, the right to privacy of individuals, copyright laws, the \nfurthering of biases and discrimination, and the potential generation of harmful content, \nincluding content that could be used to damage people.\nOne proposed solution to these issues is digital forgetting, which includes machine \nunlearning techniques. Given a model with undesirable knowledge or behavior, the objec -\ntive of digital forgetting is to obtain a new model where the detected issues are no longer \npresent. However, effective digital forgetting mechanisms have to fulfill potentially conflict-\ning requirements: (i) effectiveness of forgetting, that is, how well the new model has forgot-\nten the undesired knowledge/behavior (either with formal guarantees or through empirical \nevaluation); (ii) performance retained by the model on the desirable tasks; and (iii) cost and \nscalability of the forgetting procedure.\n1 3\n90 Page 2 of 41\nDigital forgetting in large language models: a survey of unlearning…\n1.1 Contributions and differences with other surveys\nIn this paper, we provide an up-to-date survey on digital forgetting and machine unlearning \non LLMs. We first discuss and characterize the motivations, types and requirements of digi-\ntal forgetting in LLMs. Then, we survey approaches to digital forgetting in LLMs and focus \non machine unlearning techniques, which are the most common and effective methods. We \nhave organized the surveyed methods according to the depth and reach of the modifications \nthey perform on the models, from architectural changes to no changes at all, including \nmethods that modify all or part of the model parameters. We also discuss the evaluation of \nunlearning methods in LLMs, including datasets, models, and metrics.\nMost of the surveyed works were published in 2023 and early 2024. Specifically, we \nhave analyzed and compared 22 unlearning methods, 28 datasets used for unlearning and \nevaluation, 31 datasets used for evaluation of retaining, and 15 models (plus size variations) \nused in the unlearning literature.\nFinally, we provide a summary of the challenges of machine unlearning in LLMs and \nwe state recommendations for practitioners on how to choose specific methods depending \non their needs.\nRelated surveys on digital forgetting and machine unlearning, such as Qu et al ( 2023); \nXu et al ( 2023); Nguyen et al ( 2022); Shaik et al ( 2023), have focused on the general ML \ncase by considering models used to classify tabular or image data. However, natural lan -\nguage and LLMs have substantially distinctive qualities that warrant a dedicated study. As a \nconsequence, and also looking at the large corpus of (so far unsurveyed) works on unlearn-\ning for LLMs published in the last 8 months, we believe that an up-to-date survey devoted \nspecifically to this subject is much justified and needed.\n1.2 Organization of this paper\nThis document is organized as follows. Section 2 describes the motivations, types, and \ndesired properties of digital forgetting. Section 3 introduces the approaches to digital for -\ngetting in LLMs, among which unlearning methodologies stand out as the state of the art. \nSection 4 provides a detailed taxonomy of machine unlearning methods for LLMs, and sur-\nveys and compares current approaches. Section 5 details datasets, models and metrics used \nfor the evaluation of forgetting, retaining and runtime. Section 6 discusses challenges and \nrecommendations in the area. Finally, Sect. 7 gathers concluding remarks.\n2 Digital forgetting\nDigital forgetting refers to procedures used to remove information from software systems. \nThis term originated from privacy laws related to the right of erasure or the right to be \nforgotten (e.g. GDPR, Art. 17, (European Parliament and Council of the European Union, \n2016)), which enables data subjects to request service providers to delete information \nrelated to them from their systems. However, forgetting can be extended to any request (by \ndata subjects, regulators, or the entity managing the software system) to remove any infor -\nmation or unwanted behavior from a software system. In the case of LLMs, this translates to \n1 3\nPage 3 of 41 90\nA. Blanco-Justicia et al.\nany modification in the model or the training procedures to remove any influence of specific \ntraining data or specific training procedures on the model’s capabilities or behavior.\nThis section discusses the motivations for digital forgetting, including legal and ethical \nissues, the types of digital forgetting, and the desirable properties of forgetting procedures.\n2.1 Motivations for digital forgetting\nThe need for digital forgetting stems from several ethical principles, national and suprana -\ntional regulations, and codes of conduct. In general, regulations relating to the protection \nof the privacy of individuals ( e.g., the GDPR), the protection of intellectual property ( e.g., \ncopyright laws), and the alignment with human values and the rule of law (Group of Seven \n(G7) 2023; Joseph R. Biden Jr. 2023; European Parliament 2024) motivate the research, \ndiscussion, and implementation of digital forgetting measures in software systems, includ -\ning LLMs.\nIn the following, we categorize and discuss the reasons for implementing digital forget -\nting in LLMs.\n2.1.1 Privacy\nML models, including LLMs, are trained on vast amounts of data, often obtained from open \nsources on the web. Misconfigured services sometimes include private data, like Personally \nIdentifiable Information (PII) or business data, that can be indexed by search engines and \nfreely accessed. These unintentionally public data may end up in the pre-training datas -\nets of LLMs. Additionally, private data may be used to fine-tune models and teach them \nnew downstream tasks. As an example, hospitals may fine-tune pre-trained models using \npatients’ data to teach the models how to diagnose new patients.\nGeneral ML models have been shown to memorize and leak data from the training data-\nset, and this also holds for LLMs. Membership inference attacks (MIA) (Shokri et al 2017; \nSalem et al 2018; Carlini et al 2021) seek to determine whether some information was part \nof the training set of a trained model. The vulnerability of models to such attacks can be used \nto assess the risk of data leakage. Outlying data are especially at risk of being memorized \nand leaked, which in the case of personal data may lead to privacy disclosure (Smith et al \n2023).\n2.1.2 Copyright protection\nDigital forgetting for copyright protection is closely related to privacy, as in both cases \nwe do not want the generated text to include specific information. However, there is a key \ndistinction: whereas in copyright protection the goal is to avoid leakage of verbatim text, \nin privacy protection one wants to avoid disclosing personal information no matter how it \nis expressed.\nA model that can answer questions about an individual and whose answers contain sen -\nsitive details about them is infringing their right to privacy. However, the requirements of \ncopyright protection are different. Copyright laws do not protect ideas, but the exact form in \n1 3\n90 Page 4 of 41\nDigital forgetting in large language models: a survey of unlearning…\nwhich they are expressed. 1 Furthermore, some specific items might be protected by trade -\nmark. Therefore, a model that can answer questions about some copyrighted work does not \nnecessarily infringe copyright law unless verbatim fragments of the work are returned (and \neven in that case, the law includes provisions to lawfully quote protected works).\n2.1.3 Model robustness\nThe whole LLM training pipeline includes pre-training from public data, fine-tuning with \npublic, crowdsourced, or proprietary data, and possibly further fine-tuning with RLHF, \nwhich may also be public, crowdsourced, or closed. In all these phases of training, there is \na possibility to process low-quality information. Datasets used for pre-training may include \nfalse, inconsistent, or outdated information. Datasets used for fine-tuning may be crowd -\nsourced, which opens the door to malicious actors providing wrong information. RLHF \ndepends on human annotators, who rank model outputs based on usefulness and safety. \nRogue annotators may provide wrong information.\nAll these sources of outdated information, misinformation, outliers, noise, and malicious \nreporting may influence the learning process and thus produce underperforming models \nwith potentially critical failures. Moreover, in the case of generative models, problems dur-\ning training may cause the models to hallucinate, although this may be an intrinsic feature \nof generative LLMs caused by the random sampling of output tokens. Forgetting procedures \nmay be used to correct some of these issues.\n2.1.4 Alignment with human values\nLLM pre-training datasets are compiled from diverse, often uncurated sources such as \nweb pages, online encyclopedias, social media posts, and online book libraries. Some of \nthese sources may contribute content misaligned with current societal values, including \ndiscriminatory language based on gender, race, ethnicity, sexual orientation, or age, often \nmanifesting as stereotypes or prejudices. Notably, studies have identified strong correlations \nbetween pronouns (he/she) and different careers. Furthermore, these sources may include \nhateful, violent, or harmful speech. Pre-training models on such data may not only perpetu-\nate these biased and harmful behaviors but even amplify them in some cases. Generative \nmodels may also learn tasks that are not aligned with the predefined goals and which could \nturn out to be harmful, such as how to build weapons.\nML models, including LLMs, must not only protect individual privacy rights, as dis -\ncussed above, but also adhere to ethical values and principles, taking as a reference the \nUniversal Declaration of Human Rights (United Nations 1948), and also including laws and \nsocial norms. Alignment with principles such as non-discrimination, fairness, benevolence, \nand non-maleficence is of utmost importance. In addition, the EU Guidelines for Trust -\nworthy AI (European Commission 2019) require that the machine learning models being \ndeveloped be always under human control and supervision, which means that any deviation \nfrom such principles should be addressed (or addressable).\n1 WIPO Copyright Treaty, Article 2: Scope of Copyright Protection – Copyright protection extends to expres-\nsions and not to ideas, procedures, methods of operation or mathematical concepts as such. https://www \n.wipo.int/w ipolex/en/t ext/2951 66\n1 3\nPage 5 of 41 90\nA. Blanco-Justicia et al.\nTherefore, alignment with ethical values may necessitate forgetting procedures aimed at \nidentifying and eliminating sources of discriminatory or harmful behavior.\n2.2 Types of digital forgetting\nIn this section, we discuss the types of digital forgetting requests that can be found in pre -\nvious surveys on general machine unlearning (Qu et al 2023; Xu et al 2023; Nguyen et al \n2022; Shaik et al 2023). These surveys include forgetting requests for items, features/con -\ncepts, classes, and tasks. Figure 1 visually summarizes these types of forgetting.\nWhile discussing these types of requests, we will describe how they apply to the case of \nnatural language processing and language models.\nItem removal requests. In the general ML case, item removal requests are concerned \nwith one or more specific data points or samples in the training data used to train the model. \nSuch requests are straightforward for models dealing with tabular or image data, where \ndata points are precisely defined. In the realm of tabular data, each row within a table is \nconsidered a data point, with one of the attributes serving as the class label for classification \nor regression. Likewise, in computer vision tasks, individual images are the designated data \npoints.\nHowever, the distinction between items becomes less evident in NLP tasks and LLMs. \nWhereas one might consider each token in a text dataset as an individual data point, these \ntokens often lack meaning of their own. Consequently, in NLP data points can consist of \nentire sentences or even whole documents, as the separation between meaningful units is \nless neat than in tabular or image-based data scenarios.\nFeature or concept removal requests. Requests for feature removal are concerned with \nspecific attributes about data points. This is well defined for tabular data, where attributes \ncorrespond to one or more columns of the tables.\nFor images, the definition of what is a feature is less clear-cut, as features of images may \ncorrespond to individual pixels, to information automatically extracted by the models to \nachieve some predictions, or even be the classification targets in other cases. For example, \na model that classifies images of dogs and cats may internally extract features that repre -\nsent the shape of the ears to make a decision. Therefore, a feature is information about a \ndata point, which may or may not be common to other data points and that serves to par -\ntially describe these data points (even if these descriptions are only meaningful within the \nmodels).\nFig. 1 Types of digital forgetting\n \n1 3\n90 Page 6 of 41\nDigital forgetting in large language models: a survey of unlearning…\nIn the case of NLP, features may be defined as words or tokens with strong connections2 \nto other specific words or tokens of interest (such as an identity). This information may be \nspread across different sentences and documents. An example, related to privacy protection, \ncould be that of a request in which all information related to a data subject is required to be \nremoved from a model. Where features apply to a broad range of items, we refer to them \nas concepts.\nClass removal requests.  Class removal requests consist in removing all information \nabout a whole class from a model. These requests are quite natural for models used to iden-\ntify people. For example, in facial recognition, each of the classes corresponds to a single \nindividual, and data points in the class are images of said individual. Thus, removing one \nclass amounts to making the model unable to identify a person that the model could previ -\nously identify.\nClass removal requests can be directly translated to NLP classification tasks, such as \nsentiment analysis. In this case, removing a class requires making the model unable to rec -\nognize sentences of a given sentiment, either by removing all sentences that belong to the \nclass, or by making the model classify them as a different class.\nHowever, for generative models, each class corresponds to a word or token in the vocab-\nulary of the model, and removing it could directly impact the capabilities of the model.\nTask removal requests.  As described in previous sections, LLMs are pre-trained on \nlarge text datasets with generic tasks, such as next-word prediction or masked language \nmodeling. After pre-training, models are fine-tuned to teach them different tasks, such as \nsummarization, sentiment analysis, code writing, conversational text generation, etc. Task \nremoval requests attempt to make the fine-tuned LLMs forget one or more of the tasks that \nthe model has been taught to perform.\n2.3 Requirements of digital forgetting\nRegardless of the reason or type of forgetting request, we can define a series of general \nrequirements ensuring that the forgetting procedure is carried out correctly and the resulting \nmodel still performs adequately. This is the purpose of this section, but first, we introduce \nsome preliminary definitions.\nA dataset D consists of samples {xi,y i}N\ni=1, where N is the size of the dataset, xi are \ntoken sequences, and yi the true labels. Note that in self-supervised training, used for most \npre-training tasks in LLMs, the labels yi do not need to be explicitly defined. For example, \nin next-token prediction yi is the token immediately following the sequence xi in the text. \nHowever, during fine-tuning, labels are expected to be explicitly provided. A forget dataset \nDf ⊂ D is the set of samples to be forgotten. The retain set is defined as Dr = D \\ Df .\nA learning algorithm A(D) is a probabilistic algorithm that outputs a model given a train-\ning dataset D. Due to the probabilistic nature of A(·), we cannot ensure that running the \nlearning algorithm twice on the same dataset will return the same model. However, we can \ndefine a probability distribution P(A(D)) over all possible models returned by the learning \nalgorithm when trained on the same dataset D. Additionally, we can define Dist(·, ·) as the \ndistance between two probability distributions. An example of such a distance is the Kull -\nback–Leibler (KL) divergence.\n2 A strong connection between tokens could be interpreted as closeness in the embedding space, or as strong \nattention values between them.\n1 3\nPage 7 of 41 90\nA. Blanco-Justicia et al.\nA forgetting algorithm F(Df ,A (D)) is a (probabilistic) algorithm that returns a model \nin which the influence of the samples in Df  has been removed from A(D). Definitions \nof unlearning by Nguyen et al ( 2022); Xu et al ( 2023) include D as a parameter of the \nunlearning mechanism (as in F(D,D f ,A (D))), but access to D is not always feasible. \nFor example, a service provider that leverages a foundation model such as Llama2 to offer \nsome service (possibly after some additional fine-tuning) does not have access to the data \nused by the party that conducted the pre-training. However, the service provider may still \nbe required by law to fulfill forgetting requests. For the sake of generality, we will abstain \nfrom including D as a parameter for forgetting, although D may be used in some procedures.\nWhen implementing digital forgetting in ML models (and in LLMs in particular), the \nfollowing requirements should be taken into consideration.\nForgetting guarantees. An essential requirement for any forgetting procedure, whether \nin the conventional scenario of search engines or with large language models, lies in the \nability to demonstrate the fulfillment of a forgetting request, particularly when such fulfill -\nment is a legal obligation. A forgetting guarantee serves as a theoretical proof, offering \nassurance that the content associated with a forgetting request has been forgotten, accompa-\nnied by a level of certainty. This ensures a transparent and accountable process in meeting \nlegal requirements and reinforces the credibility of the forgetting mechanism. Nguyen et \nal (2022) refer to two levels of guarantees, exact and approximate. Xu et al ( 2023) further \nexpand approximate forgetting into strong and weak forgetting, where strong forgetting is \nequivalent to the definition of approximate forgetting by Nguyen et al (2022), and weak for-\ngetting only applies to the outputs of the model. However, most of the literature on unlearn-\ning mechanisms provides no provable guarantees, relying instead on empirical evaluation \nor auditing.\n ● Exact forgetting.  A forgetting algorithm F(Df ,A (D)) provides an exact forgetting \nguarantee if \n Dist(P (F (Df ,A (D))),P (A(D \\ Df )) = 0,\n where P(·) denotes probability distribution. From this definition, we can conclude that \nretraining from scratch on the retain set Dr is a straightforward approach to exact unlearn-\ning, since P(A(Dr)) =P(A(D \\ Df ))\n ● Approximate forgetting. A forgetting algorithm F(Df ,A (D)) provides an approximate \nforgetting guarantee, if \n Dist(P (F (Df ,A (D))),P (A(D \\ Df )) ≤ t,\n for an acceptable threshold t. Nguyen et al provide a definition for ϵ-certified forgetting, \ninspired by differential privacy. Given ϵ> 0 and a sample z ∈ D, \n \ne−ϵ ≤ Pr(F (z,A (D)))\nPr(A(D \\ z)) ≤ eϵ.\n ● Weak/No guarantees. The forgetting guarantees described above may not be attainable \n1 3\n90 Page 8 of 41\nDigital forgetting in large language models: a survey of unlearning…\nin all cases, because achieving them or computing them is unfeasible. In these cases, \nwe can refer to empirical evaluation or auditing to provide a level of risk reduction (that \nis, how much the risk of the model remembering the undesired item has been reduced). \nThese evaluations will depend on the type of forgetting request that needs to be dealt \nwith. When requests are related to privacy or copyright protection, the difference in \nmembership inference vulnerabilities can serve as a measure of risk reduction. If the \nforgetting request involves corrections of biases in the models, fairness metrics can be \nused. Finally, if some task needs to be forgotten, specific task benchmarks can be used. \nRefer to Sect. 5.3 for different evaluation mechanisms.\nRetaining of performance. Whatever the method used for forgetting, the resulting models \nshould retain much of the performance of the original model with respect to the same met -\nrics and benchmarks. Note that if a given task is removed from the resulting model, some \nspecific benchmarks may no longer be applicable. Xu et al (2023) consider two performance \nmetrics for classifiers, namely consistency and accuracy. Consistency is defined as the level \nof agreement between a model resulting from a forgetting procedure and a model trained \nonly on the retain dataset. Accuracy is defined in a standard way, by evaluating the model \nresulting from a forgetting procedure on the retain dataset. While these metrics may not be \ndirectly applicable to generative models, other metrics, such as perplexity,3 could be used to \ncompare unlearned and retrained models.\nRuntime and scalability. When serving forgetting requests, especially if triggered by \nprivacy concerns or by the application of the right to be forgotten, it is important that the \nforgetting procedure can be executed promptly. Article 17 of the GDPR requires in para -\ngraph 1 that “the controller shall have the obligation to erase personal data without undue \ndelay”; additionally, paragraph 2 indicates that such procedures should be carried out “tak-\ning account of available technology and the cost of implementation”. Thus, it is important \nthat forgetting algorithms can be executed in a timely manner, so that no personal informa-\ntion is accessible for longer than needed (to minimize potential harm to individuals). Other \ntypes of forgetting requests, such as those seeking to delete copyrighted material, correct \nbiases or remove tasks may not be so urgent.\nA different, but related property is scalability, meaning how many forgetting requests \ncan be processed simultaneously and how that affects the runtime and the utility of both the \nresulting model and the forgetting procedure.\n3 Approaches to digital forgetting in LLMs\nWe next delineate the main approaches to digital forgetting in LLMs. These approaches \n(as illustrated in Fig. 2) are each suitable for different phases of the LLM pipeline, going \nfrom data pre-processing and privacy-preserving model pre-training, which are suitable \napproaches when models can be retrained from scratch, to machine unlearning, prompt \nengineering and post-processing\n3 Perplexity is an information-theoretic measure of uncertainty in the value of a sample from a discrete prob-\nability distribution. In language models, perplexity measures how well a language model predicts a text \nsample. It is calculated as the average number of bits per word a model needs to represent the  s a m p l e .  \n1 3\nPage 9 of 41 90\nA. Blanco-Justicia et al.\nData pre-processing and model retraining. Carefully choosing the data to be included \nin the pre-training and fine-tuning phases is a sensible and recommended approach to pre -\nvent any unwanted behavior from the models, be it from a privacy, a copyright, or an align-\nment perspective.\nA second potential approach to limit the amount of private information in the training \ntext is to perform text anonymization, also called text redaction or sanitization. Tradition -\nally, redaction has been manually carried out by human experts. However, with the improve-\nment of artificial intelligence mechanisms, some automated approaches have been proposed. \nMost approaches are based on named-entity recognition (either rule-based or ML-based) \n(Lison et al 2021). More more sophisticated approaches assess the sensitiveness of terms by \nleveraging the information theory (Sánchez and Batet 2016, 2017), or the semantics embed-\nded in language models (Hassan et al 2019, 2023). In both types of approaches, sensitive \nitems in the text are identified and then either removed or generalized.\nA third approach based on data pre-processing is deduplication (Kandpal et al 2022). This \nconsists in finding replications of the same text within the training corpus and deleting any \nduplicates. What constitutes a duplicate can be parameterized in terms of length. Duplicate \ndocuments tend to be more vulnerable to membership inference attacks, as shown by Carlini \net al (2021); Nasr et al ( 2023). Thus, deduplication can be an effective mechanism against \nmemorization (which may lead to privacy, copyright, robustness, and alignment issues).\nAccording to the properties described above, retraining models that exclude the data to \nbe forgotten yields an exact forgetting guarantee. However, the cost in time and resources of \nretraining models from scratch makes this option unattractive in the short term, especially if \nforgetting requests have to be executed urgently.\nPrivacy-preserving model pre-training . Using privacy-preserving ML mechanisms \nmay limit the influence of any single data point on the model. In this case, instead of pro -\ntecting the data, we use some training mechanism that ensures privacy. Mechanisms such as \ndifferentially private stochastic gradient descent (DP-SGD) (Abadi et al 2016) and private \naggregation of teacher ensembles (PATE) (Papernot et al 2018) have been used in the last \nyears to train privacy-preserving machine learning models.\nAs in the previous case, this approach will provide not only exact guarantees of unlearn-\ning, but provable guarantees of privacy. The application of these methods, however, will in \nmost cases degrade the utility of the resulting models, and the resources and time needed for \nmodel training can be even higher than for retraining from scratch.\nMachine unlearning . Given the high cost and time required to train LLMs, retrain -\ning them from scratch to eliminate undesirable behaviors is often an impractical endeavor. \nCurrently, there is a growing trend in the literature to adopt the unlearning approach as an \nFig. 2 Pipeline for training (top) and forgetting (bottom) in large language models\n \n1 3\n90 Page 10 of 41\nDigital forgetting in large language models: a survey of unlearning…\nefficient means for digital forgetting in LLMs. Methods that attempt to remove undesirable \nknowledge or behaviors from models that have already undergone pre-training (and maybe \nalso fine-tuning) without retraining those models from scratch are called machine unlearn-\ning mechanisms. These mechanisms rely on further fine-tuning, often with adversarial \nobjectives, on the identification of parameters that are correlated with unwanted informa -\ntion and their modification, and on parameter arithmetic. Sections 4.1 and 4.2 below cover \nmachine unlearning mechanisms of this kind.\nThese methods offer a wide range of forgetting guarantees, utility levels of the resulting \nmodels, and implementation costs.\nPrompt engineering. Specific prompts to trained LLMs can be used to further steer the \nmodels’ behavior after fine-tuning. These methods do not cause any changes to the model \nparameters and can in some cases be bypassed by inputting contradicting prompts. How -\never, carefully crafted prompts inserted at the beginning of a conversation with a generative \nmodel can prevent it from generating private, biased, or harmful information. For example, \nÜstün et al ( 2024) use the prompt (preamble) Does the following request contain harm -\nful, unethical, racist, sexist, toxic, dangerous, offensive or illegal content or intent? If yes, \nexplain that you do not engage in these types of requests. They find that their model rejects \nup to 88% of such requests. Section 4.4 describes methods that use prompt engineering for \ndigital forgetting.\nPost-processing. Other technical measures that can be applied to models accessible only \nthrough an API are post-processing or filtering. After the models have generated an output \nand before serving it to the user, the service provider can analyze the output to search for \nunwanted generations, possibly using other LLM-based classifiers. Other approaches use \na memory of unwanted responses to identify and filter out such generations. Section 4.4 \nincludes some of these approaches.\n4 Survey on unlearning in LLMs\nAs discussed in Sect. 3, unlearning is the most general way to efficiently eliminate undesir-\nable or to-be-forgotten knowledge from LLMs without the need for full retraining. In this \nsection, we conduct a comprehensive survey of unlearning methods applicable to LLMs, \nand we propose a fine-grained classification of methods into four primary categories: global \nweight modification, local weight modification, architecture modification, and input/output \nmodification methods. This classification is predicated on the reach and depth of the altera-\ntions performed on the target models.\nGlobal weight modification methods include those that have the potential to alter all \nmodel parameters of the target model during the unlearning process. Conversely, local \nweight modification methods are restricted to modifying a specific subset of parameters. \nArchitecture modification methods do not modify the parameters of the model, but intro -\nduce additional trainable layers into the model’s architecture. Figure 3 shows a conceptual \nillustration of the modifications associated with weight and architecture changes.\nFinally, input/output modification methods do not make any changes to the model’s \narchitecture or parameters, and function exclusively by manipulating the inputs and out -\nputs of the model. We further divide these primary categories based on how unlearning is \nperformed.\n1 3\nPage 11 of 41 90\nA. Blanco-Justicia et al.\nFigure 4 illustrates our proposed taxonomy of unlearning methods for LLMs, to be used \nas a framework for this survey.\n4.1 Global weight modification\nIn these methods (Bourtoule et al 2021; Jang et al 2022), every parameter of the model is \nsubject to modification during the unlearning process. Whereas global weight modification \nmethods offer a stronger unlearning guarantee compared to the other approaches, they often \nentail substantial computational and time overheads, which renders them impractical for \nLLMs in many cases.\n4.1.1 Data sharding\nThis approach typically entails dividing the training data into multiple disjoint shards, each \ncorresponding to a subset of the overall data, and training a separate model for each shard \nFig. 4 Taxonomy of unlearning methods in LLMs\n \nFig. 3 Conceptual illustration of weight/architecture modification\n \n1 3\n90 Page 12 of 41\nDigital forgetting in large language models: a survey of unlearning…\n(Bourtoule et al 2021; Liu and Kalinli 2023). These individual models can then be leveraged \nto effectively remove data whose unlearning has been requested.\nBourtoule et al (2021) introduce SISA (Sharded, Isolated, Sliced, and Aggregated train-\ning) as a generic exact unlearning framework.\nAs depicted in Fig. 5, the training dataset is divided into S non-overlapping shards, each \ncontaining R slices. For each shard, an independent model is trained by processing the \ndata slice by slice, saving a model checkpoint after each slice has been processed. During \ninference, the predicted label is obtained by aggregating the individual predictions of each \nmodel, such as in ensemble methods. When an unlearning request is received, the shard and \nslice where the data point is located are identified. The data point (typically a sequence of \ntokens in NLP tasks) is then removed from the this slice, and the model is retrained starting \nfrom the last unaffected checkpoint onward. For instance, in Fig. 5, retraining would com-\nmence from the checkpoint trained on Slice2,1, which by construction ensures the model \nforgets the data point to be unlearned. The main advantage of SISA is that it provides an \nexact unlearning guarantee. This method can be applied to a wide range of ML tasks and \nmodel architectures, including LLMs. However, SISA is not very practical for LLMs due to \nthe high computational/memory cost associated with model and checkpoint saving, retrain-\ning, and inference. Kadhe et al ( 2023) found that SISA can reduce fairness in LLMs and \nadapted a post-processing fairness improvement technique to make it fairer.\nLiu and Kalinli ( 2023) propose the leave-one-out (LOO) ensemble  method to reduce \nretraining cost and maintain utility at inference time. This method requires a base “stu -\ndent” model trained on the entire dataset and a collection of M “teacher” models trained \nFig. 5 Framework of SISA. Forgetting request processing in red\n \n1 3\nPage 13 of 41 90\nA. Blanco-Justicia et al.\non disjoint subsets of the training dataset. Upon receiving a forget request, a prediction \non the to-be-forgotten sequences is produced using the M − 1 models that do not include \nthe unwanted knowledge. The student model is then fine-tuned on the obtained soft labels. \nWhile offering improved utility compared to SISA, this method’s unlearning guarantee for \nthe base model is approximate, and there is no guarantee for the teacher models. Moreover, \nthe process of predicting soft labels, aggregating them, and then fine-tuning the parameters \nof the base model incurs significant computational overheads, particularly with LLMs.\n4.1.2 Gradient ascent\nMethods under this approach aim to move the model away from undesirable knowledge by \nfine-tuning all model parameters to maximize loss on the related target tokens.\nGiven a set of target token sequences representing sensitive information, the method by \nJang et al ( 2022) runs a number of optimization steps negating the original training loss \nfunction for those token sequences, therefore moving away from the minimum loss with \nrespect to such sequences.\nThe authors found that unlearning many samples at once substantially degrades the per-\nformance of LLMs, and unlearning them sequentially can mitigate this degradation.\nGradient ascent (GA) and its variants only require the to-be-forgotten data and some -\ntimes enhance the generalization capabilities of the model, as observed by Yoon et al (2023). \nHowever, GA can cause the model to lose its understanding of the language (Eldan and \nRussinovich 2023). Furthermore, the success of unlearning depends on the specific target \ndata and the domain of the to-be-forgotten data (Smith et al 2023).\nTo preserve the generation capability of LLMs, SeUL (Wang et al 2024) applies GA (Jang \net al 2022) to specific sensitive spans within the sequence instead of the entire sequence. The \nsensitive spans are selected using two annotation methods: an online method which assumes \na token to be sensitive if it has a high perplexity score, and an offline method which employs \nthe in-context learning capabilities of LLMs to annotate such sensitive spans. While this \nmethod allows for more utility-preserving, focused, and efficient unlearning compared to \nGA (Jang et al 2022), the annotation methods may lack precision in identifying sensitive \ntokens.\nYao et al (2023) observed that only applying GA as Jang et al ( 2022) do is insufficient \nto effectively unlearn unwanted (mis)behaviors (e.g., harmful responses and hallucinations) \nand that preserving a good retain performance is harder than unlearning, because the retain \nperformance greatly depends on the format of the retain data. Based on these observations, \nthe authors propose an unlearning method that minimizes three weighted loss functions. The \nmethod involves updating the LLM during unlearning by jointly (1) applying GA on forget \nsamples, (2) forcing random outputs on forget samples, and (3) minimizing the KL diver -\ngence between predictions of the original and unlearned models on retain samples. While \nthe method provides a better trade-off between unlearning and retaining utility, it requires a \nlarge number of training epochs to unlearn forget data and maintain utility simultaneously.\n4.1.3 Knowledge distillation\nMethods under this approach treat the unlearned model as a student model that aims to \nmimic the behavior of a teacher model with the desired behavior.\n1 3\n90 Page 14 of 41\nDigital forgetting in large language models: a survey of unlearning…\nWang et al (2023) propose the Knowledge Gap Alignment (KGA) method as an unlearn-\ning technique for LLMs. KGA utilizes training data, data to be forgotten, and external data \nfor unlearning to produce an updated model that exhibits a similar behavior on forgotten \ndata as on unseen data while retaining utility on the remaining data. This is achieved by \naligning the “knowledge gap,” which refers to the difference in prediction distributions \nbetween models trained with different data. Aligning the unlearned model’s behavior on the \nforget data with unseen data is achieved by minimizing the difference in the distributions of \nthe unlearned model predictions on the forget data and the original model predictions on the \nunseen data. The authors use the KL divergence to measure this difference. The main advan-\ntage of this method lies in its generic nature, which allows it to be applied to various models \nand tasks. However, the need to train two identical models and then fine-tune all model \nparameters may limit its practicality and efficiency when applied to LLMs. Additionally, \nthe unlearning process requires the training data, the data to be forgotten, and other external \ndata with no overlapping with the training data. The utility of the unlearned model is highly \ndependent on the sizes of the data to be forgotten and the external data.\n4.1.4 Generic alternatives\nThis approach aims to achieve unlearning by fine-tuning all the model parameters to predict \ngeneric alternatives instead of the to-be-forgotten tokens.\nEldan and Russinovich (2023) propose an unlearning method for erasing source-specific \ntarget data from LLMs, namely the Harry Potter books. Their three-step method involves:\n ● Token identification through reinforced modeling. This process involves creating an \naugmented model with a deeper understanding of the content that needs to be unlearned. \nThis is done by further fine-tuning the original model on the target data. Tokens with \nsignificantly increased probability are identified, indicating content-related tokens that \nshould be avoided during generation.\n ● Expression replacement. Distinctive expressions in the target data are replaced by ge -\nneric alternatives. The authors used GPT-4 to generate those alternatives automatically. \nThis helps approximate the next-token predictions of a model that has not seen the target \ndata.\n ● Fine-tuning. Armed with these alternative labels, the model undergoes fine-tuning to \nerase the original text from the model’s memory and provides a plausible alternative \nwhen prompted with its context.The authors demonstrated that the unlearned model \nno longer generates or recalls content related to the target data, while its performance \non common benchmarks remains nearly unaffected. This method provides plausible al-\nternatives to unlearned tokens, which is important for maintaining the retain utility in \nmany scenarios. However, Shi et al (2023) found that models that underwent unlearning \nwith this approach could still output related copyrighted content. Another limitation is \nthat this approach relies on replacing unique terms with their generic alternatives, which \nis challenging when extending it to non-fiction content.\n1 3\nPage 15 of 41 90\nA. Blanco-Justicia et al.\n4.1.5 Reinforcement learning from human feedback (RLHF)\nRLHF involves leveraging human feedback to guide the model’s learning process. It com -\nbines reinforcement learning (RL) with human feedback to teach the model to generate text \nthat aligns better with human preferences and intentions.\nLu et al (2022) present Quark, which considers the task of unlearning undesired behav -\niors of a target LLM by fine-tuning the model on signals of what not to do. Quark starts with \na pre-trained LLM, initial training prompts, and a reward function to initialize a pool of \nexamples. Then, it alternates between quantization, learning, and exploration. In quantiza -\ntion, the pool of examples is sorted according to the reward function and partitioned into \nquantiles. For learning, the model is fine-tuned on the quantized data pool using a standard \nlanguage modeling objective and a KL penalty. During exploration, new generations are \nadded to the data pool by sampling from the model, which is conditioned on the tokens with \nthe higher rewards. The objective of the three-step process above is to teach the model to \ngenerate texts of varying quality with respect to the rewards. At inference time, the sampling \nis conditioned with the best reward token to steer toward desirable generations. Quark is \none of the well-known state-of-the-art controllable text generation methods that effectively \naligns the model output with human expectations (Daheim et al 2023). However, in addition \nto its computational cost significantly increasing as the data pool size increases, the model \nmay still retain the sensitive information on its parameters.\n4.2 Local weight modification\nUnlike the previous methods, which were allowed to modify all model parameters, methods \nin this category are restricted to a certain subset of model weights during unlearning.\n4.2.1 Local retraining\nThis approach employs a selective retraining strategy, where only the model parameters \nrelevant to the unlearning targets are optimized, leaving the rest unaltered.\nYu et al ( 2023) introduce a gradient-based debiasing method called Partitioned Con -\ntrastive Gradient Unlearning (PCGU). PCGU systematically identifies and retrains specific \nmodel weights responsible for biased behavior. The approach involves computing the gra -\ndients for contrastive sentence pairs. The contrastive sentence pairs are pairs of sentences \nwhere only a key demographic attribute is altered, for example, changing a gendered pro -\nnoun. The differences in the computed gradients are then ranked to select which weights in \nthe model contribute the most to the biased behavior of the model, and these weights are \nsubsequently retrained. The authors demonstrate that the method is effective both in mitigat-\ning bias for the gender-profession domain as well as in generalizing these influences to other \nunseen domains. However, the work only addresses gender bias in masked LLMs and it \nremains uncertain whether the proposed method can be generalized to other kinds of LLMs \nand more complex social biases like race and social class discrimination.\n1 3\n90 Page 16 of 41\nDigital forgetting in large language models: a survey of unlearning…\n4.2.2 Task vector\nThese methods build on the idea that an LLM can be decomposed into task-specific vectors. \nThen the vector of the task to be forgotten can be eliminated to achieve unlearning.\nIlharco et al ( 2022) introduce a novel paradigm for steering neural network behavior \nbased on task vectors. A task vector τ  is defined as the difference between the parameters of \na model θt\nft fine-tuned on a specific task t and those of the corresponding pre-trained model \nθpre (a model not yet trained on task t). Employing τ  allows for selective modification of \nthe model’s behavior for task t without significant impact on other tasks. This is done via the \nnegation and addition arithmetic operations: negating τ  allows forgetting knowledge related \nto task t while adding it improves the model’s performance on t. Figure 6 illustrates how a \ntask vector τ  is obtained and how it is used for task forgetting via negation.\nThe authors show that this method can be used to improve the performance of the model \non multiple tasks by adding their vectors. However, it cannot handle forgetting discrete facts \nor specific token sequences, which makes it more suitable for broader task-based modifica-\ntions than for fine-grained ones.\nBy drawing inspiration from human cognitive processes and the task arithmetic in Ilharco \net al (2022), the authors of Ni et al (2023) propose a paradigm of knowledge updating called \nF-Learning (Forgetting before Learning). Specifically, the target model is first fine-tuned \nwith the knowledge to be unlearned. Then, the difference of the parameters of the target and \nthe fine-tuned models is subtracted from the parameters of the target model. This process \nis termed “old knowledge forgetting.” Finally, the unlearned model is fine-tuned with new \nknowledge, constituting the “new knowledge learning” stage. However, as it happened with \nIlharco et al (2022), this method is not suitable for forgetting individual facts or sequences \nof tokens.\n4.2.3 Direct modification\nInstead of gradient optimization of the original or additional parameters, this approach \nlocates and directly modifies the relevant parameters or neurons to achieve unlearning.\nDEPN (Wu et al 2023) assumes that private information, such as usernames and contact \ninformation, is encoded in specific neurons of the target LLMs. Based on that assumption, \nthe method first locates these neurons by using the integrated gradient method by Sunda -\nrarajan et al ( 2017) and then sets their activations to zero to eliminate their influence on \nthe output. A privacy neuron aggregator is also introduced to handle multiple unlearning \nrequests. An analysis of the relationship between privacy neurons and model memorization \nwas also performed. The authors found that model size, training time, and frequency of \nFig. 6 Illustration of task vectors and the \nnegation arithmetic operations for task \nforgetting\n \n1 3\nPage 17 of 41 90\nA. Blanco-Justicia et al.\noccurrence of private data are all factors that have an impact on model memorization. As \nthe model memorization of private data deepens, the aggregation of privacy-related neu -\nrons associated with those data becomes more obvious. This method is efficient, as it only \nrequires the forget set without fine-tuning. However, as the amount of forget data increases, \nthe utility of the model drops significantly because more neurons are deactivated. Also, the \nauthors found that including too many instances in a batch reduces the effect of forgetting. \nBesides, the method evaluation on forgetting private information was limited to names and \nphone numbers, due to the limited availability of datasets with a wide range of private data. \nThe authors acknowledge the necessity of expanding their dataset to improve the effective-\nness and relevance of their method.\nPochinkov and Schoots ( 2023) found that both feed-forward and attention neurons in \nLLMs are task-specialized, and that removing certain neurons from them significantly \ndecreases performance on the forget task while hardly affecting performance on the other \ntasks. Based on that, they introduce a selective pruning method for identifying and removing \nneurons from an LLM that are related to a certain capability like coding or toxic generation. \nNeurons are removed based on their relative importance on a targeted capability compared \nto the overall model performance. This method is compute- and data-efficient in the iden -\ntification and removal of task-specialized neurons. It can also be applied to remove other \nharmful skills, such as toxic generation and manipulation. However, the method requires a \ntask-representative dataset and it is only effective for capabilities directly captured by these \ndatasets. Besides, its effectiveness depends on the separability of the capabilities of an LLM. \nThis method becomes less effective with models like Pythia (trained without dropout) and \non smaller LLMs.\n4.3 Architecture modification\nIn addition to the computational burden, altering all or a substantial amount of the tar -\nget model’s weights may degrade the model utility on the remaining data. To tackle these \nissues, methods based on architecture modification add extra learnable parameters within \nthe model or use linear transformation in order to achieve efficient and utility-preserving \nunlearning (Chen and Yang 2023; Limisiewicz and Mareček 2022).\n4.3.1 Extra learnable layers\nEUL (Chen and Yang 2023) integrates an additional unlearning layer into transformer struc-\ntures after the feed-forward networks. For each forgetting request, an unlearning layer is \nindividually trained using a selective teacher-student objective. During the training of this \nlayer, the original parameters are frozen, and a joint unlearning objective is used. The design \nof this joint objective is such that it compels the unlearning layer to diverge from the original \nmodel’s predictions on the forget data, while following it on the remaining data. A fusion \nmechanism is also introduced to handle sequential forgetting requests. While this method \nmaintains the model utility on the retain set, it has its limitations. It does not constitute com-\nplete forgetting, as removing the additional layers causes the old model to revert to its origi-\nnal behavior on the data to be forgotten, thus contradicting privacy laws. Also, training an \nadditional layer is required to forget every unlearning request, which might not be scalable.\n1 3\n90 Page 18 of 41\nDigital forgetting in large language models: a survey of unlearning…\nKumar et al ( 2022) propose two variants of SISA to improve its efficiency on LLMs: \nSISA-FC and SISA-A. SISA-FC starts from a base model, pre-trained on a generic text \ncorpus, and then adds fully connected (FC) layers on top of it. Only the parameters from the \nadded layers are updated during retraining. This approach minimizes the overall retraining \ntime, as backpropagation of gradients occurs only in the final layers, and storage require -\nments are reduced to the weights of these additional parameters. However, the utility of the \nmodel will be severely affected when compared to fine-tuning the entire model. SISA-A \ntries to address this by training the model using the parameter-efficient adapter method \nfrom Houlsby et al ( 2019), which injects learnable modules into the encoder blocks of the \ntransformer. The methods were evaluated on classification tasks with the BERT model with \nno evaluation for forgetting performance. While SISA-A preserves the classification utility \nof the resulting model better than SISA-FC, it entails more computational and storage costs \nthan SISA-FC. Unlike SISA, these proposed methods do not provide exact unlearning guar-\nantees, and their retain accuracy is lower than that of SISA. However, the training time and \nmemory footprint of both methods is significantly lower than in SISA. A common drawback \nof both SISA-FC and SISA-A is that the content of the generic text corpus (learned during \nthe pre-training phase) cannot be forgotten.\nZhang et al ( 2024a) introduce a method for combining parameter-efficient modules \n(PEMs) to enhance the capabilities of LLMs. The method utilizes linear arithmetic opera -\ntions to compose different PEMs in ways that address specific needs like distribution gen -\neralization, multi-tasking, unlearning, and domain transfer. The unlearning aspect of this \nmethod employs negation operations to effectively remove or reduce undesired behaviors or \nknowledge from a model-akin to a detoxification process. This unlearning operation entails \nsubtracting the parameters learned from a negative dataset from those learned from a stan -\ndard dataset, resulting in a new unlearned PEM, thus enabling the model to “forget” these \nbehaviors. For example, a PEM trained on toxic data can be negated to serve as a plug-in \ndetoxifier, reversing its toxic effect.\n4.3.2 Linear transformation\nBelrose et al ( 2023) introduce LEACE, a closed-form method to prevent linear classifiers \nfrom detecting a concept, such as word part-of-speech, with minimal change of represen -\ntation. This is achieved by applying a procedure called concept scrubbing, which erases \nall linear information about a target concept in every intermediate representation. LEACE \nsequentially fits an affine transformation to every intermediate layer to erase the target \nconcept from its features while minimizing the distance from the original features. To fit \nLEACE parameters, samples from the respective model pre-training distribution are used. \nWhile this method can be used to improve fairness ( e.g. preventing a classifier from using \ngender or race as deciding criteria) and interpretability (e.g. removing a concept to observe \nchanges in model behavior), it has many limitations. It may degrade utility due to eras -\ning features irrelevant to the concept and it requires caching hidden states during training, \nthereby leading to a substantial demand for memory. Also, its validation is limited to part-\nof-speech, and its practical applicability is uncertain.\nGender information in representations of biased LLMs can be divided into factual gen -\nder information and gender bias. Factual gender information encompasses grammatical or \nsemantic properties indicating gender in English texts, such as explicit gendered pronouns \n1 3\nPage 19 of 41 90\nA. Blanco-Justicia et al.\nlike “he” or “she”. Gender bias, on the other hand, refers to the model’s tendency to associ-\nate certain words with specific genders, like “nurse” being more correlated with women than \nmen. Limisiewicz and Mareček (2022) aim to mitigate the gender bias of a pre-trained LLM \nby manipulating contextual embedding. They apply an orthogonal transformation to sepa -\nrate lexical and syntactic information encoded in the model embedding. Then they filter out \nthe bias subspace from the embedding space and keep the subspace encoding factual gender \ninformation. Although this method is efficient because it applies linear transformations on \nthe contextual embedding only, there is no guarantee that all bias-related dimensions will \nbe filtered. This is because the bias signal can be encoded non-linearly in LLMs and, even \nwhen the whole bias subspace is removed, the information can be recovered in the next \nlayer of the model (Ravfogel et al 2020).\nLeong et al (2023) propose a method for detoxifying LLMs by identifying and reversing \ntoxification trajectories in the internal model activations. The method operates in two stages \nduring inference: 1) generating internal activations for negative and positive prompts to \ndetermine the “toxification direction,” and 2) linearly combining the target input’s activa -\ntions with the negated difference from Stage 1 to produce non-toxic outputs. Whereas this \nproposal is promising due to its simplicity and instance-level detoxification, it relies on \ncarefully crafted prompts, which raises concerns about its ability to mitigate complex forms \nof bias and toxicity. Additionally, its effectiveness was primarily demonstrated on GPT-2, \nwhich raises questions about its generality across different models.\n4.4 Input/output modification\nMethods in this category treat the models as a black-box and only require access to the \ninputs and outputs of a model. This approach is specifically useful when the operator has \nno access to the model weights, e.g., for products and services wrapping API-based models. \nHowever, in the context of the right to be forgotten, input/output modification does not yield \nany real privacy guarantee, as the models still retain the to-be-forgotten knowledge.\n4.4.1 Input manipulation\nIn this approach, the data to be forgotten are deliberately altered or adjusted in a systematic \nway to facilitate the process of unlearning.\nGallegos et al (2024) propose a prompting-based bias mitigation method that leverages \nthe zero-shot capabilities of LLMs to reduce biased stereotypical predictions and call it \nzero-shot self-debiasing. Two techniques, self-debiasing via explanation and self-debiasing \nvia reprompting, are proposed. In the former, the model first explains invalid assumptions \nin the answer choices, whereby it identifies potential stereotyping. Then, it answers the \nquestion without stereotyping. In the second technique, the model first answers the question \nas per the baseline approach. Then, it is reprompted to remove bias from its initial answer.\nThe authors demonstrate the ability of their method to decrease stereotyping in question-\nanswering over nine different social groups with a single prompt. Unlike traditional bias \nmitigations, this method does not require any additional training data, exemplar responses, \nfine-tuning, or auxiliary models, and thus it is a more efficient and practical solution for bias \nmitigation. However, the method is task and context-dependent. It is designed for multi -\nple-choice questions, rather than for the more common open-ended question. Also, it uses \n1 3\n90 Page 20 of 41\nDigital forgetting in large language models: a survey of unlearning…\nmanually created prompts, which limits its generalization to other biases. Future research \ncould focus on detecting biases in free text and also exploring automated prompt generation \nto manage biases more effectively.\n4.4.2 Information retrieval\nThe methods in this group aim to selectively extract or manipulate information from exter-\nnal knowledge to shape the unlearning trajectory of LLMs.\nSERAC (Mitchell et al 2022) utilizes a memory-based model editing approach that treats \nan LLM as a black-box. It serves as a simple wrapper around the base model and consists \nof three main components: an explicit cache of edits, an auxiliary scope classifier, and a \ncounterfactual model. When presented with a knowledge edit (which can be the unlearning \nrequest), the wrapped model predicts a new input in two steps. Firstly, the scope classifier \nassesses the likelihood of the new input falling within the scope of each cached edit. If \ndeemed within scope, the edit with the highest probability is retrieved, and the counterfac -\ntual model provides a prediction based on both the new input and the retrieved edit. If the \nnew input is considered out-of-scope for all edits, the base model’s prediction is returned. \nAlthough SERAC was evaluated on editing question answering, fact-checking, and dia -\nlogue generation, it can also be used to unlearn undesirable behaviors of LLMs and replace \nthem with desirable ones. This method is simple and easy to implement and requires no \nmodifications to the original model. Also, it can edit multiple models with different archi -\ntectures. However, it may be prone to retrieval errors, such as noise and harmful content, \nand knowledge conflict issues (Zhang et al 2024b). Further, it relies on an edit dataset for \ntraining and may require more computational and memory resources in some settings.\nTo correct model errors via user interactions without retraining, MemPrompt (Madaan \net al 2022) pairs the model with a growing memory of recorded cases where the model \nmisunderstood the user intent. The system maintains a memory of the feedback as a set of \nkey-value pairs, where the key is a misunderstood input (e.g., question), and the value is its \ncorrection. Given a new prompt, the system looks in the memory for a similar prompt to \ncheck if the model has made a mistake on a similar prompt earlier. If a match is found, the \ncorresponding correction is appended to the prompt, and then the updated prompt is fed to \nthe model. In this sense, this approach can be seen as an instance of prompt engineering (Liu \net al 2023) which involves editing the prompts. The method was applied to lexical relations \nlike antonyms, word scrambling such as anagrams, and ethics, where user feedback is used \nas the appropriate ethical consideration in natural language. While the method is efficient \nand gives the end-user more control over the unlearning process, it might struggle with the \nscalability of the memory or in maintaining utility as the volume of user interactions grows. \nAlso, its effectiveness highly depends on the quality of user corrective feedback and on the \nsuccess of matching the new input with its similar recorded one.\n4.4.3 In-context learning\nThis approach exploits the in-context learning power of LLMs for unlearning.\nTo remove the impact of a specific training point on the model’s output, ICUL (Pawelc-\nzyk et al 2023) constructs a specific context at inference time that makes the model classify \n1 3\nPage 21 of 41 90\nA. Blanco-Justicia et al.\nit as if it had never seen the data point during training. The ICUL method involves three \nsteps: \n1. Label-flipping. Given a forgetting request, the label on the corresponding training \npoint whose influence should be removed from the model is flipped, resulting in a new \ntemplate.\n2. Addition of correctly labeled training points. Excluding the to-be-forgotten point, s \nlabeled example pairs are randomly sampled and added to the template from Step 1.\n3. Prediction. The query input is added to the template, forming the final prompt, and the \nmodel predicts the next token using a temperature of 0.\nThe label-flipping operation in Step 1 aims to remove the influence of a specific training \npoint on the model outcome. Step 2 aims to reduce the effect of the label flipping, with \nthe number of points s allowing for a trade-off between efficiency and utility. The ICUL \nmethod was empirically evaluated on three classification datasets using a test called LiRA-\nForget, which was also introduced to empirically measure unlearning effectiveness. The \nresults demonstrate that ICUL can effectively eliminate the influence of training points on \nthe model’s output, occasionally outperforming white-box methods that necessitate direct \naccess to the LLM parameters and are more computationally demanding. While this method \nis efficient and preserves the utility of the model, its effectiveness depends on the model’s \ncapacity for in-context learning. It is worth noting that its efficacy was only tested with text \nclassification tasks where the label-flipping process is applicable. Its effectiveness on other \ntasks remains to be determined.\nTable 1 summarizes and compares the LLM unlearning methods surveyed in this paper \nunder the types and requirements of digital forgetting discussed in Sect. 2.\n5 Evaluation of unlearning in LLMs\nIn this section, we study the evaluation of unlearning in LLMs, including the datasets, mod-\nels, and metrics employed by the surveyed works. Our analysis of metrics focuses on the \nmain aspects discussed in Sect. 2.3: (i) whether the model has effectively forgotten the \ntarget knowledge, (ii) whether the model retained the rest of its capabilities, and (iii) the \ncomputational cost of the forgetting process. Hereafter, these measures will be referred to as \nforgetting, retaining, and runtime, respectively.\n5.1 Datasets\nA proper assessment of forgetting generally requires three different datasets, which we refer \nto as forgetting training set , forgetting test set , and retaining test set . The two forgetting \nsets are the unlearning counterparts of the training and test sets employed during training: \nthe forgetting training set comprises samples representing the knowledge that the model \nshould forget, whereas the forgetting test set facilitates the measurement of the forgetting \ngeneralization. The retaining test set is typically disjoint from the forgetting sets and is used \nto assess the utility of the unlearned model.\n1 3\n90 Page 22 of 41\nDigital forgetting in large language models: a survey of unlearning…\nTable 1 Comparison between LLM unlearning methods. L, M and H denote low, medium and high, respectively. Sgnf denotes significant\nMethod Forgetting Target Application Retaining Runtime\nPrivacy Copyright Robustness Toxicity Fairness Capabilities\nGlobal weight modification\nSISA (Bourtoule et al 2021) Exact Item • • • • – – M-L Sgnf\nLOO (Liu and Kalinli 2023) Approx Item • • • • – – H-M H\nGA (Jang et al 2022) Approx Item • • – • – – M-L M\nSeUL (Wang et al 2024) Approx Item • • – • – – M H\nYao et al (2023) Approx Item • • • • – – M H\nKGA (Wang et al 2023) Approx Item • • • • • • M-L H\nEldan and Russinovich (2023) Approx Concept • • – • – – H-M H\nQuark (Lu et al 2022) Approx General • • • • • • H Sgnf\nLocal weight modification\nPCGU (Yu et al 2023) Approx Concept – – – – • – H M\nTask vector (Ilharco et al 2022) Approx Task – – – – – • H-M H\nF-Learning (Ni et al 2023) Approx Task – – – – – • H-M H\nDEPN (Wu et al 2023) Approx Item • – – – – – M-L M\nPochinkov and Schoots (2023) Approx Task – – – – – • H M\nArchitecture modification\nEUL (Chen and Yang 2023) Weak/No Item • • • • – – H-M M\nSISA-FC (Kumar et al 2022) Weak/No Item • • • • – – L L\nSISA-A (Kumar et al 2022) Weak/No Item • • • • – – M M\nZhang et al (2024a) Weak/No Task – – – • – – M M\nLEACE (Belrose et al 2023) Weak/No Concept – – – – • – H-M M\nLimisiewicz and Mareček (2022) Weak/No Concept – – – – • – H-M L\nLeong et al (2023) Weak/No Item – – – • – – H-M L\nInput/output modification\nGallegos et al (2024) Weak/No Concept – – – – • – H L\nSERAC (Mitchell et al 2022) Weak/No General • • • • • • H M\nMemPrompt (Madaan et al 2022) Weak/No General • • • • • • M L\nICUL (Pawelczyk et al 2023) Weak/No General • • • • • • H L\n1 3\nPage 23 of 41 90\nA. Blanco-Justicia et al.\nTable 2 lists the forgetting datasets used by the surveyed methods. The “Forgetting tar -\nget” column specifies the forgetting request type (as defined in Sect. 2.2) and a descrip -\ntion of the undesired knowledge that has to be unlearned. Note that some authors select \ncombinations of undesired knowledge and forgetting request types that do not align with \nreal-world scenarios, although their choices might be justified for experimental purposes. \nOn the one hand, Wu et al ( 2023) and Borkar (2023) consider PII as items to be forgotten. \nHowever, sensitive information should be treated as a concept to be forgotten, to preclude \nthe model from providing any details compromising privacy. For example, for an individual \nborn in Paris, even if the model does not generate the city name verbatim, it can disclose \nthat the birthplace is ’The city of light’, the sobriquet of Paris. On the other hand, Eldan and \nRussinovich (2023) experiment with forgetting a copyrighted corpus (Harry Potter books) \nas a concept. This may be excessive to avoid copyright laws, for which it is sufficient not \nto generate copies of the text. Forgetting copyrighted texts as items should be enough for \nthese cases.\nTable 3 lists datasets used to assess the retaining of models. The “Retaining target” col -\numn specifies the preserved capability under evaluation. Note that the zsRE (Levy et al \n2017) dataset and those subsequently listed were also employed for forgetting purposes \nby the same authors (see Table 2). This is because the objective was to selectively for -\nget a subset of samples, such as specific generations ( i.e., private information in Enron \nemails (Klimt and Yang 2004)), a concept (i.e., gender bias in Bias in Bios (De-Arteaga et \nal 2019)) or a task ( e.g., Python programming in CodeParrot GitHub Code (Tunstall et al \n2022)), while retaining the rest of dataset-related skills. Several researchers choose widely \nrecognized benchmarks for LLMs (Eldan and Russinovich 2023; Jang et al 2022; Yao et al \n2023; Pawelczyk et al 2023; Chen and Yang 2023) to evaluate the preservation of overall \ncapabilities. In contrast, a limited number of authors employ datasets with tasks that are \nclosely aligned with the knowledge intended for forgetting (Limisiewicz and Mareček 2022; \nBelrose et al 2023; Pochinkov and Schoots 2023; Ni et al 2023). As it will be detailed in \nSect. 5.4, this strategy is taken because tasks that are similar but peripheral to the focus of \nthe forgetting process are expected to be the most affected by it.\n5.2 Models\nTable 4 reports the LLMs used by each of the surveyed methods. Models are sorted by their \nnumber of parameters, that differ by as much as 3 orders of magnitude (i.e., from 11 million \nto 30 billion parameters). LLMs are often released in various sizes to accommodate differ -\nent use cases. The table enumerates the specific sizes used, separated by commas. In cases \nwhere multiple authors selected the same model but in different sizes, a row without the \nmodel name is used. As suggested by Carlini et al ( 2021), larger models tend to memorize \nmore, and this makes them more challenging and interesting for evaluation. Notably, 8 \nworks opted for models with over 1 billion parameters.\n5.3 Forgetting evaluation\nMost surveyed methods only offer approximate unlearning. Therefore, the forgetting suc -\ncess must be evaluated empirically. This empirical evaluation is conducted by measuring \nthe model’s capabilities before and after the unlearning process via some metrics. Namely, \n1 3\n90 Page 24 of 41\nDigital forgetting in large language models: a survey of unlearning…\nthe model’s capabilities related to the behavior the model is supposed to unlearn should \ndecrease. We can further evaluate whether the unlearning process can generalize through the \nforgetting training and test sets. Considering the capabilities of LLMs for memorizing text \n(Carlini et al 2021), it is possible that the forgetting process only works for samples in the \nforgetting training set, and even a slight paraphrasing could be sufficient to obtain undesired \npredictions.\nTable 2 Datasets used for forgetting\nDataset name Forgetting target Paper/s\nPKU-SafeRLHF (Ji et al 2023) Concept: Harmful \ngenerations\nYao et al (2023)\nHaluEval (Li et al 2023a) Concept: Hallucinations Yao et al (2023)\nRealToxicityPrompts (Gehman et al 2020) Concept: Toxicity Lu et al (2022); Ilharco et \nal (2022)\nCivil Comments (Borkan et al 2019) Concept: Toxicity Ilharco et al (2022)\nHarry Potter Universe (Eldan and Russinovich \n2023)\nConcept: Related generations Eldan and Russinovich \n(2023)\nWinoBias (Zhao et al 2018) Concept: Gender bias Limisiewicz and \nMareček (2022)\nWinoMT (Stanovsky et al 2019) Concept: Gender bias Limisiewicz and \nMareček (2022)\nCrowS Pairs (Nangia et al 2020) Concept: Gender bias Yu et al (2023)\nWinogender Schemas (Rudinger et al 2018) Concept: Gender bias Yu et al (2023)\nBias in Bios (De-Arteaga et al 2019) Concept: Gender bias Belrose et al (2023)\nStereoSet (Nadeem et al 2021) Concept: Stereotype bias Yu et al (2023)\nEnglish Universal Dependencies (Nivre et al \n2020)\nConcept: Part-of-Speech Belrose et al (2023)\nRedPajama (Computer 2023) Concept: Part-of-Speech Belrose et al (2023)\nTraining Data Extraction Challenge 4 Item: Specific generations Jang et al (2022)\nPile (Gao et al 2021) Item: Samples & Concept: \nPoS\nJang et al (2022); Belrose \net al (2023); Pochinkov \nand Schoots (2023)\nHarry Potter and the Sorcerer’s Stone (Rowling \n2000)\nItem: Copyright corpus Yao et al (2023)\nSST2 (Socher et al 2013) Item: Samples Pawelczyk et al (2023)\nAmazon polarity (Zhang et al 2015) Item: Samples Pawelczyk et al (2023)\nYelp polarity (Zhang et al 2015) Item: Samples Pawelczyk et al (2023)\nIMDB (Maas et al 2011) Item: Samples Chen and Yang (2023)\nSAMSum (Gliwa et al 2019) Item: Samples Chen and Yang (2023)\nLEDGAR (Tuggener et al 2020) Item: Samples Wang et al (2023)\nPersonaChat (Zhang et al 2018) Item: Samples Wang et al (2023)\nIWSLT14 (Cettolo et al 2014) Item: Samples Wang et al (2023)\nEnron emails (Klimt and Yang 2004) Item: Private information Wu et al (2023); Borkar \n(2023)\nzsRE (Levy et al 2017) Item: Old facts Ni et al (2023)\nCounterFact (Meng et al 2022) Item: Old facts Ni et al (2023)\nCodeParrot GitHub Code (Tunstall et al 2022) Task: Programming Pochinkov and Schoots \n(2023)\n4https:   //gith ub. com/go ogle-res ea rch/l m-ext raction-benchmark\n1 3\nPage 25 of 41 90\nA. Blanco-Justicia et al.\nIn the following, we examine the metrics employed by the surveyed works to measure \nthe attained level of forgetting:\n ● Dataset-specific metrics: Most papers (Eldan and Russinovich 2023; Jang et al 2022; \nYao et al 2023; Pawelczyk et al 2023; Chen and Yang 2023; Ni et al 2023; Pochinkov \nand Schoots 2023) leverage the predefined performance metric of the forgetting dataset, \nthat is, the accuracy in a classification or completion dataset.\n ● Toxicity: Papers seeking to mitigate the toxicity of generated texts (Ilharco et al 2022; \nLu et al 2022) sometimes rely on off-the-shelf metrics, such as Perspective API4 or Tox-\n4      h t t  p s : / / g  i t h u  b . c o m  / c o n v  e r s a t i  o n a i  / p e r s  p e c t i  v e a p i        \nTable 3 Datasets only used for retaining\nDataset name Retaining target Paper/s\nWizard of Wikipedia (Dinan et al 2019) Dialogue Jang et al (2022)\nEmpathetic Dialogues (Rashkin et al 2019) Dialogue Jang et al (2022)\nBlended Skill Talk (Smith et al 2020) Dialogue Jang et al (2022)\nWizard of Internet (Komeili et al 2022) Dialogue Jang et al (2022)\npiqa (Bisk et al 2020) Q&A Eldan and Russinovich (2023); \nJang et al (2022)\nCOPA (Gordon et al 2012) Q&A Jang et al (2022)\nARC (Clark et al 2018) Q&A Jang et al (2022)\nMathQA (Amini et al 2019) Q&A Jang et al (2022)\nPubmedQA (Jin et al 2019) Q&A Jang et al (2022)\nTruthfulQA (Lin et al 2022) Q&A Yao et al (2023)\nGAP Coreference (Webster et al 2018) Coreference Limisiewicz and Mareček (2022)\nEnglish Web Treebank (Silveira et al 2014) Dependency Limisiewicz and Mareček (2022)\nWinoGrande (Sakaguchi et al 2020) Completion Eldan and Russinovich (2023); \nJang et al (2022)\nHellaSwag (Zellers et al 2019) Completion Eldan and Russinovich (2023); \nJang et al (2022)\nLambada (Paperno et al 2016) Completion Jang et al (2022)\nzsRE (Levy et al 2017) Completion Ni et al (2023)\nCounterFact (Meng et al 2022) Completion Ni et al (2023)\nPile (Gao et al 2021) Samples subset Jang et al (2022); Pochinkov and \nSchoots (2023)\nSST2 (Socher et al 2013) Samples subset Pawelczyk et al (2023)\nAmazon polarity (Zhang et al 2015) Samples subset Pawelczyk et al (2023)\nYelp polarity (Zhang et al 2015) Samples subset Pawelczyk et al (2023)\nIMDB (Maas et al 2011) Samples subset Chen and Yang (2023)\nSAMSum (Gliwa et al 2019) Samples subset Chen and Yang (2023)\nLEDGAR (Tuggener et al 2020) Samples subset Wang et al (2023)\nPersonaChat (Zhang et al 2018) Samples subset Wang et al (2023)\nIWSLT14 (Cettolo et al 2014) Samples subset Wang et al (2023)\nEnron emails (Klimt and Yang 2004) Generation Wu et al (2023); Borkar (2023)\nCodeParrot GitHub Code (Tunstall et al 2022) Generation Pochinkov and Schoots (2023)\nWikiText-103 (Merity et al 2016) Generation Ilharco et al (2022)\nBias in Bios (De-Arteaga et al 2019) Classification Belrose et al (2023)\n1 3\n90 Page 26 of 41\nDigital forgetting in large language models: a survey of unlearning…\nicity.5 These metrics involve training an LLM on a toxic dataset to predict the toxicity \nscore of input text.\n ● Bias: Works addressing bias in LLMs ( e.g., gender, race or stereotype) (Belrose et al \n2023; Limisiewicz and Mareček 2022; Yu et al 2023) commonly rely on the bias-sen -\nsitive prediction probability. For example, for pronoun prediction of a person with a \nknown occupation ( e.g., doctor), probabilities for both pronouns are expected to be \n50%. Under the premise of profession-induced gender bias, De-Arteaga et al ( 2019) \nintroduced the TPR-GAP metric, which assesses the difference (GAP) in the true posi -\ntive rate (TPR) between the two genders for each occupation. On the same basis, Lim -\nisiewicz and Mareček ( 2022) presented the relative gender preference (RGP) metric, \nwhich can be roughly defined as the difference in gender probabilities for texts with and \nwithout the profession name.\n ● Generation: For undesired knowledge with no specific metrics, the evaluation often \nfocuses on how easily the model can generate that undesired knowledge. The evaluation \nwill depend on whether the forgetting process aims to avoid generating text that exactly \nor approximately matches that of the forgetting request:\n – Exact generation: The goal is to prevent the model from generating verbatim copies \nof the text to be forgotten. This is common in scenarios involving copyright. To that \nend, the perplexity metric is often employed (Jang et al 2022; Yao et al 2023; Wu et \nal 2023; Pochinkov and Schoots 2023). This approach is sometimes followed when \n5      h t t  p s : / / g  i t h u  b . c o m / u n i t a r y a i / T o x i c i t y      \nTable 4 LLMs used for forgetting\nModel name Number of parameters Paper/s\nALBERT (Lan et al 2020) 11 M Yu et al (2023)\nDistilBERT (Sanh et al 2019) 67 M Wang et al (2023)\nBERT (Devlin et al 2019) 110 M Wu et al (2023); Yu et al (2023); \nLimisiewicz and Mareček \n(2022); Belrose et al (2023)\nELECTRA (Clark et al 2020) 110 M Limisiewicz and Mareček (2022)\nRoBERTa (Liu et al 2019) 355 M Pochinkov and Schoots (2023)\n125 M Yu et al (2023)\nGPT-2 (Radford et al 2019) 117 M, 355 M, 774 M Ilharco et al (2022)\n355 M, 774 M Lu et al (2022)\nBloom (Scao et al 2022) 560 M, 1.1B Pawelczyk et al (2023)\nPhi (Li et al 2023b) 1.5B Eldan and Russinovich (2023)\nGPT-Neo (Black et al 2022) 125 M, 1.3B, 2.7B Jang et al (2022)\nT5 (Raffel et al 2020) 223 M, 3B Chen and Yang (2023)\nOPT (Zhang et al 2022) 125 M, 1.3B, 6.7B Pochinkov and Schoots (2023)\n125 M, 1.3B, 2.7B Jang et al (2022)\n1.3B, 2.7B Yao et al (2023)\nGalactica (Taylor et al 2022) 125 M, 1.3B, 6.7B Pochinkov and Schoots (2023)\nLlama-2 (Touvron et al 2023b) 7B Yao et al (2023); Ni et al (2023)\nPythia (Biderman et al 2023) 160 M, 1.4B, 6.9B, 12B Belrose et al (2023)\n160 M, 1.4B, 6.9B Pochinkov and Schoots (2023)\nLlama (Touvron et al 2023a) 7B, 13B, 30B Belrose et al (2023)\n7B Eldan and Russinovich (2023)\n1 3\nPage 27 of 41 90\nA. Blanco-Justicia et al.\nthe objective is to avoid the generation of PII. Jang et al (2022) propose the extrac-\ntion likelihood metric, which considers overlaps of n-grams, and the memorization \naccuracy metric, for matching identical token sequences. They define unlearning \nas complete when both metrics are lower than a threshold for the forgetting test \nset. Wu et al ( 2023) adapt their metrics to the type of PII to be forgotten, such as \nexposure for phone numbers and mean reciprocal rank for names. Note that most of \nthese evaluations do not use a forgetting test set, but focus on exact replicates. This \napproach may not be enough to deal with privacy issues, since we should prevent \nthe generation of the targeted sensitive concept, not only the concrete way it is \nexpressed (unlike copyright).\n – Similar generation : In this scenario, we consider any type of generations which \nsuggest that the model has been trained with undesired knowledge. For instance, \nWang et al (2023) measure output similarity with the text to be forgotten by using \nthe Jensen-Shannon Divergence (JSD), the Language model Probability Distance \n(LPD), and the Proportion of instances with Decreased Language model Probabil -\nity (PDLP), while Yao et al ( 2023) leverage the BiLingual Evaluation Understudy \n(BLEU) score. Eldan and Russinovich (2023), who aimed to forget the Harry Potter \nuniverse corpora, opted for a more fine-grained evaluation, using a forgetting test \nset curated to reflect knowledge of the Harry Potter books.\n ● Membership Inference Attack (MIA) : A few authors (Wang et al 2023; Chen and \nYang 2023; Pawelczyk et al 2023) use the success of a MIA as a metric. This type of at-\ntack aims to determine whether a piece of knowledge was used during the (pre-)training \nof the model. See Shokri et al (2017); Yeom et al (2018) for a primer on MIAs, Carlini \net al (2021); Nasr et al (2023) for examples of black-box MIA on LLMs, and Patil et al \n(2023) for an example of a white-box MIA on LLMs.\nAmong the surveyed works, we have found two main approaches for the evaluation of \nforgetting. The first, a more realistic scenario contemplated by Jang et al ( 2022); Yao et \nal (2023); Yu et al (2023); Limisiewicz and Mareček ( 2022), involves applying forgetting \ntechniques to a standard, publicly available, pre-trained LLM with pre-existing biases from \nits training data. This approach has some constraints, notably that the undesired knowledge \nmust be well-represented in the pre-training dataset and that an external dataset may be \nneeded if the specific undesired data are not identifiable enough. Due to these limitations, \nsome researchers, such as Eldan and Russinovich ( 2023); Chen and Yang ( 2023); Borkar \n(2023); Wu et al (2023); Wang et al (2023) prefer a second, more controllable but less real-\nistic scenario, where forgetting is applied to an LLM fine-tuned with a dataset that includes \nthe undesired knowledge. This allows for better control over the process, with the original, \nunaltered LLM serving as a baseline for comparison. However, this scenario risks unrealis-\ntic results due to the potential overfitting on the undesired knowledge.\n5.4 Retaining evaluation\nThe assessment of model retaining often involves evaluating the utility difference of the \nLLM before and after the forgetting process on tasks different from the undesired knowl -\nedge. The smaller the utility difference, the better the retention. Ideally, the retention evalu-\n1 3\n90 Page 28 of 41\nDigital forgetting in large language models: a survey of unlearning…\nation should encompass all possible knowledge except that to be forgotten. Nonetheless, \nthis is unfeasible in most cases, given the wide range of capabilities that LLMs can learn. \nInstead, researchers define the forgetting test set by selecting the subset of the most relevant \ntasks for which utility should be retained.\nIn the following, we list the most prominent strategies used to measure the retained \nutility:\n ● General benchmarks. Eldan and Russinovich (2023), Wang et al (2023), and Jang et al \n(2022) leverage well-established LLM benchmarks, such as those used for comparing \nLLMs (Sakaguchi et al 2020; Zellers et al 2019; Paperno et al 2016; Bisk et al 2020). \nThese benchmarks assess the model’s capabilities in NLU, reasoning, and human-like \ngeneration. The performance of the original and resulting models in these general tasks \nis compared to assess retaining.\n ● Related but different. A reasonable assumption is that knowledge conceptually close \nto that to be forgotten will be the most affected by the unlearning process. Therefore, the \nperformance of the models in these domains may be an appropriate indicator of whether \nthe model is retaining the remaining capabilities or unlearning is affecting more than \njust the undesired knowledge. Following this principle, several authors evaluate retain-\ning in domains related but different to the undesired knowledge (Yao et al 2023; Lu et al \n2022; Limisiewicz and Mareček 2022; Belrose et al 2023; Chen and Yang 2023; Wang \net al 2023; Pawelczyk et al 2023; Wu et al 2023; Ni et al 2023; Pochinkov and Schoots \n2023). The specific strategies employed differ in terms of closeness and/or relatedness \nto the forgetting target:\n – Related dataset. Lu et al (2022); Yao et al (2023); Limisiewicz and Mareček (2022); \nIlharco et al ( 2022) choose datasets different from but related to those used for \nunlearning. For instance, Lu et al (2022) measured toxicity, fluency and diversity in \nthe WritingPrompts dataset (Fan et al 2018) to measure the retaining of the model’s \nwriting capabilities when forgetting toxic generations from the RealToxicPrompts \ndataset (Gehman et al 2020). Similarly, Yao et al (2023) leveraged BLEURT (Sel-\nlam et al 2020) and the deberta-v3-large-v2 reward model 6 to compare the gen -\nerations for TruthfulQA (Lin et al 2022) questions by the original LLM with those \nresulting from unlearning harmful Q&A pairs of PKU-SafeRLHF (Ji et al 2023).\n – Same dataset but different samples . Chen and Yang ( 2023); Wang et al ( 2023); \nPawelczyk et al ( 2023); Ni et al ( 2023); Pochinkov and Schoots ( 2023) evaluate \nretaining on the remaining samples of the same dataset used for unlearning. For \nexample, Pochinkov and Schoots (2023) forget Python code samples from Tunstall \net al (2022) while aiming to retain the remaining code samples.\n – Same dataset but different task. The forgetting dataset is used to measure the mod -\nel’s capabilities in a task disjoint of that to be forgotten (Belrose et al 2023; Wu et \nal 2023). For instance, the Bias in Bios dataset (De-Arteaga et al 2019) is employed \nby Belrose et al ( 2023) for both forgetting gender bias and measuring retaining in \nthe job prediction task. Another example is provided by Wu et al (2023), who aim to \nforget PII from the Enron dataset (Klimt and Yang 2004), and measure retaining as \nthe generation quality for the rest of the corpus.\n6     h t t p s : / / h u g g i n g f a c e . c o / O p e n A s s i s t a n t / r e w a r d - m o d e l - d e b e r t a - v 3 - l a r g e - v 2      \n1 3\nPage 29 of 41 90\nA. Blanco-Justicia et al.\nThe utility degradation observed in these studies usually falls within the range of 1–5%, \nalthough it can occasionally reach up to 20%. Jang et al (2022) and Pawelczyk et al (2023) \nshow that larger models tend to exhibit better utility retention.\n5.5 Runtime evaluation\nThe runtime of unlearning methods should always be significantly lower than the runtime of \nretraining the models from scratch, which is the baseline for unlearning. However, unlearn-\ning is rarely a fast procedure, since it usually involves costly training-related steps, such as \nfine-tuning.\nSeveral of the surveyed works explicitly report the runtime of their methods. Reported \nvalues range from minutes (Wang et al 2023; Yao et al 2023; Limisiewicz and Mareček \n2022; Jang et al 2022; Yu et al 2023) to hours (Chen and Yang 2023) or even longer than a \nday (Lu et al 2022). However, directly comparing the runtime reported by different works \nmight be unfair, due to the different hardware configurations, models, and/or undesired \nknowledge.\nThere are some workarounds to compensate for hardware discrepancies, although they are \nnot universally applicable. First, the runtime can be normalized to the most common GPU, \nadjusting runtime from other GPUs based on their relative performance in deep learning, \nas assessed through specific benchmarks. However, the substantial memory requirements \nof LLMs often require multi-GPU configurations with the corresponding interconnections. \nThe latency of these interconnections also influences runtime, but this is rarely reported in \nthe literature. Second, for those methods whose time cost is predominantly determined by \nfine-tuning, the number of epochs can be used as a more general indicator of cost. However, \nthis information is only reported by a subset of works (Eldan and Russinovich 2023; Chen \nand Yang 2023; Jang et al 2022; Yao et al 2023; Yu et al 2023; Wu et al 2023; Ni et al 2023; \nIlharco et al 2022) and it can only be used when comparing identical models, data and for -\ngetting tasks, which is rarely the case.\n6 Challenges and recommendations\nThis section outlines several challenges of machine unlearning we have identified when \nconsidering the technical and legal requirements of digital forgetting and the surveyed \nworks. Our goal is to guide researchers and practitioners in: i) the identification and applica-\ntion of unlearning techniques that are suitable to their needs; and ii) the evaluation of the \neffectiveness of those techniques.\n6.1 Managing forgetting request types\nA first challenge for practitioners is how to process forgetting or unlearning requests. We \nstart with a description of general forgetting requests as implemented to comply with the \nright of erasure described in the GDPR (European Parliament and Council of the European \nUnion, 2016).\nThe right to erasure in Article 17 of the GDPR states that: “The data subject shall have \nthe right to obtain from the controller the erasure of personal data concerning him or her \n1 3\n90 Page 30 of 41\nDigital forgetting in large language models: a survey of unlearning…\nwithout undue delay and the controller shall have the obligation to erase personal data with-\nout undue delay [...]”. The regulation does not specify any form or information the data \nsubject has to provide to the data controller to exercise their right. Search engines, such as \nGoogle Search, provide specific forms to exercise the right to be forgotten. In their forms, \nGoogle requests the data subjects to identify, and then provide a list of URLs that contain \npersonal information about them, which specific search queries point to the documents of \ninterest, and the motivation for the erasure.7 Therefore, the implementation of the right to be \nforgotten revolves around removing documents that are present on the internet and contain \nsome personal information about the data subject from search results. This erasure does not \nimply all information about a data subject has to be deleted from search results, but only that \ninformation requested by the data subject.\nHow these requests translate to large generative models, and LLMs in particular, and \nto requests other than those concerned with data privacy is a subject to be further studied, \nbut we should expect requests that include the problematic outputs of LLMs along with \nthe inputs (or prompts) that triggered them. Given that information, the model owner will \nbe able to determine the nature of the problematic outputs and decide whether the issue is \nrelated to privacy or copyright, to the generation of non-factual or incorrect information, or \nto the generation of outputs that are not aligned with human values, such as biased, toxic, \nor harmful information.\nThe unlearning mechanisms surveyed in Sect. 4, along with other forgetting mechanisms \nmentioned in Sect. 3 apply to different unlearning targets, with different guarantees, evalu -\nation methods, and costs. Thus, the choice of a forgetting or unlearning mechanism will \ndepend on the type of knowledge to be forgotten and on the balance of three, often conflict-\ning, properties, namely, the unlearning guarantees provided by the mechanism, the utility of \nthe resulting model, and the execution time of the solution.\nWe next summarize our findings on the available unlearning mechanisms, describing the \navailable options for each of the possible unlearning requests.\nForgetting procedures aimed at removing private information are covered by many of \nthe works we have surveyed. One of the reasons for that is that privacy has been a topic of \nresearch for several years, and definitions of privacy originally thought for databases have \ncarried over into the machine learning field. Not surprisingly, the unlearning guarantees \ndefined by Nguyen et al (2022) and Xu et al (2023) are heavily inspired by differential pri-\nvacy, and privacy attacks, such as membership inference attacks or reconstruction attacks, \nhave been used to measure the risk of privacy leaks and to evaluate the effectiveness of \nunlearning. This has resulted in a very rich literature on unlearning mechanisms dealing \nwith private information, even in the context of LLMs. Therefore, practitioners can choose \nfrom a wide range of methods, which include methods that provide exact guarantees but at \nhigh computing cost, such as SISA (whose set-up cost is equivalent to that of retraining from \nscratch, but which makes the processing of unlearning requests faster) as well as methods \noffering no guarantees but fast execution, such as ICUL. Even when no ex ante guarantees \nexist, evaluation of unlearning for privacy (which amounts to evaluating whether the model \nknows a particular information) has been studied for years, and techniques such as MIAs \ncan be used for empirical evaluation (Carlini et al 2021; Nasr et al 2023). One of the main \nchallenges of unlearning private information is that most methods focus on unlearning items \nor data points from the models. Whereas that might be enough to remove PII such as identi-\n7   G o o g l e ’ s Right to be Forgotten form – https://reportcontent.google.com/forms/rtbf\n1 3\nPage 31 of 41 90\nA. Blanco-Justicia et al.\nfiers or specific sensitive information, it might not suffice for quasi-identifiers (attributes \nof individuals that are not direct identifiers but that im combination can uniquely identify \nan individual). Unlearning approaches that deal with this issue should not be devoted to \ndeleting specific information about individuals, but to removing any possible correlations \nbetween someone’s identity and all the set of attributes that could describe that person.\nAddressing copyright issues through unlearning is quite similar to the case of privacy. All \nbut one of the works we have surveyed that apply to privacy also apply to copyright issues, \nas do guarantees, costs, and evaluation mechanisms and criteria. However, we find a distinc-\ntion between forgetting or unlearning copyrighted material –needed when the rightholders \nforbid any processing of their protected work by the developers of an LLM–, where the \napproaches described above would apply, and ensuring that generative models do not output \ncopyrighted material, which could constitute plagiarism. In the latter case, an unlearning \nprocedure would just need to ensure that no verbatim fragments of a protected work are \ngenerated by the LLM. There is, however, a right to quote works for the purposes of com -\nmentary or criticism. Therefore, additional measures should be taken to make LLMs attri -\nbute quotes accurately. This is an open problem that requires more study.\nWhen forgetting requests refer to model robustness, where we include the removal of \nfactually incorrect information, fake news, or outdated information, we face a similar situ -\nation to that of privacy or copyright protection. Note that, according to the works we have \nsurveyed, methods that target items apply seamlessly to several of the issues we want to \ncorrect using unlearning. The reason is that an item is a piece of data that can be as small \nas a token or as big as a whole document. As above, choosing which mechanism to use will \ndepend on the desired guarantees and runtime.\nFinally, we cover alignment issues, which include discrimination, the generation of toxic, \nharmful, or hateful outputs, and the unlearning of unwanted capabilities. Regarding methods \nthat focus on fairness or the mitigation of biases and discrimination, our survey has revealed \nthat most studies focus on gender discrimination related to pronouns (e.g., “doctor” is more \nfrequently associated with “him” while “nurse” is more frequently associated with “her”). \nWhile addressing this issue is crucial, we believe it represents a limited perspective on bias, \nwhich in fact manifests across many more dimensions. Thus, we observe that more research \nshould be conducted in this domain, covering different aspects of discrimination and bias. \nAddressing all potential biases in text documents could significantly impact the models’ \nretaining performance. Also, the lack of a clear definition or methodology for approaching \nbias complicates the evaluation of bias mitigation efforts. Each type of bias –such as gender, \nethnicity, or race– may require unique evaluation datasets and methods. Furthermore, most \nmethods designed to correct biases offer little to no concrete guarantees, requiring practitio-\nners to continuously monitor and assess any potential biases present in their models. It may \nbe beneficial to discuss the need for standardized frameworks or benchmarks that could pro-\nvide a more systematic approach to evaluate and address biases in AI models. An even less \nconcrete type of requests are those related to the generation of toxic, harmful, hateful, or hal-\nlucinated content. These phenomena are difficult to define in many cases because of cultural \ndifferences, the use of humor and sarcasm, the importance of context, etc. Methods that deal \nwith these issues are often based on further fine-tuning and input and output manipulation, \nand therefore only provide at most weak guarantees of unlearning. For toxic and hateful \ncontent, evaluation can be carried out leveraging other LLMs such as HateBERT.\n1 3\n90 Page 32 of 41\nDigital forgetting in large language models: a survey of unlearning…\nWe have observed that methods dealing with alignment issues (with the exception of the \ngeneration of toxic content, which in some cases can be tackled with the same mechanisms \nfor privacy and copyright issues) have garnered less attention in the literature. As discussed \nabove, a possible reason is that defining what is and what is not undesirable knowledge \nis difficult in the alignment scenario. Most methods, therefore, offer at most approximate \nguarantees; even in this case, the empirical evaluation of the unlearning performance is not \nwell established.\n6.2 Guarantees of forgetting\nWe now discuss the limitations of ex ante guarantees of forgetting.\nThe forgetting guarantees presented in Sect. 2.3 were mainly devised for general neu -\nral network classifiers. Although they can still be used for LLMs, some factors limit their \napplicability. On the one hand, the guarantees refer to the parameters of the resulting models \nafter training or forgetting. This means that, for an ex ante guarantee of forgetting, a clear \napproximation of the influence of every training sample on the model weights is known or \ncan be computed. Whereas this may be feasible for small or simpler models, the complexity \nof LLMs can make such analyses difficult if not impossible.\nOn the other hand, although unlearning an image or a record is relatively straightforward \nin typical machine unlearning, the complexity increases when dealing with text data in \nLLMs. If the goal is to unlearn a text sequence containing sensitive information, not only is \nidentifying explicit sensitive data difficult, but identifying implicit sensitive data (data that \nmay allow sensitive inferences) is even more challenging. This challenge also applies when \nunlearning copyrighted documents, especially in non-fiction texts, where identifying the \nunique tokens associated with these documents becomes tedious. When the aim is to elimi-\nnate model bias or hallucination, the complexity increases further. Bias is often widespread \nand difficult to detect in training text data, as it appears in scattered patterns across many \nexamples in both explicit and implicit forms. Thus, unlearners often resort to identifying \nbiased concepts within internal model representations and addressing them at this level. \nSimilarly, addressing hallucinations poses a significant challenge, as they often stem from \nmultiple sources, making identification a nontrivial task.\n6.3 Evaluation\nRegarding the empirical evaluation of unlearning and retaining capabilities, we have found \nthat the surveyed works often use specific datasets, and this may lower the applicability \nof many of these evaluations to the general case. To ensure the methods are not dataset-\nspecific, diverse datasets for the same target task must be used.\nUnlearning in LLMs is more challenging than in traditional classification models due to \nthe vast output space of language models (their outputs are not just a class label), higher \nefficiency requirements, and limited access to training data. This makes evaluations dif -\nficult. Currently, no de facto standard datasets exist to explicitly evaluate unlearning meth -\nods. Researchers typically assess these methods across diverse datasets based on individual \nconsiderations.\nAnother challenge is that all methods are evaluated on English language datasets and \nthere is no evidence about their performance with other languages. More datasets on differ-\n1 3\nPage 33 of 41 90\nA. Blanco-Justicia et al.\nent unlearning applications are required to assess how well unlearning methods generalize \nacross different languages.\nFurthermore, even if unlearning is motivated by concerns about private information leak-\nage, the datasets used in the literature rarely contain sensitive information. To better validate \nthe effectiveness of unlearning for sensitive information, future research should use simu -\nlated datasets with representative distributions of sensitive information.\n6.4 Balancing forgetting requirements\nIn order for practitioners and operators of LLM-based services to comply with forgetting \nrequests, whatever their type and target, they can follow a tiered approach that balances the \nruntime of the solutions (in order to comply with requests in a timely manner) against the \nforgetting guarantees and the retain performance.\nInput/output modification approaches, described in Sect. 4.4, along with output post-\nprocessing methods, can be deployed almost instantly to steer the model behavior towards a \nmore desirable one without impacting the retain performance in any significant way. How-\never, these approaches can only provide weak unlearning guarantees, which in some cases \ncan be reverted by users using carefully crafted prompts. Although these approaches can \nserve as a fast hotfix to LLMs, they are only recommended as a short-term approach while \nother, more robust but more costly, approaches are applied in parallel.\nUnlearning methods based on local modifications or fine-tuning offer a better balance \nbetween speed, unlearning guarantees, and utility of the unlearned models. All of the \nsurveyed methods in these categories offer approximate guarantees, which in most cases \nshould be sufficient to comply with forgetting requests, especially those related to privacy \nand copyright issues (dealing with biases and the generation of harmful content is a more \ncomplex issue). These methods can be used to consolidate changes made with the previous \napproaches without the need to retrain the models.\nA more definitive approach to complying with forgetting requests is to retrain the models \nfrom scratch, applying any necessary data pre-processing or privacy-preserving training \ntechniques. Mechanisms such as SISA can also be applied in order to prepare for future \nunlearning requests. All these approaches provide exact guarantees and a good utility of the \nresulting models, but they require significant resources.\n7 Conclusions\nWe have surveyed the state of the art in digital forgetting and machine unlearning in LLMs \nfrom several points of view.\nFirst, we have reviewed and characterized the motivation, the types, the requirements \nand the main approaches to digital forgetting in LLMs. Next, we have conducted the most \ncomprehensive and up-to-date survey on unlearning methods for LLMs in the literature, \nencompassing 22 works, most of them published in 2023 and 2024. Unlearning methods \nhave been organized according to a fine-grained taxonomy, which classifies them accord -\ning to the type of modification they perform on the model and their fundamental working \nprinciple. We have also surveyed the way the proposed methods are evaluated for forgetting, \nretaining and runtime; we have identified 28 datasets used for evaluation of forgetting, 31 \n1 3\n90 Page 34 of 41\nDigital forgetting in large language models: a survey of unlearning…\ndatasets used for evaluation of retaining, and 15 LLMs used by the surveyed methods (plus \nsize variations).\nFinally, we have identified the most outstanding challenges of machine unlearning in \nLLMs, and we have proposed recommendations for practitioners on how to choose specific \nmethods depending on their needs. We believe the scientific community and the profession-\nals developing, managing, and deploying models can benefit from our recommendations on \nhow to apply machine unlearning methods to satisfy their needs, possibly imposed by the \nexisting regulations.\nAcknowledgements This work has been funded by Huawei Technologies Finland Research Center. Partial \nsupport to this work has been received from the European Commission (project H2020-871042 “SoBig -\nData++”), the Government of Catalonia (ICREA Acadèmia Prizes to J. Domingo-Ferrer and to D. Sánchez, \nand grant 2021SGR-00115), MCIN/AEI/ 10.13039/501100011033 and “ERDF A way of making Europe” \nunder grant PID2021-123637NB-I00 “CURLING”, and the EU’s NextGenerationEU/PRTR via INCIBE \n(project “HERMES” and INCIBE-URV cybersecurity chair).\nFunding Open Access funding provided thanks to the CRUE-CSIC agreement with Springer Nature.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons \nlicence, and indicate if changes were made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. \nIf material is not included in the article’s Creative Commons licence and your intended use is not permitted \nby statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the \ncopyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\nAbadi M, Chu A, Goodfellow I, et al (2016) Deep learning with differential privacy. In: Proceedings of the \n2016 ACM SIGSAC conference on computer and communications security, pp 308–318\nAmini A, Gabriel S, Lin S, et al (2019) MathQA: Towards interpretable math word problem solving with \noperation-based formalisms. In: Burstein J, Doran C, Solorio T (eds) Proceedings of the 2019 Confer -\nence of the North American Chapter of the Association for Computational Linguistics: Human Lan -\nguage Technologies, V olume 1 (Long and Short Papers). Association for Computational Linguistics, \nMinneapolis, Minnesota, pp 2357–2367, https://doi.org/10.18653/v1/N19-1245,  h t t p s : / / a c l a n t h o l o g y . \no r g / N 1 9 - 1 2 4 5       \nBelrose N, Schneider-Joseph D, Ravfogel S, et al (2023) Leace: Perfect linear concept erasure in closed form. \nPreprint at arXiv:2306.03819\nBiderman S, Schoelkopf H, Anthony QG, et al (2023) Pythia: A suite for analyzing large language models \nacross training and scaling. In: International Conference on Machine Learning, PMLR, pp 2397–2430\nBisk Y , Zellers R, Bras RL, et al (2020) PIQA: reasoning about physical commonsense in natural language. \nIn: The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second \nInnovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium \non Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY , USA, February 7-12, \n2020. AAAI Press, pp 7432–743https://doi.org/10.1609/AAAI.V34I05.6239,\nBlack S, Gao L, Wang P, et al (2022) Gpt-neo: Large scale autoregressive language modeling with mesh-\ntensorflow, 2021. URL: https://doiorg/105281/zenodo5297715\nBorkan D, Dixon L, Sorensen J, et al (2019) Nuanced metrics for measuring unintended bias with real data \nfor text classification. In: Companion Proceedings of The 2019 World Wide Web Conference. Associa-\ntion for Computing Machinery, New York, NY , USA, WWW ’19, p 491–500,  h t t p s : / / d o i . o r g / 1 0 . 1 1 4 5 / \n3 3 0 8 5 6 0 . 3 3 1 7 5 9 3     ,   \nBorkar J (2023) What can we learn from data leakage and unlearning for law? Preprint at arXiv:2307.10476\n1 3\nPage 35 of 41 90\nA. Blanco-Justicia et al.\nBourtoule L, Chandrasekaran V , Choquette-Choo CA, et al (2021) Machine unlearning. In: 2021 IEEE Sym-\nposium on Security and Privacy (SP), IEEE, pp 141–159\nCarlini N, Tramer F, Wallace E, et al (2021) Extracting training data from large language models. In: 30th \nUSENIX Security Symposium (USENIX Security 21), pp 2633–2650\nCettolo M, Niehues J, Stüker S, et al (2014) Report on the 11th IWSLT evaluation campaign. In: Federico M, \nStüker S, Yvon F (eds) Proceedings of the 11th International Workshop on Spoken Language Transla -\ntion: Evaluation Campaign@IWSLT 2014, Lake Tahoe, CA, USA, December 4-5, 2014,  h t t p s  : / / a c l  a n t \nh o  l o g y  . o r g / 2 0 1 4 . i w s l t - e v a l u a t i o n . 1       \nChen J, Yang D (2023) Unlearn what you want to forget: Efficient unlearning for llms. Preprint at \narXiv:2310.20150\nClark K, Luong M, Le QV , et al (2020) ELECTRA: pre-training text encoders as discriminators rather than \ngenerators. In: 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, \nEthiopia, April 26-30, 2020. OpenReview.net, https://openreview.net/forum?id=r1xMH1BtvB\nClark P, Cowhey I, Etzioni O, et al (2018) Think you have solved question answering? try arc, the AI2 reason-\ning challenge. CoRR abs/1803.05457. arXiv:1803.05457\nComputer T (2023) Redpajama: an open dataset for training large language models.  h t t p s  : / / g i t  h u b . c  o m / t  o g e \nt h e r c o m p u t e r / R e d P a j a m a - D a t a       \nDaheim N, Dziri N, Sachan M, et al (2023) Elastic weight removal for faithful and abstractive dialogue gen-\neration. Preprint at arXiv:2303.17574\nDe-Arteaga M, Romanov A, Wallach HM, et al (2019) Bias in bios: A case study of semantic representation \nbias in a high-stakes setting. In: danah boyd, Morgenstern JH (eds) Proceedings of the Conference on \nFairness, Accountability, and Transparency, FAT* 2019, Atlanta, GA, USA, January 29-31, 2019. ACM, \npp 120–128, https://doi.org/10.1145/3287560.3287572, https://doi.org/10.1145/3287560.3287572\nDevlin J, Chang MW, Lee K, et al (2018) Bert: Pre-training of deep bidirectional transformers for language \nunderstanding. Preprint at arXiv:1810.04805\nDevlin J, Chang M, Lee K, et al (2019) BERT: pre-training of deep bidirectional transformers for language \nunderstanding. In: Burstein J, Doran C, Solorio T (eds) Proceedings of the 2019 Conference of the \nNorth American Chapter of the Association for Computational Linguistics: Human Language Technolo-\ngies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, V olume 1 (Long and Short Papers). \nAssociation for Computational Linguistics, pp 4171–418https://doi.org/10.18653/V1/N19-1423\nDinan E, Roller S, Shuster K, et al (2019) Wizard of wikipedia: Knowledge-powered conversational agents. \nIn: International Conference on Learning Representations\nEldan R, Russinovich M (2023) Who’s harry potter? approximate unlearning in llms. Preprint at \narXiv:2310.02238\nCommission European (2019) Ethics guidelines for trustworthy AI. Publications Office of the EU, doi/. \nhttps://doi.org/10.2759/346720\nEuropean Parliament (2024) European parliament legislative resolution of 13 march 2024 on the proposal for \na regulation of the european parliament and of the council on laying down harmonised rules on artificial \nintelligence (artificial intelligence act) and amending certain union legislative acts (com(2021)0206 – \nc9-0146/2021 – 2021/0106(cod)).  h t t p s  : / / w w w  . e u r o  p a r l  . e u r o p a . e u / d o c e o / d o c u m e n t / T A - 9 - 2 0 2 4 - 0 1 3 8 _ \nE N . h t m l       \nEuropean Parliament, Council of the European Union (2016) Regulation (EU) 2016/679 of the European \nParliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the \nprocessing of personal data and on the free movement of such data, and repealing Directive 95/46/EC \n(General Data Protection Regulation). https://data.europa.eu/eli/reg/2016/679/oj\nFan A, Lewis M, Dauphin YN (2018) Hierarchical neural story generation. In: Gurevych I, Miyao Y (eds) \nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, \nMelbourne, Australia, July 15-20, 2018, V olume 1: Long Papers. Association for Computational Lin -\nguistics, pp 889–89https://doi.org/10.18653/V1/P18-1082, https://aclanthology.org/P18-1082/\nGallegos IO, Rossi RA, Barrow J, et al (2024) Self-debiasing large language models: Zero-shot recognition \nand reduction of stereotypes. Preprint at arXiv:2402.01981\nGao L, Biderman S, Black S, et al (2021) The pile: An 800gb dataset of diverse text for language modeling. \nCoRR abs/2101.00027. arXiv:2101.00027\nGehman S, Gururangan S, Sap M, et al (2020) Realtoxicityprompts: Evaluating neural toxic degeneration in \nlanguage models. In: Findings\nGliwa B, Mochol I, Biesek M, et al (2019) Samsum corpus: A human-annotated dialogue dataset for abstrac-\ntive summarization. CoRR abs/1911.12237. arXiv:1911.12237\n1 3\n90 Page 36 of 41\nDigital forgetting in large language models: a survey of unlearning…\nGordon A, Kozareva Z, Roemmele M (2012) SemEval-2012 task 7: Choice of plausible alternatives: An \nevaluation of commonsense causal reasoning. In: Agirre E, Bos J, Diab M, et al (eds) *SEM 2012: The \nFirst Joint Conference on Lexical and Computational Semantics – V olume 1: Proceedings of the main \nconference and the shared task, and V olume 2: Proceedings of the Sixth International Workshop on \nSemantic Evaluation (SemEval 2012). Association for Computational Linguistics, Montréal, Canada, \npp 394–398, https://aclanthology.org/S12-1052\nGroup of Seven (G7) (2023) Hiroshima process international guiding principles for organizations developing \nadvanced ai systems. https://www.mofa.go.jp/files/100573471.pdf\nHassan F, Sánchez D, Soria-Comas J, et al (2019) Automatic anonymization of textual documents: detecting \nsensitive information via word embeddings. In: 2019 18th IEEE International Conference On Trust, \nSecurity And Privacy In Computing And Communications/13th IEEE International Conference On Big \nData Science And Engineering (TrustCom/BigDataSE), IEEE, pp 358–365\nHassan F, Sá¡nchez D, Domingo-Ferrer J (2023) Utility-preserving privacy protection of textual documents \nvia word embeddings. IEEE Transactions on Knowledge and Data Engineering 35(1):1058–1071\nHoulsby N, Giurgiu A, Jastrzebski S, et al (2019) Parameter-efficient transfer learning for nlp. In: Interna -\ntional Conference on Machine Learning, PMLR, pp 2790–2799\nIlharco G, Ribeiro MT, Wortsman M, et al (2022) Editing models with task arithmetic. Preprint at \narXiv:2212.04089\nJang J, Yoon D, Yang S, et al (2022) Knowledge unlearning for mitigating privacy risks in language models. \nPreprint at arXiv:2210.01504\nJi J, Liu M, Dai J, et al (2023) Beavertails: Towards improved safety alignment of LLM via a human-prefer-\nence dataset. CoRR abs/2307.04657. https://doi.org/10.48550/ARXIV .2307.04657, arXiv:2307.04657\nJin Q, Dhingra B, Liu Z, et al. (2019) Pubmedqa: A dataset for biomedical research question answering. \nIn: Inui K, Jiang J, Ng V , et al. (eds) Proceedings of the 2019 Conference on Empirical Methods in \nNatural Language Processing and the 9th International Joint Conference on Natural Language Process-\ning, EMNLP-IJCNLP 2019, Hong Kong, China, November 3–7, 2019. Association for Computational \nLinguistics, pp 2567–2577, https://doi.org/10.18653/V1/D19-1259\nJoseph R. Biden Jr. (2023) Executive order on the safe, secure, and trustworthy development and use of \nartificial intelligence.  h t t p s  : / / w w w  . w h i t  e h o u  s e . g o  v / b r i e  fi  n g -  r o o m  / p r e s  i d e n t i  a l - a c  t i o n  s / 2 0 2 3 / 1 0 / 3 0 / e x e c \nu t i v e - o r d e r - o n - t h e - s a f e - s e c u r e - a n d - t r u s t w o r t h y - d e v e l o p m e n t - a n d - u s e - o f - a r t i fi  c i a l - i n t e l l i g e n c e /       \nKadhe S, Halimi A, Rawat A, et al (2023) Fairsisa: Ensemble post-processing to improve fairness of unlearn-\ning in llms. In: Socially Responsible Language Modelling Research\nKandpal N, Wallace E, Raffel C (2022) Deduplicating training data mitigates privacy risks in language mod-\nels. arXiv:2202.06539\nKlimt B, Yang Y (2004) Introducing the enron corpus. In: CEAS 2004 - First Conference on Email and Anti-\nSpam, July 30-31, 2004, Mountain View, California, USA, http://www.ceas.cc/papers-2004/168.pdf\nKomeili M, Shuster K, Weston J (2022) Internet-augmented dialogue generation. In: Muresan S, Nakov P, \nVillavicencio A (eds) Proceedings of the 60th Annual Meeting of the Association for Computational \nLinguistics (V olume 1: Long Papers). Association for Computational Linguistics, Dublin, Ireland, pp \n8460–8478, https://doi .org/10.186 53/v1/2022. acl-long .579, https://aclanthology.org/2022.acl-long.579\nKumar VB, Gangadharaiah R, Roth D (2022) Privacy adhering machine un-learning in nlp. Preprint at \narXiv:2212.09573\nLan Z, Chen M, Goodman S, et al (2020) ALBERT: A lite BERT for self-supervised learning of language rep-\nresentations. In: 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, \nEthiopia, April 26-30, 2020. OpenReview.net, https://openreview.net/forum?id=H1eA7AEtvS\nLeong CT, Cheng Y , Wang J, et al (2023) Self-detoxifying language models via toxification reversal. Preprint \nat arXiv:2310.09573\nLevy O, Seo M, Choi E, et al (2017) Zero-shot relation extraction via reading comprehension. In: Levy \nR, Specia L (eds) Proceedings of the 21st Conference on Computational Natural Language Learning \n(CoNLL 2017), Vancouver, Canada, August 3-4, 2017. Association for Computational Linguistics, pp \n333–342, https://doi.org/10.18653/V1/K17-1034\nLi J, Cheng X, Zhao X, et al (2023a) Halueval: A large-scale hallucination evaluation benchmark for large \nlanguage models. In: Bouamor H, Pino J, Bali K (eds) Proceedings of the 2023 Conference on Empiri-\ncal Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023. Asso -\nciation for Computational Linguistics, pp 6449–6464, https://aclanthology.org/2023.emnlp-main.397\nLi Y , Bubeck S, Eldan R, et al (2023b) Textbooks are all you need ii: phi-1.5 technical report. Preprint at \narXiv:2309.05463\nLimisiewicz T, Mareček D (2022) Don’t forget about pronouns: Removing gender bias in language models \nwithout losing factual gender information. Preprint at arXiv:2206.10744\n1 3\nPage 37 of 41 90\nA. Blanco-Justicia et al.\nLin S, Hilton J, Evans O (2022) Truthfulqa: Measuring how models mimic human falsehoods. In: Muresan \nS, Nakov P, Villavicencio A (eds) Proceedings of the 60th Annual Meeting of the Association for Com-\nputational Linguistics (V olume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022. Associa-\ntion for Computational Linguistics, pp 3214–3252, https://doi .org/10.186 53/V1/2022. ACL-LONG .229,\nLison P, Pilán I, Sánchez D, et al (2021) Anonymisation models for text data: State of the art, challenges and \nfuture directions. In: Proceedings of the 59th Annual Meeting of the Association for Computational \nLinguistics and the 11th International Joint Conference on Natural Language Processing (V olume 1: \nLong Papers), pp 4188–4203\nLiu P, Yuan W, Fu J et al (2023) Pre-train, prompt, and predict: a systematic survey of prompting methods in \nnatural language processing. ACM Comp Surv 55(9):1–35\nLiu Y , Ott M, Goyal N, et al (2019) Roberta: A robustly optimized BERT pretraining approach. CoRR \nabs/1907.11692. arXiv:1907.11692\nLiu Z, Kalinli O (2023) Forgetting private textual sequences in language models via leave-one-out ensemble. \nPreprint at arXiv:2309.16082\nLu X, Welleck S, Hessel J et al (2022) Quark: controllable text generation with reinforced unlearning. Adv \nNeural Inf Process Syst 35:27591–27609\nMaas AL, Daly RE, Pham PT, et al (2011) Learning word vectors for sentiment analysis. In: Lin D, Matsu -\nmoto Y , Mihalcea R (eds) The 49th Annual Meeting of the Association for Computational Linguistics: \nHuman Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, \nUSA. The Association for Computer Linguistics, pp 142–150, https://aclanthology.org/P11-1015/\nMadaan A, Tandon N, Clark P, et al (2022) Memory-assisted prompt editing to improve gpt-3 after deploy -\nment. Preprint at arXiv:2201.06009\nMeng K, Bau D, Andonian A, et al (2022) Locating and editing factual knowledge in GPT. CoRR \nabs/2202.05262. arXiv:2202.05262\nMerity S, Xiong C, Bradbury J, et al (2016) Pointer sentinel mixture models. arXiv:1609.07843\nMitchell E, Lin C, Bosselut A, et al (2022) Memory-based model editing at scale. In: International Confer -\nence on Machine Learning, PMLR, pp 15817–15831\nNadeem M, Bethke A, Reddy S (2021) StereoSet: Measuring stereotypical bias in pretrained language mod-\nels. In: Zong C, Xia F, Li W, et al (eds) Proceedings of the 59th Annual Meeting of the Association for \nComputational Linguistics and the 11th International Joint Conference on Natural Language Processing \n(V olume 1: Long Papers). Association for Computational Linguistics, Online, pp 5356–5371,  h t t p s  : / / d o \ni  . o r g /  1 0 . 1  8 6 5 3 / v 1 / 2 0 2 1 . a c l - l o n g . 4 1 6     , https://aclanthology.org/2021.acl-long.416\nNangia N, Vania C, Bhalerao R, et al (2020) CrowS-pairs: A challenge dataset for measuring social biases \nin masked language models. In: Webber B, Cohn T, He Y , et al (eds) Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational \nLinguistics, Online, pp 1953–1967, https://doi .org/10.186 53/v1/2020. emnlp-ma in.154,  h t t p s : / / a c l a n t h o \nl o g y . o r g / 2 0 2 0 . e m n l p - m a i n . 1 5 4       \nNasr M, Carlini N, Hayase J, et al (2023) Scalable extraction of training data from (production) language \nmodels. Preprint at arXiv:2311.17035\nNguyen TT, Huynh TT, Nguyen PL, et al (2022) A survey of machine unlearning. Preprint at arXiv:2209.02299\nNi S, Chen D, Li C, et al (2023) Forgetting before learning: Utilizing parametric arithmetic for knowledge \nupdating in large language models. Preprint at arXiv:2311.08011\nNivre J, de Marneffe M, Ginter F, et al (2020) Universal dependencies v2: An evergrowing multilingual \ntreebank collection. In: Calzolari N, Béchet F, Blache P, et al (eds) Proceedings of The 12th Language \nResources and Evaluation Conference, LREC 2020, Marseille, France, May 11-16, 2020. European \nLanguage Resources Association, pp 4034–4043, https://aclanthology.org/2020.lrec-1.497/\nPaperno D, Kruszewski G, Lazaridou A, et al (2016) The LAMBADA dataset: Word prediction requiring \na broad discourse context. In: Erk K, Smith NA (eds) Proceedings of the 54th Annual Meeting of the \nAssociation for Computational Linguistics (V olume 1: Long Papers). Association for Computational \nLinguistics, Berlin, Germany, pp 1525–1534, https://doi.org/10.18653/v1/P16-1144,  h t t p s : / / a c l a n t h o l \no g y . o r g / P 1 6 - 1 1 4 4       \nPapernot N, Song S, Mironov I, et al (2018) Scalable private learning with pate. Preprint at arXiv:1802.08908\nPatil V , Hase P, Bansal M (2023) Can sensitive information be deleted from llms? objectives for defending \nagainst extraction attacks. Preprint at arXiv:2309.17410\nPawelczyk M, Neel S, Lakkaraju H (2023) In-context unlearning: Language models as few shot unlearners. \nPreprint at arXiv:2310.07579\nPochinkov N, Schoots N (2023) Dissecting large language models. In: Socially Responsible Language Mod-\nelling Research\nQu Y , Yuan X, Ding M, et al (2023) Learn to unlearn: A survey on machine unlearning. Preprint at \narXiv:2305.07512\n1 3\n90 Page 38 of 41\nDigital forgetting in large language models: a survey of unlearning…\nRadford A, Narasimhan K, Salimans T, et al (2018) Improving language understanding by generative pre-\ntraining. https://ope nai.com/ind ex/language -unsuper vised/\nRadford A, Wu J, Child R et al (2019) Language models are unsupervised multitask learners. OpenAI blog \n1(8):9\nRaffel C, Shazeer N, Roberts A et al (2020) Exploring the limits of transfer learning with a unified text-to-text \ntransformer. J Mach Learn Res 21(1):5485–5551\nRashkin H, Smith EM, Li M, et al (2019) Towards empathetic open-domain conversation models: A new \nbenchmark and dataset. In: Korhonen A, Traum D, Màrquez L (eds) Proceedings of the 57th Annual \nMeeting of the Association for Computational Linguistics. Association for Computational Linguistics, \nFlorence, Italy, pp 5370–5381, https://doi.org/10.18653/v1/P19-1534,  h t t p s : / / a c l a n t h o l o g y . o r g / P 1 9 - 1 5 \n3 4       \nRavfogel S, Elazar Y , Gonen H, et al (2020) Null it out: Guarding protected attributes by iterative nullspace \nprojection. Preprint at arXiv:2004.07667\nRowling JK (2000) Harry Potter and the Sorcerer’s Stone. Bloomsbury\nRudinger R, Naradowsky J, Leonard B, et al (2018) Gender bias in coreference resolution. In: Walker M, Ji \nH, Stent A (eds) Proceedings of the 2018 Conference of the North American Chapter of the Association \nfor Computational Linguistics: Human Language Technologies, V olume 2 (Short Papers). Association \nfor Computational Linguistics, New Orleans, Louisiana, pp 8–14,  h t t p s : / / d o i . o r g / 1 0 . 1 8 6 5 3 / v 1 / N 1 8 - 2 0 0 \n2     , https://aclanthology.org/N18-2002\nSakaguchi K, Bras RL, Bhagavatula C, et al (2020) Winogrande: An adversarial winograd schema challenge \nat scale. In: The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-\nSecond Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI \nSymposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY , USA, Feb-\nruary 7-12, 2020. AAAI Press, pp 8732–8740, https://doi.org/10.1609/AAAI.V34I05.6399\nSalem A, Zhang Y , Humbert M, et al (2018) Ml-leaks: Model and data independent membership inference \nattacks and defenses on machine learning models. Preprint at arXiv:1806.01246\nSánchez D, Batet M (2016) C-sanitized: a privacy model for document redaction and sanitization. J Assoc \nInf Sci Tech 67(1):148–163\nSanh V , Debut L, Chaumond J, et al (2019) Distilbert, a distilled version of bert: smaller, faster, cheaper and \nlighter. Preprint at arXiv:1910.01108\nScao TL, Fan A, Akiki C, et al (2022) Bloom: A 176b-parameter open-access multilingual language model. \nPreprint at arXiv:2211.05100\nSellam T, Das D, Parikh AP (2020) BLEURT: learning robust metrics for text generation. In: Jurafsky D, \nChai J, Schluter N, et al (eds) Proceedings of the 58th Annual Meeting of the Association for Compu -\ntational Linguistics, ACL 2020, Online, July 5-10, 2020. Association for Computational Linguistics, pp \n7881–7892, https://doi .org/10.186 53/V1/2020. ACL-MAIN .704\nShaik T, Tao X, Xie H, et al (2023) Exploring the landscape of machine unlearning: A survey and taxonomy. \nPreprint at arXiv:2305.06360\nShi W, Ajith A, Xia M, et al (2023) Detecting pretraining data from large language models. Preprint at \narXiv:2310.16789\nShokri R, Stronati M, Song C, et al (2017) Membership inference attacks against machine learning models. \nIn: 2017 IEEE symposium on security and privacy (SP), IEEE, pp 3–18\nSilveira N, Dozat T, de Marneffe MC, et al (2014) A gold standard dependency corpus for English. In: \nCalzolari N, Choukri K, Declerck T, et al (eds) Proceedings of the Ninth International Conference on \nLanguage Resources and Evaluation (LREC’14). European Language Resources Association (ELRA), \nReykjavik, Iceland, pp 2897–2904, http://www. lrec-conf.o rg/proceedi ngs/lrec 2014/pdf/1089_Paper.pdf\nSmith EM, Williamson M, Shuster K, et al (2020) Can you put it all together: Evaluating conversational \nagents’ ability to blend skills. In: Jurafsky D, Chai J, Schluter N, et al (eds) Proceedings of the 58th \nAnnual Meeting of the Association for Computational Linguistics. Association for Computational Lin-\nguistics, Online, pp 2021–2030, https://doi .org/10.186 53/v1/2020. acl-main .183,  h t t p s : / / a c l a n t h o l o g y . o \nr g / 2 0 2 0 . a c l - m a i n . 1 8 3       \nSmith V , Shamsabadi AS, Ashurst C, et al (2023) Identifying and mitigating privacy risks stemming from \nlanguage models: A survey. Preprint at arXiv:2310.01424\nSocher R, Perelygin A, Wu J, et al (2013) Recursive deep models for semantic compositionality over a senti-\nment treebank. In: Yarowsky D, Baldwin T, Korhonen A, et al (eds) Proceedings of the 2013 Conference \non Empirical Methods in Natural Language Processing. Association for Computational Linguistics, \nSeattle, Washington, USA, pp 1631–1642, https://aclanthology.org/D13-1170\nStanovsky G, Smith NA, Zettlemoyer L (2019) Evaluating gender bias in machine translation. In: Korhonen \nA, Traum D, Màrquez L (eds) Proceedings of the 57th Annual Meeting of the Association for Compu-\ntational Linguistics. Association for Computational Linguistics, Florence, Italy, pp 1679–1684,  h t t p s : / / \nd o i . o r g / 1 0 . 1 8 6 5 3 / v 1 / P 1 9 - 1 1 6 4     , https://aclanthology.org/P19-1164\n1 3\nPage 39 of 41 90\nA. Blanco-Justicia et al.\nSundararajan M, Taly A, Yan Q (2017) Axiomatic attribution for deep networks. In: International conference \non machine learning, PMLR, pp 3319–3328\nSá¡nchez D, Batet M, (2017) Toward sensitive document release with privacy guarantees. Engineering Appli-\ncations of Artificial Intelligence 59:23–34\nTaylor R, Kardas M, Cucurull G, et al (2022) Galactica: A large language model for science. Preprint at \narXiv:2211.09085\nTouvron H, Lavril T, Izacard G, et al (2023a) Llama: Open and efficient foundation language models. \narXiv:2302.13971\nTouvron H, Martin L, Stone K, et al (2023b) Llama 2: Open foundation and fine-tuned chat models. efficient \nfoundation language models. arXiv:2307.09288\nTuggener D, von Däniken P, Peetz T, et al (2020) LEDGAR: A large-scale multi-label corpus for text clas -\nsification of legal provisions in contracts. In: Calzolari N, Béchet F, Blache P, et al (eds) Proceedings of \nThe 12th Language Resources and Evaluation Conference, LREC 2020, Marseille, France, May 11-16, \n2020. European Language Resources Association, pp 1235–1241,  h t t p s : / / a c l a n t h o l o g y . o r g / 2 0 2 0 . l r e c - 1 \n. 1 5 5 /       \nTunstall L, V on Werra L, Wolf T (2022) Natural language processing with transformers. \" O’Reilly Media, \nInc.\"\nUnited Nations (1948) Universal Declaration of Human Rights\nÜstün A, Aryabumi V , Yong ZX, et al (2024) Aya model: An instruction finetuned open-access multilingual \nlanguage model. Preprint at arXiv:2402.07827\nVaswani A, Shazeer N, Parmar N, et al (2017) Attention is all you need. Advances in neural information \nprocessing systems 30\nWang L, Chen T, Yuan W, et al (2023) Kga: A general machine unlearning framework based on knowledge \ngap alignment. Preprint at arXiv:2305.06535\nWang L, Zeng X, Guo J, et al (2024) Selective forgetting: Advancing machine unlearning techniques and \nevaluation in language models. Preprint at arXiv:2402.05813\nWebster K, Recasens M, Axelrod V et al (2018) Mind the GAP: a balanced corpus of gendered ambigu -\nous pronouns. Trans Assoc Comput Linguistics 6:605–617 https://doi.org/10.1162/tacl_a_00240, URL  \nhttps://aclanthology.org/Q18-1042\nWu X, Li J, Xu M, et al (2023) Depn: Detecting and editing privacy neurons in pretrained language models. \nPreprint at arXiv:2310.20138\nXu H, Zhu T, Zhang L et al (2023) Machine unlearning: a survey. ACM Comp Surv 56(1):1–36\nYao Y , Xu X, Liu Y (2023) Large language model unlearning. Preprint at arXiv:2310.10683\nYeom S, Giacomelli I, Fredrikson M, et al (2018) Privacy risk in machine learning: Analyzing the connection \nto overfitting. In: 2018 IEEE 31st computer security foundations symposium (CSF), IEEE, pp 268–282\nYoon D, Jang J, Kim S, et al (2023) Gradient ascent post-training enhances language model generalization. \nPreprint at arXiv:2306.07052\nYu C, Jeoung S, Kasi A et al (2023) Unlearning bias in language models by partitioning gradients. Find Assoc \nComput Linguistics: ACL 2023:6032–6048\nZellers R, Holtzman A, Bisk Y , et al (2019) Hellaswag: Can a machine really finish your sentence? CoRR \nabs/1905.07830. arXiv:1905.07830\nZhang J, Liu J, He J, et al (2024a) Composing parameter-efficient modules with arithmetic operation. \nAdvances in Neural Information Processing Systems 36\nZhang N, Yao Y , Tian B, et al (2024b) A comprehensive study of knowledge editing for large language mod-\nels. Preprint at arXiv:2401.01286\nZhang S, Dinan E, Urbanek J, et al (2018) Personalizing dialogue agents: I have a dog, do you have pets too? \nIn: Gurevych I, Miyao Y (eds) Proceedings of the 56th Annual Meeting of the Association for Compu-\ntational Linguistics (V olume 1: Long Papers). Association for Computational Linguistics, Melbourne, \nAustralia, pp 2204–2213https://doi.org/10.18653/v1/P18-1205, https://aclanthology.org/P18-1205\nZhang S, Roller S, Goyal N, et al (2022) Opt: Open pre-trained transformer language models. arXiv:2205.01068\nZhang X, Zhao JJ, LeCun Y (2015) Character-level convolutional networks for text classification. In: Cortes \nC, Lawrence ND, Lee DD, et al (eds) Advances in Neural Information Processing Systems 28: Annual \nConference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, \nCanada, pp 649–657,  h t t p s  : / / p r o  c e e d i  n g s .  n e u r i  p s . c c /  p a p e r  / 2 0 1  5 / h a s h / 2 5 0 c f 8 b 5 1 c 7 7 3 f 3 f 8 d c 8 b 4 b e 8 6 7 a \n9 a 0 2 - A b s t r a c t . h t m l       \nZhao J, Wang T, Yatskar M, et al (2018) Gender bias in coreference resolution: Evaluation and debiasing \nmethods. In: Walker M, Ji H, Stent A (eds) Proceedings of the 2018 Conference of the North American \nChapter of the Association for Computational Linguistics: Human Language Technologies, V olume 2 \n(Short Papers). Association for Computational Linguistics, New Orleans, Louisiana, pp 15–20,  h t t p s : / / \nd o i . o r g / 1 0 . 1 8 6 5 3 / v 1 / N 1 8 - 2 0 0 3     , https://aclanthology.org/N18-2003\n1 3\n90 Page 40 of 41\nDigital forgetting in large language models: a survey of unlearning…\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nAuthors and Affiliations\nAlberto Blanco-Justicia1 · Najeeb Jebreel1 · Benet Manzanares-Salor1 · \nDavid Sánchez1 · Josep Domingo-Ferrer1 · Guillem Collell2 · Kuan Eeik Tan2\n \r David Sánchez\ndavid.sanchez@urv.cat\nAlberto Blanco-Justicia\nalberto.blanco@urv.cat\nNajeeb Jebreel\nnajeeb.jebreel@urv.cat\nBenet Manzanares-Salor\nbenet.manzanares@urv.cat\nJosep Domingo-Ferrer\njosep.domingo@urv.cat\nGuillem Collell\nguillem.collell@huawei.com\nKuan Eeik Tan\nkuan.eeik.tan@huawei.com\n1 Department of Computer Engineering and Mathematics, Universitat Rovira i Virgili, Av. \nPaïsos Catalans 26, Catalonia, E-43007 Tarragona, Spain\n2 Huawei Technologies Finland Research Center, Itämerenkatu 9, FI-00180 Helsinki, Finland    \n1 3\nPage 41 of 41 90",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.749940037727356
    },
    {
      "name": "Forgetting",
      "score": 0.6964627504348755
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4097016155719757
    },
    {
      "name": "Natural language processing",
      "score": 0.34396857023239136
    },
    {
      "name": "Cognitive science",
      "score": 0.32448139786720276
    },
    {
      "name": "Cognitive psychology",
      "score": 0.21461409330368042
    },
    {
      "name": "Psychology",
      "score": 0.1326616406440735
    }
  ]
}