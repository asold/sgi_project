{
  "title": "The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers",
  "url": "https://openalex.org/W3197009789",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2918212282",
      "name": "Róbert Csordás",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2280321817",
      "name": "Kazuki Irie",
      "affiliations": [
        "Dalle Molle Institute for Artificial Intelligence Research",
        "University of Applied Sciences and Arts of Southern Switzerland"
      ]
    },
    {
      "id": "https://openalex.org/A1782944713",
      "name": "Juergen Schmidhuber",
      "affiliations": [
        "University of Applied Sciences and Arts of Southern Switzerland",
        "Dalle Molle Institute for Artificial Intelligence Research"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3113055895",
    "https://openalex.org/W3086307406",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3011798063",
    "https://openalex.org/W4288365402",
    "https://openalex.org/W3098843277",
    "https://openalex.org/W3211555016",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2989932438",
    "https://openalex.org/W4287324987",
    "https://openalex.org/W3173888607",
    "https://openalex.org/W3166702123",
    "https://openalex.org/W3173274550",
    "https://openalex.org/W2963187627",
    "https://openalex.org/W2972433021",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W4241166721",
    "https://openalex.org/W2325237720",
    "https://openalex.org/W4287998573",
    "https://openalex.org/W3035331128",
    "https://openalex.org/W3101133462",
    "https://openalex.org/W4288376504",
    "https://openalex.org/W2963210342",
    "https://openalex.org/W2963267799",
    "https://openalex.org/W4287995219",
    "https://openalex.org/W2118373646",
    "https://openalex.org/W3212647613",
    "https://openalex.org/W3104739822",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2990379018",
    "https://openalex.org/W2939413764",
    "https://openalex.org/W3105725479",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W1509672901",
    "https://openalex.org/W3093157543",
    "https://openalex.org/W3129421149",
    "https://openalex.org/W3121191823",
    "https://openalex.org/W3154669786",
    "https://openalex.org/W4301259831",
    "https://openalex.org/W1828163288",
    "https://openalex.org/W4288094673",
    "https://openalex.org/W2788751659",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2971079754",
    "https://openalex.org/W2173051530",
    "https://openalex.org/W3017374003",
    "https://openalex.org/W2996603747",
    "https://openalex.org/W3043172396",
    "https://openalex.org/W3093630174",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W3130602232",
    "https://openalex.org/W2996132992",
    "https://openalex.org/W2963005248",
    "https://openalex.org/W4253001367",
    "https://openalex.org/W4287755175",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W3166796161",
    "https://openalex.org/W3104534595",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W4287758639",
    "https://openalex.org/W4298392860",
    "https://openalex.org/W4288331674",
    "https://openalex.org/W3013950851",
    "https://openalex.org/W2065077271",
    "https://openalex.org/W4287554891",
    "https://openalex.org/W2956263095",
    "https://openalex.org/W2970290486",
    "https://openalex.org/W3191727383",
    "https://openalex.org/W3035618017",
    "https://openalex.org/W3106531402",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W3166703466",
    "https://openalex.org/W2971199637",
    "https://openalex.org/W2981037730",
    "https://openalex.org/W2996346899",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2530887700"
  ],
  "abstract": "Recently, many datasets have been proposed to test the systematic generalization ability of neural networks. The companion baseline Transformers, typically trained with default hyper-parameters from standard tasks, are shown to fail dramatically. Here we demonstrate that by revisiting model configurations as basic as scaling of embeddings, early stopping, relative positional embedding, and Universal Transformer variants, we can drastically improve the performance of Transformers on systematic generalization. We report improvements on five popular datasets: SCAN, CFQ, PCFG, COGS, and Mathematics dataset. Our models improve accuracy from 50% to 85% on the PCFG productivity split, and from 35% to 81% on COGS. On SCAN, relative positional embedding largely mitigates the EOS decision problem (Newman et al., 2020), yielding 100% accuracy on the length split with a cutoff at 26. Importantly, performance differences between these models are typically invisible on the IID data split. This calls for proper generalization validation sets for developing neural networks that generalize systematically. We publicly release the code to reproduce our results.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 619–634\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n619\nThe Devil is in the Detail: Simple Tricks Improve\nSystematic Generalization of Transformers\nRóbert Csordás Kazuki Irie Jürgen Schmidhuber\nThe Swiss AI Lab IDSIA, USI & SUPSI, Lugano, Switzerland\n{robert, kazuki, juergen}@idsia.ch\nAbstract\nRecently, many datasets have been proposed\nto test the systematic generalization ability\nof neural networks. The companion base-\nline Transformers, typically trained with de-\nfault hyper-parameters from standard tasks,\nare shown to fail dramatically. Here we\ndemonstrate that by revisiting model conﬁg-\nurations as basic as scaling of embeddings,\nearly stopping, relative positional embedding,\nand Universal Transformer variants, we can\ndrastically improve the performance of Trans-\nformers on systematic generalization. We re-\nport improvements on ﬁve popular datasets:\nSCAN, CFQ, PCFG, COGS, and Mathemat-\nics dataset. Our models improve accuracy\nfrom 50% to 85% on the PCFG productivity\nsplit, and from 35% to 81% on COGS. On\nSCAN, relative positional embedding largely\nmitigates the EOS decision problem (Newman\net al., 2020), yielding 100% accuracy on the\nlength split with a cutoff at 26. Importantly,\nperformance differences between these mod-\nels are typically invisible on the IID data split.\nThis calls for proper generalization validation\nsets for developing neural networks that gen-\neralize systematically. We publicly release the\ncode to reproduce our results1.\n1 Introduction\nSystematic generalization (Fodor et al., 1988) is a\ndesired property for neural networks to extrapolate\ncompositional rules seen during training beyond\ntraining distribution: for example, performing dif-\nferent combinations of known rules or applying\nthem to longer problems. Despite the progress of ar-\ntiﬁcial neural networks in recent years, the problem\nof systematic generalization still remains unsolved\n(Fodor and McLaughlin, 1990; Lake and Baroni,\n2018; Liska et al., 2018; Greff et al., 2020; Hupkes\net al., 2020). While there has been much progress\n1https://github.com/robertcsordas/\ntransformer_generalization\nin the past years (Bahdanau et al., 2019; Korrel\net al., 2019; Lake, 2019; Li et al., 2019; Russin\net al., 2019), in particular on the popular SCAN\ndataset (Lake and Baroni, 2018) where some meth-\nods even achieve 100% accuracy by introducing\nsome non-trivial symbolic components into the sys-\ntem (Chen et al., 2020; Liu et al., 2020), the ﬂex-\nibility of such solutions is questionable. In fact,\nthe existing SCAN-inspired solutions have limited\nperformance gains on other datasets (Furrer et al.,\n2020; Shaw et al., 2020). It is thus not enough\nto solely focus on the SCAN dataset to progress\nresearch on systematic generalization.\nRecently, many datasets have been proposed for\ntesting systematic generalization, including PCFG\n(Hupkes et al., 2020) and COGS (Kim and Linzen,\n2020). The baseline Transformer models which\nare released together with the dataset are typically\nshown to dramatically fail at the task. However, the\nconﬁgurations of these baseline models are ques-\ntionable. In most cases, some standard practices\nfrom machine translation are applied without mod-\niﬁcation. Also, some existing techniques such as\nrelative positional embedding (Shaw et al., 2018;\nDai et al., 2019), which are relevant for the prob-\nlem, are not part of the baseline.\nIn order to develop and evaluate methods to im-\nprove systematic generalization, it is necessary to\nhave not only good datasets but also strong base-\nlines to correctly evaluate the limits of existing ar-\nchitectures and to avoid false sense of progress over\nbad baselines. In this work, we demonstrate that the\ncapability of Transformers (Vaswani et al., 2017)\nand in particular its universal variants (Dehghani\net al., 2019) on these tasks are largely underesti-\nmated. We show that careful designs of model and\ntraining conﬁgurations are particularly important\nfor these reasoning tasks testing systematic gen-\neralization. By revisiting conﬁgurations such as\nbasic scaling of word and positional embeddings,\nearly stopping strategy, and relative positional em-\n620\nbedding, we dramatically improve the performance\nof the baseline Transformers. We conduct experi-\nments on ﬁve datasets: SCAN (Lake and Baroni,\n2018), CFQ (Keysers et al., 2020), PCFG (Hupkes\net al., 2020), COGS (Kim and Linzen, 2020), and\nMathematic dataset (Saxton et al., 2019). In partic-\nular, our new models improve the accuracy on the\nPCFG productivity split from 50% to 85%, on the\nsystematicity split from 72% to 96%, and on COGS\nfrom 35% to 81% over the existing baselines. On\nthe SCAN dataset, we show that our models with\nrelative positional embedding largely mitigates the\nso-called end-of-sentence (EOS) decision problem\n(Newman et al., 2020), achieving 100% accuracy\non the length split with a cutoff at 26.\nAlso importantly, we show that despite these dra-\nmatic performance gaps, all these models perform\nequally well on IID validation datasets. The con-\nsequence of this observation is the need for proper\ngeneralization validation sets for developing neural\nnetworks for systematic generalization.\nWe thoroughly discuss guidelines that empir-\nically yield good performance across various\ndatasets, and we will publicly release the code to\nmake our results reproducible.\n2 Datasets and Model Architectures for\nSystematic Generalization\nHere we describe the ﬁve datasets, and specify\nthe Transformer model variants we use in our\nexperiments. The selected datasets include both\nalready popular ones and recently proposed ones.\nStatistics of the datasets can be found in Table 10\nin the appendix.\n2.1 Datasets\nMany datasets in the language domain have been\nproposed to test systematic generalization. All\ndatasets we consider here can be formulated as\na sequence-to-sequence mapping task (Sutskever\net al., 2014; Graves, 2012). Common to all these\ndatasets, the test set is sampled from a distribution\nwhich is systematically different from the one for\ntraining: for example, the test set might systemati-\ncally contain longer sequences, new combinations\nor deeper compositions of known rules. We call this\nsplit the generalization split. Most of the datasets\nalso come with a conventional split, where the train\nand test (and validation, if available) sets are inde-\npendently and identically distributed samples. We\ncall this the IID split. In this paper, we consider the\nfollowing ﬁve datasets:\nSCAN (Lake and Baroni, 2018). The task con-\nsists of mapping a sentence in natural language into\na sequence of commands simulating navigation in\na grid world. The commands are compositional:\ne.g. an input jump twice should be translated\nto JUMP JUMP. It comes with multiple data splits:\nin addition to the “simple” IID split, in the “length”\nsplit, the training sequences are shorter than test\nones, and in the “add primitive” splits, some com-\nmands are presented in the training set only in iso-\nlation, without being composed with others. The\ntest set focuses on these excluded combinations.\nCFQ (Keysers et al., 2020). The task consists\nof translating a natural language question to a\nFreebase SPARQL query. For example Was\nM0 a director and producer of M1\nshould be translated to SELECT count(*)\nWHERE {M0 ns:film.director.film\nM1 . M0 ns:film.producer.film |\nns:film.production_company.films\nM1}. The authors introduce splits based on\n“compound divergence” which measures the\ndifference between the parse trees in the different\ndata splits. The authors experimentally show that\nit is well correlated with generalization difﬁculty.\nIt also comes with a length-based split.\nPCFG (Hupkes et al., 2020). The task consists\nof list manipulations and operations that should\nbe executed. For example, reverse copy\nO14 O4 C12 J14 W3 should be translated to\nW3 J14 C12 O4 O14. It comes with different\nsplits for testing different aspects of generaliza-\ntion. In this work, we focus on the “productivity”\nsplit, which focuses on generalization to longer se-\nquences, and on the “systematicity” split, which is\nabout recombining constituents in novel ways.\nCOGS (Kim and Linzen, 2020). The task\nconsists of semantic parsing which maps an\nEnglish sentence to a logical form. For example,\nThe puppy slept. should be translated to\n* puppy ( x _ 1 ) ; sleep . agent\n( x _ 2, x _ 1 ). It comes with a single\nsplit, with a training, IID validation and OOD\ngeneralization testing set.\nMathematics Dataset (Saxton et al., 2019).\nThe task consists of high school level textual\nmath questions, e.g. What is -5 - 110911?\nshould be translated to -110916. The data is\n621\nsplit into different subsets by the problem category,\ncalled modules. Some of them come with an ex-\ntrapolation set, designed to measure generalization.\nThe amount of total data is very large and thus ex-\npensive to train on, but different modules can be\nstudied individually. We focus on “add_or_sub\"\nand “place_value\" modules.\n2.2 Model Architectures\nWe focus our analysis on two Transformer archi-\ntectures: standard Transformers (Vaswani et al.,\n2017) and Universal Transformers (Dehghani et al.,\n2019), and in both cases with absolute or relative\npositional embedding (Dai et al., 2019). Our Uni-\nversal Transformer variants are simply Transform-\ners with shared weights between layers, without\nadaptive computation time (Schmidhuber, 2012;\nGraves, 2016) and timestep embedding. Positional\nembedding are only added to the ﬁrst layer.\nUniversal Transformers are particularly relevant\nfor reasoning and algorithmic tasks. For example,\nif we assume a task which consists in executing\na sequence of operations, a regular Transformer\nwill learn successive operations in successive lay-\ners with separate weights. In consequence, if only\nsome particular orderings of the operations are seen\nduring training, each layer will only learn a subset\nof the operations, and thus, it will be impossible\nfor them to recombine operations in an arbitrary\norder. Moreover, if the same operation has to be\nreused multiple times, the network has to re-learn\nit, which is harmful for systematic generalization\nand reduces the data efﬁciency of the model (Csor-\ndás et al., 2021). Universal Transformers have the\npotential to overcome this limitation: sharing the\nweights between each layer makes it possible to\nreuse the existing knowledge from different com-\npositions. On the downside, the Universal Trans-\nformer’s capacity can be limited because of the\nweight sharing.\n3 Improving Transformers on Systematic\nGeneralization\nIn this section, we present methods which greatly\nimprove Transformers on systematic generalization\ntasks, while they could be considered as details in\nstandard tasks. For each method, we provide exper-\nimental evidences on a few representative datasets.\nIn Section 4, we apply these ﬁndings to all datasets.\n3.1 Addressing the EOS Decision Problem\nwith Relative Positional Embedding\nThe EOS decision problem. A thorough analy-\nsis by Newman et al. (2020) highlights that LSTMs\nand Transformers struggle to generalize to longer\noutput lengths than they are trained for. Speciﬁ-\ncally, it is shown that the decision when to end the\nsequence (the EOS decision) often overﬁts to the\nspeciﬁc positions observed in the train set. To mea-\nsure whether the models are otherwise able to solve\nthe task, they conduct a so-calledoracle evaluation:\nthey ignore the EOS token during evaluation, and\nuse the ground-truth sequence length to stop decod-\ning. The performance with this evaluation mode\nis much better, which illustrates that the problem\nis indeed the EOS decision. More surprisingly, if\nthe model is trained without EOS token as part of\noutput vocabulary (thus it can only be evaluated in\noracle mode), the performance is further improved.\nIt is concluded that teaching the model when to\nend the sequence has undesirable side effects on\nthe model’s length generalization ability.\nWe show that the main cause of this EOS de-\ncision problem in the case of Transformers is the\nabsolute positional embedding. Generally speak-\ning, the meaning of a word is rarely dependent\non the word’s absolute position in a document but\ndepends on its neighbors. Motivated by this as-\nsumption, various relative positional embedding\nmethods (Shaw et al., 2018; Dai et al., 2019) have\nbeen proposed. Unfortunately, they have not been\nconsidered for systematic generalization in prior\nwork (however, see Sec. 5), even though they are\nparticularly relevant for that.\nWe test Transformers with relative positional em-\nbedding in the form used in Transformer XL (Dai\net al., 2019). Since it is designed for auto-regressive\nmodels, we directly apply it in the decoder of our\nmodel, while for the encoder, we use a symmetri-\ncal variant of it (see Appendix C). The interface\nbetween encoder and decoder uses the standard\nattention without any positional embedding.\nOur experimental setting is similar to Newman\net al. (2020). The length split in SCAN dataset\nrestricts the length of the train samples to 22 tokens\n(the test set consists of samples with an output of\nmore than 22 tokens). This removes some compo-\nsitions from the train set entirely, which introduces\nadditional difﬁculty to the task. 80% of the test\nset consists of these missing compositions. In or-\nder to mitigate the issue of unknown composition\n622\nTable 1: Exact match accuracies on length splits with different cutoffs. Reported results are the median of 5\nruns. Trafo denotes Transformers. The numbers in the rows +EOS+Oracle and -EOS+Oracle are taken from\nNewman et al. (2020) as reference numbers but they can not be compared to others as they are evaluated with\noracle length. Our models use different hyperparameters compared to theirs. We refer to Section 3.1 for details.\nℓ (length cutoff) 22 24 25 26 27 28 30 32 33 36 40\nReference\n+EOS 0.00 0.05 0.04 0.00 0.09 0.00 0.09 0.35 0.00 0.00 0.00\n+EOS+Oracle 0.53 0.51 0.69 0.76 0.74 0.57 0.78 0.66 0.77 1.00 0.97\n-EOS+Oracle 0.58 0.54 0.67 0.82 0.88 0.85 0.89 0.82 1.00 1.00 1.00\nOurs (+EOS)\nTrafo 0.00 0.04 0.19 0.29 0.30 0.08 0.24 0.36 0.00 0.00 0.00\n+ Relative PE 0.20 0.12 0.31 0.61 1.00 1.00 1.00 0.94 1.00 1.00 1.00\nUniversal Trafo 0.02 0.05 0.14 0.21 0.26 0.00 0.06 0.35 0.00 0.00 0.00\n+ Relative PE 0.20 0.12 0.71 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\nand focus purely on the length problem, Newman\net al. (2020) re-split SCAN by introducing different\nlength cutoffs and report the performance of each\nsplit. We test our models similarly. However, our\npreliminary experiments showed the performance\nof the original model is additionally limited by be-\ning too shallow: it uses only 2 layers for both the\nencoder and decoder. We increased the number\nof layers to 3. To compensate for the increased\nnumber of parameters, we decrease the size of the\nfeed-forward layers from 1024 to 256. In total, this\nreduces the number of parameters by 30%. We\ntrain our models with Adam optimizer, a learning\nrate of 10−4, batch size of 128 for 50k steps.\nThe results are shown in Table 1. In order to\nshow that our changes of hyperparameters are not\nthe main reason for the improved performance, we\nreport the performance of our modiﬁed model with-\nout relative positional embedding (row Trafo). We\nalso include the results from Newman et al. (2020)\nfor reference. We report the performance of Uni-\nversal Transformer models trained with identical\nhyperparameters. All our models are trained to\npredict the EOS token and are evaluated without\noracle (+EOS conﬁguration). It can be seen that\nboth our standard and Universal Transformers with\nabsolute positional embedding have near-zero ac-\ncuracy for all length cutoffs, whereas models with\nrelative positional embedding excel: they even out-\nperform the models trained without EOS prediction\nand evaluated with the ground-truth length.\nAlthough Table 1 highlights the advantages of\nrelative positional embedding and shows that they\ncan largely mitigate the EOS-overﬁtting issue, this\ndoes not mean that the problem of generalizing to\nlonger sequences is fully solved. The sub-optimal\nperformance on short length cutoffs (22-25) indi-\ncates that the model ﬁnds it hard to zero-shot gen-\neralize to unseen compositions of speciﬁc rules. To\nimprove these results further, research on models\nwhich assume analogies between rules and compo-\nsitions are necessary, such that they can recombine\nknown constituents without any training example.\nFurther beneﬁts of relative positional embed-\nding. In addition to the beneﬁt highlighted in\nthe previous paragraph, we found that models with\nrelative positional embedding are easier to train\nin general. They converge faster (Figure 6 in the\nappendix) and are less sensitive to batch size (Table\n9 in the appendix). As another empirical ﬁnding,\nwe note that relative Transformers without shared\nlayers sometimes catastrophically fail before reach-\ning their ﬁnal accuracy: the accuracy drops to 0,\nand it never recovers. We observed this with PCFG\nproductivity split and the “Math: place_value” task.\nReducing the number of parameters (either using\nUniversal Transformers or reducing the state size)\nusually stabilizes the network.\n3.2 Model Selection Should Be Done\nCarefully\nThe danger of early stopping. Another crucial\naspect greatly inﬂuencing the generalization per-\nformance of Transformers is model selection, in\nparticular early stopping. In fact, on these datasets,\nit is a common practice to use only the IID split to\ntune hyperparameters or select models with early\nstopping (e.g. Kim and Linzen (2020)). However,\nsince any reasonable models achieve nearly 100%\naccuracy on the IID validation set, there is no good\nreason to believe this to be a good practice for se-\nlecting models for generalization splits. To test\nthis hypothesis, we train models on COGS dataset\nwithout early stopping, but with a ﬁxed number of\n50k training steps. The best model achieved a test\naccuracy of 81%, while the original performance\n623\n0 10k 20k 30k 40k 50k\nTraining steps\n0\n25\n50\n75Gen. accuracy [%]\nNo scaling\nToken Emb. Up., Noam\nPosition Emb. Down.\nFigure 1: Generalization accuracy on COGS as a func-\ntion of training steps for standard Transformers with\ndifferent embedding scaling schemes. The vertical\nlines show the median of the early stopping points for\nthe ﬁve runs. Early stopping parameters are from Kim\nand Linzen (2020). “Token Emb. Up., Noam” corre-\nsponds to the baseline conﬁguration (Kim and Linzen,\n2020). See Sec. 3.3 for details on scaling.\nTable 2: Final IID validation and generalizations accu-\nracy for COGS (50k steps) and PCFG Productivity set\n(300k steps) with different scaling (Section 3.3). To-\nken Embedding Upscaling (TEU) is unstable on PCFG\nwith our hyperparameters. Position Embedding Down-\nscaling (PED) performs the best on both datasets.\nIID Validation Gen. Test\nCOGS\nTEU 1.00 ±0.00 0.78 ±0.03\nNo scaling 1.00 ±0.00 0.62 ±0.06\nPED 1.00 ±0.00 0.80 ±0.00\nPCFG\nTEU 0.92 ±0.07 0.47 ±0.27\nNo scaling 0.97 ±0.01 0.63 ±0.02\nPED 0.96 ±0.01 0.65 ±0.03\nby Kim and Linzen (2020) is 35%. Motivated by\nthis huge performance gap, we had no other choice\nbut to conduct an analysis on the generalization\nsplit to demonstrate the danger of early stopping\nand discrepancies between the performance on the\nIID and generalization split. The corresponding\nresults are shown in Figure 1 (further effect of em-\nbedding scaling is discussed in next Sec. 3.3) and\nTable 2. Following Kim and Linzen (2020), we\nmeasure the model’s performance every 500 steps,\nand mark the point where early stopping with pa-\ntience of 5 would pick the best performing model.\nIt can be seen that in some cases the model chosen\nby early stopping is not even reaching half of the\nﬁnal generalization accuracy.\nTo conﬁrm this observation in the exact setting\nof Kim and Linzen (2020), we also disabled the\nearly stopping in the original codebase 2, and ob-\nserved that the accuracy improved to 65% without\nany other tricks. We discuss further performance\nimprovements on COGS dataset in Section 4.4.\n2https://github.com/najoungkim/COGS\n3.00 4.00 5.00 6.00 7.00\nValidation loss\n10\n20\n30\n40\nTest accuracy [%]\n 1k\n50k\nFigure 2: Relationship between validation loss and test\naccuracy (same distribution) on CFQ MCD 1 split for\na relative Transformer. The color shows the training\nstep. Five runs are shown. The loss has a logarithmic\nscale. High accuracy corresponds to higher loss, which\nis unexpected. For detailed analysis, see Figure 5.\nThe lack of validation set for the generalization\nsplit. A general problem raised in the previous\nparagraph is the lack of validation set for evaluating\nmodels for generalization. Most of the datasets\ncome without a validation set for the generalization\nsplit (SCAN, COGS, and PCFG). Although CFQ\ncomes with such a set, the authors argue that only\nthe IID split should be used for hyperparameter\nsearch, and it is not clear what should be used for\nmodel development.\nIn order to test novel ideas, a way to gradually\nmeasure progress is necessary, such that the effect\nof changes can be evaluated. If the test set is used\nfor developing the model, it implicitly risks overﬁt-\nting to this test set. On the other hand, measuring\nperformance on the IID split does not necessarily\nprovide any valuable information about the gener-\nalization performance on the systematically differ-\nent test set (see Table 2). The IID accuracy of all\nthe considered datasets is 100% (except on PCFG\nwhere it’s also almost100%); thus, no further im-\nprovement, nor potential difference between gener-\nalization performance of models can be measured\n(see also Table 8 in the appendix).\nIt would be beneﬁcial if future datasets would\nhave a validation and test set for both the IID and\nthe generalization split. For the generalization split,\nthe test set could be designed to be more difﬁcult\nthan the validation set. This way, the validation set\ncan be used to measure progress during develop-\nment, but overﬁtting to it would prevent the model\nto generalize well to the test set. Such a division\ncan be easily done on the splits for testing produc-\ntivity. For other types of generalization, we could\nuse multiple datasets sharing the same generaliza-\ntion problem. Some of them could be dedicated for\ndevelopment and others for testing.\n624\n0 50k 100k 150k 200k 250k 300k\nTraining steps\n0\n25\n50\n75Accuracy [%]\nStandard\nUni.\nRel. Uni.\n0 50k 100k 150k 200k 250k 300k\nTraining steps\n2\n4\n6Loss\nStandard Uni. Rel. Uni.\nFigure 3: Test loss and accuracy on PCFG during train-\ning. The loss exhibits an epoch-wise double descent\nphenomenon (Nakkiran et al., 2019), while the accu-\nracy increases monotonically. Standard Transformer\nwith PED (Sec. 3.3), Universal Transformer with ab-\nsolute, and relative positional embeddings are shown.\nIntriguing relationship between generalization\naccuracy and loss. Finally, we also note the\nimportance of using accuracy (instead of loss) as\nthe model selection criterion. We ﬁnd that the\ngeneralization accuracy and loss do not necessarily\ncorrelate, while sometimes, model selection based\non the loss is reported in practice e.g. in Kim\nand Linzen (2020). Examples of this undesirable\nbehavior are shown on Figure 2 for CFQ and\non Figure 4 in the appendix for COGS dataset.\nOn these datasets, the loss and accuracy on the\ngeneralization split both grows during training. We\nconducted an analysis to understand the cause of\nthis surprising phenomenon, we ﬁnd that the total\nloss grows because the loss of the samples with\nincorrect outputs increases more than it improves\non the correct ones. For the corresponding\nexperimental results, we refer to Figure 5 in the\nappendix. We conclude that even if a validation\nset is available for the generalization split, it would\nbe crucial to use the accuracy instead of the loss\nfor early stopping and hyperparameter tuning.\nFinally, on PCFG dataset, we observed epoch-\nwise double descent phenomenon (Nakkiran et al.,\n2019), as shown in Figure 3. This can lead to\nequally problematic results if the loss is used for\nmodel selection or tuning.\n3.3 Large Impacts of Embedding Scaling\nThe last surprising detail which greatly inﬂuences\ngeneralization performance of Transformers is the\nchoice of embedding scaling scheme. This is espe-\ncially important for Transformers with absolute po-\nsitional embedding, where the word and positional\nembedding have to be combined. We experimented\nwith the following scaling schemes:\n1. Token Embedding Upscaling (TEU). This is\nthe standard scaling used by Vaswani et al.\n(2017). It uses Glorot initialization (Glorot\nand Bengio, 2010) for the word embeddings.\nHowever, the range of the sinusoidal posi-\ntional embedding is always in [−1,1]. Since\nthe positional embedding is directly added\nto the word embeddings, this discrepancy can\nmake the model untrainable. Thus, the authors\nupscale the word embeddings by √dmodel\nwhere dmodel is the embedding size. Open-\nNMT3, the framework used for the baseline\nmodels for PCFG and COGS datasets respec-\ntively by Hupkes et al. (2020) and Kim and\nLinzen (2020), also uses this scaling scheme.\n2. No scaling. It initializes the word embedding\nwith N(0,1) (normal distribution with mean\n0 and standard deviation of 1). Positional\nembeddings are added without scaling.\n3. Position Embedding Downscaling (PED),\nwhich uses Kaiming initialization (He et al.,\n2015), and scales the positional embeddings\nby 1√dmodel\n.\nThe PED differs from TEU used in Vaswani\net al. (2017) in two ways: instead of scaling the em-\nbedding up, PED scales the positional embedding\ndown and uses Kaiming instead of Glorot initializa-\ntion. The magnitude of the embeddings should not\ndepend on the number of words in the vocabulary\nbut on the embedding dimension.\nTable 2 shows the results. Although “no scaling”\nvariant is better than TEU on the PCFG test set,\nit is worse on the COGS test set. PED performs\nconsistently the best on both datasets. Importantly,\nthe gap between the best and worst conﬁgurations\nis large on the test sets. The choice of scaling\nthus also contributes in the large improvements we\nreport over the existing baselines.\n4 Results Across Different Datasets\nIn this section, we apply the methods we illustrated\nin the previous section across different datasets. Ta-\nble 3 provides an overview of all improvements we\n3https://opennmt.net/\n625\nobtain on all considered datasets. Unless reported\notherwise, all results are the mean and standard\ndeviation of 5 different random seeds. If multi-\nple embedding scaling schemes are available, we\npick the best performing one for a fair compari-\nson. Transformer variants with relative positional\nembedding outperform the absolute variants on al-\nmost all tested datasets. Except for COGS and\nCFQ MCD 1, the universal variants outperform the\nstandard ones. In the following, we discuss and\nhighlight the improvements we obtained for each\nindividual dataset.\n4.1 SCAN\nWe focused on the length split of the dataset. We\nshow that it is possible to mitigate the effect of\noverﬁtting to the absolute position of the EOS token\nby using relative positional embedding. We already\ndiscussed the details in Sec. 3.1 and Table 1.\n4.2 CFQ\nOn the output length split of CFQ, our Universal\nTransformer with absolute positional embedding\nachieves signiﬁcantly better performance than the\none reported in Keysers et al. (2020): 77% versus\n∼66%4. Here, we were unable to identify the ex-\nact reason for this large improvement. The only\narchitectural difference between the models is that\nours does not make use of any timestep (i.e. layer\nID) embedding. Also, the positional embedding\nis only injected to the ﬁrst layer in case of abso-\nlute positional embeddings (Sec. 2.2). The relative\npositional embedding variant performs even better,\nachieving 81%. This conﬁrms the importance of\nusing relative positional embedding as a default\nchoice for length generalization tasks, as we also\ndemonstrated on SCAN in Sec. 3.1.\nOn the MCD splits, our results slightly outper-\nform the baseline in Keysers et al. (2020), as shown\nin Table 3. Relative Universal Transformers per-\nform marginally better than all other variants, ex-\ncept for MCD 1 split, where the standard Trans-\nformer wins with a slight margin. We use hyper-\nparameters from Keysers et al. (2020). We report\nperformance after 35k training training steps.\n4.3 PCFG\nThe performance of different models on the PCFG\ndataset is shown on Table 3. First of all, simply\nby increasing the number of training epochs from\n4As Keysers et al. (2020) only report charts, the exact value\nis unknown.\n25, used by Hupkes et al. (2020), to ∼237 (300k\nsteps), our model achieves 65% on the productiv-\nity split compared to the 50% reported in Hupkes\net al. (2020) and 87% compared to 72% on the\nsystematicity split. Furthermore, we found that\nUniversal Transformers with relative positional em-\nbeddings further improve performance to a large\nextent, achieving 85% ﬁnal performance on the\nproductivity and 96% on the systematicity split.\nWe experienced instabilities while training Trans-\nformers with relative positional embeddings on the\nproductivity split; thus, the corresponding numbers\nare omitted in Table 3 and Figure 6 in the appendix.\n4.4 COGS\nOn COGS, our best model achieves the generaliza-\ntion accuracy of 81% which greatly outperforms\nthe 35% accuracy reported in Kim and Linzen\n(2020). As we discussed in Sec. 3.2, just by re-\nmoving early stopping in the setting of Kim and\nLinzen (2020), the performance improves to 65%.\nMoreover, the baseline with early stopping is very\nsensitive to the random seed and even sensitive to\nthe GPU type it is run on. Changing the seed in the\nofﬁcial repository from 1 to 2 causes a dramatic\nperformance drop with a 2.5% ﬁnal accuracy. By\nchanging the scaling of embeddings (Sec. 3.3), dis-\nabling label smoothing, ﬁxing the learning rate to\n10−4, we achieved 81% generalization accuracy,\nwhich is stable over multiple random seeds.\nTable 3 compares different model variants. Stan-\ndard Transformers with absolute and relative posi-\ntional encoding perform similarly, with the relative\npositional variant having a slight advantage. Here\nUniversal Transformers perform slightly worse.\n4.5 Mathematics Dataset\nWe also test our approaches on subsets of Mathe-\nmatics Dataset (Saxton et al., 2019). Since train-\ning models on the whole dataset is too resource-\ndemanding, we only conduct experiments on two\nsubsets: “place_value” and “add_or_sub”.\nThe results are shown in Table 3. While we\ncan not directly compare our numbers with those\nreported in Saxton et al. (2019) (a single model\nis jointly trained on the whole dataset there), our\nresults show that relative positional embedding is\nadvantageous for the generalization ability on both\nsubsets.\n626\nTable 3: Test accuracy of different Transformer (Trafo) variants on the considered datasets. See Sec. 4 for de-\ntails. The last column shows previously reported accuracies. References: [1] Newman et al. (2020), [2] Keysers\net al. (2020), [3] https://github.com/google-research/google-research/tree/master/\ncfq, [4] Hupkes et al. (2020), [5] Kim and Linzen (2020), [6] Saxton et al. (2019). Results marked with ∗\ncannot be directly compared because of different training setups. ∼denotes approximative numbers read from\ncharts reported in previous works.\nTrafo Uni. Trafo Rel. Trafo Rel. Uni. Trafo Prior Work\nSCAN (length cutoff=26) 0.30 ±0.02 0.21 ±0.01 0.72 ±0.21 1.00 ±0.00 0.00[1]\nCFQ Output length 0.57 ±0.00 0.77 ±0.02 0.64 ±0.06 0.81 ±0.01 ∼0.66[2]\nCFQ MCD 1 0.40 ±0.01 0.39 ±0.03 0.39 ±0.01 0.39 ±0.04 0.37 ±0.02[3]\nCFQ MCD 2 0.10 ±0.01 0.09 ±0.02 0.09 ±0.01 0.10 ±0.02 0.08 ±0.02[3]\nCFQ MCD 3 0.11 ±0.00 0.11 ±0.01 0.11 ±0.01 0.11 ±0.03 0.11 ±0.00[3]\nCFQ MCD mean 0.20 ±0.14 0.20 ±0.14 0.20 ±0.14 0.20 ±0.14 0.19 ±0.01[2]\nPCFG Productivity split 0.65 ±0.03 0.78 ±0.01 - 0.85 ±0.01 0.50 ±0.02[4]\nPCFG Systematicity split 0.87 ±0.01 0.93 ±0.01 0.89 ±0.02 0.96 ±0.01 0.72 ±0.00[4]\nCOGS 0.80 ±0.00 0.78 ±0.03 0.81 ±0.01 0.77 ±0.01 0.35 ±0.06[5]\nMath: add_or_sub 0.89 ±0.01 0.94 ±0.01 0.91 ±0.03 0.97 ±0.01 ∼0.91[6]∗\nMath: place_value 0.12 ±0.07 0.20 ±0.02 - 0.75 ±0.10 ∼0.69[6]∗\n5 Related Work\nMany recent papers focus on improving general-\nization on the SCAN dataset. Some of them de-\nvelop specialized architectures (Korrel et al., 2019;\nLi et al., 2019; Russin et al., 2019; Gordon et al.,\n2020; Herzig and Berant, 2020) or data augmenta-\ntion methods (Andreas, 2020), others apply meta-\nlearning (Lake, 2019). As an alternative, the CFQ\ndataset proposed in (Keysers et al., 2020) is gaining\nattention recently (Guo et al., 2020; Furrer et al.,\n2020). Mathematical problem solving has also be-\ncome a popular domain for testing generalization\nof neural networks (Kaiser and Sutskever, 2016;\nSchlag et al., 2019; Charton et al., 2021). The\nPCFG (Hupkes et al., 2020) and COGS (Kim and\nLinzen, 2020) are also datasets proposed relatively\nrecently. Despite increasing interests in systematic\ngeneralization tasks, interestingly, no prior work\nhas questioned the baseline conﬁgurations which\ncould be overﬁtted to the machine translation tasks.\nGeneralizing to longer sequences have been\nproven to be especially difﬁcult. Currently only hy-\nbrid task-speciﬁc neuro-symbolic approaches can\nsolve it (Nye et al., 2020; Chen et al., 2020; Liu\net al., 2020). In this work, we focus on a subprob-\nlem required for length generalization: the EOS\ndecision problem (Newman et al., 2020), and we\nshow that it can be mitigated by using relative posi-\ntional embeddings.\nThe study of generalization ability of neural net-\nworks at different stages of training has been a\ngeneral topic of interest (Nakkiran et al., 2019;\nRoelofs, 2019). Our analysis has shown that this\nquestion is particularly relevant to the problem of\nsystematic generalization, as demonstrated by large\nperformance gaps in our experiments, which has\nnot been discussed in prior work.\nPrior work proposed several sophisticated initial-\nization methods for Transformers (Zhang et al.,\n2019; Zhu et al., 2021), e.g. with a purpose\nof removing the layer normalization components\n(Huang et al., 2020). While our work only revisited\nbasic scaling methods, we demonstrated their par-\nticular importance for systematic generalization.\nIn recent work,5 Ontañón et al. (2021) have also\nfocused on improving the compositional general-\nization abilities of Transformers. In addition to\nrelative positional encodings and Universal Trans-\nformers, novel architectural changes such as \"copy\ndecoder\" as well as dataset-speciﬁc \"intermediate\nrepresentations\" (Herzig et al., 2021) have been\nstudied. However, other aspects we found crucial,\nsuch as early stopping, scaling of the positional\nembeddings, and the validation set issues have\nnot been considered. In consequence, our mod-\nels achieve substantially higher performance than\nthe best results reported by Ontañón et al. (2021)\nacross all standard datasets: PCFG, COGS, and\nCFQ (without intermediate representations).\nFinally, our study focused on the basic Trans-\n5Our work was submitted to EMNLP 2021 on May 17,\n2021 and has been under the anonymity period until Aug. 25.\nOntañón et al. (2021) appeared on arXiv on Aug. 9, 2021.\n627\nformer architectures. However, the details dis-\ncussed above in the context of algorithmic tasks\nshould also be relevant for other Transformer vari-\nants and fast weight programmers (Schmidhuber,\n1992; Schlag et al., 2021; Irie et al., 2021), as well\nas other architectures speciﬁcally designed for al-\ngorithmic reasoning (Graves et al., 2016; Kaiser\nand Sutskever, 2016; Csordás and Schmidhuber,\n2019; Freivalds et al., 2019).\n6 Conclusion\nIn this work we showed that the performance of\nTransformer architectures on many recently pro-\nposed datasets for systematic generalization can\nbe greatly improved by revisiting basic model and\ntraining conﬁgurations. Model variants with rel-\native positional embedding often outperform the\nones with absolute positional embedding. They\nalso mitigate the EOS decision problem, an impor-\ntant problem previously found by Newman et al.\n(2020) when considering the length generalization\nof neural networks. This allows us to focus on the\nproblem of compositions in the future, which is the\nremaining problem for the length generalization.\nWe also demonstrated that reconsidering early\nstopping and embedding scaling can greatly im-\nprove baseline Transformers, in particular on the\nCOGS and PCFG datasets. These results shed light\non the discrepancy between the model performance\non the IID validation set and the test accuracy on\nthe systematically different generalization split. As\nconsequence, currently common practice of vali-\ndating models on the IID dataset is problematic.\nWe conclude that the community should discuss\nproper ways to develop models for systematic gen-\neralization. In particular, we hope that our work\nclearly demonstrated the necessity of a validation\nset for systematic generalization in order to estab-\nlish strong baselines and to avoid a false sense of\nprogress.\nAcknowledgments\nWe thank Aleksandar Stani ´c and Imanol Schlag\nfor their helpful comments and suggestions on an\nearlier version of the manuscript. This research\nwas partially funded by ERC Advanced grant no:\n742870, project AlgoRNN, and by Swiss National\nScience Foundation grant no: 200021_192356,\nproject NEUSYM. We thank hardware donations\nfrom NVIDIA & IBM.\nReferences\nJacob Andreas. 2020. Good-enough compositional\ndata augmentation. In Proc. Association for Com-\nputational Linguistics (ACL), pages 7556–7566, Vir-\ntual only.\nDzmitry Bahdanau, Harm de Vries, Timothy J\nO’Donnell, Shikhar Murty, Philippe Beaudoin,\nYoshua Bengio, and Aaron Courville. 2019. CLO-\nSURE: Assessing systematic generalization of\nCLEVR models. In ViGIL workshop, NeurIPS, Van-\ncouver, Canada.\nFrancois Charton, Amaury Hayat, and Guillaume Lam-\nple. 2021. Learning advanced mathematical compu-\ntations from examples. In Int. Conf. on Learning\nRepresentations (ICLR), Virtual only.\nXinyun Chen, Chen Liang, Adams Wei Yu, Dawn\nSong, and Denny Zhou. 2020. Compositional gen-\neralization via neural-symbolic stack machines. In\nProc. Advances in Neural Information Processing\nSystems (NeurIPS), Virtual only.\nRóbert Csordás and Jürgen Schmidhuber. 2019. Im-\nproving differentiable neural computers through\nmemory masking, de-allocation, and link distribu-\ntion sharpness control. In Int. Conf. on Learning\nRepresentations (ICLR), New Orleans, LA, USA.\nRóbert Csordás, Sjoerd van Steenkiste, and Jürgen\nSchmidhuber. 2021. Are neural nets modular?\ninspecting functional modularity through differen-\ntiable weight masks. In Int. Conf. on Learning Rep-\nresentations (ICLR), Virtual only.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-xl: Attentive language models beyond\na ﬁxed-length context. In Proc. Association for\nComputational Linguistics (ACL), pages 2978–2988,\nFlorence, Italy.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Lukasz Kaiser. 2019. Univer-\nsal transformers. In Int. Conf. on Learning Repre-\nsentations (ICLR), New Orleans, LA, USA.\nJerry Fodor and Brian P McLaughlin. 1990. Con-\nnectionism and the problem of systematicity: Why\nsmolensky’s solution doesn’t work. Cognition,\n35(2):183–204.\nJerry A Fodor, Zenon W Pylyshyn, et al. 1988. Connec-\ntionism and cognitive architecture: A critical analy-\nsis. Cognition, 28(1-2):3–71.\nKarlis Freivalds, Emils Ozolins, and Agris Sostaks.\n2019. Neural shufﬂe-exchange networks - sequence\nprocessing in o(n log n) time. In Proc. Advances in\nNeural Information Processing Systems (NeurIPS) ,\nVancouver, Canada.\n628\nDaniel Furrer, Marc van Zee, Nathan Scales, and\nNathanael Schärli. 2020. Compositional generaliza-\ntion in semantic parsing: Pre-training vs. specialized\narchitectures. Preprint arXiv:2007.08970.\nXavier Glorot and Yoshua Bengio. 2010. Understand-\ning the difﬁculty of training deep feedforward neu-\nral networks. In Proc. Int. Conf. on Artiﬁcial Intelli-\ngence and Statistics (AISTATS), pages 249–256, Sar-\ndinia, Italy.\nJonathan Gordon, David Lopez-Paz, Marco Baroni,\nand Diane Bouchacourt. 2020. Permutation equiv-\nariant models for compositional generalization in\nlanguage. In Int. Conf. on Learning Representations\n(ICLR), Addis Ababa, Ethiopia.\nAlex Graves. 2012. Sequence transduction with recur-\nrent neural networks. In Workshop on Representa-\ntion Learning, ICML, Edinburgh, Scotland.\nAlex Graves. 2016. Adaptive computation time for re-\ncurrent neural networks. In Int. Conf. on Learning\nRepresentations (ICLR) Workshop Track, Vancouver,\nCanada.\nAlex Graves, Greg Wayne, Malcolm Reynolds,\nTim Harley, Ivo Danihelka, Agnieszka Grabska-\nBarwinska, Sergio Gomez Colmenarejo, Edward\nGrefenstette, Tiago Ramalho, John P. Agapiou,\nAdrià Puigdomènech Badia, Karl Moritz Hermann,\nYori Zwols, Georg Ostrovski, Adam Cain, Helen\nKing, Christopher Summerﬁeld, Phil Blunsom, Ko-\nray Kavukcuoglu, and Demis Hassabis. 2016. Hy-\nbrid computing using a neural network with dy-\nnamic external memory. Nature, 538(7626):471–\n476.\nKlaus Greff, Sjoerd van Steenkiste, and Jürgen Schmid-\nhuber. 2020. On the binding problem in artiﬁcial\nneural networks. Preprint arXiv:2012.05208.\nYinuo Guo, Zeqi Lin, Jian-Guang Lou, and Dongmei\nZhang. 2020. Hierarchical poset decoding for com-\npositional generalization in language. In Proc. Ad-\nvances in Neural Information Processing Systems\n(NeurIPS), Virtual only.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2015. Delving deep into rectiﬁers: Surpassing\nhuman-level performance on imagenet classiﬁcation.\nIn Proc. IEEE Int. Conf. on Computer Vision (ICCV),\npages 1026–1034, Santiago, Chile.\nJonathan Herzig and Jonathan Berant. 2020. Span-\nbased semantic parsing for compositional general-\nization. Preprint arXiv:2009.06040.\nJonathan Herzig, Peter Shaw, Ming-Wei Chang, Kelvin\nGuu, Panupong Pasupat, and Yuan Zhang. 2021. Un-\nlocking compositional generalization in pre-trained\nmodels using intermediate representations. Preprint\narXiv:2104.07478.\nXiao Shi Huang, Felipe Perez, Jimmy Ba, and Mak-\nsims V olkovs. 2020. Improving transformer opti-\nmization through better initialization. In Proc. Int.\nConf. on Machine Learning (ICML) , pages 4475–\n4483, Virtual only.\nDieuwke Hupkes, Verna Dankers, Mathijs Mul, and\nElia Bruni. 2020. Compositionality decomposed:\nHow do neural networks generalise? Journal of Ar-\ntiﬁcial Intelligence Research, pages 757–795.\nKazuki Irie, Imanol Schlag, Róbert Csordás, and Jür-\ngen Schmidhuber. 2021. Going beyond linear trans-\nformers with recurrent fast weight programmers.\nPreprint arXiv:2106.06295.\nLukasz Kaiser and Ilya Sutskever. 2016. Neural GPUs\nlearn algorithms. In Int. Conf. on Learning Repre-\nsentations (ICLR), San Juan, Puerto Rico.\nDaniel Keysers, Nathanael Schärli, Nathan Scales,\nHylke Buisman, Daniel Furrer, Sergii Kashubin,\nNikola Momchev, Danila Sinopalnikov, Lukasz\nStaﬁniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang,\nMarc van Zee, and Olivier Bousquet. 2020. Measur-\ning compositional generalization: A comprehensive\nmethod on realistic data. In Int. Conf. on Learning\nRepresentations (ICLR), Addis Ababa, Ethiopia.\nNajoung Kim and Tal Linzen. 2020. COGS: A compo-\nsitional generalization challenge based on semantic\ninterpretation. In Proc. Conf. on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n9087–9105, Virtual only.\nKris Korrel, Dieuwke Hupkes, Verna Dankers, and Elia\nBruni. 2019. Transcoding compositionally: Using\nattention to ﬁnd more generalizable solutions. In\nProc. BlackboxNLP Workshop on Analyzing and In-\nterpreting Neural Networks for NLP , ACL, pages 1–\n11, Florence, Italy.\nBrenden M Lake. 2019. Compositional generalization\nthrough meta sequence-to-sequence learning. In\nProc. Advances in Neural Information Processing\nSystems (NeurIPS) , pages 9788–9798, Vancouver,\nCanada.\nBrenden M. Lake and Marco Baroni. 2018. General-\nization without systematicity: On the compositional\nskills of sequence-to-sequence recurrent networks.\nIn Proc. Int. Conf. on Machine Learning (ICML) ,\npages 2873–2882, Stockholm, Sweden.\nYuanpeng Li, Liang Zhao, Jianyu Wang, and Joel Hest-\nness. 2019. Compositional generalization for primi-\ntive substitutions. In Proc. Conf. on Empirical Meth-\nods in Natural Language Processing and Int.Joint\nConf. on Natural Language Processing (EMNLP-\nIJCNLP), pages 4292–4301, Hong Kong, China.\nAdam Liska, Germán Kruszewski, and Marco Baroni.\n2018. Memorize or generalize? searching for a com-\npositional RNN in a haystack. In AEGAP Workshop\nICML, Stockholm, Sweden.\n629\nQian Liu, Shengnan An, Jian-Guang Lou, Bei Chen,\nZeqi Lin, Yan Gao, Bin Zhou, Nanning Zheng, and\nDongmei Zhang. 2020. Compositional generaliza-\ntion by learning analytical expressions. In Proc. Ad-\nvances in Neural Information Processing Systems\n(NeurIPS), Virtual only.\nPreetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan\nYang, Boaz Barak, and Ilya Sutskever. 2019. Deep\ndouble descent: Where bigger models and more data\nhurt. In Int. Conf. on Learning Representations\n(ICLR), Addis Ababa, Ethiopia.\nBenjamin Newman, John Hewitt, Percy Liang, and\nChristopher D Manning. 2020. The eos decision\nand length extrapolation. In Proc. BlackboxNLP\nWorkshop on Analyzing and Interpreting Neural Net-\nworks for NLP , EMNLP , pages 276–291, Virtual\nonly.\nMaxwell I Nye, Armando Solar-Lezama, Joshua B\nTenenbaum, and Brenden M Lake. 2020. Learn-\ning compositional rules via neural program synthe-\nsis. Preprint arXiv:2003.05562.\nSantiago Ontañón, Joshua Ainslie, Vaclav Cvicek, and\nZachary Fisher. 2021. Making transformers solve\ncompositional tasks. Preprint arXiv:2108.04378.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learn-\ning library. In Proc. Advances in Neural Infor-\nmation Processing Systems (NeurIPS), pages 8024–\n8035, Vancouver, Canada.\nRebecca Roelofs. 2019. Measuring Generalization\nand overﬁtting in Machine learning . Ph.D. thesis,\nUC Berkeley.\nJake Russin, Jason Jo, Randall C O’Reilly, and Yoshua\nBengio. 2019. Compositional generalization in a\ndeep seq2seq model by separating syntax and seman-\ntics. Preprint arXiv:1904.09708.\nDavid Saxton, Edward Grefenstette, Felix Hill, and\nPushmeet Kohli. 2019. Analysing mathematical rea-\nsoning abilities of neural models. In Int. Conf. on\nLearning Representations (ICLR), New Orleans, LA,\nUSA.\nImanol Schlag, Kazuki Irie, and Jürgen Schmidhuber.\n2021. Linear transformers are secretly fast weight\nprogrammers. In Proc. Int. Conf. on Machine Learn-\ning (ICML), volume 139, pages 9355–9366, Virtual\nonly.\nImanol Schlag, Paul Smolensky, Roland Fernandez,\nNebojsa Jojic, Jürgen Schmidhuber, and Jianfeng\nGao. 2019. Enhancing the transformer with ex-\nplicit relational encoding for math problem solving.\nPreprint arXiv:1910.06611.\nJürgen Schmidhuber. 1992. Learning to control fast-\nweight memories: An alternative to recurrent nets.\nNeural Computation, 4(1):131–139.\nJürgen Schmidhuber. 2012. Self-delimiting neural net-\nworks. Preprint arXiv:1210.0118.\nPeter Shaw, Ming-Wei Chang, Panupong Pasupat, and\nKristina Toutanova. 2020. Compositional general-\nization and natural language variation: Can a se-\nmantic parsing approach handle both? Preprint\narXiv:2010.12725.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proc. North American Chapter of the As-\nsociation for Computational Linguistics on Human\nLanguage Technologies (NAACL-HLT), pages 464–\n468, New Orleans, Louisiana, USA.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Proc. Advances in Neural Information Process-\ning Systems (NIPS) , pages 3104–3112, Montréal,\nCanada.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proc. Advances in Neural Information\nProcessing Systems (NIPS), pages 5998–6008, Long\nBeach, CA, USA.\nBiao Zhang, Ivan Titov, and Rico Sennrich. 2019. Im-\nproving deep transformer with depth-scaled initial-\nization and merged attention. In Proc. Conf. on\nEmpirical Methods in Natural Language Processing\nand Int.Joint Conf. on Natural Language Process-\ning (EMNLP-IJCNLP), pages 898–909, Hong Kong,\nChina.\nChen Zhu, Renkun Ni, Zheng Xu, Kezhi Kong,\nW Ronny Huang, and Tom Goldstein. 2021.\nGradinit: Learning to initialize neural networks\nfor stable and efﬁcient training. Preprint\narXiv:2102.08098.\n630\nA Evaluation Metrics\nFor all tasks, accuracy is computed on the sequence-\nlevel, i.e. all tokens in the sequence should be cor-\nrect for the output to be counted as correct. For the\nlosses, we always report the average token-wise\ncross entropy loss.\nB Hyperparameters\nFor all of our models we use an Adam opti-\nmizer with the default hyperparameters of PyTorch\n(Paszke et al., 2019). We only change the learn-\ning rate. We use dropout with probability of 0.1\nafter each component of the transformer: both af-\nter the attention heads and linear transformations.\nWe specify the dataset-speciﬁc hyperparameters\nin Table 4. For all Universal Transformer experi-\nments, we use both the “No scaling” and the “Po-\nsitional Embedding Downscaling” methods. For\nthe standard Transformers with absolute positional\nembedding we test different scaling variants on dif-\nferent datasets shown in Table 6. When multiple\nscaling methods are available, we choose the best\nperforming ones when reporting results in Table 3.\nWe always use the same number of layers for both\nencoder and decoder. The embedding and the ﬁnal\nsoftmax weights of the decoder are always shared\n(tied embeddings).\nThe number of parameters for different models\nand the corresponding to representative execution\ntime is shown in Table 5.\nC Relative Positional Embedding\nWe use the relative positional embedding variant\nof self attention from Dai et al. (2019). Here, we\nuse a decomposed attention matrix of the following\nform:\nArel\ni,j = H⊤\ni W⊤\nq Wk,EHj\n  \n(a)\n+ H⊤\ni W⊤\nq Wk,P Pi−j\n  \n(b)\n+ u⊤Wk,EHj  \n(c)\n+ v⊤Wk,P Pi−j  \n(d)\nwhere Hi is the hidden state of the ith column of\nthe Transformer, Pi is an embedding for position\n(or in this case distance) i. Matrix Wq maps the\nstates to queries, Wk,E maps states to keys, while\nWk,P maps positional embedding to keys. u and v\nare learned vectors. Component (a) corresponds to\ncontent-based addressing, (b) to content based rela-\ntive positional addressing, (c) represents a global\ncontent bias, while (d) represents a global position\nbias.\nWe use sinusoidal positional embedding Pi ∈\nRdmodel . The relative position,i, can be both positive\nand negative. Inspired by Vaswani et al. (2017), we\ndeﬁne Pi,j as:\nPi,j =\n{\nsin(i/100002j/dmodel ), if j = 2k\ncos(i/100002j/dmodel ) if j = 2k+ 1\n(1)\nPrior to applying the softmax, Arel\ni,j is scaled by\n1√dmodel\n, as in Vaswani et al. (2017).\nWe never combine absolute with relative posi-\ntional embedding. In case of a relative positional\nvariant of any Transformer model, we do not add\nabsolute positional encoding to the word embed-\ndigs. We use relative positional attention in every\nlayer, except at the interface between encoder and\ndecoder, where we use the standard formulation\nfrom Vaswani et al. (2017), without adding any\npositional embedding.\nD Embedding Scaling\nIn this section, we provide full descriptions of em-\nbedding scaling strategies we investigated. In the\nfollowing, wi denotes the word index at input posi-\ntion i, Ew ∈Rdmodel denotes learned word embed-\nding for word index w. Positional embedding for\nposition iis deﬁned as in Eq. 1.\nToken Embedding Upscaling. Vaswani et al.\n(2017) combine the input word and positional\nembeddings for each position i as Hi =√dmodelEwi + Pi. Although in the original\npaper, the initialization of E is not discussed,\nmost implementations use Glorot initialization\n(Glorot and Bengio, 2010), which in this case\nmeans that each component of E is drawn from\nU(−\n√\n6\ndmodel+Nwords\n,\n√\n6\ndmodel+Nwords\n) where U(a,b)\nrepresents the uniform distribution in range [a,b].\nNo scaling. This corresponds to how PyTorch\ninitializes embedding layers by default: each el-\nement of E is drawn from N(0,1). N(µ,σ) is\nthe normal distribution with mean µand standard\ndeviation of σ. The word embeddings are com-\nbined with the positional embeddings without any\nscaling: Hi = Ewi + Pi\n631\nTable 4: Hyperparameters used for different tasks. We denote the feedforward size as dFF. For the learning rate of\nCFQ (denoted by *), the learning rate seemingly differs from Keysers et al. (2020). In fact, although Keysers et al.\n(2020) use Noam learning rate scheduling, scaling by 1√dmodel\nis not used, so we had to compensate for this to make\nthem functionally equivalent.\ndmodel dFF nhead nlayers batch size learning rate warmup scheduler\nSCAN 128 256 8 3 256 10−3 - -\nCFQ - Non-universal 128 256 16 2 4096 0.9* 4000 Noam\nCFQ - Universal 256 512 4 6 2048 2.24* 8000 Noam\nPCFG 512 2048 8 6 64 10−4 - -\nCOGS 512 512 8 2 128 10−4 - -\nCOGS Noam 512 512 8 2 128 2 4000 Noam\nMathematics 512 2048 8 6 256 10−4 - -\nTable 5: Model sizes and execution times. One representative split is shown per dataset. Other splits have the same\nnumber of parameters, and their execution time is in the same order of magnitude.\nDataset Model No. of params Execution time GPU type\nSCAN\nStandard 992k 1:30\nTitan X MaxwellUniversal 333k 1:15\nRelative Pos. 1.1M 1:45\nUniversal, Relative Pos. 366k 1:30\nCFQ MCD 2\nStandard 685k 10:00\nTesla V100-SXM2-32GB-LSUniversal 1.4M 12:00\nRelative Pos. 751k 14:15\nUniversal, Relative Pos. 1.5M 14:00\nPCFG Systematicity\nStandard 44.7M 20:30\nTesla V100-PCIE-16GBUniversal 7.9M 17:00\nRelative Pos. 47.8M 21:30\nUniversal, Relative Pos. 8.4M 21:30\nCOGS\nStandard 9.3M 17:30\nTesla V100-SXM2-32GB-LSUniversal 5.1M 17:15\nRelative Pos. 10.3M 21:00\nUniversal, Relative Pos. 5.6M 20:00\nMath: add_or_sub\nStandard 4.4M 8:00\nTesla P100-SXM2-16GBUniversal 7.4M 7:30\nRelative Pos. 4.7M 8:30\nUniversal, Relative Pos. 7.9M 8:00\nTable 6: Scaling types used for standard transform-\ners with absolute positional embedding on different\ndatasets. TEU denotes Token Embedding Upscaling,\nPED denotes Position Embedding Downscaling.\nTEU No scaling PED\nSCAN ✓ ✓\nCFQ MCD ✓ ✓\nCFQ Length ✓ ✓ ✓\nPCFG Productivity ✓ ✓ ✓\nPCFG Systematicity ✓ ✓ ✓\nCOGS ✓ ✓ ✓\nMathematics ✓ ✓\nPosition Embedding Downscaling. We propose\nto use Kaiming initialization (He et al., 2015)\nfor the word embeddings: each element of E ∼\nN(0, 1√dmodel\n). Instead of scaling up the word em-\nbeddings, the positional embeddings are scaled\ndown: Hi = Ewi + 1√dmodel\nPi\nE Analyzing the Positively Correlated\nLoss and Accuracy\nIn Sec. 3.2, we reported that on the generaliza-\ntion splits of some datasets both the accuracy and\nthe loss grows together during training. Here we\nfurther analyze this behavior in Figure 5 (see the\ncaption).\nF Accuracies on the IID Split\nTo show that the IID accuracy does not provide any\nuseful signal for assessing the quality of the ﬁnal\nmodel, we report IID accuracies of the models from\nTable 3 in Table 8. We only show datasets for which\nan IID validation set is available in the same split\n632\nTable 7: Test accuracy of different Transformer (Trafo) variants and different initializations on the considered\ndatasets. This is a more detailed version of Table 3, with detailed scores for all initialization variants. The last col-\numn shows previously reported accuracies. References: [1] Newman et al. (2020), [2] Keysers et al. (2020),\n[3] https://github.com/google-research/google-research/tree/master/cfq, [4] Hup-\nkes et al. (2020), [5] Kim and Linzen (2020), [6] Saxton et al. (2019). Results marked with ∗cannot be directly\ncompared because of different training setups. ∼denotes imprecise numbers read from charts in prior works. For\nthe conﬁguration marked by †, the results are obtained by running 8 seeds from which 3 crashed, resulting in 5 use-\nful runs reported below. Crashed runs suddenly drop their accuracy to 0, which never recovers during the training.\nThe reason for the crashing is the overly big learning rate (2.24, from the baseline). We run another 10 seeds with\nlearning rate of 2.0, obtaining similar ﬁnal accuracy of 0.75 ±0.02, but without any crashed runs.\nInit Trafo Uni. Trafo Rel. Trafo Rel. Uni. Trafo Reported\nSCAN (length cutoff=26) PED 0.30 ±0.02 0.21 ±0.01 - - 0.00[1]\nNo scaling 0.15 ±0.07 0.14 ±0.05 0.72 ±0.21 1.00 ±0.00\nCFQ Output length\nPED 0.56 ±0.02 0.60 ±0.34 - -\n∼0.66[2]TEU 0.57 ±0.00 0.74 ±0.02 † - -\nNo scaling 0.53 ±0.04 0.77 ±0.02 0.64 ±0.06 0.81 ±0.01\nCFQ MCD 1 PED 0.36 ±0.02 0.37 ±0.05 - - 0.37 ±0.02[3]\nNo scaling 0.40 ±0.01 0.39 ±0.03 0.39 ±0.01 0.39 ±0.04\nCFQ MCD 2 PED 0.08 ±0.01 0.09 ±0.01 - - 0.08 ±0.02[3]\nNo scaling 0.10 ±0.01 0.09 ±0.02 0.09 ±0.01 0.10 ±0.02\nCFQ MCD 3 PED 0.10 ±0.00 0.11 ±0.00 - - 0.11 ±0.00[3]\nNo scaling 0.11 ±0.00 0.11 ±0.01 0.11 ±0.01 0.11 ±0.03\nCFQ MCD mean PED 0.18 ±0.13 0.19 ±0.14 - - 0.19 ±0.01[2]\nNo scaling 0.20 ±0.14 0.20 ±0.14 0.20 ±0.14 0.20 ±0.14\nPCFG Productivity split\nPED 0.65 ±0.03 0.78 ±0.01 - -\n0.50 ±0.02[4]TEU 0.47 ±0.27 0.78 ±0.01 - -\nNo scaling 0.63 ±0.02 0.76 ±0.01 - 0.85 ±0.01\nPCFG Systematicity split\nPED 0.87 ±0.01 0.93 ±0.01 - -\n0.72 ±0.00[4]TEU 0.75 ±0.08 0.92 ±0.01 - -\nNo scaling 0.86 ±0.02 0.92 ±0.00 0.89 ±0.02 0.96 ±0.01\nCOGS\nPED 0.80 ±0.00 0.77 ±0.02 - -\n0.35 ±0.06[5]TEU 0.78 ±0.03 0.78 ±0.03 - -\nNo scaling 0.62 ±0.06 0.51 ±0.07 0.81 ±0.01 0.77 ±0.01\nMath: add_or_sub PED 0.80 ±0.01 0.92 ±0.02 - - ∼0.91[6]∗\nNo scaling 0.89 ±0.01 0.94 ±0.01 0.91 ±0.03 0.97 ±0.01\nMath: place_value PED 0.00 ±0.00 0.20 ±0.02 - - ∼0.69[6]∗\nNo scaling 0.12 ±0.07 0.12 ±0.01 - 0.75 ±0.10\n633\n10−2 10−1\nValidation loss\n95\n100Val. accuracy [%]\n 1k\n50k\n(a) COGS: IID Validation set\n4.25 4.50 4.75 5.00 5.25 5.50 5.75\nTest loss\n0\n25\n50\n75\nTest accuracy [%]\n 1k\n50k\n(b) COGS: Generalization test set\nFigure 4: Relationship between the loss and accuracy\non (a) IID validation set and (b) the generalization test\nset on COGS (it comes without a validation set for the\ngeneralization splits). Standard Transformers are used.\nThe color shows the training step. Five runs are shown.\nThe loss is shown on a logarithmic scale. On the IID\nvalidation set (a), the accuracy increases when the loss\ndecreases, as expected. In contrast, on the generaliza-\ntion split (b), high accuracy corresponds to higher loss.\nFor generalization validation loss versus generalization\naccuracy on CFQ MCD 1, see Figure 2. For the analy-\nsis of the underlying reason, see Figure 5.\nas the one reported in Table 3. This complements\nthe IID and generalization accuracies on COGS\nand PCFG with different embedding scalings we\nreported in Table 2. With the exception of standard\nTransformer on PCFG and the “place_value” mod-\nule of the Mathematics dataset, all other validation\naccuracies are 100%, while their generalization\naccuracy vary wildly.\nG Additional Results\nFigure 4 shows that both the test loss and accuracy\ngrows on COGS dataset during training. Addition-\nally, it shows the expected, IID behavior on the\nsame dataset for contrast.\nFigure 6 shows the relative change in conver-\ngence speed when using relative positional embed-\ndings.\n0 10k 20k 30k 40k 50k\nTraining steps\n2\n4\n6Loss\n“Good”\n“Bad”\nTotal\n(a) Decomposed loss\n0.0 2.5 5.0 7.5 10.0 12.5 15.0\nLoss\n0\n2000\n4000Count\nTraining step 1k\nTraining step 50k\n(b) Histogram of “good” loss (ﬁrst and last measurement)\n0.0 2.5 5.0 7.5 10.0 12.5 15.0\nLoss\n0\n200\n400\n600Count\nTraining step 1k\nTraining step 50k\n(c) Histogram of “bad” loss (ﬁrst and last measurement)\nFigure 5: Analysis of the growing test loss on the sys-\ntematically different test set on CFQ MCD 1 split. We\nmeasure the loss individually for each sample in the test\nset. We categorize samples as “good” if the network\noutput on the corresponding input matched the target\nexactly any point during the training, and as “bad” oth-\nerwise. (a) The total loss (increasing) can be decom-\nposed to the loss of the “good” samples (decreasing),\nand the loss of the “bad” samples (increasing). (b, c)\nThe histogram of the loss for the “good” and “bad” sam-\nples at the beginning and end of the training. The loss\nof the “good” samples concentrates near zero, while the\n“bad” samples spread out and the corresponding loss\ncan be very high. The net effect is a growing total loss.\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\nPCFG\nCFQ MCD 1\nCFQ MCD 2\nCFQ MCD 3\nCOGS\nMath: add or sub\nMath: place value Trafo\nUni. Trafo\nFigure 6: Relative change in convergence speed by\nusing relative positional embeddings instead of abso-\nlute. Convergence speed is measured as the mean num-\nber of steps needed to achieve 80% of the ﬁnal perfor-\nmance of the model. Relative variants usually converge\nfaster. Universal Transformers beneﬁt more than the\nnon-universal ones. The non-universal variants are not\nshown for PCFG and “Math: place_value”, because the\nrelative variants do not converge (see Sec. 3.1).\n634\nTable 8: IID validation accuracy for datasets where IID test set is available. CFQ and PCFG are not shown because\nthey require the model to be trained on a separate, IID split. The other settings correspond to Table 3 in the main\ntext. Generalization split test accuracies are shown in parenthesis for easy comparison.\nTransformer Uni. Transformer Rel. Transformer Rel. Uni. Transformer\nSCAN (length cutoff=26) 1.00 ±0.00 (0.30) 1.00 ±0.00 (0.21) 1.00 ±0.00 (0.72) 1.00 ±0.00 (1.00)\nCOGS 1.00 ±0.00 (0.80) 1.00 ±0.00 (0.78) 1.00 ±0.00 (0.81) 1.00 ±0.00 (0.77)\nMath: add_or_sub 1.00 ±0.00 (0.89) 1.00 ±0.00 (0.94) 1.00 ±0.00 (0.91) 1.00 ±0.00 (0.97)\nMath: place_value 0.80 ±0.45 (0.12) 1.00 ±0.00 (0.20) - 1.00 ±0.00 (0.75)\nTable 9: Accuracy of different Transformer variants on CFQ. “Big” variant has a batch size of 4096, and is trained\nwith Noam scheduler (learning rate 0.9). “Small” variant has a batch size of 512 and a ﬁxed learning rate of 10−4.\nThe ratio of accuracies of “small” and “big” variants are also shown in the “Ratio” column, indicating the relative\nperformance drop caused by decreasing the batch size. Relative variants experience less accuracy drop.\nVariant Transformer Rel. Transformer Uni. Transformer Rel. Uni. Transformer\nCFQ MCD 1\nBig 0.40 ±0.01 0 .39 ±0.02 0 .41 ±0.03 0 .42 ±0.02\nSmall 0.26 ±0.02 0 .32 ±0.01 0 .28 ±0.00 0 .36 ±0.01\nRatio 0.65 0.80 0.68 0.85\nCFQ MCD 2\nBig 0.10 ±0.01 0 .09 ±0.01 0 .09 ±0.00 0 .09 ±0.02\nSmall 0.05 ±0.01 0 .07 ±0.01 0 .04 ±0.01 0 .10 ±0.01\nRatio 0.51 0.76 0.50 1.05\nCFQ MCD 3\nBig 0.11 ±0.00 0 .11 ±0.01 0 .11 ±0.01 0 .12 ±0.02\nSmall 0.09 ±0.00 0 .09 ±0.00 0 .09 ±0.01 0 .11 ±0.01\nRatio 0.80 0.85 0.85 0.98\nCFQ Out. len.\nBig 0.57 ±0.02 0 .64 ±0.04 0 .76 ±0.03 0 .81 ±0.02\nSmall 0.41 ±0.03 0 .51 ±0.02 0 .55 ±0.02 0 .70 ±0.03\nRatio 0.72 0.80 0.73 0.87\nTable 10: Dataset statistics. “#” denotes number of samples. V ocabulary size shows the union of input and output\nvocabularies. Train and test length denotes the maximum input/output length in the train and test set, respectively.\nDataset # train # IID valid. # gen. test # gen. valid. V oc. size Train len. Test len.\nScan (length cutoff=26) 16458 1828 2624 - 19 9 /26 9 /48\nCFQ MCD 1 95743 - 11968 11968 181 29 /95 30 /103\nCFQ MCD 2 95743 - 11968 11968 181 29 /107 30 /91\nCFQ MCD 3 95743 - 11968 11968 181 29 /107 30 /103\nCFQ Output Length 100654 - 9512 9512 181 29 /77 29 /107\nPCFG Productivity 81010 - 11333 - 535 53 /200 71 /736\nPCFG Systematicity 82168 - 10175 - 535 71 /736 71 /496\nCOGS 24155 3000 21000 - 871 22 /153 61 /480\nMath: add_or_sub 1969029 10000 10000 - 69 60 /19 62 /23\nMath: place_value 1492268 9988 10000 - 69 50 /1 52 /1",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7486230134963989
    },
    {
      "name": "Computer science",
      "score": 0.7324604392051697
    },
    {
      "name": "Generalization",
      "score": 0.7067945599555969
    },
    {
      "name": "Embedding",
      "score": 0.649094820022583
    },
    {
      "name": "Artificial intelligence",
      "score": 0.565412163734436
    },
    {
      "name": "Machine learning",
      "score": 0.5157268643379211
    },
    {
      "name": "Artificial neural network",
      "score": 0.480419397354126
    },
    {
      "name": "Deep neural networks",
      "score": 0.44831612706184387
    },
    {
      "name": "Theoretical computer science",
      "score": 0.33631086349487305
    },
    {
      "name": "Algorithm",
      "score": 0.335537314414978
    },
    {
      "name": "Mathematics",
      "score": 0.15912464261054993
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}