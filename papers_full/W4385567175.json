{
  "title": "PEVL: Position-enhanced Pre-training and Prompt Tuning for Vision-language Models",
  "url": "https://openalex.org/W4385567175",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2062353949",
      "name": "Yuan Yao",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2150604126",
      "name": "Qianyu Chen",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2004093727",
      "name": "Ao Zhang",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A1972758884",
      "name": "Wei Ji",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2051269448",
      "name": "Zhiyuan Liu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2730258684",
      "name": "Tat-Seng Chua",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2157167650",
      "name": "Maosong Sun",
      "affiliations": [
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3034538190",
    "https://openalex.org/W3035688398",
    "https://openalex.org/W3214685499",
    "https://openalex.org/W3173220247",
    "https://openalex.org/W3206072662",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W4313068342",
    "https://openalex.org/W3035017890",
    "https://openalex.org/W4281633595",
    "https://openalex.org/W4214693531",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W4281479158",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4226058394",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963536419",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3173909648",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W3184735396",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4287251580",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2963518342",
    "https://openalex.org/W3204250462",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W4313037583",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W2982555517",
    "https://openalex.org/W2964345792",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W4287125738",
    "https://openalex.org/W2963521239",
    "https://openalex.org/W3200114289",
    "https://openalex.org/W3167118264",
    "https://openalex.org/W2579549467",
    "https://openalex.org/W4287113019",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2964022527",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W2963115613",
    "https://openalex.org/W2981694290",
    "https://openalex.org/W4312480274",
    "https://openalex.org/W2970869018",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W2963109634",
    "https://openalex.org/W2489434015",
    "https://openalex.org/W4312230726",
    "https://openalex.org/W3193402170",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W3199245537",
    "https://openalex.org/W4281481330",
    "https://openalex.org/W2949693283",
    "https://openalex.org/W3204392079",
    "https://openalex.org/W3159619744",
    "https://openalex.org/W2963938081",
    "https://openalex.org/W3198377975"
  ],
  "abstract": "Vision-language pre-training (VLP) has shown impressive performance on a wide range of cross-modal tasks, where VLP models without reliance on object detectors are becoming the mainstream due to their superior computation efficiency and competitive performance. However, the removal of object detectors also deprives the capability of VLP models in explicit object modeling, which is essential to various position-sensitive vision-language (VL) tasks, such as referring expression comprehension and visual commonsense reasoning. To address the challenge, we introduce PEVL that enhances the pre-training and prompt tuning of VLP models with explicit object position modeling. Specifically, PEVL reformulates discretized object positions and language in a unified language modeling framework, which facilitates explicit VL alignment during pre-training, and also enables flexible prompt tuning for various downstream tasks. We show that PEVL enables state-of-the-art performance of detector-free VLP models on position-sensitive tasks such as referring expression comprehension and phrase grounding, and also improves the performance on position-insensitive tasks with grounded inputs. We make the data and code for this paper publicly available at https://github.com/thunlp/PEVL.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11104–11117\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nPEVL: Position-enhanced Pre-training and Prompt Tuning for\nVision-language Models\nYuan Yao1∗, Qianyu Chen1∗, Ao Zhang2, Wei Ji2\nZhiyuan Liu1†, Tat-Seng Chua2, Maosong Sun1†\n1Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China\nBeijing National Research Center for Information Science and Technology\nInnovation Center of Tsinghua University, Shanghai, China\n2Sea-NExT Joint Lab, Singapore\nSchool of Computing, National University of Singapore, Singapore\nyaoyuanthu@163.com socqyc@gmail.com\nAbstract\nVision-language pre-training (VLP) has shown\nimpressive performance on a wide range of\ncross-modal tasks, where VLP models with-\nout reliance on object detectors are becoming\nthe mainstream due to their superior compu-\ntation efficiency and competitive performance.\nHowever, the removal of object detectors also\ndeprives the capability of VLP models in ex-\nplicit object modeling, which is essential to\nvarious position-sensitive vision-language (VL)\ntasks, such as referring expression comprehen-\nsion and visual commonsense reasoning. To\naddress the challenge, we introduce PEVL that\nenhances the pre-training and prompt tuning\nof VLP models with explicit object position\nmodeling. Specifically, PEVL reformulates\ndiscretized object positions and language in\na unified language modeling framework, which\nfacilitates explicit VL alignment during pre-\ntraining, and also enables flexible prompt tun-\ning for various downstream tasks. We show that\nPEVL enables state-of-the-art performance of\ndetector-free VLP models on position-sensitive\ntasks such as referring expression comprehen-\nsion and phrase grounding, and also improves\nthe performance on position-insensitive tasks\nwith grounded inputs. We make the data\nand code for this paper publicly available at\nhttps://github.com/thunlp/PEVL.\n1 Introduction\nRecent progress on self-supervised learning has\nled to powerful vision-language pre-training (VLP)\nmodels that achieve state-of-the-art performance on\na wide range of cross-modal tasks (Lu et al., 2019;\n∗indicates equal contribution\n†Corresponding authors: Z.Liu (liuzy@tsinghua.edu.cn),\nM.Sun (sms@tsinghua.edu.cn)\nLi et al., 2020c; Radford et al., 2021; Zhang et al.,\n2021; Kamath et al., 2021). Typically, VLP models\nare first pre-trained on large-scale image-text data\nto learn universal cross-modal representations, and\nthen fine-tuned to adapt to downstream tasks (Bom-\nmasani et al., 2021). While most traditional VLP\nmodels heavily rely on external object detectors to\nobtain the visual inputs (Lu et al., 2019; Su et al.,\n2020; Li et al., 2020c; Zhang et al., 2021), recently\nthere is a growing interest in VLP models that re-\nmove the reliance on object detectors due to their\nsuperior computation efficiency and competitive\nperformance (Li et al., 2021; Kim et al., 2021; Rad-\nford et al., 2021; Kamath et al., 2021).\nHowever, the removal of object detectors also\ndeprives the capability of VLP models in explicit\nobject modeling. The drawback hinders successful\nhandling of vision-language (VL) tasks which are\ninherently object-centric, where deep understand-\ning of objects and their interactions plays an essen-\ntial role (Antol et al., 2015; Plummer et al., 2015;\nKrishna et al., 2017; Hudson and Manning, 2019).\nTherefore, it is typically difficult for detector-free1\nVLP models to handle various position-sensitive\ntasks (i.e., tasks that demand explicit object posi-\ntions as input or output), such as visual common-\nsense reasoning (Zellers et al., 2019), visual re-\nlation detection (Krishna et al., 2017), referring\nexpression comprehension (Yu et al., 2016) and\nphrase grounding (Plummer et al., 2015), which\ngreatly undermines their generality and practicality\nas foundation models (Bommasani et al., 2021).\nFor tasks that do not require explicit object mod-\neling, such as visual question answering (Antol\n1Note that by detector-free, we mean that no external ob-\nject detector tools are required. However, object annotations\nmay still be needed during pre-training and tuning.\n11104\n(a) Generalized Masked Language Modeling Pre-training\n Horse <                                             > watched by the woman <                                            > [M]\n(c) Phrase Grounding\n The woman <                                          > is            the horse <                                          > \n(e) Visual Relation Detection\n[M]\nWhy is person_1 <                                           > standing by fence_1 <                                           > ? She wants to watch horse_2 <                                           > ?    AS          \n(d) Visual Commonsense Reasoning\n[M]\n What is the woman <                                            > watching ?   MA.    \n(f) Visual Question Answering\n[M]\nQuestion Answer Candidate\n175\n[M]\n86\n[M]\n254\n[M]\n460\n[M]\n310\n[M]\n73\n[M]\n406\n[M]\n475\nwatching\nyes\nhorse\n72\n A  woman  <                                             >  is  watching the  horse  <                                             >  \nxminxmin xmaxxmax ymaxymaxyminymin xminxmin xmaxxmax ymaxymaxyminymin\n[M] [M]\nGMLM\nHead 73\nV ocab V \n74\n…\n…\ndogGMLM\nHead horse\nV ocab V \ncat\n…\n…\n475406310 86 460254175\n73 475406310 72 5114550 86 460254175\n86 460254175\nObject 1: Object 2:\nObject 2\n73 475406310\n The horse <                                             > next to the woman  [M]\n(b) Referring Expression Comprehension\n175\n[M]\n86\n[M]\n254\n[M]\n460\nObject 1\n73 475406310\nFigure 1: PEVL formulates positions and language into a unified language modeling framework. (a) During\npre-training, PEVL recovers masked text and position tokens in a generalized masked language modeling (GMLM)\ntask. (b) During prompt-tuning, PEVL reformulates various VL tasks into a fill-in-the-blank problem, which are\naddressed by the reused GMLM head.\net al., 2015), previous works have shown that in-\ntroducing explicit grounding can also lead to better\nperformance and robustness (Anderson et al., 2018;\nHuang et al., 2019), which can hardly be achieved\nin current detector-free VLP models.\nIn a preliminary exploration, MDETR (Kamath\net al., 2021) proposes to enhance detector-free VLP\nmodels by regressing object positions with Trans-\nformer decoders, serving position-output tasks such\nas referring expression comprehension. However, it\nis still unknown how to deal with various position-\ninput tasks, such as visual commonsense reasoning\nand visual relation detection. Moreover, during\nfine-tuning, task-specific classification heads are\ntypically introduced, resulting in a significant gap\nbetween pre-training and fine-tuning, which hin-\nders taking full advantage of pre-trained model\ncapabilities in downstream tasks.\nIn this work, we propose PEVL that enhances\nthe pre-training and prompt tuning of VLP models\nwith explicit object position modeling. Inspired by\nthe recent Pix2Seq (Chen et al., 2022) that casts ob-\nject detection as a language modeling task, PEVL\nreformulates object positions as discrete tokens,\nand learns the joint distribution of object positions\nand language in a unified language modeling frame-\nwork, as shown in Figure 1. Specifically, PEVL\nexploits explicit region-text alignments in existing\nVL datasets. Discretized position tokens are placed\nafter object text tokens to indicate the object loca-\ntions in both pre-training and prompt tuning:\n(1) During pre-training, PEVL learns explicit\nVL alignment based on a generalized masked lan-\nguage modeling (GMLM) task, where the model\nrecovers masked text tokens and position tokens\nfrom cross-modal context. We note that although\nthe discretization of positions enables their uni-\nfied modeling with language, it also eliminates\nthe ordering of positions as compared with tradi-\ntional continuous regression methods (i.e., predict-\ning nearby and faraway positions to ground-truth\nare equally punished). The problem is exacerbated\nby the inevitable small disturbances in the human\nannotation of bounding boxes. To address the chal-\nlenge, we present a novel ordering-aware objec-\ntive for masked position reconstruction, which as-\nsigns larger probabilistic soft labels for nearby po-\nsition tokens, and therefore retains the ordering.\n(2) During prompt tuning, PEVL can support var-\nious downstream VL tasks in a flexible prompt\ntuning framework, where VL tasks are addressed\nby the reused GMLM head in a fill-in-the-blank\nparadigm. In this way, PEVL maximally mitigates\nthe gap between pre-training and tuning, and better\nstimulates the pre-trained model capabilities.\nWe conduct comprehensive experiments on five\nVL tasks, including position- output, input and\ninsensitive tasks. Experimental results show that\n11105\nthrough position enhancement, PEVL enables state-\nof-the-art performance of detector-free VLP mod-\nels on position-sensitive tasks such as referring\nexpression comprehension and phrase grounding,\nand also improves the performance on position-\ninsensitive tasks with grounded inputs.\nOur contributions are threefold: (1) We unify\nthe modeling of positions and language in a lan-\nguage modeling framework, which enhances both\npre-training and prompt tuning of VLP models. (2)\nWe present a novel ordering-aware objective that re-\ntains the ordering of position tokens and avoids the\ninfluence of position annotation noise. (3) We con-\nduct comprehensive experiments on five position-\nsensitive and insensitive VL tasks, which demon-\nstrates the effectiveness of the proposed model.\n2 Preliminary\nIn principle, the PEVL framework is orthogonal\nto VLP architectures and can be built on any VLP\nmodels to achieve position enhancement. In this\nwork, without loss of generality, we adopt AL-\nBEF (Li et al., 2021) as the model backbone, which\nis a representative detector-free VLP model that\nachieves state-of-the-art performance on many VL\ntasks. We briefly introduce the pre-training and\nfine-tuning procedure of ALBEF, and refer readers\nto the original paper for additional details.\nPre-training. The ALBEF architecture is com-\nposed of two unimodal encoders followed by a\ncross-modal encoder. Images and text are first\nencoded using a vision Transformer (Dosovitskiy\net al., 2021) and a text Transformer (Vaswani et al.,\n2017) respectively, and then fused with a cross-\nmodal Transformer. The model is pre-trained with\nthree tasks, including masked language model-\ning, image-text contrastive learning and image-\ntext matching. (1) Masked language modeling\naims to recover masked text tokens from the cross-\nmodal context. (2) Image-text contrastive learning\naligns the intermediate unimodal representations\nof image-text pairs by a contrastive loss. (3) Image-\ntext matching classifies whether an image-text pair\nis aligned based on the [CLS] token of the cross-\nmodal Transformer. To alleviate the noise in the\npre-training text, a momentum model is maintained\nbased on the moving-average of model parameters\nto provide pseudo-targets as additional supervision.\nFine-tuning. During fine-tuning, ALBEF intro-\nduces new classification heads or decoders to han-\ndle VL tasks, which leads to significant gap from\npre-training. The gap hinders taking full advantage\nof pre-trained capabilities for downstream tasks.\nMoreover, since object positions cannot be explic-\nitly modeled, detector-free VLP models typically\nstruggle on position-sensitive tasks, which greatly\nundermines their generality and practicality.\n3 Methodology\nWe introduce the PEVL framework, including the\nposition reformulation for VL models, and position-\nenhanced VL pre-training and prompt tuning.\n3.1 Reformulating Positions for VL Models\nCross-modal position modeling that explicitly con-\nnects image regions and text units underpins a\nbroad range of VL tasks. To enable strong cross-\nmodal position modeling capability of VLP mod-\nels, a primary challenge is to find a good position\nformulation that can be (1) easily integrated and\nunified into mainstream VLP models, and can be\n(2) flexibly prompt-tuned in various downstream\ntasks with minimal gap from pre-training as well.\nTo this end, previous works attempt to indi-\ncate image regions by introducing region embed-\ndings (Cho et al., 2021) or colors (Yao et al., 2021b)\nthat correspond to pre-defined text tokens, which\nrequire pre-detected image regions from costly ex-\nternal object detectors. In contrast, we note that for\nthe mainstream VLP models with vision Transform-\ners (Dosovitskiy et al., 2021) as visual encoders,\nimage patch positions are already well indicated by\npositional embeddings (Vaswani et al., 2017), and\ntherefore no special treatments are in fact needed\nfor visual position coordination.\nTo explicitly express visual positions in text, in-\nspired by Pix2Seq (Chen et al., 2022) that casts ob-\nject detection as a language modeling task, PEVL\nreformulates object bounding box coordinates as\ndiscrete position tokens. The position tokens can be\neasily unified with text tokens in a language model-\ning framework, where the vocabulary includes both\ntext and position tokens, and can also be easily pre-\ntrained with existing VLP techniques. In addition\nto the convenience in pre-training, another impor-\ntant advantage is that VLP models can be easily\nprompt-tuned to handle various position- sensitive\nand insensitive VL tasks with minimal gap from\npre-training, as shown in Figure 1.\nSpecifically, given an image-text pair for pre-\ntraining (I,T ), we exploit the composing object\ntexts and their bounding boxes O= {(ci,bi)}N\ni=1,\n11106\nwhere ci is the object text (e.g., person) in text T,\nand bi = (xmin,ymin,xmax,ymax) is the coordinates\nof the corresponding bounding box. The bounding\nbox coordinates are discretized into position tokens\nas ⌊Mx/w⌋and ⌊My/h⌋, where wand hare the\nwidth and height of the image, and M is the total\nnumber of the position tokens. Intuitively, a larger\nnumber of position tokens will lead to a coordinate\nsystem with higher resolution, but will be more\ncompute- and data- expensive to learn. Finally the\nposition tokens are placed after the corresponding\nobject text ci in T to explicitly indicate the object\nposition. Note that two special tokens “<” and “>”\nare introduced to indicate the start and end of posi-\ntion tokens, which are useful in prompting models\nto produce position tokens in position-output tasks.\n3.2 Position-enhanced VL Pre-training\nAfter unifying positions and text in a language mod-\neling framework, PEVL can be easily integrated\ninto existing VLP models. To effectively learn\nthe position and text interactions, in addition to\nthe image-text contrastive and image-text matching\ntasks (see Section 2), we present a novel gener-\nalized masked language modeling (GMLM) pre-\ntraining task, which recovers both masked text and\nposition tokens based on a generalized vocabulary\nVthat includes both types of tokens. We introduce\ntwo main components of the GMLM task, includ-\ning masking strategy and reconstruction objective.\nMasking Strategy. While the text tokens are\nusually masked with low ratios (e.g., 15%) in tra-\nditional MLM tasks (Devlin et al., 2019; Li et al.,\n2021), we find that the same masking strategy can-\nnot well serve position modeling. The reason is that\nobject positions are relatively low-level signals, and\ntherefore models can easily reconstruct the masked\nposition tokens when the masking ratio is low. For\nexample, reconstructing a single masked position\ntoken (e.g., xmin) given the other three unmasked\nones will be largely equivalent to enclosing an ob-\nject by moving a corner of the bounding box in a\nstraight line, which does not require deep under-\nstanding of the VL semantics. Similar problems\nare also discussed in self-supervised learning on\nimages (He et al., 2022).\nTo address the issue, we adopt high masking\nratios for position tokens, and encourage mask-\ning a more complete subset of object positions.\nSpecifically, for each object, we randomly mask\nn of its four position tokens with 0.25 probabil-\nity, where n = 1 ,2,3,4. For example, for 25%\nof the time, the four object positions (i.e., n= 4)\nare completely masked for reconstruction. For text\ntokens, we follow the 15% masking strategy in pre-\nvious works (Li et al., 2021).2 In this way, models\nare forced to learn high-level semantic interactions\namong image regions, text and position tokens.\nReconstruction Objective. Traditional MLM\ntasks typically adopt a one-hot target for token re-\nconstruction. However, we note that the one-hot\ntarget essentially eliminates the ordering of the po-\nsitions: If the position prediction is not exactly\ncorrect, predicting nearby and faraway positions\nto ground-truth are equally punished. The prob-\nlem is exacerbated by the inevitable small distur-\nbances in the human annotation process of bound-\ning boxes, which confuses models in discrete posi-\ntion learning. To address the problem, we present a\nnovel ordering-aware objectivefor position recon-\nstruction that assigns larger probabilistic soft labels\nfor nearby position tokens. Specifically, given a\nmasked position token, the unnormalized proba-\nbilistic label yi for each position token pi decreases\nexponentially with its distance to the ground-truth:\nyi = e−α|pi−p⋆|, (1)\nwhere |pi −p⋆|is the distance between pi and the\nground-truth p⋆, and αis a hyperparameter control-\nling the decay rate. The normalized probabilistic\nlabel ˜yi is then used to compute the ordering-aware\nobjective for position tokens:\nLp = −\n∑\npi\n˜yilog P([MASK] = pi). (2)\nThe probability of position tokens is given by\nthe GMLM head as:\nP([MASK] = pi) =\nexp(h⊤\n[MASK]pi)∑\npj\nexp(h⊤\n[MASK]pj), (3)\nwhere h[MASK] is the hidden representation of the\n[MASK] token, and pi is the representation of po-\nsition token pi in the GMLM head. In this way, the\nobjective retains the ordering of position tokens and\navoids the influence of position annotation noise.\nFor the text token reconstruction lossLt, we follow\nthe traditional implementation in ALBEF (Li et al.,\n2021). The final GMLM loss is the weighted sum\nof the loss for position token reconstruction and\n2For the chosen text tokens, the replacements are 80%\n[MASK] tokens, 10% random tokens, and 10% unchanged.\n11107\ntext token reconstruction: LGMLM = λLp + Lt,\nwhere λis a weighting hyperparameter.\nPre-training Corpora. PEVL exploits explicit\nobject position annotation in VL datasets for po-\nsition learning. The pre-training corpora consist\nof referring expressions (Yu et al., 2016; Mao\net al., 2016), Flickr30k (Plummer et al., 2015),\nGQA (Hudson and Manning, 2019), VCR (Zellers\net al., 2019) and Visual Genome (Krishna et al.,\n2017), with 4.7M image-text pairs in total. Follow-\ning Chen et al. (2020), we remove the images in\nthe downstream test and validation sets from the\npre-training corpora.\n3.3 Position-enhanced VL Prompt Tuning\nTo adapt VLP models to downstream tasks, pre-\nvious works typically introduce new classification\nheads or even Transformer decoders (Kamath et al.,\n2021; Li et al., 2021; Chen et al., 2020), lead-\ning to significant gap from pre-training. Recent\nworks in pre-trained language models have shown\nthat a consistent tuning approach with pre-training\n(i.e., prompt tuning) can better stimulate the pre-\ntrained capability in downstream tasks (Schick and\nSchütze, 2021; Gao et al., 2021; Liu et al., 2021).\nHowever, it is still unknown whether and how\nVLP models can be prompt tuned to support both\nposition- sensitive and insensitive VL tasks.\nIn this context, a crucial advantage of unifying\npositions with language is that, VLP models can\nbe easily prompt-tuned to handle various VL tasks\nbased on the reused GMLM headwith minimal gap\nfrom pre-training. We divide VL tasks according\nto the role of positions, including position-output\ntasks, position-input tasks, and position-insensitive\ntasks. Here we introduce the main prompt tuning\nprocedure for position-sensitive tasks. In our exper-\niments, we show that position-insensitive tasks can\nalso benefit from well-grounded inputs in PEVL\nframework (see Section 4.1).\nPosition-output Tasksdemand positions as task\noutputs (e.g., predicting the positions of objects\ndescribed by text), such as referring expression\ncomprehension and phrase grounding. To handle\nposition-output tasks, we simply place four con-\nsecutive [MASK] tokens wrapped by “<” and “>”\nafter object texts to be grounded for position pre-\ndiction. (1) Referring Expression Comprehension.\nSince the task requires locating the head noun, we\nplace the mask tokens after the first object text for\nposition prediction. (2) Phrase Grounding.Since\nthe task requires locating all objects, mask tokens\nare placed after each object text. After placing\nmask tokens, the model is prompt-tuned to produce\nposition tokens with reused GMLM head based on\nthe ordering-aware objective as in Equation 2.\nPosition-input Tasks require a mixture of po-\nsition and text (i.e., grounded text) as task inputs,\nsuch as visual commonsense reasoning and visual\nrelation detection. To handle position-input tasks,\nPEVL first explicitly indicates the object positions\nin input text (see Section 3.1)3, and then produces\nanswers in a fill-in-the-blank paradigm based on\nthe reused GMLM head.\nVisual Commonsense Reasoning.Given a ques-\ntion, models are asked to choose the answer sen-\ntence (and rationale) from multiple candidates.\nFor answer selection, the question q and answer\ncandidate ai are put in a prompt template as:\n“q ai answer:[MASK]”. Then the model can be\nprompted to decide which token t ∈{yes,no}is\nmore proper to reconstruct the [MASK] token. An-\nother plausible alternative is to reuse the image-text\nmatching head to discriminate whether the image is\naligned with the concatenated question and answer.\nThe intuition is that a question concatenated with\nthe correct answer can better match the image con-\ntent than concatenated with a wrong answer. In our\nexperiments, we find that the latter approach yields\nbetter performance on VCR. Despite the essential\nequivalence of the two prompting approaches (i.e.,\nclassifying special tokens in the last layer into bi-\nnary labels with reused pre-trained heads), image-\ntext matching task focuses more on the holistic\nmatching between cross-modal signals during pre-\ntraining, which better fits the VCR task containing\ntypically long text answers.\nVisual Relation Detection.Given an object pair\n(s,o) (e.g., woman, horse) in the image, models are\nrequired to classify their semantic relation r(e.g.,\nwatching, riding). We design the prompt template\nas: “ The sis [MASK] the o”. Then the model is\nprompted to produce the relational tokens from the\nrelation set with reused GMLM head. To deal with\nrelations that consist of different numbers of tokens,\nwe pad relational tokens to a maximum length l,\nand place lconsecutive masks in the template for\nrelation prediction. We also include a special re-\nlation no relation within the relation set, which\nindicates no relation between the object pair. Dur-\n3We omit the position tokens in the task input in the fol-\nlowing for simplicity.\n11108\nModel Object\nDetector\nRefCOCO RefCOCO+ RefCOCOg Flickr30k\nval testA testB val testA testB val test val test\nMAttNet (Yu et al., 2018a) w/ 76.7 81.1 70.0 65.3 71.6 56.0 66.6 67.3 - -\nDDPN (Yu et al., 2018b) w/ 76.8 80.1 72.4 64.8 70.5 54.1 - - 72.8 73.5\nVL-T5 (Cho et al., 2021) w/ - - - - - - 71.2 71.3 - -\nViLBERT (Lu et al., 2019) w/ - - - 72.3 78.5 62.6 - - - -\nVL-BERT_L (Su et al., 2020) w/ - - - 72.6 78.6 62.3 - - - -\nUNITER_L (Chen et al., 2020) w/ 81.4 87.0 74.2 75.9 81.5 66.7 74.9 75.8 - -\nVinVL_L (Zhang et al., 2021) w/ 81.8 87.2 74.3 74.5 80.8 64.3 74.6 75.7 - -\nVILLA_L (Gan et al., 2020) w/ 82.4 87.5 74.9 76.2 81.5 66.8 76.2 76.7 - -\nERNIE-ViL_L (Yu et al., 2021) w/ - - - 80.0 82.1 66.9 - - - -\nUniTAB (Yang et al., 2022) w/o 88.6 91.1 83.8 81.0 85.4 71.6 84.6 84.7 - 79.6\nMDETR (Kamath et al., 2021) w/o 87.5 90.4 82.7 81.1 85.5 73.0 83.4 83.3 82.3 83.8\nOFA (Wang et al., 2022a) w/o 88.5 90.7 83.3 81.4 87.2 74.3 82.3 82.3 - -\nALBEF† (Li et al., 2021) w/o - - - 58.5 65.9 46.3 - - - -\nPEVL w/o 89.6 92.5 85.0 83.0 88.4 74.5 87.1 86.3 84.1 84.4\n∆ - - - - + 24.5 +22.5 +28.2 - - - -\nTable 1: Experimental results on referring expression comprehension and phrase grounding. L: large size model. †:\nweakly supervised results, where only image-expression pairs are used due to the lack of explicit position modeling\ncapability. ∆: improvements of PEVL over the ALBEF backbone.\nModel Q →A QA →R Q →AR\nR2C 63.8 (65.1) 67.2 (67.3) 43.1 (44.0)\nTAB-VCR 69.9 (70.4) 72.2 (71.7) 50.6 (50.5)\nVisualBERT 70.8 (71.6) 73.2 (73.2) 52.2 (52.4)\nViLBERT 72.4 (73.3) 74.5 (74.6) 54.0 (54.8)\nUnicoder-VL 72.6 (73.4) 74.5 (74.4) 54.4 (54.9)\nB2T2 73.2 (74.0) 77.1 (77.1) 56.6 (57.1)\nUNITER 74.6 (75.0) 77.0 (77.2) 57.8 (58.2)\nVL-BERT 73.8 (75.8) 74.4 (78.4) 55.2 (59.7)\nALBEF 71.9 (72.9) 74.5 (74.5) 54.1 (54.7)\nPEVL 75.1 (76.0) 76.4 (76.7) 57.8 (58.6)\n∆ +3.2 (+3.1) + 1.9 (+2.2) + 3.7 (+3.9)\nTable 2: Experimental results of visual commonsense\nreasoning on VCR validation (and test) sets.\ning inference, the score of relation r is given by\nthe average log probability of non-padding tokens:\nsr = 1\n|r|\n∑|r|\ni=1 log P([MASK](i) = r(i)), where\n[MASK](i) is the i-th mask token, and r(i) is the\ni-th token of r. An important advantage of prompt\ntuning for the task is that, the large number of long-\ntail relations can be better learned thanks to the rich\nknowledge in VLP models.\n4 Experiments\nWe evaluate PEVL on five popular VL tasks. The\nmodels are in base size unless otherwise specified.\n4.1 Main Results\nReferring Expression Comprehension. We adopt\nthree popular datasets for the task, including Re-\nfCOCO, RefCOCO+ (Yu et al., 2016) and Ref-\nCOCOg (Mao et al., 2016). We use accuracy@0.5\nModel R@50 R@100 mR@50 mR@100\nMSDN 64.6 66.6 15.9 17.5\nVCTree 65.5 67.4 15.4 16.6\nGPS-Net 65.2 67.1 15.2 16.6\nMotif 66.0 67.9 14.6 15.8\nVisualDS 64.4 66.4 16.1 17.5\nUnbiased 47.2 51.6 25.4 28.7\nIETrans 48.6 50.5 35.8 39.1\nDT2-ACBS 23.3 25.6 35.9 39.7\nALBEF 57.6 63.5 12.2 15.4\nPEVL 64.4 66.3 21.7 23.5\n∆ +6.8 +2.8 +9.5 +8.1\nTable 3: Experimental results of visual relation detection\non Visual Genome dataset.\nas the evaluation metric (Kamath et al., 2021). For\nbaselines, we compare with state-of-the-art models\nfor the task, and VLP models with large-size back-\nbones. We report the weakly supervised results\nfrom ALBEF (Li et al., 2021), which uses GRAD-\nCAM (Selvaraju et al., 2017) heat map to rank the\nobject candidates from external detectors.\nFrom the experimental results in Table 1, we\nhave the following observations: (1) PEVL outper-\nforms all baseline models, achieving a new state-\nof-the-art on all three datasets for the task. Specifi-\ncally, the base-size PEVL outperforms the state-of-\nthe-art regression-based MDETR by 2.9 absolute\npoints on the RefCOCO+ testA set, and large-size\nVLP models that use external detector feature in-\nputs, such as ERNIE-ViL and VILLA. (2) PEVL\nsignificantly improves the ALBEF backbone by\nexplicit object position modeling, effectively ad-\ndressing the shortcoming in position-output tasks.\n11109\nModels LXMERT BAN CTI CFR ALBEF PEVL† ∆\nAccuracy 59.8 61.5 61.7 73.6 64.8 77.0 +12.2\nTable 4: Visual question answering results on GQA validation set. †: grounded inputs.\nQuestion Type Relation Attribute Object Category Global Overall\nPercentage (%) 46.7 32.0 11.8 6.5 3.1 100.0\nALBEF 56.9 67.9 87.9 62.5 68.1 64.8\nPEVL† 68.4 84.2 98.1 68.8 68.5 77.0\n∆ +11.5 +16.3 +10.2 +6.3 +0.4 +12.2\nTable 5: Visual question answering results of different question types on GQA validation set. †: grounded inputs.\nModel RefCOCO+ Flickr30k VCR VG GQA\nval testA testB val test Q →A QA →R Q →AR R@50 mR@50 val\nPEVL 83.1 88.4 74.5 84.1 84.4 75.1 76.4 57.8 64.4 21.7 77.0\nw/o PT - - - - - 75.1 76.2 57.6 61.7 14.2 77.0\nw/o OAO 79.9 86.3 69.4 82.2 82.9 75.6 76.3 57.8 64.1 21.5 76.4\nw/o Pos - - - - - 71.9 74.5 54.1 61.5 18.9 66.6\nTable 6: Ablation results. PT: prompt tuning, OAO: ordering-aware objective, Pos: position tokens.\nPhrase Grounding. We perform experiments\non the Flickr30k entities dataset (Plummer et al.,\n2015). Following MDETR, we adopt merged-box\naccuracy@0.5 as the evaluation metric, and com-\npare our model with the state-of-the-art baselines\nfor the task (Kamath et al., 2021; Yang et al., 2022).\nFrom Table 1 we observe that PEVL achieves a\nnew state-of-the-art on the phrase grounding task\nin grounding multiple objects in text. The results\nshow that PEVL can effectively integrate positions\nwith language to achieve competitive performance\nfor various position-output tasks.\nVisual Commonsense Reasoning. We adopt\nthe popular VCR benchmark (Zellers et al., 2019),\nwhich provides human-annotated positions for ob-\njects. We report the accuracy of predicting the an-\nswer (Q →A), rationale (QA →R) and both (Q →\nAR). We compare with task-specific baselines and\nstrong VLP models. For fair comparisons, we fur-\nther pre-train ALBEF baseline on the same corpora\nas PEVL in all experiments. From the results in Ta-\nble 2, we observe that PEVL significantly improves\nthe ALBEF backbone (e.g., by 3.9 absolute points\nin Q →AR), achieving comparable performance\nto strong UNITER equipped with external object\ndetectors. While the results are not state-of-the-art\non the VCR benchmark, they are quite reasonable\nconsidering the current literature. The results show\nthat PEVL can effectively provide clues for com-\nplex reasoning through grounded inputs.\nVisual Relation Detection. We evaluate PEVL\non the widely used Visual Genome dataset (Krishna\net al., 2017), which contains 50 visual relation\ntypes. Given the human-annotated positions and\nlabels of an object pair, models are required to pre-\ndict the relations. Following previous works (Tang\net al., 2020; Lin et al., 2020), we report the re-\ncall@K (R@K) and mean recall@K (mR@K) as\nevaluation metrics. We compare PEVL with state-\nof-the-art baselines with detector feature inputs.\nFrom Table 3, we observe that: (1) Without task-\nspecific designs or heuristics, PEVL achieves com-\npetitive performance in both R@K and mR@K.\nThe results show that PEVL can effectively stimu-\nlate the knowledge in VLP models for both frequent\nand long-tail relations through prompt tuning. (2)\nALBEF struggles on the visual relation detection\ntask, since the positions of the target object pair can-\nnot be informed. In contrast, PEVL can effectively\nintegrate the object position information through\nsimple position tokens for relation prediction.\nVisual Question Answering. For position-\ninsensitive tasks such as visual question answering,\nobject positions are not required to be explicitly\nmodeled. However, we argue that explicit object\nposition modeling can provide fine-grained clues\nfor complex question reasoning. Specifically, we\nare interested in the question: Can VLP models\nbenefit from grounded text for answering complex\nquestions in PEVL framework?\n11110\nram that is eating off the groundbrown horse with grey saddle blanketsilverplaneinabluesky,readytolandwithitswheelsdown,whilespectatorswatchbehindahighfence\nFigure 2: Case study on referring expression comprehension and phrase grounding tasks.\nIn principle, to achieve explicit position augmen-\ntation, VQA can be decomposed into two position-\nsensitive stages, including an object grounding\n(position-output) stage and a question answering\n(position-input) stage. However, in our experi-\nments, we find that the unneglectable errors in\ncurrent visual grounding models constitute a bot-\ntleneck for such a two-stage model. Therefore,\nwe turn to an ideal experiment, where the ground-\ntruth object positions are available. Specifically,\nwe adopt the object position annotation provided\nby the GQA dataset (Hudson and Manning, 2019).\nWe explicitly indicate the position of each object\nin a similar approach as in position-input tasks (see\nSection 3.3). Then the position-enhanced ques-\ntion is put into the prompt template: “ q answer:\n[MASK]”. Finally models are asked to generate\nanswer tokens from answer candidate set. We use\nthe same approach as in visual relation detection to\ncope with multi-token answers.\nFrom Table 4 we observe that with grounded in-\nputs, PEVL significantly improves the performance\nof ALBEF backbone in compositional question an-\nswering. The results show that object grounding is\nstill one of the key obstacles in VQA, and PEVL\ncan effectively utilize grounded questions for VQA\nin a simple prompt tuning framework. To investi-\ngate which type of questions benefit from grounded\ninputs, we divide GQA validation set according to\nthe question types from MDETR. From Table 5,\nwe can see that high-quality grounding signals im-\nprove the performance on all question types. In-\nterestingly, relation and attribute-based questions\nbenefit more from grounded text than object-based\nquestions, indicating the fundamental role of object\nmodeling in reasoning over complex questions.\n4.2 Experimental Analysis\nAblation Study. We ablate key components of\nPEVL, including prompt tuning, ordering-aware\nMask val testA testB Epochs\n20% 89.3 92.3 84.3 5\n40% 89.0 92.2 84.7 5\n60% 89.4 92.4 83.4 7\nOurs 89.6 92.5 85.0 3\nTable 7: Performance and epochs of different masking\nstrategies on the RefCOCO dataset.\nobjective, and position tokens to investigate their\ncontribution. From the results in Table 6, we can\nsee that all components contribute to the final per-\nformance. Specifically, position enhancement and\nprompt tuning are essential for PEVL to perform\nposition-output tasks. Prompt tuning can also be\nhelpful in learning long-tail relations for visual\nrelation detection, which is consistent with the\nresults from previous works (Yao et al., 2021b).\nThe ordering-aware objective contributes more to\nposition-output tasks than position-input tasks.\nInfluence of Masking Strategies. We investi-\ngate the influence of different masking strategies\nfor position tokens during pre-training. Specifi-\ncally, for baselines, the position tokens are inde-\npendently chosen with a certain probability during\npre-training, where the ratios of masked, replaced\nand unchanged tokens for the chosen token are kept\nidentical to BERT (Devlin et al., 2019). We report\nthe performance and the number of epochs required\nin the intermediate pre-training on the RefCOCO\ndataset. From the results in Table 7, we observe\nthat our masking strategy achieves both better per-\nformance and faster convergence. The results show\nthat a high masking ratio and a more complete sub-\nset of masked positions are both important for good\nposition learning results.\nCase Study. We visualize the position predic-\ntions on the validation sets of RefCOCO+, Ref-\nCOCOg and Flicker30k. Previous visual local-\nization models are either based on continuous re-\n11111\ngression (Kamath et al., 2021), or limited to non-\nlanguage tasks (Chen et al., 2022). From Fig-\nure 2, we can see that discretized positions can be\nclosely integrated with language in Transformers\nto achieve strong visual reasoning and localization\nresults. Similar to regression-based models, the\nlocalization of small objects (e.g., wheels in the\nright figure) can also be challenging.\n5 Related Work\nPosition Enhancement for VLP.Object position\nmodeling underpins a wide range of VL tasks. To\ndeal with position-output tasks, some works (Ka-\nmath et al., 2021; Gupta et al., 2022; Yang et al.,\n2022) propose to perform object position predic-\ntion using Transformer decoders, but are unable to\nhandle various position-input tasks. To explicitly\nindicate position inputs for VLP models, previous\nworks explored learning region embeddings (Cho\net al., 2021), or color-based cross-modal corefer-\nential markers (Yao et al., 2021b), but rely on ex-\nternal object detectors. MERLOT (Zellers et al.,\n2021) also proposes to highlight objects in images\nwith colors for position-input tasks. X-VLM (Zeng\net al., 2022) aligns multi-grained concepts in text\nand image for VLP models. In comparison, PEVL\nsupports both position- input and output VL tasks\nin a unified prompt tuning framework.\nPrompt Tuning. Prompt tuning for pre-trained\nlanguage models is in rapid growth in natural lan-\nguage processing (Petroni et al., 2019; Raffel et al.,\n2020; Brown et al., 2020; Schick and Schütze,\n2021; Gao et al., 2021; Qin and Eisner, 2021; Liu\net al., 2021; Yao et al., 2022). Recently there is\nalso growing interest in prompt tuning VLP models.\nMost existing works prompt tune contrastively pre-\ntrained image-text matching models (Radford et al.,\n2021; Jia et al., 2021) for recognition tasks (Zhou\net al., 2022; Rao et al., 2022; Wang et al., 2021; Gu\net al., 2022; Xie and Zheng, 2021; Ju et al., 2022).\nCPT (Yao et al., 2021b) prompt tunes VLP models\nwith color-based prompts, and achieves promising\nresults for zero- and few-shot tasks. Some works\nperform pre-training and VL tasks using identical\nTransformer decoders in an auto-regressive fash-\nion (Wang et al., 2022b; Cho et al., 2021; Yang\net al., 2022; Tsimpoukelli et al., 2021), which\navoids the gap between pre-training and tuning,\nbut are typically limited in performance due to the\nunidirectional architecture.\n6 Conclusion and Future Work\nIn this work, we present PEVL that enhances\nthe pre-training and prompt-tuning of detector-\nfree VLP models with unified position and lan-\nguage modeling. Comprehensive experimental re-\nsults demonstrate the effectiveness of the proposed\nmodel. Future works include exploring weakly su-\npervised signals for position and language learning\nwithout human annotation.\n7 Acknowledgement\nThis work is supported by the National Key R&D\nProgram of China (No. 2020AAA0106502), In-\nstitute Guo Qiang at Tsinghua University and\nNExT++ project from the National Research Foun-\ndation, Prime Minister’s Office, Singapore under\nits IRC@Singapore Funding Initiative.\nYuan Yao designed the framework and exper-\niments, and wrote the paper. Qianyu Chen con-\nducted the experiments. Ao Zhang and Wei Ji par-\nticipated in the discussion. Zhiyuan Liu, Tat-Seng\nChua and Maosong Sun advised the project.\n8 Limitations\nWe identify several key limitations of PEVL that\nare promising for future explorations.\nComputation Cost. To explicitly model object\npositions and text in a unified framework, we intro-\nduce four additional position tokens for each target\nobject, which requires more computation cost. Im-\nproving the computation efficiency of additional\nposition tokens is an important direction for future\nimprovements.\nObject Annotation. Similar to other explicit ob-\nject position modeling VLP models, PEVL requires\nmanual object annotation in multi-modal datasets.\nIt will be promising to explore pre-training tasks\nthat can learn position tokens with less supervi-\nsion. For example, VLP models can be boot-\nstrapped from small-scale human annotations and\nlarge-scale predicted annotations.\nReferences\nChris Alberti, Jeffrey Ling, Michael Collins, and David\nReitter. 2019. Fusion of detected objects in text for vi-\nsual question answering. In Proceedings of EMNLP-\nIJCNLP.\nPeter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei Zhang.\n2018. Bottom-up and top-down attention for image\n11112\ncaptioning and visual question answering. In Pro-\nceedings of CVPR.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and\nDevi Parikh. 2015. VQA: Visual question answering.\nIn Proceedings of ICCV.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosse-\nlut, Emma Brunskill, et al. 2021. On the opportuni-\nties and risks of foundation models. arXiv preprint\narXiv:2108.07258.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In Proceddings of NeurIPS.\nTing Chen, Saurabh Saxena, Lala Li, David J Fleet, and\nGeoffrey Hinton. 2022. Pix2Seq: A language model-\ning framework for object detection. In Proceedings\nof ICLR.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. UNITER: Universal image-text\nrepresentation learning. In Proceedings of ECCV.\nJaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021.\nUnifying vision-and-language tasks via text genera-\ntion. In Proceedings of ICML.\nAlakh Desai, Tz-Ying Wu, Subarna Tripathi, and Nuno\nVasconcelos. 2021. Learning of visual relations: The\ndevil is in the tails. In Proceedings of ICCV.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL-HLT.\nTuong Do, Thanh-Toan Do, Huy Tran, Erman Tjiputra,\nand Quang D Tran. 2019. Compact trilinear interac-\ntion for visual question answering. In Proceedings\nof ICCV.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2021.\nAn image is worth 16x16 words: Transformers for\nimage recognition at scale. In Proceedings of ICLR.\nZhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu,\nYu Cheng, and Jingjing Liu. 2020. Large-scale adver-\nsarial training for vision-and-language representation\nlearning. In Proceedings of NeurIPS.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of ACL.\nXiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.\n2022. Open-vocabulary object detection via vision\nand language knowledge distillation. In Proceedings\nof ICLR.\nTanmay Gupta, Amita Kamath, Aniruddha Kembhavi,\nand Derek Hoiem. 2022. Towards general purpose\nvision systems: An end-to-end task-agnostic vision-\nlanguage architecture. In Proceedings of CVPR.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Pi-\notr Dollár, and Ross Girshick. 2022. Masked autoen-\ncoders are scalable vision learners. In Proceedings\nof CVPR.\nPingping Huang, Jianhui Huang, Yuqing Guo, Min Qiao,\nand Yong Zhu. 2019. Multi-grained attention with\nobject-level grounding for visual question answering.\nIn Proceedings of ACL.\nDrew A Hudson and Christopher D Manning. 2019.\nGQA: A new dataset for real-world visual reasoning\nand compositional question answering. In Proceed-\nings of CVPR.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc V . Le, Yun-Hsuan Sung,\nZhen Li, and Tom Duerig. 2021. Scaling up vi-\nsual and vision-language representation learning with\nnoisy text supervision. In Proceedings of ICML.\nChen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and\nWeidi Xie. 2022. Prompting visual-language models\nfor efficient video understanding. In Proceedings of\nECCV.\nAishwarya Kamath, Mannat Singh, Yann LeCun, Ishan\nMisra, Gabriel Synnaeve, and Nicolas Carion. 2021.\nMDETR: Modulated detection for end-to-end multi-\nmodal understanding. In Proceedings of ICCV.\nJin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang.\n2018. Bilinear attention networks. In Proceedings of\nNeurIPS.\nWonjae Kim, Bokyung Son, and Ildoo Kim. 2021. ViLT:\nVision-and-language transformer without convolu-\ntion or region supervision. In Proceedings of ICML.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al.\n2017. Visual Genome: Connecting language and\nvision using crowdsourced dense image annotations.\nInternational Journal of Computer Vision.\nGen Li, Nan Duan, Yuejian Fang, Ming Gong, and\nDaxin Jiang. 2020a. Unicoder-VL: A universal en-\ncoder for vision and language by cross-modal pre-\ntraining. In Proceedings of AAAI.\nJunnan Li, Ramprasaath R Selvaraju, Akhilesh Deepak\nGotmare, Shafiq Joty, Caiming Xiong, and Steven\nHoi. 2021. Align before fuse: Vision and language\nrepresentation learning with momentum distillation.\nIn Proceedings of NeurIPS.\n11113\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2020b. VisualBERT:\nA simple and performant baseline for vision and lan-\nguage. In Proceedings of ACL.\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang,\nXiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\nLi Dong, Furu Wei, et al. 2020c. Oscar: Object-\nsemantics aligned pre-training for vision-language\ntasks. In Proceedings of ECCV.\nJ. Lin, U. Jain, and A. G. Schwing. 2019. TAB-VCR:\nTags and attributes based vcr baselines. In Proceed-\nings of NeurIPS.\nXin Lin, Changxing Ding, Jinquan Zeng, and Dacheng\nTao. 2020. GPS-Net: Graph property sensing net-\nwork for scene graph generation. In Proceedings of\nCVPR.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.\n2019. ViLBERT: Pretraining task-agnostic visiolin-\nguistic representations for vision-and-language tasks.\nIn Proceedings of NeurIPS.\nJunhua Mao, Jonathan Huang, Alexander Toshev, Oana\nCamburu, Alan L Yuille, and Kevin Murphy. 2016.\nGeneration and comprehension of unambiguous ob-\nject descriptions. In Proceedings of CVPR.\nBinh X Nguyen, Tuong Do, Huy Tran, Erman Tjiputra,\nQuang D Tran, and Anh Nguyen. 2022. Coarse-\nto-fine reasoning for visual question answering. In\nProceedings of CVPR Workshops.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of EMNLP-IJCNLP.\nBryan A Plummer, Liwei Wang, Chris M Cervantes,\nJuan C Caicedo, Julia Hockenmaier, and Svetlana\nLazebnik. 2015. Flickr30k entities: Collecting\nregion-to-phrase correspondences for richer image-\nto-sentence models. In Proceedings of ICCV.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying LMs with mixtures of soft prompts.\nIn Proceedings of NAACL.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models\nfrom natural language supervision. In Proceedings\nof ICML.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research.\nYongming Rao, Wenliang Zhao, Guangyi Chen, Yan-\nsong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Ji-\nwen Lu. 2022. DenseCLIP: Language-guided dense\nprediction with context-aware prompting. In Pro-\nceedings of CVPR.\nTimo Schick and Hinrich Schütze. 2021. It’s not just\nsize that matters: Small language models are also\nfew-shot learners. In Proceedings of NAACL.\nRamprasaath R Selvaraju, Michael Cogswell, Abhishek\nDas, Ramakrishna Vedantam, Devi Parikh, and\nDhruv Batra. 2017. GRAD-CAM: Visual explana-\ntions from deep networks via gradient-based localiza-\ntion. In Proceedings of ICCV.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2020. VL-BERT: Pre-\ntraining of generic visual-linguistic representations.\nIn Proceedings of ICLR.\nHao Tan and Mohit Bansal. 2019. LXMERT: Learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of EMNLP-IJCNLP.\nKaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi,\nand Hanwang Zhang. 2020. Unbiased scene graph\ngeneration from biased training. In Proceedings of\nCVPR.\nKaihua Tang, Hanwang Zhang, Baoyuan Wu, Wenhan\nLuo, and Wei Liu. 2019. Learning to compose dy-\nnamic tree structures for visual contexts. In Proceed-\nings of CVPR.\nMaria Tsimpoukelli, Jacob Menick, Serkan Cabi,\nSM Eslami, Oriol Vinyals, and Felix Hill. 2021. Mul-\ntimodal few-shot learning with frozen language mod-\nels. In Proceedings of NeurIPS.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of NeurIPS.\nMengmeng Wang, Jiazheng Xing, and Yong Liu. 2021.\nActionCLIP: A new paradigm for video action recog-\nnition. In Proceedings of CVPR.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai\nBai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. 2022a. OFA: Unifying\narchitectures, tasks, and modalities through a sim-\nple sequence-to-sequence learning framework. In\nProceedings of ICML.\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai,\nYulia Tsvetkov, and Yuan Cao. 2022b. SimVLM:\nSimple visual language model pretraining with weak\nsupervision. In Proceedings of ICLR.\n11114\nJohnathan Xie and Shuai Zheng. 2021. ZSD-\nYOLO: Zero-shot YOLO detection using vision-\nlanguage knowledge distillation. arXiv preprint\narXiv:2109.12066.\nDanfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-\nFei. 2017. Scene graph generation by iterative mes-\nsage passing. In Proceedings of CVPR.\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei\nHu, Faisal Ahmed, Zicheng Liu, Yumao Lu, and\nLijuan Wang. 2022. UniTAB: Unifying text and box\noutputs for grounded vision-language modeling. In\nProceedings of ECCV.\nYuan Yao, Bowen Dong, Ao Zhang, Zhengyan Zhang,\nRuobing Xie, Zhiyuan Liu, Leyu Lin, Maosong Sun,\nand Jianyong Wang. 2022. Prompt tuning for discrim-\ninative pre-trained language models. In Findings of\nACL.\nYuan Yao, Ao Zhang, Xu Han, Mengdi Li, Cornelius\nWeber, Zhiyuan Liu, Stefan Wermter, and Maosong\nSun. 2021a. Visual distant supervision for scene\ngraph generation. In Proceedings of ICCV.\nYuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu,\nTat-Seng Chua, and Maosong Sun. 2021b. CPT: Col-\norful prompt tuning for pre-trained vision-language\nmodels. arXiv preprint arXiv:2109.11797.\nFei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian,\nHua Wu, and Haifeng Wang. 2021. ERNIE-ViL:\nKnowledge enhanced vision-language representa-\ntions through scene graphs. In Proceedings of AAAI.\nLicheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu,\nMohit Bansal, and Tamara L Berg. 2018a. MAttNet:\nModular attention network for referring expression\ncomprehension. In Proceedings of CVPR.\nLicheng Yu, Patrick Poirson, Shan Yang, Alexander C\nBerg, and Tamara L Berg. 2016. Modeling context\nin referring expressions. In Proceedings of ECCV.\nZhou Yu, Jun Yu, Chenchao Xiang, Zhou Zhao, Qi Tian,\nand Dacheng Tao. 2018b. Rethinking diversified and\ndiscriminative proposal generation for visual ground-\ning. In Proceedings of IJCAI.\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin\nChoi. 2019. From recognition to cognition: Visual\ncommonsense reasoning. In Proceedings of CVPR.\nRowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu,\nJae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi.\n2021. MERLOT: Multimodal neural script knowl-\nedge models. In Proceedings of NeurIPS.\nRowan Zellers, Mark Yatskar, Sam Thomson, and Yejin\nChoi. 2018. Neural motifs: Scene graph parsing with\nglobal context. In Proceedings of CVPR.\nYan Zeng, Xinsong Zhang, and Hang Li. 2022. Multi-\ngrained vision language pre-training: Aligning texts\nwith visual concepts. In Proceedings of ICML.\nAo Zhang, Yuan Yao, Qianyu Chen, Wei Ji, Zhiyuan\nLiu, Maosong Sun, and Tat-Seng Chua. 2022. Fine-\ngrained scene graph generation with data transfer. In\nProceedings of ECCV.\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei\nYang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian-\nfeng Gao. 2021. VinVL: Revisiting visual represen-\ntations in vision-language models. In Proceedings of\nCVPR.\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and\nZiwei Liu. 2022. Learning to prompt for vision-\nlanguage models. International Journal of Computer\nVision.\nA Pre-training Details\nWe provide pre-training details and statistics of the\npre-training corpora.\nImplementation details. Our backbone consists\nof a 6-layer text Transformer encoder, a ViT-B/16\nvisual encoder, and a 6-layer cross-modal Trans-\nformer encoder (commonly referred to as base size\nin the literature), with 209.5M parameters in to-\ntal. The backbone is open-sourced for research\nusage. In pre-training, we initialize PEVL with pre-\ntrained parameters from ALBEF for computation\nefficiency. PEVL is pre-trained with learning rate\n8e-5, batchsize 512 on 32 NVIDIA V100 GPUs for\n5 epochs. The number of position tokens is 512,\nwith decay rate α= 0.25, and weighting hyperpa-\nrameter λ = 2 in ordering-aware reconstruction.\nThe hyperparameters are selected by grid search on\nthe validation sets. For data augmentation, follow-\ning MDETR (Kamath et al., 2021), we augment\nimages with random size crop. We also follow\nPix2Seq (Chen et al., 2022) to adopt horizontal flip\nto augment images, where “left” and “right” in text\nare swapped after flip to ensure the semantic cor-\nrectness. Previous works suggest that an interme-\ndiate in-domain pre-training can better adapt VLP\nmodels to downstream tasks (Chen et al., 2020).\nWe therefore conduct an intermediate pre-training\nbefore tuning on each downstream task.\nPre-training Corpora. The pre-training corpora\nconsist of referring expressions (Yu et al., 2016;\nMao et al., 2016), Flickr30k (Plummer et al., 2015),\nGQA (Hudson and Manning, 2019), VCR (Zellers\net al., 2019) and Visual Genome dense captions (Kr-\nishna et al., 2017), with 4.7M image-text pairs and\n210K images in total. We provide the detailed\nstatistics of the datasets in Table 8.\n11115\nDataset RefCOCO RefCOCO+ RefCOCOg Flickr GQA VCR Visual Genome\n# Image-text pairs 107K 107K 72K 148K 762K 1.7M 1.8M\n# Images 15K 15K 20k 30K 63K 80K 46K\nTable 8: Statistics of pre-training corpora. Numbers of image-text pairs and images are reported.\nB Downstream Tasks\nWe provide details of dataset, prompt tuning and\nbaseline models for each downstream task.\nB.1 Referring Expression Comprehension\nDatasets. RefCOCO (Yu et al., 2016) is collected\nfrom a referential game between two players. The\ndataset is split into train, validation, testA and\ntestB sets, containing 120,624, 10,834, 5,657 and\n5,095 expression-object pairs respectively. Ref-\nCOCO+ (Yu et al., 2016) is also constructed in an\ninteractive fashion, and contains 120,191, 10,758,\n5,726 and 4,889 expression-object pairs in train,\nvalidation, testA and testB sets respectively. Re-\nfCOCOg (Mao et al., 2016) is built in a non-\ninteractive way, and contains 80,512, 4,896 and\n9,602 expression-object pairs in train, validation\nand test sets respectively.\nPrompt Tuning. We tune the model with learn-\ning rate 1e-5, weight decay 0.02, and batchsize 32\nfor 10 epochs. Following previous works (Dosovit-\nskiy et al., 2021; Li et al., 2021), we use a higher\nimage resolution of 512 in downstream tuning. The\nhyperparameters are selected by grid search on the\nvalidation set for all experiments. During infer-\nence, we select the position token with the largest\nreconstruction score for each of the four masked\ntokens.\nBaselines. We compare with state-of-the-art base-\nlines, including MAttNet (Yu et al., 2018a),\nDDPN (Yu et al., 2018b), VL-T5 (Cho et al., 2021),\nViLBERT (Lu et al., 2019), UNITER (Chen et al.,\n2020), VL-BERT (Su et al., 2020), VinVL (Zhang\net al., 2021), VILLA (Gan et al., 2020), ERNIE-\nViL (Yu et al., 2021), MDETR (Kamath et al.,\n2021), and ALBEF (Li et al., 2021). We also\ncompare with two concurrent works that achieve\ncompetitive performance on visual grounding\ntasks, including UniTAB (Yang et al., 2022) and\nOFA (Wang et al., 2022a). We adopt accuracy@0.5\nas the evaluation metrics, where an expression is\nconsidered correctly grounded if the intersection\nover union between the top prediction and ground\ntruth is greater than 0.5\nB.2 Phrase Grounding\nDatasets. Flickr30k entities dataset (Plummer\net al., 2015) is collected through annotating 276K\nentities in the 158K captions from Flickr30k with\nobject bounding boxes. The dataset is split into\ntrain, validation and test sets, with 148,915, 14,433,\n14,481 noun phrases respectively.\nPrompt Tuning. We tune the model with learn-\ning rate 1e-5, weight decay 0.02, and batchsize 128\nfor 10 epochs. Following previous works (Kamath\net al., 2021; Yang et al., 2022), we evaluate our\nmodel under the merged-boxes protocol, where the\nboxes of a phrase (e.g.,crowd) referring to multiple\nobjects are merged by their union. We use resolu-\ntion 512 during downstream tuning. During tuning\nand inference, our model predicts the bounding box\nof each object separately.\nBaselines. We compare with state-of-the-art\nbaselines, including DDPN (Yu et al., 2018b),\nUniTAB (Yang et al., 2022) and MDETR (Ka-\nmath et al., 2021). UniTAB (Yang et al., 2022)\nperforms multi-task fine-tuning with several down-\nstream task datasets. We adopt accuracy@0.5 as\nthe evaluation metrics.\nB.3 Visual Relation Detection\nDatasets. We use Visual Genome (Krishna et al.,\n2017) dataset for the evaluation of this task. The\ndataset is split into train, validation and test sets,\nwith 65,651, 5,000, 32,422 images respectively.\nThe of object categories and relation categories in\nthe dataset are 150 and 50 respectively.\nPrompt Tuning. We tune the model with learn-\ning rate 2e-5, weight decay 0.02, and batchsize 256\nfor 5 epochs. The resolution of images is 512. The\nratio of negative samples (i.e., no relations between\nthe object pair) and positive samples is 3:1.\nBaselines. We compare with strong baselines,\nincluding MotifNet (Zellers et al., 2018), Unbi-\nased (Tang et al., 2020), GPS-Net (Lin et al.,\n2020), MSDN (Xu et al., 2017), VCTree (Tang\net al., 2019), DT2-ACBS (Desai et al., 2021), Vi-\nsualDS (Yao et al., 2021a), and IETrans (Zhang\n11116\net al., 2022). For evaluation metrics, we adopt\nRecall@K(R@K), which is the ratio of correct re-\nlationship in the top K confident relationship pre-\ndictions, and mean Recall@K(mR@K), which is\nthe average recall upon all predicate classes.\nB.4 Visual Commonsense Reasoning\nDatasets. VCR dataset (Zellers et al., 2019) is\ncollected through creating questions requiring com-\nmonsense reasoning by workers for given images\nfrom 110K movie scenes. The dataset is split into\ntrain, validation, test sets with 212,923, 26,534,\n25,263 questions respectively.\nPrompt Tuning. We tune the model with learn-\ning rate 1e-5, weight decay 0.02, and batchsize\n4,096 for 5 epochs. We use resolution 512 in down-\nstream tuning. PEVL predicts binary labels indicat-\ning the whether candidate is correct, given the text\nof question concatenated with answer, or question,\nanswer concatenated with rationale. In Q →AR,\nwe first predict an answer from four answer candi-\ndates, and then pick a rationale from four rationale\ncandidates based on the predicted answer.\nBaselines. We compare with strong baselines, in-\ncluding R2C (Zellers et al., 2019), TAB-VCR (Lin\net al., 2019) and strong VLP models including\nVisualBERT (Li et al., 2020b), ViLBERT (Lu\net al., 2019), Unicoder-VL (Li et al., 2020a), VL-\nBERT (Su et al., 2020), B2T2 (Alberti et al., 2019)\nand UNITER (Chen et al., 2020). We report the ac-\ncuracy of predicting the answer (Q →A), rationale\n(QA → R) and both (Q → AR).\nB.5 Visual Question Answering\nDatasets. GQA dataset (Hudson and Manning,\n2019) is collected through automatically generating\nquestions and answers with functional programs\nbased on the scene graphs in Visual Genome. The\ndataset is split into train, validation, test-dev,and\ntest sets, with 14,305,356, 2,011,853, 172,174 and\n1,340,048 questions respectively.\nPrompt Tuning. We tune the model with learn-\ning rate 1e-5, weight decay 0.02, batchsize 256\nfor 5 epochs. Following MDETR (Kamath et al.,\n2021), the intermediate pre-training is conducted\non the unbalanced train set, and prompt tuning on\nthe balanced train set. The resolution of image\nis 384. We infer the answers based on the 1,853\ncandidates from Kamath et al. (2021).\nBaselines. We compare with existing methods re-\nported on the GQA balanced validation dataset,\nincluding LXMERT (Tan and Bansal, 2019),\nBAN (Kim et al., 2018), CTI (Do et al., 2019)\nand CFR (Nguyen et al., 2022).\nC Ethical Considerations\nPotential risks of this work lie in (1) privacy is-\nsues of the pre-training images and text from the\nWeb, (2) misuse of the model (e.g., visual relation\ndetection for monitoring human activity), and (3)\ntoxic model outputs. The initial version of this pa-\nper is released at https://arxiv.org/abs/\n2205.11169. The picture in Figure 1 is obtained\nfrom the RefCOCO dataset.\n11117",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8165172338485718
    },
    {
      "name": "Object (grammar)",
      "score": 0.5281736254692078
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5160403251647949
    },
    {
      "name": "Language model",
      "score": 0.508913516998291
    },
    {
      "name": "Detector",
      "score": 0.4865507185459137
    },
    {
      "name": "Position (finance)",
      "score": 0.47480976581573486
    },
    {
      "name": "Object detection",
      "score": 0.46840590238571167
    },
    {
      "name": "Phrase",
      "score": 0.4552305340766907
    },
    {
      "name": "Expression (computer science)",
      "score": 0.4427200257778168
    },
    {
      "name": "Natural language processing",
      "score": 0.3651365041732788
    },
    {
      "name": "Programming language",
      "score": 0.21019688248634338
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.13229641318321228
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Finance",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    }
  ],
  "cited_by": 33
}