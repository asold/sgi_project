{
  "title": "Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models",
  "url": "https://openalex.org/W4385567158",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5084476736",
      "name": "Mirelle Candida Bueno",
      "affiliations": [
        "Universidade Estadual de Campinas (UNICAMP)"
      ]
    },
    {
      "id": "https://openalex.org/A5013122602",
      "name": "Carlos Gemmell",
      "affiliations": [
        "University of Glasgow"
      ]
    },
    {
      "id": "https://openalex.org/A5071842569",
      "name": "Jeff Dalton",
      "affiliations": [
        "University of Glasgow"
      ]
    },
    {
      "id": "https://openalex.org/A5087970571",
      "name": "Roberto Lotufo",
      "affiliations": [
        "Universidade Estadual de Campinas (UNICAMP)"
      ]
    },
    {
      "id": "https://openalex.org/A5030647281",
      "name": "Rodrigo Nogueira",
      "affiliations": [
        "Universidade Estadual de Campinas (UNICAMP)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4287019748",
    "https://openalex.org/W4306753954",
    "https://openalex.org/W2986266667",
    "https://openalex.org/W2173051530",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3210661875",
    "https://openalex.org/W4226369848",
    "https://openalex.org/W2971094176",
    "https://openalex.org/W4385574135",
    "https://openalex.org/W3166890286",
    "https://openalex.org/W3212438831",
    "https://openalex.org/W4287755175",
    "https://openalex.org/W2951107864",
    "https://openalex.org/W3035331128",
    "https://openalex.org/W3098843277",
    "https://openalex.org/W3173274550",
    "https://openalex.org/W2970609357",
    "https://openalex.org/W2951483775",
    "https://openalex.org/W3091917188",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2548137223",
    "https://openalex.org/W1732222442",
    "https://openalex.org/W2962965465",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4225647012",
    "https://openalex.org/W3173888607",
    "https://openalex.org/W3105725479",
    "https://openalex.org/W4286986405",
    "https://openalex.org/W1966678693",
    "https://openalex.org/W4243586225",
    "https://openalex.org/W3202959906",
    "https://openalex.org/W2963143606",
    "https://openalex.org/W2939413764",
    "https://openalex.org/W2971079754",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W3201339301",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W4212865381",
    "https://openalex.org/W2981037730",
    "https://openalex.org/W3035206215",
    "https://openalex.org/W4303441863",
    "https://openalex.org/W4310625358",
    "https://openalex.org/W4297669644",
    "https://openalex.org/W4224864773",
    "https://openalex.org/W4385572831",
    "https://openalex.org/W3035428952",
    "https://openalex.org/W4297730339",
    "https://openalex.org/W4301259831",
    "https://openalex.org/W3034811314",
    "https://openalex.org/W3043172396",
    "https://openalex.org/W4286900915",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W3133029875",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W4225300578",
    "https://openalex.org/W2951756287",
    "https://openalex.org/W2996132992",
    "https://openalex.org/W2788751659",
    "https://openalex.org/W2012036715"
  ],
  "abstract": "The ability to extrapolate, i.e., to make predictions on sequences that are longer than those presented as training examples, is a challenging problem for current deep learning models. Recent work shows that this limitation persists in state-of-the-art Transformer-based models. Most solutions to this problem use specific architectures or training methods that do not generalize to other tasks. We demonstrate that large language models can succeed in extrapolation without modifying their architecture or training procedure. Our experimental results show that generating step-by-step rationales and introducing marker tokens are both required for effective extrapolation. First, we induce a language model to produce step-by-step rationales before outputting the answer to effectively communicate the task to the model. However, as sequences become longer, we find that current models struggle to keep track of token positions. To address this issue, we interleave output tokens with markup tokens that act as explicit positional and counting symbols. Our findings show how these two complementary approaches enable remarkable sequence extrapolation and highlight a limitation of current architectures to effectively generalize without explicit surface form guidance. Code available at https://anonymous.4open.science/r/induced-rationales-markup-tokens-0650/README.md",
  "full_text": "Proceedings of the 1st Workshop on Mathematical Natural Language Processing (MathNLP), pages 17 - 24\nDecember 8, 2022 ©2022 Association for Computational Linguistics\nInduced Natural Language Rationales and Interleaved Markup Tokens\nEnable Extrapolation in Large Language Models\nMirelle Bueno\nUniversity of Campinas\nCarlos Gemmell\nUniversity of Glasgow\nJeffrey Dalton\nUniversity of Glasgow\nRoberto Lotufo\nUniversity of Campinas\nNeuralMind\nRodrigo Nogueira\nUniversity of Campinas\nNeuralMind\nAbstract\nThe ability to extrapolate, i.e., to make predic-\ntions on sequences that are longer than those\npresented as training examples, is a challeng-\ning problem for current deep learning mod-\nels. Recent work shows that this limitation\npersists in state-of-the-art Transformer-based\nmodels. Most solutions to this problem use\nspecific architectures or training methods that\ndo not generalize to other tasks. We demon-\nstrate that large language models can succeed\nin extrapolation without modifying their archi-\ntecture or training procedure. Our experimental\nresults show that generating step-by-step ratio-\nnales and introducing marker tokens are both\nrequired for effective extrapolation. First, we\ninduce a language model to produce step-by-\nstep rationales before outputting the answer to\neffectively communicate the task to the model.\nHowever, as sequences become longer, we find\nthat current models struggle to keep track of\ntoken positions. To address this issue, we inter-\nleave output tokens with markup tokens that\nact as explicit positional and counting sym-\nbols. Our findings show how these two com-\nplementary approaches enable remarkable se-\nquence extrapolation and highlight a limitation\nof current architectures to effectively generalize\nwithout explicit surface form guidance. Code\navailable athttps://github.com/MirelleB/\ninduced-rationales-markup-tokens\n1 Introduction\nThe lack of compositional generalization of neu-\nral networks has been a long-standing limitation\nknown for decades (Fodor and Pylyshyn, 1988;\nSchmidhuber, 1990; Marcus, 1998, 2018; Lake and\nBaroni, 2018; Liška et al., 2018; Keysers et al.,\n2019). This is often associated with their fail-\nure to extrapolate, i.e., the ability to work on se-\nquences that are longer than those presented as\ntraining examples. Modern architectures such as\nthe Transformer (Vaswani et al., 2017), which is\nthe core component of state-of-the-art NLP models,\nGPT-3 Finetuned\non thousands of examples\n\"run around left thrice\"\nAnswer: LEFT RUN 1 LEFT RUN 2 LEFT \nRUN 3 LEFT RUN 4 LEFT RUN 5 LEFT \nRUN 6 LEFT RUN 7 LEFT RUN 8 LEFT \nRUN 9 LEFT RUN 10 LEFT RUN 11\n        (missing LEFT RUN 12)\nFrozen GPT-3 \nA few in-context examples with\nexplanations and markup tokens\n+\n\"run around left thrice\"\nExplanation: \"run around left\" corresponds to 4 \nLEFT RUN commands. Because of the word \n\"thrice\", \"run around left thrice\" results in 3 x 4 \n= 12 LEFT RUN commands.\nAnswer: LEFT RUN 1 LEFT RUN 2 LEFT RUN 3 \nLEFT RUN 4 LEFT RUN 5 LEFT RUN 6 LEFT RUN \n7 LEFT RUN 8 LEFT RUN 9 LEFT RUN 10 LEFT \nRUN 11 LEFT RUN 12\n(a) (b)\nFigure 1: Answers produced by a GPT-3 model on the\n“length” split of the SCAN dataset when (a) fine-tuned\non thousands of examples vs (b) induced via a few in-\ncontext examples to generate explanations and markup\ntokens (in yellow).\nperform poorly on this class of problems (Bhat-\ntamishra et al., 2020; Nogueira et al., 2021; Wang\net al., 2021; Pal and Baral, 2021; Welleck et al.,\n2021; Bogin et al., 2022; Finlayson et al., 2022;\nMittal et al., 2021). In Figure 1-(a), we illustrate\nhow recent large language models such as GPT-3\nfail at this task, even when fine-tuned on thousands\nof examples.\nArchitectures and training methods that tar-\nget this specific problem are often developed\nbased on synthetic tasks whose creation rules are\nknown (Das et al., 1992; Li et al., 2019b; Russin\net al., 2019; Andreas, 2020; Liu et al., 2020a; Chen\net al., 2020; Herzig and Berant, 2021; Shaw et al.,\n2021; Zhu et al., 2021). Thus, they resort to tech-\nniques such as augmenting the training data or bias-\ning the model’s architecture to internally represent\nthese rules. However, improvements obtained on\none compositional generalization benchmark do\nnot transfer to others (Furrer et al., 2020), i.e., they\nlose their ability to be used as competitive general-\npurpose models in real tasks, as these can seldom\nbe solved with a small set of rules.\nWe study the behavior of Transformer models\n17\nand demonstrate that this problem is not due to\nan intrinsic limitation of their training algorithm.\nWe show that inducing autoregressive models to\nrationalize before making a prediction (Wang et al.,\n2022; Zelikman et al., 2022) is not enough to ex-\ntrapolate on long sequences: to solve it, we intro-\nduce markup tokens (Nogueira et al., 2021; Kim\net al., 2021). The two general approaches together\nallow the models to achieve remarkable extrapo-\nlation generalization without requiring changes to\nthe model or architecture. These findings provide\nevidence that general-purpose models have the abil-\nity to both improve their effectiveness and inter-\npretability at the same time. The need to markup\ntokens also suggests there are fundamental issues\nthat need to be addressed in the Transformer archi-\ntecture, particularly the need for better positional\nrepresentations. Thus, our study confirms and sup-\nports recent results from previous work that posi-\ntional embeddings used in current state-of-the-art\nTransformer models cannot precisely track of token\npositions or perform precise counting (Liu et al.,\n2020b; Thawani et al., 2021; Press et al., 2022).\n2 Related Work\nA long list of architectures and training methods\nattempt to improve the extrapolation capabilities\nof deep learning models. For instance, some are\nspecifically designed to solve only a handful of\ntasks (Singh, 1992; Kaiser and Sutskever, 2015;\nKalchbrenner et al., 2015; Price et al., 2016; An-\ndreas et al., 2016, 2017; Trask et al., 2018). Pre-\ntrained word embeddings find it difficult to extrap-\nolate to unseen numbers in training (Wallace et al.,\n2019). Alternatives to improving the extrapola-\ntion ability of neural models include building neu-\nral models with a pre-training corpus of numeri-\ncal text (Geva et al., 2020) or using scientific no-\ntation to represent numbers (Zhang et al., 2020).\nLikewise, better numerical and compositional skills\ncan be achieved by supplementing input texts with\npre-computed numerical calculations (Andor et al.,\n2019) or explicitly assuming rules or mathematical\nequations from natural language texts (Liu et al.,\n2019; Li et al., 2019a; Zou and Lu, 2019a,b; Shi,\n2020; Qiu et al., 2021). Many of these models are\ncapable of adding numbers larger than those seen\nduring training. In contrast, more general-purpose\narchitectures fail to extrapolate on numerical tasks\n(Joulin and Mikolov, 2015; Dehghani et al., 2018;\nSchlag et al., 2019).\nOur work derives from recent findings that show\nthat inducing the model to generate explanations\nin natural language leads to better performance in\na wide variety of tasks (Recchia, 2021; Fernandes\net al., 2022; Wang et al., 2022; Zelikman et al.,\n2022; Nye et al., 2022; Katz et al., 2022; Zhou\net al., 2022; Khot et al., 2022). In particular, the\nwork proposed by (Zhou et al., 2022) achieves\nstate-of-the-art results in the extrapolation of tasks\ninvolving symbolic manipulation, compositional\ngeneralization and numerical reasoning. Tasks are\nsolved via few-shot learning applied to a large lan-\nguage model (e.g. text-davinci-002) in two main\nsteps. The first step consists of reducing the ques-\ntion into sub-questions, then, in the second phase, a\nnew interaction is made with the model, now solv-\ning sequentially the sub-questions generated in the\nprevious step.\nThe results shown in Zhou et al. (Zhou et al.,\n2022) corroborate our intuition that explanations\nalone are not enough to achieve extrapolations. By\ninducing the model to generate explanations and\nmarkup tokens, we provide evidence that compo-\nsitional generalization can be achieved without\nsacrificing the general applicability on other tasks,\nwhich is often a feature that is lost with architec-\ntural modifications.\nHowever, a limitation of Zhou et al.’s and our\nmethod is that both require a programmatic post-\nprocessing step: Zhou et al. use a python script\nto convert the model output (e.g., 3*[\"LEFT\"]),\nwhich is in python notation, into the expected for-\nmat of the final answer (e.g., LEFT LEFT LEFT );\nin our method, we programmatically remove the\nmarkup tokens from the final answer. We argue\nthat the need to call an external script exposes a\nlimitation in the current Transformer architecture,\nnamely, that it cannot handle long sequences of\nrepeated tokens.\n3 Methodology\nIn this section, we describe our proposed method\nfor inducing explanations and markup tokens us-\ning in-context learning with a few examples. We\nfirst create a prompt ic||oc that concatenates in-\ncontext training examples ic with a test exam-\nple oc. The ic examples consist of N triples\nof “Instruction”, “Explanation” and “Output”, i.e.,\nic = {(i∗\n1, e∗\n1, o∗\n1), ...,(i∗\nN, e∗\nN, o∗\nN)}. The test ex-\nample oc is made of only the “Instruction” field.\nWhen we feed ic||oc to a language model, it should\n18\nExample 1: Instruction: add the numbers E 4 D 1 C 6 B 5 A 4 and D 1 C 8 B 5 A 3 \nExplanation:This first number goes until position E, whereas the second goes until position D. The first number is the longest, so we sumdigits in positions A, B, C, D, E. \nWe sum the digits in position A, which are A 4 and A 3. That is 4 + 3, which equals to 7. Because 7 is less than or equal to 9,this results in carry 0 and A 7. \nThis carry 0 is summed with digits in position B, which are B 5 and B 5. That is 0 + 5 + 5, which equals to 10. Because 1 0 isgreater than 9, this results in carry 1 and B 0.\nThis carry 1 is summed with digits in position C, which are C 6 and C 8. That is 1 + 6 + 8, which equals to 15. Because 1 5 isgreater than 9, this results in carry 1 and C 5.\nThis carry 1 is summed with digits in position D, which are D 1 and D 1 .That is 1 + 1 + 1, which equals to 3. Because 3 isless or equal than 9, this results in carry 0 and D 3.\nThis carry 0 is summed with digits in position E, which are E 4 and E 0. That is 0 + 4 + 0, which equals to 4. Because 4 isless than or equal to 9, this results in carry 0 and E 4.\nOutput: E 4 D 3 C 5 B 0 A 7\n(...)\nExample 6: \nInstruction: add the numbers D7 C7 B8 A5 and D1 C7 B1 A7 \nInput\nExplanation:\nThis first number goes until position D, whereas the second goes until position D. Both numbers have equal length, so we\nsum digits in positions A, B, C, D.\nWe sum the digits in position A, which are A 5 and A 7. That is 5 + 7, which equals to 12. Because 1 2 is greater than 9, this\nresults in carry 1 and A 2. \nThis carry 1 is summed with digits in position B, which are B 8 and B 1. That is 1 + 8 + 1, which equals to 10. Because 1 0 is\ngreater than 9, this results in carry 1 and B 0. \nThis carry 1 is summed with digits in position C, which are C 7 and C 7. That is 1 + 7 + 7, which equals to 15. Because 1 5 is\ngreater than 9, this results in carry 1 and C 5. \nThis carry 1 is summed with digits in position D, which are D 7 and D 1. That is 1 + 7 + 1, which equals to 9. Because 9 is\nless than or equal to 9, this results in carry 0 and D 9. \nOutput: D 9 C 5 B 0 A 2\nModel output\nFigure 2: Example of a few-shot prompt and model com-\npletion for the addition task. First, a prompt composed\nof in-context (ic) samples are given, which are formed\nby {input, explanation, output} triplets concatenated\nwith an out-of-context (oc) test example that has only\nthe \"instruction\" field. The model then completes the\n“explanation” and “output” fields from the test example\nas a result.\ngenerate the remaining “Explanation” and “Output”\nfields for oc. Figure 2 illustrates the input prompt\ngiven to the model and the (correct) output given\nby the model.\nWe also interleave the tokens ic and oc with\nmarkup tokens that help the model to precisely\nidentify the tokens in the input and output se-\nquences (see Figure 1-(b) for an example). These\ntokens support the model in three ways: 1) They act\nas a form of working memory to indicate progress\nbeing made. 2) They act as sub-prompt anchors to\ninform the start of a known pattern. 3) They im-\nplicitly model a stopping condition should a certain\namount of progress be reached. We programmati-\ncally include these markups in each test input and\nremove them from the output answers before com-\nparing them with ground-truth ones.\nDue to its few-shot nature, our method can be\nadjusted for different tasks. Likewise, our approach\ndoes not require any additional modifications to the\nlanguage model such as pretraining or changes to\nthe loss function.\n4 Experimental Setup\nWe evaluate our method in two tasks that require\nextrapolation: 1) the length split of the SCAN\ndata (Lake and Baroni, 2018) and 2) the addition\nof two numbers. In all experiments, we used the\ntext-davinci-002 model, available via a paid\nAPI provided by OpenAI. We report the accuracy\nof the test set.\n4.1 SCAN\nThe SCAN synthetic dataset translates simple navi-\ngation commands into a sequence of actions (e.g.,\nthe input jump thrice results in the output JUMP\nJUMP JUMP). These commands are generated from\nthe composition of a specific grammar, combining\n“primitive” commands such asjump, walk, look,\nrun and turn; “modifiers” (left, right, around,\nopposite); repetition symbols like twice/thrice;\n“combiners” (and/after) that group two action se-\nquences.\nTo construct the prompt, we generated nine in-\ncontext training examples, each made of three parts:\nan instruction, an explanation, and the desired out-\nput. The “Instruction” is a sequence of commands\nwhile the “Explanation” is a description, in natural\nlanguage detailing the steps to generate the output.\nThe “Output” corresponds to the expected answer\nto the instruction. In addition, in the output field,\nwe inject markup tokens to delimit the end of a\nrepeating sequence or sub-instruction. Therefore,\nto indicate each repetition of a given action, we\nuse positive integers and at the end of a sequence\nof actions, we use the separator ||. For example,\nfor the input: jump twice and walk twice, we\ngenerate the output JUMP 1 JUMP 2 || WALK 1\nWALK 2.\nThe target outputs of training examples have up\nto 22 actions. The test examples were drawn from\nthe “Length” split provided by the authors.* This\nset has 3,920 examples whose target output varies\nbetween 24 and 48 actions. The instruction (input)\nof each test example is appended to the in-context\ntraining examples and the model is prompted to\ngenerate the “Explanation” and “Output” fields.\nThus, since training examples are shorter than test\nones, we are able to assess the compositional gener-\nalization of the model while extrapolating to larger\nunseen sequences. Due to the cost of using the\nGPT-3 API (approximately 0.10 USD per example),\n*https://github.com/brendenlake/SCAN\n19\nMethod Acc.\nSpecialized Architectures\nSyntactic Attn. (Russin et al., 2019) 15.2\nCGPS (Li et al., 2019b) 20.3\nT5-base DUEL (Zhu et al., 2021) 45.0\nLANE (Liu et al., 2020a) 100.0\nNSSM (Chen et al., 2020) 100.0\nSBSP (Herzig and Berant, 2021) 100.0\nNQG (Shaw et al., 2021) 100.0\nSynth (Nye et al., 2020) 100.0\nGeneral-purpose Architectures\nT5-base (Furrer et al., 2020) 14.4\nT5-Large (Furrer et al., 2020) 5.2\nT5-3B (Furrer et al., 2020) 3.3\nT5-11B (Furrer et al., 2020) 2.0\nGPT-3 Ada - fine-tuned 13.9\nGPT-3 Curie - fine-tuned 6.4\nGPT-3 Davinci - fine-tuned 8.2\nLeast-to-Most (Zhou et al., 2022) 99.7\n—\nOurs (rationales only) 2.5\nOurs (markups only) 22.5\nOurs (rationales + markups, inverted prompt) 30.0\nOurs (rationales + markups) 95.2\nTable 1: Results on the “length” split of the SCAN\ndataset.\nwe evaluated the model on 400 randomly sampled\nexamples from the test set.\n4.2 Addition Task\nExtrapolation abilities can also be tested with arith-\nmetic tasks. For this, we built a prompt for the ad-\ndition operation, where we present five in-context\ntraining examples with two numbers up to 5 digits\nand ask the model to generate the explanation and\nanswer for a test set example made of numbers with\n4 to 14 digits. We evaluate the model on 400 test\nsamples automatically generated by the “balanced\nsampling” method from Nogueira et al. (Nogueira\net al., 2021), which ensures that the set will have a\nroughly equal proportion of answers with d-digit\nnumbers, with d ∈ [4, 14].\nWe use a template similar to SCAN’s to feed the\nin-context examples to the model. We manually\ngenerate the explanations for the training examples\nand inject markup tokens in the instructions and the\ntarget output. In the expected output, these tokens\nare used during the explanation steps. We illustrate\nin Figure 2 an example of a prompt followed by a\ncompletion of the model.\n4 5 6 7 8 9 10 11 12 13 140\n20\n40\n60\n80\n100\nDigits in the ground-truth answer\nTest Acurracy (%)\nT5 + 100k examples\nOurs, rationale+markup\nOurs, markup-only\nOurs, rationale-only\nFigure 3: Test set accuracy in the addition task vs num-\nber of digits in the ground-truth answer.\n5 Results\nIn Table 1 we show the results for the length split of\nthe SCAN dataset. We see that specialized models\nlike LANE, NSSM, and SBSP solve the composi-\ntional generalization proposed by SCAN, whereas\ngeneric architectures such as T5 (Raffel et al., 2020)\nor GPT-3 (Brown et al., 2020) fine-tuned on the task\nhave poor performance.\nWe also show results for GPT-3’s Ada (300M\nparameters), Curie (6B parameters) and Davinci\n(175B parameters) models fine-tuned on all 16,990\ntraining examples of the SCAN dataset for 3\nepochs. In these cases, we do not use in-context\nexamples, explanations, or markup tokens. Our\nmethodology of providing prompts with detailed\nexplanations was shown to be more effective than\nfinetuning on thousands of examples.\nThe same behavior is also observed in the ad-\ndition task, as seen in Figure 3. Our approach\nwith explanations and markup tokens (rationale +\nmarkup) shows that even with as few as 5 examples,\nthe model can perform the task of adding numbers\nwith more than 5 digits, reaching a performance of\naround 60% in numbers with up to 14 digits and\nan average accuracy of 73% considering all 400\nexamples in the test set.\nWe also investigated the performance of fine-\ntuning a general-purpose model on this task. We\ntrained a T5-base with 100K samples on numbers\nwith 2 to 5 digits per 10 epochs without adding\nexplanations. We observe that the model reaches\n100% accuracy with numbers of up to 5 digits, but\nfails to add numbers with more than 6 digits.\n20\nInstruction : look left thrice and run around left thrice\nMarkup-only \nOutput:  LEFT LOOK 1 LEFT LOOK 2 LEFT LOOK 3 || LEFT RUN 1 LEFT RUN 2 LEFT RUN 3\nLEFT RUN 4 LEFT RUN 5 LEFT RUN 6 LEFT RUN 7 LEFT RUN 8 LEFT RUN 9 LEFT RUN 10\nLEFT RUN 11 LEFT RUN 12\nRationale-only \nExplanation:  Because of the word \"and\", the instruction can be divided in two sub-\ninstructions: 1) \"look left thrice\" and 2) \"run around left thrice\" \n1) \"look left thrice\" results in 3 LEFT LOOK commands.  \n2) \"run around left thrice\" results in 3 x 4 = 12 LEFT RUN commands.\nOutput:  LEFT LOOK LEFT LOOK LEFT LOOK LEFT RUN LEFT RUN LEFT RUN LEFT RUN\nLEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN\nLEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN\nLEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN\nLEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN\nLEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN\nLEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN\nLEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN\nLEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN\nLEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN\nLEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT RUN\nLEFT RUN LEFT RUN LEFT RUN LEFT RUN LEFT\nFigure 4: Model output differences between the markup-\nonly and rationale-only approaches.\n5.1 Ablation: Rationale-only vs. markup-only\nWe also investigate the impact of using explanation\nand markup tokens in isolation. We compare two\nscenarios: prompts without explanation (markups-\nonly) and without markup tokens (rationales-only).\nIn Table 1, we see that the rationale-only and\nmarkup-only approaches have significantly lower\ntest accuracy, demonstrating that it is not enough to\nexplain how to solve the task, but it is also impor-\ntant to inject markup tokens. We believe that these\ntokens help the model generate repeated sequences\nof tokens.\nIn Figure 4, we provide qualitative evidence of\nthis hypothesis: Without markup tokens, the model\ncorrectly generates the explanation but fails to fin-\nish the action sequence, therefore entering a loop.\n5.2 Ablation: Inverted prompt\nWe also experimented with reversing the order in\nwhich the \"explanations\" and \"outputs\" fields are\npresented to the model. Therefore we provide the\nexpected output first and then the explanation. The\nidea of this experiment was to verify if the order\nexplanation followed by the output has an impact\non the generation of the answer. In Table 1 we\nsee that the performance drops from 95.2 to 30%\n(rationales + markups - inverted prompt). This\nempirical result agrees with the literature in terms\nthat the model possibly processes the explanation\nbefore determining the final output.\n6 Conclusion\nIn this work, we show how step-by-step ratio-\nnales and positional markup tokens enable general-\npurpose architectures to extrapolate to sequences\nthat are significantly longer than those provided\nas training examples. Rationales before the an-\nswer break down the problem into small exe-\ncutable chunks and markup tokens track the work-\ning progress as the output is generated. Importantly,\nwe show how these methods are complementary\nand, when used together, enable remarkable extrap-\nolation results on two synthetic tasks.\nHowever, we note the use of markup tokens as\na limitation of current models and subword tok-\nenizers. Future models should be able to count\ntokens and keep track of individual tokens in long\nsequences without resorting to additional support-\ning tokens. As our qualitative analysis shows, most\nfailure cases are due to one or two tokens generated\nincorrectly. We see the ability to automatically ver-\nify these errors, as proposed by Cobbe et al. (Cobbe\net al., 2021), as a promising direction to improve\nthe extrapolation capabilities of current models.\nAcknowledgments\nThis research was partially funded by grants\n2020/09753-5 and 2022/01640-2 from Fundação\nde Amparo à Pesquisa do Estado de São Paulo\n(FAPESP). This work is also supported by grant\nEP/V025708/1 from the Engineering and Physical\nSciences Research Council. We also thank Google\nCloud for credits to support this work. R Lotufo\nis partially supported by CNPq (The Brazilian Na-\ntional Council for Scientific and Technological De-\nvelopment) under grant 310828/2018-0.\nReferences\nDaniel Andor, Luheng He, Kenton Lee, and Emily Pitler.\n2019. Giving BERT a calculator: Finding opera-\ntions and arguments with reading comprehension. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 5949–5954.\nJacob Andreas. 2020. Good-enough compositional data\naugmentation. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 7556–7566, Online. Association for\nComputational Linguistics.\nJacob Andreas, Dan Klein, and Sergey Levine. 2017.\nModular multitask reinforcement learning with pol-\n21\nicy sketches. In International Conference on Ma-\nchine Learning, pages 166–175. PMLR.\nJacob Andreas, Marcus Rohrbach, Trevor Darrell, and\nDan Klein. 2016. Learning to compose neural net-\nworks for question answering. In Proceedings of\nthe 2016 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1545–1554.\nSatwik Bhattamishra, Kabir Ahuja, and Navin Goyal.\n2020. On the ability and limitations of transformers\nto recognize formal languages. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 7096–7116.\nBen Bogin, Shivanshu Gupta, and Jonathan Berant.\n2022. Unobserved local structures make com-\npositional generalization hard. arXiv preprint\narXiv:2201.05899.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nXinyun Chen, Chen Liang, Adams Wei Yu, Dawn Song,\nand Denny Zhou. 2020. Compositional generaliza-\ntion via neural-symbolic stack machines. Advances\nin Neural Information Processing Systems, 33:1690–\n1701.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavar-\nian, Jacob Hilton, Reiichiro Nakano, Christopher\nHesse, and John Schulman. 2021. Training veri-\nfiers to solve math word problems. arXiv preprint\narXiv:2110.14168.\nSreerupa Das, C. Giles, and Gordon Sun. 1992. Learn-\ning context-free grammars: Capabilities and limita-\ntions of a recurrent neural network with an external\nstack memory.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Lukasz Kaiser. 2018. Universal\ntransformers. In International Conference on Learn-\ning Representations.\nPatrick Fernandes, Marcos Treviso, Danish Pruthi, An-\ndré FT Martins, and Graham Neubig. 2022. Learning\nto scaffold: Optimizing model explanations for teach-\ning. arXiv preprint arXiv:2204.10810.\nMatthew Finlayson, Kyle Richardson, Ashish Sabhar-\nwal, and Peter Clark. 2022. What makes instruc-\ntion learning hard? an investigation and a new chal-\nlenge in a synthetic environment. arXiv preprint\narXiv:2204.09148.\nJerry A Fodor and Zenon W Pylyshyn. 1988. Connec-\ntionism and cognitive architecture: A critical analysis.\nCognition, 28(1-2):3–71.\nDaniel Furrer, Marc van Zee, Nathan Scales, and\nNathanael Schärli. 2020. Compositional generaliza-\ntion in semantic parsing: Pre-training vs. specialized\narchitectures. arXiv preprint arXiv:2007.08970.\nMor Geva, Ankit Gupta, and Jonathan Berant. 2020.\nInjecting numerical reasoning skills into language\nmodels. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 946–958.\nJonathan Herzig and Jonathan Berant. 2021. Span-\nbased semantic parsing for compositional general-\nization. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 908–921, Online. Association for Computa-\ntional Linguistics.\nArmand Joulin and Tomas Mikolov. 2015. Inferring\nalgorithmic patterns with stack-augmented recurrent\nnets. Advances in Neural Information Processing\nSystems, 28:190–198.\nŁukasz Kaiser and Ilya Sutskever. 2015. Neural GPUs\nlearn algorithms. arXiv preprint arXiv:1511.08228.\nNal Kalchbrenner, Ivo Danihelka, and Alex Graves.\n2015. Grid long short-term memory. arXiv preprint\narXiv:1507.01526.\nUri Katz, Mor Geva, and Jonathan Berant. 2022. Infer-\nring implicit relations with language models. arXiv\npreprint arXiv:2204.13778.\nDaniel Keysers, Nathanael Schärli, Nathan Scales,\nHylke Buisman, Daniel Furrer, Sergii Kashubin,\nNikola Momchev, Danila Sinopalnikov, Lukasz\nStafiniak, Tibor Tihon, et al. 2019. Measuring com-\npositional generalization: A comprehensive method\non realistic data. In International Conference on\nLearning Representations.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao\nFu, Kyle Richardson, Peter Clark, and Ashish Sab-\nharwal. 2022. Decomposed prompting: A modular\napproach for solving complex tasks. arXiv preprint\narXiv:2210.02406.\nJeonghwan Kim, Giwon Hong, Kyung-min Kim, Junmo\nKang, and Sung-Hyon Myaeng. 2021. Have you seen\nthat number? investigating extrapolation in question\nanswering models. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 7031–7037.\nBrenden Lake and Marco Baroni. 2018. Generalization\nwithout systematicity: On the compositional skills\nof sequence-to-sequence recurrent networks. In In-\nternational conference on machine learning, pages\n2873–2882. PMLR.\nJierui Li, Lei Wang, Jipeng Zhang, Yan Wang, Bing Tian\nDai, and Dongxiang Zhang. 2019a. Modeling intra-\nrelation in math word problems with different func-\ntional multi-head attentions. In Proceedings of the\n22\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 6162–6167.\nYuanpeng Li, Liang Zhao, Jianyu Wang, and Joel Hest-\nness. 2019b. Compositional generalization for primi-\ntive substitutions. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 4293–4302, Hong Kong, China. Association\nfor Computational Linguistics.\nAdam Liška, Germán Kruszewski, and Marco Baroni.\n2018. Memorize or generalize? searching for a\ncompositional rnn in a haystack. arXiv preprint\narXiv:1802.06467.\nQian Liu, Shengnan An, Jian-Guang Lou, Bei Chen,\nZeqi Lin, Yan Gao, Bin Zhou, Nanning Zheng, and\nDongmei Zhang. 2020a. Compositional generaliza-\ntion by learning analytical expressions. Advances in\nNeural Information Processing Systems, 33:11416–\n11427.\nQianying Liu, Wenyv Guan, Sujian Li, and Daisuke\nKawahara. 2019. Tree-structured decoding for solv-\ning math word problems. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2370–2379.\nXuanqing Liu, Hsiang-Fu Yu, Inderjit Dhillon, and Cho-\nJui Hsieh. 2020b. Learning to encode position for\ntransformer with continuous dynamical model. In In-\nternational Conference on Machine Learning, pages\n6327–6335. PMLR.\nGary Marcus. 2018. Deep learning: A critical appraisal.\narXiv preprint arXiv:1801.00631.\nGary F Marcus. 1998. Rethinking eliminative connec-\ntionism. Cognitive psychology, 37(3):243–282.\nSarthak Mittal, Sharath Chandra Raparthy, Irina Rish,\nYoshua Bengio, and Guillaume Lajoie. 2021. Compo-\nsitional attention: Disentangling search and retrieval.\nCoRR, abs/2110.09419.\nRodrigo Nogueira, Zhiying Jiang, and Jimmy Lin.\n2021. Investigating the limitations of transform-\ners with simple arithmetic tasks. arXiv preprint\narXiv:2102.13019.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, Charles Sutton, and Augustus Odena.\n2022. Show your work: Scratchpads for interme-\ndiate computation with language models. In Deep\nLearning for Code Workshop.\nMaxwell Nye, Armando Solar-Lezama, Josh Tenen-\nbaum, and Brenden M Lake. 2020. Learning compo-\nsitional rules via neural program synthesis. Advances\nin Neural Information Processing Systems, 33:10832–\n10842.\nKuntal Kumar Pal and Chitta Baral. 2021. Investigating\nnumeracy learning ability of a text-to-text transfer\nmodel. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021, pages 3095–3101.\nOfir Press, Noah Smith, and Mike Lewis. 2022. Train\nshort, test long: Attention with linear biases enables\ninput length extrapolation. In International Confer-\nence on Learning Representations.\nEric Price, Wojciech Zaremba, and Ilya Sutskever. 2016.\nExtensions and limitations of the neural GPU. arXiv\npreprint arXiv:1611.00736.\nLinlu Qiu, Peter Shaw, Panupong Pasupat,\nPaweł Krzysztof Nowak, Tal Linzen, Fei Sha,\nand Kristina Toutanova. 2021. Improving composi-\ntional generalization with latent structure and data\naugmentation. arXiv preprint arXiv:2112.07610.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research, 21:1–\n67.\nGabriel Recchia. 2021. Teaching autoregressive lan-\nguage models complex tasks by demonstration.\narXiv preprint arXiv:2109.02102.\nJake Russin, Jason Jo, Randall C O’Reilly, and Yoshua\nBengio. 2019. Compositional generalization in a\ndeep seq2seq model by separating syntax and seman-\ntics. arXiv preprint arXiv:1904.09708.\nImanol Schlag, Paul Smolensky, Roland Fernandez,\nNebojsa Jojic, Jürgen Schmidhuber, and Jianfeng\nGao. 2019. Enhancing the transformer with explicit\nrelational encoding for math problem solving. arXiv\npreprint arXiv:1910.06611.\nJürgen Schmidhuber. 1990. Towards compositional\nlearning in dynamic networks.\nPeter Shaw, Ming-Wei Chang, Panupong Pasupat, and\nKristina Toutanova. 2021. Compositional generaliza-\ntion and natural language variation: Can a semantic\nparsing approach handle both? In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 922–938.\nHongjie Shi. 2020. A sequence-to-sequence approach\nfor numerical slot-filling dialog systems. In Pro-\nceedings of the 21th Annual Meeting of the Special\nInterest Group on Discourse and Dialogue , pages\n272–277.\nSatinder Pal Singh. 1992. Transfer of learning by com-\nposing solutions of elemental sequential tasks. Ma-\nchine learning, 8(3):323–339.\n23\nAvijit Thawani, Jay Pujara, Filip Ilievski, and Pedro\nSzekely. 2021. Representing numbers in NLP: a\nsurvey and a vision. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 644–656, Online. As-\nsociation for Computational Linguistics.\nAndrew Trask, Felix Hill, Scott E. Reed, Jack Rae, Chris\nDyer, and Phil Blunsom. 2018. Neural arithmetic\nlogic units. In Advances in Neural Information Pro-\ncessing Systems, pages 8035–8044.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nEric Wallace, Yizhong Wang, Sujian Li, Sameer Singh,\nand Matt Gardner. 2019. Do NLP models know\nnumbers? Probing numeracy in embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 5310–5318.\nCunxiang Wang, Boyuan Zheng, Yuchen Niu, and Yue\nZhang. 2021. Exploring generalization ability of pre-\ntrained language models on arithmetic and logical\nreasoning. In CCF International Conference on Nat-\nural Language Processing and Chinese Computing,\npages 758–769. Springer.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022. Self-consistency im-\nproves chain of thought reasoning in language mod-\nels. arXiv preprint arXiv:2203.11171.\nSean Welleck, Peter West, Jize Cao, and Yejin Choi.\n2021. Symbolic brittleness in sequence models: on\nsystematic generalization in symbolic mathematics.\narXiv preprint arXiv:2109.13986.\nEric Zelikman, Yuhuai Wu, and Noah D Goodman.\n2022. Star: Bootstrapping reasoning with reason-\ning. arXiv preprint arXiv:2203.14465.\nXikun Zhang, Deepak Ramachandran, Ian Tenney,\nYanai Elazar, and Dan Roth. 2020. Do language\nembeddings capture scales? In Proceedings of the\nThird BlackboxNLP Workshop on Analyzing and In-\nterpreting Neural Networks for NLP, pages 292–299.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\nLeast-to-most prompting enables complex reason-\ning in large language models. arXiv preprint\narXiv:2205.10625.\nWang Zhu, Peter Shaw, Tal Linzen, and Fei Sha. 2021.\nLearning to generalize compositionally by transfer-\nring across semantic parsing tasks. arXiv preprint\narXiv:2111.05013.\nYanyan Zou and Wei Lu. 2019a. Quantity tagger: A\nlatent-variable sequence labeling approach to solving\naddition-subtraction word problems. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 5246–5251.\nYanyan Zou and Wei Lu. 2019b. Text2Math: End-to-\nend parsing text into math expressions. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 5330–5340.\n24",
  "topic": "Markup language",
  "concepts": [
    {
      "name": "Markup language",
      "score": 0.8843500018119812
    },
    {
      "name": "Computer science",
      "score": 0.8305699825286865
    },
    {
      "name": "Security token",
      "score": 0.7649741172790527
    },
    {
      "name": "Extrapolation",
      "score": 0.755496621131897
    },
    {
      "name": "Language model",
      "score": 0.5933234691619873
    },
    {
      "name": "Transformer",
      "score": 0.5058493614196777
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4494676887989044
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4238399267196655
    },
    {
      "name": "Natural language processing",
      "score": 0.3916420042514801
    },
    {
      "name": "Theoretical computer science",
      "score": 0.35561734437942505
    },
    {
      "name": "Programming language",
      "score": 0.3411131501197815
    },
    {
      "name": "XML",
      "score": 0.317382276058197
    },
    {
      "name": "Computer network",
      "score": 0.08083346486091614
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I181391015",
      "name": "Universidade Estadual de Campinas (UNICAMP)",
      "country": "BR"
    },
    {
      "id": "https://openalex.org/I7882870",
      "name": "University of Glasgow",
      "country": "GB"
    }
  ]
}