{
    "title": "Fine-tuning Pre-trained Language Models for Few-shot Intent Detection: Supervised Pre-training and Isotropization",
    "url": "https://openalex.org/W4280597403",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3045070499",
            "name": "Haode Zhang",
            "affiliations": [
                "University of California, San Diego"
            ]
        },
        {
            "id": "https://openalex.org/A2291199045",
            "name": "Haowen Liang",
            "affiliations": [
                "University of California, San Diego"
            ]
        },
        {
            "id": "https://openalex.org/A2107635733",
            "name": "Yuwei Zhang",
            "affiliations": [
                "Nanyang Technological University"
            ]
        },
        {
            "id": "https://openalex.org/A3194826588",
            "name": "Li-Ming Zhan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2196274784",
            "name": "Xiao-Ming Wu",
            "affiliations": [
                "University of California, San Diego"
            ]
        },
        {
            "id": "https://openalex.org/A2249581475",
            "name": "Xiaolei Lu",
            "affiliations": [
                "University of California, San Diego",
                "Fano Labs (China)"
            ]
        },
        {
            "id": "https://openalex.org/A1985048037",
            "name": "Albert Lam",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3090702983",
        "https://openalex.org/W2518186251",
        "https://openalex.org/W3176625957",
        "https://openalex.org/W3115187525",
        "https://openalex.org/W2907252220",
        "https://openalex.org/W3156636935",
        "https://openalex.org/W2988299267",
        "https://openalex.org/W2948110372",
        "https://openalex.org/W3088068539",
        "https://openalex.org/W3158162131",
        "https://openalex.org/W3199296065",
        "https://openalex.org/W3199079601",
        "https://openalex.org/W3094476119",
        "https://openalex.org/W3091355780",
        "https://openalex.org/W3105816068",
        "https://openalex.org/W2964316912",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W1836465849",
        "https://openalex.org/W3200140725",
        "https://openalex.org/W3100519617",
        "https://openalex.org/W3139958517",
        "https://openalex.org/W2988217457",
        "https://openalex.org/W3189817881",
        "https://openalex.org/W2966087730",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2946364534",
        "https://openalex.org/W2963341924",
        "https://openalex.org/W3114186958",
        "https://openalex.org/W4234698323",
        "https://openalex.org/W3102854726",
        "https://openalex.org/W4287815000",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3125516434",
        "https://openalex.org/W3154229486",
        "https://openalex.org/W3106241909",
        "https://openalex.org/W3175362188",
        "https://openalex.org/W3205727812",
        "https://openalex.org/W2601450892",
        "https://openalex.org/W3161024824",
        "https://openalex.org/W2135046866",
        "https://openalex.org/W3022104542",
        "https://openalex.org/W3045492832",
        "https://openalex.org/W3201490839",
        "https://openalex.org/W3102573502",
        "https://openalex.org/W2966610483",
        "https://openalex.org/W2971048662",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3091806974",
        "https://openalex.org/W2996657533",
        "https://openalex.org/W3168386607",
        "https://openalex.org/W2952267213",
        "https://openalex.org/W3173077468",
        "https://openalex.org/W3104078590",
        "https://openalex.org/W2604763608"
    ],
    "abstract": "Haode Zhang, Haowen Liang, Yuwei Zhang, Li-Ming Zhan, Xiao-Ming Wu, Xiaolei Lu, Albert Lam. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
    "full_text": "Fine-tuning Pre-trained Language Models for Few-shot Intent Detection:\nSupervised Pre-training and Isotropization\nHaode Zhang1 Haowen Liang1 Yuwei Zhang2\nLiming Zhan1 Xiaolei Lu3 Albert Y.S. Lam4 Xiao-Ming Wu1∗\nDepartment of Computing, The Hong Kong Polytechnic University, Hong Kong S.A.R.1\nUniversity of California, San Diego2 Nanyang Technological University, Singapore3\nFano Labs, Hong Kong S.A.R.4\n{haode.zhang,michaelhw.liang,lmzhan.zhan}@connect.polyu.hk, zhangyuwei.work@gmail.com\nxiao-ming.wu@polyu.edu.hk, xiaolei.lu@ntu.edu.sg, albert@fano.ai\nAbstract\nIt is challenging to train a good intent classifier\nfor a task-oriented dialogue system with only\na few annotations. Recent studies have shown\nthat fine-tuning pre-trained language models\nwith a small amount of labeled utterances from\npublic benchmarks in a supervised manner\nis extremely helpful. However, we find that\nsupervised pre-training yields an anisotropic\nfeature space, which may suppress the ex-\npressive power of the semantic representa-\ntions. Inspired by recent research in isotropiza-\ntion, we propose to improve supervised pre-\ntraining by regularizing the feature space to-\nwards isotropy. We propose two regularizers\nbased on contrastive learning and correlation\nmatrix respectively, and demonstrate their ef-\nfectiveness through extensive experiments. Our\nmain finding is that it is promising to regu-\nlarize supervised pre-training with isotropiza-\ntion to further improve the performance of\nfew-shot intent detection. The source code\ncan be found at https://github.com/\nfanolabs/isoIntentBert-main.\n1 Introduction\nIntent detection is a core module of task-oriented di-\nalogue systems. Training a well-performing intent\nclassifier with only a few annotations, i.e., few-shot\nintent detection, is of great practical value. Re-\ncently, this problem has attracted considerable at-\ntention (Vuli´c et al., 2021; Zhang et al., b; Dopierre\net al., b) but remains a challenge.\nTo tackle few-shot intent detection, earlier\nworks employ induction network (Geng et al.,\n2019), generation-based methods (Xia et al., a),\nmetric learning (Nguyen et al., 2020), and self-\ntraining (Dopierre et al., b), to design sophisticated\nalgorithms. Recently, pre-trained language models\n(PLMs) have emerged as a simple yet promising\nsolution to a wide spectrum of natural language pro-\ncessing (NLP) tasks, triggering the surge of PLM-\n∗ Corresponding author.\nbased solutions for few-shot intent detection (Wu\net al., 2020; Zhang et al., a,b; Vuli ´c et al., 2021;\nZhang et al., b), which typically fine-tune PLMs on\nconversation data.\nA PLM-based fine-tuning method (Zhang et al.,\na), called IntentBERT, utilizes a small amount of\nlabeled utterances from public intent datasets to\nfine-tune PLMs with a standard classification task,\nwhich is referred to as supervised pre-training. De-\nspite its simplicity, supervised pre-training has been\nshown extremely useful for few-shot intent detec-\ntion even when the target data and the data used for\nfine-tuning are very different in semantics. How-\never, as will be shown in Section 3.2, IntentBERT\nsuffers from severe anisotropy, an undesirable prop-\nerty of PLMs (Gao et al., a; Ethayarajh, 2019; Li\net al., 2020).\nAnisotropy is a geometric property that seman-\ntic vectors fall into a narrow cone. It has been\nidentified as a crucial factor for the sub-optimal\nperformance of PLMs on a variety of downstream\ntasks (Gao et al., a; Arora et al., b; Cai et al.,\n2020; Ethayarajh, 2019), which is also known\nas the representation degeneration problem (Gao\net al., a). Fortunately, isotropization techniques\ncan be applied to adjust the embedding space and\nyield significant performance improvement in many\ntasks (Su et al., 2021; Rajaee and Pilehvar, 2021a).\nHence, this paper aims to answer the question:\n• Can we improve supervised pre-training via\nisotropization for few-shot intent detection?\nMany isotropization techniques have been devel-\noped based on transformation (Su et al., 2021;\nHuang et al., 2021), contrastive learning (Gao et al.,\nb), and top principal components elimination (Mu\nand Viswanath, 2018). However, these methods\nare designed for off-the-shelf PLMs. When applied\non PLMs that have been fine-tuned on some NLP\ntask such as semantic textual similarity or intent\nclassification, they may introduce an adverse effect,\narXiv:2205.07208v3  [cs.CL]  15 Sep 2024\nFigure 1: Illustration of our proposed regularized supervised pre-training. SPT denotes supervised pre-training\n(fine-tuning an off-the-shelf PLM on a set of labeled utterances), which makes the feature space more anisotropic.\nCL-Reg and Cor-Reg are designed to regularize SPT and increase the isotropy of the feature space, which leads to\nbetter performance on few-shot intent detection.\nas observed in Rajaee and Pilehvar (2021c) and our\npilot experiments.\nIn this work, we propose to regularize super-\nvised pre-training with isotropic regularizers. As\nshown in Fig. 1, we devise two regularizers, a\ncontrastive-learning-based regularizer (CL-Reg)\nand a correlation-matrix-based regularizer (Cor-\nReg), each of which can increase the isotropy of\nthe feature space during supervised training. Our\nempirical study shows that the regularizers can sig-\nnificantly improve the performance of standard su-\npervised training, and better performance can often\nbe achieved when they are combined.\nThe contributions of this work are three-fold:\n• We present the first study on the isotropy prop-\nerty of PLMs for few-shot intent detection,\nshedding light on the interaction of supervised\npre-training and isotropization.\n• We improve supervised pre-training by devis-\ning two simple yet effective regularizers to\nincrease the isotropy of the feature space.\n• We conduct a comprehensive evaluation and\nanalysis to validate the effectiveness of the\nproposed approach.\n2 Related Works\n2.1 Few-shot Intent Detection\nWith a surge of interest in few-shot learning (Finn\net al., 2017; Vinyals et al., 2016; Snell et al., 2017),\nfew-shot intent detection has started to receive at-\ntention. Earlier works mainly focus on model de-\nsign, using capsule network (Geng et al., 2019),\nvariational autoencoder (Xia et al., a), or metric\nfunctions (Yu et al., 2018; Nguyen et al., 2020). Re-\ncently, PLMs-based methods have shown promis-\ning performance in a variety of NLP tasks and be-\ncome the model of choice for few-shot intent detec-\ntion. Zhang et al. (c) cast few-shot intent detection\ninto a natural language inference (NLI) problem\nand fine-tune PLMs on NLI datasets. Zhang et al.\n(b) propose to fine-tune PLMs on unlabeled ut-\nterances by contrastive learning. Zhang et al. (a)\nleverage a small set of public annotated intent detec-\ntion benchmarks to fine-tune PLMs with standard\nsupervised training and observe promising perfor-\nmance on cross-domain few-shot intent detection.\nMeanwhile, the study of few-shot intent detection\nhas been extended to other settings including semi-\nsupervised learning (Dopierre et al., b,a), gener-\nalized setting (Nguyen et al., 2020), multi-label\nclassification (Hou et al., 2021), and incremental\nlearning (Xia et al., b). In this work, we consider\nstandard few-shot intent detection, following the\nsetup of Zhang et al. (a) and aiming to improve\nsupervised pre-training with isotropization.\n2.2 Further Pre-training PLMs with Dialogue\nCorpora\nRecent works have shown that further pre-training\noff-the-shelf PLMs using dialogue corpora (Hen-\nderson et al., b; Peng et al., 2020, 2021) are bene-\nficial for task-oriented downstream tasks such as\nintent detection. Specifically, TOD-BERT (Wu\net al., 2020) conducts self-supervised learning on\ndiverse task-oriented dialogue corpora. ConvBERT\n(Mehri et al., 2020) is pre-trained on a 700 million\nopen-domain dialogue corpus. Vuli´c et al. (2021)\npropose a two-stage procedure: adaptive conversa-\ntional fine-tuning followed by task-tailored conver-\nsational fine-tuning. In this work, we follow Zhang\net al. (a) to further pre-train PLMs using a small\namount of labeled utterances from public intent\ndetection benchmarks.\n2.3 Anisotropy of PLMs\nIsotropy is a key geometric property of the seman-\ntic space of PLMs. Recent studies identify the\nanisotropy problem of PLMs (Cai et al., 2020; Etha-\nyarajh, 2019; Mu and Viswanath, 2018; Rajaee and\nPilehvar, 2021c), which is also known as the rep-\nresentation degeneration problem (Gao et al., a):\nword embeddings occupy a narrow cone, which\nsuppresses the expressiveness of PLMs. To resolve\nthe problem, various methods have been proposed,\nincluding spectrum control (Wang et al., 2019),\nflow-based mapping (Li et al., 2020), whitening\ntransformation (Su et al., 2021; Huang et al., 2021),\ncontrastive learning (Gao et al., b), and cluster-\nbased methods (Rajaee and Pilehvar, 2021a). De-\nspite their effectiveness, these methods are de-\nsigned for off-the-shelf PLMs. The interaction\nbetween isotropization and fine-tuning PLMs re-\nmains under-explored. A most recent work by Ra-\njaee and Pilehvar shows that there might be a con-\nflict between the two operations for the semantic\ntextual similarity (STS) task. On the other hand,\nZhou et al. (2021) propose to fine-tune PLMs with\nDataset BERT IntentBERT\nBANKING .96 .71 (.04)\nHINT3 .95 .72 (.03)\nHWU64 .96 .72 (.04)\nTable 1: The impact of fine-tuning on isotropy.\nFine-tuning renders the semantic space notably more\nanisotropic. The mean and standard deviation of 5 runs\nwith different random seeds are reported.\nisotropic batch normalization on some supervised\ntasks, but it requires a large amount of training\ndata. In this work, we study the interaction be-\ntween isotropization and supervised pre-training\n(fine-tuning) PLMs on intent detection tasks.\n3 Pilot Study\nBefore introducing our approach, we present pilot\nexperiments to gain some insights into the interac-\ntion between isotropization and fine-tuning PLMs.\n3.1 Measuring isotropy\nFollowing Mu and Viswanath (2018); Bi ´s et al.\n(2021), we adopt the following measurement of\nisotropy:\nI(V) = minc ∈ C Z(c, V)\nmaxc ∈ C Z(c, V), (1)\nwhere V ∈ RN×d is the matrix of stacked embed-\ndings of N utterances (note that the embeddings\nhave zero mean),C is the set of unit eigenvectors of\nV⊤V, and Z(c, V) is the partition function (Arora\net al., b) defined as:\nZ(c, V) =\nNX\ni=1\nexp\n\u0010\nc⊤vi\n\u0011\n, (2)\nwhere vi is the ith row of V. I(V) ∈ [0, 1], and 1\nindicates perfect isotropy.\n3.2 Fine-tuning Leads to Anisotropy\nTo observe the impact of fine-tuning on isotropy,\nwe follow IntentBERT (Zhang et al., a) to fine-tune\nBERT (Devlin et al., 2019) with standard super-\nvised training on a small set of an intent detection\nbenchmark OOS (Larson et al., 2019) (details are\ngiven in Section 4.1). We then compare the isotropy\nof the original embedding space (BERT) and the\nembedding space after fine-tuning (IntentBERT)\non target datasets. As shown in Table 1, after fine-\ntuning, the isotropy of the embedding space is no-\ntably decreased on all datasets. Hence, it can be\nseen that fine-tuning may render the feature space\nmore anisotropic.\nFigure 2: The impact of contrastive learning on In-\ntentBERT with experiments on HWU64 and BANK-\nING77 datasets. The performance (blue) drops while\nthe isotropy (orange) increases.\n3.3 Isotropization after Fine-tuning May\nHave an Adverse Effect\nTo examine the effect of isotropization on a fine-\ntuned model, we apply two strong isotropiza-\ntion techniques to IntentBERT: dropout-based con-\ntrastive learning (Gao et al., b) and whitening trans-\nformation (Su et al., 2021). The former fine-tunes\nPLMs in a contrastive learning manner 1, while\nthe latter transforms the semantic feature space\ninto an isotropic space via matrix transformation.\nThese methods have been demonstrated highly ef-\nfective (Gao et al., b; Su et al., 2021) when ap-\nplied to off-the-shelf PLMs, but things are dif-\nferent when they are applied to fine-tuned mod-\nels. As shown in Fig. 2, contrastive learning im-\nproves isotropy, but it significantly lowers the per-\nformance on two benchmarks. As for whitening\ntransformation, it has inconsistent effects on the\ntwo datasets, as shown in Fig. 3. It hurts the per-\nformance on HWU64 (Fig. 3a) but yields better\nresults on BANKING77 (Fig. 3b), while produc-\ning nearly perfect isotropy on both. The above\nobservations indicate that isotropization may hurt\nfine-tuned models, which echoes the recent finding\nof Rajaee and Pilehvar.\n4 Method\nThe pilot experiments reveal the anisotropy of a\nPLM fine-tuned on intent detection tasks and the\n1We refer the reader to the original paper for details.\n(a) HWU64.\n(b) BANKING77.\nFigure 3: The impact of whitening on IntentBERT with\nexperiments on HWU64 and BANKING77 datasets.\nWhitening transformation leads to perfect isotropy but\nhas inconsistent effects on the performance.\nchallenge of applying isotropization techiniques on\nthe fine-tuned model. In this section, we propose\na joint fine-tuning and isotropization framework.\nSpecifically, we propose two regularizers to make\nthe feature space more isotropic during fine-tuning.\nBefore presenting our method, we first introduce\nsupervised pre-training.\n4.1 Supervised Pre-training for Few-shot\nIntent Detection\nFew-shot intent detection targets to train a good in-\ntent classifier with only a few labeled dataDtarget =\n{(xi, yi)}Nt , where Nt is the number of labeled\nsamples in the target dataset, xi denotes the ith\nutterance, and yi is the label.\nTo tackle the problem, Zhang et al. (a) pro-\npose to learn intent detection skills (fine-tune a\nPLM) on a small subset of public intent detection\nbenchmarks by supervised pre-training. Denote\nby Dsource = {(xi, yi)}Ns the source data used for\npre-training, where Ns is the number of examples.\nThe fine-tuned PLM can be directly used on the\ntarget dataset. It has been shown that this method\n(a) CL-Reg.\n (b) Cor-Reg.\nFigure 4: Illustration of CL-Reg (contrastive-learning-based regularizer) and Cor-Reg (correlation-matrix-based\nregularizer). xi is the ith utterance in a batch of size 3. In (a), xi is fed to the PLM twice with built-in dropout to\nproduce two different representations of xi: hi and h+\ni . Positive and negative pairs are then constructed for each xi.\nFor example, h1 and h+\n1 form a positive pair for x1, while h1 and h+\n2 , and h1 and h+\n3 , form negative pairs for x1.\nIn (b), the correlation matrix is estimated from hi, feature vectors generated by the PLM, and is regularized towards\nthe identity matrix.\ncan work well when the label spaces of Dsource and\nDtarget are disjoint.\nSpecifically, the pre-training is conducted by at-\ntaching a linear layer (as the classifier) on top of\nthe utterance representation generated by the PLM:\np(y|hi) =softmax (Whi + b) ∈ RL, (3)\nwhere hi ∈ Rd is the representation of the ith ut-\nterance in Dsource, W ∈ RL×d and b ∈ RL are the\nparameters of the linear layer, and L is the number\nof classes. The model parameters θ = {ϕ, W, b},\nwith ϕ being the parameters of the PLM, are trained\non Dsource with a cross-entropy loss:\nθ = arg min\nθ\nLce (Dsource; θ) . (4)\nAfter supervised pre-training, the linear layer is\nremoved, and the PLM can be immediately used as\na feature extractor for few-shot intent classification\non target data. As shown in Zhang et al. (a), a para-\nmetric classifier such as logistic regression can be\ntrained with only a few labeled samples to achieve\ngood performance.\nHowever, our analysis in Section 3.2 shows the\nlimitation of supervised pre-training, which yields\na anisotropic feature space.\n4.2 Regularizing Supervised Pre-training with\nIsotropization\nTo mitigate the anisotropy of the PLM fine-tuned by\nsupervised pre-training, we propose a joint training\nobjective by adding a regularization term Lreg for\nisotropization:\nL = Lce(Dsource; θ) +λLreg(Dsource; θ), (5)\nwhere λ is a weight parameter. The aim is to learn\nintent detection skills while maintaining an appro-\npriate degree of isotropy. We devise two different\nregularizers introduced as follows.\nContrastive-learning-based Regularizer.In-\nspired by the recent success of contrastive learning\nin mitigating anisotropy (Yan et al., 2021; Gao\net al., b), we employ the dropout-based contrastive\nlearning loss used in Gao et al. (b) as the regular-\nizer:\nLreg = − 1\nNb\nNbX\ni\nlog esim(hi,h+\ni )/τ\nPNb\nj=1 esim(hi,h+\nj )/τ . (6)\nIn particular, hi ∈ Rd and h+\ni ∈ Rd are two dif-\nferent representations of utterance xi generated by\nthe PLM with built-in standard dropout (Srivastava\net al., 2014), i.e., xi is passed to the PLM twice\nwith different dropout masks to producehi and h+\ni .\nsim(h1, h2) denotes the cosine similarity between\nh1 and h2. τ is the temperature parameter. Nb is\nthe batch size. Since hi and h+\ni represent the same\nutterance, they form a positive pair. Similarly, hi\nand h+\nj form a negative pair, since they represent\ndifferent utterances. An example is given in Fig. 4a.\nBy minimizing the contrastive loss, positive pairs\nare pulled together while negative pairs are pushed\naway, which in theory enforces an isotropic fea-\nture space (Gao et al., b). In Gao et al. (b), the\ncontrastive loss is used as the single objective to\nfine-tune off-the-shelf PLMs in an unsupervised\nmanner, while in this work we use it jointly with\nsupervised pre-training to fine-tune PLMs for few-\nshot learning.\nCorrelation-matrix-based Regularizer. The\nabove regularizer enforces isotropization implicitly.\nHere, we propose a new regularizer that explic-\nitly enforces isotropization. The perfect isotropy\nis characterized by zero covariance and uniform\nvariance (Su et al., 2021; Zhou et al., 2021), i.e., a\ncovariance matrix with uniform diagonal elements\nand zero non-diagonal elements. Isotropization\ncan be achieved by endowing the feature space\nwith such statistical property. However, as will be\nshown in Section 5.3, it is difficult to determine the\nappropriate scale of variance. Therefore, we base\nthe regularizer on correlation matrix :\nLreg = ∥Σ − I∥, (7)\nwhere ∥·∥ denotes Frobenius norm, I ∈ Rd×d is the\nidentity matrix, Σ ∈ Rd×d is the correlation matrix\nwith Σij being the Pearson correlation coefficient\nbetween the ith dimension and the jth dimension.\nAs shown in Fig. 4b,Σ is estimated with utterances\nin the current batch. By pushing the correlation\nmatrix towards the identity matrix during training,\nwe can learn a more isotropic feature space.\nMoreover, the proposed two regularizers can be\nused together as follows:\nL = Lce(Dsource; θ) +λ1Lcl(Dsource; θ)\n+λ2Lcor(Dsource; θ), (8)\nwhere λ1 and λ2 are the weight parameters, andLcl\nand Lcor denote CL-Reg and Cor-Reg, respectively.\nOur experiments show that better performance is\noften observed when they are used together.\n5 Experiments\nTo validate the effectiveness of the approach, we\nconduct extensive experiments.\n5.1 Experimental Setup\nDatasets. To perform supervised pre-training, we\nfollow Zhang et al. to use the OOS dataset (Lar-\nson et al., 2019) which contains diverse semantics\nof 10 domains. Also following Zhang et al., we\nexclude the domains “Banking” and “Credit Cards”\nsince they are similar in semantics to one of the test\ndataset BANKING77. We then use 6 domains for\ntraining and 2 for validation, as shown in Table 2.\nFor evaluation, we employ three datasets: BANK-\nING77 (Casanueva et al., 2020) is an intent detec-\ntion dataset for banking service. HINT3 (Arora\net al., a) covers 3 domains, “Mattress Products Re-\ntail”, “Fitness Supplements Retail”, and “Online\nTraining Validation\n“Utility”, “Auto com-\nmute”, “Work”, “Home”,\n“Meta”, “Small talk”\n“Travel”, “Kitchen din-\ning”\nTable 2: Split of domains in OOS.\nDataset #domain #intent #data\nOOS 10 150 22500\nBANKING77 1 77 13083\nHINT3 3 51 2011\nHWU64 21 64 10030\nTable 3: Dataset statistics.\nGaming”. HWU64 (Liu et al., 2019a) is a large-\nscale dataset containing 21 domains. Dataset statis-\ntics are summarized in Table 3.\nOur Method. Our method can be applied to\nfine-tune any PLM. We conduct experiments on\ntwo popular PLMs, BERT (Devlin et al., 2019) and\nRoBERTa (Liu et al., 2019b). For both of them, the\nembedding of [CLS] is used as the utterance rep-\nresentation in Eq. 3. We employ logistic regression\nas the classifier. We select the hyperparameters\nλ, λ1, λ2, and τ by validation. The best hyperpa-\nrameters are provided in Table 4.\nMethod Hyperparameter\nCL-Reg λ = 1.7, τ= 0.05\nCor-Reg λ = 0.04\nCL-Reg + Cor-Reg λ1 = 1.7, λ2 = 0.04, τ= 0.05\n(a) BERT-based.\nMethod Hyperparameter\nCL-Reg λ = 2.9, τ= 0.05\nCor-Reg λ = 0.06\nCL-Reg + Cor-Reg λ1 = 2.9, λ2 = 0.13, τ= 0.05\n(b) RoBERTa-based.\nTable 4: Hyperparameters selected via validation.\nBaselines. We compare our method to the\nfollowing baselines. First, for BERT-based\nmethods, BERT-Freeze freezes BERT; CON-\nVBERT (Mehri et al., 2020), TOD-BERT (Wu\net al., 2020), and DNNC-BERT (Zhang et al.,\nc) further pre-train BERT on conversational cor-\npus or natural language inference tasks. USE-\nConveRT (Henderson et al., a; Casanueva et al.,\n2020) is a transformer-based dual-encoder pre-\ntrained on conversational corpus. CPFT-BERT\nMethod BANKING77 HINT3 HWU64 Val.\n2-shot 10-shot 2-shot 10-shot 2-shot 10-shot 2-shot 10-shot\nBERT-Freeze 57.10 84.30 51.95 80.27 64.83 87.99 74.20 92.99\nCONVBERT¶ 68.30 86.60 72.60 87.20 81.75 92.55 90.54 96.82\nTOD-BERT¶ 77.70 89.40 68.90 83.50 83.24 91.56 88.10 96.39\nUSE-ConveRT¶ – 85.20 – – – 85.90 – –\nDNNC-BERT¶ 67.50 89.80 64.10 87.90 73.97 90.71 72.98 95.23\nCPFT-BERT 72.09 89.82 74.34 90.37 83.02 93.66 89.33 97.30\nIntentBERT¶ 82.40 91.80 80.10 90.20 – – – –\nIntentBERT-ReImp 80.38 (.35) 92.35(.12) 77.09(.89) 89.55(.63) 90.61(.44) 95.21(.15) 93.62(.38) 97.80(.18)\nBERT-White 72.95 88.86 65.70 85.70 75.98 91.26 87.33 96.05\nIntentBERT-White 82.52 (.26) 92.29(.33) 78.50(.59) 90.14(.26) 87.24(.18) 94.42(.08) 94.89(.21) 98.07(.12)\nCL-Reg 83.45(.35) 93.66(.22) 79.30(.87 91.06(.30) 91.46(.15) 95.84(.12) 94.43(.22) 98.43.02)\nCor-Reg 83.94(.45) 93.98(.26) 80.16(.71) 91.38(.55) 90.75(.35) 95.82(.14) 95.02(.22) 98.47(.07)\nCL-Reg + Cor-Reg 85.21(.58) 94.68(.01) 81.20(.45) 92.38(.01) 90.66(.42) 95.84(.19) 95.41(.25) 98.58(.01)\nTable 5: 5-way few-shot intent detection using BERT. We report the mean and standard deviation of our methods\nand IntentBERT variants. CL-Reg, Cor-Reg, and CL-Reg + CorReg denote supervised pre-training regularized by\nthe corresponding regularizer. The top 3 results are highlighted. ¶ denotes results from (Zhang et al., a).\nMethod BANKING77 HINT3 HWU64 Val.\n2-shot 10-shot 2-shot 10-shot 2-shot 10-shot 2-shot 10-shot\nRoBERTa-Freeze 60.74 82.18 57.90 79.26 75.30 89.71 74.86 90.52\nWikiHowRoBERTa 32.88 59.50 31.92 54.18 30.81 52.47 34.10 60.59\nDNNC-RoBERTa 74.32 87.30 68.06 82.34 69.87 80.22 58.51 74.46\nCPFT-RoBERTa 80.27 (.11) 93.91(.06) 79.98(.11) 92.55(.07) 83.18(.11) 92.82(.06) 86.71(.10) 96.45(.05)\nIntentRoBERTa 81.38 (.66) 92.68(.24) 78.20(1.72) 89.01(1.07) 90.48(.69) 94.49(.43) 95.33(.54) 98.32(.15)\nRoBERTa-White 79.27 93.00 73.13 89.02 82.65 94.00 89.90 97.14\nIntentRoBERTa-White 83.75 (.45) 92.68(.31) 79.64(1.38) 90.13(.66) 86.52(1.33) 93.82(.53) 96.06(.58) 98.35(.21)\nCL-Reg 84.63(.68) 94.43(.34) 81.10(.49) 91.65(.13) 91.67(.20) 95.44(.28) 96.32(.14) 98.79(.05)\nCor-Reg 86.92(.71) 95.07(.41) 82.20(.48) 92.11(.41) 91.10(.18) 95.69(.12) 96.82(.03) 98.89(.03)\nCL-Reg + Cor-Reg 87.96(.31) 95.85(.02) 83.55(.30) 93.17(.23) 90.47(.39) 95.64(.28) 96.35(.19) 98.85(.07)\nTable 6: 5-way few-shot intent detection using RoBERTa. We report the mean and standard deviation of our methods\nand IntentBERT variants. CL-Reg, Cor-Reg, and CL-Reg + CorReg denote supervised pre-training regularized by\nthe corresponding regularizer. The top 3 results are highlighted.\nis the re-implemented version of CPFT (Zhang\net al., b), by further pre-training BERT in an un-\nsupervised manner with mask-based contrastive\nlearning and masked language modeling on the\nsame training data as ours. IntentBERT (Zhang\net al., a) further pre-trains BERT via supervised\npre-training described in Section 4.1. To guaran-\ntee a fair comparison, we provide IntentBERT-\nReImp, the re-implemented version of Intent-\nBERT, which uses the same random seed, training\ndata, and validation data as our methods. Second,\nfor RoBERTa-based baselines,RoBERTa-Freeze\nfreezes the model. WikiHowRoBERTa (Zhang\net al., d) further pre-trains RoBERTa on synthe-\nsized intent detection data. DNNC-RoBERTaand\nCPFT-RoBERTaare similar to DNNC-BERT and\nCPFT-BERT except the PLM.IntentRoBERTais\nthe re-implemented version of IntentBERT based\non RoBERTa, with uses the same random seed,\ntraining data, and validation data as our method.\nFinally, to show the superiority of the joint fine-\ntuning and isotropization, we compare our method\nagainst whitening transformation (Su et al., 2021).\nBERT-White and RoBERTa-White apply the\ntransformation to BERT and RoBERTa, respec-\ntively. IntentBERT-Whiteand IntentRoBERTa-\nWhite apply the transformation to IntentBERT-\nReImp and IntentRoBERTa, respectively.\nAll baselines use logistic regression as classi-\nfier except DNNC-BERT and DNNC-RoBERTa,\nwherein we follow the original work2 to train a pair-\nwise encoder for nearest neighbor classification.\nTraining Details.We use PyTorch library and\nPython to build our model. We employ Hugging\nFace implementation3 of bert-base-uncased and\n2https://github.com/salesforce/DNNC-few-shot-intent\n3https://github.com/huggingface/transformers\nroberta-base. We use Adam (Kingma and Ba,\n2015) as the optimizer with learning rate of2e−05\nand weight decay of 1e − 03. The model is trained\nwith Nvidia RTX 3090 GPUs. The training is early\nstopped if no improvement in validation accuracy\nis observed for 100 steps. The same set of ran-\ndom seeds, {1, 2, 3, 4, 5}, is used for IntentBERT-\nReImp, IntentRoBERTa, and our method.\nEvaluation. The baselines and our method are\nevaluated on C-way K-shot tasks. For each task,\nwe randomly sampleC classes and K examples per\nclass. The C×K labeled examples are used to train\nthe logistic regression classifier. Note that we do\nnot further fine-tune the PLM using the labeled data\nof the task. We then sample another5 examples per\nclass as queries. Fig. 1 gives an example with C =\n2 and K = 1. We report the averaged accuracy of\n500 tasks randomly sampled from Dtarget.\n5.2 Main Results\nThe main results are provided in Table 5 (BERT-\nbased) and Table 6 (RoBERTa-based). The follow-\ning observations can be made. First, our proposed\nregularized supervised pre-training, with either CL-\nReg or Cor-Reg, consistently outperforms all the\nbaselines by a notable margin in most cases, indi-\ncating the effectiveness of our method. Our method\nalso outperforms whitening transformation, demon-\nstrating the superiority of the proposed joint fine-\ntuning and isotropization framework. Second, Cor-\nReg slightly outperforms CL-Reg in most cases,\nshowing the advantage of enforcing isotropy ex-\nplicitly with the correlation matrix. Finally, CL-\nReg and Cor-Reg show a complementary effect\nin many cases, especially on BANKING77. The\nabove observations are consistent for both BERT\nand RoBERTa. It can be also seen that higher per-\nformance is often attained with RoBERTa.\nMethod BANKING77 HINT3 HWU64\nIntentBERT-ReImp .71 (.04) .72(.03) .72(.03)\nSPT+CL-Reg .77 (.01) .78(.01) .75(.03)\nSPT+Cor-Reg .79 (.01) .76(.06) .80(.03)\nSPT+CL-Reg+Cor-Reg .79 (.01) .76(.05) .80(.02)\nTable 7: Impact of the proposed regularizers on isotropy.\nThe results are obtained with BERT. SPT denotes super-\nvised pre-training.\nThe observed improvement in performance\ncomes with an improvement in isotropy. We report\nthe change in isotropy by the proposed regularizers\nin Table 7. It can be seen that both regularizers\nand their combination make the feature space more\nisotropic compared to IntentBERT-ReImp that only\nuses supervised pre-training. In addition, in gen-\neral, Cor-Reg can achieve better isotropy than CL-\nReg.\n5.3 Ablation Study and Analysis\nModerate isotropy is helpful.To investigate the\nrelation between the isotropy of the feature space\nand the performance of few-shot intent detection,\nwe tune the weight parameter λ of Cor-Reg to in-\ncrease the isotropy and examine the performance.\nAs shown in Fig. 5, a common pattern is observed:\nthe best performance is achieved when the isotropy\nis moderate. This observation indicates that it is\nimportant to find an appropriate trade-off between\nlearning intent detection skills and learning an in-\nsotropic feature space. In our method, we select\nthe appropriate λ by validation.\nFigure 5: Relation between performance and isotropy.\nThe results are obtained with BERT on 5-way 2-shot\ntasks.\nCorrelation matrix is better than covariance\nmatrix as regularizer.In the design of Cor-Reg\n(Section 4.2), we use the correlation matrix, rather\nthan the covariance matrix, to characterize isotropy,\nalthough the latter contains more information –\nvariance. The reason is that it is difficult to de-\ntermine the proper scale of the variances. Here,\nwe conduct experiments using the covariance ma-\ntrix, by pushing the non-diagonal elements (covari-\nances) towards 0 and the diagonal elements (vari-\nances) towards 1, 0.5, or the mean value, which\nare denoted by Cov-Reg-1, Cov-Reg-0.5, and Cov-\nReg-mean respectively in Table 8. It can be seen\nthat all the variants perform worse than Cor-Reg.\nOur method is complementary with batch\nnormalization. Batch normalization (Ioffe and\nMethod BANKING77 Val.\nCov-Reg-1 82.19 (.84) 94.52(.19)\nCov-Reg-0.5 82.62 (.80) 94.52(.26)\nCov-Reg-mean 82.50 (1.00) 93.82(.39)\nCor-Reg (ours) 83.94(.45) 95.02(.22)\nTable 8: Comparison between using covariance matrix\nand using correlation matrix to implement Cor-Reg. The\nexperiments are conducted with BERT and evaluated on\n5-way 2-shot tasks.\nSzegedy, 2015) can potentially mitigate the\nanisotropy problem via normalizing each dimen-\nsion with unit variance. We find that combining\nour method with batch normalization yields better\nperformance, as shown in Table 9.\nSPT CL-Reg Cor-Reg BN BANKING77\n✓ 80.38(.35)\n✓ ✓ 82.38(.38)\n✓ ✓ 83.45(.35)\n✓ ✓ ✓ 84.18(.28)\n✓ ✓ 83.94(.45)\n✓ ✓ ✓ 84.67(.51)\n✓ ✓ ✓ 85.21(.58)\n✓ ✓ ✓ ✓ 85.64(.41)\nTable 9: Effect of combining batch normalization and\nour method. The experiments are conducted with BERT\nand evaluated on 5-way 2-shot tasks. SPT denotes su-\npervised pre-training. BN denotes batch normalization.\nThe performance gain is not from the reduc-\ntion in model variance.Regularization techniques\nsuch as L1 regularization (Tibshirani, 1996) and\nL2 regularization (Hoerl and Kennard, 1970) are\noften used to improve model performance by re-\nducing model variance. Here, we show that the\nperformance gain of our method is ascribed to the\nimproved isotropy (Table 7) rather than the reduc-\ntion in model variance. To this end, we compare\nour method against L2 regularization with a wide\nrange of weights, and it is observed that reducing\nmodel variance cannot achieve comparable perfor-\nmance to our method, as shown in Fig. 6.\nThe computational overhead is small.To ana-\nlyze the computational overheads incurred by CL-\nReg and Cor-Reg, we decompose the duration of\none epoch of our method using the two regulariz-\ners jointly. As shown in Fig. 7, the overheads of\nCL-Reg and Cor-Reg are small, only taking up a\nsmall portion of the time.\nFigure 6: Comparison between our methods and L2 reg-\nularization. The experiments are conducted with BERT\nand evaluated on 5-way 2-shot tasks on BANKING77.\nSPT denotes superivsed pre-training.\nFigure 7: Run time decomposition of a single epoch.\nThe unit is second.\n6 Conclusion\nIn this work, we have identified and analyzed the\nanisotropy of the feature space of a PLM fine-\ntuned on intent detection tasks. Further, we have\nproposed a joint training framework and designed\ntwo regularizers based on contrastive learning and\ncorrelation matrix respectively to increase the in-\nsotropy of the feature space during fine-tuning,\nwhich leads to notably improved performance on\nfew-shot intent detection. Our findings and solu-\ntions may have broader implications for solving\nother natural language understanding tasks with\nPLM-based models.\nAcknowledgments\nWe would like to thank the anonymous reviewers\nfor their valuable comments. This research was\nsupported by the grants of HK ITF UIM/377 and\nPolyU DaSAIL project P0030935 funded by RGC.\nReferences\nGaurav Arora, Chirag Jain, Manas Chaturvedi, and Kru-\npal Modi. a. HINT3: Raising the bar for intent detec-\ntion in the wild. In EMNLP, 2020.\nSanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,\nand Andrej Risteski. b. A latent variable model ap-\nproach to PMI-based word embeddings. TACL, 2016,\n4:385–399.\nDaniel Bi´s, Maksim Podkorytov, and Xiuwen Liu. 2021.\nToo much in common: Shifting of embeddings in\ntransformer language models and its implications. In\nNAACL, 2021.\nXingyu Cai, Jiaji Huang, Yuchen Bian, and Kenneth\nChurch. 2020. Isotropy in the contextual embedding\nspace: Clusters and manifolds. In ICLR, 2020.\nIñigo Casanueva, Tadas Tem ˇcinas, Daniela Gerz,\nMatthew Henderson, and Ivan Vuli´c. 2020. Efficient\nintent detection with dual sentence encoders. In ACL,\n2020.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL, 2019.\nThomas Dopierre, Christophe Gravier, and Wilfried\nLogerais. a. ProtAugment: Intent detection meta-\nlearning through unsupervised diverse paraphrasing.\nIn ACL-IJCNLP, 2021.\nThomas Dopierre, Christophe Gravier, Julien Subercaze,\nand Wilfried Logerais. b. Few-shot pseudo-labeling\nfor intent detection. In COLING, 2020.\nKawin Ethayarajh. 2019. How contextual are contextu-\nalized word representations? Comparing the geom-\netry of BERT, ELMo, and GPT-2 embeddings. In\nEMNLP-IJCNLP , 2019.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.\nModel-agnostic meta-learning for fast adaptation of\ndeep networks. In ICML, 2017.\nJun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and\nTie-Yan Liu. a. Representation degeneration problem\nin training natural language generation models. In\nICLR 2019.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. b. Sim-\nCSE: Simple contrastive learning of sentence embed-\ndings. In EMNLP , 2021.\nRuiying Geng, Binhua Li, Yongbin Li, Xiaodan Zhu,\nPing Jian, and Jian Sun. 2019. Induction networks\nfor few-shot text classification. In EMNLP-IJCNLP ,\n2019.\nMatthew Henderson, Iñigo Casanueva, Nikola Mrkši´c,\nPei-Hao Su, Tsung-Hsien Wen, and Ivan Vuli ´c. a.\nConveRT: Efficient and accurate conversational repre-\nsentations from transformers. In Findings of EMNLP\n2020, Online.\nMatthew Henderson, Ivan Vuli´c, Daniela Gerz, Iñigo\nCasanueva, Paweł Budzianowski, Sam Coope, Geor-\ngios Spithourakis, Tsung-Hsien Wen, Nikola Mrkši´c,\nand Pei-Hao Su. b. Training neural response se-\nlection for task-oriented dialogue systems. In ACL,\n2019.\nArthur E Hoerl and Robert W Kennard. 1970. Ridge re-\ngression: Biased estimation for nonorthogonal prob-\nlems. Technometrics, 12(1):55–67.\nYutai Hou, Yongkui Lai, Yushan Wu, Wanxiang Che,\nand Ting Liu. 2021. Few-shot learning for multi-\nlabel intent detection. AAAI, 2021.\nJunjie Huang, Duyu Tang, Wanjun Zhong, Shuai Lu,\nLinjun Shou, Ming Gong, Daxin Jiang, and Nan\nDuan. 2021. WhiteningBERT: An easy unsuper-\nvised sentence embedding approach. In Findings\nof EMNLP , 2021.\nSergey Ioffe and Christian Szegedy. 2015. Batch Nor-\nmalization: Accelerating Deep Network Training by\nReducing Internal Covariate Shift. In ICML, 2015.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR 2015.\nStefan Larson, Anish Mahendran, Joseph J. Peper,\nChristopher Clarke, Andrew Lee, Parker Hill,\nJonathan K. Kummerfeld, Kevin Leach, Michael A.\nLaurenzano, Lingjia Tang, and Jason Mars. 2019. An\nevaluation dataset for intent classification and out-of-\nscope prediction. In EMNLP-IJCNLP , 2019.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang,\nYiming Yang, and Lei Li. 2020. On the sentence\nembeddings from pre-trained language models. In\nEMNLP , 2020.\nXingkun Liu, Arash Eshghi, Pawel Swietojanski, and\nVerena Rieser. 2019a. Benchmarking natural lan-\nguage understanding services for building conversa-\ntional agents. In IWSDS,2019.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nShikib Mehri, Mihail Eric, and Dilek Hakkani-Tur.\n2020. Dialoglue: A natural language understanding\nbenchmark for task-oriented dialogue. arXiv preprint\narXiv:2009.13570.\nJiaqi Mu and Pramod Viswanath. 2018. All-but-the-\ntop: Simple and effective postprocessing for word\nrepresentations. In ICLR 2018.\nHoang Nguyen, Chenwei Zhang, Congying Xia, and\nPhilip Yu. 2020. Dynamic semantic matching and\naggregation network for few-shot intent detection. In\nFindings of EMNLP 2020.\nBaolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayan-\ndeh, Lars Liden, and Jianfeng Gao. 2021. Soloist:\nBuilding task bots at scale with transfer learning and\nmachine teaching. TACL, 9:807–824.\nBaolin Peng, Chenguang Zhu, Chunyuan Li, Xiujun\nLi, Jinchao Li, Michael Zeng, and Jianfeng Gao.\n2020. Few-shot natural language generation for task-\noriented dialog. In Findings of EMNLP 2020.\nSara Rajaee and Mohammad Taher Pilehvar. 2021a. A\ncluster-based approach for improving isotropy in con-\ntextual embedding space. In ACL-IJCNLP , 2021.\nSara Rajaee and Mohammad Taher Pilehvar. 2021b.\nHow does fine-tuning affect the geometry of embed-\nding space: A case study on isotropy. In Findings of\nEMNLP 2021.\nSara Rajaee and Mohammad Taher Pilehvar. 2021c. An\nisotropy analysis in the multilingual bert embedding\nspace. arXiv preprint arXiv:2110.04504.\nJake Snell, Kevin Swersky, and Richard Zemel. 2017.\nPrototypical networks for few-shot learning. In\nNeurIPS, 2017.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overfitting. JMLR, 15(56):1929–1958.\nJianlin Su, Jiarun Cao, Weijie Liu, and Yangyiwen Ou.\n2021. Whitening sentence representations for bet-\nter semantics and faster retrieval. arXiv preprint\narXiv:2103.15316.\nRobert Tibshirani. 1996. Regression shrinkage and\nselection via the lasso. J. R. Stat. Soc., Series B\n(Methodological), 58(1):267–288.\nOriol Vinyals, Charles Blundell, Timothy Lillicrap,\nDaan Wierstra, et al. 2016. Matching networks for\none shot learning. In NeurIPS, 2016.\nIvan Vuli ´c, Pei-Hao Su, Samuel Coope, Daniela\nGerz, Paweł Budzianowski, Iñigo Casanueva, Nikola\nMrkši´c, and Tsung-Hsien Wen. 2021. ConvFiT: Con-\nversational fine-tuning of pretrained language models.\nIn EMNLP , 2021.\nLingxiao Wang, Jing Huang, Kevin Huang, Ziniu Hu,\nGuangtao Wang, and Quanquan Gu. 2019. Improv-\ning neural language generation with spectrum control.\nIn ICLR, 2019.\nChien-Sheng Wu, Steven C.H. Hoi, Richard Socher, and\nCaiming Xiong. 2020. TOD-BERT: Pre-trained natu-\nral language understanding for task-oriented dialogue.\nIn EMNLP , 2020.\nCongying Xia, Caiming Xiong, Philip Yu, and Richard\nSocher. a. Composed variational natural language\ngeneration for few-shot intents. In Findings of\nEMNLP 2020.\nCongying Xia, Wenpeng Yin, Yihao Feng, and Philip\nYu. b. Incremental few-shot text classification with\nmulti-round new classes: Formulation, dataset and\nsystem. In NAACL, 2021.\nYuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang,\nWei Wu, and Weiran Xu. 2021. ConSERT: A con-\ntrastive framework for self-supervised sentence rep-\nresentation transfer. In ACL-IJCNLP , 2021.\nMo Yu, Xiaoxiao Guo, Jinfeng Yi, Shiyu Chang, Saloni\nPotdar, Yu Cheng, Gerald Tesauro, Haoyu Wang, and\nBowen Zhou. 2018. Diverse few-shot text classifica-\ntion with multiple metrics. In NAACL, 2018.\nHaode Zhang, Yuwei Zhang, Li-Ming Zhan, Jiaxin\nChen, Guangyuan Shi, Xiao-Ming Wu, and Al-\nbert Y .S. Lam. a. Effectiveness of pre-training for\nfew-shot intent classification. In Findings of EMNLP\n2021.\nJianguo Zhang, Trung Bui, Seunghyun Yoon, Xiang\nChen, Zhiwei Liu, Congying Xia, Quan Hung Tran,\nWalter Chang, and Philip Yu. b. Few-shot intent\ndetection via contrastive pre-training and fine-tuning.\nIn EMNLP , 2021.\nJianguo Zhang, Kazuma Hashimoto, Wenhao Liu,\nChien-Sheng Wu, Yao Wan, Philip Yu, Richard\nSocher, and Caiming Xiong. c. Discriminative near-\nest neighbor few-shot intent detection by transferring\nnatural language inference. In EMNLP , 2020.\nLi Zhang, Qing Lyu, and Chris Callison-Burch. d. Intent\ndetection with WikiHow. In AACL, 2020.\nWenxuan Zhou, Bill Yuchen Lin, and Xiang Ren. 2021.\nIsobn: Fine-tuning bert with isotropic batch normal-\nization. In AAAI, 2021."
}