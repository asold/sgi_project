{
  "title": "SpeechBERT: An Audio-and-text Jointly Learned Language Model for End-to-end Spoken Question Answering",
  "url": "https://openalex.org/W3026408381",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221356867",
      "name": "Chuang, Yung-Sung",
      "affiliations": [
        "National Taiwan University"
      ]
    },
    {
      "id": "https://openalex.org/A4282363995",
      "name": "Liu, Chi-liang",
      "affiliations": [
        "National Taiwan University"
      ]
    },
    {
      "id": "https://openalex.org/A4221356874",
      "name": "Lee, Hung-Yi",
      "affiliations": [
        "National Taiwan University"
      ]
    },
    {
      "id": "https://openalex.org/A4221356875",
      "name": "Lee, Lin-shan",
      "affiliations": [
        "National Taiwan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2982095018",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2963571336",
    "https://openalex.org/W2981991061",
    "https://openalex.org/W2752104716",
    "https://openalex.org/W3015586639",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3096485810",
    "https://openalex.org/W2963425185",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2962854302",
    "https://openalex.org/W2740747242",
    "https://openalex.org/W2891229414",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3016262400",
    "https://openalex.org/W2762484717",
    "https://openalex.org/W2963340922",
    "https://openalex.org/W2551396370",
    "https://openalex.org/W2963288440",
    "https://openalex.org/W2894164357",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W3016006013",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2582956876",
    "https://openalex.org/W2972584841",
    "https://openalex.org/W1984076147",
    "https://openalex.org/W2802557066",
    "https://openalex.org/W2982223350",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2996383576",
    "https://openalex.org/W2899377381",
    "https://openalex.org/W3015265920"
  ],
  "abstract": "While various end-to-end models for spoken language understanding tasks have been explored recently, this paper is probably the first known attempt to challenge the very difficult task of end-to-end spoken question answering (SQA). Learning from the very successful BERT model for various text processing tasks, here we proposed an audio-and-text jointly learned SpeechBERT model. This model outperformed the conventional approach of cascading ASR with the following text question answering (TQA) model on datasets including ASR errors in answer spans, because the end-to-end model was shown to be able to extract information out of audio data before ASR produced errors. When ensembling the proposed end-to-end model with the cascade architecture, even better performance was achieved. In addition to the potential of end-to-end SQA, the SpeechBERT can also be considered for many other spoken language understanding tasks just as BERT for many text processing tasks.",
  "full_text": "SpeechBERT: An Audio-and-text Jointly Learned Language Model\nfor End-to-end Spoken Question Answering\nYung-Sung Chuang Chi-Liang Liu Hung-yi Lee Lin-shan Lee\nCollege of Electrical Engineering and Computer Science, National Taiwan University\n{chuangyungsung,liangtaiwan1230,tlkagkb93901106}@gmail.com, lslee@gate.sinica.edu.tw\nAbstract\nWhile various end-to-end models for spoken language under-\nstanding tasks have been explored recently, this paper is prob-\nably the ï¬rst known attempt to challenge the very difï¬cult\ntask of end-to-end spoken question answering (SQA). Learning\nfrom the very successful BERT model for various text process-\ning tasks, here we proposed an audio-and-text jointly learned\nSpeechBERT model. This model outperformed the conven-\ntional approach of cascading ASR with the following text ques-\ntion answering (TQA) model on datasets including ASR errors\nin answer spans, because the end-to-end model was shown to\nbe able to extract information out of audio data before ASR\nproduced errors. When ensembling the proposed end-to-end\nmodel with the cascade architecture, even better performance\nwas achieved. In addition to the potential of end-to-end SQA,\nthe SpeechBERT can also be considered for many other spo-\nken language understanding tasks just as BERT for many text\nprocessing tasks.\n1. Introduction\nVarious spoken language processing tasks, such as transla-\ntion [1], retrieval [2], summarization [3] and understanding [4]\nhave been very successful with a standard cascade architecture:\nan ASR front-end module transforming the speech signals into\ntext form, followed by the downstream task module (such as\ntranslation) trained on text taking the ASR output as normal text\ninput. However, the end-to-end approach trying to consider the\ntwo modules as a whole is always attractive for the following\nreasons. The two modules in the cascade architecture locally\noptimize the two tasks with different criteria, while the end-to-\nend approach may obtain globally optimized performance for\nthe overall task. The ASR module minimizes the WER, which\nis not necessarily directly proportional to the performance mea-\nsure of the overall task. Much information is inevitably lost\nwhen the speech signals are transformed into text with errors,\nand the errors canâ€™t be recovered in the following module. The\nend-to-end approach allows the possibility of capturing infor-\nmation directly from the speech signals not shown in the ASR\noutput and offering overall performance less limited by the ASR\naccuracy.\nSome spoken language tasks such as translation, retrieval,\nand understanding (intent classiï¬cation and slot ï¬lling) have\nbeen achieved with end-to-end approaches [5, 6, 7, 8, 9], al-\nthough it remains difï¬cult to obtain signiï¬cantly better results\nthan the standard cascade architecture. These tasks are primar-\nily sentence-level, in which extracting some local information\nin a short utterance may be adequate. Spoken question answer-\ning (SQA) considered here is known as a much more difï¬cult\nproblem. The inputs to the SQA task are much longer spoken\nparagraphs. In addition to understanding the literal meaning,\nthe global information in the paragraphs needs to be organized,\nand sophisticated reasoning is usually required to ï¬nd the an-\nswers. Fine-grained information is also important to predict the\nexact position of the answer span from a very long context. This\npaper is probably the ï¬rst known attempt to try to perform such\na difï¬cult SQA task with an end-to-end approach.\nSubstantial improvements in text question answering\n(TQA) tasks trained and tested on text data have been observed\nafter the large-scale self-supervised pre-trained language mod-\nels appeared, such as BERT [10] and GPT [11]. Instead of\nlearning the TQA tasks from scratch, these models were ï¬rst\npre-trained on a large unannotated text corpus to learn self-\nsupervised representations for the general purpose and then\nï¬ne-tuned on the downstream TQA dataset. Results compa-\nrable to human performance on SQuAD datasets [12, 13] were\nobtained in this way. However, previous work [14] indicated\nthat such an approach may not be easily extendable to the SQA\ntask on audio data by directly cascading an ASR module in the\nfront. Not only the ASR caused serious problems, but very of-\nten the true answer spans included name entities or OOV words\nwhich cannot be correctly recognized at all, thus cannot be iden-\ntiï¬ed by the following TQA module trained on text. That is why\nall questions with recognition errors in answer spans were dis-\ncarded in the previous work [14]. This led to the end-to-end\napproach to SQA proposed here.\nThe BERT model [10] useful in TQA tasks was able to\ntransform word tokens into contextualized embeddings carrying\nplenty of information. For SQA task, it is certainly desirable to\ntransform audio words (audio signals for word tokens) also into\nsuch embeddings, but much more challenging. The same word\ntoken can have millions of different audio signal realizations in\ndifferent utterances. The boundaries for audio words in utter-\nances are not available. The next problem is even much more\nchallenging. BERT can learn semantic information of a word\ntoken based on its context in text form. Audio words in au-\ndio data have context only in audio form, which is much more\nnoisy, confusing, unpredictable, and difï¬cult to handle. Learn-\ning semantics from audio context is really hard.\nAudio Word2Vec [15] was the ï¬rst effort to transform audio\nwords with known boundaries into embeddings carrying pho-\nnetic information only, no semantics at all. Speech2Vec [16]\nthen tried to imitate the training process of skip-gram or CBOW\nin Word2Vec [17] to extract some semantic features. It was\nproposed [18] to learn to jointly segment the audio words out\nof utterance and extract the embeddings. Other efforts then\ntried to align the audio word embeddings with text word embed-\ndings [19, 20]. Some more recent works [21, 22, 23, 24, 25, 26]\neven tried to obtain embeddings for audio signals using ap-\nproaches very similar to BERT, but primarily extracting acous-\ntic information with very limited semantics. These approaches\nmay be able to extract some semantics with audio word embed-\ndings, but the level of semantics obtained was still very far from\nthat required for the SQA tasks.\nIn this paper, we propose an audio-and-text jointly learned\narXiv:1910.11559v4  [cs.CL]  11 Aug 2020\nInitial Phonetic-semantic\nJoint Embedding for Audio\nWords (2.2)\nAudio data sets Text BERT\nPre-training\n(2.1)\nText data sets\nSpeechBERT Pre-training\n(2.3) Fine-tuning on SQA (2.4)\nSpeechBERT\n(audio words)\nFigure 1: Overall training process for SpeechBERT.\nSpeechBERT model for the end-to-end SQA task. Speech-\nBERT is a pre-trained model learned from audio and text\ndatasets, so as to be able to produce embeddings for audio\nwords, and these embeddings can be properly aligned with the\nembeddings for the corresponding text words offered by a typ-\nical text BERT model trained with text datasets. Standard pre-\ntraining and ï¬ne-tuning as text BERT are also performed. When\nused in the SQA task, performance comparable to or better than\nthe cascade architecture was obtained.\n2. SpeechBERT for End-to-end SQA\nHere we assume the training audio datasets include the ground\ntruth transcripts, and an off-the-shelf ASR engine is available.\nSo we can segment the audio training data into audio words (au-\ndio signals for the underlying word tokens) by forced alignment.\nFor audio testing data we can use the ASR engine to obtain au-\ndio words including their boundaries too, although with errors.\nThe overall training process for the SpeechBERT for end-to-\nend SQA proposed here is shown in Fig. 1. We ï¬rst use the text\ndataset to pre-train a Text BERT (upper right block, Sec. 2.1),\nbased on which we train the initial phonetic-semantic joint em-\nbedding with the audio words in the training audio dataset (up-\nper left block, Sec. 2.2). The training of SpeechBERT (or a\nshared BERT model for both text and audio) is then in the\nbottom of the ï¬gure, including pre-training (Sec 2.3) and ï¬ne-\ntuning on SQA (Sec. 2.4). The details are given below.\n2.1. Text BERT Pre-training\nHere we follow the standard Text BERT pre-training proce-\ndure [10]. For a sentence in the training text set with n to-\nkens T = {t1, t2, ..., tn}, we represent them as vectors Etext =\n{e1, e2, ..., en}and sum them with corresponding positional\nand sentence segment embeddings to get Eâ€²\ntext to be fed into the\nmulti-layer Transformer. Masked language model (MLM) task\nis performed at the output layer of the Text BERT by randomly\nreplacing 15% of vectors inEtext with a special mask token vec-\ntor and predicting the masked tokens at the same position of\nthe output layers. Next sentence prediction (NSP) usually per-\nformed in BERT training is not used here because some recent\nstudies indicated it may not be helpful [27, 28, 29, 30].\n2.2. Initial Phonetic-Semantic Joint Embedding\nFor an utterance in the training audio dataset withn audio words\nX = {x(1), x(2), ..., x(n)}, the goal here is to encode these\nn audio words into n embeddings Eaudio = {Ëœe1, Ëœe2, ...,Ëœen}.\nLet one of the audio words include a total of T speech feature\nvectors, x = {x1, x2, ..., xT }. The complete process of initial\nphonetic-semantic joint embedding is in Fig. 2. With the speech\nfeatures for each audio word x = {x1, x2, ..., xT }, we use an\nRNN sequence-to-sequence autoencoder [15] as in Fig. 2. This\nincludes an audio encoder (low left corner of the ï¬gure) trans-\nforming the input x into a vector z (in red in the middle), and\nan audio decoder reconstructing the output y = (y1, y2, ..., yT )\nfrom z. The autoencoder is trained to minimize the reconstruc-\ntion error:\nLrecons =\nâˆ‘\nk\nTâˆ‘\nt=1\nâˆ¥xt âˆ’ytâˆ¥2\n2 , (1)\nAudio Encoder Audio Decoder\n...\nBERT Word\nEmbedding Layer\nL1 Loss in (2)\nReconstruction Loss in (1)\nSpeech\nfeatures\naudio word\nSpeech\nfeatures\nFigure 2: Training procedure for the Initial Phonetic-Semantic\nJoint Embedding. After training, the the encoded vector (z in\nred) obtained here is used to train the SpeechBERT.\nwhere k is the index for training audio words. This process en-\nables the vector z to capture the phonetic structure information\nof the audio words but not semantics at all, which is not ade-\nquate for the goal here. So we make the vector z be constrained\nby a L1-distance loss (lower right) further:\nLL1 =\nâˆ‘\nk\nâˆ¥z âˆ’Emb(t)âˆ¥1 , (2)\nwhere t is the token label for the audio word x and Emb is the\nembedding layer of the Text BERT trained in Sec. 2.1. In this\nway, we use the token label of the audio word to access the se-\nmantic information about the considered audio word extracted\nby the Text BERT as long as it is within the vocabulary of the\nText BERT. So, the autoencoder model learns to keep the pho-\nnetic structure of the audio word so as to reconstruct the origi-\nnal speech features x as much as possible, but at the same time\nit learns to ï¬t to the embedding distribution of the Text BERT\nwhich carries plenty of semantic information for the word to-\nkens. This makes it possible for the model to learn a joint em-\nbedding space integrating both phonetic and semantic informa-\ntion extracted from both audio and text datasets.\n2.3. MLM Pre-training for SpeechBERT with Both Text\nand Audio Data\nThis is the lower left block of Fig. 1 with details in Fig. 3 (a).\nHere we pre-train the SpeechBERT with the MLM task using\nboth text and audio datasets, before ï¬ne-tuning it on the down-\nstream QA task.\nAs in Fig. 3 (a), for the SpeechBERT learning to take em-\nbeddings for both discrete text words and continuous spoken\nwords, we jointly optimize the MLM loss with the mixture of\nboth audio and text data. The same training target for text\nMLM as described in Sec. 2.1 is used here. For audio data\ninput, after obtaining the phonetic-semantic joint embeddings\nEaudio = {Ëœe1, Ëœe2, ...,Ëœen}for each utterance as in Sec. 2.2, we\nalso randomly replace 15% of those embeddings with mask to-\nken vectors as we do in Sec. 2.1. With the supervised setting,\nwe can similarly predict the corresponding tokens behind the\nmasked spoken words. During training, we freeze the audio\nencoder in Fig. 2 to speed up the process, while have the text\nword embedding unfrozen to keep the joint audio-and-text em-\nbeddings ï¬‚exible, considering the earlier experiences reported\nfor end-to-end SLU [31].\n2.4. Fine-tuning on Question Answering\nThis is the last block in Fig. 1, with details in Fig. 3 (b). Here\nthe downstream QA task is ï¬ne-tuned to minimize the loss for\npredicting the correct start/end positions of the answer span, as\nproposed in BERT [10]. By introducing a start and an end vec-\ntor S, Eâˆˆ RH, we compute a dot product for S or E with\neach ï¬nal hidden vector Ti âˆˆRH for audio word i from the\nSpeechBERT\n[CLS] Which [MASK]Â ...Â  50? [SEP]\nE[CLS] EWhich E[SEP]E50E[M] ... A1 A2 A[M] AnAn-1A4 A[M]...\nT[CLS] TWhich T[SEP]T50T[M] ... T1 T2 T[M] TnTn-1T4 T[M]...\nText Question Audio Context\n[MASK][MASK]\nNFL game game\nMasked Audio LabelsÂ Masked LMÂ \nWord Embedding Lookup Audio Encoder\n(a) Pre-training Stage\nSpeechBERT\n[CLS] Which NFL ...Â  50? [SEP]\nWord Embedding Lookup Audio Encoder\nE[CLS] EWhich E[SEP]E50ENLF ... A1 A2 A3 AnAn-1A4 An-2...\nT[CLS] TWhich T[SEP]T50TNLF ... T1 T2 T3 TnTn-1T4 Tn-2...\nStart End\nText Question Audio Context (b) Fine-tuning Stage\nFigure 3: Two training stages for the SpeechBERT model: (a) Pre-training and (b) Fine-tuning. The two stages use identical model\narchitecture except for the output layers. The special tokens [CLS] and [SEP] are added following the original Text BERT.\nâ€¦.bythe brevityrockgroup called play with special â€¦.\nâ€¦.bythe Britishrockgroup Coldplaywith special â€¦.AnswerSpan\nPredictedSpanForcedAlignmentwithASRTranscriptions\nForcedAlignmentwithGroundTruthText\nOverlappingSpan\nğ´ğ‘‚ğ‘†= |ğ‘‚ğ‘£ğ‘’ğ‘Ÿğ‘™ğ‘ğ‘ğ‘ğ‘–ğ‘›ğ‘”ğ‘†ğ‘ğ‘ğ‘›||ğ´ğ‘›ğ‘ ğ‘¤ğ‘’ğ‘Ÿğ‘†ğ‘ğ‘ğ‘›âˆªğ‘ƒğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ğ‘†ğ‘ğ‘ğ‘›|ğ¹!=2Ã—ğ‘ƒÃ—ğ‘…ğ‘ƒ+ğ‘…\nğ‘ƒ=|ğ‘‚ğ‘£ğ‘’ğ‘Ÿğ‘™ğ‘ğ‘ğ‘ğ‘–ğ‘›ğ‘”ğ‘†ğ‘ğ‘ğ‘›||ğ‘ƒğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ğ‘†ğ‘ğ‘ğ‘›| ğ‘…=|ğ‘‚ğ‘£ğ‘’ğ‘Ÿğ‘™ğ‘ğ‘ğ‘ğ‘–ğ‘›ğ‘”ğ‘†ğ‘ğ‘ğ‘›||ğ´ğ‘›ğ‘ ğ‘¤ğ‘’ğ‘Ÿğ‘†ğ‘ğ‘ğ‘›|\nFigure 4: Illustration of the way to evaluate frame-level F1-\nscore and AOS on SQuAD-lost. If the predicted span overlaps\nwell with the ground truth answer span, high F1 and AOS will\nbe obtained even with recognition errors.\nSpeechBERT, as in Fig. 3 (b). The dot product value is softmax-\nnormalized over all audio words in the utterance to compute the\nprobability of audio word i being the start or end position.\n3. Experimental Setup\n3.1. Data and Evaluation\nThe SpeechBERT model was trained on the Spoken SQuAD\ndataset [14], in which the text for all audio paragraphs and\nall questions are from original SQuAD [12] dataset. It also\nincludes SQuAD format ASR transcripts plus 37k question-\nanswer pairs as the training set and 5.4k as the testing set. This\nis smaller than the ofï¬cial SQuAD dataset (with 10.6k ques-\ntions in its development set) since 5.2k of questions in SQuAD\nfor which the answers couldnâ€™t be found in the ASR transcripts\nwere removed and not shown in the Spoken SQuAD testing set.\nThese removed questions in SQuAD were collected to form an-\nother testing set referred to as SQuAD-lost (lost by ASR er-\nrors). This is helpful below in evaluating whether the end-to-\nend approach can better handle the questions with incorrectly\nrecognized answer spans. All questions in SQuAD-lost share\nthe same audio ï¬les as the Spoken SQuAD testing set.\nThe evaluation metrics we used are the Exact Matched\n(EM) percentage and F1 score for the word tokens as in the nor-\nmal QA tasks, but for SQuAD-lost frame-level F1 and Audio\nOverlapping Score (AOS) [14] based on the boundaries of the\nanswer spans as illustrated in Figure 4 were used instead. For\nthe latter case, the boundaries for the audio words were found by\nforced alignment with Kaldi [32], based on which the start/end\npoints of the answer for both the ground truth and predicted\nresults in training and testing sets were obtained. The ground\ntruth text of the testing set was used only in evaluation.\n3.2. Model Setting and Training\n3.2.1. Initial Phonetic-semantic Joint Embedding\nFor the autoencoder in Fig. 2, we used a bidirectional LSTM as\nthe encoder and a single-directional LSTM as the decoder, both\nwith input size 39 (MFCC-dim) and hidden size 768 (BERT\nembedding-dim). Two layers of the fully-connected network\nare added at the encoder output to transform the encoded vectors\nto ï¬t the BERT embedding space. We directly used the audio\nfrom Spoken SQuAD training set to train this autoencoder.\n3.2.2. Text BERT and SpeechBERT\nWe used the PyTorch implementation 1 of BERT to build the\nBERT model with 12 layers of bert-base-uncased set-\nting. We randomly initialized a new embedding layer with\nour vocabulary set counted in the dataset rather than using the\nWordPiece tokenizer [33] in processing the text because it is\ninconsistent with the audio word units. The ofï¬cial pre-trained\nweights were loaded as the weights of the BERT model. We ï¬rst\ntrained the MLM task with the text of Spoken SQuAD train-\ning set for three epochs. Next, we used the Spoken SQuAD\ntraining set by directly feeding text and audio into the BERT\nmodel. After pre-training, we ï¬ne-tuned the model with the\nSpoken SQuAD training set for another two epochs. The other\nhyper-parameters used were identical to the original PyTorch\nimplementation1.\n4. Experimental Results\n4.1. Spoken SQuAD (no ASR errors in answer spans)\nIn Table 1 Section (I) is for models trained on text data, with\nrows (a)(b)(c) for different QA models trained on the same\nSQuAD (in text), while column (A)(B) respectively for test-\ning sets of SQuAD (text) and Spoken SQuAD (ASR). Rows\n(a)(b)(c) showed the prior arts and the superiority of BERT\n(rows (c) vs (a)(b)), and the serious performance degradation\nfor these models when tested on ASR output directly (column\n(B) vs (A)). Row (d) is for BERT trained on ASR transcriptions\nof Spoken SQuAD, or the â€œcascadeâ€ of ASR and BERT, where\nwe see BERT performed much better for ASR output if trained\non ASR output (rows (d) vs (c) for column (B)).\nSection (II) is for the end-to-end QA model proposed here,\nwith row (e) for the SpeechBERT trained on Spoken SQuAD.\nWe see the end-to-end model is still 4-5% lower than the â€œcas-\ncadeâ€ architecture (row (e) vs (d)), although much better or\n1https://github.com/huggingface/transformers\nTable 1: Experimental results on Spoken SQuAD. Sections\n(I)(II)(III) are respectively for models trained on text, end-to-\nend models trained on audio, and ensembled models; while col-\numn (A)(B) are respectively for text and ASR testing sets.\nModels and Training set Testing Set\n(A) Text (B) ASR\n(I) trained on text EM F1 EM F1\n(a) BiDAF on Text [34] 58.40 69.90 37.02 50.90\n(b) Dr.QA on Text [35] 62.84 73.74 41.16 54.51\n(c) BERT on Text [10] 76.90 85.71 53.30 66.17\n(d) BERT on ASR [10](cascade) - - 56.28 68.22\n(II) End-to-end trained on Audio EM F1\n(e) SpeechBERT (proposed) 51.19 64.08\n(f) SpeechBERT w/o MLM 46.02 59.62\n(g) SpeechBERT tested on better boundaries 53.42 66.27\n(III) Ensembled models EM F1\n(h) ensembled [(e) plus (d)] 60.37 71.75\n(i) ensembled [(d) plus (d)] 57.88 69.29\nTable 2: Experimental results on SQuAD-lost, Spoken SQuAD\nand Total for the cascade and end-to-end models.\nModel Spoken SQuAD SQuAD-lost Total\nF1 AOS F1 AOS F1 AOS\nCascade (d) 66.56 63.87 30.78 27.58 48.74 45.84\nEnd-to-end (e) 62.76 59.70 37.31 33.57 50.12 46.72\ncomparable to prior models directly used on ASR output (rows\n(e) vs (a)(b)(c)). Considering the high level of the difï¬culty for\nthe end-to-end SQA model to learn the sophisticated semantic\nknowledge out of the relatively long audio signals directly in\none model without using the word tokens from ASR, the results\nhere showed the promising potential for end-to-end approaches\nfor SQA when better data, better model and better training be-\ncome possible.\nWhen we further ensembled the end-to-end model in row\n(e) with the cascade architecture in row (d) as shown in row\n(h) of Section (III), we see signiï¬cantly improved performance\ncompared to the two component models (rows (h) vs (d) or\n(e)), achieving the state-of-the-art result on Spoken SQuAD.\nNote that we can also ensemble two separately trained cascade\nmodels [(d) plus (d)] as shown in row (i), but the achievable\nimprovement was much less (row (i) vs (h)). These results\nshowed that the end-to-end SpeechBERT can learn extra knowl-\nedge complementary to that learned by the cascade architecture.\nFurthermore, the results in row (h) and column (B) trained and\ntested on audio are already higher than those in row (a) and col-\numn (A) on ground truth text and comparable to those in row\n(b) and column (A), although still much lower than row (c) and\ncolumn (A). This implies that the results obtained here are sub-\nstantial and signiï¬cant.\n4.2. SQuAD-lost (with ASR errors in answer spans)\nWe wish to ï¬nd out further with the end-to-end model if we\ncan better handle the questions with incorrectly recognized an-\nswer spans. Here we used the frame-level F1 and AOS as il-\nlustrated in Fig. 4 for evaluation and the results for cascade\n(row (d) in Table 1) and end-to-end (row (e) in Table 1) are\nin Table 2, SQuAD-lost (incorrectly recognized answer spans),\n71.06\n62.07\n57.99\n50.63\n45.63\n38.94\n36.54 36.68\n70.07\n61.06\n56.24 51.83\n45.96\n40.49 39.64\n44.61\nDocument Word Error Rate (%)\nFrame-level F1 score\n30.0\n35.0\n40.0\n45.0\n50.0\n55.0\n60.0\n65.0\n70.0\n75.0\n20 40 60 80\nBERT on ASR (Cascade)    (d)                              \nSpeechBERT (End-to-end) (e)                                                                          \nFigure 5: Frame-level F1 scores evaluated for small groups of\nTotal (Spoken SQuAD/SQuAD-lost) at different levels of WER.\nSpoken SQuAD (same as in Table 1, all with correctly recog-\nnized answer spans), and the total of the two. The results show\nthe end-to-end models did signiï¬cantly better on SQuAD-lost\n(middle), although worse by a gap on Spoken SQuAD (left),\nbut offered some improvements on the total (right). This veri-\nï¬ed the end-to-end model could learn some phonetic-semantic\nknowledge directly from audio signals before errors occurred in\nASR, and explained indirectly why the ensembled model ([(e)\nplus (d)] in row (h)) of Table 1 can do better than cascade alone\n(row (d)). The scenario on the total on the right of Table 2 was\ncloser to the real-world applications.\n4.3. Further Analysis\n4.3.1. Ablation MLM pre-training and Better Word Boundaries\nThe results in row (f) of Section (II) in Table 1 are for the end-\nto-end model trained directly with SQA ï¬ne-tuning without the\nMLM pre-training, and signiï¬cant performance drop can be ob-\nserved (rows (f) vs (e)). Also, the results in row (e) of Ta-\nble 1 are for audio word boundaries obtained by forced align-\nment using ASR transcripts with WER of 22.73% in Spoken\nSQuAD [14]. We further tested the model on better boundaries\nobtained with forced alignment using ground truth text. By do-\ning so, we can see the extent to which performance is limited by\nsegmentation quality. The results in row (g) showed some im-\nprovements could still be achieved with better boundaries (rows\n(g) vs (e)), which can be a direction for future work.\n4.3.2. Analysis on WER of ASR\nTo investigate how the cascade and end-to-end models (rows (d)\nand (e) in Table 1) worked with audio at different WER, we split\nthe questions in the total dataset including both SQuAD-lost\nand Spoken SQuAD into smaller groups with different WER.\nThe frame-level F1 results for these groups were plotted in Fig-\nure 5. Obviously, at lower WER both models offered higher\nF1, and the cascade architecture performed better. Both models\nsuffered from performance degradation at higher WER, and the\nend-to-end model outperformed cascade when WER exceeded\n40% since it never relied on ASR output.\n5. Concluding Remarks\nAudio signals are in form of phonetic structures, while carry-\ning semantics. ASR has long been used to transform the pho-\nnetic structures into word-level tokens carrying semantics, but\nwith inevitable errors causing troubles to the downstream ap-\nplication tasks. With reasonable performance of the end-to-end\nSQA task, the proposed SpeechBERT was shown to be able to\ndo similar transformation, but the semantics were somehow di-\nrectly tuned to the downstream task, or question answering here,\nbypassing the intermediate problem of ASR errors. This con-\ncept (and the SpeechBERT) is deï¬nitely useful to many other\nspoken language processing tasks to be studied in the future.\n6. Acknowledgements\nWe are grateful to the National Center for High-performance\nComputing for computer time and facilities.\n7. References\n[1] A. B Â´erard, O. Pietquin, C. Servan, and L. Besacier, â€œListen and\ntranslate: A proof of concept for end-to-end speech-to-text trans-\nlation,â€NIPS End-to-end Learning for Speech and Audio Process-\ning Workshop, 2016.\n[2] L.-s. Lee, J. Glass, H.-y. Lee, and C.-a. Chan, â€œSpoken content\nretrievalbeyond cascading speech recognition with text retrieval,â€\nIEEE/ACM Transactions on Audio, Speech, and Language Pro-\ncessing, vol. 23, no. 9, pp. 1389â€“1420, 2015.\n[3] B.-R. Lu, F. Shyu, Y .-N. Chen, H.-Y . Lee, and L.-S. Lee, â€œOrder-\npreserving abstractive summarization for spoken content based\non connectionist temporal classiï¬cation,â€Proc. Interspeech 2017,\npp. 2899â€“2903, 2017.\n[4] D. Serdyuk, Y . Wang, C. Fuegen, A. Kumar, B. Liu, and Y . Ben-\ngio, â€œTowards end-to-end spoken language understanding,â€ in\n2018 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP). IEEE, 2018, pp. 5754â€“5758.\n[5] P. Haghani, A. Narayanan, M. Bacchiani, G. Chuang, N. Gaur,\nP. Moreno, R. Prabhavalkar, Z. Qu, and A. Waters, â€œFrom audio\nto semantics: Approaches to end-to-end spoken language under-\nstanding,â€ in 2018 IEEE Spoken Language Technology Workshop\n(SLT). IEEE, 2018, pp. 720â€“726.\n[6] Y .-P. Chen, R. Price, and S. Bangalore, â€œSpoken language un-\nderstanding without speech recognition,â€ in 2018 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2018, pp. 6189â€“6193.\n[7] E. Palogiannidi, I. Gkinis, G. Mastrapas, P. Mizera, and T. Stafy-\nlakis, â€œEnd-to-end architectures for asr-free spoken language un-\nderstanding,â€ in ICASSP 2020-2020 IEEE International Con-\nference on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2020, pp. 7974â€“7978.\n[8] R. Price, â€œEnd-to-end spoken language understanding without\nmatched language speech model pretraining data,â€ in ICASSP\n2020-2020 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP). IEEE, 2020, pp. 7979â€“7983.\n[9] Y . Huang, H. Kuo, S. Thomas, Z. Kons, K. Audhkhasi, B. Kings-\nbury, R. Hoory, and M. Picheny, â€œLeveraging unpaired text\ndata for training end-to-end speech-to-intent systems,â€ inICASSP\n2020 - 2020 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), 2020, pp. 7984â€“7988.\n[10] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, â€œBert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,â€ in Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies, Volume 1 (Long and Short\nPapers), 2019, pp. 4171â€“4186.\n[11] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever,\nâ€œImproving language understanding by generative pre-training,â€\n2018.\n[12] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, â€œSquad:\n100,000+ questions for machine comprehension of text,â€ in Pro-\nceedings of the 2016 Conference on Empirical Methods in Natural\nLanguage Processing, 2016, pp. 2383â€“2392.\n[13] P. Rajpurkar, R. Jia, and P. Liang, â€œKnow what you dont know:\nUnanswerable questions for squad,â€ in Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics\n(Volume 2: Short Papers), 2018, pp. 784â€“789.\n[14] C.-H. Lee, S.-L. Wu, C.-L. Liu, and H.-y. Lee, â€œSpoken squad: A\nstudy of mitigating the impact of speech recognition errors on lis-\ntening comprehension,â€ Proc. Interspeech 2018, pp. 3459â€“3463,\n2018.\n[15] Y .-A. Chung, C.-C. Wu, C.-H. Shen, H.-Y . Lee, and L.-S. Lee,\nâ€œAudio word2vec: Unsupervised learning of audio segment repre-\nsentations using sequence-to-sequence autoencoder,â€ Interspeech\n2016, pp. 765â€“769, 2016.\n[16] Y .-A. Chung and J. Glass, â€œSpeech2vec: A sequence-to-sequence\nframework for learning word embeddings from speech,â€Proc. In-\nterspeech 2018, pp. 811â€“815, 2018.\n[17] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,\nâ€œDistributed representations of words and phrases and their com-\npositionality,â€ in Advances in neural information processing sys-\ntems, 2013, pp. 3111â€“3119.\n[18] Y .-H. Wang, H.-y. Lee, and L.-s. Lee, â€œSegmental audio\nword2vec: Representing utterances as sequences of vectors with\napplications in spoken term detection,â€ in 2018 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2018, pp. 6269â€“6273.\n[19] Y .-C. Chen, C.-H. Shen, S.-F. Huang, H.-y. Lee, and L.-s. Lee,\nâ€œAlmost-unsupervised speech recognition with close-to-zero re-\nsource based on phonetic structures learned from very small un-\npaired speech and text data,â€ arXiv preprint arXiv:1810.12566,\n2018.\n[20] Y .-A. Chung, W.-H. Weng, S. Tong, and J. Glass, â€œUnsupervised\ncross-modal alignment of speech and text embedding spaces,â€ in\nAdvances in Neural Information Processing Systems, 2018, pp.\n7354â€“7364.\n[21] A. T. Liu, S. wen Yang, P.-H. Chi, P. chun Hsu, and H. yi Lee,\nâ€œMockingjay: Unsupervised speech representation learning with\ndeep bidirectional transformer encoders,â€ 2019.\n[22] A. Baevski, S. Schneider, and M. Auli, â€œvq-wav2vec: Self-\nsupervised learning of discrete speech representations,â€ Interna-\ntional Conference on Learning Representations, 2020.\n[23] D. Jiang, X. Lei, W. Li, N. Luo, Y . Hu, W. Zou, and X. Li, â€œIm-\nproving transformer-based speech recognition using unsupervised\npre-training,â€ arXiv preprint arXiv:1910.09932, 2019.\n[24] X. Song, G. Wang, Z. Wu, Y . Huang, D. Su, D. Yu, and H. Meng,\nâ€œSpeech-xlnet: Unsupervised acoustic model pretraining for self-\nattention networks,â€arXiv preprint arXiv:1910.10387, 2019.\n[25] Z. Fan, S. Zhou, and B. Xu, â€œUnsupervised pre-traing for\nsequence to sequence speech recognition,â€ arXiv preprint\narXiv:1910.12418, 2019.\n[26] S. Ling, Y . Liu, J. Salazar, and K. Kirchhoff, â€œDeep contextualized\nacoustic representations for semi-supervised speech recognition,â€\narXiv preprint arXiv:1912.01679, 2019.\n[27] G. Lample and A. Conneau, â€œCross-lingual language model pre-\ntraining,â€ arXiv preprint arXiv:1901.07291, 2019.\n[28] M. Joshi, D. Chen, Y . Liu, D. S. Weld, L. Zettlemoyer, and\nO. Levy, â€œSpanbert: Improving pre-training by representing and\npredicting spans,â€ Transactions of the Association for Computa-\ntional Linguistics, vol. 8, pp. 64â€“77, 2020.\n[29] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and\nQ. V . Le, â€œXlnet: Generalized autoregressive pretraining for lan-\nguage understanding,â€ inAdvances in neural information process-\ning systems, 2019, pp. 5754â€“5764.\n[30] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer, and V . Stoyanov, â€œRoberta: A\nrobustly optimized bert pretraining approach,â€ arXiv preprint\narXiv:1907.11692, 2019.\n[31] L. Lugosch, M. Ravanelli, P. Ignoto, V . S. Tomar, and Y . Ben-\ngio, â€œSpeech model pre-training for end-to-end spoken language\nunderstanding,â€ Proc. Interspeech 2019, pp. 814â€“818, 2019.\n[32] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,\nN. Goel, M. Hannemann, P. Motlicek, Y . Qian, P. Schwarzet al.,\nâ€œThe kaldi speech recognition toolkit,â€ in IEEE 2011 workshop\non automatic speech recognition and understanding, no. CONF.\nIEEE Signal Processing Society, 2011.\n[33] Y . Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi, W. Macherey,\nM. Krikun, Y . Cao, Q. Gao, K. Machereyet al., â€œGoogleâ€™s neural\nmachine translation system: Bridging the gap between human and\nmachine translation,â€ arXiv preprint arXiv:1609.08144, 2016.\n[34] M. Seo, A. Kembhavi, A. Farhadi, and H. Hajishirzi, â€œBidirec-\ntional attention ï¬‚ow for machine comprehension,â€ in Interna-\ntional Conference on Learning Representations, 2017.\n[35] D. Chen, A. Fisch, J. Weston, and A. Bordes, â€œReading wikipedia\nto answer open-domain questions,â€ in Proceedings of the 55th\nAnnual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), 2017, pp. 1870â€“1879.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8511788249015808
    },
    {
      "name": "Question answering",
      "score": 0.7929236888885498
    },
    {
      "name": "End-to-end principle",
      "score": 0.6967902779579163
    },
    {
      "name": "Natural language processing",
      "score": 0.6470247507095337
    },
    {
      "name": "Language model",
      "score": 0.630060076713562
    },
    {
      "name": "Task (project management)",
      "score": 0.6017151474952698
    },
    {
      "name": "Spoken language",
      "score": 0.5863142013549805
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5466199517250061
    },
    {
      "name": "Speech recognition",
      "score": 0.400054931640625
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}