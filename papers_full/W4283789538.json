{
  "title": "Adversarial Data Augmentation for Task-Specific Knowledge Distillation of Pre-trained Transformers",
  "url": "https://openalex.org/W4283789538",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5077768924",
      "name": "Minjia Zhang",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5034914961",
      "name": "Niranjan Uma Naresh",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5040302174",
      "name": "Yuxiong He",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2979567256",
    "https://openalex.org/W2798966449",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6751608199",
    "https://openalex.org/W2948099658",
    "https://openalex.org/W6769310274",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W3035688398",
    "https://openalex.org/W1945616565",
    "https://openalex.org/W6779590286",
    "https://openalex.org/W3005617766",
    "https://openalex.org/W6638523607",
    "https://openalex.org/W2738015883",
    "https://openalex.org/W3035204084",
    "https://openalex.org/W2974875810",
    "https://openalex.org/W6772383348",
    "https://openalex.org/W2803751389",
    "https://openalex.org/W2922565841",
    "https://openalex.org/W6776215604",
    "https://openalex.org/W2640329709",
    "https://openalex.org/W3015233032",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2284660317",
    "https://openalex.org/W2969515962",
    "https://openalex.org/W3015298864",
    "https://openalex.org/W2924902521",
    "https://openalex.org/W2969601108",
    "https://openalex.org/W2970820321",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W2946232455",
    "https://openalex.org/W3204510963",
    "https://openalex.org/W3113565477",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W2911588830",
    "https://openalex.org/W6757112735",
    "https://openalex.org/W6771245373",
    "https://openalex.org/W2995926704",
    "https://openalex.org/W3127787589",
    "https://openalex.org/W3006647218",
    "https://openalex.org/W4288333985",
    "https://openalex.org/W3034368386",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W3046550781",
    "https://openalex.org/W3174544005",
    "https://openalex.org/W4293846201",
    "https://openalex.org/W3007685714",
    "https://openalex.org/W2963823140",
    "https://openalex.org/W3195710602",
    "https://openalex.org/W4287813862",
    "https://openalex.org/W4288102732",
    "https://openalex.org/W2963636205",
    "https://openalex.org/W3035743198",
    "https://openalex.org/W2963545917",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W2997666887",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2962718684",
    "https://openalex.org/W2963969878",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W2905266130",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W3004127093",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2923014074"
  ],
  "abstract": "Deep and large pre-trained language models (e.g., BERT, GPT-3) are state-of-the-art for various natural language processing tasks. However, the huge size of these models brings challenges to fine-tuning and online deployment due to latency and cost constraints. Existing knowledge distillation methods reduce the model size, but they may encounter difficulties transferring knowledge from the teacher model to the student model due to the limited data from the downstream tasks. In this work, we propose AD^2, a novel and effective data augmentation approach to improving the task-specific knowledge transfer when compressing large pre-trained transformer models. Different from prior methods, AD^2 performs distillation by using an enhanced training set that contains both original inputs and adversarially perturbed samples that mimic the output distribution from the teacher. Experimental results show that this method allows better transfer of knowledge from the teacher to the student during distillation, producing student models that retain 99.6\\% accuracy of the teacher model while outperforming existing task-specific knowledge distillation baselines by 1.2 points on average over a variety of natural language understanding tasks. Moreover, compared with alternative data augmentation methods, such as text-editing-based approaches, AD^2 is up to 28 times faster while achieving comparable or higher accuracy. In addition, when AD^2 is combined with more advanced task-agnostic distillation, we can advance the state-of-the-art performance even more. On top of the encouraging performance, this paper also provides thorough ablation studies and analysis. The discovered interplay between KD and adversarial data augmentation for compressing pre-trained Transformers may further inspire more advanced KD algorithms for compressing even larger scale models.",
  "full_text": "Adversarial Data Augmentation for Task-Speciﬁc Knowledge\nDistillation of Pre-trained Transformers\nMinjia Zhng, Niranjan Uma Naresh, Yuxiong He\nMicrosoft Corporation\nBellevue, Washington 98004\n{minjiaz,Niranjan.Uma,yuxhe}@microsoft.com\nAbstract\nDeep and large pre-trained language models (e.g., BERT, GPT-\n3) are state-of-the-art for various natural language processing\ntasks. However, the huge size of these models brings chal-\nlenges to ﬁne-tuning and online deployment due to latency\nand cost constraints. Existing knowledge distillation methods\nreduce the model size, but they may encounter difﬁculties\ntransferring knowledge from the teacher model to the student\nmodel due to the limited data from the downstream tasks. In\nthis work, we propose AD 2, a novel and effective data aug-\nmentation approach to improving the task-speciﬁc knowledge\ntransfer when compressing large pre-trained transformer mod-\nels. Different from prior methods, AD2 performs distillation\nby using an enhanced training set that contains both original\ninputs and adversarially perturbed samples that mimic the out-\nput distribution from the teacher. Experimental results show\nthat this method allows better transfer of knowledge from the\nteacher to the student during distillation, producing student\nmodels that retain 99.6% accuracy of the teacher model while\noutperforming existing task-speciﬁc knowledge distillation\nbaselines by 1.2 points on average over a variety of natural\nlanguage understanding tasks. Moreover, compared with alter-\nnative data augmentation methods, such as text-editing-based\napproaches, AD2 is up to 28 times faster while achieving com-\nparable or higher accuracy. In addition, when AD2 is combined\nwith more advanced task-agnostic distillation, we can advance\nthe state-of-the-art performance even more. On top of the\nencouraging performance, this paper also provides thorough\nablation studies and analysis. The discovered interplay be-\ntween KD and adversarial data augmentation for compressing\npre-trained Transformers may further inspire more advanced\nKD algorithms for compressing even larger scale models.\nIntroduction\nThere has been a huge paradigm shift in AI: large-scale foun-\ndation models (Bommasani et al. 2021), such as BERT (De-\nvlin et al. 2019) and GPT-3 (Brown et al. 2020), are trained on\nmassive data at scale and then are adapted to a wide range of\ndifferent domains with additional task-speciﬁc data. One in-\nteresting trend of these foundation models is their sizes grow\nat an unprecedented speed, from a few hundred million pa-\nrameters (e.g., BERT) to over one hundred billion parameters\n(e.g., GPT-3), a three-orders-of-magnitude increase. Recent\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nstudies from OpenAI have shown that the model scale has\nbeen increasing exponentially with roughly a 3.4-month dou-\nbling time and the performance of these models continues\nto improve with their sizes (Kaplan et al. 2020). Despite\ntheir remarkable performance in accuracy, huge challenges\nhave been raised when deploying applications on top of these\nfoundation models due to latency and capacity constraints.\nOne effective approach for reducing the model size is\nknowledge distillation (KD) (Hinton, Vinyals, and Dean\n2015), where a stronger model (called teacher) guides the\nlearning of another small model (called student) with an ob-\njective to minimize the discrepancy between the teacher and\nstudent outputs. Since its debut, KD has been extensively\napplied to computer vision and NLP tasks (Wang and Yoon\n2020; Gou et al. 2021). On the NLP side, several variants\nof KD have been proposed to compress BERT (Devlin et al.\n2019), including how to deﬁne the knowledge that is sup-\nposed to be transferred from the teacher BERT model to the\nstudent variations. Examples of such knowledge deﬁnitions\ninclude output logits (e.g., DistilBERT (Sanh et al. 2019))\nand intermediate knowledge such as feature maps (Sun et al.\n2019; Aguilar et al. 2020; Zhao et al. 2021) and self-attention\nmaps (Wang et al. 2020b; Sun et al. 2020) (we refer KD\nusing these additional knowledge as deep knowledge distilla-\ntion (Wang et al. 2020b)). Unfortunately, the gap between the\nteacher and the student is sometimes large, even with deep\ndistillation, especially when the downstream task data is lim-\nited. To mitigate the accuracy gap, existing work has explored\napplying knowledge distillation in the more expensive pre-\ntraining stage, which aims to provide a better initialization\nto the student for adapting to downstream tasks. As an ex-\nample, MiniLM (Wang et al. 2020b) and MobileBERT (Sun\net al. 2020) advance the state-of-the-art by applying deep\nknowledge distillation and architecture change to pre-train\na student model on the general-domain corpus, which then\ncan be directly ﬁne-tuned on downstream tasks with good\naccuracy. TinyBERT (Jiao et al. 2019) proposes to perform\ndeep distillation in both the pre-training and ﬁne-tuning stage\nand shows that these two stages of knowledge distillation are\ncomplementary to each other and can be combined to achieve\nstate-of-the-art results on GLUE tasks.\nDespite the great progress, there remains one big chal-\nlenge — while the pre-training distillation may beneﬁt from\nunsupervised learning over a huge amount of general domain\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n11685\ndata, the ﬁne-tuning distillation often has only limited labeled\ndata from the target domain for a certain task. Indeed, large\namounts of labeled data are usually prohibitive or expensive\nto obtain. Due to the limited data from the target task/domain,\nﬁne-tuning distillation can cause the adapted model overﬁt\nthe training data and therefore does not generalize well. Exist-\ning methods such as TinyBERT try to overcome this issue by\nenlarging the training datasets with text-editing-based data\naugmentation. However, such a method requires generating\nmultiple samples for each input to have adequate variations,\nwhich leads to excessive data augmentation time and drasti-\ncally increases the training cost. In this paper, we will show\nthat instead of doing text-editing data augmentation, we can\nachieve better distillation performance on low-resource down-\nstream tasks with much cheaper cost by the original KD loss\ncombined with a strong and more principled adversarial data\naugmentation scheme.\nOur Contributions. (1) We introduce AD2, a novel task-\nspeciﬁc knowledge distillation method for compressing pre-\ntrained Transformer networks via a strong adversarial data\naugmentation scheme. (2) We conduct a comprehensive com-\nparison of AD 2 with prior task-speciﬁc KD methods on a\nwide range of NLP tasks and demonstrate that AD2 teaches a\nstudent to get little or no loss in accuracy and retains 99.6%\naccuracy of the teacher model on average over GLUE bench-\nmark, outperforming existing task-speciﬁc knowledge dis-\ntillation methods by 1.2 points in accuracy under the same\ncompression ratio. (3) We also perform a comparison with an\nexisting text-edit-based data augmentation (DA) method used\nin TinyBERT (Jiao et al. 2019). Our results show that AD2\nachieves better accuracy than DA while being 8.6–28 times\nfaster than DA. (4) We show that our approach complements\nexisting pre-training distillation and enhances state-of-the-\nart pre-training distillation methods to advance the state-of-\nthe-art further (e.g., up to 1.1 points higher accuracy when\ncombined with MiniLM). (5) We perform detailed ablation\nstudies to assess the impact of our method.\nBackground and Related Work\nKnowledge distillation (KD) was ﬁrst introduced by (Bucila,\nCaruana, and Niculescu-Mizil 2006) and later generalized by\n(Hinton, Vinyals, and Dean 2015). It has been demonstrated\nas an empirically very successful technique. In particular, the\nchallenge of deploying large-scale pre-trained Transformer-\nbased language models (e.g., BERT) motivates many works\nto improve the knowledge distillation technique for Trans-\nformer models, by exploiting additional intermediate knowl-\nedge (Sun et al. 2019; Aguilar et al. 2020; Tang et al. 2019),\ntask-agnostic pre-training distillation (Wang et al. 2020b; Sun\net al. 2020), and multi-stage distillation (Wang et al. 2020b;\nJiao et al. 2019). In contrast to previous works, our goal in\nthis paper is to improve the task-speciﬁc KD performance\nfor pre-trained Transformer models where downstream task\ndata resources are limited with the help of adversarial data\naugmentation. We empirically show that this path is effective\nwhile being lightweight.\nOn a separate line of research, data augmentation has been\na prevailing technique to overcome overﬁtting and improve\ngeneralization. For example, in image classiﬁcation tasks,\ndata augmentation applies label-invariant transformations\n(such as cropping, ﬂipping, color jittering) to images in the\ntraining so that the model can learn representations robust\nto those nuisance factors. Text data augmentation has also\nbeen extensively studied in NLP. For example, prior work\nproposes to improve neural machine translation models with\nback-translation (Sennrich, Haddow, and Birch 2016). Other\nwork propose to replace words with other words that are\npredicted using a language model at the corresponding word\npositions (Kobayashi 2018; Wu et al. 2019). EDA proposes\nto augment text data through synonym replacement, ran-\ndom swap, random insertion, and random deletion, which\nshows improved performance on text classiﬁcation tasks\nusing LSTMs (Wei and Zou 2019). Unlike these methods,\nwhich focus on general data augmentation for NLP tasks, our\nwork is the ﬁrst to investigate the interplay between adver-\nsarial data augmentation and knowledge distillation loss for\ncompression of pre-trained Transformer models, with limited\ndata from downstream task.\nThe work most similar to our research is TinyBERT(Jiao\net al. 2019). TinyBERT uses a text-editing technique for data\naugmentation by randomly replacing words in a sentence\nwith their synonyms, based on their similarity measure on\nGloVe embeddings. It then uses the augmented dataset for\ntask-speciﬁc distillation of BERT models. While this kind of\naugmentation can keep semantics at the word level, it still has\none big limitation: to generate new sentence samples with\nadequate variations, it needs to sample multiple times. For ex-\nample, to achieve improved accuracy, the data augmentation\nused by (Jiao et al. 2019) increases training sets by a factor\nof 10–30, which not only leads to high augmentation cost\nbut also increases the distillation training cost by close to an\norder of magnitude. Unlike (Jiao et al. 2019), we consider a\nworst-case formulation over data distributions and propose an\nadversarial data augmentation method for distillation, which\nresults in better improvement while incurring a much cheaper\ncost on distilling task-speciﬁc student models.\nProposed Method\nProblem Statement\nConsider a pre-trained large-scale language model T\u0002(·)\nfor adapting to natural language understanding tasks, such\nas sentiment analysis, question and answering and se-\nmantic textual similarity, where the labeled input data\nis\n{xi;yi}N\ni=1; xi represents the ith input (typically sen-\ntences) where a special token [SEP] is used to indicate\nthe sentence boundary and a [CLS] symbol appended to\nthe front of the input used for tasks such as classiﬁca-\ntion, and yi is the corresponding ground-truth label. In the\nstandard ﬁne-tuning framework, the model T is initialized\nwith pre-trained parameters \u00020 and ﬁne-tuned with labeled\ndata by minimizing the task-speciﬁc objective (e.g., cross-\nentropy for classiﬁcation tasks):\nmin\u0002 E(x;y)\u0018DLce(\u0002) =\nmin\u0002\nPN\ni=1\nPjyij\nj=1 yijlog(softmax(T\u0002(xi))j). The large-\nscale (teacher) model has unwieldy computation and memory\nrequirements. Therefore, the goal is to learn a task-speciﬁc\nsmaller model, S (parameterized by \u0012), without degrading\nthe accuracy in comparison to the teacher model T\u0002.\n11686\nAdversarial Training\nAdversarial training has been proposed and studied exten-\nsively in the computer vision literature mainly for improving\nmodel robustness and withstand adversarial attacks (Good-\nfellow, Shlens, and Szegedy 2015; Madry et al. 2018). The\nkey idea is to apply small perturbation to input images that\nmaximizes the adversarial loss:\nmin\n\u0012\nE(x;y)\u0018D[ max\nk\u000ek\u0014\u000f\nl(f(x+ \u000e; \u0012);y)] (1)\nWhile adversarial training has been successfully mitigating\nadversarial attacks, traditional understanding is that adver-\nsarial training could hurt generalization performance. How-\never, there has been an increasing amount of attention paid\nto leverage adversarial training for better clean data perfor-\nmance (Xie et al. 2020; Zhu et al. 2020; Gan et al. 2020).\nIn particular, there are some studies show that adversarial\ntraining helps improve the generalizability of language mod-\neling (Cheng, Jiang, and Macherey 2019; Wang, Gong, and\nLiu 2019; Jiang et al. 2020; Liu et al. 2020). However, few\nworks have studied its interplay with knowledge distillation.\nGiven that both could improve model generalization, it poses\nthe question: to what extend does distillation of pre-trained\nTransformers beneﬁt from task-speciﬁc adversarial training?\nThe AD2 Algorithm\nAdversarial training is a form of data augmentation. In this\nsection, we introduce AD2, Adversarial Data Augmentation\nfor Distillation (AD2), to exploit adversarial data augmenta-\ntion techniques for knowledge distillation.\nDue to the intrinsic difference between the image and text\ndata, adversarial training methods for images cannot be di-\nrectly applied to the latter one. First, the image (e.g., pixels)\nis continuous-valued, but text data is discrete. In a pre-trained\nlanguage model, raw text data is ﬁrst vectorized, such as\none-hot encoding, before getting fed into the model. When\napplying gradient-based adversarial training method adopted\nfrom images on these representations, the generated adversar-\nial samples contain invalid tokens or word sequences (Alzan-\ntot et al. 2018). Second, while the perturbation of images is\nsmall changes of pixel values that are hard to be perceived\nby human eyes and label-preserving, word replacement (e.g.,\neven with synonyms) would generate syntactically incorrect\nsentences and may change the semantics of a sentence drasti-\ncally (Jia and Liang 2017).\nTo address this issue, we create adversarial samples by\napplying perturbations to the continuous lexical embeddings\nof inputs to the student model instead of directly to discrete\nwords or tokens. Using BERT as an example, for a raw input\nxi, we pass it to the lexical encoding layer of the student\nmodel gemb(xi) = LexicalEncoder(xi), which combines a\ntoken embedding (e.g., one-hot), a position embedding, and a\nsegment embedding through element-wise summation. Then\nwe add gemb(xi) with a vector\n\u000e\u0012 = arg max\nk\u000ek\u0014\u000f\n\u001e(S\u0012(gemb(xi) + \u000e);T\u0002(xi)) (2)\nwhich represents the worst-case perturbations against the\nteacher model’s outputs. We choose the teacher model’s out-\nput instead of the hard labels for two reasons. First, a data\naugmentation scheme has to supply corresponding labels as\nsupervisory information. Therefore, our data augmentation\nscheme needs not to worry about the labels as they are as-\nsigned by the teacher model. Second, the teacher’s output\nprovides richer information about the relationship between\nsamples. Therefore, we consider it a better reference point for\nthe adversarial direction, which is the direction in the input\nspace in which the label probability of the model is most\nsensitive to small perturbations. Finally, from a semantic-\npreserving perspective, we cannot perform very ”extreme”\ntransformations for data augmentation. Therefore, we restrict\nthe magnitude of the perturbation to \u000f(e.g., by simple clip-\nping), such that the perturbation lies within an L2-norm ball\nwith a radius of \u000f. For optimization, Equation 2 can be solved\nby project gradient ascent (PGA) (Madry et al. 2018), which\nis commonly used for large-scale constrained optimization.\nAfter we generate adversarial samples, unlike common\ndata augmentation where only the transformed inputs are fed\ninto the network, we pass both the original input xi and the\nadversarial sample x0\ni = gemb(xi) + \u000efor training (thus, the\nnumber of input samples during training is increased by a\nfactor of 2). The consideration of keeping both inputs is to\nmaintain the information path for the original input\nxi so\nthat we can easily see how the added information path x0\nleads to a different result. Figure 1 shows how AD2 applies\nadversarial token embeddings to the student and how they\nare used during distillation.\nFor the xi part, its loss is still the original KD loss (i.e.,\nLKD), which is a weighted sum of (1) the conventional cross-\nentropy loss between predictions and the given hard label and\n(2) the Kullback–Leibler divergence (KL) loss between the\npredictions and the teacher’s soft label. For thex0\nipart, we use\nthe KL divergence to calculate the adversarial data augmenta-\ntion loss (ADA), i.e., LADA(xi;\u000e; \u0012) := KL(S\u0012(gemb(xi) +\n\u000e);T\u0002(xi)) for classiﬁcation tasks. For regression tasks, both\nS and T output a scalar, and we set LADA as the squared\nloss, i.e., LADA(xi;\u000e; \u0012) := (S\u0012(gemb(xi) +\u000e) −T\u0002(xi))2.\nThus, our approach encourages a student network to produce\nthe softmax output from the teacher network when exposed to\nadversarial samples, i.e., to minimize the following objective:\nmin\n\u0012\nE(x;y)\u0018D[LKD(x; \u0012) + \u000bLAD2 (x0; \u0012)] (3)\nwhere \u000bis a hyperparameter that controls the trade-off be-\ntween KD loss from original data and adversarial data. In our\nexperiments, we set \u000b= 1 except otherwise noted.\nComputational cost of augmentation. Given that one pri-\nmary interest is to also improve the distillation efﬁciency,\nwe are motivated to also look into the computational cost of\nadversarial data augmentation. Generating adversarial exam-\nples requires K PGA iterations, where each iteration takes\napproximately the same time as making three forward passes\nthrough the network. This is because one step of PGA re-\nquires to make one forward and backward pass over the entire\nnetwork. When K is large, the data augmentation cost can\nstill be expensive. Inspired by prior work on reducing the\ntraining cost of adversarial training, we employ a variant of\nPGA called PGA-1 (Gupta, Dube, and Verma 2020) (K=1)\nto craft adversarial samples with one perturbation step. The\n11687\nTeacher \nTransformer \nEncoder (T)\nTeacher\nLexicon Encoder\nTok1 Tok2CLS Tok3 Tok4 SEP\nStudent\nLexicon Encoder\n+\nAdversarial \nNoise\nAdversarial \nEmbeddings\nLexical \nEmbeddings\nStudent Transformer \nEncoder (T)\nKD loss Maximization \nInput Data\nAugmented \nData\nSoft label\nFigure 1: AD2 architecture. A text input is ﬁrst fed to a teacher\nmodel to generate the soft labels. AD 2 then creates adver-\nsarial samples by applying perturbations to the continuous\nlexical embeddings of inputs. Both the original inputs and\nthe adversarial samples are passed to the student model for\nknowledge distillation.\nkey insight here is that we can often ﬁnd sufﬁciently good\nadversarial samples while being much more computationally\nefﬁcient with small K for many tasks. More advanced adver-\nsarial training approaches, such as Curriculum Adversarial\nTraining (CAT) (Cai, Liu, and Song 2018) and Annealing-\nbased Adversarial Training (Amata) (Ye et al. 2020), may\nhelp further reduce the adversarial training cost while main-\ntaining good accuracy by adjusting the strength of adversaries\nat different stages of the training. We leave the exploration of\nthese methods as future work.\nPractical considerations. Since we study the effectiveness\nof adversarial data augmentation for KD, we still use a com-\nmon temperature term tto control how much to rely on the\nteacher’s soft predictions, where we divide the logits of both\nstudent and teacher by t (e.g., byi = P(yi=t|xi)) for both\nraw data and adversarial samples during distillation. A high\ntemperature has the effect of generating a softer distribution\nof output probabilities among the classes (Hinton, Vinyals,\nand Dean 2015). And we scale the gradient of the loss with\nrespect to the model weights by a factor of t2 such that the\nrelative contributions of the loss term remain roughly un-\nchanged even when the temperature is adjusted.\nInspired by MobileBERT (Sun et al. 2020), which per-\nforms deep knowledge distillation of the teacher model dur-\ning the pre-training distillation, we let the student imitate the\nprediction output, feature maps, and self-attention maps with\nadversarial data augmentation at the task-speciﬁc distillation\nstage. We show that adversarial data augmentation is beneﬁ-\ncial to KD with and without deep knowledge distillation, but\nthey can be combined together to deliver better results. The\nfull procedure of AD2 is provided in Algorithm 1.\nAlgorithm 1: AD2\n1: Input: Teacher network T, coefﬁcient \u000b, temperature t,\nmaximum perturbation radius \u000f.\n2: Output: Converged model parameters \u0012of the distilled\nstudent network S\n3: for epoch∈1;2;:::;N epochs do\n4: for Each (x;y) ∈mini-batch (X;Y ) ∼Ddo\n5: by← Tt\n\u0002(x) .Soft labels from the teacher\n6: LCE ← CE(S\u0012(x);y) .The standard loss.\n7: LKD ← KL(St\n\u0012(x);by) .KD loss on clean data\n8: \u000e\u0012 = arg maxk\u000ek\u0014\u000f\u001e(S\u0012(gemb(x) + \u000e);by) .\nCompute data perturbation\n9: x0= gemb(x) + \u000e\u0012 .Create adversarial sample\n10: LADA ← KL(St\n\u0012(x0);by) .Calculate the KD\nloss on adversarial data\n11: LAD2 ←L CE + LKD + \u000bLADA .Final loss\n12: Update student model parameters\nExperiments\nIn this section, we describe our experiments on the proposed\nadversarial data augmentation for knowledge distillation.\nEvaluation Methodology\nDatasets. Following previous work on distilling pre-trained\nlanguage model (Sanh et al. 2019; Sun et al. 2019; Dong\net al. 2019), we evaluate the effectiveness of AD2 using the\nGLUE (General Language Understanding Evaluation) bench-\nmark (Wang et al. 2019), a collection of linguistic tasks in\ndifferent domains such as textual entailment, sentiment anal-\nysis, and question answering. It is designed to favor sample-\nefﬁcient learning and knowledge transfer across a range of\ndifferent linguistic tasks in different domains.\nExperimental settings. We focus our comparison under a\ntask-speciﬁc compression setting (Sun et al. 2019; Turc et al.\n2019) instead of a pretraining distillation setting (Sanh et al.\n2019; Wang et al. 2020b; Sun et al. 2020). That is, we do\nnot use the massive general domain corpus but only the\ntraining set of each task in GLUE to compress the model.\nThe reason is that we intend to straightforwardly verify the\neffectiveness of our adversarial data augmentation-based\ndistillation method. Moreover, our motivation comes from\nimproving compression efﬁciency for low-resource down-\nstream tasks (e.g., no longer than a few GPU hours for\nany task of GLUE). If task-speciﬁc distillation already pro-\nvides satisfactory accuracy, one may be spared from explor-\ning more time-consuming pre-training distillation schemes\n(e.g., pre-training distillation-based methods, such as Mobile-\nBERT (Sun et al. 2020) and MiniLM (Wang et al. 2020b),\ntake hundreds of GPU hours to obtain improved accuracy).\nThat said, we will show that our method can be combined\nwith pre-training distillation to achieve better results.\nImplementation Details. Previous work (Sanh et al. 2019;\nSun et al. 2019; Wang et al. 2020b) usually distil\nBERTbase (Devlin et al. 2019) into a 6-layer (BERT6) model\nwith 768 hidden size. To make the results comparable to\nother work, we conduct distillation experiments using the\nsame teacher and student architecture. We use the uncased\n11688\nversion of BERTbase 1 (denoted as BERT12), which consists\nof 12-layer Transformer blocks with 768 hidden dimension\nsize, and 12 attention heads, with about 109M parameters.\nWe ﬁne-tune BERTbase on a downstream task as the teacher\nmodel to compute soft labels for each task independently.\nWe initialize the student model (BERT6) with model weights\nfrom DistilBERT checkpoints 2. Note that DistilBERT still\nuses a pre-training distillation setting. We choose DistilBERT\nbecause it provides a better baseline for BERT-PKD than\ninitializing with weights selected from the teacher BERT.\nHyperparameters. In order to reduce the hyperparameter\nsearch space, we ﬁx the number of epochs as 6 for all the ex-\nperiments and tune the batch size from {16;32}and learning\nrate from {1e-5, 3e-5, 5e-5, 7e-5, 9e-5, 1e-4}for all conﬁg-\nurations on each task. The maximum sequence length is set\nto 512. We use a linear learning rate decay schedule with a\nwarm-up ratio of 0.1 for all experiments. We clip the gradi-\nent norm within 1. For AD2, we set the perturbation radius\n\u000f= 1e-5, PGA step size 1e-3, temperature t=1, and \u000b= 1.\nThe model with the best validation accuracy is selected for\neach task, and we report the median of 5 runs with different\nrandom seeds for each selected conﬁguration.\nExperimental Results\nWe ﬁrst compare the following schemes: (1) Fine-tune: we\ndirectly ﬁne-tune the student model on GLUE tasks to ob-\ntain a natural ﬁne-tuning baseline. (2) Vanilla KD: This is\nthe knowledge distillation in its purest form as in (Hinton,\nVinyals, and Dean 2015) and (Sanh et al. 2019). (3)BERT-\nPKD (Sun et al. 2019): a task-speciﬁc distillation technique\nthat exploits intermediate knowledge for better compression.\n(4) AD2: This is our approach as described in Algorithm 1,\nusing adversarial data augmentation. Table 1 shows the com-\nparison results. For a fair comparison, we reproduce results\nfor BERT-PKD, because several results on the GLUE devel-\nopment set were missed from their paper. Our reproduced\nresults of BERT-PKD are comparable and sometimes stronger\nthan the originally reported results in (Sun et al. 2019).\nImproving distillation accuracy with AD2: Overall, AD2\nretains 99.6% (83.6 vs. 83.9) of the BERT-base performance,\nand we have the following observations. (1) AD2 consistently\noutperforms the ﬁne-tuning baseline on every GLUE task and\nachieves 1.6 points higher accuracy (82.0 vs. 83.6), indicat-\ning that our method can effectively transfer knowledge from\nthe teacher model to the student. (2) Comparing AD 2 with\nvanilla KD and BERT-PKD, we see the proposed scheme of\nadopting adversarial data augmentation for distillation im-\nproves the accuracies of all teacher-student pairs. On 5 out of\nthe 8 pairs, the improvement is more than 1 point. Notably,\nwe observe that AD 2 achieves 1.1 points higher accuracy\nfor SST-2, 1.6 points for CoLA, 1.8/1.3 points for STS-B in\nPCC/SCC, 1.3 points for MRPC in accuracy, and 1.5 points\nfor RTE. (3) AD2 is particularly effective in improving KD\non low-resource datasets, which contain fewer samples (e.g.,\n<100K samples). We hypothesize that this is because AD2\n1https://huggingface.co/bert-base-uncased.\n2https://huggingface.co/distilbert-base-uncased.\nprovides a more diverse data view via strong adversarial data\naugmentation, such that more of the teacher’s knowledge\ncan get exposed to the student, which is challenging when\nthe number of samples is small. (4) We highlight that on\nSST-2 and MRPC, our method achieves nearly identical per-\nformance to BERT-base, and on QQP and RTE, our method\neven outperforms BERT-base by 0.4 points (90.8 vs. 91.2)\nand 4 points (64.2 vs. 68.2), respectively. This is presumably\nbecause adversarial data augmentation can effectively help\nprevent overﬁtting of the student model on downstream tasks,\nleading to improved generalization. (5) Finally, on both large\ndatasets with more than 100K samples (e.g., MNLI, QQP,\nQNLI) and low-resource datasets, AD2 achieves consistent\nimprovements, verifying the robustness of our approach.\nExploring alternative data augmentation schemes. It\nhas been show that adversarial data augmentation improves\naccuracy of KD. In Figure 2, we compare AD2 with alterna-\ntive data augmentation method. Following TinyBERT (Jiao\net al. 2019), we employ its text-editing technique (e.g., syn-\nonym replacement) and use the recommended parameters\nlisted in their code repository {pt=0.4, Na=[10 (MNLI,\nQQP), 20 (QNLI, SST-2), 30 (CoLA, MRPC, RTE)], K=15}\nfor compressing BERT models. We notice that text-editing-\nbased data augmentation only seems to provide sparse and\ninconsistent improvements on GLUE tasks (e.g., we observe\nworse performance on MRPC, SST2 with text-edit-based DA)\nfor the BERT6 student model, despite with several hyperpa-\nrameter tuning. In those cases, we report the best accuracy we\nobserve from ﬁne-tuning TinyBERT checkpoint or through\ntask-speciﬁc distillation but with clean data.\nBy our analysis, AD2 boosts KD performance more than\nTinyBERT on all tasks except RTE (the text-edit-based data\naugmentation does bring extra accuracy improvement for\nRTE). Our approach provides better accuracy because the\nreplacement-based augmentation with synonyms can only\nproduce limited diverse patterns from the original texts, and\nit is almost impossible to leverage all the possible candidates\ndue to the large vocabulary size in languages. In contrast, we\nconsider a worst-case formulation over data distributions in\nthe semantic space. Thus, AD 2 augments the dataset with\nexamples that are ”hard” under the current model. We ﬁnd\nthat such stronger data augmentation can efﬁciently transfer\nteacher knowledge to the student.\nOn the other hand, while it takes TinyBERT 85 hours on\none NVIDIA V100 GPU to perform task-speciﬁc distillation\non MNLI, it takes 5.9 hours for AD 2 to achieve higher ac-\ncuracy (83.5 vs. 83.7). The task-speciﬁc distillation part in\nTinyBERT is slow because it needs to perform a kNN search\nover the GloVe embeddings to ﬁnd synonyms for a word\nreplacement, which can be extremely expensive for large vo-\ncabulary and high embedding dimensions. Furthermore, the\ndata augmentation scheme in TinyBERT increases the train-\ning set by at least a factor of 10 to cover enough variations.\nAs a result, AD2 is able to achieve 0.7 points better accuracy\non average than TinyBERT while being 8.6–28 times faster.\nInterplay between pre-training distillation and AD 2.\nOur approach provides additive improvements on top of state-\nof-the-art pre-training based distillation methods, shown in\n11689\nModel Arch. #Params.\nMNLI-m\n-/mm (Acc.)\nQQP\n(F1/Acc.)\nQNLI\n(Acc.)\nSST-2\n(Acc.)\nCoLA\n(MCC)\nSTS-B\n(PCC/SCC)\nMRPC\n(F1/Acc.)\nRTE\n(Acc.)\nAvg\n393K 368K 108K 67K 8.5K 5.7K 3.7K 2.5K\nBERT 12L \u0002 768H 109M 84.5/84.8 87.7/90.8 90.5 92.6 55.2 90.3/89.7 90.6/86.2 64.2 83.9\nFine-tune 6L \u0002 768H 66M 82.4/82.5 87.1/90.3 89.1 90.9 53.4 85.6/85.5 89.6/85.0 63.5 82.0\nVanilla KD 6L \u0002 768H 66M 82.9/82.8 87.3/90.5 89 91.3 52.4 84.7/84.7 90.3/86.0 66 82.3\nBERT-PKD 6L \u0002 768H 66M 83.2/82.9 87.6/90.7 89.1 91.5 53.1 84.6/84.7 90.0/85.2 66.7 82.4\nAD2 6L \u0002 768H 66M 83.7/84.1 88.2/91.2 91 92.6 54.7 86.4/86.0 90.6/86.5 68.2 83.6\nTable 1: The evaluation results of the GLUE benchmark on the development set. The number below each task denotes the number\nof training examples. AD2 outperforms existing task-speciﬁc knowledge distillation techniques by 1.2 points on average.\nModel MNLI QQP QNLI SST-2 CoLA MRPC RTE Avg\nTinyBERT 83.5 90.6 90.5 91.6 49.5 88.4 72.9 81.0\nAD2 83.7 91.2 91 92.6 54.7 90.6 68.2 81.7\nFigure 2: KD evaluation accuracy and training cost com-\nparison when using different data augmentation schemes.\nTinyBERT uses text-editing based data augmentation for KD.\nAD2 uses adversarial data augmentation for KD.\nTable 2. To assess whether the gains from AD 2 is additive\nto more recent deep knowledge distillation schemes (Wang\net al. 2020b; Jiao et al. 2019; Sun et al. 2020) using general-\ndomain data at the pre-training stage, we take a checkpoint\nfrom the latest version of MiniLM (Wang et al. 2020a), a\n12-layer model with 384 hidden sizes (33M parameters) dis-\ntilled from a BERT base size model3. We choose MiniLM\nbecause it achieves state-of-the-art accuracy. We take its pub-\nlicly released checkpoint to initialize the student model, and\nwe ﬁne-tune BERT-base on each task independently as the\nteacher model. We then apply AD 2 to perform adversarial\ndata augmentation and use those data for task-speciﬁc dis-\ntillation. We exclude MobileBERT (Sun et al. 2020) in this\ncomparison due to its redesigned Transformer block and dif-\nferent model size. From Table 2, we can see that AD2 further\nenhances the performance of MiniLM. Overall, AD 2 con-\nsistently brings non-trivial accuracy improvement on GLUE\ntasks and improves the accuracy by 0.5 points on average,\nwhich demonstrates its versatility in terms of combining with\npre-training distillation to further collect performance gains\nand advance the state-of-the-art.\nAnalysis\nIn this section, we ﬁrst perform an ablation study and analyze\nthe effectiveness of AD2. We then evaluate how the proposed\n3https://huggingface.co/microsoft/MiniLM-L12-H384-uncased\nModel MNLI-m QQP QNLI SST-2 MRPC RTE Avg\nMiniLM 85.6 90.9 91.3 92.8 90.1 71.8 87.1\n+AD2 86.0 91.4 91.8 93.1 90.1 72.9 87.6\nTable 2: Evaluation results on pre-training distillation check-\npoint, w/o and with AD2 based task-speciﬁc distillation.\nmethod works with alternative initialization methods for the\nstudent model. Finally, we study how effective our method is\nas the teacher model evolves.\nAblation studies. In this section, we study the effective-\nness of AD2 by comparing the following schemes. (1) AD2:\nThis is our adversarial data augmentation based task-speciﬁc\ndistillation technique. (2) -DKD: Like the above conﬁgura-\ntion but disables deep knowledge distillation, e.g., learning\nfrom the teacher’s feature maps and self-attention maps.(3)\n-KD: Like the above conﬁguration but disables knowledge\ndistillation completely so we ﬁne-tune the student directly\nwith adversarial data augmentation. (4) -AD: We further\ndisable adversarial data augmentation so that we directly\nperforms task-speciﬁc ﬁne-tuning to a student model.\nThe results are reported in Table 3. Overall, the removal\nof either component results in a performance drop. For exam-\nple, removing deep knowledge distillation (i.e., -DKD) leads\nto 0.3 points lower accuracy (86.2 vs. 85.9), indicating that\ndeep knowledge distillation is not only useful for pre-training\ndistillation but can also bring beneﬁts to task-speciﬁc distilla-\ntion when adversarial samples are presented. Removing KD\ncompletely (i.e., -KD) leads to another 0.7 points of accuracy\ndrop (85.9 vs. 85.2), indicating that KD is still crucial for\ntransferring knowledge from the teacher to the student and ad-\nversarial data augmentation alone is not sufﬁcient to close the\ngeneralization gap between the teacher and the student model.\nFinally, removing adversarial data augmentation (i.e., -AD)\nleads to a big accuracy drop (e.g., 0.9 points drop from 85.2\nto 84.3), indicating that adversarial data augmentation is im-\nportant to obtain stronger performance, especially for small\ntasks with limited data (e.g., SST-2, MRPC). These results\ndemonstrate that these components complement each other\nand are important to obtain high accuracy for compressing\nthe pre-trained Transformer models.\nExploring different model initialization schemes. In\nSec. , we evaluated how our method performs with check-\npoints from pre-training distillation. However, the pre-\ntraining compression is still quite time-consuming. Recent\n11690\nModel MNLI-m\n-/mm (Acc.)\nQQP\n(F1/Acc.)\nQNLI\n(Acc.)\nSST-2\n(Acc.)\nMRPC\n(F1/Acc.)\nRTE\n(Acc.)\nAvg\nAD2 83.7/84.1 88.2/91.2 91 92.6 86.5/90.6 68.5 86.2\n-DKD 83.5/83.5 88.1/91.2 91 92.5 86.0/90.3 67.5 85.9\n-KD 83.0/83.2 88.2/91.2 90.3 92.5 85.2/89.9 63.8 85.2\n-AD 82.4/82.5 87.1/90.3 89.1 90.9 86.6/86.3 63.5 84.3\nTable 3: Ablation results of BERT6 student model distilled\nfrom BERT12 on GLUE. The results show that removing\neither deep knowledge distillation, KD, or adversarial data\naugmentation hurts the accuracy of the student model.\nwork (Sajjad et al. 2020) proposes a lightweight way to obtain\na compressed Transformer model by directly selecting a sub-\nset of pre-trained Transformer weights to form a compressed\nmodel. This method can be useful for compressing very large-\nscale models, such as GPT-3, where pre-training distillation\nmight be very expensive and not even feasible. To investi-\ngate the usefulness of this layer selection technique and also\nhow effective AD2 is for models initialized with layer selec-\ntion, we look into three selection strategies: (1) Skip-layer\nselection strategy (Skip): selects every other layer of the\npre-trained teacher network, starting from the ﬁrst layer of\nthe network. (2) Top-layer selection strategy (Top): selects\nthe top layers of the network. (3) Bottom-layer selection\nstrategy (Bottom): selects the bottom layers of the network.\nWe apply the above selection strategies to a 6-layer Distill-\nBERT to initialize a 3-layer student model and then ﬁne-tune\nthe student model on each downstream task. Table 4 shows\nthat bottom-layer selection (Bottom) in general outperforms\nSkip and Topby 2.2 points (80.6 vs. 78.4) and 5.0 points (80.6\nvs. 75.6) on average, respectively.Top leads to the worst ac-\ncuracy, indicating that lower layers are the most important\nones for adapting to downstream tasks. This is expected be-\ncause bottom layers are closer to the input, which are more\ncrucial for capturing the basic contextual information among\ntokens (Liu et al. 2019). Removing top layers yields the least\naccuracy drop. This is because top layers are biased towards\nthe pre-training objective, which needs to be updated to adapt\nto downstream tasks anyway, as also observed by (V oita, Sen-\nnrich, and Titov 2019). Given these observations, we apply\nAD2 to the best performing 3-layer model (Bottom) and ob-\nserve that AD2 provides consistent improvements in accuracy\non all tested tasks. This result indicates that our approach\nis compatible and complementary with alternative student\nmodel initialization methods such as layer selection, which\noffers a solution to compress large-scale pre-trained Trans-\nformer models with ﬂexibility and low cost (e.g., without\npre-training distillation).\nImpact of an evolving teacher. To evaluate how AD2 per-\nforms as the teacher model evolves (e.g., by having larger\nsizes and stronger performance), we measure the difference\nbetween BERTbase and BERTlarge teacher for model com-\npression without and with AD2. Results are summarized in\nTable 5. We observe that by simply changing the teacher\nmodel from BERTbase to BERTlarge, there is not much dif-\nference in student’s performance when using just knowledge\ndistillation. This observation is consistent with prior stud-\nModel MNLI-m\n-/mm (Acc.)\nQQP\n(F1/Acc.)\nQNLI\n(Acc.)\nSST-2\n(Acc.)\nMRPC\n(F1/Acc.)\nRTE\n(Acc.)\nAvg.\nSkip 76.9/76.5 85.6/89.2 84.6 89.4 73.7/82.4 54.5 78.4\nTop 71.4/71.6 83.2/87.7 77.3 86.1 72.0/81.4 55.5 75.6\nBottom 77.4/78.0 86.4/89.8 85.8 89.1 78.1/85.1 60.2 80.6\n+ AD2 79.4/79.8 87.2/90.5 87.4 90.4 81.3/87.4 62 82.3\nTable 4: Comparison results of different layer selection strate-\ngies for initializing the student model. The results show that\nbottom-layer selection is more effective than other strategies,\nand AD2 brings 1.7 points accuracy improvement to bottom\nlayer selection based initialization.\nTeacher MNLI QQP QNLI SST-2 MRPC RTE Avg.\nBERT12 + KD 82.9/82.8 90.5 89 91.3 90.3 66 84.2\nBERT24 + KD 82.8/82.8 90.6 88.7 90.8 90.1 65.3 84.0\nBERT24 + AD2 84.0/84.2 91.0 90.1 92.3 90.6 67.5 85.3\nTable 5: Performance of the student model as the teacher\nevolves, without and with AD2. The results show that while\nthe student struggles to learn from a better teacher using\nexisting knowledge distillation, AD 2 helps the student to\nachieve better accuracy as the teacher evolves.\nies (Cho and Hariharan 2019; Sun et al. 2019), which shows\nthat when the gap between the teacher and the student is\nlarge, it becomes more challenging for the student model to\nlearn knowledge from the teacher. Interestingly, with adver-\nsarial data augmentation, we observe an overall 1.2 points\nimprovement (85.7 vs. 84.5) when we have a stronger teacher,\nwhich indicates that AD2 allows the student model to absorb\nmore knowledge from stronger teachers. This is desirable be-\ncause it allows the student model to adapt to the current state\nof the teacher model and supports a continuously evolving\nteacher that can better teach the student. More interestingly,\nwe would like to highlight that AD2 allows the BERT6 stu-\ndent model to outperform BERT12 on QQP, MRPC, and RTE,\nby distilling knowledge from BERT24, indicating that our ap-\nproach can also help a small model to achieve comparable or\nbetter performance than its larger counterpart when advised\nby an even stronger teacher.\nConclusion\nIn this work, we propose a lightweight and effective knowl-\nedge distillation approach, AD2, for compressing pre-trained\ntransformer models on low-resource downstream tasks. AD2\nleverages adversarial data augmentation in the distillation\nprocess, presenting more diverse data views to the student\nwhen transferring knowledge from the teacher model. Such a\nscheme prevents the student from overﬁtting on small domain-\nspeciﬁc datasets, leading to improved generalization ability.\nOur empirical results suggest that AD2 effectively and efﬁ-\nciently compresses pre-trained Transformers, improving the\nstudent model’s accuracy. Our detailed analysis shows that\nthis path has much potential for future work.\n11691\nAcknowledgements\nWe thank the anonymous AAAI reviewers for their construc-\ntive comments. We thank the infrastructure supported by the\nDeepSpeed team and Azure Integrated Training Platform\n(ITP) from Microsoft.\nReferences\nAguilar, G.; Ling, Y .; Zhang, Y .; Yao, B.; Fan, X.; and Guo,\nC. 2020. Knowledge Distillation from Internal Representa-\ntions. In The Thirty-Fourth AAAI Conference on Artiﬁcial\nIntelligence, AAAI 2020, 7350–7357. AAAI Press.\nAlzantot, M.; Sharma, Y .; Elgohary, A.; Ho, B.; Srivastava,\nM. B.; and Chang, K. 2018. Generating Natural Language Ad-\nversarial Examples. In Riloff, E.; Chiang, D.; Hockenmaier,\nJ.; and Tsujii, J., eds., Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing, 2018,\n2890–2896. Association for Computational Linguistics.\nBommasani, R.; Hudson, D. A.; Adeli, E.; Altman, R.; Arora,\nS.; von Arx, S.; Bernstein, M. S.; Bohg, J.; Bosselut, A.;\nBrunskill, E.; et al. 2021. On the Opportunities and Risks of\nFoundation Models. arXiv preprint arXiv:2108.07258.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.;\nSutskever, I.; and Amodei, D. 2020. Language Models are\nFew-Shot Learners. CoRR, abs/2005.14165.\nBucila, C.; Caruana, R.; and Niculescu-Mizil, A. 2006.\nModel compression. In Eliassi-Rad, T.; Ungar, L. H.; Craven,\nM.; and Gunopulos, D., eds.,Proceedings of the Twelfth ACM\nSIGKDD International Conference on Knowledge Discov-\nery and Data Mining, Philadelphia, PA, USA, August 20-23,\n2006, 535–541. ACM.\nCai, Q.; Liu, C.; and Song, D. 2018. Curriculum Adversarial\nTraining. In Lang, J., ed., Proceedings of the Twenty-Seventh\nInternational Joint Conference on Artiﬁcial Intelligence, IJ-\nCAI 2018, July 13-19, 2018, Stockholm, Sweden, 3740–3747.\nCheng, Y .; Jiang, L.; and Macherey, W. 2019. Robust Neural\nMachine Translation with Doubly Adversarial Inputs. In Ko-\nrhonen, A.; Traum, D. R.; and M`arquez, L., eds., Proceedings\nof the 57th Conference of the Association for Computational\nLinguistics, ACL 2019, Florence, Italy, July 28- August 2,\n2019, Volume 1: Long Papers, 4324–4333.\nCho, J. H.; and Hariharan, B. 2019. On the Efﬁcacy of\nKnowledge Distillation. In 2019 IEEE/CVF International\nConference on Computer Vision, ICCV 2019, Seoul, Korea\n(South), October 27 - November 2, 2019, 4793–4801. IEEE.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies,\nNAACL-HLT 2019), 4171–4186.\nDong, L.; Yang, N.; Wang, W.; Wei, F.; Liu, X.; Wang, Y .;\nGao, J.; Zhou, M.; and Hon, H.-W. 2019. Uniﬁed language\nmodel pre-training for natural language understanding and\ngeneration. In Advances in Neural Information Processing\nSystems, 13042–13054.\nGan, Z.; Chen, Y .; Li, L.; Zhu, C.; Cheng, Y .; and Liu, J. 2020.\nLarge-Scale Adversarial Training for Vision-and-Language\nRepresentation Learning. In Larochelle, H.; Ranzato, M.;\nHadsell, R.; Balcan, M.; and Lin, H., eds.,Advances in Neural\nInformation Processing Systems 33: Annual Conference on\nNeural Information Processing Systems 2020, NeurIPS 2020,\nDecember 6-12, 2020, virtual.\nGoodfellow, I. J.; Shlens, J.; and Szegedy, C. 2015. Explain-\ning and Harnessing Adversarial Examples. In Bengio, Y .; and\nLeCun, Y ., eds.,3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA, May 7-9,\n2015, Conference Track Proceedings.\nGou, J.; Yu, B.; Maybank, S. J.; and Tao, D. 2021. Knowledge\nDistillation: A Survey. Int. J. Comput. Vis., 129(6): 1789–\n1819.\nGupta, S.; Dube, P.; and Verma, A. 2020. Improving the\naffordability of robustness training for DNNs. In 2020\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR Workshops 2020, Seattle, WA, USA, June\n14-19, 2020, 3383–3392.\nHinton, G. E.; Vinyals, O.; and Dean, J. 2015. Distilling the\nKnowledge in a Neural Network. CoRR, abs/1503.02531.\nJia, R.; and Liang, P. 2017. Adversarial Examples for Evalu-\nating Reading Comprehension Systems. In Palmer, M.; Hwa,\nR.; and Riedel, S., eds., Proceedings of the 2017 Confer-\nence on Empirical Methods in Natural Language Processing,\nEMNLP 2017, Copenhagen, Denmark, September 9-11, 2017,\n2021–2031. Association for Computational Linguistics.\nJiang, H.; He, P.; Chen, W.; Liu, X.; Gao, J.; and Zhao, T.\n2020. SMART: Robust and Efﬁcient Fine-Tuning for Pre-\ntrained Natural Language Models through Principled Regu-\nlarized Optimization. In Jurafsky, D.; Chai, J.; Schluter, N.;\nand Tetreault, J. R., eds., Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, 2177–2190. Association\nfor Computational Linguistics.\nJiao, X.; Yin, Y .; Shang, L.; Jiang, X.; Chen, X.; Li, L.; Wang,\nF.; and Liu, Q. 2019. TinyBERT: Distilling BERT for Natural\nLanguage Understanding. CoRR, abs/1909.10351.\nKaplan, J.; McCandlish, S.; Henighan, T.; Brown, T. B.;\nChess, B.; Child, R.; Gray, S.; Radford, A.; Wu, J.; and\nAmodei, D. 2020. Scaling Laws for Neural Language Mod-\nels. CoRR, abs/2001.08361.\nKobayashi, S. 2018. Contextual Augmentation: Data Aug-\nmentation by Words with Paradigmatic Relations. In Walker,\nM. A.; Ji, H.; and Stent, A., eds., Proceedings of the 2018\nConference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, NAACL-HLT, New Orleans, Louisiana, USA, June 1-6,\n2018, Volume 2 (Short Papers), 452–457.\n11692\nLiu, N. F.; Gardner, M.; Belinkov, Y .; Peters, M. E.; and\nSmith, N. A. 2019. Linguistic Knowledge and Transferabil-\nity of Contextual Representations. In Burstein, J.; Doran,\nC.; and Solorio, T., eds., Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies,\nNAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019,\nVolume 1 (Long and Short Papers), 1073–1094.\nLiu, X.; Cheng, H.; He, P.; Chen, W.; Wang, Y .; Poon, H.;\nand Gao, J. 2020. Adversarial Training for Large Neural\nLanguage Models. CoRR, abs/2004.08994.\nMadry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; and Vladu,\nA. 2018. Towards Deep Learning Models Resistant to Adver-\nsarial Attacks. In 6th International Conference on Learning\nRepresentations, ICLR 2018, Vancouver, BC, Canada, April\n30 - May 3, 2018, Conference Track Proceedings.\nSajjad, H.; Dalvi, F.; Durrani, N.; and Nakov, P. 2020. Poor\nMan’s BERT: Smaller and Faster Transformer Models.CoRR,\nabs/2004.03844.\nSanh, V .; Debut, L.; Chaumond, J.; and Wolf, T. 2019. Distil-\nBERT, a distilled version of BERT: smaller, faster, cheaper\nand lighter. CoRR, abs/1910.01108.\nSennrich, R.; Haddow, B.; and Birch, A. 2016. Improving\nNeural Machine Translation Models with Monolingual Data.\nIn Proceedings of the 54th Annual Meeting of the Association\nfor Computational Linguistics, ACL 2016, August 7-12, 2016,\nBerlin, Germany, Volume 1: Long Papers.\nSun, S.; Cheng, Y .; Gan, Z.; and Liu, J. 2019. Patient Knowl-\nedge Distillation for BERT Model Compression. In Inui,\nK.; Jiang, J.; Ng, V .; and Wan, X., eds.,Proceedings of the\n2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on\nNatural Language Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, 4322–4331. Association\nfor Computational Linguistics.\nSun, Z.; Yu, H.; Song, X.; Liu, R.; Yang, Y .; and Zhou, D.\n2020. MobileBERT: a Compact Task-Agnostic BERT for\nResource-Limited Devices. In Jurafsky, D.; Chai, J.; Schluter,\nN.; and Tetreault, J. R., eds., Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, 2158–2170.\nTang, R.; Lu, Y .; Liu, L.; Mou, L.; Vechtomova, O.; and Lin,\nJ. 2019. Distilling Task-Speciﬁc Knowledge from BERT into\nSimple Neural Networks. CoRR, abs/1903.12136.\nTurc, I.; Chang, M.; Lee, K.; and Toutanova, K. 2019. Well-\nRead Students Learn Better: The Impact of Student Initial-\nization on Knowledge Distillation. CoRR, abs/1908.08962.\nV oita, E.; Sennrich, R.; and Titov, I. 2019. The Bottom-up\nEvolution of Representations in the Transformer: A Study\nwith Machine Translation and Language Modeling Objec-\ntives. In Inui, K.; Jiang, J.; Ng, V .; and Wan, X., eds.,Proceed-\nings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Con-\nference on Natural Language Processing, EMNLP-IJCNLP\n2019, Hong Kong, China, November 3-7, 2019, 4395–4405.\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\nBowman, S. R. 2019. GLUE: A Multi-Task Benchmark and\nAnalysis Platform for Natural Language Understanding. In\n7th International Conference on Learning Representations.\nWang, D.; Gong, C.; and Liu, Q. 2019. Improving Neural\nLanguage Modeling via Adversarial Training. In Chaudhuri,\nK.; and Salakhutdinov, R., eds., Proceedings of the 36th\nInternational Conference on Machine Learning, ICML 2019,\n9-15 June 2019, Long Beach, California, USA, volume 97 of\nProceedings of Machine Learning Research, 6555–6565.\nWang, L.; and Yoon, K. 2020. Knowledge Distillation and\nStudent-Teacher Learning for Visual Intelligence: A Review\nand New Outlooks. CoRR, abs/2004.05937.\nWang, W.; Bao, H.; Huang, S.; Dong, L.; and Wei, F.\n2020a. MiniLMv2: Multi-Head Self-Attention Relation Dis-\ntillation for Compressing Pretrained Transformers. CoRR,\nabs/2012.15828.\nWang, W.; Wei, F.; Dong, L.; Bao, H.; Yang, N.; and Zhou,\nM. 2020b. MiniLM: Deep Self-Attention Distillation for\nTask-Agnostic Compression of Pre-Trained Transformers. In\nLarochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin,\nH., eds., Advances in Neural Information Processing Systems\n33: Annual Conference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\nWei, J. W.; and Zou, K. 2019. EDA: Easy Data Augmen-\ntation Techniques for Boosting Performance on Text Clas-\nsiﬁcation Tasks. In Inui, K.; Jiang, J.; Ng, V .; and Wan,\nX., eds., Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, November 3-7,\n2019, 6381–6387.\nWu, X.; Lv, S.; Zang, L.; Han, J.; and Hu, S. 2019. Con-\nditional BERT Contextual Augmentation. In Rodrigues,\nJ. M. F.; Cardoso, P. J. S.; Monteiro, J. M.; Lam, R.;\nKrzhizhanovskaya, V . V .; Lees, M. H.; Dongarra, J. J.; and\nSloot, P. M. A., eds., Computational Science - ICCS 2019 -\n19th International Conference, Faro, Portugal, June 12-14,\n2019, Proceedings, Part IV, volume 11539 ofLecture Notes\nin Computer Science, 84–95. Springer.\nXie, C.; Tan, M.; Gong, B.; Wang, J.; Yuille, A. L.; and Le,\nQ. V . 2020. Adversarial Examples Improve Image Recogni-\ntion. In 2020 IEEE/CVF Conference on Computer Vision and\nPattern Recognition, CVPR 2020, Seattle, WA, USA, June\n13-19, 2020, 816–825. Computer Vision Foundation / IEEE.\nYe, N.; Li, Q.; Zhou, X.; and Zhu, Z. 2020. Amata: An\nAnnealing Mechanism for Adversarial Training Acceleration.\nCoRR, abs/2012.08112.\nZhao, S.; Gupta, R.; Song, Y .; and Zhou, D. 2021. Extremely\nSmall BERT Models from Mixed-V ocabulary Training. In\nMerlo, P.; Tiedemann, J.; and Tsarfaty, R., eds.,Proceedings\nof the 16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, 2753–2759.\nZhu, C.; Cheng, Y .; Gan, Z.; Sun, S.; Goldstein, T.; and Liu,\nJ. 2020. FreeLB: Enhanced Adversarial Training for Natu-\nral Language Understanding. In 8th International Confer-\nence on Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\n11693",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.825127124786377
    },
    {
      "name": "Distillation",
      "score": 0.7607548236846924
    },
    {
      "name": "Transformer",
      "score": 0.7352451086044312
    },
    {
      "name": "Artificial intelligence",
      "score": 0.585178017616272
    },
    {
      "name": "Machine learning",
      "score": 0.5569091439247131
    },
    {
      "name": "Task (project management)",
      "score": 0.533359706401825
    },
    {
      "name": "Language model",
      "score": 0.4650937616825104
    },
    {
      "name": "Latency (audio)",
      "score": 0.451252818107605
    },
    {
      "name": "Voltage",
      "score": 0.08965098857879639
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}