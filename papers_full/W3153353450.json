{
  "title": "Maximal Multiverse Learning for Promoting Cross-Task Generalization of Fine-Tuned Language Models",
  "url": "https://openalex.org/W3153353450",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5067773841",
      "name": "Itzik Malkiel",
      "affiliations": [
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A5078102229",
      "name": "Lior Wolf",
      "affiliations": [
        "Tel Aviv University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2396767181",
    "https://openalex.org/W4313908941",
    "https://openalex.org/W2952729433",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2963596803",
    "https://openalex.org/W2067191022",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2978670439"
  ],
  "abstract": "Language modeling with BERT consists of two phases of (i) unsupervised pre-training on unlabeled text, and (ii) fine-tuning for a specific supervised task. We present a method that leverages the second phase to its fullest, by applying an extensive number of parallel classifier heads, which are enforced to be orthogonal, while adaptively eliminating the weaker heads during training. We conduct an extensive inter- and intra-dataset evaluation, showing that our method improves the generalization ability of BERT, sometimes leading to a +9% gain in accuracy. These results highlight the importance of a proper fine-tuning procedure, especially for relatively smaller-sized datasets. Our code is attached as supplementary.",
  "full_text": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 187–199\nApril 19 - 23, 2021. ©2021 Association for Computational Linguistics\n187\nMaximal Multiverse Learning for Promoting Cross-Task\nGeneralization of Fine-Tuned Language Models\nItzik Malkiel\nTel Aviv University\nitzik.malkiel@gmail.com\nLior Wolf\nTel Aviv University\nwolf@cs.tau.ac.il\nAbstract\nLanguage modeling with BERT consists of\ntwo phases of (i) unsupervised pre-training\non unlabeled text, and (ii) ﬁne-tuning for\na speciﬁc supervised task. We present a\nmethod that leverages the second phase to\nits fullest, by applying an extensive num-\nber of parallel classiﬁer heads, which are\nenforced to be orthogonal, while adaptively\neliminating the weaker heads during train-\ning. We conduct an extensive inter- and intra-\ndataset evaluation, showing that our method\nimproves the generalization ability of BERT ,\nsometimes leading to a +9% gain in accuracy.\nThese results highlight the importance of a\nproper ﬁne-tuning procedure, especially for\nrelatively smaller-sized datasets. Our code is\nattached as supplementary.\n1 Introduction\nRecently, there has been an increasing number\nof studies suggesting the use of general language\nmodels, for improving natural language process-\ning tasks (Dai and Le, 2015; Peters et al., 2018;\nRadford et al., 2018; Howard and Ruder, 2018).\nAmong the most promising techniques, the unsu-\npervised pretraining approach (Dai and Le, 2015;\nRadford et al., 2018) has emerged as a very suc-\ncessful method, that achieves state-of-the-art re-\nsults on many language tasks, including senti-\nment analysis (Socher et al., 2013), natural lan-\nguage inference (Williams et al., 2017) and similar-\nity and paraphrasing tasks (Dolan and Brockett,\n2005; Cer et al., 2017). This approach incorpo-\nrates a two-phase training procedure. The ﬁrst\nphase utilizes an unsupervised training of a gen-\neral language model on a large corpus. The sec-\nond phase applies supervision to ﬁne-tune the\nmodel for a given task.\nMore recently, unsupervised pretraining mod-\nels such as BERT (Devlin et al., 2018), XLNET\n(Yang et al., 2019) and RoBERTa (Liu et al., 2019),\nhave achieved unprecedented performance. For\nexample, in the GLUE benchmark (Wang et al.,\n2018), BERT (Devlin et al., 2018) was reported to\nachieve performance that exceeds human level\non a few different datasets, such as QNLI (Ra-\njpurkar et al., 2016), QQP (Chen et al., 2018) and\nMRPC (Dolan and Brockett, 2005). However, de-\nspite the great progress achieved by these task-\nspeciﬁc and dataset-speciﬁc models, it is not yet\nclear how well they can generalize to different\ntasks, how well they generalize when evaluating\nthe same task on different datasets, and how to\nimprove this generalization ability.\nIn our work, we extend the multiverse method\nof (Littwin and Wolf, 2016), which was shown to\nimprove transfer learning in the computer vision\ntask of face recognition and on the CIFAR-100\nsmall image recognition dataset. The multiverse\nloss generalizes the cross entropy loss, by simul-\ntaneously training multiple linear classiﬁcation\nheads to perform the same task. In order to pre-\nvent multiple copies of the same classiﬁer, in the\nmultiverse scheme, each classiﬁer is mutually or-\nthogonal to the rest of classiﬁers. The number of\nmultiverse heads used was limited, never more\nthan seven and typically set to ﬁve.\nWe propose a novel ﬁne-tuning procedure for\nenhancing the generalization ability of the recent\nunsupervised pretrained language models, by em-\nploying a large number of multiverse heads. The\nessence of our technique is as follows: given a pre-\ntrained language model and a downstream task\nwith labeled data, we ﬁne-tune the model using\na maximal number of multiverse classiﬁers. The\nﬁne-tuning goal is to both minimize the task loss\nand an orthogonality loss applied to the classiﬁ-\ncation heads. When enforcing orthogonality hin-\nders the classiﬁers’ performance, we detect and\neliminate the less effective classiﬁcation heads.\n188\nThe technique, therefore, preserves a maximal\nset of classiﬁers, which is comprised of the best\nperforming ones. By maintaining this maximal\nsubset during training, our method leverages mul-\ntiverse loss to its fullest. Hence, we name our\nmethod Maximal Multiverse Learning (MML).\nOur contributions are as follows: (1) we present\nMML, a general training procedure to improve\nthe transferability of neural models. (2) we ap-\nply MML on BERT and report its performance on\nvarious datasets. (3) we propose a set of cross\ndataset evaluations using common NLP bench-\nmarks, demonstrating the effectiveness of MML,\nin comparison to regular BERT ﬁne-tuning and\nto alternative regularization techniques.\n2 Related work\nMany recent breakthroughs in NLP employ unsu-\npervised pretraining of language models. The dif-\nferent variants can be categorized into two main\napproaches: (1) feature-based models, such as\n(Peters et al., 2018) and (2) ﬁne-tuning models,\nsuch as (Devlin et al., 2018; Liu et al., 2019; Yang\net al., 2019). The former technique utilizes a lan-\nguage neural-based model as a feature extractor.\nThe extracted features may be used for the train-\ning of another model, receiving the extracted fea-\ntures as input. The second approach utilizes a\nsimilar pre-trained model, but ﬁne-tunes it in\nan end-to-end manner to specialize on a given\ntask. During the ﬁne-tuning phase, all of the pa-\nrameters of the model are updated and a rela-\ntively small number of parameters are trained\nfrom scratch.\nThe usage of multiple classiﬁers can be found\nin few places in the literature. In GoogLeNet\n(Szegedy et al., 2015), the authors use multiple\nclassiﬁer heads in different places in the model\narchitecture. The additional classiﬁers led to bet-\nter propagation of the gradients during training.\nHowever, with the advent of better conditioning\nand normalization methods, as well as with the\nmodern introduction of skip connections in ar-\nchitectures such as the ResNet (He et al., 2016),\nthe practice of adding intermediate branches, for\nthe sake of introducing an auxiliary loss at lower\nlayers, was mostly abandoned.\nThe multiverse loss was shown to promote\ntransfer learning and to lead to a low-dimensional\nrepresentation in the penultimate layer (Littwin\nand Wolf, 2016). However, the current literature\ndoes not present any methodological way to se-\nlect the number of multiverse heads and the idea\nwas only applied for a handful of parallel classi-\nﬁers. In MML, hundreds of multiverse heads are\nused, leading to a tradeoff between the classiﬁer\naccuracy and the orthogonality constraint. MML\nbalances the two terms by pruning, during train-\ning, the under-performing heads.\n3 Method\nLet W = {wi }w\ni=1 be the vocabulary of tokens in a\ngiven language. Let Y be the set of all possible\nsentences generated by W , including the empty\nsentence. A language model M : Y ×Y → Rd re-\nceives a pair of elements from Y and returns a\nvectors of d dimensions. Given a dataset with n\ntraining samples, s1...sn ∈ Y ×Y , each associated\nwith a label yi ∈ [1...c], we denote the coding vec-\ntor of each sample by di := M(si ). As a concrete\nexample, for the BERT model, di is the latent em-\nbedding of the CLS token.\nCommon language models employ a classiﬁer\nC : Rd → Rc that projects the coding vectors di ∈\nRd by a d ×c matrix, Fd×c = [f1,..., fc ], ( fi ∈ Rd ),\nand then adds a bias term b ∈ Rc :\nC(di ) = dT\ni Fd×c +b (1)\nThe output of C is a logit vector, and pseudo-\nprobabilities are obtained by applying softmax\npi\n(\nyi\n)\n= eC(di )yi /∑c\nj=1 eC(di )j .\nDifferent from the single-classiﬁer\nmodels, our model utilizes a multiverse\nclassiﬁer C : Rd → Rc×m deﬁned as\nC(di ) = (C1(di ), ...,Cm(di )), where m is a\nmultiplicity parameter,\n{\nC j : Rd → Rc }m\nj=1 are\nparallel classiﬁers, each with different weights,\napplying the same function as Eq. 1. :\nC j (di ) = dT\ni F j\nd×c +bj (2)\nAdditionally, we will deﬁne B =\n{\nβj ∈ {0,1}\n}m\nj=1\nas a set of binary scalars. Each classiﬁer head C j\nwill be associated with a different binary scalarβj ,\nwhich is set during training. In our experiments,\nwe set m to be equal to the coding vector size\nd, which entails a full rank of active multiverse\nclassiﬁers at the beginning of the training.\nThe loss functionis composed of two compo-\nnents, the task loss and the multiverse loss. Deﬁn-\ning active multiverse classiﬁers as those that are\nassociated with a value βj = 1, the task loss op-\ntimizes the performance of all active multiverse\n189\nFigure 1: A schematic illustration of the MML model. The task loss comprises a loss-term for each multiverse\nclassiﬁer, using the given labels of the task at hand. The mutual orthogonality tables hold the absolute value of\nthe dot product calculated between the weights of all classiﬁers, across the different classes. Since orthogonal-\nity of a classiﬁer with itself is ignored, we set the diagonal to 0. Following multiverse loss deﬁnition and since\northogonality is symmetric, only half of each table value is passed to the multiverse loss.\nAlgorithm 1MML training. The MeanShift func-\ntion returns the center of the clusters. Params:\nK = 1000, γ = 0.99, T hreshold = 5, α = 2 ·e−5,\nλ = 0.005.\n∀1 ≤ j ≤ m : βj ← 1, aj ← 0\nfor step = 1, 2, ...do\nSample a minibatch {(si , yi )}t\ni=1\nMCθ ← ∇ θ\n[\nλ · Lmv (C,B) +\n1\nt\n∑t\ni=1 Lt ask\n(\nM,C,B,{( si , yi )}t\ni=1\n)]\nfor 1 ≤ j ≤ m do\naj ← (1 − γ) · aj +(\nγ· 1\nt\n∑t\ni=1 Lt ask\n(\nM,C,B, {(si , yi )}t\ni=1\n))\nθ ← θ + Adam(θ, MCθ, α)\nif step %K = 0 and ∑\nj βj ≥ T hreshold then\nclusters ← MeanShift\n({\naj |βj = 1\n}m\nj=1\n)\nif |cluster s | ≥2 then\nfor 1 ≤ j ≤ m do\nβj ← 0, if aj ∉ min (cluster s )\nclassiﬁers, each independently, using the supervi-\nsion obtained by the given labels. The multiverse\nloss soft-enforces orthogonality among the active\nclassiﬁers. Its purpose is to regularize the model\nby encouraging M to produce coding vectors that\nare robust enough to be effective for a large num-\nber of orthogonal classiﬁers.\nAs mentioned earlier, each classiﬁer C j is as-\nsociated with a binary value βj , which controls\nthe applicability of the classiﬁer and is conﬁg-\nured during training. Under the context of the\nloss function, setting βj to 0 would eliminate the\nimpact of the j th classiﬁer head C j for both the\ntask loss and multiverse loss.\nFor a multi-class classiﬁcation task, we apply\nthe following task loss:\nLt ask= −Σn\ni=1Σm\nj=1Lj,i\ncce βj (3)\nwhere n is the number of training samples, and\nLj,i\ncce = yi log (C j (M(si ))yi ) is the cross entropy\nloss. For a binary classiﬁcation task we set C :\nRd → R2×m and use the same loss from Eq. 3.\nFor a regression task, we replace Lj\ncce with Lj,i\nL2 =yi −C j (M(si ))\n2\n2.\nThe second loss term enforces orthogonality\nbetween the set of classiﬁers, for each class sepa-\nrately, using the multiverse loss:\nLmv = Σj,r,s>r\n⏐⏐⏐f r ⊤\nj f s\nj βr βs\n⏐⏐⏐ (4)\nwhere f r\nj is the jth column of the weight matrix\nthat corresponds to classiﬁer Cr .\nThe total loss Ltot al is deﬁned as: Ltot al =\nLt ask+λLmv . Similar to (Littwin and Wolf, 2016),\nwe set λ = 0.005, throughout all of our experi-\nments. The MML model is illustrated in Fig. 1.\n190\nThe training algorithmbegins with an initializa-\ntion of the aggregated model ( M,C,B). M may\nbe initialized by any pre-trained general language\nmodel. The multiverse classiﬁers are randomly\ninitialized from scratch, and all classiﬁers are ini-\ntially activated, by setting βj = 1 for ∀βj ∈ B.\nDuring training, we track the performance\nof each multiverse classiﬁer separately. Every\nK steps, we search for a subset of the top-\nperforming classiﬁers. When we ﬁnd such a sub-\nset, we eliminate the less performing classiﬁers\nby setting their corresponding βs to 0.\nIn order to detect the top-performing classi-\nﬁers, we calculate a moving average variable aj\nfor each multiverse classiﬁer. Speciﬁcally, aj\nholds the moving average of the task loss value\nLj\nt ask associated with classiﬁcation head C j . aj\nis being updated for every training step, using the\nmoving average momentum constant of 0.99.\nEvery K steps, we run the MeanShift algo-\nrithm (Comaniciu and Meer, 2002) on the set{\naj |βj = 1\n}\nto obtains the modes of the under-\nlying distribution. MeanShift is a clustering al-\ngorithm that reveals the number of clusters in a\ngiven data, and retrieves the corresponding cen-\ntroid for each detected cluster. In our case, Mean-\nShift is applied on 1D data and by utilizing it, we\nidentify the subset of top-performing multiverse\nheads as the cluster associated with the minimal\ncentroid value. Next, we eliminate the rest of the\nmultiverse heads, by setting their corresponding\nβ to 0. This adaptive elimination is stopped, when\nwe reach a minimal number of active heads, see\nAlg. 1.\nAt inference, we use the active multiverse heads\nto retrieve predictions. Speciﬁcally, given a sam-\nple si , we calculate the logits ˆy as:\nˆy :=\n∑m\nj=1 C j (M(si )) ·βj\n∑m\nj=1 βj\n(5)\nfor classiﬁcation tasks, we apply the softmax func-\ntion on ˆy, and return its output. For regression\ntasks, we simply return ˆy.\n4 Results\nIn this study, we evaluate MML, applied on a pre-\ntrained BERT (Devlin et al., 2018) model, using\nnine NLP datasets, while employing two differ-\nent settings: (1) ﬁne-tuning on different down-\nstream tasks from the GLUE benchmark (Wang\net al., 2018), and (2) cross dataset evaluations for\ndifferent datasets of the same or similar tasks. For\nthe ﬁrst, we ﬁne-tune MML on each dataset sepa-\nrately, and evaluate its performance on the devel-\nopment set and the test set of the same dataset.\nFor the second, we evaluate our ﬁne-tuned MML\nmodels on the train and development set of other\ndatasets within the same task category. This al-\nlows us to study the generalization level of all\nmodels, across different datasets. Additionally,\nwe perform an ablation study and report empir-\nical results that showcase the efﬁcacy of MML,\ncompared to other multiverse schemes and to a\nBERT model with a higher dropout rate.\nWe adopt eight datasets from the GLUE bench-\nmark (Wang et al., 2018) and one extra dataset\nsupporting the task of Natural Language Infer-\nence (NLI). The datasets can be arranged to form\nthe following three categories: (1) Inference tasks:\nthis category incorporates four datasets of nat-\nural language inference: RTE, MNLI, SNLI and\nQNLI. RTE and QNLI are binary classiﬁcation\ntasks, whereas MNLI and SNLI are multilabel clas-\nsiﬁcation tasks (which possess an additional “neu-\ntral” label). (2) Similarity and paraphrasing tasks:\nthis category includes three datasets: MRPC, QQP ,\nSTS-B. MRPC and QQP are binary classiﬁcations\ntasks, while STS-B is a regression task with labels\nannotating the level of similarity between each\nsentence pair. (3) Misc. datasets: this category\ncomposed of two datasets that cannot be used\nfor the cross dataset evaluation, due to the lack\nof commonality between their tasks: CoLA and\nSST-2. We refer the reader to the appendix for\nmore details on the nine datasets.\n4.1 Evaluation on GLUE datasets\nWe evaluated MML on the eight different datasets\nfrom the GLUE benchmark, and compared to\nBERT (Devlin et al., 2018). Each model was\ntrained and evaluated on a single dataset. De-\nvelopment and test set performance are being\nreported for each model. In addition, we conduct\nan ablation analysis for our method, presenting\nthe importance of our Maximal Multiverse Learn-\ning, which allows the training to adapt the num-\nber of multiverse classiﬁers to each dataset. The\nﬁrst ablation experiment disables the classiﬁer\nelimination step during training and utilizes the\nsame MML architecture with a ﬁxed number of\nheads. The second ablation also disables the mul-\ntiverse loss, leaving the training to optimize an\nensemble of classiﬁers, without enforcing the or-\n191\nModel MNLI QQP QNLI SST-2 CoLA STS-B MRPC RTE Average\n392k 363k 108k 67k 8.5k 5.7k 3.5k 2.5k -\ndevelopment set\nBERT 86.6/- 91.3 92.3 93.2 60.6 90.0 88.0 70.4 84.0\nMV-5 87.0/- 91.4 92.2 94.0 64.3 91.1 88.0 75.4 85.4\nMV-1024 86.2/- 91.4 92.2 93.2 57.9 90.6 89.7 80.1 85.2\nBERT-Drop 87.0/- 91.6 93.0 92.9 62.3 90.7 87.5 76.5 85.2\nBERT-Ensemble 86.6/- 91.4 92.3 93.2 60.9 90.2 88.0 70.4 84.1\nMML 87.2/- 91.7 93.0 94.0 64.5 91.1 89.0 80.1 86.3\ntest set\nBERT 86.7/85.9 89.3 92.7 94.9 60.5 86.5 85.4 70.1 83.2\nMV-5 86.4 /85.9 88.9 92.2 94.1 56.9 87.4 86.3 70.6 82.8\nMV-1024 87.0/85.9 89.1 92.2 93.8 54.8 86.8 86.1 74.2 82.9\nBERT-Drop 86.6/86.0 89.3 92.8 94.1 56.1 87.9 85.3 74.1 83.2\nBERT-Ensemble 86.6/86.0 89.2 92.3 94.1 60.1 86.4 86.1 70.4 83.1\nMML 87.0/86.0 89.4 92.6 94.6 58.6 88.1 86.7 74.2 83.8\nMML #heads 19 14 23 979 31 45 913 1024 -\nTable 1: Results on GLUE benchmarks (Wang et al., 2018). BERT results taken from (Devlin et al., 2018). Accu-\nracy scores are reported for all datasets, except STS-B, for which Spearman Correlation is reported. For MNLI,\naccuracy scores are reported for both matched and mismatched test sets. The last row exhibits the number of\nactive multiverse heads of the converged MML model. For example, for MRPC, our MML model used 913 active\nmultiverse heads, while for MNLI, it maintained 19.\nthogonality constraint.\nThe BERT model used is the BERT-Large\nmodel (Devlin et al., 2018). It contains 24 atten-\ntion layers, each with 16 attention heads with a\nhidden layer of size d = 1024 dimensions. The\nmodel was pre-trained using sentence pairs, to\nboth reconstruct masked words and to predict\nwhether the pairs are consecutive. BERT’ s ﬁne-\ntuning for downstream tasks employs supervision\nobtained by the given labels of each dataset.\nMML utilizes the same pre-trained BERT-Large\nmodel and is initialized with m = d = 1024 ac-\ntive multiverse classiﬁers. During training, the\nmodel converges to a smaller number of multi-\nverse classiﬁers. The number of active multiverse\nclassiﬁers of each model is presented in last row\nof Tab. 1.\nTab. 1 presents the results for six models: (1)\nBERT (as a baseline), (2) MML, (3-4) two multi-\nverse models utilizing a ﬁxed number of multi-\nverse classiﬁers, with 5 and 1024 classiﬁers, re-\nspectively, (5) BERT-Drop, a BERT model that\nwas ﬁne-tuned with a 30% dropout rate on the\nCLS embedding vector and which provides a\nbaseline with additional regularization, (6) BERT-\nEnsemble, a BERT model with 1024 heads (similar\nto MV-1024 without the multiverse loss).\nAs can be seen in the table, compared to BERT ,\nMML yields better results by a sizeable margin on\nthe test set of ﬁve out of eight datasets. The largest\ngains were reported in the relatively smaller-sized\ndataests, such as RTE, MRPC and STS-B, for\nwhom MML yields an absolute improvement of\n4.1%, 1.3%, 1.6%, respectively. This can be at-\ntributed to the ability of MML to encourage a\nmore robust learning. On the rest of the datasets,\nMML yields similar performance on the test. On\nthe development set, MML outperforms BERT on\nall datasets, sometimes by a large margin. Specif-\nically, for RTE, MML yields an improvement of\nalmost 10%.\nThe ablation models MV-5 and MV-1024, uti-\nlize a ﬁxed number of multiverse heads during\nthe entire training. We have found that this hyper\nparameter can be crucial for the model’ s conver-\ngence, and when not initialized properly, may\nsigniﬁcantly reduce performance for the given\ntask in hand. Speciﬁcally, for the CoLA dataset,\nMV-1024 and MV-5 yield a relative performance\ngap of more than 11%, in favor of MV-5, while\nin RTE, there is a gap of 6.2% in favor of MV-\n1024. When comparing both MV-5 and MV-1024\nto MML, MML produces better or similar per-\nformance on the development set of all datasets.\nMore speciﬁcally, on RTE and MRPC, MML yields\nsimilar performance as in MV-1024, and outper-\nforms it on all the other six datasets. Compared to\nMV-5, MML yields better performance, by a size-\nable margin, on four datasets out of eight, and\nproduces similar performance on the rest.\n192\nFigure 2: The number of active multiverse classiﬁers, per training step, for the MML model trained on QNLI.\nThe MeanShift algorithm detects multiple clusters, for three times during training. The two upper plots present\nthe selection of the top-performing heads (green stars), and the elimination of the heads showing lower-\nperformance (red stars). The Y values are the moving average calculated on the multiverse heads’ loss function.\nOur MML-QNLI model reaches local minima at step 85K, for which 23 heads were activated. The bottom right\nplot shows the moving average values of the activated 23 multiverse heads, by the same training step.\nThe BERT-Drop model is a BERT baseline\ntrained with a dropout rate of 30% (instead of 10%\nas used in a regular BERT). Compared to BERT ,\nBERT-Drop provides similar results on the test\nset, while showing some improvement on the de-\nvelopment set. When comparing BERT-Drop to\nMML, MML results with a higher average perfor-\nmance on both the test set and development set,\nwhere in some dataests, such as RTE, MML yields\nan improvement of +3.6% on the development\nset. On the test set, MML surpasses BERT-Drop\non seven out of eight datasets. Speciﬁcally, in\nsome datasets, such as MRPC and CoLA, MML\noutperforms BERT-Drop in 1.4 and 2.5 accuracy\npoints, respectively.\nThe BERT-Ensemble employs BERT with an en-\nsemble of 1024 parallel classiﬁers. It optimizes all\nclassiﬁers during training and infers predictions\nby calculating the mean over all classiﬁers’ log-\nits. As shown in the table, BERT-Ensemble yields\nsimilar performance to BERT .\nFig. 2 presents the amount of active mul-\ntiverse heads, when applying MML on QNLI\ndataset. During the training of the MML-QNLI\nmodel, the MeanShift algorithm detected multi-\nple clusters at three times 1, through the entire\ntraining. Each time, the model eliminated the\nlower-performance subsets, and kept the top-\nperforming multiverse classiﬁers as the active set\nof classiﬁers. The model achieved best perfor-\nmance on the development set at training step\n85K. At this step, the MML-QNLI model utilized\n23 active multiverse heads. The plots in the ﬁgure\npresent the cumulative loss of each multiverse\nhead, sorted through the X axis according to the\nindices of the active heads. The red stars are asso-\n1 The elimination is being invoked every time the Mean-\nShift algorithm detects multiple clusters. Speciﬁcally, for\nMML-QNLI experiment, multiple clusters appeared three\ntimes during the training process.\n193\nciated with the classiﬁers heads that were elimi-\nnated, and the green stars are the heads that were\nselected as the top-performing subset.\n4.2 Cross dataset evaluations\nIn the cross-task evaluations, we use the ﬁne-\ntuned MML models from Sec. 4.1. For each model\ntrained on a dataset from the two ﬁrst categories\nabove (NLI and similarity/paraphrasing), we eval-\nuate the model on all datasets from the same cate-\ngory. Train and development set performance are\nreported to give a clear view on the cross-task gen-\neralization ability, and also to exhibit the level of\noverﬁtting, when evaluating on the same dataset\nthat the model was trained on.\nIn order to conduct a clean comparison, we\nﬁnetune BERT , MML, and all other methods with\nthe same hyperparmeters, employing 10/30/100\nepochs for the relative large/medium/small\ndatasets, a batch size of 32, and a learning rate\nof 2e-5 (BERT obtains in all cases performance\nthat at least matches the one published in (De-\nvlin et al., 2018)). Our code can be found at\nhttps://github.com/ItzikMalkiel/MML.\nIn cross evaluation experiments, there is no\ntraining on the target training set, so both the tar-\nget training set and validation sets can be used\nfor evaluation. Overall, there are 36 cross-task\nexperiments. In the ﬁrst category, there are four\ndatasets, so three cross-task experiments for each.\nTaking into account the two splits, this amounts\nto 24 experiments. Similarly, in the second cat-\negory, where there are three datasets, there are\n12 experiments. Fig. 3 is a Dolan Moré plot com-\nparing the performance of BERT to the ﬁve ﬁne-\ntuning variants explored (MML, MV-5, MV-1024,\nBERT-Drop, Bert-Ensable) for all 36 experiments.\nThese plots show for each method the ratio of ex-\nperiments for which it has a performance level\nthat is at least as high a constant times the best\nresult. As can be seen, regularization helps cross-\ntask generality. However, none of the baseline\nmethods is as effective as MML. In the supple-\nmentary appendix we provide the full data, and,\nfor brevity, below we focus on comparing MML\nwith BERT .\nInference tasks Since MNLI and SNLI are mul-\nticlass classiﬁcation tasks with 3 classes, we col-\nlapse the labels “neutral” and “contradication”\ninto one label (“non entailment”). This modiﬁ-\ncation, applied only during inference, allows us\nto evaluate MNLI and SNLI models on RTE and\nFigure 3: A Dolan-Moré proﬁle, based on the results\nobtained across all cross-task experiments. The x-\naxis is the threshold τ. The y-axis depicts, for a given\nﬁne-tuning method, the ratio of datasets in which the\nobtained error is less than the threshold τ times the\nminimal error obtained by any of the six methods.\nQNLI, and vice versa.\nThe results are reported in Tab. 2. As can\nbe seen, MML exhibits a signiﬁcantly improved\ntransferability compared to BERT . Each row in the\ntable represents an MML or BERT model trained\non a single dataset associated by its name. All\nmodels are evaluated on all four datasets. In\nthe last column, we report the relative average\nimprovement obtained by MML, calculated by\nthe performance ratio between MML and BERT\nacross all three holdout datasets. For example,\nfor RTE, our MML-RTE model yields 9.9% relative\naverage performance on the train set of MNLI,\nQNLI and SNLI, and a 9.5% average improvement\non the development set of these datasets.\nSimilarity and paraphrasing Since STS-B is a\nregression task benchmark, while MRPC and QQP\naddress a binary classiﬁcation task, to support\ncross dataset evaluations, we adapt STS-B to form\na binary classiﬁcation task. The adaptation is be-\ning done by collapsing the labels in the range 1-2\n(4-5) to the value of 0 (1) and omitting all samples\nassociated with label values between 2 and 4. The\nbinary STS-B version has ∼3.5K samples.\nAs can be seen in Tab. 3, MML yields better\nperformance on the cross evaluations for the sim-\nilarity and paraphrase datasets. Similar to Tab. 2,\neach row represent a single model trained on a\nsingle dataset. We evaluate all models on all three\n194\nModel RTE MNLI QNLI SNLI MML Improvement\nBERT-RTE 96.06/70.39 69.42/69.17 52.46/52.84 68.02/69.87 -\nMML-RTE 99.39/ 80.14 79.24/78.42 50.86/51.30 80.85/82.53 +9.98%/+9.51%\nBERT-MNLI 79.15/76.89 99.59/86.58 49.88/51.05 81.65/83.65 -\nMML-MNLI 79.35/78.70 99.74/ 87.18 49.64/50.22 82.37/83.94 +0.21% /+0.35%\nBERT-QNLI 53.37/48.73 59.76/59.89 99.99/ 94.01 59.33/60.03 -\nMML-QNLI 53.41/53.79 64.93/63.85 95.75/92.86 62.13/63.58 +4.48%/+7.63%\nBERT-SNLI 71.28/70.03 75.82/75.79 49.62/50.72 99.74/91.08 -\nMML-SNLI 71.88/69.31 75.79/76.37 49.32/50.32 99.30/ 91.38 +0.07%/-0.36%\nTable 2: Cross dataset evaluation for Language Inference tasks. Train/development accuracy are reported sep-\narately for each dataset. Each model (a row in the table) was trained on a single dataset denoted by its name,\nand was evaluated on the train/development sets of all four datasets. The last columns indicates the relative\naverage improvement obtained by MML compared to BERT , and averaged across the three hold-out datasets\n(i.e. excluding the diagonal). SNLI is the only dataset for which MML does not improve cross dataset per-\nformance on average, perhaps since it is largest dataset with 570k samples. See supplementary appendix for\nfarther comparison with MV-5, MV-1024 and BERT-Drop.\nModel QQP MRPC STS-B* MML Improvement\nBERT-QQP 99.73/91.57 66.90/68.85 88.34/90.12 -\nMML-QQP 99.74/ 91.68 67.77/68.87 89.11/90.55 +1.08%/+0.25%\nBERT-MRPC 65.28/65.18 99.37/87.25 82.53/88.58 -\nMML-MRPC 68.37/68.15 99.23/ 88.97 86.12/91.32 +4.54%/+3.82%\nBERT-STS-B* 73.13/73.11 75.59/75.49 100.0/95.49 -\nMML-STS-B* 74.13/74.40 75.51/77.94 99.85/ 96.70 +0.63%/+2.50%\nTable 3: Cross dataset evaluation for similarity and paraphrasing tasks. STS-B* is the modiﬁed version of STS-B\nthat forms a binary classiﬁcation dataset (instead of regression). STS-B* models were trained as binary classi-\nﬁers on STS-B data. Accuracy scores are reported through all evaluations. The last column presents the relative\ncross dataset improvement obtained by MML, compared to BERT .\ndatasets, and report the average relative improve-\nment obtained by MML calculated on the two\nholdout datasets. We have found MML to pro-\nduce improved performance for all models, for\nexample, MML-MRPC yields a ∼+3.5% average\nimprovement calculated on both train and devel-\nopment sets across STS-B and QQP .\n5 Discussion\nThe results in both Tab. 2 and 3, reveal a signif-\nicant gap in performance for all models when\nevaluated on holdout datasets, although the\nholdout datasets share the same or similar task\neach model was trained for. For example, both\nMML-MRPC and BERT-MRPC models yield a\n∼20% degradation in absolute accuracy on the\nQQP dataset. However, compared to BERT , our\nMML method produces signiﬁcantly better per-\nformance on the cross evaluations. Speciﬁcally,\nwhen evaluated on QQP , MML-MRPC outper-\nforms BERT-MRPC by a relative improvement of\n∼4.6%, for both development and train set.\nWe do not observe a direct link between the\nimprovement obtained on the same dataset eval-\nuation to that obtained in the cross dataset one.\nFor example, our MML-QNLI model was able to\noutperform BERT-QNLI in the cross dataset eval-\nuation, although, compared to our BERT-QNLI\nreproduction, MML-QNLI exhibits a somewhat\ndegraded performance on QNLI’ s development\nset (see the third section in Tab.2) and test set (as\npublished in (Devlin et al., 2018)). We attribute\nthis to the ability of MML to encourage the model\nto produce more transferable coding vectors.\nComputational overhead During training, the\nMML multiverse heads imply a maximal addition\nof 1024 operations of matrix multiplications and\ngradient derivations, each in the size of 1024×c.\n195\nDuring the backward steps, the multiple gradi-\nents are averaged almost immediately, right be-\nfore propagating them back through the last en-\ncoder layer of BERT . Interestingly, in many cases,\nthe number of active heads shrinks in the early\nstages of the training. During inference, the ad-\nditional calculations are solely the matrix multi-\nplications, for which the average number of ac-\ntive heads, across our experiments, is 381. This\naverage number of active heads translates to\nan increase of ∼0.2% to the parameter count of\nBERT_Large.\n6 Summary\nWe introduce the MML method for ﬁne-tuning\nlanguage models. MML utilizes a large set of par-\nallel multiverse heads and eliminates the rela-\ntively weaker heads during training. We demon-\nstrate the effectiveness of MML on nine com-\nmon NLP datasets, by applying inter- and intra-\ndatasets evaluation, where it is shown to outper-\nform the originally introduced BERT ﬁne-tuning\nprocedure. The results shade light on the role\nof regularization in improving cross task gener-\nalization, and show the advantage of MML over\nalternative regularization methods.\nAcknowledgment\nThis project has received funding from the Eu-\nropean Research Council (ERC) under the Euro-\npean Union Horizon 2020 research and innova-\ntion programme (grant ERC CoG 725974). The\ncontribution of the ﬁrst author is part of a PhD\nthesis research conducted at Tel Aviv University.\nReferences\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The ﬁfth pascal recognizing\ntextual entailment challenge. In TAC.\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large anno-\ntated corpus for learning natural language infer-\nence. arXiv preprint arXiv:1508.05326.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017 task\n1: Semantic textual similarity-multilingual and\ncross-lingual focused evaluation. arXiv preprint\narXiv:1708.00055.\nZihan Chen, Hongbo Zhang, Xiaoji Zhang, and Leqi\nZhao. 2018. Quora question pairs.\nDorin Comaniciu and Peter Meer. 2002. Mean shift:\nA robust approach toward feature space analysis.\nIEEE Transactions on Pattern Analysis & Machine\nIntelligence, pages 603–619.\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079–3087.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of\ndeep bidirectional transformers for language un-\nderstanding. arXiv preprint arXiv:1810.04805.\nWilliam B Dolan and Chris Brockett. 2005. Auto-\nmatically constructing a corpus of sentential para-\nphrases. In Proceedings of the Third International\nWorkshop on Paraphrasing (IWP2005).\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages\n770–778.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation.\narXiv preprint arXiv:1801.06146.\nEtai Littwin and Lior Wolf. 2016. The multiverse loss\nfor robust transfer learning. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, pages 3957–3966.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. arXiv preprint arXiv:1802.05365.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language un-\nderstanding by generative pre-training. openai\nblog.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,\nand Percy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing,\npages 1631–1642.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser-\nmanet, Scott Reed, Dragomir Anguelov, Dumitru\n196\nErhan, Vincent Vanhoucke, and Andrew Rabi-\nnovich. 2015. Going deeper with convolutions. In\nComputer Vision and Pattern Recognition (CVPR).\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis plat-\nform for natural language understanding. arXiv\npreprint arXiv:1804.07461.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2018. Neural network acceptability judg-\nments. arXiv preprint arXiv:1805.12471.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2017. A broad-coverage challenge corpus for\nsentence understanding through inference. arXiv\npreprint arXiv:1704.05426.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nSupplementary Appendices2\nA The datasets (more details)\nWe adopt eight datasets from the GLUE bench-\nmark (Wang et al., 2018), and one extra dataset\nsupporting the task of Natural Language Infer-\nence (NLI). The datasets can be arranged by cate-\ngories as follows.\nInference tasks This category contains three\ndatasets from the GLUE benchmark (Wang et al.,\n2018), along with an external dataset named SNLI\n(Bowman et al., 2015) that shares the same task.\nRTE The Recognizing Textual Entailment dataset\n(Bentivogli et al., 2009) is composed of sentence\npairs associated with a binary classiﬁcation task\nfor entailment/non entailment relation between\nthe sentences. MNLI Multi-Genre Natural Lan-\nguage Inference Corpus (Williams et al., 2017) is\nalso an entailment dataset comprised of sentence\npairs. The task is multiclass classiﬁcation for pre-\ndicting contradiction, neutral or entailment rela-\ntion between the sentence pairs. SNLI The Stan-\nford Natural Language Inference dataset (Bow-\nman et al., 2015) is similar to MNLI, but contains\ndata gathered from different sources. QNLI The\nQuestion-answering Natural Language Inference\ndataset (Rajpurkar et al., 2016) contains question-\nsentence pairs associated with the binary classi-\nﬁcation task for the entailment/non entailment\nrelation between the question-answer pairs.\n2Put here for the reader’ s convenience.\nSimilarity and paraphrasing Tasks This cate-\ngory contains three datasets. MRPC Microsoft\nResearch Paraphrase Corpus (Dolan and Brock-\nett, 2005) is a dataset of sentence pairs. The task\nis to determine whether a pair of sentences are\nsemantically equivalent (binary classiﬁcation).\nQQP Quora Question Pairs (Chen et al., 2018) is\na dataset of question pairs taken from the Quora\nwebsite. The goal is to determine whether a pair\nof questions are semantically equivalent (binary\nclassiﬁcation). STS-B Semantic Textual Similarity\nBenchmark (Cer et al., 2017) is a dataset com-\nposed of sentence pairs, each annotated with a\nscore between 1 and 5, indicating the semantic\nsimilarity level of both sentences. The task is to\npredict these scores (regression).\nMisc. datasets The two datasets in this cate-\ngory are not used for the cross dataset evalua-\ntion, due to the lack of commonality between\ntheir tasks. CoLA The Corpus of Linguistic Ac-\nceptability dataset (Warstadt et al., 2018) consists\nof examples of expert English sentence accept-\nability judgments. Each sample is annotated by\nwhether it is a grammatically sentence of English\n(binary classiﬁcation). SST-2The Stanford Senti-\nment Treebank (Socher et al., 2013) is a dataset\ncomposed of sentences assigned with a human\nannotations of their sentiment. The task is to de-\ntermine whether the sentiment of each sentence\nis positive or negative (binary classiﬁcation).\nB Cross-task generalization results for all\nbaselines\nFor the sake of completeness, we report cross\ndataset evaluations, for all six models described\nin the main text. Speciﬁcally, we compare MML,\nBERT , MV-5, MV-1024, BERT-Drop and BERT-\nEnsemble on cross dataset evaluations for in-\nference datasets, and similarity and paraphrase\ndatasets.\nTab. 4 presents the performance of the above\nsix models evaluated on the cross dataset evalu-\nation for the NLI category. Each row represents\na different model, trained on a single dataset as-\nsociated by its name. Each column corresponds\nto an evaluation on four datasets in this category.\nThe last column presents the improvement ob-\ntained by each model, compared to BERT , and\ncalculated across the three hold-out datasets. Ac-\ncuracy is reported for all datasets on both the\ntrain and development sets, in the format train\n197\naccuracy/development accuracy. Note that no\ntraining is done on the training set in the case of\ncross-task evaluation. Therefore, the training set\ncan be seen as an additional dataset to evaluate\non.\nAs can be seen in Tab. 4, MML-RTE outper-\nforms BERT-RTE on all datasets, and results with\nsimilar performance compared to MV-5-RTE, MV-\n1024-RTE and BERT-Drop-RTE. For the models\ntrained with MNLI dataset, we found that the\nfour models MML-MNLI, MV-5-MNLI, MV-1024-\nMNLI and BERT-Drop-MNLI, yield similar results,\nwhere all produce a slight improvement com-\npared to BERT . For QNLI, MML yields a signiﬁcant\nimprovement compared to all models. Specif-\nically, MML-QNLI yields +4.48%/7.63% relative\nimprovement compared to BERT , averaged on the\ntrain and development set of the three hold-out\ndataests. BERT-Drop-QNLI yields signiﬁcantly de-\ncreased performance of -1.93%/-2.89% compared\nto BERT , MV-5 yields improved performance of\n+1.0%/+2.73%, and MV-1024 results with a signif-\nicantly decreased performance of -2.64% on the\ntrain sets, and somehow similar performance to\nBERT on the development set.\nFor SNLI, MML and BERT-Drop yield similar\nperformance compared to BERT on the train-\ning sets, where MML shows slightly better per-\nformance. On the development set, both MML\nand Bert-Drop present slightly decreased perfor-\nmance. Mv-5 and Mv-1024 exhibit signiﬁcantly\ndecreased performance on both train and devel-\nopment sets.\nIn Tab.5, we present the performance of all six\nﬁne-tune variants, evaluated on the cross simi-\nlarity and paraphrasing datasets. For QQP-based\nmodels, MML and MV-5 were able to improve the\ncross evaluation performance compared to BERT ,\nmeasured on the two hold-out datasets. Both\nMML-QQP and MV-5-QQP yield more than one\npercentage of relative improvement on the train\nsets, and less than one percentage of improve-\nment on the developments sets, where MV-5-\nQQP presents slightly better results. On the other\nhand, BERT-Drop-QQP and MV-1024-QQP yield\nsigniﬁcantly decreased performance compared\nto BERT , each results with 1.76%-3.22% of de-\ncreased performance on the train/development\nsets.\nFor MRPC, MML outperforms all models, show-\ning a signiﬁcantly improvement of +4.54%/3.82%\non the train/development sets, when compared\nto BERT . The other models where able to improve\nBERT in +2.69% up to +3.89% on the train and\n+1.66% up to +2.95% on the development sets,\nwhich are signiﬁcantly inferior to MML-MRPC.\nFor STS-B, we see that MML, MV-5 and BERT-\nDrop were able to yield similar improvement\ncompared to BERT , where MV-1024 yields de-\ncreased performance on the train sets, and simi-\nlar performance on the development set.\nAll in all, MML gained the highest improve-\nment, accumulated across all evaluations. Speciﬁ-\ncally, when averaging the cross evaluation results\nover the train sets, MML shows an average im-\nprovement of +3.0%, BERT-Drop shows average\nimprovement of +1.64%, MV-5 yields +2.27% and\nMV-1024 yields +0.46%.\n198\nModel RTE MNLI QNLI SNLI\nImprovement\ncompared\nto BERT\nBERT-RTE 96.06/70.39 69.42/69.17 52.46/52.84 68.02/69.87 -\nMML-RTE 99.39/80.14 79.24/78.42 50.86/51.30 80.85/82.53 +9.98%/+9.51%\nMV-1024-RTE 99.39/80.14 79.24/78.42 50.86/51.30 80.85/82.53 +9.98%/+9.51%\nMV-5-RTE 100.0/75.45 74.50/73.73 58.22/58.72 75.48/77.15 +9.75% /+9.36%\nBERT-D-RTE 100.0/76.53 79.31/78.29 52.55/52.80 79.53/81.10 +10.4% /+9.71%\nBERT-Ensemble-RTE 96.06/70.39 70.31/70.02 51.25/51.89 69.54/69.97 -1.20%/+0.34%\nBERT-MNLI 79.15/76.89 99.59/86.58 49.88/51.05 81.65/83.65 -\nMML-MNLI 79.35/78.70 99.74/87.18 49.64/50.22 82.37/83.94 +0.21% /+0.35%\nMV-1024-MNLI 78.91/78.70 98.71/86.24 49.93/50.48 82.33/83.60 +0.20% /+0.39%\nMV-5-MNLI 78.83/77.61 98.11/87.00 49.91/50.46 82.51/83.95 +0.23% /+0.04%\nBERT-D-MNLI 79.40/78.70 99.90/87.04 49.75/50.22 82.32/83.71 +0.29% /+0.26%\nBERT-Ensemble-MNLI 78.68/75.58 99.59/86.58 50.21/52.21 82.51/82.35 +1.54%/+0.54%\nBERT-QNLI 53.37/48.73 59.76/59.89 99.99/94.01 59.33/60.03 -\nMML-QNLI 53.41/53.79 64.93/63.85 95.75/92.86 62.13/63.58 +4.48%/+7.63%\nMV-1024-QNLI 51.04/51.26 60.28/58.6 99.49/92.24 58.68/58.78 -2.64%/+0.31%\nMV-5-QNLI 51.12/50.90 61.58/60.36 97.95/92.18 61.81/61.86 +1.00%/+2.73%\nBERT-D-QNLI 49.91/45.84 60.48/59.33 99.99/93.17 59.22/59.14 -1.93%/-2.89%\nBERT-Ensemble-QNLI 54.52/49.84 58.84/59.25 98.95/93.01 60.35/61.87 -1.75%/-1.02%\nBERT-SNLI 71.28/70.03 75.82/75.79 49.62/50.72 99.74/91.08 -\nMML-SNLI 71.88/69.31 75.79/76.37 49.32/50.32 99.30/91.38 +0.07%/-0.26%\nMV-1024-SNLI 68.87/64.98 73.41/74.09 48.89/49.33 94.54/91.16 -2.67% /-4.06%\nMV-5-SNLI 71.24/67.87 75.38/75.00 49.42/50.35 95.63/91.18 -0.35% /-1.62%\nBERT-D-SNLI 71.36/70.84 76.16/76.38 49.32/49.35 95.65/91.45 -1.35% /-1.24%\nBERT-Ensemble-SNLI 72.52/71.89 74.75/74.85 50.54/51.02 99.57/91.2 -1.23%/-1.10%\nMML impr. +0.38%/+3.90%+7.58%/+6.91%-1.37%/-1.77%+8.15%/+8.12%\nMV-1024 impr. -2.68%/+0.11%+3.94%/+2.99%-1.47%/-2.25%+6.19%/+5.32%\nMV-5 impr. -1.55%/+0.76%+3.26%/+2.11%+3.54%/3.08% +5.40%/-0.94%\nBERT-D impr. -2.01%/-0.80% +5.29%/+4.34%-0.23%/-1.46%+5.85%/+4.88%\nBERT-Ensemble impr. -1.62%/-1.40% +1.64%/+0.51%-0.52%/-0.54% -1.66%/-0.55%\nTable 4: Cross dataset evaluation for Language Inference tasks for all ablation models presented in the main\ntext (BERT-D stands for BERT-Drop). Train/development accuracy are reported separately for each dataset.\nEach model (a row in the table) was trained on a single dataset denoted by its name, and was evaluated on\nthe train/development sets of all four datasets. The last column indicates the relative average improvement\nobtained by each model compared to BERT , and averaged across the three hold-out datasets. The last four\nrows present the average column-wise improvement each technique yields, which is aggregated from the three\nmodels of the same technique. BERT models were reproduced with the same hyperparamters used for MML\n(all BERT reproductions result with similar or better performance compared to the one published in (Devlin\net al., 2018)).\n199\nModel QQP MRPC STS-B* Improvement\ncompared to BERT\nBERT-QQP 99.73/91.57 66.90/68.85 88.34/90.12 -\nMML-QQP 99.74/91.68 67.77/68.87 89.11/90.55 +1.08%/+0.25%\nMV-1024-QQP 98.78/91.42 65.59/67.40 88.20/90.01 -2.16%/-3.22%\nMV-5-QQP 99.40/91.45 68.19/68.60 89.00/91.09 +1.58%/+0.63%\nBERT-Drop-QQP 99.57/91.64 64.50/65.68 88.40/89.24 -1.76% /-2.80%\nBERT-Ensemble-QQP 99.73/91.42 67.45/69.85 89.41/90.45 -1.51%/-1.18%\nBERT-MRPC 65.28/65.18 99.37/87.25 82.53/88.58 -\nMML-MRPC 68.37/68.15 99.23/88.97 86.12/91.32 +4.54%/+3.82%\nMV-1024-MRPC 67.33/67.44 99.91/89.70 85.19/90.55 +3.18%/+2.76%\nMV-5-MRPC 66.69/66.46 99.94/87.99 85.20/89.79 +2.69%/+1.66%\nBERT-Drop-MRPC 67.65/67.66 99.97/87.5 85.97/90.45 +3.89% /+2.95%\nBERT-Ensemble-MRPC 64.48/64.88 99.56/87.25 81.23/87.98 +1.58%/+1.27%\nBERT-STS-B* 73.13/73.11 75.59/75.49 100.0/95.49 -\nMML-STS-B* 74.13/74.40 75.51/77.94 99.85/96.70 +0.63%/+2.50%\nMV-1024-STS-B* 73.96/74.25 75.59/77.20 99.85/96.48 -2.64%/+0.31%\nMV-5-STS-B* 74.93/75.15 75.21/75.98 100.0/97.03 +0.97%/+1.71%\nBERT-Drop-STS-B* 74.29/75.45 75.40/76.22 100.0/96.70 +0.66% /+2.08%\nBERT-Ensemble-STS-B* 74.45/74.21 76.01/76.45 100.0/96.15 -1.27%/-1.08%\nMML impr. +3.05%/+3.16% +0.59%/+1.63% +2.61%/+1.78%\nMV-1024 impr. +2.13%/+2.51% +0.97%/+0.07% +1.53%/+1.05%\nMV-5 impr. +2.31%/+2.37% +0.71%/+0.14% +1.99%/+1.22%\nBERT-Drop impr. +2.60%/+3.50 -1.91%/-1.81% +2.11%/+0.56%\nBERT-Ensemble impr. -0.31%/-1.02% -1.42%/-1.28% +0.21%/+0.19%\nTable 5: Cross dataset evaluation for similarity and paraphrase tasks. STS-B* is the modiﬁed version of STS-B\nthat forms a binary classiﬁcation dataset (instead of regression). STS-B* models were trained as binary classi-\nﬁers on STS-B data. Accuracy scores are reported through all evaluations. The last column presents the relative\ncross dataset improvement obtained by each model compared to BERT , and averaged across the two hold-out\ndatasets. The last four rows present the average column-wise improvement each technique yields, which is\ncomputed from the two models of the same technique.",
  "topic": "Generalization",
  "concepts": [
    {
      "name": "Generalization",
      "score": 0.7907514572143555
    },
    {
      "name": "Computer science",
      "score": 0.7781258821487427
    },
    {
      "name": "Classifier (UML)",
      "score": 0.6679809093475342
    },
    {
      "name": "Task (project management)",
      "score": 0.5981785655021667
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5974819660186768
    },
    {
      "name": "Machine learning",
      "score": 0.5219744443893433
    },
    {
      "name": "Language model",
      "score": 0.47912856936454773
    },
    {
      "name": "Code (set theory)",
      "score": 0.4788617789745331
    },
    {
      "name": "Training set",
      "score": 0.44536393880844116
    },
    {
      "name": "Natural language processing",
      "score": 0.4000755548477173
    },
    {
      "name": "Mathematics",
      "score": 0.1214931309223175
    },
    {
      "name": "Engineering",
      "score": 0.06444519758224487
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I16391192",
      "name": "Tel Aviv University",
      "country": "IL"
    }
  ],
  "cited_by": 7
}